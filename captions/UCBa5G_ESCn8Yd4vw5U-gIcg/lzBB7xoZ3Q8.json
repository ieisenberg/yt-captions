[
  {
    "start": "0",
    "end": "165000"
  },
  {
    "text": "I propose we get started, we've got lots that we want to accomplish today. Um, this is a tweet that I saw yesterday from a",
    "start": "4430",
    "end": "13139"
  },
  {
    "text": "research group at UMass, uh, and I thought this is a nice bridge between last week and today.",
    "start": "13140",
    "end": "18360"
  },
  {
    "text": "So last week we talked a lot about hyper-parameter tuning and kind of very expensive testing regimes",
    "start": "18360",
    "end": "25425"
  },
  {
    "text": "that the field has moved toward in the era of deep learning. Uh, and I kind of preached a pragmatic approach to all of this,",
    "start": "25425",
    "end": "33330"
  },
  {
    "text": "where you balance your own budget in terms of money and time and resources and everything else, when you design your experimental paradigm and I thought,",
    "start": "33330",
    "end": "41055"
  },
  {
    "text": "inevitably, you're going to have to compromise, and the, the real thing that you want to do in the paper is just be upfront about where you had to compromise,",
    "start": "41055",
    "end": "48260"
  },
  {
    "text": "which just means being open and honest about your experimental regime. One ingredient, one factor that I didn't include in that discussion but I",
    "start": "48260",
    "end": "56540"
  },
  {
    "text": "think I will for future years is just the environmental, uh, concerns that we might have around very ambitious hyperparameter tuning regimes,",
    "start": "56540",
    "end": "65809"
  },
  {
    "text": "and that's what this paper is about. I haven't seen the paper itself but this seems to be one of the core tables from the paper.",
    "start": "65809",
    "end": "71840"
  },
  {
    "text": "Uh, and these numbers are quite shocking, right? So you've got along this column here, CO2 emissions.",
    "start": "71840",
    "end": "78330"
  },
  {
    "text": "And I guess for some baseline things, it gives you like air travel, human life average and then American life average.",
    "start": "78330",
    "end": "85610"
  },
  {
    "text": "That already is a kind of shocking statistic there. Uh, car lifetime.",
    "start": "85610",
    "end": "91555"
  },
  {
    "text": "And then down here, and this is kind of striking because, I mean, training a state of the art NLP model for tagging,",
    "start": "91555",
    "end": "98805"
  },
  {
    "text": "um, it's 13, CO2. Uh, but with experimentation,",
    "start": "98805",
    "end": "103910"
  },
  {
    "text": "that number jumps up to almost one year for the average American in terms of CO2 emissions.",
    "start": "103910",
    "end": "109720"
  },
  {
    "text": "And we're gonna- this is the reason this is a nice bridge is we're gonna talk about the transformer today among other things.",
    "start": "109720",
    "end": "115270"
  },
  {
    "text": "And again, kind of, a modest but eye-opening amount of energy consumed for the transformer large,",
    "start": "115270",
    "end": "120480"
  },
  {
    "text": "but once you start to do hyperparameter optimization, you're talking about more than 10 years of",
    "start": "120480",
    "end": "125870"
  },
  {
    "text": "average American life CO2 emissions to do this hyperparameter optimization. Um, this is a kind of environment- environmental disaster, right?",
    "start": "125870",
    "end": "134390"
  },
  {
    "text": "Are you interested in deep learning for NLP but also concerned about the CO2 footprint of training? You should be.",
    "start": "134390",
    "end": "139790"
  },
  {
    "text": "[LAUGHTER] Um, yeah, I really think this is, this is food for thought, and frankly, I think this should be part of our thinking about",
    "start": "139790",
    "end": "147680"
  },
  {
    "text": "the kinds of experiments that we run and where we invest our resources. So I encourage you to check out this paper once it's available.",
    "start": "147680",
    "end": "154770"
  },
  {
    "text": "Onto the transformer. I guess if I'm successful in my lecture today it will be a, uh,",
    "start": "155390",
    "end": "160709"
  },
  {
    "text": "environmental disaster, so we'll all go forth and train large transformers.",
    "start": "160710",
    "end": "166370"
  },
  {
    "start": "165000",
    "end": "242000"
  },
  {
    "text": "So what we wanna to do for the first part of class today until probably about 5:30 is talk about contextual word representations,",
    "start": "166370",
    "end": "173330"
  },
  {
    "text": "and then Cindy is gonna take over and do a mini-lecture on practical approaches to dealing with very long text learning,",
    "start": "173330",
    "end": "179840"
  },
  {
    "text": "dense representations of long text, essentially which I think could be useful for a lot of your projects, so it's well-timed.",
    "start": "179840",
    "end": "187645"
  },
  {
    "text": "But contextual word representations, I do think this is one of the exciting things that happened in recent times in NLP.",
    "start": "187645",
    "end": "194120"
  },
  {
    "text": "I'm gonna try to motivate that, I'll do a little bit of like guided and high-level insights about why I think this is a wonderful development,",
    "start": "194120",
    "end": "201650"
  },
  {
    "text": "in addition to all the, um, practical gains that it's led to on a wide variety of tasks.",
    "start": "201650",
    "end": "206900"
  },
  {
    "text": "That's kind of the core idea behind this, contextual word representations. Then, we're gonna talk about three approaches to learning such representations,",
    "start": "206900",
    "end": "215555"
  },
  {
    "text": "ELMo, the transformer and BERT. Uh, and then the final section which we might just leave for your own self study,",
    "start": "215555",
    "end": "223390"
  },
  {
    "text": "just introduces you to that notebook that I posted at the start of the quarter which shows you some easy and practical ways that you could bring",
    "start": "223390",
    "end": "230540"
  },
  {
    "text": "ELMo and BERT into your project even without having to do a complete code overhaul so that you're embedded in their PyTorch or TensorFlow code.",
    "start": "230540",
    "end": "240069"
  },
  {
    "text": "There's a bunch of associated notebooks. So in add- in addition to contextualreps.i, the notebook, um,",
    "start": "240710",
    "end": "248020"
  },
  {
    "start": "242000",
    "end": "407000"
  },
  {
    "text": "there's also Smith to 2019 which is a nice overview paper that kind of does more of",
    "start": "248020",
    "end": "253900"
  },
  {
    "text": "providing high-level insights about how we move from word vectors to contextual word representations,",
    "start": "253900",
    "end": "260479"
  },
  {
    "text": "but also kind of the guiding linguistic and empirical ideas behind this whole movement. I also wanted to call out that the CS224n lectures on these topics are really good,",
    "start": "260480",
    "end": "271240"
  },
  {
    "text": "and they are somewhat different from my lecture. So they, they offer much more of the history and many more connections with different tasks whereas given my limited time,",
    "start": "271240",
    "end": "279775"
  },
  {
    "text": "I've decided that I'm gonna try to just convey to you how these models work, and with luck, we'll get a sense not only for how",
    "start": "279775",
    "end": "286460"
  },
  {
    "text": "they work but also kind of why they work. And this is perfectly timed because as you'll",
    "start": "286460",
    "end": "291530"
  },
  {
    "text": "see we're going to build on a lot of stuff that we've already done. It's like we're assembling raw materials in",
    "start": "291530",
    "end": "296660"
  },
  {
    "text": "a really creative and interesting way to achieve some big gains. And then, for ELMo transformer and BERT,",
    "start": "296660",
    "end": "303450"
  },
  {
    "text": "I just mentioned here the core papers and also the project sites which have code",
    "start": "303450",
    "end": "309050"
  },
  {
    "text": "and also pre-trained representations available for them. In that context I thought I would call out",
    "start": "309050",
    "end": "314270"
  },
  {
    "text": "especially this wonderful kind of notebook paper called, The Annotated Transformer by Alexander Rush,",
    "start": "314270",
    "end": "320000"
  },
  {
    "text": "who's on the faculty at Harvard. Um, it's the actual transformer paper.",
    "start": "320000",
    "end": "325264"
  },
  {
    "text": "So verbatim copy with a few comments from him. But woven into the paper is PyTorch code,",
    "start": "325265",
    "end": "331520"
  },
  {
    "text": "really nice, efficiently written clear PyTorch code, for every one of the technical concepts so that by the end he has fully",
    "start": "331520",
    "end": "339020"
  },
  {
    "text": "implemented the model and also tested it in a bunch of ways. And I just think that, I mean,",
    "start": "339020",
    "end": "344539"
  },
  {
    "text": "what a wonderful kind of contribution to make. It's like with all due respect to the original authors of the paper,",
    "start": "344540",
    "end": "350510"
  },
  {
    "text": "and it's quite a nice paper, this is strictly better, like the combination of Alexander Rush plus",
    "start": "350510",
    "end": "356210"
  },
  {
    "text": "that original author team is strictly more valuable because it- it saves you from",
    "start": "356210",
    "end": "361280"
  },
  {
    "text": "this thing that I experienced all the time in reading NLP papers that like, I can't quite tell from the language how the pieces",
    "start": "361280",
    "end": "368030"
  },
  {
    "text": "fit together or what the dimensionalities of things are, and I can't quite do the matrix multiplications in my head.",
    "start": "368030",
    "end": "373930"
  },
  {
    "text": "And then, I screw them up when I try them with pad and paper. There it's just all there in code,",
    "start": "373930",
    "end": "379120"
  },
  {
    "text": "like every single ambiguity is fully resolved there. So hats off to him and I encourage that",
    "start": "379120",
    "end": "385130"
  },
  {
    "text": "as a way to study what's happening in the transformer paper. And I also frankly encourage you to do these things",
    "start": "385130",
    "end": "390440"
  },
  {
    "text": "yourself because not only is it a wonderful contribution but I'd say there's no better way to deeply learn",
    "start": "390440",
    "end": "395660"
  },
  {
    "text": "a paper than to do something like this, quite an achievement. Let's move to the high-level motivation as I see it kind of with my linguist hat on.",
    "start": "395660",
    "end": "406070"
  },
  {
    "text": "So I'm just going to show you some examples here. This is just around the verb break. The vase broke, dawn broke,",
    "start": "406070",
    "end": "412940"
  },
  {
    "start": "407000",
    "end": "730000"
  },
  {
    "text": "the news broke, Sandy broke the world record, Sandy broke the law, the burglar broke into the house,",
    "start": "412940",
    "end": "419895"
  },
  {
    "text": "the newscaster broke into the movie broadcast and we broke even. So they all involve a word form that sounds like break.",
    "start": "419895",
    "end": "428135"
  },
  {
    "text": "I don't know how many words senses there are here, right. In the sense that I don't know whether we're dealing with",
    "start": "428135",
    "end": "434300"
  },
  {
    "text": "11 different lexical items or 2 or 6 or what. I kind of have the feeling in looking at paradigms like this that",
    "start": "434300",
    "end": "442294"
  },
  {
    "text": "the linguist notion that we can cu- cut up the world into different word senses,",
    "start": "442295",
    "end": "447875"
  },
  {
    "text": "fully differentiate them, uh, is breaking down. The nature is not quite cut up in that way.",
    "start": "447875",
    "end": "453620"
  },
  {
    "text": "It feels like we're drawing on a bunch of metaphorical connections and linguistic connections that are being shaped by",
    "start": "453620",
    "end": "459260"
  },
  {
    "text": "the morphosyntactic environment- environment in which this verb is being used together with like its particles,",
    "start": "459260",
    "end": "465175"
  },
  {
    "text": "and what we know about the world. I think a lot of these sentences are strictly speaking ambiguous in some sense and we",
    "start": "465175",
    "end": "470630"
  },
  {
    "text": "perceive one reading but not others because of what we know about the world, and on and on. That feels like the real picture of what's",
    "start": "470630",
    "end": "477650"
  },
  {
    "text": "happening with the verb break in this paradigm. Here are a few more, flat tire, flat beer, flat note,",
    "start": "477650",
    "end": "484070"
  },
  {
    "text": "flat surface, or throw a party, a fight, a ball, a fit. How many senses of the verb flat are in 2A or throw in 2B?",
    "start": "484070",
    "end": "494180"
  },
  {
    "text": "I don't know. Are they even related? Are there four lexical items or two or one?",
    "start": "494180",
    "end": "499280"
  },
  {
    "text": "I have no answer to this question but I can tell that the sense of these verbs is pretty well cut up",
    "start": "499280",
    "end": "505610"
  },
  {
    "text": "once I put it in a particular morphosyntactic context and bringing kind of world knowledge about what I expect the readings to be.",
    "start": "505610",
    "end": "513450"
  },
  {
    "text": "And all of this I think points towards contextual representations as the right way to go.",
    "start": "513450",
    "end": "519159"
  },
  {
    "text": "Because if you think back to the first unit of this course where we represented words in isolation as vectors,",
    "start": "519160",
    "end": "525685"
  },
  {
    "text": "what are we gonna do for one? We're gonna have to smoosh together all of those senses into a single vector.",
    "start": "525685",
    "end": "532630"
  },
  {
    "text": "or we're gonna have to do the really hard work of kind of breaking up this break into a bunch of different vectors.",
    "start": "532630",
    "end": "538855"
  },
  {
    "text": "And I think that neither of those is gonna work out. And moreover, even if it did work out, we have this kind of average or static representation.",
    "start": "538855",
    "end": "546610"
  },
  {
    "text": "And what we know from these examples is that, that vector ought to be influenced by the context in- that it's sitting in, right?",
    "start": "546610",
    "end": "553509"
  },
  {
    "text": "That's our intuition that the vase broke means what it means or dawn broke means what it means. Because of other words that are interacting with this verb break.",
    "start": "553510",
    "end": "562465"
  },
  {
    "text": "Here's some more examples. And these are subtly different. So I caught, uh, a crane caught a fish.",
    "start": "562465",
    "end": "567895"
  },
  {
    "text": "You probably intuited that's a bird. A crane picked up the steel beam. You probably intuit that that's a machine.",
    "start": "567895",
    "end": "573640"
  },
  {
    "text": "And for I saw a crane. You might just be uncertain about what kind of crane it is; bird or machine.",
    "start": "573640",
    "end": "578680"
  },
  {
    "text": "I think that for 3A and 3B, there are readings where a machine catches a fish or a bird picks up a beam.",
    "start": "578680",
    "end": "585250"
  },
  {
    "text": "But they're just highly unlikely given what we know about world knowledge. But I don't wanna lose sight of the fact that",
    "start": "585250",
    "end": "591449"
  },
  {
    "text": "you were steered toward one reading or another, because of the syntactic environment. The semantic environment.",
    "start": "591450",
    "end": "597210"
  },
  {
    "text": "And also what you know about the world. All of that is shaping the sense- the word sense for crane here.",
    "start": "597210",
    "end": "602615"
  },
  {
    "text": "And maybe less shaped by 3C. And then I think we could extend this to examples that I showed you, when we talk about discourse and dialogue.",
    "start": "602615",
    "end": "609220"
  },
  {
    "text": "So are there typos? I didn't see anything. We think about the sense of any there at the end. It's shaped by the preceding discourse?",
    "start": "609220",
    "end": "615970"
  },
  {
    "text": "Are there bookstores downtown? I didn't see any. Same sentence. But the sense of that entire sentence and especially any I think there",
    "start": "615970",
    "end": "623800"
  },
  {
    "text": "then maybe its elliptical content is being shaped by the preceding environment. And all of this points to the idea that static word vectors are just not the way to go.",
    "start": "623800",
    "end": "633399"
  },
  {
    "text": "This is sort of, I was reflecting on this like, if you plucked, uh, your average theoretical linguist off the street.",
    "start": "633400",
    "end": "640584"
  },
  {
    "text": "And tried to convince them that words could be represented as vectors of real numbers.",
    "start": "640585",
    "end": "647200"
  },
  {
    "text": "I think it will be tough going. But I think after a little while, you could convince them that this is a smart way to go.",
    "start": "647200",
    "end": "653019"
  },
  {
    "text": "And in fact, it's quite close to what they think about when they think about word meanings as kind of very high dimensional objects with lots of relationships, right?",
    "start": "653020",
    "end": "660640"
  },
  {
    "text": "Like, if you think about WordNet, you could represent those things as vectors. And kind of, you're not very far from what linguists are already doing.",
    "start": "660640",
    "end": "668305"
  },
  {
    "text": "But I think once you convince the linguist of that, they would say, Wait a second. Are these vectors just for words in isolation?",
    "start": "668305",
    "end": "674920"
  },
  {
    "text": "And that's where you would lose them. Because many linguists think that word senses are",
    "start": "674920",
    "end": "680050"
  },
  {
    "text": "kind of shaped by their morphosyntactic environment. And then if you talk to them about usage, they would say, well,",
    "start": "680050",
    "end": "685690"
  },
  {
    "text": "that's going to be shaped by the whole discourse context and everything else. And that's where your idea that words could just be single vectors.",
    "start": "685690",
    "end": "692230"
  },
  {
    "text": "You'd never convince them of that. And that to, I think ELMo and BERT. I think you can get them on board with that, that view of meaning.",
    "start": "692230",
    "end": "700015"
  },
  {
    "text": "So an interesting twist. There's of course lots of empirical motivation that I've kind of left out of this slideshow.",
    "start": "700015",
    "end": "706870"
  },
  {
    "text": "Because you can find those tables in the papers and all over the web. It truly is a breakthrough on lots of tasks.",
    "start": "706870",
    "end": "712960"
  },
  {
    "text": "You might have experienced this yourselves. I think some of our bake-off wins traced to BERT and ELMo and things like that.",
    "start": "712960",
    "end": "718750"
  },
  {
    "text": "So you guys know about that picture. So I thought I would fill in kind of this linguistic one here.",
    "start": "718750",
    "end": "724250"
  },
  {
    "text": "Another perspective that I thought, we might think about here, just by way of high level motivation is kind of this notion of model structure,",
    "start": "725100",
    "end": "733464"
  },
  {
    "start": "730000",
    "end": "920000"
  },
  {
    "text": "linguistic structure, and the biases that these are encoding. Right? So this is, like up here for example,",
    "start": "733465",
    "end": "741010"
  },
  {
    "text": "the rock rules where I just take all the vectors for those words. Those could be like GloVe. So isolated vectors and maybe sum them or",
    "start": "741010",
    "end": "747880"
  },
  {
    "text": "average them to get a representation of the whole sentence. That model is very high bias in the sense that it's making a lot of",
    "start": "747880",
    "end": "754630"
  },
  {
    "text": "presumptions about how word meanings will interact to form a meaning for the whole. Because you basically hard code, right?",
    "start": "754630",
    "end": "761560"
  },
  {
    "text": "And anything that deviates from GloVe and sum is gonna be out of step with your model.",
    "start": "761560",
    "end": "767980"
  },
  {
    "text": "Over here on the right, we have a model that's less biased in the sense that you can learn",
    "start": "767980",
    "end": "774970"
  },
  {
    "text": "a very free-form function for combining these three meanings into a whole meaning for the sentence.",
    "start": "774970",
    "end": "780415"
  },
  {
    "text": "We've imposed linear structure. But if it turns out that the linear structure ought to be",
    "start": "780415",
    "end": "785830"
  },
  {
    "text": "just the sum function as on the left, we can learn that. But we could of course learn lots of other functions.",
    "start": "785830",
    "end": "791320"
  },
  {
    "text": "And one thing that we're doing in terms of bias is creating a bias for a kind of linear flow through the sentence,",
    "start": "791320",
    "end": "797604"
  },
  {
    "text": "which is pretty natural if you think about sentence processing. But it is a- as a substantive thing to say,",
    "start": "797604",
    "end": "803275"
  },
  {
    "text": "about how to represent the meanings. And then down on the left here, you do that in a different way. So here, the bias is that there is tree structure,",
    "start": "803275",
    "end": "811315"
  },
  {
    "text": "that the rock forms a unit in a way that rock rules does not. And that the composition ought to proceed kinda from bottom up,",
    "start": "811315",
    "end": "818940"
  },
  {
    "text": "paying attention to what the linguists called the constituency of this example. Very high bias, right?",
    "start": "818940",
    "end": "825264"
  },
  {
    "text": "If we're wrong about this tree structure, we might be in real trouble in terms of what the model is gonna do. If we're right about the tree structure,",
    "start": "825265",
    "end": "831880"
  },
  {
    "text": "this could be a huge gain in terms of telling the model how to combine things and what to pay attention to.",
    "start": "831880",
    "end": "838060"
  },
  {
    "text": "So these are all kind of very high bias. And then if we take this RNN here, and run it bidirectionally,",
    "start": "838060",
    "end": "844795"
  },
  {
    "text": "and we add a bunch of these attention mechanisms, that we're gonna talk a lot about today. That is pretty far along the spectrum to being",
    "start": "844795",
    "end": "852415"
  },
  {
    "text": "unbiased about what kind of interactions we're gonna see. No more presumption on left, right, because we're gonna go on both directions.",
    "start": "852415",
    "end": "859375"
  },
  {
    "text": "And then not, not only that, but we're gonna have all these attention mechanisms that are kind of allow us to leap around in the sentence,",
    "start": "859375",
    "end": "865690"
  },
  {
    "text": "if that's what the data tell us to do, essentially. And that's kind of very free form.",
    "start": "865690",
    "end": "870910"
  },
  {
    "text": "And I would say that all of the models that we talk about today are in this category, pushing in the direction of assuming that we",
    "start": "870910",
    "end": "877449"
  },
  {
    "text": "don't know anything about what this data is gonna be like. But we know they're gonna be kind of lots of dependencies that we need to learn.",
    "start": "877450",
    "end": "883870"
  },
  {
    "text": "So let's just learn them in a very free form way. Give the model as much capacity to learn them from",
    "start": "883870",
    "end": "889060"
  },
  {
    "text": "data and not bias it in any one particular direction. And then I think like, of,",
    "start": "889060",
    "end": "894385"
  },
  {
    "text": "of the models we look at today, BERT is the farthest along on this continuum, BERT and the transformer.",
    "start": "894385",
    "end": "900800"
  },
  {
    "text": "Okay. A few more things by way of setting the stage, because I did wanna impress upon you that you kind of",
    "start": "902250",
    "end": "908380"
  },
  {
    "text": "already have all the raw ingredients for all of these models. They're incredibly complicated, because there's like tons of stuff happening in them.",
    "start": "908380",
    "end": "915339"
  },
  {
    "text": "But at their core, these are things that are familiar to you. So one team will be attention.",
    "start": "915340",
    "end": "920605"
  },
  {
    "start": "920000",
    "end": "1010000"
  },
  {
    "text": "You'll see this in the Transformer and in Bert. And so I thought I'd just remind you that we talked about attention.",
    "start": "920605",
    "end": "925779"
  },
  {
    "text": "Here is an NLI example. Every dog danced. Some poodle danced. You have all those word representations down here.",
    "start": "925780",
    "end": "933010"
  },
  {
    "text": "And then these hidden states, for an RNN for the premise, and an RNN for the hypothesis.",
    "start": "933010",
    "end": "938800"
  },
  {
    "text": "And the state to watch is HC up here. In the simplest form of global attention that we considered,",
    "start": "938800",
    "end": "945340"
  },
  {
    "text": "what I do is calculate a dot product score by multiplying this vector by all the vectors in the on-premise.",
    "start": "945340",
    "end": "952524"
  },
  {
    "text": "That's what happens here. Those get normalized via a softmax function. And those scores then down here are used to weight everything in the premise.",
    "start": "952525",
    "end": "961915"
  },
  {
    "text": "And in the standard approach, we take the mean. So this Kappa here is a vector that kind of",
    "start": "961915",
    "end": "967180"
  },
  {
    "text": "summarizes everything that was in the premise as weighted by attention. And then you combine that into the classifier,",
    "start": "967180",
    "end": "975100"
  },
  {
    "text": "so that the ultimate classification decision here, this hidden representation is Kappa. And this final state over here,",
    "start": "975100",
    "end": "981940"
  },
  {
    "text": "multiplied through some weights. There's a couple ways you can do that. And then finally, you get the classifier on top of that hidden state.",
    "start": "981940",
    "end": "988960"
  },
  {
    "text": "Remember that? Simplest form of global attention with dot products. And the thing to keep in mind is,",
    "start": "988960",
    "end": "994870"
  },
  {
    "text": "the core mechanism which we applied just for the final state looking back at all the premise states.",
    "start": "994870",
    "end": "1000795"
  },
  {
    "text": "When we get to the transformer, you're gonna see this all over the place. That's the idea. But it's exactly the same calculation.",
    "start": "1000795",
    "end": "1007020"
  },
  {
    "text": "I'll try to call that out. Another thing that we haven't really talked about a lot",
    "start": "1007020",
    "end": "1012825"
  },
  {
    "start": "1010000",
    "end": "1164000"
  },
  {
    "text": "since the first unit is this notion of sub-word modeling. It came up in the first unit,",
    "start": "1012825",
    "end": "1017910"
  },
  {
    "text": "because we built these vector spaces. And I think some of you saw that, if you kind of broke them down into the character level and then built up representations that way,",
    "start": "1017910",
    "end": "1026100"
  },
  {
    "text": "you could make up for some of the deficiencies in the original space by kind of taking advantage of strengthened information that's shared across the sub-parts of words.",
    "start": "1026100",
    "end": "1035670"
  },
  {
    "text": "In the papers that we'll look at, especially ELMo, that's done in a somewhat more sophisticated way.",
    "start": "1035670",
    "end": "1041380"
  },
  {
    "text": "So I thought I would just introduce this impressionistically, um, and, uh, let you go off and study it in more detail on your own.",
    "start": "1041380",
    "end": "1048010"
  },
  {
    "text": "So starting down here at the bottom. I just have the word rules, operating at the character level.",
    "start": "1048010",
    "end": "1053665"
  },
  {
    "text": "I'm assuming that we learn embeddings for each one of the characters, just like you would for words. But now, things get interesting.",
    "start": "1053665",
    "end": "1060280"
  },
  {
    "text": "So what I've tried to signal here are a bunch of convolutional filters. This is not an idea that we've talked about in this class.",
    "start": "1060280",
    "end": "1067150"
  },
  {
    "text": "They are not so common in NLP. But they're used by the ELMo team to get word representations.",
    "start": "1067150",
    "end": "1072610"
  },
  {
    "text": "So the idea here is, I have a bunch of filters like, this is a filter of length 1, this is of length 2,",
    "start": "1072610",
    "end": "1078210"
  },
  {
    "text": "and this is of length 3. And what the line signal is that I'm doing a kind of moving window across this example.",
    "start": "1078210",
    "end": "1085060"
  },
  {
    "text": "And I get representations via a dense layer that connects the characters that I'm connecting up into this representation.",
    "start": "1085060",
    "end": "1092875"
  },
  {
    "text": "And then those are combined via what's often called max pooling. So take this top one here which had a window of 3.",
    "start": "1092875",
    "end": "1100960"
  },
  {
    "text": "If these are the vectors that I formed at each one of those steps, the max pooling representation is just taking",
    "start": "1100960",
    "end": "1106929"
  },
  {
    "text": "all the max values from all three of the different vectors to form a final representation.",
    "start": "1106930",
    "end": "1112445"
  },
  {
    "text": "For convolutions in general, this could be the mean or the sum or some other operation like that. Uh, but max pooling is common,",
    "start": "1112445",
    "end": "1119049"
  },
  {
    "text": "and that's what the ELMo team uses. And then to get a representation for the entire sentence,",
    "start": "1119050",
    "end": "1124870"
  },
  {
    "text": "you just piece together all the representations that you got from max pooling and through all of those filters.",
    "start": "1124870",
    "end": "1130855"
  },
  {
    "text": "So you get something that looks like this where this would be the part that goes to this first filter,",
    "start": "1130855",
    "end": "1135865"
  },
  {
    "text": "blue for the second, and green for the third. And that's kind of at the lowest level how the ELMo team starts to represent words.",
    "start": "1135865",
    "end": "1145220"
  },
  {
    "text": "They actually add a few more layers as you'll see. But I thought I would call this out as a separate contribution because this is",
    "start": "1145220",
    "end": "1150290"
  },
  {
    "text": "one approach to kind of gathering strength from the parts of words. The transformer, uh, does this in a somewhat different way.",
    "start": "1150290",
    "end": "1157925"
  },
  {
    "text": "And that's this next idea which again, we haven't discussed but I think this will be really intuitive for you.",
    "start": "1157925",
    "end": "1163400"
  },
  {
    "text": "So part of what you do for the transformer and for BERT is have what's called positional encodings.",
    "start": "1163400",
    "end": "1170455"
  },
  {
    "start": "1164000",
    "end": "1405000"
  },
  {
    "text": "And I've signaled that here. So I have word vectors, the Rock and rules and they have their usual embeddings.",
    "start": "1170455",
    "end": "1176760"
  },
  {
    "text": "But in addition, I'm gonna learn a separate embedding space for different positions in the sentence.",
    "start": "1176760",
    "end": "1183110"
  },
  {
    "text": "That's what signaled by this dark red here one, two, three. They each have their own vector representations.",
    "start": "1183110",
    "end": "1188924"
  },
  {
    "text": "And then it's common to just sum the positional vector and the word vector to get a representation of the word that is context sensitive.",
    "start": "1188925",
    "end": "1197930"
  },
  {
    "text": "That knows what position it was in the linear flow. And you could learn those representations,",
    "start": "1197930",
    "end": "1204155"
  },
  {
    "text": "but a nice idea that's kind of very free, that comes from the transformer paper is that instead of learning",
    "start": "1204155",
    "end": "1211375"
  },
  {
    "text": "them you might just construct them to encode some positional information. So what they do is do this kind of",
    "start": "1211375",
    "end": "1216710"
  },
  {
    "text": "sinusoidal thing where the position is correlated with the, um, periodicity of these waves.",
    "start": "1216710",
    "end": "1223840"
  },
  {
    "text": "And that means that you could generate them for basically any length that you wanted. So that if you saw a length at test time that you'd never seen in training,",
    "start": "1223840",
    "end": "1231715"
  },
  {
    "text": "you could generate the positional vector for it. Sum it with your word vector and you'd be able to do something useful.",
    "start": "1231715",
    "end": "1237395"
  },
  {
    "text": "Whereas, if it's a learned embedding space, you're kind of stuck with what you've seen during training in terms of example lengths.",
    "start": "1237395",
    "end": "1244630"
  },
  {
    "text": "So this is a nice free-form idea that you could apply anywhere. If you fit one of those sum of vectors classifiers,",
    "start": "1244630",
    "end": "1251510"
  },
  {
    "text": "you might have positional encoding as a way of infusing them with a little bit of word order information.",
    "start": "1251510",
    "end": "1257419"
  },
  {
    "text": "And that would be very easy to test that experimentally. It might really give you gains because of this added sensitivity to position",
    "start": "1257420",
    "end": "1263815"
  },
  {
    "text": "that your model would then have. Yeah. You have those instances like the internal eight.",
    "start": "1263815",
    "end": "1270769"
  },
  {
    "text": "Will you use the same positional vectors as like the Rock rules for each one of the positions, or would it because the later two words are",
    "start": "1270770",
    "end": "1277130"
  },
  {
    "text": "different would each of those words have its own separate positional encoding? I think the positional encodings would be the",
    "start": "1277130",
    "end": "1282400"
  },
  {
    "text": "same but the word vectors would be different, and that's how you'd get both commonality between the first words of those two sentences but also a difference.",
    "start": "1282400",
    "end": "1290115"
  },
  {
    "text": "And you might do something different yet again, if it was like an NLI pair where you might wanna be sensitive to position",
    "start": "1290115",
    "end": "1295730"
  },
  {
    "text": "within the example as well as position across the, uh, premise and hypothesis.",
    "start": "1295730",
    "end": "1301290"
  },
  {
    "text": "You'll see that they do that for BERT. Thank you. Yeah.",
    "start": "1301290",
    "end": "1306424"
  },
  {
    "text": "So to clarify, you take the sinusoid [NOISE] as weight and do you convolve that with the vector,",
    "start": "1306425",
    "end": "1312114"
  },
  {
    "text": "er, dimension wise or what exactly do you do with the weight? Yeah, they have a procedure for generating",
    "start": "1312114",
    "end": "1317490"
  },
  {
    "text": "these vectors that follow these sinusoidal curves that are sensitive. You can see it's like dim four, five, six, and seven,",
    "start": "1317490",
    "end": "1323120"
  },
  {
    "text": "and I think four is the one with the laziest pattern. And then what do you do with that sinusoidal vector?",
    "start": "1323120",
    "end": "1330620"
  },
  {
    "text": "You add it to your word vector. That's what's happening up here.",
    "start": "1330620",
    "end": "1335870"
  },
  {
    "text": "And that's a way of encoding in a high-dimensional vector, the position that you're in. Yeah.",
    "start": "1335870",
    "end": "1342679"
  },
  {
    "text": "Why does addition sort of work? It always feels to me weird that you add two vectors, because it seems like you just kinda gobble off both of them.",
    "start": "1342680",
    "end": "1349809"
  },
  {
    "text": "Like, why not concatenate or is it just purely empirical? I think it's a good question. Throughout all these papers it's some.",
    "start": "1349810",
    "end": "1356570"
  },
  {
    "text": "I think it keeps the dimensonalities low and allows some freedom in terms of how you're combining different pieces in these networks.",
    "start": "1356570",
    "end": "1363260"
  },
  {
    "text": "But yeah, I kind of. I would for- if I was trying this just as a, you know,",
    "start": "1363260",
    "end": "1368670"
  },
  {
    "text": "taking a simple model and adding positional encoding, I would try both sum and concatenation. Because it sure seems like concatenation would be just as good if not better,",
    "start": "1368670",
    "end": "1377210"
  },
  {
    "text": "assuming that I didn't have to worry about dimensionality stuff.",
    "start": "1377210",
    "end": "1380890"
  },
  {
    "text": "Those are the high level ideas. Let's dive into these models a little bit. [NOISE] And again, I think that this can't be a substitute for reading the papers.",
    "start": "1384460",
    "end": "1393304"
  },
  {
    "text": "I think you should think of this more as a kind of guided tour, um, that might help you as you work through them.",
    "start": "1393305",
    "end": "1399350"
  },
  {
    "text": "But there's no substitute for that, you know, hours spent banging your head against these papers and reflecting on them.",
    "start": "1399350",
    "end": "1405710"
  },
  {
    "start": "1405000",
    "end": "1580000"
  },
  {
    "text": "Let's start in with ELMo, cause I think this builds the most on stuff that we've already done. This is a pretty simple idea that I think they just executed on beautifully.",
    "start": "1405710",
    "end": "1415235"
  },
  {
    "text": "So for ELMo, the idea is, down here I have my- my standard example,",
    "start": "1415235",
    "end": "1420414"
  },
  {
    "text": "the Rock rules with a start symbol. And each one of these- them has a standard embedding.",
    "start": "1420415",
    "end": "1425885"
  },
  {
    "text": "But now, I'm gonna transfer that into two RNNs, one traveling forward and one traveling reverse.",
    "start": "1425885",
    "end": "1432530"
  },
  {
    "text": "That's kind of orange and green here. So standard RNNs of the sort you've seen before they use, I think LSTM cells.",
    "start": "1432530",
    "end": "1439325"
  },
  {
    "text": "Um, the paper I think it's deeper than this network. I forget the details. I've shown you just two layers.",
    "start": "1439325",
    "end": "1444919"
  },
  {
    "text": "But obviously, you can imagine that this could be as deep as you wanted. And then once I've got those two RNN- RNNs running",
    "start": "1444920",
    "end": "1452540"
  },
  {
    "text": "for the classification decisions that are gonna produce the, uh, next token, uh, I share parameters.",
    "start": "1452540",
    "end": "1458750"
  },
  {
    "text": "So up here shared parameters, down here shared parameters. And these two are separate RNNs with their own parameters to allow them",
    "start": "1458750",
    "end": "1465820"
  },
  {
    "text": "to run in both directions without any masking or anything like that. Does that makes sense? Yeah.",
    "start": "1465820",
    "end": "1472290"
  },
  {
    "text": "Okay. So the two RNNs are language models that are trying to predict the underlying sentence? Yeah. So if you follow just this left one here you do the start symbol.",
    "start": "1472290",
    "end": "1480440"
  },
  {
    "text": "You hope it produces The as its prediction. And then the next one, Th- The gets inserted here and you hope it produces Rock and so forth.",
    "start": "1480440",
    "end": "1489245"
  },
  {
    "text": "And then over here it's just gonna run in the reverse direction doing those same predictions.",
    "start": "1489245",
    "end": "1494995"
  },
  {
    "text": "And I think the other important thing that seems meaningful, they point this out in the paper is that the- there are shared parameters for",
    "start": "1494995",
    "end": "1500760"
  },
  {
    "text": "the m- map that takes you from the embedding into the RNN, and then shared parameters that take you into the softmax layer.",
    "start": "1500760",
    "end": "1509010"
  },
  {
    "text": "Then for actual word representations, conceptual- contextual word representations, like,",
    "start": "1510340",
    "end": "1515875"
  },
  {
    "text": "if I wanna represent rules here, that word, the final word, then I can do that, um,",
    "start": "1515875",
    "end": "1521210"
  },
  {
    "text": "by combining its embedding and canca- concatenating the two hidden states that go along with it for each of these RNNs.",
    "start": "1521210",
    "end": "1529070"
  },
  {
    "text": "So it's like these are the two states that I drew from here and from here, and I drew these two states from here and from here.",
    "start": "1529070",
    "end": "1535570"
  },
  {
    "text": "They get concatenated. And then part of ELMo learning, uh,",
    "start": "1535570",
    "end": "1540740"
  },
  {
    "text": "when you apply this to another task is learning a task-specific weighting on those vectors.",
    "start": "1540740",
    "end": "1545870"
  },
  {
    "text": "So these are softmax normalized scores that you can learn from data that tell you how to weight the various levels that you've learned from the core ELMo model.",
    "start": "1545870",
    "end": "1555090"
  },
  {
    "text": "Those all get summed together after the weighting. And then I have this single representation up here in yellow which represents rules,",
    "start": "1555490",
    "end": "1563135"
  },
  {
    "text": "but notice it represents rules not on its own, but rather from all the surrounding morphosyntactic context.",
    "start": "1563135",
    "end": "1570020"
  },
  {
    "text": "So, um, rules will be different if the preceding words are different fron- from another example.",
    "start": "1570020",
    "end": "1576110"
  },
  {
    "text": "[NOISE] I mentioned before that they do some other,",
    "start": "1576110",
    "end": "1583290"
  },
  {
    "start": "1580000",
    "end": "1722000"
  },
  {
    "text": "ah, tricky stuff in terms of representing words. And this is gonna draw directly on that convolutional stuff that I reviewed before.",
    "start": "1583290",
    "end": "1589429"
  },
  {
    "text": "So imagine I wanna represent just some anonymous word up here. The way they build that up is first at the character level that's down at the bottom.",
    "start": "1589430",
    "end": "1597965"
  },
  {
    "text": "And then they have a bunch of these convolutional filters, like up to seven of varying lengths.",
    "start": "1597965",
    "end": "1605000"
  },
  {
    "text": "Those get combined via maxpooling in the way that I showed you before, and all those representations from all the different filters get",
    "start": "1605000",
    "end": "1612380"
  },
  {
    "text": "concatenated into what's quite a wide representation at this point.",
    "start": "1612380",
    "end": "1617605"
  },
  {
    "text": "And then they have what they call highway layers. Which is a separately developed machine-learning intuition about how essentially you can",
    "start": "1617605",
    "end": "1624650"
  },
  {
    "text": "learn some gating information across different layers in a deep network. Um, so like in the extreme case you could actually learn parameters that would tell",
    "start": "1624650",
    "end": "1632990"
  },
  {
    "text": "you to skip a layer and just pass up the lower representation. But the actual highway layer idea is more flexible, it allows you to learn essentially",
    "start": "1632990",
    "end": "1642270"
  },
  {
    "text": "gating information that will tell you what percentage to allow to pass on and what percentage not.",
    "start": "1642270",
    "end": "1647840"
  },
  {
    "text": "That's what I've put in yellow here. They're just more dense representations that function as these gates. Yeah.",
    "start": "1647840",
    "end": "1654680"
  },
  {
    "text": "Gatings are different than the LSTM version of gating or is it just sort of a simplified?",
    "start": "1654680",
    "end": "1659750"
  },
  {
    "text": "It is a very similar idea. It's drawing to- it's, it's from, um, one of the same authors, the developers of LSTMs and it draws on a very similar intuition,",
    "start": "1659750",
    "end": "1667910"
  },
  {
    "text": "just now applied in potentially very deep layers. The ELMo layers are only- only two is their max.",
    "start": "1667910",
    "end": "1674645"
  },
  {
    "text": "But I think the idea is experimentally, you can have like 25 layers and you would learn how to skip around in that space.",
    "start": "1674645",
    "end": "1681400"
  },
  {
    "text": "And then finally they do a dimensionality reduction down to get the final word representation.",
    "start": "1682510",
    "end": "1688430"
  },
  {
    "text": "[NOISE] And you might be able to tell from back here that you're kind of constrained.",
    "start": "1688430",
    "end": "1694160"
  },
  {
    "text": "The word representation has to be double the length of the two hidden states so that I can do this summing.",
    "start": "1694160",
    "end": "1701150"
  },
  {
    "text": "And that kind of adds some constraints to this model. And that's why you have this final projection layer,",
    "start": "1701150",
    "end": "1707519"
  },
  {
    "text": "because these are of much higher dimensionality than you'd wanna accommodate for the rest of the ELMo model.",
    "start": "1707520",
    "end": "1712920"
  },
  {
    "text": "But finally, that gives you your word representation up here and that's what gets passed into the two RNNs.",
    "start": "1713140",
    "end": "1721049"
  },
  {
    "text": "And then here's some information about the models that they released. Small, medium, original, and original trained on",
    "start": "1722530",
    "end": "1729590"
  },
  {
    "text": "much more data than this simple original one here. Uh, shows you the parameter counts,",
    "start": "1729590",
    "end": "1735804"
  },
  {
    "text": "the hidden sizes for these networks. Some information. Thi- this is both LSTM information and",
    "start": "1735805",
    "end": "1741149"
  },
  {
    "text": "then the number of highway layers in the word representations. That's a kind of summary of how big these models get.",
    "start": "1741150",
    "end": "1746770"
  },
  {
    "text": "And one tip I have for you if you wanna know even more about how they're set up, the way AllenNLP works is you have these options files,",
    "start": "1746770",
    "end": "1755090"
  },
  {
    "text": "which more or less setup the hyper parameters to the model. And so you can just look at them at the website and see which activation functions were used,",
    "start": "1755090",
    "end": "1762590"
  },
  {
    "text": "what the dimensionalities of everything were, what the filters were like, and so forth. Which is an even deeper dive on the kind of",
    "start": "1762590",
    "end": "1768919"
  },
  {
    "text": "mechanics of the specific models that they released.",
    "start": "1768920",
    "end": "1772410"
  },
  {
    "text": "And that's ELMo. Any questions about ELMo before I move on? Yeah.",
    "start": "1774880",
    "end": "1781280"
  },
  {
    "text": "Are there forward RNN and reverse RNNs, are they trained, like, separately?",
    "start": "1781280",
    "end": "1787430"
  },
  {
    "text": "[NOISE] Or like, yeah. Because I definitely don't understand how if you're trying",
    "start": "1787430",
    "end": "1794980"
  },
  {
    "text": "to predict a word from one RNN and that you've seen from the other one. They are trained separately and that gets them around this problem,",
    "start": "1794980",
    "end": "1802845"
  },
  {
    "text": "which you'll see come up when we talk about the Transformers and BERT. That you can't have these things run in both directions without doing some masking,",
    "start": "1802845",
    "end": "1808700"
  },
  {
    "text": "otherwise words kind of see themselves. So I think, imagine that these RNNs are being trained separately",
    "start": "1808700",
    "end": "1814889"
  },
  {
    "text": "against what might as well be separate, uh, output states. I just drew them this way as a kind of reminder that the parameters are shared,",
    "start": "1814890",
    "end": "1822980"
  },
  {
    "text": "that it's kind of doing this multitask learning from the same data. Yeah.",
    "start": "1822980",
    "end": "1829005"
  },
  {
    "text": "Wouldn't it. [BACKGROUND] Just sort of trivially talking. Like you take the vector representation and instead of doing all this",
    "start": "1829005",
    "end": "1835740"
  },
  {
    "text": "[inaudible] you just kind of know that you can just get straight back out what you put back in. Like, do they block out words?",
    "start": "1835740",
    "end": "1841665"
  },
  {
    "text": "Wha- what is it that makes this task ultimately hard to the RNN to learn? Well, I think each of these RNNs is doing the hard task of language modeling.",
    "start": "1841665",
    "end": "1849840"
  },
  {
    "text": "So it doesn't have those kind of degenerate solutions. It's just as hard as that task is. Um, it's just that you have tied parameters up here. Yeah.",
    "start": "1849840",
    "end": "1865035"
  },
  {
    "text": "So the end goal then is to use these representations in another task, right?",
    "start": "1865035",
    "end": "1870645"
  },
  {
    "text": "Yeah. In the context of the paper, the idea that they're really pitching is that I can learn",
    "start": "1870645",
    "end": "1876345"
  },
  {
    "text": "task spe- task specific word representations that could be the input to another model. So in the simplest case,",
    "start": "1876345",
    "end": "1882510"
  },
  {
    "text": "and this is kind of what you see in my notebook, I have these contextual word representations. They could be input to a further RNN,",
    "start": "1882510",
    "end": "1889095"
  },
  {
    "text": "that's about your task trained against your labels, and then you're just benefiting from all this other stuff ultimately going",
    "start": "1889095",
    "end": "1895350"
  },
  {
    "text": "down to the web-scale data that these representations were trained on. So is there any taking",
    "start": "1895350",
    "end": "1902130"
  },
  {
    "text": "your actual end task performance and propagating that back onto your RNNs,",
    "start": "1902130",
    "end": "1907530"
  },
  {
    "text": "or we train the RNNs and we freeze them, uh, and then we use them on our actual task?",
    "start": "1907530",
    "end": "1913215"
  },
  {
    "text": "Oh, this is a good question. I, I guess I assume that the task-specific learning,",
    "start": "1913215",
    "end": "1918270"
  },
  {
    "text": "the fine tuning that you did, was just on this Softmax normalize, uh, weights here. But there's no reason in principle why you couldn't have that propagate",
    "start": "1918270",
    "end": "1926340"
  },
  {
    "text": "down further to the ELMo parameters themselves. Now, I'll have to check to see what's normally done.",
    "start": "1926340",
    "end": "1931485"
  },
  {
    "text": "Do you propagate all the way down? Anyone know? Or just these weights here? [NOISE] Both of them make sense as far as I can tell. It's a good question.",
    "start": "1931485",
    "end": "1941290"
  },
  {
    "text": "I think what you might have in mind though is that for transformers and BERT, in principle, we can propagate down to all the weights as part of- part of fine tuning.",
    "start": "1942950",
    "end": "1952679"
  },
  {
    "text": "Let's skip to that now. So the transformer. Again, okay so let's build up this core model structure.",
    "start": "1952680",
    "end": "1960630"
  },
  {
    "start": "1955000",
    "end": "2095000"
  },
  {
    "text": "There's a lot here, and I just want to kind of make sure I highlight what I think are the really innovative ideas and how they come together.",
    "start": "1960630",
    "end": "1968445"
  },
  {
    "text": "So first down at the bottom, this will be familiar. Because what I've done is for our example the rock rules,",
    "start": "1968445",
    "end": "1974445"
  },
  {
    "text": "positional encoding exactly the way I showed you before, and they don't learn these embeddings in the paper,",
    "start": "1974445",
    "end": "1980700"
  },
  {
    "text": "they rather do them in that kind of s- um, sinewave based way. So that they can generalize to example lengths that they didn't see in training,",
    "start": "1980700",
    "end": "1988710"
  },
  {
    "text": "which I think is a really nice idea. But they point out that you get very similar results if you learn the embedding for this dark gray.",
    "start": "1988710",
    "end": "1996179"
  },
  {
    "text": "So those get summed, and that gives you the word representations for the rock and rules.",
    "start": "1996180",
    "end": "2003270"
  },
  {
    "text": "And I just wanted- I'm gonna call this out with these equations here because they're presented in a kind of non-linear way in the paper,",
    "start": "2003490",
    "end": "2009799"
  },
  {
    "text": "and I wanted to organize them. So c-input is the sum of these two vectors. I'll concentrate on the c,",
    "start": "2009800",
    "end": "2016205"
  },
  {
    "text": "but all of this is true for a and b as well. Then the next layer is the one where the real action lies.",
    "start": "2016205",
    "end": "2023419"
  },
  {
    "text": "This is where the pap- the title attention is all you need comes from. So to form the next representation,",
    "start": "2023420",
    "end": "2030440"
  },
  {
    "text": "the ones I've got in orange here, you do that dot-product attention that I showed you before.",
    "start": "2030440",
    "end": "2035750"
  },
  {
    "text": "I want to call out that that is exactly what I showed you for the NLI case with",
    "start": "2035750",
    "end": "2040790"
  },
  {
    "text": "the tweak that they do this normalization by the length of the representations,",
    "start": "2040790",
    "end": "2045920"
  },
  {
    "text": "which they find just helps with scaling as learning proceeds. But other than that, it's like the scores Softmax normalized on they",
    "start": "2045920",
    "end": "2054349"
  },
  {
    "text": "happen to do the sum in the paper of the weighted other vectors. It's called self-attention because instead of looking",
    "start": "2054350",
    "end": "2060889"
  },
  {
    "text": "back from the hypothesis into the premise, now we're just looking around. When we're at c, we looked at b and a,",
    "start": "2060890",
    "end": "2067204"
  },
  {
    "text": "and formed attention weights based on them, and that's what gives us this weighted view,",
    "start": "2067205",
    "end": "2073280"
  },
  {
    "text": "uh, attention weighted view of the input vector. But the mechanics are ones that are entire- entirely familiar as you can see here.",
    "start": "2073280",
    "end": "2080990"
  },
  {
    "text": "[inaudible] the length of the string? No, by the length of the representations.",
    "start": "2080990",
    "end": "2086599"
  },
  {
    "text": "K here is the dimensionality of everything that we're gonna see as we build up.",
    "start": "2086600",
    "end": "2091620"
  },
  {
    "start": "2095000",
    "end": "2545000"
  },
  {
    "text": "Make sense? So good so far. Self-attention is just attention all around me.",
    "start": "2095460",
    "end": "2101049"
  },
  {
    "text": "[LAUGHTER] Okay, then we keep building. So then they have these two kind of sublayer structure pieces,",
    "start": "2101050",
    "end": "2109075"
  },
  {
    "text": "where to form what I've called a layer here, I sum the attention weighted representation",
    "start": "2109075",
    "end": "2116020"
  },
  {
    "text": "that I got here with the dropout on the input, and this is like- this is familiar too.",
    "start": "2116020",
    "end": "2123079"
  },
  {
    "text": "If you think back on the attention lecture, a lot of what we do in attention is kind of take the representation we care about,",
    "start": "2123080",
    "end": "2129380"
  },
  {
    "text": "and mix in the attentional values, and I take it that that's what's happening here.",
    "start": "2129380",
    "end": "2134390"
  },
  {
    "text": "So the orange representation is actually just this kind of attentional thing on what's around me. It's not really a proper representation of c. So to mix them together,",
    "start": "2134390",
    "end": "2143330"
  },
  {
    "text": "they do sum and then the dropout thing is just a bit of regularization, and that gives you c_alayer here in this, um, yellow.",
    "start": "2143330",
    "end": "2151805"
  },
  {
    "text": "Uh, and I've tried to draw this here, this kind of sublayer structure where it's kind of like you elevate",
    "start": "2151805",
    "end": "2157910"
  },
  {
    "text": "something below with something that was an intermediate representation, and again that's done by sum,",
    "start": "2157910",
    "end": "2163790"
  },
  {
    "text": "and that's constraining the fact that all these dimension, uh, these, uh, representations have to have the same dimensionality.",
    "start": "2163790",
    "end": "2169010"
  },
  {
    "text": "[NOISE] Then they do layer normalization.",
    "start": "2169010",
    "end": "2174785"
  },
  {
    "text": "I think, again, this is a kind of optimization trick that helps the network to learn. I've given the calculations here.",
    "start": "2174785",
    "end": "2181130"
  },
  {
    "text": "This is a kind of version of z scoring, where I just kind of make sure that the vectors for each of one of these layers are scaled in a sensible way.",
    "start": "2181130",
    "end": "2190799"
  },
  {
    "text": "Then you get two layers of a feed-forward network.",
    "start": "2191980",
    "end": "2197165"
  },
  {
    "text": "The first with a ReLU activation function in the paper. But this is the- a kind of similar thing where s- this, um,",
    "start": "2197165",
    "end": "2205460"
  },
  {
    "text": "a- this nor- this value here is here multiplied by some weights and a bias,",
    "start": "2205460",
    "end": "2210815"
  },
  {
    "text": "that gets a non-linear activation, and then another dense layer over here; and that gives you what I've called c_ff in blue.",
    "start": "2210815",
    "end": "2218700"
  },
  {
    "text": "Then one- again, one of these sub layer tricks. So take c_anorm, and kind of elevate it and combine it with c_ff,",
    "start": "2219310",
    "end": "2228830"
  },
  {
    "text": "having done some dropout on c_ff, and that gives you c_fflayer. This is the kind of, uh,",
    "start": "2228830",
    "end": "2235160"
  },
  {
    "text": "darker or lighter yellow up here. It's kind of similar to this yellow down at this point. That's why I gave them the same color. Same trick.",
    "start": "2235160",
    "end": "2243260"
  },
  {
    "text": "It's just that here where we steered around attention, now we steer around the dense representation.",
    "start": "2243260",
    "end": "2249020"
  },
  {
    "text": "[NOISE] Then finally, a normalization step; same one as the one we showed you before,",
    "start": "2249020",
    "end": "2254060"
  },
  {
    "text": "and that's the output of this block of the Transformer. [NOISE] That was a mouthful.",
    "start": "2254060",
    "end": "2263329"
  },
  {
    "text": "Any questions about it? [inaudible] earlier [inaudible] decay [inaudible] layers?",
    "start": "2263330",
    "end": "2272390"
  },
  {
    "text": "No, no. So decay would be like whatever we picked here, like 50 for the green. Isn't that right? Yeah.",
    "start": "2272390",
    "end": "2279020"
  },
  {
    "text": "Is the dimensions of c input like a concatenation of the x,",
    "start": "2279020",
    "end": "2284885"
  },
  {
    "text": "word embedding, and positional embedding. I think it follows from this network structure that every single one of",
    "start": "2284885",
    "end": "2291529"
  },
  {
    "text": "these representations from the gray on up has to have the same dimensionality. Because here I sum, so these have to be the same here. Here I sum.",
    "start": "2291530",
    "end": "2298220"
  },
  {
    "text": "Here I sum. I don't see any space for kind of growth and shrinkage. So if we pick 50 down here.",
    "start": "2298220",
    "end": "2304100"
  },
  {
    "text": "It's 50 all the way up. And then this normalization is the square root of 50. Hope I have that right. [NOISE] Yeah, all these sums,",
    "start": "2304100",
    "end": "2313325"
  },
  {
    "text": "at least simplify some of these design choices because it's- at least you don't have to do tuning over the dimensionality of each one of these things.",
    "start": "2313325",
    "end": "2320060"
  },
  {
    "text": "And it's only gonna get worse. If you've read the transformer paper, you know that I've just started here.",
    "start": "2320060",
    "end": "2325410"
  },
  {
    "text": "There's two more ways in this net- in which this network is gonna get [NOISE] bigger. And the first, is what's called multi-headed attention.",
    "start": "2325410",
    "end": "2332815"
  },
  {
    "text": "This isn't gonna be anything new. It's just we're gonna do what we did before a lot more. Okay, so start with our example and lo- follow the green here.",
    "start": "2332815",
    "end": "2342575"
  },
  {
    "text": "So what we'll do is map this into its own attentional space.",
    "start": "2342575",
    "end": "2347615"
  },
  {
    "text": "And the [NOISE] reason that's meaningful is that, this is that attention calculation before but now we have parameters W1Q,",
    "start": "2347615",
    "end": "2355175"
  },
  {
    "text": "W1K, and W1V, that are particular to this head of the multi-headed attention that we're building.",
    "start": "2355175",
    "end": "2364700"
  },
  {
    "text": "But it's the same calculation. It's just that each one of the representations is, is fed through these dense layers.",
    "start": "2364700",
    "end": "2372230"
  },
  {
    "text": "So we do that, call that H1. That gives us three vectors over here. And then you do it, you start to build that up.",
    "start": "2372230",
    "end": "2379505"
  },
  {
    "text": "Do it for H2 as well. Same calculation except now it's W2 for all three of those Ws,",
    "start": "2379505",
    "end": "2386060"
  },
  {
    "text": "and that gives us these representations. Do it for W3,",
    "start": "2386060",
    "end": "2391339"
  },
  {
    "text": "same calculation but new parameters and so forth, all the way up to what they do in the paper is eight, I think.",
    "start": "2391340",
    "end": "2398180"
  },
  {
    "text": "So they have eight heads for these attention layers. What they do is like, if I picked 800 as my dimensionality,",
    "start": "2398180",
    "end": "2405815"
  },
  {
    "text": "the kind of conceptual dimensionality for my transformer network. Then each one of these would have dimension 100.",
    "start": "2405815",
    "end": "2411950"
  },
  {
    "text": "So that the overall network didn't grow. What they're trying to do is kind of provide more ways of",
    "start": "2411950",
    "end": "2418039"
  },
  {
    "text": "ensuring that you have lots of parameters to lots of interactions, and avoid kind of I think the network ossifying in certain ways, right?",
    "start": "2418040",
    "end": "2426619"
  },
  {
    "text": "Really flexibly learning how to map into this attentional representation. Makes sense?",
    "start": "2426620",
    "end": "2435240"
  },
  {
    "text": "And then the final thing here, is that there's not just one transformer block,",
    "start": "2436150",
    "end": "2441815"
  },
  {
    "text": "in the paper there are six. So six repetitions of everything that I have in gray.",
    "start": "2441815",
    "end": "2447305"
  },
  {
    "text": "And I've just reminded you here that we do multi-headed attention at the bottom of each one [NOISE] of these layers and then repeat the whole calculation.",
    "start": "2447305",
    "end": "2454460"
  },
  {
    "text": "[inaudible]",
    "start": "2454460",
    "end": "2460250"
  },
  {
    "text": "The eight can be done in parallel. A huge advantage of the transformer over like RNNs, is that a lot of this stuff can be parallelized,",
    "start": "2460250",
    "end": "2466970"
  },
  {
    "text": "which leads to real speed gains. And that's one of the advertising points for the paper,",
    "start": "2466970",
    "end": "2472039"
  },
  {
    "text": "in addition to kind of being very free-form and unbiased about how you learn how words interact with each other.",
    "start": "2472040",
    "end": "2478115"
  },
  {
    "text": "You get a lot of optimization gains with this. And I think that's why this network can be so massive and still train in reasonable times.",
    "start": "2478115",
    "end": "2485910"
  },
  {
    "text": "Yeah.",
    "start": "2486820",
    "end": "2492110"
  },
  {
    "text": "[inaudible] um, because there's this kinda like each word to",
    "start": "2492110",
    "end": "2497240"
  },
  {
    "text": "word has its kind of [inaudible].",
    "start": "2497240",
    "end": "2503030"
  },
  {
    "text": "Well, I drew it in a way that was trying to reveal which parameters were shared versus which, which were split up.",
    "start": "2503030",
    "end": "2508775"
  },
  {
    "text": "But I might as well have copied all these inputs and just had a bunch of different attention things.",
    "start": "2508775",
    "end": "2515059"
  },
  {
    "text": "And that kind of reveals to you the independence of these calculations. And then if you think about gradient information flowing back, you know,",
    "start": "2515060",
    "end": "2521930"
  },
  {
    "text": "it's all gonna be kind of independent and just brought together in the end. [NOISE] I think that's why you can think of these as separate calculations.",
    "start": "2521930",
    "end": "2529040"
  },
  {
    "text": "[NOISE] So now I feel like having done that,",
    "start": "2529040",
    "end": "2537710"
  },
  {
    "text": "I at least I'm [NOISE] in a position to understand the diagram in the paper, which I absolutely did not understand the",
    "start": "2537710",
    "end": "2543440"
  },
  {
    "text": "first time I read the paper but now I kinda get it. So the left side is,",
    "start": "2543440",
    "end": "2548900"
  },
  {
    "text": "of course repeated for every state in the encoder. They've only shown one state for compactness",
    "start": "2548900",
    "end": "2554390"
  },
  {
    "text": "but like the rock rules would be three repetitions of this. Each decoder state self attends,",
    "start": "2554390",
    "end": "2562400"
  },
  {
    "text": "with all the decoder states and with all the encoder states. That's something that I didn't quite confront in my core discussion.",
    "start": "2562400",
    "end": "2568880"
  },
  {
    "text": "But just imagine everything that I was doing on our single example, if we were doing like a dialogue agent or summarization or translation.",
    "start": "2568880",
    "end": "2577910"
  },
  {
    "text": "All the decoder states, like in NLI, the hypothesis could attend back to everything",
    "start": "2577910",
    "end": "2583940"
  },
  {
    "text": "in the premise as well as attending to all things that were, that were in their own space. So that's what this line is meant to signal here.",
    "start": "2583940",
    "end": "2591965"
  },
  {
    "text": "Self-attention in both sides and also attention backwards. The right side of this, of course,",
    "start": "2591965",
    "end": "2598910"
  },
  {
    "text": "repeated for every decoder state. Um, yeah, I think that's all I have to say there.",
    "start": "2598910",
    "end": "2606890"
  },
  {
    "text": "And then finally, I just wanted to point out. This is a complexity that I'm not going to talk about. But in the decoder self-attention is limited to preceding words,",
    "start": "2606890",
    "end": "2613550"
  },
  {
    "text": "so that words can kind of see themselves. So there's a bunch of masking that has to happen in order for us to do this on the decoder side.",
    "start": "2613550",
    "end": "2620120"
  },
  {
    "text": "But for things like NLI, you don't have to confront this. Because you're just trying to predict a label and you can attend everywhere if you want.",
    "start": "2620120",
    "end": "2627240"
  },
  {
    "text": "Yeah. That's it for the transformer. Questions about the transformer?",
    "start": "2629620",
    "end": "2637680"
  },
  {
    "text": "Great. So now BERT. This will be comparatively easy, if you have the transformer in mind.",
    "start": "2639070",
    "end": "2645305"
  },
  {
    "start": "2645000",
    "end": "2950000"
  },
  {
    "text": "Because it's obvious- it's called Bidirectional Encoder Representations from Transformers.",
    "start": "2645305",
    "end": "2651125"
  },
  {
    "text": "Okay. Once more we build up a model. Down here I have the rock rules.",
    "start": "2651125",
    "end": "2660380"
  },
  {
    "text": "You have a special class token that BERT has. And it's got its own embedding.",
    "start": "2660380",
    "end": "2665944"
  },
  {
    "text": "And then the three familiar tokens. They all have positional embeddings just as before.",
    "start": "2665945",
    "end": "2672095"
  },
  {
    "text": "And now they also have a sentence-level embedding. Which is just repeated in this example in dark red.",
    "start": "2672095",
    "end": "2679715"
  },
  {
    "text": "And the idea there is that if I've got a two example thing like premise and hypothesis. Then I have two embeddings;",
    "start": "2679715",
    "end": "2685970"
  },
  {
    "text": "one for the first. One for the second. So each word is infused with that high level positional information.",
    "start": "2685970",
    "end": "2692089"
  },
  {
    "text": "But again, it's just summing those representations to get the green embedding here.",
    "start": "2692090",
    "end": "2697895"
  },
  {
    "text": "Which you might think of as the token representation. And then, you just do a bunch of transformer stuff, right?",
    "start": "2697895",
    "end": "2706580"
  },
  {
    "text": "And the important thing I think to signal there, is that you do a bunch of self-attention on these representations and that get- that flows into the transformer.",
    "start": "2706580",
    "end": "2715234"
  },
  {
    "text": "And then once you've done all the self-attention stuff, it's that kind of norming and dropout and sublayer structure with a feed-forward network at a certain point,",
    "start": "2715235",
    "end": "2723470"
  },
  {
    "text": "you know, that mixture of ideas. But surely the important thing here is that, at the start of each one of these transformer blocks,",
    "start": "2723470",
    "end": "2730444"
  },
  {
    "text": "we do all of this attention all around us, right? So it's not an RNN and it's not a back and forth RNN.",
    "start": "2730445",
    "end": "2737270"
  },
  {
    "text": "It's more like this dense thicket of connections that we can make to form those initial representations.",
    "start": "2737270",
    "end": "2743645"
  },
  {
    "text": "And you can repeat the transformer blocks. I think they do two in the paper?",
    "start": "2743645",
    "end": "2748744"
  },
  {
    "text": "So it's not as deep as the transformer but some number of them get repeated.",
    "start": "2748745",
    "end": "2754950"
  },
  {
    "text": "And then to use this model to make predictions. Since it is flowing in both directions,",
    "start": "2755620",
    "end": "2762529"
  },
  {
    "text": "you can't just do a standard language modeling task. So what they do instead is a test they call masked language modeling.",
    "start": "2762530",
    "end": "2768935"
  },
  {
    "text": "So in this case, uh, you just make predictions about certain words that have the mask value.",
    "start": "2768935",
    "end": "2774770"
  },
  {
    "text": "So just block out an actual word and then learn to predict the word that was there. And they do that on about 15% of the tokens,",
    "start": "2774770",
    "end": "2782614"
  },
  {
    "text": "the rest are shown to the network. And that's the sense in which it can train against these missing points and",
    "start": "2782614",
    "end": "2789290"
  },
  {
    "text": "learn a bidirectional model without any of those problems or words seeing their own future and past.",
    "start": "2789290",
    "end": "2796085"
  },
  {
    "text": "So it's a kind of standard language modeling thing just trained in a really innovative way. That makes sense?",
    "start": "2796085",
    "end": "2804900"
  },
  {
    "text": "And then the other piece is for doing transfer learning and fine tuning, the convention that they adopt in the paper is,",
    "start": "2805720",
    "end": "2813320"
  },
  {
    "text": "that this class token here, is used as the summary representation of the whole example.",
    "start": "2813320",
    "end": "2818765"
  },
  {
    "text": "And what that means is that, in the simplest case you could just add a classifier on top of it.",
    "start": "2818765",
    "end": "2823880"
  },
  {
    "text": "The way we did for NLI for example, where we took the last state or the first and the last states and use them for a classifier.",
    "start": "2823880",
    "end": "2830194"
  },
  {
    "text": "Here, you just use this final transformer representation [NOISE] associated with the class label.",
    "start": "2830195",
    "end": "2835760"
  },
  {
    "text": "So that's the simple case of just doing like transfer learning. And then they advocate doing a lot of fine tuning of the parameters.",
    "start": "2835760",
    "end": "2843940"
  },
  {
    "text": "So you put this classifier on top, and then not only do you update the parameters for that classifier",
    "start": "2843940",
    "end": "2849440"
  },
  {
    "text": "but allow that gradient information to flow throughout the network. And in that way, all the BERT parameters potentially get updated for your task.",
    "start": "2849440",
    "end": "2858260"
  },
  {
    "text": "And if you've done BERT fine tuning, I think you did that.",
    "start": "2858260",
    "end": "2863645"
  },
  {
    "text": "And that's what the, um, code that they released makes very easy to do.",
    "start": "2863645",
    "end": "2867900"
  },
  {
    "text": "In addition to that masked langua- uh, language modeling task they train the network, the one that parameters that they",
    "start": "2870280",
    "end": "2877580"
  },
  {
    "text": "released against a second prediction task which is binary sentence prediction.",
    "start": "2877580",
    "end": "2882800"
  },
  {
    "text": "So, for this they can generate their own data again just like they could by masking out certain tokens.",
    "start": "2882800",
    "end": "2888770"
  },
  {
    "text": "Here the positive instances are actual sequences of examples from their corpus like,",
    "start": "2888770",
    "end": "2894455"
  },
  {
    "text": "CLS, the man went to MASK store SEP. He bought a gallon MASK milk SEP and that's labeled IsNext",
    "start": "2894455",
    "end": "2901790"
  },
  {
    "text": "because those two sentences did actually occur in a corpus that they're learning from. And then for the negative examples,",
    "start": "2901790",
    "end": "2908705"
  },
  {
    "text": "they choose some random second sentence and label that NotNext. And so then training proceeds in",
    "start": "2908705",
    "end": "2916000"
  },
  {
    "text": "the way that you would expect using this class label here and you're fine tuning the whole network against",
    "start": "2916000",
    "end": "2921470"
  },
  {
    "text": "this high level next sentence prediction task. And so that's the sense in which it's,",
    "start": "2921470",
    "end": "2926750"
  },
  {
    "text": "it's been jointly trained against the language modeling task and the sentence prediction task.",
    "start": "2926750",
    "end": "2932690"
  },
  {
    "text": "And I guess that's like local coherence and more global or discourse coherence for the model.",
    "start": "2932690",
    "end": "2939710"
  },
  {
    "text": "So two notions of context dependence which seems again really right to me in terms of the way you'd want for learning representations.",
    "start": "2939710",
    "end": "2949730"
  },
  {
    "text": "A funny and delightful twist",
    "start": "2949730",
    "end": "2954799"
  },
  {
    "text": "about the BERT code release and the BERT data assets,",
    "start": "2954799",
    "end": "2960530"
  },
  {
    "text": "uh, is that the vocabulary is tiny. If you download a, a BERT model here I did the uncased one from",
    "start": "2960530",
    "end": "2967760"
  },
  {
    "text": "the small model and just read in the vocabulary it's only got 30,000 tokens in it,",
    "start": "2967760",
    "end": "2973340"
  },
  {
    "text": "30,000 word things in it. And you might think, how on Earth is BERT gonna be",
    "start": "2973340",
    "end": "2979490"
  },
  {
    "text": "so good at new text if it's got this tiny little vocabulary? And by contrast I think the ELMo vocabulary is like huge.",
    "start": "2979490",
    "end": "2988490"
  },
  {
    "text": "So this is a real puzzler, \"How is it gonna do well with, well with only 30,000 tokens?\" Well, if you look at those tokens as I've done here you've got like folder, that's a word.",
    "start": "2988490",
    "end": "2997144"
  },
  {
    "text": "But then, uh, you know, ##gged, and principles, then moving.",
    "start": "2997145",
    "end": "3004080"
  },
  {
    "text": "And then this other funny word fragment. And then if you use their tokenizer which you know,",
    "start": "3004080",
    "end": "3009960"
  },
  {
    "text": "if you just download their code and load in the tokenizer here, then this isn't too surprising,",
    "start": "3009960",
    "end": "3015640"
  },
  {
    "text": "that does a normal thing, basically, except maybe it's unusual about how it handles this contraction but more or less this looks like words but then if you",
    "start": "3015640",
    "end": "3022510"
  },
  {
    "text": "say does BERT knows Snuffleupagus which is another Sesame Street character.",
    "start": "3022510",
    "end": "3027655"
  },
  {
    "text": "It break- it does not know that word, that is not in the vocabulary and so it breaks it up into a huge number of",
    "start": "3027655",
    "end": "3032980"
  },
  {
    "text": "smaller tokens with those boundary symbols and those boundary symbols tell the model that it's word internal.",
    "start": "3032980",
    "end": "3039280"
  },
  {
    "text": "And the actual tokenizer uses an underscore for whitespace. So no information is lost as you do tokenization with",
    "start": "3039280",
    "end": "3045849"
  },
  {
    "text": "this tool and you get a whole lot of strength from sub words. So this is great and this also shows",
    "start": "3045850",
    "end": "3052270"
  },
  {
    "text": "you why you shouldn't do your own tokenization with BERT, because you wanna let it kind of do its own divisions and use its own proper embedding space.",
    "start": "3052270",
    "end": "3060625"
  },
  {
    "text": "But then wow, talk about an efficient representation of the vocabulary. I would not have guessed if you just asked me two years ago",
    "start": "3060625",
    "end": "3068515"
  },
  {
    "text": "that this would be an effective thing to do but wildly successful.",
    "start": "3068515",
    "end": "3072890"
  },
  {
    "text": "And then here's some basic information about the models they released. So BERT base.",
    "start": "3074130",
    "end": "3079180"
  },
  {
    "text": "This is manageable for your computer, I mean it's large but you can work with it.",
    "start": "3079180",
    "end": "3084519"
  },
  {
    "text": "Oops, this should say large here. This is the large one. This is truly enormous. And if you try to work with this on your laptop everything might fall apart,",
    "start": "3084520",
    "end": "3091900"
  },
  {
    "text": "I guess that's the lesson here. And they are limited to sequences of 512 tokens that's",
    "start": "3091900",
    "end": "3098079"
  },
  {
    "text": "longest you can get because that's how many positional embeddings they learned, um, which reminds me of the, um,",
    "start": "3098080",
    "end": "3105220"
  },
  {
    "text": "transformer paper where they gave themselves the flexibility to learn for longer sequences.",
    "start": "3105220",
    "end": "3111110"
  },
  {
    "text": "That's BERT. Any questions about BERT? Lots of transformers.",
    "start": "3111960",
    "end": "3118960"
  },
  {
    "text": "BERT is just mostly the encoder part of the transformer right? Yes. It doesn't have the decoding part that's kinda handled implicitly by",
    "start": "3118960",
    "end": "3125920"
  },
  {
    "text": "the way you're doing this multi like back and forth approach? I think that's exactly right.",
    "start": "3125920",
    "end": "3131050"
  },
  {
    "text": "Yeah, dropped the decoder it was a hassle, it was getting in the way of being truly bidirectional.",
    "start": "3131050",
    "end": "3136255"
  },
  {
    "text": "And instead changed to this masked language modeling task. Yeah. Surely that's one of the shining insights of the paper.",
    "start": "3136255",
    "end": "3144800"
  },
  {
    "text": "Next generation using BERT if you were to give it like a second sentence that was a pure mask.",
    "start": "3147870",
    "end": "3153430"
  },
  {
    "text": "Yeah, sure. Yeah, no. It will learn to- yeah if you just mask out it will learn to make predictions about those tokens.",
    "start": "3153430",
    "end": "3159910"
  },
  {
    "text": "Yeah. Okay. I have a few minutes before I wanna,",
    "start": "3159910",
    "end": "3166934"
  },
  {
    "text": "wanna turn the stage over to Cindy. So I, I thought I would just point out the guiding idea for",
    "start": "3166935",
    "end": "3172170"
  },
  {
    "text": "that notebook is whatever architecture you're developing for your project,",
    "start": "3172170",
    "end": "3177549"
  },
  {
    "text": "[NOISE] it might benefit from contextual word representations. Speaking as a linguist I would say surely it's going to because",
    "start": "3177550",
    "end": "3184060"
  },
  {
    "text": "these representations were trained on a lot of data and they're very good. And whatever your problem is word order and contexts matter.",
    "start": "3184060",
    "end": "3191890"
  },
  {
    "text": "So, might as well try to bring them in. Now it might be hard for you to bring them in to whatever codebase you're working with,",
    "start": "3191890",
    "end": "3199000"
  },
  {
    "text": "because maybe you're using scikit or you're using PyTorch in your own way that would get in the way of doing the fine tuning that they advocate.",
    "start": "3199000",
    "end": "3206109"
  },
  {
    "text": "But don't let that stop you because you could just use the notebook tools to initialize your network using these representations.",
    "start": "3206110",
    "end": "3213235"
  },
  {
    "text": "That's a kind of twist on the standard thing for RNNs right. So for RNNs usually we have this fixed embedding space that",
    "start": "3213235",
    "end": "3220270"
  },
  {
    "text": "makes that mistake of not having context for its representations. Uh, and you just look up your tokens, get their indices,",
    "start": "3220270",
    "end": "3227920"
  },
  {
    "text": "look them up in the embedding and that finally gives you the vector sequence that the RNN is actually processing as its inputs.",
    "start": "3227920",
    "end": "3235405"
  },
  {
    "text": "With these contextual representations you just skip all those intermediate steps and go",
    "start": "3235405",
    "end": "3240790"
  },
  {
    "text": "directly from your vocabulary to sequences of vectors.",
    "start": "3240790",
    "end": "3245845"
  },
  {
    "text": "And now whereas a and a here had to be identical and b and b had to be identical.",
    "start": "3245845",
    "end": "3252280"
  },
  {
    "text": "Now they can be changed depending on their context. And that might give your network a real head-start.",
    "start": "3252280",
    "end": "3259060"
  },
  {
    "text": "It's really easy to do this. This is a complete use of the SST sentiment framework.",
    "start": "3259060",
    "end": "3264984"
  },
  {
    "text": "It's a whole experimental framework with ELMo. And you'd turn, you'd get impressive results with ELMo if",
    "start": "3264985",
    "end": "3270970"
  },
  {
    "text": "you compare back to the baselines for this ternary task for this, um, Torch RNN big boost.",
    "start": "3270970",
    "end": "3277420"
  },
  {
    "text": "And then for BERT similarly very easy to bring these things and you've gotta do a little bit more careful preprocessing of BERT vectors because they are very large,",
    "start": "3277420",
    "end": "3286450"
  },
  {
    "text": "uh, but still straight forward to bring it into the paradigm. And then you're off and running, BERT is even better",
    "start": "3286450",
    "end": "3292270"
  },
  {
    "text": "according to this experimental run for these parameters. So I hope that's empowering.",
    "start": "3292270",
    "end": "3298600"
  },
  {
    "text": "Um, any questions about any of that? Yeah. If you look at the task of producing, uh,",
    "start": "3298600",
    "end": "3306820"
  },
  {
    "text": "like let's say the vector representation, uh, the single word in context, uh, like I guess I'm wondering if there's anything else like a next step or what would be",
    "start": "3306820",
    "end": "3316599"
  },
  {
    "text": "the next challenge to overcome as far as making these representations really, uh, predictive?",
    "start": "3316600",
    "end": "3323060"
  },
  {
    "text": "Oh I don't know if you have ideas that can lead to a major contribution. Uh, I mean surely more discourse sensitivity like",
    "start": "3323370",
    "end": "3331795"
  },
  {
    "text": "going beyond maybe the binary sentence task that's something even higher level. Because I think that all my work meetings are",
    "start": "3331795",
    "end": "3338230"
  },
  {
    "text": "being influenced by everything that's happening in the discourse around us. So the more of that we can bring in the better will be.",
    "start": "3338230",
    "end": "3344799"
  },
  {
    "text": "And if you could bring in stuff like tasks, sensitivity that would be even better and on and on. Yeah. You know, why,",
    "start": "3344800",
    "end": "3352675"
  },
  {
    "text": "why stop at linguistic context, the physical context matters, gesture matters, visual salience matters, bring those things in.",
    "start": "3352675",
    "end": "3362125"
  },
  {
    "text": "Uh, maybe there's a Sesame Street character out there waiting for you. [LAUGHTER]",
    "start": "3362125",
    "end": "3372025"
  },
  {
    "text": "All right. Good. That was a giant gulp from this very rich fountain of ideas,",
    "start": "3372025",
    "end": "3379404"
  },
  {
    "text": "um, go forth and read the papers and let me know if you have any questions. I think what I'll do now is turn it over to Cindy for a related topic.",
    "start": "3379405",
    "end": "3389420"
  },
  {
    "text": "And be sure to wear this. Okay. Um, so this short talk is just going to be about um,",
    "start": "3390660",
    "end": "3399160"
  },
  {
    "start": "3395000",
    "end": "3600000"
  },
  {
    "text": "representation learning for long texts. So so far in the course we've mostly talked",
    "start": "3399160",
    "end": "3404620"
  },
  {
    "text": "about kind of learning distributed representations um, for words or for sentences but like mostly limited to sequences of a few words, right.",
    "start": "3404620",
    "end": "3414670"
  },
  {
    "text": "If you think about the assignments that we've done um, for like word similarity or like NLI um, or sentiment classification,",
    "start": "3414670",
    "end": "3422140"
  },
  {
    "text": "that's kind of been capped out at sentences that are like just a single sentence, right,",
    "start": "3422140",
    "end": "3427974"
  },
  {
    "text": "we haven't really thought past that to a paragraph or even like a long document. But for some of your projects you might be interested in doing um,",
    "start": "3427975",
    "end": "3435430"
  },
  {
    "text": "these sorts of NLU tasks for longer texts. Um, so we're going to talk about a couple of ways, um,",
    "start": "3435430",
    "end": "3441135"
  },
  {
    "text": "you can kind of get the same distributed, ah, representation um, but for um, something longer.",
    "start": "3441135",
    "end": "3448835"
  },
  {
    "text": "Um, so yeah. So so far, you know like we've been doing representation learning at",
    "start": "3448835",
    "end": "3453880"
  },
  {
    "text": "the word or sentence level and this is good because we can kind of get like a vector that represents our input that we can pass",
    "start": "3453880",
    "end": "3460570"
  },
  {
    "text": "into a classifier or we can compute distance, um, using some distance metric,",
    "start": "3460570",
    "end": "3466165"
  },
  {
    "text": "um and those things are like very useful for a wide variety of NLU tasks.",
    "start": "3466165",
    "end": "3471280"
  },
  {
    "text": "Um, yeah. But our goal for today is how can we apply NLU methods like the ones that we've been using to longer texts.",
    "start": "3471280",
    "end": "3479109"
  },
  {
    "text": "So for example, news articles, scientific papers or even books um, or like transcripts of conversations um, things like that.",
    "start": "3479110",
    "end": "3487944"
  },
  {
    "text": "Um, so some example tasks that we might want to do are like document classification. So for example, you have um,",
    "start": "3487945",
    "end": "3494934"
  },
  {
    "text": "like the transcript of a parole hearing and you wanna predict um, whether ah, that transcript, ah,",
    "start": "3494935",
    "end": "3500980"
  },
  {
    "text": "you know, ah, ended up in a positive or negative decision. Um, you might want to like cluster documents together.",
    "start": "3500980",
    "end": "3508600"
  },
  {
    "text": "So for example, you have like um, hundreds of different news articles and you want to cluster together the ones that um,",
    "start": "3508600",
    "end": "3514600"
  },
  {
    "text": "are talking about the like same event or concept. Um, maybe that's ah, something that you wanna do.",
    "start": "3514600",
    "end": "3520510"
  },
  {
    "text": "Um, or even reading comprehension. So, um, if you're familiar with SQuAD that's um,",
    "start": "3520510",
    "end": "3525595"
  },
  {
    "text": "a reading comprehension dataset um, that kind of takes like a paragraph long context and tries to find the answer um,",
    "start": "3525595",
    "end": "3532990"
  },
  {
    "text": "and within some span of that paragraph, um, but there are even reading comprehension um, data sets that look at like you know, longer inputs.",
    "start": "3532990",
    "end": "3540655"
  },
  {
    "text": "So for example, the NewsQA dataset takes an entire CNN article and kind of does a similar thing to SQuAD or um,",
    "start": "3540655",
    "end": "3547630"
  },
  {
    "text": "you know you ask a question and try to find the answer somewhere within the whole article. So you can see how that would be a significantly more difficult task because you",
    "start": "3547630",
    "end": "3554530"
  },
  {
    "text": "kind of have to look across this much longer context for the answer. Um, and then the final example will be summarization.",
    "start": "3554530",
    "end": "3561250"
  },
  {
    "text": "Um, where you know, you wanna go from a very long um, text to a shorter representation um, but still capturing all the meaning.",
    "start": "3561250",
    "end": "3570235"
  },
  {
    "text": "So um, yeah. I just want to talk about a couple of methods um, that are like hopefully practically useful um,",
    "start": "3570235",
    "end": "3576700"
  },
  {
    "text": "for you guys on while you're thinking about um, these things and like working on your projects. Um, so just to start off,",
    "start": "3576700",
    "end": "3584319"
  },
  {
    "text": "I wanna refresh, um, on like vector representations of words, we've seen a ton of different methods to do this,",
    "start": "3584320",
    "end": "3590380"
  },
  {
    "text": "you know from the very basic um, just using like one-hot representations um, to represent one word in your whole vocabulary um,",
    "start": "3590380",
    "end": "3598135"
  },
  {
    "text": "or to use some of these like VSM um, type approaches or even from today um,",
    "start": "3598135",
    "end": "3603445"
  },
  {
    "text": "using something like BERT or ELMo um, to get word representations, right?",
    "start": "3603445",
    "end": "3609339"
  },
  {
    "text": "But kind of like the question, um, like when you wanna go from word vectors to something longer is like, you know,",
    "start": "3609340",
    "end": "3616330"
  },
  {
    "text": "if I have vectors that are re- representing on like each of the individual words in my input,",
    "start": "3616330",
    "end": "3621400"
  },
  {
    "text": "how can I like kind of compose those into representations for my whole paragraph or document?",
    "start": "3621400",
    "end": "3626484"
  },
  {
    "text": "Um, so does anyone have any like ideas of something that they would try, just, you know,",
    "start": "3626485",
    "end": "3631780"
  },
  {
    "text": "as like maybe a baseline or like something just off the top of your head. Yeah.",
    "start": "3631780",
    "end": "3638940"
  },
  {
    "text": "[NOISE] Like average the vectors of the all the words in the text.",
    "start": "3638940",
    "end": "3644400"
  },
  {
    "text": "Yeah, um. Yeah, so ah, if people didn't catch that it was just like take all your word vectors and average them.",
    "start": "3644400",
    "end": "3651270"
  },
  {
    "text": "And that's actually a really good approach um, which is like, you know, often done like not even as a baseline but just like as the primary approach um,",
    "start": "3651270",
    "end": "3659460"
  },
  {
    "text": "to get like a document ah, vector, given a bunch of word vectors, right? Um, and especially if you have like kind of ah,",
    "start": "3659460",
    "end": "3665770"
  },
  {
    "text": "context aware representations, like BERT vectors maybe that's good enough. Um, yeah.",
    "start": "3665770",
    "end": "3670960"
  },
  {
    "text": "So that's like the first good baseline method um, just to do sort of like a bag of word vector sort of thing.",
    "start": "3670960",
    "end": "3676900"
  },
  {
    "text": "Where you sum or average or kind of do a max pool over the word vectors um,",
    "start": "3676900",
    "end": "3682525"
  },
  {
    "text": "in your paragraph or document. Um, but can anyone think of like some drawbacks to using this um,",
    "start": "3682525",
    "end": "3689025"
  },
  {
    "text": "as like the only thing um, to create your like document embedding? Yeah.",
    "start": "3689025",
    "end": "3694590"
  },
  {
    "text": "A word in the first sentence can be very different context than a word from the last sentence. So it doesn't really regularize the,",
    "start": "3694590",
    "end": "3700560"
  },
  {
    "text": "[NOISE] the whole document and what you are looking at. Yeah, yeah definitely. You kind of lose um, like the, ah,",
    "start": "3700560",
    "end": "3706945"
  },
  {
    "text": "like the structure of the sentence or like, um, the ordering of the words, yeah for sure.",
    "start": "3706945",
    "end": "3712420"
  },
  {
    "text": "Um, so a one kind of like, you know, step above doing that um, is to instead of just like do a pure averaging to um,",
    "start": "3712420",
    "end": "3720715"
  },
  {
    "text": "use like the structure of the sentence itself um, you know, to like do these sort of like matrix operations but in a- like a structured way.",
    "start": "3720715",
    "end": "3731470"
  },
  {
    "text": "So like, you know, maybe you can parse your sentence first, um, and then combine the words based on that parse tree that comes out.",
    "start": "3731470",
    "end": "3738595"
  },
  {
    "text": "Um, and that's also like a pretty good approach, um, but it does like rely on your parsing to be",
    "start": "3738595",
    "end": "3744040"
  },
  {
    "text": "accurate and doesn't work as well once you have more than one sentence. Um, so another thing that you might ah,",
    "start": "3744040",
    "end": "3751525"
  },
  {
    "text": "wanna try is like using an RNN as your document encoder, right? So like something on but you could potentially um,",
    "start": "3751525",
    "end": "3757480"
  },
  {
    "text": "think of doing is like training an RNN as an autoencoder for the paragraph or document that you have um,",
    "start": "3757480",
    "end": "3764320"
  },
  {
    "text": "or even you know, like having the loss propagate all the way back from whatever downstream task, so like maybe you are doing a classification.",
    "start": "3764320",
    "end": "3770770"
  },
  {
    "text": "Um, and then something that you could do is use the output at the last time step as kind of like an embedding for your whole document.",
    "start": "3770770",
    "end": "3777970"
  },
  {
    "text": "And like, we've seen this when we're doing like sentence embeddings, right? Um, I guess I already have this on the slide here but,",
    "start": "3777970",
    "end": "3784930"
  },
  {
    "text": "ah, what's kind of like um, you know, a situation where like doing this um, would be like not the best solution? Yeah.",
    "start": "3784930",
    "end": "3794380"
  },
  {
    "text": "I mean maybe your most important information is in the first sentence and then you have a bunch of just kinda filler information. Ah, so if you use the very last thing maybe,",
    "start": "3794380",
    "end": "3802210"
  },
  {
    "text": "you basically have the vanishing gradient problem where all of your signal has gotten diluted along the way.",
    "start": "3802210",
    "end": "3807295"
  },
  {
    "text": "Yeah. That's exactly right. Um, so especially for the cases that we're thinking about um, now where you have a very very long text.",
    "start": "3807295",
    "end": "3815080"
  },
  {
    "text": "Um, if your document you know, is like many sentences long um, then like and you're taking kind of",
    "start": "3815080",
    "end": "3821680"
  },
  {
    "text": "the output of the last time step as your representation. Um, then yes exactly. So through the vanishing gradient problem, you know,",
    "start": "3821680",
    "end": "3828220"
  },
  {
    "text": "a lot of the signal from the beginning gets lost. Um, and maybe that context from the beginning is important, right?",
    "start": "3828220",
    "end": "3834850"
  },
  {
    "text": "And you'll like- often in a lot of documents the context from the beginning like for example the abstract or like the first sentence of an article,",
    "start": "3834850",
    "end": "3841944"
  },
  {
    "text": "like that's um, like sending a lot of the signal for whatever tasks you might want to do [NOISE].",
    "start": "3841945",
    "end": "3848207"
  },
  {
    "text": "And so one approach that, uh, was developed specifically kind of for this task,um, of representing long texts like document-length texts is called Doc2vec,",
    "start": "3848207",
    "end": "3857925"
  },
  {
    "text": "and this might look familiar because it's from the authors of word2vec. Um, and so like, uh,",
    "start": "3857925",
    "end": "3864765"
  },
  {
    "text": "to kind of put this in context since the Doc2vec algorithm is actually quite close to the ones used for word2vec.",
    "start": "3864765",
    "end": "3870724"
  },
  {
    "text": "I'm just do a- I'll just do a quick like brief review of kind of what is going on when we do train word2vec vectors.",
    "start": "3870725",
    "end": "3878355"
  },
  {
    "text": "Um, so essentially, uh, we have kind of like a sliding window, uh, over the current word.",
    "start": "3878355",
    "end": "3884910"
  },
  {
    "text": "Um, so here the current word would be jumped and our window, uh, like lets say the length is 5, right,",
    "start": "3884910",
    "end": "3890910"
  },
  {
    "text": "so it's including like this whole section of the sentence. And we're using the surrounding words of our current word to try to",
    "start": "3890910",
    "end": "3897660"
  },
  {
    "text": "predict our current word and that's kinda like the objective that we're optimizing for. Um, and then through backpropagation this word",
    "start": "3897660",
    "end": "3905490"
  },
  {
    "text": "embedding matrix will get updated and then by the end of training, we'll have a matrix containing vectors,",
    "start": "3905490",
    "end": "3910845"
  },
  {
    "text": "um, for all of the words in our vocabulary. Um, each of which is like a distributed representation of that word and hopefully, you know,",
    "start": "3910845",
    "end": "3917670"
  },
  {
    "text": "embeds some sort of context information, um, er, and like is something that is semantically meaningful.",
    "start": "3917670",
    "end": "3924704"
  },
  {
    "text": "And so going from word2vec to Doc2vec, it's actually very, very similar.",
    "start": "3924705",
    "end": "3931570"
  },
  {
    "text": "Um, so the only thing we're really adding here, um, is this extra vector that gets combined with, um,",
    "start": "3931570",
    "end": "3938610"
  },
  {
    "text": "the words in our context window, um, which is the document vector. So in addition to our word embedding matrix",
    "start": "3938610",
    "end": "3945765"
  },
  {
    "text": "we also have a document matrix which has you know, like a row for, um, each document ID in our training corpus.",
    "start": "3945765",
    "end": "3953285"
  },
  {
    "text": "And so when we are trying to predict the kind of like center word in our context,",
    "start": "3953285",
    "end": "3959640"
  },
  {
    "text": "we're also incorporating the document vector, which you can kind of think of, um, as like a memory, ah,",
    "start": "3959640",
    "end": "3966540"
  },
  {
    "text": "for like the concept of the document overall. Um, so in this kind of paradigm,",
    "start": "3966540",
    "end": "3972959"
  },
  {
    "text": "we're training this word embedding matrix simultaneously with the document matrix. Um, and, you know, you can get like,",
    "start": "3972959",
    "end": "3979950"
  },
  {
    "text": "your document representation by simply extracting, um, a row from the document matrix.",
    "start": "3979950",
    "end": "3986535"
  },
  {
    "text": "Does anyone have any questions about that? Yeah. Does that mean that the doc ID is going to be more influenced",
    "start": "3986535",
    "end": "3994635"
  },
  {
    "text": "by like the words at the beginning of the document or like where in the, like is it [NOISE]",
    "start": "3994635",
    "end": "4002545"
  },
  {
    "text": "does the doc ID like is that a token that is somewhere like in the document? Um, no. So, um,",
    "start": "4002545",
    "end": "4008890"
  },
  {
    "text": "you can kind of just like think about it as a memory for like the entire document, um, as a whole.",
    "start": "4008890",
    "end": "4014890"
  },
  {
    "text": "So like for each context window within the whole document, that document ID vector will be incorporated. Yeah.",
    "start": "4014890",
    "end": "4023710"
  },
  {
    "text": "Does the document ID never appear like in the middle position as the thing that you are predicting other things relative to or",
    "start": "4023710",
    "end": "4030799"
  },
  {
    "text": "only the kind of as an addition to the context window? Oh like, will it ever be,",
    "start": "4030800",
    "end": "4036365"
  },
  {
    "text": "uh, like in the middle of these? [OVERLAPPING] Oh I see like will that ever be the thing that we're trying to predict?",
    "start": "4036365",
    "end": "4042920"
  },
  {
    "text": "Um, according to the paper, no. I guess that would be like an interesting thing to try but, no. Yeah.",
    "start": "4042920",
    "end": "4048490"
  },
  {
    "text": "Could this actually this be the worst word embeddings? Because I can see how it's sort of allowed the word embeddings to",
    "start": "4048490",
    "end": "4054320"
  },
  {
    "text": "cheat a little bit but not actually learn as much about the language. Um.",
    "start": "4054320",
    "end": "4059650"
  },
  {
    "text": "Might be an article about jumping so now, it's all over, right? Like even imagine that it can just kind of",
    "start": "4059650",
    "end": "4064715"
  },
  {
    "text": "guess that it's jumping even though the sentence is not. Oh like you could just like kind of be",
    "start": "4064715",
    "end": "4070280"
  },
  {
    "text": "lucky and always like predictive regardless of the context. Um, yeah, that's possible,",
    "start": "4070280",
    "end": "4075650"
  },
  {
    "text": "um, and like, I think it's like one of the trade-offs. And the so like, so something that's commonly done is like you might want",
    "start": "4075650",
    "end": "4082700"
  },
  {
    "text": "to initialize your word embedding matrix with like some word vectors that are already pretrained and then",
    "start": "4082700",
    "end": "4088710"
  },
  {
    "text": "you're kind of like using this as more of like a retro-fitting sort of scheme. Um, yeah.",
    "start": "4088710",
    "end": "4094385"
  },
  {
    "text": "Like I think that -that's definitely possible but it might be more of an edge case. Because this way, uh, like one could argue you even get",
    "start": "4094385",
    "end": "4101359"
  },
  {
    "text": "like better word embeddings because you kind of have like both the overall context of",
    "start": "4101360",
    "end": "4106910"
  },
  {
    "text": "the document as well as like the words that are trained. Um because even though these word embeddings are, um,",
    "start": "4106910",
    "end": "4113094"
  },
  {
    "text": "like used for all of the documents, um, like you could think of the representation for one word in one document as kind of",
    "start": "4113095",
    "end": "4120270"
  },
  {
    "text": "being like the combination of both the document vector and the word vector.",
    "start": "4120270",
    "end": "4125380"
  },
  {
    "text": "Well, has there ever been any work done on sort of like- so this is",
    "start": "4125380",
    "end": "4130409"
  },
  {
    "text": "sort of like the micro-level of words and then like the macro-level global documents. But has there been work done on like doing",
    "start": "4130410",
    "end": "4137029"
  },
  {
    "text": "some medium level like the beginning middle and end of the document. However, that's defined like the first,",
    "start": "4137030",
    "end": "4142640"
  },
  {
    "text": "second, and third, third [NOISE] I don't know, [OVERLAPPING] of anything like that.",
    "start": "4142640",
    "end": "4149120"
  },
  {
    "text": "I mean, I don't know of work in this mode but there's old work on what was called text tiling. And that was the unsupervised method for",
    "start": "4149120",
    "end": "4156055"
  },
  {
    "text": "discovering which parts of the document were important. And I think you might be drawing on similar insights but maybe learning them in a more free form way.",
    "start": "4156055",
    "end": "4164105"
  },
  {
    "text": "And I think the idea that Cindy is pointing out here has enough generality that you could do it at the paragraph level,",
    "start": "4164105",
    "end": "4169940"
  },
  {
    "text": "or hierarchically, right, you could learn a lot of these different embedding letters. Yeah, its a good idea.",
    "start": "4169940",
    "end": "4175615"
  },
  {
    "text": "Yeah. Um, should I recap for the recording? I think people can hear.",
    "start": "4175615",
    "end": "4180930"
  },
  {
    "text": "Okay. Cool. Um, yeah. So trying to summarize the Doc2vec, uh,",
    "start": "4180930",
    "end": "4188880"
  },
  {
    "text": "approach, uh, simultaneously learns, um, a word vector for every word and also a document vector for every document.",
    "start": "4188880",
    "end": "4196450"
  },
  {
    "text": "Um, and some of the benefits are that it's kind of like this unsupervised training paradigm so you can get,",
    "start": "4196450",
    "end": "4202945"
  },
  {
    "text": "um, good document vectors, uh, without, you know, having to train for some specific downstream task.",
    "start": "4202945",
    "end": "4209315"
  },
  {
    "text": "Um, a-and it's kind of like purely based on like the word distributions, um, within your corpus.",
    "start": "4209315",
    "end": "4215785"
  },
  {
    "text": "Um, and then one thing that I didn't talk about was what kind of happens at inference time.",
    "start": "4215785",
    "end": "4220820"
  },
  {
    "text": "Um, so like, you know, if you use, uh, like the Doc2vec kind of approach to get document vectors like, you know,",
    "start": "4220820",
    "end": "4227844"
  },
  {
    "text": "what happens when you have a document that wasn't part of your original um, training corpus so like isn't present in that document matrix.",
    "start": "4227845",
    "end": "4235350"
  },
  {
    "text": "Um, so what the paper actually proposes, um, for inference time is when, ah,",
    "start": "4235350",
    "end": "4241400"
  },
  {
    "text": "we're trying to do this, we will fix the word matrix. So like those, the word vectors are no longer updated",
    "start": "4241400",
    "end": "4246680"
  },
  {
    "text": "but we'll augment the document matrix, um, with this new document and then actually like retrain for a few epochs,",
    "start": "4246680",
    "end": "4252849"
  },
  {
    "text": "um, to get the, um, document vector for that new document. Um, so one potential issue with this, um,",
    "start": "4252850",
    "end": "4259489"
  },
  {
    "text": "is that it's actually like non-deterministic because of that, ah, kind of preexisting state like within the model.",
    "start": "4259490",
    "end": "4266164"
  },
  {
    "text": "Right, so, um, if you kind of do this like twice for the same,",
    "start": "4266165",
    "end": "4271780"
  },
  {
    "text": "um, like input document, uh, you might actually get two different vectors.",
    "start": "4271780",
    "end": "4276965"
  },
  {
    "text": "Um, and like you can solve this by just like training for more epochs, um, that'll make it a little bit more stable, but that is kind of a risk.",
    "start": "4276965",
    "end": "4284170"
  },
  {
    "text": "Cool. And just to wrap it up, um, there- here are some good resources that",
    "start": "4284620",
    "end": "4290540"
  },
  {
    "text": "you guys can turn to if this is something that you're tackling, um, in your own project. Um, kind of like when doing something involving",
    "start": "4290540",
    "end": "4297690"
  },
  {
    "text": "long texts that needs representations for that. Um, so the gensim package of which we've seen",
    "start": "4297690",
    "end": "4303140"
  },
  {
    "text": "before with word2vec provides a very good Doc2vec, um, implementation that you can just kinda like use out of the box,",
    "start": "4303140",
    "end": "4309440"
  },
  {
    "text": "so that's really helpful. Um, and then for general document embedding, um, the flair library, ah,",
    "start": "4309440",
    "end": "4314740"
  },
  {
    "text": "has like some good classes that you can use to, uh, do like various different types of embedding,",
    "start": "4314740",
    "end": "4320545"
  },
  {
    "text": "so you can kind of love- have like a whole pipeline, um, to kind of extract uh, you know,",
    "start": "4320545",
    "end": "4326150"
  },
  {
    "text": "like BERT embeddings and like word2vec embeddings and then like combine them in a variety of ways. Whether that be through like, you know,",
    "start": "4326150",
    "end": "4332400"
  },
  {
    "text": "simple pooling or like also, um, RNN methods. So yeah, if, uh,",
    "start": "4332400",
    "end": "4337560"
  },
  {
    "text": "if you guys are working, um, on something that, ah, is using long texts, um, I'd like to definitely point you guys towards these resources.",
    "start": "4337560",
    "end": "4343985"
  },
  {
    "text": "So hopefully, it's helpful. [APPLAUSE].",
    "start": "4343985",
    "end": "4355290"
  },
  {
    "text": "Any last questions though? Have people tried using transformers for this?",
    "start": "4355290",
    "end": "4361500"
  },
  {
    "text": "Um, that's an excellent question. Um, yeah, so I was talking with Chris earlier about like how effective it would be,",
    "start": "4361500",
    "end": "4368570"
  },
  {
    "text": "um, to use transformers or like to even use BERT as like an encoder, um, for like a document length, um,",
    "start": "4368570",
    "end": "4376910"
  },
  {
    "text": "like sort of input, and Bert for sure [NOISE] kind of has like, like a length cutoff.",
    "start": "4376910",
    "end": "4383845"
  },
  {
    "text": "At least for the pretrained models, um, the maximum sequence length is like 512 I believe.",
    "start": "4383845",
    "end": "4389300"
  },
  {
    "text": "So there could be long enough for something that you want, um, but it might be too short. So kinda just depends on your task.",
    "start": "4389300",
    "end": "4396150"
  },
  {
    "text": "[BACKGROUND].",
    "start": "4396150",
    "end": "4402239"
  },
  {
    "text": "Yeah, you can do that, yeah. And then you kind of have to make a design decision as to like how you want to combine the representations for different sentences.",
    "start": "4402240",
    "end": "4409054"
  },
  {
    "text": "But yeah, I think like at the sentence encoding level, um, there's been a lot of work done and there's like a whole plethora of things,",
    "start": "4409055",
    "end": "4416115"
  },
  {
    "text": "um, yeah, BERT just being one of them. The only thing I would say for that is that I feel that it's not in",
    "start": "4416115",
    "end": "4421545"
  },
  {
    "text": "the spirit of the BERT paper to sentence tokenize. But I feel like that should come.",
    "start": "4421545",
    "end": "4428070"
  },
  {
    "text": "Certainly, you should just be putting set symbols but learning continuously as opposed to doing a kind of hard aggregation at each sentence.",
    "start": "4428070",
    "end": "4435670"
  },
  {
    "text": "So I feel like even if BERT is practically useful in that setting, you know, judging by sentences, there's still an opportunity there to think about applying it,",
    "start": "4435670",
    "end": "4444020"
  },
  {
    "text": "you know, at the document level. But I think that would lead to a bunch of obstacles right now,",
    "start": "4444020",
    "end": "4449425"
  },
  {
    "text": "not just because of the pretrained representations but also because of the amount of compute power required.",
    "start": "4449425",
    "end": "4455130"
  },
  {
    "text": "[NOISE] Well, thank you again Cindy,",
    "start": "4455130",
    "end": "4462620"
  },
  {
    "text": "that was great. [APPLAUSE]",
    "start": "4462620",
    "end": "4475000"
  }
]