[
  {
    "start": "0",
    "end": "4687"
  },
  {
    "text": "SPEAKER: Welcome back, everyone.",
    "start": "4687",
    "end": "6019"
  },
  {
    "text": "This is part 7 in our series\non contextual representations.",
    "start": "6020",
    "end": "9220"
  },
  {
    "text": "We're going to talk\nabout the ELECTRA model.",
    "start": "9220",
    "end": "11920"
  },
  {
    "text": "Recall that I finished\nthe BERT screencast",
    "start": "11920",
    "end": "14230"
  },
  {
    "text": "by listing out some known\nlimitations of that model.",
    "start": "14230",
    "end": "17560"
  },
  {
    "text": "RoBERTa addressed\nitem one on that list.",
    "start": "17560",
    "end": "19990"
  },
  {
    "text": "And we can think of\nELECTRA as keying",
    "start": "19990",
    "end": "21760"
  },
  {
    "text": "into items two and three.",
    "start": "21760",
    "end": "23650"
  },
  {
    "text": "Item two is about\nthe masked token.",
    "start": "23650",
    "end": "25570"
  },
  {
    "text": "The BERT team\nobserved that they had",
    "start": "25570",
    "end": "27160"
  },
  {
    "text": "created a mismatch between the\npre-training and fine-tuning",
    "start": "27160",
    "end": "30520"
  },
  {
    "text": "vocabularies.",
    "start": "30520",
    "end": "31630"
  },
  {
    "text": "Because the masked\ntoken is never",
    "start": "31630",
    "end": "33700"
  },
  {
    "text": "seen during fine-tuning,\nonly during training.",
    "start": "33700",
    "end": "35950"
  },
  {
    "text": "And you could think that\nmismatch might reduce",
    "start": "35950",
    "end": "38740"
  },
  {
    "text": "the effectiveness of the model.",
    "start": "38740",
    "end": "41200"
  },
  {
    "text": "Item three is about efficiency.",
    "start": "41200",
    "end": "43060"
  },
  {
    "text": "The BERT team observed\nthat the MLM objective",
    "start": "43060",
    "end": "46570"
  },
  {
    "text": "means that they only\nuse around 15% of tokens",
    "start": "46570",
    "end": "50410"
  },
  {
    "text": "when they are training.",
    "start": "50410",
    "end": "51970"
  },
  {
    "text": "Only 15% of them even\ncontribute to the MLM objective.",
    "start": "51970",
    "end": "55900"
  },
  {
    "text": "We have to do all this\nwork of processing",
    "start": "55900",
    "end": "57970"
  },
  {
    "text": "every item in the sequence,\nbut we get very few learning",
    "start": "57970",
    "end": "61030"
  },
  {
    "text": "signals from that process.",
    "start": "61030",
    "end": "62989"
  },
  {
    "text": "And that's certainly\ndata inefficient.",
    "start": "62990",
    "end": "64730"
  },
  {
    "text": "And we might think\nabout finding ways",
    "start": "64730",
    "end": "66470"
  },
  {
    "text": "to make more use of\nthe available data.",
    "start": "66470",
    "end": "70370"
  },
  {
    "text": "ELECTRA is going to make\nprogress on both these fronts.",
    "start": "70370",
    "end": "72690"
  },
  {
    "text": "Let's explore the\ncore model structure.",
    "start": "72690",
    "end": "75770"
  },
  {
    "text": "For our example, we have\nthis input sequence x,",
    "start": "75770",
    "end": "78680"
  },
  {
    "text": "the chef cooked the meal.",
    "start": "78680",
    "end": "80720"
  },
  {
    "text": "The first thing we do\nis create x masked,",
    "start": "80720",
    "end": "83280"
  },
  {
    "text": "which is a masked version\nof that input sequence.",
    "start": "83280",
    "end": "86119"
  },
  {
    "text": "And we could do that\nusing the same protocol",
    "start": "86120",
    "end": "88310"
  },
  {
    "text": "as they used for BERT by masking\nout, say, 15% of the tokens",
    "start": "88310",
    "end": "92509"
  },
  {
    "text": "at random.",
    "start": "92510",
    "end": "94050"
  },
  {
    "text": "Then we have our generator.",
    "start": "94050",
    "end": "95510"
  },
  {
    "text": "This is a small BERT-like model\nthat processes that input,",
    "start": "95510",
    "end": "100280"
  },
  {
    "text": "and produces what\nwe call x corrupt.",
    "start": "100280",
    "end": "102900"
  },
  {
    "text": "This is an output sequence\npredicted by the model.",
    "start": "102900",
    "end": "105530"
  },
  {
    "text": "And the twist here\nis that we're going",
    "start": "105530",
    "end": "108350"
  },
  {
    "text": "to replace some of those tokens,\nnot with their original inputs,",
    "start": "108350",
    "end": "111920"
  },
  {
    "text": "but rather with\ntokens that come out",
    "start": "111920",
    "end": "114200"
  },
  {
    "text": "with probabilities proportional\nto the probability generators.",
    "start": "114200",
    "end": "117950"
  },
  {
    "text": "And what that means is\nthat sometimes we'll",
    "start": "117950",
    "end": "120170"
  },
  {
    "text": "replace with the\nactual input token,",
    "start": "120170",
    "end": "122330"
  },
  {
    "text": "and sometimes with\na different token,",
    "start": "122330",
    "end": "124400"
  },
  {
    "text": "like in this case\nof cooked coming in,",
    "start": "124400",
    "end": "126500"
  },
  {
    "text": "being replaced by ate.",
    "start": "126500",
    "end": "128660"
  },
  {
    "text": "That is where ELECTRA, the\ndiscriminator, takes over.",
    "start": "128660",
    "end": "132770"
  },
  {
    "text": "The job of the discriminator,\nwhich is really",
    "start": "132770",
    "end": "135200"
  },
  {
    "text": "the heart of the\nELECTRA model, is",
    "start": "135200",
    "end": "137360"
  },
  {
    "text": "to figure out which of\nthose tokens in x corrupt",
    "start": "137360",
    "end": "140660"
  },
  {
    "text": "is an original, and\nwhich was replaced.",
    "start": "140660",
    "end": "144310"
  },
  {
    "text": "So we train this model\njointly with the generator,",
    "start": "144310",
    "end": "147330"
  },
  {
    "text": "and a weighted version of\nthe discriminator or ELECTRA",
    "start": "147330",
    "end": "150090"
  },
  {
    "text": "objective.",
    "start": "150090",
    "end": "151260"
  },
  {
    "text": "And then, essentially, we\ncan allow the generator",
    "start": "151260",
    "end": "154830"
  },
  {
    "text": "to drop away, and focus\non the discriminator",
    "start": "154830",
    "end": "157260"
  },
  {
    "text": "as the primary pre-trained\nartifact from this process.",
    "start": "157260",
    "end": "161959"
  },
  {
    "text": "One thing that I really\nlove about the ELECTRA paper",
    "start": "161960",
    "end": "164320"
  },
  {
    "text": "is that it includes very rich\nstudies of how best to set up",
    "start": "164320",
    "end": "168610"
  },
  {
    "text": "the ELECTRA model itself.",
    "start": "168610",
    "end": "170920"
  },
  {
    "text": "I'll review some\nof that evidence",
    "start": "170920",
    "end": "172390"
  },
  {
    "text": "here, starting with\nthe relationship",
    "start": "172390",
    "end": "173980"
  },
  {
    "text": "that they uncover\nbetween the generator",
    "start": "173980",
    "end": "176140"
  },
  {
    "text": "and the discriminator.",
    "start": "176140",
    "end": "177955"
  },
  {
    "text": "First thing is an observation.",
    "start": "177955",
    "end": "179500"
  },
  {
    "text": "Where the generator\nand discriminator",
    "start": "179500",
    "end": "181420"
  },
  {
    "text": "are the same size, they\ncould, in principle,",
    "start": "181420",
    "end": "184000"
  },
  {
    "text": "share their\ntransformer parameters.",
    "start": "184000",
    "end": "186310"
  },
  {
    "text": "And the team found that more\nsharing is indeed better.",
    "start": "186310",
    "end": "189770"
  },
  {
    "text": "However, the best results come\nfrom having a generator that",
    "start": "189770",
    "end": "193750"
  },
  {
    "text": "is small compared to\nthe discriminator, which",
    "start": "193750",
    "end": "196510"
  },
  {
    "text": "means less sharing.",
    "start": "196510",
    "end": "198510"
  },
  {
    "text": "Here's a chart summarizing\ntheir evidence for this.",
    "start": "198510",
    "end": "201209"
  },
  {
    "text": "Along the x-axis, I\nhave the generator size,",
    "start": "201210",
    "end": "204180"
  },
  {
    "text": "going up to 1024.",
    "start": "204180",
    "end": "206310"
  },
  {
    "text": "And along the y-axis,\nwe have GLUE score,",
    "start": "206310",
    "end": "208890"
  },
  {
    "text": "which will be our proxy\nfor overall quality.",
    "start": "208890",
    "end": "212490"
  },
  {
    "text": "The blue line up here is the\ndiscriminator at size 768.",
    "start": "212490",
    "end": "216630"
  },
  {
    "text": "And we're tracking different\ngenerator sizes, as I said.",
    "start": "216630",
    "end": "219570"
  },
  {
    "text": "And you see this\ncharacteristic reverse U-shape,",
    "start": "219570",
    "end": "222510"
  },
  {
    "text": "where, for example, the best\ndiscriminator at size 768",
    "start": "222510",
    "end": "226620"
  },
  {
    "text": "corresponds to a\ngenerator of size 256.",
    "start": "226620",
    "end": "229950"
  },
  {
    "text": "And indeed, as the generator\ngets larger, and even gets",
    "start": "229950",
    "end": "233400"
  },
  {
    "text": "larger than the discriminator,\nperformance drops off.",
    "start": "233400",
    "end": "236819"
  },
  {
    "text": "And that U-shape\nis repeated for all",
    "start": "236820",
    "end": "238920"
  },
  {
    "text": "these different discriminator\nsizes, suggesting",
    "start": "238920",
    "end": "241980"
  },
  {
    "text": "a real finding about the model.",
    "start": "241980",
    "end": "243900"
  },
  {
    "text": "I think the intuition\nhere is that it's",
    "start": "243900",
    "end": "246480"
  },
  {
    "text": "good to have a small and\nrelatively weak generator,",
    "start": "246480",
    "end": "249569"
  },
  {
    "text": "so that the discriminator has a\nlot of interesting work to do.",
    "start": "249570",
    "end": "252780"
  },
  {
    "text": "Because after all, the\ndiscriminator is our focus.",
    "start": "252780",
    "end": "257920"
  },
  {
    "text": "The paper also includes a\nlot of efficiency studies.",
    "start": "257920",
    "end": "260709"
  },
  {
    "text": "And those, too, are\nreally illuminating.",
    "start": "260709",
    "end": "262660"
  },
  {
    "text": "This is a summary of\nsome of their evidence.",
    "start": "262660",
    "end": "264790"
  },
  {
    "text": "Along the x-axis, we\nhave pre-train FLOPs,",
    "start": "264790",
    "end": "267460"
  },
  {
    "text": "which you can think of as a\nraw amount of overall compute",
    "start": "267460",
    "end": "271060"
  },
  {
    "text": "needed for training.",
    "start": "271060",
    "end": "272470"
  },
  {
    "text": "And along the y-axis, again,\nwe have the GLUE score.",
    "start": "272470",
    "end": "275930"
  },
  {
    "text": "The blue line at the top here\nis the full ELECTRA model.",
    "start": "275930",
    "end": "278889"
  },
  {
    "text": "And the core result here is\nthat for any compute budget you",
    "start": "278890",
    "end": "282040"
  },
  {
    "text": "have-- that is, any\npoint along the x-axis--",
    "start": "282040",
    "end": "284470"
  },
  {
    "text": "ELECTRA is the best model.",
    "start": "284470",
    "end": "286720"
  },
  {
    "text": "It looks like, in second\nplace, is adversarial ELECTRA.",
    "start": "286720",
    "end": "290200"
  },
  {
    "text": "That's an intriguing\nvariation of the model, where",
    "start": "290200",
    "end": "293050"
  },
  {
    "text": "the generator is\nactually trained to try",
    "start": "293050",
    "end": "295270"
  },
  {
    "text": "to fool the discriminator.",
    "start": "295270",
    "end": "296949"
  },
  {
    "text": "That's a clear\nintuition that turns out",
    "start": "296950",
    "end": "299350"
  },
  {
    "text": "to be slightly less good than\nthe more cooperative objective",
    "start": "299350",
    "end": "302500"
  },
  {
    "text": "that I presented before.",
    "start": "302500",
    "end": "304420"
  },
  {
    "text": "And then the green lines\nare intriguing as well.",
    "start": "304420",
    "end": "306680"
  },
  {
    "text": "So for the green lines,\nwe begin by training just",
    "start": "306680",
    "end": "310120"
  },
  {
    "text": "in a standard BERT fashion.",
    "start": "310120",
    "end": "311949"
  },
  {
    "text": "And then, at a certain\npoint, we switch over",
    "start": "311950",
    "end": "314680"
  },
  {
    "text": "to the full ELECTRA model.",
    "start": "314680",
    "end": "316449"
  },
  {
    "text": "And what you see there is\nthat, in switching over",
    "start": "316450",
    "end": "318830"
  },
  {
    "text": "to full ELECTRA, you get\na gain in performance",
    "start": "318830",
    "end": "321830"
  },
  {
    "text": "for any compute budget\nrelative to the standard BERT",
    "start": "321830",
    "end": "324979"
  },
  {
    "text": "training continuing\nas before, which is",
    "start": "324980",
    "end": "327200"
  },
  {
    "text": "the lowest line in the chart.",
    "start": "327200",
    "end": "329730"
  },
  {
    "text": "So a clear win for\nELECTRA, relative to",
    "start": "329730",
    "end": "332420"
  },
  {
    "text": "these interesting competitors.",
    "start": "332420",
    "end": "334940"
  },
  {
    "text": "And they did further\nefficiency analyses.",
    "start": "334940",
    "end": "338300"
  },
  {
    "text": "Let me review some of\nwhat they found there.",
    "start": "338300",
    "end": "340310"
  },
  {
    "text": "This is the full ELECTRA model,\nas I presented it before.",
    "start": "340310",
    "end": "344060"
  },
  {
    "text": "We could also think\nabout ELECTRA 15%.",
    "start": "344060",
    "end": "347400"
  },
  {
    "text": "And this is the case where,\nfor the discriminator, instead",
    "start": "347400",
    "end": "350720"
  },
  {
    "text": "of having it make predictions\nabout all of the input tokens,",
    "start": "350720",
    "end": "353690"
  },
  {
    "text": "we just zoom in\non the tokens that",
    "start": "353690",
    "end": "356870"
  },
  {
    "text": "were part of this\nx corrupt sequence,",
    "start": "356870",
    "end": "358820"
  },
  {
    "text": "ignoring all the rest.",
    "start": "358820",
    "end": "359900"
  },
  {
    "text": "That's a very\nBERT-like intuition,",
    "start": "359900",
    "end": "361550"
  },
  {
    "text": "where the ones that\nmatter were these",
    "start": "361550",
    "end": "364099"
  },
  {
    "text": "that got masked down here.",
    "start": "364100",
    "end": "366110"
  },
  {
    "text": "That makes fewer predictions\nfor the discriminator.",
    "start": "366110",
    "end": "369669"
  },
  {
    "text": "Replace MLM is where we\nuse the discriminator",
    "start": "369670",
    "end": "372610"
  },
  {
    "text": "with no-- sorry, where\nwe use the generator",
    "start": "372610",
    "end": "374740"
  },
  {
    "text": "with no discriminator.",
    "start": "374740",
    "end": "376009"
  },
  {
    "text": "This is a kind of\nablation of BERT.",
    "start": "376010",
    "end": "377800"
  },
  {
    "text": "And then all-tokens\nMLM is a kind",
    "start": "377800",
    "end": "379930"
  },
  {
    "text": "of variant of BERT, where\ninstead of turning off",
    "start": "379930",
    "end": "382600"
  },
  {
    "text": "the objective for\nsome of the items,",
    "start": "382600",
    "end": "384400"
  },
  {
    "text": "we make predictions\nabout all of them.",
    "start": "384400",
    "end": "387560"
  },
  {
    "text": "And here's a summary\nof the evidence",
    "start": "387560",
    "end": "389380"
  },
  {
    "text": "that they found in\nfavor of ELECTRA.",
    "start": "389380",
    "end": "391420"
  },
  {
    "text": "That's at the top here,\naccording to the GLUE score.",
    "start": "391420",
    "end": "394030"
  },
  {
    "text": "All-tokens MLM and replace\nMLM, those BERT variants,",
    "start": "394030",
    "end": "397780"
  },
  {
    "text": "are just behind.",
    "start": "397780",
    "end": "399100"
  },
  {
    "text": "And that's sort of intriguing.",
    "start": "399100",
    "end": "400450"
  },
  {
    "text": "Because it shows\nthat, even if we",
    "start": "400450",
    "end": "402400"
  },
  {
    "text": "stick to the BERT architecture,\nwe could have done better",
    "start": "402400",
    "end": "405550"
  },
  {
    "text": "simply by making more\npredictions than BERT",
    "start": "405550",
    "end": "408909"
  },
  {
    "text": "was making initially.",
    "start": "408910",
    "end": "410890"
  },
  {
    "text": "Behind those is ELECTRA 15%.",
    "start": "410890",
    "end": "413500"
  },
  {
    "text": "And that shows that on\nthe discriminator side,",
    "start": "413500",
    "end": "416200"
  },
  {
    "text": "again, it pays to\nmake more predictions.",
    "start": "416200",
    "end": "418690"
  },
  {
    "text": "If we retreat to the more\nBERT-like mode, where",
    "start": "418690",
    "end": "421030"
  },
  {
    "text": "we predict only for\nthe corrupted elements,",
    "start": "421030",
    "end": "423460"
  },
  {
    "text": "we find that\nperformance degrades.",
    "start": "423460",
    "end": "425830"
  },
  {
    "text": "And then at the\nbottom of this list",
    "start": "425830",
    "end": "427960"
  },
  {
    "text": "is the original BERT model,\nshowing a clear win overall",
    "start": "427960",
    "end": "431720"
  },
  {
    "text": "for ELECTRA according\nto this GLUE benchmark.",
    "start": "431720",
    "end": "436120"
  },
  {
    "text": "The ELECTRA team released three\nmodels initially-- small, base,",
    "start": "436120",
    "end": "439510"
  },
  {
    "text": "and large.",
    "start": "439510",
    "end": "440620"
  },
  {
    "text": "Base and large correspond\nroughly to BERT releases.",
    "start": "440620",
    "end": "443889"
  },
  {
    "text": "And small is a\ntiny one that they",
    "start": "443890",
    "end": "445660"
  },
  {
    "text": "say is designed to be quickly\ntrained on a single GPU.",
    "start": "445660",
    "end": "449320"
  },
  {
    "text": "Again, another nod toward\nincreasing emphasis",
    "start": "449320",
    "end": "452380"
  },
  {
    "text": "on efficiency for compute\nas an important ingredient",
    "start": "452380",
    "end": "456160"
  },
  {
    "text": "in research in this space.",
    "start": "456160",
    "end": "458730"
  },
  {
    "start": "458730",
    "end": "464000"
  }
]