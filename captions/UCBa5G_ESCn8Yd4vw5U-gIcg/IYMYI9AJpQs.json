[
  {
    "text": "Welcome. Today we're gonna dive into the material in earnest.",
    "start": "4760",
    "end": "12780"
  },
  {
    "text": "Start building a foundation for thinking about natural language understanding models. Before we do that though,",
    "start": "12780",
    "end": "18670"
  },
  {
    "text": "I wanna do just a few logistical things, um, because, well frankly, because they're not so intuitive.",
    "start": "18670",
    "end": "24460"
  },
  {
    "text": "So let me start with the least intuitive one of all. Uh, as I said last time, we're gonna be using Canvas to do all the submission of work,",
    "start": "24460",
    "end": "33265"
  },
  {
    "text": "which basically means that you're gonna use it to upload notebooks. Um, and I think that part is straightforward, um,",
    "start": "33265",
    "end": "39570"
  },
  {
    "text": "because then we're gonna take those notebooks and, uh, e- evaluate them separately outside of the system. So you don't have to deal with very much data entry.",
    "start": "39570",
    "end": "46660"
  },
  {
    "text": "The one thing you do have to confront if you wanna work in groups is actually joining a group,",
    "start": "46660",
    "end": "52390"
  },
  {
    "text": "and that's the part on Canvas that I find quite strange. So let me just walk you through what life is like",
    "start": "52390",
    "end": "58190"
  },
  {
    "text": "from my perspective in understanding Canvas. Here, I'm logged in as an instructor and I can get",
    "start": "58190",
    "end": "64280"
  },
  {
    "text": "a feel for what you all see when I choose the student view.",
    "start": "64280",
    "end": "68909"
  },
  {
    "text": "So here I am in the student view and this is what I hope happens for you if you want to,",
    "start": "70120",
    "end": "76210"
  },
  {
    "text": "say, form a group for assignment one and bake-off one. You go to People. I'm not sure why it's called People,",
    "start": "76210",
    "end": "82880"
  },
  {
    "text": "I guess groups are made of [NOISE] people, okay? And then choose the Groups tab,",
    "start": "82880",
    "end": "88480"
  },
  {
    "text": "and what it's gonna do is load a ton of pre-created groups.",
    "start": "89660",
    "end": "96715"
  },
  {
    "text": "That's the least intuitive part of all for me that you guys can't just create your own groups under the rubric of assignment one,",
    "start": "96715",
    "end": "102820"
  },
  {
    "text": "but rather that I have to create hundreds of empty groups for you, and then you can join them on your own.",
    "start": "102820",
    "end": "108955"
  },
  {
    "text": "But what- and I see that people have done it, so I'm reassured. Like, for example, this is a group that has currently one person in it and this one has two.",
    "start": "108955",
    "end": "116185"
  },
  {
    "text": "Um, now I can't verify that this works because it says group is not available for me,",
    "start": "116185",
    "end": "122659"
  },
  {
    "text": "and I did in a panic Google around and I discovered that I think that's because I'm merely a test student who can only aspire to be a real student,",
    "start": "122660",
    "end": "131980"
  },
  {
    "text": "[LAUGHTER] and a test student can't actually join a group. But I am reassured that a bunch of people have,",
    "start": "131980",
    "end": "137450"
  },
  {
    "text": "and so my assumption is, like, if a few of you wanted to start your own group, what you would do is scroll",
    "start": "137450",
    "end": "144110"
  },
  {
    "text": "this endless list until you found one that had the right name, like, Assign 1 and Bake-off 1,",
    "start": "144110",
    "end": "149390"
  },
  {
    "text": "or Assign 3 and Bake-off 3 in the future. Claim it for yourself so that you can all join and then you've formed a group.",
    "start": "149390",
    "end": "156180"
  },
  {
    "text": "So I hope that works out for everyone, and I'm just recognizing that it's a kinda strange system.",
    "start": "156320",
    "end": "161975"
  },
  {
    "text": "Let us know if you see any unusual behavior for this. But once you've done that, then again,",
    "start": "161975",
    "end": "167750"
  },
  {
    "text": "I'm reassured that if I had joined a group when I went to the Assignments tab,",
    "start": "167750",
    "end": "174310"
  },
  {
    "text": "it would prompt me if I wanted to submit Assignment 1 and it would give this warning, like, \"Hey, just so you know,",
    "start": "174310",
    "end": "180954"
  },
  {
    "text": "the work you're about to submit is being submitted for your full group.\" It's taking a long time to load which leaves me a little bit nervous that",
    "start": "180955",
    "end": "190180"
  },
  {
    "text": "lots of you are downloading the data distribution file right now, in which case the Inter- Internet might slow to a crawl.",
    "start": "190180",
    "end": "197110"
  },
  {
    "text": "But in any case, that's what would happen. Uh, and I'm hoping it works out. And the other thing that I wanted to mention is the reason that we created that",
    "start": "197110",
    "end": "204640"
  },
  {
    "text": "we want you to get different groups for the different assignments, and then the final project is just that I couldn't tell from the system whether or",
    "start": "204640",
    "end": "211900"
  },
  {
    "text": "not if you formed a group at the start you could change it without having things go crazy. And so I thought, the worst case scenario,",
    "start": "211900",
    "end": "218325"
  },
  {
    "text": "people just reconstitute the same group for each assignment and then the final project. Um, maximum flexibility in it just means that at",
    "start": "218325",
    "end": "225500"
  },
  {
    "text": "some point you have to do a whole lot of scrolling. Okay. I'm gonna give up on that,",
    "start": "225500",
    "end": "231035"
  },
  {
    "text": "but I- I think it's going okay. Let's get oriented a little bit.",
    "start": "231035",
    "end": "237110"
  },
  {
    "text": "So here's the- the main page for the course which you can find by Googling CS224U Stanford.",
    "start": "237110",
    "end": "243460"
  },
  {
    "text": "Nice addition, we have a calendar now that shows all the office hours, that's a more intuitive interface.",
    "start": "243460",
    "end": "249650"
  },
  {
    "text": "And it also is a reminder here. You can see that we have a special session on Friday 3:15,",
    "start": "249650",
    "end": "255230"
  },
  {
    "text": "uh, a kind of refresher or an intro to Python and Jupyter Notebooks. Uh, and Lucy and Ashcan,",
    "start": "255230",
    "end": "261725"
  },
  {
    "text": "two of our teaching team are doing that. And I took the liberty of posting Lucy's Jupyter notebook tutorial here which is nice and interactive.",
    "start": "261725",
    "end": "270229"
  },
  {
    "text": "I think you could use it with them and- on Friday and then going forward. It has lots of great tips about how to use notebooks.",
    "start": "270230",
    "end": "277009"
  },
  {
    "text": "And you can also see everyone's office hours including- and it's gonna have office hours on Saturday.",
    "start": "277010",
    "end": "282185"
  },
  {
    "text": "So if you're working over the weekend he's available in the- in the Huang auditorium basement.",
    "start": "282185",
    "end": "288210"
  },
  {
    "text": "Another quick update. So I did scroll through this last time. You can kind of see our units,",
    "start": "288710",
    "end": "294810"
  },
  {
    "text": "we're gonna start building a foundation for NLU today and then we kind of branch out and explore some tasks largely in the service of giving you",
    "start": "294810",
    "end": "301939"
  },
  {
    "text": "a sense for what different models you might use for your own projects, and to start building up some best practices.",
    "start": "301940",
    "end": "308150"
  },
  {
    "text": "The one scheduling changes, um, I know a lot of you are probably thinking about",
    "start": "308150",
    "end": "313730"
  },
  {
    "text": "going off into industry and maybe doing NLU in industry. And so I had scheduled a day on May 22, where I was gonna have, like,",
    "start": "313730",
    "end": "320690"
  },
  {
    "text": "a panel of people come in, who are people who are doing NLU in industry now. And just kind of interview them and get questions from you all about what",
    "start": "320690",
    "end": "327830"
  },
  {
    "text": "their experiences are like to give you a sense for what's happening out there in the world. I should have predicted this but it's very hard to schedule people.",
    "start": "327830",
    "end": "335044"
  },
  {
    "text": "Everyone is very busy. And so I think what we're actually gonna do is have two of those panelists come on April 24,",
    "start": "335045",
    "end": "342490"
  },
  {
    "text": "and that'll be Marta Recasens, who was a post-doc here and did beautiful work on co-ref.",
    "start": "342490",
    "end": "348155"
  },
  {
    "text": "Very linguistically informed, but also successful on the engineering side, great work. And she's been at Google for a long time, having gone kind of in and out of academia.",
    "start": "348155",
    "end": "357270"
  },
  {
    "text": "So I think she has a really interesting story. So she'll be here on April 24, and she'll be joined by Guillaume Genthial,",
    "start": "357270",
    "end": "363600"
  },
  {
    "text": "who was a master student here and is now working in industry doing things like NLU for healthcare.",
    "start": "363600",
    "end": "370250"
  },
  {
    "text": "I've worked quite, quite closely with Guillaume. And he's also got an interesting story to tell about why he",
    "start": "370250",
    "end": "375979"
  },
  {
    "text": "decided to stick in industry as opposed to pursuing a PhD. And another thing I like about Guillaume's profile is",
    "start": "375980",
    "end": "382040"
  },
  {
    "text": "that he's been very helpful to me in producing lots of notebooks as you can see here",
    "start": "382040",
    "end": "387185"
  },
  {
    "text": "that kind of show you how some of these interfaces work. And so for example, if you look at the TensorFlow code that I distributed for this course,",
    "start": "387185",
    "end": "394340"
  },
  {
    "text": "you can see that I have, like, shamelessly ripped off all of his best practices and tips and tricks for doing,",
    "start": "394340",
    "end": "400455"
  },
  {
    "text": "uh, TensorFlow with these new estimator classes. So I feel indebted to Guillaume.",
    "start": "400455",
    "end": "405635"
  },
  {
    "text": "So they'll be here on April 24, and then I think we will have a similar panel discussion in late May on the 22nd.",
    "start": "405635",
    "end": "412685"
  },
  {
    "text": "And I think I have successfully guilted Bill into being one of those panelists because Bill has an incredibly interesting story where he started",
    "start": "412685",
    "end": "420230"
  },
  {
    "text": "out as a philosopher and did all this stuff in finance, and then did a PhD. And now he's played essentially every role you can",
    "start": "420230",
    "end": "427280"
  },
  {
    "text": "play doing things related to NLU in industry. So it'll be Bill and maybe somebody else.",
    "start": "427280",
    "end": "433100"
  },
  {
    "text": "Uh, I'll tell you more about those things in the future and I think I'll be soliciting questions from you all that we could,",
    "start": "433100",
    "end": "438680"
  },
  {
    "text": "uh, could ask these panelists. Let's see, I have a few other assignments,",
    "start": "438680",
    "end": "445039"
  },
  {
    "text": "uh, oh, sorry, other announcements. Um, I hope you've gotten set up with the course materials.",
    "start": "445040",
    "end": "452710"
  },
  {
    "text": "There's a notebook here for doing that, and also I'm thankful to [inaudible] for posting a really detailed post on Piazza about how to get set up with things.",
    "start": "452710",
    "end": "461920"
  },
  {
    "text": "And we're going to try to continue to do that kind of FAQ for the assignments and other important aspects of the course because Piazza can be hard to browse.",
    "start": "461920",
    "end": "469600"
  },
  {
    "text": "There's like lots of times in my life where I've said, \"Oh, this student is the 12th student asking the same question,\" and then I",
    "start": "469600",
    "end": "475960"
  },
  {
    "text": "can't find the 11 other posts about exactly that topic on Piazza, and I feel sympathy for people just doing fresh posts.",
    "start": "475960",
    "end": "482635"
  },
  {
    "text": "We're gonna try to gather together all of that information so that it's available to you. Um, another announcement Ignacio wanted me to make.",
    "start": "482635",
    "end": "490510"
  },
  {
    "text": "We're gonna update the setup instructions for people who might have access to GPUs on their laptops,",
    "start": "490510",
    "end": "495759"
  },
  {
    "text": "like if you have a Linux box. But I think the primary advice is, if you want help getting set up so that you could take advantage of your local GPU,",
    "start": "495760",
    "end": "504355"
  },
  {
    "text": "come to Ignacio's office hours. There he is. He's happy to help.",
    "start": "504355",
    "end": "510460"
  },
  {
    "text": "He says it's inevitably gonna be a mess, but in the end, you can probably get it working together.",
    "start": "510460",
    "end": "516175"
  },
  {
    "text": "And just a quick note about that, I am working very hard, shamelessly asking as many people as I can about",
    "start": "516175",
    "end": "521829"
  },
  {
    "text": "getting you cla- cloud credits for doing projects. Um, more, more details to come on that.",
    "start": "521830",
    "end": "528470"
  },
  {
    "text": "Okay. I think that's it for announcements. Any questions or comments before I dive in? Yeah.",
    "start": "528470",
    "end": "534450"
  },
  {
    "text": "What were the differences between Assignment 1 and Bake-off, are they the same thing? I wanna leave some time at the end to talk about this in",
    "start": "534450",
    "end": "540450"
  },
  {
    "text": "a little more detail but the gist of it is, the assignments are nine points and they involve, er,",
    "start": "540450",
    "end": "547015"
  },
  {
    "text": "each one of them involves setting up some baseline systems for a task, and then developing your own original system.",
    "start": "547015",
    "end": "553120"
  },
  {
    "text": "Then you enter that system into the bake-off and you get an extra point for just entering,",
    "start": "553120",
    "end": "558790"
  },
  {
    "text": "and you get some extra credit if you're the top-performing system in the bake-off. Um, so it's a kind of little project.",
    "start": "558790",
    "end": "565060"
  },
  {
    "text": "The assignment plus the bake-off, is a kind of little project and we're trying to exemplify best practices as we do",
    "start": "565060",
    "end": "570790"
  },
  {
    "text": "that with the chance of having a little win. Yeah. I don't understand the screen-cast sound?",
    "start": "570790",
    "end": "578425"
  },
  {
    "text": "The screen-casts are videos that are on YouTube that I think of as like,",
    "start": "578425",
    "end": "583540"
  },
  {
    "text": "the essence of what I wanted to say. Like just the really important details. So I'm gonna talk for example a lot about vector comparison and re-weighting today.",
    "start": "583540",
    "end": "592930"
  },
  {
    "text": "If you want the fast version, you could go to those screen-casts. It's like I've tried to distill it down and do some example calculations.",
    "start": "592930",
    "end": "600560"
  },
  {
    "text": "And I'm glad you brought this up. The way I think about this unit here right now is, we're on April 3.",
    "start": "600560",
    "end": "605790"
  },
  {
    "text": "We're doing distributed word representations. I'm gonna be working with these slides today and next time and probably a little bit next week.",
    "start": "605790",
    "end": "613595"
  },
  {
    "text": "Simultaneous with that, you could explore these notebooks which also reinforce the material.",
    "start": "613595",
    "end": "619390"
  },
  {
    "text": "But the nice thing about them is they mix everything with code. So you could be hands on all the way through in a way that you just can't with a lecture.",
    "start": "619390",
    "end": "627370"
  },
  {
    "text": "Um, and then you could reinforce that with these screen-casts if you wanted to. Um, we have a lot of coverage for thi- this first",
    "start": "627370",
    "end": "635290"
  },
  {
    "text": "unit because I think that the backgrounds for you all are very different. I think some of you are in like your fourth AI course and have done lots of projects and some of you are basically new",
    "start": "635290",
    "end": "644740"
  },
  {
    "text": "to the field and this is a way for us to kind of get everybody well onto a solid foundation.",
    "start": "644740",
    "end": "651460"
  },
  {
    "text": "But, like kind of officially none of this is required. You should just do what you can.",
    "start": "651460",
    "end": "658089"
  },
  {
    "text": "We're making a lot of materials available for you to get, to build up that foundation.",
    "start": "658090",
    "end": "663740"
  },
  {
    "text": "Anything else before I begin? Then the slides for the,",
    "start": "664410",
    "end": "671125"
  },
  {
    "text": "for today are posted here, and they should carry us through all the way to next week as I said.",
    "start": "671125",
    "end": "677050"
  },
  {
    "text": "The reason you might want to download them is, that I am going to encourage you at various points to do a little of,",
    "start": "677050",
    "end": "682380"
  },
  {
    "text": "to do a little coding on your own. So if you do have the data distribution and you do have all the code set up, then I would encourage you to like open a terminal or a notebook if you",
    "start": "682380",
    "end": "691660"
  },
  {
    "text": "want and paste in code, and then maybe play around with the code. Okay? Just because you know,",
    "start": "691660",
    "end": "698529"
  },
  {
    "text": "it might lead to new questions or give you a better feel for what I'm talking about. And I hope you do a lot of that as part of solving the homework problem one.",
    "start": "698530",
    "end": "707394"
  },
  {
    "text": "And then just generally thinking about these issues. Alright, so but let's begin with this content.",
    "start": "707395",
    "end": "715410"
  },
  {
    "text": "So this is the unit on distributed word representations. This is a big slideshow,",
    "start": "715410",
    "end": "720510"
  },
  {
    "text": "but you can navigate it by clicking around up here, if you have it locally and it's pretty intuitive. This is our plot.",
    "start": "720510",
    "end": "726450"
  },
  {
    "text": "We're gonna talk about matrix designs, comparison, re-weighting, do a little subword modeling and then move on to",
    "start": "726450",
    "end": "733630"
  },
  {
    "text": "dimensionality reduction which here is going to encompass LSA, GloVe and Word2Vec.",
    "start": "733630",
    "end": "738865"
  },
  {
    "text": "To give you a sense for kind of the things people are doing in that space. And then the final unit is on retrofitting where you might go beyond",
    "start": "738865",
    "end": "746050"
  },
  {
    "text": "just co-occurrence and bring in some more structured information into these representations.",
    "start": "746050",
    "end": "751240"
  },
  {
    "text": "So I hope that makes it easier to navigate with what is a very large slideshow. Yeah, that's the kind of the plan here and you can see",
    "start": "751240",
    "end": "758680"
  },
  {
    "text": "it reflected in that plot on the top.",
    "start": "758680",
    "end": "761690"
  },
  {
    "text": "Here's the big idea that you need to get used to if you haven't already.",
    "start": "763830",
    "end": "769450"
  },
  {
    "text": "And I think this idea is like, incredible and subtle and interesting. So you can always just continue to meditate on how and why it works.",
    "start": "769450",
    "end": "777459"
  },
  {
    "text": "But the core idea is that if I gathered together into a matrix like this,",
    "start": "777460",
    "end": "783085"
  },
  {
    "text": "a whole bunch of co-occurrence information about words from large collections of text,",
    "start": "783085",
    "end": "788890"
  },
  {
    "text": "then the resulting representations, we're going to think of them as rows here in these matrices.",
    "start": "788890",
    "end": "793915"
  },
  {
    "text": "Those representations latently contain lots of important information about linguistic meaning.",
    "start": "793915",
    "end": "799960"
  },
  {
    "text": "[NOISE] Some of you might ha- already be familiar with that idea,",
    "start": "799960",
    "end": "805270"
  },
  {
    "text": "for some of you that might sound really strange. I can still remember it sounding very strange to me and I'm gonna try to make it seem less strange to you.",
    "start": "805270",
    "end": "812860"
  },
  {
    "text": "But yeah, this is a co-occurrence matrix. This is like our starting point for the whole lecture, right?",
    "start": "812860",
    "end": "818214"
  },
  {
    "text": "It's a word by word matrix in the sense that it's just counting the cell values here, or the number of times that those words occur with each other.",
    "start": "818215",
    "end": "826735"
  },
  {
    "text": "And the- my pitch to you is that latent in there is a whole bunch of really interesting semantic information.",
    "start": "826735",
    "end": "832765"
  },
  {
    "text": "And just one other way to plant this idea, here's a kinda little thought experiment for you.",
    "start": "832765",
    "end": "839380"
  },
  {
    "text": "I've called this a hopeless learning experiment. On the left, I have a little labeled data set of",
    "start": "839380",
    "end": "845290"
  },
  {
    "text": "words and you can kind of see that 0 for that class value means a negative word in terms",
    "start": "845290",
    "end": "850839"
  },
  {
    "text": "of sentiment and a 1 means positive sentiment. And that seems perfectly sensible. [NOISE] If you were to try to generalize that information",
    "start": "850840",
    "end": "858819"
  },
  {
    "text": "to this data set over here where I haven't told you what the words are, of course, this is completely hopeless.",
    "start": "858819",
    "end": "864820"
  },
  {
    "text": "No matter what model you have, there's just no way that you're gonna be able to make predictions about these anonymous new cases.",
    "start": "864820",
    "end": "871690"
  },
  {
    "text": "Right? Compare that with this much more promising learning scenario.",
    "start": "871690",
    "end": "877000"
  },
  {
    "text": "So over on the left, I have that same labeled data set. Except what I've done is represent each word just with some numbers",
    "start": "877000",
    "end": "884740"
  },
  {
    "text": "that represent the association of that word with two other words, excellent and terrible.",
    "start": "884740",
    "end": "889810"
  },
  {
    "text": "And the kind of plot for this course is, I might have started with the co-occurrence counts and had done a bunch of",
    "start": "889810",
    "end": "895210"
  },
  {
    "text": "massaging and stuff and gotten down to more interesting values. And having done that,",
    "start": "895210",
    "end": "901395"
  },
  {
    "text": "it doesn't really matter what your learning model is. You can just intuit that like hey,",
    "start": "901395",
    "end": "906660"
  },
  {
    "text": "if a word has 0 class, it likely has a high, terrible value, right?",
    "start": "906660",
    "end": "912845"
  },
  {
    "text": "Those are all these values here, and a highly negative excellent value.",
    "start": "912845",
    "end": "918200"
  },
  {
    "text": "And then when I switch to the positive ones, it's the reverse, right. They tend to be strong positive and negative associati- association with terrible.",
    "start": "918200",
    "end": "927005"
  },
  {
    "text": "And once you see that and you can easily imagine that a machine-learning model would see this more or less instantly, then when I give you this generalization problem,",
    "start": "927005",
    "end": "934805"
  },
  {
    "text": "it's trivial to solve, right, and I think that's one of the ways in which you can start",
    "start": "934805",
    "end": "940220"
  },
  {
    "text": "to see that latent in that co-occurrence information. Especially if I do some work, there might be a lot of important information about meaning.",
    "start": "940220",
    "end": "948270"
  },
  {
    "text": "Yeah, so these are high level goals. Let's start, let's start thinking in a deep way about how",
    "start": "949650",
    "end": "954890"
  },
  {
    "text": "these vectors actually encode the semantic information. We're also going to build up some foundational concepts",
    "start": "954890",
    "end": "960860"
  },
  {
    "text": "that will be with us throughout this course. And that's gonna be a useful foundation for the deep learning models that we develop.",
    "start": "960860",
    "end": "967805"
  },
  {
    "text": "And also for you, if you're maybe just thinking about word meanings, I think this is a great tool kit for that.",
    "start": "967805",
    "end": "973905"
  },
  {
    "text": "And then down the line what, what you might end up doing with the stuff that I introduce now is, kind of thinking about how these representations are interesting for linguistic problems,",
    "start": "973905",
    "end": "982930"
  },
  {
    "text": "or social problems, or whatever you decide to tackle, or just feeding them into other models.",
    "start": "982930",
    "end": "988400"
  },
  {
    "text": "Because they are a great starting point for the reason that I just showed you. They end up encoding a whole lot of semantic information and your model starts in a really interesting place.",
    "start": "988400",
    "end": "997070"
  },
  {
    "text": "[NOISE] Here are the associated materials which have kind of accumulated.",
    "start": "997070",
    "end": "1003750"
  },
  {
    "text": "There are three notebooks, and they're kind of backed by this code module that I'm gonna encourage you to play around with,",
    "start": "1003750",
    "end": "1009180"
  },
  {
    "text": "uh, as I talk. I'm gonna try to leave some time today to talk about homework 1 and bake-off",
    "start": "1009180",
    "end": "1015330"
  },
  {
    "text": "1 to kind of reinforce the connections and give you a feel for how that will work. And as I mentioned before, there are a bunch of screencasts for this.",
    "start": "1015330",
    "end": "1022800"
  },
  {
    "text": "And I would think these are the core readings. We've posted some other ones, but Turney and Pantel is a great kind of",
    "start": "1022800",
    "end": "1028365"
  },
  {
    "text": "compendium of ideas and insights about these vector space models. Um, Smith is a newer paper that's kind of",
    "start": "1028365",
    "end": "1035279"
  },
  {
    "text": "an informal introduction along the lines that I just gave you, like why does any of this work? Uh, and then Pennington et al is the GloVe paper, we'll probably talk about GloVe next week.",
    "start": "1035280",
    "end": "1045495"
  },
  {
    "text": "And then Faruqui et al is a paper on retrofitting which I think is a really inspiring idea. So I don't know,",
    "start": "1045495",
    "end": "1052710"
  },
  {
    "text": "experience all of this through this week and next week.",
    "start": "1052710",
    "end": "1057309"
  },
  {
    "text": "Some guiding hypotheses, I guess I just feel obligated as somebody giving this lecture to give this Firth quote at the top.",
    "start": "1058820",
    "end": "1065550"
  },
  {
    "text": "How many people have heard that before? It's a kind of canard by now. You shall know a word by the company it keeps.",
    "start": "1065550",
    "end": "1071820"
  },
  {
    "text": "Uh, Firth has lots of quotes like this. The complete meaning of a word is always contextual,",
    "start": "1071820",
    "end": "1076845"
  },
  {
    "text": "and no study of meaning apart from context can be taken seriously. I also have a quote from Wittgenstein,",
    "start": "1076845",
    "end": "1083159"
  },
  {
    "text": "the meaning of a word is its use in the language, who knows really what Wittgenstein meant but it sounds really interesting [LAUGHTER].",
    "start": "1083159",
    "end": "1090090"
  },
  {
    "text": "I also have a quote from Zellig Harris. He was an American structural linguist, uh, strongly believed in this distributional hypothesis.",
    "start": "1090090",
    "end": "1097320"
  },
  {
    "text": "Distributional statements can cover all of the material of a language without requiring support from other types of information.",
    "start": "1097320",
    "end": "1103875"
  },
  {
    "text": "That quotation and actually also these Firth ones, just as an aside. These are kind of interesting because these linguists, um,",
    "start": "1103875",
    "end": "1113160"
  },
  {
    "text": "philosophically were associated with this tradition of nominalism which was kind of the view that you could only trust things that were more or less in the physical record.",
    "start": "1113160",
    "end": "1122340"
  },
  {
    "text": "And so the one thing they felt they could trust was the stuff that they would see in corpora which really comes down to just the distributional things that they can observe.",
    "start": "1122340",
    "end": "1130815"
  },
  {
    "text": "So I have no idea what they would make of the models that I'm presenting you today. Uh, I have a feeling that they might be quite at odds",
    "start": "1130815",
    "end": "1138539"
  },
  {
    "text": "with their philosophical position but at least superficially, it sounds like they're on our side. [LAUGHTER] Uh, and then Turney and Pantel really",
    "start": "1138540",
    "end": "1146100"
  },
  {
    "text": "argue- articulate the modern version of this hypothesis. If units of text have similar vectors in a text frequency matrix,",
    "start": "1146100",
    "end": "1153090"
  },
  {
    "text": "they tend to have similar meanings. That's kind of our guiding idea for now.",
    "start": "1153090",
    "end": "1158260"
  },
  {
    "text": "Also by way of sted- setting the stage and gi- giving you a sense for why this lecture has the thought that it does.",
    "start": "1160070",
    "end": "1166320"
  },
  {
    "text": "I just wanted to walk you through this overarching set of ideas here [NOISE]. So if you're gonna build one of these models,",
    "start": "1166320",
    "end": "1173909"
  },
  {
    "text": "the first choice that you might make is what your matrix design is gonna be like. And I'm gonna show you a few but think like,",
    "start": "1173909",
    "end": "1181005"
  },
  {
    "text": "word by document, that would be words along the rows, the columns would be documents,",
    "start": "1181005",
    "end": "1186630"
  },
  {
    "text": "and the cell counts would be the number of times that those words appeared in those documents. So that would be like not too tall,",
    "start": "1186630",
    "end": "1193455"
  },
  {
    "text": "but incredibly wide, if you had a really big corpus. Word by word is the one that I just showed you.",
    "start": "1193455",
    "end": "1199080"
  },
  {
    "text": "Those are the two most familiar designs but you could also have a whole bunch of other notions of matrix, because you can have different notions of context along those columns.",
    "start": "1199080",
    "end": "1207585"
  },
  {
    "text": "And those will give you very different results, right? Like this is a really fundamental issue, uh, for building these representations.",
    "start": "1207585",
    "end": "1215265"
  },
  {
    "text": "But that's not really the first choice that you make because to build one of these matrices, you make lots and lots of decisions about how you'll tokenize, whether you'll annotate,",
    "start": "1215265",
    "end": "1225465"
  },
  {
    "text": "how you'll chunk up units of text, whether you'll do some feature selection for your vocabulary,",
    "start": "1225465",
    "end": "1230700"
  },
  {
    "text": "and on and on, right? So these are all design choices that you have to make.",
    "start": "1230700",
    "end": "1235785"
  },
  {
    "text": "Most feed into the matrix design. And then you might wanna do some re-weighting,",
    "start": "1235785",
    "end": "1240885"
  },
  {
    "text": "because as you'll see today, the raw counts are not so good as representations for meaning.",
    "start": "1240885",
    "end": "1247530"
  },
  {
    "text": "You have to do some massaging as I said. And so the first way that you might massage is by doing a",
    "start": "1247530",
    "end": "1253260"
  },
  {
    "text": "re-weighting to kind of amplify the important things, and diminish the things that aren't so important.",
    "start": "1253260",
    "end": "1258555"
  },
  {
    "text": "And you have lots of options there. Oh and I wanted to say [NOISE] if you're experienced in this space,",
    "start": "1258555",
    "end": "1267330"
  },
  {
    "text": "a kind of interesting thing that emerges from this whole unit is that the star of our show is pointwise mutual information, or PMI.",
    "start": "1267330",
    "end": "1276554"
  },
  {
    "text": "It's incredible how often the insight behind that re-weighting scheme emerges in the models that I'll present.",
    "start": "1276555",
    "end": "1283965"
  },
  {
    "text": "It's like the hero of the story. After re-weighting, you might wanna do",
    "start": "1283965",
    "end": "1289470"
  },
  {
    "text": "some dimensionality reduction and again you have lots of options. Latent semantic analysis is one that I'll show you,",
    "start": "1289470",
    "end": "1295680"
  },
  {
    "text": "but you might have heard of some of these others like latent Dirichlet allocation, or principal components analysis, and on and on.",
    "start": "1295680",
    "end": "1302760"
  },
  {
    "text": "I'm not so concerned about these acronyms, I'm just planting in your heads the idea that if you're building one of these things,",
    "start": "1302760",
    "end": "1308850"
  },
  {
    "text": "you have lots of options. And then finally, you have to decide what vector comparison method you're going to use.",
    "start": "1308850",
    "end": "1315345"
  },
  {
    "text": "That's gonna be like your fundamental notion of what it means to be similar. And if you believe that Turney and Pantel quote,",
    "start": "1315345",
    "end": "1321180"
  },
  {
    "text": "that's pretty central here. And different notions of vector comparison are gonna give you very different notions of what it means to be similar.",
    "start": "1321180",
    "end": "1329740"
  },
  {
    "text": "One problem in this space is that basically, you can choose from any one of these columns and get some kind of model.",
    "start": "1330980",
    "end": "1338820"
  },
  {
    "text": "And there's not a whole lot of guidance in the literature about what's sensible and what isn't. Um, so this is like you're kind of looking at a methodological disaster zone, right?",
    "start": "1338820",
    "end": "1348899"
  },
  {
    "text": "Because I'm not giving you much guidance about what to do, and there's untold number of things that you could try.",
    "start": "1348900",
    "end": "1355680"
  },
  {
    "text": "I will say that in the time that Bill and I have been teaching this course, there have been so- there has been some real progress on this question,",
    "start": "1355680",
    "end": "1362520"
  },
  {
    "text": "I've kind of noted that here. Models like GloVe and Word2Vec which I'll show you later,",
    "start": "1362520",
    "end": "1368040"
  },
  {
    "text": "they kind of attempt to unite or like take care of re-weighting and dimensionality reduction.",
    "start": "1368040",
    "end": "1374175"
  },
  {
    "text": "And some of them even dictate what your matrix design should be like. And that's a kind of like a way of saying that it's one stop shopping, right?",
    "start": "1374175",
    "end": "1381840"
  },
  {
    "text": "If I choose GloVe, then I don't have so many design choices. Or if I choose Word2Vec, I don't have so many different options to explore.",
    "start": "1381840",
    "end": "1389294"
  },
  {
    "text": "And that's interesting you- and actually some of them might even diminish the importance of which comparison metric that you use because of the scaling that they do on the values.",
    "start": "1389295",
    "end": "1398385"
  },
  {
    "text": "So there has been some progress but still you'll find actually the bake-off and Homework 1 are trying to get you to confront this.",
    "start": "1398385",
    "end": "1405315"
  },
  {
    "text": "There's lots of stuff that you can try. [LAUGHTER] Uh, and you might feel like you're doing it kind of blindly. Questions so far?",
    "start": "1405315",
    "end": "1415500"
  },
  {
    "text": "That's kind of my way of introducing this stuff. That's my framework for these lectures.",
    "start": "1415500",
    "end": "1420700"
  },
  {
    "text": "Okay. So let me just show you a few of these matrix designs, um, because already this is important as I said.",
    "start": "1421340",
    "end": "1428310"
  },
  {
    "text": "So here's that word by word one. And I would say that its most salient property is that it's very dense.",
    "start": "1428310",
    "end": "1434505"
  },
  {
    "text": "Um, if you have a large enough corpus, most words will tend to co-occur with most other words,",
    "start": "1434505",
    "end": "1440280"
  },
  {
    "text": "and so you get this relatively few zeros. I mean there aren't a whole lot of zeros here,",
    "start": "1440280",
    "end": "1446040"
  },
  {
    "text": "in a re- in a real matrix, there'll be a substantial number of them. All of these things are pretty sparse, but this will be like the densest of all.",
    "start": "1446040",
    "end": "1452919"
  },
  {
    "text": "Word by document, that's another common thing. Words along the rows, documents across the columns,",
    "start": "1453170",
    "end": "1459450"
  },
  {
    "text": "that will be much sparser of course. And if your documents are short, it will have an incredible number of zeros in it.",
    "start": "1459450",
    "end": "1465600"
  },
  {
    "text": "Very different from the word by word one. Sorry, someone, yeah. Yeah, just a small question.",
    "start": "1465600",
    "end": "1470745"
  },
  {
    "text": "Like in the last word to word matrix, So when we say, there's a column against and row",
    "start": "1470745",
    "end": "1476549"
  },
  {
    "text": "against that would mean the count up against in all documents that are here? Yeah or some multiplying version thereof depending on how you built the matrix.",
    "start": "1476550",
    "end": "1484769"
  },
  {
    "text": "But yeah, you could think of these- the diagonal as giving just the token. [NOISE] Okay. Yeah. [NOISE]",
    "start": "1484770",
    "end": "1494085"
  },
  {
    "text": "Right. This one is kind of nice because [NOISE] it's not gonna grow very rapidly even as you introduce more data.",
    "start": "1494085",
    "end": "1500970"
  },
  {
    "text": "The worst-case scenario here is like large vocab by large vocab. So like 100,000 by 100,000,",
    "start": "1500970",
    "end": "1507420"
  },
  {
    "text": "but you might have underlying that a billion documents, right? Whereas this one is gonna get big very fast.",
    "start": "1507420",
    "end": "1514139"
  },
  {
    "text": "On the other hand, this one will be sparse. So you might have some interesting ways that you could represent it in a way",
    "start": "1514140",
    "end": "1519360"
  },
  {
    "text": "that actually gives you some gains. Yeah. [inaudible] they're like neighbors or would it be,",
    "start": "1519360",
    "end": "1527669"
  },
  {
    "text": "based on like [NOISE] neighbors based on like arbitrary values? It's a, uh, that's a great question.",
    "start": "1527670",
    "end": "1533360"
  },
  {
    "text": "I wanna return to that [OVERLAPPING] because it's another one of these design choices. What you mean when you say co-occur? Yeah. Uh, let me just show you",
    "start": "1533360",
    "end": "1539570"
  },
  {
    "text": "a few more matrices [OVERLAPPING] and then we'll, we'll get to that. Uh, here, I just wanted to give you a sense for",
    "start": "1539570",
    "end": "1545220"
  },
  {
    "text": "creative thinking around this, this issue. So this is a word by discourse context matrix. In the Switchboard Dialog Act Corpus,",
    "start": "1545220",
    "end": "1551790"
  },
  {
    "text": "you have acts of dialogue and they've been labeled with things like, this is a question, or this is an introduct- interjection,",
    "start": "1551790",
    "end": "1558330"
  },
  {
    "text": "or this is a back channel. And there are, those are all these funny symbols here. And so I could create a matrix of words by",
    "start": "1558330",
    "end": "1565680"
  },
  {
    "text": "dialog acts and that would give me a very interesting perspective on the usage patterns for words,",
    "start": "1565680",
    "end": "1570929"
  },
  {
    "text": "very different from what we saw before because the notion of context is so different.",
    "start": "1570930",
    "end": "1576250"
  },
  {
    "text": "And here's another one that's kind of unusual and this one is less distributional. So this one is, uh,",
    "start": "1577970",
    "end": "1584715"
  },
  {
    "text": "the linguists will like this one, I hope. This is phonological segments along the rows.",
    "start": "1584715",
    "end": "1589815"
  },
  {
    "text": "Those are symbols from the international phonetic alphabet. And then they have their feature representations along the columns.",
    "start": "1589815",
    "end": "1596235"
  },
  {
    "text": "Like, linguists do things like saying, this segment is plus voicing or minus voicing.",
    "start": "1596235",
    "end": "1601590"
  },
  {
    "text": "And so that would be a column there and you can, and they have lots of these. I guess there's ah, uh, 28 different dimensions that this fo- font,",
    "start": "1601590",
    "end": "1609164"
  },
  {
    "text": "uh, this linguist has measured. So it's not distributional, it's more like, uh, a feature representation.",
    "start": "1609165",
    "end": "1616470"
  },
  {
    "text": "And here's a picture of it visualized and you can see that it's really good. Even if you don't know this alphabet,",
    "start": "1616470",
    "end": "1621540"
  },
  {
    "text": "you can kind of see like all these nasal N type things are up here. [NOISE] and [NOISE] differ only by their voicing and they've been clustered together,",
    "start": "1621540",
    "end": "1630270"
  },
  {
    "text": "but otherwise, they're very similar. Here are all these rhotic sounds and, and so forth. And that's a nice transitional idea,",
    "start": "1630270",
    "end": "1637340"
  },
  {
    "text": "what I'm showing you here because what I showed you before is all about co-occurrence in large corpora. This is more like a careful analysis of these phonological segments.",
    "start": "1637340",
    "end": "1646270"
  },
  {
    "text": "And that's a nice reminder that when you think about these vector space models, you're actually doing something that's pervasive in all of science,",
    "start": "1646270",
    "end": "1654450"
  },
  {
    "text": "which is taking natural objects and representing them with a handful of features that you could measure.",
    "start": "1654450",
    "end": "1661530"
  },
  {
    "text": "I've given some other examples here. So I might just as, um, somebody doing more hand-crafted work say,",
    "start": "1661530",
    "end": "1667350"
  },
  {
    "text": "the movie was horrible. Is this vector representation? Which is just counting some abstract properties of that phrase.",
    "start": "1667350",
    "end": "1675195"
  },
  {
    "text": "Um, or this is even clear, like, you might model a human being like a subject in your experiment as a vector 24,",
    "start": "1675195",
    "end": "1682395"
  },
  {
    "text": "140, 5, and 12. And that's just a reminder that you might know that",
    "start": "1682395",
    "end": "1687929"
  },
  {
    "text": "the first is an age and the second is a weight, but it really has that meaning only because you're embedding it in",
    "start": "1687930",
    "end": "1694049"
  },
  {
    "text": "a larger dataset where the first column is the age and the second one is the weight.",
    "start": "1694050",
    "end": "1699600"
  },
  {
    "text": "And that's the sense in which those numbers have any meaning at all. And it's exactly the same when we think about these co-occurrence matrices.",
    "start": "1699600",
    "end": "1708015"
  },
  {
    "text": "The vector representations acquire their meaning because they're embedded in a larger matrix,",
    "start": "1708015",
    "end": "1713340"
  },
  {
    "text": "and it's essentially about those comparisons along the column that you get meaning coming out of the dimensions.",
    "start": "1713340",
    "end": "1719565"
  },
  {
    "text": "So the, the really special thing about the models that we're exploring is not that they're vectors,",
    "start": "1719565",
    "end": "1726465"
  },
  {
    "text": "but rather that the vectors are coming from co-occurrence counts. Maybe that's the idea that you really need to get used to,",
    "start": "1726465",
    "end": "1732570"
  },
  {
    "text": "which is that somehow just from seeing all those associations and enough text, you can extract what we think of as a semantic meaning.",
    "start": "1732570",
    "end": "1740950"
  },
  {
    "text": "And here, I've listed just a bunch of other designs by no mean- means exhaustive,",
    "start": "1742100",
    "end": "1747510"
  },
  {
    "text": "but some of them are kind of unusual. Uh, and again, I'm just trying to hammer home the point that this is a big deal.",
    "start": "1747510",
    "end": "1754424"
  },
  {
    "text": "Like, again, if you're thinking of building a social sciences project on top of these representations or you're",
    "start": "1754425",
    "end": "1760529"
  },
  {
    "text": "thinking about developing machine learning models, this is your first choice and it's going to have",
    "start": "1760530",
    "end": "1765929"
  },
  {
    "text": "really large impacts on the subsequent results. Uh, and there's lots of options that you can pick from.",
    "start": "1765930",
    "end": "1772020"
  },
  {
    "text": "[NOISE] Let's get to that. The final thing I wanna address just for this unit",
    "start": "1772020",
    "end": "1778845"
  },
  {
    "text": "is the question that you raised before which is what we think of, what we mean when we say co-occurrence.",
    "start": "1778845",
    "end": "1783930"
  },
  {
    "text": "[NOISE] And I've kind of broken it down into two things, window and scaling. So just by way of making this concrete, here,",
    "start": "1783930",
    "end": "1791640"
  },
  {
    "text": "I have part of the first sentence of the novel Finnegans Wake, which I chose because the first sentence is also the last sentence.",
    "start": "1791640",
    "end": "1798990"
  },
  {
    "text": "And so who knows where that novel begins and ends. That's kind of cool for co-occurrence. [NOISE] But just some abstract text.",
    "start": "1798990",
    "end": "1805370"
  },
  {
    "text": "And what I've done is picked \"to\"  as our focal word. [NOISE] In fact, we wanna process the whole text. So they're all going to be focal words at some point,",
    "start": "1805370",
    "end": "1812390"
  },
  {
    "text": "but it, let's just say we've, we've homed in on \"to\" and I've numbered going out from there.",
    "start": "1812390",
    "end": "1817790"
  },
  {
    "text": "[NOISE] Let's suppose that our window is 3. We could impose a hard window,",
    "start": "1817790",
    "end": "1824240"
  },
  {
    "text": "[NOISE] and what we would be saying at that point is that to co-occur is to just be in that window. [NOISE] And obviously I could pick different window sizes from 1 to,",
    "start": "1824240",
    "end": "1834975"
  },
  {
    "text": "it could be as high a number as you want it, right? So that's a design choice. [NOISE] And then another choice",
    "start": "1834975",
    "end": "1841860"
  },
  {
    "text": "that you might make is how to scale those co-occurrence counts. So you know that you have co-occurrence within the window.",
    "start": "1841860",
    "end": "1847980"
  },
  {
    "text": "[NOISE] If I did [NOISE] what I call flat scaling here, then I would be saying that each one of these is a 1,",
    "start": "1847980",
    "end": "1853470"
  },
  {
    "text": "standing for one co-occurrence instance. Maybe that's the default idea.",
    "start": "1853470",
    "end": "1858660"
  },
  {
    "text": "[NOISE] But I could also do something more interesting, which is to scale those values. [NOISE] So I might say that to be closer is to be",
    "start": "1858660",
    "end": "1865679"
  },
  {
    "text": "more meaningful in terms of co-occurrence, and as I get farther out, those numbers should diminish because they're less strongly associated.",
    "start": "1865680",
    "end": "1873840"
  },
  {
    "text": "And this is a standard thing. Like, 1 over n, where n is the position from the focal element,",
    "start": "1873840",
    "end": "1879164"
  },
  {
    "text": "that would give you this like really rapid drop-off in what it meant to co-occur. [NOISE] And you can imagine mixing and matching these ideas, right?",
    "start": "1879165",
    "end": "1890100"
  },
  {
    "text": "If you set a really large window but you have a scaling property like this, at a certain point, it's gonna be like you're not co-occurring even if you're in the window,",
    "start": "1890100",
    "end": "1898335"
  },
  {
    "text": "whereas with flat scaling, everything in the window counts equally. [NOISE] Yeah.",
    "start": "1898335",
    "end": "1904605"
  },
  {
    "text": "Do you think what matters is what, what, like right now shore and bend have the same weight regardless.",
    "start": "1904605",
    "end": "1910754"
  },
  {
    "text": "Do you think that's positional as well? Like if it's after 4? It's a great idea, I hope everyone heard that.",
    "start": "1910755",
    "end": "1916680"
  },
  {
    "text": "The question was, could it be asymmetric? That is, could I scale it differently right and left, right? And I think the answer is yes and it might be reflecting",
    "start": "1916680",
    "end": "1923760"
  },
  {
    "text": "a linguistic intu- intuition that you have that maybe things that come after have a different, different weight, different significance, yeah.",
    "start": "1923760",
    "end": "1930360"
  },
  {
    "text": "And the proof would be downstream in how well your representations performed. Yeah. And you could have it scaled at different, different, um, strengths.",
    "start": "1930360",
    "end": "1940830"
  },
  {
    "text": "[NOISE] Yeah, I have a couple of generalizations to offer here. Larger flatter windows will capture",
    "start": "1940830",
    "end": "1947490"
  },
  {
    "text": "more semantic information because it's kind of like what you're doing there is just saying,",
    "start": "1947490",
    "end": "1953580"
  },
  {
    "text": "hey, you're in the same topical space as me. Whereas if you pick small more scaled windows,",
    "start": "1953580",
    "end": "1961378"
  },
  {
    "text": "it's going to be much more syntactic and collocational information because you're saying like, what I really care about is that you're in this local linguistic environment.",
    "start": "1961379",
    "end": "1969674"
  },
  {
    "text": "And so those matrices will en- encode a lot of idiomatic information and a lot of stuff that's just like,",
    "start": "1969675",
    "end": "1974760"
  },
  {
    "text": "hey, this adjective tends to modify this noun, which will be a very different picture than the first one where you'll",
    "start": "1974760",
    "end": "1980580"
  },
  {
    "text": "get almost none of that information because you were saying, hey, just being in this document with me is enough to count as a co-occurrence.",
    "start": "1980580",
    "end": "1988320"
  },
  {
    "text": "The other thing I want to say is, even if you've settled these questions, you probably still have some design choices.",
    "start": "1988320",
    "end": "1993930"
  },
  {
    "text": "So you're gonna have to impose some textual boundaries. Like, you might say that sentences,",
    "start": "1993930",
    "end": "2000110"
  },
  {
    "text": "or paragraphs, or documents, or collections of documents are your basic unit, and that will interact with what you choose for the window, right?",
    "start": "2000110",
    "end": "2008150"
  },
  {
    "text": "So for example, if I was focused on swerve here, then as I go left,",
    "start": "2008150",
    "end": "2013220"
  },
  {
    "text": "I just get a very narrow window. [NOISE] But if I decided that my textual boundary wasn't that from but rather like,",
    "start": "2013220",
    "end": "2019730"
  },
  {
    "text": "stretched back into the other part of the novel, um, then I would get different values for co-occurrence.",
    "start": "2019730",
    "end": "2026120"
  },
  {
    "text": "[NOISE] And I'm going to try to show you that all this matters.",
    "start": "2026120",
    "end": "2032110"
  },
  {
    "text": "And, and one way that you could start to get a feel for how it matters is to do a little bit of coding as I talk from now on.",
    "start": "2032110",
    "end": "2037900"
  },
  {
    "text": "So I included these code snippets. Actually, I think I wanna run them. You should be able to paste them out,",
    "start": "2037900",
    "end": "2043750"
  },
  {
    "text": "[NOISE] copy and paste them out [NOISE].",
    "start": "2043750",
    "end": "2058815"
  },
  {
    "text": "There we go. And it'll take a minute to load. While it loads, let me just say that he- here's the way I'm- I'm",
    "start": "2058816",
    "end": "2065690"
  },
  {
    "text": "trying to give you exposure to a bunch of different designs. So what I have is two fundamental datasets;",
    "start": "2065690",
    "end": "2072560"
  },
  {
    "text": "IMDB movie reviews and Gigaword. So IMDB, that's like user-supplied reviews of movies and Gigaword is newswire text.",
    "start": "2072560",
    "end": "2082294"
  },
  {
    "text": "So already, these are gonna be very different in terms of their semantic content, and you expect that to be reflected in the- the representations that we develop.",
    "start": "2082295",
    "end": "2092120"
  },
  {
    "text": "What I've also done is try to pick two extremes. So for each one, you have one version where the window size is",
    "start": "2092120",
    "end": "2099190"
  },
  {
    "text": "5, and the scaling is 1 over n. By what I said before, that should be a lot of like collocational syntactic information.",
    "start": "2099190",
    "end": "2107065"
  },
  {
    "text": "And you also have one where the window size is 20. So that's really big, and it's- the scaling is flat.",
    "start": "2107065",
    "end": "2113600"
  },
  {
    "text": "And if I'm right about these generalizations, then you would expect that that's going to be a lot of kind of topical information.",
    "start": "2113600",
    "end": "2119090"
  },
  {
    "text": "Uh, you have that for Gigaword and IMDB.",
    "start": "2119090",
    "end": "2123540"
  },
  {
    "text": "Uh, I should say that all of these objects are Pandas DataFrames.",
    "start": "2124630",
    "end": "2131299"
  },
  {
    "text": "Um, in my experience, Pandas is great, it's fast, it's really flexible, but it does have a steep learning curve.",
    "start": "2131300",
    "end": "2137445"
  },
  {
    "text": "I've tried in the notebooks to show you kinda how to interact with these dataframes. If you panic for an object,",
    "start": "2137445",
    "end": "2144190"
  },
  {
    "text": "just two dot values and you'll be- have a NumPy array, and maybe it will calm you down and you'll be able to interact with it more fruitfully.",
    "start": "2144190",
    "end": "2151840"
  },
  {
    "text": "But like for example, I could do this, um, and that shows me a series.",
    "start": "2151840",
    "end": "2159500"
  },
  {
    "text": "What that's doing is showing me in series form the entire row for the word happy, uh, and that's a bit weird.",
    "start": "2159500",
    "end": "2165710"
  },
  {
    "text": "So you might say, .values, and then you get, you know, the- the actual representation in a more digestible format.",
    "start": "2165710",
    "end": "2174420"
  },
  {
    "text": "Um, all of these matrices have- they're all word by word and they're all just 5,000 by 5,000.",
    "start": "2175720",
    "end": "2182810"
  },
  {
    "text": "So that's a pretty small vocabulary. I want you to be able to work with them pretty quickly, and we can't have you loading massive objects into memory.",
    "start": "2182810",
    "end": "2189935"
  },
  {
    "text": "Um, but I think it's a pretty interesting vocabulary as you'll see. So they- they are meaningful spaces to explore. All right.",
    "start": "2189935",
    "end": "2200900"
  },
  {
    "text": "Any questions so far? Okay. So another major unit here is vector comparison.",
    "start": "2200900",
    "end": "2210569"
  },
  {
    "text": "You've got your design sorted out. You- you've got a bunch of counts.",
    "start": "2211210",
    "end": "2216935"
  },
  {
    "text": "Let's say for now that it's word by word, and you want to think about that core hypothesis, that similarity in the space is gonna correspond to similarity of meaning.",
    "start": "2216935",
    "end": "2226099"
  },
  {
    "text": "What does it mean to be similar? That will turn on what vector comparison method that you choose.",
    "start": "2226100",
    "end": "2232220"
  },
  {
    "text": "I'm gonna use this as a running example. This is also the one that- that appears in the screencasts.",
    "start": "2232220",
    "end": "2238520"
  },
  {
    "text": "Um, there it is as a tiny little word by document matrix over there. It could be word by word as well.",
    "start": "2238520",
    "end": "2243710"
  },
  {
    "text": "Uh, and I've also plotted it. So here's A down at (2, 4). B and C kind of keyed into document x and document y,",
    "start": "2243710",
    "end": "2253250"
  },
  {
    "text": "and the conceit of this example is that two things are happening with those counts. So first, A and B are similar in the sense that if you mentally kind of rescale them,",
    "start": "2253250",
    "end": "2266630"
  },
  {
    "text": "then you can see that there's more y than x, right? The- the numbers are larger for y than for x.",
    "start": "2266630",
    "end": "2273860"
  },
  {
    "text": "So that's the- that's the sense in which A and B are similar. B and C are similar in the sense that their overall magnitudes intuitively are similar.",
    "start": "2273860",
    "end": "2282350"
  },
  {
    "text": "They both have pretty large counts, whereas A is pretty small. And you can see that here in the way they've been plotted,",
    "start": "2282350",
    "end": "2289099"
  },
  {
    "text": "like A is kind of lonely down in this corner, and B and C are up here pretty close together. Make sense?",
    "start": "2289100",
    "end": "2297630"
  },
  {
    "text": "First distance measure, Euclidean distance. This is the standard metric that you might think of if you looked at that example.",
    "start": "2298960",
    "end": "2307130"
  },
  {
    "text": "It's measuring just the shortest point between A and B in the plane here for two dimensions.",
    "start": "2307130",
    "end": "2313295"
  },
  {
    "text": "So you get this dimension here and this dimension here. And it's the sum of the squares of all these differences,",
    "start": "2313295",
    "end": "2319835"
  },
  {
    "text": "and then you take the square root of it to rescale it again. And it's just reflecting what you might think from the picture,",
    "start": "2319835",
    "end": "2328460"
  },
  {
    "text": "which is that in Euclidean distance, A and B are very far apart,",
    "start": "2328460",
    "end": "2333650"
  },
  {
    "text": "and B and C are very close together. So it's already if- you- I mean,",
    "start": "2333650",
    "end": "2340760"
  },
  {
    "text": "the heart of this is that Euclidean distance is favoring the magnitude over",
    "start": "2340760",
    "end": "2346075"
  },
  {
    "text": "that more abstract notion that I pitched to you before about A and B kinda being proportionally similar.",
    "start": "2346075",
    "end": "2352660"
  },
  {
    "text": "And that might make you kind of unhappy as a linguist because I contend that,",
    "start": "2352660",
    "end": "2359164"
  },
  {
    "text": "abstractly speaking, A and B are kind of like, for example, superb and good.",
    "start": "2359165",
    "end": "2364970"
  },
  {
    "text": "Both are positive, but superb is vastly less frequent than good,",
    "start": "2364970",
    "end": "2370040"
  },
  {
    "text": "whereas B and C are kind of like good and bad. They're very different in terms of their polarity,",
    "start": "2370040",
    "end": "2375680"
  },
  {
    "text": "but in terms of their raw token counts, they're very similar. And I think this is borne out if you look at these matrices,",
    "start": "2375680",
    "end": "2382669"
  },
  {
    "text": "that just kind of having the same frequency is going to give you very similar Euclidean distance,",
    "start": "2382669",
    "end": "2388175"
  },
  {
    "text": "and that might be at odds with what you want from semantics. I think frequency is correlated with meaning.",
    "start": "2388175",
    "end": "2394100"
  },
  {
    "text": "I think it's not an accident that superb, and terrible, are infrequent, and good and bad are frequent.",
    "start": "2394100",
    "end": "2399560"
  },
  {
    "text": "But I probably primarily care about that polarity, that sentiment information, and so",
    "start": "2399560",
    "end": "2405619"
  },
  {
    "text": "Euclidean distance looks like it's not my choice if that's my objective.",
    "start": "2405620",
    "end": "2410400"
  },
  {
    "text": "But let's work on that a little bit. So once- one way you could work on that is by doing length normalization of these vectors.",
    "start": "2412060",
    "end": "2419675"
  },
  {
    "text": "And the way that works is you first calculate this quantity for the vector that I'm gonna call the L2-length.",
    "start": "2419675",
    "end": "2425809"
  },
  {
    "text": "There's lots of names for it. It's like Euclidean, um, norm, and if you look on Wikipedia,",
    "start": "2425810",
    "end": "2431210"
  },
  {
    "text": "it's a- a whole bunch of names. But there's the calculation there. It's the sum of the squares of all those values and then you take the square root.",
    "start": "2431210",
    "end": "2438455"
  },
  {
    "text": "And what you do to length normalize the vector is defied- divide each element in the vector by that normalization quantity,",
    "start": "2438455",
    "end": "2446150"
  },
  {
    "text": "and it gives you a new vector down here. Here's a kind of look at how that happens.",
    "start": "2446150",
    "end": "2452630"
  },
  {
    "text": "So if I start with that original matrix, I put in this column, the L2-lengths,",
    "start": "2452630",
    "end": "2458915"
  },
  {
    "text": "and then to get the new matrix, you just divide every element by its corresponding row length- L2-length.",
    "start": "2458915",
    "end": "2467089"
  },
  {
    "text": "Here's what happens to the space. So that's the original one on the left. And when I length normalize,",
    "start": "2467090",
    "end": "2473359"
  },
  {
    "text": "they kind of all go up. They're kind of all sitting here in this unit sphere now.",
    "start": "2473360",
    "end": "2478430"
  },
  {
    "text": "And now A and B are really close together, and B and C are far apart.",
    "start": "2478430",
    "end": "2483785"
  },
  {
    "text": "Because what I've done with this L2 norm here, is kind of abstract away from a lot of the information about the overall magnitude,",
    "start": "2483785",
    "end": "2491599"
  },
  {
    "text": "and that has pulled A and B together. And what we're seeing is much more like their proportional comparison.",
    "start": "2491600",
    "end": "2497789"
  },
  {
    "text": "Cosine distance, this is like the workhorse of this, um, this is the distance measure that everybody",
    "start": "2500830",
    "end": "2508310"
  },
  {
    "text": "uses if they don't mention which distance measure they used. And I think there's a good argument for that because what cosine distance is doing is",
    "start": "2508310",
    "end": "2515870"
  },
  {
    "text": "simultaneously a Euclidean-like comparison but in that length-normalized [NOISE] space. And here's the calculation here.",
    "start": "2515870",
    "end": "2522725"
  },
  {
    "text": "The part after the 1 - is cosine similarity,",
    "start": "2522725",
    "end": "2527824"
  },
  {
    "text": "but I've kind of regularized everything about this framework into distances. So I do 1 - that value.",
    "start": "2527824",
    "end": "2534680"
  },
  {
    "text": "The numerator there is the dot product of the two vectors and the denominator is the normalization factor,",
    "start": "2534680",
    "end": "2542330"
  },
  {
    "text": "and it's the product of the two L2 lengths. So this is kind of like I'm doing",
    "start": "2542330",
    "end": "2549530"
  },
  {
    "text": "this dot product comparison but I'm also controlling for the length of the vectors.",
    "start": "2549530",
    "end": "2555230"
  },
  {
    "text": "This is a look at what the calculations are like. A, B, and C are sitting there in their original positions,",
    "start": "2555230",
    "end": "2562250"
  },
  {
    "text": "but when you walk through these calculations, you'll find that, um, A and B are very close together,",
    "start": "2562250",
    "end": "2569210"
  },
  {
    "text": "0.008, and B and C are pretty far apart, 0.065.",
    "start": "2569210",
    "end": "2575000"
  },
  {
    "text": "So cosine in one step is kind of incorporating that length normalization,",
    "start": "2575000",
    "end": "2580160"
  },
  {
    "text": "you can see that there in the calculation and as a result, it's favoring that proportional similarity,",
    "start": "2580160",
    "end": "2585770"
  },
  {
    "text": "that sentiment similarity that I was pitching to you. Another way to think about this is after you first do",
    "start": "2585770",
    "end": "2591500"
  },
  {
    "text": "the L2 norm and then you do cosine distance, you get exactly the same result. I know it's small but I've walked through",
    "start": "2591500",
    "end": "2598190"
  },
  {
    "text": "the calculations and the point is just that you get exactly the same values, whether or not you first do the normalization or not,",
    "start": "2598190",
    "end": "2604850"
  },
  {
    "text": "and that's because of the denominator there. [NOISE] A couple of hands. Yeah. Go ahead.",
    "start": "2604850",
    "end": "2610609"
  },
  {
    "text": "Sorry, you said, you do the, the L2 norm and then you do the Euclidean distance,",
    "start": "2610610",
    "end": "2616250"
  },
  {
    "text": "is it the same as calculating [NOISE] the cosine distance? I do [NOISE] cosine here. Oh, I should have checked this, I meant to.",
    "start": "2616250",
    "end": "2621935"
  },
  {
    "text": "I think that up to ranking, if you first length normalize, then Euclidean distance will give you the same rank.",
    "start": "2621935",
    "end": "2627589"
  },
  {
    "text": "[NOISE] Yeah. So tha- that's the sense in which, like, if you start to massage the space, your distance measure might not matter.",
    "start": "2627590",
    "end": "2634160"
  },
  {
    "text": "Because if I first normalized all of these vectors, Euclidean and cosine are probably gonna be approximately the same,",
    "start": "2634160",
    "end": "2640310"
  },
  {
    "text": "although I should check on the exact relationship. Yeah. Having normalized, now, we're sort",
    "start": "2640310",
    "end": "2646430"
  },
  {
    "text": "of highlighting the relationship between the value of the, the x axis and the y axis, [NOISE] and that I guess to some degree is trying to",
    "start": "2646430",
    "end": "2652430"
  },
  {
    "text": "give us the relationship between the two documents. Is that the part that highlights the actual value of the sentiments?",
    "start": "2652430",
    "end": "2657695"
  },
  {
    "text": "Or where is this [NOISE] sentiment data actually coming from as well. Well, I was sort of making up the sentiment part.",
    "start": "2657695",
    "end": "2663920"
  },
  {
    "text": "That's my just so story about these things and I do think it's realistic. I've focused on the,",
    "start": "2663920",
    "end": "2668975"
  },
  {
    "text": "the rows not the documents. So I'm sort of about how A and B are similar, and how B and C are similar,",
    "start": "2668975",
    "end": "2674869"
  },
  {
    "text": "um, not so much the documents. Although I think you could tell a similar story about the documents. Yeah. Go ahead.",
    "start": "2674870",
    "end": "2682130"
  },
  {
    "text": "[inaudible] that ratio like,",
    "start": "2682130",
    "end": "2687259"
  },
  {
    "text": "[NOISE] actually y would be half and then it would be about 2-3. And then, C is the only case where x",
    "start": "2687260",
    "end": "2693545"
  },
  {
    "text": "has the higher value in the ratio than y. Is that what we're highlighting? Yeah. That's kind of what I meant.",
    "start": "2693545",
    "end": "2698555"
  },
  {
    "text": "And um, I-, we can substantiate this in lots of ways, but you just like, uh, articulated the core insight.",
    "start": "2698555",
    "end": "2703910"
  },
  {
    "text": "You had like, B is the only one where the left value is bigger than the right once I adjust for their overall values. Yes.",
    "start": "2703910",
    "end": "2710375"
  },
  {
    "text": "Thank you. [LAUGHTER] And then what I'm showing you here is kind of a bunch of ways that you could cache that out.",
    "start": "2710375",
    "end": "2715430"
  },
  {
    "text": "So one would be the L2 norm, uh, and that does reflect that, right? So um, well, I didn't make it so transparent,",
    "start": "2715430",
    "end": "2722839"
  },
  {
    "text": "but once I've normalized in this way, you can see [NOISE] this one is bigger than this one and those two are kind of very similar in their proportional values.",
    "start": "2722839",
    "end": "2730400"
  },
  {
    "text": "[NOISE] Other questions? [NOISE] That's kind of the heart of it.",
    "start": "2730400",
    "end": "2741155"
  },
  {
    "text": "As I said, cosine is a good default. I wanna show you a few more just because you might see them in the literature.",
    "start": "2741155",
    "end": "2747800"
  },
  {
    "text": "And again, there are some interesting things to say about them. So another popular family [NOISE] of these comparison methods,",
    "start": "2747800",
    "end": "2754835"
  },
  {
    "text": "I've called matching-based methods. So the matching [NOISE] coefficient is the one that I've given at the top there.",
    "start": "2754835",
    "end": "2761675"
  },
  {
    "text": "Um, and you're just summing up all of the smaller of the two values, doing a point-wise comparison across the vectors that you wanna compare.",
    "start": "2761675",
    "end": "2769520"
  },
  {
    "text": "[NOISE] That's the matching coefficient. And then, Jaccard, Dice, and Overlap [NOISE] are all defined in terms of that matching thing.",
    "start": "2769520",
    "end": "2777725"
  },
  {
    "text": "So like, matching, matching, matching in the numerators. And then, you can see that what they're doing is just",
    "start": "2777725",
    "end": "2784190"
  },
  {
    "text": "normalizing the vectors in different ways. [NOISE] But that's it. Yeah.",
    "start": "2784190",
    "end": "2789815"
  },
  {
    "text": "Is there a theory behind why cosine works so well or we just, we just observed is what.",
    "start": "2789815",
    "end": "2795150"
  },
  {
    "text": "I guess we should be careful [NOISE] about what it means to work well because if your goal was to really capture a lot of the,",
    "start": "2795310",
    "end": "2804665"
  },
  {
    "text": "um, raw frequency values, then it would be a poor choice. Um, I think that it works well for NLu problems because",
    "start": "2804665",
    "end": "2813829"
  },
  {
    "text": "primarily we care about what I'm calling the sentiment association. We don't so much care that superb is less frequent than good,",
    "start": "2813830",
    "end": "2821945"
  },
  {
    "text": "we care about how they're alike. And so in doing cosine, you're kind of homing right in on that.",
    "start": "2821945",
    "end": "2828349"
  },
  {
    "text": "[NOISE] But it's worth mentioning that there's a danger to abstracting away from frequency information.",
    "start": "2828350",
    "end": "2834290"
  },
  {
    "text": "And I'm gonna, you'll see that actually as we go, especially through these weighting schemes, that frequency is important in language because it's important,",
    "start": "2834290",
    "end": "2841970"
  },
  {
    "text": "and also because a lot of our datasets have mistakes in them. [LAUGHTER] Uh, so infrequent events,",
    "start": "2841970",
    "end": "2847460"
  },
  {
    "text": "you don't want to amplify them too much. Um, so I don't know, I think it depends on the problem.",
    "start": "2847460",
    "end": "2852650"
  },
  {
    "text": "[NOISE] Yeah. [NOISE] Can you, um, explain a little bit more about the next slide,",
    "start": "2852650",
    "end": "2858140"
  },
  {
    "text": "what the actual matching method case, [NOISE] the very first, um, algorithm.",
    "start": "2858140",
    "end": "2866540"
  },
  {
    "text": "It's the sum of all the smaller of the values. [NOISE] That's sort of the beginning and end of it.",
    "start": "2866540",
    "end": "2872660"
  },
  {
    "text": "What, wha- what's behind your question? I guess, why does that mean anything? Ah, how could I substantiate that?",
    "start": "2872660",
    "end": "2883160"
  },
  {
    "text": "[NOISE] One way to think about this is that, sometimes you see all of these metrics just defined for binary vectors.",
    "start": "2883160",
    "end": "2892775"
  },
  {
    "text": "And then for example, what Dice reduces to is the intersection over the union.",
    "start": "2892775",
    "end": "2898549"
  },
  {
    "text": "And then it's giving you some abstract measure of overlap. And then, you can see that in that context,",
    "start": "2898550",
    "end": "2905180"
  },
  {
    "text": "matching is like intersection. Uh, but when you move to continuous values,",
    "start": "2905180",
    "end": "2912710"
  },
  {
    "text": "you can't just do a straightforward set theoretic intersection, and the closest you can come kind of in continuous space is the smaller of the two.",
    "start": "2912710",
    "end": "2920735"
  },
  {
    "text": "Because that's like, if I have a 0 and a 1, I'm gonna get the 0, uh, and that corresponds to the intersection idea.",
    "start": "2920735",
    "end": "2927380"
  },
  {
    "text": "If I have two 1s, I'll get a 1. And again, just like intersection.",
    "start": "2927380",
    "end": "2932390"
  },
  {
    "text": "That's the best I can do. Is there any other, can anybody else help to substantiate this? What's going on with matching?[LAUGHTER] [NOISE].",
    "start": "2932390",
    "end": "2938270"
  },
  {
    "text": "Maybe a concise way",
    "start": "2938270",
    "end": "2947240"
  },
  {
    "text": "to say it is that any dimension that has a 0, it's gonna contribute nothing to the matching score.",
    "start": "2947240",
    "end": "2954215"
  },
  {
    "text": "And intuitively, there's no match because there's a 0 there. And more broadly,",
    "start": "2954215",
    "end": "2960875"
  },
  {
    "text": "any small number that's close to 0 contributes little to the matching score. That's right. [NOISE] Yeah.",
    "start": "2960875",
    "end": "2970760"
  },
  {
    "text": "Sorry, sorry. Oh, yeah. Um, so in the previous slide, you showed,",
    "start": "2970760",
    "end": "2976474"
  },
  {
    "text": "um, the co- the cosine distance between length normalized vectors. Why did you, uh,",
    "start": "2976475",
    "end": "2981770"
  },
  {
    "text": "why do we try to normalize the length? [NOISE] So why does length, [NOISE] this one shouldn't be, right?",
    "start": "2981770",
    "end": "2987650"
  },
  {
    "text": "Affecting our notion of distance here. [NOISE]",
    "start": "2987650",
    "end": "2993410"
  },
  {
    "text": "Let's see. Let me, let me see if I can unpack that a little bit. So in showing you cosine with and without a first length normalization step,",
    "start": "2993410",
    "end": "3001630"
  },
  {
    "text": "I was just trying to get you to see that cosine is doing that length normalization. That's what its denominator is doing.",
    "start": "3001630",
    "end": "3008740"
  },
  {
    "text": "And that's why it doesn't matter whether I first length normalize or just skip that step because fundamentally,",
    "start": "3008740",
    "end": "3014410"
  },
  {
    "text": "it's building in that re-scaling. [NOISE] That's all I meant there. The broader question of why you're normalizing,",
    "start": "3014410",
    "end": "3021699"
  },
  {
    "text": "which you can see reflected here because I could pick matching and just use that,",
    "start": "3021699",
    "end": "3027520"
  },
  {
    "text": "or I could pick one that has some kind of normalization. That's kind of like picking Euclidean or picking cosine.",
    "start": "3027520",
    "end": "3034420"
  },
  {
    "text": "And this gets back to this deeper issue of, what am I doing when I abstract away from some of",
    "start": "3034420",
    "end": "3039609"
  },
  {
    "text": "the information about the overall frequency in the vector? [NOISE].",
    "start": "3039610",
    "end": "3047935"
  },
  {
    "text": "Uh, oh, yeah. I saw your hand up and then we could go back to- [inaudible] It's okay. Yeah, go ahead. [inaudible]",
    "start": "3047935",
    "end": "3061659"
  },
  {
    "text": "I'm not sure. I probably wouldn't use any of these. You see, these are allowed for- [LAUGHTER]",
    "start": "3061659",
    "end": "3067360"
  },
  {
    "text": "We, we'll, we'll talk about this later. I mean, you see them all over. So presumably people have arguments for why they've chosen them.",
    "start": "3067360",
    "end": "3073210"
  },
  {
    "text": "Um, you do see Dice in the context of binary vectors. And I can kind of see why that is because it's equivalent to the F1 score, um,",
    "start": "3073210",
    "end": "3083545"
  },
  {
    "text": "which is a very deep intuition about how we do kind of, um, basic metrics in NLU.",
    "start": "3083545",
    "end": "3091315"
  },
  {
    "text": "Because it's capturing kind of your, the ability of your model to reproduce maybe some human behavior in an exact way,",
    "start": "3091315",
    "end": "3097840"
  },
  {
    "text": "capturing both, like, all kinds of mistakes essentially. And so the, it doesn't surprise me that you see Dice sometimes,",
    "start": "3097840",
    "end": "3105430"
  },
  {
    "text": "but for the others, I'm just not sure. It might not matter too much. Maybe that's why people just pick one.",
    "start": "3105430",
    "end": "3111670"
  },
  {
    "text": "[NOISE] Let me show you a few more.",
    "start": "3111670",
    "end": "3117280"
  },
  {
    "text": "And then I do have some generalizations that I can offer about how these relate to each other and be a little more precise about that.",
    "start": "3117280",
    "end": "3122690"
  },
  {
    "text": "But just one more family that you, that you'll encounter that are interesting to think about are",
    "start": "3122690",
    "end": "3127710"
  },
  {
    "text": "kind of probability-based comparison methods. One of them is the KL divergence.",
    "start": "3127710",
    "end": "3132810"
  },
  {
    "text": "Um, and this isn't strictly speaking a distance metric because you're comparing kind of the deviation from the reference distribution p to q,",
    "start": "3132810",
    "end": "3143995"
  },
  {
    "text": "and this is the value that you get here. Oh, and I wanted to say this can be a little bit fiddly.",
    "start": "3143995",
    "end": "3152515"
  },
  {
    "text": "If qi there for example is 0, then this whole value becomes undefined,",
    "start": "3152515",
    "end": "3159160"
  },
  {
    "text": "and so you have to do a step for any real-world application of kind of adding some small value to all of these things, so that your calculations don't explode.",
    "start": "3159160",
    "end": "3168174"
  },
  {
    "text": "And that's one way in which KL is kind of fiddly. And the other is just the fact that it's not symmetric.",
    "start": "3168175",
    "end": "3173950"
  },
  {
    "text": "So it matters whether you pick p as your reference distribution or q.",
    "start": "3173950",
    "end": "3179150"
  },
  {
    "text": "Here's an example of it happening. And I have to remind you that because this is a probabilistic notion,",
    "start": "3179820",
    "end": "3186190"
  },
  {
    "text": "you have to first normalize all the vectors, that is, sum up their values and then divide each value by",
    "start": "3186190",
    "end": "3192130"
  },
  {
    "text": "that sum so that you get a proper distribution, which imposes lots of other design considerations.",
    "start": "3192130",
    "end": "3197880"
  },
  {
    "text": "Like for example, this has to be a bunch of, uh, positive values so that the normalization step makes sense.",
    "start": "3197880",
    "end": "3206105"
  },
  {
    "text": "Uh, basically, you have to be thinking I'm gonna use this only if it makes sense for me to think of these values as a probability distribution.",
    "start": "3206105",
    "end": "3214015"
  },
  {
    "text": "Uh, but if you're in that kind of environment, then this might be a good choice. I've given some calculations here, I know they're small.",
    "start": "3214015",
    "end": "3220810"
  },
  {
    "text": "I think the fundamental thing to observe is that like cosine and like length normalization,",
    "start": "3220810",
    "end": "3228055"
  },
  {
    "text": "A and B come out very close together, and B and C far apart, and that's because of the probability step that moves them kind of,",
    "start": "3228055",
    "end": "3237790"
  },
  {
    "text": "abstracts away from their overall magnitude differences. [NOISE] And then, here are the calculations,",
    "start": "3237790",
    "end": "3243730"
  },
  {
    "text": "just to demystify all this notation and so forth, if you want to check that out later. [NOISE] There are a bunch of",
    "start": "3243730",
    "end": "3251470"
  },
  {
    "text": "variants that might be interesting and are a little bit less fiddly. So one of them would be symmetric KL,",
    "start": "3251470",
    "end": "3257590"
  },
  {
    "text": "the sum of the two values. And then it doesn't matter which you pick as your reference distribution, that's kind of nice.",
    "start": "3257590",
    "end": "3263424"
  },
  {
    "text": "KL divergence with skew gives you a parameter that lets you control how much you trust the reference distribution versus the other one.",
    "start": "3263425",
    "end": "3271780"
  },
  {
    "text": "[NOISE] Uh, and you can kind of, like, pull them together. And then Jensen-Shannon distance is",
    "start": "3271780",
    "end": "3277540"
  },
  {
    "text": "a proper distance metric that is defined by this calculation, which uses KL twice.",
    "start": "3277540",
    "end": "3283585"
  },
  {
    "text": "And a nice thing about this is, you don't have to worry about that smoothing or anything. Uh, and it's symmetric, it's a proper distance metric, right?",
    "start": "3283585",
    "end": "3291670"
  },
  {
    "text": "So this will be like less fiddly than the other options, especially KL.",
    "start": "3291670",
    "end": "3296299"
  },
  {
    "text": "And I, I guess my answer to why you might pick one of these is just that maybe you're already dealing with values that you think of as probability values,",
    "start": "3300300",
    "end": "3309100"
  },
  {
    "text": "in which case, these are very natural metrics to choose. [NOISE] Let me sum this up with a few relationships and generalizations.",
    "start": "3309100",
    "end": "3319809"
  },
  {
    "text": "Euclidean, Jaccard, and Dice with raw counts will favor raw frequency over the distributional patterns.",
    "start": "3319810",
    "end": "3327415"
  },
  {
    "text": "That's my generalization there. [NOISE] I showed you to number 2 before. Euclidean with L2 norm vectors is equivalent to cosine with regard to ranking.",
    "start": "3327415",
    "end": "3336400"
  },
  {
    "text": "[NOISE] Oh, that's a firmer answer to your question. I, I thought I knew that from somewhere. I guess I learned it from Manning and Schutze's textbook.",
    "start": "3336400",
    "end": "3342870"
  },
  {
    "text": "[NOISE] Uh, so that's kind of nice, because then once you've normalized your space,",
    "start": "3342870",
    "end": "3347910"
  },
  {
    "text": "it doesn't matter whether you pick Euclidean or cosine. Jaccard and Dice are equivalent with regard to ranking.",
    "start": "3347910",
    "end": "3353670"
  },
  {
    "text": "You can probably see that if you look at their two definitions, they just have slightly different denominators.",
    "start": "3353670",
    "end": "3359175"
  },
  {
    "text": "Both L2-norms and probability distributions can obscure differences in the amount or strength of evidence,",
    "start": "3359175",
    "end": "3365838"
  },
  {
    "text": "which can in turn have an effect on the reliability of cosine, normed-Euclidean, and KL divergence.",
    "start": "3365839",
    "end": "3373140"
  },
  {
    "text": "And then skew, which I showed you briefly is KL but with a step that gives you,",
    "start": "3373140",
    "end": "3378164"
  },
  {
    "text": "lets you give special credence to the reference distribution. Any other questions or comments? [NOISE] Yeah.",
    "start": "3378165",
    "end": "3385120"
  },
  {
    "text": "If you have a sparse dataset that, in, in which like one incorrect co-occurrence",
    "start": "3386040",
    "end": "3392680"
  },
  {
    "text": "might mess with the frequency of the overall dataset, is there a default, uh, [NOISE] like comparison method you should go to use?",
    "start": "3392680",
    "end": "3400569"
  },
  {
    "text": "[NOISE] Again, you're asking a deep question, which is like,",
    "start": "3400570",
    "end": "3406270"
  },
  {
    "text": "what to do with the fact that we know these matrices have some problems. [LAUGHTER] Um, I, I think,",
    "start": "3406270",
    "end": "3414025"
  },
  {
    "text": "let, I have some, a bunch of examples that key right into this issue that you're raising. [NOISE] And I think it's not purely about the, um,",
    "start": "3414025",
    "end": "3421870"
  },
  {
    "text": "comparison method but also about what you're doing to the raw counts. [NOISE] So let me show you some examples and then you",
    "start": "3421870",
    "end": "3428019"
  },
  {
    "text": "could reraise this question if it doesn't resolve them. Um, finally, here, here are some code snippets.",
    "start": "3428020",
    "end": "3435234"
  },
  {
    "text": "Um, it's kinda small I guess, but I hope you can read it locally. So I've loaded this VSM,",
    "start": "3435235",
    "end": "3442165"
  },
  {
    "text": "that's from the course repository, and it contains a bunch of functions that are useful for all the stuff that I'm talking about now.",
    "start": "3442165",
    "end": "3449695"
  },
  {
    "text": "Um, and actually, this example is from the notebook here. This is the, this is the space we've been working with A, B,",
    "start": "3449695",
    "end": "3455140"
  },
  {
    "text": "C. And I'm just showing you here is for example, vsm.euclidean with A, B,",
    "start": "3455140",
    "end": "3461080"
  },
  {
    "text": "C. Looking at the A vector and the B vector will give you their Euclidean distance as the return value.",
    "start": "3461080",
    "end": "3466810"
  },
  {
    "text": "[NOISE] Um, you could also do vector length. You can do length-norm, cosine, matching, Jaccard.",
    "start": "3466810",
    "end": "3476319"
  },
  {
    "text": "That might be all of the ones that I defined in there. [NOISE].",
    "start": "3476320",
    "end": "3483460"
  },
  {
    "text": "Yeah. And you could add [NOISE] your own if you want to. I think I didn't do KL because it's too fiddly.",
    "start": "3483460",
    "end": "3489595"
  },
  {
    "text": "Um, but if you're interested in KL, it's straightforward to implement. Uh, [NOISE] good.",
    "start": "3489595",
    "end": "3495810"
  },
  {
    "text": "[NOISE] And he- here, oh, this is, this is kinda more substantive.",
    "start": "3495810",
    "end": "3501400"
  },
  {
    "text": "So [NOISE] this is another set of code snippets. At the top there, I just reloaded for good measure that imdb5 matrix.",
    "start": "3501400",
    "end": "3509829"
  },
  {
    "text": "And then I took the cosine distance of good and excellent and it's 0.96 or so.",
    "start": "3509830",
    "end": "3515155"
  },
  {
    "text": "And then I did the cosine distance of good and bad and it's 0.94. That's actually not what I wanted, right?",
    "start": "3515155",
    "end": "3523165"
  },
  {
    "text": "Because I've been telling you that we want to capture the sentiment similarity in these spaces, and I think we can.",
    "start": "3523165",
    "end": "3529285"
  },
  {
    "text": "But so far, those two very similar words even with cosine are farther apart than good and bad.",
    "start": "3529285",
    "end": "3535734"
  },
  {
    "text": "And another way you can see that the counts aren't so great, you have this function vsm.neighbors and you can give it a word,",
    "start": "3535735",
    "end": "3543745"
  },
  {
    "text": "and one of these vector spaces as the second argument. And it will give you a series that gives you",
    "start": "3543745",
    "end": "3549010"
  },
  {
    "text": "the ranking of that word and all its closest neighbors. [NOISE] And the closest neighbors in imdb5 to bad are guys,",
    "start": "3549010",
    "end": "3557934"
  },
  {
    "text": "a period, taste, and guy. I think that validates something I've been saying before.",
    "start": "3557935",
    "end": "3565315"
  },
  {
    "text": "Does any- anyone want to say, like, do you agree? [NOISE] Yeah.",
    "start": "3565315",
    "end": "3570579"
  },
  {
    "text": "Should the punctuation be treated as words? Punctuation shouldn't be treated as words.",
    "start": "3570580",
    "end": "3575769"
  },
  {
    "text": "Shouldn't, the punctuation should be treated as what? I-I want to defend the idea that punctuation could be important.",
    "start": "3575770",
    "end": "3581800"
  },
  {
    "text": "So that's why I've left them in. I guess we're both being provocative about this.",
    "start": "3581800",
    "end": "3586930"
  },
  {
    "text": "I would wanna say two things. First, punctuation could be super meaningful like exclamation mark, versus period, versus question mark.",
    "start": "3586930",
    "end": "3593860"
  },
  {
    "text": "That'll be important for lots of models but also if you're right and the period shouldn't be important then I should. What's that?",
    "start": "3593860",
    "end": "3602170"
  },
  {
    "text": "I didn't say it wasn't important, sorry. Oh you didn't say, okay. But if somebody said [LAUGHTER] and you see lots of people",
    "start": "3602170",
    "end": "3608200"
  },
  {
    "text": "with these stop-word lists where they're filtering off lots of stuff that they regard as unimportant and a priori.",
    "start": "3608200",
    "end": "3613300"
  },
  {
    "text": "I'd say that's a failure, that your model should be able to pick up on whether or not those things are important. And that's why I deliberately left in a bunch of stuff that you might think is junk.",
    "start": "3613300",
    "end": "3622089"
  },
  {
    "text": "And I say if it's junk your model should treat it as junk. If I think punctuation can be good,",
    "start": "3622090",
    "end": "3627910"
  },
  {
    "text": "what I just meant is that, I told you that narrow window with scaling would capture a lot of collocational",
    "start": "3627910",
    "end": "3634300"
  },
  {
    "text": "information and it doesn't surprise me that bad guys is a very common bigram in IMDB movie reviews.",
    "start": "3634300",
    "end": "3641244"
  },
  {
    "text": "Um, but it's also not such a happy picture here [NOISE] and I did",
    "start": "3641245",
    "end": "3646405"
  },
  {
    "text": "bad with Jaccard and it's a totally different picture maybe less understandable.",
    "start": "3646405",
    "end": "3651775"
  },
  {
    "text": "Uh, but in any case, this is kinda of a cliffhanger for you, if things aren't looking so good, right?",
    "start": "3651775",
    "end": "3657505"
  },
  {
    "text": "With these raw counts. Ah, but we'll improve on it [NOISE].",
    "start": "3657505",
    "end": "3665710"
  },
  {
    "text": "And let's see, I'm trying to, yeah, let me show you one thing from this. And then I want to talk a little bit about",
    "start": "3665710",
    "end": "3671829"
  },
  {
    "text": "the homework because then we'll have lots of tools under our belt. So the next big unit here is, basic re-weighting schemes.",
    "start": "3671830",
    "end": "3678940"
  },
  {
    "text": "And again, there're goals of re-weighting as I said before we wanna amplify the things that are important and trustworthy, this is relevant for a question I got before.",
    "start": "3678940",
    "end": "3687894"
  },
  {
    "text": "Oh yeah, for your question about idiosyncratic stuff we want the trustworthy stuffs that's important and unusual but trustworthy.",
    "start": "3687895",
    "end": "3694240"
  },
  {
    "text": "And we wanna deemphasize the mundane and also the quirky, and problematic, and so forth.",
    "start": "3694240",
    "end": "3699790"
  },
  {
    "text": "Now, absent a defined objective function. This is gonna remain pretty fuzzy.",
    "start": "3699790",
    "end": "3705655"
  },
  {
    "text": "But I think it's a pretty clear idea intuitively. Um, the idea is that raw counts have important information in them but there are",
    "start": "3705655",
    "end": "3713740"
  },
  {
    "text": "kind of poor proxy for the things that we actually wanna care about but we can find the relevant information if we look.",
    "start": "3713740",
    "end": "3720395"
  },
  {
    "text": "So I'm gonna show you a bunch of these basic re-weighting schemes and give you a feel for the different properties and",
    "start": "3720395",
    "end": "3726780"
  },
  {
    "text": "what I suggest is a framework for thinking about what they're doing is that you ask these questions as we go.",
    "start": "3726780",
    "end": "3732930"
  },
  {
    "text": "So first, how does the re-weighted set of values compare to the raw counts?",
    "start": "3732930",
    "end": "3739809"
  },
  {
    "text": "For example, if it was identical to the raw counts it would be not such a very interesting weighting scheme, right?",
    "start": "3739810",
    "end": "3746410"
  },
  {
    "text": "Or if it was just a proportional re-scaling of the counts then you might think why bother.",
    "start": "3746410",
    "end": "3751870"
  },
  {
    "text": "But if it gives very different values from the raw counts then you might be onto something.",
    "start": "3751870",
    "end": "3757734"
  },
  {
    "text": "You might also want to ask how does it compare to the word frequency, which is kind of getting back to this theme I was articulating before which is,",
    "start": "3757735",
    "end": "3765820"
  },
  {
    "text": "how much of these methods are just capturing the fact that words have different rates of usage?",
    "start": "3765820",
    "end": "3771655"
  },
  {
    "text": "And then finally, what is the overall distribution of values? You might know from other linguistics classes that,",
    "start": "3771655",
    "end": "3778510"
  },
  {
    "text": "the distribution of the raw counts is gonna be like this horrible Zipfian drop-off where a few words have",
    "start": "3778510",
    "end": "3785530"
  },
  {
    "text": "very high frequency and lots of words are used hardly at all. And you might also have an intuition that that's kind of",
    "start": "3785530",
    "end": "3791920"
  },
  {
    "text": "a terrible distribution for lots of different modelling that you might want to do.",
    "start": "3791920",
    "end": "3797500"
  },
  {
    "text": "So you might ask of your re-weighting scheme that it give you something better than that crazy log-log Zipfian drop-off. Yeah?",
    "start": "3797500",
    "end": "3807160"
  },
  {
    "text": "[inaudible] what's the difference between this kind of re-weighting in like the scheme that we did earlier [inaudible].",
    "start": "3807160",
    "end": "3814329"
  },
  {
    "text": "Very similar. Or something? Yeah very similar. That's a good question. Um, when we did L2 norm and when we did",
    "start": "3814330",
    "end": "3820570"
  },
  {
    "text": "probability distributions I think we were doing re-weighting. Yeah, they're kinda coupled.",
    "start": "3820570",
    "end": "3826160"
  },
  {
    "text": "Oh, and I articulated this as a value before. I don't wanna do feature selection based on counts,",
    "start": "3826200",
    "end": "3832000"
  },
  {
    "text": "I don't wanna get rid of the most frequent words or you know use stop-word dictionaries. Rather, I want the models to know that for this dataset it's not so",
    "start": "3832000",
    "end": "3841210"
  },
  {
    "text": "interesting to have used the word \"the\" but maybe in another dataset that's an important property.",
    "start": "3841210",
    "end": "3847100"
  },
  {
    "text": "Oh, this is a good way to connect with the question you just asked. So the most basic kind of re-weighting that we could do would be normalization.",
    "start": "3847920",
    "end": "3856930"
  },
  {
    "text": "And you've already seen two ways of doing that before. L2 norming you know,",
    "start": "3856930",
    "end": "3862140"
  },
  {
    "text": "dividing all the values in the row by its L2 length and",
    "start": "3862140",
    "end": "3867180"
  },
  {
    "text": "probability distribution which just divides all the values in the row by the sum of those values.",
    "start": "3867180",
    "end": "3873310"
  },
  {
    "text": "That makes sense, right? But it is a re-weighting scheme and you can already think about how it's gonna impact, how it's gonna be relevant here.",
    "start": "3875010",
    "end": "3882430"
  },
  {
    "text": "Like you're going to lose a lot of frequency information and gain a lot of, a really rich picture of how things proportionately differ from each other.",
    "start": "3882430",
    "end": "3890870"
  },
  {
    "text": "Observed over expected. This isn't involves a lot of notation but I think it's a pretty intuitive idea.",
    "start": "3892700",
    "end": "3899520"
  },
  {
    "text": "I just wanted to be precise. So let me just use row sum X i to mean the sum of all the values in row",
    "start": "3899520",
    "end": "3906010"
  },
  {
    "text": "i and column sum X j to be the sum of all the values going down that column.",
    "start": "3906010",
    "end": "3913255"
  },
  {
    "text": "And then some of the X is the sum of all the values in the entire matrix.",
    "start": "3913255",
    "end": "3918835"
  },
  {
    "text": "The observed over expected calculation just takes each value in your matrix and divides it by this value the expected value.",
    "start": "3918835",
    "end": "3929570"
  },
  {
    "text": "I think stepping back from the calculations intuitively the way that, what this, what this is saying is,",
    "start": "3929640",
    "end": "3936325"
  },
  {
    "text": "for a given cell value, if I think about the probability of the column and the probability of",
    "start": "3936325",
    "end": "3942609"
  },
  {
    "text": "the row, that's giving me a default expectation for what that count should be.",
    "start": "3942610",
    "end": "3948410"
  },
  {
    "text": "And then I can compare that kind of default null hypothesis expectation with the actual value.",
    "start": "3948720",
    "end": "3955885"
  },
  {
    "text": "And if the actual value is larger than I would expect given those row and column probabilities,",
    "start": "3955885",
    "end": "3961105"
  },
  {
    "text": "I wanna amplify it. And if it's smaller, I wanna make it much smaller. And this is a kind of like that phrase I gave you before like amplify the important.",
    "start": "3961105",
    "end": "3971650"
  },
  {
    "text": "This is like amplifying the things that are departing from my default expectation given the row and the column.",
    "start": "3971650",
    "end": "3978625"
  },
  {
    "text": "And you're gonna see that again and again through all of these weighting schemes and on through GloVe, and Word2Vec, and all of",
    "start": "3978625",
    "end": "3985930"
  },
  {
    "text": "them is kind of like, null hypothesis based on the row and the column and then a comparison with what you actually have. Yeah.",
    "start": "3985930",
    "end": "3994160"
  },
  {
    "text": "I think multiplying a numerator you're supposed to say something. That's the joint probability.",
    "start": "3994160",
    "end": "4002200"
  },
  {
    "text": "An- and actually, like I haven't done this here but if you think about the sums that you are doing,",
    "start": "4002300",
    "end": "4008640"
  },
  {
    "text": "like I have this count sum here and these two sums here, that's actually once I've done, gone all through",
    "start": "4008640",
    "end": "4014970"
  },
  {
    "text": "all the calculations, it's gonna give me a probability value and then the product is the joint probability and down there is",
    "start": "4014970",
    "end": "4021090"
  },
  {
    "text": "the full probability for the whole table which will be 1.",
    "start": "4021090",
    "end": "4025180"
  },
  {
    "text": "So here's an example of the calculations. And I've done some color highlighting here just to",
    "start": "4028040",
    "end": "4033660"
  },
  {
    "text": "show you what quantities are involved like first, compute the row sum and the column sum.",
    "start": "4033660",
    "end": "4039539"
  },
  {
    "text": "And then appear is the actual count. And then this gives you that expected value down here.",
    "start": "4039540",
    "end": "4046330"
  },
  {
    "text": "One more thing, and I'm going to return to this next time because this is all really important. Pointwise mutual information, which I said is the hero of this whole unit.",
    "start": "4050030",
    "end": "4059295"
  },
  {
    "text": "This is just observed over expected in log-space. That's all it is.",
    "start": "4059295",
    "end": "4066750"
  },
  {
    "text": "And so PMI is also drawing on this insight that I have a null hypothesis based on the row and the column,",
    "start": "4066750",
    "end": "4073860"
  },
  {
    "text": "and then I can compare that to the actual value. It just puts them into log-space so that you see many more differences than you saw in",
    "start": "4073860",
    "end": "4080460"
  },
  {
    "text": "that original space. Yeah. So previously, you mentioned that KL divergence is very finicky. Why does PMI work then?",
    "start": "4080460",
    "end": "4091905"
  },
  {
    "text": "Because PMI is very related to KL divergence. Yeah and it's going to be finicky.",
    "start": "4091905",
    "end": "4098625"
  },
  {
    "text": "This is also related to your question. You guys have the same intuition and I've given an example calculation here,",
    "start": "4098625",
    "end": "4104970"
  },
  {
    "text": "and what I've done is just highlight this value. So this is a 1. It's maybe, maybe that's just a mistake in your corpus.",
    "start": "4104970",
    "end": "4113355"
  },
  {
    "text": "That's what I'm asking you to imagine. But you can see that, this value if I compare it to its row and column values",
    "start": "4113355",
    "end": "4120089"
  },
  {
    "text": "is gonna look huge compared to these other ones. And so I formed this joint probability table with the row and the column probabilities.",
    "start": "4120090",
    "end": "4129060"
  },
  {
    "text": "This is kind of working off this version and not this one but these are equivalent. And then when I applied PMI look how big that one has",
    "start": "4129060",
    "end": "4137220"
  },
  {
    "text": "gotten compared to all these other values and that's the,",
    "start": "4137220",
    "end": "4142275"
  },
  {
    "text": "like a standard property which I think you're keying into that. Often, these log and log odds comparison values",
    "start": "4142275",
    "end": "4148409"
  },
  {
    "text": "really amplify very small values that you cannot trust. And this is a problem with PMI is that it",
    "start": "4148410",
    "end": "4155759"
  },
  {
    "text": "does too much amplification of these very small values. And you'll definitely see that if you use it, yeah.",
    "start": "4155760",
    "end": "4161220"
  },
  {
    "text": "[inaudible] values like that. I think I'm going to opt for the latter.",
    "start": "4161220",
    "end": "4167009"
  },
  {
    "text": "If you look at the Turney and Pantel, they consider the Laplacian smoothing which you mentioned and they also consider",
    "start": "4167010",
    "end": "4172770"
  },
  {
    "text": "a method of contextual discounting which is like adjusting the rows and the columns a little bit. Um, but I think that you know playing this forward",
    "start": "4172770",
    "end": "4180540"
  },
  {
    "text": "a little bit you'll see that GloVe is basically, regularized reduced dimensional PMI.",
    "start": "4180540",
    "end": "4186404"
  },
  {
    "text": "And you might say \"oh, well, they just discovered PMI then.\" But that's not quite true because what GloVe is",
    "start": "4186405",
    "end": "4191759"
  },
  {
    "text": "doing is all that stuff that you just mentioned. And then you might have a hypothesis which I think is borne out that it suffers less from this problem here.",
    "start": "4191760",
    "end": "4200500"
  },
  {
    "text": "I'm gonna stop here because I want to take the last few minutes to just show you what the homework is like.",
    "start": "4201440",
    "end": "4208125"
  },
  {
    "text": "I think we'll do that again next time in a little more detail. And we're gonna continue working through this slideshow and call back on previous themes.",
    "start": "4208125",
    "end": "4215430"
  },
  {
    "text": "But this is a good moment to kind of plant in your ideas that I think you now have all the tools that you need to make progress on this homework.",
    "start": "4215430",
    "end": "4223300"
  },
  {
    "text": "So I will just describe the task basically and then we'll wrap up and we'll return to this later.",
    "start": "4224630",
    "end": "4231100"
  },
  {
    "text": "So the homework and the bake-off are based around a classic task in NLU for these vector space models which is word similarity.",
    "start": "4233000",
    "end": "4244270"
  },
  {
    "text": "I've given you for your development phase, five datasets.",
    "start": "4244760",
    "end": "4251445"
  },
  {
    "text": "These are all kind of famous datasets from the literature, um, they are human curated.",
    "start": "4251445",
    "end": "4256710"
  },
  {
    "text": "So like human annotation measures of similarity for pairs of words.",
    "start": "4256710",
    "end": "4262020"
  },
  {
    "text": "So they're all just these lists of two words and then a score. And the overall task for you is to develop",
    "start": "4262020",
    "end": "4269550"
  },
  {
    "text": "a vector space model where when you do comparison between vectors, the values that you get are proportional to,",
    "start": "4269550",
    "end": "4278370"
  },
  {
    "text": "directly related to the ranking that the humans have provided. That is that they reflect those similarity scores from",
    "start": "4278370",
    "end": "4286170"
  },
  {
    "text": "these human curated datasets. That's the heart of it. Um, I will say that there are kind of two notions at work here.",
    "start": "4286170",
    "end": "4294974"
  },
  {
    "text": "Sometimes the first three, people call those relatedness tasks and the second two,",
    "start": "4294975",
    "end": "4300210"
  },
  {
    "text": "they call similarity tasks. I'm not gonna make such a big deal about this, but you can imagine that the authors of these datasets do make a big deal out of it.",
    "start": "4300210",
    "end": "4309010"
  },
  {
    "text": "Um, and you'll have to think for yourself about whether you want your model to key into one or another of these notions.",
    "start": "4309020",
    "end": "4316080"
  },
  {
    "text": "So you'll do all your development on these datasets. Um, and I've given you some information about exactly the metrics involved.",
    "start": "4316080",
    "end": "4324630"
  },
  {
    "text": "And then for the bake-off, I'm gonna give you two new datasets that you haven't seen before.",
    "start": "4324630",
    "end": "4330434"
  },
  {
    "text": "Um, and you will have developed your system and you'll simply run that system on those two new datasets and we're all kind of on the honor system here",
    "start": "4330435",
    "end": "4338520"
  },
  {
    "text": "that you don't do subsequent tuning to these new datasets, right? The way the field works is we trust you to run it once and report that score, ah,",
    "start": "4338520",
    "end": "4347820"
  },
  {
    "text": "and that should give you a good measure for how well your model generalizes to new notions of relatedness and/or similarity.",
    "start": "4347820",
    "end": "4356830"
  },
  {
    "text": "The notebook embeds all the readers that you need. And if you've already downloaded the data distribution,",
    "start": "4357350",
    "end": "4363330"
  },
  {
    "text": "then you basically won't to have to fiddle with any of this. You can just read in all of these datasets. I do recommend looking at this code just so that",
    "start": "4363330",
    "end": "4370020"
  },
  {
    "text": "you know exactly what it's doing but basically, it's just reading in these pairs of words with the scores.",
    "start": "4370020",
    "end": "4376000"
  },
  {
    "text": "There are all the readers there. And then in the middle of this notebook I did a bunch of stuff again,",
    "start": "4376340",
    "end": "4383940"
  },
  {
    "text": "trying to illustrate best practices like, if this were really your project, you would wanna understand these datasets",
    "start": "4383940",
    "end": "4389790"
  },
  {
    "text": "and how they're related to each other and you would want that exploration to inform how you talked about your model and your results.",
    "start": "4389790",
    "end": "4395940"
  },
  {
    "text": "And so I've just done a bunch of analysis unlike how the datasets relate to each other at the level of vocabulary and at the level of the scores they provide.",
    "start": "4395940",
    "end": "4404940"
  },
  {
    "text": "So that's the middle of this notebook. And you might want to fiddle around with this to get a feel for where you can tweak your model to get improvements.",
    "start": "4404940",
    "end": "4413440"
  },
  {
    "text": "And then finally, at the bottom we get to the evaluation. And this is the, the main thing that you'll want to use.",
    "start": "4416510",
    "end": "4422700"
  },
  {
    "text": "So you have word similarity evaluation which you know basically, a reader is a dataset here and a df dataframe.",
    "start": "4422700",
    "end": "4429360"
  },
  {
    "text": "That's the vector space model that you created. And then you can optionally change the distance function if you want.",
    "start": "4429360",
    "end": "4436620"
  },
  {
    "text": "And so that gives you a correlation coefficient and the evaluation which has a bunch of information in it.",
    "start": "4436620",
    "end": "4444855"
  },
  {
    "text": "There's some stuff in here already for doing error analysis which I recommend.",
    "start": "4444855",
    "end": "4449950"
  },
  {
    "text": "And then finally, here's the full evaluation. Full word similarity evaluation if you",
    "start": "4450680",
    "end": "4456510"
  },
  {
    "text": "just feed in the vector space model that you've built, it will run it on all the datasets,",
    "start": "4456510",
    "end": "4462315"
  },
  {
    "text": "and it gives you scores like this for each one.",
    "start": "4462315",
    "end": "4467550"
  },
  {
    "text": "You can think of this as my baseline and it's truly horrible. Um, so like for example,",
    "start": "4467550",
    "end": "4474090"
  },
  {
    "text": "all I did was read in giga5, so raw counts, narrow window, heavy scaling,",
    "start": "4474090",
    "end": "4480030"
  },
  {
    "text": "you actually have a negative correlation with SimVerb dev and test. And the other ones are pretty dismal.",
    "start": "4480030",
    "end": "4487260"
  },
  {
    "text": "So there's lots of room for improvement over that baseline. What the homework questions ask you to do is build up some interesting models.",
    "start": "4487260",
    "end": "4494700"
  },
  {
    "text": "So first of all, just do positive pointwise mutual information which I almost got to. I showed you PMI. So you would do that.",
    "start": "4494700",
    "end": "4502305"
  },
  {
    "text": "I'm gonna do LSA with you next time. Explore different, a bunch of different values of LSA. We'll do",
    "start": "4502305",
    "end": "4508680"
  },
  {
    "text": "GloVe maybe next week, I'm asking you to do a little bit of exploration of GloVe. All of this is just kind of coaxing you in directions that might be interesting.",
    "start": "4508680",
    "end": "4516495"
  },
  {
    "text": "So you produce a little bit of code. T-test reweighting uh, that's not what I'm going to talk about in",
    "start": "4516495",
    "end": "4521940"
  },
  {
    "text": "the lecture but that turns out to be a really powerful reweighting scheme. And if you stare at it you'll see that it builds on",
    "start": "4521940",
    "end": "4527700"
  },
  {
    "text": "this row column intuition just the way PMI does. So you'll implement that.",
    "start": "4527700",
    "end": "4532875"
  },
  {
    "text": "And then finally, you just develop an original system. Drawing on all those ideas that you had",
    "start": "4532875",
    "end": "4539460"
  },
  {
    "text": "before, maybe you pipeline a whole bunch of these models. Maybe you do something that I never even mentioned, some crazy thing that you think will work.",
    "start": "4539460",
    "end": "4546675"
  },
  {
    "text": "And you can hammer away at those readers. You can do as much evaluation as you want. Don't overfit to them because I'm gonna give you different datasets to test on",
    "start": "4546675",
    "end": "4555570"
  },
  {
    "text": "but you know, do as much development as you want and you submit that. And that's how you get 9 of the 10 homework points.",
    "start": "4555570",
    "end": "4563190"
  },
  {
    "text": "On the day that the submission is due which is April 15, we'll distribute those two new datasets.",
    "start": "4563190",
    "end": "4570045"
  },
  {
    "text": "All you have to do at that point is run the model that you developed on those datasets, and submit the resulting notebook.",
    "start": "4570045",
    "end": "4577260"
  },
  {
    "text": "There's actually what we want you to do is fill out these cells down here but pretty much just submit the completed thing.",
    "start": "4577260",
    "end": "4584055"
  },
  {
    "text": "And then you've entered the bake-off. So there's no real work beyond just, you know, you have to",
    "start": "4584055",
    "end": "4589650"
  },
  {
    "text": "like, paste in some code and then run your system. And then we're gonna see what works best.",
    "start": "4589650",
    "end": "4596295"
  },
  {
    "text": "And once all the scores are in, we'll announce the winner, and then one of the TA's, or a pair of the TA's is gonna reflect back to you",
    "start": "4596295",
    "end": "4604665"
  },
  {
    "text": "everything that we learned from looking at your systems and thinking about the results that we got in. With luck, having done that, not only did we have a fun competition,",
    "start": "4604665",
    "end": "4612179"
  },
  {
    "text": "but we also learned something about what works and what doesn't for these problems. All right.",
    "start": "4612179",
    "end": "4619530"
  },
  {
    "text": "Let's wrap up there and we're off to a great start. Next time a lot more re-weighting [NOISE].",
    "start": "4619530",
    "end": "4629680"
  }
]