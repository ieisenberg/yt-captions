[
  {
    "start": "0",
    "end": "152000"
  },
  {
    "start": "0",
    "end": "690"
  },
  {
    "text": "OK.",
    "start": "690",
    "end": "1370"
  },
  {
    "text": "We've learnt about\nmethods for regression,",
    "start": "1370",
    "end": "3120"
  },
  {
    "text": "and for classification,\nand building predictors,",
    "start": "3120",
    "end": "5790"
  },
  {
    "text": "and for making\npredictions from our data.",
    "start": "5790",
    "end": "9330"
  },
  {
    "text": "How do we test these out?",
    "start": "9330",
    "end": "10900"
  },
  {
    "text": "Well, ideally we'd like to get\na new sample from the population",
    "start": "10900",
    "end": "14160"
  },
  {
    "text": "and see how well\nour predictions do.",
    "start": "14160",
    "end": "16710"
  },
  {
    "text": "Well, we don't\nalways have new data.",
    "start": "16710",
    "end": "18570"
  },
  {
    "text": "So what do we do?",
    "start": "18570",
    "end": "19820"
  },
  {
    "text": "And we can't use our training\ndata just straight off",
    "start": "19820",
    "end": "22710"
  },
  {
    "text": "because it's going to be\na little bit optimistic.",
    "start": "22710",
    "end": "25810"
  },
  {
    "text": "So we're going to tell you\nabout cross-validation.",
    "start": "25810",
    "end": "28949"
  },
  {
    "text": "Rob is going to tell you\nabout cross-validation, which",
    "start": "28950",
    "end": "31200"
  },
  {
    "text": "is a very clever device\nfor using the same training",
    "start": "31200",
    "end": "33780"
  },
  {
    "text": "data to tell you how well\nyour prediction method works.",
    "start": "33780",
    "end": "38280"
  },
  {
    "text": "The other thing--",
    "start": "38280",
    "end": "39133"
  },
  {
    "text": "Are you going to give\nmy entire lecture?",
    "start": "39133",
    "end": "40800"
  },
  {
    "text": "Oh, I'll try not to, Rob.",
    "start": "40800",
    "end": "42120"
  },
  {
    "text": "Just in case you miss out\nsome of the salient points.",
    "start": "42120",
    "end": "44879"
  },
  {
    "text": "The other thing we're\ngoing to look at",
    "start": "44880",
    "end": "46530"
  },
  {
    "text": "is standard errors\nof estimators.",
    "start": "46530",
    "end": "49120"
  },
  {
    "text": "Sometimes our estimators\nare quite complex",
    "start": "49120",
    "end": "51719"
  },
  {
    "text": "and we'd like to know\nthe standard error, which",
    "start": "51720",
    "end": "53790"
  },
  {
    "text": "means what happens if you've got\nnew samples from the population",
    "start": "53790",
    "end": "56970"
  },
  {
    "text": "over and over again, and\nwe recomputed our estimate,",
    "start": "56970",
    "end": "60280"
  },
  {
    "text": "and the standard error\nis the standard deviation",
    "start": "60280",
    "end": "63030"
  },
  {
    "text": "of those estimates\nunder resampling.",
    "start": "63030",
    "end": "65309"
  },
  {
    "text": "Well, of course, we can't\nresample from the population,",
    "start": "65310",
    "end": "68200"
  },
  {
    "text": "we only have one sample.",
    "start": "68200",
    "end": "69816"
  },
  {
    "text": "Again, Rob's going to tell\nyou about the bootstrap which",
    "start": "69817",
    "end": "72150"
  },
  {
    "text": "is a very clever device for\nusing one single training",
    "start": "72150",
    "end": "75700"
  },
  {
    "text": "sample you have to estimate\nthings like standard deviations.",
    "start": "75700",
    "end": "79930"
  },
  {
    "text": "Rob?",
    "start": "79930",
    "end": "80430"
  },
  {
    "text": "OK.",
    "start": "80430",
    "end": "80930"
  },
  {
    "text": "Well, thanks for that\ngreat introduction.",
    "start": "80930",
    "end": "83550"
  },
  {
    "text": "Good overview.",
    "start": "83550",
    "end": "84460"
  },
  {
    "text": "So we're going to talk\nabout cross-validation",
    "start": "84460",
    "end": "85799"
  },
  {
    "text": "and the bootstrap,\nas Trevor mentioned.",
    "start": "85800",
    "end": "87425"
  },
  {
    "text": "And these are\nresampling methods.",
    "start": "87425",
    "end": "89770"
  },
  {
    "text": "So the word resampling,\noriginal data is a sample.",
    "start": "89770",
    "end": "93060"
  },
  {
    "text": "We're going to resample.",
    "start": "93060",
    "end": "94600"
  },
  {
    "text": "We actually sample from\nour data set in order",
    "start": "94600",
    "end": "96960"
  },
  {
    "text": "to learn about the\nquantity of interest.",
    "start": "96960",
    "end": "99659"
  },
  {
    "text": "And the cross-validation,\nthe bootstrap",
    "start": "99660",
    "end": "101460"
  },
  {
    "text": "are both ways of\nsampling from the data.",
    "start": "101460",
    "end": "105250"
  },
  {
    "text": "And the purpose of it\nis to get information",
    "start": "105250",
    "end": "107133"
  },
  {
    "text": "about the--\nadditional information",
    "start": "107133",
    "end": "108550"
  },
  {
    "text": "about the fitted model.",
    "start": "108550",
    "end": "109508"
  },
  {
    "text": "For example, the main thing\nwe use cross-validation for",
    "start": "109508",
    "end": "112840"
  },
  {
    "text": "is to get an idea of the\ntest set error of our model.",
    "start": "112840",
    "end": "117679"
  },
  {
    "text": "We're going to review the\nconcept of training error",
    "start": "117680",
    "end": "120010"
  },
  {
    "text": "and we'll see, as we've\ntalked about before,",
    "start": "120010",
    "end": "121990"
  },
  {
    "text": "the training error\nis too optimistic.",
    "start": "121990",
    "end": "124113"
  },
  {
    "text": "The more we fit to the data,\nthe lower the training error.",
    "start": "124113",
    "end": "126530"
  },
  {
    "text": "But the test error can\nget higher if we overfit.",
    "start": "126530",
    "end": "128880"
  },
  {
    "text": "It often will.",
    "start": "128880",
    "end": "129889"
  },
  {
    "text": "So cross-validation is\na very important tool",
    "start": "129889",
    "end": "132673"
  },
  {
    "text": "that we're going to talk\nabout in this section",
    "start": "132673",
    "end": "134590"
  },
  {
    "text": "and we'll use\nthroughout the course",
    "start": "134590",
    "end": "136360"
  },
  {
    "text": "to get a good idea of the\ntest set error of a model.",
    "start": "136360",
    "end": "140800"
  },
  {
    "text": "Bootstrap, on the other\nhand, is most useful",
    "start": "140800",
    "end": "143170"
  },
  {
    "text": "to of the variability\nor standard deviation",
    "start": "143170",
    "end": "145360"
  },
  {
    "text": "of an estimate and its bias.",
    "start": "145360",
    "end": "147310"
  },
  {
    "text": "So we'll talk about\nfirst cross-validation",
    "start": "147310",
    "end": "149390"
  },
  {
    "text": "and then bootstrap.",
    "start": "149390",
    "end": "151925"
  },
  {
    "text": "But before we get\ninto those, let's",
    "start": "151925",
    "end": "154340"
  },
  {
    "start": "152000",
    "end": "207000"
  },
  {
    "text": "review the idea of the concept\nof training error versus test",
    "start": "154340",
    "end": "157010"
  },
  {
    "text": "error.",
    "start": "157010",
    "end": "158480"
  },
  {
    "text": "Remember, test error is the\nerror that we incur on new data.",
    "start": "158480",
    "end": "161700"
  },
  {
    "text": "So we fit our model\nto a training set.",
    "start": "161700",
    "end": "163650"
  },
  {
    "text": "We take our model and then\nwe apply it to new data",
    "start": "163650",
    "end": "166099"
  },
  {
    "text": "that the model hasn't seen.",
    "start": "166100",
    "end": "167810"
  },
  {
    "text": "The test error is actually how\nwell we'll do on future data",
    "start": "167810",
    "end": "171440"
  },
  {
    "text": "the model hasn't seen.",
    "start": "171440",
    "end": "172880"
  },
  {
    "text": "Training error is much\neasier to compute.",
    "start": "172880",
    "end": "175553"
  },
  {
    "text": "We can do it on\nthe same data set.",
    "start": "175553",
    "end": "176970"
  },
  {
    "text": "What is it?",
    "start": "176970",
    "end": "177570"
  },
  {
    "text": "It's the error we get applying\nthe model to the same data",
    "start": "177570",
    "end": "181400"
  },
  {
    "text": "from which we trained.",
    "start": "181400",
    "end": "183545"
  },
  {
    "text": "And as you can imagine,\ntraining error is often quite--",
    "start": "183545",
    "end": "187400"
  },
  {
    "text": "it's lower than test error.",
    "start": "187400",
    "end": "188819"
  },
  {
    "text": "The model is already seen\nthat the training set",
    "start": "188820",
    "end": "190830"
  },
  {
    "text": "so it's going to fit the\ntraining set with lower error",
    "start": "190830",
    "end": "194660"
  },
  {
    "text": "than it was going to\noccur on a test set.",
    "start": "194660",
    "end": "196580"
  },
  {
    "text": "And the more we\noverfit, the harder",
    "start": "196580",
    "end": "198560"
  },
  {
    "text": "we fit the data, the lower\nthe training error looks.",
    "start": "198560",
    "end": "201160"
  },
  {
    "text": "On the other hand, the test\nerror can be quite a bit higher.",
    "start": "201160",
    "end": "203660"
  },
  {
    "text": "So training error is not a\ngood surrogate for test error.",
    "start": "203660",
    "end": "207060"
  },
  {
    "start": "207000",
    "end": "417000"
  },
  {
    "text": "And this picture is\na good one to look at",
    "start": "207060",
    "end": "210450"
  },
  {
    "text": "to summarize the concepts here.",
    "start": "210450",
    "end": "212980"
  },
  {
    "text": "So what do we have here?",
    "start": "212980",
    "end": "214420"
  },
  {
    "text": "Well, first of all,\nalong the horizontal axis",
    "start": "214420",
    "end": "216750"
  },
  {
    "text": "is the model complexity,\nfrom low to high.",
    "start": "216750",
    "end": "220660"
  },
  {
    "text": "For example, in\nour linear model,",
    "start": "220660",
    "end": "224100"
  },
  {
    "text": "the model complexity\nis the number",
    "start": "224100",
    "end": "226195"
  },
  {
    "text": "of features, the\nnumber of coefficients",
    "start": "226195",
    "end": "227820"
  },
  {
    "text": "that we fit in the model.",
    "start": "227820",
    "end": "228940"
  },
  {
    "text": "So low means a few\nnumber of features,",
    "start": "228940",
    "end": "231000"
  },
  {
    "text": "or coefficients, or predictors.",
    "start": "231000",
    "end": "232710"
  },
  {
    "text": "High means a large number.",
    "start": "232710",
    "end": "235370"
  },
  {
    "text": "Think about fitting a polynomial\nwith higher and higher degree.",
    "start": "235370",
    "end": "239510"
  },
  {
    "text": "You can see our model complexity\nincreases with degree.",
    "start": "239510",
    "end": "242047"
  },
  {
    "text": "So we move to the right, we'd\nhave a higher complexity,",
    "start": "242048",
    "end": "244340"
  },
  {
    "text": "a higher order of polynomial.",
    "start": "244340",
    "end": "246099"
  },
  {
    "text": "The prediction error is\non the vertical axis.",
    "start": "246100",
    "end": "248250"
  },
  {
    "text": "And we have two curves here.",
    "start": "248250",
    "end": "249750"
  },
  {
    "text": "The training error in blue\nand the test error in red.",
    "start": "249750",
    "end": "253400"
  },
  {
    "text": "So what do we see?",
    "start": "253400",
    "end": "254582"
  },
  {
    "text": "Let's first look\nat the blue curve.",
    "start": "254582",
    "end": "256040"
  },
  {
    "text": "On the left, the model\ncomplexity is low.",
    "start": "256040",
    "end": "259467"
  },
  {
    "text": "For example, we're fitting a\nsmall number of parameters,",
    "start": "259467",
    "end": "261799"
  },
  {
    "text": "maybe just a single constant,\nthe training error is high.",
    "start": "261800",
    "end": "266759"
  },
  {
    "text": "Now, as we increase\nthe model complexity,",
    "start": "266760",
    "end": "268550"
  },
  {
    "text": "we fit more and more\nfeatures in the model,",
    "start": "268550",
    "end": "271190"
  },
  {
    "text": "or higher complexity, or higher\norder polynomial, the training",
    "start": "271190",
    "end": "274250"
  },
  {
    "text": "error goes down.",
    "start": "274250",
    "end": "275390"
  },
  {
    "text": "And actually in this\npicture, it continues",
    "start": "275390",
    "end": "277248"
  },
  {
    "text": "to go down in a consistent way.",
    "start": "277248",
    "end": "278539"
  },
  {
    "text": "In most cases, for most models,\nthe more complex the model,",
    "start": "278540",
    "end": "282800"
  },
  {
    "text": "the training error will go\ndown, as it does in this case.",
    "start": "282800",
    "end": "287970"
  },
  {
    "text": "On the other hand, the test\nerror is the red curve,",
    "start": "287970",
    "end": "290830"
  },
  {
    "text": "it does not\nconsistently go down.",
    "start": "290830",
    "end": "293039"
  },
  {
    "text": "It starts off high like\nthe training error,",
    "start": "293040",
    "end": "294930"
  },
  {
    "text": "comes down for a while, but\nthen it starts to come up again.",
    "start": "294930",
    "end": "297537"
  },
  {
    "text": "It has a minimum looks\nlike around the middle here",
    "start": "297537",
    "end": "299620"
  },
  {
    "text": "but after this point, the\nmore complex the model,",
    "start": "299620",
    "end": "303810"
  },
  {
    "text": "the higher the test error.",
    "start": "303810",
    "end": "305230"
  },
  {
    "text": "What's happened there?",
    "start": "305230",
    "end": "306280"
  },
  {
    "text": "Well, that is an\nexample of overfitting.",
    "start": "306280",
    "end": "310670"
  },
  {
    "text": "On the left, we've\nadded complexity,",
    "start": "310670",
    "end": "313527"
  },
  {
    "text": "some features that actually\nare important for predicting",
    "start": "313527",
    "end": "315860"
  },
  {
    "text": "the response.",
    "start": "315860",
    "end": "316439"
  },
  {
    "text": "So they reduce the test error.",
    "start": "316440",
    "end": "318000"
  },
  {
    "text": "But at that point,\nwe seem to have",
    "start": "318000",
    "end": "320000"
  },
  {
    "text": "fit all the important\nfeatures and now we're",
    "start": "320000",
    "end": "322280"
  },
  {
    "text": "putting in things\nwhich are just noise.",
    "start": "322280",
    "end": "324530"
  },
  {
    "text": "The train error goes down as\nit has to but the test error",
    "start": "324530",
    "end": "327530"
  },
  {
    "text": "is starting to go up.",
    "start": "327530",
    "end": "328520"
  },
  {
    "text": "That's overfitting.",
    "start": "328520",
    "end": "329811"
  },
  {
    "text": "So we don't want to\noverfit because we'll",
    "start": "329812",
    "end": "331520"
  },
  {
    "text": "increase the test error.",
    "start": "331520",
    "end": "332758"
  },
  {
    "text": "The training error has\nnot told us anything",
    "start": "332758",
    "end": "334550"
  },
  {
    "text": "about overfitting because it's\nusing the same data to measure",
    "start": "334550",
    "end": "338569"
  },
  {
    "text": "error.",
    "start": "338570",
    "end": "339210"
  },
  {
    "text": "The more parameters,\nthe better it looks.",
    "start": "339210",
    "end": "342210"
  },
  {
    "text": "So it's not a good--",
    "start": "342210",
    "end": "344750"
  },
  {
    "text": "does not give us a good\nidea of the test error.",
    "start": "344750",
    "end": "346800"
  },
  {
    "text": "The test error curve,\non the other hand,",
    "start": "346800",
    "end": "348466"
  },
  {
    "text": "is minimized at this\nmodel complexity",
    "start": "348467",
    "end": "351530"
  },
  {
    "text": "and beyond that is overfitting.",
    "start": "351530",
    "end": "354582"
  },
  {
    "text": "The ingredients of\nprediction error",
    "start": "354582",
    "end": "356040"
  },
  {
    "text": "are actually are\nbias and variance.",
    "start": "356040",
    "end": "359100"
  },
  {
    "text": "So the bias is how\nfar off on the average",
    "start": "359100",
    "end": "361770"
  },
  {
    "text": "the model is from the truth.",
    "start": "361770",
    "end": "363330"
  },
  {
    "text": "The variance is how\nmuch the estimate",
    "start": "363330",
    "end": "366780"
  },
  {
    "text": "varies around its average.",
    "start": "366780",
    "end": "368430"
  },
  {
    "text": "When we don't fit very\nhard, the bias is high,",
    "start": "368430",
    "end": "372341"
  },
  {
    "text": "the variance is\nlow because there's",
    "start": "372342",
    "end": "373800"
  },
  {
    "text": "few parameters being fit.",
    "start": "373800",
    "end": "375259"
  },
  {
    "text": "As we increase the model\ncomplexity, moving to the right,",
    "start": "375260",
    "end": "378030"
  },
  {
    "text": "the bias goes down\nbecause the model",
    "start": "378030",
    "end": "379980"
  },
  {
    "text": "can adapt to more and more\nsubtleties in the data",
    "start": "379980",
    "end": "384922"
  },
  {
    "text": "but the variance goes up because\nwe have more and more parameters",
    "start": "384922",
    "end": "387630"
  },
  {
    "text": "to estimate from the\nsame amount of data.",
    "start": "387630",
    "end": "389440"
  },
  {
    "text": "So bias and variance together\ngive us prediction error",
    "start": "389440",
    "end": "392130"
  },
  {
    "text": "and there's a trade-off.",
    "start": "392130",
    "end": "393590"
  },
  {
    "text": "They sum together to\ngive prediction error.",
    "start": "393590",
    "end": "395382"
  },
  {
    "text": "And the trade-off is minimized\nin this case around the model",
    "start": "395382",
    "end": "398669"
  },
  {
    "text": "complexity of the middle here.",
    "start": "398670",
    "end": "400530"
  },
  {
    "text": "So bias and variance\ntogether give us test error.",
    "start": "400530",
    "end": "404310"
  },
  {
    "text": "We want to find the\nmodel complexity",
    "start": "404310",
    "end": "406600"
  },
  {
    "text": "giving the smallest test error.",
    "start": "406600",
    "end": "408382"
  },
  {
    "text": "And training error does\nnot give us a good idea",
    "start": "408382",
    "end": "410340"
  },
  {
    "text": "of that, of the test error.",
    "start": "410340",
    "end": "412120"
  },
  {
    "text": "And they refer to it as the\nbias-variance trade-off.",
    "start": "412120",
    "end": "416383"
  },
  {
    "text": "So we can't use training\nerror to estimate test error,",
    "start": "416383",
    "end": "421320"
  },
  {
    "start": "417000",
    "end": "491000"
  },
  {
    "text": "as the previous\npicture shows us.",
    "start": "421320",
    "end": "423250"
  },
  {
    "text": "So what do we do?",
    "start": "423250",
    "end": "424390"
  },
  {
    "text": "Well, the best solution,\nif we have a large test",
    "start": "424390",
    "end": "427200"
  },
  {
    "text": "set we can use that.",
    "start": "427200",
    "end": "428410"
  },
  {
    "text": "We simply take our model that\nwe've fit on the training set,",
    "start": "428410",
    "end": "432330"
  },
  {
    "text": "apply it to the test set.",
    "start": "432330",
    "end": "434169"
  },
  {
    "text": "But very often, we don't\nhave a large test set.",
    "start": "434170",
    "end": "437420"
  },
  {
    "text": "So if we can't do\nthat, what do we do?",
    "start": "437420",
    "end": "439580"
  },
  {
    "text": "Well, there are some ways\nto get an idea of test error",
    "start": "439580",
    "end": "444310"
  },
  {
    "text": "using an adjustment\nto training error.",
    "start": "444310",
    "end": "446180"
  },
  {
    "text": "Basically, training\nerror can be too small,",
    "start": "446180",
    "end": "448090"
  },
  {
    "text": "as we've seen in the\nprevious picture.",
    "start": "448090",
    "end": "449690"
  },
  {
    "text": "So these methods adjust\nthe training error",
    "start": "449690",
    "end": "453220"
  },
  {
    "text": "by increasing it\nby a factor that",
    "start": "453220",
    "end": "457030"
  },
  {
    "text": "involves the amount of\nfitting that we've done",
    "start": "457030",
    "end": "459070"
  },
  {
    "text": "to the data and the variance.",
    "start": "459070",
    "end": "460880"
  },
  {
    "text": "And these methods include the CP\nstatistic, the AIC, and the BIC.",
    "start": "460880",
    "end": "465943"
  },
  {
    "text": "We'll talk about these\nlater on in the course,",
    "start": "465943",
    "end": "467860"
  },
  {
    "text": "not in this section.",
    "start": "467860",
    "end": "469030"
  },
  {
    "text": "Here instead, we're going\nto talk about validation",
    "start": "469030",
    "end": "471730"
  },
  {
    "text": "or cross-validation.",
    "start": "471730",
    "end": "473090"
  },
  {
    "text": "And these involve\nremoving part of the data,",
    "start": "473090",
    "end": "475180"
  },
  {
    "text": "holding it out, fitting the\nmodel to the remaining part,",
    "start": "475180",
    "end": "479530"
  },
  {
    "text": "and then applying the\nfitted model to the data",
    "start": "479530",
    "end": "484300"
  },
  {
    "text": "that we've held out.",
    "start": "484300",
    "end": "485240"
  },
  {
    "text": "And this is called validation or\ncross-validation, as we'll see.",
    "start": "485240",
    "end": "490259"
  },
  {
    "text": "So let's first of all talk\nabout the validation approach.",
    "start": "490260",
    "end": "493010"
  },
  {
    "start": "491000",
    "end": "538000"
  },
  {
    "text": "So here the simple\nidea is basically",
    "start": "493010",
    "end": "495990"
  },
  {
    "text": "we're going to divide the\ndata into two parts at random,",
    "start": "495990",
    "end": "498810"
  },
  {
    "text": "approximately of equal size.",
    "start": "498810",
    "end": "500565"
  },
  {
    "text": "We'll call the first\npart of the training set",
    "start": "500565",
    "end": "502440"
  },
  {
    "text": "and the second part, the\nvalidation or holdout set.",
    "start": "502440",
    "end": "504760"
  },
  {
    "text": "So the idea is simple.",
    "start": "504760",
    "end": "505930"
  },
  {
    "text": "We take the model, we fit\nit on the training half,",
    "start": "505930",
    "end": "508800"
  },
  {
    "text": "and then we apply\nthe fitted model",
    "start": "508800",
    "end": "511020"
  },
  {
    "text": "to the other half, the\nvalidation or holdout set.",
    "start": "511020",
    "end": "513390"
  },
  {
    "text": "And we record the error that\nwe get on the holdout set.",
    "start": "513390",
    "end": "516419"
  },
  {
    "text": "And that error, the\nvalidation set error",
    "start": "516419",
    "end": "518610"
  },
  {
    "text": "provides us a good\nidea of the test error.",
    "start": "518610",
    "end": "521678"
  },
  {
    "text": "Well, at least some\nidea of the test error.",
    "start": "521679",
    "end": "525372"
  },
  {
    "text": "And the error we'll\nmeasure by mean",
    "start": "525372",
    "end": "526830"
  },
  {
    "text": "squared error in the case\nof quantitative response",
    "start": "526830",
    "end": "529230"
  },
  {
    "text": "or misclassification\nerror rate in the case",
    "start": "529230",
    "end": "532779"
  },
  {
    "text": "of a qualitative or discrete\nresponse classification.",
    "start": "532780",
    "end": "536100"
  },
  {
    "text": "So just to make that clear,\nlet's look at the next slide.",
    "start": "536100",
    "end": "538750"
  },
  {
    "start": "538000",
    "end": "595000"
  },
  {
    "text": "We have our data set here.",
    "start": "538750",
    "end": "540400"
  },
  {
    "text": "I've divided it\nin two at random.",
    "start": "540400",
    "end": "543275"
  },
  {
    "text": "The blue part on the\nleft is the training set",
    "start": "543275",
    "end": "545150"
  },
  {
    "text": "and the orange part or\npink part on the right",
    "start": "545150",
    "end": "548030"
  },
  {
    "text": "is the validation\nor holdout set.",
    "start": "548030",
    "end": "550140"
  },
  {
    "text": "These observations, for example,\n7, 22, and 13 and more of them",
    "start": "550140",
    "end": "553700"
  },
  {
    "text": "are at random chosen to\nbe in the training set.",
    "start": "553700",
    "end": "557390"
  },
  {
    "text": "Observation 91 was, at random,\nchosen to be in the holdout set.",
    "start": "557390",
    "end": "560700"
  },
  {
    "text": "We fit the model\nto the blue part",
    "start": "560700",
    "end": "562640"
  },
  {
    "text": "and then we apply it and\npredict the observations",
    "start": "562640",
    "end": "565130"
  },
  {
    "text": "in the pink part.",
    "start": "565130",
    "end": "567080"
  },
  {
    "text": "And that's validation or\nwe might call that two-fold",
    "start": "567080",
    "end": "570380"
  },
  {
    "text": "validation where we--",
    "start": "570380",
    "end": "571760"
  },
  {
    "text": "well, I shouldn't call it\ntwo-fold because as we'll see,",
    "start": "571760",
    "end": "574190"
  },
  {
    "text": "we don't cross over.",
    "start": "574190",
    "end": "575490"
  },
  {
    "text": "This is simply a\none-stage process.",
    "start": "575490",
    "end": "577770"
  },
  {
    "text": "We divide it in half,\ntrain on one half,",
    "start": "577770",
    "end": "580370"
  },
  {
    "text": "and predict on the other half.",
    "start": "580370",
    "end": "582390"
  },
  {
    "text": "That seems a little\nwasteful if you've",
    "start": "582390",
    "end": "584430"
  },
  {
    "text": "got a very small data set, Rob.",
    "start": "584430",
    "end": "586050"
  },
  {
    "text": "Yeah, that is wasteful.",
    "start": "586050",
    "end": "587260"
  },
  {
    "text": "And as we'll see,\ncross-validation",
    "start": "587260",
    "end": "590280"
  },
  {
    "text": "will remove that waste\nand be more efficient.",
    "start": "590280",
    "end": "593010"
  },
  {
    "text": "But let's first of all\nsee how this works.",
    "start": "593010",
    "end": "596630"
  },
  {
    "start": "595000",
    "end": "794000"
  },
  {
    "text": "In the auto data, recall, we're\ncomparing the linear model",
    "start": "596630",
    "end": "602510"
  },
  {
    "text": "to higher order\npolynomials in regression.",
    "start": "602510",
    "end": "605000"
  },
  {
    "text": "And we had 392\nobservations divided up",
    "start": "605000",
    "end": "607490"
  },
  {
    "text": "into-- so we divide up into two\nparts at random, 196 in one part",
    "start": "607490",
    "end": "611839"
  },
  {
    "text": "and 196 in the other part.",
    "start": "611840",
    "end": "614690"
  },
  {
    "text": "First part being the training\nset and the other part",
    "start": "614690",
    "end": "617510"
  },
  {
    "text": "being the validation set.",
    "start": "617510",
    "end": "618775"
  },
  {
    "start": "618775",
    "end": "621740"
  },
  {
    "text": "If we do this once,\ndo a single split",
    "start": "621740",
    "end": "625330"
  },
  {
    "text": "and we record the\nmean square error,",
    "start": "625330",
    "end": "627670"
  },
  {
    "text": "we get this red curve as\na function of the degree",
    "start": "627670",
    "end": "630610"
  },
  {
    "text": "of the polynomial.",
    "start": "630610",
    "end": "631579"
  },
  {
    "text": "So a linear fits on\nthe left and quadratic",
    "start": "631580",
    "end": "634510"
  },
  {
    "text": "is here, et cetera, as we\nincrease the polynomial order.",
    "start": "634510",
    "end": "637210"
  },
  {
    "text": "And we see the minimum seems\nto occur maybe around 2.",
    "start": "637210",
    "end": "640840"
  },
  {
    "text": "Well, it's rising a bit and\ncoming down a bit after that.",
    "start": "640840",
    "end": "643915"
  },
  {
    "text": "But it's pretty\nflat after about 2.",
    "start": "643915",
    "end": "645740"
  },
  {
    "text": "So it looks like\na linear model--",
    "start": "645740",
    "end": "647890"
  },
  {
    "text": "or actually, a quadratic model,\nexcuse me, is probably the best.",
    "start": "647890",
    "end": "651170"
  },
  {
    "text": "And after that we're not getting\nmuch gain, if any at all.",
    "start": "651170",
    "end": "654160"
  },
  {
    "text": "But look what happens\nwhen we repeat",
    "start": "654160",
    "end": "655660"
  },
  {
    "text": "this process with more and more\nsplits at random into two parts.",
    "start": "655660",
    "end": "659180"
  },
  {
    "text": "We get a lot of\nvariability, right?",
    "start": "659180",
    "end": "662320"
  },
  {
    "text": "The minimums does tend to\noccur around 2 generally.",
    "start": "662320",
    "end": "667880"
  },
  {
    "text": "But look at the error, it's\nvarying from about 16 up to 24",
    "start": "667880",
    "end": "671330"
  },
  {
    "text": "depending on the split.",
    "start": "671330",
    "end": "672710"
  },
  {
    "text": "So this is a consequence\nprobably of the fact",
    "start": "672710",
    "end": "676340"
  },
  {
    "text": "that we divided the\ndata up into two parts.",
    "start": "676340",
    "end": "678180"
  },
  {
    "text": "And when you divide data in two,\nyou get a lot of variability",
    "start": "678180",
    "end": "682370"
  },
  {
    "text": "depending on the split.",
    "start": "682370",
    "end": "683580"
  },
  {
    "text": "The training set, for\nexample, is half as big",
    "start": "683580",
    "end": "685790"
  },
  {
    "text": "as it was originally.",
    "start": "685790",
    "end": "688190"
  },
  {
    "text": "It's interesting, Rob, that even\nthough you get that variability,",
    "start": "688190",
    "end": "691100"
  },
  {
    "text": "it seems to often have\nthis pattern where",
    "start": "691100",
    "end": "693199"
  },
  {
    "text": "the shape of the curves are\nmuch the same but the height,",
    "start": "693200",
    "end": "696440"
  },
  {
    "text": "the level hops around.",
    "start": "696440",
    "end": "699080"
  },
  {
    "text": "So that's a good point.",
    "start": "699080",
    "end": "700410"
  },
  {
    "text": "And it actually\nreminds me to say",
    "start": "700410",
    "end": "702079"
  },
  {
    "text": "there's two things we want\nto use validation for,",
    "start": "702080",
    "end": "704540"
  },
  {
    "text": "cross-validation,\nboth to pick the best",
    "start": "704540",
    "end": "706910"
  },
  {
    "text": "size of the model, in this\ncase the degree of polynomial,",
    "start": "706910",
    "end": "709324"
  },
  {
    "text": "and also to give us an idea of\nhow good the error is to give us",
    "start": "709325",
    "end": "711950"
  },
  {
    "text": "an idea of the actual\ntest error at the end",
    "start": "711950",
    "end": "713742"
  },
  {
    "text": "of the fitting process.",
    "start": "713742",
    "end": "715950"
  },
  {
    "text": "So this breaking\nup into two parts",
    "start": "715950",
    "end": "721010"
  },
  {
    "text": "is successful at\nthe first thing,",
    "start": "721010",
    "end": "722510"
  },
  {
    "text": "as Trevor just mentioned,\nthe minimum around 2",
    "start": "722510",
    "end": "724550"
  },
  {
    "text": "pretty consistently.",
    "start": "724550",
    "end": "726330"
  },
  {
    "text": "So that seems to be OK.",
    "start": "726330",
    "end": "727990"
  },
  {
    "text": "But the actual level of\nthe curve is varying a lot.",
    "start": "727990",
    "end": "730450"
  },
  {
    "text": "So it wouldn't be so good at\ntelling this idea of the error",
    "start": "730450",
    "end": "732908"
  },
  {
    "text": "because we get a very\nwide range here in error.",
    "start": "732908",
    "end": "736100"
  },
  {
    "text": "So I've said a little\nbit of the first point",
    "start": "736100",
    "end": "740850"
  },
  {
    "text": "already, that this\nmethod is highly variable",
    "start": "740850",
    "end": "745009"
  },
  {
    "text": "because we're splitting\ninto two parts",
    "start": "745010",
    "end": "746630"
  },
  {
    "text": "and there's lots of ways of\nsplitting it into two parts.",
    "start": "746630",
    "end": "750242"
  },
  {
    "text": "And because we're\nsplitting in two,",
    "start": "750242",
    "end": "751700"
  },
  {
    "text": "we're losing a lot of the\npower in the training set.",
    "start": "751700",
    "end": "755220"
  },
  {
    "text": "We're throwing away half the\ndata each time in training.",
    "start": "755220",
    "end": "757779"
  },
  {
    "start": "757780",
    "end": "761920"
  },
  {
    "text": "And another\nconsequence of that is",
    "start": "761920",
    "end": "765800"
  },
  {
    "text": "that you remember one of\nour original questions",
    "start": "765800",
    "end": "768560"
  },
  {
    "text": "was, well, what's the\nbest sized model to pick?",
    "start": "768560",
    "end": "772095"
  },
  {
    "text": "Second of all, how\nwell does that model",
    "start": "772095",
    "end": "773720"
  },
  {
    "text": "do in terms of test error?",
    "start": "773720",
    "end": "775339"
  },
  {
    "text": "Our training sets\nhere are half as big",
    "start": "775340",
    "end": "777050"
  },
  {
    "text": "as our original training set.",
    "start": "777050",
    "end": "778470"
  },
  {
    "text": "If you go back to\nthis previous slide,",
    "start": "778470",
    "end": "781519"
  },
  {
    "text": "we've split the data in half.",
    "start": "781520",
    "end": "783440"
  },
  {
    "text": "We actually want the test error\nfor a training set of size n,",
    "start": "783440",
    "end": "786830"
  },
  {
    "text": "we're getting an\nidea of test error",
    "start": "786830",
    "end": "788360"
  },
  {
    "text": "for a training set\nof size n over 2.",
    "start": "788360",
    "end": "792430"
  },
  {
    "text": "And that's likely to be quite\na bit higher than the error",
    "start": "792430",
    "end": "798310"
  },
  {
    "start": "794000",
    "end": "842000"
  },
  {
    "text": "for a training set of size n.",
    "start": "798310",
    "end": "802110"
  },
  {
    "text": "Why, Rob?",
    "start": "802110",
    "end": "802690"
  },
  {
    "text": "Why?",
    "start": "802690",
    "end": "803230"
  },
  {
    "text": "[CHUCKLES] That was my question.",
    "start": "803230",
    "end": "805779"
  },
  {
    "text": "Well, that's because in\ngeneral, the more data one has,",
    "start": "805780",
    "end": "808450"
  },
  {
    "text": "the lower the error.",
    "start": "808450",
    "end": "809650"
  },
  {
    "text": "If I offer you a choice\nof would you rather",
    "start": "809650",
    "end": "811660"
  },
  {
    "text": "have 100 observations\nor 200 observations,",
    "start": "811660",
    "end": "813920"
  },
  {
    "text": "you'd generally like\n200 observations",
    "start": "813920",
    "end": "815620"
  },
  {
    "text": "to train on because\nthe more data, the more",
    "start": "815620",
    "end": "817690"
  },
  {
    "text": "information you have and in\ngeneral, the lower your error",
    "start": "817690",
    "end": "820340"
  },
  {
    "text": "is.",
    "start": "820340",
    "end": "820840"
  },
  {
    "text": "So if your training\nset's only half",
    "start": "820840",
    "end": "822580"
  },
  {
    "text": "as big as your original training\nset, as it is in this situation,",
    "start": "822580",
    "end": "826270"
  },
  {
    "text": "the error that it\nyields is probably",
    "start": "826270",
    "end": "829240"
  },
  {
    "text": "going to be higher than the\nactual error you want to get.",
    "start": "829240",
    "end": "832760"
  },
  {
    "text": "So as we can see, there are\nsome drawbacks of validation.",
    "start": "832760",
    "end": "836280"
  },
  {
    "text": "In the next section, we'll\ntalk about cross-validation",
    "start": "836280",
    "end": "838610"
  },
  {
    "text": "which will help to remove\nsome of these drawbacks.",
    "start": "838610",
    "end": "841500"
  },
  {
    "start": "841500",
    "end": "842000"
  }
]