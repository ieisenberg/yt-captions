[
  {
    "start": "0",
    "end": "20000"
  },
  {
    "start": "0",
    "end": "4348"
  },
  {
    "text": "CHRIS POTTS: Welcome, everyone.",
    "start": "4348",
    "end": "5640"
  },
  {
    "text": "This is part 4 in our series\non Natural Language Inference.",
    "start": "5640",
    "end": "8000"
  },
  {
    "text": "We're going to be talking about\ndifferent modeling strategies.",
    "start": "8000",
    "end": "10380"
  },
  {
    "text": "You might think of this\nscreencast as a companion",
    "start": "10380",
    "end": "12590"
  },
  {
    "text": "to the associated\nnotebook that explores",
    "start": "12590",
    "end": "14810"
  },
  {
    "text": "a lot of different\nmodeling ideas",
    "start": "14810",
    "end": "16490"
  },
  {
    "text": "and suggests some architectural\nvariations that you",
    "start": "16490",
    "end": "18710"
  },
  {
    "text": "might explore yourself.",
    "start": "18710",
    "end": "20554"
  },
  {
    "start": "20000",
    "end": "180000"
  },
  {
    "text": "I thought we would start by just\nconsidering hand-built feature",
    "start": "20555",
    "end": "23180"
  },
  {
    "text": "functions because they can still\nbe quite powerful for the NLI",
    "start": "23180",
    "end": "26420"
  },
  {
    "text": "problem.",
    "start": "26420",
    "end": "27600"
  },
  {
    "text": "So some standard\nhand-built feature ideas",
    "start": "27600",
    "end": "29810"
  },
  {
    "text": "include, as a kind\nof baseline, word",
    "start": "29810",
    "end": "32149"
  },
  {
    "text": "overlap between the\npremise and hypothesis.",
    "start": "32150",
    "end": "34670"
  },
  {
    "text": "This is giving you a kind of\npretty small feature space,",
    "start": "34670",
    "end": "38600"
  },
  {
    "text": "essentially measuring how the\npremise and hypothesis are",
    "start": "38600",
    "end": "41600"
  },
  {
    "text": "alike.",
    "start": "41600",
    "end": "43149"
  },
  {
    "text": "If you want to\nstep it up a level,",
    "start": "43150",
    "end": "44740"
  },
  {
    "text": "you could consider the word\ncross-product features,",
    "start": "44740",
    "end": "47200"
  },
  {
    "text": "which you just consider\nas its feature space",
    "start": "47200",
    "end": "49720"
  },
  {
    "text": "every pairing of a word\nin the premise and a word",
    "start": "49720",
    "end": "52300"
  },
  {
    "text": "in the hypothesis.",
    "start": "52300",
    "end": "53719"
  },
  {
    "text": "This will give you a\nmassive feature space.",
    "start": "53720",
    "end": "56290"
  },
  {
    "text": "Very large.",
    "start": "56290",
    "end": "57220"
  },
  {
    "text": "Very sparse, but the\nintuition behind it",
    "start": "57220",
    "end": "59648"
  },
  {
    "text": "might be that you're\nallowing your model a chance",
    "start": "59648",
    "end": "61690"
  },
  {
    "text": "to discover points of\nalignment, and disalignment",
    "start": "61690",
    "end": "64900"
  },
  {
    "text": "between the premise\nand the hypothesis.",
    "start": "64900",
    "end": "67150"
  },
  {
    "text": "And so that could\nbe very powerful.",
    "start": "67150",
    "end": "69680"
  },
  {
    "text": "You might also consider\nadditional WordNet relations,",
    "start": "69680",
    "end": "72490"
  },
  {
    "text": "bringing those in.",
    "start": "72490",
    "end": "73900"
  },
  {
    "text": "These would be things\nlike entailment,",
    "start": "73900",
    "end": "75490"
  },
  {
    "text": "and contradiction,\nantonomy, synonymy.",
    "start": "75490",
    "end": "78400"
  },
  {
    "text": "And those of course,\ncould be nicely keyed",
    "start": "78400",
    "end": "80230"
  },
  {
    "text": "into the underlying\nlogic of the NLI problem.",
    "start": "80230",
    "end": "83970"
  },
  {
    "text": "Edit distance is another\ncommon feature and just",
    "start": "83970",
    "end": "86880"
  },
  {
    "text": "a raw float valued between\npremise and hypothesis",
    "start": "86880",
    "end": "89790"
  },
  {
    "text": "as a kind of high level way\nof comparing those two texts.",
    "start": "89790",
    "end": "93930"
  },
  {
    "text": "Word differences might\nbe a nice juxtaposition",
    "start": "93930",
    "end": "96660"
  },
  {
    "text": "with word overlap.",
    "start": "96660",
    "end": "97627"
  },
  {
    "text": "You could be considering\nways in which",
    "start": "97627",
    "end": "99210"
  },
  {
    "text": "the premise and the\nhypothesis contrast",
    "start": "99210",
    "end": "101460"
  },
  {
    "text": "with each other in\nthat feature space.",
    "start": "101460",
    "end": "104170"
  },
  {
    "text": "And we have had also moved\nto alignment-based features.",
    "start": "104170",
    "end": "106659"
  },
  {
    "text": "I mentioned that\nword cross-product",
    "start": "106660",
    "end": "108250"
  },
  {
    "text": "is kind of an attempt\nto have the model learn",
    "start": "108250",
    "end": "110800"
  },
  {
    "text": "points of alignment between\nthe premise and hypothesis.",
    "start": "110800",
    "end": "113410"
  },
  {
    "text": "But of course, we can also\ndo some things kind of",
    "start": "113410",
    "end": "115960"
  },
  {
    "text": "even before we begin to\nlearn feature rates, trying",
    "start": "115960",
    "end": "118600"
  },
  {
    "text": "to figure out which\npieces in the premise",
    "start": "118600",
    "end": "120490"
  },
  {
    "text": "correspond to which\npieces in the hypothesis.",
    "start": "120490",
    "end": "124060"
  },
  {
    "text": "We could consider negation.",
    "start": "124060",
    "end": "125380"
  },
  {
    "text": "We've seen of course that that's\nan important indicator function",
    "start": "125380",
    "end": "128110"
  },
  {
    "text": "in a lot of NLI data sets.",
    "start": "128110",
    "end": "130000"
  },
  {
    "text": "There's a powerful\nintuition behind that,",
    "start": "130000",
    "end": "131900"
  },
  {
    "text": "especially as it pertains\nto contradiction.",
    "start": "131900",
    "end": "134025"
  },
  {
    "text": "And so maybe, we would\nwrite some feature functions",
    "start": "134025",
    "end": "136150"
  },
  {
    "text": "that were explicitly\nkeyed in to the presence",
    "start": "136150",
    "end": "138549"
  },
  {
    "text": "or absence of negation\nin various spots",
    "start": "138550",
    "end": "140800"
  },
  {
    "text": "in the premise and hypothesis.",
    "start": "140800",
    "end": "143590"
  },
  {
    "text": "And we can also step\nthat up a level as well",
    "start": "143590",
    "end": "145569"
  },
  {
    "text": "and consider, more\ngenerally, all kinds",
    "start": "145570",
    "end": "147790"
  },
  {
    "text": "of different interesting\nquantifier relationships that",
    "start": "147790",
    "end": "150219"
  },
  {
    "text": "would hold possibly at the level\nof an alignment as in item 6",
    "start": "150220",
    "end": "154000"
  },
  {
    "text": "here between the\npremise and hypothesis.",
    "start": "154000",
    "end": "156520"
  },
  {
    "text": "And this is kind of keyed into\nthe underlying logic of the NLI",
    "start": "156520",
    "end": "160420"
  },
  {
    "text": "problem.",
    "start": "160420",
    "end": "161830"
  },
  {
    "text": "And then finally, named\nentity recognition.",
    "start": "161830",
    "end": "163990"
  },
  {
    "text": "We've seen that\nthese features might",
    "start": "163990",
    "end": "165850"
  },
  {
    "text": "be important in\nfiguring out which",
    "start": "165850",
    "end": "167650"
  },
  {
    "text": "entities co-refer across\nthe premise and hypotheses.",
    "start": "167650",
    "end": "170930"
  },
  {
    "text": "And so having some devices\nfor figuring that out",
    "start": "170930",
    "end": "173709"
  },
  {
    "text": "could be useful as a kind\nof low level grounding",
    "start": "173710",
    "end": "176830"
  },
  {
    "text": "for your system.",
    "start": "176830",
    "end": "177610"
  },
  {
    "start": "177610",
    "end": "180830"
  },
  {
    "start": "180000",
    "end": "582000"
  },
  {
    "text": "Now let's move\ninto a mode that's",
    "start": "180830",
    "end": "182710"
  },
  {
    "text": "more like the deep learning\nmode because as we saw earlier",
    "start": "182710",
    "end": "185980"
  },
  {
    "text": "in this screencast series,\nthese models have proven that",
    "start": "185980",
    "end": "188530"
  },
  {
    "text": "at this point to be the most\npowerful models for the NLI",
    "start": "188530",
    "end": "191680"
  },
  {
    "text": "problem.",
    "start": "191680",
    "end": "192230"
  },
  {
    "text": "So it's productive to think also\nabout different deep learning",
    "start": "192230",
    "end": "194890"
  },
  {
    "text": "architectures.",
    "start": "194890",
    "end": "195608"
  },
  {
    "text": "And I'd like to start with\nwhat I've called here,",
    "start": "195608",
    "end": "197650"
  },
  {
    "text": "sentence encoding models.",
    "start": "197650",
    "end": "199450"
  },
  {
    "text": "And the most basic\nform of that would",
    "start": "199450",
    "end": "201430"
  },
  {
    "text": "return to the idea of\ndistributed representations",
    "start": "201430",
    "end": "204250"
  },
  {
    "text": "as features.",
    "start": "204250",
    "end": "205600"
  },
  {
    "text": "So the idea here is that we\nhave in this diagram the premise",
    "start": "205600",
    "end": "209110"
  },
  {
    "text": "and the hypothesis.",
    "start": "209110",
    "end": "210590"
  },
  {
    "text": "So the premise is,\n\"Every dog danced,\"",
    "start": "210590",
    "end": "212300"
  },
  {
    "text": "then the hypothesis is,\n\"Every poodle moved.\"",
    "start": "212300",
    "end": "215140"
  },
  {
    "text": "And our approach using\ndistributed representations,",
    "start": "215140",
    "end": "217990"
  },
  {
    "text": "the simplest one,\nwould be that we're",
    "start": "217990",
    "end": "219910"
  },
  {
    "text": "going to simply look\nup all of those words",
    "start": "219910",
    "end": "221890"
  },
  {
    "text": "in some fixed embedding space,\nwhich would be like a GloVe",
    "start": "221890",
    "end": "224560"
  },
  {
    "text": "embedding space, for example.",
    "start": "224560",
    "end": "226502"
  },
  {
    "text": "And then we're\ngoing to separately",
    "start": "226503",
    "end": "227920"
  },
  {
    "text": "encode the premise and\nhypothesis by, for example,",
    "start": "227920",
    "end": "230950"
  },
  {
    "text": "doing the sum or\naverage of the vectors",
    "start": "230950",
    "end": "234129"
  },
  {
    "text": "in each of those two texts.",
    "start": "234130",
    "end": "235720"
  },
  {
    "text": "And so that gives us\na vector xp and xh.",
    "start": "235720",
    "end": "239200"
  },
  {
    "text": "And then we might\nconcatenate those two",
    "start": "239200",
    "end": "241690"
  },
  {
    "text": "or do something, some\nother kind of comparison",
    "start": "241690",
    "end": "243940"
  },
  {
    "text": "like difference,\nor max, or mean,",
    "start": "243940",
    "end": "246820"
  },
  {
    "text": "to get a single\nfixed dimensional",
    "start": "246820",
    "end": "248440"
  },
  {
    "text": "representation x that is\nthen the input to a kind of,",
    "start": "248440",
    "end": "251890"
  },
  {
    "text": "could be a simple\nsoftmax classifier.",
    "start": "251890",
    "end": "254620"
  },
  {
    "text": "So all we've done here\nis take our old approach",
    "start": "254620",
    "end": "257350"
  },
  {
    "text": "using distributed\nrepresentations as features",
    "start": "257350",
    "end": "259989"
  },
  {
    "text": "and move it into\nthe NLI problem,",
    "start": "259990",
    "end": "261970"
  },
  {
    "text": "where we have both the\npremise and hypothesis.",
    "start": "261970",
    "end": "264460"
  },
  {
    "text": "And I've called this a\nsentence encoding model",
    "start": "264460",
    "end": "266500"
  },
  {
    "text": "because we are separately\nencoding the two sentences.",
    "start": "266500",
    "end": "269473"
  },
  {
    "text": "And then the model is\ngoing to learn, we hope,",
    "start": "269473",
    "end": "271389"
  },
  {
    "text": "something about how those\ntwo representations interact.",
    "start": "271390",
    "end": "276270"
  },
  {
    "text": "On this slide and the next,\nI've given a complete recipe",
    "start": "276270",
    "end": "279270"
  },
  {
    "text": "for doing exactly\nwhat I just described.",
    "start": "279270",
    "end": "281759"
  },
  {
    "text": "I'm not going to\nlinger over it here",
    "start": "281760",
    "end": "283260"
  },
  {
    "text": "because it's also\nin the notebooks.",
    "start": "283260",
    "end": "284718"
  },
  {
    "text": "And it just shows you how\nusing our course code--",
    "start": "284718",
    "end": "287099"
  },
  {
    "text": "it can be relatively easy\nto set up models like this.",
    "start": "287100",
    "end": "290070"
  },
  {
    "text": "Most of the code is devoted\nto doing the low level",
    "start": "290070",
    "end": "292860"
  },
  {
    "text": "processing of the words\ninto their embedding space.",
    "start": "292860",
    "end": "298879"
  },
  {
    "text": "Here's the rationale for\nsentence-encoding models.",
    "start": "298880",
    "end": "300988"
  },
  {
    "text": "I think this is kind\nof interesting, right?",
    "start": "300988",
    "end": "302780"
  },
  {
    "text": "We might want to encode\nthe premise and hypothesis",
    "start": "302780",
    "end": "305210"
  },
  {
    "text": "separately in order to\ngive the model a chance",
    "start": "305210",
    "end": "308030"
  },
  {
    "text": "to find rich abstract\nrelationships between them.",
    "start": "308030",
    "end": "313130"
  },
  {
    "text": "The sentence encoding approach\nmight also facilitate transfer",
    "start": "313130",
    "end": "316250"
  },
  {
    "text": "to other kinds of tasks,\nright, to the extent",
    "start": "316250",
    "end": "318320"
  },
  {
    "text": "that we are separately\nencoding the two sentences.",
    "start": "318320",
    "end": "320750"
  },
  {
    "text": "We might have sentence\nlevel representations",
    "start": "320750",
    "end": "323480"
  },
  {
    "text": "that are useful\neven for problems",
    "start": "323480",
    "end": "325370"
  },
  {
    "text": "that don't fit into the\nspecific NLI mode of having",
    "start": "325370",
    "end": "328490"
  },
  {
    "text": "a single premise and\na single hypothesis",
    "start": "328490",
    "end": "330860"
  },
  {
    "text": "for the sake of classification.",
    "start": "330860",
    "end": "332185"
  },
  {
    "text": "And that can be\nan important part",
    "start": "332185",
    "end": "333560"
  },
  {
    "text": "of that vision from\nDagan et al that NLI",
    "start": "333560",
    "end": "336410"
  },
  {
    "text": "is a kind of source of\neffective pretraining for more",
    "start": "336410",
    "end": "340010"
  },
  {
    "text": "general problems.",
    "start": "340010",
    "end": "342680"
  },
  {
    "text": "Let's move to a\nmore complex model.",
    "start": "342680",
    "end": "344587"
  },
  {
    "text": "We'll follow the same narrative\nthat we've used before.",
    "start": "344588",
    "end": "346880"
  },
  {
    "text": "We just had that\nsimple fixed model",
    "start": "346880",
    "end": "348608"
  },
  {
    "text": "that was going to\ncombine the premise",
    "start": "348608",
    "end": "350150"
  },
  {
    "text": "and hypothesis via\nsome fixed function,",
    "start": "350150",
    "end": "352490"
  },
  {
    "text": "like sum, or average, or max.",
    "start": "352490",
    "end": "355412"
  },
  {
    "text": "Here, we're going\nto have functions",
    "start": "355412",
    "end": "356870"
  },
  {
    "text": "that learn about how those\ninteractions should happen,",
    "start": "356870",
    "end": "359180"
  },
  {
    "text": "but we're going to follow\nthe sentence encoding mode.",
    "start": "359180",
    "end": "361850"
  },
  {
    "text": "So I have our same example.",
    "start": "361850",
    "end": "363200"
  },
  {
    "text": "\"Every dog danced,\" and,\n\"Every poodle moved.\"",
    "start": "363200",
    "end": "365600"
  },
  {
    "text": "And the idea is that\neach one of those",
    "start": "365600",
    "end": "367520"
  },
  {
    "text": "is processed by its own,\nseparate recurrent neural",
    "start": "367520",
    "end": "370729"
  },
  {
    "text": "network.",
    "start": "370730",
    "end": "371270"
  },
  {
    "text": "And I've indicated in green that\nalthough these two models would",
    "start": "371270",
    "end": "374330"
  },
  {
    "text": "have the same structure,\nthese are different parameters",
    "start": "374330",
    "end": "376729"
  },
  {
    "text": "for the premise and hypothesis.",
    "start": "376730",
    "end": "379230"
  },
  {
    "text": "So they function separately and\nthen in the simplest approach,",
    "start": "379230",
    "end": "382190"
  },
  {
    "text": "we would take the final hidden\nrepresentation from each",
    "start": "382190",
    "end": "385310"
  },
  {
    "text": "of those and combine\nthem somehow,",
    "start": "385310",
    "end": "387410"
  },
  {
    "text": "probably would be\na concatenation.",
    "start": "387410",
    "end": "389300"
  },
  {
    "text": "And that would be the input\nto the final classifier",
    "start": "389300",
    "end": "391909"
  },
  {
    "text": "layer or layers that actually\nlearn the NLI problem.",
    "start": "391910",
    "end": "396330"
  },
  {
    "text": "So it's a sentence-encoding\napproach in the sense",
    "start": "396330",
    "end": "398599"
  },
  {
    "text": "that h3 and hd are taken to\nbe kind of separate summary",
    "start": "398600",
    "end": "402410"
  },
  {
    "text": "representations of the premise\nand hypothesis respectively.",
    "start": "402410",
    "end": "405930"
  },
  {
    "text": "And we have a vision that\nthose representations",
    "start": "405930",
    "end": "408470"
  },
  {
    "text": "might be independently useful\neven outside of the NLI",
    "start": "408470",
    "end": "411320"
  },
  {
    "text": "context.",
    "start": "411320",
    "end": "413440"
  },
  {
    "text": "Now, in the associated\nnotebook, nli_02_models,",
    "start": "413440",
    "end": "418690"
  },
  {
    "text": "there are a bunch of different\nimplementations including",
    "start": "418690",
    "end": "421150"
  },
  {
    "text": "a full PyTorch implementation\nusing R PyTorch based classes",
    "start": "421150",
    "end": "426009"
  },
  {
    "text": "of the sentence-encoding\nRNN approach",
    "start": "426010",
    "end": "428110"
  },
  {
    "text": "that I just described to you.",
    "start": "428110",
    "end": "429466"
  },
  {
    "text": "And I thought I would just\nbriefly give you a high level",
    "start": "429467",
    "end": "431800"
  },
  {
    "text": "overview of how that\nmodeling approach works",
    "start": "431800",
    "end": "435237"
  },
  {
    "text": "because they're actually\njust a few moving pieces.",
    "start": "435237",
    "end": "437320"
  },
  {
    "text": "And the rest is kind of low\nlevel implementation details.",
    "start": "437320",
    "end": "440780"
  },
  {
    "text": "So the first thing\nthat you need to do",
    "start": "440780",
    "end": "442389"
  },
  {
    "text": "is modify the dataset class\nso that conceptually, it",
    "start": "442390",
    "end": "447100"
  },
  {
    "text": "is going to create lists\nof pairs of examples",
    "start": "447100",
    "end": "450130"
  },
  {
    "text": "with their lengths and\ntheir associated labels.",
    "start": "450130",
    "end": "452830"
  },
  {
    "text": "By default, the underlying\ncode that we're using",
    "start": "452830",
    "end": "454960"
  },
  {
    "text": "expects one sequence of tokens,\none length, and one label.",
    "start": "454960",
    "end": "458590"
  },
  {
    "text": "And here, we just\nneed to raise that",
    "start": "458590",
    "end": "460270"
  },
  {
    "text": "up so that we have two\nas you can see here.",
    "start": "460270",
    "end": "462220"
  },
  {
    "text": "Every dog danced.",
    "start": "462220",
    "end": "463180"
  },
  {
    "text": "Every poodle moved.",
    "start": "463180",
    "end": "464229"
  },
  {
    "text": "Both happened to have length 3,\nand their label is entailment.",
    "start": "464230",
    "end": "467470"
  },
  {
    "text": "So we make some changes\nto the dataset class",
    "start": "467470",
    "end": "469630"
  },
  {
    "text": "to accommodate that change\nin format, essentially.",
    "start": "469630",
    "end": "473970"
  },
  {
    "text": "Then the core model for this\nis conceptually just two RNNs.",
    "start": "473970",
    "end": "479790"
  },
  {
    "text": "And the forward method\nis just essentially,",
    "start": "479790",
    "end": "482220"
  },
  {
    "text": "bringing those two pieces\ntogether and feeding them",
    "start": "482220",
    "end": "485040"
  },
  {
    "text": "to the subsequent\nclassifier layers.",
    "start": "485040",
    "end": "487110"
  },
  {
    "text": "And so that's very\nconceptually natural,",
    "start": "487110",
    "end": "488889"
  },
  {
    "text": "and it's just down to having\ntwo separate RNNs that you",
    "start": "488890",
    "end": "491700"
  },
  {
    "text": "implement using the raw\nmaterials that are already",
    "start": "491700",
    "end": "494400"
  },
  {
    "text": "there in the code.",
    "start": "494400",
    "end": "496199"
  },
  {
    "text": "And then finally for the actual\ninterface, the TorchRNNSentence",
    "start": "496200",
    "end": "499550"
  },
  {
    "text": "EncoderClassifier,\nthis is basically",
    "start": "499550",
    "end": "502229"
  },
  {
    "text": "unchanged with the\none modification",
    "start": "502230",
    "end": "505020"
  },
  {
    "text": "that you need to change\nthe predict problem",
    "start": "505020",
    "end": "507110"
  },
  {
    "text": "method, the fundamental\nmethod for prediction,",
    "start": "507110",
    "end": "509639"
  },
  {
    "text": "because it too needs to deal\nwith this different dataset",
    "start": "509640",
    "end": "512370"
  },
  {
    "text": "format that we've\nestablished up here.",
    "start": "512370",
    "end": "514502"
  },
  {
    "text": "And that is, again, a\nkind of low level change.",
    "start": "514502",
    "end": "516460"
  },
  {
    "text": "And so what I hope\nyou're seeing is",
    "start": "516460",
    "end": "517918"
  },
  {
    "text": "that the first and third steps\nare kind of managing the data.",
    "start": "517918",
    "end": "522240"
  },
  {
    "text": "And the middle step is\nthe one that actually",
    "start": "522240",
    "end": "524159"
  },
  {
    "text": "modifies the computation graph.",
    "start": "524159",
    "end": "526320"
  },
  {
    "text": "But that step is very intuitive\nbecause we're basically,",
    "start": "526320",
    "end": "528900"
  },
  {
    "text": "just reflecting encode--",
    "start": "528900",
    "end": "530130"
  },
  {
    "text": "our idea that we have\nseparate RNNs for the premise",
    "start": "530130",
    "end": "532890"
  },
  {
    "text": "and hypothesis.",
    "start": "532890",
    "end": "535853"
  },
  {
    "text": "And then finally, I\njust want to mention",
    "start": "535853",
    "end": "537520"
  },
  {
    "text": "that a common approach\nyou see, especially",
    "start": "537520",
    "end": "539270"
  },
  {
    "text": "in the early literature, is a\nsentence-encoding TreeNN that",
    "start": "539270",
    "end": "543100"
  },
  {
    "text": "has exactly the same intuition\nbehind it as the RNNs",
    "start": "543100",
    "end": "546339"
  },
  {
    "text": "that we just looked at, except\nthat the premise and hypothesis",
    "start": "546340",
    "end": "549850"
  },
  {
    "text": "are processed by tree structured\nrecursive neural networks.",
    "start": "549850",
    "end": "554290"
  },
  {
    "text": "And since the\nunderlying dataset often",
    "start": "554290",
    "end": "556690"
  },
  {
    "text": "have full parse representations,\nthis is an avenue",
    "start": "556690",
    "end": "560140"
  },
  {
    "text": "that you could explore.",
    "start": "560140",
    "end": "562130"
  },
  {
    "text": "It can be tricky to\nimplement these efficiently,",
    "start": "562130",
    "end": "564130"
  },
  {
    "text": "but conceptually, it's a kind\nof very natural thing, where",
    "start": "564130",
    "end": "566890"
  },
  {
    "text": "you just repeatedly\nhave a dense layer",
    "start": "566890",
    "end": "569290"
  },
  {
    "text": "at every one of the\nconstituent nodes",
    "start": "569290",
    "end": "571509"
  },
  {
    "text": "on up to a final\nrepresentation here, pb and pd.",
    "start": "571510",
    "end": "575770"
  },
  {
    "text": "That is then then fed\ninto the classifier layer",
    "start": "575770",
    "end": "578080"
  },
  {
    "text": "in just the way we've done\nfor the previous models.",
    "start": "578080",
    "end": "583017"
  },
  {
    "start": "582000",
    "end": "799000"
  },
  {
    "text": "So those are the sentence\nincluding RNNs, now let's move",
    "start": "583017",
    "end": "585350"
  },
  {
    "text": "to a different vision.",
    "start": "585350",
    "end": "586279"
  },
  {
    "text": "And I've called\nthese change models",
    "start": "586280",
    "end": "587780"
  },
  {
    "text": "because they're going\nto just mush together",
    "start": "587780",
    "end": "590300"
  },
  {
    "text": "the premise and hypothesis, as\nopposed to separately encoding",
    "start": "590300",
    "end": "593269"
  },
  {
    "text": "the two.",
    "start": "593270",
    "end": "594170"
  },
  {
    "text": "Of course, the simplest thing\nwe could do in the chain mode",
    "start": "594170",
    "end": "597709"
  },
  {
    "text": "would be to essentially\nignore the fact that we",
    "start": "597710",
    "end": "599960"
  },
  {
    "text": "have two text, the\npremise and hypothesis",
    "start": "599960",
    "end": "602960"
  },
  {
    "text": "and just feed them in\nas one long sequence",
    "start": "602960",
    "end": "605300"
  },
  {
    "text": "into a standard\nrecurrent neural network.",
    "start": "605300",
    "end": "608089"
  },
  {
    "text": "And since that\ninvolves no changes",
    "start": "608090",
    "end": "609860"
  },
  {
    "text": "to any of the code we've been\nusing for RNN classifiers,",
    "start": "609860",
    "end": "612950"
  },
  {
    "text": "it seems like a pretty\nnatural baseline.",
    "start": "612950",
    "end": "615200"
  },
  {
    "text": "And so that's depicted\nhere and actually this",
    "start": "615200",
    "end": "618260"
  },
  {
    "text": "can be surprisingly\neffective, all right?",
    "start": "618260",
    "end": "620570"
  },
  {
    "text": "The rationale for doing this?",
    "start": "620570",
    "end": "622360"
  },
  {
    "text": "In this context here, you could\nsay that the premise is simply",
    "start": "622360",
    "end": "626750"
  },
  {
    "text": "establishing context for\nprocessing the hypothesis.",
    "start": "626750",
    "end": "629902"
  },
  {
    "text": "And that seems like\na very natural notion",
    "start": "629902",
    "end": "631610"
  },
  {
    "text": "of conditioning on one text\nas you process the second one.",
    "start": "631610",
    "end": "635214"
  },
  {
    "text": "And correspondingly, at\nthe level of human language",
    "start": "635215",
    "end": "637340"
  },
  {
    "text": "processing, this might actually\ncorrespond to something",
    "start": "637340",
    "end": "639710"
  },
  {
    "text": "that we do as we read through\na premise hypothesis text,",
    "start": "639710",
    "end": "643190"
  },
  {
    "text": "and figure out what\nthe relationship is.",
    "start": "643190",
    "end": "646470"
  },
  {
    "text": "And here's a simple\nrecipe for doing this.",
    "start": "646470",
    "end": "648990"
  },
  {
    "text": "The one change from the diagram\nthat you might think about",
    "start": "648990",
    "end": "651540"
  },
  {
    "text": "is that I did when\nrepresenting the examples here.",
    "start": "651540",
    "end": "654509"
  },
  {
    "text": "Flatten them, out\nof course, but also,",
    "start": "654510",
    "end": "657090"
  },
  {
    "text": "insert this boundary marker.",
    "start": "657090",
    "end": "658380"
  },
  {
    "text": "That would at least\ngive the model",
    "start": "658380",
    "end": "659797"
  },
  {
    "text": "a chance to learn that\nthere was a separation",
    "start": "659797",
    "end": "661950"
  },
  {
    "text": "happening, some kind of\ntransition between the premise",
    "start": "661950",
    "end": "664380"
  },
  {
    "text": "and hypothesis.",
    "start": "664380",
    "end": "665818"
  },
  {
    "text": "But that's at just the\nlevel of feature causation",
    "start": "665818",
    "end": "667860"
  },
  {
    "text": "and in terms of modeling.",
    "start": "667860",
    "end": "669089"
  },
  {
    "text": "You hardly need to make\nany changes in order",
    "start": "669090",
    "end": "671040"
  },
  {
    "text": "to run this kind of experiment.",
    "start": "671040",
    "end": "674589"
  },
  {
    "text": "We can also think\nabout a modification",
    "start": "674590",
    "end": "676600"
  },
  {
    "text": "that would bring together\nsentence encoding with chains.",
    "start": "676600",
    "end": "679029"
  },
  {
    "text": "And this would be where we have\ntwo sets of RNN parameters--",
    "start": "679030",
    "end": "682850"
  },
  {
    "text": "one for the premise and\none for the hypothesis.",
    "start": "682850",
    "end": "684850"
  },
  {
    "text": "But we nonetheless\nchain them together",
    "start": "684850",
    "end": "686529"
  },
  {
    "text": "instead of separately\nencoding them.",
    "start": "686530",
    "end": "688360"
  },
  {
    "text": "So as before, I have a\npremise RNN in green.",
    "start": "688360",
    "end": "692079"
  },
  {
    "text": "I have a hypothesis\nRNN in purple.",
    "start": "692080",
    "end": "694930"
  },
  {
    "text": "They have the same structure but\ndifferent learned parameters.",
    "start": "694930",
    "end": "698170"
  },
  {
    "text": "And the handoff is essentially,\nthat the initial hidden",
    "start": "698170",
    "end": "701050"
  },
  {
    "text": "state for the hypothesis--",
    "start": "701050",
    "end": "702790"
  },
  {
    "text": "is the final output\nstate for the premise.",
    "start": "702790",
    "end": "705310"
  },
  {
    "text": "And then that way, you\nget a seamless transition",
    "start": "705310",
    "end": "707350"
  },
  {
    "text": "between these two models.",
    "start": "707350",
    "end": "709120"
  },
  {
    "text": "And this would allow\nthe models in space",
    "start": "709120",
    "end": "711100"
  },
  {
    "text": "to learn that premise,\ntokens, and premise sequences,",
    "start": "711100",
    "end": "714730"
  },
  {
    "text": "have a different\nstatus than those",
    "start": "714730",
    "end": "716380"
  },
  {
    "text": "that appear in the hypothesis.",
    "start": "716380",
    "end": "719850"
  },
  {
    "text": "And let me just\nclose by mentioning",
    "start": "719850",
    "end": "721470"
  },
  {
    "text": "a few other strategies because\nthis is by no means exhaustive,",
    "start": "721470",
    "end": "724689"
  },
  {
    "text": "but it's kind of interesting at\nthe high level of architecture",
    "start": "724690",
    "end": "727830"
  },
  {
    "text": "thinking about sentence encoding\nversus these chained models.",
    "start": "727830",
    "end": "732090"
  },
  {
    "text": "So first, the\nTorchRNN Classifier",
    "start": "732090",
    "end": "734690"
  },
  {
    "text": "feeds its hidden state directly\nto the classifier layer.",
    "start": "734690",
    "end": "737420"
  },
  {
    "text": "But we have options like\nbidirectional equals true,",
    "start": "737420",
    "end": "740540"
  },
  {
    "text": "which would use as the summary\nrepresentation-- both the final",
    "start": "740540",
    "end": "743630"
  },
  {
    "text": "and the initial hidden states,\nessentially, and feed those",
    "start": "743630",
    "end": "746480"
  },
  {
    "text": "into the classifier.",
    "start": "746480",
    "end": "747470"
  },
  {
    "text": "So it's a different notion\nof sentence encoding",
    "start": "747470",
    "end": "750709"
  },
  {
    "text": "or of sequence encoding.",
    "start": "750710",
    "end": "753340"
  },
  {
    "text": "And other ideas here, right?",
    "start": "753340",
    "end": "754540"
  },
  {
    "text": "So we could, instead\nof restricting,",
    "start": "754540",
    "end": "756370"
  },
  {
    "text": "just to one or a few\nof the final states",
    "start": "756370",
    "end": "758950"
  },
  {
    "text": "do some kind of pooling\nwith max or mean across all",
    "start": "758950",
    "end": "761530"
  },
  {
    "text": "of the output states.",
    "start": "761530",
    "end": "763270"
  },
  {
    "text": "And different\npooling options can",
    "start": "763270",
    "end": "764860"
  },
  {
    "text": "be combined with\ndifferent versions",
    "start": "764860",
    "end": "766360"
  },
  {
    "text": "of these models, either\nsentence encoding or chained.",
    "start": "766360",
    "end": "769918"
  },
  {
    "text": "We could also, of course,\nhave additional hidden layers",
    "start": "769918",
    "end": "772210"
  },
  {
    "text": "between the classifier\nlayer and the embedding.",
    "start": "772210",
    "end": "775360"
  },
  {
    "text": "I've shown you just one\nfor the sake of simplicity,",
    "start": "775360",
    "end": "777579"
  },
  {
    "text": "but deeper might be better\nespecially for the very large",
    "start": "777580",
    "end": "780460"
  },
  {
    "text": "NLI datasets that we have.",
    "start": "780460",
    "end": "782600"
  },
  {
    "text": "And finally, an important\nsource of innovation",
    "start": "782600",
    "end": "784779"
  },
  {
    "text": "in this and many other\nspaces is the idea",
    "start": "784780",
    "end": "787240"
  },
  {
    "text": "of adding attention\nmechanisms to these models.",
    "start": "787240",
    "end": "789795"
  },
  {
    "text": "And that's such\nan important idea",
    "start": "789795",
    "end": "791170"
  },
  {
    "text": "that I'm going to save it\nfor the next screencast",
    "start": "791170",
    "end": "793690"
  },
  {
    "text": "in the series.",
    "start": "793690",
    "end": "795520"
  },
  {
    "start": "795520",
    "end": "800000"
  }
]