[
  {
    "start": "0",
    "end": "5540"
  },
  {
    "text": "Today we'll be talking\nabout model based RL for multitask learning. And so far we've\ntalked-- all the RL",
    "start": "5540",
    "end": "11396"
  },
  {
    "text": "that we've talked\nabout is model free, and so we'll be talking about\na different class of approaches that you leverage models.",
    "start": "11397",
    "end": "17820"
  },
  {
    "text": "So some logistics. We sent out the\nmid-quarter survey.",
    "start": "17820",
    "end": "23000"
  },
  {
    "text": "This is to supplement the\nhigh resolution feedback that we've already been\ndoing, but we'd really",
    "start": "23000",
    "end": "28010"
  },
  {
    "text": "value your feedback. The feedback that we've\nused in previous quarters kind of helped us\nimprove the course.",
    "start": "28010",
    "end": "34135"
  },
  {
    "text": "It's one of the\nreasons why we're using PyTorch for\nthe assignments this quarter, among a\nnumber of other things.",
    "start": "34135",
    "end": "41300"
  },
  {
    "text": "Also, homework 4 is out and\nit's due Wednesday next week.",
    "start": "41300",
    "end": "47739"
  },
  {
    "text": "And then the optional\nhomework 4 will also go out next week on Wednesday.",
    "start": "47740",
    "end": "53220"
  },
  {
    "text": "Great. So today I'll briefly recap goal\nconditioned RL and relabeling.",
    "start": "53220",
    "end": "58820"
  },
  {
    "text": "This is the topic\nof homework three, and so we'll just do\na quick recap of that",
    "start": "58820",
    "end": "64430"
  },
  {
    "text": "to get on the same page. And then we'll transition\ninto model based RL. And we'll talk about\nhow model based",
    "start": "64430",
    "end": "70550"
  },
  {
    "text": "RL can be used for\nmultitask learning, and then we'll also\nbe talking about model based URL when you\nhave image observations",
    "start": "70550",
    "end": "77000"
  },
  {
    "text": "or other high dimensional\ninputs because that introduces an additional\nchallenge in the model based",
    "start": "77000",
    "end": "82280"
  },
  {
    "text": "setting. And then the goals\nof the lecture will be to understand\nhow to use and implement",
    "start": "82280",
    "end": "88880"
  },
  {
    "text": "some simple model\nbased RL algorithms, to understand how model based\nRL can be used for multitask",
    "start": "88880",
    "end": "94670"
  },
  {
    "text": "reinforcement learning,\nand some of the challenges and strategies for\nperforming model based",
    "start": "94670",
    "end": "100140"
  },
  {
    "text": "RL in high dimensional spaces.  Cool.",
    "start": "100140",
    "end": "105530"
  },
  {
    "text": "So to recap the lecture\non Wednesday last week and Monday this\nweek, we introduced",
    "start": "105530",
    "end": "112280"
  },
  {
    "text": "kind of model free reinforcement\nlearning methods such as policy gradients and Q learning.",
    "start": "112280",
    "end": "117650"
  },
  {
    "text": "And we saw this sort of\nanatomy of an algorithm where you collect data by, for\nexample, running your policy.",
    "start": "117650",
    "end": "123679"
  },
  {
    "text": "Then you fit some form of\nmodel to estimate return, and then you use that\nto improve your policy.",
    "start": "123680",
    "end": "130770"
  },
  {
    "text": "And we saw a couple different\ncases of actually instantiating this kind of framework.",
    "start": "130770",
    "end": "138000"
  },
  {
    "text": "So for example, you could try\nto estimate the return directly by just summing your rewards,\nor you could actually",
    "start": "138000",
    "end": "145130"
  },
  {
    "text": "fit a critic Q function\nto try to estimate your expected return\nfrom your future state--",
    "start": "145130",
    "end": "151610"
  },
  {
    "text": "from your current state. And then once you have this\nestimate of your future return,",
    "start": "151610",
    "end": "158120"
  },
  {
    "text": "you can use that to\neither update your policy using a policy gradient directly\nor by taking the actions that",
    "start": "158120",
    "end": "167090"
  },
  {
    "text": "will maximize your Q value. Now, in this\nlecture, we're going",
    "start": "167090",
    "end": "172160"
  },
  {
    "text": "to be focusing on model based\nRL methods, which will actually incorporate a\ndifferent kind of model in order to improve the policy.",
    "start": "172160",
    "end": "178775"
  },
  {
    "text": " Previously, we also talked about\nwhat a reinforcement learning task is.",
    "start": "178775",
    "end": "184640"
  },
  {
    "text": "A task essentially corresponds\nto a Markov decision process, which has a state space, an\naction space, initial state,",
    "start": "184640",
    "end": "190520"
  },
  {
    "text": "distribution\ndynamics, and reward. And different aspects\nof this problem",
    "start": "190520",
    "end": "196220"
  },
  {
    "text": "can essentially correspond\nto different tasks. Some tasks might share a\nlot of aspects of these",
    "start": "196220",
    "end": "203330"
  },
  {
    "text": "and only have one thing varying. Alternatively, we\ncould view this as having a task identifier\nbe part of the state, where",
    "start": "203330",
    "end": "213739"
  },
  {
    "text": "s bar is your\noriginal state space and zi is your task identifier.",
    "start": "213740",
    "end": "218989"
  },
  {
    "text": "In this case, you\ncan view your policy as essentially conditioning\non your task identifier.",
    "start": "218990",
    "end": "225828"
  },
  {
    "text": "And likewise, also your\nkey function conditioning on the task identifier. zi could be a number\nof different things.",
    "start": "225828",
    "end": "232230"
  },
  {
    "text": "It could be a one hot task ID. It could be a natural language\ndescription of the task. It could also be a desired goal\nstate, like a particular state",
    "start": "232230",
    "end": "240080"
  },
  {
    "text": "that you want to\nbe able to reach. And in this case, this is what's\nreferred to as goal conditioned",
    "start": "240080",
    "end": "245120"
  },
  {
    "text": "RL, which is a special\ncase of multitask reinforcement learning. ",
    "start": "245120",
    "end": "251940"
  },
  {
    "text": "Great. So we also looked\nat this example where we want to hit\na puck into a goal",
    "start": "251940",
    "end": "259040"
  },
  {
    "text": "or either one can\nhit it into a goal or pass it to our teammate. So task 1 is passing and\ntask 2 is shooting goals.",
    "start": "259040",
    "end": "267510"
  },
  {
    "text": "And if you accidentally\nperform a good pass when you're trying\nto shoot a goal, then you can actually store\nthat experience as normal,",
    "start": "267510",
    "end": "274520"
  },
  {
    "text": "but also pretend that\nthat was actually what you were trying\nto do and relabel",
    "start": "274520",
    "end": "280340"
  },
  {
    "text": "that experience with\nthe task identifier of-- ",
    "start": "280340",
    "end": "286910"
  },
  {
    "text": "actually, of task 1. I think that this should\nbe-- that's a typo. Relabel it with task 1 and\nstore it in your buffer.",
    "start": "286910",
    "end": "293610"
  },
  {
    "text": "So this is referred to as\ndata sharing in multitask RL, and also known as\nhindsight relabeling",
    "start": "293610",
    "end": "300140"
  },
  {
    "text": "where we're going to be kind\nof, in hindsight, relabeling the task that we were\nactually doing in order",
    "start": "300140",
    "end": "305570"
  },
  {
    "text": "to improve learning for\nthese different tasks. Or hindsight experience replay.",
    "start": "305570",
    "end": "313960"
  },
  {
    "text": "Great. So we saw this\nalgorithm, which you're going to be implementing in\nhomework three, where you",
    "start": "313960",
    "end": "321400"
  },
  {
    "text": "collect data using some policy. Then you store that data\nin your replay buffer.",
    "start": "321400",
    "end": "327379"
  },
  {
    "text": "And then when you want to\nperform hindsight relabeling, you take your\nexperience in decay",
    "start": "327380",
    "end": "335440"
  },
  {
    "text": "and you relabel the last state. When you choose what the goal is\nor what the task identifier is",
    "start": "335440",
    "end": "342850"
  },
  {
    "text": "for that particular transition\nor for that particular trajectory, you\nwill set the goal",
    "start": "342850",
    "end": "349360"
  },
  {
    "text": "to be the last state\nin that trajectory. ",
    "start": "349360",
    "end": "354390"
  },
  {
    "text": "Great. And so you can see\nthat right here where you have your\nexperience and then your goal is the third\nelement of that tuple, which",
    "start": "354390",
    "end": "361510"
  },
  {
    "text": "is story sT instead\nof the original goal that you were trying\nto accomplish.",
    "start": "361510",
    "end": "367682"
  },
  {
    "text": "Then you'll store that relabeled\ndata in the replay buffer and then update\nyour policy using the data that you have so far.",
    "start": "367683",
    "end": "373570"
  },
  {
    "text": " And you'll iterate and so forth.",
    "start": "373570",
    "end": "378943"
  },
  {
    "text": "There's also other relabeling\nstrategies rather than just relabeling the\ngoal as the last state.",
    "start": "378943",
    "end": "384560"
  },
  {
    "text": "You could also really\nchoose, for example, any state in the trajectory, or\nyou could pick a random state",
    "start": "384560",
    "end": "391310"
  },
  {
    "text": "as well. And in your homework,\nyou'll explore these different strategies. ",
    "start": "391310",
    "end": "398062"
  },
  {
    "text": "And the result of\nthis procedure is that this can help\nwith exploration because if we incidentally\ncollected data that's",
    "start": "398062",
    "end": "404710"
  },
  {
    "text": "useful for other\ntasks, we are sharing that data across\nthe tasks so you don't have to explore every\nsingle task independently.",
    "start": "404710",
    "end": "412618"
  },
  {
    "text": " Cool. So that was a\nbrief recap of goal",
    "start": "412618",
    "end": "418160"
  },
  {
    "text": "conditioned RL Any\nquestions on that before we get to model based RL? ",
    "start": "418160",
    "end": "428450"
  },
  {
    "text": "OK. So in model based\nreinforcement learning, really, the main\nidea here is to learn",
    "start": "428450",
    "end": "433660"
  },
  {
    "text": "a model of the environment. And what I mean by\nthat is we're actually",
    "start": "433660",
    "end": "439360"
  },
  {
    "text": "going to try to learn what\nthe next state is given the current state and action. We're going to actually\nlearn how to predict",
    "start": "439360",
    "end": "445493"
  },
  {
    "text": "the future, basically. And this can be\nuseful because if you can learn a model\nlike this, it often",
    "start": "445493",
    "end": "451090"
  },
  {
    "text": "leads to better data\nefficiency because then you can query that model rather\nthan the true environment.",
    "start": "451090",
    "end": "458620"
  },
  {
    "text": "And you can also reuse this\nmodel across different tasks. And so at a high level,\nthe way that this works",
    "start": "458620",
    "end": "465310"
  },
  {
    "text": "is you try to estimate the\ndistribution of the next state given the current state and\naction with some function f.",
    "start": "465310",
    "end": "474903"
  },
  {
    "text": "And then once you\nhave this model, you use it to plan\nactions to take.",
    "start": "474903",
    "end": "480790"
  },
  {
    "text": "That's kind of the high\nlevel gist of how it works. We try to fit a\nmodel, we try to learn how to predict the\nfuture, and then",
    "start": "480790",
    "end": "486448"
  },
  {
    "text": "we use that model\nwhen choosing actions. ",
    "start": "486448",
    "end": "491800"
  },
  {
    "text": "It's also worth mentioning that\ninstead of planning actions, you could also use\nit to fit a policy.",
    "start": "491800",
    "end": "497148"
  },
  {
    "text": "In this lecture, we're\ngoing to be focusing on using it for planning. But if you're interested\nin learning more about how",
    "start": "497148",
    "end": "504070"
  },
  {
    "text": "to optimize policies\nwith it, I'm happy to discuss that in\noffice hours or something. And one of the things\nthat's nice about this",
    "start": "504070",
    "end": "510880"
  },
  {
    "text": "is, how do you go about\nlearning this model? You can learn it\njust with supervised learning where,\nfor example, you're",
    "start": "510880",
    "end": "517007"
  },
  {
    "text": "trying to minimize something\nlike the mean squared error between your model's predictions\nand the true next state.",
    "start": "517007",
    "end": "524080"
  },
  {
    "text": "And supervised learning\nis pretty nice. We know how to do\nit pretty well. ",
    "start": "524080",
    "end": "532352"
  },
  {
    "text": "OK, cool. And like I mentioned,\nyou can also use it to optimize a policy. So what does this have to do\nwith multitask reinforcement",
    "start": "532352",
    "end": "538839"
  },
  {
    "text": "learning? One thing that's really\nnice about model based RL is that there is a special\nclass of multitask RL problems",
    "start": "538840",
    "end": "545980"
  },
  {
    "text": "that it is actually\nquite amenable to. Which is that, in\nthe general case,",
    "start": "545980",
    "end": "551170"
  },
  {
    "text": "we consider tasks\nthat look like this. But in many situations,\nonly the reward function",
    "start": "551170",
    "end": "557020"
  },
  {
    "text": "is going to be\nvarying across tasks. And you'll have the same\nstate space, action space, and dynamics.",
    "start": "557020",
    "end": "563020"
  },
  {
    "text": "And really today we're\ngoing to be focusing on this special case where each\ntask corresponds to something like this, where you notice that\nonly the reward is indexed by i",
    "start": "563020",
    "end": "571420"
  },
  {
    "text": "and all of the other\naspects of the task are shared across\nall of the tasks. ",
    "start": "571420",
    "end": "579510"
  },
  {
    "text": "So we can think about a few\nexamples, so maybe in robotics. The laws of physics are\nconstant across different parts",
    "start": "579510",
    "end": "587420"
  },
  {
    "text": "of the world, but you\nmight want your robot to perform different objectives,\nlike doing laundry for you,",
    "start": "587420",
    "end": "594030"
  },
  {
    "text": "cooking for you,\nthat sort of stuff. And so in this case,\nthese different tasks",
    "start": "594030",
    "end": "599750"
  },
  {
    "text": "correspond to this\nspecial case where only the reward is changing and\neverything else is constant. ",
    "start": "599750",
    "end": "607430"
  },
  {
    "text": "Another example is maybe\nwe want a virtual assistant or some form of dialogue system\nwhere the user and the language",
    "start": "607430",
    "end": "615200"
  },
  {
    "text": "that it's speaking in our\nconstant across the tasks, but you have\ndifferent objectives. So maybe in some cases, you\nwant it to order food for you.",
    "start": "615200",
    "end": "623660"
  },
  {
    "text": "Maybe you want it to\nprovide emotional support. In this case, also, the\ndynamics and the state space and everything are the same,\nbut the reward function--",
    "start": "623660",
    "end": "631350"
  },
  {
    "text": "like, ultimately, what\nyou want it to accomplish for you, is different. Yeah? Just for the robotics case, what\nif we had two different robots?",
    "start": "631350",
    "end": "640460"
  },
  {
    "text": "Then that would not\nbe the case, right? Yeah. So if your different tasks\ncorrespond to different robots,",
    "start": "640460",
    "end": "647618"
  },
  {
    "text": "if those robots have different\nembodiments or something like that, then these things\nwould vary across tasks.",
    "start": "647618",
    "end": "652670"
  },
  {
    "text": "Yeah.  Cool. And then in autonomous\ndriving, another example",
    "start": "652670",
    "end": "660100"
  },
  {
    "text": "is that usually the\nrules of the road are relatively constant if\nyou're in the same country, for example, but different\npreferences-- users",
    "start": "660100",
    "end": "667990"
  },
  {
    "text": "have different preferences. Some people maybe want\nyou to drive fast, some people might want\nyou to drive smoothly.",
    "start": "667990",
    "end": "674350"
  },
  {
    "text": "Maybe you also want to get\nto different destinations. And so in this case,\nagain, generally",
    "start": "674350",
    "end": "680140"
  },
  {
    "text": "the dynamics are\nthe same, assuming that it's the same car as well.",
    "start": "680140",
    "end": "686390"
  },
  {
    "text": "But what's differing\nis the objective that you're trying\nto accomplish. ",
    "start": "686390",
    "end": "692990"
  },
  {
    "text": "So one thing that\nyou can notice here is that, in this case,\nwhere this is our task,",
    "start": "692990",
    "end": "698560"
  },
  {
    "text": "our sets of tasks\nlook like this. The model is the same\nacross all the tasks. And so estimating\nthe model is actually",
    "start": "698560",
    "end": "705460"
  },
  {
    "text": "a single task learning problem. And what is the-- where the\nmultitasking part comes in",
    "start": "705460",
    "end": "711130"
  },
  {
    "text": "is actually how you use\nthat model to plan actions. And this is pretty nice\nbecause learning the model",
    "start": "711130",
    "end": "718449"
  },
  {
    "text": "or generally learning about\nan MDP can be challenging. And if it's a single\ntask learning problem,",
    "start": "718450",
    "end": "725260"
  },
  {
    "text": "then it's going to be easier\nthan a multitask problem.  So we can fit a single\nmodel for all of the tasks",
    "start": "725260",
    "end": "732670"
  },
  {
    "text": "and then just plan to\naccomplish different tasks using that single model. ",
    "start": "732670",
    "end": "740207"
  },
  {
    "text": "OK, so how do we actually\nuse a model to plan? The gist of the approach\nthat we'll consider",
    "start": "740208",
    "end": "748290"
  },
  {
    "text": "is to optimize your\nactions using the model, and we have this\nobjective right here,",
    "start": "748290",
    "end": "754150"
  },
  {
    "text": "which is to maximize\nour sum of rewards. And we're performing\nthis maximization",
    "start": "754150",
    "end": "760200"
  },
  {
    "text": "over our sequence of\nactions, so we're essentially trying to find a\ncourse of action that",
    "start": "760200",
    "end": "767310"
  },
  {
    "text": "will lead to maximal reward. ",
    "start": "767310",
    "end": "772900"
  },
  {
    "text": "And this optimization is\nreferred to more colloquially as planning in the sense\nthat you're essentially",
    "start": "772900",
    "end": "778500"
  },
  {
    "text": "planning what you\nwant to do in order to accomplish an objective. Yeah?",
    "start": "778500",
    "end": "784216"
  },
  {
    "text": "So in this setting\n[INAUDIBLE],, we're trying to estimate\nthe probability p,",
    "start": "784216",
    "end": "789430"
  },
  {
    "text": "but is there more function\nknown as well beforehand? Or are we controlled\nabout that as well?",
    "start": "789430",
    "end": "794830"
  },
  {
    "text": "Yeah, that's a great question. So the question was, do we\nknow the form or the reward function? We're learning the\ndynamics, do we",
    "start": "794830",
    "end": "800187"
  },
  {
    "text": "know the form of\nthe reward function? In some cases, yes. In some cases, no. I'll talk about the\ntwo cases after we",
    "start": "800187",
    "end": "806870"
  },
  {
    "text": "go through the process. You can assume that we\nknow-- in all these cases, you can assume that we know\nthe functional form of r,",
    "start": "806870",
    "end": "813470"
  },
  {
    "text": "that we can essentially evaluate\nr given a state in action, but we might have known it\nor we might have learned it.",
    "start": "813470",
    "end": "821250"
  },
  {
    "text": "Yeah? So when you say planning, do\nyou mean open loop planning or closed loop planning?",
    "start": "821250",
    "end": "827835"
  },
  {
    "text": "Yeah, great question. So do I mean open loop planning\nor closed loop planning? We're going to start\nwith open loop planning",
    "start": "827835",
    "end": "833780"
  },
  {
    "text": "and then we'll talk about\nclosed loop planning after that. And it's a pretty\nimportant distinction",
    "start": "833780",
    "end": "839330"
  },
  {
    "text": "and it will come up later.  Cool. So how do we actually go about\noptimizing this objective?",
    "start": "839330",
    "end": "848220"
  },
  {
    "text": "So we have our model,\nand you can think of-- you can essentially\nroll out your model,",
    "start": "848220",
    "end": "854779"
  },
  {
    "text": "kind of apply it iteratively\nfrom your current state given your actions to predict\nwhat the future will",
    "start": "854780",
    "end": "861320"
  },
  {
    "text": "look like in a computation\ngraph like this. And then we can also measure\nthe reward given the state",
    "start": "861320",
    "end": "868010"
  },
  {
    "text": "and action at each time step.  And so if we want to\noptimize the actions that",
    "start": "868010",
    "end": "875959"
  },
  {
    "text": "maximize rewards and we know\nour model, then what we can do is we can essentially back\npropagate from the rewards",
    "start": "875960",
    "end": "883850"
  },
  {
    "text": "into the actions and\nessentially run this computation graph-- compute the gradients\nof the rewards with respect",
    "start": "883850",
    "end": "891710"
  },
  {
    "text": "to the actions using\nyour dynamics model. ",
    "start": "891710",
    "end": "896935"
  },
  {
    "text": "And so note here\nthat we're going to be doing gradient\ndescent, but instead of doing gradient descent\non some parameters,",
    "start": "896935",
    "end": "902600"
  },
  {
    "text": "we're going to be doing\ngradient descent on our actions. And so explicitly what this\nwill look like is first",
    "start": "902600",
    "end": "912590"
  },
  {
    "text": "we're going to run some\npolicy to collect some data. This could be a random\npolicy, for example.",
    "start": "912590",
    "end": "919040"
  },
  {
    "text": "Then we'll fit a model using\nthe data from this policy.",
    "start": "919040",
    "end": "924110"
  },
  {
    "text": "This is just done with\nsupervised learning. And then we'll back\npropagate from the reward,",
    "start": "924110",
    "end": "929300"
  },
  {
    "text": "through the model, into the\nactions to choose actions.",
    "start": "929300",
    "end": "934459"
  },
  {
    "text": "And this last step\nhere will essentially pick an initial set of actions\nthat are sampled randomly",
    "start": "934460",
    "end": "943055"
  },
  {
    "text": "and then run gradient\ndescent by iteratively, computing the gradient\nof the action-- of the reward with\nrespect to the action,",
    "start": "943055",
    "end": "948470"
  },
  {
    "text": "applying that gradient to the\naction, and then repeating. ",
    "start": "948470",
    "end": "954100"
  },
  {
    "text": "Yeah? Why is that [INAUDIBLE] from\ncomputing the expected value [INAUDIBLE] before?",
    "start": "954100",
    "end": "961778"
  },
  {
    "text": "Because you have the\nmodel now in that. Yeah, so because\nwe have a model,",
    "start": "961778",
    "end": "967090"
  },
  {
    "text": "if the model is accurate, we\ncan actually leverage the grad-- leverage that functional\nform of that model o back propagate through it.",
    "start": "967090",
    "end": "973100"
  },
  {
    "text": "And this is going to be\nbetter than something like Q learning if your model\nis accurate because the model is",
    "start": "973100",
    "end": "978602"
  },
  {
    "text": "not only going to tell\nyou what the future will look like, it's also\ngoing to tell you the gradients of your action.",
    "start": "978602",
    "end": "985440"
  },
  {
    "text": "How your action affects\nyour state, basically. Yeah? What if the actions were\ndiscrete [INAUDIBLE]",
    "start": "985440",
    "end": "991427"
  },
  {
    "text": "or something like that\nrather than continuous? Yeah. So the question is, what if\nyour actions are discrete rather",
    "start": "991427",
    "end": "997130"
  },
  {
    "text": "than continuous? In that case, I probably\nwouldn't use back propagation.",
    "start": "997130",
    "end": "1002339"
  },
  {
    "text": "And on the approach\n1b, we'll see something that would work\nfor discrete actions.",
    "start": "1002340",
    "end": "1008220"
  },
  {
    "text": "You can also consider trying\nto optimize with respect to some sampling\ndistribution and kind of use",
    "start": "1008220",
    "end": "1013260"
  },
  {
    "text": "a gradient estimator\nto differentiate through the sampling\nprocess from that discrete distribution. But in general, that sort\nof gradient estimation",
    "start": "1013260",
    "end": "1019920"
  },
  {
    "text": "is prone to errors. Yeah? So if you do this\nprocess, like step three,",
    "start": "1019920",
    "end": "1026922"
  },
  {
    "text": "wouldn't you only\nget-- so you'd only get gradients respective\nof actions in the data set? Right?",
    "start": "1026922",
    "end": "1032199"
  },
  {
    "text": "So would that just\ntell you about what would have been better\nactions within that data set?",
    "start": "1032200",
    "end": "1037599"
  },
  {
    "text": "Yeah. So the question is, does this\nonly give you actions with-- sorry. Does this only give you\ngradients with respect",
    "start": "1037599",
    "end": "1043136"
  },
  {
    "text": "to actions in your data set? And the answer is\nno, you can actually",
    "start": "1043137",
    "end": "1048480"
  },
  {
    "text": "get gradients for actions\nthat aren't in your data set. And essentially what\nthis third process",
    "start": "1048480",
    "end": "1054419"
  },
  {
    "text": "corresponds to is first you\npick some initial a1 to h.",
    "start": "1054420",
    "end": "1067170"
  },
  {
    "text": "This could be like\nin neural networks you randomly initialize\nyour parameters. You're going to\nrandomly initialize",
    "start": "1067170",
    "end": "1072558"
  },
  {
    "text": "your action sequence. And then you can basically\ncompute the gradient",
    "start": "1072558",
    "end": "1079740"
  },
  {
    "text": "of your sum of\nrewards with respect to this action sequence.",
    "start": "1079740",
    "end": "1085770"
  },
  {
    "text": "And the way that\nyou can compute this is basically with the chain\nrule where you use dr d--",
    "start": "1085770",
    "end": "1092413"
  },
  {
    "text": "I'm going to assume\nthat the reward only depends on state for now. dr ds and then ds--",
    "start": "1092413",
    "end": "1100080"
  },
  {
    "text": "I guess this is\ntechnically dr d-- like, all of the time steps.",
    "start": "1100080",
    "end": "1105600"
  },
  {
    "text": "s1 to h. And then you're going to\ntake the gradient of this with respect to your actions.",
    "start": "1105600",
    "end": "1114390"
  },
  {
    "text": "1 to h, and this is going to-- this gradient is what's going\nto be using your dynamics model by essentially back\npropagating through that model.",
    "start": "1114390",
    "end": "1123150"
  },
  {
    "text": "This does assume\nthat your model-- the model that you\nlearn is differentiable. Fortunately, things\nlike neural networks",
    "start": "1123150",
    "end": "1128460"
  },
  {
    "text": "you can differentiate\nthrough them. And so you're going\nto be differentiating into the inputs\nof the model, not",
    "start": "1128460",
    "end": "1134269"
  },
  {
    "text": "into the parameters\nof the model. ",
    "start": "1134270",
    "end": "1142350"
  },
  {
    "text": "And then you'll--\nfor completeness, you'll then apply this\ngradient to update.",
    "start": "1142350",
    "end": "1150075"
  },
  {
    "text": " So it'll be 1\nminus the gradient.",
    "start": "1150075",
    "end": "1158527"
  },
  {
    "text": "Something like that. And you'll have\nsome learning rate. Yeah? If you wanted to apply\nanother gradient step,",
    "start": "1158527",
    "end": "1165639"
  },
  {
    "text": "would you have to reroll\nout using these actions or? Yeah. So then when you want to\nthen iterate this process,",
    "start": "1165639",
    "end": "1174640"
  },
  {
    "text": "you will need to then run these\nactions through your model again and back propagate\ninto them again.",
    "start": "1174640",
    "end": "1180270"
  },
  {
    "text": "And the reason why\nthis works is that you can evaluate your model on lots\nof different action sequences,",
    "start": "1180270",
    "end": "1186120"
  },
  {
    "text": "not just the ones in the data. Yeah? I feel like the squared\nerror L2 loss is not true,",
    "start": "1186120",
    "end": "1192125"
  },
  {
    "text": "because it doesn't\nwork in some cases, like if you have L2 loss--",
    "start": "1192125",
    "end": "1199269"
  },
  {
    "text": "irrelevant information in\nstates where the square error L2 loss will punish, [INAUDIBLE]",
    "start": "1199270",
    "end": "1209630"
  },
  {
    "text": "Yeah. So the question is, when\nyou're fitting your model, if there's a lot of\ndimensions of the state that",
    "start": "1209630",
    "end": "1215620"
  },
  {
    "text": "have irrelevant information-- information that's\nirrelevant to the reward-- will the square,\nlike the L2 loss,",
    "start": "1215620",
    "end": "1222287"
  },
  {
    "text": "will that cause\nproblems, essentially? And the model will actually\nhave to model everything in the state.",
    "start": "1222287",
    "end": "1227480"
  },
  {
    "text": "And if there's only a very\ntiny part of the state that is relevant towards the\nreward, then this approach",
    "start": "1227480",
    "end": "1232929"
  },
  {
    "text": "might struggle if it doesn't\nmodel that part of the state well. And so that is one\nof the downsides",
    "start": "1232930",
    "end": "1238420"
  },
  {
    "text": "of model based\napproaches is that there is this mismatch between\nthe goal of fitting a model and the goal of\nfitting the return,",
    "start": "1238420",
    "end": "1246405"
  },
  {
    "text": "especially if there's large\nparts of the state that aren't relevant to the return. ",
    "start": "1246405",
    "end": "1257520"
  },
  {
    "text": "Cool. So gradient based\noptimization is the first way that we can go about\noptimizing for our actions",
    "start": "1257520",
    "end": "1265800"
  },
  {
    "text": "for going about\nplanning, essentially. The second way that\nwe can go about this",
    "start": "1265800",
    "end": "1271290"
  },
  {
    "text": "is with a sampling\nbased optimization. And we're going\nto be still trying to optimize the same objective,\nbut instead of using gradients,",
    "start": "1271290",
    "end": "1281490"
  },
  {
    "text": "we're going to use a gradient\nfree operation or optimization that tries to essentially\njust sample actions",
    "start": "1281490",
    "end": "1287640"
  },
  {
    "text": "and choose the actions\nthat perform the best. So the algorithm itself\nwill look basically the same",
    "start": "1287640",
    "end": "1294059"
  },
  {
    "text": "as before. We're going to run some\npolicy, use a model to fit the dynamics\nfrom that data,",
    "start": "1294060",
    "end": "1302010"
  },
  {
    "text": "and then we're going to perform\nthe sampling based optimization to choose actions. ",
    "start": "1302010",
    "end": "1308730"
  },
  {
    "text": "What do different sampling\nbased optimizations look like? The dumbest approach is to\nbasically just randomly sample",
    "start": "1308730",
    "end": "1315659"
  },
  {
    "text": "and pick the best one. So what this looks like is\nwe'll denote our action sequence",
    "start": "1315660",
    "end": "1321720"
  },
  {
    "text": "with capital A just for brevity. And really, kind of the simplest\napproach that we could do here",
    "start": "1321720",
    "end": "1331050"
  },
  {
    "text": "is we guess and check. So we'll sample a bunch of\ndifferent action sequences,",
    "start": "1331050",
    "end": "1336059"
  },
  {
    "text": "like maybe 1,000 or\n10,000 or something from some distribution,\nlike a uniform distribution.",
    "start": "1336060",
    "end": "1342210"
  },
  {
    "text": "And then we'll just\nchoose the action sequence that maximizes reward. ",
    "start": "1342210",
    "end": "1349860"
  },
  {
    "text": "This is maybe not the smartest\nway to optimize the objective, but if you actually parallelize\nthe sampling process",
    "start": "1349860",
    "end": "1357608"
  },
  {
    "text": "and the evaluation\nprocess, it can actually be super fast, which is nice. It also works with\ndiscrete actions.",
    "start": "1357608",
    "end": "1363270"
  },
  {
    "text": " And yeah.",
    "start": "1363270",
    "end": "1370038"
  },
  {
    "text": "Of course, this also\nhas limitations, which is that if your sampling\ndistribution is very broad",
    "start": "1370038",
    "end": "1375140"
  },
  {
    "text": "or isn't very good, then the\naction sequence that you'll",
    "start": "1375140",
    "end": "1382310"
  },
  {
    "text": "get also won't be very good. Yeah? So what we don't know\n[INAUDIBLE] didn't",
    "start": "1382310",
    "end": "1387950"
  },
  {
    "text": "seem as though they--\nacross multiple steps, anything new has come up,\nand it's literally just",
    "start": "1387950",
    "end": "1394702"
  },
  {
    "text": "doing automatically the\nsame thing again and again. So this is the kind\nof optimization",
    "start": "1394702",
    "end": "1400279"
  },
  {
    "text": "for choosing actions with\nrespect to our model. And we're going to be\nlearning the model,",
    "start": "1400280",
    "end": "1405360"
  },
  {
    "text": "and then after we\nlearn the model we're going to be using\nthis to plan actions with respect to that model. The model is still\nbeing optimized",
    "start": "1405360",
    "end": "1411309"
  },
  {
    "text": "with gradient descent? Yeah. The model is still optimized\nwith gradient descent.",
    "start": "1411310",
    "end": "1416690"
  },
  {
    "text": "Now this is kind of not\nthe smartest approach, and it's referred to\nas random shooting",
    "start": "1416690",
    "end": "1422270"
  },
  {
    "text": "because you are simply\nshooting out different action sequences at random\nand trying to pick the one that works the best.",
    "start": "1422270",
    "end": "1430070"
  },
  {
    "text": "Can we improve\nthis distribution? One version of this algorithm\nthat is slightly better is,",
    "start": "1430070",
    "end": "1437000"
  },
  {
    "text": "instead of just picking\na single distribution and sampling from it, is we\ncould actually iteratively improve that distribution.",
    "start": "1437000",
    "end": "1444330"
  },
  {
    "text": "And this is what's known as\nthe cross entropy method. So we sample-- first start\nby sampling action sequences",
    "start": "1444330",
    "end": "1451460"
  },
  {
    "text": "from some distribution, p. This could initially be\nthe uniform distribution.",
    "start": "1451460",
    "end": "1458809"
  },
  {
    "text": "Then we evaluate the\nsum of rewards for each of those action sequences.",
    "start": "1458810",
    "end": "1464509"
  },
  {
    "text": "We see how well they did. And then what we'll do is we'll\npick the action sequences that",
    "start": "1464510",
    "end": "1472010"
  },
  {
    "text": "had the best reward and\nwe'll refit a distribution",
    "start": "1472010",
    "end": "1477500"
  },
  {
    "text": "to those best action sequences. And then we can essentially\nrepeat this process",
    "start": "1477500",
    "end": "1484070"
  },
  {
    "text": "and then resample from\nthat improved distribution, basically from the distribution\nthat did the best, and then",
    "start": "1484070",
    "end": "1491000"
  },
  {
    "text": "kind of iteratively\nrefine our distribution that we're sampling from. ",
    "start": "1491000",
    "end": "1500590"
  },
  {
    "text": "And kind of in terms\nof some of the details, I think that this process\nis fairly straightforward.",
    "start": "1500590",
    "end": "1506090"
  },
  {
    "text": "You evaluate each of\nyour action sequences and you pick the ones that\nare the best or the elites.",
    "start": "1506090",
    "end": "1511727"
  },
  {
    "text": "In terms of fitting a\ndistribution to this, you could fit a Gaussian\ndistribution to it. You could fit a multi--",
    "start": "1511728",
    "end": "1518770"
  },
  {
    "text": "sorry. A mixture of Gaussians to those\naction sequences or something else.",
    "start": "1518770",
    "end": "1524298"
  },
  {
    "text": "This is probably the\nmost complicated step, but once you do this,\nthen you just resample from that distribution\nand repeat.",
    "start": "1524298",
    "end": "1530230"
  },
  {
    "text": " There are also more\nadvanced methods",
    "start": "1530230",
    "end": "1535370"
  },
  {
    "text": "for sampling based optimization. There's what's called kind\noff evolutionary methods",
    "start": "1535370",
    "end": "1540410"
  },
  {
    "text": "and a method called CMAES.",
    "start": "1540410",
    "end": "1546290"
  },
  {
    "text": "But it turns out that\nactually, in a lot of cases, these two approaches\naren't actually as bad as you might think.",
    "start": "1546290",
    "end": "1553700"
  },
  {
    "text": "Yeah? Because we're refitting to\nthe samples that are related",
    "start": "1553700",
    "end": "1559300"
  },
  {
    "text": "and just the most off of these\ninitial random samples so you kind of get stuck in\nlocal optimum due to that.",
    "start": "1559300",
    "end": "1565982"
  },
  {
    "text": "How do you maintain good\nprobability of cases that you don't get in\nthe initial distribution?",
    "start": "1565982",
    "end": "1573060"
  },
  {
    "text": "Yeah. So you do need to\nget good coverage from your initial distribution. And the question was, do you\nget stuck in a local optimum,",
    "start": "1573060",
    "end": "1578630"
  },
  {
    "text": "potentially? If you have good coverage\nin this initial distribution and your optimization\nlandscape is somewhat smooth,",
    "start": "1578630",
    "end": "1586190"
  },
  {
    "text": "then it shouldn't get stuck. But if, for example, say you\nsample from this distribution",
    "start": "1586190",
    "end": "1595429"
  },
  {
    "text": "and right here is really good\nbut everything around that is not very good. And you draw some samples that\ndon't include that region right",
    "start": "1595430",
    "end": "1604310"
  },
  {
    "text": "there, then you are\ngoing to miss this because maybe there's\na spot over here that's",
    "start": "1604310",
    "end": "1610280"
  },
  {
    "text": "a little bit better. And in that case, you will\nnot hit the global optimum",
    "start": "1610280",
    "end": "1617960"
  },
  {
    "text": "because you didn't-- because you have this\npretty nonsmooth landscape. If you have a smoother\nlandscape where",
    "start": "1617960",
    "end": "1626000"
  },
  {
    "text": "this part is clearly better than\nother parts of the landscape, then it will be easier\nfor it to optimize.",
    "start": "1626000",
    "end": "1631400"
  },
  {
    "text": "That's also true for gradient\nbased methods as well. Gradient based\nmethods can also-- they are a local optimization,\nand if the gradient,",
    "start": "1631400",
    "end": "1638299"
  },
  {
    "text": "for example, points them\ntowards an optimum that isn't as good as another one,\nthen they'll also get stuck. ",
    "start": "1638300",
    "end": "1648799"
  },
  {
    "text": "So to recap, the way that we\nkind of iteratively resample is we sample at random\nto start, and then we",
    "start": "1648800",
    "end": "1655280"
  },
  {
    "text": "pick some subset of those\nsamples that are the best. We refit a distribution\nto those samples",
    "start": "1655280",
    "end": "1660530"
  },
  {
    "text": "and then resample from\nthat distribution. ",
    "start": "1660530",
    "end": "1667320"
  },
  {
    "text": "Cool. So for a sampling\nbased optimization, one of the benefits\nof this is that it's",
    "start": "1667320",
    "end": "1673770"
  },
  {
    "text": "pretty fast, especially\nif you parallelize things. The only thing that's iterative\nis the different iterations",
    "start": "1673770",
    "end": "1679830"
  },
  {
    "text": "of the cross entropy method. Whereas gradient descent\nis fully sequential and you can't\nparallelize it easily.",
    "start": "1679830",
    "end": "1687630"
  },
  {
    "text": "It's also super simple, so\nit's pretty easy to implement. And you don't need\nto worry about-- you don't need to worry as much\nabout the initialization,",
    "start": "1687630",
    "end": "1694323"
  },
  {
    "text": "for example. The downside is that it doesn't\nscale to high dimensions.",
    "start": "1694323",
    "end": "1699720"
  },
  {
    "text": "Fortunately, action\nsequences are usually much lower dimensional than,\nsay, neural network parameters.",
    "start": "1699720",
    "end": "1705550"
  },
  {
    "text": "So this is one\nreason why we would use this kind of optimization\nfor action sequences",
    "start": "1705550",
    "end": "1711029"
  },
  {
    "text": "and we definitely\nwouldn't use it for neural network parameters. Although actually, some people\nhave found it to sometimes work",
    "start": "1711030",
    "end": "1716620"
  },
  {
    "text": "for neural network parameters.  In terms of dimensionality,\nthis includes both the horizon",
    "start": "1716620",
    "end": "1723000"
  },
  {
    "text": "and your action dimensionality. And that's because when\nyou do this optimization, you're optimizing over the\nentire sequence of actions",
    "start": "1723000",
    "end": "1730890"
  },
  {
    "text": "and not just, like, the\nfirst action, for example. And it is important to optimize\nover the entire sequence",
    "start": "1730890",
    "end": "1737710"
  },
  {
    "text": "so that you actually\nconsider long term outcomes. ",
    "start": "1737710",
    "end": "1748800"
  },
  {
    "text": "Cool, so in terms of the\nalgorithm, jumping back up,",
    "start": "1748800",
    "end": "1756840"
  },
  {
    "text": "we're going to be collecting\nsome data, fitting a model, and then improving or\noptimizing for action sequences",
    "start": "1756840",
    "end": "1764120"
  },
  {
    "text": "with respect to that model. For example, using random\nshooting or a cross entropy method or using a gradient\nbased optimization.",
    "start": "1764120",
    "end": "1770179"
  },
  {
    "text": " Cool. So that was essentially\nversion one and really",
    "start": "1770180",
    "end": "1778100"
  },
  {
    "text": "one of the basics\nof model based RL. Now, how can this approach fail?",
    "start": "1778100",
    "end": "1785630"
  },
  {
    "text": "So say, for example,\nyou are here and you want to get up to\nbe as high as possible.",
    "start": "1785630",
    "end": "1791940"
  },
  {
    "text": "You want to get to the top of-- you want to be higher\nso that you can see",
    "start": "1791940",
    "end": "1798378"
  },
  {
    "text": "further or something like that.  You're first going\nto run some policy,",
    "start": "1798378",
    "end": "1803630"
  },
  {
    "text": "and you'll collect data that\nlooks maybe something like this because it's some random policy.",
    "start": "1803630",
    "end": "1808970"
  },
  {
    "text": "Then you'll learn a model\nto fit this data that tells you essentially\nwhat will happen if you",
    "start": "1808970",
    "end": "1817549"
  },
  {
    "text": "go in certain directions. And one thing that you\nmight learn from this data is that if you go towards the\nright, then you can go higher.",
    "start": "1817550",
    "end": "1826850"
  },
  {
    "text": "Because in this region\nwhere you collected data, you always saw that\nif you went right, you got to a higher elevation.",
    "start": "1826850",
    "end": "1833510"
  },
  {
    "text": " And then now you're going\nto optimize for actions.",
    "start": "1833510",
    "end": "1839540"
  },
  {
    "text": "You're going to plan actions\nso that you maximize reward. And so if you want\nto go higher, you'll",
    "start": "1839540",
    "end": "1844909"
  },
  {
    "text": "learn that you should\ngo to the right. And then now one thing\nthat you might notice here",
    "start": "1844910",
    "end": "1850070"
  },
  {
    "text": "is that if you do\nthat, then you'll kind of keep on going right\nand fall off the cliff.",
    "start": "1850070",
    "end": "1856840"
  },
  {
    "text": "Now, what went wrong here? So the issue here is that\nthere's this data distribution",
    "start": "1856840",
    "end": "1866169"
  },
  {
    "text": "mismatch between the data\nthat you fit the model on and the data that you're\nusing the model on--",
    "start": "1866170",
    "end": "1871240"
  },
  {
    "text": "that you're evaluating\nthe model on. And essentially the distribution\nfrom the initial policy",
    "start": "1871240",
    "end": "1877870"
  },
  {
    "text": "on the left is different\nfrom the state distribution that you're actually evaluating\nyour final policy on.",
    "start": "1877870",
    "end": "1885700"
  },
  {
    "text": "And your model didn't see any\ndata falling off the cliff, and so it doesn't know how to\navoid falling off the cliff.",
    "start": "1885700",
    "end": "1890919"
  },
  {
    "text": " Cool.",
    "start": "1890920",
    "end": "1896210"
  },
  {
    "text": "So a thought exercise\nfor all of you. How might you go about\nalleviating this issue? ",
    "start": "1896210",
    "end": "1903889"
  },
  {
    "text": "Yeah? [INAUDIBLE] random,\nyou intentionally try to explore as\nmuch as possible.",
    "start": "1903890",
    "end": "1910620"
  },
  {
    "text": "Yeah. So the one thing that you could\ndo is in your initial policy, try to explore as much\nas possible so that you can see a much\nbroader distribution",
    "start": "1910620",
    "end": "1917420"
  },
  {
    "text": "and so that you can cover more. That's a good idea. Yeah? You can stream your\npolicy by [INAUDIBLE]..",
    "start": "1917420",
    "end": "1926864"
  },
  {
    "text": " Yeah. So when you optimize\nfor actions,",
    "start": "1926864",
    "end": "1933313"
  },
  {
    "text": "you could optimize not just\nfor maximizing reward but also staying close to\nyour initial policy.",
    "start": "1933313",
    "end": "1939750"
  },
  {
    "text": "Other thoughts?  Yeah? One question.",
    "start": "1939750",
    "end": "1945545"
  },
  {
    "text": "[INAUDIBLE]?  Sorry, can you repeat that?",
    "start": "1945546",
    "end": "1950920"
  },
  {
    "text": "Would many iterations recognize\nthe true data distribution? With many iterations, would\nyou recognize the true data",
    "start": "1950920",
    "end": "1957955"
  },
  {
    "text": "distribution? Yeah. So the-- I think that you\ncould essentially view this",
    "start": "1957955",
    "end": "1963400"
  },
  {
    "text": "as a suggestion for how to\nalleviate the problem, which is that one thing\nthat you could do is kind of collect\ndata using your model",
    "start": "1963400",
    "end": "1972280"
  },
  {
    "text": "and then add that data\nto your buffer of data and refit your model using\nthe data that you collected",
    "start": "1972280",
    "end": "1979720"
  },
  {
    "text": "by planning. So that's a third\nthing that we can do. Any other ideas?",
    "start": "1979720",
    "end": "1985360"
  },
  {
    "text": " OK. So those are good suggestions.",
    "start": "1985360",
    "end": "1991230"
  },
  {
    "text": "The first suggestion was\ncollect more diverse data at the beginning. The second was\njust to try to stay close to your initial\npolicy, and then the third",
    "start": "1991230",
    "end": "1997789"
  },
  {
    "text": "was to recollect data.  I'm going to kind of\noutline the last one, which",
    "start": "1997790",
    "end": "2004720"
  },
  {
    "text": "is that when you kind of start\nplanning with your model, you can then execute\nthose planned actions",
    "start": "2004720",
    "end": "2013059"
  },
  {
    "text": "and append that data\nto your data set and then refit your\nmodel with that data set.",
    "start": "2013060",
    "end": "2020297"
  },
  {
    "text": "And this is a nice\napproach because it means that you can explicitly,\nin a very targeted way, try to explore to regions where\nyou think that there might be",
    "start": "2020297",
    "end": "2030760"
  },
  {
    "text": "reward because you're planning\nto try to reach those regions,",
    "start": "2030760",
    "end": "2036370"
  },
  {
    "text": "and learn a good model in\nthe parts of the state space where you think that there\nmight be good reward. ",
    "start": "2036370",
    "end": "2044540"
  },
  {
    "text": "Cool, OK.  So then if you do this, if you\nexecute your planned actions,",
    "start": "2044540",
    "end": "2052129"
  },
  {
    "text": "you'll collect data\nthat looks like this. So you'll learn what it's\nlike to fall off cliffs and then you'll learn to kind\nof eventually go to the top",
    "start": "2052130",
    "end": "2061580"
  },
  {
    "text": "and then stop because\nyour model will learn that if you keep on\ngoing from that point,",
    "start": "2061580",
    "end": "2067129"
  },
  {
    "text": "you will no longer go higher. ",
    "start": "2067130",
    "end": "2074860"
  },
  {
    "text": "Yeah? So could you go back\none slide, please? Yeah. ",
    "start": "2074860",
    "end": "2081458"
  },
  {
    "text": "So if you do this, then\nyou're appending more damage to your data set\nrewards the actions",
    "start": "2081458",
    "end": "2087078"
  },
  {
    "text": "that you think are good. So does this kind of\ngive us a bad estimate of the other actions,\nother state action",
    "start": "2087079",
    "end": "2093385"
  },
  {
    "text": "tuples that aren't so good?  You mean does it\ngive you an estimate",
    "start": "2093385",
    "end": "2100610"
  },
  {
    "text": "for-- a bad estimate for\nkind of the initial data or? Whatever you use\nit in step four, you're appending actions to\nyour data set that you think",
    "start": "2100610",
    "end": "2107930"
  },
  {
    "text": "are good based on step three. So wouldn't you have less data\nfor actions that are good?",
    "start": "2107930",
    "end": "2114290"
  },
  {
    "text": "Yeah, great question. So the question is,\ndoesn't this mean that we're going to have less\ndata for parts of the state",
    "start": "2114290",
    "end": "2120260"
  },
  {
    "text": "space or parts of kind of\nthe action distributions that are less good? And the short answer is yes.",
    "start": "2120260",
    "end": "2127250"
  },
  {
    "text": "So we'll have less\ndata in those regions. The good news is that even\nthough you'll have less data",
    "start": "2127250",
    "end": "2134480"
  },
  {
    "text": "and your model will be less\naccurate in those regions,",
    "start": "2134480",
    "end": "2139970"
  },
  {
    "text": "if it then thinks that some\nof those action sequences are good because it erroneously\nthinks that they're going",
    "start": "2139970",
    "end": "2146180"
  },
  {
    "text": "to be good, then\nit will eventually kind of optimize for\nreward, pick those actions,",
    "start": "2146180",
    "end": "2153273"
  },
  {
    "text": "and then collect data. And it's essentially\ngoing to be self correcting where whenever it\nerroneously thinks something is good, then it will\ncollect data in that region",
    "start": "2153273",
    "end": "2160910"
  },
  {
    "text": "and then ultimately\nlearn that it's not good.",
    "start": "2160910",
    "end": "2166497"
  },
  {
    "text": "And if there's any\nparts of the state space that you don't have a lot of\ndata in but think are bad,",
    "start": "2166498",
    "end": "2171570"
  },
  {
    "text": "then it will choose not to visit\nthose parts of the state space. ",
    "start": "2171570",
    "end": "2183640"
  },
  {
    "text": "Cool. So this is kind of\nthe complete version",
    "start": "2183640",
    "end": "2193230"
  },
  {
    "text": "one of model based RL where\nyou fit a model to some data, you plan using that\nmodel, and then you",
    "start": "2193230",
    "end": "2200880"
  },
  {
    "text": "add that data to your buffer.  Now, the last version\nI'd like to talk about",
    "start": "2200880",
    "end": "2208790"
  },
  {
    "text": "is trying to do a little\nbit better than this. And this is getting back to\nthe question of open loop",
    "start": "2208790",
    "end": "2214850"
  },
  {
    "text": "versus closed loop. So one of the things that\nyou might have noticed",
    "start": "2214850",
    "end": "2221960"
  },
  {
    "text": "is that right now we are\nplanning a sequence of actions and then executing those\nactions in the environment.",
    "start": "2221960",
    "end": "2230780"
  },
  {
    "text": "And what this ends up looking\nlike is if you have your agent",
    "start": "2230780",
    "end": "2237620"
  },
  {
    "text": "and you have your world,\nthen what it's going to do is it's going to kind of execute\nthe actions that it optimized",
    "start": "2237620",
    "end": "2248690"
  },
  {
    "text": "in order to try to\naccomplish the task. And this is what's\nreferred to as open loop.",
    "start": "2248690",
    "end": "2259130"
  },
  {
    "text": "And there's an kind of\nalternative paradigm where you're actually going to\ntry to close this loop where",
    "start": "2259130",
    "end": "2266030"
  },
  {
    "text": "at each time step, instead\nof just passing in or kind of taking the sequence of\nactions and running them,",
    "start": "2266030",
    "end": "2273470"
  },
  {
    "text": "you're going to take\none action and then see",
    "start": "2273470",
    "end": "2279140"
  },
  {
    "text": "what the next state is and\nthen use that next state to determine your next action. ",
    "start": "2279140",
    "end": "2286918"
  },
  {
    "text": "And so this is what's\nreferred to as closed loop. ",
    "start": "2286918",
    "end": "2292560"
  },
  {
    "text": "Hopefully for reasons\nthat are clear because we see a loop there. Now, does anyone\nhave thoughts on when",
    "start": "2292560",
    "end": "2301290"
  },
  {
    "text": "open loop might be a bad idea? Yeah? If my environment is stochastic.",
    "start": "2301290",
    "end": "2308710"
  },
  {
    "text": "Yeah. So if your environment\nis stochastic then this open loop\nversion is going",
    "start": "2308710",
    "end": "2316830"
  },
  {
    "text": "to be suboptimal, most likely. And the reason for\nthat is that if you don't know what your\nnext state is, then",
    "start": "2316830",
    "end": "2324630"
  },
  {
    "text": "if there's basically\nstochasticity in your dynamics, then if you base your actions\non your most recent state, then",
    "start": "2324630",
    "end": "2331283"
  },
  {
    "text": "you'll be able to\naccount for what happens in the stochasticity\nand the dynamics. Whereas in this case,\nyou don't have any way",
    "start": "2331283",
    "end": "2336900"
  },
  {
    "text": "to account for the fact that\nst plus 1 or st plus 2 might be different from kind\nof the average case.",
    "start": "2336900",
    "end": "2342974"
  },
  {
    "text": " Cool. So then does anyone\nhave thoughts",
    "start": "2342975",
    "end": "2348700"
  },
  {
    "text": "on how we might accomplish\nthis closed loop approach in model based RL?",
    "start": "2348700",
    "end": "2354490"
  },
  {
    "text": "Yeah? I just have a question. So in both cases [INAUDIBLE]\nfor each one [INAUDIBLE] in closed loop if we're just\napplying one action, say, the next state is [INAUDIBLE]",
    "start": "2354490",
    "end": "2372190"
  },
  {
    "text": "Yeah, exactly. So in both cases,\nwe are going to be trying to look ahead\nand predict what's",
    "start": "2372190",
    "end": "2378550"
  },
  {
    "text": "going to happen in the future. The key difference\nis that if we're uncertain about what's going\nto happen then we need to--",
    "start": "2378550",
    "end": "2385839"
  },
  {
    "text": "in this case, we need\nto plan with respect to that uncertainty. We need to plan under\nlots of different possible",
    "start": "2385840",
    "end": "2392680"
  },
  {
    "text": "circumstances that might happen\nand then just kind of execute an action sequence\nthat seems reasonable",
    "start": "2392680",
    "end": "2399068"
  },
  {
    "text": "according to those predictions. Where, in this case,\nwe essentially get to look at the next\nstate and kind of decide",
    "start": "2399068",
    "end": "2406090"
  },
  {
    "text": "what actions to take, again,\nbased off of the state that we saw. ",
    "start": "2406090",
    "end": "2412790"
  },
  {
    "text": "Yeah? In one of my other classes\nwe do a similar thing.",
    "start": "2412790",
    "end": "2419330"
  },
  {
    "text": "So you have this, sort of\nlike a tracking approach where you have the state, the state\nsequence that you think you're",
    "start": "2419330",
    "end": "2426289"
  },
  {
    "text": "going to get based on\nthe action sequence, and then the state sequence\nthat you actually do.",
    "start": "2426290",
    "end": "2431720"
  },
  {
    "text": "And then when you try to\nmatch those, try to stay close to that [INAUDIBLE],, you\nmake that state sequence",
    "start": "2431720",
    "end": "2438920"
  },
  {
    "text": "that you actually take closer\nto the one that you want [INAUDIBLE].",
    "start": "2438920",
    "end": "2444380"
  },
  {
    "text": "Yeah, so that's an\ninteresting idea. So one thing that you could\nessentially try to do, if I'm understanding\ncorrectly, is to try to--",
    "start": "2444380",
    "end": "2452059"
  },
  {
    "text": " basically try to\nchoose actions so that you are staying in\na part of the space that",
    "start": "2452060",
    "end": "2458660"
  },
  {
    "text": "is fairly predictable. And so if you're\ntrying to, essentially, match the trajectories\nbetween what you think",
    "start": "2458660",
    "end": "2466400"
  },
  {
    "text": "and what actually\nhappens, then you're essentially going to try to\nstay in parts of the space where the dynamics are\nmore deterministic.",
    "start": "2466400",
    "end": "2474049"
  },
  {
    "text": "And in that case,\nactually, an open loop plan might work a lot\nbetter because it's going to be-- if there are\nparts of the state space",
    "start": "2474050",
    "end": "2479964"
  },
  {
    "text": "that are more\ndeterministic, then you can try to solve\nthe task by staying in those parts of\nthe state space.",
    "start": "2479965",
    "end": "2485535"
  },
  {
    "text": "Is that kind of what\nyou were thinking? Yeah, I think. Cool. So that's actually\na pretty cool thing.",
    "start": "2485535",
    "end": "2491645"
  },
  {
    "text": "It's not actually that common\nto do that because estimating how deterministic things are\nis a little bit difficult, but there are some more advanced\ntechniques that do something",
    "start": "2491645",
    "end": "2499002"
  },
  {
    "text": "like that that are pretty cool.  And then in terms of how to--",
    "start": "2499002",
    "end": "2505580"
  },
  {
    "text": "so something like that would\nstill be somewhat open loop. In terms of how we\ncould learn, how",
    "start": "2505580",
    "end": "2511250"
  },
  {
    "text": "we could move towards the second\nparadigm here, what we can do is-- instead of planning a sequence\nof actions at the first time",
    "start": "2511250",
    "end": "2518720"
  },
  {
    "text": "step and rolling them\nout, what we can do is we can plan out\nthe first time step, optimize for our\nsequence of actions,",
    "start": "2518720",
    "end": "2524960"
  },
  {
    "text": "take only the first\naction in that sequence, and then observe what\nthe next state is.",
    "start": "2524960",
    "end": "2530720"
  },
  {
    "text": "And then once you\nobserve the next state, you can replan and rerun\nyour optimization to, again,",
    "start": "2530720",
    "end": "2536630"
  },
  {
    "text": "pick an action sequence\nfrom that new state. ",
    "start": "2536630",
    "end": "2541790"
  },
  {
    "text": "And this sort of\nreplanning approach can essentially account for the\ndynamics in the environment,",
    "start": "2541790",
    "end": "2547380"
  },
  {
    "text": "especially stochastic dynamics. It may be a little bit\nexpensive because you're going to be running\nthis planning",
    "start": "2547380",
    "end": "2553190"
  },
  {
    "text": "step at every single\ntime step, potentially. But one thing that you could\ndo is you could potentially warm start the\noptimization from the plan",
    "start": "2553190",
    "end": "2559605"
  },
  {
    "text": "that you had at the\nprevious time step. ",
    "start": "2559605",
    "end": "2564810"
  },
  {
    "text": "And so in terms\nof how this works, this is essentially\nwhat's referred to as model predictive\ncontrol or MPC, which",
    "start": "2564810",
    "end": "2571740"
  },
  {
    "text": "is maybe a bit of a fancy name. But essentially what it is is\nyou run some initial policy,",
    "start": "2571740",
    "end": "2577590"
  },
  {
    "text": "you learn a model\nfrom that policy, you use that model to optimize\nfor your sequence of actions.",
    "start": "2577590",
    "end": "2585000"
  },
  {
    "text": "Then instead of executing\nall of the actions, you execute only the\nfirst planned action",
    "start": "2585000",
    "end": "2591810"
  },
  {
    "text": "and observe the resulting state.  And then you kind of go back to\noptimizing your action sequence",
    "start": "2591810",
    "end": "2600390"
  },
  {
    "text": "from there. We'll also append\nthis to the data set, but we're going to have\nessentially these two loops.",
    "start": "2600390",
    "end": "2608130"
  },
  {
    "text": "One loop is this. This inner loop is the\nclosed loop process where we're going to be kind of\nreplanning at every single time",
    "start": "2608130",
    "end": "2613890"
  },
  {
    "text": "step, and this\nouter loop is going to be the process of\nadding data to your buffer and refitting your model.",
    "start": "2613890",
    "end": "2619575"
  },
  {
    "text": " Yeah? One quick question. So our f refitted\nmodel, isn't it going",
    "start": "2619575",
    "end": "2626686"
  },
  {
    "text": "to be stochastic then too? So wouldn't we need to then\ndo a stochastic look ahead",
    "start": "2626686",
    "end": "2634010"
  },
  {
    "text": "when we solve the [INAUDIBLE]? Yeah. So the question is, shouldn't\nour model f also be stochastic?",
    "start": "2634010",
    "end": "2641320"
  },
  {
    "text": "So if your dynamics\nare stochastic, you should also try to fit\na model that's stochastic.",
    "start": "2641320",
    "end": "2646960"
  },
  {
    "text": "In the slides, for\nsimplicity, I was denoting it as kind of just using\nmean squared error in a deterministic model.",
    "start": "2646960",
    "end": "2654040"
  },
  {
    "text": "One of the things that's\ncool about the sampling based approach is even if you\nhave a stochastic model, you don't have to\nchange anything",
    "start": "2654040",
    "end": "2659825"
  },
  {
    "text": "about this optimization. With gradient based approaches,\nyou could also choose to--",
    "start": "2659825",
    "end": "2666340"
  },
  {
    "text": "I guess it depends on how\nyou implement your model, but it's not too difficult to\noptimize through that depending",
    "start": "2666340",
    "end": "2675400"
  },
  {
    "text": "on some of the details. And in that case,\nyou would be fitting a model f that is like modeling\nthe distribution over s prime",
    "start": "2675400",
    "end": "2685840"
  },
  {
    "text": "rather than just outputting\na deterministic estimate for s prime. ",
    "start": "2685840",
    "end": "2693230"
  },
  {
    "text": "And then the that way this works\nis if you kind of accidentally make a mistake at one\ntime step and notice that you made that mistake, then\nyou can actually essentially",
    "start": "2693230",
    "end": "2700422"
  },
  {
    "text": "correct for that\nerror that you made or correct for some aspect\nof the dynamics that",
    "start": "2700422",
    "end": "2706460"
  },
  {
    "text": "were unpredictable.  Cool.",
    "start": "2706460",
    "end": "2711838"
  },
  {
    "text": "So this can correct\nfor model errors. It is more compute\nintensive because we are trying to run things--",
    "start": "2711838",
    "end": "2717790"
  },
  {
    "text": "run this sort of planning\nat every single time step. In particular, it's compute\nintensive at test time rather than at training time.",
    "start": "2717790",
    "end": "2726070"
  },
  {
    "text": "But there are ways to try to\nwarm start the optimization to alleviate that challenge. ",
    "start": "2726070",
    "end": "2735490"
  },
  {
    "text": "OK. So what does this all have\nto do with multitask RL?",
    "start": "2735490",
    "end": "2742520"
  },
  {
    "text": "Like I mentioned before,\nthere are these-- there are two cases with\nrespect to the reward function.",
    "start": "2742520",
    "end": "2747980"
  },
  {
    "text": "In one case, maybe you know\nthe form of the reward function exactly. So maybe you can observe\nthe positions of objects",
    "start": "2747980",
    "end": "2755900"
  },
  {
    "text": "and your goal is to\nget the object position to be in a certain position.",
    "start": "2755900",
    "end": "2761960"
  },
  {
    "text": "Then what you could do\nis learn a single model and then plan with respect to\neach of your reward functions",
    "start": "2761960",
    "end": "2768470"
  },
  {
    "text": "at test time. And so at test time,\nif you're given-- if you're told that\nyou should do task one, then you can plan with respect\nto the reward for task one,",
    "start": "2768470",
    "end": "2775867"
  },
  {
    "text": "and likewise for\nthe other tasks. So as an example\nfor this, ri might",
    "start": "2775867",
    "end": "2783260"
  },
  {
    "text": "correspond to\ntrying to accomplish some different pencil\ntrajectories by essentially",
    "start": "2783260",
    "end": "2789289"
  },
  {
    "text": "writing different characters. And what this\napproach is doing is",
    "start": "2789290",
    "end": "2795260"
  },
  {
    "text": "it's running planning\nwith respect to a model that it learned to try to write\nthese different trajectories.",
    "start": "2795260",
    "end": "2801200"
  },
  {
    "text": "And-- sorry, write\nthese different digits. So for example, the\nsecond one is a 7. The third one is a 4. I think the last\none might be a 5.",
    "start": "2801200",
    "end": "2809510"
  },
  {
    "text": "So yeah, ri corresponds to\nthese different trajectories and we can just-- once we have our\nmodel, we can plan",
    "start": "2809510",
    "end": "2815510"
  },
  {
    "text": "to accomplish all of these\ndifferent trajectories at test time. As another example,\nri could also",
    "start": "2815510",
    "end": "2822049"
  },
  {
    "text": "correspond to different\ntrajectories or positions of a ball held in the hand.",
    "start": "2822050",
    "end": "2827480"
  },
  {
    "text": "And so for example, here,\nyou see these little red dots",
    "start": "2827480",
    "end": "2832850"
  },
  {
    "text": "that are spinning. And this is showing the\ndesired trajectory of the two balls and the hand,\nand it's running",
    "start": "2832850",
    "end": "2838940"
  },
  {
    "text": "planning with respect\nto a learned model in order to spin the\nballs in its hand.",
    "start": "2838940",
    "end": "2846109"
  },
  {
    "text": "And also you can\ngive it, for example, different goal positions\nof a single ball and it can also figure out how\nto move its hand in a way that",
    "start": "2846110",
    "end": "2852920"
  },
  {
    "text": "will hit that target position.  And here's another example.",
    "start": "2852920",
    "end": "2858080"
  },
  {
    "text": "I think in this case, it's\ngoing clockwise instead of counterclockwise. ",
    "start": "2858080",
    "end": "2866425"
  },
  {
    "text": "Cool.  Then one last caveat\nthat I'll mention is that in the multitask\nscenario, if you",
    "start": "2866425",
    "end": "2874710"
  },
  {
    "text": "are iteratively collecting\ndata, the reward is going to change\nwhat data you collect.",
    "start": "2874710",
    "end": "2881170"
  },
  {
    "text": "And so you will want to-- during training, you're going\nto want to have different reward",
    "start": "2881170",
    "end": "2888360"
  },
  {
    "text": "functions in order\nto collect-- in order to cover the distribution of\ntasks in terms of the data that you collect.",
    "start": "2888360",
    "end": "2893700"
  },
  {
    "text": " Cool. And then one other\nthing that's pretty cool",
    "start": "2893700",
    "end": "2899652"
  },
  {
    "text": "is model based RL tends to\nbe fairly sample efficient. And this means that you can\nactually run the same approach",
    "start": "2899652",
    "end": "2906560"
  },
  {
    "text": "on a real robot. And this is a five\nfinger shadow hand that is kind of twirling\nthese two balls with exactly",
    "start": "2906560",
    "end": "2914330"
  },
  {
    "text": "the approach that we\ntalked about before, where it's learning a\ndynamics model from a couple",
    "start": "2914330",
    "end": "2920960"
  },
  {
    "text": "hours of data on\nthe robot and then running planning with respect\nto this trajectorylike goal",
    "start": "2920960",
    "end": "2929030"
  },
  {
    "text": "at test time. ",
    "start": "2929030",
    "end": "2934720"
  },
  {
    "text": "Cool. So that was the case where\nwe knew the reward function as a function of the state. And in particular, we knew\nthe positions of the objects",
    "start": "2934720",
    "end": "2941590"
  },
  {
    "text": "and then we just measured\nthe reward function as the distance-- or\nthe negative distance between the object position\nand the goal position.",
    "start": "2941590",
    "end": "2949690"
  },
  {
    "text": "In other cases, we might\nnot know the reward function directly. And there's a few different\nways that we can do this.",
    "start": "2949690",
    "end": "2957928"
  },
  {
    "text": "We can try to learn\na reward function with supervised\nlearning and use that learned reward function in\naddition to the learned model",
    "start": "2957928",
    "end": "2965920"
  },
  {
    "text": "in order to plan. Another thing that\nwe could do is we could also meta-learn\na reward function where",
    "start": "2965920",
    "end": "2972220"
  },
  {
    "text": "we're given a few examples\nof success and failure for a reward\nfunction and we want to kind of adapt our reward\nfunction based on that data.",
    "start": "2972220",
    "end": "2980950"
  },
  {
    "text": " And so an example of doing\nthis is you can give it--",
    "start": "2980950",
    "end": "2988394"
  },
  {
    "text": "if you do some sort of\nmeta learning approach for learning a\nsuccess classifier, you can give it some\npositive examples of success.",
    "start": "2988395",
    "end": "2994630"
  },
  {
    "text": "So in this case, the goal\nis to put the pencil case behind the notebook.",
    "start": "2994630",
    "end": "2999900"
  },
  {
    "text": "And the robot is just\nlooking at the images here. The state space is just images.",
    "start": "2999900",
    "end": "3005570"
  },
  {
    "text": "And we'll use this to\nacquire a reward function for this particular\ntask where it's",
    "start": "3005570",
    "end": "3011119"
  },
  {
    "text": "going-- it should assign\nkind of a positive value for when it's behind the\nnotebook and a negative value when it's not.",
    "start": "3011120",
    "end": "3017647"
  },
  {
    "text": "And then once we have this\nbinary reward function, we can use it for planning. So we'll learn a model and then\nplan with respect to that model",
    "start": "3017647",
    "end": "3024680"
  },
  {
    "text": "to try to accomplish the\ngoal and push the pencil case to be behind the notebook. ",
    "start": "3024680",
    "end": "3037400"
  },
  {
    "text": "And this allows you to both\nsolve multitask RL as well as the meta RL\nproblem statement,",
    "start": "3037400",
    "end": "3043790"
  },
  {
    "text": "depending on the kind of\nsupervision that you have. ",
    "start": "3043790",
    "end": "3048859"
  },
  {
    "text": "Cool. Any questions on this\nbefore we move on? ",
    "start": "3048860",
    "end": "3065630"
  },
  {
    "text": "Cool. So next we'll talk\nabout model based RL with image observations, or\njust generally when you have",
    "start": "3065630",
    "end": "3071710"
  },
  {
    "text": "very high dimensional inputs. And this is a scenario that\nI think actually comes up",
    "start": "3071710",
    "end": "3079420"
  },
  {
    "text": "a fair amount. Oftentimes you can't directly\nobserve some really nice low dimensional state.",
    "start": "3079420",
    "end": "3084829"
  },
  {
    "text": "And so, for example,\nsay you want the robot to be able to\nuse the spatula to lift",
    "start": "3084830",
    "end": "3091420"
  },
  {
    "text": "the white object into the bowl. In this case, like\nin general, the robot",
    "start": "3091420",
    "end": "3096670"
  },
  {
    "text": "can access camera images. It can't access the position\nof the objects directly.",
    "start": "3096670",
    "end": "3102820"
  },
  {
    "text": "And we'd like to be able to-- yeah. We'd like to be able to solve\nthis task with model based",
    "start": "3102820",
    "end": "3108970"
  },
  {
    "text": "RL with only access to images.  Also, it's difficult to\ndefine a reward function",
    "start": "3108970",
    "end": "3116290"
  },
  {
    "text": "with only observations. One thing that we could\ndo is learn the classifier like we talked about before.",
    "start": "3116290",
    "end": "3122200"
  },
  {
    "text": "But in other cases,\nmaybe we might not want to learn a classifier\nbecause maybe it would take",
    "start": "3122200",
    "end": "3127960"
  },
  {
    "text": "a fair amount of supervision.  So for the reward\nsignal, one option",
    "start": "3127960",
    "end": "3134110"
  },
  {
    "text": "is to learn a classifier. Another option is\nto try to provide an image of the goal of what\nyou want the robot to solve,",
    "start": "3134110",
    "end": "3141280"
  },
  {
    "text": "and then this essentially\ncorresponds to goal conditioned RL. So you could give it an image\nthat looks like this and say,",
    "start": "3141280",
    "end": "3147170"
  },
  {
    "text": "I want you to try\nto reach this image. And you could use\nthis to try to derive",
    "start": "3147170",
    "end": "3152613"
  },
  {
    "text": "some reward function that tries\nto match images, for example. ",
    "start": "3152613",
    "end": "3158280"
  },
  {
    "text": "OK. So in terms of approaches\nfor this problem setting, there's three classes\nof approaches. One is to learn a\nlatent space and learn",
    "start": "3158280",
    "end": "3164740"
  },
  {
    "text": "a model in that latent space. The second is to learn a model\ndirectly on observations. And the third is to try to\npredict alternative quantities",
    "start": "3164740",
    "end": "3172299"
  },
  {
    "text": "rather than predicting\nfuture states.  So first let's talk\nabout learning models",
    "start": "3172300",
    "end": "3178910"
  },
  {
    "text": "in latent space. So really the key\nidea here is, instead of trying to model in\nthe original image space,",
    "start": "3178910",
    "end": "3186380"
  },
  {
    "text": "we want to learn a\nlower dimensional representation of\nimages and then learn a model in that lower\ndimensional space.",
    "start": "3186380",
    "end": "3194700"
  },
  {
    "text": "So for example, if you\nhave a graphical model that looks like this where your\nobservations are being observed",
    "start": "3194700",
    "end": "3201500"
  },
  {
    "text": "but you can't observe your\nlow dimensional states, then what you could\ntry to do is learn",
    "start": "3201500",
    "end": "3207440"
  },
  {
    "text": "a model g that can\nestimate your latent states from your observation.",
    "start": "3207440",
    "end": "3212690"
  },
  {
    "text": " And so the key idea is\nto essentially just try",
    "start": "3212690",
    "end": "3217800"
  },
  {
    "text": "to learn this latent encoder g. And then once you get this\nlatent dimensional state space,",
    "start": "3217800",
    "end": "3226079"
  },
  {
    "text": "then you can just do model\nbased RL in that space. ",
    "start": "3226080",
    "end": "3231677"
  },
  {
    "text": "There are a couple\ndifferent approaches that have looked at a\nproblem like this, and we'll go over, essentially,\nthese two approaches.",
    "start": "3231677",
    "end": "3241628"
  },
  {
    "text": "Really, the key\nchallenge is actually how do we do this first step\nof learning the latent-- like the low dimensional\nlatent state space.",
    "start": "3241628",
    "end": "3247640"
  },
  {
    "text": " OK. So in terms of the high level\nsketch of the algorithm, first",
    "start": "3247640",
    "end": "3254760"
  },
  {
    "text": "we'll collect some data. Then we're going to learn the\nembedding of your observations",
    "start": "3254760",
    "end": "3260309"
  },
  {
    "text": "as well as a model\nin that latent space. And then we'll use this model\nto optimize for actions.",
    "start": "3260310",
    "end": "3266790"
  },
  {
    "text": "And then we can execute the\nactions and potentially iterate this process.",
    "start": "3266790",
    "end": "3273373"
  },
  {
    "text": "So this is the same\nalgorithm as before. It's just the first step of-- the first part of step two\nhas changed to also learn",
    "start": "3273373",
    "end": "3279813"
  },
  {
    "text": "this latent embedding space.  Now, what is the reward\nfunction for optimizing actions?",
    "start": "3279813",
    "end": "3286770"
  },
  {
    "text": "One thing that we can do is if\nwe have an image of the goal, we can use the negative\ndistance between the embedding",
    "start": "3286770",
    "end": "3292590"
  },
  {
    "text": "of the goal and the embedding\nof your current observation. And this can give\nyou a reward signal",
    "start": "3292590",
    "end": "3298170"
  },
  {
    "text": "that assumes that distance\nin your latent space is a reasonable metric\nfor measuring distances",
    "start": "3298170",
    "end": "3304865"
  },
  {
    "text": "between states. ",
    "start": "3304865",
    "end": "3310240"
  },
  {
    "text": "Cool. Now the key question\nis then, how do you optimize this latent embedding?",
    "start": "3310240",
    "end": "3316109"
  },
  {
    "text": "One approach-- I don't want to\ngo into too much detail here, but one approach is\nto essentially learn",
    "start": "3316110",
    "end": "3322080"
  },
  {
    "text": "something that looks a lot\nlike a variational encoder to learn a latent space.",
    "start": "3322080",
    "end": "3327540"
  },
  {
    "text": "And in particular, you'll try\nto learn a latent space that can reconstruct your images.",
    "start": "3327540",
    "end": "3333120"
  },
  {
    "text": "This approach is slightly\nmore sophisticated than that. Instead of just learning a\nVE for a single time step,",
    "start": "3333120",
    "end": "3338970"
  },
  {
    "text": "it's also going to jointly\nlearn that with a dynamics model in your latent space. And this should\nproduce a latent space",
    "start": "3338970",
    "end": "3345240"
  },
  {
    "text": "that not only captures\ninformation about the image, but also allows you to learn\nhow to predict in that space.",
    "start": "3345240",
    "end": "3353385"
  },
  {
    "text": "And this makes sense because if\nyou only learned an autoencoder and then try to\npredict, it could be that the latent\nspace is organized",
    "start": "3353385",
    "end": "3358980"
  },
  {
    "text": "in a way that's difficult\nto predict the next state from the current state. ",
    "start": "3358980",
    "end": "3364920"
  },
  {
    "text": "Cool. And so an example\nof what this does. You can give it a\ngoal state of reaching",
    "start": "3364920",
    "end": "3371520"
  },
  {
    "text": "a particular image, shown here. And then once you have\nyour latent state space,",
    "start": "3371520",
    "end": "3381570"
  },
  {
    "text": "you can run planning\nin that space to try to learn how\nto reach that goal. ",
    "start": "3381570",
    "end": "3388930"
  },
  {
    "text": "And so the left is\nshowing the image from actually rolling\nout the planning process, and here's another goal state.",
    "start": "3388930",
    "end": "3395460"
  },
  {
    "text": "And then the right is showing-- the right is showing the\ngenerated predictions",
    "start": "3395460",
    "end": "3401670"
  },
  {
    "text": "from predicting forward\none step and then reconstructing the\nframe by passing it",
    "start": "3401670",
    "end": "3407160"
  },
  {
    "text": "through the decoder\nof the autoencoder. ",
    "start": "3407160",
    "end": "3413000"
  },
  {
    "text": "One thing that's cool\nabout this approach is that it's quite\nefficient even though it's trying to learn skills directly\nfrom pixel observations here.",
    "start": "3413000",
    "end": "3420630"
  },
  {
    "text": "And so it's learned\nthis behavior with just about\n300 trials, which",
    "start": "3420630",
    "end": "3427040"
  },
  {
    "text": "would correspond to probably\nabout 25 minutes of real time.",
    "start": "3427040",
    "end": "3432080"
  },
  {
    "text": "And it's also able to solve\nmultiple different goals rather than just a single goal. ",
    "start": "3432080",
    "end": "3440180"
  },
  {
    "text": "Cool. And then there's\none more example. Instead of trying to\nuse an autoencoder--",
    "start": "3440180",
    "end": "3445873"
  },
  {
    "text": "I guess it's still going\nto be using an autoencoder. But instead of trying\nto just only give it",
    "start": "3445873",
    "end": "3453818"
  },
  {
    "text": "the structure of\npredicting the next state, we can also impose some sort\nof regularization on our latent state as well.",
    "start": "3453818",
    "end": "3460369"
  },
  {
    "text": "And in particular, in an\nexample like robotics, we might want our\nlatent state space",
    "start": "3460370",
    "end": "3466190"
  },
  {
    "text": "to correspond to\npositions of objects. And if we want to do\nthat, what we can do",
    "start": "3466190",
    "end": "3471980"
  },
  {
    "text": "is we can pass our\nimage through an encoder and then constrain our latent\nspace to correspond to 2D",
    "start": "3471980",
    "end": "3478970"
  },
  {
    "text": "feature points in the image. And the way this\ncan work is that you",
    "start": "3478970",
    "end": "3484340"
  },
  {
    "text": "take your last\nconvolutional map, then you take a softmax over the\nspatial dimension of that map.",
    "start": "3484340",
    "end": "3490280"
  },
  {
    "text": "So then you're going to get\na probability distribution for each channel of your each\nchannel of your conv3 map.",
    "start": "3490280",
    "end": "3500630"
  },
  {
    "text": "And then once you get this\ndistribution over 2D positions then you can essentially\ntake an expectation,",
    "start": "3500630",
    "end": "3507289"
  },
  {
    "text": "which will take the expected\nxy-coordinate or expected",
    "start": "3507290",
    "end": "3513830"
  },
  {
    "text": "ij-coordinate, which\nlooks something like this. Which will take that\ndistribution over 2D positions",
    "start": "3513830",
    "end": "3520640"
  },
  {
    "text": "and give you a single point out. It's going essentially\ngive you the mean xy",
    "start": "3520640",
    "end": "3525680"
  },
  {
    "text": "position for that\nparticular distribution. You can essentially think of\nthis as a spatial attention",
    "start": "3525680",
    "end": "3533450"
  },
  {
    "text": "where it's essentially attending\nto the part of the feature map that has high activations. ",
    "start": "3533450",
    "end": "3541619"
  },
  {
    "text": "And the result of this\nis that you'll get-- each two dimensions\nof your latent space",
    "start": "3541620",
    "end": "3547920"
  },
  {
    "text": "are going to correspond to a\n2D coordinate in your image, and you can then\nactually overlay those 2D",
    "start": "3547920",
    "end": "3553770"
  },
  {
    "text": "coordinates back on\nthe image itself. And here are two examples\nof feature points",
    "start": "3553770",
    "end": "3559260"
  },
  {
    "text": "that this autoencoder\nwill give you. And you can see that this\nis visualizing the feature",
    "start": "3559260",
    "end": "3564540"
  },
  {
    "text": "point over time, and you\ncan see that one of them is tracking the\nfinger of the gripper and the other one is\ntracking the object.",
    "start": "3564540",
    "end": "3571755"
  },
  {
    "text": " Cool. And so essentially,\nthe embedding",
    "start": "3571755",
    "end": "3577540"
  },
  {
    "text": "here is much more\nstructured than if you were to just have a more\nunconstrained autoencoder. ",
    "start": "3577540",
    "end": "3585300"
  },
  {
    "text": "Cool. And then if you use a\nlatent space like this and run model based RL\nwith this latent space,",
    "start": "3585300",
    "end": "3592280"
  },
  {
    "text": "this is what the kind of\nlearning process looks like. It was initialized with\nthe policy that generally knows how to move its arm to\nthe right position, but not",
    "start": "3592280",
    "end": "3598550"
  },
  {
    "text": "the object. And then over time it's\nlearning to improve its model and learning how\nto eventually get",
    "start": "3598550",
    "end": "3605270"
  },
  {
    "text": "a behavior that can\nsuccessfully push the block over to the left. This was given a goal\nimage of pushing it",
    "start": "3605270",
    "end": "3613303"
  },
  {
    "text": "to a particular position. ",
    "start": "3613303",
    "end": "3618340"
  },
  {
    "text": "And then in terms\nof the final policy it looks something\nlike this, where it's able to push it to\nabout the correct position",
    "start": "3618340",
    "end": "3627070"
  },
  {
    "text": "on this mat. And then if we go\nback to the task that we wanted to do\nbefore where we needed",
    "start": "3627070",
    "end": "3632582"
  },
  {
    "text": "to lift the object and\nplace it into the bowl, I can also use this model based\nRL approach in the latent space",
    "start": "3632582",
    "end": "3639609"
  },
  {
    "text": "in order to solve the task.  And then lastly, you can also\nvisualize the feature points",
    "start": "3639610",
    "end": "3646330"
  },
  {
    "text": "on top of the image,\nwhich is kind of cool, and see what kinds of\nrepresentations it's learning.",
    "start": "3646330",
    "end": "3651910"
  },
  {
    "text": "And we see that, in\neach of these cases, it typically learns at\nleast one feature point that tracks the relevant object\nand other feature points that",
    "start": "3651910",
    "end": "3659650"
  },
  {
    "text": "track the arm or track\nother parts of the scene. ",
    "start": "3659650",
    "end": "3665680"
  },
  {
    "text": "Cool. And so this was looking\nat three different tasks, and for each of\nthese tasks, it was",
    "start": "3665680",
    "end": "3670950"
  },
  {
    "text": "learned using about 11\nminutes of robot time. So this is, again,\nemphasizing that model based RL approaches\nare pretty efficient.",
    "start": "3670950",
    "end": "3677328"
  },
  {
    "text": " Yeah. And then the caveat here is that\nthe representation was actually",
    "start": "3677328",
    "end": "3683820"
  },
  {
    "text": "learned also in a fairly\nenvironment specific way. ",
    "start": "3683820",
    "end": "3690174"
  },
  {
    "text": "Cool. So that was one\napproach, which is to try to learn a latent\nspace, and then run planning",
    "start": "3690174",
    "end": "3696890"
  },
  {
    "text": "in that latent space. Now, in both approaches, we're\nlearning this latent space",
    "start": "3696890",
    "end": "3702230"
  },
  {
    "text": "by reconstructing the image. And the first approach\nreconstructed the image and also learned a dynamics\nmodel in that latent space.",
    "start": "3702230",
    "end": "3711380"
  },
  {
    "text": "Now a thought exercise\nfor you is, why do we need to reconstruct the image? And in particular, could we\nlearn an embedding space--",
    "start": "3711380",
    "end": "3719870"
  },
  {
    "text": "learn a representation\nspace and the encoder for that representation\nsuch that you're able to predict the next\nrepresentation of that encoder?",
    "start": "3719870",
    "end": "3729440"
  },
  {
    "text": "Any thoughts on whether we\ncould do something like this? ",
    "start": "3729440",
    "end": "3741220"
  },
  {
    "text": "And to be clear,\nthe objective would be that we have some\nencoder g that tells us--",
    "start": "3741220",
    "end": "3750250"
  },
  {
    "text": "that encodes our\nobservation, and we're going to be trying to optimize\nfor the parameters of g",
    "start": "3750250",
    "end": "3757630"
  },
  {
    "text": "with respect to the ability\nto predict st plus 1.",
    "start": "3757630",
    "end": "3764890"
  },
  {
    "start": "3764890",
    "end": "3773880"
  },
  {
    "text": "So we're learning-- I guess this\nwould be both with respect to g and with respect to f. Does anyone have any thoughts on\nwhether you could do something",
    "start": "3773880",
    "end": "3781800"
  },
  {
    "text": "like this? Yeah? [INAUDIBLE]",
    "start": "3781800",
    "end": "3787682"
  },
  {
    "start": "3787682",
    "end": "3797420"
  },
  {
    "text": "Yeah. So when you also include\nan objective that tries to reconstruct\nthe observation,",
    "start": "3797420",
    "end": "3802720"
  },
  {
    "text": "essentially you're\nsaying that we are regularizing the\nembedding to separate out different observations.",
    "start": "3802720",
    "end": "3809810"
  },
  {
    "text": "Yeah. Any additional thoughts on that? ",
    "start": "3809810",
    "end": "3818730"
  },
  {
    "text": "Any thoughts on-- I guess, what\ndo you think would happen if-- what g would you get if you\noptimized this objective.",
    "start": "3818730",
    "end": "3826619"
  },
  {
    "text": "Yeah? Grid would probably\njust collapse. Yeah. So what could happen is if\nyou optimize this objective,",
    "start": "3826620",
    "end": "3834780"
  },
  {
    "text": "the representation\ncould just collapse because what it could do\nis it could learn to map g, for example.",
    "start": "3834780",
    "end": "3841180"
  },
  {
    "text": "g could just learn\nto always output 0. And if you always\noutput 0, then you",
    "start": "3841180",
    "end": "3848310"
  },
  {
    "text": "can perfectly predict\nthis objective because it's always 0, right? Or it could-- I mean,\nit doesn't have to be 0.",
    "start": "3848310",
    "end": "3854070"
  },
  {
    "text": "It could always\noutput 5 or something. And then this model just knows\nthat I should always output 5. And then you get 0\nloss here and life",
    "start": "3854070",
    "end": "3862890"
  },
  {
    "text": "is good in terms\nof this objective, but life is not good\nin terms of actually being able to use this\nembedding for performing tasks.",
    "start": "3862890",
    "end": "3871005"
  },
  {
    "text": " Does that make sense? ",
    "start": "3871005",
    "end": "3879180"
  },
  {
    "text": "Cool. So the takeaway here\nis that we can't just try to optimize for\npredictability in some latent space.",
    "start": "3879180",
    "end": "3884750"
  },
  {
    "text": "We also need that latent\nspace to reconstruct parts",
    "start": "3884750",
    "end": "3890410"
  },
  {
    "text": "of the observation, or somehow\nit needs to be told that it needs to encode information\nabout the observation",
    "start": "3890410",
    "end": "3897880"
  },
  {
    "text": "because this objective isn't-- there's nothing\nin this objective that tells it that s needs\nto encode anything about O.",
    "start": "3897880",
    "end": "3908770"
  },
  {
    "text": "Cool. So the benefits of\nthese approaches that learn in a latent\nspace is that they",
    "start": "3908770",
    "end": "3914860"
  },
  {
    "text": "are able to learn pretty complex\nskills fairly efficiently. If you use a structured\nrepresentation space,",
    "start": "3914860",
    "end": "3920960"
  },
  {
    "text": "then this enables pretty\neffective learning. The downsides are that the\nreconstructive objections--",
    "start": "3920960",
    "end": "3927340"
  },
  {
    "text": "the reconstruction\nobjectives might not recover the right representation. And we kind of talked\nabout before how",
    "start": "3927340",
    "end": "3936045"
  },
  {
    "text": "there was a question\nthat came up that if your state has\na lot of stuff in it that isn't relevant\nto the task, then",
    "start": "3936045",
    "end": "3941890"
  },
  {
    "text": "the model might choose not\nto model the stuff that's relevant to the task. And kind of as an explicit\nillustration of this,",
    "start": "3941890",
    "end": "3949240"
  },
  {
    "text": "at one point when\nI was running some of the experiments for\nthe previous paper, I wanted to get the\nrobot to essentially toss",
    "start": "3949240",
    "end": "3959770"
  },
  {
    "text": "a ping pong ball. And this is an\nexample, grayscale,",
    "start": "3959770",
    "end": "3965140"
  },
  {
    "text": "low dimensional image\nfrom the robot's camera. We can see the ping\npong ball in white.",
    "start": "3965140",
    "end": "3970810"
  },
  {
    "text": "And then I tried to train\nan autoencoder on this data and what I got was a\nreconstruction that",
    "start": "3970810",
    "end": "3976119"
  },
  {
    "text": "looks like this. And essentially what you see\nis that the autoencoder is just deciding to ignore the ping\npong ball because it was",
    "start": "3976120",
    "end": "3982692"
  },
  {
    "text": "kind of hard to reconstruct. It's also only a small\npart of the image. And it doesn't really get\nthat much of a loss incurred",
    "start": "3982693",
    "end": "3989920"
  },
  {
    "text": "by essentially erasing\nthe ping pong ball. And so this means\nthat sometimes these",
    "start": "3989920",
    "end": "3995797"
  },
  {
    "text": "approaches, if there's a large\npart of the image, a large part of the observation that\nisn't relevant to the task and a small part that\nis relevant to the task,",
    "start": "3995797",
    "end": "4002830"
  },
  {
    "text": "it might choose not to\nrepresent the things that are highly relevant to the task.",
    "start": "4002830",
    "end": "4008530"
  },
  {
    "text": "This can be a big downside. And so essentially, we\nneed better representation",
    "start": "4008530",
    "end": "4013890"
  },
  {
    "text": "learning methods, ideally\nunsupervised representation learning methods, in order to\nsolve this kind of problem.",
    "start": "4013890",
    "end": "4021090"
  },
  {
    "text": " Cool. And then it's also worth\nmentioning that these--",
    "start": "4021090",
    "end": "4027820"
  },
  {
    "text": "using low dimensional embeddings\ncan also be useful for model free approaches, not just\nfor model based approaches.",
    "start": "4027820",
    "end": "4034690"
  },
  {
    "text": "There's this older\nwork from 2012 that learns the\nlatent space and does",
    "start": "4034690",
    "end": "4040120"
  },
  {
    "text": "fitted Q iteration,\nwhich is essentially a variant on Q learning\nin that latent space.",
    "start": "4040120",
    "end": "4047410"
  },
  {
    "text": "You can also do a TRPO,\nwhich is a policy gradient method in a latent\nspace on a real robot,",
    "start": "4047410",
    "end": "4052625"
  },
  {
    "text": "and it ends up being much more\nefficient than if you were to do it on the original space.",
    "start": "4052625",
    "end": "4058462"
  },
  {
    "text": "Low dimensional\nembeddings can also be useful for computing rewards. So some works have essentially\nused a video representation",
    "start": "4058462",
    "end": "4067630"
  },
  {
    "text": "and used an encoding of that\nrepresentation using something like ImageNet features in\norder to measure the reward",
    "start": "4067630",
    "end": "4075310"
  },
  {
    "text": "for the task of opening a door. And then you can train a robot\nto optimize that objective and they can learn to\nopen a different door,",
    "start": "4075310",
    "end": "4081760"
  },
  {
    "text": "for example, using a reward\nfunction in that latent space. Great. ",
    "start": "4081760",
    "end": "4091380"
  },
  {
    "text": "And then also if you have\nreward function labels, then you could also use it\nto form an even better latent",
    "start": "4091380",
    "end": "4096540"
  },
  {
    "text": "space as well. A lot of the works that\nwe talked about before, we're trying to learn\nthese latent spaces",
    "start": "4096540",
    "end": "4101728"
  },
  {
    "text": "in a more unsupervised way. But in cases where you\ndo have some supervision, you can use that and\nleverage it to, for example,",
    "start": "4101728",
    "end": "4107830"
  },
  {
    "text": "pay attention to\nthe ping pong ball and encourage it to do that.",
    "start": "4107830",
    "end": "4112920"
  },
  {
    "text": " Great.",
    "start": "4112920",
    "end": "4119350"
  },
  {
    "text": "So the previous\nclass of methods was trying to learn a\nlatent space and learn",
    "start": "4119350",
    "end": "4125170"
  },
  {
    "text": "a model in that space. You can also actually\njust learn a model in the original\nobservation space as well.",
    "start": "4125170",
    "end": "4132318"
  },
  {
    "text": "So essentially, if you\ntake the MPC algorithm, you can essentially just replace\nall the s's with o's and just",
    "start": "4132319",
    "end": "4143949"
  },
  {
    "text": "do the same thing as before. In this case, the model needs\nto directly operate on o,",
    "start": "4143950",
    "end": "4150580"
  },
  {
    "text": "and so this means\nthat you'll need to learn a fairly\ncomplex model in order to, for example, predict\na video into the future.",
    "start": "4150580",
    "end": "4156894"
  },
  {
    "text": " And so kind of as one concrete\ninstantiation of this,",
    "start": "4156894",
    "end": "4162670"
  },
  {
    "text": "we could run a random\npolicy to collect some data, like data that looks like this.",
    "start": "4162670",
    "end": "4168759"
  },
  {
    "text": "Then learn a model\nto try to predict what the future will look like. We can get a model\nthat essentially",
    "start": "4168760",
    "end": "4175239"
  },
  {
    "text": "can predict different futures-- different videos into the\nfuture based off of the actions that the robot takes.",
    "start": "4175240",
    "end": "4180443"
  },
  {
    "text": " One of the things that's cool\nabout operating on images",
    "start": "4180444",
    "end": "4186460"
  },
  {
    "text": "is that you could also\napply these images-- these models to things\ndeformable objects like towels.",
    "start": "4186460",
    "end": "4192549"
  },
  {
    "text": "It's pretty difficult\nto think about how to represent the state of the\nworld if it involves a towel. But if you can operate\ndirectly on images",
    "start": "4192550",
    "end": "4199844"
  },
  {
    "text": "or learn a latent\nrepresentation, then that allows you to\nmanipulate deformable objects.",
    "start": "4199845",
    "end": "4205330"
  },
  {
    "text": "And then once you have this\nvideo prediction model, you can use it to optimize\nfor action sequences.",
    "start": "4205330",
    "end": "4211150"
  },
  {
    "text": " As a couple details here in\nterms of how to predict video,",
    "start": "4211150",
    "end": "4222280"
  },
  {
    "text": "some of the kind\nof best approaches right now will use a deep\nrecurrent neural network. They'll try to predict\nmultiple frames into the future",
    "start": "4222280",
    "end": "4228760"
  },
  {
    "text": "and they'll condition\nthat network on actions so that it's actually\nincorporating the robot's",
    "start": "4228760",
    "end": "4234130"
  },
  {
    "text": "actions into its\nprediction of the future. And oftentimes, these\nmodels are stochastic in order to try to capture\nthe uncertainty about what",
    "start": "4234130",
    "end": "4241360"
  },
  {
    "text": "future images will look like. Yeah? Quick question from the\nslide before if you go back.",
    "start": "4241360",
    "end": "4248860"
  },
  {
    "text": "How do we optimize for\nthe action sequence? So we're given a goal image of\nwhere we want to be and then?",
    "start": "4248860",
    "end": "4256599"
  },
  {
    "text": "Yeah. I'll talk about that in\nthe slide after this one. Yeah.",
    "start": "4256600",
    "end": "4262180"
  },
  {
    "text": "So in terms of\npredicting video, one of the things that I'll\nmention for predicting video is that one thing\nthat's pretty cool",
    "start": "4262180",
    "end": "4268270"
  },
  {
    "text": "is if you scale up these\nmodels to be larger, you can actually get\npretty good predictions.",
    "start": "4268270",
    "end": "4273559"
  },
  {
    "text": "So these are predictions\non a deformable object data set and you can see that\nthe predictions which are shown",
    "start": "4273560",
    "end": "4280840"
  },
  {
    "text": "on the left are actually-- I mean, in this case, maybe\neven indistinguishable from the ground truth.",
    "start": "4280840",
    "end": "4287675"
  },
  {
    "text": "And this means that if you\ncan get very good predictions, then you should be able to\nplan very well with this model. ",
    "start": "4287675",
    "end": "4296679"
  },
  {
    "text": "I'll talk about how\nto plan and then I'll talk about\nhow to get rewards. So for how to plan, we\ncan do the same sort",
    "start": "4296680",
    "end": "4302350"
  },
  {
    "text": "of sampling based optimization\nthat we talked about before. So as an example\nfor random shooting,",
    "start": "4302350",
    "end": "4308350"
  },
  {
    "text": "this would look like sampling\na bunch of different action sequences, such as\nthese two actions,",
    "start": "4308350",
    "end": "4314050"
  },
  {
    "text": "then predicting the\nfuture for those actions. And so you can run those actions\nthrough your video prediction",
    "start": "4314050",
    "end": "4319660"
  },
  {
    "text": "model to predict\ndifferent futures and then pick the future\nthat you like the best",
    "start": "4319660",
    "end": "4326530"
  },
  {
    "text": "and execute the\ncorresponding action. And then you can repeat\nthe steps one through three",
    "start": "4326530",
    "end": "4333880"
  },
  {
    "text": "to replan like we\ntalked about before. ",
    "start": "4333880",
    "end": "4339300"
  },
  {
    "text": "So essentially, this is\njust this MPC approach that we talked about before, but\nin visual space or visual MPC.",
    "start": "4339300",
    "end": "4344600"
  },
  {
    "text": " Cool. Actually, I don't actually have\na slide on getting rewards,",
    "start": "4344600",
    "end": "4351558"
  },
  {
    "text": "but there's a few different\napproaches that you can use. One is to learn a classifier.",
    "start": "4351558",
    "end": "4357000"
  },
  {
    "text": "So if you learn an\nimage classifier that tells you if a future is\ngood or not, then you can--",
    "start": "4357000",
    "end": "4362570"
  },
  {
    "text": "when you're evaluating\nyour futures, you can run those predicted\nimages through your image classifier and it will tell\nyou if a future is good",
    "start": "4362570",
    "end": "4369230"
  },
  {
    "text": "or if a future is bad. Your classifier will\nneed to generalize to predictions from the\nvideo prediction model.",
    "start": "4369230",
    "end": "4375927"
  },
  {
    "text": "And so if they're\nblurry, for example, you'll need to account for that. Another thing that\nyou could do is you could give it a goal image.",
    "start": "4375927",
    "end": "4381922"
  },
  {
    "text": "And if you give it a\ngoal image, then you can just look at like\nL2 distance or distance in some space between the\npredicted image and that goal",
    "start": "4381922",
    "end": "4388220"
  },
  {
    "text": "image. One last thing that you can\ndo that's a little bit more complex is you can\ntell it where you want",
    "start": "4388220",
    "end": "4395780"
  },
  {
    "text": "objects to move in the image. Tell it, like, I want\nthis-- like, this pixel to move over here, for example.",
    "start": "4395780",
    "end": "4401800"
  },
  {
    "text": "And there are some\nvideo prediction models that implicitly\npredict how pixels are going to move in the image.",
    "start": "4401800",
    "end": "4407210"
  },
  {
    "text": "And for those kinds of models,\nyou can actually essentially use their flow predictions to\nevaluate whether or not it's",
    "start": "4407210",
    "end": "4413202"
  },
  {
    "text": "accomplishing a certain goal. ",
    "start": "4413202",
    "end": "4418220"
  },
  {
    "text": "So as an example of that\nlast approach, you could say, I want you to move this red\npixel up to the top left.",
    "start": "4418220",
    "end": "4426027"
  },
  {
    "text": "This would correspond\nto a task that's kind of roughly along the\nlines of folding the short.",
    "start": "4426027",
    "end": "4432770"
  },
  {
    "text": "Then you can run planning\nwith respect to that goal, and it's going to give you-- it's going to say that\nthis prediction looks",
    "start": "4432770",
    "end": "4438860"
  },
  {
    "text": "pretty good in terms of\naccomplishing this objective. And then you'll run--",
    "start": "4438860",
    "end": "4444518"
  },
  {
    "text": "you'll run these\nactions on the robot and you'll get behavior that\nlooks something like this.",
    "start": "4444518",
    "end": "4450095"
  },
  {
    "text": " Now, one of the things\nthat's cool about this",
    "start": "4450095",
    "end": "4455560"
  },
  {
    "text": "is that you could actually\nuse a single model, not just to solve this one task, but\nlots of different tasks.",
    "start": "4455560",
    "end": "4461300"
  },
  {
    "text": "So you can give it\na goal of picking up the stuffed animal on the left.",
    "start": "4461300",
    "end": "4468910"
  },
  {
    "text": "You can give it a goal of\nmoving the sleeve of the shirt over to the right, putting\nan apple on a plate.",
    "start": "4468910",
    "end": "4476620"
  },
  {
    "text": "And essentially it's\ngoing to use its model to plan to accomplish these\ndifferent goals at test time,",
    "start": "4476620",
    "end": "4481840"
  },
  {
    "text": "and you don't even need to tell\nit these tasks during training. Yeah? is there a reason why the robot\nis moving in discrete steps?",
    "start": "4481840",
    "end": "4490710"
  },
  {
    "text": "Yeah. So the question is, is there\na reason why the robot is moving in discrete steps?",
    "start": "4490710",
    "end": "4496610"
  },
  {
    "text": "I guess there's a\nfew different ways to actually do robot control. One is in a very synchronous\nfashion where you move and then",
    "start": "4496610",
    "end": "4504620"
  },
  {
    "text": "you pick an action and\nthen you move again and you pick an action. You can also do\nit asynchronously.",
    "start": "4504620",
    "end": "4511190"
  },
  {
    "text": "One of the things that's-- I guess if you're replanning\nat every time step,",
    "start": "4511190",
    "end": "4516410"
  },
  {
    "text": "that planning time might\ntake some computation time. ",
    "start": "4516410",
    "end": "4524480"
  },
  {
    "text": "And I guess there's\na few things here, but one is that if\nyou want to replan every timestamp for\nthat current image,",
    "start": "4524480",
    "end": "4529699"
  },
  {
    "text": "then it's easiest to do\nthings synchronously.  You can, in principle,\ndo it asynchronously,",
    "start": "4529700",
    "end": "4536750"
  },
  {
    "text": "but it gets more complicated. And you also need to control\nfor the frequency at which you",
    "start": "4536750",
    "end": "4542270"
  },
  {
    "text": "collected the data\nand the frequency that you're actually rolling\nout the actions at test",
    "start": "4542270",
    "end": "4547460"
  },
  {
    "text": "time because you don't want to\nhave a mismatch in your control frequency because then you'll\nhave a mismatch in the data",
    "start": "4547460",
    "end": "4552590"
  },
  {
    "text": "distribution.  And here are kind of a few\nmore examples of tasks,",
    "start": "4552590",
    "end": "4559610"
  },
  {
    "text": "including the last example of\nfolding a towel over an object. ",
    "start": "4559610",
    "end": "4567530"
  },
  {
    "text": "Cool.  So in terms of modeling\ndirectly in observation space,",
    "start": "4567530",
    "end": "4573685"
  },
  {
    "text": "one thing that's\npretty nice about this is, I guess, in comparison to\nsome of the other approaches,",
    "start": "4573685",
    "end": "4579060"
  },
  {
    "text": "this is operating\non real images. It also requires pretty\nlittle human involvement.",
    "start": "4579060",
    "end": "4584160"
  },
  {
    "text": " Training this video\nproduction model is fully self\nsupervised on your data,",
    "start": "4584160",
    "end": "4592972"
  },
  {
    "text": "and you can also accomplish\na lot of different tasks with the single model. The downsides is that if\nyou have a lot of background",
    "start": "4592973",
    "end": "4600300"
  },
  {
    "text": "variability, it's\ngoing to be much harder to learn a good model. And this kind of\nplanning approach",
    "start": "4600300",
    "end": "4607800"
  },
  {
    "text": "here can't handle as complex\nskills as other methods because it's just using a\nrandom shooting planner.",
    "start": "4607800",
    "end": "4614490"
  },
  {
    "text": "And yeah, more\nsophisticated planners and so forth can accomplish\nmore challenging skills.",
    "start": "4614490",
    "end": "4622290"
  },
  {
    "text": "Also, because the\nmodel is so large, you can't roll out\nthat many samples",
    "start": "4622290",
    "end": "4627956"
  },
  {
    "text": "unless you have a ton\nof compute, basically. So it's fairly compute\nintensive at test time. ",
    "start": "4627957",
    "end": "4636320"
  },
  {
    "text": "OK. And the last thing\nI'd like to mention is that instead of\npredicting future states",
    "start": "4636320",
    "end": "4641500"
  },
  {
    "text": "or future observations,\nin some cases you can also predict\nother quantities.",
    "start": "4641500",
    "end": "4646720"
  },
  {
    "text": "And there's a few different\nkind of instantiations of this. So if you want to\nlearn how to grasp,",
    "start": "4646720",
    "end": "4652120"
  },
  {
    "text": "you could try to predict,\nlike, if I take these actions, will my grasp be successful?",
    "start": "4652120",
    "end": "4658510"
  },
  {
    "text": "Or if I take a\nsequence of actions, will I collide into something? Or if I take a\nsequence of actions,",
    "start": "4658510",
    "end": "4664210"
  },
  {
    "text": "what will my health be,\nwhat will my damage be, and so forth. You can essentially\nthink of this",
    "start": "4664210",
    "end": "4670420"
  },
  {
    "text": "as a different variant\nof model based RL whereas instead of predicting\nthe state into the future,",
    "start": "4670420",
    "end": "4676960"
  },
  {
    "text": "you're predicting some\nstatistics about that state. And then you can use the same\nkinds of planning approaches",
    "start": "4676960",
    "end": "4683890"
  },
  {
    "text": "with these predictive models. This also has a fairly close\nconnection to Q learning.",
    "start": "4683890",
    "end": "4689750"
  },
  {
    "text": "So if you think about\nthe reward being the probability of\nsome event happening, then these kinds of\napproaches are predicting",
    "start": "4689750",
    "end": "4697673"
  },
  {
    "text": "the probabilities of events. And so in that case,\nthey're essentially",
    "start": "4697673",
    "end": "4703420"
  },
  {
    "text": "predicting future\nreward and Q functions are predicting future\nreward as well. ",
    "start": "4703420",
    "end": "4711860"
  },
  {
    "text": "So the upside of this approach\nis that you're potentially only predicting these\ntask relevant quantities, and this can be quite\nhelpful, quite useful,",
    "start": "4711860",
    "end": "4719290"
  },
  {
    "text": "when you have very complex\nobservation spaces. The downside is that\nyou need to have--",
    "start": "4719290",
    "end": "4725720"
  },
  {
    "text": "you need to be able to\nobserve those quantities. You need to have labels of\nthose quantities in your data. And you, of course, you need\nto pick the quantities somewhat",
    "start": "4725720",
    "end": "4733150"
  },
  {
    "text": "manually.  Great. So to wrap up, we talked about\nmodel based RL in this lecture",
    "start": "4733150",
    "end": "4741160"
  },
  {
    "text": "and we also talked about model\nfree learning previously. And so I'd like to\nillustrate some of the pros",
    "start": "4741160",
    "end": "4746170"
  },
  {
    "text": "and cons of these approaches. For models, it's pretty\neasy to collect data in a self supervised way and\ntrain the model in a self",
    "start": "4746170",
    "end": "4753715"
  },
  {
    "text": "supervised way. These models are\nvery transferable across different\nreward functions.",
    "start": "4753715",
    "end": "4760480"
  },
  {
    "text": "And typically they require\na smaller quantity of reward supervised data because you're\noffloading a lot of the effort",
    "start": "4760480",
    "end": "4766120"
  },
  {
    "text": "onto learning the model\nrather than learning a model of reward. The downside is that these\nmodels are optimizing",
    "start": "4766120",
    "end": "4772630"
  },
  {
    "text": "for being able to\npredict the future and not for task performance,\nand sometimes it's",
    "start": "4772630",
    "end": "4778360"
  },
  {
    "text": "harder to learn a model than\nit is to learn a policy. And then lastly, you\nsometimes need assumptions",
    "start": "4778360",
    "end": "4786088"
  },
  {
    "text": "in order to learn\nmore complex skills because the models for\nthose more complex skills might also be fairly complex.",
    "start": "4786088",
    "end": "4793510"
  },
  {
    "text": "In the model free case, it\nmakes very little assumptions",
    "start": "4793510",
    "end": "4798907"
  },
  {
    "text": "with regard to the complexity\nof your observation space, the complexity of\nyour dynamics, and so forth. And it's fairly effective for\nlearning complex policies.",
    "start": "4798908",
    "end": "4807280"
  },
  {
    "text": "The downside is that it ends\nup requiring more experience, typically, and it's also a\nharder optimization problem",
    "start": "4807280",
    "end": "4814660"
  },
  {
    "text": "in the multitask\nsetting because you have to learn kind\nof multiple Q-- like, you have to\nlearn a multitask Q",
    "start": "4814660",
    "end": "4819670"
  },
  {
    "text": "function or a multitask policy. Ultimately, I think that\nwe, in many scenarios,",
    "start": "4819670",
    "end": "4825580"
  },
  {
    "text": "probably want elements of both. If we can learn a good model,\nthen we should use that.",
    "start": "4825580",
    "end": "4831880"
  },
  {
    "text": "But it would also\nbe nice to have some of the pros of\nmodel free methods. ",
    "start": "4831880",
    "end": "4838640"
  },
  {
    "text": "Great. So that concludes the\nmain content for today.",
    "start": "4838640",
    "end": "4844270"
  },
  {
    "text": "For next time-- we talked about\nmultitask RLs so far-- and so next week on Monday\nand Wednesday,",
    "start": "4844270",
    "end": "4850190"
  },
  {
    "text": "we'll talk about meta\nreinforcement learning. And then as a couple reminders,\nthe mid-quarter survey is out",
    "start": "4850190",
    "end": "4855250"
  },
  {
    "text": "and homework 3 is out as well. ",
    "start": "4855250",
    "end": "4863000"
  }
]