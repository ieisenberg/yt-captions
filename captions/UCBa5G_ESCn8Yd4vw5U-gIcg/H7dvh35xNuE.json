[
  {
    "start": "0",
    "end": "5410"
  },
  {
    "text": "So let's continue our journey\ninto the image compression.",
    "start": "5410",
    "end": "10480"
  },
  {
    "text": "The next few lectures will be\na little bit more advanced. We won't have any specific\nhomeworks on them,",
    "start": "10480",
    "end": "17380"
  },
  {
    "text": "but please try to take in as\nmuch as you can in the class.",
    "start": "17380",
    "end": "22750"
  },
  {
    "text": "Be participative,\nhave discussions around it, and everything, OK?",
    "start": "22750",
    "end": "27890"
  },
  {
    "text": "Cool. Let's get started. So just a quick recap, like,\nwhat is an image, right? So you have an array\nof pixels, which",
    "start": "27890",
    "end": "34938"
  },
  {
    "text": "is, like, this height,\nwidth, and channels. And typically, you\nwould have-- you will store, like, each\nof these values as,",
    "start": "34938",
    "end": "41500"
  },
  {
    "text": "like, 1 byte or sometimes\n10 bits or 12 bits, right?",
    "start": "41500",
    "end": "46660"
  },
  {
    "text": "And then we looked\nat, like, what happens when you do image compression. So it has been, I\nguess, a long week's",
    "start": "46660",
    "end": "51790"
  },
  {
    "text": "break, so I'm just\ngoing to reproduce these slides from the last\ntime just so that you guys can refresh what we saw.",
    "start": "51790",
    "end": "58300"
  },
  {
    "text": "And so this is a raw image. And this takes like 1.1\nmegabyte, uncompressed.",
    "start": "58300",
    "end": "63880"
  },
  {
    "text": "And if you compress\nit using JPEG by 40x, so this file becomes,\nlike, 27 kilobytes.",
    "start": "63880",
    "end": "69307"
  },
  {
    "text": "And if I go back and\nforth, you don't really see any difference. So lossless-- sorry, lossy\nimage compression with JPEG",
    "start": "69307",
    "end": "77110"
  },
  {
    "text": "seems to be doing fine. But now, like, we have studied\nabout the rate distortion. So you can ask\nthe question like,",
    "start": "77110",
    "end": "83170"
  },
  {
    "text": "what happens if I\nkeep compressing it further and further? So if you compress it\nfurther at this point, you can start seeing\nsome noticeable artifacts",
    "start": "83170",
    "end": "90400"
  },
  {
    "text": "in the image. So this file is at\n14 kB at this point, but you can see quite a high\ndistortion so as to say, right?",
    "start": "90400",
    "end": "98890"
  },
  {
    "text": "And if you go even\nfurther, if you try to compress this to, like,\nby two orders of magnitude, 8 kilobytes, now this picture\nis completely unrecognizable.",
    "start": "98890",
    "end": "108030"
  },
  {
    "text": "And then we saw last\nclass that-- oh, but-- and we learned a lot about JPEG\nlike how JPEG exactly works.",
    "start": "108030",
    "end": "113970"
  },
  {
    "text": "But then towards the end, we\nalso just briefly touched upon, like, why JPEG is like-- there are many places\nwhere JPEG can be improved.",
    "start": "113970",
    "end": "120659"
  },
  {
    "text": "Like JPEG is not really\nan upper bound so as to say in terms of performance. And we learned something about\n[? a ?] compressor called BPG.",
    "start": "120660",
    "end": "128580"
  },
  {
    "text": "What BPG does is just allows you\nto have variable block sizes as well as predictive coding.",
    "start": "128580",
    "end": "134670"
  },
  {
    "text": "And we had a quiz question\non predictive coding just to get you guys practice. Like I said, it's nothing\ntough, just very obvious steps.",
    "start": "134670",
    "end": "143550"
  },
  {
    "text": "And so if you compare the\nperformance of JPEG with BPG, so now both of them\nare at 8 kilobytes.",
    "start": "143550",
    "end": "148980"
  },
  {
    "text": "But as you can see like--\noops, OK. yeah, as you can see,",
    "start": "148980",
    "end": "153989"
  },
  {
    "text": "there's, like, a big difference\nin the reconstructed image, right? So obviously, like, JPEG can\nbe improved by quite a lot.",
    "start": "153990",
    "end": "160800"
  },
  {
    "text": "It's very obvious. But BPG still uses, like, a\nlot of hand-tuned parameters.",
    "start": "160800",
    "end": "167580"
  },
  {
    "text": "And we'll come to\nthat in a second. And that's what today's\nlecture is going to be about. Like how can you go\neven beyond, like,",
    "start": "167580",
    "end": "173130"
  },
  {
    "text": "the traditional\nimage compressors? So with this revision\njust to go over, like, the quiz\nquestions quickly,",
    "start": "173130",
    "end": "179489"
  },
  {
    "text": "so the first question\nwas just, like, trying to understand, like,\nthe same basic principles.",
    "start": "179490",
    "end": "185160"
  },
  {
    "text": "I would say just go over the\nsolution in your free time. It's, like, quite\nstraightforward,",
    "start": "185160",
    "end": "190830"
  },
  {
    "text": "the answers for the first three. It's just like literally\ndoing this weight times",
    "start": "190830",
    "end": "198510"
  },
  {
    "text": "height times number\nof bytes math, but now we added\nan alpha channel to make things more interesting.",
    "start": "198510",
    "end": "206070"
  },
  {
    "text": "And then the last question\nwas somewhat interesting, which is like in this\nquestion, we saw that PNG still seems to perform better than\neven if you were using just one",
    "start": "206070",
    "end": "214920"
  },
  {
    "text": "bit for color and one\nbit for alpha channel, but it seems like all the\nbest you can do in this case. And the question asks like, why?",
    "start": "214920",
    "end": "221490"
  },
  {
    "text": "And we didn't really talk\nabout PNG in the class. The idea was to-- now\nyou know about JPEG.",
    "start": "221490",
    "end": "226590"
  },
  {
    "text": "I was to guess how you can do\neven better, why this lossless compressor is performing\nso much better.",
    "start": "226590",
    "end": "233730"
  },
  {
    "text": "And there are lots\nof correct answers, but the just basic idea\nwhat we were looking for--",
    "start": "233730",
    "end": "239310"
  },
  {
    "text": "or anything is fine-- is that\nPNG basically does much better entropy coding than JPEG.",
    "start": "239310",
    "end": "247200"
  },
  {
    "text": "Recall JPEG was just\ndoing a basic run length encoding plus Huffman\nencoding, but we know that you can do--\nyou can do better.",
    "start": "247200",
    "end": "254040"
  },
  {
    "text": "And in this case, it's like a\nreally, really uniform image. So PNG, which uses actually\nan LZ77-based lossless",
    "start": "254040",
    "end": "263319"
  },
  {
    "text": "entropy coder, can perform\nvery well because it gets you a lot of matches. So if you look back to\nyour homework three Pok√©mon",
    "start": "263320",
    "end": "270580"
  },
  {
    "text": "question, right, so that's\nreally the intuition. It's the same\nintuition at play here.",
    "start": "270580",
    "end": "276790"
  },
  {
    "text": "OK, any questions? ",
    "start": "276790",
    "end": "281949"
  },
  {
    "text": "Good. And then in the\nsecond question, we made this JPEE274G\ncompressor, right?",
    "start": "281950",
    "end": "289990"
  },
  {
    "text": "And the idea was really to\nwork through some of the design decisions like what we\ntalked about in the class.",
    "start": "289990",
    "end": "296860"
  },
  {
    "text": "The first question was\naround like, when would you think increasing the block size\ngives you the more benefit?",
    "start": "296860",
    "end": "305350"
  },
  {
    "text": "And the main intuition\nthere is that when you have the same image,\nand you are representing it",
    "start": "305350",
    "end": "312340"
  },
  {
    "text": "in much higher\nresolution, that means you have more number\nof pixels representing the same information,\nand hence there",
    "start": "312340",
    "end": "319060"
  },
  {
    "text": "is lesser correlation between\nneighboring pixels, right? And so when you\nexpand the block size,",
    "start": "319060",
    "end": "324960"
  },
  {
    "text": "you will be able to capture\nmore correlation between pixels within a block size.",
    "start": "324960",
    "end": "330370"
  },
  {
    "text": "And that's how DCT works,\nand it really helps you. And so the answer\nfor the first one was basically like you\nexpect to see more benefits",
    "start": "330370",
    "end": "338560"
  },
  {
    "text": "for the image with\nhigher resolution as you increase the block size.",
    "start": "338560",
    "end": "344440"
  },
  {
    "text": "And then the second question\nwas now on like, OK-- so OK, the first question--\nthe idea was like,",
    "start": "344440",
    "end": "350811"
  },
  {
    "text": "what happens within\na block, right? We were trying to see how\nthe correlations work out within a block.",
    "start": "350812",
    "end": "356169"
  },
  {
    "text": "The second question\nwas like, OK, now, we said that this\ncompressor also",
    "start": "356170",
    "end": "361800"
  },
  {
    "text": "uses prediction of blocks based\non previously encoded blocks. So now, instead of just looking\nat pixels within a block,",
    "start": "361800",
    "end": "367890"
  },
  {
    "text": "we are trying to see if\nthe neighboring blocks and everything-- are they\npredictive of the next block?",
    "start": "367890",
    "end": "373500"
  },
  {
    "text": "And again, the idea really is if\nyou can do a better prediction, then you can use that prediction\nto have lower residuals",
    "start": "373500",
    "end": "381870"
  },
  {
    "text": "and compress your stream\nmuch better, right? This is exactly the two\nideas which BPG also uses",
    "start": "381870",
    "end": "389250"
  },
  {
    "text": "to improve upon JPEG, right? So it allows you to\nhave bigger block sizes, and it also allows you\nto have prediction.",
    "start": "389250",
    "end": "395580"
  },
  {
    "text": "And so the idea there\nreally was that, oh, if you have more homogeneous\nblocks, if your image is really",
    "start": "395580",
    "end": "401430"
  },
  {
    "text": "consisting of very\nsimilar looking blocks, you can do better predictive\ncoding because it's probably",
    "start": "401430",
    "end": "406650"
  },
  {
    "text": "just a copy, paste. So in this Charizard\nimage, you'll probably have blocks,\nwhich are basically",
    "start": "406650",
    "end": "412140"
  },
  {
    "text": "all same color versus\nthis assorted Pok√©mon image, which has lots\nof things happening, OK?",
    "start": "412140",
    "end": "418590"
  },
  {
    "text": "So this is basically,\nagain, the ideas of how BPG improves in some\nsense compared to JPEG.",
    "start": "418590",
    "end": "427040"
  },
  {
    "text": "Any questions? ",
    "start": "427040",
    "end": "432500"
  },
  {
    "text": "No? Cool. And then the third question\nwas around predictive coding.",
    "start": "432500",
    "end": "437940"
  },
  {
    "text": "So we described an algorithm\nfor predictive coding. In class, I just\nmentioned that we'll do-- predictive coding\nis not that hard.",
    "start": "437940",
    "end": "444330"
  },
  {
    "text": "So if you work\nthrough this question, you hopefully\nunderstand the algorithm for doing predictive coding.",
    "start": "444330",
    "end": "449340"
  },
  {
    "text": "And then we gave just,\nlike, two questions, one on decoding and one\non finding the errors.",
    "start": "449340",
    "end": "455400"
  },
  {
    "text": "And I think it's\nquite straightforward. But if you didn't understand,\njust go through this and reach out to us in\nthe office hours using it.",
    "start": "455400",
    "end": "463370"
  },
  {
    "text": "OK.  Cool. Any questions in the--",
    "start": "463370",
    "end": "470820"
  },
  {
    "text": "so far on image\ncompression or quizzes? ",
    "start": "470820",
    "end": "478090"
  },
  {
    "text": "Nope. OK, great. So let's continue. OK, again, JPEG at\n8 kilobytes-- this",
    "start": "478090",
    "end": "485285"
  },
  {
    "text": "is the reconstructed\nimage, which you get BPG at 8 kilobytes,\nsuper better than what JPEG is.",
    "start": "485285",
    "end": "493230"
  },
  {
    "text": "But if you look at\nit carefully, like, you still see some\nartifacts, right? Like you see some\nblockiness in this nose,",
    "start": "493230",
    "end": "500039"
  },
  {
    "text": "similarly here, some\nringing stuff, right? So this is what you\nget with ML-based image",
    "start": "500040",
    "end": "507300"
  },
  {
    "text": "compression, again, at 8\nkilobytes, OK, same file size. And so if I go back and\nforth between BPG and this,",
    "start": "507300",
    "end": "513839"
  },
  {
    "text": "can you see the difference? So it's quite-- maybe if\nyou look at the girl's eye, I think--",
    "start": "513840",
    "end": "519360"
  },
  {
    "text": "oops, sorry.  So it's quite drastic, right? So the point really is OK,\nBPG is also not the same.",
    "start": "519360",
    "end": "527490"
  },
  {
    "text": "Like it's not really\nan upper bound. You can still\nperform much better. And again, if you have to\nreally take a stark contrast,",
    "start": "527490",
    "end": "534150"
  },
  {
    "text": "this is what JPEG\nreconstruction looks at, same file size versus-- this is what an ML-based\nreconstruction looks.",
    "start": "534150",
    "end": "540060"
  },
  {
    "text": "So there's clearly a lot\nof room for improvement. OK, and so today we'll be\nlearning more about how do you,",
    "start": "540060",
    "end": "547680"
  },
  {
    "text": "like, employ ML and deploy\nit for the use of image compression particularly.",
    "start": "547680",
    "end": "554759"
  },
  {
    "text": " OK, so just a reminder, this is\nwhat our pipeline for the JPEG",
    "start": "554760",
    "end": "561320"
  },
  {
    "text": "looked like. And we have seen similar things\nover the past few classes, so now you should be\ncomfortable with the ideas.",
    "start": "561320",
    "end": "567019"
  },
  {
    "text": "You start with something LGB. You do color treatment. This is mostly to decorrelate\nthe natural signals.",
    "start": "567020",
    "end": "573890"
  },
  {
    "text": "And then we do, I would say,\na vanilla lossy compressor",
    "start": "573890",
    "end": "578990"
  },
  {
    "text": "pipeline, which is, first,\nyou do some transform to decorrelate your components. Then you do quantization.",
    "start": "578990",
    "end": "584329"
  },
  {
    "text": "Finally, you do some lossless\nencoding, some entropy coding, and that gives you\nyour JPEG file, right?",
    "start": "584330",
    "end": "589459"
  },
  {
    "text": "And we went into\ndetails of each of these components last lecture.",
    "start": "589460",
    "end": "594660"
  },
  {
    "text": "So now the question\nis, like, how can we improve this\neven further, OK? So the first idea really\nis, like, maybe we",
    "start": "594660",
    "end": "602310"
  },
  {
    "text": "can go beyond linear transform. So if you saw, like, JPEG,\nJPEG2000, BPG, all of these",
    "start": "602310",
    "end": "609300"
  },
  {
    "text": "uses some variants\nof discrete cosine transform, discrete\nwavelet transform, discrete Fourier transform.",
    "start": "609300",
    "end": "614370"
  },
  {
    "text": "But all of these transforms\nare just linear maps. It's a matrix multiplication. Maybe we can do\nsomething better if we",
    "start": "614370",
    "end": "621120"
  },
  {
    "text": "allow for some nonlinear\ntransform coding to take place, which we haven't\nstudied in class at all so far,",
    "start": "621120",
    "end": "626880"
  },
  {
    "text": "right? And so this is really the idea. So think of these plots as\nthe two-dimensional vector",
    "start": "626880",
    "end": "635008"
  },
  {
    "text": "coding which we have looked at. So x-axis is just some\nsource dimension one. Y-axis is the source dimension\ntwo so like x1, x2 things",
    "start": "635008",
    "end": "642420"
  },
  {
    "text": "which we have been looking at. These purple dots are\nthe codebook vectors, and the regions are the\ncodebook boundaries.",
    "start": "642420",
    "end": "650759"
  },
  {
    "text": "And let's say your density\nfunction, the actual source which you are encoding, is\nin this banana kind of shape,",
    "start": "650760",
    "end": "656830"
  },
  {
    "text": "where yellow color means\nhigher probability density, and blue color means\nless probability density. And so if you were to\nvector quantize this block,",
    "start": "656830",
    "end": "664420"
  },
  {
    "text": "and you are only\nallowed to do, like, linear boundaries with\nthis vector transform,",
    "start": "664420",
    "end": "669580"
  },
  {
    "text": "this is probably the best\nyou can do to capture this. But if you were allowed\nnonlinear boundaries,",
    "start": "669580",
    "end": "674769"
  },
  {
    "text": "then I can exactly\ncover this vanilla-- sorry, this banana\npeel kind of shape. And I only need to transmit\nthese few vectors, right?",
    "start": "674770",
    "end": "682420"
  },
  {
    "text": "And you can be much\nmore efficient. Sorry, only these few\ncodebook vectors here, right?",
    "start": "682420",
    "end": "688210"
  },
  {
    "text": "And so in this very trivial,\nlike, synthetic example, you can already see that if your\nPDF of real sources-- nobody",
    "start": "688210",
    "end": "696970"
  },
  {
    "text": "said they have to follow\nsome linear boundaries, or they can be\nlinearly decorrelated. Maybe there is some\nnonlinear transform",
    "start": "696970",
    "end": "703240"
  },
  {
    "text": "which decorrelates them. So that's one.",
    "start": "703240",
    "end": "709240"
  },
  {
    "text": "The other idea is really more\nrelated to ML, which is-- so in JPEG, we saw\nthere were, like,",
    "start": "709240",
    "end": "715899"
  },
  {
    "text": "lots of these hand-determined\nheuristics, right? Like we had these quantization\nmatrices which were like--",
    "start": "715900",
    "end": "722050"
  },
  {
    "text": "which just came out of nowhere. These were, like,\nspecific numbers. When I say nowhere, it\nisn't really nowhere.",
    "start": "722050",
    "end": "727240"
  },
  {
    "text": "We motivated it\nbased on NLG and lots of psychovisual experiments. But nobody said, like,\nthese quantization matrix",
    "start": "727240",
    "end": "733930"
  },
  {
    "text": "is the best one for your image\nor your particular data set. Similarly, there were, like,\nother design decisions like 8",
    "start": "733930",
    "end": "741190"
  },
  {
    "text": "by 8 block size, why\nnot more, for example. And so there are lots\nof heuristics involved",
    "start": "741190",
    "end": "747550"
  },
  {
    "text": "during the design of JPEG. A lot of smart people\nreally, over the years, like, fine-tune each\nsmall knob to optimize",
    "start": "747550",
    "end": "754810"
  },
  {
    "text": "every bit, which contributes\nand goes into JPEG, right? But this is really hard, right?",
    "start": "754810",
    "end": "760660"
  },
  {
    "text": "Also, like, if we think\nabout R-D optimization, which we actually didn't\ntalk about much-- but somehow we have to do this.",
    "start": "760660",
    "end": "767270"
  },
  {
    "text": "We have to trade off\nrate with distortion. And the way you do it in JPEG\nis by changing your quantization matrix, right?",
    "start": "767270",
    "end": "772940"
  },
  {
    "text": "And again, like,\nthis changed also. So not only the values\nwhich were determined for one particular\nrate distortion,",
    "start": "772940",
    "end": "778820"
  },
  {
    "text": "but the change between\ndifferent rates and distortion, the trade-offs between\nrate and distortion were also\nhand-determined, right?",
    "start": "778820",
    "end": "785660"
  },
  {
    "text": "And there is no way really\nto know that that is optimal. And so one question\nreally is, can we",
    "start": "785660",
    "end": "793430"
  },
  {
    "text": "motivate this decision in\na more end-to-end fashion? When I say end-to-end\nfashion, like in some way",
    "start": "793430",
    "end": "798500"
  },
  {
    "text": "to determine these way like--\nto determine our compressor algorithms or the exact\nweights depending on,",
    "start": "798500",
    "end": "804889"
  },
  {
    "text": "like, the final\nvalue of the rate, distortion somehow figure\nthat out backwards.",
    "start": "804890",
    "end": "810649"
  },
  {
    "text": "And if you know ML, you know\nwhat I mean by really end to end, right? So the whole premise\nof ML is like you",
    "start": "810650",
    "end": "818030"
  },
  {
    "text": "try to do this end-to-end\nlearning for whatever you",
    "start": "818030",
    "end": "823340"
  },
  {
    "text": "are trying to optimize. And so these are\nbasically, I would say,",
    "start": "823340",
    "end": "828450"
  },
  {
    "text": "the two main motivation and\nideas for learned compression.",
    "start": "828450",
    "end": "833550"
  },
  {
    "text": "Like learned compression just\nmeans using machine learning. And it goes beyond all these\ntraditional compressors",
    "start": "833550",
    "end": "840540"
  },
  {
    "text": "by going beyond, first of all,\nlinear-to-nonlinear transform and then also doing this\nend-to-end training.",
    "start": "840540",
    "end": "845940"
  },
  {
    "text": "And there has been,\nlike, a ton of work, I would say, in just past. I would say the one\nof the early papers",
    "start": "845940",
    "end": "854640"
  },
  {
    "text": "by Toderici in CVPR15\nand Theis in ICLR17-- like those were the first\npapers which started",
    "start": "854640",
    "end": "861630"
  },
  {
    "text": "using ML for compression. But since then, there have been\nlike lots and lots of work. Basically, it's a\nreally hot field.",
    "start": "861630",
    "end": "868020"
  },
  {
    "text": "A lot of people\nare contributing. And hopefully, by the\nend of this lecture, I can convince you that it's\na really important topic",
    "start": "868020",
    "end": "874320"
  },
  {
    "text": "to learn if you want to-- if you want to\ncontinue in compression and keep contributing. At least you should know about\nthe techniques and ideas.",
    "start": "874320",
    "end": "882810"
  },
  {
    "text": "It's also very interesting\nbecause there is a whole CLIC Challenge. It's called Challenge on\nLearned Image Compression, which",
    "start": "882810",
    "end": "888335"
  },
  {
    "text": "is I think in its fourth\nor fifth iteration now. And this is happening at\nData Compression Conference.",
    "start": "888335",
    "end": "893839"
  },
  {
    "text": "And this is a really active\nchannel, a lot of industry, a lot of people,\na lot of academia participate in this to come\nup with the best compressors",
    "start": "893840",
    "end": "900590"
  },
  {
    "text": "for images and videos right? So if-- after this class,\nI hope you should also",
    "start": "900590",
    "end": "906560"
  },
  {
    "text": "be prepared if you\nwould like to take participate in\nchallenges like this.",
    "start": "906560",
    "end": "911589"
  },
  {
    "text": "OK, so lots of-- another point which\nI want to make-- like lots and lots of\ninteresting works and ideas",
    "start": "911590",
    "end": "917750"
  },
  {
    "text": "will only focus on the basic\nideas like the fundamentals. And then you can go\non and in future,",
    "start": "917750",
    "end": "923550"
  },
  {
    "text": "read whichever of these papers\nyou want to build on them. ",
    "start": "923550",
    "end": "931220"
  },
  {
    "text": "Cool. Everyone with me? [INAUDIBLE] OK, so before I go\ninto learned image compression,",
    "start": "931220",
    "end": "937839"
  },
  {
    "text": "since we decided we'll\nnot assume any background, next few slides might\nbe boring for people",
    "start": "937840",
    "end": "943360"
  },
  {
    "text": "who are already aware about ML. But if you don't know\nanything about ML, just hear through the\nnext five minutes.",
    "start": "943360",
    "end": "950319"
  },
  {
    "text": "And hopefully, you\nshould have everything which you need to\nknow or at least appreciating today's lecture\nwith regards to compression.",
    "start": "950320",
    "end": "956770"
  },
  {
    "text": "But if you already know\nit, don't worry about it. So this is really 101,\nthousand feet review, right?",
    "start": "956770",
    "end": "964660"
  },
  {
    "text": "There are lots and lots of\ninteresting classes which you can take, which will\nteach you in much more depth,",
    "start": "964660",
    "end": "969940"
  },
  {
    "text": "and I'll encourage\nyou guys to do that. But there are three main\ncomponents to any ML pipeline,",
    "start": "969940",
    "end": "975760"
  },
  {
    "text": "really. The first is data, right? So you try to figure out--",
    "start": "975760",
    "end": "980980"
  },
  {
    "text": "you try to optimize some\nproblem based on the data which you have been given. The second is some\nnonlinear model",
    "start": "980980",
    "end": "987850"
  },
  {
    "text": "and differentiable\narchitectures, which is this f of x, which\nis a predictor basically",
    "start": "987850",
    "end": "993910"
  },
  {
    "text": "on your data. So you can think of\nx as your data inputs and y as your labels.",
    "start": "993910",
    "end": "999530"
  },
  {
    "text": "Let's look at some examples. For example, your\nx can be images. Y can be whether it is\na dog or cat or not.",
    "start": "999530",
    "end": "1005350"
  },
  {
    "text": "So y can be a binary\nlabel per image saying, OK, whether there is a dog\nin this image or not, right?",
    "start": "1005350",
    "end": "1013029"
  },
  {
    "text": "So your nonlinear model\nor your function f is basically trying to learn.",
    "start": "1013030",
    "end": "1020560"
  },
  {
    "text": "It's trying to predict\nwhether there is a cat or not in your image. So it's a function of\nthe input data x image.",
    "start": "1020560",
    "end": "1028349"
  },
  {
    "text": "And typically, it is a\nparameterizable model. What that means is it\ntypically has-- it's defined",
    "start": "1028349",
    "end": "1034949"
  },
  {
    "text": "based on some parameters theta. The standard notation for\nthis is something like this. All that means is like--",
    "start": "1034950",
    "end": "1041040"
  },
  {
    "text": "OK, if you recall a\nrate-distortion water-filling thing, water-filling\nformula, there also we had a theta, which was\na parameter to parameterize",
    "start": "1041040",
    "end": "1050820"
  },
  {
    "text": "a rate-distortion curve. Think of theta in the same way. Basically, your function f just\ndepends on parameter theta.",
    "start": "1050820",
    "end": "1057390"
  },
  {
    "text": "And it takes input x, OK? The output of this is y hat.",
    "start": "1057390",
    "end": "1062460"
  },
  {
    "text": "Now, your y hat can be-- it can be involved, but\nfor now, think of it as--",
    "start": "1062460",
    "end": "1070070"
  },
  {
    "text": "what your model\nthinks is the label for this particular image, OK? So maybe based on\nwhatever model I have,",
    "start": "1070070",
    "end": "1078559"
  },
  {
    "text": "when I input a particular\nimage, it thinks it's a cat. And it was indeed a cat. Or maybe it says it's dog, even\nthough the input image is cat.",
    "start": "1078560",
    "end": "1087110"
  },
  {
    "text": "So it's figuring out\nbased on this whatever function f on data\nand parameters",
    "start": "1087110",
    "end": "1092180"
  },
  {
    "text": "to get this prediction y hat. And this is really the second\nmain component of our pipeline.",
    "start": "1092180",
    "end": "1098990"
  },
  {
    "text": "And the final main one\nis the loss function. So you need to\ndetermine like, OK,",
    "start": "1098990",
    "end": "1104000"
  },
  {
    "text": "so if my model predicts some\ncat or dog, whether it's correct or not, right? So if given an image,\nmy model predicts cat,",
    "start": "1104000",
    "end": "1110900"
  },
  {
    "text": "one of the loss functions could\nreally be just a 1 hat, right? So you can just decide\nif it's an indicator, so",
    "start": "1110900",
    "end": "1118580"
  },
  {
    "text": "if the predicted value is\ncat, whether it is indeed cat or not, given the\noriginal input label y.",
    "start": "1118580",
    "end": "1126770"
  },
  {
    "text": "OK, so very, very\nhigh level overview. Mostly all of you are already\naware of this in today's world",
    "start": "1126770",
    "end": "1132410"
  },
  {
    "text": "and age, but start\nfrom here, OK?",
    "start": "1132410",
    "end": "1137460"
  },
  {
    "text": "And then the way this\nML pipeline really",
    "start": "1137460",
    "end": "1142919"
  },
  {
    "text": "works is your f can\nbe anything, right? It's just a nonlinear\ndifferentiable architecture.",
    "start": "1142920",
    "end": "1148289"
  },
  {
    "text": "It could be thousand\ndifferent things. But the basic idea is you use\nbackpropagation or gradients",
    "start": "1148290",
    "end": "1155010"
  },
  {
    "text": "to actually learn the\nmodel parameters theta. And that's how you--\nthat's what's called",
    "start": "1155010",
    "end": "1160410"
  },
  {
    "text": "training or learning, right? And that happens based\non whatever given data you have like x and y.",
    "start": "1160410",
    "end": "1168200"
  },
  {
    "text": "The way this\nprogresses is by taking gradients of your loss function\nwith respect to theta, OK?",
    "start": "1168200",
    "end": "1174500"
  },
  {
    "text": "So here, all I\nhave done is like-- remember our loss was just-- it tells you whether you were\ncorrect or not with respect",
    "start": "1174500",
    "end": "1180920"
  },
  {
    "text": "to the label, which\nwas given to you based on the predicted label. Here, I've just replaced\nmy predicted label",
    "start": "1180920",
    "end": "1188630"
  },
  {
    "text": "by f, X of theta, which is\nwhat that prediction really is. And what you want\nto do is you want",
    "start": "1188630",
    "end": "1195470"
  },
  {
    "text": "to minimize this loss function. So this is an\noptimization problem. You want to figure out\nparameters theta, which",
    "start": "1195470",
    "end": "1202670"
  },
  {
    "text": "basically minimizes your loss. So you want to train\nyour model or parameters so that your overall\nobjective function reduces,",
    "start": "1202670",
    "end": "1210230"
  },
  {
    "text": "which is this loss function. And the way you reduce this-- so\nthis is a general optimization",
    "start": "1210230",
    "end": "1215899"
  },
  {
    "text": "problem. So if you know about\nconvex optimization and non-convex\noptimization, this",
    "start": "1215900",
    "end": "1221779"
  },
  {
    "text": "is, like, maybe easy to solve,\nmaybe hard to solve typically in our modern day machine\nlearning and world.",
    "start": "1221780",
    "end": "1227690"
  },
  {
    "text": "It's typically a non-convex\noptimization problem, and so it's really hard to\nsolve this problem exactly.",
    "start": "1227690",
    "end": "1234620"
  },
  {
    "text": "But what you can do is you can\ndo a vanilla gradient descent. What that means is that you\nupdate the parameters theta",
    "start": "1234620",
    "end": "1240650"
  },
  {
    "text": "at each time step by taking a\ngradient with respect to theta of this loss function.",
    "start": "1240650",
    "end": "1247010"
  },
  {
    "text": "OK? And you keep doing\nthis repeatedly, and that basically updates\nyour parameter theta",
    "start": "1247010",
    "end": "1253510"
  },
  {
    "text": "and gets it to a point so as\nthis optimization function",
    "start": "1253510",
    "end": "1259310"
  },
  {
    "text": "is really-- this objective function\nis really optimized. So you are minimizing the\nloss, or you are basically",
    "start": "1259310",
    "end": "1266300"
  },
  {
    "text": "finding a model f, which is\ngiving you the correct labels y. ",
    "start": "1266300",
    "end": "1274710"
  },
  {
    "text": "OK? So if you are seeing\nthis for the first time, are you with me?",
    "start": "1274710",
    "end": "1280559"
  },
  {
    "text": "Is there any questions? Are there any questions? So this is, like, the basic\ntraining architecture like what",
    "start": "1280560",
    "end": "1287372"
  },
  {
    "text": "you are really trying to do. All you are trying\nto do is you are trying to find a model, which\ntakes an input from data",
    "start": "1287373",
    "end": "1293280"
  },
  {
    "text": "x, outputs a\nprediction label y hat. The way you are trying to find--\nyou are trying to find out",
    "start": "1293280",
    "end": "1299790"
  },
  {
    "text": "this model is instead of, OK,\nmaybe doing DFT, DCT, blah, blah, blah, some\nhand-driven model,",
    "start": "1299790",
    "end": "1306420"
  },
  {
    "text": "you are trying to\nlearn this model using the data which you have. The way you learn it is you need\nto define this loss function,",
    "start": "1306420",
    "end": "1314230"
  },
  {
    "text": "which basically\ntells you difference between the actual labels. So is it actually cat\nor not versus predicted",
    "start": "1314230",
    "end": "1319980"
  },
  {
    "text": "labels, which is what\nyour model is saying. And then you backpropagate it\nor optimize this loss function",
    "start": "1319980",
    "end": "1326280"
  },
  {
    "text": "using-- by passing gradients\nthrough the network. And that updates your\nmodel parameter theta",
    "start": "1326280",
    "end": "1334050"
  },
  {
    "text": "eventually leading in reduction\nof this loss function. ",
    "start": "1334050",
    "end": "1340179"
  },
  {
    "text": "OK, and so again, like\nthere are a lot of-- I'm trying to summarize\nthis in five minutes",
    "start": "1340180",
    "end": "1347110"
  },
  {
    "text": "like a lot of details and like-- yeah, I'm not sure if\nit's helpful or not, but there are a lot\nof classes which",
    "start": "1347110",
    "end": "1354879"
  },
  {
    "text": "goes into much more details. Do check it. But this is\nbasically the summary at the very least which you need\nto understand and appreciate",
    "start": "1354880",
    "end": "1362200"
  },
  {
    "text": "to understand today's class. So the first thing\nis, in ML, you",
    "start": "1362200",
    "end": "1367570"
  },
  {
    "text": "use data to learn\nthe model parameters for optimizing a loss. So these are the three\nthings which we wanted to--",
    "start": "1367570",
    "end": "1374020"
  },
  {
    "text": "the three main components\nwhich are there in your machine learning pipeline. You do this by taking\ngradients of your loss function",
    "start": "1374020",
    "end": "1382030"
  },
  {
    "text": "with respect to theta. And your losses now-- it depends on the theta\nbecause your prediction",
    "start": "1382030",
    "end": "1387880"
  },
  {
    "text": "depends on the theta. OK? And the main\nadvantage it provides over this non-learned\nmodels is really,",
    "start": "1387880",
    "end": "1395730"
  },
  {
    "text": "it allows optimizing essentially\nany objective function loss",
    "start": "1395730",
    "end": "1400919"
  },
  {
    "text": "as long as your function\nis differentiable, OK? What that means is so as to\nbe able to do these gradients,",
    "start": "1400920",
    "end": "1408659"
  },
  {
    "text": "you should be able\nto take this-- you should be able-- your\nloss should be differentiable.",
    "start": "1408660",
    "end": "1415200"
  },
  {
    "text": "And that typically means\nlike if your f consists of multiple layers, you should\nbe able to differentiate through them.",
    "start": "1415200",
    "end": "1421380"
  },
  {
    "text": "It's basic application\nof chain rule. And again, I know like, go\ncheck out this if you are not",
    "start": "1421380",
    "end": "1427350"
  },
  {
    "text": "following it completely. If you don't know about\nit, trust me on this. You need the layers to\nbe differentiable so as",
    "start": "1427350",
    "end": "1433110"
  },
  {
    "text": "to be able to do any learning. What that means for us is\nthat it doesn't play well",
    "start": "1433110",
    "end": "1438360"
  },
  {
    "text": "with discrete\ndistributions, right? So our whole\nlossless compression has worked with some\ndiscrete distributions.",
    "start": "1438360",
    "end": "1445770"
  },
  {
    "text": "And so you can't just\nlike go and backpropagate or fit in a standard\nmachine learning model",
    "start": "1445770",
    "end": "1450809"
  },
  {
    "text": "and get a learned\ncompressor because you have to somehow deal with\nthe discreteness part of the entropy coder.",
    "start": "1450810",
    "end": "1458910"
  },
  {
    "text": "The second main advantage is\nthat it allows you stacking of these nonlinear layers.",
    "start": "1458910",
    "end": "1465240"
  },
  {
    "text": "So your model f need not\nbe linear anymore, OK? What is a linear model?",
    "start": "1465240",
    "end": "1471120"
  },
  {
    "text": "It's basically multiplication--\nit's just a matrix, right, a matrix multiplication.",
    "start": "1471120",
    "end": "1477389"
  },
  {
    "text": "And if you think about it like\nstacking of linear layers-- so going back to your\nlinear algebra again.",
    "start": "1477390",
    "end": "1483960"
  },
  {
    "text": "Stacking of linear layers\nis basically not so powerful because if you were\nmultiplying three, or four,",
    "start": "1483960",
    "end": "1489240"
  },
  {
    "text": "or whatever number\nof matrices together, you could just represent this\nas a single matrix, right?",
    "start": "1489240",
    "end": "1494550"
  },
  {
    "text": "So if you had ABC,\nwhatever, stacked matrix, it still gives you--\nyou can represent it",
    "start": "1494550",
    "end": "1500760"
  },
  {
    "text": "as a single matrix. So it's not really giving\nyou any more expressivity in your model, whereas if\nyou have nonlinear layer,",
    "start": "1500760",
    "end": "1508590"
  },
  {
    "text": "and then you stack them,\nthen this is no longer true. And that's really the second\nmain benefit of this ML.",
    "start": "1508590",
    "end": "1514169"
  },
  {
    "text": "That's why if you have ever\nheard about deep neural nets, and like let's-- we went from billion parameters\nto million parameters",
    "start": "1514170",
    "end": "1521903"
  },
  {
    "text": "to billion parameters to\nhundred million parameters. And the way we do it is keep\non increasing the depth, and we need more data.",
    "start": "1521903",
    "end": "1527880"
  },
  {
    "text": "All of these are very punchlines\nyou keep hearing outside. All of them have basically\ngot to do with this idea that",
    "start": "1527880",
    "end": "1536130"
  },
  {
    "text": "in machine learning-- or sorry,\nin-- yeah, in linear case, there is no advantage of,\nlike, stacking linear layers,",
    "start": "1536130",
    "end": "1542880"
  },
  {
    "text": "but in a nonlinear case, there\nis a lot of advantage of, like, stacking linear layers--",
    "start": "1542880",
    "end": "1548620"
  },
  {
    "text": "oh, sorry, nonlinear layers.  Cool. Any questions?",
    "start": "1548620",
    "end": "1557390"
  },
  {
    "text": "So this is just like an overview\nof a standard ML pipeline. Finally, this is\nthe last thing which",
    "start": "1557390",
    "end": "1563380"
  },
  {
    "text": "we need for today's lecture. Again, you don't\nhave to worry if you haven't seen this before.",
    "start": "1563380",
    "end": "1569950"
  },
  {
    "text": "But there is another\narchitecture called autoencoder architecture. And the idea really is\nthat you have some input x.",
    "start": "1569950",
    "end": "1576760"
  },
  {
    "text": "So this is the same input, OK? This whole thing\nyou can think of it as your model like the-- sorry.",
    "start": "1576760",
    "end": "1583450"
  },
  {
    "text": "Yeah, so I was saying so\nnow, the last thing which you need to know to appreciate\nthis lecture from ML",
    "start": "1583450",
    "end": "1589000"
  },
  {
    "text": "is this idea from\nautoencoder architecture. Just really so now\nyou have some input x.",
    "start": "1589000",
    "end": "1594610"
  },
  {
    "text": "You can think of this\nencoder/decoder pair as your model. And that outputs your\ny or x dash here, which",
    "start": "1594610",
    "end": "1602980"
  },
  {
    "text": "is the reconstructed input. OK, so the main idea\nhere really is that-- so in our typical\npipeline, we also",
    "start": "1602980",
    "end": "1610330"
  },
  {
    "text": "needed the true labels,\nso whether it's a cat or not, right? But the idea of this\nautoencoder architecture",
    "start": "1610330",
    "end": "1617300"
  },
  {
    "text": "really is you try to\nrecover an output which is similar as the input.",
    "start": "1617300",
    "end": "1622810"
  },
  {
    "text": "OK, and then your loss\nfunction can be determined such that you basically want your\nx dash to be similar to x.",
    "start": "1622810",
    "end": "1630240"
  },
  {
    "text": "So your loss function\ncan be over x and x dash. So you can think you\nstart with an image. You make it go through\nsome architecture, which",
    "start": "1630240",
    "end": "1636837"
  },
  {
    "text": "we'll come to in a second. This is your f, X of,\nin this case, theta phi, which are the parameters.",
    "start": "1636837",
    "end": "1642300"
  },
  {
    "text": "And then the output\nis just x dash. And then your loss function\nwill be over some x and x dash.",
    "start": "1642300",
    "end": "1647700"
  },
  {
    "text": "So you start with an image. You recover an image, right? The way you do it is\nthat your model actually",
    "start": "1647700",
    "end": "1655170"
  },
  {
    "text": "has a very specific\narchitecture where you first have an encoder, which reduces\nthe dimension of your input",
    "start": "1655170",
    "end": "1662220"
  },
  {
    "text": "to something small,\nthis bottleneck z. And then using that bottleneck\nz, you try to decompress it.",
    "start": "1662220",
    "end": "1670380"
  },
  {
    "text": "So you have\nsomething which tries to increase the\nnumber of parameters and bring it back\nto the image, OK?",
    "start": "1670380",
    "end": "1677070"
  },
  {
    "text": "Does this architecture\nremind you of something? ",
    "start": "1677070",
    "end": "1683000"
  },
  {
    "text": "They hint us like this. Look at the words\nwith respect to z. ",
    "start": "1683000",
    "end": "1693870"
  },
  {
    "text": "OK, come on. Yeah, so this is exactly\nin some-- not exactly, but this is in some sense very\nsimilar to compression problem,",
    "start": "1693870",
    "end": "1702150"
  },
  {
    "text": "right? So what you are trying to do\nis you are forcing your network to learn an encoder\nand a decoder pair--",
    "start": "1702150",
    "end": "1709250"
  },
  {
    "text": "so these parameters\nphi and theta-- such that whatever this\nlow-dimensional bottleneck is,",
    "start": "1709250",
    "end": "1715970"
  },
  {
    "text": "which is typically called\nthe latent variables, you are able to recover an\nimage from that or x dash",
    "start": "1715970",
    "end": "1722780"
  },
  {
    "text": "from that, which looks very\nsimilar to the original input. This is exactly what we want\nin some sense in compression,",
    "start": "1722780",
    "end": "1729490"
  },
  {
    "text": "right? Like what JPEG does is you\nstart with that input images, x. You want some\nlow-dimensional bitstream",
    "start": "1729490",
    "end": "1737400"
  },
  {
    "text": "or not-- actually, there is no\nlow dimensional, no continuous thing there. But you basically want\nto represent this image",
    "start": "1737400",
    "end": "1744000"
  },
  {
    "text": "in terms of as few number\nof bits as possible, which is, like, your bottleneck. And then you start with that\nbottleneck like few bits",
    "start": "1744000",
    "end": "1751020"
  },
  {
    "text": "and try to recover back\nan image, which looks very similar to the original one.",
    "start": "1751020",
    "end": "1756309"
  },
  {
    "text": "OK, so this is a very\npopular architecture called autoencoder architecture. And again, there are hundred--",
    "start": "1756310",
    "end": "1763130"
  },
  {
    "text": "tens of fundamentals variants\nof it and then hundreds of more variants on top of\neach of these tens of variants.",
    "start": "1763130",
    "end": "1769940"
  },
  {
    "text": "So it's a very, very\npopular architecture. But the idea really\nis you are trying to learn parameters,\nsuch that you",
    "start": "1769940",
    "end": "1776690"
  },
  {
    "text": "compress so as to say whatever\noriginal input you start with. And then you\ndecompress to get back",
    "start": "1776690",
    "end": "1783080"
  },
  {
    "text": "something which is original. And you do try to learn this\nin an end-to-end fashion. You determine a loss\nover this x and x dash.",
    "start": "1783080",
    "end": "1789580"
  },
  {
    "text": " Cool. Everyone with me?",
    "start": "1789580",
    "end": "1797002"
  },
  {
    "text": "Huh? So now, let's get into our-- again, the image compression\nproblem, so same thing",
    "start": "1797002",
    "end": "1804230"
  },
  {
    "text": "what we looked at, but now\nin words and terminologies which we understand and how we\nhave looked into this problem.",
    "start": "1804230",
    "end": "1810920"
  },
  {
    "text": "So let's say you have some\ntarget image like these cats. What do you want to do\nis you want to encode it.",
    "start": "1810920",
    "end": "1817190"
  },
  {
    "text": "That's your encoder\nto some bitstream. And then using\nthis bitstream, you want to decode it and get\nsome target reconstruction",
    "start": "1817190",
    "end": "1823400"
  },
  {
    "text": "like I hat. And the goal really is to do\nthis R-D optimization, right? So you want to\nminimize the distortion",
    "start": "1823400",
    "end": "1830000"
  },
  {
    "text": "between original image and\nthe reconstructed image given some constraint on\nthe rate, which is just",
    "start": "1830000",
    "end": "1836660"
  },
  {
    "text": "the length of this bit should\nbe less than or equals to B. OK, so this is what you\nreally want to achieve",
    "start": "1836660",
    "end": "1842030"
  },
  {
    "text": "at the end of the day. And so in traditional\nimage codecs,",
    "start": "1842030",
    "end": "1847630"
  },
  {
    "text": "essentially, the encoding was\nworking in three steps, right? So we first did DCT. And earlier, we call this\nsymbol something else,",
    "start": "1847630",
    "end": "1854750"
  },
  {
    "text": "but for this talk,\nlet's call it z. And the reason is we're\ngoing to correlate it back to these latent variables.",
    "start": "1854750",
    "end": "1860780"
  },
  {
    "text": "It's a very commonly\nused symbol z for it. So first step was\nDCT to decorrelate.",
    "start": "1860780",
    "end": "1865820"
  },
  {
    "text": "The second step is\nquantization, right? So this is where the\nloss gets introduced like loss of precision.",
    "start": "1865820",
    "end": "1872570"
  },
  {
    "text": "And then you would do\nsome arithmetic encoding on these discrete symbols, let's\nsay, or your favorite lossless",
    "start": "1872570",
    "end": "1878330"
  },
  {
    "text": "entropy coder for that purpose. For this talk, let's assume\nyou do arithmetic encoding. The idea you do arithmetic\nencoding is now,",
    "start": "1878330",
    "end": "1886190"
  },
  {
    "text": "you can assume that\nyour length of bits is approximately equal\nto log 1 over P of z hat. And again, we have looked into\nthis through many homeworks",
    "start": "1886190",
    "end": "1894170"
  },
  {
    "text": "and classes. So I am hopeful now this\nidea is really, deep down, you guys understand it.",
    "start": "1894170",
    "end": "1900830"
  },
  {
    "text": "OK. And so then what happens\non the decoder side-- you start with this bits. You do decoding, which\nbasically gives you",
    "start": "1900830",
    "end": "1908030"
  },
  {
    "text": "these discretized\nlatent variables z hat. And now you do directly\ninverse DCT transform, right?",
    "start": "1908030",
    "end": "1914000"
  },
  {
    "text": "So this was where the loss\nwas introduced, right? IDCT and DCT were completely\nreversible transform.",
    "start": "1914000",
    "end": "1919680"
  },
  {
    "text": "And this gives you this\ntarget reconstruction.  So now the question\nis, like, how",
    "start": "1919680",
    "end": "1926630"
  },
  {
    "text": "can you improve this, right? So the first idea was,\nOK, we don't really need these linear\ntransforms, right?",
    "start": "1926630",
    "end": "1934130"
  },
  {
    "text": "Like here, we were\ndoing DCT and IDCT. And we justified it saying that,\nOK, DCT [AUDIO OUT] similar way",
    "start": "1934130",
    "end": "1942200"
  },
  {
    "text": "as we would expect in\nnatural signals, right? But after that, we had some\ndifferent quantization method,",
    "start": "1942200",
    "end": "1947900"
  },
  {
    "text": "and so on and so forth. We are not limited to\nthe linear transform. So let's try to learn\nsomething general.",
    "start": "1947900",
    "end": "1953809"
  },
  {
    "text": "Instead of doing\nDCT and IDCT, let's try to learn some of these--",
    "start": "1953810",
    "end": "1960020"
  },
  {
    "text": "let's try to learn nonlinear\ntransforms for doing this image",
    "start": "1960020",
    "end": "1965120"
  },
  {
    "text": "compression, right? And that basically just\nmeans replacing so this-- replacing this DCT\nand IDCT with deep,",
    "start": "1965120",
    "end": "1973309"
  },
  {
    "text": "let's say, feedforward networks\nor any network for that sort. Don't worry about the\narchitecture for most part",
    "start": "1973310",
    "end": "1978410"
  },
  {
    "text": "of this talk. We are not going to get\ninto much of the ML details, but rather stick to the\nfundamental concepts, OK?",
    "start": "1978410",
    "end": "1986160"
  },
  {
    "text": "And the idea really\nis that if you use these deep\nfeedforward networks, we should still get this\nenergy segregation, right?",
    "start": "1986160",
    "end": "1993600"
  },
  {
    "text": "Like otherwise, there is no\nbenefit of really doing that. And how do you\nensure you do that?",
    "start": "1993600",
    "end": "1999220"
  },
  {
    "text": "So you need to determine some\nloss function really, right? Like so how do you\ntry to figure out what",
    "start": "1999220",
    "end": "2004790"
  },
  {
    "text": "these parameters should be? In an ML pipeline, we need to\ndetermine some loss function.",
    "start": "2004790",
    "end": "2011020"
  },
  {
    "text": "So one obvious loss\nfunction is like, which is what we started out to do\nis the rate-distortion loss",
    "start": "2011020",
    "end": "2018733"
  },
  {
    "text": "function, right? So what we want to do is\nwe want to minimize rate. And so this is just the\nLagrangian formulation.",
    "start": "2018733",
    "end": "2024830"
  },
  {
    "text": "So lambda is just a\npositive constant. So when you are trying to\noptimize two variables,",
    "start": "2024830",
    "end": "2030320"
  },
  {
    "text": "you can use this lambda to\ntrade off rate with distortion. So one thing you\nwant to minimize",
    "start": "2030320",
    "end": "2035630"
  },
  {
    "text": "is this distortion\nbetween I and I hat. The other thing you\nwant to minimize is this length of bitstream\nin the middle right.",
    "start": "2035630",
    "end": "2044250"
  },
  {
    "text": "OK? But we know that we can\napproximate the bitstream",
    "start": "2044250",
    "end": "2050100"
  },
  {
    "text": "length by this log 1\nover P of z hat, OK?",
    "start": "2050100",
    "end": "2055690"
  },
  {
    "text": "So this is one very\nobvious loss function, which we can use to train\nthis network back using",
    "start": "2055690",
    "end": "2062379"
  },
  {
    "text": "backpropagating and\nusing gradients. ",
    "start": "2062380",
    "end": "2069300"
  },
  {
    "text": "OK? So one way to think about this\nis instead of directly doing",
    "start": "2069300",
    "end": "2075510"
  },
  {
    "text": "arithmetic coding, what\nwe need in our pipeline is this probability\nmodel really, right? So you need to\ndetermine-- you need",
    "start": "2075510",
    "end": "2081840"
  },
  {
    "text": "to know what this P of\nz hat is so as to be able to train this network.",
    "start": "2081840",
    "end": "2087754"
  },
  {
    "text": " OK, so what-- anyone\nwants to point out what",
    "start": "2087755",
    "end": "2096080"
  },
  {
    "text": "is an issue with this pipeline? So we have defined this. If I just backpropagate\non this loss function,",
    "start": "2096080",
    "end": "2103010"
  },
  {
    "text": "take gradients with respect\nto this loss functions, can we just learn the END. ",
    "start": "2103010",
    "end": "2119020"
  },
  {
    "text": "Anyone who's familiar with ML\nor what we just talked about? So for learning, what\ndo you really need?",
    "start": "2119020",
    "end": "2128230"
  },
  {
    "text": "You need your objective function\nto be differentiable, right? Is this whole pipeline\ndifferentiable?",
    "start": "2128230",
    "end": "2134859"
  },
  {
    "text": "Can I-- when I back-- when\nI compute the gradients",
    "start": "2134860",
    "end": "2140640"
  },
  {
    "text": "with respect to my\nparameters, I will have to compute it\nall the way backwards.",
    "start": "2140640",
    "end": "2145850"
  },
  {
    "text": " Can somebody point some issues? ",
    "start": "2145850",
    "end": "2156660"
  },
  {
    "text": "Yeah? Getting the probability\ndistribution involves quantizing first,\nand quantizing isn't",
    "start": "2156660",
    "end": "2163290"
  },
  {
    "text": "going to be differentiable. Yeah, OK, so as\none of the students answered, getting the\nprobability distribution,",
    "start": "2163290",
    "end": "2170317"
  },
  {
    "text": "we first have to get\nthe z hat, right? There is a quantization\nstep in the middle. But I can't really take--",
    "start": "2170317",
    "end": "2176430"
  },
  {
    "text": "I can't do differentiation\nacross the quantization because that's a\ndiscrete variable. So discrete operation--\nI can't really do that.",
    "start": "2176430",
    "end": "2183900"
  },
  {
    "text": "That's correct. Anything else? Is there any other\noperation which you think you can't\nbackpropagate or take gradients",
    "start": "2183900",
    "end": "2191425"
  },
  {
    "text": "across?  So let's say instead\nof this z hat, I had z.",
    "start": "2191425",
    "end": "2199450"
  },
  {
    "text": "For a second, assume there\nwas no quantization step. I just had log 1 over--",
    "start": "2199450",
    "end": "2206109"
  },
  {
    "text": "actually, no. Let's say you had\nquantization step. So the other issue\nis you also have",
    "start": "2206110",
    "end": "2213260"
  },
  {
    "text": "to take gradients across the\nprobability model, right? So one issue is obviously\nquantization as the student",
    "start": "2213260",
    "end": "2221090"
  },
  {
    "text": "pointed out, right? Like how do you-- how\ndo you take gradients across sort of this layer? But the other issue\nis also like, how",
    "start": "2221090",
    "end": "2226970"
  },
  {
    "text": "do you take gradients\nacross your log 1 over probability of z hat? This is a discrete\n[AUDIO OUT] right?",
    "start": "2226970",
    "end": "2234619"
  },
  {
    "text": "So this is like--\nthis is what you are-- this is what your-- what you\nwere entropy coding losslessly.",
    "start": "2234620",
    "end": "2241700"
  },
  {
    "text": "So both of these\nlayers-- actually, you can backpropagate through it.",
    "start": "2241700",
    "end": "2246770"
  },
  {
    "text": "So that basically stops you\nfrom learning end to end. And so there are some\nsimple workarounds.",
    "start": "2246770",
    "end": "2255140"
  },
  {
    "text": "Let's look at both of them. So for quantizer, let's say\nour quantizer is just simple",
    "start": "2255140",
    "end": "2260359"
  },
  {
    "text": "rounding for now. So that's like a\nscalar quantizer. I just round it to\nnearest integer.",
    "start": "2260360",
    "end": "2266220"
  },
  {
    "text": "So the way people work on it\nis like during training, you model the quantizer\nas if you are adding",
    "start": "2266220",
    "end": "2272490"
  },
  {
    "text": "some independent noise,\nwhich is like uniform in minus 0.5 to 0.5, OK? So basically, your\n2.3 is going to 2.",
    "start": "2272490",
    "end": "2279420"
  },
  {
    "text": "3.7 is going to 4. Let me just model it as adding a\nnoise, which is plus minus 0.5.",
    "start": "2279420",
    "end": "2286140"
  },
  {
    "text": "And now this operation\nbecomes differentiable, right? What that basically\nmeans is when",
    "start": "2286140",
    "end": "2293270"
  },
  {
    "text": "you are differentiating through\nthis, you straight away-- like your d of z of hat\nis going to be d of z.",
    "start": "2293270",
    "end": "2300500"
  },
  {
    "text": "So you have something called\na straight-through estimator. So you behave as if this\nquantization layer didn't even",
    "start": "2300500",
    "end": "2307365"
  },
  {
    "text": "exist.  OK? While taking forward pass,\nwhile you are actually",
    "start": "2307365",
    "end": "2314000"
  },
  {
    "text": "calculating the parameters, you\nmodel it as by adding noise. But while taking gradients\nand updating your parameters,",
    "start": "2314000",
    "end": "2321290"
  },
  {
    "text": "you assume you are not--\nyou're not doing anything. And again, this is an--\nthis is an assumption. Like there is no proof to it.",
    "start": "2321290",
    "end": "2328070"
  },
  {
    "text": "This is just an assumption. And it seems to work\nextremely good empirically. That's what has been observed.",
    "start": "2328070",
    "end": "2333950"
  },
  {
    "text": "And this is, like,\nactually trick number one for dealing\nwith, I guess, quantization steps anywhere or whenever you\ndeal with anything discrete.",
    "start": "2333950",
    "end": "2341180"
  },
  {
    "text": "You model it as some\ncontinuous variable z plus some noise added.",
    "start": "2341180",
    "end": "2346430"
  },
  {
    "text": "And in terms of\ngradients, you just do a straight-through\nestimator, which just means you assume\nthat layer doesn't exist.",
    "start": "2346430",
    "end": "2353119"
  },
  {
    "text": "It's just identity. OK.",
    "start": "2353120",
    "end": "2358329"
  },
  {
    "text": "So that's also issue one, but-- sorry. Yeah, but we have a bigger\nissue at hand, which is,",
    "start": "2358330",
    "end": "2366320"
  },
  {
    "text": "I also don't know how to compute\nthis gradient, like gradient of probability of z hat\nwith respect to z hat.",
    "start": "2366320",
    "end": "2374310"
  },
  {
    "text": "Remember that hat\nis discrete, right? So this probability distribution\nis really a table, right?",
    "start": "2374310",
    "end": "2380789"
  },
  {
    "text": "So for each [AUDIO OUT]\nhat, I have sub P of z hat. And how do you differentiate\nback through this table?",
    "start": "2380790",
    "end": "2388300"
  },
  {
    "text": "And so people in the field\ncame with a, I would say, neat workaround-- again,\nit's a workaround,",
    "start": "2388300",
    "end": "2394810"
  },
  {
    "text": "and it's an assumption-- which is you can parameterize\nyour model P of z hat",
    "start": "2394810",
    "end": "2401740"
  },
  {
    "text": "using a density function. So what you are-- so\nthis is a bit tricky.",
    "start": "2401740",
    "end": "2407280"
  },
  {
    "text": "And you're like, why we are\ndoing this and, again, all of that? But let's just try to understand\nwhat we are trying to do that.",
    "start": "2407280",
    "end": "2414040"
  },
  {
    "text": "So I want-- so this probability\nmodel is in my hands, OK? I want to assume that\nit is Gaussian, OK?",
    "start": "2414040",
    "end": "2423180"
  },
  {
    "text": "So I'll assume my P\nof z hat is Gaussian. P of z is Gaussian. But it isn't really\ncontinuous variable.",
    "start": "2423180",
    "end": "2429300"
  },
  {
    "text": "It's a discrete variable. So one assumption is like,\nI can write my P of z hat",
    "start": "2429300",
    "end": "2434940"
  },
  {
    "text": "as if it's the\ncumulative density function of a Gaussian,\nnormal 0, 1 at z hat",
    "start": "2434940",
    "end": "2440730"
  },
  {
    "text": "plus 0.5 minus whatever\nit is at z hat minus 0.5.",
    "start": "2440730",
    "end": "2446460"
  },
  {
    "text": "OK? So it's a trick. Like let me say I\ngave this to you, that I could have\nchosen my probability",
    "start": "2446460",
    "end": "2453510"
  },
  {
    "text": "distribution to be anything. This is what I'm\nchoosing it to be. So my probability of discrete\nsymbols z hat, I'm saying,",
    "start": "2453510",
    "end": "2459840"
  },
  {
    "text": "is function of this\ncumulative density function of z hat plus\n0.5 and z hat minus 0.5.",
    "start": "2459840",
    "end": "2465890"
  },
  {
    "text": "Remember, my z hat was\njust a rounding function. So plus minus 0.5--\nit's probably-- it's the only integer\naround that, right?",
    "start": "2465890",
    "end": "2475010"
  },
  {
    "text": "But now since I\nhave assumed this, OK, I can now take the\nderivative of P of z hat",
    "start": "2475010",
    "end": "2483280"
  },
  {
    "text": "with respect to z hat, right,\nbecause my cumulative density function is differentiable.",
    "start": "2483280",
    "end": "2489510"
  },
  {
    "text": "And so if I-- I'll just come back to\nthis plot in a second. But if you take the gradient now\nof this function with respect",
    "start": "2489510",
    "end": "2497830"
  },
  {
    "text": "to z hat, that's just\nthe PDF of z hat plus 0.5 minus PDF of z hat minus 0.5.",
    "start": "2497830",
    "end": "2506970"
  },
  {
    "text": "OK? And what I'm showing\nhere is the solid curve",
    "start": "2506970",
    "end": "2513690"
  },
  {
    "text": "is basically the\nGaussian PDF, right? And the dots are the Gaussian\nCDF at z hat plus 0.5",
    "start": "2513690",
    "end": "2522510"
  },
  {
    "text": "and Gaussian CDF\nat z hat minus 0.5. So as you can see, dots are\nroughly mimicking a Gaussian",
    "start": "2522510",
    "end": "2530550"
  },
  {
    "text": "distribution. So what assumption or what\nyou forced your model, your probability\nmodel to be is such",
    "start": "2530550",
    "end": "2538619"
  },
  {
    "text": "that your discrete variables\napproximately come across like your Gaussian as if they\nhad the Gaussian probability",
    "start": "2538620",
    "end": "2545068"
  },
  {
    "text": "distribution function. ",
    "start": "2545068",
    "end": "2551060"
  },
  {
    "text": "OK this is a bit tricky. Is everybody following\nwhat happened here? ",
    "start": "2551060",
    "end": "2558520"
  },
  {
    "text": "No more question? Yeah. Was the reason that\nyou can do this-- because you're\ngoing to be learning",
    "start": "2558520",
    "end": "2565059"
  },
  {
    "text": "what is you have is anyway\nand just learn it in Gaussian? Yeah.",
    "start": "2565060",
    "end": "2570339"
  },
  {
    "text": "So that's a great question. And I was going to-- that's excellent question. I was going to come to\nit right here, actually.",
    "start": "2570340",
    "end": "2578740"
  },
  {
    "text": "OK, the reason I could assume\nthat my probability model looks something like Gaussian--",
    "start": "2578740",
    "end": "2584740"
  },
  {
    "text": "I could have actually\nassumed it anything-- is because at the\nend of the day, I am going to train a lot--",
    "start": "2584740",
    "end": "2590930"
  },
  {
    "text": "I'm going to train my parameters\nbased on this log function, right?",
    "start": "2590930",
    "end": "2596049"
  },
  {
    "text": "And my log function has\nthis log 1 over P of z hat. So it's going to-- it's\ngoing to try and produce",
    "start": "2596050",
    "end": "2602829"
  },
  {
    "text": "z hat, which tries to minimize\nmy log P of z hat, which",
    "start": "2602830",
    "end": "2608630"
  },
  {
    "text": "if you recall, gets minimized\nif my z hat, the actual produced empirical symbols, follow the\nsame distribution as the model",
    "start": "2608630",
    "end": "2616580"
  },
  {
    "text": "distribution, right? So if you remember the\nKL divergence idea, you'll have to go back\nmaybe 10 lectures now,",
    "start": "2616580",
    "end": "2624020"
  },
  {
    "text": "but we have kept on\nseeing this, right. So you are assuming\nthat my parameters-- I am encoding.",
    "start": "2624020",
    "end": "2631280"
  },
  {
    "text": "I'm going to compress my\ndiscrete distribution z hat,",
    "start": "2631280",
    "end": "2636320"
  },
  {
    "text": "assuming it has this probability\ndistribution, which is Gaussian of this and this, OK?",
    "start": "2636320",
    "end": "2643000"
  },
  {
    "text": "If my produced latent\nquantized, latent variable z hat don't have that distribution,\nthen the value of log 1 over P",
    "start": "2643000",
    "end": "2652240"
  },
  {
    "text": "will be higher, right? So that's basically saying\nI'm compressing a stream,",
    "start": "2652240",
    "end": "2660575"
  },
  {
    "text": "assuming some other distribution\nthan what it is actually coming from, OK? So when you try to optimize\nthis loss function, when",
    "start": "2660575",
    "end": "2667900"
  },
  {
    "text": "you try to minimize this\nloss function by trying to learn the parameters\nof encoder and decoder,",
    "start": "2667900",
    "end": "2674140"
  },
  {
    "text": "the parameters will\ntune themselves such that the actual\noutput value of z hat",
    "start": "2674140",
    "end": "2679810"
  },
  {
    "text": "starts mimicking whatever\ndistribution I chose out to be. ",
    "start": "2679810",
    "end": "2685590"
  },
  {
    "text": "OK? So that's why that\nworkaround is OK. You could have chosen anything. You should have asked\nlike [INAUDIBLE]",
    "start": "2685590",
    "end": "2691550"
  },
  {
    "text": "why are we doing Gaussian? Why not-- I don't know-- Laplacian exponential,\ngamma, zeta, beta,",
    "start": "2691550",
    "end": "2696890"
  },
  {
    "text": "whatever your favorite\ndistribution really is, binomial Bernoulli. It's just because it's\neasier to track Gaussian.",
    "start": "2696890",
    "end": "2703849"
  },
  {
    "text": "And as we will see, we'll\nprobably just touch upon it.",
    "start": "2703850",
    "end": "2709310"
  },
  {
    "text": "Like you can do better. So here, it's not just-- it's not even just\nGaussian, which we assumed.",
    "start": "2709310",
    "end": "2716869"
  },
  {
    "text": "We actually assumed\na normal 0, 1. What this is saying is\neach of my discrete symbols",
    "start": "2716870",
    "end": "2723140"
  },
  {
    "text": "which I'm outputting are\nI, Id, not just I, Id. They have mean 0 and variance 1.",
    "start": "2723140",
    "end": "2729380"
  },
  {
    "text": "You can clearly see I can\nimprove on this even before I say anything else, right? Like you could probably play\nwith the mean and variance",
    "start": "2729380",
    "end": "2736232"
  },
  {
    "text": "or have different\nmean and variance for each latent variable. But for now, let's assume that.",
    "start": "2736232",
    "end": "2741410"
  },
  {
    "text": "OK, this is the distribution. And then my learned parameters\nfor encoder and decoder will adjust themselves so that\nyou get this distribution.",
    "start": "2741410",
    "end": "2750760"
  },
  {
    "text": "And once you have this, now\nyou have this architecture. And now you can train it back\nlike all the way from I hat",
    "start": "2750760",
    "end": "2757930"
  },
  {
    "text": "to I, every place you can take\nthe gradient at each operation.",
    "start": "2757930",
    "end": "2763910"
  },
  {
    "text": "OK?  Next, we are going to look at--",
    "start": "2763910",
    "end": "2770530"
  },
  {
    "text": "let me just-- one second. OK. Next, we are going to\ngo through a Notebook",
    "start": "2770530",
    "end": "2777420"
  },
  {
    "text": "where we'll do a live demo\nof running through all of these operations. We're going to train a very\nsimple machine learning",
    "start": "2777420",
    "end": "2783000"
  },
  {
    "text": "compressor for images\nand see how it goes. But before I do\nthat, I'll actually do this again in the Notebook.",
    "start": "2783000",
    "end": "2788760"
  },
  {
    "text": "I really want-- so this\nidea is a bit tricky like what we discussed. And for me to really see\nthat we have understood this,",
    "start": "2788760",
    "end": "2795720"
  },
  {
    "text": "let me ask you a few questions. Let's discuss a\nfew questions, OK?",
    "start": "2795720",
    "end": "2800910"
  },
  {
    "text": "Let's assume I didn't have\nthis term in the loss function. I didn't care about\nthe distortion, OK?",
    "start": "2800910",
    "end": "2807750"
  },
  {
    "text": "This doesn't exist. Now, I try to learn\nmy encoder and decoder",
    "start": "2807750",
    "end": "2813420"
  },
  {
    "text": "with a loss function, which\nis just log 1 over P of z hat. And my P of z hat is\ndefined as the CDF",
    "start": "2813420",
    "end": "2821550"
  },
  {
    "text": "of z hat plus 0.5 minus this\nfor a normal variable, OK?",
    "start": "2821550",
    "end": "2828040"
  },
  {
    "text": "What do you expect\nmy z hats would be? ",
    "start": "2828040",
    "end": "2838330"
  },
  {
    "text": "So do people understand\nthe question? I don't have any distortion\nterm in my loss function.",
    "start": "2838330",
    "end": "2843670"
  },
  {
    "text": "I'm only trying\nto minimize this. And then I'm trying to learn\nthe parameters of e, which",
    "start": "2843670",
    "end": "2849640"
  },
  {
    "text": "is equivalent to\nsaying, I'm trying to figure out what my z hat\nshould look like, right?",
    "start": "2849640",
    "end": "2854650"
  },
  {
    "text": "So let me just try to minimize\nthis thing, log 1 over P.",
    "start": "2854650",
    "end": "2860710"
  },
  {
    "text": "And so what do you\nthink my z hats would look like if I tried to learn\nsuch an encoder/decoder pair?",
    "start": "2860710",
    "end": "2867385"
  },
  {
    "start": "2867385",
    "end": "2874060"
  },
  {
    "text": "It's fine. Any thoughts which are\ncoming to your head? ",
    "start": "2874060",
    "end": "2881970"
  },
  {
    "text": "So you want to\nminimize log 1 over P, which is equivalent to what?",
    "start": "2881970",
    "end": "2887145"
  },
  {
    "text": " Maximizing your P\nof z hat, right?",
    "start": "2887145",
    "end": "2893370"
  },
  {
    "text": "When is P of z hat maximized? ",
    "start": "2893370",
    "end": "2899540"
  },
  {
    "text": "It's like a Gaussian-- it's Gaussian-like\nlatent variables, right?",
    "start": "2899540",
    "end": "2904670"
  },
  {
    "text": "So when is P of z hat maximized? [? 7. ?] Exactly.",
    "start": "2904670",
    "end": "2910620"
  },
  {
    "text": "Let's make it like-- so when\nz hat is 0, at that point, you have the\nmaximum probability.",
    "start": "2910620",
    "end": "2915870"
  },
  {
    "text": "You have a probability\ndistribution model which assumes maximum\nprobability for z hat equals to 0, right?",
    "start": "2915870",
    "end": "2922710"
  },
  {
    "text": "So if I didn't have\nthis distortion term, and I just tried to\nminimize this log 1 over P,",
    "start": "2922710",
    "end": "2930510"
  },
  {
    "text": "and if I do a good job\nat minimizing this, all of my z hats will\nlook like 0, right?",
    "start": "2930510",
    "end": "2939770"
  },
  {
    "text": "But now you know that's\na very bad that-- what's the issue? Why don't I do that always?",
    "start": "2939770",
    "end": "2946500"
  },
  {
    "text": "In practice, why don't\nI always do that? You'll lose all information.",
    "start": "2946500",
    "end": "2952630"
  },
  {
    "text": "Exactly. So that's an excellent answer. Like if z hat is\nalways 0, you'll lose all the information.",
    "start": "2952630",
    "end": "2957790"
  },
  {
    "text": "That's like transmitting\nmaybe something like just that d, z value, right? So if all your\nvariables z hat are 0--",
    "start": "2957790",
    "end": "2966140"
  },
  {
    "text": "all your variables z\nhat are 0, your decoder can't really do anything. It has nothing to go\non forward with, right?",
    "start": "2966140",
    "end": "2971810"
  },
  {
    "text": "So at that point,\nit's completely lossy. You are basically sending\na static value, OK?",
    "start": "2971810",
    "end": "2978640"
  },
  {
    "text": "And so why doesn't\nour machine learning model actually do that? [INAUDIBLE]",
    "start": "2978640",
    "end": "2985010"
  },
  {
    "text": "OK, again, so the incentive\nreally here for our machine learning compressor is to\nspread out z hat, right?",
    "start": "2985010",
    "end": "2992270"
  },
  {
    "text": "So if I just keep my z\nhat at the exact same 0, I'm doomed, right? Like my rate would be 0.",
    "start": "2992270",
    "end": "2998533"
  },
  {
    "text": "I don't have to\ntransmit anything. The reconstructor\ncan just recover 0, but it will get a completely\nlost image, right?",
    "start": "2998533",
    "end": "3005589"
  },
  {
    "text": "So I have to spread out. I have to give other symbols\nz hat so that my decoder can",
    "start": "3005590",
    "end": "3011680"
  },
  {
    "text": "do something meaningful. And this term in\nthe loss function is basically ensuring\nthat you do that, right?",
    "start": "3011680",
    "end": "3019190"
  },
  {
    "text": "So there is also\na penalty you pay based on the distortion between\nimage and image hat, right?",
    "start": "3019190",
    "end": "3025310"
  },
  {
    "text": "So if you had a non-zero\nlambda, if this term exists, you are trading off.",
    "start": "3025310",
    "end": "3030530"
  },
  {
    "text": "You're not really reducing\njust the one term. Reducing this term would try\nto push everything towards 0.",
    "start": "3030530",
    "end": "3037400"
  },
  {
    "text": "But this term would\nsay, no, no, no, my distortion loss\nwill become very high, and so I wouldn't\nreally want to do that.",
    "start": "3037400",
    "end": "3043410"
  },
  {
    "text": "So essentially,\nbased on this value of lambda, whatever\nit might be, you are trying to figure out\nhow should I trade off",
    "start": "3043410",
    "end": "3049549"
  },
  {
    "text": "my rate with distortion really.  OK?",
    "start": "3049550",
    "end": "3056900"
  },
  {
    "text": "Is this point clear? This is probably one\nof the confusing points",
    "start": "3056900",
    "end": "3064210"
  },
  {
    "text": "of what's happening. So let me just summarize again. What we did in this\nwhole pipeline is,",
    "start": "3064210",
    "end": "3070090"
  },
  {
    "text": "there were two issues\nA, my quantizer was a discrete operation. Second, I had basically a table\nover which I was optimizing,",
    "start": "3070090",
    "end": "3078050"
  },
  {
    "text": "right? Like log 1 over P\nof z hat is a table. The way we work around\nthis workaround one",
    "start": "3078050",
    "end": "3084160"
  },
  {
    "text": "was OK for quantizer. I can just do a\nstraight-through, backward pass. And while forward, I\ncan just add some noise.",
    "start": "3084160",
    "end": "3091630"
  },
  {
    "text": "For my probability\ntable, I said I'll assume some distribution,\nwhich I can backpropagate from.",
    "start": "3091630",
    "end": "3097090"
  },
  {
    "text": "And then my encoder,\ndecoder-- we'll try to learn-- we'll try to output\nz hats, which have roughly similar distribution.",
    "start": "3097090",
    "end": "3103450"
  },
  {
    "text": "And the way we do this\ntrade-off is by-- this rate and distortion\nterm automatically",
    "start": "3103450",
    "end": "3109510"
  },
  {
    "text": "figures out this trade-off,\nunlike how much z hat you want to be spread\nversus concentrated.",
    "start": "3109510",
    "end": "3114580"
  },
  {
    "text": " OK? So any questions?",
    "start": "3114580",
    "end": "3121710"
  },
  {
    "text": "Otherwise, let's\njust-- so this Notebook will post on the website. It's also on the lecture slide.",
    "start": "3121710",
    "end": "3127230"
  },
  {
    "text": "But let's just do a live\ndemo of learned compression",
    "start": "3127230",
    "end": "3133859"
  },
  {
    "text": "while I'm starting\nto run this file. It will take some--\na little bit. So anyone has any\nquestions so far?",
    "start": "3133860",
    "end": "3140730"
  },
  {
    "text": "Do we understand the basic idea? Yeah.",
    "start": "3140730",
    "end": "3146550"
  },
  {
    "text": "We could also do an-- over\nhow we are quantizing, right? Yes. So that's-- actually, yes, we\ncan, but we are not doing that.",
    "start": "3146550",
    "end": "3158890"
  },
  {
    "text": "So do you-- there are\nvarious ways you can do that. I'm not sure what you\nhave in your mind. Do you mean like\nvarious discrete options",
    "start": "3158890",
    "end": "3165700"
  },
  {
    "text": "for quantization? Yeah, you can now bring\nreinforcement learning, do all that. Actually, some of\nthe recent works--",
    "start": "3165700",
    "end": "3173695"
  },
  {
    "text": "that's what they have done. Like there is MuZero. So there, they, for\nexample, try to introduce",
    "start": "3173695",
    "end": "3179200"
  },
  {
    "text": "some reinforcement learning to\ndo rate-distortion trade off.",
    "start": "3179200",
    "end": "3185500"
  },
  {
    "text": "And the idea was not just\nnot change the quantization, but roughly like\nthat, where you were playing with different losses--",
    "start": "3185500",
    "end": "3192640"
  },
  {
    "text": "discrete loss options. And then you figure out which\ndiscrete loss option to pick.",
    "start": "3192640",
    "end": "3197710"
  },
  {
    "text": "But another way you can\nthink of it is like--  it's actually very interesting.",
    "start": "3197710",
    "end": "3203470"
  },
  {
    "text": "There are lots of things\nyou can play with because-- OK, one idea could be you have\nimages, which is consisting",
    "start": "3203470",
    "end": "3208599"
  },
  {
    "text": "of various blocks. Why am I quantizing it using\nthe same quantization matrix,",
    "start": "3208600",
    "end": "3214040"
  },
  {
    "text": "right? Maybe I can do different\ndistortion trade-off per block. And overall, that will\ngive me a better image",
    "start": "3214040",
    "end": "3219580"
  },
  {
    "text": "because, OK, some blocks\nare just consistent. Others have a lot of\ndifferent things happening, so I can do that smartly.",
    "start": "3219580",
    "end": "3224590"
  },
  {
    "text": "I think that was one\npart of what they did. And you can play a lots of such\ntricks, but it becomes as--",
    "start": "3224590",
    "end": "3231340"
  },
  {
    "text": "if you have experience with RL,\nit becomes very computationally intensive very fast. But yeah, that's\na fair idea too.",
    "start": "3231340",
    "end": "3237760"
  },
  {
    "text": " OK, any other questions?",
    "start": "3237760",
    "end": "3243480"
  },
  {
    "text": "No. Cool. So then let's go\nover this Notebook. So here, we are going to\nbuild a learned compressor",
    "start": "3243480",
    "end": "3251170"
  },
  {
    "text": "for MNIST digits. So this is just an\nexample from the data set.",
    "start": "3251170",
    "end": "3256180"
  },
  {
    "text": "And we're going to do\nexactly what we talked about in the class. So we'll just go over each cell.",
    "start": "3256180",
    "end": "3261190"
  },
  {
    "text": "So first, we make this\nencoder and decoder transform. Sometimes these encoder\nand decoder transforms",
    "start": "3261190",
    "end": "3267190"
  },
  {
    "text": "are also called analysis\nand synthesis transforms. The idea is this is like--",
    "start": "3267190",
    "end": "3274089"
  },
  {
    "text": "OK, the idea is this\nencoder is like doing some analysis over the image\nand giving you this latents.",
    "start": "3274090",
    "end": "3279280"
  },
  {
    "text": "And your decoder is doing\nsynthesis of the image from this bottleneck layer. So this is just another\nterm for the same layers.",
    "start": "3279280",
    "end": "3286930"
  },
  {
    "text": "Here, all we are doing is like-- so again, if you've\ndone machine learning, you would appreciate it.",
    "start": "3286930",
    "end": "3292310"
  },
  {
    "text": "If not, it's OK. Just we are going to\nshow it in live code. So here, you are\njust determining--",
    "start": "3292310",
    "end": "3298540"
  },
  {
    "text": "defining your analysis\nand synthesis transform as some layers using Keras.",
    "start": "3298540",
    "end": "3303700"
  },
  {
    "text": "So our analysis transform is\njust some convolutional layers.",
    "start": "3303700",
    "end": "3309185"
  },
  {
    "text": "Don't worry about if\nyou haven't heard of it. If you have heard of\nit, that's what we have. And then we have some\nnonlinearities, which",
    "start": "3309185",
    "end": "3316160"
  },
  {
    "text": "are Leaky ReLU in our case. And so what this\nanalysis transform needs to do though is start\nfrom the size of image",
    "start": "3316160",
    "end": "3324200"
  },
  {
    "text": "and finally give you some latent\ndimensions, number of vectors, which is hopefully much\nmore small in number",
    "start": "3324200",
    "end": "3330980"
  },
  {
    "text": "than the image dimensions. That's the really\nimportant thing. And then your\nsynthesis transform",
    "start": "3330980",
    "end": "3336892"
  },
  {
    "text": "starts with a dimension,\nwhich is roughly of like this latent dimension,\nand then does, like,",
    "start": "3336892",
    "end": "3342890"
  },
  {
    "text": "upsampling in our case. So that's being done\nthrough this Conv2DTranspose in our case.",
    "start": "3342890",
    "end": "3349100"
  },
  {
    "text": "And that's basically the\narchitecture for your analysis and synthesis transform. So you can also look at\nwhat these layers look like.",
    "start": "3349100",
    "end": "3357070"
  },
  {
    "text": "Again, so you have\nsome output shapes, some number of parameters\nsimilarly for synthesis",
    "start": "3357070",
    "end": "3362440"
  },
  {
    "text": "transform. You have that so-- in our case, we have a total of\n1 million trainable parameters.",
    "start": "3362440",
    "end": "3368570"
  },
  {
    "text": "So you are tuning 1\nmillion different values. Theta is 1 million\ndimensional, OK, which you are going\nto be tuning based",
    "start": "3368570",
    "end": "3375040"
  },
  {
    "text": "on this data trying to\noptimize for rate distortion. Then we do some\nstandard image stuff,",
    "start": "3375040",
    "end": "3380535"
  },
  {
    "text": "which if you have started\nworking on homework four, you would have seen\nsimilar transform. You need to be careful about\nhow you are treating images.",
    "start": "3380535",
    "end": "3387535"
  },
  {
    "text": " OK, so here, let's just look at\nwhat an untrained network looks",
    "start": "3387535",
    "end": "3394680"
  },
  {
    "text": "like. So right now, we haven't\ndone any training. We just pre-processed the image. We apply the analysis\ntransform to get this y.",
    "start": "3394680",
    "end": "3402280"
  },
  {
    "text": "And then we use this\ny to get x tilde. So this is like our autoencoder\nstep, right, exactly that.",
    "start": "3402280",
    "end": "3408910"
  },
  {
    "text": "And then we optimize\nfor L1 loss. You can do L2 loss. You can do many\ndifferent things. That's not the point\nof this lecture,",
    "start": "3408910",
    "end": "3414742"
  },
  {
    "text": "but let's say we\nwork with L1 loss. And so this is what your\noriginal image looks like.",
    "start": "3414742",
    "end": "3420890"
  },
  {
    "text": "And this is what your\nreconstructed looked like. Like no surprises. Right now, we haven't\nreally trained this. We just have some random values\nfor this encoder, decoder.",
    "start": "3420890",
    "end": "3428260"
  },
  {
    "text": "Like why would you\neven expect the x hat to come out to be same, OK?",
    "start": "3428260",
    "end": "3433380"
  },
  {
    "text": "Now, what we are going\nto do is we are going to define a loss function, OK?",
    "start": "3433380",
    "end": "3439080"
  },
  {
    "text": "So remember like-- so we\nhave to determine some log 1 over P of z hat.",
    "start": "3439080",
    "end": "3444990"
  },
  {
    "text": "That was our\nestimated rate loss. And the way we did it was\nsome prior distribution,",
    "start": "3444990",
    "end": "3451380"
  },
  {
    "text": "z hat plus 0.5 minus\nprior distribution CDF z hat minus 0.5. So this function-- so\ngo home and look at it.",
    "start": "3451380",
    "end": "3458220"
  },
  {
    "text": "I'm not going to do each line. But basically, we are doing\nexactly same what we have discussed in this Notebook.",
    "start": "3458220",
    "end": "3463500"
  },
  {
    "text": "Nothing more fancier. Our MNIST compression\ntrainer basically",
    "start": "3463500",
    "end": "3470160"
  },
  {
    "text": "now takes care of\nthese two workarounds during training which\nwe talked about. So now we first\npre-process the image.",
    "start": "3470160",
    "end": "3476940"
  },
  {
    "text": "Then we do this analysis\ntransform or encoding. Then we add noise, right?",
    "start": "3476940",
    "end": "3482700"
  },
  {
    "text": "So this is to take care\nof the quantization step and your y tilde, which\nis an approximation",
    "start": "3482700",
    "end": "3488790"
  },
  {
    "text": "of your compressed latents\nis just y plus noise. And then your second workaround\nwas this, figuring out log 1",
    "start": "3488790",
    "end": "3496259"
  },
  {
    "text": "over P of z hat, right? So-- sorry. So your workaround one was just\nrecall, just adding some noise.",
    "start": "3496260",
    "end": "3505120"
  },
  {
    "text": "This is exactly-- this is\nexactly what this step does.",
    "start": "3505120",
    "end": "3510720"
  },
  {
    "text": "You just added some\nuniform random noise. And then your workaround two was\nto represent your probability",
    "start": "3510720",
    "end": "3518280"
  },
  {
    "text": "distribution as this\nCDF a minus b right? And this is what our estimated\nrate loss is actually",
    "start": "3518280",
    "end": "3524310"
  },
  {
    "text": "doing, right? It's basically just\ncomputing probability as differentiation of two CDFs.",
    "start": "3524310",
    "end": "3530410"
  },
  {
    "text": "And then it's computing the\nnegative log probability. Oops.",
    "start": "3530410",
    "end": "3535720"
  },
  {
    "text": "And this basically\ngives you the rate loss. And then you can\nalso find the x tilde",
    "start": "3535720",
    "end": "3541150"
  },
  {
    "text": "like recovered x, reconstructed\nx rate, which is just the synthesis transform\nover your approximation",
    "start": "3541150",
    "end": "3548470"
  },
  {
    "text": "of quantized variables,\nwhich is again-- ",
    "start": "3548470",
    "end": "3553940"
  },
  {
    "text": "so you got the z hat\nafter adding noise, which is y hat in the Notebook. Sorry, we should have\nkept the notation same.",
    "start": "3553940",
    "end": "3562620"
  },
  {
    "text": "And then from the z\nhat, you can apply the synthesis\ntransform to recover the original image I had.",
    "start": "3562620",
    "end": "3567650"
  },
  {
    "text": "This is what this step does. And your final loss basically\nconsists of both the rate loss",
    "start": "3567650",
    "end": "3574730"
  },
  {
    "text": "and the distortion loss. And our distortion loss is\njust the original image x minus x tilde.",
    "start": "3574730",
    "end": "3581590"
  },
  {
    "text": "OK? And so now we can just\nsee if this works. We can apply this\ntrainer to something",
    "start": "3581590",
    "end": "3588160"
  },
  {
    "text": "to get some values\nof rate distortion. And so in the next step, let's\njust go ahead and train it.",
    "start": "3588160",
    "end": "3595720"
  },
  {
    "text": "Let's just, like,\nbackpropagate our variables, take gradients, try to learn\nthese million parameters which",
    "start": "3595720",
    "end": "3602380"
  },
  {
    "text": "optimizes for rate, distortion. Here, I'm taking lambda\nto be 2,000 and my latent",
    "start": "3602380",
    "end": "3607810"
  },
  {
    "text": "dimensions to be just 50, OK? So MNIST is a data set\nwith 28 by 28 images.",
    "start": "3607810",
    "end": "3615460"
  },
  {
    "text": "So from 28 by 28\ndimension variable, we are just converting it into\nsome 50 dimension variable.",
    "start": "3615460",
    "end": "3621580"
  },
  {
    "text": "And so it's training\nlive, so you can see. So something which you see\nis that overall loss is going",
    "start": "3621580",
    "end": "3628630"
  },
  {
    "text": "down, which is a good sign. So we want the loss to reduce\nat each of these iteration.",
    "start": "3628630",
    "end": "3634060"
  },
  {
    "text": "So that's good. But if you see-- I don't know. Let's say rate loss. If you see rate loss,\nrate loss actually",
    "start": "3634060",
    "end": "3640900"
  },
  {
    "text": "increased first, right? But that's not a\n[AUDIO OUT],, right,",
    "start": "3640900",
    "end": "3646170"
  },
  {
    "text": "because we are not just\noptimizing for rate. We are optimizing for\nrate plus lambda d. And for this particular\nvalue of lambda,",
    "start": "3646170",
    "end": "3652860"
  },
  {
    "text": "I might be trading off\nmy rate and distortion. I'm more focused on maybe\ndistortion from the starting point than the initial point.",
    "start": "3652860",
    "end": "3659190"
  },
  {
    "text": "They're basically\ntrading off both these. OK, so this is training live.",
    "start": "3659190",
    "end": "3666055"
  },
  {
    "text": " So let it finish.",
    "start": "3666055",
    "end": "3672079"
  },
  {
    "text": "We'll just take a minute more. So while it finishes, so\nwhat's the next step, right?",
    "start": "3672080",
    "end": "3679730"
  },
  {
    "text": "So this is-- all of this we\ndid for training, correct? But after we have\nactually trained,",
    "start": "3679730",
    "end": "3686100"
  },
  {
    "text": "we'll just go back and apply a\nnormal pipeline, which is this. So we'll have the encoder.",
    "start": "3686100",
    "end": "3692039"
  },
  {
    "text": "We'll quantize it. We'll have some z hat. We'll assume it has this\nprobability distribution P of z hat and actually\nuse an arithmetic encoder.",
    "start": "3692040",
    "end": "3699870"
  },
  {
    "text": "So all of this was to find\nthe weights, encoder, decoder. But finally, our\ncompressor needs to come back and actually use\nsome real lossless compressor,",
    "start": "3699870",
    "end": "3708600"
  },
  {
    "text": "right? And so that's exactly what\nwe are going to do next. So at this point--\nyeah, so we are just",
    "start": "3708600",
    "end": "3715290"
  },
  {
    "text": "running it for 15 epochs,\nnot doing anything fancy. It's finished running. So the next step is basically\nto actually implement",
    "start": "3715290",
    "end": "3723630"
  },
  {
    "text": "my compressor. And my compressor here--",
    "start": "3723630",
    "end": "3729380"
  },
  {
    "text": "we basically use\nthe AEC compressor. So if you looked into the\ncode from homework three,",
    "start": "3729380",
    "end": "3734809"
  },
  {
    "text": "I guess, problem fourth, you\nprobably know what this is. But even otherwise, we have\nlooked at arithmetic entropy",
    "start": "3734810",
    "end": "3740490"
  },
  {
    "text": "coder. So this is just the\nimplementation from ICL. ",
    "start": "3740490",
    "end": "3746460"
  },
  {
    "text": "So this is a context\nadaptive encoder. But here, we are not really\nadapting the context.",
    "start": "3746460",
    "end": "3752070"
  },
  {
    "text": "We are just looking at the\nencoder from the image, OK? So now let's use an\nactual compressor",
    "start": "3752070",
    "end": "3759599"
  },
  {
    "text": "and see the performance. So now we can determine some--\ndefine some encoder and decoder",
    "start": "3759600",
    "end": "3765000"
  },
  {
    "text": "and use this to actually\nencode and decode images. So the first thing\nwhich you see is like--",
    "start": "3765000",
    "end": "3771150"
  },
  {
    "text": " OK, this is what the\nfrequency dictionary looks like when you actually\nencoded some images from--",
    "start": "3771150",
    "end": "3782350"
  },
  {
    "text": "what do I want to say? Yeah, so this is your\nactual frequency dictionary",
    "start": "3782350",
    "end": "3788770"
  },
  {
    "text": "from the empirically\nencoded images. So this is the\nactual distribution",
    "start": "3788770",
    "end": "3794260"
  },
  {
    "text": "of z hat, which you got when\nyou worked with this encoder and decoder, right?",
    "start": "3794260",
    "end": "3799390"
  },
  {
    "text": "And there should be a\nplot below, I think. Let's see where is that.",
    "start": "3799390",
    "end": "3805195"
  },
  {
    "text": " OK. ",
    "start": "3805195",
    "end": "3812700"
  },
  {
    "text": "Sorry, I think I might\nhave removed it in the end. OK, never mind.",
    "start": "3812700",
    "end": "3818310"
  },
  {
    "text": "So basically, if you\nlook at this plot, right, now you can\nsee this has roughly",
    "start": "3818310",
    "end": "3824900"
  },
  {
    "text": "what we expected to see, right? So 0-- most of the output\nlatent variables, z hat,",
    "start": "3824900",
    "end": "3831260"
  },
  {
    "text": "actually field at 0. And then you have this\nfall down criteria.",
    "start": "3831260",
    "end": "3838370"
  },
  {
    "text": "So basically here, this is\nwhat the actual my P of z hat looks like.",
    "start": "3838370",
    "end": "3843980"
  },
  {
    "text": "OK, and it roughly\nlooks like what we might have hoped to be Gaussian. The second thing\nwhich you see is",
    "start": "3843980",
    "end": "3849650"
  },
  {
    "text": "that if you look at the final\nrate and estimated rate,",
    "start": "3849650",
    "end": "3855380"
  },
  {
    "text": "they are very close. So final rate is\nwhen you actually apply an entropy encoder or\nwhatever discrete symbols",
    "start": "3855380",
    "end": "3862040"
  },
  {
    "text": "you got. Your estimated rate is just\nthe log 1 over P of z hat, where we had an assumed\nprobability model, right?",
    "start": "3862040",
    "end": "3869720"
  },
  {
    "text": "So this is a good sight. It basically says, again,\nour arithmetic encoder is doing the right job.",
    "start": "3869720",
    "end": "3875630"
  },
  {
    "text": "Like it's roughly\ncompressing things too. Actually, you can be more--",
    "start": "3875630",
    "end": "3881660"
  },
  {
    "text": "you can be more\nrigorous about this. We know that arithmetic\nentropy coder stays within 2 bits of overhead.",
    "start": "3881660",
    "end": "3887960"
  },
  {
    "text": "And if you look at this final\nrate and estimated rate, you will see, OK, they are\nroughly-- they are always",
    "start": "3887960",
    "end": "3893450"
  },
  {
    "text": "almost within 2 bits, right? So OK, our arithmetic\nencoder did the correct job.",
    "start": "3893450",
    "end": "3900260"
  },
  {
    "text": "And so at this point,\nOK, we can actually look at the reconstructed\nimages and what",
    "start": "3900260",
    "end": "3907640"
  },
  {
    "text": "the original images are. So on the left here,\nwhat we are showing is the original image\nx, and on the right",
    "start": "3907640",
    "end": "3914450"
  },
  {
    "text": "is the reconstructed image x hat\nusing this learned compressor, OK?",
    "start": "3914450",
    "end": "3919700"
  },
  {
    "text": "And so for example here\njust using 89 bits, you start with this 5, and\nyou get this reconstruction 5.",
    "start": "3919700",
    "end": "3927190"
  },
  {
    "text": "And so overall, this\ncompressor doesn't look back. We literally\n[AUDIO OUT] right now.",
    "start": "3927190",
    "end": "3932840"
  },
  {
    "text": "And it's doing\nsomewhat [AUDIO OUT] These are, again, 28 by 28, so\nquite 900 dimensional images.",
    "start": "3932840",
    "end": "3940810"
  },
  {
    "text": "Like bits and [AUDIO OUT]\nblack and whites are just 28",
    "start": "3940810",
    "end": "3945910"
  },
  {
    "text": "by 28, which is\nroughly like 900 bits, [? something ?] like\napproximately 90 bits.",
    "start": "3945910",
    "end": "3952810"
  },
  {
    "text": "We have come up with\na learned compressor, which reconstructs [AUDIO OUT]\nmost like the original.",
    "start": "3952810",
    "end": "3958309"
  },
  {
    "text": "So pretty cool, right? And we didn't have to\nworry about figuring out all the designing, all\nthe hand-tuned parameters",
    "start": "3958310",
    "end": "3966350"
  },
  {
    "text": "for this particular one. We figured out the\nexact transform. You didn't have to\nknow about, oh, there exists something called\nDCT, and so on and so forth",
    "start": "3966350",
    "end": "3973730"
  },
  {
    "text": "if you had this\ntechnology and everything. Before, you would have\nprobably rediscovered DCT",
    "start": "3973730",
    "end": "3979130"
  },
  {
    "text": "using something like this. So that's pretty cool. ",
    "start": "3979130",
    "end": "3985440"
  },
  {
    "text": "So the next thing which I just\nwant to show you-- oh, yeah. So here is the plot. OK.",
    "start": "3985440",
    "end": "3990770"
  },
  {
    "text": "I thought I had it above. Sorry. OK. So this is again, the same\nplot, which we looked back",
    "start": "3990770",
    "end": "3998559"
  },
  {
    "text": "in the lecture,\nright, so this thing.",
    "start": "3998560",
    "end": "4003980"
  },
  {
    "text": "But just to show you\nguys again, here, , the blue curve is the\nstandard normal PDF, OK?",
    "start": "4003980",
    "end": "4011660"
  },
  {
    "text": "The red is basically\nthe quantized PDF value. So this is, like, what\nyour model suggests.",
    "start": "4011660",
    "end": "4016940"
  },
  {
    "text": "So this is just the CDF of z hat\nplus 0.5 and z hat minus 0.5,",
    "start": "4016940",
    "end": "4023810"
  },
  {
    "text": "OK? So the red points are same\nas the blue dots here. This is what we expect\nour model to be.",
    "start": "4023810",
    "end": "4031280"
  },
  {
    "text": "And green is actually the\nempirical probabilities. So remember this z hat table,\nwhich I was showing you",
    "start": "4031280",
    "end": "4039200"
  },
  {
    "text": "for different values of z hat. This is, like, exactly\nthese values, green one.",
    "start": "4039200",
    "end": "4044330"
  },
  {
    "text": "So you can see the green\nis not exactly same as red, but it's roughly following\nthe red architecture, right?",
    "start": "4044330",
    "end": "4050840"
  },
  {
    "text": "And so it's roughly Gaussian. And there is a trade-off\nbetween what distortion and all those things,\nwhich will actually",
    "start": "4050840",
    "end": "4056300"
  },
  {
    "text": "determine the empirical\ndistribution compared to actual.",
    "start": "4056300",
    "end": "4061684"
  },
  {
    "text": "The final thing which\nI want to show is this. Like if you increase\nthe lambda-- OK, this is interesting.",
    "start": "4061685",
    "end": "4066870"
  },
  {
    "text": "So what do you think happens\nwhen you increase the lambda? ",
    "start": "4066870",
    "end": "4073540"
  },
  {
    "text": "So how do you actually do the\nrate-distortion trade-off, right? So we have trained a compressor. Actually, this is\na one tricky point",
    "start": "4073540",
    "end": "4080200"
  },
  {
    "text": "about the ML-based compressor,\nso learned compressors. So we have trained\nour compressors, which work really well for\na given value of lambda.",
    "start": "4080200",
    "end": "4087250"
  },
  {
    "text": "It trades off rate and\ndistortion very well when I fixed what\nshould be my trade-off.",
    "start": "4087250",
    "end": "4092530"
  },
  {
    "text": "But you can imagine if I\nchange my value of lambda, my learned rates\nwill be different.",
    "start": "4092530",
    "end": "4097870"
  },
  {
    "text": "And they might value rate\nmore or distortion more, which is something a\nknob, which we always want in lossy compression, right?",
    "start": "4097870",
    "end": "4103989"
  },
  {
    "text": "So in our case, it's lambda. So let's say here, earlier the\ncurves were with lambda 2,000.",
    "start": "4103990",
    "end": "4109420"
  },
  {
    "text": "Now, let's say I\nreduce lambda to 500. And I'm optimizing for rate\nplus lambda times distortion.",
    "start": "4109420",
    "end": "4115420"
  },
  {
    "text": "What do you expect? Do you expect your\nreconstructions to be more lossy or less lossy?",
    "start": "4115420",
    "end": "4121089"
  },
  {
    "text": "More lossy. OK, I hear more lossy. How many of you think it's\ngoing to be more lossy?",
    "start": "4121090",
    "end": "4126109"
  },
  {
    "text": " How many of you think it's\ngoing to be less lossy?",
    "start": "4126109",
    "end": "4133270"
  },
  {
    "text": "Cool. I think everyone\nalmost said more lossy, and some people are\nprobably not sure. Yes, exactly.",
    "start": "4133270",
    "end": "4138950"
  },
  {
    "text": "It's going to be more lossy\nbecause now you're putting less weight to distortion. You're reducing lambda.",
    "start": "4138950",
    "end": "4144103"
  },
  {
    "text": "So in your loss function,\nyou're putting less weight to distortion. So you're in a\nsense, saying, OK, I'm fine with giving me more\ndistortion, but reduce my rate.",
    "start": "4144103",
    "end": "4152180"
  },
  {
    "text": "Maybe I'm in a really\nnetwork constrained thing, constrained environment. Reduce my rate. I'm OK with taking\nmore distortion.",
    "start": "4152180",
    "end": "4159649"
  },
  {
    "text": "And so this exactly-- oops, I had actually trained it. Now I'm retraining it.",
    "start": "4159649",
    "end": "4165410"
  },
  {
    "text": "But this has gone to train,\nand this is exactly what we are going to see. Violet trains-- let me just come\nback here and talk a little bit",
    "start": "4165410",
    "end": "4173420"
  },
  {
    "text": "about what are some of\nthe benefits and issues with this learned compressor.",
    "start": "4173420",
    "end": "4178910"
  },
  {
    "text": "And then we'll come back,\nand I'll show you that part. So a couple of\nadvantages-- first,",
    "start": "4178910",
    "end": "4186079"
  },
  {
    "text": "there is no hand-tuning\nof parameters. You can learn the parameters. What that means is\nyou have adaptivity,",
    "start": "4186080",
    "end": "4193370"
  },
  {
    "text": "which was not possible with\ntraditional codecs, right? Your traditional codecs\nalways had the same matrices,",
    "start": "4193370",
    "end": "4198470"
  },
  {
    "text": "but now you can\nhave, for example, custom codec for each domain. You can have something for\ncartoons, something for games,",
    "start": "4198470",
    "end": "4204680"
  },
  {
    "text": "something for TV series. And you can figure out\ndifferent transform code values",
    "start": "4204680",
    "end": "4209985"
  },
  {
    "text": "for each of them,\nwhich performs better for that particular\nkind of data. You can have something\ncontent-aware",
    "start": "4209985",
    "end": "4215180"
  },
  {
    "text": "or even task-aware. So you basically\nget the flexibility because now you can train these\nend-to-end learned compressors",
    "start": "4215180",
    "end": "4220373"
  },
  {
    "text": "for a specific task. The second idea is you get\na better distortion model",
    "start": "4220373",
    "end": "4226340"
  },
  {
    "text": "separation, which basically says\nthat here, my distortion could have been anything, right, as\nlong as it's differentiable.",
    "start": "4226340",
    "end": "4235730"
  },
  {
    "text": "What that means is that-- so if you recall,\nlong time back, we talked about human vision.",
    "start": "4235730",
    "end": "4241159"
  },
  {
    "text": "And we said MSE is\nnot the right thing. Guess what? We train something right now,\nwhich was optimized for L1.",
    "start": "4241160",
    "end": "4248810"
  },
  {
    "text": "I could have easily\noptimized for L2. But nobody is\nstopping us at L2-- or L1.",
    "start": "4248810",
    "end": "4254300"
  },
  {
    "text": "We could have\nbasically plugged in any differentiable distortion\nmetric, which even maybe maps",
    "start": "4254300",
    "end": "4259760"
  },
  {
    "text": "human vision better or\nhuman perception better. And actually, a lot of\nwork has gone into this.",
    "start": "4259760",
    "end": "4264800"
  },
  {
    "text": "And we'll try to touch\na little bit on that in the next lecture, right? So you can basically\ncompletely replace distortion.",
    "start": "4264800",
    "end": "4271457"
  },
  {
    "text": "Remember, this is very hard to\ndo it in the traditional codecs because a lot of hand-designed\ndecisions were made,",
    "start": "4271457",
    "end": "4276470"
  },
  {
    "text": "assuming you are optimizing\nfor MSE or PSNR, right? So these are like\nreally the main benefits",
    "start": "4276470",
    "end": "4284420"
  },
  {
    "text": "of the learned image compressor. And let's just go back and see\nwhether this finished or not.",
    "start": "4284420",
    "end": "4290420"
  },
  {
    "text": "Oh, it actually finished. And now you see reconstructions. If you look at number of\nbits, you get much fewer bits.",
    "start": "4290420",
    "end": "4296780"
  },
  {
    "text": "Like for each of them, you're\napproximately getting 40 bits. So it indeed reduced the rate.",
    "start": "4296780",
    "end": "4301820"
  },
  {
    "text": "But now you can see the quality\ndeteriorated at some places, right? So like, if I look at 8 and 5\nhere, and they were still bad,",
    "start": "4301820",
    "end": "4310340"
  },
  {
    "text": "but it's better. Yeah, another thing\nyou see is that",
    "start": "4310340",
    "end": "4315370"
  },
  {
    "text": "these ML-based\ncompressors-- they are doing a lot of low-pass\nfiltering like smoothening,",
    "start": "4315370",
    "end": "4320620"
  },
  {
    "text": "which is somewhat,\nagain, we expect because it's hard to encode\nthe high-frequency components generally, right?",
    "start": "4320620",
    "end": "4327515"
  },
  {
    "text": "OK.  So last thing which I\njust want to cover today",
    "start": "4327515",
    "end": "4337480"
  },
  {
    "text": "is, so what are-- ",
    "start": "4337480",
    "end": "4342760"
  },
  {
    "text": "what are some of the issues? And even actually before that-- ",
    "start": "4342760",
    "end": "4349780"
  },
  {
    "text": "OK, let me-- OK, sorry. Let's just look at this.",
    "start": "4349780",
    "end": "4355800"
  },
  {
    "text": "Yeah, so this is\njust a benchmark of how these compressors\nperform in real life. So this is a paper from 2022.",
    "start": "4355800",
    "end": "4363480"
  },
  {
    "text": "Here, different colors\nand different compressors. The x-axis is bits per pixel,\nso higher value of x-axis",
    "start": "4363480",
    "end": "4371460"
  },
  {
    "text": "is higher rate. The y-axis is PSNR or MS-SSIM. Don't worry about them. These are just some metrics.",
    "start": "4371460",
    "end": "4377520"
  },
  {
    "text": "Here, these are quality metrics\ninstead of loss metrics, so higher value is better.",
    "start": "4377520",
    "end": "4382560"
  },
  {
    "text": "And what you see here is\nlike, this brown curve",
    "start": "4382560",
    "end": "4388200"
  },
  {
    "text": "is JPEG, this guy here. OK, this pink one is\nBPG, pink dashed one.",
    "start": "4388200",
    "end": "4396510"
  },
  {
    "text": "So this is BPG. It's this guy, OK? And there is this\nsomething called VVC/VTM,",
    "start": "4396510",
    "end": "4401975"
  },
  {
    "text": "which is one of\nthe, I would say, state-of-the-art\ntraditional codec, which is actually\nsitting somewhere here.",
    "start": "4401975",
    "end": "4407820"
  },
  {
    "text": "I don't know if you\ncan see it clearly. Can you? Like let's see this\ndash brown, OK?",
    "start": "4407820",
    "end": "4414790"
  },
  {
    "text": "And this is like really\nstate of the art, even includes some ML-based\ncomponents in the middle",
    "start": "4414790",
    "end": "4419980"
  },
  {
    "text": "for some small decision-making. But this is not an end-to-end\nlearned image compressor.",
    "start": "4419980",
    "end": "4426141"
  },
  {
    "text": "It's still traditional\nin the sense that it still have all the\ntraditional blocks, just uses something to learn a specific\nblock versus the blue one,",
    "start": "4426142",
    "end": "4432940"
  },
  {
    "text": "which you see, is\nwhat their work is, which is a learned\nimage compressor.",
    "start": "4432940",
    "end": "4439590"
  },
  {
    "text": "And you can see,\nin PSNR, basically, it's on the most on the top\nleft, which is what you want.",
    "start": "4439590",
    "end": "4444810"
  },
  {
    "text": "You want higher quality\nat lower bit rate. So it's performing best. So what I'm trying\nto highlight is",
    "start": "4444810",
    "end": "4450240"
  },
  {
    "text": "that these lower image\ncompresses are really state of the art. Like this VVC has almost done\n5 to 10 years of research",
    "start": "4450240",
    "end": "4459960"
  },
  {
    "text": "over various components,\nand blocks, and everything, and, like, big teams. And then now, you have\nlearned compressor,",
    "start": "4459960",
    "end": "4466530"
  },
  {
    "text": "which just comes and beats it. No more [INAUDIBLE]\noptimization. So it's really cool\nfrom that perspective, that given the amount of\neffort, the output you can get",
    "start": "4466530",
    "end": "4475020"
  },
  {
    "text": "is really crazy. And this is just PSNR. MS-SSIM is some\nother metric, which is known to optimize, correlate\nbetter with human vision.",
    "start": "4475020",
    "end": "4483030"
  },
  {
    "text": "And there, the gap is\neven higher, right? So here, you see BPG gives\nyou a lot of benefit.",
    "start": "4483030",
    "end": "4489210"
  },
  {
    "text": "This guy gives you a little bit\nmore, and so on and so forth. A lot of these\ntraditional codecs are optimized for PSNR, which is\nlike MSC, just MSC in dB scale.",
    "start": "4489210",
    "end": "4500170"
  },
  {
    "text": "But MS-SSIM, which is also by\nthe way, a lot of these codecs still look into. Even though it's a perceptual\nmetric, a lot of these codecs",
    "start": "4500170",
    "end": "4507700"
  },
  {
    "text": "have still tried to\noptimize for this, not JPEG. So you can see JPEG\nis very far away.",
    "start": "4507700",
    "end": "4514030"
  },
  {
    "text": "It's like this guy and all of\nthese guys versus this blue guy is like really beaten. So there's a lot of promise.",
    "start": "4514030",
    "end": "4521370"
  },
  {
    "text": "So not only you can have\nthese domain-aware things. It's really at the forefront. It's state of the art.",
    "start": "4521370",
    "end": "4526830"
  },
  {
    "start": "4526830",
    "end": "4534970"
  },
  {
    "text": "OK, so everything is green. Why is not-- why isn't there a\nML-based compressor everywhere?",
    "start": "4534970",
    "end": "4543460"
  },
  {
    "text": "Why do we-- why do\nwe still studied like for 17 or [INAUDIBLE]\nlectures of this class?",
    "start": "4543460",
    "end": "4549190"
  },
  {
    "text": "Why didn't we start with this? One of the main issues with\nML-based encoder, decoder",
    "start": "4549190",
    "end": "4555370"
  },
  {
    "text": "is speed [INAUDIBLE] Now, you have to train\nthis, for example, in a simple example,\nmillion parameters,",
    "start": "4555370",
    "end": "4562000"
  },
  {
    "text": "whereas in DCT or in\ncompression in general, I think really one point which\nwe hope we have driven home",
    "start": "4562000",
    "end": "4567520"
  },
  {
    "text": "is that complexity is\na really big thing. Like it really makes or breaks\nwhether something is practical,",
    "start": "4567520",
    "end": "4572949"
  },
  {
    "text": "whether something\nwill be used or not, whether something will\nbe deployed because these are like ubiquitous operations. They are just\nrunning all the time.",
    "start": "4572950",
    "end": "4579238"
  },
  {
    "text": "You need something efficient\nwhich can run at speed, right? So this is just\nlike a screenshot",
    "start": "4579238",
    "end": "4584680"
  },
  {
    "text": "from George Toderici's\ntalk at CVPR20. And what it shows here is\nlike H266 plus enhancement",
    "start": "4584680",
    "end": "4591579"
  },
  {
    "text": "is, again, one of the more\nadvanced traditional codec, which is still more\ncomputationally",
    "start": "4591580",
    "end": "4597110"
  },
  {
    "text": "intensive than JPEG. But this is what ML-based\ncodecs were using back then, which was like almost 20x\nmore computational complexity.",
    "start": "4597110",
    "end": "4607250"
  },
  {
    "text": "So now you can\nimagine, right, like, what does 20x more\nmean on the surface. It just feels like\nit's 20, right?",
    "start": "4607250",
    "end": "4613400"
  },
  {
    "text": "But no, it's like if\nsomething was taking you one hour to encode, decode, now\nit's going to take 20 hours.",
    "start": "4613400",
    "end": "4619550"
  },
  {
    "text": "Something was taking\nyou one minute. It's going to take 20 minutes. Now, think you are\nworking on your computer,",
    "start": "4619550",
    "end": "4624710"
  },
  {
    "text": "and you say, oh, this\nthing-- one minute. I'll wait. This will decompress. Instead of that, now, if\nyou have to wait 20 minutes,",
    "start": "4624710",
    "end": "4631280"
  },
  {
    "text": "will you use that technology? It's hard to justify that. So speed has really been one of\nthe areas where a lot of work",
    "start": "4631280",
    "end": "4641360"
  },
  {
    "text": "goes into. You don't have to\nworry about this. But even though\nthis is a blocker,",
    "start": "4641360",
    "end": "4647570"
  },
  {
    "text": "I think one thing which we\nshould really take into account is that machine learning, deep\nneural networks have really",
    "start": "4647570",
    "end": "4654170"
  },
  {
    "text": "taken world by storm. And compression is not the\nonly application, right? Like there are tons\nand tons and tons",
    "start": "4654170",
    "end": "4659960"
  },
  {
    "text": "of application on your\nphone or your devices to apply this, right?",
    "start": "4659960",
    "end": "4665300"
  },
  {
    "text": "So a lot of work has\nbeen really going on into hardware\nsupport for neural net. And now you can\nthink if you have",
    "start": "4665300",
    "end": "4671570"
  },
  {
    "text": "hardware support\nfor neural nets, then you can deploy\nall of this much more efficiently and faster\non any devices you want.",
    "start": "4671570",
    "end": "4678719"
  },
  {
    "text": "And this is happening\nnot only for compression, but to enable a\ntons of applications which are out there.",
    "start": "4678720",
    "end": "4685989"
  },
  {
    "text": "So this is happening anyways. And our compressors\ncan take advantage of this hardware lottery or\nhardware boom in this area.",
    "start": "4685990",
    "end": "4695710"
  },
  {
    "text": "By the way, we have\nbeen talking about this. We are not going to\ntouch this in the class. I don't think any team\nhas picked up a project",
    "start": "4695710",
    "end": "4703120"
  },
  {
    "text": "this time around this, but a\nlot of these image and video compressors, which\nwe talked about,",
    "start": "4703120",
    "end": "4709450"
  },
  {
    "text": "even traditional image\nand video compressors actually have hardware\nsupport in your computers. What that means is\nIntel makes chip",
    "start": "4709450",
    "end": "4716770"
  },
  {
    "text": "so that you can decode your\nimages or videos efficiently on the hardware. If you were just using\na commodity hardware,",
    "start": "4716770",
    "end": "4723969"
  },
  {
    "text": "you wouldn't be able to actually\nachieve the speeds which you would like. And again, what\nthis speech means",
    "start": "4723970",
    "end": "4729293"
  },
  {
    "text": "is whether you're\ngoing to watch YouTube or not, whether that circle of\ndeath on YouTube, or Netflix, or whatever it is, whether\nit's going to be one second,",
    "start": "4729293",
    "end": "4736869"
  },
  {
    "text": "or whether it's going\nto be one minute. So a lot of effort\ngoes into that. And so this is a problem\nwhich is there even",
    "start": "4736870",
    "end": "4742810"
  },
  {
    "text": "in traditional compressors. But already a lot of investment\nand work has gone into it. And hopefully, in\nneural nets also",
    "start": "4742810",
    "end": "4748489"
  },
  {
    "text": "we are going to see the same. And yeah, so this is\nlike a huge AI chip landscape like thousands--",
    "start": "4748490",
    "end": "4755050"
  },
  {
    "text": "not thousands, but\nhundreds of companies working to make things better. Qualcomm-- OK, this is a\nlittle bit older numbers,",
    "start": "4755050",
    "end": "4761740"
  },
  {
    "text": "but basically Qualcomm\nhas a recent chip which does extremely\nefficient ML inference. Today, we have M3, I\nthink, Apple chips,",
    "start": "4761740",
    "end": "4768430"
  },
  {
    "text": "which are doing the same. Nvidia chips are coming\nout for basically doing much more efficient\ncompression, decompression",
    "start": "4768430",
    "end": "4773920"
  },
  {
    "text": "for learning stuff on GPUs. So a lot of stuff is\nhappening in this field. So even though this is\na problem right now,",
    "start": "4773920",
    "end": "4779530"
  },
  {
    "text": "it's probably going\nto be resolved. And the second\nissue really is just determinism, which is\nsomething to be careful about",
    "start": "4779530",
    "end": "4787000"
  },
  {
    "text": "is like different\nhardwares might have different floating point\noperation implementations,",
    "start": "4787000",
    "end": "4792087"
  },
  {
    "text": "and that could lead to\njust catastrophic failures. Like CPU world has\nreally come together. I think GPU world is\nstill coming together.",
    "start": "4792087",
    "end": "4798340"
  },
  {
    "text": "Every Nvidia update--\nyou have to be careful, and what devices you\nare working with. So this is like really--",
    "start": "4798340",
    "end": "4804741"
  },
  {
    "text": "even though this\ndoesn't sound big, this is really important\nwhen you are working with this ML-based coders.",
    "start": "4804742",
    "end": "4809830"
  },
  {
    "text": "Like whatever you\nare doing should be reproducible across\nmachines and platforms. Otherwise, again,\nyou're not going",
    "start": "4809830",
    "end": "4816199"
  },
  {
    "text": "to decode what you thought\nyou were going to decode. OK. And so this last slide from\ntoday and then we are done.",
    "start": "4816200",
    "end": "4825610"
  },
  {
    "text": "So these are basically, I guess,\nthe main takeaways from today. So ML-based codecs-- they allow\nyou for learned encoder-decoder",
    "start": "4825610",
    "end": "4832540"
  },
  {
    "text": "transforms, better fidelity\nto chosen probability models, right? So that basically gives you\nthe better rate-distortion.",
    "start": "4832540",
    "end": "4839170"
  },
  {
    "text": "So this is a point which we\nspent a significant time today. You can choose\nwhatever distortion you want as long as it's\ndifferentiable, again,",
    "start": "4839170",
    "end": "4846070"
  },
  {
    "text": "very powerful because it\nallows you to just tomorrow if there is a\nbetter vision model, you can make a better\ncompressor for that.",
    "start": "4846070",
    "end": "4851560"
  },
  {
    "text": "Then you can have something\ndomain adaptable and flexible. The main idea to achieve the\nML-based things was like just",
    "start": "4851560",
    "end": "4859570"
  },
  {
    "text": "be able to differentiate\nacross this quantization and discrete probability\nmodels, and most likely are going to see more\nand more ML-based",
    "start": "4859570",
    "end": "4865840"
  },
  {
    "text": "codecs in your\nfuture compressors. So very hot and active area. Participate in it. Cool. That's it.",
    "start": "4865840",
    "end": "4871240"
  },
  {
    "text": "Thank you. ",
    "start": "4871240",
    "end": "4878000"
  }
]