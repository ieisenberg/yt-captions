[
  {
    "text": "Thanks. Yeah. I want to go through\nfactor analysis and continue our tour of\nEM, because it puts us",
    "start": "5140",
    "end": "10920"
  },
  {
    "text": "in this position\nwhere we are going to have to make some pretty\nserious modeling assumptions",
    "start": "10920",
    "end": "16030"
  },
  {
    "text": "to make progress. And it's kind of\ngoing to force us to walk through what that looks\nlike in an unsupervised way.",
    "start": "16030",
    "end": "21770"
  },
  {
    "text": "So that's what the\nfactor analysis piece is. We will then cover\na little bit of PCA, which is an old standby.",
    "start": "21770",
    "end": "27640"
  },
  {
    "text": "And it's good to contrast these\ntwo kind of methods together. It's kind of the old, more, less\nBayesian, less probabilistic",
    "start": "27640",
    "end": "36180"
  },
  {
    "text": "standby that people use a\nlot in unsupervised learning. Just to give you a sense of\nwhere we are in the course,",
    "start": "36180",
    "end": "45020"
  },
  {
    "text": "up next for us will\nbe this problem called ICA, which will be great. That's going to be\nin your homework.",
    "start": "45020",
    "end": "50290"
  },
  {
    "text": "It's always one of people's\nfavorite homework problems. This is the Cocktail\nParty Problem, which we'll go through.",
    "start": "50290",
    "end": "55410"
  },
  {
    "text": "And if we have time-- I'm not sure we will. But if we have time on\nour current trajectory, we'll go through something\ncalled weak supervision, which",
    "start": "55410",
    "end": "62769"
  },
  {
    "text": "is a setup that looks just\nlike EM, this Expectation Maximization setting,\nexcept for we can solve",
    "start": "62769",
    "end": "68590"
  },
  {
    "text": "the underlying problem exactly. So some folks were asking\nquestions last time, hey, why don't I run method\nx, y, or z on this?",
    "start": "68590",
    "end": "75100"
  },
  {
    "text": "And there are these latent\nproblems where, actually, you can cut to the chase and exactly\nsolve to the right answer.",
    "start": "75100",
    "end": "81210"
  },
  {
    "text": "You don't have to run this\nback-and-forth style algorithm. And so we'll see\nthat if we have time",
    "start": "81210",
    "end": "86900"
  },
  {
    "text": "and give you a sense of\nwhy that's interesting. This is more modern stuff\nthat we'll talk about.",
    "start": "86900",
    "end": "93470"
  },
  {
    "text": "All right? Now, on to factor\nanalysis, this is what we're going to talk about. And this is the setting\nwhere we have more dimensions",
    "start": "93470",
    "end": "100009"
  },
  {
    "text": "than we have data points. Now, just by way\nof history, when we used to give these lectures\nabout factor analysis,",
    "start": "100009",
    "end": "105710"
  },
  {
    "text": "it seemed like kind\nof an odd situation. Why would you have so\nmany more dimensions that you cared\nabout in your model",
    "start": "105710",
    "end": "111790"
  },
  {
    "text": "than you have data points? But actually,\nweirdly, if anything in the last couple of years,\nmodern machine learning",
    "start": "111790",
    "end": "116899"
  },
  {
    "text": "has switched to that\nbeing almost the default. We tend to train much, much\nlarger models, as you saw,",
    "start": "116900",
    "end": "122060"
  },
  {
    "text": "than we have available data. And there's a couple\nof reasons for that. They're not the reasons\nthat are in factor analysis, just to be clear.",
    "start": "122060",
    "end": "128509"
  },
  {
    "text": "But this setting is actually\na pretty interesting one. And so we'll see how people\naddressed it initially.",
    "start": "128509",
    "end": "134750"
  },
  {
    "text": "Awesome. So let's talk about factor\nanalysis, factor analysis.",
    "start": "134750",
    "end": "143770"
  },
  {
    "text": "All right, so we have many\nfewer points, data points,",
    "start": "143770",
    "end": "157760"
  },
  {
    "text": "than dimensions. That's the setting\nthat we're in, OK? And terse notation, this\nis n is much less than d.",
    "start": "157760",
    "end": "165470"
  },
  {
    "text": "OK? So it's worth comparing\nthis, by the way, with GMMs.",
    "start": "165470",
    "end": "173040"
  },
  {
    "text": "In GMMs, if you remember,\nwe had only a couple of parameters, right? We had the source centers. We had the source variances.",
    "start": "173040",
    "end": "178959"
  },
  {
    "text": "And we had the fraction\nthat were in each source. But we assumed implicitly--\nwe had a ton of photons,",
    "start": "178959",
    "end": "185640"
  },
  {
    "text": "if you remember, from all\nof the different things that were going on, from\nall the different sources.",
    "start": "185640",
    "end": "191069"
  },
  {
    "text": "And so n was much,\nmuch greater than d. OK? And that was implicit\nin what we were doing, but it's worth calling out.",
    "start": "191070",
    "end": "197800"
  },
  {
    "text": "And the reason is we would\naverage over some sources, and we didn't have to\nworry about a couple",
    "start": "197800",
    "end": "203530"
  },
  {
    "text": "of problems, which we'll\nhighlight in this lecture. OK? All right, so let me give you\nan example of how this happens",
    "start": "203530",
    "end": "209090"
  },
  {
    "text": "because at first, you may\nlook at that and say, well, is that really realistic? And even though I tell you it's\ngoing to become the default",
    "start": "209090",
    "end": "216680"
  },
  {
    "text": "setting, it's good to have\nsomething concretely in mind. So what's one way\nthat this happens?",
    "start": "216680",
    "end": "222040"
  },
  {
    "text": "So one example, which is\nactually Stanford-based, is that we could place\nsensors all over campus, OK,",
    "start": "222040",
    "end": "230500"
  },
  {
    "text": "all over campus. And let's say those\nsensors read tons of information,\ntemperatures, and wind speed, all kinds of things.",
    "start": "230500",
    "end": "236900"
  },
  {
    "text": "And so they record at\nthousands of locations, at thousands of locations,\nlocations, and values, OK?",
    "start": "236900",
    "end": "247260"
  },
  {
    "text": "So just a huge amount of\ninformation-- but so d is in the thousands\nor tens of thousands,",
    "start": "247260",
    "end": "253140"
  },
  {
    "text": "or tens of thousands. OK? So these are like little\nsmartdust sensors everywhere.",
    "start": "253140",
    "end": "258320"
  },
  {
    "text": "OK? But they only record for 30\ndays, but only for 30 days.",
    "start": "258320",
    "end": "268490"
  },
  {
    "text": "So we get-- n here\nis we get 30 samples. Now, the sample we\nget, if you like, is a measurement across\nthe entirety of campus.",
    "start": "268490",
    "end": "275090"
  },
  {
    "text": "So it's like we're\ngetting a matrix that's in the opposite direction\nfrom what we're used to. It's skinny, but it's\nnot tall and skinny.",
    "start": "275090",
    "end": "280870"
  },
  {
    "text": "It's n rows, and then the\nrows are really, really big. OK? All right.",
    "start": "280870",
    "end": "286530"
  },
  {
    "text": "So I would just point out,\nmaybe it's not obvious why. It will be obvious,\nhopefully, in a second that we would want to\nfit a density to this.",
    "start": "286530",
    "end": "297970"
  },
  {
    "text": "But it kind of seems hopeless. And the reason it seems\nhopeless is every model that we've had had a\nparameter for every dimension.",
    "start": "297970",
    "end": "307220"
  },
  {
    "text": "And so now there's a\nhuge number intuitively, a huge number of models\nthat are out there, that",
    "start": "307220",
    "end": "312330"
  },
  {
    "text": "will all satisfy this data. If we were to run something\nlike least squares, if you only gave me 5 examples,\nand you had 1,000 dimensions,",
    "start": "312330",
    "end": "320590"
  },
  {
    "text": "there are tons of models that\nline up on those 5 points, right, that touch\nthat rank 5 subspace.",
    "start": "320590",
    "end": "326650"
  },
  {
    "text": "But all the rest\nof them are free. So how do we pick from it? And if we try to do a\nprobabilistic thing, it gets even worse.",
    "start": "326650",
    "end": "332930"
  },
  {
    "text": "OK? So let's see the key idea. And if it's not obvious to\nyou that it's a problem, we'll show you something\nvery concrete in a moment",
    "start": "332930",
    "end": "340250"
  },
  {
    "text": "that it's a problem. So this is mainly\nintuition right now. This is not\nmathematically grounded. We'll show you the math in a\nsecond, and what will happen",
    "start": "340250",
    "end": "346620"
  },
  {
    "text": "is you'll see our\nequations will just break in some fundamental way. And so our key idea to get out\nof this is that we're going",
    "start": "346620",
    "end": "353870"
  },
  {
    "text": "to assume there is\nsome latent structure--",
    "start": "353870",
    "end": "360639"
  },
  {
    "text": "because that's the tool\nwe're using right now-- random variable that\nis not too complex.",
    "start": "360639",
    "end": "368680"
  },
  {
    "text": "And not too complex means\nthat I can estimate it. That's really what it means\noperationally; and captures",
    "start": "368680",
    "end": "377620"
  },
  {
    "text": "most of the behavior, captures\nthe interesting behavior, right? And this is the tired\nmaxim in this course",
    "start": "377620",
    "end": "383970"
  },
  {
    "text": "that all models are wrong. Some models are useful. Right? It's not going to be a perfect\nmodel of what's going on, but it captures most of\nthe interesting behavior",
    "start": "383970",
    "end": "390250"
  },
  {
    "text": "in this relatively compact way. And so we'll see, at the\nend of this little section, we'll see a concrete\ngenerative model",
    "start": "390250",
    "end": "397240"
  },
  {
    "text": "that builds with\nthese building blocks and allows us to recover\nthose parameters. That's the key idea.",
    "start": "397240",
    "end": "403570"
  },
  {
    "text": "OK, so let's see\nthe first problem with why GMMs would\nhave challenges here",
    "start": "403570",
    "end": "408860"
  },
  {
    "text": "and see the first example. And actually, we're going to\nlook at something even simpler than GMMs. We're going to look at\nfitting a single Gaussian.",
    "start": "408860",
    "end": "416080"
  },
  {
    "text": "OK? So let's try and fit a single\nGaussian in the situation so I can make more concrete\nmy assertion before,",
    "start": "416080",
    "end": "421800"
  },
  {
    "text": "like it's hard to\nfit models here. So let's fit one Gaussian.",
    "start": "421800",
    "end": "427900"
  },
  {
    "text": "OK? So the Gaussian\nhas two parameters. Recall there's mu\nand sigma squared.",
    "start": "427900",
    "end": "435190"
  },
  {
    "text": "So how would we be\ntempted to compute them? So first, we want to compute\nthe mean of our data. Well, that actually\nseems to make sense.",
    "start": "435190",
    "end": "441490"
  },
  {
    "text": "We compute the sum here-- oops-- sum over all\nof our data 1 to n.",
    "start": "441490",
    "end": "449080"
  },
  {
    "text": "So just to make sure, let\nme write the data because I should have written it out. We have data points\nRn, element of Rd,",
    "start": "449080",
    "end": "455930"
  },
  {
    "text": "just to make sure those\ntypes are in your head. OK?",
    "start": "455930",
    "end": "461400"
  },
  {
    "text": "This is OK. This works fine. We compute the mean. That seems fine.",
    "start": "461400",
    "end": "466790"
  },
  {
    "text": "It doesn't matter how high\nthe dimensions are, right? You can compute the mean, and\nit's a sensible thing to do. Right?",
    "start": "466790",
    "end": "472340"
  },
  {
    "text": "It may not be a great\nestimate of the center, because you may not have all\ncombinations of the directions. But it's still an\nOK enough estimate.",
    "start": "472340",
    "end": "480310"
  },
  {
    "text": "The trouble comes when we\nlook at the covariance. And I'm going to write\nhere the covariance in the full general\nform because we need",
    "start": "480310",
    "end": "485990"
  },
  {
    "text": "to talk about the dimension. Because the dimension\nis high, so I can't just use the 1 sigma.",
    "start": "485990",
    "end": "493020"
  },
  {
    "text": "And if you remember, you have\nsome expression which roughly looks like this, Xi minus mu t.",
    "start": "493020",
    "end": "500698"
  },
  {
    "text": "OK, so that's a fair\nenough quantity. You can form that quantity. But one thing should cause\nus a little bit of pause.",
    "start": "500699",
    "end": "508669"
  },
  {
    "text": "What's the rank of sigma? Well, generally,\nwe've been assuming,",
    "start": "508669",
    "end": "516729"
  },
  {
    "text": "when we did linear regression\nand everything else, the rank was at least d,\nthe number of dimensions. It was a full ranked\nobject, right?",
    "start": "516729",
    "end": "523919"
  },
  {
    "text": "But the problem here is that\nd is much bigger than n. And I've just shown you\nthat sigma can be written",
    "start": "523919",
    "end": "531080"
  },
  {
    "text": "as a sum of n rank 1 vectors. And if you remember\nyour linear algebra,",
    "start": "531080",
    "end": "537290"
  },
  {
    "text": "that means that the\nrank here is less than-- which is always true-- the minimum of n and\nd, which, in this case,",
    "start": "537290",
    "end": "545279"
  },
  {
    "text": "is going to be strictly\nless than d because n is smaller than d. That's what we're assuming.",
    "start": "545279",
    "end": "551010"
  },
  {
    "text": "OK? So this isn't full rank. So where does that\ncause us a problem? Well, we have to go back and\nwrite our favorite function,",
    "start": "551010",
    "end": "557620"
  },
  {
    "text": "the Gaussian likelihood, and\nsee where this causes a problem. Likelihood-- OK.",
    "start": "557620",
    "end": "566810"
  },
  {
    "text": "So remember our favorite\nthing looked like this.",
    "start": "566810",
    "end": "572170"
  },
  {
    "text": "Sigma is equal to at 1\nover 2 pi sigma 1/2 exp--",
    "start": "572170",
    "end": "582350"
  },
  {
    "text": "xi minus mu transpose\nsigma inverse xi--",
    "start": "582350",
    "end": "588410"
  },
  {
    "text": "oop-- oh, there should be no xi. So it should just be x. Let's get through this.",
    "start": "588410",
    "end": "594300"
  },
  {
    "text": "Get to the bottom\nof it, and so on.",
    "start": "594300",
    "end": "602070"
  },
  {
    "text": "OK, awesome. Why is this formula\nproblematic if sigma doesn't have full rank?",
    "start": "602070",
    "end": "607680"
  },
  {
    "text": "Well, there's two\nparts that kind of don't make sense, right? One of them is this.",
    "start": "607680",
    "end": "614529"
  },
  {
    "text": "If it's not full rank, the\ninverse doesn't make sense. It's not even defined.",
    "start": "614529",
    "end": "621410"
  },
  {
    "text": "And second, this\nis the determinant.",
    "start": "621410",
    "end": "629699"
  },
  {
    "text": "What's the determinant? Now you're toast\nbecause that's 1 over 0. This is now infinity,\nor something like it,",
    "start": "629700",
    "end": "637320"
  },
  {
    "text": "or undefined. No matter what, it's\ngoing to cause you a problem if you\nstart to normalize by something like this.",
    "start": "637320",
    "end": "642430"
  },
  {
    "text": "OK? So how are we going to fix this? Right? So this, by the way, just\nto make sure that's clear,",
    "start": "642430",
    "end": "647899"
  },
  {
    "text": "this determinant is equal to 0. Now, we're going to fix\nthese issues-- just one",
    "start": "647899",
    "end": "654970"
  },
  {
    "text": "second-- we're going to fix\nthese issues by changing the model. And that model is\ngoing to come out. And the thing that I\nwant you to think about",
    "start": "654970",
    "end": "660899"
  },
  {
    "text": "is we're going to try and make\nthat covariance full rank. OK? And we're going to see various\nassumptions that we can place",
    "start": "660899",
    "end": "667350"
  },
  {
    "text": "on the underlying noise model-- this will make\nsense in a minute-- that allow us to insist that\nsigma, in fact, is full rank.",
    "start": "667350",
    "end": "674660"
  },
  {
    "text": "Yeah, you had a question. Yeah, I just [INAUDIBLE]? Oh, because the determinant is\na product of the eigenvalues,",
    "start": "674660",
    "end": "680149"
  },
  {
    "text": "and at least some\nnumber of them are 0. Cool.",
    "start": "680149",
    "end": "685810"
  },
  {
    "text": "Awesome. OK, so the way we're\ngoing to fix this is we're",
    "start": "685810",
    "end": "691420"
  },
  {
    "text": "going to examine\nthese simple models. Now, I'm going to put\nthese models pedagogically.",
    "start": "691420",
    "end": "696639"
  },
  {
    "text": "And why I'm doing\nit in this order is the final model is going\nto take these building blocks and put them together.",
    "start": "696640",
    "end": "702100"
  },
  {
    "text": "OK? So if you were really\ninto the suspense of what's going on\nwith our stigmas here, I've ruined it for you.",
    "start": "702100",
    "end": "707980"
  },
  {
    "text": "That's what's going to\nhappen in about 20 minutes. But what I'm going to show you\nare different ways that we can get around this\nchallenge, so that we",
    "start": "707980",
    "end": "713950"
  },
  {
    "text": "can estimate the covariance,\nget a Gaussian likelihood for that setting,\nand they're all",
    "start": "713950",
    "end": "719230"
  },
  {
    "text": "going to boil down\nto ways that we make sure that our\ncovariance is parameterized",
    "start": "719230",
    "end": "724290"
  },
  {
    "text": "by something fewer\nthan all of the roughly n squared parameters. Covariance has to be\npositive semi definite.",
    "start": "724290",
    "end": "731490"
  },
  {
    "text": "But it still has a\nlot of parameters. So it's not all full. Not every matrix is a covariance\nmatrix, but a lot of them are.",
    "start": "731490",
    "end": "739819"
  },
  {
    "text": "All right, so let's\ngo through it. Now, let me just recall\nthe MLE for Gaussians",
    "start": "739820",
    "end": "749350"
  },
  {
    "text": "because it will make our life\na little bit easier here--",
    "start": "749350",
    "end": "756209"
  },
  {
    "text": "whoops. The reason I'm recalling this\nis I want to store some facts",
    "start": "756210",
    "end": "763209"
  },
  {
    "text": "or remind you of some\nfacts that will be useful. Now, one thing, just so\nyou're clear on using this,",
    "start": "763209",
    "end": "770029"
  },
  {
    "text": "these are just helpful things\nto know about computing these EM models again and again.",
    "start": "770029",
    "end": "775769"
  },
  {
    "text": "So hopefully, this\nis useful practice even if you're not\ncomputing these. So remember, the way we\nfit these things to data",
    "start": "775769",
    "end": "782370"
  },
  {
    "text": "looks like this. We sum over all of the data. We take the log of 1 over 2\ntimes the likelihood exp of--",
    "start": "782370",
    "end": "792320"
  },
  {
    "text": "oh, sorry, there's\na negative 1/2 here. That doesn't change the story.",
    "start": "792320",
    "end": "801350"
  },
  {
    "text": "inverse x minus mu, and so on. OK? Now, this is equivalent to\nminimizing the log, which",
    "start": "801350",
    "end": "811740"
  },
  {
    "text": "we do everywhere and\nexpand out, which is equivalent to minimizing this. And you've seen this before.",
    "start": "811740",
    "end": "817300"
  },
  {
    "text": "I just want to remind\nyou where it comes from. Sum i equals 1 to n x minus\nmu transpose sigma inverse.",
    "start": "817300",
    "end": "826990"
  },
  {
    "text": "This is why I\nconsistently drop the 1/2 because it doesn't matter. But it is distracting.",
    "start": "826990",
    "end": "832820"
  },
  {
    "text": "OK? All right. All right, now,\none key fact, which",
    "start": "832820",
    "end": "841029"
  },
  {
    "text": "we will use again and\nagain, and I'm just going to dispense of once,\nis if sigma is full rank,",
    "start": "841029",
    "end": "848389"
  },
  {
    "text": "the covariance is full rank,\nthen sigma u of this function",
    "start": "848389",
    "end": "855449"
  },
  {
    "text": "here-- so I'll just call--",
    "start": "855450",
    "end": "860870"
  },
  {
    "text": "whoop-- f of mu-- or fi of mu is equal\nto the sum i equals 1.",
    "start": "860870",
    "end": "870580"
  },
  {
    "text": "Let me extend the purple\nblob, pink blob, 1 to n,",
    "start": "870580",
    "end": "876660"
  },
  {
    "text": "sigma inverse xi minus\nu implies mu equals 1",
    "start": "876660",
    "end": "884490"
  },
  {
    "text": "by n sum i equals 1n. So I apologize if\nthis is too pedantic. If not, what I'm saying\nhere is, this is a plug-in.",
    "start": "884490",
    "end": "891889"
  },
  {
    "text": "If we know, no matter\nwhat, that sigma is full rank, then I can use\nthis estimate for mu.",
    "start": "891889",
    "end": "900230"
  },
  {
    "text": "OK? So I was saying before that\nthe average was no problem. So long as sigma is of the right\nform, I can always do this.",
    "start": "900230",
    "end": "907220"
  },
  {
    "text": "And remember why. If this is full rank,\nI can pull it out. This is the same argument\nwe went through last time. So ask me a question\nif it's not clear,",
    "start": "907220",
    "end": "913290"
  },
  {
    "text": "because you have\nseen this before. And so then we satisfy for mu. We can just use that as\na plug-in for everything",
    "start": "913290",
    "end": "919100"
  },
  {
    "text": "we do later. Sure. Well, why couldn't we only\nput it out if it's full rank? So if it's not full\nrank, so let's imagine",
    "start": "919100",
    "end": "925290"
  },
  {
    "text": "that it had some null space. Then this x minus mu could\nbe satisfied by either being",
    "start": "925290",
    "end": "931860"
  },
  {
    "text": "identically 0, which would be in\nthe null space no matter what, or anything that's co-linear\nwith the null space, that has--",
    "start": "931860",
    "end": "938680"
  },
  {
    "text": "so if the null space is\na particular direction, then it doesn't define mu. It can be anything along\nthat direction will",
    "start": "938680",
    "end": "945310"
  },
  {
    "text": "be set to 0 by this, right? So it means the mean wouldn't\nbe the unique minimizer of it.",
    "start": "945310",
    "end": "951820"
  },
  {
    "text": "Cool. Yeah, it'd be 0 plus\nnothing Go ahead.",
    "start": "951820",
    "end": "958890"
  },
  {
    "text": "Oh, so it affects\nthe [INAUDIBLE].. That's exactly right\nand the summation.",
    "start": "958890",
    "end": "964550"
  },
  {
    "text": "It was just so I didn't\nhave to copy it again. Yeah, this is just exactly this. And yeah, this is\nthe exact calculation",
    "start": "964550",
    "end": "970470"
  },
  {
    "text": "we've seen a couple of times. [INAUDIBLE] complication\non the first line that we have done\nout here, right?",
    "start": "970470",
    "end": "977320"
  },
  {
    "text": "[INAUDIBLE] Yeah, so this is\nbasically saying, these two are equivalent, right? We saw this before, that if we\nwanted to maximize likelihood,",
    "start": "977320",
    "end": "983470"
  },
  {
    "text": "it was equivalent to minimizing\nthis loss functional. I want to use that\nbecause I want to--",
    "start": "983470",
    "end": "988519"
  },
  {
    "text": "this is a lot simpler,\nand it doesn't have all kinds of\nextraneous terms and things that I need to deal with.",
    "start": "988519",
    "end": "993980"
  },
  {
    "text": "And so I want to\nget to the point where I can just\noperate on this, so that I can give you some\nmathematical facts when",
    "start": "993980",
    "end": "999670"
  },
  {
    "text": "I draw the pictures. That's why I'm doing it. Yeah, you could do everything in\nterms of the original equation.",
    "start": "999670",
    "end": "1006100"
  },
  {
    "text": "There's no harm or\nfoul for doing that. And yeah, you may prefer\nto do it that way.",
    "start": "1006100",
    "end": "1013589"
  },
  {
    "text": "Cool. And we're using the fact that\nthe constants don't matter and all the usual stuff\nwhen we go to minimizing.",
    "start": "1013589",
    "end": "1021110"
  },
  {
    "text": "OK, so let's see our\nfirst building block.",
    "start": "1021110",
    "end": "1030660"
  },
  {
    "text": "There's a picture coming. I don't know why I think this\nwill not pique your interest, but I'm still going to tell you. There's a picture coming that\nI think is very fun to draw,",
    "start": "1030660",
    "end": "1039100"
  },
  {
    "text": "that uses all the\nbuilding blocks together. And I'll highlight for\nyou when we get there. It's a real privilege to\ndraw this ridiculous picture",
    "start": "1039100",
    "end": "1044520"
  },
  {
    "text": "for you. And it's going to be really\ndisappointing after this setup. So I'm very excited for you. That like maximizes\nmy enjoyment, OK?",
    "start": "1044520",
    "end": "1051179"
  },
  {
    "text": "So here, I want to think\nabout the first building block that we're going to draw. This is not a fun picture\nto draw, but it's OK.",
    "start": "1051179",
    "end": "1057059"
  },
  {
    "text": "So suppose that we assume\nthat every direction has",
    "start": "1057059",
    "end": "1062279"
  },
  {
    "text": "independent and identical,\nindependent and identical,",
    "start": "1062280",
    "end": "1069549"
  },
  {
    "text": "correlation or covariance. So if I think about the noise\nball, what does it look like?",
    "start": "1069549",
    "end": "1074990"
  },
  {
    "text": "I'll draw 2d here. Oops, come on. Snap to.",
    "start": "1074990",
    "end": "1081270"
  },
  {
    "text": "So what this means is that our\nmodel of Gaussian noise, if you think about it-- so\nthis will be sigma",
    "start": "1081270",
    "end": "1086779"
  },
  {
    "text": "is equal to sigma squared I, OK? So that means it's I. And\nthe only free parameter",
    "start": "1086780",
    "end": "1094889"
  },
  {
    "text": "is this sigma squared,\nwhich is a scalar. This is a scalar. OK? So how many free\nparameters are there?",
    "start": "1094890",
    "end": "1100610"
  },
  {
    "text": "Precisely one, right? The structure is, the covariance\nis, sigma squared times I. So what does that\nactually look like?",
    "start": "1100610",
    "end": "1106610"
  },
  {
    "text": "It looks like I have a point\nwhich can be centered anywhere. The Gaussians can be\ncentered anywhere. They can have arbitrary means.",
    "start": "1106610",
    "end": "1112908"
  },
  {
    "text": "But their covariance\nstructure looks like circles. It's one circle, second circle.",
    "start": "1112909",
    "end": "1119419"
  },
  {
    "text": "They should be concentric,\nthe limitations of my notability skills.",
    "start": "1119420",
    "end": "1125809"
  },
  {
    "text": "OK? There could be\nanother one down here. OK, they all have the same.",
    "start": "1125809",
    "end": "1134590"
  },
  {
    "text": "OK? So you may look at this and\nsay-- so this is the pictures. These covariances are circles.",
    "start": "1134590",
    "end": "1139620"
  },
  {
    "text": "But I was just\ntrying to emphasize what this restriction means.",
    "start": "1139620",
    "end": "1145320"
  },
  {
    "text": "And so that circle is just\nparameterized by two points, by two numbers and vectors,\nthe center, the mu, and then",
    "start": "1145320",
    "end": "1154840"
  },
  {
    "text": "its radius. OK? And that's what\nsigma squared is. OK?",
    "start": "1154840",
    "end": "1160870"
  },
  {
    "text": "So in this situation,\nwhat is the MLE?",
    "start": "1160870",
    "end": "1166440"
  },
  {
    "text": "What is the x? Well, we know mu because\nthis is full rank. Why is this full rank? Because it's scalar\ntimes the identity. As long as sigma squared\nis bigger than 0,",
    "start": "1166440",
    "end": "1173860"
  },
  {
    "text": "this is a full\nrank matrix, right? Rank is exactly n.",
    "start": "1173860",
    "end": "1179710"
  },
  {
    "text": "So what is the MLE? Well, we're minimizing now\nover one free parameter.",
    "start": "1179710",
    "end": "1192779"
  },
  {
    "text": "And it looks something like\nthis, 1 to n xi minus mu.",
    "start": "1192780",
    "end": "1202030"
  },
  {
    "text": "Except for when we take\nthe transpose here, it's all times a scalar. So I'll pull the\nscalar out in front,",
    "start": "1202030",
    "end": "1207840"
  },
  {
    "text": "and it will be minus\nsigma squared, OK,",
    "start": "1207840",
    "end": "1213799"
  },
  {
    "text": "plus d log sigma squared. Right? So the determinant of this thing\nis the dimension times sigma",
    "start": "1213800",
    "end": "1221280"
  },
  {
    "text": "squared. Well, the determinant of this\nthing is sigma to the 2d. And the log of that is d\ntimes log sigma squared.",
    "start": "1221280",
    "end": "1229380"
  },
  {
    "text": "OK? Determine as a product\nof the eigenvalues. All the eigenvalues\nare sigma squared. How many eigenvalues\nare there? d.",
    "start": "1229380",
    "end": "1235520"
  },
  {
    "text": "So I'm just saying in a very\ncomplicated-- unnecessarily complicated way this is true.",
    "start": "1235520",
    "end": "1241039"
  },
  {
    "text": "Then I'm just taking the log\nand writing it in that form. OK?",
    "start": "1241039",
    "end": "1247190"
  },
  {
    "text": "So just for notational purposes,\nlet z equal sigma squared.",
    "start": "1247190",
    "end": "1252899"
  },
  {
    "text": "What does this\nactually look like? Well, it's minimizing",
    "start": "1252900",
    "end": "1258700"
  },
  {
    "text": "this thing is a constant here. This does not depend on\nsigma squared in any way. So I'm just going to call\nthis c, c plus d log z--",
    "start": "1258700",
    "end": "1272640"
  },
  {
    "text": "min z. So just take the derivative. What do I get?",
    "start": "1272640",
    "end": "1278080"
  },
  {
    "text": "Minus z over 2c plus\nd over z equals 0. And if I've done\nmy job correctly,",
    "start": "1278080",
    "end": "1286010"
  },
  {
    "text": "this is going to be z is\nequal to c over nd, i.e.",
    "start": "1286010",
    "end": "1291279"
  },
  {
    "text": "sigma squared equals c over nd. OK.",
    "start": "1291279",
    "end": "1297990"
  },
  {
    "text": "Just to write it to make sure\nit's-- just to unpack it so you understand, so you don't get\nlost in the weird notational",
    "start": "1297990",
    "end": "1303080"
  },
  {
    "text": "pieces, this is basically\nwhat I'm saying. This is the estimate. OK?",
    "start": "1303080",
    "end": "1308350"
  },
  {
    "text": "Pi minus mu, OK? Hopefully, this makes sense.",
    "start": "1308350",
    "end": "1316400"
  },
  {
    "text": "What is it saying? It's saying average over all\nof the different variances that are there. Treat them as if they're,\nbasically, nd of these things,",
    "start": "1316400",
    "end": "1322990"
  },
  {
    "text": "that are estimates, that\nyou're seeing sigma nd times. Average over them, and\nthat's what you should get.",
    "start": "1322990",
    "end": "1329799"
  },
  {
    "text": "So another way to describe the\nrule here is subtract the mean and square all the entries.",
    "start": "1329799",
    "end": "1336850"
  },
  {
    "text": "Subtract the mean and\nsquare roughly, OK?",
    "start": "1336850",
    "end": "1344970"
  },
  {
    "text": "OK, so what do you\nknow at this point? Oh, please. Yeah, it's just\na quick question.",
    "start": "1344970",
    "end": "1352278"
  },
  {
    "text": "On the second line,\nwhere we write the min, aren't we missing a sum\nover i to [INAUDIBLE]??",
    "start": "1352279",
    "end": "1360289"
  },
  {
    "text": "Oh, sorry. So yes, these are part\nof the two things. So when this comes out,\nthis should have an nd here. Sorry. Yeah, my simplification on\nthe fly to simplify my notes",
    "start": "1360289",
    "end": "1367670"
  },
  {
    "text": "did not make sense. That's a great catch. This thing is going to be c,\nand this sum is inside each",
    "start": "1367670",
    "end": "1373179"
  },
  {
    "text": "of the n's. And so this should be nd. Sorry about that,\nwonderful catch.",
    "start": "1373179",
    "end": "1378480"
  },
  {
    "text": "Awesome. Please. What is the [INAUDIBLE]\nof the sigma",
    "start": "1378480",
    "end": "1389909"
  },
  {
    "text": "squared and the previous sigma? Yeah, so the idea here is\nwe have some direction. And so one intuitive\nway to think about this is there's some radius\nthat makes sense.",
    "start": "1389910",
    "end": "1396559"
  },
  {
    "text": "And you know how to fit a\nradius on any dimension, right? So you know exactly\nhow to do this if it were one-dimensional.",
    "start": "1396559",
    "end": "1402149"
  },
  {
    "text": "And what this is\nbasically saying is I collapse all the\nvariance in all directions. And it's almost like\nI just lay them out in a line and average\nacross all of them.",
    "start": "1402149",
    "end": "1409049"
  },
  {
    "text": "And what I'm saying is\nthat's not too surprising. This math is the correct way to\nunderstand what should happen. That intuition is not\ngoing to help you too much,",
    "start": "1409049",
    "end": "1416010"
  },
  {
    "text": "but it at least will let you\ntype check that it makes sense, and it has the right dimension. And what's great there,\nas was pointed out,",
    "start": "1416010",
    "end": "1421720"
  },
  {
    "text": "was this nd is just averaging\nover all the entries, right? And the n has to come from\naveraging over all the entries.",
    "start": "1421720",
    "end": "1429150"
  },
  {
    "text": "Is this still the matrix, right? So sigma squared is a scalar. So it's just going to\nbe a single number.",
    "start": "1429150",
    "end": "1434960"
  },
  {
    "text": "But then we're going to\nmultiply it times the identity. So if you picture\nit, it's just going to be that number\nrepeated on the diagonal. Right, correct.",
    "start": "1434960",
    "end": "1440630"
  },
  {
    "text": "Yeah, please. This is a hypothetical situation\nwhen a covariance [INAUDIBLE] actually a scalar times\nan identity matrix.",
    "start": "1440630",
    "end": "1448250"
  },
  {
    "text": "Right. So yeah, so and\nany model is always going to be a\nhypothetical situation. We're always talking about,\nwhat are we modeling here?",
    "start": "1448250",
    "end": "1454620"
  },
  {
    "text": "And so what we're\nmodeling here is let's imagine a situation where\nthis would be appropriate. It'd clearly be\nperfect in the case",
    "start": "1454620",
    "end": "1459840"
  },
  {
    "text": "where we had noise that\nwe knew was actually the same magnitude in\nabsolutely every direction",
    "start": "1459840",
    "end": "1465809"
  },
  {
    "text": "and didn't depend on\nthe underlying data. We'll talk about\nprocedures that try and make us close to that in\na later part of the lecture.",
    "start": "1465809",
    "end": "1471809"
  },
  {
    "text": "But ignore that for one minute. That will probably never\nbe true empirically",
    "start": "1471809",
    "end": "1477500"
  },
  {
    "text": "for a variety of reasons. But this is a pretty\ngood approximation. If they don't\nfluctuate too much,",
    "start": "1477500",
    "end": "1482539"
  },
  {
    "text": "do you want to\npay the expressive power of having to model all the\ndifferent fluctuations in all the different possible ways,\nall the different ellipses",
    "start": "1482539",
    "end": "1488880"
  },
  {
    "text": "that you could possibly have? And this is saying, no, if they\nhave the even distribution,",
    "start": "1488880",
    "end": "1495070"
  },
  {
    "text": "then this is an OK model. Cool?",
    "start": "1495070",
    "end": "1502740"
  },
  {
    "text": "Let's see our second\nbuilding block. There are not lots\nof building blocks. There are two. Let's see our second\nbuilding block.",
    "start": "1502740",
    "end": "1510180"
  },
  {
    "text": "All right, building block 2.",
    "start": "1510180",
    "end": "1517620"
  },
  {
    "text": "So sigma is going\nto look like this. It's going to be\ndiagonal, sigma d square.",
    "start": "1517620",
    "end": "1526990"
  },
  {
    "text": "OK? Now, what does this\nlook like in my head?",
    "start": "1526990",
    "end": "1532910"
  },
  {
    "text": "These are axis-aligned ellipses. What do I mean? Well, they may\ndiffer like this--",
    "start": "1532910",
    "end": "1540409"
  },
  {
    "text": "oops. That's pretty good.",
    "start": "1540410",
    "end": "1547778"
  },
  {
    "text": "All right, so this\nnoise parameter",
    "start": "1547779",
    "end": "1554510"
  },
  {
    "text": "is still parameterized by a\nmean, which we know how to set. This is clearly full rank.",
    "start": "1554510",
    "end": "1559590"
  },
  {
    "text": "Why is this clearly full rank? Well, because all the\nnumbers are greater than 0. Its eigenvalues are all\npositive, so it's full rank.",
    "start": "1559590",
    "end": "1565960"
  },
  {
    "text": "it's also, by the\nway, it's clearly positive semi-definite\nor positive definite because all those\nnumbers are positive.",
    "start": "1565960",
    "end": "1571559"
  },
  {
    "text": "OK, so it is a\ncovariance matrix. And this is what it looks like. It's basically saying that\nthe ellipses, the errors,",
    "start": "1571559",
    "end": "1578179"
  },
  {
    "text": "are not correlated across\nmultiple dimensions. They're basically independent\non each of the dimension. And so the error\nprofile is going",
    "start": "1578179",
    "end": "1584039"
  },
  {
    "text": "to look like a flat ellipse. All right?",
    "start": "1584039",
    "end": "1590020"
  },
  {
    "text": "All right, so this\nis basically saying, I have dimensions\nthat differ wildly,",
    "start": "1590020",
    "end": "1595679"
  },
  {
    "text": "but I don't care about how\noften they interact, right? That's roughly the model that\nyou have in your head, right?",
    "start": "1595679",
    "end": "1600980"
  },
  {
    "text": "There's a lot more variance in\ncomponent 1 than component 2. That's worth modeling to me, but\nnot at all how they interact.",
    "start": "1600980",
    "end": "1607960"
  },
  {
    "text": "All right, so we're going to\nset zi equal to sigma i squared,",
    "start": "1607960",
    "end": "1614898"
  },
  {
    "text": "as above, same\nthing we did above. We're going to minimize\nnow over z1 to zd.",
    "start": "1614899",
    "end": "1624460"
  },
  {
    "text": "These are all scalar\nvalues, all positive, too. Doesn't matter too much.",
    "start": "1624460",
    "end": "1630028"
  },
  {
    "text": "Sum jd. Well, j equals 1.",
    "start": "1630029",
    "end": "1635700"
  },
  {
    "text": "I'll erase that--",
    "start": "1635700",
    "end": "1641510"
  },
  {
    "text": "xi minus mu j squared-- this is now a scalar--",
    "start": "1641510",
    "end": "1647350"
  },
  {
    "text": "plus log zi. OK? And this is inside. The parentheses are\ninside this thing.",
    "start": "1647350",
    "end": "1652780"
  },
  {
    "text": "I just write it\njust because it's the way it should associate. OK? Oh, that should be j. See if I made any other bugs.",
    "start": "1652780",
    "end": "1661279"
  },
  {
    "text": "Probably. We'll get there. All right, so far, so good. What does this look like?",
    "start": "1661280",
    "end": "1667259"
  },
  {
    "text": "Well, the reason I wrote it\nout like this-- so first, be clear about what we did here. This broke out per dimension.",
    "start": "1667260",
    "end": "1673370"
  },
  {
    "text": "So really this is just\nd independent problems.",
    "start": "1673370",
    "end": "1680789"
  },
  {
    "text": "Right? So what should we expect?",
    "start": "1680789",
    "end": "1685950"
  },
  {
    "text": "Well, we can go ahead\nand compute this thing. But it's going to look\nlike a problem we've",
    "start": "1685950",
    "end": "1692299"
  },
  {
    "text": "solved many times before. j-- sorry, that's j. That's the typo there.",
    "start": "1692299",
    "end": "1698240"
  },
  {
    "text": "j square plus log zj, which\nimplies sigma j square",
    "start": "1698240",
    "end": "1705760"
  },
  {
    "text": "equals 1 over n. Sum j equals 1 to n.",
    "start": "1705760",
    "end": "1711518"
  },
  {
    "text": "Sorry. j is the dimension. i equals 1.",
    "start": "1711519",
    "end": "1718110"
  },
  {
    "text": "xi minus mu i square. Let me make sure I have--",
    "start": "1718110",
    "end": "1724760"
  },
  {
    "text": "oh, mu j mu j. All right, let me\nwrite that again so",
    "start": "1724760",
    "end": "1729940"
  },
  {
    "text": "that it's clear, since there\nwere a bunch of typos there. Sum i goes from 1 to n\nxi j minus mu j square.",
    "start": "1729940",
    "end": "1745590"
  },
  {
    "text": "OK, so what's going on here? The thing that's\ngoing on here is we have d independent problems.",
    "start": "1745590",
    "end": "1752450"
  },
  {
    "text": "That means we have d\none-dimensional problems. And this is exactly the\nestimate that we had for one-dimensional covariance.",
    "start": "1752450",
    "end": "1758110"
  },
  {
    "text": "There's really\nnothing else going on. There are a couple of bits\nhere that we're using the jth mean and the jth component.",
    "start": "1758110",
    "end": "1763690"
  },
  {
    "text": "It's like every component\nwas handled independently. OK? All right, we are ready to\ntalk about our factor model.",
    "start": "1763690",
    "end": "1773289"
  },
  {
    "text": "OK? So what is the purpose of me\ngoing through all of this? One is I want you to be\npretty comfortable with going",
    "start": "1773289",
    "end": "1781490"
  },
  {
    "text": "back and forth between,\nhey, what is my MLE? What does the\nstructure I put in,",
    "start": "1781490",
    "end": "1786730"
  },
  {
    "text": "and how does that\nchange my estimate? OK? That actually is\npretty interesting. You put in the\ndifferent structure.",
    "start": "1786730",
    "end": "1791809"
  },
  {
    "text": "It reduces the number\nof free parameters. And that allows you\nto estimate things with far fewer amounts of data.",
    "start": "1791809",
    "end": "1797490"
  },
  {
    "text": "We're going to combine that now\nin a pretty interesting way, which is what the factor model\nactually is, our factor model.",
    "start": "1797490",
    "end": "1807070"
  },
  {
    "text": "OK? And the other reason\nto give this is it's an example of a more\nsophisticated generative model",
    "start": "1807070",
    "end": "1813640"
  },
  {
    "text": "than we've typically\nbeen used to. Yeah? Yes, can you go back\nto the previous page?",
    "start": "1813640",
    "end": "1818910"
  },
  {
    "text": "Sure. You got it. Could you explain the\ngraphical interpretation of this character here? Sure. So in general, a covariance\nmatrix is an ellipse.",
    "start": "1818910",
    "end": "1826180"
  },
  {
    "text": "You can picture\nthat ellipse, where the different directions\ncorrespond to the eigenvalues,",
    "start": "1826180",
    "end": "1831340"
  },
  {
    "text": "right? And so what's going on\nhere is that instead, because we are putting\nit on the diagonal, that",
    "start": "1831340",
    "end": "1840019"
  },
  {
    "text": "means that the stretch is\nonly on the individual axis. So you have something\nthat's axis-aligned.",
    "start": "1840020",
    "end": "1846649"
  },
  {
    "text": "The major axes\nhave to be aligned. So when would that be\na reasonable model? It'd be a reasonable\nmodel if you",
    "start": "1846649",
    "end": "1853290"
  },
  {
    "text": "thought there are different\namounts of spread-- or different amounts of variance\nfor every dimension, right?",
    "start": "1853290",
    "end": "1859980"
  },
  {
    "text": "The previous model said, all\nthe variance in any direction was, effectively, the same. There was one parameter. So that's why it was a circle.",
    "start": "1859980",
    "end": "1865460"
  },
  {
    "text": "Now it says, there's\ndifferent amounts in different\ndirections, but I'm not going to model\ntheir interaction. In contrast, a general\ncovariance matrix can tilt",
    "start": "1865460",
    "end": "1872429"
  },
  {
    "text": "and say, no, the\nprinciple direction, which we'll come to when we talk\nabout PCA, is in one direction. And I'm a little bit setting\nup for those calculations",
    "start": "1872429",
    "end": "1880220"
  },
  {
    "text": "later, is why I draw these. So this says, I don't\ncare about interactions between the features,\nvery roughly speaking.",
    "start": "1880220",
    "end": "1886300"
  },
  {
    "text": "OK, but basically,\nsaying that there's [INAUDIBLE] in each direction\nor they're all the same?",
    "start": "1886300",
    "end": "1892010"
  },
  {
    "text": "No, no, they're different. There's a different\nparameter for everything. So there's sigma potentially different numbers.",
    "start": "1892010",
    "end": "1898830"
  },
  {
    "text": "That's why it's an ellipse\nrather than a circle. If they were all the same,\nit would be the circle.",
    "start": "1898830",
    "end": "1904480"
  },
  {
    "text": "Cool. Awesome. Fantastic. So let's look at our parameters.",
    "start": "1904480",
    "end": "1909590"
  },
  {
    "text": "There are going to\nbe a lot of them. But it gives us a more\ninteresting generative model",
    "start": "1909590",
    "end": "1915120"
  },
  {
    "text": "to look at. And if you kind of wade\nyour way through this, you understand EM.",
    "start": "1915120",
    "end": "1921600"
  },
  {
    "text": "You understand how\nthese models work. And hopefully, it\ngives you confidence that you're not looking at\njust one setting, Rd by d.",
    "start": "1921600",
    "end": "1930050"
  },
  {
    "text": "It's going to be\na diagonal matrix. OK?",
    "start": "1930050",
    "end": "1935580"
  },
  {
    "text": "All right, so let's see the\nmodel and then the worlds.",
    "start": "1935580",
    "end": "1941500"
  },
  {
    "text": "I don't know. It's not really that remarkable\na picture, but I enjoy it.",
    "start": "1941500",
    "end": "1947010"
  },
  {
    "text": "All right, Px of z, so we\ndo the usual thing here, that it factors\nas a latent model.",
    "start": "1947010",
    "end": "1952360"
  },
  {
    "text": "This is exactly the same,\nsince z is our latent model. OK?",
    "start": "1952360",
    "end": "1957419"
  },
  {
    "text": "By the way, u is in Rd.",
    "start": "1957419",
    "end": "1963539"
  },
  {
    "text": "This is a linear transformation\nthat we're going to learn. This is a diagonal\ntransformation, hearkening back to what\nwe just talked about.",
    "start": "1963539",
    "end": "1970940"
  },
  {
    "text": "And here is the way\nx is distributed. x-- or sorry, z, start with z,\nis going to be some 0, 1.",
    "start": "1970940",
    "end": "1982549"
  },
  {
    "text": "So it's just some random\nnoise vector in Rs for some s smaller than d?",
    "start": "1982549",
    "end": "1988658"
  },
  {
    "text": "OK? I'm going to call\nthis s because I want you to think about\nit as the small dimension.",
    "start": "1988659",
    "end": "1994860"
  },
  {
    "text": "So the latent\nstructure is small. We want to pick the latent\nstructure to be much smaller.",
    "start": "1994860",
    "end": "2000019"
  },
  {
    "text": "d is 1,000, 10,000, a million,\na billion, something like this. We're going to pick s as saying\nthere's a small subspace that characterizes and classifies\nall of its behavior.",
    "start": "2000019",
    "end": "2007809"
  },
  {
    "text": "That's like the compression. That's the bottleneck. Then we're going to say x-- and I'll walk through\nthis slowly in a second--",
    "start": "2007809",
    "end": "2015210"
  },
  {
    "text": "is this expression, epsilon, OK? All right, so the model\nthat generates x--",
    "start": "2015210",
    "end": "2022950"
  },
  {
    "text": "and I'll write this\nmore compactly-- x is not just centered\nat the origin. It has some mean.",
    "start": "2022950",
    "end": "2029000"
  },
  {
    "text": "z is going to be mapped\nfrom the small dimension to the large dimension\nby a transformation that we're going to learn.",
    "start": "2029000",
    "end": "2035140"
  },
  {
    "text": "So imagine the sampling\nprocedure is z gets selected. If you knew lambda, you\nwould map it up into Rd.",
    "start": "2035140",
    "end": "2042730"
  },
  {
    "text": "You'll shift it by mu, the mean\nthat you're going to learn. And then you're going to\nput some epsilon noise here.",
    "start": "2042730",
    "end": "2052888"
  },
  {
    "text": "OK. All right. Now, let's get the\nmodel for these things.",
    "start": "2052889",
    "end": "2059148"
  },
  {
    "text": "So epsilon-- oops. Epsilon is going to be normal. It's noise, so\nit'll have 0 mean.",
    "start": "2059149",
    "end": "2066480"
  },
  {
    "text": "And it will have phi\nas its parameters.",
    "start": "2066480",
    "end": "2074020"
  },
  {
    "text": "This implies-- by the way, now\nwe have all the information. We could also write x is\ndistributed as N mu plus lambda",
    "start": "2074020",
    "end": "2082200"
  },
  {
    "text": "z plus phi. Please.",
    "start": "2082200",
    "end": "2087540"
  },
  {
    "text": "Oh, what is the physical\nmeaning of s [INAUDIBLE]??",
    "start": "2087540",
    "end": "2093340"
  },
  {
    "text": "Yeah, great question. So let me just say it. So I just want to\nannotate this to the mean. So I'm going to draw what goes\non on this, for an example,",
    "start": "2093340",
    "end": "2102710"
  },
  {
    "text": "in one second. That will answer this\na little bit better than I'm able to because\nonce you see what's going on. But the intuition\nis, you're going",
    "start": "2102710",
    "end": "2107880"
  },
  {
    "text": "to sample in the small space. You don't know where it is. That's your latent variable. That's your link to clusters.",
    "start": "2107880",
    "end": "2113380"
  },
  {
    "text": "That's the thing that you say,\nlike, I don't know what it is, but I know there's a small\nstructure lurking out there. Then lambda says,\nfrom that small space,",
    "start": "2113380",
    "end": "2119880"
  },
  {
    "text": "I'm going to go into the big\nspace, right, the d much higher dimensional space.",
    "start": "2119880",
    "end": "2124900"
  },
  {
    "text": "And lambda, I'm going to\nlearn that transformation that says, how do I go from the\nsmall space to the big space? And then the big space is going\nto be the actual dimensions.",
    "start": "2124900",
    "end": "2131670"
  },
  {
    "text": "So imagine I actually\nhad these sensors, and they were getting\ntemperature readings all over campus, and\nthey were getting",
    "start": "2131670",
    "end": "2137720"
  },
  {
    "text": "wind readings all over campus. Well, clearly, they're\ncorrelated, right? If I put a bunch of\nsensors all in a line,",
    "start": "2137720",
    "end": "2143300"
  },
  {
    "text": "they're not independent\nreadings, right? There's some sensor,\nand then there's some map that tells\nme, from the reading that I got, what should I expect\nat all the sensors with pretty",
    "start": "2143300",
    "end": "2150680"
  },
  {
    "text": "high probability? You could imagine\nsuch a map existing? And so the lambda\nis that map that goes from the low dimension\nto the high dimension.",
    "start": "2150680",
    "end": "2158109"
  },
  {
    "text": "We're just doing it in\na very abstract way, and we're not specifying\nwhat those dimensions even mean, to start with.",
    "start": "2158109",
    "end": "2163770"
  },
  {
    "text": "Please. This could be\n[INAUDIBLE] end rate. We probably don't want\nto set a greater than,",
    "start": "2163770",
    "end": "2169740"
  },
  {
    "text": "for a variety reasons. But yeah, you'll see in a\nsecond what the conditions are when you go to solve it.",
    "start": "2169740",
    "end": "2175890"
  },
  {
    "text": "Great question. There are some conditions\nthat's lurking there. You need to be able to estimate. There's no free lunch, right? But yeah, it's going\nto be smaller than d.",
    "start": "2175890",
    "end": "2182630"
  },
  {
    "text": "d is a billion, and n is 20. There exists some s where we\ncould potentially solve this. That's the way to\nthink about it, not",
    "start": "2182630",
    "end": "2188510"
  },
  {
    "text": "we can solve it for every s. Awesome. All right, so let's see an\nexample of this whole thing,",
    "start": "2188510",
    "end": "2197960"
  },
  {
    "text": "where d is equal to 2, so\nnot that high a dimension, but good for drawing.",
    "start": "2197960",
    "end": "2204319"
  },
  {
    "text": "s is equal to 1, and\nn is equal to 5, OK? And our model is x mu plus\nlambda z plus epsilon.",
    "start": "2204319",
    "end": "2213650"
  },
  {
    "text": "And let's see how the\nforward sampling works. So how does it work? One, we generate z1\nto zn from n 0, 1.",
    "start": "2213650",
    "end": "2229900"
  },
  {
    "text": "So what does this look like? All right, so here's 0.",
    "start": "2229900",
    "end": "2236270"
  },
  {
    "text": "And we get-- let me see\nif I can draw this nicely. We get a bunch of samples.",
    "start": "2236270",
    "end": "2241650"
  },
  {
    "text": "Oh, here's 0. Let me mark down 0. didn't look right--",
    "start": "2241650",
    "end": "2248570"
  },
  {
    "text": "is 0. So let me draw the\ndensity, actually. So the density looks\nsomething like this.",
    "start": "2248570",
    "end": "2255300"
  },
  {
    "text": "This is the Gaussian and loosely\ninterpreted artist's rendition of Gaussians.",
    "start": "2255300",
    "end": "2261589"
  },
  {
    "text": "Then what I'll do\nis I'll sample. So maybe I'll sample\nthis point first. And I sample over here, sample\nhere, sample here, sample here.",
    "start": "2261590",
    "end": "2267800"
  },
  {
    "text": "Those are my five points. How did I pick them? I don't know. I sampled.",
    "start": "2267800",
    "end": "2272910"
  },
  {
    "text": "So this is z-- oops. This is z1.",
    "start": "2272910",
    "end": "2278589"
  },
  {
    "text": "This is z2. I'm not remembering the\norder that I did it, and it doesn't really matter-- z3, z5, z4.",
    "start": "2278589",
    "end": "2286690"
  },
  {
    "text": "OK? So I just generated\nthose in one dimensions. All right, now\ntwo, what happens?",
    "start": "2286690",
    "end": "2292940"
  },
  {
    "text": "So now so far, I've\ndone this piece. So this character\nis handling this.",
    "start": "2292940",
    "end": "2299339"
  },
  {
    "text": "OK, now I map by lambda. So let's suppose we've\nalready learned lambda.",
    "start": "2299339",
    "end": "2307170"
  },
  {
    "text": "So lambda is equal to 1 2. So what happens next?",
    "start": "2307170",
    "end": "2312869"
  },
  {
    "text": "Now I get fancy. Take this, copy, paste.",
    "start": "2312869",
    "end": "2322130"
  },
  {
    "text": "So now we need the x-axis back. So these things get mapped.",
    "start": "2322130",
    "end": "2327490"
  },
  {
    "text": "Here's the line, 1 2, which\ngoes through the origin.",
    "start": "2327490",
    "end": "2333210"
  },
  {
    "text": "OK? What happens? So then these are all\nmapped onto the line.",
    "start": "2333210",
    "end": "2340780"
  },
  {
    "text": "Oops-- so small,\nbut good enough. This one's mapped all the way up\nhere, down here, and down here,",
    "start": "2340780",
    "end": "2350500"
  },
  {
    "text": "OK? So this point, for example, is--",
    "start": "2350500",
    "end": "2360920"
  },
  {
    "text": "this point is lambda z3. This point is lambda z2.",
    "start": "2360920",
    "end": "2368920"
  },
  {
    "text": "So far so good? So this whole thing, now\nwe're in the high dimension.",
    "start": "2368920",
    "end": "2377500"
  },
  {
    "text": "So we've gone from\nour small dimension up to the whole dimension\nof all our sensors. Think in your head dimension OK?",
    "start": "2377500",
    "end": "2386510"
  },
  {
    "text": "Now three, we add mu.",
    "start": "2386510",
    "end": "2392870"
  },
  {
    "text": "What does that do? I'll put that in green.",
    "start": "2392870",
    "end": "2403180"
  },
  {
    "text": "Oop. That's this piece.",
    "start": "2403180",
    "end": "2412580"
  },
  {
    "text": "Copy, paste. Mu, let's say, is\nthis vector here.",
    "start": "2412580",
    "end": "2419470"
  },
  {
    "text": "This is mu. We add mu to everything, and\nthat translates all our points",
    "start": "2419470",
    "end": "2427650"
  },
  {
    "text": "now by mu.",
    "start": "2427650",
    "end": "2434328"
  },
  {
    "text": "And there's one off the screen\nover here somewhere, OK?",
    "start": "2434329",
    "end": "2439349"
  },
  {
    "text": "This point is-- oops--",
    "start": "2439349",
    "end": "2448710"
  },
  {
    "text": "mu plus lambda z2. All right? It's this point\ntranslated by mu.",
    "start": "2448710",
    "end": "2457400"
  },
  {
    "text": "Then four, we add epsilon noise.",
    "start": "2457400",
    "end": "2463279"
  },
  {
    "text": "And this epsilon noise\nis full dimensional, so I'll make this in purple.",
    "start": "2463280",
    "end": "2469589"
  },
  {
    "text": "There's a character here.",
    "start": "2469589",
    "end": "2474680"
  },
  {
    "text": "This is the equation\nthat we're dealing with. What happens next? We get some purple stuff,\nand it goes like this--",
    "start": "2474680",
    "end": "2481430"
  },
  {
    "text": "whoops, make that a\nlittle bit thicker. It goes like this. It goes down here. It goes over there.",
    "start": "2481430",
    "end": "2487660"
  },
  {
    "text": "But it's full dimensional noise. It lives in a d\ndimensional space. And that is our\nfinal expression.",
    "start": "2487660",
    "end": "2493450"
  },
  {
    "text": "So for example, this\ncharacter here--",
    "start": "2493450",
    "end": "2499430"
  },
  {
    "text": "oops-- this character here is mu\nplus lambda z2 plus epsilon 2.",
    "start": "2499430",
    "end": "2507910"
  },
  {
    "text": "And epsilon 2 is the noise. And notice the noise is\nin different direction for each one. All right?",
    "start": "2507910",
    "end": "2513310"
  },
  {
    "text": "So I want this. By the way, this is in the\nhigh dimensional space, in dimension d. OK?",
    "start": "2513310",
    "end": "2519450"
  },
  {
    "text": "So what's going on here? We were talking about that\nsensor and temperature example. So just walk through.",
    "start": "2519450",
    "end": "2525010"
  },
  {
    "text": "The z's here were generated\nin one dimension, let's say. That was the underlying\ntrue temperature, that you were going to\nsee several samples of it,",
    "start": "2525010",
    "end": "2531400"
  },
  {
    "text": "maybe at different\ntimes of the day, right? Because z was collected\nat various different times for different data\npoints, right?",
    "start": "2531400",
    "end": "2536640"
  },
  {
    "text": "We were collecting it\nmany times in the day. Then we mapped,\nusing lambda, to say, like, oh, if the temperature\nwere the true temperature,",
    "start": "2536640",
    "end": "2543808"
  },
  {
    "text": "or some hidden temperature\nwas 50 degrees, then all of the other\ntemperatures that are nearby",
    "start": "2543809",
    "end": "2550808"
  },
  {
    "text": "are going to jiggle in\na predictable amount, predictable amount meaning\nlambda maps them from wherever their value was to the high\ndimensional space, right?",
    "start": "2550809",
    "end": "2559790"
  },
  {
    "text": "But the problem is, there's\nstill some residual noise. That fit may not\nbe perfect, right? We can't predict things.",
    "start": "2559790",
    "end": "2565059"
  },
  {
    "text": "Noise means just stuff\nwe don't want to model. And that's where\nthis purple comes in. That's the epsilon jitter\nthat's in the high dimension.",
    "start": "2565060",
    "end": "2571960"
  },
  {
    "text": "Does that make sense? And that's our underlying model.",
    "start": "2571960",
    "end": "2577490"
  },
  {
    "text": "And the data we see\nare the purple dots. So the data are the purple dots.",
    "start": "2577490",
    "end": "2582540"
  },
  {
    "text": "That's what we actually\nsee in our-- the data are the purple dots.",
    "start": "2582540",
    "end": "2588440"
  },
  {
    "text": "OK? Just to make sure, it's x equals\nmu plus lambda z plus epsilon.",
    "start": "2588440",
    "end": "2595869"
  },
  {
    "text": "All right, please? Oh, is it assuming that\nthe-- actually, the-- so even though the sensors\ngive us a big dimension d,",
    "start": "2595870",
    "end": "2603390"
  },
  {
    "text": "which is really huge,\nbut are we assuming that the actual\nthing that we want to model of, say, the\nactual temperature",
    "start": "2603390",
    "end": "2610380"
  },
  {
    "text": "is of the dimension that\nis much smaller than that?",
    "start": "2610380",
    "end": "2619309"
  },
  {
    "text": "That's exactly right,\nso much smaller than d. So what we're assuming\nis we have-- again, I think the illustrative\nexample, which",
    "start": "2619310",
    "end": "2625550"
  },
  {
    "text": "will get you most\nof the way there, is I put 1,000 temperature\nsensors in a row. So I get 1,000 numbers\nevery time I measure them.",
    "start": "2625550",
    "end": "2632640"
  },
  {
    "text": "But if they're all\nclose enough, it feels like there should\nbe one temperature, and there's a mapping from them.",
    "start": "2632640",
    "end": "2639700"
  },
  {
    "text": "That mapping we're\ngoing to learn. That's what lambda is. Lambda takes us from\nthat one true temperature to our guess at all the\ndifferent temperatures.",
    "start": "2639700",
    "end": "2646039"
  },
  {
    "text": "But we're aware that\nthat's imperfect. And since we're aware\nthat that's imperfect, we also allow ourselves some\nerror, epsilon, in each one.",
    "start": "2646040",
    "end": "2652690"
  },
  {
    "text": "Maybe one temperature\nsensor is damaged. It's dirty. Someone walked by it. Someone blew on it. Who knows?",
    "start": "2652690",
    "end": "2658420"
  },
  {
    "text": "That's what epsilon models. Does that make sense? And so it's the small to big. And that's what allows\nus to learn this because we're saying,\nalthough there's",
    "start": "2658420",
    "end": "2664010"
  },
  {
    "text": "this huge amount\nof free parameters, we only have a little\namount of data.",
    "start": "2664010",
    "end": "2669079"
  },
  {
    "text": "We have to make\nsome compression. And so that's what the\nlatent structure is doing. Please. I have two questions. So first is that [INAUDIBLE]\nis actually really [INAUDIBLE]",
    "start": "2669080",
    "end": "2676050"
  },
  {
    "text": "assuming the major assumption\nof value that this is something close to--",
    "start": "2676050",
    "end": "2681619"
  },
  {
    "text": "it's linearly--\nit's a lineral form of data just having a bit of\nnoise in it that's [INAUDIBLE]..",
    "start": "2681619",
    "end": "2691119"
  },
  {
    "text": "Yeah, so here we're making\nan assumption-- that's right. We're making an assumption that\nthere is a small distribute--",
    "start": "2691119",
    "end": "2696570"
  },
  {
    "text": "there's a distribution\nthat's low and that there's a linear\nmap up into the larger space.",
    "start": "2696570",
    "end": "2702230"
  },
  {
    "text": "You could imagine more\nsophisticated models that had a learnable, non-linear\nmap into the larger space.",
    "start": "2702230",
    "end": "2708440"
  },
  {
    "text": "And there are\nvarious regions that may be hard and\nlimited data regimes. You may not have enough\ndata to learn that piece.",
    "start": "2708440",
    "end": "2713680"
  },
  {
    "text": "The second question\npertains to s. So that s is equal to 1.",
    "start": "2713680",
    "end": "2720640"
  },
  {
    "text": "Right. And therefore, it's\neasier to imagine what's actually happening. But what would be the case\nwhen s is [INAUDIBLE]??",
    "start": "2720640",
    "end": "2726300"
  },
  {
    "text": "Yeah, wonderful\nquestion, and we'll get into this a\nlittle bit in PCA2. And sometimes they're\ninterpretable. Sometimes they're not.",
    "start": "2726300",
    "end": "2731640"
  },
  {
    "text": "But let's imagine, when\nwe come back to it, we'll have this language\nof principal components in about 15 minutes,\nthat will maybe",
    "start": "2731640",
    "end": "2736800"
  },
  {
    "text": "be able to give a little\nbit more precise answer. So I may punt there in a second. The idea is that s captures\nthe important directions",
    "start": "2736800",
    "end": "2742869"
  },
  {
    "text": "of variation. So if you imagine the\ntemperature sensor-- so I have temperature, and\nI have electrical current that are there.",
    "start": "2742869",
    "end": "2748900"
  },
  {
    "text": "And maybe there's\nalso some wind thing. And if I combined\nall three of those,",
    "start": "2748900",
    "end": "2753980"
  },
  {
    "text": "I would get a really\ngood estimate. But I need an estimate\nof the wind speed at a location and all the rest. Then s would be some\nmixture, potentially,",
    "start": "2753980",
    "end": "2761109"
  },
  {
    "text": "of those true directions\nthat are out there. Now, often, that story\nthat we tell ourselves",
    "start": "2761110",
    "end": "2766619"
  },
  {
    "text": "is difficult to verify\nbecause we don't actually get to see the latent variable. And in fact, we can\nget away with making",
    "start": "2766619",
    "end": "2771930"
  },
  {
    "text": "a much weaker\nassumption, which we'll make in PCA, which is\nbasically, that there exists some low dimensional\nspace that captures our system.",
    "start": "2771930",
    "end": "2778690"
  },
  {
    "text": "And that turns out to\nbe a fairly robust kind of assumptions. It's not that we know there are\nonly these three parameters.",
    "start": "2778690",
    "end": "2785349"
  },
  {
    "text": "If we knew it was\nwind and temperature, and we were just\nlearning the map, we would just feed\nthat in, right? We would just measure\nthat and feed that in.",
    "start": "2785349",
    "end": "2790740"
  },
  {
    "text": "We're hypothesizing that there\nexists this small bottleneck. And that's what's going to allow\nus to learn in this setting.",
    "start": "2790740",
    "end": "2795810"
  },
  {
    "text": "And to me, that's the really\nrich, pedagogical reason to teach this, is it\nforces you to think about,",
    "start": "2795810",
    "end": "2801599"
  },
  {
    "text": "what can you really recover? This is more of a statistics\nview of this part of machine learning. But what can you really\nrecover from data?",
    "start": "2801600",
    "end": "2807290"
  },
  {
    "text": "OK, so US entity, [INAUDIBLE]\ns, you got to [INAUDIBLE].. Yeah, you're just saying there\nexists something small in your",
    "start": "2807290",
    "end": "2815820"
  },
  {
    "text": "[INAUDIBLE]. Oh. You got it. Do you do this by-- Do you create this\ntransformation",
    "start": "2815820",
    "end": "2821369"
  },
  {
    "text": "for every data point? Wonderful question. So here, lambda is across\nall the different data points, this transformation.",
    "start": "2821369",
    "end": "2827510"
  },
  {
    "text": "So if you look at\nthis notation here, you'll see that z is\ngenerated once per data point.",
    "start": "2827510",
    "end": "2834280"
  },
  {
    "text": "So there are n of those\nthings and latent samples There are also n\nlatent noise samples.",
    "start": "2834280",
    "end": "2839569"
  },
  {
    "text": "Those are different for every\ndata point that are there. But lambda is shared\nacross all of them, as is mu, in the model.",
    "start": "2839570",
    "end": "2845099"
  },
  {
    "text": "Now, if you knew something, like\nyou knew there were k clusters, and there were k\ndifferent maps, you",
    "start": "2845100",
    "end": "2850119"
  },
  {
    "text": "could potentially fold\nthat into your model or k different means. But this is the model\nwe're looking at now. It doesn't necessarily\nneed to be that way,",
    "start": "2850119",
    "end": "2855960"
  },
  {
    "text": "but for now, that's\nwhere we're looking at. I have one more question.",
    "start": "2855960",
    "end": "2861920"
  },
  {
    "text": "Do you-- so from\nmy understanding is that we start from z,\nand then we work our way from generated z first. Correct.",
    "start": "2861920",
    "end": "2867690"
  },
  {
    "text": "Then work our way to the data. But how do we estimate, like, s? Or do we start from [INAUDIBLE].",
    "start": "2867690",
    "end": "2872750"
  },
  {
    "text": "Oh, great question-- I get\nexactly what you're saying. So when we went up here,\nremember in this model-- so one of the messages is\nwe start with this model.",
    "start": "2872750",
    "end": "2880430"
  },
  {
    "text": "And in the generative\nway of viewing things-- the real reason I\nlike to keep this in the class in a\nvery fundamental way",
    "start": "2880430",
    "end": "2886460"
  },
  {
    "text": "is because it forces you to\nthink about what is actually in the model. When you see the EM thing,\nit's in a similar situation,",
    "start": "2886460",
    "end": "2892740"
  },
  {
    "text": "where you're like,\noh, there's just this one hidden aspect to it. This z is latent to us.",
    "start": "2892740",
    "end": "2897800"
  },
  {
    "text": "So the part of the\nmodel is you pick s because it's one\nof the parameters. So you have to tell\nme ahead of time, I want you to model\nwith a size s model.",
    "start": "2897800",
    "end": "2905720"
  },
  {
    "text": "And then you run this\nwhole thing forward. And then our goal is,\namong all those things that could have\ngone forward, what are the most likely settings of\nthe parameters in that model?",
    "start": "2905720",
    "end": "2913559"
  },
  {
    "text": "So we're not learning\ns, in some sense. We'll talk about,\nin PCA, when we're learning a linear subspace,\nwe can, effectively,",
    "start": "2913559",
    "end": "2919790"
  },
  {
    "text": "look at a particular\nmeasure and see if adding one more\ndimension would be there. And it'll come\nwith some caveats--",
    "start": "2919790",
    "end": "2925089"
  },
  {
    "text": "great, great questions. Awesome. All right.",
    "start": "2925089",
    "end": "2932300"
  },
  {
    "text": "OK, so we need\none technical tool to make this whole thing go. Let's do some technical tools.",
    "start": "2932300",
    "end": "2938109"
  },
  {
    "text": "I'm not sure that these are\nsuper useful to prove here,",
    "start": "2938109",
    "end": "2946048"
  },
  {
    "text": "but I will happily point them\nout to you as we use them. All right, so here, we're\ngoing to use this notation.",
    "start": "2946049",
    "end": "2954230"
  },
  {
    "text": "You probably have\nseen this before. If not, don't worry. We'll review it right now.",
    "start": "2954230",
    "end": "2959680"
  },
  {
    "text": "x1 is at Rd1. X2 is at Rd2.",
    "start": "2959680",
    "end": "2965049"
  },
  {
    "text": "And d equals d1 plus d2. OK? So this is just a nice\nway of partitioning. This is because we're dealing\nwith something that's linear.",
    "start": "2965049",
    "end": "2971558"
  },
  {
    "text": "And we can have this\nkind of block structure. OK. We can also do\nthis for matrices. And that will allow us to state\nsome theorems just a little bit",
    "start": "2971559",
    "end": "2978859"
  },
  {
    "text": "more concisely. Sigma 1, 1; sigma 2, 1; sigma--",
    "start": "2978859",
    "end": "2985880"
  },
  {
    "text": "oh, so this should be 2, 1. Sorry, one is backwards; 1,",
    "start": "2985880",
    "end": "2993020"
  },
  {
    "text": "OK, I hope I didn't\ndo something else. All right, so this is d1.",
    "start": "2993020",
    "end": "3000470"
  },
  {
    "text": "This is d2. This is going to be d1,\nand this is going to be d2.",
    "start": "3000470",
    "end": "3007670"
  },
  {
    "text": "All right. And sigma ij, if\nI've done my job,",
    "start": "3007670",
    "end": "3015880"
  },
  {
    "text": "should be-- oh, that's why\nI did it the backwards. That makes more sense. Sigma d is going to be dj--",
    "start": "3015880",
    "end": "3022210"
  },
  {
    "text": "hold on. Let me actually change\nthe numbering just",
    "start": "3022210",
    "end": "3027838"
  },
  {
    "text": "to make it consistent\nwith the notes. Sorry. This doesn't really\nmatter conceptually,",
    "start": "3027839",
    "end": "3034020"
  },
  {
    "text": "but just to explain it. OK, 1, 2.",
    "start": "3034020",
    "end": "3039338"
  },
  {
    "text": "All right, so the point is, I\ncan factor this in some way. And it makes sense to multiply\na matrix in an obvious way",
    "start": "3039339",
    "end": "3046730"
  },
  {
    "text": "if it's compatible\nwith the block. So if I multiply this\nmatrix here, x times sigma,",
    "start": "3046730",
    "end": "3052970"
  },
  {
    "text": "I can write it in\nterms of these blocks. Hopefully, that's clear, right? And it would be sigma",
    "start": "3052970",
    "end": "3060240"
  },
  {
    "text": "and that would be what I\nwould want to put there. All right, this is a very\nwidely used notation.",
    "start": "3060240",
    "end": "3066710"
  },
  {
    "text": "And it's going to let us state\ntwo facts about Gaussians that are really helpful.",
    "start": "3066710",
    "end": "3071809"
  },
  {
    "text": "If you're not familiar with it,\njust take a look at it and see. So the first one is the\nmarginalization identity",
    "start": "3071809",
    "end": "3077869"
  },
  {
    "text": "for Gaussians, x2 Px1 x2, OK?",
    "start": "3077869",
    "end": "3084980"
  },
  {
    "text": "For Gaussians, this\nhas a nice form. This is not true, in general. This is why we love\nGaussians so much, really, is that it has\nthis really nice form.",
    "start": "3084980",
    "end": "3092830"
  },
  {
    "text": "Px1 is going to be\nequal to a normal. It's going to be distributed\nlike a normal mu 1, 2, sigma 1,",
    "start": "3092830",
    "end": "3102829"
  },
  {
    "text": "OK? This is called marginalization.",
    "start": "3102830",
    "end": "3108380"
  },
  {
    "text": "It basically means I can grab\nthe mean that I want to grab and the covariance I want to\ngrab when I marginalize them",
    "start": "3108380",
    "end": "3114000"
  },
  {
    "text": "out. And this is because\nGaussians have this nice property\nabout being independent",
    "start": "3114000",
    "end": "3119099"
  },
  {
    "text": "in these directions. OK. This is a much nastier\nstatement, fact two,",
    "start": "3119099",
    "end": "3125210"
  },
  {
    "text": "which we will use. Px1 conditioned on x2 is also\nGaussian-- pretty remarkable,",
    "start": "3125210",
    "end": "3133150"
  },
  {
    "text": "if I'm going to be honest. Not a lot of distributions\nare closed in this way. What I mean is closed, if I do\nan operation like conditioning,",
    "start": "3133150",
    "end": "3140950"
  },
  {
    "text": "do I get back the\nsame distribution? That's actually pretty unlikely,\nbut Gaussians, it happens.",
    "start": "3140950",
    "end": "3147210"
  },
  {
    "text": "And so again, it makes some\nof our calculations easier. This is marginalization.",
    "start": "3147210",
    "end": "3154900"
  },
  {
    "text": "This is conditioning. OK? And so what are\nthese two values? This is going to be equal to mu",
    "start": "3154900",
    "end": "3165520"
  },
  {
    "text": "And I'll put up notes,\nand you can look and see the notes do this x2 minus mu. This looks super mysterious.",
    "start": "3165520",
    "end": "3170579"
  },
  {
    "text": "It's actually not, but\nit's not probably worth going into too much detail. If you remember your matrix\ninversion lemmas-- if you",
    "start": "3170580",
    "end": "3176450"
  },
  {
    "text": "don't, again, don't worry. These are not super\nconceptual, important details. It's just how this works.",
    "start": "3176450",
    "end": "3182609"
  },
  {
    "text": "You can go through, and I can\ngladly upload a proof for you, if that makes you feel\nmore solidly grounded.",
    "start": "3182610",
    "end": "3189520"
  },
  {
    "text": "If not, you can just use this. OK? And these formulas,\nwhen I say it's not conceptually important, you\nmay think that's a cop out.",
    "start": "3189520",
    "end": "3196760"
  },
  {
    "text": "Maybe it is. But really the\nreason is it doesn't matter to me, because\nthere's only one way these formulas make sense.",
    "start": "3196760",
    "end": "3202048"
  },
  {
    "text": "The types all work out you. You have d1's and d2's,\nand you're multiplying them in the right way. Otherwise, a formula\nis wrong, OK?",
    "start": "3202049",
    "end": "3208100"
  },
  {
    "text": "So this is the matrix\ninversion lemma. If that helps you, great. I'm happy to prove it.",
    "start": "3208100",
    "end": "3214270"
  },
  {
    "text": "The thing that I care\nthat you take away is that we have these formulas,\nand we can use them later. So when we do a conditioning\nstep, we can use them.",
    "start": "3214270",
    "end": "3221730"
  },
  {
    "text": "OK? When we do a marginalization,\nwe can use them. OK? And I realize this is impossible\nto appreciate at this moment",
    "start": "3221730",
    "end": "3230269"
  },
  {
    "text": "probably. But it's relatively\nrare that we can go from a distribution\ncondition on it",
    "start": "3230270",
    "end": "3236940"
  },
  {
    "text": "and get back a\ndistribution, that we have the same what are\ncalled sufficient statistics. But it happens here.",
    "start": "3236940",
    "end": "3242180"
  },
  {
    "text": "Just don't expect that now. Please. So I think through both of\nthese facts, we are assuming,",
    "start": "3242180",
    "end": "3248150"
  },
  {
    "text": "right, the independencies\nbetween not just within different\ndata sample,",
    "start": "3248150",
    "end": "3255380"
  },
  {
    "text": "like the mean data\nsample, but also within the features\nof the data sample. So I wouldn't look at\nit through that lens. These are mathematical facts\nabout Gaussian distributions.",
    "start": "3255380",
    "end": "3261960"
  },
  {
    "text": "These have nothing to\ndo with data, per se. This is just true about\nany kind of wild Gaussian that you would want to meet. And so you would\nuse it that way.",
    "start": "3261960",
    "end": "3269740"
  },
  {
    "text": "We are going to\nshow, in one second, that crazy model\nI just showed you can be written in a nice\nway, using these things.",
    "start": "3269740",
    "end": "3278190"
  },
  {
    "text": "So how do we use it? So remember we have two\nvariables, n0 sigma, right?",
    "start": "3278190",
    "end": "3292578"
  },
  {
    "text": "And this is since epsilon of\nz, the expected value of z",
    "start": "3292579",
    "end": "3298890"
  },
  {
    "text": "is equal to 0, and the\nexpected value of x equals mu. Now, we don't know sigma yet.",
    "start": "3298890",
    "end": "3305400"
  },
  {
    "text": "We just say there's a\ncovariance out there. We're going to derive\nwhat it is in one second. But the point is, is that whole\nfactor model, basically, boils",
    "start": "3305400",
    "end": "3312960"
  },
  {
    "text": "down to some gigantic Gaussian. And that's going to be\nreally helpful to us. The problem is, this form,\nthis is going to be data.",
    "start": "3312960",
    "end": "3321619"
  },
  {
    "text": "This is going to be hidden. And so we're going to have\nto deal with marginalization and conditionaling\nand all the rest of it to be able to recover it.",
    "start": "3321619",
    "end": "3327490"
  },
  {
    "text": "But the point is we can write\nthat model super compactly. Now, I think if I\nwrote that first, probably you wouldn't\nbe super happy.",
    "start": "3327490",
    "end": "3334369"
  },
  {
    "text": "Maybe you'd be really happy. I don't know.",
    "start": "3334369",
    "end": "3339380"
  },
  {
    "text": "Maybe you're happy folks. [INAUDIBLE] Which one?",
    "start": "3339380",
    "end": "3346039"
  },
  {
    "text": "This one [INAUDIBLE]. And again, you can type\ncheck these things. Just make sure it makes sense.",
    "start": "3346039",
    "end": "3352109"
  },
  {
    "text": "It's got to be a\nd by d map or an n by d map for everything\nto make sense and all the types checked\nout, if there are typos,",
    "start": "3352109",
    "end": "3358700"
  },
  {
    "text": "which I'm sure there are. OK. So now our job is to try and\nfigure out, what is sigma?",
    "start": "3358700",
    "end": "3365220"
  },
  {
    "text": "And by the way, this\nis kind of remarkable. We went through this\npretty elaborate discussion",
    "start": "3365220",
    "end": "3371289"
  },
  {
    "text": "about various different\nmodels, how the model worked.",
    "start": "3371289",
    "end": "3377300"
  },
  {
    "text": "And lo and behold, it's\njust some nice Gaussian.",
    "start": "3377300",
    "end": "3382970"
  },
  {
    "text": "So because it's block, 1, with itself.",
    "start": "3382970",
    "end": "3388090"
  },
  {
    "text": "Well, what's the outer\nproduct of z with itself? It's just the identity. Why is it the identity? Because that's the\nway we sampled.",
    "start": "3388090",
    "end": "3393230"
  },
  {
    "text": "Remember we sampled\nin a low space? This is using this fact-- whoops-- is using\nthis fact, right?",
    "start": "3393230",
    "end": "3401240"
  },
  {
    "text": "So expected value\nof zz transpose, that is exactly I. OK.",
    "start": "3401240",
    "end": "3408350"
  },
  {
    "text": "What is sigma 1, 2? Well, it's going to be the\nexpected value of z times x",
    "start": "3408350",
    "end": "3415240"
  },
  {
    "text": "minus mu transpose. What does that actually equal?",
    "start": "3415240",
    "end": "3421890"
  },
  {
    "text": "Well, that's going to be\nthe expected value of z z",
    "start": "3421890",
    "end": "3427558"
  },
  {
    "text": "transpose, right? So x minus mu equals what? It equals lambda z\nplus epsilon, right?",
    "start": "3427559",
    "end": "3435170"
  },
  {
    "text": "That's just the model. I just subtracted off\nthe mu on both sides. So it's going to be z z\ntranspose lambda transpose",
    "start": "3435170",
    "end": "3445710"
  },
  {
    "text": "plus E z epsilon transpose.",
    "start": "3445710",
    "end": "3452420"
  },
  {
    "text": "Now, these two things\nare independent. Whoops.",
    "start": "3452420",
    "end": "3457500"
  },
  {
    "text": "z and epsilon are independent,\nso boom, this goes to 0.",
    "start": "3457500",
    "end": "3463310"
  },
  {
    "text": "This we just concluded. I'm sorry. This we just concluded\nwas the identity.",
    "start": "3463310",
    "end": "3469180"
  },
  {
    "text": "So this equals lambda transpose. And sigma 2, 1-- whoops,\nI'll write in a proper-- in the other color.",
    "start": "3469180",
    "end": "3478410"
  },
  {
    "text": "Sigma 2, 1 equals\nsigma 1, 2 transpose",
    "start": "3478410",
    "end": "3484338"
  },
  {
    "text": "because it's a covariance\nmatrix, positive sym-- it's symmetric. OK?",
    "start": "3484339",
    "end": "3490100"
  },
  {
    "text": "The last one-- or you\ncan just compute it. Take my word for it.",
    "start": "3490100",
    "end": "3495630"
  },
  {
    "text": "But you don't have to\ntake my word for it, is this one, x minus mu\nx minus mu transpose.",
    "start": "3495630",
    "end": "3504349"
  },
  {
    "text": "We just use this formula again. That's going to be the expected\nvalue of lambda z plus epsilon",
    "start": "3504349",
    "end": "3515068"
  },
  {
    "text": "lambda z plus epsilon transpose. We have some\nmultiplications to do. We know that all the\ncross terms with epsilon",
    "start": "3515069",
    "end": "3521970"
  },
  {
    "text": "are going to cancel out. So this is going to look like-- and this will maybe\nlook mysterious.",
    "start": "3521970",
    "end": "3527568"
  },
  {
    "text": "So maybe I'll go a\nlittle bit slower here than I wrote in my notes,\nepsilon epsilon transpose.",
    "start": "3527569",
    "end": "3534010"
  },
  {
    "text": "OK so why? Notice you're going to have\nan epsilon times this term and the transpose. But the expected\nvalue of epsilon is 0.",
    "start": "3534010",
    "end": "3541500"
  },
  {
    "text": "So those cross-term\nis going to cancel. The only one that's\ngoing to remain are this one and this\none, where the epsilons",
    "start": "3541500",
    "end": "3546880"
  },
  {
    "text": "are multiplied by itself. So both cross terms fall away. What does this equal? Well, we just saw this\nis a random variable.",
    "start": "3546880",
    "end": "3554680"
  },
  {
    "text": "This is a deterministic\nparameter of the problem. This is going to be\nlambda lambda transpose, and this is going\nto be plus the phi.",
    "start": "3554680",
    "end": "3561780"
  },
  {
    "text": "Why did that happen? This phi is the model-- sorry for the scrolling,\nso please avert your eyes",
    "start": "3561780",
    "end": "3568920"
  },
  {
    "text": "if it makes you nauseous--\nis this character. Sound good?",
    "start": "3568920",
    "end": "3579410"
  },
  {
    "text": "So let's write the\nwhole model in summary. And again, you can check\nthe types on this thing.",
    "start": "3579410",
    "end": "3585180"
  },
  {
    "text": "Make sure I didn't make\nsome silly mistake. OK?",
    "start": "3585180",
    "end": "3591720"
  },
  {
    "text": "And we're good.",
    "start": "3591720",
    "end": "3597930"
  },
  {
    "text": "So now it's in this\nnice Gaussian form. And oh my gosh, I claim you\nalready know how to solve this.",
    "start": "3597930",
    "end": "3604099"
  },
  {
    "text": "E-step-- what is Qi of z? I won't go through the\nwhole EM algorithm, you remember is Pz of\ni, given xi and theta.",
    "start": "3604099",
    "end": "3613510"
  },
  {
    "text": "What do we use here? We only have two rules. Conditional.",
    "start": "3613510",
    "end": "3619170"
  },
  {
    "text": "Yeah, use the conditional. That's all we've got. Use the conditional time step.",
    "start": "3619170",
    "end": "3627690"
  },
  {
    "text": "Well, it's a normal\ndistribution, where we filled in the x's, and\nwe're marginalizing out after",
    "start": "3627690",
    "end": "3634510"
  },
  {
    "text": "seeing the x's that are in there\nor conditionally, on the z's. We have closed forms. You already know how to do this.",
    "start": "3634510",
    "end": "3645309"
  },
  {
    "text": "OK?",
    "start": "3645309",
    "end": "3650520"
  },
  {
    "text": "So what is the summary here?",
    "start": "3650520",
    "end": "3659309"
  },
  {
    "text": "We saw here, this factor\nanalysis structure that allowed us to have a\nlower dimensional space,",
    "start": "3659309",
    "end": "3664720"
  },
  {
    "text": "have this elaborate\nsampling and moving around, pretty wild set of\ngenerative models,",
    "start": "3664720",
    "end": "3671359"
  },
  {
    "text": "basically boiled down to a\none-liner to shoe and horn into the M algorithm.",
    "start": "3671359",
    "end": "3678299"
  },
  {
    "text": "We had to use some fancy\ntricks with how you deal with normal distributions.",
    "start": "3678299",
    "end": "3683849"
  },
  {
    "text": "If you didn't use\nthose fancy tricks for normal\ndistributions, you would have to do some empirical\nextra work, right? You'd have to understand,\nif I had a distribution that",
    "start": "3683849",
    "end": "3690560"
  },
  {
    "text": "looked like blah, and I wanted\nto marginalize or condition it, what would I do on top? Yeah, please?",
    "start": "3690561",
    "end": "3696010"
  },
  {
    "text": "So I'm not sure I really\nunderstand [INAUDIBLE] forms for the next step.",
    "start": "3696010",
    "end": "3702990"
  },
  {
    "text": "Oh, because once\nwe have this, these are just like-- so think\nabout what happens once we have the estimates for the z.",
    "start": "3702990",
    "end": "3708799"
  },
  {
    "text": "Then it's just conditioning\non the z's and removing them. And we know how to estimate\nthe parameters of--",
    "start": "3708799",
    "end": "3714088"
  },
  {
    "text": "this becomes just a standard. Oop, where's the formula? Sorry for the mad scrolling,\nthe crazy scrolling is here.",
    "start": "3714089",
    "end": "3722830"
  },
  {
    "text": "Once you know z, you\nhave a guess of z, then this is just fitting a\nGaussian with an unknown mu and sigma.",
    "start": "3722830",
    "end": "3729609"
  },
  {
    "text": "And you need to figure\nout what the sigma is that you're fitting. But you know how to do that. So I guess this\nis something we've",
    "start": "3729609",
    "end": "3734940"
  },
  {
    "text": "done maybe k lectures in a row. And I agree with you. It is abstract and weird\nthat that falls out.",
    "start": "3734940",
    "end": "3740910"
  },
  {
    "text": "And I want to say it in that\nabstract and a weird way so that you're not afraid of it. Because if you want to\ngo into these things, you want to use these things,\nit'll look very strange.",
    "start": "3740910",
    "end": "3747740"
  },
  {
    "text": "But once you get it in\nthat form, it's just a plug and chug kind of mentality to\nget all the results from things",
    "start": "3747740",
    "end": "3753000"
  },
  {
    "text": "you already, derivatives you\nalready know how to compute. Cool. Oop, go ahead. Why do [INAUDIBLE]\nsigma [INAUDIBLE]",
    "start": "3753000",
    "end": "3760190"
  },
  {
    "text": "as expectations [INAUDIBLE]? Oh, so that's just the\ndefinition of covariance. So because I know\nthe distributions,",
    "start": "3760190",
    "end": "3766750"
  },
  {
    "text": "I want to compute\nwhat sigma must be. And I'm just trying to tell\nyou, the reason I wrote it that way is to tell you\nthere's nothing else going on.",
    "start": "3766750",
    "end": "3773700"
  },
  {
    "text": "In the model, I'm just\ncomputing the expectations. I know the covariance of\nthe individual objects, and that is telling me that\nI can write it in this form.",
    "start": "3773700",
    "end": "3783819"
  },
  {
    "text": "Cool. All right, let's use and we should be in good shape.",
    "start": "3783819",
    "end": "3790960"
  },
  {
    "text": "All right. PCA-- all right,\nso PCA, I'm going",
    "start": "3790960",
    "end": "3798549"
  },
  {
    "text": "to draw this little diagram\nto explain where we are,",
    "start": "3798550",
    "end": "3806900"
  },
  {
    "text": "principal component analysis. So we looked. I want to fill out this table. You say, why do you want\nto fill out the table?",
    "start": "3806900",
    "end": "3812130"
  },
  {
    "text": "I don't know. It's just how I do it. I like that it fits. Also, you should know it.",
    "start": "3812130",
    "end": "3817380"
  },
  {
    "text": "It's an important algorithm. It'd be weird if\nwe didn't teach it. Although, probably much of\nyou have seen it before.",
    "start": "3817380",
    "end": "3824020"
  },
  {
    "text": "If not, don't worry about it. So one of the things that's\ninside machine learning, which",
    "start": "3824020",
    "end": "3830190"
  },
  {
    "text": "is really interesting\nhistorically-- I won't bore you too much\nwith the various tribes of machine-learning theory,\nand how we got where we got,",
    "start": "3830190",
    "end": "3838900"
  },
  {
    "text": "and who fought with whom\nat which conference, although it's kind of funny\nbecause it's so bizarre.",
    "start": "3838900",
    "end": "3844250"
  },
  {
    "text": "But there are two\nschools of thought of doing machine learning\nand at kind of a broad level.",
    "start": "3844250",
    "end": "3851940"
  },
  {
    "text": "It's kind of the Bayesian\nschool of thought, which is everything\nshould be written down as a probability distribution,\nmaximum likelihood.",
    "start": "3851940",
    "end": "3857559"
  },
  {
    "text": "We write these priors. We use conjugate priors. We crank through the model,\nand we have this forward story.",
    "start": "3857559",
    "end": "3862619"
  },
  {
    "text": "And then there was\nanother camp that's more the frequentist\nstyle world, just from the stat\nside, that was like,",
    "start": "3862619",
    "end": "3867920"
  },
  {
    "text": "we don't want to\nuse probabilities. We just want to have\nthese nice estimates. They also like\nmaximum likelihood. They also like\nBayes' rule, but they",
    "start": "3867920",
    "end": "3873470"
  },
  {
    "text": "differ in some ways of\nhow you approach modeling and what a valid model is, how\nyou know to trust the model.",
    "start": "3873470",
    "end": "3878808"
  },
  {
    "text": "OK? It doesn't matter at all. The point is, almost\neverything in machine learning has these two\ndifferent approaches.",
    "start": "3878809",
    "end": "3884400"
  },
  {
    "text": "And I'll say, at one point in\nmy career, early in my career, I built a system which was very\nmuch in the probabilistic camp",
    "start": "3884400",
    "end": "3891900"
  },
  {
    "text": "and then became\ndisillusioned with it and moved to doing\nsomething else. But there are merits\nto both approaches.",
    "start": "3891900",
    "end": "3897440"
  },
  {
    "text": "PCA is what we're\ngoing to talk about. So that's just\nhistorical context. I just want you to know\nthere are many ways",
    "start": "3897440",
    "end": "3902819"
  },
  {
    "text": "to model a lot of these\ndifferent problems. And we're going to talk\nabout this one here. And I want you to\nexplicitly contrast it",
    "start": "3902819",
    "end": "3909960"
  },
  {
    "text": "with factor analysis, which is\nthe probabilistic version, kind of a PCA. In the same way, k means in\nGMM have a parallel structure.",
    "start": "3909960",
    "end": "3917440"
  },
  {
    "text": "You saw k means. We did GMM. They had a nice kind\nof rhythm of how they were solving the\nunderlying problem,",
    "start": "3917440",
    "end": "3922890"
  },
  {
    "text": "so it was nice to\nput them together. These don't have\nthat nice rhythm, but they have something\nelse that is nice to see.",
    "start": "3922890",
    "end": "3929490"
  },
  {
    "text": "All right, so let's see PCA. Please. [INAUDIBLE]",
    "start": "3929490",
    "end": "3935150"
  },
  {
    "text": "Thank you. Yep. OK, so we're going to\nbe given pairs here.",
    "start": "3935150",
    "end": "3942540"
  },
  {
    "text": "And this is a weird example,\nbut it's something that makes, hopefully, the main point clear.",
    "start": "3942540",
    "end": "3952690"
  },
  {
    "text": "OK? So imagine someone\ngave us a data set,",
    "start": "3952690",
    "end": "3963099"
  },
  {
    "text": "and this data set had\na bunch of cars on it, and the cars are going\nto be rated on highway",
    "start": "3963099",
    "end": "3971450"
  },
  {
    "text": "miles per gallon and city. OK? And these are gas cars.",
    "start": "3971450",
    "end": "3977289"
  },
  {
    "text": "These aren't electric cars. So they're scattered\nall over this. And I actually went\nand got the real data.",
    "start": "3977289",
    "end": "3986570"
  },
  {
    "text": "It's not that\ninteresting, but there's a couple that are substantially\nbetter, substantially worse than the line.",
    "start": "3986570",
    "end": "3991960"
  },
  {
    "text": "OK? Now, imagine these\nclusters up here. Just to give you-- complete the story,\nthese are maybe hybrids.",
    "start": "3991960",
    "end": "3998150"
  },
  {
    "text": "Whoops. These are hybrids.",
    "start": "3998150",
    "end": "4008109"
  },
  {
    "text": "Maybe these are SUVs. They have bad gas mileage\nor trucks, or something.",
    "start": "4008109",
    "end": "4015200"
  },
  {
    "text": "And these are our economy cars. I don't clear. OK? Clear what the\nvisualization is saying?",
    "start": "4015200",
    "end": "4022930"
  },
  {
    "text": "OK. So we ask a question, which\nat first, seems strange,",
    "start": "4022930",
    "end": "4029140"
  },
  {
    "text": "but it's a question we\ncould ask nonetheless. What does it mean for a car to\nhave a good miles per gallon?",
    "start": "4029140",
    "end": "4035240"
  },
  {
    "text": "Now, some cars are going\nto be better in the city. Some cars are going to\nbe better on the highway. We come in a single direction\nor a single kind of a way that",
    "start": "4035240",
    "end": "4042980"
  },
  {
    "text": "says, what is the component\nof principal variation for good miles per gallon? And that's what\nwe're trying to ask.",
    "start": "4042980",
    "end": "4049530"
  },
  {
    "text": "And by the way, PCA\nis typically employed in these settings, when we don't\nknow too much about our data, we want to visualize it in some\nway or get some understanding.",
    "start": "4049530",
    "end": "4058049"
  },
  {
    "text": "Right? Modern methods incorporate\na lot of its ideas, so you don't need to run it,\nusually, as a standalone ahead",
    "start": "4058049",
    "end": "4064109"
  },
  {
    "text": "of time anymore. They worry about these\nfeatures and things, but it focuses on one thing\nI really think is great.",
    "start": "4064109",
    "end": "4069859"
  },
  {
    "text": "So how do we deal with this? All right? So the first thing we do is\nwe take our data and the PCA",
    "start": "4069859",
    "end": "4077788"
  },
  {
    "text": "worldview, and we center it. OK?",
    "start": "4077789",
    "end": "4083230"
  },
  {
    "text": "OK. Now, what does it mean center?",
    "start": "4083230",
    "end": "4090730"
  },
  {
    "text": "We compute mu, which is equal to And then we subtract\nmu from every point",
    "start": "4090730",
    "end": "4101969"
  },
  {
    "text": "here to get the center\nof the data set at 0. It should be\nbalanced in all ways.",
    "start": "4101969",
    "end": "4107990"
  },
  {
    "text": "This is just to make sure that\nthe scale, the raw numbers, don't matter. Because we're going to\ntalk about geometry. And also, we're going to use\nsome ideas from linear algebra.",
    "start": "4107990",
    "end": "4114359"
  },
  {
    "text": "And linear algebra likes things\nthat go through the origin. We like linear maps. Linear maps go\nthrough the origin. So we want to--",
    "start": "4114360",
    "end": "4120298"
  },
  {
    "text": "the origin has a special\nrole in linear algebra. This is still highway. This is city.",
    "start": "4120299",
    "end": "4125630"
  },
  {
    "text": "OK? Now, if I look, and I just\neyeball it, roughly speaking,",
    "start": "4125630",
    "end": "4131568"
  },
  {
    "text": "it feels like the\ncomponent of variation is something like this. Right? And what I mean\nis, if I look along",
    "start": "4131569",
    "end": "4138100"
  },
  {
    "text": "that direction, that tells\nme the more I go along that, the better the\nhighway mileage is, or some variation\nthat's around that.",
    "start": "4138100",
    "end": "4144540"
  },
  {
    "text": "But that's good miles per\ngallon, and below, as you go, those are slightly worse. So along this line is kind\nof the principal direction",
    "start": "4144540",
    "end": "4152310"
  },
  {
    "text": "of variation. Doesn't capture everything. There's this character\nway over here. There's some character\nway over here. There's an automated way of kind\nof denoising that as data sets",
    "start": "4152310",
    "end": "4160210"
  },
  {
    "text": "and other things\nthat I'm looking at. And so what we'll talk\nabout is this direction mu be a unit vector,",
    "start": "4160210",
    "end": "4166508"
  },
  {
    "text": "is a component of\nprincipal variation. This is very intuitive. I'll define it formally\nin a second, mu 1, mu 1.",
    "start": "4166509",
    "end": "4174960"
  },
  {
    "text": "OK? And if I look, I can still\ndescribe all my points-- this is just linear algebra--\nby something that's orthogonal, and I'll call it u2.",
    "start": "4174960",
    "end": "4181980"
  },
  {
    "text": "OK? Now, I'll construct u1 and\nu2, because I only care",
    "start": "4181980",
    "end": "4187580"
  },
  {
    "text": "about their directions,\nto be unit vectors. OK? And so you can think\nabout u1 as how",
    "start": "4187580",
    "end": "4193330"
  },
  {
    "text": "good is the mile per gallon? And u2 is explaining its\nfirst variance, right?",
    "start": "4193330",
    "end": "4198870"
  },
  {
    "text": "If you thought about\nit as probabilistic, it would be the first direction\nthat they vary, a core varies.",
    "start": "4198870",
    "end": "4203908"
  },
  {
    "text": "And then, what's that\nsecond order effect? You can imagine doing\nthis in higher dimensions. All right.",
    "start": "4203909",
    "end": "4210520"
  },
  {
    "text": "Formally, we can write xi\nequals alpha i 1 times mu 1",
    "start": "4210520",
    "end": "4222320"
  },
  {
    "text": "plus alpha i 2 mu 2. And these are scalars.",
    "start": "4222320",
    "end": "4228448"
  },
  {
    "text": "This is just linear algebra. Please go ahead. You had a question. So [INAUDIBLE] principal?",
    "start": "4228449",
    "end": "4234800"
  },
  {
    "text": "Yeah, we're just\neyeballing it for now. I'm saying that it\nlooks roughly correct.",
    "start": "4234800",
    "end": "4239948"
  },
  {
    "text": "We're going to\nfind, how do we find that line in just two minutes. So how can we know\nthat [INAUDIBLE]??",
    "start": "4239949",
    "end": "4247630"
  },
  {
    "text": "So once we write the\ndefinition, let's come back. And then hopefully, your\nintuition or my intuition will be a little\nbit closer, right?",
    "start": "4247630",
    "end": "4253658"
  },
  {
    "text": "But yeah, that's\na great question. You're thinking\nexactly the right way. Why is that the right line? Why is this maniac\ndrawing that line?",
    "start": "4253659",
    "end": "4259489"
  },
  {
    "text": "Right? Now, the thing is, is we\nmay only decide, by the way, to use this as dimensionality.",
    "start": "4259489",
    "end": "4264560"
  },
  {
    "text": "We may only keep this component. OK?",
    "start": "4264560",
    "end": "4272030"
  },
  {
    "text": "Because it explains\nmore variation. And that's going to be how\nwe define it, the variation.",
    "start": "4272030",
    "end": "4279480"
  },
  {
    "text": "Oop. OK. So what we want to do here,\njust so you're clear-- so here, I drew it from 2 to 1,\njust as I was drawing before.",
    "start": "4279480",
    "end": "4286840"
  },
  {
    "text": "But think about if I had\nthousands of dimensions. You can't possibly visualize\nthousands of dimensions. It's really tough.",
    "start": "4286840",
    "end": "4294580"
  },
  {
    "text": "And I want to get it down\nto, say, 2 or 3 or 5 or 10. Right?",
    "start": "4294580",
    "end": "4300370"
  },
  {
    "text": "I want to find those\nprincipal variances, OK? And so PCA also functions as\na dimensionality reduction",
    "start": "4300370",
    "end": "4305429"
  },
  {
    "text": "method. So here, we show just\na two-dimensional plot. But imagine I gave you the\ncars have thousands of numbers,",
    "start": "4305429",
    "end": "4311860"
  },
  {
    "text": "and you want to get a very\nsuccinct description of them as just five numbers. Or I gave you 10,000 sensors,\nas we were talking about before,",
    "start": "4311860",
    "end": "4318350"
  },
  {
    "text": "and you wanted to figure out,\ngive me three numbers which really describe every\ndifferent individual sensor and its variation\nfrom that number.",
    "start": "4318350",
    "end": "4325110"
  },
  {
    "text": "It's very similar to\nfactor analysis, right? It's looking for that\nlow-dimensional subspace that you want. In fact, it is formally a\nlow-dimensional subspace.",
    "start": "4325110",
    "end": "4331510"
  },
  {
    "text": "It is a dimension\nof dimension 2 or 3 or k underneath the covers. And we'll come to\nthat in one second.",
    "start": "4331510",
    "end": "4337570"
  },
  {
    "text": "OK? So at this point,\nthis is intuition. You should have more\nquestions than answers. Let me state the\nalgorithm that we're",
    "start": "4337570",
    "end": "4343448"
  },
  {
    "text": "going to do, the preprocessing\nthat we're going to do. And then you'll see why some\nof these variation things are. Please go ahead.",
    "start": "4343449",
    "end": "4348910"
  },
  {
    "text": "Well, I think what\nyou are ultimately proposing is that\nyes, xi has the number",
    "start": "4348910",
    "end": "4358130"
  },
  {
    "text": "of principal components. But certainly, most\nof them are noise except for the initial two.",
    "start": "4358130",
    "end": "4364330"
  },
  {
    "text": "That's-- what are you saying? Great question. So imagine I gave\nyou these x1 to xn",
    "start": "4364330",
    "end": "4369440"
  },
  {
    "text": "that live in some\nhigh-dimensional space d. Then what I'm\ngoing to try and do is find a smaller\nnumber of directions.",
    "start": "4369440",
    "end": "4376250"
  },
  {
    "text": "But those directions\nare not necessarily aligned with the axis,\nwhich means they're not component 5, 7, or nine.",
    "start": "4376250",
    "end": "4381908"
  },
  {
    "text": "There's some mixture\nof all of them. And so u is not aligned\nwith an axis, right? It's some mixture of the\nhighway direction and the city",
    "start": "4381909",
    "end": "4389119"
  },
  {
    "text": "direction. And I'm going to\nfind that direction, and then I'm going to say that's\nthe direction that the data set varies most in,\nthere's the most spread in.",
    "start": "4389120",
    "end": "4395310"
  },
  {
    "text": "So I want to keep where\nyou are on that line. I'll project you\non that line, then maybe the second, the third.",
    "start": "4395310",
    "end": "4401070"
  },
  {
    "text": "And that's going to be a three\nnumbers that best capture you in the data set, if you're\nan individual data point.",
    "start": "4401070",
    "end": "4407550"
  },
  {
    "text": "And [INAUDIBLE] noise? Not noise, but I'm just\nnot modeling them, yeah.",
    "start": "4407550",
    "end": "4412888"
  },
  {
    "text": "Please. I guess what I've\nbeen [INAUDIBLE] quadratic relationship\non [INAUDIBLE] instead",
    "start": "4412889",
    "end": "4420920"
  },
  {
    "text": "of [INAUDIBLE]\nrelationship shown there? Yeah. How does it capture\nthe direction?",
    "start": "4420920",
    "end": "4426888"
  },
  {
    "text": "Wonderful question--\nso we'll come back to that in a little bit. Effectively, what\nwe're going to do is we're going to capture\nsome variation that's there.",
    "start": "4426889",
    "end": "4433010"
  },
  {
    "text": "So there may actually be some of\nthese hidden correlations that are underneath the covers. And we won't model them\nwell, or we may only",
    "start": "4433010",
    "end": "4438889"
  },
  {
    "text": "model them well in\ndifferent pieces of data. There are methods, if I know\nthat there's a nonlinear relationship between a lot of\nthe different elements of data,",
    "start": "4438889",
    "end": "4446948"
  },
  {
    "text": "that I can capture. Here, we're assuming there\nexists a linear subspace. And that's a fairly\nrobust assumption.",
    "start": "4446949",
    "end": "4453080"
  },
  {
    "text": "So people do use more\nadvanced techniques, like t-SNE and\nUMAP, which do have, actually, the\nassumption that there's",
    "start": "4453080",
    "end": "4458389"
  },
  {
    "text": "some very compressible\nnonlinear map that explains what's going on. But PCA is kind of the first\nand the cleanest to describe.",
    "start": "4458389",
    "end": "4466980"
  },
  {
    "text": "Awesome question. OK, so we center the data.",
    "start": "4466980",
    "end": "4472320"
  },
  {
    "text": "By the way, just I'm going\nto review this in one second. I'm just trying to write\nso we can get through this. Mu, we know what it is.",
    "start": "4472320",
    "end": "4483640"
  },
  {
    "text": "We may need to re-scale\nthe components. Why is this? Well, what if one component\nwas miles per gallon,",
    "start": "4483640",
    "end": "4492710"
  },
  {
    "text": "and another was feet per gallon? They're just different\nby a factor of 5,000. There would be a lot of\nvariation in the feet",
    "start": "4492710",
    "end": "4498170"
  },
  {
    "text": "per gallon, right, not\nthe miles per gallon. The numbers would be different. So not only do we want to\ncenter them and get them",
    "start": "4498170",
    "end": "4503870"
  },
  {
    "text": "both centered around 0. We want to re-scale\nthem so that they have exactly the same width.",
    "start": "4503870",
    "end": "4510870"
  },
  {
    "text": "And what we'll do is we'll\njust divide by the variance, by the sample variance\nin that direction,",
    "start": "4510870",
    "end": "4518270"
  },
  {
    "text": "so that all the numbers are\nroughly normally distributed, you should think. OK? So we're going to assume that\nthe data are preprocessed.",
    "start": "4518270",
    "end": "4524760"
  },
  {
    "text": "If you actually do\nthis method, you should do the preprocessing. So let's see PCA\nas an optimization",
    "start": "4524760",
    "end": "4529989"
  },
  {
    "text": "problem, All right.",
    "start": "4529990",
    "end": "4535170"
  },
  {
    "text": "Oops, monkey business--",
    "start": "4535170",
    "end": "4540350"
  },
  {
    "text": "I'll live with it.",
    "start": "4540350",
    "end": "4547260"
  },
  {
    "text": "OK. All right, so here's what\nwe're trying to solve.",
    "start": "4547260",
    "end": "4554780"
  },
  {
    "text": "And this will get back to\nsome of the questions that were asked about,\nhey, why do we think",
    "start": "4554780",
    "end": "4560110"
  },
  {
    "text": "this is a principal direction? We need a little bit\nof math before we",
    "start": "4560110",
    "end": "4566570"
  },
  {
    "text": "can make that precise x, OK?",
    "start": "4566570",
    "end": "4571590"
  },
  {
    "text": "So what we want to solve-- what we have is we\nhave some direction u1. We have some other direction\nu2, which is orthogonal.",
    "start": "4571590",
    "end": "4577489"
  },
  {
    "text": "This is mu 1. We have some direction mu We have these are unit vectors.",
    "start": "4577489",
    "end": "4586760"
  },
  {
    "text": "Mu i equals 1, so unit vectors. And they're orthogonal.",
    "start": "4586760",
    "end": "4593739"
  },
  {
    "text": "Mu i dot mu j equals 0--\nor sorry, equals delta ij--",
    "start": "4593739",
    "end": "4601719"
  },
  {
    "text": "sorry-- orthogonal. OK? So how do you find the closest\npoint on this line to x?",
    "start": "4601719",
    "end": "4610050"
  },
  {
    "text": "Well, this line,\njust to be clear, can be parameterized as t times\nu1 for t element of R. Right?",
    "start": "4610050",
    "end": "4618050"
  },
  {
    "text": "It's just a line,\none-dimensional line. And so to find alpha, how do we\nfind alpha above closest point?",
    "start": "4618050",
    "end": "4632750"
  },
  {
    "text": "That's the following expression. Alpha 1 equals the augment over\nalpha of x minus alpha mu 1",
    "start": "4632750",
    "end": "4642280"
  },
  {
    "text": "norm square. OK? All right, so hopefully,\nthis makes sense. If I want to find\nthe closest point,",
    "start": "4642280",
    "end": "4648469"
  },
  {
    "text": "intuitively it should be on the\nline that's orthogonal here. It's not a very great drawing,\nbut that's where it should be. It should be projected\nonto this line.",
    "start": "4648469",
    "end": "4654750"
  },
  {
    "text": "And we can minimize\nover the whole line by minimizing over alpha. We do the standard thing here.",
    "start": "4654750",
    "end": "4661520"
  },
  {
    "text": "We compute the derivatives. We multiply it out. All right, so this is equivalent\nto, when we multiply it out,",
    "start": "4661520",
    "end": "4667730"
  },
  {
    "text": "argmin x square plus\nalpha square times",
    "start": "4667730",
    "end": "4673400"
  },
  {
    "text": "mu 1 square minus This is argmin over alpha.",
    "start": "4673400",
    "end": "4682590"
  },
  {
    "text": "This doesn't change. This is a function of the input. This is 1.",
    "start": "4682590",
    "end": "4690110"
  },
  {
    "text": "So when we go to compute the\nderivative, what do we get? We get 2 alpha minus",
    "start": "4690110",
    "end": "4705750"
  },
  {
    "text": "Or said more simply,\nalpha equals 0, right? This is the gradient\nwith respect to alpha.",
    "start": "4705750",
    "end": "4711340"
  },
  {
    "text": "So alpha equals mu 1 dot x. This is saying the dot product\nin this direction tells us",
    "start": "4711340",
    "end": "4718760"
  },
  {
    "text": "exactly the closest point. This is just differentiating\nthis expression with respect to alpha.",
    "start": "4718760",
    "end": "4726510"
  },
  {
    "text": "OK? All right, now, you\ncan generalize this",
    "start": "4726510",
    "end": "4734030"
  },
  {
    "text": "to any set of directions,\northogonal directions. So if I have mu 1 to\nmu k element of Rd",
    "start": "4734030",
    "end": "4743889"
  },
  {
    "text": "and x element of Rd, how\ndo I find the closest point to the subspace span by mu i?",
    "start": "4743890",
    "end": "4749350"
  },
  {
    "text": "Here, we're going\nto use the fact that mu ij equals delta ij. Well, it's going to\nbe the following--",
    "start": "4749350",
    "end": "4755100"
  },
  {
    "text": "argmin alpha 1 alpha k x\nminus sum k equals 1 to n--",
    "start": "4755100",
    "end": "4764750"
  },
  {
    "text": "or sorry, j equals",
    "start": "4764750",
    "end": "4776370"
  },
  {
    "text": "Now expand that out again,\nexactly as we did before, and you'll see that alpha\ni is equal to mu i--",
    "start": "4776370",
    "end": "4785139"
  },
  {
    "text": "or ui dotted into x. This is the inner\nproduct, if you like.",
    "start": "4785139",
    "end": "4791230"
  },
  {
    "text": "OK? We call this quantity\nx minus sum alpha j uj.",
    "start": "4791230",
    "end": "4798290"
  },
  {
    "text": "Sorry, this is uj,\nuj square to k.",
    "start": "4798290",
    "end": "4803800"
  },
  {
    "text": "OK, the residual. OK, so we have\neverything we need, rushing a little bit here.",
    "start": "4803800",
    "end": "4809580"
  },
  {
    "text": "But so the point I want to make\nis, if you have a subspace, and you want to find the\nclosest point in that subspace",
    "start": "4809580",
    "end": "4814849"
  },
  {
    "text": "to x, that's the same\nas minimizing over all the possible\ndirections in the subspace",
    "start": "4814850",
    "end": "4820698"
  },
  {
    "text": "this distance squared. Distance squared makes\nthe computation easier. Then alpha i, if you solve\nit, is going to break apart.",
    "start": "4820699",
    "end": "4826889"
  },
  {
    "text": "And the reason it's\ngoing to break apart is, when you multiply\nit apart like this, you're going to get\nproducts of mu i and mu j.",
    "start": "4826889",
    "end": "4832329"
  },
  {
    "text": "They will all go away. So all you'll be left with\nis a bunch of the alpha i's",
    "start": "4832330",
    "end": "4838909"
  },
  {
    "text": "and alpha j's that are\nmultiplied in exactly the same way as the 1dks. And you should check that. And if you're\nrusty on this, this",
    "start": "4838909",
    "end": "4845930"
  },
  {
    "text": "is from your linear\nalgebra class. And I'm super happy to\nwrite it out in more detail, if that confuses you.",
    "start": "4845930",
    "end": "4851240"
  },
  {
    "text": "But for now, hopefully, we\ncan just move through this. OK. This thing here is the residual.",
    "start": "4851240",
    "end": "4856960"
  },
  {
    "text": "That says, how close am I? What's my distance\nfrom x to the subspace? And that's going to feature\nprominently in what we do next.",
    "start": "4856960",
    "end": "4865810"
  },
  {
    "text": "Please. So this subspace is\n[INAUDIBLE] approximates",
    "start": "4865810",
    "end": "4871880"
  },
  {
    "text": "this whole [INAUDIBLE]? So what we're\ngoing to try and do is we're going to try and find\na low-dimensional subspace.",
    "start": "4871880",
    "end": "4877300"
  },
  {
    "text": "And I'll describe how\nwe're going to do it next. And PCA, when we're\ntalking about, what's the principal\ncomponent of variation?",
    "start": "4877300",
    "end": "4883110"
  },
  {
    "text": "We're going to search\nacross all the directions when we're looking for\nthe principal component. And we're going to say which one\nof them has this-- for example,",
    "start": "4883110",
    "end": "4889369"
  },
  {
    "text": "there are two ways we can find\nPCA-- but has the smallest residual or will be equivalent,\nmaximizes the amount of alpha,",
    "start": "4889370",
    "end": "4898030"
  },
  {
    "text": "the amount we projected\nonto the subset. OK? So there are two\nways you can do this. So we can find PCA by one,\nmaximizing the projected",
    "start": "4898030",
    "end": "4916199"
  },
  {
    "text": "amount. That means trying to make alpha\nas large as possible, right? So if you give me a subspace,\nwhich is a section of vectors,",
    "start": "4916199",
    "end": "4926480"
  },
  {
    "text": "I can compute the alphas\nby solving this problem. I want to compute something\nso that a direction-- so",
    "start": "4926480",
    "end": "4932670"
  },
  {
    "text": "that for almost all\nthe data points, my alphas are as\nlarge as possible. So getting back to the\nearlier question about,",
    "start": "4932670",
    "end": "4938240"
  },
  {
    "text": "why did I say that direction\nwas the principal component of variation? It's because if I put\nit in that direction, I was explaining the most,\nand the residuals, the errors,",
    "start": "4938240",
    "end": "4946638"
  },
  {
    "text": "were small. Duly, these residuals for every\npoint are as small as possible. And we're going to minimize\nthose in one second.",
    "start": "4946639",
    "end": "4953579"
  },
  {
    "text": "Yeah, go ahead. Couldn't we just\nnormalize the [INAUDIBLE]",
    "start": "4953580",
    "end": "4958790"
  },
  {
    "text": "the data as the\npreprocessing [? stuff? ?] We did. So wouldn't that make the\nresidual on, essentially, the same [INAUDIBLE]? Awesome question.",
    "start": "4958790",
    "end": "4964510"
  },
  {
    "text": "No, not necessarily, right? Because, for example, think\nabout even if we normalized it.",
    "start": "4964510",
    "end": "4969830"
  },
  {
    "text": "There may be one direction\nthat explains, for example, the temperature,\nhow far it is away from the heat source, back to\nmy 1,000 temperature sensor",
    "start": "4969830",
    "end": "4976680"
  },
  {
    "text": "example. And there's one direction\nthat explains that. But there still could be\nvariations in other directions that are not captured\nby any of the process.",
    "start": "4976680",
    "end": "4983590"
  },
  {
    "text": "And so they would be\northogonal to that. So it's not necessary that\neven though all the directions,",
    "start": "4983590",
    "end": "4991340"
  },
  {
    "text": "on average, are normalized,\nthat for any particular point, if there's a correlation,\nwould be the same.",
    "start": "4991340",
    "end": "4996990"
  },
  {
    "text": "OK, so we actually normalize all\nthe axes with the same scale? So all the axes are\nput into the same scale",
    "start": "4996990",
    "end": "5004770"
  },
  {
    "text": "by taking each\nindividual component and computing its\nsample variance and multiplying by\na diagonal matrix.",
    "start": "5004770",
    "end": "5012710"
  },
  {
    "text": "Please. When you're taking\nalpha, which-- how do we take the\n[INAUDIBLE] just",
    "start": "5012710",
    "end": "5021429"
  },
  {
    "text": "is between the line\nand [INAUDIBLE]?? Or is it [INAUDIBLE]?",
    "start": "5021429",
    "end": "5026541"
  },
  {
    "text": "Great question. So here, alpha is basically t. So it tells you how far along\nthe line you're going to go. So alpha is constrained\nto live on the line.",
    "start": "5026541",
    "end": "5032530"
  },
  {
    "text": "So when we minimize\nover alpha, we're minimizing this\ncoordinate along the line. And that's well-defined, right?",
    "start": "5032530",
    "end": "5038659"
  },
  {
    "text": "There's a closest\npoint here, right? We know that intuitively\nfrom undergrad,",
    "start": "5038659",
    "end": "5044260"
  },
  {
    "text": "whatever geometry\nwe took, right, that there's a closest point. But this proves it, right?",
    "start": "5044260",
    "end": "5050300"
  },
  {
    "text": "Awesome. Cool. Please. [INAUDIBLE]? Yeah, so it's either you want\nto capture as much of your data",
    "start": "5050300",
    "end": "5058610"
  },
  {
    "text": "as possible. And in the linear\ncase, it's either you want to capture\nas much as possible or minimize the residual or\nturn out to be equivalent.",
    "start": "5058610",
    "end": "5066300"
  },
  {
    "text": "Because here, if you look at it,\nbecause these are unit vectors, making alpha j's bigger--",
    "start": "5066300",
    "end": "5072900"
  },
  {
    "text": "if I had two sets of u's-- [INAUDIBLE] --and I'm comparing them, if the\nalphas are bigger for this one,",
    "start": "5072900",
    "end": "5079280"
  },
  {
    "text": "on average, than\nfor this one, then that says that they\ncapture more of my data because they're a projection.",
    "start": "5079280",
    "end": "5084480"
  },
  {
    "text": "The alphas are defined\nby the u's. Yeah. All right.",
    "start": "5084480",
    "end": "5090880"
  },
  {
    "text": "So let's find the principal\ncomponent of direction, right? So what this boils down to-- I'll just do one of the\nmethods, the maximization one.",
    "start": "5090880",
    "end": "5098860"
  },
  {
    "text": "You have to do the other one\nin your homework, by the way, so I'm-- one reason to pay\nattention, is this.",
    "start": "5098860",
    "end": "5106830"
  },
  {
    "text": "It says, look over\nall the directions--",
    "start": "5106830",
    "end": "5112030"
  },
  {
    "text": "sorry, u. OK? So it says, look over all the\ndirections that are out there,",
    "start": "5112030",
    "end": "5124500"
  },
  {
    "text": "dot them into the data\nset-- this is alpha; this is the alpha we\nwere just talking about-- and try and maximize the one\nthat captures most of it.",
    "start": "5124500",
    "end": "5131670"
  },
  {
    "text": "So if you imagine me sweeping\nthat line across the earlier data set, where we're\nlooking here, here,",
    "start": "5131670",
    "end": "5139300"
  },
  {
    "text": "and I'm sweeping that\nline here, here, here, all from the origin-- sorry-- go ahead, here, here, clearly\nthis captures much more.",
    "start": "5139300",
    "end": "5148760"
  },
  {
    "text": "The alphas are much\nlarger than they are here. Just take the dot products.",
    "start": "5148760",
    "end": "5153889"
  },
  {
    "text": "Right? Because it's just projecting\nonto a relatively narrow band. It doesn't capture\nas much spread.",
    "start": "5153889",
    "end": "5159170"
  },
  {
    "text": "I want more spread. It's quadratically more spread.",
    "start": "5159170",
    "end": "5164719"
  },
  {
    "text": "OK. So how do we solve\nthis question here?",
    "start": "5164719",
    "end": "5170070"
  },
  {
    "text": "Well, we need a\ncouple of facts, OK?",
    "start": "5170070",
    "end": "5175888"
  },
  {
    "text": "Maybe we will pick this\nup in a little bit. Yeah, I think that's\nprobably best.",
    "start": "5175889",
    "end": "5181909"
  },
  {
    "text": "So ask me any questions about,\nin particular, this spot. We'll come back about how to\nsolve this kind of equation in a second.",
    "start": "5181910",
    "end": "5188130"
  },
  {
    "text": "Any other questions\nabout this piece? Yeah? Will you go back\nto the first line?",
    "start": "5188130",
    "end": "5197100"
  },
  {
    "text": "So essentially, this\npoint [INAUDIBLE].. So what you describe,\nmaximizing [INAUDIBLE] subspace,",
    "start": "5197100",
    "end": "5203489"
  },
  {
    "text": "you're just trying to ensure\nthat whatever [INAUDIBLE].. Yeah. So maximum [INAUDIBLE]\ndata points?",
    "start": "5203489",
    "end": "5209080"
  },
  {
    "text": "[INAUDIBLE]-- Exactly right. [INAUDIBLE]? They capture the\nmost of the data. So think about when\nwe were going back to our earlier example.",
    "start": "5209080",
    "end": "5215080"
  },
  {
    "text": "This is important. We were going to have u1 and u2. That was our basis, right? That's formally\nwhat it's called.",
    "start": "5215080",
    "end": "5220900"
  },
  {
    "text": "We want to capture. So it turns out that the\nsquares of alpha 1 and alpha 2",
    "start": "5220900",
    "end": "5226780"
  },
  {
    "text": "sum to the square\nof xi's components. Their norms are the same, OK?",
    "start": "5226780",
    "end": "5232739"
  },
  {
    "text": "That's just a mathematical fact. Because this is\nwhat our basis is. So the point is, is we want\nto capture-- of these two,",
    "start": "5232739",
    "end": "5240320"
  },
  {
    "text": "we're going to\nthrow away alpha 2, if we only are allowed\nto compute one component. So we want the u that we\nselect to be the one that",
    "start": "5240320",
    "end": "5247239"
  },
  {
    "text": "for most of the data, captures\nsome relevant information, right? If we found something\nlike this direction",
    "start": "5247239",
    "end": "5252929"
  },
  {
    "text": "is mostly orthogonal to\nwhere-- the line that the data lives on, so almost\neverybody is going to have a projection\nthat's really small.",
    "start": "5252929",
    "end": "5258780"
  },
  {
    "text": "The data, when I\nproject onto the line, this is going to be\nclustered really, really tightly together. Whereas, if I project\nonto this line,",
    "start": "5258780",
    "end": "5264920"
  },
  {
    "text": "it's still going to be\nnice and spread out. And this mathematically\njust says, it corresponds to either making\nsure that the alphas overall",
    "start": "5264920",
    "end": "5272510"
  },
  {
    "text": "are very close to the x's, so\nthey're larger, on average, or that the amount that I\nlose-- this is the residual,",
    "start": "5272510",
    "end": "5279219"
  },
  {
    "text": "is the amount that I lose\nfrom the other directions, is equivalently small. And for PCA and Euclidean\nspace, this is the case--",
    "start": "5279219",
    "end": "5286989"
  },
  {
    "text": "not relevant for this class. But there are other\nkinds of geometries where that's not true. This happens to be true\nfor Euclidean geometry.",
    "start": "5286989",
    "end": "5294630"
  },
  {
    "text": "If you decide to take\nnon-Euclidean geometry, it's a great course. What is this\nsuperscript [INAUDIBLE]??",
    "start": "5294630",
    "end": "5302039"
  },
  {
    "text": "Oh, xi. Let me just write it again. Nice catch. I's don't show up very\nwell with this pen",
    "start": "5302040",
    "end": "5308909"
  },
  {
    "text": "in my handwriting, which is\ncriminal, since we use them so frequently.",
    "start": "5308909",
    "end": "5314250"
  },
  {
    "text": "Just assume it's an i. But that's all that\ngoes on, right? So think about what this\nequation is saying, what this optimization is saying.",
    "start": "5314250",
    "end": "5319590"
  },
  {
    "text": "It says, pick over all\nthe u's that are there. We're normalizing. We'll come back to\nwhy we normalize.",
    "start": "5319590",
    "end": "5325250"
  },
  {
    "text": "It doesn't really\nmatter too much. But what we want to do is\nexplain as much of the data as we can.",
    "start": "5325250",
    "end": "5330969"
  },
  {
    "text": "OK? Awesome. So next time, we will cover\na little bit of eigenvalues",
    "start": "5330969",
    "end": "5337800"
  },
  {
    "text": "and how we solve this. This is an eigenvalue problem. And we'll cover the\nCocktail Problem as well.",
    "start": "5337800",
    "end": "5343510"
  },
  {
    "text": "And you saw a little bit of PCA. Thanks so much for your\ntime and attention. Talk soon.",
    "start": "5343510",
    "end": "5347360"
  }
]