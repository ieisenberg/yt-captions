[
  {
    "start": "0",
    "end": "2500"
  },
  {
    "text": "And we just said this, that\none drawback of a ridge",
    "start": "2500",
    "end": "5470"
  },
  {
    "text": "regression, it doesn't actually\nselect variables and set",
    "start": "5470",
    "end": "8380"
  },
  {
    "text": "things to zero when, as\nyou say in situations",
    "start": "8380",
    "end": "10390"
  },
  {
    "text": "like that previous picture,\nwhere things are small.",
    "start": "10390",
    "end": "12603"
  },
  {
    "text": "It'd be nice if they could\njust say, these are zero,",
    "start": "12603",
    "end": "14769"
  },
  {
    "text": "we can forget about them.",
    "start": "14770",
    "end": "16120"
  },
  {
    "text": "So the lasso is a\nmore recent technique",
    "start": "16120",
    "end": "19150"
  },
  {
    "text": "for shrinking coefficients\nin regression.",
    "start": "19150",
    "end": "22456"
  },
  {
    "text": "It looks very much like ridge\nregression, but with one change.",
    "start": "22457",
    "end": "25040"
  },
  {
    "text": "So here's the lasso\ncriterion, again, we",
    "start": "25040",
    "end": "27370"
  },
  {
    "text": "have the RSS as before.",
    "start": "27370",
    "end": "30700"
  },
  {
    "text": "Now we have a penalty,\nwhereas before the penalty was",
    "start": "30700",
    "end": "33032"
  },
  {
    "text": "the sum of the squares\nof the coefficients, now",
    "start": "33032",
    "end": "34990"
  },
  {
    "text": "it's the sum of the absolute\nvalues of the coefficients.",
    "start": "34990",
    "end": "37323"
  },
  {
    "text": "So it's a shrinkage towards\nzero using an absolute value",
    "start": "37323",
    "end": "40390"
  },
  {
    "text": "rather than a sum of squares.",
    "start": "40390",
    "end": "42130"
  },
  {
    "text": "And this is called an L1 penalty\nby analogy to the L2 penalty.",
    "start": "42130",
    "end": "47080"
  },
  {
    "text": "The L1 penalty is just the\nsum of the absolute values,",
    "start": "47080",
    "end": "50950"
  },
  {
    "text": "it's a norm, but it's called the\nL1 norm rather than the L2 norm.",
    "start": "50950",
    "end": "56330"
  },
  {
    "text": "So what's the effect\nof changing this",
    "start": "56330",
    "end": "58790"
  },
  {
    "text": "from a square to\nan absolute value.",
    "start": "58790",
    "end": "60460"
  },
  {
    "text": "It's actually a small change,\nbut it's quite important.",
    "start": "60460",
    "end": "64819"
  },
  {
    "text": "It turns out that the\nlasso, like the ridge,",
    "start": "64819",
    "end": "67850"
  },
  {
    "text": "shrinks towards zero, but it has\nthe effect of actually setting",
    "start": "67850",
    "end": "72049"
  },
  {
    "text": "the coefficients of\nvariables exactly equal to 0",
    "start": "72050",
    "end": "75650"
  },
  {
    "text": "when lambda is large enough.",
    "start": "75650",
    "end": "77450"
  },
  {
    "text": "So it's neat, it\nshrinks, but also it",
    "start": "77450",
    "end": "80240"
  },
  {
    "text": "does subset selection\nin a similar way",
    "start": "80240",
    "end": "82280"
  },
  {
    "text": "to best subset selection.",
    "start": "82280",
    "end": "84479"
  },
  {
    "text": "So it'll set\ncoefficients to 0 exactly",
    "start": "84480",
    "end": "86450"
  },
  {
    "text": "if that feature is not important\nand lambda is large enough.",
    "start": "86450",
    "end": "90829"
  },
  {
    "text": "There's a term for this,\nit's called sparsity.",
    "start": "90830",
    "end": "93770"
  },
  {
    "text": "So the lasso is what's\ncalled sparse models, models",
    "start": "93770",
    "end": "97280"
  },
  {
    "text": "which only involve a\nsubset of the variables.",
    "start": "97280",
    "end": "100770"
  },
  {
    "text": "And again, it's a function of\nthis tuning parameter, lambda,",
    "start": "100770",
    "end": "104579"
  },
  {
    "text": "which as in ridge regression,\nwe have to choose somehow,",
    "start": "104580",
    "end": "107340"
  },
  {
    "text": "and we'll do so by\ncross-validation.",
    "start": "107340",
    "end": "111060"
  },
  {
    "text": "The lasso seems like a\nreally good idea, so clever,",
    "start": "111060",
    "end": "113740"
  },
  {
    "text": "I wonder who came\nup with it, Rob?",
    "start": "113740",
    "end": "115290"
  },
  {
    "text": "Thanks, So Danielle is\ntrying to embarrass me.",
    "start": "115290",
    "end": "118600"
  },
  {
    "text": "So this was actually\na paper that I",
    "start": "118600",
    "end": "120570"
  },
  {
    "text": "wrote in 1996, and at the\ntime actually it was published",
    "start": "120570",
    "end": "126633"
  },
  {
    "text": "and didn't get a\nlot of attention.",
    "start": "126633",
    "end": "128050"
  },
  {
    "text": "But in the last\nabout 10 years or so,",
    "start": "128050",
    "end": "130826"
  },
  {
    "text": "it's become a very hot topic,\nboth in statistics, and computer",
    "start": "130827",
    "end": "133409"
  },
  {
    "text": "science, and other areas.",
    "start": "133410",
    "end": "135050"
  },
  {
    "text": "And there's been a lot\nof work in sparsity",
    "start": "135050",
    "end": "136800"
  },
  {
    "text": "in general, not just in\nregression, but the use of L1",
    "start": "136800",
    "end": "139170"
  },
  {
    "text": "penalties in a lot\nof different areas.",
    "start": "139170",
    "end": "140800"
  },
  {
    "text": "I think one reason for its\npopularity now is computation.",
    "start": "140800",
    "end": "144180"
  },
  {
    "text": "This computation\nis actually what's",
    "start": "144180",
    "end": "145829"
  },
  {
    "text": "called a convex optimization,\nso that's good news.",
    "start": "145830",
    "end": "149490"
  },
  {
    "text": "And there's a lot of work\nin convex optimization,",
    "start": "149490",
    "end": "151680"
  },
  {
    "text": "especially in the\nlast 10 or so years.",
    "start": "151680",
    "end": "153459"
  },
  {
    "text": "And along with the progress\nin convex optimization",
    "start": "153460",
    "end": "157140"
  },
  {
    "text": "and fast computers, people can\nsolve this problem now, a lasso,",
    "start": "157140",
    "end": "163650"
  },
  {
    "text": "for very large\nvalues of p and n.",
    "start": "163650",
    "end": "166034"
  },
  {
    "text": "And this is actually just been\na fun thing that I've even seen.",
    "start": "166035",
    "end": "168660"
  },
  {
    "text": "When I started grad school,\nthere was like approach",
    "start": "168660",
    "end": "171870"
  },
  {
    "text": "that statisticians were\nusing to fit this model.",
    "start": "171870",
    "end": "174940"
  },
  {
    "text": "And this was a famous paper\nthat had just come out",
    "start": "174940",
    "end": "177030"
  },
  {
    "text": "when I started,\nand it was written",
    "start": "177030",
    "end": "178447"
  },
  {
    "text": "by Rob, and Trevor,\nand a few other people",
    "start": "178447",
    "end": "180870"
  },
  {
    "text": "at Stanford in statistics.",
    "start": "180870",
    "end": "182260"
  },
  {
    "text": "And then a new paper came\nout with a better idea,",
    "start": "182260",
    "end": "185069"
  },
  {
    "text": "and then 20 more papers\ncame out with better ideas",
    "start": "185070",
    "end": "187170"
  },
  {
    "text": "for how to fit this model.",
    "start": "187170",
    "end": "188410"
  },
  {
    "text": "And this has suddenly\nbecome something",
    "start": "188410",
    "end": "189993"
  },
  {
    "text": "that anyone can solve\non their laptop,",
    "start": "189993",
    "end": "191580"
  },
  {
    "text": "no matter how big your\ndata is, basically.",
    "start": "191580",
    "end": "193570"
  },
  {
    "text": "And so it's just become an\nincredibly useful tool in a way",
    "start": "193570",
    "end": "196050"
  },
  {
    "text": "that it even wasn't when\nI started grad school.",
    "start": "196050",
    "end": "198250"
  },
  {
    "text": "So we'll talk\nabout Glmnet, which",
    "start": "198250",
    "end": "200040"
  },
  {
    "text": "is in our library, which we use\na lot in the book in the course.",
    "start": "200040",
    "end": "204819"
  },
  {
    "text": "And we'll show you how you\ncan the lasso using Glmnet",
    "start": "204820",
    "end": "209340"
  },
  {
    "text": "in R. Again, with the\nnumbers of variables",
    "start": "209340",
    "end": "211642"
  },
  {
    "text": "might be in the\ntens of thousands,",
    "start": "211643",
    "end": "213060"
  },
  {
    "text": "you can solve it on\na standard desktop",
    "start": "213060",
    "end": "216000"
  },
  {
    "text": "computer in less than a minute.",
    "start": "216000",
    "end": "217500"
  },
  {
    "text": "So we'll talk about\nthe computation later",
    "start": "217500",
    "end": "219660"
  },
  {
    "text": "on in the course.",
    "start": "219660",
    "end": "222420"
  },
  {
    "text": "But let's first see what it\nlooks like in the same example",
    "start": "222420",
    "end": "225810"
  },
  {
    "text": "now.",
    "start": "225810",
    "end": "226430"
  },
  {
    "text": "So again, the credit\ndata set, and we're",
    "start": "226430",
    "end": "229500"
  },
  {
    "text": "plotting the\nstandardized coefficients",
    "start": "229500",
    "end": "231810"
  },
  {
    "text": "as a function of\nlambda for the lasso.",
    "start": "231810",
    "end": "233483"
  },
  {
    "text": "Again, we haven't talked\nabout how to choose lambda,",
    "start": "233483",
    "end": "235650"
  },
  {
    "text": "and that's going\nto be important,",
    "start": "235650",
    "end": "236569"
  },
  {
    "text": "we use cross-validation.",
    "start": "236570",
    "end": "237520"
  },
  {
    "text": "But let's for now\nlook at the solutions",
    "start": "237520",
    "end": "239145"
  },
  {
    "text": "as a function of lambda\nfor all values of lambda.",
    "start": "239145",
    "end": "241770"
  },
  {
    "text": "And now you can see again,\nwhen lambda is small,",
    "start": "241770",
    "end": "245730"
  },
  {
    "text": "we get essentially the full\nleast squares estimates.",
    "start": "245730",
    "end": "248170"
  },
  {
    "text": "When lambda is 0, we get\nexactly the least squares",
    "start": "248170",
    "end": "250380"
  },
  {
    "text": "if I plotted this all\nthe way to the left.",
    "start": "250380",
    "end": "252570"
  },
  {
    "text": "And now as we increase\nlambda, we get shrinkage",
    "start": "252570",
    "end": "255420"
  },
  {
    "text": "as we did for ridge regression.",
    "start": "255420",
    "end": "256764"
  },
  {
    "text": "But something special\nhappens, at this point,",
    "start": "256765",
    "end": "258640"
  },
  {
    "text": "for example, here,\nbeyond this point,",
    "start": "258640",
    "end": "260850"
  },
  {
    "text": "all these gray variables,\nthe coefficients,",
    "start": "260850",
    "end": "263370"
  },
  {
    "text": "are exactly zero, whereas\nfor ridge regression",
    "start": "263370",
    "end": "266080"
  },
  {
    "text": "they were small,\nbut they weren't 0.",
    "start": "266080",
    "end": "267580"
  },
  {
    "text": "So actually it tells\nus, you can throw away",
    "start": "267580",
    "end": "271319"
  },
  {
    "text": "all these variables\nat this point",
    "start": "271320",
    "end": "272790"
  },
  {
    "text": "and just retain these three,\nthe blue, red and orange.",
    "start": "272790",
    "end": "278055"
  },
  {
    "text": "Similarly, this plot\nshows you the same thing",
    "start": "278055",
    "end": "279930"
  },
  {
    "text": "in the other direction.",
    "start": "279930",
    "end": "281130"
  },
  {
    "text": "So it's a combination of\nboth shrinkage and selection",
    "start": "281130",
    "end": "285160"
  },
  {
    "text": "of variables.",
    "start": "285160",
    "end": "286793"
  },
  {
    "text": "And so one thing\nthat's worth mentioning",
    "start": "286793",
    "end": "288460"
  },
  {
    "text": "is that in a lot\nof applications,",
    "start": "288460",
    "end": "290139"
  },
  {
    "text": "selecting variables is\nactually really important",
    "start": "290140",
    "end": "292360"
  },
  {
    "text": "because let's say I'm\nworking with a doctor who",
    "start": "292360",
    "end": "295444"
  },
  {
    "text": "wants to come up with\na really good way",
    "start": "295445",
    "end": "297070"
  },
  {
    "text": "to test for some\nparticular disease.",
    "start": "297070",
    "end": "299420"
  },
  {
    "text": "And he might start out by\ngetting 30,000 gene expression",
    "start": "299420",
    "end": "302530"
  },
  {
    "text": "measurements for patients\nwith this type of disease.",
    "start": "302530",
    "end": "304730"
  },
  {
    "text": "So he starts out\nwith p equals 30,000,",
    "start": "304730",
    "end": "306683"
  },
  {
    "text": "and he wants to find a really\ngreat model that can be used",
    "start": "306683",
    "end": "309100"
  },
  {
    "text": "to test for this disease.",
    "start": "309100",
    "end": "310460"
  },
  {
    "text": "But when push comes to shove and\nhe's actually going to use this",
    "start": "310460",
    "end": "313240"
  },
  {
    "text": "test in the clinic, he doesn't\nwant a test that involves all",
    "start": "313240",
    "end": "315880"
  },
  {
    "text": "30,000 genes, because a\ntest like that would be too",
    "start": "315880",
    "end": "318160"
  },
  {
    "text": "expensive, it just wouldn't\nbe feasible to actually use.",
    "start": "318160",
    "end": "320960"
  },
  {
    "text": "But if he can get a test\nthat works really well,",
    "start": "320960",
    "end": "323050"
  },
  {
    "text": "that only involves\nsix, or eight,",
    "start": "323050",
    "end": "324650"
  },
  {
    "text": "or 25 genes, that could be a\nreal breakthrough in testing",
    "start": "324650",
    "end": "327940"
  },
  {
    "text": "for this disease.",
    "start": "327940",
    "end": "328960"
  },
  {
    "text": "And so just from a\npractical perspective,",
    "start": "328960",
    "end": "330940"
  },
  {
    "text": "the lasso is just hugely\nuseful, because it",
    "start": "330940",
    "end": "333070"
  },
  {
    "text": "allows us to efficiently find\nthese types of sparse models",
    "start": "333070",
    "end": "336160"
  },
  {
    "text": "that involve just a really\nsmall subset of the features.",
    "start": "336160",
    "end": "339970"
  },
  {
    "text": "You should be my personal\nsalesman, doing a good job.",
    "start": "339970",
    "end": "343490"
  },
  {
    "text": "But seriously,\nDanielle is right.",
    "start": "343490",
    "end": "345580"
  },
  {
    "text": "I myself, I use the\nlasso in projects",
    "start": "345580",
    "end": "348819"
  },
  {
    "text": "here at the medical school.",
    "start": "348820",
    "end": "350090"
  },
  {
    "text": "And it's very\nsatisfying to apply it,",
    "start": "350090",
    "end": "352600"
  },
  {
    "text": "and to see it helping scientists\nto find the signal in their data",
    "start": "352600",
    "end": "357520"
  },
  {
    "text": "and come up with interpretable\nsubsets among the thousands",
    "start": "357520",
    "end": "361210"
  },
  {
    "text": "of features they present to me.",
    "start": "361210",
    "end": "363110"
  },
  {
    "text": "So at this point it\nmight seem like magic.",
    "start": "363110",
    "end": "366629"
  },
  {
    "text": "Why is it just using an\nabsolute value penalty,",
    "start": "366630",
    "end": "369350"
  },
  {
    "text": "gives us the sparsity property.",
    "start": "369350",
    "end": "371190"
  },
  {
    "text": "Why do we get exactly 0.",
    "start": "371190",
    "end": "372420"
  },
  {
    "text": "And I'm going to show\nyou that in a picture.",
    "start": "372420",
    "end": "374310"
  },
  {
    "text": "So let's think about that.",
    "start": "374310",
    "end": "377000"
  },
  {
    "text": "First of all, we can formulate\nthe problem in an equivalent way",
    "start": "377000",
    "end": "381650"
  },
  {
    "text": "rather than putting a penalty.",
    "start": "381650",
    "end": "383639"
  },
  {
    "text": "Remember, before I had\nthe RSS plus lambda",
    "start": "383640",
    "end": "386450"
  },
  {
    "text": "times the sum of\nthe absolute values,",
    "start": "386450",
    "end": "388250"
  },
  {
    "text": "an equivalent way to\npose the lasso problem",
    "start": "388250",
    "end": "392150"
  },
  {
    "text": "is to say, minimize the\nRSS with a constraint,",
    "start": "392150",
    "end": "395990"
  },
  {
    "text": "a budget on the total L1\nnorm of the coefficients.",
    "start": "395990",
    "end": "399724"
  },
  {
    "text": "So this is an equivalent\nproblem in the sense",
    "start": "399725",
    "end": "401600"
  },
  {
    "text": "that if you give me\na budget s, there's",
    "start": "401600",
    "end": "404540"
  },
  {
    "text": "a lambda in the previous\nformulation that",
    "start": "404540",
    "end": "407150"
  },
  {
    "text": "corresponds to the same\nproblem and vice versa.",
    "start": "407150",
    "end": "410030"
  },
  {
    "text": "And by the way, if that\nlooks like a total mystery,",
    "start": "410030",
    "end": "412160"
  },
  {
    "text": "if you can reach back to your\ndistant, or not so distant past,",
    "start": "412160",
    "end": "415850"
  },
  {
    "text": "if you ever took AP calculus\nand you saw Lagrange multipliers",
    "start": "415850",
    "end": "418850"
  },
  {
    "text": "in high school, this\nis really something",
    "start": "418850",
    "end": "420800"
  },
  {
    "text": "that you might have truly\nseen in high school calculus",
    "start": "420800",
    "end": "423990"
  },
  {
    "text": "a long time ago, but for\nsimpler types of problems.",
    "start": "423990",
    "end": "426569"
  },
  {
    "text": "And this is just a more complex\napplication of that same idea.",
    "start": "426570",
    "end": "429485"
  },
  {
    "text": "But in a way that\nit's bound form",
    "start": "429485",
    "end": "430860"
  },
  {
    "text": "is, to me, more intuitive than\nthe Lagrange form, because think",
    "start": "430860",
    "end": "433680"
  },
  {
    "text": "of it this way, suppose that\nyou do a full least squares,",
    "start": "433680",
    "end": "436199"
  },
  {
    "text": "and you get a certain answer.",
    "start": "436200",
    "end": "437790"
  },
  {
    "text": "And let's suppose the sum\nof the absolute values",
    "start": "437790",
    "end": "439890"
  },
  {
    "text": "of your coefficients is 10.",
    "start": "439890",
    "end": "441698"
  },
  {
    "text": "So you give me an answer.",
    "start": "441698",
    "end": "442740"
  },
  {
    "text": "And I say, well, actually I want\nto make your budget smaller,",
    "start": "442740",
    "end": "446522"
  },
  {
    "text": "you've spent too\nmuch coefficient.",
    "start": "446523",
    "end": "447940"
  },
  {
    "text": "So rather than 10, I want to\ngive you a budget of maybe 5.",
    "start": "447940",
    "end": "451500"
  },
  {
    "text": "So now I ask you to\nsolve the same problem,",
    "start": "451500",
    "end": "454500"
  },
  {
    "text": "but you're not allowed to use\nthe coefficients as large as you",
    "start": "454500",
    "end": "457350"
  },
  {
    "text": "want, the total budget you have\nis 5, and that's the bound here.",
    "start": "457350",
    "end": "461320"
  },
  {
    "text": "So the lasso is\ngiving you a budget",
    "start": "461320",
    "end": "464610"
  },
  {
    "text": "on the total L1 norm\nthat you can spend,",
    "start": "464610",
    "end": "466569"
  },
  {
    "text": "and within that budget you have\nto fit as well as possible.",
    "start": "466570",
    "end": "469540"
  },
  {
    "text": "And as a budget gets\nsmaller and smaller,",
    "start": "469540",
    "end": "471330"
  },
  {
    "text": "the coefficients get\nsmaller and smaller.",
    "start": "471330",
    "end": "473038"
  },
  {
    "text": "If the budget is 0, the\ncoefficients have to be 0.",
    "start": "473038",
    "end": "476310"
  },
  {
    "text": "If the budget is large enough,\nyou free to use fully squares.",
    "start": "476310",
    "end": "480639"
  },
  {
    "text": "But in between the\nbudget is going",
    "start": "480640",
    "end": "482070"
  },
  {
    "text": "to trade off the size of the\ncoefficients with the fit.",
    "start": "482070",
    "end": "485850"
  },
  {
    "text": "So I think it's quite\nan intuitive way",
    "start": "485850",
    "end": "487740"
  },
  {
    "text": "of looking at these problems.",
    "start": "487740",
    "end": "489180"
  },
  {
    "text": "For ridge regression, you\nget exactly the same analogy,",
    "start": "489180",
    "end": "491759"
  },
  {
    "text": "but now the budget is in\nterms of the sum of squares.",
    "start": "491760",
    "end": "494230"
  },
  {
    "text": "So again, this is equivalent\nto the Lagrange formulation",
    "start": "494230",
    "end": "497940"
  },
  {
    "text": "for ridge regression\nwe saw earlier.",
    "start": "497940",
    "end": "500408"
  },
  {
    "text": "But the reason why I\nwant to bring this up,",
    "start": "500408",
    "end": "502199"
  },
  {
    "text": "is this following picture\nwhich helps to explain",
    "start": "502200",
    "end": "505440"
  },
  {
    "text": "why the lasso gives sparsity.",
    "start": "505440",
    "end": "509350"
  },
  {
    "text": "So on the right is\nridge regression,",
    "start": "509350",
    "end": "513090"
  },
  {
    "text": "and on the left is the lasso.",
    "start": "513090",
    "end": "515200"
  },
  {
    "text": "Now this is a bit more mathy\nthan most things in this course,",
    "start": "515200",
    "end": "517860"
  },
  {
    "text": "but hang in there,\nand I think if you do,",
    "start": "517860",
    "end": "521399"
  },
  {
    "text": "there'll be some payoff.",
    "start": "521400",
    "end": "522400"
  },
  {
    "text": "So this picture is\nridge regression.",
    "start": "522400",
    "end": "524980"
  },
  {
    "text": "So what's going on here?",
    "start": "524980",
    "end": "526380"
  },
  {
    "text": "First of all, p is 2, so\nthere's two coefficients.",
    "start": "526380",
    "end": "530010"
  },
  {
    "text": "And I've indicated here the\nfull least squares estimate,",
    "start": "530010",
    "end": "532480"
  },
  {
    "text": "so if there was\nno penalty, I just",
    "start": "532480",
    "end": "534095"
  },
  {
    "text": "did least squares on\nthe two variables,",
    "start": "534095",
    "end": "535720"
  },
  {
    "text": "I'll call the solution beta\nhat, and that's this point.",
    "start": "535720",
    "end": "538079"
  },
  {
    "text": "And now the sums of\nsquares in the RSS,",
    "start": "538080",
    "end": "543780"
  },
  {
    "text": "the contours of that function,\nand that was its lowest here,",
    "start": "543780",
    "end": "547140"
  },
  {
    "text": "because that's the\nleast squares estimate.",
    "start": "547140",
    "end": "548890"
  },
  {
    "text": "But now as I move away,\nif I think of this",
    "start": "548890",
    "end": "551340"
  },
  {
    "text": "as maybe a cereal bowl, and\nI slice the cereal bowl,",
    "start": "551340",
    "end": "554340"
  },
  {
    "text": "here are the contours.",
    "start": "554340",
    "end": "555490"
  },
  {
    "text": "So here's the value at\nwhich RSS is a bigger value.",
    "start": "555490",
    "end": "559390"
  },
  {
    "text": "This next contour is a\nhigher contour of RSS.",
    "start": "559390",
    "end": "562870"
  },
  {
    "text": "So these are the contours of RSS\nas I move away from the minimum.",
    "start": "562870",
    "end": "566130"
  },
  {
    "start": "566130",
    "end": "568026"
  },
  {
    "text": "And this is the\nconstraint region.",
    "start": "568026",
    "end": "570210"
  },
  {
    "text": "Remember in ridge regression,\nand this formulation here",
    "start": "570210",
    "end": "573140"
  },
  {
    "text": "says you have a budget on\nthe total sum of squares",
    "start": "573140",
    "end": "576890"
  },
  {
    "text": "of the betas.",
    "start": "576890",
    "end": "577950"
  },
  {
    "text": "So the budget is the\nradius of a circle,",
    "start": "577950",
    "end": "582350"
  },
  {
    "text": "Here I have a fixed budget, and\nso in words, the ridge problem",
    "start": "582350",
    "end": "587209"
  },
  {
    "text": "says, find me the first\nplace these contours hit",
    "start": "587210",
    "end": "589910"
  },
  {
    "text": "the constraint region.",
    "start": "589910",
    "end": "592160"
  },
  {
    "text": "In other words, find\nme the smallest RSS",
    "start": "592160",
    "end": "594379"
  },
  {
    "text": "you can get within the budget\ndefined by the circle, that's",
    "start": "594380",
    "end": "601072"
  },
  {
    "text": "ridge regression.",
    "start": "601072",
    "end": "601780"
  },
  {
    "text": "And the solution in this\npicture is right here.",
    "start": "601780",
    "end": "604710"
  },
  {
    "text": "So this is the ridge\nestimates for this budget",
    "start": "604710",
    "end": "608340"
  },
  {
    "text": "and this particular\ndata, and the data",
    "start": "608340",
    "end": "610530"
  },
  {
    "text": "is determining the\nshape of these contours",
    "start": "610530",
    "end": "612510"
  },
  {
    "text": "and the location of beta hat.",
    "start": "612510",
    "end": "614500"
  },
  {
    "text": "So ridge regression\nsays, find me",
    "start": "614500",
    "end": "618370"
  },
  {
    "text": "the first place the\ncontours hit this constraint",
    "start": "618370",
    "end": "620589"
  },
  {
    "text": "region for this solution.",
    "start": "620590",
    "end": "621782"
  },
  {
    "text": "And you can see, because\nthe constraint region,",
    "start": "621782",
    "end": "623740"
  },
  {
    "text": "the sum of squares\nis a circle, this",
    "start": "623740",
    "end": "626200"
  },
  {
    "text": "is where the sum of squares\nof beta 1 and beta 2",
    "start": "626200",
    "end": "629740"
  },
  {
    "text": "is less than a\nbudget, it's a circle,",
    "start": "629740",
    "end": "632076"
  },
  {
    "text": "and unless you're very\nlucky that you're not",
    "start": "632077",
    "end": "633910"
  },
  {
    "text": "going to hit exactly at a place\nwhere one or the other is 0.",
    "start": "633910",
    "end": "637725"
  },
  {
    "start": "637725",
    "end": "640639"
  },
  {
    "text": "Now let's move\nover to the lasso.",
    "start": "640640",
    "end": "642130"
  },
  {
    "text": "Same picture, least\nsquares, same thing.",
    "start": "642130",
    "end": "644250"
  },
  {
    "text": "The sum of squares function\nis all the same up here,",
    "start": "644250",
    "end": "647550"
  },
  {
    "text": "but the constraint region is now\nthe sum of the absolute values.",
    "start": "647550",
    "end": "651200"
  },
  {
    "text": "So rather than a\ncircle, it's a diamond.",
    "start": "651200",
    "end": "653540"
  },
  {
    "text": "A diamond has corners.",
    "start": "653540",
    "end": "655279"
  },
  {
    "text": "So in this picture,\nI've hit this corner,",
    "start": "655280",
    "end": "657710"
  },
  {
    "text": "and now I get a place\nwhere beta 1 hat is 0.",
    "start": "657710",
    "end": "661400"
  },
  {
    "text": "So here's the lasso estimate.",
    "start": "661400",
    "end": "663290"
  },
  {
    "text": "So in other words, to summarize,\nthe absolute values give you",
    "start": "663290",
    "end": "666709"
  },
  {
    "text": "a constraint region\nthat has sharp corners",
    "start": "666710",
    "end": "669500"
  },
  {
    "text": "and high dimensions, you\nhave edges and corners.",
    "start": "669500",
    "end": "673190"
  },
  {
    "text": "And along an edge or corner,\nif you hit there, you get a 0.",
    "start": "673190",
    "end": "676610"
  },
  {
    "text": "So this is geometrically why\nyou get sparsity in the lasso.",
    "start": "676610",
    "end": "680649"
  },
  {
    "start": "680650",
    "end": "685130"
  },
  {
    "text": "So here's our case.",
    "start": "685130",
    "end": "690260"
  },
  {
    "text": "First of all return\nto the example where",
    "start": "690260",
    "end": "692240"
  },
  {
    "text": "we had 45 variables, and they\nall had non-zero coefficients.",
    "start": "692240",
    "end": "699110"
  },
  {
    "text": "So now I'm looking at\ncomparing the lasso to ridge.",
    "start": "699110",
    "end": "706149"
  },
  {
    "text": "So on the left\npicture for the lasso,",
    "start": "706150",
    "end": "708430"
  },
  {
    "text": "we see the bias variance\nand mean squared error",
    "start": "708430",
    "end": "711370"
  },
  {
    "text": "as a function of\nlambda, and the right,",
    "start": "711370",
    "end": "713510"
  },
  {
    "text": "we've superimposed the\nbias variance and mean",
    "start": "713510",
    "end": "715810"
  },
  {
    "text": "squared error of\nridge regression",
    "start": "715810",
    "end": "717279"
  },
  {
    "text": "with a broken line in the lasso.",
    "start": "717280",
    "end": "719740"
  },
  {
    "text": "And we can see that, overall,\nso again, the solid line here",
    "start": "719740",
    "end": "722950"
  },
  {
    "text": "for mean squared error is the\nlasso, the broken line is ridge.",
    "start": "722950",
    "end": "726970"
  },
  {
    "text": "They're very similar, ridge\nis a little better perhaps.",
    "start": "726970",
    "end": "730329"
  },
  {
    "text": "So we don't do better at\nall with the lasso here.",
    "start": "730330",
    "end": "734900"
  },
  {
    "text": "And the reason is, that the\ntrue model is not sparse,",
    "start": "734900",
    "end": "738200"
  },
  {
    "text": "the true model actually involves\n45 variables, all of which",
    "start": "738200",
    "end": "742270"
  },
  {
    "text": "have been given non-zero\ncoefficients in the population.",
    "start": "742270",
    "end": "745550"
  },
  {
    "text": "So it's not surprising\nthat we don't do better",
    "start": "745550",
    "end": "747820"
  },
  {
    "text": "than ridge in this case.",
    "start": "747820",
    "end": "748850"
  },
  {
    "text": "And one thing we should mention\non this right-hand panel,",
    "start": "748850",
    "end": "751269"
  },
  {
    "text": "the x-axis is something\nwe haven't seen before,",
    "start": "751270",
    "end": "754060"
  },
  {
    "text": "which is the R squared\non the training data.",
    "start": "754060",
    "end": "756310"
  },
  {
    "text": "And the reason we\nhave that x-axis m",
    "start": "756310",
    "end": "757960"
  },
  {
    "text": "because in this figure\non the right-hand side,",
    "start": "757960",
    "end": "759877"
  },
  {
    "text": "we're plotting both ridge\nregression and the lasso,",
    "start": "759877",
    "end": "762260"
  },
  {
    "text": "so it wouldn't make sense\nto show ridge regression",
    "start": "762260",
    "end": "765640"
  },
  {
    "text": "and the lasso with\nlambda on the x-axis,",
    "start": "765640",
    "end": "768022"
  },
  {
    "text": "because the lambda means two\ndifferent things for those two",
    "start": "768022",
    "end": "770480"
  },
  {
    "text": "models.",
    "start": "770480",
    "end": "771329"
  },
  {
    "text": "So when we look at R squared on\nthe training data on the x axis,",
    "start": "771330",
    "end": "775010"
  },
  {
    "text": "that's a universally\nsensible thing",
    "start": "775010",
    "end": "778790"
  },
  {
    "text": "to measure, regardless of\nwhat the type of model is.",
    "start": "778790",
    "end": "783035"
  },
  {
    "text": "You must have drawn this\npicture in the book.",
    "start": "783035",
    "end": "784910"
  },
  {
    "text": "Yeah, I made this picture.",
    "start": "784910",
    "end": "786699"
  },
  {
    "text": "It's a beauty.",
    "start": "786700",
    "end": "787480"
  },
  {
    "text": "Thank you.",
    "start": "787480",
    "end": "789743"
  },
  {
    "text": "I wouldn't have noticed\nthat detail otherwise.",
    "start": "789743",
    "end": "791660"
  },
  {
    "text": "[LAUGHS]",
    "start": "791660",
    "end": "792160"
  },
  {
    "text": "So now here's a situation where\nwe do we do perform better",
    "start": "792160",
    "end": "796600"
  },
  {
    "text": "with the lasso.",
    "start": "796600",
    "end": "797300"
  },
  {
    "text": "And this is a case where\nnow in the population,",
    "start": "797300",
    "end": "799790"
  },
  {
    "text": "only two of the predictors\nhave non-zero coefficients.",
    "start": "799790",
    "end": "802040"
  },
  {
    "text": "So the previous situation\nwas dense or non-sparse.",
    "start": "802040",
    "end": "806930"
  },
  {
    "text": "This situation is sparse.",
    "start": "806930",
    "end": "809635"
  },
  {
    "text": "There's only two predictors\nin the true model",
    "start": "809635",
    "end": "811510"
  },
  {
    "text": "that are non-zero coefficients.",
    "start": "811510",
    "end": "813217"
  },
  {
    "text": "And now we can see what happens.",
    "start": "813217",
    "end": "814550"
  },
  {
    "text": "Well, the lasso's\nmean square error here",
    "start": "814550",
    "end": "818440"
  },
  {
    "text": "is minimized for quite\na large value of lambda,",
    "start": "818440",
    "end": "821590"
  },
  {
    "text": "because it wants to make the\nmodel sparse, as it needs to,",
    "start": "821590",
    "end": "824690"
  },
  {
    "text": "there's only two\nnon-zero coefficients.",
    "start": "824690",
    "end": "826370"
  },
  {
    "text": "And now when we compare the\nlasso to ridge, remember, ridge",
    "start": "826370",
    "end": "830380"
  },
  {
    "text": "is the broken line, and\nthe lasso is a solid line.",
    "start": "830380",
    "end": "835700"
  },
  {
    "text": "Here's the lasso's mean square\nerror, and ridge is here.",
    "start": "835700",
    "end": "841300"
  },
  {
    "text": "You can see we're doing\nquite a bit better",
    "start": "841300",
    "end": "843279"
  },
  {
    "text": "using the lasso\nin this situation.",
    "start": "843280",
    "end": "845270"
  },
  {
    "text": "And again, it's not surprising,\nthe true model is sparse,",
    "start": "845270",
    "end": "847670"
  },
  {
    "text": "so it pays to use a\ntechnique which encourages",
    "start": "847670",
    "end": "851930"
  },
  {
    "text": "sparse models coming\nout of its estimation,",
    "start": "851930",
    "end": "854630"
  },
  {
    "text": "whereas with ridge, we\ndon't get a sparse model,",
    "start": "854630",
    "end": "857750"
  },
  {
    "text": "we get a dense model.",
    "start": "857750",
    "end": "858625"
  },
  {
    "start": "858625",
    "end": "861280"
  },
  {
    "text": "So in conclusion, comparing\nthese two techniques,",
    "start": "861280",
    "end": "865600"
  },
  {
    "text": "as is usually the case in most\nthings in statistics and science",
    "start": "865600",
    "end": "869860"
  },
  {
    "text": "in general, there's\nno simple rule",
    "start": "869860",
    "end": "873255"
  },
  {
    "text": "that means that\nyou should always",
    "start": "873255",
    "end": "874630"
  },
  {
    "text": "use one technique over another.",
    "start": "874630",
    "end": "875920"
  },
  {
    "text": "It depends, you always hear that\nfrom statisticians, it depends.",
    "start": "875920",
    "end": "878587"
  },
  {
    "text": "Well, it depends\non the situation.",
    "start": "878587",
    "end": "881630"
  },
  {
    "text": "In this particular case, if\nthe true model is quite dense,",
    "start": "881630",
    "end": "885640"
  },
  {
    "text": "most predictors have\nnon-zero coefficients,",
    "start": "885640",
    "end": "887870"
  },
  {
    "text": "we can expect to do\nbetter with ridge.",
    "start": "887870",
    "end": "890170"
  },
  {
    "text": "If the true model\nis quite sparse,",
    "start": "890170",
    "end": "892149"
  },
  {
    "text": "only a few coefficients\nhave are non-zero,",
    "start": "892150",
    "end": "894970"
  },
  {
    "text": "then the lasso can be\nexpected to do better.",
    "start": "894970",
    "end": "897125"
  },
  {
    "text": "Of course, we don't\nknow that, usually.",
    "start": "897125",
    "end": "898750"
  },
  {
    "text": "We hope actually that\nthings are sparse,",
    "start": "898750",
    "end": "900430"
  },
  {
    "text": "because life is simpler then.",
    "start": "900430",
    "end": "901880"
  },
  {
    "text": "But going into a\ndata analysis, we",
    "start": "901880",
    "end": "903880"
  },
  {
    "text": "have no idea whether the true\nnumber of non-zero coefficients",
    "start": "903880",
    "end": "907030"
  },
  {
    "text": "is large or small.",
    "start": "907030",
    "end": "908770"
  },
  {
    "text": "So it's typically good\nto apply both methods,",
    "start": "908770",
    "end": "913810"
  },
  {
    "text": "and to use cross-validation to\ndetermine the best model coming",
    "start": "913810",
    "end": "917260"
  },
  {
    "text": "out of each m and then compare\nthe cross-validated error",
    "start": "917260",
    "end": "919720"
  },
  {
    "text": "for the two methods.",
    "start": "919720",
    "end": "921500"
  },
  {
    "start": "921500",
    "end": "922000"
  }
]