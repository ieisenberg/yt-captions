[
  {
    "text": "all right so thanks all for coming uh today i want to talk about large language models the topic of of intense",
    "start": "11200",
    "end": "16480"
  },
  {
    "text": "interest by by many faculty here um and i think i want to highlight some risks of being too excited about them",
    "start": "16480",
    "end": "23519"
  },
  {
    "text": "but also the opportunities that they enable for problems of safety and broader societal concerns",
    "start": "23519",
    "end": "30480"
  },
  {
    "text": "um i'm i'm imagining that not all of you are familiar with language models and what they've done for the field of nlp",
    "start": "30480",
    "end": "36960"
  },
  {
    "text": "so i'm going to open with this kind of slide and in fact the slide is now pretty old but i think it gets a",
    "start": "36960",
    "end": "42640"
  },
  {
    "text": "the huge gains that have been a result of language models so this chart shows the benchmark performance on",
    "start": "42640",
    "end": "48399"
  },
  {
    "text": "squad the stanford question answering data set um and this is a really classic benchmark that tests our ability to",
    "start": "48399",
    "end": "55280"
  },
  {
    "text": "answer questions based on wikipedia articles right one of these classic challenges um and performance had been pretty",
    "start": "55280",
    "end": "61039"
  },
  {
    "text": "stagnant i mean there were some improvements up to 2018 and you see this like real inflection point where we got",
    "start": "61039",
    "end": "66880"
  },
  {
    "text": "this big jump of maybe like 10 plus percent or whatever um and this is all really attributable to the use of",
    "start": "66880",
    "end": "73360"
  },
  {
    "text": "pre-training that is the use of large internet corpora uh to initialize our models um and then uh as well as like a",
    "start": "73360",
    "end": "81040"
  },
  {
    "text": "particular model called from google and so these kinds of systems have really driven dramatic performance so",
    "start": "81040",
    "end": "87280"
  },
  {
    "text": "this is one example from a question answering data set but just kind of imagine this you know several percentage gains applied",
    "start": "87280",
    "end": "93920"
  },
  {
    "text": "everywhere in the field it's a very dramatic change so they really power nlp systems today",
    "start": "93920",
    "end": "99759"
  },
  {
    "text": "everywhere and so i think empirical nlp systems building 2018 onwards is really",
    "start": "99759",
    "end": "105040"
  },
  {
    "text": "this paradigm of you take this big language model which is designed to sort of predict what words come next on large",
    "start": "105040",
    "end": "111920"
  },
  {
    "text": "internet text when we take these systems and then we fine tune them on some downstream tasks maybe you want to make a system that can",
    "start": "111920",
    "end": "118880"
  },
  {
    "text": "judge the quality of a piece of text or whatever you would take your language model you would fine tune it and then this is really how a lot of these things",
    "start": "118880",
    "end": "125600"
  },
  {
    "text": "work so the performance of this paradigm is really impressive but there's important",
    "start": "125600",
    "end": "131120"
  },
  {
    "text": "questions that remain even though we have high performance the first thing is can we truly trust them in kind of like",
    "start": "131120",
    "end": "136400"
  },
  {
    "text": "high-stakes situations and i'll talk about one example in a few minutes um and the second thing is that this",
    "start": "136400",
    "end": "142160"
  },
  {
    "text": "pre-training paradigm we've it feels like we've gotten something for free right we go on the internet take publicly available data and we've gotten",
    "start": "142160",
    "end": "149200"
  },
  {
    "text": "performance improvements what costs are we paying uh through that paradigm are there new risks or harm so that's the",
    "start": "149200",
    "end": "154879"
  },
  {
    "text": "second part of this talk that i want to talk about so question one is can we trust these",
    "start": "154879",
    "end": "160239"
  },
  {
    "text": "models and because of the impressive performance that these systems have achieved and the continual improvement",
    "start": "160239",
    "end": "165440"
  },
  {
    "text": "of this like pre-training paradigm over the last let's say three or so years people are getting more ambitious and",
    "start": "165440",
    "end": "171200"
  },
  {
    "text": "more excited about what we can use these models for so on the top left this is a this is a really nice paper sort of a",
    "start": "171200",
    "end": "177440"
  },
  {
    "text": "sort of inspiring thing by some people from google research saying maybe we can use language models to in fact guide the",
    "start": "177440",
    "end": "183440"
  },
  {
    "text": "creation of data sets right and that's actually pretty cool and also a little bit crazy because data sets are you know",
    "start": "183440",
    "end": "189200"
  },
  {
    "text": "our most cherished things that guide how machine learning systems behave and they show some nice results here on the right",
    "start": "189200",
    "end": "195360"
  },
  {
    "text": "some people have tried to use pre-trained language models plus fine tuning as a way to evaluate models and",
    "start": "195360",
    "end": "201200"
  },
  {
    "text": "that's also another high-stakes setting right so now we're applying these pre-trained language models to the most",
    "start": "201200",
    "end": "206640"
  },
  {
    "text": "critical parts of the machine learning pipeline data generation and evaluation and are these models truly so good that",
    "start": "206640",
    "end": "212720"
  },
  {
    "text": "we can use them in these kinds of high stakes tasks so i want to answer that question in the first part of this talk",
    "start": "212720",
    "end": "219120"
  },
  {
    "text": "the second thing i want to talk about is new kinds of harms and attack vectors and things like that that arise from the",
    "start": "219120",
    "end": "225040"
  },
  {
    "text": "use of language models as part of our systems i want to highlight privacy in particular because i think it's a new",
    "start": "225040",
    "end": "231360"
  },
  {
    "text": "and unique thing that happens because we're training these huge models on large internet text we can now",
    "start": "231360",
    "end": "238840"
  },
  {
    "text": "recover various pieces of text from the internet at first this seems relatively harmless why do we care about the fact",
    "start": "238840",
    "end": "245599"
  },
  {
    "text": "that we can recover pieces of text from the internet but we'll see that there's valid concerns about privacy and the",
    "start": "245599",
    "end": "251439"
  },
  {
    "text": "protection of sort of what was formerly hidden information being revealed by these language models so language models",
    "start": "251439",
    "end": "258239"
  },
  {
    "text": "even though they help performance they may be very detrimental for things like privacy and cause some serious sort of",
    "start": "258239",
    "end": "264000"
  },
  {
    "text": "security risks as well in the in the process so um this really i think a lot of my",
    "start": "264000",
    "end": "270560"
  },
  {
    "text": "thoughts and some of this work the directions of the work have been shaped uh by the center for research of foundation models i'm sort of giving to",
    "start": "270560",
    "end": "277280"
  },
  {
    "text": "a slide plug here it's a sort of big initiative with with many faculty across many departments um and",
    "start": "277280",
    "end": "284320"
  },
  {
    "text": "if you want to know more about language models and their use and sort of the background surrounding this i would",
    "start": "284320",
    "end": "289360"
  },
  {
    "text": "encourage you to go and look at the relevant parts of this white paper that we wrote that interest you um the the",
    "start": "289360",
    "end": "295360"
  },
  {
    "text": "title of this talk is sort of a reference and a homage to exactly this white paper so i think a lot of it i",
    "start": "295360",
    "end": "300720"
  },
  {
    "text": "think you'll find interesting if you find this talk interesting um so the first thing i want to talk",
    "start": "300720",
    "end": "305759"
  },
  {
    "text": "about i want to talk about two pieces of work uh that i've sort of recently been doing one of them is in this evaluation metric",
    "start": "305759",
    "end": "312639"
  },
  {
    "text": "sense um what kinds of new capabilities and harms and pitfalls might we see when we apply language",
    "start": "312639",
    "end": "319680"
  },
  {
    "text": "models to high stakes tasks so especially evaluation so i want to highlight again",
    "start": "319680",
    "end": "326080"
  },
  {
    "text": "this like data generation thing because i think it's pretty striking of the amount of trust that we're putting in",
    "start": "326080",
    "end": "331280"
  },
  {
    "text": "these kind of language model systems i think data generation is really an emerging thing this paper appeared i",
    "start": "331280",
    "end": "337120"
  },
  {
    "text": "think the europe's data sets track just last winter but i think talking to other",
    "start": "337120",
    "end": "342400"
  },
  {
    "text": "researchers in the area this is a thing that people really think is going to like continue to grow right so this is",
    "start": "342400",
    "end": "348320"
  },
  {
    "text": "an example of what's called synth bio i'll talk about this in a little bit more detail now on the left side um",
    "start": "348320",
    "end": "355039"
  },
  {
    "text": "is a fictitious biography this person doesn't actually exist it was synthetically generated",
    "start": "355039",
    "end": "360479"
  },
  {
    "text": "of something that looks like a wikipedia info box right so if you go to wikipedia you'll see a box that has like a name",
    "start": "360479",
    "end": "366400"
  },
  {
    "text": "and their occupation and you know the notable works of a novelist so basically you have this in structured table form",
    "start": "366400",
    "end": "373840"
  },
  {
    "text": "and the task is to generate a piece of text on the right and if you sort of stare at this it's actually pretty good",
    "start": "373840",
    "end": "379520"
  },
  {
    "text": "this is mostly written by a language model and slightly edited by a human um and so",
    "start": "379520",
    "end": "385520"
  },
  {
    "text": "the success of these kinds of things really give us optimism that language models can maybe in fact be used in a",
    "start": "385520",
    "end": "391759"
  },
  {
    "text": "lot of high-stakes tasks data generation and evaluation um and i think this has been a trend that",
    "start": "391759",
    "end": "398080"
  },
  {
    "text": "has been going on for a few years now of trying to use language model fine tuning",
    "start": "398080",
    "end": "403600"
  },
  {
    "text": "and things like this to automate model evaluations um and so these are some",
    "start": "403600",
    "end": "409039"
  },
  {
    "text": "sample papers i'm not making a commentary on whether these are good or bad or so on but they're sort of attempts at taking",
    "start": "409039",
    "end": "415840"
  },
  {
    "text": "the high performance of language models and using them to evaluate dialogue systems so the setup is something like this you",
    "start": "415840",
    "end": "422800"
  },
  {
    "text": "have built a chatbot you would like to evaluate how good this chatbot is but of course chatbots return natural language",
    "start": "422800",
    "end": "429440"
  },
  {
    "text": "and there's no easy metric like accuracy that you can compute so the traditional paradigm is to sit a bunch of users in",
    "start": "429440",
    "end": "435520"
  },
  {
    "text": "front of this chatbot and ask them how good was this interaction that's actually pretty expensive and",
    "start": "435520",
    "end": "440800"
  },
  {
    "text": "pretty hard and so there is this dream i think of training language model based evaluators based on human judgments that",
    "start": "440800",
    "end": "447680"
  },
  {
    "text": "we've collected offline and then evaluating our models against this predictor of how good a human would",
    "start": "447680",
    "end": "454479"
  },
  {
    "text": "think the system is so then we can get low-cost evaluations of our models and we can sort of improve our chatbots and",
    "start": "454479",
    "end": "460319"
  },
  {
    "text": "so on based on this evaluation scheme right like it's a very compelling vision for how we might eventually",
    "start": "460319",
    "end": "466400"
  },
  {
    "text": "replace humans but at the same time",
    "start": "466400",
    "end": "471599"
  },
  {
    "text": "you know it's sort of a question of are these evaluation metrics really going to succeed right in the sense that can we",
    "start": "471599",
    "end": "478639"
  },
  {
    "text": "really build something that's as good as humans um and i think the thing that people have been sort of excited about",
    "start": "478639",
    "end": "484800"
  },
  {
    "text": "is the fact that some of these proposed metrics seem to be coming close to human level performance so on the right side",
    "start": "484800",
    "end": "490720"
  },
  {
    "text": "here is a whole bunch of correlations right and on the very top you see human correlations with each other so that's",
    "start": "490720",
    "end": "496160"
  },
  {
    "text": "like roughly not the very best that you can do but a good sense of what a good system should do and the next two sets of numbers is like",
    "start": "496160",
    "end": "503520"
  },
  {
    "text": "traditional quality evaluation metrics in natural language generation and at the bottom are these learning based ones that are",
    "start": "503520",
    "end": "510240"
  },
  {
    "text": "like powered off of these language models and you see at the very bottom like some of the newest methods that people have developed are in like this",
    "start": "510240",
    "end": "516560"
  },
  {
    "text": "0.7 correlation range that's you know the human average correlation so you see this table right like someone shows you",
    "start": "516560",
    "end": "522399"
  },
  {
    "text": "this and then you're going to say wow you know our learning based evaluation metrics are getting really good right like are they ready to be deployed and",
    "start": "522399",
    "end": "529040"
  },
  {
    "text": "used and trusted right but looking at this as a skeptical uh",
    "start": "529040",
    "end": "534640"
  },
  {
    "text": "researcher you know it gives you pause because we know that neural systems especially like language model based",
    "start": "534640",
    "end": "539920"
  },
  {
    "text": "systems they can achieve incredibly high performance and yet at the same time fail in really silly ways um this is an",
    "start": "539920",
    "end": "546640"
  },
  {
    "text": "example that i got from from my student last night um this is a similar benchmark or generation",
    "start": "546640",
    "end": "552320"
  },
  {
    "text": "task as the synth bio data set i showed you before so this is a task of uh you get as an input some data fields about a",
    "start": "552320",
    "end": "559839"
  },
  {
    "text": "restaurant so this is a restaurant called the aromi which is chinese and so on and so forth and the task is to",
    "start": "559839",
    "end": "565600"
  },
  {
    "text": "generate that sentence on the right that one was human written or actually no this was this was uh generated by model",
    "start": "565600",
    "end": "571040"
  },
  {
    "text": "aromi is a chinese restaurant near the crown plaza hotel blah blah blah blah right so if you use a pre-trained neural",
    "start": "571040",
    "end": "577279"
  },
  {
    "text": "model for this task you get beautiful text like this like you know fluently written",
    "start": "577279",
    "end": "582320"
  },
  {
    "text": "matches the input so on and so forth this is a task called eqe and i think most people consider this to be a",
    "start": "582320",
    "end": "587680"
  },
  {
    "text": "completely solved task around 2020. one of my students even calls it easy to easy now the thing is it's not really easy",
    "start": "587680",
    "end": "595680"
  },
  {
    "text": "because if you replace the restaurant name with starbucks which is a completely reasonable substitution to do",
    "start": "595680",
    "end": "602000"
  },
  {
    "text": "suddenly the system completely falls apart it now thinks that the chinese restaurant is called the crown plaza",
    "start": "602000",
    "end": "607519"
  },
  {
    "text": "hotel which is the thing that it's near it's not the name at all so it can just blow up in completely",
    "start": "607519",
    "end": "613200"
  },
  {
    "text": "unexpected ways when you go a little bit outside of the training data set and so",
    "start": "613200",
    "end": "618720"
  },
  {
    "text": "these kinds of experiences i think make me sort of as a prior a little bit suspicious of this belief that we can",
    "start": "618720",
    "end": "625360"
  },
  {
    "text": "just take models that look high performance you know in our normal evaluation and then go out and deploy",
    "start": "625360",
    "end": "631440"
  },
  {
    "text": "them into the wild or in high stakes settings so okay",
    "start": "631440",
    "end": "636880"
  },
  {
    "text": "um my my postdocs s and physol were sort of similarly skeptical and they were looking into these evaluations and our",
    "start": "636880",
    "end": "643040"
  },
  {
    "text": "initial interest was in doing things like evaluating the factuality of language generation systems and what",
    "start": "643040",
    "end": "649360"
  },
  {
    "text": "they found was something a little bit troubling at the very beginning they found that factuality metrics for",
    "start": "649360",
    "end": "654959"
  },
  {
    "text": "summarization tasks had this really weird structure like first of all they didn't seem to perform particularly well",
    "start": "654959",
    "end": "661360"
  },
  {
    "text": "compared to baseline so if you just measure word overlap they seem to do quite well and second of all they seem",
    "start": "661360",
    "end": "666720"
  },
  {
    "text": "to have higher correlation with simple things like word overlap than with human evaluation and those two combined make",
    "start": "666720",
    "end": "673519"
  },
  {
    "text": "us a little bit suspicious are they truly learning the task or are they learning these surface structures like",
    "start": "673519",
    "end": "679200"
  },
  {
    "text": "are they learning to just look at word overlap in a slightly more clever way",
    "start": "679200",
    "end": "685120"
  },
  {
    "text": "and i think that's the first thing that i want to emphasize in this first part of the talk the brittleness of these high performance systems that can arise",
    "start": "685120",
    "end": "691600"
  },
  {
    "text": "and the problems that can arise from it these things called spurious correlations where a model is picking up",
    "start": "691600",
    "end": "697760"
  },
  {
    "text": "the surface level features rather than the underlying task they're pervasive in uh these kinds of automated evaluations",
    "start": "697760",
    "end": "704160"
  },
  {
    "text": "as well as more broadly in machine learning and what we sort of showed in this set of works is that really they",
    "start": "704160",
    "end": "710160"
  },
  {
    "text": "rely on just really simple stuff like the word overlap to your input perplexity which is a measure of",
    "start": "710160",
    "end": "715839"
  },
  {
    "text": "compressibility of your sentence and length of the sentence these are just really simple stuff that has very little",
    "start": "715839",
    "end": "721600"
  },
  {
    "text": "to do with the underlying quality or factuality of generated text and yet it seems like these are the",
    "start": "721600",
    "end": "728399"
  },
  {
    "text": "things that are driving a lot of the performance rather than understanding the underlying task so we'll go through",
    "start": "728399",
    "end": "733600"
  },
  {
    "text": "a few more examples next but i want to give you the bigger picture because not all of you are excited or interested in",
    "start": "733600",
    "end": "740000"
  },
  {
    "text": "the nlp or natural language generation evaluation but i think all of you are hopefully interested in questions of",
    "start": "740000",
    "end": "746160"
  },
  {
    "text": "robustness and reliability and spurious correlations are exactly the sort of",
    "start": "746160",
    "end": "751519"
  },
  {
    "text": "thing that is troubling for these systems so what do i mean by that",
    "start": "751519",
    "end": "756560"
  },
  {
    "text": "they're sort of a standard place where we usually see spurious correlations and i've talked about this or worked on this",
    "start": "756560",
    "end": "761760"
  },
  {
    "text": "in the context of machine learning and nlp broadly so i'll give an example and i'll talk through this example",
    "start": "761760",
    "end": "767360"
  },
  {
    "text": "so that this top thing shown in the the top half of the slide is a task called entailment so",
    "start": "767360",
    "end": "773760"
  },
  {
    "text": "you're supposed to get in two sentences like a premise and a hypothesis and what your task is is to say is the",
    "start": "773760",
    "end": "780320"
  },
  {
    "text": "hypothesis entailed or logically implied by the premise so here the economy could",
    "start": "780320",
    "end": "785360"
  },
  {
    "text": "still be better hypothesis the economy has never been better and so that's not entailed from one to the other right so",
    "start": "785360",
    "end": "791839"
  },
  {
    "text": "this is a classic sort of ai challenge um in nlp and what's interesting is that when",
    "start": "791839",
    "end": "798160"
  },
  {
    "text": "people constructed the very first benchmarks for this task there was a spurious correlate it turns",
    "start": "798160",
    "end": "804800"
  },
  {
    "text": "out that crowd workers when asked to write non-entailing sentences really like to put negations in right so this",
    "start": "804800",
    "end": "811600"
  },
  {
    "text": "is a reasonable human bias when you're asked to write something that contradicts an input you might just negate it right",
    "start": "811600",
    "end": "817600"
  },
  {
    "text": "but what this does is it introduces a really simple spurious correlate or a surface level feature that a model can",
    "start": "817600",
    "end": "824240"
  },
  {
    "text": "pick up and then exploit to solve the task so that's what happens here so we can sort of think about this in",
    "start": "824240",
    "end": "829839"
  },
  {
    "text": "this like two by two diagram on the bottom here so what we would like to do is to predict the label whether",
    "start": "829839",
    "end": "835040"
  },
  {
    "text": "something is an entailment or a contradiction but really it turns out that this thing is correlated with less spurious",
    "start": "835040",
    "end": "841440"
  },
  {
    "text": "attribute whether or not a sentence has a negation or not and the key thing is that because of the",
    "start": "841440",
    "end": "847279"
  },
  {
    "text": "dataset bias we only have or we mostly have data on the diagonal here entailment with no negation or",
    "start": "847279",
    "end": "853680"
  },
  {
    "text": "contradiction with negation because that's true it turns out to be the case that just",
    "start": "853680",
    "end": "859440"
  },
  {
    "text": "picking up on negation is a really effective strategy for predicting entailment right this is not what we",
    "start": "859440",
    "end": "865440"
  },
  {
    "text": "want the models to learn and this is the general principle i want you to remember from this first part of the talk this idea of spurious correlates and how it",
    "start": "865440",
    "end": "872240"
  },
  {
    "text": "can sort of deceive us into thinking a model has understood a task when in reality it has not at all right so here",
    "start": "872240",
    "end": "879600"
  },
  {
    "text": "in this case using the spurious attribute gives us low average error even though there are parts of the space",
    "start": "879600",
    "end": "884959"
  },
  {
    "text": "that we're doing really badly on any questions on this high level thing",
    "start": "884959",
    "end": "890079"
  },
  {
    "text": "okay so the problem really is the same thing",
    "start": "890079",
    "end": "895120"
  },
  {
    "text": "in this evaluation case what we would like to predict is something like in this summarization task do we have a",
    "start": "895120",
    "end": "900639"
  },
  {
    "text": "factual uh summary or do we have a non-factual summary and the spurious attribute in this case is overlap if",
    "start": "900639",
    "end": "906560"
  },
  {
    "text": "you're using a lot of words from the original article during summarization you're probably factual right so you",
    "start": "906560",
    "end": "912240"
  },
  {
    "text": "have this spurious correlate that's really easy for the model to pick up that gives you low average error",
    "start": "912240",
    "end": "918560"
  },
  {
    "text": "on the other hand we see that in the worst case when we're looking at examples of non-factual text with",
    "start": "918560",
    "end": "924399"
  },
  {
    "text": "overlap so you add like a negation or something to like intentionally break the factuality of your text or you're",
    "start": "924399",
    "end": "929600"
  },
  {
    "text": "looking at the bottom left a factual but very little overlap where you've paraphrased the summary in both of these",
    "start": "929600",
    "end": "935199"
  },
  {
    "text": "cases these kinds of evaluation systems will be expected to fail right so the way to think about this conceptually is",
    "start": "935199",
    "end": "942079"
  },
  {
    "text": "that a lot of the cases where we have spurious correlates our metrics are very very accurate in the easy case right",
    "start": "942079",
    "end": "948000"
  },
  {
    "text": "when the spurious correlates match the label but they fail sometimes catastrophically in the hard cases where",
    "start": "948000",
    "end": "954720"
  },
  {
    "text": "these heuristics that the model has learned are no longer relevant",
    "start": "954720",
    "end": "959839"
  },
  {
    "text": "okay so now i'll get into slightly more details of this and the point of this is maybe not to convince you that this is",
    "start": "960000",
    "end": "965279"
  },
  {
    "text": "you know a truly exciting area for natural language generation evaluation but i want to show you the scope of how deep this goes it",
    "start": "965279",
    "end": "972320"
  },
  {
    "text": "seems like this problem is really pervasive so we looked at a couple different metrics and we looked at a whole bunch of different benchmarks for",
    "start": "972320",
    "end": "978880"
  },
  {
    "text": "for dialogue and summarization and we really found the same phenomenon across all of them",
    "start": "978880",
    "end": "984880"
  },
  {
    "text": "so to just to remind you right these metrics that we picked uh mod dialogue rpt and usl these are supposed to",
    "start": "984880",
    "end": "991839"
  },
  {
    "text": "measure the quality of uh a chat bot system and its response right and the higher the better because it's more",
    "start": "991839",
    "end": "998639"
  },
  {
    "text": "correlated with humans and on the left side we've taken some very simple heuristics just looking at the length",
    "start": "998639",
    "end": "1004240"
  },
  {
    "text": "looking at the compressibility of the sentence called perplexity or a combination of the two and what we often",
    "start": "1004240",
    "end": "1009680"
  },
  {
    "text": "see is that these very simple heuristics if we like carefully figure out what the model is trying to learn",
    "start": "1009680",
    "end": "1015519"
  },
  {
    "text": "we can actually get them to do better than a lot of these learned models and that's disturbing and once again on the right we see another disturbing trend",
    "start": "1015519",
    "end": "1022160"
  },
  {
    "text": "which is that these automated evaluation metrics seems to correlate more with the heuristics like perplexity or length",
    "start": "1022160",
    "end": "1028240"
  },
  {
    "text": "than with humans so on the y-axis here is the correlation with our learned metrics and we see it's more correlated",
    "start": "1028240",
    "end": "1033839"
  },
  {
    "text": "with the spurious correlate than with humans so this is kind of the general pattern that we see these systems are",
    "start": "1033839",
    "end": "1039280"
  },
  {
    "text": "very high performance but when we put them in situations they haven't seen before they really seem to be relying",
    "start": "1039280",
    "end": "1044720"
  },
  {
    "text": "upon these heuristics we see the same thing on persona chat which is another dialogue evaluation",
    "start": "1044720",
    "end": "1050799"
  },
  {
    "text": "case you see that most of these learned evaluation metrics aren't doing much better although usl is doing a little",
    "start": "1050799",
    "end": "1056080"
  },
  {
    "text": "bit better and on the right side once again we see that the spurious correlates are more correlated with the learned metric than with humans",
    "start": "1056080",
    "end": "1064160"
  },
  {
    "text": "finally to drive the point home this is the very last one um daily dialogue is another dialogue evaluations that here",
    "start": "1064160",
    "end": "1070000"
  },
  {
    "text": "we're basically getting no correlation because daily dialogue somewhat has a different distribution than the data on",
    "start": "1070000",
    "end": "1075120"
  },
  {
    "text": "which these models are trained so in all these cases we see a very different story from the original",
    "start": "1075120",
    "end": "1080160"
  },
  {
    "text": "optimistic view that we had just because the system seems to be doing well on some sort of benchmark",
    "start": "1080160",
    "end": "1085679"
  },
  {
    "text": "performance numbers doesn't necessarily mean that it's going to do well out of distribution nor does it mean that it's",
    "start": "1085679",
    "end": "1091919"
  },
  {
    "text": "actually learned the task right we can't really trust uh average case performance numbers on face we really have to look",
    "start": "1091919",
    "end": "1098320"
  },
  {
    "text": "deep and look at what the system is learning and test it out of its distribution and so what we found here is that we",
    "start": "1098320",
    "end": "1104720"
  },
  {
    "text": "found at least three different spurious correlates in that they were very predictive of performance and that they",
    "start": "1104720",
    "end": "1109760"
  },
  {
    "text": "correlated really highly with these learned metrics right that was the thing that we found these three things and i think the key problem was sort of",
    "start": "1109760",
    "end": "1116640"
  },
  {
    "text": "the current evaluation paradigm is that we are testing models on the data on which they are trained not literally",
    "start": "1116640",
    "end": "1122080"
  },
  {
    "text": "train test overlap but we're taking an iid sampled set right but when we take these models and we evaluate them on",
    "start": "1122080",
    "end": "1128240"
  },
  {
    "text": "different data sets different kinds of dialogues to evaluate they seem to totally fall apart and they seem to rely",
    "start": "1128240",
    "end": "1134000"
  },
  {
    "text": "a lot more on their heuristics than on actually understanding the task and so the key point here",
    "start": "1134000",
    "end": "1139919"
  },
  {
    "text": "is that to make progress longer term on these kinds of hard problems we need something that's more of like a robust",
    "start": "1139919",
    "end": "1145679"
  },
  {
    "text": "evaluation metric right we need to take these systems intentionally out of their comfort zone and then test to see what",
    "start": "1145679",
    "end": "1150799"
  },
  {
    "text": "they've learned yes od performance right like it's not like",
    "start": "1150799",
    "end": "1156000"
  },
  {
    "text": "um a priori bad that it's worriously correlated or is it actually a prior event what do you mean by a priori",
    "start": "1156000",
    "end": "1164799"
  },
  {
    "text": "did not exist and using heuristics would be kind of fun right right so i guess the well it depends on what you what you",
    "start": "1165919",
    "end": "1172160"
  },
  {
    "text": "believe right so if you think that the original training data set is truly representative and it's the performance",
    "start": "1172160",
    "end": "1178320"
  },
  {
    "text": "you care about then you're right that like you know at least for that setting you do have a good system that relies on",
    "start": "1178320",
    "end": "1183679"
  },
  {
    "text": "heuristics i think in almost no situation is the training data sets we use in academia",
    "start": "1183679",
    "end": "1189200"
  },
  {
    "text": "actually reflective of the real world use cases or the real world deployment conditions that we expect they're they're an approximation",
    "start": "1189200",
    "end": "1195840"
  },
  {
    "text": "right hopefully it's a good approximation but but often it's not the case i think especially for evaluation this",
    "start": "1195840",
    "end": "1202320"
  },
  {
    "text": "is true because you're almost never going to be evaluating the same conditions because you're making a new model and evaluating that and that's",
    "start": "1202320",
    "end": "1208960"
  },
  {
    "text": "almost intentionally a distribution shift so this is a case where it's really hard to defend that it's going to",
    "start": "1208960",
    "end": "1214000"
  },
  {
    "text": "be iid but good question okay so",
    "start": "1214000",
    "end": "1219600"
  },
  {
    "text": "you know this is a little bit abstract i'm talking about nlg evaluation which not all of you are probably you know",
    "start": "1219600",
    "end": "1224640"
  },
  {
    "text": "really familiar with and i think one question you might ask because i've shown you a bunch of correlation numbers right like you know correlations can get",
    "start": "1224640",
    "end": "1230559"
  },
  {
    "text": "low but you might not really care about this you might say like why do i care if the correlation numbers are low right",
    "start": "1230559",
    "end": "1236320"
  },
  {
    "text": "really the the thing that i'm going to use this thing for is for developing models and maybe i can still pick the right model or the best models using",
    "start": "1236320",
    "end": "1242640"
  },
  {
    "text": "these evaluation metrics even if their correlations are low so i'm going to now demonstrate a direct harm that can",
    "start": "1242640",
    "end": "1248320"
  },
  {
    "text": "result from using these kinds of evaluation metrics and trusting them so i'm going to describe the setup in",
    "start": "1248320",
    "end": "1254080"
  },
  {
    "text": "one slide once again i'm going to talk about summarization so the the goal is to build a summarization system that can",
    "start": "1254080",
    "end": "1260480"
  },
  {
    "text": "take in an article and produce some sort of a summary and the interest in a lot of this field",
    "start": "1260480",
    "end": "1265840"
  },
  {
    "text": "is producing abstractive summary so you don't want to just take a couple of important sentences you want to like paraphrase it and synthesize it into a",
    "start": "1265840",
    "end": "1272320"
  },
  {
    "text": "useful summary that's the task that we're interested in and factuality is a huge problem for",
    "start": "1272320",
    "end": "1277919"
  },
  {
    "text": "summarization so we want systems that do not lie to their users when they're performing summarization so we're going",
    "start": "1277919",
    "end": "1283840"
  },
  {
    "text": "to look at a big evaluation data set that some people from salesforce produced that evaluated 16 different",
    "start": "1283840",
    "end": "1289440"
  },
  {
    "text": "systems and did fine-grained human evaluations for factuality this allows us to see whether the rankings produced",
    "start": "1289440",
    "end": "1295840"
  },
  {
    "text": "by these automated evaluations match those produced by humans right and really what we can show is actual direct",
    "start": "1295840",
    "end": "1302400"
  },
  {
    "text": "harms from this kind of framework and i think one thing that's really interesting",
    "start": "1302400",
    "end": "1307840"
  },
  {
    "text": "is people have said something like actually these kinds of automated evaluations are really good at",
    "start": "1307840",
    "end": "1313919"
  },
  {
    "text": "distinguishing which models are good or bad but once again there's kind of a spurious correlate i've been talking about this like word overlap thing and",
    "start": "1313919",
    "end": "1320720"
  },
  {
    "text": "that's really reflective people find that things with high word overlap are generally more factual that's just kind of true right",
    "start": "1320720",
    "end": "1326799"
  },
  {
    "text": "but really what we would like to be able to distinguish is the top left corner if we have a system that doesn't have very",
    "start": "1326799",
    "end": "1332400"
  },
  {
    "text": "high word overlap can we distinguish how factual it is and the answer is no so this is a little",
    "start": "1332400",
    "end": "1338400"
  },
  {
    "text": "story that i want to tell there's three different systems on this slide and they come from kind of three different generations or three different time",
    "start": "1338400",
    "end": "1344080"
  },
  {
    "text": "periods pointer generator is one of the very first like neural summarization systems developed here at stanford",
    "start": "1344080",
    "end": "1349760"
  },
  {
    "text": "pegasus and bart are much more modern pre-training based models and here we see the very top row human",
    "start": "1349760",
    "end": "1356400"
  },
  {
    "text": "evaluation slowly and steadily improves over time this is just models getting better right but what we find is a very",
    "start": "1356400",
    "end": "1362159"
  },
  {
    "text": "different story for the automated evaluations if we look at fact cc or dae these like neurally based evaluation",
    "start": "1362159",
    "end": "1367679"
  },
  {
    "text": "models on the bottom what we find is that these same systems say that the oldest model is the best one and that's",
    "start": "1367679",
    "end": "1374400"
  },
  {
    "text": "because this oldest model the point pointer generator did the least amount of rephrasing it just sort of copied out",
    "start": "1374400",
    "end": "1379600"
  },
  {
    "text": "parts of the article because this these systems rely on the spurious correlates they're actively",
    "start": "1379600",
    "end": "1384720"
  },
  {
    "text": "saying these newest best models are actually not the ones that we want right and so the direct harm here is that if",
    "start": "1384720",
    "end": "1390559"
  },
  {
    "text": "we were to use these systems to try to rank the factuality of our models we would actually find that you know",
    "start": "1390559",
    "end": "1396080"
  },
  {
    "text": "these oldest models are the best and we would be harming sort of progress right so these are kind of demonstrations of direct harms from this kind of spurious",
    "start": "1396080",
    "end": "1402799"
  },
  {
    "text": "correlation that we see um in the interest of time i'm going to skip over some of the some of the",
    "start": "1402799",
    "end": "1408400"
  },
  {
    "text": "details but the point i will get to here is one thing that we can do is not just",
    "start": "1408400",
    "end": "1413919"
  },
  {
    "text": "rely on you know the progress of larger models to make our systems better if we think a little bit more carefully about",
    "start": "1413919",
    "end": "1419520"
  },
  {
    "text": "what's happening we know that there's some spurious correlates like word overlap or the length of a sentence",
    "start": "1419520",
    "end": "1426000"
  },
  {
    "text": "and what we would like to do is we would like to minimize the model's use of those spurious correlates so we can",
    "start": "1426000",
    "end": "1431919"
  },
  {
    "text": "design an architecture that does exactly that taken from domain adaptation and if we actually do this what we find is we",
    "start": "1431919",
    "end": "1438720"
  },
  {
    "text": "will get dramatic improvements in the predictive power of these kinds of",
    "start": "1438720",
    "end": "1444080"
  },
  {
    "text": "automated evaluations on the set of models that we care about things that do abstractive factual summaries i'm going",
    "start": "1444080",
    "end": "1450960"
  },
  {
    "text": "through this a little bit fast but the point here is to say that by explicitly trying to prevent the model from using",
    "start": "1450960",
    "end": "1456880"
  },
  {
    "text": "these kinds of spurious correlates we can then get uh improvements in terms of",
    "start": "1456880",
    "end": "1462240"
  },
  {
    "text": "the actual metrics and rankings of models that we care about the final punch line here is that i was",
    "start": "1462240",
    "end": "1469039"
  },
  {
    "text": "only talking about robustness here right like i wanted to improve so the worst case performance in some sense of these models and make them use more reliable",
    "start": "1469039",
    "end": "1476000"
  },
  {
    "text": "features but when we did that and we tried to enforce robustness we didn't really pay a cost in the average case",
    "start": "1476000",
    "end": "1481120"
  },
  {
    "text": "accuracy either um here the the error bars are large enough so i'm not going to say that the adversarial robust model",
    "start": "1481120",
    "end": "1487840"
  },
  {
    "text": "is going to be better than the others but it's suggestive evidence that actually enforcing robustness could",
    "start": "1487840",
    "end": "1493200"
  },
  {
    "text": "maybe lead to general improvements in the performance of our system so i think that's kind of a hopeful note to try to",
    "start": "1493200",
    "end": "1498559"
  },
  {
    "text": "end on here okay so so we went sort of into the low level",
    "start": "1498559",
    "end": "1503679"
  },
  {
    "text": "here talking about a lot of results and a lot of like failures specific failures of language models so i want to pop back",
    "start": "1503679",
    "end": "1509360"
  },
  {
    "text": "up and get to the bigger picture again and talk about that so language models are increasingly",
    "start": "1509360",
    "end": "1515200"
  },
  {
    "text": "being used in high stakes areas data generation evaluation and i think it's good that we are being optimistic and",
    "start": "1515200",
    "end": "1521120"
  },
  {
    "text": "trying to push the boundaries at the same time just because they seem to do well on some benchmark numbers and",
    "start": "1521120",
    "end": "1526720"
  },
  {
    "text": "some evaluations we can concoct doesn't mean that they're actually good we need to take a very skeptical view of how",
    "start": "1526720",
    "end": "1532559"
  },
  {
    "text": "good these systems actually are and so i think that sort of requires us to look at the robustness of these",
    "start": "1532559",
    "end": "1539039"
  },
  {
    "text": "systems to rule out whether or not they're using these kinds of simple heuristics or",
    "start": "1539039",
    "end": "1544320"
  },
  {
    "text": "whether they're actually learning the underlying task and the final thing which is specific to our work is there are ways of removing",
    "start": "1544320",
    "end": "1551039"
  },
  {
    "text": "these kinds of things and explicitly building those in rather than relying on sort of scale and you know more data to",
    "start": "1551039",
    "end": "1556559"
  },
  {
    "text": "solve these problems is a very effective direct approach to addressing some of these issues",
    "start": "1556559",
    "end": "1561919"
  },
  {
    "text": "okay i'm going to stop here in case anyone has questions about the first part i'm going to totally switch gears and talk about privacy next so i want to",
    "start": "1561919",
    "end": "1569200"
  },
  {
    "text": "make sure that i i answer any questions that anyone may have about this this first segment yes uh so when you like um",
    "start": "1569200",
    "end": "1574960"
  },
  {
    "text": "sort of talked about how you resolved um or tried to tackle this specific issue um is that largely through like um data",
    "start": "1574960",
    "end": "1582080"
  },
  {
    "text": "tuning or is it actually it's actually i would say more from the loss",
    "start": "1582080",
    "end": "1587440"
  },
  {
    "text": "um i skipped over this because i didn't know how deeply i wanted to go into this but you basically make a neural",
    "start": "1587440",
    "end": "1592640"
  },
  {
    "text": "architecture that has two parts the top part is the same as your normal prediction task so you're just trying to predict whether a system is let's say",
    "start": "1592640",
    "end": "1598640"
  },
  {
    "text": "good or factual or not that's the very top half of this architecture the bottom half is is an adversarial component that",
    "start": "1598640",
    "end": "1604799"
  },
  {
    "text": "basically says if i can predict my spurious correlate like the word overlap then that's bad because it means that",
    "start": "1604799",
    "end": "1610320"
  },
  {
    "text": "the model could be using that feature so i have this adversarial head on the bottom that prevents my model from",
    "start": "1610320",
    "end": "1615679"
  },
  {
    "text": "making use of that it makes sure that there's no such information in the model this is all very hand wavy but i think",
    "start": "1615679",
    "end": "1621200"
  },
  {
    "text": "that's a little bit better than digging through the math of the main adversarial neural networks yes",
    "start": "1621200",
    "end": "1628080"
  },
  {
    "text": "yes yeah in this case it's word overlap which we know is extremely strong spurious correlate",
    "start": "1628480",
    "end": "1633919"
  },
  {
    "text": "yes if the correlation level is higher than",
    "start": "1633919",
    "end": "1640480"
  },
  {
    "text": "what it would be with the humanoid okay or i guess that the way i would formally define it is a spurious correlation",
    "start": "1640480",
    "end": "1646000"
  },
  {
    "text": "is a is a predictive correlation that goes away under some distribution shift",
    "start": "1646000",
    "end": "1651039"
  },
  {
    "text": "okay um and as for like identifying what those are yes and you know about the",
    "start": "1651039",
    "end": "1656080"
  },
  {
    "text": "word you mentioned about the negation right do we have a sense of how many other",
    "start": "1656080",
    "end": "1663279"
  },
  {
    "text": "stories yeah that's a good question i don't think we have a like a comprehensive inventory i do think perplexity has",
    "start": "1663279",
    "end": "1670640"
  },
  {
    "text": "often been a perplexity or like simplicity of a sentence as measured by some language model is is a really",
    "start": "1670640",
    "end": "1676000"
  },
  {
    "text": "strong correlate to almost everything right like if you build a toxicity classifier for example you'll find that",
    "start": "1676000",
    "end": "1681440"
  },
  {
    "text": "like toxicity is very unfortunately correlated with with fluency like as measured by perplexity so we have some",
    "start": "1681440",
    "end": "1687919"
  },
  {
    "text": "sense of like things people have talked about but i think the full universe of serious correlates is completely unknown",
    "start": "1687919",
    "end": "1694559"
  },
  {
    "text": "okay cool this is a good good timing so so now i want to kind of almost completely switch gears i want to talk",
    "start": "1695760",
    "end": "1701520"
  },
  {
    "text": "about privacy um so the first part i think was partially a fully depressing story it was like well there's some",
    "start": "1701520",
    "end": "1707840"
  },
  {
    "text": "great things but also there are some like really big pitfalls the second part is more more mixed and some of this",
    "start": "1707840",
    "end": "1713360"
  },
  {
    "text": "content is taken from um my class on on large language models with percy in last quarter",
    "start": "1713360",
    "end": "1719039"
  },
  {
    "text": "so now let's talk about privacy and privacy risks like why is there sort of this",
    "start": "1719039",
    "end": "1724399"
  },
  {
    "text": "big issue about privacy that people talk about for language modeling so one of the reasons is because",
    "start": "1724399",
    "end": "1731440"
  },
  {
    "text": "models are extremely data-hungry i think this is the thing that sets the stage shown here um this is not a language",
    "start": "1731440",
    "end": "1738159"
  },
  {
    "text": "modeling task this is i believe for a machine translation task but what you see here is on the x-axis",
    "start": "1738159",
    "end": "1744559"
  },
  {
    "text": "the training data set size in log scale that's important and on the y-axis is kind of an error measure like how many",
    "start": "1744559",
    "end": "1750880"
  },
  {
    "text": "words we got wrong when we're performing translation and that's on the y-axis um also logged",
    "start": "1750880",
    "end": "1756960"
  },
  {
    "text": "and what you see here is this fairly nice line on a loglog scale right and what that means is if we you know",
    "start": "1756960",
    "end": "1763760"
  },
  {
    "text": "double the amount of data set we have then we'll get some multiplicative decrease in our error rate",
    "start": "1763760",
    "end": "1769360"
  },
  {
    "text": "but this requires and necessitates that we continually increase our data set size by some multiplicative factor if we",
    "start": "1769360",
    "end": "1775679"
  },
  {
    "text": "want to continually decrease our error rates right so that's actually that's kind of tough",
    "start": "1775679",
    "end": "1781760"
  },
  {
    "text": "right it requires ever larger data set sizes in order to get ever smaller errors",
    "start": "1781760",
    "end": "1786799"
  },
  {
    "text": "um and the issue here is that there's kind of a trade-off even right the amount the",
    "start": "1786799",
    "end": "1792799"
  },
  {
    "text": "kinds of data that we can get our hands on is limited on the left we could go to the internet we can scrape the internet",
    "start": "1792799",
    "end": "1798880"
  },
  {
    "text": "and produce enormous data sets right the largest open domain chatbots are generated by looking at um uh",
    "start": "1798880",
    "end": "1805919"
  },
  {
    "text": "conversations on reddit right there's a lot of those available right but they're fairly low quality that's subjective",
    "start": "1805919",
    "end": "1812399"
  },
  {
    "text": "judgment um on the right side we could have really motivated annotators writing",
    "start": "1812399",
    "end": "1818799"
  },
  {
    "text": "down really really well thought out careful conversations right but those are very expensive if you're being a",
    "start": "1818799",
    "end": "1824399"
  },
  {
    "text": "good person you're paying these people you know a really decent wage right and how many samples can you collect maybe",
    "start": "1824399",
    "end": "1830159"
  },
  {
    "text": "10 000 conversations that's tiny by neural network training standards right",
    "start": "1830159",
    "end": "1835200"
  },
  {
    "text": "and so there's a forbidden third path that i think people have started to think a lot about which is can we",
    "start": "1835200",
    "end": "1841200"
  },
  {
    "text": "utilize things like private user data or even like slightly sketchy copyright protected data when we're training",
    "start": "1841200",
    "end": "1846240"
  },
  {
    "text": "language models right there's an ever greater pressure to increase the scope of the data that we collect and use and",
    "start": "1846240",
    "end": "1852720"
  },
  {
    "text": "this is just kind of a natural dynamic from the fact that our models are so data-hungry",
    "start": "1852720",
    "end": "1858159"
  },
  {
    "text": "and i want to talk about a really sad example of this um so this is an article that i found while",
    "start": "1858159",
    "end": "1864000"
  },
  {
    "text": "doing my research earlier this year uh talking about a south korean startup which owned a",
    "start": "1864000",
    "end": "1870399"
  },
  {
    "text": "dating app and the the dating app was you know used to share sort of intimate",
    "start": "1870399",
    "end": "1876399"
  },
  {
    "text": "romantic conversations between people um they have this data so they train the neural chatbot based on the dating app",
    "start": "1876399",
    "end": "1883279"
  },
  {
    "text": "information and then they released this neural chat bot to the while hopefully you all recognize that this is",
    "start": "1883279",
    "end": "1889760"
  },
  {
    "text": "a truly terrible terrible idea and people interacting with this chatbot",
    "start": "1889760",
    "end": "1894799"
  },
  {
    "text": "actually found that they can they could get the system to like emit very intimate details about",
    "start": "1894799",
    "end": "1900640"
  },
  {
    "text": "people's lives back to them um and so this is like a truly terrible story",
    "start": "1900640",
    "end": "1906240"
  },
  {
    "text": "right i was hoping it was the april fool's joke but it's april 2nd and not first and it's actually a real story",
    "start": "1906240",
    "end": "1911679"
  },
  {
    "text": "and there's going to be more stories like this right as there's a increasing pressure to build complex",
    "start": "1911679",
    "end": "1918559"
  },
  {
    "text": "interactions and complex systems like chat bots people will want to leverage these kinds of data",
    "start": "1918559",
    "end": "1925039"
  },
  {
    "text": "and so i think there's going to be an increasing privacy risk that we need to grapple",
    "start": "1925039",
    "end": "1930559"
  },
  {
    "text": "with when we talk about language models i think a pretty common thing that people say is you know either users have",
    "start": "1930559",
    "end": "1936559"
  },
  {
    "text": "consented to the release of the data or we're scraping public pieces or semi-public pieces of information like say reddit conversations",
    "start": "1936559",
    "end": "1943200"
  },
  {
    "text": "um so there's really no privacy harm i think this is a common thing that i hear a lot of machine learning people say",
    "start": "1943200",
    "end": "1949519"
  },
  {
    "text": "but i do want to stress and this is not a purely machine learning topic but an important one that privacy harms do not",
    "start": "1949519",
    "end": "1956640"
  },
  {
    "text": "just come from the release of truly private data there's a really nice survey by salov in",
    "start": "1956640",
    "end": "1962799"
  },
  {
    "text": "2006 called a taxonomy of privacy in which he goes through all the different ways in which you can harm somebody's",
    "start": "1962799",
    "end": "1968720"
  },
  {
    "text": "privacy without really actually releasing their private information actually he will also talk about private information but",
    "start": "1968720",
    "end": "1975039"
  },
  {
    "text": "also other kinds of privacy harms that you may not have thought about before um",
    "start": "1975039",
    "end": "1980880"
  },
  {
    "text": "and i think the important component here is that there's many different ways to harm privacy and there's a whole pipeline of privacy harms right you",
    "start": "1980880",
    "end": "1987039"
  },
  {
    "text": "could invade somebody's home and that's obviously a privacy violation you could collect information like watch",
    "start": "1987039",
    "end": "1992480"
  },
  {
    "text": "them with a camera that's a privacy violation but actually the thing that's most relevant to us as computer",
    "start": "1992480",
    "end": "1997760"
  },
  {
    "text": "scientists is the information processing component right there are privacy harms that arise from taking lots of different",
    "start": "1997760",
    "end": "2004720"
  },
  {
    "text": "data scattered all over the place combining them into one centralized source and that's what language models",
    "start": "2004720",
    "end": "2009840"
  },
  {
    "text": "and models do right they aggregate pieces of data and they form inferences and generalization and of course disseminating that information can",
    "start": "2009840",
    "end": "2016240"
  },
  {
    "text": "obviously have privacy harms right so i want to talk about sort of information processing in particular um",
    "start": "2016240",
    "end": "2021679"
  },
  {
    "text": "before i talk about language models and like their specific harms and so on right because this is an important",
    "start": "2021679",
    "end": "2026720"
  },
  {
    "text": "higher level point that i want to stress so when we aggregate information this is",
    "start": "2026720",
    "end": "2032159"
  },
  {
    "text": "the act of taking different kinds of public information and combining it together into some other new piece of",
    "start": "2032159",
    "end": "2038399"
  },
  {
    "text": "information and really you can't avoid aggregation when you're training models right because the whole point of a model",
    "start": "2038399",
    "end": "2044960"
  },
  {
    "text": "is to take information that was scattered in documents across the internet or whatever and to bring them",
    "start": "2044960",
    "end": "2050158"
  },
  {
    "text": "together into a centralized place this model right and the other thing is accessibility",
    "start": "2050159",
    "end": "2055280"
  },
  {
    "text": "right when we release a model we inherently make it more accessible originally right to get access to these",
    "start": "2055280",
    "end": "2061040"
  },
  {
    "text": "pieces of information you have to go crawl through the internet find all these different pieces of information and find the relevant things right you",
    "start": "2061040",
    "end": "2067358"
  },
  {
    "text": "have to do a lot of research right but now there's a single central repository this big language model that you've",
    "start": "2067359",
    "end": "2073520"
  },
  {
    "text": "trained that contains all that information in one place right it's like kind of a google of private information",
    "start": "2073520",
    "end": "2078638"
  },
  {
    "text": "and you can go look everything up so you could think about a lot of different harms that arise from these two",
    "start": "2078639",
    "end": "2084720"
  },
  {
    "text": "processes and i want to outline a couple to motivate what will follow in the rest of this talk",
    "start": "2084720",
    "end": "2090320"
  },
  {
    "text": "so the first thing is that aggregation can can violate expected privacy so the example i like to give for this is let's",
    "start": "2090320",
    "end": "2096638"
  },
  {
    "text": "say we have a system that can build a synthetic biography right so maybe some of you have seen these like weird kind of very sketchy websites",
    "start": "2096639",
    "end": "2104160"
  },
  {
    "text": "that will find public information about you like your address your gender where you work and so on and they'll like make",
    "start": "2104160",
    "end": "2109520"
  },
  {
    "text": "a website that's like a kind of a synthetic biography of all the things you know and",
    "start": "2109520",
    "end": "2115119"
  },
  {
    "text": "they know a lot of different things it's kind of shocking if you go to one of these websites they'll maybe even be able to do things like in for your",
    "start": "2115119",
    "end": "2120880"
  },
  {
    "text": "income right now imagine a model doing this at scale you might type in like tatsu hashimoto's",
    "start": "2120880",
    "end": "2125920"
  },
  {
    "text": "biography and they'll be able to like spit out sort of all the inferences i can make right you know my my uh",
    "start": "2125920",
    "end": "2131599"
  },
  {
    "text": "leanings politically and so on and so forth that's definitely something that feels like a privacy harm right because",
    "start": "2131599",
    "end": "2137599"
  },
  {
    "text": "i definitely didn't consent to having a system build the synthetic biography with my",
    "start": "2137599",
    "end": "2142960"
  },
  {
    "text": "some intimate details of my life in there other things that can happen is inferences right um even accurate",
    "start": "2142960",
    "end": "2149520"
  },
  {
    "text": "inferences could be harmful and one way i think to sort of talk about this is like let's say i have a whole bunch of",
    "start": "2149520",
    "end": "2154640"
  },
  {
    "text": "writing published on the internet through blog posts right and then i asked gpt2 or some other language model",
    "start": "2154640",
    "end": "2160640"
  },
  {
    "text": "you know here's the writings of somebody like what is their sexual orientation right building that system can itself be a",
    "start": "2160640",
    "end": "2167200"
  },
  {
    "text": "privacy harm right in the sense that you're trying to infer information that is private like technically speaking",
    "start": "2167200",
    "end": "2173040"
  },
  {
    "text": "that is a piece of information you could have gone from the public text that existed",
    "start": "2173040",
    "end": "2178079"
  },
  {
    "text": "but this is definitely new pieces of information that's much more accessible to somebody",
    "start": "2178079",
    "end": "2184079"
  },
  {
    "text": "and finally accessibility can harm expectations of privacy right if i have a website that's not scraped by",
    "start": "2184079",
    "end": "2190160"
  },
  {
    "text": "googlebot it's technically speaking public but i have an expectation of privacy and so the boundaries of what",
    "start": "2190160",
    "end": "2195920"
  },
  {
    "text": "constitutes private information is honestly a little bit pretty loose um so for all of these reasons language",
    "start": "2195920",
    "end": "2203119"
  },
  {
    "text": "models represent a new sort of frontier of privacy harms and like safety violations for people um",
    "start": "2203119",
    "end": "2209839"
  },
  {
    "text": "any questions i feel like sometimes this is controversial uh with people who who especially want to stick to like",
    "start": "2209839",
    "end": "2216079"
  },
  {
    "text": "i guess a question i had was so is this how is this different from let's say me",
    "start": "2216079",
    "end": "2221599"
  },
  {
    "text": "like taking your example like googling about tatsu hashimoto and saying okay",
    "start": "2221599",
    "end": "2226720"
  },
  {
    "text": "you know what based on this i think this is their income this is their political leanings and then creating a blog post",
    "start": "2226720",
    "end": "2231920"
  },
  {
    "text": "about it is it just a matter of scale right or is there something else right that makes this",
    "start": "2231920",
    "end": "2238160"
  },
  {
    "text": "worse than maybe just meet with india yeah so i think the question was whether",
    "start": "2238160",
    "end": "2243839"
  },
  {
    "text": "um there's a difference between you googling things and like a centralized system that that does this i think the key thing is",
    "start": "2243839",
    "end": "2251760"
  },
  {
    "text": "everything is sort of continuous in the sense that in order for you to find everything about me it will take",
    "start": "2251760",
    "end": "2256800"
  },
  {
    "text": "substantial amounts of effort right substantial amounts of searching effort and so on and so forth and decreasing",
    "start": "2256800",
    "end": "2262079"
  },
  {
    "text": "the barrier the cost barrier to having all this information available to you immediately",
    "start": "2262079",
    "end": "2268000"
  },
  {
    "text": "um that's a qualitative difference because whether or not it's going to take you a month versus a day will",
    "start": "2268000",
    "end": "2273200"
  },
  {
    "text": "really change whether or not you actually act upon it right and this is a kind of a good segue",
    "start": "2273200",
    "end": "2278320"
  },
  {
    "text": "because the courts actually have thought about this and they come to essentially the same conclusion",
    "start": "2278320",
    "end": "2283359"
  },
  {
    "text": "um as me which is uh so doj versus reporters com for free presses is the",
    "start": "2283359",
    "end": "2289359"
  },
  {
    "text": "case that that is relevant for this and sort of to give you the background i believe this was a case in which uh some",
    "start": "2289359",
    "end": "2295359"
  },
  {
    "text": "reporters wanted access to fbi rap sheets and rap sheets are the",
    "start": "2295359",
    "end": "2300400"
  },
  {
    "text": "centralized piece of information for like all the crimes that someone has committed and so",
    "start": "2300400",
    "end": "2306160"
  },
  {
    "text": "on and so forth aggregated from all over the country and the reporters argued that rap sheets",
    "start": "2306160",
    "end": "2312720"
  },
  {
    "text": "were public information because all the little pieces that went into it like you know whether or not you were convicted",
    "start": "2312720",
    "end": "2317760"
  },
  {
    "text": "in like a small town that is public information like you could go to the court records you could look it up and you could find out right",
    "start": "2317760",
    "end": "2323920"
  },
  {
    "text": "so that was their argument and and the doj argued that it was not public",
    "start": "2323920",
    "end": "2329280"
  },
  {
    "text": "because you would have to spend substantial effort to look it up and that was sort of the the same uh logic",
    "start": "2329280",
    "end": "2334320"
  },
  {
    "text": "that was uh reached by sort of the justices on the bottom here the issue is whether or not the compilation of hard",
    "start": "2334320",
    "end": "2339440"
  },
  {
    "text": "to obtain information um alters the privacy interest and the nature of the privacy guarantees and the argument is",
    "start": "2339440",
    "end": "2345599"
  },
  {
    "text": "that if you have a whole bunch of information scattered across courts and you have to go look them up yourself that's kind of a different mode of",
    "start": "2345599",
    "end": "2351599"
  },
  {
    "text": "privacy expectation than if it's like directly available to you and hand it to you um",
    "start": "2351599",
    "end": "2357119"
  },
  {
    "text": "and also i think the other thing about accessibility that they also noted is important right so you know in",
    "start": "2357119",
    "end": "2362400"
  },
  {
    "text": "conversations that we have and you know in semi-private disclosures you know most pieces of information are disclosed",
    "start": "2362400",
    "end": "2368560"
  },
  {
    "text": "to somebody at some time right so the notion of like privacy is is a very shifting quantity like at least like",
    "start": "2368560",
    "end": "2374400"
  },
  {
    "text": "things i put on the internet is a very binary quantity but there's a lot of pieces of information that i share that i don't expect to be disseminated",
    "start": "2374400",
    "end": "2380640"
  },
  {
    "text": "there's an expectation of privacy component to all of this so changing the accessibility of of private or",
    "start": "2380640",
    "end": "2386160"
  },
  {
    "text": "non-private information is itself they argue uh a privacy violation so i think",
    "start": "2386160",
    "end": "2392079"
  },
  {
    "text": "this is kind of important especially the legal aspect i think is useful to think about because that is in some sense the kinds of things you'll have to follow",
    "start": "2392079",
    "end": "2398320"
  },
  {
    "text": "when you're making these systems if you're ever involved in making one of these systems okay",
    "start": "2398320",
    "end": "2404640"
  },
  {
    "text": "so i spent two slides talking about sort of the legal and ethical aspects of this",
    "start": "2404640",
    "end": "2409760"
  },
  {
    "text": "and hopefully that is of interest to you but now we'll return to questions of more technical questions about machine learning",
    "start": "2409760",
    "end": "2415520"
  },
  {
    "text": "for a few slides i want to talk about harms that other people have discovered this is not my work",
    "start": "2415520",
    "end": "2420720"
  },
  {
    "text": "extracting training data from language models some nice work by carlini shows that if you have a language model like",
    "start": "2420720",
    "end": "2426000"
  },
  {
    "text": "gpt2 trained on the internet you can very easily extract parts of the training",
    "start": "2426000",
    "end": "2431760"
  },
  {
    "text": "data and so the example figure you should have in mind is something like if you put in as an input to this model you",
    "start": "2431760",
    "end": "2437680"
  },
  {
    "text": "know east strasbourg whatever then the model will spit out some sort of address that was originally contained in the",
    "start": "2437680",
    "end": "2443280"
  },
  {
    "text": "training data and this is exactly the kind of like potential privacy harms that you could imagine",
    "start": "2443280",
    "end": "2449119"
  },
  {
    "text": "and one thing that they found which i found to be really interesting given the trends in the field is that the larger",
    "start": "2449119",
    "end": "2455920"
  },
  {
    "text": "the model the stronger its tendency to memorize and that's actually kind of an interesting observation so this table",
    "start": "2455920",
    "end": "2462880"
  },
  {
    "text": "taken from that paper what they did was they found a bunch of reddit links that was contained in the data set and",
    "start": "2462880",
    "end": "2469839"
  },
  {
    "text": "it was contained in a very specific place it was like some paste bin piece of text that was contained and so they",
    "start": "2469839",
    "end": "2474880"
  },
  {
    "text": "can exactly track the number of times this very specific string appeared in the training data and so as you go to",
    "start": "2474880",
    "end": "2481280"
  },
  {
    "text": "the top it's more and more common so it appeared more times in the training data and as you go to the bottom it didn't appear very often",
    "start": "2481280",
    "end": "2487359"
  },
  {
    "text": "and on the sort of different columns the memorized column on the right it's showing which models happen to memorize",
    "start": "2487359",
    "end": "2492960"
  },
  {
    "text": "this piece of text and they have a technical definition of memorize that i won't get into but you can just kind of think of it in your informal way",
    "start": "2492960",
    "end": "2499200"
  },
  {
    "text": "now what we find is that this excel model on the left the biggest model memorizes way more stuff than the others",
    "start": "2499200",
    "end": "2505200"
  },
  {
    "text": "and in fact like the more frequent the the piece of text the more memorized it is so what this shows is like if a piece of",
    "start": "2505200",
    "end": "2511040"
  },
  {
    "text": "text appears maybe 30 times in the training data set the gpt 2 xl one of the larger models at the",
    "start": "2511040",
    "end": "2518000"
  },
  {
    "text": "time will happily memorize that piece of text whereas if you have a much smaller model it won't memorize and this is an",
    "start": "2518000",
    "end": "2523920"
  },
  {
    "text": "important piece because the trend of the field is to go to ever larger models to improve performance so this problem is",
    "start": "2523920",
    "end": "2529599"
  },
  {
    "text": "only going to get worse and there's arguments to be made by fairly smart people with nice evidence",
    "start": "2529599",
    "end": "2536240"
  },
  {
    "text": "that memorization is a fundamental property of neural networks and in fact with our current training",
    "start": "2536240",
    "end": "2542560"
  },
  {
    "text": "paradigm it is unavoidable so shown in this plot is the performance of a language model",
    "start": "2542560",
    "end": "2548160"
  },
  {
    "text": "and on the x-axis is how long we train for and there's two different y-axis the blue y-axis is how much we've memorized",
    "start": "2548160",
    "end": "2555280"
  },
  {
    "text": "that's a technical term but you can sort of think about it colloquially on the right side is how well our model fits",
    "start": "2555280",
    "end": "2561119"
  },
  {
    "text": "both the training and the test data and so what you see let's start with the red lines right as you train for longer",
    "start": "2561119",
    "end": "2567200"
  },
  {
    "text": "training data goes down and test data goes down to a point and it comes back up right because it starts over fitting",
    "start": "2567200",
    "end": "2572960"
  },
  {
    "text": "now the blue line the amount we memorize goes up as we train more because we see",
    "start": "2572960",
    "end": "2578000"
  },
  {
    "text": "data more and more times the key thing to notice here is the point at which our test and train losses are low is also",
    "start": "2578000",
    "end": "2584880"
  },
  {
    "text": "the point at which memorization has peaked right and so people look at these kinds of plots and they say it may be the case",
    "start": "2584880",
    "end": "2592160"
  },
  {
    "text": "that our current paradigm of using large neural networks and public data means that we will inevitably get",
    "start": "2592160",
    "end": "2598079"
  },
  {
    "text": "memorization right this is a thing that we will have to live with in some ways and so this kind of sets the stage for",
    "start": "2598079",
    "end": "2604000"
  },
  {
    "text": "for the harms that that might appear um and so to sort of wrap up this sort of",
    "start": "2604000",
    "end": "2610560"
  },
  {
    "text": "like summary of harm's kind of view right so large language models have really driven large-scale public data",
    "start": "2610560",
    "end": "2616880"
  },
  {
    "text": "collection because we know that more data and bigger models are going to help us they're going to improve the performance of our systems",
    "start": "2616880",
    "end": "2624160"
  },
  {
    "text": "at the same time we know that they're going to lead to further memorization and that could be a form of privacy harm",
    "start": "2624160",
    "end": "2630160"
  },
  {
    "text": "right and so using the current approaches that we have now it seems like this is inevitable models seem to",
    "start": "2630160",
    "end": "2636319"
  },
  {
    "text": "prefer to memorize data and the highest performance largest models that we have will naturally memorize data",
    "start": "2636319",
    "end": "2642400"
  },
  {
    "text": "and so how can we get away from this problem right like now we sort of need to try to fix this problem and now i'm",
    "start": "2642400",
    "end": "2647839"
  },
  {
    "text": "going to talk about that so one thing that you might begin with and think about that i want to sort of",
    "start": "2647839",
    "end": "2653599"
  },
  {
    "text": "prevent you from doing maybe is to think about simple privatization schemes right just look at some names in the training",
    "start": "2653599",
    "end": "2659040"
  },
  {
    "text": "data mask them out look find addresses and mask them out um the problem is that privacy is kind of a high-stakes thing",
    "start": "2659040",
    "end": "2665760"
  },
  {
    "text": "in some ways if you mess up and you release a model it's kind of there forever and even smart people will make",
    "start": "2665760",
    "end": "2671680"
  },
  {
    "text": "mistakes uh this example on the left is some very smart uh researchers coming up with a way to scramble and privatize",
    "start": "2671680",
    "end": "2679119"
  },
  {
    "text": "data called instahide um and it was you know made public on on february 21st",
    "start": "2679119",
    "end": "2685599"
  },
  {
    "text": "last year uh or february last year and then it was broken just two months later",
    "start": "2685599",
    "end": "2690960"
  },
  {
    "text": "by some security researchers that showed that you could almost exactly recover the original pieces of text based on",
    "start": "2690960",
    "end": "2696880"
  },
  {
    "text": "like training information and so even really smart people can make mistakes this is not to say that this insta hype",
    "start": "2696880",
    "end": "2702480"
  },
  {
    "text": "thing was a bad idea this is to say privacy is hard when we don't have provable guarantees",
    "start": "2702480",
    "end": "2707520"
  },
  {
    "text": "and so what we need is some stronger guarantee that we're not going to leak user data that we care about",
    "start": "2707520",
    "end": "2713680"
  },
  {
    "text": "in the gold standard method the thing that has stood the test of time is differential privacy some of you may",
    "start": "2713680",
    "end": "2719440"
  },
  {
    "text": "already know about this but i'm going to give you the the kids version of differential privacy differential privacy is a fairly simple guarantee it",
    "start": "2719440",
    "end": "2726480"
  },
  {
    "text": "says i have a bunch of users which people call records in the differential privacy literature so i have alice bob",
    "start": "2726480",
    "end": "2732560"
  },
  {
    "text": "xavier donna ernie and my guarantee is the following if i train a model using everybody",
    "start": "2732560",
    "end": "2738480"
  },
  {
    "text": "i will get this blue distribution over a model so my predictor has to be randomized i have a randomized estimator",
    "start": "2738480",
    "end": "2745119"
  },
  {
    "text": "and my estimator emits one distribution and if i remove xavier i'm going to have",
    "start": "2745119",
    "end": "2750240"
  },
  {
    "text": "a very similar distribution right like my output of my randomized algorithm will remain close whether or not i have",
    "start": "2750240",
    "end": "2756560"
  },
  {
    "text": "included xavier that is the guarantee of differential privacy and there's a more formal definition but i don't want to",
    "start": "2756560",
    "end": "2762160"
  },
  {
    "text": "get into that too much this is really the gold standard in the sense that there hasn't really been strong attacks that were demonstrated",
    "start": "2762160",
    "end": "2768800"
  },
  {
    "text": "against this and it was used in the 2020 census with some controversy but it's really something that's hard to achieve",
    "start": "2768800",
    "end": "2775520"
  },
  {
    "text": "i'm going to skip this slide because i don't want to get into the technical bits but really the issue with differential",
    "start": "2775520",
    "end": "2781280"
  },
  {
    "text": "privacy is it's such a strong guarantee it says that when i remove a user no adversary with arbitrary amounts of side",
    "start": "2781280",
    "end": "2788560"
  },
  {
    "text": "information can determine whether or not i was included in the data set that is incredibly strong and prior attempts to",
    "start": "2788560",
    "end": "2794800"
  },
  {
    "text": "applying differential privacy to natural language processing was kind of a complete failure i just want to give you",
    "start": "2794800",
    "end": "2800160"
  },
  {
    "text": "a funny example here so this is an example from i think a two years ago now kerrigan at all that tried to generate a",
    "start": "2800160",
    "end": "2806079"
  },
  {
    "text": "language generation system using reddit data privately um and what they had is you know this is an example bob lives",
    "start": "2806079",
    "end": "2812000"
  },
  {
    "text": "close to the and if you have a non-private system you'll get a pretty reasonable output here you know lives",
    "start": "2812000",
    "end": "2817119"
  },
  {
    "text": "close to the station and so on and so forth if you use a private system you get complete garbage this is not even",
    "start": "2817119",
    "end": "2822800"
  },
  {
    "text": "reasonable english if you look at the continuation over here and so it seemed for a long time that",
    "start": "2822800",
    "end": "2828400"
  },
  {
    "text": "differential privacy and deep learning were just fundamentally incompatible concepts like you know you can't even",
    "start": "2828400",
    "end": "2834000"
  },
  {
    "text": "put the two together and people have very reasonable reasons for believing this you know large language models have",
    "start": "2834000",
    "end": "2839760"
  },
  {
    "text": "millions of parameters like one typical example is 300 million parameters so if you think about each parameter as",
    "start": "2839760",
    "end": "2845599"
  },
  {
    "text": "potentially containing a private piece of information that is a ton of information to privatize right so it's",
    "start": "2845599",
    "end": "2852240"
  },
  {
    "text": "really hard to privatize large language models because they have so many parameters each of which could potentially leak information",
    "start": "2852240",
    "end": "2859280"
  },
  {
    "text": "now theory also agrees with this it says there are very good bounds that say that differential privacy performance should",
    "start": "2859280",
    "end": "2865359"
  },
  {
    "text": "degrade as the square root of the number of parameters so now theory matches our intuition and it also matches empirical",
    "start": "2865359",
    "end": "2871280"
  },
  {
    "text": "results the only successful dp applications used to be in low dimensional statistics like things like",
    "start": "2871280",
    "end": "2877040"
  },
  {
    "text": "the mean or the number of people living in a household but that's really not the complete",
    "start": "2877040",
    "end": "2883200"
  },
  {
    "text": "picture and this is maybe the optimistic part of the talk um we can actually change our perspective a",
    "start": "2883200",
    "end": "2888800"
  },
  {
    "text": "little bit we've been thinking about language models as the cause of privacy harms but it can in some ways also be",
    "start": "2888800",
    "end": "2895040"
  },
  {
    "text": "empowering us to have better privacy and this is a slightly different perspective so we can't train large language models",
    "start": "2895040",
    "end": "2901359"
  },
  {
    "text": "yet using differential privacy but what we can do is the following idea we can look for nicely curated public data",
    "start": "2901359",
    "end": "2907839"
  },
  {
    "text": "let's set aside the problem of public privacy harms for a moment train a large language model and then use this",
    "start": "2907839",
    "end": "2913280"
  },
  {
    "text": "language model along with private data to achieve privacy so now we can have privacy on a small subset of data that",
    "start": "2913280",
    "end": "2919119"
  },
  {
    "text": "we truly care about while leveraging large public data sets and this is kind of the paradigm that i",
    "start": "2919119",
    "end": "2925040"
  },
  {
    "text": "went back to at the very beginning of the talk i was saying language models have driven enormous gains in nlp",
    "start": "2925040",
    "end": "2931599"
  },
  {
    "text": "through through a range of subtasks and the reason is because these models learn really useful things like syntax of the",
    "start": "2931599",
    "end": "2937760"
  },
  {
    "text": "english language right we don't want to be using our precious private data to relearn the english syntax that is crazy",
    "start": "2937760",
    "end": "2944800"
  },
  {
    "text": "right so we want to learn this stuff from hopefully safe sanitized public",
    "start": "2944800",
    "end": "2949839"
  },
  {
    "text": "data from somewhere and then use that to empower our sort of downstream private",
    "start": "2949839",
    "end": "2955839"
  },
  {
    "text": "applications that's sort of the idea here and so we it's wasteful to spend our private data learning this kind of",
    "start": "2955839",
    "end": "2961760"
  },
  {
    "text": "public information and that's the main idea so what we found was actually kind of",
    "start": "2961760",
    "end": "2967920"
  },
  {
    "text": "funny so once we started thinking about it this way we realized that maybe language models actually are useful",
    "start": "2967920",
    "end": "2974559"
  },
  {
    "text": "so what explains previous failures in using uh in using these kinds of systems",
    "start": "2974559",
    "end": "2979839"
  },
  {
    "text": "sorry in using differential privacy in language generation systems it turns out it was something very silly it was hyper",
    "start": "2979839",
    "end": "2984960"
  },
  {
    "text": "parameters if you use typical hyper parameters that you use for non-private training you get",
    "start": "2984960",
    "end": "2990800"
  },
  {
    "text": "catastrophically bad performance this is uh performance on the e2e benchmark that i showed you before and around 60 is",
    "start": "2990800",
    "end": "2998319"
  },
  {
    "text": "state-of-the-art 70 is state-of-the-art and 10 is totally unusable it's garbage so if you use a typical hyper parameter",
    "start": "2998319",
    "end": "3004640"
  },
  {
    "text": "for non-private training you get basically garbage but this was actually about two orders of magnitude off if you",
    "start": "3004640",
    "end": "3010240"
  },
  {
    "text": "change the hyper parameters substantially to be more suitable for differential privacy you actually get",
    "start": "3010240",
    "end": "3015599"
  },
  {
    "text": "close to non-private performance this was really surprising to us and we had to come up with a different kind of",
    "start": "3015599",
    "end": "3021440"
  },
  {
    "text": "explanation for why you would use these weird hyper parameters but the naive choices were way off",
    "start": "3021440",
    "end": "3026960"
  },
  {
    "text": "and equipped with this we found a very different story what we found was that",
    "start": "3026960",
    "end": "3032000"
  },
  {
    "text": "you could actually get the private systems that were almost as good as their non-private counterparts so on the",
    "start": "3032000",
    "end": "3038160"
  },
  {
    "text": "left side is a classification task so you're predicting uh entailment which is the task that i told you about on the",
    "start": "3038160",
    "end": "3044079"
  },
  {
    "text": "right-hand side is the e2e task the the restaurant review generation on the right",
    "start": "3044079",
    "end": "3049119"
  },
  {
    "text": "and we see that as we increase the number of parameters from left to right the performance gets better and better",
    "start": "3049119",
    "end": "3054240"
  },
  {
    "text": "the y-axis here is performance and each of these lines is a private model that we train",
    "start": "3054240",
    "end": "3060240"
  },
  {
    "text": "and so larger models by virtue of their pre-trained language models being better actually get much better private models",
    "start": "3060240",
    "end": "3067280"
  },
  {
    "text": "in the end so this runs counter to the maybe belief that people had that large models were",
    "start": "3067280",
    "end": "3072720"
  },
  {
    "text": "hard to privatize because they might be leaking private pieces of information we actually found that large language",
    "start": "3072720",
    "end": "3078720"
  },
  {
    "text": "models actually give us the opportunity to get even better performance under a stringent privacy guarantee",
    "start": "3078720",
    "end": "3085280"
  },
  {
    "text": "um so in the non-private case the sorry the pre-training is sort of a small game but for private learning the",
    "start": "3085280",
    "end": "3092000"
  },
  {
    "text": "difference is huge right so having large models gives us enormous gains going from unusable uh performance to go",
    "start": "3092000",
    "end": "3098559"
  },
  {
    "text": "getting to completely usable and nearly non-private performance",
    "start": "3098559",
    "end": "3103599"
  },
  {
    "text": "um what is the time we have five minutes left okay so i will talk about one last technical piece and then i will conclude",
    "start": "3103599",
    "end": "3109520"
  },
  {
    "text": "um one really interesting side note especially if you're sort of more systems or computational people here is",
    "start": "3109520",
    "end": "3116480"
  },
  {
    "text": "the real technical challenge that we found was not necessarily statistical usually people think about private",
    "start": "3116480",
    "end": "3121920"
  },
  {
    "text": "learning in terms of statistics how large does your data set need to be in order to be able to learn privately what",
    "start": "3121920",
    "end": "3128160"
  },
  {
    "text": "we found was a really big challenge was computational differentially private learning requires",
    "start": "3128160",
    "end": "3133680"
  },
  {
    "text": "a lot more memory than non-private learning and what we found is is sort of this following very uh",
    "start": "3133680",
    "end": "3140000"
  },
  {
    "text": "challenging thing so for various technical reasons differentially private sgd is very",
    "start": "3140000",
    "end": "3145599"
  },
  {
    "text": "memory intensive and so for looking at a non-private uh training task we can fit 34 examples in a batch for a medium",
    "start": "3145599",
    "end": "3152240"
  },
  {
    "text": "model or 10 examples for a large model if we're doing private training we can't even fit a single example into a",
    "start": "3152240",
    "end": "3159040"
  },
  {
    "text": "titan x our titan rtx gpu when we're doing private training and this turned out to turn into the bottleneck because",
    "start": "3159040",
    "end": "3164880"
  },
  {
    "text": "we were no longer having these statistical challenges because we were using pre-trained models but we have these computational challenges",
    "start": "3164880",
    "end": "3173040"
  },
  {
    "text": "and by using various tricks involved in like modifying the back propagation algorithm what you could actually get is",
    "start": "3173040",
    "end": "3179680"
  },
  {
    "text": "uh memory usage that was on parts of the non-private algorithm and that's kind of a cool little trick that we developed",
    "start": "3179680",
    "end": "3185040"
  },
  {
    "text": "but because of that we can now really scale up oh sorry scale up the size of these models and finally get",
    "start": "3185040",
    "end": "3191520"
  },
  {
    "text": "private models that were almost close in performance their non-private counterparts um",
    "start": "3191520",
    "end": "3197680"
  },
  {
    "text": "so before i sort of conclude um in the last couple of minutes i want to just show you examples of what a private system generates because i think in",
    "start": "3197680",
    "end": "3204079"
  },
  {
    "text": "generation you know i can show you numbers like blue scores and what on but looking at the outputs actually i think",
    "start": "3204079",
    "end": "3209119"
  },
  {
    "text": "is more compelling uh so for the top one this is the e2e generation task this um",
    "start": "3209119",
    "end": "3214559"
  },
  {
    "text": "you have the table at the top that describes the restaurant we have the reference that was written by humans and",
    "start": "3214559",
    "end": "3219680"
  },
  {
    "text": "below that this is a private version of a neural language model that we train so this thing has a very formal and strong",
    "start": "3219680",
    "end": "3226240"
  },
  {
    "text": "privacy guarantee of epsilon of three um and yet it generates perfectly fluent and sort of grammatical and correct uh",
    "start": "3226240",
    "end": "3232960"
  },
  {
    "text": "restaurant reviews and so that's sort of the point that we're at now by leveraging large pre-trained language models combining",
    "start": "3232960",
    "end": "3239440"
  },
  {
    "text": "that with differential privacy we can actually get privacy for a lot of these interesting downstream tasks that we",
    "start": "3239440",
    "end": "3244800"
  },
  {
    "text": "care about we can get this very strong notion of privacy called differential privacy and so that's where we're kind of at",
    "start": "3244800",
    "end": "3251520"
  },
  {
    "text": "right so to wrap up right this was in two parts so the first thing that i do want to for you to remember",
    "start": "3251520",
    "end": "3258079"
  },
  {
    "text": "is that language models do pose a privacy risk it scrapes the internet it collects all this data it aggregates",
    "start": "3258079",
    "end": "3264640"
  },
  {
    "text": "them that is a privacy risk right let's not feed around the bush here and large language models will memorize training",
    "start": "3264640",
    "end": "3271119"
  },
  {
    "text": "data so these these threats are real but at the same time there's opportunities right i was talking about risks here but",
    "start": "3271119",
    "end": "3277359"
  },
  {
    "text": "the the flip side of this is that large language models by virtue of you know the fact that they can teach you about",
    "start": "3277359",
    "end": "3283520"
  },
  {
    "text": "english grammar and so on allows you new kinds of privacy guarantees much stronger than what was available before",
    "start": "3283520",
    "end": "3290160"
  },
  {
    "text": "so even though there's some privacy risks that we get we also get some other kinds of privacy in return which is sort",
    "start": "3290160",
    "end": "3295760"
  },
  {
    "text": "of an exciting frontier so i'll stop here we'll have a couple of minutes at the end for questions if anyone hasn't",
    "start": "3295760",
    "end": "3302790"
  },
  {
    "text": "[Applause]",
    "start": "3302790",
    "end": "3308640"
  },
  {
    "text": "sure i guess why is it that the memorization problem can't be trivialized with pre-processing",
    "start": "3308640",
    "end": "3314960"
  },
  {
    "text": "why can't the memorization be trivialized with pre-processing like trivially avoided i guess what do you",
    "start": "3314960",
    "end": "3320640"
  },
  {
    "text": "mean by that you can make the model not memorize data by just not like by just please stop pre-processing the data",
    "start": "3320640",
    "end": "3326480"
  },
  {
    "text": "somehow like for example you could imagine just running like a simple arithmetic encoder on it so it's lossy",
    "start": "3326480",
    "end": "3331680"
  },
  {
    "text": "so then by pigeonhole principle you have something guaranteed so let me think about that so you can certainly noise up your data and that",
    "start": "3331680",
    "end": "3338319"
  },
  {
    "text": "will prevent memorization but you will pay a very large price in the accuracy in return because your goal is not to",
    "start": "3338319",
    "end": "3344799"
  },
  {
    "text": "predict noisy english it is to predict english right um there are certainly i mean in in some",
    "start": "3344799",
    "end": "3351119"
  },
  {
    "text": "ways okay so there is a truth to sort of what you were you were thinking about but it doesn't operate at the pre-processing level to be very formal",
    "start": "3351119",
    "end": "3357839"
  },
  {
    "text": "uh about this like differential privacy is kind of this idea it says instead of taking a normal",
    "start": "3357839",
    "end": "3363920"
  },
  {
    "text": "gradient step taking out the very top what you do is you take your gradients you clip them so each gradient is like",
    "start": "3363920",
    "end": "3370880"
  },
  {
    "text": "you know bounded in size and then you add a bunch of noise and that noise step is kind of like this intuition that you have",
    "start": "3370880",
    "end": "3376559"
  },
  {
    "text": "but if you just do it in the right way with the right amounts of noise and the right amounts of clipping then you can get guaranteed so it's a lot more subtle",
    "start": "3376559",
    "end": "3382559"
  },
  {
    "text": "than just like let's just add some noise to the training data",
    "start": "3382559",
    "end": "3386640"
  },
  {
    "text": "second solution yes you share oh like let's not worry about the public data privacy issues but let's focus on like",
    "start": "3388960",
    "end": "3394960"
  },
  {
    "text": "the specific private data we have do you think it is worthy to think about the public",
    "start": "3394960",
    "end": "3400000"
  },
  {
    "text": "data privacy issues and whether there's any remedy to having a large corpus with",
    "start": "3400000",
    "end": "3407599"
  },
  {
    "text": "private data yeah or are you so so okay i'll fully acknowledge right",
    "start": "3407599",
    "end": "3414799"
  },
  {
    "text": "the first part of this section was like okay it's really bad to pretend that private the public data is public right",
    "start": "3414799",
    "end": "3420640"
  },
  {
    "text": "um i don't think we have a great solution for that yet but people are working on differentially",
    "start": "3420640",
    "end": "3426559"
  },
  {
    "text": "private pre-training and if that gets solved then this whole thing will be private",
    "start": "3426559",
    "end": "3432960"
  },
  {
    "text": "and we would sort of be able to avoid all these issues um i guess like you know once you solve",
    "start": "3432960",
    "end": "3438079"
  },
  {
    "text": "those problems there are other additional problems about what does it mean to do differential privacy on internet text what what is our",
    "start": "3438079",
    "end": "3444000"
  },
  {
    "text": "definition of privacy um those are subtle questions but i think we're slowly making progress",
    "start": "3444000",
    "end": "3449839"
  },
  {
    "text": "yes just to make sure i'm on the right page here um you took a pre-trained model um and then you",
    "start": "3449839",
    "end": "3456799"
  },
  {
    "text": "basically did hyper parameter tuning on the private data set is that correct you fine tune on the privacy right",
    "start": "3456799",
    "end": "3463040"
  },
  {
    "text": "so the the hyper parameter story is not that we're doing hyper parameter tuning on the private set but we needed to just",
    "start": "3463040",
    "end": "3468880"
  },
  {
    "text": "change the hyper parameters when we fine tune on the private set so it's the well i guess we can go all",
    "start": "3468880",
    "end": "3475520"
  },
  {
    "text": "the way back to the very first part of the talk but it is basically",
    "start": "3475520",
    "end": "3480799"
  },
  {
    "text": "this paradigm you have a pre-trained language model you have a downstream task that you would like to do so in",
    "start": "3480799",
    "end": "3485920"
  },
  {
    "text": "stage two you take your pre-trained model and you fine-tune it for the downstream task it's exactly this",
    "start": "3485920",
    "end": "3493640"
  },
  {
    "text": "[Applause]",
    "start": "3498040",
    "end": "3501360"
  },
  {
    "text": "you",
    "start": "3503599",
    "end": "3505680"
  }
]