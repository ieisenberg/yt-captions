[
  {
    "text": " All right.",
    "start": "0",
    "end": "5500"
  },
  {
    "text": "So the main event\ntoday is we're going to discuss cache\ncoherence, but before we get into cache coherence,\nlet's finish up",
    "start": "5500",
    "end": "13080"
  },
  {
    "text": "our discussion of spark. You notice that instead of\nme teaching on Thursday,",
    "start": "13080",
    "end": "21790"
  },
  {
    "text": "Kayvon did. And that's because\noriginally we thought that based on when the programming\nassignments would be out,",
    "start": "21790",
    "end": "28030"
  },
  {
    "text": "that you would need to\nknow about optimizing DNN code on the GPU.",
    "start": "28030",
    "end": "34180"
  },
  {
    "text": "But given that we've delayed\nthe programming assignment 3 and so 4 is delayed, turned\nout that we didn't quite",
    "start": "34180",
    "end": "42150"
  },
  {
    "text": "need the order of\nlectures and so things are a little not as\nsmooth as we might like,",
    "start": "42150",
    "end": "51460"
  },
  {
    "text": "but nonetheless you\nguys can context switch. All right. So back to spark.",
    "start": "51460",
    "end": "57340"
  },
  {
    "text": "So where we left\noff, we were talking about how to design a\nsystem for doing distributed",
    "start": "57340",
    "end": "66370"
  },
  {
    "text": "computing on a cluster. Remember, the characteristics\nof a cluster, of course, is you've got separate\nnodes or servers that",
    "start": "66370",
    "end": "75460"
  },
  {
    "text": "are connected by a network,\nand each of the servers is running its own\noperating system and has its own\nindependent memory system.",
    "start": "75460",
    "end": "83079"
  },
  {
    "text": "And the way that\nwe communicate is by using message passing as\nopposed to shared memory.",
    "start": "83080",
    "end": "89620"
  },
  {
    "text": "So the idea of in-memory fault\ntolerant distributed computing, the goals of spark\nis, what if you",
    "start": "89620",
    "end": "95800"
  },
  {
    "text": "have an application\nthat makes lots of use of intermediate data? So we know that we can make\nit fault tolerant by using",
    "start": "95800",
    "end": "104230"
  },
  {
    "text": "the MapReduce system. So what were the\ncharacteristics? How did we keep fault tolerance\nin the MapReduce system?",
    "start": "104230",
    "end": "112305"
  },
  {
    "text": " Where do we put the\nintermediate data? ",
    "start": "112305",
    "end": "122130"
  },
  {
    "text": "Yeah. On the hard drive. On the hard drive. On the distributed file system,\nthe HDFS system, which we know",
    "start": "122130",
    "end": "128570"
  },
  {
    "text": "is fault-tolerant\nbecause it's replicated. But the problem, as we saw\nfrom by looking at the system,",
    "start": "128570",
    "end": "134940"
  },
  {
    "text": "it's slow. Especially compared\nto the memory, it's 100x slower\nthan the memory.",
    "start": "134940",
    "end": "142860"
  },
  {
    "text": "And so the question is can we--\nif we want an application that makes lots of use\nof intermediate data",
    "start": "142860",
    "end": "148980"
  },
  {
    "text": "such as an iterative\nalgorithm or a query",
    "start": "148980",
    "end": "155450"
  },
  {
    "text": "where you've got the data in\nthat you want to continuously make ad hoc queries to\nthen, the MapReduce system",
    "start": "155450",
    "end": "164390"
  },
  {
    "text": "is actually pretty inefficient. So how can we make\nsomething that's much more efficient but\nstill keep our quality",
    "start": "164390",
    "end": "172340"
  },
  {
    "text": "that we want in the distributed\nsystem as something that is still fault-tolerant?",
    "start": "172340",
    "end": "177420"
  },
  {
    "text": "And so the key\nabstraction we said was the resilient\ndistributed data set, which was this read-only\nordered collection of records.",
    "start": "177420",
    "end": "184780"
  },
  {
    "text": "So essentially, the\nsequence, and the way that we get RDDs is we\nstart with some data",
    "start": "184780",
    "end": "194310"
  },
  {
    "text": "on the disk on the\nHDFS, and then we apply transformations to it. So we can extract the\nlines from the text file",
    "start": "194310",
    "end": "203610"
  },
  {
    "text": "and get an RDD called\nlines, we can filter, which is another transformation\nto get mobile views,",
    "start": "203610",
    "end": "208980"
  },
  {
    "text": "and we can filter again\nto get Safari views, and then finally, we can\ncreate a count from that,",
    "start": "208980",
    "end": "214810"
  },
  {
    "text": "which is not a transformation. And, of course, so the\nresult is a single scalar instead of an RDD.",
    "start": "214810",
    "end": "222640"
  },
  {
    "text": "And so we were looking\nat how we implement RDDs and we said that\nthe problem was,",
    "start": "222640",
    "end": "227920"
  },
  {
    "text": "is if you didn't do\nthings efficiently, you could end up with\nhuge amounts of memory for the intermediate data.",
    "start": "227920",
    "end": "235060"
  },
  {
    "text": "And so, of course,\nthis course is all about improving\nparallel performance,",
    "start": "235060",
    "end": "241000"
  },
  {
    "text": "and we said that parallel\nperformance comes from-- you've got to have lots of\nparallelism, lots of things to do at the same\ntime, but you also",
    "start": "241000",
    "end": "247290"
  },
  {
    "text": "have to optimize for locality. We've talked to talked\nabout a couple of mechanisms",
    "start": "247290",
    "end": "252780"
  },
  {
    "text": "for optimizing for\nlocality, and you just saw these used in\nthe last lecture.",
    "start": "252780",
    "end": "260940"
  },
  {
    "text": "One was the idea of loop fusion,\nand here, of course, we're",
    "start": "260940",
    "end": "265980"
  },
  {
    "text": "trying to minimize the need\nfor external memory access,",
    "start": "265980",
    "end": "274245"
  },
  {
    "text": "and we're trying to improve or\nincrease arithmetic intensity. And this is an example, we\nsaw that here in this example",
    "start": "274245",
    "end": "282889"
  },
  {
    "text": "and you saw it in\ndoing attention.",
    "start": "282890",
    "end": "288510"
  },
  {
    "text": "So Kayvon talked\nabout flash attention, and that's essentially\na combination of fusion and tiling.",
    "start": "288510",
    "end": "296380"
  },
  {
    "text": "And so tiling was the other\nlocality optimization mechanism",
    "start": "296380",
    "end": "301440"
  },
  {
    "text": "that we talked about. And we said that\nessentially in order to get these sorts\nof transformations,",
    "start": "301440",
    "end": "307270"
  },
  {
    "text": "you have to understand the\napplication because you",
    "start": "307270",
    "end": "312780"
  },
  {
    "text": "potentially have to\nglobally restructure things. And so the nice thing about\nthe spark implementation",
    "start": "312780",
    "end": "321720"
  },
  {
    "text": "is that you have the ability\nto do fusion with RDDs",
    "start": "321720",
    "end": "327280"
  },
  {
    "text": "since you've got a set\nof bulk operations, these transformations on\nRDDs, and the runtime system,",
    "start": "327280",
    "end": "334539"
  },
  {
    "text": "the spark runtime system can\nlook at what is happening",
    "start": "334540",
    "end": "339780"
  },
  {
    "text": "and optimize these things. So in this example, we've\ngot these transformations",
    "start": "339780",
    "end": "346950"
  },
  {
    "text": "to create lines, lower mobile\nviews, and then finally",
    "start": "346950",
    "end": "352170"
  },
  {
    "text": "an action to create, how many. And what you'd like\nto do, of course, is implement it in a fused\nmanner, such as this,",
    "start": "352170",
    "end": "361360"
  },
  {
    "text": "in which you just fetch one\nline from the file system",
    "start": "361360",
    "end": "366419"
  },
  {
    "text": "and then everything\nelse is kept in memory, and you only do it\na line at a time.",
    "start": "366420",
    "end": "372789"
  },
  {
    "text": "And so this is ideally\nwhat you'd want to create,",
    "start": "372790",
    "end": "378280"
  },
  {
    "text": "and the spark runtime\nsystem will do this for you, and it does it when it can\nanalyze the transformations",
    "start": "378280",
    "end": "387270"
  },
  {
    "text": "and determine that you\nhave narrow dependencies. And the narrow dependencies,\none RTD only depend--",
    "start": "387270",
    "end": "396960"
  },
  {
    "text": "the partition of\none RDD only depends on a single previous partition.",
    "start": "396960",
    "end": "404400"
  },
  {
    "text": "For instance, the\nlower partition, 0 only",
    "start": "404400",
    "end": "414229"
  },
  {
    "text": "depends on the\nline's partition 0, and the mobile views\npartition 0 only",
    "start": "414230",
    "end": "419330"
  },
  {
    "text": "depends on the\nlower partition 0. So you can imagine then\nthat the runtime system can",
    "start": "419330",
    "end": "426230"
  },
  {
    "text": "fuse all of these\ntransformations, such that you don't have\nany extra memory accesses",
    "start": "426230",
    "end": "436220"
  },
  {
    "text": "or external memory access. So there's no communications\nbetween nodes of the cluster,",
    "start": "436220",
    "end": "442470"
  },
  {
    "text": "and so everything can\nbe fully optimized and run very efficiently.",
    "start": "442470",
    "end": "449539"
  },
  {
    "text": "So the problem is that you\ncan have transformations that have wide\ndependencies, and which",
    "start": "449540",
    "end": "457460"
  },
  {
    "text": "would require communication\nbetween the different nodes in the system. So an example would\nbe group by key",
    "start": "457460",
    "end": "465320"
  },
  {
    "text": "where in order to determine the\ngroup by partition 0 for RDD B,",
    "start": "465320",
    "end": "479900"
  },
  {
    "text": "you would have to communicate\nwith all the other nodes in the system to get the data\nfrom the different partitions",
    "start": "479900",
    "end": "486860"
  },
  {
    "text": "from partition 1, partition 2\nand partition 3, for example. So in this case, of course,\nyou can't do the fusion",
    "start": "486860",
    "end": "496280"
  },
  {
    "text": "within a single\nnode, and so this is something that would\nhave a lot of communication.",
    "start": "496280",
    "end": "502500"
  },
  {
    "text": "So the question is,\nin what cases can you optimize operations\nlike group by key?",
    "start": "502500",
    "end": "509690"
  },
  {
    "text": "So look at this example in which\nwe are trying to do a join.",
    "start": "509690",
    "end": "515580"
  },
  {
    "text": "So you're going to join\nby key, and depending on how the RDD A and\nRDD B which you're",
    "start": "515580",
    "end": "526580"
  },
  {
    "text": "trying to join to create RDD\nC, if you didn't know anything",
    "start": "526580",
    "end": "534030"
  },
  {
    "text": "about the RDD A and RDD B,\nthen clearly you would have",
    "start": "534030",
    "end": "539880"
  },
  {
    "text": "to communicate across all the\nnodes in the cluster in order",
    "start": "539880",
    "end": "545520"
  },
  {
    "text": "to compute RDD C. So in what cases might\nyou be able to get away",
    "start": "545520",
    "end": "553180"
  },
  {
    "text": "without communicating, if\nyou're trying to do a join,",
    "start": "553180",
    "end": "558395"
  },
  {
    "text": "for example?  So as shown here, you\nhave to communicate.",
    "start": "558395",
    "end": "564430"
  },
  {
    "text": "But could you\nimagine a situation where you didn't\nhave to communicate?",
    "start": "564430",
    "end": "571520"
  },
  {
    "text": "Yeah. This is specific. He would be like only\nin prediction 0 RDD A",
    "start": "571520",
    "end": "579180"
  },
  {
    "text": "and only in\npredictions 0 in RDD B and not in the other conditions. Exactly, so that's the\nsituation in which you--",
    "start": "579180",
    "end": "588205"
  },
  {
    "text": "if you partitioned, RDD A and\nRDD B such that the keys--",
    "start": "588205",
    "end": "594420"
  },
  {
    "text": "only the common keys only\nexisted in a single partition,",
    "start": "594420",
    "end": "600100"
  },
  {
    "text": "then you would have\nnarrow dependencies. So that is shown in the\nsecond example here.",
    "start": "600100",
    "end": "609690"
  },
  {
    "text": "So essentially RDD A and RDD B\nhave the same hash partition, and so that's shown in this\nexample or this code here.",
    "start": "609690",
    "end": "619980"
  },
  {
    "text": "So you have an explicit\nhash partitioner,",
    "start": "619980",
    "end": "625320"
  },
  {
    "text": "and you've named it\npartitioner, and you use it to partition\nmobile views and you use",
    "start": "625320",
    "end": "635900"
  },
  {
    "text": "it to partition client info.  So when you do the join then,\nthe runtime system says,",
    "start": "635900",
    "end": "645930"
  },
  {
    "text": "hey, these two RDDs are being\npartitioned using the same hash",
    "start": "645930",
    "end": "654350"
  },
  {
    "text": "partitioner. So it can detect\nthat these are equal, and it will determine that there\nare only narrow dependencies.",
    "start": "654350",
    "end": "662910"
  },
  {
    "text": "And then it will make sure\nthat you can do fusion.",
    "start": "662910",
    "end": "668639"
  },
  {
    "text": "All right. Everybody follow that. Any questions?",
    "start": "668640",
    "end": "674190"
  },
  {
    "text": "Yeah. There might still be an\nimbalance of keys like one RDD because there'll be a few\nkeys, and the other one",
    "start": "674190",
    "end": "681630"
  },
  {
    "text": "has a lot of entries\nwith the same key. Right. So there is this\nquestion of load balance,",
    "start": "681630",
    "end": "688410"
  },
  {
    "text": "but to a first order,\nprobably load balance is less of a problem\ncompared to communication.",
    "start": "688410",
    "end": "696130"
  },
  {
    "text": " So that's a good point.",
    "start": "696130",
    "end": "702060"
  },
  {
    "text": "All right.  So now we have a way of\nscheduling the computation",
    "start": "702060",
    "end": "711090"
  },
  {
    "text": "for locality to improve\nperformance and reduce the amount of memory required. Now, let's talk about\nfault tolerance.",
    "start": "711090",
    "end": "717280"
  },
  {
    "text": "So we said that the whole\ngoal of the spark system was to give you high performance\nin-memory computation with fault",
    "start": "717280",
    "end": "725910"
  },
  {
    "text": "tolerance. So then the question\nis, how do you maintain the fault tolerance?",
    "start": "725910",
    "end": "732340"
  },
  {
    "text": "So remember this\nidea of lineage. So you've got these\ntransformations",
    "start": "732340",
    "end": "737940"
  },
  {
    "text": "which are these bulk\ndeterministic transform operations on RDDs, right.",
    "start": "737940",
    "end": "745960"
  },
  {
    "text": "And so the lineage then or\nthe set of transformations that you apply, starting\nwith the data load",
    "start": "745960",
    "end": "757080"
  },
  {
    "text": "from the distributed file system\nis the set of transformation,",
    "start": "757080",
    "end": "764230"
  },
  {
    "text": "gives you a log\nof the operations you need to perform to\nget to any particular RDD.",
    "start": "764230",
    "end": "770920"
  },
  {
    "text": "So you know that if you\nwant to get to timestamps,",
    "start": "770920",
    "end": "776120"
  },
  {
    "text": "you first load from the\nHDFS, do the filter,",
    "start": "776120",
    "end": "782830"
  },
  {
    "text": "do another filter,\nand then the map. And so this list\nof transformations",
    "start": "782830",
    "end": "788200"
  },
  {
    "text": "then is known as the lineage. So you've lineage is essentially\na log of transformations",
    "start": "788200",
    "end": "795250"
  },
  {
    "text": "that can be used to\nrecreate any RDD.",
    "start": "795250",
    "end": "802070"
  },
  {
    "text": "And we know what property did\nwe say that RDDs had, right? They're read-only and\ntransformations are functional,",
    "start": "802070",
    "end": "812790"
  },
  {
    "text": "which means they do not\nmutate their inputs. So we know that it's\nalways possible,",
    "start": "812790",
    "end": "818680"
  },
  {
    "text": "given an RDD to recreate\nit from the hard disk,",
    "start": "818680",
    "end": "826180"
  },
  {
    "text": "from the persistent data that\nwe know will never go away",
    "start": "826180",
    "end": "832020"
  },
  {
    "text": "in the system because\nit's replicated across all the nodes in the system. Question.",
    "start": "832020",
    "end": "838320"
  },
  {
    "text": "If we're only always-- we're\nnot storing the intermediates, we're just storing\nthe log of them,",
    "start": "838320",
    "end": "845140"
  },
  {
    "text": "so why do we actually\nmaterialize all this data?",
    "start": "845140",
    "end": "850780"
  },
  {
    "text": "Is it just we just do all the\ncomputation and then at the end we just store\nwhatever checkpoints.",
    "start": "850780",
    "end": "856770"
  },
  {
    "text": "So why do you want\nto materialize data? Because-- Because at some point,\nwe need it, right.",
    "start": "856770",
    "end": "862260"
  },
  {
    "text": "Yeah. So some point in this case,\nwhat does the user want?",
    "start": "862260",
    "end": "868750"
  },
  {
    "text": "Timestamps probably, right? Does the user necessarily care\nabout the intermediate data?",
    "start": "868750",
    "end": "876680"
  },
  {
    "text": "So you were just trying to\nfuse as much as you can. Right. You'll get a fuse\nas much as you can, and not keep intermediate data\nthat the user hasn't asked for.",
    "start": "876680",
    "end": "885889"
  },
  {
    "text": "If the user asks\nfor it, yes, you've got to provide it to them. But if the user didn't ask\nfor it, it's intermediate and it could\npotentially go away,",
    "start": "885890",
    "end": "894450"
  },
  {
    "text": "the only issue is\nwhat happens if you lose the intermediate\ndata before you create the final result\nthat the user wanted?",
    "start": "894450",
    "end": "900720"
  },
  {
    "text": " So what happens\nin this situation",
    "start": "900720",
    "end": "906610"
  },
  {
    "text": "where we're not done\nwith the computation, but there's a node that crashes?",
    "start": "906610",
    "end": "912820"
  },
  {
    "text": "So this is the example here. So we've got a set of\ntransformations that give us",
    "start": "912820",
    "end": "919180"
  },
  {
    "text": "a lineage that allows us\nto recreate any of the RDDs that we have in our computation,\nand in the middle of computing",
    "start": "919180",
    "end": "930470"
  },
  {
    "text": "the timestamps, node 1 crashes.",
    "start": "930470",
    "end": "936000"
  },
  {
    "text": "So it crashes and we lose the\npartition 2 and partition 3",
    "start": "936000",
    "end": "943040"
  },
  {
    "text": "of timestamps and mobile views. So now what do we do?",
    "start": "943040",
    "end": "949730"
  },
  {
    "text": "Yeah. You just kind of-- you just we run the log. We run the log.",
    "start": "949730",
    "end": "955410"
  },
  {
    "text": "So we run the log. We assume that the data exists\nsomewhere else because it's replicated in the HDFS, and\nwe have a log of operations,",
    "start": "955410",
    "end": "965250"
  },
  {
    "text": "which is fairly coarse grained. So it's not a huge number of\nthings that we have to remember,",
    "start": "965250",
    "end": "973470"
  },
  {
    "text": "and we reapply the log in order\nto create the partitions 2 and 3",
    "start": "973470",
    "end": "986930"
  },
  {
    "text": "of timestamps. And so we create that from the\npartitions of mobile views,",
    "start": "986930",
    "end": "1003100"
  },
  {
    "text": "and well, we start\nwith lines, we get to mobile\nviews, chrome views, and then we recreate timestamps.",
    "start": "1003100",
    "end": "1010372"
  },
  {
    "text": "So that's the way that\nwe recover from a crash. Question. The first word is the\n[INAUDIBLE] versus [INAUDIBLE]",
    "start": "1010372",
    "end": "1019340"
  },
  {
    "text": "the master node. Yeah, the master node is\nkeeping track of the lineage. We assume that that's\none of the nodes",
    "start": "1019340",
    "end": "1025230"
  },
  {
    "text": "and it's not likely to fail\nor it may be replicated. One more question\nis like if you use",
    "start": "1025230",
    "end": "1035272"
  },
  {
    "text": "the standard core [INAUDIBLE]. Yeah, it's just the log\nof the transformation.",
    "start": "1035272",
    "end": "1040500"
  },
  {
    "text": "So is it like if\nyou wanted functions on your own kind of data, if you\nwanted to do completely custom",
    "start": "1040500",
    "end": "1047300"
  },
  {
    "text": "functions, how do you do that? [INAUDIBLE] Well, remember,\nyou've got to live within the spark\nworld, which means",
    "start": "1047300",
    "end": "1052860"
  },
  {
    "text": "you have to do transformations\non RDDs, which are functional. So if you do things\nthat are nonfunctional,",
    "start": "1052860",
    "end": "1059290"
  },
  {
    "text": "you start mutating\nyour input, then you've broken the\nspark abstraction and things won't work.",
    "start": "1059290",
    "end": "1065370"
  },
  {
    "text": "So if I have, let's say\nlike just a function that takes the input, the\noutput is not [INAUDIBLE].",
    "start": "1065370",
    "end": "1071540"
  },
  {
    "text": "You've got to set your\ndata type is RDDs. The set of things you can\ndo with transformations.",
    "start": "1071540",
    "end": "1078000"
  },
  {
    "text": "If you go outside\nthat abstraction, you've broken things, and spark\nsays, hey, you're on your own.",
    "start": "1078000",
    "end": "1086075"
  },
  {
    "text": " Other questions?",
    "start": "1086075",
    "end": "1092460"
  },
  {
    "text": "All right. So we have a way of\nrecovering from crashes, and we get, the nice thing\nabout spark, of course,",
    "start": "1092460",
    "end": "1101110"
  },
  {
    "text": "is that you get to use memory\nand you get the performance benefits of memory,\nbut you don't lose fault tolerance,\nwhich is critical if you're",
    "start": "1101110",
    "end": "1109230"
  },
  {
    "text": "doing data processing. So how much performance\nimprovement do you get? Well, this is the Spark\npaper when it came out",
    "start": "1109230",
    "end": "1119490"
  },
  {
    "text": "2012 or something like that. They compared to Hadoop,\nand Hadoop is always",
    "start": "1119490",
    "end": "1126720"
  },
  {
    "text": "a good whipping boy\nbecause it's so slow. And so in Hadoop,\nthis is the case.",
    "start": "1126720",
    "end": "1132793"
  },
  {
    "text": "They're doing logistic\nregression, which is a very simple\nML operation, and K",
    "start": "1132793",
    "end": "1139167"
  },
  {
    "text": "means which you're very familiar\nwith because of course you've played with that. And so we see that Hadoop,\nthe first iteration 80",
    "start": "1139167",
    "end": "1150429"
  },
  {
    "text": "seconds and\nsubsequent iterations don't get much faster. And of course each iteration\nrequires an HDFS read",
    "start": "1150430",
    "end": "1157840"
  },
  {
    "text": "and an HDFS, right. The Hadoop binary\nmemory basically",
    "start": "1157840",
    "end": "1166450"
  },
  {
    "text": "keeps a binary memory copy\ninstead of a text copy.",
    "start": "1166450",
    "end": "1172100"
  },
  {
    "text": "And so it's slightly faster\non the second iteration, but still has to\naccess the disk.",
    "start": "1172100",
    "end": "1177490"
  },
  {
    "text": "And then spark only has\nto do the HDFS read, doesn't do the write, just\nwrite straight to memory,",
    "start": "1177490",
    "end": "1184520"
  },
  {
    "text": "and so subsequent\niterations are much faster. So spark is significantly faster\norders of magnitude, at least",
    "start": "1184520",
    "end": "1193150"
  },
  {
    "text": "one, maybe two compared\nto using the disk, which is consistent with\nthe performance.",
    "start": "1193150",
    "end": "1200430"
  },
  {
    "text": "Delta we saw in the bandwidths\nbetween S, the storage system,",
    "start": "1200430",
    "end": "1210210"
  },
  {
    "text": "and memory, DRAM and HDFS.",
    "start": "1210210",
    "end": "1215880"
  },
  {
    "text": "So we also see K-Means give you\nthe same performance benefit,",
    "start": "1215880",
    "end": "1221560"
  },
  {
    "text": "but as I said, Hadoop is easy to\nbeat because it's using the file",
    "start": "1221560",
    "end": "1228130"
  },
  {
    "text": "system so intensively. But spark has gotten a\nhuge amount of traction",
    "start": "1228130",
    "end": "1237610"
  },
  {
    "text": "in the data processing world. So it enables you to compose\na bunch of different domain",
    "start": "1237610",
    "end": "1243850"
  },
  {
    "text": "specific frameworks together\nwith this underlying RDD spark",
    "start": "1243850",
    "end": "1250510"
  },
  {
    "text": "implementation, which\nis it works pretty well. And so you can combine the\ntransformations with SQL,",
    "start": "1250510",
    "end": "1260480"
  },
  {
    "text": "so you can do\ndatabase processing, there's a spark MLlib which\nhas a bunch of machine learning",
    "start": "1260480",
    "end": "1271600"
  },
  {
    "text": "operations library to do machine\nlearning based on the Spark abstraction. So of course, you can\nuse distributed clusters",
    "start": "1271600",
    "end": "1285060"
  },
  {
    "text": "and get all the benefits\nof the Spark system while doing machine learning.",
    "start": "1285060",
    "end": "1290490"
  },
  {
    "text": "And there's also Spark graphics,\nwhich adds graph operations",
    "start": "1290490",
    "end": "1296480"
  },
  {
    "text": "to the Spark ecosystem. And in previous\nversions of this class,",
    "start": "1296480",
    "end": "1305639"
  },
  {
    "text": "you did graph analytics\nbreakfast search.",
    "start": "1305640",
    "end": "1314580"
  },
  {
    "text": " You don't do it\nthis quarter there's no breakfast search, right.",
    "start": "1314580",
    "end": "1321020"
  },
  {
    "text": "Yeah, in previous\niterations, you had to do graph\nanalytics operations and so you might have been\nmore familiar with the sorts",
    "start": "1321020",
    "end": "1328210"
  },
  {
    "text": "of things that are in graph X. All right. So in summary then Spark\nintroduces this idea of the RDD",
    "start": "1328210",
    "end": "1336040"
  },
  {
    "text": "as its key abstraction,\nand the observation is that using HDFS as a place\nto put intermediate data when",
    "start": "1336040",
    "end": "1345309"
  },
  {
    "text": "you're trying to do these\niterative kinds of computations or when you're trying to\ncontinuously modify or query--",
    "start": "1345310",
    "end": "1354070"
  },
  {
    "text": "sorry, if you query a\nparticular set of data to do data analytics\nsorts of operations the,",
    "start": "1354070",
    "end": "1361870"
  },
  {
    "text": "HDFS does not work\nas a good place to store intermediate data. And so RDDs are a\nmuch better idea",
    "start": "1361870",
    "end": "1370630"
  },
  {
    "text": "and they can be\nused as a mechanism for creating fault\ntolerance because you've",
    "start": "1370630",
    "end": "1379309"
  },
  {
    "text": "got these\ntransformations which are these bulk deterministic\nfunctional operations on RDDs,",
    "start": "1379310",
    "end": "1386420"
  },
  {
    "text": "and they can give you a log that\nyou can replay whenever there's a failure, and you\ncan make sure that you",
    "start": "1386420",
    "end": "1392270"
  },
  {
    "text": "can both get high performance\nand fault tolerance.",
    "start": "1392270",
    "end": "1397850"
  },
  {
    "text": "And as we saw spark\ncan be extended beyond the set of\nthings that we showed",
    "start": "1397850",
    "end": "1403970"
  },
  {
    "text": "in terms of\ntransformations and actions to do all sorts of things like\ngraph analysis and database",
    "start": "1403970",
    "end": "1411590"
  },
  {
    "text": "operations. So one thing to be aware\nof is that scale out",
    "start": "1411590",
    "end": "1417110"
  },
  {
    "text": "is not the whole story. So scale out was\ninvented because people wanted to be able\nto analyze data",
    "start": "1417110",
    "end": "1423800"
  },
  {
    "text": "that would not fit in the\nmemory of a single server.",
    "start": "1423800",
    "end": "1429210"
  },
  {
    "text": "So how much memory can you\nput on a single server today?",
    "start": "1429210",
    "end": "1434380"
  },
  {
    "text": "Somebody give me a number. Typically, what might you\nsee in a large server?",
    "start": "1434380",
    "end": "1440380"
  },
  {
    "text": "How much memory? Gigabytes. Gigabytes. Terabytes. Terabytes.",
    "start": "1440380",
    "end": "1446510"
  },
  {
    "text": "So maybe one half to two\nterabytes of main memory on a big server. So there are a lot of studies\nshowing how Spark could be used,",
    "start": "1446510",
    "end": "1456590"
  },
  {
    "text": "but the data sizes\nweren't really big enough. 5.7 gigabytes for this Twitter\ngraph, and 14.72 gigabytes",
    "start": "1456590",
    "end": "1467530"
  },
  {
    "text": "for this synthetic graph. And so if you can fit\nyour data in memory,",
    "start": "1467530",
    "end": "1475210"
  },
  {
    "text": "then you don't want to use the\ndistributed system to operate",
    "start": "1475210",
    "end": "1480520"
  },
  {
    "text": "on it because you're going\nto have a lot of overheads. And so that's what\nthis table is showing.",
    "start": "1480520",
    "end": "1486860"
  },
  {
    "text": "It's showing that for 20\niterations of PageRank on these graphs,\nwhich fit in memory,",
    "start": "1486860",
    "end": "1493519"
  },
  {
    "text": "you can run Spark on 128\ncores, and you're still",
    "start": "1493520",
    "end": "1502030"
  },
  {
    "text": "two times slower than\nrunning on a single thread.",
    "start": "1502030",
    "end": "1510820"
  },
  {
    "text": "So clearly, if the\nsize of your data",
    "start": "1510820",
    "end": "1517299"
  },
  {
    "text": "does not demand using\na distributed system, then clearly you\ndon't want to use it.",
    "start": "1517300",
    "end": "1523622"
  },
  {
    "text": "And so there's a\nresearcher called",
    "start": "1523622",
    "end": "1529450"
  },
  {
    "text": "Frank McSherry who kind really\ntook aim at the distributed",
    "start": "1529450",
    "end": "1535750"
  },
  {
    "text": "systems people. He has this quote he says,\npublished work on big data",
    "start": "1535750",
    "end": "1540760"
  },
  {
    "text": "systems has fetishized\nscalability over everything else, and basically\nhe says, they've",
    "start": "1540760",
    "end": "1547120"
  },
  {
    "text": "been creating all\nthese overheads and then coming\nup with mechanisms",
    "start": "1547120",
    "end": "1553180"
  },
  {
    "text": "to reduce the overheads\nwhich they have created. ",
    "start": "1553180",
    "end": "1558639"
  },
  {
    "text": "And he's arguing for\nlet's look at performance",
    "start": "1558640",
    "end": "1563830"
  },
  {
    "text": "as the metric instead\nof just scalability. So it's an important\npoint, but the point",
    "start": "1563830",
    "end": "1569530"
  },
  {
    "text": "is that you don't want\nto use these distributed systems if the size of your\ndata doesn't call for it.",
    "start": "1569530",
    "end": "1576140"
  },
  {
    "text": "So if you've got hundreds\nof terabytes of data, then the only way that you're\ngoing to be able to process it",
    "start": "1576140",
    "end": "1581380"
  },
  {
    "text": "is by using a\ndistributed system. But if you just have less\nthan a terabyte of data,",
    "start": "1581380",
    "end": "1588380"
  },
  {
    "text": "then a single system is going\nto be much more efficient. So keep that in mind.",
    "start": "1588380",
    "end": "1593950"
  },
  {
    "text": "Any questions? Yeah. What is scale out?",
    "start": "1593950",
    "end": "1600720"
  },
  {
    "text": "Scale out is as we've\nbeen describing, where you've got individual\nservers connected by a network.",
    "start": "1600720",
    "end": "1608190"
  },
  {
    "text": "So there's two\ndimensions to scale. People say scale out is when you\nare connecting nodes together",
    "start": "1608190",
    "end": "1618420"
  },
  {
    "text": "by a network, and\nscale up is when you are connecting\ncores together",
    "start": "1618420",
    "end": "1624539"
  },
  {
    "text": "in a shared memory system. So until now, we've been-- well, until the\ndiscussion of Spark,",
    "start": "1624540",
    "end": "1630220"
  },
  {
    "text": "we've been basically\ntalking about scale up where we are thinking about how\nto program multiple cores that",
    "start": "1630220",
    "end": "1639240"
  },
  {
    "text": "are sharing a memory. So if you share a\nmemory, it scale up. If you're not sharing\nmemory, it scale out.",
    "start": "1639240",
    "end": "1645950"
  },
  {
    "text": " All right. So now let's change our topic to\nthe main topic of today, which",
    "start": "1645950",
    "end": "1659500"
  },
  {
    "text": "is cache coherence, which\nis a very important topic because it both has\nperformance ramifications,",
    "start": "1659500",
    "end": "1668510"
  },
  {
    "text": "and it has correctness\nramifications, and it's important from the\npoint of view of software",
    "start": "1668510",
    "end": "1676179"
  },
  {
    "text": "developers because\nyou need to think about how you write your\nprograms in the face of cache",
    "start": "1676180",
    "end": "1683770"
  },
  {
    "text": "coherence. So how many people here have\nheard of cache coherence? Good.",
    "start": "1683770",
    "end": "1689250"
  },
  {
    "text": "How many people here know about\ncache coherence protocols? OK, good.",
    "start": "1689250",
    "end": "1695150"
  },
  {
    "text": "All right, well, then you\ncan help me teach this class. All right, so cache coherence.",
    "start": "1695150",
    "end": "1701210"
  },
  {
    "text": "So if you look at\na modern processor, like the ones in\nyour myth machine,",
    "start": "1701210",
    "end": "1706320"
  },
  {
    "text": "you'll see that large\nfraction of the chip is cache,",
    "start": "1706320",
    "end": "1712259"
  },
  {
    "text": "30% or more is cache,\nand we've talked about the importance\nof caches and locality",
    "start": "1712260",
    "end": "1719180"
  },
  {
    "text": "in getting performance\nbecause, of course, if you have to go outside the\nchip to access the data that you",
    "start": "1719180",
    "end": "1728480"
  },
  {
    "text": "need, it's going to take you 100\nor maybe on very modern chips",
    "start": "1728480",
    "end": "1734660"
  },
  {
    "text": "several hundred cycles. And if the CPU is stalled\nwhile this is happening, then,",
    "start": "1734660",
    "end": "1744560"
  },
  {
    "text": "of course, there's a lot\nof time that you waste and your performance is\nnot going to be that good.",
    "start": "1744560",
    "end": "1749840"
  },
  {
    "text": "All right. So let's return to the caverns\nfirst or second lecture,",
    "start": "1749840",
    "end": "1756870"
  },
  {
    "text": "which is on this cache example. And so we're looking at an\narray of 16 values in memory,",
    "start": "1756870",
    "end": "1765320"
  },
  {
    "text": "and we said that we were\ngoing to divide the memory",
    "start": "1765320",
    "end": "1770509"
  },
  {
    "text": "addresses into cache lines. So cache lines are multiple\nbytes or multiple words that",
    "start": "1770510",
    "end": "1779030"
  },
  {
    "text": "are contiguous or consecutive\naddresses in memory,",
    "start": "1779030",
    "end": "1784216"
  },
  {
    "text": "and lots of reasons that you\nwant to have cache lines,",
    "start": "1784216",
    "end": "1789330"
  },
  {
    "text": "but one of them, we said,\nwhat was one of the reasons that you wanted to cache lines?",
    "start": "1789330",
    "end": "1795640"
  },
  {
    "text": "Yeah. Spatial locality. To exploit spatial locality. That's one of the reasons\nyou want cache lines.",
    "start": "1795640",
    "end": "1801490"
  },
  {
    "text": "The other reason you\nwant cache lines is it helps you do the implementation\nof cache coherency",
    "start": "1801490",
    "end": "1808420"
  },
  {
    "text": "more efficiently,\nand it makes use of the data paths\nwithin the memory system",
    "start": "1808420",
    "end": "1814179"
  },
  {
    "text": "because you move whole cache\nlines and moving things in bulk is more efficient than\nmoving things one at a time.",
    "start": "1814180",
    "end": "1821970"
  },
  {
    "text": "All right. So we talked about-- we defined what was a cold miss. ",
    "start": "1821970",
    "end": "1829332"
  },
  {
    "text": "It's called miss. Somebody back. Yeah. [INAUDIBLE] it's for the first\ntime you have loaded anything--",
    "start": "1829332",
    "end": "1834539"
  },
  {
    "text": "The first time that you have\naccess and address it cannot be in the cache.",
    "start": "1834540",
    "end": "1839710"
  },
  {
    "text": "So the cache is said\nto be cold as far as that address is concerned. So it's a cold miss.",
    "start": "1839710",
    "end": "1845710"
  },
  {
    "text": "All right. So then we get access for,\nand we get another cache miss.",
    "start": "1845710",
    "end": "1854049"
  },
  {
    "text": "We talked about\nspatial locality. So what was on the topic\nof spatial locality?",
    "start": "1854050",
    "end": "1861258"
  },
  {
    "text": "Architects think about\nhardware mechanisms to exploit program behavior.",
    "start": "1861258",
    "end": "1868590"
  },
  {
    "text": "So what kind of program behavior\nleads to spatial locality?",
    "start": "1868590",
    "end": "1874279"
  },
  {
    "text": "Yeah. Sequential access. Sequential access. Where?",
    "start": "1874280",
    "end": "1879735"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "1879735",
    "end": "1885910"
  },
  {
    "text": "Exactly. So that's sequential. Where's the other place that\nyou see spatial locality?",
    "start": "1885910",
    "end": "1892030"
  },
  {
    "text": "Yeah. In the instructions-- In the instruction\naccess stream. So both in data and\ninstruction, you're",
    "start": "1892030",
    "end": "1897669"
  },
  {
    "text": "going to see spatial\nlocality and it's exploited",
    "start": "1897670",
    "end": "1905320"
  },
  {
    "text": "by having a cache line. So we said coldness\nthen temporal locality",
    "start": "1905320",
    "end": "1911020"
  },
  {
    "text": "is the other type of locality. Where might this exhibit\nitself in program behavior?",
    "start": "1911020",
    "end": "1920480"
  },
  {
    "text": "Temporal locality. Yeah. The counter variable\n[INAUDIBLE]. Yeah. The counter variable\nyou keep accessing.",
    "start": "1920480",
    "end": "1926880"
  },
  {
    "text": "Maybe stack accesses when you're\ndoing a recursive function.",
    "start": "1926880",
    "end": "1934126"
  },
  {
    "text": "So repeated access to the same\naddress as temporal locality.",
    "start": "1934126",
    "end": "1940256"
  },
  {
    "text": "So we said that we\nhad cold misses, and then at some\npoint in this cache,",
    "start": "1940256",
    "end": "1950580"
  },
  {
    "text": "we needed to replace something\nbecause we had run out",
    "start": "1950580",
    "end": "1957950"
  },
  {
    "text": "of places in our cache. And I think this was\nmislabeled a conflict miss,",
    "start": "1957950",
    "end": "1964380"
  },
  {
    "text": "but it was actually\ncalled a capacity miss. If we had a bigger cache, we\ncould contain three or four",
    "start": "1964380",
    "end": "1975890"
  },
  {
    "text": "cache lines, but we can\nonly have two cache lines, and so we have to replace one\nand we have a capacity miss.",
    "start": "1975890",
    "end": "1984590"
  },
  {
    "text": "So we have two types of misses,\ncold misses and capacity misses.",
    "start": "1984590",
    "end": "1990679"
  },
  {
    "text": "And now let's talk about\na third type of miss.",
    "start": "1990680",
    "end": "1996270"
  },
  {
    "text": "There's a miss model\nthat was created by a researcher named\nMark Hill, and it's",
    "start": "1996270",
    "end": "2003880"
  },
  {
    "text": "called the three Cs model. So you've heard about cold,\nyou've heard about capacity,",
    "start": "2003880",
    "end": "2009600"
  },
  {
    "text": "Now let's talk about the\nloss, which is conflict. So in order to talk\nabout conflict,",
    "start": "2009600",
    "end": "2015990"
  },
  {
    "text": "let me introduce roughly the\ndesign of the Intel Skylake",
    "start": "2015990",
    "end": "2024140"
  },
  {
    "text": "chip, which is the chip\nin the myth machines. So it's got three\nlevels of cache--",
    "start": "2024140",
    "end": "2031730"
  },
  {
    "text": "L1 data cache,\nwhich is 32 in size,",
    "start": "2031730",
    "end": "2038690"
  },
  {
    "text": "and L2 cache, which is\nper core, so four cores.",
    "start": "2038690",
    "end": "2046340"
  },
  {
    "text": "The data cache is\nprivate per core, the L2 cache is\nprivate per core.",
    "start": "2046340",
    "end": "2051480"
  },
  {
    "text": "And then there's a\nring interconnect, which is, as you\nmight expect, is a ring, that connects all of the\ndifferent L2 caches together,",
    "start": "2051480",
    "end": "2062899"
  },
  {
    "text": "and they connect to\na shared L3 cache, which is 8 megabytes in size.",
    "start": "2062900",
    "end": "2071840"
  },
  {
    "text": "So what do we mean\nby a conflict miss? Well, it turns out that in order\nto simplify how you find things",
    "start": "2071840",
    "end": "2083949"
  },
  {
    "text": "in caches, you limit\nthe number of places that any line could go.",
    "start": "2083949",
    "end": "2089830"
  },
  {
    "text": "This is called associativity. So the L1 cache here\nin the Skylake chip",
    "start": "2089830",
    "end": "2099580"
  },
  {
    "text": "is eight-way set associative. And that basically\nsays that I only need to look in eight places\nfor any particular line.",
    "start": "2099580",
    "end": "2112700"
  },
  {
    "text": "So in this case, our\nline size is 64 bytes.",
    "start": "2112700",
    "end": "2122530"
  },
  {
    "text": "So if I have a\n64-byte line size, a favorite thing to do is cache\narithmetic, right which says,",
    "start": "2122530",
    "end": "2133980"
  },
  {
    "text": "OK, if I've got 64\nbytes in my cache line, how many lines do I have\nin my 32 kilobyte cache?",
    "start": "2133980",
    "end": "2141980"
  },
  {
    "start": "2141980",
    "end": "2147320"
  },
  {
    "text": "512. 512. 512 lines.",
    "start": "2147320",
    "end": "2154299"
  },
  {
    "text": "So if I have 512\nlines in my cache,",
    "start": "2154300",
    "end": "2160920"
  },
  {
    "text": "then a fully general way of\nfinding lines in my cache,",
    "start": "2160920",
    "end": "2167589"
  },
  {
    "text": "I would have to look in 512\nplaces at the same time.",
    "start": "2167590",
    "end": "2173130"
  },
  {
    "text": "This is expensive. And so to make it\nless expensive, I'm going to limit\nit to eight places.",
    "start": "2173130",
    "end": "2180869"
  },
  {
    "text": "However, the difference\nbetween having a cache where I can look in\n512 places versus eight places",
    "start": "2180870",
    "end": "2189089"
  },
  {
    "text": "is going to lead to more misses. These misses are\ncalled conflict misses.",
    "start": "2189090",
    "end": "2195030"
  },
  {
    "text": "So conflict misses\nare the extra misses you get because\nyour cache is not",
    "start": "2195030",
    "end": "2201150"
  },
  {
    "text": "what is called fully\nset associative, which means a line could go\nanywhere in the cache.",
    "start": "2201150",
    "end": "2207450"
  },
  {
    "text": "And this eight way set\nassociative cache can only go into one of eight places.",
    "start": "2207450",
    "end": "2214369"
  },
  {
    "text": "So now you know about cold\ncapacity and conflict. Yeah. [INAUDIBLE] you're going to\nrestore lines in your cache.",
    "start": "2214370",
    "end": "2222270"
  },
  {
    "text": "No, no. It just saying that\nthere's only eight places any particular address could go.",
    "start": "2222270",
    "end": "2227560"
  },
  {
    "text": "So I don't have to look\nall over the cache, I only have to look\nin eight places. Turns out it's cheaper to look\nin eight places than 512 places.",
    "start": "2227560",
    "end": "2237980"
  },
  {
    "text": "That's the intuition. We could go into details,\nbut that's the intuition. Yeah. If you have this eight-way\nsubstance here, does that mean",
    "start": "2237980",
    "end": "2244869"
  },
  {
    "text": "it's [INAUDIBLE]. Yeah, that's a good\nway to think about it. Would it be rare for it\nto get to 100% utilization",
    "start": "2244870",
    "end": "2252980"
  },
  {
    "text": "if that's the case? You mean some of those buckets\ncould be underutilized? Yeah. Yeah, potentially.",
    "start": "2252980",
    "end": "2258830"
  },
  {
    "text": "Yeah, but usually not. One question is [INAUDIBLE]. So basically you're saying\nthat the cache itself",
    "start": "2258830",
    "end": "2265240"
  },
  {
    "text": "is kind of free, like\nyou're kind of branching out and then you have things\ninside [INAUDIBLE].",
    "start": "2265240",
    "end": "2272490"
  },
  {
    "text": "There are only eight\nbuckets, and basically you look at the address,\nyou say, which bucket",
    "start": "2272490",
    "end": "2277800"
  },
  {
    "text": "are you going to go in. So when you have larger\nand larger caches, you have multiple\nlevels of these buckets?",
    "start": "2277800",
    "end": "2284250"
  },
  {
    "text": "No, no. [INAUDIBLE] you don't need them? No, no. You're still going to do it.",
    "start": "2284250",
    "end": "2290580"
  },
  {
    "text": "You're still going to\nlook and have a way of-- usually you just have one\nlevel of set associativity.",
    "start": "2290580",
    "end": "2298030"
  },
  {
    "text": "You don't have multiple\nlevels, unless you've got another level of cache. ",
    "start": "2298030",
    "end": "2306240"
  },
  {
    "text": "This is kind of weird\nactually, because what you'd expect is\nas you get bigger,",
    "start": "2306240",
    "end": "2311829"
  },
  {
    "text": "the set associativity\nshould increase. But it starts with 8, goes\nto 4 and then goes to 16.",
    "start": "2311830",
    "end": "2319550"
  },
  {
    "text": "It's kind of strange. I don't know why the\ndesigners did it that way, but that's not what you\nwould typically teach.",
    "start": "2319550",
    "end": "2325870"
  },
  {
    "text": "You'd say, hey,\nas you get bigger, you get more set\nassociative because you're",
    "start": "2325870",
    "end": "2331230"
  },
  {
    "text": "trying to-- because remember,\nhigher set associativity",
    "start": "2331230",
    "end": "2338730"
  },
  {
    "text": "means lower conflict miss rate. Because as you increase\nthe set associativity,",
    "start": "2338730",
    "end": "2344859"
  },
  {
    "text": "you're going to\ndecrease the miss rate.  All right.",
    "start": "2344860",
    "end": "2350019"
  },
  {
    "text": "So let's talk\nabout cache design. So you've got a\nline in the cache.",
    "start": "2350020",
    "end": "2359380"
  },
  {
    "text": "There are two\npieces to the line. One is the data that is going\nto be contained in the cache,",
    "start": "2359380",
    "end": "2369380"
  },
  {
    "text": "and the rest is the metadata,\nwhich basically tells you",
    "start": "2369380",
    "end": "2374990"
  },
  {
    "text": "about the context\nof the cache line. So a key part of the metadata\nis the tag, which is essentially",
    "start": "2374990",
    "end": "2385520"
  },
  {
    "text": "the address of the data.",
    "start": "2385520",
    "end": "2390800"
  },
  {
    "text": "So you've got this\ndata from memory, and you need to know\nwhich addresses--",
    "start": "2390800",
    "end": "2398210"
  },
  {
    "text": "which cache line-- sorry, which\nmemory line is in the cache.",
    "start": "2398210",
    "end": "2403911"
  },
  {
    "text": "So the tag is going\nto tell you that.  And then there's\na dirty bit, which",
    "start": "2403912",
    "end": "2411700"
  },
  {
    "text": "tells you about whether the data\nin the cache has been modified",
    "start": "2411700",
    "end": "2419920"
  },
  {
    "text": "or not. And so in this\nexample, we are going to write one to integer x which\nexists at the address shown",
    "start": "2419920",
    "end": "2431770"
  },
  {
    "text": "there, x12345604.",
    "start": "2431770",
    "end": "2437800"
  },
  {
    "text": "And the 4 indicates that\nit's the fourth element in--",
    "start": "2437800",
    "end": "2445940"
  },
  {
    "text": "that's the address of\nthe byte in the line.  So reading from caches is\nfairly straightforward.",
    "start": "2445940",
    "end": "2456730"
  },
  {
    "text": "Writing is a little\nmore complicated because writing is always\na little more complicated. So there's two types of writing.",
    "start": "2456730",
    "end": "2466743"
  },
  {
    "text": "Can anybody tell\nme the difference between a write back cache\nand a write through cache? Yeah.",
    "start": "2466743",
    "end": "2472809"
  },
  {
    "text": "So the write through\ncache [INAUDIBLE] and then write back cache\nonly update [INAUDIBLE]",
    "start": "2472810",
    "end": "2481370"
  },
  {
    "text": "postpone [INAUDIBLE]\ndistinction. Exactly. So write through says when\nwe do the right to the cache,",
    "start": "2481370",
    "end": "2490759"
  },
  {
    "text": "we also write to the main\nmemory, and write back says we only write to the cache,\nand later the data is actually",
    "start": "2490760",
    "end": "2499930"
  },
  {
    "text": "written to main memory\nbecause after all, you want to write to the main memory. That's the whole goal.",
    "start": "2499930",
    "end": "2506050"
  },
  {
    "text": "The cache is just this\nintermediate storage buffer. What about write allocate\nversus no write allocate?",
    "start": "2506050",
    "end": "2515079"
  },
  {
    "text": "Does anybody know what that is? Yeah. [INAUDIBLE]",
    "start": "2515080",
    "end": "2520540"
  },
  {
    "start": "2520540",
    "end": "2528350"
  },
  {
    "text": "So the question is, what happens\nwhen I write to the cache,",
    "start": "2528350",
    "end": "2533410"
  },
  {
    "text": "and the line that I\nwant is not there? Do I allocate?",
    "start": "2533410",
    "end": "2542340"
  },
  {
    "text": "Do I actually fetch\nthe rest of the line and write into the cache?",
    "start": "2542340",
    "end": "2548430"
  },
  {
    "text": "Or do I just write\ndirectly to main memory? So with that in mind, let's\nlook at an example where write",
    "start": "2548430",
    "end": "2557910"
  },
  {
    "text": "allocate write back\ncache on a write miss, and I'm going to\nwrite 1 to the address X.",
    "start": "2557910",
    "end": "2567280"
  },
  {
    "text": "So what happens? So the process performs a write,\nbut it misses in the cache.",
    "start": "2567280",
    "end": "2573490"
  },
  {
    "text": "The cache selects a location. If there's a dirty line\ncurrently in the location, then,",
    "start": "2573490",
    "end": "2578800"
  },
  {
    "text": "of course. What does the dirty bit indicate\nabout the data in the cache",
    "start": "2578800",
    "end": "2586190"
  },
  {
    "text": "line. yeah. It's different than\nwhat's in the memory. It's different than\nwhat's in memory. It may be the only\nplace that this data",
    "start": "2586190",
    "end": "2593450"
  },
  {
    "text": "exists in the memory system. Should we lose it? No, we should not lose it. And to make sure we don't lose\nit, we indicate that it's dirty,",
    "start": "2593450",
    "end": "2602490"
  },
  {
    "text": "and then when we\nreplace the line, we know that we can't\njust drop it on the floor,",
    "start": "2602490",
    "end": "2608069"
  },
  {
    "text": "that we need to write that\ndata back to main memory. And so that's what we do. And then since this\nis write allocate,",
    "start": "2608070",
    "end": "2617309"
  },
  {
    "text": "we need to allocate the line. We need to go get the\ndata from the main memory,",
    "start": "2617310",
    "end": "2624600"
  },
  {
    "text": "but we know that the data in the\nmain memory is not up to date. So of course, once we bring\nthat data into the cache,",
    "start": "2624600",
    "end": "2632760"
  },
  {
    "text": "we then write the\ndata, in this case 1,",
    "start": "2632760",
    "end": "2641590"
  },
  {
    "text": "and then we set\nthe dirty bit to 1.",
    "start": "2641590",
    "end": "2648930"
  },
  {
    "text": "Any questions? Yeah. Can you just explain\nagain to the write back",
    "start": "2648930",
    "end": "2656330"
  },
  {
    "text": "versus write through? Yes, write back\nversus write through. So right back is this\nexample where we had",
    "start": "2656330",
    "end": "2666350"
  },
  {
    "text": "a miss in the cache and we-- ",
    "start": "2666350",
    "end": "2672440"
  },
  {
    "text": "so write back versus\nwrite through--",
    "start": "2672440",
    "end": "2677510"
  },
  {
    "text": "I was thinking allocate. So write back versus\nwrite through. So write back says I'm going\nto write into the cache",
    "start": "2677510",
    "end": "2684769"
  },
  {
    "text": "and save the dirty bit,\nand then later, that data",
    "start": "2684770",
    "end": "2689900"
  },
  {
    "text": "is going to be written to memory\nwhen the line is replaced. That's right back. Write through says, when\nI write into the cache,",
    "start": "2689900",
    "end": "2696960"
  },
  {
    "text": "I also write to memory. And so that means I\ndon't need a dirty bit,",
    "start": "2696960",
    "end": "2702980"
  },
  {
    "text": "because I know memory is\nup to date at that point. ",
    "start": "2702980",
    "end": "2710410"
  },
  {
    "text": "All right. So the shared\nmemory abstraction. We said that essentially\nwhat we're going to do",
    "start": "2710410",
    "end": "2717280"
  },
  {
    "text": "is we're going to share-- we're going to\ncommunicate-- and yeah. Question.",
    "start": "2717280",
    "end": "2724000"
  },
  {
    "text": "I had a few questions. Just what was step four\nagain for cache line,",
    "start": "2724000",
    "end": "2729470"
  },
  {
    "text": "especially for each\nof its updated-- Yeah. So remember, it's allocate. So allocate means\nthat we want to--",
    "start": "2729470",
    "end": "2736750"
  },
  {
    "text": "we're on a miss. We are going to allocate\na line in the cache. So of course, we only\nhave 32 bits of data",
    "start": "2736750",
    "end": "2745210"
  },
  {
    "text": "that we're writing\nfrom the processor. So what about the other words\nor bytes in the cache line?",
    "start": "2745210",
    "end": "2753430"
  },
  {
    "text": "Those have to be updated. Those have to be valid. So we have to go and get\nthe data from main memory.",
    "start": "2753430",
    "end": "2761440"
  },
  {
    "text": "Essentially, you perform the\nread as if you were doing a read miss, so you get the full\ncontext of the cache line,",
    "start": "2761440",
    "end": "2771700"
  },
  {
    "text": "and then you update the\nparticular word that's",
    "start": "2771700",
    "end": "2779560"
  },
  {
    "text": "being written by the store. [INAUDIBLE] as for the\nupdate, and is just",
    "start": "2779560",
    "end": "2789630"
  },
  {
    "text": "the load into the cache. Yeah. ",
    "start": "2789630",
    "end": "2797490"
  },
  {
    "text": "Loads line from memory. It seems a little redundant.",
    "start": "2797490",
    "end": "2803410"
  },
  {
    "text": " [INAUDIBLE] ",
    "start": "2803410",
    "end": "2809360"
  },
  {
    "text": "So loads live from memory. OK. So maybe we do this\nto make it right.",
    "start": "2809360",
    "end": "2817495"
  },
  {
    "text": " Is that better. ",
    "start": "2817495",
    "end": "2825260"
  },
  {
    "text": "What was the tag again. Sorry, I just missed it. What was this? Tag? Yes. So tag essentially is the\naddress of X, goes in here.",
    "start": "2825260",
    "end": "2836595"
  },
  {
    "text": "Address.  So remember, how do you\nknow what's in the cache?",
    "start": "2836595",
    "end": "2842715"
  },
  {
    "text": " It can't have all the\ncontents of memory.",
    "start": "2842715",
    "end": "2850630"
  },
  {
    "text": "So you need to tag\nthe cache lines and to tell the system\nwhat data is in the cache.",
    "start": "2850630",
    "end": "2861308"
  },
  {
    "text": "Is there anything other than\nthe dirty bit [INAUDIBLE] state? Yeah, we'll get to that.",
    "start": "2861308",
    "end": "2866670"
  },
  {
    "text": "That's the topic of the\nrest of the lecture. Is the tag the address\nof [INAUDIBLE]?",
    "start": "2866670",
    "end": "2875089"
  },
  {
    "text": "What do you think?  No, it's the address\nof the memory.",
    "start": "2875090",
    "end": "2882789"
  },
  {
    "text": "The address of the cache\nis kind of unimportant. It's an array, but what's--",
    "start": "2882790",
    "end": "2890900"
  },
  {
    "text": "the way to really\nthink about caches is the content\naddressable arrays.",
    "start": "2890900",
    "end": "2898852"
  },
  {
    "text": "And the contents that\nyou're after is the tag.",
    "start": "2898852",
    "end": "2904631"
  },
  {
    "text": "You're saying given\nan address that gets generated\nfrom the processor,",
    "start": "2904631",
    "end": "2910250"
  },
  {
    "text": "does the cache contain it? So essentially, the action\nthat's going to happen",
    "start": "2910250",
    "end": "2917150"
  },
  {
    "text": "is I'm going to take the\naddress that was generated from the processor,\nand I'm going to compare it across all\nof the tags in my cache.",
    "start": "2917150",
    "end": "2925900"
  },
  {
    "text": "And if any of them\nmatch or typically one, not more than one,\nthen I'm going to say, hey,",
    "start": "2925900",
    "end": "2933990"
  },
  {
    "text": "I've got a hit. So do I really care about\nthe particular address",
    "start": "2933990",
    "end": "2939329"
  },
  {
    "text": "of the location in the cache? Not really. I only care that it has a tag\nfor the data that I'm after.",
    "start": "2939330",
    "end": "2949770"
  },
  {
    "text": "Yeah. But if you address like memory\nthat's right adjacent to the one in the tag, that might\nstill be in the cache line,",
    "start": "2949770",
    "end": "2956530"
  },
  {
    "text": "by the way, because it's a\ncontiguous bit of memory. Yeah.",
    "start": "2956530",
    "end": "2962645"
  },
  {
    "text": "I've kind of glossed over\na bit in that this is not the address of X, it's the\naddress of the cache line",
    "start": "2962645",
    "end": "2972688"
  },
  {
    "text": "because if it were\nthe address of X, then how would I be able to\nexploit spatial locality?",
    "start": "2972688",
    "end": "2980000"
  },
  {
    "text": "So every address that\nfalls into this cache line",
    "start": "2980000",
    "end": "2990380"
  },
  {
    "text": "is going to be\nrepresented by the tag. ",
    "start": "2990380",
    "end": "2996002"
  },
  {
    "text": "And you basically you just\ndrop off bits from the address in order to get the address\nthat actually goes in there.",
    "start": "2996002",
    "end": "3005070"
  },
  {
    "text": "So is it that-- just going back to [INAUDIBLE]\nlike a particular session",
    "start": "3005070",
    "end": "3011520"
  },
  {
    "text": "of tags are supposed to\ngo in particular bucket? I said, we could go into\ndetails here, but believe me,",
    "start": "3011520",
    "end": "3018868"
  },
  {
    "text": "they're not necessary. We can talk about it later. Yes, there's this\nwhole set of things",
    "start": "3018868",
    "end": "3024480"
  },
  {
    "text": "that you have to do to\nactually get the data, and actually do a cache lookup. And that has to do\nwith set associativity.",
    "start": "3024480",
    "end": "3031240"
  },
  {
    "text": "But as I've explained it here,\nwe won't go into the details,",
    "start": "3031240",
    "end": "3038880"
  },
  {
    "text": "but the thing to remember is\nthat higher set associativity will give you lower miss rates,\nand that the higher the set",
    "start": "3038880",
    "end": "3049560"
  },
  {
    "text": "associativity,\nthe more difficult it is to do the lookup.",
    "start": "3049560",
    "end": "3055490"
  },
  {
    "text": "All right, good. So we said essentially that\nshared memory programming,",
    "start": "3055490",
    "end": "3064800"
  },
  {
    "text": "thread-based programming is\nwe're going to share addresses, we're going to make sure\nthat we access the data when",
    "start": "3064800",
    "end": "3073670"
  },
  {
    "text": "we want to properly\nusing synchronization. And now we want to talk about\nhow we expect the shared memory",
    "start": "3073670",
    "end": "3083390"
  },
  {
    "text": "multiprocessor to behave. So we're going to read and\nwrite to shared variables by--",
    "start": "3083390",
    "end": "3091670"
  },
  {
    "text": "the processor is going to\nissue loads and stores, and so if I said, hey, here\nare a bunch of processes,",
    "start": "3091670",
    "end": "3101760"
  },
  {
    "text": "they communicate over some\ninterconnect to memory,",
    "start": "3101760",
    "end": "3107930"
  },
  {
    "text": "how do you expect these\nprocesses to behave? And your intuitive\nanswer would be something",
    "start": "3107930",
    "end": "3114320"
  },
  {
    "text": "like, well, if I store\na value to a variable X,",
    "start": "3114320",
    "end": "3124500"
  },
  {
    "text": "on one processor and I load that\nvalue from another processor,",
    "start": "3124500",
    "end": "3129790"
  },
  {
    "text": "I should get the last value that\nwas stored to that variable.",
    "start": "3129790",
    "end": "3135772"
  },
  {
    "text": "And so that's kind\nof intuitively what you'd expect a shared\nmemory multiprocessor the way",
    "start": "3135772",
    "end": "3143790"
  },
  {
    "text": "you'd expect it to behave. So the problem is that\nonce you introduce caches",
    "start": "3143790",
    "end": "3150270"
  },
  {
    "text": "into the system, now you\nhave more than one place that the data or the addresses\nin the memory system can be.",
    "start": "3150270",
    "end": "3163325"
  },
  {
    "text": " And if you have any\nparticular memory location,",
    "start": "3163325",
    "end": "3171930"
  },
  {
    "text": "you can now get into trouble. So in this example,\nwe've got these processes",
    "start": "3171930",
    "end": "3178710"
  },
  {
    "text": "with their individual\nprivate caches, and they're connected via an\ninterconnect to the main memory,",
    "start": "3178710",
    "end": "3186420"
  },
  {
    "text": "and we have a variable foo\nwhich is stored at address X.",
    "start": "3186420",
    "end": "3192230"
  },
  {
    "text": "And so let's track\nhow the processes",
    "start": "3192230",
    "end": "3197990"
  },
  {
    "text": "access this variable or the\nvariable foo at address X.",
    "start": "3197990",
    "end": "3204600"
  },
  {
    "text": "So processor 1 loads X\nand it gets a cache miss,",
    "start": "3204600",
    "end": "3209870"
  },
  {
    "text": "and so it goes to memory\nand gets the value. And so now there's 0\nin processor 1's cache.",
    "start": "3209870",
    "end": "3218130"
  },
  {
    "text": "So abbreviation of cache\nis, of course dollar sign.",
    "start": "3218130",
    "end": "3223388"
  },
  {
    "text": "And then processor 2 does the\nsame thing and also gets a cache miss and gets the value 0.",
    "start": "3223388",
    "end": "3231289"
  },
  {
    "text": "Now processor 1 does a store\nto X says, hey, cache hit.",
    "start": "3231290",
    "end": "3238120"
  },
  {
    "text": "I'm going to update\nthe write back cache. I'm going to update\nmy value of X to 1.",
    "start": "3238120",
    "end": "3247804"
  },
  {
    "text": "Then processor 3\ndoes a load of X and it gets a cache\nmiss, goes to memory.",
    "start": "3247804",
    "end": "3256680"
  },
  {
    "text": "It also has a value of 0. ",
    "start": "3256680",
    "end": "3263910"
  },
  {
    "text": "Now processor 3\ndoes a store of X,",
    "start": "3263910",
    "end": "3270730"
  },
  {
    "text": "and it says, hey, cache\nhit and now the value is 2.",
    "start": "3270730",
    "end": "3276500"
  },
  {
    "text": "And processor 2 does a load\nof X and gets the value of 0",
    "start": "3276500",
    "end": "3283780"
  },
  {
    "text": "because it's got it in its\ncache, and it says a cache hit, and then processor 1 does a load\nof Y. Let's suppose we only have",
    "start": "3283780",
    "end": "3293950"
  },
  {
    "text": "a single entry in our cache. So we have a capacity\nmiss, and so it gets--",
    "start": "3293950",
    "end": "3303040"
  },
  {
    "start": "3303040",
    "end": "3308970"
  },
  {
    "text": "yeah, so it gets replaced. And so the value that it\nhad goes back to memory.",
    "start": "3308970",
    "end": "3317190"
  },
  {
    "text": "So now we have in\nour memory system. The value of X in memory is 1,\nthe value of X in processor 3's",
    "start": "3317190",
    "end": "3325530"
  },
  {
    "text": "cache is 2, the value of X\nin processor's 2 cache is 0,",
    "start": "3325530",
    "end": "3330900"
  },
  {
    "text": "and process of 1 does not have\nthe address X in the cache.",
    "start": "3330900",
    "end": "3338109"
  },
  {
    "text": "This looks like a disaster. ",
    "start": "3338110",
    "end": "3344440"
  },
  {
    "text": "So this is not a memory\nsystem that we could use.",
    "start": "3344440",
    "end": "3349640"
  },
  {
    "text": "And so the question is, could we\nfix this problem by using locks? ",
    "start": "3349640",
    "end": "3356190"
  },
  {
    "text": "No, no, we can't fix\nit by using locks, because fundamentally, the\nproblem is inherent to the fact",
    "start": "3356190",
    "end": "3364920"
  },
  {
    "text": "that we've got multiple\nplaces in the system that",
    "start": "3364920",
    "end": "3370170"
  },
  {
    "text": "have addresses that\ncorrespond to memory, and we have multiple places in\nthe system that are changing",
    "start": "3370170",
    "end": "3377850"
  },
  {
    "text": "those addresses with\nmultiple processes changing their private copies of the\naddresses in the memory system.",
    "start": "3377850",
    "end": "3387010"
  },
  {
    "text": "So how do we fix this problem? So we have a memory\ncoherence problem.",
    "start": "3387010",
    "end": "3395940"
  },
  {
    "text": "As I said, what you\nwant intuitively is that reading the\nvalue and address",
    "start": "3395940",
    "end": "3402210"
  },
  {
    "text": "X should return the\nlast value written by X, but because we've got the\nmain memory being replicated",
    "start": "3402210",
    "end": "3412950"
  },
  {
    "text": "by local storage\nand processor caches and then we have updates\nto this local copies,",
    "start": "3412950",
    "end": "3420240"
  },
  {
    "text": "sharing the same address\nspace, we get incoherence.",
    "start": "3420240",
    "end": "3425680"
  },
  {
    "text": "So you can get incoherence even\non a single CPU system whenever",
    "start": "3425680",
    "end": "3437720"
  },
  {
    "text": "you have places that-- have a situation where\nyou have multiple places",
    "start": "3437720",
    "end": "3443630"
  },
  {
    "text": "that you can read or write from. So for example, in a\ncase of an I/O card,",
    "start": "3443630",
    "end": "3457670"
  },
  {
    "text": "the I/O network\ncard delivers data into a message buffer\nusing direct memory access.",
    "start": "3457670",
    "end": "3467589"
  },
  {
    "text": "It could happen that the\naddresses in the message buffer are also in the cache.",
    "start": "3467590",
    "end": "3474559"
  },
  {
    "text": "They could be\nstale in the cache. So stale because they've\nbeen updated in memory,",
    "start": "3474560",
    "end": "3481619"
  },
  {
    "text": "and those updates have not\nbeen reflected in the cache.",
    "start": "3481620",
    "end": "3487140"
  },
  {
    "text": "So this happens rarely enough\nthat you could probably fix it using software mechanisms.",
    "start": "3487140",
    "end": "3494080"
  },
  {
    "text": "So you the software could flush\nall of the entries in the cache",
    "start": "3494080",
    "end": "3501660"
  },
  {
    "text": "corresponding to the addresses\nin the message buffer, for example. So you could solve\nit with software,",
    "start": "3501660",
    "end": "3508320"
  },
  {
    "text": "and this might be a reasonably\nperformance solution given",
    "start": "3508320",
    "end": "3513360"
  },
  {
    "text": "the frequency of\noccurrence of I/O. But if I'm actively sharing\nmemory with multiple processes,",
    "start": "3513360",
    "end": "3523370"
  },
  {
    "text": "doing things with software\nis not really a solution. ",
    "start": "3523370",
    "end": "3529210"
  },
  {
    "text": "So the question\nthen is we talked about this intuitive\nnotion that I should get the last\nvalue of any address that",
    "start": "3529210",
    "end": "3536800"
  },
  {
    "text": "was written by some other\nprocessor in the system. So the problem is, what\nexactly does lost mean?",
    "start": "3536800",
    "end": "3544579"
  },
  {
    "text": "So what if two processes\nwrite at the same time, who",
    "start": "3544580",
    "end": "3552760"
  },
  {
    "text": "should be the-- which\nprocessors should be the lost? What if a write from\nprocessor 1 is closely",
    "start": "3552760",
    "end": "3561490"
  },
  {
    "text": "followed by a read\nof processor 2 such that it doesn't have\ntime to get that value?",
    "start": "3561490",
    "end": "3568180"
  },
  {
    "text": "How do we make sure that this\nsituation works correctly? So in a sequential program, lost\nis determined by program order.",
    "start": "3568180",
    "end": "3578300"
  },
  {
    "text": "And so this is also\nthe way that we want to think about the\nordering in a thread",
    "start": "3578300",
    "end": "3583900"
  },
  {
    "text": "of a parallel program. And now we need to think about\nhow these threads values--",
    "start": "3583900",
    "end": "3592089"
  },
  {
    "text": "these thread updates are\ngoing to be interleaved. So the way to think\nabout coherence is--",
    "start": "3592090",
    "end": "3599829"
  },
  {
    "text": "coherence, remember,\nis just talking about a single memory location\nor maybe a single cache line.",
    "start": "3599830",
    "end": "3607930"
  },
  {
    "text": "So for any single\nmemory location, we want to serialize accesses\nto that location, such",
    "start": "3607930",
    "end": "3616599"
  },
  {
    "text": "that for any write, in this\ncase P0 writing value 5,",
    "start": "3616600",
    "end": "3627200"
  },
  {
    "text": "if in the serialization the P1\nreading comes off to the write,",
    "start": "3627200",
    "end": "3633270"
  },
  {
    "text": "then it should get the value 5,\nand P2 comes off to that write, so it should get the\nvalue 5, and so on.",
    "start": "3633270",
    "end": "3641010"
  },
  {
    "text": "All subsequent reads\nshould get the value 5 until P1 does a write of 25\nand then all subsequent reads",
    "start": "3641010",
    "end": "3649099"
  },
  {
    "text": "should get the value 25. So you have to be able\nto serialize access",
    "start": "3649100",
    "end": "3655670"
  },
  {
    "text": "to a particular address\nsuch that you can-- ",
    "start": "3655670",
    "end": "3662405"
  },
  {
    "text": "and the notion is that the\nreads and writes should--",
    "start": "3662405",
    "end": "3668030"
  },
  {
    "text": "for any particular thread\nshould occur in the order",
    "start": "3668030",
    "end": "3673040"
  },
  {
    "text": "issued by the processor.  And then the value\nthat you read is",
    "start": "3673040",
    "end": "3681510"
  },
  {
    "text": "given by the last write\nin the serial order. Does that all make sense?",
    "start": "3681510",
    "end": "3687910"
  },
  {
    "text": "So that's kind of what\nbehavior that we'd like to see.",
    "start": "3687910",
    "end": "3693599"
  },
  {
    "text": "And the question then is, how\ndo we get the serial order,",
    "start": "3693600",
    "end": "3699630"
  },
  {
    "text": "and how do we make sure that you\nget the right values from reads or writes?",
    "start": "3699630",
    "end": "3705099"
  },
  {
    "text": "So one way to\nprovide coherence is",
    "start": "3705100",
    "end": "3711120"
  },
  {
    "text": "to think about it in\nterms of invariance. So what invariance do I\nwant my system to hold?",
    "start": "3711120",
    "end": "3717820"
  },
  {
    "text": "And so the two invariants\nthat are important,",
    "start": "3717820",
    "end": "3723010"
  },
  {
    "text": "so for any address X or\nany cache line address X,",
    "start": "3723010",
    "end": "3728730"
  },
  {
    "text": "We want to have a single writer,\nmultiple reader invariant.",
    "start": "3728730",
    "end": "3733930"
  },
  {
    "text": "So this means that\nany point in time you are in a read-write\nepoch where there's",
    "start": "3733930",
    "end": "3742780"
  },
  {
    "text": "only one processor that can\nchange the state of that cache line.",
    "start": "3742780",
    "end": "3749510"
  },
  {
    "text": "So read-write epoch,\nonly one processor can change the state\nof the cache line,",
    "start": "3749510",
    "end": "3756280"
  },
  {
    "text": "or you're in a read-only epoch\nwhere any number of processors",
    "start": "3756280",
    "end": "3761950"
  },
  {
    "text": "can read that cache line. So single writer,\nmultiple reader.",
    "start": "3761950",
    "end": "3767010"
  },
  {
    "text": " That's the first invariant. The second invariant is called\nthe data value invariant,",
    "start": "3767010",
    "end": "3774970"
  },
  {
    "text": "and this just makes\nsure that the value that",
    "start": "3774970",
    "end": "3780859"
  },
  {
    "text": "was written in the\nlast read-write epoch is what every subsequent\nreader will see.",
    "start": "3780860",
    "end": "3791000"
  },
  {
    "text": "So in this example, address\nX, we have a read-write epoch.",
    "start": "3791000",
    "end": "3796610"
  },
  {
    "text": "P0 writes to address X, and then\nthe following read-only epoch,",
    "start": "3796610",
    "end": "3805710"
  },
  {
    "text": "the value that was written\nby P0 is the value that",
    "start": "3805710",
    "end": "3814030"
  },
  {
    "text": "is seen in the read only epoch. And then we have a read-write\nepoch where Only P1 can write,",
    "start": "3814030",
    "end": "3824750"
  },
  {
    "text": "and then the value\nthat is written by P1 or the last value written by.",
    "start": "3824750",
    "end": "3830599"
  },
  {
    "text": "P1 is the one that is seen in\nthe read only epoch following the read-write epoch.",
    "start": "3830600",
    "end": "3838329"
  },
  {
    "text": "Does that mean we avoid\nsome of flush or switching between read-only\nand read-write?",
    "start": "3838330",
    "end": "3844220"
  },
  {
    "text": "Yeah, well we will switch. We flush, but there may be\nslightly more efficient ways",
    "start": "3844220",
    "end": "3852430"
  },
  {
    "text": "of doing things,\nbut there has to be a mechanism of switching\nbetween read-write epochs",
    "start": "3852430",
    "end": "3859029"
  },
  {
    "text": "and read-only epochs. So that part of the\nintuition is exactly right. ",
    "start": "3859030",
    "end": "3866790"
  },
  {
    "text": "[INAUDIBLE] you just told me\nabout locks and [INAUDIBLE].",
    "start": "3866790",
    "end": "3875190"
  },
  {
    "text": "It feels if you have multiple\nprocessors accessing the same or writing to the same point,\nthe same address in memory,",
    "start": "3875190",
    "end": "3882940"
  },
  {
    "text": "you should anyways\nbe like having some kind of locking mechanism.",
    "start": "3882940",
    "end": "3888690"
  },
  {
    "text": "I guess I'm having trouble\nvisualizing why this is still a problem if when you're doing\nthese kinds of parallel writes--",
    "start": "3888690",
    "end": "3896890"
  },
  {
    "text": "Well, it is possible for\nme to say you can write",
    "start": "3896890",
    "end": "3903510"
  },
  {
    "text": "and he can write\nand you can write and you can-- can't all\ndo it at the same time. But you could still have an\nincoherent system in that he",
    "start": "3903510",
    "end": "3911250"
  },
  {
    "text": "could write to his\nown cache, and you could write to your\nown cache, and she could write to their\nown cache, and you still",
    "start": "3911250",
    "end": "3917820"
  },
  {
    "text": "have an incoherent system, even\nthough it was synchronized.",
    "start": "3917820",
    "end": "3924882"
  },
  {
    "text": "So the two issues\nare independent. One is a function of\nthe fact that you've",
    "start": "3924882",
    "end": "3931960"
  },
  {
    "text": "got two multiple places\nthat the data can exist, and that is kind of inherent\nin a cache-based multiprocessor",
    "start": "3931960",
    "end": "3941770"
  },
  {
    "text": "system in which you are\ncaching shared data. So you could imagine a system\nthat did not cache shared data,",
    "start": "3941770",
    "end": "3947830"
  },
  {
    "text": "and you wouldn't have a\ncache coherence problem. What problem would you have?",
    "start": "3947830",
    "end": "3954335"
  },
  {
    "text": "[INAUDIBLE] A performance problem. You have a performance\nproblem, but you wouldn't have a cache coherence problem.",
    "start": "3954335",
    "end": "3960710"
  },
  {
    "text": "But, yeah, so every reference\nwould have to go to-- every shared memory reference\nwould have to go to memory,",
    "start": "3960710",
    "end": "3969911"
  },
  {
    "text": "and maybe-- so I would design a\nvery careful system to minimize",
    "start": "3969912",
    "end": "3976090"
  },
  {
    "text": "the amount of shared\ndata, but you're-- this is not a useful system. ",
    "start": "3976090",
    "end": "3988160"
  },
  {
    "text": "So how can you\nimplement coherence? Well, there's some\nsoftware-based solutions.",
    "start": "3988160",
    "end": "3993490"
  },
  {
    "text": "You could try and do\nthings on the granularity of virtual memory pages, and\nmake use of the operating",
    "start": "3993490",
    "end": "3998820"
  },
  {
    "text": "system, but that would be slow. It'd be slow, and you'd\nalso have a problem that we're going to\ntalk about later, which",
    "start": "3998820",
    "end": "4006380"
  },
  {
    "text": "is called full sharing. So what you want is a\nfine-grained, hardware-based",
    "start": "4006380",
    "end": "4011420"
  },
  {
    "text": "solution based on cache lines. And so we're going to talk about\ntwo different ways of doing",
    "start": "4011420",
    "end": "4017390"
  },
  {
    "text": "cache coherence or fixing\nthe cache coherence problem or implementing cache coherence. One is called snooping, which\nis a classic way of implementing",
    "start": "4017390",
    "end": "4026089"
  },
  {
    "text": "cache coherence,\nand then the one that is used most often\ntoday in most systems",
    "start": "4026090",
    "end": "4034370"
  },
  {
    "text": "is directory-based system. But we're going to\nstart with snooping",
    "start": "4034370",
    "end": "4040490"
  },
  {
    "text": "since I think it's slightly\nmore interesting and easier to understand.",
    "start": "4040490",
    "end": "4046359"
  },
  {
    "text": " So we have-- one way of\ndealing with cache coherence is",
    "start": "4046360",
    "end": "4056220"
  },
  {
    "text": "kind of back to not\nhaving separate caches.",
    "start": "4056220",
    "end": "4063349"
  },
  {
    "text": "Having a single shared cache. And so what would be the problem\nof a single shared cache?",
    "start": "4063350",
    "end": "4070390"
  },
  {
    "text": " Performance. Performance. So you have a\nbandwidth bottleneck",
    "start": "4070390",
    "end": "4077420"
  },
  {
    "text": "because all of these\nprocesses are trying to hit the same\ncache, and you've",
    "start": "4077420",
    "end": "4083480"
  },
  {
    "text": "got a limited number\nof amount of bandwidth you can get to the cache, and\nso you might be able to do it",
    "start": "4083480",
    "end": "4088880"
  },
  {
    "text": "for a couple processors,\nbut if you wanted to scale a number of\nprocessors, you would quickly",
    "start": "4088880",
    "end": "4093950"
  },
  {
    "text": "run out of bandwidth,\nor that cache would be extremely expensive.",
    "start": "4093950",
    "end": "4099929"
  },
  {
    "text": "The other issue is that\nwhat if the cache contains data that is not being shared?",
    "start": "4099930",
    "end": "4108710"
  },
  {
    "text": "Then you can imagine one\nprocessor conflicting",
    "start": "4108710",
    "end": "4113989"
  },
  {
    "text": "or having capacity misses\ncaused by another processor.",
    "start": "4113990",
    "end": "4120899"
  },
  {
    "text": "So this is called interference\nor destructive interference.",
    "start": "4120899",
    "end": "4126890"
  },
  {
    "text": "But you can also have\nconstructive interference. So imagine you\nhad a for all loop",
    "start": "4126890",
    "end": "4133040"
  },
  {
    "text": "like this in which you had\nthe iterations interleaved",
    "start": "4133040",
    "end": "4138799"
  },
  {
    "text": "across the processes. So in this code, you can\nimagine that iteration",
    "start": "4138800",
    "end": "4149939"
  },
  {
    "text": "I would fetch the data\nassociated with the following",
    "start": "4149939",
    "end": "4155670"
  },
  {
    "text": "iterations. Assume that you didn't have-- you had cache lines\nthat were small,",
    "start": "4155670",
    "end": "4164100"
  },
  {
    "text": "say one word just for\nthe sake of argument.",
    "start": "4164100",
    "end": "4170200"
  },
  {
    "text": "So you can have constructive\ninterference in addition",
    "start": "4170200",
    "end": "4176040"
  },
  {
    "text": "to destructive interference. So shared caches don't\nreally work that well",
    "start": "4176040",
    "end": "4183450"
  },
  {
    "text": "for level 1 caches, but you can\nuse them for level 2 caches.",
    "start": "4183450",
    "end": "4188890"
  },
  {
    "text": "So this is a process that I\ndesigned many years ago called The SUN Niagara 2.",
    "start": "4188890",
    "end": "4194139"
  },
  {
    "text": "And it had a shared cache-- a shared second-level\ncache, and the way",
    "start": "4194140",
    "end": "4201450"
  },
  {
    "text": "that you provided enough\nbandwidth to that shared cache was by having a crossbar.",
    "start": "4201450",
    "end": "4206670"
  },
  {
    "text": "And a crossbar is\nkind of an all to all network that provides\nyou a lot of bandwidth,",
    "start": "4206670",
    "end": "4211989"
  },
  {
    "text": "but it takes up a fair amount\nof the area on the chip, and it doesn't scale.",
    "start": "4211990",
    "end": "4218850"
  },
  {
    "text": "In this case, you\nget to eight cores, but if you wanted\nto go to more cores, then you'd have to come up\nwith a different mechanism",
    "start": "4218850",
    "end": "4225360"
  },
  {
    "text": "because crossbars are quadratic\nin the amount of wires you",
    "start": "4225360",
    "end": "4230730"
  },
  {
    "text": "need because it's all-to-all.  So let's talk about snooping\ncache coherence schemes",
    "start": "4230730",
    "end": "4241880"
  },
  {
    "text": "in the time that we have\nleft, and we will, of course,",
    "start": "4241880",
    "end": "4247710"
  },
  {
    "text": "not finish it today, but we'll\ncome back to it on Thursday.",
    "start": "4247710",
    "end": "4253360"
  },
  {
    "text": "So the idea is that\nyou've got a processor with its individual cache, and\nthe caches of all the processors",
    "start": "4253360",
    "end": "4265559"
  },
  {
    "text": "are connected by an\ninterconnect, which is just this set of wires or\na way of communicating",
    "start": "4265560",
    "end": "4273580"
  },
  {
    "text": "between the processes and\nthe processes and memory. So the processor issues loads\nand stores to the cache,",
    "start": "4273580",
    "end": "4284950"
  },
  {
    "text": "and then the cache\nreceives coherence messages",
    "start": "4284950",
    "end": "4292660"
  },
  {
    "text": "from the interconnect\nassociated with other actions that other processors\nare making.",
    "start": "4292660",
    "end": "4300226"
  },
  {
    "text": "So a very simple\ncoherence mechanism",
    "start": "4300226",
    "end": "4305410"
  },
  {
    "text": "would be a write\nthrough mechanism, where upon a write, the\ncache controller broadcasts",
    "start": "4305410",
    "end": "4314950"
  },
  {
    "text": "an invalidation message\nwhich says, hey, I'm writing address X. If you\nhave address X in your cache,",
    "start": "4314950",
    "end": "4324349"
  },
  {
    "text": "get rid of it. Invalidate it. So then we're sure\nthat nobody else",
    "start": "4324350",
    "end": "4329860"
  },
  {
    "text": "in the system, no other cache\nin the system has that address. So what is the problem of the\nright through cache mechanism?",
    "start": "4329860",
    "end": "4340040"
  },
  {
    "text": " All the other\nprocessors, do they",
    "start": "4340040",
    "end": "4345610"
  },
  {
    "text": "have to keep kind of\npulling the interconnect-- Yeah, so assume that the\ncache is responsible for that.",
    "start": "4345610",
    "end": "4351340"
  },
  {
    "text": "So the cache kind of\ndecouples the processor from the interconnect. So the cache is always looking\nat the interconnect saying, hey,",
    "start": "4351340",
    "end": "4359950"
  },
  {
    "text": "is there an invalidation\nmessage on the interconnect? If so, let me check my cache.",
    "start": "4359950",
    "end": "4365720"
  },
  {
    "text": "Do I have that address? Yes, invalidate it. No, ignore it.",
    "start": "4365720",
    "end": "4372350"
  },
  {
    "text": "Yeah, what's the problem.  If two processors\nbroadcast the invalidation",
    "start": "4372350",
    "end": "4380060"
  },
  {
    "text": "for the same address same time-- Well, somebody has to win.",
    "start": "4380060",
    "end": "4385940"
  },
  {
    "text": "So assume that this\ninterconnect only allows one message at a\ntime, and there's a winner.",
    "start": "4385940",
    "end": "4392224"
  },
  {
    "text": " So the problem with this\nis the right through cache.",
    "start": "4392225",
    "end": "4399480"
  },
  {
    "text": "It says every right\nthat the processor makes has to appear\non the interconnect.",
    "start": "4399480",
    "end": "4406400"
  },
  {
    "text": "Again, we're going to\nrun out of bandwidth. So this is a\nlow-performance solution. ",
    "start": "4406400",
    "end": "4413200"
  },
  {
    "text": "So what we want then is\nhow do we do coherence",
    "start": "4413200",
    "end": "4418660"
  },
  {
    "text": "with write back caches? So first of all, we're going\nto change this interconnect",
    "start": "4418660",
    "end": "4425640"
  },
  {
    "text": "to a bus. So a bus has two\nvery nice properties.",
    "start": "4425640",
    "end": "4431077"
  },
  {
    "text": "Does anybody know what they are? ",
    "start": "4431077",
    "end": "4440290"
  },
  {
    "text": "They're really wide. They're wide because they\nhave lots of bandwidth. But from the point of view\nof coherence, what properties",
    "start": "4440290",
    "end": "4448420"
  },
  {
    "text": "do buses have? Not allowed to\ntransact at the same-- Right. So there's only one\ntransaction at a time.",
    "start": "4448420",
    "end": "4456500"
  },
  {
    "text": "So somebody asked about that. So bus by definition, one\ntransaction at a time.",
    "start": "4456500",
    "end": "4461929"
  },
  {
    "text": "Not true for a ring, not true\nfor an arbitrary network, but true for a bus.",
    "start": "4461930",
    "end": "4468590"
  },
  {
    "text": "One transaction at a time. So what does that mean?",
    "start": "4468590",
    "end": "4475510"
  },
  {
    "text": "When somebody says\none at a time, what's the first thing\nthat comes to your mind?",
    "start": "4475510",
    "end": "4480626"
  },
  {
    "text": "Serialization. Serialization. So the buses act as a\nserialization point.",
    "start": "4480626",
    "end": "4487540"
  },
  {
    "text": "What's the other\nbenefit of a bus? ",
    "start": "4487540",
    "end": "4493070"
  },
  {
    "text": "Kind of like the\nair in this room. ",
    "start": "4493070",
    "end": "4501270"
  },
  {
    "text": "The broadcast medium. One processor says something\nand everybody else hears it.",
    "start": "4501270",
    "end": "4509469"
  },
  {
    "text": "So two properties of a bus. ",
    "start": "4509470",
    "end": "4519719"
  },
  {
    "text": "We have broadcast\nand serialization. So what we want then\nin our write back cache",
    "start": "4519720",
    "end": "4529949"
  },
  {
    "text": "is we want to make sure that\nwhenever a processor is writing a value, that is the only\nprocessor in the system that",
    "start": "4529950",
    "end": "4539520"
  },
  {
    "text": "is actually allowed to write. And so one way of doing\nthat is by indicating",
    "start": "4539520",
    "end": "4549120"
  },
  {
    "text": "exclusive ownership\nof the cache line, and let's assume that exclusive\nownership is indicated",
    "start": "4549120",
    "end": "4556650"
  },
  {
    "text": "by having the dirty bit set. So what we need\nto ensure, though,",
    "start": "4556650",
    "end": "4562060"
  },
  {
    "text": "is that only one\ncache in the system can ever have its dirty\nbit set at any time.",
    "start": "4562060",
    "end": "4568290"
  },
  {
    "text": "Otherwise, we're going to\nviolate that invariant that says, hey, there's\nonly one processor",
    "start": "4568290",
    "end": "4575520"
  },
  {
    "text": "in the read-write epoch. So we only have one. So we need a way of\nenforcing only one.",
    "start": "4575520",
    "end": "4585190"
  },
  {
    "text": "And the way of enforcing\nonly one processor at a time is in the read-write epoch\nis called a cache coherence",
    "start": "4585190",
    "end": "4592150"
  },
  {
    "text": "protocol. So it maintains the cache\ncoherent invariance. It assumes that there's\nsome hardware logic that's",
    "start": "4592150",
    "end": "4605050"
  },
  {
    "text": "going to look at\nloads and stores made by the local\nprocessor, and messages",
    "start": "4605050",
    "end": "4612700"
  },
  {
    "text": "from other caches on the bus.  So in an invalidation\nbased write back protocol,",
    "start": "4612700",
    "end": "4622400"
  },
  {
    "text": "which is what we're\ngoing to describe, there's a set key ideas.",
    "start": "4622400",
    "end": "4627590"
  },
  {
    "text": "Somebody asked about\nwhat other state is in the metadata\nfor the cache line. Well, you need some\ncache coherency state,",
    "start": "4627590",
    "end": "4637550"
  },
  {
    "text": "and the idea is\nthat you're going to have some cache\ncoherency, state",
    "start": "4637550",
    "end": "4642659"
  },
  {
    "text": "and you're going to be\nable to allow only one",
    "start": "4642660",
    "end": "4648300"
  },
  {
    "text": "processor to be in a certain\nstate at any one time. And you need to be able to tell\nthe other processors to not",
    "start": "4648300",
    "end": "4660660"
  },
  {
    "text": "be in that state. So let me see. I think we have time to\njust introduce the idea.",
    "start": "4660660",
    "end": "4668260"
  },
  {
    "text": "So we talked about dirty bit,\nand we talked about the fact that we need some cache\ncoherency line state.",
    "start": "4668260",
    "end": "4675480"
  },
  {
    "text": "Let's talk about\na cache coherency protocol for a write-back\ninvalidate based cache.",
    "start": "4675480",
    "end": "4686800"
  },
  {
    "text": "So key toss of the protocol,\nensuring that one processor can",
    "start": "4686800",
    "end": "4693119"
  },
  {
    "text": "get exclusive\naccess for a write, and locating the most recent\ncopy of the cache line",
    "start": "4693120",
    "end": "4700890"
  },
  {
    "text": "based on the cache line's\ndata on a cache miss.",
    "start": "4700890",
    "end": "4706950"
  },
  {
    "text": "So in the MSI protocol,\nthere are three states.",
    "start": "4706950",
    "end": "4712440"
  },
  {
    "text": "Modified, shared, and\ninvalid, hence the name.",
    "start": "4712440",
    "end": "4717780"
  },
  {
    "text": "And invalid is the same as\nthe cache line is not there.",
    "start": "4717780",
    "end": "4723150"
  },
  {
    "text": "Shared is this state where\nmultiple processes can only",
    "start": "4723150",
    "end": "4729630"
  },
  {
    "text": "read the line, so\nread-only, and modify the line is valid\nin only one cache,",
    "start": "4729630",
    "end": "4737969"
  },
  {
    "text": "or it's in what\nwe've been calling the dirty or exclusive state.",
    "start": "4737970",
    "end": "4744000"
  },
  {
    "text": "So invalid, the\ncache line is not there, shared read-only modified\nvalid in exactly one cache.",
    "start": "4744000",
    "end": "4755800"
  },
  {
    "text": "So we have two processor\noperations, processor read",
    "start": "4755800",
    "end": "4760860"
  },
  {
    "text": "and processor write, three\ncoherence, bus transactions.",
    "start": "4760860",
    "end": "4767909"
  },
  {
    "text": "So these come from the bus. So these come from\nthe processor. Processor read and processor\nwrite, and then bus read,",
    "start": "4767910",
    "end": "4777840"
  },
  {
    "text": "exclusive bus write\nback come from the bus. And these are caused by\nactions of other processes.",
    "start": "4777840",
    "end": "4785460"
  },
  {
    "text": "So bus read means hey, give\nme a copy of the cache line",
    "start": "4785460",
    "end": "4790550"
  },
  {
    "text": "because I want to read it. Bus read exclusive says, give\nme a copy of the cache line",
    "start": "4790550",
    "end": "4797150"
  },
  {
    "text": "because I want to write it. So first of all,\nbefore I can write it, I need the whole cache line,\nbut I'm going to write it.",
    "start": "4797150",
    "end": "4803850"
  },
  {
    "text": "And then bus write\nback says, hey, this is a cache line that\nwas dirty in my cache",
    "start": "4803850",
    "end": "4810830"
  },
  {
    "text": "that I'm writing back to memory. So keep this in mind\nwhen we continue",
    "start": "4810830",
    "end": "4817340"
  },
  {
    "text": "with the story on\nThursday and talk about how exactly we maintain\ncoherence using these three",
    "start": "4817340",
    "end": "4825810"
  },
  {
    "text": "states and these actions. ",
    "start": "4825810",
    "end": "4837000"
  }
]