[
  {
    "start": "0",
    "end": "627000"
  },
  {
    "start": "0",
    "end": "10770"
  },
  {
    "text": "So the title of\nthis talk is based on a book chapter I\nwrote at this point,",
    "start": "10770",
    "end": "16800"
  },
  {
    "text": "more than a year ago with my\nformer colleague, Kush Varshney, at IBM Research. I used to work at IBM.",
    "start": "16800",
    "end": "23369"
  },
  {
    "text": "So we did a selected overview\nof both our own work and broader HCI work that study this\ntopic of explainable AI",
    "start": "23370",
    "end": "31020"
  },
  {
    "text": "and take human-centered\napproaches, meaning, focusing\non people's needs, interactions with\nAI explanation.",
    "start": "31020",
    "end": "38460"
  },
  {
    "text": "At that time, it was also for me\nto reflect on this question of, what can human-centered\napproaches",
    "start": "38460",
    "end": "44610"
  },
  {
    "text": "do for explainable AI? And use as a lens to also\nreflect on how the HCI and AI",
    "start": "44610",
    "end": "51240"
  },
  {
    "text": "community can better\nwork together, I'm sure that's a question that\nyou all hear reflect on a lot.",
    "start": "51240",
    "end": "56820"
  },
  {
    "text": "So I really appreciate\nthe opportunity to speak here and hopefully,\nhaving conversations with all of you.",
    "start": "56820",
    "end": "62550"
  },
  {
    "text": "I also realize, my\nanswer to this question has also been\ndriving my own work.",
    "start": "62550",
    "end": "68190"
  },
  {
    "text": "So today, I will also share\nsome of our very recent work. It's also the first\ntime I talk about them,",
    "start": "68190",
    "end": "73720"
  },
  {
    "text": "so please bear with me. I want to start\nwith the definition. What does explainability mean?",
    "start": "73720",
    "end": "80950"
  },
  {
    "text": "If you work in a field,\nyou have probably seen a lot of this kind\nof discussion also related to term like interpretability,\ntransparency.",
    "start": "80950",
    "end": "88750"
  },
  {
    "text": "But I'm going to take a broader\nand simple definition that also",
    "start": "88750",
    "end": "93790"
  },
  {
    "text": "really highlights this\nhuman-centered nature of this topic that is\nexplainability is about making",
    "start": "93790",
    "end": "100420"
  },
  {
    "text": "AI understandable by people. With that, my focus is often on\nthis AI system people interact",
    "start": "100420",
    "end": "107680"
  },
  {
    "text": "with rather than model\nper se, and also not just about explaining AI decision\nprocess because people",
    "start": "107680",
    "end": "114820"
  },
  {
    "text": "are interested in broad range of\ninformation, how the model was developed, what's the\nlimitation, and obviously,",
    "start": "114820",
    "end": "121000"
  },
  {
    "text": "not just algorithm\nand technique, and today, I'm\ngoing to focus more on the interaction, the\ndesign side of things.",
    "start": "121000",
    "end": "128350"
  },
  {
    "text": "I think I don't need to convince\nyou that explainability AI-- explainable AI is a hot\ntopic, even in popular news.",
    "start": "128350",
    "end": "135510"
  },
  {
    "text": "It's also in the regulation. For example, the\nGDPR law in Europe",
    "start": "135510",
    "end": "140969"
  },
  {
    "text": "state that whenever you have\nautomated decision process, people have rights to\nmeaningful explanation.",
    "start": "140970",
    "end": "149100"
  },
  {
    "text": "Explainability is\nalso considered as a principle or a\npillar of responsible",
    "start": "149100",
    "end": "154590"
  },
  {
    "text": "AI, which is about developing\nAI technology that are ethical, minimize harms to\nindividual and society.",
    "start": "154590",
    "end": "162629"
  },
  {
    "text": "And I'd like to\nsee explainability as the enabler or foundation for\nmany other principles of RAI,",
    "start": "162630",
    "end": "172530"
  },
  {
    "text": "because again, explainability\nis about understanding. And understanding allows\nfor human scrutinization",
    "start": "172530",
    "end": "179520"
  },
  {
    "text": "and if necessary, intervention. And with that, we can\nhave better model, ensure safety, fairness,\nand help end user",
    "start": "179520",
    "end": "187560"
  },
  {
    "text": "to have better usability,\nappropriate trust, and also have oversight\nfor things like privacy,",
    "start": "187560",
    "end": "194970"
  },
  {
    "text": "accountability, and things. So where my own work and\nthis HCI work situated",
    "start": "194970",
    "end": "201720"
  },
  {
    "text": "is also a world where the\ntechnical AI community has made prior strength.",
    "start": "201720",
    "end": "207330"
  },
  {
    "text": "So there has already\nbeen a vast collection of XAI algorithms developed.",
    "start": "207330",
    "end": "213390"
  },
  {
    "text": "So this is not the\nfocus of this talk but I want to give\none minute overview,",
    "start": "213390",
    "end": "218400"
  },
  {
    "text": "want to highlight that\nthese many algorithms, 100s, if not more, also\ntake various forms,",
    "start": "218400",
    "end": "225420"
  },
  {
    "text": "even if we disregard the\nmany different computational properties. So there are directly\nexplainable model,",
    "start": "225420",
    "end": "233099"
  },
  {
    "text": "some people call\nglassbox model that follow this relatively human\nunderstandable intuitive",
    "start": "233100",
    "end": "239910"
  },
  {
    "text": "process, things\nlike decision tree, rule-based model\nlinear regression. There are also lots\nof algorithm producing",
    "start": "239910",
    "end": "248189"
  },
  {
    "text": "post-hoc explainability,\nthat is when you already use this kind of\nblack box model, say,",
    "start": "248190",
    "end": "254650"
  },
  {
    "text": "deep neural network or large\nensemble that they don't really follow human intuitive process.",
    "start": "254650",
    "end": "260768"
  },
  {
    "text": "And you will need\nanother set of algorithms to generate explanation.",
    "start": "260769",
    "end": "265930"
  },
  {
    "text": "There can be global\nexplanation, for example, using a simpler model to\napproximate this black box",
    "start": "265930",
    "end": "273580"
  },
  {
    "text": "models and logic. You can also explain a\nparticular prediction",
    "start": "273580",
    "end": "278650"
  },
  {
    "text": "or decision called\nlocal explanation. A very popular explanation\nis feature importance,",
    "start": "278650",
    "end": "284740"
  },
  {
    "text": "essentially highlights, for\nthis instance, this input, which feature is\nimportant contribute",
    "start": "284740",
    "end": "291669"
  },
  {
    "text": "to the final decision? There are also\ncounterfactual explanation that answer this why not.",
    "start": "291670",
    "end": "297310"
  },
  {
    "text": "Why not a different prediction? For example, if you\nchange this feature, you will get a\ndifferent prediction.",
    "start": "297310",
    "end": "302860"
  },
  {
    "text": "This is because people are often\ninterested in different, often, more desirable outcome.",
    "start": "302860",
    "end": "309880"
  },
  {
    "text": "Another a notable\ntrend here is also, there have been lots of\nXAI toolkits developed",
    "start": "309880",
    "end": "315330"
  },
  {
    "text": "in the industry. So these are open\nsource toolkits, people can plug-in\ntheir own model data,",
    "start": "315330",
    "end": "321270"
  },
  {
    "text": "and they can leverage this\nstate of the art algorithms. So in fact, what got me\ninto this topic was in 2019,",
    "start": "321270",
    "end": "328979"
  },
  {
    "text": "when I was working\non IBM, I started working on this toolkit\ncalled AI Explainability 360.",
    "start": "328980",
    "end": "335430"
  },
  {
    "text": "And Microsoft has a\nversion called InterpretML. So with all these toolkits\nlowering the barrier,",
    "start": "335430",
    "end": "344160"
  },
  {
    "text": "I think we're\nreally on the verge of going to see more and more\nreal world AI system that",
    "start": "344160",
    "end": "350340"
  },
  {
    "text": "has explanation component. And I like to view this\nhuman-centered research also",
    "start": "350340",
    "end": "356700"
  },
  {
    "text": "as bridging work, bridge\nthis algorithmic capability to having real world XAI\nsystem, effective system.",
    "start": "356700",
    "end": "365880"
  },
  {
    "text": "So this system will\nwork in many domains, serve many different users. And very importantly,\nthey're going",
    "start": "365880",
    "end": "371949"
  },
  {
    "text": "to be built by practitioners,\ndata scientists, developers, designers.",
    "start": "371950",
    "end": "378340"
  },
  {
    "text": "I think it's\nhelpful to also take a step back to see this\ntechnical works as producing",
    "start": "378340",
    "end": "385120"
  },
  {
    "text": "a toolbox for practitioners. Well, there are incredibly\ninteresting technical challenge",
    "start": "385120",
    "end": "391930"
  },
  {
    "text": "for practitioner at\nthe end of the day. What they need is a good, a\nreliable toolbox, and also",
    "start": "391930",
    "end": "398800"
  },
  {
    "text": "know how to use it. So with that metaphor\nof a toolbox, I summarize these three\nkinds of contribution",
    "start": "398800",
    "end": "406979"
  },
  {
    "text": "that this HCI work make. Of course, there are far more\nthan three HCI contribution,",
    "start": "406980",
    "end": "412080"
  },
  {
    "text": "but simply a way for me to\ncluster some emerging work. The first one is to\nnavigate this toolbox,",
    "start": "412080",
    "end": "419610"
  },
  {
    "text": "to inform technical choices by\npeople's explainability needs. And second one is to assess\nthe toolbox, especially",
    "start": "419610",
    "end": "427410"
  },
  {
    "text": "to uncover pitfalls\nof existing methods through empirical studies. And with that, we can expand\nthe toolbox, inform new method,",
    "start": "427410",
    "end": "437430"
  },
  {
    "text": "conceptual framework for more\nhuman compatible explanation.",
    "start": "437430",
    "end": "442470"
  },
  {
    "text": "So I will start\nfrom the first one, how to navigate this toolbox.",
    "start": "442470",
    "end": "447720"
  },
  {
    "text": "One place to start is to have\nthis top-down characterization",
    "start": "447720",
    "end": "453210"
  },
  {
    "text": "of this space of people's\nexplainability needs, then we can make design\nrecommendations accordingly.",
    "start": "453210",
    "end": "460530"
  },
  {
    "text": "So I think the field\nhas always recognized that there are many\ndifferent stakeholders who",
    "start": "460530",
    "end": "466890"
  },
  {
    "text": "need explanation. So many early work try to\nanswer this question of,",
    "start": "466890",
    "end": "472650"
  },
  {
    "text": "who are the prototypical\nusers of XAI? One group that many\nearly work focus on",
    "start": "472650",
    "end": "479639"
  },
  {
    "text": "are the model developers\nwho may want explanation to investigate, verify the\nmodel, and debug the model.",
    "start": "479640",
    "end": "486840"
  },
  {
    "text": "In HCI well, there are\nlots of system work in HCI-based community on this\nkind of explanatory debugging",
    "start": "486840",
    "end": "494430"
  },
  {
    "text": "tool. More recently, I\nthink, much focus is on AI decision support\nsystem, where the direct users",
    "start": "494430",
    "end": "502470"
  },
  {
    "text": "are decision maker. So this is a real world example\nof this kind of a loan risk",
    "start": "502470",
    "end": "510630"
  },
  {
    "text": "assessment AI that\ncan help a loan officer to make loan decisions.",
    "start": "510630",
    "end": "516659"
  },
  {
    "text": "There are many problems issue\nwith this kind of system I'll not get into,\nbut the hope has",
    "start": "516659",
    "end": "522429"
  },
  {
    "text": "been if we give\nthem explanation, we'll be able to help them\nmake more informed decisions.",
    "start": "522429",
    "end": "528070"
  },
  {
    "text": "And there is another group\nin this kind of use case is the loan applicants.",
    "start": "528070",
    "end": "533290"
  },
  {
    "text": "They are not directly\ninteracting with the system, but their life can be impacted. And they may want\nexplanation, if their loan get",
    "start": "533290",
    "end": "540880"
  },
  {
    "text": "rejected, to be able to seek\nrecourse, improve future chance, as well as to contest the AI.",
    "start": "540880",
    "end": "547870"
  },
  {
    "text": "There can also be business\nowner who may want explanation to assess the suitability of AI\nfor their particular use case,",
    "start": "547870",
    "end": "554530"
  },
  {
    "text": "and also regulatory body,\nwho want explanation to be able to audit for\nlegal, ethical compliance,",
    "start": "554530",
    "end": "561250"
  },
  {
    "text": "whether it's bias,\nprivacy, security. So whenever we want\nto make this kind",
    "start": "561250",
    "end": "566860"
  },
  {
    "text": "of a top down characterization,\nthis granularity level of abstraction is\nthe key question,",
    "start": "566860",
    "end": "573370"
  },
  {
    "text": "how do we make actionable\nenough recommendation? So recent work, for\nexample, this work",
    "start": "573370",
    "end": "579339"
  },
  {
    "text": "came out of MIT by\nSuresh et al, they propose that the persona\nis not granular enough.",
    "start": "579340",
    "end": "587320"
  },
  {
    "text": "For example, a decision maker,\nat different usage points, may want different\nexplanation because they",
    "start": "587320",
    "end": "593440"
  },
  {
    "text": "have different goals. When they're\nintroduced to a system, they want to assess\nthe compliance.",
    "start": "593440",
    "end": "600070"
  },
  {
    "text": "When they work on\na particular case, they may want to\ntake better actions.",
    "start": "600070",
    "end": "605800"
  },
  {
    "text": "So they suggest to\nconsider user goals. And this also very much aligns\nwith social science literature",
    "start": "605800",
    "end": "613720"
  },
  {
    "text": "of this kind of\nfunctional position or goal-oriented position. Understanding is\na means to an end.",
    "start": "613720",
    "end": "620440"
  },
  {
    "text": "We cannot really talk about\nunderstanding good enough without considering\nwhat's the end goal.",
    "start": "620440",
    "end": "626860"
  },
  {
    "text": "And in my own work, I tend to\nfind that even this goal is not granular enough.",
    "start": "626860",
    "end": "633310"
  },
  {
    "start": "627000",
    "end": "1116000"
  },
  {
    "text": "On the other hand, there\nhas been HCI literature, social science\nliterature showing",
    "start": "633310",
    "end": "638680"
  },
  {
    "text": "that people's precise\nexplanatory goal can be expressed by what kind\nof questions they ask,",
    "start": "638680",
    "end": "645430"
  },
  {
    "text": "a why question, a what if\nquestion, or why not question, we need different\nkinds of explanation.",
    "start": "645430",
    "end": "651220"
  },
  {
    "text": "Taking this debugging\na model as a task goal. People may ask, what's\nthe performance?",
    "start": "651220",
    "end": "657130"
  },
  {
    "text": "Is it good enough? How does the model\nmake decision? And why the model make a\nparticular kind of mistake?",
    "start": "657130",
    "end": "664180"
  },
  {
    "text": "So this paints a\nrather nuanced picture that people's explanatory\nneeds vary a lot,",
    "start": "664180",
    "end": "671890"
  },
  {
    "text": "depending on the\nquestion, depending on their goal, knowledge. So having top-down\ncharacterization is useful.",
    "start": "671890",
    "end": "679360"
  },
  {
    "text": "At the end of the\nday, we also need bottom-up user-centered\napproach to understand, for particular application, for\nparticular interaction, what",
    "start": "679360",
    "end": "687760"
  },
  {
    "text": "kind of needs people have? So that has been my\nown work, the focus of my work for a\ncouple of years.",
    "start": "687760",
    "end": "694420"
  },
  {
    "text": "We wanted to develop a\nuser-centered design process",
    "start": "694420",
    "end": "699430"
  },
  {
    "text": "starting from understanding\npeople's explanatory needs, and use that to drive\ntechnical and design choice.",
    "start": "699430",
    "end": "706300"
  },
  {
    "text": "So I'm going to give a very\nhigh level overview today. You may have noticed, I\nalways have the reference",
    "start": "706300",
    "end": "711782"
  },
  {
    "text": "at the bottom, if you're\ninterested in checking the detail. So where we started\nwas, again, in 2019,",
    "start": "711782",
    "end": "719920"
  },
  {
    "text": "I was working on\nthis XAI toolkit. And I got very interested\nin this question of, we're now putting\nall this toolkit,",
    "start": "719920",
    "end": "726550"
  },
  {
    "text": "do we even know what\npractitioner need? What kind of\nexplanation they need to incorporate in\ntheir application?",
    "start": "726550",
    "end": "732520"
  },
  {
    "text": "So I want to understand\nthis design space. To do that, we decided\nto interview designers",
    "start": "732520",
    "end": "740710"
  },
  {
    "text": "working on different\napplications, who are also this bridging role,\nunderstand their users.",
    "start": "740710",
    "end": "745990"
  },
  {
    "text": "And this kind of\nquestion-oriented view was very helpful because\nwe're doing a little bit",
    "start": "745990",
    "end": "751000"
  },
  {
    "text": "more forward-looking,\npractitioner are not necessarily using this algorithm yet. So in this, instead of getting\ninto different algorithm,",
    "start": "751000",
    "end": "757899"
  },
  {
    "text": "we simply try to understand\nwhat kind of questions their users have.",
    "start": "757900",
    "end": "763260"
  },
  {
    "text": "So we ask them in this\nopen-ended fashion. We also prepare this\npre-populated question cards,",
    "start": "763260",
    "end": "770100"
  },
  {
    "text": "also grounded in, what\nare some common questions that current\ntechniques can answer?",
    "start": "770100",
    "end": "775590"
  },
  {
    "text": "And then with that\npaper, we try to make two kinds of contribution. One is that we look at\nwhat kind of question",
    "start": "775590",
    "end": "783300"
  },
  {
    "text": "emerged, why they emerge\nin real world contexts, and try to make a\nrelatively high level design",
    "start": "783300",
    "end": "789480"
  },
  {
    "text": "recommendation of how to\naddress these questions. We also did a gap\nanalysis, looking at,",
    "start": "789480",
    "end": "795900"
  },
  {
    "text": "what kind of\nquestions people tend to ask but current technique\ndo not answer well, and make",
    "start": "795900",
    "end": "801329"
  },
  {
    "text": "a recommendation for what kind\nof future methods may be needed. Another contribution we made\nwas this XAI question bank.",
    "start": "801330",
    "end": "810450"
  },
  {
    "text": "So we quote unquote,\n\"designer sourced\" people's common question. And we did content\nanalysis, we cluster them,",
    "start": "810450",
    "end": "817770"
  },
  {
    "text": "so we have this set of\nmore than 50 questions that happening across\ndifferent AI application that",
    "start": "817770",
    "end": "824590"
  },
  {
    "text": "represent this kind of\ncommon space of people's explanatory needs. And they're also clustered into\nthese nine different categories,",
    "start": "824590",
    "end": "832480"
  },
  {
    "text": "from data, output, performance,\nhow at a global level, why question, as well as\nmultiple counterfactual",
    "start": "832480",
    "end": "840280"
  },
  {
    "text": "question. And then the follow\nup work we did is we took these nine\ncategories of question,",
    "start": "840280",
    "end": "847060"
  },
  {
    "text": "then we mapped them to XAI\ntechniques that can answer them. The nice thing about\nquestions is also",
    "start": "847060",
    "end": "854350"
  },
  {
    "text": "they are granular\nenough, you can map them to specific algorithm. And we particularly\nfocus on ones",
    "start": "854350",
    "end": "861070"
  },
  {
    "text": "that are already available\nin open source toolkits because we want\npractitioners to use them.",
    "start": "861070",
    "end": "867040"
  },
  {
    "text": "And by doing that, we also\narrive this recommendation in the middle about\nwhat kind of explanation",
    "start": "867040",
    "end": "873250"
  },
  {
    "text": "can answer those questions\nand also grounded in technical feasibilities.",
    "start": "873250",
    "end": "878410"
  },
  {
    "text": "Yes, Michael. This is really interesting. I'm curious, you've mapped\nfrom question to technique,",
    "start": "878410",
    "end": "886266"
  },
  {
    "text": "and you talked about gaps. I want to ask a\ndifferent kind of gap. So let's say, we have these\nsets of techniques that",
    "start": "886267",
    "end": "892120"
  },
  {
    "text": "are trying to address\neach question between how do we know how closely\nthey actually address",
    "start": "892120",
    "end": "898888"
  },
  {
    "text": "the uncertainties\nin those questions, like, have we crossed\nthose gaps or are we-- So my answer is in\nthe next slides.",
    "start": "898888",
    "end": "905680"
  },
  {
    "text": "And-- [LAUGHS] Yeah. So before my next slides,\nI want to call out, there are two core ideas.",
    "start": "905680",
    "end": "912190"
  },
  {
    "text": "One is we're using\nquestion as a way to reframe the technical\nspace, rather than think about,",
    "start": "912190",
    "end": "918520"
  },
  {
    "text": "is a feature important,\ncounterfactual, we want to reframe it as,\nwhat kind of user questions",
    "start": "918520",
    "end": "924279"
  },
  {
    "text": "this algorithm address? And that will also\nencourage practitioner to foreground\nunderstanding user needs",
    "start": "924280",
    "end": "930850"
  },
  {
    "text": "rather than technical\nfeasibility. And another idea is we want\nto turn question as a boundary",
    "start": "930850",
    "end": "937420"
  },
  {
    "text": "objects to support designers and\ndata scientists to work together",
    "start": "937420",
    "end": "942970"
  },
  {
    "text": "to find the right technique and\nfill the gaps that Michael was asking because designers\nlook at people,",
    "start": "942970",
    "end": "949390"
  },
  {
    "text": "they understand what kind\nof question people ask, what's the underlying needs. And data scientists look at it.",
    "start": "949390",
    "end": "955000"
  },
  {
    "text": "There's also a URL\nto the code library. They can look at a\ntechnical details so they can make a\ndecision together.",
    "start": "955000",
    "end": "961150"
  },
  {
    "text": "So then we propose this\nquestion-driven XAI design process, which has four step.",
    "start": "961150",
    "end": "968740"
  },
  {
    "text": "The first step is you want to\nidentify your user question. And this can really be just\na light-weighted exercise",
    "start": "968740",
    "end": "974980"
  },
  {
    "text": "in your user research. Introduce the AI system,\nget people's reaction, and followed by asking, what\nkind of questions you have?",
    "start": "974980",
    "end": "982420"
  },
  {
    "text": "You want to gather the\nquestion, analyze the question, identify priorities. For example, what\nquestions many people ask?",
    "start": "982420",
    "end": "988990"
  },
  {
    "text": "And the third step is\nwhere the designer and data scientist needs to\nsit down together, find the right solution.",
    "start": "988990",
    "end": "995350"
  },
  {
    "text": "They can use that\ntable as a reference. But importantly, they have\nto consider their user,",
    "start": "995350",
    "end": "1000750"
  },
  {
    "text": "their model, their data. With that, data scientists\ncan implement those technique",
    "start": "1000750",
    "end": "1006389"
  },
  {
    "text": "and designer can\ncreate a design based on the technique they chose. And we emphasize, it must be\nalso be an iterative process.",
    "start": "1006390",
    "end": "1013860"
  },
  {
    "text": "So while I was at\nIBM, we did a lot of work really trying\nto encourage product",
    "start": "1013860",
    "end": "1019260"
  },
  {
    "text": "teams to adopt this process. So one example here is-- so this\nis a collaboration with Watson",
    "start": "1019260",
    "end": "1026730"
  },
  {
    "text": "Health. They're developing this health\ncare adverse event prediction system, meaning, a\ndoctor doing a discharge.",
    "start": "1026730",
    "end": "1034589"
  },
  {
    "text": "The system can tell\nthem, this patient have high risk of\nunplanned hospitalization,",
    "start": "1034589",
    "end": "1039959"
  },
  {
    "text": "and they need to design\nthe care plan accordingly. So we started with the process\nof interviewing doctor,",
    "start": "1039960",
    "end": "1046260"
  },
  {
    "text": "gathering their question. For example, everyone\nask the why question. Why this patient\nhave this high risk?",
    "start": "1046260",
    "end": "1053700"
  },
  {
    "text": "So lots of doctor,\nsomewhat surprisingly, are interested in\nthe data because they want to know if the data aligns\nwith their patient population.",
    "start": "1053700",
    "end": "1061400"
  },
  {
    "text": "And you can see the\ndesign we created on the right has a\nclear correspondence with the questions we collected.",
    "start": "1061400",
    "end": "1068360"
  },
  {
    "text": "So that's a very\nhigh level overview. I do want to move to the\nsecond part of my talk, which",
    "start": "1068360",
    "end": "1074690"
  },
  {
    "text": "is something I'm more\nexcited these days. But I want to warn\nyou, things are going",
    "start": "1074690",
    "end": "1080059"
  },
  {
    "text": "to take a downturn from here. So I want to move to the\nsecond type of contribution",
    "start": "1080060",
    "end": "1085940"
  },
  {
    "text": "is that human-centered\napproaches and empirical studies can help us uncover pitfalls.",
    "start": "1085940",
    "end": "1093080"
  },
  {
    "text": "So there has been a\nlot of excitement, including system work,\nincorporating those algorithms.",
    "start": "1093080",
    "end": "1100490"
  },
  {
    "text": "But today, I want to\ndraw your attention to some hidden\nrisk of XAI methods",
    "start": "1100490",
    "end": "1106610"
  },
  {
    "text": "that are discovered\nby empirical studies. And I want to particularly\nfocus on this use",
    "start": "1106610",
    "end": "1112850"
  },
  {
    "text": "case of decision support. The general setup of\nAI decision support",
    "start": "1112850",
    "end": "1119179"
  },
  {
    "start": "1116000",
    "end": "1333000"
  },
  {
    "text": "is human make a\ndecision for a case, but now we're introducing\na model into the middle.",
    "start": "1119180",
    "end": "1125300"
  },
  {
    "text": "The model is making\na prediction. And that essentially turned as\na human decision maker's task",
    "start": "1125300",
    "end": "1131090"
  },
  {
    "text": "into a reliance decision. Do you take the model\nprediction, rely on it,",
    "start": "1131090",
    "end": "1136318"
  },
  {
    "text": "or you do something else? And whether this\nreliance is appropriate, depending on whether the model\nprediction is appropriate.",
    "start": "1136318",
    "end": "1144140"
  },
  {
    "text": "So there has also been the\nhope that explanation can help.",
    "start": "1144140",
    "end": "1149990"
  },
  {
    "text": "If you add explanation\ntogether with the prediction, then people can gain more\ninsights about model decision,",
    "start": "1149990",
    "end": "1157250"
  },
  {
    "text": "then that help them to have\nmore appropriate reliance and ultimately, better\ndecision outcome.",
    "start": "1157250",
    "end": "1164210"
  },
  {
    "text": "But the reality is\nquite the opposite. So we have this paper\npublished at FAccT 2020,",
    "start": "1164210",
    "end": "1171559"
  },
  {
    "text": "together with my former\ncolleague, Yunfeng Zhang and Rachel Bellamy. So we did this\ncontrolled experiment",
    "start": "1171560",
    "end": "1177950"
  },
  {
    "text": "where we use this semi toy\ndecision task, where we give you someone's profile and ask you\nto judge if this person belongs",
    "start": "1177950",
    "end": "1186270"
  },
  {
    "text": "to a high or low income group. And then we use this very basic\nfeature importance explanation",
    "start": "1186270",
    "end": "1192540"
  },
  {
    "text": "tells you, for this profile,\nwhich feature indicate is a high income, which\nfeature indicate a low income.",
    "start": "1192540",
    "end": "1199230"
  },
  {
    "text": "And that's how, overall, I\nthink this person belongs to a high income group. What we find that showing\nexplanation versus not showing",
    "start": "1199230",
    "end": "1207870"
  },
  {
    "text": "explanation, just\nshowing the prediction, actually reduce the\ndecision performance.",
    "start": "1207870",
    "end": "1213210"
  },
  {
    "text": "And the reason is when\nthe model is uncertain or even the model is wrong,\nshowing explanation actually",
    "start": "1213210",
    "end": "1221700"
  },
  {
    "text": "increase people's tendency\nto blindly follow the model. So that's what's\ncalled overreliance.",
    "start": "1221700",
    "end": "1228179"
  },
  {
    "text": "And we're not the only\none finding this problem. There has been follow\nup work, also not just",
    "start": "1228180",
    "end": "1235350"
  },
  {
    "text": "in this kind of decision\nsupport use case. So for example, my MSR colleague\nwith the former intern, Kaur",
    "start": "1235350",
    "end": "1243060"
  },
  {
    "text": "et al, they look at this\nkind of a debugging use case. Data scientists use this\nkind of visualization,",
    "start": "1243060",
    "end": "1249450"
  },
  {
    "text": "XAI visualization. They also find\nevidence that this lead to overconfidence that the\nmodel is ready for deployment.",
    "start": "1249450",
    "end": "1258179"
  },
  {
    "text": "Some study even find\nthat placebic explanation without actually giving\nyou useful information",
    "start": "1258180",
    "end": "1264270"
  },
  {
    "text": "about the model\ncan increase trust. So I want to call out\nthat overreliance on AI",
    "start": "1264270",
    "end": "1270420"
  },
  {
    "text": "is a serious problem, it's not\njust this one off arrows that",
    "start": "1270420",
    "end": "1275640"
  },
  {
    "text": "can lead to many\ndownstream harms as well and long term issues\nsuch as descaling,",
    "start": "1275640",
    "end": "1281610"
  },
  {
    "text": "and also problem like,\nhow do we assign blame? How do we assign accountability? So that's a concerning pitfall.",
    "start": "1281610",
    "end": "1289350"
  },
  {
    "text": "So many recent work try to\nexplain this phenomenon, why we have this pitfall.",
    "start": "1289350",
    "end": "1295200"
  },
  {
    "text": "Many from this aspect of\nlacking cognitive engagement. People do not really\ncarefully examine",
    "start": "1295200",
    "end": "1303810"
  },
  {
    "text": "reason about explanation,\neither because they have trouble understanding it\nor they're not motivated enough",
    "start": "1303810",
    "end": "1309750"
  },
  {
    "text": "to do so. And this is not really\nbecause participants didn't care about the task.",
    "start": "1309750",
    "end": "1315450"
  },
  {
    "text": "For example, Halina's\npaper, hi, Halina, shows that people\nmay consciously",
    "start": "1315450",
    "end": "1320670"
  },
  {
    "text": "choose not to engage with\nexplanation if they perceive the cognitive cost, the\ncognitive burden is too high",
    "start": "1320670",
    "end": "1327570"
  },
  {
    "text": "and does not really\njustify the benefit of engaging with explanation. So this kind of\ninterpretation is largely",
    "start": "1327570",
    "end": "1336630"
  },
  {
    "start": "1333000",
    "end": "1441000"
  },
  {
    "text": "based on this kind of a\ndual cognitive process. So you may have read the\nbook Thinking, Fast and Slow,",
    "start": "1336630",
    "end": "1342860"
  },
  {
    "text": "that human process information,\nwe follow two kinds of system. There is this system\nto slow thinking",
    "start": "1342860",
    "end": "1350580"
  },
  {
    "text": "that you try to gather\ninformation, try to make a rational judgment.",
    "start": "1350580",
    "end": "1355740"
  },
  {
    "text": "But people often also resort\nto the System 1 fast thinking by invoking cognitive\nheuristic or rule of thumb",
    "start": "1355740",
    "end": "1363050"
  },
  {
    "text": "from their experience. So this points to a\nblind spot in XAI work",
    "start": "1363050",
    "end": "1369230"
  },
  {
    "text": "that will seem to\nmake this underlying assumption that the only\nideal users will carefully",
    "start": "1369230",
    "end": "1375410"
  },
  {
    "text": "engage with explanation\nand able to understand it. But in reality,\nyou're actually more",
    "start": "1375410",
    "end": "1380780"
  },
  {
    "text": "likely to resort to\nthe System 1 thinking. And in some of our\nwork, we find evidence",
    "start": "1380780",
    "end": "1386750"
  },
  {
    "text": "that people tend to have\npositive heuristics associated with explanation.",
    "start": "1386750",
    "end": "1392090"
  },
  {
    "text": "I think this overall\nframing of explainable AI is such a good thing\nis also adding to that.",
    "start": "1392090",
    "end": "1398120"
  },
  {
    "text": "We find that people tend\nto superficially associate being explainable to being\ntrustworthy, being capable,",
    "start": "1398120",
    "end": "1405590"
  },
  {
    "text": "and that lead to over-optimistic\nand overreliance as well.",
    "start": "1405590",
    "end": "1411000"
  },
  {
    "text": "So including myself,\nI have been using this kind of framing to explain\nthis overreliance phenomenon.",
    "start": "1411000",
    "end": "1418669"
  },
  {
    "text": "But at the back of\nmy head, I always have this a little\nbit discomfort of lacking cognitive engagement.",
    "start": "1418670",
    "end": "1425610"
  },
  {
    "text": "Are we blaming the user? Is there actually inherent limit\nto what an XAI method can do?",
    "start": "1425610",
    "end": "1433260"
  },
  {
    "text": "So I think it would\nbe helpful to get a bit more principled\nunderstanding of what are people",
    "start": "1433260",
    "end": "1438780"
  },
  {
    "text": "doing with explanation. I'm going to borrow some\nconcept and terminology",
    "start": "1438780",
    "end": "1444090"
  },
  {
    "start": "1441000",
    "end": "1574000"
  },
  {
    "text": "from this work coming out of\nChenhao Tan's lab at University of Chicago, who is also\nmy long term collaborator,",
    "start": "1444090",
    "end": "1450570"
  },
  {
    "text": "and the lead author is\nPhD CLN, Chacha Chen. So it's a theoretical paper\nthat they differentiate",
    "start": "1450570",
    "end": "1458670"
  },
  {
    "text": "model decision boundary, which\nis how the model make decision, how the model assign label, and\ntask decision boundary, which",
    "start": "1458670",
    "end": "1466230"
  },
  {
    "text": "is how the decision\nshould be made. What matters is human\nhave certain intuition",
    "start": "1466230",
    "end": "1471960"
  },
  {
    "text": "about task decision boundary. Someone who are highly educated\ntend to make more income.",
    "start": "1471960",
    "end": "1479220"
  },
  {
    "text": "So explanation itself, at least\nthis kind of feature importance explanation, can only give you\ninformation about the model",
    "start": "1479220",
    "end": "1487170"
  },
  {
    "text": "decision boundary. But when you are using\nit, you are constantly comparing that to your own\nintuition about the task",
    "start": "1487170",
    "end": "1495540"
  },
  {
    "text": "decision boundary. And the contrast is where\nyou can detect model errors.",
    "start": "1495540",
    "end": "1501870"
  },
  {
    "text": "And I would go a step\nfurther in arguing that this kind of contrast\nbetween model decision",
    "start": "1501870",
    "end": "1508020"
  },
  {
    "text": "boundary and your intuition\nhappens to almost all these different use\ncases we talk about.",
    "start": "1508020",
    "end": "1514860"
  },
  {
    "text": "And it makes a\ndifference whether you take the position that is the\nhuman know more than the AI.",
    "start": "1514860",
    "end": "1521040"
  },
  {
    "text": "If you assume human\nknows more than the AI, these are the kind of\nhuman oversight use case.",
    "start": "1521040",
    "end": "1526679"
  },
  {
    "text": "You want to detect bugs. You want to detect compliance\nissue or contacts the model",
    "start": "1526680",
    "end": "1532710"
  },
  {
    "text": "through explanation. If you assume the AI\nknows more than the human, that's the kind of\nAI insight use case.",
    "start": "1532710",
    "end": "1540090"
  },
  {
    "text": "You want to gain\ndomain knowledge or scientific discovery\nfrom explanation.",
    "start": "1540090",
    "end": "1546180"
  },
  {
    "text": "But the interesting\nthing is, where do we fit decision support? There seems to be an\ninherent tension here",
    "start": "1546180",
    "end": "1553590"
  },
  {
    "text": "that when we talk about\nAI decision support, we want humans to learn\nsomething new to make better",
    "start": "1553590",
    "end": "1558779"
  },
  {
    "text": "decision, but we also want\nhuman to have this oversight to be able to override the AI.",
    "start": "1558780",
    "end": "1565120"
  },
  {
    "text": "So what is exactly\nhappening here? And what is this interplay\nbetween human intuition and AI",
    "start": "1565120",
    "end": "1571950"
  },
  {
    "text": "output? So to answer that\nquestion, last summer, we did this project with\nour intern, Valerie Chen,",
    "start": "1571950",
    "end": "1579420"
  },
  {
    "start": "1574000",
    "end": "1633000"
  },
  {
    "text": "who is a PhD student at CMU. It's also a joint project with\nJen Wertman Vaughan and Gagan",
    "start": "1579420",
    "end": "1584970"
  },
  {
    "text": "Bansal. So we did an experiment to look\nat this decision making process.",
    "start": "1584970",
    "end": "1591990"
  },
  {
    "text": "Again, we used a fairly\nsimple decision task, one is, again, this\nincome-guessing kind of task.",
    "start": "1591990",
    "end": "1598200"
  },
  {
    "text": "Another is we give you this\nsomeone's online biography and you get\nsomeone's profession.",
    "start": "1598200",
    "end": "1604200"
  },
  {
    "text": "So one is a tabular data and\nthe other is a text data. So there's nothing\nnew about the task.",
    "start": "1604200",
    "end": "1610890"
  },
  {
    "text": "They have been used\nby prior HCI studies because these are\naccessible tasks,",
    "start": "1610890",
    "end": "1616350"
  },
  {
    "text": "and honestly, also the\ntraining data is easy to get. But what we did\ndifferently is we went to this good old\npsychology XAI method.",
    "start": "1616350",
    "end": "1624180"
  },
  {
    "text": "We ask people to think-aloud. Then we look at their\nthink-aloud data to trace, to understand what is\nexactly going on in the decision",
    "start": "1624180",
    "end": "1631920"
  },
  {
    "text": "making process. We also want you to look beyond\nthis kind of feature importance",
    "start": "1631920",
    "end": "1638519"
  },
  {
    "start": "1633000",
    "end": "1733000"
  },
  {
    "text": "explanation and another\ncategories of explanation called example-based.",
    "start": "1638520",
    "end": "1644160"
  },
  {
    "text": "There are different forms\nof example-based explanation but we did something\nvery simple.",
    "start": "1644160",
    "end": "1649350"
  },
  {
    "text": "We just show people similar\ninstance using nearest neighbor",
    "start": "1649350",
    "end": "1654480"
  },
  {
    "text": "to find similar instance. We show people the\nmodel's prediction, as well as the ground truth.",
    "start": "1654480",
    "end": "1660880"
  },
  {
    "text": "So you may argue, this is not\nreally about a model decision process if you adopt this\nvery narrow definition",
    "start": "1660880",
    "end": "1667480"
  },
  {
    "text": "of explanation, but\nthe argument has been that they give people idea,\nthe model's decision boundary.",
    "start": "1667480",
    "end": "1673990"
  },
  {
    "text": "And I would argue,\nthey also give information about\nthe task boundary because the ground truth.",
    "start": "1673990",
    "end": "1680230"
  },
  {
    "text": "So what we find is, again,\nfeature-based explanation, very consistent\nwith the other work",
    "start": "1680230",
    "end": "1687010"
  },
  {
    "text": "we talk about is they\nled to overreliance. So when the model is\nwrong, it actually",
    "start": "1687010",
    "end": "1693580"
  },
  {
    "text": "decrease people's\nperformance accuracy. And overall, we did not\nsee human-AI joint decision",
    "start": "1693580",
    "end": "1700060"
  },
  {
    "text": "overperform AI alone\nor human alone. But when we look at\nexample-based explanation,",
    "start": "1700060",
    "end": "1706420"
  },
  {
    "text": "we find that they did not have\nthis overreliance problem, they increase\npeople's performance",
    "start": "1706420",
    "end": "1711790"
  },
  {
    "text": "when the model is\nright, and overall, we observe this\ncomplimentary performance",
    "start": "1711790",
    "end": "1718150"
  },
  {
    "text": "that this joint outcome\noverperform human or AI alone. You will be surprised,\nactually, pretty",
    "start": "1718150",
    "end": "1724680"
  },
  {
    "text": "rare in this kind of\na study to observe this complimentary performance.",
    "start": "1724680",
    "end": "1730320"
  },
  {
    "text": "So what matters is\nwe want to understand what exactly happening.",
    "start": "1730320",
    "end": "1736290"
  },
  {
    "start": "1733000",
    "end": "1738000"
  },
  {
    "text": "Part is our frustration is\nthere has been lots of work on human AI reliance or trust.",
    "start": "1736290",
    "end": "1742980"
  },
  {
    "start": "1738000",
    "end": "2531000"
  },
  {
    "text": "But we actually don't know\nhow this appropriate reliance on human override\nAI actually happen.",
    "start": "1742980",
    "end": "1750130"
  },
  {
    "text": "So we look at this\nthink-aloud data. We summarize these\nthree we call pathway",
    "start": "1750130",
    "end": "1756270"
  },
  {
    "text": "for people to override AI. The first one is when\nthe human already",
    "start": "1756270",
    "end": "1761300"
  },
  {
    "text": "have a strong intuition\nabout the decision outcome and the outcome is different\nfrom AI prediction,",
    "start": "1761300",
    "end": "1767450"
  },
  {
    "text": "the interesting\nthing is we observe when people have a\nstrong intuition, they're actually less\nlikely to engage carefully",
    "start": "1767450",
    "end": "1774950"
  },
  {
    "text": "with the explanation. And also coming back\nto the question of, is the decision support about\nhuman oversight or AI insight?",
    "start": "1774950",
    "end": "1783290"
  },
  {
    "text": "And I think that's\nactually both and it really is a case by case basis. And I also point to this\nkind of decision support,",
    "start": "1783290",
    "end": "1790970"
  },
  {
    "text": "we need to consider different\nneeds at a case level. And the pathway too is quite\nsimilar to that Chen et al's",
    "start": "1790970",
    "end": "1799610"
  },
  {
    "text": "paper is people apply\nintuition about the features to reason about negative\nevidence in explanation,",
    "start": "1799610",
    "end": "1806660"
  },
  {
    "text": "whether they disagree\nwith how the model weighs or they disagree whether\nthat is actually similar",
    "start": "1806660",
    "end": "1813380"
  },
  {
    "text": "how the instance should go. There's also a\nthird path is people try to recognize AI limitation,\nwhether it's at global level,",
    "start": "1813380",
    "end": "1822120"
  },
  {
    "text": "they think, AI is not\ngood at this kind of case because biases, or they try to\nrecognize, for this decision,",
    "start": "1822120",
    "end": "1829530"
  },
  {
    "text": "it's an ambivalent decision,\nthey should not rely on it. Now let's look at\nthe two explanation",
    "start": "1829530",
    "end": "1836730"
  },
  {
    "text": "how they make a difference. Very interesting,\nwe find that when people are given these\nfeature-based explanation,",
    "start": "1836730",
    "end": "1843510"
  },
  {
    "text": "they are much more likely to\nacknowledge, they don't know, they don't have a\nstrong intuition.",
    "start": "1843510",
    "end": "1849150"
  },
  {
    "text": "One of the reasons\nis this is actually visually quite overwhelming.",
    "start": "1849150",
    "end": "1854610"
  },
  {
    "text": "It's overlaid on the instance\nwhere it's a visualization. They disrupt people's\nnatural intuition process.",
    "start": "1854610",
    "end": "1861090"
  },
  {
    "text": "While with example-based,\npeople can just focus on the case itself before\nmoving on to other information.",
    "start": "1861090",
    "end": "1868320"
  },
  {
    "text": "Another reason is if we\nlook at a second pathway, we find that people actually\nhave a lot of difficulty",
    "start": "1868320",
    "end": "1874530"
  },
  {
    "text": "to reason about explanation. One main issue is\nthat human intuition",
    "start": "1874530",
    "end": "1880470"
  },
  {
    "text": "tend to be qualitative. We may recognize, this\nis a relevant feature,",
    "start": "1880470",
    "end": "1886020"
  },
  {
    "text": "this are relevant\nkeywords, but we may not know like this word should\nbe two shades darker than your other word or this\nfeature should be 20% more.",
    "start": "1886020",
    "end": "1894419"
  },
  {
    "text": "And when people are\nconfronted with that kind of quantified information,\nthey tend to feel confused.",
    "start": "1894420",
    "end": "1899620"
  },
  {
    "text": "And many of them choose\nto acknowledge they don't know and delegate to AI.",
    "start": "1899620",
    "end": "1905140"
  },
  {
    "text": "The interesting thing with\nexample-based here is also, it actually has this\nbenefit allowing people",
    "start": "1905140",
    "end": "1911320"
  },
  {
    "text": "to form new intuition. For example, they\nlook at example, they say, oh, professors tend\nto publish this kind of journals",
    "start": "1911320",
    "end": "1919210"
  },
  {
    "text": "so they have this new\nintuition about feature through this kind of inductive\nreasoning because example,",
    "start": "1919210",
    "end": "1925480"
  },
  {
    "text": "bringing additional context. And if you look at\npath 3, it's also",
    "start": "1925480",
    "end": "1930490"
  },
  {
    "text": "interesting is people try to-- from explanation,\nalso try to recognize",
    "start": "1930490",
    "end": "1936730"
  },
  {
    "text": "this unreliability signal\nthrough certain kind of pattern. For feature-- sorry,\nfor example-based,",
    "start": "1936730",
    "end": "1943450"
  },
  {
    "text": "it's very obvious that\nis when the model makes mistake on similar instance.",
    "start": "1943450",
    "end": "1948490"
  },
  {
    "text": "They tend to actually correlate\nwith error, especially when the model make\nmistake on both example,",
    "start": "1948490",
    "end": "1954879"
  },
  {
    "text": "but the nice thing is\nit's a very strong signal, people really\ngravitate towards that.",
    "start": "1954880",
    "end": "1960610"
  },
  {
    "text": "In contrast,\nfeature-based, in theory, they have a signal that\nis if the model sees",
    "start": "1960610",
    "end": "1967390"
  },
  {
    "text": "feature importance\ngoing both sides, and it's a very\nambivalent decision, but people don't recognize that.",
    "start": "1967390",
    "end": "1974470"
  },
  {
    "text": "Only very few participants\nrecognize that as a signal. So I think that kind of--",
    "start": "1974470",
    "end": "1981515"
  },
  {
    "text": "go ahead. Oh, yeah. Sorry. I haven't been able to\nread Valerie's paper yet. I'm curious because I like\nsomething that you said,",
    "start": "1981515",
    "end": "1990280"
  },
  {
    "text": "which is feature-based\nexplanations seem to be more like quantitative\nwhereas example-based are more",
    "start": "1990280",
    "end": "1996160"
  },
  {
    "text": "qualitative, you're\ntrying to figure out what the patterns are. Do you know-- because I\nthink maybe in my head,",
    "start": "1996160",
    "end": "2004049"
  },
  {
    "text": "maybe the quantitative\nelements could be more useful for\ngeneralizability, maybe that's why we try to\ngive it to people.",
    "start": "2004050",
    "end": "2010140"
  },
  {
    "text": "This is a formula\nthat you can follow to predict the future\npredictions that an AI would",
    "start": "2010140",
    "end": "2016140"
  },
  {
    "text": "make. Do you know whether if there's\na difference between these two",
    "start": "2016140",
    "end": "2021600"
  },
  {
    "text": "and how people are able\nto then predict or project what the AI'S abilities\nin the future?",
    "start": "2021600",
    "end": "2027550"
  },
  {
    "text": "Right. Right. Right. Yeah, that's a good question. So this is not to deny a feature\nimportance is not useful at all.",
    "start": "2027550",
    "end": "2033870"
  },
  {
    "text": "I think there are-- at least for some\nkind of user, they might not be compatible\nwith how people reason,",
    "start": "2033870",
    "end": "2038970"
  },
  {
    "text": "but I also think there is\na space for design solution to make them more compatible. You can imagine,\none way is we don't",
    "start": "2038970",
    "end": "2046050"
  },
  {
    "text": "start with showing\nyou super quantified, super detailed information. We can tell you what\nare the top feature. If you want to dig deeper,\nyou can choose to look at it.",
    "start": "2046050",
    "end": "2053638"
  },
  {
    "text": "So that's just one example. But I think the lesson is,\nthere's certain incompatibility,",
    "start": "2053639",
    "end": "2059219"
  },
  {
    "text": "and we need to bridge\nthat part of gap. So yeah. So I think that results really\nresonates with some reflection",
    "start": "2059219",
    "end": "2068908"
  },
  {
    "text": "we had in that book\nchapter about this root cause of this XAI\npitfall, which can be,",
    "start": "2068909",
    "end": "2075510"
  },
  {
    "text": "again, broadly\napplied to AI that we put on some AI\ntechnologies, then they have this kind of hidden risk.",
    "start": "2075510",
    "end": "2082230"
  },
  {
    "text": "One reason is that the output\nmay disconnect with people's cognitive process.",
    "start": "2082230",
    "end": "2087658"
  },
  {
    "text": "We don't consider\ndue quality process. We don't realize this feature\nimportance is actually incompatible with\nhow people reason.",
    "start": "2087659",
    "end": "2094859"
  },
  {
    "text": "But I think there\nis a deeper issue here is that algorithm\ndevelopment do not",
    "start": "2094860",
    "end": "2101490"
  },
  {
    "text": "take into account this diverse\ndownstream usage context. This is not to blame\nindividual user--",
    "start": "2101490",
    "end": "2108990"
  },
  {
    "text": "the individual researchers. When they don't have\nthis established practice or scaffold.",
    "start": "2108990",
    "end": "2114540"
  },
  {
    "text": "When AI researcher\ndevelop algorithm, they may think about use\ncase that they themselves",
    "start": "2114540",
    "end": "2120000"
  },
  {
    "text": "are familiar with,\nsuch as debugging, instead of what\nthe other use cases we talk about, like decision\nsupport, auditing biases.",
    "start": "2120000",
    "end": "2129059"
  },
  {
    "text": "So that need to blind\nspot of important criteria that people care in\ndifferent downstream use",
    "start": "2129060",
    "end": "2136200"
  },
  {
    "text": "case that are not going into\nthis technology development. For example, people care\nabout detecting errors.",
    "start": "2136200",
    "end": "2144210"
  },
  {
    "text": "Why aren't we optimizing or at\nleast, evaluating with that?",
    "start": "2144210",
    "end": "2149580"
  },
  {
    "text": "So with that reflection,\nI want to move to the third part is\nhow HCI approaches can",
    "start": "2149580",
    "end": "2156540"
  },
  {
    "text": "expand the toolbox. For that question,\ntoday, I want to plug some of our most recent\nwork, which I also",
    "start": "2156540",
    "end": "2164520"
  },
  {
    "text": "think is our effort to fill\nthese two sets of disconnect. One line of work\nI'm excited about",
    "start": "2164520",
    "end": "2171270"
  },
  {
    "text": "is to make explanation\nmore human-compatible by drawing inspiration from\nhuman communication, where",
    "start": "2171270",
    "end": "2179039"
  },
  {
    "text": "I think the primary\ncontribution are frameworks or ways to rethink how\nwe have explanation",
    "start": "2179040",
    "end": "2185250"
  },
  {
    "text": "rather than algorithm per se. So broadly, there has been\ncriticism around the fact",
    "start": "2185250",
    "end": "2191790"
  },
  {
    "text": "that current AI explanation have\nsignificant gaps with how human",
    "start": "2191790",
    "end": "2197160"
  },
  {
    "text": "explanation. And the best place\nto read about it is this Tim Miller's\npaper, where",
    "start": "2197160",
    "end": "2202660"
  },
  {
    "text": "he did this very nice overview\nsurvey of social science literature, getting into what\nare some fundamental properties",
    "start": "2202660",
    "end": "2210940"
  },
  {
    "text": "of human explanation. Today, I want to focus\non the bottom two, which",
    "start": "2210940",
    "end": "2216670"
  },
  {
    "text": "is human explanations\nare selective. And when being\nselective, we're also tailoring to the recipients.",
    "start": "2216670",
    "end": "2223420"
  },
  {
    "text": "When do I mean by selective? Taking an example,\nthere is a car accident.",
    "start": "2223420",
    "end": "2228700"
  },
  {
    "text": "And usually, there can\nbe dozens of events leading to that accident. But when you explain, you're not\ngoing to talk about everything,",
    "start": "2228700",
    "end": "2236350"
  },
  {
    "text": "you are going to be selective. And social science literature\nis showing that this selection is not arbitrary.",
    "start": "2236350",
    "end": "2242440"
  },
  {
    "text": "There are common\nheuristics we follow. And that also depend on\nwhat kind of goals we have.",
    "start": "2242440",
    "end": "2248859"
  },
  {
    "text": "If someone is interested in\nthis fact of the damaging-- the damage is\nsurprising, then you",
    "start": "2248860",
    "end": "2255970"
  },
  {
    "text": "are going to pick courses\nthat most relevant to that that the speed is very high.",
    "start": "2255970",
    "end": "2261970"
  },
  {
    "text": "If you talk to a\npolice who wants to diagnose the\ncourses, the problem,",
    "start": "2261970",
    "end": "2267070"
  },
  {
    "text": "you are likely to pick what's\nthe most abnormal event, like this car made a\nsudden lane change.",
    "start": "2267070",
    "end": "2273940"
  },
  {
    "text": "If you want to give\na lesson to your kid, you may pick what's the most\ntangible that really should not",
    "start": "2273940",
    "end": "2279039"
  },
  {
    "text": "have happened that\nthe driver is drunk. So we want to mimic\nthis kind of selectivity",
    "start": "2279040",
    "end": "2288490"
  },
  {
    "text": "of human explanation. So this is joint work\nwith Chenhao Tan, and lead authors are Vivian Lai,\nYiming Zhang and Chacha Chen.",
    "start": "2288490",
    "end": "2297160"
  },
  {
    "text": "So we propose this\ngeneral framework to produce selective\nexplanation.",
    "start": "2297160",
    "end": "2304480"
  },
  {
    "text": "And it's to augment\nexisting explanations. So take any kind of feature\nimportance explanation.",
    "start": "2304480",
    "end": "2310599"
  },
  {
    "text": "Rather than showing you all\nthe feature information, we want to select ones\nthat you would believe",
    "start": "2310600",
    "end": "2316450"
  },
  {
    "text": "to be relevant, abnormal,\nor changeable, depending on the selective goal.",
    "start": "2316450",
    "end": "2321640"
  },
  {
    "text": "And how do we get\nthere is we want to learn about your\nbelief of relevance,",
    "start": "2321640",
    "end": "2326800"
  },
  {
    "text": "abnormal, or changeability\nby your input, your annotation\non a small sample.",
    "start": "2326800",
    "end": "2332680"
  },
  {
    "text": "We-- in this work, we\npropose also different ways to gather kind of input,\nbut I want to walk you through just an instantiation.",
    "start": "2332680",
    "end": "2339550"
  },
  {
    "text": "So this is a task that the\nmodel is doing this sentiment judgment of movie review.",
    "start": "2339550",
    "end": "2347440"
  },
  {
    "text": "And then you have this kind\nof feature-based explanation you can generate from any\nkind of feature importance",
    "start": "2347440",
    "end": "2353290"
  },
  {
    "text": "algorithm that\ntell you, the model think this is a positive\nmovie because it picked up",
    "start": "2353290",
    "end": "2359109"
  },
  {
    "text": "this positive keywords. Why is that it is still\nvisually overwhelming",
    "start": "2359110",
    "end": "2365080"
  },
  {
    "text": "and I would also\nsay, disruptive. You might end up getting hung up\non some relatively trivial thing",
    "start": "2365080",
    "end": "2370930"
  },
  {
    "text": "like, why the model picks up\nWestern as a positive indicator. So we want to generate\nselective explanation",
    "start": "2370930",
    "end": "2378069"
  },
  {
    "text": "by learning your belief of what\nis relevant to movie sentiment",
    "start": "2378070",
    "end": "2383080"
  },
  {
    "text": "task. We can do that by\ncritic-based feedback. We show you example, we\nshow you model explanation,",
    "start": "2383080",
    "end": "2388900"
  },
  {
    "text": "and you can tell\nme which one you think is relevant or\nirrelevant to movie sentiment. What we can get from open-ended\nfeedback, we give you sample,",
    "start": "2388900",
    "end": "2396730"
  },
  {
    "text": "you tell us which keywords\nyou consider as relevant. And we just do a small sample,\nsay, 10 movie review, then we",
    "start": "2396730",
    "end": "2404950"
  },
  {
    "text": "train a prediction\nmodel to predict the unseen new instance,\nwhich keywords align",
    "start": "2404950",
    "end": "2412300"
  },
  {
    "text": "with your relevance belief? Is this model tailored\nper individual",
    "start": "2412300",
    "end": "2417730"
  },
  {
    "text": "or are you learning a\nsingle model across? Good question. So in that general framework,\nwe do consider different ways",
    "start": "2417730",
    "end": "2426580"
  },
  {
    "text": "to get this input. You can get at individual level. You can get a group of\nsimilar user to annotate it.",
    "start": "2426580",
    "end": "2433030"
  },
  {
    "text": "So both are fine. We actually did two\nsets of experiments, one with your own input,\none with similar input.",
    "start": "2433030",
    "end": "2439270"
  },
  {
    "text": "That's the details in the paper. And we use that to\naugment the explanation.",
    "start": "2439270",
    "end": "2445390"
  },
  {
    "text": "So we take this raw explanation. We predict whether the words\nalign with your relevance",
    "start": "2445390",
    "end": "2451600"
  },
  {
    "text": "belief. If it does not align,\nwe'd gray out the words. So we're essentially\nsaying, this",
    "start": "2451600",
    "end": "2458140"
  },
  {
    "text": "is what the model picked\nup, but these words, you're going to think\nthem irrelevant anyway, so don't look at them.",
    "start": "2458140",
    "end": "2463960"
  },
  {
    "text": "Just look at, see,\nthe model picked up the keywords pretty good. So then we did a\ncontrolled experiment.",
    "start": "2463960",
    "end": "2470770"
  },
  {
    "text": "We compare this\nexperiment condition with selective explanation\nand the raw explanation,",
    "start": "2470770",
    "end": "2477099"
  },
  {
    "text": "the unselected one. And we find several benefits. So the first two\nare more related",
    "start": "2477100",
    "end": "2483670"
  },
  {
    "text": "to people's\nsubjective perception. We find that they\nimprove people's",
    "start": "2483670",
    "end": "2488980"
  },
  {
    "text": "subjective understanding because\nthe selective explanation is sparser, they're\nclearer, they're",
    "start": "2488980",
    "end": "2494860"
  },
  {
    "text": "easier to understand so people\nfeel they understand better. And the second one,\ninterestingly, we also",
    "start": "2494860",
    "end": "2501080"
  },
  {
    "text": "find that improve people's\nperceived useness of the AI. Back to Michael's\nquestion, we only",
    "start": "2501080",
    "end": "2507800"
  },
  {
    "text": "find that in the condition when\npeople provide their own input. So that really attribute to--",
    "start": "2507800",
    "end": "2513140"
  },
  {
    "text": "the reason is\nbecause now I can-- my own input augment AI output. And I end up liking\nthe AI better,",
    "start": "2513140",
    "end": "2519080"
  },
  {
    "text": "which I find interesting. The last one, I think,\nis most promising, we also find\nselective explanation",
    "start": "2519080",
    "end": "2525800"
  },
  {
    "text": "improve decision performance\nand reduce overreliance. So let's look at one example.",
    "start": "2525800",
    "end": "2532070"
  },
  {
    "start": "2531000",
    "end": "2902000"
  },
  {
    "text": "So this is an example where\nthe model made a mistake. The ground truth is\na positive review,",
    "start": "2532070",
    "end": "2538280"
  },
  {
    "text": "but the AI think it's negative. If you look at the\noriginal explanation,",
    "start": "2538280",
    "end": "2543560"
  },
  {
    "text": "the reason is the\nmodel picked up some questionable negative word.",
    "start": "2543560",
    "end": "2549350"
  },
  {
    "text": "It thinks stopwords and\nalso the word \"depriving\" to be indicator of negative\nsentiment, which is questionable",
    "start": "2549350",
    "end": "2557330"
  },
  {
    "text": "that's true for movie\nsentiment judgment. But once the selective\nexplanation is in,",
    "start": "2557330",
    "end": "2563030"
  },
  {
    "text": "those questionable words are\ngrayed out, and what's left is actually more consistent\nwith the ground truth.",
    "start": "2563030",
    "end": "2570230"
  },
  {
    "text": "Now the strongest word\nis the \"greatest.\" I want to point out here that\nthis is because the grayed out",
    "start": "2570230",
    "end": "2578000"
  },
  {
    "text": "can be seen as a form this\ncontrast we talk about. This contrast of what the\nmodel think the decision",
    "start": "2578000",
    "end": "2585710"
  },
  {
    "text": "boundary, what the\nmodel think is relevant, and this human intuition, what\nthe human think is irrelevant.",
    "start": "2585710",
    "end": "2592610"
  },
  {
    "text": "And I want to emphasize\nthat this work, because in this task,\nsentiment judgment, human can provide\nreasonably good intuition,",
    "start": "2592610",
    "end": "2601610"
  },
  {
    "text": "so that this gray out provide a\nreasonably good signal of model error. And in fact, if we look at\nthe percentage of highlights,",
    "start": "2601610",
    "end": "2609890"
  },
  {
    "text": "we can see that with a\nselective explanation, the highlights is actually more\naligned with the ground truth.",
    "start": "2609890",
    "end": "2617660"
  },
  {
    "text": "They increase the\npercentage that align with the ground truth. So now people look at\nselective explanation.",
    "start": "2617660",
    "end": "2622940"
  },
  {
    "text": "If they pay attention\nto the highlights, they're actually more\nlikely to get it right rather than following\nthe AI prediction.",
    "start": "2622940",
    "end": "2629390"
  },
  {
    "text": "So that explain\nthis-- we observe this improved performance.",
    "start": "2629390",
    "end": "2634850"
  },
  {
    "text": "So those are some\npromising results. I hope that pique your\ninterest to read the paper. I'd love to hear your feedback.",
    "start": "2634850",
    "end": "2640730"
  },
  {
    "text": "I also hope this study\nshowcase the opportunity as well as interesting\nnuances when",
    "start": "2640730",
    "end": "2647359"
  },
  {
    "text": "you try to operationalize the\nhuman communication behavior into computational interaction.",
    "start": "2647360",
    "end": "2655725"
  },
  {
    "text": "I have a few minutes. I want to spend a few minutes\nto briefly touch on this work.",
    "start": "2655725",
    "end": "2661490"
  },
  {
    "text": "So this is a line work I'm\nexcited about coming back to this second disconnect\nof downstream use case",
    "start": "2661490",
    "end": "2670070"
  },
  {
    "text": "when AI researcher\ndevelop algorithms. I think one place to close that\ngap is to bring more insights,",
    "start": "2670070",
    "end": "2678920"
  },
  {
    "text": "bring more consideration of\nthis downstream use case. I quote usage in\nform evaluation.",
    "start": "2678920",
    "end": "2685610"
  },
  {
    "text": "And I also think this is\nwhere HCI research can make valuable contribution\nby bringing our understanding",
    "start": "2685610",
    "end": "2692660"
  },
  {
    "text": "of downstream use case. So we have this paper published\nat HCOMP end of last year.",
    "start": "2692660",
    "end": "2699740"
  },
  {
    "text": "So we propose this concept\nof contextualized evaluation.",
    "start": "2699740",
    "end": "2705010"
  },
  {
    "text": "So we're motivated by\nthis observation of a gap that in the AI\ncommunity, there has",
    "start": "2705010",
    "end": "2711910"
  },
  {
    "text": "been dozens of evaluation\ncriteria or construct proposed.",
    "start": "2711910",
    "end": "2718420"
  },
  {
    "text": "You can use them either for user\nstudy evaluation or developing computational metrics.",
    "start": "2718420",
    "end": "2723790"
  },
  {
    "text": "For example, lots of work trying\nto quantify the explanation faithfulness, compactness,\nrobustness, and things",
    "start": "2723790",
    "end": "2732070"
  },
  {
    "text": "like that. But on the other\nside, as I discussed in the beginning,\nthe HCI work, we're",
    "start": "2732070",
    "end": "2737619"
  },
  {
    "text": "trying to point out that there\nare different downstream use case. People have different\ngoals they want",
    "start": "2737620",
    "end": "2743529"
  },
  {
    "text": "to achieve with explanation. So our goal is to be able to\nexplicitly consider this context",
    "start": "2743530",
    "end": "2752800"
  },
  {
    "text": "dependency. In what kind of use case which\nevaluation criteria matter?",
    "start": "2752800",
    "end": "2759440"
  },
  {
    "text": "So I think this kind of\nthinking can be broadly applied to whenever we develop\nAI technology has",
    "start": "2759440",
    "end": "2765920"
  },
  {
    "text": "diverse downstream use case. What's in my mind\nthese days is focus this kind of a, quote unquote,\n\"general purpose large model.\"",
    "start": "2765920",
    "end": "2773720"
  },
  {
    "text": "I think this has some similarity\nwith the recent evaluation work come out of Stanford that\nwhen you evaluate large language",
    "start": "2773720",
    "end": "2780920"
  },
  {
    "text": "model, you want to first\narticulate the scenario, and that guide your choice\nof evaluation metrics.",
    "start": "2780920",
    "end": "2786859"
  },
  {
    "text": "But what matters here\nis also, I think, empirical study\ncan help us really to get to that\ncontext dependency.",
    "start": "2786860",
    "end": "2793640"
  },
  {
    "text": "And this helps with\nresponsible research, help AI researcher articulate\nwhich downstream use",
    "start": "2793640",
    "end": "2801200"
  },
  {
    "text": "case this explanation\nis good for, and also help practitioners. When they work on a\nparticular use case,",
    "start": "2801200",
    "end": "2807829"
  },
  {
    "text": "they know what to evaluate,\nwhat to choose for. So I think what matters is\nthis kind of a high level idea.",
    "start": "2807830",
    "end": "2815089"
  },
  {
    "text": "The methodology is actually\npretty straightforward, I would say, a little\noversimplified.",
    "start": "2815090",
    "end": "2820160"
  },
  {
    "text": "We use this kind of\nscenario-based survey. We recruit both XAI\nexperts and also crowd",
    "start": "2820160",
    "end": "2826730"
  },
  {
    "text": "worker as this target\nuser of an application. Then we show people scenario.",
    "start": "2826730",
    "end": "2833210"
  },
  {
    "text": "This is an AI\napplication, and this is a scenario of you\nare using explanation",
    "start": "2833210",
    "end": "2838250"
  },
  {
    "text": "to achieve a certain goal. And we develop this scenario\nby serving HCI literature,",
    "start": "2838250",
    "end": "2844280"
  },
  {
    "text": "looking at those taxonomy of use\ncase, taxonomy of user goals. And we just ask participants\nin this scenario,",
    "start": "2844280",
    "end": "2851930"
  },
  {
    "text": "how do you rate the importance\nof different evaluation criteria? So the details are in the paper,\nso we can get this kind of rank.",
    "start": "2851930",
    "end": "2860540"
  },
  {
    "text": "How do we rank this\nevaluation criteria for different kinds of use case? One is they give you this\nnuanced understanding",
    "start": "2860540",
    "end": "2867680"
  },
  {
    "text": "of what evaluation criteria\nmatter for what use case. And you look through them\nalso from participants'",
    "start": "2867680",
    "end": "2874730"
  },
  {
    "text": "qualitative comments that\ngive you understanding of why. And another point is also give\nus this critical understanding",
    "start": "2874730",
    "end": "2882770"
  },
  {
    "text": "of what people care\nabout and where are the gaps in the\ncurrent practice.",
    "start": "2882770",
    "end": "2888740"
  },
  {
    "text": "For example again,\nfor decision support, people care about this\nunreliability slash uncertainty",
    "start": "2888740",
    "end": "2895580"
  },
  {
    "text": "communication, but we don't have\na good metric to evaluate that. So I'd like to conclude my\ntalk with some personal lessons",
    "start": "2895580",
    "end": "2906530"
  },
  {
    "start": "2902000",
    "end": "3494000"
  },
  {
    "text": "learned from working in this\nin-between space of algorithms and user experience,\nespecially by working close",
    "start": "2906530",
    "end": "2914630"
  },
  {
    "text": "to practitioners. One is, I think,\nit's useful to have",
    "start": "2914630",
    "end": "2920359"
  },
  {
    "text": "this kind of human-centered\nreframing of technical space to be able to articulate,\nfor technique, what kind",
    "start": "2920360",
    "end": "2929120"
  },
  {
    "text": "of downstream use case they\nserve, what user needs, what user values they serve.",
    "start": "2929120",
    "end": "2934430"
  },
  {
    "text": "And it was also\nhelpful to develop means to help AI researchers\nto articulate that as well.",
    "start": "2934430",
    "end": "2941130"
  },
  {
    "text": "And second is we\nwant to facilitate responsible use of technology.",
    "start": "2941130",
    "end": "2946400"
  },
  {
    "text": "I think, especially\nwith AI, sometimes, the research advancement is\ndetached from specific use case.",
    "start": "2946400",
    "end": "2953240"
  },
  {
    "text": "And we need to carefully\nexamine, what are the pitfalls? What are limitations? What are the mismatch of\nassumption that will also",
    "start": "2953240",
    "end": "2960950"
  },
  {
    "text": "help practitioners\nto develop technology in a more responsible way?",
    "start": "2960950",
    "end": "2965990"
  },
  {
    "text": "And third one is,\nit may sound obvious that I think HCI\ncontribution is also about expand practitioners\ntoolbox with design tool.",
    "start": "2965990",
    "end": "2974960"
  },
  {
    "text": "I think the issue is sometimes,\nwhen people talk about AI, they mix the model\nand application,",
    "start": "2974960",
    "end": "2980960"
  },
  {
    "text": "but there is a whole design\nspace that a lot of things can happen that practitioner\ncan really take advantage of.",
    "start": "2980960",
    "end": "2988730"
  },
  {
    "text": "And lastly, I think,\nis as HCI researchers, I think we have a responsibility\nto engage with deployment",
    "start": "2988730",
    "end": "2996800"
  },
  {
    "text": "contexts and people's\nlived experience, and bring that back into\ntechnology development.",
    "start": "2996800",
    "end": "3002650"
  },
  {
    "text": "And criticism is\nnot enough, it's also about how we\nformalize, operationalize",
    "start": "3002650",
    "end": "3008050"
  },
  {
    "text": "human needs as computational\nor evaluation framework. And with that, I want to\nthank you for your attention.",
    "start": "3008050",
    "end": "3015310"
  },
  {
    "text": "I want to thank my\nHCXAI community. And I'm happy to take questions. [APPLAUSE]",
    "start": "3015310",
    "end": "3022864"
  },
  {
    "text": " All right. We've got a few minutes,\nif folks want to engage.",
    "start": "3022864",
    "end": "3028080"
  },
  {
    "start": "3028080",
    "end": "3033560"
  },
  {
    "text": "Hi. I was curious about\nyour experiment where you had explainable\nexamples instead",
    "start": "3033560",
    "end": "3040700"
  },
  {
    "text": "of using the feature-based. Is that for-- is that\nin terms of you're",
    "start": "3040700",
    "end": "3045703"
  },
  {
    "text": "trying to do it for each\nmodel than they would do it or is that for every time\nthe AI makes a prediction?",
    "start": "3045703",
    "end": "3050990"
  },
  {
    "text": "Because I'm curious\nabout the time that it would take for them to\nread through all of the examples and if you're, say, doing\ncontent moderation or something",
    "start": "3050990",
    "end": "3058430"
  },
  {
    "text": "like that where you have tons. Is it feasible? Or do you expect them\nto be able to do those--",
    "start": "3058430",
    "end": "3063859"
  },
  {
    "text": "the examples each time? Yeah. Yeah, that's a great question. I think there are\ntwo separate things. One is I want to clarify,\nall the explanation",
    "start": "3063860",
    "end": "3071240"
  },
  {
    "text": "we're studying in\nthat experiment are local explanation,\nso everything is for that particular\nprediction, so the feature",
    "start": "3071240",
    "end": "3079790"
  },
  {
    "text": "for that prediction or\nexample for that prediction. But you touch on another\nmore important topic is this calculative cost and\nengagement that, I think,",
    "start": "3079790",
    "end": "3092060"
  },
  {
    "text": "that's a general\ntension with explanation is it can be overwhelming,\nit take additional costs",
    "start": "3092060",
    "end": "3097710"
  },
  {
    "text": "again with Halina's paper. So I do think part of the\nexample-based explanation",
    "start": "3097710",
    "end": "3105870"
  },
  {
    "text": "reduce costs come from\nsome of the signals are very easy to recognize.",
    "start": "3105870",
    "end": "3111030"
  },
  {
    "text": "You don't necessarily\nhave to read the detail. You can look at\nwhether the model makes",
    "start": "3111030",
    "end": "3116530"
  },
  {
    "text": "a mistake on those example. So that is actually,\nI would say, a lower cost signal that\nhelp you to detect model",
    "start": "3116530",
    "end": "3124410"
  },
  {
    "text": "without necessarily\nread the content, but you also have the choice to\nmake deeper engagement to reason",
    "start": "3124410",
    "end": "3131820"
  },
  {
    "text": "about, is this example\nactually close? Or where this differentiating\nfeature should go?",
    "start": "3131820",
    "end": "3138210"
  },
  {
    "text": "So they give you\nthis kind of choice. But that's an\nexcellent question. Thank you. Thank you.",
    "start": "3138210",
    "end": "3143400"
  },
  {
    "start": "3143400",
    "end": "3148839"
  },
  {
    "text": "I'm happy to-- we can\nchat later but I'm happy to ask here, one of the\nthings that you've elicited",
    "start": "3148840",
    "end": "3156260"
  },
  {
    "text": "through the one of the project\nthat you were describing was essentially, feedback on\nthe explanation model itself,",
    "start": "3156260",
    "end": "3165512"
  },
  {
    "text": "like that you're learning\nthat the following features of explanation are not useful. But it's not just that they're--",
    "start": "3165512",
    "end": "3171648"
  },
  {
    "text": "it seemed like from the\nexamples you were giving, it's not just that they were\nnot useful explanations,",
    "start": "3171648",
    "end": "3177230"
  },
  {
    "text": "it's that they were actually\nnot good for the model to be paying attention to at all.",
    "start": "3177230",
    "end": "3182850"
  },
  {
    "text": "So it does raise\nthe question of, why are we using that to\nimprove the explanation rather than just to provide\nthat as feedback for the model?",
    "start": "3182850",
    "end": "3189200"
  },
  {
    "text": "Yeah. That's-- If you subtract that\nout of the model, then maybe the explanation\nwould have made more sense. Right.",
    "start": "3189200",
    "end": "3194270"
  },
  {
    "text": "Right. That's a really good question. I think we definitely\nstruggle with the question,",
    "start": "3194270",
    "end": "3199310"
  },
  {
    "text": "should it improve\nthe explanation or should we improve the model? I think there are use cases you\njust can't improve the model.",
    "start": "3199310",
    "end": "3206090"
  },
  {
    "text": "You take a black box\nmodel, then that's the only thing you can do. But there has been\nalso been work",
    "start": "3206090",
    "end": "3212660"
  },
  {
    "text": "around using human\nrationale as training signal itself to improve the model.",
    "start": "3212660",
    "end": "3217940"
  },
  {
    "text": "I think a reasonable\nfollow up work we do is probably use that as\na model training signal and see",
    "start": "3217940",
    "end": "3223010"
  },
  {
    "text": "whether that improve the model. I think that's definitely\nsomething we should do. But that's an excellent point.",
    "start": "3223010",
    "end": "3228453"
  },
  {
    "text": "I don't have a clear answer\nlike which way will go better, but there are situations\nwe just have to use",
    "start": "3228453",
    "end": "3233750"
  },
  {
    "text": "it to augment the explanation. I mean, if I were to\npush back on myself, one argument you could give\nis that, the thing that would make me better\nat making decisions",
    "start": "3233750",
    "end": "3240540"
  },
  {
    "text": "does not necessarily\nmake me a better teacher. If I'm trying to\nexplain something, that doesn't mean\nlike how I do and how",
    "start": "3240540",
    "end": "3247290"
  },
  {
    "text": "I explain might be\ntotally different skills. Yeah. Yeah. Yeah. OK. The other question,\nwhich is you're",
    "start": "3247290",
    "end": "3252480"
  },
  {
    "text": "probably expecting but\njust given recent movement, where does all this sit\nrelative to generative models?",
    "start": "3252480",
    "end": "3260660"
  },
  {
    "text": "Where are we? Where we ought to go? This was mostly focused on\ndiscriminative models and things where there's explicit\nfeatures and so on?",
    "start": "3260660",
    "end": "3267360"
  },
  {
    "text": "Yeah. Yeah. Thanks for that question. And I actually have a\nslide I didn't have time to touch on today,\nbut I can talk more.",
    "start": "3267360",
    "end": "3274859"
  },
  {
    "text": "So I think-- again,\nI think there's a human-centered perspective,\none important is,",
    "start": "3274860",
    "end": "3280440"
  },
  {
    "text": "we have to consider end goal. We cannot talk about\nexplanation without articulating what's the end goal.",
    "start": "3280440",
    "end": "3286440"
  },
  {
    "text": "So I think, with this large\ngeneral purpose model,",
    "start": "3286440",
    "end": "3291839"
  },
  {
    "text": "it's become even\nharder to articulate. We don't even know what\nare the different use case.",
    "start": "3291840",
    "end": "3297060"
  },
  {
    "text": "So I think explanation\nwill be useful, but we need to look broader\nthan algorithmic explanation.",
    "start": "3297060",
    "end": "3304670"
  },
  {
    "text": "We need to think in\nterms of transparency. For example, one use case\nthat I didn't have time",
    "start": "3304670",
    "end": "3310460"
  },
  {
    "text": "to touch on today, we\nhave this two [INAUDIBLE] paper coming out, actually a\ncollaboration with Harry here.",
    "start": "3310460",
    "end": "3318020"
  },
  {
    "text": "So I'm very interested in\nthis space of this question of responsible use.",
    "start": "3318020",
    "end": "3323030"
  },
  {
    "text": "You have this general purpose. Where should I use it,\nespecially for designers?",
    "start": "3323030",
    "end": "3329390"
  },
  {
    "text": "And in this kind of use\ncase, I don't necessarily think the answer lies in\nalgorithmic explanation.",
    "start": "3329390",
    "end": "3335660"
  },
  {
    "text": "Tell me with this input,\nhow I got that output. What I'm interested is, what\nis this design material?",
    "start": "3335660",
    "end": "3342110"
  },
  {
    "text": "What is the boundary? What's the capability? And where things break? So in that kind of\nuse case, I think",
    "start": "3342110",
    "end": "3349100"
  },
  {
    "text": "this concept of\nmodel interrogation you mentioned in the\nbeginning is more useful. You want to give people a\ntool to help them to see",
    "start": "3349100",
    "end": "3356630"
  },
  {
    "text": "where things might break. And that will be more\nuseful than explanation.",
    "start": "3356630",
    "end": "3361640"
  },
  {
    "text": "So that's just an\nexample I feel like we need to look beyond\nalgorithmic explanation",
    "start": "3361640",
    "end": "3367579"
  },
  {
    "text": "and think about what's the end\ngoal people want to achieve. And we need to\nhave this broad set",
    "start": "3367580",
    "end": "3373490"
  },
  {
    "text": "of how do we help people\nunderstand them better. That's my answer. Thank you.",
    "start": "3373490",
    "end": "3378620"
  },
  {
    "text": " So going back to\nthat example where",
    "start": "3378620",
    "end": "3385150"
  },
  {
    "text": "there's the loop between\npeople getting the explanations and then critiquing\nwhich parts are useful",
    "start": "3385150",
    "end": "3391120"
  },
  {
    "text": "and then that informing\nfuture explanations, I'm curious about\nhow that could be",
    "start": "3391120",
    "end": "3397059"
  },
  {
    "text": "done for problems where people\nhave a harder time making those decisions.",
    "start": "3397060",
    "end": "3402369"
  },
  {
    "text": "So for like positive or\nnegative movie reviews, that's something that-- that's a\nhuman task that's easy to do,",
    "start": "3402370",
    "end": "3408970"
  },
  {
    "text": "but sometimes,\nwe're using models for things that are not\neasy to do for people. Are there ways to still extract\nthe insights from people",
    "start": "3408970",
    "end": "3416500"
  },
  {
    "text": "about what sort of\nexplanations are useful, even for those more opaque tasks?",
    "start": "3416500",
    "end": "3422150"
  },
  {
    "text": "Yup. Yeah. Yeah. That's a great question. And it really gets\nto this, I think, fundamental problem when\nwe study human-AI decision",
    "start": "3422150",
    "end": "3429640"
  },
  {
    "text": "making that we're mixing\ndifferent kinds of decision support, like a movie\nsentiment review, then human",
    "start": "3429640",
    "end": "3436385"
  },
  {
    "text": "has certain knowledge,\nbut you can also have a domain that\nhuman actually want to gain-- complete\ngain insight from the AI.",
    "start": "3436385",
    "end": "3443360"
  },
  {
    "text": "So I think a lot of benefit we\nsee in that experiment comes from because that\ntask, human can",
    "start": "3443360",
    "end": "3450920"
  },
  {
    "text": "give reasonably good intuition. So that may not generalize\nto the other kinds of task.",
    "start": "3450920",
    "end": "3458270"
  },
  {
    "text": "But in our framework,\nwe did propose that in that kind\nof task, maybe it's",
    "start": "3458270",
    "end": "3463520"
  },
  {
    "text": "more useful to get input\nfrom domain experts. You get it from\ndomain expert once, and then you propagate that.",
    "start": "3463520",
    "end": "3470030"
  },
  {
    "text": "And that may help people\nwho are not domain experts to make better decisions. Got you.",
    "start": "3470030",
    "end": "3475190"
  },
  {
    "text": "Thanks for the question. ",
    "start": "3475190",
    "end": "3481470"
  },
  {
    "text": "You landed that exactly on time. [LAUGHTER] Literally as we hit 12:30. So let's thank her.",
    "start": "3481470",
    "end": "3486869"
  },
  {
    "text": "Thank you so much. [APPLAUSE] ",
    "start": "3486870",
    "end": "3494000"
  }
]