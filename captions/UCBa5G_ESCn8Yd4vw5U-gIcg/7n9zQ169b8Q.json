[
  {
    "start": "0",
    "end": "18000"
  },
  {
    "start": "0",
    "end": "3920"
  },
  {
    "text": "CHRISTOPHER POTTS:\nWelcome back, everyone.",
    "start": "3920",
    "end": "5670"
  },
  {
    "text": "This is part 8 in our series\non supervised sentiment",
    "start": "5670",
    "end": "7940"
  },
  {
    "text": "analysis, the final\nscreencast in the series.",
    "start": "7940",
    "end": "10590"
  },
  {
    "text": "We're going to be talking about\nrecurrent neural network or RNA",
    "start": "10590",
    "end": "13220"
  },
  {
    "text": "classifiers.",
    "start": "13220",
    "end": "14180"
  },
  {
    "text": "I suppose this is\nofficially our first step",
    "start": "14180",
    "end": "16100"
  },
  {
    "text": "into the world of deep learning\nfor sentiment analysis.",
    "start": "16100",
    "end": "19640"
  },
  {
    "start": "18000",
    "end": "185000"
  },
  {
    "text": "This slide gives an\noverview of the model.",
    "start": "19640",
    "end": "21980"
  },
  {
    "text": "Let's work through\nit in some detail.",
    "start": "21980",
    "end": "23619"
  },
  {
    "text": "So we've got a single\nexample with three tokens,",
    "start": "23620",
    "end": "26210"
  },
  {
    "text": "\"the Rock rules.\"",
    "start": "26210",
    "end": "27199"
  },
  {
    "text": "These models are prepared for\nvariable length sequences.",
    "start": "27200",
    "end": "29870"
  },
  {
    "text": "But this example happens\nto have like three.",
    "start": "29870",
    "end": "33440"
  },
  {
    "text": "And the first step to\nget this model started",
    "start": "33440",
    "end": "35510"
  },
  {
    "text": "is a familiar one.",
    "start": "35510",
    "end": "36308"
  },
  {
    "text": "We're going to look up each\none of those tokens in what",
    "start": "36308",
    "end": "38600"
  },
  {
    "text": "is presumably a fixed\nembedding space here.",
    "start": "38600",
    "end": "41670"
  },
  {
    "text": "So for each token, we'll\nget a vector representation.",
    "start": "41670",
    "end": "45199"
  },
  {
    "text": "The next step is\nthat we have some",
    "start": "45200",
    "end": "46640"
  },
  {
    "text": "learned parameters,\na weight matrix, wxh.",
    "start": "46640",
    "end": "49820"
  },
  {
    "text": "And the subscript indicates that\nwe're going from the inputs x",
    "start": "49820",
    "end": "52790"
  },
  {
    "text": "into the hidden layer h.",
    "start": "52790",
    "end": "54650"
  },
  {
    "text": "So that's a first transformation\nand that weight matrix",
    "start": "54650",
    "end": "57050"
  },
  {
    "text": "is used at each one\nof these time steps.",
    "start": "57050",
    "end": "60350"
  },
  {
    "text": "There is a second\nlearned weight matrix",
    "start": "60350",
    "end": "62090"
  },
  {
    "text": "which I've called\nwhh to indicate",
    "start": "62090",
    "end": "64280"
  },
  {
    "text": "that we are now traveling\nthrough the hidden layer.",
    "start": "64280",
    "end": "67369"
  },
  {
    "text": "And so we start at\nsome initial state",
    "start": "67370",
    "end": "69060"
  },
  {
    "text": "h0, which could be an all 0 or\na randomly initialized vector,",
    "start": "69060",
    "end": "72350"
  },
  {
    "text": "or a vector coming from some\nother component in the model.",
    "start": "72350",
    "end": "76040"
  },
  {
    "text": "And that representation\nis combined",
    "start": "76040",
    "end": "77858"
  },
  {
    "text": "with the representation that\nwe derive going vertically",
    "start": "77858",
    "end": "80150"
  },
  {
    "text": "up from the embedding, usually\nin some additive fashion",
    "start": "80150",
    "end": "83300"
  },
  {
    "text": "to create this\nhidden state here h1.",
    "start": "83300",
    "end": "86300"
  },
  {
    "text": "And those parameters\nwhh are used again",
    "start": "86300",
    "end": "88850"
  },
  {
    "text": "at each one of these time steps.",
    "start": "88850",
    "end": "90810"
  },
  {
    "text": "So that we have two learned\nweight matrices as part",
    "start": "90810",
    "end": "93649"
  },
  {
    "text": "of the core structure\nof this model,",
    "start": "93650",
    "end": "95588"
  },
  {
    "text": "the one that takes\nus from embeddings",
    "start": "95588",
    "end": "97130"
  },
  {
    "text": "into the hidden\nlayer and the one",
    "start": "97130",
    "end": "98810"
  },
  {
    "text": "that travels us across\nthe hidden layer.",
    "start": "98810",
    "end": "100790"
  },
  {
    "text": "And again, those are\ntypically combined",
    "start": "100790",
    "end": "102710"
  },
  {
    "text": "in some additive\nfashion to create",
    "start": "102710",
    "end": "104330"
  },
  {
    "text": "these internal hidden\nrepresentations.",
    "start": "104330",
    "end": "107090"
  },
  {
    "text": "Now we can do anything we want\nwith those internal hidden",
    "start": "107090",
    "end": "109640"
  },
  {
    "text": "representations.",
    "start": "109640",
    "end": "110960"
  },
  {
    "text": "When we use RNNs\nas classifiers, we",
    "start": "110960",
    "end": "113030"
  },
  {
    "text": "do what is arguably the\nsimplest thing, which",
    "start": "113030",
    "end": "115520"
  },
  {
    "text": "is take the final representation\nand use that as the input",
    "start": "115520",
    "end": "119899"
  },
  {
    "text": "to a standard\nsoftmax classifier.",
    "start": "119900",
    "end": "122300"
  },
  {
    "text": "So from the point of view\nof h3 going to y here,",
    "start": "122300",
    "end": "125120"
  },
  {
    "text": "we just have a learned weight\nmatrix for the classifier,",
    "start": "125120",
    "end": "128220"
  },
  {
    "text": "maybe also a bias term.",
    "start": "128220",
    "end": "129919"
  },
  {
    "text": "But from this point here, this\nis really just a classifier",
    "start": "129919",
    "end": "132403"
  },
  {
    "text": "of the sort we've\nbeen studying up",
    "start": "132403",
    "end": "133820"
  },
  {
    "text": "until this point in the unit.",
    "start": "133820",
    "end": "136067"
  },
  {
    "text": "But of course, we could\nelaborate this model",
    "start": "136067",
    "end": "137900"
  },
  {
    "text": "in all sorts of ways.",
    "start": "137900",
    "end": "139079"
  },
  {
    "text": "It could run bidirectionally.",
    "start": "139080",
    "end": "140330"
  },
  {
    "text": "We could make more full\nuse of the different hidden",
    "start": "140330",
    "end": "143120"
  },
  {
    "text": "representations here.",
    "start": "143120",
    "end": "144200"
  },
  {
    "text": "But in the simplest\nmode, our RNN classifiers",
    "start": "144200",
    "end": "147110"
  },
  {
    "text": "will just derive\nhidden representations",
    "start": "147110",
    "end": "149420"
  },
  {
    "text": "at each time step\nand use the final one",
    "start": "149420",
    "end": "151280"
  },
  {
    "text": "as the input to a classifier.",
    "start": "151280",
    "end": "152903"
  },
  {
    "text": "Couple of things I would\nsay about this first,",
    "start": "152903",
    "end": "154820"
  },
  {
    "text": "if you would like.",
    "start": "154820",
    "end": "155570"
  },
  {
    "text": "A further layer of detail on\nhow these models are structured",
    "start": "155570",
    "end": "158540"
  },
  {
    "text": "and optimized, I encourage\nyou to look at this pure NumPy",
    "start": "158540",
    "end": "162230"
  },
  {
    "text": "reference implementation\nof an RNN classifier",
    "start": "162230",
    "end": "165080"
  },
  {
    "text": "that is included in our\ncourse code distribution.",
    "start": "165080",
    "end": "167370"
  },
  {
    "text": "I think that's a\ngreat way to get",
    "start": "167370",
    "end": "168830"
  },
  {
    "text": "a feel for the recursive\nprocess of computing",
    "start": "168830",
    "end": "173000"
  },
  {
    "text": "through full sequences and then\nhaving the error signals back",
    "start": "173000",
    "end": "176030"
  },
  {
    "text": "propagate through to\nupdate the weight matrix.",
    "start": "176030",
    "end": "179030"
  },
  {
    "text": "But for now, I think\njust understanding",
    "start": "179030",
    "end": "182060"
  },
  {
    "text": "the core structure of\nthis model is sufficient.",
    "start": "182060",
    "end": "186500"
  },
  {
    "start": "185000",
    "end": "250000"
  },
  {
    "text": "I just want to remind you\nfrom the previous screencast,",
    "start": "186500",
    "end": "188840"
  },
  {
    "text": "that we're very close to\nthe idea of distributed",
    "start": "188840",
    "end": "191510"
  },
  {
    "text": "representations of features\nthat I introduced before.",
    "start": "191510",
    "end": "194629"
  },
  {
    "text": "Recall that for this\nmode, what we do",
    "start": "194630",
    "end": "196520"
  },
  {
    "text": "is look up each token\nin an embedding space,",
    "start": "196520",
    "end": "198680"
  },
  {
    "text": "just as we do for the RNN.",
    "start": "198680",
    "end": "200659"
  },
  {
    "text": "But instead of learning\nsome complicated combination",
    "start": "200660",
    "end": "203420"
  },
  {
    "text": "function with a bunch\nof learned parameters,",
    "start": "203420",
    "end": "205880"
  },
  {
    "text": "we simply combine them\nvia sum or average.",
    "start": "205880",
    "end": "208400"
  },
  {
    "text": "And that's the basis.",
    "start": "208400",
    "end": "209519"
  },
  {
    "text": "That's the input to\nthe classifier here.",
    "start": "209520",
    "end": "212060"
  },
  {
    "text": "The RNN can be considered an\nelaboration of that because",
    "start": "212060",
    "end": "214700"
  },
  {
    "text": "instead of assuming\nthat these vectors here",
    "start": "214700",
    "end": "216590"
  },
  {
    "text": "will be combined in some\nsimple way like sum or mean,",
    "start": "216590",
    "end": "219410"
  },
  {
    "text": "we now have really\nvast capacity to learn",
    "start": "219410",
    "end": "222500"
  },
  {
    "text": "a much more complicated\nway of combining them",
    "start": "222500",
    "end": "224930"
  },
  {
    "text": "that is optimal with\nrespect to the classifier",
    "start": "224930",
    "end": "227450"
  },
  {
    "text": "that we're trying to fit.",
    "start": "227450",
    "end": "229099"
  },
  {
    "text": "But fundamentally, these\nare very similar ideas.",
    "start": "229100",
    "end": "231320"
  },
  {
    "text": "And if it happened that sum\nor mean as in this picture",
    "start": "231320",
    "end": "234390"
  },
  {
    "text": "was exactly the right function\nto learn for your data,",
    "start": "234390",
    "end": "237140"
  },
  {
    "text": "then the RNN would certainly\nhave the capacity to do that.",
    "start": "237140",
    "end": "240110"
  },
  {
    "text": "We just tend to favor the\nRNN because it can learn,",
    "start": "240110",
    "end": "242600"
  },
  {
    "text": "of course, a much wider range\nof complicated custom functions",
    "start": "242600",
    "end": "246140"
  },
  {
    "text": "that are particular to the\nproblem that you've posed.",
    "start": "246140",
    "end": "250850"
  },
  {
    "start": "250000",
    "end": "332000"
  },
  {
    "text": "Now so far, we've been\noperating in a mode which",
    "start": "250850",
    "end": "253580"
  },
  {
    "text": "I've called standard RNN\ndata set preparation.",
    "start": "253580",
    "end": "256167"
  },
  {
    "text": "Let's linger over that in\na little bit of detail.",
    "start": "256167",
    "end": "258208"
  },
  {
    "text": "Suppose that we have\ntwo examples containing",
    "start": "258209",
    "end": "260750"
  },
  {
    "text": "the tokens a, b, a and b, c.",
    "start": "260750",
    "end": "263060"
  },
  {
    "text": "Those are our two raw inputs.",
    "start": "263060",
    "end": "265800"
  },
  {
    "text": "The first step in\nthe standard mode",
    "start": "265800",
    "end": "267379"
  },
  {
    "text": "is to look up each one of\nthose in some list of indices.",
    "start": "267380",
    "end": "271790"
  },
  {
    "text": "And then those indices are\nkeyed into an embedding space.",
    "start": "271790",
    "end": "275300"
  },
  {
    "text": "And those finally give us\nthe vector representations",
    "start": "275300",
    "end": "278150"
  },
  {
    "text": "of each examples.",
    "start": "278150",
    "end": "279479"
  },
  {
    "text": "So that really and truly,\nthe inputs of the RNN",
    "start": "279480",
    "end": "282380"
  },
  {
    "text": "is a list of vectors.",
    "start": "282380",
    "end": "284060"
  },
  {
    "text": "It's just that we typically\nobtain those vectors",
    "start": "284060",
    "end": "286580"
  },
  {
    "text": "by looking them up in a\nfixed embedding space.",
    "start": "286580",
    "end": "289370"
  },
  {
    "text": "And so for example,\nsince a occurs twice",
    "start": "289370",
    "end": "291650"
  },
  {
    "text": "in this first example,\nit is literally",
    "start": "291650",
    "end": "293840"
  },
  {
    "text": "repeated as the first\nand third vectors here.",
    "start": "293840",
    "end": "297410"
  },
  {
    "text": "Now I think you can see\nlatent in this picture",
    "start": "297410",
    "end": "299870"
  },
  {
    "text": "the possibility\nthat we might drop",
    "start": "299870",
    "end": "301610"
  },
  {
    "text": "the embedding space and\ninstead just directly",
    "start": "301610",
    "end": "304310"
  },
  {
    "text": "input lists of vectors.",
    "start": "304310",
    "end": "305870"
  },
  {
    "text": "And that is one way\nthat we will explore",
    "start": "305870",
    "end": "307820"
  },
  {
    "text": "later on in the quarter of using\ncontextual models like BERT.",
    "start": "307820",
    "end": "311420"
  },
  {
    "text": "We would simply look\nup entire token streams",
    "start": "311420",
    "end": "313970"
  },
  {
    "text": "and get back lists of vectors\nand use those as fixed inputs",
    "start": "313970",
    "end": "317780"
  },
  {
    "text": "to a model like an RNN.",
    "start": "317780",
    "end": "319520"
  },
  {
    "text": "And that's a first step toward\nfine-tuning models like BERT",
    "start": "319520",
    "end": "323210"
  },
  {
    "text": "on problems like the ones\nwe've posed in this unit.",
    "start": "323210",
    "end": "326669"
  },
  {
    "text": "So have that idea in\nmind as we talk next",
    "start": "326670",
    "end": "329510"
  },
  {
    "text": "about fine-tuning strategies.",
    "start": "329510",
    "end": "332900"
  },
  {
    "start": "332000",
    "end": "405000"
  },
  {
    "text": "Now another practical note.",
    "start": "332900",
    "end": "334610"
  },
  {
    "text": "What I've shown you so\nfar is what you would",
    "start": "334610",
    "end": "336830"
  },
  {
    "text": "call a simple vanilla RNN.",
    "start": "336830",
    "end": "339680"
  },
  {
    "text": "LSTMs, Long Short\nTerm Memory networks",
    "start": "339680",
    "end": "342830"
  },
  {
    "text": "are much more powerful models.",
    "start": "342830",
    "end": "344330"
  },
  {
    "text": "And we'll kind of default to\nthem when we do experiments.",
    "start": "344330",
    "end": "347060"
  },
  {
    "text": "The fundamental issue\nis that plain RNNs",
    "start": "347060",
    "end": "349250"
  },
  {
    "text": "tend to perform poorly\nwith very long sequences.",
    "start": "349250",
    "end": "352340"
  },
  {
    "text": "You get that error signal\nfrom the classifier",
    "start": "352340",
    "end": "354710"
  },
  {
    "text": "there at the final token.",
    "start": "354710",
    "end": "356180"
  },
  {
    "text": "But now information has to\nflow all the way back down",
    "start": "356180",
    "end": "359090"
  },
  {
    "text": "through the network.",
    "start": "359090",
    "end": "359940"
  },
  {
    "text": "Could be a very long sequence.",
    "start": "359940",
    "end": "361760"
  },
  {
    "text": "And the result is\nthat the information",
    "start": "361760",
    "end": "363950"
  },
  {
    "text": "coming from that error signal\nis often lost or distorted.",
    "start": "363950",
    "end": "368180"
  },
  {
    "text": "Now LSTM cells are a prominent\nresponse to this problem.",
    "start": "368180",
    "end": "371539"
  },
  {
    "text": "They introduce mechanisms that\ncontrol the flow of information",
    "start": "371540",
    "end": "375320"
  },
  {
    "text": "and help you avoid the\nproblems of optimization",
    "start": "375320",
    "end": "377750"
  },
  {
    "text": "that arise for regular RNNs.",
    "start": "377750",
    "end": "379965"
  },
  {
    "text": "Now I'm not going to\ntake the time here",
    "start": "379965",
    "end": "381590"
  },
  {
    "text": "to review this\nmechanism in detail.",
    "start": "381590",
    "end": "383900"
  },
  {
    "text": "I would instead recommend\nthese two excellent blog",
    "start": "383900",
    "end": "386389"
  },
  {
    "text": "posts that have great\ndiagrams and really",
    "start": "386390",
    "end": "388910"
  },
  {
    "text": "detailed discussions.",
    "start": "388910",
    "end": "390730"
  },
  {
    "text": "They can do a much\nbetter job than I",
    "start": "390730",
    "end": "392360"
  },
  {
    "text": "can at really conveying the\nintuitions visually and also",
    "start": "392360",
    "end": "395599"
  },
  {
    "text": "with math.",
    "start": "395600",
    "end": "396500"
  },
  {
    "text": "And I think you could\npick one or both",
    "start": "396500",
    "end": "398480"
  },
  {
    "text": "and really pretty quickly gain a\ndeep understanding of precisely",
    "start": "398480",
    "end": "401960"
  },
  {
    "text": "how LSTM cells are functioning.",
    "start": "401960",
    "end": "405751"
  },
  {
    "start": "405000",
    "end": "557000"
  },
  {
    "text": "The final thing\nhere is just a code",
    "start": "405752",
    "end": "407210"
  },
  {
    "text": "snippet to show\nyou how easy it is",
    "start": "407210",
    "end": "408949"
  },
  {
    "text": "to use our course\ncode repository",
    "start": "408950",
    "end": "411140"
  },
  {
    "text": "to fit models like this.",
    "start": "411140",
    "end": "412670"
  },
  {
    "text": "In the context of\nsentiment analysis,",
    "start": "412670",
    "end": "414530"
  },
  {
    "text": "you can again make use\nof this sst library.",
    "start": "414530",
    "end": "417450"
  },
  {
    "text": "And what I've done\nhere is a kind",
    "start": "417450",
    "end": "418850"
  },
  {
    "text": "of complicated\nversion showing you",
    "start": "418850",
    "end": "420650"
  },
  {
    "text": "a bunch of different features.",
    "start": "420650",
    "end": "422040"
  },
  {
    "text": "So in cell 2, you can\nsee that I'm going",
    "start": "422040",
    "end": "425090"
  },
  {
    "text": "to have a pointer to GloVe.",
    "start": "425090",
    "end": "426650"
  },
  {
    "text": "And I'm going to create\na GloVe lookup using",
    "start": "426650",
    "end": "429710"
  },
  {
    "text": "the 50 dimensional vectors\njust to keep things simple.",
    "start": "429710",
    "end": "432949"
  },
  {
    "text": "The feature function\nfor this model",
    "start": "432950",
    "end": "434930"
  },
  {
    "text": "is not one that returns\ncount dictionaries.",
    "start": "434930",
    "end": "437660"
  },
  {
    "text": "It's important for the\nstructure of the model",
    "start": "437660",
    "end": "439790"
  },
  {
    "text": "we're going to use that you\ninput raw sequences of tokens.",
    "start": "439790",
    "end": "443060"
  },
  {
    "text": "So all we're doing here is\ndown casing the sequence",
    "start": "443060",
    "end": "445820"
  },
  {
    "text": "and then splitting\non whitespace.",
    "start": "445820",
    "end": "447530"
  },
  {
    "text": "Of course, you could do\nsomething more sophisticated.",
    "start": "447530",
    "end": "450093"
  },
  {
    "text": "The idea, though, is that you\nwant to align with the GloVe",
    "start": "450093",
    "end": "452509"
  },
  {
    "text": "vocabulary.",
    "start": "452510",
    "end": "453800"
  },
  {
    "text": "Our model wrapper is\ndoing a few things.",
    "start": "453800",
    "end": "456090"
  },
  {
    "text": "It's creating a vocabulary\nand loading it and embedding",
    "start": "456090",
    "end": "458990"
  },
  {
    "text": "using this GloVe space.",
    "start": "458990",
    "end": "460639"
  },
  {
    "text": "That'll be the initial\nembedding for our model.",
    "start": "460640",
    "end": "463287"
  },
  {
    "text": "And if you leave\nthis step out, you'll",
    "start": "463287",
    "end": "464870"
  },
  {
    "text": "have a randomly\ninitialized embedding space",
    "start": "464870",
    "end": "467120"
  },
  {
    "text": "which might be fine as well.",
    "start": "467120",
    "end": "468840"
  },
  {
    "text": "But presumably GloVe\nwill give us a step up.",
    "start": "468840",
    "end": "471590"
  },
  {
    "text": "And then we set up the\nTorch RNN classifier.",
    "start": "471590",
    "end": "474175"
  },
  {
    "text": "And what I've done\nhere is expose",
    "start": "474175",
    "end": "475550"
  },
  {
    "text": "a lot of the different keyword\narguments, not all of them.",
    "start": "475550",
    "end": "478337"
  },
  {
    "text": "There are lots of knobs\nthat you can fiddle with,",
    "start": "478338",
    "end": "480380"
  },
  {
    "text": "as is typical for\ndeep learning models.",
    "start": "480380",
    "end": "482695"
  },
  {
    "text": "Maybe the one I\nwould call out is",
    "start": "482695",
    "end": "484070"
  },
  {
    "text": "that we are using\nthat fixed embedding",
    "start": "484070",
    "end": "485660"
  },
  {
    "text": "that we got from GloVe.",
    "start": "485660",
    "end": "487070"
  },
  {
    "text": "And I have set\nearly_stopping equals true,",
    "start": "487070",
    "end": "489260"
  },
  {
    "text": "which might help you efficiently\noptimize these models.",
    "start": "489260",
    "end": "492230"
  },
  {
    "text": "Otherwise, you'll\nhave to figure out",
    "start": "492230",
    "end": "493730"
  },
  {
    "text": "how many iterations you\nactually want it to run for.",
    "start": "493730",
    "end": "496490"
  },
  {
    "text": "And you might run it for much\ntoo long or much less time",
    "start": "496490",
    "end": "499639"
  },
  {
    "text": "than is needed to\nget an optimal model.",
    "start": "499640",
    "end": "501680"
  },
  {
    "text": "The early stopping\noptions, and there",
    "start": "501680",
    "end": "503570"
  },
  {
    "text": "are a few other parameters\ninvolved in that,",
    "start": "503570",
    "end": "505940"
  },
  {
    "text": "might help you\noptimize these models",
    "start": "505940",
    "end": "507770"
  },
  {
    "text": "efficiently and effectively.",
    "start": "507770",
    "end": "509940"
  },
  {
    "text": "In the end though, having\nset up all that stuff,",
    "start": "509940",
    "end": "511940"
  },
  {
    "text": "you call fit as usual and\nreturn the trained model.",
    "start": "511940",
    "end": "515070"
  },
  {
    "text": "And in that context, you can\nsimply use sst.experiment",
    "start": "515070",
    "end": "518059"
  },
  {
    "text": "with these previous components\nto conduct experiments",
    "start": "518059",
    "end": "520909"
  },
  {
    "text": "with RNNs, just as you did\nfor simpler linear models",
    "start": "520909",
    "end": "524179"
  },
  {
    "text": "as in previous screencasts.",
    "start": "524179",
    "end": "525610"
  },
  {
    "text": "The one change which\nwill be familiar",
    "start": "525610",
    "end": "528443"
  },
  {
    "text": "from the previous\nscreencast, is that you need",
    "start": "528443",
    "end": "530360"
  },
  {
    "text": "to set vectorize equals false.",
    "start": "530360",
    "end": "532550"
  },
  {
    "text": "And that is important\nbecause, again, we're",
    "start": "532550",
    "end": "535220"
  },
  {
    "text": "going to let the model\nprocess these examples.",
    "start": "535220",
    "end": "537246"
  },
  {
    "text": "We don't want to pipe\neverything through some kind",
    "start": "537247",
    "end": "539330"
  },
  {
    "text": "of DictVectorizer.",
    "start": "539330",
    "end": "540800"
  },
  {
    "text": "That's strictly for\nhandbuilt feature functions",
    "start": "540800",
    "end": "543019"
  },
  {
    "text": "and sparse linear models.",
    "start": "543020",
    "end": "544610"
  },
  {
    "text": "Here in the land\nof deep learning,",
    "start": "544610",
    "end": "546110"
  },
  {
    "text": "vectorize equals false.",
    "start": "546110",
    "end": "547760"
  },
  {
    "text": "And we'll use the\ncomponents of the model",
    "start": "547760",
    "end": "549710"
  },
  {
    "text": "to represent each example\nas I discussed before.",
    "start": "549710",
    "end": "553840"
  },
  {
    "start": "553840",
    "end": "558000"
  }
]