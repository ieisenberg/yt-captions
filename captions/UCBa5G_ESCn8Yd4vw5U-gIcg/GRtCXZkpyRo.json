[
  {
    "start": "0",
    "end": "137000"
  },
  {
    "text": "A few reminders for- just for the course in general. So I'll be sending out the project guidelines very soon,",
    "start": "4430",
    "end": "10440"
  },
  {
    "text": "as well as different suggestions for projects if you're still looking for different project topics. Uh, a reminder that the homework is due- Homework 1 is due next Wednesday,",
    "start": "10440",
    "end": "18420"
  },
  {
    "text": "and then Homework 2 will be coming out on the same day. Uh, for the presentations today we'll have around 20 minutes for each presentation, uh,",
    "start": "18420",
    "end": "26310"
  },
  {
    "text": "and people can ask questions and discuss the papers throughout the presentations, uh,",
    "start": "26310",
    "end": "31349"
  },
  {
    "text": "just like during lecture and we'll, uh, we'll try to sharply cut off the presentations,",
    "start": "31349",
    "end": "36675"
  },
  {
    "text": "uh, at even increments so that we can get on to the next one. Uh, also for those of you that are presenting,",
    "start": "36675",
    "end": "42650"
  },
  {
    "text": "pre- if, uh, if there's an audience question, please remember to repeat the question. Uh, this isn't necessarily for the purpose of the room,",
    "start": "42650",
    "end": "48290"
  },
  {
    "text": "but for people watching the recording, then they'll be able to hear what the question was. Um, either repeat the question or answer the question the way that, uh,",
    "start": "48290",
    "end": "55895"
  },
  {
    "text": "that the question is apparent from your answer. Um, yeah. So those are all the, uh, the reminders and information,",
    "start": "55895",
    "end": "63140"
  },
  {
    "text": "and we'll start with the first presentation in a couple of minutes once we figure out the, uh, the Internet.",
    "start": "63140",
    "end": "69930"
  },
  {
    "text": "Yeah. Hi all. So we're just starting the paper review session.",
    "start": "70070",
    "end": "75244"
  },
  {
    "text": "So the first one is meta-learning for a low resource neural machine translation.",
    "start": "75245",
    "end": "80435"
  },
  {
    "text": "It's by. So over to you.",
    "start": "80435",
    "end": "85020"
  },
  {
    "text": "Sweet, thank you. [APPLAUSE] [NOISE] Hi, everyone. Please use the microphone.",
    "start": "85870",
    "end": "91825"
  },
  {
    "text": "Okay, awesome. Hello. [BACKGROUND] Hi, I'm Sabri.",
    "start": "91825",
    "end": "104415"
  },
  {
    "text": "Hi. I'm Ryan. I'm Manurith. Cool. We're going to be talking about meta-learning for low resource neural machine translation.",
    "start": "104415",
    "end": "111085"
  },
  {
    "text": "Awesome. So to start off, um, to give like a brief introduction on the history of like, um, NMT progress.",
    "start": "111085",
    "end": "117470"
  },
  {
    "text": "So historically, like neural machine translation, um, was first based off like statistical methods for language translation,",
    "start": "117470",
    "end": "124159"
  },
  {
    "text": "um, and then over the last few years, as deep learning has gotten a lot better, uh, models have moved towards, um,",
    "start": "124160",
    "end": "130849"
  },
  {
    "text": "NMT basically using deep learning for NLP type tasks, and those have outperformed statistical models on the vast majority of,",
    "start": "130850",
    "end": "140990"
  },
  {
    "start": "137000",
    "end": "157000"
  },
  {
    "text": "uh, like trans- multilingual translation pairs. Uh, and then there's the issue that still exists that for low-resource language pairs,",
    "start": "140990",
    "end": "151280"
  },
  {
    "text": "uh, statistical models are still outperforming because deep learning is very data hungry. So basically,",
    "start": "151280",
    "end": "158540"
  },
  {
    "start": "157000",
    "end": "214000"
  },
  {
    "text": "neural- neural machine translation first started off as just targeting like large monolingual,",
    "start": "158540",
    "end": "163790"
  },
  {
    "text": "uh, corpora, so like, targeting just like English to German or like, just like English to French or something of that nature.",
    "start": "163790",
    "end": "170365"
  },
  {
    "text": "Um, slowly over time, different papers came out basically treating, uh, multiple languages at the same time, uh,",
    "start": "170365",
    "end": "176719"
  },
  {
    "text": "and the context of a single task learning prac- uh, learning, uh, procedure, and then after that they applied like direct transfer learning methods.",
    "start": "176720",
    "end": "187280"
  },
  {
    "text": "So initially, like training a model on like English to German, and then fine tuning on like a different dataset for like English to French or something of that nature.",
    "start": "187280",
    "end": "194675"
  },
  {
    "text": "So the idea here was that we could use neural machine [NOISE] translation methods and like meta-learning",
    "start": "194675",
    "end": "201605"
  },
  {
    "text": "for optimizing fine-tuning on low resource pairs, so that classification learning would go much faster and much more efficient.",
    "start": "201605",
    "end": "211380"
  },
  {
    "text": "So the way the authors first set this up, uh, as a meta-learning problem, is that first they have to define the tasks.",
    "start": "213290",
    "end": "219200"
  },
  {
    "start": "214000",
    "end": "234000"
  },
  {
    "text": "Um, so they divided into two sets. One is the high-resource language translation tasks and then the low-resource language translation tasks.",
    "start": "219200",
    "end": "225620"
  },
  {
    "text": "So each task is essentially a translation between two languages, whether it's like Spanish to English, or French to English, or in the low-resource case, Turkish to English.",
    "start": "225620",
    "end": "234055"
  },
  {
    "start": "234000",
    "end": "420000"
  },
  {
    "text": "So the idea is during meta-training phase, you sample your tasks from these high-resource language translation tasks, uh,",
    "start": "234055",
    "end": "241215"
  },
  {
    "text": "and then you train on these and during meta-test time, you fine-tune on these lower resource,",
    "start": "241215",
    "end": "247655"
  },
  {
    "text": "uh, language trans- translation tasks. Now, um, it's important to note that these low-resource languages are lower resource than the high-resource languages,",
    "start": "247655",
    "end": "255709"
  },
  {
    "text": "but they're still not true low-resource languages. So the authors had to sub-sample in order to mimic a- a low-resource setting.",
    "start": "255710",
    "end": "262884"
  },
  {
    "text": "So typically, um, if you are doing meta-training, your- during the gradient updates,",
    "start": "262885",
    "end": "269745"
  },
  {
    "text": "would first occur, uh, on your training dataset for your task, right? So you're gonna update your parameters theta to theta prime using your training dataset,",
    "start": "269745",
    "end": "278065"
  },
  {
    "text": "and then you compute, uh, the predictions on the test dataset and then you take the derivative of the loss",
    "start": "278065",
    "end": "284150"
  },
  {
    "text": "with respect to the original parameters theta to update the original theta. But what this involves is a second derivative,",
    "start": "284150",
    "end": "290510"
  },
  {
    "text": "and because these guys are using transformers for NMT, transformers are larger models, um, so this is going to be computationally very expensive.",
    "start": "290510",
    "end": "297920"
  },
  {
    "text": "So instead, what they decided to do is a first-order approximation. So instead of taking the derivative of the gradient of the loss with respect to theta,",
    "start": "297920",
    "end": "305690"
  },
  {
    "text": "they take the gradient of the loss with respect to theta prime, and- but they still do the update on theta.",
    "start": "305690",
    "end": "311030"
  },
  {
    "text": "Now, the authors show that these are roughly equivalent, and even in Dr. Finn's MAML paper,",
    "start": "311030",
    "end": "316670"
  },
  {
    "text": "she talks about how, um, they are roughly equivalent, uh, and the intuition behind this is that the ReLU neural networks locally are linear,",
    "start": "316670",
    "end": "325660"
  },
  {
    "text": "so the second order derivatives tend to be 0 in many cases. Uh, so that most of the gains actually happened from these post update parameters,",
    "start": "325660",
    "end": "332570"
  },
  {
    "text": "which are the theta primes. [BACKGROUND]. Cool. Okay, so we've got this cool idea.",
    "start": "332570",
    "end": "339610"
  },
  {
    "text": "We're gonna be using meta-learning for neural machine translations that we get these low-resource languages, in this case like Turkish and Finnish,",
    "start": "339610",
    "end": "345310"
  },
  {
    "text": "and we'll be able to kinda perform better by first meta-learning on a bunch of high resource languages. So just kinda to recap the- the setting is,",
    "start": "345310",
    "end": "351955"
  },
  {
    "text": "we're going meta-train on some language, right? So for example, here we have a Spanish sentence, we're gonna be translating that into English, right?",
    "start": "351955",
    "end": "358345"
  },
  {
    "text": "Um, and then we're going to also be then meta-testing on a low-resource language like Turkish in this case. Um, and kinda one of the main issues or kind of the central issue that the- the author has",
    "start": "358345",
    "end": "367720"
  },
  {
    "text": "kinda tried to overcome with applying meta-learning to this particular problem, is that the, uh,",
    "start": "367720",
    "end": "373775"
  },
  {
    "text": "the meta-train and the meta-test input spaces, aren't aligned or they are not in the same space, right?",
    "start": "373775",
    "end": "379200"
  },
  {
    "text": "And so that is to say, if we have a sentence like Spanish, in Spanish, right, and then we wanna take the word nombre and look up its Spanish word embedding and",
    "start": "379200",
    "end": "386210"
  },
  {
    "text": "that Spanish word embedding is gonna be trained on some monolingual corpora in Spanish. If we look up that embedding, we'll get some embedding for nombre,",
    "start": "386210",
    "end": "393280"
  },
  {
    "text": "but then if we do the same thing in another sentence in a low-resource language like Turkish,",
    "start": "393280",
    "end": "398314"
  },
  {
    "text": "we look up adim, right? That means name roughly in Turkish, right? But when we look it up, right,",
    "start": "398315",
    "end": "404240"
  },
  {
    "text": "we're gonna get another embedding and there's gonna be no correspondence between those two different word embeddings, right? And so we want the embeddings,",
    "start": "404240",
    "end": "410650"
  },
  {
    "text": "you know, in the meta-train, uh, languages and the embeddings and the meta-test languages to be sort of in the same space so that we can really get the most out of meta-training.",
    "start": "410650",
    "end": "419335"
  },
  {
    "text": "Cool. And so the idea that the- that the authors had to kind of address this issue was what they call a universal lexical representation,",
    "start": "419335",
    "end": "426980"
  },
  {
    "start": "420000",
    "end": "982000"
  },
  {
    "text": "and the idea is that they're going to have sort of one set of word embeddings that they're going to share across all of these different languages,",
    "start": "426980",
    "end": "433370"
  },
  {
    "text": "both the meta-train high-resource languages and the, um, low-resource languages at meta-test time.",
    "start": "433370",
    "end": "439599"
  },
  {
    "text": "Okay, so the way that they kinda go about constructing this universal lexical representation, is they first train word embeddings independently for all the different languages.",
    "start": "439600",
    "end": "448520"
  },
  {
    "text": "So all of the low-resource languages and all of the high-resource languages, they get a set of word embeddings and they can do this using monolingual corpora and then like Word2vec or whatever they wanna use.",
    "start": "448520",
    "end": "456675"
  },
  {
    "text": "Cool. So we get word embeddings for all these different languages. And what they're gonna do is they want to create this set of universal embeddings that",
    "start": "456675",
    "end": "463550"
  },
  {
    "text": "all the languages they're gonna share and so what they do actually is they just take the English embeddings, right? But then they also had this set of what they call universal embedding keys,",
    "start": "463550",
    "end": "471785"
  },
  {
    "text": "and the idea is that these are going to be like lookups to look up values in the universal embedding values, right?",
    "start": "471785",
    "end": "477620"
  },
  {
    "text": "So if a word embedding is close to one of the keys, that means that the corresponding value in the universal embedding values probably like has a lot of weight.",
    "start": "477620",
    "end": "485810"
  },
  {
    "text": "So getting pointed that to put this in my- in my shirt.",
    "start": "485810",
    "end": "491375"
  },
  {
    "text": "Okay, cool, awesome.",
    "start": "491375",
    "end": "496970"
  },
  {
    "text": "So how do they actually go about using this? So going back to this example with the word nombre, we take the word nombre, right?",
    "start": "496970",
    "end": "503360"
  },
  {
    "text": "We look up the Spanish word embedding, just like before, except now what we're gonna do is we're gonna pass that Spanish word embedding through a linear transformation,",
    "start": "503360",
    "end": "509675"
  },
  {
    "text": "and then we're going to compare it to all of those different keys in the universal word embeddings keys, right?",
    "start": "509675",
    "end": "514760"
  },
  {
    "text": "And what that's going to produce is going to produce this vector, right? That gives us like a match value, like or how well the Spanish word embedding match with",
    "start": "514760",
    "end": "522770"
  },
  {
    "text": "all the different keys and we're going to call that vector alpha, right? So each of the keys now has this one scalar value telling us how much",
    "start": "522770",
    "end": "528980"
  },
  {
    "text": "it matched with the word embedding in Spanish. Then what we can do is we can multiply that alpha by the universal embedding values.",
    "start": "528980",
    "end": "537170"
  },
  {
    "text": "Remember, those are the universal embeddings that we're sharing across all the different languages, and that's equivalent to doing sort of like a convex combination or like a linear combination of",
    "start": "537170",
    "end": "543815"
  },
  {
    "text": "all of those different embeddings in the universal embedding values. Cool. So the idea is that the words,",
    "start": "543815",
    "end": "550385"
  },
  {
    "text": "the word nombre in Spanish, is going to now be a combination, some weighted sum of all of",
    "start": "550385",
    "end": "555470"
  },
  {
    "text": "the different embeddings in the universal lexical representation, and the cool thing about this is now we can do the same thing with adim, right?",
    "start": "555470",
    "end": "561995"
  },
  {
    "text": "But the advantage is that, so while adim kinda means name in Turkish, in Turkish we decline nouns and so that means that adim kinda means more",
    "start": "561995",
    "end": "569060"
  },
  {
    "text": "like my name and so maybe you could imagine that when we do this linear combination, we're going to get a little bit more wor- weight to",
    "start": "569060",
    "end": "574340"
  },
  {
    "text": "also the word like I or the word my, right? So this is kind of this flexible way of having this one universal representation,",
    "start": "574340",
    "end": "581160"
  },
  {
    "text": "and then at train time, they hold they- they freeze the weights for the Spanish word embeddings.",
    "start": "581160",
    "end": "586430"
  },
  {
    "text": "They also freeze those keys which are just English word embeddings, and then they allow you to train those universal word- the universal word embeddings and they",
    "start": "586430",
    "end": "593510"
  },
  {
    "text": "also train that transformation matrix, um, going from Spanish to the universal embedding keys.",
    "start": "593510",
    "end": "599029"
  },
  {
    "text": "Cool.",
    "start": "599030",
    "end": "605450"
  },
  {
    "text": "So one of the first experiments they ran was to show, um, how does this perform versus a multi-task setting?",
    "start": "605450",
    "end": "613030"
  },
  {
    "text": "So in both the meta-learning case and the multitask case, they first, uh, trained their model on all the high resource tasks,",
    "start": "613030",
    "end": "620275"
  },
  {
    "text": "um, and then they fine tune on the low-resource tasks. Um, so what we- the first thing that we do see is that um, the meta- the",
    "start": "620275",
    "end": "627175"
  },
  {
    "text": "meta-learned ah, model outperforms the equivalent multitask model, uh, across all categories.",
    "start": "627175",
    "end": "633370"
  },
  {
    "text": "So across all of the, uh, low resource language tasks. And now it's interesting note in their diagrams the way they split this out.",
    "start": "633370",
    "end": "639640"
  },
  {
    "text": "So they have all, uh, embedding plus encoding and then just embedding. So what's going on here is that they're not- in the all case,",
    "start": "639640",
    "end": "646450"
  },
  {
    "text": "they're fine tuning all the parameters while in embedding plus encoding, uh, the encoder they're just fine tuning that portion and then they also just fine tune the embedding.",
    "start": "646450",
    "end": "654670"
  },
  {
    "text": "So, uh, over here what's happening is that most the gains happen when you fine-tune both the embedding plus the encoder as opposed to,",
    "start": "654670",
    "end": "661945"
  },
  {
    "text": "uh, fine-tuning the entire model. And this is interesting to see because, um,",
    "start": "661945",
    "end": "667000"
  },
  {
    "text": "the decoder typically in an NMT model, ah, you're going from one language to another language. Here all the tasks are translating to English.",
    "start": "667000",
    "end": "673480"
  },
  {
    "text": "So the decoder is taking the encoded representation of some language and then translating that to English. So it's better not to fine tune",
    "start": "673480",
    "end": "679960"
  },
  {
    "text": "that portion because that's not- that shouldn't- that part shouldn't change. It's mainly the encoder and then the embeddings.",
    "start": "679960",
    "end": "686649"
  },
  {
    "text": "Cool. Okay. I think I'm getting pointed at to use this. Hello. I'm not sure.",
    "start": "687200",
    "end": "693265"
  },
  {
    "text": "Okay, uh, so- one of the- one of the- one other thing to mention here um also,",
    "start": "693265",
    "end": "702670"
  },
  {
    "text": "so they also- ah, when you're doing meta-learning, it can also be useful, like, to have actually like a validation meta test task, right?",
    "start": "702670",
    "end": "712615"
  },
  {
    "text": "To determine when to- to early stop during your meta-training. And so what they do is they actually just compare",
    "start": "712615",
    "end": "718300"
  },
  {
    "text": "different options of which meta lea- which of these low-resource tasks that uses that validation meta-learning um, task.",
    "start": "718300",
    "end": "725440"
  },
  {
    "text": "And so they kinda showed that, that choice is actually significant and that the performance varies depending on which, uh, which validation and task you choose.",
    "start": "725440",
    "end": "733850"
  },
  {
    "text": "Yeah, and then the next experiment they do is they try and vary the amount of training data they have in order to compare",
    "start": "736020",
    "end": "742810"
  },
  {
    "text": "between the multitask and the meta-setting- meta-learning setting. Um, and as the hypothesis, um, suggests, um,",
    "start": "742810",
    "end": "750010"
  },
  {
    "text": "when you have lesser training examples, like in the zero shots setting or a few shots setting, um, the multitask models or the meta-learning model is gonna do a lot better.",
    "start": "750010",
    "end": "758230"
  },
  {
    "text": "Um, but as you keep increasing the size of your, uh, training examples, um, slowly the gap between the multitask and the meta-learning model ah, the gap reduces.",
    "start": "758230",
    "end": "767650"
  },
  {
    "text": "So both perform equ- almost equivalent.",
    "start": "767650",
    "end": "770510"
  },
  {
    "text": "Okay, cool. And so then here they also kinda asked this question of how does our performance vary after fine tuning,",
    "start": "772890",
    "end": "780279"
  },
  {
    "text": "uh, depending on sort of which languages we used during that meta-learning phase? And the main takeaway of the idea was basically, the more languages, the better, right?",
    "start": "780280",
    "end": "787180"
  },
  {
    "text": "So the more languages we used during meta-learning, the better our performance after fine-tuning was, but kind of an interesting study to do there.",
    "start": "787180",
    "end": "793180"
  },
  {
    "text": "The other kind of interesting takeaway from- from this ah, from this table is that last row right there which is fully supervised.",
    "start": "793180",
    "end": "799360"
  },
  {
    "text": "So one thing that we mentioned at the beginning was that these aren't real low-resource languages or I guess like Turkish for example,",
    "start": "799360",
    "end": "804610"
  },
  {
    "text": "is a lower resource language than French, maybe, right? But it certainly isn't like the lowest resource language.",
    "start": "804610",
    "end": "810475"
  },
  {
    "text": "So compared to like real low-resource languages, like Berber, or like Basque or something, right? These are actually pretty high resource languages.",
    "start": "810475",
    "end": "816400"
  },
  {
    "text": "And so to simulate this setting of low-resource languages, what they do is they sub-sample the tokens",
    "start": "816400",
    "end": "822084"
  },
  {
    "text": "in these low-resource languages like Turkish and Finnish, right? And so when they actually do like fully supervised training on that full set,",
    "start": "822085",
    "end": "830995"
  },
  {
    "text": "they get blue scores that are way higher than what they were getting with these sub-samplings, which is to be expected but it also begs this question of like,",
    "start": "830995",
    "end": "836350"
  },
  {
    "text": "\"Well, why didn't you also validate this method on- on like real low-resource languages, right?\"",
    "start": "836350",
    "end": "841975"
  },
  {
    "text": "Uh, I think it would have been be cool if they had, for example, like, yes, some case study for a really, really low resource language-like Berber,",
    "start": "841975",
    "end": "847645"
  },
  {
    "text": "and then push the state of the art on that particular language um, as opposed to just like doing the simulated setting where they're,",
    "start": "847645",
    "end": "853315"
  },
  {
    "text": "uh, where they're kinda under-sampling from- from these different medium resource languages.",
    "start": "853315",
    "end": "858910"
  },
  {
    "text": "Yeah, I mean, I think what Sabri is saying makes a lot of sense because, like, if we were going to build a product to do language translations on low-resource tests, um,",
    "start": "858910",
    "end": "867415"
  },
  {
    "text": "at least for languages that they show, it doesn't seem like the meta NMT model is what you would use, um, because it's clearly not the best performing model.",
    "start": "867415",
    "end": "875120"
  },
  {
    "text": "Any questions or? Yeah.",
    "start": "877830",
    "end": "897230"
  },
  {
    "text": "[inaudible]. Got it. Okay, so I think the question was um,",
    "start": "897230",
    "end": "903010"
  },
  {
    "text": "how does the universe lexical representation get around the low resource problem? And I think that the way that I see it's like",
    "start": "903010",
    "end": "908545"
  },
  {
    "text": "the universal lexical representation doesn't directly address the low-resource problem. What it does is it creates the setting in which",
    "start": "908545",
    "end": "915084"
  },
  {
    "text": "all of these different languages that have totally different vocabularies, often different orthography also, right?",
    "start": "915085",
    "end": "920560"
  },
  {
    "text": "What it allows you to do is, it allows you to, kind of, embed all of these languages into a similar space, which then makes meta learning more effective.",
    "start": "920560",
    "end": "927145"
  },
  {
    "text": "And the meta learning is what's really addressing the low-resource issue. Sure. Well like- like are",
    "start": "927145",
    "end": "932560"
  },
  {
    "text": "the embeddings for the low resource languages, like, [inaudible]",
    "start": "932560",
    "end": "937750"
  },
  {
    "text": "Well, so the- the key thing here is that while like for a low-resource language in neural machine translation, we're usually talking about low resource in the sense that we",
    "start": "937750",
    "end": "944680"
  },
  {
    "text": "don't have a lot of parallel corpora, right? So that is to say we don't have a lot of texts where we have like, you know, Turkish texts and then also English text.",
    "start": "944680",
    "end": "951220"
  },
  {
    "text": "Um, but you might have like a lot of modeling of corpora that you can train those word embeddings on.",
    "start": "951220",
    "end": "956740"
  },
  {
    "text": "So those word embeddings could actually be like kinda decent, I don't know. I think that's kind of the idea here. What makes you think that languages are linearly related?",
    "start": "956740",
    "end": "965450"
  },
  {
    "text": "So like, sorry, the- the idea that like oh, like in the- in the- in this sense, sorry,",
    "start": "965520",
    "end": "971889"
  },
  {
    "text": "the question was, what makes me think that that [LAUGHTER] languages are linearly related?",
    "start": "971890",
    "end": "977365"
  },
  {
    "text": "Um, I don't know if I think that languages are linearly related, but I think that's kind of like a- a question about like sort of",
    "start": "977365",
    "end": "982690"
  },
  {
    "start": "982000",
    "end": "1125000"
  },
  {
    "text": "this part of their- this part of their approach. Um, yeah. I mean, I think the idea is that basically like each one of",
    "start": "982690",
    "end": "991240"
  },
  {
    "text": "these rows in the universal lexical representation doesn't represent like a specific word as much as it represents like some of,",
    "start": "991240",
    "end": "996445"
  },
  {
    "text": "like semantic notion, right? And that different words could be combinations, like, a linear combination of different semantic, like ideas.",
    "start": "996445",
    "end": "1003990"
  },
  {
    "text": "Kind of like yeah, like- with like adim, it means like my name. And so it's like, yeah- yeah also- Um, yeah, so yeah,",
    "start": "1003990",
    "end": "1010560"
  },
  {
    "text": "we didn't mention this during the presentation, but one of the critiques that we were thinking of while reading through this paper is, this transformation matrix A,",
    "start": "1010560",
    "end": "1017670"
  },
  {
    "text": "is shared across all the different, um, languages for the embedding keys. And so there is an implicit assumption there that along these-",
    "start": "1017670",
    "end": "1025665"
  },
  {
    "text": "along the same sub spaces that these keys belong to that they are undergoing the same transformation. So that was like an implicit assumption that they could have",
    "start": "1025665",
    "end": "1034140"
  },
  {
    "text": "gone around by using different transformation matrices for the different keys look ups.",
    "start": "1034140",
    "end": "1039430"
  },
  {
    "text": "During the meta training, uh, with, uh, the languages that have a lot of examples,",
    "start": "1041420",
    "end": "1047804"
  },
  {
    "text": "do they sub-sample those examples to sort of simulate, um, these low-resource languages?",
    "start": "1047805",
    "end": "1053910"
  },
  {
    "text": "I remember- remember in like er, what we were talking about last lecture, we would, you know, simulate handing five pictures over to a",
    "start": "1053910",
    "end": "1061485"
  },
  {
    "text": "measuring algorithm and then teach it to then classify a small number. So, like, is that this same idea being used here?",
    "start": "1061485",
    "end": "1068355"
  },
  {
    "text": "Yes. So yeah, um, I- l think when we meant sub-sampling what that actually is during the fine-tuning phase.",
    "start": "1068355",
    "end": "1075360"
  },
  {
    "text": "So for example, like when they're comparing against the multitask model, um, they don't fine tune on all the available data.",
    "start": "1075360",
    "end": "1081554"
  },
  {
    "text": "They only fine tune on a sub-sample of the data, uh, in order to simulate the low-resource comparison. Yeah. [NOISE]",
    "start": "1081555",
    "end": "1096149"
  },
  {
    "text": "[inaudible] Sorry. We need to switch laptops. So you guys, you can answer the question [OVERLAPPING] Cool. Sure, sure. Did you want to say something?",
    "start": "1096150",
    "end": "1103620"
  },
  {
    "text": "[inaudible]. Is a question of the meta test? [OVERLAPPING] I still don't quite understand what",
    "start": "1103620",
    "end": "1108720"
  },
  {
    "text": "the test based",
    "start": "1108720",
    "end": "1110799"
  },
  {
    "text": "measures [inaudible] So, yeah, if you're talking about specifically during the- so both during meta train and meta test,",
    "start": "1122930",
    "end": "1131450"
  },
  {
    "text": "we're gonna be doing the language translation task. Uh, so any of the embedding lookups that happen during meta train will also happen during meta test.",
    "start": "1131450",
    "end": "1138550"
  },
  {
    "text": "Right. Yeah. I think here maybe- I think maybe the- We had one slide up there which was like we've kind of freeze, like froze some weights,",
    "start": "1138550",
    "end": "1146730"
  },
  {
    "start": "1141000",
    "end": "1627000"
  },
  {
    "text": "uh, maybe with that kind of gave the impression that we were freezing them after meta-learning. Actually what we're doing is we're freezing them for",
    "start": "1146730",
    "end": "1151919"
  },
  {
    "text": "both meta training and meta testing, right? So the other weights were just like learn,",
    "start": "1151920",
    "end": "1157054"
  },
  {
    "text": "they were like learn in unsupervised way. They were embeddings mostly. And then we're learning all the other ones like in",
    "start": "1157055",
    "end": "1162830"
  },
  {
    "text": "both meta train and meta test and then getting the number, yeah. Cool, cool, thank you.",
    "start": "1162830",
    "end": "1175860"
  },
  {
    "text": "[APPLAUSE] Yeah. Ah, so the next talk is, uh, on few-shot autoregressive density estimation and it's been presented by Rohan and Varun.",
    "start": "1175860",
    "end": "1183970"
  },
  {
    "text": "Hi everyone. I'm Rohan and this is Varun and we'll be reviewing this paper on towards learning to learn distributions.",
    "start": "1186890",
    "end": "1194070"
  },
  {
    "text": "Uh, to get started, uh, we wanted to talk a bit about the motivation. Uh, so why would you want to learn distributions?",
    "start": "1194070",
    "end": "1201315"
  },
  {
    "text": "So generative models in general learn distributions and you could use it in many ways.",
    "start": "1201315",
    "end": "1207240"
  },
  {
    "text": "You could use it to enlarge your dataset, maybe for some data augmentation. You could- let say you have some missing pixels in that Obama image there.",
    "start": "1207240",
    "end": "1214545"
  },
  {
    "text": "You want to fill those pixels, you could use a generative model for that, or you have a specific application like",
    "start": "1214545",
    "end": "1220620"
  },
  {
    "text": "a capture thing and you want to generate certain images, you might use this learned distribution for that.",
    "start": "1220620",
    "end": "1226920"
  },
  {
    "text": "And, um, I guess the main theme of this paper is that, uh, we as humans are able to like,",
    "start": "1226920",
    "end": "1234644"
  },
  {
    "text": "uh, if you are given a few images, we are able to generate a new image or understand the latent structure of what is happening in that image.",
    "start": "1234645",
    "end": "1242400"
  },
  {
    "text": "Uh, and the hope, like the idea in this paper is basically to do the same, can you learn to learn these distributions in a few short setting?",
    "start": "1242400",
    "end": "1251775"
  },
  {
    "text": "So the learning setup would be this, uh, a task would be defined as something like given a support set of few images,",
    "start": "1251775",
    "end": "1260445"
  },
  {
    "text": "can I generate a new image which looks similar to the support set? And, uh, for that,",
    "start": "1260445",
    "end": "1266510"
  },
  {
    "text": "uh, like we will have some training tasks. So for- in this case, you have an image of a bicycle or you have an image of,",
    "start": "1266510",
    "end": "1272300"
  },
  {
    "text": "uh, ima- a few images of cups. Uh, and you have a test task where you are",
    "start": "1272300",
    "end": "1277650"
  },
  {
    "text": "given these few images of like a sewing machine. And you- the hope is that you want to generate",
    "start": "1277650",
    "end": "1283274"
  },
  {
    "text": "a new image which looks something similar to the sewing machine. Uh, and the authors in this paper rely on two major techniques,",
    "start": "1283275",
    "end": "1290910"
  },
  {
    "text": "one is the neural attention and one is meta-learning. So, uh, we wanted to take a step back and talk a bit about",
    "start": "1290910",
    "end": "1298200"
  },
  {
    "text": "some prerequisites which will help us understand the models that the, uh, authors talk about. The first is the auto-regressive models.",
    "start": "1298200",
    "end": "1305040"
  },
  {
    "text": "These are types of generative models which are basically use this assumption that there is a sequential ordering when you factorize a joint distribution into a set of marginals.",
    "start": "1305040",
    "end": "1315525"
  },
  {
    "text": "And PixelCNN is, uh, is an example of an auto-regressive model.",
    "start": "1315525",
    "end": "1320970"
  },
  {
    "text": "By, by that I mean that I will be generating these pixels in a sequential fashion.",
    "start": "1320970",
    "end": "1327225"
  },
  {
    "text": "So over here, if I am talking about generation of pixel xi, I would use all the pixels that I have generated so far,",
    "start": "1327225",
    "end": "1334770"
  },
  {
    "text": "which are shown by the blue, like all the blue boxes here. And, uh, what would- I,",
    "start": "1334770",
    "end": "1340725"
  },
  {
    "text": "I have an architecture which is like, uh, very big deep architecture of lots of convolutions and some residual connections and so on.",
    "start": "1340725",
    "end": "1347910"
  },
  {
    "text": "But the idea is that I would do a feed-forward, generate a new pixel, and then do it all over again, and so on, so forth.",
    "start": "1347910",
    "end": "1353625"
  },
  {
    "text": "Uh, this is the third concept which we'll be using. Uh, this is called- I mean, attention.",
    "start": "1353625",
    "end": "1360360"
  },
  {
    "text": "Most of you might be aware of this, but, uh, basically there is, uh, a query vector and a set of keys and values.",
    "start": "1360360",
    "end": "1367274"
  },
  {
    "text": "And in this case, you have a sequence to sequence architecture where you are using the hidden states of the encoder, uh,",
    "start": "1367275",
    "end": "1373470"
  },
  {
    "text": "as keys and the hidden state of the decoder as values and you get, uh, a weighted average as your attention vector.",
    "start": "1373470",
    "end": "1380070"
  },
  {
    "text": "Uh, so in order to solve this problem, the baseline model that the authors talk about is called as conditional PixelCNN.",
    "start": "1380070",
    "end": "1390525"
  },
  {
    "text": "Now, what do you mean by- before going into conditional, what do you mean by conditional PixelCNN?",
    "start": "1390525",
    "end": "1395730"
  },
  {
    "text": "Uh, the, the architecture that they use is called as the gated PixelCNN instead of the normal PixelCNN. Why do they do that?",
    "start": "1395730",
    "end": "1403440"
  },
  {
    "text": "It's because, uh, PixelRNNs were shown to perform better on generation tasks.",
    "start": "1403440",
    "end": "1409095"
  },
  {
    "text": "And why that is the case is because they have some multiplicative units which are modeling some complex interactions.",
    "start": "1409095",
    "end": "1414870"
  },
  {
    "text": "And the authors basically change the PixelCNN architecture to a gated PixelCNN architecture where they're having these like,",
    "start": "1414870",
    "end": "1421799"
  },
  {
    "text": "uh, gating, uh, of mechanism by having that sigmoid which acts as a gate and the tanh, which acts as an activation.",
    "start": "1421800",
    "end": "1429314"
  },
  {
    "text": "Uh, the conditioning here is basically, let's say I'm given, uh, some information about the image that I would like to generate,",
    "start": "1429314",
    "end": "1436770"
  },
  {
    "text": "maybe it's a description. I can convert that into an encoding. For example, something like h here,",
    "start": "1436770",
    "end": "1443040"
  },
  {
    "text": "and I condition on that. So my generation is conditioned on that as an, uh, input.",
    "start": "1443040",
    "end": "1449815"
  },
  {
    "text": "In our case, uh, like going back to the our task which- where we had a set of",
    "start": "1449815",
    "end": "1455299"
  },
  {
    "text": "support images and we would like to generate a new image which looks similar to the support set,",
    "start": "1455300",
    "end": "1461230"
  },
  {
    "text": "we could potentially say that the support set acts as a given information to me,",
    "start": "1461230",
    "end": "1467070"
  },
  {
    "text": "and I could encode it by this function, f of s. Now, f of s is some convolution function which does like some encoding.",
    "start": "1467070",
    "end": "1474480"
  },
  {
    "text": "And you apply the gated function and you condition based on these, these set of equations.",
    "start": "1474480",
    "end": "1480570"
  },
  {
    "text": "So, uh, this acts as the baseline model for generation of images.",
    "start": "1480570",
    "end": "1486210"
  },
  {
    "text": "So, uh, given some support set, I'll be able to use this model to generate an image. Uh, but there is no some sort of like a concept of learning here, like, uh,",
    "start": "1486210",
    "end": "1496200"
  },
  {
    "text": "there is learning per task, but then you are not- you won't be able to generalize that well.",
    "start": "1496200",
    "end": "1501840"
  },
  {
    "text": "So the authors proposed two models. The first model is called the attention PixelCNN.",
    "start": "1501840",
    "end": "1507435"
  },
  {
    "text": "And, uh, the key idea here is that as in when I'm generating these pixels,",
    "start": "1507435",
    "end": "1513690"
  },
  {
    "text": "I want to focus on different aspects of my support set. So my support set is let's say 10 images.",
    "start": "1513690",
    "end": "1520875"
  },
  {
    "text": "I want to- let say I am generating the 10th pixel, I want to focus on different pixels from",
    "start": "1520875",
    "end": "1526410"
  },
  {
    "text": "the support set as compared to gene- like generation for the 11th pixel. So how do they model that is via this CNN based architecture,",
    "start": "1526410",
    "end": "1534210"
  },
  {
    "text": "which is computing some keys and values, and then they have an attention mechanism where they do this transformation and learn their attention weights.",
    "start": "1534210",
    "end": "1541995"
  },
  {
    "text": "And there are two major important points to note here. One is that the authors augmented",
    "start": "1541995",
    "end": "1549105"
  },
  {
    "text": "the support set with some additional features which represent positional data.",
    "start": "1549105",
    "end": "1554745"
  },
  {
    "text": "And the second is that you are learning these weights, but you are not- like for a test task,",
    "start": "1554745",
    "end": "1561945"
  },
  {
    "text": "like a given new task of like, let's say 10 new images, you will not learn the weights or update the weights,",
    "start": "1561945",
    "end": "1569265"
  },
  {
    "text": "you will use the weights that you have learned before, so there is no gradient steps happening here.",
    "start": "1569265",
    "end": "1575940"
  },
  {
    "text": "Um, the next model that the authors proposed is called as the Meta PixelCNN,",
    "start": "1575940",
    "end": "1581009"
  },
  {
    "text": "uh, which is basically using the concept of, uh, meta-learning in a slightly different fashion,",
    "start": "1581010",
    "end": "1586679"
  },
  {
    "text": "uh, where, uh, it- the framework remains the same, but the key distinction is that the, the,",
    "start": "1586680",
    "end": "1593760"
  },
  {
    "text": "they had this notion in- in like meta-learning, you have this notion of the outer loss and the inner loss.",
    "start": "1593760",
    "end": "1599460"
  },
  {
    "text": "The outer loss is modeled as the normal, like negative log likelihood. But the inner loss is something which is learned.",
    "start": "1599460",
    "end": "1606449"
  },
  {
    "text": "So, uh, the g function here represents the inner loss,",
    "start": "1606449",
    "end": "1611490"
  },
  {
    "text": "which is basically taking in the output of the fixed PixelCNN and generating a scalar value,",
    "start": "1611490",
    "end": "1617460"
  },
  {
    "text": "and that scalar value acts as a loss. Uh, and now I would let, uh, Varun talk a bit about the results and the takeaways from the build.",
    "start": "1617460",
    "end": "1625275"
  },
  {
    "text": "Hey, everyone, uh, so yeah. So some of the tasks, uh,",
    "start": "1625275",
    "end": "1630510"
  },
  {
    "start": "1627000",
    "end": "1641000"
  },
  {
    "text": "some in the experiment section, they perform three different tasks. In order of increasing difficulty, uh,",
    "start": "1630510",
    "end": "1636360"
  },
  {
    "text": "they first performed image inversion, then character generation and then finally image generation. The datasets that they used were ImageNet,",
    "start": "1636360",
    "end": "1643995"
  },
  {
    "start": "1641000",
    "end": "1689000"
  },
  {
    "text": "Omniglot, which is basically, uh, 50- characters from 50 different languages and Stanford online product dataset,",
    "start": "1643995",
    "end": "1649515"
  },
  {
    "text": "which basically for each product, there's, uh, a variety of, uh, images that correspond to it.",
    "start": "1649515",
    "end": "1656115"
  },
  {
    "text": "So some of the evaluation metrics that they used for, uh, this paper were,",
    "start": "1656115",
    "end": "1661200"
  },
  {
    "text": "uh, qualitative and quantitative. So qualitatively they would look at the output of the model and determine if it looked good or not.",
    "start": "1661200",
    "end": "1666675"
  },
  {
    "text": "And quantitatively, they use nats per them. So as a review, uh, nats is a effectively a unit of",
    "start": "1666675",
    "end": "1671970"
  },
  {
    "text": "information or entropy based on natural logarithms in powers of e. So one nat would correspond to the negative natural log of 1 over e. And,",
    "start": "1671970",
    "end": "1679980"
  },
  {
    "text": "uh, in this paper, um, basically they looked at the net- the negative, uh, log likelihood and, uh,",
    "start": "1679980",
    "end": "1686295"
  },
  {
    "text": "averaged it across all the pixels. So, uh, for the first task,",
    "start": "1686295",
    "end": "1691980"
  },
  {
    "start": "1689000",
    "end": "1730000"
  },
  {
    "text": "they perf- performed one-shot image generation where the support set was this dog, and basically, they try to invert the dog.",
    "start": "1691980",
    "end": "1699135"
  },
  {
    "text": "Um, as you can see, the conditional PixelCNN and the Meta PixelCNN didn't perform",
    "start": "1699135",
    "end": "1704550"
  },
  {
    "text": "as well as their proposed attention, uh, PixelCNN. And quantitatively, we can see the results reflect the same thing.",
    "start": "1704550",
    "end": "1712095"
  },
  {
    "text": "Um, one of the things that they noted from, uh, the attention PixelCNNs performance was that",
    "start": "1712095",
    "end": "1717300"
  },
  {
    "text": "the attention had learned to effectively, uh, move left to right while the output was writing- sorry,",
    "start": "1717300",
    "end": "1722760"
  },
  {
    "text": "moved to have learned right to left, while the output was writing left to right, which is a naive approach of inverting the image.",
    "start": "1722760",
    "end": "1730210"
  },
  {
    "start": "1730000",
    "end": "1749000"
  },
  {
    "text": "Um, the next, uh, task that they did was a few shot character generation. So if you look at the first two rows,",
    "start": "1730340",
    "end": "1736320"
  },
  {
    "text": "um, you can see that the support for, for support images, uh, they were able to generate characters using three different models.",
    "start": "1736320",
    "end": "1743235"
  },
  {
    "text": "Um, quali- qualitatively, we could see that the attention in Meta PixelCNN, uh, outperformed PixelCNN.",
    "start": "1743235",
    "end": "1749490"
  },
  {
    "start": "1749000",
    "end": "1841000"
  },
  {
    "text": "These are some of the quantitative results. Um, they tried it on different, uh, support set sizes, um,",
    "start": "1749490",
    "end": "1755970"
  },
  {
    "text": "overall the attention PixelCNN outperformed the conditional PixelCNN and even out- outperform the current state of the art- at the time.",
    "start": "1755970",
    "end": "1762645"
  },
  {
    "text": "Um, in addition, um, they tried a different- a new model,",
    "start": "1762645",
    "end": "1767835"
  },
  {
    "text": "which they didn't really talk about in their, um, in their, uh, model section, which is the attention Meta PixelCNN,",
    "start": "1767835",
    "end": "1774780"
  },
  {
    "text": "which is, uh, unique and, uh, but unfortunately, it didn't perform as well as just the attention of PixelCNN,",
    "start": "1774780",
    "end": "1781889"
  },
  {
    "text": "we'll discuss about it later. Um, so in addition, they even perfo- uh, they provided basically, um,",
    "start": "1781889",
    "end": "1788970"
  },
  {
    "text": "how the samples are generated and, uh, basically, where the attention weights are, uh,",
    "start": "1788970",
    "end": "1794085"
  },
  {
    "text": "with respect to the support images as they're generating, uh, each, uh, character.",
    "start": "1794085",
    "end": "1800560"
  },
  {
    "text": "Finally, the third test that they performed was a few shot image generation. So given few images,",
    "start": "1801290",
    "end": "1807225"
  },
  {
    "text": "they wanted to understand latent- the latent structure of the image and try to generate a similar product and substance.",
    "start": "1807225",
    "end": "1817380"
  },
  {
    "text": "So they compared with attention and without attention and the results, they argued that the attention mechanism is better,",
    "start": "1817380",
    "end": "1825735"
  },
  {
    "text": "but it's questionable and they even- and if you look at it quantitatively,",
    "start": "1825735",
    "end": "1831375"
  },
  {
    "text": "it's like the difference is marginal. They argued that maybe there needs to be",
    "start": "1831375",
    "end": "1836610"
  },
  {
    "text": "a better metric to do this sort of task or to measure this sort of task. So some of the takeaways,",
    "start": "1836610",
    "end": "1844160"
  },
  {
    "start": "1841000",
    "end": "1934000"
  },
  {
    "text": "we will break it down into strengths and weaknesses. The strengths, attention is great for flipping images,",
    "start": "1844160",
    "end": "1849485"
  },
  {
    "text": "especially in the one-shot generation case and this seems to be one of the- this is one of the first papers that we've seen",
    "start": "1849485",
    "end": "1856440"
  },
  {
    "text": "that where -we can use- we can apply meta-learning. Okay. So I'll just continue.",
    "start": "1856440",
    "end": "1861540"
  },
  {
    "text": "So the meta generative, yeah, so the meta generative models.",
    "start": "1861540",
    "end": "1866565"
  },
  {
    "text": "Um, so basically some of- one of the, another strength is basically",
    "start": "1866565",
    "end": "1872055"
  },
  {
    "text": "they can apply a meta-learning framework, [NOISE] awesome, er, around generative models, um,",
    "start": "1872055",
    "end": "1878250"
  },
  {
    "text": "to generate unseen characters and then another cool thing was that they were able to learn the inner loss function in the meta-learning framework.",
    "start": "1878250",
    "end": "1884910"
  },
  {
    "text": "[NOISE] Some of the weaknesses. Um, basically, few shot image generation needs a new model.",
    "start": "1884910",
    "end": "1890054"
  },
  {
    "text": "Um, from just the result, it seems that, uh, this approach probably doesn't work well and, um,",
    "start": "1890055",
    "end": "1896640"
  },
  {
    "text": "didn't provide an analysis of the inner loop gradient steps versus performance. Um, it would've been nice if we had seen that.",
    "start": "1896640",
    "end": "1902985"
  },
  {
    "text": "Maybe it would have helped a meta-learning algorithm, er, perform better.",
    "start": "1902985",
    "end": "1907995"
  },
  {
    "text": "Um, and then finally, the -I'm sorry, they added -they had some naive combination of meta-learning and attention.",
    "start": "1907995",
    "end": "1913800"
  },
  {
    "text": "Um, this is also a strength in some sense because this has never been done before, um, but it would have been nice if they explored this idea further.",
    "start": "1913800",
    "end": "1921570"
  },
  {
    "text": "Maybe there's some- there's a model out there that could perform incredibly well. And finally they had inconsistent experiments,",
    "start": "1921570",
    "end": "1927059"
  },
  {
    "text": "specifically with the meta-learning and attention-based model. Um, they only applied it to one task.",
    "start": "1927060",
    "end": "1932085"
  },
  {
    "text": "It would've been nice to see it on all three tasks. And, uh, with that, um, I'd like to open the floor for discussion, um,",
    "start": "1932085",
    "end": "1939419"
  },
  {
    "start": "1934000",
    "end": "2277000"
  },
  {
    "text": "and any questions, if you have any questions, please feel free to talk about them. But, um, just to start,",
    "start": "1939420",
    "end": "1944985"
  },
  {
    "text": "I guess some discourse. Uh, yeah, sure. [BACKGROUND].",
    "start": "1944985",
    "end": "1966066"
  },
  {
    "text": "[inaudible] Right, that's a very good point and like, uh, I mean, we were also thinking about there's no- so, uh,",
    "start": "1966066",
    "end": "1972735"
  },
  {
    "text": "the question was that, uh, this loss that is learned, is not, uh, constrained in any manner.",
    "start": "1972735",
    "end": "1979020"
  },
  {
    "text": "So it could just be learning to be emitting out a 0 but, uh, uh, I mean, we were also thinking about the same and there's one of the points here,",
    "start": "1979020",
    "end": "1988290"
  },
  {
    "text": "the last point here about, uh, whether this loss function should be constrained in some fashion, uh,",
    "start": "1988290",
    "end": "1994380"
  },
  {
    "text": "but we asked the authors and we'll talk about that a bit later and it turns out that, uh, I mean,",
    "start": "1994380",
    "end": "2000305"
  },
  {
    "text": "the loss- the- the- the- the network G in itself is differentiable and also you are minimizing over",
    "start": "2000305",
    "end": "2007700"
  },
  {
    "text": "that and because you have some bias terms and stuff like that, you could prevent it to be 0. So you- it, it might be possible to constrain it to be 0.",
    "start": "2007700",
    "end": "2017030"
  },
  {
    "text": "Yeah. So, uh, but if,",
    "start": "2017030",
    "end": "2023165"
  },
  {
    "text": "if the inner loop actually become 0 then there is no update in the build. The parameters don't get updated in the inner loop.",
    "start": "2023165",
    "end": "2029690"
  },
  {
    "text": "Yeah. So that would mean that in the outer loop also you have very high likelihood which will lead to high loss.",
    "start": "2029690",
    "end": "2037129"
  },
  {
    "text": "Right. But But then if you have some bias terms and those bias terms are not 0 in the network,",
    "start": "2037130",
    "end": "2042169"
  },
  {
    "text": "[OVERLAPPING] then you wouldn't have a zero loss. Yeah. So yeah, so basically that's the down, uh,",
    "start": "2042170",
    "end": "2049115"
  },
  {
    "text": "that's the disadvantage of having of learning a zero loss because it would prevent your parameters from getting",
    "start": "2049115",
    "end": "2055159"
  },
  {
    "text": "updated in the inner loop [OVERLAPPING] which leads to a- Right. Yeah, yeah, that is another point. Yeah, I mean, the outer loop will not see",
    "start": "2055160",
    "end": "2061280"
  },
  {
    "text": "any updates and that's why I like, yeah [OVERLAPPING]. So that would force the- [OVERLAPPING] Inner- inner loss not to be 0.",
    "start": "2061280",
    "end": "2067450"
  },
  {
    "text": "Not to be 0. Right, yeah, that could be another intuition. Yeah. We hadn't thought about it from that perspective.",
    "start": "2067450",
    "end": "2072669"
  },
  {
    "text": "It's a good point. Thank you. Uh, any other questions? Yeah.",
    "start": "2072670",
    "end": "2078859"
  },
  {
    "text": "So in element sharing, it has generating a completely new image, uh, what's preventing this network from just copying the example like this one?",
    "start": "2078860",
    "end": "2088850"
  },
  {
    "text": "Right, but, uh, I mean, you could potentially- You wanna take that? He is asking you. [BACKGROUND].",
    "start": "2088850",
    "end": "2100010"
  },
  {
    "text": "So the question was, uh, what prevents the network from some just copying this net- the images from the support set.",
    "start": "2100010",
    "end": "2107000"
  },
  {
    "text": "Um, because you have this notion of like performance across tasks,",
    "start": "2107000",
    "end": "2112640"
  },
  {
    "text": "uh, you wouldn't like get a, a very good performance.",
    "start": "2112640",
    "end": "2118219"
  },
  {
    "text": "Or like the loss function would not go down if you are just copying. I mean, copying for example,",
    "start": "2118219",
    "end": "2125960"
  },
  {
    "text": "uh, if we go back to the, uh, site of,",
    "start": "2125960",
    "end": "2131480"
  },
  {
    "text": "you know generating like bicycles from, from example, a bicycle was made [OVERLAPPING] for- for the last ones. Yeah, yeah.",
    "start": "2131480",
    "end": "2143000"
  },
  {
    "text": "Right. This slide. Right, so what was preventing it from just copying the source, that's what I mean?",
    "start": "2143000",
    "end": "2147900"
  },
  {
    "text": "Yeah, I mean- [OVERLAPPING] For- for the inversion task, it's, it's learning like an algorithm, right?",
    "start": "2150640",
    "end": "2156470"
  },
  {
    "text": "Right? So why is that, you know, the case it has stopped learning like to copy. [OVERLAPPING]",
    "start": "2156470",
    "end": "2163490"
  },
  {
    "text": "Right, but that's a good point. But then if you see all the images, all the images are slightly different from each other.",
    "start": "2163490",
    "end": "2169835"
  },
  {
    "text": "So if it just copies like one thing from one image, another thing from another image,",
    "start": "2169835",
    "end": "2176315"
  },
  {
    "text": "the final generated image would not make sense at all. So that's why it has to- it's learning- in- in our opinion,",
    "start": "2176315",
    "end": "2183560"
  },
  {
    "text": "I think it's learning a proper distribution of the objects that are sort of important in that image because",
    "start": "2183560",
    "end": "2190490"
  },
  {
    "text": "they- all the images are slightly different from each other. You can't just like pick and copy one of them.",
    "start": "2190490",
    "end": "2196950"
  },
  {
    "text": "And also, uh, an interesting, uh, point was that, um,",
    "start": "2198340",
    "end": "2203825"
  },
  {
    "text": "that Ron made earlier was that in the, um, in the attention-based PixelCNN, uh,",
    "start": "2203825",
    "end": "2209855"
  },
  {
    "text": "they augmented the dataset to include the positional information. Um, and we're- and basically augmented for,",
    "start": "2209855",
    "end": "2219724"
  },
  {
    "text": "I guess the meta PixelCNN and also the, uh, um, the conditional based PixelCNN.",
    "start": "2219725",
    "end": "2225530"
  },
  {
    "text": "So we think maybe the attention learned basically, uh, how to flip the images by looking at that augmented component,",
    "start": "2225530",
    "end": "2233210"
  },
  {
    "text": "which is basically the positional, um, dimensions, if that makes sense.",
    "start": "2233210",
    "end": "2238339"
  },
  {
    "text": "[BACKGROUND]",
    "start": "2238340",
    "end": "2246178"
  },
  {
    "text": "[APPLAUSE]",
    "start": "2246178",
    "end": "2252710"
  },
  {
    "text": "So this one is one-shot imitation learning. It's by Sean and Dan.",
    "start": "2252710",
    "end": "2260900"
  },
  {
    "text": "Yeah, thanks, cool. Yeah. Okay.",
    "start": "2260900",
    "end": "2268454"
  },
  {
    "text": "Uh, hello, my name is Sean, uh, and this is Dan. We're gonna present the paper,",
    "start": "2268455",
    "end": "2274615"
  },
  {
    "text": "One-Shot Imitation Learning, um, to kinda describe the problem setup a little bit and motivation.",
    "start": "2274615",
    "end": "2280090"
  },
  {
    "text": "Um, Imitation Learning is pretty commonly applied to isolated tasks, um, to kinda reproduce, um, you know, demonstrations we see of different things.",
    "start": "2280090",
    "end": "2286599"
  },
  {
    "text": "Ah, but we have this desire to learn from a few demonstrations, um, if we're gonna try and learn, uh,",
    "start": "2286600",
    "end": "2292420"
  },
  {
    "text": "a new task and instantly generalize to those new setups that we might see. Um, in this specific paper, um,",
    "start": "2292420",
    "end": "2298840"
  },
  {
    "text": "the authors ask us to consider this case where there are infinite tasks to learn, each with various instantiations. However, in the paper,",
    "start": "2298840",
    "end": "2304960"
  },
  {
    "text": "they have this kind of like narrow scope of that, which is, um, this like robot- robotic arm that,",
    "start": "2304960",
    "end": "2311020"
  },
  {
    "text": "um, is given like two pairs of demonstrations. Um, and it- uh,",
    "start": "2311020",
    "end": "2317830"
  },
  {
    "text": "where- where each task is to stack blocks that are on this table, um, in a different, um,",
    "start": "2317830",
    "end": "2323635"
  },
  {
    "text": "permutation and a different, um, numbers of towers. So here, it's trying to stack, um,",
    "start": "2323635",
    "end": "2328960"
  },
  {
    "text": "various blocks on top of each other in the policy, which is, um, kind of like our test time to match what's going on in the demonstration.",
    "start": "2328960",
    "end": "2336219"
  },
  {
    "text": "Um, and- um, oh, sorry. Cool. Um, to- to kinda like go over how it splits up into train and test.",
    "start": "2336219",
    "end": "2344515"
  },
  {
    "text": "Um, in the- the train state, you know, we get this- we get a pair of demonstrations. Ah, we have one full demonstration that we look at,",
    "start": "2344515",
    "end": "2350530"
  },
  {
    "text": "uh, with it- with attention. And then from the other demonstration, um, we sa- we have one state from it that we sample.",
    "start": "2350530",
    "end": "2356995"
  },
  {
    "text": "And we compute this optimal action for that state, And we compare it, um, to like the optimal action for that state,",
    "start": "2356995",
    "end": "2363340"
  },
  {
    "text": "um, for the- the unseen kind of policy. That's how we compute our loss. And then during test time, we are also shown a new demonstration for a new task.",
    "start": "2363340",
    "end": "2370450"
  },
  {
    "text": "Um, we- it- I mean, the-- the task will be something we haven't seen before. So some new permutation of blocks that we have to stack in",
    "start": "2370450",
    "end": "2376599"
  },
  {
    "text": "some different order or in some different number of towers, um, and we get some state, we compute some action for that state,",
    "start": "2376600",
    "end": "2382000"
  },
  {
    "text": "and then we'd run this loop multiple times until we achieve some terminal state. Um, to talk about how we implement",
    "start": "2382000",
    "end": "2388510"
  },
  {
    "text": "the architecture or how the authors implemented the architecture of this. There's three neural networks, um, that are,",
    "start": "2388510",
    "end": "2394015"
  },
  {
    "text": "you know, computing these- these different embeddings and taking different actions. So there's the demonstration network, the context network, and the manipulation network.",
    "start": "2394015",
    "end": "2400315"
  },
  {
    "text": "Um, to go through them one by one. In the demonstration network, um, it receives as input some,",
    "start": "2400315",
    "end": "2406330"
  },
  {
    "start": "2401000",
    "end": "2483000"
  },
  {
    "text": "um, trajectory, some demonstrations directory, which is a sequence of frames taken from the camera of the robotic arm,",
    "start": "2406330",
    "end": "2412465"
  },
  {
    "text": "or is some embedding from the neural network that analyzes the images coming from that robotic arm.",
    "start": "2412465",
    "end": "2418090"
  },
  {
    "text": "Um, its output is an embedding of the demonstration, um, to be used by the subsequent neural network in this architecture.",
    "start": "2418090",
    "end": "2424825"
  },
  {
    "text": "Um, the size of this embedding depends on the se- the number of time steps in the demonstration,",
    "start": "2424825",
    "end": "2432130"
  },
  {
    "text": "as well as the number of blocks on the table. Um, so the way that, um, we kind of, um,",
    "start": "2432130",
    "end": "2439450"
  },
  {
    "text": "capture info of- or the- the way we make this problem tractable, um, in this neural network is we- we use",
    "start": "2439450",
    "end": "2444460"
  },
  {
    "text": "temporal dropout because we might have like thousands of- of time steps within, um, our observation.",
    "start": "2444460",
    "end": "2450294"
  },
  {
    "text": "We throw away 95% of them, um, randomly, and then, um, we use a dilated temporal convolution to capture information,",
    "start": "2450294",
    "end": "2458200"
  },
  {
    "text": "um, from the past, but focusing more on the more recent time steps. Um, we then use neighborhood attention to map these- um,",
    "start": "2458200",
    "end": "2466090"
  },
  {
    "text": "these like variable dimensional input into, um, an output, um, representing the number of query hits, um,",
    "start": "2466090",
    "end": "2475210"
  },
  {
    "text": "in- in the context of attention, um, that have the same number of outputs as we have, um, inputs.",
    "start": "2475210",
    "end": "2481090"
  },
  {
    "text": "So, um, it's really important, I guess. Um, the next network is this context network,",
    "start": "2481090",
    "end": "2488050"
  },
  {
    "start": "2483000",
    "end": "2564000"
  },
  {
    "text": "which takes in, um, the current state, the observation from our camera, as well as the embedding produced by the demonstration network.",
    "start": "2488050",
    "end": "2494140"
  },
  {
    "text": "Um, and it produces a context embedding, um, which kind of will tell our manipulation network,",
    "start": "2494140",
    "end": "2499720"
  },
  {
    "text": "um, like what context is to a form and what action we have to take. Um, so it is important for the- the final network that its input is a fixed dimension.",
    "start": "2499720",
    "end": "2511465"
  },
  {
    "text": "So the- basically, the context network traps, um, all of the contexts that we have in this, um,",
    "start": "2511465",
    "end": "2516910"
  },
  {
    "text": "variably sized dimension input, um, into a fixed size output. And the way we do that is temporal attention, um,",
    "start": "2516910",
    "end": "2524380"
  },
  {
    "text": "which produces a vector whose size is proportional, or basically, it removes the dependency on time in our output vector.",
    "start": "2524380",
    "end": "2531520"
  },
  {
    "text": "So, um, we're only dependent on the number of blocks now, um, and then when we apply attention over a current state and we produce",
    "start": "2531520",
    "end": "2539079"
  },
  {
    "text": "fixed dimensional vectors that are no longer dependent upon the number of blocks in our state. Um, and the intuition behind,",
    "start": "2539080",
    "end": "2545605"
  },
  {
    "text": "um, like why we're able to, uh, remove kind of like a lot of information in these like variably sized dimensional vectors,",
    "start": "2545605",
    "end": "2553540"
  },
  {
    "text": "um, is that the number of relevant objects is usually small and fixed. For instance, um, if we have this robotic arm, we're trying to pick up blocks,",
    "start": "2553540",
    "end": "2559720"
  },
  {
    "text": "we probably only care about the block we're trying to pick up and the block trying to put it on top of. And finally, the man- the manipulation network is the simplest network of them all.",
    "start": "2559720",
    "end": "2568990"
  },
  {
    "start": "2564000",
    "end": "2608000"
  },
  {
    "text": "It's a simple multilayered perceptron. Um, basically, it takes in some context embedding and produces",
    "start": "2568990",
    "end": "2574150"
  },
  {
    "text": "some n-dimensional output vector representing the action for this robotic arm to take. Uh, one of the- the things that the authors leave",
    "start": "2574150",
    "end": "2581170"
  },
  {
    "text": "open is the potential for modular training here. So if you have some, um, specialized, um,",
    "start": "2581170",
    "end": "2586390"
  },
  {
    "text": "task where you can effectively train this manipulation network, um, it doesn't need to- necessarily need to be trained in parallel with the rest of these networks.",
    "start": "2586390",
    "end": "2593079"
  },
  {
    "text": "It can be more highly specialized. Um, and that's a solution for one of the problems they noticed later on, which is where a lot of the errors they encounter in training",
    "start": "2593080",
    "end": "2599559"
  },
  {
    "text": "are just because there's a result in mis-manipulation. So it will knock like some block off the table and it'll be like some irre- irrecoverable failure.",
    "start": "2599560",
    "end": "2607285"
  },
  {
    "text": "Um, so you go through like just broadly, um, we have some demonstration.",
    "start": "2607285",
    "end": "2613420"
  },
  {
    "start": "2608000",
    "end": "2650000"
  },
  {
    "text": "Um, we- it's a series of frames. We've applied temporal dropout to it. Um, we apply this temporal convolution to it, um,",
    "start": "2613420",
    "end": "2621715"
  },
  {
    "text": "to- to kind of attract the mos- uh, to folks on most the recent steps.",
    "start": "2621715",
    "end": "2626785"
  },
  {
    "text": "Um, we apply neighborhood attention, um, such that, ah, we can produce this like,",
    "start": "2626785",
    "end": "2632769"
  },
  {
    "text": "uh, variably sized, um, embedding. Um, we apply our context network to this embedding to",
    "start": "2632770",
    "end": "2638410"
  },
  {
    "text": "remove dependencies on time steps and, um, the number of blocks on the table. And then finally, that context embedding is passed into our manipulation network,",
    "start": "2638410",
    "end": "2646210"
  },
  {
    "text": "which takes actions, um, to move the robotic arm. Um, to kind of stop here and briefly summarize and discuss.",
    "start": "2646210",
    "end": "2653455"
  },
  {
    "start": "2650000",
    "end": "2746000"
  },
  {
    "text": "Um, one of the questions I had about- once I read this paper, was that like stacking blocks on top of each other,",
    "start": "2653455",
    "end": "2659395"
  },
  {
    "text": "um, to me might not necessarily be a Meta learning problem, if the only like, um, difference is, um,",
    "start": "2659395",
    "end": "2666295"
  },
  {
    "text": "like the order in which- or the permutation which you're supposed to stack the blocks or the number of tabs you're trying to create.",
    "start": "2666295",
    "end": "2672205"
  },
  {
    "text": "Um, what do you guys think? Is this a- a Meta learning problem or not?",
    "start": "2672205",
    "end": "2677560"
  },
  {
    "text": "[NOISE] So I guess I feel like some of the Meta learning problems that we,",
    "start": "2677560",
    "end": "2684715"
  },
  {
    "text": "you know, talked about, like for- for example, last, uh, April, we talked about inverting an image, which is basically like you- after you've learned an- an algorithm,",
    "start": "2684715",
    "end": "2693310"
  },
  {
    "text": "you're basically done, right? So I guess that could come as Meta learning. And also, if you had different number of blocks,",
    "start": "2693310",
    "end": "2699910"
  },
  {
    "text": "that's like a significant difference in the things you're observing.",
    "start": "2699910",
    "end": "2705460"
  },
  {
    "text": "And on the other hand, you have like potentially [inaudible] restricting yourself to like two blocks at a time. So I guess I just [inaudible]",
    "start": "2705460",
    "end": "2714795"
  },
  {
    "text": "Yeah. Thank you. That's a super good answer. Um, so yeah, I think there's a potential for- for that kind of thing here too.",
    "start": "2714795",
    "end": "2721740"
  },
  {
    "text": "Um, if you guys have any thoughts on, you know, whatever, I look other kinds of tasks this could generalize too [NOISE] this problem setup,",
    "start": "2721740",
    "end": "2728130"
  },
  {
    "text": "I'd be, you know, curious to hear. If not, we can go into the experiments and results. [NOISE]",
    "start": "2728130",
    "end": "2735630"
  },
  {
    "text": "All right. Cool. All right. Um, so the question then becomes,",
    "start": "2735630",
    "end": "2741240"
  },
  {
    "text": "does- how does this model perform? So the authors tried to devise experiments that could answer the following questions, uh, listed here.",
    "start": "2741240",
    "end": "2748500"
  },
  {
    "text": "So the first thing they wanted to answer was, what happens if they changed their training scheme? So the two training schemes they take- that they tested was,",
    "start": "2748500",
    "end": "2756395"
  },
  {
    "text": "uh, behavioral cloning and DAGGER. And for those of you who are not familiar with the terms, by behavioral cloning, what we're doing is we're just literally directly learning-",
    "start": "2756395",
    "end": "2763535"
  },
  {
    "text": "trying to learn to policy using, uh, supervised learning. But by DAGGER, what we're doing is we have",
    "start": "2763535",
    "end": "2768930"
  },
  {
    "text": "an expert that is demonstrating these trajectories, which we initially tried to learn from. And then after that, we also gathered trajectories that our agents,",
    "start": "2768930",
    "end": "2777780"
  },
  {
    "text": "that our learned policy is performing, and then we check those states and ask the expert to perform to like give the good actions,",
    "start": "2777780",
    "end": "2785210"
  },
  {
    "text": "to like label them. And then we're gonna add that to our data so that we're like interacting with the expert as we're learning.",
    "start": "2785210",
    "end": "2792005"
  },
  {
    "text": "So that's, uh, what's going on with DAGGER, uh, repeatedly aggregating, uh, data. So these are the two training schemes that we wanted- uh,",
    "start": "2792005",
    "end": "2798570"
  },
  {
    "text": "that the authors wanted to test. The second one they wanted to answer is, what is the effect of conditioning on different data?",
    "start": "2798570",
    "end": "2805140"
  },
  {
    "text": "So if you, uh, remember back to our network, what we were doing was looking back to the whole demonstration and use that as the input.",
    "start": "2805140",
    "end": "2812984"
  },
  {
    "text": "So what happens if we just look at the very last frame or what happens if we are allowed to look at only key frames,",
    "start": "2812985",
    "end": "2819330"
  },
  {
    "text": "which the, uh, authors manually selected? So how does that affect our results? The last thing was, of course,",
    "start": "2819330",
    "end": "2825810"
  },
  {
    "text": "how does this generalize, does this network learn to generalize? So, um, this is the setup.",
    "start": "2825810",
    "end": "2832260"
  },
  {
    "text": "They had different training and test tasks, each with 2 to 10 blocks of different layouts.",
    "start": "2832260",
    "end": "2837434"
  },
  {
    "text": "And then they collected about 1,000 trajectories per task using a hard-coded policy.",
    "start": "2837434",
    "end": "2842730"
  },
  {
    "text": "And they compared four models. Uh, the first two were using the same architecture that we introduced,",
    "start": "2842730",
    "end": "2848025"
  },
  {
    "text": "but using two different training schemes, uh, behavior cloning and DAGGER. And, uh, the third and fourth one is- uses a slightly different input.",
    "start": "2848025",
    "end": "2857115"
  },
  {
    "text": "Um, the third one only looks at the final state of the trajectory, while the fourth one, um,",
    "start": "2857115",
    "end": "2862950"
  },
  {
    "text": "looks at different, uh, key, uh, snapshots of the whole trajectory. So, um, I just wanna stop here a bit and ask, uh,",
    "start": "2862950",
    "end": "2871095"
  },
  {
    "text": "if anyone has any idea on like expe- expectations on how these models would differ or like how would they perform?",
    "start": "2871095",
    "end": "2878260"
  },
  {
    "text": "Any guesses? If not,",
    "start": "2878570",
    "end": "2883890"
  },
  {
    "text": "[LAUGHTER] I can just proceed and, uh, report what happened. But, uh, just looking at these definitions,",
    "start": "2883890",
    "end": "2889560"
  },
  {
    "text": "what we could guess is, for example, um, because of the third one and fourth one, we are looking at only a subset of the whole trajectory,",
    "start": "2889560",
    "end": "2896310"
  },
  {
    "text": "that's probably gonna be faster, right? And also, um, we might expect that if we only look at the final state,",
    "start": "2896310",
    "end": "2902910"
  },
  {
    "text": "which is the final end image that we want to achieve as compared to the whole trajectory, maybe the performance will not be as good.",
    "start": "2902910",
    "end": "2909795"
  },
  {
    "text": "On- on the other hand, if we know what are the key steps that we have to follow, maybe it's gonna be using, uh,",
    "start": "2909795",
    "end": "2916230"
  },
  {
    "text": "the data efficiently that the model actually performs better. So those are some of the, uh, guesses that the authors made before they proceeded with experiments.",
    "start": "2916230",
    "end": "2924675"
  },
  {
    "start": "2924000",
    "end": "3139000"
  },
  {
    "text": "And these are the actual results of the experiments. There's kind of a lot of colorful bars here, um, but the x-axis is the number of stages required to achieve the task at hand,",
    "start": "2924675",
    "end": "2934935"
  },
  {
    "text": "uh, the number of stages. So for example, one is just- you just need to put a block on top of the other, and so on.",
    "start": "2934935",
    "end": "2940500"
  },
  {
    "text": "So it's- it corresponds to the difficulty of the problem. The y-axis is the success rate. So how successful the models were in performing these tasks.",
    "start": "2940500",
    "end": "2949079"
  },
  {
    "text": "And in terms of the policy, from left to right, we have the hard- hard-coded policy, behavior cloning, DAGGER, uh,",
    "start": "2949080",
    "end": "2956100"
  },
  {
    "text": "only looking at the snapshots or the key frames, and the last one is only looking at the final state.",
    "start": "2956100",
    "end": "2961335"
  },
  {
    "text": "Uh, yes, uh, you can make a few observations. The first observation is that behavior cloning and",
    "start": "2961335",
    "end": "2966779"
  },
  {
    "text": "DAGGER actually seems to perform pretty much equally. Sort of there was- hasn't been too much benefit",
    "start": "2966780",
    "end": "2972494"
  },
  {
    "text": "from human intervention or the hard policy intervention, uh, with DAGGER, the authors assumed that this might be because,",
    "start": "2972495",
    "end": "2979415"
  },
  {
    "text": "um, we're already adding noise, uh, to the data. Uh, they did not really specify what noise was,",
    "start": "2979415",
    "end": "2984590"
  },
  {
    "text": "but [LAUGHTER] they're adding noise to the trajectory. So they assumed that this might be what's, uh, what- why we are able to learn well.",
    "start": "2984590",
    "end": "2990825"
  },
  {
    "text": "And, uh, another point that we can see is that the snapshots and final states, they're actually performing not too well, uh,",
    "start": "2990825",
    "end": "2997920"
  },
  {
    "text": "which is a bit surprising for snapshots. And the authors, again, assumed because, uh,",
    "start": "2997920",
    "end": "3004115"
  },
  {
    "text": "re- uh, if you remember, we were doing those temporal dropouts, we were dropping 95%. Uh, so we're doing some kind of regularization already when we're training,",
    "start": "3004115",
    "end": "3011990"
  },
  {
    "text": "so that, um, when we actually get the data, we kind of know how to deal with it. Um, so those are some possible explanations.",
    "start": "3011990",
    "end": "3018730"
  },
  {
    "text": "And the- this one was for the training task and this one is for the testing task. As you obviously would imagine,",
    "start": "3018730",
    "end": "3025060"
  },
  {
    "text": "they perform much worse. Um, but the story is about similar here, um, the intuition that you get from the results is about the same.",
    "start": "3025060",
    "end": "3034310"
  },
  {
    "text": "And another fun chart here is how- looking at how attention,",
    "start": "3034310",
    "end": "3039664"
  },
  {
    "text": "um, differs across the blocks. So on the x-axis, we have different blocks, A to J,",
    "start": "3039664",
    "end": "3045220"
  },
  {
    "text": "and from the y-axis, starting from the top, we have the time steps across the policy. So like as the policy carries on,",
    "start": "3045220",
    "end": "3051715"
  },
  {
    "text": "which blocks we are paying more attention to. And the one- and the configuration that we want to achieve is ab, cde, fg, hij,",
    "start": "3051715",
    "end": "3059150"
  },
  {
    "text": "which means that we want block A on top of block B, uh, block C on top of D,",
    "start": "3059150",
    "end": "3064370"
  },
  {
    "text": "which is on top of E, and so on. So if you look at this plot, it's actually kind of interesting because they're actually blocked into blocks.",
    "start": "3064370",
    "end": "3072335"
  },
  {
    "text": "Um, so, uh, in the beginning, we're looking at A and B because we have- we know that",
    "start": "3072335",
    "end": "3077570"
  },
  {
    "text": "we have to put block A on top of block B and so on. So these are actually kind of well-separated and kind of well",
    "start": "3077570",
    "end": "3084020"
  },
  {
    "text": "demonstrates intuitively that the model is looking at the right blocks at the right time, that it knows what to do.",
    "start": "3084020",
    "end": "3089869"
  },
  {
    "text": "[NOISE] So this- um, if you- uh, recall what we were talking about.",
    "start": "3089870",
    "end": "3095870"
  },
  {
    "text": "So they had two attentions. One attention was looking at, uh, different blocks. They had another attention looking at different time steps of the demonstration.",
    "start": "3095870",
    "end": "3103130"
  },
  {
    "text": "So this is what we're doing here. So on the x-axis, now, instead of blocks, we have different time steps of the demonstration of, uh,",
    "start": "3103130",
    "end": "3110750"
  },
  {
    "text": "BI that was shown to the model to learn, and our y-axis is the same. So here we see that again,",
    "start": "3110750",
    "end": "3117950"
  },
  {
    "text": "we have the separations. Um, in the beginning, we have- we are focusing on this beginning time steps and that might be because",
    "start": "3117950",
    "end": "3125150"
  },
  {
    "text": "the model demonstration actually moved AB in the earlier time steps and so on.",
    "start": "3125150",
    "end": "3130234"
  },
  {
    "text": "So this is kinda interest- interesting in that they show what these attention values are and it makes intuitive sense when you look at the- uh, the, uh, plots.",
    "start": "3130235",
    "end": "3139640"
  },
  {
    "start": "3139000",
    "end": "3202000"
  },
  {
    "text": "And they also broke down what kind of failures they had, uh, in term- because none- none of the models are perfect,",
    "start": "3139640",
    "end": "3146450"
  },
  {
    "text": "including their hard-coded policy. So they have three types of failure. The wrong move, manipulation failure, and recoverable failure.",
    "start": "3146450",
    "end": "3152720"
  },
  {
    "text": "A wrong move is when the final result is just wrong. The second one is manipulation, uh, failure,",
    "start": "3152720",
    "end": "3158360"
  },
  {
    "text": "when, uh, the- robot basically, for example, drops the block on the ta- on- on- from the table,",
    "start": "3158360",
    "end": "3163970"
  },
  {
    "text": "so it just doesn't know what to do. And the third one is recoverable failure, which is they make a mistake, uh,",
    "start": "3163970",
    "end": "3169430"
  },
  {
    "text": "which they could have fixed if they were given time. [NOISE] So one example was they have been building a tower and",
    "start": "3169430",
    "end": "3174440"
  },
  {
    "text": "they were trying to pick a block up and they just crashed the tower. And over time, they would fix it, but they did not finish it on time.",
    "start": "3174440",
    "end": "3181265"
  },
  {
    "text": "So those are some different, uh, failures that they kind of ran into. And you see, uh,",
    "start": "3181265",
    "end": "3186455"
  },
  {
    "text": "there are actually not that many wrong moves, um, uh, it's- the plot is very hard to see. But [LAUGHTER] the summary is, uh,",
    "start": "3186455",
    "end": "3192319"
  },
  {
    "text": "there weren't actually that many wrong moves, except the case of when we conditioned the final state. Um, and a lot of them were manipulation failures,",
    "start": "3192320",
    "end": "3200119"
  },
  {
    "text": "so they just need to manipulate the arm better. So in terms of takeaways and strengths, um,",
    "start": "3200120",
    "end": "3205250"
  },
  {
    "start": "3202000",
    "end": "3249000"
  },
  {
    "text": "it seems like learning a family of skills actually make learning/performing relevant tasks easier, in this case, stacking the blocks.",
    "start": "3205250",
    "end": "3210800"
  },
  {
    "text": "And it had an interesting breakdown of modular structures. If you remember, there were three modules.",
    "start": "3210800",
    "end": "3216110"
  },
  {
    "text": "Um, some results just like that attention one was very intuitive and clear. And the neighbor attention, uh,",
    "start": "3216110",
    "end": "3221690"
  },
  {
    "text": "which they introduced allowed them to translate, uh, input of variable size to weights of variable size as well.",
    "start": "3221690",
    "end": "3228050"
  },
  {
    "text": "And single-shot result is actually rather impressive of, uh- you can look up the videos in your own time if you feel like it.",
    "start": "3228050",
    "end": "3234440"
  },
  {
    "text": "And while not presented in this paper, the data was actually collected using simulations, not real-life images, and the person was demonstrating using VRs.",
    "start": "3234440",
    "end": "3243395"
  },
  {
    "text": "So that was actually kind of cool as well. Um, some weaknesses, um, that we could think of was this is kind of the weakness of imitation learning itself,",
    "start": "3243395",
    "end": "3252275"
  },
  {
    "start": "3249000",
    "end": "3323000"
  },
  {
    "text": "is that we have to assume that there are successful demonstrations that we- that the model can learn from,",
    "start": "3252275",
    "end": "3258215"
  },
  {
    "text": "and that is not necessarily always the case. And if the demonstration is not optimal, it- if it's just running around,",
    "start": "3258215",
    "end": "3264770"
  },
  {
    "text": "fooling around, then that's what our model might learn. So that's some limitation just innate in imitation learning.",
    "start": "3264770",
    "end": "3270830"
  },
  {
    "text": "And as we've asked before, the tasks are quite similar, so it is a little bit questionable if you can say,",
    "start": "3270830",
    "end": "3276650"
  },
  {
    "text": "\"Hey, this is meta-learning.\" Um, it's definitely a- a step in it, um, but it is still arguable.",
    "start": "3276650",
    "end": "3282530"
  },
  {
    "text": "And the algorithm just doesn't know what to do when we have a block just under four and things like that.",
    "start": "3282530",
    "end": "3288484"
  },
  {
    "text": "Um, there were some other assumptions. And in terms of the actual paper itself, um, if you read through the paper,",
    "start": "3288485",
    "end": "3294170"
  },
  {
    "text": "there are actually not that many equations and descriptions. So there are a lot of natural language descriptions that it's kinda hard to",
    "start": "3294170",
    "end": "3300020"
  },
  {
    "text": "comprehend what the network is actually doing unless you read the actual algorithm, which is like a full page in the appendix.",
    "start": "3300020",
    "end": "3305735"
  },
  {
    "text": "Um, and they only discuss a single experiment, which is stacking the blocks. Uh, so it'd be nice to see",
    "start": "3305735",
    "end": "3311810"
  },
  {
    "text": "what other tasks could have been carried out using this scheme. [NOISE] And also, they actually never really defined what action is,",
    "start": "3311810",
    "end": "3318964"
  },
  {
    "text": "uh, in this paper, at least we could- um, from what we could read. So that was, uh, kind of sad. Um, so these are some further questions that we could think of,",
    "start": "3318965",
    "end": "3326690"
  },
  {
    "start": "3323000",
    "end": "3599000"
  },
  {
    "text": "um, and we have some discussions as well, and, uh, we could go into discussion questions,",
    "start": "3326690",
    "end": "3332885"
  },
  {
    "text": "but as we're almost done with time, if there are any questions, we'll be happy to accept them right now.",
    "start": "3332885",
    "end": "3337820"
  },
  {
    "text": "So is this super clear? [LAUGHTER] So how did the artist [LAUGHTER] take care of the different task which has different blocks?",
    "start": "3339750",
    "end": "3346685"
  },
  {
    "text": "Sorry. So how did the artist take care of different task each having different blocks?",
    "start": "3346685",
    "end": "3351740"
  },
  {
    "text": "So how is this case entered when the project has to have different number of blocks? Right. Um, so [NOISE] that'll depend on,",
    "start": "3351740",
    "end": "3361250"
  },
  {
    "text": "uh, the- the network structure as well as the neighborhood attention mechanism. Um, so that's what allowed us to have like variable length of inputs.",
    "start": "3361250",
    "end": "3370630"
  },
  {
    "text": "And so, for example, like a- at the attention of a demonstration, at that stage we'll have still variable length,",
    "start": "3370630",
    "end": "3376885"
  },
  {
    "text": "but after context network, then we would end up in as an aggregated fixed size dimension that they",
    "start": "3376885",
    "end": "3383195"
  },
  {
    "text": "could like process and go with the action like- choose an action. Yeah.",
    "start": "3383195",
    "end": "3388980"
  },
  {
    "text": "Okay. Let's [inaudible] Okay. [APPLAUSE] [NOISE]",
    "start": "3391270",
    "end": "3398300"
  },
  {
    "text": "Yeah. So the next presentation is massively multitask networks for drug discovery and it's being presented by Andrew, Weston, and Justin.",
    "start": "3398300",
    "end": "3406350"
  },
  {
    "text": "Hi, I'm Andrew. I'm Weston. I'm Justin.",
    "start": "3408070",
    "end": "3413090"
  },
  {
    "text": "Er, we'll be presenting on the paper, massively multitask networks for drug discovery. Uh, so the first question that we have when reading this paper is what is drug discovery,",
    "start": "3413090",
    "end": "3421940"
  },
  {
    "text": "uh, as that we're not in, uh, pharmaceutical engineering or anything like that. Uh, so the goal for pharmaceutical companies is going to be something like, um,",
    "start": "3421940",
    "end": "3430295"
  },
  {
    "text": "they have different drugs that are testing out, um, in an effort to find some attractive molecules for further optimization.",
    "start": "3430295",
    "end": "3436505"
  },
  {
    "text": "Um, but the problem with this, um, is that there's a lot of different compounds and their interactions are pretty complicated.",
    "start": "3436505",
    "end": "3444229"
  },
  {
    "text": "And so one of the ways that they try to go about doing this is by automating the process using machine learning.",
    "start": "3444229",
    "end": "3449990"
  },
  {
    "text": "Um, and the goal there is to predict the interactions between the different targets and the small molecules that are present within, um, their compounds.",
    "start": "3449990",
    "end": "3458345"
  },
  {
    "text": "So for example, in this dataset, like the dude-inha, um, the target might be to, um,",
    "start": "3458345",
    "end": "3465095"
  },
  {
    "text": "find which of the active molecules, uh, might be targeting the enoyl reductase.",
    "start": "3465095",
    "end": "3471180"
  },
  {
    "text": "So, um, for studying this as a machine learning problem, one of the motivations is that the datasets are extremely imbalanced.",
    "start": "3471190",
    "end": "3478985"
  },
  {
    "text": "Um, in fact, only 1-2% of the screen compounds were active against a given target. Um, and we know that these kind of extremely skewed, um,",
    "start": "3478985",
    "end": "3486170"
  },
  {
    "text": "distributions can cause our machine learning algorithms to have problems. Um, the second is that they're",
    "start": "3486170",
    "end": "3491750"
  },
  {
    "text": "very disparate sources of experimental data across multiple targets. And so there's 259 datasets altogether,",
    "start": "3491750",
    "end": "3498470"
  },
  {
    "text": "uh, with 249 tasks. Um, but as you can see here, there are different classes of tasks that we have.",
    "start": "3498470",
    "end": "3504425"
  },
  {
    "text": "Um, and there's not perfect overlap, um, across these different datasets with the different tasks.",
    "start": "3504425",
    "end": "3509630"
  },
  {
    "text": "Um, and so it becomes an interesting, um, case of like what kind of information is present in each dataset and each task,",
    "start": "3509630",
    "end": "3517234"
  },
  {
    "text": "uh, such that the algorithms are able to, uh, profit from multitask learning.",
    "start": "3517235",
    "end": "3522290"
  },
  {
    "text": "And finally, uh, at least in the domain of drug discovery, uh, prior work is unclear",
    "start": "3522290",
    "end": "3528230"
  },
  {
    "text": "whether as to multitask learning is actually beneficial in drug discovery. Uh, so there was a Kaggle competition in 2012,",
    "start": "3528230",
    "end": "3535670"
  },
  {
    "text": "um, but wit- one of the criticisms, um, to the model that was the winner in this competition was that they",
    "start": "3535670",
    "end": "3542390"
  },
  {
    "text": "had too small of a sample size and the gains in predictive accuracy were too small. Uh, another paper by Unterthiner,",
    "start": "3542390",
    "end": "3549220"
  },
  {
    "text": "uh, was that the performance gains, uh, were positive. But then there was a different paper in 2006 by Erhan,",
    "start": "3549220",
    "end": "3555220"
  },
  {
    "text": "which said that, uh, multitasking networks did not consistently outperform single task networks. And so this paper tries to add an extra data point in, um,",
    "start": "3555220",
    "end": "3563975"
  },
  {
    "text": "this literature of multitask learning, oh, at whether as to it actually is better than single task networks.",
    "start": "3563975",
    "end": "3571290"
  },
  {
    "text": "So to give an overview of the method, uh, the first thing that's necessary to do this task is to figure out a way",
    "start": "3573400",
    "end": "3580460"
  },
  {
    "text": "to featurize molecules as that's what our datasets are, there are molecules with whether they're active with a specific target or not.",
    "start": "3580460",
    "end": "3588815"
  },
  {
    "text": "And it turns out prior work has done this before the paper. And there is a way to featurize molecules by looking at different points on",
    "start": "3588815",
    "end": "3596240"
  },
  {
    "text": "the molecule and looking at what it is connected to and have an embeddings for different molecule pieces.",
    "start": "3596240",
    "end": "3602134"
  },
  {
    "text": "And then, sort of hashing them together into a fixed length vector. And the way the method works is it takes as an input layer,",
    "start": "3602135",
    "end": "3611900"
  },
  {
    "text": "this fixed length representation of molecules, um, using the embedding we talked about in the previous slide.",
    "start": "3611900",
    "end": "3618590"
  },
  {
    "text": "And it feeds it through several hidden layers, 1-4, with a variable number of nodes.",
    "start": "3618590",
    "end": "3625190"
  },
  {
    "text": "The, um, the paper tries several different architectures with the different variable number of nodes in each hidden layer.",
    "start": "3625190",
    "end": "3633125"
  },
  {
    "text": "And the last layer, it's a very simple multitask network. It's just a different softmax node for each particular dataset.",
    "start": "3633125",
    "end": "3641180"
  },
  {
    "text": "So, um, most of the parameters are shared, and it's only at the last layer where you have task specific parameters. All right.",
    "start": "3641180",
    "end": "3651240"
  },
  {
    "text": "So they tried to, uh, understand how their models are working through a number of experiments to answer a bunch of",
    "start": "3651670",
    "end": "3658130"
  },
  {
    "text": "questions which are: \"How do you know multitask neural nets perform relative to baselines? How does adding more tasks affect the accuracy on a held in set of tasks?",
    "start": "3658130",
    "end": "3667010"
  },
  {
    "text": "Uh, would we rather have more tasks or more examples? How does adding more tasks affect pre-training accuracy?",
    "start": "3667010",
    "end": "3674360"
  },
  {
    "text": "And then, when do datasets benefit from multitask training?\" Uh, so Experiment 1,",
    "start": "3674360",
    "end": "3680735"
  },
  {
    "text": "how do multitask neuron- neural nets perform relative to baselines? Basically, they're just training a bunch of simple machine learning models on each of their tasks and then compare their,",
    "start": "3680735",
    "end": "3690260"
  },
  {
    "text": "uh, multitask networks to those baselines. So the first five lines here are four simple baselines,",
    "start": "3690260",
    "end": "3697954"
  },
  {
    "text": "logistic regression, random forest, and two simpler neural nets. Uh, and then the max here is just, uh,",
    "start": "3697955",
    "end": "3704270"
  },
  {
    "text": "the like task Y's max. So taking the best model for each task and evaluating those.",
    "start": "3704270",
    "end": "3710780"
  },
  {
    "text": "And then, they train two multitask networks and showed that these multitask networks do better.",
    "start": "3710780",
    "end": "3716000"
  },
  {
    "text": "Um, the first one just has a single, uh, hidden layer of length 1,200.",
    "start": "3716000",
    "end": "3721595"
  },
  {
    "text": "And they note that this is not the best design because if your final hidden layer is 1,200 long,",
    "start": "3721595",
    "end": "3727985"
  },
  {
    "text": "then each of your softmax nodes needs to have 1,200 parameters. Whereas if you have a, uh,",
    "start": "3727985",
    "end": "3734570"
  },
  {
    "text": "two hidden layers where you have 2,000 in the first one and then 100 in the second one, then you only need 100 parameters per softmax layer.",
    "start": "3734570",
    "end": "3742505"
  },
  {
    "text": "And so that allows you to avoid overfitting, uh, to individual tasks in those softmax layers.",
    "start": "3742505",
    "end": "3749869"
  },
  {
    "text": "Um, so then, in Experiment 2, they ask, \"How does adding more tests- tasks, excuse me,",
    "start": "3749870",
    "end": "3756155"
  },
  {
    "text": "affect the accuracy on a set of held-in tasks?\" So they set this up as, they trained a bunch of models on the same 10 tasks, and then, uh,",
    "start": "3756155",
    "end": "3765350"
  },
  {
    "text": "add a variable number of additional randomly sampled tasks, and observe the accuracy as a function of the number of additional tasks.",
    "start": "3765350",
    "end": "3771920"
  },
  {
    "text": "So the things that can happen as you add, uh, more tasks to your model are either you kind of increase your accuracy, uh,",
    "start": "3771920",
    "end": "3781040"
  },
  {
    "text": "continually as you add more tasks, you plateau as you add more task and you don't really get any gains in accuracy,",
    "start": "3781040",
    "end": "3786710"
  },
  {
    "text": "or you go up for a while and then back down and some amount of tasks begins to hurt you.",
    "start": "3786710",
    "end": "3792290"
  },
  {
    "text": "Uh, so they run it on some held in set of 10 tasks and they find that on average,",
    "start": "3792290",
    "end": "3800195"
  },
  {
    "text": "uh, the accuracy continues to increase over time. Uh, but notably, for many of them,",
    "start": "3800195",
    "end": "3806900"
  },
  {
    "text": "the accuracy initially decreases with addition of some tasks. Yeah. Sir, I think you guys have said this before.",
    "start": "3806900",
    "end": "3812809"
  },
  {
    "text": "What- what's the definition of a task? Great. So a task is, uh, something like this.",
    "start": "3812810",
    "end": "3820940"
  },
  {
    "text": "So you have a- uh, basically, you wanna know whether something [NOISE] will interact with some target.",
    "start": "3820940",
    "end": "3827360"
  },
  {
    "text": "Uh, so you have a bunch of different input mol- molecules and you're asking, will it interact with this thing? [BACKGROUND] Um.",
    "start": "3827360",
    "end": "3834600"
  },
  {
    "text": "So yeah, so each of these colored lines is an- different individual, uh, interaction.",
    "start": "3837640",
    "end": "3843394"
  },
  {
    "text": "And then the black line with the confidence interval in the middle is the average. So on average, it continues to increase over, uh,",
    "start": "3843395",
    "end": "3851585"
  },
  {
    "text": "with the addition of additional tasks, but in many cases, it actually hurts the accuracy at first and then goes up.",
    "start": "3851585",
    "end": "3857525"
  },
  {
    "text": "And in some cases, it continues to hurt the accuracy and it doesn't really recover from that, no matter how many tasks you add.",
    "start": "3857525",
    "end": "3864065"
  },
  {
    "text": "Uh, and they only really addressed the average case here. They note that sometimes it decreases,",
    "start": "3864065",
    "end": "3869809"
  },
  {
    "text": "but they don't offer any explanation of that, which I really would have liked. Um, [NOISE] and they also choose these 10 tasks,",
    "start": "3869810",
    "end": "3876770"
  },
  {
    "text": "but don't analyze any of the other tasks, uh, which I, I think that this result is probably sensitive to the task that they chose,",
    "start": "3876770",
    "end": "3885095"
  },
  {
    "text": "uh, and I would have liked to see it on all of the tasks, kind of different types.",
    "start": "3885095",
    "end": "3890240"
  },
  {
    "text": "Uh, so then in Experiment 3, they asked, would you rather have more tasks or more examples in our dataset?",
    "start": "3890240",
    "end": "3897515"
  },
  {
    "text": "Uh, and so here, uh, the x-axis is the addition of new tasks,",
    "start": "3897515",
    "end": "3902900"
  },
  {
    "text": "the y-axis is accuracy, and then these different colored lines are additional examples in the dataset.",
    "start": "3902900",
    "end": "3909889"
  },
  {
    "text": "Uh, and basically, they both help kind of orthogonally, uh, and you can see there's trends in, uh,",
    "start": "3909889",
    "end": "3918185"
  },
  {
    "text": "in both directions as you add data points and as we add tasks, which is what we'd expect.",
    "start": "3918185",
    "end": "3923360"
  },
  {
    "text": "Uh, unfortunately, the confidence intervals here are really wide, and I think that's because they did the same setup as here where they",
    "start": "3923360",
    "end": "3929900"
  },
  {
    "text": "chose these just 10 held out, uh, hel- held in, rather, uh,",
    "start": "3929900",
    "end": "3935030"
  },
  {
    "text": "tasks and evaluated on those rather than evaluating on all kinds of different tasks. Um, and so I wish that they had done that, uh,",
    "start": "3935030",
    "end": "3943265"
  },
  {
    "text": "to have some more, uh, general- generalizable results to all of their tasks.",
    "start": "3943265",
    "end": "3949010"
  },
  {
    "text": "Uh, and then finally, Experiment 4, they asked how does adding more tasks affect pre-training accuracy?",
    "start": "3949010",
    "end": "3955970"
  },
  {
    "text": "So if you pre-train a model on a bunch of tasks and then, uh, use those ways to initialize a model- a single task model for a new task,",
    "start": "3955970",
    "end": "3964535"
  },
  {
    "text": "how does that affect the accuracy? Uh, and again, they said that on average, it continues to increase a little bit,",
    "start": "3964535",
    "end": "3971450"
  },
  {
    "text": "but in some cases, it hurts a lot. Uh, and this time they actually point that out and say, \"We hypothesize that the extent of this generalizability is determined by",
    "start": "3971450",
    "end": "3979280"
  },
  {
    "text": "the presence or absence of relevant data in the multi-task training set.\" Uh, and this is the only analysis they give of this,",
    "start": "3979280",
    "end": "3986589"
  },
  {
    "text": "which, uh, I feel like this is kind of an obvious statement, that if you're pre-trained on something that it",
    "start": "3986590",
    "end": "3992150"
  },
  {
    "text": "matters how relevant the data you're pre-training is, uh, and really I'd, I'd like the whole paper to kinda be addressing this point,",
    "start": "3992150",
    "end": "3999140"
  },
  {
    "text": "that how do we decide if data is relevant or not? Uh, but they don't. They just say that sometimes it'll hurt you,",
    "start": "3999140",
    "end": "4006190"
  },
  {
    "text": "uh, and that's it. Um, cool. You're gonna talk about the last one.",
    "start": "4006190",
    "end": "4011950"
  },
  {
    "text": "Yeah. So the last experiments, um, or the last set,",
    "start": "4011950",
    "end": "4016990"
  },
  {
    "text": "set of experiments they do is to try to figure out when specifically do datasets benefit from multi-task training?",
    "start": "4016990",
    "end": "4024235"
  },
  {
    "text": "So uh, first way we going to test this is they create this metric called active occurrence rates,",
    "start": "4024235",
    "end": "4030700"
  },
  {
    "text": "which is basically for a specific dataset and a specific molecule. They measure the number of other datasets that that molecule is also active in,",
    "start": "4030700",
    "end": "4040930"
  },
  {
    "text": "and they sort of want to see if a molecule is active in a lot of datasets, um,",
    "start": "4040930",
    "end": "4047440"
  },
  {
    "text": "does multitask learning help that, um, train better for a specific task than it would",
    "start": "4047440",
    "end": "4053170"
  },
  {
    "text": "if it's only active for like a small number of tasks? And as you can see, um,",
    "start": "4053170",
    "end": "4058780"
  },
  {
    "text": "this graph sort of shows what the active occurrence rate is versus how much it helps the specific tasks problem for doing multitask learning.",
    "start": "4058780",
    "end": "4068815"
  },
  {
    "text": "And, um, well, it's a- it's a very noisy graph. You can see [NOISE] there's clearly a positive correlation between what",
    "start": "4068815",
    "end": "4076480"
  },
  {
    "text": "the active occurrence rate is and how much it helps to do multitask learning.",
    "start": "4076480",
    "end": "4081790"
  },
  {
    "text": "And in this vein, another thing they want to figure out was, is there a specific class of target molecules for which, uh,",
    "start": "4081790",
    "end": "4090775"
  },
  {
    "text": "multi-task training is more beneficial than any other task, um, or any other category of tasks?",
    "start": "4090775",
    "end": "4097975"
  },
  {
    "text": "So they put them in these categories, and ones that didn't fit an obvious category,",
    "start": "4097975",
    "end": "4104020"
  },
  {
    "text": "they just put in a miscellaneous category. And, um, each black dot represents a specific target, um, dataset.",
    "start": "4104020",
    "end": "4113755"
  },
  {
    "text": "And it appears there's like, not much difference in what kind of benefit you",
    "start": "4113755",
    "end": "4119440"
  },
  {
    "text": "can get from being in a different category, although some of them do have like marginal variation,",
    "start": "4119440",
    "end": "4125770"
  },
  {
    "text": "uh, but in general, it appears this, um, the benefit for multi-task learning",
    "start": "4125770",
    "end": "4131095"
  },
  {
    "text": "isn't really that dependent on which category it's in. All right. And uh, to give an overview of",
    "start": "4131095",
    "end": "4138819"
  },
  {
    "text": "what we thought the strengths of the paper were. Um, primarily, we thought one,",
    "start": "4138820",
    "end": "4144339"
  },
  {
    "text": "it's a very detailed empirical analysis on real-world data, which is always a good thing in a machine-learning paper.",
    "start": "4144340",
    "end": "4151690"
  },
  {
    "text": "Um, they're able to tackle a challenging problem with an extreme data skew where you're doing a binary classification,",
    "start": "4151690",
    "end": "4159520"
  },
  {
    "text": "but only 1 to 2% of the input data is actually active, uh, which is a very difficult problem in general and they do it well here.",
    "start": "4159520",
    "end": "4168085"
  },
  {
    "text": "They use a simple network which enables for very detailed and simple analysis.",
    "start": "4168085",
    "end": "4173694"
  },
  {
    "text": "And they're exploring under what conditions multitask learning produces positive and negative results and have",
    "start": "4173695",
    "end": "4180790"
  },
  {
    "text": "very thorough experiments on the specific real-world problem. And they also achieve results using",
    "start": "4180790",
    "end": "4187600"
  },
  {
    "text": "a multi-task network that outperformed any other baseline that they could come up with,",
    "start": "4187600",
    "end": "4192609"
  },
  {
    "text": "which, um, is a positive showing that this multitask learning is actually beneficial for this particular type of problem.",
    "start": "4192610",
    "end": "4200110"
  },
  {
    "text": "[NOISE] Um, but at the same time, there were some weaknesses, um,",
    "start": "4200110",
    "end": "4206679"
  },
  {
    "text": "as are there in any other paper. Um, so the first weakness is that there was a huge correlation between the data size and the number of tasks.",
    "start": "4206680",
    "end": "4214615"
  },
  {
    "text": "Um, just because for any given dataset, uh, you might find that it only has five tasks or something like that.",
    "start": "4214615",
    "end": "4220390"
  },
  {
    "text": "And so, you know, if you have 40 tasks but each dataset is only able to have, uh, five tasks in each,",
    "start": "4220390",
    "end": "4226315"
  },
  {
    "text": "um, then there's not going to be as much overlap. And so, um, that was a big limitation that we saw in one of the, uh,",
    "start": "4226315",
    "end": "4232270"
  },
  {
    "text": "quite a few of the experiments, uh, where if you actually wanted to see, um, what is the difference when you have 20 tasks versus 40 tasks, um,",
    "start": "4232270",
    "end": "4240355"
  },
  {
    "text": "the amount of data that you can actually have in the, um, 20 task case is going to be very different and very much smaller.",
    "start": "4240355",
    "end": "4246970"
  },
  {
    "text": "Um, the second is, um, while they gave us some good intuition, um, there wasn't much, uh,",
    "start": "4246970",
    "end": "4252655"
  },
  {
    "text": "theoretical justification when or when to not use multitask learning. Um, obviously they draw upon some, uh,",
    "start": "4252655",
    "end": "4259840"
  },
  {
    "text": "a level of, um, like domain knowledge and saying that, uh, perhaps these things are categorized as certain types of tasks, uh,",
    "start": "4259840",
    "end": "4268150"
  },
  {
    "text": "or maybe there is some relationship between the size of the dataset and then, um, the variety of tasks that are employed.",
    "start": "4268150",
    "end": "4273760"
  },
  {
    "text": "Uh, but again, I think as an empirical paper, um, it does give us intuition,",
    "start": "4273760",
    "end": "4279550"
  },
  {
    "text": "but not so much a strong theory to go upon. Um, and lastly, uh, whereas we did praise the simplicity of the network,",
    "start": "4279550",
    "end": "4287020"
  },
  {
    "text": "uh, for this analysis, uh, we all do also wished that they could have explored all of the architectures, uh, given that, uh,",
    "start": "4287020",
    "end": "4293784"
  },
  {
    "text": "this is a real-world problem. It could be interesting to see, okay, how far can we go with this drug discovery,",
    "start": "4293785",
    "end": "4299050"
  },
  {
    "text": "um, as an automated process. Cool. And then potential improvements which are related to weaknesses.",
    "start": "4299050",
    "end": "4307389"
  },
  {
    "text": "Uh, so both more theoretical and more empirical results on task overlap,",
    "start": "4307390",
    "end": "4313300"
  },
  {
    "text": "uh, and how that is going to affect our accuracy, um, maybe something like an analysis of how covariance between classes is going to, uh,",
    "start": "4313300",
    "end": "4323425"
  },
  {
    "text": "help classes learn from each other, uh, or something simpler, just like saying, \"So we have these different categories of tasks.",
    "start": "4323425",
    "end": "4330175"
  },
  {
    "text": "If we train a single model that does all of the tasks in one category,",
    "start": "4330175",
    "end": "4335574"
  },
  {
    "text": "does that give us better accuracy than mixing all these tests together?\" Uh, and I think you probably could have gotten some more,",
    "start": "4335575",
    "end": "4341140"
  },
  {
    "text": "uh, fine grained results from doing that. Uh, controlling the training set size versus the number of tasks.",
    "start": "4341140",
    "end": "4346900"
  },
  {
    "text": "So it would have been good to say like, uh, control the number of like fixed X-Y pairs that are",
    "start": "4346900",
    "end": "4354520"
  },
  {
    "text": "going- being fed into the model during training time, whether they are on different tasks or the same x value rather- the,",
    "start": "4354520",
    "end": "4362470"
  },
  {
    "text": "the same x-value on different tasks or the, the opposite.",
    "start": "4362470",
    "end": "4368085"
  },
  {
    "text": "Uh, also comparing different architectures like we said, um, they only do this one very simple, uh,",
    "start": "4368085",
    "end": "4375570"
  },
  {
    "text": "embedding and the one, uh, multi-layer perceptron, uh, setup.",
    "start": "4375570",
    "end": "4381909"
  },
  {
    "text": "And they do say that their results are kind of dependent on using these smaller second hidden layers,",
    "start": "4381910",
    "end": "4387310"
  },
  {
    "text": "but they don't offer any analysis of how it changes, uh, as they change their architecture. Uh, and then finally,",
    "start": "4387310",
    "end": "4393160"
  },
  {
    "text": "they could have bench-marked against, uh, models from related papers. They say, basically, that,",
    "start": "4393160",
    "end": "4398500"
  },
  {
    "text": "uh, it's hard to do, so they didn't do it. But I feel like there must have been some result that they could have",
    "start": "4398500",
    "end": "4404500"
  },
  {
    "text": "said compared to some other model or some other method that people are using. Uh, takeaways.",
    "start": "4404500",
    "end": "4411665"
  },
  {
    "text": "Yeah. So, um, as, um, machine learning researchers and not chemists,",
    "start": "4411665",
    "end": "4417675"
  },
  {
    "text": "we're interested in what can we take away from this in our own modeling. Um, so first is, uh,",
    "start": "4417675",
    "end": "4422699"
  },
  {
    "text": "what seems to be the thesis of this paper that multitask learning can yield superior results to single task learning.",
    "start": "4422700",
    "end": "4428489"
  },
  {
    "text": "Um, and that there- second, there's limited transferability to task not contained in the training set.",
    "start": "4428490",
    "end": "4434010"
  },
  {
    "text": "Um, they say limited because they saw it in some cases and not others. Um, third, multitask affects stronger for some datasets than others,",
    "start": "4434010",
    "end": "4441550"
  },
  {
    "text": "so it is pretty heterogeneous and, um, it seems to require some level of domain knowledge if you're going to employ multitask learning.",
    "start": "4441550",
    "end": "4447905"
  },
  {
    "text": "Um, fourth, the presence of shared active compounds moderately correlated with multitask improvement.",
    "start": "4447905",
    "end": "4453795"
  },
  {
    "text": "Um, again, this, uh, this was in relation to the fifth experiment,",
    "start": "4453795",
    "end": "4459140"
  },
  {
    "text": "um, where they saw that, like if a molecule was, uh, in 30 task versus only in 5 tasks,",
    "start": "4459140",
    "end": "4466660"
  },
  {
    "text": "then it would, uh, benefit more from multi- multi-task training. Um, and lastly, the efficacy of multitask learning is directly,",
    "start": "4466660",
    "end": "4474085"
  },
  {
    "text": "uh, related to the availability of the relevant data. Uh, again, pointing to the fifth experiment, um,",
    "start": "4474085",
    "end": "4479769"
  },
  {
    "text": "looking at just how many tasks each data point is associated with. And that concludes our presentation.",
    "start": "4479770",
    "end": "4487240"
  },
  {
    "text": "Are there any questions? [NOISE] Then thank you.",
    "start": "4487240",
    "end": "4495790"
  },
  {
    "text": "[APPLAUSE] So hopefully, some of the limitations in those papers will",
    "start": "4495790",
    "end": "4503050"
  },
  {
    "text": "inspire some good final projects for the class, uh, and I'll see everyone on Monday.",
    "start": "4503050",
    "end": "4510110"
  }
]