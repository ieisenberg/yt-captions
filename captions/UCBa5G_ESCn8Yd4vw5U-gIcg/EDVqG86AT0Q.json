[
  {
    "start": "0",
    "end": "21000"
  },
  {
    "start": "0",
    "end": "4897"
  },
  {
    "text": "SPEAKER: Welcome back, everyone.",
    "start": "4897",
    "end": "6229"
  },
  {
    "text": "This is part four in our series\non information retrieval.",
    "start": "6230",
    "end": "9220"
  },
  {
    "text": "We come to the heart of it,\nneural information retrieval.",
    "start": "9220",
    "end": "11890"
  },
  {
    "text": "This is the class\nof models that has",
    "start": "11890",
    "end": "13390"
  },
  {
    "text": "done so much to bring NLP\nand IR back together again",
    "start": "13390",
    "end": "17260"
  },
  {
    "text": "and open new doors for\nboth of those fields.",
    "start": "17260",
    "end": "20830"
  },
  {
    "text": "In the background\nthroughout this screencast,",
    "start": "20830",
    "end": "23060"
  },
  {
    "start": "21000",
    "end": "21000"
  },
  {
    "text": "I think you should imagine\nthat the name of the game",
    "start": "23060",
    "end": "25270"
  },
  {
    "text": "is to take a\npre-trained BERT model",
    "start": "25270",
    "end": "27790"
  },
  {
    "text": "and fine tune it for\ninformation retrieval.",
    "start": "27790",
    "end": "31390"
  },
  {
    "text": "And in that context,\ncross-encoders",
    "start": "31390",
    "end": "33850"
  },
  {
    "text": "are kind of conceptually\nthe simplest approach",
    "start": "33850",
    "end": "36190"
  },
  {
    "text": "that you could take.",
    "start": "36190",
    "end": "37390"
  },
  {
    "text": "For cross-encoders, we're\ngoing to concatenate",
    "start": "37390",
    "end": "40660"
  },
  {
    "text": "the query text and\nthe document text",
    "start": "40660",
    "end": "42970"
  },
  {
    "text": "together into one single text,\nprocess that text with BERT,",
    "start": "42970",
    "end": "46990"
  },
  {
    "text": "and then use representations\nin that BERT model as the basis",
    "start": "46990",
    "end": "50590"
  },
  {
    "text": "for IR fine tuning.",
    "start": "50590",
    "end": "52690"
  },
  {
    "text": "In a bit more detail,\nwe process the query",
    "start": "52690",
    "end": "54850"
  },
  {
    "text": "in the document and then\nprobably take the final output",
    "start": "54850",
    "end": "58030"
  },
  {
    "text": "state above the CLS\ntoken, add some task",
    "start": "58030",
    "end": "61239"
  },
  {
    "text": "specific parameters\non top, and fine",
    "start": "61240",
    "end": "63670"
  },
  {
    "text": "tune the model against\nour information retrieval",
    "start": "63670",
    "end": "66470"
  },
  {
    "text": "objective.",
    "start": "66470",
    "end": "67430"
  },
  {
    "text": "So that will be incredibly\nsemantically expressive",
    "start": "67430",
    "end": "70130"
  },
  {
    "text": "because we have all of these\ninteresting interactions",
    "start": "70130",
    "end": "72740"
  },
  {
    "text": "between query and\ndocument in this mode.",
    "start": "72740",
    "end": "75770"
  },
  {
    "text": "In a bit more detail,\nin the background",
    "start": "75770",
    "end": "77600"
  },
  {
    "text": "here, I'm imagining that we have\na data set of triples, where",
    "start": "77600",
    "end": "80630"
  },
  {
    "text": "we have a query, one positive\ndocument for that query",
    "start": "80630",
    "end": "84479"
  },
  {
    "text": "and some number, one or\nmore negative documents",
    "start": "84480",
    "end": "87500"
  },
  {
    "text": "for that query.",
    "start": "87500",
    "end": "89390"
  },
  {
    "text": "The basis for scoring\nis as I described it",
    "start": "89390",
    "end": "92030"
  },
  {
    "text": "before, we're going to\ntake our BERT encoder,",
    "start": "92030",
    "end": "94549"
  },
  {
    "text": "concatenate the query in the\ndocument and process that text,",
    "start": "94550",
    "end": "97730"
  },
  {
    "text": "and then retrieve the final\noutput state above the CLS",
    "start": "97730",
    "end": "101330"
  },
  {
    "text": "token that's given\nhere and that's",
    "start": "101330",
    "end": "102920"
  },
  {
    "text": "fed through a dense layer\nthat is used for scoring.",
    "start": "102920",
    "end": "106939"
  },
  {
    "text": "And then the loss\nfunction for the model",
    "start": "106940",
    "end": "109340"
  },
  {
    "text": "is typically the\nnegative log likelihood",
    "start": "109340",
    "end": "111409"
  },
  {
    "text": "of the positive passage.",
    "start": "111410",
    "end": "112730"
  },
  {
    "text": "So in the numerator\nhere, we have our score",
    "start": "112730",
    "end": "115280"
  },
  {
    "text": "for the positive\npassage according",
    "start": "115280",
    "end": "116990"
  },
  {
    "text": "to our scoring function.",
    "start": "116990",
    "end": "118430"
  },
  {
    "text": "And the denominator is that\npositive passage score, again,",
    "start": "118430",
    "end": "121520"
  },
  {
    "text": "summed together with the total\nfor all the negative passages.",
    "start": "121520",
    "end": "125890"
  },
  {
    "text": "So let's step back.",
    "start": "125890",
    "end": "127060"
  },
  {
    "text": "This will be incredibly\nsemantically rich,",
    "start": "127060",
    "end": "129520"
  },
  {
    "text": "but it simply won't scale.",
    "start": "129520",
    "end": "131350"
  },
  {
    "text": "The richness comes from\nus using the BERT model",
    "start": "131350",
    "end": "134140"
  },
  {
    "text": "to jointly encode the\nquery and the document.",
    "start": "134140",
    "end": "136730"
  },
  {
    "text": "So we have all these rich\ntoken level interactions",
    "start": "136730",
    "end": "139659"
  },
  {
    "text": "but that is the\nmodel's downfall.",
    "start": "139660",
    "end": "141730"
  },
  {
    "text": "This won't scale because\nwe need to encode",
    "start": "141730",
    "end": "143890"
  },
  {
    "text": "every document at query time.",
    "start": "143890",
    "end": "146840"
  },
  {
    "text": "So in principle, this means that\nif we have a billion documents,",
    "start": "146840",
    "end": "149709"
  },
  {
    "text": "we need to do a billion forward\npasses with the Bert model, one",
    "start": "149710",
    "end": "154630"
  },
  {
    "text": "for every document with\nrespect to our query,",
    "start": "154630",
    "end": "157060"
  },
  {
    "text": "get all those scores, and then\nmake decisions on that basis.",
    "start": "157060",
    "end": "160120"
  },
  {
    "text": "And that will be\nsimply infeasible.",
    "start": "160120",
    "end": "162790"
  },
  {
    "text": "So although there's\nsomething conceptually right",
    "start": "162790",
    "end": "164920"
  },
  {
    "text": "about this approach, it's simply\nintractable for modern search.",
    "start": "164920",
    "end": "170940"
  },
  {
    "text": "DPR can be seen\nas a model that's",
    "start": "170940",
    "end": "173250"
  },
  {
    "text": "at the other end\nof the spectrum.",
    "start": "173250",
    "end": "174690"
  },
  {
    "text": "This stands for dense\npassage retriever.",
    "start": "174690",
    "end": "177030"
  },
  {
    "text": "In this mode, we're going\nto separately encode",
    "start": "177030",
    "end": "179640"
  },
  {
    "text": "queries and documents.",
    "start": "179640",
    "end": "181200"
  },
  {
    "text": "So on the left here, I've\ngot our query encoded",
    "start": "181200",
    "end": "183540"
  },
  {
    "text": "with a BERT like model.",
    "start": "183540",
    "end": "184890"
  },
  {
    "text": "And I've grayed out\nall of the states",
    "start": "184890",
    "end": "187110"
  },
  {
    "text": "except the final output\nstate above the CLS token.",
    "start": "187110",
    "end": "189900"
  },
  {
    "text": "That's the only one\nthat we'll really need.",
    "start": "189900",
    "end": "192180"
  },
  {
    "text": "I separately encode\nthe document.",
    "start": "192180",
    "end": "194170"
  },
  {
    "text": "And again, we just need\nthat final output state",
    "start": "194170",
    "end": "196349"
  },
  {
    "text": "above the CLS token.",
    "start": "196350",
    "end": "198390"
  },
  {
    "text": "And then we're going\nto do scoring as a dot",
    "start": "198390",
    "end": "200550"
  },
  {
    "text": "product of those two vectors.",
    "start": "200550",
    "end": "202740"
  },
  {
    "text": "In a bit more detail\nagain, we have a data",
    "start": "202740",
    "end": "204780"
  },
  {
    "text": "set consisting of those\ntriples for our query, one",
    "start": "204780",
    "end": "208020"
  },
  {
    "text": "positive document and one\nor more negative documents.",
    "start": "208020",
    "end": "211560"
  },
  {
    "text": "Now, our core\ncomparison function",
    "start": "211560",
    "end": "213810"
  },
  {
    "text": "is what I've called Sim here.",
    "start": "213810",
    "end": "215610"
  },
  {
    "text": "And the basis for Sim is\nthat we encode our query",
    "start": "215610",
    "end": "218370"
  },
  {
    "text": "using our query encoder and\nget the final output state.",
    "start": "218370",
    "end": "221760"
  },
  {
    "text": "And we get the dot\nproduct of that",
    "start": "221760",
    "end": "223560"
  },
  {
    "text": "with the encoding\nfor our document,",
    "start": "223560",
    "end": "225480"
  },
  {
    "text": "again, focused on the output\nstate above the CLS token.",
    "start": "225480",
    "end": "229440"
  },
  {
    "text": "The loss is as before,\nthis is the negative log",
    "start": "229440",
    "end": "232240"
  },
  {
    "text": "likelihood of the\npositive passage,",
    "start": "232240",
    "end": "234100"
  },
  {
    "text": "so the positive score\nup here and then again",
    "start": "234100",
    "end": "236860"
  },
  {
    "text": "u's down here summed\ntogether with the sum for all",
    "start": "236860",
    "end": "239590"
  },
  {
    "text": "of the negative passages.",
    "start": "239590",
    "end": "242050"
  },
  {
    "text": "This will be highly\nscalable, but it's",
    "start": "242050",
    "end": "244990"
  },
  {
    "text": "very limited in terms of its\nquery document interactions.",
    "start": "244990",
    "end": "248140"
  },
  {
    "text": "Let's unpack that a bit.",
    "start": "248140",
    "end": "249580"
  },
  {
    "text": "The core of the scalability\nis that we can now",
    "start": "249580",
    "end": "252670"
  },
  {
    "text": "encode all of our documents\noffline ahead of time.",
    "start": "252670",
    "end": "255520"
  },
  {
    "text": "And indeed, we\nonly need to store",
    "start": "255520",
    "end": "258549"
  },
  {
    "text": "one single vector\nassociated with each one",
    "start": "258550",
    "end": "260889"
  },
  {
    "text": "of those documents.",
    "start": "260890",
    "end": "262150"
  },
  {
    "text": "And then at query time, we\njust encode the query, get",
    "start": "262150",
    "end": "265360"
  },
  {
    "text": "that one representation\nabove the CLS token,",
    "start": "265360",
    "end": "268240"
  },
  {
    "text": "and do a fast dot product\nwith all of our documents.",
    "start": "268240",
    "end": "271120"
  },
  {
    "text": "So it's highly scalable\nin that sense too.",
    "start": "271120",
    "end": "273580"
  },
  {
    "text": "But at the same\ntime, we have lost",
    "start": "273580",
    "end": "275860"
  },
  {
    "text": "all of those token\nlevel interactions",
    "start": "275860",
    "end": "277719"
  },
  {
    "text": "we had with the cross-encoder.",
    "start": "277720",
    "end": "280030"
  },
  {
    "text": "Now, we have to hope that all of\nthe information about the query",
    "start": "280030",
    "end": "283060"
  },
  {
    "text": "and the document is summarized\nin those single vector",
    "start": "283060",
    "end": "286690"
  },
  {
    "text": "representations.",
    "start": "286690",
    "end": "287770"
  },
  {
    "text": "And we might worry\nthat results in a loss",
    "start": "287770",
    "end": "290410"
  },
  {
    "text": "of expressivity for the model.",
    "start": "290410",
    "end": "293600"
  },
  {
    "start": "293000",
    "end": "293000"
  },
  {
    "text": "Before moving on to\nsome additional models",
    "start": "293600",
    "end": "295418"
  },
  {
    "text": "in this space, I\nthought I would just",
    "start": "295418",
    "end": "296960"
  },
  {
    "text": "pause here and point\nout that we have",
    "start": "296960",
    "end": "299270"
  },
  {
    "text": "a little bit of modularity.",
    "start": "299270",
    "end": "301560"
  },
  {
    "text": "The loss function for both of\nthe models that I've presented",
    "start": "301560",
    "end": "304500"
  },
  {
    "text": "is the negative log likelihood\nof the positive passage.",
    "start": "304500",
    "end": "307680"
  },
  {
    "text": "Here's how I presented it for\nthe cross-encoder and the core",
    "start": "307680",
    "end": "310590"
  },
  {
    "text": "of that is this rep function.",
    "start": "310590",
    "end": "312360"
  },
  {
    "text": "And here's how I presented it\nfor DPR, where the core of it",
    "start": "312360",
    "end": "315780"
  },
  {
    "text": "is the Sim function.",
    "start": "315780",
    "end": "317250"
  },
  {
    "text": "And you can now see that there's\na general form of this, where",
    "start": "317250",
    "end": "320460"
  },
  {
    "text": "we just have some\ncomparison function",
    "start": "320460",
    "end": "322410"
  },
  {
    "text": "and everything else\nremains the same.",
    "start": "322410",
    "end": "324360"
  },
  {
    "text": "And this is freeing because\nif you developed variants",
    "start": "324360",
    "end": "328139"
  },
  {
    "text": "of DPR or cross-encoders,\nthe way that might play out",
    "start": "328140",
    "end": "331830"
  },
  {
    "text": "is that you've simply adjusted\nthe comparison function here",
    "start": "331830",
    "end": "335189"
  },
  {
    "text": "and everything else\nabout how you're",
    "start": "335190",
    "end": "336870"
  },
  {
    "text": "setting up models\nand optimizing them",
    "start": "336870",
    "end": "338669"
  },
  {
    "text": "could potentially stay the same.",
    "start": "338670",
    "end": "342700"
  },
  {
    "text": "All right.",
    "start": "342700",
    "end": "343200"
  },
  {
    "text": "Let's move to ColBERT.",
    "start": "343200",
    "end": "344340"
  },
  {
    "text": "This model is near\nand dear to me.",
    "start": "344340",
    "end": "346320"
  },
  {
    "text": "ColBERT was developed by Omar\nKhattab, who is my student,",
    "start": "346320",
    "end": "349380"
  },
  {
    "text": "along with Matei Zaharia, who\nis my long time collaborator",
    "start": "349380",
    "end": "352830"
  },
  {
    "text": "and co-advises Omar with me.",
    "start": "352830",
    "end": "355300"
  },
  {
    "text": "And Omar would want\nme to point out",
    "start": "355300",
    "end": "357300"
  },
  {
    "text": "for you that ColBERT stands for\ncontextualized late interaction",
    "start": "357300",
    "end": "361169"
  },
  {
    "text": "with BERT.",
    "start": "361170",
    "end": "362340"
  },
  {
    "text": "That's an homage to the\nlate night talk show",
    "start": "362340",
    "end": "364740"
  },
  {
    "text": "host, Stephen Colbert, who\nhas late night contextual",
    "start": "364740",
    "end": "368310"
  },
  {
    "text": "interactions with his guests.",
    "start": "368310",
    "end": "370380"
  },
  {
    "text": "But you are also free to\npronounce this ColBERT",
    "start": "370380",
    "end": "373320"
  },
  {
    "text": "because obviously,\nthe BERT in that name",
    "start": "373320",
    "end": "375930"
  },
  {
    "text": "is the famous BERT model.",
    "start": "375930",
    "end": "378800"
  },
  {
    "text": "Here's how ColBERT works.",
    "start": "378800",
    "end": "380300"
  },
  {
    "text": "First, we encode\nqueries using BERT.",
    "start": "380300",
    "end": "382509"
  },
  {
    "text": "I've drawn this on its side for\nreasons that will become clear",
    "start": "382510",
    "end": "385270"
  },
  {
    "text": "when I show you my full\ndiagram, but it's just",
    "start": "385270",
    "end": "387340"
  },
  {
    "text": "a BERT encoding of the query.",
    "start": "387340",
    "end": "389050"
  },
  {
    "text": "And I've grayed out all the\nstates except the final ones",
    "start": "389050",
    "end": "391960"
  },
  {
    "text": "because the only states\nwe need are the output",
    "start": "391960",
    "end": "394960"
  },
  {
    "text": "states from this model.",
    "start": "394960",
    "end": "397360"
  },
  {
    "text": "Similarly, we encode the\ndocument again with BERT",
    "start": "397360",
    "end": "400569"
  },
  {
    "text": "and here, again, the only states\nwe need are the output states.",
    "start": "400570",
    "end": "404890"
  },
  {
    "text": "And then the basis\nfor ColBERT scoring",
    "start": "404890",
    "end": "407230"
  },
  {
    "text": "is a matrix of similarity\nscores between query tokens",
    "start": "407230",
    "end": "411130"
  },
  {
    "text": "and document tokens,\nagain, as represented",
    "start": "411130",
    "end": "413650"
  },
  {
    "text": "by these final output layers.",
    "start": "413650",
    "end": "415389"
  },
  {
    "text": "So we get scores.",
    "start": "415390",
    "end": "416660"
  },
  {
    "text": "And in fact, we get a\nfull grid of these scores.",
    "start": "416660",
    "end": "419950"
  },
  {
    "text": "And then the basis for scoring\nis a MaxSim comparison.",
    "start": "419950",
    "end": "423400"
  },
  {
    "text": "For every query token,\nwe get the value",
    "start": "423400",
    "end": "426940"
  },
  {
    "text": "of the maximum similarity\nfor document tokens",
    "start": "426940",
    "end": "430150"
  },
  {
    "text": "and we sum those together to\nget the maximum value that",
    "start": "430150",
    "end": "433449"
  },
  {
    "text": "is the basis for the model.",
    "start": "433450",
    "end": "435750"
  },
  {
    "text": "In a bit more detail\nagain, we have a data set",
    "start": "435750",
    "end": "438120"
  },
  {
    "text": "consisting of those triples.",
    "start": "438120",
    "end": "440280"
  },
  {
    "text": "The loss is the\nnegative log likelihood",
    "start": "440280",
    "end": "442650"
  },
  {
    "text": "of the positive passage, but\nnow, MaxSim is the basis.",
    "start": "442650",
    "end": "445530"
  },
  {
    "text": "And here is the MaxSim scoring\nfunction in full detail.",
    "start": "445530",
    "end": "449500"
  },
  {
    "text": "But again, the essence of this\nis that for each query token,",
    "start": "449500",
    "end": "452730"
  },
  {
    "text": "we get the MaxSim for\nsome document token",
    "start": "452730",
    "end": "455880"
  },
  {
    "text": "and sum all those\nMaxSim values together.",
    "start": "455880",
    "end": "459580"
  },
  {
    "text": "This will be highly\nscalable, but it",
    "start": "459580",
    "end": "462090"
  },
  {
    "text": "has late contextual interactions\nbetween tokens in the query",
    "start": "462090",
    "end": "466199"
  },
  {
    "text": "and tokens in the document.",
    "start": "466200",
    "end": "467500"
  },
  {
    "text": "Let me unpack that.",
    "start": "467500",
    "end": "468420"
  },
  {
    "text": "It's highly scalable\nbecause as with DPR,",
    "start": "468420",
    "end": "471360"
  },
  {
    "text": "we can store all of our\ndocuments ahead of time.",
    "start": "471360",
    "end": "474629"
  },
  {
    "text": "We just need to score this\nvector of output vectors here",
    "start": "474630",
    "end": "479100"
  },
  {
    "text": "to represent documents.",
    "start": "479100",
    "end": "480750"
  },
  {
    "text": "At query time, we\nencode the query",
    "start": "480750",
    "end": "482730"
  },
  {
    "text": "and get the output\nstates and then",
    "start": "482730",
    "end": "484560"
  },
  {
    "text": "perform a bunch of very fast\nMaxSim comparisons for scoring.",
    "start": "484560",
    "end": "489300"
  },
  {
    "text": "But it's also semantically rich.",
    "start": "489300",
    "end": "491159"
  },
  {
    "text": "We have retained some of the\nadvantages of the cross-encoder",
    "start": "491160",
    "end": "494220"
  },
  {
    "text": "because we do have token level\ninteractions between query",
    "start": "494220",
    "end": "498160"
  },
  {
    "text": "and document.",
    "start": "498160",
    "end": "498820"
  },
  {
    "text": "It's now it's just that they\nhappen only on the output",
    "start": "498820",
    "end": "501970"
  },
  {
    "text": "states, whereas the\ncross-encoder allowed",
    "start": "501970",
    "end": "503770"
  },
  {
    "text": "them to happen at every\nlayer in the BERT model.",
    "start": "503770",
    "end": "506860"
  },
  {
    "text": "That was too expensive and this\nlooks like a nice compromise.",
    "start": "506860",
    "end": "510759"
  },
  {
    "text": "And ColBERT has\nindeed proven to be",
    "start": "510760",
    "end": "512979"
  },
  {
    "text": "an extremely powerful and\neffective IR mechanism.",
    "start": "512980",
    "end": "518490"
  },
  {
    "start": "518000",
    "end": "518000"
  },
  {
    "text": "One thing I really\nlike about ColBERT",
    "start": "518490",
    "end": "520890"
  },
  {
    "text": "is that it brings in an\nolder insight from IR, which",
    "start": "520890",
    "end": "524340"
  },
  {
    "text": "is that essentially we want\nto do some level of term",
    "start": "524340",
    "end": "527250"
  },
  {
    "text": "matching between\nqueries and documents",
    "start": "527250",
    "end": "529260"
  },
  {
    "text": "except now, since this\nis a neural model,",
    "start": "529260",
    "end": "531540"
  },
  {
    "text": "we get to do that in a\nsemantically very rich space.",
    "start": "531540",
    "end": "535050"
  },
  {
    "text": "Let me show you that\nby way of an example.",
    "start": "535050",
    "end": "536880"
  },
  {
    "text": "Here, I have the query, \"when\ndid the Transformers cartoon",
    "start": "536880",
    "end": "540000"
  },
  {
    "text": "series come out?\"",
    "start": "540000",
    "end": "541230"
  },
  {
    "text": "And we have the document,\n\"the animated Transformers",
    "start": "541230",
    "end": "543959"
  },
  {
    "text": "was released in August 1986.\"",
    "start": "543960",
    "end": "546660"
  },
  {
    "text": "And I'm going to show\nyou some MaxSim values.",
    "start": "546660",
    "end": "549720"
  },
  {
    "text": "The largest score is between\nTransformers in the query",
    "start": "549720",
    "end": "552810"
  },
  {
    "text": "and Transformers\nin the document.",
    "start": "552810",
    "end": "555000"
  },
  {
    "text": "That makes good sense.",
    "start": "555000",
    "end": "556690"
  },
  {
    "text": "But we also have a very strong\nMaxSim match between cartoon",
    "start": "556690",
    "end": "560400"
  },
  {
    "text": "in the query and\nanimated in the document.",
    "start": "560400",
    "end": "563190"
  },
  {
    "text": "And that's a very\nsemantic connection",
    "start": "563190",
    "end": "565140"
  },
  {
    "text": "that only neural\nmodels like ColBERT",
    "start": "565140",
    "end": "567000"
  },
  {
    "text": "can make without extra effort.",
    "start": "567000",
    "end": "568890"
  },
  {
    "text": "Similarly, for come out in\nthe context of the query,",
    "start": "568890",
    "end": "572820"
  },
  {
    "text": "we have a strong match to\nreleased in the document.",
    "start": "572820",
    "end": "575970"
  },
  {
    "text": "And then for when\nin the query, that",
    "start": "575970",
    "end": "577930"
  },
  {
    "text": "matches to the two parts of the\ndata expression August 1986.",
    "start": "577930",
    "end": "581710"
  },
  {
    "text": "Here, I've shown the\ntop two MaxSim values",
    "start": "581710",
    "end": "584085"
  },
  {
    "text": "to show that we're\nreally getting",
    "start": "584085",
    "end": "585460"
  },
  {
    "text": "a semantic connection to that\nfull unit in the document.",
    "start": "585460",
    "end": "588980"
  },
  {
    "text": "So this kind of thing makes\nthe model highly interpretable",
    "start": "588980",
    "end": "592420"
  },
  {
    "text": "and also reveals to us why this\nis such an effective retrieval",
    "start": "592420",
    "end": "596529"
  },
  {
    "text": "mechanism because it can make\nall of these deep associations.",
    "start": "596530",
    "end": "601780"
  },
  {
    "start": "601000",
    "end": "601000"
  },
  {
    "text": "Before moving on to splayed,\nthe final model that I wanted",
    "start": "601780",
    "end": "604900"
  },
  {
    "text": "to talk about, I thought I\nwould pause here and just talk",
    "start": "604900",
    "end": "607750"
  },
  {
    "text": "a little bit with you about\nhow you take ColBERT or any",
    "start": "607750",
    "end": "611380"
  },
  {
    "text": "of these neural models\nand then turn them",
    "start": "611380",
    "end": "613510"
  },
  {
    "text": "into something that could be\neffective as a deployed search",
    "start": "613510",
    "end": "617530"
  },
  {
    "text": "technology.",
    "start": "617530",
    "end": "618610"
  },
  {
    "text": "Because in the\nbackground here is",
    "start": "618610",
    "end": "620470"
  },
  {
    "text": "that we have semantic\nexpressiveness",
    "start": "620470",
    "end": "622720"
  },
  {
    "text": "but it comes at a price.",
    "start": "622720",
    "end": "624339"
  },
  {
    "text": "We need to do forward\ninference in Bert models",
    "start": "624340",
    "end": "627310"
  },
  {
    "text": "and that can be very\nexpensive prohibitively",
    "start": "627310",
    "end": "630820"
  },
  {
    "text": "so if we have very tight\nlatency restrictions.",
    "start": "630820",
    "end": "633760"
  },
  {
    "text": "And so the question\nfor us is, can we",
    "start": "633760",
    "end": "636070"
  },
  {
    "text": "overcome those limitations and\nmake this a practical solution.",
    "start": "636070",
    "end": "641070"
  },
  {
    "text": "One easy thing to do\nto make this practical",
    "start": "641070",
    "end": "644070"
  },
  {
    "text": "is to employ these\nmodels as rerankers.",
    "start": "644070",
    "end": "646770"
  },
  {
    "text": "Here, this is how this\nwould play out for ColBERT.",
    "start": "646770",
    "end": "649740"
  },
  {
    "text": "For ColBERT, remember,\nwe have an index",
    "start": "649740",
    "end": "651870"
  },
  {
    "text": "that essentially consists of\ntoken level representations.",
    "start": "651870",
    "end": "655560"
  },
  {
    "text": "Those are each associated\nwith documents.",
    "start": "655560",
    "end": "658230"
  },
  {
    "text": "Given an index structure like\nthis, a simple thing to do",
    "start": "658230",
    "end": "661260"
  },
  {
    "text": "would be to take our query\nencode it as a bunch of tokens.",
    "start": "661260",
    "end": "664830"
  },
  {
    "text": "Get the top K documents\nfor that query",
    "start": "664830",
    "end": "667680"
  },
  {
    "text": "using a fast term-based\nmodel like BM25",
    "start": "667680",
    "end": "671130"
  },
  {
    "text": "and then use ColBERT\nonly at stage two",
    "start": "671130",
    "end": "673860"
  },
  {
    "text": "to rerank the top\nK documents there.",
    "start": "673860",
    "end": "677170"
  },
  {
    "text": "So we use BM25 for the\nexpensive first phase where",
    "start": "677170",
    "end": "680790"
  },
  {
    "text": "we need to do brute force\nsearch over our entire index",
    "start": "680790",
    "end": "684089"
  },
  {
    "text": "of documents.",
    "start": "684090",
    "end": "685080"
  },
  {
    "text": "And the model like ColBERT\ncomes in only at phase two",
    "start": "685080",
    "end": "688470"
  },
  {
    "text": "to do reranking.",
    "start": "688470",
    "end": "690180"
  },
  {
    "text": "It sounds like a small\nthing, but in fact,",
    "start": "690180",
    "end": "692320"
  },
  {
    "text": "the reranking that happens\nin that second phase",
    "start": "692320",
    "end": "694500"
  },
  {
    "text": "can be incredibly powerful\nand add a lot of value",
    "start": "694500",
    "end": "697740"
  },
  {
    "text": "as a result of the fact\nthat ColBERT and models like",
    "start": "697740",
    "end": "700500"
  },
  {
    "text": "it are so good at doing\nretrieval in this context.",
    "start": "700500",
    "end": "704740"
  },
  {
    "text": "But they're expensive.",
    "start": "704740",
    "end": "706209"
  },
  {
    "text": "One nice thing\nabout this, though,",
    "start": "706210",
    "end": "707720"
  },
  {
    "text": "is that we can control our costs\nbecause if we set K very low,",
    "start": "707720",
    "end": "711790"
  },
  {
    "text": "we'll do very little\nprocessing with ColBERT.",
    "start": "711790",
    "end": "714220"
  },
  {
    "text": "If we set K high, we'll\nuse ColBERT more often.",
    "start": "714220",
    "end": "716980"
  },
  {
    "text": "And we can kind of calibrate\nthat against other constraints",
    "start": "716980",
    "end": "720250"
  },
  {
    "text": "that we're operating under.",
    "start": "720250",
    "end": "722450"
  },
  {
    "text": "So this is a perfectly\nreasonable solution.",
    "start": "722450",
    "end": "725020"
  },
  {
    "text": "The one concern you might\nhave maybe as a purist",
    "start": "725020",
    "end": "728110"
  },
  {
    "text": "is that you now have two\nretrieval mechanisms in play,",
    "start": "728110",
    "end": "730899"
  },
  {
    "text": "BM25, which does\na lot of the work,",
    "start": "730900",
    "end": "733300"
  },
  {
    "text": "and ColBERT, which performs\nthe reranking function.",
    "start": "733300",
    "end": "736300"
  },
  {
    "text": "We might hope for a more\nintegrated solution.",
    "start": "736300",
    "end": "739220"
  },
  {
    "start": "738000",
    "end": "738000"
  },
  {
    "text": "So could we get beyond\nreranking for ColBERT?",
    "start": "739220",
    "end": "742149"
  },
  {
    "text": "I think the answer is yes.",
    "start": "742150",
    "end": "743920"
  },
  {
    "text": "We're going to make\na slight adjustment",
    "start": "743920",
    "end": "745810"
  },
  {
    "text": "to how we set up the index.",
    "start": "745810",
    "end": "747160"
  },
  {
    "text": "Now, the primary thing\nwill be that we'll",
    "start": "747160",
    "end": "749019"
  },
  {
    "text": "have these token level vectors,\nwhich of course as before",
    "start": "749020",
    "end": "753280"
  },
  {
    "text": "associate with documents.",
    "start": "753280",
    "end": "755410"
  },
  {
    "text": "Now, when a query\ncomes in, we encode",
    "start": "755410",
    "end": "758079"
  },
  {
    "text": "that into a sequence\nof vectors and then",
    "start": "758080",
    "end": "761230"
  },
  {
    "text": "for each vector in that\nquery representation,",
    "start": "761230",
    "end": "764079"
  },
  {
    "text": "we retrieve the P most\nsimilar token vectors",
    "start": "764080",
    "end": "767650"
  },
  {
    "text": "and then travel through them\nto their associated documents.",
    "start": "767650",
    "end": "770770"
  },
  {
    "text": "And then the only\nColBERT work that we do",
    "start": "770770",
    "end": "773680"
  },
  {
    "text": "is scoring this potentially\nsmall set of documents",
    "start": "773680",
    "end": "776529"
  },
  {
    "text": "that we end up in phase two.",
    "start": "776530",
    "end": "777910"
  },
  {
    "text": "Because in phase\none, all we're doing",
    "start": "777910",
    "end": "780310"
  },
  {
    "text": "is a bunch of\nsimilarity calculations",
    "start": "780310",
    "end": "782470"
  },
  {
    "text": "between vector representations.",
    "start": "782470",
    "end": "784939"
  },
  {
    "text": "So again, we have\na lot of control",
    "start": "784940",
    "end": "786550"
  },
  {
    "text": "over how much we actually\nuse the full ColBERT",
    "start": "786550",
    "end": "789670"
  },
  {
    "text": "model at step two here.",
    "start": "789670",
    "end": "791360"
  },
  {
    "text": "And therefore, we can calibrate\nagainst other constraints",
    "start": "791360",
    "end": "794320"
  },
  {
    "text": "that we're operating under.",
    "start": "794320",
    "end": "796490"
  },
  {
    "text": "So this is certainly\nworkable, but we can probably",
    "start": "796490",
    "end": "799240"
  },
  {
    "text": "do even better.",
    "start": "799240",
    "end": "801100"
  },
  {
    "start": "801000",
    "end": "801000"
  },
  {
    "text": "And the way we\ncan do even better",
    "start": "801100",
    "end": "802720"
  },
  {
    "text": "is with centroid-based ranking.",
    "start": "802720",
    "end": "804790"
  },
  {
    "text": "And this begins from the\ninsight that this index",
    "start": "804790",
    "end": "807399"
  },
  {
    "text": "that we've constructed\nhere will have",
    "start": "807400",
    "end": "809560"
  },
  {
    "text": "a lot of semantic structure.",
    "start": "809560",
    "end": "811300"
  },
  {
    "text": "And we can capture that by\nclustering the token level",
    "start": "811300",
    "end": "814600"
  },
  {
    "text": "vectors that represent our\ndocuments into clusters",
    "start": "814600",
    "end": "818529"
  },
  {
    "text": "and then taking their centroids\nto be representative summaries",
    "start": "818530",
    "end": "821950"
  },
  {
    "text": "of those clusters.",
    "start": "821950",
    "end": "823580"
  },
  {
    "text": "And we can use those as\nthe basis for search.",
    "start": "823580",
    "end": "827120"
  },
  {
    "text": "So now, given a query\nthat we encode again",
    "start": "827120",
    "end": "829720"
  },
  {
    "text": "as a sequence of vectors, for\neach one of those vectors,",
    "start": "829720",
    "end": "832839"
  },
  {
    "text": "we retrieve the\nclosest centroids",
    "start": "832840",
    "end": "835540"
  },
  {
    "text": "and then travel from them\nto similar document tokens",
    "start": "835540",
    "end": "839649"
  },
  {
    "text": "and then from them\nto similar documents.",
    "start": "839650",
    "end": "842080"
  },
  {
    "text": "And then again, we use\nColBERT, the full model,",
    "start": "842080",
    "end": "844660"
  },
  {
    "text": "only at step three here.",
    "start": "844660",
    "end": "846430"
  },
  {
    "text": "All these other\ncomparisons are just fast",
    "start": "846430",
    "end": "849010"
  },
  {
    "text": "similarity comparisons.",
    "start": "849010",
    "end": "850900"
  },
  {
    "text": "And this gives us huge\ngains because instead",
    "start": "850900",
    "end": "853390"
  },
  {
    "text": "of having to search\nover this entire index,",
    "start": "853390",
    "end": "855670"
  },
  {
    "text": "we search over a\npotentially very",
    "start": "855670",
    "end": "857680"
  },
  {
    "text": "small number of\ncentroid representations",
    "start": "857680",
    "end": "860440"
  },
  {
    "text": "and use those as the\nbasis for getting down",
    "start": "860440",
    "end": "863530"
  },
  {
    "text": "to a small set of documents that\nwe're going to score completely",
    "start": "863530",
    "end": "866710"
  },
  {
    "text": "with ColBERT.",
    "start": "866710",
    "end": "869600"
  },
  {
    "start": "869000",
    "end": "869000"
  },
  {
    "text": "That's a bunch of the\nwork that we've done.",
    "start": "869600",
    "end": "872329"
  },
  {
    "text": "I thought I would just mention\na little bit of the work",
    "start": "872330",
    "end": "874850"
  },
  {
    "text": "that we've done\nspecifically to address",
    "start": "874850",
    "end": "876800"
  },
  {
    "text": "latency concerns for ColBERT.",
    "start": "876800",
    "end": "878540"
  },
  {
    "text": "And this comes from the\npaper that we called Plaid.",
    "start": "878540",
    "end": "881389"
  },
  {
    "text": "And it begins from\nthe observation",
    "start": "881390",
    "end": "883310"
  },
  {
    "text": "that despite all the hard work\nthat I just described for you,",
    "start": "883310",
    "end": "886220"
  },
  {
    "text": "the latency for\nthe ColBERT model",
    "start": "886220",
    "end": "888259"
  },
  {
    "text": "was still kind of prohibitively\nhigh at 287 milliseconds,",
    "start": "888260",
    "end": "892613"
  },
  {
    "text": "whereas you might hope\nyou could get this down",
    "start": "892613",
    "end": "894529"
  },
  {
    "text": "to around 50 milliseconds for\na feasible deployable solution",
    "start": "894530",
    "end": "898790"
  },
  {
    "text": "at a minimum.",
    "start": "898790",
    "end": "900709"
  },
  {
    "text": "And this chart here is showing\nyou where the work actually",
    "start": "900710",
    "end": "903480"
  },
  {
    "text": "happens.",
    "start": "903480",
    "end": "903980"
  },
  {
    "text": "And one surprising\nthing for ColBERT",
    "start": "903980",
    "end": "905810"
  },
  {
    "text": "is that only a small\npart of the overall time",
    "start": "905810",
    "end": "909290"
  },
  {
    "text": "there is actually spent\non the core modeling",
    "start": "909290",
    "end": "911720"
  },
  {
    "text": "steps of representing\nexamples and doing scoring.",
    "start": "911720",
    "end": "915889"
  },
  {
    "text": "And in fact, only a small part\nis even used with the centroids",
    "start": "915890",
    "end": "919940"
  },
  {
    "text": "that I described before.",
    "start": "919940",
    "end": "921170"
  },
  {
    "text": "The bulk of the\nwork is being done",
    "start": "921170",
    "end": "923510"
  },
  {
    "text": "when we have to look things\nup in this giant index",
    "start": "923510",
    "end": "926780"
  },
  {
    "text": "and also when we\ndo decompression.",
    "start": "926780",
    "end": "929052"
  },
  {
    "text": "That's a point that I\nhaven't mentioned before,",
    "start": "929052",
    "end": "931010"
  },
  {
    "text": "but the essence of this is\nthat the ColBERT index can",
    "start": "931010",
    "end": "934850"
  },
  {
    "text": "get very large because we\nneed to store token level",
    "start": "934850",
    "end": "937850"
  },
  {
    "text": "representations.",
    "start": "937850",
    "end": "939139"
  },
  {
    "text": "But we find that we can make\nthem relatively low resolution,",
    "start": "939140",
    "end": "943070"
  },
  {
    "text": "four or even two bit\nrepresentations, because all",
    "start": "943070",
    "end": "946310"
  },
  {
    "text": "they need to do is\nrepresent individual tokens.",
    "start": "946310",
    "end": "949520"
  },
  {
    "text": "But that does mean that we\nwould like to decompress them",
    "start": "949520",
    "end": "952490"
  },
  {
    "text": "at some point to get back to\ntheir full semantic richness.",
    "start": "952490",
    "end": "955580"
  },
  {
    "text": "And we found that\nstep of unpacking them",
    "start": "955580",
    "end": "958880"
  },
  {
    "text": "was also expensive.",
    "start": "958880",
    "end": "961020"
  },
  {
    "text": "And so what the team\ndid is do a lot of work",
    "start": "961020",
    "end": "963410"
  },
  {
    "text": "to reduce the amount\nof heavy duty lookup",
    "start": "963410",
    "end": "966410"
  },
  {
    "start": "964000",
    "end": "964000"
  },
  {
    "text": "and decompression that the\nColBERT model was doing.",
    "start": "966410",
    "end": "969290"
  },
  {
    "text": "They trade that a little bit\noff against using more centroids",
    "start": "969290",
    "end": "972769"
  },
  {
    "text": "as part of that initial\nsearch phase that I described,",
    "start": "972770",
    "end": "976040"
  },
  {
    "text": "but they did successfully remove\nalmost all the overhead that",
    "start": "976040",
    "end": "979490"
  },
  {
    "text": "was coming from these\nlarge data structures",
    "start": "979490",
    "end": "981740"
  },
  {
    "text": "and the corresponding\ndecompression",
    "start": "981740",
    "end": "983750"
  },
  {
    "text": "that we had to do.",
    "start": "983750",
    "end": "984860"
  },
  {
    "text": "And they got the latency all\nthe way down to 58 milliseconds.",
    "start": "984860",
    "end": "988850"
  },
  {
    "text": "I regard this as absolutely\nan amazing achievement.",
    "start": "988850",
    "end": "992790"
  },
  {
    "text": "And I think it shows you how\nmuch innovative work can happen",
    "start": "992790",
    "end": "995790"
  },
  {
    "text": "in this space not focused\non hill climbing on accuracy",
    "start": "995790",
    "end": "999420"
  },
  {
    "text": "but rather thinking\nabout issues like latency",
    "start": "999420",
    "end": "1001910"
  },
  {
    "text": "and how they impact the\ndeployability of systems",
    "start": "1001910",
    "end": "1005300"
  },
  {
    "text": "like this.",
    "start": "1005300",
    "end": "1006110"
  },
  {
    "text": "And there's lots more room\nfor innovation in this space.",
    "start": "1006110",
    "end": "1009640"
  },
  {
    "text": "So I would exhort\nyou all to think",
    "start": "1009640",
    "end": "1011090"
  },
  {
    "text": "about how you could\ncontribute to making systems",
    "start": "1011090",
    "end": "1013610"
  },
  {
    "text": "not only more accurate\nbut also more efficient",
    "start": "1013610",
    "end": "1016100"
  },
  {
    "text": "along this and other dimensions.",
    "start": "1016100",
    "end": "1019470"
  },
  {
    "start": "1019000",
    "end": "1019000"
  },
  {
    "text": "There's one more\nmodel that I wanted",
    "start": "1019470",
    "end": "1021480"
  },
  {
    "text": "to mention because I\nthink this is incredibly",
    "start": "1021480",
    "end": "1023670"
  },
  {
    "text": "powerful and competitive\nand also offers yet again",
    "start": "1023670",
    "end": "1027540"
  },
  {
    "text": "another perspective on how\nto use neural representations",
    "start": "1027540",
    "end": "1030720"
  },
  {
    "text": "in this space.",
    "start": "1030720",
    "end": "1031650"
  },
  {
    "text": "This model is SPLADE.",
    "start": "1031650",
    "end": "1034439"
  },
  {
    "text": "Here's how SPLADE works.",
    "start": "1034440",
    "end": "1035858"
  },
  {
    "text": "I've got at the bottom\nhere our encoding mechanism",
    "start": "1035859",
    "end": "1038849"
  },
  {
    "text": "for sequences.",
    "start": "1038849",
    "end": "1039939"
  },
  {
    "text": "And I'm trying to be\nagnostic about whether this",
    "start": "1039940",
    "end": "1042059"
  },
  {
    "text": "is a query sequence\nor a document sequence",
    "start": "1042060",
    "end": "1044459"
  },
  {
    "text": "because we do both of those with\nthe same kind of calculations.",
    "start": "1044460",
    "end": "1049419"
  },
  {
    "text": "So just imagine we're\nprocessing some text,",
    "start": "1049420",
    "end": "1052050"
  },
  {
    "text": "the core shift in\nperspective here",
    "start": "1052050",
    "end": "1054540"
  },
  {
    "text": "is that now we're going\nto do scoring with respect",
    "start": "1054540",
    "end": "1057420"
  },
  {
    "text": "not to some other text\nbut rather with respect",
    "start": "1057420",
    "end": "1060240"
  },
  {
    "text": "to our entire vocabulary.",
    "start": "1060240",
    "end": "1062309"
  },
  {
    "text": "So here, I have a small\nvocabulary of just seven items,",
    "start": "1062310",
    "end": "1065730"
  },
  {
    "text": "but of course, you could have\ntens of thousands of items.",
    "start": "1065730",
    "end": "1068535"
  },
  {
    "text": "And that's important\nfor SPLADE because we're",
    "start": "1068535",
    "end": "1070410"
  },
  {
    "text": "going to have very sparse\nrepresentations by comparison",
    "start": "1070410",
    "end": "1073740"
  },
  {
    "text": "with cross-encoders,\nDPR, and ColBERT.",
    "start": "1073740",
    "end": "1077340"
  },
  {
    "text": "So here's how this works.",
    "start": "1077340",
    "end": "1079130"
  },
  {
    "text": "We're going to form, like with\nColBERT, a matrix of scores,",
    "start": "1079130",
    "end": "1082240"
  },
  {
    "text": "but now, the scoring\nis with respect",
    "start": "1082240",
    "end": "1084010"
  },
  {
    "text": "to tokens in the sequence\nthat we're processing",
    "start": "1084010",
    "end": "1086890"
  },
  {
    "text": "and all of our vocabulary items.",
    "start": "1086890",
    "end": "1089410"
  },
  {
    "text": "The scoring function\nfor that is detailed.",
    "start": "1089410",
    "end": "1091900"
  },
  {
    "text": "I've depicted it here.",
    "start": "1091900",
    "end": "1093070"
  },
  {
    "text": "You should think of it as\na bunch of neural layers",
    "start": "1093070",
    "end": "1096070"
  },
  {
    "text": "that help you represent\nall of these comparisons.",
    "start": "1096070",
    "end": "1099399"
  },
  {
    "text": "So you do all of\nthat work and then",
    "start": "1099400",
    "end": "1101590"
  },
  {
    "text": "the SPLADE scoring\nfunction is a kind",
    "start": "1101590",
    "end": "1103360"
  },
  {
    "text": "of sparsification of the\nscores that we get out of that.",
    "start": "1103360",
    "end": "1107049"
  },
  {
    "text": "That's depicted here.",
    "start": "1107050",
    "end": "1108430"
  },
  {
    "text": "And the essential insight is\nthat with this SPLADE function,",
    "start": "1108430",
    "end": "1112060"
  },
  {
    "text": "we're going to get a\nscore for every vocabulary",
    "start": "1112060",
    "end": "1114610"
  },
  {
    "text": "item with respect\nto the sequence",
    "start": "1114610",
    "end": "1116650"
  },
  {
    "text": "that we have processed.",
    "start": "1116650",
    "end": "1117970"
  },
  {
    "text": "And that's what's\ndepicted in orange here.",
    "start": "1117970",
    "end": "1120250"
  },
  {
    "text": "And you should think of this\norange thing as a vector",
    "start": "1120250",
    "end": "1123550"
  },
  {
    "text": "with the same dimensionality\nas our vocabulary,",
    "start": "1123550",
    "end": "1126700"
  },
  {
    "text": "giving what are probably very\nsparse scores for our sequence",
    "start": "1126700",
    "end": "1130929"
  },
  {
    "text": "with respect to everything\nin that vocabulary.",
    "start": "1130930",
    "end": "1134320"
  },
  {
    "text": "Again, we do that for\nqueries and for documents.",
    "start": "1134320",
    "end": "1136960"
  },
  {
    "text": "And then the similarity\nfunction that's",
    "start": "1136960",
    "end": "1138789"
  },
  {
    "text": "at the heart of\nall of these models",
    "start": "1138790",
    "end": "1140530"
  },
  {
    "text": "is now SimSPLADE\nwhich is a dot product",
    "start": "1140530",
    "end": "1143920"
  },
  {
    "text": "between the SPLADE\nrepresentation for the query",
    "start": "1143920",
    "end": "1146830"
  },
  {
    "text": "and the SPLADE representation\nfor the document.",
    "start": "1146830",
    "end": "1149960"
  },
  {
    "text": "So these are big\nlong sparse vectors",
    "start": "1149960",
    "end": "1151929"
  },
  {
    "text": "and we take the dot product\nof them for scoring.",
    "start": "1151930",
    "end": "1155050"
  },
  {
    "text": "The loss is our usual\nnegative log likelihood",
    "start": "1155050",
    "end": "1157990"
  },
  {
    "text": "plus importantly, a\nregularization term that",
    "start": "1157990",
    "end": "1161500"
  },
  {
    "text": "leads to sparse balanced\nscores, which I think",
    "start": "1161500",
    "end": "1164350"
  },
  {
    "text": "is an important modification\ngiven how different the SPLADE",
    "start": "1164350",
    "end": "1167770"
  },
  {
    "text": "representations are compared\nto the others we've discussed.",
    "start": "1167770",
    "end": "1171790"
  },
  {
    "text": "But this is an\nincredibly powerful model",
    "start": "1171790",
    "end": "1173950"
  },
  {
    "text": "and I love this perspective\nwhere we're now even further",
    "start": "1173950",
    "end": "1177340"
  },
  {
    "text": "back to original IR\ninsights about how",
    "start": "1177340",
    "end": "1180640"
  },
  {
    "text": "the vocabulary and term\nmatching is so important.",
    "start": "1180640",
    "end": "1183350"
  },
  {
    "text": "But again, it's happening in\nthis very rich neural space",
    "start": "1183350",
    "end": "1187419"
  },
  {
    "text": "defined by this grid of scores.",
    "start": "1187420",
    "end": "1191550"
  },
  {
    "start": "1191000",
    "end": "1191000"
  },
  {
    "text": "I'm not going to go through\nthis slide in detail,",
    "start": "1191550",
    "end": "1193680"
  },
  {
    "text": "but I couldn't resist\nmentioning a bunch",
    "start": "1193680",
    "end": "1196080"
  },
  {
    "text": "of other recent developments.",
    "start": "1196080",
    "end": "1197549"
  },
  {
    "text": "They are biased toward\nColBERT because I'm",
    "start": "1197550",
    "end": "1200220"
  },
  {
    "text": "biased toward ColBERT, but\nI think the list does point",
    "start": "1200220",
    "end": "1203520"
  },
  {
    "text": "to a general set of\ndirections around making",
    "start": "1203520",
    "end": "1206790"
  },
  {
    "text": "systems more efficient and also\nmaking them more multilingual.",
    "start": "1206790",
    "end": "1210480"
  },
  {
    "text": "And that can happen with\nthings like distillation",
    "start": "1210480",
    "end": "1213390"
  },
  {
    "text": "and also innovative ways\nof training the models",
    "start": "1213390",
    "end": "1216270"
  },
  {
    "text": "and setting up new\nobjectives for them,",
    "start": "1216270",
    "end": "1218430"
  },
  {
    "text": "while balancing lots of\nconsiderations, not just",
    "start": "1218430",
    "end": "1221280"
  },
  {
    "text": "accuracy but also efficiency\nfor these systems.",
    "start": "1221280",
    "end": "1225090"
  },
  {
    "text": "Tremendously exciting and active\narea of research for the field.",
    "start": "1225090",
    "end": "1229650"
  },
  {
    "start": "1229000",
    "end": "1229000"
  },
  {
    "text": "And to round out\nthat point, I thought",
    "start": "1229650",
    "end": "1231600"
  },
  {
    "text": "I would return to the thing\nthat I emphasized so much when",
    "start": "1231600",
    "end": "1235890"
  },
  {
    "text": "we talked about\nIR metrics, which",
    "start": "1235890",
    "end": "1237600"
  },
  {
    "text": "is that there is more at\nstake here than just accuracy.",
    "start": "1237600",
    "end": "1241960"
  },
  {
    "text": "So this is from a series\nof controlled experiments",
    "start": "1241960",
    "end": "1244950"
  },
  {
    "text": "that we did in\nthis paper, trying",
    "start": "1244950",
    "end": "1246929"
  },
  {
    "text": "to get a sense for the system\nrequirements, latency, costs,",
    "start": "1246930",
    "end": "1251160"
  },
  {
    "text": "and accuracy for a\nvariety of systems.",
    "start": "1251160",
    "end": "1254280"
  },
  {
    "text": "And there's no simple way\nto navigate this table.",
    "start": "1254280",
    "end": "1256840"
  },
  {
    "text": "So let me just\nhighlight a few things.",
    "start": "1256840",
    "end": "1258909"
  },
  {
    "text": "First, BM25 is the only\nmodel that we could even",
    "start": "1258910",
    "end": "1262950"
  },
  {
    "text": "get to run with this tiny\nlittle compute budget.",
    "start": "1262950",
    "end": "1266049"
  },
  {
    "text": "So if you are absolutely\ncompute constrained",
    "start": "1266050",
    "end": "1268800"
  },
  {
    "text": "or cost constrained, you might\nbe forced to choose BM25.",
    "start": "1268800",
    "end": "1273090"
  },
  {
    "text": "It's a reasonably\neffective model.",
    "start": "1273090",
    "end": "1276039"
  },
  {
    "text": "But assuming you can have\nmore heavy duty hardware,",
    "start": "1276040",
    "end": "1278900"
  },
  {
    "text": "you might think about\ntrade offs within the space",
    "start": "1278900",
    "end": "1281020"
  },
  {
    "text": "of possible ColBERT setups.",
    "start": "1281020",
    "end": "1283250"
  },
  {
    "text": "And this is illuminating\nbecause, for example, these two",
    "start": "1283250",
    "end": "1286180"
  },
  {
    "text": "models are pretty\nclose in accuracy",
    "start": "1286180",
    "end": "1288640"
  },
  {
    "text": "but very far apart in\nterms of cost and latency.",
    "start": "1288640",
    "end": "1292180"
  },
  {
    "text": "And so you might think, I\ncan sacrifice this amount",
    "start": "1292180",
    "end": "1295120"
  },
  {
    "text": "of accuracy here to do this\nmuch in terms of reduced latency",
    "start": "1295120",
    "end": "1301090"
  },
  {
    "text": "and cost.",
    "start": "1301090",
    "end": "1302210"
  },
  {
    "text": "Here's another such comparison.",
    "start": "1302210",
    "end": "1303880"
  },
  {
    "text": "ColBERT small has\nlatency of 206.",
    "start": "1303880",
    "end": "1307420"
  },
  {
    "text": "BT Splade large\nhas latency of 246",
    "start": "1307420",
    "end": "1310510"
  },
  {
    "text": "and costs a fraction of what\nthe ColBERT model costs.",
    "start": "1310510",
    "end": "1313960"
  },
  {
    "text": "Now, the ColBERT model\nis much more accurate,",
    "start": "1313960",
    "end": "1316690"
  },
  {
    "text": "but maybe this is\nan affordable drop",
    "start": "1316690",
    "end": "1318970"
  },
  {
    "text": "here given the\nother considerations",
    "start": "1318970",
    "end": "1321460"
  },
  {
    "text": "that are in play.",
    "start": "1321460",
    "end": "1322600"
  },
  {
    "text": "And here's another comparison\nbetween two BT SPLADE",
    "start": "1322600",
    "end": "1325330"
  },
  {
    "text": "large models.",
    "start": "1325330",
    "end": "1326559"
  },
  {
    "text": "For a modest\nreduction in latency",
    "start": "1326560",
    "end": "1329350"
  },
  {
    "text": "that comes from\nrunning on a GPU,",
    "start": "1329350",
    "end": "1331809"
  },
  {
    "text": "I have to pay a whole lot more\nmoney for the same accuracy.",
    "start": "1331810",
    "end": "1336900"
  },
  {
    "text": "So for example, you\nstart to see that it's",
    "start": "1336900",
    "end": "1338810"
  },
  {
    "text": "very unlikely that you'd be able\nto justify using a GPU with BT",
    "start": "1338810",
    "end": "1343460"
  },
  {
    "text": "SPLADE large when it's\nonly a modest latency",
    "start": "1343460",
    "end": "1346549"
  },
  {
    "text": "reduction but a huge\nballooning in the overall cost",
    "start": "1346550",
    "end": "1349910"
  },
  {
    "text": "that you pay.",
    "start": "1349910",
    "end": "1351080"
  },
  {
    "text": "And I think there are\nlots of other comparisons",
    "start": "1351080",
    "end": "1353299"
  },
  {
    "text": "like this that we\ncan make, and we're",
    "start": "1353300",
    "end": "1355130"
  },
  {
    "text": "going to talk later in the\ncourse about how we might",
    "start": "1355130",
    "end": "1357950"
  },
  {
    "text": "systemize some of\nthese observations",
    "start": "1357950",
    "end": "1360649"
  },
  {
    "text": "into a leaderboard that\ntakes account of all",
    "start": "1360650",
    "end": "1363170"
  },
  {
    "text": "of these different pressures.",
    "start": "1363170",
    "end": "1364910"
  },
  {
    "text": "And IR is a wonderful playground\nfor thinking about such trade",
    "start": "1364910",
    "end": "1368630"
  },
  {
    "text": "offs.",
    "start": "1368630",
    "end": "1370180"
  },
  {
    "start": "1370180",
    "end": "1375000"
  }
]