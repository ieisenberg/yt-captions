[
  {
    "start": "0",
    "end": "17000"
  },
  {
    "start": "0",
    "end": "5190"
  },
  {
    "text": "Welcome to CS224N lecture 15. So I'm Megan. And I'm one of the\nCAs in this course. And I'm also a PhD student\nworking at [INAUDIBLE]..",
    "start": "5190",
    "end": "12510"
  },
  {
    "text": "And today I'll be talking\nabout integrating knowledge in language models. ",
    "start": "12510",
    "end": "18750"
  },
  {
    "start": "17000",
    "end": "144000"
  },
  {
    "text": "So some quick reminders. Your project milestones\nwere due today. So hopefully, you\nturned those in already or will be turning them in\nthe next couple of days.",
    "start": "18750",
    "end": "25180"
  },
  {
    "text": "And we'll try to\ngive you feedback on those as fast as possible. So something to be aware of is\nthe change in grading basis.",
    "start": "25180",
    "end": "31320"
  },
  {
    "text": "And of course, withdrawal\ndeadline is this Friday. So if you want to make\nany change in your grade,",
    "start": "31320",
    "end": "36500"
  },
  {
    "text": "make sure you do that by then. And we'll be getting you the\ngrades back on assignment 5 by then as well, in case\nthat's helpful in making",
    "start": "36500",
    "end": "42062"
  },
  {
    "text": "your decision. And finally, your final\nprojects are due in two weeks so hopefully, those\nare going smoothly.",
    "start": "42062",
    "end": "49167"
  },
  {
    "text": "So the topic of the\nday is integrating knowledge in language models. You've seen a bit about\nthis idea in a assignment 5",
    "start": "49167",
    "end": "54620"
  },
  {
    "text": "and also on Colin [INAUDIBLE]\nlecture last class. So in assignment\nfive, the task was to train a model to predict the\nbirthplace of a person, given",
    "start": "54620",
    "end": "62000"
  },
  {
    "text": "their name. And you saw that by pretraining\non a larger data set, you're actually able to\ndo better on this task",
    "start": "62000",
    "end": "67430"
  },
  {
    "text": "since you could encode some\nreal knowledge into the language model. And then last lecture,\nColin [INAUDIBLE]",
    "start": "67430",
    "end": "73360"
  },
  {
    "text": "presented how T5 could actually\nfine-tune for a closed domain question answering task\nsuch that you can give T5",
    "start": "73360",
    "end": "80750"
  },
  {
    "text": "a natural language question,\nand it will return an answer. So today we'll be\nbuilding on these threads and looking at techniques\nthat researchers have recently",
    "start": "80750",
    "end": "87580"
  },
  {
    "text": "been developing to increase\nthe amount of knowledge in language models. So we're going to start with a\nquick recap of language models",
    "start": "87580",
    "end": "94610"
  },
  {
    "text": "just to make sure we're\nall on the same page. Then we're going to talk\nabout what types of knowledge language models\ncan already encode,",
    "start": "94610",
    "end": "100278"
  },
  {
    "text": "and what they might struggle on. We'll also motivate\nwhy researchers are interested in increasing\nthe amount of knowledge",
    "start": "100278",
    "end": "105920"
  },
  {
    "text": "in language models and\nwhat this could enable for a future systems if we\nhave language models that can actually reliably\nrecall knowledge.",
    "start": "105920",
    "end": "114587"
  },
  {
    "text": "We'll talk about three\nbroad classes of techniques that researchers have been\nusing to add knowledge to language models.",
    "start": "114587",
    "end": "119590"
  },
  {
    "text": "These include adding\npretrained entity embeddings, using external memory\nor key-value store,",
    "start": "119590",
    "end": "124680"
  },
  {
    "text": "or even just modifying\nthe training data. And for each of\nthese techniques, we'll talk about at\nleast one recent work",
    "start": "124680",
    "end": "130237"
  },
  {
    "text": "that used that technique. So hopefully, it's clear to\nsee how to actually employ it in practice.",
    "start": "130238",
    "end": "135500"
  },
  {
    "text": "And then finally, we'll\nwrap up by talking about how to evaluate the\nknowledge in language models and the challenges that come\nup when trying to do this.",
    "start": "135500",
    "end": "142002"
  },
  {
    "text": " So let's dive right in. We're going to start by talking\nabout standard language models.",
    "start": "142002",
    "end": "148640"
  },
  {
    "start": "144000",
    "end": "246000"
  },
  {
    "text": "You learned about these at\nthe beginning of the course. And the task is to predict the\nnext word in a sequence a text",
    "start": "148640",
    "end": "153820"
  },
  {
    "text": "and to compute the\nprobability of a sequence. So you may remember the\nexample, the students opened their blank.",
    "start": "153820",
    "end": "159530"
  },
  {
    "text": "And we talked about\ncould be minds, exams, bring those books here. And the task of\nstandard language model",
    "start": "159530",
    "end": "165250"
  },
  {
    "text": "is to predict the most likely\nnext word in the sequence. A couple of lectures\nago, John also",
    "start": "165250",
    "end": "170860"
  },
  {
    "text": "introduced the notion of\nmasked language models. And so in predicting the next\nword in a sequence of text, the task is to predict\nthe masked token.",
    "start": "170860",
    "end": "177879"
  },
  {
    "text": "And this is done using\nbidirectional context. So you may remember the\nexample, I masked the mask.",
    "start": "177880",
    "end": "183670"
  },
  {
    "text": "And the goal of the\nmasked language model is to make the most likely\ntoken for each of the masked out words.",
    "start": "183670",
    "end": "189450"
  },
  {
    "text": "So maybe I went to the store. So while there are some\ndifferences in these two types of language models whether\nyou're predicting the next word",
    "start": "189450",
    "end": "196310"
  },
  {
    "text": "or whether you're predicting\nthe masked out token, they're similar\nand that they can both be trained over large\namounts of unlabeled text.",
    "start": "196310",
    "end": "203142"
  },
  {
    "text": "And this is one\nof the reasons why they've been so widely adopted. They don't require any\nhuman annotated data.",
    "start": "203143",
    "end": "210131"
  },
  {
    "text": "So you've seen that\nlanguage models can be used for a variety\nof tasks from summarization to dialogue to fluency\nevaluation Tests involved would",
    "start": "210132",
    "end": "218210"
  },
  {
    "text": "be the generating\ntext or evaluating the probability of text. And more recently, we've seen\nthat language models can also",
    "start": "218210",
    "end": "225189"
  },
  {
    "text": "be used to generate pretrained\nrepresentations of text that encode some notion of\nlanguage understanding",
    "start": "225190",
    "end": "230855"
  },
  {
    "text": "and has been shown to be widely\nuseful for different downstream NLP tasks.",
    "start": "230855",
    "end": "236090"
  },
  {
    "text": "And then finally, today, we're\ngoing to touch on this idea that if language\nmodels are trained over massive amounts\nof text, can they even",
    "start": "236090",
    "end": "243020"
  },
  {
    "text": "be used as a knowledge base?  So we're going to\nstart by looking",
    "start": "243020",
    "end": "248090"
  },
  {
    "start": "246000",
    "end": "396000"
  },
  {
    "text": "at what types of\nfactual knowledge a language model\nmight already know. And these examples\nare taken from a paper",
    "start": "248090",
    "end": "253430"
  },
  {
    "text": "by Petroni, et al in EMNLP\na couple of years ago. And the goal is to test\nthe factual or commonsense",
    "start": "253430",
    "end": "259430"
  },
  {
    "text": "knowledge in existing language\nmodels such as BERT-Large. So let's check out what\nBERT-Large predicts.",
    "start": "259430",
    "end": "266449"
  },
  {
    "text": "iPod touch is produced by Apple. London Jazz Festival\nis located in London.",
    "start": "266450",
    "end": "272480"
  },
  {
    "text": "Dani Alves plays with Santos. Carl III used to\ncommunicate in German.",
    "start": "272480",
    "end": "277730"
  },
  {
    "text": "And ravens can fly. So here, we have the\ncorrect predictions in green and incorrect\npredictions in red.",
    "start": "277730",
    "end": "284255"
  },
  {
    "text": "And if you know\nanything about sports, you may know that Dani\nAlves is a soccer player. Santos is a soccer team.",
    "start": "284255",
    "end": "290400"
  },
  {
    "text": "Here they were hoping that\nit would predict Barcelona. Because at least at the\ntime of this data set, apparently he played\nfor Barcelona.",
    "start": "290400",
    "end": "296340"
  },
  {
    "text": "And Carl III is actually used\nto communicate in Swedish, not German. So what's good\nabout these examples",
    "start": "296340",
    "end": "302890"
  },
  {
    "text": "is the predictions are\ngenerally reasonable. If you didn't know the ground\ntruth, they all make sense.",
    "start": "302890",
    "end": "308047"
  },
  {
    "text": "When you want to produce-- when you want to\npredict a language, you do, in fact,\npredict the language.",
    "start": "308047",
    "end": "313700"
  },
  {
    "text": "But of course, they're\nnot all factually correct. So what made this happen?",
    "start": "313700",
    "end": "319180"
  },
  {
    "text": "Well, for one, the fact might\nnot have been seen in training. And you can't expect\nthe language model to do more than recall facts\nthat it has seen in training.",
    "start": "319180",
    "end": "326280"
  },
  {
    "text": "It can't make up facts about\nthe world, for instance. It's also possible that the\nfact is just really rare.",
    "start": "326280",
    "end": "331800"
  },
  {
    "text": "So maybe the language model has\nseen the fact during training, but it hasn't seen\nit enough times to actually memorize the fact.",
    "start": "331800",
    "end": "338759"
  },
  {
    "text": "And the last issue is\na little more subtle, which a model might just be\nvery sensitive to the phrasing of the fill in the\nblank statement.",
    "start": "338760",
    "end": "345735"
  },
  {
    "text": "And so for example, you\nmight have statements like x was created in\nblank, that the model can't predict correctly.",
    "start": "345735",
    "end": "351510"
  },
  {
    "text": "But if you change it\nto x was made in blank, suddenly, it can\npredict it correctly. And I will come\nback to this and how",
    "start": "351510",
    "end": "357060"
  },
  {
    "text": "to actually evaluate the\nknowledge in these language models.",
    "start": "357060",
    "end": "362420"
  },
  {
    "text": "So it's inability to\nreliably recall knowledge is a key challenge facing\nlanguage models today. And they will be the\nfocus of this talk.",
    "start": "362420",
    "end": "369597"
  },
  {
    "text": "Recent works have found that\nlanguage models can recover some knowledge, including\nthe work that Colin presented",
    "start": "369597",
    "end": "375139"
  },
  {
    "text": "last class. They've had very\nencouraging results. But there's still\na way to go as we saw with the fill in\nthe blank statements",
    "start": "375140",
    "end": "381648"
  },
  {
    "text": "and with these challenges\nthat we just discussed above. So as a result, the\npast couple of years have had a ton of rapid progress\nin this area of research",
    "start": "381648",
    "end": "389300"
  },
  {
    "text": "in terms of trying to figure out\nhow do you actually encode more knowledge in language models. ",
    "start": "389300",
    "end": "397570"
  },
  {
    "start": "396000",
    "end": "468000"
  },
  {
    "text": "So I also want to\nmotivate why researchers are interested in building\nlanguage models that can more",
    "start": "397570",
    "end": "402580"
  },
  {
    "text": "reliably recall knowledge. And one of these reasons is that\nthe pretrained representations",
    "start": "402580",
    "end": "408000"
  },
  {
    "text": "are used in a variety\nof downstream tasks. And some of these downstream\ntasks are knowledge intensive.",
    "start": "408000",
    "end": "413750"
  },
  {
    "text": "So for instance, you might\nhave a downstream task to extract the relations between\ntwo entities in a sentence.",
    "start": "413750",
    "end": "419840"
  },
  {
    "text": "And this is commonly known\nas relation extraction. And this is much\neasier if you have some knowledge of\nthe entities, which",
    "start": "419840",
    "end": "426229"
  },
  {
    "text": "could be potentially provided by\nthis pretrained language model representation.",
    "start": "426230",
    "end": "431683"
  },
  {
    "text": "And when we talk\nabout evaluation, we'll talk about\nwhat types of tests are most likely to benefit\nfrom this knowledge",
    "start": "431683",
    "end": "437000"
  },
  {
    "text": "rich pretrained representations. And then as a stretch\ngoal, some researchers",
    "start": "437000",
    "end": "442830"
  },
  {
    "text": "are starting to\npropose the idea that, can language models\nactually ultimately be used to replace traditional\nknowledge bases?",
    "start": "442830",
    "end": "450307"
  },
  {
    "text": "So instead of querying a\nknowledge base for a fact like you might\nright now with SQL, you'd create a language model\nwith a natural language prompt.",
    "start": "450307",
    "end": "458150"
  },
  {
    "text": "And of course, this does\nrequire the language model to have high quality\non recalling facts. So we might not be\nthere yet, but it's",
    "start": "458150",
    "end": "464870"
  },
  {
    "text": "an interesting direction\nfor us to be moving towards. So I want to make\nit super clear what",
    "start": "464870",
    "end": "470449"
  },
  {
    "start": "468000",
    "end": "597000"
  },
  {
    "text": "I mean by a knowledge base. Here we're just talking\nabout a knowledge graph where the nodes in the\ngraph would be entities,",
    "start": "470450",
    "end": "477170"
  },
  {
    "text": "and the edges are going to be\nrelations between the entities. So for example, here\nwe have a subset of a knowledge graph for\nFranklin D. Roosevelt.",
    "start": "477170",
    "end": "484670"
  },
  {
    "text": "And you see the information\nabout his spouse, his place of birth, his\ndate of birth, and so on.",
    "start": "484670",
    "end": "490380"
  },
  {
    "text": "An important thing\nto note is this is a structured way of storing\nthe knowledge since it's just in a graph form.",
    "start": "490380",
    "end": "496290"
  },
  {
    "text": "And you can actually\ndescribe these graphs with knowledge\ngraph triples, which will be an important vocabulary\nword throughout this talk.",
    "start": "496290",
    "end": "502680"
  },
  {
    "text": "So knowledge graph\ntriple would be consisting of a subject entity,\na relation, and an object",
    "start": "502680",
    "end": "509069"
  },
  {
    "text": "entity. So for instance, here we might\nhave Franklin D. Roosevelt, date of birth, January\n30, 1882, and that would",
    "start": "509070",
    "end": "516150"
  },
  {
    "text": "form a knowledge graph triple. We'll also refer to this as a\napparent entity, a relation, and a tail entity.",
    "start": "516150",
    "end": "523669"
  },
  {
    "text": "So Wikidata is one\nvery popular knowledge base you might come across if\nyou're working in this area. It's a free\nknowledge base that's",
    "start": "523669",
    "end": "529245"
  },
  {
    "text": "actually populated by humans. So they're filling in these\nrelations and entities. And it's also multilingual.",
    "start": "529245",
    "end": "536785"
  },
  {
    "text": "So if you want information from\nthis knowledge base, what you'd do is you'd write a SQL query.",
    "start": "536785",
    "end": "542199"
  },
  {
    "text": "This is a simplified one. But the idea is if\nyou'd want to figure out the date of birth of\nFranklin Roosevelt,",
    "start": "542200",
    "end": "548269"
  },
  {
    "text": "so you would write a\nquery like follows. Now if instead you want\nto create a language model",
    "start": "548270",
    "end": "554140"
  },
  {
    "text": "as the knowledge\nbase, you'll have something like this diagram that\nyou've actually probably seen in several lectures now.",
    "start": "554140",
    "end": "560350"
  },
  {
    "text": "And the idea is you're\ntraining a language model over this unstructured text. And then you'll use\na language model",
    "start": "560350",
    "end": "566529"
  },
  {
    "text": "to just answer these natural\nlanguage query statements. So here, this is the\nwork on T5, where they're",
    "start": "566530",
    "end": "573070"
  },
  {
    "text": "training T5 over natural\nlanguage over unstructured text with the span corruption task.",
    "start": "573070",
    "end": "578800"
  },
  {
    "text": "And then they're asking T5, when\nwas Franklin D Roosevelt born? And the idea is TD will\nproduce a textual answer.",
    "start": "578800",
    "end": "586400"
  },
  {
    "text": "So you can see this\ncontrast very much with the old approach of\nusing a traditional knowledge base where the knowledge\nbase is structured,",
    "start": "586400",
    "end": "592183"
  },
  {
    "text": "and you have these SQL\nstatements to query it. ",
    "start": "592183",
    "end": "597944"
  },
  {
    "start": "597000",
    "end": "761000"
  },
  {
    "text": "So what are the advantages\nof using language models over traditional\nknowledge bases? And why might people think\nthis could be a good idea?",
    "start": "597945",
    "end": "604600"
  },
  {
    "text": "Well, for one, the\nlanguage models are pretrained\nover large amounts of unstructured\nand unlabeled text,",
    "start": "604600",
    "end": "610248"
  },
  {
    "text": "whereas traditional\nknowledge bases require manual annotation,\nlike with Wikidata, people are actually\npopulating it,",
    "start": "610248",
    "end": "616150"
  },
  {
    "text": "or complex NLP pipelines to\nextract from unstructured text into a structured form that\nforms the knowledge base.",
    "start": "616150",
    "end": "624355"
  },
  {
    "text": "Language models can also support\nmore flexible natural language queries. So if we take the example, what\nis the final F in the song UFOF",
    "start": "624355",
    "end": "632720"
  },
  {
    "text": "stand for, a knowledge\nbase probably won't have a feel for\nfinal F. So it won't be able to answer your query.",
    "start": "632720",
    "end": "639440"
  },
  {
    "text": "But there's a chance that a\nlanguage model could actually learn and have a response for\nthis natural language query.",
    "start": "639440",
    "end": "646290"
  },
  {
    "text": "They also had a less\nextreme example, in this paper by\nPetroni and others, where maybe your\nrelation would be,",
    "start": "646290",
    "end": "651750"
  },
  {
    "text": "is works for in your knowledge\nbase, and then you ask for, is working for, and\nthe knowledge base",
    "start": "651750",
    "end": "657540"
  },
  {
    "text": "doesn't have an exact\nmatch on the field, and so it returns\nan empty response. And as much-- it's reasonable\nto believe that your language",
    "start": "657540",
    "end": "664440"
  },
  {
    "text": "model could figure out that\nthese relations are similar. So if I know the\nanswer to one of them, I probably know the\nanswer to the other.",
    "start": "664440",
    "end": "673040"
  },
  {
    "text": "Of course, it's\nnot all advantages. There's also many open\nchallenges using language models as knowledge bases.",
    "start": "673040",
    "end": "679410"
  },
  {
    "text": "So for one, it's\nharder to interpret. When a traditional knowledge\nbase produces an answer, there's actually provenance\ninformation associated with,",
    "start": "679410",
    "end": "685963"
  },
  {
    "text": "why did it return\nthat particular query? But with a language model,\nit's really not clear",
    "start": "685963",
    "end": "692290"
  },
  {
    "text": "why it might produce\na prediction. The knowledge is just encoded\nin the parameters of the model.",
    "start": "692290",
    "end": "698260"
  },
  {
    "text": "It's also harder to trust. So you saw this\nin assignment five where the language model could\nproduce realistic predictions,",
    "start": "698260",
    "end": "704970"
  },
  {
    "text": "but they are incorrect. So it's not easy to know when\nthe language model actually knows a fact versus it's using\nbiases to make its prediction.",
    "start": "704970",
    "end": "713627"
  },
  {
    "text": "And in the case of the\ntraditional knowledge base, if it doesn't know\na fact, it's just going to have an empty response.",
    "start": "713627",
    "end": "720010"
  },
  {
    "text": "And then finally, knowledge\nbases-- or language models are harder to modify.",
    "start": "720010",
    "end": "725170"
  },
  {
    "text": "So in a knowledge base, if\nyou want to update a fact, you just change\nthe fact directly in the structured data.",
    "start": "725170",
    "end": "731665"
  },
  {
    "text": "But in a language model,\nit's not quite clear how you would do this. You could fine-tune the model\nlonger on the updated data,",
    "start": "731665",
    "end": "737950"
  },
  {
    "text": "but how do if it still has some\nmemorization of the old fact?",
    "start": "737950",
    "end": "743288"
  },
  {
    "text": "So there are a lot\nof open challenges to this goal of actually\nusing language models as traditional knowledge bases.",
    "start": "743288",
    "end": "749232"
  },
  {
    "text": "But hopefully, you'll\nsee why some people think this could actually\nbe a good idea and why researchers are\ninterested in training language",
    "start": "749232",
    "end": "756650"
  },
  {
    "text": "models that can actually\nintegrate more knowledge. So that brings us to\nsection two of the talk.",
    "start": "756650",
    "end": "763570"
  },
  {
    "start": "761000",
    "end": "810000"
  },
  {
    "text": "So I want to pause here just in\ncase there's any questions OK?",
    "start": "763570",
    "end": "773480"
  },
  {
    "text": "I think that's OK. Yeah. OK. Awesome. So now we're going to be\ntalking about what techniques",
    "start": "773480",
    "end": "778580"
  },
  {
    "text": "researchers are using\nto actually add more knowledge to language models. So we're going to talk\nabout three broad classes",
    "start": "778580",
    "end": "785450"
  },
  {
    "text": "of techniques. This is by no means exhaustive. But hopefully, it gives\nyou a good overview so that if you want to\ndive deeper, you can.",
    "start": "785450",
    "end": "793560"
  },
  {
    "text": "So we'll start by talking\nabout adding pretrained entity embeddings. And for each section, we'll\nfocus on the first work",
    "start": "793560",
    "end": "799773"
  },
  {
    "text": "that you see in the bullets. But we'll also talk about,\nbriefly, some of the variants. So you see how the works\nwithin each class can differ",
    "start": "799773",
    "end": "807363"
  },
  {
    "text": "and what knobs you can turn.  So for adding\npretrained embeddings,",
    "start": "807363",
    "end": "813150"
  },
  {
    "start": "810000",
    "end": "900000"
  },
  {
    "text": "we first need to figure out\nwhat pretrained embeddings would actually be the\nmost useful to add knowledge to language models.",
    "start": "813150",
    "end": "819630"
  },
  {
    "text": "And this can start\nwith the observation that facts about the world are\nusually in terms of entities.",
    "start": "819630",
    "end": "825250"
  },
  {
    "text": "So if we have a\nfact like Washington was the first president\nof the United States, we have the entities\nWashington and United States.",
    "start": "825250",
    "end": "832930"
  },
  {
    "text": "But pretrained word\nembeddings don't have this notion of entities. So we'd have different\nword embeddings for USA,",
    "start": "832930",
    "end": "839230"
  },
  {
    "text": "United States of America, and\nAmerica even though these all refer to the same entity.",
    "start": "839230",
    "end": "844930"
  },
  {
    "text": "And this makes it challenging\nfor the language model to actually learn\nany representations over these entities since\nthey may be referred",
    "start": "844930",
    "end": "851529"
  },
  {
    "text": "to many ways in the text.  So what if instead, we have a\nsingle embedding per entity?",
    "start": "851530",
    "end": "858910"
  },
  {
    "text": "And we'll refer to these\nas entity embeddings. So now you'd have a single\nentity embedding for USA,",
    "start": "858910",
    "end": "865520"
  },
  {
    "text": "United States of\nAmerica, and America. And whenever you see a phrase in\ntext referring to this entity,",
    "start": "865520",
    "end": "872329"
  },
  {
    "text": "you would use the\nsame entity embedding. And these entity\nembeddings can actually be pretrained to encode\nthis factual knowledge",
    "start": "872330",
    "end": "879870"
  },
  {
    "text": "about the world. And these first class\ntechniques we'll be looking at will be how do you actually\nbest use these pretrained entity",
    "start": "879870",
    "end": "885790"
  },
  {
    "text": "embeddings in a language model.  So I need to make a quick\nnote that these entity",
    "start": "885790",
    "end": "892710"
  },
  {
    "text": "embeddings are only useful\nto a language model. So if you can do\nanother NLP task called entity linking well.",
    "start": "892710",
    "end": "900470"
  },
  {
    "start": "900000",
    "end": "1069000"
  },
  {
    "text": "So I'm going to take a\nquick aside and explain what is entity linking. So a definition\nof entity linking",
    "start": "900470",
    "end": "906839"
  },
  {
    "text": "is the link mentions in text\nto entities in a knowledge I like to think\nabout this in terms",
    "start": "906840",
    "end": "912390"
  },
  {
    "text": "of how we use word embeddings. So if you want to\nuse word embeddings, and you have a sentence,\nyou're going to first tokenize",
    "start": "912390",
    "end": "917832"
  },
  {
    "text": "that sentence into words. And then for each\nword, you're going to look up their corresponding\nID in some word embedding",
    "start": "917832",
    "end": "922980"
  },
  {
    "text": "matrix. And now, you have\nyour word embedding. Well, for entity embeddings,\nthe dictionary lookup",
    "start": "922980",
    "end": "928190"
  },
  {
    "text": "isn't so easy. You might have sentences\nlike Washington is the first president\nof United States.",
    "start": "928190",
    "end": "934220"
  },
  {
    "text": "Well, Washington has two\ndifferent candidates. Are we talking about\nGeorge Washington? Or are we talking\nabout Washington State?",
    "start": "934220",
    "end": "940370"
  },
  {
    "text": "And these are different entities\nthat have different entity embeddings. And the QIDs here would just be\ntheir identifiers in Wikidata.",
    "start": "940370",
    "end": "949250"
  },
  {
    "text": "And then United States\njust has a single entity. So the task of entity linking\nis to figure out correctly",
    "start": "949250",
    "end": "955420"
  },
  {
    "text": "these ambiguous mentions. What entity do they actually\nlink to in a knowledge base? And there's many different ways\nyou can do this entity linking.",
    "start": "955420",
    "end": "963163"
  },
  {
    "text": "So one way you\nmight be able to do this is to figure out\nthat, oh, I see the context word of president so\nWashington probably",
    "start": "963163",
    "end": "968930"
  },
  {
    "text": "links to George Washington. Just some more definitions. We're going to\nrefer to Washington",
    "start": "968930",
    "end": "974279"
  },
  {
    "text": "as a mention, the United\nStates as a mention, and then the things that\nthe mention could link to. So the two options\nfor Washington",
    "start": "974280",
    "end": "981210"
  },
  {
    "text": "are going to be candidates. So this is a whole\nresearch area of its own. And I encourage you check out\nthe resources at the bottom",
    "start": "981210",
    "end": "988195"
  },
  {
    "text": "if you're interested\nin learning more. But right now the most\nimportant thing to understand is that entity thinking is\nwhat is going to tell us",
    "start": "988195",
    "end": "994295"
  },
  {
    "text": "which entity embeddings are\nactually relevant to the text and which ones you\nwant to use as you iterate through a sequence.",
    "start": "994295",
    "end": "1000906"
  },
  {
    "text": "Megan, there are a few\nquestions around here. One of them is, so\nthat's entity linking,",
    "start": "1000906",
    "end": "1008880"
  },
  {
    "text": "but what about the relations? Yeah. So some of the works\nwe'll talk about",
    "start": "1008880",
    "end": "1014680"
  },
  {
    "text": "will only use the\nentity embeddings. Some of these have\nbeen pretrained with relation information.",
    "start": "1014680",
    "end": "1020450"
  },
  {
    "text": "But in the end, you only\nhave an entity embedding. And so relation extraction\nis yet another NLP task",
    "start": "1020450",
    "end": "1025959"
  },
  {
    "text": "you could also do. But here, we're just talking\nabout entity linking. Then if you have the knowledge\ngraph you showed earlier,",
    "start": "1025960",
    "end": "1032079"
  },
  {
    "text": "it had relations in it, right? Do you get any connection\nbetween that and the text?",
    "start": "1032079",
    "end": "1037510"
  },
  {
    "text": " I mean, that's the goal of\nrelation extraction, right?",
    "start": "1037510",
    "end": "1044047"
  },
  {
    "text": "It's to figure out,\ngiven the entities, what is the relation between\nthem, which would then form the full triple of\nhead entity, tail entity,",
    "start": "1044047",
    "end": "1050980"
  },
  {
    "text": "and relation. ",
    "start": "1050980",
    "end": "1056770"
  },
  {
    "text": "OK. Then I think people want\nto know more about how this is going to be used. Maybe you should go on\nand show some examples",
    "start": "1056770",
    "end": "1063550"
  },
  {
    "text": "Yeah. I will. For sure. OK. ",
    "start": "1063550",
    "end": "1069590"
  },
  {
    "start": "1069000",
    "end": "1155000"
  },
  {
    "text": "So entity embeddings,\njust to summarize, they're like word\nembeddings, but they're for entities in\na knowledge base.",
    "start": "1069590",
    "end": "1074600"
  },
  {
    "text": "So you'll have some\nvector associated to George Washington. And it should be meaningful\nin embedding space such",
    "start": "1074600",
    "end": "1080445"
  },
  {
    "text": "that maybe the George\nWashington vector is close to the vectors\nfor other founding fathers. So we're going to briefly talk\nabout some methods for training",
    "start": "1080445",
    "end": "1087750"
  },
  {
    "text": "entity embeddings. There is knowledge\ngraph embedding methods. You might have heard of the\nTransE embedding method.",
    "start": "1087750",
    "end": "1093157"
  },
  {
    "text": "So this starts from the idea\nof having these knowledge graph triples. And you want to learn pretrained\nentity and pretrained relation",
    "start": "1093157",
    "end": "1099210"
  },
  {
    "text": "embeddings. And you want it to be\nthe case that the subject embedding and the\nrelation embedding, the sum of those two,\nis close to the object",
    "start": "1099210",
    "end": "1106289"
  },
  {
    "text": "embedding in vector space. So it's an algorithm to\nlearn that constraint.",
    "start": "1106290",
    "end": "1111321"
  },
  {
    "text": "There's also word entity\nco-occurrence methods. So these build off of Word2Vec. One of them's even\ncalled Wikipedia2Vec.",
    "start": "1111322",
    "end": "1117240"
  },
  {
    "text": "And the idea is\ngiven an entity, you want to figure out\nwhat words are most likely to co-occur around it.",
    "start": "1117240",
    "end": "1124130"
  },
  {
    "text": "And then the last method, or\none of the other methods that is common now is actually\njust using the transformer",
    "start": "1124130",
    "end": "1129980"
  },
  {
    "text": "to learn representations\nof an entity by encoding the\nentity description. And so BLINK from Facebook is\nan approach that does this.",
    "start": "1129980",
    "end": "1137910"
  },
  {
    "text": "So the methods we'll\ntalk about today are actually agnostic to how\nyou train your pretrained entity embedding. But I think it's important\nto know that there's actually",
    "start": "1137910",
    "end": "1144920"
  },
  {
    "text": "a wide variety of methods to\ntrain these pretrained entity embeddings. And it's actually\nnot clear which",
    "start": "1144920",
    "end": "1150470"
  },
  {
    "text": "method is best for using them\ndownstream in language models. ",
    "start": "1150470",
    "end": "1156298"
  },
  {
    "text": "So one of the key challenges\nof using pretrained entity embeddings in language\nmodels is figuring out how to incorporate\nthem when they're",
    "start": "1156298",
    "end": "1162580"
  },
  {
    "text": "from a different embeddings\nspace in the language model. And so what we'll\ndo, or the approach that we'll look at today\nwill learn a fusion layer",
    "start": "1162580",
    "end": "1170040"
  },
  {
    "text": "to combine this context\nand entity information. So we have entity\nembeddings, and we have the contextualized\nword embeddings",
    "start": "1170040",
    "end": "1176580"
  },
  {
    "text": "from our language model. So if we take a\nsequence of text, and we imagine that j indicates\nthe jth element in a sequence,",
    "start": "1176580",
    "end": "1184953"
  },
  {
    "text": "then the challenge\nhere is we want to figure out how do we\ncombine some word embedding wj with some aligned\nentity embedding ek.",
    "start": "1184953",
    "end": "1192448"
  },
  {
    "text": "So here an alignment could\nbe like in the example where we had, Washington was\nthe first President.",
    "start": "1192448",
    "end": "1197530"
  },
  {
    "text": "Washington would be\na word embedding, and George Washington would be\nthe aligned entity embedding there.",
    "start": "1197530",
    "end": "1203350"
  },
  {
    "text": "So you could imagine,\nin this case, let's say, your\nwj is Washington, and your ek is your\nentity embedding",
    "start": "1203350",
    "end": "1208720"
  },
  {
    "text": "for George Washington, and you\nwant to align them together. So what you can do is learn a\nweight matrix, Wt for the text",
    "start": "1208720",
    "end": "1215470"
  },
  {
    "text": "and we for the entity to\nproject this embeddings to the same dimension before\nyou sum them, and finally, take",
    "start": "1215470",
    "end": "1222400"
  },
  {
    "text": "an activation\nfunction over them. So the idea is that by\nhaving some fusion layer",
    "start": "1222400",
    "end": "1227770"
  },
  {
    "text": "mechanism like this,\nyou can actually use these entity embeddings\nand these contextual word",
    "start": "1227770",
    "end": "1232990"
  },
  {
    "text": "embeddings that are in\ndifferent embedding spaces and fuse them together\nto have the single hidden",
    "start": "1232990",
    "end": "1238390"
  },
  {
    "text": "representation for the\nelement in the sequence. ",
    "start": "1238390",
    "end": "1243533"
  },
  {
    "text": "So the approaches\nwe'll talk about today all have some\nmechanism either very similar to this or\nsome variation of this",
    "start": "1243533",
    "end": "1249640"
  },
  {
    "text": "to do this combination of the\ncontext and entity information. ",
    "start": "1249640",
    "end": "1255302"
  },
  {
    "start": "1255000",
    "end": "1334000"
  },
  {
    "text": "So the first approach\nwe're talking about is called ERNIE, Enhanced\nLanguage Representation with Informative Entities.",
    "start": "1255302",
    "end": "1261217"
  },
  {
    "text": "And so this just builds on\nwhat we already talked about. It uses pretrained\nentity embeddings. And it also uses this\nnotion of a fusion layer.",
    "start": "1261217",
    "end": "1269400"
  },
  {
    "text": "So the first block in\nERNIE is a text encoder which is a multi-layer\nbidirectional transformer",
    "start": "1269400",
    "end": "1274740"
  },
  {
    "text": "encoder. For their experiments,\nthey use BERT. But it doesn't have to be BERT.",
    "start": "1274740",
    "end": "1280180"
  },
  {
    "text": "And this is followed by\na knowledge encoder which has stacked blocks composed of\ntwo multi-headed attentions,",
    "start": "1280180",
    "end": "1286059"
  },
  {
    "text": "one is over the\nentity embeddings and one is over your token\nor subword embeddings.",
    "start": "1286060",
    "end": "1291700"
  },
  {
    "text": "And then the output of these\ncontextualized entities and token embeddings on\nmulti-headed attentions",
    "start": "1291700",
    "end": "1297010"
  },
  {
    "text": "are passed a fusion layer,\nwhich looks very similar to what we just looked at.",
    "start": "1297010",
    "end": "1302120"
  },
  {
    "text": "But now you also have new\nword and entity embeddings that you're producing as\noutput of your fusion layer.",
    "start": "1302120",
    "end": "1307960"
  },
  {
    "text": "So you see this wj\nand it's ek, which are produced as the next layer\nof word and entity embeddings.",
    "start": "1307960",
    "end": "1315800"
  },
  {
    "text": "So the i here indicates\nthat it's the ith block in the knowledge encoder. So you'll actually have multiple\nstacks of these knowledge",
    "start": "1315800",
    "end": "1322490"
  },
  {
    "text": "encoders, and you'll be doing\na fusion of the word entity embedding, producing new\nword and entity embeddings",
    "start": "1322490",
    "end": "1328497"
  },
  {
    "text": "and then passing this to the\nnext block of the knowledge encoder. ",
    "start": "1328497",
    "end": "1334820"
  },
  {
    "start": "1334000",
    "end": "1406000"
  },
  {
    "text": "So this is what the\narchitecture diagram looks like. On the left side, we have the\nT-encoder, or the text encoder,",
    "start": "1334820",
    "end": "1340850"
  },
  {
    "text": "followed by the K-encoder,\nor the knowledge encoder. And then on the\nright side, you have a zoomed in version of\nyour knowledge encoder.",
    "start": "1340850",
    "end": "1347337"
  },
  {
    "text": "So you see the\nmulti-headed attentions over the tokens\nin orange and then over the entities in yellow. And then you have this\nalignment between the word",
    "start": "1347337",
    "end": "1354260"
  },
  {
    "text": "and entities with\nthe dashed lines. So they had this example as, Bob\nDylan wrote Blowing in the Wind",
    "start": "1354260",
    "end": "1360470"
  },
  {
    "text": "in 1962. The entities here are Bob\nDylan and Blowing in the Wind.",
    "start": "1360470",
    "end": "1365510"
  },
  {
    "text": "And they have a\nsimple alignment rule where you want to align the\nentity to the first word in the entity phrase.",
    "start": "1365510",
    "end": "1371340"
  },
  {
    "text": "So you want to align\nBob Dylan to Bob. That's what the dash\nline's trying to indicate. And you want to align\nBlowing in the wind to blow.",
    "start": "1371340",
    "end": "1379210"
  },
  {
    "text": "Here this already assumes that\nentity linking has been done. And you know your\nentities in advance. So you can see that the\nentities are actually input",
    "start": "1379210",
    "end": "1385430"
  },
  {
    "text": "into the model. So after you have your\nword entity alignment, this goes to the\ninformation fusion",
    "start": "1385430",
    "end": "1391659"
  },
  {
    "text": "layer in this light\npurple gray color. And then finally, it produces\nthese new word entities embedded things as output.",
    "start": "1391660",
    "end": "1398605"
  },
  {
    "text": "And then remember that you\nhave multiple blocks of these, so this will be passed onto the\nnext block of your knowledge encoder. ",
    "start": "1398605",
    "end": "1406670"
  },
  {
    "start": "1406000",
    "end": "1499000"
  },
  {
    "text": "So how do you\nactually train this? It's pretty similar to BERT. You have a masked\nlanguage model loss, and you have a next\nsentence prediction loss.",
    "start": "1406670",
    "end": "1413472"
  },
  {
    "text": "And then they also introduce\na knowledge pretraining task, which they refer\nto as the DEA task.",
    "start": "1413473",
    "end": "1419060"
  },
  {
    "text": "It's named after a\ndenoising entity autoencoder from an ICML paper in 2008.",
    "start": "1419060",
    "end": "1425622"
  },
  {
    "text": "And the idea is they're\ngoing to randomly mask these token entity alignments. So the idea that Bob\ngoes to Bob Dylan,",
    "start": "1425623",
    "end": "1431559"
  },
  {
    "text": "they're going to mask that out\nwith some random percentage. And then they're\ngoing to predict the corresponding\nentity for a token are",
    "start": "1431560",
    "end": "1437860"
  },
  {
    "text": "the entities in the sequence. So this looks like as follows. The summation is over M\nentities in the sequence.",
    "start": "1437860",
    "end": "1445019"
  },
  {
    "text": "So this would be over\nBob Dylan and Blowing in the Wind in the\nprevious example. And given a\nparticular word, they",
    "start": "1445020",
    "end": "1451169"
  },
  {
    "text": "want to figure out what entities\nare most likely to align to in that sequence. So does Bob align to Bob Dylan?",
    "start": "1451170",
    "end": "1457800"
  },
  {
    "text": "Or does Bob align to\nBlowing in the Wind? And their motivation\nfor doing this is that if you don't\nhave this task,",
    "start": "1457800",
    "end": "1464500"
  },
  {
    "text": "all you're ever going\nto be predicting is the token with the\nmasked language model loss. And you really think that\nknowledge should also probably",
    "start": "1464500",
    "end": "1470940"
  },
  {
    "text": "be predicting over entities. So by adding this task,\nthey have some kind of task that is actually\npredicting the entity.",
    "start": "1470940",
    "end": "1477957"
  },
  {
    "text": "And they also suggest\nthat this might better fuse the knowledge or the entity\nand the word representations",
    "start": "1477957",
    "end": "1484380"
  },
  {
    "text": "than just using\nthe fusion layer. Their final loss is\nthen the summation",
    "start": "1484380",
    "end": "1490080"
  },
  {
    "text": "of the masked language model\nloss, the next sentence prediction loss and this DEA\nknowledge pretraining task",
    "start": "1490080",
    "end": "1495460"
  },
  {
    "text": "loss.  So they showed that\nablation experiment",
    "start": "1495460",
    "end": "1501360"
  },
  {
    "start": "1499000",
    "end": "1540000"
  },
  {
    "text": "that it's actually\nvery important to have this knowledge\npretraining task. So this has BERT on\nthe leftmost bar,",
    "start": "1501360",
    "end": "1508259"
  },
  {
    "text": "ERNIE has the second\nbar from the left. And so that's with all\nthe features of ERNIE. And then they try removing the\npretrained identity embeddings",
    "start": "1508260",
    "end": "1514860"
  },
  {
    "text": "and removing this\nknowledge pretraining task. So you see that BERT\nperforms the worst-- this isn't very surprising-- and\nthat ERNIE performs the best.",
    "start": "1514860",
    "end": "1522678"
  },
  {
    "text": "But what's interesting is\nthat if you remove the entity embeddings, or you remove\nthe pretraining task, they only do a little\nbetter than BERT.",
    "start": "1522678",
    "end": "1530760"
  },
  {
    "text": "And so it's really\nnecessary to actually use this pretraining\ntask to get the most use of your pretrained\nentity embeddings.",
    "start": "1530760",
    "end": "1537802"
  },
  {
    "text": " So some strengths\nof this work were that they introduced some way\nto combine entity and context",
    "start": "1537802",
    "end": "1544770"
  },
  {
    "start": "1540000",
    "end": "1751000"
  },
  {
    "text": "information through this\nfusion layer and this knowledge pretraining task. And then they also showed\nimproved performance",
    "start": "1544770",
    "end": "1550978"
  },
  {
    "text": "on downstream tasks which\nwe'll come back to when we talk about evaluation. But of course, there's\nalso some limitations.",
    "start": "1550978",
    "end": "1558270"
  },
  {
    "text": "So it needs text data with the\nentities annotated as input. And this is even true\nfor downstream tasks.",
    "start": "1558270",
    "end": "1563471"
  },
  {
    "text": "So if you remember on\nthe architecture diagram, we had the entity\ninformation actually input into the architecture.",
    "start": "1563472",
    "end": "1569905"
  },
  {
    "text": "But it's not very realistic\nthat you're necessarily going to have a good entity\nlinker for any downstream task that you want to use ERNIE on.",
    "start": "1569905",
    "end": "1577737"
  },
  {
    "text": "And the next challenge is\nthis requires more pretraining of your language model. So now you don't just\nneed to pretrain BERT,",
    "start": "1577737",
    "end": "1583460"
  },
  {
    "text": "but you also need to pretrain\nyour knowledge encoder on top. For the first\nchallenge, we're going to actually talk about a\nwork that presents a solution",
    "start": "1583460",
    "end": "1590480"
  },
  {
    "text": "to address this. For the second challenge, I\nencourage you to check out the footnote on the bottom. This introduces a work that\nactually uses pretrained entity",
    "start": "1590480",
    "end": "1598860"
  },
  {
    "text": "embeddings, uses them\nin a language model, and doesn't require\nany more pretraining. So it's pretty cool. ",
    "start": "1598860",
    "end": "1605490"
  },
  {
    "text": "I guess that's all\nI have for ERNIE. So I want a pause\nhere for questions. ",
    "start": "1605490",
    "end": "1613480"
  },
  {
    "text": "Well, here's one that's up here. So on the fusion\nlayer, it observed",
    "start": "1613480",
    "end": "1619120"
  },
  {
    "text": "that passing the entity\nembedding into a fusion layer to combine with\nthe word embedding is more powerful than\njust concatenating",
    "start": "1619120",
    "end": "1625360"
  },
  {
    "text": "the entity embedding onto\nthe end of the word embedding question mark. So I guess people are\nstill a little bit confused",
    "start": "1625360",
    "end": "1631779"
  },
  {
    "text": "as to the motivation\nfor that fusion layer. And so I guess here is this,\nthe simplest strategy would be,",
    "start": "1631780",
    "end": "1639490"
  },
  {
    "text": "since you've got\nthe entity linking, you could just concatenate\nentity embeddings onto the end of word\nembeddings and do regular BERT.",
    "start": "1639490",
    "end": "1646929"
  },
  {
    "text": "Would that work just as well? ",
    "start": "1646930",
    "end": "1653000"
  },
  {
    "text": "I think the idea is it wouldn't. Because if you imagine\nthat, let's say, your magnitudes\nare very different,",
    "start": "1653000",
    "end": "1659570"
  },
  {
    "text": "you need some way to, I\nguess, align the spaces so that anything meaningful\nin the entity embedding space",
    "start": "1659570",
    "end": "1665420"
  },
  {
    "text": "is still meaningful in\nthe word embedding space. So if you're close in\nthe word embedding space, you also would be-- you'd\nwant to be close to an entity",
    "start": "1665420",
    "end": "1671552"
  },
  {
    "text": "embedding space. So I guess that's one argument. ",
    "start": "1671552",
    "end": "1678799"
  },
  {
    "text": "Yeah. I mean, I think the\nquestion isn't--",
    "start": "1678800",
    "end": "1683970"
  },
  {
    "text": "it's a good question\nas people say. I mean, it's not\ncompletely obvious that it wouldn't\nwork to do that.",
    "start": "1683970",
    "end": "1690039"
  },
  {
    "text": "It seems like one, the potential\nproblem is some words have the entity links to them\nand some words don't.",
    "start": "1690040",
    "end": "1697960"
  },
  {
    "text": "And so you then, you\nhave zero vectors for the ones that\ndon't have anything-- That's a good point. Linked, and that might\nact a bit weirdly.",
    "start": "1697960",
    "end": "1705630"
  },
  {
    "text": "But-- Yeah. In this case, when they\ndon't have entities linked,",
    "start": "1705630",
    "end": "1711330"
  },
  {
    "text": "which is a great point,\nyeah, the first equation just simplifies to the\nfirst term plus the bias.",
    "start": "1711330",
    "end": "1717610"
  },
  {
    "text": "So an obvious\nsolution in that case when you're not\nconcatenating, that you just don't add on the term.",
    "start": "1717610",
    "end": "1723200"
  },
  {
    "text": "Yeah. That could be one reason to. ",
    "start": "1723200",
    "end": "1736250"
  },
  {
    "text": "Are there any other questions? ",
    "start": "1736250",
    "end": "1741450"
  },
  {
    "text": "I think you can go on. OK. Cool. ",
    "start": "1741450",
    "end": "1747490"
  },
  {
    "text": "So now we're talking\nabout KnowBERT. And this is from the same folks\nthat introduced the ELMo BERT.",
    "start": "1747490",
    "end": "1754929"
  },
  {
    "start": "1751000",
    "end": "1897000"
  },
  {
    "text": "And the idea here\nis that they're going to pretrain and\nintegrate an entity linker as an extension to BERT.",
    "start": "1754930",
    "end": "1760630"
  },
  {
    "text": " And so their loss\nfunction will now",
    "start": "1760630",
    "end": "1765960"
  },
  {
    "text": "be the summation of the\nnext sentence prediction, the masked language model loss,\nand its entity linking loss. So instead, the knowledge\npretraining DEA task from ERNIE",
    "start": "1765960",
    "end": "1773760"
  },
  {
    "text": "will have an entity\nlinking loss. And the idea of\nthe entity linker is you'll now have just as\nnormal sequence as input.",
    "start": "1773760",
    "end": "1780840"
  },
  {
    "text": "And the integrated entity\nlinker will figure out what are the entities\nin the sentence",
    "start": "1780840",
    "end": "1785890"
  },
  {
    "text": "or what are the mentions\nin the sentence, what are the candidates\nof those mentions, and then what\nshould be the scores",
    "start": "1785890",
    "end": "1791167"
  },
  {
    "text": "of those entities\nfor the candidates, given the context\nof the sentence. So this is all done now\nas part of the model",
    "start": "1791167",
    "end": "1797640"
  },
  {
    "text": "rather than requiring it as\nsome external pipeline stage before you could even\nuse ERNIE, for instance.",
    "start": "1797640",
    "end": "1803650"
  },
  {
    "text": "So now for downstream\ntasks, you no longer need this entity annotations. Your integrated\nentity linker will figure out what the\ncorrect entity is",
    "start": "1803650",
    "end": "1810740"
  },
  {
    "text": "and be able to use the\ncorrect entity embedding. So there's also this idea that\nlearning this entity linking",
    "start": "1810740",
    "end": "1816750"
  },
  {
    "text": "may actually better\nencode knowledge than this DEA pretraining\ntask because they show that KnowBERT\nactually outperforms",
    "start": "1816750",
    "end": "1822450"
  },
  {
    "text": "ERNIE on downstream tasks. So one reason this may occur is\nthat if you think about the DEA",
    "start": "1822450",
    "end": "1828570"
  },
  {
    "text": "task, it's actually\na bit simpler than just entity linking. So you're trying to\npredict, for instance,",
    "start": "1828570",
    "end": "1834600"
  },
  {
    "text": "what Bob linked to out of Bob\nDylan and Blowing in the Wind. And it's much easier\neven as a human to see that Bob Dylan will\nmore likely link to-- or Bob",
    "start": "1834600",
    "end": "1842070"
  },
  {
    "text": "will more likely\nlink to Bob Dylan than that Bob will link\nto Blowing in the Wind. In the entity linking\ntask, you actually",
    "start": "1842070",
    "end": "1848142"
  },
  {
    "text": "have a much harder set of\ncandidates to predict over. You're not just looking\nat the ones in a sentence. So does Washington link to\nGeorge Washington or Washington",
    "start": "1848142",
    "end": "1855809"
  },
  {
    "text": "State, actually requires\nyou using more information about the entity. So given it's a harder task,\nit's not too surprising",
    "start": "1855810",
    "end": "1862500"
  },
  {
    "text": "that it might perform\nbetter than just this easier knowledge pretraining task\nthat ERNIE introduced.",
    "start": "1862500",
    "end": "1870090"
  },
  {
    "text": "So otherwise, KnowBERT has a\nlot of similarities to Ernie. It uses a fusion layer\nthat combines this context",
    "start": "1870090",
    "end": "1875100"
  },
  {
    "text": "and entity information. And it introduces some\nknowledge pretraining task. So I'd say the high\nlevel takeaways",
    "start": "1875100",
    "end": "1881040"
  },
  {
    "text": "if you want to use pretrained\nentity embeddings in a language model, you probably\nat least want to consider both\nof these components",
    "start": "1881040",
    "end": "1887070"
  },
  {
    "text": "in terms of actually going to\nintegrate the pretrained entity embeddings and take the most\nadvantage of the knowledge",
    "start": "1887070",
    "end": "1892582"
  },
  {
    "text": "in them as possible.  So that brings us to the next\nclass of techniques which",
    "start": "1892582",
    "end": "1899510"
  },
  {
    "start": "1897000",
    "end": "2025000"
  },
  {
    "text": "is using an external memory. And here, we'll mainly focus\non this work called KGLM.",
    "start": "1899510",
    "end": "1905410"
  },
  {
    "text": "And then we'll also\nbriefly talk about kNN-LM. So the previous methods\nI've talked about",
    "start": "1905410",
    "end": "1911330"
  },
  {
    "text": "have relied on pretrained\nentity embeddings to encode the factual\nknowledge from knowledge bases.",
    "start": "1911330",
    "end": "1916850"
  },
  {
    "text": "And the one problem\nwith this, or one of the problems with this,\nis if you want to, let's say, modify your knowledge\nbase, you now",
    "start": "1916850",
    "end": "1923030"
  },
  {
    "text": "need to retrain your\nentity embeddings and then retrain your language\nmodel on top of these entity embeddings.",
    "start": "1923030",
    "end": "1928720"
  },
  {
    "text": "So this begs the\nquestion, are there more direct ways in\npretrained entity embeddings to provide the model\nfactual knowledge?",
    "start": "1928720",
    "end": "1936598"
  },
  {
    "text": "And so what we're\ngoing to talk about is how you can actually\nuse an external memory or a key-value store\nto give the model",
    "start": "1936598",
    "end": "1942220"
  },
  {
    "text": "access to either knowledge graph\ntriples or context information. And a key thing about\nthis external memory",
    "start": "1942220",
    "end": "1948520"
  },
  {
    "text": "is that it's independent of\nthe learned model parameters. So this means you can actually\nsupport injecting and updating",
    "start": "1948520",
    "end": "1956010"
  },
  {
    "text": "factual knowledge. You can do this directly to the\nsymbolic external memory by, let's say, changing the\nvalue for a particular key",
    "start": "1956010",
    "end": "1962400"
  },
  {
    "text": "or maybe adding another key. And you don't have to\nretrain, or pretrain, your entity embeddings\nwhen you make this change.",
    "start": "1962400",
    "end": "1970690"
  },
  {
    "text": "And the next part\nwe'll talk about today can actually even have these\nupdates to the external memory without more pretraining\nof the language model.",
    "start": "1970690",
    "end": "1978860"
  },
  {
    "text": "So that's pretty neat. And another benefit of\nusing external memory over these pretrained\nentity embedding approaches",
    "start": "1978860",
    "end": "1985760"
  },
  {
    "text": "is it can also be\nmore interpretable. So if you have a bug, or not\na bug, an air in your model,",
    "start": "1985760",
    "end": "1991730"
  },
  {
    "text": "where it's not\npredicting a fact fact, it's very challenging to figure\nout with pretrained embeddings",
    "start": "1991730",
    "end": "1997880"
  },
  {
    "text": "what the problem might be. Was it the original\nknowledge base? Was it the encoding in\nthe entity embeddings? Is it how the language model\nis using the entity embeddings?",
    "start": "1997880",
    "end": "2005200"
  },
  {
    "text": "And here you have a\nlittle more information with an external\nmemory, and that you can look in the external\nmemory and see what's",
    "start": "2005200",
    "end": "2010870"
  },
  {
    "text": "a fact in the external memory\nwith a not in external memory and so on. So it adds a little bit\nmore interpretabilability",
    "start": "2010870",
    "end": "2018309"
  },
  {
    "text": "than just using these\npretrained entity embeddings as an indirect way\nto encode the knowledge base. ",
    "start": "2018310",
    "end": "2025780"
  },
  {
    "start": "2025000",
    "end": "2270000"
  },
  {
    "text": "So first, what we're going\nto talk about is called KGLM. And unlike the other approaches\nwe've talked about so far,",
    "start": "2025780",
    "end": "2031450"
  },
  {
    "text": "this actually uses LSTMs\nand not transformers. So the key idea here is to\ncondition the language model",
    "start": "2031450",
    "end": "2038730"
  },
  {
    "text": "on a knowledge graph. So recall with the\nstandard language model, we want to predict the next\nword, given the previous words",
    "start": "2038730",
    "end": "2045030"
  },
  {
    "text": "in the sequence. Well, now we also want to\npredict the next entity, given the previous words\nin the sequence,",
    "start": "2045030",
    "end": "2051320"
  },
  {
    "text": "and given the previous\nentities in the sentence, or the entities that are\nrelevant to the sentence,",
    "start": "2051320",
    "end": "2056510"
  },
  {
    "text": "I should say. So KGLM will be building\na local knowledge graph",
    "start": "2056510",
    "end": "2062110"
  },
  {
    "text": "as it iterates\nover the sequence. And a local knowledge\ngraph is just a subset of a full\nknowledge graph that",
    "start": "2062110",
    "end": "2067475"
  },
  {
    "text": "only has the entities\nthat are actually relevant to the sequence. So if we have this example\nhere, a simplified example",
    "start": "2067475",
    "end": "2075129"
  },
  {
    "text": "from the paper, that\nSuper Mario Land is a game developed by blank. And Super Mario Land\nhere is an entity.",
    "start": "2075130",
    "end": "2082873"
  },
  {
    "text": "You'd want a local\nknowledge graph as follows, where you see\nthat Super Mario Land is in the local knowledge graph.",
    "start": "2082873",
    "end": "2088310"
  },
  {
    "text": "But we also have the\nrelation to Super Mario Land to other entities that are\ncopied from the full knowledge",
    "start": "2088310",
    "end": "2093350"
  },
  {
    "text": "graph into this local\nknowledge graph. And you would build up\nthis local knowledge graph as you iterate\nover the sentence.",
    "start": "2093350",
    "end": "2099400"
  },
  {
    "text": "So whenever you\nsee an entity, you would add it to the local\nknowledge graph as well as its relations to other entities.",
    "start": "2099400",
    "end": "2106430"
  },
  {
    "text": "So obviously, this is\na much smaller example than when we would really\nhave all the relations to Super Mario Land, just for\nthe purpose of the example.",
    "start": "2106430",
    "end": "2113505"
  },
  {
    "text": "But hopefully, it's\nclear that all of these are relevant to the sequence. ",
    "start": "2113505",
    "end": "2119452"
  },
  {
    "text": "Something important\nto note here is that this does assume that\nthe entities are known during training so that you do\nhave this entity annotated data",
    "start": "2119452",
    "end": "2126280"
  },
  {
    "text": "for training. And therefore, your\nlocal knowledge graph is always the ground truth\nlocal knowledge graph as you iterate\nover the sequence.",
    "start": "2126280",
    "end": "2133690"
  },
  {
    "text": "So why might this be a\ngood idea to do this? Well, here, the next word we\nwant to predict is Nintendo.",
    "start": "2133690",
    "end": "2139327"
  },
  {
    "text": "And you may notice that Nintendo\nisn't in your local knowledge graph. So sometimes, this\nlocal knowledge graph",
    "start": "2139327",
    "end": "2144520"
  },
  {
    "text": "can actually serve as a very\nstrong signal for what you want to predict for your next word.",
    "start": "2144520",
    "end": "2150105"
  },
  {
    "text": "Now you may be thinking,\nwell, this wouldn't always be helpful, and\nthat's true as well.",
    "start": "2150105",
    "end": "2155380"
  },
  {
    "text": "So if you look at just the\nthird word in the sequence, and you want to\npredict that word, so is a game, for\ninstance, well, if this",
    "start": "2155380",
    "end": "2162652"
  },
  {
    "text": "isn't in the local\nknowledge graph, this wouldn't be\nnecessarily that helpful. You will just do a standard\nlanguage model prediction.",
    "start": "2162652",
    "end": "2170130"
  },
  {
    "text": "Or if you at the\nbeginning of a sequence, your local knowledge\ngraph is empty, so of course, you're not going\nto get any signal from it.",
    "start": "2170130",
    "end": "2176850"
  },
  {
    "text": "So the first question they ask\nin KGLM is how can a language model know when to use\na local knowledge graph",
    "start": "2176850",
    "end": "2182570"
  },
  {
    "text": "and when it might actually\nbe useful for predicting the next word? ",
    "start": "2182570",
    "end": "2189207"
  },
  {
    "text": "So we're going to keep the same\nexample as a running example. And we have our local\nknowledge graph here. We now have an LSTM that looks\nsimilar to the representations",
    "start": "2189208",
    "end": "2196720"
  },
  {
    "text": "you've seen\nthroughout this class. And normally, you've seen the\nLSTM predicts the next word.",
    "start": "2196720",
    "end": "2202150"
  },
  {
    "text": "Well, now we're also going\nto use the LSTM to predict the next type of the word. So is the next word going\nto be a related entity,",
    "start": "2202150",
    "end": "2209320"
  },
  {
    "text": "meaning it's in the local\nknowledge graph already? Is it going to be a new\nentity, meaning it's not",
    "start": "2209320",
    "end": "2214360"
  },
  {
    "text": "in the local knowledge graph? Or is it going to be not an\nentity, in which case, you just revert to a normal\nLSTM prediction.",
    "start": "2214360",
    "end": "2221840"
  },
  {
    "text": "And they're going to use\nthe LSTM hidden state to do this prediction of the type of\nthe next word over this three",
    "start": "2221840",
    "end": "2226934"
  },
  {
    "text": "way-- three different\nclasses that they might want to consider. So in the case of\nSuper Mario Land",
    "start": "2226935",
    "end": "2233200"
  },
  {
    "text": "is a game developed\nby Nintendo, we saw that this would\nbe a related entity case because you saw\nthat Nintendo was",
    "start": "2233200",
    "end": "2239260"
  },
  {
    "text": "in the local knowledge graph. But in the other\ncases, Super Mario Land would be a new entity case\nsince the local knowledge",
    "start": "2239260",
    "end": "2246069"
  },
  {
    "text": "graph is empty at that point. And then any of the\nwords between Super Mario Land and Nintendo\nwould be non-entity",
    "start": "2246070",
    "end": "2253030"
  },
  {
    "text": "as they're just a standard LSTM\nlanguage model prediction that doesn't involve any entities.",
    "start": "2253030",
    "end": "2260210"
  },
  {
    "text": "So now we need to talk\nabout what the language model actually does in these\nthree different scenarios to predict the next\nentity and the next word.",
    "start": "2260210",
    "end": "2267530"
  },
  {
    "text": " So we're going to keep the\nexample up at the top in case",
    "start": "2267530",
    "end": "2272960"
  },
  {
    "start": "2270000",
    "end": "2354000"
  },
  {
    "text": "you want to go back to\nthe three different cases. And we're going to start\nwith a related entity case.",
    "start": "2272960",
    "end": "2279130"
  },
  {
    "text": "So here we assume that the\nnext word or entity is actually in your local knowledge graph.",
    "start": "2279130",
    "end": "2284380"
  },
  {
    "text": "And remember that we can\ndescribe a knowledge graph in terms of triples,\nso in terms of pairs of parent entities,\nrelations, and tail entities.",
    "start": "2284380",
    "end": "2292320"
  },
  {
    "text": "And in the case of predicting\nthe next word as Nintendo, there's only one\npossible parent entity",
    "start": "2292320",
    "end": "2297690"
  },
  {
    "text": "in the local knowledge graph,\nwhich is Super Mario Land. And the goal is you\nwant to figure out",
    "start": "2297690",
    "end": "2302720"
  },
  {
    "text": "what is the most relevant triple\nthat will be useful in helping to predict the next word.",
    "start": "2302720",
    "end": "2308220"
  },
  {
    "text": "So in this case, you could\nhave the triple Super Mario Land, publisher, Nintendo. You might have the triple\nSuper Mario Land, genre,",
    "start": "2308220",
    "end": "2314390"
  },
  {
    "text": "platform game, which\nof these is actually helpful in predicting\nthat Nintendo should be the next word.",
    "start": "2314390",
    "end": "2320780"
  },
  {
    "text": "So here what you\nwould want KGLM to do is predict that the\ntop scoring parent entity is Super Mario Land.",
    "start": "2320780",
    "end": "2326829"
  },
  {
    "text": "And the top scoring\nrelation is publisher. And you can see\nthere are actually contextual cues\nin a sentence that",
    "start": "2326830",
    "end": "2332410"
  },
  {
    "text": "could help you figure out which\ntriple you're talking about. And then, given that\nyour top scoring",
    "start": "2332410",
    "end": "2338050"
  },
  {
    "text": "parent entity is Super Mario\nLand, and your top scoring relation is publisher,\nyou can figure out that using knowledge graph\ntriples, the tail entity",
    "start": "2338050",
    "end": "2345789"
  },
  {
    "text": "he has to be Nintendo. And therefore, this\ngives you a strong signal that the next word\nwill be Nintendo.",
    "start": "2345790",
    "end": "2350815"
  },
  {
    "text": " So the goal is you're going\nto find the top scoring",
    "start": "2350815",
    "end": "2356680"
  },
  {
    "text": "parent entity and the\ntop scoring relation using the nodes in your\nlocal knowledge graph. And you can do this by using\nthe LSTM hidden state combined",
    "start": "2356680",
    "end": "2364150"
  },
  {
    "text": "with pretrained entity\nand relation embeddings. So I do admit I cheated\nhere a little bit, and that this does use\npretrained embeddings.",
    "start": "2364150",
    "end": "2370990"
  },
  {
    "text": "But hopefully, you'll see by\nthe end of this discussion why I think it fits a bit better\nin this external memory use case as well.",
    "start": "2370990",
    "end": "2378319"
  },
  {
    "text": "So what I'm going\nto do here is take a softmax using LSTM hidden\nstate and the entity embeddings",
    "start": "2378320",
    "end": "2383390"
  },
  {
    "text": "for each of the potential\nparent entities. And they will take the top\nscoring one as a parent entity.",
    "start": "2383390",
    "end": "2388460"
  },
  {
    "text": "And they'll do the same thing\nfor the relation embeddings. The next entity is then\njust this tail entity",
    "start": "2388460",
    "end": "2394700"
  },
  {
    "text": "from the knowledge graph triple. So it's relatively\ntrivial to figure out what the next entity\nshould be once you've",
    "start": "2394700",
    "end": "2400010"
  },
  {
    "text": "figured out the top\nscoring parent entity and your top scoring relation. And then finally, to\npredict the next word,",
    "start": "2400010",
    "end": "2407450"
  },
  {
    "text": "they take the\nvocabulary, and they expand it to include\ndifferent aliases that could refer to that entity.",
    "start": "2407450",
    "end": "2413950"
  },
  {
    "text": "So what I mean by\naliases here are phrases that could refer\nto the entity and text. So you might not just\ncall it Nintendo.",
    "start": "2413950",
    "end": "2420690"
  },
  {
    "text": "You might also say\nNintendo company or Koppai. And you want any of these\nto be possible words they",
    "start": "2420690",
    "end": "2426090"
  },
  {
    "text": "could predict as the next word. So the goal of this\nvocabulary expansion",
    "start": "2426090",
    "end": "2431200"
  },
  {
    "text": "is to increase the probability\nthat the next where you predict will actually be related\nto this next entity",
    "start": "2431200",
    "end": "2440100"
  },
  {
    "text": "So a new entity case\nis a bit simpler. This means that the entity\nthat you're predicting is not in the local\nknowledge graph.",
    "start": "2440100",
    "end": "2445540"
  },
  {
    "text": "So you're not getting any signal\nfrom this local knowledge graph that you've been building\nup, and all you want to do",
    "start": "2445540",
    "end": "2450930"
  },
  {
    "text": "is find the top scoring entity\nin the full knowledge graph. And you can do this using\nthe LSTM hidden state",
    "start": "2450930",
    "end": "2456089"
  },
  {
    "text": "and pretrained\nentity embeddings, similar to how we found the\nscore for the top parent entity.",
    "start": "2456090",
    "end": "2461920"
  },
  {
    "text": "Your next entity will just\nbe the top scoring entity out of the full knowledge graph. And then your next\nword is once again",
    "start": "2461920",
    "end": "2467950"
  },
  {
    "text": "this vocabulary expanded to\ninclude aliases of that entity.",
    "start": "2467950",
    "end": "2473180"
  },
  {
    "text": "The not an entity\ncase is as simple as you just revert\nto normal LSTM. You don't have a next\nentity to predict.",
    "start": "2473180",
    "end": "2479970"
  },
  {
    "text": "And your next word is just\nthe most likely next token of your normal vocabulary. ",
    "start": "2479970",
    "end": "2486980"
  },
  {
    "start": "2486000",
    "end": "2539000"
  },
  {
    "text": "So here's a diagram\nfrom their paper that hopefully summarizes and\nmakes even clear what I just",
    "start": "2486980",
    "end": "2491990"
  },
  {
    "text": "went over. So they have a longer\nexample than the one we are looking at but\nthe same prediction",
    "start": "2491990",
    "end": "2497270"
  },
  {
    "text": "as Nintendo is the next word. And they have their\npredictions in red. So this is what they\nwant KGLM to predict.",
    "start": "2497270",
    "end": "2503089"
  },
  {
    "text": "The three different cases\nare in the horizontals. And we see that here, you're\nin the related entity case",
    "start": "2503090",
    "end": "2508910"
  },
  {
    "text": "since Nintendo is in your\nlocal knowledge graph. So they want KGLM to\npredict that Nintendo should",
    "start": "2508910",
    "end": "2515130"
  },
  {
    "text": "be a related entity\ntype of word, that Super Mario Land\nshould be its parent entity, that publisher should\nbe the relevant relation.",
    "start": "2515130",
    "end": "2523200"
  },
  {
    "text": "And as a result, the\nnext entity is Nintendo. And then they expand\ntheir vocabulary. You see the aliases of\nNintendo at the bottom.",
    "start": "2523200",
    "end": "2530664"
  },
  {
    "text": "And then finally,\nthey actually predict Nintendo is the next word. And the other cases\njust summarize",
    "start": "2530665",
    "end": "2535970"
  },
  {
    "text": "what we also already went over.  So you find that KGLM actually\noutperforms GPT-2 and AWD-LSTM,",
    "start": "2535970",
    "end": "2545369"
  },
  {
    "text": "which is a strong LSTM language\nmodel on a fast completion task similar to the fill\nin the blank examples",
    "start": "2545370",
    "end": "2551280"
  },
  {
    "text": "that we looked at the\nbeginning of the talk. They also find qualitatively\nthat compared to GPT-2,",
    "start": "2551280",
    "end": "2557570"
  },
  {
    "text": "KGLM tends to predict more\nspecific tokens since it can predict these\ntokens from just copying",
    "start": "2557570",
    "end": "2563210"
  },
  {
    "text": "from the local knowledge\ngraph, whereas GPT-2 will tend to predict more generic tokens. So if you want to predict\nthe birthplace of someone,",
    "start": "2563210",
    "end": "2569960"
  },
  {
    "text": "GPT-2 is more likely to\npredict New York, for example, and KGLM might predict\nsome obscure place.",
    "start": "2569960",
    "end": "2576523"
  },
  {
    "text": "And then they have\nthese really cool set of experiments where they showed\nthat KGLM actually supports modifying or updating facts.",
    "start": "2576523",
    "end": "2583703"
  },
  {
    "text": "So they made a direct change\nin the knowledge graph, and then they saw what is the\nchange in KGLM's predictions.",
    "start": "2583703",
    "end": "2590200"
  },
  {
    "text": "So they have this example\nwhere the sequence was, Barack Obama was born on blank.",
    "start": "2590200",
    "end": "2595507"
  },
  {
    "text": "They had their knowledge\ngraph triple as Barack Obama's original birth date, and then\nthe most likely next tokens,",
    "start": "2595508",
    "end": "2601010"
  },
  {
    "text": "whereas expected,\nAugust 4th 1961. And then they just changed\ntheir knowledge graph. So they changed the\nbirthday of Obama.",
    "start": "2601010",
    "end": "2608180"
  },
  {
    "text": "They said, OK,\nhe's now born 2013. And they looked to see what the\nnext predictions were for KGLM,",
    "start": "2608180",
    "end": "2614250"
  },
  {
    "text": "and it changed its\npredictions to match what was in the local knowledge graph. So this is something\nthat's pretty cool,",
    "start": "2614250",
    "end": "2619760"
  },
  {
    "text": "and that really only external\nmemory approaches can do compared to the original\npretrained entity embedding",
    "start": "2619760",
    "end": "2625910"
  },
  {
    "text": "approaches we talked about. And I think it's\none of the reasons that KGLM, at least my\nopinion, fits better",
    "start": "2625910",
    "end": "2631111"
  },
  {
    "text": "in these external\nmemory use cases. ",
    "start": "2631112",
    "end": "2636200"
  },
  {
    "text": "Right. So the next slide\nis a different paper so I guess I'll take questions\nabout KGLM, if there are any.",
    "start": "2636200",
    "end": "2642340"
  },
  {
    "start": "2642340",
    "end": "2647360"
  },
  {
    "text": "It's a pretty complex method. So feel free to have questions. Yeah. Can you, one more\ntime, explain what",
    "start": "2647360",
    "end": "2653630"
  },
  {
    "text": "the definition of the\nlocal knowledge graph is in relationship to the\nglobal knowledge graph? Yep.",
    "start": "2653630",
    "end": "2660410"
  },
  {
    "start": "2660000",
    "end": "2830000"
  },
  {
    "text": "So local knowledge\ngraph is supposed to be a subset of the\nfull knowledge graph. And it's only\nsupposed to consist",
    "start": "2660410",
    "end": "2666170"
  },
  {
    "text": "of entities that\nhave actually been seen in the sequence as well\nas their relevant entities.",
    "start": "2666170",
    "end": "2675319"
  },
  {
    "text": "OK.  All right. So here, you see\nthat Super Mario Land",
    "start": "2675320",
    "end": "2681335"
  },
  {
    "text": "is in the local knowledge\ngraph because Super Mario Land is an entity that\nis seen in the sequence. And then you also want to\ncopy over all the edges",
    "start": "2681335",
    "end": "2688740"
  },
  {
    "text": "from Super Mario Land that would\nbe in the full knowledge graph. So this is just a subset of them\nfor the purpose of the example.",
    "start": "2688740",
    "end": "2695110"
  },
  {
    "text": "But you see that\nSuper Mario Land has an edge to Nintendo to\nGame Boy to platform game. And so you would copy all\nedges that Super Mario",
    "start": "2695110",
    "end": "2701160"
  },
  {
    "text": "Land has to another node in\nthe full knowledge graph. And they know in advance like,\nthey have the labels here",
    "start": "2701160",
    "end": "2707340"
  },
  {
    "text": "for what the entities\nare during training. So that's how they can actually\ncreate this ground truth knowledge graph.",
    "start": "2707340",
    "end": "2714240"
  },
  {
    "text": "Then briefly, a student\nasked why we can't just use the whole knowledge graph?",
    "start": "2714240",
    "end": "2719470"
  },
  {
    "text": "And I gave an answer but\nmaybe you know better. Yeah, I think the\nidea is the signal",
    "start": "2719470",
    "end": "2725140"
  },
  {
    "text": "be much stronger if you just\nuse local knowledge graph. So in the softmax for\nthe related entity case,",
    "start": "2725140",
    "end": "2732340"
  },
  {
    "text": "you would just be predicting\nover the potential parent entities in your local\nknowledge graph, which",
    "start": "2732340",
    "end": "2737710"
  },
  {
    "text": "is a much smaller set than\nwhat's in your full knowledge graph. So I guess it's more\nlikely that you're",
    "start": "2737710",
    "end": "2743043"
  },
  {
    "text": "going to predict\nsomething that is correct in that case than\nwhen you have 5 million or so entities in your\nfull knowledge graph.",
    "start": "2743043",
    "end": "2749020"
  },
  {
    "text": "It's also much\ncheaper to compute. In this case, there's only\na single parent entity, but you could have\nmultiple parent entities",
    "start": "2749020",
    "end": "2754809"
  },
  {
    "text": "that you're trying to compute\nwhich ones the most likely over. Is that what you were\nalso thinking, John?",
    "start": "2754810",
    "end": "2761880"
  },
  {
    "text": "Yeah. I mainly just said\nthe efficiency. So the signal thing is cool too.",
    "start": "2761880",
    "end": "2767900"
  },
  {
    "text": "Here's an exciting question. What about queries that\nrequire more than one step",
    "start": "2767900",
    "end": "2773930"
  },
  {
    "text": "in the knowledge graph such as\nthe location of the publisher of Super Mario Land?",
    "start": "2773930",
    "end": "2779720"
  },
  {
    "text": " Yeah. That's a good question.",
    "start": "2779720",
    "end": "2785500"
  },
  {
    "text": "So the idea is it can\nsupport those types? Like, does it support\nmulti-hop, kind of,",
    "start": "2785500",
    "end": "2790740"
  },
  {
    "text": "building of the knowledge graph? Yeah. How does KGLM perform\nin those cases?",
    "start": "2790740",
    "end": "2796590"
  },
  {
    "text": "Yeah. I don't know. That's a very good question. They built up the\nknowledge graph so that is just a single\nhop as far as I know.",
    "start": "2796590",
    "end": "2802910"
  },
  {
    "text": "But if you saw the\nother entities, if you were to see the\nentities along the hops, that it would have them in the\nlocal knowledge graph, yeah,",
    "start": "2802910",
    "end": "2810572"
  },
  {
    "text": "that's a good question. I don't if they support that. ",
    "start": "2810572",
    "end": "2820560"
  },
  {
    "text": "Great. OK. Let's move along then. ",
    "start": "2820560",
    "end": "2830307"
  },
  {
    "start": "2830000",
    "end": "3144000"
  },
  {
    "text": "So the next piece of\nwe're going to talk about, you guys, have actually briefly\nseen in the language generation",
    "start": "2830307",
    "end": "2836089"
  },
  {
    "text": "lecture. But I'm going to go over\nit again quickly here. So unlike the other works\nthat we talked about that use",
    "start": "2836090",
    "end": "2842090"
  },
  {
    "text": "knowledge graph triples,\nthis is actually going to take a looser notion of\nknowledge in that the knowledge",
    "start": "2842090",
    "end": "2847460"
  },
  {
    "text": "will just be encoded in the\ntext in the training data set. So this is called kNN-LM.",
    "start": "2847460",
    "end": "2852800"
  },
  {
    "text": "And the idea is that-- or it's brilliant idea\nis language models not only learn to predict\nthe next word in text,",
    "start": "2852800",
    "end": "2858740"
  },
  {
    "text": "but they also learn these\nrepresentations of text. And the author suggests that\nit might actually be easier",
    "start": "2858740",
    "end": "2864650"
  },
  {
    "text": "to learn similarities\nbetween text sequences than it is to predict the\nnext word in the text.",
    "start": "2864650",
    "end": "2870680"
  },
  {
    "text": "So you have this example that\nDickens is the author of blank, and Dickens wrote blank. And they argue that it's\neasier to tell for a human",
    "start": "2870680",
    "end": "2877819"
  },
  {
    "text": "but also for a model that\nthese sequences are similar, and they should probably\nhave the same next word",
    "start": "2877820",
    "end": "2883250"
  },
  {
    "text": "even if you don't know\nwhat the next word is. So that's suggesting\nthat it's easier to learn these similarities\nthan it is to actually predict",
    "start": "2883250",
    "end": "2890420"
  },
  {
    "text": "the next word. And they argue that this is\neven more true for long tail patterns, where it's very\nchallenging for the model",
    "start": "2890420",
    "end": "2896579"
  },
  {
    "text": "to predict that the next word\nis some rarely seen token or rare entity, than it is to\nfind another similar sequence",
    "start": "2896580",
    "end": "2903130"
  },
  {
    "text": "that it's already seen and\njust copy the next word from that sequence.",
    "start": "2903130",
    "end": "2908490"
  },
  {
    "text": "So what they proposed to do\nis store all representations of text sequences in the\nnearest neighbor datastore.",
    "start": "2908490",
    "end": "2913800"
  },
  {
    "text": "And then an inference,\nwhat you want to do is you find the k most\nsimilar sequences of text,",
    "start": "2913800",
    "end": "2919170"
  },
  {
    "text": "you then retrieve their\ncorresponding values, so you just peek\nat the sequences and see what were\ntheir next words.",
    "start": "2919170",
    "end": "2924990"
  },
  {
    "text": "And then you combine\nthe probability from this nearest neighbor\ndatastore with just a typical language\nmodel prediction.",
    "start": "2924990",
    "end": "2931898"
  },
  {
    "text": "And so they call this\nan interpolation step in that they're weighting\nhow much to pay attention to the probability\nfrom this kNN approach",
    "start": "2931898",
    "end": "2938730"
  },
  {
    "text": "and how much to pay attention\nto this language model approach. And the lambda here is just\na hyperparameter they tune.",
    "start": "2938730",
    "end": "2945120"
  },
  {
    "text": " So I have this diagram\nfrom their paper where they want to predict\nthe next word in the sequence,",
    "start": "2945120",
    "end": "2952020"
  },
  {
    "text": "Shakespeare's play blank. So what they do is they have\nall the training context already encoded in their datastore.",
    "start": "2952020",
    "end": "2958710"
  },
  {
    "text": "So they have representations\nof all of the training context. And then they compute\nthe representation of the text context.",
    "start": "2958710",
    "end": "2964500"
  },
  {
    "text": "And they want to figure\nout which representations in the training context are most\nsimilar to this task context",
    "start": "2964500",
    "end": "2970319"
  },
  {
    "text": "representation. And so here, an external\nmemory view of things, the keys",
    "start": "2970320",
    "end": "2976370"
  },
  {
    "text": "would be the representations\nof the training context, and the values would\nbe the next words.",
    "start": "2976370",
    "end": "2982640"
  },
  {
    "text": "So to get the k nearest\ntraining representations, they then copy\nover their values.",
    "start": "2982640",
    "end": "2987980"
  },
  {
    "text": "That's what you see with\nthis Macbeth, Hamlet, Macbeth example. They have a normalization\nstep where they convert this",
    "start": "2987980",
    "end": "2993590"
  },
  {
    "text": "to probability space. And then finally, they\nhave an aggregation step. So if a word is seen as\nthe next word in several",
    "start": "2993590",
    "end": "3001300"
  },
  {
    "text": "of these k nearest\nneighbors, then they want to count more for that. So that's why they aggregate. So if they see Macbeth twice,\nit means Macbeth is more likely.",
    "start": "3001300",
    "end": "3010090"
  },
  {
    "text": "And then finally, they have\nthis interpolation step where they try to balance\nbetween the classification",
    "start": "3010090",
    "end": "3015100"
  },
  {
    "text": "probabilities from\nthe language model and from the kNN-LM approach.",
    "start": "3015100",
    "end": "3020920"
  },
  {
    "text": "So some immediate observation\nyou might have is this seems really expensive. They do propose ways\nto try to minimize",
    "start": "3020920",
    "end": "3028807"
  },
  {
    "text": "the expense of actually having\nto store all the training contexts in this datastore\nbecause they actually store it",
    "start": "3028807",
    "end": "3033990"
  },
  {
    "text": "for every single window of next\nword in the training context. And you can do quantization\non some nearest neighbor",
    "start": "3033990",
    "end": "3040680"
  },
  {
    "text": "approaches to try to\nmake this less expensive. But I imagine this would still\nbe pretty expensive for really",
    "start": "3040680",
    "end": "3046350"
  },
  {
    "text": "large training data sets. They also have some\ncool experiments that show that this is very\ngood for domain adaptation.",
    "start": "3046350",
    "end": "3052970"
  },
  {
    "text": "So if you take your\nlanguage model, and you have a new domain that\nyou want a higher language model too, you could just create\na nearest neighbor datastore",
    "start": "3052970",
    "end": "3060850"
  },
  {
    "text": "of your new domain. So you encode all\nthe representations of that new domain. You stick it in a datastore.",
    "start": "3060850",
    "end": "3067150"
  },
  {
    "text": "And then, you can just\nuse your language model with these kNN probabilities\nas well just immediately",
    "start": "3067150",
    "end": "3073360"
  },
  {
    "text": "on this new domain\nwithout actually having to further train\nyour language model. So I thought that\nwas a pretty cool use",
    "start": "3073360",
    "end": "3080710"
  },
  {
    "text": "case of this external\nmemory approach. So while it doesn't leverage\nknowledge bases directly,",
    "start": "3080710",
    "end": "3086230"
  },
  {
    "text": "it does have this\nloose knowledge of-- or a loose idea of\nencoding knowledge that is in a textual\nrepresentation",
    "start": "3086230",
    "end": "3092230"
  },
  {
    "text": "form into some external\nmemory that the model can then take advantage of. ",
    "start": "3092230",
    "end": "3099740"
  },
  {
    "text": "That's all I have\nfor this approach. Are there any questions\non this approach? ",
    "start": "3099740",
    "end": "3109970"
  },
  {
    "text": "Well, suddenly, one\nperson is asking, how does the kNN make\npredictions for the next word?",
    "start": "3109970",
    "end": "3116540"
  },
  {
    "text": "The k neighbors\nare for the context instead of the next word. Oh, OK. That wasn't clear.",
    "start": "3116540",
    "end": "3122490"
  },
  {
    "text": "So the keys are the\nrepresentations of the context. The values in your external\nmemory are the next words.",
    "start": "3122490",
    "end": "3128900"
  },
  {
    "text": "So when you figure out,\nyou figure out your nearest neighbors using your\nkeys, and then you copy over their values.",
    "start": "3128900",
    "end": "3134400"
  },
  {
    "text": "So it does actually know what\nthe next words are for each of those representations. ",
    "start": "3134400",
    "end": "3142550"
  },
  {
    "text": "Yeah OK so finally, we're going\nto talk about how you can just",
    "start": "3142550",
    "end": "3147590"
  },
  {
    "text": "modify the training\ndata to better encode knowledge in language models. So the approaches we've\ntalked about so far",
    "start": "3147590",
    "end": "3153690"
  },
  {
    "start": "3151000",
    "end": "3197000"
  },
  {
    "text": "are actually incorporating\nknowledge explicitly by using the\npretrained embeddings or an external memory.",
    "start": "3153690",
    "end": "3160738"
  },
  {
    "text": "We also want to talk\nabout how can you just incorporate knowledge implicitly\nthrough the unstructured text?",
    "start": "3160738",
    "end": "3167855"
  },
  {
    "text": "So what we're going to do\nis either mask or corrupt the data to introduce\nadditional training tasks that require factual knowledge\nto figure out what",
    "start": "3167855",
    "end": "3175559"
  },
  {
    "text": "data was masked, for instance. So this has some\nclear advantages. It doesn't have any additional\nmemory or computation",
    "start": "3175560",
    "end": "3181650"
  },
  {
    "text": "requirements, you\ndon't have a datastore to deal with, you don't\nhave extra knowledge encoded layers to train.",
    "start": "3181650",
    "end": "3186810"
  },
  {
    "text": "All you do is modify\nthe training data. And you don't have to modify\nyour architecture either so you can continue using\nyour favorite BERT model",
    "start": "3186810",
    "end": "3194340"
  },
  {
    "text": "and just make these changes\nto the training data. So the first work\nwe're going to look at",
    "start": "3194340",
    "end": "3199560"
  },
  {
    "start": "3197000",
    "end": "3276000"
  },
  {
    "text": "is called WKLM,\nWeakly Supervised Knowledge-Pretraining Language\nModel, or Pretrained Language Model.",
    "start": "3199560",
    "end": "3205480"
  },
  {
    "text": "And the key idea here\nis to train the model to distinguish between\ntrue and false knowledge.",
    "start": "3205480",
    "end": "3211130"
  },
  {
    "text": "So they're going to corrupt\nthe data by replacing mentions in the text with\nmentions that refer to different entities\nof the same type",
    "start": "3211130",
    "end": "3217819"
  },
  {
    "text": "to create what they refer to as\nnegative knowledge statements. And then the model\nwill just predict",
    "start": "3217820",
    "end": "3223360"
  },
  {
    "text": "has the entity been\nreplaced or corrupted. This type constraint is\nnecessary to make sure that--",
    "start": "3223360",
    "end": "3230390"
  },
  {
    "text": "or to encourage the model to\nactually use factual knowledge to figure out that this\ncorruption is taking place. So you can imagine\nif you replace it",
    "start": "3230390",
    "end": "3236900"
  },
  {
    "text": "with something that's\nnot realistic at all, the model could just be basing\nits prediction based on, is this sentence\nlinguistically correct?",
    "start": "3236900",
    "end": "3244530"
  },
  {
    "text": "So as an example, we have\na true knowledge statement as JK Rowling is the\nauthor of Harry Potter.",
    "start": "3244530",
    "end": "3251770"
  },
  {
    "text": "And then we want to\nmodify this to replace it with another author. So let's say we change\nthis to J.R.R. Tolkien",
    "start": "3251770",
    "end": "3257319"
  },
  {
    "text": "is the author of Harry Potter. So you can see\nthat this requires some amount of knowledge,\nbackground knowledge,",
    "start": "3257320",
    "end": "3263545"
  },
  {
    "text": "to actually be able to figure\nout which statement is true and which statement is false. And the idea is\nthat the model will",
    "start": "3263545",
    "end": "3268620"
  },
  {
    "text": "be able to predict for\neach of these mentions whether it's a true\nor false mention. ",
    "start": "3268620",
    "end": "3276720"
  },
  {
    "text": "So this diagram here\nis from the paper. And hopefully, it explains\nthis a bit better. They have the original\narticle on the left.",
    "start": "3276720",
    "end": "3282090"
  },
  {
    "text": "And then they have\nthe replaced article with the corruptions\non the right, and the entities are in blue.",
    "start": "3282090",
    "end": "3287470"
  },
  {
    "text": "So what they do is\nfor a given entity, they first look up its type. They find other\nentities of that type,",
    "start": "3287470",
    "end": "3293610"
  },
  {
    "text": "and then they randomly\nsample the entity and get an alias of it\nto replace in the text.",
    "start": "3293610",
    "end": "3299015"
  },
  {
    "text": "So they're going to replace\nStan Lee, for instance, with Bryan Johnson, and\nMarvel Comics with DC Comics.",
    "start": "3299015",
    "end": "3304710"
  },
  {
    "text": "And the replacements\nare in red on the right. And then the idea\nis that the model",
    "start": "3304710",
    "end": "3309890"
  },
  {
    "text": "will be able to predict\nfor each of these mentions, was it replaced or not. So in the case of\nBryan Johnson, they",
    "start": "3309890",
    "end": "3315350"
  },
  {
    "text": "have the red X for this\nis a false mention. And in the case of\nthe true mention, they have the checkmark.",
    "start": "3315350",
    "end": "3322260"
  },
  {
    "text": "So it's a pretty\nsimple approach. But they actually\nshow that it can help a model increase the\namount of knowledge that's",
    "start": "3322260",
    "end": "3328560"
  },
  {
    "text": "encoded as parameters. ",
    "start": "3328560",
    "end": "3336230"
  },
  {
    "text": "So WKLM uses an entity\nreplacement loss to train the model\nto distinguish between these true\nand false mentions.",
    "start": "3336230",
    "end": "3342510"
  },
  {
    "text": "And this just looks like a\nbinary classification loss where your true mentions\nare on the left, and your false mentions\nare on the right.",
    "start": "3342510",
    "end": "3349310"
  },
  {
    "text": "And you want to\nincrease the probability that this P of e given C, so\nthe probability of entity given",
    "start": "3349310",
    "end": "3355130"
  },
  {
    "text": "the context, you\nwant to increase that for the true mentions\nand decrease it for the false mentions.",
    "start": "3355130",
    "end": "3361450"
  },
  {
    "text": "The total loss is then\njust a combination of the masked language\nmodel loss and this entity replacement loss.",
    "start": "3361450",
    "end": "3368050"
  },
  {
    "text": "The masked language loss-- the masked language model loss\nis defined at the token level. And the entity replacement loss\nis defined at the entity level,",
    "start": "3368050",
    "end": "3376690"
  },
  {
    "text": "meaning it's not\njust over subwords, it's even potentially\nover words, if you have multi-word\nentities, phrases, for instance.",
    "start": "3376690",
    "end": "3385000"
  },
  {
    "text": "And this is an important\npoint, or an important theme, that we really see occurring\nthroughout these works",
    "start": "3385000",
    "end": "3390130"
  },
  {
    "text": "that we look at\nin that modifying the data at the\nentity level seems to be an important component\nof actually increasing",
    "start": "3390130",
    "end": "3396755"
  },
  {
    "text": "the amount of knowledge that\na language model can encode. ",
    "start": "3396755",
    "end": "3402779"
  },
  {
    "text": "So they find that WKLM\nimproves over BERT and GPT-2, in fact, completion\ntasks like the fill",
    "start": "3402780",
    "end": "3407910"
  },
  {
    "text": "in the blank statements that\nwe looked at the beginning. They also find that it\nimproves over the ERNIE paper",
    "start": "3407910",
    "end": "3413190"
  },
  {
    "text": "that we talked about\non a downstream task. And then they had a set\nof ablation experiments where they looked at,\ncan you just remove",
    "start": "3413190",
    "end": "3419870"
  },
  {
    "text": "this masked language\nmodel off now. And if you just train\nBERT for longer, do you really need this\nentity replacement loss?",
    "start": "3419870",
    "end": "3427850"
  },
  {
    "text": "So that's what the table\nhere is looking at. The second row is looking at if\nwe remove the masked language model loss, what happens?",
    "start": "3427850",
    "end": "3434160"
  },
  {
    "text": "We see that it\nperforms much worse without the masked\nlanguage model loss. So you really need both losses.",
    "start": "3434160",
    "end": "3439279"
  },
  {
    "text": "The intuition there was\nthe masked language model loss helps to encode just\ngeneral language understanding.",
    "start": "3439280",
    "end": "3446710"
  },
  {
    "text": "And then training\nBERT for longer performs much worse than using\nits entity replacement loss.",
    "start": "3446710",
    "end": "3451750"
  },
  {
    "text": "So that motivates even further\nthat you really do need-- or the entity replacement\nloss is actually",
    "start": "3451750",
    "end": "3457090"
  },
  {
    "text": "really helping encode more\nknowledge in these language models. ",
    "start": "3457090",
    "end": "3463280"
  },
  {
    "start": "3462000",
    "end": "3528000"
  },
  {
    "text": "So in addition to\ncorrupting the data, we're also going\nto look at, can we just mask the data differently? Can we be more clever about\nhow we do the masking?",
    "start": "3463280",
    "end": "3470710"
  },
  {
    "text": "And this is a thread in\nseveral recent works. So there's actually\nanother paper called ERNIE. So this is different than the\none I talked about before.",
    "start": "3470710",
    "end": "3477760"
  },
  {
    "text": "And this is an\nEnhanced Representation through Knowledge Integration. And what they do is\nshow improvements",
    "start": "3477760",
    "end": "3483000"
  },
  {
    "text": "on downstream Chinese\nNLP tasks by doing phrase-level and\nentity-level masking.",
    "start": "3483000",
    "end": "3488400"
  },
  {
    "text": "So instead of just\nmasking out subwords, they're going to\nmask out phrases that have multiple words.",
    "start": "3488400",
    "end": "3493440"
  },
  {
    "text": "And entities-- the full phrase\nof an entity which corresponds to-- entity and text that\nthey might find that",
    "start": "3493440",
    "end": "3499245"
  },
  {
    "text": "are like NER\ntechniques, for example. And then the second\nwork is actually",
    "start": "3499245",
    "end": "3505240"
  },
  {
    "text": "something you heard about\nin the last lecture, which is the idea of using\nsalient span masking",
    "start": "3505240",
    "end": "3510400"
  },
  {
    "text": "to mask out salient spans. And a salient span is just\na named entity or a date. So you can see this is pretty\nsimilar to what ERNIE is doing.",
    "start": "3510400",
    "end": "3517960"
  },
  {
    "text": "And they found that using\nsalient span masking actually significantly helped\nT5 performance on these closed domain\nquestion and answering tasks.",
    "start": "3517960",
    "end": "3525040"
  },
  {
    "text": " So just to make sure\nwe're all on the same page",
    "start": "3525040",
    "end": "3530287"
  },
  {
    "start": "3528000",
    "end": "3550000"
  },
  {
    "text": "with the different\nmasking techniques, this diagram from\nthe ERNIE paper is comparing to what BERT\ndoes versus what ERNIE does.",
    "start": "3530287",
    "end": "3536490"
  },
  {
    "text": "The top shows that ERNIE\nmasked up the subword tokens, or that BERT masked up the\nsubword tokens, whereas ERNIE",
    "start": "3536490",
    "end": "3542610"
  },
  {
    "text": "masked up phrases like a\nseries of, as well as entities like JK Rowling. ",
    "start": "3542610",
    "end": "3550838"
  },
  {
    "start": "3550000",
    "end": "3599000"
  },
  {
    "text": "There's some interesting results\non showing that salient span masking is helping\nencode more knowledge",
    "start": "3550838",
    "end": "3556670"
  },
  {
    "text": "in these representations. So on the left, we're\nlooking at the results of the original paper that\nproposed salient span masking.",
    "start": "3556670",
    "end": "3564710"
  },
  {
    "text": "So this is the REALM work. And the idea here was that\nthey were training a knowledge",
    "start": "3564710",
    "end": "3569870"
  },
  {
    "text": "retriever. So it's actually more\nof an external memory class of techniques,\nbut they find that by using the salient\nspan masking technique,",
    "start": "3569870",
    "end": "3578330"
  },
  {
    "text": "they could actually train a\nmuch better knowledge retriever. So that's a good example\nof how these techniques are",
    "start": "3578330",
    "end": "3584660"
  },
  {
    "text": "really complementary. So while I presented three\nclasses of techniques, you can definitely get benefits\nby doing multiple techniques",
    "start": "3584660",
    "end": "3590450"
  },
  {
    "text": "together. And they found that doing\nsalient span masking compared to using masking\nfrom BERT, which",
    "start": "3590450",
    "end": "3596369"
  },
  {
    "text": "would be the random\nuniform masks, or doing random masking spans\nfrom a paper called SpanBERT,",
    "start": "3596370",
    "end": "3603420"
  },
  {
    "text": "it performs much better to\ndo salient span masking. So you see a 38 exact match\nscore versus a 32 exact match",
    "start": "3603420",
    "end": "3611280"
  },
  {
    "text": "score, for instance. And on the right,\nwe have results from fine-tuning T5\nwith either salient span",
    "start": "3611280",
    "end": "3619410"
  },
  {
    "text": "masking or the span\ncorruption task that you saw on assignment 5. And you can see that on\nthese different QA data sets,",
    "start": "3619410",
    "end": "3625720"
  },
  {
    "text": "salient span masking does\nsignificantly better than just using the span\ncorruption technique.",
    "start": "3625720",
    "end": "3631880"
  },
  {
    "text": "So this really suggests that\ndoing the salient span masking and masking out these salient\nspans of these entities",
    "start": "3631880",
    "end": "3638290"
  },
  {
    "text": "is, in fact, helping to\nencode more knowledge in these language models. ",
    "start": "3638290",
    "end": "3646192"
  },
  {
    "text": "So to recap, we talked about\nthree different classes of techniques to add\nknowledge of language models.",
    "start": "3646193",
    "end": "3651820"
  },
  {
    "text": "We talked about using\npretrained entity embeddings. These weren't too difficult to\napply to existing architectures",
    "start": "3651820",
    "end": "3657640"
  },
  {
    "text": "and is a way to leverage this\nknowledge graph pretraining. But it's a rather indirect way\nof incorporating knowledge,",
    "start": "3657640",
    "end": "3663250"
  },
  {
    "text": "and it could be\nhard to interpret. We also talked about approaches\nto add an external memory.",
    "start": "3663250",
    "end": "3669920"
  },
  {
    "text": "This could support modifying\nthe knowledge base. It was also easier to interpret.",
    "start": "3669920",
    "end": "3675290"
  },
  {
    "text": "But they tended to be more\ncomplex in implementation like we saw in KGLM. And they also\nrequired more memory",
    "start": "3675290",
    "end": "3681170"
  },
  {
    "text": "like we saw at the\nkNN-LM approach. And then, finally, we talked\nabout modifying the training",
    "start": "3681170",
    "end": "3686460"
  },
  {
    "text": "data. So this requires\nno model changes or additional computation. It also might be the easiest\nto theoretically analyze.",
    "start": "3686460",
    "end": "3693930"
  },
  {
    "text": "So this is actually an active\narea of research right now. But it's still an open question\nif modifying the training data",
    "start": "3693930",
    "end": "3700400"
  },
  {
    "text": "is always as effective\nas model changes and what the trade-offs are in\nterms amount of data required",
    "start": "3700400",
    "end": "3705980"
  },
  {
    "text": "versus doing one of these\nother knowledge enhancement approaches. ",
    "start": "3705980",
    "end": "3712680"
  },
  {
    "text": "So that leads us to section 3. So I guess I'll pause\nagain for questions. ",
    "start": "3712680",
    "end": "3723070"
  },
  {
    "text": "[INAUDIBLE] may be good. Awesome. OK. So section 3 is about how\nresearchers are actually",
    "start": "3723070",
    "end": "3729220"
  },
  {
    "text": "going about evaluating the\nknowledge in language models and, I guess, how\nsome of the techniques",
    "start": "3729220",
    "end": "3734320"
  },
  {
    "text": "we actually just talked about\nstand up in this evaluation. So first, we're going\nto talk about probes",
    "start": "3734320",
    "end": "3739720"
  },
  {
    "text": "which don't require any\nfine-tuning of the language model. And then we're going to talk\nabout downstream tasks which",
    "start": "3739720",
    "end": "3745395"
  },
  {
    "text": "look at how well do these\npretrained representations actually transfer their\nknowledge to other tasks.",
    "start": "3745395",
    "end": "3752750"
  },
  {
    "text": "So one of the initial works\nin this area was called LAMA. And this really started\na series of works",
    "start": "3752750",
    "end": "3758260"
  },
  {
    "text": "to look into how much\nknowledge is already encoded in these language models.",
    "start": "3758260",
    "end": "3763510"
  },
  {
    "text": "So their question was how\nmuch relational common sense and factual knowledge is in\noff-the-shelf language models?",
    "start": "3763510",
    "end": "3769190"
  },
  {
    "text": "So this is just taking\npretrained language models and evaluating the\nknowledge in them. And this is without\nany additional training",
    "start": "3769190",
    "end": "3775660"
  },
  {
    "text": "or fine-tuning. So they mainly constructed\na set of what they refer to as closed statements.",
    "start": "3775660",
    "end": "3780840"
  },
  {
    "text": "And these are just the fill\nin the blank statements that we actually drew from\nat the beginning of the talk. And we have some\nmore examples here.",
    "start": "3780840",
    "end": "3786870"
  },
  {
    "text": " And they manually\ncreated these templates",
    "start": "3786870",
    "end": "3792500"
  },
  {
    "text": "of closed statements using\nknowledge graph triples and question-answer pairs\nfrom existing data sets.",
    "start": "3792500",
    "end": "3799120"
  },
  {
    "text": "They wanted to compare\npretrained language models to supervised relation\nextraction and question and answering systems to see how\ndo these language models that",
    "start": "3799120",
    "end": "3807400"
  },
  {
    "text": "were trained in an\nunsupervised fashion compare to these baseline\nsystems that are not",
    "start": "3807400",
    "end": "3812410"
  },
  {
    "text": "only supervised\nbut really targeted for this task of\nknowledge extraction And their goal was to evaluate\nthe knowledge in existing",
    "start": "3812410",
    "end": "3820140"
  },
  {
    "text": "pretrained language models. And a key point about\nthis is they're just using the language\nmodels as they",
    "start": "3820140",
    "end": "3825809"
  },
  {
    "text": "are available to researchers. So this means there could be\ndifferences in the pretraining corpora, for example.",
    "start": "3825810",
    "end": "3830953"
  },
  {
    "text": "So when you look at\nthe following table, and you're comparing\nlanguage models, also keep in mind\nthat these don't",
    "start": "3830953",
    "end": "3836610"
  },
  {
    "text": "account for the differences\nin the pretrained corpora. So a lot of these\nlanguage models",
    "start": "3836610",
    "end": "3841880"
  },
  {
    "text": "probably look familiar to you\neither from previous lectures or maybe your final projects.",
    "start": "3841880",
    "end": "3847250"
  },
  {
    "text": "And what we see is\nthat overall, BERT-base and BERT-Large pretrained\nmodels are performing",
    "start": "3847250",
    "end": "3852619"
  },
  {
    "text": "much better than the previous\nlanguage or the other language models here. I guess I forgot to mention\nwhat mean precision at one is.",
    "start": "3852620",
    "end": "3860330"
  },
  {
    "text": "This a pretty simple metric. The idea is if you\nlook at the blank, and you look at the\ntop predictions, or the top prediction, for the\nblank, is it correct or not.",
    "start": "3860330",
    "end": "3868300"
  },
  {
    "text": "That's what precision\nat one means. Precision at 10 would be, let's\nlook at the top 10 predictions, is the correct\nprediction in the top 10?",
    "start": "3868300",
    "end": "3874700"
  },
  {
    "text": " So in addition to\nBERT-Large and BERT-base",
    "start": "3874700",
    "end": "3879730"
  },
  {
    "text": "performing well overall, we do\nsee that in the T-REx data set, their relation extraction\nbaseline is performing",
    "start": "3879730",
    "end": "3886000"
  },
  {
    "text": "a bit better than BERT. One thing they notice here\nthat's pretty interesting is that this data set has\na lot of different types",
    "start": "3886000",
    "end": "3893260"
  },
  {
    "text": "of relations. And relations can be\nclassified in terms of, are they one-to-one relation,\nare they n-to-one relation,",
    "start": "3893260",
    "end": "3899860"
  },
  {
    "text": "are they n-to-n relation? An example of a\none-to-one relation would be your\nstudent ID relation.",
    "start": "3899860",
    "end": "3906160"
  },
  {
    "text": "So you have a unique student ID. An example of an n-to-n would\nbe the enrolled in relation.",
    "start": "3906160",
    "end": "3913030"
  },
  {
    "text": "So there's lots of students\nenrolled in lots of classes. So this would be\nan n-to-n relation. And they find that\nBERT really struggles",
    "start": "3913030",
    "end": "3919000"
  },
  {
    "text": "on these n-to-n relations. So while it performs better than\nrelation extraction baseline",
    "start": "3919000",
    "end": "3924520"
  },
  {
    "text": "on some types of\nrelations, overall, it does pretty terribly on\nthese n-to-n relations. So overall, it does a bit\nworse than the baseline",
    "start": "3924520",
    "end": "3931240"
  },
  {
    "text": "on this T-REx data set. They also compared\nto SQuAD on DrQA.",
    "start": "3931240",
    "end": "3936690"
  },
  {
    "text": "And they find that it\ndoes a fair amount worse. They note that the language\nmodel is not fine-tuned here.",
    "start": "3936690",
    "end": "3942059"
  },
  {
    "text": "And also, it has no access to\nan information retrieval system. And then when they look\nat the precision at 10,",
    "start": "3942060",
    "end": "3947370"
  },
  {
    "text": "they find that this gap between\nDrQA's performance and BERT actually closes\nquite a bit, which",
    "start": "3947370",
    "end": "3952980"
  },
  {
    "text": "suggests that these\nlanguage models do have some amount of knowledge\nencoded in them and that they're\neven competitive",
    "start": "3952980",
    "end": "3959820"
  },
  {
    "text": "with these knowledge extraction\nsupervised baselines. ",
    "start": "3959820",
    "end": "3966400"
  },
  {
    "text": "So you can also try out\nexamples on their GitHub repo for the LAMA probe. We have an example that was\nfrom their repo that was,",
    "start": "3966400",
    "end": "3973269"
  },
  {
    "text": "the cat is on the mask. You can see what the top\n10 predictions are to fill in the closed statement.",
    "start": "3973270",
    "end": "3979240"
  },
  {
    "text": "Here, they have the\ncat is on the phone. So this can be a fun\nway to figure out",
    "start": "3979240",
    "end": "3984520"
  },
  {
    "text": "what factual and\ncommon sense knowledge is in existing language models. And it's pretty easy to use\nwith this interactive prompt.",
    "start": "3984520",
    "end": "3993470"
  },
  {
    "text": "So some limitations\nof the LAMA probe are that it can be\nhard to understand why the models performed\nwell when they do.",
    "start": "3993470",
    "end": "4000310"
  },
  {
    "text": "So for instance, BERT might\njust be the most popular token, and this happens to be right. Maybe it's just memorizing\nco-occurrence patterns",
    "start": "4000310",
    "end": "4007119"
  },
  {
    "text": "and doesn't really understand\nthe knowledge statement and doesn't understand\nwhat the fact is.",
    "start": "4007120",
    "end": "4014480"
  },
  {
    "text": "It might also just\nbe identifying similarities between\nsurface forms of the subject and object. So for instance,\nin this example,",
    "start": "4014480",
    "end": "4020750"
  },
  {
    "text": "Pope Clement VII has\na position of blank. Even if you don't know anything\nabout Pope Clement VII,",
    "start": "4020750",
    "end": "4026902"
  },
  {
    "text": "you might be able to\nfigure out that Pope is the likely next word for this\ntriple, or for this template.",
    "start": "4026902",
    "end": "4035180"
  },
  {
    "text": "So the problem with this\nis if the model is just making these predictions\nbased on these surface forms or co-occurrence\npatterns, it's",
    "start": "4035180",
    "end": "4042073"
  },
  {
    "text": "difficult to know if\nwe're actually evaluating the knowledge in the model. Maybe it's just making correct\npredictions for other reasons.",
    "start": "4042073",
    "end": "4049542"
  },
  {
    "text": "And the more subtle issue\nthat we've brought up is that language\nmodels might be just be sensitive to the\nphrasing of the statement.",
    "start": "4049542",
    "end": "4055440"
  },
  {
    "text": "So for each triple in their\ndata set, or for each relation their data set, they just had\none mainly defined template.",
    "start": "4055440",
    "end": "4061872"
  },
  {
    "text": "And qualitatively, they\nfound that if they just make small changes\nto this template, it could actually change whether\nor not the model could recall",
    "start": "4061872",
    "end": "4068480"
  },
  {
    "text": "the correct prediction or not. And so this means\nthat the probe results are really a lower bound\non the knowledge that's",
    "start": "4068480",
    "end": "4075060"
  },
  {
    "text": "encoded in the language model. So if you change\nyour phrasing, it's possible that the model\nmight show that actually, it",
    "start": "4075060",
    "end": "4081600"
  },
  {
    "text": "does have the knowledge\nencoded in it. So the next lines of\nwork we'll talk about",
    "start": "4081600",
    "end": "4087060"
  },
  {
    "text": "are really building on\nthese two limitations of this original LAMA probe.",
    "start": "4087060",
    "end": "4092480"
  },
  {
    "text": "So the first one is called\nLAMA-UHN, or LAMA-Unhelpful Names. And the key idea is to\nremove these examples",
    "start": "4092480",
    "end": "4098521"
  },
  {
    "text": "from LAMA that can be answered\nwithout the relational knowledge. So this is just addressing\nthe first limitation on the left side.",
    "start": "4098521",
    "end": "4105540"
  },
  {
    "text": "So they observed that BERT\nrelies on the surface forms entities, might not\nbe using knowledge to make these predictions.",
    "start": "4105540",
    "end": "4111359"
  },
  {
    "text": "This includes a\nstring match situation that we talked\nabout with the pope. This also is dealing with the\nrevealing person name issue",
    "start": "4111359",
    "end": "4118649"
  },
  {
    "text": "that you saw on assignment 5. So this is where\nthe name could be an incorrect prior\nfor the native",
    "start": "4118649",
    "end": "4124020"
  },
  {
    "text": "language of someone, their place\nof birth, their nationality. They have this example\nfrom the table,",
    "start": "4124020",
    "end": "4129270"
  },
  {
    "text": "or from the paper, where they\nlooked at different people names or person's\nnames, and then they looked at BERT's prediction\nfor their native language.",
    "start": "4129270",
    "end": "4136390"
  },
  {
    "text": "And these are all\nFrench-speaking actors. And BERT just predicts very\nbiased and stereotypical",
    "start": "4136390",
    "end": "4141630"
  },
  {
    "text": "languages for these\nparticular names. So this can really\nwork both ways. It can lead BERT to make\nincorrect predictions sometimes",
    "start": "4141630",
    "end": "4148625"
  },
  {
    "text": "or in some cases. But it could also work\nto make-- or to let BERT make correct\npredictions, even",
    "start": "4148625",
    "end": "4154120"
  },
  {
    "text": "if it has no factual\nknowledge of those people. So that's the issue they're\ntrying to get at here is do we know that BERT\nactually knows this fact,",
    "start": "4154120",
    "end": "4160479"
  },
  {
    "text": "or is it just using some\nbias to make its prediction? So what they do\nis they introduce",
    "start": "4160479",
    "end": "4165528"
  },
  {
    "text": "a couple of heuristics to\nbasically just filter out these examples from the\nLAMA probe that can either",
    "start": "4165529",
    "end": "4170960"
  },
  {
    "text": "be solved by the\nstring match setting or this revealing\nperson name setting. So they make a harder subset\nof the data set, essentially.",
    "start": "4170960",
    "end": "4179520"
  },
  {
    "text": "They find that when they test\nBERT on this harder subset, that its performance\ndrops about 8%.",
    "start": "4179520",
    "end": "4184603"
  },
  {
    "text": "But when they test\ntheir knowledge on the enhanced model,\nwhich they call E-BERT, the score only drops about 1%.",
    "start": "4184603",
    "end": "4190390"
  },
  {
    "text": "So it's possible that as you\nmake harder knowledge probes, we'll actually see even bigger\ndifferences in the performance",
    "start": "4190390",
    "end": "4195730"
  },
  {
    "text": "of knowledge-enhanced models to\nmodels without these knowledge enhancements. ",
    "start": "4195730",
    "end": "4202605"
  },
  {
    "text": "The next piece of work\nwe'll talk about is actually getting at this issue of\nthe phrasing of the prompt",
    "start": "4202605",
    "end": "4210489"
  },
  {
    "text": "might actually trigger\ndifferent responses from the language model. So the language model\nmight know the fact,",
    "start": "4210490",
    "end": "4215500"
  },
  {
    "text": "but it might fail on the\ntask due to the phrasing. One reason this might\nhappen is the pretraining",
    "start": "4215500",
    "end": "4220900"
  },
  {
    "text": "is on different\ncontexts and sentence structures in the query. So for example, you might have\nin your pretraining corpus,",
    "start": "4220900",
    "end": "4227350"
  },
  {
    "text": "the birthplace of Barack\nObama is Honolulu, Hawaii. And this might be something you\nsee in Wikipedia, for instance.",
    "start": "4227350",
    "end": "4232420"
  },
  {
    "text": "That's a common\ntraining data set. And then as a\nresearcher, you write, Barack Obama was born in blank.",
    "start": "4232420",
    "end": "4237635"
  },
  {
    "text": "And you can see that\nthis sentence structures are pretty different. So the model might have\nseen the first fact.",
    "start": "4237635",
    "end": "4242768"
  },
  {
    "text": "But the sentence\nstructure difference is actually enough to confuse it\nso it can't answer this query.",
    "start": "4242768",
    "end": "4249198"
  },
  {
    "text": "So what they do is they\ngenerate a lot more of these prompts by mining\ntemplates from Wikipedia. One of the techniques\nactually uses",
    "start": "4249198",
    "end": "4255190"
  },
  {
    "text": "dependency parsing and also\ngenerating paraphrase prompts by taking inspiration from the\nmachine translation literature",
    "start": "4255190",
    "end": "4263200"
  },
  {
    "text": "and using back translation. So you generate a\nlot more prompts to try to query the\nlanguage models and figure",
    "start": "4263200",
    "end": "4268690"
  },
  {
    "text": "out do small variations\nin the prompt trigger the correct prediction\nfrom the language model.",
    "start": "4268690",
    "end": "4274740"
  },
  {
    "text": "They also experimented\nensembling prompts. So if we give the model\nmultiple prompts and then take some probability averaged\nover these different prompts,",
    "start": "4274740",
    "end": "4283409"
  },
  {
    "text": "can we improve the performance\non the model returning the correct prediction? So we give it a higher\nchance of seeing a context",
    "start": "4283410",
    "end": "4288780"
  },
  {
    "text": "that it might have actually\nseen during pretraining. They find that the\nperformance on LAMA",
    "start": "4288780",
    "end": "4294450"
  },
  {
    "text": "increases when they either\nuse a top performing prompt or when they use this\nensembling approach.",
    "start": "4294450",
    "end": "4299613"
  },
  {
    "text": "So this suggests that\nthe original LAMA really was a lower bound on the\namount of knowledge encoded in these language models.",
    "start": "4299613",
    "end": "4305460"
  },
  {
    "text": "And changing the\nphrasing can actually help the model recall\nthe correct answer.",
    "start": "4305460",
    "end": "4312678"
  },
  {
    "text": "This table's a bit frightening. But they find that small\nchanges in the query can lead to really large\ngains on performance.",
    "start": "4312678",
    "end": "4318870"
  },
  {
    "text": "So if you just have a query\nlike x plays in y position, and then you change that\nto x plays that y position,",
    "start": "4318870",
    "end": "4325250"
  },
  {
    "text": "this can actually lead\nto a 23% accuracy gain on this particular relation\nin terms of the model",
    "start": "4325250",
    "end": "4330500"
  },
  {
    "text": "actually being able to\ncall the correct answer. Or even just x was created in y\nto x is created in y, 10% gain.",
    "start": "4330500",
    "end": "4339660"
  },
  {
    "text": "So I think this motivates a need\nto not only develop better ways to query these models\nbut probably also build",
    "start": "4339660",
    "end": "4344787"
  },
  {
    "text": "language models that\nare actually more robust to the query itself. ",
    "start": "4344787",
    "end": "4351580"
  },
  {
    "text": "So in addition to\nprobes, another way to evaluate these\nlanguage models is by looking at how\nwell they transfer",
    "start": "4351580",
    "end": "4357720"
  },
  {
    "text": "from the pretrained\nrepresentation to downstream tasks. And so the idea here\nis you're actually",
    "start": "4357720",
    "end": "4363490"
  },
  {
    "text": "going to fine-tune the\npretrained representation on different downstream tasks\nsimilar to how you would evaluate BERT on GLUE tasks.",
    "start": "4363490",
    "end": "4371530"
  },
  {
    "text": "So common tasks that\nare used for this are relation extraction,\nentity typing, and question and answering.",
    "start": "4371530",
    "end": "4377578"
  },
  {
    "text": "Relation extraction\nis where you want to predict the relation\nbetween two entities. So this is getting back at\none of the questions earlier",
    "start": "4377578",
    "end": "4383740"
  },
  {
    "text": "in the talk in\nterms of, well, how do you get the relation that's\nin the edges in these knowledge bases. So",
    "start": "4383740",
    "end": "4388780"
  },
  {
    "text": "Given two entities,\nyou learn a model to predict what is the\nrelation between then. Entity typing is\nthe task of given",
    "start": "4388780",
    "end": "4394660"
  },
  {
    "text": "an entity, what is the\ntype of the entity. So here, Alice robbed the bank. You want to predict\nhere as a criminal.",
    "start": "4394660",
    "end": "4399910"
  },
  {
    "text": "And then, you guys, are\nvery familiar with question and answering. So the idea of these\ncommon-- of these tasks,",
    "start": "4399910",
    "end": "4405863"
  },
  {
    "text": "is that they're knowledge\nintensive so they're good candidates\nto see how do all of these pretrained\nrepresentations",
    "start": "4405863",
    "end": "4410989"
  },
  {
    "text": "actually transfer the knowledge\nto these downstream tasks. ",
    "start": "4410990",
    "end": "4416380"
  },
  {
    "text": "Here we look at the performance\non a relation extraction benchmark called TACRED. And all the models that we\nshow here were at one point,",
    "start": "4416380",
    "end": "4423430"
  },
  {
    "text": "state-of-the-art on TACRED. So this C-GCN is a graph\nconvolutional neural network",
    "start": "4423430",
    "end": "4429190"
  },
  {
    "text": "over dependency trees. The BERT-LSTM-base is ONE of\nthe first works that showed that",
    "start": "4429190",
    "end": "4434385"
  },
  {
    "text": "you could actually get\nstate-of-the-art performance with BERT on\nrelation extraction. And this is just putting LSTM\nlayer over BERT's output.",
    "start": "4434385",
    "end": "4441557"
  },
  {
    "text": "ERNIE is the work\nthat we talked about with the pretrained\nentity embeddings. Matching the Blanks,\nwe didn't get to today.",
    "start": "4441557",
    "end": "4447110"
  },
  {
    "text": "But it's a really\ninteresting work about learning meaningful\nrelational representations. And it falls more into the\ntraining data modification",
    "start": "4447110",
    "end": "4454600"
  },
  {
    "text": "approaches and that\nthey are actually masking out entities again. And then KnowBERT is\nwhat we talked about.",
    "start": "4454600",
    "end": "4461745"
  },
  {
    "text": "The W and W here means they\nactually encode two knowledge bases in KnowBERT. So they're encoding BERT\nNet, and they're also",
    "start": "4461745",
    "end": "4467680"
  },
  {
    "text": "encoding Wikipedia. And the high level takeaway from\nthis table is that you can see",
    "start": "4467680",
    "end": "4472800"
  },
  {
    "text": "that the recent knowledge\nenhanced models have achieved state-of-the-art over the\noriginal models that once",
    "start": "4472800",
    "end": "4478830"
  },
  {
    "text": "performed very well on TACRED. And we have about\nfive F1 gains here. Another interesting\ntakeaway from this table",
    "start": "4478830",
    "end": "4485460"
  },
  {
    "text": "is there seems to be a trade-off\nin the size of the language model that's necessary to\nget a certain performance.",
    "start": "4485460",
    "end": "4490930"
  },
  {
    "text": "So if you just consider the\nsize of the language model, then KnowBERT performs the best. But if you don't\nconsider that, then it",
    "start": "4490930",
    "end": "4497700"
  },
  {
    "text": "ties with Matching the Blanks. So overall, this is\npretty good evidence that these knowledge-enhanced\nmethods are, in fact,",
    "start": "4497700",
    "end": "4505409"
  },
  {
    "text": "transferring to these\nknowledge-intensive downstream tasks that can\nreally take advantage of these pretrained\nrepresentations.",
    "start": "4505410",
    "end": "4514139"
  },
  {
    "text": "We also have results\non entity typing. So here we're comparing\na slightly different set of models. Some of the baselines\nare LSTM models",
    "start": "4514140",
    "end": "4520830"
  },
  {
    "text": "that were designed\nfor entity typing. And we have ERNIE and\nKnowBERT leading the, I guess,",
    "start": "4520830",
    "end": "4527080"
  },
  {
    "text": "leaderboard here on the entity\ntyping task of open entity. And we see gains of about 15 F1\npoints with ERNIE and KnowBERT.",
    "start": "4527080",
    "end": "4534510"
  },
  {
    "text": "So once again, we\nreally do see that these knowledge-rich\npretrained representations are transferring and helping on\nthese knowledge-intensive",
    "start": "4534510",
    "end": "4541572"
  },
  {
    "text": "downstream tasks.  So just to recap, we\ntalked about probes",
    "start": "4541572",
    "end": "4548130"
  },
  {
    "text": "which evaluate the knowledge\nalready present in models. These don't require\nany more training. But it can be challenging\nto construct benchmarks",
    "start": "4548130",
    "end": "4554760"
  },
  {
    "text": "to actually make\nsure you're testing the knowledge in\nthese language models. It can also be\nchallenging to construct",
    "start": "4554760",
    "end": "4560220"
  },
  {
    "text": "the queries using the probe. We then talked about\ndownstream tasks. These are a bit of an indirect\nway to evaluate knowledge",
    "start": "4560220",
    "end": "4567390"
  },
  {
    "text": "and that they have this extra\ncomponent of fine-tuning. But it's a good\nway to evaluate how useful is this knowledge-rich\npretrained representation",
    "start": "4567390",
    "end": "4574500"
  },
  {
    "text": "in actual applications.  So I just touched on the\nexciting work in this area,",
    "start": "4574500",
    "end": "4582090"
  },
  {
    "text": "but there's many\nother directions if you want to dive\nmore into this. So there's retrieval-augmented\nlanguage models",
    "start": "4582090",
    "end": "4587580"
  },
  {
    "text": "which learn knowledge\nretrievers to figure out what documents might be relevant\nfor predicting the next word.",
    "start": "4587580",
    "end": "4593790"
  },
  {
    "text": "There's work in modifying the\nknowledge in language models. So I talked about\nhow this is one of the obstacles and\nchallenges to using language",
    "start": "4593790",
    "end": "4601140"
  },
  {
    "text": "models as knowledge bases. So there's been recent\nwork in this area. We also saw how important the\nknowledge pretraining task was.",
    "start": "4601140",
    "end": "4608675"
  },
  {
    "text": "Although there's\nmany papers that are proposing different tasks\nto do the knowledge pretraining, so it's still an open\nquestion in terms",
    "start": "4608675",
    "end": "4614910"
  },
  {
    "text": "of what tasks are best to\nadd to encode more knowledge. There's also been work on more\nefficient knowledge systems,",
    "start": "4614910",
    "end": "4622139"
  },
  {
    "text": "so at NeuIPRS\nefficient QA challenge which aims at building\nthe smallest QA system.",
    "start": "4622140",
    "end": "4627642"
  },
  {
    "text": "And then finally,\nthere's been work on building better\nknowledge benchmarks that build on the benchmarks\nthat we saw today.",
    "start": "4627642",
    "end": "4633440"
  },
  {
    "text": " So that's all I have for today. And I hope your final\nprojects are going well.",
    "start": "4633440",
    "end": "4640530"
  },
  {
    "start": "4640530",
    "end": "4645000"
  }
]