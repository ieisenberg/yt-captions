[
  {
    "start": "0",
    "end": "5530"
  },
  {
    "text": "Let's get started. So we are entering the\nsecond half of the course. And in the first\nhalf, we learned a lot",
    "start": "5530",
    "end": "12280"
  },
  {
    "text": "about lossless compression. And in the second half,\nwe'll start exploring the lossy compression.",
    "start": "12280",
    "end": "18200"
  },
  {
    "text": "Cool. Let's get going. So last time-- I'll just quickly\ngo over the quiz.",
    "start": "18200",
    "end": "23560"
  },
  {
    "text": "Last time, we\nlearned about LZ77. And so in quiz question 1, you\nwere given this decoding table",
    "start": "23560",
    "end": "31810"
  },
  {
    "text": "and were asked\nbasically to decode using this decoding table.",
    "start": "31810",
    "end": "38350"
  },
  {
    "text": "I think the question was\nreally straightforward. So for the first part, you have\nmatch offset 1, so that's B.",
    "start": "38350",
    "end": "44559"
  },
  {
    "text": "And then you have\nmatch length 4, so you're going to\noutput four extra B's. So you have AA triple B\nand then four extra B's.",
    "start": "44560",
    "end": "53950"
  },
  {
    "text": "Similarly, for the\nsecond one, you have match offset 9,\nso you can work it out that basically gets you\nhere at the beginning",
    "start": "53950",
    "end": "61030"
  },
  {
    "text": "of the whole string. And then you have match\nlength 5, so the extra bits which you send out\nas AA triple B.",
    "start": "61030",
    "end": "67640"
  },
  {
    "text": "And finally, the output string\nafter decoding the third row is just match length\n2 and match offset 2.",
    "start": "67640",
    "end": "75260"
  },
  {
    "text": "So you just go behind 2\nhere and add two more CDs. So this is just like\ndirect application",
    "start": "75260",
    "end": "82760"
  },
  {
    "text": "of what we learned in\nthe last class, decoding. Something which I want\nto highlight and somebody",
    "start": "82760",
    "end": "87920"
  },
  {
    "text": "asked this on Ed, I'm not sure\nif it was a private post or not, but I think this is a\ngood question, which",
    "start": "87920",
    "end": "94399"
  },
  {
    "text": "is, if I looked at this string,\nthe final string which I would have gotten would have\nbeen AA triple B, then",
    "start": "94400",
    "end": "103310"
  },
  {
    "text": "four more B's, then again AA,\ntriple B, and then CDCDCD.",
    "start": "103310",
    "end": "115009"
  },
  {
    "text": "So this would have\nbeen the input string. And then we got this\ndecoding table here,",
    "start": "115010",
    "end": "123820"
  },
  {
    "text": "but as Shubham\ntalked about there are various different\nparsing rules in LZ77.",
    "start": "123820",
    "end": "129190"
  },
  {
    "text": "The parsing scheme\nreally is not unique. And you may go, for example,\nthe greedy approach to parse",
    "start": "129190",
    "end": "134260"
  },
  {
    "text": "or the lazy approach to pass\nor do dynamic programming. So there are various\ndifferent ways to get to this encoding table.",
    "start": "134260",
    "end": "141760"
  },
  {
    "text": "But the good thing\nabout this scheme, really, is once you have this\ndecoding table, no matter",
    "start": "141760",
    "end": "146770"
  },
  {
    "text": "what the parsing\nscheme would have been, you would have recovered\nthis original string.",
    "start": "146770",
    "end": "151810"
  },
  {
    "text": "And the question also\nspecifically mentioned the same thing. So if somebody was confused\nthat, given this table,",
    "start": "151810",
    "end": "158800"
  },
  {
    "text": "the encoding algorithm which\nShubham taught in class didn't really lead to this\nparticular encoded LZ77",
    "start": "158800",
    "end": "167920"
  },
  {
    "text": "table, that's still fine because\nit's just a different parsing scheme, which resulted\nin this thing.",
    "start": "167920",
    "end": "174010"
  },
  {
    "text": "So that's, I guess, one\nthing to really understand in this problem.",
    "start": "174010",
    "end": "179680"
  },
  {
    "text": "And then the second\none was, again, like-- so I think the\ninput was Tatwawadi,",
    "start": "179680",
    "end": "186880"
  },
  {
    "text": "which is Kedar's last name, and\nwe saw this, and we were like,",
    "start": "186880",
    "end": "191920"
  },
  {
    "text": "oh, we have to apply\nthe LZ77 to this. So we did. And in this problem,\nactually, we",
    "start": "191920",
    "end": "199000"
  },
  {
    "text": "go over and do the exact\nsame encoding scheme which we taught in class. So it was like a very\nspecific LZ77 parser.",
    "start": "199000",
    "end": "205840"
  },
  {
    "text": "And you were asked to\nfill in this table. And it kind of is\nquite straightforward.",
    "start": "205840",
    "end": "212470"
  },
  {
    "text": "So first, matched\nliteral is just TA. Match length is 1\nbecause T gets repeated.",
    "start": "212470",
    "end": "219610"
  },
  {
    "text": "And match offset would be 2. Following that, W\nis a unique symbol.",
    "start": "219610",
    "end": "226090"
  },
  {
    "text": "But after W, you\nget A. A is being matched three points behind.",
    "start": "226090",
    "end": "232510"
  },
  {
    "text": "And match length is just going\nto be one because it's just A. And finally, after A, you\nsee WA being repeated,",
    "start": "232510",
    "end": "241340"
  },
  {
    "text": "which you had already encoded. So now you have\noffset 2, length 2, which was already given to you.",
    "start": "241340",
    "end": "247370"
  },
  {
    "text": "So you don't really have\nany unmatched literal after that step. And finally, you have DI,\nwhich is just unmatched.",
    "start": "247370",
    "end": "254090"
  },
  {
    "text": "So your last\nliteral is just DIN. And there is no\nlength and offset. So this is now an example of\none particular parsing scheme",
    "start": "254090",
    "end": "261079"
  },
  {
    "text": "with LZ77 and encoding that. Questions on LZ77 encoding,\ndecoding, parsing?",
    "start": "261079",
    "end": "269415"
  },
  {
    "text": " So then quiz 3 was around--",
    "start": "269415",
    "end": "276960"
  },
  {
    "text": "so now you have been\ngiven an English text. But now what you did was\nyou change the letter such",
    "start": "276960",
    "end": "282750"
  },
  {
    "text": "that every A becomes B, every B\nbecomes C, so on and so forth. And the first question\nwas does the std",
    "start": "282750",
    "end": "288810"
  },
  {
    "text": "perform similarly on\nboth the original text and transformed word? And what was the answer?",
    "start": "288810",
    "end": "294900"
  },
  {
    "text": "Somebody wants to say? ",
    "start": "294900",
    "end": "300510"
  },
  {
    "text": "Anyone?  Yes. Yeah, exactly, this is what I\nwant to hear with confidence.",
    "start": "300510",
    "end": "307930"
  },
  {
    "text": "Yes, right? So zstd doesn't really care\nabout the exact probability distribution.",
    "start": "307930",
    "end": "313330"
  },
  {
    "text": "It's just looking for matches. And your matches\nwill remain as is. Your literals will change.",
    "start": "313330",
    "end": "318640"
  },
  {
    "text": "So if it was this example, your\nliterals would have changed. But your performance would have\nremained the same, similar.",
    "start": "318640",
    "end": "330520"
  },
  {
    "text": "And then the second\nquestion was we also learned about this\nLLM-based compressor in just one class\nbefore, lecture 9.",
    "start": "330520",
    "end": "336850"
  },
  {
    "text": "And what do we think,\nwould LLM-based compressor perform here?",
    "start": "336850",
    "end": "343770"
  },
  {
    "text": "OK, I see some-- at least a\nfew people shaking their head. And yeah, the answer is no. And the reason really\nis LLM-based predictor",
    "start": "343770",
    "end": "351810"
  },
  {
    "text": "is only as good as\nits probability model. And when you shift\nthese symbols, your probability model\ncompletely changes.",
    "start": "351810",
    "end": "358740"
  },
  {
    "text": "You need to go and update\nyour LLM probability model. Otherwise, it's\nbasically useless.",
    "start": "358740",
    "end": "364050"
  },
  {
    "text": "It's just completely\nwrong model. It can't really do anything. And this is basically like\nthe conceptual difference",
    "start": "364050",
    "end": "369180"
  },
  {
    "text": "between LZ-based, I would\nsay, encoding schemes, versus arithmetic\ncoding or RNNS,",
    "start": "369180",
    "end": "375510"
  },
  {
    "text": "where the probability is needed\nto be really matching up. ",
    "start": "375510",
    "end": "383639"
  },
  {
    "text": "OK, and then the final\nquestion for the quiz was-- I think I'll just\ntell the answer.",
    "start": "383640",
    "end": "388710"
  },
  {
    "text": "I think Shubham really\ntook half an hour-- or not half an hour, but 15\nminutes to really drill down",
    "start": "388710",
    "end": "393810"
  },
  {
    "text": "this thought. But we are learning so many\ncool techniques in the class. This is almost like,\nin a sense, a joke.",
    "start": "393810",
    "end": "399300"
  },
  {
    "text": "But to be really\nhonest, this is what's going to be important\nonce you're, let's say, out in the industry or working\non some compressors on your own.",
    "start": "399300",
    "end": "406830"
  },
  {
    "text": "And the answer really is\nalways, always, always try some standard\ncompressor, even",
    "start": "406830",
    "end": "412560"
  },
  {
    "text": "before going to anything else. That should be your step 0.1\ngiven any compression problem.",
    "start": "412560",
    "end": "418530"
  },
  {
    "text": "And then go and do all the\nother things which we learned. ",
    "start": "418530",
    "end": "425050"
  },
  {
    "text": "OK, one second. I'm going to try this again. ",
    "start": "425050",
    "end": "431729"
  },
  {
    "text": "OK, let me just keep this open. OK, any questions so far?",
    "start": "431730",
    "end": "437190"
  },
  {
    "text": " Good. So let's get to\nthe new material.",
    "start": "437190",
    "end": "442330"
  },
  {
    "text": "So again, as a recap,\nwe have learned so far about lossless\ncompression and tradeoffs",
    "start": "442330",
    "end": "447610"
  },
  {
    "text": "between various\ndifferent entropy coders. So we started the\nclass, and we learned about the fundamental limits\non a lossless compressor,",
    "start": "447610",
    "end": "454570"
  },
  {
    "text": "which was entropy. We learned about a thumb\nrule, which we have seen time",
    "start": "454570",
    "end": "460330"
  },
  {
    "text": "and again going\nthrough this course so far, to really\nderive our intuition and understanding of how well\nthe compressors are doing.",
    "start": "460330",
    "end": "469210"
  },
  {
    "text": "We followed this up by\nvarious lossless compressors. Sometimes these are also\ncalled entropy coders.",
    "start": "469210",
    "end": "474460"
  },
  {
    "text": "And that's just because they\nare trying to code something to the entropy. And we also looked at\ntheir implementation.",
    "start": "474460",
    "end": "481780"
  },
  {
    "text": "So we looked at block codes\nlike Shannon and Huffman. We looked at streaming codes,\nlike arithmetic and ANS.",
    "start": "481780",
    "end": "489190"
  },
  {
    "text": "And finally, we also looked at\nthis pattern matching code idea, or a universal code,\nwhich was Lz77.",
    "start": "489190",
    "end": "496930"
  },
  {
    "text": "And we kind of wrapped\nthis all up by-- OK, earlier we were\ntalking about IID sources.",
    "start": "496930",
    "end": "504250"
  },
  {
    "text": "We wrapped it up by\ntalking about how to deal with non-IID sources,\nwhich is basically life. So we talked about\ncontext-based coding,",
    "start": "504250",
    "end": "511480"
  },
  {
    "text": "and we talked about\nadaptive coding. And yeah, you'll\nhave some questions",
    "start": "511480",
    "end": "516729"
  },
  {
    "text": "on context-based coding,\nLZ77 in homework 3. So look forward to it\nand some other ideas",
    "start": "516730",
    "end": "523400"
  },
  {
    "text": "around dealing with\nnon-IID sources. So that should be fun.",
    "start": "523400",
    "end": "528490"
  },
  {
    "text": "OK, but now we are going to\nmove to the lossy compression.",
    "start": "528490",
    "end": "535370"
  },
  {
    "text": "And the first point which I want\nto really make very strongly is that whatever we have\nlearned so far, all of it",
    "start": "535370",
    "end": "542210"
  },
  {
    "text": "will remain applicable in the\nsecond half of the course. In fact, lossless\ncompression is just",
    "start": "542210",
    "end": "548329"
  },
  {
    "text": "a special case of\nlossy compression, with just loss equals to 0. Even more so, it's not just\nlike at an intuitive level",
    "start": "548330",
    "end": "557060"
  },
  {
    "text": "that, OK, sure, lossless\nis loss equals to 0. You will see that any\npractical compressor out there,",
    "start": "557060",
    "end": "563990"
  },
  {
    "text": "like design of any\nlossy compressor, will involve a lossless\ncompression part. After you introduce\nthe lossy step,",
    "start": "563990",
    "end": "571310"
  },
  {
    "text": "finally you would losslessly\ncompress something. So whatever we have learned,\nall our intuition so far",
    "start": "571310",
    "end": "578390"
  },
  {
    "text": "will carry forward\nto lossy and help us improve lossy compressors\nin terms of designing the loss",
    "start": "578390",
    "end": "584030"
  },
  {
    "text": "aspect of it because\nthe final step would be to losslessly\ncompress some symbols.",
    "start": "584030",
    "end": "590450"
  },
  {
    "text": "So again, we'll learn a lot more\nabout this in coming lectures. But this is really important.",
    "start": "590450",
    "end": "596820"
  },
  {
    "text": "So you really need to\nunderstand lossless for lossy. And finally, something really\nimportant as in lossless",
    "start": "596820",
    "end": "605670"
  },
  {
    "text": "compression, we assume\ndiscrete sources. So far, we have always\ntalked about finite alphabet,",
    "start": "605670",
    "end": "612150"
  },
  {
    "text": "some discrete sources,\nsome probability distribution over it, and then\nwe went on to compress it.",
    "start": "612150",
    "end": "618660"
  },
  {
    "text": "But in reality, in practice,\nnothing is discrete, right? All these signals\nwhich we are receiving,",
    "start": "618660",
    "end": "625650"
  },
  {
    "text": "be it image, any sensory\ninformation we are receiving, all of this is\nactually continuous.",
    "start": "625650",
    "end": "631140"
  },
  {
    "text": "It's not really\na discrete value. And so in the second\nhalf of the course, we'll start learning\nabout how to deal",
    "start": "631140",
    "end": "637770"
  },
  {
    "text": "with these continuous values. OK, and so before we\neven go and start--",
    "start": "637770",
    "end": "644790"
  },
  {
    "text": "so again, like all sensors,\nimages, videos in reality are continuous sources.",
    "start": "644790",
    "end": "649889"
  },
  {
    "text": "And the whole point\nof storing data is to somehow represent\nthis information, correct?",
    "start": "649890",
    "end": "656370"
  },
  {
    "text": "So let's just start this\nlecture with like the first quiz question, which is, how\nmuch information do you",
    "start": "656370",
    "end": "663540"
  },
  {
    "text": "think a continuous source has? So we have talked about entropy. We have talked about\ndiscrete sources.",
    "start": "663540",
    "end": "669990"
  },
  {
    "text": "What would be your\nguess on how much information does a\ncontinuous source has?",
    "start": "669990",
    "end": "676580"
  },
  {
    "text": "Yeah. Whatever the floating point\nprecision of the machine is. OK, so the answer I got was\nwhatever the floating point",
    "start": "676580",
    "end": "684290"
  },
  {
    "text": "precision of a machine is. But at that step, you\nhave already assumed some discretization, right?",
    "start": "684290",
    "end": "689570"
  },
  {
    "text": "[INAUDIBLE] Exactly. OK, the answer-- OK, just\nfor the rest of the class,",
    "start": "689570",
    "end": "695690"
  },
  {
    "text": "the answer I got\nwas that it would be whatever the precision\nof your machine is.",
    "start": "695690",
    "end": "701210"
  },
  {
    "text": "That will determine how much\ninformation the source contains. But if you think about\nit, at that point, you have already discarded some\ninformation about the source,",
    "start": "701210",
    "end": "708980"
  },
  {
    "text": "right? Like, who says 64-bit is the\nfundamental precision at which",
    "start": "708980",
    "end": "714019"
  },
  {
    "text": "we should store everything? That's as arbitrary a\nnumber as it could be.",
    "start": "714020",
    "end": "720200"
  },
  {
    "text": "And actually, a continuous-- sorry.",
    "start": "720200",
    "end": "725480"
  },
  {
    "text": "OK, so the answer really\nis infinite, right? And this is like a\nfundamental, I would say,",
    "start": "725480",
    "end": "732740"
  },
  {
    "text": "math fact, which this infinite\nreally stems from the fact that between two real\nnumbers, you can basically",
    "start": "732740",
    "end": "740519"
  },
  {
    "text": "find infinite more numbers. This is like a, I don't\nknow, number theory 101 fact.",
    "start": "740520",
    "end": "748860"
  },
  {
    "text": "And this kind of leads\nto, OK, no matter what precision you store your signal\nto be at, unless it's infinite,",
    "start": "748860",
    "end": "755970"
  },
  {
    "text": "there is nothing infinite. Whatever high end you\ncome up with-- tomorrow, let's say we go crazy with the\nnumber of bits we can store.",
    "start": "755970",
    "end": "762930"
  },
  {
    "text": "Instead of 64-bit computers, we\nhave 1-million bit computers. Even at that point, you are\nthrowing away some information",
    "start": "762930",
    "end": "769470"
  },
  {
    "text": "about the actual source\nyou are recording. The limit will come to\nthe physical acquiring",
    "start": "769470",
    "end": "775380"
  },
  {
    "text": "of the source at that point. But still, this is\nreally important. ",
    "start": "775380",
    "end": "782130"
  },
  {
    "text": "And so what this means is that\nyou cannot represent a container source exactly.",
    "start": "782130",
    "end": "787500"
  },
  {
    "text": "It's a lost cause. You can't really do\nanything about it. And we need to approximate it.",
    "start": "787500",
    "end": "795269"
  },
  {
    "text": "And any approximation basically\nimplies loss of information.",
    "start": "795270",
    "end": "800940"
  },
  {
    "text": "OK, so it's very natural. You have to do lossy\ncompression at some point.",
    "start": "800940",
    "end": "806490"
  },
  {
    "text": "The way lossy compression\nliterature deals with it is we introduce a\nparameter called",
    "start": "806490",
    "end": "813480"
  },
  {
    "text": "distortion, which\nis like some measure of the loss of information.",
    "start": "813480",
    "end": "818640"
  },
  {
    "text": "So for example,\nyour distortion-- if you have seen\nmean squared error,",
    "start": "818640",
    "end": "823829"
  },
  {
    "text": "then your distortion\nlooks something like expected value of x minus\nx hat, where x is your symbol,",
    "start": "823830",
    "end": "829590"
  },
  {
    "text": "and x hat is whatever you\nare representing it with. So you can imagine\nx hat to be 64-bit",
    "start": "829590",
    "end": "834720"
  },
  {
    "text": "representation or\nwhatever physical signal we are talking about. But this is just like one\nparticular distortion.",
    "start": "834720",
    "end": "840750"
  },
  {
    "text": "Nobody is forcing\nyou to take it. You may come and say, oh, this\ndistortion is not valid for me.",
    "start": "840750",
    "end": "846510"
  },
  {
    "text": "Maybe what's valid for me is\nthis mean absolute error, which is just the expected value\nof mode of x minus x hat.",
    "start": "846510",
    "end": "854640"
  },
  {
    "text": "And you can come up with\nbasically any distance function if you have-- if you know\nabout distances in maths",
    "start": "854640",
    "end": "861240"
  },
  {
    "text": "as a distortion measure. And the distortion you want to\nmeasure, you want to work with,",
    "start": "861240",
    "end": "868079"
  },
  {
    "text": "would depend on the application\nwhich you are working on. In fact, in the second half\nof the lossy compression,",
    "start": "868080",
    "end": "876120"
  },
  {
    "text": "when we go to start learning\nabout images, videos, maybe even a little bit\nof audio compression,",
    "start": "876120",
    "end": "883290"
  },
  {
    "text": "you will see that a lot\nof study has happened into how these distortion\nmeasures, even though they're",
    "start": "883290",
    "end": "888720"
  },
  {
    "text": "very nice mathematically to\ndeal with, are actually wrong. Just don't correlate\nwith how humans perceive",
    "start": "888720",
    "end": "895589"
  },
  {
    "text": "a higher distortion\nmight not mean actually a higher loss in\ninformation, so as to say.",
    "start": "895590",
    "end": "903060"
  },
  {
    "text": "And so this really--\njust this part, choosing a distortion measure,\nis actually very important",
    "start": "903060",
    "end": "908310"
  },
  {
    "text": "and has really influenced the\ndesign of a lot of multimedia compressors. It's a very active area of\nresearch in various places.",
    "start": "908310",
    "end": "916050"
  },
  {
    "text": "And we'll even have a lecture\non perceptual compression, which",
    "start": "916050",
    "end": "921880"
  },
  {
    "text": "is really important\nfor multimedia. OK, so distortion part\nis clear for everyone?",
    "start": "921880",
    "end": "929150"
  },
  {
    "text": "But it's not just\nabout distortion. Sure, we decided how much\ninformation I can lose.",
    "start": "929150",
    "end": "935990"
  },
  {
    "text": "You also like-- after\nwe have decided like,",
    "start": "935990",
    "end": "941170"
  },
  {
    "text": "OK, I'm OK with some\ndistortion, we then need to optimize for\nthe number of bits",
    "start": "941170",
    "end": "947770"
  },
  {
    "text": "used to represent the source. OK, so you are saying, I'm\nOK with this much error",
    "start": "947770",
    "end": "953350"
  },
  {
    "text": "given my application. Now the question becomes\nvery similar to what we have been studying so far,\nwhich is, how can I represent",
    "start": "953350",
    "end": "960640"
  },
  {
    "text": "the source given that I'm\nOK with this [AUDIO OUT] to happen in as few\nbits as [AUDIO OUT]??",
    "start": "960640",
    "end": "968060"
  },
  {
    "text": "And that's really\nthe rate component. And very intuitively, this is--",
    "start": "968060",
    "end": "975340"
  },
  {
    "text": "I think, I hope this\nshould be very obvious that really a higher rate\nshould allow you to get a lower",
    "start": "975340",
    "end": "983050"
  },
  {
    "text": "distortion because you\ncan spend more bits and representing the source. So ideally, you are choosing\nto allocate those bits",
    "start": "983050",
    "end": "991270"
  },
  {
    "text": "for representing the\nsource in a way that minimizes the distortion. ",
    "start": "991270",
    "end": "997980"
  },
  {
    "text": "And this is really the\nfundamental rate distortion tradeoff in lossy compression,\nlike one of the most fundamental",
    "start": "997980",
    "end": "1006560"
  },
  {
    "text": "thing in lossy compression. ",
    "start": "1006560",
    "end": "1011850"
  },
  {
    "text": "So this is just a curve. I think we saw this in--",
    "start": "1011850",
    "end": "1017300"
  },
  {
    "text": "we also saw this in lecture 1,\nwhen I gave a lot of examples about how this rate\ndistortion tradeoff might",
    "start": "1017300",
    "end": "1023540"
  },
  {
    "text": "look for images, how it might\nlook for audio, et cetera. And so if you think\nabout this like, again,",
    "start": "1023540",
    "end": "1029930"
  },
  {
    "text": "what this curve is saying\nas your rate increases-- so x-axis is rate. R is for rate, D\nis for distortion.",
    "start": "1029930",
    "end": "1036980"
  },
  {
    "text": "So on the x-axis,\nas rate increases, your distortion is decreasing.",
    "start": "1036980",
    "end": "1042740"
  },
  {
    "text": "And this is something\nreally fundamental. And now, actually, you see I\nhave done a little bit more.",
    "start": "1042740",
    "end": "1050050"
  },
  {
    "text": "I have made it like\na U-shaped thing. I have made it like this.",
    "start": "1050050",
    "end": "1055910"
  },
  {
    "text": "I have not made it like this. I have not made it like this. I could have-- I could have drawn many\ndifferent cartoon curves.",
    "start": "1055910",
    "end": "1063100"
  },
  {
    "text": "There is a very specific\ncartoon curve I drew. And that's because\nrate distortion has some very\ninteresting properties",
    "start": "1063100",
    "end": "1071669"
  },
  {
    "text": "and fundamental theory\nassociated with it. And Tsachy actually will\nbe talking a bit about it",
    "start": "1071670",
    "end": "1077130"
  },
  {
    "text": "in the next lecture. So this is, again, the part of\ninformation theory where there",
    "start": "1077130",
    "end": "1083070"
  },
  {
    "text": "is very deep connection\nbetween lossy compression and the theory aspects to it. You can say a lot about\nfundamental aspects",
    "start": "1083070",
    "end": "1089790"
  },
  {
    "text": "about these things given\nsome known distributions. ",
    "start": "1089790",
    "end": "1099160"
  },
  {
    "text": "OK, so really, practically,\nor in theory, or anything,",
    "start": "1099160",
    "end": "1104800"
  },
  {
    "text": "what are you trying to do? So there is-- there\nare always two ways to think about this rate\ndistortion tradeoff.",
    "start": "1104800",
    "end": "1111070"
  },
  {
    "text": "And I think this\nis very obvious, but in practice\nand in my life, I",
    "start": "1111070",
    "end": "1118900"
  },
  {
    "text": "have found these two\nviewpoints very useful. So whenever you are looking at\na lossy compression framework,",
    "start": "1118900",
    "end": "1125050"
  },
  {
    "text": "you should really think\nabout it in two sense. One is let's say I have been\ngiven some distortion D. I don't",
    "start": "1125050",
    "end": "1131540"
  },
  {
    "text": "know what your application is. Maybe it is that\nI can't tolerate an error more than something,\nsome distortion measure I just",
    "start": "1131540",
    "end": "1137720"
  },
  {
    "text": "can't tolerate more than this. If it is more than\nthis, nobody is going to watch my streaming service.",
    "start": "1137720",
    "end": "1143840"
  },
  {
    "text": "So you have been\ngiven some distortion which you are supposed to meet. You can't go above it.",
    "start": "1143840",
    "end": "1149660"
  },
  {
    "text": "And then the way to think\nabout this rate distortion really is like, so\ngiven some distortion D, you want to minimize the rate.",
    "start": "1149660",
    "end": "1156230"
  },
  {
    "text": "So given this\ndistortion, you could have represented the source\nwith this many number",
    "start": "1156230",
    "end": "1161450"
  },
  {
    "text": "of bits, this many number of\nbits, this many number of bits. And you really want to\nfind the scheme which is let's say the best\npossible representation given",
    "start": "1161450",
    "end": "1169490"
  },
  {
    "text": "that distortion.  Another way to think\nabout this problem",
    "start": "1169490",
    "end": "1175500"
  },
  {
    "text": "is that, no, maybe you are,\nagain, a streaming service, but now you are serving some\ncustomers which just don't",
    "start": "1175500",
    "end": "1183180"
  },
  {
    "text": "have bandwidth to have 4k. So you can't really-- no matter what you do, you can't\nreally get to this distortion",
    "start": "1183180",
    "end": "1189690"
  },
  {
    "text": "because you just need way\nmore bits to get there, and you can't get there.",
    "start": "1189690",
    "end": "1195960"
  },
  {
    "text": "So another way to think about\nthis is you're rate-limited. So you have been given\nsome rate at which you",
    "start": "1195960",
    "end": "1201240"
  },
  {
    "text": "can transmit some number of\nbits, which is your limit. And then, again, you could-- when you just use\nthose many bits",
    "start": "1201240",
    "end": "1208080"
  },
  {
    "text": "and use it to represent\ninformation in different ways, you may have gotten this much\ndistortion or this much or this",
    "start": "1208080",
    "end": "1213570"
  },
  {
    "text": "much. And what you want to\ndo is really optimize, get the minimum distortion\nfor that particular rate.",
    "start": "1213570",
    "end": "1221910"
  },
  {
    "text": "So this is like a-- so as to say you are always\nlooking for this Pareto optimal rate distortion curve.",
    "start": "1221910",
    "end": "1227370"
  },
  {
    "start": "1227370",
    "end": "1233080"
  },
  {
    "text": "This is very obvious but I\nthink really important that",
    "start": "1233080",
    "end": "1238510"
  },
  {
    "text": "should be drilled right now,\nwill be used continuously in different ways. ",
    "start": "1238510",
    "end": "1244809"
  },
  {
    "text": "Any questions? ",
    "start": "1244810",
    "end": "1250220"
  },
  {
    "text": "OK.  So let's just start\nwith some examples now.",
    "start": "1250220",
    "end": "1257060"
  },
  {
    "text": "So let's say-- OK, let's say you are measuring\ntemperature T in a room,",
    "start": "1257060",
    "end": "1263500"
  },
  {
    "text": "like say in this room. Let's say it's in Celsius\nat some hourly interval. So this is some sensor\ndata you are recording.",
    "start": "1263500",
    "end": "1271120"
  },
  {
    "text": "Again, remember that the\nactual physical temperature is a continuous source. It actually requires\ninfinite bit",
    "start": "1271120",
    "end": "1277120"
  },
  {
    "text": "to represent it exactly, which\nour sensors are not able to do. Our sensors themselves\nare doing something lossy",
    "start": "1277120",
    "end": "1284310"
  },
  {
    "text": "when they are recording. That's OK. Let's say this particular sensor\nis actually very sensitive",
    "start": "1284310",
    "end": "1291520"
  },
  {
    "text": "and actually gives you up to\nsix decimals in Celsius when you are recording this temperature.",
    "start": "1291520",
    "end": "1296679"
  },
  {
    "text": "So first data point\nwas, I don't know, 38 point blah, blah, blah\n1.0001 degrees Celsius.",
    "start": "1296680",
    "end": "1302889"
  },
  {
    "text": "Next one was 36 point,\nagain, six decimal. This is what your\nsensor's outputting. ",
    "start": "1302890",
    "end": "1312445"
  },
  {
    "text": "So if your sensor is actually\nrecording this kind of data, an obvious question to you\nat that point is, do I need--",
    "start": "1312445",
    "end": "1321559"
  },
  {
    "text": "how do I store it? How many-- how many\nbits should I even think of allocating to represent\nthis particular sensory data?",
    "start": "1321560",
    "end": "1334620"
  },
  {
    "text": "Any thoughts, guesses? ",
    "start": "1334620",
    "end": "1340289"
  },
  {
    "text": "There are no wrong answers. Let's keep discussing\nand shooting. I just like interactive classes.",
    "start": "1340290",
    "end": "1345375"
  },
  {
    "text": "So just whatever\ncomes to your head. ",
    "start": "1345375",
    "end": "1352090"
  },
  {
    "text": "If we know a set range for\nthe temperature of the room, like if we assume it's only\ngoing to go from 0 to 100,",
    "start": "1352090",
    "end": "1357768"
  },
  {
    "text": "that's going to make\nit easier because that seems reasonable for a room. But if we have some really\nweird room, it might go lower.",
    "start": "1357768",
    "end": "1363940"
  },
  {
    "text": "OK, so the answer which\none of the students gave was, oh, it depends on\nwhich room you are in.",
    "start": "1363940",
    "end": "1369820"
  },
  {
    "text": "Like, if it's a room--\nif it's a normal room and it only depends on 0\nto 100, which is already",
    "start": "1369820",
    "end": "1375730"
  },
  {
    "text": "I think quite a high\nvariance for a room. You are going from freezing\nwater to boiling it,",
    "start": "1375730",
    "end": "1380929"
  },
  {
    "text": "so I wouldn't want\nto be in that room. But still, OK, if\nthis is a normal room,",
    "start": "1380930",
    "end": "1386050"
  },
  {
    "text": "then that gives me some\ninformation versus I don't know if it's an abnormal\nroom even beyond that,",
    "start": "1386050",
    "end": "1393700"
  },
  {
    "text": "then you might need more bits. And actually, that's correct. This was really,\nin some sense, I",
    "start": "1393700",
    "end": "1400269"
  },
  {
    "text": "would say a trick but\nnot really a trick. A lot of time and practice\ndesigning lossy compressors",
    "start": "1400270",
    "end": "1405370"
  },
  {
    "text": "just goes into figuring out\nthis question, essentially, which is it really depends\non the application you",
    "start": "1405370",
    "end": "1413240"
  },
  {
    "text": "are working with. So let's say if you are-- the example I came up with\nwas not room temperature but maybe you are\ncontrolling the AC--",
    "start": "1413240",
    "end": "1420019"
  },
  {
    "text": " Yeah, if you are\ncontrolling the AC using",
    "start": "1420020",
    "end": "1426460"
  },
  {
    "text": "this particular thermostat, you\nmight want a few more decimals versus if you are\nusing this data just",
    "start": "1426460",
    "end": "1432430"
  },
  {
    "text": "on your FitBit or something\nto determine should I take a hoodie with me out or not. These two are very\ndifferent things.",
    "start": "1432430",
    "end": "1438760"
  },
  {
    "text": "So a FitBit would want\nto store this data at a very different\nlevel of distortion and with very different\nnumber of bits",
    "start": "1438760",
    "end": "1444070"
  },
  {
    "text": "than maybe a\nthermostat or a car, like a sensitive\ntemperature measure",
    "start": "1444070",
    "end": "1450250"
  },
  {
    "text": "in an industrial setting. So this basically leads\nto a question we always",
    "start": "1450250",
    "end": "1456430"
  },
  {
    "text": "need to keep asking, which\nis, we first of all need to decide on the distortion\nwe are OK with when we go",
    "start": "1456430",
    "end": "1462955"
  },
  {
    "text": "to lossy compress such a data.  So in this particular case--",
    "start": "1462955",
    "end": "1470020"
  },
  {
    "text": "continuing with the example,\nin this particular case, I think all of us can agree\nsix decimals behind temperature",
    "start": "1470020",
    "end": "1476470"
  },
  {
    "text": "seems just wasteful,\nno matter almost what the application is, unless\nyou're doing some material",
    "start": "1476470",
    "end": "1482510"
  },
  {
    "text": "science maybe. It seems very wasteful.",
    "start": "1482510",
    "end": "1487890"
  },
  {
    "text": "So the question then\nchanges now slightly. So now I have told\nyou that six seems",
    "start": "1487890",
    "end": "1494430"
  },
  {
    "text": "wasteful for whatever\napplication we are working with. So now that you\nlook at this series,",
    "start": "1494430",
    "end": "1500490"
  },
  {
    "text": "I don't know, what do you think\nare some reasonable values to encode? ",
    "start": "1500490",
    "end": "1510640"
  },
  {
    "text": "Again, not a trick.  So this is just basics. We are not even going to get\ninto hard technical problems",
    "start": "1510640",
    "end": "1519190"
  },
  {
    "text": "yet. ",
    "start": "1519190",
    "end": "1525250"
  },
  {
    "text": "No one? Oh, OK. I would have just\ndropped all the decimals.",
    "start": "1525250",
    "end": "1531620"
  },
  {
    "text": "So it's something\ntemperature in Celsius. I don't really care\nif it's 38.1 or 37.9.",
    "start": "1531620",
    "end": "1538610"
  },
  {
    "text": "And if I were storing this data\nfor [INAUDIBLE]---- so again, I said just some reasonable\nvalues to encode,",
    "start": "1538610",
    "end": "1544400"
  },
  {
    "text": "being reasonably hand-wavy. You can come back and say,\nno, I want another decimal. It's OK. But some reasonable values\ncould be just storing 38, 36,",
    "start": "1544400",
    "end": "1552912"
  },
  {
    "text": "37, again 37, 35.",
    "start": "1552912",
    "end": "1558910"
  },
  {
    "text": "Correct? And at this point, we have\nalready introduced loss.",
    "start": "1558910",
    "end": "1564440"
  },
  {
    "text": "So just rounding of\nthis data has led to an introduction of the loss.",
    "start": "1564440",
    "end": "1571430"
  },
  {
    "text": "And this is really important. And it's really\nsimple technique, but it's a first basic lossy\ncompression technique, which",
    "start": "1571430",
    "end": "1579590"
  },
  {
    "text": "is you can just round the data. And that introduces loss.",
    "start": "1579590",
    "end": "1585230"
  },
  {
    "text": "This is not exactly but similar\nto converting the data types, so like converting the data\ntype from float to int.",
    "start": "1585230",
    "end": "1595040"
  },
  {
    "text": "Maybe the sensor data in\npractice you have been given is actually, I don't\nknow, float 64; somebody is using float 64\nto encode income of someone.",
    "start": "1595040",
    "end": "1605540"
  },
  {
    "text": "Seems really wasteful, right? Like, as soon as\nyou get the data, the first thing you would do is\nconvert it into float 32 maybe,",
    "start": "1605540",
    "end": "1612470"
  },
  {
    "text": "or just some ints. At that point-- OK,\nactually not ints. Maybe they care about--\npeople care about cents.",
    "start": "1612470",
    "end": "1620480"
  },
  {
    "text": "But as soon as you do\nthat, you have already introduced some loss.",
    "start": "1620480",
    "end": "1625549"
  },
  {
    "text": "You have introduced\nsome distortion into your source symbol. And that's really one\nof the most, how should",
    "start": "1625550",
    "end": "1634919"
  },
  {
    "text": "I say, obvious ways\nin which we are doing lossy compression going about. And it's quite useful.",
    "start": "1634920",
    "end": "1640679"
  },
  {
    "text": "Just changing data types,\nthat's lossy compression. And it might lead\nto much fewer bits to represent anything post that.",
    "start": "1640680",
    "end": "1649215"
  },
  {
    "text": " OK, everybody with me so far?",
    "start": "1649215",
    "end": "1655070"
  },
  {
    "text": "The more formal way to think\nabout the obvious rounding thing, which I said\nand very obvious things",
    "start": "1655070",
    "end": "1661009"
  },
  {
    "text": "which we have been talking\nabout, is quantization. So what basically we did\nin the previous example,",
    "start": "1661010",
    "end": "1666409"
  },
  {
    "text": "it's called quantization. So this is not the\ndictionary definition.",
    "start": "1666410",
    "end": "1671690"
  },
  {
    "text": "I kind of got it\nfrom Professor Gray from Stanford, who was really\nan expert in this area.",
    "start": "1671690",
    "end": "1679070"
  },
  {
    "text": "In one of his notes,\nhe says that-- and I really like this\nsimple definition, which is, quantization is\njust the process of mapping",
    "start": "1679070",
    "end": "1686390"
  },
  {
    "text": "a continuous source\nto a discrete source. As soon as you do that, you have\nmoved into the lossy compression",
    "start": "1686390",
    "end": "1692885"
  },
  {
    "text": "world.  So some obvious points, like\nquantization is a lossy process.",
    "start": "1692885",
    "end": "1699820"
  },
  {
    "text": "It introduces distortion. And in fact, it is a fundamental\noperation in lossy compression.",
    "start": "1699820",
    "end": "1705985"
  },
  {
    "text": " Yeah, I don't think any lossy\ncompressor in the world exists",
    "start": "1705985",
    "end": "1712820"
  },
  {
    "text": "which doesn't do some form\nof quantization somewhere, no matter what domain,\nwhat application, what",
    "start": "1712820",
    "end": "1719149"
  },
  {
    "text": "rate distortion level\nyou are talking about. So it is really like-- it's present somewhere as part\nof the lossy compression scheme.",
    "start": "1719150",
    "end": "1728850"
  },
  {
    "text": "So some technical terms--\nthe quantized values are called symbols-- sometimes\ncalled symbols, sometimes",
    "start": "1728850",
    "end": "1735059"
  },
  {
    "text": "called codewords. And the set of available\nquantized values",
    "start": "1735060",
    "end": "1742080"
  },
  {
    "text": "is sometimes called codebook,\nsometimes called dictionary. So in the previous example\nwhich we looked at,",
    "start": "1742080",
    "end": "1749539"
  },
  {
    "text": "where we had 38.1\nblah, blah, blah, let's say these were\nthe only five symbols. So I rounded it, and I\ngot 38, 36, 37, 37, 35.",
    "start": "1749540",
    "end": "1760790"
  },
  {
    "text": "And so in this particular\ncase, my symbols or codewords",
    "start": "1760790",
    "end": "1767270"
  },
  {
    "text": "after quantization were just\nthese rounded-up things. And if you look at\nthis, it basically",
    "start": "1767270",
    "end": "1773210"
  },
  {
    "text": "had only four unique\nvalues, 35, 36, 37, 38.",
    "start": "1773210",
    "end": "1778490"
  },
  {
    "text": "And this is really\njust your codebook. So these are the four quantized\nvalues you would transmit.",
    "start": "1778490",
    "end": "1783965"
  },
  {
    "text": " OK. So the next obvious\nthing which we",
    "start": "1783965",
    "end": "1792140"
  },
  {
    "text": "are going to ask-- so doing this\nquantization reduce to some sort of [AUDIO OUT]. We could think of\nit in two ways.",
    "start": "1792140",
    "end": "1798410"
  },
  {
    "text": "One, we could have thought of\nit as, like I did, rounding. But another way to think\nabout this same thing",
    "start": "1798410",
    "end": "1805160"
  },
  {
    "text": "is really that I\nhad this codebook. And then I wanted to represent\nall these symbols using",
    "start": "1805160",
    "end": "1814200"
  },
  {
    "text": "the codebook. And so I will just\nbasically choose the element which is nearest\nin the codebook, in some sense,",
    "start": "1814200",
    "end": "1819780"
  },
  {
    "text": "being extremely hand-wavy will\nget and fill in the details. But talking in English,\nit makes sense, right?",
    "start": "1819780",
    "end": "1826328"
  },
  {
    "text": "That's what you would do. These are the only four\nvalues you can transmit ever. Then you would just\nchoose the value",
    "start": "1826328",
    "end": "1832200"
  },
  {
    "text": "which is nearest to one\nof these four values. So then let's say now I\nhave a codebook of size",
    "start": "1832200",
    "end": "1839320"
  },
  {
    "text": "N. What is the rate going to be? ",
    "start": "1839320",
    "end": "1854330"
  },
  {
    "text": "Again, no trick\nquestions, nothing wrong.  Another way to think about it--",
    "start": "1854330",
    "end": "1860480"
  },
  {
    "text": "think about-- OK, now we\nare-- now we are in a world which you guys\nunderstand, hopefully.",
    "start": "1860480",
    "end": "1867650"
  },
  {
    "text": "These are discrete, right? Your codewords are discrete. Let's say in this\ncase, there are",
    "start": "1867650",
    "end": "1872960"
  },
  {
    "text": "four elements, four codewords. How would you transmit\nthese four codewords? Let's say it's uniformly\ndistributed amongst these bits.",
    "start": "1872960",
    "end": "1881510"
  },
  {
    "text": "How many bits you\nwould have required? I see a couple people pointing.",
    "start": "1881510",
    "end": "1886970"
  },
  {
    "text": "Two, right? Why two? What exactly is two? Is it the log 2 of the\nsize of the codebook?",
    "start": "1886970",
    "end": "1893880"
  },
  {
    "text": "Exactly. So it's just the log 2 of\nthe size of the codebook because what you can do\nin this case is let's",
    "start": "1893880",
    "end": "1901680"
  },
  {
    "text": "say I have codeword 1,\ncodeword 2, so on and so forth,",
    "start": "1901680",
    "end": "1911120"
  },
  {
    "text": "codeword n. And if I have to transmit\njust the codewords,",
    "start": "1911120",
    "end": "1918320"
  },
  {
    "text": "I could encode it by\njust encoding the index.",
    "start": "1918320",
    "end": "1923429"
  },
  {
    "text": "And then if I only have to send\nthe index-- so for example, let's say if I receive index i,\nI will reconstruct codeword i.",
    "start": "1923430",
    "end": "1932880"
  },
  {
    "text": "My decoder will\nreconstruct codeword i. So in this case, you only\nneed to send log N bits.",
    "start": "1932880",
    "end": "1939870"
  },
  {
    "text": " So if you have N\ncodewords, you only",
    "start": "1939870",
    "end": "1947000"
  },
  {
    "text": "need to send\nactually log N bits. ",
    "start": "1947000",
    "end": "1953040"
  },
  {
    "text": "And so if you have a\ncodebook of size N, irrespective of what\nexactly the code vector is, your rate is just going to\nbe log 2 of N. So again, sorry,",
    "start": "1953040",
    "end": "1969540"
  },
  {
    "text": "there should be a ceil here. Let me just add it now. ",
    "start": "1969540",
    "end": "1980730"
  },
  {
    "text": "Another way to think about this\nis if you had bits per symbol,",
    "start": "1980730",
    "end": "1986940"
  },
  {
    "text": "you could have really used\n2 to power R unique values. And this is the\nexact same principle",
    "start": "1986940",
    "end": "1992670"
  },
  {
    "text": "if we go back to,\nI think, lecture 5 about asymptotic\nequipartition property which",
    "start": "1992670",
    "end": "1998110"
  },
  {
    "text": "Tsachy talked about. This is the exact same thing. You can just--\nyou can just count",
    "start": "1998110",
    "end": "2004460"
  },
  {
    "text": "the indices and the\nsequence and use that to just encode the index. ",
    "start": "2004460",
    "end": "2010850"
  },
  {
    "text": "So this tells if you have\na codebook of size N, you need roughly log N bits\nper symbol to transmit it.",
    "start": "2010850",
    "end": "2021290"
  },
  {
    "text": "And this tells you about\nthe rate distortion tradeoff now immediately.",
    "start": "2021290",
    "end": "2026570"
  },
  {
    "text": "So let's say you had an\nopportunity for higher rate. In that case, you would\nhave a larger codebook.",
    "start": "2026570",
    "end": "2037750"
  },
  {
    "text": "A larger codebook would\nresult in a lower distortion because now you have more\noptions to choose from.",
    "start": "2037750",
    "end": "2044840"
  },
  {
    "text": "Alternate way to think about\nthe same thing, if you had, I don't know, a lower\ncodebook, in that case,",
    "start": "2044840",
    "end": "2050609"
  },
  {
    "text": "you would probably have\na higher distortion. And then you would\nneed fewer bits.",
    "start": "2050610",
    "end": "2055804"
  },
  {
    "text": " Maybe let's just\nquickly write it down.",
    "start": "2055804",
    "end": "2062500"
  },
  {
    "text": "So if your rate is\nhigher, that basically means your number of\ncodewords is higher.",
    "start": "2062500",
    "end": "2070179"
  },
  {
    "text": "And that would typically means\nyour distortion goes down.",
    "start": "2070179",
    "end": "2075629"
  },
  {
    "text": "Or other way if we have\nto think, if you're-- I don't know, if you're\nOK with higher distortion,",
    "start": "2075630",
    "end": "2081989"
  },
  {
    "text": "you would do so by reducing\nyour codebook size. And that would lead\nto a lower rate.",
    "start": "2081989",
    "end": "2091110"
  },
  {
    "text": "So you see the tradeoff\nbetween rate distortion immediately in the scheme. ",
    "start": "2091110",
    "end": "2100319"
  },
  {
    "text": "Any questions so far? ",
    "start": "2100320",
    "end": "2107670"
  },
  {
    "text": "OK, so now let's talk\nabout another example. Like we have been saying,\nthe lossy compression",
    "start": "2107670",
    "end": "2115260"
  },
  {
    "text": "makes sense and\ncontinuous sources. And whenever sources\ncome in life, the first thought\nfor everyone should--",
    "start": "2115260",
    "end": "2122730"
  },
  {
    "text": "like modeling continuous\nsources comes in, the first thought should go\nto a Gaussian random variable.",
    "start": "2122730",
    "end": "2129480"
  },
  {
    "text": "Is everyone comfortable with\nGaussian random variable? Yeah?",
    "start": "2129480",
    "end": "2135200"
  },
  {
    "text": "Good. OK, so now let's say\nyou have symbol X--",
    "start": "2135200",
    "end": "2140450"
  },
  {
    "text": "set of symbols X, which are\ncoming from a Gaussian source. And let's just say it\nhas mean 0 variance 1.",
    "start": "2140450",
    "end": "2145505"
  },
  {
    "text": " So it's a continuous source.",
    "start": "2145505",
    "end": "2152180"
  },
  {
    "text": "And for whatever reason,\nyour friend comes and says, oh, I can only send\nlike 1 bit per symbol.",
    "start": "2152180",
    "end": "2159020"
  },
  {
    "text": "I just can't do more than that. That's my budget. That's the-- I'm in, I don't\nknow, 1930s or, whatever, 1960s.",
    "start": "2159020",
    "end": "2165877"
  },
  {
    "text": "There wasn't [INAUDIBLE]\nthen, and I can only send a bit per second or something. But this is my source.",
    "start": "2165877",
    "end": "2174450"
  },
  {
    "text": "Now, what do you think? What's some reasonable values\nto encode for this source? ",
    "start": "2174450",
    "end": "2185010"
  },
  {
    "text": "The sign. The sign, OK, yeah. That makes sense. So one of the students\nsuggested, OK, maybe we",
    "start": "2185010",
    "end": "2191190"
  },
  {
    "text": "should encode the sign. And that basically just\ncomes from the fact that Gaussian really\nis like a symmetrical",
    "start": "2191190",
    "end": "2199019"
  },
  {
    "text": "distribution in terms\nof probability density. And so if you have\nonly one bit that",
    "start": "2199020",
    "end": "2204910"
  },
  {
    "text": "says you can only send some two\nchoices, maybe sending the sign makes intuitive sense.",
    "start": "2204910",
    "end": "2211359"
  },
  {
    "text": "OK, so again, we are not saying\nso far if it's the best--",
    "start": "2211360",
    "end": "2216835"
  },
  {
    "text": "we can't say if\nit is the best way to do until you come\nand tell me what's the distortion I'm\nreally OK with.",
    "start": "2216835",
    "end": "2226190"
  },
  {
    "text": "But it still makes\nintuitive sense that, OK, hopefully whatever\ndistortion someone comes up",
    "start": "2226190",
    "end": "2231560"
  },
  {
    "text": "with will be symmetric around 0. And otherwise, they probably-- something is wrong or\nGaussian model is wrong right.",
    "start": "2231560",
    "end": "2237980"
  },
  {
    "text": "But it's OK. OK, so one reasonable valuable\nvalue to encode is sign of X,",
    "start": "2237980",
    "end": "2244819"
  },
  {
    "text": "and that's just because\nyour distribution is symmetric around 0. ",
    "start": "2244820",
    "end": "2251420"
  },
  {
    "text": "Everyone with me? ",
    "start": "2251420",
    "end": "2256700"
  },
  {
    "text": "OK, so let's continue. So your friend says you can just\ntransmit one bit per symbol.",
    "start": "2256700",
    "end": "2263930"
  },
  {
    "text": "And so you say, OK, I'll\nencode the sign of X hat. Sorry, I'll just\nencode the sign of X.",
    "start": "2263930",
    "end": "2270380"
  },
  {
    "text": "So no matter what my X is,\nat each of these symbols, I'll just send you the sign.",
    "start": "2270380",
    "end": "2277550"
  },
  {
    "text": "So now let's say\nyour friend receives a positive value for X hat,\nwhat should he really recover?",
    "start": "2277550",
    "end": "2285109"
  },
  {
    "text": "What should he-- what should\nhe represent that symbol with?",
    "start": "2285110",
    "end": "2290400"
  },
  {
    "text": "So other words, what should\nbe the quantized value of the recovered symbol?",
    "start": "2290400",
    "end": "2295800"
  },
  {
    "text": "To remind, like it was, for\nexample, last time from 37.1, we went to 37, right? So that's the quantized value.",
    "start": "2295800",
    "end": "2302040"
  },
  {
    "text": " And to actually\nanswer this question,",
    "start": "2302040",
    "end": "2307390"
  },
  {
    "text": "this is what I was just\nsaying a second ago, you need to decide what\ndistortion are we OK with.",
    "start": "2307390",
    "end": "2312730"
  },
  {
    "text": "So different distortions might\nmean a differently colored symbol may make better sense.",
    "start": "2312730",
    "end": "2319030"
  },
  {
    "text": " OK, so let's say we are talking\nabout mean squared error",
    "start": "2319030",
    "end": "2324080"
  },
  {
    "text": "distortion, one\nspecific distortion. Any guesses?",
    "start": "2324080",
    "end": "2329560"
  },
  {
    "text": "OK, so this one is technical. This one is not a joke question. ",
    "start": "2329560",
    "end": "2335500"
  },
  {
    "text": "The variance. The variance? Why do you say so?",
    "start": "2335500",
    "end": "2341920"
  },
  {
    "text": "The variance is the expectation\nof the MSE [INAUDIBLE]..",
    "start": "2341920",
    "end": "2347559"
  },
  {
    "text": "OK, actually, no,\nthat's incorrect.",
    "start": "2347560",
    "end": "2353940"
  },
  {
    "text": "So the answer really is\nthe expected value of X given your observed symbol,\nwhich is X hat greater than 0.",
    "start": "2353940",
    "end": "2362430"
  },
  {
    "text": "And your obvious question should\nbe, where does this come from?",
    "start": "2362430",
    "end": "2368616"
  },
  {
    "text": "This is a really celebrated-- do I have links? Actually, I don't have links\non these slides, but it's OK.",
    "start": "2368616",
    "end": "2375000"
  },
  {
    "text": "So this is a really\ncelebrated result, and it's called minimum\nmean square estimator.",
    "start": "2375000",
    "end": "2382110"
  },
  {
    "text": "So minimum mean\nsquare estimator.",
    "start": "2382110",
    "end": "2391190"
  },
  {
    "text": "And if you have taken, I\nguess, a signal processing class for mean squared error,\nthis should be obvious for you.",
    "start": "2391190",
    "end": "2399320"
  },
  {
    "text": "But it's OK if you don't know\nthis result. For this class purposes, you can assume\nthat this is true.",
    "start": "2399320",
    "end": "2407030"
  },
  {
    "text": "What this means is that\nif somebody comes and says to me that I have X\nhat greater than 0,",
    "start": "2407030",
    "end": "2417560"
  },
  {
    "text": "the value which minimizes this\nthing, given X hat greater",
    "start": "2417560",
    "end": "2428250"
  },
  {
    "text": "than 0, is basically\nexpected value of X given X hat greater than 0.",
    "start": "2428250",
    "end": "2433455"
  },
  {
    "start": "2433455",
    "end": "2441100"
  },
  {
    "text": "There are more\nreferences in the notes, which should be pushed by now\non the class notes website.",
    "start": "2441100",
    "end": "2451619"
  },
  {
    "text": "So if you are eager, you\ncan go look into this. There are some signal\nprocessing references there also which you can go through.",
    "start": "2451620",
    "end": "2459060"
  },
  {
    "text": "But yeah, so this is really-- this is a really\nfundamental result. A lot of people\nlossy compression",
    "start": "2459060",
    "end": "2464820"
  },
  {
    "text": "will just replace it. But it's really\nimportant to understand that this particular codebook\nis for mean square estimator.",
    "start": "2464820",
    "end": "2475090"
  },
  {
    "text": "So this is like a minimum\nmean square estimator. If I came up with some\ncrazy distortion function--",
    "start": "2475090",
    "end": "2480342"
  },
  {
    "text": "I don't know, I could\nhave come up with-- ",
    "start": "2480342",
    "end": "2485470"
  },
  {
    "text": "I don't-- let's say-- ",
    "start": "2485470",
    "end": "2498930"
  },
  {
    "text": "I don't know, let's say I came\nup with a distortion function like this. ",
    "start": "2498930",
    "end": "2505470"
  },
  {
    "text": "It may not be true. Or some more nonlinear, like\na neural net X minus X hat",
    "start": "2505470",
    "end": "2510740"
  },
  {
    "text": "something--  so the point which I\nreally want to make here",
    "start": "2510740",
    "end": "2517400"
  },
  {
    "text": "is that this optimal codebook,\nonce you have the rate,",
    "start": "2517400",
    "end": "2526950"
  },
  {
    "text": "so you basically\nknow your codebook, and you really need to know\nthe distortion for which you",
    "start": "2526950",
    "end": "2533580"
  },
  {
    "text": "are optimizing. Otherwise, there is\nno optimal codebook you need to be always--\nlike whenever somebody comes and talks to you-- talks to\nyou about lossy, ask them",
    "start": "2533580",
    "end": "2540870"
  },
  {
    "text": "what's the distortion\nwe are talking about. Or you might have to define\nthe appropriate distortion.",
    "start": "2540870",
    "end": "2547800"
  },
  {
    "text": "And actually, if you\nwork this out-- again, this is there in the notes.",
    "start": "2547800",
    "end": "2553440"
  },
  {
    "text": "So this-- or we can just\nquickly work it out here also just so that people\nfeel comfortable.",
    "start": "2553440",
    "end": "2561869"
  },
  {
    "text": "So expected value of X\ngiven X hat greater than 0.",
    "start": "2561870",
    "end": "2568230"
  },
  {
    "text": "So we know fX is Gaussian, which\njust means it's density is this.",
    "start": "2568230",
    "end": "2574320"
  },
  {
    "text": " And expected value of X\ngiven X hat equals to 0, just",
    "start": "2574320",
    "end": "2583579"
  },
  {
    "text": "by the definition,\nwould be minus infinity to infinity X times fX given\nX hat greater than 0 of X dX.",
    "start": "2583580",
    "end": "2595940"
  },
  {
    "text": "And this is just a\nconditional distribution, so you can use Bayes\nrule to recover this.",
    "start": "2595940",
    "end": "2602150"
  },
  {
    "text": "In this case, it's symmetric. So f of X, given X\nhat greater than 0",
    "start": "2602150",
    "end": "2607340"
  },
  {
    "text": "of X. Since it's\nsymmetric around 0, it will just be twice\nthis distribution,",
    "start": "2607340",
    "end": "2613940"
  },
  {
    "text": "so it will be 2 times f of X\ngiven X. You can show this.",
    "start": "2613940",
    "end": "2624300"
  },
  {
    "text": "This is just straightforward-- This is straightforward maths.",
    "start": "2624300",
    "end": "2632390"
  },
  {
    "text": "And so if you write this\ndown, this basically becomes,",
    "start": "2632390",
    "end": "2637700"
  },
  {
    "text": "let's say, square\nroot 2 over pi times X e raised to power\nminus X pi 2 dX.",
    "start": "2637700",
    "end": "2644240"
  },
  {
    "text": "And actually, it's X\nhat greater than 0. So this is this.",
    "start": "2644240",
    "end": "2649700"
  },
  {
    "text": "If X is greater than 0, 0\nelse because N equals to 0.",
    "start": "2649700",
    "end": "2655900"
  },
  {
    "text": "So this is also called-- this is just half Gaussian,\nlike it's just looking",
    "start": "2655900",
    "end": "2661570"
  },
  {
    "text": "at one side of the Gaussian. And if you work\nout this integral, it's actually quite\nstraightforward,",
    "start": "2661570",
    "end": "2669310"
  },
  {
    "text": "like you could do a variable\nsubstitution here of X square by 2.",
    "start": "2669310",
    "end": "2674670"
  },
  {
    "text": "OK, this is not\na calculus class. So we can skip this you. Can trust me on this part.",
    "start": "2674670",
    "end": "2679810"
  },
  {
    "text": "This thing will basically--\nand again, the notes have more details. So if you want to look into,\nyou can-- you're happy to look.",
    "start": "2679810",
    "end": "2686829"
  },
  {
    "text": "Please check it out. So this gives you square root\n2 over pi, which is this.",
    "start": "2686830",
    "end": "2693920"
  },
  {
    "text": "And then expected value of X,\nagain, given X hat less than 0 is just symmetric.",
    "start": "2693920",
    "end": "2699020"
  },
  {
    "text": "So it's just negative\nvalue of that. ",
    "start": "2699020",
    "end": "2704369"
  },
  {
    "text": "So in this example-- so this match was just to do\nit once in front of you guys",
    "start": "2704370",
    "end": "2713950"
  },
  {
    "text": "so that you feel comfortable\nwith what's happening. It's not really required from\nthe purpose of [INAUDIBLE]..",
    "start": "2713950",
    "end": "2724000"
  },
  {
    "text": "We'll not assume you know\nsignal processing a lot, so don't worry about\nif you are seeing",
    "start": "2724000",
    "end": "2729700"
  },
  {
    "text": "this for the first time in\nyour life and are like, OK. But this is really one of the-- I don't know, it's one of\nthe most celebrated results",
    "start": "2729700",
    "end": "2737410"
  },
  {
    "text": "in signal processing, used\nhundreds of different of places. ",
    "start": "2737410",
    "end": "2743200"
  },
  {
    "text": "Main takeaway really is-- OK, so if somebody gave\nyou a Gaussian and said, OK, one bit per symbol,\nthat basically implies you",
    "start": "2743200",
    "end": "2750460"
  },
  {
    "text": "have a codebook which can\nonly contain two symbols. And one of the choices\nfor mean square, estimator like if you\nwere minimizing distortion",
    "start": "2750460",
    "end": "2757840"
  },
  {
    "text": "for mean square estimator, is\nyou would choose these values.",
    "start": "2757840",
    "end": "2764300"
  },
  {
    "text": "So again, the communication\nscheme, more importantly, looks like this.",
    "start": "2764300",
    "end": "2770900"
  },
  {
    "text": "Oops, what did I do? So let's just quickly talk about\nthe sort of scheme which we did.",
    "start": "2770900",
    "end": "2780910"
  },
  {
    "text": "What did we do? We started with a\ncontinuous source.",
    "start": "2780910",
    "end": "2786280"
  },
  {
    "text": "We decided that we want to\nencode it at one bit per symbol. ",
    "start": "2786280",
    "end": "2792549"
  },
  {
    "text": "Actually, let me not even put\narrows because some of these are decisions. ",
    "start": "2792550",
    "end": "2800319"
  },
  {
    "text": "We thought that encoding sign\nseems a reasonable thing to do.",
    "start": "2800320",
    "end": "2805720"
  },
  {
    "text": "So what you did was you\ncommunicated and encoded just the sign, so this\nthing you encoded,",
    "start": "2805720",
    "end": "2814070"
  },
  {
    "text": "which is a discrete distribution\nso you can see this step here, encoding a discrete\ndistribution,",
    "start": "2814070",
    "end": "2820369"
  },
  {
    "text": "is where everything we learnt\nin lossless will come into play. It doesn't right now,\nbut you can already",
    "start": "2820370",
    "end": "2827090"
  },
  {
    "text": "see what's going to happen. So you encode the sign. You send over index\nof the codebook.",
    "start": "2827090",
    "end": "2839430"
  },
  {
    "text": "Or send over as just encoded\nthe index of the codebook.",
    "start": "2839430",
    "end": "2844630"
  },
  {
    "text": "On the other side,\nyour decoder receives this, this exact same thing,\nindex of the codebook,",
    "start": "2844630",
    "end": "2853119"
  },
  {
    "text": "and use it to recover the sign. So they know the sign. But actually, not just\nthe sign, since you",
    "start": "2853120",
    "end": "2858700"
  },
  {
    "text": "guys agreed on mean\nsquare estimation. So you already had\nthe codeword shared, codebook either shared or\ncan be computed independently",
    "start": "2858700",
    "end": "2866980"
  },
  {
    "text": "at the decoder side. And that basically\ntold them, OK, I'll just represent my\nthing as given the sign",
    "start": "2866980",
    "end": "2873880"
  },
  {
    "text": "as plus minus\nsquare root 2 by pi. And that would minimize my\noverall mean square distortion.",
    "start": "2873880",
    "end": "2880810"
  },
  {
    "text": "This is really the\nscheme which we did. ",
    "start": "2880810",
    "end": "2890460"
  },
  {
    "text": "OK, everyone with me? At least schematically\nwe understand",
    "start": "2890460",
    "end": "2897619"
  },
  {
    "text": "what's happening in\nboth these examples and how are we introducing loss? What are the key\nthings to look out for?",
    "start": "2897620",
    "end": "2905030"
  },
  {
    "text": "We'll keep on making it more\nand more formal as we go ahead. ",
    "start": "2905030",
    "end": "2912640"
  },
  {
    "text": "Cool. So what we did was-- so this is the\nGaussian histogram.",
    "start": "2912640",
    "end": "2918820"
  },
  {
    "text": "So same Gaussian, normal 0, 1. I generated a lot of samples.",
    "start": "2918820",
    "end": "2924430"
  },
  {
    "text": "I just plotted it\nas a histogram. This is X. These are the counts.",
    "start": "2924430",
    "end": "2930339"
  },
  {
    "text": "So this is not the\nprobability, but you can see different counts. And this is roughly my Gaussian.",
    "start": "2930340",
    "end": "2936775"
  },
  {
    "text": " So in this whole\nscheme, you basically did two things,\nwhich is how we'll",
    "start": "2936775",
    "end": "2944930"
  },
  {
    "text": "go ahead and formalize this. You decided a decision boundary\nfor one bit per symbol.",
    "start": "2944930",
    "end": "2952970"
  },
  {
    "text": "So one of your decision\nboundary was X greater than 0. The other decision boundary was,\nI don't know, X less than 0,",
    "start": "2952970",
    "end": "2962020"
  },
  {
    "text": "correct? And then you went\nahead and mapped it to a specific value\nof the codebook.",
    "start": "2962020",
    "end": "2967840"
  },
  {
    "text": " And so, let's say, if\nyou have a point here--",
    "start": "2967840",
    "end": "2975040"
  },
  {
    "text": "like if your actual\nsample was, I don't know, 2 point something, you\nwould basically just",
    "start": "2975040",
    "end": "2980140"
  },
  {
    "text": "map it to the-- so this value\nis square root 2 over pi. You'll just map it to this guy.",
    "start": "2980140",
    "end": "2987500"
  },
  {
    "text": "Or if there was a point here-- you'll basically see which\npoint of the decision boundary your actual symbol\nlies 2 and just",
    "start": "2987500",
    "end": "2995300"
  },
  {
    "text": "map it to the corresponding\ncodebook word. ",
    "start": "2995300",
    "end": "3005070"
  },
  {
    "text": "And so more formally, like\nexactly the whole thing which we did, is an\nexample of something",
    "start": "3005070",
    "end": "3011279"
  },
  {
    "text": "called scalar quantization. So the idea of scalar\nis you are just",
    "start": "3011280",
    "end": "3016470"
  },
  {
    "text": "looking at each\nindividual symbol, and you are defining two things. One, so now here\nwhat I'm showing",
    "start": "3016470",
    "end": "3023460"
  },
  {
    "text": "is input signal X, so another\nway to show the same thing, and the output X hat.",
    "start": "3023460",
    "end": "3029940"
  },
  {
    "text": "Your quantizer\nreally is a function. It takes an input X and maps\nit to a quantized value X hat.",
    "start": "3029940",
    "end": "3038990"
  },
  {
    "text": "OK, so you can see this\nfunction as a 2D xy plot, right?",
    "start": "3038990",
    "end": "3044540"
  },
  {
    "text": "And the two things\nwhich we looked at was the decision thresholds. So right now, we only\ntalked about one bit,",
    "start": "3044540",
    "end": "3050300"
  },
  {
    "text": "but obviously we\ncould have decided we could have had two bits. We could have had N bits. We could have various different\nnumber of bits, right?",
    "start": "3050300",
    "end": "3058020"
  },
  {
    "text": "And so a scalar\nquantizer is basically being defined by the\ndecision thresholds.",
    "start": "3058020",
    "end": "3063119"
  },
  {
    "text": "So you are selecting\nthe regions which will be mapped to a\nsingle value and then",
    "start": "3063120",
    "end": "3069300"
  },
  {
    "text": "the corresponding\nquantized values. Again, in the previous\nexample, my decision regions",
    "start": "3069300",
    "end": "3076930"
  },
  {
    "text": "were X greater than\n0, X less than 0. So my decision threshold was 0. And my quantize values were plus\nminus square root 2 over pi.",
    "start": "3076930",
    "end": "3085300"
  },
  {
    "text": "But more generally,\nI can select-- given any bit rate, I\ncan select the codebook,",
    "start": "3085300",
    "end": "3091750"
  },
  {
    "text": "so I know exactly\nthe quantized values which are going to be\npart of that codebook. And then I can select\nthese decision thresholds.",
    "start": "3091750",
    "end": "3099430"
  },
  {
    "start": "3099430",
    "end": "3104940"
  },
  {
    "text": "Again, like here, this\nparticular quantized value was determined\nbased on optimizing",
    "start": "3104940",
    "end": "3111609"
  },
  {
    "text": "MSC, where we did expected\nvalue-- we calculated this expected value. But I could have chosen this.",
    "start": "3111610",
    "end": "3118370"
  },
  {
    "text": "I could have chosen\nsomething else. So these two elements completely\ndetermine a scalar quantizer.",
    "start": "3118370",
    "end": "3125960"
  },
  {
    "start": "3125960",
    "end": "3131609"
  },
  {
    "text": "And then, OK, so I\ndon't have it here, but something which you see\nis your decision regions",
    "start": "3131610",
    "end": "3138150"
  },
  {
    "text": "need not be uniform. Nobody is saying that\nthey need to be uniform. So basically what I mean is\neach of these widths on x,",
    "start": "3138150",
    "end": "3147630"
  },
  {
    "text": "they can be different. That's still a scalar quantizer. You can decide what\nthese needs to be.",
    "start": "3147630",
    "end": "3153390"
  },
  {
    "text": "And that can be determined based\non whatever your distortion function really is. ",
    "start": "3153390",
    "end": "3161990"
  },
  {
    "text": "OK, great. Everyone with me? ",
    "start": "3161990",
    "end": "3169210"
  },
  {
    "text": "So then-- ",
    "start": "3169210",
    "end": "3176797"
  },
  {
    "text": "We looked at quantization. We understand this method. Seems very simple. ",
    "start": "3176797",
    "end": "3182430"
  },
  {
    "text": "Any suggestions on what more we\ncan do with this quantization?",
    "start": "3182430",
    "end": "3187680"
  },
  {
    "text": "Again, looking back at maybe\nwhat we did in lossless compression. ",
    "start": "3187680",
    "end": "3194680"
  },
  {
    "text": "So right now, remember,\na scheme right now-- sorry, that page somehow\nvanished, which I just wrote.",
    "start": "3194680",
    "end": "3204145"
  },
  {
    "text": " Yeah, OK, so what did we do?",
    "start": "3204145",
    "end": "3211040"
  },
  {
    "text": "We took one symbol at a time. We decide where it lies in\nthe corresponding decision",
    "start": "3211040",
    "end": "3217460"
  },
  {
    "text": "threshold. And then we map it to some\nquantized value X hat. We encode it, transmit\nit, do whatever.",
    "start": "3217460",
    "end": "3223970"
  },
  {
    "text": " Can we do something--",
    "start": "3223970",
    "end": "3229450"
  },
  {
    "text": "any other ideas, actually? Again, you can think about what\nwe think we did in lossless.",
    "start": "3229450",
    "end": "3236537"
  },
  {
    "text": "Based on our previous\nsymbol, we could adjust the decision thresholds. OK, so one idea\nwhich I got was--",
    "start": "3236537",
    "end": "3243510"
  },
  {
    "text": "one student said based\non the previous symbol, we can adjust the threshold. That's actually a\nvery valid idea.",
    "start": "3243510",
    "end": "3249789"
  },
  {
    "text": "People do that, but that's\none step even more ahead. We are not going to talk\nabout this in today's class.",
    "start": "3249790",
    "end": "3255700"
  },
  {
    "text": "But again, I have some added\nsome references in the notes. So if you want to look into\nthat, you can look into that.",
    "start": "3255700",
    "end": "3261880"
  },
  {
    "text": "We'll see some examples of\nthat when we learn about, I think, image codecs,\nlike BPG, et cetera.",
    "start": "3261880",
    "end": "3268810"
  },
  {
    "text": "But that's a\nperfectly valid idea. Even before that, think of this\nas a parallel to lecture 1 or 2,",
    "start": "3268810",
    "end": "3275559"
  },
  {
    "text": "when we started with lossless. ",
    "start": "3275560",
    "end": "3286970"
  },
  {
    "text": "What was one thing which always\ngave you a better compression if you didn't care\nabout complexity? ",
    "start": "3286970",
    "end": "3295985"
  },
  {
    "text": "Sorry?  Yes, so the answer is-- yeah,\nit's like combining two blocks.",
    "start": "3295985",
    "end": "3303220"
  },
  {
    "text": "So that was one idea which\nseemed to have worked really well for lossless compression.",
    "start": "3303220",
    "end": "3309010"
  },
  {
    "text": "Question is, can we use it-- does it work well? Or how does it do for\nthe lossy compression?",
    "start": "3309010",
    "end": "3315369"
  },
  {
    "text": "Maybe combining\nsymbols together, we could have done better\ninstead of just deciding based on a single symbol.",
    "start": "3315370",
    "end": "3322030"
  },
  {
    "text": "And that's exactly the idea\nof vector quantization.",
    "start": "3322030",
    "end": "3327080"
  },
  {
    "text": "So scalar to vector, vector kind\nof refers to, in some sense,",
    "start": "3327080",
    "end": "3332570"
  },
  {
    "text": "the symbols in a block. [INAUDIBLE] idea is exactly--",
    "start": "3332570",
    "end": "3337985"
  },
  {
    "text": "I have written the same like how\nthe student put it down maybe. Maybe we can work with two\nsymbols at a time or three",
    "start": "3337985",
    "end": "3343220"
  },
  {
    "text": "or N symbols at a time. Can it do something better? ",
    "start": "3343220",
    "end": "3349840"
  },
  {
    "text": "To start with the\ndiscussion, let's say we are working with\njust two symbols.",
    "start": "3349840",
    "end": "3354880"
  },
  {
    "text": "So then you can think of your\nX as blocks of two, right?",
    "start": "3354880",
    "end": "3359946"
  },
  {
    "text": "So you are just-- so there are\nmany different ways of thinking of vector quantization.",
    "start": "3359947",
    "end": "3365020"
  },
  {
    "text": "So you can think of it as like\nyou generated two N symbols and converted into two\ngroups of sizes N each.",
    "start": "3365020",
    "end": "3372190"
  },
  {
    "text": "So there's like\nsimilar block codes. I'll just take the two symbol,\ntwo symbol, two symbol, two symbol, and then I'll try\nto quantize this vector instead",
    "start": "3372190",
    "end": "3379990"
  },
  {
    "text": "of quantizing just the scalar. [INAUDIBLE] ",
    "start": "3379990",
    "end": "3386610"
  },
  {
    "text": "It could also have been-- instead of block\ncodes, [INAUDIBLE] is a bit [INAUDIBLE]. Instead of block, you\ncould have thought of it",
    "start": "3386610",
    "end": "3394050"
  },
  {
    "text": "as two sensors which are\nmeasuring the same source. So you had-- so\none way is you can",
    "start": "3394050",
    "end": "3399420"
  },
  {
    "text": "think you had a single\ntemperature sensor, and then you were\nclubbing every two symbol, and that's a vector of size two,\nand then you will do something",
    "start": "3399420",
    "end": "3406320"
  },
  {
    "text": "on top of it. But maybe it was just\ntwo different sensors, two different time series. And then that can be represented\n[AUDIO OUT] individually.",
    "start": "3406320",
    "end": "3418340"
  },
  {
    "text": "Or you could have even had two\ndifferent sensors just measuring two different sources.",
    "start": "3418340",
    "end": "3423432"
  },
  {
    "text": "You could have one sensor\nmeasuring temperature of one room, another\nmeasuring of another room.",
    "start": "3423432",
    "end": "3429579"
  },
  {
    "text": "So vector is just like-- you could have combined\nany two tuples, and they would\nhave made a vector.",
    "start": "3429580",
    "end": "3435080"
  },
  {
    "text": "So it's a bit more general. But the basic way to\nthink about it really is--",
    "start": "3435080",
    "end": "3440740"
  },
  {
    "text": "as a student also pointed out,\nit's basically block codes. ",
    "start": "3440740",
    "end": "3448160"
  },
  {
    "text": "OK, so now let's say\nI have specific case.",
    "start": "3448160",
    "end": "3453319"
  },
  {
    "text": "I have a block size of two.  And I want to design\na vector [AUDIO OUT]",
    "start": "3453320",
    "end": "3461120"
  },
  {
    "text": "the reasons and\ndecision thresholds. Instead of along a line,\nlike we did in the Gaussian,",
    "start": "3461120",
    "end": "3466730"
  },
  {
    "text": "we'll do it over a 2D plane, and\nI'll show you some [AUDIO OUT] do it. ",
    "start": "3466730",
    "end": "3480680"
  },
  {
    "text": "So I want to compare it,\nlike whether it helps or not. I want to compare it with\nthe one bit per symbol scalar",
    "start": "3480680",
    "end": "3487490"
  },
  {
    "text": "quantizer.  What is the size of\ncodebook allowed now?",
    "start": "3487490",
    "end": "3495530"
  },
  {
    "text": "So again, remember this game. Now I'll map each X1 X2--",
    "start": "3495530",
    "end": "3502960"
  },
  {
    "text": "let me actually\njust write it down. So you'll have a\ncodebook, which is, let's say, indexed by 1,\n2, N, some size codebook,",
    "start": "3502960",
    "end": "3510100"
  },
  {
    "text": "code vectors. You get a tuple,\nlet's say, X1 X2.",
    "start": "3510100",
    "end": "3517660"
  },
  {
    "text": "And you want to find-- you'll map it to one\nof these indices.",
    "start": "3517660",
    "end": "3523780"
  },
  {
    "text": "And then encode\nthat index, right? So what can be the size of\nyour codebook if you were--",
    "start": "3523780",
    "end": "3531010"
  },
  {
    "text": "if your rate was\n1 bit per symbol IF that's what you\nwant to work with?",
    "start": "3531010",
    "end": "3536440"
  },
  {
    "start": "3536440",
    "end": "3541460"
  },
  {
    "text": "4. OK, 4 is the answer\nby this student. Why 4? Because you have 2\nsymbols equals 2 bits",
    "start": "3541460",
    "end": "3548174"
  },
  {
    "text": "so you'll have 4 symbols. Exactly. So the student said\nwe have 2 symbols.",
    "start": "3548174",
    "end": "3554359"
  },
  {
    "text": "So basically 1 bit\nper symbol-- so this is exact same logic as we worked\nthrough during the block codes.",
    "start": "3554360",
    "end": "3561050"
  },
  {
    "text": "This allows for 2 bit\nper block, correct?",
    "start": "3561050",
    "end": "3566470"
  },
  {
    "text": "And then this basically\nallows-- so as I said, we are encoding block as\none element of the codebook.",
    "start": "3566470",
    "end": "3572980"
  },
  {
    "text": "So this basically says I can\nhave 4 distinct codewords. ",
    "start": "3572980",
    "end": "3583610"
  },
  {
    "text": "So now doing this\nthing basically allowed you to increase\nthe number of codewords which you can have. ",
    "start": "3583610",
    "end": "3591840"
  },
  {
    "text": "More generally, so 4 kind of\ncomes from, again, this 1 bit--",
    "start": "3591840",
    "end": "3597150"
  },
  {
    "text": "oh, sorry-- this 1 bit per symbol and 2\nsymbol per code vector, which",
    "start": "3597150",
    "end": "3604080"
  },
  {
    "text": "basically gives you 4. Generalizing this exact thing,\nso let's say you have vectors--",
    "start": "3604080",
    "end": "3610950"
  },
  {
    "text": "or if you are more comfortable\nwith the idea of blocks, blocks which are of size k, and your\nrate is R bits per symbol,",
    "start": "3610950",
    "end": "3618089"
  },
  {
    "text": "then your codebook is of\nthe size 2 to R times k. It's like generalizing\nthe exact same thing.",
    "start": "3618090",
    "end": "3624890"
  },
  {
    "start": "3624890",
    "end": "3632000"
  },
  {
    "text": "In other words, your rate\nis just-- again, sorry, there should be-- yeah, there should be ceil\neverywhere, so I apologize.",
    "start": "3632000",
    "end": "3640819"
  },
  {
    "text": "But I hope you guys understand\nthe ceil part for now. Your rate [AUDIO OUT] per symbol\nif you have an N-size codeword.",
    "start": "3640820",
    "end": "3651020"
  },
  {
    "text": " So now, here, what I'm\nshowing is a 2D Gaussian.",
    "start": "3651020",
    "end": "3660510"
  },
  {
    "text": "So here, I'm plotting X1-- ",
    "start": "3660510",
    "end": "3666510"
  },
  {
    "text": "nothing fancy. It's just same X1, which\nwas standard Gaussian so it's symmetric around 0.",
    "start": "3666510",
    "end": "3671520"
  },
  {
    "text": "This is X2, which is\nagain symmetric around 0. So this is how your\ntuples might look like.",
    "start": "3671520",
    "end": "3677190"
  },
  {
    "text": "So for example, your one\nsymbol might look like this, and that corresponds\nto, I don't know, X1 2--",
    "start": "3677190",
    "end": "3683609"
  },
  {
    "text": "X2 1.5, let's say. So these are just a PDF\nof the static function",
    "start": "3683610",
    "end": "3692270"
  },
  {
    "text": "of the two-dimensional Gaussian. Let's say I want to represent\nit with 1 bits per symbol.",
    "start": "3692270",
    "end": "3698539"
  },
  {
    "text": "So you guys just told me that\nI can have four codewords.",
    "start": "3698540",
    "end": "3706500"
  },
  {
    "text": "Correct?  So now any thoughts on\nwhat words could be?",
    "start": "3706500",
    "end": "3714090"
  },
  {
    "start": "3714090",
    "end": "3721362"
  },
  {
    "text": "Well, let's not start\nwith the codewords. Let's start with the same thing. ",
    "start": "3721362",
    "end": "3727820"
  },
  {
    "text": "After determine some decisions\n[AUDIO OUT] more obvious ones.",
    "start": "3727820",
    "end": "3733620"
  },
  {
    "text": "Let's not worry about\nthe optimality at all. If you were given this,\nwhat would you do?",
    "start": "3733620",
    "end": "3740180"
  },
  {
    "text": "Split it into quadrants. Split it into quadrants, OK. So that's-- so we'll just\nlook through some examples.",
    "start": "3740180",
    "end": "3749270"
  },
  {
    "text": "So one is-- so let's\nsay this is origin. And so what I can do is I\ncan split it into quadrants.",
    "start": "3749270",
    "end": "3758920"
  },
  {
    "text": "So these are my-- oops, these are my\ndecision thresholds.",
    "start": "3758920",
    "end": "3765490"
  },
  {
    "text": " OK, let's do this.",
    "start": "3765490",
    "end": "3771490"
  },
  {
    "text": "OK, so these are my four\ndifferent decision thresholds. And since this\nsource is symmetric,",
    "start": "3771490",
    "end": "3777700"
  },
  {
    "text": "I know somewhere in the circle\nbasically I can have the--",
    "start": "3777700",
    "end": "3784740"
  },
  {
    "text": "because it's\nspherically symmetric, so your quantized values\nwould be somewhere here if this was like--",
    "start": "3784740",
    "end": "3790200"
  },
  {
    "text": " especially for-- let's just\nstick with the distortion",
    "start": "3790200",
    "end": "3795590"
  },
  {
    "text": "mean square error. Let's not complicate things. So let's say our distortion\nis mean square error. This is option one. ",
    "start": "3795590",
    "end": "3805549"
  },
  {
    "text": "OK, this is option one. Let's say your friend\ncomes and says--",
    "start": "3805550",
    "end": "3811970"
  },
  {
    "text": "let's see. Your friend comes and\nsays, I don't like this. ",
    "start": "3811970",
    "end": "3819500"
  },
  {
    "text": "I don't like my symbols\nbeing in the quadrants.",
    "start": "3819500",
    "end": "3825800"
  },
  {
    "text": "I want to have some quantize\nsymbols on the axis. Again, this would be\nat the same radius.",
    "start": "3825800",
    "end": "3832820"
  },
  {
    "text": "Let's not even worry\nabout the radius so that you can figure out\nusing the similar minimum mean square estimator kind of logic.",
    "start": "3832820",
    "end": "3839825"
  },
  {
    "text": " So what are the decision\nregions in this case? ",
    "start": "3839825",
    "end": "3848850"
  },
  {
    "text": "Yeah, OK, very nice. Got the visual decision-- vision from some students. So in this case,\nyou're going to look",
    "start": "3848850",
    "end": "3855690"
  },
  {
    "text": "at the point which is\nlike halfway through, which is nearest to--",
    "start": "3855690",
    "end": "3860730"
  },
  {
    "text": " I wanted that to-- ",
    "start": "3860730",
    "end": "3868920"
  },
  {
    "text": "OK, nice. OK, so we are going\nto look at this point.",
    "start": "3868920",
    "end": "3875150"
  },
  {
    "text": "Similarly, your decision regions\nwill turn out to be this.",
    "start": "3875150",
    "end": "3880460"
  },
  {
    "text": "OK, any ideas? Let's say if we were doing\nmean square estimation, which",
    "start": "3880460",
    "end": "3885710"
  },
  {
    "text": "of these two would be better? ",
    "start": "3885710",
    "end": "3896390"
  },
  {
    "text": "Shouldn't it be the same? Yeah, so one of\nthe students says-- [INAUDIBLE] so you should\nbe able to rotate it.",
    "start": "3896390",
    "end": "3904050"
  },
  {
    "text": "Exactly. So it should be the same. One of the students\nsays it should be the same because our\ndistribution is spherically",
    "start": "3904050",
    "end": "3909925"
  },
  {
    "text": "symmetric. It doesn't matter how am I\nrotating this whole thing. If you know of any orthonormal\nbasis, you can use that.",
    "start": "3909925",
    "end": "3916410"
  },
  {
    "text": "And you'll basically\nget the same thing. The idea really here is that-- what I want to highlight\nquickly is your code",
    "start": "3916410",
    "end": "3924240"
  },
  {
    "text": "vectors could be anything. They could be complicated in\nyour coordinate system, maybe",
    "start": "3924240",
    "end": "3929640"
  },
  {
    "text": "not so complicated in\nyour coordinate system. It doesn't matter\nbecause you only have to send the index\nof the code vector.",
    "start": "3929640",
    "end": "3936494"
  },
  {
    "start": "3936495",
    "end": "3941630"
  },
  {
    "text": "But let me be, I don't know,\na bit more adventurous.",
    "start": "3941630",
    "end": "3946730"
  },
  {
    "text": "And let's say I pick one of my\ncodewords to be in the center.",
    "start": "3946730",
    "end": "3952160"
  },
  {
    "text": "I'm very greedy. I don't know. I want origin. I love origin. So I picked one of\nmy codewords here.",
    "start": "3952160",
    "end": "3960050"
  },
  {
    "text": "OK, in that case, again, going\nby the spherical symmetry,",
    "start": "3960050",
    "end": "3965060"
  },
  {
    "text": "my other three codewords will\nlie somewhere on a circle. And it doesn't matter\nwhere exactly but somewhere",
    "start": "3965060",
    "end": "3972079"
  },
  {
    "text": "spherically distributed. And in this case,\nmy decision regions",
    "start": "3972080",
    "end": "3977892"
  },
  {
    "text": "could be determined by\nbasically what you are doing. You are basically having\na separating hyperplane between any of these two points.",
    "start": "3977893",
    "end": "3985500"
  },
  {
    "text": "So it should pass\nthrough origin. ",
    "start": "3985500",
    "end": "3990770"
  },
  {
    "text": "So these are just the\nseparating hyper planes between any of these points.",
    "start": "3990770",
    "end": "3996440"
  },
  {
    "text": "And so your decision boundaries\nwould just be this, right?",
    "start": "3996440",
    "end": "4001579"
  },
  {
    "start": "4001580",
    "end": "4008480"
  },
  {
    "text": "These would be your\ndecision boundaries. What this means is if\nyou get a point X1, X2 here, you would map it\nto this particular guy.",
    "start": "4008480",
    "end": "4015277"
  },
  {
    "text": "If you get it here, map\nit to this particular guy. So on and so forth. ",
    "start": "4015277",
    "end": "4020800"
  },
  {
    "text": "And actually, in this\ncase, it's interesting that you can show that\nthis is not optimal.",
    "start": "4020800",
    "end": "4028120"
  },
  {
    "text": "Actually, I have a notebook\nwhich we'll see in a second",
    "start": "4028120",
    "end": "4034390"
  },
  {
    "text": "before we head out\nfor today, where we can see that this\nkind of performs better. ",
    "start": "4034390",
    "end": "4043700"
  },
  {
    "text": "OK, so more formally, what did\nwe do in vector quantization? So this is, again,\njust generalization",
    "start": "4043700",
    "end": "4050660"
  },
  {
    "text": "of what we saw before. So your vector quantizer\nis just a mapping",
    "start": "4050660",
    "end": "4057500"
  },
  {
    "text": "from some k dimensional space to\na codebook, where a codebook is N dimensional vectors.",
    "start": "4057500",
    "end": "4063110"
  },
  {
    "text": "So it's a dictionary\nwhich is comprising of N k-dimensional vectors.",
    "start": "4063110",
    "end": "4068300"
  },
  {
    "text": "Then you have a mapping\nfrom this vector x, which is in k dimension,\nyour function Q to y bar.",
    "start": "4068300",
    "end": "4074930"
  },
  {
    "text": "And then, where if x belongs to\nsome partition of this space,",
    "start": "4074930",
    "end": "4081780"
  },
  {
    "text": "you will basically map\nit to the i-th codeword. So for example here,\nin this example,",
    "start": "4081780",
    "end": "4090200"
  },
  {
    "text": "this was one partition\nof the space. This was another\npartition of the space.",
    "start": "4090200",
    "end": "4097310"
  },
  {
    "text": "And so this is just\nmathematically. So your partition should\nspan the whole space",
    "start": "4097310",
    "end": "4103910"
  },
  {
    "text": "so that any vector which\nis coming in that space should be mapped to\nsome quantized value.",
    "start": "4103910",
    "end": "4109195"
  },
  {
    "text": "Otherwise, you can\nhave a symbol which you don't know where to map to. And then in this case, we saw\nthat the rate will be just",
    "start": "4109196",
    "end": "4116210"
  },
  {
    "text": "log N by k bits per sample.  Good?",
    "start": "4116210",
    "end": "4122920"
  },
  {
    "text": "So I want to highlight\na couple of things, like vector quantization.",
    "start": "4122920",
    "end": "4128040"
  },
  {
    "text": "So basically, the kind of\nbenefits we can exploit is that it can obviously exploit\nthe dependence between vector",
    "start": "4128040",
    "end": "4134040"
  },
  {
    "text": "components, so that's\nwhat we are working with. I'll give you an example of\nthis thing really quickly.",
    "start": "4134040",
    "end": "4140549"
  },
  {
    "text": "And another thing is we can\nhave much more general decision regions. So to give an example\nof both of these things,",
    "start": "4140550",
    "end": "4148020"
  },
  {
    "text": "let's say you had some\ndistribution X1, X2 in 2D space. Instead of Gaussian, let's say\nit looks something like this.",
    "start": "4148020",
    "end": "4156240"
  },
  {
    "text": "And this just basically\nmeans it's like X1, X2 always have the same sign. If X1 is positive,\nX2 is positive.",
    "start": "4156240",
    "end": "4162330"
  },
  {
    "text": "If X1 is negative, x2 is\nnegative, so on and so forth.",
    "start": "4162330",
    "end": "4168330"
  },
  {
    "text": "If you were to do\na uniform scalar quantization of this\nthing, what were you doing?",
    "start": "4168330",
    "end": "4174270"
  },
  {
    "text": "You were basically making a\ngrid along x and a grid along y. And your decision regions would\nhave looked like these blocks",
    "start": "4174270",
    "end": "4182609"
  },
  {
    "text": "here. And in this particular\nexample, you can work it out since\nI have 36 blocks.",
    "start": "4182609",
    "end": "4190285"
  },
  {
    "text": " For vector of size 2, it's\napproximately log 2 the base 6.",
    "start": "4190285",
    "end": "4199560"
  },
  {
    "text": "This is your rate. But obviously, if I knew this\nfact that my distribution always",
    "start": "4199560",
    "end": "4206030"
  },
  {
    "text": "is correlated, like X1,\nX2 are always together, positive or negative, I\ncould have instead done",
    "start": "4206030",
    "end": "4211220"
  },
  {
    "text": "vector quantization. And I could have only\ndefined my regions as these square\nblocks here, which",
    "start": "4211220",
    "end": "4218060"
  },
  {
    "text": "overlap with my actual density. In that case, I don't need\nall the different codewords",
    "start": "4218060",
    "end": "4225470"
  },
  {
    "text": "corresponding to the other side. And in this case, you\ncan again work it out.",
    "start": "4225470",
    "end": "4231019"
  },
  {
    "text": "But it's just\nbasically log 18 by 2. So that's your bits, which is\nI think something like this.",
    "start": "4231020",
    "end": "4240960"
  },
  {
    "text": "So in this case, you can save\nhalf a bit for, it's obvious, same distortion because we\nhave the exact same regions",
    "start": "4240960",
    "end": "4250630"
  },
  {
    "text": "in the decision regions. So this is how\nvector quantization can be really helpful.",
    "start": "4250630",
    "end": "4255969"
  },
  {
    "text": "So it can help you\nexploit correlation. ",
    "start": "4255970",
    "end": "4262310"
  },
  {
    "text": "But actually, it's much\nstronger than that. And we'll not cover\nthis in class.",
    "start": "4262310",
    "end": "4269060"
  },
  {
    "text": "Again, this goes into a little\nbit of signal processing. But you can work out some\ncomments without proofs.",
    "start": "4269060",
    "end": "4276260"
  },
  {
    "text": "And I think we have some\nproofs again in the notes. So if you are-- if you are\nmathematically inclined, interested, I\nwould encourage you",
    "start": "4276260",
    "end": "4282445"
  },
  {
    "text": "to go read them, work them out. But basically, what you can\nshow that optimal regions are generally not uniform,\nwhich is basically",
    "start": "4282445",
    "end": "4289610"
  },
  {
    "text": "saying scalar quantization,\neven in simple uniform IID case. So in previous example, it was--",
    "start": "4289610",
    "end": "4296719"
  },
  {
    "text": "you had correlated sources. But what now I'm saying is you\ncan show that mathematically. Like, if you were to break\nit down into square grids,",
    "start": "4296720",
    "end": "4304190"
  },
  {
    "text": "that's not optimal even\nfor uniform IID case. And the idea really is you can\nhave an hexagonal lattice, so",
    "start": "4304190",
    "end": "4313190"
  },
  {
    "text": "a uniform--  a uniform encoder\nreally is what it's--",
    "start": "4313190",
    "end": "4322650"
  },
  {
    "text": "let's see, sorry. ",
    "start": "4322650",
    "end": "4328350"
  },
  {
    "text": "So a uniform encoder\nis grids like this,",
    "start": "4328350",
    "end": "4334770"
  },
  {
    "text": "make square grids like this,\nsomething for uniform IID scalar quantization. ",
    "start": "4334770",
    "end": "4343409"
  },
  {
    "text": "Again, so I have determined\nsome decision regions. I have determined the\ncodebook contains values. What I'm saying is\nthere is another grid.",
    "start": "4343410",
    "end": "4351000"
  },
  {
    "text": "So if I take my quantize values\nas shifted half from each other,",
    "start": "4351000",
    "end": "4357780"
  },
  {
    "text": "like somewhere--\nso each next row is in between the\nnext one, something like a honeycomb structure,\nand if these were my decision",
    "start": "4357780",
    "end": "4365220"
  },
  {
    "text": "regions, you can work\nout that, I don't know, the region corresponding to one\nthing would look like hexagonal.",
    "start": "4365220",
    "end": "4373050"
  },
  {
    "text": "And this vector\nquantizer is actually better than the first one.",
    "start": "4373050",
    "end": "4378300"
  },
  {
    "text": "And you can work out the math. And it basically turns out\nto be-- like for same rate,",
    "start": "4378300",
    "end": "4384480"
  },
  {
    "text": "it turns out to be-- your MSE turns out to\nbe around 4% lower.",
    "start": "4384480",
    "end": "4389550"
  },
  {
    "text": " This is a good idea to know. And this is called lattice\nquantization or Voronoi",
    "start": "4389550",
    "end": "4397020"
  },
  {
    "text": "diagrams. And you can also accommodate\nmore than two dimensions. So this is just a hexagonal\nlattice in two dimension.",
    "start": "4397020",
    "end": "4403980"
  },
  {
    "text": "But it's actually--\nnow you can see you can get very\ncreative with respect to this lattice design\nand your distortion.",
    "start": "4403980",
    "end": "4409860"
  },
  {
    "text": "And that can actually\ndo much better than just doing\nscalar quantization.",
    "start": "4409860",
    "end": "4415740"
  },
  {
    "text": "In fact, this paper, Ziv,\non universal quantization--",
    "start": "4415740",
    "end": "4421680"
  },
  {
    "text": "OK, sorry, before that, so there\nare two comments here you see. One is that, obviously, vector\nquantization helps, even",
    "start": "4421680",
    "end": "4430650"
  },
  {
    "text": "in the most trivial of cases. But the second thing\nis it helps, but is it",
    "start": "4430650",
    "end": "4436740"
  },
  {
    "text": "like scalar uniform quantizer? I don't have to do anything. I just divided my\nthing and fixed bits,",
    "start": "4436740",
    "end": "4442200"
  },
  {
    "text": "didn't have to think anything? Like, 32-bit floats,\nthat's my scalar vector quantized-- scalar quantizer,\nuniform scalar quantizer.",
    "start": "4442200",
    "end": "4448710"
  },
  {
    "text": "This one I had to\nfigure out, oh, this is how I need to\nwork out my vectors. This needs to be the letters.",
    "start": "4448710",
    "end": "4455170"
  },
  {
    "text": "And this is the optimal one in\nthat it gives like 4% benefit.",
    "start": "4455170",
    "end": "4460510"
  },
  {
    "text": "So the takeaway really is that-- and this is a paper by Ziv,\nwho is like same Lempel Ziv.",
    "start": "4460510",
    "end": "4469840"
  },
  {
    "text": "And here he shows that\nfor certain cases-- we'll not go into details-- for certain cases, you can show\nthat scalar quantizer doesn't",
    "start": "4469840",
    "end": "4478599"
  },
  {
    "text": "take more than 0.754\nbits per sample higher than vector\nfor any design.",
    "start": "4478600",
    "end": "4484540"
  },
  {
    "text": " OK, so in general, your optimal\nregions are not easy to compute",
    "start": "4484540",
    "end": "4493220"
  },
  {
    "text": "and may even give\nyou very less value addition to whatever\nquantization scheme",
    "start": "4493220",
    "end": "4498740"
  },
  {
    "text": "you are designing. So you need to be\ncareful when designing on how much computational\ncomplexity am",
    "start": "4498740",
    "end": "4505320"
  },
  {
    "text": "I going to increase if I'm\ndesigning a practical compressor around vector quantization. So you can see it's like the\nsame idea around the block",
    "start": "4505320",
    "end": "4512130"
  },
  {
    "text": "codes. Block codes are amazing. They'll give you\ngreat performance, great theoretical guarantees. But they're exponential\nin terms of complexity.",
    "start": "4512130",
    "end": "4519810"
  },
  {
    "text": "And that really makes them\nrare in practice relatively. And we had to come\naround workarounds.",
    "start": "4519810",
    "end": "4526410"
  },
  {
    "text": "Things similar is the case\nwith vector quantization. ",
    "start": "4526410",
    "end": "4533910"
  },
  {
    "text": "OK, so just the last thing. I'll go five minutes over. ",
    "start": "4533910",
    "end": "4541710"
  },
  {
    "text": "OK, so in general, we saw that\nwe got this hexagonal lattice we had to work out the maths.",
    "start": "4541710",
    "end": "4546720"
  },
  {
    "text": "We actually didn't\nshow the maths, but you can look at the notes. Or even before, we had\nto really figure out",
    "start": "4546720",
    "end": "4552480"
  },
  {
    "text": "what's the optimal value. And it's kind of\nobvious, as we are",
    "start": "4552480",
    "end": "4558210"
  },
  {
    "text": "working through this, that your\nquantize value and decision regions are intertwined.",
    "start": "4558210",
    "end": "4565560"
  },
  {
    "text": "The quantize value will\ndepend on the decision region, and the decision\nregion will depend on the nearby quantize values.",
    "start": "4565560",
    "end": "4572250"
  },
  {
    "text": "And these things are\nreally hard to compute. And so one way to\nresolve this in practice",
    "start": "4572250",
    "end": "4579489"
  },
  {
    "text": "is we can do some sort of\nan iterative algorithm. ",
    "start": "4579490",
    "end": "4584950"
  },
  {
    "text": "Before I move on, have\nyou guys seen this before, this vector\nquantization idea? ",
    "start": "4584950",
    "end": "4596980"
  },
  {
    "text": "Think about what we\nare trying to do. ",
    "start": "4596980",
    "end": "4605300"
  },
  {
    "text": "Sorry? Clustering. Clustering. OK, exactly. So one of the students\nsaid clustering.",
    "start": "4605300",
    "end": "4611179"
  },
  {
    "text": "So we are exactly\ndoing clustering. Given the sample\npoints, we are trying to figure out the decision\nregions, clusters,",
    "start": "4611180",
    "end": "4617840"
  },
  {
    "text": "in other words, under which like\nwe have one quantized value.",
    "start": "4617840",
    "end": "4623400"
  },
  {
    "text": "So if you know about\nclustering, you would have heard of the k-means\nalgorithm in one of your machine learning class.",
    "start": "4623400",
    "end": "4629810"
  },
  {
    "text": "And basically,\n[INAUDIBLE] is very similar to k-means algorithm. In fact, it's the same thing.",
    "start": "4629810",
    "end": "4637070"
  },
  {
    "text": "It's also called Lloyd-Max\nalgorithm or Generalized Lloyd algorithm if you are coming from\na compression angle or signal",
    "start": "4637070",
    "end": "4644870"
  },
  {
    "text": "processing or quantization way. But if you witnessed ML\nand clustering before, you would have seen\nthis as k-means.",
    "start": "4644870",
    "end": "4651710"
  },
  {
    "text": "And k-means is just a special\ncase of Generalized Lloyd. Generalized Lloyd is a bit more\ngeneral, as the name suggests.",
    "start": "4651710",
    "end": "4659240"
  },
  {
    "text": "And we are not going\nto look into it. But basically, if you want\nto-- so the thing which I say,",
    "start": "4659240",
    "end": "4664310"
  },
  {
    "text": "if you want to cluster your\ndata points into N clusters corresponding to\nthe codebook-- so we",
    "start": "4664310",
    "end": "4671140"
  },
  {
    "text": "had the endpoints\nin the codebook. You can think of them as N\nclusters, which is basically",
    "start": "4671140",
    "end": "4678740"
  },
  {
    "text": "the same as k in k-means.  And you choose these points\nsuch that the average distortion",
    "start": "4678740",
    "end": "4685720"
  },
  {
    "text": "is minimized.  And this is just like\na fun historical note,",
    "start": "4685720",
    "end": "4692590"
  },
  {
    "text": "which you can use. It's actually a\nvery old algorithm. It wasn't even\npublished in 1982.",
    "start": "4692590",
    "end": "4699190"
  },
  {
    "text": "And basically\nGeneralized Lloyd, which is specialized to\nthe squared error, is the k-means\nclustering algorithm,",
    "start": "4699190",
    "end": "4705750"
  },
  {
    "text": "which you would have seen. And so the main idea\nof k-means clustering is given some data\npoint [AUDIO OUT]",
    "start": "4705750",
    "end": "4713690"
  },
  {
    "text": "codebook and the corresponding\npartition of the data points. So you want to figure out-- I have been told what's\nmy N. I have been",
    "start": "4713690",
    "end": "4720860"
  },
  {
    "text": "told the size of my codebook. So in other words,\nrate is given. Now I need to figure out what\nshould be my optimal decision",
    "start": "4720860",
    "end": "4727250"
  },
  {
    "text": "regions and the quantized\nvalues, clusters, and decision boundaries. Now, how k-means work is you\nbasically do it iteratively.",
    "start": "4727250",
    "end": "4735440"
  },
  {
    "text": "So given a codebook,\nyou basically compute the best partition\nfor the data points. And then given a\npartition of the data",
    "start": "4735440",
    "end": "4742070"
  },
  {
    "text": "points, you compute\nthe optimal codebook. And then you keep repeating\nuntil convergence. ",
    "start": "4742070",
    "end": "4750960"
  },
  {
    "text": "So we don't have\ntime, so I'm not going to go into the\nalgorithmic details, but there is some\npseudocode here.",
    "start": "4750960",
    "end": "4757260"
  },
  {
    "text": "And the idea is,\nagain, very simple. It's quite detailed, so\nwe can-- you can go over. But you start with some\ncentroids, which are,",
    "start": "4757260",
    "end": "4764440"
  },
  {
    "text": "other words, your quantized\nvalues, till some convergence. You first determine clusters.",
    "start": "4764440",
    "end": "4771120"
  },
  {
    "text": "So you basically assign each\nsymbol or data to some centroid. And then, given these clusters,\nyou compute the new centroids.",
    "start": "4771120",
    "end": "4780280"
  },
  {
    "text": "And you keep doing this till\nsome convergence is hit. And that gives you these\nclusters and centroids. And something to just\nremember, don't get confused, k",
    "start": "4780280",
    "end": "4788590"
  },
  {
    "text": "is the size of the codebook. K is the number of\nclusters in k-means, which we have been calling N.\nSo just k in our discussion",
    "start": "4788590",
    "end": "4799840"
  },
  {
    "text": "so far was the size\nof the vectors. ",
    "start": "4799840",
    "end": "4805369"
  },
  {
    "text": "And so you can go over this. Just quickly want to show\nthere is this notebook.",
    "start": "4805370",
    "end": "4812105"
  },
  {
    "start": "4812105",
    "end": "4818160"
  },
  {
    "text": "OK, so this is just a\nnotebook on k-means. So I did a bunch of stuff.",
    "start": "4818160",
    "end": "4823890"
  },
  {
    "text": "All the data from here\ncan be taught here. So again, we talked\nabout this as just a 2D--",
    "start": "4823890",
    "end": "4830400"
  },
  {
    "text": "oops, just like the 2D Gaussian. I just generated some\nnumber of samples. And then for k\nequals to 4-- sorry,",
    "start": "4830400",
    "end": "4838680"
  },
  {
    "text": "let's just say rate\n1 bits per symbol-- so again, we want to determine\nfour different centroids",
    "start": "4838680",
    "end": "4846360"
  },
  {
    "text": "and region. We just run the\nk-means algorithm. And so it's an\niterative algorithm.",
    "start": "4846360",
    "end": "4852000"
  },
  {
    "text": "So you can see\nthat, OK, I started with these four different colors\nat four different regions. Four different crosses are the\nfour different quantize values",
    "start": "4852000",
    "end": "4859260"
  },
  {
    "text": "you would have used. And as you keep\nrunning this, you",
    "start": "4859260",
    "end": "4864599"
  },
  {
    "text": "get something which is\nspherically symmetrical. It converges to something\nspherically symmetrical.",
    "start": "4864600",
    "end": "4869760"
  },
  {
    "text": "So these four are your\nquantization points. And different colors\nare the regions. Similarly, you can do so with\nmore number of codewords.",
    "start": "4869760",
    "end": "4876570"
  },
  {
    "text": "You could have\ngiven more weight. And now you start with\nsomething like this. Oh, by the way, this is\nspherically symmetric",
    "start": "4876570",
    "end": "4883445"
  },
  {
    "text": "like we discussed earlier,\nso it seems right. And so if you had eight\ndifferent codewords,",
    "start": "4883445",
    "end": "4889040"
  },
  {
    "text": "you keep doing this. Now it looks\nsomething different. Now it looks something\nwhich turned out more",
    "start": "4889040",
    "end": "4894740"
  },
  {
    "text": "towards the other center. So you can see, like, k-means,\nby the way, is not unique. It depends on the initial points\nyou start working on it with.",
    "start": "4894740",
    "end": "4904310"
  },
  {
    "text": "And finally, what you\ncan do is do two things. You can look at it-- you can\nplot the distortion, which",
    "start": "4904310",
    "end": "4909740"
  },
  {
    "text": "is mean squared error,\naverage mean squared error. As your number of iterations\nare increasing in k-means, obviously that goes\ndown because it's trying",
    "start": "4909740",
    "end": "4916670"
  },
  {
    "text": "to optimize for the distortion. It's trying to find the points\nand regions which does that. And also, you can look at\nthe mean squared distortion",
    "start": "4916670",
    "end": "4926510"
  },
  {
    "text": "with respect to\nthe codebook size. So codebook size is\nthe rate, and you can look at what distortion\ndid it converge to.",
    "start": "4926510",
    "end": "4934160"
  },
  {
    "text": "And now you see that\nrate distortion curve kind of showing up\nhere very obviously.",
    "start": "4934160",
    "end": "4939652"
  },
  {
    "text": "So you have some mean\nsquared distortion. As you increase codebook\nsize, that decreases.",
    "start": "4939652",
    "end": "4945650"
  },
  {
    "text": "And GL is just\nGeneralized Lloyd. So I'll leave it here.",
    "start": "4945650",
    "end": "4951680"
  },
  {
    "text": "Actually, I want to just\nlet you guys know you can--",
    "start": "4951680",
    "end": "4957110"
  },
  {
    "text": "why is this not running? Sorry.  OK, so I also ran\nk-means on images.",
    "start": "4957110",
    "end": "4964949"
  },
  {
    "text": "So it's quite well commented,\nso you can go look at it. So I just chose, I don't know, a\nthousand different mnist images,",
    "start": "4964950",
    "end": "4973080"
  },
  {
    "text": "ran clustering on it. These were my different\ncluster centers. So in this case,\nif you think about,",
    "start": "4973080",
    "end": "4979080"
  },
  {
    "text": "I'm not working with\nvectors of size 2. I'm actually working\nwith-- so this is mnist, so",
    "start": "4979080",
    "end": "4984420"
  },
  {
    "text": "vectors of size 28 times 28. 28 squared, that's really my k.",
    "start": "4984420",
    "end": "4989910"
  },
  {
    "text": "And I can still do clustering. And I can assign bit words. And you can again look at MSE\nversus number of clusters.",
    "start": "4989910",
    "end": "4997469"
  },
  {
    "text": "And again, this is like a\nrate distortion kind of curve. So you can go and play\nwith this notebook.",
    "start": "4997470",
    "end": "5004670"
  },
  {
    "text": "So this is important because\nyour k need not be 2. It can be much higher. And this is like-- in some sense, you\nhave already started",
    "start": "5004670",
    "end": "5011330"
  },
  {
    "text": "looking into one idea of lossy\ncompression of images, which is I can just cluster\nmy images, and I",
    "start": "5011330",
    "end": "5016969"
  },
  {
    "text": "can assign bits accordingly. And yeah, so this\nis the curve I'm",
    "start": "5016970",
    "end": "5022130"
  },
  {
    "text": "going to leave you guys with. So here you see blue, which\nis what we have been-- we observed through k-means.",
    "start": "5022130",
    "end": "5029329"
  },
  {
    "text": "And you also see something\nin red, which says D of R 2 raise to power 2R.",
    "start": "5029330",
    "end": "5034610"
  },
  {
    "text": "And that's basically the\ntheoretical bound for Gaussian. And Tsachy will\ntalk more about how",
    "start": "5034610",
    "end": "5040580"
  },
  {
    "text": "you get that, like how do you\nshow that you couldn't have done anything better\nthan this red curve,",
    "start": "5040580",
    "end": "5045950"
  },
  {
    "text": "no matter whatever\nalgorithm you came up with, vector quantizer\npredictor, vector",
    "start": "5045950",
    "end": "5051800"
  },
  {
    "text": "quantizer for this Gaussian\nIID case, so on and so forth. Cool. Yeah, sorry, I went\nover five minutes.",
    "start": "5051800",
    "end": "5058173"
  },
  {
    "text": "But thanks for staying with me. ",
    "start": "5058173",
    "end": "5066000"
  }
]