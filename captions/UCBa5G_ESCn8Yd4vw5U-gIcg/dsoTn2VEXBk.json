[
  {
    "start": "0",
    "end": "6220"
  },
  {
    "text": "Yeah, so today we'll talk\nabout reinforcement learning. It's a new exciting topic.",
    "start": "6220",
    "end": "12370"
  },
  {
    "text": "It will be mostly a review. There was a tutorial yesterday\nthat I think some of you attended.",
    "start": "12370",
    "end": "18070"
  },
  {
    "text": "We'll go over similar things. And hopefully this is\nmostly a review for you",
    "start": "18070",
    "end": "24099"
  },
  {
    "text": "but we'll go into certain-- I think into quite\nsome detail into some",
    "start": "24100",
    "end": "29890"
  },
  {
    "text": "of those reinforcement\nlearning concepts. First, a few reminders. So today the project\nproposal is due.",
    "start": "29890",
    "end": "39370"
  },
  {
    "text": "This should be mostly\nhelpful to you. This will be a\nrather light review. But we want to make sure that\nthe project proposals make",
    "start": "39370",
    "end": "46000"
  },
  {
    "text": "sense and you're\non the right track to doing something exciting. And then another reminder\nis that homework 2",
    "start": "46000",
    "end": "55120"
  },
  {
    "text": "is due on Monday, next week. And also homework 3 will\nbe out on Monday as well.",
    "start": "55120",
    "end": "60399"
  },
  {
    "text": " All right, so\nreinforcement learning is a new topic in this class.",
    "start": "60400",
    "end": "67369"
  },
  {
    "text": "So I first wanted to start\nwith, why should we care? Why should we work on\nreinforcement learning?",
    "start": "67370",
    "end": "75010"
  },
  {
    "text": "And reinforcement\nlearning is a field that deals with\npredicting long term consequences of your\nactions and then",
    "start": "75010",
    "end": "83080"
  },
  {
    "text": "learning intelligent\nbehaviors based on that. And I was thinking how to\nintroduce reinforcement",
    "start": "83080",
    "end": "89840"
  },
  {
    "text": "learning. I was thinking of maybe showing\nsome applications that you might have seen already,\nrobots grasping, doing all kinds of different\nthings, and so on.",
    "start": "89840",
    "end": "97190"
  },
  {
    "text": "But actually I\nthought that maybe we could reverse that question. And given that\nreinforcement learning deals",
    "start": "97190",
    "end": "102580"
  },
  {
    "text": "with long term consequences\nof your actions, maybe we can think of actions\nthat are isolated that don't",
    "start": "102580",
    "end": "110530"
  },
  {
    "text": "affect the future, all right? So if we figure out what\nare those kind of settings, then reinforcement\nlearning would",
    "start": "110530",
    "end": "116860"
  },
  {
    "text": "deal with everything else. And actually\nthinking about this, it's quite tricky to\nthink of an action that",
    "start": "116860",
    "end": "124840"
  },
  {
    "text": "doesn't influence the future. Even if I do\nsomething meaningless,",
    "start": "124840",
    "end": "129940"
  },
  {
    "text": "I don't know, like I'll\ntap the microphone, right here, maybe\nyou might think",
    "start": "129940",
    "end": "136692"
  },
  {
    "text": "that it doesn't affect\nthe future at all, but maybe I exerted\nso much energy that it would require me to\ntake a nap later in the day",
    "start": "136692",
    "end": "143050"
  },
  {
    "text": "and that would actually\nhave quite a consequence. Or maybe it would distract\nsomeone listening to this class",
    "start": "143050",
    "end": "148270"
  },
  {
    "text": "because they hear a\nlittle bit of a noise and then they won't attend\nthe class, which would be a horrible consequence.",
    "start": "148270",
    "end": "154280"
  },
  {
    "text": "We wouldn't want to do that,\nso I apologize for that. It's actually really\ntricky to think of actions",
    "start": "154280",
    "end": "160480"
  },
  {
    "text": "that don't affect the future. And here's a little\nexample demonstrating this. So this is a double pendulum.",
    "start": "160480",
    "end": "169750"
  },
  {
    "text": "That is the same double pendulum\njust recorded six times. It starts basically from\nthe same initial conditions",
    "start": "169750",
    "end": "176110"
  },
  {
    "text": "or at least\nindistinguishable to us. And the initial\naction is the same, the person just lets it go.",
    "start": "176110",
    "end": "182470"
  },
  {
    "text": "And let's see how it evolves. So we can see that the\naction seems fairly similar,",
    "start": "182470",
    "end": "189500"
  },
  {
    "text": "the initial conditions\nseem quite similar as well. And initially, all\nof these recordings",
    "start": "189500",
    "end": "194540"
  },
  {
    "text": "seem basically\nindistinguishable, it looks exactly the same. However, over time,\nthe consequences",
    "start": "194540",
    "end": "202100"
  },
  {
    "text": "of the slight difference\nin the initial action and the initial conditions\ncompound quite a bit.",
    "start": "202100",
    "end": "207980"
  },
  {
    "text": "And towards the end, all\nof these six recordings look completely different. The pendulum behaves in a\ncompletely different way.",
    "start": "207980",
    "end": "216709"
  },
  {
    "text": "This is something that is\nwell-known in chaos theory. It's also referred to\nas butterfly effect and I encourage\nyou to look it up.",
    "start": "216710",
    "end": "223170"
  },
  {
    "text": "But that's just\none example of how something that seems\nvery small and doesn't",
    "start": "223170",
    "end": "228988"
  },
  {
    "text": "really matter at the\nbeginning, it actually can have quite large\nlong-term consequences.",
    "start": "228988",
    "end": "234200"
  },
  {
    "text": " And so far in this class\nwe've been talking a lot",
    "start": "234200",
    "end": "239730"
  },
  {
    "text": "about supervised learning. And supervised learning\nis one of these cases. Or the modeling in\nsupervised learning",
    "start": "239730",
    "end": "246450"
  },
  {
    "text": "is one of these cases\nwhere we pretend that the action that\nwe'll take doesn't",
    "start": "246450",
    "end": "251700"
  },
  {
    "text": "affect the future, all right? So for instance we'll be\nclassifying cats and dogs and we'll be looking\nat the image.",
    "start": "251700",
    "end": "257970"
  },
  {
    "text": "And based on what we see\nin the image, we'll say, this is a cat. And then in our\nmodeling procedure",
    "start": "257970",
    "end": "263730"
  },
  {
    "text": "we pretend that this\nis the end, all right? This is where the world\nends, this is the block world",
    "start": "263730",
    "end": "269070"
  },
  {
    "text": "that we are in and nothing\nhappens after that. But in reality, you would make\na decision whether this is a cat",
    "start": "269070",
    "end": "276350"
  },
  {
    "text": "or not. And then this would have\nsome further consequences and you would use that\ninformation somehow.",
    "start": "276350",
    "end": "281550"
  },
  {
    "text": "So supervised learning is this\nvery small toyish world where we've been operating so far.",
    "start": "281550",
    "end": "287610"
  },
  {
    "text": "And reinforcement\nlearning allows us to make it a little\nbit more realistic and think about\nconsequences of our actions.",
    "start": "287610",
    "end": "294465"
  },
  {
    "text": " There's a lot of common\napplications of reinforcement",
    "start": "294465",
    "end": "299640"
  },
  {
    "text": "learning because of the fact\nthat it can model the world quite well, such as robotics.",
    "start": "299640",
    "end": "305115"
  },
  {
    "text": "There is also a lot of\nreinforcement learning in language and dialogue. In autonomous driving,\nalso maybe another example",
    "start": "305115",
    "end": "310979"
  },
  {
    "text": "of robotics. In business operations\nwhere we definitely have to think about long-term\nconsequences or in finance.",
    "start": "310980",
    "end": "319210"
  },
  {
    "text": "So it's actually used in\nmany deployed ML systems.",
    "start": "319210",
    "end": "324240"
  },
  {
    "text": "And it's also one of the\nkey aspects of intelligence. This is how we make\ndecisions, this is how we model our\nintelligent behaviors.",
    "start": "324240",
    "end": "330510"
  },
  {
    "text": "We think about long\nterm consequences of what we are doing. So I think it's a little\nbit closer to intelligence",
    "start": "330510",
    "end": "337620"
  },
  {
    "text": "as we know it. All right, so the plan\nfor today's lecture is the following, we'll briefly\ntalk about the reinforcement",
    "start": "337620",
    "end": "345000"
  },
  {
    "text": "learning problem. I hope that this will\nbe mostly a review, so we'll just briefly talk\nabout the concepts and so on.",
    "start": "345000",
    "end": "352410"
  },
  {
    "text": "Then we'll talk about one way\nof solving a reinforcement learning problem with\npolicy gradients.",
    "start": "352410",
    "end": "357840"
  },
  {
    "text": "And then, time permitting,\nwe'll talk about Q-learning because it'll be split into\ntwo parts, the prediction part",
    "start": "357840",
    "end": "363300"
  },
  {
    "text": "as well as the policy\nimprovement part. And I don't think we'll\nbe able to go through all of this today, but\nit might spill over",
    "start": "363300",
    "end": "370870"
  },
  {
    "text": "onto Monday's lecture.  All right, so let's start\nwith the RL problem.",
    "start": "370870",
    "end": "377630"
  },
  {
    "text": "And for this we'll consider two\ncases, an object classification",
    "start": "377630",
    "end": "384410"
  },
  {
    "text": "case that you're\nmore familiar with, this is a supervised\nlearning case, and an object manipulation case, where,\nlet's say, an agent",
    "start": "384410",
    "end": "390949"
  },
  {
    "text": "has to manipulate\na bunch of objects. And this is an example of a\nsequential decision making problem, a problem\nthat reinforcement",
    "start": "390950",
    "end": "397730"
  },
  {
    "text": "learning deals with. So in the supervised\nlearning case, we could make an\nassumption that we have iid",
    "start": "397730",
    "end": "404780"
  },
  {
    "text": "data available to us, right? It's in the iid data, where we\ncan just take from a data set",
    "start": "404780",
    "end": "414050"
  },
  {
    "text": "and we just assume that the data\nis distributed in iid fashion.",
    "start": "414050",
    "end": "420560"
  },
  {
    "text": "In a sequential\ndecision making process, that's not the case, right? So when I perform an action,\nthat action would influence,",
    "start": "420560",
    "end": "429260"
  },
  {
    "text": "would affect the next data\nthat I'm going to see. So because of that, dependent\non what action I'm taking,",
    "start": "429260",
    "end": "435350"
  },
  {
    "text": "I will see a different data\ndistribution afterwards. So it's clearly not iid. ",
    "start": "435350",
    "end": "441830"
  },
  {
    "text": "In addition, in\nsupervised learning, we have these large labeled\ncurated datasets that are just available to us\nand we can work with them.",
    "start": "441830",
    "end": "450139"
  },
  {
    "text": "But in sequential\ndecision making case, this is a little\nbit more tricky. There will be some\ncases that we'll discuss where we have\ndatasets available to us.",
    "start": "450140",
    "end": "458460"
  },
  {
    "text": "And we have to think about how\nthey were generated and so on. But in general, we have\nto answer the question, how do we collect the data in\nthe first place, all right?",
    "start": "458460",
    "end": "466190"
  },
  {
    "text": "Since the data is\nnot distributed iid, we are the ones influencing\nthe data distribution, so we have to think\nhow to collect it.",
    "start": "466190",
    "end": "473102"
  },
  {
    "text": "And it's also a\nlittle unclear what the labels are for the data,\nare we labeling actions? Are we labeling the\noutcomes we want to achieve?",
    "start": "473102",
    "end": "480260"
  },
  {
    "text": "Are we labeling what\nmodality do we use for this? And so on.",
    "start": "480260",
    "end": "486033"
  },
  {
    "text": "In supervised learning, we\nalso have a well-defined notion of success. We have our training and tester\nand we can minimize down error",
    "start": "486033",
    "end": "493460"
  },
  {
    "text": "and that defines what\nwe are trying to do. Versus in sequential\ndecision making,",
    "start": "493460",
    "end": "498509"
  },
  {
    "text": "it's even not very clear\nwhat does success mean and how to define it. ",
    "start": "498510",
    "end": "507020"
  },
  {
    "text": "All right, let's start\nwith a quick terminology. So far we've been talking\nabout a supervised setting",
    "start": "507020",
    "end": "512240"
  },
  {
    "text": "where you, for\ninstance, see an image and it goes through some\nmodel, for instance, a deep neural network,\nthat is parameterized",
    "start": "512240",
    "end": "519110"
  },
  {
    "text": "by some parameters theta. And at the end we have to\nclassify what's in the image,",
    "start": "519110",
    "end": "525020"
  },
  {
    "text": "for instance, what kind of cat\nwe are seeing in the image. Now in our sequential\ndecision making process,",
    "start": "525020",
    "end": "533430"
  },
  {
    "text": "first, it would be\ndependent on time, but more importantly, the\ndecision that we'll make",
    "start": "533430",
    "end": "539138"
  },
  {
    "text": "will influence the next\nobservation we're going to see. So there is this\nfeedback loop here.",
    "start": "539138",
    "end": "544670"
  },
  {
    "text": "Then in addition to this,\nwe won't be just classifying what's in the image, but\nwe'll be actually performing actions in the real world,\nactions that have consequences.",
    "start": "544670",
    "end": "552029"
  },
  {
    "text": "So when we see an\nimage of a tiger, the actions that we have to,\nthat we have to choose from,",
    "start": "552030",
    "end": "557839"
  },
  {
    "text": "actually have real\nworld consequences, and the next observation\nreally depends on what action you will choose here.",
    "start": "557840",
    "end": "563720"
  },
  {
    "text": " So in terms of the\nnotation, Ot, we",
    "start": "563720",
    "end": "572870"
  },
  {
    "text": "would use it for the\nobservations that we see.",
    "start": "572870",
    "end": "579320"
  },
  {
    "text": "at is the action that\nwe perform, all right? Then we'll have\nour policy, which",
    "start": "579320",
    "end": "585470"
  },
  {
    "text": "we'll usually refer as pi. It's pi parameterized with\nsome parameters theta, these are the weights of\nthe neural network, which",
    "start": "585470",
    "end": "593330"
  },
  {
    "text": "gives us the probability of an\naction given the observation. So pi of at given Ot.",
    "start": "593330",
    "end": "601478"
  },
  {
    "text": "Then we'll also have\na notion of a state, and we'll talk about\nthat in a second.",
    "start": "601478",
    "end": "606660"
  },
  {
    "text": "And then we'll\nalso have a policy that acts on the state as\nopposed to on the observation. This is so-called a\nfully observed case.",
    "start": "606660",
    "end": "614870"
  },
  {
    "text": "So just to tell you a little\nbit about the difference between state and\nthe observation.",
    "start": "614870",
    "end": "621560"
  },
  {
    "text": "Let's go back to philosophy. This is Plato's cave.",
    "start": "621560",
    "end": "627770"
  },
  {
    "text": "This is a concept that Plato\ndescribed in some of his works. And the idea was the following.",
    "start": "627770",
    "end": "633120"
  },
  {
    "text": "So Plato was basically\nthinking about, how is it possible\nthat even though we see objects that\nlook very differently",
    "start": "633120",
    "end": "640170"
  },
  {
    "text": "we have these abstract\nconcepts that we can refer to? For instance, even though you\nsee 10,000 different tables,",
    "start": "640170",
    "end": "646009"
  },
  {
    "text": "they all look very\ndifferently, you can still refer to a concept of a table. And people understand\nwhat you mean",
    "start": "646010",
    "end": "651260"
  },
  {
    "text": "when you say a table, right? So there are some\ngeneral concepts, some general idea of a table.",
    "start": "651260",
    "end": "656330"
  },
  {
    "text": "So the way he was\nthinking about it, the way he thought that\nworks is that we actually don't get to observe\nthe true reality,",
    "start": "656330",
    "end": "663830"
  },
  {
    "text": "but we just get to observe a\nreflection of the true concepts in the world.",
    "start": "663830",
    "end": "669260"
  },
  {
    "text": "So this is us sitting in a\ncave, there is a God behind us,",
    "start": "669260",
    "end": "674450"
  },
  {
    "text": "or, I guess, multiple\ngods behind us. And then there is a godly\nfire behind us as well.",
    "start": "674450",
    "end": "680630"
  },
  {
    "text": "And the gods are presenting\nthe true nature of an object that we don't get to see.",
    "start": "680630",
    "end": "686930"
  },
  {
    "text": "And then we only get\nto see the reflection of that object projected\nonto the wall in front of us.",
    "start": "686930",
    "end": "692129"
  },
  {
    "text": "And that projection can\ntake different forms. These are the different\ntypes of tables and so on, but we never actually get to\nsee the true nature of what",
    "start": "692130",
    "end": "700160"
  },
  {
    "text": "a table is, right? And the true nature is\nbeing hidden from us but we can somehow refer to it.",
    "start": "700160",
    "end": "706820"
  },
  {
    "text": "And that's how we can\nabstract these concepts. So using this\nanalogy, it will be",
    "start": "706820",
    "end": "713120"
  },
  {
    "text": "referring to as observation and\nstate, is the things that we see, the things that\nwe can actually sense",
    "start": "713120",
    "end": "719420"
  },
  {
    "text": "or the robot can sense,\nare the observations. And they might not\nhave all the data that we need to make\nthe perfect decision,",
    "start": "719420",
    "end": "726830"
  },
  {
    "text": "or they might have\ntoo much data. And the state is the true\nnature, the true thing",
    "start": "726830",
    "end": "732440"
  },
  {
    "text": "that we need to know in order\nto make the perfect decision in the world, all right?",
    "start": "732440",
    "end": "737959"
  },
  {
    "text": "So to make it a little\nbit more practical, an observation could be\nan image that we see,",
    "start": "737960",
    "end": "743120"
  },
  {
    "text": "this is just directly\nwhat we sense. And then based on this image\nwe need to make a decision. And the state would\nbe all the information",
    "start": "743120",
    "end": "749930"
  },
  {
    "text": "that we truly need to make\nthe decision, which would be, for instance, the positions and\nthe orientations and velocities",
    "start": "749930",
    "end": "756380"
  },
  {
    "text": "of all the objects in the\nscene and maybe the category of these objects as well. In addition, the observation\nmay be incomplete.",
    "start": "756380",
    "end": "763190"
  },
  {
    "text": "So we can have an occlusion. There could be a\ncar in front of us and we won't have\nall the information we need to make the optimal\ndecision, all right?",
    "start": "763190",
    "end": "770105"
  },
  {
    "text": " So we are dealing with this\nsequential decision making",
    "start": "770105",
    "end": "778600"
  },
  {
    "text": "problem. Let's think a\nlittle bit about how we can solve it using the\ntools that we already have.",
    "start": "778600",
    "end": "784370"
  },
  {
    "text": "So I'll switch the example\nto a driving example. So let's say as input we get\nan image from a camera in a car",
    "start": "784370",
    "end": "793220"
  },
  {
    "text": "and then as the action we need\nto pick the steering angle and the acceleration,\nfor instance.",
    "start": "793220",
    "end": "800020"
  },
  {
    "text": "And we need to learn this\npolicy, pi theta of at given Ot",
    "start": "800020",
    "end": "807130"
  },
  {
    "text": "so that we know\nwhat kind of action to take given what\nkind of image.",
    "start": "807130",
    "end": "812690"
  },
  {
    "text": "So there is actually\none way we can do this that we already know. We can just apply\nsupervised learning to it.",
    "start": "812690",
    "end": "818120"
  },
  {
    "text": "And this is what was\ndone in NVIDIA in 2016. So what they did is they\ncollected a bunch of data",
    "start": "818120",
    "end": "823810"
  },
  {
    "text": "from human drivers\nfrom truck drivers, collected observations and\nthe corresponding actions that they were doing, put\nthat in one big dataset,",
    "start": "823810",
    "end": "832840"
  },
  {
    "text": "and then used supervised\nlearning to train that. So to train our\npolicy that would then",
    "start": "832840",
    "end": "838780"
  },
  {
    "text": "tell us what action to take\ngiven a certain observation.",
    "start": "838780",
    "end": "843830"
  },
  {
    "text": "So now my question\nto you is, what do you think could be a problem\nwith an approach like this?",
    "start": "843830",
    "end": "849930"
  },
  {
    "text": "Yep. So if force is going\nto give you results, like that's the input record. So if you square\noff, as like because",
    "start": "849930",
    "end": "857386"
  },
  {
    "text": "you're making your\npredictions, and that's wrong. Or not wrong, but\nlike reduce work. You're going to have\na hard time actually",
    "start": "857387",
    "end": "863590"
  },
  {
    "text": "finding out what's wrong, what\nwill get you back on track, because if the\nnext step is going to be the human driver,\nwho never would have been--",
    "start": "863590",
    "end": "869820"
  },
  {
    "text": "or not never would have made\nthat mistake, because they can't see which regards\nthe second world separate, got it wrong.",
    "start": "869820",
    "end": "876450"
  },
  {
    "text": "Yeah. Won't get the consequences\nof your actions, but you maybe won't\nlearn how to correct it,",
    "start": "876450",
    "end": "881460"
  },
  {
    "text": "because you're always given\nlike you have the bias. That's right. Yeah, so we'll have a lot\nof positive data, data",
    "start": "881460",
    "end": "888190"
  },
  {
    "text": "that where the drivers\ndrive very comfortably and everything makes sense. But as soon as you make\na little bit of an error,",
    "start": "888190",
    "end": "893842"
  },
  {
    "text": "we won't really have\nthat data to tell us how to correct that behavior. Yep. Also, is there a temporal\nmemory that this forms.",
    "start": "893842",
    "end": "901570"
  },
  {
    "text": "Yes, there's no temporal memory. Yeah. That's true. Is that the problem?",
    "start": "901570",
    "end": "906910"
  },
  {
    "text": "It would be like another\ntruck comes to your side, and to use something, but you\nsurely wouldn't-- it should go",
    "start": "906910",
    "end": "914793"
  },
  {
    "text": "in there. You have like a breakdown lane,\nyou're probably going to have,",
    "start": "914793",
    "end": "920200"
  },
  {
    "text": "you don't have a\nfree back side there Right. Yeah, so that's a good point. Yeah, so if you for instance,\nsaw a truck on the side,",
    "start": "920200",
    "end": "926047"
  },
  {
    "text": "for instance, in a side\nmirror, and then you don't see it for a\nsecond, as a human you would remember that\nthere is still a truck there",
    "start": "926047",
    "end": "932057"
  },
  {
    "text": "and you shouldn't change\nyour lane, versus here, because you're seeing\nonly one frame at a time, you'll immediately\nforget about this",
    "start": "932057",
    "end": "937600"
  },
  {
    "text": "and potentially change the\nlane, resulting in an accident. Yep. You can't really get\nbetter in performance.",
    "start": "937600",
    "end": "944170"
  },
  {
    "text": "Yes, it can't get better. That's true. Yeah. Well, we can collect\nmore human data.",
    "start": "944170",
    "end": "949750"
  },
  {
    "text": "And maybe with more\ndata it will get better. But there is no kind of\nstraightforward procedure",
    "start": "949750",
    "end": "956050"
  },
  {
    "text": "that would guarantee that we'll\nget better with more data. Yeah.",
    "start": "956050",
    "end": "961730"
  },
  {
    "text": "All right, cool. So let's talk how we can do this\nwith reinforcement learning. And in order to do this\nwith reinforcement learning,",
    "start": "961730",
    "end": "968460"
  },
  {
    "text": "we have to be able to\ntell whether an action is good or bad, right? In the imitation learning, in\na supervised learning setting,",
    "start": "968460",
    "end": "974910"
  },
  {
    "text": "we could just say every\naction that the human did is a good action,\nand anything else we don't even get to see.",
    "start": "974910",
    "end": "981590"
  },
  {
    "text": "So we have to be able to\ndecide which action is better. And in order to do this, we\nintroduce a reward function.",
    "start": "981590",
    "end": "988410"
  },
  {
    "text": "So the reward\nfunction will tell us which states that our actions\nare better than others.",
    "start": "988410",
    "end": "993449"
  },
  {
    "text": "So for instance, in\nthe driving case, if we could assign high\nreward to the states that",
    "start": "993450",
    "end": "999650"
  },
  {
    "text": "are desirable, for instance,\nwe are driving smoothly, maybe fast, safely, and\nso on, and low reward,",
    "start": "999650",
    "end": "1006130"
  },
  {
    "text": "that's to the states where\nthey result in collisions. ",
    "start": "1006130",
    "end": "1013390"
  },
  {
    "text": "All right, then all\nof these things, so the states, actions,\nreward function, as well as",
    "start": "1013390",
    "end": "1018460"
  },
  {
    "text": "the transition\ndynamics that tell us how our world evolves, so\nthe probability of next state given the current\nstate and the action",
    "start": "1018460",
    "end": "1024280"
  },
  {
    "text": "that we're going to take,\ndefine what we call the Markov decision process, which is the\nframework that we are going",
    "start": "1024280",
    "end": "1030189"
  },
  {
    "text": "to use, in\nreinforcement learning, and what we'll be solving\nin reinforcement learning.",
    "start": "1030190",
    "end": "1037990"
  },
  {
    "text": "All right, so the goal\nof reinforcement learning is to train a policy, pi\ntheta, this policy that's",
    "start": "1037990",
    "end": "1046089"
  },
  {
    "text": "parameterized by\ntheta, that maps from observations to actions. So as input it\ntakes observations.",
    "start": "1046089",
    "end": "1051400"
  },
  {
    "text": "As output it outputs actions. These observations\nactually are some result of a state that we\ndon't get to see, right?",
    "start": "1051400",
    "end": "1058650"
  },
  {
    "text": "Like there is something\nin the real world. And then we just get\nto see the image of it. And then the action, and the\nstate, result in the next state",
    "start": "1058650",
    "end": "1068410"
  },
  {
    "text": "that we get to see. And we don't really get to\ncontrol these transition dynamics. This is the p of s\nprime, given s, a.",
    "start": "1068410",
    "end": "1074770"
  },
  {
    "text": "This is just how the\nworld works, right? Like given that I'm\nin a certain state and I'm going to perform\na certain action,",
    "start": "1074770",
    "end": "1080840"
  },
  {
    "text": "I'm going to see something,\nwhat's going to happen next? So that results\nin the next state,",
    "start": "1080840",
    "end": "1085960"
  },
  {
    "text": "and that results in the\nnext observation, and so on. So we can write it down as a\nprobabilistic-graphical model,",
    "start": "1085960",
    "end": "1094150"
  },
  {
    "text": "where we have all the\ndependencies that we just described. So we have the state that\nleads to the observation.",
    "start": "1094150",
    "end": "1101380"
  },
  {
    "text": "Then, given the observation,\nwe need to perform the action. And then given the\naction and the state",
    "start": "1101380",
    "end": "1106540"
  },
  {
    "text": "at a certain time step, given\nour transition dynamics, we can transport\nto the next state.",
    "start": "1106540",
    "end": "1111970"
  },
  {
    "text": "So this part here is\nour policy, pi theta. And this part here is our\ntransition dynamics, p of st",
    "start": "1111970",
    "end": "1119289"
  },
  {
    "text": "plus 1, given st, at.  Now the important part is\nthat the transition dynamics",
    "start": "1119290",
    "end": "1127760"
  },
  {
    "text": "are independent of\nthe previous state. This is the assumption that\nwe make in the Markov decision process. And this is what we\ncall a Markov property.",
    "start": "1127760",
    "end": "1135200"
  },
  {
    "text": "So what that means\nis that, to predict what s3 I'm going\nto see, I only need",
    "start": "1135200",
    "end": "1140720"
  },
  {
    "text": "to know what s2 was and a2 was. I don't need to know anything\nabout the past, right?",
    "start": "1140720",
    "end": "1147572"
  },
  {
    "text": "This is the assumption\nthat we make and that makes math\nmuch, much simpler. And this is actually\nquite important.",
    "start": "1147572",
    "end": "1154070"
  },
  {
    "text": "And it's often\ndifficult to make sure and think about that the\nsystem that you're working with",
    "start": "1154070",
    "end": "1159710"
  },
  {
    "text": "is actually Markovian,\nand actually satisfies this Markov property where\nthe transition dynamics only depends on the previous state\nand the previous action.",
    "start": "1159710",
    "end": "1166385"
  },
  {
    "text": " All right, so for the\npurpose of today's lecture,",
    "start": "1166385",
    "end": "1174522"
  },
  {
    "text": "and I think for\nmost of the course, we'll make the\nassumption that we are in the fully\nobservable world and we get to see\nthe state itself.",
    "start": "1174522",
    "end": "1181422"
  },
  {
    "text": "So we'll just say that the\nobservations are basically our states. So we'll just get rid\nof the observations, and we'll be mapping straight\nfrom states to actions.",
    "start": "1181422",
    "end": "1189413"
  },
  {
    "text": "This would just\nmake our notations a little bit simpler and the\nwhole problem quite a bit simpler as well.",
    "start": "1189413",
    "end": "1195679"
  },
  {
    "text": "So now, let's start with\na little bit of math. So, given this\nprobabilistic-graphical model,",
    "start": "1195680",
    "end": "1202330"
  },
  {
    "text": "I'm going to write down the\njoint probability of all the probability\ndistributions that we",
    "start": "1202330",
    "end": "1208300"
  },
  {
    "text": "see from random variables that\nwe see in this graphical model. So this will be a joint\ndistribution of all the states",
    "start": "1208300",
    "end": "1214300"
  },
  {
    "text": "and actions, starting\nfrom time step 1 up to time step\ncapital T, all right?",
    "start": "1214300",
    "end": "1221770"
  },
  {
    "text": "And what this actually is,\nthis joint distribution, is a trajectory, right? So the trajectory is just a\nsequence of states and actions",
    "start": "1221770",
    "end": "1229690"
  },
  {
    "text": "that happen one after the other. Now given this\ngraphical model, we can factorize this\ndistribution as following.",
    "start": "1229690",
    "end": "1238810"
  },
  {
    "text": "So we can start with the\nprobability of starting with in s1, so this\nis the probability",
    "start": "1238810",
    "end": "1243880"
  },
  {
    "text": "of our initial\nstate distribution, times this big product of\nthe probability of taking",
    "start": "1243880",
    "end": "1249610"
  },
  {
    "text": "a specific action in that state\ntimes the probability of then jumping to the next state, given\nthe action that I just picked",
    "start": "1249610",
    "end": "1256240"
  },
  {
    "text": "and the state that\nI was in, right? And we can keep multiplying\nthese up until capital T. Yep,",
    "start": "1256240",
    "end": "1262990"
  },
  {
    "text": "there's a question. Will we also, capital T,\nwith kind of over to plus t plus 1, lost only that plus 1?",
    "start": "1262990",
    "end": "1270460"
  },
  {
    "text": "Yeah, I think there\nwe will just end with probability of the\naction at the very final step,",
    "start": "1270460",
    "end": "1276250"
  },
  {
    "text": "and that's it. We'll just finish\nwith that action. All right, so I'll be referring\nto this first term here,",
    "start": "1276250",
    "end": "1283750"
  },
  {
    "text": "this joint probability\ndistribution, as tau. And this is the\nprobability, this is the random variable that\ndepicts the whole trajectory.",
    "start": "1283750",
    "end": "1290650"
  },
  {
    "text": "So tau would correspond\nto a trajectory. And now the goal of\nreinforcement learning",
    "start": "1290650",
    "end": "1297250"
  },
  {
    "text": "is to find the parameters\ntheta, right here, we'll be calling this\ntheta star, which",
    "start": "1297250",
    "end": "1304420"
  },
  {
    "text": "solve the following equation. It's the arg max\nof the expectation",
    "start": "1304420",
    "end": "1311140"
  },
  {
    "text": "under our trajectory\ndistribution, which we'll be referring to as\npi theta of tau,",
    "start": "1311140",
    "end": "1317350"
  },
  {
    "text": "of the future sum of the rewards\nthat we are going to get. So what that means is that we\nhave some kind of distribution",
    "start": "1317350",
    "end": "1325420"
  },
  {
    "text": "of our trajectory, right? We get to control certain parts\nof this distribution, just",
    "start": "1325420",
    "end": "1331750"
  },
  {
    "text": "this part. We only get to\ncontrol the policy. We don't get to control the\ntransition dynamics and so on. This is the only\nkind of knob we have.",
    "start": "1331750",
    "end": "1338890"
  },
  {
    "text": "And under this, we then\ntake the expectation under this distribution.",
    "start": "1338890",
    "end": "1344539"
  },
  {
    "text": "And we want to maximize the\nsum of the rewards, right? So given that, well,\nour policy will",
    "start": "1344540",
    "end": "1350080"
  },
  {
    "text": "result in certain\ntrajectories, how can we pick the right\nset of parameters so that we maximize the sum\nof the rewards we will get.",
    "start": "1350080",
    "end": "1357020"
  },
  {
    "text": " All right, and since this\nis a course about multitask",
    "start": "1357020",
    "end": "1364550"
  },
  {
    "text": "and meta-learning,\nit's important to think about what is a\nreinforcement learning task.",
    "start": "1364550",
    "end": "1370740"
  },
  {
    "text": "So in supervised learning,\nwe were calling a task data-generating distributions\nas well as the loss.",
    "start": "1370740",
    "end": "1377779"
  },
  {
    "text": "In the reinforcement learning,\nthe task is the whole MDP. So in that case, this\nwill be the state space,",
    "start": "1377780",
    "end": "1384650"
  },
  {
    "text": "the action space\nthat we get to use, the initial state distribution. So the probability of s1,\nthe transition dynamics,",
    "start": "1384650",
    "end": "1393470"
  },
  {
    "text": "as well as the reward\nfunction itself. So the hallmark of\ndecision process is a task.",
    "start": "1393470",
    "end": "1400220"
  },
  {
    "text": "And it's important\nto note that this is much more than just the\nsemantic meaning of a task, right?",
    "start": "1400220",
    "end": "1405260"
  },
  {
    "text": "So whether you're just, I don't\nknow, grasping or pushing, these are considered\ndifferent tasks.",
    "start": "1405260",
    "end": "1410750"
  },
  {
    "text": "But here we have much\nmore strict definition of what a task is. So just a few examples of what\ndifferent task distributions",
    "start": "1410750",
    "end": "1420020"
  },
  {
    "text": "could be. So, for instance, in\ncharacter animations, where we have characters that\nhave to do different tricks,",
    "start": "1420020",
    "end": "1427810"
  },
  {
    "text": "these would correspond\nto different tasks, because the reward functions\nbetween the different tasks will vary.",
    "start": "1427810",
    "end": "1434270"
  },
  {
    "text": "We can also do\ncharacter animations where we are trying\nto put on or animate how to put on a shirt\nor some kind of garment.",
    "start": "1434270",
    "end": "1441470"
  },
  {
    "text": "And in this case,\nwhat would vary would be the initial state\ndistribution as well as the dynamics.",
    "start": "1441470",
    "end": "1447170"
  },
  {
    "text": "So every piece of\nclothing would have slightly different dynamics. We can also consider\na multi-robot case,",
    "start": "1447170",
    "end": "1452960"
  },
  {
    "text": "where we're trying to learn\na policy and gathering data from all kinds\nof different robots. And in this case,\nwhat would vary",
    "start": "1452960",
    "end": "1458540"
  },
  {
    "text": "would be the state\nspace, the action space, the robots will be controlled\nslightly differently, the initial state distribution,\nas well as the dynamics,",
    "start": "1458540",
    "end": "1465657"
  },
  {
    "text": "even though they could be\nperforming the exact same task and they could be using\nthe same reward function.",
    "start": "1465657",
    "end": "1471809"
  },
  {
    "text": "All right, are there any\nquestions at this point? Yep. Can you track one\nposition from the state",
    "start": "1471810",
    "end": "1478079"
  },
  {
    "text": "that, won't you lose\nsomething like velocity in Markov decision process? Yeah, so you can use that. And you can include\nthat in your state.",
    "start": "1478079",
    "end": "1484820"
  },
  {
    "text": "OK. So state doesn't necessarily\njust correspond to a position. State could be, in our case,\ncould be even the whole image.",
    "start": "1484820",
    "end": "1492971"
  },
  {
    "text": "Are there any other questions?  All right, if there\nare no more questions,",
    "start": "1492971",
    "end": "1499510"
  },
  {
    "text": "then let's talk about how we can\nactually solve it, all right? And we can start, we'll\nstart with something",
    "start": "1499510",
    "end": "1505270"
  },
  {
    "text": "fairly simple, or\nfairly similar to what you've been doing before. And then we'll\nbuild on top of that",
    "start": "1505270",
    "end": "1511600"
  },
  {
    "text": "and kind of see how we can\nget to more complex methods. ",
    "start": "1511600",
    "end": "1517500"
  },
  {
    "text": "All right, so there\nis this anatomy of a reinforcement\nlearning algorithm, that",
    "start": "1517500",
    "end": "1523860"
  },
  {
    "text": "can help you to classify\nall kinds of different RL algorithms and provide\nlike a mental framework",
    "start": "1523860",
    "end": "1531210"
  },
  {
    "text": "that you can use to\nthink about them. So this anatomy looks like this.",
    "start": "1531210",
    "end": "1536710"
  },
  {
    "text": "We have three boxes. So we start with some way\nof generating samples. This is this orange box.",
    "start": "1536710",
    "end": "1543130"
  },
  {
    "text": "So what that means is that we\njust run the policy, our policy pi, in the real world. We collect the trajectories.",
    "start": "1543130",
    "end": "1549270"
  },
  {
    "text": "We can score them based\non our reward function. And that's it. Then we have the\ngreen box here, where",
    "start": "1549270",
    "end": "1556020"
  },
  {
    "text": "we fit the model to\nestimate the return, so the sum of the rewards. And here we'll talk\nabout different options.",
    "start": "1556020",
    "end": "1561840"
  },
  {
    "text": "We can just calculate\nthe sum of the rewards. This is called a\nMonte Carlo, and this is what we'll be using in\nthe policy gradient, which",
    "start": "1561840",
    "end": "1568020"
  },
  {
    "text": "we'll talk about in a second. We can also fit a Q function. We'll also talk about\nthat a little bit later.",
    "start": "1568020",
    "end": "1574410"
  },
  {
    "text": "Or we can estimate a model that\nmodels the transition dynamics. This is also used,\nand you'll probably",
    "start": "1574410",
    "end": "1581340"
  },
  {
    "text": "discuss it in the lecture\nthing on Wednesday next week. And then we have\nthe blue box, which,",
    "start": "1581340",
    "end": "1587549"
  },
  {
    "text": "given that we\ncollected some samples, we can fit a model to\nestimate the returns. Now we have to have a way\nof improving our policy.",
    "start": "1587550",
    "end": "1595410"
  },
  {
    "text": "And here we have a\nfew different choices. So we can either take the direct\ngradient on the objective.",
    "start": "1595410",
    "end": "1600920"
  },
  {
    "text": "This is what we'll be doing\nin the policy gradient case. We can also do an arg max\non the Q function, which",
    "start": "1600920",
    "end": "1606193"
  },
  {
    "text": "is what we'll be doing\nin the Q learning case, which we'll talk about. Or we can optimize the policy\nusing other methods that people",
    "start": "1606193",
    "end": "1613650"
  },
  {
    "text": "use in model-based\nlearning, by, for instance, back propagating\nthrough the model or using a shooting method\nor something like that.",
    "start": "1613650",
    "end": "1619620"
  },
  {
    "text": " All right, so we will take a\nlook at the policy gradients",
    "start": "1619620",
    "end": "1626610"
  },
  {
    "text": "approach, which is the most\ndirect way of optimizing the objective we care about. So just again, the\nobjective that we care about",
    "start": "1626610",
    "end": "1634200"
  },
  {
    "text": "is the following. We are trying to find\nthe parameters theta that would maximize this\nexpectation, right here.",
    "start": "1634200",
    "end": "1641950"
  },
  {
    "text": "So this is the expectation\nof the sum of the rewards. And this is the\nexpectation under our trajectory distribution,\npi theta of tau.",
    "start": "1641950",
    "end": "1651740"
  },
  {
    "text": "So I'll be just calling\nthe pure objective, I'll just call it\nJ of theta, right?",
    "start": "1651740",
    "end": "1657260"
  },
  {
    "text": "This is our objective\nthat is dependent on our parameters theta. So our J of theta is equal\nto that, I just rewrote it.",
    "start": "1657260",
    "end": "1665360"
  },
  {
    "text": "And now, because we have\nan expectation here, we can actually approximate\nthis expectation,",
    "start": "1665360",
    "end": "1670670"
  },
  {
    "text": "as we usually do in machine\nlearning, with samples, right? So in that case, we would\ntake multiple samples.",
    "start": "1670670",
    "end": "1678170"
  },
  {
    "text": "We'll take n samples. And we will take\njust the average. We'll be averaging,\nfor every i trajectory,",
    "start": "1678170",
    "end": "1683840"
  },
  {
    "text": "we'll be averaging\nthe sum of rewards. So what this is\ngoing to allow us to do is just to evaluate\nthe objective, right?",
    "start": "1683840",
    "end": "1690950"
  },
  {
    "text": "We are not optimizing it yet. We just want to say how well am\nI doing based on my objective.",
    "start": "1690950",
    "end": "1697580"
  },
  {
    "text": "So what that means is\nthat this expectation was under the trajectory\ndistribution.",
    "start": "1697580",
    "end": "1703110"
  },
  {
    "text": "So we are approximating\nit with samples. So what we'll be doing, we'll\nbe just rolling out the policy",
    "start": "1703110",
    "end": "1708380"
  },
  {
    "text": "multiple times, all right? So we'll be just\nallowing the robot to run a few times given\nour current policy.",
    "start": "1708380",
    "end": "1714980"
  },
  {
    "text": "And let's say we'll\ncollect three trajectories like this, all right? And in this case,\nn is equal to 3.",
    "start": "1714980",
    "end": "1723440"
  },
  {
    "text": "Then we will score them. So we will calculate\nthe sum of the rewards for each one of them.",
    "start": "1723440",
    "end": "1729360"
  },
  {
    "text": "And then we would sum over\nthose samples from pi. And that's it.",
    "start": "1729360",
    "end": "1734540"
  },
  {
    "text": "That would tell us how\ngood is our current policy. We'll take the average\nof all the trajectories that we rolled out.",
    "start": "1734540",
    "end": "1740540"
  },
  {
    "text": "And that average is\ntelling us how well am I performing on average, how\ngood is my objective right now,",
    "start": "1740540",
    "end": "1746960"
  },
  {
    "text": "with my current\npolicy, pi theta.  All right, so we can\nevaluate the objective.",
    "start": "1746960",
    "end": "1754720"
  },
  {
    "text": "Now let's think about how we\ncan improve on that objective, right? So again, we start with our\nobjective, as we had it.",
    "start": "1754720",
    "end": "1762700"
  },
  {
    "text": "And I'm going to rewrite this\nobjective slightly, as this. So our objective here, instead\nof having the sum of rewards,",
    "start": "1762700",
    "end": "1770920"
  },
  {
    "text": "I'm just going to\ncall it r of tau, which is the full trajectory. So this is just a\nshortcut notation.",
    "start": "1770920",
    "end": "1777350"
  },
  {
    "text": "So it's a little\nbit easier for us, but it denotes the\nsum of all the rewards that we get for the\ntrajectory, all right?",
    "start": "1777350",
    "end": "1785320"
  },
  {
    "text": "So now, the way we\ncan write it out is we can write out\nthis expectation,",
    "start": "1785320",
    "end": "1792243"
  },
  {
    "text": "don't approximate\nwith the sample, but actually use the\ndefinition of the expectation. And because this is a\ncontinuous distribution,",
    "start": "1792243",
    "end": "1799390"
  },
  {
    "text": "we would write it out as an\nintegral of our pi theta of tau",
    "start": "1799390",
    "end": "1805390"
  },
  {
    "text": "times r of tau, d tau. Right? So we have this\nugly integral here.",
    "start": "1805390",
    "end": "1810790"
  },
  {
    "text": "And we have to do\nsomething with it. But now, we want to\nimprove on that objective.",
    "start": "1810790",
    "end": "1817160"
  },
  {
    "text": "And the way we usually improve\non objectives in modern machine learning is we do\ngradient descent.",
    "start": "1817160",
    "end": "1822700"
  },
  {
    "text": "So we compute the\ngradient with respect to our parameters\nof the objective, and then we perform the gradient\ndescent or ascent procedure",
    "start": "1822700",
    "end": "1829540"
  },
  {
    "text": "to improve on the objective. So let's try to write out the\ngradient of our objective.",
    "start": "1829540",
    "end": "1838250"
  },
  {
    "text": "So we can start by\njust adding gradient with respect to parameters\ntheta, of our objective,",
    "start": "1838250",
    "end": "1844990"
  },
  {
    "text": "and we can actually swap the\norder between the integral and the gradient itself.",
    "start": "1844990",
    "end": "1851450"
  },
  {
    "text": "These are two linear operations,\nyou're allowed to do that. And now we have a little\nbit of a problem, right?",
    "start": "1851450",
    "end": "1859059"
  },
  {
    "text": "So we have this\nvery awful gradient we have to integrate over\nall kinds, all trajectories.",
    "start": "1859060",
    "end": "1865420"
  },
  {
    "text": "And then we need to\nnot only do this, but we also need to take\na gradient with respect to all of this, and this\nis this distribution,",
    "start": "1865420",
    "end": "1873417"
  },
  {
    "text": "the trajectory distribution,\nwhich we don't really get to control. So the first link that\nwe want to deal with",
    "start": "1873417",
    "end": "1879220"
  },
  {
    "text": "is that this gradient,\nthis integral, is not tractable, right? Like we are not able\nto integrate over",
    "start": "1879220",
    "end": "1886630"
  },
  {
    "text": "all kinds of trajectories. So when we have that,\nwhat we usually try to do",
    "start": "1886630",
    "end": "1892510"
  },
  {
    "text": "is try to somehow change\nit back to an expectation. So let's try to do that.",
    "start": "1892510",
    "end": "1899000"
  },
  {
    "text": "Let's try to do that. So what we will have in this\ncase is that we have our--",
    "start": "1899000",
    "end": "1910340"
  },
  {
    "start": "1910340",
    "end": "1915659"
  },
  {
    "text": "let's just write it like this. So we have our\ngradient with respect to parameters theta, of\npi theta of tau, right?",
    "start": "1915660",
    "end": "1928650"
  },
  {
    "text": "This is the first term that\nwe have under that equation.",
    "start": "1928650",
    "end": "1934630"
  },
  {
    "text": "And the way we\nwould want to change it is that we would\nwant to somehow pull",
    "start": "1934630",
    "end": "1940710"
  },
  {
    "text": "the distribution of tau\nin front of the integral,",
    "start": "1940710",
    "end": "1945760"
  },
  {
    "text": "so that then we can take\nthe expectation, right? Because we have\nhere some integral. Here is our gradient operator.",
    "start": "1945760",
    "end": "1953795"
  },
  {
    "text": "And then we have\na bunch of stuff here, including our\ndistribution pi theta of tau.",
    "start": "1953795",
    "end": "1961440"
  },
  {
    "text": "And what we want to do is\nsomehow pull that pi in here, because if-- and this\nis d tau-- right,",
    "start": "1961440",
    "end": "1968910"
  },
  {
    "text": "because if we do that, then\nonce we have the integral, and here we have the probability\ndistribution pi theta of tau,",
    "start": "1968910",
    "end": "1975540"
  },
  {
    "text": "then we can just say that this\nis the expectation under pi",
    "start": "1975540",
    "end": "1981690"
  },
  {
    "text": "theta of tau, right? And then we can\njust keep whatever was inside here in there.",
    "start": "1981690",
    "end": "1988475"
  },
  {
    "text": "And once we have\nthe expectation, we can approximate it with\nsamples, and we are good. We don't have to\ndo the integral.",
    "start": "1988475",
    "end": "1993840"
  },
  {
    "text": "So now what we\nwant to do is take that, what we have right now,\nand somehow change it, such",
    "start": "1993840",
    "end": "2000230"
  },
  {
    "text": "that we have pi theta of tau,\ntimes some kind of gradient",
    "start": "2000230",
    "end": "2011049"
  },
  {
    "text": "of something, right? So the question is\nwhat is that something.",
    "start": "2011050",
    "end": "2016360"
  },
  {
    "text": "So we have to do a little bit\nof arithmetic to get there. So let's try to do that.",
    "start": "2016360",
    "end": "2022468"
  },
  {
    "text": "And to do this, I'm\ngoing to tell you about a certain identity that\nshould come in very handy.",
    "start": "2022468",
    "end": "2031640"
  },
  {
    "text": "So that identity is, I think\nyou might be familiar with it.",
    "start": "2031640",
    "end": "2037780"
  },
  {
    "text": "It's fairly straightforward. It says that the gradient with\nrespect to x of log of f of x",
    "start": "2037780",
    "end": "2046779"
  },
  {
    "text": "is equal to-- can anybody tell me\nwhat this is equal to? Do you remember that?",
    "start": "2046780",
    "end": "2053430"
  },
  {
    "text": "Yeah. 1 over f of x\ntimes the gradient.",
    "start": "2053431",
    "end": "2058690"
  },
  {
    "text": "1 over f of x times the gradient\nwith respect to x of f of x.",
    "start": "2058690",
    "end": "2064690"
  },
  {
    "text": "That's right, OK? A simple identity. So now what we can do is let's\nmultiply both sides by f of x.",
    "start": "2064690",
    "end": "2072610"
  },
  {
    "text": "So if we do that, what\nwe'll end up with-- I'm just going to\nwrite it here-- is we'll have f of x,\ngrad x log f of x is equal",
    "start": "2072610",
    "end": "2087460"
  },
  {
    "text": "grad x f of x, OK?",
    "start": "2087460",
    "end": "2093080"
  },
  {
    "text": "I just multiplied\nboth sides by f of x. All right, so now we have\nan interesting formula here",
    "start": "2093080",
    "end": "2100349"
  },
  {
    "text": "that actually gives us the\nanswer to what we want. So we want to somehow transform\nthis part, grad with respect",
    "start": "2100350",
    "end": "2108480"
  },
  {
    "text": "to theta of pi\ntheta of tau, to pi theta of tau times the\ngradient of something.",
    "start": "2108480",
    "end": "2115650"
  },
  {
    "text": "So the part that we have\non the left side here is what we have on the right\nside right here, right? This is just instead of\nx, we have theta here.",
    "start": "2115650",
    "end": "2123010"
  },
  {
    "text": "And instead of f of x we have\npi of tau, pi theta of tau.",
    "start": "2123010",
    "end": "2129630"
  },
  {
    "text": "All right, so then the\nmissing part here would be, we have f of x, which is that\npart here, pi theta of tau.",
    "start": "2129630",
    "end": "2139380"
  },
  {
    "text": "This checks out. And then we have the\ngradient with respect to x, gradient with\nrespect of theta. And the only missing part\nis the log of f of x.",
    "start": "2139380",
    "end": "2145890"
  },
  {
    "text": "So in this case, this would\nbe log of pi theta of tau.",
    "start": "2145890",
    "end": "2152819"
  },
  {
    "text": " All right? So we just transformed\nit slightly, in this way,",
    "start": "2152820",
    "end": "2161290"
  },
  {
    "text": "using this log trick,\nthis simple identity. We can actually,\nso I'm just going",
    "start": "2161290",
    "end": "2167200"
  },
  {
    "text": "to write this down\nright here, this is just I'm repeating\nexactly the same thing that I wrote on the board.",
    "start": "2167200",
    "end": "2172579"
  },
  {
    "text": "And this is how\nwe can rewrite it. And this is just what we got\nto on the whiteboard as well.",
    "start": "2172580",
    "end": "2180050"
  },
  {
    "text": "And now, given\nthat, we can go back and roll it back into\nan expectation, right?",
    "start": "2180050",
    "end": "2186010"
  },
  {
    "text": "So now we have the integral. And under our integral we have\na probability distribution",
    "start": "2186010",
    "end": "2192160"
  },
  {
    "text": "of d tau. So we can use the\ndefinition of an expectation and roll it back\ninto the expectation,",
    "start": "2192160",
    "end": "2199150"
  },
  {
    "text": "and just say this\nis the expectation under the distribution\nof pi theta of tau,",
    "start": "2199150",
    "end": "2205089"
  },
  {
    "text": "of the term that\nwas inside in there, which is this grad\nlog pi times r.",
    "start": "2205090",
    "end": "2210415"
  },
  {
    "text": " All right, so what\nwe did so far is",
    "start": "2210415",
    "end": "2218300"
  },
  {
    "text": "we came up with a\nformula that gives us the gradient with\nrespect to our objective.",
    "start": "2218300",
    "end": "2224359"
  },
  {
    "text": "And we got rid of this integral\nthat was a little nasty. So there is one\nmore piece, right?",
    "start": "2224360",
    "end": "2230780"
  },
  {
    "text": "We had two nasty things. One was the integral\nand the other thing was that we have\nthis pi theta of tau,",
    "start": "2230780",
    "end": "2238768"
  },
  {
    "text": "which is the whole\ntrajectory distribution. And we don't really control the\nwhole trajectory distribution. We only control the policy.",
    "start": "2238768",
    "end": "2243920"
  },
  {
    "text": "This is the only part of this\ndependent on the parameters theta. So let's just write it out.",
    "start": "2243920",
    "end": "2250200"
  },
  {
    "text": "So this is our pi theta of tau. This is just I wrote\nout the whole tau. And this is the factorization\nthat we had before.",
    "start": "2250200",
    "end": "2259710"
  },
  {
    "text": "So this is pi theta of tau. And let's take the\nlog of both sides, because we need the log here.",
    "start": "2259710",
    "end": "2266670"
  },
  {
    "text": "So if we take the\nlog on both sides, we see that the product,\nthe log of the product",
    "start": "2266670",
    "end": "2271700"
  },
  {
    "text": "is the sum of the logs. So we do that. And now we can just\nsubstitute whatever",
    "start": "2271700",
    "end": "2278329"
  },
  {
    "text": "we got here under here. So let's do that. So we are substituting\nthis log pi theta of tau",
    "start": "2278330",
    "end": "2286790"
  },
  {
    "text": "with all the terms that we\nused in the factorization. So this is done now, the\ninitial state distribution,",
    "start": "2286790",
    "end": "2292369"
  },
  {
    "text": "our policy, and the\ndynamics, right? And our policy is\nthe only part that is dependent on the\nparameter theta.",
    "start": "2292370",
    "end": "2300770"
  },
  {
    "text": "And we take the gradient with\nrespect to theta of that, right? So if we do that,\ncan you tell me,",
    "start": "2300770",
    "end": "2306210"
  },
  {
    "text": "what would be the\ngradient of this term with respect to theta? 0. 0, that's right.",
    "start": "2306210",
    "end": "2312049"
  },
  {
    "text": "And what about this term? 0 as well, yep, that's right. These two terms are\nnot dependent on theta.",
    "start": "2312050",
    "end": "2318560"
  },
  {
    "text": "We take the gradient with\nrespect to theta, these are 0. They cancel out. And what we end up\nwith is a formula",
    "start": "2318560",
    "end": "2325850"
  },
  {
    "text": "like this, where the gradient\nwith respect to our objective is just this expectation under\nour trajectory distribution pi",
    "start": "2325850",
    "end": "2332630"
  },
  {
    "text": "theta of tau of the sum\nof this grad log pi. And this we can actually\nevaluate, right?",
    "start": "2332630",
    "end": "2338570"
  },
  {
    "text": "This is our neural network. This is the neural network\nthat given an input state,",
    "start": "2338570",
    "end": "2343700"
  },
  {
    "text": "outputs an action. We can take the log of\nthat, compute the gradient. That's OK, and we then multiply\nit by the sum of the rewards.",
    "start": "2343700",
    "end": "2352660"
  },
  {
    "text": "All right, so now we have\na way of actually having a gradient of the objective.",
    "start": "2352660",
    "end": "2358589"
  },
  {
    "text": "So we can now improve\non our objective. So let's try to\nevaluate it first.",
    "start": "2358590",
    "end": "2364220"
  },
  {
    "text": "So the same way as we were\nevaluating the objective, how would we evaluate the\ngradient of that objective?",
    "start": "2364220",
    "end": "2370110"
  },
  {
    "text": "So this is, I just\nrewrote the formula, and recall that we\nwere able to, before,",
    "start": "2370110",
    "end": "2375230"
  },
  {
    "text": "approximate the expectation of\nsamples from our trajectory. So let's do the exact\nsame thing here.",
    "start": "2375230",
    "end": "2382230"
  },
  {
    "text": "So we will be\napproximating this gradient with a bunch of samples. These are samples\nfrom our trajectory.",
    "start": "2382230",
    "end": "2390619"
  },
  {
    "text": "So we will have n\ndifferent rollouts and we'll be averaging\nover these terms",
    "start": "2390620",
    "end": "2396339"
  },
  {
    "text": "for each one of these rollouts. And then once we do that,\nthen we have our gradient,",
    "start": "2396340",
    "end": "2401540"
  },
  {
    "text": "and we can perform our\ngradient ascent procedure to improve our parameters,\nso that our objective gets better and better.",
    "start": "2401540",
    "end": "2408318"
  },
  {
    "text": "All right, are there any\nquestions at this point? Yep. So that your real rollout is\nthe difference between them?",
    "start": "2408318",
    "end": "2413932"
  },
  {
    "text": "Usually like the\nMarkovian trajectories? Could you repeat the\nquestion, please?",
    "start": "2413933",
    "end": "2420280"
  },
  {
    "text": "What explains the difference\nbetween the trajectories? Oh, I see. Something like an initial. Right.",
    "start": "2420280",
    "end": "2426340"
  },
  {
    "text": "So how is it that, if you\nroll out a single policy, it results in three\ndifferent trajectories?",
    "start": "2426340",
    "end": "2432910"
  },
  {
    "text": "Yeah. So there are\nmultiple differences. So first thing what can\nhappen is that your policy",
    "start": "2432910",
    "end": "2437950"
  },
  {
    "text": "is stochastic, right? So the policy is a\nprobability distribution of actions given a state.",
    "start": "2437950",
    "end": "2443589"
  },
  {
    "text": "So then, according to those\nprobability distributions, you'll be taking slightly\ndifferent actions every time. That's one thing.",
    "start": "2443590",
    "end": "2449480"
  },
  {
    "text": "The other thing is that\nthe dynamics itself can be stochastic as well, right?",
    "start": "2449480",
    "end": "2454660"
  },
  {
    "text": "So given a certain\nstate in action, sometimes it will result\nin a certain s prime, sometimes it will result in\na slightly different s prime.",
    "start": "2454660",
    "end": "2461140"
  },
  {
    "text": "And that would correspond\nto differences in the way you roll out the policy,\nand this will result in different trajectories.",
    "start": "2461140",
    "end": "2466903"
  },
  {
    "text": "All right, are there\nany other questions? ",
    "start": "2466903",
    "end": "2473270"
  },
  {
    "text": "All right, cool. So what we did here is,\nin terms of the anatomy",
    "start": "2473270",
    "end": "2479260"
  },
  {
    "text": "of our RL algorithm, we have\nthe generate samples part, which is the part that we have here. We just roll out a bunch\nof policies a few times.",
    "start": "2479260",
    "end": "2488770"
  },
  {
    "text": "Then we fit the model\nto estimate the return. So the green box, in\nour case, is this.",
    "start": "2488770",
    "end": "2494420"
  },
  {
    "text": "We just sum the rewards that\nwe got for these trajectories. And then, to improve the\npolicy, the blue box,",
    "start": "2494420",
    "end": "2500920"
  },
  {
    "text": "we are just doing the\ngradient ascent procedure. We computed the derivative\nwith respect to our objective,",
    "start": "2500920",
    "end": "2505960"
  },
  {
    "text": "and we do gradient ascent\nand our policy gets better.",
    "start": "2505960",
    "end": "2511060"
  },
  {
    "text": "And this results in one of\nthe simplest RL algorithms called Reinforce. And it works exactly\nhow you would imagine.",
    "start": "2511060",
    "end": "2518143"
  },
  {
    "text": "We sample a bunch\nof trajectories. So we run the policy. Then we compute the derivative\nwith respect to our objective,",
    "start": "2518143",
    "end": "2524890"
  },
  {
    "text": "according to our formula\nthat we just established. And then we perform\ngradient ascent,",
    "start": "2524890",
    "end": "2530710"
  },
  {
    "text": "and we do it over and\nover again, right? So then we improve\nour policy, then we roll out this\nnew policy again.",
    "start": "2530710",
    "end": "2536660"
  },
  {
    "text": "And we keep going. All right, so just one\nquick side note here.",
    "start": "2536660",
    "end": "2545920"
  },
  {
    "text": "We also talked about\nimitation learning, right? In imitation learning,\nwhat you would do is you would do\nmaximum likelihood.",
    "start": "2545920",
    "end": "2552910"
  },
  {
    "text": "So the policy gradient\nformula is written right here. This is what we just discussed.",
    "start": "2552910",
    "end": "2558750"
  },
  {
    "text": "But in the imitation learning\ncase, which we discussed, for people from NVIDIA, we're\njust collecting the data",
    "start": "2558750",
    "end": "2564240"
  },
  {
    "text": "and trying to fit our\npolicy to replicate what the human drivers did.",
    "start": "2564240",
    "end": "2570210"
  },
  {
    "text": "The objective would be to\nmaximize the log likelihood. So we were trying to\nmaximize the likelihood",
    "start": "2570210",
    "end": "2576150"
  },
  {
    "text": "of our data given the model. And the objective would\nactually look like this. And it would be\nvery, very similar",
    "start": "2576150",
    "end": "2582420"
  },
  {
    "text": "to the objective you see up\nhere, that we just derived through the policy gradient.",
    "start": "2582420",
    "end": "2588180"
  },
  {
    "text": "So could you tell me\nwhat's the difference, and/or how can we make\nthe policy gradient",
    "start": "2588180",
    "end": "2594180"
  },
  {
    "text": "look exactly the same as\nthe maximum likelihood?",
    "start": "2594180",
    "end": "2599454"
  },
  {
    "text": "OK, so how can we make our RL\nalgorithm behave exact same way",
    "start": "2599455",
    "end": "2605430"
  },
  {
    "text": "as an imitation\nlearning objective? Yep. Your responsive reward?",
    "start": "2605430",
    "end": "2612240"
  },
  {
    "text": "Constant, you can give\nit constant reward. What kind of reward? Like you can forget for\nfree, the learning before",
    "start": "2612240",
    "end": "2619140"
  },
  {
    "text": "with all these things, stuff\nlike parent, early just RL, s i t or a i t, and\nit would become the same",
    "start": "2619140",
    "end": "2628166"
  },
  {
    "text": "as the best solved algorithm. Right, so the reward\nwould need to be constant. But it would need to be\nconstant for specific actions,",
    "start": "2628166",
    "end": "2636010"
  },
  {
    "text": "the same actions that\nthe humans did, right? Yeah. Yep. Are those expensive-",
    "start": "2636010",
    "end": "2641525"
  },
  {
    "text": "Expensive is what\nyou're about to say? OK, yeah, so we\nwould need to reward the actions that the humans\ndid, let's say with the reward 1",
    "start": "2641525",
    "end": "2647710"
  },
  {
    "text": "at the very end\nof the trajectory. And every other\naction would be 0. And in this case,\nthe policy gradient",
    "start": "2647710",
    "end": "2654520"
  },
  {
    "text": "would correspond exactly to\ndoing imitation learning.",
    "start": "2654520",
    "end": "2659890"
  },
  {
    "text": "All right, cool. So a little bit of\nintuition of what's happening here, if you maybe got\nlost a little bit in the math.",
    "start": "2659890",
    "end": "2667510"
  },
  {
    "text": "So we have our\nReinforce algorithm. We have our objective. This objective is fairly similar\nto the maximum likelihood",
    "start": "2667510",
    "end": "2674500"
  },
  {
    "text": "objective. But what that actually\nmeans is that we would look at our trajectory distribution,\nwe would roll it out,",
    "start": "2674500",
    "end": "2682150"
  },
  {
    "text": "so this would result in this\nfree trajectory, so let's say. So our trajectory\ndistribution will look like this, this heatmap\nthat you can see underneath,",
    "start": "2682150",
    "end": "2690020"
  },
  {
    "text": "right? So certain trajectories will\nbe more likely than others. And then through\nour policy gradient,",
    "start": "2690020",
    "end": "2696040"
  },
  {
    "text": "we would shift our\nprobability mass towards the trajectories that\nachieve higher reward, right?",
    "start": "2696040",
    "end": "2701710"
  },
  {
    "text": "So it's dependent on the\nsum of rewards right here. So the trajectories\nor the actions",
    "start": "2701710",
    "end": "2708040"
  },
  {
    "text": "that result in higher\nrewards, would increase their likelihood, and actions\nthat result in lower rewards",
    "start": "2708040",
    "end": "2715990"
  },
  {
    "text": "will decrease their likelihood. All right? So then we would change\nour trajectory distribution",
    "start": "2715990",
    "end": "2721120"
  },
  {
    "text": "to move towards the trajectories\nthat achieved higher returns.",
    "start": "2721120",
    "end": "2726140"
  },
  {
    "text": "So basically, it makes\nthe good stuff more likely and the bad stuff less likely. It just formalizes the\nnotion of trial and error.",
    "start": "2726140",
    "end": "2733430"
  },
  {
    "text": "It just formalizes\nit mathematically. ",
    "start": "2733430",
    "end": "2738790"
  },
  {
    "text": "All right, so to summarize,\nthe pros of policy gradients is that it's fairly simple.",
    "start": "2738790",
    "end": "2744500"
  },
  {
    "text": "It's easy, because of\nthat, it's fairly easy to combine with\nexisting multitask and meta-learning algorithms. And we'll talk about\nit in the next class.",
    "start": "2744500",
    "end": "2752650"
  },
  {
    "text": "And in terms of cons, the\ngradient that you see here, I can barely see my pointer.",
    "start": "2752650",
    "end": "2760599"
  },
  {
    "text": "This gradient, this\nestimate of this gradient, actually has very high variance. So people usually use various\ntricks to reduce that variance.",
    "start": "2760600",
    "end": "2769670"
  },
  {
    "text": "So in practice, it's usually\nused with so-called baselines. People use trust regions and\nall kinds of different tricks",
    "start": "2769670",
    "end": "2775390"
  },
  {
    "text": "to make it better. But it also requires\non-policy data. And what I mean by\nthis is the following.",
    "start": "2775390",
    "end": "2782150"
  },
  {
    "text": "So this expectation\nhere is with respect to the trajectory distribution\ninduced by our current policy",
    "start": "2782150",
    "end": "2789250"
  },
  {
    "text": "pi theta, so we\niteratively have to be running our current\npolicy all the time.",
    "start": "2789250",
    "end": "2795490"
  },
  {
    "text": "We can't really look at the data\nthat we generated in the past. We can only look at the data\nthat we generated right now.",
    "start": "2795490",
    "end": "2801880"
  },
  {
    "text": "The policy is improving\nwith every iteration. So we cannot reuse existing\nexperience to estimate",
    "start": "2801880",
    "end": "2808360"
  },
  {
    "text": "the gradient. We can only use the current\nexperience, which is actually a really big limitation.",
    "start": "2808360",
    "end": "2814180"
  },
  {
    "text": "And this is something we'll\ndiscuss a little bit more. We can sort of do it\nwith importance weights",
    "start": "2814180",
    "end": "2820630"
  },
  {
    "text": "but that produces\neven higher variance, and this is quite\ntricky to get to work",
    "start": "2820630",
    "end": "2826210"
  },
  {
    "text": "All right, so to\njust briefly tell you what's on-policy\nversus off-policy,",
    "start": "2826210",
    "end": "2831910"
  },
  {
    "text": "the on-policy\nmeans that the data has to come from\nthe current policy, as it does in the\npolicy gradient case.",
    "start": "2831910",
    "end": "2838960"
  },
  {
    "text": "It's compatible with all RL\nalgorithms, so any algorithm you'll learn about, it's\npossible to run it on-policy.",
    "start": "2838960",
    "end": "2844871"
  },
  {
    "text": "So that means you're\ngenerating the data, and then you're using that\ndata that you just generated to improve that policy.",
    "start": "2844872",
    "end": "2852370"
  },
  {
    "text": "But it cannot re-use data\nfrom previous policies. So you can only use\nthe data that you just generated with your\ncurrent policy, as opposed",
    "start": "2852370",
    "end": "2861280"
  },
  {
    "text": "to off-policy, where data\ncan come from any policy. And we'll learn\nabout it in a second.",
    "start": "2861280",
    "end": "2866590"
  },
  {
    "text": "It works with specific\nRL algorithms, with value-based reinforcement\nlearning algorithms.",
    "start": "2866590",
    "end": "2871900"
  },
  {
    "text": "And it's much more\nsample-efficient, because now we can do gradient\nsteps on the old data as well.",
    "start": "2871900",
    "end": "2877910"
  },
  {
    "text": "So we can be reusing the\ndata many, many times, as opposed to just running\nthe policy once, generating",
    "start": "2877910",
    "end": "2883870"
  },
  {
    "text": "the data, and you can only\nuse the data in this moment. And then you will\nimprove the policy. The policy will\nchange, then you have",
    "start": "2883870",
    "end": "2889462"
  },
  {
    "text": "to generate the data again. All right, so are\nthere any questions",
    "start": "2889462",
    "end": "2895770"
  },
  {
    "text": "regarding policy gradients? Yep. If I change\nsomething [INAUDIBLE]",
    "start": "2895770",
    "end": "2901035"
  },
  {
    "text": "like some components,\nfactory problem, like say through like\n[INAUDIBLE] out on policy.",
    "start": "2901035",
    "end": "2914220"
  },
  {
    "text": "So the question\nis, if you change-- Something perhaps like a\ncross-expectation trajectory,",
    "start": "2914220",
    "end": "2919490"
  },
  {
    "text": "yeah.  I see. So what would happen if you\nchange that expectation to be",
    "start": "2919490",
    "end": "2926490"
  },
  {
    "text": "under some other distribution. If you change your\nsampling strategy so you sample from\nsomething else, would that--",
    "start": "2926490",
    "end": "2932880"
  },
  {
    "text": "Would that still make it work. If I just do my code bot, I just\nchange the sampling structure. Like from your point,\nsampling from two labs,",
    "start": "2932880",
    "end": "2939060"
  },
  {
    "text": "some preoccupied subject. I see. So if you change your sampling\nstrategy but you would still",
    "start": "2939060",
    "end": "2946530"
  },
  {
    "text": "use the current\npolicy, I think what that would mean if\nyou would mean-- if you mean like\nyou would just do",
    "start": "2946530",
    "end": "2952320"
  },
  {
    "text": "uniform sampling, uniform\nsampling of trajectories, that's not really\nfeasible to do.",
    "start": "2952320",
    "end": "2958530"
  },
  {
    "text": "Or you would have to have a\nuniform policy then, right? So the policy would assign\nthe same probability",
    "start": "2958530",
    "end": "2963930"
  },
  {
    "text": "to every action\nthat it can perform. And then that would be\nthe uniform sampling",
    "start": "2963930",
    "end": "2969180"
  },
  {
    "text": "strategy of trajectories. But then you are\nchanging the policy. How to imply my large--",
    "start": "2969180",
    "end": "2976200"
  },
  {
    "text": "I use my policy to\ngenerate trajectories, that I sample those trajectories\nthat were the same weight,",
    "start": "2976200",
    "end": "2983520"
  },
  {
    "text": "but I call that\nkind of sampling, about doing uniform sampling. But another like kind\nof prioritized sampling",
    "start": "2983520",
    "end": "2991260"
  },
  {
    "text": "is by a sample with true\nmore recent [INAUDIBLE] I see. I see.",
    "start": "2991260",
    "end": "2997188"
  },
  {
    "text": "OK, so you would still generate\nthe trajectories the same way, but then you would use them\nto estimate the expectations",
    "start": "2997188",
    "end": "3003109"
  },
  {
    "text": "differently. Yeah, so you can do that. And usually, people do this\nwith importance sampling,",
    "start": "3003110",
    "end": "3008849"
  },
  {
    "text": "which tells you exactly\nhow you should change those and how you should\nsample them, so that you adjust for\nthe probabilities",
    "start": "3008850",
    "end": "3014630"
  },
  {
    "text": "of your current policy. But that results in\neven higher variance. So that's tricky to do.",
    "start": "3014630",
    "end": "3019820"
  },
  {
    "text": "But, yeah, it's possible. Yep. But what happens if you get\na trajectory that's like not very common, because it's making\nthe trajectories more likely.",
    "start": "3019820",
    "end": "3029450"
  },
  {
    "text": "And so if you ever get\nto a trajectory that's not very common, you wouldn't\nreally know what to do.",
    "start": "3029450",
    "end": "3034500"
  },
  {
    "text": "I feel like in other\nalgorithms, you can add points to\nincrease like provision.",
    "start": "3034500",
    "end": "3039907"
  },
  {
    "text": "For example, If you add this\ncase, it wouldn't be on policy. Yeah, this is a great question.",
    "start": "3039907",
    "end": "3045390"
  },
  {
    "text": "So just to repeat\nthe question, you were saying that in this\ncase, you can't really change the policy that you're\ndoing the rollouts with.",
    "start": "3045390",
    "end": "3052610"
  },
  {
    "text": "You can't add noise to it\nor something like this. And if you just, by luck, you're\njust got really high return",
    "start": "3052610",
    "end": "3058579"
  },
  {
    "text": "for that particular policy,\nbecause you just rolled it out once, then you would\nshift your probability",
    "start": "3058580",
    "end": "3066740"
  },
  {
    "text": "mass towards that policy, even\nthough you were just lucky. And, yeah, this is the\nproblem with high variance.",
    "start": "3066740",
    "end": "3072210"
  },
  {
    "text": "So you are just estimating\nthe entire objective, based on few rollouts or a\nsingle rollout in this case. And if you were\njust lucky, then you",
    "start": "3072210",
    "end": "3078859"
  },
  {
    "text": "didn't estimate that\ngradient correctly. And then you'd shift\nthat distribution mass",
    "start": "3078860",
    "end": "3084908"
  },
  {
    "text": "towards the thing that you\nshouldn't be really shifting it. And we'll talk about how\nwe can improve on that.",
    "start": "3084908",
    "end": "3091550"
  },
  {
    "text": "This is a great question. All right, cool. All right, so we talk about\noff-policy and on-policy.",
    "start": "3091550",
    "end": "3098790"
  },
  {
    "text": "So there is one more small note\nthat I wanted to make here. So let's take a look at\nour formula here again.",
    "start": "3098790",
    "end": "3105290"
  },
  {
    "text": "This is our policy\ngradient formula. And I'm going to make a slight\ntransformation of this formula",
    "start": "3105290",
    "end": "3111700"
  },
  {
    "text": "here. So what we are doing here is\nwe are summing a bunch of terms in this parentheses.",
    "start": "3111700",
    "end": "3118490"
  },
  {
    "text": "And then we are multiplying\nit by a bunch of terms that we are summing as\nwell in these parentheses.",
    "start": "3118490",
    "end": "3126270"
  },
  {
    "text": "So basically, this corresponds\nto something like this. We have a plus b plus c,\nright, times x plus y plus z.",
    "start": "3126270",
    "end": "3142160"
  },
  {
    "text": "Right? So what we can do instead is\nwe can rewrite this as a times",
    "start": "3142160",
    "end": "3150440"
  },
  {
    "text": "x plus y plus z plus bx plus\ny plus z plus c times x plus y",
    "start": "3150440",
    "end": "3160849"
  },
  {
    "text": "plus z. All right? So this is just a\nslight re-write.",
    "start": "3160850",
    "end": "3165970"
  },
  {
    "text": "All right, so\nlet's do that here. What this would result in is\nwe now will be summing over--",
    "start": "3165970",
    "end": "3172520"
  },
  {
    "text": "where is the pointer. Let me just show\nyou with my hand. So we'll be summing\nover all of these terms,",
    "start": "3172520",
    "end": "3179780"
  },
  {
    "text": "and each one of\nthese terms will be multiplied by the sum\nof the rewards, right? This is the exact\nsame transformation",
    "start": "3179780",
    "end": "3185360"
  },
  {
    "text": "that we just did on\nthe whiteboard, right? So let's do that. But now, there is one thing\nthat is a little peculiar.",
    "start": "3185360",
    "end": "3193500"
  },
  {
    "text": "So we are summing for\na particular action that we took at a certain time\nstep t for a trajectory i.",
    "start": "3193500",
    "end": "3201809"
  },
  {
    "text": "We will be evaluating this\nand we will be potentially increasing the likelihood\nof that action, based",
    "start": "3201810",
    "end": "3208350"
  },
  {
    "text": "on the sum of all of the\nrewards that we achieved for that entire trajectory,\nso starting from time step",
    "start": "3208350",
    "end": "3213599"
  },
  {
    "text": "1 up until the end, all right? So in other case, we have\nthis whole trajectory",
    "start": "3213600",
    "end": "3219000"
  },
  {
    "text": "that we are looking at. And we are in a particular\npoint in that trajectory, let's say in this green\npoint right here, all right?",
    "start": "3219000",
    "end": "3226930"
  },
  {
    "text": "And now, we are summing all of\nthe rewards, the rewards that came before that we\nare taking this action",
    "start": "3226930",
    "end": "3232950"
  },
  {
    "text": "and what the rewards that came\nafter we took that action. And we use that entire sum to\ntell us whether this action was",
    "start": "3232950",
    "end": "3239160"
  },
  {
    "text": "good or bad. Can anybody tell me\nwhat's wrong with that? Yep.",
    "start": "3239160",
    "end": "3244650"
  },
  {
    "text": "The actions should only be\nbased on like you should only roll out from that\npoint in time,",
    "start": "3244650",
    "end": "3250589"
  },
  {
    "text": "that's what our e is based on\nwhat you learned at the top. That's right. Yeah, so the action, that can't\nreally control the past, right?",
    "start": "3250590",
    "end": "3256260"
  },
  {
    "text": "It only influences the future. So what we can do\ninstead, is we can just sum together the\nrewards, starting",
    "start": "3256260",
    "end": "3262620"
  },
  {
    "text": "from that action moving forward. And this would actually reduce\nthe variance of our estimate.",
    "start": "3262620",
    "end": "3267790"
  },
  {
    "text": "So we can do this slight change. And instead of summing the\nrewards at time step 1,",
    "start": "3267790",
    "end": "3274530"
  },
  {
    "text": "we can sum the rewards\nstarting at time step t, at our current time\nstep where we're at.",
    "start": "3274530",
    "end": "3280410"
  },
  {
    "text": "And this is what we\ncall the rewards to go. So this is how much\nmore reward there is left, starting from this\naction up until the end.",
    "start": "3280410",
    "end": "3289320"
  },
  {
    "text": "All right, yeah. The two will be the same\nweight in the correct format.",
    "start": "3289320",
    "end": "3295080"
  },
  {
    "text": "Yes, but the variance of\nthe estimate will be lower. That's right. All right, so this summarizes\nour policy gradient part.",
    "start": "3295080",
    "end": "3302520"
  },
  {
    "text": "And now let's talk\nabout Q-learning. So this would allow\nus, we'll talk about off-policy\nalgorithms, that where",
    "start": "3302520",
    "end": "3309030"
  },
  {
    "text": "we don't need to have this\nassumption where you have to roll out the policy,\nand that you can only use",
    "start": "3309030",
    "end": "3314670"
  },
  {
    "text": "those samples to improve it. And it's generally a very\nexciting area of reinforcement",
    "start": "3314670",
    "end": "3320609"
  },
  {
    "text": "learning research. And it's used all\nover the place. And it's very commonly\nused in robotics, where we very much care\nabout data efficiency,",
    "start": "3320610",
    "end": "3328230"
  },
  {
    "text": "and we want to take\nmultiple gradient steps over the same data.",
    "start": "3328230",
    "end": "3333520"
  },
  {
    "text": "All right, so we talked about\nthe anatomy of reinforcement learning algorithm. And we talked about\none particular way",
    "start": "3333520",
    "end": "3340390"
  },
  {
    "text": "of doing reinforcement learning,\nwhich is with policy gradients, so directly optimizing\nin the objective,",
    "start": "3340390",
    "end": "3345760"
  },
  {
    "text": "doing the gradient\nascent procedure, and this is how we can\napproximate this gradient.",
    "start": "3345760",
    "end": "3351990"
  },
  {
    "text": "And this green part\nhere corresponded, or the sum of\nrewards corresponds",
    "start": "3351990",
    "end": "3357270"
  },
  {
    "text": "to the green box, where we\nare trying to fit the model to estimate the return. Here we do multiple\nrollouts, that",
    "start": "3357270",
    "end": "3364050"
  },
  {
    "text": "correspond to the orange box. And the whole thing corresponds\nto improving the policy. This is the gradient\nascent procedure.",
    "start": "3364050",
    "end": "3372400"
  },
  {
    "text": "All right, so now let's think\na little bit how we can improve on that policy gradient. And the exact way I\nwould like to improve",
    "start": "3372400",
    "end": "3379150"
  },
  {
    "text": "is by improving the green box. So we will try to improve that\nestimate of the rewards to go.",
    "start": "3379150",
    "end": "3387160"
  },
  {
    "text": "All right, so let's call\nthis some kind of estimate. We somehow need to estimate\nwhat kind of rewards",
    "start": "3387160",
    "end": "3394690"
  },
  {
    "text": "we are going to get starting\nfrom a current time step up until the end if I\ncontinue with this policy.",
    "start": "3394690",
    "end": "3399880"
  },
  {
    "text": "And let's call this estimate the\nestimate of the reward to go, Q hat for the\nparticular trajectory",
    "start": "3399880",
    "end": "3405730"
  },
  {
    "text": "i at the specific time step t. So this is the estimate\nof our expected reward,",
    "start": "3405730",
    "end": "3411520"
  },
  {
    "text": "if we take a specific action,\na i t in a state s i t.",
    "start": "3411520",
    "end": "3417840"
  },
  {
    "text": "Now can we get a\nbetter estimate of that rewards to go than what\nwe are currently doing?",
    "start": "3417840",
    "end": "3424950"
  },
  {
    "text": "And for this, let's go back\nto our little trajectory plot. So right now, what\nwe are doing is",
    "start": "3424950",
    "end": "3432300"
  },
  {
    "text": "we are summing together\nall of the rewards starting at our green\ndot up until the end.",
    "start": "3432300",
    "end": "3440010"
  },
  {
    "text": "We were summing the rewards\nthat we got from this trajectory that we are actually executing\nin the world, all right?",
    "start": "3440010",
    "end": "3446610"
  },
  {
    "text": "So this is how we are just\nsumming the rewards that we got",
    "start": "3446610",
    "end": "3452730"
  },
  {
    "text": "for that specific trajectory. All right, so let's\nnow imagine that we",
    "start": "3452730",
    "end": "3458580"
  },
  {
    "text": "are trying to do this\nfor our life, all right? So let's say you're trying to\nevaluate a certain decision",
    "start": "3458580",
    "end": "3465630"
  },
  {
    "text": "that you made in your life. Let's say you decided\nto apply to Stanford. All right, and now you\nwant to estimate how good",
    "start": "3465630",
    "end": "3475110"
  },
  {
    "text": "was that decision, all right? And the way we want\nto do this, or the way we are doing it\nhere, is that you",
    "start": "3475110",
    "end": "3481289"
  },
  {
    "text": "wait until the very\nend of your life. And then you look back\nat all the rewards",
    "start": "3481290",
    "end": "3486570"
  },
  {
    "text": "that you gathered,\nstarting from the decision that you made to\napply to Stanford up until the very,\nvery end, you know,",
    "start": "3486570",
    "end": "3493260"
  },
  {
    "text": "you're on your deathbed\nthinking about this. And then, based on this, based\non the sum of these rewards,",
    "start": "3493260",
    "end": "3499770"
  },
  {
    "text": "you can now tell was\nthat action good or bad. All right?",
    "start": "3499770",
    "end": "3505599"
  },
  {
    "text": "So there is this movie, and\nI apologize for spoilers. I don't think that's\ntoo big of a spoiler.",
    "start": "3505600",
    "end": "3510940"
  },
  {
    "text": "But let's just think\nabout this movie. There's this movie called\n\"About Time,\" where",
    "start": "3510940",
    "end": "3516280"
  },
  {
    "text": "the main character, I\nthink his name is Tim, he gets to travel in time. All right?",
    "start": "3516280",
    "end": "3521740"
  },
  {
    "text": "So I think the way it\nworks is that he just needs to close his eyes and\nclose his fists really hard,",
    "start": "3521740",
    "end": "3527020"
  },
  {
    "text": "and then think of a moment\nwhere he wants to get back to. And then he can just live\nhis life kind of again",
    "start": "3527020",
    "end": "3532299"
  },
  {
    "text": "from that moment on, all right? So now if Tim were to\nevaluate his decision",
    "start": "3532300",
    "end": "3539860"
  },
  {
    "text": "to apply to Stanford,\nwhat he would do is that he would go\nback to that point",
    "start": "3539860",
    "end": "3545512"
  },
  {
    "text": "where he made the decision,\nor to the time step right after this, and live\nmultiple lives, right?",
    "start": "3545512",
    "end": "3550990"
  },
  {
    "text": "So live that life many,\nmany times, and kind of go to the very end, calculate\nall of the rewards,",
    "start": "3550990",
    "end": "3556569"
  },
  {
    "text": "and then teleport back\nin time to the decision when he decided to\napply to Stanford,",
    "start": "3556570",
    "end": "3561610"
  },
  {
    "text": "and then live that life again. So his estimate of\nthe rewards to go would be much, much\nbetter than yours, right?",
    "start": "3561610",
    "end": "3568000"
  },
  {
    "text": "Your estimate is just\nbased on your life. And maybe there was a\nlittle bit of luck involved,",
    "start": "3568000",
    "end": "3573610"
  },
  {
    "text": "and your life turned out\nto be an amazing life. But it actually\ndidn't have anything to do with you\napplying to Stanford.",
    "start": "3573610",
    "end": "3579520"
  },
  {
    "text": "You were just, maybe, you\nknow, you won the lottery a little bit later. And it just seemed like this\nwas an extremely happy life,",
    "start": "3579520",
    "end": "3587320"
  },
  {
    "text": "and you thought that the\nreward to go was very high. But it's just because of the\nstochasticity of the dynamics.",
    "start": "3587320",
    "end": "3592750"
  },
  {
    "text": "But Tim would be able to live\nthat life many, many times, and kind of integrate out all\nthe little lucky and unlucky",
    "start": "3592750",
    "end": "3600640"
  },
  {
    "text": "things, so that he can\nactually tell what's the reward to go if I apply to Stanford.",
    "start": "3600640",
    "end": "3607810"
  },
  {
    "text": "All right, so Tim can estimate\nthe future sum of rewards. This is still under the\nexpectation of Tim of pi theta.",
    "start": "3607810",
    "end": "3617400"
  },
  {
    "text": "So he still lives as Tim\nevery single time, right? Like he doesn't make\nrandom actions after this.",
    "start": "3617400",
    "end": "3623270"
  },
  {
    "text": "He is just Tim, so he behaves\nlike Tim the entire time. So but his estimate\nof the rewards to go",
    "start": "3623270",
    "end": "3629990"
  },
  {
    "text": "is much better than s, right?  Cool, so we called\nthe Q hat the estimate",
    "start": "3629990",
    "end": "3639130"
  },
  {
    "text": "of the expected reward. So let's call Q the\ntrue expected reward to go, all right?",
    "start": "3639130",
    "end": "3644500"
  },
  {
    "text": "So this is actually\nthe true expectation, under the expectation under our\ncurrent policy of the rewards",
    "start": "3644500",
    "end": "3650865"
  },
  {
    "text": "that we are going to-- of\nthe sum of the rewards we are going to experience. ",
    "start": "3650865",
    "end": "3657060"
  },
  {
    "text": "And now, if we could do\nthis, if we could somehow get a better estimate\nof the rewards to go,",
    "start": "3657060",
    "end": "3663920"
  },
  {
    "text": "then the entire\ngradient estimate would be better, right? So because we would\nbe improving on this,",
    "start": "3663920",
    "end": "3671090"
  },
  {
    "text": "the whole gradient\nwould be better. So basically, Tim not only\nwould be able to better estimate",
    "start": "3671090",
    "end": "3676880"
  },
  {
    "text": "how many rewards are there\nto go, based on the decision, but he would also be able\nto make better decisions.",
    "start": "3676880",
    "end": "3685450"
  },
  {
    "text": "All right, so now let's\nintroduce three concepts here.",
    "start": "3685450",
    "end": "3690890"
  },
  {
    "text": "So first, this function Q\nthat we are talking about, which is the estimated, the\nexpected sum of rewards,",
    "start": "3690890",
    "end": "3698800"
  },
  {
    "text": "let's continue\ncalling it Q. And this would be the sum of the\nrewards under the expectation of our current\npolicy, given that we",
    "start": "3698800",
    "end": "3704560"
  },
  {
    "text": "start at a certain state\ndoing certain actions. So this is the total\nrewards that we",
    "start": "3704560",
    "end": "3709690"
  },
  {
    "text": "would get from taking\naction at in state st and then following our policy.",
    "start": "3709690",
    "end": "3714980"
  },
  {
    "text": "So in fact, we shouldn't\njust call it Q, but this Q is also\ndependent on our policy.",
    "start": "3714980",
    "end": "3720130"
  },
  {
    "text": "So let's call it Q pi. So this is the Q function\ngiven the current state",
    "start": "3720130",
    "end": "3726362"
  },
  {
    "text": "and the action\nwe're going to take. And then we continue acting\naccording to our policy. Tim continues being Tim.",
    "start": "3726362",
    "end": "3731470"
  },
  {
    "text": "Yeah, there's a question. Yes, sir, a number. Like based on the\nstructure of the reward,",
    "start": "3731470",
    "end": "3740740"
  },
  {
    "text": "but is the total\npi fixed, or is it like you can keep\n[INAUDIBLE] to make decisions",
    "start": "3740740",
    "end": "3746800"
  },
  {
    "text": "and correcting your states for,\nthat's all [INAUDIBLE] like you have serious [INAUDIBLE] Right. Is it like you could do this\nfor ranking for the reward.",
    "start": "3746800",
    "end": "3753873"
  },
  {
    "text": "Yeah. Yeah, that's a good question. So the question was,\ndo we get to act for a certain number\nof time steps,",
    "start": "3753873",
    "end": "3759160"
  },
  {
    "text": "or do we get to act up until the\nvery last state, or some kind of final state. And these are two\ndifferent formulations.",
    "start": "3759160",
    "end": "3765820"
  },
  {
    "text": "And you can do either one. You can do the\ntime-based version,",
    "start": "3765820",
    "end": "3771430"
  },
  {
    "text": "where you act for a certain\nnumber of time steps, capital T. Or you can do the infinite\nhorizon reinforcement learning,",
    "start": "3771430",
    "end": "3777613"
  },
  {
    "text": "where instead, you\nintroduce a little discount factor that allows you\nto deal with the math. Yeah.",
    "start": "3777613",
    "end": "3784329"
  },
  {
    "text": "Cool, all right, so we\nhave the Q function. Now let's introduce\nalso another function,",
    "start": "3784330",
    "end": "3790990"
  },
  {
    "text": "V. We'll call it\nthe value function. And the value function would\nbe just the expectation",
    "start": "3790990",
    "end": "3796210"
  },
  {
    "text": "of our Q function according to\nour policy, under our policy. So it's very similar\nto the Q function,",
    "start": "3796210",
    "end": "3803170"
  },
  {
    "text": "except the Q function,\nyou had the choice of picking any action you\nwant at the current time step,",
    "start": "3803170",
    "end": "3808870"
  },
  {
    "text": "right? The Q function is\ndependent on that action. So Tim can decide to apply to\nStanford, do something else, so then he'll do something\ncompletely random, and so on.",
    "start": "3808870",
    "end": "3815300"
  },
  {
    "text": "But after he makes\nthat decision, he just continues acting as Tim. He doesn't have any\nfreedom anymore.",
    "start": "3815300",
    "end": "3821230"
  },
  {
    "text": "Versus in value\nfunction, Tim gets to teleport to a certain\nstate, and he gets to act as Tim straight away.",
    "start": "3821230",
    "end": "3827230"
  },
  {
    "text": "He can't really try\ndifferent things and do something completely\nrandom that is very unlike Tim.",
    "start": "3827230",
    "end": "3834369"
  },
  {
    "text": "All right, and then we also\nhave the advantage function, which we call A pi of\nst, at, which is just",
    "start": "3834370",
    "end": "3842710"
  },
  {
    "text": "the difference between the two. So it's the difference between\nthe Q function and the value function. And that just tells\nus how much better",
    "start": "3842710",
    "end": "3850030"
  },
  {
    "text": "is the action at compared to\nthe standard action that the Tim would have taken, right?",
    "start": "3850030",
    "end": "3856000"
  },
  {
    "text": "So this is just how\nmuch better is action at compared to the action that\nthe policy would have picked. ",
    "start": "3856000",
    "end": "3863890"
  },
  {
    "text": "All right, so now one way we can\nrewrite our policy gradient is",
    "start": "3863890",
    "end": "3868930"
  },
  {
    "text": "rather than summing\nthe sum of the rewards that we got for the\nparticular trajectory, we can use the advantage\nfunction or the Q function",
    "start": "3868930",
    "end": "3876250"
  },
  {
    "text": "or something like that.  All right, so now in terms of\nthe anatomy of our algorithm,",
    "start": "3876250",
    "end": "3888710"
  },
  {
    "text": "what we are changing is that we\nare still generating samples. But to fit the model, we\nwill now fit the model",
    "start": "3888710",
    "end": "3894650"
  },
  {
    "text": "to estimate the returns. We want to just sum the\nreturns, the rewards that we got for that specific trajectory.",
    "start": "3894650",
    "end": "3900020"
  },
  {
    "text": "But instead we'll try to fit\nour Q function, value function, or the advantage function. We have these three\nthings that we",
    "start": "3900020",
    "end": "3906410"
  },
  {
    "text": "can use that help us\nestimate the future rewards. And then, to improve\nthe policy, we",
    "start": "3906410",
    "end": "3912230"
  },
  {
    "text": "will continue doing our\ngradient ascent procedure. ",
    "start": "3912230",
    "end": "3917610"
  },
  {
    "text": "All right, so we just introduced\nthese three new concepts. So I wanted to run\na little exercise",
    "start": "3917610",
    "end": "3923190"
  },
  {
    "text": "to see if that makes sense. So the exercise\nis the following. You are this little\nstick figure.",
    "start": "3923190",
    "end": "3930960"
  },
  {
    "text": "And you start at some\nst. And your dream is to become a\ndrummer, all right?",
    "start": "3930960",
    "end": "3938460"
  },
  {
    "text": "And what that means is that\nyou will be evaluated a month from now on this drummer test.",
    "start": "3938460",
    "end": "3946770"
  },
  {
    "text": "And you get a reward equal\n1 if you can play it, and if you can play it in a\nmonth, and 0 otherwise, right?",
    "start": "3946770",
    "end": "3953940"
  },
  {
    "text": "So you'll be\nevaluated in a month on how good are you at\nachieving your dream of becoming a drummer.",
    "start": "3953940",
    "end": "3960240"
  },
  {
    "text": "And now you have three\nactions that you can do. One action is just chill,\njust don't do anything.",
    "start": "3960240",
    "end": "3966960"
  },
  {
    "text": "The second action\nis that you can watch other drummers perform. You can watch videos of them\nand try to learn from that.",
    "start": "3966960",
    "end": "3974370"
  },
  {
    "text": "And the third action is you\ncan actually practice drumming. You can play and try\nto get better that way.",
    "start": "3974370",
    "end": "3981520"
  },
  {
    "text": "Then your current\npolicy is the following. Your current policy is that\nyou always take action a1.",
    "start": "3981520",
    "end": "3987240"
  },
  {
    "text": "You always want to chill, right? And all of the probability\nmasses for that action. So you will always do that.",
    "start": "3987240",
    "end": "3995619"
  },
  {
    "text": "So now my question is, what's\nthe value function, V pi of st? ",
    "start": "3995620",
    "end": "4003750"
  },
  {
    "text": "Yes? Most likely for the\nzero, you could always show like, given that you\nwant to feel from sideways.",
    "start": "4003750",
    "end": "4011150"
  },
  {
    "text": "Right. Yeah. So the value function\nis most likely 0, because we want to\nbe able to board. Like he's like the following.",
    "start": "4011150",
    "end": "4016408"
  },
  {
    "text": "Yeah, unless you're\nsuper-talented and you can really-- Yes? Is it still not\npossible for, OK,",
    "start": "4016408",
    "end": "4021650"
  },
  {
    "text": "so what do you mean\nby V pi template. I mean, is it a common\nentity transition? Because, I mean, if\nI compartment, then",
    "start": "4021650",
    "end": "4028725"
  },
  {
    "text": "I would never, I wouldn't\nbe able to pick up these likely deterministic,\nor is this stochastic?",
    "start": "4028725",
    "end": "4036869"
  },
  {
    "text": "Well, you know, let's consider\nthis in real life, right? So like you're being\nborn right now.",
    "start": "4036870",
    "end": "4042760"
  },
  {
    "text": "And these are the actions\nyou can do at your current, so the t is equal 0. Or maybe that shouldn't\nbe in a month, that should",
    "start": "4042760",
    "end": "4048870"
  },
  {
    "text": "be like 10 years or something. And then the reward function\nitself is not stochastic,",
    "start": "4048870",
    "end": "4055030"
  },
  {
    "text": "right? The reward function, given\na state and an action, so given you playing\nthe drums, can tell you,",
    "start": "4055030",
    "end": "4061950"
  },
  {
    "text": "did you play it or not, right? It can give you a reward 1 or 0. Sorry. What's the foundation?",
    "start": "4061950",
    "end": "4067859"
  },
  {
    "text": "That's what I think. Yeah, so the\nprobability of you can't",
    "start": "4067860",
    "end": "4073950"
  },
  {
    "text": "play that, given your\ncompartment, yeah, is sign of 1.",
    "start": "4073950",
    "end": "4081360"
  },
  {
    "text": "Correct. Yeah, so for the purpose of this\nexercise, we would assume that. Yeah, so there is\nno stochasticity",
    "start": "4081360",
    "end": "4087180"
  },
  {
    "text": "in their transmission dynamics\nthat somehow by luck you can play it.",
    "start": "4087180",
    "end": "4092820"
  },
  {
    "text": "Or you will just\nrandomly play the drums and somehow it ended up\nbeing that you played what they asked you to play.",
    "start": "4092820",
    "end": "4097859"
  },
  {
    "text": "And then the probability\nof that is 0. Yeah. All right, cool, so the\nvalue function is 0.",
    "start": "4097859",
    "end": "4104609"
  },
  {
    "text": "Now what's the Q function?  The Q of pi.",
    "start": "4104609",
    "end": "4110710"
  },
  {
    "text": "Better ask somebody else maybe. Yes? For the same thing in this case.",
    "start": "4110710",
    "end": "4116926"
  },
  {
    "text": "For the same thing\nin this case, yeah. And that's part of the question. What do you think it is?",
    "start": "4116927",
    "end": "4122620"
  },
  {
    "text": "So you have to, the answer is\na little bit longer than just in the value function case.",
    "start": "4122620",
    "end": "4127915"
  },
  {
    "start": "4127915",
    "end": "4132990"
  },
  {
    "text": "OK? 0.001. 0.001?",
    "start": "4132990",
    "end": "4139409"
  },
  {
    "text": "0-- would the Q get worse,\nmore like the-- would it fall?",
    "start": "4139409",
    "end": "4144457"
  },
  {
    "text": "It would be a\nfloating point value. So a Q function just\ntells you the predicted sum of future rewards, right?",
    "start": "4144457",
    "end": "4150540"
  },
  {
    "text": "So this is just a single scalar. Yeah. ",
    "start": "4150540",
    "end": "4155770"
  },
  {
    "text": "It'll be 0 for all actions,\nbecause the minute you choose to play drums, that is to say\nyou always connect to the next",
    "start": "4155770",
    "end": "4162210"
  },
  {
    "text": "state, which was\nstill to [INAUDIBLE] .  Right, yeah, so\nthe answer you gave",
    "start": "4162210",
    "end": "4170318"
  },
  {
    "text": "was that it would be\n0 for all the actions. So what that means is that the\nQ for st, at, where at equals a1",
    "start": "4170319",
    "end": "4180759"
  },
  {
    "text": "would be 0, because that will\nbe exactly the same as the value function, right? You continue doing nothing. You continue chilling.",
    "start": "4180760",
    "end": "4187609"
  },
  {
    "text": "If it was a2, so Q pi of st,\na2 would be maybe a little bit",
    "start": "4187609",
    "end": "4194290"
  },
  {
    "text": "higher, but over a\nzero, maybe you'll learn like a tiny, tiny bit. Maybe there's a tiny\nbit of probability",
    "start": "4194290",
    "end": "4199510"
  },
  {
    "text": "that you somehow\nlearn how to play it. But you're saying\nprobably not, zero again.",
    "start": "4199510",
    "end": "4204670"
  },
  {
    "text": "And now a3 means that you only\ndecided to play in that time step, right?",
    "start": "4204670",
    "end": "4209739"
  },
  {
    "text": "You decided to practice\nthis one time step. But after this, you continue\nacting as you, with your policy",
    "start": "4209740",
    "end": "4215080"
  },
  {
    "text": "pi, which is to always chill. So that means you decided\nto practice once and then you chill afterwards.",
    "start": "4215080",
    "end": "4221420"
  },
  {
    "text": "So you're saying\nit will still be 0. Let's give it the\nbenefit of the doubt, maybe it will be a little\nbit higher, all right?",
    "start": "4221420",
    "end": "4226770"
  },
  {
    "text": "You at least practice once. Yep. Now if the Q function is\none number for each action",
    "start": "4226770",
    "end": "4233870"
  },
  {
    "text": "you can take? That's correct. That's correct. So given a specific state\nand a specific action,",
    "start": "4233870",
    "end": "4239990"
  },
  {
    "text": "it's one number. Here because we are\nchoosing between three different actions, we have\nto say for that action,",
    "start": "4239990",
    "end": "4245842"
  },
  {
    "text": "it will be this number. For this action it\nwill be that number. For that action it will\nbe a different number. ",
    "start": "4245842",
    "end": "4252770"
  },
  {
    "text": "So the Q function kind of thinks\nabout the immediate action that you can take. And then you act according\nto your policy afterwards.",
    "start": "4252770",
    "end": "4261370"
  },
  {
    "text": "All right, cool. Now one more question was\nthe advantage function.",
    "start": "4261370",
    "end": "4266390"
  },
  {
    "text": "So A pi of st, at. ",
    "start": "4266390",
    "end": "4272830"
  },
  {
    "text": "Yeah? It was useful, to\nbe valuable, come 0? Yes, so it would be the\nsame as the Q function,",
    "start": "4272830",
    "end": "4278740"
  },
  {
    "text": "because the value function is 0. So the advantage function\nis the difference between the Q function and\nthe value function, right?",
    "start": "4278740",
    "end": "4285790"
  },
  {
    "text": "So because in our case,\nthe value function was 0, it would be exactly the\nsame as the Q function.",
    "start": "4285790",
    "end": "4293650"
  },
  {
    "text": "Right, does that make sense? Are there any questions on that?",
    "start": "4293650",
    "end": "4299880"
  },
  {
    "text": "All right, cool. All right, so there's\none more thing",
    "start": "4299880",
    "end": "4305400"
  },
  {
    "text": "that I wanted to ask you about. So before we were talking\nabout summing together the rewards from\nthe trajectories",
    "start": "4305400",
    "end": "4311340"
  },
  {
    "text": "that we actually experienced. Now we talked a\nlittle bit how we can do this a little\nbit better, like kind",
    "start": "4311340",
    "end": "4316590"
  },
  {
    "text": "of Tim, how Tim was doing this. But Tim still had to live\nhis life till the very end",
    "start": "4316590",
    "end": "4321960"
  },
  {
    "text": "to evaluate that action that\nhe took sometime in his 20s, let's say.",
    "start": "4321960",
    "end": "4329040"
  },
  {
    "text": "So let's consider\nthe game of chess. And in the game of\nchess, let's say you get a reward 1 at\nthe very end if you win,",
    "start": "4329040",
    "end": "4336330"
  },
  {
    "text": "and reward 0 if you lose. And, you know, what we\ndiscussed so far just talks",
    "start": "4336330",
    "end": "4343770"
  },
  {
    "text": "about predictions, right? Like how well can you predict\nthe sum of the future reward, so how well can you\npredict whether you're",
    "start": "4343770",
    "end": "4350070"
  },
  {
    "text": "going to win in chess match or\nwhether you're going to lose. And so far what we've been\ntalking about is that how",
    "start": "4350070",
    "end": "4359670"
  },
  {
    "text": "every time you would\nupdate your predictions, you would have to wait until\nthe very end of the game",
    "start": "4359670",
    "end": "4366030"
  },
  {
    "text": "to tell whether-- to update your belief\nabout the game. You would have to\nwait, make a move,",
    "start": "4366030",
    "end": "4371483"
  },
  {
    "text": "and then after you made\nthat move, you have to wait, continue playing the game\nand on the very, very end,",
    "start": "4371483",
    "end": "4377290"
  },
  {
    "text": "once you get all the rewards,\nyou get the reward 1 or 0, you'll be able to tell, oh,\nwell, I was wrong, right?",
    "start": "4377290",
    "end": "4383670"
  },
  {
    "text": "But that's not actually\nhow it works, right? So like what happens if\nyou don't finish the game?",
    "start": "4383670",
    "end": "4388860"
  },
  {
    "text": "What happens if\nyou made the move, right, you thought that\nthis was a good move. And but to update\nthat prediction,",
    "start": "4388860",
    "end": "4395768"
  },
  {
    "text": "that model of yours, you had to\nwait till the very, very end. But let's say you waited\nuntil the very, very end, except for the very\nlast move, and the power",
    "start": "4395768",
    "end": "4403800"
  },
  {
    "text": "ran out or something. And you never finished\nthe game of chess, but you were just one move\naway from being checkmated.",
    "start": "4403800",
    "end": "4411270"
  },
  {
    "text": "So you were about to\nupdate your belief, but you never finished the game. So, sorry, you don't get\nto update your prediction.",
    "start": "4411270",
    "end": "4417480"
  },
  {
    "text": " So, in reality, we don't always\nwait till the end, right?",
    "start": "4417480",
    "end": "4424220"
  },
  {
    "text": "Like you can update your\nprediction right after you made that move, right? So like you made a\nmove and then you",
    "start": "4424220",
    "end": "4429500"
  },
  {
    "text": "realize in the very next\nmoment, oh, I screwed up, right? Like this is my probability\nof winning actually",
    "start": "4429500",
    "end": "4435140"
  },
  {
    "text": "went down significantly. I don't have to wait till\nthe very end of the game. I can update it\nimmediately after the move.",
    "start": "4435140",
    "end": "4443530"
  },
  {
    "text": "All right, so let's\ntry to do that. And we'll do this on the example\nwhere we were trying to fit,",
    "start": "4443530",
    "end": "4450200"
  },
  {
    "text": "we were trying to actually\nlearn the value function. So we will try to learn it\nusing supervised learning.",
    "start": "4450200",
    "end": "4457053"
  },
  {
    "text": "So we'll do supervised\nregression, where we have our value function,\nwhich is a neural network that takes as inputs some states,\noutputs a single scalar value.",
    "start": "4457053",
    "end": "4465410"
  },
  {
    "text": "And it's parameterized\nby some parameters phi. And we need some kind of target\nlabels for that value function,",
    "start": "4465410",
    "end": "4473130"
  },
  {
    "text": "all right? All right, so this is\nour value function. It just has a single\noutput, a single scalar.",
    "start": "4473130",
    "end": "4482230"
  },
  {
    "text": "So now our ideal target would\nbe the true expected sum of rewards, given that we\nstart at a certain state.",
    "start": "4482230",
    "end": "4488680"
  },
  {
    "text": "But we don't really\nhave access to that. Now the Monte Carlo target,\nwhat we've been using before",
    "start": "4488680",
    "end": "4495219"
  },
  {
    "text": "in policy gradients, would be\njust the sum of the rewards that we got for that\nspecific trajectory, up",
    "start": "4495220",
    "end": "4501040"
  },
  {
    "text": "until the very end, all right? Now, one thing\nthat we can see is",
    "start": "4501040",
    "end": "4507639"
  },
  {
    "text": "that we can approximate this. And this is actually a\nvery good approximation",
    "start": "4507640",
    "end": "4513340"
  },
  {
    "text": "with the following. We can take the reward that\nwe got at the current time step, plus the sum\nof the rewards,",
    "start": "4513340",
    "end": "4520449"
  },
  {
    "text": "so the true rewards to go, the\ntrue expected reward, starting from st plus 1, all right?",
    "start": "4520450",
    "end": "4527890"
  },
  {
    "text": "So this is kind of like\nour value function is equal to reward that I got\nin this state plus the state",
    "start": "4527890",
    "end": "4533125"
  },
  {
    "text": "that I actually experienced\nin my trajectory, and the expected\nvalue from that state. Now can anybody tell me why\nthis is an approximation here?",
    "start": "4533125",
    "end": "4541525"
  },
  {
    "start": "4541525",
    "end": "4548489"
  },
  {
    "text": "Any ideas? Yeah? This imported framework for\ntime step, so it's finite,",
    "start": "4548490",
    "end": "4553770"
  },
  {
    "text": "supported to your two-- so\nthis model unfortunately isn't. Right, so this is a final.",
    "start": "4553770",
    "end": "4561000"
  },
  {
    "text": "You're saying that this is\na final time step problem. So we have a final\nnumber of time steps? Yeah, that's not\nexactly it, right?",
    "start": "4561000",
    "end": "4566840"
  },
  {
    "text": "Because it would still\nwork even though we are summing over a certain\nnumber of time steps t.",
    "start": "4566840",
    "end": "4575050"
  },
  {
    "text": "This should still work, right? Like regardless of whether\nthis is an infinite horizon or finite horizon.",
    "start": "4575050",
    "end": "4580536"
  },
  {
    "text": "Yeah. We need to make some, like, for\nit to be approximately the same and approximately for then\nlike for action framing IT,",
    "start": "4580536",
    "end": "4588989"
  },
  {
    "text": "it would have to be\ncalled thus the given hour of actual\npolicy, at a review.",
    "start": "4588990",
    "end": "4594900"
  },
  {
    "text": "Yes, that's correct. Yeah, so basically\nthe approximation is here because\nthis st plus 1, this",
    "start": "4594900",
    "end": "4602610"
  },
  {
    "text": "is the state that we actually\nexperience in our trajectory, right? So it's kind of like let's\ngo back to Tim example.",
    "start": "4602610",
    "end": "4609570"
  },
  {
    "text": "What Tim would do is he would\nperform one action, right? He would teleport to a\ncertain moment in time.",
    "start": "4609570",
    "end": "4616440"
  },
  {
    "text": "He would take one action. And then he would\nsay, let's calculate",
    "start": "4616440",
    "end": "4623490"
  },
  {
    "text": "the value of everything\nthat I've done after this. And let's just say that\nit's equal to my value",
    "start": "4623490",
    "end": "4629130"
  },
  {
    "text": "at the previous state is\nthe reward that I got, plus everything\nthat I experience after this single action.",
    "start": "4629130",
    "end": "4634530"
  },
  {
    "text": "But he still just took\nthis one action, right? Like we still just\nteleported to the next state. And this is from the\nactual trajectory",
    "start": "4634530",
    "end": "4640980"
  },
  {
    "text": "that we've experienced. So this st plus 1, this\nis a small approximation that we're doing here.",
    "start": "4640980",
    "end": "4648290"
  },
  {
    "text": "Right, now one thing\nthat we can notice is that, even though\nthis is an approximation,",
    "start": "4648290",
    "end": "4653350"
  },
  {
    "text": "it's not too bad of\nan approximation. We are just making\nthis one step. We just think that we\ntook this one action and we experienced\na certain st plus 1.",
    "start": "4653350",
    "end": "4660400"
  },
  {
    "text": "But we can notice\nthat this part here is actually the value\nfunction at the next state,",
    "start": "4660400",
    "end": "4665876"
  },
  {
    "text": "OK, the state at\nthe next time step. So this is a little\nrecursive, right? Or the target at the current\nstate is equal to the reward",
    "start": "4665876",
    "end": "4674840"
  },
  {
    "text": "that we got at this state plus\nthe value at the next state. And we can approximate\nit actually even more,",
    "start": "4674840",
    "end": "4683429"
  },
  {
    "text": "or actually it will be a\nless accurate estimate, by saying that this would\nbe equal to the reward at the current time\nstep, plus the value",
    "start": "4683430",
    "end": "4690300"
  },
  {
    "text": "that we right now\nhave, our fitted value. So V pi of the parameters\nphi of st plus 1.",
    "start": "4690300",
    "end": "4699750"
  },
  {
    "text": "So basically we will be\nfitting this over and over. And then we'll be using that\nestimate, that neural network,",
    "start": "4699750",
    "end": "4707430"
  },
  {
    "text": "to give us the label\nfor the previous state. ",
    "start": "4707430",
    "end": "4713790"
  },
  {
    "text": "All right, so in that\ncase our training data would be the state,\nand the target would be the reward\nplus the value,",
    "start": "4713790",
    "end": "4720580"
  },
  {
    "text": "so what we actually are\nestimating at the next state. So this is a little recursive.",
    "start": "4720580",
    "end": "4727050"
  },
  {
    "text": "And this would be our target. And this is what we\nwould sometimes refer to as the bootstrap estimate.",
    "start": "4727050",
    "end": "4732480"
  },
  {
    "text": "We're estimating\nthe value function, bootstrapping it from itself. ",
    "start": "4732480",
    "end": "4739820"
  },
  {
    "text": "Cool, and a few examples. This is actually, this\npolicy evaluation, this estimating value function,\nhas been used many times.",
    "start": "4739820",
    "end": "4748250"
  },
  {
    "text": "Originally it was used in\nthe game of backgammon, where the reward was the\ngame outcome, whether you",
    "start": "4748250",
    "end": "4754430"
  },
  {
    "text": "win or lose. And the value function\nwould correspond to the expected outcome,\ngiven a board state.",
    "start": "4754430",
    "end": "4759829"
  },
  {
    "text": "So we would look at the\nboard, and the value function will tell you how likely it\nis that you are going to win.",
    "start": "4759830",
    "end": "4764930"
  },
  {
    "text": "It was also used in the\nexact same way in AlphaGo, where a computer was playing\nagainst the human champion",
    "start": "4764930",
    "end": "4770600"
  },
  {
    "text": "of Go. And the reward was\nthe game outcome, and the value function\nwas exactly the same.",
    "start": "4770600",
    "end": "4775710"
  },
  {
    "text": "It was a neural network that,\nby looking at the board, was able to tell you are you\ngoing to win or not, right? And the value function\nwas fitted in this way.",
    "start": "4775710",
    "end": "4782930"
  },
  {
    "text": " Cool, so we had our\nReinforce algorithm",
    "start": "4782930",
    "end": "4788655"
  },
  {
    "text": "that we were using before,\nwhere we were just rolling out the samples and then we were\nestimating the gradient. And for this estimation we were\nusing the sum of the rewards",
    "start": "4788655",
    "end": "4795900"
  },
  {
    "text": "that we have experienced. And now we arrived at a\nslightly different algorithm, which we call an online\nactor-critic algorithm.",
    "start": "4795900",
    "end": "4804239"
  },
  {
    "text": "So in that case, we\nwill take the action according to our policy. We will update our\nvalue function,",
    "start": "4804240",
    "end": "4811880"
  },
  {
    "text": "using the target that\nwe just described. So the value function\nof the current state is equal to reward\nplus the value",
    "start": "4811880",
    "end": "4818450"
  },
  {
    "text": "function of the next state. And here it's a\nlittle discounted, but you can ignore that. This is to deal with\nthe infinite horizon RL.",
    "start": "4818450",
    "end": "4826640"
  },
  {
    "text": "So we are fitting\nthe value function. Then we can evaluate\nthe advantage function, and we can do this by\njust saying the advantage",
    "start": "4826640",
    "end": "4833840"
  },
  {
    "text": "function of s, comma\na is equal to reward for that state and\naction, plus the value",
    "start": "4833840",
    "end": "4840050"
  },
  {
    "text": "at the next state,\nminus the value. So this is our Q\nfunction, in other words, minus the value at\nthe current state.",
    "start": "4840050",
    "end": "4847760"
  },
  {
    "text": "And given the\ndisadvantage function, we can now compute this\ngradient a little bit better, because now we have a better\nestimate of the rewards to go.",
    "start": "4847760",
    "end": "4855140"
  },
  {
    "text": "And this is what we can use\nthe advantage function for. And we can perform\ngradient descent,",
    "start": "4855140",
    "end": "4860869"
  },
  {
    "text": "or gradient ascent in this case. All right, cool. So this was just\nthe prediction part.",
    "start": "4860870",
    "end": "4866762"
  },
  {
    "text": "And we're going\nto end right here. Are there any questions? Yep.",
    "start": "4866762",
    "end": "4871890"
  },
  {
    "text": "You got the alpha pi. Let's just go on for a break. Yeah, this is so\nyou can estimate",
    "start": "4871890",
    "end": "4877929"
  },
  {
    "text": "the advantage of policy, but you\nhave to do this step on policy. ",
    "start": "4877930",
    "end": "4885822"
  },
  {
    "text": "How would you estimate\nthe advantage of policy, because you still need\nto get the d of pi.",
    "start": "4885822",
    "end": "4891020"
  },
  {
    "text": "Yeah, so we'll talk\nabout that on Monday. Yeah, that's a great question.",
    "start": "4891020",
    "end": "4896130"
  },
  {
    "text": "Right, any other questions? None? OK, cool, so, again,\non Monday we'll",
    "start": "4896130",
    "end": "4901730"
  },
  {
    "text": "talk about how we can actually\nimprove the policy using these concepts. And I hope to see you there. Thank you.",
    "start": "4901730",
    "end": "4908110"
  },
  {
    "start": "4908110",
    "end": "4912000"
  }
]