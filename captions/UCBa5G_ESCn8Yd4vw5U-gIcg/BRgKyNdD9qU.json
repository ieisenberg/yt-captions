[
  {
    "start": "0",
    "end": "15000"
  },
  {
    "text": "Okay. [NOISE] Uh, welcome back everyone. This is the second lecture on machine learning.",
    "start": "5090",
    "end": "11940"
  },
  {
    "text": "Um, so just before we get started, a couple of announcements. Um, homework 1 foundations is due tomorrow at 11:00 PM.",
    "start": "11940",
    "end": "20355"
  },
  {
    "start": "15000",
    "end": "87000"
  },
  {
    "text": "Note that it's 11:00 PM, not 11:59. Um, and please I would recommend everyone try to do a test submission early, right.",
    "start": "20355",
    "end": "29370"
  },
  {
    "text": "Um, it would be unfortunate if, uh, you wait until 10:59 and you realize that your computer,",
    "start": "29370",
    "end": "35415"
  },
  {
    "text": "uh, you can't login to the website. Um, if that happens, please don't just bombard me or- or with emails.",
    "start": "35415",
    "end": "44010"
  },
  {
    "text": "Just- just wondered- so there is- you can- you can resubmit as much as you want before the deadline?",
    "start": "44010",
    "end": "50309"
  },
  {
    "text": "So there's no penalty to just submitting something and checking to make sure it works. Yeah. So just to remind you,",
    "start": "50310",
    "end": "56580"
  },
  {
    "text": "you're responsible for any technical issues you encounter, so please do the test submission early. So you have peace of mind,",
    "start": "56580",
    "end": "62320"
  },
  {
    "text": "and then you can go back to finishing your, um, your homework. Okay? Uh, homework 2, sentiment is out.",
    "start": "62320",
    "end": "69410"
  },
  {
    "text": "This is the homework on machine learning, um, and it will be due next Tuesday. Um, and finally, there's a section this Thursday which will talk about, uh,",
    "start": "69410",
    "end": "77845"
  },
  {
    "text": "back propagation and nearest neighbors, and maybe a overview of scikit-learn which might be useful for your projects.",
    "start": "77845",
    "end": "83830"
  },
  {
    "text": "So please, uh, come to that. Okay. So let's jump in. I'm gonna spend a few minutes reviewing what we did last time.",
    "start": "83830",
    "end": "90740"
  },
  {
    "start": "87000",
    "end": "133000"
  },
  {
    "text": "It's kind of starting at the very abstract level and drilling down into the details. So ab- abstract level,",
    "start": "90740",
    "end": "96710"
  },
  {
    "text": "learning is about taking a data-set and outputting a predictor F,",
    "start": "96710",
    "end": "101744"
  },
  {
    "text": "which will be able to take inputs x, for example an image, and output a label or output y,",
    "start": "101745",
    "end": "108230"
  },
  {
    "text": "for example whether it's a cat or a truck or so on. And if you unpack the learner,",
    "start": "108230",
    "end": "114115"
  },
  {
    "text": "we talked about how we want to frame it as a optimization problem which captures what we want to,",
    "start": "114115",
    "end": "121250"
  },
  {
    "text": "uh, optimize, what properties a predictor app should satisfy. And apart from the optimization algorithm,",
    "start": "121250",
    "end": "128280"
  },
  {
    "text": "which is how we accomplish our, um, objective. So the optimization problem that we talked about last time was,",
    "start": "128280",
    "end": "135595"
  },
  {
    "start": "133000",
    "end": "164000"
  },
  {
    "text": "uh, minimizing the training loss. Um, and in symbols, this is the training loss which depends on a particular weight vector.",
    "start": "135595",
    "end": "143989"
  },
  {
    "text": "Is the average over all examples in the training set of the loss of that particular example,",
    "start": "143990",
    "end": "150319"
  },
  {
    "text": "uh, with respect to the wave vector w. Okay. And we want to find the w that minimizes the training loss.",
    "start": "150319",
    "end": "156200"
  },
  {
    "text": "So we want to find the single w that, um, makes sure that on average, all the examples have low loss.",
    "start": "156200",
    "end": "163410"
  },
  {
    "text": "Okay. So looking at the loss functions, um, now, this is where it depends on what we're trying to do.",
    "start": "163410",
    "end": "169400"
  },
  {
    "start": "164000",
    "end": "419000"
  },
  {
    "text": "If we're doing regression, then the pertinent thing to look at is the residual, which remember, is the model's prediction minus the true label.",
    "start": "169400",
    "end": "177650"
  },
  {
    "text": "So this is kind of how much we overshoot. And the loss is going to be zero if the residual is zero,",
    "start": "177650",
    "end": "184040"
  },
  {
    "text": "and in-increases either quadratically for the square loss, or linearly for the absolute, uh,",
    "start": "184040",
    "end": "189060"
  },
  {
    "text": "deviation depending on how much we want to penalize large, uh, deviations. Um, for classification or binary classification more specifically, um,",
    "start": "189060",
    "end": "197940"
  },
  {
    "text": "the pertinent quantity to look at is the margin, which is the score times, uh, the label y, which remember is plus 1 or minus 1.",
    "start": "197940",
    "end": "205790"
  },
  {
    "text": "So the margin, um, is a single number that captures how correct we are. So a large margin is good.",
    "start": "205790",
    "end": "212885"
  },
  {
    "text": "In that case we, uh, obtain either a 0 or a near 0, uh, loss.",
    "start": "212885",
    "end": "218820"
  },
  {
    "text": "And margin less than 0 means that we're making a mistake. So the 0 in loss captures that we're making a mistake of a loss 1.",
    "start": "218820",
    "end": "227005"
  },
  {
    "text": "Um, but, uh, the hinge loss and logistics loss kind of grow linearly, because it allows us to optimize the function better. Question?",
    "start": "227005",
    "end": "234260"
  },
  {
    "text": "So I have a question about residuals. Yeah. Like, I know that I see the regression curve, the loss squared curve there with the",
    "start": "234260",
    "end": "241680"
  },
  {
    "text": "residual- what- what would a residual look like on a graph? Would it be just a point away from the resid- away from the regression curve?",
    "start": "241680",
    "end": "248535"
  },
  {
    "text": "Or what would the residual look like on this graph, if you were to put it? Um, so there are multiple graphs, uh, here.",
    "start": "248535",
    "end": "254780"
  },
  {
    "text": "So remember last time we looked at residual. If you look at, um, x or rather Phi of x over y,",
    "start": "254780",
    "end": "262009"
  },
  {
    "text": "so here's the line. Um, here is a particular point Phi of x,",
    "start": "262010",
    "end": "267615"
  },
  {
    "text": "um, Phi of x, uh, y. And the residual is the, uh,",
    "start": "267615",
    "end": "273160"
  },
  {
    "text": "basically the difference between the model's prediction and, uh, the actual point here.",
    "start": "273160",
    "end": "279685"
  },
  {
    "text": "This graph is different. This graph is, um, visualizing, um, in- is- um,",
    "start": "279685",
    "end": "287295"
  },
  {
    "text": "in a different space. Right. I'll show you another graph that might make some of these things,",
    "start": "287295",
    "end": "292340"
  },
  {
    "text": "uh, a bit clearer in a second. So the residuals won't look exactly like that on this curve of velocity graph? Correct?",
    "start": "292340",
    "end": "298700"
  },
  {
    "text": "Um, well okay, I guess one way to think about the residual is, um, the residual is a number.",
    "start": "298700",
    "end": "304080"
  },
  {
    "text": "So if your residual is 2, then you're kind of here, and this is the loss that you pay, which is, uh, 2 in this case.",
    "start": "304080",
    "end": "310530"
  },
  {
    "text": "Oh. And if the residual is minus 2, then you pay, uh, 2. So the residual is the x-axis? Okay.",
    "start": "310530",
    "end": "317430"
  },
  {
    "text": "Yes. The residual is the x-axis here. Oh, okay. Okay. And the margin is the x-axis over here. All right.",
    "start": "317430",
    "end": "322610"
  },
  {
    "text": "Yeah. Okay. Any other questions about this?",
    "start": "322610",
    "end": "327969"
  },
  {
    "text": "When would you use the absolute value? [BACKGROUND] Um, yeah. The question is,",
    "start": "327970",
    "end": "335000"
  },
  {
    "text": "when would you use absolute value versus the square loss? Um, there is a slide from the, uh, previous lecture which I skipped over which talks about when you would want it.",
    "start": "335000",
    "end": "344065"
  },
  {
    "text": "Um, most of the time people tend to use the square loss because it's easier to optimize,",
    "start": "344065",
    "end": "349939"
  },
  {
    "text": "but, um, you also see absolute, um, you know, deviation. Um, um, the- the square loss will penalize large outliers a lot more.",
    "start": "349940",
    "end": "361040"
  },
  {
    "text": "Which means that it has kinda mean- uh, mean-like, uh, qualities. Whereas the absolute deviation, um, penalizes less,",
    "start": "361040",
    "end": "368610"
  },
  {
    "text": "so it's more like a median, uh, just for kind of intuition. Um, but the general point is that all of",
    "start": "368610",
    "end": "375740"
  },
  {
    "text": "these loss functions capture properties of a desired predictor. They basically say, hand me a predictor, and I'll try to assess for you how good this is, right.",
    "start": "375740",
    "end": "383550"
  },
  {
    "text": "This is kind of establishing what we want out of it. And, um, you know, also another comment is that, you know,",
    "start": "383550",
    "end": "389689"
  },
  {
    "text": "I'm presenting this loss minimization framework because it is so general. Anything basically that you see, um,",
    "start": "389690",
    "end": "396485"
  },
  {
    "text": "in machine learning can be viewed as some sort of, you know, loss minimization. If you think about PCA or deep neural networks, um, different, um,",
    "start": "396485",
    "end": "404225"
  },
  {
    "text": "types of auto-encoders, they can all be viewed as some sort of a loss function, um,",
    "start": "404225",
    "end": "409770"
  },
  {
    "text": "which you're trying to minimize. So, um, tha- that's why, uh, I'm kind of keeping this ge-framework somewhat general.",
    "start": "409770",
    "end": "417880"
  },
  {
    "text": "Okay. So let's, uh, go to the opposite direction of generality. Let's look at a particular example,",
    "start": "418130",
    "end": "425405"
  },
  {
    "start": "419000",
    "end": "874000"
  },
  {
    "text": "and try to put all the pieces together. Um, so suppose we have a simple regression problem.",
    "start": "425405",
    "end": "431570"
  },
  {
    "text": "We have three training examples: 1, 0, the output is 2, 1, 0, the output is 4,",
    "start": "431570",
    "end": "436815"
  },
  {
    "text": "and 0, 1 the output is minus 1. Right. Um, so, um, how do we visualize what learning on this,",
    "start": "436815",
    "end": "445470"
  },
  {
    "text": "uh, training set looks like? Um, so let's try to form the training loss. The training loss, remember,",
    "start": "445470",
    "end": "450950"
  },
  {
    "text": "is the average over the losses on the indivi- individual examples. So let's look at the losses on individual examples.",
    "start": "450950",
    "end": "457750"
  },
  {
    "text": "Um, so we're doing linear regression, so and x is two-dimensional, and Phi of x equals x.",
    "start": "457750",
    "end": "462930"
  },
  {
    "text": "So, uh, in this example, so, um, we're basically trying to fit two numbers, w_1 and w_2.",
    "start": "462930",
    "end": "470635"
  },
  {
    "text": "Um, so if you plug in these values for x and y into this loss function,",
    "start": "470635",
    "end": "477650"
  },
  {
    "text": "then you get the following quantities. So the dot product between w and x is just w_1, right.",
    "start": "477650",
    "end": "483260"
  },
  {
    "text": "Um, because x- x2 is 0. And you minus 2 and you square it because we're looking at the square loss.",
    "start": "483260",
    "end": "489754"
  },
  {
    "text": "Um, the same thing for, uh, this point instead of 2 you have a 4. Um, and then for this point, um, uh,",
    "start": "489755",
    "end": "497910"
  },
  {
    "text": "w.Phi of x minus y is w_2 now because, um, now the- the x2 is, uh,",
    "start": "497910",
    "end": "504630"
  },
  {
    "text": "active, uh, minus minus 1 squared. Okay. So these are the individual loss functions.",
    "start": "504630",
    "end": "511784"
  },
  {
    "text": "Each of which tells what I kind of want out of w. So if here I'm looking at this,",
    "start": "511785",
    "end": "518240"
  },
  {
    "text": "if w_1 is 2, then that's great, I get a loss of 0. This one says if w_1 is 4,",
    "start": "518240",
    "end": "523890"
  },
  {
    "text": "then that's great, and I get a loss of 0. And obviously you can't have both. And the goal of the training loss is trying to look at the average,",
    "start": "523890",
    "end": "531949"
  },
  {
    "text": "so that you can pick one w that works for, as kind of on average, is good for all the points.",
    "start": "531949",
    "end": "538075"
  },
  {
    "text": "Okay? So now, this is a function in two dimensions. It depends on uh, w_1 and w_2.",
    "start": "538075",
    "end": "543935"
  },
  {
    "text": "So let me try to draw this on the board to give you some more intuition what this, uh, looks like. Okay. So I'm gonna draw a w_1, uh, w_2.",
    "start": "543935",
    "end": "552790"
  },
  {
    "text": "And so the first, uh, function is, uh, w_1 minus 2.",
    "start": "552790",
    "end": "559170"
  },
  {
    "text": "Okay. So, um, so what does this function want to do? It wants w_1 to be close to or,",
    "start": "559170",
    "end": "566610"
  },
  {
    "text": "uh, close to 2, and it doesn't care about w_2. Right? So, um, I'm not really sure how to draw this function,",
    "start": "566610",
    "end": "573380"
  },
  {
    "text": "but it it really requires something in 3-D. So you can think about a ball-shape kind of coming out of the board, uh,",
    "start": "573380",
    "end": "578765"
  },
  {
    "text": "like this, if this direction is meant to be the- the loss. Okay. So I'm gonna try to do,",
    "start": "578765",
    "end": "585685"
  },
  {
    "text": "uh, um, well let's- let's try it this way. So it's going to be like I have kind of a bunch of, um,",
    "start": "585685",
    "end": "591110"
  },
  {
    "text": "problems that look like this coming out of the board. Okay? Uh- Um, okay. So what about the second one?",
    "start": "591110",
    "end": "597690"
  },
  {
    "text": "The second one is, uh, w1 minus 4 squared. So that's going to be basically the same thing [NOISE],",
    "start": "597690",
    "end": "604079"
  },
  {
    "text": "but kind of centered, uh, around 4. So around this axis.",
    "start": "604080",
    "end": "609639"
  },
  {
    "text": "Okay. So again, there is gonna be some parabolas coming out of the board. Um, and then finally,",
    "start": "610190",
    "end": "616140"
  },
  {
    "text": "the other point is, uh, w2 minus, minus 1. So it's going to be, um,",
    "start": "616140",
    "end": "621885"
  },
  {
    "text": "happiest when, um, um, w2 is minus 1.",
    "start": "621885",
    "end": "626970"
  },
  {
    "text": "Um, so it's going to be kind of a bunch of, uh, parabolas coming out of the board here, okay?",
    "start": "626970",
    "end": "635339"
  },
  {
    "text": "So you add all three functions up, and what do you get?",
    "start": "635340",
    "end": "640514"
  },
  {
    "text": "You get something that is, um, has- first of all, where do you think the minimum should be?",
    "start": "640515",
    "end": "648279"
  },
  {
    "text": "One of the two intersections of the [NOISE] on the- One of the two intersections.",
    "start": "649310",
    "end": "655425"
  },
  {
    "text": "Yeah. Like the first, like the first, uh, vertical and horizontal or the second square vertical and horizontal.",
    "start": "655425",
    "end": "663330"
  },
  {
    "text": "Oh, the red lines, I mean. Oh, yeah. There's gonna be some sort of intersection here. So if you look at, um, the w2 axis,",
    "start": "663330",
    "end": "671069"
  },
  {
    "text": "right, um, it should definitely be minus 1, because this is the fun- only function that cares about w1.",
    "start": "671070",
    "end": "676214"
  },
  {
    "text": "So it's gonna be somewhere here and both by symmetry, while this one wants it to be a 2, this one it wants it to be a 4,",
    "start": "676215",
    "end": "682950"
  },
  {
    "text": "so the average is somewhere between. You can work all of this kinda actually mathematically out, I'm just kinda giving the rough intuition.",
    "start": "682950",
    "end": "689310"
  },
  {
    "text": "Uh, and now let me draw the level curves here. The level curves are going to be something like this where, um, ag- again,",
    "start": "689310",
    "end": "699375"
  },
  {
    "text": "if you draw it in 3D, it's like a parabola, uh, or- coming out of the- a board here,",
    "start": "699375",
    "end": "705435"
  },
  {
    "text": "um, where here's the lowest point. Um, and as you venture away from this point,",
    "start": "705435",
    "end": "713175"
  },
  {
    "text": "your loss is going to increase. [NOISE] Right? Okay. Yeah.",
    "start": "713175",
    "end": "719970"
  },
  {
    "text": "Can you explain that bit again, that middle point? Uh, how do I get this middle point?",
    "start": "719970",
    "end": "726255"
  },
  {
    "text": "Um, [NOISE] so one way is that if you add these two functions up and it kind of,",
    "start": "726255",
    "end": "731760"
  },
  {
    "text": "um, just, you know, plot it, uh, it turns out to be a 3. Intuitively, um, the, the square loss when you average,",
    "start": "731760",
    "end": "741150"
  },
  {
    "text": "uh, it acts kind of like, uh, um, a mean. So kind of, you know,",
    "start": "741150",
    "end": "747330"
  },
  {
    "text": "it's gonna be somewhere in between. It's also, um, related to one of the homework problems. So hopefully, you'll h- have a better appreciation for that.",
    "start": "747330",
    "end": "755370"
  },
  {
    "text": "Um, okay. So, so I guess- yeah, question. Once we have the 3, how do we merge it with the negative 1 as well?",
    "start": "755370",
    "end": "763890"
  },
  {
    "text": "Do we need to do another addition? Um, so the question is, once we have the 3, how do you merge it with a minus 1?",
    "start": "763890",
    "end": "770250"
  },
  {
    "text": "Um, so the 3 is regarding w1 and the minus 1 is regarding w2.",
    "start": "770250",
    "end": "776430"
  },
  {
    "text": "So you just add them together. They kind of don't- in this particular example, they don't interact. In general, they will.",
    "start": "776430",
    "end": "782760"
  },
  {
    "text": "I still like this example, could you quickly summarize exactly what's going on with this example.",
    "start": "782760",
    "end": "788025"
  },
  {
    "text": "Yeah. So this plot shows for every possible wave vector w1, w2,",
    "start": "788025",
    "end": "793695"
  },
  {
    "text": "you have a point and the amount that the function comes out of the board is the loss, right?",
    "start": "793695",
    "end": "800025"
  },
  {
    "text": "And the loss function is defined on- in the slides, right there. And all I'm doing is trying to plot this loss function.",
    "start": "800025",
    "end": "807915"
  },
  {
    "text": "Okay. So it's actually w1 and w2 points, the loss is coming out of the board you're plotting. Yes? No?",
    "start": "807915",
    "end": "814110"
  },
  {
    "text": "Um, so unfortunately, it's hard to kind of draw it in 3D here. So-",
    "start": "814110",
    "end": "819180"
  },
  {
    "text": "Okay. What I'm trying to do here is taking each of the pieces and trying to explain what each piece is trying to do.",
    "start": "819180",
    "end": "826635"
  },
  {
    "text": "All right. Yeah. Okay. So, um, in general,",
    "start": "826635",
    "end": "834900"
  },
  {
    "text": "the, the training loss, you don't have to think about kind of how exactly it composes the individual losses.",
    "start": "834900",
    "end": "842700"
  },
  {
    "text": "Um, this is probably as complex of an example we'll have to, you know, we'll get to right trying to understand it.",
    "start": "842700",
    "end": "848834"
  },
  {
    "text": "Um, but this kind of gives you an idea of how you connect these pictures where you see kind of these are parabolas,",
    "start": "848835",
    "end": "856185"
  },
  {
    "text": "um, with, uh, the picture which is actually the- of the, you know, training loss.",
    "start": "856185",
    "end": "862710"
  },
  {
    "text": "Okay. But for now, let's assume you have the training loss. It's a function of the parameter- it's some function.",
    "start": "862710",
    "end": "868650"
  },
  {
    "text": "And how do you optimize this function? So you do some sort of gradient descent.",
    "start": "868650",
    "end": "874785"
  },
  {
    "start": "874000",
    "end": "1081000"
  },
  {
    "text": "So last time we talked about how you can just do a v- vanilla gradient ascend where you'd initialize with 0.",
    "start": "874785",
    "end": "881010"
  },
  {
    "text": "And then you compute the gradient of that entire training loss. And then you update once.",
    "start": "881010",
    "end": "886709"
  },
  {
    "text": "And the problem with that is the, uh, up to computing the gradient requires going through all the training examples,",
    "start": "886710",
    "end": "892800"
  },
  {
    "text": "and if you have a million training examples that's really slow. So instead we looked at stochastic gradient descent which allows you to pick up",
    "start": "892800",
    "end": "899519"
  },
  {
    "text": "an individual example and then make a gradient step right away, right? And, um, empirically we SHA encode how it can be a lot faster, you know.",
    "start": "899520",
    "end": "909230"
  },
  {
    "text": "Of course there are cases where, um, it can also be less stable. So there's kinda in general going to be some, you know, trade off here.",
    "start": "909230",
    "end": "916130"
  },
  {
    "text": "But by and large, stochastic gradient descent it kind of really dominates, um, you know, machine learning applications today because",
    "start": "916130",
    "end": "923080"
  },
  {
    "text": "you- there's only way to really kind of scale to large, um, you know, datasets. Okay. Yeah.",
    "start": "923080",
    "end": "931680"
  },
  {
    "text": "Is there any other benefit of stochastic gradient descent or gradient descent? Um, so apart from being able to scale up,",
    "start": "931680",
    "end": "938670"
  },
  {
    "text": "is there any advantage of stochastic gradient descent? Um, another besides computation,",
    "start": "938670",
    "end": "945510"
  },
  {
    "text": "another advantage might be that, um, your data might be coming in an online fashion, like over time,",
    "start": "945510",
    "end": "951480"
  },
  {
    "text": "and you want to, you know, update kind of on the fly. Um, so there are cases where you don't actually have all the data at once.",
    "start": "951480",
    "end": "959980"
  },
  {
    "text": "Okay. So that was a quick overview of the general concepts. Um, now to set the stage for what we're gonna do in this lecture,",
    "start": "961760",
    "end": "970110"
  },
  {
    "text": "I wanna ask you guys the following question. So can we obtain decision boundaries, um,",
    "start": "970110",
    "end": "976230"
  },
  {
    "text": "remember a decision boundary is the- the- it's kind of a line that or the curve that separates the region of the space which",
    "start": "976230",
    "end": "983760"
  },
  {
    "text": "is classified positively versus negatively. Um, can we obtain decision boundaries which are circles,",
    "start": "983760",
    "end": "990540"
  },
  {
    "text": "um, by using linear classifiers? Okay. So, um, does that make sense?",
    "start": "990540",
    "end": "997350"
  },
  {
    "text": "So we want to get something like this, um, where you have- now we're going into,",
    "start": "997350",
    "end": "1005480"
  },
  {
    "text": "uh, Phi1 of x, um, you know, Phi2 of x. And we want decision boundaries that look like this",
    "start": "1005480",
    "end": "1012860"
  },
  {
    "text": "where you classify maybe these as positive and these as negative. Okay. Is that possible? Yeah.",
    "start": "1012860",
    "end": "1019250"
  },
  {
    "text": "If you map, if you like take a square of those inputs. Then you get something to be linear. You [inaudible]. [OVERLAPPING]",
    "start": "1019250",
    "end": "1024410"
  },
  {
    "text": "Yeah, yeah. Okay. [LAUGHTER] So you're saying, yes? Yeah. Okay.",
    "start": "1024410",
    "end": "1029539"
  },
  {
    "text": "[inaudible]. Okay. Uh, okay. Well, there's a punchline there. Um, so it turns out, um,",
    "start": "1029540",
    "end": "1036470"
  },
  {
    "text": "that you can actually do this which maybe on the surface seems kinda surprising, right?",
    "start": "1036470",
    "end": "1041900"
  },
  {
    "text": "Because we're talking about linear classifiers. But as we'll see it really depends on what you mean by",
    "start": "1041900",
    "end": "1047209"
  },
  {
    "text": "linear classifiers and hopefully that will become clear soon. Okay. So we're gonna start by talking about",
    "start": "1047210",
    "end": "1053000"
  },
  {
    "text": "features which is going to be able to answer this question. Then we're gonna shift gears a little bit and talk about neural networks which is, uh,",
    "start": "1053000",
    "end": "1061070"
  },
  {
    "text": "in some sense an automatic way to learn features. And we're gonna, ah, show you how to train neural networks using",
    "start": "1061070",
    "end": "1067160"
  },
  {
    "text": "back propagation hopefully without tears. And, um, then talk about nearest neighbors which is",
    "start": "1067160",
    "end": "1073340"
  },
  {
    "text": "another way to get really expressive models which is gonna be, um, much simpler in a way.",
    "start": "1073340",
    "end": "1081080"
  },
  {
    "start": "1081000",
    "end": "1379000"
  },
  {
    "text": "Okay. So recall that we have the score. So the score is a dot product between the wave vector and the feature vector,",
    "start": "1081080",
    "end": "1088940"
  },
  {
    "text": "and the score drives prediction. So if you're doing regression, you just output the score as the number. If you're doing classification,",
    "start": "1088940",
    "end": "1094940"
  },
  {
    "text": "binary classification then you output the sign of the score. Um, and so far we've focused on learning which is how you",
    "start": "1094940",
    "end": "1103090"
  },
  {
    "text": "choose the wave vector based on a bunch of data and how you optimize for that.",
    "start": "1103090",
    "end": "1108400"
  },
  {
    "text": "And so now what we're gonna do is focus on phi of x, um, and talk about how you choose these features in the first place.",
    "start": "1108400",
    "end": "1117485"
  },
  {
    "text": "And this actually, feature extraction is such a really critical important part of kinda",
    "start": "1117485",
    "end": "1125195"
  },
  {
    "text": "a machine learning pipeline which often gets neglected because when you take a class you're saying, Okay, well there's some feature vector and then let's focus on all of these algorithms.",
    "start": "1125195",
    "end": "1133520"
  },
  {
    "text": "But whenever you go and apply machine learning in the world, um, feature extraction, um, turns out to be kinda the main bottleneck.",
    "start": "1133520",
    "end": "1142280"
  },
  {
    "text": "And neural nets can mitigate this to some extent but it doesn't completely make feature extraction um, obsolete.",
    "start": "1142280",
    "end": "1150725"
  },
  {
    "text": "So recall that a feature extractor takes an input, um,",
    "start": "1150725",
    "end": "1156995"
  },
  {
    "text": "such as this uh, string and outputs a set of properties which are- are useful for prediction.",
    "start": "1156995",
    "end": "1164075"
  },
  {
    "text": "So in this case, it's a set of um, named feature values okay.",
    "start": "1164075",
    "end": "1170975"
  },
  {
    "text": "And last time, we didn't really say much about this. We just kinda waved our hands and say, okay here's some features.",
    "start": "1170975",
    "end": "1176615"
  },
  {
    "text": "So you in general how do you approach this problem, what features do you include? Um, do you just like start making them up and how many features you have?",
    "start": "1176615",
    "end": "1185855"
  },
  {
    "text": "We need maybe a better organizational principle here. Um, and you know in general a feature engineer is gonna be someone of art.",
    "start": "1185855",
    "end": "1193850"
  },
  {
    "text": "So I'm not gonna give you a recipe, but at least some framework for thinking about features.",
    "start": "1193850",
    "end": "1199055"
  },
  {
    "text": "So the first notion um, is a feature template, and a feature template is informally",
    "start": "1199055",
    "end": "1205580"
  },
  {
    "text": "just a group of features that are all computed in the same way. Um, this is kind of a somewhat pedantic but kinda,",
    "start": "1205580",
    "end": "1211955"
  },
  {
    "text": "um, a terminology point that I want you all to kinda be aware of. Um, so a feature template is basically a feature um, name with holes.",
    "start": "1211955",
    "end": "1222425"
  },
  {
    "text": "So for example length greater than blank. So remember the concrete feature has length greater than 10.",
    "start": "1222425",
    "end": "1228560"
  },
  {
    "text": "Now, we're gonna say length greater than blank, where blank can be replaced with 10, 9, 8 or any kind of number. And it's a template that gives rise to multiple features.",
    "start": "1228560",
    "end": "1236195"
  },
  {
    "text": "Last week, characters equals blank, contains character blank. These are all examples of feature templates.",
    "start": "1236195",
    "end": "1243695"
  },
  {
    "text": "So when you go in your project or whatever and you describe your features or when you think",
    "start": "1243695",
    "end": "1249350"
  },
  {
    "text": "about kind of grouping these features in terms of, you know, um, these blanks. Another example is pixel intensity of position.",
    "start": "1249350",
    "end": "1256730"
  },
  {
    "text": "So even if you have what you consider to be like a raw input, like an image, right?",
    "start": "1256730",
    "end": "1262715"
  },
  {
    "text": "There's still implicitly some sort of way to think about it as a feature template, um,",
    "start": "1262715",
    "end": "1269135"
  },
  {
    "text": "which corresponds to the pixel intensity of position, blank comma blank, ah, is a feature template where it gives a rise",
    "start": "1269135",
    "end": "1277429"
  },
  {
    "text": "to the number of features equals to the number of pixels in the image. And this is useful because maybe your input isn't just an image.",
    "start": "1277430",
    "end": "1284585"
  },
  {
    "text": "Maybe it's an image plus some metadata. Then having this kind of language for describing",
    "start": "1284585",
    "end": "1289835"
  },
  {
    "text": "all the features in a unified way is really important for clarity.",
    "start": "1289835",
    "end": "1295085"
  },
  {
    "text": "Okay. So as I alluded to, each feature template maps to a set of features.",
    "start": "1295085",
    "end": "1302600"
  },
  {
    "text": "So by writing last three characters equals blank, I'm implicitly saying, well,",
    "start": "1302600",
    "end": "1308095"
  },
  {
    "text": "I'm going to define a feature for each value of blank and that feature is gonna be associated with",
    "start": "1308095",
    "end": "1315130"
  },
  {
    "text": "a value which is just the natural evaluation of that feature on the input. Okay. So all of these are 0 except for ends with .com is 1.",
    "start": "1315130",
    "end": "1325279"
  },
  {
    "text": "Okay. So, um, and in general you are going to have",
    "start": "1325830",
    "end": "1331419"
  },
  {
    "text": "each feature template that might give rise to many, many features, right?",
    "start": "1331420",
    "end": "1338330"
  },
  {
    "text": "Um, the number of possible three-letter characters, you know some number of characters to a cube which is a large number.",
    "start": "1338330",
    "end": "1345890"
  },
  {
    "text": "Ah, so one question is how do we represent this, right?",
    "start": "1345890",
    "end": "1351460"
  },
  {
    "text": "First vector. Yes, first vector. [LAUGHTER].",
    "start": "1351460",
    "end": "1356680"
  },
  {
    "text": "Yeah good answer. So mathematically, it's really useful. Just think about this vector as a d-dimensional vector, right.",
    "start": "1356680",
    "end": "1365090"
  },
  {
    "text": "Just d numbers just laid out, right? And because that's mathematically convenient but when you",
    "start": "1365090",
    "end": "1372919"
  },
  {
    "text": "go to actually implement this stuff you might not represent things that way. In particular, you know, what are the ways you can represent a vector?",
    "start": "1372920",
    "end": "1381125"
  },
  {
    "start": "1379000",
    "end": "1500000"
  },
  {
    "text": "Well, you can say, I'm going to represent it as an array which is just this list of numbers that you have.",
    "start": "1381125",
    "end": "1386870"
  },
  {
    "text": "But this is inefficient if you have a huge number of features. But in the cases where you have sparse features which means",
    "start": "1386870",
    "end": "1396830"
  },
  {
    "text": "that only a very few of the feature values are non-zero,",
    "start": "1396830",
    "end": "1401899"
  },
  {
    "text": "then you're better off representing as a map or in Python, a dictionary, which you specify the feature name um,",
    "start": "1401900",
    "end": "1410360"
  },
  {
    "text": "is a key and the value is, you know, the value of that feature, right? And all the,um, the- the home",
    "start": "1410360",
    "end": "1418775"
  },
  {
    "text": "homework two will basically work in this sparse feature framework. Um, and you know, just kind of a note,",
    "start": "1418775",
    "end": "1426800"
  },
  {
    "text": "a lot of, um, especially in NLP and we have discrete objects and traditionally,",
    "start": "1426800",
    "end": "1433429"
  },
  {
    "text": "it's been common to use kind of these sparse feature maps. Ah, you know one thing that has happened with",
    "start": "1433430",
    "end": "1440840"
  },
  {
    "text": "the rise of neural networks is that often um, you take basically your inputs and embed them into some sort of",
    "start": "1440840",
    "end": "1448400"
  },
  {
    "text": "fixed dimensional vector space and dense feature representations have been more um, dominant.",
    "start": "1448400",
    "end": "1453815"
  },
  {
    "text": "But sparse features if you wanna use linear classifiers is still kinda a good way to go.",
    "start": "1453815",
    "end": "1459919"
  },
  {
    "text": "So it's important to understand this. Okay. So now instead of storing possibly a lot of features now you just",
    "start": "1459920",
    "end": "1468200"
  },
  {
    "text": "store the key and the value.",
    "start": "1468200",
    "end": "1472289"
  },
  {
    "text": "All right. Um, So this was the feature templates. The overall point is that it's kind of organizational principle,",
    "start": "1473770",
    "end": "1480859"
  },
  {
    "text": "um, and you know, um, okay so now let's switch gears a little bit.",
    "start": "1480859",
    "end": "1487085"
  },
  {
    "text": "So which features or feature templates should you actually write down?",
    "start": "1487085",
    "end": "1495635"
  },
  {
    "text": "And to get at that, I wanna introduce another notion which is pretty important especially if you think about the theory of machine learning,",
    "start": "1495635",
    "end": "1504755"
  },
  {
    "start": "1500000",
    "end": "1681000"
  },
  {
    "text": "and that's the notion of a hypothesis class. Okay. So remember we have this predictor.",
    "start": "1504755",
    "end": "1509990"
  },
  {
    "text": "So for a particular wave vector, that defines a function that maps inputs into some sort of score or prediction.",
    "start": "1509990",
    "end": "1520205"
  },
  {
    "text": "And the hypothesis class is just the set of all predictors that you can get if you vary the wave vector.",
    "start": "1520205",
    "end": "1528890"
  },
  {
    "text": "Okay. So- so let me give you um, we're gonna come back to this slide. Let me give you a kinda example here.",
    "start": "1528890",
    "end": "1535325"
  },
  {
    "text": "So suppose you are doing a regression and you're doing linear regression in particular.",
    "start": "1535325",
    "end": "1541500"
  },
  {
    "text": "So you're in one dimension. Here is x and, um,",
    "start": "1542860",
    "end": "1548179"
  },
  {
    "text": "here is, ah, I guess y. Um,",
    "start": "1548180",
    "end": "1553230"
  },
  {
    "text": "so if your feature map is just identity, so maps x to x, um,",
    "start": "1553230",
    "end": "1559715"
  },
  {
    "text": "then this notation just means the set of all, ah,",
    "start": "1559715",
    "end": "1564960"
  },
  {
    "text": "linear functions like this. Then the set of functions you get you can visualize um, as this, right?",
    "start": "1564960",
    "end": "1574210"
  },
  {
    "text": "So you have one function here and for every possible value of w1,",
    "start": "1574210",
    "end": "1581270"
  },
  {
    "text": "you have, ah, a slope. You also have 0. They should all go through the origin.",
    "start": "1581270",
    "end": "1588455"
  },
  {
    "text": "Um, so you have- these are your functions, right? So your hypothesis class F1 here is essentially all lines that go through the origin.",
    "start": "1588455",
    "end": "1600290"
  },
  {
    "text": "Okay. So just wanna think about it when you write down a feature vector you're implicitly committing yourself to saying hey,",
    "start": "1600290",
    "end": "1607370"
  },
  {
    "text": "I want to think about all possible predictors defined by this feature map.",
    "start": "1607370",
    "end": "1616415"
  },
  {
    "text": "Okay. So here's another example. Suppose I define the feature map to be x comma x squared.",
    "start": "1616415",
    "end": "1623375"
  },
  {
    "text": "Okay. So now what are the possible functions, you know, I'm gonna get?",
    "start": "1623375",
    "end": "1630660"
  },
  {
    "text": "So does anyone wanna say what, read off this slide what it is [LAUGHTER].",
    "start": "1631180",
    "end": "1639890"
  },
  {
    "text": "It's gonna be all quadratic functions, right? Okay. So in particular, because they don't have a bias term, it's gonna be all quadratic functions that go through the origin.",
    "start": "1639890",
    "end": "1647870"
  },
  {
    "text": "So let me actually draw another. [NOISE]. Um.",
    "start": "1647870",
    "end": "1660790"
  },
  {
    "text": "So it's gonna be all quadratic functions that go through the origin, which look like this, but it could be upside down.",
    "start": "1660790",
    "end": "1667230"
  },
  {
    "text": "Um, and do it like that, I'm not gonna draw all of them.",
    "start": "1667230",
    "end": "1672255"
  },
  {
    "text": "Um, in particular, it also includes the linear functions, right?",
    "start": "1672255",
    "end": "1677760"
  },
  {
    "text": "Because I can always set w_2 equals 0, and vary w_1 which means that I also get all the linear functions too, right?",
    "start": "1677760",
    "end": "1686895"
  },
  {
    "start": "1681000",
    "end": "1758000"
  },
  {
    "text": "So this means that w- f_2, if you think about the set of functions is a larger set than f_1, it's more expressive.",
    "start": "1686895",
    "end": "1694320"
  },
  {
    "text": "That's what we mean by expressive. That means that it can represent more things. Okay. So for every feature vector,",
    "start": "1694320",
    "end": "1702735"
  },
  {
    "text": "you should think also about the set of functions that you can get by, uh, that new feature vector.",
    "start": "1702735",
    "end": "1709419"
  },
  {
    "text": "Okay. So let's- is there a question? Yeah. We need to assess the time here- the best set of w's,",
    "start": "1711560",
    "end": "1721544"
  },
  {
    "text": "are- are the more expressive sets harder to optimize over? The question is, are the more expressive sets harder to optimize over?",
    "start": "1721545",
    "end": "1730230"
  },
  {
    "text": "In terms of, ah, you know, the short answer is not necessarily.",
    "start": "1730230",
    "end": "1735825"
  },
  {
    "text": "Um, um, in terms of- sure, you have more features so that it req- is more expensive.",
    "start": "1735825",
    "end": "1741660"
  },
  {
    "text": "Yeah. At that level, um, but the difficulty optimization depends on a number of different, you know, factors.",
    "start": "1741660",
    "end": "1748725"
  },
  {
    "text": "Um, and sometimes, adding more features can be easier to optimize because it's easier to figure out training data, um, okay.",
    "start": "1748725",
    "end": "1756780"
  },
  {
    "text": "So now, let's go back to this picture. Okay. So, uh, this is- on the board is concrete examples of feature or,",
    "start": "1756780",
    "end": "1763620"
  },
  {
    "start": "1758000",
    "end": "2113000"
  },
  {
    "text": "or ah, uh, hypothesis classes. Um, now, let's think about this big blob as the set of all predictors.",
    "start": "1763620",
    "end": "1771915"
  },
  {
    "text": "Any predictor in your wildest dreams, you know, they're in this, this set. Okay? And whenever you go and you define a feature map,",
    "start": "1771915",
    "end": "1781904"
  },
  {
    "text": "that's going to carve out, uh, you know, much smaller set of, um, you know, uh, functions, right?",
    "start": "1781905",
    "end": "1789315"
  },
  {
    "text": "And- and then what is learning doing, learning is choosing a particular element of that,",
    "start": "1789315",
    "end": "1796184"
  },
  {
    "text": "um, function family based on the data. Okay. So this picture shows you",
    "start": "1796185",
    "end": "1801240"
  },
  {
    "text": "kind of the full pipeline of how you're doing machine learning. Is, you know, there- you first declare structurally a set of, ah,",
    "start": "1801240",
    "end": "1810170"
  },
  {
    "text": "of functions that you're interested in, and then you say [NOISE] okay, now, based on data, let me go and search through",
    "start": "1810170",
    "end": "1817020"
  },
  {
    "text": "that set and find the one that is, you know, best for me. Okay. So now, there are,",
    "start": "1817020",
    "end": "1824925"
  },
  {
    "text": "you know, two places where things can go wrong. Well, for feature extraction, maybe you, um, didn't have enough features.",
    "start": "1824925",
    "end": "1831960"
  },
  {
    "text": "So now, yours- your- your, uh, purple set is too small. Then, no matter how much learning you do,",
    "start": "1831960",
    "end": "1838410"
  },
  {
    "text": "you're just not going to get good accuracy. Right? And then conversely,",
    "start": "1838410",
    "end": "1843480"
  },
  {
    "text": "even if you define a nice, um, you know, uh, hypothesis class, if you don't optimize properly,",
    "start": "1843480",
    "end": "1850200"
  },
  {
    "text": "you're not gonna find the element of that null hypothesis class that fulfills your,",
    "start": "1850200",
    "end": "1855600"
  },
  {
    "text": "um, your goals. Question?",
    "start": "1855600",
    "end": "1862600"
  },
  {
    "text": "The function F- the feature function is extracted to get from the input,",
    "start": "1863630",
    "end": "1869190"
  },
  {
    "text": "since that, you know, self as a function, how come you can assume that your weights,",
    "start": "1869190",
    "end": "1874515"
  },
  {
    "text": "will be able to compute, um, that function also?",
    "start": "1874515",
    "end": "1881800"
  },
  {
    "text": "So the question is- so you're defining a function Phi,",
    "start": "1883040",
    "end": "1888960"
  },
  {
    "text": "right? This is fixed. Um, and then learning sets weights,",
    "start": "1888960",
    "end": "1895334"
  },
  {
    "text": "and together jointly, they specify a particular function or predictor.",
    "start": "1895334",
    "end": "1900630"
  },
  {
    "text": "There's something that saying that if you don't choose Phi appropriately, you're limiting the space they will be able to predict.",
    "start": "1900630",
    "end": "1908040"
  },
  {
    "text": "Yeah. But so I'm wondering like why my under- my intuition tells me that the whole point of learning is that, uh,",
    "start": "1908040",
    "end": "1915420"
  },
  {
    "text": "regardless of the Phi that you choose, the actual model that you choose should be able to,",
    "start": "1915420",
    "end": "1922110"
  },
  {
    "text": "you know, learn the function Phi that you would have picked. Ah, I see. So the question is, ah,",
    "start": "1922110",
    "end": "1929325"
  },
  {
    "text": "does- doesn't learning kind of compensate and just figure out the Phi that you would have picked.",
    "start": "1929325",
    "end": "1936845"
  },
  {
    "text": "Um, so the answer is- short answer is no. The- the Phi is really kind of a bottleneck here. For example, it just- if you define Phi to be,",
    "start": "1936845",
    "end": "1944935"
  },
  {
    "text": "um, X, so that's the linear function. Linear function is all you're going to get. Right? So if your data moves around, um,",
    "start": "1944935",
    "end": "1951270"
  },
  {
    "text": "in a sinusoidal way, you're just gonna, like, fit a line through that and you'll get, you know, horrible accuracy.",
    "start": "1951270",
    "end": "1956850"
  },
  {
    "text": "And no amount of learning, um, can, you know, fix that. The only way to fix that is by, um,",
    "start": "1956850",
    "end": "1962355"
  },
  {
    "text": "changing your, you know, feature representation. So does that assume that W is- is a linear model though?",
    "start": "1962355",
    "end": "1971160"
  },
  {
    "text": "So yes. So all of this assumes that W- we're talking about linear predictors here.",
    "start": "1971160",
    "end": "1977100"
  },
  {
    "text": "Okay. But of course, the same general idea applies to any sort of function family in neural nets.",
    "start": "1977100",
    "end": "1983640"
  },
  {
    "text": "Um, the arc- so the equivalent there would be not just, ah, the feature map, but also the neural network architecture.",
    "start": "1983640",
    "end": "1990285"
  },
  {
    "text": "It's a constraint on what kind of things you can express. So if you have in your- only a two-layer neural network,",
    "start": "1990285",
    "end": "1996450"
  },
  {
    "text": "then there's just some things that you just, you know, ah, with ah, with a fixed size, there's just some things you just can't express. Yeah. Another question.",
    "start": "1996450",
    "end": "2005285"
  },
  {
    "text": "Just to follow onto that as well as an alternative interpretation of the question, I felt it was more of a question of why for",
    "start": "2005285",
    "end": "2011120"
  },
  {
    "text": "a visualization rather than kicking in the raw data, and have like a neural net it still functions linear classifiers,",
    "start": "2011120",
    "end": "2016895"
  },
  {
    "text": "but it has enough complexity that it can strive for non-linear behavior. Yeah. So the question is why bother doing feature in engineering?",
    "start": "2016895",
    "end": "2024020"
  },
  {
    "text": "Hasn't neural nets kind of basically, solved that? Um, so to some extent, the- the amount of feature engineering you have to do today is,",
    "start": "2024020",
    "end": "2031565"
  },
  {
    "text": "you know, much less. One thing that I think it's still important to think about in feature engineering is it's",
    "start": "2031565",
    "end": "2037610"
  },
  {
    "text": "really- think about it as what sources of information you want to, you know, predict. For example, if you want to predict, um, you know,",
    "start": "2037610",
    "end": "2044030"
  },
  {
    "text": "this, uh, you know, some property about a movie review. And you know what- what if- part of",
    "start": "2044030",
    "end": "2050899"
  },
  {
    "text": "the first-order bits are like what even goes into that. Does it- the text go into that? Do you have metadata? Do you have other star ratings?",
    "start": "2050900",
    "end": "2057470"
  },
  {
    "text": "And those are [NOISE] you know, features you can- there's- I guess no such thing as like raw, um,",
    "start": "2057470",
    "end": "2063530"
  },
  {
    "text": "because there's always some code that takes, you know, the- the, you know,",
    "start": "2063530",
    "end": "2069440"
  },
  {
    "text": "the world and distills it down into something that fits in memory. So that's you can think about it as feature extraction.",
    "start": "2069440",
    "end": "2075274"
  },
  {
    "text": "Thank you. Yeah. Okay. One last question and then I'll go. What is the problem with, uh,",
    "start": "2075275",
    "end": "2080659"
  },
  {
    "text": "too many features, don't you want your hypothesis class to be too big, is it like an overfitting thing? Yeah. Yeah. Um, so the question is why don't you just make Phi as large as possible,",
    "start": "2080660",
    "end": "2090470"
  },
  {
    "text": "throw on all the features, and overfitting is, um, you know, one of the main concerns there which,",
    "start": "2090470",
    "end": "2095929"
  },
  {
    "text": "you know, we'll come back to in the next lecture. Okay, great questions. Um, so let's,",
    "start": "2095930",
    "end": "2103880"
  },
  {
    "text": "um, let's actually skip over this, um. So there's another type of, uh,",
    "start": "2103880",
    "end": "2111069"
  },
  {
    "text": "feature function you can define, but in interest of time, I'm going to skip over that. Um, okay, so now let's come back to this question,",
    "start": "2111070",
    "end": "2119540"
  },
  {
    "start": "2113000",
    "end": "2316000"
  },
  {
    "text": "this linear- I keep on saying near linear predictors. So what, what, what is linear, right?",
    "start": "2119540",
    "end": "2124850"
  },
  {
    "text": "Uh, so remember the prediction is driven by the score. Right. So here's a question.",
    "start": "2124850",
    "end": "2130190"
  },
  {
    "text": "Is this score linear in w? Yes, right?",
    "start": "2130190",
    "end": "2135920"
  },
  {
    "text": "Because, um, what is a, you know, linear function is basically some kind of weighted, er, combination of your inputs.",
    "start": "2135920",
    "end": "2143600"
  },
  {
    "text": "Okay, so is it linear in Phi of x? By symmetry, it should be because it's just a dot-product.",
    "start": "2143600",
    "end": "2151865"
  },
  {
    "text": "So is it linear in x? No, in fact this,",
    "start": "2151865",
    "end": "2157565"
  },
  {
    "text": "this question doesn't even make sense because think about x. X remember was a string. Right, it's not a- it's not even a number.",
    "start": "2157565",
    "end": "2164329"
  },
  {
    "text": "So, um, and that's when you know the answer should be no because you know it doesn't, there's a type error.",
    "start": "2164330",
    "end": "2170539"
  },
  {
    "text": "Um, okay so here's, here's kind of the cool thing now is, um,",
    "start": "2170540",
    "end": "2176119"
  },
  {
    "text": "you know these predictors can be expressive nonlinear function and decision boundaries of x,",
    "start": "2176120",
    "end": "2182885"
  },
  {
    "text": "you know, in the case where x is, uh, uh, is actually a real vector. Um, but the score is a linear function of w, okay?",
    "start": "2182885",
    "end": "2193790"
  },
  {
    "text": "So this is cool because, you know, from a pr- there's two perspectives, right?",
    "start": "2193790",
    "end": "2199175"
  },
  {
    "text": "From the point of actually doing prediction, you know, you're thinking about like wh- how does this function operate on x?",
    "start": "2199175",
    "end": "2205295"
  },
  {
    "text": "And you can get all sorts of you know, crazy functions coming out. Um, we just looked at quadratic functions was",
    "start": "2205295",
    "end": "2210770"
  },
  {
    "text": "clearly non-linear but you can do all sorts of, you know, crazy things. But from the point of view of learning,",
    "start": "2210770",
    "end": "2217520"
  },
  {
    "text": "it doesn't care about x. All it sees is Phi of x. In particular, your learning asked the question how does this function depend on w?",
    "start": "2217520",
    "end": "2226820"
  },
  {
    "text": "Right? Because it's tuning w. And from that perspective, it's a linear function w and,",
    "start": "2226820",
    "end": "2232535"
  },
  {
    "text": "um, for reasons I'm not gonna, you know, go into, um, these functions, er,",
    "start": "2232535",
    "end": "2238265"
  },
  {
    "text": "permit efficient learning because the loss function becomes convex, um, which I'll, that's all I say about that.",
    "start": "2238265",
    "end": "2244930"
  },
  {
    "text": "Okay. So, um, so one kind of cool way to visualize what's going on",
    "start": "2244930",
    "end": "2251630"
  },
  {
    "text": "here is when you're going back to our circle as example. So remember we want this, um, two-dimensional classification problem where the true decision boundary is,",
    "start": "2251630",
    "end": "2258935"
  },
  {
    "text": "you know, let's say a circle. So how do we fit that and what does it mean for a linear thing because when you think linear it like, should be a line, right?",
    "start": "2258935",
    "end": "2266480"
  },
  {
    "text": "Um, so here's a kind of a cool graphic. So, okay. So here is, um,",
    "start": "2266480",
    "end": "2274085"
  },
  {
    "text": "these points inside the circle and, you know, it can't be classified. But the point is when you look at",
    "start": "2274085",
    "end": "2281150"
  },
  {
    "text": "the feature map it actually lifts these points into a higher dimensional space. Now I will have three features, right?",
    "start": "2281150",
    "end": "2287540"
  },
  {
    "text": "And- and you know, in this higher-dimensional space, I can actually- things are linear.",
    "start": "2287540",
    "end": "2292805"
  },
  {
    "text": "I can slice it with a kind of a knife. And then, you know,",
    "start": "2292805",
    "end": "2299075"
  },
  {
    "text": "in that high dimensional space if things are cut and what that induces in the lower-dimensional space is, you know, this circle.",
    "start": "2299075",
    "end": "2307020"
  },
  {
    "text": "Okay. Okay, I don't wanna- Okay,",
    "start": "2308890",
    "end": "2314930"
  },
  {
    "text": "so hopefully that was, er, a nice visualization that shows how you can actually get",
    "start": "2314930",
    "end": "2322535"
  },
  {
    "start": "2316000",
    "end": "2959000"
  },
  {
    "text": "nonlinear machine functions out of kind of essentially linear machinery, right?",
    "start": "2322535",
    "end": "2329825"
  },
  {
    "text": "So someone- the next time someone says, well, you know, um, you know, linear classifiers are really limited, um,",
    "start": "2329825",
    "end": "2337115"
  },
  {
    "text": "and you really need neural nets, um, you know, that's technically false because you can actually get really expressive models out of,",
    "start": "2337115",
    "end": "2345035"
  },
  {
    "text": "er, you know, neural networks- sorry, out of linear models. The point with neural networks is not that they're not- you're more necessarily more",
    "start": "2345035",
    "end": "2354020"
  },
  {
    "text": "ex- expre- They can be more expressive but the fact that they have other advantages, for example, the inductive bias that comes with the architectures and, um,",
    "start": "2354020",
    "end": "2361910"
  },
  {
    "text": "the fact that they are more efficient, ah, when you go to more expressive models and so on.",
    "start": "2361910",
    "end": "2367680"
  },
  {
    "text": "Okay, so- so to kind of wrap up all things,",
    "start": "2369430",
    "end": "2374930"
  },
  {
    "text": "I want to kind of do a you know, simple exercise. So here's a task. So imagine you're doing a final project and you want to,",
    "start": "2374930",
    "end": "2382010"
  },
  {
    "text": "um, predict, you know, whether two consecutive messages in some forum or a chat are,",
    "start": "2382010",
    "end": "2389015"
  },
  {
    "text": "um, where the second one is a response to the first. So it's binary classification, input is two messages,",
    "start": "2389015",
    "end": "2394355"
  },
  {
    "text": "and you're asked to predict whether a second is a response to the first. Okay, so we're gonna go through this exercise of coming up with, um, you know,",
    "start": "2394355",
    "end": "2402980"
  },
  {
    "text": "features that might be or feature templates might be useful to pick out properties of x that might be useful.",
    "start": "2402980",
    "end": "2410540"
  },
  {
    "text": "Um, and we're gonna assume that we're dealing with linear predictors. Okay. So what are some features that might be useful?",
    "start": "2410540",
    "end": "2418250"
  },
  {
    "text": "Let's, um, you know, let's- here's- let's start with a few. Okay. So how about time elapsed, um,",
    "start": "2418250",
    "end": "2425585"
  },
  {
    "text": "between the two messages, is that a useful feature or not? How many of you say yes?",
    "start": "2425585",
    "end": "2431270"
  },
  {
    "text": "Okay, so this information is definitely good. Um, one subtle point is that this time elapsed is a single number,",
    "start": "2431270",
    "end": "2442520"
  },
  {
    "text": "and this number is going to go into the score kind of in a linear fashion, okay?",
    "start": "2442520",
    "end": "2449765"
  },
  {
    "text": "So what does- what does that mean? That means, um, you know, if I double the time,",
    "start": "2449765",
    "end": "2456275"
  },
  {
    "text": "then the score is going to or that can p- the contribution to the score is going to like multiply by 2, right?",
    "start": "2456275",
    "end": "2464690"
  },
  {
    "text": "So think about it, it's, it's kinda like saying them, as I increase the time, you know, the,",
    "start": "2464690",
    "end": "2470585"
  },
  {
    "text": "it becomes linearly more likely that I'm going to be let's say not a response or- or a response.",
    "start": "2470585",
    "end": "2477424"
  },
  {
    "text": "So this is, you know, maybe it's kind of not what you want because, you know,",
    "start": "2477425",
    "end": "2482810"
  },
  {
    "text": "the difference and from that perspective like if you, the time elapsed is like a year then that really kind of dominates the- the score function.",
    "start": "2482810",
    "end": "2491990"
  },
  {
    "text": "Um, and it's like way more likely that it's going to be a response than if it were like one minute,",
    "start": "2491990",
    "end": "2497315"
  },
  {
    "text": "which is kind of not what you want. Yeah, question? Can you normalize it to teach them?",
    "start": "2497315",
    "end": "2503420"
  },
  {
    "text": "Yeah, so the question is, can you normalize it? Um, so you have to be careful with normalization.",
    "start": "2503420",
    "end": "2509870"
  },
  {
    "text": "So you have- if you normalize let's say the span of like over one year. Now, now, there's no difference between like, you know,",
    "start": "2509870",
    "end": "2517190"
  },
  {
    "text": "five seconds and one minute because everything gets squashed down to 0, right? So, er, one way to kind of approach that is to,",
    "start": "2517190",
    "end": "2525905"
  },
  {
    "text": "um, discretize the features. So one trick that people often do is if you",
    "start": "2525905",
    "end": "2531619"
  },
  {
    "text": "have a numerical value which you really kind of want to, um, treat kind of in a sensitive way,",
    "start": "2531620",
    "end": "2538430"
  },
  {
    "text": "you can kind of break up into pieces. So the feature template would look something like time elapsed is between blah and blah.",
    "start": "2538430",
    "end": "2545690"
  },
  {
    "text": "So you can do things like okay is it between zero seconds and five seconds and is it between five seconds and like a minute and between",
    "start": "2545690",
    "end": "2552740"
  },
  {
    "text": "a minute and an hour and an hour and a year or something? And then after that, it doesn't matter. Um, because that will give you kind of more, um,",
    "start": "2552740",
    "end": "2560885"
  },
  {
    "text": "it's more domain knowledge that tells you kind of what things to look out for.",
    "start": "2560885",
    "end": "2565970"
  },
  {
    "text": "That difference between let's say a year and a year plus two seconds is really, you know, it doesn't matter, right?",
    "start": "2565970",
    "end": "2572570"
  },
  {
    "text": "Whereas the difference between one second and five seconds might be significant. So this is all a long way of saying you know",
    "start": "2572570",
    "end": "2580145"
  },
  {
    "text": "if you're using linear classifiers or even if you're using neural networks, I think it's really important to think about how",
    "start": "2580145",
    "end": "2586310"
  },
  {
    "text": "your raw features are kind of entering the system and think about like, if I change this feature by like scaling it up,",
    "start": "2586310",
    "end": "2594109"
  },
  {
    "text": "does the prediction change in a way that, you know, I expect? Yeah, you got a question?",
    "start": "2594110",
    "end": "2600170"
  },
  {
    "text": "So if we approve that second feature right there, er, what prevents us from having let's say, er,",
    "start": "2600170",
    "end": "2607400"
  },
  {
    "text": "if we had a whole sort of 35 seconds from 30 to 40 seconds and maybe so on what prevents us from getting just the entire time?",
    "start": "2607400",
    "end": "2619490"
  },
  {
    "text": "[NOISE] Yeah, so, er, the question like if you have every possible range isn't that like an infinite number of features?",
    "start": "2619490",
    "end": "2627320"
  },
  {
    "text": "Um, so, er, there's two answers to that. One is that even if you did that,",
    "start": "2627320",
    "end": "2634145"
  },
  {
    "text": "you might still be okay because there's probably some, um, if you think about it like discretizing the space of, you know,",
    "start": "2634145",
    "end": "2643070"
  },
  {
    "text": "here is your time elapsed, time, um, elapsed.",
    "start": "2643070",
    "end": "2649505"
  },
  {
    "text": "And you're basically saying for every bucket I'm going to have a feature. Um, it is true that you have an infinite number of, you know,",
    "start": "2649505",
    "end": "2657200"
  },
  {
    "text": "features but, you know, at some point you might just cut it off. And if you didn't cut it off and use a sparse feature representation, um,",
    "start": "2657200",
    "end": "2663050"
  },
  {
    "text": "you don't have to, um, pra- have a pre-set, you know, maximum because remember,",
    "start": "2663050",
    "end": "2668660"
  },
  {
    "text": "most of these features are gonna be zero because the chances of some data point being like,",
    "start": "2668660",
    "end": "2674255"
  },
  {
    "text": "you know, 10 years is going to be essentially you know, nil. Um, another answer is that, um,",
    "start": "2674255",
    "end": "2682835"
  },
  {
    "text": "in general when you have features that, er, have multiple timescales, um,",
    "start": "2682835",
    "end": "2687980"
  },
  {
    "text": "you want a kind of space that will work kind of logarithmically. Um, so you know, one to two, two to four, four to eight, um,",
    "start": "2687980",
    "end": "2694190"
  },
  {
    "text": "so that you can have both kinds of sensitivity in lower events but also, um, kind of cover a large, um, magnitude.",
    "start": "2694190",
    "end": "2702960"
  },
  {
    "text": "Yeah, in the back. Is it possible to learn, like how to discretize the features make it the most important?",
    "start": "2704070",
    "end": "2712735"
  },
  {
    "text": "Um, question is, is it possible [NOISE] to learn how to discretize the, the features?",
    "start": "2712735",
    "end": "2718030"
  },
  {
    "text": "Um, there are- there's definitely more automatic things you can do besides,",
    "start": "2718030",
    "end": "2724210"
  },
  {
    "text": "you know, just like spans specifying them. Uh, at some level though, you have to kind of input the value in a",
    "start": "2724210",
    "end": "2732175"
  },
  {
    "text": "form, like, er, if you've inputted into x versus, let's say log of x, um,",
    "start": "2732175",
    "end": "2737785"
  },
  {
    "text": "those choices often can make a, you know, big difference. Um, but, um, if you use more expressive models like neural networks you can,",
    "start": "2737785",
    "end": "2745795"
  },
  {
    "text": "you know, mitigate some of this. Yeah. I see the value in changing time elapsed, uh,",
    "start": "2745795",
    "end": "2751225"
  },
  {
    "text": "from a number to like a Boolean whether it falls between a range. Why would you wanna retain a,",
    "start": "2751225",
    "end": "2756595"
  },
  {
    "text": "a numerical value for teaching? When would you not wanna discretize it? Yeah, good question. So when would you actually want to not discretize it?",
    "start": "2756595",
    "end": "2763390"
  },
  {
    "text": "Um, [NOISE] so there are- um, essentially when you expect kind of the- the scale of that feature to,",
    "start": "2763390",
    "end": "2774010"
  },
  {
    "text": "um, [NOISE] really kind of matter in, in some, in some sense.",
    "start": "2774010",
    "end": "2779620"
  },
  {
    "text": "So, so, so certainly when you think that some things behave linearly, um, then you just wanna preserve the linear.",
    "start": "2779620",
    "end": "2786609"
  },
  {
    "text": "Or if you think that it behaves quadratically, then you wanna keep the feature but also add a squared term to it.",
    "start": "2786610",
    "end": "2794065"
  },
  {
    "text": "Okay. I wanna maybe move on, um. Uh, these are all good questions, happy to discuss more offline.",
    "start": "2794065",
    "end": "2799780"
  },
  {
    "text": "Um, so some other features might include, the first message contains blank where blank is a string.",
    "start": "2799780",
    "end": "2806365"
  },
  {
    "text": "Right. So maybe things like, you know, question marks are more indicative of you know, things being",
    "start": "2806365",
    "end": "2812440"
  },
  {
    "text": "the second message being a response. Second message contains certain words. Um, um, two messages both contain a particular word.",
    "start": "2812440",
    "end": "2820960"
  },
  {
    "text": "Um, you know, there's cases where, um, it doesn't really m- it's not the presence and absence of",
    "start": "2820960",
    "end": "2827770"
  },
  {
    "text": "particular words in the- in individual, um, messages. But like the fact that they both share a common word,",
    "start": "2827770",
    "end": "2834790"
  },
  {
    "text": "you know, that might be useful. Um, here's another feature which is, you know, two meshes have the, um, some number of common words together.",
    "start": "2834790",
    "end": "2843040"
  },
  {
    "text": "Um, so this feature is kind of interesting because it's, um, there's, you know, the,",
    "start": "2843040",
    "end": "2849730"
  },
  {
    "text": "the- for example you look at this feature, it's how the number of- when I say feature,",
    "start": "2849730",
    "end": "2855070"
  },
  {
    "text": "I actually mean feature template. Um, so for this feature template, um, there are many, many features,",
    "start": "2855070",
    "end": "2862135"
  },
  {
    "text": "one for possibly any number of words. And this again leads to cases where you might have a lot of,",
    "start": "2862135",
    "end": "2868360"
  },
  {
    "text": "um, you know, sparsity and you might not have enough data to fit all the features. Whereas, this one is very compact.",
    "start": "2868360",
    "end": "2875485"
  },
  {
    "text": "That says, I just have to look at the, um, number of overlap.",
    "start": "2875485",
    "end": "2880570"
  },
  {
    "text": "So, er, the, the two messages might contain a word that I've never seen before,",
    "start": "2880570",
    "end": "2885730"
  },
  {
    "text": "but I know it's the same word and I can kind of recognize that pattern. Um, so, you know, there's quite a bit of things you can do",
    "start": "2885730",
    "end": "2892840"
  },
  {
    "text": "to play around with features that capture, um, you know, the intuitions about what might be relevant to your task. Question. Yeah.",
    "start": "2892840",
    "end": "2900940"
  },
  {
    "text": "We have a lot of these sparse features like the working different point here. Is that when we want to do like dimensionality reduction,",
    "start": "2900940",
    "end": "2907600"
  },
  {
    "text": "like knockout some of those many, many features? Um, so the question is when you have a lot of sparse features,",
    "start": "2907600",
    "end": "2914349"
  },
  {
    "text": "do you wanna do dimensionality reduction? Um, not necessarily. Um, so in terms of computation,",
    "start": "2914350",
    "end": "2922315"
  },
  {
    "text": "having sparse features it doesn't necessarily mean that it's gonna be, you know, really slow, um,",
    "start": "2922315",
    "end": "2927370"
  },
  {
    "text": "because there's efficient ways of, um, you know, representing sparse features, um,",
    "start": "2927370",
    "end": "2932920"
  },
  {
    "text": "in terms of, you know, expressivity, one thing that, um,",
    "start": "2932920",
    "end": "2937960"
  },
  {
    "text": "in a lot of NLP applications, you actually do want a lot of features. Um, and you can have a lot more features than you might think you can handle.",
    "start": "2937960",
    "end": "2948760"
  },
  {
    "text": "Um, and because you really wanted, the first orbit is just to, you know, be expressive enough to even fit the data.",
    "start": "2948760",
    "end": "2956365"
  },
  {
    "text": "Yeah. Okay. Let me move on, um, since, you know, I'm running short on time. Okay. So summary so far,",
    "start": "2956365",
    "end": "2963534"
  },
  {
    "start": "2959000",
    "end": "3001000"
  },
  {
    "text": "you know, uh, we're looking at features. We can define these feature templates which organize these,",
    "start": "2963534",
    "end": "2969760"
  },
  {
    "text": "uh, features, uh, in a kind of meaningful way. And then we talked about hypothesis classes which are,",
    "start": "2969760",
    "end": "2977080"
  },
  {
    "text": "are defined by features. And this defines what is possible f- out of, uh, from learning.",
    "start": "2977080",
    "end": "2984430"
  },
  {
    "text": "Um, and all of this in the context of linear classifiers which incidentally can actually produce",
    "start": "2984430",
    "end": "2991180"
  },
  {
    "text": "these nice non-linear decision, you know, boundaries. [NOISE] So at this point you can actually have kind of enough tools to,",
    "start": "2991180",
    "end": "2997319"
  },
  {
    "text": "um, you know, do a lot. Um, but in the next section,",
    "start": "2997320",
    "end": "3002600"
  },
  {
    "start": "3001000",
    "end": "3046000"
  },
  {
    "text": "I wanna talk about neural networks because, um, these are even more expressive models which can be,",
    "start": "3002600",
    "end": "3010214"
  },
  {
    "text": "you know, more powerful. Um, um, one thing I, I often recommend is that,",
    "start": "3010215",
    "end": "3016455"
  },
  {
    "text": "um, you know, when you're given a problem, you know, always try the simplest thing. I will always try kind of a linear classifier and just",
    "start": "3016455",
    "end": "3024089"
  },
  {
    "text": "see where it gets because sometimes you'd be surprised at how, uh, far you can get with linear classifiers.",
    "start": "3024090",
    "end": "3030420"
  },
  {
    "text": "And then, and then go and kind of increase the complexity as you need it. I know there's sometimes this temptation to, you know,",
    "start": "3030420",
    "end": "3037110"
  },
  {
    "text": "fire the fancy new shot, um, you know, uh, hammer, but, um, sometimes keeping it simple is,",
    "start": "3037110",
    "end": "3043545"
  },
  {
    "text": "you know, really, really good. Okay. So neural nets, um.",
    "start": "3043545",
    "end": "3049875"
  },
  {
    "start": "3046000",
    "end": "3168000"
  },
  {
    "text": "There's a couple of ways of motivating this, um, one motivation is, um,",
    "start": "3049875",
    "end": "3056640"
  },
  {
    "text": "you know, comes from the brain. Um, I'm going to use a kind of slightly different, um,",
    "start": "3056640",
    "end": "3063330"
  },
  {
    "text": "motivation which comes from, um, kind of this idea of decomposing a problem,",
    "start": "3063330",
    "end": "3070770"
  },
  {
    "text": "you know, into parts, right. So this is a somewhat contrived example, but hopefully,",
    "start": "3070770",
    "end": "3077010"
  },
  {
    "text": "it'll allow us to build up the intuitions for, you know, what's going on in a neural network. Um, okay.",
    "start": "3077010",
    "end": "3083130"
  },
  {
    "text": "So suppose I am building some sort of, uh, system to detect whether two cars are gonna collide.",
    "start": "3083130",
    "end": "3089355"
  },
  {
    "text": "Okay. So the way it works is I have this car at position x_1 and it's,",
    "start": "3089355",
    "end": "3095025"
  },
  {
    "text": "you know, driving, uh, this way. And then I have another car at position x_2 and it's driving this way.",
    "start": "3095025",
    "end": "3102315"
  },
  {
    "text": "And I want to determine whether it's safe, um, which is positive or it's- if it's gonna collide.",
    "start": "3102315",
    "end": "3109095"
  },
  {
    "text": "Okay. And let's suppose for, uh, simplicity that the true function is as follows.",
    "start": "3109095",
    "end": "3116235"
  },
  {
    "text": "Okay. So it's just measuring whether the distance is at least 1 apart. Now th- this is kind of a little bit,",
    "start": "3116235",
    "end": "3122880"
  },
  {
    "text": "uh, you know, s- like what we did in, uh, the last lecture where we",
    "start": "3122880",
    "end": "3129165"
  },
  {
    "text": "suppose there was a true function and then see if learning can recover that, um, where in practice,",
    "start": "3129165",
    "end": "3135089"
  },
  {
    "text": "obviously we don't know the true function, but this is for- kind of pedagogical purposes. Okay. So just to kind of making sure we understand what function we're talking about.",
    "start": "3135090",
    "end": "3143130"
  },
  {
    "text": "So if, um, x_1 is 1 and x_2 is 3, um, kind of like that on the board,",
    "start": "3143130",
    "end": "3150644"
  },
  {
    "text": "then here, plus 1. So this is like driving in the US. This is like driving in the, er,",
    "start": "3150645",
    "end": "3156704"
  },
  {
    "text": "UK. Um, and that's fine too. Um, but if you're, um, uh, you know,",
    "start": "3156705",
    "end": "3163305"
  },
  {
    "text": "too close together then that's bad news. Okay? All right. So let's think about decomposing the problem, right.",
    "start": "3163305",
    "end": "3171839"
  },
  {
    "start": "3168000",
    "end": "3351000"
  },
  {
    "text": "Because if you look at this, you know, this, this could be a kind of a complicated, um,",
    "start": "3171840",
    "end": "3176880"
  },
  {
    "text": "you know, function, but let's try to break it down into kind of linear functions, right.",
    "start": "3176880",
    "end": "3182069"
  },
  {
    "text": "Because at the end of the day, neural networks are just a bunch of linear functions with, um, which are stitched together with some nonlinearities.",
    "start": "3182070",
    "end": "3189539"
  },
  {
    "text": "So like there are a kind of linear components that are, um, critical to neural nets.",
    "start": "3189540",
    "end": "3194940"
  },
  {
    "text": "Okay. So one subproblem is detecting if car 1 is to the far right of car 2.",
    "start": "3194940",
    "end": "3200444"
  },
  {
    "text": "Okay. So x_1 less x_2 is greater than or equal to 1. Um, another problem is testing whether car 2 is far right of car 1.",
    "start": "3200445",
    "end": "3208215"
  },
  {
    "text": "And then, um, and then you can put these together by saying, um, if at least one of them is, you know, 1,",
    "start": "3208215",
    "end": "3215625"
  },
  {
    "text": "then I'm going to predict safe, um, otherwise I will predict, uh, not safe.",
    "start": "3215625",
    "end": "3220635"
  },
  {
    "text": "Okay. So here's the kind of concrete examples. So for 1, 3, uh, car 2 is far right of car 1. So that's a 1.",
    "start": "3220635",
    "end": "3227430"
  },
  {
    "text": "You add these up, take the sign, that's plus 1, in the opposite direction it's still fine. And in this, this case both h_1 and h_2 are 0,",
    "start": "3227430",
    "end": "3235335"
  },
  {
    "text": "so that's, uh, bad news. Okay. So this is just kind of trying to take",
    "start": "3235335",
    "end": "3242010"
  },
  {
    "text": "this expression which is a true function and kind of write it in, uh, kind of a more modular way,",
    "start": "3242010",
    "end": "3248714"
  },
  {
    "text": "where you have different pieces corresponding to different competitions. Okay. So now, um, we,",
    "start": "3248715",
    "end": "3256540"
  },
  {
    "text": "we could just write this down, obviously to solve this problem but th- we already knew what the right answer was. But suppose we didn't know what the true function is and we just had data.",
    "start": "3256540",
    "end": "3264779"
  },
  {
    "text": "So, so we don't actually know what these functions are. So can we kind of learn,",
    "start": "3264780",
    "end": "3270000"
  },
  {
    "text": "learn these functions automatically? So what I'm gonna do is I'm, I'm gonna define a feature vector now,",
    "start": "3270000",
    "end": "3277665"
  },
  {
    "text": "um, of x which is gonna be a 1, x_1, x_2. Okay. Um, and then I'm going to rewrite this intermediate subproblem as follows.",
    "start": "3277665",
    "end": "3287070"
  },
  {
    "text": "So x_1 is x_2 greater than 1, is going to be represented as this, uh, vector v_1.v of x,",
    "start": "3287070",
    "end": "3295245"
  },
  {
    "text": "where v_1 is minus 1 plus 1 minus 1. So you can- you pause for a second.",
    "start": "3295245",
    "end": "3300570"
  },
  {
    "text": "Um, you can verify that this is x_1 minus x_2, you know greater than equal to 1.",
    "start": "3300570",
    "end": "3306570"
  },
  {
    "text": "Okay. So this is just another way of writing, um, you know, what we wanted in terms of this like dot product and you can see kind of",
    "start": "3306570",
    "end": "3314010"
  },
  {
    "text": "how this is maybe moving more towards something that looks more general. Yeah. Why is that 1 there?",
    "start": "3314010",
    "end": "3319935"
  },
  {
    "text": "So the question, why is there this 1 here? Um, so this 1 typically is known as a bias term which allows you to,",
    "start": "3319935",
    "end": "3327755"
  },
  {
    "text": "um, not just, uh, you know, threshold on 0, but threshold on, uh, any arbitrary number.",
    "start": "3327755",
    "end": "3333920"
  },
  {
    "text": "So in the linear classifiers that I've, you know, talked about, I've kinda swept this under the rug.",
    "start": "3333920",
    "end": "3339734"
  },
  {
    "text": "Generally, you always have a bias term that allows you to kind of modulate how likely you're gonna pre- predict 1 versus, uh, minus 1.",
    "start": "3339735",
    "end": "3348750"
  },
  {
    "text": "Okay. So you can also do it for h_2. It's the same thing, but just, um,",
    "start": "3348750",
    "end": "3353925"
  },
  {
    "start": "3351000",
    "end": "3431000"
  },
  {
    "text": "you know, switching the roles of x_1 and x_2. Um, and now also the first sign of final sign prediction,",
    "start": "3353925",
    "end": "3360135"
  },
  {
    "text": "you can write it as follows. Um, now th- these are just weights on, um, h_1 and h_2.",
    "start": "3360135",
    "end": "3367515"
  },
  {
    "text": "Okay? So now, here is the, the kind of the punchline, is,",
    "start": "3367515",
    "end": "3372974"
  },
  {
    "text": "you know, for a neural network, we're just going to leave v_1, v_2,",
    "start": "3372975",
    "end": "3379440"
  },
  {
    "text": "and w as unknown, uh, quantities that we're going to try to,",
    "start": "3379440",
    "end": "3384720"
  },
  {
    "text": "uh, fit through training. Right. We motivated this problem by saying, okay,",
    "start": "3384720",
    "end": "3390105"
  },
  {
    "text": "in this case, there is some choice of v_1, v_2, w that works. But now we're kind of generalizing.",
    "start": "3390105",
    "end": "3397005"
  },
  {
    "text": "If we didn't know these quantities, we can just leave them as variables and we can actually still fit them- fit these parameters.",
    "start": "3397005",
    "end": "3404740"
  },
  {
    "text": "Okay. So, um, before we were just tuning w,",
    "start": "3406970",
    "end": "3413055"
  },
  {
    "text": "and now we're tuning both V and w. V specifies the choice of",
    "start": "3413055",
    "end": "3418589"
  },
  {
    "text": "the hidden problems that we're interested in and w governs how do we take the results of the hidden problems and,",
    "start": "3418590",
    "end": "3425430"
  },
  {
    "text": "uh, come to a final prediction. Okay. So there's one problem here,",
    "start": "3425430",
    "end": "3434295"
  },
  {
    "start": "3431000",
    "end": "3574000"
  },
  {
    "text": "which is that if you look at the gradient of h1 with respect to v1,",
    "start": "3434295",
    "end": "3439829"
  },
  {
    "text": "um, it happens to be 0, okay? So if you look at, um, the, uh,",
    "start": "3439830",
    "end": "3446355"
  },
  {
    "text": "horizontal axis is v1 dot Phi of x and the vertical axis is h1,",
    "start": "3446355",
    "end": "3453060"
  },
  {
    "text": "um, that function, um, is- looks like the step function, right?",
    "start": "3453060",
    "end": "3460950"
  },
  {
    "text": "Because indicator function of some quantity greater than or equal to 0. It's 1 over here, 0 over here.",
    "start": "3460950",
    "end": "3466815"
  },
  {
    "text": "Um, and remember, we don't like 0 gradients because SGD doesn't work.",
    "start": "3466815",
    "end": "3472500"
  },
  {
    "text": "So the solution, um, here is to, um,",
    "start": "3472500",
    "end": "3479115"
  },
  {
    "text": "take some sandpaper, um, and you, you know, sand out this function to smooth it out and,",
    "start": "3479115",
    "end": "3485984"
  },
  {
    "text": "uh, then you get something that is, um, you know, differentiable. So, uh, the logistic function is this function which is,",
    "start": "3485985",
    "end": "3494235"
  },
  {
    "text": "um, a smoothed out version of this, which, um, rises. So it doesn't hit 1 or 0 ever,",
    "start": "3494235",
    "end": "3502109"
  },
  {
    "text": "but it becomes extremely close. But it kind of, um, goes up in the, in the middle.",
    "start": "3502110",
    "end": "3507900"
  },
  {
    "text": "And you could think about this as, um, a differentiable,",
    "start": "3507900",
    "end": "3513480"
  },
  {
    "text": "um, or I, I guess a smooth version of, uh, the step function, okay?",
    "start": "3513480",
    "end": "3520695"
  },
  {
    "text": "So it kinda behaves and looks like the step function. It serves kind of the same intuition that you're trying to test whether some quantity is greater than 0,",
    "start": "3520695",
    "end": "3527895"
  },
  {
    "text": "but it doesn't have 0 gradients anywhere, okay?",
    "start": "3527895",
    "end": "3533100"
  },
  {
    "text": "And you can double-check. If you take the derivative, then this is actually- has this kind of really interesting nice form,",
    "start": "3533100",
    "end": "3538710"
  },
  {
    "text": "which is the value of the function times 1 minus the value of the function. And the value of the function never hits 0,",
    "start": "3538710",
    "end": "3545279"
  },
  {
    "text": "so this quantity never hits 0, okay?",
    "start": "3545280",
    "end": "3551550"
  },
  {
    "text": "So, so now we can define, uh, neural nets in contrast to linear functions.",
    "start": "3551550",
    "end": "3556694"
  },
  {
    "text": "So remember, linear functions, um, we can visualize it as, um, inputs go in, um,",
    "start": "3556695",
    "end": "3563550"
  },
  {
    "text": "and each of the inputs gets, um, weighted by some, uh,",
    "start": "3563550",
    "end": "3568920"
  },
  {
    "text": "w and you get the score, okay? [NOISE] So this is what a linear- what a linear function looks like.",
    "start": "3568920",
    "end": "3574410"
  },
  {
    "start": "3574000",
    "end": "4943000"
  },
  {
    "text": "Now, neural networks with one hidden layer and two hidden units, 1, 2, looks something like this where you have, um,",
    "start": "3574410",
    "end": "3583830"
  },
  {
    "text": "these intermediate hidden units, which are the sigmoid function, um, applied or logi- logistic function in this case in, uh,",
    "start": "3583830",
    "end": "3591840"
  },
  {
    "text": "to be concrete, um, applied to, um, this wave vector Vj times Phi of x.",
    "start": "3591840",
    "end": "3600600"
  },
  {
    "text": "So h1 is, uh, going to be taking the input and multiplying it by a vector of- and you get some number here,",
    "start": "3600600",
    "end": "3608370"
  },
  {
    "text": "and then you send it through this, um, logistic function to get some number.",
    "start": "3608370",
    "end": "3614220"
  },
  {
    "text": "And then finally you take the output of h1 and h2 and you, uh,",
    "start": "3614220",
    "end": "3619545"
  },
  {
    "text": "take the dot product with respect to w, and then you get the final score, okay?",
    "start": "3619545",
    "end": "3626370"
  },
  {
    "text": "So again, the intuition is that neural nets are trying to break down the problem into a set of,",
    "start": "3626370",
    "end": "3632954"
  },
  {
    "text": "you know, subproblems where you- the subproblems are the kind of the, the result of these intermediate computations.",
    "start": "3632955",
    "end": "3639360"
  },
  {
    "text": "[NOISE] And you can think about these as like, you know, h1 is really kind of the output of a mini linear classifier.",
    "start": "3639360",
    "end": "3646500"
  },
  {
    "text": "h2 is the output of a mini linear classifier. And then you're taking those outputs and then you're, you know, sticking them through another linear classifier and getting the score.",
    "start": "3646500",
    "end": "3653700"
  },
  {
    "text": "So this is what I mean by, you know, at the end of the day, it's kind of linear classifiers packaged up and strung together.",
    "start": "3653700",
    "end": "3659805"
  },
  {
    "text": "And their expressive power comes from, from the kind of the composition. Um, yeah, question.",
    "start": "3659805",
    "end": "3667780"
  },
  {
    "text": "Phi h sub j when there's like multiple Phi, like, how do you combine them?",
    "start": "3668300",
    "end": "3674240"
  },
  {
    "text": "Uh, the question, how do you get h sub j when there's multiple Phis?",
    "start": "3674240",
    "end": "3680265"
  },
  {
    "text": "There's only one Phi of x. Oh, so this is, this is the first component of Phi of x.",
    "start": "3680265",
    "end": "3685890"
  },
  {
    "text": "So this vector, this, this is a three-dimensional vector, which is Phi of x. And it has three components.",
    "start": "3685890",
    "end": "3691515"
  },
  {
    "text": "Yeah. Yeah. [inaudible] uh, isn't that effectively features, kind of?",
    "start": "3691515",
    "end": "3699075"
  },
  {
    "text": "Yeah. Then they're like, they're like the- like, I mean, some kind of function of the original features that you've put in and they make the new features that are better than the ones before?",
    "start": "3699075",
    "end": "3705585"
  },
  {
    "text": "Yeah. [NOISE] Yeah. So that's my- kind of my next point, which is that, um, one way you can think about it is that the hjs are actually just, you know,",
    "start": "3705585",
    "end": "3715500"
  },
  {
    "text": "features which are learned automatically from data as opposed to having, a fixed, uh, set of your features Phi, right?",
    "start": "3715500",
    "end": "3723420"
  },
  {
    "text": "Because at this layer, w always sees these, you know, hs which are coming through which look like,",
    "start": "3723420",
    "end": "3729825"
  },
  {
    "text": "you know, uh, features. Um, and for deeper neural networks,",
    "start": "3729825",
    "end": "3734940"
  },
  {
    "text": "you kind of just keep on stacking this. So, you know, this output of one set of classifiers becomes the features to",
    "start": "3734940",
    "end": "3741330"
  },
  {
    "text": "the next layer and then the output of that class sort of becomes the features to the next layer, and so on.",
    "start": "3741330",
    "end": "3747270"
  },
  {
    "text": "Um, and the intuition for, you know, deeper networks, um, is that, you know,",
    "start": "3747270",
    "end": "3752819"
  },
  {
    "text": "as you proceed you can, uh, derive more abstract, you know, features. For example, images.",
    "start": "3752820",
    "end": "3758220"
  },
  {
    "text": "You start with pixels and then you find kind of the edges, and then you define kind of object parts,",
    "start": "3758220",
    "end": "3763500"
  },
  {
    "text": "and then now you define kind of, uh, things which are closer to the actual classification problem. Yeah. [NOISE]",
    "start": "3763500",
    "end": "3770820"
  },
  {
    "text": "What if you wanted h2 to develop the exact same value, like, do you have to have a bias to start with?",
    "start": "3770820",
    "end": "3775859"
  },
  {
    "text": "Ah, yeah. That's a good question. So why don't h1 and h2 do, uh, basically end up in the same place because,",
    "start": "3775860",
    "end": "3782445"
  },
  {
    "text": "you know, because of symmetry? Um, if you're not careful that will happen. So if you initialize all your weights to 0 and, uh,",
    "start": "3782445",
    "end": "3790890"
  },
  {
    "text": "or initialize these weights the same way then, um, they will be kinda moving in locks- lockstep.",
    "start": "3790890",
    "end": "3797100"
  },
  {
    "text": "Um, so what is typically done is you randomly initialize. So they're, kinda, you break the symmetry.",
    "start": "3797100",
    "end": "3802425"
  },
  {
    "text": "And then what the network is going to do is it's trying to, um, use- learn auto- it kind of automatically learns these subproblems to,",
    "start": "3802425",
    "end": "3810660"
  },
  {
    "text": "uh, be kind of complementary because you're doing this joint learning. [NOISE] Yeah. Final question then.",
    "start": "3810660",
    "end": "3816120"
  },
  {
    "text": "How do you choose the Sigma function? [NOISE] Uh, how do I choose the Sigma function? Um, so this is- so in general,",
    "start": "3816120",
    "end": "3823395"
  },
  {
    "text": "sigmoid functions are these or activation functions are these nonlinear functions. So the important thing, uh, it's, it's a nonlinear function.",
    "start": "3823395",
    "end": "3829815"
  },
  {
    "text": "Um, I chose this particular logistic function because it's kind of the classic, um,",
    "start": "3829815",
    "end": "3835065"
  },
  {
    "text": "neural net and it looks like the step function, which is kind of, uh, takes the score and outputs,",
    "start": "3835065",
    "end": "3840390"
  },
  {
    "text": "uh, a classification result. I should, you know, responsibly note that, um,",
    "start": "3840390",
    "end": "3846075"
  },
  {
    "text": "these are, um, maybe, uh, less in style than they used to be. And the, the cool thing to do now is to use,",
    "start": "3846075",
    "end": "3854205"
  },
  {
    "text": "uh, what is called a ReLU or a rectified linear, which looks like this. Um, and you might ask, like, why this one?",
    "start": "3854205",
    "end": "3861525"
  },
  {
    "text": "Um, well, there's no one reason, but, um,",
    "start": "3861525",
    "end": "3866609"
  },
  {
    "text": "this, um, this function has less of a kind of this, um, gradient going to zero problem.",
    "start": "3866610",
    "end": "3873390"
  },
  {
    "text": "It's also simpler because it doesn't require exponentials. Um, but there's, um, um, I'm gonna just leave it at that.",
    "start": "3873390",
    "end": "3879660"
  },
  {
    "text": "[NOISE] [BACKGROUND] What- the benefit of this function is, uh,",
    "start": "3879660",
    "end": "3886260"
  },
  {
    "text": "pedagogical reasons and it's a little bit of a throwback too. [NOISE] Um, okay. [NOISE]",
    "start": "3886260",
    "end": "3894255"
  },
  {
    "text": "Yeah, if you read the notes in the lecture slides, there's more details on, like, why you would like change, choose one versus another.",
    "start": "3894255",
    "end": "3901140"
  },
  {
    "text": "Okay, so now we're kind of ready to do neural net learning, right?",
    "start": "3901140",
    "end": "3907619"
  },
  {
    "text": "So- okay, remember we have this optimization problem, it's, the training loss now depends on both V and w,",
    "start": "3907620",
    "end": "3915720"
  },
  {
    "text": "and a training loss remember, is averaged over the losses of individual examples, uh, the loss of the individual example,",
    "start": "3915720",
    "end": "3921870"
  },
  {
    "text": "let's say we're doing regression, is the square difference between y and the function value,",
    "start": "3921870",
    "end": "3928185"
  },
  {
    "text": "and remember the function value is the summation over the- the weights at the last layer,",
    "start": "3928185",
    "end": "3934080"
  },
  {
    "text": "times the activations of the hidden layer, and- and that's the basic idea, okay?",
    "start": "3934080",
    "end": "3939675"
  },
  {
    "text": "And now all I have to do is compute this gradient. Um, so you look at this and you say okay, well,",
    "start": "3939675",
    "end": "3946019"
  },
  {
    "text": "if you get- have, enough scratch paper, you can probably like, work it out. Um, I'm gonna show you,",
    "start": "3946020",
    "end": "3953865"
  },
  {
    "text": "a different way to do this, um, without grinding through the chain rule.",
    "start": "3953865",
    "end": "3960435"
  },
  {
    "text": "Um, so this is going to be based on the computation graph, which will give you, um,",
    "start": "3960435",
    "end": "3968130"
  },
  {
    "text": "insight- more additional insight into the kind of the structure of computations, and visualize what it means,",
    "start": "3968130",
    "end": "3973575"
  },
  {
    "text": "what does a gradient kind of mean in some sense? And it also happens that these computation graphs, is really at the foundation of all of",
    "start": "3973575",
    "end": "3980609"
  },
  {
    "text": "these modern deep learning frameworks like TensorFlow and PyTorch. So, um, this is a real thing.",
    "start": "3980610",
    "end": "3987555"
  },
  {
    "text": "Um, it turns out that we've taught this it, many people still kinda prefer,",
    "start": "3987555",
    "end": "3993959"
  },
  {
    "text": "uh, to grind out the amount. I can't really tell why, except for maybe you're more familiar with that,",
    "start": "3993959",
    "end": "3999765"
  },
  {
    "text": "and so I would encourage everyone to kind of at least try to, um, think about the computation graph as a way to understand your gradients,",
    "start": "3999765",
    "end": "4007805"
  },
  {
    "text": "even though initially it might not be faster. And it's not to say that you always have to draw a graph, um,",
    "start": "4007805",
    "end": "4014675"
  },
  {
    "text": "to compute gradients, but doing a few times might give you additional insight that you wouldn't otherwise get.",
    "start": "4014675",
    "end": "4020510"
  },
  {
    "text": "Okay, so here we go. Um, so functions,",
    "start": "4020510",
    "end": "4025820"
  },
  {
    "text": "we can think about them as just boxes, right? The boxes you have some inputs going in,",
    "start": "4025820",
    "end": "4031535"
  },
  {
    "text": "and then you get some output. That's all a function is, okay? And partial derivatives or you know, gradients asked the question- the following question,",
    "start": "4031535",
    "end": "4043279"
  },
  {
    "text": "how much does the output change if the input changes a little bit? Okay? So for example if we have this function,",
    "start": "4043280",
    "end": "4050660"
  },
  {
    "text": "that just computes two times in1 plus in2in3. Um, you ask the question like,",
    "start": "4050660",
    "end": "4057619"
  },
  {
    "text": "you take input one and you just add a little epsilon. So like 0.001. And you ask hmm,",
    "start": "4057620",
    "end": "4064130"
  },
  {
    "text": "and- and you sti- uh, read out the output, and you say, \"Well, what happens to the output?",
    "start": "4064130",
    "end": "4069755"
  },
  {
    "text": "While in this case, uh, the output changed by 2 epsilon additively.",
    "start": "4069755",
    "end": "4074869"
  },
  {
    "text": "Okay? So then you conclude that the gradient of this function with respect to in1 is, is what?",
    "start": "4074870",
    "end": "4082700"
  },
  {
    "text": "[NOISE]. 2. 2, right?",
    "start": "4082700",
    "end": "4088400"
  },
  {
    "text": "Because the gradient is kind of the amplification. If I put an epsilon, then I get 2 epsilon out,",
    "start": "4088400",
    "end": "4093890"
  },
  {
    "text": "the gradient is 2, or the partial derivative. So okay, let's do this one. So if I add epsilon to in2,",
    "start": "4093890",
    "end": "4103025"
  },
  {
    "text": "then I- simple algebra shows I get a- a change in, in3 epsilon, so what's, um,",
    "start": "4103025",
    "end": "4109279"
  },
  {
    "text": "the partial with respect to in2?",
    "start": "4109280",
    "end": "4112380"
  },
  {
    "text": "In3, right? Okay, good. So you know, you could have done basic calculus and gotten that,",
    "start": "4115090",
    "end": "4121909"
  },
  {
    "text": "but I- I really kind of want to stress the kind of interpretation of, you know, perturbing inputs and witnessing the output,",
    "start": "4121910",
    "end": "4129170"
  },
  {
    "text": "because I think that's a useful, um, interpretation. Okay, so now, um,",
    "start": "4129170",
    "end": "4136100"
  },
  {
    "text": "all functions are- well, not all functions are made out of building blocks,",
    "start": "4136100",
    "end": "4141469"
  },
  {
    "text": "but most of the functions that we're interested in, in this class are going to be made out of these- these five pieces, okay?",
    "start": "4141470",
    "end": "4149975"
  },
  {
    "text": "And so for each of these pieces, it's you know, it's a function, it has inputs, a and b,",
    "start": "4149975",
    "end": "4156365"
  },
  {
    "text": "and you pump these things in and you get some output. Um, this, so there is a plus,",
    "start": "4156365",
    "end": "4162290"
  },
  {
    "text": "minus, times, max, and the logistic function. Okay, so on these edges,",
    "start": "4162290",
    "end": "4171140"
  },
  {
    "text": "I'm going to write down in green the partial derivative with respect to the input that's going into that function.",
    "start": "4171140",
    "end": "4180589"
  },
  {
    "text": "Okay? So let's do this. So if I have the function a plus b,",
    "start": "4180590",
    "end": "4186170"
  },
  {
    "text": "the partial derivative with respect to a is 1, and the partial derivative with respect to b is 1, okay?",
    "start": "4186170",
    "end": "4194280"
  },
  {
    "text": "And if you have minus, then it's 1 and minus 1,",
    "start": "4194470",
    "end": "4200135"
  },
  {
    "text": "um, if you have times, then the partial is b and a, okay?",
    "start": "4200135",
    "end": "4209239"
  },
  {
    "text": "Everyone follow so far, okay? Okay so max, uh, what is this?",
    "start": "4209240",
    "end": "4215030"
  },
  {
    "text": "This is maybe a little bit, you know, trickier. Um, so remember we kind of experienced the max last time.",
    "start": "4215030",
    "end": "4222815"
  },
  {
    "text": "So when the max, um, example you have, uh, a formula, just refresh.",
    "start": "4222815",
    "end": "4228380"
  },
  {
    "text": "Uh, uh,",
    "start": "4228380",
    "end": "4234560"
  },
  {
    "text": "so- so remember our last time we had the- we saw the max in the context of,",
    "start": "4234560",
    "end": "4239735"
  },
  {
    "text": "um, uh, the- the hinge loss, right? So you have the max of these two functions, which is this,",
    "start": "4239735",
    "end": "4247145"
  },
  {
    "text": "which means that, um, you know, let's say one is- one is a and the other is b. Um,",
    "start": "4247145",
    "end": "4253445"
  },
  {
    "text": "so if a is greater than b, then the, um,",
    "start": "4253445",
    "end": "4259895"
  },
  {
    "text": "then we need to take the derivative with, uh- sorry, then, uh- Okay, let me do it this way.",
    "start": "4259895",
    "end": "4267065"
  },
  {
    "text": "Okay, um, ig- ignore that thing on the board. So I just have max a of b, okay?",
    "start": "4267065",
    "end": "4272450"
  },
  {
    "text": "So suppose a is, uh, 7 and b is, uh, 3.",
    "start": "4272450",
    "end": "4277770"
  },
  {
    "text": "Uh, okay, so max, uh, a and b and let's say this is 7 and this is 3,",
    "start": "4281590",
    "end": "4288665"
  },
  {
    "text": "so that means a is greater than b. So now, if I change, um, a by a little bit,",
    "start": "4288665",
    "end": "4295610"
  },
  {
    "text": "then that change is going to be reflected by an output of a max function, right?",
    "start": "4295610",
    "end": "4300635"
  },
  {
    "text": "Because this, uh- this region is small and it doesn't matter. And, um, in this case,",
    "start": "4300635",
    "end": "4307355"
  },
  {
    "text": "if I change b by a little bit, then does the output change? No, because like, you know, 3.1,",
    "start": "4307355",
    "end": "4314480"
  },
  {
    "text": "2.9 is all, the output doesn't change, so the gradient is going to be 0 there. So the max function is partial derivatives, look like this.",
    "start": "4314480",
    "end": "4324590"
  },
  {
    "text": "So if a is greater than b, then this is going to be a 1, if a is less than b, this is going to be a 0 and you know, conversely over here,",
    "start": "4324590",
    "end": "4332210"
  },
  {
    "text": "if b is greater than a, then this is going to be a 1, if b is less than a,",
    "start": "4332210",
    "end": "4337670"
  },
  {
    "text": "then this is going to be a 0. Okay? So the partial of maximum, there's always 1 or 0 depending on this particular co- you know, condition.",
    "start": "4337670",
    "end": "4345905"
  },
  {
    "text": "Okay, and then the logistic function, um, this is just a fact you can derive it in your,",
    "start": "4345905",
    "end": "4351470"
  },
  {
    "text": "you know, free time but I had on a previous slide. It's just like the sigmoid, um, uh, logistic function, times 1,",
    "start": "4351470",
    "end": "4358850"
  },
  {
    "text": "minus the logistic function. Okay so now you have these building blocks,",
    "start": "4358850",
    "end": "4364715"
  },
  {
    "text": "now you can compose and you can build castles out of them. It turns out like all- basically all functions that you see in,",
    "start": "4364715",
    "end": "4372094"
  },
  {
    "text": "you know, you know, deep learning are just basically bail- built- built out of these blocks. Um, and how do you compose things?",
    "start": "4372095",
    "end": "4378980"
  },
  {
    "text": "Um, there's this nice, uh, thing called, the chain rule, which says that,",
    "start": "4378980",
    "end": "4386000"
  },
  {
    "text": "\"If you think about input going to one function and that output going into input in a new function,",
    "start": "4386000",
    "end": "4392945"
  },
  {
    "text": "then the partial derivative with respect to the input of the output is just the product of the partial derivative.\"",
    "start": "4392945",
    "end": "4399770"
  },
  {
    "text": "This is just the chain rule, right? And you can think about as like- you know, think about amplification.",
    "start": "4399770",
    "end": "4405739"
  },
  {
    "text": "So this function amplifies by two times, and this amplifi- this function amplifies by 5,",
    "start": "4405740",
    "end": "4411860"
  },
  {
    "text": "then total amplification is going to be 2 times 5, okay?",
    "start": "4411860",
    "end": "4416489"
  },
  {
    "text": "All right, so now let's take an example, we're going to do, uh,",
    "start": "4416980",
    "end": "4423380"
  },
  {
    "text": "binary classification with the hinge loss, um, just as a warm-up, um,",
    "start": "4423380",
    "end": "4428505"
  },
  {
    "text": "and I'm going to draw this computation graph, and then compute the partial derivative with respect to w. Okay,",
    "start": "4428505",
    "end": "4435440"
  },
  {
    "text": "so what is this graph? Um, so I have w times Phi of X, that's a score, times y, that's a margin,",
    "start": "4435440",
    "end": "4442450"
  },
  {
    "text": "1 minus margin, um, max of 1 minus margin 0 is a loss, okay?",
    "start": "4442450",
    "end": "4448660"
  },
  {
    "text": "So now for every edge I can draw the partial, uh, derivative, okay?",
    "start": "4448660",
    "end": "4453800"
  },
  {
    "text": "So here remember the partial derivative here is, uh, left-hand-side greater than d or the right- the right branch.",
    "start": "4453800",
    "end": "4461120"
  },
  {
    "text": "So 1 minus margin greater than 0. Um, for minus, this is a minus 1. For a times, this is going to be whatever is over here.",
    "start": "4461120",
    "end": "4469429"
  },
  {
    "text": "Uh, for this times, it's going to be whatever is over here. And by the chain rule,",
    "start": "4469430",
    "end": "4475235"
  },
  {
    "text": "if you multiply what's on all the edges, then you get the gradient of the loss with respect to w.",
    "start": "4475235",
    "end": "4483600"
  },
  {
    "text": "Okay. So this is kind of a graphical way of doing what you,",
    "start": "4484060",
    "end": "4489320"
  },
  {
    "text": "you know, probably wha- what I did last time, which is, um, if the margin is, um, uh,",
    "start": "4489320",
    "end": "4495350"
  },
  {
    "text": "less than- greater than 1, then it's- everything is 0.",
    "start": "4495350",
    "end": "4500659"
  },
  {
    "text": "And if the margin's less than 1 then I'd perform this, uh, particular update. Okay? So in the interest of time,",
    "start": "4500660",
    "end": "4509585"
  },
  {
    "text": "um, I'm not going to do it for the simple neural network. Uh, I will do this in section.",
    "start": "4509585",
    "end": "4515869"
  },
  {
    "text": "But, you know, at a high level, you basically do the same thing. You multiply all the, you know, blue edge, uh,",
    "start": "4515870",
    "end": "4521554"
  },
  {
    "text": "the edges and you get the- the, uh, partial derivatives. Okay. So- so now,",
    "start": "4521555",
    "end": "4528619"
  },
  {
    "text": "you know, we've kind of done everything kind of manually. I wanted to kind of systematized this and talk about an algorithm called back-propagation,",
    "start": "4528620",
    "end": "4538070"
  },
  {
    "text": "uh, that, um, allows you to compute gradients for arbitrary computation graph. That means, any kind of, uh,",
    "start": "4538070",
    "end": "4543710"
  },
  {
    "text": "function that you can build out of these building blocks, you can actually just get the derivatives.",
    "start": "4543710",
    "end": "4548975"
  },
  {
    "text": "So, you know, one nice thing about these packages like PyTorch or TensorFlow is that, you actually don't have to compute the derivatives on your own.",
    "start": "4548975",
    "end": "4557255"
  },
  {
    "text": "It used to be the case that, you know, uh, before these, people would have to crank- implement this derivatives by- by,",
    "start": "4557255",
    "end": "4563290"
  },
  {
    "text": "um, hand, which is really tedious and error prone. And part of why it's been so easy to kind of develop new models is that all that's done for you automatically.",
    "start": "4563290",
    "end": "4571195"
  },
  {
    "text": "Okay. So back-propagation is gonna compute two types of values; a forward value and a backward value.",
    "start": "4571195",
    "end": "4578375"
  },
  {
    "text": "So f_i for every, um, node I is the simply the value of that expression tree.",
    "start": "4578375",
    "end": "4587285"
  },
  {
    "text": "And, um, the backward value, g_i,is going to be the partial derivative with respect to output of,",
    "start": "4587285",
    "end": "4596105"
  },
  {
    "text": "uh, that- the value at that node. Okay? So for example, f_i here is gonna be,",
    "start": "4596105",
    "end": "4602300"
  },
  {
    "text": "um, w_1 times, uh, um, Sigma v_1 times, uh, phi of x.",
    "start": "4602300",
    "end": "4608599"
  },
  {
    "text": "And g of that node is going to be, uh, the, basically the product of all these edges.",
    "start": "4608600",
    "end": "4614929"
  },
  {
    "text": "Basically, how much does this node change the output at the fin- uh, at- at the very top.",
    "start": "4614930",
    "end": "4621480"
  },
  {
    "text": "Okay. So the algorithm itself is- is, you know, quite straight forward.",
    "start": "4622090",
    "end": "4627745"
  },
  {
    "text": "There is a forward pass which computes all the f_i's, and then there's a backward pass that computes all the g_i's.",
    "start": "4627745",
    "end": "4633655"
  },
  {
    "text": "So in the forward pass, you start from the leaves and you go to the root, and you compute each of these,",
    "start": "4633655",
    "end": "4640430"
  },
  {
    "text": "uh, values kind of recursively. Where the computation depends on,",
    "start": "4640430",
    "end": "4645830"
  },
  {
    "text": "you know, the sub-expressions. Um, and in the backward pass, um, you, similarly have a recurrence that, uh,",
    "start": "4645830",
    "end": "4653405"
  },
  {
    "text": "gives you the value of a particular- a g_i of a particular node is equal",
    "start": "4653405",
    "end": "4659150"
  },
  {
    "text": "to the g_i of its parent times whatever is on, um, this edge.",
    "start": "4659150",
    "end": "4665449"
  },
  {
    "text": "Okay? So it's like you take a forward pass, you fill in all the f_i's and then you take a backward pass,",
    "start": "4665450",
    "end": "4671119"
  },
  {
    "text": "and you fill in all the g_i's that you care about.",
    "start": "4671120",
    "end": "4673770"
  },
  {
    "text": "Okay? All right. So section will go through this in, uh, detail. I realize this might have been a little bit quick.",
    "start": "4676360",
    "end": "4683060"
  },
  {
    "text": "Um, one quick note about optimization is that, now, you have all the tools that you can do,",
    "start": "4683060",
    "end": "4688730"
  },
  {
    "text": "you can run SLG on in which doesn't really care about whether you're, um, you're, you know, what the function is.",
    "start": "4688730",
    "end": "4695420"
  },
  {
    "text": "It's just like a function. You have it, you can compute the gradient, that's all you need. But one kind of, eh, important thing to note is that just",
    "start": "4695420",
    "end": "4702050"
  },
  {
    "text": "because you can compute a gradient doesn't mean you optimize the function. So for a linear function,",
    "start": "4702050",
    "end": "4707539"
  },
  {
    "text": "it turns out that if you define these loss functions on top, you get these convex functions.",
    "start": "4707540",
    "end": "4712550"
  },
  {
    "text": "So convex functions are these functions that you can hold in your hand, and, eh, um, and have a one global, uh, minimum.",
    "start": "4712550",
    "end": "4719420"
  },
  {
    "text": "And so if you think about SLG, it's going- going downhill. You converge to the global minimum and you solve the problem.",
    "start": "4719420",
    "end": "4724639"
  },
  {
    "text": "Whereas neural nets, it turns out that the loss functions are non-convex, which means that if you try to go downhill,",
    "start": "4724640",
    "end": "4731195"
  },
  {
    "text": "you might get stuck in local optima. And in general, optimization of neural nets is hard.",
    "start": "4731195",
    "end": "4736805"
  },
  {
    "text": "In practice, people somehow managed to do it anyway and it works. There's a gap between theory and practice which is",
    "start": "4736805",
    "end": "4743284"
  },
  {
    "text": "an active area of research. Okay. So in one minute,",
    "start": "4743285",
    "end": "4749704"
  },
  {
    "text": "I I have to do nearest neighbors. [BACKGROUND] Um, it will actually be fine because nearest neighbors is really simple,",
    "start": "4749705",
    "end": "4755929"
  },
  {
    "text": "so you can do it in one minute. So here it goes. Um, so let's throw away everything we knew about linear classifiers in neural nets.",
    "start": "4755930",
    "end": "4763655"
  },
  {
    "text": "Here's the algorithm. You're training as you store your training examples. That's it. And then,",
    "start": "4763655",
    "end": "4769880"
  },
  {
    "text": "the predictor of a particular example that you get is you're gonna go through all the training examples",
    "start": "4769880",
    "end": "4776480"
  },
  {
    "text": "and find the one which is closest- has input which is closest to your- uh,",
    "start": "4776480",
    "end": "4783500"
  },
  {
    "text": "your, um, input x prime. And then you're just gonna train- you're gonna return, um, y.",
    "start": "4783500",
    "end": "4790360"
  },
  {
    "text": "Okay? So, um, and the intuition here is that similar examples- it's similar inputs should get similar outputs.",
    "start": "4790360",
    "end": "4798014"
  },
  {
    "text": "Okay? So here's an, uh, pictorial example. So suppose we're in two dimensions and you're doing classification and [NOISE] you have,",
    "start": "4798015",
    "end": "4806120"
  },
  {
    "text": "a plus over here. Um, let's do this plus and you have,",
    "start": "4806120",
    "end": "4813070"
  },
  {
    "text": "um, you know, [NOISE] a minus here. Okay? So if you are asking what is the pro- uh,",
    "start": "4813070",
    "end": "4818920"
  },
  {
    "text": "label assigned to that point, it should be plus because this is closer. Um, this should be minus.",
    "start": "4818920",
    "end": "4825469"
  },
  {
    "text": "This region should be minus. This should be plus. And, [NOISE] you know, one kind of cool thing is that is, where's the decision boundary?",
    "start": "4825470",
    "end": "4831485"
  },
  {
    "text": "So if you look at the point that is equidistant from these, and draw perpendicular, um,",
    "start": "4831485",
    "end": "4837905"
  },
  {
    "text": "that's the decision boundary there, um, same thing over here, um, and, uh,",
    "start": "4837905",
    "end": "4843215"
  },
  {
    "text": "so you have basically carved out this region where this [NOISE] is minus and,",
    "start": "4843215",
    "end": "4848690"
  },
  {
    "text": "[NOISE] um, everything here is [NOISE], you know, plus. Okay? [NOISE] Um, in general, this is, um,",
    "start": "4848690",
    "end": "4856310"
  },
  {
    "text": "what I've drawn is an instance of a Voronoi diagram which if you're given a bunch of points,",
    "start": "4856310",
    "end": "4862085"
  },
  {
    "text": "um, the defined regions of points which are closest to that point. And everything in a particular region like",
    "start": "4862085",
    "end": "4868070"
  },
  {
    "text": "this yellow region is assigned the same label as, um, this point here.",
    "start": "4868070",
    "end": "4873110"
  },
  {
    "text": "And this- this is, um, what is called a non-parametric model which means that,",
    "start": "4873110",
    "end": "4878315"
  },
  {
    "text": "the number- it doesn't mean that there's no parameters. It means that the number of parameters is not fixed.",
    "start": "4878315",
    "end": "4883864"
  },
  {
    "text": "The more points you have, the more kind of each point has its own parameter. Um, so you can actually fit really expressive models, um, using that.",
    "start": "4883865",
    "end": "4892715"
  },
  {
    "text": "It's very simple, uh, but it's kind of computationally expensive because you have to store your entire training examples.",
    "start": "4892715",
    "end": "4899330"
  },
  {
    "text": "Okay. So we looked at three different, um, models and, you know, there's a saying that well, and,",
    "start": "4899330",
    "end": "4905285"
  },
  {
    "text": "uh, I guess in school, you- there's three things study, sleep, and party or something, and you have to only pick two of them.",
    "start": "4905285",
    "end": "4911345"
  },
  {
    "text": "Well, so for learning, it's kinda the same. It can either be fast to predict for linear models and neural nets.",
    "start": "4911345",
    "end": "4918364"
  },
  {
    "text": "Um, it can be easy to learn for linear models and uh, um, nearest neighbors or it could be powerful.",
    "start": "4918365",
    "end": "4925520"
  },
  {
    "text": "For example, like neural networks and nearest neighbors but there's always some sort of compromise and exactly what method you choose,",
    "start": "4925520",
    "end": "4932120"
  },
  {
    "text": "um, will depend on kind of what you care about. Okay. See you next time.",
    "start": "4932120",
    "end": "4936840"
  }
]