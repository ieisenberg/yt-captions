[
  {
    "start": "0",
    "end": "5400"
  },
  {
    "text": "All right. Good morning, everyone. I'm Joshua Ott. I work in the Stanford\nintelligent systems lab.",
    "start": "5400",
    "end": "10950"
  },
  {
    "text": "And my research\nis largely focused on autonomous exploration. So as you can imagine,\na large part of that",
    "start": "10950",
    "end": "17520"
  },
  {
    "text": "involves decision-making\nunder uncertainty, specifically reasoning about how\nwe plan out a series of actions",
    "start": "17520",
    "end": "24660"
  },
  {
    "text": "into the future,\nand quantifying, and thinking about how the\nuncertainty will propagate",
    "start": "24660",
    "end": "30210"
  },
  {
    "text": "through those actions. And so very closely\nrelated to that is what we're going to\nbe talking about today,",
    "start": "30210",
    "end": "35850"
  },
  {
    "text": "which is policy optimization. So with policy optimization,\nwhere we're going to start",
    "start": "35850",
    "end": "41910"
  },
  {
    "text": "is looking at this parameterized\npolicy that we set up. And then looking at just local\nchanges that we can make to it.",
    "start": "41910",
    "end": "49630"
  },
  {
    "text": "So that's the policy\nsearch section. Then we're going to\ntransition to talking about how can we estimate\nthe gradient with respect",
    "start": "49630",
    "end": "57180"
  },
  {
    "text": "to that policy. And we're going to use that\ngradient estimate to then help us optimize our policy.",
    "start": "57180",
    "end": "62379"
  },
  {
    "text": "So that's the layout, the\nscope for policy optimization. But I think before\ndiving in to any of that,",
    "start": "62380",
    "end": "69670"
  },
  {
    "text": "it's helpful to zoom all the way\nout, look at the big picture, orient ourselves of where\nwe are, what we're doing,",
    "start": "69670",
    "end": "76090"
  },
  {
    "text": "and why we're doing it. So we have this model that\nwe're working with here, where we have an agent\nacting in some environment.",
    "start": "76090",
    "end": "83530"
  },
  {
    "text": "And the agent can take an action\nand receive some observation. So this is very general. The agent could be a Rover on\nthe surface of another planet,",
    "start": "83530",
    "end": "92020"
  },
  {
    "text": "or it could be a financial\ntrading agent that's making buy and sell decisions. It's very, very general.",
    "start": "92020",
    "end": "98170"
  },
  {
    "text": "So maybe if we go one\nand two levels down now. We have this idea of the\nMarkov decision process, which",
    "start": "98170",
    "end": "105130"
  },
  {
    "text": "is saying essentially\nthe same thing as the picture on the left, just\nwith a little more structure. So we have some\nagent in a state.",
    "start": "105130",
    "end": "112540"
  },
  {
    "text": "That agent can take actions. And based on the action\nthat it takes in that state, it receives some reward and\nthen transitions to a new state.",
    "start": "112540",
    "end": "120670"
  },
  {
    "text": "So this idea of receiving\nrewards in a particular state brings out the idea of the\nutility function, which",
    "start": "120670",
    "end": "127510"
  },
  {
    "text": "is telling us what's the\nutility or value for being in a particular state. And then linked to that, we\nhave this idea of the policy.",
    "start": "127510",
    "end": "135069"
  },
  {
    "text": "And that's what we're\ntalking about today. So the policy is\nlike our strategy. It's how we take actions.",
    "start": "135070",
    "end": "140930"
  },
  {
    "text": "It's telling us what\naction we should take in a particular state. So that's the big picture\nwe should keep in mind",
    "start": "140930",
    "end": "147532"
  },
  {
    "text": "as we're going through this. That's how all of these\npieces fit together. And now, we're going to\ndive into the details of it.",
    "start": "147533",
    "end": "153500"
  },
  {
    "text": "So we're starting\nwith policy search. And policy search is really\nfocused on just searching",
    "start": "153500",
    "end": "161290"
  },
  {
    "text": "the space of policies. So it's exactly\nwhat's in the title. So how we're going to start here\nis by parameterizing our policy.",
    "start": "161290",
    "end": "169850"
  },
  {
    "text": "So we're going to\nrepresent our policy with some vector of\nparameters here that we're",
    "start": "169850",
    "end": "176380"
  },
  {
    "text": "going to call our thetas. So we have n\ndifferent parameters that we're calling\ntheta 1 to theta n, OK.",
    "start": "176380",
    "end": "184130"
  },
  {
    "text": "So these parameters right now\ncould be anything, really. Whatever we want, we're\nnot saying how we're",
    "start": "184130",
    "end": "189519"
  },
  {
    "text": "parameterizing our policy. We're just saying that\nwe have these parameters. So our policy could\nbe parameterized",
    "start": "189520",
    "end": "196120"
  },
  {
    "text": "by some neural network. And these would be\nthe weights and biases of our neural network. But it doesn't have to be.",
    "start": "196120",
    "end": "201590"
  },
  {
    "text": "It can be some other\ngeneral function. Question. When you say this,\nhow does it differ",
    "start": "201590",
    "end": "206718"
  },
  {
    "text": "or how does it fit into\nthe world like what we learned about lectures ago\nwith policy and recognizing it?",
    "start": "206718",
    "end": "214239"
  },
  {
    "text": "That's a great question. That's exactly where\nwe're going to next. Yeah, we'll dive into that.",
    "start": "214240",
    "end": "219447"
  },
  {
    "text": "You want to repeat the question? The question was, how\ndoes policy search relate to what we've seen\nin the past with value",
    "start": "219447",
    "end": "226300"
  },
  {
    "text": "iteration or policy iteration? So we'll see that right now. So we have this set\nof parameters here.",
    "start": "226300",
    "end": "234490"
  },
  {
    "text": "And we can say our policy\nnow is just this pi theta. So our policy just parameterized\nby this set of parameters.",
    "start": "234490",
    "end": "243850"
  },
  {
    "text": "So, now, going to your question,\nwhat we've seen in the past is that we know we can say\nwhat the utility when following",
    "start": "243850",
    "end": "252519"
  },
  {
    "text": "a particular policy is. So we've seen that we can say\nthe utility when following a policy from some state S is\nequal to the immediate reward",
    "start": "252520",
    "end": "262540"
  },
  {
    "text": "that we receive,\nplus the action, for the action that our policy\ntells us to take in that state.",
    "start": "262540",
    "end": "269450"
  },
  {
    "text": "And then we have the\ndiscounted future reward here. So sum over all next states\nthat we could then transition",
    "start": "269450",
    "end": "276490"
  },
  {
    "text": "to when following that policy. And then we also have the U of\nS prime that's cut off here.",
    "start": "276490",
    "end": "284660"
  },
  {
    "text": "So that's in there. So that's telling\nus the utility when following a particular policy.",
    "start": "284660",
    "end": "290630"
  },
  {
    "text": "But that's only from\na certain state. So what we want to\nget an idea of now",
    "start": "290630",
    "end": "295900"
  },
  {
    "text": "is the utility of our\npolicy in general. Not just from one state, but\nfrom every possible state",
    "start": "295900",
    "end": "302169"
  },
  {
    "text": "that we could be in. So what that looks like is when\nwe say the utility of a policy,",
    "start": "302170",
    "end": "308350"
  },
  {
    "text": "we're talking about--\nso this is utility of a policy\nparameterized by theta. What we'll do is we're\ngoing to equivalently",
    "start": "308350",
    "end": "314620"
  },
  {
    "text": "just write this as U of theta. So this is just\nshorthand notation here to drop the pie theta.",
    "start": "314620",
    "end": "321520"
  },
  {
    "text": "Because really what controls\nour policy is these parameters. So these mean the\nsame thing here. It's just convention.",
    "start": "321520",
    "end": "327660"
  },
  {
    "text": "All right. So what we're saying now\nis we want the utility of our policy in general.",
    "start": "327660",
    "end": "332900"
  },
  {
    "text": "So what we're going\nto do is we're going to look at over\nevery possible state that we could be in.",
    "start": "332900",
    "end": "338590"
  },
  {
    "text": "The probability that\nwe're in that state, then times the utility when following\nour policy from that state.",
    "start": "338590",
    "end": "345280"
  },
  {
    "text": "So that's what we\nmean when we're talking about this general\nidea of the utility of a particular policy.",
    "start": "345280",
    "end": "351770"
  },
  {
    "text": "So using this same concept\nthat we've seen in the past, and now applying it to looking\nat every possible state",
    "start": "351770",
    "end": "358246"
  },
  {
    "text": "that we could be in. This is the probability\nthat we're in that state. So, in practice, though, this\nis pretty hard to compute.",
    "start": "358247",
    "end": "367020"
  },
  {
    "text": "If we have a very\nlarge state space or our state space\nis continuous, it's almost impossible to get\nthis actual quantitative value.",
    "start": "367020",
    "end": "375440"
  },
  {
    "text": "So that's what we\nwant to talk about now is how can we\nactually approximate this utility of a policy.",
    "start": "375440",
    "end": "382010"
  },
  {
    "text": "Because in practice, when\nwe're working with large scale problems, it's not feasible\nto directly compute this.",
    "start": "382010",
    "end": "388539"
  },
  {
    "text": "Question? When you say (b(s)) the\nprevious one, like T the transition\nprobabilities make sense.",
    "start": "388540",
    "end": "394250"
  },
  {
    "text": "But how do you calculate\na general probability for what the probability is? Yeah, good question.",
    "start": "394250",
    "end": "399300"
  },
  {
    "text": "So this would be either you have\nsome prior knowledge of what",
    "start": "399300",
    "end": "404750"
  },
  {
    "text": "the probability\nwould be of starting in that particular state. You might have some\ndata set in which case",
    "start": "404750",
    "end": "411259"
  },
  {
    "text": "you could back out a\nprobability from that data. But some domain knowledge\nthat you have here",
    "start": "411260",
    "end": "416780"
  },
  {
    "text": "would give you this\nprior knowledge of what the probability\nof being in that state is.",
    "start": "416780",
    "end": "423860"
  },
  {
    "text": "And sorry the question was, how\ndo we know this B of S here?",
    "start": "423860",
    "end": "429830"
  },
  {
    "text": "So we want to come\nup with a way that we can get some approximation\nfor this utility.",
    "start": "429830",
    "end": "435655"
  },
  {
    "text": "And that's what we're\ngoing to talk about now. And I think it helps to start\nwith this visual example here.",
    "start": "435655",
    "end": "442610"
  },
  {
    "text": "So for this example,\nconsider that your a rover on the surface of Mars.",
    "start": "442610",
    "end": "448680"
  },
  {
    "text": "And let's say that we're\npretty sure our rover is starting somewhere in\nthis ellipse region.",
    "start": "448680",
    "end": "454590"
  },
  {
    "text": "We're not exactly\nsure where, but we have some initial\nbelief about where we think the rover is here.",
    "start": "454590",
    "end": "460410"
  },
  {
    "text": "So what we're going to do to\nestimate this utility here is we're going to\nsimulate policy rollouts.",
    "start": "460410",
    "end": "466630"
  },
  {
    "text": "And that's what we're\ngoing to visualize here. So to simulate a policy rollout. What we do, we have this\ninitial belief distribution.",
    "start": "466630",
    "end": "473169"
  },
  {
    "text": "We're going to sample an\ninitial state from that. So this distribution could\nbe a Gaussian distribution, it could be a\nuniform distribution.",
    "start": "473170",
    "end": "479650"
  },
  {
    "text": "Doesn't matter right\nnow, we're just saying we have some distribution\nthat we're starting with. We sample an initial state\nfrom that distribution",
    "start": "479650",
    "end": "487080"
  },
  {
    "text": "and then we're going to give\nthat state to our policy. So we say, hey policy I'm\nin this state right now,",
    "start": "487080",
    "end": "493660"
  },
  {
    "text": "what should I do? The policy says you\nshould go 1 meter forward. So you go 1 meter forward.",
    "start": "493660",
    "end": "499030"
  },
  {
    "text": "And then now you ask your policy\nagain, what should I do now? Turn 1 meter to the right. So you turn 1 meter\nto the right and then",
    "start": "499030",
    "end": "504780"
  },
  {
    "text": "you ask your policy again. Now it says, go 1 meter forward. And you just continue to do that\nand you roll out your policy",
    "start": "504780",
    "end": "511020"
  },
  {
    "text": "into the future. So that's a policy rollout. So we do that, we\nroll out our policy",
    "start": "511020",
    "end": "517589"
  },
  {
    "text": "to some depth that we specified. And we get a trajectory now. So that's one policy\nrollout that we've done.",
    "start": "517590",
    "end": "524380"
  },
  {
    "text": "We can repeat this\nprocess, right? We can repeat it as\nmany times as we want. So we sample a new state now.",
    "start": "524380",
    "end": "529769"
  },
  {
    "text": "And then we roll out our\npolicy into the future. And we can just\ncontinue repeating this.",
    "start": "529770",
    "end": "535150"
  },
  {
    "text": "In our case, we'll do\nthis three times here. So now we have\nthree trajectories that we've rolled\nout into the future.",
    "start": "535150",
    "end": "541410"
  },
  {
    "text": "With each of these right,\nwe have some reward model that we can then query right\nfor each of these trajectories",
    "start": "541410",
    "end": "548370"
  },
  {
    "text": "that we roll out, we get\nsome associated reward. So this top one probably\nis not so good, right?",
    "start": "548370",
    "end": "553620"
  },
  {
    "text": "Because it drives us\nover the rocks there. That's going to be a very\nbumpy ride for our rover, so we don't like that.",
    "start": "553620",
    "end": "558660"
  },
  {
    "text": "The middle one does\na little bit better. But still gets us close\nto the rock, so not great.",
    "start": "558660",
    "end": "563980"
  },
  {
    "text": "And then this bottom one,\nwe'd say that's pretty good. Steers us clear there. So we have that\napproximation now",
    "start": "563980",
    "end": "570947"
  },
  {
    "text": "that we've done just with these. By simulating these policy\nrollouts into the future. So now we're going to\nuse that to come back",
    "start": "570947",
    "end": "576870"
  },
  {
    "text": "to how do we estimate this\nutility of our policy. ",
    "start": "576870",
    "end": "584022"
  },
  {
    "text": "So what we're\ngoing to do here is we've collected these trajectory\nrollouts that we've simulated.",
    "start": "584022",
    "end": "590519"
  },
  {
    "text": "So now we can say\nutility of our policy",
    "start": "590520",
    "end": "595880"
  },
  {
    "text": "is just the expected reward\nthat we think will receive along those trajectories.",
    "start": "595880",
    "end": "601830"
  },
  {
    "text": "So this is just an expectation\nover the rewards that we got. The red, yellow, and\ngreen trajectories",
    "start": "601830",
    "end": "608420"
  },
  {
    "text": "those were the rewards\nthat we received. So we're taking the\nexpectation over that. So from the definition\nof just expectations,",
    "start": "608420",
    "end": "615709"
  },
  {
    "text": "we can write this out\nin an integral form. Where we have the probability\nof that trajectory occurring",
    "start": "615710",
    "end": "623000"
  },
  {
    "text": "when following our\nparticular policy. And then just from\ndefinition of expectation",
    "start": "623000",
    "end": "628129"
  },
  {
    "text": "it's just times\nthe reward, right? Expectation is probability of\nthis term times the term, right?",
    "start": "628130",
    "end": "634200"
  },
  {
    "text": "And then we just integrate\nthat, that's the expectation. So again-- yes question?",
    "start": "634200",
    "end": "642551"
  },
  {
    "text": "Parallel about like how\nvariables in the work maybe? If there are some cases\nwhere we do really badly",
    "start": "642551",
    "end": "647960"
  },
  {
    "text": "and like that policy talk? Yeah so the question\nwas, are there cases where we care about how\nvariable the reward might be?",
    "start": "647960",
    "end": "655500"
  },
  {
    "text": "So that's an excellent\nquestion and that's a very big part of policy\noptimization in general.",
    "start": "655500",
    "end": "661920"
  },
  {
    "text": "Is that the variance of the\nrewards from your policy rollouts tends to be quite high. So dealing with that\nvariance is a big challenge.",
    "start": "661920",
    "end": "669180"
  },
  {
    "text": "And that's something\nwe're been get to later, so that's a great question. So we have this expectation\nhere and similar",
    "start": "669180",
    "end": "679130"
  },
  {
    "text": "to what we have before, right? We're not actually going\nto compute this integral.",
    "start": "679130",
    "end": "684140"
  },
  {
    "text": "Maybe in certain cases where\nwe have a simple policy set up, we could do this.",
    "start": "684140",
    "end": "689310"
  },
  {
    "text": "But in the general sense,\nwe're going to approximate this with Monte Carlo samples.",
    "start": "689310",
    "end": "694350"
  },
  {
    "text": "So we know we've already\nsampled our different policy trajectories here.",
    "start": "694350",
    "end": "700470"
  },
  {
    "text": "And to approximate\nthis integral, we're going to say that this\nis approximately equal to just",
    "start": "700470",
    "end": "707839"
  },
  {
    "text": "the average reward that we\nreceive along our trajectories.",
    "start": "707840",
    "end": "713210"
  },
  {
    "text": "Where this is, This\nis the Ith trajectory.",
    "start": "713210",
    "end": "720290"
  },
  {
    "text": "So we go from I equals 1 to m. So in that example that we saw.",
    "start": "720290",
    "end": "725870"
  },
  {
    "text": "We did three policy\nrollouts, right? So we had three trajectories. So m was 3 here.",
    "start": "725870",
    "end": "731200"
  },
  {
    "text": "And then we're just\naveraging the reward, right? So the average between\nred, yellow, and green that would give us somewhere\nin the yellow, right?",
    "start": "731200",
    "end": "737220"
  },
  {
    "text": "So it's just an OK\npolicy, it's not great. Yes? So when you say the reward. Do you mean the instantaneous\nreward and just one point?",
    "start": "737220",
    "end": "744079"
  },
  {
    "text": "Or do you mean like the\nwhole Bellman equations into the future added on thing? Yeah, good question.",
    "start": "744080",
    "end": "749550"
  },
  {
    "text": "This is the discounted-- the\ntotal discounted reward we receive along that trajectory. Yep the question was\nwhen we say reward here.",
    "start": "749550",
    "end": "757310"
  },
  {
    "text": "Are we talking about\nthe immediate reward or the discounted\nreward into the future.",
    "start": "757310",
    "end": "762630"
  },
  {
    "text": "So this is the discounted\nreward that we receive along the whole trajectory.",
    "start": "762630",
    "end": "767660"
  },
  {
    "text": "So that's how we use this\nidea of policy rollouts. Why are we doing this, right?",
    "start": "767660",
    "end": "773040"
  },
  {
    "text": "What's the point of\nthe policy rollouts? We wanted to get this\nconcept of the utility",
    "start": "773040",
    "end": "778860"
  },
  {
    "text": "of a particular policy. This allows us to say is a\npolicy good or not right. It's giving us some\nquantitative way to say,",
    "start": "778860",
    "end": "786450"
  },
  {
    "text": "is our policy doing well. We saw that we would have to\nlook at every possible state",
    "start": "786450",
    "end": "793140"
  },
  {
    "text": "right. So instead of doing\nthat, we're just saying from our current state,\nlet's simulate trajectories out",
    "start": "793140",
    "end": "798570"
  },
  {
    "text": "into the future. And then look at the average\nreward that we're getting. And that gives us this\nestimate for how good",
    "start": "798570",
    "end": "803850"
  },
  {
    "text": "is our policy right now. So that's the whole picture of\nwhat we've looked at so far.",
    "start": "803850",
    "end": "809529"
  },
  {
    "text": "Now, we're going to use\nthis idea for policy search. So this was just the\nsetup of how we quantify",
    "start": "809530",
    "end": "815580"
  },
  {
    "text": "is a policy good or not. So what we're going to\nstart with is local search.",
    "start": "815580",
    "end": "822110"
  },
  {
    "text": "So what we're showing\nhere in this picture. You have theta 2 on the y-axis.",
    "start": "822110",
    "end": "827380"
  },
  {
    "text": "Theta 1 is on the x-axis. So this is just a\nvery simple policy. It's just a two--",
    "start": "827380",
    "end": "833610"
  },
  {
    "text": "we have two parameters in this\npolicy, theta 1 and theta 2, very simple. And what we're showing\nin the color scheme",
    "start": "833610",
    "end": "840300"
  },
  {
    "text": "there is the utility\nof those parameters. Which we don't know\nthat to start with.",
    "start": "840300",
    "end": "846459"
  },
  {
    "text": "So we don't know any of that. That's hidden to us. But for the visualization, we're\nseeing it in the background.",
    "start": "846460",
    "end": "852420"
  },
  {
    "text": "So what we do is we start\nwith our current parameters. So our current set of theta 1\nand theta 2 puts us right here.",
    "start": "852420",
    "end": "861412"
  },
  {
    "text": "Then what we do in\nlocal search is we just simply take a plus\nstep, and a minus",
    "start": "861412",
    "end": "867450"
  },
  {
    "text": "step along each\nparameter direction. So because we have two\nparameters here that would lead",
    "start": "867450",
    "end": "874230"
  },
  {
    "text": "us to taking 2n so 2 times\nn, where n is the number of parameters. That's the number of candidate\npoints we're going to evaluate.",
    "start": "874230",
    "end": "881710"
  },
  {
    "text": "So here we have two parameters. 2 times 2 is 4. So we're looking at four\ncandidate parameters,",
    "start": "881710",
    "end": "887160"
  },
  {
    "text": "and that's what\nyou're seeing here. So this was a plus step\nin the theta 2 direction. Minus step in the\ntheta 2 direction.",
    "start": "887160",
    "end": "895440"
  },
  {
    "text": "Skipping ahead there, this is a\nplus step in the theta 1 minus step in the theta 1.",
    "start": "895440",
    "end": "900670"
  },
  {
    "text": "So we're looking at those\nfour candidate points. Now, at each of these\ncandidate points,",
    "start": "900670",
    "end": "907230"
  },
  {
    "text": "we have a new set of\nparameters here, right? So with this new set. We then do some m set of policy\nrollouts into the future.",
    "start": "907230",
    "end": "916480"
  },
  {
    "text": "And that gives us an estimate\nfor our utility at each of those parameter sets.",
    "start": "916480",
    "end": "923570"
  },
  {
    "text": "So when we do that. We see well which one\nis the best performer? It will be this point.",
    "start": "923570",
    "end": "929950"
  },
  {
    "text": "Assuming we do a large\nnumber of rollouts to get a reasonable estimate. So what we then do is\nwe move our parameters",
    "start": "929950",
    "end": "937260"
  },
  {
    "text": "to that candidate point. So we move over there because we\nsaid that's the best performer. And now we just\nrepeat this process.",
    "start": "937260",
    "end": "944170"
  },
  {
    "text": "So we again do our 2n. We evaluate 2n candidate points. We do m sets of\ntrajectory rollouts,",
    "start": "944170",
    "end": "951330"
  },
  {
    "text": "for each of those candidate\npoints we're looking at. We get some estimate\nof the utility for that particular policy.",
    "start": "951330",
    "end": "957850"
  },
  {
    "text": "And then we step to the\ndirection of the best one. So here the best one\nwill be this one.",
    "start": "957850",
    "end": "963430"
  },
  {
    "text": "And I should mention the\ndark blue color here, that's the higher utility. The green and yellow\nis the lower utility.",
    "start": "963430",
    "end": "971502"
  },
  {
    "text": "So we step to that\nbottom point now, and then we just repeat again. So we look which one of\nthese four candidate points",
    "start": "971502",
    "end": "978269"
  },
  {
    "text": "has the highest utility. Well, it's the lowest one. So we're going to step\nto the lowest one. Again, we repeat this process.",
    "start": "978270",
    "end": "985470"
  },
  {
    "text": "Again it's that lowest point\nthere so we move down there. But now notice when we're\nhere, the best performing point",
    "start": "985470",
    "end": "994080"
  },
  {
    "text": "is this one. And it might be hard to\nsee that color gradient, but it is this point here. So none of the candidate\npoints we're evaluating",
    "start": "994080",
    "end": "1002180"
  },
  {
    "text": "are better than the point\nthat we're currently at. So what do we do now? Well, we're going to shrink.",
    "start": "1002180",
    "end": "1008190"
  },
  {
    "text": "Now, we shrink our step size. So we bring our\nstep size in and now we're not stepping as far\nin each candidate direction.",
    "start": "1008190",
    "end": "1014779"
  },
  {
    "text": "So now we evaluate. Does anyone improve? No so we're going\nto shrink again.",
    "start": "1014780",
    "end": "1019839"
  },
  {
    "text": "And we just continue\nshrinking down until we get a better\ncandidate point.",
    "start": "1019840",
    "end": "1024980"
  },
  {
    "text": "So in this case,\nwe've shrunk down. And now actually the one on the\nright is the better performer, and so we step there.",
    "start": "1024980",
    "end": "1030549"
  },
  {
    "text": "And you just repeat this\nprocess over and over. Until you reach some\nthreshold where your step size",
    "start": "1030550",
    "end": "1035829"
  },
  {
    "text": "has dropped between. And we call that our\nconvergence point. So that's our local\nsearch method.",
    "start": "1035829",
    "end": "1042400"
  },
  {
    "text": "Is there always a relationship\none parameter being updated? Or are there cases\nwhere multiple are updated at the same time?",
    "start": "1042400",
    "end": "1048790"
  },
  {
    "text": "Because I guess here it's\nonly one being updated in. Right. Right. Yes, good question. So yeah, because we're\nstepping in that plus or minus",
    "start": "1048790",
    "end": "1057670"
  },
  {
    "text": "for each of our candidate\nparameters, right? We'll end up taking just\nthe one candidate point that",
    "start": "1057670",
    "end": "1063700"
  },
  {
    "text": "was the best performer. So it was only going\nto be that changing one of those parameters.",
    "start": "1063700",
    "end": "1069190"
  },
  {
    "text": "Yep? Showing the utility\nor the loss in time? In the diagram we're\nshowing the utility.",
    "start": "1069190",
    "end": "1075440"
  },
  {
    "text": "So the blue is the higher-- OK. Yeah, the better value.",
    "start": "1075440",
    "end": "1081840"
  },
  {
    "text": "So some of the issues with this. This seems like it's\nreasonable, right? But this was a case where\nwe only had two parameters.",
    "start": "1081840",
    "end": "1089010"
  },
  {
    "text": "So if we have billions\nof parameters, this is going to be not\nvery feasible, right?",
    "start": "1089010",
    "end": "1094500"
  },
  {
    "text": "Because for each of\nthese candidate points, we have to do a set. A new set of m rollouts.",
    "start": "1094500",
    "end": "1100140"
  },
  {
    "text": "So that's just going to take\nforever if our parameter set is very large. And then the other\nfactor here is",
    "start": "1100140",
    "end": "1107690"
  },
  {
    "text": "that we have to set the step\nsize right to begin with. We have to choose that\ninitial step size.",
    "start": "1107690",
    "end": "1113039"
  },
  {
    "text": "So that could be\na tuning parameter due to whatever you think\nyour utility space looks like.",
    "start": "1113040",
    "end": "1118980"
  },
  {
    "text": "So those are some of the\ndrawbacks of this local search method. ",
    "start": "1118980",
    "end": "1125270"
  },
  {
    "text": "So the next method that\nwe're going to talk about is the cross entropy method. And this is very similar\nidea to local search,",
    "start": "1125270",
    "end": "1132500"
  },
  {
    "text": "but goes about it in\nslightly different way. OK. So what we're\nworking with here now",
    "start": "1132500",
    "end": "1138860"
  },
  {
    "text": "with the cross entropy method\nis a search distribution.",
    "start": "1138860",
    "end": "1147240"
  },
  {
    "text": "So before we were just\ntaking those fixed step size. Now we're introducing this\nidea of a search distribution.",
    "start": "1147240",
    "end": "1153230"
  },
  {
    "text": "And our search distribution\nwe write as P theta of psi. So psi here is our search\ndistribution parameters.",
    "start": "1153230",
    "end": "1162450"
  },
  {
    "text": "So don't get confused here\nbecause theta is our policy parameters, and psi are our\nsearch distribution parameters.",
    "start": "1162450",
    "end": "1170070"
  },
  {
    "text": "So what do we mean by that? A common choice for\nour search distribution would just be a\nGaussian distribution.",
    "start": "1170070",
    "end": "1175799"
  },
  {
    "text": "So if we're using a\nGaussian distribution, all psi is just the\nmean and covariance",
    "start": "1175800",
    "end": "1180980"
  },
  {
    "text": "of our Gaussian distribution. So that's what psi is. We still-- what we\nreally care about,",
    "start": "1180980",
    "end": "1186080"
  },
  {
    "text": "right, is our theta parameters. That's what we're\ntrying to change here. Our search distribution\nis just now",
    "start": "1186080",
    "end": "1191240"
  },
  {
    "text": "how we're going to be moving\naround the policy space. So we have this\nsearch distribution.",
    "start": "1191240",
    "end": "1197130"
  },
  {
    "text": "We have our search\ndistribution parameters. For now, let's say we're working\nwith a Gaussian distribution.",
    "start": "1197130",
    "end": "1202440"
  },
  {
    "text": " So I think it helps\nto see this visually",
    "start": "1202440",
    "end": "1207629"
  },
  {
    "text": "to understand what's going on. So with our search\ndistribution here,",
    "start": "1207630",
    "end": "1213840"
  },
  {
    "text": "what we're showing these\nwhite contour lines, that's our search distribution.",
    "start": "1213840",
    "end": "1219399"
  },
  {
    "text": "So that's our\nGaussian distribution that we're showing with\nthe contour lines there.",
    "start": "1219400",
    "end": "1225000"
  },
  {
    "text": "What we're going to do\nnow is-- so for now, ignore these the\nred points, just treat them as if they\nwere all white points.",
    "start": "1225000",
    "end": "1231730"
  },
  {
    "text": "So what we do, how the\ncross-entropy method works is we start by taking m\nsamples of theta",
    "start": "1231730",
    "end": "1238920"
  },
  {
    "text": "from this search distribution. So from this\ninitial distribution that we have shown by\nthese contour lines,",
    "start": "1238920",
    "end": "1244860"
  },
  {
    "text": "we take m samples. So those are all these\nwhite and red points. Treat them as if they\nwere all white right now.",
    "start": "1244860",
    "end": "1250120"
  },
  {
    "text": "OK, so we take those m samples. Those give us our\ncandidate points. So those are our\nstarting candidate points",
    "start": "1250120",
    "end": "1255840"
  },
  {
    "text": "that we want to evaluate. Very similar to\nlocal search, we now",
    "start": "1255840",
    "end": "1261090"
  },
  {
    "text": "for each of those\ncandidate points, we do our m set\nof policy rollouts to get some estimate\nof the utility for each",
    "start": "1261090",
    "end": "1268080"
  },
  {
    "text": "of those points. And then what we're going to\ndo, we're going to look at, OK, which of those candidate\npoints are our best performers?",
    "start": "1268080",
    "end": "1276280"
  },
  {
    "text": "We call these our elite samples. So we specify some\nnumber of elite samples.",
    "start": "1276280",
    "end": "1281710"
  },
  {
    "text": "We can say, let's take the\ntop five performing samples from our candidates here.",
    "start": "1281710",
    "end": "1287460"
  },
  {
    "text": "And those become-- those\nare these red points here that you're seeing. Those are our elite samples.",
    "start": "1287460",
    "end": "1292530"
  },
  {
    "text": "So we take our elite\nsamples and now we just refit our\nsearch distribution. So we refit our\nsearch distribution",
    "start": "1292530",
    "end": "1298889"
  },
  {
    "text": "to these elite performers. And then we just\nrepeat this process.",
    "start": "1298890",
    "end": "1305200"
  },
  {
    "text": "OK, so what does that look\nlike if we go through it? Well, we have those\nelite samples there.",
    "start": "1305200",
    "end": "1312640"
  },
  {
    "text": "So now we're just going to\nrefit our distribution to it. So we get that new\ndistribution shown there.",
    "start": "1312640",
    "end": "1318130"
  },
  {
    "text": "And then we just\nrepeat this process. So from our new distribution\nthat we're showing here, we sample a new set of m points.",
    "start": "1318130",
    "end": "1325059"
  },
  {
    "text": "That's what we're showing. And then we evaluate which\nones are the elite samples. So what are our best\nperforming points.",
    "start": "1325060",
    "end": "1331059"
  },
  {
    "text": "Those are the red points. And then we refit\nto those red points. Then we repeat\nthis process again.",
    "start": "1331060",
    "end": "1337320"
  },
  {
    "text": "Where's our best\nperforming points? Well, they're sort of\ndown at the bottom. So then we refit to those.",
    "start": "1337320",
    "end": "1343200"
  },
  {
    "text": "And we just continue doing\nthis until our distribution sort of converges, and we're not\nreally making that many changes",
    "start": "1343200",
    "end": "1349680"
  },
  {
    "text": "anymore. So how does this improve\nupon local search.",
    "start": "1349680",
    "end": "1355020"
  },
  {
    "text": "Well, we saw that with local\nsearch, we're doing this 2 times n number of candidate\npoints every single time.",
    "start": "1355020",
    "end": "1362919"
  },
  {
    "text": "And we're only changing\neach one parameters for each of those points.",
    "start": "1362920",
    "end": "1369370"
  },
  {
    "text": "So the cross entropy method\nis a little more efficient with how it goes about\ndistributing the samples",
    "start": "1369370",
    "end": "1376230"
  },
  {
    "text": "that we're looking at. In addition to that, we\nspecify the number of samples. So we could still have\nbillions of parameters,",
    "start": "1376230",
    "end": "1383490"
  },
  {
    "text": "but we're only going to\nbe looking m candidate points that we sample from\nthat search distribution. So it sort of helps us scale\nthe problem a little bit better.",
    "start": "1383490",
    "end": "1392290"
  },
  {
    "text": "You still have to do\na new set of rollouts for each of these points\nthat you're evaluating. But we've sort of fixed one of\nthe problems with local search.",
    "start": "1392290",
    "end": "1402080"
  },
  {
    "text": "Question. Do you do one rollout\nfor point or do you do m rollouts per point?",
    "start": "1402080",
    "end": "1407600"
  },
  {
    "text": "Good question. The question was, do you do one\nrollout for each candidate point or do you do m rollouts?",
    "start": "1407600",
    "end": "1413040"
  },
  {
    "text": "So you do m rollouts. For every single\npoint that you have. You're doing m policy rollouts.",
    "start": "1413040",
    "end": "1419809"
  },
  {
    "text": "Yes. That's a different m\nthan the number of points that you're sampling. Yes. Yeah. Thank you.",
    "start": "1419810",
    "end": "1424880"
  },
  {
    "text": "Thank you for that, Robert. Yes so we do have two ms here. The m that I wrote up\nhere for the policy",
    "start": "1424880",
    "end": "1431180"
  },
  {
    "text": "rollouts different from\nthis m samples here. So sort of overloading the\nm, they're different ms. Yes.",
    "start": "1431180",
    "end": "1440830"
  },
  {
    "text": "How do you decide how many\npoints to take for your m elite?",
    "start": "1440830",
    "end": "1446080"
  },
  {
    "text": "Yeah, good question. So the question was, how do you\ndecide how many points to take for your m elite samples.",
    "start": "1446080",
    "end": "1453070"
  },
  {
    "text": "That would really just be\neither your domain knowledge or sort of trial and error. Sort of if you have\na lot of parameters,",
    "start": "1453070",
    "end": "1460090"
  },
  {
    "text": "right, you should probably\ntake more elite samples to get a better sense there. And also, like if you're only\ntaking one elite sample, right,",
    "start": "1460090",
    "end": "1467687"
  },
  {
    "text": "that would not be\nso great because you would converge very quickly. So you sort of want to--",
    "start": "1467687",
    "end": "1473169"
  },
  {
    "text": "it's sort of that trade\noff of taking too many and not taking enough. So it's sort of a\ntrial and error there.",
    "start": "1473170",
    "end": "1478679"
  },
  {
    "text": " OK, great.",
    "start": "1478680",
    "end": "1483830"
  },
  {
    "text": "So just wrapping up, sort\nof summarizing what we saw with policy search, right? We started with the\npolicy rollouts,",
    "start": "1483830",
    "end": "1491510"
  },
  {
    "text": "and we saw that we used\nthose policy rollouts to get us an estimate of the\nutility of a particular policy.",
    "start": "1491510",
    "end": "1498410"
  },
  {
    "text": "With that estimate, we then\nuse that in local search to do this sort of fixed\nstep size in each parameter",
    "start": "1498410",
    "end": "1504620"
  },
  {
    "text": "direction, and then do those\nrollouts to evaluate the policy in each of those directions.",
    "start": "1504620",
    "end": "1510440"
  },
  {
    "text": "And then what we just\nsaw was cross entropy. And that sort of\nchanges on local search where now we're using this\nsearch distribution instead.",
    "start": "1510440",
    "end": "1518700"
  },
  {
    "text": "OK. So-- yes. The point of the question\nwas bunch of sample points because that seemed very\ncomputationally expensive.",
    "start": "1518700",
    "end": "1526040"
  },
  {
    "text": "Yes, that is a\ngreat, great point. So the question was, if we have\nto do a new set of m rollouts",
    "start": "1526040",
    "end": "1532309"
  },
  {
    "text": "for each one of\nthese sample points, isn't that very\ncomputationally expensive? And the answer is in\nthe general sense, yes.",
    "start": "1532310",
    "end": "1540090"
  },
  {
    "text": "That's pretty inefficient\nto do all of those rollouts every single time we're looking\nat a new candidate point.",
    "start": "1540090",
    "end": "1546910"
  },
  {
    "text": "And so that's where\nwe're going now is how can we avoid having to\ndo this new set of rollouts",
    "start": "1546910",
    "end": "1553590"
  },
  {
    "text": "every time we want\nto make a change. And so the whole focus now is\nhow can we get more efficient",
    "start": "1553590",
    "end": "1559289"
  },
  {
    "text": "in these evaluations. Yes. When you say Monte Carlo policy\nevaluation, when you sample",
    "start": "1559290",
    "end": "1565050"
  },
  {
    "text": "the initial states\nuse for rollouts, do you do the whole Monte\nCarlo business of if we saw",
    "start": "1565050",
    "end": "1570120"
  },
  {
    "text": "this a lot, we don't select it? If we saw this a lot,\nwe don't select it?",
    "start": "1570120",
    "end": "1575299"
  },
  {
    "text": "Last time we mentioned\nthat, like with like, for instance, with\nMonte Carlo tree search,",
    "start": "1575300",
    "end": "1580360"
  },
  {
    "text": "we would keep track of\nhow many like start states you ended up at. And you say, we don't\nwant to go to states",
    "start": "1580360",
    "end": "1585490"
  },
  {
    "text": "that we ended up at too much. Yeah, I see. I see what you're saying. Yeah, so this is different\nfrom Monte Carlo tree search.",
    "start": "1585490",
    "end": "1591320"
  },
  {
    "text": "What we mean by the Monte\nCarlo estimates here is just where we sample\nfrom our distribution.",
    "start": "1591320",
    "end": "1597070"
  },
  {
    "text": "And then we do this-- this is our Monte\nCarlo estimate here where we've just we're\napproximating the integral",
    "start": "1597070",
    "end": "1604120"
  },
  {
    "text": "over there, this expectation. We're approximating that with\nthe samples of the trajectories that we've taken.",
    "start": "1604120",
    "end": "1609710"
  },
  {
    "text": "Yeah, good question. So different from Monte\nCarlo tree search. Yes.",
    "start": "1609710",
    "end": "1615190"
  },
  {
    "text": "We're choosing the\nspecified depth? Yes. Yeah. So we set the depth\nthat we want to go to.",
    "start": "1615190",
    "end": "1621070"
  },
  {
    "text": "So we could say we want to\ntake, you know, 10 actions. So our depth would be 10. And we're going\nto do that rollout",
    "start": "1621070",
    "end": "1626710"
  },
  {
    "text": "for 10 different actions. In each of those 10\nstates we end up in,",
    "start": "1626710",
    "end": "1631990"
  },
  {
    "text": "we'll multiply by\nthe discount factor depending on how\nfar along it is? Yes. Yep.",
    "start": "1631990",
    "end": "1637789"
  },
  {
    "text": "Yeah, so the question was for\nthat depth that we specify, we're looking at the\ndiscounted reward",
    "start": "1637790",
    "end": "1644690"
  },
  {
    "text": "essentially along\nwhat we receive.  OK.",
    "start": "1644690",
    "end": "1649980"
  },
  {
    "text": "So we're going to zoom out\nagain to the big picture, right? We just looked at policy search.",
    "start": "1649980",
    "end": "1656650"
  },
  {
    "text": "The main idea that\nwe're working with. Right, is that we have\nthis parameterized policy. So we have some\nset of parameters",
    "start": "1656650",
    "end": "1663750"
  },
  {
    "text": "and that's how we're sort\nof representing our policy. And we're looking at estimating\nthe utility of that policy.",
    "start": "1663750",
    "end": "1670480"
  },
  {
    "text": "So with this sort of\nframework in mind. And some of the maybe\ndrawbacks that we've",
    "start": "1670480",
    "end": "1675779"
  },
  {
    "text": "seen of policy search,\nwhere are we headed now? So what we're going\nto look at now",
    "start": "1675780",
    "end": "1681180"
  },
  {
    "text": "is policy gradient estimation. So none of these methods that\nwe just saw with local search",
    "start": "1681180",
    "end": "1687780"
  },
  {
    "text": "that none of those use\nthe gradient information of our policy. And so we can get\nmore information out",
    "start": "1687780",
    "end": "1694620"
  },
  {
    "text": "of how to improve our\npolicy if we do look at the gradient of our policy. But why would we want to\nestimate the gradient?",
    "start": "1694620",
    "end": "1701350"
  },
  {
    "text": "Well, the reason is that we want\nto use that for optimization. So we have our\npolicy pi of theta.",
    "start": "1701350",
    "end": "1709340"
  },
  {
    "text": "We know we can estimate\nthe utility of that policy. Now we want to\nlook at how can we",
    "start": "1709340",
    "end": "1714519"
  },
  {
    "text": "use gradient information of\nthis parameterized policy to improve our policy.",
    "start": "1714520",
    "end": "1721090"
  },
  {
    "text": "So when we're talking\nabout gradients, I think the most natural\nplace that we would start",
    "start": "1721090",
    "end": "1726370"
  },
  {
    "text": "is with looking at a\nfinite difference method. And so that's where\nwe're going to start with gradient estimation here.",
    "start": "1726370",
    "end": "1734100"
  },
  {
    "text": " So for finite\ndifference, what we're",
    "start": "1734100",
    "end": "1740740"
  },
  {
    "text": "going to start with\nis just looking at the definition of\nthe gradient, right? So what we want to get\nto here is an expression",
    "start": "1740740",
    "end": "1749020"
  },
  {
    "text": "for the gradient with respect\nto theta of U of theta.",
    "start": "1749020",
    "end": "1754240"
  },
  {
    "text": "So just from the\ndefinition of the gradient, this will be the\npartial derivative of U",
    "start": "1754240",
    "end": "1760270"
  },
  {
    "text": "with respect to theta 1. And then that will\nbe-- we'll just",
    "start": "1760270",
    "end": "1765340"
  },
  {
    "text": "do that for each of our\nn parameters, right. So then with respect to theta n.",
    "start": "1765340",
    "end": "1772390"
  },
  {
    "text": "OK, so that's just\ndefinition of the gradient that we're working with. What the finite\ndifference method does",
    "start": "1772390",
    "end": "1778720"
  },
  {
    "text": "is basically just let's\napproximate this gradient. So what we're\ngoing to do here is",
    "start": "1778720",
    "end": "1784270"
  },
  {
    "text": "say this is approximately\nequal to U of theta plus delta",
    "start": "1784270",
    "end": "1791350"
  },
  {
    "text": "times this basis vector. We'll talk about what\nthis basis vector is.",
    "start": "1791350",
    "end": "1796690"
  },
  {
    "text": "Minus U of theta over delta. So we do that for\nour first parameter.",
    "start": "1796690",
    "end": "1804830"
  },
  {
    "text": "And remember, just from the\ndefinition of a derivative, right, with a\nderivative, we're just",
    "start": "1804830",
    "end": "1810399"
  },
  {
    "text": "looking at the limit of this\nexpression as delta goes to 0. So with our finite difference,\nwe're just saying, OK,",
    "start": "1810400",
    "end": "1816010"
  },
  {
    "text": "we'll take a small enough\ndelta and sort of call this good enough. OK. That's our finite\ndifference approximation.",
    "start": "1816010",
    "end": "1822370"
  },
  {
    "text": "So we then repeat this for\neach of our parameters. So this will then be U of theta\nplus delta the n-th basis vector",
    "start": "1822370",
    "end": "1833050"
  },
  {
    "text": "minus U of theta over delta.",
    "start": "1833050",
    "end": "1838780"
  },
  {
    "text": "OK, so what's that basis vector. Well, so e1, it's very simple. It's just telling us which\nindices of this vector is not 0.",
    "start": "1838780",
    "end": "1847640"
  },
  {
    "text": "So e1 will just look\nlike this, where we have one in the first index\nand then zeros all the way down.",
    "start": "1847640",
    "end": "1854710"
  },
  {
    "text": "And then similarly, en, the n-th\nbasis, vector is just zeros. And then the last\ncomponent is 1.",
    "start": "1854710",
    "end": "1862270"
  },
  {
    "text": "So that's all the\nbasis vector is. And so what we're doing here is\nwe're just looking at our theta",
    "start": "1862270",
    "end": "1871690"
  },
  {
    "text": "vector, right? We have our theta 1\nto theta n, and all",
    "start": "1871690",
    "end": "1877480"
  },
  {
    "text": "I'm saying in that\nfirst one is let me add some small delta to\nmy theta, my first component",
    "start": "1877480",
    "end": "1883450"
  },
  {
    "text": "here, only the first component. Let me add some small delta. Let me evaluate\nthe utility there.",
    "start": "1883450",
    "end": "1889790"
  },
  {
    "text": "And look at the difference. And that's all we're doing to\nestimate that partial derivative",
    "start": "1889790",
    "end": "1895000"
  },
  {
    "text": "of the first term. We then do that for each\nof our n parameters. And so for each one\nof these, notice",
    "start": "1895000",
    "end": "1903340"
  },
  {
    "text": "that what we have to do\nto compute this, right, we have to do a new set\nof m policy rollouts.",
    "start": "1903340",
    "end": "1909860"
  },
  {
    "text": "So first, we have to do a\nset of m policy rollouts to estimate this utility.",
    "start": "1909860",
    "end": "1915730"
  },
  {
    "text": "Then when we change\njust that theta one, we have to do a new\nset of m rollouts.",
    "start": "1915730",
    "end": "1920899"
  },
  {
    "text": "And then for every\nsingle one of these, we have to do another\nnew set of m rollouts. So again, that's\na lot of rollouts",
    "start": "1920900",
    "end": "1927879"
  },
  {
    "text": "that we're having to do. So that's one of the drawbacks. The other is that sort\nof local search, right,",
    "start": "1927880",
    "end": "1935150"
  },
  {
    "text": "we're only changing one\nof these thetas each time we're doing this. So not super robust\nin that estimate",
    "start": "1935150",
    "end": "1943030"
  },
  {
    "text": "that we're going to be getting. So an extension of the\nfinite difference method",
    "start": "1943030",
    "end": "1949780"
  },
  {
    "text": "is just looking at\nbasically same thing, but how can we do this a\nlittle bit more smartly?",
    "start": "1949780",
    "end": "1956290"
  },
  {
    "text": "So the way we're going\nto do that is just with linear regression. So this is called the\nregression gradient.",
    "start": "1956290",
    "end": "1963679"
  },
  {
    "text": "But what it comes down to is\njust doing linear regression to estimate this gradient.",
    "start": "1963680",
    "end": "1970059"
  },
  {
    "text": "So we're going to introduce\nthis delta theta matrix here. And what this delta\ntheta is, is basically",
    "start": "1970060",
    "end": "1978820"
  },
  {
    "text": "we're just going to have\nthis delta theta 1 here.",
    "start": "1978820",
    "end": "1984255"
  },
  {
    "text": "And we're going\nto transpose that and we're going to\nbreak this down here, but we're going to\ndo this delta theta, so this perturbation\nhere is what",
    "start": "1984255",
    "end": "1990310"
  },
  {
    "text": "you can think about that as. We're going to do that\nm different times. So there's a lot of ms here, so\ndon't get confused with the ms.",
    "start": "1990310",
    "end": "1998919"
  },
  {
    "text": "So we have m different\nperturbations here. So looking again at\nour theta vector,",
    "start": "1998920",
    "end": "2009450"
  },
  {
    "text": "one of these delta thetas,\nso this first delta theta, what we do is we go and we just\nrandomly perturb each component",
    "start": "2009450",
    "end": "2017340"
  },
  {
    "text": "of our theta vector. So for theta one, we add some\nrandom perturbation, theta two, add some random perturbation.",
    "start": "2017340",
    "end": "2023320"
  },
  {
    "text": "We do that all the way\ndown to our theta n, just adding random perturbation\nto each one of those. We then take that\ndelta theta, which",
    "start": "2023320",
    "end": "2030060"
  },
  {
    "text": "is just a random\nperturbation, and we just lay it flat in this matrix here. OK, so we have n components\nthat we've just laid flat.",
    "start": "2030060",
    "end": "2038010"
  },
  {
    "text": "We're doing that random\nperturbation m different times. OK, so the dimensions of\nthis delta theta are m by n.",
    "start": "2038010",
    "end": "2046440"
  },
  {
    "text": "So just random perturbation\nthat we've laid flat and we're doing that random\nperturbation m different times.",
    "start": "2046440",
    "end": "2053129"
  },
  {
    "text": "Yes. Question. From the last change\nvalue of theta?",
    "start": "2053130",
    "end": "2059489"
  },
  {
    "text": "Yes. Yes. So the, the delta theta here. So we start with--\nso the question was,",
    "start": "2059489",
    "end": "2065699"
  },
  {
    "text": "is the random perturbation\nfrom the initial theta that you were starting with?",
    "start": "2065699",
    "end": "2070960"
  },
  {
    "text": "And yes, that's correct. So we have some initial theta. We then look at perturbing that. So just adding some random\nperturbation to that,",
    "start": "2070960",
    "end": "2078119"
  },
  {
    "text": "and then that becomes\nare delta theta here. So similarly, we can write\nout the delta U now using",
    "start": "2078120",
    "end": "2086489"
  },
  {
    "text": "this delta theta. And that is just very\nsimilar to what we have with the finite difference.",
    "start": "2086489",
    "end": "2093129"
  },
  {
    "text": "That's just U of theta. So whatever initial theta we\nwere working with plus delta",
    "start": "2093130",
    "end": "2099150"
  },
  {
    "text": "theta 1 minus U of theta.",
    "start": "2099150",
    "end": "2104160"
  },
  {
    "text": "And then we're going to do that\nagain m different times now because we have m\ndelta thetas here.",
    "start": "2104160",
    "end": "2110350"
  },
  {
    "text": "So this will be U of theta plus\ndelta theta m minus U of theta.",
    "start": "2110350",
    "end": "2117530"
  },
  {
    "text": " OK. So that becomes our delta u.",
    "start": "2117530",
    "end": "2123329"
  },
  {
    "text": "And now what you can think\nabout in the scalar case, right. If we want to look at the\nslope of a line, right?",
    "start": "2123330",
    "end": "2130850"
  },
  {
    "text": "What we would say\nfor the scalar case is, well, that's just\ndelta u over delta theta.",
    "start": "2130850",
    "end": "2136579"
  },
  {
    "text": "Just the rise over\nrun relationship. OK. That's not what we're doing here\nbecause these are vectors here.",
    "start": "2136580",
    "end": "2142760"
  },
  {
    "text": "So this doesn't make any sense. But just sort of\nmapping that back to give you that intuition\nof in the scalar case,",
    "start": "2142760",
    "end": "2148240"
  },
  {
    "text": "if we wanted to approximate\nthis slope of a line, we would just do delta\nu over delta theta.",
    "start": "2148240",
    "end": "2154000"
  },
  {
    "text": "So we're going to do\na very similar thing along that similar\nidea just using linear regression, because\nwe have our delta u,",
    "start": "2154000",
    "end": "2161320"
  },
  {
    "text": "we have are delta theta. What we want is our gradient\nwith respect to U of theta.",
    "start": "2161320",
    "end": "2166580"
  },
  {
    "text": "So we're just going to-- now\nthis is just a linear regression problem that we can set up here.",
    "start": "2166580",
    "end": "2172580"
  },
  {
    "text": "So the solution to\nthis is just going to be the gradient\nwith respect to theta, well, that's approximately\nequal to delta theta,",
    "start": "2172580",
    "end": "2182559"
  },
  {
    "text": "the pseudo inverse\nof delta theta. So we're going to\ntalk about this. And then times our\ndelta U. So we just",
    "start": "2182560",
    "end": "2190569"
  },
  {
    "text": "want to check the\ndimensions here to make sure this\nmakes sense, right? So our delta U, well, that\njust has m components, right.",
    "start": "2190570",
    "end": "2196640"
  },
  {
    "text": "So this is m by 1. Delta theta was m by n, but we\ntook the pseudo inverse of it.",
    "start": "2196640",
    "end": "2204079"
  },
  {
    "text": "So that gives us n by m now. And so when we multiply\nthose two together,",
    "start": "2204080",
    "end": "2209380"
  },
  {
    "text": "we're left with n\nby 1, and we know that's what our\ngradient should be.",
    "start": "2209380",
    "end": "2214660"
  },
  {
    "text": "OK, so what is this pseudo\ninverse of delta theta? Well, the pseudo\ninverse just gives us",
    "start": "2214660",
    "end": "2221290"
  },
  {
    "text": "the least squares estimator for\nour linear regression problem. So the pseudo inverse is just\nthe general form of an inverse.",
    "start": "2221290",
    "end": "2230180"
  },
  {
    "text": "When our matrix delta theta\nhere might not necessarily be invertible. So if we actually wanted\nto go and write out",
    "start": "2230180",
    "end": "2236440"
  },
  {
    "text": "the expression for delta\ntheta pseudo inverse, that would involve taking the\nsingular value decomposition.",
    "start": "2236440",
    "end": "2242510"
  },
  {
    "text": "But in Python or Julia\nthere's just a built in p inv. So it's just like this.",
    "start": "2242510",
    "end": "2248155"
  },
  {
    "text": " P inv function\nthat you can call,",
    "start": "2248155",
    "end": "2253349"
  },
  {
    "text": "and that will give you\nthe pseudo inverse. So it will do that singular\nvalue decomposition for you. OK, so it's just\nthe general form",
    "start": "2253350",
    "end": "2259200"
  },
  {
    "text": "of getting this least\nsquares estimate here between our delta\nthetas and delta Us.",
    "start": "2259200",
    "end": "2266220"
  },
  {
    "text": "So how does this help us? How is this better than what we\nsaw with the finite difference",
    "start": "2266220",
    "end": "2271559"
  },
  {
    "text": "method? Well, for one, and\nI'd say the main thing is that now you're looking\nat this delta theta.",
    "start": "2271560",
    "end": "2278890"
  },
  {
    "text": "So you're looking\nat this perturbation among all of your n parameters,\nnot just one of them at a time.",
    "start": "2278890",
    "end": "2285970"
  },
  {
    "text": "And then you're repeating\nthat m different times and each time you're\nrepeating that you're doing",
    "start": "2285970",
    "end": "2292890"
  },
  {
    "text": "a new set of rollouts here. So we still have\nthat problem where you have to do a new set of\nrollouts for each of these.",
    "start": "2292890",
    "end": "2298110"
  },
  {
    "text": "But this gives us a little\nsmoother estimate, right. It's a little more\nrobust because we're",
    "start": "2298110",
    "end": "2303240"
  },
  {
    "text": "looking at the net effect of\nall of these small changes in our theta vector. Question.",
    "start": "2303240",
    "end": "2309750"
  },
  {
    "text": "Can you motivate taking a break\nagain, starting from the fact that we have delta u, and\nhow does low rank end up",
    "start": "2309750",
    "end": "2317099"
  },
  {
    "text": "at the estimate? Good question. So the question was,\nhow do we get here using",
    "start": "2317100",
    "end": "2323160"
  },
  {
    "text": "this linear regression idea. So I think one way\nto think about it is that before getting\nto here, right,",
    "start": "2323160",
    "end": "2330630"
  },
  {
    "text": "we would have this idea of\ndelta u would be equal to-- should be delta theta times,\nthis gradient estimate.",
    "start": "2330630",
    "end": "2339580"
  },
  {
    "text": "And maybe the best way to\nthink about this, at least how I like to think\nabout it, is if you",
    "start": "2339580",
    "end": "2345330"
  },
  {
    "text": "think about just like looking at\nthe slope of a line here, right.",
    "start": "2345330",
    "end": "2350440"
  },
  {
    "text": "So where this would be our U\nand this would be our theta. So when we're looking\nat the slope of this,",
    "start": "2350440",
    "end": "2356590"
  },
  {
    "text": "right, we're just saying-- can everyone see this or\nis the podium blocking it?",
    "start": "2356590",
    "end": "2363150"
  },
  {
    "text": "Everyone-- OK. All right. So when we're looking\nat the slope here, we can just say, well,\nthis is delta u, right?",
    "start": "2363150",
    "end": "2370640"
  },
  {
    "text": "This is delta theta. So we're looking at that. And that's what I\nwas saying before, is like, delta U\nover delta theta",
    "start": "2370640",
    "end": "2377590"
  },
  {
    "text": "is what we're interested in. So when we get\nthis here, this is sort of like, right,\nit's not at all exact",
    "start": "2377590",
    "end": "2383560"
  },
  {
    "text": "because we're working with\nvectors and matrices here. So this is like delta U over\ndelta theta with this inverse.",
    "start": "2383560",
    "end": "2389770"
  },
  {
    "text": "Not equivalent at all,\nbut just sort of that's the idea to have in mind. Yes.",
    "start": "2389770",
    "end": "2395530"
  },
  {
    "text": "Why is delta theta not\ninvertible in the general case? Good question. So the question was,\nwhy is delta theta",
    "start": "2395530",
    "end": "2401710"
  },
  {
    "text": "not invertible in\nthe general case? Well, for one, if you\nhave m different from n,",
    "start": "2401710",
    "end": "2408520"
  },
  {
    "text": "then you could have sort of what\nwe call a short matrix, right? And so that wouldn't that\nwouldn't be an invertible",
    "start": "2408520",
    "end": "2415630"
  },
  {
    "text": "because it's not\nsquare to begin with. But you could also have issues\nwith the rank of the delta theta",
    "start": "2415630",
    "end": "2421000"
  },
  {
    "text": "matrix since you're doing\nthese random perturbations, it's not necessarily\nguaranteed to be invertible.",
    "start": "2421000",
    "end": "2427820"
  },
  {
    "text": "Yes. Can't the m by n\nmatrix just to make sure m is the\nnumber of rollouts?",
    "start": "2427820",
    "end": "2433190"
  },
  {
    "text": "This matrix here? Yeah. Yes. So the question was,\nwhat is n, what is m?",
    "start": "2433190",
    "end": "2438480"
  },
  {
    "text": "N here is the\nnumber of parameters that we have in our\ntheta, theta vector.",
    "start": "2438480",
    "end": "2444660"
  },
  {
    "text": "And then m is just the number\nof these random perturbations that we're doing.",
    "start": "2444660",
    "end": "2450290"
  },
  {
    "text": "But then we also like\nmentioned, we have m rollouts that we're doing as well,\nbut those are different ms.",
    "start": "2450290",
    "end": "2456420"
  },
  {
    "text": "So we're using a lot of ms here. Yep.",
    "start": "2456420",
    "end": "2461840"
  },
  {
    "text": "Yes. And just to be clear,\nall the perturbations are just completely random\nfor each of those m rows?",
    "start": "2461840",
    "end": "2469290"
  },
  {
    "text": "Yep. Yeah. So the question was, are all of\nthese perturbations just random? Yeah. So you could use some--",
    "start": "2469290",
    "end": "2476030"
  },
  {
    "text": "how you-- like what\ntype of random noise you use that sort of up to you. But yeah, it's just if we\nuse just some Gaussian noise,",
    "start": "2476030",
    "end": "2483110"
  },
  {
    "text": "we could just add that Gaussian\nnoise to each component. ",
    "start": "2483110",
    "end": "2490020"
  },
  {
    "text": "OK, so that's the\nregression gradient. ",
    "start": "2490020",
    "end": "2498950"
  },
  {
    "text": "Now, with that, we\nstill have the issue that we have to do this new set\nof rollouts for each of these,",
    "start": "2498950",
    "end": "2504910"
  },
  {
    "text": "right? And I think you asked\nabout that earlier. Like, isn't this still a lot\nof new roll outs to compute?",
    "start": "2504910",
    "end": "2511550"
  },
  {
    "text": "And the answer is yes,\nthat is still an issue. So where we're\ngoing now is we want",
    "start": "2511550",
    "end": "2518630"
  },
  {
    "text": "to look at just an analytical\nexpression for this gradient. ",
    "start": "2518630",
    "end": "2530380"
  },
  {
    "text": "So if you recall from before,\nwe had this expression",
    "start": "2530380",
    "end": "2536799"
  },
  {
    "text": "for U of theta, where we had\nit as this expectation, which",
    "start": "2536800",
    "end": "2543880"
  },
  {
    "text": "we could write as\nthe probability of that trajectory\noccurring times the reward we receive\nalong that trajectory.",
    "start": "2543880",
    "end": "2551109"
  },
  {
    "text": "So that's where we're\ngoing to start from, right? This is just what we saw\nbefore with this expectation",
    "start": "2551110",
    "end": "2556450"
  },
  {
    "text": "over trajectories. So we're going to\nstart from here, and our goal is to get\nsome analytical expression",
    "start": "2556450",
    "end": "2563530"
  },
  {
    "text": "of this gradient, OK? So no more finite difference\napproximation here. We want to work this out.",
    "start": "2563530",
    "end": "2569740"
  },
  {
    "text": "So we're just going to\ntake the gradient now. We're going to say gradient\nwith respect to U of theta.",
    "start": "2569740",
    "end": "2576790"
  },
  {
    "text": "And so we can just look\nat this integral now. Our integral is\nwith respect to tau, so we can bring this\ngradient with respect",
    "start": "2576790",
    "end": "2583810"
  },
  {
    "text": "to theta inside the integral,\nand we just get this expression. ",
    "start": "2583810",
    "end": "2591880"
  },
  {
    "text": "So nothing too crazy yet, right? And now what we're going to\ndo is just multiply by 1, OK?",
    "start": "2591880",
    "end": "2598750"
  },
  {
    "text": "We're just going to\nsimply multiply by 1. So we have this p pi of\ntau over p pi of tau.",
    "start": "2598750",
    "end": "2607180"
  },
  {
    "text": "And then we still have\nthis expression here. So still gradient with respect\nto theta p pi of tau r of tau.",
    "start": "2607180",
    "end": "2616750"
  },
  {
    "text": "So we might be saying,\nwhy would we do that? Why did we just multiply by 1? That doesn't look too nice of\nan expression to work with.",
    "start": "2616750",
    "end": "2625960"
  },
  {
    "text": "But what we can\nnotice is that this allows us to rewrite some things\nin terms of log probabilities.",
    "start": "2625960",
    "end": "2634990"
  },
  {
    "text": "So the fact that\nwe can notice is if we look at the\ngradient with respect to theta of log p\npi of tau, right?",
    "start": "2634990",
    "end": "2647400"
  },
  {
    "text": "If we take the gradient\nof log of theta, that would give us\n1 over theta, right?",
    "start": "2647400",
    "end": "2653130"
  },
  {
    "text": "So just the derivative\nof log of x is 1 over x. So if we take the gradient\nwith respect to this,",
    "start": "2653130",
    "end": "2658370"
  },
  {
    "text": "and we use the chain\nrule, this would give us 1 over p pi of tau.",
    "start": "2658370",
    "end": "2664400"
  },
  {
    "text": "And then chain rule says take\nthe gradient of the outside term times gradient of the\ninside term, right? So the gradient\nof the inside term",
    "start": "2664400",
    "end": "2670940"
  },
  {
    "text": "is just gradient with\nrespect to theta p pi of tau. OK, so this was\njust the chain rule",
    "start": "2670940",
    "end": "2676970"
  },
  {
    "text": "to get this\nsimplification here, OK? All we did was take the\nchain rule of this term here.",
    "start": "2676970",
    "end": "2683150"
  },
  {
    "text": "But now notice this\ngradient, with respect to theta p pi of tau, shows\nup in this expression.",
    "start": "2683150",
    "end": "2689100"
  },
  {
    "text": "That's why we multiplied by 1. So now we can substitute\nthis back in, OK? We can substitute this\nexpression in here,",
    "start": "2689100",
    "end": "2696830"
  },
  {
    "text": "and we'll bring\nthe log inside, OK? So this expression\nup here becomes",
    "start": "2696830",
    "end": "2706070"
  },
  {
    "text": "p pi of tau gradient\nwith respect to theta log p pi of\ntau R of tau d tau.",
    "start": "2706070",
    "end": "2718820"
  },
  {
    "text": "Now, you might be\nsaying, well, this doesn't look a whole lot better\nthan what we started with. In fact, this, this kind\nof looks worse, right?",
    "start": "2718820",
    "end": "2725850"
  },
  {
    "text": "But I assure you, we are\ngetting somewhere here, OK? So what we can notice here is\nthat this is just an expectation",
    "start": "2725850",
    "end": "2734310"
  },
  {
    "text": "now. We've just sort of\nrewritten the expectation that we started with in a\ndifferent form now using the log",
    "start": "2734310",
    "end": "2739880"
  },
  {
    "text": "probability, OK? So it'll come together,\nI promise you. So we can rewrite this\nnow as the expectation",
    "start": "2739880",
    "end": "2748309"
  },
  {
    "text": "over trajectories of the\ngradient with respect to theta log p pi of tau r of tau.",
    "start": "2748310",
    "end": "2759140"
  },
  {
    "text": "OK. So that's where we're at now. We're getting somewhere, but\nnow we're focused on this term,",
    "start": "2759140",
    "end": "2767355"
  },
  {
    "text": "right? We want to simplify\nthis a little bit because we need to know,\nwhat is this p pi of tau?",
    "start": "2767355",
    "end": "2775000"
  },
  {
    "text": "What's the probability\nof a trajectory occurring when\nfollowing policy pi? OK, so that's what we're going\nto talk about now is, how do we",
    "start": "2775000",
    "end": "2783810"
  },
  {
    "text": "get that probability? ",
    "start": "2783810",
    "end": "2791500"
  },
  {
    "text": "So I'm going to come over\nhere just so you can finish writing that down if you are. So probability of a\ntrajectory, right?",
    "start": "2791500",
    "end": "2798650"
  },
  {
    "text": "Remember that a\ntrajectory is just a sequence of\nstates and actions.",
    "start": "2798650",
    "end": "2804820"
  },
  {
    "text": "So we have state one,\nwe took action a1, and then we can just\ncontinue this out all",
    "start": "2804820",
    "end": "2809920"
  },
  {
    "text": "the way up to the depth,\nso state d, action d.",
    "start": "2809920",
    "end": "2815079"
  },
  {
    "text": "OK, so that's just\nour trajectory, sequence of states and actions. What we're asking now is,\nwhat is the probability",
    "start": "2815080",
    "end": "2821920"
  },
  {
    "text": "of this trajectory\noccurring, OK? And I think a good way\nto think about that is back to the picture we\nhad with the Markov decision",
    "start": "2821920",
    "end": "2830410"
  },
  {
    "text": "process, where we\nhad some state. From that state, we\ntook some action.",
    "start": "2830410",
    "end": "2838180"
  },
  {
    "text": "That action gave us\nsome reward here, and then we transitioned\nto a new state. ",
    "start": "2838180",
    "end": "2847390"
  },
  {
    "text": "So when we're talking\nabout the probability of this trajectory occurring,\nwhat does that look like?",
    "start": "2847390",
    "end": "2853340"
  },
  {
    "text": "Well, we have to start\nsomewhere, right? What's the probability that we\nstart in that initial state?",
    "start": "2853340",
    "end": "2858960"
  },
  {
    "text": "And that's back to the B of\nS term, where that's saying, what's this probability\nof this state occurring?",
    "start": "2858960",
    "end": "2865970"
  },
  {
    "text": "OK, what's the probability that\nwe are starting in this state? So that's our starting point\nfor this probability expression,",
    "start": "2865970",
    "end": "2872589"
  },
  {
    "text": "right? Just what's the probability\nthat we start in that state? Now that we're in state\n1, so we're in state 1,",
    "start": "2872590",
    "end": "2880640"
  },
  {
    "text": "we take action a1,\nwhat's the probability that we transition\nto state 2, OK?",
    "start": "2880640",
    "end": "2886960"
  },
  {
    "text": "So that's just our\ntransition probability, probability that we\ntransition to state 2, given we were in state\n1 and took action a1.",
    "start": "2886960",
    "end": "2894550"
  },
  {
    "text": " But we're not done there. There's one more term,\nand this specific term",
    "start": "2894550",
    "end": "2901799"
  },
  {
    "text": "is when we're working with\nstochastic policies, OK? So a stochastic\npolicy is different",
    "start": "2901800",
    "end": "2908280"
  },
  {
    "text": "from a deterministic policy. So we write our stochastic\npolicy like this. ",
    "start": "2908280",
    "end": "2915750"
  },
  {
    "text": "So that's our\nstochastic policy, where this is a distribution\nover actions, given our current state.",
    "start": "2915750",
    "end": "2922470"
  },
  {
    "text": "That's different from a\ndeterministic policy, which we would write just like this, OK?",
    "start": "2922470",
    "end": "2928560"
  },
  {
    "text": "That would just tell\nus, we're in some state, we're going to take an action. Our stochastic policy\nis a distribution now.",
    "start": "2928560",
    "end": "2935010"
  },
  {
    "text": "We're in some state. I now have a distribution of\nactions that I could take. So I'm not always going\nto take the same action",
    "start": "2935010",
    "end": "2940770"
  },
  {
    "text": "in the same state, OK? So that's what\nwe're, we're saying when we're talking about\nstochastic policies.",
    "start": "2940770",
    "end": "2946980"
  },
  {
    "text": "The reason we're working\nwith stochastic policies in this likelihood ratio\npolicy gradient method",
    "start": "2946980",
    "end": "2952559"
  },
  {
    "text": "will become clearer, but for\nnow, just focus on the fact, this is a distribution\nover actions,",
    "start": "2952560",
    "end": "2959500"
  },
  {
    "text": "which means that we're not just\ndone with this transition term because we also have the\nprobability that we took",
    "start": "2959500",
    "end": "2966460"
  },
  {
    "text": "that action, so the probability\nwe took action a1 in state 1,",
    "start": "2966460",
    "end": "2974020"
  },
  {
    "text": "OK? So we can continue\nthis out now all the way for our\nentire trajectory,",
    "start": "2974020",
    "end": "2979660"
  },
  {
    "text": "but we can just write it a\nlittle simpler with some product notation here, where\nwe have p of s1,",
    "start": "2979660",
    "end": "2987790"
  },
  {
    "text": "then this becomes a\nproduct now, we'll say, from k equals 1 to\nd of the probability",
    "start": "2987790",
    "end": "2996190"
  },
  {
    "text": "that we transition to state k\nplus 1, given we were in state k and took action ak then\ntimes the probability that we",
    "start": "2996190",
    "end": "3006450"
  },
  {
    "text": "took action k in state sk. OK, so that's the expression\nfor this probability term",
    "start": "3006450",
    "end": "3015000"
  },
  {
    "text": "that we had over here. Any questions on how\nwe got that probability",
    "start": "3015000",
    "end": "3020490"
  },
  {
    "text": "or why we're doing this? Yes.",
    "start": "3020490",
    "end": "3026000"
  },
  {
    "text": "What's the letter on\ntop of the product? Yes. The question was,\nwhat's this letter? That is d.",
    "start": "3026000",
    "end": "3031339"
  },
  {
    "text": "So that's the depth\nof our trajectory. Yep. ",
    "start": "3031340",
    "end": "3037700"
  },
  {
    "text": "OK, so from here, it's all\ndownhill from here, all right? We have this expression.",
    "start": "3037700",
    "end": "3043560"
  },
  {
    "text": "We just need to\ntake the log of it and then take the gradient, OK? So those are the next two\nsteps we're going to do.",
    "start": "3043560",
    "end": "3048960"
  },
  {
    "text": "So there's a light at the\nend of the tunnel here. So first, we're just going\nto take the log of this.",
    "start": "3048960",
    "end": "3054500"
  },
  {
    "text": "We know the nice part\nabout logs is this is just one giant product here. So when we take the\nlog of a product,",
    "start": "3054500",
    "end": "3060950"
  },
  {
    "text": "it just becomes the\nsum of logs, right? So that's all we're going to do. We're going to take\nthe log of this.",
    "start": "3060950",
    "end": "3065970"
  },
  {
    "text": "And when we take the log of\nthis, it becomes log p of s1.",
    "start": "3065970",
    "end": "3071330"
  },
  {
    "text": "This product here becomes a sum. So we get the sum from k equals\n1 to d of log of this term,",
    "start": "3071330",
    "end": "3081540"
  },
  {
    "text": "so log p, s k plus 1. Given we were in\nsk, took action ak.",
    "start": "3081540",
    "end": "3090920"
  },
  {
    "text": "And then the last term here\nis this pi theta, so then plus log pi theta ak, given sk.",
    "start": "3090920",
    "end": "3102830"
  },
  {
    "text": "OK. So one out of two\nsteps done now. The last thing we have to\ndo is just take the gradient",
    "start": "3102830",
    "end": "3108710"
  },
  {
    "text": "of that expression. So you might be like, oh, no, we\nhave to take the gradient now. But it's very nice\nhere in this case",
    "start": "3108710",
    "end": "3115580"
  },
  {
    "text": "because what happens when we\ntake the gradient with respect to theta of this term? It just drops out, right?",
    "start": "3115580",
    "end": "3120990"
  },
  {
    "text": "It doesn't depend on theta. So that goes away. Does this term depend on theta?",
    "start": "3120990",
    "end": "3127850"
  },
  {
    "text": "No. So we can also just\nget rid of that term. And that's a huge\nresult when working",
    "start": "3127850",
    "end": "3133700"
  },
  {
    "text": "with stochastic policies, OK? That's why I made this\ndifferentiation here about, we're working with\nstochastic policies",
    "start": "3133700",
    "end": "3140060"
  },
  {
    "text": "because when we're looking at\nthis likelihood ratio policy gradient, it does not depend\non the transition model",
    "start": "3140060",
    "end": "3147220"
  },
  {
    "text": "when we take the gradient\nof the log probability. This transition model drops out. And that's a huge\nresult, which we'll",
    "start": "3147220",
    "end": "3153390"
  },
  {
    "text": "see a real-world example\nof that in a second. But that's something you\nwant to keep in mind is,",
    "start": "3153390",
    "end": "3158970"
  },
  {
    "text": "when I look at the gradient\nof this log probability, this transition model\ndoes not show up.",
    "start": "3158970",
    "end": "3164230"
  },
  {
    "text": "And that's a really\nnice result for us. So all we're left now is the\ngradient, with respect to theta,",
    "start": "3164230",
    "end": "3171030"
  },
  {
    "text": "of this term. OK, so we'll rewrite\nthis over here. ",
    "start": "3171030",
    "end": "3186470"
  },
  {
    "text": "OK, so the expression\nthat we come to at the end is going to be this\nexpectation over trajectories,",
    "start": "3186470",
    "end": "3195470"
  },
  {
    "text": "the sum from k\nequals 1 to our depth times the gradient with\nrespect to theta log pi theta",
    "start": "3195470",
    "end": "3206780"
  },
  {
    "text": "ak given sk times the\nreward along the trajectory.",
    "start": "3206780",
    "end": "3215930"
  },
  {
    "text": "So that's the end result of\nthe likelihood ratio policy",
    "start": "3215930",
    "end": "3221300"
  },
  {
    "text": "gradient. Now, you might be saying,\nwell, but we didn't take the gradient of this yet.",
    "start": "3221300",
    "end": "3227089"
  },
  {
    "text": "What is this here? Well, remember, for\nour particular policy",
    "start": "3227090",
    "end": "3232310"
  },
  {
    "text": "that we've parameterized,\nwe can typically take the gradient\nwith respect to that.",
    "start": "3232310",
    "end": "3237960"
  },
  {
    "text": "So if pi theta is\na neural network, we can evaluate the gradient\nof that neural network through back propagation.",
    "start": "3237960",
    "end": "3244450"
  },
  {
    "text": "If pi theta is some other\nfunction that we've set up, we typically can--\nwe know the gradient",
    "start": "3244450",
    "end": "3249720"
  },
  {
    "text": "of that function with\nrespect to our parameters. So this is good news for us that\nthis is the only term that's",
    "start": "3249720",
    "end": "3254760"
  },
  {
    "text": "showing up in this gradient\nbecause very often in practice, when we've set up our\nown parameterized policy,",
    "start": "3254760",
    "end": "3260339"
  },
  {
    "text": "we can take the\ngradient of that policy. So this is a nice\nspot to be in for us.",
    "start": "3260340",
    "end": "3268109"
  },
  {
    "text": "Any questions on how we\ngot to here, this point? Sorry, back to\nsomething a little bit.",
    "start": "3268110",
    "end": "3274150"
  },
  {
    "text": "You had the previous\nexpression where substituted in the log of\nthe gradient of the log.",
    "start": "3274150",
    "end": "3279803"
  },
  {
    "text": "How did you end up\nwith that expression? So not that one, even\nbefore, below that line. ",
    "start": "3279803",
    "end": "3286624"
  },
  {
    "text": "So the one on the very\ntop of this board. This one? Yes. How'd you end up\nwith that expression? Ah, yes. So the question is, how\ndid we get this expression?",
    "start": "3286624",
    "end": "3293980"
  },
  {
    "text": "So this expression, typically\ncalled the log derivative trick, I don't really like that\nname because it's just",
    "start": "3293980",
    "end": "3299880"
  },
  {
    "text": "the chain rule. You're just taking the gradient\nof the log probability. So you can think\nabout if I had--",
    "start": "3299880",
    "end": "3306000"
  },
  {
    "text": "instead of thinking about\nthis, if I just had log of x,",
    "start": "3306000",
    "end": "3311070"
  },
  {
    "text": "the derivative of log of\nx is just 1 over x, right? So when I'm taking the\ngradient with respect",
    "start": "3311070",
    "end": "3317400"
  },
  {
    "text": "to theta of this term, I'm\njust taking the gradient of the outside, which\nis 1 over p of tau,",
    "start": "3317400",
    "end": "3322770"
  },
  {
    "text": "and then times the\ngradient of the inside. Yeah, so just chain rule there. And they call it the\nlog derivative trick,",
    "start": "3322770",
    "end": "3328980"
  },
  {
    "text": "but I don't know. It's not a trick to me. It's just, you're taking\nthe gradients, right?",
    "start": "3328980",
    "end": "3334890"
  },
  {
    "text": "OK. So I mentioned that\nit's a nice result",
    "start": "3334890",
    "end": "3343605"
  },
  {
    "text": "that we don't have the\ntransition model here, that the transition\nmodel doesn't show up. And so this is just an example\nto get you thinking about,",
    "start": "3343605",
    "end": "3351630"
  },
  {
    "text": "if I had this robotic\narm, and I wanted to use either the likelihood\nratio policy gradient or one",
    "start": "3351630",
    "end": "3360539"
  },
  {
    "text": "of the extensions that we'll\ntalk about to train a policy, to manipulate and pick up all\nof these diverse set of objects,",
    "start": "3360540",
    "end": "3368710"
  },
  {
    "text": "if I had to do that and I\nneeded to know the transition model, that's going to be pretty\ntedious because I have to think",
    "start": "3368710",
    "end": "3375030"
  },
  {
    "text": "about, OK, if I'm\npicking up a lemon, and I move my manipulator\nin this direction, what's",
    "start": "3375030",
    "end": "3380829"
  },
  {
    "text": "that going to do to the lemon? Now, if I came from the\nother direction instead, how is that going to\nchange the lemon, right?",
    "start": "3380830",
    "end": "3386140"
  },
  {
    "text": "So I have to think about all\nof these different cases. And then I also have\nto do that, well, what if I'm picking\nup an apple instead?",
    "start": "3386140",
    "end": "3392020"
  },
  {
    "text": "So knowing that\ntransition probability can be pretty tedious. Not impossible, right? There's simulators out\nthere that do this,",
    "start": "3392020",
    "end": "3398650"
  },
  {
    "text": "and that's available, but it\njust makes our lives easier if we don't have to\nthink about, what",
    "start": "3398650",
    "end": "3404140"
  },
  {
    "text": "is this transition probability? Because it just dropped out\nof our gradient expression. So that's just\none of the reasons",
    "start": "3404140",
    "end": "3410230"
  },
  {
    "text": "that that's such a nice result. And that example on the\nright, actually, from OpenAI",
    "start": "3410230",
    "end": "3415690"
  },
  {
    "text": "was trained using proximal\npolicy optimization. So time permitting,\nwe'll hopefully get to that by the end\nof the lecture just",
    "start": "3415690",
    "end": "3422650"
  },
  {
    "text": "to give you a sense\nof where we're going. ",
    "start": "3422650",
    "end": "3427980"
  },
  {
    "text": "OK. So now sort of where we're at\nhere with the likelihood ratio",
    "start": "3427980",
    "end": "3435240"
  },
  {
    "text": "policy gradient. We have this estimate, and\nit's an unbiased estimate,",
    "start": "3435240",
    "end": "3440970"
  },
  {
    "text": "but it has pretty\nhigh variance, OK? So what do we mean when we're\ntalking about bias and variance",
    "start": "3440970",
    "end": "3446730"
  },
  {
    "text": "here? So this image that we\nhave is showing here",
    "start": "3446730",
    "end": "3452160"
  },
  {
    "text": "for the high-bias case, so\nthe red is the true value, the black is our mean estimate.",
    "start": "3452160",
    "end": "3460030"
  },
  {
    "text": "So when we have\nhigh bias, our mean is deviating from\nthe true value.",
    "start": "3460030",
    "end": "3465360"
  },
  {
    "text": "When we have high\nvariance, you can see that there's this large\nspread in the possible values",
    "start": "3465360",
    "end": "3471150"
  },
  {
    "text": "that we could have, OK? So where we sort of\nwant to be is over here, where we have low\nbias and low variance.",
    "start": "3471150",
    "end": "3479019"
  },
  {
    "text": "Where we're sitting at right now\nwith our likelihood ratio policy gradient is here. We have low bias, but\nwe have high variance.",
    "start": "3479020",
    "end": "3485560"
  },
  {
    "text": "So now what we're\ngoing to focus on is, how can we reduce the\nvariance of this estimate? And I think that goes back to\nyour question earlier about,",
    "start": "3485560",
    "end": "3492670"
  },
  {
    "text": "don't these rollout methods\ngive us very high variance? And the answer is,\nyes, especially",
    "start": "3492670",
    "end": "3497740"
  },
  {
    "text": "for when we're working with\nthe likelihood ratio policy gradient. So how can we now\nreduce that variance?",
    "start": "3497740",
    "end": "3503540"
  },
  {
    "text": "That's what we're going\nto talk about now. And one other thing\njust to keep in mind",
    "start": "3503540",
    "end": "3509110"
  },
  {
    "text": "is that the variance\nof this estimate generally increases\nwith rollout depth.",
    "start": "3509110",
    "end": "3515210"
  },
  {
    "text": "So looking at this\ntrajectory here, as our depth gets further and further\ninto the future, the variance",
    "start": "3515210",
    "end": "3521110"
  },
  {
    "text": "of our estimate increases. So why might that be the case? Well, you can think about, as\nyou go further into the future,",
    "start": "3521110",
    "end": "3529630"
  },
  {
    "text": "it gets harder and\nharder to attribute. This reward that I received\nsomewhere in the middle, was that because of one\nof the early actions",
    "start": "3529630",
    "end": "3536320"
  },
  {
    "text": "I took, or was it from\nsomething I did in the middle? Which action attributed to that?",
    "start": "3536320",
    "end": "3542060"
  },
  {
    "text": "So a similar example,\nif you get a promotion, what specific action led to\nthat promotion occurring?",
    "start": "3542060",
    "end": "3549770"
  },
  {
    "text": "Was it what you ate for\nbreakfast that morning, or was it 20 years before\nwhen you went to some event",
    "start": "3549770",
    "end": "3556839"
  },
  {
    "text": "at your college, and you met\nsomebody who then ended up working at that company that\nthen got you a job there?",
    "start": "3556840",
    "end": "3563050"
  },
  {
    "text": "Which specific action was it\nthat led to that promotion? So that's what we're\ntalking about here,",
    "start": "3563050",
    "end": "3568300"
  },
  {
    "text": "is it gets harder to\nmap that causality in these actions as our\ntrajectory depth gets longer.",
    "start": "3568300",
    "end": "3575470"
  },
  {
    "text": "And so that's what we're\nlooking at reducing now. And the reward to go is one way\nthat we can go about doing this.",
    "start": "3575470",
    "end": "3583520"
  },
  {
    "text": "OK, so reward to go, we're going\nto start from this expression that we have. So we're starting from the\nresult of the likelihood ratio",
    "start": "3583520",
    "end": "3590869"
  },
  {
    "text": "policy gradient. And I think the\nbest way to think about what the reward\nto go is trying",
    "start": "3590870",
    "end": "3596270"
  },
  {
    "text": "to do is by looking\nat a timeline here. So we have at the start, we take\nsome action, our first action.",
    "start": "3596270",
    "end": "3607310"
  },
  {
    "text": "We get some reward here. We then take a new action. We get some new reward.",
    "start": "3607310",
    "end": "3613670"
  },
  {
    "text": "We repeat this, right? So now we're at action k. We get some reward k.",
    "start": "3613670",
    "end": "3619529"
  },
  {
    "text": "And then after that, we\ntake action k plus 1. We get reward k plus 1.",
    "start": "3619530",
    "end": "3626990"
  },
  {
    "text": "But what you might\nsee here is when we're looking at the\nsum over our trajectory and looking at action\nk specifically,",
    "start": "3626990",
    "end": "3634640"
  },
  {
    "text": "that action has no influence\nover the rewards that occurred in the past, right?",
    "start": "3634640",
    "end": "3640990"
  },
  {
    "text": "That this action here cannot\nchange any of those rewards we got in the past. It can only change what we\ndo now or in the future.",
    "start": "3640990",
    "end": "3649170"
  },
  {
    "text": "So if we have an agent\nhere, that agent, when it's looking at\nwhat action to take,",
    "start": "3649170",
    "end": "3654710"
  },
  {
    "text": "it should not care\nabout any good things it did in the past, any\nmistakes that it made. It should forget about that and\nonly focus on the future, OK?",
    "start": "3654710",
    "end": "3663780"
  },
  {
    "text": "I didn't intend to\nget so deep there, but there's some philosophy\nhere in this reward to go.",
    "start": "3663780",
    "end": "3671340"
  },
  {
    "text": "OK, so if we have action\nak, we're basically saying, we don't want--",
    "start": "3671340",
    "end": "3676440"
  },
  {
    "text": "or we have no influence over\nwhat any of these rewards are, right? We can't change what\nthose rewards are.",
    "start": "3676440",
    "end": "3682530"
  },
  {
    "text": "Only now or in the future. OK? And So that's what the reward\nto go is trying to correct for,",
    "start": "3682530",
    "end": "3688230"
  },
  {
    "text": "is this causality of\nactions and rewards. So how we're going to\ngo about this is we just",
    "start": "3688230",
    "end": "3694110"
  },
  {
    "text": "look at this r of tau is\njust the discounted reward that we receive along\nthat trajectory.",
    "start": "3694110",
    "end": "3700590"
  },
  {
    "text": "So r of tau is just the sum\nfrom k equals 1 to d of r of k,",
    "start": "3700590",
    "end": "3709080"
  },
  {
    "text": "gamma to the k minus 1. So it's just the\ndiscounted reward that we're getting along\nthese trajectories.",
    "start": "3709080",
    "end": "3717559"
  },
  {
    "text": "What we're doing with the reward\nto go approach is we're just going to split up this sum, OK?",
    "start": "3717560",
    "end": "3722869"
  },
  {
    "text": "We're just going\nto split up the sum and then we're going to\nsimplify it a little bit. ",
    "start": "3722870",
    "end": "3734880"
  },
  {
    "text": "OK, so just writing\nout the same expression that we have, but then we're\ngoing to substitute in the sum.",
    "start": "3734880",
    "end": "3740770"
  },
  {
    "text": "So this is just\nthe expectation sum k equals 1 to d gradient\nwith respect to theta.",
    "start": "3740770",
    "end": "3748350"
  },
  {
    "text": "This is the log here. ",
    "start": "3748350",
    "end": "3760089"
  },
  {
    "text": "OK, so now instead\nof r of tau, we're just going to bring that sum in.",
    "start": "3760090",
    "end": "3765220"
  },
  {
    "text": "But instead of just writing it\nhow we've written it up there, we're going to split it\nup into two separate sums.",
    "start": "3765220",
    "end": "3770630"
  },
  {
    "text": "Yes. Quick question--\nI didn't hear you, but when you say\nr of k over there, that is distinct\nfrom gamma k minus 1.",
    "start": "3770630",
    "end": "3776710"
  },
  {
    "text": "The r of k is r a function\nof k, like r that time? Yes. Yes. Good point. So the question was,\nwhen we write r of k",
    "start": "3776710",
    "end": "3782530"
  },
  {
    "text": "with these parentheses,\nthat's not an exponent. That's the index to our r of k,\nwhereas gamma to the k minus 1",
    "start": "3782530",
    "end": "3790270"
  },
  {
    "text": "is an exponent. Good question. OK, so we're just going\nto split up that sum here.",
    "start": "3790270",
    "end": "3797619"
  },
  {
    "text": "So how we're going\nto split it up is just by introducing\nthis new index that we're going to call l.",
    "start": "3797620",
    "end": "3803170"
  },
  {
    "text": "So we're just\ngoing to go from l. This l is different from\nour k index here, OK? So we're going from l\nequals 1 to k minus 1,",
    "start": "3803170",
    "end": "3812020"
  },
  {
    "text": "and that's going to be r of\nl gamma to the l minus 1.",
    "start": "3812020",
    "end": "3817360"
  },
  {
    "text": "So we've done that,\nand then we're just splitting up that sum here. So now we're going to have\nplus l equals k to d here.",
    "start": "3817360",
    "end": "3830200"
  },
  {
    "text": "OK, and that's going to be r\nof l gamma to the l minus 1.",
    "start": "3830200",
    "end": "3836500"
  },
  {
    "text": "So all we've done, we\njust took r of tau, we wrote out the expression\nfor r of tau as that summation,",
    "start": "3836500",
    "end": "3842600"
  },
  {
    "text": "and then we just split up that\nsummation from k equals 1 to d. Now, we're just going\nl equals 1 to k minus 1",
    "start": "3842600",
    "end": "3848140"
  },
  {
    "text": "and l equals k to d. OK, that's all we've done. Why would we do this? Well, remember our\ntimeline of events here.",
    "start": "3848140",
    "end": "3856490"
  },
  {
    "text": "At action k, we're\nsaying we want our agent to forget about the past, right? Don't worry about what\nhappened in the past.",
    "start": "3856490",
    "end": "3862800"
  },
  {
    "text": "Only focus on the future. So if we want our agent to move\non and forget about the past, we're just going to drop the\npast and focus on the future,",
    "start": "3862800",
    "end": "3869910"
  },
  {
    "text": "OK? So all of those terms there\nare terms, reward terms, that occurred in the past.",
    "start": "3869910",
    "end": "3876110"
  },
  {
    "text": "We're now only focused on the\nreward terms that occur now or in the future, OK?",
    "start": "3876110",
    "end": "3881530"
  },
  {
    "text": "And so we can just rewrite this\nslightly as if we factor out a gamma to the k\nminus 1, this is just",
    "start": "3881530",
    "end": "3888619"
  },
  {
    "text": "sort of convention of how\nit's typically written. This becomes still l\nequals k to d r of l.",
    "start": "3888620",
    "end": "3896210"
  },
  {
    "text": "Now, it's just gamma\nto the l minus k. And what this term here is\ncalled is the reward to go.",
    "start": "3896210",
    "end": "3904140"
  },
  {
    "text": "So we call this\nthe reward to go. All it is, is just correcting\nfor that reward action",
    "start": "3904140",
    "end": "3910849"
  },
  {
    "text": "causality. OK, so now if we rewrite\nthis whole expression",
    "start": "3910850",
    "end": "3916700"
  },
  {
    "text": "using this reward to go, what\nwe're left with is just this. So we still have this\nsum on the outside.",
    "start": "3916700",
    "end": "3923035"
  },
  {
    "start": "3923035",
    "end": "3933130"
  },
  {
    "text": "And now it's just gamma to the\nk minus 1 times r of k to go.",
    "start": "3933130",
    "end": "3941950"
  },
  {
    "text": "OK, so all we did here was sort\nof just remove those terms that occurred in the past so that\nwhen we're looking at action k,",
    "start": "3941950",
    "end": "3948700"
  },
  {
    "text": "that's only being weighted\nhere in this expectation. It's only being weighted by\nthe rewards that occur now",
    "start": "3948700",
    "end": "3955750"
  },
  {
    "text": "or in the future, OK? That's the reward\nto go approach. And the nice part about\nthis is that it leads",
    "start": "3955750",
    "end": "3961840"
  },
  {
    "text": "to a decrease in variance, OK? Why does it lead to a\ndecrease in variance? Well, we've sort of made the\nsignal a little bit clearer now.",
    "start": "3961840",
    "end": "3970400"
  },
  {
    "text": "We've made that causality\na little bit clearer. Instead of just having this\nlarge reward term that's multiplying every single\naction that we're looking at,",
    "start": "3970400",
    "end": "3978330"
  },
  {
    "text": "we're only weighting\nit now by the rewards that that action\ncan influence, OK?",
    "start": "3978330",
    "end": "3984390"
  },
  {
    "text": "Any questions on reward to go? Yes. Sorry, about the\nnotation again--",
    "start": "3984390",
    "end": "3990759"
  },
  {
    "text": "can you clarify what this\nnotation switch was, like,",
    "start": "3990760",
    "end": "3996390"
  },
  {
    "text": "there, the line below? Yes. OK, so the question was,\nwhat's the notation switch we did from here to here?",
    "start": "3996390",
    "end": "4002950"
  },
  {
    "text": "So at this step, we just\nsplit up the sum up there. So we just went from l equals\n1 to k minus 1 and then k to d.",
    "start": "4002950",
    "end": "4010950"
  },
  {
    "text": "So just going-- it's the\nsame thing from 1 to d, but we've just split it up. Then going here-- so let me\ndraw this to distinguish them.",
    "start": "4010950",
    "end": "4020910"
  },
  {
    "text": "All we did here was we\njust factored out a gamma to the k minus 1 from this term. OK, so we just brought gamma\nto the k minus 1 out in front.",
    "start": "4020910",
    "end": "4028900"
  },
  {
    "text": "And what that left us with\nwas just gamma to the l minus k instead of l minus 1.",
    "start": "4028900",
    "end": "4033910"
  },
  {
    "text": "So if you just multiply gamma\nto the k minus 1 back inside, it will bring us back to here.",
    "start": "4033910",
    "end": "4038970"
  },
  {
    "text": "And the reason we did that\nis just for convention. The reward to go is written\ntypically like this.",
    "start": "4038970",
    "end": "4044180"
  },
  {
    "text": " Yes. Why are both gammas\nraised to l minus 1?",
    "start": "4044180",
    "end": "4051260"
  },
  {
    "text": "Why are both gammas what? Raised to l minus 1. Why are both gammas\nraised to l minus 1?",
    "start": "4051260",
    "end": "4057190"
  },
  {
    "text": "OK, so yeah, that's\na good question. So why are the gammas\nhere raised to l minus 1?",
    "start": "4057190",
    "end": "4062599"
  },
  {
    "text": "Well, it comes back\nfrom this expression, which is just the discounted\nreward into the future.",
    "start": "4062600",
    "end": "4069090"
  },
  {
    "text": "So you can think about along\nthis timeline, at time step 1, we get the immediate reward.",
    "start": "4069090",
    "end": "4075510"
  },
  {
    "text": "But then at time step\ntwo, we have a discount, so we multiply that\nby gamma because we're discounting the future rewards.",
    "start": "4075510",
    "end": "4082109"
  },
  {
    "text": "So then gamma k will then\nhave a factor gamma k minus 1, and we'll just\ncontinue that out.",
    "start": "4082110",
    "end": "4088320"
  },
  {
    "text": "Yes. Before we apply to\nthe work term there, does the work term go inside\nthe simulation, or is it--",
    "start": "4088320",
    "end": "4097939"
  },
  {
    "text": "This term here? Yes. Yeah. So it's weighting each of\nthose actions in the sum there.",
    "start": "4097939",
    "end": "4104160"
  },
  {
    "text": "So each of those actions,\nwe're multiplying by the-- when we're doing\nthis sum k equals 1 to d.",
    "start": "4104160",
    "end": "4109650"
  },
  {
    "text": " Does that answer your question? ",
    "start": "4109650",
    "end": "4118750"
  },
  {
    "text": "Yeah? OK. Did you also-- I think that clarification\nhere is we're using l in on both of those cases.",
    "start": "4118750",
    "end": "4125049"
  },
  {
    "text": "So it's a different l in the\ncontext of each of those sums.",
    "start": "4125050",
    "end": "4130068"
  },
  {
    "text": "Yes. Yeah. Between this one and-- yeah. Just those two here? Yeah. So the l's here are\ndifferent from--",
    "start": "4130069",
    "end": "4137576"
  },
  {
    "text": "Different from each other? From each other, yeah. This l is only for this sum. This l is only for this sum.",
    "start": "4137577",
    "end": "4145060"
  },
  {
    "text": "Thanks. Yeah.  OK.",
    "start": "4145060",
    "end": "4150390"
  },
  {
    "text": "So one other thing we can do\nnow to decrease this variance",
    "start": "4150390",
    "end": "4155870"
  },
  {
    "text": "is what's called\nbaseline subtraction. And baseline subtraction, we're\njust starting from the reward",
    "start": "4155870",
    "end": "4161420"
  },
  {
    "text": "to go expression, and now\nwe're just subtracting some baseline off of that.",
    "start": "4161420",
    "end": "4167210"
  },
  {
    "text": "OK, so the reason why we're\ndoing this is if you think about",
    "start": "4167210",
    "end": "4174020"
  },
  {
    "text": "this example, where\nyou're in some state, and you have three possible\nactions available to you,",
    "start": "4174020",
    "end": "4181189"
  },
  {
    "text": "and each of those actions\ngive you plus 1,000 reward.",
    "start": "4181189",
    "end": "4186743"
  },
  {
    "text": "When you first see this,\nyou might be like, awesome. This is a great state\nto be in, right? No matter what I do, I'm\ngetting plus 1,000 reward.",
    "start": "4186743",
    "end": "4193500"
  },
  {
    "text": "That's great. But when we're talking about\nestimating the gradient",
    "start": "4193500",
    "end": "4198800"
  },
  {
    "text": "and using that gradient\ninformation to change our policy parameters, this plus 1,000 in\neach of these different actions",
    "start": "4198800",
    "end": "4206239"
  },
  {
    "text": "is not really giving us a whole\nlot of information to work with on how we should change\nour policy, right?",
    "start": "4206240",
    "end": "4212512"
  },
  {
    "text": "Maybe in a global\nsense, it would give us some way to say, well, we\nwant to get to this state.",
    "start": "4212512",
    "end": "4218380"
  },
  {
    "text": "But just looking at this\nstate in specifically, the plus 1,000 for each action,\nevery action in that state is",
    "start": "4218380",
    "end": "4224670"
  },
  {
    "text": "the same to us. So what the reward to go-- what the baseline\nsubtraction method is saying",
    "start": "4224670",
    "end": "4230909"
  },
  {
    "text": "is instead of looking at the\nnet reward that you receive, let's look at the reward\nrelative to some baseline.",
    "start": "4230910",
    "end": "4237760"
  },
  {
    "text": "So in this case, we\ncould choose the baseline that we want to use as the\naverage reward from the Actions",
    "start": "4237760",
    "end": "4244590"
  },
  {
    "text": "that we get. OK, so in that case, when we\nlook at the baseline reward,",
    "start": "4244590",
    "end": "4250485"
  },
  {
    "text": "that would be 1,000, right? Because each action\nis giving us 1,000. So now when we look at this\nreward from this action,",
    "start": "4250485",
    "end": "4257160"
  },
  {
    "text": "relative to that\nbaseline, it will be zero because it's just average. It's just an average action.",
    "start": "4257160",
    "end": "4262599"
  },
  {
    "text": "Similarly for this\none and this one. Now, if instead we had\nplus a million here,",
    "start": "4262600",
    "end": "4269250"
  },
  {
    "text": "then when looking at this\nreward relative to the average, so relative to our baseline,\nthat's going to say, well, hey,",
    "start": "4269250",
    "end": "4277420"
  },
  {
    "text": "this is a pretty\ngood action, right? It's way better than the\naverage action that we had, so let's shift our\npolicy parameters",
    "start": "4277420",
    "end": "4283920"
  },
  {
    "text": "to get us to take that action. So that's the whole idea\nof baseline subtraction. Don't look at just\nthe net reward.",
    "start": "4283920",
    "end": "4291190"
  },
  {
    "text": "Look at the reward relative\nto some baseline, OK? What baseline you\nchoose, that's up to you.",
    "start": "4291190",
    "end": "4296739"
  },
  {
    "text": "There's many different\nbaselines out there, so you could use average reward. There's a proof that you can get\nthe minimum variance baseline.",
    "start": "4296740",
    "end": "4304619"
  },
  {
    "text": "But really, the\nmain idea is just using a baseline also helps\nyou to bring out the signal,",
    "start": "4304620",
    "end": "4310500"
  },
  {
    "text": "bring out the signal in that\npolicy rollout of what actions are actually more beneficial\nrelative to some baseline.",
    "start": "4310500",
    "end": "4317910"
  },
  {
    "text": "So that's baseline subtraction,\njust another method that we can use to decrease\nthe variance of that estimate.",
    "start": "4317910",
    "end": "4326155"
  },
  {
    "text": "Yes. Is the baseline\ndifferent for every node, or do you set it globally?",
    "start": "4326155",
    "end": "4332267"
  },
  {
    "text": "The question was, is\nyour baseline different for every specific\naction you're looking at",
    "start": "4332267",
    "end": "4337700"
  },
  {
    "text": "or do you set it globally? And the answer would\nbe, it depends. So you could have a baseline\nthat is action dependent, right?",
    "start": "4337700",
    "end": "4345690"
  },
  {
    "text": "That gets a little complicated. So in the general\ncase, you can just set a global baseline, right? You could just set the baseline\nto be the mean reward you",
    "start": "4345690",
    "end": "4353540"
  },
  {
    "text": "received across all actions. So that's one option. But it's sort of up to you\nin how you set that baseline.",
    "start": "4353540",
    "end": "4359250"
  },
  {
    "text": " OK, so just to recap\nwhat we've seen.",
    "start": "4359250",
    "end": "4366990"
  },
  {
    "text": "We started with\nfinite difference and saw that we can\nestimate the gradient using that finite difference.",
    "start": "4366990",
    "end": "4372100"
  },
  {
    "text": "We then saw that we could get\na little more robust estimate using linear regression. And then we looked at\nthe likelihood ratio,",
    "start": "4372100",
    "end": "4378840"
  },
  {
    "text": "which gave us basically an\nanalytical expression for doing this. But it was a pretty\nhigh-variance estimate,",
    "start": "4378840",
    "end": "4385780"
  },
  {
    "text": "so then we were looking\nat, how can we reduce the variance of that estimate? OK, so that is our policy\ngradient estimation.",
    "start": "4385780",
    "end": "4393239"
  },
  {
    "text": "So now we have some estimate\nof the policy gradient, and you might be\nthinking like, OK,",
    "start": "4393240",
    "end": "4399809"
  },
  {
    "text": "wow, that felt\nlike a lot of math, that likelihood ratio\nderivation was kind of long.",
    "start": "4399810",
    "end": "4405210"
  },
  {
    "text": "Why did we do all of this work? What was it for? And so now we're\ngoing to put our work",
    "start": "4405210",
    "end": "4410699"
  },
  {
    "text": "to use for us with policy\ngradient optimization, OK? So the whole reason\nwe were looking",
    "start": "4410700",
    "end": "4415770"
  },
  {
    "text": "at those gradient\nestimates was so that we could optimize our policy, OK? We wanted to use the gradient\ninformation to guide our policy",
    "start": "4415770",
    "end": "4424390"
  },
  {
    "text": "improvement. So that's what we're going\nto be talking about now. So what we're going to start\nwith is just basic gradient",
    "start": "4424390",
    "end": "4433030"
  },
  {
    "text": "ascent, OK? So when we're doing\ngradient ascent, we're trying to\nmaximize the utility.",
    "start": "4433030",
    "end": "4441280"
  },
  {
    "text": "And how we're going\nto do that is just by taking these iterative steps. So what we're showing\non the left here",
    "start": "4441280",
    "end": "4448120"
  },
  {
    "text": "is the true function\nthat we're working with. On the right is the contour\nplot of that function.",
    "start": "4448120",
    "end": "4454340"
  },
  {
    "text": "So the initial point\nthat we're starting at is this white point here, so\nthat's our initial parameters.",
    "start": "4454340",
    "end": "4460280"
  },
  {
    "text": "And then we're just\ngoing to look at, what's the direction\nof the gradient there? And then we're going to\nstep in that direction.",
    "start": "4460280",
    "end": "4466850"
  },
  {
    "text": "OK, so all we're doing is\nwe're updating our parameters theta by some fixed\nstep size alpha,",
    "start": "4466850",
    "end": "4472790"
  },
  {
    "text": "so alpha is our step\nsize in this case, times the gradient\nof our function. And so we can just do this\nover and over and over,",
    "start": "4472790",
    "end": "4480260"
  },
  {
    "text": "and based on that\nfixed step size that we've set,\nhopefully, eventually, we'll converge to the maximum.",
    "start": "4480260",
    "end": "4487119"
  },
  {
    "text": "Step factor. Step factor, yes. Thank you. Alpha, the step factor.",
    "start": "4487120",
    "end": "4493000"
  },
  {
    "text": "So this is a relatively\nnice function, right? It's very smooth, pretty\neasy to work with and follow",
    "start": "4493000",
    "end": "4498850"
  },
  {
    "text": "the gradient to the maximum. A lot of the time, the functions\nthat we're working with are not that nice.",
    "start": "4498850",
    "end": "4504829"
  },
  {
    "text": "So we can't just close\nthe book right now and say, awesome, let's\njust use gradient ascent.",
    "start": "4504830",
    "end": "4510639"
  },
  {
    "text": "Let's fix our alpha\nas some constant and go from there because\nthe devil is really",
    "start": "4510640",
    "end": "4516489"
  },
  {
    "text": "in the details of how we\nchoose that alpha, OK? ",
    "start": "4516490",
    "end": "4522610"
  },
  {
    "text": "So to see an example\nof this, if we set alpha to be too large,\nwhat's going to happen?",
    "start": "4522610",
    "end": "4529980"
  },
  {
    "text": "So if we have just some\nlarge constant alpha, we're going to start\nat this initial theta. We're going to look\nat the gradient.",
    "start": "4529980",
    "end": "4536320"
  },
  {
    "text": "And then we're going to step in\nthe direction of the gradient based on our large alpha.",
    "start": "4536320",
    "end": "4541699"
  },
  {
    "text": "And that will send us way\nover to the left side here. And now for this\nvisual example, all",
    "start": "4541700",
    "end": "4547030"
  },
  {
    "text": "of the gradient that I've\ndrawn are not actually correct. So don't be like, wait, with\nan alpha there, it wouldn't",
    "start": "4547030",
    "end": "4552820"
  },
  {
    "text": "actually go that far, OK? Just for visualization here. So then we repeat\nthis process now.",
    "start": "4552820",
    "end": "4558670"
  },
  {
    "text": "We look at the\ngradient at this point, and that would send a\nshooting way over here. So now we're over here,\nand we're going to repeat.",
    "start": "4558670",
    "end": "4565010"
  },
  {
    "text": "And now, we've jumped\nway over there. So you can see when we\nhave too large of an alpha, we don't really converge.",
    "start": "4565010",
    "end": "4571070"
  },
  {
    "text": "We're just sort of\nbouncing around, right? And on the other side, when\nour alpha is too small,",
    "start": "4571070",
    "end": "4577420"
  },
  {
    "text": "what would happen? Well, we'd look at our\ngradient, we'd step, and then it would just\nnot really move, right?",
    "start": "4577420",
    "end": "4583880"
  },
  {
    "text": "And we'd do it again and again. And we're just going to\nvery slowly converge, OK?",
    "start": "4583880",
    "end": "4589090"
  },
  {
    "text": "So we're going to waste a\nlot of computation there. So our goal here is\nthat we want to be",
    "start": "4589090",
    "end": "4595690"
  },
  {
    "text": "able to optimize some\napproximation of this U of theta objective with these constraints\nthat our new thetas, our theta",
    "start": "4595690",
    "end": "4604330"
  },
  {
    "text": "primes, are not too far from\nour initial starting theta. That's our overall goal here.",
    "start": "4604330",
    "end": "4611820"
  },
  {
    "text": "So how we do that is we\nintroduce this constraint function, and we're going to\ncall this constraint function g.",
    "start": "4611820",
    "end": "4617590"
  },
  {
    "text": "So g is sort of like a\nsimilarity measure or a distance measure, not exactly\nright, but it's",
    "start": "4617590",
    "end": "4623400"
  },
  {
    "text": "just some way that we are\nmeasuring the distance here between our new theta primes\nagainst our old thetas.",
    "start": "4623400",
    "end": "4629650"
  },
  {
    "text": "And we want that to be less\nthan or equal to some epsilon that we're setting as\nthis free parameter.",
    "start": "4629650",
    "end": "4636300"
  },
  {
    "text": "So the methods that we're going\nto talk about in the next three minutes are these four\nmethods that we're not",
    "start": "4636300",
    "end": "4643770"
  },
  {
    "text": "going to be able to\nget to all of them, but we're going to start with\nrestricted gradient here. And so the main idea that I\nwant you to keep in mind as we",
    "start": "4643770",
    "end": "4651630"
  },
  {
    "text": "go through this is the main\ntwo ways that these approaches differ are, one, how they're\nforming the objective that we're",
    "start": "4651630",
    "end": "4659309"
  },
  {
    "text": "trying to optimize, and\ntwo, how they're structuring that constraint, OK? Other than that, they're\npretty much the same.",
    "start": "4659310",
    "end": "4665850"
  },
  {
    "text": "But they're just taking\ndifferent flavors on how we set the objective,\nhow we set the constraints.",
    "start": "4665850",
    "end": "4672300"
  },
  {
    "text": "So starting with restricted\ngradient, which I'd say is the simplest of\nthese approaches. We're starting\nwith-- remember, we",
    "start": "4672300",
    "end": "4677940"
  },
  {
    "text": "have those two knobs,\nobjective and constraint. So we're looking at now\nrestricted gradient, how it sets this objective.",
    "start": "4677940",
    "end": "4685080"
  },
  {
    "text": "The restricted\ngradient method just does a very simple\nfirst-order approximation. So we're just saying, OK, our\ntrue function looks like that.",
    "start": "4685080",
    "end": "4692590"
  },
  {
    "text": "Our current theta is here. Let's look at the gradient. And that blue line becomes our\napproximation of the function",
    "start": "4692590",
    "end": "4698830"
  },
  {
    "text": "that we're trying\nto optimize, OK? So we don't actually know\nthe true U of theta, right? But we're just saying, this is\na pretty, pretty good estimate",
    "start": "4698830",
    "end": "4706360"
  },
  {
    "text": "for us. As long as we stay close\nto that initial theta point, the first-order\napproximation, hopefully, is OK.",
    "start": "4706360",
    "end": "4713030"
  },
  {
    "text": "So when we move to\ntheta prime now, and we evaluate a new point,\nwhat this surrogate objective is",
    "start": "4713030",
    "end": "4720310"
  },
  {
    "text": "telling us is that our\nnew U of theta prime will be approximately equal to\njust this first-order Taylor",
    "start": "4720310",
    "end": "4727030"
  },
  {
    "text": "expansion that we've done. So this is just the\nfirst-order approximation of the true U of theta\nthat we're looking at.",
    "start": "4727030",
    "end": "4733780"
  },
  {
    "text": "And that's how the restricted\ngradient method goes about that. So using this first-order\napproximation now,",
    "start": "4733780",
    "end": "4741280"
  },
  {
    "text": "what our optimization\nproblem looks like, we have this U theta prime\nand our constraint function,",
    "start": "4741280",
    "end": "4748989"
  },
  {
    "text": "remember, that's our other knob\nthat we're introducing here, is just this expression here. So all we're doing is\nwe're looking at theta",
    "start": "4748990",
    "end": "4755410"
  },
  {
    "text": "prime minus theta,\nthe transpose of that. This is the identity matrix\nhere, and then theta prime minus",
    "start": "4755410",
    "end": "4761700"
  },
  {
    "text": "theta. OK, so that's just\nthe two norm, right? So if we're working in\na two-dimensional space,",
    "start": "4761700",
    "end": "4767090"
  },
  {
    "text": "that's just like the Euclidean\ndistance between our theta and our theta prime. So we're just saying, keep\nthat distance less than",
    "start": "4767090",
    "end": "4774010"
  },
  {
    "text": "or equal to some epsilon. So don't move theta too far. That's all we're saying here. That's what that expression\nwritten out is saying.",
    "start": "4774010",
    "end": "4781480"
  },
  {
    "text": "So very quickly, wrapping\nthis up, we can basically--",
    "start": "4781480",
    "end": "4788890"
  },
  {
    "text": "so we can make some\nsimplifications here, but essentially,\nwhat we're left with is that restricted\ngradient is trying",
    "start": "4788890",
    "end": "4794560"
  },
  {
    "text": "to maximize this expression\nsubject to keeping our constraints, our\ntheta prime, and our theta",
    "start": "4794560",
    "end": "4801400"
  },
  {
    "text": "within some epsilon\ndistance, OK? All of the other methods\nthat we didn't get to, they're basically just\ndifferent variations",
    "start": "4801400",
    "end": "4807880"
  },
  {
    "text": "of this, where they're\nchanging the objective or changing the\nconstraint function. So zooming back out again now\nto the big picture to wrap up,",
    "start": "4807880",
    "end": "4815650"
  },
  {
    "text": "all we've done today is we've\nlooked at policy optimization, right? So we started with\npolicy search,",
    "start": "4815650",
    "end": "4821980"
  },
  {
    "text": "where we had some\nparameterized policy. And we made these\nlocal changes to it",
    "start": "4821980",
    "end": "4827170"
  },
  {
    "text": "without looking at any\ngradient information. Then we switched and we\nstarted looking at, OK, how can I estimate the\ngradient of this policy.",
    "start": "4827170",
    "end": "4833960"
  },
  {
    "text": "And then we sort concluded with\nlooking at, how can I then use that gradient estimate\nto optimize my policy",
    "start": "4833960",
    "end": "4841300"
  },
  {
    "text": "subject to these constraints\nthat I don't move too far, OK? So that is policy\noptimization in a nutshell.",
    "start": "4841300",
    "end": "4847610"
  },
  {
    "text": "I hope that was helpful to you. I hope you got something out\nof it, and we'll wrap up here. Thank you all for listening, and\nI'm happy to take any questions.",
    "start": "4847610",
    "end": "4855510"
  },
  {
    "start": "4855510",
    "end": "4859000"
  }
]