[
  {
    "start": "0",
    "end": "38000"
  },
  {
    "text": "okay so hello everyone my name is Alie hon I am at BYO research actually at the",
    "start": "11320",
    "end": "17800"
  },
  {
    "text": "Silicon Valley AI lab uh which is part of the broader BYO research and like",
    "start": "17800",
    "end": "23840"
  },
  {
    "text": "Andy said today I'm going to talk about speech recognition and in particular a focus on how we've been using deep",
    "start": "23840",
    "end": "29640"
  },
  {
    "text": "learning at by research to uh to make progress in speech",
    "start": "29640",
    "end": "35079"
  },
  {
    "text": "recognition so I guess the thing that we're really interested in doing at uh at at this",
    "start": "35079",
    "end": "42280"
  },
  {
    "start": "38000",
    "end": "143000"
  },
  {
    "text": "silen Valley AI lab is really solving speech recognition right and when I say",
    "start": "42280",
    "end": "47399"
  },
  {
    "text": "solving speech recognition I think what I mean and what we mean is uh not that it kind of works",
    "start": "47399",
    "end": "54199"
  },
  {
    "text": "sometimes when you're holding your phone really close to your mouth and maybe you're speaking like particular really",
    "start": "54199",
    "end": "60280"
  },
  {
    "text": "slowly and clearly and like loudly so that it understands you and you have to",
    "start": "60280",
    "end": "65439"
  },
  {
    "text": "say the same thing a couple of times uh what we mean rather is that speech recognition works in say a robust or",
    "start": "65439",
    "end": "73439"
  },
  {
    "text": "wide array of contexts so a couple of examples uh",
    "start": "73439",
    "end": "79520"
  },
  {
    "text": "might be say one you're driving right and you put your phone on your passenger seat and there's a lot of noise because",
    "start": "79520",
    "end": "87360"
  },
  {
    "text": "cars are noisy and uh and maybe the windows are down maybe the radio is on and speech",
    "start": "87360",
    "end": "94360"
  },
  {
    "text": "recognition isn't going to work there most of the time uh and I and I certainly know that humans work really",
    "start": "94360",
    "end": "100399"
  },
  {
    "text": "well they like they can recognize my speech so why can't a machine",
    "start": "100399",
    "end": "106040"
  },
  {
    "text": "uh another example might be say I'm watching TV and I'm sitting kind of far",
    "start": "106040",
    "end": "112439"
  },
  {
    "text": "away from the television uh and the TV is on right so it's even creating its own noise if there's volume coming out",
    "start": "112439",
    "end": "118000"
  },
  {
    "text": "of the TV uh Speech if I want to interact with that TV using my voice like using speech as my interface uh",
    "start": "118000",
    "end": "125399"
  },
  {
    "text": "that's not going to work so that's what I mean when when I say s speech recognition those are the",
    "start": "125399",
    "end": "131039"
  },
  {
    "text": "kinds of examples that we're thinking about okay so deep speech is uh kind of",
    "start": "131039",
    "end": "136080"
  },
  {
    "text": "the latest and greatest research uh coming out of out of BYU to try and Tackle these sorts of",
    "start": "136080",
    "end": "141959"
  },
  {
    "text": "problems okay so here's an outline of the things I'm going to cover uh and",
    "start": "141959",
    "end": "148000"
  },
  {
    "start": "143000",
    "end": "265000"
  },
  {
    "text": "actually my goal was to talk about deep speech like originally when I came to do",
    "start": "148000",
    "end": "153319"
  },
  {
    "text": "this I wanted to mostly talk about deep speech but I actually realized that uh a lot of you are not experts in speech",
    "start": "153319",
    "end": "159440"
  },
  {
    "text": "recognition and a lot of you depending on when you were born are also not experts in deep learning um so I'm going",
    "start": "159440",
    "end": "166280"
  },
  {
    "text": "to spend probably most of the time actually just talking about what's going on in the world with speech",
    "start": "166280",
    "end": "172239"
  },
  {
    "text": "recognition uh so I'll first get into kind of where does speech where is",
    "start": "172239",
    "end": "177760"
  },
  {
    "text": "speech recognition used uh what why does it work why does it not work uh and and how does it work for that matter I'll",
    "start": "177760",
    "end": "183920"
  },
  {
    "text": "get into some of the details of how how does it actually work uh similarly similarly for deep learning I'll talk",
    "start": "183920",
    "end": "189720"
  },
  {
    "text": "about some of the the details of deep learning uh hopefully without getting lost in the the technical weeds this is",
    "start": "189720",
    "end": "196680"
  },
  {
    "text": "a little easy to do with deep learning but uh I'll also get into some of the",
    "start": "196680",
    "end": "202519"
  },
  {
    "text": "applications uh and hopefully leave you guys with some intuition for why why it works so well or why it's so popular",
    "start": "202519",
    "end": "208480"
  },
  {
    "text": "right now uh at this point uh hopefully you guys",
    "start": "208480",
    "end": "213799"
  },
  {
    "text": "should have enough context about speech recognition deep learning uh and then I will get into deep speech and talk about",
    "start": "213799",
    "end": "220879"
  },
  {
    "text": "some of the key ingredients that we have been using to scale up deep learning and",
    "start": "220879",
    "end": "226560"
  },
  {
    "text": "speech recognition and really solve some of these or make progress I should say on some of these",
    "start": "226560",
    "end": "232519"
  },
  {
    "text": "problems the last thing is I'll I'll just briefly cover some next steps both in terms of our research directions but",
    "start": "232519",
    "end": "238280"
  },
  {
    "text": "also for you guys if you're interested in learning more for those of you that don't already know a lot about deep learning or speech",
    "start": "238280",
    "end": "244159"
  },
  {
    "text": "recognition and uh I I take it these uh these seminar series are usually pretty",
    "start": "244159",
    "end": "250159"
  },
  {
    "text": "interactive so you know don't hesitate to interrupt me with questions as I'm",
    "start": "250159",
    "end": "255760"
  },
  {
    "text": "talking at any point sound good yes great okay so let's",
    "start": "255760",
    "end": "262440"
  },
  {
    "text": "get started okay so the state of speech",
    "start": "262440",
    "end": "267560"
  },
  {
    "start": "265000",
    "end": "575000"
  },
  {
    "text": "recognition so as a I alluded to earlier with a couple of examples there are still many failure modes of speech",
    "start": "267840",
    "end": "274560"
  },
  {
    "text": "recognition one of the biggest failure modes at least in my in my experience uh",
    "start": "274560",
    "end": "279680"
  },
  {
    "text": "is and kind of with some quantitative backing is low signal to noise ratio",
    "start": "279680",
    "end": "285919"
  },
  {
    "text": "so low SNR signal to noise ratio can be brought about for several reasons perhaps the most easy to see is when",
    "start": "285919",
    "end": "293160"
  },
  {
    "text": "there's a lot of background noise so imagine that you guys were all talking right now and I wanted to have my phone",
    "start": "293160",
    "end": "298880"
  },
  {
    "text": "recognize my speak there's going to be a lot of noise so it's going to be less clear to the phone what the actual signal is that it should be focusing on",
    "start": "298880",
    "end": "305880"
  },
  {
    "text": "right low SNR can also be brought about for a couple of other reasons so so one reason I guess is what's",
    "start": "305880",
    "end": "313360"
  },
  {
    "text": "called farfield speech recognition so in the in the TV example that I I mentioned earlier where you're kind of sitting far",
    "start": "313360",
    "end": "319479"
  },
  {
    "text": "away from your television right the microphone is actually going to be in the television and maybe you're 10 feet away from it so the signal is going to",
    "start": "319479",
    "end": "327080"
  },
  {
    "text": "die out before it has a chance to uh at least it will partially die out before it has a chance to get all the way to",
    "start": "327080",
    "end": "333039"
  },
  {
    "text": "that microphone one imagines that there are actors on screen",
    "start": "333039",
    "end": "339280"
  },
  {
    "text": "talking yeah and that would be a source of background noise right",
    "start": "339560",
    "end": "346000"
  },
  {
    "text": "yeah uh another potential source of low SNR is what's called reverberation so",
    "start": "346000",
    "end": "351080"
  },
  {
    "text": "just like Echo basically uh when when you're trying to speak into a microphone right like some of the sound goes",
    "start": "351080",
    "end": "356759"
  },
  {
    "text": "straight towards the microphone but a lot of it actually bounces off random places like say the walls or you know",
    "start": "356759",
    "end": "363120"
  },
  {
    "text": "you guys and will eventually arrive back at the microphone but it will just arrive at a slightly later time so the",
    "start": "363120",
    "end": "369919"
  },
  {
    "text": "the signal is going to get kind of smeared out and that's a that's a source of noise too okay so that's low SNR that's one",
    "start": "369919",
    "end": "377199"
  },
  {
    "text": "big failure mode of speech recognition another big failure mode is Speaker",
    "start": "377199",
    "end": "382759"
  },
  {
    "text": "variability so actually uh so Adam coats who's the director of uh the the lab",
    "start": "382759",
    "end": "389240"
  },
  {
    "text": "that I'm part of the Silicon Valley aab he uh told me this fun Story how he has a friend uh who was really excited when",
    "start": "389240",
    "end": "396240"
  },
  {
    "text": "we published our deep speech results uh because his friend has a really thick Italian accent and no speech recognizers",
    "start": "396240",
    "end": "403720"
  },
  {
    "text": "work for this guy like none none of the ones on the market uh and he was hoping that deep speech would help make",
    "start": "403720",
    "end": "409280"
  },
  {
    "text": "progress on this uh we still have a lot of work cut out to solve this problem",
    "start": "409280",
    "end": "414560"
  },
  {
    "text": "but uh we're hoping that it will make progress on it as well your honesty is appreciated",
    "start": "414560",
    "end": "421520"
  },
  {
    "text": "so accents is one source of speaker variability it's not the only source though I mean other sources might be",
    "start": "421759",
    "end": "427879"
  },
  {
    "text": "differences in age differences in gender uh even the length of your vocal tract",
    "start": "427879",
    "end": "433680"
  },
  {
    "text": "which varies just on a human to human basis can make the signal look very different to a machine so some of these",
    "start": "433680",
    "end": "440599"
  },
  {
    "text": "were actually already pretty good at sorry yes how much of this is oriented toward multilingual you know English is",
    "start": "440599",
    "end": "447479"
  },
  {
    "text": "one thing but Mand yeah so that's Chinese dialects others",
    "start": "447479",
    "end": "453639"
  },
  {
    "text": "that's a good question uh right now I'm mostly talking about",
    "start": "453639",
    "end": "459319"
  },
  {
    "text": "English speech recognition uh and generally like there have not been good",
    "start": "459319",
    "end": "464520"
  },
  {
    "text": "solutions for one type of recognizer which can recognize multiple languages you kind of have to instantiate",
    "start": "464520",
    "end": "470879"
  },
  {
    "text": "different recognizers for different languages uh that said though I believe that our",
    "start": "470879",
    "end": "476800"
  },
  {
    "text": "algorithm if you were to take it and plop a bunch of Mandarin data I believe it would work and that's things that",
    "start": "476800",
    "end": "483440"
  },
  {
    "text": "we're kind of looking into doing would not also help you the accent if you know what the accent came from yeah",
    "start": "483440",
    "end": "490319"
  },
  {
    "text": "potentially but it sometimes it's hard to know where the accent came from right you should figure it out I agree yeah I",
    "start": "490319",
    "end": "498400"
  },
  {
    "text": "agree cool so speaker variability is another big failure mode of",
    "start": "498400",
    "end": "503680"
  },
  {
    "text": "speech a third one is just natural and conversational speech so when you're",
    "start": "503680",
    "end": "509199"
  },
  {
    "text": "talking to your phone right you're speaking in a really effective manner like almost like you're reading off of a",
    "start": "509199",
    "end": "514360"
  },
  {
    "text": "sheet of paper into your phone or like you were like scolding a 2-year-old or something you're speaking really loudly",
    "start": "514360",
    "end": "520080"
  },
  {
    "text": "and slowly so conversational speech is like the other side of that if I were to try",
    "start": "520080",
    "end": "526160"
  },
  {
    "text": "to transcribe like a phone conversation that two of us might have or or even me",
    "start": "526160",
    "end": "531240"
  },
  {
    "text": "talking right now transcribing this is actually quite hard for a machine and the reason that is is because I actually",
    "start": "531240",
    "end": "537480"
  },
  {
    "text": "change a lot of my property the properties of speech as I'm talking in a natural way uh like the tempo I change",
    "start": "537480",
    "end": "543680"
  },
  {
    "text": "the rate at which I speak sometimes I speak really quickly sometimes I speak more slowly without even thinking about",
    "start": "543680",
    "end": "549680"
  },
  {
    "text": "it uh other things that I do are are called they're called disfluencies and",
    "start": "549680",
    "end": "554760"
  },
  {
    "text": "these are things like starting and stopping A word without finishing it or us and ums these sorts of things that",
    "start": "554760",
    "end": "561440"
  },
  {
    "text": "you don't you don't typically do when you're dictating into your phone but you might you might do or you actually",
    "start": "561440",
    "end": "566839"
  },
  {
    "text": "always do when you're speaking naturally okay so those are some of the failure",
    "start": "566839",
    "end": "571880"
  },
  {
    "text": "modes of speech recognition now actually recently Google",
    "start": "571880",
    "end": "577800"
  },
  {
    "start": "575000",
    "end": "645000"
  },
  {
    "text": "actually published I found a really interesting uh analysis on where people are currently using speech",
    "start": "577800",
    "end": "584839"
  },
  {
    "text": "recognition and they split it into two demographics uh teens and adults as you can see here",
    "start": "584839",
    "end": "591680"
  },
  {
    "text": "uh so the the biggest place was for mostly for both groups was amongst",
    "start": "591680",
    "end": "597839"
  },
  {
    "text": "friends another really surprising place was in the bathroom not entirely sure what people are doing with speech",
    "start": "597839",
    "end": "604079"
  },
  {
    "text": "recognition in the bathroom but all right so they're using speech recognition in the bathroom uh other",
    "start": "604079",
    "end": "610079"
  },
  {
    "text": "places are as you can see like while exercising while cooking more so while",
    "start": "610079",
    "end": "615480"
  },
  {
    "text": "cooking adults than teens which makes sense uh but all of these places are you",
    "start": "615480",
    "end": "621720"
  },
  {
    "text": "can imagine easily places where you would expect a lot of noise how did Google learn this I think surve is but I",
    "start": "621720",
    "end": "630959"
  },
  {
    "text": "can't recall I have this this link here if you're curious to know more about the study you can follow that I think they",
    "start": "630959",
    "end": "637360"
  },
  {
    "text": "will give more details yep so those are some of the places that",
    "start": "637360",
    "end": "644560"
  },
  {
    "text": "people are using speech recognition they also oh sorry they also uh they also",
    "start": "644560",
    "end": "651839"
  },
  {
    "start": "645000",
    "end": "722000"
  },
  {
    "text": "published some of the tasks that people achieve with speech recognition so the",
    "start": "651839",
    "end": "657279"
  },
  {
    "text": "most common of these tasks were such things as trying to call someone on your",
    "start": "657279",
    "end": "662399"
  },
  {
    "text": "phone so I would say like call Jesse and Jesse might be in my contact book others were were I think here you",
    "start": "662399",
    "end": "669800"
  },
  {
    "text": "see like check the time setting your alarm I don't think that's on the list but that's kind of in the same bucket uh",
    "start": "669800",
    "end": "677160"
  },
  {
    "text": "but you might notice looking at these tasks they have kind of one thing in common and that's that they're all very",
    "start": "677160",
    "end": "682880"
  },
  {
    "text": "very simple right you don't need a huge vocabulary to understand these types of",
    "start": "682880",
    "end": "688399"
  },
  {
    "text": "things uh and and thus they're actually really easy for typically much easier for a machine to do at least to",
    "start": "688399",
    "end": "695800"
  },
  {
    "text": "transcribe your your speech then then would be say something like composing a",
    "start": "695800",
    "end": "701079"
  },
  {
    "text": "text message or transcribing a voicemail or transcribing like a lecture",
    "start": "701079",
    "end": "707639"
  },
  {
    "text": "for example uh and and those things aren't really at least in terms of how people are using their phones for speech",
    "start": "707639",
    "end": "713680"
  },
  {
    "text": "recognition they're not up on the list not even close and that's because speech recognition doesn't really work there",
    "start": "713680",
    "end": "719000"
  },
  {
    "text": "yet all right so I don't know if you guys have",
    "start": "719000",
    "end": "726120"
  },
  {
    "start": "722000",
    "end": "924000"
  },
  {
    "text": "noticed but I've certainly noticed a lot of devices coming out lately that are",
    "start": "726120",
    "end": "731639"
  },
  {
    "text": "essentially driven in their interfaces with only speech recognition so I saw",
    "start": "731639",
    "end": "737000"
  },
  {
    "text": "one of these SmartWatches earlier uh which you can I think swipe you know",
    "start": "737000",
    "end": "742240"
  },
  {
    "text": "side to side on them but if you want to interact with them in a meaningful way the way to do it is with speech right",
    "start": "742240",
    "end": "750360"
  },
  {
    "text": "and that's not a seamless process I don't think uh and that's because speech recognition doesn't work very well there",
    "start": "750360",
    "end": "756680"
  },
  {
    "text": "so it's it'd be cool to be able to you know like talk to your watch as you're walking and compose a text message but",
    "start": "756680",
    "end": "762639"
  },
  {
    "text": "that's not a use case that is very common yeah uh BYU has this really cool",
    "start": "762639",
    "end": "767920"
  },
  {
    "text": "product called by ey which is the one up here on the upper right and you wear",
    "start": "767920",
    "end": "774040"
  },
  {
    "text": "this thing around your head right around the back of your head and it's got a camera on your right ear and a a speaker",
    "start": "774040",
    "end": "781079"
  },
  {
    "text": "in your left ear and it can kind of see and perceive things in in your field of vision and the way that you communicate",
    "start": "781079",
    "end": "787360"
  },
  {
    "text": "with it is via speech recognition solely with speech recognition and then it talks to your phone via Bluetooth so for that to be a",
    "start": "787360",
    "end": "794880"
  },
  {
    "text": "seamless experience uh we need to really make progress on at least the natural and low SNR uh failure modes of speech",
    "start": "794880",
    "end": "803040"
  },
  {
    "text": "recognition I'm not sure if you guys have seen this probably have the Amazon Echo it's essentially a smart speaker",
    "start": "803040",
    "end": "809800"
  },
  {
    "text": "which can do things like uh let's see it can you can add things to your shopping",
    "start": "809800",
    "end": "814880"
  },
  {
    "text": "cart with it say like Echo add milk to my shopping cart or or you can ask it to",
    "start": "814880",
    "end": "820160"
  },
  {
    "text": "play music you can say Echo turn on some pop music but the funny thing is",
    "start": "820160",
    "end": "825320"
  },
  {
    "text": "anecdotally I've heard that when you ask the Amazon Echo to turn on music it",
    "start": "825320",
    "end": "831440"
  },
  {
    "text": "starts playing music and creates kind of a cloud of noise around itself and you can no longer communicate with it",
    "start": "831440",
    "end": "837560"
  },
  {
    "text": "because speech isn't going to work anymore so this is a problem right we need to",
    "start": "837560",
    "end": "842720"
  },
  {
    "text": "make progress in the in the noisy setting uh the other cool thing about Amazon Echo actually I think well the",
    "start": "842720",
    "end": "849160"
  },
  {
    "text": "other interesting thing is that it has seven microphones uh and the reason that it has all of these microphones is",
    "start": "849160",
    "end": "855920"
  },
  {
    "text": "because it uses all of them to kind of uh zoom in on who it's trying to listen",
    "start": "855920",
    "end": "860959"
  },
  {
    "text": "to this is a technique called beam forming which which allows it to use multiple",
    "start": "860959",
    "end": "866600"
  },
  {
    "text": "microphones to find the source of signal uh but I think this is interesting because we as humans we only really have",
    "start": "866600",
    "end": "873120"
  },
  {
    "text": "two microphones right we just have our two ears and yet we're able to transcribe speech much much better than",
    "start": "873120",
    "end": "879519"
  },
  {
    "text": "the Amazon Echo so I think this suggests to me that there's something algorithmic",
    "start": "879519",
    "end": "885720"
  },
  {
    "text": "that's missing from from this picture oh so your ears are shaped our",
    "start": "885720",
    "end": "891240"
  },
  {
    "text": "ears are shaped nicely this is true yep it's probably pretty important yeah and",
    "start": "891240",
    "end": "897079"
  },
  {
    "text": "we have a lot of infrastructure going on in here that I'm glossing over when I say our ears are identical to two",
    "start": "897079",
    "end": "902399"
  },
  {
    "text": "microphones this is true but we certainly only have two",
    "start": "902399",
    "end": "907440"
  },
  {
    "text": "sources okay so here's some some kind of history of speech recognition over over",
    "start": "910079",
    "end": "915320"
  },
  {
    "text": "the past half century or so I hope I got this right uh",
    "start": "915320",
    "end": "920880"
  },
  {
    "text": "so the the early days of speech it was really trivial like you would just take",
    "start": "920880",
    "end": "926839"
  },
  {
    "text": "a a single word template match it against your database of other words and that's the only thing you could do uh",
    "start": "926839",
    "end": "932920"
  },
  {
    "text": "this so speech recognition improved due to primarily what I think are two breakthroughs two big uh sources of",
    "start": "932920",
    "end": "939880"
  },
  {
    "text": "steep Improvement in performance right so if you look at over the year you see kind of these two humps where speech",
    "start": "939880",
    "end": "946079"
  },
  {
    "text": "recognition really got a lot better and the first wasn't quite such a crystal clear hump but it was primarily due to",
    "start": "946079",
    "end": "952680"
  },
  {
    "text": "the introduction of hidden Markov models and speech recognition and hidden Markov models are basically a way of",
    "start": "952680",
    "end": "959480"
  },
  {
    "text": "statistically understanding time series data and over the 70s to the present day",
    "start": "959480",
    "end": "966120"
  },
  {
    "text": "they have been at the core of all speech recognition technology and have allowed it to improve uh more recently the",
    "start": "966120",
    "end": "973160"
  },
  {
    "text": "introduction of deep learning to one of the small components of well a large component but a subcomponent of the",
    "start": "973160",
    "end": "979920"
  },
  {
    "text": "speech recognition pipeline uh you see this in 2012 actually increased",
    "start": "979920",
    "end": "985040"
  },
  {
    "text": "performance a lot but other than that most Improvement",
    "start": "985040",
    "end": "990079"
  },
  {
    "text": "has been actually prettyy marginal and slow and I don't I I don't know about you guys but I don't want to wait",
    "start": "990079",
    "end": "995519"
  },
  {
    "text": "another you know 40 years for the next big Improvement in speech recognition so",
    "start": "995519",
    "end": "1000639"
  },
  {
    "text": "we're asking ourselves the question of like how can we get another one of these big big breakthroughs in speech",
    "start": "1000639",
    "end": "1007240"
  },
  {
    "text": "recognition well how would you say that the performance improvements happen like",
    "start": "1008360",
    "end": "1015120"
  },
  {
    "text": "a lot of the claims for early artificial intelligence work were really only because of device individual device",
    "start": "1015120",
    "end": "1022160"
  },
  {
    "text": "speed up in the hardware technology yet the software guys CL you know",
    "start": "1022160",
    "end": "1028120"
  },
  {
    "text": "overclaimed a lot of things I think that's totally fair I think basically everything you see from hmm onward more",
    "start": "1028120",
    "end": "1035600"
  },
  {
    "text": "SL is more SL yeah I think that's true like the reason we were able to do large",
    "start": "1035600",
    "end": "1040720"
  },
  {
    "text": "vocabulary continuous speech recognition in the 9s didn't have anything to do with changing the underlying algorithms",
    "start": "1040720",
    "end": "1047360"
  },
  {
    "text": "for the most part just suggesting that deep learning gives you gave you a slightly better higher jump",
    "start": "1047360",
    "end": "1055280"
  },
  {
    "text": "than hidden marker I'm not I'm not implicitly suggesting that in this picture I don't know it's not drawn to",
    "start": "1055280",
    "end": "1060600"
  },
  {
    "text": "scale yeah sorry uh and even deep learning You could argue that I mean",
    "start": "1060600",
    "end": "1065720"
  },
  {
    "text": "neural networks have been around for a long time right you could argue that now we can actually train them successfully",
    "start": "1065720",
    "end": "1071000"
  },
  {
    "text": "because of the the hardware that's available to us by saying successfully",
    "start": "1071000",
    "end": "1079400"
  },
  {
    "text": "sorry one more time you can you're assuming a lot to say that you can train them successfully you can certainly",
    "start": "1079520",
    "end": "1087000"
  },
  {
    "text": "train right right okay",
    "start": "1087000",
    "end": "1093440"
  },
  {
    "text": "so how does speech recognition work today well it all starts when I record",
    "start": "1093720",
    "end": "1099840"
  },
  {
    "start": "1095000",
    "end": "1325000"
  },
  {
    "text": "some snippet of audio someone saying something and I put it into a machine",
    "start": "1099840",
    "end": "1105120"
  },
  {
    "text": "and then that audio goes into an acoustic model so the job of the acoustic model is actually to predict",
    "start": "1105120",
    "end": "1112360"
  },
  {
    "text": "what phones might be present in the audio so sound is at least human speech",
    "start": "1112360",
    "end": "1117919"
  },
  {
    "text": "is made up of phones and the job of the acoustic model is to guess what phones you might have just said those phones",
    "start": "1117919",
    "end": "1124880"
  },
  {
    "text": "then go into a phony model and the job of the phony model is to try to make sense of what words might be present in",
    "start": "1124880",
    "end": "1131640"
  },
  {
    "text": "the guest phones from the acoustic model and those words those predicted",
    "start": "1131640",
    "end": "1137200"
  },
  {
    "text": "words essentially go into the language model and the job of the language model is to assign higher probability to words",
    "start": "1137200",
    "end": "1144120"
  },
  {
    "text": "that are likely to co-occur together and lower probability to words that are unlikely to co-occur together so an",
    "start": "1144120",
    "end": "1149480"
  },
  {
    "text": "example might be like say I said the catat and the",
    "start": "1149480",
    "end": "1154720"
  },
  {
    "text": "hatat uh the catat is much more probable because a cat can sit but the hatat",
    "start": "1154720",
    "end": "1160840"
  },
  {
    "text": "sounds similar so maybe the acoustic model and the phony model didn't get it but the language model can come in and",
    "start": "1160840",
    "end": "1166240"
  },
  {
    "text": "say no that that couldn't possibly have been said because the Hat that doesn't make any sense so from that I'm able to get an",
    "start": "1166240",
    "end": "1173559"
  },
  {
    "text": "output transcription okay so a very high level that's how that's the the the",
    "start": "1173559",
    "end": "1179679"
  },
  {
    "text": "pipeline and I actually want to get a bit more into the acoustic model because",
    "start": "1179679",
    "end": "1185960"
  },
  {
    "text": "that's where a lot of the kind of the magic of today's state-of-the-art speech",
    "start": "1185960",
    "end": "1191280"
  },
  {
    "text": "recognition happens so what's going on in the",
    "start": "1191280",
    "end": "1196480"
  },
  {
    "text": "acoustic model okay so the first step is the stage of",
    "start": "1196480",
    "end": "1202240"
  },
  {
    "text": "the stage called feature extraction so essentially what this stage is doing",
    "start": "1202240",
    "end": "1208280"
  },
  {
    "text": "is taking the time domain audio and converting it into a frequency domain but it's not just doing a simple foro",
    "start": "1208280",
    "end": "1214240"
  },
  {
    "text": "transform it's a it's it's trying to use uh trying to take insights from how",
    "start": "1214240",
    "end": "1219679"
  },
  {
    "text": "humans hear actually how we perceive sound and use that knowledge uh to",
    "start": "1219679",
    "end": "1225320"
  },
  {
    "text": "better extract relevant features from the audio so the simplest thing is that we",
    "start": "1225320",
    "end": "1230440"
  },
  {
    "text": "actually only hear between I think 2 and 20,000 Kilz so anything outside of the",
    "start": "1230440",
    "end": "1237280"
  },
  {
    "text": "frequency 20 sorry anything outside of two uh 20 and 20,000 Hertz I Miss B uh",
    "start": "1237280",
    "end": "1246280"
  },
  {
    "text": "anything outside of 20,000 Hertz we're going to just cut off right because",
    "start": "1246280",
    "end": "1251360"
  },
  {
    "text": "humans can't hear that so presumably there's no useful speech in that in that range the other interesting thing about",
    "start": "1251360",
    "end": "1258240"
  },
  {
    "text": "the way humans here is we actually hear on a nonlinear scale as we increase frequency so uh we might hear the",
    "start": "1258240",
    "end": "1266640"
  },
  {
    "text": "difference between 100 and 200 Hertz in the same way that we hear the difference",
    "start": "1266640",
    "end": "1271960"
  },
  {
    "text": "between say 10,000 and 20,000 htz so we're going to take account that nonlinear scale it's called the the Mel",
    "start": "1271960",
    "end": "1278120"
  },
  {
    "text": "scale in speech recognition we're going to take that into account when we extract features is that a lot different",
    "start": "1278120",
    "end": "1283720"
  },
  {
    "text": "than just working at a log scale it's similar it's similar it's not quite a log scale but yeah mean there also like",
    "start": "1283720",
    "end": "1289960"
  },
  {
    "text": "similar nonlinearities when it comes to volume and stuff like that does that",
    "start": "1289960",
    "end": "1295600"
  },
  {
    "text": "have a notable Improvement than just working in the log scale just uh yes it has in the past uh you'll see",
    "start": "1295600",
    "end": "1304799"
  },
  {
    "text": "that we don't actually use this currently for deep speech okay so I would argue that if you I would argue",
    "start": "1304799",
    "end": "1311559"
  },
  {
    "text": "later and I will argue later that if you have enough data and the right models probably not",
    "start": "1311559",
    "end": "1317279"
  },
  {
    "text": "yeah so okay so feature extraction and then the next stage after feature extraction the the feature extraction",
    "start": "1317279",
    "end": "1323279"
  },
  {
    "text": "module sends uh relevant features to speaker adaptation and I mentioned earlier that speaker variability is a",
    "start": "1323279",
    "end": "1329960"
  },
  {
    "start": "1325000",
    "end": "1489000"
  },
  {
    "text": "source of problem for speech recognition and so people uh so one thing we can try",
    "start": "1329960",
    "end": "1335880"
  },
  {
    "text": "to do is to try to kind of push our feutures towards a global speaker or you",
    "start": "1335880",
    "end": "1341240"
  },
  {
    "text": "can also think of it as trying to remove the speaker specific effects of the",
    "start": "1341240",
    "end": "1346559"
  },
  {
    "text": "sound out and try to make everything look the same to the model and they have",
    "start": "1346559",
    "end": "1351919"
  },
  {
    "text": "fairly Advanced statistical methods for doing this but they are still very much prone",
    "start": "1351919",
    "end": "1358320"
  },
  {
    "text": "to error and uh accents are actually still very hard for them so the next step after speaker",
    "start": "1358320",
    "end": "1364400"
  },
  {
    "text": "adaptation is uh is the phoning prediction and this is the part here the",
    "start": "1364400",
    "end": "1369520"
  },
  {
    "text": "phoning prediction which currently is done with deep neural networks and the job of that module is essentially to",
    "start": "1369520",
    "end": "1376919"
  },
  {
    "text": "given the speaker adapted feature predict what phony might be",
    "start": "1376919",
    "end": "1382000"
  },
  {
    "text": "present so the the phony prediction model doesn't have to be a deep neur Network I should say it just has to be",
    "start": "1382600",
    "end": "1387919"
  },
  {
    "text": "any supervised uh machine learning algorithm and I'll actually get more into what that means in a minute but uh",
    "start": "1387919",
    "end": "1394080"
  },
  {
    "text": "first do you guys have any more questions about the state of speech recognition yes could you give some",
    "start": "1394080",
    "end": "1400039"
  },
  {
    "text": "indication of how much CPU goes into each of these steps yeah so feature extraction is",
    "start": "1400039",
    "end": "1406080"
  },
  {
    "text": "pretty cheap uh I mean if you have a huge data set",
    "start": "1406080",
    "end": "1412799"
  },
  {
    "text": "you're going to use a lot of CPUs because uh so to answer your question it's hard to answer your question in a",
    "start": "1412799",
    "end": "1419240"
  },
  {
    "text": "generic framework but if I if I am just going to speak to my my phone and I want",
    "start": "1419240",
    "end": "1424320"
  },
  {
    "text": "it to recognize that speech in just like the one example case then the feature",
    "start": "1424320",
    "end": "1430120"
  },
  {
    "text": "extraction is pretty cheap like I can do that extremely quickly with a single thread uh the speaker adaptation",
    "start": "1430120",
    "end": "1436000"
  },
  {
    "text": "likewise uh the problem with speaker adaptation is not the computational complexity of it or the expense but uh",
    "start": "1436000",
    "end": "1442640"
  },
  {
    "text": "actually knowing who the speaker is and knowing enough properties about that speaker to then remove the the",
    "start": "1442640",
    "end": "1448640"
  },
  {
    "text": "information that is kind of different about that speaker yes how do we know that phonemes are the right things to be",
    "start": "1448640",
    "end": "1454159"
  },
  {
    "text": "looking for we don't and I'm going to tell you that they're not and we should look for other things",
    "start": "1454159",
    "end": "1460919"
  },
  {
    "text": "sell USS not quite but it but so let me just finish answering your question uh the",
    "start": "1460919",
    "end": "1467200"
  },
  {
    "text": "last the last part the phoning prediction model is actually the most expensive and and they still typically",
    "start": "1467200",
    "end": "1472279"
  },
  {
    "text": "do this with just single CPUs with one example a single thread one one CPU and this is one box of three or four on the",
    "start": "1472279",
    "end": "1479399"
  },
  {
    "text": "previous slide how big is that right uh as in this slide",
    "start": "1479399",
    "end": "1485120"
  },
  {
    "text": "here uh so that that the acoustic modeling is by far the most expensive",
    "start": "1485120",
    "end": "1490799"
  },
  {
    "start": "1489000",
    "end": "1657000"
  },
  {
    "text": "part uh that's not true actually the acoustic modeling is expensive and then you have to do this process of decoding",
    "start": "1490799",
    "end": "1496240"
  },
  {
    "text": "in order to actually retrieve the correct word uh again most of the stuff is",
    "start": "1496240",
    "end": "1502279"
  },
  {
    "text": "typically done with a single thread and it can be done in uh so we kind of have",
    "start": "1502279",
    "end": "1507559"
  },
  {
    "text": "this way of measuring speech is called like Factor real time so if I'm speaking",
    "start": "1507559",
    "end": "1513000"
  },
  {
    "text": "can I do like a half real time with a machine transcribing my speech at a half real time uh and",
    "start": "1513000",
    "end": "1519640"
  },
  {
    "text": "usually uh production recognizers can do production speech recognizers can do something like 70% Real Time with a",
    "start": "1519640",
    "end": "1526399"
  },
  {
    "text": "single CPU yes is stuff like noise cancellation and",
    "start": "1526399",
    "end": "1532679"
  },
  {
    "text": "that kind of stuff were you wrapping all that into feature extraction uh so no I",
    "start": "1532679",
    "end": "1537720"
  },
  {
    "text": "wasn't uh there there is very the problem with noise cancellation is that different Hardware sometimes does it in",
    "start": "1537720",
    "end": "1544080"
  },
  {
    "text": "different ways uh even on the front end before we see the signal in the speech recognizer so typically uh Speech",
    "start": "1544080",
    "end": "1551159"
  },
  {
    "text": "recognizers don't even try to do any kind of noise cancellation they just take the signal and assume it's been properly scrubbed from so I was thinking",
    "start": "1551159",
    "end": "1557679"
  },
  {
    "text": "like in the case the speaker that you mentioned like it knows what song it's playing and so hypothetically just zero",
    "start": "1557679",
    "end": "1563600"
  },
  {
    "text": "that out yeah the I think a lot of the methods they they use now are attempting to do this without explicitly kind of",
    "start": "1563600",
    "end": "1570159"
  },
  {
    "text": "saying that we're removing like this type of background noise yes is it known among these boxes",
    "start": "1570159",
    "end": "1578080"
  },
  {
    "text": "which one is kind of the most important uh to improving the the output",
    "start": "1578080",
    "end": "1583399"
  },
  {
    "text": "recognition that's a great question uh I think one reason that speech recognition",
    "start": "1583399",
    "end": "1589320"
  },
  {
    "text": "is hard to improve upon is because there are so many subm modules and it's hard",
    "start": "1589320",
    "end": "1595840"
  },
  {
    "text": "to probe the system and actually say if I change this one uh I will get much",
    "start": "1595840",
    "end": "1601960"
  },
  {
    "text": "better results so I'm going to show you our system in a little bit which tries to just replace this whole thing and we",
    "start": "1601960",
    "end": "1609360"
  },
  {
    "text": "don't have we don't suffer from this problem of having to figure out exactly where the failure is occurring in the",
    "start": "1609360",
    "end": "1616320"
  },
  {
    "text": "pipeline yes do you guys employ like neuroscience and level uh not more than",
    "start": "1616320",
    "end": "1624200"
  },
  {
    "text": "just the stealing a few uh a few words of their dialect um I'm I'm I'm kind of",
    "start": "1624200",
    "end": "1632320"
  },
  {
    "text": "going to be careful to uh say yes but because a lot of the stuff we talk about",
    "start": "1632320",
    "end": "1637960"
  },
  {
    "text": "in deep learning is very very Loosely inspired from you know like Neuroscience",
    "start": "1637960",
    "end": "1643279"
  },
  {
    "text": "Concepts about the human brain um but very very Loosely so",
    "start": "1643279",
    "end": "1650158"
  },
  {
    "text": "yeah any other questions okay so let's move on so deep",
    "start": "1651520",
    "end": "1657799"
  },
  {
    "start": "1657000",
    "end": "1844000"
  },
  {
    "text": "learning great okay so deep learning is uh at the at",
    "start": "1657799",
    "end": "1663360"
  },
  {
    "text": "the core of deep learning is uh what are called artificial neural networks and",
    "start": "1663360",
    "end": "1668720"
  },
  {
    "text": "it's socalled deep because we take kind of many of these neural networks and stack them on top of each other you can",
    "start": "1668720",
    "end": "1675240"
  },
  {
    "text": "think of it and at the core of an artificial neural network is a is a neuron they're",
    "start": "1675240",
    "end": "1683440"
  },
  {
    "text": "made up of these little sub components these little neurons uh and what's Happening Here is we have some some",
    "start": "1683440",
    "end": "1691600"
  },
  {
    "text": "input this could be anything like if I had an image this would be pixels say or if I had a speech this would be the",
    "start": "1691600",
    "end": "1700080"
  },
  {
    "text": "frequen the the amount of frequency at each at each band uh and so I have some",
    "start": "1700080",
    "end": "1707000"
  },
  {
    "text": "input I scale that input along these connections or synapses uh and the the W1 W2 W3 W4 that",
    "start": "1707000",
    "end": "1717600"
  },
  {
    "text": "represents actually the strength of these connections so I scale the input by the the strength of those connections",
    "start": "1717600",
    "end": "1724640"
  },
  {
    "text": "when you say scale what exactly I mean multiply okay that's all I mean they can",
    "start": "1724640",
    "end": "1730120"
  },
  {
    "text": "be they can be negative and I take literally take W4 multiply by X1 that's",
    "start": "1730120",
    "end": "1736640"
  },
  {
    "text": "what I mean and there's no constraint on what these numbers can be other than",
    "start": "1736640",
    "end": "1741840"
  },
  {
    "text": "when we actually turn to our problem domain what are the constraints there okay",
    "start": "1741840",
    "end": "1747760"
  },
  {
    "text": "so I then take the results of that I sum them together I aggregate them and I",
    "start": "1747760",
    "end": "1753600"
  },
  {
    "text": "apply what's called a thresholding operation onto that so I uh I kind of",
    "start": "1753600",
    "end": "1759159"
  },
  {
    "text": "squash the output and then if it's above a certain threshold I kind of treat it",
    "start": "1759159",
    "end": "1765000"
  },
  {
    "text": "as this neuron being on and if it's below a certain threshold I treat it as this neuron being",
    "start": "1765000",
    "end": "1771399"
  },
  {
    "text": "off okay how do you determine the threshold the threshold so in practice",
    "start": "1771399",
    "end": "1777799"
  },
  {
    "text": "we don't actually determine the threshold we just let it be soft it's a soft Spectrum uh it's a nice analogy",
    "start": "1777799",
    "end": "1783799"
  },
  {
    "text": "though because you can think of this neuron as activating when the input uh the the the aggregated scaled input is",
    "start": "1783799",
    "end": "1791120"
  },
  {
    "text": "large enough and not activating when it's small enough um but when it comes",
    "start": "1791120",
    "end": "1796399"
  },
  {
    "text": "to actually training these things we treat this is a soft function and that makes it easy to uh for mathematical",
    "start": "1796399",
    "end": "1802720"
  },
  {
    "text": "reasons it makes it easy ratio when you say a soft uh I mean uh sorry uh I mean",
    "start": "1802720",
    "end": "1810360"
  },
  {
    "text": "continuous right that's what I mean and differentiable but this again there are",
    "start": "1810360",
    "end": "1817240"
  },
  {
    "text": "caveats to this this isn't always the case um so you can have lots of things here the simplest thing you can have is",
    "start": "1817240",
    "end": "1823519"
  },
  {
    "text": "something that say squashes your input between Zer and one and if it's above a half turns it on and if it's below half",
    "start": "1823519",
    "end": "1830159"
  },
  {
    "text": "turns it off but that's not the that's not what's usually there",
    "start": "1830159",
    "end": "1836240"
  },
  {
    "text": "okay okay so we get some output from this neuron okay so what what do I actually want to do with this neuron",
    "start": "1837559",
    "end": "1843720"
  },
  {
    "text": "well let me just take a brief aside uh and tell you guys about supervised learning uh for those of you that I know",
    "start": "1843720",
    "end": "1851600"
  },
  {
    "start": "1844000",
    "end": "2103000"
  },
  {
    "text": "probably a lot of you already know a lot about supervised learning but just so everyone's on the same page Okay so",
    "start": "1851600",
    "end": "1858519"
  },
  {
    "text": "say that my goal is to recognize an image uh in this case an image of a",
    "start": "1858519",
    "end": "1864240"
  },
  {
    "text": "coffee mode the way that I'm going to do that is I'm going to take some kind of algorithm we call it a learning",
    "start": "1864240",
    "end": "1870440"
  },
  {
    "text": "algorithm and the only thing that this algorithm has to do is be able to kind of tune its internal State uh Based on",
    "start": "1870440",
    "end": "1877919"
  },
  {
    "text": "data and then and then I'm going to actually get a bunch of examples of",
    "start": "1877919",
    "end": "1885519"
  },
  {
    "text": "images with or without coffee mugs present present and supposedly I'm going to get a human to label these images as",
    "start": "1885519",
    "end": "1892880"
  },
  {
    "text": "having coffee mug or not having coffee mug okay and I'm going to take these images and I'm going to show them one at",
    "start": "1892880",
    "end": "1899000"
  },
  {
    "text": "a time to my learning algorithm and after seeing a bunch of these images coffee mugs are not coffee",
    "start": "1899000",
    "end": "1905080"
  },
  {
    "text": "mugs hopefully it learns what's relevant to predicting a coffee mug and then I can show it an image that it's never",
    "start": "1905080",
    "end": "1911679"
  },
  {
    "text": "seen before without a label and uh it should correctly classify this image is",
    "start": "1911679",
    "end": "1918240"
  },
  {
    "text": "having a coffee mug in it does it need a database of non coffee mugs yeah so all of them here are coffee",
    "start": "1918240",
    "end": "1924440"
  },
  {
    "text": "mugs but it's it's good to have non- examples too yes yeah yeah that's a good point",
    "start": "1924440",
    "end": "1931240"
  },
  {
    "text": "yes okay so supervis Lear now I can actually take my neuron and in order to",
    "start": "1931240",
    "end": "1939840"
  },
  {
    "text": "use it as a supervised learning algorithm I can then feed an image into it and if I correctly classify this as a",
    "start": "1939840",
    "end": "1947440"
  },
  {
    "text": "coffee mode then what I can do is try to determine what features of this image were",
    "start": "1947440",
    "end": "1954559"
  },
  {
    "text": "important to correctly classifying it so say that you know if X1 X2 X3 and X4 are",
    "start": "1954559",
    "end": "1960720"
  },
  {
    "text": "just the pixels of this image right there's probably way more than four pixels but assume that there are just",
    "start": "1960720",
    "end": "1966320"
  },
  {
    "text": "these four then I can try to determine that say X1 and X3 were the relevant",
    "start": "1966320",
    "end": "1973279"
  },
  {
    "text": "pixels to me predict correctly predicting this was a coffee mug and then I can increase inre the strength of",
    "start": "1973279",
    "end": "1978760"
  },
  {
    "text": "the synapses or literally increase the value of W1 and W3 uh so that I'm more likely to predict",
    "start": "1978760",
    "end": "1987080"
  },
  {
    "text": "that it's a coffee muge next time now on the flip side suppose that I",
    "start": "1987080",
    "end": "1993120"
  },
  {
    "text": "incorrectly predicted that it was not a coffee mug well I'm just going to do the same thing I'm going to try to find which weights were responsible for this",
    "start": "1993120",
    "end": "2000039"
  },
  {
    "text": "and decrease the scale from those uh sorry find which features were responsible and decrease the weights",
    "start": "2000039",
    "end": "2007919"
  },
  {
    "text": "for those features now at prediction",
    "start": "2007919",
    "end": "2013039"
  },
  {
    "text": "time what I can do very simple again is just take an image of a coffee mug that I haven't seen feed it through this",
    "start": "2013039",
    "end": "2019799"
  },
  {
    "text": "neuron if the neuron turns on in this case if it is greater than a half I will",
    "start": "2019799",
    "end": "2025039"
  },
  {
    "text": "classify it as a Cofe mode and if it's less than a half I will not",
    "start": "2025039",
    "end": "2030840"
  },
  {
    "text": "okay so it turns out that I can actually make these things much more complex this model this neuron and I can do that in a",
    "start": "2030840",
    "end": "2037880"
  },
  {
    "text": "couple ways one is by wiring many of these neurons together so you see here I've just added",
    "start": "2037880",
    "end": "2043519"
  },
  {
    "text": "another neuron in the this layer here uh I can also stack these",
    "start": "2043519",
    "end": "2050358"
  },
  {
    "text": "layers right so I can have layer one layer two layer three and I can do this",
    "start": "2050359",
    "end": "2055919"
  },
  {
    "text": "in arbitrary amounts right like I can add as many neurons here as I want in each layer and have as many layers as I",
    "start": "2055919",
    "end": "2062599"
  },
  {
    "text": "like you know I'm limited here by computation and data mostly",
    "start": "2062599",
    "end": "2068720"
  },
  {
    "text": "anticyclic what's that anticyclic typically it's better if it's a if it's",
    "start": "2068720",
    "end": "2075320"
  },
  {
    "text": "anti if it's a dag if it's directed and and there's no Cycles yeah typically that's better yeah",
    "start": "2075320",
    "end": "2084079"
  },
  {
    "text": "so there's only one threshold here what do you mean well",
    "start": "2084280",
    "end": "2089440"
  },
  {
    "text": "I four there's four okay in theory in theory it could it could be but usually",
    "start": "2089440",
    "end": "2095960"
  },
  {
    "text": "you choose one or one per layer or something sure yeah okay so instead of showing you guys this image from now on",
    "start": "2095960",
    "end": "2102800"
  },
  {
    "text": "when I'm talking about a neural network I'm going to show you something that looks like this and same thing just a slightly higher abstraction so I have",
    "start": "2102800",
    "end": "2109000"
  },
  {
    "start": "2103000",
    "end": "2266000"
  },
  {
    "text": "neurons in one layer neurons in another layer and these fat arrows just mean that all the neurons in the first layer",
    "start": "2109000",
    "end": "2115079"
  },
  {
    "text": "are kind of densely connected every pairwise connection between neurons in",
    "start": "2115079",
    "end": "2120599"
  },
  {
    "text": "each layer okay and on the far I guess left are uh the",
    "start": "2120599",
    "end": "2127280"
  },
  {
    "text": "input on the far right is the output and everything in the middle we're going to call Hidden layers and they're so-called",
    "start": "2127280",
    "end": "2133680"
  },
  {
    "text": "hidden because we don't actually see them or look at them when we're using this model and this is this this model is",
    "start": "2133680",
    "end": "2141079"
  },
  {
    "text": "going to be deep when say I have maybe more than one or two hidden layers and",
    "start": "2141079",
    "end": "2147079"
  },
  {
    "text": "shallow otherwise okay yes I'm going layers to",
    "start": "2147079",
    "end": "2152160"
  },
  {
    "text": "use in practice uh uh varies quite a lot uh for for the problem that I'm talking",
    "start": "2152160",
    "end": "2157760"
  },
  {
    "text": "about speech recognition typically in the 5 to seven range uh but that also gets a bit hairy when you uh look at the",
    "start": "2157760",
    "end": "2164720"
  },
  {
    "text": "model I'll show you in a minute called recurrent network but I'll get into that yes just the opposite question so I mean",
    "start": "2164720",
    "end": "2171560"
  },
  {
    "text": "I know it varies a lot but vertically how many do you have is so like a 50 or a million or oh uh usually in the small",
    "start": "2171560",
    "end": "2178800"
  },
  {
    "text": "number of thousands neurons in each layer but again that can vary a lot yeah depending on the",
    "start": "2178800",
    "end": "2185520"
  },
  {
    "text": "problem yes is there an intellig way of picking those numbers or is it just uh",
    "start": "2185520",
    "end": "2190560"
  },
  {
    "text": "yeah there is an intelligent way and the most intelligent way is to do what's",
    "start": "2190560",
    "end": "2195680"
  },
  {
    "text": "called cross validation okay so are you guys mostly familiar with cross validation well let",
    "start": "2195680",
    "end": "2200839"
  },
  {
    "text": "me just explain for those of you that don't uh you take kind of a part of your data set that you uh are not training",
    "start": "2200839",
    "end": "2206920"
  },
  {
    "text": "your algorithm on and you try to tune the parameters such as the number of layers and the number of neurons based",
    "start": "2206920",
    "end": "2213480"
  },
  {
    "text": "on how well you're doing on the part that you're not actually teaching your learning algorithm",
    "start": "2213480",
    "end": "2219400"
  },
  {
    "text": "okay I mean I would say that you know cross validation will get you uh a little bit but there is also a lot of",
    "start": "2219400",
    "end": "2226720"
  },
  {
    "text": "kind of like black art into having intuition about what these models should look like um that comes with",
    "start": "2226720",
    "end": "2235560"
  },
  {
    "text": "experience okay so why deep learning all right uh I",
    "start": "2236040",
    "end": "2243400"
  },
  {
    "text": "mean the short answer is I don't think anybody really knows why deep learning like I'm not going to give you a very",
    "start": "2243400",
    "end": "2248800"
  },
  {
    "text": "specific answer but if you look at uh if you look at some examples or some",
    "start": "2248800",
    "end": "2255920"
  },
  {
    "text": "kind of anecdotal evidence at least uh and say you're looking at kind of traditional machine learning algorithms",
    "start": "2255920",
    "end": "2262240"
  },
  {
    "text": "uh well I guess I should say machine learn supervised machine learning algorithms other than deep learning what",
    "start": "2262240",
    "end": "2267560"
  },
  {
    "text": "you often see is uh as we increase the amount of training data the data with",
    "start": "2267560",
    "end": "2273319"
  },
  {
    "text": "which we are teaching the algorithm performance starts to saturate",
    "start": "2273319",
    "end": "2279560"
  },
  {
    "text": "so we get marginal Returns on say doubling the amount of training",
    "start": "2279560",
    "end": "2285880"
  },
  {
    "text": "data now the thing about deep learning that we've noticed is that if we increase the amount of training data",
    "start": "2285880",
    "end": "2292520"
  },
  {
    "text": "actually it keeps improving so if you guys kind of take",
    "start": "2292520",
    "end": "2297760"
  },
  {
    "text": "away one insight about deep learning I think it should be this slide uh and let",
    "start": "2297760",
    "end": "2304880"
  },
  {
    "text": "me just tell you what this slide is and what it isn't uh what this slide is is in the supervised learning regime when",
    "start": "2304880",
    "end": "2311800"
  },
  {
    "text": "we have data and we have the corresponding labels and we have a lot of it deep learning usually or probably",
    "start": "2311800",
    "end": "2318640"
  },
  {
    "text": "is going to work really well for you on the flip side of that when we're all the",
    "start": "2318640",
    "end": "2323800"
  },
  {
    "text": "way over here that worries me what's that it's your hesitation that worries me um is going to improve or it's it may",
    "start": "2323800",
    "end": "2335280"
  },
  {
    "text": "improve I mean I I think can't answer you with 100 a guarantee I can tell you",
    "start": "2335280",
    "end": "2340800"
  },
  {
    "text": "that it will probably work well so you've not hit the ceil oh certainly not no yeah did you",
    "start": "2340800",
    "end": "2347800"
  },
  {
    "text": "have a question back I was just wondering if this is just a different scale if you move the uh Arrow",
    "start": "2347800",
    "end": "2354160"
  },
  {
    "text": "out just flens up there too I certainly there some you know fundamentally",
    "start": "2354160",
    "end": "2359680"
  },
  {
    "text": "mathematical difference that uh makes it always scale up right so I'll tell you where we are right now from my",
    "start": "2359680",
    "end": "2365920"
  },
  {
    "text": "experience uh we have big models we have lots of data if I had more data and I",
    "start": "2365920",
    "end": "2371880"
  },
  {
    "text": "could make my model bigger that would be a good thing and the things that are limit limiting me are more data and the",
    "start": "2371880",
    "end": "2378119"
  },
  {
    "text": "computational power to actually make my model bigger right to be able to do that in a reasonable amount of time so we",
    "start": "2378119",
    "end": "2385200"
  },
  {
    "text": "certainly I don't think have hit the uh the ceiling although you're probably",
    "start": "2385200",
    "end": "2390560"
  },
  {
    "text": "right like probably eventually you can stop learning I mean all all these things have a limited capacity they",
    "start": "2390560",
    "end": "2395640"
  },
  {
    "text": "don't have an infinite capacity okay yes as far as you I'm just questioning like",
    "start": "2395640",
    "end": "2401280"
  },
  {
    "text": "like one thing about deep networks though is that they have a lot of free parameters so do you think that your",
    "start": "2401280",
    "end": "2407200"
  },
  {
    "text": "your other plot would apply to something like nearest neighbor where nearest neighbor number equals four and then if",
    "start": "2407200",
    "end": "2413119"
  },
  {
    "text": "you have all the data in the world you can still do a pretty good job uh with some noise assumptions yeah I mean I",
    "start": "2413119",
    "end": "2419160"
  },
  {
    "text": "think there's a trade-off there so more free parameters isn't necessarily going to give you this kind of behavior right",
    "start": "2419160",
    "end": "2425880"
  },
  {
    "text": "there's something else going on with deep learning that that is allowing us to have this kind of behavior so we can take a lot of supervised learning",
    "start": "2425880",
    "end": "2432000"
  },
  {
    "text": "algorithms just like uh like say I take a logistic regression just a a simple a",
    "start": "2432000",
    "end": "2437200"
  },
  {
    "text": "simple learning algorithm uh and I just throw more parameters at it uh that's",
    "start": "2437200",
    "end": "2443079"
  },
  {
    "text": "like and and I do this by like T putting it into a higher dimensional feature space for example that's not going to",
    "start": "2443079",
    "end": "2448400"
  },
  {
    "text": "work as well right like as deep learning with the same amount of data um and the reason for that I'll try to give a",
    "start": "2448400",
    "end": "2454680"
  },
  {
    "text": "little bit of intuition about is the hierarch structure of these models allows them to do a lot with relatively",
    "start": "2454680",
    "end": "2461319"
  },
  {
    "text": "less data than than say something like nearest Neighbors which is is really hard to come up with enough data to make",
    "start": "2461319",
    "end": "2468000"
  },
  {
    "text": "that model work really really well yes what is the typical amount of",
    "start": "2468000",
    "end": "2473319"
  },
  {
    "text": "data at which the uh neural network starts to gain over other simpler",
    "start": "2473319",
    "end": "2479319"
  },
  {
    "text": "models uh that's a tough one to answer uh I",
    "start": "2479319",
    "end": "2484640"
  },
  {
    "text": "think it really depends on the on the domain uh so with with computer vision",
    "start": "2484640",
    "end": "2492200"
  },
  {
    "text": "something on the order of millions of images with speech recognition something on the order of many thousands of hours",
    "start": "2492200",
    "end": "2500040"
  },
  {
    "text": "of speech uh but again like this could be like it's hard to even say how those two",
    "start": "2500040",
    "end": "2505800"
  },
  {
    "text": "things correspond to each other I think one kind of rule of thumb people use in",
    "start": "2505800",
    "end": "2511079"
  },
  {
    "text": "practice is uh is like for every kind of uh",
    "start": "2511079",
    "end": "2516640"
  },
  {
    "text": "parameter you're trying to learn you should have 10 examples in your data",
    "start": "2516640",
    "end": "2521760"
  },
  {
    "text": "set are you measuring performance performance again varies depend depending um and depending on the task",
    "start": "2521760",
    "end": "2529680"
  },
  {
    "text": "right so let me give a couple of examples and hopefully that will um this is a very general idea of",
    "start": "2529680",
    "end": "2537838"
  },
  {
    "text": "performance so in computer vision one place that deep learning has actually worked really really well uh and a",
    "start": "2538280",
    "end": "2545920"
  },
  {
    "start": "2540000",
    "end": "2931000"
  },
  {
    "text": "common task in computer vision like the one I described to you guys earlier which might be take this image and",
    "start": "2545920",
    "end": "2551520"
  },
  {
    "text": "correctly classify if there's a coffee mug or not and so the way that this has",
    "start": "2551520",
    "end": "2557200"
  },
  {
    "text": "typically worked is I have my first step in my Pipeline",
    "start": "2557200",
    "end": "2562440"
  },
  {
    "text": "and it's very distinct and Standalone step is feature extraction so I as a",
    "start": "2562440",
    "end": "2567559"
  },
  {
    "text": "human I'm going to look at the problem I'm going to look at these images and try to think oh edges happen to be",
    "start": "2567559",
    "end": "2573079"
  },
  {
    "text": "really important to my understanding of this image to my correct classification so then I'm going to try to handd design",
    "start": "2573079",
    "end": "2579280"
  },
  {
    "text": "some kind of algorithm which can detect the edges in this image or perhaps I notice that like circular objects are",
    "start": "2579280",
    "end": "2586760"
  },
  {
    "text": "really important because it's a coffee mug so I'm going to again try to handd design some kind of uh circular object",
    "start": "2586760",
    "end": "2593240"
  },
  {
    "text": "detector which can then extract where the circular objects are and how big they are from this image and then those",
    "start": "2593240",
    "end": "2598920"
  },
  {
    "text": "are going to be features so instead of taking raw pixels of this image like this image is essentially pixels instead",
    "start": "2598920",
    "end": "2604760"
  },
  {
    "text": "of taking those I'm going to try to extract from those the features which I believe are",
    "start": "2604760",
    "end": "2609920"
  },
  {
    "text": "relevant and I'm going to feed those features into my learning algorithm here",
    "start": "2609920",
    "end": "2615319"
  },
  {
    "text": "which will enable me to make a prediction so those are two very distinct steps so we have hand design",
    "start": "2615319",
    "end": "2622640"
  },
  {
    "text": "features and a learning algorithm so it turns out that if instead of uh trying to handd design these features myself I",
    "start": "2622640",
    "end": "2629680"
  },
  {
    "text": "just rip that component out of this system and instead of uh feeding that into a learning algorithm I just",
    "start": "2629680",
    "end": "2636200"
  },
  {
    "text": "replaced the whole Pipeline with a deep neural network which reads directly from the",
    "start": "2636200",
    "end": "2642839"
  },
  {
    "text": "raw pixels that actually works much much",
    "start": "2642839",
    "end": "2648318"
  },
  {
    "text": "better and in this case my metric for performance is how accurate I am in",
    "start": "2648480",
    "end": "2654200"
  },
  {
    "text": "correctly classifying objects so one thing that I think is",
    "start": "2654200",
    "end": "2661160"
  },
  {
    "text": "pretty cool is we can actually look at the different layers of this network and",
    "start": "2661160",
    "end": "2666520"
  },
  {
    "text": "we can kind of probe the different layers and try to see what's going on in them so say that I had a data set of",
    "start": "2666520",
    "end": "2674640"
  },
  {
    "text": "phasis uh like like this face that you see here of Barack Obama and I then take",
    "start": "2674640",
    "end": "2681960"
  },
  {
    "text": "my neural network and I probe its lower layers and I ask the question of the network I say what do the lower layer",
    "start": "2681960",
    "end": "2689720"
  },
  {
    "text": "neurons respond to in these images what excites them well it turns out what what",
    "start": "2689720",
    "end": "2695880"
  },
  {
    "text": "excites them is edges and edges at different rotations and different positions but it's",
    "start": "2695880",
    "end": "2702240"
  },
  {
    "text": "edges now if I ask the same question of say the middle layers in the",
    "start": "2702240",
    "end": "2707760"
  },
  {
    "text": "network they might be excited by face parts and they actually this is real I mean these aren't fabricated images so",
    "start": "2707760",
    "end": "2717119"
  },
  {
    "text": "the middle layers might be excited by say eyes or ears for example different",
    "start": "2717119",
    "end": "2722760"
  },
  {
    "text": "subcomponents of faces then if I ask the same question about the higher layer",
    "start": "2722760",
    "end": "2727839"
  },
  {
    "text": "it turns out they might be excited by even different faces or faces in different positions so they will learn",
    "start": "2727839",
    "end": "2734200"
  },
  {
    "text": "to discriminate different faces so you get this really cool notion",
    "start": "2734200",
    "end": "2739319"
  },
  {
    "text": "of like levels of abstraction that are happening within the network and all of",
    "start": "2739319",
    "end": "2746240"
  },
  {
    "text": "this is happening without me ever explicitly saying to the network like learn edges in the lower layers learn",
    "start": "2746240",
    "end": "2752760"
  },
  {
    "text": "face Parts in the middle layers and then learn specific faces in the upper layers what I do is I give it a data set of",
    "start": "2752760",
    "end": "2759160"
  },
  {
    "text": "faces and perhaps the which face that is a label and then I just let the learning",
    "start": "2759160",
    "end": "2764640"
  },
  {
    "text": "algorithm learn what's relevant uh from that image to correctly classifying that",
    "start": "2764640",
    "end": "2771920"
  },
  {
    "text": "face okay so that's deep learning in computer vision so now if we look at yes",
    "start": "2771920",
    "end": "2778319"
  },
  {
    "text": "did the computation change drastically or same yeah uh the computation is quite",
    "start": "2778319",
    "end": "2784480"
  },
  {
    "text": "different I'm not going to I don't want to get into exactly what but the no the order of magnitude of competition",
    "start": "2784480",
    "end": "2790040"
  },
  {
    "text": "between these two different algorithms right uh as in when you use deep learning when",
    "start": "2790040",
    "end": "2797480"
  },
  {
    "text": "you use this takes CPU half an hour the other one 5 Seconds yeah so the the the",
    "start": "2797480",
    "end": "2805440"
  },
  {
    "text": "naive methods for doing this yeah you would find that this took a lot longer a lot more computation power um but but",
    "start": "2805440",
    "end": "2813559"
  },
  {
    "text": "people have made a lot of progress on how to do this quickly and using computational resources and I'll talk a",
    "start": "2813559",
    "end": "2818880"
  },
  {
    "text": "little bit about that in a sec yes so if the learning algorithm only gets it feedback at the last layer of the out",
    "start": "2818880",
    "end": "2825040"
  },
  {
    "text": "doesn't know which weights along the whole nwork uh that's a good question and the way that it it knows that is",
    "start": "2825040",
    "end": "2832520"
  },
  {
    "text": "uh it's really this is a hard question to answer because there's an algorithm for it it's called back propagation uh",
    "start": "2832520",
    "end": "2838760"
  },
  {
    "text": "and basically what it allows you to do is differentiate this entire network with respect to each individual",
    "start": "2838760",
    "end": "2844680"
  },
  {
    "text": "parameter and then use a learning algorithm them called gradient descent to try to learn each individual",
    "start": "2844680",
    "end": "2851599"
  },
  {
    "text": "weight uh I don't think I should get more into the details of that but uh if you're Cur if you're curious about more",
    "start": "2851599",
    "end": "2857119"
  },
  {
    "text": "we can",
    "start": "2857119",
    "end": "2859400"
  },
  {
    "text": "talk so okay speech recognition so this is what I showed you",
    "start": "2862760",
    "end": "2868760"
  },
  {
    "text": "guys earlier this is kind of the state of speech recogn in the world and what I mentioned Also earlier is that where",
    "start": "2868760",
    "end": "2874599"
  },
  {
    "text": "deep learning has kind of Taken hold in this speech recognition pipeline is in the acoustic modeling stage so the stage",
    "start": "2874599",
    "end": "2881319"
  },
  {
    "text": "where giving a where I'm given a snippet of audio and my goal is to classify what phoning that audio",
    "start": "2881319",
    "end": "2888119"
  },
  {
    "text": "is so we've successfully replaced that module with deep learning and that actually has resulted in a lot of",
    "start": "2888119",
    "end": "2893800"
  },
  {
    "text": "improvement in speech recognition so you know drawing on",
    "start": "2893800",
    "end": "2898960"
  },
  {
    "text": "computer vision as an inspiration and a lot of other stuff that people have done with deep learning what we'd like to do",
    "start": "2898960",
    "end": "2905040"
  },
  {
    "text": "is just replace the whole Pipeline with learning with just one big neural network which goes right from input to",
    "start": "2905040",
    "end": "2912800"
  },
  {
    "text": "output and deep speech by new system I mean keep this slide in mind because",
    "start": "2912800",
    "end": "2918240"
  },
  {
    "text": "this is kind of our inspiration for deep speech uh we haven't completely done this but we've made we've taken a couple",
    "start": "2918240",
    "end": "2925520"
  },
  {
    "text": "steps in this direction so there's one kind of problem",
    "start": "2925520",
    "end": "2931760"
  },
  {
    "start": "2931000",
    "end": "3043000"
  },
  {
    "text": "that you'll encounter when you're trying to do this with audio and this what I call the variable length problem uh and",
    "start": "2931760",
    "end": "2938480"
  },
  {
    "text": "so if you guys I guess it's pretty clear not all audio Snippets are the same like right so when I say something and then I",
    "start": "2938480",
    "end": "2944960"
  },
  {
    "text": "say another thing those are not the same length in terms of the number of kind of time steps in that",
    "start": "2944960",
    "end": "2950559"
  },
  {
    "text": "audio all of the models I've shown you thus far though actually it's hardcoded in those models that the input is a",
    "start": "2950559",
    "end": "2957040"
  },
  {
    "text": "specific size so we need we need a way around this and",
    "start": "2957040",
    "end": "2962799"
  },
  {
    "text": "the first potential solution is to make everything the same size uh it's",
    "start": "2962799",
    "end": "2969280"
  },
  {
    "text": "actually not as crazy as it sounds because this is kind of what uh people have been doing in in computer vision",
    "start": "2969280",
    "end": "2975920"
  },
  {
    "text": "right like when I go and collect a million images and I put them into my database those images aren't all the",
    "start": "2975920",
    "end": "2982119"
  },
  {
    "text": "same size some of them are 100 by 200 some of them are 400 by 600 pixels and",
    "start": "2982119",
    "end": "2988000"
  },
  {
    "text": "what I do is I scale them all to 256 x 256 or some nice even number",
    "start": "2988000",
    "end": "2993960"
  },
  {
    "text": "and it turns out well as you can guess that for the most part uh the objects",
    "start": "2993960",
    "end": "2999400"
  },
  {
    "text": "the locations and the class of the objects within those images is actually invariant to deformations like like",
    "start": "2999400",
    "end": "3005920"
  },
  {
    "text": "shrinking or or uh growing the image up to an extent with audio this is much",
    "start": "3005920",
    "end": "3012440"
  },
  {
    "text": "less so like if I take 60 seconds of audio and shorten it to say 20 seconds",
    "start": "3012440",
    "end": "3017640"
  },
  {
    "text": "it's not going to sound reasonable to any of you guys uh and there's no reason to expect that it's going to look",
    "start": "3017640",
    "end": "3022920"
  },
  {
    "text": "reasonable to a machine so instead we have another solution",
    "start": "3022920",
    "end": "3028599"
  },
  {
    "text": "uh we we have a solution too which is trying to find a model which can actually process variable length",
    "start": "3028599",
    "end": "3037359"
  },
  {
    "text": "inputs and the way that we're going to do this is with a recurrent neural network so say that I have some audio",
    "start": "3037359",
    "end": "3043799"
  },
  {
    "start": "3043000",
    "end": "3112000"
  },
  {
    "text": "form looks like this uh I'm going to just bucket it into",
    "start": "3043799",
    "end": "3048880"
  },
  {
    "text": "three bins three time steps like this time step Z time step one time step two",
    "start": "3048880",
    "end": "3054720"
  },
  {
    "text": "time step three and on top of the first time step I'm just going to plop a deep",
    "start": "3054720",
    "end": "3060040"
  },
  {
    "text": "neural network and then on top of the second time step I'm also going to plop a deep",
    "start": "3060040",
    "end": "3066200"
  },
  {
    "text": "neural network and I'm actually going to have the second time step both read from",
    "start": "3066200",
    "end": "3071880"
  },
  {
    "text": "input of the audio at the second time step and the hidden State or the hidden",
    "start": "3071880",
    "end": "3077880"
  },
  {
    "text": "layer of the network at the first time step okay and then I can just do the",
    "start": "3077880",
    "end": "3083359"
  },
  {
    "text": "same thing with the third time step so you can imagine that I can do this for an arbitrary number of time",
    "start": "3083359",
    "end": "3090200"
  },
  {
    "text": "steps so it's called a recurrent Network because of this kind of recurrence in",
    "start": "3090200",
    "end": "3095280"
  },
  {
    "text": "the in the layer where it's connecting almost to itself",
    "start": "3095280",
    "end": "3100760"
  },
  {
    "text": "okay so file this one away also because this is the essentially one of the core",
    "start": "3101319",
    "end": "3106559"
  },
  {
    "text": "technologies that we use with deep speech okay so what are some of the challenges",
    "start": "3106559",
    "end": "3113000"
  },
  {
    "start": "3112000",
    "end": "3240000"
  },
  {
    "text": "you might encounter as you're trying to scale deep learning up to your own application well one of the biggest challenges is",
    "start": "3113000",
    "end": "3119440"
  },
  {
    "text": "actually coming up with enough data to train these algorithms effectively and there's a couple routes",
    "start": "3119440",
    "end": "3125160"
  },
  {
    "text": "you can go here the first is supervised which you guys know and the second is unsupervised so if I have a data set and",
    "start": "3125160",
    "end": "3131960"
  },
  {
    "text": "I don't have any labels for that data is then I have a whole bunch of images but I have no idea what those images what",
    "start": "3131960",
    "end": "3137760"
  },
  {
    "text": "what's in those images well I can actually try to learn from that in in an learn I can still actually try to learn",
    "start": "3137760",
    "end": "3143720"
  },
  {
    "text": "from that in an unsupervised manner uh but actually this is really hard and we haven't made a lot of progress on this",
    "start": "3143720",
    "end": "3149760"
  },
  {
    "text": "in deep learning so I'm actually just not going to talk anymore about unsupervised learning we we don't use it",
    "start": "3149760",
    "end": "3155119"
  },
  {
    "text": "for deep speech yet and uh we're mostly pursuing the supervised route okay and",
    "start": "3155119",
    "end": "3161960"
  },
  {
    "text": "within supervised learning I can do a couple things one I can actually try to capture more real data or I can actually",
    "start": "3161960",
    "end": "3168200"
  },
  {
    "text": "synthesize data in a clever way and I'll explain both of those to you quickly so real data is pretty straightforward like",
    "start": "3168200",
    "end": "3174160"
  },
  {
    "text": "there are a few big sources of real data but really when it comes down to it anything you can do to get your hands on",
    "start": "3174160",
    "end": "3179280"
  },
  {
    "text": "real data you should try to do so uh benchmarks are a good source of this so",
    "start": "3179280",
    "end": "3184680"
  },
  {
    "text": "imag net is a famous Benchmark where it's it's basically millions of images more than 14 million images that all",
    "start": "3184680",
    "end": "3191680"
  },
  {
    "text": "have their corresponding labels and this Benchmark has enabled like great",
    "start": "3191680",
    "end": "3197079"
  },
  {
    "text": "progress in in computer vision because it's free and it's available and it's labeled but unfortunately for for most",
    "start": "3197079",
    "end": "3203720"
  },
  {
    "text": "problems there is not such a handy giant benchmark um so one thing you can do is go work for a big company they have tons",
    "start": "3203720",
    "end": "3210359"
  },
  {
    "text": "of data uh bu do for example Amazon Google these guys all have you know AI",
    "start": "3210359",
    "end": "3216440"
  },
  {
    "text": "labs and they have reams of data that they're trying to make sense of so they can bake into their products and bake",
    "start": "3216440",
    "end": "3222040"
  },
  {
    "text": "some intelligence into their products uh another route that you have which I actually kind of think of as the Dark",
    "start": "3222040",
    "end": "3228839"
  },
  {
    "text": "Horse of machine learning research is is what's called Amazon Mechanical Turk so are you guys familiar with Amazon",
    "start": "3228839",
    "end": "3234720"
  },
  {
    "text": "Mechanical Turk okay um uh well fun fact about Mechanical Turk is that it's so",
    "start": "3234720",
    "end": "3243799"
  },
  {
    "start": "3240000",
    "end": "3366000"
  },
  {
    "text": "called because uh because of this this this automaton chess player built by an",
    "start": "3243799",
    "end": "3249760"
  },
  {
    "text": "inventor in I think the 18th century and uh what he did was he built a a fake or",
    "start": "3249760",
    "end": "3257720"
  },
  {
    "text": "an artificial uh chess playing Agent and took it around to the kings and queens of his time and telling them that he'",
    "start": "3257720",
    "end": "3263720"
  },
  {
    "text": "built an you know an automaton chess player he actually had it beat like Napoleon in",
    "start": "3263720",
    "end": "3269440"
  },
  {
    "text": "in chess I think but it turns out that there was just a man under this box right and this man was just moving some",
    "start": "3269440",
    "end": "3275160"
  },
  {
    "text": "Contraptions and he was a really good chess player so that gives you an idea of why Amazon Mechanical Turk is called",
    "start": "3275160",
    "end": "3280400"
  },
  {
    "text": "Amazon Mechanical Turk named after this contraption uh but I think of Mechanical Turk as kind of the Dark Horse of",
    "start": "3280400",
    "end": "3286760"
  },
  {
    "text": "machine learning research because if you look at a lot of like the most recent progress in in in most",
    "start": "3286760",
    "end": "3292960"
  },
  {
    "text": "machine learning research like a lot of that would not have been possible POS without Mechanical Turk it gives us an",
    "start": "3292960",
    "end": "3298559"
  },
  {
    "text": "efficient and cheap way of collecting large data sets without large data sets we can't make",
    "start": "3298559",
    "end": "3304400"
  },
  {
    "text": "progress so that's those are kind of some of the big methods I have for collecting real data",
    "start": "3304400",
    "end": "3309920"
  },
  {
    "text": "yes NPR has a lot of their audio on the web with transcripts I I oh yeah that's",
    "start": "3309920",
    "end": "3316760"
  },
  {
    "text": "that's good yep uh there there are a couple problems with that uh it's not",
    "start": "3316760",
    "end": "3321880"
  },
  {
    "text": "aligned so it's it's like you have maybe say half an hour of audio and a transcription and you don't actually",
    "start": "3321880",
    "end": "3328520"
  },
  {
    "text": "know uh when you train these models you can't train them on such long audio segments so you need a way of kind of",
    "start": "3328520",
    "end": "3335079"
  },
  {
    "text": "separating out that audio into like like 30 second clips and that's kind of a",
    "start": "3335079",
    "end": "3340280"
  },
  {
    "text": "hard problem it's not that hard it's solvable but it's kind of a hard problem um another source that has recently been",
    "start": "3340280",
    "end": "3347240"
  },
  {
    "text": "used is uh is a what are those called books on tape uh so there's a data set",
    "start": "3347240",
    "end": "3353520"
  },
  {
    "text": "of like a thousand hours of books on tape and they actually have to solve this problem in order to make it a usable data set so",
    "start": "3353520",
    "end": "3359520"
  },
  {
    "text": "there's a prolif a proliferation for speech recognition of these kinds of data sets",
    "start": "3359520",
    "end": "3364880"
  },
  {
    "text": "but nothing on the scale of image net okay so so synthetic data so uh one",
    "start": "3364880",
    "end": "3373319"
  },
  {
    "start": "3366000",
    "end": "3491000"
  },
  {
    "text": "thing that I can do if I'm in the situation where uh I can preserve uh the",
    "start": "3373319",
    "end": "3378559"
  },
  {
    "text": "the underlying semantics of this this object without uh while still making the",
    "start": "3378559",
    "end": "3384079"
  },
  {
    "text": "data the item look a little different to the machine so I can take this house right and if I just translate the house",
    "start": "3384079",
    "end": "3390880"
  },
  {
    "text": "I slide it over it's still a house if I reflect a house just flip it around its",
    "start": "3390880",
    "end": "3398520"
  },
  {
    "text": "axis it's still a house if I rotate it it's still a house so I can make these kinds of Transformations which preserve",
    "start": "3398520",
    "end": "3404480"
  },
  {
    "text": "the fact that it's a house and these are just a small example uh and I can combinatorially explode the size of my",
    "start": "3404480",
    "end": "3410079"
  },
  {
    "text": "data set uh and for all of these Transformations like the the pixels of",
    "start": "3410079",
    "end": "3415839"
  },
  {
    "text": "this image will look very very different to a machine than the pixels of the top image of the house so it actually helps",
    "start": "3415839",
    "end": "3421680"
  },
  {
    "text": "to do these kinds of things I should say though it doesn't help nearly as much as typically getting real data like raw",
    "start": "3421680",
    "end": "3427480"
  },
  {
    "text": "kind of data caught in the wild but it can help so with speech recognition what can",
    "start": "3427480",
    "end": "3433319"
  },
  {
    "text": "I do well it's less clear because in speech recognition it's less obvious how you can change the underlying signal but",
    "start": "3433319",
    "end": "3439520"
  },
  {
    "text": "still preserve the transcription right but one thing that I can do is say I",
    "start": "3439520",
    "end": "3445200"
  },
  {
    "text": "have this snippet of speech fox jumped over the lazy dog so I",
    "start": "3445200",
    "end": "3451200"
  },
  {
    "text": "said the quick brown fox jumped over the lazy dog and I I have a source of",
    "start": "3451200",
    "end": "3457300"
  },
  {
    "text": "[Music] noise so I can literally take the speech",
    "start": "3457300",
    "end": "3464480"
  },
  {
    "text": "take the noise add them together the quick brown fox jumped over",
    "start": "3464480",
    "end": "3469920"
  },
  {
    "text": "the lazy dog I get something that sounds pretty realistic like it sounds like I was actually in that noisy environment",
    "start": "3469920",
    "end": "3476200"
  },
  {
    "text": "when I was saying the quick brown fox jumped over the lazy dog uh so this is one way we can try to",
    "start": "3476200",
    "end": "3482200"
  },
  {
    "text": "make a data set of a bunch of noisy speech uh there are other things we can do but this is perhaps the most",
    "start": "3482200",
    "end": "3489160"
  },
  {
    "text": "straightforward okay another challenge with deep learning is scale so how do we",
    "start": "3489160",
    "end": "3494799"
  },
  {
    "start": "3491000",
    "end": "3599000"
  },
  {
    "text": "train these models as in tune their internal state so that they can learn to predict things well how do we do that",
    "start": "3494799",
    "end": "3501520"
  },
  {
    "text": "efficiently uh as we get more data you can imagine that it takes longer to do this also as we get more data and we we",
    "start": "3501520",
    "end": "3508400"
  },
  {
    "text": "want to increase the size of the model so that it has more capacity but that also is going to lead to longer training",
    "start": "3508400",
    "end": "3514559"
  },
  {
    "text": "time so the way that we're going to do this is by taking advantage of Hardware",
    "start": "3514559",
    "end": "3520839"
  },
  {
    "text": "called GPU Graphics processing units uh so modern day CPUs are I guess capable",
    "start": "3520839",
    "end": "3529200"
  },
  {
    "text": "of you know Computing at around 100 Giga floating Point operations per second so",
    "start": "3529200",
    "end": "3534920"
  },
  {
    "text": "floating Point operations like addition multiplication things like that now if I look at a CPU uh sorry a GPU it's almost",
    "start": "3534920",
    "end": "3541880"
  },
  {
    "text": "like an order of magnitude and a half better than that uh now a GPU is not like a panace I can",
    "start": "3541880",
    "end": "3549599"
  },
  {
    "text": "just take a CPU and plop in a GPU and things will just run faster I actually need a very specific type of uh",
    "start": "3549599",
    "end": "3556640"
  },
  {
    "text": "computation that Maps well to the GPU and the computation has to be of the sort where the instruction that I'm",
    "start": "3556640",
    "end": "3562880"
  },
  {
    "text": "giving is typically the same and it just has to operate many different instances of this data it's called like same",
    "start": "3562880",
    "end": "3569599"
  },
  {
    "text": "instruction multiple data and this this type of computation mats really really well to this type of hardware and it so",
    "start": "3569599",
    "end": "3576039"
  },
  {
    "text": "turns out also that deep learning which is mostly like when you look into the kind of the guts of deep learning it's",
    "start": "3576039",
    "end": "3581680"
  },
  {
    "text": "mostly Matrix multiplies Maps also very well onto this Hardware okay so we're",
    "start": "3581680",
    "end": "3586960"
  },
  {
    "text": "going to use gpus and of course one GPU is not enough for us so we're just going to get a whole bunch of",
    "start": "3586960",
    "end": "3593640"
  },
  {
    "text": "gpus and we're going to wire them all together there with super fast networking technology so infin band here",
    "start": "3593640",
    "end": "3599839"
  },
  {
    "text": "is one such networking technology known for its really low latency really high",
    "start": "3599839",
    "end": "3604880"
  },
  {
    "text": "throughput uh really high throughput so this is kind of what we",
    "start": "3604880",
    "end": "3610640"
  },
  {
    "text": "could have we could imagine one kind of compute cluster looking like where we have several gpus on a machine and",
    "start": "3610640",
    "end": "3616599"
  },
  {
    "text": "they're all they're all communicating with each other via super fast so they need to be able to talk to each other really",
    "start": "3616599",
    "end": "3623280"
  },
  {
    "text": "quickly Okay so a few of you might be familiar with this Google brain project that was out of",
    "start": "3623280",
    "end": "3629839"
  },
  {
    "text": "Google I think in in 2012 uh actually Andrew and Professor here and if I do",
    "start": "3629839",
    "end": "3636240"
  },
  {
    "text": "research he started this project at Google uh and what they did there was they built this giant neural network a",
    "start": "3636240",
    "end": "3643400"
  },
  {
    "text": "network that had uh a billion parameters uh and and it was trained on many many",
    "start": "3643400",
    "end": "3649240"
  },
  {
    "text": "millions of images and it had billions of connections so",
    "start": "3649240",
    "end": "3654319"
  },
  {
    "text": "billions of those synapses that you saw earlier and the way that they were able to compute this thing efficiently on",
    "start": "3654319",
    "end": "3660440"
  },
  {
    "text": "their their huge data set was by Distributing this computation across 16,000 CPU cores that's a lot of CPU",
    "start": "3660440",
    "end": "3669000"
  },
  {
    "text": "cores so actually Adam coats who I mentioned",
    "start": "3669000",
    "end": "3674680"
  },
  {
    "text": "earlier uh uh Brody huall who's actually here at Stanford and uh and Brian Karo",
    "start": "3674680",
    "end": "3680880"
  },
  {
    "text": "who's also at at by research they they had the Insight that perhaps we can do this much much faster if we use gpus and",
    "start": "3680880",
    "end": "3689240"
  },
  {
    "text": "uh and then have them communicate with each other in the right way and they were able to take this problem and",
    "start": "3689240",
    "end": "3695559"
  },
  {
    "text": "actually do the same thing that the Google brand had done with 16,000 CPU cores like literally replicate this",
    "start": "3695559",
    "end": "3701000"
  },
  {
    "text": "problem and do it with only 12 gpus and so not only do they need way",
    "start": "3701000",
    "end": "3706680"
  },
  {
    "text": "fewer machines but this actually costs way less money enables people you know without the infrastructure of Google and",
    "start": "3706680",
    "end": "3712160"
  },
  {
    "text": "the resources to recreate this kind of work can you tell us what the power different show this I don't I couldn't",
    "start": "3712160",
    "end": "3718000"
  },
  {
    "text": "tell you off the top of my head I imagine it's pretty pretty large though yeah the the way that the the CPU cores",
    "start": "3718000",
    "end": "3724559"
  },
  {
    "text": "were being used was not a a way to minimize power I think gpus are more",
    "start": "3724559",
    "end": "3733079"
  },
  {
    "text": "efficient could be yes you don't have old next cues signal processors",
    "start": "3734839",
    "end": "3742440"
  },
  {
    "text": "inside so Steve Jobs was three decad ahead of his time but I don't know what those are",
    "start": "3742440",
    "end": "3751880"
  },
  {
    "text": "but yeah you should just move on okay um do you guys have any questions",
    "start": "3752000",
    "end": "3757599"
  },
  {
    "text": "about deep pointing or about kind of the overview that I just gave are the numerical operands and the",
    "start": "3757599",
    "end": "3764799"
  },
  {
    "text": "gpus floating Point numbers or are they small integers floating Point numbers 32 bit floting Point numbers typically the",
    "start": "3764799",
    "end": "3772079"
  },
  {
    "text": "people are doing work to see if they can make them smaller yes the classification is essentially",
    "start": "3772079",
    "end": "3779359"
  },
  {
    "text": "free independent of the model the classification what do you mean so you",
    "start": "3779359",
    "end": "3785039"
  },
  {
    "text": "train your model right and then you give it samples and they the model needs to classify them right this is this this",
    "start": "3785039",
    "end": "3792960"
  },
  {
    "text": "much cheaper than the learning uh yes the type of so training is what we care",
    "start": "3792960",
    "end": "3800520"
  },
  {
    "text": "about more because it bottlenecks our rate of research so we'd like to be able to them quickly so that we can kind of",
    "start": "3800520",
    "end": "3807640"
  },
  {
    "text": "make a lot of progress quickly um but when we're actually trying to deploy these things in real products that's",
    "start": "3807640",
    "end": "3813880"
  },
  {
    "text": "what we care about doing fast classification and it's a totally different type of problem uh which I'm not going to get into but it's not done",
    "start": "3813880",
    "end": "3821520"
  },
  {
    "text": "at all in the way that I just described typically okay yes so when you want like what is",
    "start": "3821520",
    "end": "3829200"
  },
  {
    "text": "your view of when you want actually deploy this for products does it have to come to the data center and get computed",
    "start": "3829200",
    "end": "3835079"
  },
  {
    "text": "and then transfer back to the like a handheld device or whatever or is it actually a localized uh unit on the on",
    "start": "3835079",
    "end": "3842440"
  },
  {
    "text": "the yeah usually it's comput things are computed in the cloud uh it depends on",
    "start": "3842440",
    "end": "3847760"
  },
  {
    "text": "the task so so for voice specifically for voice is typically done a little bit",
    "start": "3847760",
    "end": "3853279"
  },
  {
    "text": "on both um so when I actually get the full speech signal I'm going to send",
    "start": "3853279",
    "end": "3859119"
  },
  {
    "text": "that to the cloud do my transcription send the transcription back but sometimes you might notice that like if",
    "start": "3859119",
    "end": "3865039"
  },
  {
    "text": "you want your phone to recognize when you say Okay Google like you want it to wake up that is not going to the cloud",
    "start": "3865039",
    "end": "3872319"
  },
  {
    "text": "that's often done on like a little tiny processor on the phone so you think that this will continue with deep learning in",
    "start": "3872319",
    "end": "3879680"
  },
  {
    "text": "place for spee right yeah I think so but I'm not really sure yeah I",
    "start": "3879680",
    "end": "3887920"
  },
  {
    "text": "think like it would be great if we could compute these things on the phone like itself but we're still a long ways away",
    "start": "3887920",
    "end": "3894119"
  },
  {
    "text": "from doing that I think so like not only do we use a lot of compute power but we also use a lot of",
    "start": "3894119",
    "end": "3900319"
  },
  {
    "text": "memory for these models and memory is pretty easy when we have like Big Data Centers but it's it's a little less easy",
    "start": "3900319",
    "end": "3906520"
  },
  {
    "text": "when you have a cell phone your entire model is fully connected right yeah",
    "start": "3906520",
    "end": "3911680"
  },
  {
    "text": "question so on an individual level there each NE sort of like a",
    "start": "3911680",
    "end": "3918279"
  },
  {
    "text": "logistical yeah you can totally think of a single neuron as exactly logistic regression yes that's correct any other",
    "start": "3918279",
    "end": "3925400"
  },
  {
    "text": "questions why couldn't use synthesiz speech to",
    "start": "3925400",
    "end": "3931680"
  },
  {
    "text": "train would that be to oh it's not crazy one of my one of my colleagues has been",
    "start": "3931680",
    "end": "3937400"
  },
  {
    "text": "suggesting we do this for a really long time um as in like take a take a you",
    "start": "3937400",
    "end": "3943160"
  },
  {
    "text": "know the speech the text to speech module record that or something and then feed that to train your Al yeah um I",
    "start": "3943160",
    "end": "3951960"
  },
  {
    "text": "think so there will be what you might encounter which I would predict is that there are a lot of subtle differences",
    "start": "3951960",
    "end": "3957520"
  },
  {
    "text": "between what that signal looks like and what a signal from a real human looks like and thus you will hit a pretty high",
    "start": "3957520",
    "end": "3965760"
  },
  {
    "text": "ceiling in terms of performance doing it that way that would be what I would predict but you could go to",
    "start": "3965760",
    "end": "3971160"
  },
  {
    "text": "75% sure you could augment your data that way sure yeah I agree yes do you guys do any semi supervised learning is",
    "start": "3971160",
    "end": "3978359"
  },
  {
    "text": "that useful uh yeah it's useful and no we don't do it yet um yeah uh but I can",
    "start": "3978359",
    "end": "3984720"
  },
  {
    "text": "I mean happy to chat about that after yes are there any like analogs of sort",
    "start": "3984720",
    "end": "3989960"
  },
  {
    "text": "of like convolution uh sort of networks for devices or you always use like fully",
    "start": "3989960",
    "end": "3996799"
  },
  {
    "text": "connected sort of uh no we don't always use fully connected networks um I'll show you an image of our model in a",
    "start": "3996799",
    "end": "4003240"
  },
  {
    "text": "second actually the first layer is a composition okay so yes um are they",
    "start": "4003240",
    "end": "4008920"
  },
  {
    "text": "anyut to like try to use say some kind of analog hardware for replicating the",
    "start": "4008920",
    "end": "4014599"
  },
  {
    "text": "new one instead of [Music] I think there are I don't know too much",
    "start": "4014599",
    "end": "4021520"
  },
  {
    "text": "about them though yes oh right y let me let me let me finish let me uh let me",
    "start": "4021520",
    "end": "4027440"
  },
  {
    "text": "get through this part um thank you uh so deep speech okay so deep speech as I",
    "start": "4027440",
    "end": "4032720"
  },
  {
    "text": "mentioned earlier is kind of uh our latest technology in using deep learning",
    "start": "4032720",
    "end": "4038319"
  },
  {
    "text": "kind of trying to replace the speech Pipeline with mostly deep learning okay and at the core of deep speech are two",
    "start": "4038319",
    "end": "4045039"
  },
  {
    "text": "big ingredient so trying to address the two main challenges that I mentioned earlier with deep learning the first is",
    "start": "4045039",
    "end": "4050559"
  },
  {
    "text": "data the second is scale before I talk about these two things though I want to briefly show you",
    "start": "4050559",
    "end": "4056960"
  },
  {
    "text": "what the model actually looks like okay so what we have here is this is a",
    "start": "4056960",
    "end": "4062119"
  },
  {
    "text": "spectrogram uh and essentially you can think of it as taking our raw audio waveform that you saw earlier and window",
    "start": "4062119",
    "end": "4069760"
  },
  {
    "text": "that and Computing a fore transform of each of those windows so that's exactly what this is nothing more like lab",
    "start": "4069760",
    "end": "4076279"
  },
  {
    "text": "python they all have modules and they uh spectrogram modules like that's what we're using very um welldeveloped",
    "start": "4076279",
    "end": "4083079"
  },
  {
    "text": "technology so that's the input to the model then we feed it into these layers and this is actually a convolution for",
    "start": "4083079",
    "end": "4089400"
  },
  {
    "text": "those of you that know about that and uh not that important though and then this",
    "start": "4089400",
    "end": "4094799"
  },
  {
    "text": "goes into the next layers those are all densely connected and then up top where",
    "start": "4094799",
    "end": "4100920"
  },
  {
    "text": "I can't reach we have where the blue and red arrows are we have the recurrent layer so showed you the recurrent",
    "start": "4100920",
    "end": "4106238"
  },
  {
    "text": "Network earlier and this is like not actually any more complex than the the model I showed you earlier the only",
    "start": "4106239",
    "end": "4111798"
  },
  {
    "text": "difference is uh is we have two directions in the recurrent layer so we both are taking",
    "start": "4111799",
    "end": "4119040"
  },
  {
    "text": "information from the past and using that to inform ourselves but we're also taking information from the future so at",
    "start": "4119040",
    "end": "4126000"
  },
  {
    "text": "time step T I might also include information from time step t plus one so the next thing I do is aggregate both",
    "start": "4126000",
    "end": "4133719"
  },
  {
    "text": "the forward chain and the back chain time at the next layer and then I make a prediction uh and what I'm going to",
    "start": "4133719",
    "end": "4139679"
  },
  {
    "text": "predict here is a letter in the alphabet so uh the Deep speech system is what's",
    "start": "4139679",
    "end": "4146199"
  },
  {
    "text": "called a graphine based system where a graphine is the smallest unit of uh the",
    "start": "4146199",
    "end": "4152719"
  },
  {
    "text": "the smallest unit of symbols in your written language right so it's a graphing based system so we're in this",
    "start": "4152719",
    "end": "4158120"
  },
  {
    "text": "case because it's English we're predicting letters in our alphabet plus a few punctuation marks the space symbol",
    "start": "4158120",
    "end": "4165920"
  },
  {
    "text": "as well so you can see here from raw spectrogram to",
    "start": "4165920",
    "end": "4172560"
  },
  {
    "text": "transcription we're going straight from raw spectogram to transcription now it's not this simple we actually also include",
    "start": "4172560",
    "end": "4179199"
  },
  {
    "text": "uh the constraints of a language model at the very end I'm not going to picture that here but so we haven't totally",
    "start": "4179199",
    "end": "4185838"
  },
  {
    "text": "replaced everything I just want to make that point uh but we're this is like a step in that",
    "start": "4185839",
    "end": "4191318"
  },
  {
    "text": "direction uh so one thing that's interesting about this model is that",
    "start": "4191319",
    "end": "4197840"
  },
  {
    "text": "there's actually no alignment needed for it uh and actually more specifically uh",
    "start": "4197840",
    "end": "4203840"
  },
  {
    "text": "when we have this audio it say it's like 100 time steps if you will uh we don't",
    "start": "4203840",
    "end": "4209199"
  },
  {
    "text": "necessarily have 100 output characters in our transcription like like the sentence that I said is not made up of",
    "start": "4209199",
    "end": "4216120"
  },
  {
    "text": "100 characters it's probably made up of like 20 characters so what I'm going to allow the model to do is actually output",
    "start": "4216120",
    "end": "4222960"
  },
  {
    "text": "a noop essentially or output a blank as in I predict that you said nothing in",
    "start": "4222960",
    "end": "4228719"
  },
  {
    "text": "this time step uh and then what I'm going to do is take this series of blanks letters repeat letters and I'm",
    "start": "4228719",
    "end": "4236040"
  },
  {
    "text": "going to collapse it as follows I'm just going to yank out all the blanks and repeat letters which are not separated",
    "start": "4236040",
    "end": "4241640"
  },
  {
    "text": "by blank I'm just going to collapse into one letter and then that becomes this here",
    "start": "4241640",
    "end": "4247320"
  },
  {
    "text": "the cat so you can see both of these things map onto the cat uh this is an algorithm we use an algorithm called CTC",
    "start": "4247320",
    "end": "4254520"
  },
  {
    "text": "which was developed uh by Graves at all in like 06 uh so you can read more about",
    "start": "4254520",
    "end": "4259719"
  },
  {
    "text": "it from there uh so back to uh the key ingredients of",
    "start": "4259719",
    "end": "4265880"
  },
  {
    "text": "D so data right so we have access to some public benchmarks uh and this gives",
    "start": "4265880",
    "end": "4272640"
  },
  {
    "text": "us about like 2,000 hours of speech but that's not enough for deep speech deep speech has a big appetite for data uh so",
    "start": "4272640",
    "end": "4279880"
  },
  {
    "text": "we actually have about five more thousand hours of our own data that we added into the mix",
    "start": "4279880",
    "end": "4286159"
  },
  {
    "text": "uh and this gave us a total of 7,000 hours of raw data to train our model and",
    "start": "4286159",
    "end": "4291360"
  },
  {
    "text": "this is not a crazy amount of data like most state-of-the-art recognizers train",
    "start": "4291360",
    "end": "4296400"
  },
  {
    "text": "their their algorithms on between 5,000 and say 10,000 hours of data so this we're like in the ballpark there uh of",
    "start": "4296400",
    "end": "4303159"
  },
  {
    "text": "of like the biggest data sets but what we did is we actually then took that 7,000 hours and we added about 100,000",
    "start": "4303159",
    "end": "4311000"
  },
  {
    "text": "hours of synthesized data and the way that we did this was uh by by finding",
    "start": "4311000",
    "end": "4316120"
  },
  {
    "text": "like scoring the web for a bunch of raw audio like any kind of audio not specifically speech and then taking all",
    "start": "4316120",
    "end": "4321880"
  },
  {
    "text": "of those segments that we found say 10 at a time summing them all up and that",
    "start": "4321880",
    "end": "4326920"
  },
  {
    "text": "sum of 10 raw audio segments would be a one noise example so if you listen to that it sounds a lot like real",
    "start": "4326920",
    "end": "4332280"
  },
  {
    "text": "background noise like background noise that you would actually encounter in real life we took that noise example and",
    "start": "4332280",
    "end": "4338600"
  },
  {
    "text": "we added it to our speech signal uh and then we did this in many different ways for each speech signal and this allowed",
    "start": "4338600",
    "end": "4344840"
  },
  {
    "text": "us to train our model on about 100,000 hours of new examples but keep in mind",
    "start": "4344840",
    "end": "4351639"
  },
  {
    "text": "that this is not 100,000 hours of raw real data it's it's 100,000 hours of synthesized data it's it's not quite as",
    "start": "4351639",
    "end": "4357920"
  },
  {
    "text": "good as if we had 100,000 hours of real data okay so scaling how do we scale",
    "start": "4357920",
    "end": "4363040"
  },
  {
    "text": "this model well one thing that is uh let me briefly go through this we're running",
    "start": "4363040",
    "end": "4368159"
  },
  {
    "text": "low on time so uh the bottleneck of this algorithm is actually the recurrent layer and it",
    "start": "4368159",
    "end": "4374239"
  },
  {
    "text": "might be clear because you can kind of see that there's a sequential dependency between each time step so it's really",
    "start": "4374239",
    "end": "4379320"
  },
  {
    "text": "hard to break this model up and parallelize it in the recurrent layer like these layers are actually pretty easy because they're totally independent",
    "start": "4379320",
    "end": "4385960"
  },
  {
    "text": "they can be computed quickly but the recurrent layer like the forward recurrent layer it's totally sequential",
    "start": "4385960",
    "end": "4391480"
  },
  {
    "text": "the backward recurrent layer is totally sequential so you might think that maybe we can we might make the observation",
    "start": "4391480",
    "end": "4397320"
  },
  {
    "text": "that the blue nodes are independent of the red nodes so we can compute those independently we could have one",
    "start": "4397320",
    "end": "4403400"
  },
  {
    "text": "processor work on the forward Direction One processor work on the backward Direction and that would uh speed up the",
    "start": "4403400",
    "end": "4410560"
  },
  {
    "text": "time by two the problem with this method is and it's not clear from this image is",
    "start": "4410560",
    "end": "4415800"
  },
  {
    "text": "that if I do it this way I actually have to communicate a lot of information between my two processors and so that's",
    "start": "4415800",
    "end": "4423000"
  },
  {
    "text": "going to slow me down more than it's worth so instead what we do is we just",
    "start": "4423000",
    "end": "4428360"
  },
  {
    "text": "cut the model in two like literally in half and we have the first processor work on the left half the first GPU the",
    "start": "4428360",
    "end": "4434840"
  },
  {
    "text": "second GPU works on the right half and the way that this works is the first GPU goes in the forward time step up until",
    "start": "4434840",
    "end": "4440840"
  },
  {
    "text": "it gets to the middle the second GPU goes backwards up until it gets to the middle and then they exchange a tiny",
    "start": "4440840",
    "end": "4447159"
  },
  {
    "text": "little bit of information when they both reach the middle and then they switch and then the forward process does the",
    "start": "4447159",
    "end": "4452920"
  },
  {
    "text": "first process does the the rest of the backward layer and this the second process does the rest of the forward",
    "start": "4452920",
    "end": "4458199"
  },
  {
    "text": "layer so doing it this way enables us to very much minimize the amount of communication at a very little cost and",
    "start": "4458199",
    "end": "4464719"
  },
  {
    "text": "th where able to get basically a 2X Improvement now this isn't the only thing that we do it's kind of the most",
    "start": "4464719",
    "end": "4470520"
  },
  {
    "text": "interesting perhaps way that we've scaled this problem uh but we actually do a lot of other things which I'm not going to go into because we don't have",
    "start": "4470520",
    "end": "4476080"
  },
  {
    "text": "time but if we take a look at some of the results uh so deep speech this is this is a",
    "start": "4476080",
    "end": "4482440"
  },
  {
    "text": "results in terms of the metric here is word error rate so this is like an edit distance between the words in my output",
    "start": "4482440",
    "end": "4489520"
  },
  {
    "text": "my guest transcription and the words in my ground truth transcription uh and if",
    "start": "4489520",
    "end": "4495360"
  },
  {
    "text": "we look at word error rate and Benchmark ourself against some of the best speech recognizers in the world say like the",
    "start": "4495360",
    "end": "4501560"
  },
  {
    "text": "Googles the microsofts uh apples the the technology",
    "start": "4501560",
    "end": "4507120"
  },
  {
    "text": "that powers Apple Siri wit.ai which is a smaller startup but actually has really good speech technology they're actually",
    "start": "4507120",
    "end": "4512960"
  },
  {
    "text": "now recently bought by Facebook uh we actually don't do that much better",
    "start": "4512960",
    "end": "4518520"
  },
  {
    "text": "than these guys in the clean speech setting when I say clean I mean uh when there's no background noise",
    "start": "4518520",
    "end": "4525480"
  },
  {
    "text": "but if you look at the noisy speech that's where deep speech really shines so we've really made a lot of progress",
    "start": "4525480",
    "end": "4530840"
  },
  {
    "text": "there uh the the point though is you know we've made a lot of progress but",
    "start": "4530840",
    "end": "4536239"
  },
  {
    "text": "noisy speech is still much much worse than clean speech and you as a human presumably can do much much better and",
    "start": "4536239",
    "end": "4542320"
  },
  {
    "text": "this is actually true you could do much much better so we still have a long ways to go okay so that's that's what's going",
    "start": "4542320",
    "end": "4547760"
  },
  {
    "text": "on with deep speech what was the performance measure on that graph word error rate word error rate yeah so",
    "start": "4547760",
    "end": "4554480"
  },
  {
    "text": "that's that's what I was saying earlier is like the edit distance between the the the guest transcription and the",
    "start": "4554480",
    "end": "4560520"
  },
  {
    "text": "ground truth transcription as in the number of insertions at a distance you can look",
    "start": "4560520",
    "end": "4565960"
  },
  {
    "text": "that up at a distance yep what was your benchmark for the plots that The Benchmark was a test data set",
    "start": "4565960",
    "end": "4572840"
  },
  {
    "text": "that we collected oh yes you train your network you created your own network and",
    "start": "4572840",
    "end": "4577920"
  },
  {
    "text": "the noisy data that you created we did not train it on the noisy data that we created is is is that what",
    "start": "4577920",
    "end": "4585199"
  },
  {
    "text": "you mean no but you created your network you built your network based on your",
    "start": "4585199",
    "end": "4590400"
  },
  {
    "text": "created oh sure the noisy data we created we trained our Network on but when we actually tested it we tested it",
    "start": "4590400",
    "end": "4596040"
  },
  {
    "text": "on real data with real noise that we had not seen right so we're we're not cheating in this case or at least we",
    "start": "4596040",
    "end": "4601920"
  },
  {
    "text": "don't think we are okay so next step so briefly next steps for",
    "start": "4601920",
    "end": "4609159"
  },
  {
    "text": "deep speech uh I think we haven't Tapped Out the more data the more uh the bigger model sort of improvement I also don't",
    "start": "4609159",
    "end": "4616760"
  },
  {
    "text": "think that that our algorithm is necessarily the the speech algorithm uh",
    "start": "4616760",
    "end": "4621880"
  },
  {
    "text": "I think there is a lot of room and probably will have to be algorithmic Innovations to really solve speech recognition and lastly I myself think",
    "start": "4621880",
    "end": "4629320"
  },
  {
    "text": "that unsupervised learning is also going to be really important to unlocking speech recognition and I think if you look at humans like we listen to audio",
    "start": "4629320",
    "end": "4636080"
  },
  {
    "text": "all the time right we're constantly synthesizing and integrating information from audio and we do this in a totally",
    "start": "4636080",
    "end": "4641760"
  },
  {
    "text": "unsupervised manner so I think there's a lot of room to explore that so for you guys if you're interested to learn more",
    "start": "4641760",
    "end": "4648080"
  },
  {
    "text": "about deep learning or about any any of these topics really so there's machine learning class Andrew's class here",
    "start": "4648080",
    "end": "4653679"
  },
  {
    "text": "entering uh and and he also has the same class or a similar class on corsera uh",
    "start": "4653679",
    "end": "4658920"
  },
  {
    "text": "back when I was at Stanford I actually worked on the the Deep learning tutorial which I highly encourage you to take a",
    "start": "4658920",
    "end": "4664040"
  },
  {
    "text": "look at if you're interested in uh the technical details of deep learning and if you want to read about deep speech",
    "start": "4664040",
    "end": "4670400"
  },
  {
    "text": "there's a there's a link there to our archive paper for",
    "start": "4670400",
    "end": "4677840"
  }
]