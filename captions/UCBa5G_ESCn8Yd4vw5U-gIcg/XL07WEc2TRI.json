[
  {
    "start": "0",
    "end": "100000"
  },
  {
    "text": "well thank you very dense for the deduction and invitation to be here I",
    "start": "11260",
    "end": "16869"
  },
  {
    "text": "think the last time I gave a lecture at Stanford was something in the 90s and so",
    "start": "16869",
    "end": "23960"
  },
  {
    "text": "it's always a pleasure and great honor to be to this university I so the story",
    "start": "23960",
    "end": "30680"
  },
  {
    "text": "I want to tell you has been around for a",
    "start": "30680",
    "end": "36140"
  },
  {
    "text": "while and and actually quite a few YouTube versions of earlier versions of this talk I don't know how many how many",
    "start": "36140",
    "end": "43280"
  },
  {
    "text": "of you have seen it or heard about it but I promise some new things here now",
    "start": "43280",
    "end": "49840"
  },
  {
    "text": "this is a colloquium so I have to be clear to everyone first of all that this is as you see from the title that there",
    "start": "49840",
    "end": "56960"
  },
  {
    "text": "are essentially two themes there one is information theory and the other one is",
    "start": "56960",
    "end": "64009"
  },
  {
    "text": "learning or in particular much called deep deep neural networks and in between",
    "start": "64009",
    "end": "70640"
  },
  {
    "text": "there is this button like which I'm going to describe but essentially as you",
    "start": "70640",
    "end": "76159"
  },
  {
    "text": "may know I mean deep learning is is one of those things that changed the world I",
    "start": "76159",
    "end": "84080"
  },
  {
    "text": "think we can say it today without exaggerating too much and so so neural",
    "start": "84080",
    "end": "89840"
  },
  {
    "text": "networks have been around for a long time the idea it's starting actually in the 40s with the AI formal idea of a",
    "start": "89840",
    "end": "97610"
  },
  {
    "text": "neuron as this what we call the McCulloch pitts neuron which is essentially just this gate which is to",
    "start": "97610",
    "end": "104360"
  },
  {
    "start": "100000",
    "end": "443000"
  },
  {
    "text": "take a dot product of some weights by some inputs and then pass them through some sort of non-linearity which",
    "start": "104360",
    "end": "110750"
  },
  {
    "text": "originally was just a sine function and then was softened to some sort of a sigmoid function and then today we are",
    "start": "110750",
    "end": "116869"
  },
  {
    "text": "using all sorts of other function mostly what we call values essentially of just rectified linear rectifiers now the idea",
    "start": "116869",
    "end": "124190"
  },
  {
    "text": "of actually putting them in networks with many layers is quite old of it goes all the way to the 50s and through the",
    "start": "124190",
    "end": "131030"
  },
  {
    "text": "work of Frank Rosenblatt essentially who suggested also tender the notion",
    "start": "131030",
    "end": "136430"
  },
  {
    "text": "perceptron or multi-layer perceptron and this was essentially when you look at",
    "start": "136430",
    "end": "142490"
  },
  {
    "text": "his book from 1959 or something like that see it has all of all the story in it I mean the idea of actually putting many",
    "start": "142490",
    "end": "149400"
  },
  {
    "text": "layers of these nonlinear special functions connected to each other as",
    "start": "149400",
    "end": "154890"
  },
  {
    "text": "some sort of a pattern recognition system nobody was talking about learning then but is there but this was ruled out",
    "start": "154890",
    "end": "162530"
  },
  {
    "text": "as not practical in the standards of the 60s and 70s having too many parameters",
    "start": "162530",
    "end": "170390"
  },
  {
    "text": "highly nonlinear impossible to train because you're going to get rough in",
    "start": "170390",
    "end": "176190"
  },
  {
    "text": "local optima all the good stories that we all know how to tell our students that too many parameters it's not going",
    "start": "176190",
    "end": "182970"
  },
  {
    "text": "to work and it was essentially ruled out in a very in a very bright brilliant",
    "start": "182970",
    "end": "188730"
  },
  {
    "text": "book by a remote area of Minsky and Papert two really important names in the",
    "start": "188730",
    "end": "194460"
  },
  {
    "text": "story of history I and who gave us all day completely rigorously by the way all",
    "start": "194460",
    "end": "200310"
  },
  {
    "text": "the good reasons why it will never work now it came back in the 80s and through",
    "start": "200310",
    "end": "208080"
  },
  {
    "text": "what we now call the the connection is an area era and the when I actually got",
    "start": "208080",
    "end": "214170"
  },
  {
    "text": "interested in these things after finishing my PhD in physics in in on aerodynamics actually and then I I went",
    "start": "214170",
    "end": "221280"
  },
  {
    "text": "into this story of physics of neural networks which emerge actually from the hopfield model and a few other things",
    "start": "221280",
    "end": "228239"
  },
  {
    "text": "that came in the early eighties 82 both the formal definition of machine learning through the work of Ellis",
    "start": "228239",
    "end": "234750"
  },
  {
    "text": "valiant just later on connected with the works of buttnik and other Russians then",
    "start": "234750",
    "end": "240030"
  },
  {
    "text": "who were there even before and it was actually buttnik who killed the idea at",
    "start": "240030",
    "end": "246120"
  },
  {
    "text": "the end of the 80s and early 90s and by essentially introducing this notion of kernel methods and support vector",
    "start": "246120",
    "end": "253110"
  },
  {
    "text": "machines which are some sort of neural networks but they are simpler and much",
    "start": "253110",
    "end": "258209"
  },
  {
    "text": "easier to analyze and proof things about them and for about a decade they really",
    "start": "258209",
    "end": "263419"
  },
  {
    "text": "dominated the machine learning field in the 90s during the late eighties by the",
    "start": "263419",
    "end": "269730"
  },
  {
    "text": "way some physicists including me started to do something which we then call statistical physics of neural networks",
    "start": "269730",
    "end": "276330"
  },
  {
    "text": "which is essentially analyzing large-scale diamond in the thermodynamic limits in some sense networks like this",
    "start": "276330",
    "end": "283240"
  },
  {
    "text": "but what we could do is really show off our mathematical techniques on a very",
    "start": "283240",
    "end": "290020"
  },
  {
    "text": "simple toy problems like one layer perceptron in some cases two layers this",
    "start": "290020",
    "end": "295389"
  },
  {
    "text": "three layer system we couldn't really do anything interesting those using these methods and of course the whole idea of",
    "start": "295389",
    "end": "301479"
  },
  {
    "text": "using this large-scale statistical physics went wasn't essentially didn't",
    "start": "301479",
    "end": "307720"
  },
  {
    "text": "leave a mark I mean it was unnoticed after if some physicists still like it but it's didn't make any big difference",
    "start": "307720",
    "end": "314080"
  },
  {
    "text": "on the machine learning community at least but then during the last decade",
    "start": "314080",
    "end": "319720"
  },
  {
    "text": "and since the late 2000 2006 seven eight nine and a certain group of people I'm",
    "start": "319720",
    "end": "326560"
  },
  {
    "text": "sure you all know about them Geoff Hinton Ian lacunae and a few others essentially pushed the idea of neural networks to",
    "start": "326560",
    "end": "332620"
  },
  {
    "text": "what they called on we call now deep neural networks my adding many many layers and the number and of course it's",
    "start": "332620",
    "end": "339970"
  },
  {
    "text": "not only the number of layers it's the size of the problem the size of the input which grew up by three orders of",
    "start": "339970",
    "end": "348880"
  },
  {
    "text": "magnitude sometimes more for I mean so we moved from from inputs of the size of",
    "start": "348880",
    "end": "354430"
  },
  {
    "text": "hundreds as a pixels of an image or something like this to megapixels or",
    "start": "354430",
    "end": "360159"
  },
  {
    "text": "sometimes more and we are now talking about problem which are really loud scream and and and with it came this big",
    "start": "360159",
    "end": "369550"
  },
  {
    "text": "surprise and at around two thousand seven eight-nine those deep system",
    "start": "369550",
    "end": "375490"
  },
  {
    "text": "different networks is a lot of layers started to beat all the previous technologies in competitions on image",
    "start": "375490",
    "end": "382060"
  },
  {
    "text": "recognition object recognition speech recognition a lot of other problems in signal processing later on in control in",
    "start": "382060",
    "end": "391050"
  },
  {
    "text": "networks with feedbacks and so on essentially they took over almost",
    "start": "391050",
    "end": "396099"
  },
  {
    "text": "everything we used to call AI I mean both vision speech natural language processing playing go playing Atari",
    "start": "396099",
    "end": "405460"
  },
  {
    "text": "games whatever you want essentially whatever you want it's really very surprising because it's essentially the",
    "start": "405460",
    "end": "410500"
  },
  {
    "text": "same architecture many layers of these non linear threshold gates which are which",
    "start": "410500",
    "end": "418570"
  },
  {
    "text": "essentially somehow miraculously solve many different problems and of course",
    "start": "418570",
    "end": "425080"
  },
  {
    "text": "they're also inspired by the brain in various ways I mean we called them neural networks because there's some",
    "start": "425080",
    "end": "431380"
  },
  {
    "text": "sort of resembles of analogy with the way we understand the neural system but the resemblance is very weak at best so",
    "start": "431380",
    "end": "438940"
  },
  {
    "text": "the question this was actually very bad news for traditions of people like me we really try to understand and why these",
    "start": "438940",
    "end": "446530"
  },
  {
    "start": "443000",
    "end": "743000"
  },
  {
    "text": "things work so well and so III I'm going to start slowly because I'm going to just lay the foundation but so what",
    "start": "446530",
    "end": "452470"
  },
  {
    "text": "during the last five years essentially together with two students whose name was mentioned there and no galovski and",
    "start": "452470",
    "end": "459760"
  },
  {
    "text": "which was even both brain science students actually in jerusalem and we",
    "start": "459760",
    "end": "465310"
  },
  {
    "text": "started to explore the connection which was something which means something which i have essentially thought of in",
    "start": "465310",
    "end": "471070"
  },
  {
    "text": "the 80s in the context of speech recognition I mean a principle the information theoretic way of extracting",
    "start": "471070",
    "end": "476470"
  },
  {
    "text": "relevant small dimensional relevant variables from large complex data which",
    "start": "476470",
    "end": "482260"
  },
  {
    "text": "later in the 90s we call the information button like matter I come back to this and I had a very strong feeling that",
    "start": "482260",
    "end": "488979"
  },
  {
    "text": "this is the right theory or the right way of understanding why this deep neural networks work and of course this",
    "start": "488979",
    "end": "495220"
  },
  {
    "text": "was largely ignored as well for a while but then we start to do some experiments and and some simulation in ourselves to",
    "start": "495220",
    "end": "502390"
  },
  {
    "text": "give some talks which are on YouTube and in the last year the whole thing you know there was some sort of face elation",
    "start": "502390",
    "end": "507970"
  },
  {
    "text": "everybody somehow see this picture and I am invited to many places grants fund",
    "start": "507970",
    "end": "513370"
  },
  {
    "text": "some food to talk about it so so this is something which for me was a it was very very nice but quite a big surprise so",
    "start": "513370",
    "end": "521260"
  },
  {
    "text": "what is really there this story so it turns out that we we are really combining three different ingredients",
    "start": "521260",
    "end": "527860"
  },
  {
    "text": "which in a sense change the paradigm a first of all about learning theory",
    "start": "527860",
    "end": "533760"
  },
  {
    "text": "so in learning theory which is what I call rethinking learning theory we actually move from what we used to call",
    "start": "533760",
    "end": "540880"
  },
  {
    "text": "distribution independent bound of the type of the the probably approximately correct the",
    "start": "540880",
    "end": "546700"
  },
  {
    "text": "model of Valiant which essentially gives you bound on generalization error which are independent of the distribution",
    "start": "546700",
    "end": "552490"
  },
  {
    "text": "which means independent of the problem but are very strongly dependent on the architecture or what we usually call the",
    "start": "552490",
    "end": "559300"
  },
  {
    "text": "hypothesis class I mean what type of functions my machine or my algorithm can",
    "start": "559300",
    "end": "564430"
  },
  {
    "text": "generate so we move from these to a different type of of bounds which are",
    "start": "564430",
    "end": "570640"
  },
  {
    "text": "actually strongly sensitive to the problem the problem dependent their distribution dependent but they're much",
    "start": "570640",
    "end": "577270"
  },
  {
    "text": "less sensitive to the architecture so III moved the universality from the the",
    "start": "577270",
    "end": "583210"
  },
  {
    "text": "problem of the distributional problem to the architecture of the slam chased by time so from worst case distribution",
    "start": "583210",
    "end": "591130"
  },
  {
    "text": "dependent results we moved to typical case distribution dependent but architecture independent result and I",
    "start": "591130",
    "end": "597790"
  },
  {
    "text": "believe that this is very important for this type of machines the worst-case",
    "start": "597790",
    "end": "603040"
  },
  {
    "text": "analysis doesn't mean seem to get us anywhere yes I do by actually using all the ideas that are just rethinking about",
    "start": "603040",
    "end": "610930"
  },
  {
    "text": "information theory of the shanno's communication theory in a slightly different way so using the same",
    "start": "610930",
    "end": "618240"
  },
  {
    "text": "techniques or the same mathematics that is used in information theory which is the notion of typical sequences or",
    "start": "618240",
    "end": "624280"
  },
  {
    "text": "typicality of very large patterns I'm reusing it but the typicality here is",
    "start": "624280",
    "end": "629410"
  },
  {
    "text": "not in time but actually let's say in the patches of the images that we train or in pieces of speech or whatever the",
    "start": "629410",
    "end": "636640"
  },
  {
    "text": "signal is I take into account the fact that it's really big now because it's",
    "start": "636640",
    "end": "642250"
  },
  {
    "text": "really big we can use those typical average arguments and actually get very",
    "start": "642250",
    "end": "648790"
  },
  {
    "text": "precise results so this is the way I information theory comes into the game what information theory is not going to",
    "start": "648790",
    "end": "655480"
  },
  {
    "text": "tell us much about the most important aspect of this question of deep neural networks which is that the the fact that",
    "start": "655480",
    "end": "661660"
  },
  {
    "text": "we actually can train it effectively in finite time I mean in hours or days or",
    "start": "661660",
    "end": "667480"
  },
  {
    "text": "weeks but not in tens of years on our computers today which means that there's",
    "start": "667480",
    "end": "673660"
  },
  {
    "text": "something that scales and very nicely with the size of the problem without paying",
    "start": "673660",
    "end": "678819"
  },
  {
    "text": "financially long times now so time is a different issue which is really the",
    "start": "678819",
    "end": "684369"
  },
  {
    "text": "computational aspects and it's not going to be answered by information theoretic ideas because information measures our",
    "start": "684369",
    "end": "690970"
  },
  {
    "text": "environment to computational complexity in a very profound sense so way what",
    "start": "690970",
    "end": "698049"
  },
  {
    "text": "what come then the third ingredient here is really these stochastic dynamics of the training algorithms were using which",
    "start": "698049",
    "end": "703689"
  },
  {
    "text": "is what we call stochastic gradient descent or back propagation or error back propagation or all these kind of",
    "start": "703689",
    "end": "708910"
  },
  {
    "text": "things that we are using for deep neural networks and those algorithms turn out to be not so stupid as we use the thing",
    "start": "708910",
    "end": "714729"
  },
  {
    "text": "I mean this is exactly what Minsky and Papert told us is never going to work actually they work miraculously well and",
    "start": "714729",
    "end": "721149"
  },
  {
    "text": "any attempt to improve them let's say from by moving to things like second-order methods or methods which",
    "start": "721149",
    "end": "727419"
  },
  {
    "text": "are not just now sliding a lot of the gradient or in a noisy version of the gradient tend to usually don't work well",
    "start": "727419",
    "end": "733749"
  },
  {
    "text": "actually don't work at all with this type of large problems so the question is why so I'm actually going to connect",
    "start": "733749",
    "end": "740259"
  },
  {
    "text": "all this rain in three things all right let me skip this so essentially the idea",
    "start": "740259",
    "end": "746169"
  },
  {
    "start": "743000",
    "end": "851000"
  },
  {
    "text": "is is simple and I just want to establish a few facts it and I apologize for those of you who may have seen it",
    "start": "746169",
    "end": "752019"
  },
  {
    "text": "elsewhere so I'm I'm going to think about the neural network remember this",
    "start": "752019",
    "end": "757749"
  },
  {
    "text": "is each one of those circles is is a neuron which means a linear threshold gate which and now think about the input",
    "start": "757749",
    "end": "765699"
  },
  {
    "text": "layer let's say pixels of an image or something like this as a very high entropy variable a very high dimensional",
    "start": "765699",
    "end": "773199"
  },
  {
    "text": "variable which I call which I call X and think about the labels and I'm not",
    "start": "773199",
    "end": "780039"
  },
  {
    "text": "talking about the simplest possible form of deep learning which is supervised learning I'm showing the images and give",
    "start": "780039",
    "end": "786069"
  },
  {
    "text": "let's say is it you or not you or is it a face or is it a car or is it a dog or",
    "start": "786069",
    "end": "792100"
  },
  {
    "text": "a cat oh very simple classification problems now this so I can actually think it's why the desired label is a",
    "start": "792100",
    "end": "800169"
  },
  {
    "text": "very simple variable which can be only just one bit anyway it's a lot simpler than X okay the problem is that this one",
    "start": "800169",
    "end": "807970"
  },
  {
    "text": "bit is not encoded in any simple way in the image there's no one pixel or one",
    "start": "807970",
    "end": "813789"
  },
  {
    "text": "bit in the image that tells me this it's highly distributed if that's the so what",
    "start": "813789",
    "end": "819309"
  },
  {
    "text": "what actually happens in neural network is that this input or this first layer is now going through a transformation of",
    "start": "819309",
    "end": "825429"
  },
  {
    "text": "layers and each one of them is a new representation of my image and just by",
    "start": "825429",
    "end": "831759"
  },
  {
    "text": "thinking looking at it you see that there is a Markov chain of representations here so each representation can be calculate only",
    "start": "831759",
    "end": "837489"
  },
  {
    "text": "from the previous one and affects only the next one so the story I want to",
    "start": "837489",
    "end": "842529"
  },
  {
    "text": "understand is what's actually going on through this a representation transformation of this cascade of",
    "start": "842529",
    "end": "848529"
  },
  {
    "text": "representation changes now before I go I must I must introduce some quantities",
    "start": "848529",
    "end": "855069"
  },
  {
    "start": "851000",
    "end": "1036000"
  },
  {
    "text": "which I'm sure if you are mostly in electrical engineers you all know them so so I need some information measures",
    "start": "855069",
    "end": "862149"
  },
  {
    "text": "and in particular I need the notion of mutual information so how many of you don't need it know what mutual",
    "start": "862149",
    "end": "868389"
  },
  {
    "text": "information is not that may owe you don't dare to say that you knew that's",
    "start": "868389",
    "end": "874449"
  },
  {
    "text": "stranger okay so way so I need two quantities one of them is known as the KL divergence or the cross entropy or the information",
    "start": "874449",
    "end": "882159"
  },
  {
    "text": "divergence has many different names and this is this averaged log likelihood of",
    "start": "882159",
    "end": "888819"
  },
  {
    "text": "two distributions so I have P and Q over some random variable take the log of the",
    "start": "888819",
    "end": "894939"
  },
  {
    "text": "ratio and average respect to the first one this quantity is non-negative it's zero precisely when you're equal almost",
    "start": "894939",
    "end": "902049"
  },
  {
    "text": "everywhere and and it's a very fundamental quantity which has I can teach a whole course about why it's important so many different ways but",
    "start": "902049",
    "end": "909249"
  },
  {
    "text": "anybody know something about statistical hypothesis testing for example know that the log lie feel ratio is a very important thing and and they and the",
    "start": "909249",
    "end": "916359"
  },
  {
    "text": "average of it is really telling us a lot about the rate of achieving making decisions and so many other things for",
    "start": "916359",
    "end": "922089"
  },
  {
    "text": "the has entirely different ways and meanings information theory the mutual",
    "start": "922089",
    "end": "927639"
  },
  {
    "text": "information which I need all of you to to appreciate and know at this point is the KL divergence between the Joint",
    "start": "927639",
    "end": "935499"
  },
  {
    "text": "Distribution of two variables and the product of the marginals and it's going to be zero precisely when the two",
    "start": "935499",
    "end": "942549"
  },
  {
    "text": "variables are independent the joint is the product and then and",
    "start": "942549",
    "end": "948560"
  },
  {
    "text": "otherwise it's a non-negative quantity which can be interpreted as the entropy or the uncertainty in the variable X -",
    "start": "948560",
    "end": "955740"
  },
  {
    "text": "now answer to be removed from X when I know why this is a symmetric on T thing",
    "start": "955740",
    "end": "960839"
  },
  {
    "text": "and it's actually going to be very important for us because I'm actually going to from now hold on I'm going to",
    "start": "960839",
    "end": "966720"
  },
  {
    "text": "look at mutual information quantities in my neural networks all over the place now there are two things about mutual",
    "start": "966720",
    "end": "972209"
  },
  {
    "text": "information that I want you to appreciate one of them is known as the data processing inequality which means",
    "start": "972209",
    "end": "977310"
  },
  {
    "text": "that if I'm moving along a Markov chain let's say X Y Z in this case and then",
    "start": "977310",
    "end": "982470"
  },
  {
    "text": "information can only decrease this is true not only finished information so for a whole range of other measures like",
    "start": "982470",
    "end": "989339"
  },
  {
    "text": "this but there's something very special about new information which is uniquely determined not only the person quality",
    "start": "989339",
    "end": "994770"
  },
  {
    "text": "it has another another property which I need which we usually call the mean mean is minimizer I'm going to need it about",
    "start": "994770",
    "end": "1002000"
  },
  {
    "text": "I'm going to talk about now so first an immediate consequence of the dpr data processing in equality is that under any",
    "start": "1002000",
    "end": "1009370"
  },
  {
    "text": "invertible maps of x and y the information doesn't change so this is",
    "start": "1009370",
    "end": "1014720"
  },
  {
    "text": "actually the bad news for anybody was trying to use information for computational complexity because it",
    "start": "1014720",
    "end": "1020209"
  },
  {
    "text": "means that i can encrypt my i can take any any hard transformation how to invert information of my data this is",
    "start": "1020209",
    "end": "1027140"
  },
  {
    "text": "not going to affect information measures but it's going to create a big headache in terms of the computational complexity",
    "start": "1027140",
    "end": "1032510"
  },
  {
    "text": "so information is not telling us anything about computational complexity but now as i look at it my at my new",
    "start": "1032510",
    "end": "1037850"
  },
  {
    "start": "1036000",
    "end": "1718000"
  },
  {
    "text": "neural networks there is immediately a chain of inequalities which i want to call the information path which is",
    "start": "1037850",
    "end": "1044089"
  },
  {
    "text": "essentially how much information is there it layer one about the input how much information is there about layer in",
    "start": "1044089",
    "end": "1049610"
  },
  {
    "text": "layer two about the input and because it's a Markov chain of transformations information can only decrease about the",
    "start": "1049610",
    "end": "1055670"
  },
  {
    "text": "input and if i ask the same about the desired output remember I'm talking about generalization about what the",
    "start": "1055670",
    "end": "1061820"
  },
  {
    "text": "label should be not what the label is at the end of the network and this is also",
    "start": "1061820",
    "end": "1067190"
  },
  {
    "text": "decreasing when I go through the layers so I have these two chains of inequality",
    "start": "1067190",
    "end": "1072290"
  },
  {
    "text": "which I'm going to call the information paths of a network like this and",
    "start": "1072290",
    "end": "1077440"
  },
  {
    "text": "essentially what you should appreciate that only this Markov chain of of layers",
    "start": "1077440",
    "end": "1084100"
  },
  {
    "text": "is essentially inducing some sort of partitioning which of the data so think",
    "start": "1084100",
    "end": "1090370"
  },
  {
    "text": "about all the images that are met to the same same value of the layer the same representation of the units and and and",
    "start": "1090370",
    "end": "1096519"
  },
  {
    "text": "this partition can only get closer when I move from one layer to the next so there's some sort of questioning of the",
    "start": "1096519",
    "end": "1102580"
  },
  {
    "text": "representation so this is an information theory this is related to something we call successful refinement I'm going to",
    "start": "1102580",
    "end": "1108730"
  },
  {
    "text": "come back to this at the end and and but notice that of course I can scramble or",
    "start": "1108730",
    "end": "1113950"
  },
  {
    "text": "the network can scramble my representation in an arbitrary way and this will not affect those information",
    "start": "1113950",
    "end": "1120009"
  },
  {
    "text": "regions ok so another slightly different",
    "start": "1120009",
    "end": "1125259"
  },
  {
    "text": "way of thinking about it is to think about each layer as being encoded by a",
    "start": "1125259",
    "end": "1130929"
  },
  {
    "text": "some sort so there's a a meta stochastic map in general from the input to the",
    "start": "1130929",
    "end": "1135940"
  },
  {
    "text": "layer here I call it t it was the same thing so any any layer T t1 t2 t3 any",
    "start": "1135940",
    "end": "1141700"
  },
  {
    "text": "one of those hidden layers is actually mapped and met from the input which I",
    "start": "1141700",
    "end": "1146830"
  },
  {
    "text": "call the encoder of the layer and and so any layer has an encoder which is how it",
    "start": "1146830",
    "end": "1152740"
  },
  {
    "text": "is mapped from the input both have maps it generates and there is a decoder which is the the way I'm trying to",
    "start": "1152740",
    "end": "1159309"
  },
  {
    "text": "extract the label of the desired level there this is the label I'm I'm actually training I call it Y hat here because",
    "start": "1159309",
    "end": "1166120"
  },
  {
    "text": "they're not exactly why this what this is what the network is generating and during the training two things happen",
    "start": "1166120",
    "end": "1171669"
  },
  {
    "text": "first of all I'm trying to push y hat to be as close as possible to Y then I said that the network is working will and the",
    "start": "1171669",
    "end": "1177970"
  },
  {
    "text": "other thing is that when you move from one layer to the next the encoder becomes more and more complicated the",
    "start": "1177970",
    "end": "1185919"
  },
  {
    "text": "first time code and the first encoder is very simple and but the decoder is very complicated when you it and then then",
    "start": "1185919",
    "end": "1192340"
  },
  {
    "text": "code they become more more complicated and the decoder becomes simpler at the last hidden layer the decoder is",
    "start": "1192340",
    "end": "1197830"
  },
  {
    "text": "essentially just a linear social function was just a perceptron just one layer position ok so the question is how",
    "start": "1197830",
    "end": "1204580"
  },
  {
    "text": "is this miracle happens that somehow from layer to layer I",
    "start": "1204580",
    "end": "1210130"
  },
  {
    "text": "more information by the encoder and eventually get to a very simple decoder and now I actually here is a very",
    "start": "1210130",
    "end": "1216970"
  },
  {
    "text": "informal formulation of a theorem which we now have a much clearer formulation and I I dare to say that essentially the",
    "start": "1216970",
    "end": "1224800"
  },
  {
    "text": "only two numbers out of those zillion of parameters and that are really important",
    "start": "1224800",
    "end": "1230410"
  },
  {
    "text": "for each layer and this is the mutual information of the encoder and the",
    "start": "1230410",
    "end": "1235630"
  },
  {
    "text": "mutual information of the decoder essentially the mutual information of the desired output given the layer and I",
    "start": "1235630",
    "end": "1241600"
  },
  {
    "text": "am you that if you know these two numbers for each layer and in particular for the last hidden layer we we can",
    "start": "1241600",
    "end": "1249340"
  },
  {
    "text": "predict both the two things you really care about which are the accuracy the probability of making an error outside",
    "start": "1249340",
    "end": "1255700"
  },
  {
    "text": "of my training data what we call the generalization error and the number of samples of what we call the sample",
    "start": "1255700",
    "end": "1261820"
  },
  {
    "text": "complexity so the trade-off within complexity of the samples how many data a I really need and what accuracy this",
    "start": "1261820",
    "end": "1268600"
  },
  {
    "text": "is going to give me it's going to depend for large enough problems where I can use typicality arguments and for large",
    "start": "1268600",
    "end": "1276850"
  },
  {
    "text": "enough networks such that they can actually learn this rule if you have to depend only on these two quantities the",
    "start": "1276850",
    "end": "1283750"
  },
  {
    "text": "information of the encoding information on the decoder so this is quite remarkable because it's going to simplify the problem dramatically if you",
    "start": "1283750",
    "end": "1290560"
  },
  {
    "text": "actually believe me then I don't need to know anything about everything else architecture is not so important I need",
    "start": "1290560",
    "end": "1296230"
  },
  {
    "text": "to know only what is how much information there is in the encoder of the decoder of delays now this is what I",
    "start": "1296230",
    "end": "1303160"
  },
  {
    "text": "call the information plan sometimes we call it the there so these two coordinates the information about the",
    "start": "1303160",
    "end": "1308230"
  },
  {
    "text": "input versus the information about the output of the desired output are going to tell us a very interesting story so",
    "start": "1308230",
    "end": "1314410"
  },
  {
    "text": "in order to see this I I'm going to use this movie that some of you may have seen but it's always very important to",
    "start": "1314410",
    "end": "1320050"
  },
  {
    "text": "see it again so what you see here are these two axis that the x-axis is the",
    "start": "1320050",
    "end": "1326340"
  },
  {
    "text": "information that each of the layer has about the input the y-axis is the",
    "start": "1326340",
    "end": "1332110"
  },
  {
    "text": "information that this layer has about the output and the colors represent layers of a specific neural network",
    "start": "1332110",
    "end": "1340270"
  },
  {
    "text": "which I trained hundred times with different random initial country and different training data and I change",
    "start": "1340270",
    "end": "1347560"
  },
  {
    "text": "both the sample size it's the same size but different selection of the sample and and oh and the ordering of the",
    "start": "1347560",
    "end": "1354460"
  },
  {
    "text": "examples because there are it's a stochastic gradient descent which I'm going to change so other than that I am now training this deep neural networks",
    "start": "1354460",
    "end": "1361210"
  },
  {
    "text": "in the most plain vanila using tens of flow or something like this a way like everybody else and what we want to see",
    "start": "1361210",
    "end": "1368020"
  },
  {
    "text": "is how those information values evolve in time through the efforts of training",
    "start": "1368020",
    "end": "1373750"
  },
  {
    "text": "so what you see now is the initial condition of the network so right up here in blue is my first hidden layer",
    "start": "1373750",
    "end": "1381340"
  },
  {
    "text": "this first hidden layer essentially maintained even with random initialization all the almost all the",
    "start": "1381340",
    "end": "1388210"
  },
  {
    "text": "information about both the input in the output that's why it's hid so this is the first hidden layer the one closer closest to the input and the last hidden",
    "start": "1388210",
    "end": "1396880"
  },
  {
    "text": "layer this one day in orange is the the last layer which is supposed to learn",
    "start": "1396880",
    "end": "1402070"
  },
  {
    "text": "you see that the initial condition they know very little about both the input and the output so they're in the left",
    "start": "1402070",
    "end": "1409110"
  },
  {
    "text": "left down corner now this is the initial",
    "start": "1409110",
    "end": "1414910"
  },
  {
    "text": "condition what happens when you start training it with this big with stochastic gradient descent and so this",
    "start": "1414910",
    "end": "1422140"
  },
  {
    "text": "is the this is usually the picture which works up everyone so essentially what",
    "start": "1422140",
    "end": "1428380"
  },
  {
    "text": "happens and as you see there the number of epochs running up there so you see it",
    "start": "1428380",
    "end": "1435880"
  },
  {
    "text": "was very fast so I'll I'll do it I'll move it slowly so it's actually up to",
    "start": "1435880",
    "end": "1441070"
  },
  {
    "text": "this point up to this point which is about 300 epochs of training so what we",
    "start": "1441070",
    "end": "1447760"
  },
  {
    "text": "call epoch of training essentially cycling of all the examples in",
    "start": "1447760",
    "end": "1452770"
  },
  {
    "text": "mini-batches and the way we calculate the grad and in stochastic gradient descent is we don't calculate the gradient over all the examples we",
    "start": "1452770",
    "end": "1459160"
  },
  {
    "text": "actually cut the examples into small sets which we call mini batch which can be hundred of 200 or 300 examples and",
    "start": "1459160",
    "end": "1465160"
  },
  {
    "text": "and we calculate this noisy version of the gradient and we update the weights for each one of those many batches so",
    "start": "1465160",
    "end": "1472630"
  },
  {
    "text": "any update of the weights through all the many batches what we call",
    "start": "1472630",
    "end": "1478100"
  },
  {
    "text": "so after 300 epochs which means 300 cycles of cycling of the data through my",
    "start": "1478100",
    "end": "1483780"
  },
  {
    "text": "networks we get more or less to this point which is interesting so first of all you see the data processing inequality you see that the information",
    "start": "1483780",
    "end": "1490050"
  },
  {
    "text": "goes down as you expect actually this points are almost linearly down and you see there are actually six hidden layers",
    "start": "1490050",
    "end": "1496410"
  },
  {
    "text": "the reason you see these clouds is because as I said we repeated the experiments hundred times with random",
    "start": "1496410",
    "end": "1502530"
  },
  {
    "text": "initial conditions but so up to this point around 300 epochs all the networks",
    "start": "1502530",
    "end": "1508860"
  },
  {
    "text": "that the older layers went up which means they improve increase the information about the label but also to",
    "start": "1508860",
    "end": "1515790"
  },
  {
    "text": "the left to the right which means they also acquired more information about the input all of them all the layers",
    "start": "1515790",
    "end": "1523020"
  },
  {
    "text": "together and from this point on something interesting happen it starts very very slowly I'm sorry",
    "start": "1523020",
    "end": "1529440"
  },
  {
    "text": "and it's very very slowly moving to the",
    "start": "1529440",
    "end": "1534840"
  },
  {
    "text": "left and up and if you look at the number of epochs out of this about 90",
    "start": "1534840",
    "end": "1542060"
  },
  {
    "text": "10000 epochs only 300 which is very little it took to get to that to this",
    "start": "1542060",
    "end": "1549510"
  },
  {
    "text": "same middle point which I argue is a very important point this is where the data the training error essentially is",
    "start": "1549510",
    "end": "1555510"
  },
  {
    "text": "saturated now it's very small this is what we all believe is the most",
    "start": "1555510",
    "end": "1560730"
  },
  {
    "text": "important part of training fitting the labels of the data but this happens very quickly after around 300 a box and then",
    "start": "1560730",
    "end": "1568200"
  },
  {
    "text": "the rest 10000 - 300 epochs essentially slowly moving by what I'm going to call",
    "start": "1568200",
    "end": "1577710"
  },
  {
    "text": "diffusion or by by the noise in the gradients of course there's Tim Robbins there's still something that pushing you",
    "start": "1577710",
    "end": "1583680"
  },
  {
    "text": "down but it's dominated by by the noise in the gradient that's what I'm going to",
    "start": "1583680",
    "end": "1588840"
  },
  {
    "text": "show you well it's slower relative to the faster I mean it's 10000 epochs no",
    "start": "1588840",
    "end": "1599520"
  },
  {
    "text": "no everything will be clear there ok ok I hope not everything but some things ok",
    "start": "1599520",
    "end": "1604710"
  },
  {
    "text": "so so so there are these two - what I call phases of learning in the first one you feed the data in the set",
    "start": "1604710",
    "end": "1611370"
  },
  {
    "text": "then you do this very irritably slow I mean it's ten or a hundred times slower than the first place where you move to",
    "start": "1611370",
    "end": "1619440"
  },
  {
    "text": "the left and I call it the forgetting phase of the learning now you're already",
    "start": "1619440",
    "end": "1624690"
  },
  {
    "text": "going to give you the gist of the story during the first part of the learning you actually fit the labels you do this",
    "start": "1624690",
    "end": "1631050"
  },
  {
    "text": "what we call empirical risk minimization we should consider the holy grail of learning for many many years this is",
    "start": "1631050",
    "end": "1636300"
  },
  {
    "text": "what we want to do but it's actually the second part which is done when the error",
    "start": "1636300",
    "end": "1643380"
  },
  {
    "text": "is essentially fluctuating I mean it's not zero but it's moving up and down between those mini epochs which are",
    "start": "1643380",
    "end": "1650100"
  },
  {
    "text": "subsets of the training data during this phase which is not supposed to do much",
    "start": "1650100",
    "end": "1655410"
  },
  {
    "text": "according to the classical theories most of the improvement in my information about the label happens and this is",
    "start": "1655410",
    "end": "1662910"
  },
  {
    "text": "strange I mean to surprise an explanation and my story is that the gist of it is at this",
    "start": "1662910",
    "end": "1669240"
  },
  {
    "text": "phase we actually forget or learn to ignore the irrelevant details of the",
    "start": "1669240",
    "end": "1674309"
  },
  {
    "text": "patterns those things which are not labeled the changes the differences in the background the fact that there are",
    "start": "1674309",
    "end": "1680340"
  },
  {
    "text": "so many dimensions of the problem which are not important for the label so that's the hard part",
    "start": "1680340",
    "end": "1686420"
  },
  {
    "text": "ignoring or forgetting or learning to ignore the irrelevant details of the program now so the rest of the story is",
    "start": "1686420",
    "end": "1694620"
  },
  {
    "text": "just trying to justify what I just told you based on some other analysis yes yes",
    "start": "1694620",
    "end": "1705300"
  },
  {
    "text": "I'm going to tell you okay this is the story so anyway so so the this was some",
    "start": "1705300",
    "end": "1712080"
  },
  {
    "text": "of you know a mystery story attracted the quanta magazine the attention wander magazine this is a nice picture that",
    "start": "1712080",
    "end": "1717600"
  },
  {
    "text": "they gave for this so what you actually there are several other questions I wanted to ask before going to this so",
    "start": "1717600",
    "end": "1722870"
  },
  {
    "start": "1718000",
    "end": "1923000"
  },
  {
    "text": "what just looking at this create this picture first of all you want to ask is",
    "start": "1722870",
    "end": "1727920"
  },
  {
    "text": "this the general picture or maybe just this is a special case with a very special network and a very small problem and so on so believe me we see this",
    "start": "1727920",
    "end": "1735510"
  },
  {
    "text": "precisely the same story for very large problems or at least as large as we could different the second question that",
    "start": "1735510",
    "end": "1741450"
  },
  {
    "text": "usually people ask how do you estimate new information okay so we estimate mutual information using",
    "start": "1741450",
    "end": "1748210"
  },
  {
    "text": "in this case very small problem and we've been the data for last problems",
    "start": "1748210",
    "end": "1753220"
  },
  {
    "text": "use a much more sophisticated methods all sorts of parametric representation like Gaussian mixture models on each one",
    "start": "1753220",
    "end": "1758650"
  },
  {
    "text": "of the layers we know how to do it if you do it's wrong you don't see this picture actually that's a very nice",
    "start": "1758650",
    "end": "1764830"
  },
  {
    "text": "paper came from Harvard they are lucky at taking us precisely on this this is not the issue now so we see note the",
    "start": "1764830",
    "end": "1772030"
  },
  {
    "text": "second question is which is list for me was very interesting among these hundred points why do they",
    "start": "1772030",
    "end": "1778030"
  },
  {
    "text": "concentrate them and why do you get this relatively concentrated the point I mean",
    "start": "1778030",
    "end": "1783130"
  },
  {
    "text": "so this requires an explanation I mean these are entirely different networks if you actually compare the weights then no",
    "start": "1783130",
    "end": "1790030"
  },
  {
    "text": "correlation they look very different or you're compared to even the values of the neurons there are not they're not",
    "start": "1790030",
    "end": "1795070"
  },
  {
    "text": "correlated but the in value of the information turned out to be very very sharp and it gets sharper the larger the",
    "start": "1795070",
    "end": "1801130"
  },
  {
    "text": "prominent this is precisely what we like when things get concentrated in the large limit then we know that we have",
    "start": "1801130",
    "end": "1807340"
  },
  {
    "text": "what we call good order parameters now the third question is ok what do these",
    "start": "1807340",
    "end": "1812740"
  },
  {
    "text": "numbers mean I mean why they are interesting so I already told you that the y-axis is",
    "start": "1812740",
    "end": "1818650"
  },
  {
    "text": "going to dominate the generalization error the higher the information about the label the smaller the error and this",
    "start": "1818650",
    "end": "1825370"
  },
  {
    "text": "is a rigorous nature it's not an approximate statement we can prove it and it gets tighter and tighter bound",
    "start": "1825370",
    "end": "1830740"
  },
  {
    "text": "when you get to a larger prominence again the second x-axis or the amount of",
    "start": "1830740",
    "end": "1837220"
  },
  {
    "text": "what I call compression I mean how much I forget the input in bits is precisely",
    "start": "1837220",
    "end": "1843550"
  },
  {
    "text": "going to dominate the general the number of examples in it or the sample complexity so this is why this is this",
    "start": "1843550",
    "end": "1850570"
  },
  {
    "text": "is the theorem that I mentioned some it's some sort of a coding theorem because it's relating informational",
    "start": "1850570",
    "end": "1856150"
  },
  {
    "text": "quantities those entropic like functions to really things we care about like code",
    "start": "1856150",
    "end": "1862030"
  },
  {
    "text": "lengths or arrows or in this case generation error and number of examples that's why this is in the flavor and the",
    "start": "1862030",
    "end": "1868810"
  },
  {
    "text": "spirit of both be like and love about information theory now so say the cell",
    "start": "1868810",
    "end": "1874690"
  },
  {
    "text": "of course in the most at the last question e which is the most fascinate one is what governs that this strange",
    "start": "1874690",
    "end": "1880160"
  },
  {
    "text": "dynamics I mean why does it move so quickly first and so slowly later and one that why does it compress so I want",
    "start": "1880160",
    "end": "1887630"
  },
  {
    "text": "to answer all these questions to you of course not completely as much as I can with so",
    "start": "1887630",
    "end": "1893810"
  },
  {
    "text": "again because they concentrate we can average those subject to reason and this is the simple version of the story I now",
    "start": "1893810",
    "end": "1900950"
  },
  {
    "text": "average those points and you get it is very nice to Dexter we from A to C I remember I fit the data from C to e I",
    "start": "1900950",
    "end": "1908270"
  },
  {
    "text": "generalize and it happens that all the layers are doing this type of story more",
    "start": "1908270",
    "end": "1914840"
  },
  {
    "text": "or less together and they all come to this inflection point point C more or",
    "start": "1914840",
    "end": "1920210"
  },
  {
    "text": "less together okay so why what is the story now I hesitate to give you this proof again because it takes time and",
    "start": "1920210",
    "end": "1928040"
  },
  {
    "text": "it's rough I mean it's not it's not as rigorous as I would like it to me but I just want to give you the gist of it as",
    "start": "1928040",
    "end": "1934640"
  },
  {
    "text": "we like to say so those of you who know something about learning theory I hope all of you know something about learning",
    "start": "1934640",
    "end": "1940250"
  },
  {
    "text": "theory should be familiar with what we call the old what I call the old the type of him",
    "start": "1940250",
    "end": "1946570"
  },
  {
    "text": "generalization and let me eliminate account so the old type of generation",
    "start": "1946570",
    "end": "1951980"
  },
  {
    "text": "bar looks like that this type of bound on on the on the on the left which is",
    "start": "1951980",
    "end": "1957590"
  },
  {
    "text": "essentially and let me just magnify so this is a the generation error which is",
    "start": "1957590",
    "end": "1963290"
  },
  {
    "text": "this epsilon which is again the probability of making an error on my new unseen data out of my training it's",
    "start": "1963290",
    "end": "1971750"
  },
  {
    "text": "bounded by essentially two terms the number of examples which is in the",
    "start": "1971750",
    "end": "1977900"
  },
  {
    "text": "denominator which okay then larger the number of samples I've seen the smaller there but on the on the numerator which",
    "start": "1977900",
    "end": "1984950"
  },
  {
    "text": "is the important term is the log of the cardinality of my hypothesis class this",
    "start": "1984950",
    "end": "1990950"
  },
  {
    "text": "is valiant and this is beautiful because it gives us bound which is completely",
    "start": "1990950",
    "end": "1996080"
  },
  {
    "text": "independent anything except these two numbers of course this this log 1 over Delta which is what we call the",
    "start": "1996080",
    "end": "2001300"
  },
  {
    "text": "confidence and the confidence is this the probability of seeing a very bad sample ok the probability of saying a",
    "start": "2001300",
    "end": "2008410"
  },
  {
    "text": "very bad sample is 0 when you talk about million images when once once you move to very large",
    "start": "2008410",
    "end": "2014150"
  },
  {
    "text": "data this is completely negligible for small problem is important but we still",
    "start": "2014150",
    "end": "2019280"
  },
  {
    "text": "keep it there the I mean log of over Delta think of Delta is ten to the minus ten or",
    "start": "2019280",
    "end": "2024680"
  },
  {
    "text": "something like this so this is ten okay but we're talking millions of examples for log of age and M of order of millions now so it's the",
    "start": "2024680",
    "end": "2032660"
  },
  {
    "text": "competition between the log of the cardinality of the of the hypothesis class and the number of examples one of",
    "start": "2032660",
    "end": "2039230"
  },
  {
    "text": "is controlling the complexity of my learning the other one is controlling the data actually gained from the data",
    "start": "2039230",
    "end": "2045560"
  },
  {
    "text": "and of course what we usually do is we we we want to work with the infinite",
    "start": "2045560",
    "end": "2051500"
  },
  {
    "text": "classes so we approximate and we cover the hypothesis class which is usually",
    "start": "2051500",
    "end": "2057290"
  },
  {
    "text": "infinite like you know rectangular things like this by what we call epsilon grid or epsilon quantization which are",
    "start": "2057290",
    "end": "2063260"
  },
  {
    "text": "essentially cover it with a finite number of points which are not more than",
    "start": "2063260",
    "end": "2068300"
  },
  {
    "text": "epsilon might pass on any other episode and find a grid like this an epsilon net also agreed now usually if you have a",
    "start": "2068300",
    "end": "2076790"
  },
  {
    "text": "finite dimensional system a class then we can cover it the number the size of",
    "start": "2076790",
    "end": "2083389"
  },
  {
    "text": "the cover grows like one over epsilon to the dimension of the class this is geometry so this can be the house of dimension or",
    "start": "2083390",
    "end": "2090770"
  },
  {
    "text": "the fractal dimension or the or the VC dimension or the shattering dimension with that many different types of",
    "start": "2090770",
    "end": "2096620"
  },
  {
    "text": "dimension likely they all behave like this eventually when you need that that advanced cover the side of the cover",
    "start": "2096620",
    "end": "2102350"
  },
  {
    "text": "grow exponentially with the dimension so if we plug this here and this is really",
    "start": "2102350",
    "end": "2107720"
  },
  {
    "text": "the miracle of a of the VC Theory you plug this in the first the first time you get the D over the number of example",
    "start": "2107720",
    "end": "2114230"
  },
  {
    "text": "is the dominus factor once the number of examples is is larger than d your",
    "start": "2114230",
    "end": "2119390"
  },
  {
    "text": "sentries begin to generalize the dimension as long as it's smaller than D you don't generalize this doesn't mean",
    "start": "2119390",
    "end": "2124940"
  },
  {
    "text": "anything this is beautiful we were have been working with this for 30 years now there are many books in them but the",
    "start": "2124940",
    "end": "2131510"
  },
  {
    "text": "problem is that we all know although some don't admit these bounds don't give",
    "start": "2131510",
    "end": "2136940"
  },
  {
    "text": "us anything they're vacuous for deep learning which means that any",
    "start": "2136940",
    "end": "2143670"
  },
  {
    "text": "reasonable estimate of the dimensionality of the class gave us something which off the order of the",
    "start": "2143670",
    "end": "2149069"
  },
  {
    "text": "number of weights may be the square root of the number of weights may be the weights log the number of weight or something like this but it's the number",
    "start": "2149069",
    "end": "2155609"
  },
  {
    "text": "of weight the band depending on how you exactly count it which is millions or actually hundreds of millions or",
    "start": "2155609",
    "end": "2160859"
  },
  {
    "text": "billions today where as the number of examples okay some of them Google told",
    "start": "2160859",
    "end": "2165869"
  },
  {
    "text": "me that they are doing costs of billions of examples but then they're talking about hundreds of billions of parameters so there's a there are at least two or",
    "start": "2165869",
    "end": "2172530"
  },
  {
    "text": "300 order of magnitudes between the number of examples and the dimensionality estimated any reasonable",
    "start": "2172530",
    "end": "2179940"
  },
  {
    "text": "way of determining things now this is a telling to meet me at least this is not",
    "start": "2179940",
    "end": "2185760"
  },
  {
    "text": "the right theory I mean it doesn't work of course there's a lot of good clever",
    "start": "2185760",
    "end": "2191460"
  },
  {
    "text": "people who are trying to estimate the dimensionality and to show you that this known F is actually very expressive that",
    "start": "2191460",
    "end": "2197609"
  },
  {
    "text": "they can express extremely complicated function which is cause true but this doesn't explain why they are performing",
    "start": "2197609",
    "end": "2204150"
  },
  {
    "text": "so well with so little data there must be something which regularize the problem in a very profound way so",
    "start": "2204150",
    "end": "2209849"
  },
  {
    "text": "what what I'm sure I'm saying is actually it's the essence of e instead of covering the hypothesis class let's",
    "start": "2209849",
    "end": "2216089"
  },
  {
    "text": "quantize the input so this is a gate for engineers this is a very very well-known",
    "start": "2216089",
    "end": "2222089"
  },
  {
    "text": "story because this is exactly what we do in sampling instead think about the Nyquist limit about the been limited",
    "start": "2222089",
    "end": "2228750"
  },
  {
    "text": "functions we actually in order to estimate the function we need it we need to know it in very small number of",
    "start": "2228750",
    "end": "2234960"
  },
  {
    "text": "points I mean it was only in the Indian IQ surfacing so most of the point we",
    "start": "2234960",
    "end": "2240299"
  },
  {
    "text": "actually actually don't don't know the value and still we estimate the function pretty well from samples okay because",
    "start": "2240299",
    "end": "2245630"
  },
  {
    "text": "because the sample functions are cluster Abell in some sense so the general the general notion here is take the class of",
    "start": "2245630",
    "end": "2253770"
  },
  {
    "text": "patterns X and cover it by sphere so X here this ellipse blue ellipse is all my",
    "start": "2253770",
    "end": "2261450"
  },
  {
    "text": "patterns all the images that I care about and now imagine that I can group",
    "start": "2261450",
    "end": "2266730"
  },
  {
    "text": "them or cluster them somehow in a partition this data into cells which are essentially homogeneous with respect to",
    "start": "2266730",
    "end": "2273000"
  },
  {
    "text": "the label which means that the probability of having the wrong label in each cell is smaller than epsilon how",
    "start": "2273000",
    "end": "2280660"
  },
  {
    "text": "I get it this is different story but let's say that I gave you this partition then I know that the number that the",
    "start": "2280660",
    "end": "2287020"
  },
  {
    "text": "number of functions go from as a boolean function from 2 to the cardinality of X to 2 to the graph of the partition now",
    "start": "2287020",
    "end": "2295560"
  },
  {
    "text": "ok so so this doesn't look like a very dramatic things but now I go to information theory and again I'm going",
    "start": "2295560",
    "end": "2302380"
  },
  {
    "text": "to skip all the notion of typicality do I need to remind you what it is ok so so we are talking about typical",
    "start": "2302380",
    "end": "2308680"
  },
  {
    "start": "2303000",
    "end": "2456000"
  },
  {
    "text": "patterns no and I actually need it and this is where the heart of information theory comes into the story typical",
    "start": "2308680",
    "end": "2315820"
  },
  {
    "text": "patterns are patterns where the probability of a pattern can be approximately factorized into men it",
    "start": "2315820",
    "end": "2322630"
  },
  {
    "text": "products a long product of conditionally independent terms so this is the story let's say with hidden Markov models or",
    "start": "2322630",
    "end": "2330010"
  },
  {
    "text": "with Markov random fields or with the Bayesian networks with some finite degree or with a physical system with",
    "start": "2330010",
    "end": "2337390"
  },
  {
    "text": "Hamiltonian with finite interaction and so many other systems that we know that essentially the distribution can be",
    "start": "2337390",
    "end": "2344350"
  },
  {
    "text": "written as a product or approximately asymptotically as a product dominated by single program in this case the log of",
    "start": "2344350",
    "end": "2350860"
  },
  {
    "text": "the probability is going to approach a limit and this limit is known as the general McMillan entropy so if and this",
    "start": "2350860",
    "end": "2358570"
  },
  {
    "text": "is of course the limiting probability but we know that if such limits exist the system has has a well-defined",
    "start": "2358570",
    "end": "2364390"
  },
  {
    "text": "entropy and at the same time almost all the patterns because this is going to",
    "start": "2364390",
    "end": "2370390"
  },
  {
    "text": "concentrate to this limit due to the central limit theorem eventually when n",
    "start": "2370390",
    "end": "2375670"
  },
  {
    "text": "is large enough most of the patterns are going to be very close to this limit which means that most of the patterns",
    "start": "2375670",
    "end": "2380830"
  },
  {
    "text": "are going to have the same probability and this is a property we called information theory asymptotic",
    "start": "2380830",
    "end": "2385870"
  },
  {
    "text": "equipartition I think this name is actually due to some cover but I'm not sure so so it's a it's a it's a it's",
    "start": "2385870",
    "end": "2393670"
  },
  {
    "text": "this particular number which tells us all typical patterns not only most almost all patterns are going to be",
    "start": "2393670",
    "end": "2400270"
  },
  {
    "text": "typical book all of all of them are going to be essentially equally likely which means that the size of the the",
    "start": "2400270",
    "end": "2406750"
  },
  {
    "text": "number of typical patterns is actually 1 over this probability which is 2 to the entropy okay so this is the the",
    "start": "2406750",
    "end": "2413770"
  },
  {
    "text": "fundamental trick that is used in information theory in both source coding and chunker and there and I'm going to",
    "start": "2413770",
    "end": "2419980"
  },
  {
    "text": "assume as that my partition is such that the pattern associate which each of these cell is are also typical so this",
    "start": "2419980",
    "end": "2428859"
  },
  {
    "text": "is again I divided my inputs into cells very the same way that I divided the glass of water into little drops which",
    "start": "2428859",
    "end": "2435820"
  },
  {
    "text": "are all in equilibrium and in the same trick exactly this works in statistical physics works here and in that sense I",
    "start": "2435820",
    "end": "2442450"
  },
  {
    "text": "assume not only that the pattern itself is difficult but the conditional on the partition is also typical so I can write",
    "start": "2442450",
    "end": "2448270"
  },
  {
    "text": "it as 2 to the minus conditional entropy now this is as I said this is where it's a very familiar trick but it's still",
    "start": "2448270",
    "end": "2454720"
  },
  {
    "text": "very important here so what I do now is that I can estimate the cardinality of",
    "start": "2454720",
    "end": "2460180"
  },
  {
    "start": "2456000",
    "end": "2596000"
  },
  {
    "text": "my partition using information essentially the size of the typical patterns to to the age divided by the",
    "start": "2460180",
    "end": "2466119"
  },
  {
    "text": "average size of the typical cell which is 2 to the conditional entropy age X given 3 so this is the cardinality of",
    "start": "2466119",
    "end": "2474190"
  },
  {
    "text": "the partition so now we plug so ganado the partition again is this 1 to 2 the",
    "start": "2474190",
    "end": "2480700"
  },
  {
    "text": "information because it's age X minus H is given to the same reason it appears in information theory but now it means",
    "start": "2480700",
    "end": "2487570"
  },
  {
    "text": "that the number of functions is 2 to the 2 to the information I have another",
    "start": "2487570",
    "end": "2494410"
  },
  {
    "text": "exponent now this is this is a tricky with those not terribly surprising when",
    "start": "2494410",
    "end": "2500589"
  },
  {
    "text": "you think about it I get this type of bound the bound is now that generation error is bounded by the exponent of the",
    "start": "2500589",
    "end": "2506950"
  },
  {
    "text": "mutual information between my representation and it should layers divided by number of examples which",
    "start": "2506950",
    "end": "2513010"
  },
  {
    "text": "means that any bit of compression of the representation is equivalent to doubling the data this is exponential improvement",
    "start": "2513010",
    "end": "2521410"
  },
  {
    "text": "in the bound so essentially if I compress as the layers seem to be doing",
    "start": "2521410",
    "end": "2526900"
  },
  {
    "text": "in this phase of compression I actually compress the representation by K beats",
    "start": "2526900",
    "end": "2532030"
  },
  {
    "text": "it's like doubling multiplying my data by 2 to the side of my data by 2 to the",
    "start": "2532030",
    "end": "2537580"
  },
  {
    "text": "K ok so I argued that this bounds much more realistic",
    "start": "2537580",
    "end": "2544770"
  },
  {
    "text": "if you actually look at the value of the information of course those of you who are a little more familiar with planning",
    "start": "2544770",
    "end": "2550000"
  },
  {
    "text": "Theory know that so essentially this bount become interesting when the information is essentially a bill of the",
    "start": "2550000",
    "end": "2556599"
  },
  {
    "text": "order of the log of the sample size now the log of the sample size is precisely the minimum number of samples",
    "start": "2556599",
    "end": "2563859"
  },
  {
    "text": "which I really need to label if I could choose them carefully this is the minimal number of queries so rather than",
    "start": "2563859",
    "end": "2569920"
  },
  {
    "text": "having randomly label example I could if I choose the labels I could settle for",
    "start": "2569920",
    "end": "2574930"
  },
  {
    "text": "this log M and this is precisely the classical sample complexity example",
    "start": "2574930",
    "end": "2580180"
  },
  {
    "text": "compression results of mantra BrahMos and few others from the 80s so we know this is not really surprising what is",
    "start": "2580180",
    "end": "2586839"
  },
  {
    "text": "surprising is that the network is actually minimizing this information and this is happening through diffusion so",
    "start": "2586839",
    "end": "2592750"
  },
  {
    "text": "okay so now I move to the last part of the story well actually it says the second part of story so okay so I hope I convinced you",
    "start": "2592750",
    "end": "2601089"
  },
  {
    "text": "that this information plan is interesting first of all you see there interesting dynamics things concentrate",
    "start": "2601089",
    "end": "2606940"
  },
  {
    "text": "somehow in a very nice way so the question is what is the bound the",
    "start": "2606940",
    "end": "2612640"
  },
  {
    "text": "maximal value of compression that I can achieve for a certain number of bits of information so this turned out to be a",
    "start": "2612640",
    "end": "2618760"
  },
  {
    "text": "question we asked a long time ago in the as I asked it in the context of understanding what is the minimum",
    "start": "2618760",
    "end": "2624339"
  },
  {
    "text": "sufficient statistic the middle sufficient statistic is as you may know is the simplest fashion compact function",
    "start": "2624339",
    "end": "2630640"
  },
  {
    "text": "of a sample and that capture all the information about the parameter of a distributor this is the classical",
    "start": "2630640",
    "end": "2636010"
  },
  {
    "text": "definition if we don't think about the parameter we think about the label is the thing we care about so if I want to",
    "start": "2636010",
    "end": "2641529"
  },
  {
    "text": "compress my data without losing information about the label and the way we do it is essentially by solving a",
    "start": "2641529",
    "end": "2647770"
  },
  {
    "text": "constrained variational problem minimize the mutual information between X and the representation which in this case I call",
    "start": "2647770",
    "end": "2653470"
  },
  {
    "text": "X hat because this is the notion use the information theory and radiation theory it's just a coincidence and and from",
    "start": "2653470",
    "end": "2660609"
  },
  {
    "text": "minimize the information to the representation subject to constrain on the information on the label now this",
    "start": "2660609",
    "end": "2667480"
  },
  {
    "text": "looks like a tricky problem it's actually solvable and we solve it by essentially an extension of what we call",
    "start": "2667480",
    "end": "2675250"
  },
  {
    "text": "the blot or Emoto algorithm theory or essentially it's it's it's an iteration between the L colder and the",
    "start": "2675250",
    "end": "2681700"
  },
  {
    "text": "decoder alternately and this iteration converged we know how to prove it almost always except some very important",
    "start": "2681700",
    "end": "2688779"
  },
  {
    "text": "critical points on the on the curve and they give you this interesting limit so essentially there is in this plan",
    "start": "2688779",
    "end": "2694319"
  },
  {
    "text": "information about the output information about the input there is a line this",
    "start": "2694319",
    "end": "2699369"
  },
  {
    "text": "black line which is the rate distortion like function which tells you above this line that no representation whatsoever",
    "start": "2699369",
    "end": "2706049"
  },
  {
    "text": "below this line that many and the optimal representation are exactly sitting on this line now this if I have",
    "start": "2706049",
    "end": "2713500"
  },
  {
    "text": "all the data so in principle not our first intuition was this is something we wrote in 2015 is that if nothing else",
    "start": "2713500",
    "end": "2719920"
  },
  {
    "text": "stops the network the layers should somehow converge to this line there was",
    "start": "2719920",
    "end": "2726430"
  },
  {
    "text": "absolutely no reason for saying there but but I said okay if they work so well maybe their optimal in this information",
    "start": "2726430",
    "end": "2732099"
  },
  {
    "text": "theoretic sense now notice that this is precisely the boundary was talking about this is a distribution dependent I mean",
    "start": "2732099",
    "end": "2739269"
  },
  {
    "text": "problem dependent bound but the bound is universal for any architecture whether it's your computer or your brain or an",
    "start": "2739269",
    "end": "2747009"
  },
  {
    "text": "alien from another galaxy nobody can do better than this bound okay so this is the absolute absolute",
    "start": "2747009",
    "end": "2753220"
  },
  {
    "text": "bound on this problem now the real problem that I really always need to emphasize is that you don't have an",
    "start": "2753220",
    "end": "2760539"
  },
  {
    "text": "infinite data unfortunately we always have a finite sample and if you have a",
    "start": "2760539",
    "end": "2766450"
  },
  {
    "start": "2764000",
    "end": "3385000"
  },
  {
    "text": "finite sample we actually cannot reach this this black line we reach a very",
    "start": "2766450",
    "end": "2772690"
  },
  {
    "text": "different bound which is this red line which means if I'm using a very little",
    "start": "2772690",
    "end": "2779380"
  },
  {
    "text": "compression a very fine partition of my data I will not generalize because most",
    "start": "2779380",
    "end": "2784839"
  },
  {
    "text": "of the cell would be empty I will not see enough labels to actually fill myself so in this case I'm going to",
    "start": "2784839",
    "end": "2792279"
  },
  {
    "text": "get a very poor prediction of my label from very fine partition on the other if I compress too much then the black line",
    "start": "2792279",
    "end": "2798700"
  },
  {
    "text": "hits me and I simply cannot generalize so there is some maximum somewhere here where where I want to be with finite",
    "start": "2798700",
    "end": "2807069"
  },
  {
    "text": "samples this is the best you can achieve this finite amount of data and this is the rigorous bounded we know how to calculate again only from the problem",
    "start": "2807069",
    "end": "2815560"
  },
  {
    "text": "and the number of random examples nothing else comes into the screen so so the argument is that indeed the network",
    "start": "2815560",
    "end": "2822010"
  },
  {
    "text": "are somehow fitting this pushing the last layer into this region now in these",
    "start": "2822010",
    "end": "2827800"
  },
  {
    "text": "pictures like highly exaggerated I mean in real in reality that the black line is a lot closer to the to the line but",
    "start": "2827800",
    "end": "2834970"
  },
  {
    "text": "it's still there and the red line depends of course on the sample size so there are two type of losses we need to",
    "start": "2834970",
    "end": "2840340"
  },
  {
    "text": "to work on one is that what I call the compression loss due to the fact that",
    "start": "2840340",
    "end": "2845680"
  },
  {
    "text": "actually compressed representation which is this the first part and then there's the finite sample loss which is the",
    "start": "2845680",
    "end": "2850870"
  },
  {
    "text": "difference in the red and the end and the end and the black and I want to minimize both of them okay so now",
    "start": "2850870",
    "end": "2856270"
  },
  {
    "text": "without so here is just just to give you a flavor what happens when you train with finite data so it's the same",
    "start": "2856270",
    "end": "2862810"
  },
  {
    "text": "problem the same put all small network but again I argue that this is the general picture we see it in all",
    "start": "2862810",
    "end": "2868240"
  },
  {
    "text": "problems on the right you see training with a lot of data in this case eighty percent of the data this one so you see",
    "start": "2868240",
    "end": "2875290"
  },
  {
    "text": "again there is this trajectories of the layers remember I'm talking about information plane information or a perversion output and you see this very",
    "start": "2875290",
    "end": "2882220"
  },
  {
    "text": "fast convergence to this green line which is what I call the inflection point where there's a sharp transition",
    "start": "2882220",
    "end": "2887380"
  },
  {
    "text": "and the behavior of the gradient so in a minute and then you see that there is this slow convergence see that all the",
    "start": "2887380",
    "end": "2894550"
  },
  {
    "text": "layers shift to the left and you see this by the number of epochs here is the color and you see that the color is",
    "start": "2894550",
    "end": "2900880"
  },
  {
    "text": "yellow for very high number of epochs and it's blue for very small number of epochs and you see that indeed they",
    "start": "2900880",
    "end": "2907150"
  },
  {
    "text": "slowly converge to some points on the other hand if you train them with small data it's only 5% of the data you get",
    "start": "2907150",
    "end": "2914380"
  },
  {
    "text": "the same initial phase they fit the data very well but then the information",
    "start": "2914380",
    "end": "2919660"
  },
  {
    "text": "through the compression goes down so this is something which we may call overfitting it's not obvious we think it's actually",
    "start": "2919660",
    "end": "2926080"
  },
  {
    "text": "over compression I mean you're simplifying your presentation beyond what you are actually allowed to do by",
    "start": "2926080",
    "end": "2932110"
  },
  {
    "text": "the data to her given so you cannot keep the homogeneity of the cells respect to",
    "start": "2932110",
    "end": "2937420"
  },
  {
    "text": "the labels because not have enough data okay so and this is somewhere in between is somewhere in between so of course the",
    "start": "2937420",
    "end": "2944080"
  },
  {
    "text": "nice the nice thing is that we actually have a theory for this line and the theory is",
    "start": "2944080",
    "end": "2949569"
  },
  {
    "text": "again data dependent but model independent okay so just to understand",
    "start": "2949569",
    "end": "2954670"
  },
  {
    "text": "what happens look at the same video again but this time at the bottom I show you the the training error",
    "start": "2954670",
    "end": "2960910"
  },
  {
    "text": "I'll show you generalization error but does method they look very similar and you see that the first phase was essentially changed exactly when the",
    "start": "2960910",
    "end": "2968230"
  },
  {
    "text": "error got into this me where it's essentially saturated so of course when",
    "start": "2968230",
    "end": "2973299"
  },
  {
    "text": "that error becomes more or less constant it's still going down but it's very slowly going down you see that the",
    "start": "2973299",
    "end": "2978880"
  },
  {
    "text": "gradients are going to be smaller because the oils more but now the mini-batches the difference between the",
    "start": "2978880",
    "end": "2985539"
  },
  {
    "text": "training of some examples another examples of these fluctuations start to dominate the story and indeed this is",
    "start": "2985539",
    "end": "2991510"
  },
  {
    "text": "the picture I really like to show so what you see here this is actually the network we used we used but as I said",
    "start": "2991510",
    "end": "2998260"
  },
  {
    "text": "this is not it's important only in one aspect that the layers get as the width",
    "start": "2998260",
    "end": "3003329"
  },
  {
    "text": "of the layers gets smaller and smaller so this sort of an Eiffel Tower if I use the different architecture I'm going to",
    "start": "3003329",
    "end": "3008670"
  },
  {
    "text": "see a different different type of behavior in the plan but the gist of it is that the last layer is doing",
    "start": "3008670",
    "end": "3014579"
  },
  {
    "text": "something very very similar so so so what you see here in different colors the the mean of the gradients I mean",
    "start": "3014579",
    "end": "3022380"
  },
  {
    "text": "those solid lines and the broken line are the standard deviation of the gradient pair weights in every one of",
    "start": "3022380",
    "end": "3028049"
  },
  {
    "text": "the layers and what I want to emphasize and actually turns out that we are not the first to see this so this is well",
    "start": "3028049",
    "end": "3034140"
  },
  {
    "text": "established that there are actually two phases to clear phases of the training",
    "start": "3034140",
    "end": "3040160"
  },
  {
    "text": "in the first one you see that the gradient is much larger than the standard deviation this is what",
    "start": "3040160",
    "end": "3047099"
  },
  {
    "text": "engineers like to call high signal-to-noise ratio this by the way is a log log plot so the difference in log",
    "start": "3047099",
    "end": "3052859"
  },
  {
    "text": "is the rate log of the ratio so this is a log signal-to-noise ratio now so in",
    "start": "3052859",
    "end": "3058440"
  },
  {
    "text": "the first phase is essentially two order of magnitude difference between the fluctuations and the mean this is what",
    "start": "3058440",
    "end": "3065430"
  },
  {
    "text": "we call clean credit okay on the other end when they reach this this gray line",
    "start": "3065430",
    "end": "3071970"
  },
  {
    "text": "around 300 epochs the as I said the gradient essentially saturate",
    "start": "3071970",
    "end": "3078180"
  },
  {
    "text": "it's not exactly saturated but it get to be very small so the Graduate the norm of the gradient is actually the norm of",
    "start": "3078180",
    "end": "3084420"
  },
  {
    "text": "the gradient in every layers squared and and and you get and you get a that the",
    "start": "3084420",
    "end": "3090029"
  },
  {
    "text": "gradient slow I get down there so those are the source of dispersion among the layers there the lower layers get a",
    "start": "3090029",
    "end": "3095760"
  },
  {
    "text": "larger gradient and then the smaller gradient and then them the highlight of em are dead but what is",
    "start": "3095760",
    "end": "3101730"
  },
  {
    "text": "really dramatic is that the variance jumps and from this point on essentially",
    "start": "3101730",
    "end": "3108150"
  },
  {
    "text": "the standard deviation is much larger than the mean so this is what you all call low signal generalization which",
    "start": "3108150",
    "end": "3115559"
  },
  {
    "text": "means a very noisy gradient so so there's like a very clear and an analogy",
    "start": "3115559",
    "end": "3122279"
  },
  {
    "text": "dual picture of the same story that we saw with information in the gradients actually I think that the I know that",
    "start": "3122279",
    "end": "3127289"
  },
  {
    "text": "the gradients determined the picture actually actually solve a Fokker Planck equation using those gradients you know",
    "start": "3127289",
    "end": "3134490"
  },
  {
    "text": "that the propagates back to a Rayleigh distribution and show you that information indeed does it so but notice what happens here there are actually two",
    "start": "3134490",
    "end": "3140640"
  },
  {
    "text": "phases one of them is also called drift so those of you who know what the fork of my equation is the Pokemon equation is a linear term which is governed",
    "start": "3140640",
    "end": "3146880"
  },
  {
    "text": "governing the drape and has a quadratical second derivative term which is governed by the fixations if the",
    "start": "3146880",
    "end": "3152819"
  },
  {
    "text": "linear time is larger essentially you just move and the distribution does move linearly if the second term is larger",
    "start": "3152819",
    "end": "3158819"
  },
  {
    "text": "during a diffusion equation much larger so you essentially dominated by diffusion and it turns out that if my",
    "start": "3158819",
    "end": "3165299"
  },
  {
    "text": "story is correct that the diffusion phase of the gradient is the most important part of the learning and you",
    "start": "3165299",
    "end": "3172349"
  },
  {
    "text": "see that there is the diffusion phase actually have many many details that I can see in this in this picture for example this peak is related to the fall",
    "start": "3172349",
    "end": "3180119"
  },
  {
    "text": "into a very flat minima and and and and the fact that the difference between the",
    "start": "3180119",
    "end": "3185940"
  },
  {
    "text": "variance and the mean in log scale is essentially constant telling is telling me that the log signal-to-noise ratio of",
    "start": "3185940",
    "end": "3191490"
  },
  {
    "text": "the gradient is essentially constant when I approach the convergence which is completely consistent with the Gaussian",
    "start": "3191490",
    "end": "3196890"
  },
  {
    "text": "bound on the information because the log of the one to signal-to-noise ratio is the Gaussian bound on the mutual information so there's a beautiful",
    "start": "3196890",
    "end": "3204319"
  },
  {
    "text": "confirmation or reassurance of the whole story just coming from the gradients actually what we do now is really",
    "start": "3204319",
    "end": "3209339"
  },
  {
    "text": "solving the fokker-planck equation the first two moments of the distribution using the fokker-planck",
    "start": "3209339",
    "end": "3214680"
  },
  {
    "text": "equation and we see exactly the same story without measuring information at all to talk about it by the way so so",
    "start": "3214680",
    "end": "3228569"
  },
  {
    "text": "the story now the king began becomes a little clearer there are two phases to the gradient here you see the signal-to-noise ratio",
    "start": "3228569",
    "end": "3235170"
  },
  {
    "text": "on the left here so the second Arizona has this very sharp drop it's not only a",
    "start": "3235170",
    "end": "3240779"
  },
  {
    "text": "phase condition is some sort of a crossover and and and and in this low phase gradients are very noisy and this",
    "start": "3240779",
    "end": "3248279"
  },
  {
    "text": "- is great are very clean in the gray phase I essentially fit the labels in the slow phase I essentially learn to",
    "start": "3248279",
    "end": "3255119"
  },
  {
    "text": "forget the irrelevant details of the problem so this is some sort of an unsupervised learning I mean because",
    "start": "3255119",
    "end": "3260759"
  },
  {
    "text": "nobody as isolate is labeling the background but the fact that I see some",
    "start": "3260759",
    "end": "3265829"
  },
  {
    "text": "pictures with this background something other picture with another background is giving me a lot of information on what",
    "start": "3265829",
    "end": "3271319"
  },
  {
    "text": "to ignore because there are fluctuations in the mini-batches and by the way if you change the size of",
    "start": "3271319",
    "end": "3277349"
  },
  {
    "text": "the mini batch you see that these two points they the point where you you start to forget I mean this point is is",
    "start": "3277349",
    "end": "3284579"
  },
  {
    "text": "exactly linearly related to the phase transition in the gardens so any any",
    "start": "3284579",
    "end": "3290039"
  },
  {
    "text": "actually but even when you train with the full batch I mean you and some people do I mean let's say that I don't",
    "start": "3290039",
    "end": "3295440"
  },
  {
    "text": "want many batches so this is actually done in another attempt to to show this be wrong and you train it with with full",
    "start": "3295440",
    "end": "3303059"
  },
  {
    "text": "batch and you get compression as well but so if we do it with full batch you see that the full batch is somewhere",
    "start": "3303059",
    "end": "3309690"
  },
  {
    "text": "along this line it's just a much much slower compression and indeed even with full batch there's noise there's",
    "start": "3309690",
    "end": "3316680"
  },
  {
    "text": "quantization noise in time there is a noise due to all sorts of sampling error and so on so this small noise is enough",
    "start": "3316680",
    "end": "3323759"
  },
  {
    "text": "to eventually compress the plantation now okay so we see this in many problems",
    "start": "3323759",
    "end": "3329579"
  },
  {
    "text": "and and I just want to skip it because we don't have time the the gist of it again I'm using this",
    "start": "3329579",
    "end": "3335819"
  },
  {
    "text": "tool to often surprisingly or not the layers in this problem converge to the optimal bound so",
    "start": "3335819",
    "end": "3344369"
  },
  {
    "text": "what you see here in blue is the information button the curve which I calculate using rate distortion using the blotter Emoto algorithm",
    "start": "3344369",
    "end": "3350700"
  },
  {
    "text": "because I know the distribution of the problem and what you see in red those crosses the endpoint of the layers in",
    "start": "3350700",
    "end": "3358530"
  },
  {
    "text": "this plan and of course we magnified everything this is just the region between 0.99 to 1 so everything is just",
    "start": "3358530",
    "end": "3364830"
  },
  {
    "text": "blow-up of the top of the curve but this is this is beautiful because indeed it",
    "start": "3364830",
    "end": "3370680"
  },
  {
    "text": "turns out that those layers form some sort of what we call a successively refine durable code of the problem say",
    "start": "3370680",
    "end": "3377640"
  },
  {
    "text": "and and and and now it's entirely different story and why it happens okay so why it happens I'll give you just the",
    "start": "3377640",
    "end": "3383910"
  },
  {
    "text": "the again they they're very sketchy proof of it okay so we know I mean what",
    "start": "3383910",
    "end": "3392910"
  },
  {
    "start": "3385000",
    "end": "3599000"
  },
  {
    "text": "what what is it in in in the stochastic dynamics it actually put you to optimal compression this is not something we",
    "start": "3392910",
    "end": "3399780"
  },
  {
    "text": "used to have an information theory I mean nobody as far as I know is composed are many random codes all over the place",
    "start": "3399780",
    "end": "3406530"
  },
  {
    "text": "but nobody is using noise I mean random walks I mean diffusion processes in order to compress maybe you should so so",
    "start": "3406530",
    "end": "3414300"
  },
  {
    "text": "if you look at that what actually happens again in a very sketchy way there is a lunch of in dynamics which",
    "start": "3414300",
    "end": "3420210"
  },
  {
    "text": "means the weight as a function exact the time derivative of the weights is proportional to minus the gradient of",
    "start": "3420210",
    "end": "3427050"
  },
  {
    "text": "the error but it's not a clean arrow there's some noise added to it and the noise is exactly due to those many",
    "start": "3427050",
    "end": "3433410"
  },
  {
    "text": "batches so this is a gradient descent or a gradient flow with noise and we know",
    "start": "3433410",
    "end": "3439740"
  },
  {
    "text": "that you know for a long time now that this under some conditions which convex which are not true here like convex a",
    "start": "3439740",
    "end": "3445920"
  },
  {
    "text": "energy surface of things like this and your converge to a gift distribution adventure or to a maximum entropy",
    "start": "3445920",
    "end": "3451950"
  },
  {
    "text": "distribution so so the stationary distribution of this dynamics is exponential in the training err so this",
    "start": "3451950",
    "end": "3459570"
  },
  {
    "text": "is known since the twenties hundred years sooner now a this is that",
    "start": "3459570",
    "end": "3466440"
  },
  {
    "text": "usually the convergence is very fast if you are indeed in a convex space but you",
    "start": "3466440",
    "end": "3471840"
  },
  {
    "text": "are not in a convex place forget it so actually they are they think about a very high dimensional space where only",
    "start": "3471840",
    "end": "3478410"
  },
  {
    "text": "few of the irrelevant just thought that features actually changed the label think about",
    "start": "3478410",
    "end": "3484109"
  },
  {
    "text": "our face recognition so I know yeah I know they distantly in the eyes and the size of the nose and so if you are the",
    "start": "3484109",
    "end": "3489330"
  },
  {
    "text": "very twenty features that you can these are the relevant dimensions and all the",
    "start": "3489330",
    "end": "3495540"
  },
  {
    "text": "other million dimensions of the network are essentially irrelevant so what if",
    "start": "3495540",
    "end": "3500910"
  },
  {
    "text": "you think about the covariance matrix of the noise it's going to be extremely elongated very small in the relevant",
    "start": "3500910",
    "end": "3507780"
  },
  {
    "text": "dimension is very and very wide in the area of the dimensions now is it good or bad it's actually good because what",
    "start": "3507780",
    "end": "3514349"
  },
  {
    "text": "happens at you is very quickly converging the relevant dimension of the problem and you stay there this happens",
    "start": "3514349",
    "end": "3519750"
  },
  {
    "text": "in the drift phase but then in the in the diffuser phase they essentially do a random walk in all the irrelevant",
    "start": "3519750",
    "end": "3525150"
  },
  {
    "text": "dimensions this random walk is increasing the entropy of the irrelevant",
    "start": "3525150",
    "end": "3531420"
  },
  {
    "text": "parameters by and I do it in every one of the million dimension of the problem",
    "start": "3531420",
    "end": "3537270"
  },
  {
    "text": "so the fact that each one of them is just like one random walk is is average with the fact that another one is doing",
    "start": "3537270",
    "end": "3542580"
  },
  {
    "text": "another random walk so on average you actually get this gives distribution in",
    "start": "3542580",
    "end": "3547890"
  },
  {
    "text": "the irrelevant dimensions and as you know the dynamics of diffusion I'm sure",
    "start": "3547890",
    "end": "3555660"
  },
  {
    "text": "you all know that diffusion grows like a Gaussian where the width grows like the square root of time okay in flat space",
    "start": "3555660",
    "end": "3563480"
  },
  {
    "text": "okay so this is what we all learned in school in elementary school I think that",
    "start": "3563480",
    "end": "3568650"
  },
  {
    "text": "the diffusion grows like a square root of time and there and essentially this tells us that the time that it takes to",
    "start": "3568650",
    "end": "3575270"
  },
  {
    "text": "grow the entropy is exponential in the entropy because I forgot to tell you",
    "start": "3575270",
    "end": "3581460"
  },
  {
    "text": "that the entropy of a Gaussian is the log of the stun duration so entropy grows like square root of time but like",
    "start": "3581460",
    "end": "3590339"
  },
  {
    "text": "log of square root of time and time grows like exponent of the entropy so this turns out to be quite interesting",
    "start": "3590339",
    "end": "3596160"
  },
  {
    "text": "because this gives us a completely new understanding of why the layers help you",
    "start": "3596160",
    "end": "3601619"
  },
  {
    "text": "and I'm going to finish with this it's one hour and five minutes I was told",
    "start": "3601619",
    "end": "3607619"
  },
  {
    "text": "that I have 75 minutes okay so essentially this is really a beautiful",
    "start": "3607619",
    "end": "3614400"
  },
  {
    "text": "picture you like it so it's the same problem the same specific network that",
    "start": "3614400",
    "end": "3619920"
  },
  {
    "text": "I'm playing with all the time but again this is not specifically this problem I trained it once with one hidden layer on",
    "start": "3619920",
    "end": "3627600"
  },
  {
    "text": "the top top left and then two hidden layers three hidden layers and so on until you get to five or six hidden",
    "start": "3627600",
    "end": "3634050"
  },
  {
    "text": "layers and the remarkable thing about this is that when you look at the colors",
    "start": "3634050",
    "end": "3639150"
  },
  {
    "text": "okay remember yellow is ten thousand epochs a lot of time and blue purple is",
    "start": "3639150",
    "end": "3645500"
  },
  {
    "text": "very short time so we saw that is the first layer this one one hidden layer",
    "start": "3645500",
    "end": "3650940"
  },
  {
    "text": "you see that he takes essentially forever I mean within 10,000 efforts I didn't converge to a good solution it's",
    "start": "3650940",
    "end": "3657720"
  },
  {
    "text": "a very simple Network I know it can learn the problem the problem can be learned was one hidden layer takes take",
    "start": "3657720",
    "end": "3664440"
  },
  {
    "text": "a very long time now when I start to increase the network move to six hidden",
    "start": "3664440",
    "end": "3669630"
  },
  {
    "text": "layers a lot more parameters and lost not complicated but you see this all down in the purple essentially very",
    "start": "3669630",
    "end": "3676740"
  },
  {
    "text": "quickly within a few hundred epochs I converts to very good solution good solution means high information about",
    "start": "3676740",
    "end": "3682500"
  },
  {
    "text": "the label so surprising or contra intuitively the larger the network the",
    "start": "3682500",
    "end": "3690210"
  },
  {
    "text": "faster the convergence okay so no this",
    "start": "3690210",
    "end": "3696390"
  },
  {
    "text": "is the experiment I mean these are just the theory has to come now okay so what this is what we see in simulations",
    "start": "3696390",
    "end": "3703020"
  },
  {
    "text": "actually so this is a small problem you can actually compare these things so",
    "start": "3703020",
    "end": "3708270"
  },
  {
    "text": "there is some advantage to having many layers and most of the advantages in this diffusion process because we all we",
    "start": "3708270",
    "end": "3714900"
  },
  {
    "text": "see that we get very quickly even with one even layer to the point of compression and then it starts to slow",
    "start": "3714900",
    "end": "3720980"
  },
  {
    "text": "okay so let's think again about diffusion so I told you that diffusion",
    "start": "3720980",
    "end": "3726120"
  },
  {
    "text": "is actually a Langevin process I mean no adding noise and I converge to this",
    "start": "3726120",
    "end": "3732720"
  },
  {
    "text": "distribution but think about I just told you that essentially increasing the",
    "start": "3732720",
    "end": "3738660"
  },
  {
    "text": "entropy of the width the width of a line increased the width of the distribution in the irrelevant variable the second",
    "start": "3738660",
    "end": "3744420"
  },
  {
    "text": "show you varrick's specifically you decrease the signal-to-noise ratio of the relevant irrelevant variables which means that",
    "start": "3744420",
    "end": "3750000"
  },
  {
    "text": "you forget them you don't transfer the information the capacity of the area of a viable is this decreased this is just a Gaussian bound",
    "start": "3750000",
    "end": "3757230"
  },
  {
    "text": "on the on the capacity so so how long does it take to it should actually",
    "start": "3757230",
    "end": "3762720"
  },
  {
    "text": "increase this this entropy so we know that delta T is exponential in time that's what I told you before so for",
    "start": "3762720",
    "end": "3769800"
  },
  {
    "text": "this flecked irrelevant variables the increase of entropy is very slow I mean",
    "start": "3769800",
    "end": "3776580"
  },
  {
    "text": "it takes exponential time in every bit of compression okay so this will take forever to compress ten bits or",
    "start": "3776580",
    "end": "3782820"
  },
  {
    "text": "something yeah but imagine now that I put in this this strain of a Markov chain and all of the layers are doing",
    "start": "3782820",
    "end": "3790290"
  },
  {
    "text": "parallel diffusion because that's what I do I add noise to all of them so they",
    "start": "3790290",
    "end": "3795570"
  },
  {
    "text": "start to push each other so instead of having time which is exponentially in the some of those compression but the",
    "start": "3795570",
    "end": "3802020"
  },
  {
    "text": "compression is the difference in information between layer to layer I moved to the maximum actually of those",
    "start": "3802020",
    "end": "3807630"
  },
  {
    "text": "turns which is larger than the sum so this means that there is some sort of a boost of compression because all of them",
    "start": "3807630",
    "end": "3814350"
  },
  {
    "text": "move together they really help each other compressing so there is some sort of enormous like a train with a lot of",
    "start": "3814350",
    "end": "3820920"
  },
  {
    "text": "engines the first moves faster it's no it's not a good analogy bits a lot more in just a linear increase its exponential increase in time because I",
    "start": "3820920",
    "end": "3828840"
  },
  {
    "text": "move from an exponent of a sum to a sum of exponents or actually the maximum of those exponents so this is my really",
    "start": "3828840",
    "end": "3836310"
  },
  {
    "text": "very intuitive and very rough picture at this point of why the layers help you during this diffusion they actually",
    "start": "3836310",
    "end": "3842040"
  },
  {
    "text": "enhance the diffusion and each each layer only have to compress from the point that the previous one forgot",
    "start": "3842040",
    "end": "3848430"
  },
  {
    "text": "already of course this is not exactly true because you see that the layers the first layers don't forget everything for",
    "start": "3848430",
    "end": "3855120"
  },
  {
    "text": "it only part of the story okay so so the next part of the story which I am only going to to sketch and just so the",
    "start": "3855120",
    "end": "3862320"
  },
  {
    "text": "interesting question is now okay so we understand quite a lot already we know that it's the noise in the gradients",
    "start": "3862320",
    "end": "3868620"
  },
  {
    "text": "which is doing the work for us we know that it's doing it because it managed to D correlate the and the",
    "start": "3868620",
    "end": "3875580"
  },
  {
    "text": "irrelevant variables of the problem from the label this is a slow process because nobody's",
    "start": "3875580",
    "end": "3880740"
  },
  {
    "text": "labeling it for us so it happens because there are fluctuations batch to batch fluctuations because their variant",
    "start": "3880740",
    "end": "3886829"
  },
  {
    "text": "there's a variance in the background the problem the next question is what governs the convergence of the layers",
    "start": "3886829",
    "end": "3892380"
  },
  {
    "text": "were where do they converge to this into B so this is actually where the layers end up in this flan but not of the",
    "start": "3892380",
    "end": "3898710"
  },
  {
    "text": "function of the epochs but actually the function of the number of examples and you see that it's remarkably consistent",
    "start": "3898710",
    "end": "3904700"
  },
  {
    "text": "so now we have I have a completely different story to tell which I'm not going to do what governs the location of",
    "start": "3904700",
    "end": "3911549"
  },
  {
    "text": "the layers in this plane it has to do with the structure of the problem itself because now we have new map deep",
    "start": "3911549",
    "end": "3916559"
  },
  {
    "text": "learning into information theory and now we can use information theory to",
    "start": "3916559",
    "end": "3922049"
  },
  {
    "text": "actually calculate things for example if there is symmetry in the problem we can show that the symmetric problem can",
    "start": "3922049",
    "end": "3927690"
  },
  {
    "text": "learn faster because the structure of this landscape this energy landscape is much simpler what I want to show you",
    "start": "3927690",
    "end": "3933749"
  },
  {
    "text": "just to end and end the top with a direct simulation actually looking",
    "start": "3933749",
    "end": "3940289"
  },
  {
    "text": "inside those layers using what we call the dimensional scaling I'm taking this 10 dimensional or 20 dimensional layer",
    "start": "3940289",
    "end": "3948390"
  },
  {
    "text": "and project it to two dimension and I run all the 4,000 inputs and mark the",
    "start": "3948390",
    "end": "3956819"
  },
  {
    "text": "points in two dimensions and if if the label of the output is is is one is I",
    "start": "3956819",
    "end": "3962039"
  },
  {
    "text": "mark it with black and if the label of the output is is zero I mark it red and",
    "start": "3962039",
    "end": "3968640"
  },
  {
    "text": "if it's somewhere in between we market somewhere in between these colors so essentially what I want to service the dynamics of the topology of the network",
    "start": "3968640",
    "end": "3975739"
  },
  {
    "text": "so this is the the first hidden layer and remember in order for my theory to",
    "start": "3975739",
    "end": "3982410"
  },
  {
    "text": "make sense I need those partitions to be homogeneous respect to the label at some",
    "start": "3982410",
    "end": "3987569"
  },
  {
    "text": "point and notice that again so you really see this really crystallization I mean red attracts red and black attracts",
    "start": "3987569",
    "end": "3994380"
  },
  {
    "text": "red and eventually you get this this you know pepper-and-salt picture of clusters",
    "start": "3994380",
    "end": "3999450"
  },
  {
    "text": "that some of them are red and some of them are black and you see that the colors became strong which means that",
    "start": "3999450",
    "end": "4005289"
  },
  {
    "text": "indeed those clusters those partitions become more homogeneous and this is only a two-dimensional projection so even in",
    "start": "4005289",
    "end": "4011720"
  },
  {
    "text": "to diminish where can you see this but this is highly non linearly separable if I want",
    "start": "4011720",
    "end": "4017420"
  },
  {
    "text": "to separate the black from the red here I need to find a very funny and very complicated curve even for this simple",
    "start": "4017420",
    "end": "4023720"
  },
  {
    "text": "problem now if I'm right this representation of the layers should get",
    "start": "4023720",
    "end": "4029330"
  },
  {
    "text": "better and better when I move to higher layers in terms of ability to decode the",
    "start": "4029330",
    "end": "4036770"
  },
  {
    "text": "label so okay so this is the first hidden layer this is the fourth hidden layer in this page in this in this",
    "start": "4036770",
    "end": "4042320"
  },
  {
    "text": "particular problem look what happened during the same a box in the fourth layer so again at the beginning there is",
    "start": "4042320",
    "end": "4049220"
  },
  {
    "text": "a strong bias to black I mean the offset is not yet set and so on but eventually even in the student master scaling you",
    "start": "4049220",
    "end": "4057290"
  },
  {
    "text": "see that the red goes in one side and the black goes in another side but you",
    "start": "4057290",
    "end": "4063530"
  },
  {
    "text": "see something else which is quite remarkable you see this dimensionality collapse you see that eventually the",
    "start": "4063530",
    "end": "4069230"
  },
  {
    "text": "manifold on which the layer lies is some low dimensional manifold which is highly",
    "start": "4069230",
    "end": "4074540"
  },
  {
    "text": "convoluted and highly folded in this space but this is precisely this relevant dimension which in this case is",
    "start": "4074540",
    "end": "4080090"
  },
  {
    "text": "one dimensional so you see that after enough training the higher layers really",
    "start": "4080090",
    "end": "4086090"
  },
  {
    "text": "represent this low dimensional occurred manifold of relevant variables and all",
    "start": "4086090",
    "end": "4091130"
  },
  {
    "text": "the environment variable disappeared they were forgotten and so if you wait long enough you you really see red",
    "start": "4091130",
    "end": "4097339"
  },
  {
    "text": "separated from black very nicely again it's a highly nonlinear but I can still almost separate them completely by one",
    "start": "4097340",
    "end": "4103370"
  },
  {
    "text": "line and you see this collapse of dimensionality eventually the manifold",
    "start": "4103370",
    "end": "4108680"
  },
  {
    "text": "goes to one dimensional so I actually believe that this is the picture in general I mean and actually that many",
    "start": "4108680",
    "end": "4114589"
  },
  {
    "text": "and we can it's hard to see in high dimension how to visualize an animation but indeed what happens is that the",
    "start": "4114590",
    "end": "4120560"
  },
  {
    "text": "irrelevant dimensions are disappeared because they added variants to the",
    "start": "4120560",
    "end": "4126230"
  },
  {
    "text": "gradients okay so I have a lot of other things to say but I think I'll stop here so essentially I argue that this",
    "start": "4126230",
    "end": "4134089"
  },
  {
    "text": "information plane picture gives you really not only nice visualization which is unique it's some sort of an x-ray of",
    "start": "4134090",
    "end": "4140240"
  },
  {
    "text": "the of the network it's also we are giving us a lot of insight to understand what's actually going on",
    "start": "4140240",
    "end": "4146040"
  },
  {
    "text": "when you combine it with them actually this summer is somewhat oh it's a if I",
    "start": "4146040",
    "end": "4153660"
  },
  {
    "text": "combine it with the picture of the gradients I get this very consistent picture and that there's a first drift",
    "start": "4153660",
    "end": "4160109"
  },
  {
    "text": "phase which is very fitting the label and then a noisy phase with the essentially consistent with we're moving",
    "start": "4160109",
    "end": "4166680"
  },
  {
    "text": "be irrelevant variables and the reduction of the dimensionality because all the irrelevant dimensions eventually",
    "start": "4166680",
    "end": "4173160"
  },
  {
    "text": "don't pass through the layers and remain irrelevant now there are many many interesting questions has come out of",
    "start": "4173160",
    "end": "4179040"
  },
  {
    "text": "this analysis I mentioned some already I mean okay so can you predict and the",
    "start": "4179040",
    "end": "4184529"
  },
  {
    "text": "location of the lay layers yes in some cases we can in the cases where there is an exact symmetry underneath and",
    "start": "4184530",
    "end": "4189960"
  },
  {
    "text": "actually all the problem I was actually using was I chose it very carefully to",
    "start": "4189960",
    "end": "4195810"
  },
  {
    "text": "have an explicit oh three symmetry so these are points on the sphere and the label depends on some ratio of the",
    "start": "4195810",
    "end": "4201840"
  },
  {
    "text": "deeper type of the quadrupole moments so those of you know some something about the spherical harmonics in degrees this",
    "start": "4201840",
    "end": "4208380"
  },
  {
    "text": "is a invariant rotation and essentially what I argue that the layers in this case are going to collapse on",
    "start": "4208380",
    "end": "4215220"
  },
  {
    "text": "irreducible representations of the symmetry that is a whole new theory which really relates learning to atomic",
    "start": "4215220",
    "end": "4221760"
  },
  {
    "text": "physics if you want but it's very nice on the other hand so this is directly related to the structure of the",
    "start": "4221760",
    "end": "4228920"
  },
  {
    "text": "solutions on this information but on the curl and I know that it is this your",
    "start": "4228920",
    "end": "4234240"
  },
  {
    "text": "this representation essentially water feeling like the Gaussian channel water feeling these are collapsed to load the",
    "start": "4234240",
    "end": "4239580"
  },
  {
    "text": "measure representation of the problem to preserve information so this is just an extension of the Gaussian channels and",
    "start": "4239580",
    "end": "4245850"
  },
  {
    "text": "Gaussian very distortion problems to this so this is one the other thing is what can you do with with for example",
    "start": "4245850",
    "end": "4252750"
  },
  {
    "text": "transfer learning I mean you want to learn one problem and so I know already that the lower layers represent fine",
    "start": "4252750",
    "end": "4259050"
  },
  {
    "text": "details of the problem usually they'll learn local symmetries and the higher layers are really those global features",
    "start": "4259050",
    "end": "4265590"
  },
  {
    "text": "you see it's very nicely here because you see it was level of information they",
    "start": "4265590",
    "end": "4270750"
  },
  {
    "text": "actually remember the problem this has something to do with okay I want to move",
    "start": "4270750",
    "end": "4276720"
  },
  {
    "text": "from one to the other I want to learn from on data about another data it tells it also has some very poor very",
    "start": "4276720",
    "end": "4282739"
  },
  {
    "text": "bad message to neuroscience at least if you believe that this has anything to do with neuroscience is that neurons but",
    "start": "4282739",
    "end": "4288530"
  },
  {
    "text": "it's individual neurons actually tell you nothing actually so I don't believe",
    "start": "4288530",
    "end": "4293780"
  },
  {
    "text": "all these stories that you find in deep learning you know if a cell or a castle",
    "start": "4293780",
    "end": "4299150"
  },
  {
    "text": "or an ISIL or whatever this is completely anecdotal I mean it's highly sensitive to the",
    "start": "4299150",
    "end": "4304340"
  },
  {
    "text": "architecture of course if using convolution neural networks and you constrain the receptive fields and you",
    "start": "4304340",
    "end": "4309470"
  },
  {
    "text": "do also to other things then this scrambling of the layers disappear but with fully connected methodologies but",
    "start": "4309470",
    "end": "4316670"
  },
  {
    "text": "we actually have a complete theory of what do these layers really represent and it has to do again with the",
    "start": "4316670",
    "end": "4322760"
  },
  {
    "text": "structure of the face transitions of the program but this is another talk ok so",
    "start": "4322760",
    "end": "4327770"
  },
  {
    "text": "I'll stop here thank you very much [Applause]",
    "start": "4327770",
    "end": "4338010"
  },
  {
    "text": "just a little bit of information is the",
    "start": "4338010",
    "end": "4361120"
  },
  {
    "text": "graduating assented us I mean the fact that you use stochastic gradient descent",
    "start": "4361120",
    "end": "4367000"
  },
  {
    "text": "I mean you you a noisy version of the gradient of the empirical laws so first",
    "start": "4367000",
    "end": "4372340"
  },
  {
    "text": "of all empirical laws the false if you do use it actually the cross-entropy laws or the log laws and which is",
    "start": "4372340",
    "end": "4378010"
  },
  {
    "text": "essentially mutual information but that's that's the site okay but but because you're using stochastic gradient",
    "start": "4378010",
    "end": "4384310"
  },
  {
    "text": "descent you get these two phases of gradients okay this is has nothing to do",
    "start": "4384310",
    "end": "4389860"
  },
  {
    "text": "with mutual information okay you get that you very quickly reduce the error",
    "start": "4389860",
    "end": "4395170"
  },
  {
    "text": "on the labels and then very slowly when the error in the level essentially saturates it's these noise in the",
    "start": "4395170",
    "end": "4402520"
  },
  {
    "text": "gradients which all people think is random it's not random at all it's actually very informative now what I'm",
    "start": "4402520",
    "end": "4408100"
  },
  {
    "text": "telling you okay this is stochastic where this descent has nothing to do information I didn't need information to tell you that but now I look at what",
    "start": "4408100",
    "end": "4415810"
  },
  {
    "text": "Socastee gradients actually do I look at the dynamics of diffusion and the",
    "start": "4415810",
    "end": "4421270"
  },
  {
    "text": "dynamic of diffusion using physics it's pushing us to maximum entropy",
    "start": "4421270",
    "end": "4426400"
  },
  {
    "text": "distributions is losing information so",
    "start": "4426400",
    "end": "4431560"
  },
  {
    "text": "surprisingly it's the dynamic of the stochastic gradient descent which is decreasing the information so okay so",
    "start": "4431560",
    "end": "4437980"
  },
  {
    "text": "that's the connection between gradient descent and information on the other hand I showed you at the beginning of my",
    "start": "4437980",
    "end": "4443860"
  },
  {
    "text": "talk that reducing information is the improving generalization so if I actually compress the input by the way I",
    "start": "4443860",
    "end": "4449560"
  },
  {
    "text": "can compress the input by many different other means for example adding nodes so there's a very elegant working in",
    "start": "4449560",
    "end": "4455950"
  },
  {
    "text": "computer science a call of stability bounds for example if you if your problem is is stable to fluctuations in",
    "start": "4455950",
    "end": "4461800"
  },
  {
    "text": "the inputs then you generalize what you think about its stability noise even if I add some little noise to the input and",
    "start": "4461800",
    "end": "4468250"
  },
  {
    "text": "the label doesn't change means that there is some sort of sphere around my patterns with which sex to",
    "start": "4468250",
    "end": "4474250"
  },
  {
    "text": "which I mean violence a label is invariant so this is compression but the same is true about symmetry if the",
    "start": "4474250",
    "end": "4480130"
  },
  {
    "text": "Association symmetry rotation symmetry anything like this it means that there are always of some class of transformations which to which the label",
    "start": "4480130",
    "end": "4487030"
  },
  {
    "text": "is invariant and this is the compression bound which means that there is a group of of patterns which I can put together",
    "start": "4487030",
    "end": "4494079"
  },
  {
    "text": "in one cell which will have the same label so this is again by the same",
    "start": "4494079",
    "end": "4500110"
  },
  {
    "text": "argument the number of labels that you need is not smaller it depends on you and or which of this group and so on and so forth I give you essentially any",
    "start": "4500110",
    "end": "4506619"
  },
  {
    "text": "generalization bound except those that are what we call problems just wrongly",
    "start": "4506619",
    "end": "4512199"
  },
  {
    "text": "mixing like parity functions like encrypted there I mean whenever you have",
    "start": "4512199",
    "end": "4517750"
  },
  {
    "text": "high sensitivity to noise in your input you're not going to learn information",
    "start": "4517750",
    "end": "4523060"
  },
  {
    "text": "theory is not telling you that but you're not going to learn because you don't have this ability to compress the representation if a single bit in your",
    "start": "4523060",
    "end": "4530500"
  },
  {
    "text": "in your inputs is going to change the label forget it deep learning will not learn it so it's not a direct connection",
    "start": "4530500",
    "end": "4536980"
  },
  {
    "text": "the connection is between the physics and the dynamics of the problem and then",
    "start": "4536980",
    "end": "4542560"
  },
  {
    "text": "the miraculous fact that information theory is telling you something about typical behavior of large problems so",
    "start": "4542560",
    "end": "4548679"
  },
  {
    "text": "I'm using combining it's just like you know entropy and energy tell us something about the equilibrium state of matter here it's not because the",
    "start": "4548679",
    "end": "4554860"
  },
  {
    "text": "molecules are calculate entropy is because entropy is describing the distribution for us it's not an",
    "start": "4554860",
    "end": "4561250"
  },
  {
    "text": "intrinsic properties it's a global so it's the same story again it's not a",
    "start": "4561250",
    "end": "4566980"
  },
  {
    "text": "side effect this is the right mathematics to think about it but it's for the same reason exactly the same",
    "start": "4566980",
    "end": "4572409"
  },
  {
    "text": "rationale that molecules in this room are very much very close to gives",
    "start": "4572409",
    "end": "4577659"
  },
  {
    "text": "distribution subsistence and temperature and energies it's the same rational it's not because the molecule calculate",
    "start": "4577659",
    "end": "4583300"
  },
  {
    "text": "entropy it's because this is the most likely or maximum entropy state of the system and that's what we are going to",
    "start": "4583300",
    "end": "4589449"
  },
  {
    "text": "find also something very similar I know it's not easy to digest but think about",
    "start": "4589449",
    "end": "4595000"
  },
  {
    "text": "it more questions yes",
    "start": "4595000",
    "end": "4601050"
  },
  {
    "text": "[Music]",
    "start": "4612200",
    "end": "4615319"
  },
  {
    "text": "if you are working on writing a reverse paper for information theory and this hopefully you'll see it sooner but but",
    "start": "4623130",
    "end": "4630699"
  },
  {
    "text": "essentially what I'm saying here is that these two numbers for the last hidden layer which is essentially the point",
    "start": "4630699",
    "end": "4636610"
  },
  {
    "text": "where you managed to put your last layer in this plan is telling you both the",
    "start": "4636610",
    "end": "4644199"
  },
  {
    "text": "generalization error which is how much information you have about Y and the number of examples that you use because",
    "start": "4644199",
    "end": "4649510"
  },
  {
    "text": "you are converged to essentially it's it's this maximum of the red curve that is shown so that's the argument so",
    "start": "4649510",
    "end": "4657070"
  },
  {
    "text": "because the the maximum of this red the compression is dominated is dominating the generation error I think I more or",
    "start": "4657070",
    "end": "4663909"
  },
  {
    "text": "less argue let and in the information about the level is done anything I'm sorry the compression is dominating the",
    "start": "4663909",
    "end": "4670330"
  },
  {
    "text": "number of examples the sample complexity and the information about the label is dominating the is bounding the",
    "start": "4670330",
    "end": "4675790"
  },
  {
    "text": "information the generation error so it's these two numbers that really give you the whole trade-off now the best rate of",
    "start": "4675790",
    "end": "4683170"
  },
  {
    "text": "this you can achieve is determined by the optimal by the information base on a curve which is essentially what is the",
    "start": "4683170",
    "end": "4689710"
  },
  {
    "text": "most compression that you can get is a certain amount of information but it it",
    "start": "4689710",
    "end": "4697119"
  },
  {
    "text": "stucks it at some point and this is the interesting time and where it ends so",
    "start": "4697119",
    "end": "4703210"
  },
  {
    "text": "when you get to this particular value you know that this is what you could do with this number for examples the",
    "start": "4703210",
    "end": "4713170"
  },
  {
    "text": "infinite number of replicas this is a convergence plan if so you converge to a point look at the network so if you look",
    "start": "4713170",
    "end": "4719469"
  },
  {
    "text": "at one of those networks I mean I don't know where for example those I showed you here so it's the last point this one",
    "start": "4719469",
    "end": "4727239"
  },
  {
    "text": "here or this one here get the two numbers here the number of",
    "start": "4727239",
    "end": "4733060"
  },
  {
    "text": "bits here and the number of bits here look at how much you compressed their presentation and this tells me what is",
    "start": "4733060",
    "end": "4739930"
  },
  {
    "text": "going to be generalization error and what was the number of examples that you had to use in order to get there yeah I",
    "start": "4739930",
    "end": "4748150"
  },
  {
    "text": "know I know it's not it's not formulated precise enough and and but this is the",
    "start": "4748150",
    "end": "4753940"
  },
  {
    "text": "intuition yeah I'm sorry I you get into the the",
    "start": "4753940",
    "end": "4759970"
  },
  {
    "text": "nasty question session sorry please ask me everything you want this yes what",
    "start": "4759970",
    "end": "4769570"
  },
  {
    "text": "happens if you go to I mean this was a",
    "start": "4769570",
    "end": "4775510"
  },
  {
    "text": "canonical example the time runs through this paper but but we train with tried",
    "start": "4775510",
    "end": "4780880"
  },
  {
    "text": "it with different number of lis so the real question is what if I'm right that essentially the number of layers is",
    "start": "4780880",
    "end": "4786580"
  },
  {
    "text": "improving exponentially the convergence of use infinite number of layers so this is obviously wrong first of all you pay",
    "start": "4786580",
    "end": "4793210"
  },
  {
    "text": "computationally heavily with this I mean there are many more promises and then and then in order for this to make sense I need I need enough space between the",
    "start": "4793210",
    "end": "4800680"
  },
  {
    "text": "layers that they take actually move and if you just use too many layers they're",
    "start": "4800680",
    "end": "4808090"
  },
  {
    "text": "going to be redundant I mean most of them about to have exactly the same please actually see it here I mean there are six layers but you actually see only",
    "start": "4808090",
    "end": "4813940"
  },
  {
    "text": "four or five and the sixth one is on top of the first one so adding more layers",
    "start": "4813940",
    "end": "4819970"
  },
  {
    "text": "is not going to change the picture dramatically the question is what does change the picture dramatically and this has to do with the with the face",
    "start": "4819970",
    "end": "4826690"
  },
  {
    "text": "structure of the problem so so if you notice in in my information a button a curve there are those blue lines which I",
    "start": "4826690",
    "end": "4834160"
  },
  {
    "text": "didn't mention those blue line has sub optimal solutions of the prone which means that there are different phases of",
    "start": "4834160",
    "end": "4840640"
  },
  {
    "text": "the topology of representation and this is actually what I'm aiming it so essentially you don't need more",
    "start": "4840640",
    "end": "4846040"
  },
  {
    "text": "layers than the number of different phases anything beyond this will be redundant I mean we do just another",
    "start": "4846040",
    "end": "4851680"
  },
  {
    "text": "layer which is I want to want to summation on the previous one how do you know they're not usually there are many",
    "start": "4851680",
    "end": "4857650"
  },
  {
    "text": "many sub optimal solutions unless you have symmetry if the problem is special",
    "start": "4857650",
    "end": "4863350"
  },
  {
    "text": "or symmetric or simple then that those those facial dishes those are those those this sub of the solutions really",
    "start": "4863350",
    "end": "4869829"
  },
  {
    "text": "collapse to very few and that that's when the simple actually works over so our problem is symmetric and I believe",
    "start": "4869829",
    "end": "4876549"
  },
  {
    "text": "that this is why you see such many pictures in general it's going to be a mess I mean just to give you some",
    "start": "4876549",
    "end": "4883359"
  },
  {
    "text": "understanding what's going on there in general those but this bifurcation diagram is getting more complicated and if if there is symmetry all those phase",
    "start": "4883359",
    "end": "4892510"
  },
  {
    "text": "transitions or splits of clusters happen then in the same places this is actually",
    "start": "4892510",
    "end": "4898689"
  },
  {
    "text": "the phase diagram of this particular problem but you see that it's enough to put a layer between two major",
    "start": "4898689",
    "end": "4904719"
  },
  {
    "text": "transitions here so this is some sort of connection between the the number of",
    "start": "4904719",
    "end": "4910359"
  },
  {
    "text": "layers is determined by the really intrinsic structure of the Joint",
    "start": "4910359",
    "end": "4915999"
  },
  {
    "text": "Distribution of X and Y which is determining the spaced additions just say longer I can show you much more about it but it's a very good question",
    "start": "4915999",
    "end": "4924569"
  },
  {
    "text": "okay so that's again a tricky question so first of all I argued that the mini-batches",
    "start": "4937619",
    "end": "4943029"
  },
  {
    "text": "enhance the performance significantly I mean so you see again in this in this in",
    "start": "4943029",
    "end": "4949899"
  },
  {
    "text": "this picture that when you increase the mini batch size you actually shift the",
    "start": "4949899",
    "end": "4955569"
  },
  {
    "text": "transition point because the noise is smaller so what what comes out of it is",
    "start": "4955569",
    "end": "4961269"
  },
  {
    "text": "another interesting consequence or you know prediction of the theory is that",
    "start": "4961269",
    "end": "4967769"
  },
  {
    "text": "the best mini batch size should be such that the covariance matrix of the gradients due to the mini batches is",
    "start": "4967769",
    "end": "4975010"
  },
  {
    "text": "aligned nicely with the Hessian matrix of the minimum because then you're",
    "start": "4975010",
    "end": "4980589"
  },
  {
    "text": "actually pushing it in there in the right directions correctly I mean though so these two ellipses thought ellipses",
    "start": "4980589",
    "end": "4987459"
  },
  {
    "text": "its ellipses in very high dimension where they should be aligned if they align then you are most efficient so",
    "start": "4987459",
    "end": "4993099"
  },
  {
    "text": "there is a prediction here now once you take full bitumen you completely ignore",
    "start": "4993099",
    "end": "4998589"
  },
  {
    "text": "the many base the noise becomes is there only from for numerical reasons for for discoloration",
    "start": "4998589",
    "end": "5004199"
  },
  {
    "text": "of time and indeed the whole thing is very very slow and if you see here I mean even for this process if it is a",
    "start": "5004199",
    "end": "5011010"
  },
  {
    "text": "full bridge you already get to thousands of iterations before you actually begin",
    "start": "5011010",
    "end": "5017580"
  },
  {
    "text": "to see the compression but eventually there is compression due to numerical noise it's just it's just not effective",
    "start": "5017580",
    "end": "5025260"
  },
  {
    "text": "so this is my prediction if you choose the first of all land with a lot of",
    "start": "5025260",
    "end": "5031320"
  },
  {
    "text": "variability of the background this is really important because this is what's going to give you a better",
    "start": "5031320",
    "end": "5037610"
  },
  {
    "text": "stochasticity in the between the mini-batches which is going to eventually learn",
    "start": "5037610",
    "end": "5042840"
  },
  {
    "text": "better but now measure this variability of the background actually did the covariance of the of the of the of the",
    "start": "5042840",
    "end": "5049440"
  },
  {
    "text": "mini-batches gradients and match it to the covariance of the pronin and science",
    "start": "5049440",
    "end": "5054929"
  },
  {
    "text": "in many cases we can actually do this this will enhance the learning that's my prediction this will be some sort of",
    "start": "5054929",
    "end": "5060840"
  },
  {
    "text": "matching between the noise and the channel in some sense of this kind of this process",
    "start": "5060840",
    "end": "5068270"
  },
  {
    "text": "[Applause]",
    "start": "5070280",
    "end": "5075670"
  },
  {
    "text": "you",
    "start": "5080610",
    "end": "5082670"
  }
]