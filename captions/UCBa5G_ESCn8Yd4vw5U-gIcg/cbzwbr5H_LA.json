[
  {
    "start": "0",
    "end": "5280"
  },
  {
    "text": "Good afternoon, CS109. How are you guys doing today? Whoo! Yay, fantastic. We have an exciting class.",
    "start": "5280",
    "end": "11370"
  },
  {
    "text": "We get to talk about some of\nthe probability behind ethics and machine learning. And boy, is this\nan important topic?",
    "start": "11370",
    "end": "18590"
  },
  {
    "text": "We've learned some pretty\ninteresting machine learning in CS109. And we are currently\nliving the experience",
    "start": "18590",
    "end": "26400"
  },
  {
    "text": "of watching the world\ncontend with smarter and smarter algorithms. And it brings up two\nimportant things.",
    "start": "26400",
    "end": "32478"
  },
  {
    "text": "I'm glad you guys machine\nlearning because maybe you'll do something really helpful\nfor the world with it. And also, it just feels like\nfor us to understand what's",
    "start": "32478",
    "end": "40980"
  },
  {
    "text": "going on in this\nworld, we should know the machine learning\nthat is impacting our society. And we should be\nable to see it, look",
    "start": "40980",
    "end": "47879"
  },
  {
    "text": "at it with a sophisticated\nlens, if possible. And that's what we're\ngoing to do today.",
    "start": "47880",
    "end": "53340"
  },
  {
    "text": "Oh, and this is such a cute\nthing to start out with. I've been thinking\nabout like, what is my own personal\nbasis of ethics?",
    "start": "53340",
    "end": "59988"
  },
  {
    "text": "And before we jump\ninto this lecture, I just want to tell you\nwhere I think I am right now.",
    "start": "59988",
    "end": "65040"
  },
  {
    "text": "Where does this idea of\nhuman goodness come from? It's a really interesting\nethical question before we jump into\nour cool lecture,",
    "start": "65040",
    "end": "71920"
  },
  {
    "text": "I want to tell you\nthat I'm gravitating towards this Mencius\nphilosophy of ethics. And the Mencius\nphilosophy of ethics",
    "start": "71920",
    "end": "78210"
  },
  {
    "text": "says that there is\ngoodness in all of us. There is a sprout of goodness. Goodness is not something\nthat is given to us.",
    "start": "78210",
    "end": "83467"
  },
  {
    "text": "It's not something that\ncomes later in life. It is in our hearts\nfrom the beginning. And some people\nfeed those sprouts.",
    "start": "83467",
    "end": "89439"
  },
  {
    "text": "So he uses this agricultural\nmetaphor of goodness is a sprout within us. And if you water that sprout,\nit will grow and germinate",
    "start": "89440",
    "end": "95890"
  },
  {
    "text": "into the fully benevolent\nperson that we are all capable of being. And he often argues\nthat this point,",
    "start": "95890",
    "end": "103030"
  },
  {
    "text": "by using the allegory of the\nbaby and the well, if any of us",
    "start": "103030",
    "end": "108040"
  },
  {
    "text": "saw a baby walking\ntowards a well-- if we thought that baby was\ngoing to fall into the well,",
    "start": "108040",
    "end": "113860"
  },
  {
    "text": "you would have this not\nreflexive conscious but very",
    "start": "113860",
    "end": "120490"
  },
  {
    "text": "predictable reaction\nto go help that baby. And in fact, all of us can\njust feel uncomfortable",
    "start": "120490",
    "end": "125680"
  },
  {
    "text": "imagining a baby\nfalling into a well. And he says, this is\nhis example of saying, there is something\nhardwired about humans",
    "start": "125680",
    "end": "132190"
  },
  {
    "text": "that has some goodness. There is that sprout in us. Some people don't exhibit a\nmature version of that sprout,",
    "start": "132190",
    "end": "138370"
  },
  {
    "text": "but it's everywhere. And I just love this because\nyou guys know me by now. I'm an optimistic person.",
    "start": "138370",
    "end": "144730"
  },
  {
    "text": "I do see this\ngoodness in everybody. So I do like this\nethical construct. But my own personal\nethical construct",
    "start": "144730",
    "end": "150823"
  },
  {
    "text": "is a little bit less\nimportant than what we're going to talk about today. The framing for\ntoday is very simple.",
    "start": "150823",
    "end": "157480"
  },
  {
    "text": "AI is impacting our\nlives all over the place. It lives in our smartphones. There's security\ncameras, social media,",
    "start": "157480",
    "end": "164270"
  },
  {
    "text": "all these different\nfacets of our life we're starting to see algorithms\nplay a role in human existence.",
    "start": "164270",
    "end": "169600"
  },
  {
    "text": "And therefore, we\nmust think about AI and how it could be fair.",
    "start": "169600",
    "end": "175629"
  },
  {
    "text": "There's a few ways of\nthinking about this. One is with great power\ncomes great responsibility. Another way of\nthinking about this,",
    "start": "175630",
    "end": "181930"
  },
  {
    "text": "there's a whole field\nand set of careers in people who are thoughtful\nabout what fairness means in this new phase.",
    "start": "181930",
    "end": "188810"
  },
  {
    "text": "So I said at the\nbeginning of class, no matter what\nyour interests are, there is something you can\ncombine with probability.",
    "start": "188810",
    "end": "194950"
  },
  {
    "text": "If you're into dance, you\ncan do dance and probability. And certainly, if\nyou're into philosophy",
    "start": "194950",
    "end": "200110"
  },
  {
    "text": "there's a lot of\nroom for careers in philosophy and probability.",
    "start": "200110",
    "end": "205330"
  },
  {
    "text": "Now, we do live in a time\nwith real work to be done. And sometimes the affordances\nof machine learning",
    "start": "205330",
    "end": "211450"
  },
  {
    "text": "can be helpful. We can all imagine\nbetter health care. I think of my daughter who has\ncochlear implants and like,",
    "start": "211450",
    "end": "217270"
  },
  {
    "text": "the AI in those\ncochlear implants will make her life better. We can make smart grids to\nbe more intelligent about how",
    "start": "217270",
    "end": "224200"
  },
  {
    "text": "we can balance electricity in\nthis coming electrical age. And maybe we can increase access\nto high-quality education.",
    "start": "224200",
    "end": "231310"
  },
  {
    "text": "There's all this\nreal work that could be done with machine learning. But also, it is worth\ntalking about some",
    "start": "231310",
    "end": "238240"
  },
  {
    "text": "of the examples of\nshortcomings, where we can look at deployments\nof machine learning thing. That's not good-- and some\nversions that show biases.",
    "start": "238240",
    "end": "249250"
  },
  {
    "text": "Sometimes computer science\ncan hit beyond biases. And in fact, we'll talk\nabout this example later,",
    "start": "249250",
    "end": "258121"
  },
  {
    "text": "or there's just case studies\nwe can point to where like, there could have\nbeen a better way. And we're going to\nsee if we can come up with a nice construct for that.",
    "start": "258121",
    "end": "264680"
  },
  {
    "text": "Can I give you guys\nlearning goals? A couple of things. Let's imagine\nwhere would we like to be when we walk\nout of today's class?",
    "start": "264680",
    "end": "271289"
  },
  {
    "text": "I would like you to\nunderstand the limits of this idea of fairness\nthrough unawareness. That's something I would\nlike everybody to know.",
    "start": "271290",
    "end": "278300"
  },
  {
    "text": "I'd like you guys to know these\ntwo ways of measuring fairness. And there's some way\nof relaxed calibration.",
    "start": "278300",
    "end": "285090"
  },
  {
    "text": "I want all of you\nguys to know that. In fact, those are\nthings that-- this is something you will\nneed for your problem set.",
    "start": "285090",
    "end": "290180"
  },
  {
    "text": "Now, this isn't something\nthat I'll test you on. But I also want you\nguys to walk out",
    "start": "290180",
    "end": "295250"
  },
  {
    "text": "with some sense of\ntechniques and principles to mitigate fairness issues. So that's our goal for today.",
    "start": "295250",
    "end": "301110"
  },
  {
    "text": "Does that sound good? Let's do it. There is this other\nmedical, which",
    "start": "301110",
    "end": "307100"
  },
  {
    "text": "is like, how to become\na responsible scientist and not show up in the\nnews, like this researcher?",
    "start": "307100",
    "end": "313100"
  },
  {
    "text": " A couple of concepts\nthat are going to come up in today's class.",
    "start": "313100",
    "end": "319400"
  },
  {
    "text": "What is a protected demographic? What's the difference\nbetween distributive harm versus quality-of-service harm?",
    "start": "319400",
    "end": "325220"
  },
  {
    "text": "And then what is fairness? What's the philosophy\nof procedural versus distributive\nfairness and then",
    "start": "325220",
    "end": "330620"
  },
  {
    "text": "these different\ndefinitions of fairness? So you're going to see a\nwhole bunch of very technical interesting things that come\nfrom the world of philosophy",
    "start": "330620",
    "end": "336535"
  },
  {
    "text": "that apply to the world\nof computer science. And then I did want to\nput up this warning. This is my own take.",
    "start": "336535",
    "end": "342110"
  },
  {
    "text": "By the way, this is a lecture\nmade by Katie Creel and myself.",
    "start": "342110",
    "end": "347539"
  },
  {
    "text": "A lot of the research\ndone on fairness comes from the US, especially\nfairness in machine learning.",
    "start": "347540",
    "end": "353780"
  },
  {
    "text": "And I do think of that\nas a funny meta bias. We'll talk about fairness,\nbut it'll be mostly centered",
    "start": "353780",
    "end": "359509"
  },
  {
    "text": "within our own community. And you can imagine,\nevery community has axes of discrimination\nthat are worth talking about.",
    "start": "359510",
    "end": "365670"
  },
  {
    "text": "So even though our examples\ndo come from the US-- and I would like to highlight\nthat the framing of what you will learn does go\nbeyond the sorts of axes",
    "start": "365670",
    "end": "374360"
  },
  {
    "text": "that you will see in the US. A little bit of\nreview, shall we?",
    "start": "374360",
    "end": "380272"
  },
  {
    "text": "Machine learning, we've been\nlearning about it for a while. And particularly, we've been\ntalking about black box models,",
    "start": "380272",
    "end": "385670"
  },
  {
    "text": "where you have a model which\nwe can take in some inputs and make a prediction,\nand that model",
    "start": "385670",
    "end": "391310"
  },
  {
    "text": "is governed by parameters. We call it black box model\nbecause we imagine using it like a black box, where you\njust put your inputs in,",
    "start": "391310",
    "end": "397430"
  },
  {
    "text": "and outputs come out. And particularly, we have been\ntalking about classification algorithms, where your inputs\ncould be some set of features",
    "start": "397430",
    "end": "405110"
  },
  {
    "text": "for an individual,\nand the output could be some discrete\nprediction, like a 0 or a 1",
    "start": "405110",
    "end": "410270"
  },
  {
    "text": "for a class label. We've talked about\ntwo major algorithms.",
    "start": "410270",
    "end": "416009"
  },
  {
    "text": "We learned about Naive Bayes. And then we learned about\nlogistic regression. I do want to tell you something\nthat we didn't go too deep into",
    "start": "416010",
    "end": "423270"
  },
  {
    "text": "but is very, very interesting. If you took logistic\nregression, some people",
    "start": "423270",
    "end": "428840"
  },
  {
    "text": "notice that if the input\nto the sigmoid is positive,",
    "start": "428840",
    "end": "433910"
  },
  {
    "text": "it predicts a 1. And if the input to the sigmoid\nis a negative, it predicts a 0. And so you can think\nabout this set of points.",
    "start": "433910",
    "end": "443419"
  },
  {
    "text": "What are all the inputs where\nthe input to the sigmoid is a 0? So it's perfectly between\nthe positive and negative.",
    "start": "443420",
    "end": "450690"
  },
  {
    "text": "And for those of you guys\nwho have seen this before, you might recognize that\nthis is the description",
    "start": "450690",
    "end": "455900"
  },
  {
    "text": "of a straight line. So what this is telling us\nis that logistic regression--",
    "start": "455900",
    "end": "462200"
  },
  {
    "text": "imagine you only had two\nfeatures because then I can visualize it. You only had features x1 and x2. And based on that, you're either\ngoing to predict a 1 or a 0.",
    "start": "462200",
    "end": "470330"
  },
  {
    "text": "The idea is that logistic\nregression is only able to draw a straight line between\nthe 0's and 1's.",
    "start": "470330",
    "end": "477509"
  },
  {
    "text": "And if it can find a\nnice, straight line, it can do a perfect\njob on this task. But if your points aren't\nlinearly separable,",
    "start": "477510",
    "end": "483870"
  },
  {
    "text": "it can't do a perfect job. Does that make sense? Very cool way of\nunderstanding the limitation",
    "start": "483870",
    "end": "489590"
  },
  {
    "text": "of logistic regression. It does pretty well, but we\ncan think about the cases where it will break.",
    "start": "489590",
    "end": "495003"
  },
  {
    "text": "Of course, if you have\nmore than two features, it's really hard\nto think about this because you end up having\na straight line in higher",
    "start": "495003",
    "end": "500670"
  },
  {
    "text": "dimensional space. But it still has this\nstraight line property.",
    "start": "500670",
    "end": "506660"
  },
  {
    "text": "Interestingly, turns\nout Naive Bayes is also creating a linear\nseparator and that there's",
    "start": "506660",
    "end": "512809"
  },
  {
    "text": "no interaction between the\ndifferent features that are allowed in terms of\nmaking its prediction.",
    "start": "512809",
    "end": "519599"
  },
  {
    "text": "So both logistic\nregression and Naive Bayes have this similar limitation.",
    "start": "519600",
    "end": "525710"
  },
  {
    "text": "And then in last\nWednesday's class, we took it to the next level.",
    "start": "525710",
    "end": "531360"
  },
  {
    "text": "We're like, well, what\nif we want to have more powerful predictors? And we did. We said, hey,\nlogistic regression,",
    "start": "531360",
    "end": "537460"
  },
  {
    "text": "you look like a cartoon\nmodel of a neuron. What if we just stacked\nlogistic regressions on top of logistic regressions?",
    "start": "537460",
    "end": "542622"
  },
  {
    "text": "Would that be crazy? And at first, the\nworld thought, yes. But then the world\ngot powerful computers and said, not crazy,\ngame-changing.",
    "start": "542622",
    "end": "548710"
  },
  {
    "text": "And that is the backbone\nof all modern AI-- a bunch of logistic\nregressions put together called neural networks. It turns out these\nneural networks",
    "start": "548710",
    "end": "555370"
  },
  {
    "text": "can make predictions that\nare much more sophisticated than just straight lines.",
    "start": "555370",
    "end": "561459"
  },
  {
    "text": "We saw them in class. We talked about how they\nmake their predictions. We even talked about how\nthey use chain rule in order",
    "start": "561460",
    "end": "569560"
  },
  {
    "text": "to gain intelligence. So you can use maximum\nlikelihood estimation, not on just one\nlogistic regression,",
    "start": "569560",
    "end": "575290"
  },
  {
    "text": "but on a whole neural network\nof many logistic regressions. What a time to be alive. And this was an\ninteresting thing",
    "start": "575290",
    "end": "582523"
  },
  {
    "text": "that we pointed out in\nlast Wednesday's class. Not only can they learn\nsomething more complicated than a straight\nline classifier--",
    "start": "582523",
    "end": "590020"
  },
  {
    "text": "when they train neural\nnetworks on things like recognizing faces,\nthe craziest thing happens, where the first layers\nstart to become edge detectors.",
    "start": "590020",
    "end": "598418"
  },
  {
    "text": "The middle layers start\nto become part detectors, and final layers start\nto look like ghost faces. And the crazy thing\nabout that is,",
    "start": "598418",
    "end": "604870"
  },
  {
    "text": "even though these were just\ngetting their weights from MLE,",
    "start": "604870",
    "end": "610440"
  },
  {
    "text": "applied with chain\nrule, they learned something that is so similar\nto how our human brains work.",
    "start": "610440",
    "end": "616402"
  },
  {
    "text": "If you looked at\nour human brains, the V1 cortex is\nan edge detector. And if you looked at what\nparticular neurons are looking",
    "start": "616403",
    "end": "622680"
  },
  {
    "text": "for, it looks a lot like that. Later in our brain, you\nfind part detectors. And if you looked at\nwhat inputs would most",
    "start": "622680",
    "end": "628470"
  },
  {
    "text": "stimulate different neurons\nlater on in your V2 cortex, you would see stuff like that.",
    "start": "628470",
    "end": "633600"
  },
  {
    "text": "And later on again, you\nget these ghost faces. Insane. Something crazy is going on.",
    "start": "633600",
    "end": "640900"
  },
  {
    "text": "But it turns out these neural\nnetworks, while amazing, are not panaceas. And not every example\nof neural networks",
    "start": "640900",
    "end": "647639"
  },
  {
    "text": "applied to human problems\nhas been successful. We're going to talk about places\nwhere this stuff is broken.",
    "start": "647640",
    "end": "655440"
  },
  {
    "text": "And in order to do\nthat, it is worth noting that not every broken\nneural network is equally bad.",
    "start": "655440",
    "end": "662070"
  },
  {
    "text": "And so let's bring in our\nfirst idea from philosophy. Thank you. There are different\nways of saying, well,",
    "start": "662070",
    "end": "668340"
  },
  {
    "text": "that model was really janky. Here's one. We think it is bad if\na neural network that",
    "start": "668340",
    "end": "675220"
  },
  {
    "text": "does something wrong introduces\na quality-of-service harm. And a quality-of-service\nharm is one where we say,",
    "start": "675220",
    "end": "682840"
  },
  {
    "text": "this neural network works\nbetter from certain demographics than others. So for example, maybe\nyou have generative art",
    "start": "682840",
    "end": "689530"
  },
  {
    "text": "or facial recognition, and\nit doesn't work for everybody equally. That's considered to be bad\nand harmful, but not nearly as",
    "start": "689530",
    "end": "697860"
  },
  {
    "text": "harmful as if you had machine\nlearning that does something called a distributive harm.",
    "start": "697860",
    "end": "703079"
  },
  {
    "text": "We think that there's particular\nopportunities, resources, or information that\nare really essential.",
    "start": "703080",
    "end": "709470"
  },
  {
    "text": "Like for example, getting\na job, or getting a loan, or getting into a school. These are really\nimportant decisions.",
    "start": "709470",
    "end": "715720"
  },
  {
    "text": "So if neural networks\nstart to make mistakes, over here we think of\nthat as even more harmful. And then finally, just to\ncap off this conversation,",
    "start": "715720",
    "end": "723630"
  },
  {
    "text": "there are existential harms. I'm not sure when we're\nready to talk about this. But do we need to start\nthinking about when",
    "start": "723630",
    "end": "729330"
  },
  {
    "text": "AI becomes intelligent? I'm not sure I feel like I\nwould just be speculating as much as anyone else. But we do think\nof degrees of harm",
    "start": "729330",
    "end": "736320"
  },
  {
    "text": "all the way from\nquality-of-service harm to existential harm. Let me give you two case\nstudies that give us",
    "start": "736320",
    "end": "742920"
  },
  {
    "text": "an idea of AI producing\nquality-of-service harms and distributive harms.",
    "start": "742920",
    "end": "749100"
  },
  {
    "text": "You guys ready? So my first case study\nfor you is a case study",
    "start": "749100",
    "end": "754340"
  },
  {
    "text": "in quality-of-service harm. And it comes down\nto these cameras that were actually released.",
    "start": "754340",
    "end": "760340"
  },
  {
    "text": "And we're trying\nto do autofocus. But the autofocus did not\nwork if you were not white,",
    "start": "760340",
    "end": "767750"
  },
  {
    "text": "and particularly would\nmake some awful decisions for different demographics.",
    "start": "767750",
    "end": "773900"
  },
  {
    "text": "How did this happen? We consider this to be a\nquality-of-service harm. One way of telling the story\nis to go a little bit back",
    "start": "773900",
    "end": "781090"
  },
  {
    "text": "into the world of\nunderstanding neural networks. So I told you this\nstory on Wednesday of a neural network that was\ntrained to look at images",
    "start": "781090",
    "end": "788530"
  },
  {
    "text": "and tell you what's\nin those images. The neural network,\nwe got really good",
    "start": "788530",
    "end": "793720"
  },
  {
    "text": "at predicting whether\nor not an image was one of 22,000 categories.",
    "start": "793720",
    "end": "803010"
  },
  {
    "text": "But can I show you some places\nwhere that neural network makes mistakes? And these are a little bit more\nneutral than what we just saw.",
    "start": "803010",
    "end": "810240"
  },
  {
    "text": "These really, really\nsmart neural networks that can perform almost\nas well as humans",
    "start": "810240",
    "end": "815399"
  },
  {
    "text": "make some pretty\nhilarious mistakes. So for example, it's pretty\nsure this is a sea lion.",
    "start": "815400",
    "end": "822050"
  },
  {
    "text": "It's pretty sure that\nthis is a manhole cover. What could possibly lead to this\nvery intelligent neural network",
    "start": "822050",
    "end": "829050"
  },
  {
    "text": "making those sorts of mistakes? ",
    "start": "829050",
    "end": "834800"
  },
  {
    "text": "The problems with\nthe training data. Oh, yeah, totally. But can you tell me\na little bit more than just problems\nbecause you were right?",
    "start": "834800",
    "end": "840600"
  },
  {
    "text": "But it is worth getting a little\nbit more deeper into that. What do you think?",
    "start": "840600",
    "end": "845900"
  },
  {
    "text": "What sort of problems? Why the manhole cover for\nthat particular image? Lack of coverage for\nthat specific feature.",
    "start": "845900",
    "end": "853040"
  },
  {
    "text": "And because it is so few\ndata points to choose from, it can pick and\nchoose and get noise.",
    "start": "853040",
    "end": "858920"
  },
  {
    "text": "Yeah. So it's probably not\nseen enough dragonflies. That's certainly the case. And there's something\nspecific about this picture.",
    "start": "858920",
    "end": "866480"
  },
  {
    "text": "Yes. Maybe it's just the\nangle that it's taken at. And even for the sea lion\nphoto, most sea lions",
    "start": "866480",
    "end": "873470"
  },
  {
    "text": "are seen doing that thing. Yes, yes, yes. So angle is definitely\nis part of it.",
    "start": "873470",
    "end": "879500"
  },
  {
    "text": "Good idea. Not to just read\noff the slide, but I mean, it seems like\ntexture comes into it. The sea lion has\na smooth surface,",
    "start": "879500",
    "end": "887060"
  },
  {
    "text": "and the manhole cover\nis like graded as well. Yeah. It turns out they get\nreally obsessed over things",
    "start": "887060",
    "end": "893030"
  },
  {
    "text": "that humans can see past. For example, they're\nreally into textures. So this just happened\nto have a texture that",
    "start": "893030",
    "end": "899220"
  },
  {
    "text": "looks like a manhole\ncover, and this happened to have a texture from\nthe water over here that looked like a sea lion. Now, there is a good argument\nthat it's just a data problem.",
    "start": "899220",
    "end": "907840"
  },
  {
    "text": "So training data can really\nbe a source of biases within neural networks. So what you train your neural\nnetwork on really does matter.",
    "start": "907840",
    "end": "915959"
  },
  {
    "text": "There's lots of ways\nof telling the story. One story is what we just saw. It got confused because it saw\ntoo many textures associated",
    "start": "915960",
    "end": "926040"
  },
  {
    "text": "with manholes. And so when it saw a particular\ntexture, always associated with manholes. Here's another way, though,\nof getting a little bit deeper",
    "start": "926040",
    "end": "933780"
  },
  {
    "text": "into this data problem. Imagine you have a classifier,\nand it could be linear,",
    "start": "933780",
    "end": "939209"
  },
  {
    "text": "it could be more complicated. But let's imagine\nlinear for now. Imagine you now have a\npopulation that has a majority",
    "start": "939210",
    "end": "946290"
  },
  {
    "text": "group and a minority group. And the majority group,\nhere is how you would classify class 0 from class 1.",
    "start": "946290",
    "end": "953399"
  },
  {
    "text": "But for the minority\ngroup, let's say it works pretty differently. When you put all this data\ntogether into your data set,",
    "start": "953400",
    "end": "961620"
  },
  {
    "text": "imagine you're learning\na linear classifier. Is it going to be\npaying attention",
    "start": "961620",
    "end": "966690"
  },
  {
    "text": "to the signal that\ncomes from the majority group or the minority group\nwhen it's trying to figure out",
    "start": "966690",
    "end": "972150"
  },
  {
    "text": "some classifier to best split\nthe circles from the pluses? Talk about with the person\nnext to you for a minute",
    "start": "972150",
    "end": "977820"
  },
  {
    "text": "and then we will talk about what\nwill actually happen together. Go for it. I'll take a time.",
    "start": "977820",
    "end": "983070"
  },
  {
    "text": "Chance to draw over here. [SIDE CONVERSATIONS]",
    "start": "983070",
    "end": "989430"
  },
  {
    "start": "989430",
    "end": "1069170"
  },
  {
    "text": "So I'm sure a lot of you guys\ncame to something like this. It's using maximum likelihood\nestimation to train.",
    "start": "1069170",
    "end": "1076720"
  },
  {
    "text": "And so the likelihood is going\nto treat each point equally. Every point will have\nexactly one vote.",
    "start": "1076720",
    "end": "1082539"
  },
  {
    "text": "And so because of\nthat, because there's so many more points voting\nfor algorithm that separates",
    "start": "1082540",
    "end": "1088750"
  },
  {
    "text": "like this, then there are coming\nfrom the minority demographic. MLE really wants to just\nignore that minority",
    "start": "1088750",
    "end": "1094690"
  },
  {
    "text": "demographic and just focus\non what is the overall trend. And that really\ncomes from the fact that MLE, whether or not\nyou're in the log version,",
    "start": "1094690",
    "end": "1102130"
  },
  {
    "text": "it has that sum\nover data points. Or if you're in the\nnon-log version, you have the product\nover data points,",
    "start": "1102130",
    "end": "1107830"
  },
  {
    "text": "where it treats every\nsingle data point the same.  So at this point, lack of\ndata can be a huge problem.",
    "start": "1107830",
    "end": "1120410"
  },
  {
    "text": "And in fact, going\nback to that camera, we really do think that is\none of the biggest problems, was just one group look like\na minority to the algorithm.",
    "start": "1120410",
    "end": "1129693"
  },
  {
    "text": "And then the\nalgorithm just hadn't been trained on enough data\nand was making bad mistakes.",
    "start": "1129693",
    "end": "1134890"
  },
  {
    "text": "It was trained on a\nFaces In The Wild data set, which was\nmaybe unsurprisingly 84% White and 78% male.",
    "start": "1134890",
    "end": "1142720"
  },
  {
    "text": "And because of this,\nit did very well poorly when it was out of distribution. And actually, this is\nsomething that we've",
    "start": "1142720",
    "end": "1149290"
  },
  {
    "text": "gotten much better at. They started this process of\npointing out those mistakes.",
    "start": "1149290",
    "end": "1154299"
  },
  {
    "text": "They took a whole\nbunch of models that were in the wild\nthat had been published and were actually\nbeing used and showed",
    "start": "1154300",
    "end": "1160210"
  },
  {
    "text": "how they were making\nbad decisions based on demographics. And then they produced\ndata sets that you",
    "start": "1160210",
    "end": "1166510"
  },
  {
    "text": "can use that are\nmuch more balanced across different contexts that\npeople might be coming from. And once people started\ntraining on these data sets,",
    "start": "1166510",
    "end": "1173380"
  },
  {
    "text": "those particular examples that\nwe saw became less prevalent. So anyways, a cool,\nlittle story that took",
    "start": "1173380",
    "end": "1180049"
  },
  {
    "text": "place here at Stanford. Now, there are other\nlevels of harm.",
    "start": "1180050",
    "end": "1185390"
  },
  {
    "text": "So that was an example of\nquality-of-service harm, but things get a\nlittle bit more serious",
    "start": "1185390",
    "end": "1190630"
  },
  {
    "text": "when they affect people's\nlives, and that makes sense. If you're making a decision\nabout whether or not somebody goes to\nschool or prison,",
    "start": "1190630",
    "end": "1196929"
  },
  {
    "text": "that's a different level of\nharm, if you get it wrong. And here's a case study of\na distributive harm in AI.",
    "start": "1196930",
    "end": "1204679"
  },
  {
    "text": "So there is this case of\nSt. George's Hospital. It's a medical\nschool, and they had a whole bunch of applicants.",
    "start": "1204680",
    "end": "1210549"
  },
  {
    "text": "They have 2,500 applicants. They would interview\napproximately 625.",
    "start": "1210550",
    "end": "1216670"
  },
  {
    "text": "So 3/4 of people get\nrejected between this stage and this stage. And then eventually,\nthey offer spots to 425.",
    "start": "1216670",
    "end": "1222670"
  },
  {
    "text": "So 70% of interviewees\nare accepted. In 1979, one of the deans\nhad this great idea.",
    "start": "1222670",
    "end": "1231400"
  },
  {
    "text": "They said, hey, we've got a lot\nof historical data of people who have applied and\ngotten past this stage.",
    "start": "1231400",
    "end": "1237909"
  },
  {
    "text": "I could train a\nclassification algorithm to decide who is going\nto get to the stage,",
    "start": "1237910",
    "end": "1244820"
  },
  {
    "text": "and we can just have a computer\ndo this whole first pass. So here is the timeline\nof what happened.",
    "start": "1244820",
    "end": "1252590"
  },
  {
    "text": "1982, he argues that he's\nbuilt this classifier, and then he argues he's getting\n90% to 95% classification",
    "start": "1252590",
    "end": "1261890"
  },
  {
    "text": "accuracy. So when he makes a\nprediction, 95% of the time, it agrees with what a human\nwould have said on the panel.",
    "start": "1261890",
    "end": "1268790"
  },
  {
    "text": "That sounded like\nhigh enough accuracy. So in 1982, they\nstarted using it.",
    "start": "1268790",
    "end": "1276940"
  },
  {
    "text": "Then later on, there\nis an internal review which asks why are applicants\nbeing weighted by factors,",
    "start": "1276940",
    "end": "1283450"
  },
  {
    "text": "like name and place of birth. When they actually looked\ninto the algorithm, it would be putting\nweights on things",
    "start": "1283450",
    "end": "1288910"
  },
  {
    "text": "that people thought\nweren't that important, and it led to a review\nof this algorithm. And then eventually,\nthis went to ethics",
    "start": "1288910",
    "end": "1298150"
  },
  {
    "text": "hearing because it\nfound out that the name and place of birth were\nbeing used to dock points",
    "start": "1298150",
    "end": "1304000"
  },
  {
    "text": "from female and\nnon-Caucasian applicants. So I guess this story tells us\nof hey, when you make mistakes,",
    "start": "1304000",
    "end": "1313490"
  },
  {
    "text": "it is much worse when\nit's affecting something like-- well, people\nend up in school.",
    "start": "1313490",
    "end": "1318799"
  },
  {
    "text": "Now, that was just to give\nyou a case study of these two different harms. But along the way, we have\npicked up some insights.",
    "start": "1318800",
    "end": "1327170"
  },
  {
    "text": "We have this term in AI-- garbage in, garbage\nout-- which means, if you have bad data coming in,\nyou will make bad predictions.",
    "start": "1327170",
    "end": "1334970"
  },
  {
    "text": "If we go back to the\nSt. George's one, one of the reasons that\nthe algorithm was making biased predictions is\nbecause before the 1980s,",
    "start": "1334970",
    "end": "1342560"
  },
  {
    "text": "the humans were also\nmaking bias predictions, and the algorithm learned to\nrecreate the garbage that it",
    "start": "1342560",
    "end": "1347600"
  },
  {
    "text": "was trained upon. Similarly, we can say, improper\nuse of sensitive features.",
    "start": "1347600",
    "end": "1353539"
  },
  {
    "text": "That algorithm really\nshouldn't have been looking at things like people's names. That's a red flag\nthat says, that",
    "start": "1353540",
    "end": "1359780"
  },
  {
    "text": "should not matter\nfor whether or not you're ready for medical school. And then there's this idea\nthat it's very believable",
    "start": "1359780",
    "end": "1366350"
  },
  {
    "text": "that somebody who\ntrained this algorithm didn't intend to be evil. They maybe thought\nthey were doing",
    "start": "1366350",
    "end": "1372140"
  },
  {
    "text": "something pretty reasonable. But just having not\nthought it through, they ended up creating\nquite a lot of harm.",
    "start": "1372140",
    "end": "1379040"
  },
  {
    "text": "So at this point,\nwe've just pointed out a couple of times harm. Can we get a little bit\nmore formal about fairness?",
    "start": "1379040",
    "end": "1385930"
  },
  {
    "text": "And the answer is, philosophers\nare so excited to tell us yes. Yes, we can. We've been thinking about\nfairness for a long time.",
    "start": "1385930",
    "end": "1392495"
  },
  {
    "text": "We can actually get into more\nspecifics about how we can call an algorithm fair or not fair.",
    "start": "1392495",
    "end": "1399400"
  },
  {
    "text": "To start this\nconversation, it's worth pointing out that there is\ntwo big fields of thought for what we call fairness.",
    "start": "1399400",
    "end": "1405820"
  },
  {
    "text": "One field of thought\nsays, fairness should come from the\nprocedure you use. They call this\nprocedural fairness.",
    "start": "1405820",
    "end": "1411430"
  },
  {
    "text": "So what is the process\nof your classification? And what do you do to ensure\nthat your algorithm doesn't",
    "start": "1411430",
    "end": "1418240"
  },
  {
    "text": "rely on unfair features? That's procedural fairness. And there is another\nschool of thought",
    "start": "1418240",
    "end": "1423880"
  },
  {
    "text": "that says, well,\nwhat really matters is the outcome of the\ndecisions you make.",
    "start": "1423880",
    "end": "1429380"
  },
  {
    "text": "And so there is these two\nbig philosophical camps. Let's be fair in our\nprocedure, or let's be fair in our outcome.",
    "start": "1429380",
    "end": "1436340"
  },
  {
    "text": "Let's jump into a few different\ndefinitions of fairness that speak to these\ndifferent philosophies.",
    "start": "1436340",
    "end": "1441530"
  },
  {
    "text": "The first one speaks\nto procedural fairness. And a lot of people\nhistorically subscribe",
    "start": "1441530",
    "end": "1447470"
  },
  {
    "text": "to this being a good idea. The motivating idea for this\nfirst definition of fairness--",
    "start": "1447470",
    "end": "1452960"
  },
  {
    "text": "fairness through\nunawareness is-- the best way to\nstop discrimination is to stop discriminating\nbased on race.",
    "start": "1452960",
    "end": "1460640"
  },
  {
    "text": "That's a Chief Justice Roberts. That's a little bit\nobscure, so let's unpack it. It's saying that if you're\nmaking a decision, if you know",
    "start": "1460640",
    "end": "1468559"
  },
  {
    "text": "nothing about somebody's\nsensitive features, then you'll make\na good decision. That's the idea of unawareness.",
    "start": "1468560",
    "end": "1474470"
  },
  {
    "text": "Be unaware of the demographics,\nand you'll be fair with respect to demographics.",
    "start": "1474470",
    "end": "1479670"
  },
  {
    "text": "How to do it gets a little\nbit more interesting. The first idea is to\nexclude sensitive features.",
    "start": "1479670",
    "end": "1486010"
  },
  {
    "text": "So if you're making\na prediction, it is procedurally\nunfair if you're looking at any of these\nprotected demographics.",
    "start": "1486010",
    "end": "1491940"
  },
  {
    "text": "And the second idea\nis, we also have to be thoughtful about proxies. Let's jump into\nthat a little bit.",
    "start": "1491940",
    "end": "1498090"
  },
  {
    "text": "It is worth also\nnoting, what do I mean when I say protected groups? In the US, there are\nlegally protected groups",
    "start": "1498090",
    "end": "1506940"
  },
  {
    "text": "in a lot of different\nimportant cases. The idea of a\nprotected group often",
    "start": "1506940",
    "end": "1512200"
  },
  {
    "text": "means that regardless\nof people's race, color, national origin, religion, age\nor sex, sexual orientation,",
    "start": "1512200",
    "end": "1519980"
  },
  {
    "text": "physical or mental\ndisability or reprisal, people should be getting\nfair decisions made for them.",
    "start": "1519980",
    "end": "1526870"
  },
  {
    "text": "And reprisal, I think, is\nlike if somehow somebody had been harmed\nin the past, they shouldn't be\ndiscriminated in making",
    "start": "1526870",
    "end": "1533770"
  },
  {
    "text": "in the decisions that are made\nagainst them in the future. This is the definition that\ncomes from the Equal Employment",
    "start": "1533770",
    "end": "1538990"
  },
  {
    "text": "Opportunity act, but\nthis same definition lives in a lot of places\nin legal code for the US,",
    "start": "1538990",
    "end": "1544870"
  },
  {
    "text": "like for example,\nhousing and loans. So this seems pretty reasonable. But can we jump into how\nfairness through unawareness",
    "start": "1544870",
    "end": "1552310"
  },
  {
    "text": "can be surprisingly difficult? Let's take some people who\nlive across the street.",
    "start": "1552310",
    "end": "1557950"
  },
  {
    "text": "Facebook. They do ads. And at some point,\nthey were making-- people would be publishing\nads on Facebook for housing.",
    "start": "1557950",
    "end": "1565840"
  },
  {
    "text": "Housing is important\nbecause it's legally protected with respect to\nthose protected demographics.",
    "start": "1565840",
    "end": "1571640"
  },
  {
    "text": "Now, I've never made\nan ad in Facebook, but here's how I\nunderstand it works. When you go to make\nan ad, at some point,",
    "start": "1571640",
    "end": "1579050"
  },
  {
    "text": "you had an option of\nsaying, give me lookalikes. And what lookalikes\nmeans would be like,",
    "start": "1579050",
    "end": "1584809"
  },
  {
    "text": "you would create your\nad, and you'd be like, here's five people. I want my ad to go to people\nwho look like these five people.",
    "start": "1584810",
    "end": "1591260"
  },
  {
    "text": "And they did this\nfor all their ads. And at some point in\n2018, some people said,",
    "start": "1591260",
    "end": "1597740"
  },
  {
    "text": "that is not a good way to do\nadvertisements for housing because advertisements\nfor housing are not supposed to be biased\nbased off of these protected",
    "start": "1597740",
    "end": "1604730"
  },
  {
    "text": "demographics. And Facebook had to\nchange their algorithm. And so Facebook\nmade this change.",
    "start": "1604730",
    "end": "1610970"
  },
  {
    "text": "They said, we're going to do\nfairness through unawareness. We can do lookalikes,\nbut our algorithms are not going to look at any\nof the protected demographics.",
    "start": "1610970",
    "end": "1618380"
  },
  {
    "text": "We're not going to look at\nthe age of the lookalike. We're not going to look at\nthe gender of the lookalike. We're not going to look at\nthe relationship status.",
    "start": "1618380",
    "end": "1623630"
  },
  {
    "text": "We're not going to look at the\nreligious views, the schools, the political\nviews, any of this. We're not going to\nlook at any of that. You can still give\nlookalikes, but we",
    "start": "1623630",
    "end": "1629997"
  },
  {
    "text": "will be unaware of\ntheir demographics, and therefore it will be fair. ",
    "start": "1629997",
    "end": "1637940"
  },
  {
    "text": "Anyway, we'll jump into this\nbut turns out long story is no. And I'm going to give\nyou guys a second",
    "start": "1637940",
    "end": "1644270"
  },
  {
    "text": "to try and think about why. Now, this is a funny plot. It comes from a paper,\nwhich basically said,",
    "start": "1644270",
    "end": "1650220"
  },
  {
    "text": "hey, Facebook, your\nnew lookalike thing still is able to pull people\nbased on demographics.",
    "start": "1650220",
    "end": "1657030"
  },
  {
    "text": "So what they did is, can you\ntry to flip this x and y-axis because the x is what the\nresearchers were controlling?",
    "start": "1657030",
    "end": "1663890"
  },
  {
    "text": "They would make lookalikes with\ndifferent fractions of men, and they did this in the\nold version of Facebook",
    "start": "1663890",
    "end": "1671240"
  },
  {
    "text": "and in the new version\nof Facebook that didn't look at demographics. So in this set of lookalikes,\nthere would be no men at all.",
    "start": "1671240",
    "end": "1677790"
  },
  {
    "text": "And in the top\nset of lookalikes, there would be 100% men. And then this is--",
    "start": "1677790",
    "end": "1683120"
  },
  {
    "text": "the yellow one is when they\nwere looking at demographics, and you would actually get 0 men\nand men that look proportional",
    "start": "1683120",
    "end": "1691970"
  },
  {
    "text": "to the lookalike distribution. But when Facebook didn't\nlook at demographics,",
    "start": "1691970",
    "end": "1697740"
  },
  {
    "text": "look how closely it\nwas able to produce the same fraction of men\nas in the lookalike set,",
    "start": "1697740",
    "end": "1705630"
  },
  {
    "text": "even though the algorithm was\nnot looking at any demographics at all.",
    "start": "1705630",
    "end": "1711000"
  },
  {
    "text": "How is this possible? Algorithm doesn't\nlook at whether or not somebody is a male, but it\ncan figure out a population",
    "start": "1711000",
    "end": "1719180"
  },
  {
    "text": "with the exact same\nproportion of men as in the lookalike population. A little bit of mystery,\nbut a mystery that you guys",
    "start": "1719180",
    "end": "1724940"
  },
  {
    "text": "could figure out if you talked\nwith the person next to you for about a minute. So think about that. How could Facebook\nstill be pulling out",
    "start": "1724940",
    "end": "1731660"
  },
  {
    "text": "the lookalike percentage of men? ",
    "start": "1731660",
    "end": "1738290"
  },
  {
    "text": "[SIDE CONVERSATIONS] ",
    "start": "1738290",
    "end": "1784410"
  },
  {
    "text": "How does this happen? They said, don't look at gender. They said, don't look at age. No, I'm not looking your age.",
    "start": "1784410",
    "end": "1790298"
  },
  {
    "text": "I'm not looking at any of that. Anyone have an idea? Yes. My guess is maybe you can\nlook out for features commonly",
    "start": "1790298",
    "end": "1797430"
  },
  {
    "text": "associated with the users. Do you want to give\nme one that you think maybe you\ncould hypothesize might be associated?",
    "start": "1797430",
    "end": "1802740"
  },
  {
    "text": "Maybe like shaving products. Oh, yeah. Do you like shaving products? Yes. ",
    "start": "1802740",
    "end": "1810360"
  },
  {
    "text": "Everybody shaves. Maybe like your movie selection. I'm not saying that we don't\nall like the same movies.",
    "start": "1810360",
    "end": "1816630"
  },
  {
    "text": "But maybe if you're like, all\nJurassic Parks and Star Wars, then maybe that\ncan just influence.",
    "start": "1816630",
    "end": "1823059"
  },
  {
    "text": "But if you put enough\nof these features together-- maybe this\ndoesn't tell you for sure, and maybe this doesn't\ntell you for sure.",
    "start": "1823060",
    "end": "1830360"
  },
  {
    "text": "But there's this\ninteresting result, which is, as we add more\nand more features, like you tell me more\nand more about a person,",
    "start": "1830360",
    "end": "1837720"
  },
  {
    "text": "even if you don't tell\nme their demographics, I can predict their demographics\nbetter and better and better.",
    "start": "1837720",
    "end": "1843710"
  },
  {
    "text": "As you give me more of these\nthings and more of them leak some information\nabout demographics, you put it all together.",
    "start": "1843710",
    "end": "1849260"
  },
  {
    "text": "And by the time you're\nFacebook and you've told me everything about this\nperson, like their friends, their likes, their history\non the social network,",
    "start": "1849260",
    "end": "1857309"
  },
  {
    "text": "then you have a really good way\nof guessing their demographics without being told.",
    "start": "1857310",
    "end": "1862940"
  },
  {
    "text": "And we call those proxies. Yes. Why would we add all\nthat redundant data in the first place? So what does your\nmovie interest have",
    "start": "1862940",
    "end": "1869600"
  },
  {
    "text": "to do with wanting\nto rent a house? Good question. It should not. I agree with you.",
    "start": "1869600",
    "end": "1875000"
  },
  {
    "text": "But the lookalikes algorithm\ndid that because people would do ads for things\nthat were not houses.",
    "start": "1875000",
    "end": "1881090"
  },
  {
    "text": "Maybe you want to give ads for-- I don't know. You're selling like\nthe coolest PSET app, and you want lots\nof people to buy it.",
    "start": "1881090",
    "end": "1887180"
  },
  {
    "text": "And you think somehow\npeople's likes are going to be really\nimportant in predicting who's going to buy your PSET app.",
    "start": "1887180",
    "end": "1892790"
  },
  {
    "text": "I'm not trying to\nsell the PSET app. It's staying free\nforever just for 109. But does that make sense? It's because it was\nfor the ads platform.",
    "start": "1892790",
    "end": "1898909"
  },
  {
    "text": "And they thought\nthese things would be important for ad sellers. Facebook, their customer\nare the ad buyers.",
    "start": "1898910",
    "end": "1905830"
  },
  {
    "text": "Yes. Don't we have our pictures? They have our pictures. I believe the lookalikes,\nyeah, might have even been",
    "start": "1905830",
    "end": "1911200"
  },
  {
    "text": "using the pictures possibly. I don't know. But if they did, that would\nbe a really strong signal.",
    "start": "1911200",
    "end": "1917110"
  },
  {
    "text": "And maybe you wouldn't\nneed 80 features to predict somebody's mail. Good, good question.",
    "start": "1917110",
    "end": "1922480"
  },
  {
    "text": "I owe you a Mandarin, but\ncome forward after class. ",
    "start": "1922480",
    "end": "1928474"
  },
  {
    "text": "I have a question? Yes.  Lost that previous slide.",
    "start": "1928474",
    "end": "1934880"
  },
  {
    "text": "So how much features--\nhow much information can I tell without\nme compromising",
    "start": "1934880",
    "end": "1941500"
  },
  {
    "text": "when you're figuring out? That's a deep question.",
    "start": "1941500",
    "end": "1947210"
  },
  {
    "text": "So the question was,\nhow much information could I give you without\ncompromising your proxy?",
    "start": "1947210",
    "end": "1953770"
  },
  {
    "text": "It depends on the\nfeature itself. And if we had a\nset of features, we could talk about the\nprobability that somebody",
    "start": "1953770",
    "end": "1960610"
  },
  {
    "text": "could predict this demographic. You know I would\nprobably do to answer that question a simple way?",
    "start": "1960610",
    "end": "1966520"
  },
  {
    "text": "I would take the features,\nand I'd take the data I have. And then I'd build a deep\nlearning or logistic regression",
    "start": "1966520",
    "end": "1973030"
  },
  {
    "text": "model that would try and\npredict the demographics. And I would see how\naccurate it got. And that's how I would test.",
    "start": "1973030",
    "end": "1978250"
  },
  {
    "text": "But it depends on the features. And if you have\nthe right proxies, it's just not that many.",
    "start": "1978250",
    "end": "1984010"
  },
  {
    "text": "Such a good question. I love it. So at this point\nin our story, we",
    "start": "1984010",
    "end": "1989679"
  },
  {
    "text": "would like to get\nformal about fairness. And the first natural\nthing was, well, we'll be fairer by just not\nlooking at the demographics,",
    "start": "1989680",
    "end": "1996380"
  },
  {
    "text": "but it turns out\nthat's really hard. Fairness through unawareness\nis really difficult to do,",
    "start": "1996380",
    "end": "2002720"
  },
  {
    "text": "which maybe invites\nus to start thinking about this other\nphilosophy of maybe",
    "start": "2002720",
    "end": "2008800"
  },
  {
    "text": "we can be fair but by\nlooking at outcomes. So we can try fairness. If not through unawareness,\nthen fairness through awareness.",
    "start": "2008800",
    "end": "2016810"
  },
  {
    "text": "Let's be aware of\ndemographics and make sure that our algorithms\nare fair that way. Now, we've talked about\nthis in section before.",
    "start": "2016810",
    "end": "2023140"
  },
  {
    "text": "So this is a little\nbit of a review, but it's review worth restating\nbecause you're going to need it for your problem sets.",
    "start": "2023140",
    "end": "2029440"
  },
  {
    "text": "If you have a\nclassification algorithm, one of the things\nyou could do is you could build this\njoint distribution table.",
    "start": "2029440",
    "end": "2035710"
  },
  {
    "text": "This is a joint distribution\nbetween three random variables, which you can calculate\nbased off your training data.",
    "start": "2035710",
    "end": "2042170"
  },
  {
    "text": "So one random variable\nis, for any point in the training\ndata set, you can",
    "start": "2042170",
    "end": "2047590"
  },
  {
    "text": "calculate D, which is going\nto be your protected there. Now, there could be\nmore than two groups. But just to keep it clear\nor simple, we can say,",
    "start": "2047590",
    "end": "2054250"
  },
  {
    "text": "we have two different\ndemographics-- D0 and D1. But because you have\na training algorithm",
    "start": "2054250",
    "end": "2060888"
  },
  {
    "text": "there's other variables\nyou have access to. So G aka y hat\ncould be your guess.",
    "start": "2060889",
    "end": "2066199"
  },
  {
    "text": "And G is the guess\nof your model. So if you put this training\ndata through the model that you're\nevaluating, if G is 0,",
    "start": "2066199",
    "end": "2072649"
  },
  {
    "text": "it's going to guess\nthat the label is 0. And if G is one,\nyour algorithm is guessing that the label is 1.",
    "start": "2072650",
    "end": "2077810"
  },
  {
    "text": "And then there is\na separate thing. The guess of the algorithm\nis not always correct.",
    "start": "2077810",
    "end": "2083399"
  },
  {
    "text": "So because this\nis training data, you also have access\nto the truth aka y. And the truth\ncould say, in fact,",
    "start": "2083400",
    "end": "2092510"
  },
  {
    "text": "the true output of\nthis algorithm was 0, or in fact, the true output\nof this algorithm is 1.",
    "start": "2092510",
    "end": "2098230"
  },
  {
    "text": "You can take all\nyour training data, and you can have a\njoint table because you can think about these\nrandom variables together.",
    "start": "2098230",
    "end": "2104769"
  },
  {
    "text": "From these random\nvariables, there are many definitions of\nfairness through awareness.",
    "start": "2104770",
    "end": "2110680"
  },
  {
    "text": "We're going to just\ntalk about a couple of them that are the\nmost commonly used.",
    "start": "2110680",
    "end": "2115760"
  },
  {
    "text": "The first one is, hey, maybe\nI'll think about your algorithm as being fair, if\nit satisfies parity.",
    "start": "2115760",
    "end": "2123680"
  },
  {
    "text": "And if you read the algorithm\ndefinition of parity, it gives a probabilistic\ndefinition. It says, the\nprobability that you",
    "start": "2123680",
    "end": "2130660"
  },
  {
    "text": "guess 1, for somebody\nfrom one demographic, is the same as the probability\nthat you guess 1 for somebody",
    "start": "2130660",
    "end": "2136960"
  },
  {
    "text": "of another demographic. So regardless of\npeople's background, your algorithm has the same\nprobability of predicting a 1.",
    "start": "2136960",
    "end": "2144100"
  },
  {
    "text": "And maybe we think that\nthis is a good sign that the algorithm is\nmaking fair decisions,",
    "start": "2144100",
    "end": "2149650"
  },
  {
    "text": "but it's not the only good sign. A different good sign is that--",
    "start": "2149650",
    "end": "2155329"
  },
  {
    "text": "I want condition on demographics\na different thing to be true. The thing that I\nwant to be true is that G equals T. Now,\nthat's different from saying",
    "start": "2155330",
    "end": "2163630"
  },
  {
    "text": "G equals 1. This is saying that G equals\nT. What's the difference?",
    "start": "2163630",
    "end": "2168730"
  },
  {
    "text": "Yeah. The parity thing on\nthe board, I believe differs from what's\non the slide.",
    "start": "2168730",
    "end": "2174769"
  },
  {
    "text": "Oh. Am I missing one here? Better? Yeah.",
    "start": "2174770",
    "end": "2180890"
  },
  {
    "text": "Yes. So is it saying that the\nalgorithm predicts correctness regardless of demographic\nor regardless of",
    "start": "2180890",
    "end": "2187369"
  },
  {
    "text": "whether demographic is included? Yeah. So this is saying, regardless\nof the true demographic of the person, the\nalgorithm is just as likely",
    "start": "2187370",
    "end": "2195770"
  },
  {
    "text": "to make the right decision. So not like whether we\ninclude demographic or not? Yes. No matter what-- It could be included.",
    "start": "2195770",
    "end": "2201280"
  },
  {
    "text": "It could be not\nincluded, but the outcome is that correctness\nis not conditionally",
    "start": "2201280",
    "end": "2206690"
  },
  {
    "text": "different based on demographic. You could say that\nit is conditionally independent of demographic.",
    "start": "2206690",
    "end": "2212460"
  },
  {
    "text": "Yes. Question. I was going to ask that-- Yes. So it's independent? Yeah. So knowing demographic\nwon't change your belief in",
    "start": "2212460",
    "end": "2219400"
  },
  {
    "text": "whether or not the\nalgorithm get it right. And this first one\nparity says, knowing the demographic won't\nchange your belief that it's",
    "start": "2219400",
    "end": "2225619"
  },
  {
    "text": "going to predict a 1. Those are slightly\ndifferent things. Yeah. Is there a situation\nwhere we wouldn't",
    "start": "2225620",
    "end": "2230960"
  },
  {
    "text": "want it to be the\nsame [? version? ?] But would it, in some cases,\nactually make sense and be",
    "start": "2230960",
    "end": "2239100"
  },
  {
    "text": "beneficial to actually\nmatch the true algorithm? Yeah. There are cases\nwhere you might say--",
    "start": "2239100",
    "end": "2246000"
  },
  {
    "text": " for this one particularly.",
    "start": "2246000",
    "end": "2251265"
  },
  {
    "text": "For different\ndemographics, maybe there is a different underlying rate. So an algorithm that's always\npredicting one independently",
    "start": "2251265",
    "end": "2258480"
  },
  {
    "text": "of demographic. Then maybe it's\ninaccurate because there's a different base rate. But some people say that even if\nthere's a different base rate,",
    "start": "2258480",
    "end": "2264510"
  },
  {
    "text": "that might still be\nsomething worth aiming for because we'd like our\nsociety to move towards this. But then this is a\nlittle bit more saying,",
    "start": "2264510",
    "end": "2271530"
  },
  {
    "text": "there could be a\ndifferent base rate. So you could be predicting G\nequals 1 in different rates, but you're correct the\nsame amount of time.",
    "start": "2271530",
    "end": "2278890"
  },
  {
    "text": "And there are different\nphilosophically. And there's certainly\nbeen algorithms where I've seen people debate very\nhard about what they consider",
    "start": "2278890",
    "end": "2285059"
  },
  {
    "text": "to be fairness because\nit does turn out that not all of these fairnesses\ncan be satisfied together.",
    "start": "2285060",
    "end": "2290520"
  },
  {
    "text": "You sometimes have to make a\ndecision of what fairness you think you're going\nto be going for, and that really\ndepends on the case.",
    "start": "2290520",
    "end": "2297630"
  },
  {
    "text": "I do want to note\na little bit more that there is a legal standard\nfor what level of independence",
    "start": "2297630",
    "end": "2306750"
  },
  {
    "text": "we're looking for. I know things are--\nindependence is binary. You're either\nindependent or not, but people do allow\nthese things to be",
    "start": "2306750",
    "end": "2312940"
  },
  {
    "text": "slightly different in\nsome legal situations. So for example, the US\nhas this legal standard",
    "start": "2312940",
    "end": "2318430"
  },
  {
    "text": "of disparate impact, which\nsays an algorithm is acting in a biased way with\nrespect to calibration,",
    "start": "2318430",
    "end": "2325100"
  },
  {
    "text": "and that's not saying\nthat these two things have to be exactly equal. It allows the ratio\nto be either--",
    "start": "2325100",
    "end": "2331502"
  },
  {
    "text": "this ratio has to be\ngreater than or equal to 1 minus some epsilon. Sorry, that should be 0.2.",
    "start": "2331502",
    "end": "2338900"
  },
  {
    "text": "And regardless of which\nthing goes on the numerator. And we think that\nif this numerator is close enough to\nbeing 1, then we",
    "start": "2338900",
    "end": "2346130"
  },
  {
    "text": "think that this follows\nunder calibration, according to legal standards.",
    "start": "2346130",
    "end": "2352200"
  },
  {
    "text": "Now, there is this\nhistorical story, which is not that historical.",
    "start": "2352200",
    "end": "2357980"
  },
  {
    "text": "It's not that old. Back in the day, there\nwas this algorithm that people were using-- California and Florida-- to\npredict whether or not somebody",
    "start": "2357980",
    "end": "2366310"
  },
  {
    "text": "would commit a crime again. And this is very\nimportant because it was used in courts\nin order to decide",
    "start": "2366310",
    "end": "2371560"
  },
  {
    "text": "if people would get lenient\nsentences or stronger sentences. And I think also to\ndecide whether or not",
    "start": "2371560",
    "end": "2377109"
  },
  {
    "text": "people would be given bail. So this algorithm, which\nhas the name COMPAS,",
    "start": "2377110",
    "end": "2383920"
  },
  {
    "text": "was being used in these\nstates making predictions. But regardless of\nwhether or not you",
    "start": "2383920",
    "end": "2390460"
  },
  {
    "text": "measured calibration or\nparity, it was doing a bad job and that the gap was very large\nbetween Black inmates and White",
    "start": "2390460",
    "end": "2399670"
  },
  {
    "text": "inmates. So at this point, I do think\nfairness through awareness",
    "start": "2399670",
    "end": "2407690"
  },
  {
    "text": "has a lot of strengths. Fairness through unawareness\nis very difficult. Fairness through awareness\nhas this idea that, OK, we're",
    "start": "2407690",
    "end": "2415226"
  },
  {
    "text": "going to pay attention\nto demographics and try and be fair, but it\ndoes have weaknesses that are also worth pointing out.",
    "start": "2415227",
    "end": "2421170"
  },
  {
    "text": "The first one is, you have\nto make a stance about what your definition of fairness is. And there's more than these two.",
    "start": "2421170",
    "end": "2427040"
  },
  {
    "text": "There are 17 definitions. There's people in\nCS and Philosophy who come up with new\ndefinitions all the time.",
    "start": "2427040",
    "end": "2433230"
  },
  {
    "text": "And so you have to choose\nwhat fairness means to you. There is this more\nsubtle harm, though,",
    "start": "2433230",
    "end": "2439940"
  },
  {
    "text": "that can come from\nfairness through awareness that is worth pointing out. And this came out in a paper\nby Cynthia Dwork and Omer",
    "start": "2439940",
    "end": "2448820"
  },
  {
    "text": "Reingold, who were professors\nhere in the CS department, of the self-fulfilling\nprophecy challenge that",
    "start": "2448820",
    "end": "2454369"
  },
  {
    "text": "can come through fairness\nthrough awareness. The idea is, let's\nsay, a classifier",
    "start": "2454370",
    "end": "2460670"
  },
  {
    "text": "is made to be really good\nat one of these definitions of fairness.",
    "start": "2460670",
    "end": "2466170"
  },
  {
    "text": "If it turns out that\npursuing that fairness means that the algorithm\nis a lot worse at choosing",
    "start": "2466170",
    "end": "2473600"
  },
  {
    "text": "strong candidates\nin different groups, then you might see a\nquality of service disparity",
    "start": "2473600",
    "end": "2479810"
  },
  {
    "text": "later on, which\nmight lead people to look at differences\nin the groups and assume that there is a\nstronger difference than there",
    "start": "2479810",
    "end": "2486362"
  },
  {
    "text": "are between the groups, which\ncan change people's mindsets to think that some group is\nstronger than the other, which they call allocation disparity.",
    "start": "2486363",
    "end": "2494670"
  },
  {
    "text": "And so there is an\nargument for there's a lot more nuance to this. Though a lot of--\nthough that's something",
    "start": "2494670",
    "end": "2501109"
  },
  {
    "text": "worth keeping in mind. This is still the state of\nthe art that a lot of people go towards when we\nthink about fairness.",
    "start": "2501110",
    "end": "2507005"
  },
  {
    "text": " So where are we in CS109? Yeah, great question.",
    "start": "2507005",
    "end": "2512285"
  },
  {
    "text": "I was just wondering. So I mean in all of\nthese, I understand that not considering demographic\ninformation like race,",
    "start": "2512285",
    "end": "2517990"
  },
  {
    "text": "for example, would\nbe really important, especially if you're\nlooking at crimes. But if you're looking\nat something that's not,",
    "start": "2517990",
    "end": "2523660"
  },
  {
    "text": "like humans discriminating\nagainst humans but diseases discriminating based on\ngenetics or something like that,",
    "start": "2523660",
    "end": "2529690"
  },
  {
    "text": "how do you draw a line between\nnot including race because you don't necessarily\nhave enough data but still that being\na major factor that",
    "start": "2529690",
    "end": "2536770"
  },
  {
    "text": "contributes to different\ndisease outcomes for people? That's a good question. Let's think about\nthe disease thing.",
    "start": "2536770",
    "end": "2542770"
  },
  {
    "text": "Often, fairness comes down to\nlike decisions made by humans. So let's imagine in\nthe world of disease. What's a decision that\nhumans could be making?",
    "start": "2542770",
    "end": "2550140"
  },
  {
    "text": "Like whether to treat\nor not, I guess. Yeah. Let's say, whether\nto treat or not. So in this world,\nwhether to treat",
    "start": "2550140",
    "end": "2555870"
  },
  {
    "text": "or not is a decision\nthat you're making. It's a decision that we could\nthink through the fairness construct.",
    "start": "2555870",
    "end": "2561432"
  },
  {
    "text": "So now that I'm in this\nworld, what was the question? Sorry. So I mean, I guess, like\nfor example, a doctor is trying to use this\nto decide, should I",
    "start": "2561432",
    "end": "2567960"
  },
  {
    "text": "treat this patient or not? Maybe it's shown that for\nthis subclass of patients,",
    "start": "2567960",
    "end": "2573180"
  },
  {
    "text": "this race of patients,\nthis treatment doesn't work as well for whatever reason. That's what our data shows. That might actually be true.",
    "start": "2573180",
    "end": "2579800"
  },
  {
    "text": "But if we're not\nignoring that, then we're potentially putting\nthose people at harm. Yeah. So that's a good\nargument, I think,",
    "start": "2579800",
    "end": "2585240"
  },
  {
    "text": "for-- you should not be\nusing parity in that case. You should not be saying treat\nat the same rate for the two",
    "start": "2585240",
    "end": "2591150"
  },
  {
    "text": "different groups. I think that's still an\nargument, where you might want to say you're\ncalibrated, which is,",
    "start": "2591150",
    "end": "2596203"
  },
  {
    "text": "I'm not going to treat\nat the same rates because we know that there\nmight be different effects, yet I want the accuracy of\nmy decision to be the same.",
    "start": "2596203",
    "end": "2603790"
  },
  {
    "text": "So if I decide to treat-- and\ntreat was the right decision. That's plus 1 for accuracy. And if I decide\nto treat but that",
    "start": "2603790",
    "end": "2609100"
  },
  {
    "text": "was the wrong decision,\nthat's a 0 for accuracy, and we want that\nrate to be the same. So in calibration,\nyou're still--",
    "start": "2609100",
    "end": "2615010"
  },
  {
    "text": "if you're training\nan algorithm, you can still use demographic\ndata to train it? Yeah, potentially.",
    "start": "2615010",
    "end": "2620740"
  },
  {
    "text": "I guess calibration\ndoesn't speak about the procedure you use. It just speaks\nabout the outcomes. Though, is it possible to\ntry and do both of these",
    "start": "2620740",
    "end": "2627592"
  },
  {
    "text": "at the same time? Absolutely. And I feel like that's\nmaybe a good practice. Don't try and use demographics\nin your prediction.",
    "start": "2627592",
    "end": "2633700"
  },
  {
    "text": "And then make sure that your\noutcomes are somehow fair. Even if demographics have\nbeen shown to impact?",
    "start": "2633700",
    "end": "2640580"
  },
  {
    "text": "No. Maybe that is a\ncase where you say, using demographics\ncould be important. So maybe yeah. Maybe in the particular\ncase you brought up,",
    "start": "2640580",
    "end": "2646780"
  },
  {
    "text": "that might be a good\nexample for we want fairness through awareness. And we, in particular, want\ncalibration and not parity.",
    "start": "2646780",
    "end": "2652900"
  },
  {
    "text": "Yes. If a big model is going to infer\ndemographics through proxies, why not just include\nit at the beginning",
    "start": "2652900",
    "end": "2658840"
  },
  {
    "text": "of the model if\nwe're so sure based on these common considerations\nthat it will be reconstructed?",
    "start": "2658840",
    "end": "2667059"
  },
  {
    "text": "I mean, going back to Facebook,\nthey're legally not allowed to. So you're not supposed to be\nlooking at demographics when",
    "start": "2667060",
    "end": "2672369"
  },
  {
    "text": "making the decision\nbecause the law was built around\nprocedural fairness.",
    "start": "2672370",
    "end": "2678070"
  },
  {
    "text": "So your question is,\nhey, these algorithms are going to build proxies. Should we just throw\nin demographics?",
    "start": "2678070",
    "end": "2683952"
  },
  {
    "text": "And I'd say that the arms race\ngoes in the other direction. We shouldn't include\nthe demographics, and we should be\nmindful about proxies,",
    "start": "2683952",
    "end": "2690100"
  },
  {
    "text": "and we should think of ways that\nwe can deal with the proxies, because it turns out there\nare things we could do.",
    "start": "2690100",
    "end": "2695390"
  },
  {
    "text": "Want to learn more about those? Let's jump into it. So at this point in\nclass, we've said,",
    "start": "2695390",
    "end": "2702230"
  },
  {
    "text": "hey, machine learning\nis an interesting tool, but it's possible\nto create harms. We've talked about two\nspecific types of harms",
    "start": "2702230",
    "end": "2707589"
  },
  {
    "text": "we could talk about. Then we said, we would like to\nbe fair so that whatever harm-- if we're going to possibly\nhave harms, though--",
    "start": "2707590",
    "end": "2713590"
  },
  {
    "text": "it'd be great if\nwe had zero harms-- we should be fair. And we've talked about three\nversions of fairness-- fairness",
    "start": "2713590",
    "end": "2718720"
  },
  {
    "text": "through unawareness,\nand then we've got parity and calibration. Now, at this point,\nthough, I think",
    "start": "2718720",
    "end": "2724380"
  },
  {
    "text": "it's nice to leave our\nstory on the note of, what could we\npossibly do about it?",
    "start": "2724380",
    "end": "2730830"
  },
  {
    "text": "So here are some pro tips. So one of the pro\ntips is being balanced",
    "start": "2730830",
    "end": "2735890"
  },
  {
    "text": "in your training data that comes\nfrom our first set of stories. If you're going to be\ntraining something that does facial recognition,\ntry and represent all people",
    "start": "2735890",
    "end": "2743030"
  },
  {
    "text": "in your training data. There's this other\nthing that's growing in the world of\nAI, which is being",
    "start": "2743030",
    "end": "2749060"
  },
  {
    "text": "very transparent about where\nyour black boxes come from. So if you make a\nblack box algorithm,",
    "start": "2749060",
    "end": "2754415"
  },
  {
    "text": "there's now this thing that you\ncan attach to it called a Model Card, and it tells\neverything, like, who",
    "start": "2754415",
    "end": "2760280"
  },
  {
    "text": "are you making this model? What is your intended use? What are the factors\nthat go into it? How did you evaluate your model?",
    "start": "2760280",
    "end": "2766940"
  },
  {
    "text": "What was the training data\nthat you allowed on it? And what was the\nquantitative analysis. What are all the measures\nyou've done maybe",
    "start": "2766940",
    "end": "2773660"
  },
  {
    "text": "about parity calibration? So this is becoming a standard.",
    "start": "2773660",
    "end": "2779150"
  },
  {
    "text": "And then here's a\nfunny, little story that actually starts in\nCS1 that's interesting. One of the things you\ncould do is, you could ask,",
    "start": "2779150",
    "end": "2787160"
  },
  {
    "text": "can we train the bias out? So back to your question. If you include\ndemographics, of course,",
    "start": "2787160",
    "end": "2794207"
  },
  {
    "text": "it can make decisions\nbased on demographics. If you exclude demographics,\nit might make decisions based on proxies.",
    "start": "2794207",
    "end": "2800630"
  },
  {
    "text": "But can we build an\nalgorithm that tries to remove out those proxies? And that was a question\nthat two seniors",
    "start": "2800630",
    "end": "2807620"
  },
  {
    "text": "at Stanford, former CS109\nstudents were interested in. They're like, can we\nremove these proxies?",
    "start": "2807620",
    "end": "2813770"
  },
  {
    "text": "So they looked at COMPAS. They actually got-- there is a\npublic data set now on COMPAS on decisions it has made.",
    "start": "2813770",
    "end": "2820579"
  },
  {
    "text": "And in COMPAS, the inputs\nlook like all sorts of data about an inmate.",
    "start": "2820580",
    "end": "2826370"
  },
  {
    "text": "It has their zip code, their\npast crimes, their age, lots of inputs like that.",
    "start": "2826370",
    "end": "2831380"
  },
  {
    "text": "And then the prediction\nis recidivism. Are they going to\ncommit a crime again? Now, first of all, when I\ntalk about these inputs,",
    "start": "2831380",
    "end": "2837685"
  },
  {
    "text": "a whole bunch of\nred flags should go off and be like, why do\nyou care about their zip code? Why is that going to make\nyou make a good decision?",
    "start": "2837685",
    "end": "2844760"
  },
  {
    "text": "And just to remind\nyou guys, this was actually used in the state\nof California, in Florida.",
    "start": "2844760",
    "end": "2850160"
  },
  {
    "text": "Now, COMPAS from this\ndata, we can show was very biased, whether\nor not you measure bias",
    "start": "2850160",
    "end": "2856710"
  },
  {
    "text": "in calibration or in parity. You want these gaps to\nbe 0 because the gaps are the difference between the\nsize of your equations.",
    "start": "2856710",
    "end": "2863243"
  },
  {
    "text": "So the bigger the gaps, the\nbigger problem you have, and they had pretty big gaps. So these two students said,\nwhat if we tried to be creative",
    "start": "2863243",
    "end": "2872970"
  },
  {
    "text": "and came up with a way\nto remove the bias that was in this COMPAS algorithm? And so they built a\ntwo-step neural network.",
    "start": "2872970",
    "end": "2881080"
  },
  {
    "text": "The first neural network\nwas the prediction, and this was recreating\nsomething like COMPAS. You had taken the\ninputs, and then you",
    "start": "2881080",
    "end": "2887820"
  },
  {
    "text": "would predict out whether or\nnot somebody would recidivate. But then they had a\nsecond neural network",
    "start": "2887820",
    "end": "2893040"
  },
  {
    "text": "that would take the output\nof the first neural network and then try and\npredict demographic out. And they said, if you train\nthis so that you do really",
    "start": "2893040",
    "end": "2901830"
  },
  {
    "text": "well on model 1-- so likelihood\nis really high in model 1. But then they said,\nmake likelihood",
    "start": "2901830",
    "end": "2907559"
  },
  {
    "text": "really low on the second one. This combined model\nshould be really",
    "start": "2907560",
    "end": "2912960"
  },
  {
    "text": "good at making this prediction\nbut very bad at pulling out demographics. And the theory was\nif you did this,",
    "start": "2912960",
    "end": "2919180"
  },
  {
    "text": "then you could remove\nsome of the proxies that exist within the data. It worked very well.",
    "start": "2919180",
    "end": "2925117"
  },
  {
    "text": "So they ended up with a model\nthat was just as accurate, but they were able to remove\ncalibration gaps and parity",
    "start": "2925117",
    "end": "2930599"
  },
  {
    "text": "gaps. But my favorite thing\nabout this paper was, they did all\nthese cool things. And then their big\nclaim in the paper",
    "start": "2930600",
    "end": "2936780"
  },
  {
    "text": "from the beginning to the\nend was, yes, we can do this, but you should not be\nusing black box algorithms",
    "start": "2936780",
    "end": "2942470"
  },
  {
    "text": "to make recidivism predictions,\nand you should absolutely not be using these features, like\nzip codes shouldn't matter. And it should be something that\nwe can be much more thoughtful.",
    "start": "2942470",
    "end": "2950150"
  },
  {
    "text": "Yes, judges have been\nbiased, but biased algorithms are not the direction that\nwe want to go towards.",
    "start": "2950150",
    "end": "2955160"
  },
  {
    "text": "And particularly,\nthey picked out this idea of using a\nblack box algorithm because it's something\nthat defendants couldn't argue against.",
    "start": "2955160",
    "end": "2961099"
  },
  {
    "text": "If the black box algorithm\njust said like, yep, you're committing a crime\nagain-- no, I'm just joking. You haven't committed any\ncrimes but as an example.",
    "start": "2961100",
    "end": "2968210"
  },
  {
    "text": "And you can argue again. You're like, the\nneural network's wrong. You're like, prove it. You're like, oh,\nlook at the weights. The weights are wrong.",
    "start": "2968210",
    "end": "2973850"
  },
  {
    "text": "And it becomes really\nunfair to the person to argue against a black box\nalgorithm, which brings us back",
    "start": "2973850",
    "end": "2980840"
  },
  {
    "text": "to an idea we saw\nearlier in class. Neural networks are great,\nbut people can't really argue against them.",
    "start": "2980840",
    "end": "2986780"
  },
  {
    "text": "Black boxes might be very\naccurate at making predictions, but you can't\nunderstand or figure out",
    "start": "2986780",
    "end": "2992090"
  },
  {
    "text": "how it's making its decisions. There's some things we can\ndo, some nice, little tricks we can do to try and understand\nwhat the neural network is",
    "start": "2992090",
    "end": "2997970"
  },
  {
    "text": "doing. But the long story\nshort is, understanding a complicated black box can\nbe as hard as understanding",
    "start": "2997970",
    "end": "3003829"
  },
  {
    "text": "how a human makes a decision. That's less true for\nBayesian networks. Bayesian networks--\nthis thing we",
    "start": "3003830",
    "end": "3009770"
  },
  {
    "text": "learned about earlier,\nwhere you have variables, and you have conditional\nprobabilities between the variables. If somebody had an algorithm\nthat was making prediction",
    "start": "3009770",
    "end": "3017359"
  },
  {
    "text": "using a Bayesian network, it's a\nlot easier for somebody to say, hey, this is why your\nalgorithm is wrong in my case",
    "start": "3017360",
    "end": "3023390"
  },
  {
    "text": "because even though\nI have this variable, this conditional\nprobability is wrong for me. ",
    "start": "3023390",
    "end": "3030619"
  },
  {
    "text": "So that's a nice little aside So one of the\nthings you could do is, maybe you could\ntry and train out bias,",
    "start": "3030620",
    "end": "3035750"
  },
  {
    "text": "but you should always\nbe careful about what tasks you're solving. Actually, maybe I should\nhave put that in here.",
    "start": "3035750",
    "end": "3041849"
  },
  {
    "text": "How come I didn't\nput that in there? It's one of the most\nimportant things. Choose the task\nyou're working on.",
    "start": "3041850",
    "end": "3048180"
  },
  {
    "text": "Think about what the\nworld really needs. And can you make something that\nwill make the world a better place? For example, you\nthink about the person",
    "start": "3048180",
    "end": "3053965"
  },
  {
    "text": "who made the gaydar detector\nbased on people's photos. Is that actually\nwhat the world needs? But you imagine, what\nif instead of doing",
    "start": "3053965",
    "end": "3059835"
  },
  {
    "text": "all that intellectual work,\nwe put that intellectual work into making better smart\ngrids so that we can better balance powers?",
    "start": "3059835",
    "end": "3064970"
  },
  {
    "text": "Maybe that's the\nmost important thing. But anyways, here's a\nfun little story, though.",
    "start": "3064970",
    "end": "3070520"
  },
  {
    "text": "One of the interesting things\nis, you will go on from here, and some of you guys will\ngo work in tech companies. And it's interesting\nto think about what",
    "start": "3070520",
    "end": "3077420"
  },
  {
    "text": "responsibilities we can have. And even if you're in\nacademia or tech companies, what responsibility can we\nhave if we're in those roles?",
    "start": "3077420",
    "end": "3085160"
  },
  {
    "text": "Here's a nice, little case\nstudy that's quite interesting. This is from a while ago. It's in Polaroid.",
    "start": "3085160",
    "end": "3090770"
  },
  {
    "text": "So back in the day, do you\nguys know Polaroid cameras from the shake it,\nshake it, shake it?",
    "start": "3090770",
    "end": "3097200"
  },
  {
    "text": "No? Anyways. So you take a photo. It would print it. You take the photo. Actually, it doesn't\nmatter if you shake it.",
    "start": "3097200",
    "end": "3102890"
  },
  {
    "text": "Over time it\ndevelops, and you get to see it in 10 seconds later. Anyways, Polaroid was a\npopular American company.",
    "start": "3102890",
    "end": "3110240"
  },
  {
    "text": "Originally, they\nonly had one flash. And the idea was that,\nit didn't really work",
    "start": "3110240",
    "end": "3116240"
  },
  {
    "text": "with the single flash\nthat they had on people who were not White. So it was really built to\nwork for people who are white.",
    "start": "3116240",
    "end": "3121490"
  },
  {
    "text": "But then Polaroid\nturns out was also",
    "start": "3121490",
    "end": "3126830"
  },
  {
    "text": "selling it to the South\nAfrican government. And they made a feature for\nthe South African government to make the flash work so that\nit could capture black skin",
    "start": "3126830",
    "end": "3136670"
  },
  {
    "text": "tones better. And this was used by\nthe apartheid government to build these\npassbooks that were",
    "start": "3136670",
    "end": "3142310"
  },
  {
    "text": "a central part of discrimination\nin apartheid South Africa. And the interesting\nstory about this",
    "start": "3142310",
    "end": "3147809"
  },
  {
    "text": "is, this would\nhave kept on going, except for an\nengineer who worked at Polaroid, who brought\nthis to light and said, hey,",
    "start": "3147810",
    "end": "3154509"
  },
  {
    "text": "this is wrong. We should talk about this. And that's how change happened. It came from inside the company.",
    "start": "3154510",
    "end": "3161217"
  },
  {
    "text": "We're going to talk\nabout a little bit more, but I thought this\nwould be a great time for a good pedagogical pause. So we've talked about fairness.",
    "start": "3161217",
    "end": "3167850"
  },
  {
    "text": "We've talked about harms. We've talked about what\nyou can do about it. And then I'll just tell you a\nlittle bit about my thoughts",
    "start": "3167850",
    "end": "3173369"
  },
  {
    "text": "on ethics on my own. So take 2 minutes,\nand I'll come back, and we'll continue\nour conversation.",
    "start": "3173370",
    "end": "3178530"
  },
  {
    "text": "Thank you guys so much for\nfollowing along so far. [SIDE CONVERSATIONS]",
    "start": "3178530",
    "end": "3186380"
  },
  {
    "start": "3186380",
    "end": "3311869"
  },
  {
    "text": "Let's bring it on back for the\nhomestretch of this lecture. ",
    "start": "3311870",
    "end": "3319549"
  },
  {
    "text": "Post-pedagogical pause. Let's go back to learning goals. So learning goals--\nthe first thing",
    "start": "3319550",
    "end": "3326060"
  },
  {
    "text": "we talked about in terms of\nformal definitions of fairness was fairness\nthrough unawareness.",
    "start": "3326060",
    "end": "3331759"
  },
  {
    "text": "I hope you guys understand why\nfairness through unawareness is a little more complicated. That Facebook example\nreally drove that home",
    "start": "3331760",
    "end": "3338329"
  },
  {
    "text": "because there could\nbe lots of proxies. If you wanted to\nmake a decision, just not including demographics\nis often not enough.",
    "start": "3338330",
    "end": "3344400"
  },
  {
    "text": "The second learning\ngoal-- know the two ways that we talk about in CS109\nof measuring fairness. So hopefully, this is making\na little bit more sense,",
    "start": "3344400",
    "end": "3352700"
  },
  {
    "text": "even asterisk version of\nthe relaxed calibration.",
    "start": "3352700",
    "end": "3358400"
  },
  {
    "text": "And then we just recently\ntalked about some techniques that we have for\nmitigating fairness issues.",
    "start": "3358400",
    "end": "3365670"
  },
  {
    "text": "Now, when I was thinking\nabout this lecture, I came to this\ndecision that, I don't",
    "start": "3365670",
    "end": "3371720"
  },
  {
    "text": "think a lot of computer\nscientists want to be evil. I just don't. I've seen a lot of people. I feel like a lot of\npeople have-- maybe this",
    "start": "3371720",
    "end": "3378230"
  },
  {
    "text": "is Mencius in me. I believe there's the sprout\nof goodness in everybody, though I do think about the\nblind spots more and more.",
    "start": "3378230",
    "end": "3385940"
  },
  {
    "text": "And let me tell you a story\nthat you might have heard of. And you can let me know if\nyou haven't heard of it.",
    "start": "3385940",
    "end": "3393205"
  },
  {
    "text": "If you haven't,\nthen certainly, I think a story we\nshould all know. My idea is that,\neven though we've",
    "start": "3393205",
    "end": "3399109"
  },
  {
    "text": "got this sprout\nof goodness in us, well-intentioned people can\nstill break things at scale,",
    "start": "3399110",
    "end": "3404339"
  },
  {
    "text": "especially while moving fast. And the story that\nreally struck me-- and actually, when\nthis developed,",
    "start": "3404340",
    "end": "3410180"
  },
  {
    "text": "I think it was a phase change\nfor me as a computer scientist. I was like, oh, we must be\nso much more thoughtful.",
    "start": "3410180",
    "end": "3416518"
  },
  {
    "text": "I don't think anyone\nmeant to do harm here, but real harm was done. Let's take a pause and think\nabout what we're doing.",
    "start": "3416518",
    "end": "3421730"
  },
  {
    "text": "And this is the\nstory of free basics. Just quickly, who\nhere knows the story of the harm from free basic?",
    "start": "3421730",
    "end": "3427710"
  },
  {
    "text": "A couple of-- this is going\nto be a good story to hear. So the story starts\nwith this product",
    "start": "3427710",
    "end": "3434912"
  },
  {
    "text": "that Facebook wanted to give. It was called free basics. And the idea was, we're going\nto give free internet to people",
    "start": "3434912",
    "end": "3439950"
  },
  {
    "text": "around the world. And that's a good start. You're like, yeah, getting\npeople access to internet feels like a good thing. Now, maybe because Facebook\nwanted to get something out",
    "start": "3439950",
    "end": "3447780"
  },
  {
    "text": "of it, or maybe\nfor other reasons, they made the internet be\ndelivered through Facebook.",
    "start": "3447780",
    "end": "3453450"
  },
  {
    "text": "Like, we'll give\nyou free internet, but it comes through Facebook. So Facebook is really the\ninternet that you get.",
    "start": "3453450",
    "end": "3460109"
  },
  {
    "text": "This was deployed in a\nfew countries-- deployed in Sri Lanka. It was deployed,\nI think, in Kenya.",
    "start": "3460110",
    "end": "3466140"
  },
  {
    "text": "No, actually not in Kenya, but\nit was deployed in Myanmar.",
    "start": "3466140",
    "end": "3471234"
  },
  {
    "text": "You had a country\nwhere a lot of people didn't have access\nto the internet. And all of a sudden,\nalmost everyone has access to internet,\nbut it was internet",
    "start": "3471235",
    "end": "3478797"
  },
  {
    "text": "mediated through Facebook. So everybody had\na smartphone now that could open up Facebook,\nand you can get access to it.",
    "start": "3478797",
    "end": "3485490"
  },
  {
    "text": "Myanmar had a military junta. And the military junta started\na misinformation campaign",
    "start": "3485490",
    "end": "3491110"
  },
  {
    "text": "against a particular Muslim\nminority in the country called the Rohingya-- a\nlot of misinformation. Now, we know a lot\nabout misinformation.",
    "start": "3491110",
    "end": "3497770"
  },
  {
    "text": "At that time, it was a\nlittle bit more novel. But you can imagine\nall these people who didn't have much access to\nthe internet before now have it",
    "start": "3497770",
    "end": "3504460"
  },
  {
    "text": "and now are faced with a very\nsophisticated misinformation campaign. But to add fuel to the\nfire, Facebook only",
    "start": "3504460",
    "end": "3512049"
  },
  {
    "text": "had two moderators for the\nentire country of Myanmar who could speak Burmese. So they had just\nunleashed free basics.",
    "start": "3512050",
    "end": "3518410"
  },
  {
    "text": "Lots of people were using it. A huge misinformation\ncampaign starts, and there's just not enough\npeople paying attention",
    "start": "3518410",
    "end": "3524110"
  },
  {
    "text": "to realize it. It led to a genocide. We'll talk about\ncausality in a second.",
    "start": "3524110",
    "end": "3529360"
  },
  {
    "text": "But the genocide that ensued\nafterwards was really awful. Millions of people displaced.",
    "start": "3529360",
    "end": "3535180"
  },
  {
    "text": "And the UN, whenever\nthere's a genocide, they have a mission to try\nand figure out what happened.",
    "start": "3535180",
    "end": "3540490"
  },
  {
    "text": "And the UN concludes that\nthis Facebook free basics was a critical component in\nleading up to this genocide.",
    "start": "3540490",
    "end": "3547809"
  },
  {
    "text": "And so I think they say,\nthe role of social media is significant. Facebook has been\na useful instrument for those seeking to spread\nhate in a context, where,",
    "start": "3547810",
    "end": "3554960"
  },
  {
    "text": "for most users, Facebook\nis the internet. Although improved\nin recent months, the response of Facebook has\nbeen slow and ineffective.",
    "start": "3554960",
    "end": "3561100"
  },
  {
    "text": "And I think this is--\nwhen this happened, it was such a drastic harm.",
    "start": "3561100",
    "end": "3567309"
  },
  {
    "text": "We'd thought about-- I'd seen examples of\nsmall harms, medium harms. And then all of a sudden, this\nwas like, that's a big harm.",
    "start": "3567310",
    "end": "3574690"
  },
  {
    "text": "And I don't think anyone\nat Facebook was like, yes, genocide. We're going to do it. That's not what\npeople were thinking.",
    "start": "3574690",
    "end": "3580609"
  },
  {
    "text": "I think it was\njust a blind spot. They're like, we're\ngoing to give internet. It's going to be great. It's going to go\nthrough Facebook. That'll be fantastic.",
    "start": "3580610",
    "end": "3587020"
  },
  {
    "text": "Why moderate that much? And they just didn't see this\nbig misinformation problem and what it could\npossibly lead to.",
    "start": "3587020",
    "end": "3593500"
  },
  {
    "text": "And so you can imagine this\nmove fast and break things. Mentality, those seemed really\nexciting at the beginning,",
    "start": "3593500",
    "end": "3599079"
  },
  {
    "text": "really came into\nconflict with reality when they started to see\nthese big blind spots. And I think that's also\nespecially true at scale.",
    "start": "3599080",
    "end": "3606610"
  },
  {
    "text": "Facebook was still just used\nin universities-- probably wouldn't be facing\nconsequences like genocide.",
    "start": "3606610",
    "end": "3612869"
  },
  {
    "text": "And suddenly at scale,\nwhen you have blind spots, it can certainly be worse.",
    "start": "3612870",
    "end": "3618180"
  },
  {
    "text": "Facebook has been asked,\nwhat's the answer to this by lots of people? We've seen misinformation\nnot just in Myanmar.",
    "start": "3618180",
    "end": "3624120"
  },
  {
    "text": "And their answer,\ninterestingly, is that the only way\nwe can prevent hate",
    "start": "3624120",
    "end": "3629880"
  },
  {
    "text": "is better machine learning. And I'm not-- we can all\nhave opinions on that. But it's quite interesting to\nthink that scaled computers can",
    "start": "3629880",
    "end": "3639575"
  },
  {
    "text": "be the problem. And then the claim can\nbe that scales computers could be the solution. Though I think some\nintelligent people",
    "start": "3639575",
    "end": "3645780"
  },
  {
    "text": "could argue that maybe this\nis just a way to get around, not actually\naddressing the issue.",
    "start": "3645780",
    "end": "3652912"
  },
  {
    "text": "But the point of this\nwasn't really just to dump on Facebook. I do think, though,\nif I could drive",
    "start": "3652913",
    "end": "3658630"
  },
  {
    "text": "home a point is that, I\ndon't think anyone that I've met in the field\nof computer science really wants to do\nharm, but we should",
    "start": "3658630",
    "end": "3665830"
  },
  {
    "text": "be thoughtful about\nour blind spots. Maybe it's whether or not\nwe're predicting somebody gets admitted to a\nschool, or whether or not",
    "start": "3665830",
    "end": "3672910"
  },
  {
    "text": "we're trying to give\ninternet to other people. We should be humble in that. If we're building something that\nwill affect a lot of people's",
    "start": "3672910",
    "end": "3679570"
  },
  {
    "text": "lives, we should\nthink about what are all the things, the\nunknown unknowns that we might be facing.",
    "start": "3679570",
    "end": "3684980"
  },
  {
    "text": "And then I do want--\nthis is like a little bit of a personal thing. But I would like to\nhighlight one blind spot",
    "start": "3684980",
    "end": "3690190"
  },
  {
    "text": "that I'm trying to contend\nwith, like I think many of us are trying to contend with. This is a computer science blind\nspot that worth bringing up.",
    "start": "3690190",
    "end": "3698860"
  },
  {
    "text": "You guys know this story. Hopefully, you know the\nstory because it's becoming a little bit more popular. Bitcoin is using\na lot of energy.",
    "start": "3698860",
    "end": "3707289"
  },
  {
    "text": "And if you think about it in\nterms of hashes per second, it's a lot. If you think of in\nterms of carbon dioxide",
    "start": "3707290",
    "end": "3714220"
  },
  {
    "text": "in the environment,\nit's a lot too. And despite the fact that we're\nseeing more evidence that this",
    "start": "3714220",
    "end": "3720820"
  },
  {
    "text": "is a problem, it's\nreally hard for us to look at this blind\nspot straight on. And one piece of evidence\nI'd point towards that",
    "start": "3720820",
    "end": "3727119"
  },
  {
    "text": "is that we teach a\nlot of blockchain. But I don't think Bitcoin and\nthe effect on the environment",
    "start": "3727120",
    "end": "3732940"
  },
  {
    "text": "is something that even shows up\nin an ethics class on Stanford. And I will update this\nslide when somebody tells me that has changed.",
    "start": "3732940",
    "end": "3739780"
  },
  {
    "text": "And it's not too hard to\nsee why increased CO2 could be a problem. This is CO2 in the\natmosphere, and you",
    "start": "3739780",
    "end": "3746200"
  },
  {
    "text": "can see it's become\nquite large since humans have started contributing. It's not too hard to\nknow the physics of why",
    "start": "3746200",
    "end": "3753280"
  },
  {
    "text": "CO2 will make our world warmer. We know the fact\nthat it can have-- a cool video if you're curious.",
    "start": "3753280",
    "end": "3759829"
  },
  {
    "text": "It's not too hard to know\nthat the impacts would be really harsh if we have a\nlarge change in temperature.",
    "start": "3759830",
    "end": "3765470"
  },
  {
    "text": "We've seen the world,\nor we have ways of understanding what the world\nwas like when it was quite",
    "start": "3765470",
    "end": "3771110"
  },
  {
    "text": "a lot colder, and\nthere was a lot less, or the change in temperature\nwas a lot smaller.",
    "start": "3771110",
    "end": "3776897"
  },
  {
    "text": "And you can imagine\nwhen we get to the point where the change in temperature\nis quite deviant from what it was in the historical\naverage that it could",
    "start": "3776897",
    "end": "3783257"
  },
  {
    "text": "be something really scary. And one of the reasons it's\nnot too hard to imagine is that we're already\nstarting to see impacts,",
    "start": "3783257",
    "end": "3789260"
  },
  {
    "text": "like in California--\nfires and droughts. But other places,\nlike Hurricane Idai,",
    "start": "3789260",
    "end": "3794420"
  },
  {
    "text": "impacted over 3 million people. They weren't very used to\ngetting huge hurricanes in Mozambique.",
    "start": "3794420",
    "end": "3800290"
  },
  {
    "text": "One of the reasons, I think,\nthat this is such a blind spot is, I think, a lot of\npeople in the world are not versed in the language\nthat you're now versed in.",
    "start": "3800290",
    "end": "3807195"
  },
  {
    "text": "I think the challenge\nof climate change is filled with uncertainties. You have to think about\nfuture amounts of CO2.",
    "start": "3807195",
    "end": "3815110"
  },
  {
    "text": "You have to think\nabout what this will do to the environment,\nthe climate sensitivity. And then on top\nof that, you have",
    "start": "3815110",
    "end": "3820540"
  },
  {
    "text": "to think about how that\nwill impact people. And these could be future\npeople in places far away.",
    "start": "3820540",
    "end": "3826115"
  },
  {
    "text": "And even though there's\na lot of uncertainty, one of the takeaways\nfrom CS109 is, that's not the end of the story.",
    "start": "3826115",
    "end": "3831340"
  },
  {
    "text": "You can reason\nunder uncertainty. You can make good\ndecisions based on uncertain probabilistic\ndistributions",
    "start": "3831340",
    "end": "3837040"
  },
  {
    "text": "that you might have access to. But I am also going to\nacknowledge that maybe this is",
    "start": "3837040",
    "end": "3842480"
  },
  {
    "text": "less of a blind spot. And do you guys ever\nfeel like it's just",
    "start": "3842480",
    "end": "3847550"
  },
  {
    "text": "hard to do something about it? I can tell you climate change\nis a problem, but then, what",
    "start": "3847550",
    "end": "3852890"
  },
  {
    "text": "are we going to do about it? And you felt like that? Yeah. There's something that\nthey're doing for Bitcoin,",
    "start": "3852890",
    "end": "3859802"
  },
  {
    "text": "specifically, around\n[INAUDIBLE] proof-of-stake. Yes. That's very interesting. So the point is that\nthey have actually",
    "start": "3859802",
    "end": "3866480"
  },
  {
    "text": "started to address this. Like for Ethereum, they\nswitched to proof-of-stake from proof-of-work,\nbut Bitcoin has not.",
    "start": "3866480",
    "end": "3872033"
  },
  {
    "text": "Bitcoin has not. And actually, there's something\nreally interesting to that. I was thinking, why\ncan't Bitcoin switch",
    "start": "3872033",
    "end": "3877650"
  },
  {
    "text": "to proof-of-stake? So there's this\nidea of something that's a little less\ndemocratic but will not have this huge\nenvironmental consequence.",
    "start": "3877650",
    "end": "3883590"
  },
  {
    "text": "You could do this for any\nblockchain-based coin. And Bitcoin doesn't change. Why doesn't Bitcoin change?",
    "start": "3883590",
    "end": "3890284"
  },
  {
    "text": "Hard to change it. It's completely decentralized. It's like, there's no one\nin control of Bitcoin. There is no one--",
    "start": "3890284",
    "end": "3895820"
  },
  {
    "text": "I feel like any one person\nwas in control of Bitcoin. You'd be like, yo,\nlook at all the CO2. And they're like,\nyeah, that sucks.",
    "start": "3895820",
    "end": "3901250"
  },
  {
    "text": "Let's change that immediately. But the great idea\nof decentralization seems to have hit\nthis problem of like, there's an obvious thing\nthat we should fix,",
    "start": "3901250",
    "end": "3908070"
  },
  {
    "text": "but decentralized, we\ncan't make that change. Anyway, this is something\nthat I reflect on a lot",
    "start": "3908070",
    "end": "3913280"
  },
  {
    "text": "as I get deeper into thinking\nabout decentralized education. That's a different story.",
    "start": "3913280",
    "end": "3919039"
  },
  {
    "text": "There is this philosophy\nthat I sometimes find myself in which is, with regards\nto climate change,",
    "start": "3919040",
    "end": "3924500"
  },
  {
    "text": "can't we just wait\nand see what happens? And man, I'm such in\nuncomfortable place.",
    "start": "3924500",
    "end": "3932390"
  },
  {
    "text": "I've convinced myself\nthat I can't say that. I've benefited too much. I produce more CO2 than the\naverage person in the world.",
    "start": "3932390",
    "end": "3939650"
  },
  {
    "text": "It's really unfair\nfor me to say, oh, I benefited from all this. And then I'm just going to\nwait and see what happens,",
    "start": "3939650",
    "end": "3945428"
  },
  {
    "text": "especially because I'm\nnot a wealthy person. I'm a professor. But with a meager amounts\nof wealth I've accumulated,",
    "start": "3945428",
    "end": "3952000"
  },
  {
    "text": "which is more than most\npeople in the world, I probably won't be affected\nby the results as much. So I've benefited\nfrom burning CO2.",
    "start": "3952000",
    "end": "3958650"
  },
  {
    "text": "And I probably won't be\naffected by the worst changes in climate\nchange because I",
    "start": "3958650",
    "end": "3963780"
  },
  {
    "text": "could move if I needed to. California might\nnot be hit as hard as places that maybe\ndidn't burn as much CO2.",
    "start": "3963780",
    "end": "3970470"
  },
  {
    "text": "So isn't it totally\nunfair of Chris to say, I'm just going to wait\nand see what happens? And there's ways of\ntelling the story.",
    "start": "3970470",
    "end": "3977290"
  },
  {
    "text": "You can imagine, this is\na very extreme example. But there is a lot of people\nafter World War II who",
    "start": "3977290",
    "end": "3983860"
  },
  {
    "text": "asked the question, what was\nthe problem with bureaucrats in Hitler's empire? And there was a\nlot of people who",
    "start": "3983860",
    "end": "3989020"
  },
  {
    "text": "are like, you know what, I'm\njust going to wait and see what happens. That was a common idea\nof like, let's just see how this thing develops.",
    "start": "3989020",
    "end": "3996520"
  },
  {
    "text": "So instead of\nending on that note, I'm trying to think for\nmyself, what can I do? And maybe this is\ninteresting for you",
    "start": "3996520",
    "end": "4003060"
  },
  {
    "text": "guys thinking about\nwhat can we do. I've decided on--\nan individual level, it's really unfair because I\ncan cut all my CO2 emissions,",
    "start": "4003060",
    "end": "4010672"
  },
  {
    "text": "and that just won't\nchange things. There's big companies. There's big nation\nstates that are producing so much CO2 that might\nchange-- on individual level",
    "start": "4010673",
    "end": "4017820"
  },
  {
    "text": "is hard. And then boy, do I wish I could\naffect the nation state scale? Actually, I don't\nreally want to.",
    "start": "4017820",
    "end": "4024210"
  },
  {
    "text": "But it would be cool if\nI could be like, hey, just this one thing. Why don't we just figure\nout this whole CO2? Let's make a lot\nof clean energy.",
    "start": "4024210",
    "end": "4029859"
  },
  {
    "text": "It'll be fantastic. If I could, I would, but I\ndon't live in that world. But I do think for\na lot of us, there",
    "start": "4029860",
    "end": "4035970"
  },
  {
    "text": "is the sweet spot in\nthe middle of community. And so that brings me\nback to the Bitcoin thing.",
    "start": "4035970",
    "end": "4041190"
  },
  {
    "text": "Maybe as computer scientists who\nare at Stanford and therefore have a lot of\nclout, maybe we can",
    "start": "4041190",
    "end": "4047670"
  },
  {
    "text": "start thinking about ways to\ntalk about this blind spot that might exist around\nthe proof of work that",
    "start": "4047670",
    "end": "4053970"
  },
  {
    "text": "is behind the\nblockchain in Bitcoin. And then this is a\nvery small thing. As computer scientists, if we\nhad a perfectly clean grid,",
    "start": "4053970",
    "end": "4061770"
  },
  {
    "text": "every time you run\nan algorithm, you don't have to worry about it. You can go to sleep\nhappy and be like, it's going to be doing\na bunch of hashes,",
    "start": "4061770",
    "end": "4068048"
  },
  {
    "text": "but I know that hash\nis powered by the wind. And so I think all\nof us could sleep a lot better if we could get\na clean grid in California",
    "start": "4068048",
    "end": "4075270"
  },
  {
    "text": "and in our other locales. And then certainly, I don't like\nto think about AI as a panacea.",
    "start": "4075270",
    "end": "4081428"
  },
  {
    "text": "I think we should\naddress our problems that are rooted in things that\nare outside of AI, sometimes",
    "start": "4081428",
    "end": "4086970"
  },
  {
    "text": "with human institutions. And yet there's probably\nreally cool problems to work on in the world of AI.",
    "start": "4086970",
    "end": "4092460"
  },
  {
    "text": "I've seen people who\nare going to predict the electrical spikes\nthat happen when people plug in electrical vehicles. And if you figure\nthis problem out,",
    "start": "4092460",
    "end": "4098729"
  },
  {
    "text": "then you could really\nreduce the impact. You could predict buildings\nthat are using a lot more CO2, and they're probably\nnot heated well,",
    "start": "4098729",
    "end": "4104759"
  },
  {
    "text": "so you could retrofit them. So there's a lot of cool\nthings that we could do. And if any of you guys\ndo that, you rock,",
    "start": "4104760",
    "end": "4109799"
  },
  {
    "text": "and you should tell me about it. Now, to end this\nlecture, can I just",
    "start": "4109800",
    "end": "4114899"
  },
  {
    "text": "give you guys a\nsmall thing to do? Because ethics is a\ncomplicated thing. I can come and give\nyou formal definitions.",
    "start": "4114899",
    "end": "4121210"
  },
  {
    "text": "I can give you\nsome case studies. But I really think that the\nwork belongs in our hearts.",
    "start": "4121210",
    "end": "4126609"
  },
  {
    "text": "So I'm not going to give you\nany more homework than I already have. But if I could\ngive you something",
    "start": "4126609",
    "end": "4132399"
  },
  {
    "text": "to do after this lecture, it'd\nbe to give yourself some space. Maybe I should just\nput a period here.",
    "start": "4132399",
    "end": "4137689"
  },
  {
    "text": "Man, you guys work hard. I really respect how hard\nyou guys work at Stanford. You're trying to do\nall your classes.",
    "start": "4137689",
    "end": "4143200"
  },
  {
    "text": "You're trying to maintain\nall your friendships and be there for each other. And actually, just giving\noneself space, in general,",
    "start": "4143200",
    "end": "4149589"
  },
  {
    "text": "is great. Put away our phones and\nconnect with yourself is always a good thing. And maybe give\nyourself some space",
    "start": "4149590",
    "end": "4155770"
  },
  {
    "text": "and then reflect on what's your\nown sense of what is right. Because one of the\nthings I've learned by working with wonderful\nstudents like yourselves is,",
    "start": "4155770",
    "end": "4162984"
  },
  {
    "text": "you guys have it in you. And I really believe in\nyour sense of what is right. And if you're\nthoughtful about it,",
    "start": "4162985",
    "end": "4169299"
  },
  {
    "text": "not only will you guys\ndo wonderful things, I think, well, you'll do\nwonderful and fair things",
    "start": "4169300",
    "end": "4174818"
  },
  {
    "text": "and both what is right. And also, how can you craft a\nlife well-lived with respect",
    "start": "4174819",
    "end": "4180068"
  },
  {
    "text": "to what you think is right? I appreciate you guys so much. Thank you so much.",
    "start": "4180069",
    "end": "4185679"
  },
  {
    "text": "Come back next week. We have just two more\nlectures, and then we're done. Oh, finally say, cheers CS109.",
    "start": "4185680",
    "end": "4192540"
  },
  {
    "start": "4192540",
    "end": "4198000"
  }
]