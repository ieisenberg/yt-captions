[
  {
    "start": "0",
    "end": "0"
  },
  {
    "text": "So we've got a few more topics.",
    "start": "0",
    "end": "2280"
  },
  {
    "text": "The next topic is\nfitting neural networks.",
    "start": "2280",
    "end": "4330"
  },
  {
    "text": "And you see we put the\nlittle uphill car there",
    "start": "4330",
    "end": "7470"
  },
  {
    "text": "because this is potentially a\nlittle bit more challenging.",
    "start": "7470",
    "end": "11860"
  },
  {
    "text": "I'm so sorry.",
    "start": "11860",
    "end": "13769"
  },
  {
    "text": "Can I leave now?",
    "start": "13770",
    "end": "14650"
  },
  {
    "text": "Can you leave now.",
    "start": "14650",
    "end": "16689"
  },
  {
    "text": "Now, this is--",
    "start": "16690",
    "end": "17585"
  },
  {
    "text": "If you're going to fall\nasleep, please don't snore.",
    "start": "17585",
    "end": "19710"
  },
  {
    "text": "It's fascinating stuff.",
    "start": "19710",
    "end": "22539"
  },
  {
    "text": "So we've put up a picture of\nour simplest neural network.",
    "start": "22540",
    "end": "26340"
  },
  {
    "text": "We put an objective,\nsay, sum of squares,",
    "start": "26340",
    "end": "28350"
  },
  {
    "text": "and we want to minimize\nthe sum of squares.",
    "start": "28350",
    "end": "30720"
  },
  {
    "text": "And we've expanded f of x there.",
    "start": "30720",
    "end": "34050"
  },
  {
    "text": "And you see, we've got\nparameters, beta and the w's",
    "start": "34050",
    "end": "38399"
  },
  {
    "text": "in this simple little one.",
    "start": "38400",
    "end": "40310"
  },
  {
    "text": "We've got training data.",
    "start": "40310",
    "end": "41340"
  },
  {
    "text": "We've got n observations.",
    "start": "41340",
    "end": "43010"
  },
  {
    "text": "We want to minimize\nsum of squares.",
    "start": "43010",
    "end": "44769"
  },
  {
    "text": "That means we want to\nlearn the w's and the beta",
    "start": "44770",
    "end": "48180"
  },
  {
    "text": "from those data.",
    "start": "48180",
    "end": "51210"
  },
  {
    "text": "Turns out the\nproblem is difficult",
    "start": "51210",
    "end": "53070"
  },
  {
    "text": "because the objective\nis non-convex.",
    "start": "53070",
    "end": "55980"
  },
  {
    "text": "And we'll show you what\nthat means in a moment.",
    "start": "55980",
    "end": "58680"
  },
  {
    "text": "It turns out, however,\ndespite this, the effect",
    "start": "58680",
    "end": "61920"
  },
  {
    "text": "of-- that effect\nof algorithms have",
    "start": "61920",
    "end": "63899"
  },
  {
    "text": "evolved that can optimize\ncomplex neural networks",
    "start": "63900",
    "end": "67470"
  },
  {
    "start": "66000",
    "end": "66000"
  },
  {
    "text": "efficiently.",
    "start": "67470",
    "end": "68680"
  },
  {
    "text": "So let's look at that in\na little bit more detail.",
    "start": "68680",
    "end": "72240"
  },
  {
    "text": "So here's a non-convex function.",
    "start": "72240",
    "end": "74200"
  },
  {
    "text": "We just show you a one\ndimensional function.",
    "start": "74200",
    "end": "76560"
  },
  {
    "text": "And we're going to\nuse gradient descent",
    "start": "76560",
    "end": "79200"
  },
  {
    "text": "to optimize this function.",
    "start": "79200",
    "end": "81420"
  },
  {
    "text": "So you can see here, we\nwrite the function now.",
    "start": "81420",
    "end": "85830"
  },
  {
    "text": "We use the collective term theta\nto represent all the parameters.",
    "start": "85830",
    "end": "90390"
  },
  {
    "text": "So our objective, we\nwrite as r of theta",
    "start": "90390",
    "end": "94620"
  },
  {
    "text": "and it's a sum of squares.",
    "start": "94620",
    "end": "96920"
  },
  {
    "text": "And theta consists of\nall the w's and the beta.",
    "start": "96920",
    "end": "100619"
  },
  {
    "text": "And now we try and depict\nit in one dimension.",
    "start": "100620",
    "end": "103470"
  },
  {
    "text": "And the way gradient\ndescent works",
    "start": "103470",
    "end": "105630"
  },
  {
    "text": "is you start with\nsome guess for theta,",
    "start": "105630",
    "end": "107880"
  },
  {
    "text": "and we start off with\na guess over here.",
    "start": "107880",
    "end": "110219"
  },
  {
    "text": "And this orange curve here\nis the function R of theta.",
    "start": "110220",
    "end": "116230"
  },
  {
    "text": "So if theta was one\ndimensional, this",
    "start": "116230",
    "end": "118750"
  },
  {
    "text": "is a non-convex function,\nwhich what that means is it",
    "start": "118750",
    "end": "123910"
  },
  {
    "text": "doesn't have a single minimum.",
    "start": "123910",
    "end": "125920"
  },
  {
    "text": "This one's got two\nminimums, right?",
    "start": "125920",
    "end": "127990"
  },
  {
    "text": "Comes down and then\nit goes up again.",
    "start": "127990",
    "end": "130220"
  },
  {
    "text": "It's got lots of\nhills and valleys.",
    "start": "130220",
    "end": "132550"
  },
  {
    "text": "Two valleys in this case.",
    "start": "132550",
    "end": "135560"
  },
  {
    "text": "And what gradient descent\ndoes is you start at a guess,",
    "start": "135560",
    "end": "140620"
  },
  {
    "text": "you figure out a\ndirection to move.",
    "start": "140620",
    "end": "143000"
  },
  {
    "text": "In this case, we're either\ngoing to go left or right,",
    "start": "143000",
    "end": "145400"
  },
  {
    "text": "and you move a little\nbit in that direction",
    "start": "145400",
    "end": "147970"
  },
  {
    "text": "and you pick the\ndirection so that you're",
    "start": "147970",
    "end": "150010"
  },
  {
    "text": "going downhill because we\nwant to get to a minimum.",
    "start": "150010",
    "end": "154030"
  },
  {
    "text": "And so you go\ndownhill a little way",
    "start": "154030",
    "end": "156010"
  },
  {
    "text": "and you get to the next guess.",
    "start": "156010",
    "end": "157780"
  },
  {
    "text": "And you keep on doing\nthat moving downhill,",
    "start": "157780",
    "end": "160030"
  },
  {
    "text": "move in downhill until you\ncan't move downhill anymore.",
    "start": "160030",
    "end": "165010"
  },
  {
    "text": "And so in this case, we get down\nto this point, which is, as far",
    "start": "165010",
    "end": "171129"
  },
  {
    "text": "as we know, a local minimum.",
    "start": "171130",
    "end": "172520"
  },
  {
    "text": "Actually, in this picture, it\nlooks like a global minimum.",
    "start": "172520",
    "end": "175240"
  },
  {
    "text": "If we started a little bit to\nthe left of the starting point,",
    "start": "175240",
    "end": "180010"
  },
  {
    "text": "we would have gone in\nthe opposite direction",
    "start": "180010",
    "end": "181870"
  },
  {
    "text": "and got caught up\nin a local minimum.",
    "start": "181870",
    "end": "185230"
  },
  {
    "text": "So that's how gradient\ndescent works.",
    "start": "185230",
    "end": "189239"
  },
  {
    "text": "So the trick is to find\nthese directions to move.",
    "start": "189240",
    "end": "193430"
  },
  {
    "text": "So this is in one dimensional.",
    "start": "193430",
    "end": "194730"
  },
  {
    "text": "As I said, you only go left\nor right a certain distance.",
    "start": "194730",
    "end": "197720"
  },
  {
    "text": "But when you're in\nhigher dimensions,",
    "start": "197720",
    "end": "199550"
  },
  {
    "text": "you have to decide\nwhich direction",
    "start": "199550",
    "end": "201740"
  },
  {
    "text": "to move in that\nparameter space and how",
    "start": "201740",
    "end": "205130"
  },
  {
    "text": "much to move in that direction.",
    "start": "205130",
    "end": "208240"
  },
  {
    "start": "206000",
    "end": "206000"
  },
  {
    "text": "So a little bit more detail.",
    "start": "208240",
    "end": "210880"
  },
  {
    "text": "In the simple\nexample, as I said,",
    "start": "210880",
    "end": "213040"
  },
  {
    "text": "we reached the global minimum.",
    "start": "213040",
    "end": "214489"
  },
  {
    "text": "If we start a little to\nthe left of theta naught,",
    "start": "214490",
    "end": "216550"
  },
  {
    "text": "we would have got\nto a local minimum.",
    "start": "216550",
    "end": "219140"
  },
  {
    "text": "And if the dimension is high, we\nhave to decide in the direction.",
    "start": "219140",
    "end": "223550"
  },
  {
    "text": "So how to find the direction\ndelta that points downhill.",
    "start": "223550",
    "end": "227070"
  },
  {
    "text": "Well, we need to compute what's\nknown as the gradient vector.",
    "start": "227070",
    "end": "231170"
  },
  {
    "text": "And the gradient vector computed\nat some current point theta t",
    "start": "231170",
    "end": "237860"
  },
  {
    "text": "is the vector of derivatives\nof the function, which",
    "start": "237860",
    "end": "243150"
  },
  {
    "text": "is a function of this multi\nparameter theta at each",
    "start": "243150",
    "end": "246870"
  },
  {
    "text": "of the coordinates of theta.",
    "start": "246870",
    "end": "249180"
  },
  {
    "text": "So if theta has\ngot m coordinates,",
    "start": "249180",
    "end": "252909"
  },
  {
    "text": "this gradient vector is\ngoing to be an m vector",
    "start": "252910",
    "end": "256230"
  },
  {
    "text": "and consists of what's known\nas partial derivatives.",
    "start": "256230",
    "end": "260739"
  },
  {
    "text": "Now, it's a fact of calculus\nthat the gradient always",
    "start": "260740",
    "end": "264539"
  },
  {
    "text": "points uphill.",
    "start": "264540",
    "end": "266300"
  },
  {
    "text": "So the gradient has\ncomponents in it",
    "start": "266300",
    "end": "269310"
  },
  {
    "text": "and it tells you which\ndirection in space to move.",
    "start": "269310",
    "end": "271760"
  },
  {
    "text": "And if you move a little\nbit in that direction,",
    "start": "271760",
    "end": "273800"
  },
  {
    "text": "you're going to be going uphill.",
    "start": "273800",
    "end": "276289"
  },
  {
    "text": "So that means the\nway we want to move",
    "start": "276290",
    "end": "278420"
  },
  {
    "text": "is the opposite of the gradient.",
    "start": "278420",
    "end": "279810"
  },
  {
    "text": "So we put a minus sign\nin front of the gradient",
    "start": "279810",
    "end": "282050"
  },
  {
    "text": "and that will make\nus go downhill,",
    "start": "282050",
    "end": "283610"
  },
  {
    "text": "which is what we want to do.",
    "start": "283610",
    "end": "285479"
  },
  {
    "text": "And then we multiply it\nby some small number Rho.",
    "start": "285480",
    "end": "288690"
  },
  {
    "text": "You can see Rho here is 0.001.",
    "start": "288690",
    "end": "291110"
  },
  {
    "text": "That's a typical number.",
    "start": "291110",
    "end": "292969"
  },
  {
    "text": "And that says-- this\ngradient is going",
    "start": "292970",
    "end": "296120"
  },
  {
    "text": "to recommend a certain distance\nto go as well as direction.",
    "start": "296120",
    "end": "300560"
  },
  {
    "text": "We want to be really cautious.",
    "start": "300560",
    "end": "302190"
  },
  {
    "text": "So we want to just go a small\nlittle bit in that direction",
    "start": "302190",
    "end": "305420"
  },
  {
    "text": "to make sure we're\ngoing downhill",
    "start": "305420",
    "end": "309290"
  },
  {
    "text": "because you can overstep and\nend up the gradient may be off",
    "start": "309290",
    "end": "312720"
  },
  {
    "text": "and if you go the full distance,\nyou can start going up again.",
    "start": "312720",
    "end": "316080"
  },
  {
    "text": "So this row is a way\nof slowing things down.",
    "start": "316080",
    "end": "319520"
  },
  {
    "text": "And so that leads to the\nupdate in the parameter.",
    "start": "319520",
    "end": "323490"
  },
  {
    "text": "Theta t plus 1 is theta t\nminus Rho times the gradient.",
    "start": "323490",
    "end": "328569"
  },
  {
    "text": "If this is a little beyond\nyour math skills, don't worry.",
    "start": "328570",
    "end": "332690"
  },
  {
    "text": "This is built into the\nalgorithms that we use.",
    "start": "332690",
    "end": "336170"
  },
  {
    "text": "And this is just to\ngive you-- for those",
    "start": "336170",
    "end": "339190"
  },
  {
    "text": "who understand the calculus,\na way of understanding",
    "start": "339190",
    "end": "341680"
  },
  {
    "text": "what's going on.",
    "start": "341680",
    "end": "342669"
  },
  {
    "text": "Let me go back to the\nprevious picture there.",
    "start": "342670",
    "end": "344770"
  },
  {
    "text": "One of the things that\nmakes this area so complex",
    "start": "344770",
    "end": "347650"
  },
  {
    "text": "and this is actually\na very hot area now",
    "start": "347650",
    "end": "349750"
  },
  {
    "text": "in machine learning\nand optimization,",
    "start": "349750",
    "end": "352330"
  },
  {
    "text": "figuring out algorithms\nfor neural networks,",
    "start": "352330",
    "end": "355639"
  },
  {
    "text": "even if you could get the global\nminimum on the right there,",
    "start": "355640",
    "end": "358920"
  },
  {
    "text": "it's not clear that's a\nsolution you want, right?",
    "start": "358920",
    "end": "361230"
  },
  {
    "text": "Remember the example we had with\n60,000 observations and 200,000",
    "start": "361230",
    "end": "365385"
  },
  {
    "text": "parameters?",
    "start": "365385",
    "end": "365885"
  },
  {
    "text": "Yeah.",
    "start": "365885",
    "end": "366385"
  },
  {
    "text": "The global minimum could\nbe overfit quite badly.",
    "start": "366385",
    "end": "369180"
  },
  {
    "text": "Yes.",
    "start": "369180",
    "end": "369680"
  },
  {
    "text": "So even if we could\nget the global minimum,",
    "start": "369680",
    "end": "373820"
  },
  {
    "text": "we often don't want it.",
    "start": "373820",
    "end": "375200"
  },
  {
    "text": "So there's a lot of study\nnow of the gradient descent",
    "start": "375200",
    "end": "378290"
  },
  {
    "text": "and stochastic gradient\ndescent that figures out-- just",
    "start": "378290",
    "end": "380597"
  },
  {
    "text": "tries to figure out just what\nsolution does it find because it",
    "start": "380597",
    "end": "383180"
  },
  {
    "text": "seems to work well in practice,\nbut people don't really",
    "start": "383180",
    "end": "385160"
  },
  {
    "text": "understand that.",
    "start": "385160",
    "end": "385980"
  },
  {
    "text": "Yeah.",
    "start": "385980",
    "end": "386540"
  },
  {
    "text": "Exactly.",
    "start": "386540",
    "end": "387980"
  },
  {
    "text": "I think in the early days of\nneural networks, they would--",
    "start": "387980",
    "end": "392270"
  },
  {
    "text": "you have to start at some value.",
    "start": "392270",
    "end": "393720"
  },
  {
    "text": "They'll pick some small value\nto-- random value to start with.",
    "start": "393720",
    "end": "396780"
  },
  {
    "text": "They'd fit the\nnetwork many times,",
    "start": "396780",
    "end": "398360"
  },
  {
    "text": "starting at different\nrandom values",
    "start": "398360",
    "end": "399919"
  },
  {
    "text": "and average the solutions.",
    "start": "399920",
    "end": "401750"
  },
  {
    "text": "And that turned out\nto be a better thing",
    "start": "401750",
    "end": "403910"
  },
  {
    "text": "to do than just go to\none global minimum.",
    "start": "403910",
    "end": "406275"
  },
  {
    "text": "So I'm saying typically,\nthese problems",
    "start": "406275",
    "end": "407900"
  },
  {
    "text": "can have thousands of\nlocal minima, right?",
    "start": "407900",
    "end": "410610"
  },
  {
    "text": "And just which one we want and\nwhich one the algorithms tend",
    "start": "410610",
    "end": "414319"
  },
  {
    "text": "to find is really not clear.",
    "start": "414320",
    "end": "415990"
  },
  {
    "text": "Yeah.",
    "start": "415990",
    "end": "417620"
  },
  {
    "text": "OK.",
    "start": "417620",
    "end": "418400"
  },
  {
    "text": "So this is a gradient.",
    "start": "418400",
    "end": "422060"
  },
  {
    "start": "422000",
    "end": "422000"
  },
  {
    "text": "If you thought this was complex,\nthis slide is even more complex.",
    "start": "422060",
    "end": "426320"
  },
  {
    "text": "OK.",
    "start": "426320",
    "end": "428190"
  },
  {
    "text": "So how do we compute\nthe gradients?",
    "start": "428190",
    "end": "429960"
  },
  {
    "text": "So there's a technique\ncalled back propagation.",
    "start": "429960",
    "end": "433250"
  },
  {
    "text": "So again, let's look\nat the simplest case.",
    "start": "433250",
    "end": "435710"
  },
  {
    "text": "We see the loss function.",
    "start": "435710",
    "end": "437810"
  },
  {
    "text": "r of theta is actually a sum\nfrom 1 to n of ri of theta",
    "start": "437810",
    "end": "442669"
  },
  {
    "text": "because it's just summed\nover the observations.",
    "start": "442670",
    "end": "445820"
  },
  {
    "text": "And ri of theta, I've\njust written it out here,",
    "start": "445820",
    "end": "448580"
  },
  {
    "text": "is yi minus f theta of xi\nsquared and in expanded form,",
    "start": "448580",
    "end": "454490"
  },
  {
    "text": "we can write it like this.",
    "start": "454490",
    "end": "456680"
  },
  {
    "text": "Well, when we compute the\ngradient of r of theta,",
    "start": "456680",
    "end": "463220"
  },
  {
    "text": "the way gradients works is you\ncan compute the gradient of each",
    "start": "463220",
    "end": "466550"
  },
  {
    "text": "of these components\nand then sum the,",
    "start": "466550",
    "end": "468419"
  },
  {
    "text": "then you get the gradient of\nr of theta, because r of theta",
    "start": "468420",
    "end": "470900"
  },
  {
    "text": "is a sum over these n terms.",
    "start": "470900",
    "end": "473190"
  },
  {
    "text": "We can compute the\ngradient of each piece",
    "start": "473190",
    "end": "475040"
  },
  {
    "text": "and then sum to\nget the gradient.",
    "start": "475040",
    "end": "476890"
  },
  {
    "text": "So now we need to be able to\ncompute the derivative of this",
    "start": "476890",
    "end": "480890"
  },
  {
    "text": "function with respect to all\nthe betas and all the w's.",
    "start": "480890",
    "end": "485150"
  },
  {
    "text": "So just a bit of notation.",
    "start": "485150",
    "end": "486660"
  },
  {
    "text": "We'll let zk be the linear\ncombination inside here",
    "start": "486660",
    "end": "491090"
  },
  {
    "text": "just for ease of notation.",
    "start": "491090",
    "end": "493639"
  },
  {
    "text": "So now we use what's\nknown as the chain",
    "start": "493640",
    "end": "495830"
  },
  {
    "text": "rule of differentiation.",
    "start": "495830",
    "end": "498530"
  },
  {
    "text": "So let's first differentiate\nwith respect to a beta.",
    "start": "498530",
    "end": "502820"
  },
  {
    "text": "So we first differentiate the\nsum of squares within respect",
    "start": "502820",
    "end": "506630"
  },
  {
    "text": "to f of theta and then f of\ntheta with respect to beta.",
    "start": "506630",
    "end": "511940"
  },
  {
    "text": "And when you do it\nin those two steps",
    "start": "511940",
    "end": "514700"
  },
  {
    "text": "and then multiply\nthem together, this",
    "start": "514700",
    "end": "516530"
  },
  {
    "text": "is what you get, this\nexpression over here.",
    "start": "516530",
    "end": "518870"
  },
  {
    "text": "For the w's, we do differentiate\nwith respect to f of theta",
    "start": "518870",
    "end": "523740"
  },
  {
    "text": "again, then f of theta\nwith respect to g of zk,",
    "start": "523740",
    "end": "528899"
  },
  {
    "text": "g of zk with respect to zk,\nand then zk with respect to w.",
    "start": "528900",
    "end": "534330"
  },
  {
    "text": "So four steps in the chain.",
    "start": "534330",
    "end": "536990"
  },
  {
    "text": "And when you do each of those\nand multiply them together,",
    "start": "536990",
    "end": "539510"
  },
  {
    "text": "you get this\nexpression over here.",
    "start": "539510",
    "end": "542510"
  },
  {
    "text": "And the nice thing here in this\nexample and squared error loss,",
    "start": "542510",
    "end": "546740"
  },
  {
    "text": "we see that the first piece in\nboth of them is the residual.",
    "start": "546740",
    "end": "550770"
  },
  {
    "text": "So the residual is telling\nyou what error you're",
    "start": "550770",
    "end": "553020"
  },
  {
    "text": "making for that observation.",
    "start": "553020",
    "end": "555420"
  },
  {
    "text": "And what you can think of these\nmultiplies on the residual,",
    "start": "555420",
    "end": "560399"
  },
  {
    "text": "they're a way of spreading\nthe residual down",
    "start": "560400",
    "end": "562542"
  },
  {
    "text": "to all the different\nunits that are",
    "start": "562542",
    "end": "564000"
  },
  {
    "text": "responsible for that residual.",
    "start": "564000",
    "end": "566640"
  },
  {
    "text": "So you propagate in the residual\nbackwards to the activations",
    "start": "566640",
    "end": "571680"
  },
  {
    "text": "and therefore the\nweights in the network.",
    "start": "571680",
    "end": "574920"
  },
  {
    "text": "And so this is a simple\nnetwork, but the same idea",
    "start": "574920",
    "end": "579720"
  },
  {
    "text": "gets used on the most\ncomplex networks.",
    "start": "579720",
    "end": "582660"
  },
  {
    "text": "And the nice thing\ntoday is you can have--",
    "start": "582660",
    "end": "585540"
  },
  {
    "text": "the computer programs know how\nto automatically differentiate",
    "start": "585540",
    "end": "588750"
  },
  {
    "text": "functions.",
    "start": "588750",
    "end": "589630"
  },
  {
    "text": "And so we don't even\nhave to think about it.",
    "start": "589630",
    "end": "591660"
  },
  {
    "text": "Just gets done automatically.",
    "start": "591660",
    "end": "593399"
  },
  {
    "text": "So this I agree, this\nis quite technical,",
    "start": "593400",
    "end": "596010"
  },
  {
    "text": "but this is a technical\nsection and that's",
    "start": "596010",
    "end": "598507"
  },
  {
    "text": "the idea of back propagation.",
    "start": "598507",
    "end": "599715"
  },
  {
    "text": "Can you go over that again?",
    "start": "599715",
    "end": "600840"
  },
  {
    "start": "600840",
    "end": "603720"
  },
  {
    "text": "I will not go over it again.",
    "start": "603720",
    "end": "604990"
  },
  {
    "start": "604990",
    "end": "607680"
  },
  {
    "start": "605000",
    "end": "605000"
  },
  {
    "text": "I guess importantly, there's\nsome tricks of the trade.",
    "start": "607680",
    "end": "611010"
  },
  {
    "text": "So one is slow learning.",
    "start": "611010",
    "end": "612990"
  },
  {
    "text": "So it turns out gradient descent\nis a slow way of optimizing",
    "start": "612990",
    "end": "616920"
  },
  {
    "text": "and a small learning rate\nRho even slows it further.",
    "start": "616920",
    "end": "621730"
  },
  {
    "text": "And so one of the ideas\nis to use early stopping",
    "start": "621730",
    "end": "624940"
  },
  {
    "text": "as a form of regularization.",
    "start": "624940",
    "end": "627040"
  },
  {
    "text": "So even though you've got a very\nhighly parameterized network,",
    "start": "627040",
    "end": "630550"
  },
  {
    "text": "and if you fit it\nall the way down",
    "start": "630550",
    "end": "632769"
  },
  {
    "text": "until you reach the minimum,\nyou're going to overfit.",
    "start": "632770",
    "end": "635720"
  },
  {
    "text": "The gradient descent is a\nnice slow way of getting",
    "start": "635720",
    "end": "639040"
  },
  {
    "text": "and if you stop early, you may\nend up in a really good spot.",
    "start": "639040",
    "end": "643329"
  },
  {
    "text": "We saw that with\nthe IMDb reviews",
    "start": "643330",
    "end": "649430"
  },
  {
    "text": "when we train the network,\nthere was a figure.",
    "start": "649430",
    "end": "651880"
  },
  {
    "text": "The performance of the\nnetwork on the test data,",
    "start": "651880",
    "end": "655760"
  },
  {
    "text": "it peaked very\nquickly at around 0.88",
    "start": "655760",
    "end": "658720"
  },
  {
    "text": "and then started going down.",
    "start": "658720",
    "end": "661149"
  },
  {
    "text": "Those epochs were measuring\niterations of training.",
    "start": "661150",
    "end": "665620"
  },
  {
    "text": "What's used pretty much\nin all neural networks",
    "start": "665620",
    "end": "668440"
  },
  {
    "text": "now is stochastic\ngradient descent.",
    "start": "668440",
    "end": "672020"
  },
  {
    "text": "And what you do is rather than\ncompute the gradient using",
    "start": "672020",
    "end": "676100"
  },
  {
    "text": "all the data, you use\na small mini batch",
    "start": "676100",
    "end": "679279"
  },
  {
    "text": "drawn at random at each step.",
    "start": "679280",
    "end": "681830"
  },
  {
    "text": "So for example, with MNIST,\nyou've got 60,000 training",
    "start": "681830",
    "end": "685970"
  },
  {
    "text": "observations, but you may\nuse a mini batch of just 128",
    "start": "685970",
    "end": "690680"
  },
  {
    "text": "observations drawn at random\nfrom those training data.",
    "start": "690680",
    "end": "695170"
  },
  {
    "text": "So you take the\n128 observations,",
    "start": "695170",
    "end": "697570"
  },
  {
    "text": "you compute the\ngradient based on them",
    "start": "697570",
    "end": "699550"
  },
  {
    "text": "and you make your small\ngradient step just using them.",
    "start": "699550",
    "end": "703660"
  },
  {
    "text": "You can imagine that cuts down\non the computation dramatically.",
    "start": "703660",
    "end": "707949"
  },
  {
    "text": "So we talked of epochs.",
    "start": "707950",
    "end": "710830"
  },
  {
    "text": "So the epoch is a\ncount of iterations",
    "start": "710830",
    "end": "713590"
  },
  {
    "text": "and it amounts to the\nnumber of mini batch updates",
    "start": "713590",
    "end": "716650"
  },
  {
    "text": "such that n samples in\ntotal have been processed.",
    "start": "716650",
    "end": "720620"
  },
  {
    "text": "So if we use 128, it's\ngoing to be 60,000 over 128,",
    "start": "720620",
    "end": "724850"
  },
  {
    "text": "which means 469 mini batch\nupdates amounts to going through",
    "start": "724850",
    "end": "730940"
  },
  {
    "text": "an equivalent of the full\nn training samples 60,000.",
    "start": "730940",
    "end": "735780"
  },
  {
    "text": "That's an epoch.",
    "start": "735780",
    "end": "736780"
  },
  {
    "text": "In addition, we\nuse regularization.",
    "start": "736780",
    "end": "739420"
  },
  {
    "text": "So Ridge and Lasso\nregularization",
    "start": "739420",
    "end": "741899"
  },
  {
    "text": "can be used to shrink the\nweights at each layer.",
    "start": "741900",
    "end": "745080"
  },
  {
    "text": "And two other popular\nforms of regularization",
    "start": "745080",
    "end": "747930"
  },
  {
    "text": "are dropout and\naugmentation, which",
    "start": "747930",
    "end": "750870"
  },
  {
    "text": "we're going to discuss next.",
    "start": "750870",
    "end": "752740"
  },
  {
    "start": "751000",
    "end": "751000"
  },
  {
    "text": "So what you do with dropout\nis a simple little network.",
    "start": "752740",
    "end": "756730"
  },
  {
    "text": "And you can see the\noriginal network.",
    "start": "756730",
    "end": "759050"
  },
  {
    "text": "And here, we've colored\nsome of the units gray.",
    "start": "759050",
    "end": "761529"
  },
  {
    "text": "We've colored the\ninput unit gray",
    "start": "761530",
    "end": "763240"
  },
  {
    "text": "and we've colored two of\nthe hidden units gray.",
    "start": "763240",
    "end": "766600"
  },
  {
    "text": "So at each mini batch stochastic\ngradient descent update,",
    "start": "766600",
    "end": "771459"
  },
  {
    "text": "what you do is\nrandomly remove units",
    "start": "771460",
    "end": "774370"
  },
  {
    "text": "with probability phi, maybe\n0.4 or something like that",
    "start": "774370",
    "end": "778420"
  },
  {
    "text": "and scale up the weights of\nthose retained by an amount",
    "start": "778420",
    "end": "782500"
  },
  {
    "text": "to compensate.",
    "start": "782500",
    "end": "783470"
  },
  {
    "text": "It turns out you want to\ndo 1 over 1 minus phi.",
    "start": "783470",
    "end": "786970"
  },
  {
    "text": "Just remove those\nunits and ignore them.",
    "start": "786970",
    "end": "789339"
  },
  {
    "text": "So in simple scenarios\nlike linear regression,",
    "start": "789340",
    "end": "793660"
  },
  {
    "text": "it turns out a version\nof this process",
    "start": "793660",
    "end": "795519"
  },
  {
    "text": "can be shown to be equivalent\nto ridge regularization.",
    "start": "795520",
    "end": "799270"
  },
  {
    "text": "So as in ridge, the\nother units stand",
    "start": "799270",
    "end": "801910"
  },
  {
    "text": "in for those temporarily\nremoved and their weights",
    "start": "801910",
    "end": "804879"
  },
  {
    "text": "are drawn closer together.",
    "start": "804880",
    "end": "806620"
  },
  {
    "text": "And so that's become\na very popular form",
    "start": "806620",
    "end": "809500"
  },
  {
    "text": "for regularization\nof deep networks.",
    "start": "809500",
    "end": "811820"
  },
  {
    "text": "And of course, it speeds\nup the learning as well",
    "start": "811820",
    "end": "813820"
  },
  {
    "text": "because you've got\nless units to process.",
    "start": "813820",
    "end": "816760"
  },
  {
    "text": "It's similar to randomly\nomitting variables",
    "start": "816760",
    "end": "819160"
  },
  {
    "text": "when growing trees\nin random forests.",
    "start": "819160",
    "end": "820858"
  },
  {
    "text": "So if you remember\nin random forests,",
    "start": "820858",
    "end": "822399"
  },
  {
    "text": "each time you get a bootstrap\nsample, you train a tree,",
    "start": "822400",
    "end": "825640"
  },
  {
    "text": "you randomly ignore\nsubsets of variables.",
    "start": "825640",
    "end": "828770"
  },
  {
    "text": "It's the same idea.",
    "start": "828770",
    "end": "830470"
  },
  {
    "text": "And the inventors\nof dropout, they",
    "start": "830470",
    "end": "832660"
  },
  {
    "text": "acknowledge random forests and\nLeo Breiman's contribution.",
    "start": "832660",
    "end": "837440"
  },
  {
    "start": "836000",
    "end": "836000"
  },
  {
    "text": "The other interesting\nform of regularization",
    "start": "837440",
    "end": "841580"
  },
  {
    "text": "is called data augmentation.",
    "start": "841580",
    "end": "845570"
  },
  {
    "text": "So here's a picture, just\ntwo dimensional picture.",
    "start": "845570",
    "end": "848490"
  },
  {
    "text": "The orange points are\nyour actual data points.",
    "start": "848490",
    "end": "851130"
  },
  {
    "text": "You can see the data.",
    "start": "851130",
    "end": "852350"
  },
  {
    "text": "You've just got x1 and x2.",
    "start": "852350",
    "end": "853889"
  },
  {
    "text": "They somewhat correlated.",
    "start": "853890",
    "end": "856010"
  },
  {
    "text": "What we do is in\nthis case, we've",
    "start": "856010",
    "end": "858110"
  },
  {
    "text": "randomly added a little\ncloud of data points",
    "start": "858110",
    "end": "861709"
  },
  {
    "text": "around each original data point.",
    "start": "861710",
    "end": "864020"
  },
  {
    "text": "So we augment the data\nset with random versions",
    "start": "864020",
    "end": "867290"
  },
  {
    "text": "of the original data point\nwith a little bit of noise.",
    "start": "867290",
    "end": "872266"
  },
  {
    "text": "But for each of those,\nwe use the same y value",
    "start": "872266",
    "end": "876110"
  },
  {
    "text": "as the original data point.",
    "start": "876110",
    "end": "878360"
  },
  {
    "text": "So we've just perturbed\nthe feature point",
    "start": "878360",
    "end": "881600"
  },
  {
    "text": "by these random draws.",
    "start": "881600",
    "end": "884610"
  },
  {
    "text": "So what this does is makes\nthe fit robust to small",
    "start": "884610",
    "end": "888589"
  },
  {
    "text": "perturbations in\neach of the xi's.",
    "start": "888590",
    "end": "891570"
  },
  {
    "text": "And it turns out in an\nordinary least square setting,",
    "start": "891570",
    "end": "894500"
  },
  {
    "text": "this is exactly equivalent\nto Ridge regularization.",
    "start": "894500",
    "end": "897610"
  },
  {
    "start": "897610",
    "end": "901037"
  },
  {
    "text": "So that's just the\nheuristic that we want",
    "start": "901038",
    "end": "903380"
  },
  {
    "start": "902000",
    "end": "902000"
  },
  {
    "text": "to use in the case for images.",
    "start": "903380",
    "end": "906160"
  },
  {
    "text": "So in images, what you can\ndo is something similar.",
    "start": "906160",
    "end": "910519"
  },
  {
    "text": "Each time you draw a mini batch\nfor stochastic gradient descent,",
    "start": "910520",
    "end": "915740"
  },
  {
    "text": "you draw an image.",
    "start": "915740",
    "end": "917130"
  },
  {
    "text": "Let's say it's one\nof the 128 images",
    "start": "917130",
    "end": "919970"
  },
  {
    "text": "you're going to draw at random.",
    "start": "919970",
    "end": "921410"
  },
  {
    "text": "You draw an image and\nthen at random, you",
    "start": "921410",
    "end": "924170"
  },
  {
    "text": "apply some natural\ntransformation to that image.",
    "start": "924170",
    "end": "927589"
  },
  {
    "text": "Like you rotate the\ntiger's head a little bit.",
    "start": "927590",
    "end": "930680"
  },
  {
    "text": "You enlarge it a little bit.",
    "start": "930680",
    "end": "932450"
  },
  {
    "text": "You change the attitude.",
    "start": "932450",
    "end": "933710"
  },
  {
    "text": "You stretch it.",
    "start": "933710",
    "end": "934610"
  },
  {
    "text": "You maybe change the\ncolor or the hue.",
    "start": "934610",
    "end": "936959"
  },
  {
    "text": "But these transformations\nare all transformations",
    "start": "936960",
    "end": "940820"
  },
  {
    "text": "that wouldn't confuse a human.",
    "start": "940820",
    "end": "942360"
  },
  {
    "text": "We'll still recognize\nthis as a tiger.",
    "start": "942360",
    "end": "945320"
  },
  {
    "text": "So you can think\nof this as creating",
    "start": "945320",
    "end": "948260"
  },
  {
    "text": "around the original tiger, a\ncloud of different versions",
    "start": "948260",
    "end": "952790"
  },
  {
    "text": "of the same tiger,\nbut somehow they all",
    "start": "952790",
    "end": "955370"
  },
  {
    "text": "represent that same tiger.",
    "start": "955370",
    "end": "956970"
  },
  {
    "text": "So that's like adding\nthe random noise.",
    "start": "956970",
    "end": "959389"
  },
  {
    "text": "And of course, the label for\nall of those images is tiger.",
    "start": "959390",
    "end": "964110"
  },
  {
    "text": "And when you do that, it\nimproves the performance a lot.",
    "start": "964110",
    "end": "971130"
  },
  {
    "text": "And you can think of it as a\nform of Ridge regularization.",
    "start": "971130",
    "end": "976320"
  },
  {
    "text": "In the community, they\nthink of it as augmentation.",
    "start": "976320",
    "end": "978940"
  },
  {
    "text": "You're just increasing\nthe training sample size,",
    "start": "978940",
    "end": "981180"
  },
  {
    "text": "which of course, is going to--",
    "start": "981180",
    "end": "983580"
  },
  {
    "text": "that's going to prevent\noverfitting because now you've",
    "start": "983580",
    "end": "985860"
  },
  {
    "text": "got a bigger\ntraining sample size,",
    "start": "985860",
    "end": "987820"
  },
  {
    "text": "you can fit the model\nmore accurately.",
    "start": "987820",
    "end": "990382"
  },
  {
    "text": "But through this heuristic,\nyou can think of it",
    "start": "990382",
    "end": "992340"
  },
  {
    "text": "as a form of Ridge\nregularization.",
    "start": "992340",
    "end": "993972"
  },
  {
    "text": "It sounds like it would do\nmore than just regularize,",
    "start": "993972",
    "end": "996180"
  },
  {
    "text": "but also make it more robust\nfor future predictions, right?",
    "start": "996180",
    "end": "998890"
  },
  {
    "text": "Yes.",
    "start": "998890",
    "end": "999390"
  },
  {
    "text": "Because if the system was\npresented with a rotated tiger,",
    "start": "999390",
    "end": "1002930"
  },
  {
    "text": "you might not\nnormally recognize it.",
    "start": "1002930",
    "end": "1004860"
  },
  {
    "text": "Yeah.",
    "start": "1004860",
    "end": "1005510"
  },
  {
    "text": "And this idea has been around\nfor a while, wasn't it, Rob?",
    "start": "1005510",
    "end": "1008892"
  },
  {
    "text": "I think they called it Hints.",
    "start": "1008892",
    "end": "1010100"
  },
  {
    "text": "Hints?",
    "start": "1010100",
    "end": "1010610"
  },
  {
    "text": "Yeah.",
    "start": "1010610",
    "end": "1011390"
  },
  {
    "text": "Yeah.",
    "start": "1011390",
    "end": "1012050"
  },
  {
    "text": "So you can think\nof these as hints.",
    "start": "1012050",
    "end": "1014390"
  },
  {
    "text": "You put in different\nversions of the tiger.",
    "start": "1014390",
    "end": "1016440"
  },
  {
    "text": "And you say, by the way,\nthese are also tiger",
    "start": "1016440",
    "end": "1018860"
  },
  {
    "text": "and you don't have\nto get new data.",
    "start": "1018860",
    "end": "1021480"
  },
  {
    "text": "You can just do that.",
    "start": "1021480",
    "end": "1023920"
  }
]