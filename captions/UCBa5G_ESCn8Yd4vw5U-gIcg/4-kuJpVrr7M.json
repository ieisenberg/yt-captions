[
  {
    "start": "0",
    "end": "5420"
  },
  {
    "text": "All right, welcome, everyone. Welcome back. Let's get started.",
    "start": "5420",
    "end": "11059"
  },
  {
    "text": "We have another-action\npacked day for you. Time's a-wasting. To start here, I'm going\nto finish up our big slide",
    "start": "11060",
    "end": "19760"
  },
  {
    "text": "deck on contextual\nword representations. There are just a few more\nsmall things to cover.",
    "start": "19760",
    "end": "25280"
  },
  {
    "text": "And then Sid is going\nto help us get hands on with training\nreally big models.",
    "start": "25280",
    "end": "32780"
  },
  {
    "text": "So there's the link, as\nusual, from the website, if you want to follow along.",
    "start": "32780",
    "end": "37910"
  },
  {
    "text": "And we're going to skip right\nto this section called ELECTRA. ELECTRA is a model that came\nfrom Stanford, from Kevin",
    "start": "37910",
    "end": "44240"
  },
  {
    "text": "Clark and collaborators. And I think it's\nreally exciting. It shows you the\nkind of design space",
    "start": "44240",
    "end": "51050"
  },
  {
    "text": "we're in, a really creative\nexample of doing something that was different from\nwhat had come before it",
    "start": "51050",
    "end": "57274"
  },
  {
    "text": "in the space of transformers. Last time, we talked about\nsome known limitations of the BERT model, most of them\nidentified in the BERT paper",
    "start": "57275",
    "end": "65250"
  },
  {
    "text": "itself. I covered that first one. We just wanted more ablation\nstudies, more exploration",
    "start": "65250",
    "end": "71070"
  },
  {
    "text": "of the BERT architecture. The RoBERTa team\nkicked that off. I think they did a great job.",
    "start": "71070",
    "end": "76140"
  },
  {
    "text": "With ELECTRA, we're going\nto address known limitations two and three. The first is that we have a\nmismatch between the trained",
    "start": "76140",
    "end": "84630"
  },
  {
    "text": "vocabulary and the\nfine-tuned vocabulary because of the role of the MASK token\nin training BERT models.",
    "start": "84630",
    "end": "91710"
  },
  {
    "text": "And the second one, which might\nfeel more pressing to you, is that BERT is\npretty inefficient",
    "start": "91710",
    "end": "97350"
  },
  {
    "text": "when it comes to learning\nfrom data because we mask out or replace\nabout 15% of the tokens.",
    "start": "97350",
    "end": "103740"
  },
  {
    "text": "And as you recall from the\nBERT learning objective, those are the only tokens that\ncontribute to the learning",
    "start": "103740",
    "end": "110010"
  },
  {
    "text": "objective itself. All of the other work\nis kind of redundant. And so we might\nhope that we could",
    "start": "110010",
    "end": "115050"
  },
  {
    "text": "make more efficient use\nof all these sequences that we're processing. ELECTRA is going to make\nsome progress on that too.",
    "start": "115050",
    "end": "122040"
  },
  {
    "text": "So let's focus on the\ncore model structure. And then we'll look at\nall the other things they did in the paper.",
    "start": "122040",
    "end": "127680"
  },
  {
    "text": "We'll start with our\ninput sequence x. This is the chef\ncooked the meal. And the first thing we do is\nmask out some of those tokens.",
    "start": "127680",
    "end": "135330"
  },
  {
    "text": "And that could be a random\nsample of 15% of the tokens, just like in most\nwork with BERT.",
    "start": "135330",
    "end": "141570"
  },
  {
    "text": "Then we have what could\nbe literally a BERT model. We're going to call it\nthe generator, typically a small one, that has a masked\nlanguage modeling objective.",
    "start": "141570",
    "end": "149700"
  },
  {
    "text": "And that can produce\noutput sequences as usual. However the twist\nhere is that we're",
    "start": "149700",
    "end": "155939"
  },
  {
    "text": "going to replace some\nof the tokens that came from the input with\nkind of randomly sampled ones",
    "start": "155940",
    "end": "162360"
  },
  {
    "text": "from the MLMs. You can see here that\nwe've copied over \"the\" and copied over \"chef.\"",
    "start": "162360",
    "end": "167580"
  },
  {
    "text": "But now \"ate\" has been\nreplaced by \"cook.\" That might not have been\nthe most probable output for the model.",
    "start": "167580",
    "end": "172770"
  },
  {
    "text": "But we're going to do that\nreplacement step there. So what we've created\nhere is a sequence",
    "start": "172770",
    "end": "178380"
  },
  {
    "text": "that we can call x corrupt, a\ncorrupted version of the input. And that is the primary job\nof this generator model.",
    "start": "178380",
    "end": "186420"
  },
  {
    "text": "At this point, the heart\nof ELECTRA takes over. This is called\nthe discriminator. But we can also talk about it\nas the ELECTRA model itself,",
    "start": "186420",
    "end": "194020"
  },
  {
    "text": "in essence. The job of the discriminator\nis to figure out which of those\ntokens were originals",
    "start": "194020",
    "end": "200920"
  },
  {
    "text": "and which ones\nwere replacements. So that's a contrastive\nlearning objective. You can see here\nthat the actual label",
    "start": "200920",
    "end": "207220"
  },
  {
    "text": "it's going to learn from, from\n\"ate,\" is that it was replaced, and for \"the,\" that it was\nan original even though it",
    "start": "207220",
    "end": "213160"
  },
  {
    "text": "was a sampled token. And the actual loss for\nthe model is the generator. That is the typical BERT MLM\nloss together with this ELECTRA",
    "start": "213160",
    "end": "222610"
  },
  {
    "text": "loss with a weighting. That's how the model is trained. But there is, as I said, an\nasymmetry here in the sense",
    "start": "222610",
    "end": "228370"
  },
  {
    "text": "that once we've done\nthe pretraining phase, we can let the generator\nfall away entirely",
    "start": "228370",
    "end": "234099"
  },
  {
    "text": "and focus just on the\ndiscriminator as the model that we're going to use for\ndownstream fine-tuning tasks.",
    "start": "234100",
    "end": "240710"
  },
  {
    "text": "And so you can see already\nthat we've, in a way, solved the problem of having\nthis weird MASK token that",
    "start": "240710",
    "end": "246819"
  },
  {
    "text": "comes from the pretraining phase\nbecause the discriminator never sees mask tokens.",
    "start": "246820",
    "end": "251920"
  },
  {
    "text": "All it sees are these\ncorrupted inputs. And it learns to\nfigure out which ones are the corrupted\nversions and which",
    "start": "251920",
    "end": "258729"
  },
  {
    "text": "ones are the originals, which\nis a different capability,",
    "start": "258730",
    "end": "263770"
  },
  {
    "text": "intuitively, than the one we\nwere imbuing the core BERT model with, right? So for BERT, the\nobjective is to figure out",
    "start": "263770",
    "end": "271060"
  },
  {
    "text": "what was missing from\nthe surrounding context. And here, it's\ntrying to figure out which of the words in the\nsequence doesn't belong",
    "start": "271060",
    "end": "278800"
  },
  {
    "text": "and which of them do\nbelong, kind of more discriminating objective.",
    "start": "278800",
    "end": "284610"
  },
  {
    "text": "So that is ELECTRA. Before we dive into the\nexperiments and stuff,",
    "start": "284610",
    "end": "290100"
  },
  {
    "text": "questions about how\nthat model works? Yeah. I guess I'm wondering what\nthe uses of this model are.",
    "start": "290100",
    "end": "296870"
  },
  {
    "text": "So it tries to predict which\nones have been replaced. What applications do\nyou use ELECTRA for?",
    "start": "296870",
    "end": "303860"
  },
  {
    "text": "For pretraining. Yeah, that's what you've\ngot to get your head around. This is a great\nsubtlety to bring out. So the discriminator\nis now going",
    "start": "303860",
    "end": "310490"
  },
  {
    "text": "to be our pre-trained artifact. So just the way\nyou download BERT, and when you do that,\nyou're downloading some MLN-trained thing,\nnow you download ELECTRA,",
    "start": "310490",
    "end": "319670"
  },
  {
    "text": "which is the discriminator. And it's been trained to do this\ndistinguishing thing as opposed",
    "start": "319670",
    "end": "326330"
  },
  {
    "text": "to the filling in the\nblank or continuing thing from the models\nwe've seen so far. The eye-opening thing is\nthat contrastive objective",
    "start": "326330",
    "end": "333470"
  },
  {
    "text": "leads to a really\ngood pre-trained state for fine tuning.",
    "start": "333470",
    "end": "338700"
  },
  {
    "text": "And we might hope that it's\ndoing it much more efficiently. But that's what we can\ndive into now, here.",
    "start": "338700",
    "end": "343900"
  },
  {
    "text": "So first,\ngenerator-discriminator relationships-- they\nobserve, in the paper,",
    "start": "343900",
    "end": "349139"
  },
  {
    "text": "that when the generator\nand the discriminator are the same size, they can\nshare all their transformer",
    "start": "349140",
    "end": "354690"
  },
  {
    "text": "parameters. And more sharing is better. So already we have\nan efficiency gain. And that's intriguing, that\none and the same set of weights",
    "start": "354690",
    "end": "361770"
  },
  {
    "text": "would be playing the role of\nthe MLM and the discriminator-- generator-discriminator.",
    "start": "361770",
    "end": "366780"
  },
  {
    "text": "But they observe that\nthey get the best results from having a\ngenerator that is small",
    "start": "366780",
    "end": "373500"
  },
  {
    "text": "compared to the discriminator. And this plot teases that out. So we've got our GLUE\nscore along the y-axis.",
    "start": "373500",
    "end": "379800"
  },
  {
    "text": "This will just be a measure\nof system quality for them. And along the x-axis here,\nwe have generator size.",
    "start": "379800",
    "end": "387000"
  },
  {
    "text": "And what they mean by\nthat is the dimensionality of the model in the BERT\nsense, so essentially",
    "start": "387000",
    "end": "392610"
  },
  {
    "text": "the size of each\none of the layers. And if we zoom in, for example,\non this blue line, the best",
    "start": "392610",
    "end": "398110"
  },
  {
    "text": "performing model, this\nis where we have 768 as our dimensionality\nfor the discriminator",
    "start": "398110",
    "end": "404060"
  },
  {
    "text": "and 768 for the generator. As we make the generator\nsmaller, all the way down",
    "start": "404060",
    "end": "411280"
  },
  {
    "text": "to 256, performance improves. And that's what we\nmean when we say better",
    "start": "411280",
    "end": "417100"
  },
  {
    "text": "to have a small generator\nand a large discriminator. And that of u-shaped\npattern is repeated",
    "start": "417100",
    "end": "422620"
  },
  {
    "text": "across all the different\ndiscriminator sizes. And that's probably\nan insight about how",
    "start": "422620",
    "end": "428110"
  },
  {
    "text": "this model is working, which\nis the sense that you want the generator to be a little\nbit of a noisy process",
    "start": "428110",
    "end": "434060"
  },
  {
    "text": "so that the discriminator has\nsome interesting work to do. And by making the\ndiscriminator more powerful,",
    "start": "434060",
    "end": "440590"
  },
  {
    "text": "I guess you're creating\nthat kind of opportunity. ",
    "start": "440590",
    "end": "446250"
  },
  {
    "text": "They also do a lot of\nwork looking at efficiency because one of the side\ngoals of the ELECTRA paper",
    "start": "446250",
    "end": "451740"
  },
  {
    "text": "was to end up with\nmodels that were overall more efficient in terms\nof the pretraining compute and in terms of the model size.",
    "start": "451740",
    "end": "458460"
  },
  {
    "text": "Here's another way\nto quantify that. Again, along the y-axis,\nwe have the GLUE score. And along the x-axis now,\nwe have pre-train FLOPs,",
    "start": "458460",
    "end": "465690"
  },
  {
    "text": "which you could just think\nof as a very low level measure of how much\ncompute resources we need",
    "start": "465690",
    "end": "471449"
  },
  {
    "text": "to do the pre-training part. The blue line at\nthe top is ELECTRA. And it's the best, no matter\nwhat your computational budget",
    "start": "471450",
    "end": "479129"
  },
  {
    "text": "is, along the x-axis. They also explore\nadversarial ELECTRA.",
    "start": "479130",
    "end": "484229"
  },
  {
    "text": "This is very intuitive to me. That is a slightly\ndifferent objective, where the generator is trying\nto fool the discriminator",
    "start": "484230",
    "end": "491370"
  },
  {
    "text": "by creating corrupted sequences\nthat are hard for the generator to distinguish.",
    "start": "491370",
    "end": "496920"
  },
  {
    "text": "That's a really good model. But it's less than the\nkind of more cooperative, joint objective that\nI showed you before.",
    "start": "496920",
    "end": "504189"
  },
  {
    "text": "And then the green\nline is cool too. So the green line is where\nI start training with BERT.",
    "start": "504190",
    "end": "509819"
  },
  {
    "text": "And then at a certain\npoint, I switch to having also the\ndiscriminator loss. And at that point,\nthe BERT model",
    "start": "509820",
    "end": "516360"
  },
  {
    "text": "is less good for\nany compute budget, whereas the ELECTRA variant\nstarts to do its ELECTRA thing",
    "start": "516360",
    "end": "521820"
  },
  {
    "text": "and get better and better. So a bunch of\nperspectives on ELECTRA, all pointing to it being a\ngood and efficient model.",
    "start": "521820",
    "end": "530600"
  },
  {
    "text": "And then finally, they do a\nbunch more efficiency analyses. So this is that picture that I\nshowed you of the full ELECTRA",
    "start": "530600",
    "end": "536360"
  },
  {
    "text": "model before, where I have\nthe generator creating corrupted sequences and\nthen the discriminator",
    "start": "536360",
    "end": "542060"
  },
  {
    "text": "doing its discriminating\npart there. You could also\nexplore ELECTRA 15%.",
    "start": "542060",
    "end": "547410"
  },
  {
    "text": "And this is different from\nfull ELECTRA in the sense that on the right,\nfor default ELECTRA,",
    "start": "547410",
    "end": "552680"
  },
  {
    "text": "we make predictions about\nall of these tokens. whether, they were\noriginal or replaced. For the 15% version, we\ndo a BERT-like thing,",
    "start": "552680",
    "end": "560571"
  },
  {
    "text": "where we're going to assume\nthat the ones that weren't part of those corrupted chains\nthere, the sampled part,",
    "start": "560572",
    "end": "566870"
  },
  {
    "text": "are just not part\nof the objective. It'll be fewer tokens there. Replace MLM-- this is an\nablation where actually, we",
    "start": "566870",
    "end": "574490"
  },
  {
    "text": "drop away the ELECTRA part. And we're just looking\nat the MLM here. And we're going to not\nhave the mask token at all",
    "start": "574490",
    "end": "582230"
  },
  {
    "text": "because remember, for BERT,\nthere are a few ways that they do this learning. They do the MASK token. And they also do the\none where they just",
    "start": "582230",
    "end": "588560"
  },
  {
    "text": "replace it with some random\ntokens here, like cook to run. And then the model has\nto reproduce a new token.",
    "start": "588560",
    "end": "595160"
  },
  {
    "text": "Oh, that should say \"cooked,\" I\nguess, because it's pure BERT.",
    "start": "595160",
    "end": "601339"
  },
  {
    "text": "And this is a look at what\nhappens if we don't introduce that MASK token, addressing\nthat question about whether that",
    "start": "601340",
    "end": "607550"
  },
  {
    "text": "was disrupting learning. And then finally, all tokens\nMLM-- this is again, just a BERT-based objective over\nhere, where instead of turning",
    "start": "607550",
    "end": "614540"
  },
  {
    "text": "off the objective\nfor these ones here, that weren't part of\nthe corrupted sequence, we do learning from all of them.",
    "start": "614540",
    "end": "620630"
  },
  {
    "text": "And that's a way of\nsaying, for BERT, if we were making more\nefficient use of the data, could we learn more quickly?",
    "start": "620630",
    "end": "627668"
  },
  {
    "text": "And here are the results. So ELECTRA is at the top. But just below it\nis all-tokens MLM.",
    "start": "627668",
    "end": "634020"
  },
  {
    "text": "So that's just BERT learning\nfrom all of the tokens. And I think that\ndoes show that BERT could have been a little better\nif they had not turned off",
    "start": "634020",
    "end": "641220"
  },
  {
    "text": "the objective for\nevery single token that wasn't part of the masking or\ncorruption for that learning",
    "start": "641220",
    "end": "646350"
  },
  {
    "text": "process. Replace MLM is just below that. And that's where we don't have\nany MASK tokens so there's",
    "start": "646350",
    "end": "651600"
  },
  {
    "text": "no fine-tuning pre-trained\nmismatch, ELECTRA 15 below that, and then\nBERT at the bottom.",
    "start": "651600",
    "end": "656879"
  },
  {
    "text": "So overall, you're seeing\nthese ablations are showing us that every piece of ELECTRA\nis contributing something",
    "start": "656880",
    "end": "663630"
  },
  {
    "text": "to the overall\nperformance of the model. And that's quite nice as well. Yeah? How is the efficiency\non all-tokens MLM?",
    "start": "663630",
    "end": "672520"
  },
  {
    "text": "The efficiency? Yeah. Well, we're making more\nefficient use of the data because we're getting a learning\nsignal from every token.",
    "start": "672520",
    "end": "679930"
  },
  {
    "text": "And I guess that would be\nthe important dimension because a funny thing about\nBERT, where we turn off",
    "start": "679930",
    "end": "685960"
  },
  {
    "text": "the learning for the ones that\nweren't masked or corrupted, is that we still have to do\nthe work of computing them.",
    "start": "685960",
    "end": "691460"
  },
  {
    "text": "It's just that then they don't\nbecome part of the objective. And here, we're just\nkind of bringing that in.",
    "start": "691460",
    "end": "696700"
  },
  {
    "text": "So for free or close to it. Other questions? My question was how is\nthe GLUE score calculated?",
    "start": "696700",
    "end": "704460"
  },
  {
    "text": "What does it represent, some\naccuracy in language generation afterwards? Or is it the classifier\nthat's being scored?",
    "start": "704460",
    "end": "711720"
  },
  {
    "text": "Oh, yeah, so GLUE is a big,\nmulti-task classification benchmark. It's a pretty diverse set\nof tasks, maybe biased",
    "start": "711720",
    "end": "719639"
  },
  {
    "text": "toward natural\nlanguage inference. And the reason they're\nusing it in the paper is just that it has been adopted\nas a general-purpose measure",
    "start": "719640",
    "end": "729930"
  },
  {
    "text": "of performance. And it's driven a lot of\nreasoning about what's good and what's bad in the field. ",
    "start": "729930",
    "end": "737690"
  },
  {
    "text": "And then here are\nsome model releases-- base and large,\naligned with BERT. And then we have this\nsmall model here.",
    "start": "737690",
    "end": "744380"
  },
  {
    "text": "And that was designed\nto be quickly trained on a single GPU, again, as\na nod toward efficiency. And all three are\nreally good models.",
    "start": "744380",
    "end": "750755"
  },
  {
    "text": " Yeah? Do the things that\nwe've observed",
    "start": "750755",
    "end": "756110"
  },
  {
    "text": "on ELECTRA, when\nit comes to putting our text into some kind\nof representation space,",
    "start": "756110",
    "end": "762260"
  },
  {
    "text": "that it's better than\nBERT for and RoBERTa? Or is it just generally,\nGLUE-wise, it's",
    "start": "762260",
    "end": "769069"
  },
  {
    "text": "slightly better? Oh, I like that question. That could cue up\nsome analysis work",
    "start": "769070",
    "end": "774079"
  },
  {
    "text": "that you could do\nfor a final project because I think the insight\nbehind your question is that a lot of the time,\nwe reason about these models",
    "start": "774080",
    "end": "780980"
  },
  {
    "text": "just based on their performance\non something like GLUE. You could ask a\ndeeper question, what are their internal\nrepresentations like, and are",
    "start": "780980",
    "end": "788360"
  },
  {
    "text": "there places where they're\ntransformatively better or worse? And that you could tie\nthat back to the fact",
    "start": "788360",
    "end": "793640"
  },
  {
    "text": "that the learning\nobjective is different. We're doing this\ndiscrimination thing as opposed to filling in\nthe blanks in some sense.",
    "start": "793640",
    "end": "800209"
  },
  {
    "text": "Maybe there are some\nunderlying differences. I love that. ",
    "start": "800210",
    "end": "810050"
  },
  {
    "text": "All right, a couple more\ntopics here, just quickly, because we're going to do more\nwork with seq2seq models later.",
    "start": "810050",
    "end": "816576"
  },
  {
    "text": "We're going to train some\nof our own from scratch. And you all might use\nsome fine-tuned ones. And so I thought it would\nbe good to just get them",
    "start": "816577",
    "end": "822363"
  },
  {
    "text": "on the table as well. seq2seq-- here are some natural\ntasks that fall into seq2seq",
    "start": "822363",
    "end": "828400"
  },
  {
    "text": "structure-- machine\ntranslation, right, source language to target\nlanguage; summarization,",
    "start": "828400",
    "end": "833920"
  },
  {
    "text": "big text to hopefully\nsmaller texts; free-form question answering,\nwhere you go from a question",
    "start": "833920",
    "end": "839350"
  },
  {
    "text": "and then maybe\nyou're generating, as opposed to just extracting\nan answer; dialogue, of course;",
    "start": "839350",
    "end": "845380"
  },
  {
    "text": "semantic parsing-- this is the one we're\ngoing to tackle, where you go from a sentence\nto some kind of logical form,",
    "start": "845380",
    "end": "850900"
  },
  {
    "text": "representing its meaning-- code generation, of\ncourse-- that's similar-- and on and on.",
    "start": "850900",
    "end": "856400"
  },
  {
    "text": "I think there are\nlots of problems that are pretty naturally cast\nas seq2seq problems, especially",
    "start": "856400",
    "end": "861490"
  },
  {
    "text": "when you've got kind\nof different stuff on the input and\nthe output side.",
    "start": "861490",
    "end": "867920"
  },
  {
    "text": "Yeah, and the more\ngeneral class of things we could be talking about\nwould be encoder decoder, where that's just more\ngeneral in the sense",
    "start": "867920",
    "end": "874490"
  },
  {
    "text": "that at that point, the\ninput could be a picture you're encoding and\nthe output, a text, picture, to picture,\nvideo to picture.",
    "start": "874490",
    "end": "881662"
  },
  {
    "text": "In principle, anything could\nbe happening on the two sides. And seq2seq would for me,\njust be the special case",
    "start": "881662",
    "end": "886790"
  },
  {
    "text": "where we're looking at\nsequential data, typically language data or computer\ncode or something.",
    "start": "886790",
    "end": "892190"
  },
  {
    "text": " From the RNN era-- this is just nice.",
    "start": "892190",
    "end": "898020"
  },
  {
    "text": "If you hearken back to that\nera, if you lived through it-- this is a paper from\nThang Luong doing seq2seq",
    "start": "898020",
    "end": "905250"
  },
  {
    "text": "on the left, in the\ntraditional way, with a recurrent\nneural network, an RNN. Pretty simple. We've got a A-B-C-D coming in.",
    "start": "905250",
    "end": "911820"
  },
  {
    "text": "And then it transitions\ninto maybe other parameters. And it's trying to produce this\nnew sequence, left to right, coming out.",
    "start": "911820",
    "end": "917790"
  },
  {
    "text": "And just to remind\nyou-- this is part of the journey the field\nwent on-- what Thang did, very influentially,\nis think a lot",
    "start": "917790",
    "end": "924450"
  },
  {
    "text": "about how you would\nadd attention layers in to that RNN.",
    "start": "924450",
    "end": "929603"
  },
  {
    "text": "That's what you\nsee depicted here. This is a schematic\ndiagram, hinting at the fact that we were moving into\nan era, the [? Vaswani ?]",
    "start": "929603",
    "end": "937200"
  },
  {
    "text": "et al. era--\nattention is all you need-- where basically,\nthat attention layer would do all the work.",
    "start": "937200",
    "end": "943380"
  },
  {
    "text": "And that's where we're at now. For seq2seq problems\nin general--",
    "start": "943380",
    "end": "949519"
  },
  {
    "text": "this is a nice framework\nfrom the T5 paper-- there are a few different ways\nyou could think about them.",
    "start": "949520",
    "end": "954830"
  },
  {
    "text": "On the left is the one\nI'm nudging you toward, where we have an\nencoder and a decoder. And if we're talking about\ntransformer models, what",
    "start": "954830",
    "end": "962390"
  },
  {
    "text": "essentially that means is\nthat when we do encoding, we can connect everything\nto everything else. And you can think of that as\na process of simultaneously",
    "start": "962390",
    "end": "970160"
  },
  {
    "text": "encoding the entire input\nwith all its connections. But as we do decoding, for\nmany of these problems,",
    "start": "970160",
    "end": "976700"
  },
  {
    "text": "we need to do some kind\nof sequential generation. And the result of that is we\ncan look back to the decoder--",
    "start": "976700",
    "end": "983390"
  },
  {
    "text": "sorry, the encoder--\nall we want. But for the decoder, we\nhave to do that masking that I described, with\nthe auto-regressive loss,",
    "start": "983390",
    "end": "990080"
  },
  {
    "text": "last time, so that we\ndon't look into the future. But with that constraint, we can\ndo this encoder-decoder thing",
    "start": "990080",
    "end": "996889"
  },
  {
    "text": "with a decoding step being\ntruly sequential decoding. But that's not the only way\nto think about these problems.",
    "start": "996890",
    "end": "1003459"
  },
  {
    "text": "And in fact, I don't\nwant to presuppose that for a\nsequence-to-sequence thing, you'll use a\nsequence-to-sequence model.",
    "start": "1003460",
    "end": "1009230"
  },
  {
    "text": "You could, for example,\nuse a language model. And the way you might\ndo that is to say I'm just going to encode\neverything left to right.",
    "start": "1009230",
    "end": "1015870"
  },
  {
    "text": "That's the version\nin the middle. And then a compromise\nposition would be that you would take\nyour language model, which",
    "start": "1015870",
    "end": "1021770"
  },
  {
    "text": "might be auto regressive,\nbut simultaneously encode the entire input. And then begin your\nprocess of decoding",
    "start": "1021770",
    "end": "1029270"
  },
  {
    "text": "without explicitly having an\nencoder part and a decoder part. I think all of them\nare on the table.",
    "start": "1029270",
    "end": "1035809"
  },
  {
    "text": "And people are solving\nseq2seq tasks right now, using all of these variants. ",
    "start": "1035810",
    "end": "1042919"
  },
  {
    "text": "T5-- I'm going to show you two. There are lots out there. But these are very prominent\nones that you might download.",
    "start": "1042920",
    "end": "1048770"
  },
  {
    "text": "So T5-- this is a wonderful,\nvery rich paper that does a lot of exploration of\nwhich of these architectures",
    "start": "1048770",
    "end": "1054350"
  },
  {
    "text": "are effective. And T5 ended up on an\nencoder-decoder variant. And what they did is\nan impressive amount",
    "start": "1054350",
    "end": "1061190"
  },
  {
    "text": "of multi-task training,\nunsupervised and supervised objectives. And an innovative\nthing that they did",
    "start": "1061190",
    "end": "1068300"
  },
  {
    "text": "is have these task prefixes,\nlike translate English to German, and then\nan English sentence,",
    "start": "1068300",
    "end": "1073850"
  },
  {
    "text": "or this is a CoLA\nsentence-- that's just a data set people\nuse-- or an STSB sentence.",
    "start": "1073850",
    "end": "1079940"
  },
  {
    "text": "And that's the model's cue\nto take that input and kind of condition it, very\ninformally, on that task",
    "start": "1079940",
    "end": "1086310"
  },
  {
    "text": "so that the output behavior\nis the expected behavior. There are lots of T5 models\nthat you can download.",
    "start": "1086310",
    "end": "1093648"
  },
  {
    "text": "This is nice for development\nbecause some of them are very small and some of\nthem are very, very large. And more recently, there\nare these FLAN models",
    "start": "1093648",
    "end": "1100789"
  },
  {
    "text": "which took a T5 architecture\nand did a lot of reinforcement learning with human feedback,\nto even further specialize them",
    "start": "1100790",
    "end": "1108000"
  },
  {
    "text": "to different tasks,\nin interesting ways. So that's T5. And then the other one\nthat you often hear about,",
    "start": "1108000",
    "end": "1114300"
  },
  {
    "text": "that's very effective, is BART. And BART is interestingly\ndifferent, yet again. So BART is an\nencoder-decoder framework.",
    "start": "1114300",
    "end": "1121710"
  },
  {
    "text": "And it's really got kind\nof a BERT-style thing on the left and then a\nGPT-style thing on the right.",
    "start": "1121710",
    "end": "1127770"
  },
  {
    "text": "That is joint encoding\nof everything and then that auto-regressive\npart, if you want to do sequential generation.",
    "start": "1127770",
    "end": "1134190"
  },
  {
    "text": "The innovative\nthing about BART is that the training involves a\nlot of corrupting of that input",
    "start": "1134190",
    "end": "1140700"
  },
  {
    "text": "sequence. They tried to do text\ninfilling of pieces. They shuffled sentences around.",
    "start": "1140700",
    "end": "1146040"
  },
  {
    "text": "They did some masking. They did some token\ndeletion, rotating of documents, all of this\ncorrupting of the input.",
    "start": "1146040",
    "end": "1152519"
  },
  {
    "text": "And then the\nmodel's objective is to learn how to\nessentially uncorrupt what it got as the input.",
    "start": "1152520",
    "end": "1158340"
  },
  {
    "text": "And they found that\nthe joint process of this text-infilling\nthing and sentence shuffling",
    "start": "1158340",
    "end": "1163530"
  },
  {
    "text": "was the most effective\nfor training BART. So that was for the\npre-training phase. And then when you fine tune with\nBART, for classification tasks,",
    "start": "1163530",
    "end": "1172500"
  },
  {
    "text": "you just put in two uncorrupted\ncopies of your sentence. And then you could fit\nyour task-specific labels",
    "start": "1172500",
    "end": "1178650"
  },
  {
    "text": "on the class token or the\nfinal token of the GPT output. And for seq2seq, you just use it\nas a standard encoder-decoder.",
    "start": "1178650",
    "end": "1186420"
  },
  {
    "text": "And again, the intuition is that\nthe pre-training phase, which did all this corruption,\nhas helped the model learn",
    "start": "1186420",
    "end": "1192450"
  },
  {
    "text": "what sequences are like. And that blends\ntogether, for me, a lot of the\nintuitions we've seen from MLM and from what we just\ntalked about, with ELECTRA.",
    "start": "1192450",
    "end": "1202630"
  },
  {
    "text": "Yeah? A question that I\nasked last week-- Talib's models worked with\nspelling mistakes, things",
    "start": "1202630",
    "end": "1209090"
  },
  {
    "text": "like that. Yeah, so BART is a\nreally good option if you want to do\nspelling correction. And I actually think that might\nbe because spelling correction",
    "start": "1209090",
    "end": "1217420"
  },
  {
    "text": "is, as a task, a corrupting\nof the input, where you're trying to learn\nthe uncorrupted version",
    "start": "1217420",
    "end": "1222790"
  },
  {
    "text": "of the output. So I think if you want to do\ngrammar correction, spelling correction, things like that,\nit's outstanding to use BART.",
    "start": "1222790",
    "end": "1228820"
  },
  {
    "text": "And you might just think\nof training from scratch on a model that is\ngoing to be aware of characters, for these\ncharacter-level things.",
    "start": "1228820",
    "end": "1237179"
  },
  {
    "text": "Yeah? Sorry, what does\ntext infilling mean? That was where they removed\nparts of the text, essentially,",
    "start": "1237180",
    "end": "1244530"
  },
  {
    "text": "and added other\npieces, to corrupt it. Oh, different from masking,\nwhere you just hide it. Yeah, where you just-- that's\nmore like the BERT-style thing,",
    "start": "1244530",
    "end": "1251507"
  },
  {
    "text": "where you hide some. Yeah.  And final quick topic--",
    "start": "1251507",
    "end": "1259260"
  },
  {
    "text": "I just want you all to know\nabout distillation, again, because a theme of this\ncourse could be how can",
    "start": "1259260",
    "end": "1264600"
  },
  {
    "text": "we do more with less. And distillation is a vision for\nhow we could do more with less.",
    "start": "1264600",
    "end": "1269790"
  },
  {
    "text": "We saw this trend\nin model sizes here, where they're getting\nbigger and bigger. And there is some hope that they\nmight now be getting smaller.",
    "start": "1269790",
    "end": "1277350"
  },
  {
    "text": "But we should all be pushing\nto make them ever smaller. And one way to think about\ndoing that is distillation.",
    "start": "1277350",
    "end": "1282630"
  },
  {
    "text": "And the metaphor\nhere is that we're going to have two models, maybe\na really big teacher model that",
    "start": "1282630",
    "end": "1287880"
  },
  {
    "text": "was trained in a\nvery expensive way and might run only\non a supercomputer, and then a much smaller student.",
    "start": "1287880",
    "end": "1294150"
  },
  {
    "text": "And we're going to\ntrain that student to mimic the behavior\nof the teacher. And we could do that\nby just observing",
    "start": "1294150",
    "end": "1300269"
  },
  {
    "text": "the output behavior\nof the teacher, and then trying to get\nthe student to align at the level of the output.",
    "start": "1300270",
    "end": "1306090"
  },
  {
    "text": "And that would basically\njust be treating this teacher as a input-output device. We could also, though,\nthink about aligning",
    "start": "1306090",
    "end": "1313530"
  },
  {
    "text": "the internal\nrepresentations of these two to get a deeper alignment\nbetween teacher and student.",
    "start": "1313530",
    "end": "1320610"
  },
  {
    "text": "Here are some\nobjectives in fact. And this is from least\nto most heavy duty. And you could combine them.",
    "start": "1320610",
    "end": "1326140"
  },
  {
    "text": "So we could just use our\ngold data for the task. I put that step zero because you\nmight want it in the mix here,",
    "start": "1326140",
    "end": "1332040"
  },
  {
    "text": "even as you use your teacher. We could also learn just from\nthe teacher's output labels.",
    "start": "1332040",
    "end": "1337440"
  },
  {
    "text": "That's a bit of a funny idea. But I think the intuition\nis that the teacher might be doing some very complicated\nregularization that",
    "start": "1337440",
    "end": "1345540"
  },
  {
    "text": "helps the student\nlearn more efficiently. So even if there are mistakes\nin the teacher's behavior, the student actually benefits.",
    "start": "1345540",
    "end": "1353100"
  },
  {
    "text": "You could also think about\ngoing one level deeper and using the full output\nscores, like the logits, so not just the discrete outputs,\nbut the whole distribution",
    "start": "1353100",
    "end": "1360810"
  },
  {
    "text": "that the model predicts. And that's what they did in one\nof the original distillation papers.",
    "start": "1360810",
    "end": "1366120"
  },
  {
    "text": "You could also tie together\nthe final output states. If the two models have the same\nlayer-wise dimensionality, then",
    "start": "1366120",
    "end": "1372480"
  },
  {
    "text": "for example, in this\ndistilBERT paper, they enforce, as part\nof the objective, a cosine similarity\nbetween the output",
    "start": "1372480",
    "end": "1379080"
  },
  {
    "text": "states of teacher and student. And now you need to have\naccess to the model itself. And this will be\nmuch more expensive",
    "start": "1379080",
    "end": "1384900"
  },
  {
    "text": "because you need to\nrun the teacher as part of distillation. You could also think\nabout doing this with lots",
    "start": "1384900",
    "end": "1391210"
  },
  {
    "text": "of other hidden states. People have explored\nlots of other things. And you could even--\nthis is a paper that we did-- try\nto mimic them under",
    "start": "1391210",
    "end": "1398049"
  },
  {
    "text": "different counterfactuals, where\nyou change around the input representations of the\nteacher, observe the output,",
    "start": "1398050",
    "end": "1404080"
  },
  {
    "text": "and then try to get\nthe student to do that, to mimic very strange\nbehavior from the teacher.",
    "start": "1404080",
    "end": "1409179"
  },
  {
    "text": " And then there are a bunch\nof other things you can do. So standard\ndistillation is where",
    "start": "1409180",
    "end": "1415590"
  },
  {
    "text": "you have your big model frozen. And the teacher is being\nupdated by the process. You can have multi-teacher.",
    "start": "1415590",
    "end": "1421065"
  },
  {
    "text": "That's where there are\nlots of big models, maybe doing multiple tasks. And you try to distill them, all\nat once, down into a student.",
    "start": "1421065",
    "end": "1427110"
  },
  {
    "text": "That's a very\nexciting new frontier. Code distillation is where\nthey're trained jointly,",
    "start": "1427110",
    "end": "1433179"
  },
  {
    "text": "sometimes also called\nonline-distillation. That's where both the\nteacher and the student are learning together,\nsimultaneously.",
    "start": "1433180",
    "end": "1438990"
  },
  {
    "text": "It might be unnerving\nin the classroom, but effective for a model. And then self-distillation\nis actually",
    "start": "1438990",
    "end": "1444850"
  },
  {
    "text": "where you try to get usually\nlower parts of the model to be like other\nparts of the model,",
    "start": "1444850",
    "end": "1450100"
  },
  {
    "text": "by having them mimic themselves\nas part of the core model training. So that's a special case, I\nguess, of code distillation,",
    "start": "1450100",
    "end": "1457054"
  },
  {
    "text": "where there's only\none model and you're trying to distill parts\nof it into other parts. That's wild to think about.",
    "start": "1457055",
    "end": "1464660"
  },
  {
    "text": "And this has been\napplied in many domains. And the reason I can be\nencouraging about this is that as we get better\nand better at distillation,",
    "start": "1464660",
    "end": "1471580"
  },
  {
    "text": "we're finding that distilled\nmodels are as good or better than the teacher models, maybe\nfor a fraction of the cost.",
    "start": "1471580",
    "end": "1478990"
  },
  {
    "text": "And this is especially\nrelevant if the model is being used in production,\non a small device or something.",
    "start": "1478990",
    "end": "1484670"
  },
  {
    "text": "So here are just\nsome GLUE performance numbers that show,\nacross a bunch of these different papers,\nthat with distillation, you",
    "start": "1484670",
    "end": "1490750"
  },
  {
    "text": "can still get GLUE\nperformance, like the teacher, with a tiny model. ",
    "start": "1490750",
    "end": "1497899"
  },
  {
    "text": "Yeah? Something really\npuzzling to me is, how can a smaller-scale model\nbe able to mimic a teacher when",
    "start": "1497900",
    "end": "1505090"
  },
  {
    "text": "the training set is fixed? Couldn't you just\ntrain the simple model? Or is it just that\nthe teacher has access",
    "start": "1505090",
    "end": "1511210"
  },
  {
    "text": "to some point of learning that\nis easier to navigate through for a student, but not for\na student to get to know?",
    "start": "1511210",
    "end": "1521140"
  },
  {
    "text": "I think something like what\nyou just said has to be right. I actually don't-- so you're\nasking about the special case",
    "start": "1521140",
    "end": "1527440"
  },
  {
    "text": "where the teacher just does its\ninput-output thing and produces a data set that we train\nthe student on, right?",
    "start": "1527440",
    "end": "1533500"
  },
  {
    "text": "And you're asking, why\nis that better than just training the student\non your original data? It's very mysterious to me.",
    "start": "1533500",
    "end": "1541120"
  },
  {
    "text": "The best metaphor I\ncan give you is that it is a kind of a regularizer. So the teacher is doing\nsomething very complicated.",
    "start": "1541120",
    "end": "1547450"
  },
  {
    "text": "And even its mistakes are\nuseful for the student. I guess maybe a simple way\nthat I'm understanding is,",
    "start": "1547450",
    "end": "1554470"
  },
  {
    "text": "it's OK to make\ncertain mistakes. And the teacher's figured\nout which mistakes you can can make and not worry about.",
    "start": "1554470",
    "end": "1560440"
  },
  {
    "text": "I like that. That's a beautiful\nopening line of a paper. We need to make it\nsubstantive, by actually",
    "start": "1560440",
    "end": "1565945"
  },
  {
    "text": "explaining what that means. But I like it as a\nvision, for sure.",
    "start": "1565945",
    "end": "1571230"
  },
  {
    "text": "I want to be a little\ncareful of time. One more question and\nthen I'll just wrap up. Do we have some comparisons\nwhere the student is, in a way,",
    "start": "1571230",
    "end": "1578520"
  },
  {
    "text": "less general versus the teacher? Does it overfit the data, in a\nsense, more than the teacher?",
    "start": "1578520",
    "end": "1588510"
  },
  {
    "text": "The student? Yeah. I don't know. You would guess less. If it has a tiny capacity,\nit won't have as much",
    "start": "1588510",
    "end": "1594630"
  },
  {
    "text": "of a capacity to overfit\nthan the teacher. And maybe that's why\nin some situations,",
    "start": "1594630",
    "end": "1600370"
  },
  {
    "text": "the students outperform\nthe teachers. I hope that's\ninspiring to you all.",
    "start": "1600370",
    "end": "1605670"
  },
  {
    "text": "Let me wrap up here so you\ncan go on to outperform me. Architectures- I\ndidn't mention--",
    "start": "1605670",
    "end": "1611070"
  },
  {
    "text": "Transformer XL--\nwonderful, creative attempt to model long sequences,\nby essentially creating",
    "start": "1611070",
    "end": "1616890"
  },
  {
    "text": "a recurrent process across\ncached versions of earlier parts of the long document\nyou're processing.",
    "start": "1616890",
    "end": "1624419"
  },
  {
    "text": "XLNet-- this is a beautiful\nand creative attempt to use masked language\nmodeling-- sorry,",
    "start": "1624420",
    "end": "1630790"
  },
  {
    "text": "an auto-regressive language\nmodeling objective, but still have bidirectional context.",
    "start": "1630790",
    "end": "1636280"
  },
  {
    "text": "And they do this by creating\nall these permutation orders of the original sequence\nso that you can effectively",
    "start": "1636280",
    "end": "1641800"
  },
  {
    "text": "condition on the\nleft and the right, even though you can't\nlook into the future.",
    "start": "1641800",
    "end": "1647500"
  },
  {
    "text": "And then DeBERTa. This is really cool. I regret not fitting this in. DeBERTa is an attempt\nto separate out",
    "start": "1647500",
    "end": "1653800"
  },
  {
    "text": "the word and positional\nencodings for these models and make the word embeddings\nmore like first-class citizens.",
    "start": "1653800",
    "end": "1661000"
  },
  {
    "text": "And that's very intuitive\nfor me because that's like showing that we\nwant the model to learn some semantics for\nthese things that's",
    "start": "1661000",
    "end": "1667750"
  },
  {
    "text": "separate from their position. And they did that by\nreorganizing the attention mechanisms.",
    "start": "1667750",
    "end": "1673780"
  },
  {
    "text": "The known limitations--\nwe did a good job on these except for this final one. BERT assumes that the\npredicted tokens are all",
    "start": "1673780",
    "end": "1679960"
  },
  {
    "text": "independent of each other,\ngiven the unmasked tokens. I gave you that example of\nmasking \"New\" and \"York,\"",
    "start": "1679960",
    "end": "1685690"
  },
  {
    "text": "and it thinking\nthat both of those are independent of the other,\ngiven the surrounding context. ExcelNet, again, addresses that.",
    "start": "1685690",
    "end": "1692960"
  },
  {
    "text": "And that might be something\nthat you want to meditate on. Pre-training data-- here's\na whole mess of resources",
    "start": "1692960",
    "end": "1699547"
  },
  {
    "text": "if you did want to pre\ntrain your own model. Maybe Sid will talk\nmore about this. I'm offering these\nprimarily because I think",
    "start": "1699547",
    "end": "1705430"
  },
  {
    "text": "you might want to audit them. As you observe strange behavior\nfrom your large models, the data might be the\nkey to figuring out",
    "start": "1705430",
    "end": "1712389"
  },
  {
    "text": "where that behavior came from. And then finally,\ncurrent trends-- auto-regressive architectures\nseem to have taken over.",
    "start": "1712390",
    "end": "1719230"
  },
  {
    "text": "But that could be\njust because everyone is so focused on generation. I have an intuition\nthat models like BERT",
    "start": "1719230",
    "end": "1725799"
  },
  {
    "text": "are still better if you just\nwant to represent examples as opposed to doing generation.",
    "start": "1725800",
    "end": "1731220"
  },
  {
    "text": "seq2seq is still\na dominant choice for tasks with that structure,\nalthough again, point one might be pushing everyone\nto just use GPT-3 or 4,",
    "start": "1731220",
    "end": "1740550"
  },
  {
    "text": "even for models which\nto seq2seq structure. We'll see how that plays out. And then people are still\nobsessed with scaling up.",
    "start": "1740550",
    "end": "1746760"
  },
  {
    "text": "But we might be seeing a\ncounter movement towards smaller models, especially\nwith reinforcement",
    "start": "1746760",
    "end": "1751860"
  },
  {
    "text": "learning with human feedback. And that's something that we're\ngoing to talk about next week and the week after.",
    "start": "1751860",
    "end": "1758779"
  },
  {
    "text": "So I restructured a\nlittle bit of my talk. So we're only going\nto get to part",
    "start": "1758780",
    "end": "1763850"
  },
  {
    "text": "one today, which is\nactually back to basics, how transformers work. And then we're going to\ntalk about the other stuff.",
    "start": "1763850",
    "end": "1770570"
  },
  {
    "text": "I should maybe introduce myself. I'm Sid. I am a fourth-year PhD. I actually work primarily\non language for robotics.",
    "start": "1770570",
    "end": "1778400"
  },
  {
    "text": "And channeling one of the\ncore concepts of the class, it's all about doing a whole\nlot with very, very little.",
    "start": "1778400",
    "end": "1784640"
  },
  {
    "text": "I'm really just\nworking on how we get robots to follow\ninstructions, given just one",
    "start": "1784640",
    "end": "1790429"
  },
  {
    "text": "example of a human opening\na fridge or pouring coffee, things like that.",
    "start": "1790430",
    "end": "1797190"
  },
  {
    "text": "But in doing that research,\nit became really, really clear that we needed\nbetter raw materials, better starting points.",
    "start": "1797190",
    "end": "1803100"
  },
  {
    "text": "So I started working on\npretraining, first in language, and then more recently,\nin vision video",
    "start": "1803100",
    "end": "1808910"
  },
  {
    "text": "and robotics with language\nas the central theme. So I want to talk today about\nfantastic language models",
    "start": "1808910",
    "end": "1815720"
  },
  {
    "text": "and how to build them. So Richard Feynman is\nprobably not only one of the greatest\nphysicists of all time.",
    "start": "1815720",
    "end": "1821760"
  },
  {
    "text": "But he's one of the\ngreatest educators of all time, one of the\ngreatest science educators.",
    "start": "1821760",
    "end": "1826889"
  },
  {
    "text": "And he has this quote,\n\"what I cannot create, I do not understand.\"",
    "start": "1826890",
    "end": "1831940"
  },
  {
    "text": "And for him, it was really\njust about building blocks. How do I understand what is\ngoing on at the lowest level",
    "start": "1831940",
    "end": "1837190"
  },
  {
    "text": "so I can compose them together\nand figure out what to do next? Where does the next innovation--\nwhere does the next discovery",
    "start": "1837190",
    "end": "1842460"
  },
  {
    "text": "come from? And so kind of\nwith that in mind, I actually just want to spend\nthe next 12 or so minutes",
    "start": "1842460",
    "end": "1848789"
  },
  {
    "text": "talking about building language\nmodels, building transformers, and how that all happened. So it's a practical take on\nthese large-scale language",
    "start": "1848790",
    "end": "1856870"
  },
  {
    "text": "models. We're probably not going to\nget to the large-scale bit. And we're going to get\nto the full pipeline. Again, we're only going to\nfocus on the model architecture",
    "start": "1856870",
    "end": "1863717"
  },
  {
    "text": "today, the evolution of the\ntransformer, how we got there. Training at scale, we'll\nprobably cover some other time.",
    "start": "1863717",
    "end": "1870810"
  },
  {
    "text": "And then we'll talk about, very,\nvery briefly, efficient fine tuning and inference. And we have some other great TAs\nwho might actually be talking",
    "start": "1870810",
    "end": "1876963"
  },
  {
    "text": "about this more in depth. But the punch line is,\nthe last few years--",
    "start": "1876963",
    "end": "1882059"
  },
  {
    "text": "I started my PhD in 2019. I trained my first deep\nlearning MNIST model in 2018.",
    "start": "1882060",
    "end": "1889799"
  },
  {
    "text": "Field's changed\na lot since then. And with every new model,\nwith every new GPT--",
    "start": "1889800",
    "end": "1895320"
  },
  {
    "text": "1, 2, 3, 4-- the 5 that's\ntraining right now-- there's been more and\nmore folk knowledge,",
    "start": "1895320",
    "end": "1901320"
  },
  {
    "text": "things that are hidden\nfrom plain sight, that we never get to see. And it's been the job\nof people like us,",
    "start": "1901320",
    "end": "1908220"
  },
  {
    "text": "students, people in academia,\nto rediscover find the insights, find the intuition\nbehind these ideas.",
    "start": "1908220",
    "end": "1914580"
  },
  {
    "text": "And rediscovering\nthose pipelines, it's actually our comparative\nadvantage in figuring out,",
    "start": "1914580",
    "end": "1920950"
  },
  {
    "text": "OK, so these are how\nthese pieces came to be. What do I do next? And so I don't really\ncare about time.",
    "start": "1920950",
    "end": "1928230"
  },
  {
    "text": "If we can get\nthrough five slides, that is a success for me. But be selfish. This is your class.",
    "start": "1928230",
    "end": "1934030"
  },
  {
    "text": "So if you have any\nquestions, if I say anything you\ndon't understand, that's the contract. Call me out. Ask a question.",
    "start": "1934030",
    "end": "1939300"
  },
  {
    "text": "We're just going\nto go step by step. So how did we get\nto the transformer?",
    "start": "1939300",
    "end": "1945870"
  },
  {
    "text": "How did this become the bedrock\nof language modeling and now vision, also video,\nalso robotics",
    "start": "1945870",
    "end": "1952800"
  },
  {
    "text": "for some reason as of late? How did we get here? So what is the recipe for\na good language model?",
    "start": "1952800",
    "end": "1959399"
  },
  {
    "text": "We've talked a bit about\ncontextual representations. Chris was talking through\nthe various different phase changes in language-modeling\nhistory, at least",
    "start": "1959400",
    "end": "1966030"
  },
  {
    "text": "with segment-diffusion language\nmodels, which is a completely different perspective. I'm going to simplify\nthings-- oversimplify",
    "start": "1966030",
    "end": "1972690"
  },
  {
    "text": "things-- to two steps. I need massive amounts of cheap,\neasy-to-acquire data, where language modeling,\nwe're building",
    "start": "1972690",
    "end": "1978330"
  },
  {
    "text": "these contextual\nrepresentations because we want to learn patterns. We want to learn truths about\nthe world from data at scale.",
    "start": "1978330",
    "end": "1985290"
  },
  {
    "text": "And to do that at scale,\nwe need data at scale. So that's one component.",
    "start": "1985290",
    "end": "1990299"
  },
  {
    "text": "And the other is we need a\nsimple and high-throughput way to consume it. So what does that mean?",
    "start": "1990300",
    "end": "1997770"
  },
  {
    "text": "We need to be able to chew\nthrough all of this data as fast as we possibly can,\nin the least-opinionated way,",
    "start": "1997770",
    "end": "2005580"
  },
  {
    "text": "to figure out all of the\npossible patterns, all of the possible things\nthat could be useful for people, fine tuning,\ngenerating, using",
    "start": "2005580",
    "end": "2012630"
  },
  {
    "text": "these models for arbitrary\nthings, downstream. This isn't just\napplicable for language. It's applicable to\npretty much everything.",
    "start": "2012630",
    "end": "2018700"
  },
  {
    "text": "So vision does this. Video does this. Video and language, vision\nand language, language",
    "start": "2018700",
    "end": "2024840"
  },
  {
    "text": "and robotics-- all of them\nfollow a similar strategy. So simple, in that\nit's natural to scale",
    "start": "2024840",
    "end": "2031470"
  },
  {
    "text": "the approach with data. As we get go from 300 billion\nto 600 billion tokens, we can maybe make\nthe model bigger,",
    "start": "2031470",
    "end": "2037679"
  },
  {
    "text": "to handle that in a\npretty simple way. The model should be\ncomposable and general.",
    "start": "2037680",
    "end": "2042870"
  },
  {
    "text": "The training-- the way that\nwe actually ingest this data, it should be fast\nand parallelizable.",
    "start": "2042870",
    "end": "2047940"
  },
  {
    "text": "And we should be making\nthe most of our hardware. If we're going to run\na data center with-- I don't know-- 512 GPUs,\nwith each 8 GPU box costing",
    "start": "2047940",
    "end": "2056408"
  },
  {
    "text": "$120,000, I better be\ngetting my money's worth at the end of the day. And the consumption part,\nright, this minimal assumptions",
    "start": "2056409",
    "end": "2064320"
  },
  {
    "text": "on relationships, the\nless opinionated I am about how different parts\nof my data is connected,",
    "start": "2064320",
    "end": "2070440"
  },
  {
    "text": "the more I can learn, given the\nfirst thing, massive amounts of data at scale.",
    "start": "2070440",
    "end": "2076869"
  },
  {
    "text": "So to figure out how we\ngot to the transformer, I want to wind time\nback, to what Chris was",
    "start": "2076870",
    "end": "2082800"
  },
  {
    "text": "alluding to earlier, with RNNs. So this is an RNN model--",
    "start": "2082800",
    "end": "2088230"
  },
  {
    "text": "kind of complicated. But it's from 224n. I took it, literally,\nfrom their slides.",
    "start": "2088230",
    "end": "2093419"
  },
  {
    "text": "I hope John doesn't\nget mad at me. And it's this very powerful\nclass of model in theory.",
    "start": "2093420",
    "end": "2100800"
  },
  {
    "text": "I am ingesting arbitrary-length\nsequences left to right. And I'm learning arbitrary\npatterns around them.",
    "start": "2100800",
    "end": "2107670"
  },
  {
    "text": "People decided, later on, to\nadd these attention mechanisms on top of these\nsequence-to-sequence RNN models",
    "start": "2107670",
    "end": "2114840"
  },
  {
    "text": "to figure out how to\nsharpen their focus as they are decoding, token by token.",
    "start": "2114840",
    "end": "2119882"
  },
  {
    "text": "So the strengths are\nlike I get to handle arbitrary long contexts. And we see kind of the\nfirst semblance of attention",
    "start": "2119882",
    "end": "2125760"
  },
  {
    "text": "appear kind of very\nmotivated by like the way we do language translation. When I'm translating\nword by word,",
    "start": "2125760",
    "end": "2131550"
  },
  {
    "text": "there are certain\nwords in the input that are going to\nmatter, that I'm going to sharpen my focus to. But there are issues with RNNs.",
    "start": "2131550",
    "end": "2137859"
  },
  {
    "text": "They're not the most scalable. Producing the next\ntoken requires me to produce every\nsingle token beforehand.",
    "start": "2137860",
    "end": "2144130"
  },
  {
    "text": "I can't really make them deeper\nwithout training stability going to pieces.",
    "start": "2144130",
    "end": "2149170"
  },
  {
    "text": "So that's rough. And so chewing through a large\namount of data with an RNN is hard.",
    "start": "2149170",
    "end": "2155329"
  },
  {
    "text": "Some people refuse\nto believe that. And they've actually\ndone immense work in trying to scale up RNNS,\nmake them more parallelizable,",
    "start": "2155330",
    "end": "2161290"
  },
  {
    "text": "using lots and lots of really\ncool linear algebra tricks. I'll post some links. And then separately,\nfrom the vision community",
    "start": "2161290",
    "end": "2169240"
  },
  {
    "text": "that led into the\nlanguage community, we have convolutional\nneural networks. And this is from a other course,\nfrom Lena Voita about using",
    "start": "2169240",
    "end": "2179560"
  },
  {
    "text": "CNNs for language modeling. And the idea here is\nwe have this ability",
    "start": "2179560",
    "end": "2185200"
  },
  {
    "text": "to do immense, deep,\nparallelizable training by taking these little windows.",
    "start": "2185200",
    "end": "2191892"
  },
  {
    "text": "I'm going to look at--\neach layer is only going to look at the three\nword contexts at a time and going to give\nme representation.",
    "start": "2191892",
    "end": "2197420"
  },
  {
    "text": "But if I stack\nthis enough times, and I have these\nresidual connections that combine earlier inputs\nwith the later inputs,",
    "start": "2197420",
    "end": "2204250"
  },
  {
    "text": "by the time I'm\nlike 10 layers deep, I've seen everything\nin the window. But I need that depth. And that's kind of a drawback.",
    "start": "2204250",
    "end": "2212110"
  },
  {
    "text": "But they're really cool,\npowerful ideas here. And I'd actually say that the\ntransformers have way more to do with CNNs and the way that\nthey behave than the way RNNs",
    "start": "2212110",
    "end": "2219580"
  },
  {
    "text": "behave, right? So we have this\nidea of a CNN layer kind of having multiple\nfilters, multiple kernels,",
    "start": "2219580",
    "end": "2226360"
  },
  {
    "text": "different ways of looking\nat and extracting features from an image or\nfeatures from text.",
    "start": "2226360",
    "end": "2232119"
  },
  {
    "text": "You have this ability\nto kind of scale depth using these residual\nconnections.",
    "start": "2232120",
    "end": "2237580"
  },
  {
    "text": "The deepest networks that we\nhad from 2012, 2015, even now, are still vision\nmodels ResNet151",
    "start": "2237580",
    "end": "2245470"
  },
  {
    "text": "isn't called 151 because\nit's the 151st edition of the ResNet. It's 151 because\nit's 151 layers deep.",
    "start": "2245470",
    "end": "2252070"
  },
  {
    "text": "It's actually 151 blocks deep-- layers-- it's actually\nprobably 4x that.",
    "start": "2252070",
    "end": "2258119"
  },
  {
    "text": "And it's parallelizable, right? Every little window that\nI see, at every layer, can be computed\ncompletely independently",
    "start": "2258120",
    "end": "2264360"
  },
  {
    "text": "of every other layer,\nwhich is really, really great for a modern\nhardware, modern GPUs. ",
    "start": "2264360",
    "end": "2271210"
  },
  {
    "text": "So looking at this, it\nseems like CNNs are cool. It seems like RNNs are cool.",
    "start": "2271210",
    "end": "2277210"
  },
  {
    "text": "There's a natural question,\nwhich is, how do you do better? This is the picture from Chris's\nslides, that Lisa also used.",
    "start": "2277210",
    "end": "2285170"
  },
  {
    "text": "This is a very scary-looking\npicture, right? What does self-attention mean? Where do those ideas come from?",
    "start": "2285170",
    "end": "2292420"
  },
  {
    "text": "So one idea, one key component,\nthe one missing component",
    "start": "2292420",
    "end": "2297799"
  },
  {
    "text": "for how you get from\na CNN to an RNN, and an RNN to a\ntransformer, is the idea",
    "start": "2297800",
    "end": "2303140"
  },
  {
    "text": "that each individual token is\nits own query, key, and value. It's its own entity\nthat can be used",
    "start": "2303140",
    "end": "2309800"
  },
  {
    "text": "to shape the\nrepresentations of all of the other tokens, all right? So I'm going to turn this word,\n\"the,\" into its own query,",
    "start": "2309800",
    "end": "2316050"
  },
  {
    "text": "key, and value. I'm going to use the\nattention from the RNNs. And I'm going to use the\ndepth-parallelizability scaling",
    "start": "2316050",
    "end": "2322610"
  },
  {
    "text": "from the CNNs. And then the multi-headed\npart of self-attention is exactly like what the\ndifferent convolutional",
    "start": "2322610",
    "end": "2330380"
  },
  {
    "text": "filters, the different\nkernels, are doing in a CNN. It's giving you\ndifferent perspectives on that same token, different\nways to come up with queries,",
    "start": "2330380",
    "end": "2337610"
  },
  {
    "text": "different insights into how\nwe can use a single token and come up with multiple\ndifferent representations,",
    "start": "2337610",
    "end": "2343730"
  },
  {
    "text": "and fuse them together. This is a code. Code is semi unimportant.",
    "start": "2343730",
    "end": "2349470"
  },
  {
    "text": "It's a very terse\ndescription of what multi-head self-attention\nlooks like. The key parts here\nare this little bit,",
    "start": "2349470",
    "end": "2357110"
  },
  {
    "text": "where we project an\ninput sequence of tokens to the queries,\nkeys, and values. And then we're just going to\nrearrange them in some way.",
    "start": "2357110",
    "end": "2365310"
  },
  {
    "text": "And the important part here is\nthat we are rearranging them in a way that splits them\ninto these different views,",
    "start": "2365310",
    "end": "2370580"
  },
  {
    "text": "these different heads,\nwhere each head has some fixed dimension,\nwhich is the key dimension of the transformer.",
    "start": "2370580",
    "end": "2377118"
  },
  {
    "text": "And then we have these\nqueries, keys, and values, that we're then going to use for\nthis attention operation, which",
    "start": "2377118",
    "end": "2382849"
  },
  {
    "text": "comes directly from\nthe RNN literature. It is really just\nthis dot product between queries and keys.",
    "start": "2382850",
    "end": "2388760"
  },
  {
    "text": "That is a very\ncomplicated way of saying that's a matrix multiply. And then we're going to just\nproject them and combine them",
    "start": "2388760",
    "end": "2397280"
  },
  {
    "text": "back into our tensor of batch\nsize, by sequence length, by embedding dimension.",
    "start": "2397280",
    "end": "2403620"
  },
  {
    "text": "There's a nice\nsubtlety here, I think, that caught me off\nguard at one point. When you look at these models--",
    "start": "2403620",
    "end": "2410339"
  },
  {
    "text": "you download BERT. And it says multi-headed\nattention or whatever. There's only one set of weights,\neven though it's multi-headed.",
    "start": "2410340",
    "end": "2418170"
  },
  {
    "text": "Do you want to unpack\nthat for us a little? Yeah, so the\nconvolutional kernel",
    "start": "2418170",
    "end": "2424290"
  },
  {
    "text": "has this really nice\nway of expressing-- I have multiple resolutions\nof an image, right?",
    "start": "2424290",
    "end": "2429480"
  },
  {
    "text": "It's like that depth channel\nof a convolutional filter. And if you can unpack the\nConv2d layer in PyTorch,",
    "start": "2429480",
    "end": "2436590"
  },
  {
    "text": "you see that come out. You don't really see that here. You see just like this one, big,\nweight matrix that is literally",
    "start": "2436590",
    "end": "2442710"
  },
  {
    "text": "this dimensionality-- embed dimension by three times. Embed dimension is usually\nwhat it is in BERT or GPT.",
    "start": "2442710",
    "end": "2449099"
  },
  {
    "text": "That \"three\" is the\nway you split it into queries, keys, and values.",
    "start": "2449100",
    "end": "2454829"
  },
  {
    "text": "But we're actually going\nto take that vector, that is embedding size,\nand just chunk it up",
    "start": "2454830",
    "end": "2461430"
  },
  {
    "text": "into each of these\ndifferent filters, right? So rather than make\nthose filters explicit, as by providing them as\na parameter that defines",
    "start": "2461430",
    "end": "2468280"
  },
  {
    "text": "some weight layer, for\nefficiency purposes, we're actually just going to\ntreat it all as one matrix and then just chunk it up as\nwe're doing the linear algebra",
    "start": "2468280",
    "end": "2476440"
  },
  {
    "text": "operations. Does that make sense? So you're chunking it twice. The times 3 is queries,\nkeys, and values.",
    "start": "2476440",
    "end": "2482320"
  },
  {
    "text": "And then number of\nheads is further-- is one of them. Yeah, so one of the key rules\nthat no one ever tells you",
    "start": "2482320",
    "end": "2488890"
  },
  {
    "text": "about transformers is\nthat your number of heads has to evenly divide your\ntransformer hidden dimension.",
    "start": "2488890",
    "end": "2495475"
  },
  {
    "text": "And that's usually a\ncheck that is explicitly done in the code for\ntraining BERT or GPT.",
    "start": "2495475",
    "end": "2500984"
  },
  {
    "text": "And usually the code doesn't\nwork if that doesn't happen. And that's kind of\nhow you get away with a lot of these\nefficiency tricks.",
    "start": "2500985",
    "end": "2506103"
  },
  {
    "text": "It's a great question. So we should do a whole\ncourse on broadcasting before you even start\nthis, so that you",
    "start": "2506103",
    "end": "2511450"
  },
  {
    "text": "can do this mess of things. Yeah, and there's\na great professor at Cornell Tech, Sasha Rush,\nwho has a bunch of tutorials",
    "start": "2511450",
    "end": "2517990"
  },
  {
    "text": "on just basic broadcasting\ntensor operations-- it's fantastic-- that\nyou should check out. Is there a question?",
    "start": "2517990",
    "end": "2523119"
  },
  {
    "text": "Yeah, could you just clarify\nwhat you mean by chunking? Yeah, so if I have a vector\nof, let's say, length 1,024,",
    "start": "2523120",
    "end": "2530110"
  },
  {
    "text": "that is my embedding dimensions,\nthe hidden dimensions for my transformer. And if I have keys\nof say, dimension--",
    "start": "2530110",
    "end": "2537590"
  },
  {
    "text": "let's say 2, make it easy-- chunking just means\nthat I'm going to split that vector of\n1,024 into two heads,",
    "start": "2537590",
    "end": "2543880"
  },
  {
    "text": "each of dimension 512, right? So I'm literally just going\nto reshape that vector",
    "start": "2543880",
    "end": "2549090"
  },
  {
    "text": "and chunk them up into two\nviews of the same input. ",
    "start": "2549090",
    "end": "2554530"
  },
  {
    "text": "Cool. Is this actually better? Is this, alone, enough\nto define a transformer?",
    "start": "2554530",
    "end": "2562480"
  },
  {
    "text": "Maybe not. All right. The answer's no. So it's good because we get\nall of the parallelization",
    "start": "2562480",
    "end": "2569682"
  },
  {
    "text": "advantages and all of\nthe attention advantages that I talked about\nin the previous slide. This is a slide from-- Justin Johnson and Danfei Xu\nwere both ex-Stanford alums,",
    "start": "2569683",
    "end": "2577839"
  },
  {
    "text": "now teaching courses\nabout transformers and deep learning\nat various colleges.",
    "start": "2577840",
    "end": "2583720"
  },
  {
    "text": "But you're missing kind of\none key component, right? So if you just look\nat this and squint",
    "start": "2583720",
    "end": "2591190"
  },
  {
    "text": "at this for a little bit of\ntime, what you're missing is-- OK, so I am just taking\ndifferent weighted averages",
    "start": "2591190",
    "end": "2598150"
  },
  {
    "text": "of the same underlying\nvalues, over and over again.",
    "start": "2598150",
    "end": "2604727"
  },
  {
    "text": "Relative to the things that are\ncoming out of my transformer, there actually is\nno non-linearity with just self-attention.",
    "start": "2604728",
    "end": "2611470"
  },
  {
    "text": "This is basically a\nglorified linear network. So we need some way to\nfix that because that's",
    "start": "2611470",
    "end": "2619230"
  },
  {
    "text": "where the expressivity, the\nkind of magic of deep learning, happens. It's when we stack\nthese non-linearities,",
    "start": "2619230",
    "end": "2624450"
  },
  {
    "text": "go deeper, and learn\nnew patterns at scale. So this is how we do it.",
    "start": "2624450",
    "end": "2632110"
  },
  {
    "text": "We add an MLP. We add an MLP to the very\nend of the transformer block. And it's very simple, right? And all it does is, it takes\nthe embedding dimension that",
    "start": "2632110",
    "end": "2639359"
  },
  {
    "text": "comes out of a\nself-attention block, that we just\ndefined, projects it to a higher-dimensional space,\nadds a reLU non-linearity,",
    "start": "2639360",
    "end": "2648359"
  },
  {
    "text": "and then down projects it back\nto the embedding dimension. Usually what you're going\nto see is a factor of 4. Why is it a factor of 4?",
    "start": "2648360",
    "end": "2654180"
  },
  {
    "text": "No one knows, is\nthe honest answer. 2 didn't seem to\nwork well enough. 8 seemed to be too big.",
    "start": "2654180",
    "end": "2660180"
  },
  {
    "text": "But here's some\nlike soft intuition for why this might work. And this is a throwback\nto like 229 OG ML days.",
    "start": "2660180",
    "end": "2670310"
  },
  {
    "text": "So you want your\nnetwork, as a whole, to be able to kind of both\nforget the things that",
    "start": "2670310",
    "end": "2676900"
  },
  {
    "text": "are unimportant but also\nremember the things that are important, right? That's the rule. So the sharpening,\nthe remembering,",
    "start": "2676900",
    "end": "2681940"
  },
  {
    "text": "the things that are important,\nare these residual connections, the fact that I'm adding\nx to some transform of x.",
    "start": "2681940",
    "end": "2687430"
  },
  {
    "text": "The forgetting is what\nthis MLP is doing. It's basically\nsaying, what stuff can I throw Away And should I\nbasically forget because it's",
    "start": "2687430",
    "end": "2694722"
  },
  {
    "text": "not really relevant\nto what I care about, at the end of the day,\nwhich are good contextual representations. And so the role\nof the MLP is very",
    "start": "2694722",
    "end": "2701140"
  },
  {
    "text": "similar to the role of a kernel\nin the good old support-vector",
    "start": "2701140",
    "end": "2706450"
  },
  {
    "text": "machine literature. So if I have two classes\nthat are kind of like this, in a plane, and I want to draw\na line that partitions them,",
    "start": "2706450",
    "end": "2714290"
  },
  {
    "text": "how do I do it? Well, it's hard if I'm\nonly working in 2D. But with just a very\nsimple learn transform,",
    "start": "2714290",
    "end": "2721300"
  },
  {
    "text": "if I just implicitly lift\nthese things up to 3D, I can turn this into\na surface, in 3D,",
    "start": "2721300",
    "end": "2727790"
  },
  {
    "text": "that I can just cut in\nhalf, separates my stuff. So projecting up with\nthis MLP is basically this way of aligning\nor crystallizing",
    "start": "2727790",
    "end": "2734660"
  },
  {
    "text": "the structure of our features,\nlearning a good decision boundary in space, and\ncompressing from there.",
    "start": "2734660",
    "end": "2740817"
  },
  {
    "text": "I think we're out of time. So we're going to go through\nthe rest of the transformer evolution in a bit. But all the slides are up.",
    "start": "2740817",
    "end": "2746532"
  },
  {
    "text": "I have office hours tomorrow. And I will be back. Thanks. [APPLAUSE]",
    "start": "2746532",
    "end": "2753200"
  },
  {
    "start": "2753200",
    "end": "2757000"
  }
]