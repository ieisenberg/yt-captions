[
  {
    "start": "0",
    "end": "116000"
  },
  {
    "start": "0",
    "end": "6080"
  },
  {
    "text": "Hi, in this module,\nI'm going to talk",
    "start": "6080",
    "end": "7670"
  },
  {
    "text": "about Gibbs sampling, a simple\nalgorithm for approximately",
    "start": "7670",
    "end": "10820"
  },
  {
    "text": "computing marginal\nprobabilities and [AUDIO OUT]..",
    "start": "10820",
    "end": "15309"
  },
  {
    "text": "You'll recall that\na Markov network",
    "start": "15310",
    "end": "17080"
  },
  {
    "text": "is based on a factor graph.",
    "start": "17080",
    "end": "19930"
  },
  {
    "text": "And a factor graph\ngives a weight",
    "start": "19930",
    "end": "24600"
  },
  {
    "text": "to every possible assignment of\nvariables in that factor graph.",
    "start": "24600",
    "end": "27930"
  },
  {
    "text": "And in a Markov network,\nwe'll convert that weight",
    "start": "27930",
    "end": "30450"
  },
  {
    "text": "into a probability\nby first computing",
    "start": "30450",
    "end": "33360"
  },
  {
    "text": "the normalization constant.",
    "start": "33360",
    "end": "35200"
  },
  {
    "text": "Which is sum over all the\nassignments of the weight",
    "start": "35200",
    "end": "39120"
  },
  {
    "text": "of that assignment.",
    "start": "39120",
    "end": "40649"
  },
  {
    "text": "Divide by that\nnormalization constant,",
    "start": "40650",
    "end": "42630"
  },
  {
    "text": "and we get the probability\nof assignment little x.",
    "start": "42630",
    "end": "49650"
  },
  {
    "text": "So in this object\ntracking example,",
    "start": "49650",
    "end": "52110"
  },
  {
    "text": "you see how we have a bunch\nof different assignments.",
    "start": "52110",
    "end": "56130"
  },
  {
    "text": "There are weights.",
    "start": "56130",
    "end": "57480"
  },
  {
    "text": "The partition function\nin this case is 26.",
    "start": "57480",
    "end": "59760"
  },
  {
    "text": "We divide each of\nthese weights by 26.",
    "start": "59760",
    "end": "62010"
  },
  {
    "text": "And we get these probabilities.",
    "start": "62010",
    "end": "66307"
  },
  {
    "text": "So the cool thing\nwith Markov networks",
    "start": "66307",
    "end": "67890"
  },
  {
    "text": "is that you can compute\nmarginal probability.",
    "start": "67890",
    "end": "71360"
  },
  {
    "text": "And that's going\nto be our focus.",
    "start": "71360",
    "end": "73550"
  },
  {
    "text": "So marginal probability\nis going to be focusing",
    "start": "73550",
    "end": "77420"
  },
  {
    "text": "on one particular variable Xi.",
    "start": "77420",
    "end": "79280"
  },
  {
    "text": "And asking what values\ncould it take on?",
    "start": "79280",
    "end": "82700"
  },
  {
    "text": "And to get that,\nwe're going to sum",
    "start": "82700",
    "end": "84990"
  },
  {
    "text": "over all possible assignments\nwhere Xi does actually",
    "start": "84990",
    "end": "88790"
  },
  {
    "text": "equal P, the joint probability\nof that assignment.",
    "start": "88790",
    "end": "93710"
  },
  {
    "text": "And this example,\nif you look and ask",
    "start": "93710",
    "end": "97670"
  },
  {
    "text": "for the probability\nof x2 equals 1.",
    "start": "97670",
    "end": "100460"
  },
  {
    "text": "You sum over all\nthe rows where x2",
    "start": "100460",
    "end": "102470"
  },
  {
    "text": "is equal to 1, that\ngives you 0.62.",
    "start": "102470",
    "end": "106580"
  },
  {
    "text": "And if you ask for x2 equals 2.",
    "start": "106580",
    "end": "110780"
  },
  {
    "text": "Then you're summing\nover the last two rows",
    "start": "110780",
    "end": "112909"
  },
  {
    "text": "and that gives [AUDIO OUT].",
    "start": "112910",
    "end": "114334"
  },
  {
    "start": "114334",
    "end": "116990"
  },
  {
    "start": "116000",
    "end": "568000"
  },
  {
    "text": "So now let me present\nGibbs sampling,",
    "start": "116990",
    "end": "120470"
  },
  {
    "text": "just a simple algorithm\nfor approximately computing",
    "start": "120470",
    "end": "123290"
  },
  {
    "text": "these marginals.",
    "start": "123290",
    "end": "124160"
  },
  {
    "text": "You could iterate over\nall possible assignments",
    "start": "124160",
    "end": "126980"
  },
  {
    "text": "and compute, but that would\ntake exponential time.",
    "start": "126980",
    "end": "130350"
  },
  {
    "text": "So Gibbs sampling\nis going to follow",
    "start": "130350",
    "end": "132110"
  },
  {
    "text": "the template of local search.",
    "start": "132110",
    "end": "133910"
  },
  {
    "text": "Where we're going to go through\neach variable one at a time",
    "start": "133910",
    "end": "136610"
  },
  {
    "text": "and update them.",
    "start": "136610",
    "end": "138050"
  },
  {
    "text": "But unlike iterated conditional\nmodes, which we saw before.",
    "start": "138050",
    "end": "141680"
  },
  {
    "text": "Gibbs sampling is a\nrandomized algorithm",
    "start": "141680",
    "end": "144019"
  },
  {
    "text": "tailored for the purpose\nof computing a marginal.",
    "start": "144020",
    "end": "148020"
  },
  {
    "text": "So let's present the algorithm.",
    "start": "148020",
    "end": "149740"
  },
  {
    "text": "So, we're going to\ninitialize the assignment",
    "start": "149740",
    "end": "152770"
  },
  {
    "text": "to some completely\nrandom assignment.",
    "start": "152770",
    "end": "156743"
  },
  {
    "text": "And then we're going to loop\nthrough each of the variables",
    "start": "156743",
    "end": "159160"
  },
  {
    "text": "until convergence, which I'll\ntalk about a little bit later.",
    "start": "159160",
    "end": "163150"
  },
  {
    "text": "We're going to set the\nassignment Xi equals v",
    "start": "163150",
    "end": "169599"
  },
  {
    "text": "with this probability,",
    "start": "169600",
    "end": "171560"
  },
  {
    "text": "The probability of Xi\nequals v, given x minus i",
    "start": "171560",
    "end": "176840"
  },
  {
    "text": "equals x minus i.",
    "start": "176840",
    "end": "179379"
  },
  {
    "text": "So this my x minus\ni notation, just",
    "start": "179380",
    "end": "181900"
  },
  {
    "text": "refers to all the\nvariables except for Xi.",
    "start": "181900",
    "end": "186459"
  },
  {
    "text": "So I'll come back\nto this in a second.",
    "start": "186460",
    "end": "188300"
  },
  {
    "text": "But let me just highlight\nkind of the general flow",
    "start": "188300",
    "end": "191110"
  },
  {
    "text": "of the algorithm.",
    "start": "191110",
    "end": "192220"
  },
  {
    "text": "So suppose you have\nthree variables.",
    "start": "192220",
    "end": "194230"
  },
  {
    "text": "Gibbs sampling is going\nto provide a sample x1,",
    "start": "194230",
    "end": "197680"
  },
  {
    "text": "holding the other ones fixed.",
    "start": "197680",
    "end": "199629"
  },
  {
    "text": "And now it's going to move on\nto x2, holding the others fixed.",
    "start": "199630",
    "end": "203020"
  },
  {
    "text": "And update x2,\nand then go to x3,",
    "start": "203020",
    "end": "205870"
  },
  {
    "text": "and then it's going to cycle\nback to x1, x2, x3 and so on.",
    "start": "205870",
    "end": "211330"
  },
  {
    "text": "So now how do I\nsample Xi equals v?",
    "start": "211330",
    "end": "216420"
  },
  {
    "text": "So here is one example.",
    "start": "216420",
    "end": "221170"
  },
  {
    "text": "What we're going to\ndo, is we're going",
    "start": "221170",
    "end": "223440"
  },
  {
    "text": "to try assigning Xi equals\nv, and getting some weight.",
    "start": "223440",
    "end": "233350"
  },
  {
    "text": "So for every possible\nassignment of x2,",
    "start": "233350",
    "end": "236850"
  },
  {
    "text": "I'm going to get some weight.",
    "start": "236850",
    "end": "238950"
  },
  {
    "text": "And now remember in\nICM, I would just",
    "start": "238950",
    "end": "241110"
  },
  {
    "text": "simply take the value that\nproduced the largest weight.",
    "start": "241110",
    "end": "245040"
  },
  {
    "text": "But the main difference\nwith Gibbs sampling",
    "start": "245040",
    "end": "247019"
  },
  {
    "text": "is that I'm going to\ntake these weights,",
    "start": "247020",
    "end": "249120"
  },
  {
    "text": "and I'm going to normalize\nthem to produce a probability.",
    "start": "249120",
    "end": "252840"
  },
  {
    "text": "Again, normalizing is summing\nthese values, so I get 5.",
    "start": "252840",
    "end": "257820"
  },
  {
    "text": "And dividing by 5 to get\nprobability 0.2, 0.4, 0.4.",
    "start": "257820",
    "end": "263260"
  },
  {
    "text": "And now I'm going to sample\nx2 equals one of these values,",
    "start": "263260",
    "end": "266950"
  },
  {
    "text": "according to this\nprobability distribution.",
    "start": "266950",
    "end": "270240"
  },
  {
    "text": "You can visualize\nthat sampling process",
    "start": "270240",
    "end": "272660"
  },
  {
    "text": "by the interval from 0 to 1.",
    "start": "272660",
    "end": "276800"
  },
  {
    "text": "Where I have a\nnumber of segments",
    "start": "276800",
    "end": "279259"
  },
  {
    "text": "representing the different\npossible values of x2.",
    "start": "279260",
    "end": "283160"
  },
  {
    "text": "And the length is\nexactly the probability.",
    "start": "283160",
    "end": "286380"
  },
  {
    "text": "So probability of x2 equal to\n0, probability of x2 equals 1,",
    "start": "286380",
    "end": "291770"
  },
  {
    "text": "and probability\nof x2 equal to 2.",
    "start": "291770",
    "end": "294604"
  },
  {
    "text": "And then I'm going to\nthrow a one dimensional",
    "start": "294605",
    "end": "296480"
  },
  {
    "text": "dart at this line.",
    "start": "296480",
    "end": "298970"
  },
  {
    "text": "I'm going to hit it\nsomewhere and I'm",
    "start": "298970",
    "end": "300620"
  },
  {
    "text": "going to take whatever value\nis specified by that interval.",
    "start": "300620",
    "end": "307260"
  },
  {
    "text": "OK, so now I have a\nnew value for x2 here.",
    "start": "307260",
    "end": "312720"
  },
  {
    "text": "And now I proceed\nto the next variable",
    "start": "312720",
    "end": "314910"
  },
  {
    "text": "and so on and so forth.",
    "start": "314910",
    "end": "316920"
  },
  {
    "start": "316920",
    "end": "320120"
  },
  {
    "text": "So that produces a sequence\nof samples of the assignments.",
    "start": "320120",
    "end": "327470"
  },
  {
    "text": "And the remaining thing to\ndo is to aggregate them.",
    "start": "327470",
    "end": "332680"
  },
  {
    "text": "So, I'm going to every time\nI go through this loop,",
    "start": "332680",
    "end": "336880"
  },
  {
    "text": "I'm going to increment\na counter or variable",
    "start": "336880",
    "end": "340150"
  },
  {
    "text": "i of the particular\nvalue that I saw.",
    "start": "340150",
    "end": "346400"
  },
  {
    "text": "OK, and at the very end, I'm\ngoing to compute an estimate P",
    "start": "346400",
    "end": "351860"
  },
  {
    "text": "hat, of Xi equals little xi.",
    "start": "351860",
    "end": "356569"
  },
  {
    "text": "And this is going to be\nsimply the normalized version",
    "start": "356570",
    "end": "359870"
  },
  {
    "text": "of the count.",
    "start": "359870",
    "end": "362600"
  },
  {
    "text": "So this is going to be the\nrelative frequency of seeing",
    "start": "362600",
    "end": "367300"
  },
  {
    "text": "a particular value\nlittle xi, compared",
    "start": "367300",
    "end": "370449"
  },
  {
    "text": "to everything else I've seen.",
    "start": "370450",
    "end": "373440"
  },
  {
    "text": "OK, but there's a lot of\ncounting and normalizing.",
    "start": "373440",
    "end": "379380"
  },
  {
    "text": "But let's look at this demo to\ngive us a kind of a more fuller",
    "start": "379380",
    "end": "384360"
  },
  {
    "text": "sense of what's going on.",
    "start": "384360",
    "end": "386259"
  },
  {
    "text": "So here is the object\ntracking example,",
    "start": "386260",
    "end": "388060"
  },
  {
    "text": "I have three variables.",
    "start": "388060",
    "end": "389310"
  },
  {
    "text": "And here I'm going--",
    "start": "389310",
    "end": "390900"
  },
  {
    "text": "I can specify the query\nwhich is, which variable am I",
    "start": "390900",
    "end": "394949"
  },
  {
    "text": "interested in calculating\nthe marginal of?",
    "start": "394950",
    "end": "397440"
  },
  {
    "text": "And I'm going to run\nGibbs sampling here.",
    "start": "397440",
    "end": "400450"
  },
  {
    "text": "And then at the beginning,\nI sample a variable x1 given",
    "start": "400450",
    "end": "407080"
  },
  {
    "text": "everything else, so consider\nall the possible values of x1.",
    "start": "407080",
    "end": "411616"
  },
  {
    "text": "I'm going to look at their\npotentials or factors,",
    "start": "411616",
    "end": "416470"
  },
  {
    "text": "compute a weight, normalize\nto get a distribution.",
    "start": "416470",
    "end": "420740"
  },
  {
    "text": "And I'm going to sample\na value according",
    "start": "420740",
    "end": "422889"
  },
  {
    "text": "to these probabilities.",
    "start": "422890",
    "end": "425350"
  },
  {
    "text": "So in this case, it's\njust a coin flip.",
    "start": "425350",
    "end": "427190"
  },
  {
    "text": "I choose xy equals 0.",
    "start": "427190",
    "end": "429100"
  },
  {
    "text": "And then I update my counter.",
    "start": "429100",
    "end": "432310"
  },
  {
    "text": "So I'm recording that\nI saw x2 equals 1 once.",
    "start": "432310",
    "end": "439330"
  },
  {
    "text": "OK, and then I'm going to move\non to the next variable X2,",
    "start": "439330",
    "end": "443110"
  },
  {
    "text": "do the same thing.",
    "start": "443110",
    "end": "444909"
  },
  {
    "text": "Move to the next variable x3,\nkind of do the same thing.",
    "start": "444910",
    "end": "448370"
  },
  {
    "text": "And I'm going to just cycle\nfor this for a moment.",
    "start": "448370",
    "end": "451160"
  },
  {
    "text": "You can see that the assignment,\nwhich is depicted up here,",
    "start": "451160",
    "end": "456670"
  },
  {
    "text": "is changing.",
    "start": "456670",
    "end": "459210"
  },
  {
    "text": "And down here, I can\nsee that the count",
    "start": "459210",
    "end": "465569"
  },
  {
    "text": "of the number of times x2\nequals 1 has gone up to 25.",
    "start": "465570",
    "end": "472180"
  },
  {
    "text": "And now look, I actually\nhit a different value.",
    "start": "472180",
    "end": "475560"
  },
  {
    "text": "I went to a configuration\nwhere x2 equals 2 now.",
    "start": "475560",
    "end": "480900"
  },
  {
    "text": "And then I might sample\na little bit more,",
    "start": "480900",
    "end": "483750"
  },
  {
    "text": "and they'll come back to 1.",
    "start": "483750",
    "end": "486000"
  },
  {
    "text": "And you can just watch\nthis for a little while.",
    "start": "486000",
    "end": "489460"
  },
  {
    "text": "And you can see over\nhere, that these",
    "start": "489460",
    "end": "491819"
  },
  {
    "text": "are the estimates of the\nmarginal probability of x2",
    "start": "491820",
    "end": "495900"
  },
  {
    "text": "based on the counts.",
    "start": "495900",
    "end": "496990"
  },
  {
    "text": "So these numbers are simply\nthese normalized versions",
    "start": "496990",
    "end": "500940"
  },
  {
    "text": "of the counts.",
    "start": "500940",
    "end": "503030"
  },
  {
    "text": "So I'm going to speed\nthis up a little bit.",
    "start": "503030",
    "end": "505170"
  },
  {
    "text": "So let me do just\n1,000 steps at a time.",
    "start": "505170",
    "end": "509040"
  },
  {
    "text": "OK, so now if I did 1,000\nsteps of Gibbs sampling.",
    "start": "509040",
    "end": "513049"
  },
  {
    "text": "Now I have a lot of\ncounts of x2 equals",
    "start": "513049",
    "end": "515840"
  },
  {
    "text": "1, some counts of x2 equals 2.",
    "start": "515840",
    "end": "518510"
  },
  {
    "text": "And now you can see\nthe probabilities",
    "start": "518510",
    "end": "521599"
  },
  {
    "text": "are kind of converging to\nsomething like 0.6 and 0.3.",
    "start": "521600",
    "end": "525800"
  },
  {
    "text": "Let me just hit step\na few more times.",
    "start": "525800",
    "end": "528560"
  },
  {
    "text": "And you can see that these\nprobabilities are indeed",
    "start": "528560",
    "end": "532700"
  },
  {
    "text": "converging to 0.61.",
    "start": "532700",
    "end": "535400"
  },
  {
    "text": "Which if you remember\nfrom here, is",
    "start": "535400",
    "end": "538160"
  },
  {
    "text": "pretty close to the true\nmarginal probability.",
    "start": "538160",
    "end": "543290"
  },
  {
    "text": "OK, so it seems at first glance,\nkind of a wild thing, right?",
    "start": "543290",
    "end": "548570"
  },
  {
    "text": "So we're running this algorithm,\nit's just generating samples",
    "start": "548570",
    "end": "551870"
  },
  {
    "text": "left and right.",
    "start": "551870",
    "end": "552750"
  },
  {
    "text": "It's kind of random.",
    "start": "552750",
    "end": "554660"
  },
  {
    "text": "And yet, if I compute\nthe randomness,",
    "start": "554660",
    "end": "558079"
  },
  {
    "text": "it's very carefully\norchestrated.",
    "start": "558080",
    "end": "560420"
  },
  {
    "text": "So that when I sum\nthings up properly,",
    "start": "560420",
    "end": "562490"
  },
  {
    "text": "I actually get the\nright answer out.",
    "start": "562490",
    "end": "565040"
  },
  {
    "start": "565040",
    "end": "568850"
  },
  {
    "text": "So let me now go to the\nimage denoising examples.",
    "start": "568850",
    "end": "571790"
  },
  {
    "text": "So here the goal is given\na noisy image, clean it up.",
    "start": "571790",
    "end": "576350"
  },
  {
    "text": "And in our simplified\nversion, I have",
    "start": "576350",
    "end": "579680"
  },
  {
    "text": "Xi which represents the\nclean pixel value, which",
    "start": "579680",
    "end": "584750"
  },
  {
    "text": "I don't know.",
    "start": "584750",
    "end": "585750"
  },
  {
    "text": "Now, a subset of the\npixels are observed.",
    "start": "585750",
    "end": "588300"
  },
  {
    "text": "So for example,\nthese in green here.",
    "start": "588300",
    "end": "590990"
  },
  {
    "text": "And I'm going to clamp those\npixel values to the observed",
    "start": "590990",
    "end": "594050"
  },
  {
    "text": "value.",
    "start": "594050",
    "end": "594870"
  },
  {
    "text": "And then I have a\nfactor that says",
    "start": "594870",
    "end": "598700"
  },
  {
    "text": "neighboring pixels are twice\nas likely to be the same,",
    "start": "598700",
    "end": "602450"
  },
  {
    "text": "than different.",
    "start": "602450",
    "end": "605120"
  },
  {
    "text": "So let's do Gibbs sampling\nin this image denoising case.",
    "start": "605120",
    "end": "607600"
  },
  {
    "text": "So what Gibbs\nsampling would do, is",
    "start": "607600",
    "end": "609459"
  },
  {
    "text": "it's going to sweep\nacross the image,",
    "start": "609460",
    "end": "612460"
  },
  {
    "text": "and sample each variable\ncondition on the way.",
    "start": "612460",
    "end": "616380"
  },
  {
    "text": "So suppose I'm landing on\nthis particular pixel value,",
    "start": "616380",
    "end": "622020"
  },
  {
    "text": "and I'm trying to figure out\nwhat should its value be.",
    "start": "622020",
    "end": "625360"
  },
  {
    "text": "So again, I look at the possible\nvalues that could be 0 or 1.",
    "start": "625360",
    "end": "628769"
  },
  {
    "text": "And for each value, I'm\ngoing to compute a weight.",
    "start": "628770",
    "end": "631740"
  },
  {
    "text": "So remember from\nICM, that I actually",
    "start": "631740",
    "end": "634830"
  },
  {
    "text": "don't need to compute the\nweight of the entire assignment.",
    "start": "634830",
    "end": "638730"
  },
  {
    "text": "And I just only need to\nlook at the factors which",
    "start": "638730",
    "end": "641699"
  },
  {
    "text": "are dependent on this value.",
    "start": "641700",
    "end": "644870"
  },
  {
    "text": "OK, so let's\nconsider v equals 0.",
    "start": "644870",
    "end": "648200"
  },
  {
    "text": "So here if I put 0 here,\nthat means this potential",
    "start": "648200",
    "end": "651705"
  },
  {
    "text": "is going to be happy.",
    "start": "651705",
    "end": "652580"
  },
  {
    "text": "Because [INAUDIBLE] and\nI'm going to get a 2.",
    "start": "652580",
    "end": "656510"
  },
  {
    "text": "And this one is\ngoing to disagree.",
    "start": "656510",
    "end": "660050"
  },
  {
    "text": "This one's going to disagree and\nthis one is going to disagree.",
    "start": "660050",
    "end": "663000"
  },
  {
    "text": "So the weight is 2 times 1\ntimes 1 times 1, which is 2.",
    "start": "663000",
    "end": "668000"
  },
  {
    "text": "So now if I try to put\na 1 in this position,",
    "start": "668000",
    "end": "672040"
  },
  {
    "text": "now this potential says\n1, while the others say 2.",
    "start": "672040",
    "end": "678880"
  },
  {
    "text": "So now that has a weight of 8.",
    "start": "678880",
    "end": "681610"
  },
  {
    "text": "So now to get the\nprobability of Xi equals",
    "start": "681610",
    "end": "684310"
  },
  {
    "text": "v given everything\nelse, I'm simply",
    "start": "684310",
    "end": "687130"
  },
  {
    "text": "going to sum up and normalize.",
    "start": "687130",
    "end": "689750"
  },
  {
    "text": "So I have 2 and 8 here, the\nnormalization constant is 10.",
    "start": "689750",
    "end": "694280"
  },
  {
    "text": "So I get probabilities\n0.2 and 0.8.",
    "start": "694280",
    "end": "697570"
  },
  {
    "text": "Now given this\ndistribution, I'm going",
    "start": "697570",
    "end": "699790"
  },
  {
    "text": "to set this value to 1\nwith probability of 0.8,",
    "start": "699790",
    "end": "704560"
  },
  {
    "text": "and 0 with probability 0.2.",
    "start": "704560",
    "end": "706960"
  },
  {
    "text": "And then I'm going\nto keep on going.",
    "start": "706960",
    "end": "708460"
  },
  {
    "start": "708460",
    "end": "711180"
  },
  {
    "start": "710000",
    "end": "869000"
  },
  {
    "text": "So here is a fun\nlittle demo of Gibbs",
    "start": "711180",
    "end": "713720"
  },
  {
    "text": "sampling for an image denoising\nthat runs in your browser.",
    "start": "713720",
    "end": "717620"
  },
  {
    "text": "OK, so the idea is\nthat here is an image.",
    "start": "717620",
    "end": "720570"
  },
  {
    "text": "And if you hit\nControl Enter here.",
    "start": "720570",
    "end": "725120"
  },
  {
    "text": "You'll see that this is\nthe input to the system.",
    "start": "725120",
    "end": "728670"
  },
  {
    "text": "So we have black\npixels and red pixels,",
    "start": "728670",
    "end": "732709"
  },
  {
    "text": "these are the observed pixels.",
    "start": "732710",
    "end": "734960"
  },
  {
    "text": "And white pixels are unobserved.",
    "start": "734960",
    "end": "737810"
  },
  {
    "text": "And these are the ones\nthat we want to fill in.",
    "start": "737810",
    "end": "740792"
  },
  {
    "text": "So there's a bunch of\nsettings where I'll",
    "start": "740792",
    "end": "742500"
  },
  {
    "text": "talk to you about in a second.",
    "start": "742500",
    "end": "744270"
  },
  {
    "text": "But if you click here,\nyou can see how--",
    "start": "744270",
    "end": "747150"
  },
  {
    "text": "get a feeling for what\nGibbs sampling is doing.",
    "start": "747150",
    "end": "750600"
  },
  {
    "text": "Each frame here, each\niteration is a full pass",
    "start": "750600",
    "end": "754500"
  },
  {
    "text": "over all the pixels.",
    "start": "754500",
    "end": "756450"
  },
  {
    "text": "And you can see that\nit's kind of dancing",
    "start": "756450",
    "end": "758370"
  },
  {
    "text": "around, because it's trying to\nexplore different assignments.",
    "start": "758370",
    "end": "764620"
  },
  {
    "text": "So one thing you\ncan do, is you can",
    "start": "764620",
    "end": "767640"
  },
  {
    "text": "set showMarginals equals true.",
    "start": "767640",
    "end": "770020"
  },
  {
    "text": "And what this does, is\nthat instead of visualizing",
    "start": "770020",
    "end": "773730"
  },
  {
    "text": "the assignment at a particular\niteration for each pixel here,",
    "start": "773730",
    "end": "778199"
  },
  {
    "text": "I'm actually visualizing the\nmarginal probability estimate.",
    "start": "778200",
    "end": "783090"
  },
  {
    "text": "So this is in general, going\nto be a number between 0 and 1",
    "start": "783090",
    "end": "785730"
  },
  {
    "text": "which is represented as a shade\nbetween black and red here.",
    "start": "785730",
    "end": "789670"
  },
  {
    "text": "So this, in some sense,\nis the kind of best guess",
    "start": "789670",
    "end": "792269"
  },
  {
    "text": "at what the reconstruction is.",
    "start": "792270",
    "end": "796760"
  },
  {
    "text": "So there are a number of\nthings you can play with.",
    "start": "796760",
    "end": "798950"
  },
  {
    "text": "So for example, the\nfraction of missing pixels.",
    "start": "798950",
    "end": "801250"
  },
  {
    "text": "If I reduce this\nto let's say 0.3.",
    "start": "801250",
    "end": "804820"
  },
  {
    "text": "Then the problem becomes easier.",
    "start": "804820",
    "end": "808160"
  },
  {
    "text": "And you can see that\nthe reconstruction",
    "start": "808160",
    "end": "809949"
  },
  {
    "text": "gets pretty reasonable results.",
    "start": "809950",
    "end": "814050"
  },
  {
    "text": "Another fun thing\nyou can play with",
    "start": "814050",
    "end": "815640"
  },
  {
    "text": "is, well actually, let\nme bring down the--",
    "start": "815640",
    "end": "820080"
  },
  {
    "text": "bring up the missing\nfraction to 1.",
    "start": "820080",
    "end": "823390"
  },
  {
    "text": "OK, so that means I\ndon't see any pixel.",
    "start": "823390",
    "end": "825900"
  },
  {
    "text": "So here, this is\njust going to be--",
    "start": "825900",
    "end": "830310"
  },
  {
    "text": "actually let me do that.",
    "start": "830310",
    "end": "831960"
  },
  {
    "text": "showMarginals\nequals false, oops.",
    "start": "831960",
    "end": "834500"
  },
  {
    "start": "834500",
    "end": "838330"
  },
  {
    "text": "So here you can see kind of just\nblind samples from the model,",
    "start": "838330",
    "end": "842980"
  },
  {
    "text": "OK?",
    "start": "842980",
    "end": "844360"
  },
  {
    "text": "And if I bump up the\ncoherence, if I bump it down,",
    "start": "844360",
    "end": "850060"
  },
  {
    "text": "then you'll see kind of\na more random pattern.",
    "start": "850060",
    "end": "853420"
  },
  {
    "text": "If I bump it up to 10, then\nyou'll see more coherence.",
    "start": "853420",
    "end": "858490"
  },
  {
    "text": "So remember, this is kind of\nlike the phase transitions",
    "start": "858490",
    "end": "860980"
  },
  {
    "text": "that we saw for the [AUDIO OUT].",
    "start": "860980",
    "end": "864570"
  },
  {
    "text": "OK, so I will let you play\nwith this on your own.",
    "start": "864570",
    "end": "869630"
  },
  {
    "start": "869000",
    "end": "1012000"
  },
  {
    "text": "So let me just conclude here.",
    "start": "869630",
    "end": "871840"
  },
  {
    "text": "Actually, one thing\nbefore we conclude.",
    "start": "871840",
    "end": "873880"
  },
  {
    "text": "So let me try to go back to\nIterated Conditional Modes.",
    "start": "873880",
    "end": "877570"
  },
  {
    "text": "And compare that\nwith Gibbs sampling.",
    "start": "877570",
    "end": "879160"
  },
  {
    "text": "Both of them have the\nsame kind of template.",
    "start": "879160",
    "end": "882670"
  },
  {
    "text": "You're working with\ncomplete assignments,",
    "start": "882670",
    "end": "884470"
  },
  {
    "text": "and you're going\nthrough each variable",
    "start": "884470",
    "end": "886053"
  },
  {
    "text": "and updating the assignment to\nthat variable one at a time.",
    "start": "886053",
    "end": "890139"
  },
  {
    "text": "But there's a few\ndifferences here.",
    "start": "890140",
    "end": "892360"
  },
  {
    "text": "One, the first salient one is\nthat Iterated Conditional Modes",
    "start": "892360",
    "end": "896410"
  },
  {
    "text": "was for solving CSPs,\nwhere we're trying to find",
    "start": "896410",
    "end": "898449"
  },
  {
    "text": "the maximum weight assignment.",
    "start": "898450",
    "end": "899890"
  },
  {
    "text": "Gibbs sampling is\nfor Markov networks,",
    "start": "899890",
    "end": "901870"
  },
  {
    "text": "where we're trying to compute\nmarginal probabilities.",
    "start": "901870",
    "end": "905279"
  },
  {
    "text": "So as a consequence\nfor ICM, at each step",
    "start": "905280",
    "end": "910280"
  },
  {
    "text": "we're choosing the value to\nassign to a variable, which",
    "start": "910280",
    "end": "915560"
  },
  {
    "text": "maximizes weight.",
    "start": "915560",
    "end": "917540"
  },
  {
    "text": "Whereas in Gibbs\nsampling, where using",
    "start": "917540",
    "end": "920690"
  },
  {
    "text": "the weights to\nform a distribution",
    "start": "920690",
    "end": "922220"
  },
  {
    "text": "and sampling from\nthat distribution.",
    "start": "922220",
    "end": "925399"
  },
  {
    "text": "In ICM, we noticed that\nthe algorithm does converge",
    "start": "925400",
    "end": "929630"
  },
  {
    "text": "but often to a local optimum.",
    "start": "929630",
    "end": "931190"
  },
  {
    "text": "Which is not the best\nmaximum weight assignment.",
    "start": "931190",
    "end": "935300"
  },
  {
    "text": "For Gibbs sampling, as you\ncan see from these samples,",
    "start": "935300",
    "end": "938779"
  },
  {
    "text": "there's no traditional\nnotions of convergence.",
    "start": "938780",
    "end": "941860"
  },
  {
    "text": "The samples are going to keep on\nchanging and keep on changing.",
    "start": "941860",
    "end": "944880"
  },
  {
    "text": "So, the Iterates are not the\nones which are converging.",
    "start": "944880",
    "end": "947960"
  },
  {
    "text": "What is actually\ngoing to converge",
    "start": "947960",
    "end": "949610"
  },
  {
    "text": "are the marginal estimates.",
    "start": "949610",
    "end": "952070"
  },
  {
    "text": "And under some\ntechnical assumptions,",
    "start": "952070",
    "end": "956330"
  },
  {
    "text": "these estimates\nare actually going",
    "start": "956330",
    "end": "958340"
  },
  {
    "text": "to converge to the\ncorrect answer.",
    "start": "958340",
    "end": "960620"
  },
  {
    "text": "We saw that for object tracking.",
    "start": "960620",
    "end": "962360"
  },
  {
    "text": "It did a pretty good job there.",
    "start": "962360",
    "end": "964800"
  },
  {
    "text": "But there were some\ntechnical conditions.",
    "start": "964800",
    "end": "967490"
  },
  {
    "text": "One sufficient condition is that\nall the weights be positive.",
    "start": "967490",
    "end": "972140"
  },
  {
    "text": "But more generally,\nwhat we need is",
    "start": "972140",
    "end": "975440"
  },
  {
    "text": "that for the probability of\ngoing from one assignment",
    "start": "975440",
    "end": "978500"
  },
  {
    "text": "to another assignment\nvia Gibbs sampling",
    "start": "978500",
    "end": "980480"
  },
  {
    "text": "has positive probability.",
    "start": "980480",
    "end": "982339"
  },
  {
    "text": "As if you have two\ndisconnected regions.",
    "start": "982340",
    "end": "985880"
  },
  {
    "text": "Then if you start\na Gibbs sampling",
    "start": "985880",
    "end": "988520"
  },
  {
    "text": "at one particular\npoint, then you",
    "start": "988520",
    "end": "989900"
  },
  {
    "text": "will never reach\nthe other point.",
    "start": "989900",
    "end": "993110"
  },
  {
    "text": "The one important caveat is\nGibbs sampling is wonderful.",
    "start": "993110",
    "end": "996990"
  },
  {
    "text": "But in the worst case, it\ndoes take exponential time.",
    "start": "996990",
    "end": "1000230"
  },
  {
    "text": "So these are really--",
    "start": "1000230",
    "end": "1001360"
  },
  {
    "text": "computing marginal probabilities\nis a really hard problem.",
    "start": "1001360",
    "end": "1004100"
  },
  {
    "text": "And Gibbs sampling\nis just a heuristic",
    "start": "1004100",
    "end": "1006940"
  },
  {
    "text": "with some nice\nasymptotic energy.",
    "start": "1006940",
    "end": "1010285"
  },
  {
    "start": "1010285",
    "end": "1013490"
  },
  {
    "start": "1012000",
    "end": "1074000"
  },
  {
    "text": "So wrapping up, we\nlooked at computing",
    "start": "1013490",
    "end": "1017510"
  },
  {
    "text": "the marginal probabilities\nof a Markov network.",
    "start": "1017510",
    "end": "1023100"
  },
  {
    "text": "And we saw that Gibbs\nsampling did this",
    "start": "1023100",
    "end": "1026160"
  },
  {
    "text": "by sampling one\nvariable at a time.",
    "start": "1026160",
    "end": "1029040"
  },
  {
    "text": "And it counts visitations to\neach of the values for a given",
    "start": "1029040",
    "end": "1035609"
  },
  {
    "text": "variable.",
    "start": "1035609",
    "end": "1037160"
  },
  {
    "text": "And it's one of these kind\nof astonishing things,",
    "start": "1037160",
    "end": "1039949"
  },
  {
    "text": "that Gibbs sampling is\nso carefully constructed,",
    "start": "1039950",
    "end": "1043519"
  },
  {
    "text": "that it actually kind of works.",
    "start": "1043520",
    "end": "1045170"
  },
  {
    "text": "And you can prove lots of\ninteresting theorems about it.",
    "start": "1045170",
    "end": "1048900"
  },
  {
    "text": "Finally, Gibbs sampling\nis just the first taste",
    "start": "1048900",
    "end": "1051870"
  },
  {
    "text": "of a much more broad class\nof techniques of Markov chain",
    "start": "1051870",
    "end": "1056610"
  },
  {
    "text": "Monte Carlo which\nare used to produce",
    "start": "1056610",
    "end": "1061860"
  },
  {
    "text": "much kind of richer ways\nof estimating probabilities",
    "start": "1061860",
    "end": "1065490"
  },
  {
    "text": "in Markov.",
    "start": "1065490",
    "end": "1067429"
  },
  {
    "text": "All right, that's the\nend of this module.",
    "start": "1067430",
    "end": "1069940"
  },
  {
    "start": "1069940",
    "end": "1074070"
  }
]