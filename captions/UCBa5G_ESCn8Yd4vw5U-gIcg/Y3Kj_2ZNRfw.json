[
  {
    "start": "0",
    "end": "119000"
  },
  {
    "start": "0",
    "end": "11339"
  },
  {
    "text": "[APPLAUSE] ",
    "start": "11339",
    "end": "16860"
  },
  {
    "text": "Thank you so much for that very,\nvery generous introduction. So I'm going to\ntalk about how we",
    "start": "16860",
    "end": "22140"
  },
  {
    "text": "can reshape human AI interaction\naround societal disagreements. And to demonstrate why\nthis might be a good idea,",
    "start": "22140",
    "end": "29640"
  },
  {
    "text": "I think we need look no\nfurther than ChatGPT. Just one really exciting\nexample of recent progress in human AI interaction.",
    "start": "29640",
    "end": "37050"
  },
  {
    "text": "And so up at the top here in\na natural language prompt, I've asked you to write an\node to computer science.",
    "start": "37050",
    "end": "46190"
  },
  {
    "text": "This is really cool. So, again, I work in human\nAI interaction, where we have these foundational\nvisions of designing",
    "start": "46190",
    "end": "52130"
  },
  {
    "text": "interactive systems in which\nhumans and AI work together to solve problems,\nof empowering users",
    "start": "52130",
    "end": "59030"
  },
  {
    "text": "to design and train\ntheir own AIs, and of how we might trade-off\nagency and automation.",
    "start": "59030",
    "end": "65900"
  },
  {
    "text": "But ChatGPT, also, highlights\na growing challenge in human AI interaction, which\nI focus on in my dissertation",
    "start": "65900",
    "end": "71750"
  },
  {
    "text": "research, which is\nthat, increasingly, we are using AI for\nproblems, where what qualifies as a good\nanswer is going",
    "start": "71750",
    "end": "80090"
  },
  {
    "text": "to depend upon who you ask. And to get at that\nchallenge, I want to try a slightly\ndifferent prompt here,",
    "start": "80090",
    "end": "86100"
  },
  {
    "text": "which is write an ode to\nthe most important subarea of computer science.",
    "start": "86100",
    "end": "91138"
  },
  {
    "text": "And so what happens here? Well, it begins, Oh, artificial\nintelligence, how vast and grand",
    "start": "91138",
    "end": "96530"
  },
  {
    "text": "they field. With algorithms and models\nthat make the future yield. And goes on from there. Now, again, I'm a human\ninteraction researcher.",
    "start": "96530",
    "end": "104430"
  },
  {
    "text": "So I happen to think\nthat both HCI and AI are pretty important subareas. So I'm not sure I'm\ntotally on board here.",
    "start": "104430",
    "end": "111188"
  },
  {
    "text": "And more broadly, this\nis a task for people who might disagree about what\nthe right answer ought to be.",
    "start": "111188",
    "end": "116640"
  },
  {
    "text": "You and I might have\ndifferent priorities here. And that task are just one\nexample of how user facing AI",
    "start": "116640",
    "end": "123900"
  },
  {
    "text": "is increasingly moving from a\ndomain in which there was little to no disagreement about what\nthe right answer ought to be,",
    "start": "123900",
    "end": "130440"
  },
  {
    "text": "to domains in which\nthere might be enormous disagreement about what\nthe right answer ought to be. So, for instance, in medicine,\nwhen specialists disagree,",
    "start": "130440",
    "end": "138480"
  },
  {
    "text": "there are entire\ntumor boards that are set up to discuss and\ndebate diagnosis and treatment approaches.",
    "start": "138480",
    "end": "144660"
  },
  {
    "text": "In design, closer\nto my own field, different designers might\nhave very different ideas",
    "start": "144660",
    "end": "150120"
  },
  {
    "text": "about what that ought to do. If you're a Bauhaus\ndesigner, you're thinking pretty differently\nabout a visual design",
    "start": "150120",
    "end": "155940"
  },
  {
    "text": "problem than someone who comes\nfrom a different design school. Or in social computing tasks,\nlike content moderation",
    "start": "155940",
    "end": "161610"
  },
  {
    "text": "or misinformation\ndetection, clearly, people are going to have very\ndifferent ideas of what",
    "start": "161610",
    "end": "166920"
  },
  {
    "text": "acceptable behavior\nlooks like online. And in this talk, I'm\ngoing to focus on what I'm",
    "start": "166920",
    "end": "172000"
  },
  {
    "start": "170000",
    "end": "338000"
  },
  {
    "text": "calling societal disagreements. So these are decisions\nthat impact multiple people or stakeholders in the\nface of competing goals",
    "start": "172000",
    "end": "178690"
  },
  {
    "text": "and values, where it's not\njust about what I want, but also what other people want. So that might be\ncontent moderation",
    "start": "178690",
    "end": "185530"
  },
  {
    "text": "that we're making decisions\nthat impact the health of entire communities. It might be creative tasks\nlike designing a poster",
    "start": "185530",
    "end": "192280"
  },
  {
    "text": "or even designing\nthis talk, where I was trying to reason\nover and optimize for how different people in the\naudience might respond",
    "start": "192280",
    "end": "199120"
  },
  {
    "text": "to what I'm saying here. Let's take content moderation as\nan example used to address hate",
    "start": "199120",
    "end": "205900"
  },
  {
    "text": "speech, misinformation,\nharassment, radicalization, even just enforced\ncommunity-specific norms.",
    "start": "205900",
    "end": "211450"
  },
  {
    "text": "Moderators, researchers,\nengineers all want classifiers for these\ncritical social computing tasks.",
    "start": "211450",
    "end": "217659"
  },
  {
    "text": "So they can easily\ndeploy at scale. But in content moderation,\ndifferent communities",
    "start": "217660",
    "end": "224380"
  },
  {
    "text": "online can have vastly\ndifferent norms and values. That's true both\nacross communities. And that's true\nwithin communities,",
    "start": "224380",
    "end": "230680"
  },
  {
    "text": "where Reddit moderators\ndisagree with each other on 29% of removed posts.",
    "start": "230680",
    "end": "238150"
  },
  {
    "text": "So when we're using AI to\nhelp keep communities healthy, whose labels, whose perspectives\nshould the model be emulating?",
    "start": "238150",
    "end": "246920"
  },
  {
    "text": "Today, the existence of\nmultiple perspectives is largely hidden away\nor treated as noise.",
    "start": "246920",
    "end": "252170"
  },
  {
    "text": "And so as a result, AI often\nsolves the wrong problems, behaves in unexpected ways\ncausing societal harm.",
    "start": "252170",
    "end": "259760"
  },
  {
    "text": "But I think that--\nrather than thinking of this as a problem\nfor AI, we can actually",
    "start": "259760",
    "end": "266389"
  },
  {
    "text": "treat it as a opportunity. I think we have an opportunity\nto create human AI systems that",
    "start": "266390",
    "end": "272090"
  },
  {
    "text": "help us adapt new perspectives\nand expertise towards our goals.",
    "start": "272090",
    "end": "277730"
  },
  {
    "text": "So how are we going to do that? Well, this is an\nexample of a problem that I find really interesting.",
    "start": "277730",
    "end": "283310"
  },
  {
    "text": "I think that the exciting\nchallenge of working in human AI interaction is that to\nreally make progress,",
    "start": "283310",
    "end": "289520"
  },
  {
    "text": "we have to bridge\ndesign principles that we get from HCI\nwith the modeling",
    "start": "289520",
    "end": "295490"
  },
  {
    "text": "realities of machine learning. It's not going to be\nenough to simply wrap some interaction around an\nexisting model like a black box.",
    "start": "295490",
    "end": "303097"
  },
  {
    "text": "And I'm going to get at\nthose realities of machine learning in a few minutes. But let's start with\nsome principles of HCI.",
    "start": "303097",
    "end": "308635"
  },
  {
    "text": "In particular,\nI'm going to build on this idea of intelligence\naugmentation, which is a vision from the '60s of\nhow machines might help us solve",
    "start": "308635",
    "end": "316130"
  },
  {
    "text": "problems by making us smarter. And so how do we\ncreate those tools?",
    "start": "316130",
    "end": "321710"
  },
  {
    "text": "Well, Simon and Norma talked\nabout how representations shape our thinking. They shape how we consider\nproblems and how we solve them.",
    "start": "321710",
    "end": "328370"
  },
  {
    "text": "And so in order to reason over\nthe trade-offs and perspectives",
    "start": "328370",
    "end": "333380"
  },
  {
    "text": "in these disagreed\nupon tasks, we have to be able\nto represent them.",
    "start": "333380",
    "end": "338610"
  },
  {
    "text": "And so fronting the power of\nproblem representation here, I'm going to argue that by\nembedding representations",
    "start": "338610",
    "end": "345120"
  },
  {
    "text": "of people in society into\nour interactions and models and metrics that power\nhuman AI interaction,",
    "start": "345120",
    "end": "350730"
  },
  {
    "text": "we can reshape AI into\na tool for thought that lets us reason over\nand evaluate models",
    "start": "350730",
    "end": "358920"
  },
  {
    "text": "under societal disagreement. And just to provide an\nintuition for what I mean here,",
    "start": "358920",
    "end": "364360"
  },
  {
    "text": "we can go back to our ChatGPT\nexample with this ode. Today, to create\nthat system, we'd",
    "start": "364360",
    "end": "371289"
  },
  {
    "text": "start with some\ndata, which we then use to train a model, which\nthen produces some output. In the case I just\nshowed you, that",
    "start": "371290",
    "end": "376600"
  },
  {
    "text": "might be determined\nby the annotator, who did the instruction tuning. But now in this talk,\nwhat I want to do",
    "start": "376600",
    "end": "382870"
  },
  {
    "text": "is open this black box\nof training data up. And articulate\nthat there might be",
    "start": "382870",
    "end": "389409"
  },
  {
    "text": "a series of different\nperspectives inside it. And then provide\nyou with the tools to actually control\nthose voices.",
    "start": "389410",
    "end": "395860"
  },
  {
    "text": "And in doing so, change\nthe behavior of the AI. So, for instance, you might\ncare about how undergrads or PhD students or faculty might\nhave written that ode.",
    "start": "395860",
    "end": "404642"
  },
  {
    "text": "And over the next\n40 minutes or so, I'm going to unroll that insight\nover the machine learning pipeline.",
    "start": "404642",
    "end": "409700"
  },
  {
    "text": "I'm going to focus\nfirst on what we can do in terms of the\ninteractions and the models that we're building.",
    "start": "409700",
    "end": "415010"
  },
  {
    "text": "To do that, I'm going to present\nan interactive AI architecture called jury learning, which\nenables developers to explicitly",
    "start": "415010",
    "end": "421610"
  },
  {
    "text": "reason over whose voice, a\nmodel ought to emulate through",
    "start": "421610",
    "end": "427250"
  },
  {
    "text": "the metaphor of a jury . And then I'm going to ask, what\nif we can't change the model?",
    "start": "427250",
    "end": "432350"
  },
  {
    "text": "At the very least,\nis there something that we can do at the\nevaluation stage that helps us become more\naware of the problems",
    "start": "432350",
    "end": "439729"
  },
  {
    "text": "that disagreement can cause? I'm going to talk there about\nthe disagreement deconvolution, which is a metric\ntransformation showing how",
    "start": "439730",
    "end": "447979"
  },
  {
    "text": "in an abstracting away\nthe individual people that models emulate. Current metrics can dramatically\noverstate the performance",
    "start": "447980",
    "end": "455840"
  },
  {
    "text": "of many user facing ML tasks. Well, let's start\nwith how we're going to design these\nhuman AI systems that",
    "start": "455840",
    "end": "462430"
  },
  {
    "text": "navigate societal disagreement. And so to unpack what's going\non here, what we do about it--",
    "start": "462430",
    "end": "467879"
  },
  {
    "start": "465000",
    "end": "588000"
  },
  {
    "text": "I'm actually going to\nstart at ground truth. So ground truth is,\nof course, the stuff that we use to train\nAI classifiers.",
    "start": "467880",
    "end": "474630"
  },
  {
    "text": "And it's incredibly\nconsequential, because, today, it's how\nwe represent our goals and values in these systems.",
    "start": "474630",
    "end": "481800"
  },
  {
    "text": "So imagine you're\ntraining a model to remove toxic comments online. Should this training\nexample here,",
    "start": "481800",
    "end": "488039"
  },
  {
    "text": "which is making some\naggressive comments about pizza and politics-- should that be an example of\ncontent to remove or keep up?",
    "start": "488040",
    "end": "495908"
  },
  {
    "text": "Well, that's a tough one. It turns out that in the public\ndata set that we took it from, around half of people feel\neach way about this comment.",
    "start": "495908",
    "end": "502520"
  },
  {
    "text": "And so in today's typical\nsupervised learning pipeline, we then model a aggregate\npseudo human predicting,",
    "start": "502520",
    "end": "509479"
  },
  {
    "text": "typically, the\nmajority vote label, while ignoring the annotators\nwho disagree with that majority.",
    "start": "509480",
    "end": "515610"
  },
  {
    "text": "Or maybe you could\ndo a bit better. And you saw there, AI\nshould predict, say, a 46 6-year distribution.",
    "start": "515610",
    "end": "521881"
  },
  {
    "text": "But if you're a\npractitioner who's then got to decide\nwhether you're going to remove\nthat comment or not, what do you do with\nthis information?",
    "start": "521882",
    "end": "529210"
  },
  {
    "text": "There's not really\na good answer here, other than just siding\nwith the majority.",
    "start": "529210",
    "end": "535240"
  },
  {
    "text": "And for a long time, that\nmajoritarian approach mostly made sense. So tasks, like, is\nthis a cat or a dog?",
    "start": "535240",
    "end": "542830"
  },
  {
    "text": "I would argue these\ntasks tend to operate at the perceptual\nlevel of our systems. There's not this higher level\ncognitive reasoning going on.",
    "start": "542830",
    "end": "550360"
  },
  {
    "text": "So you're hopefully\ngoing to say. This is a dog. Otherwise, I have some issues\nwith my own perceptual systems.",
    "start": "550360",
    "end": "556720"
  },
  {
    "text": "But when this disagreement\nis irreconcilable, when it's endemic to a task,\nwho we sample for the labels",
    "start": "556720",
    "end": "565000"
  },
  {
    "text": "is going to matter. But, again, today, once\nwe've chosen our data sets annotators, whose voice\nthe model that emulates",
    "start": "565000",
    "end": "573449"
  },
  {
    "text": "is often left implicit in that\ndata collection and training procedure? And so if we can make\nthat voice explicit,",
    "start": "573450",
    "end": "579030"
  },
  {
    "text": "then we can empower\npractitioners to choose from and\nreason over the decisions",
    "start": "579030",
    "end": "584430"
  },
  {
    "text": "that different people and\nexpertise would make here. So what are we going to do here?",
    "start": "584430",
    "end": "589660"
  },
  {
    "start": "588000",
    "end": "692000"
  },
  {
    "text": "How are we going to do that? We're going back to\nmy thesis statement. I think our goal\nshould be to embed",
    "start": "589660",
    "end": "594770"
  },
  {
    "text": "a representation of this problem\ndirectly into the interactions in the models.",
    "start": "594770",
    "end": "600889"
  },
  {
    "text": "And to do that, I'm going\nto introduce a system called jury learning, which\nis an interactive, supervised",
    "start": "600890",
    "end": "607490"
  },
  {
    "text": "learning architecture. A technical and\nnormative approach that lets practitioners\ninteractively explore, and tune,",
    "start": "607490",
    "end": "616310"
  },
  {
    "text": "and shift the behavior\nof a classifier by explicitly articulating\nwhich voices from your data",
    "start": "616310",
    "end": "623330"
  },
  {
    "text": "set the model emulates\nin what proportion. You would now specify a jury. It can be any size here.",
    "start": "623330",
    "end": "629480"
  },
  {
    "text": "I'm just drawing 12, because\n12 people fit on a slide. And you say, what\nproportion of that jury",
    "start": "629480",
    "end": "635630"
  },
  {
    "text": "should represent each group\nor an intersectional identity that you care about?",
    "start": "635630",
    "end": "641140"
  },
  {
    "text": "And to make this\ninteraction possible, we're, actually,\ngoing to tease this back apart, such that the\nAI's task is no longer",
    "start": "641140",
    "end": "648790"
  },
  {
    "text": "to predict a single\nground truth. but to take every annotator\nin your data set or every--",
    "start": "648790",
    "end": "655870"
  },
  {
    "text": "or juror here, and\nthen to predict their individual responses. And then we can aggregate\nthat as a result to create",
    "start": "655870",
    "end": "662980"
  },
  {
    "text": "a classification decision. And so this really\nchanges the semantics of how the AI works in\nthe way that we think",
    "start": "662980",
    "end": "668740"
  },
  {
    "text": "is fairly straightforward\nand interpretable. And so the output\nof a system here",
    "start": "668740",
    "end": "674080"
  },
  {
    "text": "carries a different\nkind of interpretation than your traditional\nclassifier. So, essentially, there is\nsomething explicitly stating",
    "start": "674080",
    "end": "679750"
  },
  {
    "text": "for the following\njury composition. Here, I'm just giving\nyou an example. 56% of people agree\nthis comment is toxic.",
    "start": "679750",
    "end": "686500"
  },
  {
    "text": "And so now, the AI is\nbeing very explicit about whose voice\nis listening to.",
    "start": "686500",
    "end": "693029"
  },
  {
    "start": "692000",
    "end": "729000"
  },
  {
    "text": "So just to give an outline\nof where we're going here, I'm going to trace your learning\nthrough the following stages.",
    "start": "693030",
    "end": "699150"
  },
  {
    "text": "So first, our normative\ngoals, then interaction, our technical approach,\nevaluation, and, finally,",
    "start": "699150",
    "end": "705930"
  },
  {
    "text": "some implications of the jury\nmetaphor and opportunities. Let's start here\nwith our interaction.",
    "start": "705930",
    "end": "711830"
  },
  {
    "text": "So why do we use\na jury metaphor? Well, we wanted an\ninteraction that makes it clear\nthat there's going",
    "start": "711830",
    "end": "717440"
  },
  {
    "text": "to be a group of\nindividual decision makers from which a single\ndecision must emerge.",
    "start": "717440",
    "end": "724220"
  },
  {
    "text": "And we think that a jury is\na useful metaphor for this. In fact, what I want to claim\nis that every data set already",
    "start": "724220",
    "end": "733160"
  },
  {
    "start": "729000",
    "end": "984000"
  },
  {
    "text": "has a jury. And what I mean by that\nis every data set already has some existing\ndistribution of annotators,",
    "start": "733160",
    "end": "740089"
  },
  {
    "text": "whose voices make up its\nclassification decisions. So in the example\nthat we've been using,",
    "start": "740090",
    "end": "745100"
  },
  {
    "text": "these five annotators\nhere make up a jury. But today, this is\nan implicit jury.",
    "start": "745100",
    "end": "751130"
  },
  {
    "text": "It's baked into\nthe training data. And then swept under the rug. And so the interaction\nthat we've created",
    "start": "751130",
    "end": "756530"
  },
  {
    "text": "is really our version of\na juror selection process. We want practitioners deciding\nwho these decision makers are",
    "start": "756530",
    "end": "763910"
  },
  {
    "text": "going to be in a way that's\nexplicit and visible and easily changeable.",
    "start": "763910",
    "end": "769010"
  },
  {
    "text": "And so now, let's walk through\nour jury learning architecture for this toxicity task.",
    "start": "769010",
    "end": "774290"
  },
  {
    "text": "And so given an\ninput, practitioners start by defining\na jury composition, selecting from characteristics\nin their data set.",
    "start": "774290",
    "end": "781970"
  },
  {
    "text": "So, for instance, if a data\nset contains each annotator's political leanings,\nthe practitioner might select, for instance,\ntwo liberal jurors.",
    "start": "781970",
    "end": "788672"
  },
  {
    "text": "They'll also choose\nhere two jurors who are parents with\nhigh school diplomas. Juries can be any size.",
    "start": "788672",
    "end": "794209"
  },
  {
    "text": "Here, I'm just using\nfour, because for work for this simple example. And so we now have these\nfour empty juror slots.",
    "start": "794210",
    "end": "802649"
  },
  {
    "text": "And we need to choose\nindividual annotators from our data set to fill them. And so we're going to start with\nall the annotators in our data",
    "start": "802650",
    "end": "810820"
  },
  {
    "text": "set. Again, these are all\nreal individual people. And just like in any\ntypical data set,",
    "start": "810820",
    "end": "816420"
  },
  {
    "text": "they've each only annotated some\nsmall subset of the examples in the full data set.",
    "start": "816420",
    "end": "822829"
  },
  {
    "text": "We can now filter through all\nof the relevant annotators in our data set. Jury learning currently\nrequires that data sets",
    "start": "822830",
    "end": "830120"
  },
  {
    "text": "contain these explicit\nlabels about annotators. We can then randomly\nselect a matching juror",
    "start": "830120",
    "end": "836060"
  },
  {
    "text": "from the data set to\nfill each juror slot. And the AI then predicts\nhow each individual juror",
    "start": "836060",
    "end": "843367"
  },
  {
    "text": "would vote, using\nall the data that we have about their annotations,\nand the annotations of other similar jurors,\ngiving us this jury's verdict.",
    "start": "843367",
    "end": "852112"
  },
  {
    "text": "And I'll talk more about\nthe model architecture that powers this in a minute. And, of course, this can\nwork with regression too,",
    "start": "852112",
    "end": "858990"
  },
  {
    "text": "where you predict a numeric\nvalue, instead of yes or no class. But now, just\nrandomly selecting one",
    "start": "858990",
    "end": "864940"
  },
  {
    "text": "set of jurors, that's\ngoing to give you an idea of what one particular\nset of people might think.",
    "start": "864940",
    "end": "870670"
  },
  {
    "text": "But jury learning\nenables something impossible with\nreal-world juries, which is that we can, actually,\nresample our jurors many times,",
    "start": "870670",
    "end": "879069"
  },
  {
    "text": "creating many parallel juries. And then each parallel jury\nhas its own predicted verdict.",
    "start": "879070",
    "end": "886005"
  },
  {
    "text": "And so this approach,\nactually, results in a direct measure\nof uncertainty, which is, how often the outcome\nchanged across our jury samples?",
    "start": "886005",
    "end": "892910"
  },
  {
    "text": "We get this distribution\nhere of jury verdicts. And each dot is\na resampled jury.",
    "start": "892910",
    "end": "898820"
  },
  {
    "text": "And then for a single\nfinal decision, we can take the median, giving\nus a media-of-means estimator.",
    "start": "898820",
    "end": "906180"
  },
  {
    "text": "And to safeguard\nagainst overgeneralizing a whole group of opinions from\njust a few people, what's often",
    "start": "906180",
    "end": "912420"
  },
  {
    "text": "called the atomistic\nfallacy, the system warns you that, for\ninstance, if you selected the characteristic, doctor,\nbut your data set only",
    "start": "912420",
    "end": "919872"
  },
  {
    "text": "has a few doctors,\nyou really first have to go collect\ndata for more doctors. And by modeling\nindividual annotators,",
    "start": "919872",
    "end": "926880"
  },
  {
    "text": "we've also enabled\ninteractive sensemaking about the nature\nof disagreements. So, for instance, we can\nplot where all the selected",
    "start": "926880",
    "end": "933840"
  },
  {
    "text": "jurors fall among their whole\npopulations distribution, helping us prevent the\necological fallacy in which you",
    "start": "933840",
    "end": "940707"
  },
  {
    "text": "might assume that all\nindividuals in a group are the same. And so we can really\nground the jurors here",
    "start": "940707",
    "end": "945900"
  },
  {
    "text": "as individual people with\nspecific characteristics, which enable some other\nexplainability-related",
    "start": "945900",
    "end": "951990"
  },
  {
    "text": "information as well. So you could, for instance,\nstart with our distribution here of juries.",
    "start": "951990",
    "end": "957050"
  },
  {
    "text": "Select then a specific jury. And then display information\nabout a specific juror, which",
    "start": "957050",
    "end": "963529"
  },
  {
    "text": "allows practitioners\nto, for instance, reflect on how that particular\njuror might have labeled",
    "start": "963530",
    "end": "968810"
  },
  {
    "text": "similar inputs in\nthe training data, or look at that particular\njurors modeling error rate over all their\ntest examples.",
    "start": "968810",
    "end": "974990"
  },
  {
    "text": " So that's the interaction.",
    "start": "974990",
    "end": "980290"
  },
  {
    "text": "But now, walking back\nto this prediction step. How can we predict each\nannotator's response",
    "start": "980290",
    "end": "986690"
  },
  {
    "start": "984000",
    "end": "1277000"
  },
  {
    "text": "to some unseen example? What we're going to assume that\nwe have a typical classification",
    "start": "986690",
    "end": "992790"
  },
  {
    "text": "data set. Each of these jurors are,\nagain, just annotators. And like in a\ntypical data set, we",
    "start": "992790",
    "end": "999450"
  },
  {
    "text": "assume that annotators have some\nmostly disjoint, occasionally, overlapping set of\nexamples they annotated.",
    "start": "999450",
    "end": "1006830"
  },
  {
    "text": "And a bunch of training\nexamples they didn't annotate. And, of course, we\nhave this not just",
    "start": "1006830",
    "end": "1012770"
  },
  {
    "text": "for annotators that\nwe selected as jurors, but for every annotator.",
    "start": "1012770",
    "end": "1018230"
  },
  {
    "text": "And for every annotator, we\nhave a bunch of characteristics. And then what we want is to\npredict each juror's response",
    "start": "1018230",
    "end": "1026420"
  },
  {
    "text": "to some unseen example. So how are we tackle\nthis prediction problem?",
    "start": "1026420",
    "end": "1032650"
  },
  {
    "text": "Well, some of you\nmight have noticed that the structure\nis starting to look a lot like a\ncollaborative filtering",
    "start": "1032650",
    "end": "1039309"
  },
  {
    "text": "problem, what you'd use to\ncreate a recommender system. So just like how Netflix\ndecides whether all like",
    "start": "1039310",
    "end": "1046650"
  },
  {
    "text": "a movie, because\nyou like a movie. We might predict\nwhether I'll find a common toxic by whether people\nsimilar to me find it toxic.",
    "start": "1046650",
    "end": "1054039"
  },
  {
    "text": "But we have one additional\ncomplexity here, which is that unlike Netflix,\nessentially, every inference,",
    "start": "1054040",
    "end": "1059529"
  },
  {
    "text": "every prediction is\non a comment that's never been seen by anyone\nin our data set before.",
    "start": "1059530",
    "end": "1066399"
  },
  {
    "text": "Every inference suffers\nfrom what's often called the cold start problem. And now, that is a\nstandard assumption",
    "start": "1066400",
    "end": "1073420"
  },
  {
    "text": "in most classification problems. Classifiers are designed to\ngeneralize to unseen examples.",
    "start": "1073420",
    "end": "1078748"
  },
  {
    "text": "But in a recommender\nsystem, it means that we need to use what's\ncalled a hybrid approach",
    "start": "1078748",
    "end": "1084250"
  },
  {
    "text": "and rely heavily on\ncontent features. So to do that, we started out\nwith a deep and cross network, which is a hybrid architecture\ndesigned for sparse web scale",
    "start": "1084250",
    "end": "1091450"
  },
  {
    "text": "data sets by Google. But given how heavily we needed\nto rely on our content features,",
    "start": "1091450",
    "end": "1097030"
  },
  {
    "text": "it was helpful for us to\nnot just train our content embeddings from scratch,\nwhich is what was typically",
    "start": "1097030",
    "end": "1102490"
  },
  {
    "text": "done today, but to instead\nstart our content embeddings off with a pre-trained\nlarge language model.",
    "start": "1102490",
    "end": "1107710"
  },
  {
    "text": "And then fine tune that\nwithin the recommender system from there. So, essentially,\nwhat this is doing is borrowing the same\npre-trained models used",
    "start": "1107710",
    "end": "1115630"
  },
  {
    "text": "for classification and NLP. And then integrating them into\na hybrid recommender system.",
    "start": "1115630",
    "end": "1120684"
  },
  {
    "text": " And so on a technical level,\nhow does that perform?",
    "start": "1120685",
    "end": "1127770"
  },
  {
    "text": "Well, I want to start\nwith one cool observation. So if we go all the way back to\nthis traditional aggregated task",
    "start": "1127770",
    "end": "1137010"
  },
  {
    "text": "that we talked about at the\nstart of the talk, where our goal is to predict this\ngray label at the bottom,",
    "start": "1137010",
    "end": "1142679"
  },
  {
    "text": "it turns out that our model\narchitecture can, actually, do that a lot better than\na typical state of the art",
    "start": "1142680",
    "end": "1149670"
  },
  {
    "text": "classifier. So the x-axis here is\nmean absolute error. The top bar is for standard\nstate-of-the-art classifier.",
    "start": "1149670",
    "end": "1157860"
  },
  {
    "text": "The bottom bar is jury learning. Lower is better. And so why is jury\nlearning do better here?",
    "start": "1157860",
    "end": "1164388"
  },
  {
    "text": "Well, I think this\nfinding highlights the inherent instability\nof the traditional approach was that the value\nof that gray label",
    "start": "1164388",
    "end": "1171059"
  },
  {
    "text": "was really going to change\nbased on who the annotators are. And so when we make predictions\nbased on who the annotators are,",
    "start": "1171060",
    "end": "1179580"
  },
  {
    "text": "and then aggregate, then\nwe can do better than if we tried to directly\npredict aggregated labels.",
    "start": "1179580",
    "end": "1187300"
  },
  {
    "text": "That's some thoughts\non the technical side. But now, let's talk\nabout the user side, because, really, our motivation\nin creating jury learning",
    "start": "1187300",
    "end": "1194200"
  },
  {
    "text": "was that by providing these\nlevers by asking people to make their values\nexplicit, you're",
    "start": "1194200",
    "end": "1200679"
  },
  {
    "text": "often going to get a more\ndiverse representation in who's making decisions. And that's exactly what we\nsaw in our field evaluations.",
    "start": "1200680",
    "end": "1207039"
  },
  {
    "text": "We recruited 18 moderators\nfrom Reddit and Discord. We gave these moderators\na jury learning-- sorry.",
    "start": "1207040",
    "end": "1214929"
  },
  {
    "text": "We gave these moderators\na jury learning interface and asked them to compose\na jury that they'd",
    "start": "1214930",
    "end": "1221440"
  },
  {
    "text": "want to use in their community. And we can then compare\nthe existing distribution",
    "start": "1221440",
    "end": "1227440"
  },
  {
    "text": "of annotators in our data\nset, what we've been calling this implicit\njury, to the juries",
    "start": "1227440",
    "end": "1233800"
  },
  {
    "text": "that moderators chose to create. And what we can see here\nis that, for instance, in terms of race\nrepresentation, they certainly",
    "start": "1233800",
    "end": "1241900"
  },
  {
    "text": "do represent a much broader\ncross-section of society. And these moderator\ncomposed juries.",
    "start": "1241900",
    "end": "1247789"
  },
  {
    "text": "They behaved differently. They made different predictions\nthan a standard aggregated",
    "start": "1247790",
    "end": "1252950"
  },
  {
    "text": "classifier. And those predictions\nchanged for some of the most contentious and\ndivisive issues, when it really",
    "start": "1252950",
    "end": "1258650"
  },
  {
    "text": "matters whose voice\nyou're listening to. We found that 14% of\nclassification decision change.",
    "start": "1258650",
    "end": "1265880"
  },
  {
    "text": "And these comments\non the edge cases-- these comments at the\nedge are really important, because behaviors in edge cases\ntend to become de facto platform",
    "start": "1265880",
    "end": "1274440"
  },
  {
    "text": "policies.  So this approach also enables\nsome cool opportunities.",
    "start": "1274440",
    "end": "1282590"
  },
  {
    "start": "1277000",
    "end": "1641000"
  },
  {
    "text": "For instance, we\ncan easily spin up different juries conditioned\non the particular content",
    "start": "1282590",
    "end": "1287930"
  },
  {
    "text": "or situation. So yes, you could\ncompose different juries for different\nonline communities.",
    "start": "1287930",
    "end": "1293900"
  },
  {
    "text": "But you could also, for example,\ncompose a jury consisting of medical professionals\nwhenever health-related content",
    "start": "1293900",
    "end": "1301010"
  },
  {
    "text": "comes up. Another cool opportunity,\ncounterfactual juries.",
    "start": "1301010",
    "end": "1306880"
  },
  {
    "text": "So we can ask if the\njury that I composed says this comment\nis toxic, how much",
    "start": "1306880",
    "end": "1312610"
  },
  {
    "text": "might we need to change\nthe jury composition by to flip that outcome?",
    "start": "1312610",
    "end": "1318340"
  },
  {
    "text": "We can set that up as a\nquadratic program solvable via convex optimization. And that allows\nsensemaking on like,",
    "start": "1318340",
    "end": "1324850"
  },
  {
    "text": "well, I thought\ngiven the composition of this jury, clearly, they\nwould have found this toxic.",
    "start": "1324850",
    "end": "1330100"
  },
  {
    "text": "But they didn't. So what's going on here?  And now, taking\na step back here.",
    "start": "1330100",
    "end": "1338309"
  },
  {
    "text": "Important to make\nclear that while we view jury learning as a\nstep forward, it's by no means a panacea.",
    "start": "1338310",
    "end": "1343730"
  },
  {
    "text": "It's going to create or really\nbring previously hidden problems up to the forefront.",
    "start": "1343730",
    "end": "1350179"
  },
  {
    "text": "One challenge-- jurors\ncan have biases. So, for instance, if you\nselect a medical doctor,",
    "start": "1350180",
    "end": "1355340"
  },
  {
    "text": "there's a small,\nbut non-zero chance that you'll get someone who's\nspreading misinformation about vaccines.",
    "start": "1355340",
    "end": "1361559"
  },
  {
    "text": "Our perspective here\nis that there's really two orthogonal and\ncomplementary worldviews.",
    "start": "1361560",
    "end": "1367710"
  },
  {
    "text": "So today, most approaches\nassume that the world is broken. And then try to apply post-talk\nstatistical transformations",
    "start": "1367710",
    "end": "1376980"
  },
  {
    "text": "to de-bias model outputs. Now, with jury learning, we're\nsaying that it could also be really useful\nto select or train",
    "start": "1376980",
    "end": "1384150"
  },
  {
    "text": "the people whose voices\nshould influence the model. And work in social\nscience and HCI",
    "start": "1384150",
    "end": "1389520"
  },
  {
    "text": "tells us that it's often more\ntractable to do the latter.",
    "start": "1389520",
    "end": "1394750"
  },
  {
    "text": "A second consideration--\nwho determines these jury compositions? Well, again, every data set\nalready has an implicit jury.",
    "start": "1394750",
    "end": "1403180"
  },
  {
    "text": "What jury learning does is make\nthat fact explicit and visible and easily changeable. It's providing this\nnew way to communicate",
    "start": "1403180",
    "end": "1411970"
  },
  {
    "text": "and debate whose perspectives\nare being included. We think that\npractitioners should make",
    "start": "1411970",
    "end": "1417250"
  },
  {
    "text": "their compositions transparent. So they can be\ndiscussed and debated. ",
    "start": "1417250",
    "end": "1423670"
  },
  {
    "text": "Finally, it's just important to\nnote that this jury metaphor is not an isomorphism. We wanted, again, this\ncrisp way to communicate,",
    "start": "1423670",
    "end": "1432500"
  },
  {
    "text": "that there is a group of\nindividual decision makers from which a single\ndecision must emerge.",
    "start": "1432500",
    "end": "1437990"
  },
  {
    "text": "Then we have some notion of\na juror selection process. Practitioners decide who the\ndecision makers are going to be.",
    "start": "1437990",
    "end": "1445850"
  },
  {
    "text": "But there's definitely\na tension here between what jury\nlearning does today and what juries offer in\npractice and legal systems.",
    "start": "1445850",
    "end": "1453799"
  },
  {
    "text": "We view that tension as\na feature, not a bug, giving us a path to\nreflect on the existing",
    "start": "1453800",
    "end": "1460070"
  },
  {
    "text": "literature on the biases\nand challenges of juries. So, for instance, we're now\nworking on tools and methods",
    "start": "1460070",
    "end": "1465710"
  },
  {
    "text": "for participatory\ndetermination of the jury. We've also been thinking about\ndeliberation within juries.",
    "start": "1465710",
    "end": "1471785"
  },
  {
    "text": " Now, moving forward,\nI've been really heartened to see that\nover the past year",
    "start": "1471785",
    "end": "1477679"
  },
  {
    "text": "or so, since we first started\npublishing in this space, AI researchers are increasingly\nrethinking disagreement.",
    "start": "1477680",
    "end": "1485120"
  },
  {
    "text": "For instance, AI researchers\nthinking about misinformation and toxicity, data annotation,\nevaluation approaches,",
    "start": "1485120",
    "end": "1491630"
  },
  {
    "text": "explainable AI, have\nall cited jury learning. Concurrently, we've seen\nnew technical approaches",
    "start": "1491630",
    "end": "1496790"
  },
  {
    "text": "for modeling\nindividual annotators. But, perhaps, even\nmore exciting to me",
    "start": "1496790",
    "end": "1503039"
  },
  {
    "text": "is that jury learning could\nalso create new opportunities in human computer interaction.",
    "start": "1503040",
    "end": "1508260"
  },
  {
    "text": "So, for instance, in a project\nled by my lab mate Michelle Lamb, we were able to use the\njury learning model architecture",
    "start": "1508260",
    "end": "1513810"
  },
  {
    "text": "to create interactive systems\nfor empowering end user audits, which you're\nseeing a video of now.",
    "start": "1513810",
    "end": "1520245"
  },
  {
    "text": " And while, so far, I focused\non content moderation,",
    "start": "1520245",
    "end": "1526550"
  },
  {
    "text": "I think that this\nidea that we can be is adding interactions\nthat empower",
    "start": "1526550",
    "end": "1531620"
  },
  {
    "text": "end users to control whose\nvoices are model to emulate. I think that extends\nfar more broadly.",
    "start": "1531620",
    "end": "1536690"
  },
  {
    "text": "So, for instance,\nif you're using a generative artistic\nco-creation model like DALL-E",
    "start": "1536690",
    "end": "1542630"
  },
  {
    "text": "and ask it to generate an\nastronaut riding a horse, it clearly matters, which\nartists, which school of",
    "start": "1542630",
    "end": "1547700"
  },
  {
    "text": "thought the model emulates. Another area-- translation.",
    "start": "1547700",
    "end": "1553010"
  },
  {
    "text": "While translation is sometimes\nan exact word for word match. Often, different people might\ntranslate the same sentence",
    "start": "1553010",
    "end": "1560090"
  },
  {
    "text": "very differently. So which translator\nshould you model emulate? Summarization-- imagine asking\na republican versus a democrat",
    "start": "1560090",
    "end": "1568270"
  },
  {
    "text": "to summarize a news article. Even captioning an image\nfor visually impaired users.",
    "start": "1568270",
    "end": "1573698"
  },
  {
    "text": "These are all spaces\nin which I think we can be designing these\nnew interactive systems that help us make\nexplicit, carefully,",
    "start": "1573698",
    "end": "1580280"
  },
  {
    "text": "considered choices about whose\nvoices are models or emulating. ",
    "start": "1580280",
    "end": "1587070"
  },
  {
    "text": "So we spent some time\ntalking about what we can do about disagreements on\nthe data model and interaction",
    "start": "1587070",
    "end": "1593070"
  },
  {
    "text": "side. I want to move down this\npipeline to evaluation here. What does the style\nof disagreement",
    "start": "1593070",
    "end": "1599490"
  },
  {
    "text": "mean for evaluation? And here's I want to ask. So jury learning is this\nambitious new approach.",
    "start": "1599490",
    "end": "1607120"
  },
  {
    "text": "It requires rethinking how\nwe collect our data sets, how we train our models. But what if the model is fixed?",
    "start": "1607120",
    "end": "1614230"
  },
  {
    "text": "What can we still do about\nsocietal disagreement when we're creating\nhuman AI systems?",
    "start": "1614230",
    "end": "1620450"
  },
  {
    "text": "I'd posit that one\nvital step is to, at least, ensure that our\nevaluation metrics are not",
    "start": "1620450",
    "end": "1627890"
  },
  {
    "text": "hiding the user experience that\nthese models can, actually, deliver. And why is that important?",
    "start": "1627890",
    "end": "1633430"
  },
  {
    "text": "Well, my theory\nof change here is that we will make different\nproduct decisions, if we realign",
    "start": "1633430",
    "end": "1639130"
  },
  {
    "text": "our performance measures. So how can we evaluate today's\nclassifiers under disagreement?",
    "start": "1639130",
    "end": "1646640"
  },
  {
    "start": "1641000",
    "end": "1771000"
  },
  {
    "text": "Well, let's start by talking\nabout today's metrics. So same example that\nwe've been using, this common toxicity task.",
    "start": "1646640",
    "end": "1653630"
  },
  {
    "text": "There are Kaggle\ncompetitions and so on. And if you look at the ROC AUC\nscores, where 1.0 means perfect,",
    "start": "1653630",
    "end": "1660710"
  },
  {
    "text": "0.5 means basically random. These things look like they're\ndoing really amazingly.",
    "start": "1660710",
    "end": "1667580"
  },
  {
    "text": "0.95, this isn't quite\nlike class grading. But it looks like they're\ngetting an A here.",
    "start": "1667580",
    "end": "1673169"
  },
  {
    "text": "But in reality,\nthese classifiers appear to be a study in\ncontrast, where on the one hand,",
    "start": "1673170",
    "end": "1678840"
  },
  {
    "text": "we have our metrics telling\nus that we're almost maxed out on this task. On the other hand, we have\naudits, and press articles,",
    "start": "1678840",
    "end": "1686010"
  },
  {
    "text": "and [? RNI ?] is telling us\nthat we're really struggling. That there's maybe\na lot of users who are not happy with how\nthese classifiers behave.",
    "start": "1686010",
    "end": "1694389"
  },
  {
    "text": "And so it appears that\nsomething implicitly, perhaps, is not\nbeing captured here. That maybe there is a dark\nmatter that we're not observing.",
    "start": "1694390",
    "end": "1703060"
  },
  {
    "text": "And now, over 30\nminutes into this talk, you, actually, have\na sense of where I'm going with this, which is\ndisagreement, specifically,",
    "start": "1703060",
    "end": "1710200"
  },
  {
    "text": "that today's common\nmetrics flatten opinions into a single ground\ntruth, label per example,",
    "start": "1710200",
    "end": "1718240"
  },
  {
    "text": "incorrectly ignoring\ndisagreement. And just to give a super\nsimple demonstration of this,",
    "start": "1718240",
    "end": "1724809"
  },
  {
    "text": "let's say that two people\nsaid their comment was toxic. And three said it was non-toxic.",
    "start": "1724810",
    "end": "1731500"
  },
  {
    "text": "Metrics that aggregate\nvia majority vote would assign 100%\naccuracy to a classifier",
    "start": "1731500",
    "end": "1737980"
  },
  {
    "text": "that predicts non-toxic. But if we were measuring\nuser facing performance,",
    "start": "1737980",
    "end": "1744320"
  },
  {
    "text": "we would never say here\nthat 100% of users approve. We'd report that three\napproved the system response.",
    "start": "1744320",
    "end": "1750470"
  },
  {
    "text": "And two did not. And so, specifically, what\nI'm going to demonstrate is now that 0.95 ROC\nAUC score is inflated,",
    "start": "1750470",
    "end": "1759440"
  },
  {
    "text": "as are many scores in\nsocial computing tasks. We're going to show you that\nspecific score should, actually,",
    "start": "1759440",
    "end": "1764720"
  },
  {
    "text": "be closer to a 0.73,\nwhich is this much less convincing number. ",
    "start": "1764720",
    "end": "1772530"
  },
  {
    "start": "1771000",
    "end": "1947000"
  },
  {
    "text": "And the technique, I\ndeveloped to do this, is called the disagreement\ndeconvolution. And the idea is\nthat we should be",
    "start": "1772530",
    "end": "1779880"
  },
  {
    "text": "able to transform any of\ntoday's standard classification evaluation measures to better\nrepresent how classifiers",
    "start": "1779880",
    "end": "1788280"
  },
  {
    "text": "perform under disagreement. So can we create a\ntwist that brings us closer to how HCI has\nlearned to evaluate systems?",
    "start": "1788280",
    "end": "1798687"
  },
  {
    "text": "And how are we going to do that? We're going back to my\nthesis statement here. Instead of measuring predictions\nagainst some aggregated pseudo",
    "start": "1798687",
    "end": "1806400"
  },
  {
    "text": "human, we're going\nto directly embed a representation of individual\npeople into today's ML metrics.",
    "start": "1806400",
    "end": "1815300"
  },
  {
    "text": "And so intuitively,\nwhat you'd want here-- and if you agree\nwith me thus far, this shouldn't be a\nvery scary claim--",
    "start": "1815300",
    "end": "1821293"
  },
  {
    "text": "is that we should be\ncomparing predictions to each annotator's belief.",
    "start": "1821293",
    "end": "1827300"
  },
  {
    "text": "Essentially, inserting one label\nper annotator into the test set.",
    "start": "1827300",
    "end": "1832830"
  },
  {
    "text": "So in this case,\nnow, the classifier predicts toxic or receive a\nscore and accuracy of 0.6,",
    "start": "1832830",
    "end": "1838770"
  },
  {
    "text": "because three annotators\nagree with that prediction. And two annotators disagree. In fact, there is no\nway here to get a 1.0.",
    "start": "1838770",
    "end": "1846480"
  },
  {
    "text": "The best you can do is 0.6. And, of course, we\ncould then compute almost any existing popular\nmetric using this new test set.",
    "start": "1846480",
    "end": "1857680"
  },
  {
    "text": "And so, essentially,\nour approach is to ask not what proportion\nof ground truth labels",
    "start": "1857680",
    "end": "1863440"
  },
  {
    "text": "does the classifier agree with. But instead, what\nproportion of the population",
    "start": "1863440",
    "end": "1869140"
  },
  {
    "text": "does the classifier agree with? And our goal in doing\nso is to incorporate an estimate of how\ncontested each label is",
    "start": "1869140",
    "end": "1877179"
  },
  {
    "text": "going to be into our ML metrics\nin an interpretable way. But why do they\nsay estimate here?",
    "start": "1877180",
    "end": "1884470"
  },
  {
    "text": "Well, it turns out that\nthis problem is harder than it sounds. In reality, things are\na bit more complicated.",
    "start": "1884470",
    "end": "1890139"
  },
  {
    "text": "We can't just take\nthese labels as is. There are really two possible\nviews of labels here.",
    "start": "1890140",
    "end": "1896200"
  },
  {
    "text": "So in tasks, like,\nimage classification, that you might expect that\nalmost all of the label",
    "start": "1896200",
    "end": "1902080"
  },
  {
    "text": "uncertainty is going to\ncome from annotator error. And then the right\nthing to do is, indeed,",
    "start": "1902080",
    "end": "1907570"
  },
  {
    "text": "to aggregate the label\nacross multiple annotators as a standard today.",
    "start": "1907570",
    "end": "1912720"
  },
  {
    "text": "And these more\ncontroversial tasks, you might argue instead that there\nis no such thing as noise.",
    "start": "1912720",
    "end": "1919410"
  },
  {
    "text": "All disagreement\nin the labels is going to come from\nunobserved features. And in that case, what\nwe should be doing",
    "start": "1919410",
    "end": "1925200"
  },
  {
    "text": "is called disaggregation. What I just showed you, where\nyou don't average across",
    "start": "1925200",
    "end": "1930330"
  },
  {
    "text": "annotators at all. So you then evaluate\nmodels against all these individual labels,\nacknowledging that no model can",
    "start": "1930330",
    "end": "1935363"
  },
  {
    "text": "achieve this perfect accuracy. Now, the reality,\nof course, is going to be somewhere in between.",
    "start": "1935363",
    "end": "1941070"
  },
  {
    "text": "These contested tasks,\nlike, toxicity, clearly, do include variation\nfrom personal beliefs.",
    "start": "1941070",
    "end": "1947539"
  },
  {
    "start": "1947000",
    "end": "2044000"
  },
  {
    "text": "But we also know that some\nannotations might be noise. There might be\nmisunderstandings.",
    "start": "1947540",
    "end": "1953780"
  },
  {
    "text": "Lapses in attention. Maybe, your office\nmate knocks on the door while you're labeling data. Accidental clicks.",
    "start": "1953780",
    "end": "1959180"
  },
  {
    "text": "Essentially, there is generally\na consensus that we cannot take label annotations as\nstrict face value either.",
    "start": "1959180",
    "end": "1967350"
  },
  {
    "text": "And so our approach\nhere is going to be to try to remove\nthese sources of noise,",
    "start": "1967350",
    "end": "1972870"
  },
  {
    "text": "leaving only the\nirresolvable disagreement. So how can we estimate\nwhether the disagreement",
    "start": "1972870",
    "end": "1980090"
  },
  {
    "text": "is noise or sincere beliefs? What we're going to start\nwith a simplifying assumption.",
    "start": "1980090",
    "end": "1985490"
  },
  {
    "text": "Our idealized model\nof the world is going to be a generative model.",
    "start": "1985490",
    "end": "1990919"
  },
  {
    "text": "We're going to assume that each\nannotator has one stable belief. And when an annotator\nlabel an example,",
    "start": "1990920",
    "end": "1997700"
  },
  {
    "text": "we assume that people\nwill, generally, answer that stable belief,\ntheir true opinion. And that's reflected in the\nlabels that they're generating.",
    "start": "1997700",
    "end": "2006410"
  },
  {
    "text": "But sometimes, they're going\nto generate a different answer. Maybe, they've been more\nlikely to flip their answer on really tough questions, less\nlikely to flip on easy ones.",
    "start": "2006410",
    "end": "2014692"
  },
  {
    "text": "Essentially, we're\ngoing to say that they have a certain probability\nof flipping away",
    "start": "2014692",
    "end": "2020630"
  },
  {
    "text": "from that stable belief,\nconditioned on the item. And we're going to call\nthat probability p flip.",
    "start": "2020630",
    "end": "2027860"
  },
  {
    "text": "And it turns out that\nif we know p flip, then we can resample a new test set\nby adjusting the observed label",
    "start": "2027860",
    "end": "2034880"
  },
  {
    "text": "distribution, as if they\ndid not flip at all. And while there's\nnot time to get into all the details\nof this approach,",
    "start": "2034880",
    "end": "2040250"
  },
  {
    "text": "I want to try to give a\nbrief intuition of how we can find that p flip value.",
    "start": "2040250",
    "end": "2046169"
  },
  {
    "start": "2044000",
    "end": "2115000"
  },
  {
    "text": "And so our key insight here\nis that many crowdsourced data sets already contain\nthe data that we need.",
    "start": "2046170",
    "end": "2053669"
  },
  {
    "text": "And we've developed a few\ndifferent plug-in estimators, depending on what\nthe data looks like.",
    "start": "2053670",
    "end": "2059230"
  },
  {
    "text": "So first, we're going to use\nwhat's called test-retest data. So crowdsourced data\nsets often do this thing",
    "start": "2059230",
    "end": "2065129"
  },
  {
    "text": "that you would refer to\nsometimes as inserting attention checks, where,\nessentially, you'll",
    "start": "2065130",
    "end": "2070980"
  },
  {
    "text": "ask the same question twice. And then see if people\nagree with themselves.",
    "start": "2070980",
    "end": "2076530"
  },
  {
    "text": "That's what test-retest data is. So now, consider some\nimagined scenario in which we had unlimited\ntest-retest data in which we",
    "start": "2076530",
    "end": "2084179"
  },
  {
    "text": "asked the same annotator\nthe same question, an unlimited number of times. But we wiped their mind\neach time we did that.",
    "start": "2084179",
    "end": "2091199"
  },
  {
    "text": "Well, from that data, you could\nthen directly observe p flip. It would be the disagreement\nrate with their majority label.",
    "start": "2091199",
    "end": "2099660"
  },
  {
    "text": "Now, of course, in practice,\nthis test-retest data is not unlimited. It's often quite sparse.",
    "start": "2099660",
    "end": "2105210"
  },
  {
    "text": "And so the technical\nchallenge here, that I'm omitting for this talk,\nis how we stratify and aggregate",
    "start": "2105210",
    "end": "2111839"
  },
  {
    "text": "that sparse test-retest data.  Now, second.",
    "start": "2111840",
    "end": "2117470"
  },
  {
    "start": "2115000",
    "end": "2138000"
  },
  {
    "text": "If there's insufficient\ntest-retest data for that, then we can,\nactually, set this up",
    "start": "2117470",
    "end": "2122569"
  },
  {
    "text": "as a collaborative filtering\nproblem, where we essentially try to impute. We try to predict every\nannotator stable belief.",
    "start": "2122570",
    "end": "2130819"
  },
  {
    "text": "And then we can\nmeasure how often an annotator's observe\nlabels disagree with these predicted stable labels.",
    "start": "2130820",
    "end": "2139180"
  },
  {
    "text": "And so what do we do once\nwe have that flip rate estimate, that p flip, that\nestimate of the amount of label",
    "start": "2139180",
    "end": "2144310"
  },
  {
    "text": "disagreements\narising from noise? Well, we can then use it to\ntransform our existing test set.",
    "start": "2144310",
    "end": "2150490"
  },
  {
    "text": "So for each annotated\nitem, we can take here the empirical frequency\nof annotator labels.",
    "start": "2150490",
    "end": "2157270"
  },
  {
    "text": "And adjust it for\nthe effect of noise. This takes the form of a\nsimple noise level subtraction and normalization to one.",
    "start": "2157270",
    "end": "2163609"
  },
  {
    "text": "And then given this\nadjusted label distribution, we can sample labels from it.",
    "start": "2163610",
    "end": "2168890"
  },
  {
    "text": "And treat these\nsamples as representing the noise-free\nopinions of annotators.",
    "start": "2168890",
    "end": "2175040"
  },
  {
    "text": "And when we apply\nthis, what we see is that many classifiers look\nway better than they actually",
    "start": "2175040",
    "end": "2180855"
  },
  {
    "text": "are when you account\nfor disagreements. So problems that appear solved\nin reality are really not.",
    "start": "2180855",
    "end": "2186440"
  },
  {
    "text": "So, again, here is that state\nof the art toxicity from before. That 0.95 from the leaderboard.",
    "start": "2186440",
    "end": "2194180"
  },
  {
    "text": "When we adjusted\nfor disagreement, we get that 0.73\nI was arguing for. So you go from your straight to\nA to more like a C minus here.",
    "start": "2194180",
    "end": "2202520"
  },
  {
    "text": "So current state-of-the-art\ntoxicity claims to be that 0.95. And our deconvolution\nreduces to 0.73.",
    "start": "2202520",
    "end": "2207619"
  },
  {
    "text": "The next thing we\nwant to point out here is that we can, actually,\ndo the same thing now, but with what's called\nan Oracle classifier.",
    "start": "2207620",
    "end": "2216200"
  },
  {
    "text": "And what that means\nis, imagine you had a magic AI that always\nguessed, quote, unquote,",
    "start": "2216200",
    "end": "2222360"
  },
  {
    "text": "\"corrected.\" It always guessed what\nthe majoritarian view is going to be. On no-classifer, we\nare able to show,",
    "start": "2222360",
    "end": "2229640"
  },
  {
    "text": "will ever do better\nthan that 0.73, because, even if this\nOracle, this magic classifier",
    "start": "2229640",
    "end": "2237800"
  },
  {
    "text": "always guessed that quote,\nunquote \"right thing,\" there's a lot of people who are\ngoing to disagree with that.",
    "start": "2237800",
    "end": "2243710"
  },
  {
    "text": "And so it turns out that\non a technical level, we are basically maxed\nout on this task, given",
    "start": "2243710",
    "end": "2250160"
  },
  {
    "text": "the inherent amount of\ndisagreement in the population. We're never going to be able\nto do much better than this.",
    "start": "2250160",
    "end": "2256339"
  },
  {
    "text": "And across a bunch of other\nsocial computing tasks ranging from misinformation detection\nto adult content detection",
    "start": "2256340",
    "end": "2262670"
  },
  {
    "text": "and so on, we just see these\nlarge drops, huge corrections",
    "start": "2262670",
    "end": "2268220"
  },
  {
    "text": "across the board based on\ndisagreement about what the right answer ought to be. But in traditional AI\ntasks that are, again,",
    "start": "2268220",
    "end": "2275420"
  },
  {
    "text": "more perceptual like word\nsense disambiguation, image classification, we get this\nmuch softer correction,",
    "start": "2275420",
    "end": "2282319"
  },
  {
    "text": "because people tend to agree\non what the right answer ought to be. And so what I want\nto point out here",
    "start": "2282320",
    "end": "2288170"
  },
  {
    "text": "is that even if we can't\nchange the model, simply by paying attention to\ndisagreements, we can, at least,",
    "start": "2288170",
    "end": "2295090"
  },
  {
    "text": "hopefully, really\nchange how we think. And that can help\nlead us to design better interactive systems.",
    "start": "2295090",
    "end": "2300940"
  },
  {
    "text": "And, maybe, lead you to use\nsomething like jury learning.  Now, with that.",
    "start": "2300940",
    "end": "2306100"
  },
  {
    "text": "I'm going to rest my\ndisagreement case. And so in the two\nparts of this talk, I've told you how these\nkinds of new interactions",
    "start": "2306100",
    "end": "2314200"
  },
  {
    "text": "can help with societal\ndisagreements, how we can design interactive systems that help\nus explicitly reason over",
    "start": "2314200",
    "end": "2320080"
  },
  {
    "text": "disagreement, how we can\ncreate evaluation metrics that reflect the user experience\nthat models actually deliver.",
    "start": "2320080",
    "end": "2328220"
  },
  {
    "text": "And there's, of course,\nmore that we can do here, when thinking about\ndisagreements, to start to loosen some\nof these assumptions",
    "start": "2328220",
    "end": "2334910"
  },
  {
    "text": "that our work has made so far. So, for instance, people don't\nalways have a single belief.",
    "start": "2334910",
    "end": "2341600"
  },
  {
    "text": "They can be uncertain. They can hold conflicting views. Disagreements aren't\nall weighted equally.",
    "start": "2341600",
    "end": "2347330"
  },
  {
    "text": "Think social choice theory. And there are new tensions\nin data set collection here to think about, how\nwe balance clear task",
    "start": "2347330",
    "end": "2354410"
  },
  {
    "text": "designs while still\neliciting sincere beliefs. But, really, the\npoint I want to make",
    "start": "2354410",
    "end": "2361920"
  },
  {
    "text": "is that more and\nmore things that we would think of as human tasks,\nhuman judgment and subjectivity",
    "start": "2361920",
    "end": "2368430"
  },
  {
    "text": "are now open to AI systems. So as we start moving into\nthese higher and higher level cognition tasks,\nwe're taking these things",
    "start": "2368430",
    "end": "2375600"
  },
  {
    "text": "that traditionally\npeople have done. And we're going to be\nnavigating large design spaces",
    "start": "2375600",
    "end": "2380640"
  },
  {
    "text": "with many possible answers. And if that's going\nto be happening,",
    "start": "2380640",
    "end": "2385680"
  },
  {
    "start": "2383000",
    "end": "2439000"
  },
  {
    "text": "then, I think, our\npoint of view needs to shift from just\nbeing right to how do",
    "start": "2385680",
    "end": "2390720"
  },
  {
    "text": "we use AI to help make people-- to help people make\nsmarter decisions. And so coming back to these\nfoundational ideas in HCI,",
    "start": "2390720",
    "end": "2398670"
  },
  {
    "text": "the real insight behind\nwhat I'm doing here is not about\ndisagreements per se. It's about reimagining these\ninteractions and models",
    "start": "2398670",
    "end": "2405540"
  },
  {
    "text": "and metrics to create these\ninteractive systems that help make us smarter.",
    "start": "2405540",
    "end": "2410849"
  },
  {
    "text": "And so with jury learning,\nwe reimagine today's supervised learning pipeline,\nas a tool for intelligence",
    "start": "2410850",
    "end": "2417720"
  },
  {
    "text": "augmentation that puts\ndifferent perspectives and voices at your command.",
    "start": "2417720",
    "end": "2423030"
  },
  {
    "text": "And we adapted a jury\nmetaphor as a representation,",
    "start": "2423030",
    "end": "2428310"
  },
  {
    "text": "both in the interaction\nand the model itself. And in the disagreement\ndeconvolution,",
    "start": "2428310",
    "end": "2434750"
  },
  {
    "text": "we reimagine how evaluation\nmetrics represent end users.",
    "start": "2434750",
    "end": "2440210"
  },
  {
    "start": "2439000",
    "end": "2533000"
  },
  {
    "text": "And so thinking beyond toxicity\ndetection or jury metaphor is my goal really\nin the large scale",
    "start": "2440210",
    "end": "2445430"
  },
  {
    "text": "is to create these\nhuman AI systems that empower us to adapt new\nperspectives and knowledge",
    "start": "2445430",
    "end": "2451940"
  },
  {
    "text": "towards our goals. That help us see beyond\nour cognitive limits. And put those insights to work\nalgorithmically for end users.",
    "start": "2451940",
    "end": "2459770"
  },
  {
    "text": "And to do that, I\nthink we're going to need to continue to\nreally co-design interactions",
    "start": "2459770",
    "end": "2464930"
  },
  {
    "text": "and models and metrics\nalongside human needs. And I'm interested in designing\ntechniques for human AI",
    "start": "2464930",
    "end": "2471515"
  },
  {
    "text": "interaction not just\nfor single end users, but when we're using AI across,\nmany users across communities,",
    "start": "2471515",
    "end": "2479150"
  },
  {
    "text": "across society. And so what might\nthat agenda look like? Well, I want to discuss\na few different ideas.",
    "start": "2479150",
    "end": "2486740"
  },
  {
    "text": "Starting with the\ninteraction layer, how can we draw on the knowledge\nembedded in these models",
    "start": "2486740",
    "end": "2492880"
  },
  {
    "text": "to help make us smarter? To me, the most exciting natural\nnext step here is in asking,",
    "start": "2492880",
    "end": "2498530"
  },
  {
    "text": "how we can not just model the\ndecisions that different people would make.",
    "start": "2498530",
    "end": "2503540"
  },
  {
    "text": "But also how they\nmake those decisions. So a kinesthetic\npair of glasses,",
    "start": "2503540",
    "end": "2509089"
  },
  {
    "text": "could a radiologist see\nwhat an oncologist would notice about a CT scan. Could a student\ndesigner see a gallery",
    "start": "2509090",
    "end": "2515990"
  },
  {
    "text": "of how Bauhaus or modernist\nor other designers might react to their work. I have, actually, been\ntalking to some doctors",
    "start": "2515990",
    "end": "2521477"
  },
  {
    "text": "at UCSF, who want\nthis exact capability. And so I'm excited\nto keep exploring how we can model another\nperson's expertise",
    "start": "2521477",
    "end": "2528740"
  },
  {
    "text": "and create the interactive\ntools that help end users learn from it.",
    "start": "2528740",
    "end": "2534359"
  },
  {
    "start": "2533000",
    "end": "2604000"
  },
  {
    "text": "Second, creating a science\nof human evaluation. How we evaluate these\nmodels is vitally important.",
    "start": "2534360",
    "end": "2541790"
  },
  {
    "text": "And so how can we overcome\ntoday's false separation of concerns between the\ntechnical performance",
    "start": "2541790",
    "end": "2548330"
  },
  {
    "text": "of a model, and the\nuser experience they provide with an interaction? I think there are\ntwo approaches here.",
    "start": "2548330",
    "end": "2555020"
  },
  {
    "text": "One approach is to find clever\nways to embed the ideas of HCI",
    "start": "2555020",
    "end": "2561050"
  },
  {
    "text": "into today's AI benchmarks. I've done that in the\ndistributed convolution.",
    "start": "2561050",
    "end": "2567500"
  },
  {
    "text": "I also did that in\na nurse paper called Hype, which was learning\npsychometric functions to evaluate generative models.",
    "start": "2567500",
    "end": "2573050"
  },
  {
    "text": "That's one approach. The second approach\nis about how we can take what we've learned\nabout user centered evaluation",
    "start": "2573050",
    "end": "2579740"
  },
  {
    "text": "in HCI and amplify that\nup to the scale needed to iterate, not\njust on UI design,",
    "start": "2579740",
    "end": "2585028"
  },
  {
    "text": "but on models themselves. I think a key to\nachieving that is going to be in creating formal\nmethods around how we can",
    "start": "2585028",
    "end": "2591770"
  },
  {
    "text": "combine the complementary\nstrengths of human evaluations, which are great at reasoning\nover these point-wise instances",
    "start": "2591770",
    "end": "2598010"
  },
  {
    "text": "of quality and usability, fun\nand automated evaluations, which can reason over a large\nscale set of a model's outputs.",
    "start": "2598010",
    "end": "2605230"
  },
  {
    "start": "2604000",
    "end": "2653000"
  },
  {
    "text": "And then, finally, thinking\nabout our data sets and models, one key challenge here.",
    "start": "2605230",
    "end": "2610270"
  },
  {
    "text": "How can we discover the dark\nmatter in our data sets? So much like the jury metaphor,\nborrowing a physics metaphor",
    "start": "2610270",
    "end": "2617839"
  },
  {
    "text": "here. And the question is,\nhow can we create tools that afford\nreasoning over,",
    "start": "2617840",
    "end": "2623720"
  },
  {
    "text": "not just the existing\ndata, but also tools to reason over what's\nnot yet in your data set?",
    "start": "2623720",
    "end": "2629930"
  },
  {
    "text": "That dark matter that\nhasn't been observed. So, for instance, let's\nsay that you're using ChatGPT to critique an essay.",
    "start": "2629930",
    "end": "2637160"
  },
  {
    "text": "Whose perspectives might\nbe missing from a data set used to train that model?",
    "start": "2637160",
    "end": "2642585"
  },
  {
    "text": "And so the question here is, how\ncan we couple human creativity with the power of these recent\nadvances in generative models",
    "start": "2642585",
    "end": "2648300"
  },
  {
    "text": "to discover and start to\ninfill these missing voices, perspectives, and ideas?",
    "start": "2648300",
    "end": "2653310"
  },
  {
    "start": "2653000",
    "end": "3156000"
  },
  {
    "text": "[APPLAUSE] ",
    "start": "2653310",
    "end": "2663800"
  },
  {
    "text": "All right. Break out the vuvuzelas. Definitely, a talk that\nMitchell's never given before.",
    "start": "2663800",
    "end": "2670450"
  },
  {
    "text": "Sorry, if it sounded\na bit rehearsed. But that was my\n15th time giving it. So we have a few minutes\nfor questions, comments,",
    "start": "2670450",
    "end": "2676120"
  },
  {
    "text": "disagreements,\nand anything else. Sneaky. ",
    "start": "2676120",
    "end": "2684400"
  },
  {
    "text": "With the disagreement\ndeconvolution paper, what is your opinion about these\ngoals that we should reach for?",
    "start": "2684400",
    "end": "2691945"
  },
  {
    "text": "Like you were saying, even\nwith the Oracle example, like 0.76 or whatever it was\nlike the best you could do.",
    "start": "2691945",
    "end": "2697990"
  },
  {
    "text": "Should we change what we\nthink as what the best is? What do you think about that?",
    "start": "2697990",
    "end": "2703450"
  },
  {
    "text": "Yeah. That's a great question. So I think the glib\nanswer here is well, jury learning is what I\nthink we should be doing.",
    "start": "2703450",
    "end": "2709432"
  },
  {
    "text": "Essentially, what\nI think we should be doing is given that\nthere's this disagreement, we ought to first be aware\nit exists, not hiding it.",
    "start": "2709432",
    "end": "2717020"
  },
  {
    "text": "And then second, be able to make\nexplicit intentional choices about how we deal with it. That's the vision\nthat I'm trying",
    "start": "2717020",
    "end": "2723655"
  },
  {
    "text": "to pitch here, instead of\njust hiding disagreement, as we tend to do today. ",
    "start": "2723655",
    "end": "2731570"
  },
  {
    "text": "[INAUDIBLE]  Thank you so much. This is great defense talk.",
    "start": "2731570",
    "end": "2737119"
  },
  {
    "text": "I guess one question\nis, first of all, you showed this on toxicity. So do you really\nrequire a data set,",
    "start": "2737120",
    "end": "2744812"
  },
  {
    "text": "where you have all these\ndifferent people's opinions. And so you can really do\nthis fine grained selection of different voices\nto represent?",
    "start": "2744812",
    "end": "2751610"
  },
  {
    "text": "And then also, have you thought\nabout how within a group, there might be diversity.",
    "start": "2751610",
    "end": "2758270"
  },
  {
    "text": "Within the perspective\nof women, there might be like a lot of\ndifferent perspectives, and how do you incorporate that?",
    "start": "2758270",
    "end": "2763555"
  },
  {
    "text": "Great question. So in terms of the data set, so\nback when we built this tool,",
    "start": "2763555",
    "end": "2769460"
  },
  {
    "text": "this was in the very\nearly, even before the days of the generative\nmodels that we have now.",
    "start": "2769460",
    "end": "2775083"
  },
  {
    "text": "And so at the time, I think\nthe perspective was that, yes. We're going to-- we need a data\nset that explicitly collect data",
    "start": "2775083",
    "end": "2782240"
  },
  {
    "text": "from different groups or people\nthat you might care about that we can then model. Now, there are lots of\nrecent work, at least,",
    "start": "2782240",
    "end": "2788630"
  },
  {
    "text": "trying to show that\nwith what we're seeing with these generative\nmodels like ChatGPT, GPT-4, et cetera, embedded\nin them somewhere,",
    "start": "2788630",
    "end": "2796230"
  },
  {
    "text": "different perspectives\nand ideas. And so I think that's\nstill an emerging space. But it might mean that we\nmight need a lot less data,",
    "start": "2796230",
    "end": "2802890"
  },
  {
    "text": "or, at least, a lot\nless explicit data, if we can try to find\nthese different viewpoints",
    "start": "2802890",
    "end": "2808770"
  },
  {
    "text": "within existing models. In terms of different\nviewpoints in single groups,",
    "start": "2808770",
    "end": "2814648"
  },
  {
    "text": "I think that's super important. That was a key design principle\nin creating jury learning. And so, for instance, we\ndon't just let you say,",
    "start": "2814648",
    "end": "2825510"
  },
  {
    "text": "there's one or two people\nfrom a particular group in your data set. We're going to then\nput that in your jury,",
    "start": "2825510",
    "end": "2831240"
  },
  {
    "text": "and model the opinion,\nand there you go. We require that you have a\nlot of people from each group",
    "start": "2831240",
    "end": "2836640"
  },
  {
    "text": "so that we can then model\nthe entire distribution of those people. We have those plots\nI was showing, where you can plot\nindividual people",
    "start": "2836640",
    "end": "2843089"
  },
  {
    "text": "in a group against\nthe entire group to see where they fall\nin the distribution. So I think really important\nto have these sensemaking",
    "start": "2843090",
    "end": "2849270"
  },
  {
    "text": "tools that make it clear\nwhat the whole distribution of opinions look like\nwithin a single group. ",
    "start": "2849270",
    "end": "2863170"
  },
  {
    "text": "You talked a little bit about\nthe aspects of the jury metaphor that are not yet\nrealized, but you're",
    "start": "2863170",
    "end": "2869589"
  },
  {
    "text": "thinking about like the\njury selection process. I was wondering if there\nwere any parts of a jury",
    "start": "2869590",
    "end": "2875950"
  },
  {
    "text": "or the idea of juries that\nyou think are not worth incorporating into this, like\nsomething bad about juries,",
    "start": "2875950",
    "end": "2882790"
  },
  {
    "text": "or we shouldn't bring\ninto the modeling process? That's a good question. So maybe, just\nfirst, I'll briefly",
    "start": "2882790",
    "end": "2889067"
  },
  {
    "text": "note that what is\njury conception looks like that clearly\ndepends on the legal system",
    "start": "2889067",
    "end": "2896353"
  },
  {
    "text": "of the country you're\noperating in, et cetera. So I'm going to just\ntalk here in terms of the Western legal system.",
    "start": "2896353",
    "end": "2902380"
  },
  {
    "text": "But there's a lot\nof great work that has been done by sociologists,\npsychologists, lots of folks",
    "start": "2902380",
    "end": "2908650"
  },
  {
    "text": "getting into some of the\nchallenges and biases that juries have.",
    "start": "2908650",
    "end": "2913930"
  },
  {
    "text": "I think one key thing\nthat's missing right now is deliberation.",
    "start": "2913930",
    "end": "2918993"
  },
  {
    "text": "So that's one thing, I\nthink, would be great to add. But, at the same\ntime, that's also one area that can\nreally introduce",
    "start": "2918993",
    "end": "2925190"
  },
  {
    "text": "a lot of complexity and bias. And so, to me, that's\nthe area that's both the most\nexciting and the place where you have to\nbe the most careful.",
    "start": "2925190",
    "end": "2930943"
  },
  {
    "start": "2930943",
    "end": "2939330"
  },
  {
    "text": "I'm curious what\ndeliberation means. Are you thinking again\nof generative things, where you would, actually,\nsimulate conversations?",
    "start": "2939330",
    "end": "2946050"
  },
  {
    "text": "Or is there some abstract\nway of modeling deliberation? Also, fine, if you want to\nkeep that under your hat.",
    "start": "2946050",
    "end": "2952990"
  },
  {
    "text": "No. It's a great question. In fact, a month or two ago,\nI was talking to someone at DeepMind, who was saying\nthat he was currently",
    "start": "2952990",
    "end": "2960450"
  },
  {
    "text": "working on trying to have\ngenerative models take",
    "start": "2960450",
    "end": "2966000"
  },
  {
    "text": "on different points of view\nand deliberate with each other. Now, the way that I envision\nthis is, maybe, my talk would",
    "start": "2966000",
    "end": "2973260"
  },
  {
    "text": "suggest is a more intelligent\naugmentation point of view, where I think there\nare really cool opportunities",
    "start": "2973260",
    "end": "2980519"
  },
  {
    "text": "to use these generative\nmodels to, maybe, help augment the way in which\nwe debate with each other,",
    "start": "2980520",
    "end": "2986250"
  },
  {
    "text": "rather than just saying, two\nmodels should go have at it and decide for themselves. But I think there's\nlots of cool things",
    "start": "2986250",
    "end": "2991980"
  },
  {
    "text": "to be done in both\nof those directions. ",
    "start": "2991980",
    "end": "2998602"
  },
  {
    "text": "I'm curious if you\nhave any parting shots for the grad\nstudents in the audience, having now finished your\nPhD, advice you'd pass on",
    "start": "2998602",
    "end": "3005730"
  },
  {
    "text": "to the current students. The biggest advice\nhas been already said. I'm going to say it again,\nwhich is, for me, it",
    "start": "3005730",
    "end": "3014369"
  },
  {
    "text": "was so important and\nrewarding, both intellectually and otherwise, to\ntake on big ideas,",
    "start": "3014370",
    "end": "3024010"
  },
  {
    "text": "to take on these big\nchallenges and attempt to make sense of the world,\nand then execute on them.",
    "start": "3024010",
    "end": "3031315"
  },
  {
    "text": "It can be scary at first. But it was I think\nreally worthwhile. And I'm so glad that I did it. And that's what I would suggest\nother PhD students think",
    "start": "3031315",
    "end": "3037450"
  },
  {
    "text": "about doing as well. ",
    "start": "3037450",
    "end": "3043250"
  },
  {
    "text": "Do you want to close this out?  Thank you.",
    "start": "3043250",
    "end": "3049000"
  },
  {
    "text": "So Mitchell, you showed us how\nwith the Kaggle competition, the majoritistic\nobjective ended up",
    "start": "3049000",
    "end": "3056500"
  },
  {
    "text": "producing a model that\ndid as well as it could in a more disaggregated metric.",
    "start": "3056500",
    "end": "3062920"
  },
  {
    "text": "Can you describe the scenario,\nwhere a majoritistic goal would reduce-- would result in\na classifier that",
    "start": "3062920",
    "end": "3068950"
  },
  {
    "text": "does not perform optimally\nby that other definition of success? Does that make sense?",
    "start": "3068950",
    "end": "3074786"
  },
  {
    "text": "So what I was trying to\nshow with the Kaggle thing is that we have a\nmajoritarian viewpoint that",
    "start": "3074787",
    "end": "3079840"
  },
  {
    "text": "made it look like the model\nwas performing really well. But in practice, it wasn't. And so I'm not quite\nsure I understand",
    "start": "3079840",
    "end": "3087217"
  },
  {
    "text": "the point that you're getting\nat, the opposite of that, maybe. So you told us,\nthis model is not performing as well\nas you think it is.",
    "start": "3087217",
    "end": "3093550"
  },
  {
    "text": "But then you ended\nup showing us, it's performing\nas well as it can based on this other definition. So what's the scenario\nwhere it wouldn't",
    "start": "3093550",
    "end": "3098650"
  },
  {
    "text": "be performing as well as it\ncould based on your definition? ",
    "start": "3098650",
    "end": "3104522"
  },
  {
    "text": "That's a good question. I think that you could just use\nlike a worse version of the--",
    "start": "3104522",
    "end": "3111320"
  },
  {
    "text": "on the Kaggle leader board,\ngo down the leader board and find a really bad model. And then that would be already\nperforming not state-of-the-art.",
    "start": "3111320",
    "end": "3119480"
  },
  {
    "text": "I think the real answer is\njust more and more complex tasks that we're\ntrying to do today.",
    "start": "3119480",
    "end": "3125240"
  },
  {
    "text": "Misinformation\ndetection, for instance, is sometimes considered a more\nhigh-dimensional or complex task.",
    "start": "3125240",
    "end": "3130410"
  },
  {
    "text": "So I suspect those models\nmight be a case, where we're going to see that. But I think that's like a\nleapfrog game of wherever models",
    "start": "3130410",
    "end": "3137090"
  },
  {
    "text": "are not currently\nperforming optimally, maybe, they'll get there eventually. And then they'll start\nto really exhibit",
    "start": "3137090",
    "end": "3142100"
  },
  {
    "text": "this issue of their maxed out. OK. Thank you. All right.",
    "start": "3142100",
    "end": "3147170"
  },
  {
    "text": "Let's thank Mitchell\nfor his time. Thank you. [APPLAUSE]",
    "start": "3147170",
    "end": "3152710"
  },
  {
    "start": "3152710",
    "end": "3156000"
  }
]