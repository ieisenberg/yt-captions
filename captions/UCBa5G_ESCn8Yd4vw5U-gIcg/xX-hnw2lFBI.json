[
  {
    "text": "Hello, welcome to the section on multi-class classification.",
    "start": "4640",
    "end": "9850"
  },
  {
    "text": "So remember the key ideas here, we're trying to do classification where the target",
    "start": "12800",
    "end": "19270"
  },
  {
    "text": "variable can take one of K possible values. And we have, uh,",
    "start": "19270",
    "end": "26470"
  },
  {
    "text": "raw target values, which are say, numbered 1 through K. But we embed those,",
    "start": "26470",
    "end": "35070"
  },
  {
    "text": "uh, using the map y is equal to Psi of b.",
    "start": "35070",
    "end": "40630"
  },
  {
    "text": "And as a result, y can take one of K values, Psi 1 through Psi K. Each one of those is a vector in R_m.",
    "start": "40630",
    "end": "50750"
  },
  {
    "text": "We get to choose what m is. We get to choose what those Psi 1 through Psi K's are.",
    "start": "50750",
    "end": "56695"
  },
  {
    "text": "And once we've done, once we've got a predictor that produces a y from an x,",
    "start": "56695",
    "end": "64190"
  },
  {
    "text": "we have to get back to v. And the way we do that is we",
    "start": "64190",
    "end": "70100"
  },
  {
    "text": "look at the the y hat that we've got produced by the predictor and we look over all of the different Psi's,",
    "start": "70100",
    "end": "76190"
  },
  {
    "text": "Psi 1 through Psi K to find the one that's closest to the y hat. And that's going to tell us which",
    "start": "76190",
    "end": "82670"
  },
  {
    "text": "of the classes to unembed and give us the resulting v hat.",
    "start": "82670",
    "end": "87875"
  },
  {
    "text": "And of course, the predictor that produces y hat from x is constructed via regularized empirical risk minimization.",
    "start": "87875",
    "end": "97400"
  },
  {
    "text": "And at the end of the day, what we do is we validate using the Neyman-Pearson performance metric on the test data.",
    "start": "97400",
    "end": "104420"
  },
  {
    "text": "So we've picked our Kappas where Kappa j is a distaste,",
    "start": "104420",
    "end": "110525"
  },
  {
    "text": "uh, displeasure in mistaking V_j. And, uh, performance metric is then the sum over j of Kappa j E_j,",
    "start": "110525",
    "end": "120619"
  },
  {
    "text": "where E_j counts the number of times that we mistook V_j.",
    "start": "120620",
    "end": "126700"
  },
  {
    "text": "And of course if all the Kappas are one, that's just the error rate.",
    "start": "126700",
    "end": "132670"
  },
  {
    "text": "Now we're going to want to construct a loss function.",
    "start": "138770",
    "end": "145795"
  },
  {
    "text": "And it's not quite so easy as it was in the Boolean case.",
    "start": "145795",
    "end": "151010"
  },
  {
    "text": "So in the Boolean case, remember how it worked. Let me just use this as a little sketch pad here. Um, we embedded- so this is R m,",
    "start": "151010",
    "end": "164444"
  },
  {
    "text": "where m is 1. So we embedded at plus 1 and minus 1.",
    "start": "164445",
    "end": "171099"
  },
  {
    "text": "And we'll call this Psi 1 and Psi 2 and then we had ideal loss functions from the Neyman-Pearson loss.",
    "start": "172340",
    "end": "182540"
  },
  {
    "text": "So if the true y was- y was, uh, 1,",
    "start": "182540",
    "end": "191385"
  },
  {
    "text": "um, then, uh, our ideal loss function would be this.",
    "start": "191385",
    "end": "203260"
  },
  {
    "text": "Our ideal loss function would be this. And it would say that since y was 1,",
    "start": "205340",
    "end": "211560"
  },
  {
    "text": "our ideal y hat should be anything negative.",
    "start": "211560",
    "end": "217060"
  },
  {
    "text": "So we want to generalize this idea to the case",
    "start": "218690",
    "end": "224345"
  },
  {
    "text": "where we've now got multiple Psi I's not just two,",
    "start": "224345",
    "end": "230110"
  },
  {
    "text": "and we've embedded them in our m and m is not just 1.",
    "start": "230110",
    "end": "235840"
  },
  {
    "text": "So the first idea we need is simply, uh, to, uh,",
    "start": "235840",
    "end": "240910"
  },
  {
    "text": "know when a vector is closer to one point than another point.",
    "start": "240910",
    "end": "246905"
  },
  {
    "text": "And that's-, er, so here we have two points. Uh, we have point a and point b and we have here this green line and",
    "start": "246905",
    "end": "258979"
  },
  {
    "text": "this green line is the perpendicular bisector of those two points.",
    "start": "258980",
    "end": "267945"
  },
  {
    "text": "And that comes about mathematically in the following way.",
    "start": "267945",
    "end": "274085"
  },
  {
    "text": "If I ask when is a vector y hat closer to a than it is to b, well, the distance,",
    "start": "274085",
    "end": "281435"
  },
  {
    "text": "um, from y hat to a is just the norm of y hat minus a,",
    "start": "281435",
    "end": "288005"
  },
  {
    "text": "and the distance from y to b is the norm of y hat minus b. And so we can ask ourselves the question,",
    "start": "288005",
    "end": "293659"
  },
  {
    "text": "when is one close or less than the other? That's this inequality right here.",
    "start": "293660",
    "end": "299330"
  },
  {
    "text": "Now we square both sides, so those are both two norms. What happens is we get an expression which looks like y hat minus",
    "start": "299330",
    "end": "308525"
  },
  {
    "text": "a norm squared is less than or equal to the norm of y hat minus b squared.",
    "start": "308525",
    "end": "318514"
  },
  {
    "text": "And if we expand their squares, well, then we get that the y hat transpose y hat minus 2 a transpose y hat,",
    "start": "318515",
    "end": "329340"
  },
  {
    "text": "plus a transpose a less than or equal to y hat",
    "start": "329340",
    "end": "334650"
  },
  {
    "text": "transpose y hat minus 2 b transpose y hat plus b transpose b.",
    "start": "334650",
    "end": "343930"
  },
  {
    "text": "And the nice thing about this is that those quadratic terms in y hat go away.",
    "start": "344060",
    "end": "350825"
  },
  {
    "text": "And this inequality reduces to this inequality.",
    "start": "350825",
    "end": "359184"
  },
  {
    "text": "It is this inequality, 2 b minus a transpose y hat,",
    "start": "359184",
    "end": "364425"
  },
  {
    "text": "minus norm of b squared plus the norm of a squared is less than 0.",
    "start": "364425",
    "end": "369759"
  },
  {
    "text": "And in particular, the thing to notice here is that this only depends on y hat in this nice simple way here. All right.",
    "start": "369760",
    "end": "380175"
  },
  {
    "text": "It's a vector transpose times y hat. And so this is equal to 0 along this green line,",
    "start": "380175",
    "end": "389170"
  },
  {
    "text": "it's less than 0 when, uh, y hat is on",
    "start": "389170",
    "end": "396990"
  },
  {
    "text": "the a side and it's greater than 0 when y hat is on the b side of the green line.",
    "start": "396990",
    "end": "406240"
  },
  {
    "text": "And the decision boundary, especially just replacing the inequality with an equality is right here.",
    "start": "407240",
    "end": "414430"
  },
  {
    "text": "That's a hyperplane, that's an equation of the form c transpose y is equal to d,",
    "start": "414430",
    "end": "422300"
  },
  {
    "text": "where c is a vector in R_m and d is a scalar.",
    "start": "422300",
    "end": "427379"
  },
  {
    "text": "And here in particular, c is 2 b minus",
    "start": "428880",
    "end": "435534"
  },
  {
    "text": "a and d is the norm of a squared minus the norm of b squared.",
    "start": "435535",
    "end": "444615"
  },
  {
    "text": "Um, and this hyperplane has to pass through the midpoint of a and b.",
    "start": "444615",
    "end": "454560"
  },
  {
    "text": "And you can check that it does by substituting y hat is a plus b on 2,",
    "start": "454560",
    "end": "463270"
  },
  {
    "text": "into this equation right here, and checking that you get 0.",
    "start": "463270",
    "end": "469069"
  },
  {
    "text": "Now, we can use this analysis to construct a notion of signed distance",
    "start": "472590",
    "end": "481840"
  },
  {
    "text": "from the hyperplane h. So the hyperplane h is here.",
    "start": "481840",
    "end": "491169"
  },
  {
    "text": "It's this green hyperplane that's the perpendicular bisector between a and b.",
    "start": "491170",
    "end": "500840"
  },
  {
    "text": "And we're gonna say that we're gonna measure the distance from h. And so,",
    "start": "500840",
    "end": "507305"
  },
  {
    "text": "I'm going to have some points on this side and some points on this side.",
    "start": "507305",
    "end": "515310"
  },
  {
    "text": "And these points, we're going to say a distance minus 1 from the hyperplane h,",
    "start": "515310",
    "end": "527660"
  },
  {
    "text": "and these points will say a distance 1. So instead of labeling them both as having distance 1,",
    "start": "527660",
    "end": "536510"
  },
  {
    "text": "the ones on one side will get negative distance and the ones on the other will get positive distance. And our point y hat",
    "start": "536510",
    "end": "542930"
  },
  {
    "text": "here has some distance",
    "start": "542930",
    "end": "550370"
  },
  {
    "text": "here which is about minus 2 from the hyperplane. I might have another point, say over here.",
    "start": "550370",
    "end": "557740"
  },
  {
    "text": "And that would have a distance, uh, plus 2, or maybe slightly more than 2 the way I've drawn it from the hyperplane.",
    "start": "557740",
    "end": "566600"
  },
  {
    "text": "And where does this formula come from? Well, we have this nice formula here, which is, uh,",
    "start": "566600",
    "end": "580350"
  },
  {
    "text": "this inequality here, which is- uh, and it- uh,",
    "start": "581810",
    "end": "587660"
  },
  {
    "text": "defines for us which side of the hyperplane we're on and it has the form of c transpose y plus d. Now,",
    "start": "587660",
    "end": "597475"
  },
  {
    "text": "over here, we've simply taken that and taken that the value of the left-hand side of",
    "start": "597475",
    "end": "604580"
  },
  {
    "text": "that inequality and made it equal to our distance measure. But we've scaled it. We've scaled it by 2 is the norm of b minus a.",
    "start": "604580",
    "end": "614220"
  },
  {
    "text": "And as a result, d has this form, d is equal to u transpose y plus v,",
    "start": "614220",
    "end": "627020"
  },
  {
    "text": "where u here is a unit vector. It's b minus a divided by the norm of b minus a.",
    "start": "627020",
    "end": "635505"
  },
  {
    "text": "And because u is a unit vector, it's measuring distance, uh,",
    "start": "635505",
    "end": "643120"
  },
  {
    "text": "which is corresponding to the unit distances that we use in the plane.",
    "start": "643120",
    "end": "649765"
  },
  {
    "text": "So in other words, if y hat actually is a distance d from the hyperplane,",
    "start": "649765",
    "end": "658605"
  },
  {
    "text": "then, uh, D here will be the appropriate sign appended on to y hat.",
    "start": "658605",
    "end": "666269"
  },
  {
    "text": "So D is zero, that's the decision boundary. That's the decision boundary that",
    "start": "671150",
    "end": "678180"
  },
  {
    "text": "decides whether we are on the a side or the b side of the hyperplane. And, uh, if it's negative,",
    "start": "678180",
    "end": "686144"
  },
  {
    "text": "we're closer to a than we are to b.",
    "start": "686145",
    "end": "690430"
  },
  {
    "text": "So why do we want a notion of signed distance?",
    "start": "694280",
    "end": "700590"
  },
  {
    "text": "Um, and the idea goes like this. Suppose we have- let me draw a picture.",
    "start": "700590",
    "end": "705840"
  },
  {
    "text": "Uh, so this will be R2,",
    "start": "705840",
    "end": "713760"
  },
  {
    "text": "so here M is 2. So we've embedded our categorical variables in the plane R2.",
    "start": "713760",
    "end": "726780"
  },
  {
    "text": "And the embedding points, well, let's just pick them. Uh, let's put, uh,",
    "start": "726780",
    "end": "732390"
  },
  {
    "text": "one over here, one over here, and one over here. Uh, and I- I will label those, uh,",
    "start": "732390",
    "end": "741375"
  },
  {
    "text": "Psi 1, Psi 2, and Psi 3.",
    "start": "741375",
    "end": "747720"
  },
  {
    "text": "And those are the points to which our categorical target variables embed.",
    "start": "747720",
    "end": "754095"
  },
  {
    "text": "Now, with those points, we're going to, um,",
    "start": "754095",
    "end": "759840"
  },
  {
    "text": "say, okay, well, those are embedded points, we'd like to define a loss function with those points.",
    "start": "759840",
    "end": "765000"
  },
  {
    "text": "That's what we'd like to do. Now, the idea is, of course, is that we're going to have a predictor that produces for us a y-hat.",
    "start": "765000",
    "end": "773430"
  },
  {
    "text": "And we would like that predictor to be inclined to give us a y-hat that's close to Psi 1 when y is 1,",
    "start": "773430",
    "end": "785265"
  },
  {
    "text": "and a y-hat that's close to Psi 2 when y is 2, and a y-hat that's close to Psi 3 when y is 3.",
    "start": "785265",
    "end": "795015"
  },
  {
    "text": "So let's divide up the plane into regions,",
    "start": "795015",
    "end": "799900"
  },
  {
    "text": "the Voronoi partition of the plane, like that.",
    "start": "800390",
    "end": "806000"
  },
  {
    "text": "And then this region right here,",
    "start": "806000",
    "end": "812100"
  },
  {
    "text": "this region here is the region, uh, which,",
    "start": "815560",
    "end": "821990"
  },
  {
    "text": "if we have a y-hat within that region, then it's unembedded back to Psi 1, and, therefore,",
    "start": "821990",
    "end": "828570"
  },
  {
    "text": "back to the target value corresponding to Psi 1. And so what we'd like is we'd like to have a loss function which encourages",
    "start": "828570",
    "end": "839370"
  },
  {
    "text": "y-hat to lie within that region when y is equal to Psi 1.",
    "start": "839370",
    "end": "847005"
  },
  {
    "text": "We can imagine such a loss function. Here's what it would be. It would be zero if y-hat lies in that region and one elsewhere.",
    "start": "847005",
    "end": "858690"
  },
  {
    "text": "It would be 1 here",
    "start": "858690",
    "end": "864690"
  },
  {
    "text": "and 0 here.",
    "start": "864690",
    "end": "870630"
  },
  {
    "text": "And such a loss function would count up for us the rate at which we mistook y1 for something else.",
    "start": "870630",
    "end": "879390"
  },
  {
    "text": "And so, in our loss function, if we summed up that and divide it by n,",
    "start": "879390",
    "end": "884940"
  },
  {
    "text": "that would give us precisely the rate which we mistook for- y1 for something else.",
    "start": "884940",
    "end": "890745"
  },
  {
    "text": "And so Kappa 1 multiplied by that loss function would count up for us",
    "start": "890745",
    "end": "896925"
  },
  {
    "text": "the Neyman-Pearson contribution of the loss for the target Psi 1.",
    "start": "896925",
    "end": "906000"
  },
  {
    "text": "And then we would do another one. We'd have a loss function. Of course, we've got a different loss function for y1,",
    "start": "906000",
    "end": "914820"
  },
  {
    "text": "y2, and y3. And so,",
    "start": "914820",
    "end": "919030"
  },
  {
    "text": "if I look- if this is y1, this is y2, this is y3,",
    "start": "927650",
    "end": "933045"
  },
  {
    "text": "I would have a loss function which was 0 here and 1 here.",
    "start": "933045",
    "end": "941430"
  },
  {
    "text": "And that would be the loss function corresponding to y is equal to 2.",
    "start": "941430",
    "end": "948135"
  },
  {
    "text": "And so we have three different loss functions, functions of y-hat; one when y is Psi 1,",
    "start": "948135",
    "end": "956385"
  },
  {
    "text": "one when y is Psi 2, and one is- when y is Psi 3, and that each one is 0 on the region corresponding to its Psi i.",
    "start": "956385",
    "end": "966875"
  },
  {
    "text": "So how do we construct such a loss? Well, what we can do is",
    "start": "966875",
    "end": "979034"
  },
  {
    "text": "we can use the signed distance functions D_ij. And what D_ij is going to be,",
    "start": "979034",
    "end": "985695"
  },
  {
    "text": "it's gonna be the distance, the signed distance of y-hat from the boundary between Psi i and Psi j.",
    "start": "985695",
    "end": "994695"
  },
  {
    "text": "Let's look back at our example.",
    "start": "994695",
    "end": "998350"
  },
  {
    "text": "This is Psi 1, Psi 2, and Psi 3.",
    "start": "1006640",
    "end": "1012470"
  },
  {
    "text": "Now, uh, D_1, 2 is less than",
    "start": "1012470",
    "end": "1019879"
  },
  {
    "text": "0 when we are closer to Psi 1 than we are to Psi 2.",
    "start": "1019880",
    "end": "1027214"
  },
  {
    "text": "So D_1, 2 is less than 0. Where is that? That's everywhere to the left of this line.",
    "start": "1027215",
    "end": "1035340"
  },
  {
    "text": "Where is D_1, 3 less than 0? That's everywhere below that line.",
    "start": "1037990",
    "end": "1047750"
  },
  {
    "text": "Let me mark my two lines. And so if D_1, 2 and D_1,",
    "start": "1047750",
    "end": "1053855"
  },
  {
    "text": "3 are less than 0,",
    "start": "1053855",
    "end": "1056820"
  },
  {
    "text": "well, then I'm in the region belonging to Psi 1. And similarly, I'm in the region belonging to Psi 2,",
    "start": "1058930",
    "end": "1067530"
  },
  {
    "text": "that's this region. If D- if",
    "start": "1068560",
    "end": "1076390"
  },
  {
    "text": "D_2, 1 is less than 0, then D_2, 3 is less than 0.",
    "start": "1081940",
    "end": "1087905"
  },
  {
    "text": "And finally, over here I have D_3, 1 is less than 0 and D_3, 2 is less than 0.",
    "start": "1087905",
    "end": "1096380"
  },
  {
    "text": "So in other words, I need, for any given region, for any given i,",
    "start": "1096380",
    "end": "1102155"
  },
  {
    "text": "if I want y-hat to be in the region belonging to Psi i,",
    "start": "1102155",
    "end": "1107960"
  },
  {
    "text": "then I need D_ij to be less than 0 for all the other js.",
    "start": "1107960",
    "end": "1113700"
  },
  {
    "text": "Now that we've got these signed distance functions, we can use them to construct our ideal loss functions.",
    "start": "1114880",
    "end": "1121830"
  },
  {
    "text": "Here's some examples to be explicit in the most common cases.",
    "start": "1123760",
    "end": "1130655"
  },
  {
    "text": "The most common cases are, of course, the Boolean. In the Boolean case,",
    "start": "1130655",
    "end": "1135980"
  },
  {
    "text": "M is 1, and so I've just got R here. Psi 1 is minus 1 and Psi 2 is 1.",
    "start": "1135980",
    "end": "1147125"
  },
  {
    "text": "And then I've got D_1, 2 is less than 0 over here and D_2,",
    "start": "1147125",
    "end": "1157880"
  },
  {
    "text": "1 is less than 0 over here.",
    "start": "1157880",
    "end": "1161760"
  },
  {
    "text": "And so if y is in the region corresponding to Psi 1,",
    "start": "1162940",
    "end": "1170240"
  },
  {
    "text": "so that means that y is minus 1, well, then we need- we would like D_1,",
    "start": "1170240",
    "end": "1179660"
  },
  {
    "text": "2 to be less than 0. Um, and that would be- and then we'd have a loss function which was 0 over here,",
    "start": "1179660",
    "end": "1190490"
  },
  {
    "text": "say, and large over here. And that would be our ideal loss function, our Neyman-Pearson loss function.",
    "start": "1190490",
    "end": "1197549"
  },
  {
    "text": "Similarly, if y is plus 1, then, uh, we want D_2, 1- or,",
    "start": "1197650",
    "end": "1206150"
  },
  {
    "text": "uh, if y is plus 1 and that corresponds to Psi 2, and so we want D_2,",
    "start": "1206150",
    "end": "1212649"
  },
  {
    "text": "1 of y-hat to be less than 0. And D_2, 1 of y-hat is minus y-hat,",
    "start": "1212650",
    "end": "1219265"
  },
  {
    "text": "and so we want y-hat to be greater than 0. If we look at the one-hot case,",
    "start": "1219265",
    "end": "1227015"
  },
  {
    "text": "that's their Psi j- this should say e_j. Let me just correct that, Psi j is e_j.",
    "start": "1227015",
    "end": "1233570"
  },
  {
    "text": "Um, and, uh, if we,",
    "start": "1233570",
    "end": "1240783"
  },
  {
    "text": "uh, compute what D_ij is, well, the distance between D_ij- let's just look at it.",
    "start": "1240784",
    "end": "1248400"
  },
  {
    "text": "This is Psi 1 and this is Psi 2.",
    "start": "1248590",
    "end": "1254029"
  },
  {
    "text": "In two dimensions, if we're embedding using one-hot, this is, of course, 1, 0, and this is 0, 1.",
    "start": "1254030",
    "end": "1261799"
  },
  {
    "text": "And so the distance between those two points is simply root 2. And so D_ij is y_j minus y_i on root 2.",
    "start": "1261800",
    "end": "1271520"
  },
  {
    "text": "Where, in order to compute that, of course, we've used our definition here,",
    "start": "1271520",
    "end": "1280970"
  },
  {
    "text": "2b minus a transposed y-hat divided by 2 the norm of b minus a.",
    "start": "1280970",
    "end": "1287000"
  },
  {
    "text": "Obviously, the twos cancel and b minus a transpose times y-hat gives us y-hat i minus y-hat j,",
    "start": "1287000",
    "end": "1296195"
  },
  {
    "text": "or y-hat j minus y-hat i, depending on the sign. So when y is e_i,",
    "start": "1296195",
    "end": "1303125"
  },
  {
    "text": "we want to max of all j not equal to i of D_ij to be less than 0.",
    "start": "1303125",
    "end": "1308435"
  },
  {
    "text": "And that means that we want y_j minus y_i to be less than 0.",
    "start": "1308435",
    "end": "1317270"
  },
  {
    "text": "And that's the same as saying that y_j is less than y_i. So in other words, we would like the corresponding,",
    "start": "1317270",
    "end": "1326190"
  },
  {
    "text": "uh, uh, component of y-hat to be the maximum component to y-hat.",
    "start": "1331960",
    "end": "1340590"
  },
  {
    "text": "So let's use these to construct loss functions explicitly. Um, what we gotta do, well,",
    "start": "1346250",
    "end": "1352410"
  },
  {
    "text": "we've got to give K different loss functions, one corresponding to each of the possible values of y.",
    "start": "1352410",
    "end": "1360210"
  },
  {
    "text": "Remember, loss is a function of y-hat and y, and y here can only take K possible values,",
    "start": "1360210",
    "end": "1366359"
  },
  {
    "text": "which we've denoted as l of y-hat comma Psi_i. It's how much we dislike predicting y-hat when y is Psi_i.",
    "start": "1366359",
    "end": "1375720"
  },
  {
    "text": "In other words, if to come back to our example,",
    "start": "1375720",
    "end": "1381340"
  },
  {
    "text": "if D_12 is less than zero and D_13 is less than 0,",
    "start": "1390020",
    "end": "1399420"
  },
  {
    "text": "then we'd like a loss function corresponding to l of",
    "start": "1399420",
    "end": "1404850"
  },
  {
    "text": "y-hat Psi_1 to be small and large when y-hat doesn't satisfy these two conditions.",
    "start": "1404850",
    "end": "1417460"
  },
  {
    "text": "And that gives us immediately the Neyman-Pearson loss. We can set, uh,",
    "start": "1422210",
    "end": "1427409"
  },
  {
    "text": "a loss function of, um, 0 on the region corresponding to Psi_i and Kappa_i elsewhere.",
    "start": "1427410",
    "end": "1438750"
  },
  {
    "text": "And that means when we take the average of that Neyman-Pearson loss,",
    "start": "1438750",
    "end": "1444075"
  },
  {
    "text": "we get Kappa_i times the frequency with which y-hat is mistaken,",
    "start": "1444075",
    "end": "1451950"
  },
  {
    "text": "y-hat gives an answer which is not i when y truly is i.",
    "start": "1451950",
    "end": "1458200"
  },
  {
    "text": "And this is, of course, the analog of our nice square loss functions that we've seen before.",
    "start": "1458240",
    "end": "1469240"
  },
  {
    "text": "Now, the downside is, of course, that it's hard to minimize this- these discontinuous loss functions which have derivative 0 almost everywhere.",
    "start": "1470570",
    "end": "1482460"
  },
  {
    "text": "And so instead, we use what's called a proxy loss, a loss that approximates the Neyman-Pearson loss,",
    "start": "1482460",
    "end": "1492120"
  },
  {
    "text": "but isn't more easily optimized. So we either would like it to be convex,",
    "start": "1492120",
    "end": "1497550"
  },
  {
    "text": "or differentiable, or both. So here's the, perhaps,",
    "start": "1497550",
    "end": "1505440"
  },
  {
    "text": "one of the- there- there are two really common loss functions that are used. One is the hinge loss,",
    "start": "1505440",
    "end": "1511799"
  },
  {
    "text": "this is the multi-class hinge loss. It's Kappa_i times the maximum of 1 plus D_ij of y-hat,",
    "start": "1511800",
    "end": "1521595"
  },
  {
    "text": "and we're maximizing it over all j not equal to i. And this is a loss function which is",
    "start": "1521595",
    "end": "1532635"
  },
  {
    "text": "zero if y-hat is in the right region within margin of at least 1.",
    "start": "1532635",
    "end": "1540255"
  },
  {
    "text": "And so it has to be not only within the right region, but it need- it has to be deep within the right region.",
    "start": "1540255",
    "end": "1547005"
  },
  {
    "text": "And this is a convex but not differentiable loss. If you use quadratic regularization,",
    "start": "1547005",
    "end": "1553860"
  },
  {
    "text": "then it's multi-class support vector machine. If we'd have a Boolean embedding,",
    "start": "1553860",
    "end": "1561284"
  },
  {
    "text": "then Psi_1 is minus 1 and Psi_2 is 1, then it reduces to the usual hinge loss that we've seen before,",
    "start": "1561285",
    "end": "1568065"
  },
  {
    "text": "is 1 plus y-hat, the positive part times Kappa_1 when y is minus 1,",
    "start": "1568065",
    "end": "1575265"
  },
  {
    "text": "and it's the flip of that, uh, when y is 1.",
    "start": "1575265",
    "end": "1581860"
  },
  {
    "text": "Here is the corresponding surface plots for it in the case which we've been discussing so far.",
    "start": "1583250",
    "end": "1592035"
  },
  {
    "text": "This is our, um, uh, example, we've got three Psis,",
    "start": "1592035",
    "end": "1598155"
  },
  {
    "text": "Psi_1, Psi_2, and Psi_3, and you can see if you've got a loss function which is convex,",
    "start": "1598155",
    "end": "1606605"
  },
  {
    "text": "which is piecewise linear, and which is 0 when you're deep within each of the three regions.",
    "start": "1606605",
    "end": "1615030"
  },
  {
    "text": "So, of course, you've got three loss functions here. In the top right graph here, we have the loss function corresponding to the case when y is one,",
    "start": "1615030",
    "end": "1626670"
  },
  {
    "text": "er, y is Psi_1. In the bottom right, we have the case where y is Psi_3,",
    "start": "1626670",
    "end": "1632985"
  },
  {
    "text": "and in the bottom left, we have the case when y is Psi_2. Now, if you look in particular,",
    "start": "1632985",
    "end": "1638264"
  },
  {
    "text": "to the top right when y is Psi_1, you can see that the loss function is zero when,",
    "start": "1638265",
    "end": "1646990"
  },
  {
    "text": "uh, we are, uh, deep within the region corresponding to y_1.",
    "start": "1646990",
    "end": "1657250"
  },
  {
    "text": "Here's the other most common loss function, the multi-class logistic loss.",
    "start": "1661340",
    "end": "1667380"
  },
  {
    "text": "So this is, uh, the following. This is the log of the sum of the D_ijs.",
    "start": "1667380",
    "end": "1676950"
  },
  {
    "text": "So if all of the D_ijs are less than 0,",
    "start": "1676950",
    "end": "1684104"
  },
  {
    "text": "then this is going to be, well, the sum of e to the- e to the negative numbers,",
    "start": "1684104",
    "end": "1691050"
  },
  {
    "text": "and so it'll be a- a relatively small quantity, and it will grow as the D_ijs grow.",
    "start": "1691050",
    "end": "1699220"
  },
  {
    "text": "It's convex and differentiable. It's called a multi-class logistic loss,",
    "start": "1699230",
    "end": "1704850"
  },
  {
    "text": "and we use this as our loss function, we're doing multi-class logistic regression.",
    "start": "1704850",
    "end": "1710710"
  },
  {
    "text": "So here's, uh, a plot of that loss function.",
    "start": "1713690",
    "end": "1720269"
  },
  {
    "text": "Again, we can see three different loss functions, each one associated with the different region of",
    "start": "1720270",
    "end": "1727049"
  },
  {
    "text": "the plane corresponding to our different representatives, and for each one,",
    "start": "1727050",
    "end": "1732435"
  },
  {
    "text": "the loss function is small in the corresponding region and grows outside the region.",
    "start": "1732435",
    "end": "1739000"
  },
  {
    "text": "Uh, the log sum x function is what's at the heart of this loss function.",
    "start": "1742250",
    "end": "1748530"
  },
  {
    "text": "Uh, this is an interesting function because it is convex differentiable,",
    "start": "1748530",
    "end": "1753780"
  },
  {
    "text": "and its approximately the maximum function. Sometimes people call it the softmax function,",
    "start": "1753780",
    "end": "1759434"
  },
  {
    "text": "but you should be careful about that because there are other functions also used in machine learning as it happens,",
    "start": "1759435",
    "end": "1765525"
  },
  {
    "text": "they're also called a softmax function. Um, one of the nice thing about the log-sum-exp function",
    "start": "1765525",
    "end": "1773145"
  },
  {
    "text": "is then we know how far it is from the max function. It's always greater- greater than or equal to the maximum of the x_is,",
    "start": "1773145",
    "end": "1783509"
  },
  {
    "text": "and is always less than or equal to the maximum of the x_is plus log n. Uh,",
    "start": "1783510",
    "end": "1794520"
  },
  {
    "text": "let's look at an example. Uh, this is, uh, uh, a three class example.",
    "start": "1794520",
    "end": "1801510"
  },
  {
    "text": "This goes back to 1936, is a very famous dataset which was collected by the statistician Fisher.",
    "start": "1801510",
    "end": "1808185"
  },
  {
    "text": "He took measurements for 150 different plants, all of which were three different species of iris,",
    "start": "1808185",
    "end": "1817079"
  },
  {
    "text": "so 50 from each of the three different species. And for each of those samples,",
    "start": "1817079",
    "end": "1823410"
  },
  {
    "text": "he made four measurements; the sepal length, the sepal width, the petal length, and the petal width.",
    "start": "1823410",
    "end": "1829390"
  },
  {
    "text": "We can plot them here. So this plot here, this is simply an arrangement of all the data.",
    "start": "1831140",
    "end": "1837075"
  },
  {
    "text": "Um, so for example, in this plot on the top right, we've plotted sepal length on",
    "start": "1837075",
    "end": "1843060"
  },
  {
    "text": "the vertical axis against petal width on the horizontal axis. Uh, you can see that there are three species.",
    "start": "1843060",
    "end": "1850890"
  },
  {
    "text": "The widths- there's some- one of the species is indicated in red, one of them in green, and one of them in blue.",
    "start": "1850890",
    "end": "1856290"
  },
  {
    "text": "You can see that telling the red from the others is rather straight forward,",
    "start": "1856290",
    "end": "1861320"
  },
  {
    "text": "and telling the green and blue apart is a bit more difficult.",
    "start": "1861320",
    "end": "1865409"
  },
  {
    "text": "So here, we've done classification with- using only two of the features, sepal length and sepal width,",
    "start": "1870470",
    "end": "1877095"
  },
  {
    "text": "and there's that plotted, uh, in this, um, figure.",
    "start": "1877095",
    "end": "1882210"
  },
  {
    "text": "Again, we can see green dots, blue dots, and red dots, and it's relatively easy to separate the red dots.",
    "start": "1882210",
    "end": "1887370"
  },
  {
    "text": "And this is the- uh, using a linear predictor with one-hot embedding multi-class logistic loss,",
    "start": "1887370",
    "end": "1894674"
  },
  {
    "text": "and we're minimizing the probability of error so that the Kappas are all one. And here, there's no training and test set,",
    "start": "1894675",
    "end": "1901340"
  },
  {
    "text": "we shouldn't be trained in on all the data. Of course, if we had more data, we would, uh, uh, be using a training and test set,",
    "start": "1901340",
    "end": "1908090"
  },
  {
    "text": "and we'd also be using regularization. Uh, the confusion matrix shows us what we can see from the figure,",
    "start": "1908090",
    "end": "1915045"
  },
  {
    "text": "that we've got all the red dots perfectly correct, we haven't mistaken anything for a red dot.",
    "start": "1915045",
    "end": "1921240"
  },
  {
    "text": "Uh, but some of the greens and blues we've got wrong, we've mistaken, uh, 12 greens for blues and 13 blues for greens.",
    "start": "1921240",
    "end": "1931560"
  },
  {
    "text": "And, uh, and as you can see in the- in the figure, it's- there's no clear way to separate the greens from the blues.",
    "start": "1931560",
    "end": "1940544"
  },
  {
    "text": "Now, if you do classification with all four features, of course, we can plot it anymore.",
    "start": "1940545",
    "end": "1946260"
  },
  {
    "text": "Um, same thing, one-hot embedding, minimize probability of error. Well, then you can do much better, then you can get a confusion matrix where there's",
    "start": "1946260",
    "end": "1953760"
  },
  {
    "text": "only one error out of 150 plots. One- uh, sorry. Two errors, two misclassifications, um,",
    "start": "1953760",
    "end": "1961695"
  },
  {
    "text": "we've got one blue classified as green and one green classified as blue.",
    "start": "1961695",
    "end": "1967049"
  },
  {
    "text": "So let's summarize. When we are doing multi-class classification, we'd like a loss function that encourages the correct un-embedding.",
    "start": "1972130",
    "end": "1982600"
  },
  {
    "text": "So when y-hat is close to Psi_i, well,",
    "start": "1982600",
    "end": "1988020"
  },
  {
    "text": "then we want l of y-hat Psi_i to be small, and we want to be not small when y-hat is not close to Psi_i.",
    "start": "1988020",
    "end": "1998470"
  },
  {
    "text": "Uh, the most common losses people use are the multi-class hinge loss and the multi-class logistic loss,",
    "start": "1998630",
    "end": "2005179"
  },
  {
    "text": "and these two classifiers are called the SVM and the logistic classifiers.",
    "start": "2005180",
    "end": "2011165"
  },
  {
    "text": "And both of these classes are- both of these losses are convex. So we can solve easily the ERM or the RERM problems",
    "start": "2011165",
    "end": "2018800"
  },
  {
    "text": "in the case where the predictor is a linear predictor.",
    "start": "2018800",
    "end": "2022980"
  }
]