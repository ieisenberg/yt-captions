[
  {
    "start": "0",
    "end": "8000"
  },
  {
    "start": "0",
    "end": "5650"
  },
  {
    "text": "Great. OK, perfect. So a sample from this\nmodel looks like this. \"They also point to\nninety nine point",
    "start": "5650",
    "end": "12242"
  },
  {
    "start": "8000",
    "end": "27000"
  },
  {
    "text": "six billion dollars\nfrom two hundred four oh six three percent.\" It's a bunch of\nkind of gibberish.",
    "start": "12242",
    "end": "18570"
  },
  {
    "text": "So the sentence\nisn't too coherent, but at least the words do\nseem to be somewhat related,",
    "start": "18570",
    "end": "23606"
  },
  {
    "text": "like they come from\nthe same space.  Now, jumping forwards to the\nbeginning of the deep learning",
    "start": "23607",
    "end": "30230"
  },
  {
    "start": "27000",
    "end": "72000"
  },
  {
    "text": "boom in 2011, we have language\nmodeling with neural networks now, and in particular with\nrecurrent neural networks.",
    "start": "30230",
    "end": "38280"
  },
  {
    "text": "So you can get rid of\nthis giant lookup table from the n-gram models. And instead, we can have\nour inputs be these tokens",
    "start": "38280",
    "end": "46620"
  },
  {
    "text": "and let this kind of return\ncell remember some state and persistent state.",
    "start": "46620",
    "end": "51739"
  },
  {
    "text": "So if we set up a\nneural model like this, we get a sample as shown below.",
    "start": "51740",
    "end": "56810"
  },
  {
    "text": "\"The meaning of life\nis the tradition of the ancient\nhuman reproduction-- it is less favorable to the good\nboy for when to remove bigger.\"",
    "start": "56810",
    "end": "63650"
  },
  {
    "text": "So again, this doesn't\nreally make any sense, but it kind of starts to have\nthe flow of a real sentence.",
    "start": "63650",
    "end": "69545"
  },
  {
    "text": " Yeah, so jumping forward\neven more to 2016,",
    "start": "69545",
    "end": "74970"
  },
  {
    "start": "72000",
    "end": "112000"
  },
  {
    "text": "we have LSTM models. And of course, LSTMs are\nan architectural innovation",
    "start": "74970",
    "end": "81090"
  },
  {
    "text": "on top of RNNs. And they have better\ngradient flow, so they can better model\nlong-term dependencies.",
    "start": "81090",
    "end": "88680"
  },
  {
    "text": "And so with an LSTM model,\nwe get a sample like this. \"With even more new technologies\ncoming onto the market",
    "start": "88680",
    "end": "95070"
  },
  {
    "text": "quickly during the\npast three years, an increasing\nnumber of companies must tackle the ever\nchanging and ever",
    "start": "95070",
    "end": "101220"
  },
  {
    "text": "changing environmental\nchallenges online.\" So this sentence is starting\nto make a little bit of sense, though there are clear\nartifacts, like the repetition",
    "start": "101220",
    "end": "108330"
  },
  {
    "text": "of the phrase ever changing. ",
    "start": "108330",
    "end": "113500"
  },
  {
    "start": "112000",
    "end": "153000"
  },
  {
    "text": "Now, starting in 2018, we\nhave our first autoregressive transformer based\nlanguage models,",
    "start": "113500",
    "end": "118659"
  },
  {
    "text": "which are even better\nat modeling these very long-term dependencies. And here, what I'm showing is\nan example of a completion.",
    "start": "118660",
    "end": "126050"
  },
  {
    "text": "So in a completion, the\nuser supplies the prompt. In this case, it's this\ntext, Wings Over Kansas.",
    "start": "126050",
    "end": "134080"
  },
  {
    "text": "And the model will\ncontinue from this prompt. So you can see that\nthis completion",
    "start": "134080",
    "end": "139930"
  },
  {
    "text": "is coherent across\nmultiple sentences now, though there are\nnotable spelling mistakes.",
    "start": "139930",
    "end": "145080"
  },
  {
    "text": "So you see this\nwhatever \"daknfi\" is. So it doesn't kind\nof make sense.",
    "start": "145080",
    "end": "151075"
  },
  {
    "text": " And now we arrive at GPT-2,\nwhich is a 1.5 billion",
    "start": "151075",
    "end": "157440"
  },
  {
    "start": "153000",
    "end": "218000"
  },
  {
    "text": "parameter transformer model. And I copied in\nwhat I personally found was the most compelling\nconclusion from GPT-2.",
    "start": "157440",
    "end": "165540"
  },
  {
    "text": "And in contrast with the\nlast slide, what this does is it sets up a\nclearly fake prompt.",
    "start": "165540",
    "end": "171880"
  },
  {
    "text": "So we have something about\nfinding unicorns and scientists in South America.",
    "start": "171880",
    "end": "178330"
  },
  {
    "text": "And so the model\nhas probably not seen this exact prompt before. It has to make up something\nthat's consistent.",
    "start": "178330",
    "end": "184330"
  },
  {
    "text": "So the thing I find most\nimpressive is it does so, and it's coherent across\nmultiple paragraphs.",
    "start": "184330",
    "end": "190469"
  },
  {
    "text": "It invents this\nfictional Dr. Perez, and it persists Perez\nthroughout multiple paragraphs.",
    "start": "190470",
    "end": "197580"
  },
  {
    "text": "And I think it's\nvery aptly named. You have him from\nUniversity of La Paz.",
    "start": "197580",
    "end": "203010"
  },
  {
    "text": "And yeah, we just have\nbarely coherent completions at this point. So it's worth\ndisclosing that this",
    "start": "203010",
    "end": "209670"
  },
  {
    "text": "was the best of 10 samples. So we still had to\nsample multiple times",
    "start": "209670",
    "end": "214860"
  },
  {
    "text": "to get a sample like this.  And finally, to\nend this section--",
    "start": "214860",
    "end": "221030"
  },
  {
    "start": "218000",
    "end": "312000"
  },
  {
    "text": "I'm sorry. Can I interrupt? Yeah, for sure. We're not just thinking of\nexamples of the failing, the worst of the text. I can post them up, yes.",
    "start": "221030",
    "end": "228322"
  },
  {
    "text": "[INAUDIBLE] what's bad\nand what's [INAUDIBLE].. Yes, yes, yes, yes. [INAUDIBLE] [LAUGHTER]",
    "start": "228322",
    "end": "235220"
  },
  {
    "text": "Wait, sorry. One last question. When you have\nthese 10-- you said we took the best of the 10. Best in what sense? Yeah, so this is human-judged.",
    "start": "235220",
    "end": "242020"
  },
  {
    "text": "And I'll probably\nexpand a little bit on that more today, yeah. So I want to end this kind of\nfly by overview with GPT-3.",
    "start": "242020",
    "end": "251620"
  },
  {
    "text": "And since GPT-2 already\nproduces such coherent text, how do you characterize GPT-3?",
    "start": "251620",
    "end": "257500"
  },
  {
    "text": "And I would say\nthat the best way to do so is that say you\ntook the best of out of five",
    "start": "257500",
    "end": "264340"
  },
  {
    "text": "or ten completions from GPT-2. That would be kind of your\nfirst completion from GPT-3. And of course, best is kind\nof a personal metric here.",
    "start": "264340",
    "end": "274220"
  },
  {
    "text": "So here, I'm showing a\ncompletion from the book Three-Body Problem.",
    "start": "274220",
    "end": "279980"
  },
  {
    "text": "And you can see that\nthe impressive things about this completion\nare that it really stays",
    "start": "279980",
    "end": "285310"
  },
  {
    "text": "true to the style of the novel. I think the second thing\nthat kind of impressed",
    "start": "285310",
    "end": "290980"
  },
  {
    "text": "me was just how poetic like the\nmetaphors and similes that it produces are. So you have this\nstuff like blood",
    "start": "290980",
    "end": "297280"
  },
  {
    "text": "was seeping through a\njacket and a dark red flower was blooming on her chest,\nlike these kind of very, very",
    "start": "297280",
    "end": "302560"
  },
  {
    "text": "poetic and stylistic sentences. So it definitely understands\nit's part of a novel, and it's trying to\ngenerate this kind of prose",
    "start": "302560",
    "end": "309910"
  },
  {
    "text": "in the same style. So as generated text becomes\nmore and more coherent,",
    "start": "309910",
    "end": "316004"
  },
  {
    "start": "312000",
    "end": "549000"
  },
  {
    "text": "I think one of the really-- [INAUDIBLE] how much bigger is\nit in terms of the parameters, is GPT-3?",
    "start": "316005",
    "end": "321539"
  },
  {
    "text": "Yeah, yeah, so it's 175 billion\nparameters versus GPT-2, which is around 1 billion.",
    "start": "321540",
    "end": "326580"
  },
  {
    "text": "[INAUDIBLE]  Do you feel like that very\nsubtle increase in accuracy",
    "start": "326580",
    "end": "334305"
  },
  {
    "text": "is the root cause of how\nmuch difference [INAUDIBLE]?? Yeah, that's a\nvery good question. ",
    "start": "334305",
    "end": "340061"
  },
  {
    "text": "So there's kind of\nstuff-- maybe we can dive into it a\nlittle bit after, but there is work on\nneural scaling laws.",
    "start": "340062",
    "end": "345280"
  },
  {
    "text": "And so the idea is like, can\nyou predict the performance of a larger model from a\nseries of smaller models?",
    "start": "345280",
    "end": "350610"
  },
  {
    "text": "And so I would rather\ncharacterize the increase in performance not by the\nsmall gain in perplexity,",
    "start": "350610",
    "end": "355710"
  },
  {
    "text": "but whether it lines up\nwith the projections. And in that sense, GPT-3 does.",
    "start": "355710",
    "end": "360840"
  },
  {
    "text": "So yeah, that's\nsome intuition for-- yeah. I think personally, I hope\nOpenAI would have stopped",
    "start": "360840",
    "end": "366690"
  },
  {
    "text": "the experiment if it did it. So yeah. No, I just think it's\ninteresting for, this is more of a general thing.",
    "start": "366690",
    "end": "372600"
  },
  {
    "text": "[INAUDIBLE] In machine learning,\nyou see people pushing for like an extra\n1% to probably 5% accuracy,",
    "start": "372600",
    "end": "380100"
  },
  {
    "text": "but the models are increasing\nat a scale that's exponential. Right. So I wonder sometimes\nwhether it's worth it",
    "start": "380100",
    "end": "387460"
  },
  {
    "text": "and where you should\nstop [INAUDIBLE].. Right. Yeah, I think maybe this slide\nwill get to it a little bit.",
    "start": "387460",
    "end": "393490"
  },
  {
    "text": "But there's also\nsome sense in which like as you reach kind of like\nthe entropy floor of modeling,",
    "start": "393490",
    "end": "399930"
  },
  {
    "text": "every halving kind\nof gives you-- if you think about accuracy,\nit's not on a linear scale.",
    "start": "399930",
    "end": "407039"
  },
  {
    "text": "A 1% early on isn't the\nsame as that last 1%. And so those last bits\nreally do help you squeeze",
    "start": "407040",
    "end": "415530"
  },
  {
    "text": "a little bit out of that. That's obvious. Yep. Sorry. [INAUDIBLE] the\n[INAUDIBLE] access too?",
    "start": "415530",
    "end": "420755"
  },
  {
    "text": "Oh yes. Sorry, this is\naccuracy [INAUDIBLE].. I will explain this slide. Cool.",
    "start": "420755",
    "end": "426930"
  },
  {
    "text": "So as generated text becomes\nmore and more realistic, I think one very\nnatural question to ask is whether humans\ncan still distinguish",
    "start": "426930",
    "end": "433800"
  },
  {
    "text": "between real and\nfake attempts, right? And here we have-- this is, of course, a\nvery set up scenario.",
    "start": "433800",
    "end": "441870"
  },
  {
    "text": "In all cases, the models\nwouldn't trick humans. But this is for news\narticles, we kind of",
    "start": "441870",
    "end": "447240"
  },
  {
    "text": "presented GPT-3\ngenerated samples against real news articles. And you can tell as the number\nof parameters increases,",
    "start": "447240",
    "end": "454889"
  },
  {
    "text": "the ability of\nhumans to distinguish between the real\nand fake articles-- that ability goes down\nto near random chance.",
    "start": "454890",
    "end": "461825"
  },
  {
    "text": "And, oh, yes? How did you generate\nthe news articles?",
    "start": "461825",
    "end": "466950"
  },
  {
    "text": "What prompts did you use? Oh, I'm actually\nnot completely sure.",
    "start": "466950",
    "end": "472600"
  },
  {
    "text": "So I didn't do this\nwork particularly, but I think one\npossible approach would be to prime with a couple\nof news articles and then",
    "start": "472600",
    "end": "480240"
  },
  {
    "text": "just to have a\ndelimiter and just have it start generating\nnews articles from there. Yeah?",
    "start": "480240",
    "end": "487060"
  },
  {
    "text": "Any other quick questions?  Great.",
    "start": "487060",
    "end": "492660"
  },
  {
    "text": "So even with all of\nthese impressive results, I think it's worth taking a step\nback at this point and asking,",
    "start": "492660",
    "end": "498060"
  },
  {
    "text": "what do we really care\nabout language modeling for? And what is it\nactually useful for?",
    "start": "498060",
    "end": "503669"
  },
  {
    "text": "I think one can make the\nargument that it is actually a fairly narrow capability. Why would you just\nwant some system that",
    "start": "503670",
    "end": "509760"
  },
  {
    "text": "just continues text for you? And you could argue that\nthere's more important tasks to solve, like summarization\nor translation.",
    "start": "509760",
    "end": "517567"
  },
  {
    "text": "And I think most\nresearchers at OpenAI would agree with\nthis point of view. And in fact, GPT was\nnot really a project",
    "start": "517567",
    "end": "524760"
  },
  {
    "text": "that was focused on language\nmodeling as an end goal, but mostly as a tool to\nsolve a problem called",
    "start": "524760",
    "end": "530880"
  },
  {
    "text": "unsupervised learning, which\nI'm going to go through in the next couple of slides. So I want to do a history of\nlanguage modeling at OpenAI",
    "start": "530880",
    "end": "539070"
  },
  {
    "text": "and hopefully motivate\nwhy we ended up at the GPT series of models, and\nkind of how we arrived there.",
    "start": "539070",
    "end": "545190"
  },
  {
    "text": "And hopefully it will\nbecome much more intuitive after this section.",
    "start": "545190",
    "end": "551209"
  },
  {
    "start": "549000",
    "end": "638000"
  },
  {
    "text": "So the deep learning\nboom started in 2012 with AlexNet, which\nwas a system that could take images and labels,\nand it could classify images",
    "start": "551210",
    "end": "559740"
  },
  {
    "text": "to their labels. And what we found with\nAlexNet was these systems were able to generalize\nsurprisingly well.",
    "start": "559740",
    "end": "566220"
  },
  {
    "text": "You could take data sets\nthat weren't necessarily in the training\ndistribution, and you just flag pretty good features on it.",
    "start": "566220",
    "end": "572170"
  },
  {
    "text": "And since then, this kind\nof supervised approach has been really,\nreally powerful, right? We've been able to train models\nin many different domains",
    "start": "572170",
    "end": "579360"
  },
  {
    "text": "to classify very accurately. And you can even\nhave some guarantees",
    "start": "579360",
    "end": "584370"
  },
  {
    "text": "that supervised\nlearning will work well. So there's empirical\nrisk minimization.",
    "start": "584370",
    "end": "589543"
  },
  {
    "text": "But the problem with\nsupervised learning is that oftentimes the\nlabels are scarce, right,",
    "start": "589543",
    "end": "594570"
  },
  {
    "text": "especially in language tasks. There isn't really\nthat many kind of text paired with their\nsummaries, or too many pairs",
    "start": "594570",
    "end": "600750"
  },
  {
    "text": "across languages for instance. So collecting a lot of data can\nbe not too hard, but actually",
    "start": "600750",
    "end": "607590"
  },
  {
    "text": "scalably labeling\nall of that data, it could be very time\nconsuming, and expensive.",
    "start": "607590",
    "end": "612850"
  },
  {
    "text": "So the main problem with\nunsupervised learning is can we also learn\nfrom unlabeled data?",
    "start": "612850",
    "end": "618209"
  },
  {
    "text": "And this is a lot scarier,\nbecause, all of a sudden, we're starting to optimize\nan objective, which",
    "start": "618210",
    "end": "623460"
  },
  {
    "text": "isn't the one we care\nabout downstream, right? So a lot of the guarantees\nthat we used to have,",
    "start": "623460",
    "end": "628740"
  },
  {
    "text": "we no longer have. And we can only\nkind of hope that we learn some features that are\nadaptable to a wide variety",
    "start": "628740",
    "end": "636360"
  },
  {
    "text": "of downstream tasks. But nevertheless,\nthere's a reason to be very optimistic\nin language.",
    "start": "636360",
    "end": "643630"
  },
  {
    "start": "638000",
    "end": "671000"
  },
  {
    "text": "And the reason is that there is\na huge trove of unlabeled data. And it's called the internet.",
    "start": "643630",
    "end": "648910"
  },
  {
    "text": "And so the real\nquestion is, can we leverage all this available\ndata from the internet to solve language tasks\nwhere we don't really",
    "start": "648910",
    "end": "655829"
  },
  {
    "text": "have that much data? And the hope is that\nif we kind of pretrain this model on the\ninternet, it will",
    "start": "655830",
    "end": "661870"
  },
  {
    "text": "see all of these words\nused in different settings, kind of understand\nthe relationships, and they'll be able to leverage\nthis kind of understanding",
    "start": "661870",
    "end": "668160"
  },
  {
    "text": "for any kind of task du jour. So now that we've\nestablished why language",
    "start": "668160",
    "end": "674130"
  },
  {
    "start": "671000",
    "end": "780000"
  },
  {
    "text": "is such a good domain to try\nunsupervised learning in, let's talk about why use\ngenerative models for it,",
    "start": "674130",
    "end": "679920"
  },
  {
    "text": "and also why use autoregressive\ngenerative models. And I do want to stress that a\nlot of the guarantees we have",
    "start": "679920",
    "end": "686129"
  },
  {
    "text": "with supervised learning are no\nlonger there for unsupervised learning. So some of these\narguments will be a little bit kind of intuitive.",
    "start": "686130",
    "end": "694380"
  },
  {
    "text": "And so the first argument I\nwant to present is this quote by Richard Feynman which\nis pretty widespread,",
    "start": "694380",
    "end": "700379"
  },
  {
    "text": "\"What I cannot create,\nI do not understand.\" And there's the inverse of this\nidea, which we call analysis",
    "start": "700380",
    "end": "706110"
  },
  {
    "text": "by synthesis. And it's \"What I can create,\nI can also understand.\" And this has been studied\nby Josh Tenenbaum.",
    "start": "706110",
    "end": "713460"
  },
  {
    "text": "There's definitely some kind\nof biological motivation as well for it.",
    "start": "713460",
    "end": "720310"
  },
  {
    "text": "But the idea here\nis that if you're able to create a language\nmodel which can generate",
    "start": "720310",
    "end": "725820"
  },
  {
    "text": "diverse samples that\nare coherent, then it must also build\nup representations that can help you solve\nlanguage understanding tasks.",
    "start": "725820",
    "end": "733800"
  },
  {
    "text": "And then the next\nquestion is, why do we use autoregressive models? You might argue that\nautoregressive models",
    "start": "733800",
    "end": "739890"
  },
  {
    "text": "are a kind of local objective. You're just predicting\nthe next words. You could do really well with\nsome n-gram approximation.",
    "start": "739890",
    "end": "747800"
  },
  {
    "text": "Why would it be good\nat solving things that allow you to summarize\nan entire piece of text?",
    "start": "747800",
    "end": "753540"
  },
  {
    "text": "And so, an intuitive\nargument here could be, say that you wanted\nto do very well on language",
    "start": "753540",
    "end": "759690"
  },
  {
    "text": "modeling for a mystery novel. And there's this grand\nreveal at the end, like, oh, the culprit was--",
    "start": "759690",
    "end": "765510"
  },
  {
    "text": "and then you want to\npredict that next token. And to do really\nwell at that task, you really need to have\na good understanding",
    "start": "765510",
    "end": "771600"
  },
  {
    "text": "of what happened in the story\nalong with all the twists and turns, and maybe even some\nof this kind of like deductive",
    "start": "771600",
    "end": "776700"
  },
  {
    "text": "reasoning built in.  So the first sign of life--",
    "start": "776700",
    "end": "782582"
  },
  {
    "start": "780000",
    "end": "851000"
  },
  {
    "text": "did you have a question? [INAUDIBLE] Oh, yeah.",
    "start": "782582",
    "end": "788120"
  },
  {
    "text": "So the first sign of\nlife we had at OpenAI was in the task of predicting\nwhether Amazon reviews were",
    "start": "788120",
    "end": "793920"
  },
  {
    "text": "positive or negative. And this was worked on in 2017. So instead of\ntraining a classifier",
    "start": "793920",
    "end": "799620"
  },
  {
    "text": "in the kind of typical\nsupervised way, what we did was we trained an\nLSTM model just to predict the next\ncharacter in Amazon reviews.",
    "start": "799620",
    "end": "808300"
  },
  {
    "text": "And when we trained a\nlinear model on the features from this LSTM, what\nwe found, surprisingly, was one of these cells\nor one of these neurons",
    "start": "808300",
    "end": "816660"
  },
  {
    "text": "was firing in terms of\npredicting sentiment. And positive activations\nfor this neuron",
    "start": "816660",
    "end": "823230"
  },
  {
    "text": "corresponded to\npositive reviews, and negative activations\nto negative reviews. And this was despite not\nseeing any of the labels",
    "start": "823230",
    "end": "830730"
  },
  {
    "text": "at training time. So you can even track\nkind of what this neuron value is across a sample.",
    "start": "830730",
    "end": "837352"
  },
  {
    "text": "So it's a little\nbit hard to read, but these are reviews\nwhere maybe someone says, oh, I really liked this film,\nbut I didn't like this part.",
    "start": "837353",
    "end": "843326"
  },
  {
    "text": "And you can kind of see the\nsentiment switching as you go from positive to negative.",
    "start": "843327",
    "end": "849690"
  },
  {
    "text": "So yeah, just predicting\nthe next character resulted in-- oh yeah? Was there any sort of\n[INAUDIBLE] architecture",
    "start": "849690",
    "end": "857900"
  },
  {
    "start": "851000",
    "end": "921000"
  },
  {
    "text": "to encourage this? No, this was just a pure LSTM. OK.",
    "start": "857900",
    "end": "862960"
  },
  {
    "text": "So you guys came up\nwith all the neurons, saw which ones were closest? Yeah, in the hidden state. Yeah. So you train a linear\nclassifier on top of that.",
    "start": "862960",
    "end": "868270"
  },
  {
    "text": "And one neuron is firing with-- yeah, just outsized\npredictive power.",
    "start": "868270",
    "end": "873930"
  },
  {
    "text": "Yeah, great. So next, GPT-1 was one of\nthe first demonstrations that this kind of approach\ncould work broadly for text.",
    "start": "873930",
    "end": "880710"
  },
  {
    "text": "So GPT-1 was trained on the\ninternet, not on Amazon reviews anymore. And it was fine tuned on a bunch\nof different downstream tasks.",
    "start": "880710",
    "end": "888769"
  },
  {
    "text": "And one thing to stress here\nwas, kind of to your point that the fine tuning was very--",
    "start": "888770",
    "end": "895310"
  },
  {
    "text": "I guess minimally--\nyou're not kind of bashing the architecture\napart and kind of repurposing",
    "start": "895310",
    "end": "903050"
  },
  {
    "text": "a new module. So it's just a new head that\nclassifies for your task.",
    "start": "903050",
    "end": "909560"
  },
  {
    "text": "And this showed that you\ncan use this approach not just for sentiment analysis,\nbut also for entailments,",
    "start": "909560",
    "end": "915500"
  },
  {
    "text": "and semantic similarity,\nand getting SotAs on a lot of these\nbenchmarks downstream.",
    "start": "915500",
    "end": "921760"
  },
  {
    "start": "921000",
    "end": "1004000"
  },
  {
    "text": "So I've already presented\nGPT-2 from the point of view of a very\npowerful language model. And now, I think it's worth\nrevisiting from the viewpoint",
    "start": "921760",
    "end": "929410"
  },
  {
    "text": "of unsupervised learning. So like GPT-1, GPT-2 was\ntrained on a large chunk of the internet.",
    "start": "929410",
    "end": "936080"
  },
  {
    "text": "And it's only trained to\npredict the next token or word from previous words. But the key insight of GPT-2\nis that many downstream tasks",
    "start": "936080",
    "end": "944199"
  },
  {
    "text": "can be expressed naturally as\na language model in the past. And yeah, so GPT-2\nexplores how well",
    "start": "944200",
    "end": "951010"
  },
  {
    "text": "we can perform on\ndownstream tasks simply by using this method\nwithout any fine tuning, right?",
    "start": "951010",
    "end": "956140"
  },
  {
    "text": "So let me start with\na couple of examples. So let's say you want to solve\nsome reading comprehension",
    "start": "956140",
    "end": "961420"
  },
  {
    "text": "benchmark. And this is usually set\nup as a prompt, which is some passage\nyou have to read, and then a bunch of questions,\nwhich you have to answer.",
    "start": "961420",
    "end": "968140"
  },
  {
    "text": "So you can literally just stick\nthe entire project in context. You put a question,\ncolon, you write out",
    "start": "968140",
    "end": "973959"
  },
  {
    "text": "the question, answer, colon. And then have the model\ncomplete from there. And this gives you Zero-Shot\nreading comprehension.",
    "start": "973960",
    "end": "982920"
  },
  {
    "text": "We can also use it for other\ntasks, like summarization. For instance,\nhere's the beginning",
    "start": "982920",
    "end": "988490"
  },
  {
    "text": "of a CNN article about kind of\nsome archaeological finding.",
    "start": "988490",
    "end": "993600"
  },
  {
    "text": "And you can just put TLDR\nafter you see this passage. And the model, hopefully,\nif it's good enough,",
    "start": "993600",
    "end": "999890"
  },
  {
    "text": "will produce good summaries.  And the final example\nI want to show",
    "start": "999890",
    "end": "1005760"
  },
  {
    "start": "1004000",
    "end": "1095000"
  },
  {
    "text": "is that you can do Zero-Shot\ntranslation as well. So the way you would do this\nis if you wanted to convert,",
    "start": "1005760",
    "end": "1012329"
  },
  {
    "text": "let's say, a French\nsentence into English, you could set up a\nprompt like the sentence, insert the French sentence,\n\"translated from French",
    "start": "1012330",
    "end": "1018900"
  },
  {
    "text": "to English means,\" and then\nthe model will complete. And they can sometimes\ndo this well.",
    "start": "1018900",
    "end": "1024970"
  },
  {
    "text": "And one kind of\ncritical thing to note here is here's a\nchart of performance as you increase the\nnumber of parameters.",
    "start": "1024970",
    "end": "1032220"
  },
  {
    "text": "And all these models are\ntrained on the same data sets,",
    "start": "1032220",
    "end": "1037799"
  },
  {
    "text": "so the only kind of\nconfounding variable is scale. And you can see that as\nwe scale up the models, these kind of\nZero-Shot capabilities",
    "start": "1037800",
    "end": "1044699"
  },
  {
    "text": "emerge and kind of\nsmoothly get better. So the role of scale\nis important here.",
    "start": "1044700",
    "end": "1052710"
  },
  {
    "text": "And I think these are\nstarting to approach-- I guess they're not great\nbenchmarks, but at least respectable benchmarks.",
    "start": "1052710",
    "end": "1059590"
  },
  {
    "text": "[INAUDIBLE] Yeah, exactly. It's not going to be\ngreat in a lot of cases. And to be honest,\nthe blue metric",
    "start": "1059590",
    "end": "1067170"
  },
  {
    "text": "used for translation\nis actually often-- thank you very much. It's not a great metric.",
    "start": "1067170",
    "end": "1072900"
  },
  {
    "text": "What it does is it takes\na reference solution. And basically, it does some\nkind of like n-gram comparison.",
    "start": "1072900",
    "end": "1079840"
  },
  {
    "text": "So it is a big problem\nto have good translation metrics in NLP.",
    "start": "1079840",
    "end": "1086080"
  },
  {
    "text": " And yeah, I think when\nI talk about code, I'll talk a little\nmore about [INAUDIBLE]..",
    "start": "1086080",
    "end": "1092481"
  },
  {
    "text": " So let's finally talk about how\nGPT-3 fits into this picture.",
    "start": "1092481",
    "end": "1100500"
  },
  {
    "start": "1095000",
    "end": "1163000"
  },
  {
    "text": "So the primary\ninsight of GPT-3 is that the training process\nitself can be interpreted in the context of metalearning,\nwhich is kind of like learning",
    "start": "1100500",
    "end": "1108080"
  },
  {
    "text": "over a distribution of tasks. And during training,\nwhat the model is doing is it's developing\ncertain kind of capabilities,",
    "start": "1108080",
    "end": "1114410"
  },
  {
    "text": "it's picking up some\nset of skills in terms of modeling certain passages.",
    "start": "1114410",
    "end": "1121640"
  },
  {
    "text": "And during inference\ntime, what it's doing, it's kind of quickly picking up\non what a task is based on what",
    "start": "1121640",
    "end": "1128450"
  },
  {
    "text": "the prompt is so far,\nand adapting to that task to predict the next token. So you can kind of view this\nas outward loop of all the SGDs",
    "start": "1128450",
    "end": "1136730"
  },
  {
    "text": "that you're doing\nduring training, and this inward loop\nof kind of picking up on what the task is, and\nthen modeling the next token.",
    "start": "1136730",
    "end": "1144120"
  },
  {
    "text": "So you can imagine a lot of\ntasks being framed in this way. For instance, on the left,\nyou can have addition.",
    "start": "1144120",
    "end": "1150260"
  },
  {
    "text": "You have a lot of examples\nof machine context. And hopefully, that would\nhelp you with a new addition",
    "start": "1150260",
    "end": "1156530"
  },
  {
    "text": "problem, or you can try\nto kind of unscramble a word for instance. And I'll explore results on\nthese two kind of benchmarks",
    "start": "1156530",
    "end": "1163910"
  },
  {
    "start": "1163000",
    "end": "1214000"
  },
  {
    "text": "in the next slides. So this setting, you can\ncall two-shot arithmetic. And just to explain\nwhat's going on,",
    "start": "1163910",
    "end": "1170510"
  },
  {
    "text": "you're taking the entire context\nslide of your transformer and you're putting in as\nmany examples as will fit.",
    "start": "1170510",
    "end": "1176179"
  },
  {
    "text": "And then finally, you\nput in the example that you would like to solve. So here, these examples\ncould be these kind",
    "start": "1176180",
    "end": "1185450"
  },
  {
    "text": "of first three\naddition problems, and then you have\n31 plus 41 equals.",
    "start": "1185450",
    "end": "1190970"
  },
  {
    "text": "And you ask the\nmodel to complete. So you notice that as the\nlanguage model gets bigger,",
    "start": "1190970",
    "end": "1195990"
  },
  {
    "text": "it's better able to\nrecognize this task. And you can see that performance\non addition, subtraction,",
    "start": "1195990",
    "end": "1202460"
  },
  {
    "text": "even some kind of\nmultiplication tasks increases sharply as you go\ntowards 200 billion parameters.",
    "start": "1202460",
    "end": "1208250"
  },
  {
    "text": "And there does seem to be kind\nof some step function change right here. And looking at\nword unscrambling,",
    "start": "1208250",
    "end": "1215750"
  },
  {
    "start": "1214000",
    "end": "1236000"
  },
  {
    "text": "this is also true. So we have parameters again on\nthe x-axis, we have accuracy. And each of these is a different\nkind of unscramble task.",
    "start": "1215750",
    "end": "1223190"
  },
  {
    "text": "So this blue line\nis you kind of do a cyclic shift of the letters,\nand you wanted to uncycle.",
    "start": "1223190",
    "end": "1228620"
  },
  {
    "text": "And there's a lot\nof other transforms you can do, like randomly\ninserting words for instance. ",
    "start": "1228620",
    "end": "1236980"
  },
  {
    "start": "1236000",
    "end": "1422000"
  },
  {
    "text": "So the final point\nhere is that this is a pretty general phenomenon. We didn't just test it on\nthese two aforementioned tasks.",
    "start": "1236980",
    "end": "1245100"
  },
  {
    "text": "We tried an array of\nI think 40 plus tasks. And here, you can see how\nthe Zero-Shot, One-Shot,",
    "start": "1245100",
    "end": "1250920"
  },
  {
    "text": "and Few-Shot\nperformance increases as we scale the models. So of course, they're\nall smoothly increasing.",
    "start": "1250920",
    "end": "1256740"
  },
  {
    "text": "But one thing to be aware of is\nthat the gap between Zero-Shot and Few-Shot is also improving\nas a function of scale.",
    "start": "1256740",
    "end": "1263190"
  },
  {
    "text": " Awesome.",
    "start": "1263190",
    "end": "1269140"
  },
  {
    "text": "So we've just seen that\nwe can pretrain the-- Oh. Go ahead. Sorry, with few-shot learning,\nI was curious, [INAUDIBLE]..",
    "start": "1269140",
    "end": "1278500"
  },
  {
    "text": "One is the tasks\nthemselves that were used. Two is the number of parameters.",
    "start": "1278500",
    "end": "1284710"
  },
  {
    "text": "And then three, my\nunderstanding is also the quantity of [INAUDIBLE]. I was curious, between\nthose three, which ones--",
    "start": "1284710",
    "end": "1291460"
  },
  {
    "text": "you've shown a lot of examples. The number of parameters\ndefinitely helps. I was curious though if you\nhad a sense of the degree",
    "start": "1291460",
    "end": "1296860"
  },
  {
    "text": "to which also the training\ntasks and the sophistication of the tasks, as well as\nthe quantity of [INAUDIBLE] adjustments [INAUDIBLE].",
    "start": "1296860",
    "end": "1303570"
  },
  {
    "text": "Yeah. So I guess I can dive-- maybe it's something\nto save for or after.",
    "start": "1303570",
    "end": "1309669"
  },
  {
    "text": "Yeah, let's dig into that after. Yes? Just a thought, [INAUDIBLE]\na little bit, too, right?",
    "start": "1309670",
    "end": "1315940"
  },
  {
    "text": "I guess GPT-2 and\n3 aren't different. GPT-1 just has an extra\nclassification head",
    "start": "1315940",
    "end": "1321467"
  },
  {
    "text": "for certain tasks here.  Great, yeah. Good questions.",
    "start": "1321467",
    "end": "1328270"
  },
  {
    "text": "So yeah, we've just seen\nthat we can use a transformer in this kind of pretrained\nthe [INAUDIBLE] setups, where we have a lot of unlabeled\ndata in the pretraining",
    "start": "1328270",
    "end": "1337070"
  },
  {
    "text": "setting. And we have just a\nlittle bit of data in the fine-tuned settings. And we can solve a lot of\nlanguage tasks in this way.",
    "start": "1337070",
    "end": "1344419"
  },
  {
    "text": "And I would say this has\nbecome the dominant paradigm in language over the\nlast couple of years. So there's follow up\nobjectives, like BERT and T5,",
    "start": "1344420",
    "end": "1352060"
  },
  {
    "text": "which have done extremely\ngood at pushing the SotA. But there's nothing\nreally that says that these transformer models\nhave to be applied to language.",
    "start": "1352060",
    "end": "1360309"
  },
  {
    "text": "The transformer is\na sequence model. And as such, it can just\ningest any sequence of bytes and model them.",
    "start": "1360310",
    "end": "1366613"
  },
  {
    "text": "And when you think about\nthis, all of the data that we consume,\nlike videos or audio, they're represented\non our computers",
    "start": "1366613",
    "end": "1372550"
  },
  {
    "text": "as sequences of bytes, right? And so we might think,\noh, could this approach be used to just model\nwhatever modality we want?",
    "start": "1372550",
    "end": "1381070"
  },
  {
    "text": "And I think this\nkind of paradigm is very at least interesting\nwhen we don't really",
    "start": "1381070",
    "end": "1388350"
  },
  {
    "text": "have good inductive biases. We don't necessarily\nupdate them. But one question to\nask is, does it even",
    "start": "1388350",
    "end": "1393779"
  },
  {
    "text": "work when you do have really\nstrong inductive biases? So I'm going to\npresent some work that",
    "start": "1393780",
    "end": "1400350"
  },
  {
    "text": "suggests that the\nanswer is yes, it still works fairly well in this case\nin the domain of images, where",
    "start": "1400350",
    "end": "1406950"
  },
  {
    "text": "convolutions are already\nso popular and proven out. And I'm going to show a second\nresult very briefly here,",
    "start": "1406950",
    "end": "1412990"
  },
  {
    "text": "which is DALL-E, which shows\nthat it's strong enough to even ingest two\ndifferent modalities and be able to\njointly model them.",
    "start": "1412990",
    "end": "1419160"
  },
  {
    "text": " So the first question is, how\nwould you apply GPT to images?",
    "start": "1419160",
    "end": "1426670"
  },
  {
    "start": "1422000",
    "end": "1531000"
  },
  {
    "text": "And there's a few\nthings you have to do. You have to modify this\nautoregressive next word prediction objective.",
    "start": "1426670",
    "end": "1433100"
  },
  {
    "text": "So the natural analog is\nyou can think of images as a very strange language,\nwhere the words are pixels",
    "start": "1433100",
    "end": "1438790"
  },
  {
    "text": "instead. And instead, you need to predict\nthe next pixel at each point. And so we can just change the\nobjective for the next word",
    "start": "1438790",
    "end": "1445810"
  },
  {
    "text": "prediction to next\npixel prediction. And of course, we want\nthis kind of large-- yeah?",
    "start": "1445810",
    "end": "1450991"
  },
  {
    "text": "[INAUDIBLE] Oh, yeah. So you just unroll\nit as a sequence.",
    "start": "1450991",
    "end": "1456750"
  },
  {
    "text": "It's the same way it's\nstored on a computer. You just have a secret\nspice, yeah, yeah. Good question.",
    "start": "1456750",
    "end": "1463179"
  },
  {
    "text": "So in the language\nsetting, we pretrain on this large unlabeled\ndata set on the internet,",
    "start": "1463180",
    "end": "1468990"
  },
  {
    "text": "and we fine tuned on\nquestion answering or these other benchmarks. In images, one good\nanalog of the situation",
    "start": "1468990",
    "end": "1475980"
  },
  {
    "text": "is you can pretrain on\nImageNet without the labels. If you have, let's say, a\nlow resource-- a low data,",
    "start": "1475980",
    "end": "1481125"
  },
  {
    "text": "sorry, setting like CIFAR. And you can try to attack\nCIFAR classification. And of course, in both settings,\nyou can do fine tuning.",
    "start": "1481125",
    "end": "1488490"
  },
  {
    "text": "In GPT, you can do Zero-Shot. And I would say\nthe standard eval on images is you\ndo linear probes,",
    "start": "1488490",
    "end": "1493990"
  },
  {
    "text": "so you take features\nfrom your model. The model is frozen. You pass through CIFAR\nthrough-- do the model,",
    "start": "1493990",
    "end": "1499890"
  },
  {
    "text": "get some features. And you see how\npredictive these features are of the CIFAR classes.",
    "start": "1499890",
    "end": "1506010"
  },
  {
    "text": "Is it kind of PixelCNN\nwhich basically asks a model to predict the next\npixel given the [INAUDIBLE]..",
    "start": "1506010",
    "end": "1512260"
  },
  {
    "text": "Yeah. So PixelCNN is an instantiation\nof an autoregressive image generation model.",
    "start": "1512260",
    "end": "1517269"
  },
  {
    "text": "So what we're asking here\nis, can we actually take the same transformer\narchitecture that we use in language, don't make\nany modifications at all,",
    "start": "1517270",
    "end": "1524100"
  },
  {
    "text": "and just throw-- so there's no kind of\n2D prior on this, yeah.",
    "start": "1524100",
    "end": "1529230"
  },
  {
    "text": " So yeah, I'll call this model\nthat we trained Image GPT-2,",
    "start": "1529230",
    "end": "1535389"
  },
  {
    "start": "1531000",
    "end": "1584000"
  },
  {
    "text": "or IGPT for short. And here, you can see\nactually what some completions from the model look like.",
    "start": "1535390",
    "end": "1541760"
  },
  {
    "text": "So on the left column\nwhat I'm feeding in is the pixels of the\nfirst half of the image.",
    "start": "1541760",
    "end": "1547390"
  },
  {
    "text": "In the next four columns,\nwhat you're seeing is different model\ngenerated completions.",
    "start": "1547390",
    "end": "1553120"
  },
  {
    "text": "In the right column here is\nthe original reference image. And you can actually\nsee that the model",
    "start": "1553120",
    "end": "1558430"
  },
  {
    "text": "is kind of doing some\ninteresting things, right? If you look at\nthe last two rows, it's not coming up with kind of\nmagically the same completion",
    "start": "1558430",
    "end": "1564400"
  },
  {
    "text": "every single time. It's like putting these\nbirds in different settings, sometimes adding reflections.",
    "start": "1564400",
    "end": "1570040"
  },
  {
    "text": "It's putting this\nlighthouse in grassy areas and like watery\nareas for instance.",
    "start": "1570040",
    "end": "1575149"
  },
  {
    "text": "So if you buy into this\nphilosophy of analysis by synthesis, we\ndefinitely have some hint",
    "start": "1575150",
    "end": "1580779"
  },
  {
    "text": "of the synthesis part. So I don't have time to go\nthrough all of the results with",
    "start": "1580780",
    "end": "1586510"
  },
  {
    "start": "1584000",
    "end": "1940000"
  },
  {
    "text": "you, but I just want to say that\nit is fairly successful in this CIFAR setting where you don't\nhave much labelled data.",
    "start": "1586510",
    "end": "1593430"
  },
  {
    "text": "If you train a linear model\non top of the features, you get better results than\nif you do the same approach",
    "start": "1593430",
    "end": "1600820"
  },
  {
    "text": "with a ResNet trained\non ImageNet with labels. So that's the typical\napproach in the paper. You train some\nResNet on ImageNet,",
    "start": "1600820",
    "end": "1606909"
  },
  {
    "text": "you get the features-- oh, yeah? [INAUDIBLE] Oh, yeah. And if you compare to this\napproach, a generative model",
    "start": "1606910",
    "end": "1613330"
  },
  {
    "text": "on ImageNet without the\nlabels, take the features. It's actually better\npredictive of [INAUDIBLE]..",
    "start": "1613330",
    "end": "1619120"
  },
  {
    "text": "Yeah, [INAUDIBLE]. What if the architecture for\nthis is the same [INAUDIBLE]?? Oh, yeah. [INAUDIBLE] Exactly, yeah.",
    "start": "1619120",
    "end": "1624340"
  },
  {
    "text": "[INAUDIBLE] Yeah, yeah, yeah. It's the GPT\narchitecture, yeah, yeah.",
    "start": "1624340",
    "end": "1629440"
  },
  {
    "text": "So you can modify GPT\nto have like 2D bias. Like you can do 2D\nposition embeddings. We'll be able to do that.",
    "start": "1629440",
    "end": "1635282"
  },
  {
    "text": "We just want to see can you\nuse the same exact approach. Yeah? So earlier, you said the\ndata's just sequential.",
    "start": "1635282",
    "end": "1641356"
  },
  {
    "text": "But also there's\nlike metadata showing about how that sequential should\nbe reconstructed at the end. So what's the\nwidth, for example.",
    "start": "1641357",
    "end": "1648790"
  },
  {
    "text": "Oh, can you explain? Yeah. Sorry if I didn't say that well. So the data on this [INAUDIBLE]? Yes.",
    "start": "1648790",
    "end": "1654159"
  },
  {
    "text": "OK. But when you want to transform\nthis sequence into an image, you have metadata that\nwill say something like-- just like in\nNumPy arrays, it'll say,",
    "start": "1654160",
    "end": "1661059"
  },
  {
    "text": "here's the stride. So you're just going to\nrearrange it [INAUDIBLE].. I see. What I'm curious\nto know, is does",
    "start": "1661060",
    "end": "1666220"
  },
  {
    "text": "GPT, before it's given\nan image, at least given this metadata [INAUDIBLE]? I see. Yeah, that's an\nextremely good question.",
    "start": "1666220",
    "end": "1672817"
  },
  {
    "text": "Because I don't know how\nthis problem is solved. Yeah. In this case, all the\nimages have the same shape.",
    "start": "1672817",
    "end": "1679700"
  },
  {
    "text": "Oh, OK. OK, cool. But we don't tell it\nlike the concept of row",
    "start": "1679700",
    "end": "1685480"
  },
  {
    "text": "within the model, yeah. But if all images are the same? Yeah, so it needs to\nlearn it from the data. But yeah, the data\nlooks the same.",
    "start": "1685480",
    "end": "1691950"
  },
  {
    "text": "Got it. [INAUDIBLE] variable\nimage shapes, then they can just\nsubmit [INAUDIBLE]..",
    "start": "1691950",
    "end": "1697170"
  },
  {
    "text": "Yeah. Mhm.  Aren't there a lot more\npixels than there are",
    "start": "1697170",
    "end": "1704460"
  },
  {
    "text": "[INAUDIBLE] sizes [INAUDIBLE]? Yes. This is pretty low\nresolution images. ",
    "start": "1704460",
    "end": "1711478"
  },
  {
    "text": "Yeah, so we can\nactually-- the models we're comparing against\nare trained on kind of high resolution images. So I think that makes\nit even more impressive.",
    "start": "1711478",
    "end": "1717785"
  },
  {
    "text": "Yeah, we're just trading out\nthe 32 by 32 res image, yeah. ",
    "start": "1717785",
    "end": "1723350"
  },
  {
    "text": "Cool. So if we fine tune these models\nfor CIFAR classification, we can get 99% accuracy,\nwhich matches T5.",
    "start": "1723350",
    "end": "1731820"
  },
  {
    "text": "T5, for instance,\nis a system which is pretrained on ImageNet with\nlabels and then also fine tuned",
    "start": "1731820",
    "end": "1736950"
  },
  {
    "text": "with labels. So yeah, it just\nkind of shows you, even this approach which doesn't\nreally know about convolutions",
    "start": "1736950",
    "end": "1743790"
  },
  {
    "text": "can do well. I think you're going to hear\nmore about that next week with Lucas' talk.",
    "start": "1743790",
    "end": "1750120"
  },
  {
    "text": "So by now, it shouldn't\nbe surprising at all that you can model a lot\nof different modalities",
    "start": "1750120",
    "end": "1755549"
  },
  {
    "text": "with transformers. So in DALL-E, we just\nask, what about throwing",
    "start": "1755550",
    "end": "1760710"
  },
  {
    "text": "two different\nmodalities at the model and seeing if it can learn\nhow to condition on text",
    "start": "1760710",
    "end": "1765990"
  },
  {
    "text": "to produce an image. And for instance, one thing\nyou might want it to do is like you provide one\nof these text captions,",
    "start": "1765990",
    "end": "1772980"
  },
  {
    "text": "and you want it to generate\nsome image like the one below. And the easy way\nto do this is just train a transformer\non the concatenation",
    "start": "1772980",
    "end": "1779850"
  },
  {
    "text": "of a caption and an image. And of course, in a lot\nof these situations, the idea is very simple, but\nthe implementation and execution",
    "start": "1779850",
    "end": "1787590"
  },
  {
    "text": "is where the difficulty is. And I'm not going to\ntalk too much about that. I think the focus\ntoday is on language.",
    "start": "1787590",
    "end": "1793035"
  },
  {
    "text": "But you can refer to the paper\nfor a lot of those details. Could you describe if you have\na caption that's-- a variable caption?",
    "start": "1793035",
    "end": "1798408"
  },
  {
    "text": " Okay, yeah, so you have\na max caption length,",
    "start": "1798408",
    "end": "1806640"
  },
  {
    "text": "and you just kind of cut\nit off at that length. And you can pad up to that one. ",
    "start": "1806640",
    "end": "1814190"
  },
  {
    "text": "So you can see that it can\ngenerate fairly good samples. So if you want like a storefront\nwith the word OpenAI on it,",
    "start": "1814190",
    "end": "1820100"
  },
  {
    "text": "it's not perfect,\nbut at least it's kind of like reverse\nOCR problem, where you",
    "start": "1820100",
    "end": "1825740"
  },
  {
    "text": "take some text and render it. And it's kind of\ntypically rendering it in like office-looking places.",
    "start": "1825740",
    "end": "1831600"
  },
  {
    "text": "So that's one encouraging sign. But I do think my favorite\nresults here are Zero-Shot",
    "start": "1831600",
    "end": "1838130"
  },
  {
    "text": "in machine transformation. So what's going on\nhere, is, for instance, if your prompt is\n\"the exact same cat",
    "start": "1838130",
    "end": "1843590"
  },
  {
    "text": "on the top as a\nsketch on the bottom,\" and you feed in the top half\nof this image, which is a cat,",
    "start": "1843590",
    "end": "1850728"
  },
  {
    "text": "and you ask it to complete\nthe rest of the image, then it'll render the top cat\nactually as like a sketch.",
    "start": "1850728",
    "end": "1857540"
  },
  {
    "text": "And you can do the same\nthing with flipping over photos for instance. You can zoom in to a photo.",
    "start": "1857540",
    "end": "1863120"
  },
  {
    "text": "Of course, they're\nnot perfect, but it has some understanding of what\nthe text is trying to do, yeah.",
    "start": "1863120",
    "end": "1868460"
  },
  {
    "text": "In the caption originally\nlike in the training set, do they have wording such\nas extreme closeup view?",
    "start": "1868460",
    "end": "1877430"
  },
  {
    "text": "I think that-- there probably\nare some examples like that, and that's probably\nwhere it's picking up",
    "start": "1877430",
    "end": "1882860"
  },
  {
    "text": "some of this knowledge\nfrom, though we don't seek out these examples. It's just-- [INAUDIBLE] Yeah exactly.",
    "start": "1882860",
    "end": "1888800"
  },
  {
    "text": "[INAUDIBLE] OK, perfect. This is just-- we just go\nand do a massive web scrape.",
    "start": "1888800",
    "end": "1897560"
  },
  {
    "text": "We're not trying to find\nexamples like this, right? And so you can also do things\nlike colorization, right?",
    "start": "1897560",
    "end": "1902947"
  },
  {
    "text": "You can take the cat\nand color it red. And this has to kind\nof recognize what the object is in the figure.",
    "start": "1902947",
    "end": "1909980"
  },
  {
    "text": "And yeah, here, you can do stuff\nlike semantic transformations, like adding sunglasses\ninto the cat.",
    "start": "1909980",
    "end": "1917510"
  },
  {
    "text": "And you can put it on\npostage for instance. So it's just\nremarkable that you can do a lot of these like\ntransform Zero-Shot.",
    "start": "1917510",
    "end": "1924170"
  },
  {
    "text": "It wasn't trained to do\nthese things specifically. ",
    "start": "1924170",
    "end": "1931912"
  },
  {
    "text": "Cool, so moving on, the last\nsection of my talk today is on Codex, which is our most\nrecently released code writing",
    "start": "1931912",
    "end": "1937480"
  },
  {
    "text": "models. And the first question you\nshould rightly ask here is, why train them\nall on code anyway?",
    "start": "1937480",
    "end": "1946210"
  },
  {
    "start": "1940000",
    "end": "2013000"
  },
  {
    "text": "At this point, isn't it\njust another modality? And what is the novelty that\nthere is at this point, right?",
    "start": "1946210",
    "end": "1953409"
  },
  {
    "text": "So let me give you\na couple of reasons. So first is that GPT-3\nhad a rudimentary ability",
    "start": "1953410",
    "end": "1959460"
  },
  {
    "text": "to write Python code\nalready from a docstring or a descriptive method name. And we actually didn't\ntrain it on much code data.",
    "start": "1959460",
    "end": "1966757"
  },
  {
    "text": "Actually, I think there might\nhave been active filtering to get rid of code data. And so we were\nsurprised that there is this capability anyway.",
    "start": "1966757",
    "end": "1972870"
  },
  {
    "text": "So we thought if we\nactually purposed a model and trained it on the\nlarge amount of code that we can find, maybe\nsomething interesting",
    "start": "1972870",
    "end": "1979680"
  },
  {
    "text": "will happen there. Next, what sets apart\ncode from other modalities",
    "start": "1979680",
    "end": "1984840"
  },
  {
    "text": "is that there is a\nkind of ground truth correctness of a sample. And functions can be tested with\nunit tests and an interpreter.",
    "start": "1984840",
    "end": "1993133"
  },
  {
    "text": "So this is very\ndifferent from language, where to get a\nground truth eval, you might need a\nhuman to come in.",
    "start": "1993133",
    "end": "1998190"
  },
  {
    "text": "And even then, sometimes\nhumans won't agree. Like, this is the\nbetter example or this isn't the better sample.",
    "start": "1998190",
    "end": "2004540"
  },
  {
    "text": "Last thing is I used to dabble\nin competitive programming myself, and I really wanted to\ncreate a model that could solve",
    "start": "2004540",
    "end": "2009938"
  },
  {
    "text": "problems that I couldn't. ",
    "start": "2009938",
    "end": "2015746"
  },
  {
    "start": "2013000",
    "end": "2160000"
  },
  {
    "text": "Go ahead. [INAUDIBLE] Is this the same thing\n[INAUDIBLE] get up on this? [INAUDIBLE]",
    "start": "2015746",
    "end": "2021147"
  },
  {
    "text": "Yeah, exactly. [INAUDIBLE] Yeah, we wrote a paper\non it, too, so, yeah.",
    "start": "2021147",
    "end": "2027519"
  },
  {
    "text": "So I recognize that you use\nkind of a high level programming language where it's\nbasically similar to like",
    "start": "2027520",
    "end": "2033220"
  },
  {
    "text": "our human language. Have you guys ever tried\nto predict some even lower",
    "start": "2033220",
    "end": "2038350"
  },
  {
    "text": "level operations like CPP, or-- Yeah, I think there's\nfollow-up work where we just",
    "start": "2038350",
    "end": "2046532"
  },
  {
    "text": "train on a bunch of\ndifferent languages. And I don't know the metrics\noff the top of my head, but I have seen some assembly\nwriting models, cool.",
    "start": "2046532",
    "end": "2054659"
  },
  {
    "text": "So I guess, yeah, continue\non the [INAUDIBLE]..",
    "start": "2054660",
    "end": "2061819"
  },
  {
    "text": "So we have this setting where we\nhave unit test and interpreter. So how do we actually\nevaluate these models",
    "start": "2061820",
    "end": "2068712"
  },
  {
    "text": "in a way that's kind of\naware of these two concepts? So the first thing we did was\nwe have a data set, a new data",
    "start": "2068713",
    "end": "2074349"
  },
  {
    "text": "set, which is 164 handwritten\nprogramming problems. And these kind of have\nthe format shown here.",
    "start": "2074350",
    "end": "2081310"
  },
  {
    "text": "There's a function name, a\ndocstring, there's a solution, and there's an average of around\neight unit tests per problem.",
    "start": "2081310",
    "end": "2088449"
  },
  {
    "text": "And why is it important\nthat we hand wrote these? Well, the thing\nis we're training on such a large part of GitHub.",
    "start": "2088449",
    "end": "2094281"
  },
  {
    "text": "If you said, OK, I'm going\nto take like some LeetCode problems, and I'm going to\nturn them into an evaluation. That's not going to\nwork, because there's",
    "start": "2094281",
    "end": "2100531"
  },
  {
    "text": "just so many GitHub repos\nthat are like, oh, here's the solution to this\nLeetCode problem. So while this doesn't\nkind of guarantee",
    "start": "2100531",
    "end": "2106750"
  },
  {
    "text": "that this problem\nisn't duplicated, at least someone wrote\nit without copying it from another source.",
    "start": "2106750",
    "end": "2111880"
  },
  {
    "text": " So here's some kind of\nexamples of a unit test",
    "start": "2111880",
    "end": "2117730"
  },
  {
    "text": "that you would evaluate\nthe previous function on. I think it should be\nfairly clear that we",
    "start": "2117730",
    "end": "2124150"
  },
  {
    "text": "should be using this metric. This is the correct kind of\nground truth metric to use. I mean, humans do use unit\ntests to evaluate code.",
    "start": "2124150",
    "end": "2131309"
  },
  {
    "text": "And I would say if you're\nfamiliar with competitive programming, you\ncan't manually judge all like tens of thousands of\nsubmissions that are coming in.",
    "start": "2131310",
    "end": "2138369"
  },
  {
    "text": "You need the unit tests. And that is a fairly\ngood placement. So one interesting\npoint here was",
    "start": "2138370",
    "end": "2144510"
  },
  {
    "text": "we had to create a\nsandbox environment to run these kind of\ngenerated solutions in.",
    "start": "2144510",
    "end": "2149525"
  },
  {
    "text": "Because when you\ntrain on GitHub, there's a bunch\nof malicious code, there's a bunch of\nkind of insecure code. You don't want your\nmodel to be sampling",
    "start": "2149525",
    "end": "2156137"
  },
  {
    "text": "that and kind of running\nthat on your environment. Cool. So now that we\nhave an evaluation",
    "start": "2156137",
    "end": "2162450"
  },
  {
    "start": "2160000",
    "end": "2219000"
  },
  {
    "text": "data set, let's define\na metric on them. And so the metric we're going\nto use is called pass @ K.",
    "start": "2162450",
    "end": "2168329"
  },
  {
    "text": "And the definition is\nthe average probability over all the problems that\nat least 1 out of K samples",
    "start": "2168330",
    "end": "2174329"
  },
  {
    "text": "passes the unit tests. So if we evaluate this metric\nby just taking every problem",
    "start": "2174330",
    "end": "2180870"
  },
  {
    "text": "and exactly generating k\nsamples, it's actually not-- there's high variance just\nkind of sampling it that way.",
    "start": "2180870",
    "end": "2188280"
  },
  {
    "text": "Imagine the pass rate of a\nparticular sample is around 1 over k. This is kind of like an\nall or nothing metric.",
    "start": "2188280",
    "end": "2194290"
  },
  {
    "text": "So what we do instead is we\ngenerate a much larger set of samples, n greater than k--",
    "start": "2194290",
    "end": "2200910"
  },
  {
    "text": "most of the time,\nit's greater than 5k. And we count the number\nthat are correct,",
    "start": "2200910",
    "end": "2205980"
  },
  {
    "text": "and we compute this\nunbiased estimator. And it looks more complicated\nthan it actually is. It's just\ncomplementary counting.",
    "start": "2205980",
    "end": "2211560"
  },
  {
    "text": "You take the number of\ncombos where all of them fail and subtract that out.",
    "start": "2211560",
    "end": "2219830"
  },
  {
    "start": "2219000",
    "end": "2283000"
  },
  {
    "text": "Cool. So then we train our model. And like I alluded\nto earlier, there's",
    "start": "2219830",
    "end": "2226900"
  },
  {
    "text": "about 160 gigabytes of\ncode which is collected from 54 million repositories.",
    "start": "2226900",
    "end": "2232682"
  },
  {
    "text": "For efficient\ntraining, what we did was we fine tuned from GPT-3\nmodels of various sizes.",
    "start": "2232682",
    "end": "2237710"
  },
  {
    "text": "And this isn't actually\nstrictly necessary. We find that we can get to\nroughly the same final loss",
    "start": "2237710",
    "end": "2243770"
  },
  {
    "text": "in performance without\nthis, but it is slower to do it without this\npretraining step.",
    "start": "2243770",
    "end": "2248990"
  },
  {
    "text": "And so we already\nhave these models; why not just fine tune them? And one extra trick to make\ntraining much faster here is--",
    "start": "2248990",
    "end": "2256280"
  },
  {
    "text": "in code, there's a lot\nof runs of spaces, right, and those don't get compressed\nefficiently in language",
    "start": "2256280",
    "end": "2261440"
  },
  {
    "text": "because you just don't\nsee them very often. So they typically get broken up\ninto like many separate tokens.",
    "start": "2261440",
    "end": "2268550"
  },
  {
    "text": "So we introduce additionally\nsome tokens that compress runs of white space. And that makes training maybe\nlike 30% or 40% more efficient.",
    "start": "2268550",
    "end": "2277730"
  },
  {
    "text": "So the token [INAUDIBLE]? Yeah, exactly, yeah. ",
    "start": "2277730",
    "end": "2284580"
  },
  {
    "start": "2283000",
    "end": "2316000"
  },
  {
    "text": "Great, so once we\nhave these models, we can go and revisit\nthe HumanEval data set. And I can share a\ncouple of problems",
    "start": "2284580",
    "end": "2291030"
  },
  {
    "text": "to give you a sense of where\nthe models are at and also what kind of difficulty level\nthe problems in the data set",
    "start": "2291030",
    "end": "2297390"
  },
  {
    "text": "are at. So this is a 12 billion\nparameter model. The pass rate is 90%, which\nmeans that 90% of the samples",
    "start": "2297390",
    "end": "2304260"
  },
  {
    "text": "will pass the unit test. This is something\nlike anyone kind of doing a first day of\nPython would be able to do.",
    "start": "2304260",
    "end": "2310799"
  },
  {
    "text": "So you increment all the\nelements of a list by 1. Here's a problem where\nthe pass rate is 17%.",
    "start": "2310800",
    "end": "2318992"
  },
  {
    "start": "2316000",
    "end": "2340000"
  },
  {
    "text": "So this is the solution I\ngave-- that's the problem I gave earlier. So you are given a\nnon-empty list of integers.",
    "start": "2318992",
    "end": "2324775"
  },
  {
    "text": "You want to return the sum\nof all odd elements that are in even positions. And this might not sound\nthat much harder to you,",
    "start": "2324775",
    "end": "2330082"
  },
  {
    "text": "but models can often\nget confused about, oh, is odd referring to\npositions or elements?",
    "start": "2330082",
    "end": "2335319"
  },
  {
    "text": "And so here, you can\nactually see that it's doing the right thing. ",
    "start": "2335320",
    "end": "2340980"
  },
  {
    "start": "2340000",
    "end": "2486000"
  },
  {
    "text": "And finally, this\nis an example of one of the harder problems\nin the data set. So the password\nis under 1% here.",
    "start": "2340980",
    "end": "2347360"
  },
  {
    "text": "And what's going\non here is actually there's an encode function\nwhich takes a string. It kind of chunks it up into\ngroups of three characters.",
    "start": "2347360",
    "end": "2354380"
  },
  {
    "text": "And it does a cyclic\nshift on each character. And you have to write\na decoder, something that reverses this operation.",
    "start": "2354380",
    "end": "2361049"
  },
  {
    "text": "So you can see that the\nmodel-- this is a real model solution, so it trumps up the\ncharacters in the same way.",
    "start": "2361050",
    "end": "2367920"
  },
  {
    "text": "You can see that the cyclic\nshift is the opposite way. So up there, it takes the\nfirst element of each group,",
    "start": "2367920",
    "end": "2373910"
  },
  {
    "text": "moves it to the\nend, and now takes the last element of each\ngroup, moves it to the front. Yeah?",
    "start": "2373910",
    "end": "2379220"
  },
  {
    "text": "OK, I'm wondering\nwhat's the effect of-- so you had a couple\nof examples [INAUDIBLE]",
    "start": "2379220",
    "end": "2385880"
  },
  {
    "text": "in the comments. So I'm wondering\nif the model will be able to extrapolate\nwhat it's doing",
    "start": "2385880",
    "end": "2392450"
  },
  {
    "text": "by the examples [INAUDIBLE]\nunderlying [INAUDIBLE].. Right, yeah. So some of our tasks, there are\nsome examples in the docstring.",
    "start": "2392450",
    "end": "2399321"
  },
  {
    "text": "And some of them don't. I think it's just\nto kind of match the distribution of\na real kind of task",
    "start": "2399322",
    "end": "2404870"
  },
  {
    "text": "we find in the real world. In this case, it\ndoesn't have it. But definitely for the\nunit tests, none of those",
    "start": "2404870",
    "end": "2410620"
  },
  {
    "text": "appear within-- I'm just curious-- if you\njust give it the examples and not give a description\nof the task [INAUDIBLE]..",
    "start": "2410620",
    "end": "2418130"
  },
  {
    "text": "Oh, I see, I see. So can it do like pure\ninduction, where you don't tell the task at all, yeah.",
    "start": "2418130",
    "end": "2423920"
  },
  {
    "text": "I haven't tried\nit, to be honest. I think it's worth a shot. Yeah. Thanks.",
    "start": "2423920",
    "end": "2428960"
  },
  {
    "text": " At this point, we've\ntrained Codex models,.",
    "start": "2428960",
    "end": "2435349"
  },
  {
    "text": "We've evaluated on this metric. But the thing is, was it\nworth all this trouble, right?",
    "start": "2435350",
    "end": "2440550"
  },
  {
    "text": "You already had these\nmetrics like BLEU that are match-based\nin language. Couldn't we have just\nused this to [INAUDIBLE]??",
    "start": "2440550",
    "end": "2447200"
  },
  {
    "text": "We don't need an interpreter. We don't need to\ngenerate so many samples. And it would be\ngreat if it kind of",
    "start": "2447200",
    "end": "2452990"
  },
  {
    "text": "like separated out like this. But what we find\nis that this is--",
    "start": "2452990",
    "end": "2458660"
  },
  {
    "text": "if you take four random\nproblems from HumanEval and you plot the\ndistribution of BLEU scores for correct and wrong\nsolutions, you actually",
    "start": "2458660",
    "end": "2466160"
  },
  {
    "text": "find a lot of distribution\noverrule, right? It's hard to\ndistinguish the green",
    "start": "2466160",
    "end": "2472430"
  },
  {
    "text": "from the blue distributions. And so this suggests\nthat BLEU actually isn't a very good metric for\ngauging functional correctness",
    "start": "2472430",
    "end": "2478910"
  },
  {
    "text": "and that we actually do\nneed this new kind of metric and this new data set. ",
    "start": "2478910",
    "end": "2487090"
  },
  {
    "start": "2486000",
    "end": "2539000"
  },
  {
    "text": "So now, let's explore the\nsetting where in pass @ k, k is greater than 1.",
    "start": "2487090",
    "end": "2493232"
  },
  {
    "text": "And so the first\nobservation we have here is that the temperature\nthat you sample at, it affects your pass @ k.",
    "start": "2493232",
    "end": "2500550"
  },
  {
    "text": "And just for some intuition,\nif you do temperature zero sampling, you're going\nto get the same sample",
    "start": "2500550",
    "end": "2505650"
  },
  {
    "text": "every single time you're\ndoing artifact sampling. So it doesn't matter how\nmany samples you generate.",
    "start": "2505650",
    "end": "2510750"
  },
  {
    "text": "You're just going to\nget the same pass rate. And if you want to\ngenerate 100 samples, right, you can afford\nto make some mistakes.",
    "start": "2510750",
    "end": "2518340"
  },
  {
    "text": "You just want a very\ndiverse set of samples. So you can up the temperature. And yo can see kind of as you\nup the temperature, the slope",
    "start": "2518340",
    "end": "2525090"
  },
  {
    "text": "of the kind of number of\nsamples against pass rate, it becomes steep. And so you can kind of take\nthe upper whole of this",
    "start": "2525090",
    "end": "2532020"
  },
  {
    "text": "and you can find the\noptimal temperature for each number of samples. ",
    "start": "2532020",
    "end": "2539230"
  },
  {
    "start": "2539000",
    "end": "2597000"
  },
  {
    "text": "And so this brings me to\npersonally my favorite result of the paper, which I\ncall the unreasonable effectiveness of sampling.",
    "start": "2539230",
    "end": "2546130"
  },
  {
    "text": "And so let me explain\nwhat's going on here. This is the number of\nparameters in the model. And here, you have pass rate\n@ 1 and pass rate @ 100.",
    "start": "2546130",
    "end": "2554010"
  },
  {
    "text": "And the reason I use this term\nunreasonable effectiveness is that I think\nthere's a world where,",
    "start": "2554010",
    "end": "2559170"
  },
  {
    "text": "if the orange line and the\nline weren't that far apart, I might not be that surprised.",
    "start": "2559170",
    "end": "2564730"
  },
  {
    "text": "At these scales, the model, it\nrarely makes syntactical errors anymore. If you run it, it'll run and\nproduce some kind of output.",
    "start": "2564730",
    "end": "2572380"
  },
  {
    "text": "So you could imagine a world\nwhere basically the model has some approach in mind. It's just repeatedly\nsampling that approach.",
    "start": "2572380",
    "end": "2579123"
  },
  {
    "text": "And it's just either\nright or wrong. But instead what we find is\nthat the model is actually composing different\nparts and producing",
    "start": "2579123",
    "end": "2586140"
  },
  {
    "text": "functionally different things. And you get this huge boost\nfrom under 30% to over 70%",
    "start": "2586140",
    "end": "2592560"
  },
  {
    "text": "just by sampling a lot of\nsamples from the model. ",
    "start": "2592560",
    "end": "2598750"
  },
  {
    "start": "2597000",
    "end": "2752000"
  },
  {
    "text": "So unfortunately, knowing that\none of your samples is correct isn't that useful if you don't\nhave access to the unit tests.",
    "start": "2598750",
    "end": "2607350"
  },
  {
    "text": "And now one practical\nsetting where you would care about\nthis is say you're creating an autocomplete\ntool, right,",
    "start": "2607350",
    "end": "2613619"
  },
  {
    "text": "and you generate 100 samples. But you don't want to\nshow your user 100 samples and have them pick one, right?",
    "start": "2613620",
    "end": "2619590"
  },
  {
    "text": "You want to kind of\ntry to prefilter, but you don't have unit tests. So can we kind of approximate\nthis oracle sampling",
    "start": "2619590",
    "end": "2627480"
  },
  {
    "text": "with some other\nranking heuristic? So here, I'm showing a couple\nof different heuristics,",
    "start": "2627480",
    "end": "2633720"
  },
  {
    "text": "like if you randomly pick one. But the one that\nseems most promising is to rank by meaning,\nnot probability.",
    "start": "2633720",
    "end": "2640410"
  },
  {
    "text": "And it's maybe not\ntheoretically well-grounded,",
    "start": "2640410",
    "end": "2645630"
  },
  {
    "text": "but in language, this\nkind of heuristic is fairly strong as well. ",
    "start": "2645630",
    "end": "2653490"
  },
  {
    "text": "So recall that\nwhat we're doing is we have this evaluation\nset where we have kind of standalone functions.",
    "start": "2653490",
    "end": "2659500"
  },
  {
    "text": "We want to produce\nsolutions to them. But when we're doing\ntraining, there's a lot of code that isn't\nrelevant for this task.",
    "start": "2659500",
    "end": "2666865"
  },
  {
    "text": "For instance, there's a lot\nof classes that we're seeing. There's actually\ndata classes, too, which aren't relevant often.",
    "start": "2666865",
    "end": "2672397"
  },
  {
    "text": "Actually, there's a lot of\nincorrect code on GitHub too. So we might be modeling\nincorrect solutions as well as",
    "start": "2672397",
    "end": "2678240"
  },
  {
    "text": "correct ones. So one thing we thought\nwas, let's fine-tune Codex",
    "start": "2678240",
    "end": "2684000"
  },
  {
    "text": "further on a couple\nof data sets where they are standalone\nfunctions and you have kind of more guaranteed\ncorrect solutions to that.",
    "start": "2684000",
    "end": "2692559"
  },
  {
    "text": "So what we did was we\nfound these problems from a couple of sources. So one is competitive\nprogramming problems.",
    "start": "2692560",
    "end": "2698790"
  },
  {
    "text": "You can go on these sites. Oftentimes, they'll just\ngive you the unit tests. Sometimes, when they don't\ngive you the unit tests,",
    "start": "2698790",
    "end": "2704369"
  },
  {
    "text": "you can submit\nincorrect solutions and they'll tell you the\nfirst one you failed on. And you can kind of\nkeep just doing that. [LAUGHTER]",
    "start": "2704370",
    "end": "2710752"
  },
  {
    "text": "So you can get a lot of\ncompetitive programming problems. And another source is projects\nwhere continuous integration",
    "start": "2710752",
    "end": "2718830"
  },
  {
    "text": "is enabled. So why are these useful? Because you can actually kind\nof do an execution tracing.",
    "start": "2718830",
    "end": "2725130"
  },
  {
    "text": "So when you run the\nintegration tests, you can get all the\ninputs to functions that are called and\ntheir outputs as well.",
    "start": "2725130",
    "end": "2731260"
  },
  {
    "text": "And so you actually have\nthe true function body. You know what the test\noutput is supposed to be, so you know kind of the ground\ntruth inputs and outputs.",
    "start": "2731260",
    "end": "2738240"
  },
  {
    "text": "And these are kind of like\ntwo orthogonal data sets. One helps you with\nalgorithmic kind of tasks.",
    "start": "2738240",
    "end": "2744072"
  },
  {
    "text": "And one is more\nkind of like trying to manipulate command line\nutilities and [INAUDIBLE] that.",
    "start": "2744072",
    "end": "2749160"
  },
  {
    "text": " So this brings us to the main\nfigure of the Codex paper.",
    "start": "2749160",
    "end": "2755300"
  },
  {
    "start": "2752000",
    "end": "2813000"
  },
  {
    "text": "So really what we're seeing is\na progression of capabilities. So with GPT-3 on this HumanEval\ndata set, the pass rate @ 1",
    "start": "2755300",
    "end": "2762819"
  },
  {
    "text": "is 0 basically. You can generate\none or two lines coherently but never really\na whole program coherently.",
    "start": "2762820",
    "end": "2770950"
  },
  {
    "text": "Now, when you fine\ntune on code, which is Codex, this orange\nline, you start to see some non-negligible\nperformance on this data set.",
    "start": "2770950",
    "end": "2778839"
  },
  {
    "text": "When you do this additional\nsupervised fine-tuning-- that's this green line-- you get even better pass rates.",
    "start": "2778840",
    "end": "2784990"
  },
  {
    "text": "And then if you kind of generate\n100 samples from this model, rerank with mean logp,\neven better pass rates.",
    "start": "2784990",
    "end": "2792590"
  },
  {
    "text": "And finally, of course,\nwe have tests in Oracle. It gives you the\nbest pass rates. So one question here\nis, can you actually",
    "start": "2792590",
    "end": "2799180"
  },
  {
    "text": "use a reranking tool,\nlike put it in the model? Can you use it as\na backprop signal? Yeah, yeah, so we\nwe can explore that.",
    "start": "2799180",
    "end": "2806410"
  },
  {
    "text": "I don't know if I can say\ntoo much about those results. Yeah, got it, got it. But yeah.",
    "start": "2806410",
    "end": "2813310"
  },
  {
    "start": "2813000",
    "end": "2858000"
  },
  {
    "text": "And finally, I don't\nwant to suggest that these models are perfect. They have a lot of limitations\nthat human programmers",
    "start": "2813310",
    "end": "2819040"
  },
  {
    "text": "don't run into. So one is like-- actually all\ngenerative models are--",
    "start": "2819040",
    "end": "2824055"
  },
  {
    "text": "autoregressive\ngenerative models, we have some problems\nwith binding. So when there's a lot\nof variables going on,",
    "start": "2824055",
    "end": "2829059"
  },
  {
    "text": "like a lot of\noperations going on, sometimes it's hard to\nfigure out which operation is binding to which variable.",
    "start": "2829060",
    "end": "2834980"
  },
  {
    "text": "So you can kind of see some\nexamples of that on the left. And one other kind of\ncounterintuitive behavior is composition.",
    "start": "2834980",
    "end": "2840820"
  },
  {
    "text": "So we can take a bunch of\nvery simple building blocks, like take a string\nand reverse it, or delete every third\ncharacter or something.",
    "start": "2840820",
    "end": "2848000"
  },
  {
    "text": "And a human, if you can train\ntwo of these operations, you could probably\ntrain 10 of them. But our models aren't\nable to do that yet.",
    "start": "2848000",
    "end": "2855030"
  },
  {
    "text": " Cool. So moving on to the\nconclusion, we've",
    "start": "2855030",
    "end": "2862460"
  },
  {
    "start": "2858000",
    "end": "2899000"
  },
  {
    "text": "had four main points\nin today's talk. So first, progress in\nneural language modeling has been fairly rapid.",
    "start": "2862460",
    "end": "2868520"
  },
  {
    "text": "And GPT wasn't the result of a\npush on language modeling, more of a result of work on\npushing unsupervised learning",
    "start": "2868520",
    "end": "2875570"
  },
  {
    "text": "in language. The third point is that\nautoregressive modeling is universal.",
    "start": "2875570",
    "end": "2880730"
  },
  {
    "text": "And it can yield strong\nresults, even when there are strong inductive biases,\nlike in images or in text",
    "start": "2880730",
    "end": "2886280"
  },
  {
    "text": "to image. And finally, we can produce\nstrong code generating models by fine-tuning GPT-3 on code.",
    "start": "2886280",
    "end": "2893000"
  },
  {
    "text": "And sampling is an\nunreasonably effective way to improve model performance.",
    "start": "2893000",
    "end": "2898658"
  },
  {
    "text": "Cool, and to end with\nsome acknowledgments, I want to thank my Codex\nprimary co-authors, some mentors",
    "start": "2898658",
    "end": "2904290"
  },
  {
    "start": "2899000",
    "end": "2919000"
  },
  {
    "text": "at OpenAI, and the\nalgorithms team, which I've worked very closely with.",
    "start": "2904290",
    "end": "2909329"
  },
  {
    "text": "Great. Thank you guys for\nyour attention. ",
    "start": "2909330",
    "end": "2918000"
  }
]