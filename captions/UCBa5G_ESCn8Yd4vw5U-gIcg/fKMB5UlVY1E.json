[
  {
    "text": " So welcome everyone to CS 25.",
    "start": "0",
    "end": "7060"
  },
  {
    "text": "We're excited to\nkick off this class. This is the fourth iteration\nof the class we're doing.",
    "start": "7060",
    "end": "12190"
  },
  {
    "text": "In the previous iterations,\nwe had like Andrej Karpathy come last year, we also\nhad Geoffrey Hinton, a bunch of other people.",
    "start": "12190",
    "end": "18670"
  },
  {
    "text": "And so we're very\nexcited to kick this off. The purpose of this\nclass is to discuss",
    "start": "18670",
    "end": "24789"
  },
  {
    "text": "the latest in the field\nof AI, and transformers, and large language models, and\nhave all the top researchers",
    "start": "24790",
    "end": "30310"
  },
  {
    "text": "and experts in the field come\nand be able to directly give a talk and discuss\ntheir findings",
    "start": "30310",
    "end": "35649"
  },
  {
    "text": "and their new ideas\nto students here so that this can be used\nto in their own research",
    "start": "35650",
    "end": "43420"
  },
  {
    "text": "or spark new collaborations. So we're very excited\nabout the class. And yeah, let's kick it off.",
    "start": "43420",
    "end": "53370"
  },
  {
    "text": "So hi, everyone. I'm Div. ",
    "start": "53370",
    "end": "58489"
  },
  {
    "text": "So I'm currently on a leave from\nthe PhD program from Stanford. I'm working on a personal AI\nagent startup called MultiOn.",
    "start": "58490",
    "end": "66020"
  },
  {
    "text": "You can see the shirt. I'm very passionate\nabout robotics, agents, did a lot of work\non reinforcement",
    "start": "66020",
    "end": "72470"
  },
  {
    "text": "learning, a bunch of\nstate of art methods on online and offline Rl. Previously was working with\nIan Goodfellow at Apple.",
    "start": "72470",
    "end": "79430"
  },
  {
    "text": "So that was really fun. And just really\npassionate about AI, and how can you apply\nthat in the real world.",
    "start": "79430",
    "end": "85450"
  },
  {
    "text": "It's a guy. Hey, guys. I'm Steven, currently a\nsecond year a PhD student here at Stanford.",
    "start": "85450",
    "end": "90642"
  },
  {
    "text": "So I'll be interning at\nNVIDIA over the summer. And previously, I was a master's\nstudent at Carnegie Mellon, and an undergrad\nat the University",
    "start": "90642",
    "end": "97000"
  },
  {
    "text": "of Waterloo in Canada. So my research interests\nbroadly hover around NLP and working with\nlanguage and text.",
    "start": "97000",
    "end": "103630"
  },
  {
    "text": "You know, can we work on\nimproving the controllability and reasoning capabilities\nof language models.",
    "start": "103630",
    "end": "110240"
  },
  {
    "text": "And recently, I've gotten more\ninto multimodal work, as well as interdisciplinary work with\npsychology and cognitive science.",
    "start": "110240",
    "end": "115820"
  },
  {
    "text": "I'm trying to bridge\nthe gap between how humans, as well as language\nmodels learn and reason.",
    "start": "115820",
    "end": "122320"
  },
  {
    "text": "And just some for fun. You know, I'm also the\nco-founder and co-president of the Stanford Piano Club. So if anybody here is\ninterested, check us out.",
    "start": "122320",
    "end": "130130"
  },
  {
    "text": " Hi, everyone. I'm Emily.",
    "start": "130130",
    "end": "135510"
  },
  {
    "text": "I am currently an undergrad,\nabout to finish it up in math and cogsci\nhere at Stanford,",
    "start": "135510",
    "end": "140580"
  },
  {
    "text": "and also doing my master's\nin computer science. I am super interested\nsimilarly in the intersection",
    "start": "140580",
    "end": "146640"
  },
  {
    "text": "of artificial intelligence\nand natural intelligence. I think questions around\nneuroscience, philosophy, and psychology are\nreally interesting.",
    "start": "146640",
    "end": "153190"
  },
  {
    "text": "And I've been very lucky to do\nsome really cool research here at Stanford Med at NYU\nand also at CoCoLab",
    "start": "153190",
    "end": "159870"
  },
  {
    "text": "where Steven is working\nas well under Noah Goodman on some computational\nneuroscience and computational",
    "start": "159870",
    "end": "166080"
  },
  {
    "text": "cognitive science work. And currently I am beginning\na new line of research",
    "start": "166080",
    "end": "171930"
  },
  {
    "text": "with Chris Manning\nand Chris Potts doing some NLP\ninterpretability research. ",
    "start": "171930",
    "end": "179550"
  },
  {
    "text": "Hello. I'm a first year CS\nmaster's student. My name is Seonghee.",
    "start": "179550",
    "end": "184770"
  },
  {
    "text": "I do research around\nnatural language processing. I did a lot of research in HCI\nduring undergrad at Cornell.",
    "start": "184770",
    "end": "191190"
  },
  {
    "text": "And I'm currently working\non visual language models in image editors\nfor accessibility,",
    "start": "191190",
    "end": "197250"
  },
  {
    "text": "and I'm also working with\nProfessor Hari Subramonyam in the HCI department,\nand I'm also",
    "start": "197250",
    "end": "202410"
  },
  {
    "text": "working on establishing\nconsistency in long term conversations with Professor\nDiyi Yang at the NLP group.",
    "start": "202410",
    "end": "209610"
  },
  {
    "text": "Yeah. Nice to meet you all. So what we hope you guys\nwill learn from this course",
    "start": "209610",
    "end": "216450"
  },
  {
    "text": "is a broad idea of how\nexactly transformers work. How they're being applied around\nthe world beyond just NLP,",
    "start": "216450",
    "end": "224490"
  },
  {
    "text": "but other domains as\nwell as applications. Some exciting new\ndirections of research.",
    "start": "224490",
    "end": "229920"
  },
  {
    "text": "Innovative techniques\nand applications, especially these days of\nlarge language models, which have taken the world by storm.",
    "start": "229920",
    "end": "235260"
  },
  {
    "text": "And any remaining\nchallenges or weaknesses involving transformers and\nmachine learning in general.",
    "start": "235260",
    "end": "241685"
  },
  {
    "start": "241685",
    "end": "246780"
  },
  {
    "text": "Cool. So we can start with presenting\nfirst the attention timeline.",
    "start": "246780",
    "end": "252780"
  },
  {
    "text": "So I will say initially we used\nto have this prehistoric era, where we had very simple\nmethods, so language,",
    "start": "252780",
    "end": "261660"
  },
  {
    "text": "for example. So like you had a lot\nof rule-based methods, you had parsing, you\nhave RNNs, LSTMs.",
    "start": "261660",
    "end": "267270"
  },
  {
    "text": "And that all\nchanged, I will say, in the beginning of around\n2014 when people started",
    "start": "267270",
    "end": "272760"
  },
  {
    "text": "studying attention mechanisms. And this was around say\ninitially around images, can you adapt the mechanism\nof-- how attention",
    "start": "272760",
    "end": "279600"
  },
  {
    "text": "works in the human brain. Images, can you focus\non different parts, which might be more salient or\nmore relevant to a user query",
    "start": "279600",
    "end": "287669"
  },
  {
    "text": "or what you care about. And I will say\nattention exploded in the beginning of 2017, with\nthe paper Attention Is All You",
    "start": "287670",
    "end": "295380"
  },
  {
    "text": "Need by Ashish Vaswani et al. And so that was when\ntransformers became mainstream.",
    "start": "295380",
    "end": "300825"
  },
  {
    "text": "And then people realized,\nOK, like this could be its own really big thing. It's a new architecture\nthat you can use everywhere.",
    "start": "300825",
    "end": "306889"
  },
  {
    "text": "And after that, we saw an\nexplosion of transformers into NLP, with BERT, GPT-3.",
    "start": "306890",
    "end": "312880"
  },
  {
    "text": "And then into also other fields. So now you're seeing\nthat in like vision, like protein folding\nwith Alphafold.",
    "start": "312880",
    "end": "318223"
  },
  {
    "text": "You have all the video\nmodels like Sora.  Everything right\nnow is basically",
    "start": "318223",
    "end": "324070"
  },
  {
    "text": "some combination of attention,\nand some other architectures, like diffusion, for example.",
    "start": "324070",
    "end": "330010"
  },
  {
    "text": "And this has now led to the\nstart of this generative AI era, where now you have all\nthis powerful models, which",
    "start": "330010",
    "end": "337660"
  },
  {
    "text": "are billion parameters,\ntrillion parameters. And then you can use this for a\nlot of different applications.",
    "start": "337660",
    "end": "342770"
  },
  {
    "text": "And so if you think\nabout even like one year before, AI was very\nlimited to the lab, and now you can see AI\nhas escaped from the lab,",
    "start": "342770",
    "end": "349150"
  },
  {
    "text": "it's now finding real life-- real world applications. And it has started to\nbecome predominant.",
    "start": "349150",
    "end": "357053"
  },
  {
    "text": "If you look at the\ntrajectory right now, it's sort of like we're\non this upward trend, where we started like\nthis, and now it's just growing faster,\nand faster, and faster.",
    "start": "357053",
    "end": "363650"
  },
  {
    "text": "And every month, there's just\nso many new models coming out. It's like every day, there's\njust so many new things happening. And so it's going to be very\nexciting to see even a year",
    "start": "363650",
    "end": "370750"
  },
  {
    "text": "or two years from now\nhow everything changes. And the society will be led\nby this revolutions that are",
    "start": "370750",
    "end": "376690"
  },
  {
    "text": "happening in the field of AI. So a lot of things are\ngoing to change, and maybe how we interact\nwith technology, how",
    "start": "376690",
    "end": "381969"
  },
  {
    "text": "we do things in daily life,\nhow we have assistants. And I think a lot\nof that will just come from the things we might\nbe studying in this class.",
    "start": "381970",
    "end": "388215"
  },
  {
    "text": " Awesome. Thanks, Div, for going\nthrough the timeline.",
    "start": "388215",
    "end": "395240"
  },
  {
    "text": "So generally, the field of\nnatural language processing, which is kind of\nwhat transformers were originally invented for.",
    "start": "395240",
    "end": "402710"
  },
  {
    "text": "The fundamental\ndiscrete nature of text makes many things difficult.\nFor example, data augmentation is more difficult.",
    "start": "402710",
    "end": "408169"
  },
  {
    "text": "You can't just,\nfor example, flip, flip it like you flip an image,\nchange the pixel values a bit. It's not that simple.",
    "start": "408170",
    "end": "414470"
  },
  {
    "text": "Text is very precise. One wrong word, changes the\nentire meaning of a sentence, or makes it completely\nnonsensical.",
    "start": "414470",
    "end": "421370"
  },
  {
    "text": "And there's also potential\nfor long context length as well as memories. Like, if you're\nchatting with ChatGPT",
    "start": "421370",
    "end": "427009"
  },
  {
    "text": "over many different\nconversations, being able to learn and\nstore all of that information is a big challenge.",
    "start": "427010",
    "end": "433280"
  },
  {
    "text": "And some of the weaknesses\nof earlier models, which we'll get to later,\nshort context length, linear reasoning,\nas well as the fact",
    "start": "433280",
    "end": "440612"
  },
  {
    "text": "that many of the\nearlier approaches did not adapt based on context. So actually, I'll be\nrunning through briefly",
    "start": "440612",
    "end": "447560"
  },
  {
    "text": "how NLP has progressed\nthroughout the years. Actually, Seonghee\nwill be doing that.",
    "start": "447560",
    "end": "455009"
  },
  {
    "text": "Yeah. While preparing this, I found\nthis really interesting thing. This is 1966.",
    "start": "455010",
    "end": "461760"
  },
  {
    "text": "This was the earliest\nchatbot called Eliza. And it wasn't like\na real AI, but it",
    "start": "461760",
    "end": "467160"
  },
  {
    "text": "was more of simulating\npatterns of text and words and sort of creating an\nillusion that this chatbot was",
    "start": "467160",
    "end": "474330"
  },
  {
    "text": "sort of understanding\nwhat you were saying. So these were like the\nearliest forms of NLP, and they were mostly\nrule-based approaches",
    "start": "474330",
    "end": "480634"
  },
  {
    "text": "where you're trying\nto understand the patterns in\nsentences, patterns and what words are said.",
    "start": "480635",
    "end": "487350"
  },
  {
    "text": "And these were the earliest\nlinguistic foundations learning about semantic parsing.",
    "start": "487350",
    "end": "493259"
  },
  {
    "text": "Yeah. Then we needed to go\non to understand more deeper meanings within words.",
    "start": "493260",
    "end": "499530"
  },
  {
    "text": "So we come up with things\ncalled Word Embeddings, which are vector\nrepresentations of words,",
    "start": "499530",
    "end": "504810"
  },
  {
    "text": "and they gave us different\nsemantic meanings in words that we weren't\nable to understand before.",
    "start": "504810",
    "end": "510670"
  },
  {
    "text": "So we create vector\nrepresentations, words that are similar\nappear closer together in this vector space,\nand we're able to learn",
    "start": "510670",
    "end": "518169"
  },
  {
    "text": "different types of meanings. And then these examples I have\nhere are word two, vec, love,",
    "start": "518169",
    "end": "523390"
  },
  {
    "text": "bird, Elmo. These are different\ntypes of word embeddings and they sort of evolve. So word two vector is\nlike a local context word",
    "start": "523390",
    "end": "530830"
  },
  {
    "text": "embedding, word glove, we\nget like global context within documents. So now that we have\nways to represent words",
    "start": "530830",
    "end": "537550"
  },
  {
    "text": "into vector representations,\nnow we can turn them into-- put them into our models\nand do different types",
    "start": "537550",
    "end": "544180"
  },
  {
    "text": "of tasks, such as\nquestion/answering, or text summarization, or sentence\ncompletion, machine translation.",
    "start": "544180",
    "end": "550699"
  },
  {
    "text": "And we develop different\ntypes of models that are able to do that. So here we have\nRNNs, LSTMs that are",
    "start": "550700",
    "end": "557920"
  },
  {
    "text": "used for different\ntranslation tasks. And so since we have our\nmodels, the now new challenge",
    "start": "557920",
    "end": "563470"
  },
  {
    "text": "becomes to understand how we\ncan do these tasks better.",
    "start": "563470",
    "end": "568500"
  },
  {
    "text": "So thanks, Seonghee. So she talked about\nsequence to sequence models. Those are naturally just\ninefficient as well as",
    "start": "568500",
    "end": "575420"
  },
  {
    "text": "ineffective for many ways. You cannot parallelize because\nit depends on recurrence. It relies on maintaining\na hidden context",
    "start": "575420",
    "end": "583130"
  },
  {
    "text": "vector of all the previous\nwords and their information. So you couldn't parallelize\nand it was inefficient and not",
    "start": "583130",
    "end": "589399"
  },
  {
    "text": "very effective. So this led to what is now\nknown as attention as well as transformers.",
    "start": "589400",
    "end": "594590"
  },
  {
    "text": "So as the word kind\nof infers, attention means being able\nto focus attention",
    "start": "594590",
    "end": "599720"
  },
  {
    "text": "to different parts of something. In this case, a piece of text. So this is done by using a set\nof parameters called weights,",
    "start": "599720",
    "end": "606260"
  },
  {
    "text": "that basically determine\nhow much attention to should be paid to each input\nat each time step.",
    "start": "606260",
    "end": "611750"
  },
  {
    "text": "And their computer using\na combination of the input as well as the current\nhidden state of the model. So this will become clearer\nas I go through the slides.",
    "start": "611750",
    "end": "619400"
  },
  {
    "text": "But you're here, you\nhave an example-- or this is an example\nof self-attention.",
    "start": "619400",
    "end": "625370"
  },
  {
    "text": "If we're currently\nat the word \"it,\" we want to know how much\nattention do we want to place to all of the other words\nwithin our input sequence.",
    "start": "625370",
    "end": "632990"
  },
  {
    "text": "And again, this will become\nclearer as I explain more. So the attention\nmechanism relies mainly",
    "start": "632990",
    "end": "638540"
  },
  {
    "text": "on these three things called\nqueries, keys, and values. So I tried to come up with\na good analogy for this.",
    "start": "638540",
    "end": "644630"
  },
  {
    "text": "And it's basically\nlike a library system. So let's say your query is\nsomething you're looking for.",
    "start": "644630",
    "end": "649670"
  },
  {
    "text": "For example, a\nspecific topic, like I want books about\nhow to cook a pizza.",
    "start": "649670",
    "end": "655040"
  },
  {
    "text": "And each book in the\nlibrary, let's say, has a key that\nhelps identify it. For example, this\nbook is about cooking.",
    "start": "655040",
    "end": "661860"
  },
  {
    "text": "This book is about transformers. This book is about movie\nstars, and so forth.",
    "start": "661860",
    "end": "667700"
  },
  {
    "text": "And what you do is you can look. You can match between your query\nas well as each of these keys or summaries to figure\nout which books give you",
    "start": "667700",
    "end": "676670"
  },
  {
    "text": "the most information you need. And that information\nis the value, which you're trying to retrieve. But here in attention,\nwe do a soft match.",
    "start": "676670",
    "end": "684420"
  },
  {
    "text": "We're not trying to\nretrieve one book. We want to see what is the\ndistribution of relevance",
    "start": "684420",
    "end": "691320"
  },
  {
    "text": "or importance across all books. For example, this book\nmight be the most relevant. I should spend\nmost of my time on.",
    "start": "691320",
    "end": "696760"
  },
  {
    "text": "This one might be the\nsecond most relevant, I'll spend a mediocre\namount of time on. And then book three is less\nrelevant, and so forth.",
    "start": "696760",
    "end": "704000"
  },
  {
    "text": "So attention is\nbasically a soft match between finding\nwhat's most relevant,",
    "start": "704000",
    "end": "709139"
  },
  {
    "text": "which is contained\nin those values. And hence the equation,\nwhen you multiply queries by keys and then you\nmultiply that by the values",
    "start": "709140",
    "end": "717720"
  },
  {
    "text": "to get your final attention. And here's also\njust a visualization",
    "start": "717720",
    "end": "723060"
  },
  {
    "text": "from the Illustrated transformer\nabout how self-attention works. So you're basically\nable to embed your input",
    "start": "723060",
    "end": "729810"
  },
  {
    "text": "words into vectors. And then for each of these,\nyou initialize a query key",
    "start": "729810",
    "end": "735060"
  },
  {
    "text": "as well as value matrix. And these are learned as\nthe transformers strength. And you're able to multiply your\ninputs by these queries, keys,",
    "start": "735060",
    "end": "744180"
  },
  {
    "text": "and values to get\nthese final query, key, and value matrices, which\nis then used, again,",
    "start": "744180",
    "end": "749437"
  },
  {
    "text": "as shown in the formula to\ncalculate the final attention score. And the way the\ntransformer works is it",
    "start": "749437",
    "end": "755430"
  },
  {
    "text": "basically uses attention,\nbut in a way that's called multi-head attention,\nas in we do attention",
    "start": "755430",
    "end": "760949"
  },
  {
    "text": "several times. Because since each one\nis randomly initialized,",
    "start": "760950",
    "end": "766650"
  },
  {
    "text": "our goal is that each\nhead of attention will learned something\nuseful but different",
    "start": "766650",
    "end": "772580"
  },
  {
    "text": "from the other heads. So this allows you\nto get a more sort of overarching representation of\npotentially relevant information",
    "start": "772580",
    "end": "781160"
  },
  {
    "text": "from your text. And you see these blocks\nare repeated N times. The point there is once\nthe multi-head attention is",
    "start": "781160",
    "end": "788300"
  },
  {
    "text": "calculated, so the\nattention scores are calculated from each head,\nthey're then concatenated. And then this process is\nrepeated several times",
    "start": "788300",
    "end": "795260"
  },
  {
    "text": "to potentially learn things like\nhierarchical features and more in-depth information.",
    "start": "795260",
    "end": "801350"
  },
  {
    "text": "And here you'll see this\ntransformer diagram has both an encoder and decoder. This is for something,\nfor example, T5 or BART,",
    "start": "801350",
    "end": "809270"
  },
  {
    "text": "which is an encoder/decoder\nmodel used for things like machine translation. On the other hand, things\nlike GPT or ChatGPT,",
    "start": "809270",
    "end": "817069"
  },
  {
    "text": "that's simply a\ndecoder-only, because there's no second source of\ninput text, compared",
    "start": "817070",
    "end": "824307"
  },
  {
    "text": "to something like\nmachine translation, where you have a sentence\nin English which you want to translate to French.",
    "start": "824307",
    "end": "829880"
  },
  {
    "text": "When you're decoding for\nan autoregressive left to right language\nmodel like ChatGPT, it basically only has what\nhas been generated so far.",
    "start": "829880",
    "end": "837690"
  },
  {
    "text": "So that's the difference between\ndecoder-only and encoder/decoder transformers. And the way multi-head\nattention works",
    "start": "837690",
    "end": "844230"
  },
  {
    "text": "is you initialize a different\nset of queries, keys, and values, these different\nmatrices per head, which are all",
    "start": "844230",
    "end": "851010"
  },
  {
    "text": "learned separately as you\ntrain and back propagate across tons of data. So again, you embed\neach word, split these",
    "start": "851010",
    "end": "857550"
  },
  {
    "text": "into heads, so\nseparate matrices, and then you kind of\nmultiply those together",
    "start": "857550",
    "end": "863579"
  },
  {
    "text": "to get the final\nresulting attention, which are then concatenated\nand multiplied by a final weight matrix.",
    "start": "863580",
    "end": "869820"
  },
  {
    "text": "And then there's some linear\nlayer and then some softmax to help you predict the\nnext token, for example.",
    "start": "869820",
    "end": "874860"
  },
  {
    "text": "So that's the general gist of\nhow multi-head attention works. If you want a more in-depth\ndescription of this,",
    "start": "874860",
    "end": "882269"
  },
  {
    "text": "there's lots of resources\nonline as well as other courses. And I'll briefly touch upon,\nlike I said, cross-attention.",
    "start": "882270",
    "end": "889060"
  },
  {
    "text": "So here you have actually\nan input sequence and a different output sequence. For example, translating\nfrom French to English.",
    "start": "889060",
    "end": "896710"
  },
  {
    "text": "So here, when you're\ndecoding your output, your English translated\ntext, there's",
    "start": "896710",
    "end": "902339"
  },
  {
    "text": "two sources of attention. One is from the encoder. So the entire encoded\nhidden state of the input.",
    "start": "902340",
    "end": "911350"
  },
  {
    "text": "And that's called\ncross-attention because it's between two\nseparate pieces of text. Your queries here are your\ncurrent decoded outputs.",
    "start": "911350",
    "end": "920220"
  },
  {
    "text": "And your keys and values\nactually come from the encoder. But there's a second\nsource of attention,",
    "start": "920220",
    "end": "925509"
  },
  {
    "text": "which is self-attention between\nthe decoded words themselves. So they're the queries,\nkeys, and values are entirely",
    "start": "925510",
    "end": "932040"
  },
  {
    "text": "from the decoded side. And these types of architectures\ncombine both types of attention. Compared to, like I said,\na decoder-only model, which",
    "start": "932040",
    "end": "939269"
  },
  {
    "text": "would only have self-attention\namong its own tokens.",
    "start": "939270",
    "end": "945880"
  },
  {
    "text": "And so how exactly are\ntransformers compared with RNNs?",
    "start": "945880",
    "end": "952090"
  },
  {
    "text": "So RNNs or recurrent\nneural networks, they had issues representing\nlong range dependencies.",
    "start": "952090",
    "end": "957610"
  },
  {
    "text": "There were issues with gradient\nvanishing, as well as explosion. Since you're concatenating\nall of this information",
    "start": "957610",
    "end": "963160"
  },
  {
    "text": "into one single hidden vector,\nthis leads to a lot of issues potentially. There were a large number\nof training steps involved,",
    "start": "963160",
    "end": "969790"
  },
  {
    "text": "and like I said, you\ncan't parallelize because it's sequential\nand relies on recurrence. Whereas transformers can\nmodel long range dependencies.",
    "start": "969790",
    "end": "977390"
  },
  {
    "text": "There's no gradient vanishing\nor exploding problem. And it can be parallelized.",
    "start": "977390",
    "end": "982569"
  },
  {
    "text": "For example, to take\nmore advantage of things like GPU compute. So overall, it's much\nmore efficient and also",
    "start": "982570",
    "end": "988690"
  },
  {
    "text": "much more effective at\nrepresenting language. And hence why it's one of the\nmost popular deep learning",
    "start": "988690",
    "end": "994900"
  },
  {
    "text": "architectures today. So large language\nmodels are basically a",
    "start": "994900",
    "end": "1000340"
  },
  {
    "text": "scaled up version of this\ntransformer architecture. Up to millions or\nbillions of parameters. And parameters\nhere are basically",
    "start": "1000340",
    "end": "1006970"
  },
  {
    "text": "nodes in the neural network. They're typically trained on\nmassive amounts of general text data.",
    "start": "1006970",
    "end": "1012350"
  },
  {
    "text": "For example, mining a bunch of\ntext from Wikipedia, Reddit, and so forth. Typically there are processes\nto filter this text.",
    "start": "1012350",
    "end": "1020089"
  },
  {
    "text": "For example, getting rid\nof Nazi for work things and general quality filters. And the training objective is\ntypically next token prediction.",
    "start": "1020090",
    "end": "1027439"
  },
  {
    "text": "So again, it's to predict the\nnext token, or the most probable next token given all\nof the previous tokens.",
    "start": "1027440",
    "end": "1033709"
  },
  {
    "text": "So again, this is how\nthe autoregressive left to right architecture\nlike ChatGPT works.",
    "start": "1033710",
    "end": "1039333"
  },
  {
    "text": "And it's also been\nshown that they have emergent abilities\nas they scale up, which Emily will talk about.",
    "start": "1039333",
    "end": "1044589"
  },
  {
    "text": "However, they have heavy\ncomputational cost. Training these huge\nnetworks on tons of data takes a lot of time,\nmoney, and GPUs.",
    "start": "1044589",
    "end": "1051530"
  },
  {
    "text": "And it's also led to the\nfact that this can only be done effectively at\nbig companies, which have these resources\nas well as money.",
    "start": "1051530",
    "end": "1058610"
  },
  {
    "text": "And what's happened now is we\nhave very general models, which you can use and plug\nand play and use them",
    "start": "1058610",
    "end": "1064970"
  },
  {
    "text": "on very on different\ntasks without needing to retrain them using things\nlike in-context, learning,",
    "start": "1064970",
    "end": "1071630"
  },
  {
    "text": "transfer learning,\nas well as prompting. And now Emily will\ntalk about emergent.",
    "start": "1071630",
    "end": "1077860"
  },
  {
    "text": "Yeah. So I guess a natural\nquestion for why language models work so well, is\nwhat happens when you scale up.",
    "start": "1077860",
    "end": "1085600"
  },
  {
    "text": "And as we've seen\nin the past, there's been this big trend of investing\nmore money into our compute,",
    "start": "1085600",
    "end": "1091289"
  },
  {
    "text": "making our models larger,\nand larger, and larger. And actually we have seen some\nreally cool things come out of it, which we have now\ntermed emergent abilities.",
    "start": "1091290",
    "end": "1100049"
  },
  {
    "text": "We can call emergent\nabilities, an ability that is present in a larger\nmodel, but not in a smaller one.",
    "start": "1100050",
    "end": "1108180"
  },
  {
    "text": "And I think the thing that is\nmost interesting about this is emergent abilities\nare very unpredictable.",
    "start": "1108180",
    "end": "1114100"
  },
  {
    "text": "It's not necessarily like\nwe have a scaling law that we just keep training,\nand training, and training this model and we can say,\noh, at this training step,",
    "start": "1114100",
    "end": "1123510"
  },
  {
    "text": "we'll have this ability to\ndo this really cool thing. It's actually something more\nlike, it's kind of random.",
    "start": "1123510",
    "end": "1128889"
  },
  {
    "text": "And then at this\nthreshold that is pretty difficult or impossible\nto predict, it just improves.",
    "start": "1128890",
    "end": "1134530"
  },
  {
    "text": "And we call that a\nphase transition. And this is a figure\ntaken by a paper authored",
    "start": "1134530",
    "end": "1140420"
  },
  {
    "text": "by a speaker we'll\nhave next week-- Jason Wei, who I'm very\nexcited to hear from,",
    "start": "1140420",
    "end": "1145790"
  },
  {
    "text": "and he did this really\ncool research project with a bunch of\nother people, sort of characterizing and exhibiting\na lot of the emergent abilities",
    "start": "1145790",
    "end": "1154700"
  },
  {
    "text": "that you can notice\nin different models. So here we have five\ndifferent models.",
    "start": "1154700",
    "end": "1159860"
  },
  {
    "text": "And a lot of\ndifferent common tasks that we test language models on\nto see what their abilities are.",
    "start": "1159860",
    "end": "1167549"
  },
  {
    "text": "And so, for example, complicated\narithmetic, or transliteration, being able to tell\nif someone is telling",
    "start": "1167550",
    "end": "1174919"
  },
  {
    "text": "the truth, other\nthings like this. And as you can notice\non this figure, we have these eight graphs, and\nthere's this very obvious spike.",
    "start": "1174920",
    "end": "1184690"
  },
  {
    "text": "It's not necessarily like this\ngradual increase in accuracy.",
    "start": "1184690",
    "end": "1190049"
  },
  {
    "text": "And so that is sort of what we\ncan term that phase transition.",
    "start": "1190050",
    "end": "1195220"
  },
  {
    "text": "And currently there's\nvery few explanations for why these abilities emerge. Evaluation metrics used to\nmeasure these abilities,",
    "start": "1195220",
    "end": "1202750"
  },
  {
    "text": "don't fully explain\nwhy they emerge. And an interesting\nresearch paper",
    "start": "1202750",
    "end": "1208240"
  },
  {
    "text": "that came out recently by\nsome researchers at Stanford actually claimed that maybe\nemergent abilities of LLMs",
    "start": "1208240",
    "end": "1214659"
  },
  {
    "text": "are nonexistent. Maybe it's more so\nthe researchers choice",
    "start": "1214660",
    "end": "1219700"
  },
  {
    "text": "of metric being\nnonlinear rather than fundamental changes in the\nmodel responding to the scale.",
    "start": "1219700",
    "end": "1228110"
  },
  {
    "text": "And so a natural question is,\nis scaling the best thing to do?",
    "start": "1228110",
    "end": "1234600"
  },
  {
    "text": "Is it the only thing to do? Is it the most significant way\nthat we can improve our models?",
    "start": "1234600",
    "end": "1240380"
  },
  {
    "text": "And so while scaling is a factor\nin these emergent abilities, it is not the only factor,\nespecially in smaller models.",
    "start": "1240380",
    "end": "1246780"
  },
  {
    "text": "We have new architectures,\nhigher quality data, and improved training procedures\nthat could potentially",
    "start": "1246780",
    "end": "1254030"
  },
  {
    "text": "bring about these emergent\nabilities on smaller models. And so these present a lot of\ninteresting research directions,",
    "start": "1254030",
    "end": "1260300"
  },
  {
    "text": "including improving\nfew-shot prompting abilities as we've seen before\nthrough other methods,",
    "start": "1260300",
    "end": "1265820"
  },
  {
    "text": "and theoretical and\ninterpretability research, computational\nlinguistics work, and yeah,",
    "start": "1265820",
    "end": "1272390"
  },
  {
    "text": "other directions. And so as some\ninteresting questions, do you believe that emergent\nabilities will continue",
    "start": "1272390",
    "end": "1279350"
  },
  {
    "text": "to arise with more scale? Is there like maybe once\nwe get to some crazy number",
    "start": "1279350",
    "end": "1285290"
  },
  {
    "text": "of parameters, then\nour language models will suddenly be able\nto think on their own and do all sorts of cool things,\nor is there some sort of limit.",
    "start": "1285290",
    "end": "1292830"
  },
  {
    "text": "What are your thoughts\non this current trend of larger models and more data? ",
    "start": "1292830",
    "end": "1299430"
  },
  {
    "text": "Is this a good direction? Larger models obviously mean\nmore money, more compute, and Less democratization\nof AI research.",
    "start": "1299430",
    "end": "1308670"
  },
  {
    "text": "And thoughts on retrieval\nbased or retrieval augmented systems compared\nto simply learning everything",
    "start": "1308670",
    "end": "1314039"
  },
  {
    "text": "within the parameters\nof the model. So lots of cool directions. ",
    "start": "1314040",
    "end": "1321390"
  },
  {
    "text": "Yeah. So we have some quick\nintroductions on reinforcement learning from human feedback. I think a lot of you\nmight already know.",
    "start": "1321390",
    "end": "1328610"
  },
  {
    "text": "So reinforcement learning\nfrom human feedback is a technique to train\nlarge language models.",
    "start": "1328610",
    "end": "1335190"
  },
  {
    "text": "Usually, you give\nhuman like humans two outputs of the\nlanguage model. Ask them what they prefer.",
    "start": "1335190",
    "end": "1340940"
  },
  {
    "text": "We select the one they\nprefer, and feed it back into the model to train a\nmore human aligned model.",
    "start": "1340940",
    "end": "1347149"
  },
  {
    "text": "Recently, there has been more-- since reinforcement,\nlearning from human feedback",
    "start": "1347150",
    "end": "1352190"
  },
  {
    "text": "has its limitations, you\nneed quality human feedback, you need good rewards,\nyou need a good policy. It's very complicated\ntraining process.",
    "start": "1352190",
    "end": "1359810"
  },
  {
    "text": "A recent paper, DPO,\nuses just preference data and non-preference data and\nfeeds that into the language",
    "start": "1359810",
    "end": "1367130"
  },
  {
    "text": "model to-- it's a much more\nfaster algorithm to train these language models.",
    "start": "1367130",
    "end": "1374210"
  },
  {
    "text": "So quick introduction to GPT. We have ChatGPT, which\nis fine-tuned on GPT-3.5.",
    "start": "1374210",
    "end": "1382000"
  },
  {
    "text": "We have a diagram of the\ndifferent types of GPT models that have been released. And GPT-4 is the next\nversion, and it's",
    "start": "1382000",
    "end": "1391080"
  },
  {
    "text": "supervised on a large\ntraining dataset with RLHF, the APIs of text [? FNC ?]\nalso have been trained on RLHF.",
    "start": "1391080",
    "end": "1399090"
  },
  {
    "text": "Then we have Gemini,\nwhich is Gemini, a model, which was like\nbasically Google's AI from BART,",
    "start": "1399090",
    "end": "1405600"
  },
  {
    "text": "now is Gemini. And when it was released,\nthere was like a big hype because it performed\nmuch better than ChatGPT",
    "start": "1405600",
    "end": "1413340"
  },
  {
    "text": "on like 30 out of the\n32 academic benchmarks. So there was a lot of\nexcitement around this.",
    "start": "1413340",
    "end": "1419680"
  },
  {
    "text": "And now as people\nhave used it, there have been different\nlike, we realized that different models are good\nfor different types of tasks.",
    "start": "1419680",
    "end": "1426730"
  },
  {
    "text": "One interesting\nthing is that Gemini is trained on the MoE model,\nwhich is a mixture of experts",
    "start": "1426730",
    "end": "1431850"
  },
  {
    "text": "model where we have a bunch of\nsmaller neural networks that are known as experts and are\ntrained and capable of handling",
    "start": "1431850",
    "end": "1437910"
  },
  {
    "text": "different things. So we could have\none neural network that's really good at pulling\nimages from the web, one",
    "start": "1437910",
    "end": "1442980"
  },
  {
    "text": "good at pulling like text. And then we have our\nfinal gated network, which response is the best\nsuited to address the request.",
    "start": "1442980",
    "end": "1450410"
  },
  {
    "text": " So now that takes us to\nwhere we are right now.",
    "start": "1450410",
    "end": "1456800"
  },
  {
    "text": "So AI, especially NLP\nlarge language models have taken off, like Seonghee\nsaid, things like GPT-4, Gemini,",
    "start": "1456800",
    "end": "1463940"
  },
  {
    "text": "and so forth. A lot of things involving human\nalignment and interaction, such as RLHF.",
    "start": "1463940",
    "end": "1469399"
  },
  {
    "text": "There's more work now on trying\nto control the toxicity bias as well as ethical\nconcerns involving these models, especially\nas more and more people",
    "start": "1469400",
    "end": "1477110"
  },
  {
    "text": "gain access to them,\nthings like ChatGPT. There's also more use\nin unique applications,",
    "start": "1477110",
    "end": "1482570"
  },
  {
    "text": "things like audio, music,\nneuroscience, biology, and so forth.",
    "start": "1482570",
    "end": "1488720"
  },
  {
    "text": "We'll have some slides\nbriefly touching upon those, but these things are mainly\ntouched upon by our speakers.",
    "start": "1488720",
    "end": "1496050"
  },
  {
    "text": "And there's also\ndiffusion models. A separate class of models,\nalthough now there's a diffusion",
    "start": "1496050",
    "end": "1501380"
  },
  {
    "text": "transformer where they\nreplace the U-net backbone in the diffusion model with the\ntransformer architecture, which",
    "start": "1501380",
    "end": "1507470"
  },
  {
    "text": "works better for things like\ntext-to-video generation. For example, Sora uses\nthe diffusion transformer.",
    "start": "1507470",
    "end": "1514540"
  },
  {
    "text": "So what's next? So as we see the use of\ntransformers and machine",
    "start": "1514540",
    "end": "1520420"
  },
  {
    "text": "learning get more and more\nprominent throughout the world.",
    "start": "1520420",
    "end": "1525940"
  },
  {
    "text": "It's very exciting,\nbut also scary. So it can enable a\nlot more applications,",
    "start": "1525940",
    "end": "1531670"
  },
  {
    "text": "things like very\ngeneralist agents, longer video understanding\nas well as generation.",
    "start": "1531670",
    "end": "1536900"
  },
  {
    "text": "Maybe in five, 10 years, we can\ngenerate a whole Netflix series by just putting in a prompt or\nan a description of the show we",
    "start": "1536900",
    "end": "1546910"
  },
  {
    "text": "want to watch. Things like incredibly\nlong sequence modeling, which Gemini--",
    "start": "1546910",
    "end": "1552530"
  },
  {
    "text": "I think now it is\nable to handle. They claim a million\ntokens for more. So we'll see if that\ncan further scale up,",
    "start": "1552530",
    "end": "1559930"
  },
  {
    "text": "which is very exciting. Things like very domain-specific\nfoundation models, things like having a\nDoctorGPT, LawyerGPT,",
    "start": "1559930",
    "end": "1566800"
  },
  {
    "text": "any sort of GPT for any use case\nor application you might want. And also other potential\nreal world impacts,",
    "start": "1566800",
    "end": "1574530"
  },
  {
    "text": "personalized education, as\nwell as tutoring systems, advanced healthcare diagnostics,\nenvironmental monitoring,",
    "start": "1574530",
    "end": "1581010"
  },
  {
    "text": "and so forth, real-time\nmultilingual communication. You go to China, Japan\nor something, real-time,",
    "start": "1581010",
    "end": "1588000"
  },
  {
    "text": "you're able to\ninteract with everyone. As well as interactive\nentertainment and gaming. Potentially, we can\nhave more realistic NPCs",
    "start": "1588000",
    "end": "1596549"
  },
  {
    "text": "which are run by\ntransformers as well as AI. And so what's missing?",
    "start": "1596550",
    "end": "1602620"
  },
  {
    "text": "You know, there's this\nbuzzword, AGI/ASI-- artificial general intelligence\nor superintelligence.",
    "start": "1602620",
    "end": "1608470"
  },
  {
    "text": "So what's really\nmissing to get there? These are some of\nthe things that we thought might be the case.",
    "start": "1608470",
    "end": "1613960"
  },
  {
    "text": "Firstly, is reducing\ncomputation complexity. As these models and\ndatasets scale up, it will become even more\ncostly and difficult to train.",
    "start": "1613960",
    "end": "1622210"
  },
  {
    "text": "So we need a way to reduce that. And hence, human\ncontrollability of these models, the alignment of language\nmodels potentially",
    "start": "1622210",
    "end": "1629200"
  },
  {
    "text": "with the human brain, adaptive\nlearning and generalization across even more domains,\nmultisensory multimodal",
    "start": "1629200",
    "end": "1636970"
  },
  {
    "text": "embodiment. This will allow\nit to learn things intuitive physics and common\nsense that humans are able to.",
    "start": "1636970",
    "end": "1642260"
  },
  {
    "text": "But since these models,\nespecially language models are trained purely on\ntext, they don't actually have a sort of intuitive\nor human-like understanding",
    "start": "1642260",
    "end": "1651070"
  },
  {
    "text": "of the real world, since\nall they've seen is text. Infinite or external\nmemory, as well as",
    "start": "1651070",
    "end": "1657760"
  },
  {
    "text": "self-improvement and\nself-reflection capabilities. Like humans, we're able to\ncontinuously learn and improve",
    "start": "1657760",
    "end": "1663470"
  },
  {
    "text": "ourselves. Complete autonomy and long\nhorizon decision-making,",
    "start": "1663470",
    "end": "1668960"
  },
  {
    "text": "emotional intelligence\nand social understanding, as well as, of course, ethical\nreasoning and value alignment.",
    "start": "1668960",
    "end": "1674970"
  },
  {
    "text": " Cool. ",
    "start": "1674970",
    "end": "1682799"
  },
  {
    "text": "Cool. So let's get to some of the\ninteresting parts about LLMs. So there's a lot of applications\nwe are already starting",
    "start": "1682800",
    "end": "1688920"
  },
  {
    "text": "to see in the real world. Like the ChatGPT is one\nof the biggest examples. It's like the fastest growing\nconsumer app in history,",
    "start": "1688920",
    "end": "1695640"
  },
  {
    "text": "which just went really viral. Everyone started using it. Just cause this people know\nAI exists in the real world",
    "start": "1695640",
    "end": "1702477"
  },
  {
    "text": "and cause it. Before that was just\nsegment say people us who are at Stanford\nwho are using AI. And then a lot of the\npeople in the world",
    "start": "1702478",
    "end": "1709200"
  },
  {
    "text": "were like, OK, what is even AI? When they got their first\nexperience with ChatGPT, they were like,\nOK, like, OK, like. This thing actually works.",
    "start": "1709200",
    "end": "1715270"
  },
  {
    "text": "We believe in that. And now we are starting\nto see a lot of this in different applications,\nlike speeches or something,",
    "start": "1715270",
    "end": "1722130"
  },
  {
    "text": "where you have a lot of this\nnew models, like Whisper, you also have this\nElevenLabs, bunch of things that are happening.",
    "start": "1722130",
    "end": "1728020"
  },
  {
    "text": "Music is a big industry. Images and videos are also\nlike starting to transform. So we can imagine maybe\nfive years from now,",
    "start": "1728020",
    "end": "1734160"
  },
  {
    "text": "all Hollywood movies might be\nproduced by like video models. You might not even need\nactors, for example. You might just have\nfake actors and like,",
    "start": "1734160",
    "end": "1740882"
  },
  {
    "text": "you spend like\nbillions of dollars just going to different parts in\nthe world and shooting scenes. But that can all be just\ndone by a video model.",
    "start": "1740883",
    "end": "1747440"
  },
  {
    "text": "So something like Sora and\nwhat's happening right now, I think that's going to be\ngame-changing because that's",
    "start": "1747440",
    "end": "1752481"
  },
  {
    "text": "how movie production,\nadvertisement, all of maybe social media\nwill be driven by that.",
    "start": "1752482",
    "end": "1758950"
  },
  {
    "text": "And it's already\nfascinating to just see how realistic all these\nimages and the videos look.",
    "start": "1758950",
    "end": "1764620"
  },
  {
    "text": "It's almost like better than\nlike a human artist quality. So it's getting very\ninteresting and very hard",
    "start": "1764620",
    "end": "1771580"
  },
  {
    "text": "to also distinguish what's\nreal and what's fake. And one very\ninteresting application",
    "start": "1771580",
    "end": "1778000"
  },
  {
    "text": "will be when you can\ntake these models and embody them\nin the real world. So for example, if you have\nsome games like Minecraft,",
    "start": "1778000",
    "end": "1785470"
  },
  {
    "text": "for example, where you can have\nan AI that can play the game, and then we are already\nstarting to see that where",
    "start": "1785470",
    "end": "1790600"
  },
  {
    "text": "there's a lot of work\nwhere you have an AI that's masquerading as a human.",
    "start": "1790600",
    "end": "1796673"
  },
  {
    "text": "And it's actually able\nto go and win the game. So there's a lot of this stuff\nthat's happening in real-time and people are doing that.",
    "start": "1796673",
    "end": "1802273"
  },
  {
    "text": "And it's actually,\nwe are reaching some level of\nsuperhuman performance there in virtual games. Similarly in the\nrobotics, it'll be",
    "start": "1802273",
    "end": "1809470"
  },
  {
    "text": "very exciting to see\nonce you can apply AI in the physical\nworld, you can just enable so many applications.",
    "start": "1809470",
    "end": "1814840"
  },
  {
    "text": "You can have physical helpers\nin your home, industry, so on. And there's almost a race for\nbuilding the humanoid robots that's going on right now.",
    "start": "1814840",
    "end": "1820700"
  },
  {
    "text": "So if you look at\nwhat Tesla is doing, what's this company\ncalled Figure is doing. So everyone is really\nexcited about, OK,",
    "start": "1820700",
    "end": "1826450"
  },
  {
    "text": "can we go and build this\nphysical helpers that can go and help you with\na lot of different things",
    "start": "1826450",
    "end": "1833740"
  },
  {
    "text": "in real life. And so definitely a lot of\nfun research and applications",
    "start": "1833740",
    "end": "1841450"
  },
  {
    "text": "have already been applied\nby OpenAI, DeepMind, Meta, and so on.",
    "start": "1841450",
    "end": "1846680"
  },
  {
    "text": "And we've also seen a lot\nof interesting applications in biology and healthcare.",
    "start": "1846680",
    "end": "1851840"
  },
  {
    "text": "So Google introduced this\nMed-PaLM model last year. We actually had the\nfirst author of the work",
    "start": "1851840",
    "end": "1857380"
  },
  {
    "text": "give a talk in the last\niteration of the course. This is very\ninteresting because this",
    "start": "1857380",
    "end": "1862870"
  },
  {
    "text": "is a transformer\nmodel that can be applied for actual\nmedical applications. Google is right now deploying\nthis in actual hospitals",
    "start": "1862870",
    "end": "1869019"
  },
  {
    "text": "for analyzing, like the patient\nhealth data, a lot of history, medical diagnosis,\ncancer detection, so on.",
    "start": "1869020",
    "end": "1876590"
  },
  {
    "start": "1876590",
    "end": "1882130"
  },
  {
    "text": "So now we'll touch\nbriefly upon some of the recent trends in terms\nof transformers research,",
    "start": "1882130",
    "end": "1889420"
  },
  {
    "text": "as well as potentially remaining\nweaknesses and challenges. So as I explained earlier, you\nknow, a large amount of data",
    "start": "1889420",
    "end": "1895510"
  },
  {
    "text": "compute and cost to train,\nover weeks or months, thousands of GPUs. And now there's this thing\ncalled the BabyLM challenge--",
    "start": "1895510",
    "end": "1901790"
  },
  {
    "text": "can we train LLMs using similar\namounts of text data baby is exposed to while growing up?",
    "start": "1901790",
    "end": "1908140"
  },
  {
    "text": "So essentially,\ncomparing LLMs and humans is one aspect of\nmy own research.",
    "start": "1908140",
    "end": "1915250"
  },
  {
    "text": "I believe that\nchildren are different. We learn very differently\nas humans compared to LLMs.",
    "start": "1915250",
    "end": "1920510"
  },
  {
    "text": "They do statistical learning. This requires a\nlarge amount of data to actually learn statistical\nrelations between words,",
    "start": "1920510",
    "end": "1927070"
  },
  {
    "text": "in order to get things like\nabstraction, generalization, and reasoning capabilities. Whereas human's learn\nin more structured,",
    "start": "1927070",
    "end": "1935470"
  },
  {
    "text": "probably smarter ways. For example, learn in more\ncompositional or hierarchical",
    "start": "1935470",
    "end": "1941779"
  },
  {
    "text": "manners, which will allow us to\nlearn these things more easily. And so one of my\nprofessors, Michael Frank,",
    "start": "1941780",
    "end": "1949040"
  },
  {
    "text": "he made this tweet showing how-- this four to five orders\nof magnitude difference",
    "start": "1949040",
    "end": "1956000"
  },
  {
    "text": "between human and LLM\nemergence of many behaviors. And this is magnitude, not time. So like 10,000 up to a millions\nof times as much data required",
    "start": "1956000",
    "end": "1964520"
  },
  {
    "text": "for LLMs compared to humans. This may be to the fact that\nhumans have innate knowledge.",
    "start": "1964520",
    "end": "1970220"
  },
  {
    "text": "This relates to\npriors basically. You know, when we're born,\nmaybe due to evolution, we already have some\nfundamental capabilities",
    "start": "1970220",
    "end": "1976220"
  },
  {
    "text": "built into our brains. Second is multimodal grounding. We don't just learn from texts. We learn from interacting with\nthe world, with other people",
    "start": "1976220",
    "end": "1984350"
  },
  {
    "text": "through vision, smell, things we\ncan hear, see, feel and touch. The third is active\nsocial learning.",
    "start": "1984350",
    "end": "1991520"
  },
  {
    "text": "We learn while\ngrowing up by talking to our parents,\nteachers, other children. This is not just\nbasic things, but even",
    "start": "1991520",
    "end": "1997850"
  },
  {
    "text": "things like values, human values\nto treat others with kindness and so forth. And this is not\nsomething that LLM",
    "start": "1997850",
    "end": "2004420"
  },
  {
    "text": "is really exposed to\nwhen it's trained on just large amounts of text data.",
    "start": "2004420",
    "end": "2010610"
  },
  {
    "text": "Kind of related is this trend\ntowards smaller open source models, potentially\nthings we can even",
    "start": "2010610",
    "end": "2016880"
  },
  {
    "text": "run on our everyday devices. For example, there's more\nand more work on AutoGPT",
    "start": "2016880",
    "end": "2022280"
  },
  {
    "text": "as well as ChatGPT plugins. Smaller open source models\nlike LLaMA, as well as Mistral models.",
    "start": "2022280",
    "end": "2028080"
  },
  {
    "text": "And in the future,\nhopefully we'll be able to finetune and run even\nmore models locally, potentially",
    "start": "2028080",
    "end": "2034790"
  },
  {
    "text": "even on our smartphone. ",
    "start": "2034790",
    "end": "2039889"
  },
  {
    "text": "Another area of\nresearch and work is in memory augmentation\nas well as personalization.",
    "start": "2039890",
    "end": "2045780"
  },
  {
    "text": "So current big\nweakness of LLMs is they're sort of\nfrozen in knowledge at a particular point in time.",
    "start": "2045780",
    "end": "2051429"
  },
  {
    "text": "They don't augment\nknowledge on the fly as they're talking to you. It's not stored into their\nbrain, the parameters.",
    "start": "2051429",
    "end": "2058887"
  },
  {
    "text": "The next time you start\na new conversation, there's a very high\nchance it won't remember anything you said before.",
    "start": "2058887",
    "end": "2065750"
  },
  {
    "text": "I'll get to RAG in a bit. So one of our goals\nhopefully in the future is to have this wide scale\nmemory augmentation as well",
    "start": "2065750",
    "end": "2073969"
  },
  {
    "text": "as personalization. Somehow update the\nmodel on the fly while talking to hundreds or\nthousands or millions of users",
    "start": "2073969",
    "end": "2080989"
  },
  {
    "text": "around the world. And to adapt not\nonly their knowledge, but their talking style\nas well as persona",
    "start": "2080989",
    "end": "2087169"
  },
  {
    "text": "to the particular user. And this is called\npersonalization. This could have many\ndifferent applications,",
    "start": "2087170",
    "end": "2092460"
  },
  {
    "text": "such as mental health\ntherapy, and so forth. So some potential\napproaches for this",
    "start": "2092460",
    "end": "2098220"
  },
  {
    "text": "could be having a memory bank. This is not that feasible\nwith larger amounts of data. Prefix-tuning approaches, which\nfinetunes only a very small",
    "start": "2098220",
    "end": "2106980"
  },
  {
    "text": "portion of the model. However, when you\nhave such huge LLMs, even finetuning a very\nsmall portion of the model",
    "start": "2106980",
    "end": "2112349"
  },
  {
    "text": "is incredibly expensive. Maybe some prompt-based\napproaches in context learning.",
    "start": "2112350",
    "end": "2117839"
  },
  {
    "text": "However, again, this will\nnot change the model itself. It will likely not carry forward\namong different conversations.",
    "start": "2117840",
    "end": "2126210"
  },
  {
    "text": "And there's this\nthing now called RAG-- retrieval-augmented\ngeneration, which is related to memory\nbank, where you have",
    "start": "2126210",
    "end": "2131940"
  },
  {
    "text": "a data store of information. And each time when the user\nputs in an input query,",
    "start": "2131940",
    "end": "2137010"
  },
  {
    "text": "you first look at if\nthere's relevant information from this data store,\nthat you can then augment as context into LLM\nto help guide its output.",
    "start": "2137010",
    "end": "2146370"
  },
  {
    "text": "This relies on having a high\nquality external data store. And it's also typically\nnot end-to-end.",
    "start": "2146370",
    "end": "2153330"
  },
  {
    "text": "And the main thing\nhere is it's not within the brain of\nthe model, but outside.",
    "start": "2153330",
    "end": "2158530"
  },
  {
    "text": "It's suitable for knowledge\nor fact-based information. But it's not really\nsuitable for enhancing",
    "start": "2158530",
    "end": "2163660"
  },
  {
    "text": "the fundamental capabilities\nor skills of the model. There's also lots of work now\non pre-training data synthesis,",
    "start": "2163660",
    "end": "2173130"
  },
  {
    "text": "especially after ChatGPT\nand GPT-4 came out. Instead of having\nto collect data from humans, which can be very\nexpensive and time-consuming,",
    "start": "2173130",
    "end": "2180390"
  },
  {
    "text": "many researchers now are\nusing GPT-4 for example, to collect data, to\ntrain other models.",
    "start": "2180390",
    "end": "2186359"
  },
  {
    "text": "For example, model\ndistillation, training smaller and less\ncapable models with data",
    "start": "2186360",
    "end": "2193260"
  },
  {
    "text": "from larger models like GPT-4. An example is the\nMicrosoft Phi models",
    "start": "2193260",
    "end": "2198300"
  },
  {
    "text": "introduced from their paper-- Textbooks Are All You Need. And speaking a bit more\nabout the Phi model,",
    "start": "2198300",
    "end": "2204160"
  },
  {
    "text": "it's a 2.7 billion parameter\nmodel, Phi version 2, and it excels in\nreasoning in language.",
    "start": "2204160",
    "end": "2210970"
  },
  {
    "text": "Challenging or having\ncomparative performance to compare two models\nup to 25 times larger,",
    "start": "2210970",
    "end": "2217180"
  },
  {
    "text": "which is incredibly impressive. And their main takeaway here\nis the quality or source",
    "start": "2217180",
    "end": "2224860"
  },
  {
    "text": "of data is incredibly important. So they emphasize textbook\nquality training data and synthetic data.",
    "start": "2224860",
    "end": "2231490"
  },
  {
    "text": "They generated synthetic\ndata to teach the model common sense reasoning\nand general knowledge. This includes things like\nscience, daily activities,",
    "start": "2231490",
    "end": "2238540"
  },
  {
    "text": "theory of mind, and so forth. They then augmented this with\nadditional data collected from the web that\nwas filtered-based",
    "start": "2238540",
    "end": "2245560"
  },
  {
    "text": "on educational value as\nwell as content quality. And what this allowed them to do\nis train a much smaller model,",
    "start": "2245560",
    "end": "2252310"
  },
  {
    "text": "much more efficiently,\nwhile challenging models up to 25 times larger, which\nis, again, very impressive.",
    "start": "2252310",
    "end": "2260710"
  },
  {
    "text": "Another area of debate is\nare LLMs truly learning. Are they learning new knowledge? When you ask it to\ndo something, is it",
    "start": "2260710",
    "end": "2266863"
  },
  {
    "text": "generating it from scratch? Or is it simply regurgitating,\nsomething it's memorized before?",
    "start": "2266863",
    "end": "2273560"
  },
  {
    "text": "This line has been blurred\nand it's not clear, because the way LLMs learn is\nfrom, again, learning patterns",
    "start": "2273560",
    "end": "2279100"
  },
  {
    "text": "from lots of texts, which you\ncan say is somewhat memorizing. There's also the potential\nfor test time contamination.",
    "start": "2279100",
    "end": "2286869"
  },
  {
    "text": "Models might\nregurgitate information, it's seen during training\nwhile being evaluated,",
    "start": "2286870",
    "end": "2292930"
  },
  {
    "text": "and this can lead to\nmisleading benchmark results. There's also\ncognitive simulation.",
    "start": "2292930",
    "end": "2298359"
  },
  {
    "text": "So a lot of people arguing\nthat LLMs mimic human thought processes, while others say no.",
    "start": "2298360",
    "end": "2305390"
  },
  {
    "text": "It's just a sophisticated\nform of pattern matching, and it's not nearly as\ncomplex or biological",
    "start": "2305390",
    "end": "2311650"
  },
  {
    "text": "or sophisticated as a human. And this also leads to a lot\nof ethical as well as practical",
    "start": "2311650",
    "end": "2317740"
  },
  {
    "text": "limitations. So, for example,\nI'm sure you've all heard that recent lawsuit,\ncopyright lawsuit by New York",
    "start": "2317740",
    "end": "2323360"
  },
  {
    "text": "Times on OpenAI, where they\nclaim that OpenAIs ChatGPT was basically regurgitating existing\nNew York Times articles.",
    "start": "2323360",
    "end": "2331200"
  },
  {
    "text": "And this is again,\nthis issue with LLMs potentially memorizing\ntext it saw during training",
    "start": "2331200",
    "end": "2337700"
  },
  {
    "text": "rather than synthesizing\nnew information entirely from scratch. ",
    "start": "2337700",
    "end": "2344970"
  },
  {
    "text": "Another big source\nof challenge, which might be able to close the\ngap between current models",
    "start": "2344970",
    "end": "2351450"
  },
  {
    "text": "and eventually maybe\nAGI is this concept of continual learning,\nAKA, infinite",
    "start": "2351450",
    "end": "2358500"
  },
  {
    "text": "and permanent fundamental\nself-improvement. So humans were able to\nlearn constantly, every day",
    "start": "2358500",
    "end": "2364109"
  },
  {
    "text": "from every interaction. I'm learning right now\nfrom just talking to you and giving this lecture.",
    "start": "2364110",
    "end": "2369520"
  },
  {
    "text": "We don't need to\nfinetune ourselves. We don't need to sit-in a\nchair and then have someone read the whole internet\nto us every two months",
    "start": "2369520",
    "end": "2376890"
  },
  {
    "text": "or something like that. Currently, there's\nwork on finetuning, a small model based on\ntraces from a better model,",
    "start": "2376890",
    "end": "2383760"
  },
  {
    "text": "or the same model after\nfiltering those traces. However, this is closer to\nretraining and distillation",
    "start": "2383760",
    "end": "2389579"
  },
  {
    "text": "than it is to true human\ncontinual learning. So that's definitely, I think,\nat least a very exciting",
    "start": "2389580",
    "end": "2397440"
  },
  {
    "text": "direction. Another area of challenge is\ninterpreting these huge LLMs",
    "start": "2397440",
    "end": "2404210"
  },
  {
    "text": "with billions of parameters. They're essentially\nhuge black-box models, where it's really\nhard to understand",
    "start": "2404210",
    "end": "2410030"
  },
  {
    "text": "exactly what is going on. If we were able to\nunderstand them better,",
    "start": "2410030",
    "end": "2415280"
  },
  {
    "text": "this would allow us to\nknow what exactly we should try to improve. It also allows us to\ncontrol these models better,",
    "start": "2415280",
    "end": "2423210"
  },
  {
    "text": "and potentially lead to better\nalignment as well as safety. And there's this\narea of work called",
    "start": "2423210",
    "end": "2429289"
  },
  {
    "text": "mechanistic\ninterpretability, which tries to understand exactly how\nthe individual components, as",
    "start": "2429290",
    "end": "2434570"
  },
  {
    "text": "well as operations in a\nmachine learning model contribute to its overall\ndecision-making process.",
    "start": "2434570",
    "end": "2440420"
  },
  {
    "text": "And to try to unpack\nthat black-box, I guess.",
    "start": "2440420",
    "end": "2445980"
  },
  {
    "text": "So speaking a bit more\nabout a concept related to mechanistic interpretability\nas well as continual learning",
    "start": "2445980",
    "end": "2452130"
  },
  {
    "text": "is model editing. So this is a newer\nline of work which hasn't seen too much\ninvestigation, also because it's",
    "start": "2452130",
    "end": "2458010"
  },
  {
    "text": "very challenging. But basically, this\nlooks like can we edit very specific\nnodes in the model",
    "start": "2458010",
    "end": "2463320"
  },
  {
    "text": "without having to retrain it. So one of the papers\nI linked there, they developed a causal\nintervention method to trace",
    "start": "2463320",
    "end": "2471570"
  },
  {
    "text": "the neural activations for\nmodel factual predictions, and they came up\nwith this method called Rank-One Model\nEditing or ROME,",
    "start": "2471570",
    "end": "2478740"
  },
  {
    "text": "that was able to modify\nvery specific model weights for updating\nfactual associations.",
    "start": "2478740",
    "end": "2484480"
  },
  {
    "text": "For example, Ottawa is\nthe capital of Canada and then modifying\nthat to something else.",
    "start": "2484480",
    "end": "2490540"
  },
  {
    "text": "They found they didn't need\nto re-finetune the model. They were able to\ninject that information",
    "start": "2490540",
    "end": "2496650"
  },
  {
    "text": "into the model, pretty\nmuch in a permanent way by simply modifying\nvery specific nodes.",
    "start": "2496650",
    "end": "2502200"
  },
  {
    "text": "They also found that\nmid-layer feedforward modules played a very significant\nrole in storing",
    "start": "2502200",
    "end": "2507550"
  },
  {
    "text": "these sorts of factual\ninformation or associations. And the manipulation of these\ncan be a feasible approach",
    "start": "2507550",
    "end": "2514150"
  },
  {
    "text": "for model editing. So I think this is\na very cool line of work with potential\nlong term impacts.",
    "start": "2514150",
    "end": "2521800"
  },
  {
    "text": "And as Seonghee stated\nbefore, another line of work is basically mixture of experts. So this is very prevalent\nin current day LLMs,",
    "start": "2521800",
    "end": "2529210"
  },
  {
    "text": "things like GPT-4 and Gemini. And so to have several models\nor experts work together to solve a problem, and\narrive at a final generation.",
    "start": "2529210",
    "end": "2537369"
  },
  {
    "text": "And there's a lot\nof research on how to better define and\ninitialize these experts and connect them to come\nup with a final result.",
    "start": "2537370",
    "end": "2547122"
  },
  {
    "text": "And I'm thinking, is\nthere a way of potentially having a single model\nvariation of this similar to the human brain?",
    "start": "2547122",
    "end": "2552542"
  },
  {
    "text": "For example, the human brain,\nwe have different parts of our brain for\ndifferent things. One part of our brain might work\nmore for spatial reasoning, one",
    "start": "2552542",
    "end": "2560750"
  },
  {
    "text": "for physical reasoning, one for\nmathematical logical reasoning, and so forth. Maybe there's a way of\nsegmenting a single neural",
    "start": "2560750",
    "end": "2566810"
  },
  {
    "text": "network or model in such a way. For example, by adding more\nlayers on top of a foundation model, and then only finetuning\nthose specific layers",
    "start": "2566810",
    "end": "2575300"
  },
  {
    "text": "for different purposes. Related to continual learning\nis self-improvement as well as",
    "start": "2575300",
    "end": "2582050"
  },
  {
    "text": "self reflection. So there's been a\nlot of work recently that's also shown that\nmodels, especially LLMs, they",
    "start": "2582050",
    "end": "2588079"
  },
  {
    "text": "can reflect on their own output\nto iteratively refine as well as improve them.",
    "start": "2588080",
    "end": "2593420"
  },
  {
    "text": "It's been shown that\nthis improvement can happen across several\nlayers of self-reflection.",
    "start": "2593420",
    "end": "2600360"
  },
  {
    "text": "Having a mini version\nof continual learning up to a certain degree.",
    "start": "2600360",
    "end": "2605450"
  },
  {
    "text": "And some folks believe\nthat AGI is basically a constant state\nof self-reflection,",
    "start": "2605450",
    "end": "2611330"
  },
  {
    "text": "which is, again, similar\nto what a human does. ",
    "start": "2611330",
    "end": "2617140"
  },
  {
    "text": "Lastly, a big issue is\nthe hallucination problem, where a model does not\nknow what it does not know.",
    "start": "2617140",
    "end": "2623500"
  },
  {
    "text": "And due to the\nsampling procedure, there's a very high\nchance, for example, I'm sure you've also used\nChatGPT before that it sometimes",
    "start": "2623500",
    "end": "2629859"
  },
  {
    "text": "generates text, it's\nvery confident about, but is simply incorrect,\nlike factually incorrect",
    "start": "2629860",
    "end": "2634869"
  },
  {
    "text": "and does not make any sense. We can potentially enhance\nthis through different ways. Maybe some sort of\ninternal-based fact verification",
    "start": "2634870",
    "end": "2641800"
  },
  {
    "text": "approach based on\nconfidence scores. There's this line of work\ncalled model calibration, which kind of works on\nthat, potentially verifying",
    "start": "2641800",
    "end": "2650830"
  },
  {
    "text": "and regenerating output. If it finds that its\noutput is incorrect,",
    "start": "2650830",
    "end": "2656230"
  },
  {
    "text": "maybe it can be\nasked to regenerate. And of course, there's things\nlike RAG-based approaches, where",
    "start": "2656230",
    "end": "2663180"
  },
  {
    "text": "you're able to retrieve from a\nknowledge store, which is also a potential solution people\nhave investigated for reducing",
    "start": "2663180",
    "end": "2670080"
  },
  {
    "text": "this problem of hallucination. Lastly, Emily will touch upon\nsome chain of thought reasoning.",
    "start": "2670080",
    "end": "2678660"
  },
  {
    "text": "Yeah. So chain of thought\nis something I think is really\ncool because I think it combines the cognitive\nimitation and also",
    "start": "2678660",
    "end": "2689310"
  },
  {
    "text": "interpretability\nlines of research. And so chain of\nthought is the idea that all of us, unless you have\nsome extraordinary photographic",
    "start": "2689310",
    "end": "2698070"
  },
  {
    "text": "memory, think through\nthings step by step. If I asked you to multiply a\n10 digit number by a 10 digit",
    "start": "2698070",
    "end": "2704700"
  },
  {
    "text": "number, you'd probably\nhave to break that down into intermediate\nreasoning steps. And so some researchers\nthought, well,",
    "start": "2704700",
    "end": "2711329"
  },
  {
    "text": "what if we do the same things\nwith large language models and see if forcing\nthem to reason",
    "start": "2711330",
    "end": "2717300"
  },
  {
    "text": "through their ideas and\ntheir thoughts helps them have better accuracy\nand better results.",
    "start": "2717300",
    "end": "2722640"
  },
  {
    "text": "And so chain of thought\nexploits the idea that ultimately these models\nhave these weights that",
    "start": "2722640",
    "end": "2729510"
  },
  {
    "text": "know more about a\nproblem rather than just having it prompt and regurgitate\nwhat just to get a response.",
    "start": "2729510",
    "end": "2739360"
  },
  {
    "text": "And so an example of\nchain of thought reasoning is on the right. So as you can see on the left,\nthere's standard prompting.",
    "start": "2739360",
    "end": "2746480"
  },
  {
    "text": "So I give you this\ncomplicated question. Let's say we're doing\nthis entirely new problem.",
    "start": "2746480",
    "end": "2751490"
  },
  {
    "text": "I give you the question and\nI just give you the answer, I don't tell you how to do it. That's kind of difficult.",
    "start": "2751490",
    "end": "2757840"
  },
  {
    "text": "Versus chain of thought. The first example that\nyou get, I actually walk you through the answer.",
    "start": "2757840",
    "end": "2764020"
  },
  {
    "text": "And then the idea\nis that hopefully, since you have this framework of\nhow to think about a question,",
    "start": "2764020",
    "end": "2769540"
  },
  {
    "text": "you're able to produce\na more accurate output. And so chain of thought\nresulted in pretty significant",
    "start": "2769540",
    "end": "2777760"
  },
  {
    "text": "performance gains for\nlarger language models. Similarly to what I\ntouched upon before,",
    "start": "2777760",
    "end": "2783340"
  },
  {
    "text": "this is an emergent ability. And so we don't really\nsee the same performance",
    "start": "2783340",
    "end": "2788650"
  },
  {
    "text": "for smaller models. But something that I\nthink is important, as I mentioned before, is\nthis idea of interpretability.",
    "start": "2788650",
    "end": "2795339"
  },
  {
    "text": "Because we can see\nthis model's output as their reasoning and\ntheir final answer,",
    "start": "2795340",
    "end": "2800740"
  },
  {
    "text": "then you can kind\nof see, oh, hey, this is where they\nmessed up, this is where they got\nsomething incorrect. And so we're able to break down\nthe errors of chain of thought",
    "start": "2800740",
    "end": "2808150"
  },
  {
    "text": "into these different categories\nthat helps us better pinpoint, why is it doing\nthis incorrectly, how can we directly\ntarget these issues.",
    "start": "2808150",
    "end": "2817230"
  },
  {
    "text": "And so currently,\nchain of thought works really effectively for\nmodels of approximately 100",
    "start": "2817230",
    "end": "2822640"
  },
  {
    "text": "billion parameters or more. Obviously, very big. ",
    "start": "2822640",
    "end": "2827849"
  },
  {
    "text": "And so why is that? An initial paper found that\none step missing and semantic",
    "start": "2827850",
    "end": "2833580"
  },
  {
    "text": "understanding chain\nof thought errors are the most common\namong smaller models. So you can think\nof, oh, I forgot",
    "start": "2833580",
    "end": "2839850"
  },
  {
    "text": "to do this step in\nthe multiplication. Or I actually don't really\nunderstand what multiplication is to begin with.",
    "start": "2839850",
    "end": "2845099"
  },
  {
    "text": "And so some potential reasons is\nthat maybe smaller models fail at even relatively easy\nsimple mapping tasks,",
    "start": "2845100",
    "end": "2851580"
  },
  {
    "text": "they seem to have inherently\nweaker arithmetic abilities, and maybe they have\nlogical loopholes",
    "start": "2851580",
    "end": "2856800"
  },
  {
    "text": "and don't end up coming\nat a final answer. So all your reasoning is\ncorrect, but for some reason,",
    "start": "2856800",
    "end": "2862200"
  },
  {
    "text": "you just couldn't\nget quite there. And so an interesting\nline of research would be to improve chain of\nthought for smaller models,",
    "start": "2862200",
    "end": "2868890"
  },
  {
    "text": "and similarly allow more people\nto work on interesting problems.",
    "start": "2868890",
    "end": "2875789"
  },
  {
    "text": "And so how could we\npotentially do that? Well, one idea is to generalize\nthis chain of thought reasoning.",
    "start": "2875790",
    "end": "2883450"
  },
  {
    "text": "So it's not necessarily that\nwe reason in all the same ways. Like there are multiple ways\nto think through a problem",
    "start": "2883450",
    "end": "2889260"
  },
  {
    "text": "rather than breaking\nit down step by step. And so we can perhaps\ngeneralize chain of thought",
    "start": "2889260",
    "end": "2896490"
  },
  {
    "text": "to be more flexible\nin different ways. ",
    "start": "2896490",
    "end": "2901589"
  },
  {
    "text": "One example is this\ntree of thoughts idea. And so tree of\nthoughts is considering",
    "start": "2901590",
    "end": "2907710"
  },
  {
    "text": "multiple different\nreasoning paths, and evaluating their\nchoices to decide the next course of action.",
    "start": "2907710",
    "end": "2913480"
  },
  {
    "text": "And so this is\nsimilar to the idea that we can look ahead\nand go backwards, similar to a lot of the model\narchitectures that we've seen.",
    "start": "2913480",
    "end": "2921210"
  },
  {
    "text": "And so just having\nmultiple options and being able to come out\nwith some more accurate output",
    "start": "2921210",
    "end": "2927570"
  },
  {
    "text": "at the end. Another idea is\nSocratic questioning.",
    "start": "2927570",
    "end": "2932619"
  },
  {
    "text": "So the idea that we are\ndividing and conquering in order to have this\nself-questioning,",
    "start": "2932620",
    "end": "2938740"
  },
  {
    "text": "self-reflection idea\nthat Steven touched upon.",
    "start": "2938740",
    "end": "2944240"
  },
  {
    "text": "So the idea is a\nself-questioning module using a large scale\nlanguage model to propose these\nsubproblems related",
    "start": "2944240",
    "end": "2951309"
  },
  {
    "text": "to the original problem\nthat recursively backtracks and answers the subproblem\nto the original problem.",
    "start": "2951310",
    "end": "2957020"
  },
  {
    "text": "So this is similar\nto that initial idea of chain of thought, except\nrather than spelling out all the steps for you.",
    "start": "2957020",
    "end": "2962799"
  },
  {
    "text": "The language model\nreflects on how can it break down\nthese problems, how can it answer these problems\nand get to the final answer.",
    "start": "2962800",
    "end": "2969780"
  },
  {
    "text": " Cool.",
    "start": "2969780",
    "end": "2975890"
  },
  {
    "text": "OK, let's see. So let's go to some of the\nmore interesting topics that are starting to become\nrelevant, especially in 2024.",
    "start": "2975890",
    "end": "2984110"
  },
  {
    "text": "So last year, we saw a big\nexplosion in language models, especially with GPT-4 that\ncame out almost a year ago now.",
    "start": "2984110",
    "end": "2990530"
  },
  {
    "text": "And now what's happening is\nwe are starting to transition towards more AI agents. And it's very interesting to\nsee what differentiates an agent",
    "start": "2990530",
    "end": "2998840"
  },
  {
    "text": "from something like a model. So I probably talk about a\nbunch of different things,",
    "start": "2998840",
    "end": "3004450"
  },
  {
    "text": "such as actions, long-term\nmemory, communication, bunch of stuff.",
    "start": "3004450",
    "end": "3009580"
  },
  {
    "text": "But let's start by, why\nshould we go and build agents? ",
    "start": "3009580",
    "end": "3015790"
  },
  {
    "text": "Think about that. So one key hypothesis\nor I will say here is--",
    "start": "3015790",
    "end": "3022120"
  },
  {
    "text": "what's going to happen is\nhumans will communicate with AI using natural\nlanguage, and AI",
    "start": "3022120",
    "end": "3028150"
  },
  {
    "text": "will be operating our machines. Thus allowing for more intuitive\nand efficient operations.",
    "start": "3028150",
    "end": "3033859"
  },
  {
    "text": "So if you think about a laptop. If you show a laptop to\nsomeone who's maybe a kid,",
    "start": "3033860",
    "end": "3040099"
  },
  {
    "text": "or who has never used a computer\nbefore, they'll be like, OK, why do I have to use this box? Why can't I just talk to it?",
    "start": "3040100",
    "end": "3046537"
  },
  {
    "text": "Why can't it be more human-like? I can just ask you to do\nthings and you can just go do my work for me. And that seems to be the\nmore human-like interface",
    "start": "3046537",
    "end": "3055683"
  },
  {
    "text": "to how things should happen. And I think that's the way the\nworld will transition towards, where instead of us\nclicking or typing,",
    "start": "3055683",
    "end": "3061700"
  },
  {
    "text": "we talk to an AI using\nnatural language how you talk to a human, and the\nAI will go and do your work.",
    "start": "3061700",
    "end": "3069407"
  },
  {
    "text": "Actually, I have a\nblog on this, which is called Software 3.0 if\nyou want to check that out. But yeah, cool.",
    "start": "3069407",
    "end": "3075770"
  },
  {
    "text": "So for agents, why? Do you want agents?",
    "start": "3075770",
    "end": "3081590"
  },
  {
    "text": "So as it turns out, a single\ncall to a larger foundation AI model is usually not enough.",
    "start": "3081590",
    "end": "3087970"
  },
  {
    "text": "You can do a lot more\nby building systems. And by systems, you mean\ndoing more things, like model",
    "start": "3087970",
    "end": "3093609"
  },
  {
    "text": "chaining, model reflection,\nother mechanisms. And this requires a\nlot of different stuff.",
    "start": "3093610",
    "end": "3099200"
  },
  {
    "text": "So you require memory, you\nrequire large context lengths, you also want to\ndo personalization,",
    "start": "3099200",
    "end": "3104480"
  },
  {
    "text": "you want to be\nable to do actions, you want to be able\nto do internet access.",
    "start": "3104480",
    "end": "3109508"
  },
  {
    "text": "And then you can accomplish\na lot of these things with this kind of agents. Here's a diagram breaking\ndown the different parts",
    "start": "3109508",
    "end": "3116740"
  },
  {
    "text": "of the agents. This is from Lillian Weng. She's a senior\nresearcher at OpenAI.",
    "start": "3116740",
    "end": "3122200"
  },
  {
    "text": "So if you want to build\nreally powerful agents, you need to really just think\nof that as you're building this a new kind of\ncomputer, which has",
    "start": "3122200",
    "end": "3129380"
  },
  {
    "text": "all this different\ningredients to build. You have to build\nmemory, and you have to think about\nmemory from scratch.",
    "start": "3129380",
    "end": "3135080"
  },
  {
    "text": "How do you do long-term memory? How do you do short-term memory? How do you do planning? How do we think\nabout reflection?",
    "start": "3135080",
    "end": "3140690"
  },
  {
    "text": "If something goes wrong,\nhow do you correct that? How do you have\nchain of thoughts? How do you decompose a goal?",
    "start": "3140690",
    "end": "3146275"
  },
  {
    "text": "So if I say something like\nbook me a trip to Italy, for example, how do you\nbreak that down to sub goals, for example, for the agent?",
    "start": "3146275",
    "end": "3152839"
  },
  {
    "text": "And also being able to take\nall this planning and all this stuffs into actual action.",
    "start": "3152840",
    "end": "3158305"
  },
  {
    "text": "So that becomes\nreally important. And enable all of\nthat using tool use. So if you have say like\ncalculators, or calendars,",
    "start": "3158305",
    "end": "3165710"
  },
  {
    "text": "or code interpreters, and so on. So you want to be able to\nutilize existing tools that are out there.",
    "start": "3165710",
    "end": "3171029"
  },
  {
    "text": "This is similar to how we as\na human, use a calculator, for example. So we also want AI to be\nable to use existing tools",
    "start": "3171030",
    "end": "3178580"
  },
  {
    "text": "and become more\nefficient and powerful.  This was actually\none of the demos.",
    "start": "3178580",
    "end": "3185345"
  },
  {
    "text": "This is actually\nfrom my company. But this was one of the\nfirst demonstrations of agents in the real\nworld, where we actually",
    "start": "3185345",
    "end": "3191130"
  },
  {
    "text": "had it passed the online\ndriving test in California. So this was actually like a live\nexam we took as a demonstration.",
    "start": "3191130",
    "end": "3197940"
  },
  {
    "text": "So this was a friends\nor a driving test, which can actually\ntake from your home.",
    "start": "3197940",
    "end": "3203170"
  },
  {
    "text": "And so the person had their\nhands up above the keyboard, and they were being\nrecorded on the webcam.",
    "start": "3203170",
    "end": "3209700"
  },
  {
    "text": "There was also like\na screen recorded. And the DMV actually\nhad the person installed like a special software\nin the computer to detect,",
    "start": "3209700",
    "end": "3215820"
  },
  {
    "text": "it's not a bot. But still the agent could\nactually go and complete the exam. So that was interesting to see.",
    "start": "3215820",
    "end": "3221590"
  },
  {
    "text": "[LAUGHS] So we set the\nrecord in this case to be the first\nAI to actually get",
    "start": "3221590",
    "end": "3226799"
  },
  {
    "text": "a driving permit in California. And this is the agent actually\ngoing and doing things.",
    "start": "3226800",
    "end": "3232110"
  },
  {
    "text": "So here the person\nhas their hands just above the keyboard\nfor the webcam.",
    "start": "3232110",
    "end": "3238050"
  },
  {
    "text": "And the agent is\nrunning on the laptop and it's answering\nall the questions. So all of this is happening\nautonomously in this case.",
    "start": "3238050",
    "end": "3245359"
  },
  {
    "text": "And so this was roughly\naround 40 questions. The agent maybe made like\ntwo or three mistakes, but was able to\nsuccessfully pass",
    "start": "3245360",
    "end": "3250835"
  },
  {
    "text": "the whole test in this case. So this was really fun. Let me go to the end.",
    "start": "3250835",
    "end": "3257020"
  },
  {
    "text": "Yeah. [LAUGHS] So yeah.",
    "start": "3257020",
    "end": "3262380"
  },
  {
    "text": "So you can imagine,\nthere's a lot of fun things that can happen with agents. This was actually\na tech attempt. So we informed the DMV\nafter we took the exam.",
    "start": "3262380",
    "end": "3269570"
  },
  {
    "text": "So this was really funny. But you can imagine, there's\nso many different things you can enable once you have\nthis capabilities that are",
    "start": "3269570",
    "end": "3277910"
  },
  {
    "text": "available for everyone to use. And this becomes a\nquestion of, why should we",
    "start": "3277910",
    "end": "3286590"
  },
  {
    "text": "build more human-like agents? And I would say this\nis very interesting because it's almost like\nsaying, why should we",
    "start": "3286590",
    "end": "3293520"
  },
  {
    "text": "build humanoid robots? Why can't we just build a\ndifferent kind of robot? Why do you want humanoid robots?",
    "start": "3293520",
    "end": "3299170"
  },
  {
    "text": "And it's similar question\nhere, why do you want, human-like agents? And obviously, this\nis very interesting",
    "start": "3299170",
    "end": "3305190"
  },
  {
    "text": "because a lot of the technology\nwebsite is built for humans. And then we can go and\nreuse that infrastructure",
    "start": "3305190",
    "end": "3311190"
  },
  {
    "text": "instead of building new things. And so that becomes\nvery interesting because you can just deploy\nthese agents using the existing",
    "start": "3311190",
    "end": "3316680"
  },
  {
    "text": "technology. Second is you can imagine this\nagents could become almost like a digital extension of you. So they can learn about you,\nthey can know your preferences,",
    "start": "3316680",
    "end": "3323963"
  },
  {
    "text": "they can know what you\nlike, what you don't like, and be able to act\non your behalf. They also have very less\nrestrictive boundaries,",
    "start": "3323963",
    "end": "3331050"
  },
  {
    "text": "so they're able to handle\nsay, like logins, payments, and so on, which might be\nharder with things like API for example.",
    "start": "3331050",
    "end": "3336720"
  },
  {
    "text": "But this is easier to do if you\nare doing more computer-based control, like a human.",
    "start": "3336720",
    "end": "3343240"
  },
  {
    "text": "You can imagine the problem\nis also fundamentally simpler, because you just have an action\nspace like clicking and typing,",
    "start": "3343240",
    "end": "3349270"
  },
  {
    "text": "which itself is a fundamentally\nlimited action space. So that's a simpler\nproblem to solve",
    "start": "3349270",
    "end": "3354831"
  },
  {
    "text": "rather than building\nsomething that is maybe more general purpose.",
    "start": "3354832",
    "end": "3361380"
  },
  {
    "text": "And another interesting\nthing about this kind of human-like agents is\nyou can also teach them. So you can teach them\nhow you will do things,",
    "start": "3361380",
    "end": "3367990"
  },
  {
    "text": "they can maybe\nrecord you passively. And they can learn from\nyou and then improve. And this is also becomes\nan interesting way",
    "start": "3367990",
    "end": "3373829"
  },
  {
    "text": "to improve this\nagents over time.  So when we talk\nabout agents, there's",
    "start": "3373830",
    "end": "3380540"
  },
  {
    "text": "this map that\npeople like to use, which is called the 5\ndifferent levels of autonomy. This actually came\nfrom self-driving cars.",
    "start": "3380540",
    "end": "3387090"
  },
  {
    "text": "So how this works is\nyou have L0 to L5. So L0 to L2 is the\nparts of autonomy",
    "start": "3387090",
    "end": "3394910"
  },
  {
    "text": "where the human is in control. So here is the human\nis driving the car. And there might be some partial\nautomation that's happening,",
    "start": "3394910",
    "end": "3401670"
  },
  {
    "text": "which could be some\nauto assist features.",
    "start": "3401670",
    "end": "3406760"
  },
  {
    "text": "This starts becoming interesting\nwhen you have something L3. So in L3, you still\nhave a human in the car,",
    "start": "3406760",
    "end": "3412550"
  },
  {
    "text": "but most of the time, the car is\nable to drive itself on highways",
    "start": "3412550",
    "end": "3417800"
  },
  {
    "text": "or most of the roads. L4 is you still have\na human, but the car",
    "start": "3417800",
    "end": "3422960"
  },
  {
    "text": "is doing all the driving. And this is maybe if you have\ndriven a Tesla on autopilot",
    "start": "3422960",
    "end": "3428270"
  },
  {
    "text": "before, that's like an\nL4 autonomous vehicle. And L5 is basically you don't\nhave a driver in the car.",
    "start": "3428270",
    "end": "3434990"
  },
  {
    "text": "So the car is able to go and\nhandle all parts of the system, there's no fallback.",
    "start": "3434990",
    "end": "3440680"
  },
  {
    "text": "And this is what like\nWaymo is doing right now. So if you take a self-driving,\nif you sit-in a Waymo in SF,",
    "start": "3440680",
    "end": "3448590"
  },
  {
    "text": "then you can experience\na L5 autonomy car, where there's\nno human and the AI is driving the whole car itself.",
    "start": "3448590",
    "end": "3454320"
  },
  {
    "text": "And so same thing also\napplies for agents. So you can almost imagine if\nyou are building something",
    "start": "3454320",
    "end": "3460140"
  },
  {
    "text": "like an L4 level\ncapability, that's where a human is\nstill in the loop and showing that\nnothing is going wrong. And so this is you still\nhave some bottlenecks.",
    "start": "3460140",
    "end": "3468000"
  },
  {
    "text": "But if you are able to reach\nL5 level of autonomy on agents. And that's basically saying\nif you ask an agent to book",
    "start": "3468000",
    "end": "3473280"
  },
  {
    "text": "a flight and that happens. You ask it for maybe like,\ngo, maybe order this for me. Or maybe go whatever\nthings you care about.",
    "start": "3473280",
    "end": "3481320"
  },
  {
    "text": "And that can all\nhappen autonomously. So that's where things start\nto become very interesting when we can start reaching\nfrom L4 to L5",
    "start": "3481320",
    "end": "3488220"
  },
  {
    "text": "and don't even need a\nhuman in the loop anymore. Cool.",
    "start": "3488220",
    "end": "3495170"
  },
  {
    "text": "So when you think\nabout building agents, this predominantly two routes. So the first one is API, where\nyou can go and control anything",
    "start": "3495170",
    "end": "3505000"
  },
  {
    "text": "based on APIs that are\navailable out there. So OpenAI has been trying\nthis with ChatGPT Plugins,",
    "start": "3505000",
    "end": "3510490"
  },
  {
    "text": "for example. There's also a bunch\nof work from Berkeley. So Berkeley had this\nwork called Gorilla,",
    "start": "3510490",
    "end": "3516370"
  },
  {
    "text": "where you could train a\nfoundation model to control 10,000 APIs.",
    "start": "3516370",
    "end": "3521560"
  },
  {
    "text": "And there's a lot of interesting\nstuff happening here. A second direction of work is\nmore like direct interaction",
    "start": "3521560",
    "end": "3526990"
  },
  {
    "text": "with the computer. And there's different\ncompanies trying this out. So we have one of that. There's also this\nstartup called Adept",
    "start": "3526990",
    "end": "3533980"
  },
  {
    "text": "which is trying this\nhuman-like interaction. ",
    "start": "3533980",
    "end": "3540690"
  },
  {
    "text": "Yeah, maybe I can\nshow this thing. So this is an idea of what you\ncan enable by having agents.",
    "start": "3540690",
    "end": "3547380"
  },
  {
    "text": "So what we are doing here is-- here we have our\nagent and we told",
    "start": "3547380",
    "end": "3553110"
  },
  {
    "text": "it to go to Twitter\nand make a post. And so it's going and\ncontrolling the computer",
    "start": "3553110",
    "end": "3560340"
  },
  {
    "text": "during this whole interaction. And once it's done, it can\nsend me a response back,",
    "start": "3560340",
    "end": "3568710"
  },
  {
    "text": "which you can see here. And so this becomes interesting\nbecause you don't really need APIs if you have\nthis kind of agents.",
    "start": "3568710",
    "end": "3574410"
  },
  {
    "text": "So if I have an agent that\ncan go control my computer, can go control websites,\ncan do whatever it wants, almost in a human-like\nmanner, like what you can do,",
    "start": "3574410",
    "end": "3581280"
  },
  {
    "text": "then you don't really\nneed APIs because this becomes like the abstraction\nlayer to allow any control.",
    "start": "3581280",
    "end": "3588930"
  },
  {
    "text": "So it's going to be\nreally fascinating once we have this kind of agent\nstart to work in the real world, and a lot of transitions\nwe'll see in technology.",
    "start": "3588930",
    "end": "3596260"
  },
  {
    "start": "3596260",
    "end": "3606290"
  },
  {
    "text": "Cool. OK, so let's move on to the next\ntopic when it comes to agents. So one very interesting\nthing here is memory.",
    "start": "3606290",
    "end": "3615590"
  },
  {
    "text": "So, yeah, so we'll say a good\nway to think about a model is almost think of it\nlike a compute chip.",
    "start": "3615590",
    "end": "3621930"
  },
  {
    "text": "So what happens is you have\nsome sort of input tokens, which are defined in natural language,\nwhich are going as the input",
    "start": "3621930",
    "end": "3627980"
  },
  {
    "text": "to a model. And then you get some\noutput tokens out. And the output tokens are,\nagain, natural language.",
    "start": "3627980",
    "end": "3633470"
  },
  {
    "text": "And if you have\nsomething like a GPT-3.5, that used to be something\nlike a 8,000 length token.",
    "start": "3633470",
    "end": "3639200"
  },
  {
    "text": "With GPT-4, this became like a\n16,000 [INAUDIBLE],, one 28,000. So you can almost imagine\nthis as the token size",
    "start": "3639200",
    "end": "3646790"
  },
  {
    "text": "or the instruction size of\nthis compute unit, which is powered by a neural\nnetwork in this case.",
    "start": "3646790",
    "end": "3654830"
  },
  {
    "text": "And so this is basically what\nGPT-4, you can imagine it. It's almost like a CPU and\nit's taking some input tokens,",
    "start": "3654830",
    "end": "3661280"
  },
  {
    "text": "defined a natural\nlanguage, doing some computation over them,\ntransforming those tokens and giving out\nsome output tokens.",
    "start": "3661280",
    "end": "3667910"
  },
  {
    "text": "This is actually similar to how\nyou think about memory chips, for example. So here I'm showing\na MIPS32 processor.",
    "start": "3667910",
    "end": "3675670"
  },
  {
    "text": "That's one of the earliest\nprocessors out there. And so what it's doing is you\nhave input tokens and output tokens in binary,\nlike 0s and 1s.",
    "start": "3675670",
    "end": "3682319"
  },
  {
    "text": "But [INAUDIBLE] of\nthat, you can imagine we are doing very\nsimilar things, but just over\nnatural language now.",
    "start": "3682320",
    "end": "3687350"
  },
  {
    "text": "And now, if you think\nmore about this analogy. So you can start\nthinking, OK, what",
    "start": "3687350",
    "end": "3692450"
  },
  {
    "text": "we want to do is\ntake whatever we have been doing in building\ncomputers, and CPUs, and logic, and so on.",
    "start": "3692450",
    "end": "3697829"
  },
  {
    "text": "But can we generalize all\nof that to natural language? So you can start thinking about,\nhow current processors work?",
    "start": "3697830",
    "end": "3703430"
  },
  {
    "text": "How current computer chips work? You have instructions, you have\nmemory, you have variables.",
    "start": "3703430",
    "end": "3710660"
  },
  {
    "text": "And then you run this\nover and over to each line of binary sequence of\ninstructions to output code.",
    "start": "3710660",
    "end": "3718895"
  },
  {
    "text": "And you can start thinking about\ntransformers in a similar way, where you can have\nthe transformer acting as a compute unit,\nyou are passing",
    "start": "3718895",
    "end": "3725180"
  },
  {
    "text": "it some sort of\ninstructions line by line. And each instruction can\ncontain some primitives",
    "start": "3725180",
    "end": "3730682"
  },
  {
    "text": "which are defining\nlike what to do, which could be the user command. It could have some\nmemory parts, which are retrieved from external,\nlike a disk, which in this case",
    "start": "3730682",
    "end": "3737622"
  },
  {
    "text": "could be like something like\na personalization system and so on, as well as\nsome sort of variables. And then you're taking this\nand running this line by line.",
    "start": "3737622",
    "end": "3744467"
  },
  {
    "text": "And that's a pretty good way\nto think about what something like this could be doing. And you can almost imagine\nthere could be new programming",
    "start": "3744467",
    "end": "3751685"
  },
  {
    "text": "languages you can build, which\nare specific to programming like transformers.",
    "start": "3751685",
    "end": "3756826"
  },
  {
    "text": " And so when it comes to\nmemory, traditionally,",
    "start": "3756826",
    "end": "3761970"
  },
  {
    "text": "how we think about\nmemory is like a disk. So it's like long-lived,\nit's persistent. When the computer shuts\ndown, you save all your data",
    "start": "3761970",
    "end": "3769080"
  },
  {
    "text": "from the RAM to the disk,\nand then you can persist it and then you can load\nit back when you want.",
    "start": "3769080",
    "end": "3775080"
  },
  {
    "text": "You want to enable something\nvery similar when you have AI and you have agents. And so you want\nto have mechanisms",
    "start": "3775080",
    "end": "3781109"
  },
  {
    "text": "where I can store this data,\nand then retrieve this data. And right now how we're doing\nthis is like through embeddings.",
    "start": "3781110",
    "end": "3787590"
  },
  {
    "text": "So you can take any PDF or\nany modality you care about,",
    "start": "3787590",
    "end": "3793980"
  },
  {
    "text": "convert that to embeddings\nusing an embedding model. And store that embedding\nin a vector database.",
    "start": "3793980",
    "end": "3799890"
  },
  {
    "text": "And later when we actually care\nabout doing any sort of access to the memory, you can load the\nrelevant part of the embedding,",
    "start": "3799890",
    "end": "3807630"
  },
  {
    "text": "put that in as part\nof your instruction and feed that to the model. And so this is how we\nare thinking about memory",
    "start": "3807630",
    "end": "3814530"
  },
  {
    "text": "with AI, this space. And then you essentially\nhave the retrieval models, which are acting as a function\nto store and retrieve memory.",
    "start": "3814530",
    "end": "3821770"
  },
  {
    "text": "And the embeddings becomes\nthe layer of the-- it's basically the format\nyou're using to encode",
    "start": "3821770",
    "end": "3827770"
  },
  {
    "text": "the memory in this case. There's still a lot\nof open questions because how this works right\nnow is you just do simple k-NN.",
    "start": "3827770",
    "end": "3835130"
  },
  {
    "text": "So it's very simple nearest\nneighbor search, which is not efficient, it\ndoesn't really generalize, doesn't scale.",
    "start": "3835130",
    "end": "3840470"
  },
  {
    "text": "And so there's a\nlot of things you can think about, especially\nhierarchy, temporal coherence. Because a lot of memory\ndata is like a time series.",
    "start": "3840470",
    "end": "3848900"
  },
  {
    "text": "So there's a lot\nof temporal parts. There's also a lot of structure,\nusually in a lot of data. You could use a structure,\nit could be like a graph,",
    "start": "3848900",
    "end": "3855490"
  },
  {
    "text": "for example. And there's also a lot of things\nyou can do on online adaptation. Because like most\n[? chair data ?] is not static.",
    "start": "3855490",
    "end": "3861852"
  },
  {
    "text": "You're always learning,\nyou're always adapting, there's things\nchanging all the time. And a good model over here is\nmaybe how the human brain works.",
    "start": "3861852",
    "end": "3868252"
  },
  {
    "text": "So if you think about\nsomething like the hippocampus, it's like people still don't\nknow how it fully works, but it's something you're\nlearning new things on the fly,",
    "start": "3868252",
    "end": "3875832"
  },
  {
    "text": "you're creating new memories,\nyou're adapting new memories, and so on. And so I think it'll be\nvery fascinating to see",
    "start": "3875832",
    "end": "3880948"
  },
  {
    "text": "how this area of research\nevolves over time. ",
    "start": "3880948",
    "end": "3886425"
  },
  {
    "text": "Similarly, like a very\nrelevant problem with memory is personalization. Suppose now you have this agents\nthat are doing things for you.",
    "start": "3886425",
    "end": "3893830"
  },
  {
    "text": "Then you want to make sure that\nthe agent actually knows what you like, What you don't like. Suppose you tell an agent to\ngo book your $1,000 flight,",
    "start": "3893830",
    "end": "3899800"
  },
  {
    "text": "but maybe it books you the wrong\nflight and just wastes a lot of your money. Or maybe it does a lot of wrong\nactions, which is not good.",
    "start": "3899800",
    "end": "3908390"
  },
  {
    "text": "So you want the agent\nto learn about what you like and understand that.",
    "start": "3908390",
    "end": "3914839"
  },
  {
    "text": "And this becomes about\nforming a long-lived user memory, for example, where\nthe more you interact with it,",
    "start": "3914840",
    "end": "3920180"
  },
  {
    "text": "the more it should\nform a memory about you and be able to use that.",
    "start": "3920180",
    "end": "3925220"
  },
  {
    "text": "This could have\ndifferent flavors. So one could be like explicit\nwhere you can tell it like, OK, here's my allergies,\nhere's my flight preferences,",
    "start": "3925220",
    "end": "3931895"
  },
  {
    "text": "I like window\nversus aisle seats, here's my favorite\ndishes, and so on. But this could also be\nimplicit, where it could be say,",
    "start": "3931895",
    "end": "3937819"
  },
  {
    "text": "maybe I like Adidas over Nike. Or if I'm on Amazon and if I\nhave this 10 different shirts I",
    "start": "3937820",
    "end": "3943793"
  },
  {
    "text": "can buy, maybe I will\nbuy this particular type of shirt and brand and so on. And so there's also a lot of\nimplicit learning you can do,",
    "start": "3943793",
    "end": "3949427"
  },
  {
    "text": "which is more based on\nfeedback or comparisons.",
    "start": "3949427",
    "end": "3954810"
  },
  {
    "text": "And there's a lot of\nchallenges involved here. So you can imagine, it's how\ndo you collect this data? How do you form this memory?",
    "start": "3954810",
    "end": "3960400"
  },
  {
    "text": "How do you learn? And do you use supervised\nlearning versus feedback? How do you learn on the fly?",
    "start": "3960400",
    "end": "3968203"
  },
  {
    "text": "And while you are\ndoing all of this, how do you preserve user privacy? Because like for some,\na system to personalize.",
    "start": "3968203",
    "end": "3974820"
  },
  {
    "text": "It just needs to\nknow a lot about you. But then how do you\nensure that if you're building systems like that,\nthat this is actually safe?",
    "start": "3974820",
    "end": "3980910"
  },
  {
    "text": "And it's not violating\nany of your privacy. ",
    "start": "3980910",
    "end": "3989369"
  },
  {
    "text": "An interesting area\nwhen it comes to agents is also communication. So now you can imagine, suppose\nyou have this one agent that",
    "start": "3989370",
    "end": "3996540"
  },
  {
    "text": "can go and do things for you. But why not have\nmultiple agents? ",
    "start": "3996540",
    "end": "4003052"
  },
  {
    "text": "If I have an agent\nand you have an agent. And this agent starts\ncommunicating with each other. And so I think we'll start\nseeing this phenomena",
    "start": "4003052",
    "end": "4008600"
  },
  {
    "text": "where you will have multi-agent\nautonomous systems, where each agent can go and\ndo things, and then that agent can go and talk\nto other agents and so on.",
    "start": "4008600",
    "end": "4015950"
  },
  {
    "text": "And so that's going\nto be fascinating. ",
    "start": "4015950",
    "end": "4021620"
  },
  {
    "text": "Why do you want to do this? So one is if you have a single\nagent, it will always be slow. It has to do everything\nsequentially.",
    "start": "4021620",
    "end": "4028298"
  },
  {
    "text": "But if you have a\nmulti-agent system, then you can\nparalyze the system. So instead of one agent,\nI had thousands of agents.",
    "start": "4028298",
    "end": "4034277"
  },
  {
    "text": "Agent can go do something\nfor me in parallel instead of just having one. Second is you can also\nhave spatialized agents.",
    "start": "4034277",
    "end": "4040079"
  },
  {
    "text": "So I could have an agent that's\nspecifically for spreadsheets, or I have an agent that\ncan operate my Slack,",
    "start": "4040080",
    "end": "4045539"
  },
  {
    "text": "I have an agent that can\noperate my web browser. And then I can route\nto different agents to different things\nI want to do.",
    "start": "4045540",
    "end": "4051430"
  },
  {
    "text": "And that can help. Like this is almost what\nyou do in a factory. You have spatialized\nworkers, each worker",
    "start": "4051430",
    "end": "4057990"
  },
  {
    "text": "is doing something\nthey're specialized to. And this actually is\nsomething that we found over like the period\nof human history,",
    "start": "4057990",
    "end": "4064380"
  },
  {
    "text": "that this is the right way\nto divide tasks and get the maximum performance.",
    "start": "4064380",
    "end": "4071150"
  },
  {
    "text": "There's a lot of\nchallenges here, too. The biggest one is just how\ndo you exchange information. Because now what's\nhappening is like everything",
    "start": "4071150",
    "end": "4077480"
  },
  {
    "text": "is happening over\nnatural language. And natural language\nitself is lossy. So it's very easy to have\nmiscommunication gaps.",
    "start": "4077480",
    "end": "4083839"
  },
  {
    "text": "Like even when humans\ntalk to each other, there's a lot of\nmiscommunication. You lose information a lot,\nbecause natural language",
    "start": "4083840",
    "end": "4090589"
  },
  {
    "text": "is ambiguous. So you just need to have better\nprotocols or a better ways to ensure that if agents are\ncommunicating with other agents,",
    "start": "4090590",
    "end": "4097100"
  },
  {
    "text": "it doesn't cause mistakes,\nit doesn't lead to a havoc, for example.",
    "start": "4097100",
    "end": "4102799"
  },
  {
    "text": "This can also lead to\nbuilding different interesting primitives. So here's one example primitive\nyou can think about is like,",
    "start": "4102800",
    "end": "4109520"
  },
  {
    "text": "suppose what if I\nhave a manager agent? And the manager agent\ncan go and coordinate a bunch of worker agents.",
    "start": "4109520",
    "end": "4115080"
  },
  {
    "text": "And this is very similar to a\nhuman organization, for example. So you can have this hierarchy\nwhere like, OK, if I'm a user,",
    "start": "4115080",
    "end": "4122210"
  },
  {
    "text": "I'm talking to this one main\nagent, but behind the scene, this agent is going and talking\nto its own worker agents",
    "start": "4122210",
    "end": "4127818"
  },
  {
    "text": "and showing that each worker\ngoes and does the task. And once everything\nis done, then the manager agent comes\nback to me and says,",
    "start": "4127819",
    "end": "4133350"
  },
  {
    "text": "OK, this thing is done. And so you can\nimagine, there's a lot of this primitives\nthat can be built. A good way to also think\nabout this is almost",
    "start": "4133350",
    "end": "4140009"
  },
  {
    "text": "like a single-core machine\nversus a multi-core machine. So when you have a\nsingle agent, it's almost like saying I have\na single processor that's",
    "start": "4140010",
    "end": "4146670"
  },
  {
    "text": "powering my computer. But now if I have\nmultiple agents, I have this maybe a\n16-core or 64-core machine,",
    "start": "4146670",
    "end": "4153390"
  },
  {
    "text": "where a lot of these things can\nbe routed to different agents, paralyzed. And I think that that's a\nvery interesting analogy when",
    "start": "4153390",
    "end": "4159750"
  },
  {
    "text": "it comes to a lot of\nthis multi-agent systems. ",
    "start": "4159750",
    "end": "4164870"
  },
  {
    "text": "There's still a lot of\nwork that needs to be done. The biggest one is just\ncommunication is really hard.",
    "start": "4164870",
    "end": "4170299"
  },
  {
    "text": "So you need robust\ncommunication protocols to minimize miscommunication.",
    "start": "4170300",
    "end": "4175500"
  },
  {
    "text": "You might also just need\nreally good schemas. And maybe almost how\nyou have HTTP that's",
    "start": "4175500",
    "end": "4181685"
  },
  {
    "text": "used to transporting\ninformation over the internet. You might need something\nsimilar to transport information",
    "start": "4181685",
    "end": "4186979"
  },
  {
    "text": "between different agents. You can also think about\nsome primitives here. I will just walk through a\nsmall example if we have time.",
    "start": "4186979",
    "end": "4195020"
  },
  {
    "text": "Suppose this is a manager agent\nthat wants to get tasks done, so it gives a plan and a\ncontext to a worker agent.",
    "start": "4195020",
    "end": "4201320"
  },
  {
    "text": "The worker can say like,\nOK, I did this task. And then we get a response back. Then usually you want\nto actually verify",
    "start": "4201320",
    "end": "4207650"
  },
  {
    "text": "if this got done or not. Because it's possible maybe\nthe worker was lying to you, for example, maybe it\nfailed to do the task",
    "start": "4207650",
    "end": "4214130"
  },
  {
    "text": "or something went wrong. And so you want to actually go\nand verify that this actually was done properly.",
    "start": "4214130",
    "end": "4219210"
  },
  {
    "text": "And if everything was done,\nthen OK, this is good. You can tell the user this\ntask was actually finished. But what could happen is\nmaybe the worker actually",
    "start": "4219210",
    "end": "4226423"
  },
  {
    "text": "didn't do the task properly. Something went wrong. And in that case, you want to\nactually go and redo the task.",
    "start": "4226423",
    "end": "4231600"
  },
  {
    "text": "And you just have to build this\nsort of a failover mechanisms. Because otherwise, there's a\nlot of things that can go wrong.",
    "start": "4231600",
    "end": "4237400"
  },
  {
    "text": "And so thinking about\nthis syncing primitives and how can you\nensure reliability, how can you ensure fallback.",
    "start": "4237400",
    "end": "4243360"
  },
  {
    "text": "I think that becomes very\ninteresting with this kind of agent systems. And there's a lot of future\ndirections to be explored.",
    "start": "4243360",
    "end": "4251530"
  },
  {
    "text": "There's a lot of issues with\nautonomous agents still. The biggest ones are\naround reliability.",
    "start": "4251530",
    "end": "4256990"
  },
  {
    "text": "So this happens because models\nare stochastic in nature. Like if our AI model,\nit's stochastic.",
    "start": "4256990",
    "end": "4263020"
  },
  {
    "text": "It's a probabilistic function. It's not fully deterministic. And what happens is like,\nif I wanted to do something,",
    "start": "4263020",
    "end": "4269230"
  },
  {
    "text": "it's possible that with\nsome error rate [INAUDIBLE],, it will do something that\nI didn't expect it to do.",
    "start": "4269230",
    "end": "4274390"
  },
  {
    "text": "And so like it becomes\nreally hard to trust it. Because if I have traditional\ncode, I write a script and I run the script through\na bunch of test cases and unit",
    "start": "4274390",
    "end": "4281170"
  },
  {
    "text": "tests. I know, OK, if this works,\nit's going to 100% of the time. I can deploy this to millions\nor billions of people. But if our agent, it's\na stochastic function.",
    "start": "4281170",
    "end": "4288490"
  },
  {
    "text": "Maybe if it works, maybe it\nworks like 95% of the time. But 5% of the time,\nit still fails. And there's no way to\nfix this right now.",
    "start": "4288490",
    "end": "4295305"
  },
  {
    "text": "So that becomes\nvery interesting, like, how do you actually\nsolve this problems? Similarly, you see a lot of\nproblems around looping and plan",
    "start": "4295305",
    "end": "4302320"
  },
  {
    "text": "divergence. So what happens here is you need\na lot of multi-turn interactions",
    "start": "4302320",
    "end": "4307510"
  },
  {
    "text": "when it comes to agents. So you want to have the\nagent do something, then use that to do another\nthing, and so on. So take maybe hundreds\nor thousands of steps.",
    "start": "4307510",
    "end": "4314600"
  },
  {
    "text": "But if it fails in\nmaybe the 20th step, then it might just go haywire. I don't know what to do\nfor the remaining thousands",
    "start": "4314600",
    "end": "4322210"
  },
  {
    "text": "steps of its trajectory. And so how do you correct\nit, bring it back on course? I think that becomes\nan interesting problem.",
    "start": "4322210",
    "end": "4329140"
  },
  {
    "text": "Similarly, how do you test\nand benchmark these agents, especially if they're going to\nbe running in the real world? And how do you build\na lot of observability",
    "start": "4329140",
    "end": "4335409"
  },
  {
    "text": "around the systems? If I have this agent\nthat's maybe has access to my bank account, is\ndoing things for me,",
    "start": "4335410",
    "end": "4341590"
  },
  {
    "text": "how do I actually know\nit's doing safe things? Someone it's not\nhacking into it? How do I build trust?",
    "start": "4341590",
    "end": "4347680"
  },
  {
    "text": "And also how do we\nbuild human fallbacks? Like you probably want something\na 2FA if it's going to go and do",
    "start": "4347680",
    "end": "4353440"
  },
  {
    "text": "purchases for you. Or you want like some ways to\nguarantee like, OK, you just didn't wake up and didn't have\na $0 bank account or something.",
    "start": "4353440",
    "end": "4362489"
  },
  {
    "text": " And so these are\nsome of the problems that we need to solve\nfor agents for them",
    "start": "4362490",
    "end": "4368480"
  },
  {
    "text": "to become like real\nworld deployable. And this is an example\nof plan divergence problem you see with agents.",
    "start": "4368480",
    "end": "4374730"
  },
  {
    "text": "Like, if you ask the\nagent to go do something, you will usually\nexpect it to follow",
    "start": "4374730",
    "end": "4379910"
  },
  {
    "text": "a path, where it will follow\nsome ideal path to reach the goal. But what might happen is\nlike it might actually deviate from the path.",
    "start": "4379910",
    "end": "4385578"
  },
  {
    "text": "And once it deviates, it\ndoesn't know what to do. It just keeps making\nmistakes after mistakes. And this is something\nactually you",
    "start": "4385578",
    "end": "4392330"
  },
  {
    "text": "observe with early agents,\nlike AutoGPT, for example. So I'm not sure if\nanyone in this room has played with AutoGPT.",
    "start": "4392330",
    "end": "4399230"
  },
  {
    "text": "Has anyone did that? OK. And so AutoGPT, it's\na very good prototype,",
    "start": "4399230",
    "end": "4405200"
  },
  {
    "text": "but it doesn't\nactually do anything. Because it just keeps\nmaking a lot of mistakes, it keeps looping around,\nkeeps going haywire.",
    "start": "4405200",
    "end": "4410655"
  },
  {
    "text": "And that's show's\nlike, OK, you really need to have a really good ways\nto correcting agents, making",
    "start": "4410655",
    "end": "4416480"
  },
  {
    "text": "sure if it makes a mistake, it\ncan actually come back and not just go do random things. ",
    "start": "4416480",
    "end": "4424750"
  },
  {
    "text": "So building on this, there's\nalso a very good analogy from Andrej Karpathy. So he likes to call this\nthe LLM operating system.",
    "start": "4424750",
    "end": "4432565"
  },
  {
    "text": " I was talking about thing\nabout LLMs and the agents",
    "start": "4432565",
    "end": "4440079"
  },
  {
    "text": "as building computer\nchips and computers. So you can actually start\nthinking of it like that.",
    "start": "4440080",
    "end": "4445420"
  },
  {
    "text": "Here the computer\nchip is the LLM and the RAM is the context\nlength of the tokens",
    "start": "4445420",
    "end": "4451690"
  },
  {
    "text": "that you're feeding\ninto the model. Then you have this\nfile system, which is a disk, where you are\nstoring your embeddings.",
    "start": "4451690",
    "end": "4457373"
  },
  {
    "text": "You're able to retrieve\nthis embeddings. You might have traditional\nsoftware, 1.0 tools, which are like your calculator,\nyour Python interpreter,",
    "start": "4457373",
    "end": "4464980"
  },
  {
    "text": "terminal, et cetera. You actually have this. If you have an\noperating system course, there's this thing\ncalled an ALU, which",
    "start": "4464980",
    "end": "4471550"
  },
  {
    "text": "is an arithmetic\nlogical unit, which powers a lot of how you do\nmultiplications, division, so on. So it's very similar to that.",
    "start": "4471550",
    "end": "4477538"
  },
  {
    "text": "You just need tools to be\nable to do complex operations. And then you might also\nhave peripheral devices.",
    "start": "4477538",
    "end": "4485185"
  },
  {
    "text": "So you might have\ndifferent modalities, so you might have audio,\nyou might have video. You probably want to be able\nto connect to the internet,",
    "start": "4485185",
    "end": "4491700"
  },
  {
    "text": "so you want to have some sort\nof browsing capabilities. And you might also want to be\nable to talk to other LLMs.",
    "start": "4491700",
    "end": "4497179"
  },
  {
    "text": "And so this kind\nof becomes how you will think about a new\ngeneration of computers being designed with all\nthe innovations in AI.",
    "start": "4497180",
    "end": "4503090"
  },
  {
    "text": " Cool. And so I'd like to end here\nby saying how I imagine this",
    "start": "4503090",
    "end": "4511030"
  },
  {
    "text": "to look like in the future. You can think of this as a\nneural computer, where there's a user that's talking to a chat\ninterface, behind the scene,",
    "start": "4511030",
    "end": "4518710"
  },
  {
    "text": "the chat interface\nhas an action engine that can take the task route\nit to different agents,",
    "start": "4518710",
    "end": "4524020"
  },
  {
    "text": "and do the task for you, and\nsend you the results back. Cool. ",
    "start": "4524020",
    "end": "4533489"
  },
  {
    "text": "Yeah, so to end this, a\nlot of stuff that still needs to be done for agents.",
    "start": "4533490",
    "end": "4538780"
  },
  {
    "text": "And the biggest prevalent\nissues right now, I will say is error correction. So like, what happens\nif something goes wrong?",
    "start": "4538780",
    "end": "4544660"
  },
  {
    "text": "How do you prevent the\nerrors from activating in the real world? How do you build security? How do you build\nuser permissions?",
    "start": "4544660",
    "end": "4550139"
  },
  {
    "text": "What if someone tries to hijack\nyour computer or your agent? How do you build robust\nlike a security primitives?",
    "start": "4550140",
    "end": "4557400"
  },
  {
    "text": "And also, how do you\nsandbox these agents? How do you deploy them\nin risky scenarios? So if you want to\ndeploy this in finance",
    "start": "4557400",
    "end": "4562857"
  },
  {
    "text": "scenarios or legal scenarios. There's a lot of things\nwhere you just want this to be very stable and safe.",
    "start": "4562857",
    "end": "4569310"
  },
  {
    "text": "And that's like still something\nthat has not been figured out. And there's a lot of exciting\nroom to think on these problems,",
    "start": "4569310",
    "end": "4576510"
  },
  {
    "text": "both on the research side\nas well as application side. Cool.",
    "start": "4576510",
    "end": "4581960"
  },
  {
    "text": "All right. So thanks, guys, for coming to\nour first lecture this quarter. Stay back if you\nhave any questions. And we might try to\nget a group photo,",
    "start": "4581960",
    "end": "4588400"
  },
  {
    "text": "so if you want to be in\nthat, also stay back. So next week, we're going to\nhave my friend, Jason Wei,",
    "start": "4588400",
    "end": "4593520"
  },
  {
    "text": "as well as his colleague,\nKyung [? Hwan ?] from OpenAI, come give a talk. And they're doing very\ncutting edge research",
    "start": "4593520",
    "end": "4599880"
  },
  {
    "text": "involving things like large\nlanguage models at OpenAI. And he was actually\nthe first author",
    "start": "4599880",
    "end": "4605730"
  },
  {
    "text": "of several of the\nworks we talked about today, like\nchain-of-thought reasoning, and emergent behaviors. So if you're in route,\nplease come in person.",
    "start": "4605730",
    "end": "4612190"
  },
  {
    "text": "They'll be in person. So you'll be able to\ninteract with them in person. And if you're still not\nenrolled in the course",
    "start": "4612190",
    "end": "4619200"
  },
  {
    "text": "and wish to do so,\nplease do so on Axess. And for the folks on\nzoom, feel free to audit.",
    "start": "4619200",
    "end": "4625620"
  },
  {
    "text": "Lectures will be the same\ntime each week on Thursday. And we'll announce\nany notifications",
    "start": "4625620",
    "end": "4631800"
  },
  {
    "text": "by email, Canvas,\nas well as Discord. So keep your eye out for those.",
    "start": "4631800",
    "end": "4637650"
  },
  {
    "text": "And yeah, so thank you, guys. ",
    "start": "4637650",
    "end": "4648000"
  }
]