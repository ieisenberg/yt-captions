[
  {
    "start": "0",
    "end": "6200"
  },
  {
    "text": "So last time, in the\nlast three lectures,",
    "start": "6200",
    "end": "11299"
  },
  {
    "text": "we have talked about the\nbasics of uniform convergence.",
    "start": "11300",
    "end": "16820"
  },
  {
    "text": "I guess just a\nvery quick review. So I think we have proved\nthat the excess risk, this is",
    "start": "16820",
    "end": "25849"
  },
  {
    "text": "lecture 2, is bounded by this.",
    "start": "25850",
    "end": "33670"
  },
  {
    "text": "This is a difference between\nempirical and population. Can I share your\nscreen to the Zoom?",
    "start": "33670",
    "end": "39910"
  },
  {
    "text": "Oh, right. Thanks. Sorry, I forgot. Thanks for reminding me. It's going to be a problem\nif I forgot to do that.",
    "start": "39910",
    "end": "48001"
  },
  {
    "text": "I'll do that. I didn't join a\nZoom meeting here.",
    "start": "48001",
    "end": "53420"
  },
  {
    "text": "Sorry. ",
    "start": "53420",
    "end": "76520"
  },
  {
    "text": "Cool. I guess now probably\nit's working. Thanks for reminding me. And so we have shown this.",
    "start": "76520",
    "end": "84384"
  },
  {
    "start": "84385",
    "end": "90430"
  },
  {
    "text": "So this is what we saw in one\nof the claim in the lecture 2.",
    "start": "90430",
    "end": "95570"
  },
  {
    "text": "So basically, this is\nsaying that you only have to bound the difference\nbetween the population and the empirical\nfor all theta, right?",
    "start": "95570",
    "end": "102020"
  },
  {
    "text": "So the most important\nthing is the second term because the first\nterm, we have shown that it's close\nto-- it's something",
    "start": "102020",
    "end": "108250"
  },
  {
    "text": "bounded by 1 over\nsquare root of n. So the goal is to\nshow the second term. And we have discussed how to do\nit for finite hypothesis class",
    "start": "108250",
    "end": "117680"
  },
  {
    "text": "and also how to do it for\ninfinite hypothesis class to get with a relatively\nbrute force this quantization",
    "start": "117680",
    "end": "123159"
  },
  {
    "text": "technique. And so in the next few lectures,\nI guess, we are going to--",
    "start": "123160",
    "end": "131590"
  },
  {
    "text": "as I mentioned\nbefore, so we're going to have some other techniques\nto deal with the second term so that we can have\nmore informative parts.",
    "start": "131590",
    "end": "139060"
  },
  {
    "text": "And today, we are\ngoing to take a small-- in some sense, a small\ndigression, or in some sense,",
    "start": "139060",
    "end": "145180"
  },
  {
    "text": "a small preparation for some\nof the tools that we're going to use for the next lecture. So in the next lecture,\nwhat we're going to do",
    "start": "145180",
    "end": "152530"
  },
  {
    "text": "is that we're going to bound\nthe expectation of this. ",
    "start": "152530",
    "end": "158123"
  },
  {
    "text": "So this is the next lecture. ",
    "start": "158123",
    "end": "163870"
  },
  {
    "text": "And this is expectation over the\nrandomness of the data, right?",
    "start": "163870",
    "end": "169030"
  },
  {
    "text": "So this quantity itself is\na random variable, right, because it depends on the\ndata, the training data",
    "start": "169030",
    "end": "175750"
  },
  {
    "text": "you have because L hat\ndepends on the training data. And next time, we're\ngoing to upper bound this",
    "start": "175750",
    "end": "181750"
  },
  {
    "text": "by some quantity which is\ncalled Rademacher complexity. And so today, we're going to do\nsomething that is useful for--",
    "start": "181750",
    "end": "191320"
  },
  {
    "text": "it's a useful preparation\nfor doing this. ",
    "start": "191320",
    "end": "196769"
  },
  {
    "text": "So I guess here is the plan. So next lecture, we're\ngoing to do this. And then next\nlecture, we also going",
    "start": "196770",
    "end": "203769"
  },
  {
    "text": "to deal with the\ndifference between this",
    "start": "203770",
    "end": "210710"
  },
  {
    "text": "and this expectation. ",
    "start": "210710",
    "end": "218340"
  },
  {
    "text": "So that's the plan\nfor the next lecture. And today, what\nwe're going to do is we're going to have\nsome tools that prepare us",
    "start": "218340",
    "end": "225150"
  },
  {
    "text": "for proving\nquantities like this, and so that the\nnext time we don't",
    "start": "225150",
    "end": "230220"
  },
  {
    "text": "have to have a small\nsection dealing with the tool in the middle.",
    "start": "230220",
    "end": "235470"
  },
  {
    "text": "So I'm trying to prepare\nus with the right tool for the next lecture.",
    "start": "235470",
    "end": "241810"
  },
  {
    "text": "So a more concrete\noverview is the following. So the goal for this\nlecture is the following.",
    "start": "241810",
    "end": "248140"
  },
  {
    "text": "So suppose you have some\nrandom variable x1 up to xn.",
    "start": "248140",
    "end": "253380"
  },
  {
    "text": "So they are independent\nand random variables.",
    "start": "253380",
    "end": "260500"
  },
  {
    "text": "So we're going to show\ntwo type of inequalities. So the first type\nof inequality is to show that if you take\nthe sum of these kind",
    "start": "260500",
    "end": "268190"
  },
  {
    "text": "of random variables,\nthey are concentrated around the expectation. ",
    "start": "268190",
    "end": "275945"
  },
  {
    "text": "Basically, Hoeffding inequality\nis one type of this inequality. We're going to extent\nHoeffding inequality to something more general.",
    "start": "275945",
    "end": "283380"
  },
  {
    "text": "And the second\nthing is that we're going to show that for certain\ntype of function F, if you look",
    "start": "283380",
    "end": "296820"
  },
  {
    "text": "at a general function,\nnot necessarily just the sum of this random\nvariable, of course, you have to have some\nrestrictions on what",
    "start": "296820",
    "end": "303210"
  },
  {
    "text": "the functions F will look like. But suppose you have\nthe right restriction, then you can show that even\nif you have a function of x1",
    "start": "303210",
    "end": "309539"
  },
  {
    "text": "to xn, it's still concentrated\naround the expectation of this function. ",
    "start": "309540",
    "end": "316930"
  },
  {
    "text": "And this will be particularly\nuseful for showing this inequality. Maybe let's call this I here.",
    "start": "316930",
    "end": "322540"
  },
  {
    "text": "So because you can--  in some sense, maybe just assume\none, this corresponds to L",
    "start": "322540",
    "end": "331870"
  },
  {
    "text": "hat theta is close to L\ntheta because L hat theta is of the form like x1 up to--",
    "start": "331870",
    "end": "338770"
  },
  {
    "text": "plus x2 up to xn. L theta is the\nexpectation of L hat. And the second\ntype of inequality",
    "start": "338770",
    "end": "344800"
  },
  {
    "text": "will be useful for proving\nwhat I said, this equality 1. So because-- if you care\nabout something like this,",
    "start": "344800",
    "end": "356199"
  },
  {
    "text": "it's roughly equal\nto expectation. ",
    "start": "356200",
    "end": "361870"
  },
  {
    "text": "This is L hat.  So then you can view this\nentire thing as a function.",
    "start": "361870",
    "end": "370410"
  },
  {
    "text": "We would add a function\nof your training data-- of your IID training data.",
    "start": "370410",
    "end": "375990"
  },
  {
    "text": "So this is a function\nof x1 up to xn, where these are\nthe training data.",
    "start": "375990",
    "end": "381260"
  },
  {
    "text": " So basically-- so these\nkind of inequalities",
    "start": "381260",
    "end": "388480"
  },
  {
    "text": "are called concentration\ninequality. The key kind of idea is\nthat if you have a family",
    "start": "388480",
    "end": "395410"
  },
  {
    "text": "of IID random variables, then-- first of all, if you\ntake the sum of them, they become like Gaussian and\nthey become concentrated-- kind",
    "start": "395410",
    "end": "403140"
  },
  {
    "text": "of like Gaussian like\nand they concentrate around the mean of this sum. And the same thing also happens\nif you apply certain kind",
    "start": "403140",
    "end": "412520"
  },
  {
    "text": "of functions on x1 up to xn. I will tell you what\nkind of functions will have these properties.",
    "start": "412520",
    "end": "418720"
  },
  {
    "text": "And this kind of\ninequality is not only useful for\nwhat we are going to do next, but also\ngenerally pretty",
    "start": "418720",
    "end": "426670"
  },
  {
    "text": "useful for machine learning\nlike for statistical learning",
    "start": "426670",
    "end": "432010"
  },
  {
    "text": "theory. Because in some\nsense, if you think about what happens in learning\ntheory, in many cases,",
    "start": "432010",
    "end": "437913"
  },
  {
    "text": "basically you are trying\nto deal with the difference between the empirical\ndistribution and a population",
    "start": "437913",
    "end": "443276"
  },
  {
    "text": "distribution, right? So these things will show up\nin many, many different cases. And that's also\none of the reason",
    "start": "443277",
    "end": "448420"
  },
  {
    "text": "why I kind of isolate this\npart as a single lecture to talk about technique.",
    "start": "448420",
    "end": "453849"
  },
  {
    "text": "If it's just some tool that is\nonly useful for one lecture, then we can just\ninvoke that as a lemma.",
    "start": "453850",
    "end": "460900"
  },
  {
    "text": "But here, I think it's\nmore useful than that. So that's why I\nwant to kind of also show you how to prove\nsome of these things",
    "start": "460900",
    "end": "468039"
  },
  {
    "text": "and also what kind of a-- I'm not going to prove\nall the inequalities I'm going to show today, but\nI'm going to talk about some",
    "start": "468040",
    "end": "474340"
  },
  {
    "text": "of the advanced\nversion of inequalities so that you know\nthat they exist. And then when you\nneed to use them,",
    "start": "474340",
    "end": "480230"
  },
  {
    "text": "you can kind of find\nthe right tools. So that's the overview\nfor the lecture.",
    "start": "480230",
    "end": "487580"
  },
  {
    "text": "So I guess-- so let's start\nwith the simple version, right, where we're\ngoing to have a sum",
    "start": "487580",
    "end": "492940"
  },
  {
    "text": "of independent random variable. And we have discussed\nthis before about the--",
    "start": "492940",
    "end": "498440"
  },
  {
    "text": "in the context of\nHoeffding inequality. I'm going to have kind of a\nmore comprehensive discussion",
    "start": "498440",
    "end": "506620"
  },
  {
    "text": "about this. So let's consider you have\na random variable Z, which is equals to sum of\nx1 up to xn, right?",
    "start": "506620",
    "end": "513940"
  },
  {
    "text": "xi's are independent. ",
    "start": "513940",
    "end": "519559"
  },
  {
    "text": "And so a warm-up is that\nwhat if you don't ignore what",
    "start": "519559",
    "end": "527192"
  },
  {
    "text": "the structure of Z is, right? So obviously you\nknow that Z is a sum of independent random variable. What if you ignore\nthe structure?",
    "start": "527192",
    "end": "533860"
  },
  {
    "text": "So what if we ignore\nthe structure? ",
    "start": "533860",
    "end": "540820"
  },
  {
    "text": "So you still have something\nthat you can show-- you can still have\nsome inequality",
    "start": "540820",
    "end": "547120"
  },
  {
    "text": "that can show that Z is\nclose to the expectation. So here is the\ninequality, which is",
    "start": "547120",
    "end": "552700"
  },
  {
    "text": "called Chebyshev's inequality. ",
    "start": "552700",
    "end": "558463"
  },
  {
    "text": "I think probably you've heard of\nthis in some of the probability class. So the inequality is saying that\nthe probability that z deviate",
    "start": "558463",
    "end": "566500"
  },
  {
    "text": "from the expectation\nof z by some amount t is less than this thing, the\nvariance of z over t square.",
    "start": "566500",
    "end": "577580"
  },
  {
    "text": "So it's pretty intuitive, right? So if the variance\nof z is small, then you have less deviation\nfrom the expectation.",
    "start": "577580",
    "end": "584779"
  },
  {
    "text": "And of course, if\nt is bigger, right, if you look at a bigger\nwindow, then there's",
    "start": "584780",
    "end": "591770"
  },
  {
    "text": "a small probability-- smaller\nprobability outside the window, right? So in some sense, if you\ndraw this, it's something",
    "start": "591770",
    "end": "597898"
  },
  {
    "text": "like-- suppose you have\na distribution that looks like maybe this and the\nmean is here, expectation of z.",
    "start": "597898",
    "end": "604072"
  },
  {
    "text": "And what this is saying is that\nif you look at the standard deviation, z-- right--",
    "start": "604072",
    "end": "610820"
  },
  {
    "text": "so suppose you-- and you look at\nthis, standard deviation of z. And suppose you take\nt to be something",
    "start": "610820",
    "end": "618740"
  },
  {
    "text": "like standard\ndeviation of z times 1 over square root of delta,\nyou plug into this inequality,",
    "start": "618740",
    "end": "625970"
  },
  {
    "text": "what you get is that\nthe probability that you would deviate at\nthis t is less than--",
    "start": "625970",
    "end": "634490"
  },
  {
    "text": "maybe let's just\nwrite expressively. Standard division of z\nover square root delta",
    "start": "634490",
    "end": "640430"
  },
  {
    "text": "is less than delta, right? So this is saying that if you\nmultiply standard deviation",
    "start": "640430",
    "end": "648680"
  },
  {
    "text": "z by some quantity, by\nsomething like here-- so suppose this is standard deviation of z\ntimes 1 over square root delta,",
    "start": "648680",
    "end": "655160"
  },
  {
    "text": "the other is less than 1. Then the probability in this\ntail is less than delta, right?",
    "start": "655160",
    "end": "661110"
  },
  {
    "text": " So this is, in some sense, the\nweakest form of concentration",
    "start": "661110",
    "end": "669330"
  },
  {
    "text": "that you always have\nwithout using any structure about the random variable z.",
    "start": "669330",
    "end": "677519"
  },
  {
    "text": "However, this is not very\nstrong as we will see. Because if you think about what\nhappens with Gaussian, right?",
    "start": "677520",
    "end": "686730"
  },
  {
    "text": "so let's see where I'm\nmissing constant here. ",
    "start": "686730",
    "end": "694270"
  },
  {
    "text": "So if you think about-- ",
    "start": "694270",
    "end": "699490"
  },
  {
    "text": "let's see.  So if you think about a\nGaussian distribution,",
    "start": "699490",
    "end": "706060"
  },
  {
    "text": "suppose you know z is Gaussian. So Gaussian z. Then what you know is that--",
    "start": "706060",
    "end": "711115"
  },
  {
    "text": " So suppose z is something\nlike from 0, 1, right?",
    "start": "711115",
    "end": "720772"
  },
  {
    "text": "It doesn't matter\nwhether the mean is 0. Let's say, suppose\nthe mean is mu. Then what you know is that\nz minus expectation of z",
    "start": "720773",
    "end": "729840"
  },
  {
    "text": "is less than standard\ndeviation of z times",
    "start": "729840",
    "end": "739910"
  },
  {
    "text": "square root log 1 over delta.",
    "start": "739910",
    "end": "747160"
  },
  {
    "text": "I guess maybe, let's\nsay, this is just a general Gaussian\ndistribution where the standard deviation sigma.",
    "start": "747160",
    "end": "752920"
  },
  {
    "text": "And so with probability,\nat least 1 minus theta,",
    "start": "752920",
    "end": "765510"
  },
  {
    "text": "you have this. So basically, if you have\na Gaussian distribution, then what you have is that for\nthe same field of probability",
    "start": "765510",
    "end": "772410"
  },
  {
    "text": "delta, here you have\na stronger bound. It's log 1 over delta instead\nof 1 over delta squared.",
    "start": "772410",
    "end": "778769"
  },
  {
    "text": " So in some sense, what--",
    "start": "778770",
    "end": "785740"
  },
  {
    "text": "I guess I'm not showing this-- I haven't proved this\nfor you, like you can do the calculation.",
    "start": "785740",
    "end": "790820"
  },
  {
    "text": "So in some sense, this\nis saying that the tail-- you can-- the tail decays\nfaster for Gaussian.",
    "start": "790820",
    "end": "798130"
  },
  {
    "text": "So basically, for\nGaussian, you only have to multiply a little bit. So suppose this is\nGaussian, you only",
    "start": "798130",
    "end": "805209"
  },
  {
    "text": "have to consider\nstandard deviation z times log 1 over\ndelta square root. Then you know that\nthe rest of the part",
    "start": "805210",
    "end": "814000"
  },
  {
    "text": "has probability less than delta. But if you don't need\nto know it's Gaussian, then you have to be\na little bit more",
    "start": "814000",
    "end": "819422"
  },
  {
    "text": "generous in terms of the\ninterval that you draw, OK?",
    "start": "819422",
    "end": "825520"
  },
  {
    "text": "So in some sense, the goal\nthat we're going to have",
    "start": "825520",
    "end": "830920"
  },
  {
    "text": "is that we're going to\nshow that if your z is a sum of random\nvariable, that it's more like Gaussian,\ninstead of a general--",
    "start": "830920",
    "end": "838210"
  },
  {
    "text": "the worst case z. Or you have a better\nbond like this instead of the bond like\nthis from the Chebyshev's",
    "start": "838210",
    "end": "846010"
  },
  {
    "text": "inequality. ",
    "start": "846010",
    "end": "851160"
  },
  {
    "text": "And also if you see the-- if you look at it more\ncarefully on the consequences",
    "start": "851160",
    "end": "857279"
  },
  {
    "text": "of these two\ninequalities-- so maybe let's call this number\n3 and this number 4.",
    "start": "857280",
    "end": "863640"
  },
  {
    "text": "So if you have number\n3, then you can-- if you take delta to be\nsomething like inverse poly n,",
    "start": "863640",
    "end": "873529"
  },
  {
    "text": "then you'll know that with\nhigh probability, so at least 1",
    "start": "873530",
    "end": "879830"
  },
  {
    "text": "minus 1 over poly n,\nz minus expectations z",
    "start": "879830",
    "end": "886480"
  },
  {
    "text": "is less than standard deviation\nof z times square root log n. So basically, you\nonly lose a log factor",
    "start": "886480",
    "end": "894310"
  },
  {
    "text": "if you want to make the\nprobability very high. So if you want to make a\nhigh probability event,",
    "start": "894310",
    "end": "899980"
  },
  {
    "text": "then you only have to multiply\nby a square root log n to the standard\ndeviation and then the rest of the probability\nbecomes very small.",
    "start": "899980",
    "end": "907580"
  },
  {
    "text": "However, if you use 3-- if you use the number-- sorry, this is number 4.",
    "start": "907580",
    "end": "914470"
  },
  {
    "text": "Sorry for the confusion. So if you use the number 4\nif it's Gaussian like then you got this. But if you use number 3, then\nif you take delta to be--",
    "start": "914470",
    "end": "925360"
  },
  {
    "text": "if using-- take delta to be\ninverse poly n, then what",
    "start": "925360",
    "end": "935579"
  },
  {
    "text": "happens is that with\nhigh probability, you have this statement. With high probability, z minus\nexpectation z is less than std",
    "start": "935580",
    "end": "945280"
  },
  {
    "text": "of z over square root delta,\nwhich is std of z times poly n.",
    "start": "945280",
    "end": "952360"
  },
  {
    "text": "So there's a big difference\nbetween the additional factor here. So if you compare\nthis two factor,",
    "start": "952360",
    "end": "957760"
  },
  {
    "text": "you have a big difference. So that's why we want the\nso-called the faster tail",
    "start": "957760",
    "end": "964825"
  },
  {
    "text": "of the smaller tail\nlike in the inequality 4 instead of inequality is 3.",
    "start": "964825",
    "end": "971709"
  },
  {
    "text": "And a slightly\nalternative view, which we're going to kind\nof, in some sense,",
    "start": "971710",
    "end": "978730"
  },
  {
    "text": "switch between these two views. They are equivalent. But we're going to\nswitch this very often.",
    "start": "978730",
    "end": "986500"
  },
  {
    "text": "So the alternative\nview is that you can say that z minus\nexpectation z is less than.",
    "start": "986500",
    "end": "993339"
  },
  {
    "text": "So for Gaussian,\nwhat you have is that if you look at this, if you\nview this quantity like this,",
    "start": "993340",
    "end": "1000520"
  },
  {
    "text": "then you have this\nless than expectation minus 2t squared over\nvariance of z times n.",
    "start": "1000520",
    "end": "1009730"
  },
  {
    "text": "So now, you can compare this\ninequality, maybe let's call it 5, just temporarily, versus\nthe Chebyshev inequality, 1.",
    "start": "1009730",
    "end": "1017850"
  },
  {
    "text": "So if you look at\n1, then this is-- the right hand side is decay\nwith t in polynomial way.",
    "start": "1017850",
    "end": "1024329"
  },
  {
    "text": "So it's 1 over t squared. And if you look at 5, it\ndecays exponentially fast as t goes to infinity.",
    "start": "1024329",
    "end": "1030790"
  },
  {
    "text": "So that's another way to see\nthe differences, all right? So the tail probability\nfor Gaussian distribution",
    "start": "1030790",
    "end": "1036390"
  },
  {
    "text": "is decaying very fast,\nexponentially fast. But if you use the\nChebyshev inequality, you to get a polynomially\nfast decaying inequality.",
    "start": "1036390",
    "end": "1045630"
  },
  {
    "text": "And that's another way\nto see the differences. So we're going to look for\nthe faster tail, right?",
    "start": "1045630",
    "end": "1051540"
  },
  {
    "text": "That's what our goal. So the goal is to repeat. So z-- this is like a Gaussian.",
    "start": "1051540",
    "end": "1060510"
  },
  {
    "text": "That's basically our goal. But of course, how\ndo you say, in what sense this like Gaussian?",
    "start": "1060510",
    "end": "1066870"
  },
  {
    "text": "There are multiple\ndifferent versions. We're going to formalize that. What does it mean more\nlike Gaussian tail?",
    "start": "1066870",
    "end": "1073590"
  },
  {
    "text": "So we-- to do this\nformally, let's start",
    "start": "1073590",
    "end": "1080529"
  },
  {
    "text": "with some definitions. So actually, we're\ngoing to define",
    "start": "1080530",
    "end": "1087190"
  },
  {
    "text": "what is meant by\nGaussian-like to start with.",
    "start": "1087190",
    "end": "1093169"
  },
  {
    "text": "So let's say a random\nvariable x with finite mean,",
    "start": "1093170",
    "end": "1102820"
  },
  {
    "text": "this is a one-dimensional\nrandom variable mu, which is equal to expectation\nx is called sub-Gaussian",
    "start": "1102820",
    "end": "1114360"
  },
  {
    "text": "with parameter sigma if\nthe following is true.",
    "start": "1114360",
    "end": "1133225"
  },
  {
    "text": "Let me write it down. It's not very intuitive\nwhen you first look at it. ",
    "start": "1133225",
    "end": "1145020"
  },
  {
    "text": "So I don't-- I'm not expecting\nthat you can see what this is really mean. But this is the definition for\nsomething is close to Gaussian.",
    "start": "1145020",
    "end": "1153600"
  },
  {
    "text": "And this is not very intuitive. But the corollary\nis the following.",
    "start": "1153600",
    "end": "1159970"
  },
  {
    "text": " So a corollary is that\nx is sigma sub-Gaussian",
    "start": "1159970",
    "end": "1174530"
  },
  {
    "text": "if the following happens-- implies the following happens. So x minus mu larger\nthan t is less than 2",
    "start": "1174530",
    "end": "1186120"
  },
  {
    "text": "times exponential\nminus 2t squared over 2 sigma squared over--",
    "start": "1186120",
    "end": "1192738"
  },
  {
    "text": "sorry, t squared over 2\nsigma squared for every t.",
    "start": "1192738",
    "end": "1199680"
  },
  {
    "text": "So the corollary is\nprobably intuitive, right? So if x is sub-Gaussian if you\nhave this exponential decaying",
    "start": "1199680",
    "end": "1209779"
  },
  {
    "text": "tail bound. So this right hand side\ndecays very fast in K.",
    "start": "1209780",
    "end": "1216140"
  },
  {
    "text": "And as t goes to infinity,\nit's actually not only exponential in t. It's exponential in t squared.",
    "start": "1216140",
    "end": "1222170"
  },
  {
    "text": "So this is, in some sense,\nthis is a much more intuitive definition of sub-Gaussian.",
    "start": "1222170",
    "end": "1228770"
  },
  {
    "text": "But the formal\ndefinition above will be more useful for the\nmathematical cleanness.",
    "start": "1228770",
    "end": "1238310"
  },
  {
    "text": "But you can basically think\nof these two as equivalent. Actually, they are\nsomewhat equivalent.",
    "start": "1238310",
    "end": "1246200"
  },
  {
    "text": "Before talking about\nthat, let's say if you recall that if x is\nGaussian, if it were literally",
    "start": "1246200",
    "end": "1255640"
  },
  {
    "text": "Gaussian with variance sigma\nsquared, then this inequality,",
    "start": "1255640",
    "end": "1265720"
  },
  {
    "text": "maybe let's call it 6,\nthen this means 6 is true.",
    "start": "1265720",
    "end": "1271960"
  },
  {
    "text": "I didn't prove this, but this is\nsomething relatively standard. So if you have a Gaussian\nwith variance sigma squared,",
    "start": "1271960",
    "end": "1278200"
  },
  {
    "text": "then you can-- if you do some kind of\ncalculation, do some integral, which is not super trivial, you\nhave to do some calculation,",
    "start": "1278200",
    "end": "1284980"
  },
  {
    "text": "but believe me 6 is true, right? So basically sigma\nsub-Gaussian is",
    "start": "1284980",
    "end": "1290920"
  },
  {
    "text": "saying that you have the\nsame property as a Gaussian random variable with\nvariance sigma squared.",
    "start": "1290920",
    "end": "1297220"
  },
  {
    "text": "And also because of\nthis sigma squared in the sub-Gaussian definition\nis often called variance proxy.",
    "start": "1297220",
    "end": "1315975"
  },
  {
    "text": " So in some sense, if you\nare sigma sub-Gaussian,",
    "start": "1315975",
    "end": "1322720"
  },
  {
    "text": "then the sigma is kind of\nlike you can think of as sigma squared, you can think of it\nas some kind of like pseudo",
    "start": "1322720",
    "end": "1329470"
  },
  {
    "text": "variance. It's not exactly the\nvariance, but it's the kind of alternative\nversion of the variance, which",
    "start": "1329470",
    "end": "1334642"
  },
  {
    "text": "actually is probably more\nimportant than the variance itself. ",
    "start": "1334642",
    "end": "1340600"
  },
  {
    "text": "So that's the rough intuition. And also regarding\nthese two definitions of this corollary 6,\nmaybe let's call this 7,",
    "start": "1340600",
    "end": "1348070"
  },
  {
    "text": "so 6 and 7 are, in some\nsense, equivalent definitions",
    "start": "1348070",
    "end": "1358889"
  },
  {
    "text": "up to some small constant,\nup to some constant factor.",
    "start": "1358890",
    "end": "1368230"
  },
  {
    "text": "So what does it mean is that\nif you use 6 as the definition, then it means--",
    "start": "1368230",
    "end": "1374460"
  },
  {
    "text": "suppose you used 6\nas the definition, or suppose you\nsatisfy 6, then you know that x is O\nsigma sub-Gaussian",
    "start": "1374460",
    "end": "1387820"
  },
  {
    "text": "under the definition--\nunder the formal definition. So in some sense, if you don't\ncare about a constant factor",
    "start": "1387820",
    "end": "1394060"
  },
  {
    "text": "in front of the variance proxy,\nthen these two definitions are-- you can--",
    "start": "1394060",
    "end": "1400090"
  },
  {
    "text": "7 imply 6 and 6 also implies\n7, up to a small constant loss. ",
    "start": "1400090",
    "end": "1407413"
  },
  {
    "text": "So basically, the way that\nI always think about this is that I always think\nabout 6 as intuitively",
    "start": "1407413",
    "end": "1413360"
  },
  {
    "text": "that I think about it. But when I really use the-- when I really need to use some\nproperties about sub-Gaussian,",
    "start": "1413360",
    "end": "1420059"
  },
  {
    "text": "I mean, I really kind of like-- I want to prove something,\nI typically use 7.",
    "start": "1420060",
    "end": "1426800"
  },
  {
    "text": "And also, I didn't tell\nyou why these two equations are somewhat related, right?",
    "start": "1426800",
    "end": "1432559"
  },
  {
    "text": "It still sounds like mysterious,\nwhy they are related. And here is the reason\nwhy they are related.",
    "start": "1432560",
    "end": "1438560"
  },
  {
    "text": "I guess what I'm going to do\nis that I'm going to show here,",
    "start": "1438560",
    "end": "1443820"
  },
  {
    "text": "6 implies 7. I'm not going to show 7-- sorry, my number\nis-- my number is",
    "start": "1443820",
    "end": "1449600"
  },
  {
    "text": "different from my\nnumber in the notes. That's why I'm confused. I'm going to show 7 implies 7.",
    "start": "1449600",
    "end": "1457559"
  },
  {
    "text": "But 6 implies 7 would\nrequire a different proof. But if I show 7\nimplies 6, you probably would kind of get\na little intuition",
    "start": "1457560",
    "end": "1464580"
  },
  {
    "text": "why they are related quantities. ",
    "start": "1464580",
    "end": "1470100"
  },
  {
    "text": "So the kind of general intuition\nis the following, right? So if you look at the\nChebyshev inequality, so",
    "start": "1470100",
    "end": "1476650"
  },
  {
    "text": "Chebyshev inequality, how do\nyou prove Chebyshev inequality?",
    "start": "1476650",
    "end": "1485330"
  },
  {
    "text": "So the way that you prove\nChebyshev inequality is something like this. So that you say the probability\nthat z is minus expectation",
    "start": "1485330",
    "end": "1492830"
  },
  {
    "text": "z larger than t is\nequal to probability that z minus expectation\nz squared is larger than t",
    "start": "1492830",
    "end": "1500390"
  },
  {
    "text": "squared. And then you use the\nso-called Markov inequality. You say that this is\nless than the expectation",
    "start": "1500390",
    "end": "1507620"
  },
  {
    "text": "of this random\nvariable over t square. So the last step is using\nthis Markov inequality.",
    "start": "1507620",
    "end": "1516317"
  },
  {
    "text": "Is it called Markov? Yes, I think it is. So which is saying\nthat if you look at the probability of some\nrandom variable, maybe",
    "start": "1516317",
    "end": "1522480"
  },
  {
    "text": "it's called y,\nlarger than t, this is smaller than the\nexpectation of y less than t.",
    "start": "1522480",
    "end": "1534840"
  },
  {
    "text": "Because if you have so\nmuch mass larger than t, then your expectation\nhas to be high.",
    "start": "1534840",
    "end": "1540360"
  },
  {
    "text": "That's basically intuition. And you can see that the way\nto prove Chebyshev inequality",
    "start": "1540360",
    "end": "1545700"
  },
  {
    "text": "is that you raise\nto the power of 2. You raise to the second power. So that means that naturally you\ncan also consider higher power",
    "start": "1545700",
    "end": "1553770"
  },
  {
    "text": "and apply Markov inequality. Again, you get some\nother type of inequality.",
    "start": "1553770",
    "end": "1559140"
  },
  {
    "text": "So if you consider higher\nmoments, then what happens",
    "start": "1559140",
    "end": "1564260"
  },
  {
    "text": "is that you can get\nsomething like this, right? So if, for example,\nyou can say I'm going to look at\nthe fourth power.",
    "start": "1564260",
    "end": "1573779"
  },
  {
    "text": "So the fourth power,\nthis is-- sorry, this is still equal\nto this, right? Because you just\nraised everything",
    "start": "1573780",
    "end": "1580320"
  },
  {
    "text": "to the-- fourth power\nis the same event. So this is equal to this. And then you can use\nthe Markov inequality",
    "start": "1580320",
    "end": "1587370"
  },
  {
    "text": "to get expectation z\nminus expectation of z to the power of 4\nover to t to the 4.",
    "start": "1587370",
    "end": "1596560"
  },
  {
    "text": "So now, you see that you have\na better dependency on t, better--",
    "start": "1596560",
    "end": "1602059"
  },
  {
    "text": "or faster better dependency--\nor faster decay, maybe, faster decay in\nt, right, which is",
    "start": "1602060",
    "end": "1610240"
  },
  {
    "text": "something we are looking for. We are finally aiming for\nexponential decay in t. But now, we get something\nbetter than t squared.",
    "start": "1610240",
    "end": "1616300"
  },
  {
    "text": "We get t to the 4. So but of course,\nthe trade-off is that our top, this\nquantity on the top,",
    "start": "1616300",
    "end": "1624590"
  },
  {
    "text": "might be bigger, in some sense,\nthan the variance, right? This is the fourth power of\nthe deviation, in some sense.",
    "start": "1624590",
    "end": "1631440"
  },
  {
    "text": "So sometimes, you\ncan get a trade-off, an implicit trade-off, right? So you get a better\ndependency on t, but you get a worse\ndependency in the numerator.",
    "start": "1631440",
    "end": "1644900"
  },
  {
    "text": "And you can try to do\nthis with higher powers, like if you raised to the power\nof 6, raise to power of 8, so and so forth, right?",
    "start": "1644900",
    "end": "1650990"
  },
  {
    "text": "So actually there\nare, especially if you look at the early\nworks in this concentration inequality, people do\nraise to a higher power.",
    "start": "1650990",
    "end": "1658220"
  },
  {
    "text": "It turns out that there\nis a relatively simple way to deal with all the powers.",
    "start": "1658220",
    "end": "1663799"
  },
  {
    "text": "This, which is called\nmoment generating function. So this becomes-- this\nwill make it cleaner.",
    "start": "1663800",
    "end": "1669841"
  },
  {
    "text": "So that you don't have to\ndeal with each of the power and see which one has\nthe best trade-off. So the so-called moment\ngenerating functions",
    "start": "1669842",
    "end": "1678260"
  },
  {
    "text": "is exactly this thing that\nwe define in this definition,",
    "start": "1678260",
    "end": "1684710"
  },
  {
    "text": "we use in this definition of\ndefining sub-Gaussianality.",
    "start": "1684710",
    "end": "1689760"
  },
  {
    "text": "So this is the expectation of\nexponential of the deviation",
    "start": "1689760",
    "end": "1697460"
  },
  {
    "text": "between x and its expectation. So why this is an interesting\nquantity, so the reason is",
    "start": "1697460",
    "end": "1702710"
  },
  {
    "text": "that if you look at-- if\nyou Taylor expand this, or Taylor expand\nwhat's inside, this",
    "start": "1702710",
    "end": "1708380"
  },
  {
    "text": "is exponential of something. So Taylor expansion would\nbe that 1 plus lambda times x minus ex plus lambda squared\nover 2 times x minus ex squared",
    "start": "1708380",
    "end": "1722730"
  },
  {
    "text": "plus-- so and so forth, right? And if you write\nit more formally, so this is something like\nsum over k from 0 to n.",
    "start": "1722730",
    "end": "1732840"
  },
  {
    "text": "And the coefficient\nfor expansion is lambda to the power k over\nk factorial times expectation.",
    "start": "1732840",
    "end": "1739559"
  },
  {
    "text": "And you switch the\nexpectation with the sum, the expectation of x minus\nex to the power of k.",
    "start": "1739560",
    "end": "1747230"
  },
  {
    "text": "So we can see that this moment\ngenerating function is really a mixture of different\nmoments, right?",
    "start": "1747230",
    "end": "1752360"
  },
  {
    "text": " You have all the\nmoments, and every moment",
    "start": "1752360",
    "end": "1757610"
  },
  {
    "text": "have a different weight\nin front of them. In some sense, this is saying\nthat what we are going to do",
    "start": "1757610",
    "end": "1763370"
  },
  {
    "text": "is that we're going\nto change the lambda. So that you change\nthe relative weight",
    "start": "1763370",
    "end": "1768380"
  },
  {
    "text": "in front of all the moments. So that you can\nchoose, in some sense, the right trade-off between\nwhich moment you are going",
    "start": "1768380",
    "end": "1776930"
  },
  {
    "text": "to use and-- so sometimes, if you\nchoose the right lambda, you're going to choose\nthe right-- focus",
    "start": "1776930",
    "end": "1783277"
  },
  {
    "text": "on the right moment and\nget the right dependency. So that's the rough intuition.",
    "start": "1783277",
    "end": "1789290"
  },
  {
    "text": "And-- if you really do\nthis mathematically, actually, it's even\nsimpler than this.",
    "start": "1789290",
    "end": "1795600"
  },
  {
    "text": "So what you can have is that\nif you look at probability of x minus ex other than t.",
    "start": "1795600",
    "end": "1801740"
  },
  {
    "text": "Then this is-- so\nthis is formally, the way you do the\ntrade-off is the following. So you look at this and you\nsay, I'm going to raise--",
    "start": "1801740",
    "end": "1809450"
  },
  {
    "text": "instead of raising to the power,\nI'm going to use exponential. ",
    "start": "1809450",
    "end": "1815190"
  },
  {
    "text": "So this is equivalent to this. The exponential version\nis larger than--",
    "start": "1815190",
    "end": "1821720"
  },
  {
    "text": "exponential of lambda t, right? And then now, you use\nMarkov's inequality",
    "start": "1821720",
    "end": "1827540"
  },
  {
    "text": "for this exponential version. So you get expectation\ne of lambda x minus ex over this Markov's\ninequality e to the lambda",
    "start": "1827540",
    "end": "1838570"
  },
  {
    "text": "t Markov.  And now, you use the definition\nof the sub-Gaussianality.",
    "start": "1838570",
    "end": "1846860"
  },
  {
    "text": "So you say that, I\nguess I need to review what the definition,\nmaybe, or you remember it.",
    "start": "1846860",
    "end": "1853706"
  },
  {
    "text": "So the definition\nof sub-Gaussianality is that the moment\ngenerating function is bounded by exponential\nof lambda squared.",
    "start": "1853707",
    "end": "1861433"
  },
  {
    "text": "That's the important\nthing, right? So there's a lambda\nsquared in the exponent, it's exponential of some\nquadratic function of lambda.",
    "start": "1861433",
    "end": "1869010"
  },
  {
    "text": "So unless you apply that, you\nget e to the sigma squared, lambda squared over\n2 in the numerator.",
    "start": "1869010",
    "end": "1875450"
  },
  {
    "text": "And divided by e\nto the lambda t. So this is e to the\nsigma squared lambda squared over 2 minus lambda t.",
    "start": "1875450",
    "end": "1882020"
  },
  {
    "text": "And now, you can see that in the\nexponent, you have a quadratic. And this quadratic looks like--",
    "start": "1882020",
    "end": "1887524"
  },
  {
    "start": "1887525",
    "end": "1894530"
  },
  {
    "text": "wait, am I doing\nthe right thing? ",
    "start": "1894530",
    "end": "1902280"
  },
  {
    "text": "So this is a quadratic that\nlooks like this, right? Something-- maybe not--\nthere's maybe some--",
    "start": "1902280",
    "end": "1910400"
  },
  {
    "text": "this is a quadratic that\nlooks like this, right? And you can choose lambda\nwhatever you want, right?",
    "start": "1910400",
    "end": "1917270"
  },
  {
    "text": "That's a free parameter. So that's why you want to\nchoose the minimum lambda and minimize this quadratic. So that you get the best bound.",
    "start": "1917270",
    "end": "1924080"
  },
  {
    "text": "So if you take the\nbest lambda, which means that you want\nto find a lambda",
    "start": "1924080",
    "end": "1930310"
  },
  {
    "text": "and minimize this quadratic. That's relatively easy. You can just take the--",
    "start": "1930310",
    "end": "1935470"
  },
  {
    "text": "smallest lambda is\nthe global minimum. You just do the derivative and\nmake the derivative to be zero.",
    "start": "1935470",
    "end": "1941020"
  },
  {
    "text": "And the best lambda, it turns\nout to be t over sigma squared. And you plug that\nin, then this is",
    "start": "1941020",
    "end": "1946120"
  },
  {
    "text": "equal to e to the minus t\nsquared over 2 sigma squared.",
    "start": "1946120",
    "end": "1952460"
  },
  {
    "text": "So basically, we show\nthis is equation 7, right? This is the equation--\nthis is the second,",
    "start": "1952460",
    "end": "1957490"
  },
  {
    "text": "this is the corollary, I\nthink it's the equation 6. ",
    "start": "1957490",
    "end": "1964940"
  },
  {
    "text": "So basically, you start\nwith the Gaussian, so here you use the definition.",
    "start": "1964940",
    "end": "1970767"
  },
  {
    "text": "So basically, use the definition\nof the sub-Gaussianity, and you get this tail bound\nfor this random variable.",
    "start": "1970767",
    "end": "1976580"
  },
  {
    "text": " And also you can\nget the other side. So here, you only know that\nx is not too much bigger",
    "start": "1976580",
    "end": "1983840"
  },
  {
    "text": "than the Ex plus t. You can also get the other side.",
    "start": "1983840",
    "end": "1993145"
  },
  {
    "start": "1993145",
    "end": "1998520"
  },
  {
    "text": "Less than minus t, and\nhow do you do that? The truer thing would\nbe that you just flip.",
    "start": "1998520",
    "end": "2007200"
  },
  {
    "text": "You define x prime\nto be minus x. And then probability\nthat x prime minus ex",
    "start": "2007200",
    "end": "2014960"
  },
  {
    "text": "prime is larger than t is the\nsame as probability x minus Ex",
    "start": "2014960",
    "end": "2021470"
  },
  {
    "text": "is smaller than minus t. But just by a simple\ndefinition, and then you apply what we have\nalready got on x prime.",
    "start": "2021470",
    "end": "2030179"
  },
  {
    "text": "And then that implies\nwhat you have-- the other side of\nthe bound for x.",
    "start": "2030180",
    "end": "2038580"
  },
  {
    "text": "But this is not super important. It's just that the two\nsides are basically",
    "start": "2038580",
    "end": "2043740"
  },
  {
    "text": "the same for our purpose. OK.  I think OK, so what\nhappened so far?",
    "start": "2043740",
    "end": "2050319"
  },
  {
    "text": "So I have defined this\nsub-Gaussian random variable and have argued that the\nsub-Gaussian random variable is",
    "start": "2050320",
    "end": "2057969"
  },
  {
    "text": "basically saying that\nyou have two ways, right? So one way is that the\nsub-Gaussian random variable",
    "start": "2057969",
    "end": "2063638"
  },
  {
    "text": "basically means that you\nhave a very fast tail, a very fast decaying tail. Or the moment, some\nkind of moment,",
    "start": "2063639",
    "end": "2070810"
  },
  {
    "text": "you can think of e to the\nlambda x minus mu as the moment, some kind of moment is bounded.",
    "start": "2070810",
    "end": "2075850"
  },
  {
    "text": "And all the moments are bounded\nby something in this form.",
    "start": "2075850",
    "end": "2081638"
  },
  {
    "text": "So far, I only talked about\none random variable, right? But the reason why\nI care about this",
    "start": "2081639",
    "end": "2087280"
  },
  {
    "text": "is the following theorem, which\nis the mean just in some sense. So saying that if you have--",
    "start": "2087280",
    "end": "2095739"
  },
  {
    "text": "if you have all the xi's,\nall the independent random variables are sub-Gaussian,\nthe sum of them",
    "start": "2095739",
    "end": "2101380"
  },
  {
    "text": "is also sub-Gaussian. So you can compose. And that's the biggest benefit\nof this sub-Gaussianality.",
    "start": "2101380",
    "end": "2107980"
  },
  {
    "start": "2107980",
    "end": "2118790"
  },
  {
    "text": "Our independent sub-Gaussian\nrandom variables",
    "start": "2118790",
    "end": "2125490"
  },
  {
    "text": "with variance proxy sigma\n1 squared up to sigma n",
    "start": "2125490",
    "end": "2134020"
  },
  {
    "text": "squared, respectively. Then if you look at\nsum of them, it's",
    "start": "2134020",
    "end": "2146160"
  },
  {
    "text": "also sub-Gaussian with\nvariance proxy sum",
    "start": "2146160",
    "end": "2157530"
  },
  {
    "text": "of sigma i square from 1 to n. So this is as a corollary,\nbecause the sub-Gaussian",
    "start": "2157530",
    "end": "2167400"
  },
  {
    "text": "with this variance\nproxy, you know that you have the\nconcentration for z, which",
    "start": "2167400",
    "end": "2172950"
  },
  {
    "text": "is of this exponential form. ",
    "start": "2172950",
    "end": "2182020"
  },
  {
    "text": "So you know that you\nhave this tail, that decays exponentially fast.",
    "start": "2182020",
    "end": "2187090"
  },
  {
    "text": "So this is very\nimportant, because-- very useful and very\nimportant, because now,",
    "start": "2187090",
    "end": "2192310"
  },
  {
    "text": "if you have a sum of\nindependent variables, you want to know how\nfast the tail decays. You can look at whether each\nof them is sub-Gaussian.",
    "start": "2192310",
    "end": "2203320"
  },
  {
    "text": "I'm going to prove\nthis in a moment.  The proof is actually\njust the two lines,",
    "start": "2203320",
    "end": "2209810"
  },
  {
    "text": "which is actually very cool.  So but before proving\nthis statement,",
    "start": "2209810",
    "end": "2216440"
  },
  {
    "text": "let me try to give you\nsome examples on what random variables\nare sub-Gaussian. It's basically--\nthe applicability",
    "start": "2216440",
    "end": "2222490"
  },
  {
    "text": "of this theorem depends\non whether you can show each of the xi is sub-Gaussian. If you can show\neach of the xi is",
    "start": "2222490",
    "end": "2228474"
  },
  {
    "text": "sub-Gaussian with very\ngood parameter sigma i's, then the theorem applies. And you got a pretty good bound\nfor the sum of them, right?",
    "start": "2228475",
    "end": "2236530"
  },
  {
    "text": "So what random variables\nare sub-Gaussian, right? A single random variable\nare sub-Gaussian. So there are some examples here.",
    "start": "2236530",
    "end": "2242890"
  },
  {
    "start": "2242890",
    "end": "2248630"
  },
  {
    "text": "By the way, whether your\nrandom variable is sub-Gaussian sometimes depends on what\nsigma you choose, right? So if you choose bigger\nand bigger sigma,",
    "start": "2248630",
    "end": "2255350"
  },
  {
    "text": "there is at least\neither more chance that they can be sub-Gaussian. Of course, it's\nnot like-- it's not guaranteed that if you choose\nsigma to be really, really big,",
    "start": "2255350",
    "end": "2262490"
  },
  {
    "text": "you can be sub-Gaussian. That's not always guaranteed. But at least intuitively,\nit's not a binary question.",
    "start": "2262490",
    "end": "2269432"
  },
  {
    "text": "It's not saying this one is\nsub-Gaussian, this one is not. Sometimes, it depends on\nwhat parameters you choose.",
    "start": "2269433",
    "end": "2275490"
  },
  {
    "text": "So at least it's not\nalways a binary question. For example, for Rademacher\ncomplexity is also called Rademacher variable,\nRademacher variable",
    "start": "2275490",
    "end": "2282380"
  },
  {
    "text": "just means that random\nvariable, that's it. So basically means x is\nuniform from plus or minus 1.",
    "start": "2282380",
    "end": "2291930"
  },
  {
    "text": "So this one, I claim\nis sub-Gaussian. The reason is--\nintuitively, the reason",
    "start": "2291930",
    "end": "2298440"
  },
  {
    "text": "is that if you look at\nthis random variable, if you look at\ndensity, is something like you have a spike\nat 1, spike at minus 1.",
    "start": "2298440",
    "end": "2305820"
  },
  {
    "text": "So basically the\ndensity decays very fast after you go outside\nplus or minus 1. It decays extremely fast.",
    "start": "2305820",
    "end": "2312090"
  },
  {
    "text": "It becomes 0. That's why it's sub-Gaussian. And technically, you can say\nthat you can prove this larger",
    "start": "2312090",
    "end": "2322130"
  },
  {
    "text": "than t is less than 2\nexponential minus t squared",
    "start": "2322130",
    "end": "2328579"
  },
  {
    "text": "over a big constant c0 for c0\nto be O of 1, maybe let's say 2.",
    "start": "2328580",
    "end": "2335360"
  },
  {
    "text": "This is because if t is less\nthan 1, then right hand side--",
    "start": "2335360",
    "end": "2342950"
  },
  {
    "text": "is less than 1-- is equal to-- is bigger than 1. So that's always true, right? I think I choose this so that--",
    "start": "2342950",
    "end": "2350300"
  },
  {
    "text": "yes, because the right hand side\nis equal to exponential minus 1",
    "start": "2350300",
    "end": "2356480"
  },
  {
    "text": "over c0. And if you take c0 to be\na big constant, maybe 2, then this is larger than 1.",
    "start": "2356480",
    "end": "2363200"
  },
  {
    "text": "So you verify this\nfor c less than 1. And then if t is bigger than\n1, then the LHS is just zero.",
    "start": "2363200",
    "end": "2370400"
  },
  {
    "text": "So that's why it's also true.  So that's Rademacher\nrandom variable.",
    "start": "2370400",
    "end": "2377579"
  },
  {
    "text": "So that means that\nRademacher random variable",
    "start": "2377580",
    "end": "2385080"
  },
  {
    "text": "is O of 1 sub-Gaussian. ",
    "start": "2385080",
    "end": "2392520"
  },
  {
    "text": "Sub-Gaussian with variance\n1, with variance proxy of 1. ",
    "start": "2392520",
    "end": "2400210"
  },
  {
    "text": "And similarly, you\ncan prove that-- similarly, if x minus\ne of x is bound by m,",
    "start": "2400210",
    "end": "2411970"
  },
  {
    "text": "So basically suppose you have\na random variable where e of x",
    "start": "2411970",
    "end": "2417070"
  },
  {
    "text": "is here, and if you look\nat a window of size m plus m minus m, all right?",
    "start": "2417070",
    "end": "2423717"
  },
  {
    "text": " So suppose your density\nis literally 0 outside,",
    "start": "2423717",
    "end": "2430280"
  },
  {
    "text": "and you have some maybe\nwhatever density we want. And literally 0 outside.",
    "start": "2430280",
    "end": "2435770"
  },
  {
    "text": "Then once you go\nbeyond the window m, then the density\ndecays extremely fast.",
    "start": "2435770",
    "end": "2444589"
  },
  {
    "text": "Density just becomes zero. So that's why this is\nO of M sub-Gaussian.",
    "start": "2444590",
    "end": "2452160"
  },
  {
    "text": "It's not-- you still need to--\nto formally prove variables you need kind of to construct--",
    "start": "2452160",
    "end": "2457503"
  },
  {
    "text": "you still need to verify the\ndefinition, of course, right? But I guess it's\nkind of intuitive that it's sub-Gaussian, just\nbecause the tail vanishes",
    "start": "2457503",
    "end": "2465770"
  },
  {
    "text": "completely after you\nhave the window M. And there is a\nstronger claim, which",
    "start": "2465770",
    "end": "2473710"
  },
  {
    "text": "also got the right constant\nhere, I only have O of m. But you can actually get\na stronger claim, which",
    "start": "2473710",
    "end": "2480250"
  },
  {
    "text": "get the exact height constant. So this is saying that if a\nis less than x is less than b,",
    "start": "2480250",
    "end": "2487840"
  },
  {
    "text": "almost surely, so your random\nvariables almost surely bounded between a and b.",
    "start": "2487840",
    "end": "2493030"
  },
  {
    "text": "Then you can prove this, e\nto the lambda x minus ex.",
    "start": "2493030",
    "end": "2501050"
  },
  {
    "text": "This kind of generating\nfunction is always less than e to the lambda squared times\nyou want the quadratic in e",
    "start": "2501050",
    "end": "2508130"
  },
  {
    "text": "lambda squared, quadratic\nlambda in exponent. And you care about the\nconstant, because the constant is the variance proxy.",
    "start": "2508130",
    "end": "2514550"
  },
  {
    "text": "And you can prove\nthat this constant ea minus squared over 8.",
    "start": "2514550",
    "end": "2520400"
  },
  {
    "text": "And this is saying\nthat x is sub-Gaussian",
    "start": "2520400",
    "end": "2527720"
  },
  {
    "text": "with variance proxy\nb minus a over 4.",
    "start": "2527720",
    "end": "2539450"
  },
  {
    "text": "And this is actually\na homework question. It's not that\ntrivial to prove it, actually if you want to\nget a right constant.",
    "start": "2539450",
    "end": "2544700"
  },
  {
    "text": "If you just want to\nget some constant, I think if you want\nto get instead of 8, you want to get 2,\nit's relatively easy.",
    "start": "2544700",
    "end": "2551690"
  },
  {
    "text": "If you want to get 8, you\nneed to do a little bit slightly more about it.",
    "start": "2551690",
    "end": "2557839"
  },
  {
    "text": "We'll have some\nhint in the homework as well to help you to prove it. ",
    "start": "2557840",
    "end": "2564920"
  },
  {
    "text": "All right, so these\nare about all-- so this is all about\nbounding random variables. Basically, this\nsaying that if you",
    "start": "2564920",
    "end": "2571310"
  },
  {
    "text": "have a bounding random variable,\nit's going to be Gaussian. And also this works for Gaussian\nrandom variables, of course,",
    "start": "2571310",
    "end": "2578490"
  },
  {
    "text": "right? So a Gaussian random variable\nhas to be sub-Gaussian, right? So as we motivate it, right, so\nif x is from mu sigma squared,",
    "start": "2578490",
    "end": "2586460"
  },
  {
    "text": "then I guess formally, you\ncan prove the following. You can show that e to\nthe lambda x minus ex,",
    "start": "2586460",
    "end": "2594750"
  },
  {
    "text": "you can compute this. Actually, this is equals to\nexactly e to the sigma squared lambda squared over 2.",
    "start": "2594750",
    "end": "2601280"
  },
  {
    "text": "So that's why it's sub-Gaussian\nwith variance proxy sigma",
    "start": "2601280",
    "end": "2611480"
  },
  {
    "text": "squared.  I think these are the--",
    "start": "2611480",
    "end": "2617220"
  },
  {
    "text": "bounding random variables\nand Gaussian random variables",
    "start": "2617220",
    "end": "2622230"
  },
  {
    "text": "are probably the most\nimportant examples of sub-Gaussian\nrandom variables.",
    "start": "2622230",
    "end": "2629700"
  },
  {
    "text": "And just a small-- in the homework, we're going\nto talk about something called",
    "start": "2629700",
    "end": "2635490"
  },
  {
    "text": "sub-exponential\nvariables, which is a weaker version of\nsub-Gaussian random variables.",
    "start": "2635490",
    "end": "2644310"
  },
  {
    "text": "And this is precisely\nto deal with the fact that some random variables\nare not sub-Gaussian, whatever variance\nproxy you choose.",
    "start": "2644310",
    "end": "2650920"
  },
  {
    "text": "So just to give you a rough\nsense on what the homework is about, so here, when you define\na sub-Gaussian random variable,",
    "start": "2650920",
    "end": "2659430"
  },
  {
    "text": "you can-- in this corollary view,\nso this alternative view,",
    "start": "2659430",
    "end": "2665550"
  },
  {
    "text": "here you have t squared. So you insist that the decay\nis exponential in t squared.",
    "start": "2665550",
    "end": "2673470"
  },
  {
    "text": "And that's a relatively\nstrong requirement. And there are random\nvariables that doesn't have this fast decay.",
    "start": "2673470",
    "end": "2679530"
  },
  {
    "text": "So for example, I think\none typical example would be if you take\nthe Gaussian square,",
    "start": "2679530",
    "end": "2684660"
  },
  {
    "text": "if you square the\nGaussian, which becomes a-- I'm blanking on the\nname of what's called,",
    "start": "2684660",
    "end": "2690000"
  },
  {
    "text": "chi squared distribution, right? So that one doesn't have\nthis fast decay of tail.",
    "start": "2690000",
    "end": "2696900"
  },
  {
    "text": "It's not t squared. It's t. So for these random\nvariables, you still want to prove something\nabout concentration.",
    "start": "2696900",
    "end": "2702810"
  },
  {
    "text": "And you can still do\nthem almost the same as sub-Gaussian\nrandom variables,",
    "start": "2702810",
    "end": "2708210"
  },
  {
    "text": "with some minor technique--\nwith some technical kind of differences. And that's what the homework,\none of the homework questions,",
    "start": "2708210",
    "end": "2715559"
  },
  {
    "text": "your homework 1 is about. So all right, cool, so\nany questions so far?",
    "start": "2715560",
    "end": "2723130"
  },
  {
    "start": "2723130",
    "end": "2728789"
  },
  {
    "text": "OK, so now, let's\nprove this theorem",
    "start": "2728790",
    "end": "2734090"
  },
  {
    "text": "about this additivity of a\nsub-Gaussian random variable. So proof of theorem.",
    "start": "2734090",
    "end": "2742160"
  },
  {
    "text": "So our goal is to show that\nthe sum of xi is sub-Gaussian. This is the goal, all right? ",
    "start": "2742160",
    "end": "2751140"
  },
  {
    "text": "So we just use the definition. We start with a definition. The definition is\nthat if you wanted to prove it to be\nsub-Gaussian, you",
    "start": "2751140",
    "end": "2757830"
  },
  {
    "text": "need to look at the moment\ngenerating function. ",
    "start": "2757830",
    "end": "2767130"
  },
  {
    "text": "I have some-- some\ntype of [INAUDIBLE]..",
    "start": "2767130",
    "end": "2775522"
  },
  {
    "text": "OK. ",
    "start": "2775522",
    "end": "2780900"
  },
  {
    "text": "So you look at the moment\ngenerating function. And so here, you can\nsee the nice thing about this, which\nis that you can--",
    "start": "2780900",
    "end": "2789000"
  },
  {
    "text": "because this is exponential,\nit can decompose very easily. So you can write this\nas exponential lambda",
    "start": "2789000",
    "end": "2795480"
  },
  {
    "text": "x1 minus ex1. ",
    "start": "2795480",
    "end": "2804650"
  },
  {
    "text": "And again, because\nit's independent, you can switch the expectation. You can factorize.",
    "start": "2804650",
    "end": "2810830"
  },
  {
    "text": "Each of the xi's\nare independent. So you can switch\nthe expectation with the product\nto get expectation",
    "start": "2810830",
    "end": "2819170"
  },
  {
    "text": "e of lambda x1 minus ex1\ntimes expectation of e lambda",
    "start": "2819170",
    "end": "2826430"
  },
  {
    "text": "x2 minus ex2. ",
    "start": "2826430",
    "end": "2837119"
  },
  {
    "text": "OK. So this is using independence. ",
    "start": "2837120",
    "end": "2843000"
  },
  {
    "text": "And then you just\nsay, I know that each of these random variables\nis sub-Gaussian. So I just bound--\nuse my definition",
    "start": "2843000",
    "end": "2848682"
  },
  {
    "text": "that each of the\nrandom variable is sigma i squared sub-Gaussian. So you bound it by e\nto the lambda 1 lambda",
    "start": "2848682",
    "end": "2856920"
  },
  {
    "text": "squared sigma 1 squared\nover 2 times e to the lambda squared sigma 2 squared over 2. ",
    "start": "2856920",
    "end": "2864780"
  },
  {
    "text": "This is by definition. And then you got this is e\nto the lambda squared over 2",
    "start": "2864780",
    "end": "2873310"
  },
  {
    "text": "times sum of sigma i squared. ",
    "start": "2873310",
    "end": "2878740"
  },
  {
    "text": "And you get this. That means that sum\nof the xi's is sum",
    "start": "2878740",
    "end": "2884980"
  },
  {
    "text": "of sigma squared sub-Gaussian. So this is the variance\nproxy for sum of xi's.",
    "start": "2884980",
    "end": "2896930"
  },
  {
    "text": "And you can see that the benefit\nof using this moment generating function, the exponential,\nis because you can factorize",
    "start": "2896930",
    "end": "2905650"
  },
  {
    "text": "the exponential easily, right? So if you don't use\nexponential, if you use the 4th power\nor the 8th power,",
    "start": "2905650",
    "end": "2912365"
  },
  {
    "text": "right, you wouldn't have\nsuch a nice, simple proof. ",
    "start": "2912365",
    "end": "2922860"
  },
  {
    "text": "And are there questions? ",
    "start": "2922860",
    "end": "2930670"
  },
  {
    "text": "OK, so that's the first\npart of the lecture, right,",
    "start": "2930670",
    "end": "2936170"
  },
  {
    "text": "which is about a sum of\nindependent random variable. And now, I'm going to talk\nabout a more complex function",
    "start": "2936170",
    "end": "2945370"
  },
  {
    "text": "of independent random variables. ",
    "start": "2945370",
    "end": "2950549"
  },
  {
    "text": "So now, I'm going to talk\nabout something like this. How does this kind of\nthings concentrate?",
    "start": "2950550",
    "end": "2956100"
  },
  {
    "text": " And you can see\nthat in some sense,",
    "start": "2956100",
    "end": "2962320"
  },
  {
    "text": "you want to say that\nthis function F, when F is kind of close to a\nsummation in some sense,",
    "start": "2962320",
    "end": "2968840"
  },
  {
    "text": "in some weak sense,\nthen you still have very similar type of bound. That's the spirit.",
    "start": "2968840",
    "end": "2974747"
  },
  {
    "text": "But what does it mean\nby close to summation? We'll see. So here is the theorem, one of\nthe theorem, which is actually",
    "start": "2974747",
    "end": "2983930"
  },
  {
    "text": "something we're going to use\nin our future lecture, which is called McDiarmid inequality.",
    "start": "2983930",
    "end": "2989869"
  },
  {
    "text": " So there is a bunch\nof conditions.",
    "start": "2989870",
    "end": "2997970"
  },
  {
    "text": "So suppose you\nhave a function f, I guess, little f is the\ncapital F I wrote before.",
    "start": "2997970",
    "end": "3003450"
  },
  {
    "text": "So you have a function f\nsatisfy the so-called bounded",
    "start": "3003450",
    "end": "3013140"
  },
  {
    "text": "difference condition.",
    "start": "3013140",
    "end": "3020339"
  },
  {
    "text": "What does the bounded\ndifference condition mean? So it's saying that\nfor every choice of x1",
    "start": "3020340",
    "end": "3027340"
  },
  {
    "text": "up to xn and xi prime.",
    "start": "3027340",
    "end": "3032790"
  },
  {
    "text": "So I guess for every i and\nevery choice of x1 up to xn--",
    "start": "3032790",
    "end": "3038330"
  },
  {
    "text": "by the way here, these\nxi's are little xi's, because here, I haven't got\nany random variables yet.",
    "start": "3038330",
    "end": "3044240"
  },
  {
    "text": "These are just the\ngeneric numbers. So for every i, for\nevery choice of x1",
    "start": "3044240",
    "end": "3051410"
  },
  {
    "text": "up to xi, and for every\nxi prime, which is-- which will be used as\na replacement for xi,",
    "start": "3051410",
    "end": "3056900"
  },
  {
    "text": "if you look at\nthese two qualities, one is that you apply\nf on x1 up to xn.",
    "start": "3056900",
    "end": "3063690"
  },
  {
    "text": "And the other one is that\napply f on x1 up to xn.",
    "start": "3063690",
    "end": "3068770"
  },
  {
    "text": "But replace xi by xi prime. So basically replace one\ncoordinate by something else.",
    "start": "3068770",
    "end": "3077569"
  },
  {
    "text": "And you look at-- if you look at what\nkind of changes you can make by doing this. And you assume that the maximum\nchanges you can make is by ci.",
    "start": "3077570",
    "end": "3089960"
  },
  {
    "text": "So basically, this is\nsaying that you are not very sensitive to-- this function is not\nsensitive to changing",
    "start": "3089960",
    "end": "3099030"
  },
  {
    "text": "a single variable, a single\ninput, a single coordinate",
    "start": "3099030",
    "end": "3106200"
  },
  {
    "text": "of the input. And if you have this bounded\ndifference condition,",
    "start": "3106200",
    "end": "3112880"
  },
  {
    "text": "then you can say\nthat X1 up to Xn,",
    "start": "3112880",
    "end": "3120779"
  },
  {
    "text": "now they are capital X,\nindependent random variable. ",
    "start": "3120780",
    "end": "3128770"
  },
  {
    "text": "And we have probability\nthat f x1 up to xn",
    "start": "3128770",
    "end": "3136950"
  },
  {
    "text": "is deviate from its\nexpectation by t",
    "start": "3136950",
    "end": "3143780"
  },
  {
    "text": "is less than this\nexponential thing minus 2t",
    "start": "3143780",
    "end": "3150410"
  },
  {
    "text": "squared over sum of\nci squared from 1. ",
    "start": "3150410",
    "end": "3158829"
  },
  {
    "text": "So in other words, I\nguess equivalently, you are basically saying that-- ",
    "start": "3158830",
    "end": "3167260"
  },
  {
    "text": "you are essentially\nsaying that fx1 up to xn, this is sub-Gaussian with\nvariance proxy something",
    "start": "3167260",
    "end": "3182880"
  },
  {
    "text": "like sum of ci squared, a big\nO. There are some constants",
    "start": "3182880",
    "end": "3189029"
  },
  {
    "text": "that you may lose\nby doing a limit. ",
    "start": "3189030",
    "end": "3194070"
  },
  {
    "text": "Your variance-- this is\nusing the equivalence of the two definitions, right? So this is the more intuitive\ndefinition of sub-Gaussian.",
    "start": "3194070",
    "end": "3201270"
  },
  {
    "text": "And if you change to\nthe formal definition, you will lose the constant. Can I ask? You suggest that we were\nremoving this as functions of f",
    "start": "3201270",
    "end": "3210000"
  },
  {
    "text": "are kind of input sums. But would you say that\nthose conditions-- so if it looks like a\nsum, but you could have--",
    "start": "3210000",
    "end": "3217530"
  },
  {
    "text": "if xi prime and xi differ\nby greater than ci--",
    "start": "3217530",
    "end": "3222570"
  },
  {
    "text": "Yeah, so-- yeah, that's\na very good question. So I think before, I forgot\nto repeat a question.",
    "start": "3222570",
    "end": "3229740"
  },
  {
    "text": "So from now on, I should\ntry to repeat a question. The question was\nthat I mentioned",
    "start": "3229740",
    "end": "3235080"
  },
  {
    "text": "that you want to make some\nconditions on f, which make it similar to the sum.",
    "start": "3235080",
    "end": "3240360"
  },
  {
    "text": "So and why this is\nsimilar to the sum? So first of all, I think\na small clarification,",
    "start": "3240360",
    "end": "3246210"
  },
  {
    "text": "I guess, by similar is\nactually a very weak sense. You'll see that in some\nsense, all of these conditions",
    "start": "3246210",
    "end": "3253410"
  },
  {
    "text": "becomes, in some sense,\nnot very similar. But I think they are\nonly similar in the sense",
    "start": "3253410",
    "end": "3259260"
  },
  {
    "text": "that you want to make sure that\nno coordinate is very strongly",
    "start": "3259260",
    "end": "3266100"
  },
  {
    "text": "influencing your final outcome. So when you have a sum, so\nif you change one coordinate,",
    "start": "3266100",
    "end": "3271860"
  },
  {
    "text": "you wouldn't influence\nyour final outcome much. And here is the same thing.",
    "start": "3271860",
    "end": "3278470"
  },
  {
    "text": "So basically, I think\nwhether it's a sum or not, it doesn't matter. It's really about whether\nyou have certain kind of Lipschitzness property.",
    "start": "3278470",
    "end": "3285660"
  },
  {
    "text": "So maybe just\nbriefly, also, we can verify that this condition\ncontains the sum, at least.",
    "start": "3285660",
    "end": "3292665"
  },
  {
    "text": "So that probably\nwould be useful. So suppose you have fx1 up\nto xn is equal to sum of xi.",
    "start": "3292665",
    "end": "3300570"
  },
  {
    "text": "And each of the xi is\nbounded by something like bi and I don't want to put ai.",
    "start": "3300570",
    "end": "3308070"
  },
  {
    "text": "And now, suppose you\nchange one of the xi, how much you can change\nthe final outcome? So then you can say that you\nhave the bounded difference",
    "start": "3308070",
    "end": "3317750"
  },
  {
    "text": "condition where ci is\nequals to bi minus ai,",
    "start": "3317750",
    "end": "3323870"
  },
  {
    "text": "because that's the biggest\nchange you can make if you change one coordinate xi.",
    "start": "3323870",
    "end": "3331130"
  },
  {
    "text": "So that's the maximum kind of\nrange of changes for the sub.",
    "start": "3331130",
    "end": "3336442"
  },
  {
    "text": "But you can see that-- you can\nimagine many other functions that have this property, which\ndoesn't look like sum at all,",
    "start": "3336442",
    "end": "3341480"
  },
  {
    "text": "all right?  So indeed, more precisely, I\nthink the kind of the intuition",
    "start": "3341480",
    "end": "3349100"
  },
  {
    "text": "is that you want this\nfunction f to be somewhat Lipschitz in some cases. ",
    "start": "3349100",
    "end": "3357369"
  },
  {
    "text": "Lipschitz are not super\nsensitive to individual things. Yeah, that's the\ngeneral intuition.",
    "start": "3357370",
    "end": "3363149"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "3363149",
    "end": "3369640"
  },
  {
    "text": "Right, so the question was why\nyou just don't-- why don't just assume that f is\nLipschitz, right?",
    "start": "3369640",
    "end": "3374920"
  },
  {
    "text": "So this is a very good question. And the very short\nanswer is that we don't",
    "start": "3374920",
    "end": "3380680"
  },
  {
    "text": "know how to prove that version. We don't know how to prove\nthat if f is Lipschitz,",
    "start": "3380680",
    "end": "3387658"
  },
  {
    "text": "then you have this result. And a longer version\nis that people",
    "start": "3387658",
    "end": "3394630"
  },
  {
    "text": "have been actually trying to--  this is very-- a lot of\nresearchers, especially",
    "start": "3394630",
    "end": "3402099"
  },
  {
    "text": "mathematicians, have\nworked on this area. And there's a\nquestion about what's",
    "start": "3402100",
    "end": "3407170"
  },
  {
    "text": "the right definition\nof Lipschitzness. I guess you probably\nwill see in a moment,",
    "start": "3407170",
    "end": "3412660"
  },
  {
    "text": "I'm going to show two\nmore general version. And they have a different\ndefinition of Lipschitzness,",
    "start": "3412660",
    "end": "3417940"
  },
  {
    "text": "or the intuition\nof Lipschitzness. And they are\nsomewhat complicated. It's not as clean as\nyou expect, just mostly",
    "start": "3417940",
    "end": "3425740"
  },
  {
    "text": "because there are some technical\nchallenges in those cases. And you will see\nalso a case where",
    "start": "3425740",
    "end": "3432010"
  },
  {
    "text": "if xi is sub-Gaussian, then\nyou have a very clean theorem. It just literally, as you said,\nyou just assume f is Lipschitz.",
    "start": "3432010",
    "end": "3439510"
  },
  {
    "text": "We'll get to that in a moment. [INAUDIBLE]",
    "start": "3439510",
    "end": "3446422"
  },
  {
    "start": "3446422",
    "end": "3455860"
  },
  {
    "text": "Right, so I guess\nyour question is that here, you need this\nabsolute bound in some sense.",
    "start": "3455860",
    "end": "3461920"
  },
  {
    "text": "In some sense, to make sure you\nhave this bounded difference condition, right, so\nyou need some things",
    "start": "3461920",
    "end": "3469510"
  },
  {
    "text": "that kind of absolutely--\nto be absolutely bounded. For example, in sum\ncase, where you need xi's to be absolutely bounded\nbetween ai and bi, right?",
    "start": "3469510",
    "end": "3478000"
  },
  {
    "text": "And this is not very-- this is a little bit\ndifferent from the intuition we had about sub-Gaussian.",
    "start": "3478000",
    "end": "3483160"
  },
  {
    "text": "Before, we were saying that\nif each random variable has a fast tail, then the\nsum also has a fast tail.",
    "start": "3483160",
    "end": "3488289"
  },
  {
    "text": "But here, you need absolute-- some kind of absolute\nrestrictions, right? So this is actually related\nto the answer I had before.",
    "start": "3488290",
    "end": "3497200"
  },
  {
    "text": "If you look at all\nthe technical details, actually, it's not that easy\nto deal with a tail that",
    "start": "3497200",
    "end": "3503109"
  },
  {
    "text": "can go to infinite. So there are some\ntechnical challenges here, which prevent us to have\nsomething super clean,",
    "start": "3503110",
    "end": "3512599"
  },
  {
    "text": "I would say.  So for example, if you\nknow xi is sub-Gaussian,",
    "start": "3512600",
    "end": "3519310"
  },
  {
    "text": "we will see that you have\na very clean theorem. But if you don't know\nxi is sub-Gaussian, then this is kind of\ntechnically very complicated",
    "start": "3519310",
    "end": "3527890"
  },
  {
    "text": "to deal with the tail\nof each of the xi. And in some sense, you\ncan imagine, right?",
    "start": "3527890",
    "end": "3533690"
  },
  {
    "text": "So maybe this is all\na bit too advanced, but for example, if you have\nxi, so the tail is sub-Gaussian.",
    "start": "3533690",
    "end": "3540190"
  },
  {
    "text": "Suppose xi is just Gaussian. And if f can square\nit, so suppose",
    "start": "3540190",
    "end": "3547030"
  },
  {
    "text": "in the function f you\nsquare xi inside somewhere. Now, xi becomes xi squared. And the tail becomes\nslower, as I said.",
    "start": "3547030",
    "end": "3553359"
  },
  {
    "text": "So when you square\nit, it becomes chi square distribution. The tail becomes slower.",
    "start": "3553360",
    "end": "3558755"
  },
  {
    "text": "And if you take the fourth\npower, it becomes even slower. So you have to somehow\nbalance this, right?",
    "start": "3558755",
    "end": "3564049"
  },
  {
    "text": "It's not only about the input. It's also about\nwhat f does, right? If f does something super\nbad to-- for example square",
    "start": "3564050",
    "end": "3569860"
  },
  {
    "text": "the Gaussian or raise the\nGaussian to the higher power, then the tail becomes slower. And your concentration\nbecomes worse.",
    "start": "3569860",
    "end": "3576260"
  },
  {
    "text": "So that's kind of the challenge. ",
    "start": "3576260",
    "end": "3584210"
  },
  {
    "text": "Yeah, so let me proceed\nwith a more general version. And then I'm going to talk\nabout the Gaussian version.",
    "start": "3584210",
    "end": "3589310"
  },
  {
    "text": "And then at the end,\nsuppose I have time, I'm going to prove this theorem. So this theorem is something\nwe can prove ourselves,",
    "start": "3589310",
    "end": "3595970"
  },
  {
    "text": "without doing a\nlot of hybrid work. But the theorem I\nwill introduce next is somewhat kind of a\nvery challenging proof.",
    "start": "3595970",
    "end": "3603740"
  },
  {
    "text": "So this is a more\ngeneral version. ",
    "start": "3603740",
    "end": "3610170"
  },
  {
    "text": "So I think this is theorem\n3.18 in the reference book by--",
    "start": "3610170",
    "end": "3617292"
  },
  {
    "text": "I guess if you look\nat the lecture notes, there is a formal-- this is van Handel. ",
    "start": "3617292",
    "end": "3625490"
  },
  {
    "text": "So it's a book on\nprobability theory. So in this book, they\nbasically-- what happens",
    "start": "3625490",
    "end": "3632630"
  },
  {
    "text": "is that they extend\nthis bounded difference condition to something milder.",
    "start": "3632630",
    "end": "3639529"
  },
  {
    "text": "And the definition-- so you\nstart with some definition. This is b minus i.",
    "start": "3639530",
    "end": "3646920"
  },
  {
    "text": "Let's define this to be fx1\nup to xn minus you take the e",
    "start": "3646920",
    "end": "3653250"
  },
  {
    "text": "over z fx1 up to x minus 1 z.",
    "start": "3653250",
    "end": "3658455"
  },
  {
    "start": "3658455",
    "end": "3665945"
  },
  {
    "text": "So basically, you're\nsaying that if you look at x and you change\none of the coordinate,",
    "start": "3665945",
    "end": "3671290"
  },
  {
    "text": "and you want to see how much\nyou can make it smaller. ",
    "start": "3671290",
    "end": "3677349"
  },
  {
    "text": "So because this quantity\nis always larger than zero. So basically, you are\nsaying that how much you can make it smaller by changing\none coordinate z, right?",
    "start": "3677350",
    "end": "3685090"
  },
  {
    "text": "e, you can just think of e\nas mean, so minimum, right? So the difference\nbetween this and before",
    "start": "3685090",
    "end": "3692590"
  },
  {
    "text": "is that before, you require. So basically before,\nin McDiarmid, you",
    "start": "3692590",
    "end": "3700990"
  },
  {
    "text": "require di minus fx to be\nless than ci for every x.",
    "start": "3700990",
    "end": "3709320"
  },
  {
    "text": "But here, you don't make that-- you don't insist that now. You have-- at least you have an\nx as an argument of this dif,",
    "start": "3709320",
    "end": "3716720"
  },
  {
    "text": "right? So it defines sensitivity\nat every point. You have-- you didn't assume\na global sensitivity thing.",
    "start": "3716720",
    "end": "3722309"
  },
  {
    "text": "You talk about a\nsensitivity at x. That's a quantity. And then you can also\ndefine the sensitivity",
    "start": "3722310",
    "end": "3729940"
  },
  {
    "text": "on the other side,\nwhich is just sup. ",
    "start": "3729940",
    "end": "3745970"
  },
  {
    "text": "And now, these are two\nfunctions that measures the sensitivity at every point. But they are not\nglobal sensitivity.",
    "start": "3745970",
    "end": "3752110"
  },
  {
    "text": "And now you can define a\nglobal sensitivity, d plus, which is the sup over all x1.",
    "start": "3752110",
    "end": "3761000"
  },
  {
    "text": "Now, you take sup. But before taking\nsup, what's inside the sup is the sum\nof this squared.",
    "start": "3761000",
    "end": "3773355"
  },
  {
    "text": " So basically, the-- let me just\nwrite down all the definitions",
    "start": "3773355",
    "end": "3780580"
  },
  {
    "text": "and then interpret them. ",
    "start": "3780580",
    "end": "3791170"
  },
  {
    "text": "This is minus 1. ",
    "start": "3791170",
    "end": "3800359"
  },
  {
    "text": "And then maybe let me\nwrite a conclusion. So you get that probability of\nx1 up to xn minus expectation",
    "start": "3800360",
    "end": "3810160"
  },
  {
    "text": "f larger than t is\nless than expectation-- is exponential minus t\nsquared over 4 d minus.",
    "start": "3810160",
    "end": "3819718"
  },
  {
    "text": "So you have a little bit\ndifferent bound for upper side and lower side,\nwhich is probably not important for many cases.",
    "start": "3819718",
    "end": "3825940"
  },
  {
    "text": "But just for the\nsake of completeness, let's write both of them. ",
    "start": "3825940",
    "end": "3834080"
  },
  {
    "text": "4d plus. So I guess x1 and xn are\nindependent, of course.",
    "start": "3834080",
    "end": "3841250"
  },
  {
    "start": "3841250",
    "end": "3846730"
  },
  {
    "text": "So that's the-- so\nthis is the theory. So I guess the important\nthing is that what is this d plus and d minus?",
    "start": "3846730",
    "end": "3852100"
  },
  {
    "text": "And how does it different\nfrom McDiarmid, right? So basically, I\nthink the difference",
    "start": "3852100",
    "end": "3859540"
  },
  {
    "text": "is that when you do\nthe ci in the McDiarmid is you take sup\nover x1 up to xn.",
    "start": "3859540",
    "end": "3868045"
  },
  {
    "text": "And then di plus fx 1 up to xn,\nyour first x sup, all right?",
    "start": "3868045",
    "end": "3874660"
  },
  {
    "text": "This is a ci, right, which\nis a global sensitivity for the i-th correlate. And then the sum of ci\nsquared, the variance proxy",
    "start": "3874660",
    "end": "3881500"
  },
  {
    "text": "in the McDiarmid is you\ntake sum over i from 1 to n. Then you take sup\nover x1 up to xn.",
    "start": "3881500",
    "end": "3889539"
  },
  {
    "text": "Pi plus f x1 up to xn squared.",
    "start": "3889540",
    "end": "3895740"
  },
  {
    "text": " So basically, you look\nat a global sensitivity",
    "start": "3895740",
    "end": "3900930"
  },
  {
    "text": "for every accord. And you take the sum over it. And here, the difference is\nthat this d plus or d minus,",
    "start": "3900930",
    "end": "3907750"
  },
  {
    "text": "so they are-- you\nare first looking-- you are taking sum\nof the sensitivity over all coordinates\nat this point x,",
    "start": "3907750",
    "end": "3915010"
  },
  {
    "text": "you first take the sum. And then you take the sup. ",
    "start": "3915010",
    "end": "3922320"
  },
  {
    "text": "So it's probably-- it's\nnot that easy to find a concrete example to see\nthe differences of these two.",
    "start": "3922320",
    "end": "3928470"
  },
  {
    "text": "But I guess you can imagine\nthe order of doing the sup and the sum does matter.",
    "start": "3928470",
    "end": "3935125"
  },
  {
    "text": "So it's possible\nthat, for example, you have a point x, such that\nonly for one coordinate, you are very sensitive.",
    "start": "3935125",
    "end": "3940440"
  },
  {
    "text": "And for other coordinates,\nyou are not very sensitive. So then you take the sum. And then take the maximum.",
    "start": "3940440",
    "end": "3946350"
  },
  {
    "text": "It's more advantage to do that. ",
    "start": "3946350",
    "end": "3951690"
  },
  {
    "text": "And in some sense, I\nthink the mathematicians spend a lot of time\nthinking about how do you change the order.",
    "start": "3951690",
    "end": "3957120"
  },
  {
    "text": "So the best thing\nyou want to do is you take the sup at the\nvery end, so like this one.",
    "start": "3957120",
    "end": "3963132"
  },
  {
    "text": "This one, actually,\nthere is a small sup somewhere in the middle, because\nin our definition of p of f,",
    "start": "3963133",
    "end": "3968250"
  },
  {
    "text": "you still have this inf. So the best thing\nwould be that you just define the sensitivity for\nevery thing, like a gradient.",
    "start": "3968250",
    "end": "3975480"
  },
  {
    "text": "And then you take\nsup at the very end, which is what I'm going to\nshow for Gaussian distribution.",
    "start": "3975480",
    "end": "3980823"
  },
  {
    "text": "But this is the best we can\nknow for general distribution, right? So you look at a sensitivity\nat every coordinate.",
    "start": "3980823",
    "end": "3987807"
  },
  {
    "text": "And you take the sum\nof all the sensitivity. And then you take sup of f. But the sensitivity\nhave to be defined--",
    "start": "3987807",
    "end": "3994220"
  },
  {
    "text": "to be defined in this instance. ",
    "start": "3994220",
    "end": "4002515"
  },
  {
    "text": "Does it make some sense? Yeah. I'm not expecting you to\nunderstand all the nuances.",
    "start": "4002515",
    "end": "4009360"
  },
  {
    "text": "I don't even understand\nexactly all the nuances. I need to open a book to see-- to find the cases where\nthere is a difference.",
    "start": "4009360",
    "end": "4016050"
  },
  {
    "text": "I think there are actually,\nindeed, quite some differences between these two inequalities.",
    "start": "4016050",
    "end": "4021109"
  },
  {
    "text": "But it's not like-- you probably wouldn't be\nable to see the differences.",
    "start": "4021110",
    "end": "4028480"
  },
  {
    "text": "OK, and now, let's\nanswer this question about what happens if all the\nxi's are unbounded, right?",
    "start": "4028480",
    "end": "4037530"
  },
  {
    "text": "So what happens if x1\nup to xns are unbounded?",
    "start": "4037530",
    "end": "4047410"
  },
  {
    "text": "If these are unbounded, like\nGaussian random variable, even you take f to be\nyour sum, you probably wouldn't satisfy the bounded\ndifference condition.",
    "start": "4047410",
    "end": "4053950"
  },
  {
    "text": "You wouldn't satisfy\nthis condition here either in this improved case. Because here, there\nis an inf here.",
    "start": "4053950",
    "end": "4060660"
  },
  {
    "text": "So even f is a sum\nand xi's are Gaussian, this one would be infinity.",
    "start": "4060660",
    "end": "4065930"
  },
  {
    "text": "Because there's no bound\nfor any individual-- there's no absolute bounds for\nany individual random variable.",
    "start": "4065930",
    "end": "4072853"
  },
  {
    "text": "So that's the next question. How do we deal with the\ncase when x1 up to xn are not bounded? And there are some existing\nresults along this line.",
    "start": "4072853",
    "end": "4082069"
  },
  {
    "text": "So the first result is\ncalled Poincare inequality, which is one of the very\nbeautiful results also",
    "start": "4082070",
    "end": "4090100"
  },
  {
    "text": "for other reasons, not only for\nthe reason for concentration inequality, but also for\nother reasons not related",
    "start": "4090100",
    "end": "4098170"
  },
  {
    "text": "to this course. So this inequality is\nsaying the following. So if x1 up to xn are\nGaussian, which means 0 and 1.",
    "start": "4098170",
    "end": "4111380"
  },
  {
    "text": "And you have some\nfunction f, and you can look at the variance\nof this function.",
    "start": "4111380",
    "end": "4117538"
  },
  {
    "text": "You didn't prove that\nthis is sub-Gaussian, you only showed a bound\non the variance, which is something necessary to have.",
    "start": "4117538",
    "end": "4122734"
  },
  {
    "text": "So if you don't have a\nbound on the variance, you probably wouldn't be able\nto show it is sub-Gaussian.",
    "start": "4122734",
    "end": "4127759"
  },
  {
    "text": "The variance is less than--\nthis is exactly as suggested before in the question. So this is less than\nthe gradient squared.",
    "start": "4127760",
    "end": "4139839"
  },
  {
    "text": "And you take expectation\nof the random variable x. So this is the expectation\nof the gradient",
    "start": "4139840",
    "end": "4145869"
  },
  {
    "text": "of this random variable. So this is, in some sense, the\nideal type of right hand side",
    "start": "4145870",
    "end": "4151630"
  },
  {
    "text": "that you would hope for. So the concentration of\nthis random variable f",
    "start": "4151630",
    "end": "4157359"
  },
  {
    "text": "is somehow controlled\nby how sensitive, how Lipschitz the function is.",
    "start": "4157359",
    "end": "4163930"
  },
  {
    "text": "So this is the idealistic\nand basically best kind of thing you can hope for.",
    "start": "4163930",
    "end": "4171170"
  },
  {
    "text": "But the limitation here is\nthat on the left hand side, you only control the variance. You didn't control\nthe tail explicitly.",
    "start": "4171170",
    "end": "4178630"
  },
  {
    "text": "So if you want to turn the\nvariance to the a tail bound, you have to use the Chebyshev. You get 1 over t squared bound.",
    "start": "4178630",
    "end": "4185140"
  },
  {
    "text": "And you can also deal with this\nwith other kind of Gaussian variable. It doesn't have to\nbe mean 0 and 1.",
    "start": "4185140",
    "end": "4192189"
  },
  {
    "text": "That's easy. And the strongest thing\nhere is the following.",
    "start": "4192189",
    "end": "4200760"
  },
  {
    "text": "So here is the\nstronger theorem, which we can deal with the tail. ",
    "start": "4200760",
    "end": "4207900"
  },
  {
    "text": "So here, you suppose f is\nL Lipschitz with respect",
    "start": "4207900",
    "end": "4216710"
  },
  {
    "text": "to Euclidean measurement,\nEuclidean distance--",
    "start": "4216710",
    "end": "4224590"
  },
  {
    "text": "sorry, distance,",
    "start": "4224590",
    "end": "4233130"
  },
  {
    "text": "Which is saying that fx minus fy\nis less than L times x minus y",
    "start": "4233130",
    "end": "4241199"
  },
  {
    "text": "squared for every xy. So in some sense,\nthis is saying that--",
    "start": "4241200",
    "end": "4248519"
  },
  {
    "text": "basically, this is saying\nthat the gradient of fx is uniformly\nbounded by L, right?",
    "start": "4248520",
    "end": "4256579"
  },
  {
    "text": "So you can see that this\nis different from this one, because here, you require the\ngradient, for every point,",
    "start": "4256580",
    "end": "4263329"
  },
  {
    "text": "to be less than L. And above, you require\nthe average gradient to be something small.",
    "start": "4263330",
    "end": "4269659"
  },
  {
    "text": "So here, we make a\nstrong assumption to say that this function is\njust the global Lipschitz.",
    "start": "4269660",
    "end": "4274940"
  },
  {
    "text": "And then you can have a\nstronger bound on the tail. So now let x1 up to xn\nbe id from Gaussian.",
    "start": "4274940",
    "end": "4286580"
  },
  {
    "text": "And now you can have the tail\nbound that would move like f1.",
    "start": "4286580",
    "end": "4295120"
  },
  {
    "text": "So this larger than t is\nless than 2 exponential minus t squared over 2 L squared.",
    "start": "4295120",
    "end": "4302900"
  },
  {
    "text": "So basically fx is L\nsub-Gaussian, maybe O if I have sub-Gaussian.",
    "start": "4302900",
    "end": "4309468"
  },
  {
    "text": " But the L is not\nexpected gradient. The L is the absolute\nbound on the gradient.",
    "start": "4309468",
    "end": "4318270"
  },
  {
    "text": "So you can kind\nof see the flavor of all of this\nconcentration inequality, it really depends on when\nyou take the sup when",
    "start": "4318270",
    "end": "4324690"
  },
  {
    "text": "you take the expectation. For different kind\nof conditions, you can have different theorems\nwith different strengths.",
    "start": "4324690",
    "end": "4332637"
  },
  {
    "text": " Any questions? [INAUDIBLE]",
    "start": "4332638",
    "end": "4340670"
  },
  {
    "start": "4340670",
    "end": "4365120"
  },
  {
    "text": "I don't think I know the exact\nresult off the top of my head.",
    "start": "4365120",
    "end": "4371070"
  },
  {
    "text": "I think-- I think\nthe higher moment--",
    "start": "4371070",
    "end": "4376648"
  },
  {
    "text": "could you get a higher\nmoment from the one below? I guess, I think if you\nwant to have higher moment, you have to assume\nsomething stronger.",
    "start": "4376648",
    "end": "4383429"
  },
  {
    "text": "That's my hunch. So for example, this one below\nwill give you a higher moment.",
    "start": "4383430",
    "end": "4390060"
  },
  {
    "text": "So I'm not sure whether you\ncan have a higher moment bound that has weaker\nconditions than this.",
    "start": "4390060",
    "end": "4397829"
  },
  {
    "text": "I don't know. Also, I don't know\ntoo much about PDEs.",
    "start": "4397830",
    "end": "4402850"
  },
  {
    "text": "So I could miss. I don't know everything. This is the only thing I know. ",
    "start": "4402850",
    "end": "4411690"
  },
  {
    "text": "But indeed, this\nPoincare inequality has a lot of different\napplications, not only here. ",
    "start": "4411690",
    "end": "4419320"
  },
  {
    "text": "So we have-- this is-- we have 15 minutes--",
    "start": "4419320",
    "end": "4424710"
  },
  {
    "text": "we have 10 minutes. So it's a little bit\nchallenging for me",
    "start": "4424710",
    "end": "4433720"
  },
  {
    "text": "to give the full proof for\nthe McDiarmid inequality in 10 minutes, but I think\nI would try a little bit.",
    "start": "4433720",
    "end": "4439720"
  },
  {
    "text": "If I couldn't have the full\nproof, I can give you a sketch. So that's the last thing\nI was planning to do.",
    "start": "4439720",
    "end": "4445960"
  },
  {
    "text": "So for all of the\ninequality above, like this Poincare inequality,\nthis tail bound for Gaussian,",
    "start": "4445960",
    "end": "4453130"
  },
  {
    "text": "I think they are beyond\nthe scope of this course. We are already doing a lot of\nthings in the technical part.",
    "start": "4453130",
    "end": "4460340"
  },
  {
    "text": "So these things,\nprobably, even I'd do it, I would just invoke a\ntheorem from a book.",
    "start": "4460340",
    "end": "4466060"
  },
  {
    "text": "So you don't need\nto know the proof. For the McDiarmid\ninequality, I don't think you need to know the proof. But I think the proof is kind\nof interesting to some extent.",
    "start": "4466060",
    "end": "4473810"
  },
  {
    "text": "So it's probably worth showing. So let's try that in\nthe next 10 minutes.",
    "start": "4473810",
    "end": "4478960"
  },
  {
    "start": "4478960",
    "end": "4487739"
  },
  {
    "text": "So we care about bounding. We care about\nsomething like this. And we have the bounded\ndifference condition.",
    "start": "4487740",
    "end": "4494730"
  },
  {
    "text": "And the high level intuition\nis that you want to--",
    "start": "4494730",
    "end": "4500710"
  },
  {
    "text": "so this one can\ncorrelate f of x1 up to xn is kind\nof like something--",
    "start": "4500710",
    "end": "4505940"
  },
  {
    "text": "it could be a very complex\nfunction, complicated function of x1 up to xn. But somehow, you still\nwant to reduce it",
    "start": "4505940",
    "end": "4511960"
  },
  {
    "text": "to a sum in some sense. But a reduction is not that--\nit's not straightforward,",
    "start": "4511960",
    "end": "4518170"
  },
  {
    "text": "the reduction is like this. So the way you do\nit is the following. So you say that-- you defines a sequence\nof random variables.",
    "start": "4518170",
    "end": "4524290"
  },
  {
    "text": "Let's define z0 to be the\nexpectation of x1 up to xn.",
    "start": "4524290",
    "end": "4531350"
  },
  {
    "text": "So this is just nothing. It's just a scalar,\nwhich is a constant.",
    "start": "4531350",
    "end": "4536780"
  },
  {
    "text": "And then define z1 to be\nexpectation of f x1 up",
    "start": "4536780",
    "end": "4543940"
  },
  {
    "text": "to xn conditional x1. So what does this mean?",
    "start": "4543940",
    "end": "4549179"
  },
  {
    "text": "This is a function. This is a function of x1. So basically, z1 is\na function of x1.",
    "start": "4549180",
    "end": "4556390"
  },
  {
    "text": "But you average out\nall the other xi's. ",
    "start": "4556390",
    "end": "4563800"
  },
  {
    "text": "And you can also\ndefine zi, which is the expectation\nof x1 up to xn",
    "start": "4563800",
    "end": "4572430"
  },
  {
    "text": "conditional the first\ni random variable. So this is a function\nof x1 up to xi.",
    "start": "4572430",
    "end": "4582560"
  },
  {
    "text": "So given x1 up to xi,\nthis becomes a scalar, because all the other\nrandomness got ever stopped.",
    "start": "4582560",
    "end": "4589159"
  },
  {
    "text": "So in some sense,\nyou can see that z0 doesn't have any randomness. z1 has a little\nrandomness, because it's",
    "start": "4589160",
    "end": "4594590"
  },
  {
    "text": "a function of\nrandom variable, x1. So it's a random variable. And zi has more and\nmore randomness. And zn is finally\nwhat you care about,",
    "start": "4594590",
    "end": "4602699"
  },
  {
    "text": "which is the fully random case. And the important thing\nis that you care about zn",
    "start": "4602700",
    "end": "4610099"
  },
  {
    "text": "minus the z0, the f\nminus the expectations. And you can decompose this\ninto a sequence of things.",
    "start": "4610100",
    "end": "4618070"
  },
  {
    "text": "So like this telescoping sum. ",
    "start": "4618070",
    "end": "4628420"
  },
  {
    "text": "And this is what I mean\nby reduction to the sum. So basically, now you have\na sum of random variables.",
    "start": "4628420",
    "end": "4633429"
  },
  {
    "text": "And you somehow kind of\nthink of them as independent in some sense. They're definitely not\nexactly independent.",
    "start": "4633430",
    "end": "4639430"
  },
  {
    "text": "But you're going--\nwe use the proof that you use for the summation.",
    "start": "4639430",
    "end": "4645905"
  },
  {
    "text": "That's what we want to see.  And if you-- look\nat this, right.",
    "start": "4645905",
    "end": "4651800"
  },
  {
    "text": "So this is a function of x1. And this is a function of x2, of\nx1 and x2, so on and so forth.",
    "start": "4651800",
    "end": "4661380"
  },
  {
    "text": "And this is a function\nof x1 up to xn. ",
    "start": "4661380",
    "end": "4667687"
  },
  {
    "text": "This depends on all\nthe random variables. OK? And now, let's try to see what\nwe know about each of these z",
    "start": "4667687",
    "end": "4675389"
  },
  {
    "text": "and zi minus zi minus 1. All right? So first of all, we\nknow that for every zi,",
    "start": "4675390",
    "end": "4680470"
  },
  {
    "text": "if you take expectation of\nzi, this is expectation of-- expectation of f x1 up to xn.",
    "start": "4680470",
    "end": "4687790"
  },
  {
    "text": " So in the inside, you have\na function of x1 up to xi.",
    "start": "4687790",
    "end": "4695150"
  },
  {
    "text": "And then also that you averaged\nout all the randomness of x1 up to xi again. So this is-- so this is equals\nto this expectation of f by--",
    "start": "4695150",
    "end": "4705860"
  },
  {
    "text": "this is called a total\nlaw of expectation, right? ",
    "start": "4705860",
    "end": "4712430"
  },
  {
    "text": "You take the expectation\nof the conditional thing, then you get the expectation.",
    "start": "4712430",
    "end": "4718295"
  },
  {
    "text": " So this is equal to this, which\nis equals to basically the z0.",
    "start": "4718295",
    "end": "4729644"
  },
  {
    "start": "4729645",
    "end": "4735880"
  },
  {
    "text": "And then, this means that the\nexpectation of zi minus zi minus 1 is equal to zero.",
    "start": "4735880",
    "end": "4742739"
  },
  {
    "text": "So each of these random\nvariables in this decomposition is mean 0.",
    "start": "4742740",
    "end": "4748290"
  },
  {
    "text": "Unless you have-- so\nbasically, the intuition",
    "start": "4748290",
    "end": "4754070"
  },
  {
    "text": "is that this would define zi\nto be zi minus zi minus 1. What you're going to do is\nthat you're going to have--",
    "start": "4754070",
    "end": "4762110"
  },
  {
    "text": "in some sense, you want to\nbound the moment generating function of each of the di.",
    "start": "4762110",
    "end": "4767980"
  },
  {
    "text": "And then you say that\nbecause the final thing is a sum of the di, you can\nbound the moment generating",
    "start": "4767980",
    "end": "4773590"
  },
  {
    "text": "function of the sum of the di. So let's work on each\nof the di first, right?",
    "start": "4773590",
    "end": "4779740"
  },
  {
    "text": "So I guess I'm going to claim\nthat zi minus zi minus 1",
    "start": "4779740",
    "end": "4791910"
  },
  {
    "text": "is always less than\nci, where the ci is",
    "start": "4791910",
    "end": "4805250"
  },
  {
    "text": "the bounded difference\ncondition in the condition",
    "start": "4805250",
    "end": "4812400"
  },
  {
    "text": "of the McDiarmid inequality. So how do I do that? I guess-- let me see whether\nI can simplify this proof",
    "start": "4812400",
    "end": "4819750"
  },
  {
    "text": "a little bit for the sake\nof time, I guess it doesn't.",
    "start": "4819750",
    "end": "4828230"
  },
  {
    "text": "So let's only prove\nit for z1 minus z0, just in the interest of time.",
    "start": "4828230",
    "end": "4834560"
  },
  {
    "text": "So if look at z1,\nz1 is expectation",
    "start": "4834560",
    "end": "4843050"
  },
  {
    "text": "of x1 up to xn condition on x1.",
    "start": "4843050",
    "end": "4848309"
  },
  {
    "text": "And if you-- so I\nguess you can replace",
    "start": "4848310",
    "end": "4863120"
  },
  {
    "text": "the first one by the sup over\nall the possible choices of x1,",
    "start": "4863120",
    "end": "4868760"
  },
  {
    "text": "right?  And after you do\nthis, this quantity",
    "start": "4868760",
    "end": "4878980"
  },
  {
    "text": "is not a function of x1 anymore. So it doesn't matter whether\nyou condition x1 or not. So you literally just get\nexpectation sup fz x2 up to xn.",
    "start": "4878980",
    "end": "4891429"
  },
  {
    "start": "4891430",
    "end": "4897940"
  },
  {
    "text": "So-- let me see.",
    "start": "4897940",
    "end": "4905645"
  },
  {
    "text": " And also, you know that z1\nis bigger than expectation",
    "start": "4905645",
    "end": "4913560"
  },
  {
    "text": "if, for the same reason, xn.",
    "start": "4913560",
    "end": "4920010"
  },
  {
    "text": "So sometimes, you have\nsome kind of upper bound lower bound for z1.",
    "start": "4920010",
    "end": "4925562"
  },
  {
    "text": "I guess these two\nqualities are not exactly useful for the bound. What's really useful is this.",
    "start": "4925562",
    "end": "4931380"
  },
  {
    "text": "If you look at z1 minus\nz0, this is then-- ",
    "start": "4931380",
    "end": "4939449"
  },
  {
    "text": "so this is expectation\nof x1 up to xn",
    "start": "4939450",
    "end": "4947750"
  },
  {
    "text": "conditioned on x1 minus f x1\nexpectation f x1 up to xn.",
    "start": "4947750",
    "end": "4954720"
  },
  {
    "text": " So you can bound this\nby expectation sup",
    "start": "4954720",
    "end": "4962290"
  },
  {
    "text": "using what we have done above. ",
    "start": "4962290",
    "end": "4969860"
  },
  {
    "text": "And minus expectation\nof f x1 xn.",
    "start": "4969860",
    "end": "4976360"
  },
  {
    "text": "So here, both the-- and then you can\nput this inside. ",
    "start": "4976360",
    "end": "4987810"
  },
  {
    "text": "I think it's slightly\nconfusing when you really look at the math. But intuitively,\nwhat you're saying",
    "start": "4987810",
    "end": "4992880"
  },
  {
    "text": "is that what the difference\nbetween z1 and z0 is only one coordinate.",
    "start": "4992880",
    "end": "4998280"
  },
  {
    "text": "But we know that if you\nchange that one coordinate, you cannot make much\ndifference, right?",
    "start": "4998280",
    "end": "5004250"
  },
  {
    "text": "So that's what we know. For any x2 up to xn,\nif you changed only x1,",
    "start": "5004250",
    "end": "5009295"
  },
  {
    "text": "you wouldn't make\nmuch of a difference. That's why z1, z0 wouldn't\nmake much of a difference,",
    "start": "5009295",
    "end": "5014600"
  },
  {
    "text": "because the only\nthing different is x1. OK, but maybe let me\nhave the formal proof.",
    "start": "5014600",
    "end": "5022580"
  },
  {
    "text": "So on the other hand,\nyou can also prove that-- the same thing. So you can prove this\nis larger than inf z.",
    "start": "5022580",
    "end": "5028325"
  },
  {
    "start": "5028325",
    "end": "5034230"
  },
  {
    "text": "So basically, I'm\nbasically trying to say that the difference\nbetween z1 and z0 is upper and lower bounded\nby the extreme lower case,",
    "start": "5034230",
    "end": "5041505"
  },
  {
    "text": "right, where you pick\nyour z in the worst case. And this means that if\nyou define this to be a1",
    "start": "5041505",
    "end": "5048750"
  },
  {
    "text": "and you define this\nto be a2, then a-- maybe let's call this-- sorry,\nlet's call this b1 and this a1.",
    "start": "5048750",
    "end": "5057670"
  },
  {
    "text": "So you have upper bound and\nlower bound on z1 minus z0. And you can show what's the\nupper bound and lower bound.",
    "start": "5057670",
    "end": "5065180"
  },
  {
    "text": "So the b1 must be a1. So this will be expectation sup.",
    "start": "5065180",
    "end": "5073330"
  },
  {
    "text": "That's the extreme low case\nminus the inf, all right? ",
    "start": "5073330",
    "end": "5084020"
  },
  {
    "text": "So this is exactly the ci's\nthat we defined, right? So if you change your\nrandom variable--",
    "start": "5084020",
    "end": "5089570"
  },
  {
    "text": "you change your inputs\nin the first coordinate, what you can change. So the maximum change is c1.",
    "start": "5089570",
    "end": "5095420"
  },
  {
    "text": "So this is less than\nc1 by condition.",
    "start": "5095420",
    "end": "5102219"
  },
  {
    "text": "So basically, this is saying\nthat z1 is between b1 and a1.",
    "start": "5102220",
    "end": "5108810"
  },
  {
    "text": "And b1 minus a1 is less than c1. So this is saying that each\nof the random variables",
    "start": "5108810",
    "end": "5115560"
  },
  {
    "text": "z1 minus z0 is bounded\nin your small interval. And similarly, you can also\nshow that zi minus zi minus 1",
    "start": "5115560",
    "end": "5124560"
  },
  {
    "text": "is bounded between--  is bounded between\nsomething like bi and ai.",
    "start": "5124560",
    "end": "5132550"
  },
  {
    "text": "And bi minus ai is\nalso less than ci. ",
    "start": "5132550",
    "end": "5139060"
  },
  {
    "text": "So recall that our final goal\nis the zn minus z0, which is the sum of zi minus zi minus 1.",
    "start": "5139060",
    "end": "5145600"
  },
  {
    "text": "And we have proved that each\nof these random variables is somewhat bounded in\nsome small interval.",
    "start": "5145600",
    "end": "5152770"
  },
  {
    "text": "And now we can use the\nmoment generating function. So what you do is you\nsay you take expectation",
    "start": "5152770",
    "end": "5159070"
  },
  {
    "text": "of lambda zn minus z0.",
    "start": "5159070",
    "end": "5168150"
  },
  {
    "text": "And this is expectation\ne of lambda sum of zi minus zi minus 1.",
    "start": "5168150",
    "end": "5175960"
  },
  {
    "text": "So the first thing we have\nto do is to factorize them in some way, right?",
    "start": "5175960",
    "end": "5181000"
  },
  {
    "text": "So how do we factorize them? We just use the conditional-- ",
    "start": "5181000",
    "end": "5188260"
  },
  {
    "text": "we kind of do the chain--\nin some sense of the chain. So what you do is that your\nfirst condition down, x1 up",
    "start": "5188260",
    "end": "5198119"
  },
  {
    "text": "to xn minus 1. So then you have this\nexpectation e lambda",
    "start": "5198120",
    "end": "5204120"
  },
  {
    "text": "zi minus zi minus 1 conditional\nx1 up to xn minus 1.",
    "start": "5204120",
    "end": "5211160"
  },
  {
    "text": "And when condition\non it, you get this. And then the rest of the\nthings, it's a function of xn",
    "start": "5211160",
    "end": "5218710"
  },
  {
    "text": "up to xn minus 1. ",
    "start": "5218710",
    "end": "5224080"
  },
  {
    "text": "All right, so this is-- what's inside your condition\nx1 up to xn minus 1,",
    "start": "5224080",
    "end": "5230180"
  },
  {
    "text": "this one only depends on-- ",
    "start": "5230180",
    "end": "5236599"
  },
  {
    "text": "so this is a function\nof x1 up to xn minus 1. And this is the function of xn. So that's why it's\ninside that expectation.",
    "start": "5236600",
    "end": "5243590"
  },
  {
    "text": "And then this term, because zn\nminus zn minus 1 is bounded,",
    "start": "5243590",
    "end": "5251840"
  },
  {
    "text": "and it's bounded\nin a strong sense, in the sense that for every\npossible choice of x, so you know that this is a kind\nof absolute bond for zn",
    "start": "5251840",
    "end": "5259775"
  },
  {
    "text": "minus zn minus 1. So we know that this,\nlambda zn minus zn1, this",
    "start": "5259775",
    "end": "5272139"
  },
  {
    "text": "is less than\nexponential of lambda",
    "start": "5272140",
    "end": "5279190"
  },
  {
    "text": "squared sigma cn squared over 2.",
    "start": "5279190",
    "end": "5286680"
  },
  {
    "text": "This is because if you have\na bounded random variable, and we know that\nit's sub-Gaussian. So you can verify\nthis in various ways.",
    "start": "5286680",
    "end": "5294760"
  },
  {
    "text": "One way to do it is to just-- actually, this will\nshow up in the homework.",
    "start": "5294760",
    "end": "5300960"
  },
  {
    "text": "This is one of the homework\nquestions we defined. So if you have a\nbounded random variable, it's sub-Gaussian, right?",
    "start": "5300960",
    "end": "5307620"
  },
  {
    "text": "And you can bound the\nmoment generating function. And then you can\nreplace this term by--",
    "start": "5307620",
    "end": "5314550"
  },
  {
    "text": " this absolute quantity\ncn squared over 2 and times the sum\nof the other terms.",
    "start": "5314550",
    "end": "5321599"
  },
  {
    "text": " I think this is n minus 1.",
    "start": "5321600",
    "end": "5327000"
  },
  {
    "text": " And then you peel off the\nsecond term again and again. So you do this iteratively.",
    "start": "5327000",
    "end": "5333260"
  },
  {
    "text": "I guess given that we're\nalready running out of time. So look at this.",
    "start": "5333260",
    "end": "5340590"
  },
  {
    "text": "So if you have-- you can do something like this. I guess this is actually 8 if\nyou really do it carefully.",
    "start": "5340590",
    "end": "5348719"
  },
  {
    "text": "So yeah, I guess I\nwill just sketch this.",
    "start": "5348720",
    "end": "5353940"
  },
  {
    "text": "So this means that f\nminus expectation f is equals to sum of\nzi minus zi minus 1 is",
    "start": "5353940",
    "end": "5362810"
  },
  {
    "text": "sub-Gaussian with variance\nproxy sigma squared.",
    "start": "5362810",
    "end": "5375840"
  },
  {
    "text": "I guess that's the\nend of the proof. But this proof is optional. It's just that we\nhave more time. So that's why I show the proof.",
    "start": "5375840",
    "end": "5381352"
  },
  {
    "text": "OK. Any question? ",
    "start": "5381352",
    "end": "5388800"
  },
  {
    "text": "What was the step before the\nequation in the blue circle-- ",
    "start": "5388800",
    "end": "5395940"
  },
  {
    "text": "is that-- [INAUDIBLE]\nAt the end of that line,",
    "start": "5395940",
    "end": "5405460"
  },
  {
    "text": "is that just based\non the [INAUDIBLE].. You mean this one?",
    "start": "5405460",
    "end": "5412740"
  },
  {
    "text": "Yeah, so from here to here? This is just-- it's just\na triple step, I guess,",
    "start": "5412740",
    "end": "5420510"
  },
  {
    "text": "maybe technically what I\nshould write is maybe-- let me do this here.",
    "start": "5420510",
    "end": "5426880"
  },
  {
    "text": "So if you want to do two steps,\nthe first thing is you do this. Sorry, you do-- you just--",
    "start": "5426880",
    "end": "5435280"
  },
  {
    "text": "do the total expectation. You condition-- you first\ncondition x1 up to xn. ",
    "start": "5435280",
    "end": "5443130"
  },
  {
    "text": "We do this, right? So this is the law\nof total expectation. And then you find\nthat this term is",
    "start": "5443130",
    "end": "5449949"
  },
  {
    "text": "a constant when you condition\non x1 up to xn minus 1. So that's why you\ncan move it outside. ",
    "start": "5449950",
    "end": "5458030"
  },
  {
    "text": "Yeah, there's\nnothing deep there. ",
    "start": "5458030",
    "end": "5465059"
  },
  {
    "text": "OK, sounds good. OK cool, so I guess\nsee you next Monday.",
    "start": "5465060",
    "end": "5470555"
  },
  {
    "start": "5470555",
    "end": "5476000"
  }
]