[
  {
    "start": "0",
    "end": "60000"
  },
  {
    "text": "Okay. Welcome back, everyone. Welcome to the second lecture of CS229 Machine Learning.",
    "start": "4640",
    "end": "12465"
  },
  {
    "text": "So where were we? So, uh, in lecture 1, we- we were going through,",
    "start": "12465",
    "end": "19349"
  },
  {
    "text": "uh, some of the review material, linear algebra. So, uh, and today for the, uh,",
    "start": "19350",
    "end": "24884"
  },
  {
    "text": "for today the plan is to finish up the review of linear algebra, some of the topics we could not, um,",
    "start": "24885",
    "end": "31335"
  },
  {
    "text": "cover on Monday, um, review some matrix calculus or multi-variable calculus,",
    "start": "31335",
    "end": "38555"
  },
  {
    "text": "and then also review some probability theory. And with that, we will be finishing all the review.",
    "start": "38555",
    "end": "46040"
  },
  {
    "text": "And from the next class, we are gonna start, uh, with actual machine learning, uh, with linear regression.",
    "start": "46040",
    "end": "52899"
  },
  {
    "text": "Okay. So, uh, a quick recap. So- [NOISE]",
    "start": "53750",
    "end": "68010"
  },
  {
    "start": "60000",
    "end": "240000"
  },
  {
    "text": "so we went over what a vector is and a matrix is, right?",
    "start": "68010",
    "end": "91290"
  },
  {
    "text": "And, uh, we saw some of the- some of the applications of linear algebra. We saw why we need to, um, uh,",
    "start": "91290",
    "end": "99475"
  },
  {
    "text": "study linear algebra, for example, uh, to represent, um, your data,",
    "start": "99475",
    "end": "105329"
  },
  {
    "text": "um, covariance matrices, right?",
    "start": "105330",
    "end": "115980"
  },
  {
    "text": "Kernels. You're not expected to know what a kernel is at this stage,",
    "start": "115980",
    "end": "121280"
  },
  {
    "text": "but, you know, this is just to, um, um, name a few concepts that we'll be using later and- and also, uh,",
    "start": "121280",
    "end": "128179"
  },
  {
    "text": "in general, multivariate calculus, right?",
    "start": "128180",
    "end": "134219"
  },
  {
    "text": "Um, and then we went over some operations such as,",
    "start": "134220",
    "end": "142545"
  },
  {
    "text": "uh, vector-vector operation, like the inner product or the dot product, the outer product.",
    "start": "142545",
    "end": "151165"
  },
  {
    "text": "For the inner product, we- we take two matrices of the same dimension. For an outer product, they need not be the same.",
    "start": "151165",
    "end": "156920"
  },
  {
    "text": "And then we also, um, went over some operations such as matrix vector product, um,",
    "start": "156920",
    "end": "163640"
  },
  {
    "text": "where we saw two interpretations, the dot-product interpretation and the scaling interpretation. Yes, question.",
    "start": "163640",
    "end": "168950"
  },
  {
    "text": "[inaudible] Is that better?",
    "start": "168950",
    "end": "180885"
  },
  {
    "text": "Awesome. Thank you. Uh, we saw the matrix, uh, vector, um, operation.",
    "start": "180885",
    "end": "188220"
  },
  {
    "text": "Um, the both the dot-product view and- and the scaling of the columns, um, uh, interpretation.",
    "start": "188220",
    "end": "194325"
  },
  {
    "text": "And then, uh, we also covered matrix-matrix, um, uh, multiplication.",
    "start": "194325",
    "end": "200370"
  },
  {
    "text": "And we saw a couple of interpretations of that, the- the inner product interpretation and the outer product, um, interpretation.",
    "start": "200370",
    "end": "207665"
  },
  {
    "text": "And then, uh, we re-interpreted matrices as functions or operators",
    "start": "207665",
    "end": "216109"
  },
  {
    "text": "rather than thinking of matrices as a grid of numbers and I'll think of them as an operator where they operate on a vector that you are multiplying.",
    "start": "216110",
    "end": "224690"
  },
  {
    "text": "Think of the vector that you're multiplying a matrix with as the input and the vector that you get as the output, right? And with that function point of view, um,",
    "start": "224690",
    "end": "233930"
  },
  {
    "text": "[NOISE] function interpretation of matrices.",
    "start": "233930",
    "end": "240670"
  },
  {
    "start": "240000",
    "end": "360000"
  },
  {
    "text": "Right? With this function, uh, uh, point of view, we saw some geometrical interpretations of the subspaces,",
    "start": "243440",
    "end": "252770"
  },
  {
    "text": "which is closely related to, um, the rank and the inverse- rank inverse.",
    "start": "253760",
    "end": "265139"
  },
  {
    "text": "And we also covered projections, right?",
    "start": "265140",
    "end": "271900"
  },
  {
    "text": "So projecting, um, a vector onto a subspace essentially means finding",
    "start": "273320",
    "end": "279040"
  },
  {
    "text": "the point on the subspace that is nearest to the vector that we are trying to project, right?",
    "start": "279040",
    "end": "285350"
  },
  {
    "text": "Projections and we saw the, uh, um, the projection matrix. So if you're trying to project a vector v on to the columns of x- of x,",
    "start": "285960",
    "end": "299210"
  },
  {
    "text": "where x is a matrix and v is a vector, then the projection matrix is essentially x x transpose- oh,",
    "start": "299210",
    "end": "306635"
  },
  {
    "text": "sorry, x x transpose x inverse x transpose, right?",
    "start": "306635",
    "end": "315605"
  },
  {
    "text": "This is a matrix, so, um, x is a matrix, x transpose is a matrix, x transpose, x is a matrix,",
    "start": "315605",
    "end": "321485"
  },
  {
    "text": "x inverse is a matrix. You multiply this matrix from, um, x itself from the left and x transpose from the right,",
    "start": "321485",
    "end": "327784"
  },
  {
    "text": "you get one, you know, you still get a matrix. So this matrix is the projection matrix.",
    "start": "327785",
    "end": "332885"
  },
  {
    "text": "And any vector you- you, uh, multiply it with.",
    "start": "332885",
    "end": "337710"
  },
  {
    "text": "Right. You multiply this, uh, matrix with the vector, you get a new vector called u.",
    "start": "338780",
    "end": "346900"
  },
  {
    "text": "And u will be the projection of v onto the subspace spanned by the columns of x, right?",
    "start": "346900",
    "end": "353465"
  },
  {
    "text": "This is an important concept that we'll come back to when we talk about linear regression.",
    "start": "353465",
    "end": "359915"
  },
  {
    "text": "Right? So that's the, uh, projection. And we also started talking about, um, eigenvectors.",
    "start": "359915",
    "end": "367319"
  },
  {
    "start": "360000",
    "end": "675000"
  },
  {
    "text": "Eigenvectors and eigenvalues.",
    "start": "370130",
    "end": "375130"
  },
  {
    "text": "Right. So that's where we left off.",
    "start": "376430",
    "end": "380800"
  },
  {
    "text": "Eigenvectors and eigenvalues, um, are those um, special vectors for square matrices,",
    "start": "381830",
    "end": "390044"
  },
  {
    "text": "um, where the operation of the matrix on the vector does not change the angle,",
    "start": "390045",
    "end": "396170"
  },
  {
    "text": "it only rescales it by some amount. And the eigenvalue is the ratio by which",
    "start": "396170",
    "end": "402185"
  },
  {
    "text": "the vector gets scaled along those, uh, specific directions. So each eigenvector comes with a corresponding eigenvalue.",
    "start": "402185",
    "end": "407809"
  },
  {
    "text": "The remaining vectors which are not eigenvectors, are not eigenvectors in the sense they can change their direction.",
    "start": "407809",
    "end": "413960"
  },
  {
    "text": "Is there a question? Yeah. [inaudible]",
    "start": "413960",
    "end": "427830"
  },
  {
    "text": "So, uh, question. So the question is, do the columns of x span the subspace, uh, u?",
    "start": "427830",
    "end": "433835"
  },
  {
    "text": "So, um, to clarify, x, um, the columns of x define the subspace",
    "start": "433835",
    "end": "439655"
  },
  {
    "text": "onto which we want to project any given vector, right? So x, uh, the v- v over here is some any given vector and we",
    "start": "439655",
    "end": "446810"
  },
  {
    "text": "want to project it onto the- the- the subspace spanned by the columns of x. By taking x, we construct this matrix, you know,",
    "start": "446810",
    "end": "455550"
  },
  {
    "text": "x transpose x inverted, you know, and multiply from the left by x, from the right by x transpose, you get another matrix.",
    "start": "455550",
    "end": "462290"
  },
  {
    "text": "Now, take any vector v and multiply v by this matrix, right.",
    "start": "462290",
    "end": "468915"
  },
  {
    "text": "And you get an output vector u. Now, that u will be in the subspace spanned by",
    "start": "468915",
    "end": "475220"
  },
  {
    "text": "the columns of x and it will be the nearest point to v such that it, you know, uh, uh, it's- it's, um,",
    "start": "475220",
    "end": "482660"
  },
  {
    "text": "it's in the subspace and also nearest to v. Thank you. Thanks for asking that question. Yes, question.",
    "start": "482660",
    "end": "489090"
  },
  {
    "text": "[inaudible]",
    "start": "489090",
    "end": "517534"
  },
  {
    "text": "I'm not sure I followed your question completely, uh, did you mean you wanna add multiple- you want to",
    "start": "517535",
    "end": "523960"
  },
  {
    "text": "consider different vectors that you want to project onto the subspace? [inaudible].",
    "start": "523960",
    "end": "530920"
  },
  {
    "text": "Okay, so we're just different from this way. Yeah. Okay. So let's call them x_1, x_2, x_3 as, okay.",
    "start": "530920",
    "end": "536829"
  },
  {
    "text": "[inaudible]",
    "start": "536830",
    "end": "548560"
  },
  {
    "text": "Yeah. [inaudible] [BACKGROUND]",
    "start": "548560",
    "end": "554529"
  },
  {
    "text": "So the question is, uh, if- if, uh, I want to project V onto the, uh, subspace spanned by the columns of x,",
    "start": "554530",
    "end": "562584"
  },
  {
    "text": "would it be different from what we would get if we were to project V",
    "start": "562585",
    "end": "568060"
  },
  {
    "text": "onto first column of x separately to the second column of X separately to the third column of the x,",
    "start": "568060",
    "end": "574150"
  },
  {
    "text": "and then sum those projections., uh, the answer is no, they will not be the same., uh, just because um,",
    "start": "574150",
    "end": "580210"
  },
  {
    "text": "here's an example., uh, consider this to be, uh, x- x_1- x_1.",
    "start": "580210",
    "end": "588250"
  },
  {
    "text": "So this is like the first column of x, uh, and let this be, uh, the second column of x, um, x_2.",
    "start": "588250",
    "end": "596830"
  },
  {
    "text": "Now, suppose, there is a vector v, and let me, uh, the vector should ideally be out of the subspace.",
    "start": "596830",
    "end": "604089"
  },
  {
    "text": "So imagine there is a- a vector over here, right? And now I want to project this onto the subspace of X, right?",
    "start": "604090",
    "end": "611620"
  },
  {
    "text": "Now, if I were to separately project it to say, uh, x_2 first, this might be, uh,",
    "start": "611620",
    "end": "618070"
  },
  {
    "text": "the projection separately to x_1 first, this might be the projection and then sum them up. It's going to be something here.",
    "start": "618070",
    "end": "623740"
  },
  {
    "text": "But from here, the nearest prediction is- is- is this, right?",
    "start": "623740",
    "end": "629635"
  },
  {
    "text": "If- if you predict them separately and add them up, you don't necessarily reach the nearest point.",
    "start": "629635",
    "end": "635660"
  },
  {
    "text": "Does that- does that make sense? Yes. Question.",
    "start": "635790",
    "end": "641029"
  },
  {
    "text": "Will it be the same for [inaudible] Yes. If- if- if they are- if they are the, uh, uh,",
    "start": "643110",
    "end": "648279"
  },
  {
    "text": "orthonormal to each other, maybe but- but x_1 and x_2 will not be orthonormal.",
    "start": "648280",
    "end": "654780"
  },
  {
    "text": "They're- they could be any- any vectors. I'm not sure about the orthonormal. I- I need to think about that.",
    "start": "654780",
    "end": "660420"
  },
  {
    "text": "But in general, you know, it- it doesn't hold that you can project them separately. Any more questions? All right.",
    "start": "660420",
    "end": "670629"
  },
  {
    "text": "Okay, so moving on. So, uh, next we're gonna, um, focus more on eigenvalues and- and,",
    "start": "673010",
    "end": "681404"
  },
  {
    "start": "675000",
    "end": "1020000"
  },
  {
    "text": "uh, see some of- some of its, uh, properties. So, um- so here's the- the picture that we started with,",
    "start": "681405",
    "end": "692350"
  },
  {
    "text": "um, so imagine an input space in R_3 and this is your input space.",
    "start": "692350",
    "end": "702410"
  },
  {
    "text": "So this is your x-axis, y-axis, z-axis, right?",
    "start": "706050",
    "end": "712945"
  },
  {
    "text": "And, uh, the picture that we- that we kind of, uh, started with to- to build our intuitions was to consider, uh,",
    "start": "712945",
    "end": "723310"
  },
  {
    "text": "a unit ball like a soccer ball that's centered on the origin of- of radius one, right?",
    "start": "723310",
    "end": "729775"
  },
  {
    "text": "And you have the matrix your- your square symmetric matrix A, which is also in R3 and symmetric.",
    "start": "729775",
    "end": "740180"
  },
  {
    "text": "And take the shape and run it through A. Now what does that mean?",
    "start": "743250",
    "end": "750190"
  },
  {
    "text": "What- what does it mean to- I mean, we know how- how to- how we can take a vector and run it through A and get an output vector.",
    "start": "750190",
    "end": "755995"
  },
  {
    "text": "But what does it mean to take a shape and run it through A and get another shape? It just means take every point on the surface separately,",
    "start": "755995",
    "end": "762790"
  },
  {
    "text": "run it through A, and reconstruct the resulting shape on the other side, right? So, um- so this is our input.",
    "start": "762790",
    "end": "771100"
  },
  {
    "text": "Take every point on the surface. So this is a three-dimensional ball, you know, I'm- I'm just trying to do a circle, you know,",
    "start": "771100",
    "end": "776170"
  },
  {
    "text": "it has- it's a three-dimensional ball and run it through A and we",
    "start": "776170",
    "end": "781269"
  },
  {
    "text": "get an ellipsoid, right?",
    "start": "781270",
    "end": "790380"
  },
  {
    "text": "Now, um, at this point, we're still- still talking about symmetric, uh, square matrices.",
    "start": "790380",
    "end": "798475"
  },
  {
    "text": "Now the- the, uh- the principal axis of",
    "start": "798475",
    "end": "804084"
  },
  {
    "text": "this ellipse are the eigenvectors of A, right?",
    "start": "804085",
    "end": "811630"
  },
  {
    "text": "So, um, the- the- the- the- the longest axis is going to be the eigen- uh, eigenvector corresponding to the largest eigenvalue, uh,",
    "start": "811630",
    "end": "819025"
  },
  {
    "text": "and that's because points along this axis on- on the- on",
    "start": "819025",
    "end": "824080"
  },
  {
    "text": "the input sphere will get mapped to a point on the same direction,",
    "start": "824080",
    "end": "831055"
  },
  {
    "text": "but at a different- different distance from the origin, right? And the ratio of the output distance from the origin to the input distance",
    "start": "831055",
    "end": "838060"
  },
  {
    "text": "to the origin is the eigenvalue corresponding to the eigenvector, right? Now, the product of all the eigenvalues of",
    "start": "838060",
    "end": "847440"
  },
  {
    "text": "a matrix is also the determinant, right?",
    "start": "847440",
    "end": "853690"
  },
  {
    "text": "So, uh, the determinant of a matrix is the product of all eigenvalues.",
    "start": "853690",
    "end": "868040"
  },
  {
    "text": "All right? So the determinant of a matrix depends only on the eigenvalues. The eigenvectors don't matter.",
    "start": "868920",
    "end": "875020"
  },
  {
    "text": "If you have another matrix which, uh- which results in a shape that is similar to this ellipsoid,",
    "start": "875020",
    "end": "885235"
  },
  {
    "text": "it will have the same determinant, even though it may be oriented differently. All right? So the, uh,",
    "start": "885235",
    "end": "890935"
  },
  {
    "text": "determinant is the product of all the eigenvalues. And another thing that kind of, uh,",
    "start": "890935",
    "end": "898105"
  },
  {
    "text": "becomes obvious here is the product of all the eigenvalues code here is basically the ratio of the- the- the,",
    "start": "898105",
    "end": "908290"
  },
  {
    "text": "uh- the principle you know, of- of this distance to this distance is one eigenvalue,",
    "start": "908290",
    "end": "913300"
  },
  {
    "text": "the ratio of this distance to this distance is another eigenvalue, and so on. And it so happens that the product of all the lengths of, um, uh,",
    "start": "913300",
    "end": "924970"
  },
  {
    "text": "these half diameters of- of the different axes of an ellipsoid is also the volume of the ellipsoid.",
    "start": "924970",
    "end": "932755"
  },
  {
    "text": "All right. So another interpretation of the determinant is it is the volume",
    "start": "932755",
    "end": "939520"
  },
  {
    "text": "of output shape over volume of input shape.",
    "start": "939520",
    "end": "948550"
  },
  {
    "text": "[NOISE] For any shape- for any shape that has,",
    "start": "948550",
    "end": "956904"
  },
  {
    "text": "you know, some non-zero volume on the input, it need not be just a- a sphere. It could be, you know,",
    "start": "956905",
    "end": "962710"
  },
  {
    "text": "a cube, it could be any arbitrary shape. It has some volume. Run that shape through the matrix A, you get an output shape, right?",
    "start": "962710",
    "end": "971754"
  },
  {
    "text": "And that output shape has some volume as well. Now the ratio of the output volume to the input volume is the determinant of the matrix.",
    "start": "971755",
    "end": "979360"
  },
  {
    "text": "Now, it should be obvious that if the rank of the matrix is- is, if it's not a full rank matrix,",
    "start": "979360",
    "end": "986005"
  },
  {
    "text": "it means one of the eigenvalues is 0, which means this sphere would have collapsed into an ellipse rather than,",
    "start": "986005",
    "end": "992410"
  },
  {
    "text": "uh, retaining its ellipsoid shape. And the volume of an ellipse in three-dimensional space is 0, exactly, right?",
    "start": "992410",
    "end": "999925"
  },
  {
    "text": "So what you will see is that the, uh, volume of the output shape will be,",
    "start": "999925",
    "end": "1006180"
  },
  {
    "text": "um, um, for non-full rank matrices.",
    "start": "1006180",
    "end": "1011320"
  },
  {
    "text": "Right? The volume of the output shape will be 0 over, you know, some non-zero.",
    "start": "1012050",
    "end": "1018735"
  },
  {
    "text": "And again, you know, uh, this is also, you know, uh, directly clear because it's the product of eigenvalues.",
    "start": "1018735",
    "end": "1024645"
  },
  {
    "start": "1020000",
    "end": "1185000"
  },
  {
    "text": "One of the eigenvalues is 0, so, you know, the volume of the output shape will also be 0. [NOISE] Any questions about this?",
    "start": "1024645",
    "end": "1034470"
  },
  {
    "text": "Now, the reason why we focus on this- this volume interpretation is because this is gonna,",
    "start": "1034470",
    "end": "1041680"
  },
  {
    "text": "uh, help us further down in the course, right? Um, especially when we are- are talking about- talking",
    "start": "1041680",
    "end": "1049710"
  },
  {
    "text": "about operations that we perform on random variables, right?",
    "start": "1049710",
    "end": "1055270"
  },
  {
    "text": "Uh, this concept of determinant will become very important because even though we- we,",
    "start": "1055270",
    "end": "1061120"
  },
  {
    "text": "um, um, perform some kind of a linear operation on a random variable.",
    "start": "1061120",
    "end": "1066339"
  },
  {
    "text": "We will then need to adjust the resulting, um, uh, random variables probability by dividing it by",
    "start": "1066339",
    "end": "1072430"
  },
  {
    "text": "the determinant so that the volume of the- the, uh- of the PDF should still integrate to one.",
    "start": "1072430",
    "end": "1079030"
  },
  {
    "text": "So, you know, to maintain constant volume as you perform, um, uh, linear operation, you- you will,",
    "start": "1079030",
    "end": "1086159"
  },
  {
    "text": "uh- you will need to divide the- you know, divide the output by the determinant to- to uh-uh,",
    "start": "1086160",
    "end": "1091240"
  },
  {
    "text": "maintain the, uh- the volume. Uh, but if- if you don't- you know,",
    "start": "1091240",
    "end": "1097304"
  },
  {
    "text": "that's- that's, you know, advanced. We're gonna go over that again in- in a lot more detail, uh, uh, for later in the course.",
    "start": "1097305",
    "end": "1102840"
  },
  {
    "text": "But it's- it's important to kind of have this- this, uh- this understanding of the determinant.",
    "start": "1102840",
    "end": "1107950"
  },
  {
    "text": "That it is- it is, uh- the determinant kind of tells you how much the- the matrix either expands or contracts the space, right?",
    "start": "1107950",
    "end": "1116049"
  },
  {
    "text": "If- if the volume- the resulting volume is- is bigger, it's kinda expanding. And if it's- if it's, uh- if it's smaller then it's kind of contracting,",
    "start": "1116050",
    "end": "1123145"
  },
  {
    "text": "um- um, contracting the input space. All right. [NOISE] Which also means that you know,",
    "start": "1123145",
    "end": "1131730"
  },
  {
    "text": "if one of the- one of the, uh, eigenvalues is 0, it means the determinant is 0,",
    "start": "1131730",
    "end": "1138200"
  },
  {
    "text": "and therefore the matrix does not have an inverse, right? Now, that's- that's, you know- that should also be",
    "start": "1138200",
    "end": "1144495"
  },
  {
    "text": "the- be kind of obvious because you take- you take a soccer ball, um, you- if you- if you kind of transform it into an ellipsoid,",
    "start": "1144495",
    "end": "1151480"
  },
  {
    "text": "you can still map it, you know, one, uh- map every point on an ellipsoid back to the soccer ball. But if you- if you can squash it into an ellipse, then you know, uh,",
    "start": "1151480",
    "end": "1158815"
  },
  {
    "text": "there's no way you can map back a two-dimensional space back into a three-dimensional space, right? So there's no- there's no inverse if any one of the eigenvalues is zero.",
    "start": "1158815",
    "end": "1166480"
  },
  {
    "text": "[NOISE] Okay. That's all about determinants.",
    "start": "1166480",
    "end": "1171750"
  },
  {
    "text": "Any- any questions about determinants? Okay. I'll take that as a no.",
    "start": "1171750",
    "end": "1178260"
  },
  {
    "text": "Next. Moving on. [NOISE]",
    "start": "1178260",
    "end": "1187100"
  },
  {
    "start": "1185000",
    "end": "1390000"
  },
  {
    "text": "There's a technical term for the collection of all eigenvalues of a matrix and that is called? Anybody?",
    "start": "1187100",
    "end": "1195500"
  },
  {
    "text": "The spectrum. Yeah, so the- yeah, yeah, yeah. Um, the- the-, they're all pretty related.",
    "start": "1195500",
    "end": "1201320"
  },
  {
    "text": "So the spectrum is uh,",
    "start": "1201320",
    "end": "1208759"
  },
  {
    "text": "[NOISE] collection of [NOISE] eigenvalues of a matrix and you know,",
    "start": "1208760",
    "end": "1216245"
  },
  {
    "text": "you generally sort them in descending order. You write the- the largest eigenvalue first and kind of um,",
    "start": "1216245",
    "end": "1221389"
  },
  {
    "text": "sort them in the- in the- um, in the- uh, uh, uh, uh, descending order.",
    "start": "1221390",
    "end": "1227090"
  },
  {
    "text": "Now, uh, for most operations, the spectrum of a matrix pretty much contains all the information that we care about,",
    "start": "1227090",
    "end": "1234455"
  },
  {
    "text": "uh, about, you know, the matrix for- for most of the operations. Essentially, uh, you know, it tells you what is gonna be the resulting shape of- of uh,",
    "start": "1234455",
    "end": "1243725"
  },
  {
    "text": "the ellipsoid, um, for- for a given uh, in- input unit.",
    "start": "1243725",
    "end": "1248990"
  },
  {
    "text": "But the orientation may be different. But, you know, you can kind of adjust that by doing a change of basis, you know, to orient things differently.",
    "start": "1248990",
    "end": "1255500"
  },
  {
    "text": "But essentially the- the- um, uh, core of the operation of what a matrix does is kind of captured by the spectrum or you know,",
    "start": "1255500",
    "end": "1263000"
  },
  {
    "text": "the collection of uh, eigenvalues. [NOISE] And there is this, um,",
    "start": "1263000",
    "end": "1268010"
  },
  {
    "text": "theorem called the spectral theorem, [NOISE] which we're not gonna prove. [NOISE] The spectral theorem which says,",
    "start": "1268010",
    "end": "1278644"
  },
  {
    "text": "for every square matrix, um, [NOISE] uh, say d by d,",
    "start": "1278645",
    "end": "1288125"
  },
  {
    "text": "every square matrix that is also symmetric, [NOISE] right?",
    "start": "1288125",
    "end": "1295930"
  },
  {
    "text": "Has real valued eigenvalues",
    "start": "1295930",
    "end": "1301430"
  },
  {
    "text": "[NOISE]",
    "start": "1301430",
    "end": "1308780"
  },
  {
    "text": "and orthonormal eigenvectors. [NOISE]",
    "start": "1308780",
    "end": "1322010"
  },
  {
    "text": "Right? Uh, which- which makes, you know, um, um- and this actually covers a whole lot of uh,",
    "start": "1322010",
    "end": "1329450"
  },
  {
    "text": "different- different uh, matrices that we are interested in. For example, Hessians [NOISE] are square matrices and they are symmetric, right?",
    "start": "1329450",
    "end": "1339410"
  },
  {
    "text": "Which means they have real valued eigenvalues. All their eigenvalues are real.",
    "start": "1339410",
    "end": "1344480"
  },
  {
    "text": "Some could be 0, some could be negative, but they are all real valued, they are not complex eigenvalues, what- that's what it means,",
    "start": "1344480",
    "end": "1349880"
  },
  {
    "text": "and- and- and has, you know, orthonormal eigenvectors. Hessians, covariance matrices, [NOISE] right?",
    "start": "1349880",
    "end": "1358770"
  },
  {
    "text": "Kernel matrices. Again, we're gonna see what kernels are later.",
    "start": "1361420",
    "end": "1366815"
  },
  {
    "text": "But, you know, ju- just so that you- you know, why this is kind of uh, uh, relevant for the course, right?",
    "start": "1366815",
    "end": "1372305"
  },
  {
    "text": "All these have, um, uh, they're all squared, they're symmetric and therefore they have",
    "start": "1372305",
    "end": "1378620"
  },
  {
    "text": "real valued eigenvalues and orthonormal uh, eigenvectors. [NOISE] All right, with this,",
    "start": "1378620",
    "end": "1389450"
  },
  {
    "text": "next we're going to start, um, something called the uh, quadratic form, [NOISE] quadratic forms.",
    "start": "1389450",
    "end": "1402200"
  },
  {
    "start": "1390000",
    "end": "1620000"
  },
  {
    "text": "And- and- and this is closely connected to uh, uh, um, you know, uh, definitiveness.",
    "start": "1402200",
    "end": "1408559"
  },
  {
    "text": "You know, positive semi-definite, negative semi-definite. Um, and that's gonna uh, uh, pop out of this.",
    "start": "1408560",
    "end": "1413780"
  },
  {
    "text": "So what's a quadratic form? Given a matrix A assume it as a square, [NOISE] right?",
    "start": "1413780",
    "end": "1422675"
  },
  {
    "text": "And you have some vector X [NOISE] already,",
    "start": "1422675",
    "end": "1427790"
  },
  {
    "text": "the uh, quadratic form is also written as X transpose Ax.",
    "start": "1427790",
    "end": "1435480"
  },
  {
    "text": "Right? Uh, so far we have seen Ax, but X transpose Ax is- is uh,",
    "start": "1435580",
    "end": "1442880"
  },
  {
    "text": "also called the quadratic form. [NOISE] Okay?",
    "start": "1442880",
    "end": "1451174"
  },
  {
    "text": "And the- the, uh, quadratic form holds for any squared matrix.",
    "start": "1451175",
    "end": "1462185"
  },
  {
    "text": "But in general we assume, whenever we are working with a quadratic form, we just assume that A is symmetric as well.",
    "start": "1462185",
    "end": "1469205"
  },
  {
    "text": "And that's because for any um, um, quadratic form expression A,",
    "start": "1469205",
    "end": "1474919"
  },
  {
    "text": "where A is may or may not be symmetric, there always exists another B,",
    "start": "1474920",
    "end": "1481529"
  },
  {
    "text": "such that X transpose Bx equals X transpose Ax, where B is symmetric.",
    "start": "1481530",
    "end": "1490305"
  },
  {
    "text": "It's pretty easy to show, you know, uh, why this is the case. Um, it's- it's- it's- it's also in the notes.",
    "start": "1490305",
    "end": "1496940"
  },
  {
    "text": "I don't wanna spend too much time on proving this. It's a very simple proof. But in general, whenever you're dealing with quadratic forms,",
    "start": "1496940",
    "end": "1502955"
  },
  {
    "text": "it is safe to assume that the matrix is- is um, um, symmetric just because you can always represent it as another symmetric metric- matrix B,",
    "start": "1502955",
    "end": "1513424"
  },
  {
    "text": "where X transpose Bx equals X transpose Ax for all values of X. Okay? And uh, and B actually happens to be just half of A plus half of A transpose.",
    "start": "1513425",
    "end": "1526460"
  },
  {
    "text": "It's- it's, you know, um, a pretty simple proof. Yes. Question.",
    "start": "1526460",
    "end": "1532625"
  },
  {
    "text": "What is the equation X transpose Bx [inaudible]?",
    "start": "1532625",
    "end": "1538760"
  },
  {
    "text": "What's the equation X transpose Bx? [inaudible]",
    "start": "1538760",
    "end": "1545300"
  },
  {
    "text": "Oh, it just shows that for every- for any matrix square matrix A,",
    "start": "1545300",
    "end": "1550460"
  },
  {
    "text": "there is a corresponding square matrix B, which is also symmetric, which by the way is calculated like this.",
    "start": "1550460",
    "end": "1558270"
  },
  {
    "text": "Such that X transpose Ax equals X transpose Bx for all X. [NOISE] Which is why you can just assume that A is symmetric.",
    "start": "1558790",
    "end": "1571549"
  },
  {
    "text": "Uh, because even if it's not symmetric, you can use the corresponding B in its place because the values are the same at all values of X.",
    "start": "1571549",
    "end": "1578210"
  },
  {
    "text": "[NOISE] Right? Now, this quadratic form",
    "start": "1578210",
    "end": "1585934"
  },
  {
    "text": "is used to- oh.",
    "start": "1585935",
    "end": "1605750"
  },
  {
    "text": "[NOISE]. Can you pull down the whiteboard? Yeah. [inaudible] Thank you.",
    "start": "1605750",
    "end": "1617419"
  },
  {
    "text": "I just didn't want that to be distracting, that's all. All right. So we use the definition oh- the- the uh,",
    "start": "1617420",
    "end": "1624095"
  },
  {
    "start": "1620000",
    "end": "2190000"
  },
  {
    "text": "quadratic form to define what's also called as definitiveness. [NOISE] All right?",
    "start": "1624095",
    "end": "1637380"
  },
  {
    "text": "So, uh, the way we, uh, define definitiveness is, uh,",
    "start": "1637490",
    "end": "1644039"
  },
  {
    "text": "if x transpose Ax, you know, for a matrix, uh, uh, a square symmetric matrix A and any value of x, um,",
    "start": "1644040",
    "end": "1653174"
  },
  {
    "text": "is greater than zero, you know, for all x.",
    "start": "1653175",
    "end": "1658395"
  },
  {
    "text": "So this symbol means for all x. Then we call the matrix A,",
    "start": "1658395",
    "end": "1664380"
  },
  {
    "text": "we say A is positive definite, [NOISE] okay?",
    "start": "1664380",
    "end": "1674695"
  },
  {
    "text": "Similarly, if x transpose Ax is greater than equal to 0 for all x,",
    "start": "1674695",
    "end": "1684990"
  },
  {
    "text": "here, uh, for all x not equal to 0, not equal to 0 because when x is 0,",
    "start": "1686570",
    "end": "1692730"
  },
  {
    "text": "you know, uh, A transpose x, uh, A transpose x will be 0. Then A, it is positive semidefinite.",
    "start": "1692730",
    "end": "1703649"
  },
  {
    "text": "[NOISE] Okay?",
    "start": "1703650",
    "end": "1710085"
  },
  {
    "text": "If it is less than 0, for all x, uh, are not equal to 0,",
    "start": "1710085",
    "end": "1715935"
  },
  {
    "text": "then A is negative definite. [NOISE] If it is less than equal to 0,",
    "start": "1715935",
    "end": "1724950"
  },
  {
    "text": "for all x not equal to 0, then A is negative semidefinite. [NOISE] Okay?",
    "start": "1724950",
    "end": "1737490"
  },
  {
    "text": "And if you cannot say, uh, um, any statement about this where for some values of x,",
    "start": "1737490",
    "end": "1742830"
  },
  {
    "text": "it is greater than zero, some values of x, it is less than zero, then we say it's just indefinite.",
    "start": "1742830",
    "end": "1749470"
  },
  {
    "text": "Okay? So this- this is the definition of what is a positive semidefinite matrix or a positive definite matrix.",
    "start": "1752870",
    "end": "1759570"
  },
  {
    "text": "We take the square matrix A, you know, I- you can assume it to be symmetric. Um, and for any value of x,",
    "start": "1759570",
    "end": "1767115"
  },
  {
    "text": "for any vector x, calculate x transpose Ax, right? If all the- the resulting values of x transpose Ax for every x that's not equal to 0,",
    "start": "1767115",
    "end": "1777960"
  },
  {
    "text": "if it's greater than 0, then A is positive def- positive definite. If it's greater than or equal to 0, it's positive semidefinite, and so on.",
    "start": "1777960",
    "end": "1785875"
  },
  {
    "text": "Okay? Now, let- let's- l- let's try to kind of,",
    "start": "1785875",
    "end": "1792500"
  },
  {
    "text": "um, understand this geometrically. What- what- what does this actually mean? Why is this, you know,",
    "start": "1792500",
    "end": "1798840"
  },
  {
    "text": "what- what's- what's kind of- what's the meaning of x transpose Ax? Um, because, you know, we seem to care about its- its value so much.",
    "start": "1799220",
    "end": "1806670"
  },
  {
    "text": "[NOISE] Now, first recall, you know, uh, let's go back to the dot product, a transpose b where a and b are any two vectors, okay?",
    "start": "1806670",
    "end": "1815445"
  },
  {
    "text": "Let's- let- let, um, this be a, this be b, okay?",
    "start": "1815445",
    "end": "1822450"
  },
  {
    "text": "Uh, if the value of a transpose b, ah, o- or if the angle between a and b is less than 90 degrees,",
    "start": "1822450",
    "end": "1831164"
  },
  {
    "text": "then a transpose b will be? Positive, right? Um, if a and b are-",
    "start": "1831165",
    "end": "1840778"
  },
  {
    "text": "[NOISE] and a transpose b is 0,",
    "start": "1840779",
    "end": "1847710"
  },
  {
    "text": "[NOISE] and the other one is obvious. Um, if it's more than 90 degrees",
    "start": "1847710",
    "end": "1857535"
  },
  {
    "text": "then a transpose b is less than 0, [NOISE] right?",
    "start": "1857535",
    "end": "1865770"
  },
  {
    "text": "Now, um, this is for any two given matrices, a and b.",
    "start": "1865770",
    "end": "1872640"
  },
  {
    "text": "Now with the quadratic form, uh, we see over there, you know, quadratic form,",
    "start": "1872640",
    "end": "1877755"
  },
  {
    "text": "what- what- what it's actually doing is it instead of looking at, uh, a and b, it's looking at x transposed with Ax.",
    "start": "1877755",
    "end": "1886545"
  },
  {
    "text": "We're saying,  what happens to x, and now, and then you run it through A, you get Ax, right?",
    "start": "1886545",
    "end": "1892515"
  },
  {
    "text": "It's- it's- it's doing a dot product between the input and the output of A for any given, uh, value of x, right? That's- that's essentially what the quadratic form is doing.",
    "start": "1892515",
    "end": "1900645"
  },
  {
    "text": "And it is, um, and we saw in the picture previously before, let me try it again.",
    "start": "1900645",
    "end": "1906900"
  },
  {
    "text": "Um, this time I'll just write in two dimensions. Um, so assume this is the- the- the- the unit sphere",
    "start": "1906900",
    "end": "1917100"
  },
  {
    "text": "and [NOISE] this is the ellipsoid that will get you to A.",
    "start": "1917100",
    "end": "1924090"
  },
  {
    "text": "[NOISE] So this is R^2.",
    "start": "1924090",
    "end": "1929190"
  },
  {
    "text": "So this is the input- input shape- the input ball that we're gonna run through A and that's the output that we're gonna get.",
    "start": "1929190",
    "end": "1934664"
  },
  {
    "text": "And, um, what- what this is, um, essentially saying is that for",
    "start": "1934665",
    "end": "1940755"
  },
  {
    "text": "a positive semidefinite matrix or a positive definite matrix, for any value of x on the input that we take,",
    "start": "1940755",
    "end": "1954309"
  },
  {
    "text": "run it through A, we get a corresponding output, okay? Now the angle between these two is less than 90 degrees,",
    "start": "1956210",
    "end": "1966610"
  },
  {
    "text": "okay? Does that make sense? X transpose Ax is greater than 0.",
    "start": "1967070",
    "end": "1972105"
  },
  {
    "text": "So x is- this here was input x, and this was output Ax, okay?",
    "start": "1972105",
    "end": "1979169"
  },
  {
    "text": "As long as their angle is less than 90 degrees, it will always be greater than 0, right?",
    "start": "1979170",
    "end": "1985350"
  },
  {
    "text": "And this is, again, um, this is how we relate extra the- the-",
    "start": "1985350",
    "end": "1990840"
  },
  {
    "text": "the definitiveness statement, to eigenvectors- eigenvalues. Because if you have a- a matrix that has all positive eigenvalues,",
    "start": "1990840",
    "end": "2000260"
  },
  {
    "text": "then the eigenvectors only scale- gets scaled in a positive direction, right?",
    "start": "2000260",
    "end": "2007160"
  },
  {
    "text": "And this eigenvector gets scaled, you know, in a positive direction.",
    "start": "2007160",
    "end": "2012169"
  },
  {
    "text": "And x transpose Ax for eigenvectors are always positive because they're just scaled in a positive direction.",
    "start": "2012170",
    "end": "2020495"
  },
  {
    "text": "And- and- when you- when you do a dot product between two vectors which are oriented similarly, it's always positive, right?",
    "start": "2020495",
    "end": "2027020"
  },
  {
    "text": "So you kind of think of the eigen- eigenvectors as, you know, acting as pivots and anything inside",
    "start": "2027020",
    "end": "2036230"
  },
  {
    "text": "them get mapped to vectors that are also in the same quadrant, right?",
    "start": "2036230",
    "end": "2041929"
  },
  {
    "text": "If one of the eigenvalues was negative, so for example, if this eigenvalue was positive but this eigenvalue was negative,",
    "start": "2041930",
    "end": "2048530"
  },
  {
    "text": "then a vector over here could have gotten mapped to,",
    "start": "2048530",
    "end": "2053780"
  },
  {
    "text": "you know, s- to something in this quadrant, which means its angle could have been bigger than, uh, 90 degrees.",
    "start": "2053780",
    "end": "2059960"
  },
  {
    "text": "Which means x transpose Ax could have been negative, okay? If you have all positive eigenvalues,",
    "start": "2059960",
    "end": "2066605"
  },
  {
    "text": "then the eigenvectors kind of act as pivots where the vectors inside one quadrant remain in the same quadrant in the output space, right?",
    "start": "2066605",
    "end": "2076385"
  },
  {
    "text": "So, how does that connect to the, um, definition of positive, uh, definite, uh, uh, positive definitiveness?",
    "start": "2076385",
    "end": "2083089"
  },
  {
    "text": "So, uh, for a positive definite matrix,",
    "start": "2083090",
    "end": "2088550"
  },
  {
    "text": "[NOISE] all eigenvalues are greater than 0, right?",
    "start": "2088550",
    "end": "2098345"
  },
  {
    "text": "For a positive semidefinite matrix, they all are greater than or equal to 0.",
    "start": "2098345",
    "end": "2104134"
  },
  {
    "text": "For a negative definite matrix, all the eigenvalues are less than 0 and for a negative semidefinite matrix,",
    "start": "2104135",
    "end": "2112865"
  },
  {
    "text": "they are less than or equal to 0. And for an indefinite matrix, they can be, you know,",
    "start": "2112865",
    "end": "2118040"
  },
  {
    "text": "greater than or less than 0, right?",
    "start": "2118040",
    "end": "2124400"
  },
  {
    "text": "So the- the- the- uh, the definitiveness of a matrix which is defined by the quadratic form has,",
    "start": "2124400",
    "end": "2132950"
  },
  {
    "text": "uh, one to one relation with the spectrum of the matrix.",
    "start": "2132950",
    "end": "2138300"
  },
  {
    "text": "Any questions about that? Yes. [inaudible]",
    "start": "2138640",
    "end": "2152600"
  },
  {
    "text": "Yeah. So, um, when I say it is less than or greater than or equal to 0, for a few x's, it's gonna be greater than 0.",
    "start": "2152600",
    "end": "2159260"
  },
  {
    "text": "For a few other x's, it's going to be less than 0. So if you construct any positive, uh, definite matrix, where, uh,",
    "start": "2159260",
    "end": "2165410"
  },
  {
    "text": "a- any symmetric matrix where one of the eigenvalues is positive and one of the eigenvalues is negative, you're gonna get a matrix and they're gonna be kind of, you know,",
    "start": "2165410",
    "end": "2172640"
  },
  {
    "text": "you know, um, eigenvectors corresponding to the- uh, they're gonna be vectors corresponding to the eigenvectors of that matrix,",
    "start": "2172640",
    "end": "2180589"
  },
  {
    "text": "which will result in, you know, the value- the quadratic form being greater than 0 or less than 0. [NOISE] Okay?",
    "start": "2180590",
    "end": "2192920"
  },
  {
    "start": "2190000",
    "end": "3020000"
  },
  {
    "text": "Any questions on this? Okay. So next we're gonna move on to,",
    "start": "2192920",
    "end": "2198455"
  },
  {
    "text": "um, decomposing matrices. [NOISE]",
    "start": "2198455",
    "end": "2268450"
  },
  {
    "text": "So given a matrix, there are many ways to decompose it. And what do we mean by decompose it- decomposition?",
    "start": "2268450",
    "end": "2275215"
  },
  {
    "text": "We're going to look at it right away. The two decompositions that we are gonna, um,",
    "start": "2275215",
    "end": "2281460"
  },
  {
    "text": "that we're gonna look at today are the singular value decomposition [NOISE] or,",
    "start": "2281460",
    "end": "2286650"
  },
  {
    "text": "it's also called the SVD [NOISE] and the eigenvalue decomposition.",
    "start": "2286650",
    "end": "2297340"
  },
  {
    "text": "[NOISE] Right?",
    "start": "2297340",
    "end": "2305710"
  },
  {
    "text": "Those are the two things we're gonna, um, um, uh, go over today. Uh, because these are,",
    "start": "2305710",
    "end": "2311140"
  },
  {
    "text": "you know, these two are- are, uh, probably the most, um, um,",
    "start": "2311140",
    "end": "2316945"
  },
  {
    "text": "interesting ones from a theoretical point of view, there's another, you know, um, decomposition that's used very,",
    "start": "2316945",
    "end": "2323635"
  },
  {
    "text": "uh, uh, frequently for the Cholesky decomposition.",
    "start": "2323635",
    "end": "2327200"
  },
  {
    "text": "Maybe we'll, uh, um, I'll just mention in passing what- what, um, that is. But the, um, interesting ones to analyze are",
    "start": "2330390",
    "end": "2338110"
  },
  {
    "text": "the singular value decomposition and the eigenvalue decomposition, right? The singular value decomposition.",
    "start": "2338110",
    "end": "2344214"
  },
  {
    "text": "Um, so this is SVD,",
    "start": "2344215",
    "end": "2350275"
  },
  {
    "text": "this one I'm gonna call it, eigenvalue decomposition.",
    "start": "2350275",
    "end": "2355525"
  },
  {
    "text": "Right? So, uh, matrix A for singular ma-,",
    "start": "2355525",
    "end": "2361480"
  },
  {
    "text": "uh, value decomposition can be any matrix A. Right? It can be any matrix whatsoever.",
    "start": "2361480",
    "end": "2368484"
  },
  {
    "text": "Right? But for eigenvalue decomposition, you need square matrices.",
    "start": "2368485",
    "end": "2373460"
  },
  {
    "text": "That's- that's, uh, one main difference. And the decomposition itself is defined like this.",
    "start": "2373560",
    "end": "2381100"
  },
  {
    "text": "[NOISE] Right?",
    "start": "2381100",
    "end": "2389935"
  },
  {
    "text": "So singular value decomposition is generally, you say A equals USV transpose, and eigenvalue decomposition,",
    "start": "2389935",
    "end": "2402940"
  },
  {
    "text": "we generally say A equals UDU inverse.",
    "start": "2402940",
    "end": "2414230"
  },
  {
    "text": "What- what- what are we saying here are basically? Basically what we're saying is given a matrix A.",
    "start": "2415890",
    "end": "2423339"
  },
  {
    "text": "So let's- let's again think of this in terms of functions, right? Um, matrices, you know, think of matrices as function that's- that's always,",
    "start": "2423340",
    "end": "2430330"
  },
  {
    "text": "um, a useful approach. Um, what this is basically saying is Ax [NOISE] can also",
    "start": "2430330",
    "end": "2440410"
  },
  {
    "text": "be written as USV transpose x,",
    "start": "2440410",
    "end": "2449170"
  },
  {
    "text": "which means first, multiply x with V transpose and you get some output mat- vector.",
    "start": "2449170",
    "end": "2457569"
  },
  {
    "text": "Then take that vector as input to S, and you get some output. And take the output of multiplying by S and feed it as input to U,",
    "start": "2457570",
    "end": "2465714"
  },
  {
    "text": "and you get some output. And what it- it's basically saying is the operation",
    "start": "2465715",
    "end": "2471070"
  },
  {
    "text": "performed by x can be decomposed into three sub-matrices, which you apply like a pipeline.",
    "start": "2471070",
    "end": "2477775"
  },
  {
    "text": "You know, first apply V transpose to x, er, and each of these is a matrix.",
    "start": "2477775",
    "end": "2483490"
  },
  {
    "text": "First you apply, V transpose to x, you get, you know, a different, uh, vector. You take that vector,",
    "start": "2483490",
    "end": "2489415"
  },
  {
    "text": "you know, um, feed it as input to S, and you take the output of that, feed it as input to U, you know, it's- it's just a misfit operation, right?",
    "start": "2489415",
    "end": "2496510"
  },
  {
    "text": "And- and similarly, the Eigen decomposition of- of, uh,",
    "start": "2496510",
    "end": "2503140"
  },
  {
    "text": "A can be written as, um, [NOISE]",
    "start": "2503140",
    "end": "2517480"
  },
  {
    "text": "right? We saw what a matrix A, uh, essentially- essentially does, right?",
    "start": "2517480",
    "end": "2523000"
  },
  {
    "text": "So, uh, [NOISE]",
    "start": "2523000",
    "end": "2535240"
  },
  {
    "text": "so what the singular value decomposition and eigenvalue decomposition are saying is that you can decompose a matrix A into three sub-matrices,",
    "start": "2535240",
    "end": "2545020"
  },
  {
    "text": "[NOISE] such that the operation performed by A on any vector x can be split into three subparts.",
    "start": "2545020",
    "end": "2554845"
  },
  {
    "text": "Right? Uh, so the operations.",
    "start": "2554845",
    "end": "2561474"
  },
  {
    "text": "So before we move on to operations, let's talk a little more about these, uh, individual matrices.",
    "start": "2561475",
    "end": "2567309"
  },
  {
    "text": "So U and V are- [NOISE] they are ortho- orthonormal matrices,",
    "start": "2567310",
    "end": "2578419"
  },
  {
    "text": "and S is also called a singular, the- the set of singular values.",
    "start": "2581490",
    "end": "2588010"
  },
  {
    "text": "It's called, uh, uh, and this is a diagonal matrices- diagonal matrix",
    "start": "2588010",
    "end": "2593035"
  },
  {
    "text": "where the singular values are along the diagonal of- of, um, of S and D is also diagonal,",
    "start": "2593035",
    "end": "2601670"
  },
  {
    "text": "and the eigenvalues are along the diagonal of D. So",
    "start": "2602460",
    "end": "2607705"
  },
  {
    "text": "the eigenvalue decomposition decomposes into UDU transpose,",
    "start": "2607705",
    "end": "2614079"
  },
  {
    "text": "where the columns of U are the eigenvectors and D is the set of eigenvalues.",
    "start": "2614079",
    "end": "2621145"
  },
  {
    "text": "And, um, you would, for example, if this is U and [NOISE] this is D,",
    "start": "2621145",
    "end": "2632500"
  },
  {
    "text": "then there is a one-to-one mapping between the eigenvector and the corresponding eigenvalue",
    "start": "2632500",
    "end": "2638965"
  },
  {
    "text": "and a different eigenvector and the corresponding eigenvalue and so on, right?",
    "start": "2638965",
    "end": "2646975"
  },
  {
    "text": "The action performed by A is completely defined by",
    "start": "2646975",
    "end": "2652030"
  },
  {
    "text": "its set of eigenvectors and eigenvalues for a square matrix A.",
    "start": "2652030",
    "end": "2657985"
  },
  {
    "text": "And for any matrix A, you have, uh, you have a collection of singular values in place of eigenvalues,",
    "start": "2657985",
    "end": "2667990"
  },
  {
    "text": "[NOISE] and you have a U and another matrix V transpose.",
    "start": "2667990",
    "end": "2681289"
  },
  {
    "text": "For the singular value decomposition, U and V are, you know,",
    "start": "2682980",
    "end": "2688224"
  },
  {
    "text": "have, they are- they are- they are different matrices in general. They are, um, uh,",
    "start": "2688225",
    "end": "2693460"
  },
  {
    "text": "they just need to be orthonormal, which means that unit length and all of them are orthogonal to each other.",
    "start": "2693460",
    "end": "2699820"
  },
  {
    "text": "Whereas for the- the eigenvalue decomposition, the- the third matrix has to be the inverse of- of, uh, the- of U itself.",
    "start": "2699820",
    "end": "2709030"
  },
  {
    "text": "So this is just U inverse. Right? Now, um, we have- uh, so we ca- we can- we can, uh,",
    "start": "2709030",
    "end": "2720005"
  },
  {
    "text": "break down the eigenval- the- any matrix, uh, into the singular value decomposition,",
    "start": "2720005",
    "end": "2726785"
  },
  {
    "text": "but the cool thing about the s- uh, the singular value decomposition is that you are guaranteed that",
    "start": "2726785",
    "end": "2732799"
  },
  {
    "text": "the singular values of the matrix are gonna be real-valued, right? No matter, you know,",
    "start": "2732800",
    "end": "2739340"
  },
  {
    "text": "what matrix, A, U, you feed in, it needs not be full rank; it could be anything whatsoever.",
    "start": "2739340",
    "end": "2746075"
  },
  {
    "text": "Any real-valued matrix, you know, as long as, you know, the values in the- in the grids are all real-valued,",
    "start": "2746075",
    "end": "2751414"
  },
  {
    "text": "you can always come up with a decomposition of this form [NOISE] where the- [NOISE] where the U and V matrices are orthonormal and the singular values are all real.",
    "start": "2751414",
    "end": "2763575"
  },
  {
    "text": "Yes. Question? Oh, no question. [NOISE] Now, uh, however,",
    "start": "2763575",
    "end": "2770075"
  },
  {
    "text": "with eigenvalue decomposition, um, things are a little different in that,",
    "start": "2770075",
    "end": "2775160"
  },
  {
    "text": "it is limited to square matrices only. Yes. Question? [inaudible]",
    "start": "2775160",
    "end": "2786010"
  },
  {
    "text": "So uh, the question is, uh, aren't- isn't the eigenvalue decomposition diagonalizable? [inaudible]",
    "start": "2786010",
    "end": "2796140"
  },
  {
    "text": "So if you, uh, um, b- by diagonal- diagonalizable- if you mean",
    "start": "2796140",
    "end": "2803585"
  },
  {
    "text": "diagonalizable into real-valued eigen- ah, uh, eigen- uh, values, then yes, but in general,",
    "start": "2803585",
    "end": "2810200"
  },
  {
    "text": "you can [NOISE] take any square matrix and the eigenvector- uh, the eigenvalues may be complex,",
    "start": "2810200",
    "end": "2815540"
  },
  {
    "text": "but you can still break down- [inaudible]",
    "start": "2815540",
    "end": "2827240"
  },
  {
    "text": "[NOISE] breaking too much [inaudible] like, uh, [inaudible] I think, uh, [inaudible] diagonalizable? So, um, if it's not, uh, uh, diagonalizable,",
    "start": "2827240",
    "end": "2834260"
  },
  {
    "text": "then t- this is- they're gonna be complex values in there. [NOISE] But if you're okay with- with- with- with complex numbers,",
    "start": "2834260",
    "end": "2841520"
  },
  {
    "text": "then you can always represent it like this. [NOISE] Okay. So, um [NOISE] Question? Yes. Question?",
    "start": "2841520",
    "end": "2847670"
  },
  {
    "text": "Is- is S also- uh, S and D, are they both diagonal,",
    "start": "2847670",
    "end": "2854320"
  },
  {
    "text": "uh, matrices with the eigenvalues? So uh, are S and- uh, uh, D diagonal matrices with eigenvalues?",
    "start": "2854320",
    "end": "2861395"
  },
  {
    "text": "So in the eigenvalue decomposition, it is um, um, the D matrix is diagonal with eigenvalues.",
    "start": "2861395",
    "end": "2867200"
  },
  {
    "text": "In the singular value decomposition, [NOISE] the- the- uh, S is a diagonal matrix with what's called singular values, not the eigenvalues.",
    "start": "2867200",
    "end": "2874415"
  },
  {
    "text": "Eigenvalues are only defined for square matrices. Ah, [NOISE] okay.",
    "start": "2874415",
    "end": "2880809"
  },
  {
    "text": "So [inaudible] stronger case. Uh, so- so when we- when V happens to be",
    "start": "2880810",
    "end": "2886160"
  },
  {
    "text": "U inverse transpose, that is when the singular value of decomposition [inaudible]",
    "start": "2886160",
    "end": "2894740"
  },
  {
    "text": "Well, I'm- I'm- I'm coming to that. I'm coming to that, right? So this is just the way you can, you can break down a matrix into,",
    "start": "2894740",
    "end": "2900994"
  },
  {
    "text": "into, uh, into subcomponents, [NOISE] right? Now, [NOISE] it's kind of interesting to see,",
    "start": "2900995",
    "end": "2908705"
  },
  {
    "text": "you know, how- how these are related. [NOISE] And let's call this A [NOISE] Right,  um,",
    "start": "2908705",
    "end": "2917520"
  },
  {
    "text": "SVD. [NOISE] So I'm gonna call this, ah, Step 1,",
    "start": "2922870",
    "end": "2930410"
  },
  {
    "text": "[NOISE] Step 2, and Step 3, right?",
    "start": "2930410",
    "end": "2938329"
  },
  {
    "text": "Where Step 1 corresponds to what happens when you take the 1st piece and, ah, and- and- ah,",
    "start": "2938330",
    "end": "2943760"
  },
  {
    "text": "look at its action on X. Step 2 is take the output of Step 1 and run it through the next matrix,",
    "start": "2943760",
    "end": "2949280"
  },
  {
    "text": "and Step 3 is take the output of Step 2 and run it through the 3rd matrix, right? Now. U and V or U and U inverse are all orthonormal, right?",
    "start": "2949280",
    "end": "2959555"
  },
  {
    "text": "Which means [NOISE] Step 1 for both singular value and- and- um,",
    "start": "2959555",
    "end": "2964910"
  },
  {
    "text": "eigenvalue decomposition [NOISE] is um- here it is. It is, um, V transpose and here,",
    "start": "2964910",
    "end": "2972260"
  },
  {
    "text": "it is U inverse, and the action is gonna be just the rotation [NOISE] and mirroring, [NOISE] right?",
    "start": "2972260",
    "end": "2983120"
  },
  {
    "text": "Rotation [NOISE] and possibly mirroring. [NOISE] In fact, it's actually gonna be just- just, uh, rotations.",
    "start": "2983120",
    "end": "2991880"
  },
  {
    "text": "It's not even gonna be, uh, mirroring because it's orthonormal, [NOISE] okay? It's just gonna be some kind of,",
    "start": "2991880",
    "end": "2997625"
  },
  {
    "text": "um, a rotation, right? And then, Step 2 is you're multiplying it by a diagonal matrix,",
    "start": "2997625",
    "end": "3004330"
  },
  {
    "text": "[NOISE] which means you're just scaling along the axes.",
    "start": "3004330",
    "end": "3013315"
  },
  {
    "text": "That's what a diagonal matrix- multiplying by a diagonal matrix does, right? Take any vector, multiply it by a diagonal matrix of it,",
    "start": "3013315",
    "end": "3019770"
  },
  {
    "text": "then you're taking each component and scaling that component along the corresponding axes by the value of the- the,",
    "start": "3019770",
    "end": "3027580"
  },
  {
    "text": "uh, uh, element in, uh, in- in- in the corresponding diagonal entry. All right? So this is just scaling.",
    "start": "3027580",
    "end": "3033970"
  },
  {
    "text": "[NOISE] Now scaling. [NOISE] That is scaling along the axes- [NOISE] along the,",
    "start": "3033970",
    "end": "3043240"
  },
  {
    "text": "uh, the x and y-axes, not the eigen- eigenvector but scaling along the axes. [NOISE] Right?",
    "start": "3043240",
    "end": "3054130"
  },
  {
    "text": "Now, [NOISE] the scaling for the singular value decomposition is always real-valued, right?",
    "start": "3054130",
    "end": "3064660"
  },
  {
    "text": "Uh, because S is always gonna be real-valued for SVD. So this is a real-valued scaling, [NOISE] right?",
    "start": "3064660",
    "end": "3072740"
  },
  {
    "text": "But for the eigenvalue decomposition, some of your eigenvalues may be complex.",
    "start": "3073710",
    "end": "3080620"
  },
  {
    "text": "And scaling by a complex value essentially means there is some amount of rotation involved, right?",
    "start": "3080620",
    "end": "3087400"
  },
  {
    "text": "Scaling by a- a- a complex number, ah, means there could be rotation.",
    "start": "3087400",
    "end": "3093010"
  },
  {
    "text": "[NOISE] If eigenvalue is complex, [NOISE] right?",
    "start": "3093010",
    "end": "3101150"
  },
  {
    "text": "And then, Step 3 is another rotation. Here, the Step 3 is gonna be,",
    "start": "3103440",
    "end": "3109150"
  },
  {
    "text": "ah, rotating it by U. So this is just Rotation 1 [NOISE] and this is Rotation 2, [NOISE] right?",
    "start": "3109150",
    "end": "3118225"
  },
  {
    "text": "Means your- your- we rotate in- in SVD. We [NOISE] rotate it,",
    "start": "3118225",
    "end": "3124255"
  },
  {
    "text": "scale it along the, ah, diagonals after rotating. And then, rotate it by a different amount corresponding to U.",
    "start": "3124255",
    "end": "3133225"
  },
  {
    "text": "Whereas with singular value decomposition, ah, with eigenvalue decomposition, first, we rotate it, then scale it.",
    "start": "3133225",
    "end": "3139225"
  },
  {
    "text": "But the scaling may- you know, if you have complex, ah, eigenvalue, the scaling may, ah, involve some implicit rotation,",
    "start": "3139225",
    "end": "3145330"
  },
  {
    "text": "and then we [NOISE] rotate it by U, which is basically the inverse of the 1st step.",
    "start": "3145330",
    "end": "3151600"
  },
  {
    "text": "[NOISE] Inverse of Step 1, [NOISE] right? [NOISE]",
    "start": "3151600",
    "end": "3162615"
  },
  {
    "text": "In eigenvalue decomposition, first you rotate it by U inverse, then scale it, and then undo the rotation, uh,",
    "start": "3162615",
    "end": "3169410"
  },
  {
    "text": "that was done by, uh, um- yes, question. Uh, the [inaudible] is it along the eigen- eigenvector axes?",
    "start": "3169410",
    "end": "3179880"
  },
  {
    "text": "So, uh, the- so I would say ignore this case for now.",
    "start": "3179880",
    "end": "3185654"
  },
  {
    "text": "Um, uh, this is- this is, uh, not very, you know, crucial for us. I'm happy to go deeper into that after the lecture.",
    "start": "3185655",
    "end": "3192510"
  },
  {
    "text": "But, um, essentially what you wanna think of this, you give some shape as input, you know.",
    "start": "3192510",
    "end": "3198510"
  },
  {
    "text": "Uh, you can characterize what a matrix A is doing by thinking of it as first rotating it by some amount,",
    "start": "3198510",
    "end": "3207345"
  },
  {
    "text": "and then scale the rotated version by different amounts along, you know,",
    "start": "3207345",
    "end": "3212760"
  },
  {
    "text": "the x and y-axis, [NOISE] and then rotate it by a different amount, right?",
    "start": "3212760",
    "end": "3218715"
  },
  {
    "text": "And it could be- it is- it is, uh, different for SVD in the sense it could be, you know,",
    "start": "3218715",
    "end": "3225945"
  },
  {
    "text": "arbitrarily different, but for the eigenvalue decomposition, you are gonna just undo the initial rotation, right?",
    "start": "3225945",
    "end": "3232365"
  },
  {
    "text": "And what you- what you, uh, what you see is that the- the direction that ends",
    "start": "3232365",
    "end": "3242120"
  },
  {
    "start": "3240000",
    "end": "3270000"
  },
  {
    "text": "up aligning with the axis after the first rotation are the directions of your eigenvectors, right?",
    "start": "3242120",
    "end": "3248690"
  },
  {
    "text": "Because you're gonna- you're gonna scale tho- scale the points along- the points along the eigenvectors after rotation will end up aligning with the x-axis.",
    "start": "3248690",
    "end": "3259695"
  },
  {
    "text": "And when you scale them along the axis, these points are not changing their direction once you undo the rotation.",
    "start": "3259695",
    "end": "3267730"
  },
  {
    "text": "Does that make sense? So, uh,",
    "start": "3268940",
    "end": "3274869"
  },
  {
    "start": "3270000",
    "end": "3355000"
  },
  {
    "text": "what this means, again, in- is that, uh, for square matrices, um,",
    "start": "3275930",
    "end": "3282960"
  },
  {
    "text": "especially square symmetric matrices where we don't care about complex eigenvalues,",
    "start": "3282960",
    "end": "3288119"
  },
  {
    "text": "for all square symmetric matrices, we are- we- we gonna rotate the- the action of A can be",
    "start": "3288120",
    "end": "3295680"
  },
  {
    "text": "summarized at- as rotated such that the eigenvectors align with the axis.",
    "start": "3295680",
    "end": "3301464"
  },
  {
    "text": "Scale it by different amounts. The scaling could be negative, which means you're, kind of, mirroring it.",
    "start": "3301465",
    "end": "3306755"
  },
  {
    "text": "And then undo the rotation you did in the first step, which means the- the eigenvectors and the axes of",
    "start": "3306755",
    "end": "3317510"
  },
  {
    "text": "the ellipsoid are gonna be the same, right?",
    "start": "3317510",
    "end": "3325545"
  },
  {
    "text": "Now, for singular value, uh, decomposition, um, there are no eigenvectors.",
    "start": "3325545",
    "end": "3331950"
  },
  {
    "text": "Um, there are singular vectors, uh, but the- the, uh, interpretation is- is not- not- not as easy because, um,",
    "start": "3331950",
    "end": "3339285"
  },
  {
    "text": "you're effectively rotating it by some amount, scaling it along the axes, and not undoing rotation,",
    "start": "3339285",
    "end": "3344760"
  },
  {
    "text": "but just rotating by a different, you know- by U instead of V inverse.",
    "start": "3344760",
    "end": "3350470"
  },
  {
    "text": "The- the benefit of SVD is that you always get real singular values,",
    "start": "3351200",
    "end": "3356579"
  },
  {
    "text": "but with, uh, eigenvalues, you require the matrix, uh, to be square and symmetric to get real eigenvalues,",
    "start": "3356580",
    "end": "3362760"
  },
  {
    "text": "but the singular values are always real. And it so happens that, if you are having square symmetric matrices in your eigenvalue,",
    "start": "3362760",
    "end": "3371895"
  },
  {
    "text": "the singular value decomposition and eigenvalue decomposition will give you the same three components.",
    "start": "3371895",
    "end": "3378913"
  },
  {
    "text": "So singular value decomposition and eigenvalue decomposition are the same for square symmetric matrices.",
    "start": "3378914",
    "end": "3384000"
  },
  {
    "text": "For, um, for A arbitrary,",
    "start": "3384000",
    "end": "3389250"
  },
  {
    "text": "which means it need not be square, it need not be symmetric, right? SVD is the only thing that works.",
    "start": "3389250",
    "end": "3395595"
  },
  {
    "text": "So SVD is like a sledgehammer, throw any matrix at it, it's gonna decompose it into three parts, right? Um, if A is square,",
    "start": "3395595",
    "end": "3405075"
  },
  {
    "text": "you can still do SVD, but you can also do an eigenvalue decomposition. Some eigenvalues may be complex,",
    "start": "3405075",
    "end": "3412214"
  },
  {
    "text": "but if you do an SVD on a square matrix which is not symmetric, you still get, you know, real-valued singular values.",
    "start": "3412215",
    "end": "3419160"
  },
  {
    "text": "And when A is square and symmetric,",
    "start": "3419160",
    "end": "3424778"
  },
  {
    "text": "then SVD and eigenvalue decomposition are the same. You know, you get the same, uh,",
    "start": "3425270",
    "end": "3431310"
  },
  {
    "text": "um- U will be the same, S and D will be the same, V transpose and universe would be the same. [NOISE].",
    "start": "3431310",
    "end": "3439950"
  },
  {
    "text": "Professor? Yes, question. You said if A is squared, then both of those exist?",
    "start": "3439950",
    "end": "3445815"
  },
  {
    "text": "So when A is square, you can perform both the eigenvalue decomposition and the singular value decomposition,",
    "start": "3445815",
    "end": "3452565"
  },
  {
    "text": "but the decomposed components may not be identical, right? But if A is square and symmetric and you can perform both the decomposition,",
    "start": "3452565",
    "end": "3461745"
  },
  {
    "text": "and the components will also be the same. So SVD and eigenvalue decomposition are, you know- you- you- it's- it's the same decomposition if it's square symmetric.",
    "start": "3461745",
    "end": "3469860"
  },
  {
    "text": "And if it's non-square, then the eigenvalue of the decomposition does not exist? Yeah. It does not even exist. I mean, it's not even defined.",
    "start": "3469860",
    "end": "3476115"
  },
  {
    "start": "3470000",
    "end": "3530000"
  },
  {
    "text": "So eigenvalue decomposition is defined only for square matrices. So eigenvalue, you know, algebraically, um,",
    "start": "3476115",
    "end": "3482684"
  },
  {
    "text": "the eigenvalue eigenvector is basically the solution to Ax equals Lambda x, right?",
    "start": "3482685",
    "end": "3490260"
  },
  {
    "text": "What it means is take a matrix A, multiple it by a vector x, and you are gonna get an output vector which is, you know,",
    "start": "3490260",
    "end": "3497595"
  },
  {
    "text": "along the direction of x, but scaled by some eigenvalue, right? And all the solutions of this are your eigenvalues and eigenvectors, right?",
    "start": "3497595",
    "end": "3505560"
  },
  {
    "text": "And in order for this to hold true, A has to be squared, right?",
    "start": "3505560",
    "end": "3510630"
  },
  {
    "text": "For- for this- for this to be- uh, uh, for the output to be, uh, uh, a scalar ver- a- a scaled version of the input,",
    "start": "3510630",
    "end": "3516945"
  },
  {
    "text": "A has to be squared, otherwise, it's just gonna be a- a different dimension. So, uh, eigenvalue decomposition holds only for square matrices, right?",
    "start": "3516945",
    "end": "3527790"
  },
  {
    "text": "[NOISE] Uh- okay.",
    "start": "3527790",
    "end": "3542880"
  },
  {
    "start": "3530000",
    "end": "3599000"
  },
  {
    "text": "Next, we're gonna move on to matrix calculus. Any- any questions about this? [NOISE] Yes, question.",
    "start": "3542880",
    "end": "3551130"
  },
  {
    "text": "[inaudible]",
    "start": "3551130",
    "end": "3567329"
  },
  {
    "text": "Yeah. Um, for- for- for, uh- it's- it's- you know, what- what you say is true. In terms of- in terms of the intuition, you know,",
    "start": "3567330",
    "end": "3574920"
  },
  {
    "text": "you probably wanna have this as, you know, in- in- in your mind, in- for- in terms of having the right intuitions.",
    "start": "3574920",
    "end": "3580530"
  },
  {
    "text": "Yes, some matrices are- are- are- uh, you know, not diagnosable. Uh, but for the most part,",
    "start": "3580530",
    "end": "3586950"
  },
  {
    "text": "we're gonna be dealing only with symmetric matrices, you know, square symmetric matrices.",
    "start": "3586950",
    "end": "3592200"
  },
  {
    "text": "And we are gonna be mostly dealing with- with, you know, um, um, quadratic forms, you know, for any- when- in- in which case you have a corresponding symmetric matrices.",
    "start": "3592200",
    "end": "3600315"
  },
  {
    "text": "And in case of, you know, symmetric matrices, you can always, um, um, uh, you can always, uh, um,",
    "start": "3600315",
    "end": "3607230"
  },
  {
    "text": "decompose a- a- a square symmetric matrices- matrix. Professor? Yeah. Uh, on the definition of [inaudible].",
    "start": "3607230",
    "end": "3615160"
  },
  {
    "text": "Yes. So all those categories, how do we actually test it?",
    "start": "3615160",
    "end": "3621530"
  },
  {
    "text": "Because we can't try all axes. So the question is, um, how do we test, uh,",
    "start": "3621530",
    "end": "3627510"
  },
  {
    "text": "whether a given matrix is- you know, uh, belong to any one of those categories because we just cannot test,",
    "start": "3627510",
    "end": "3632549"
  },
  {
    "text": "you know, filling all the possible values of x and- and- and see it. And, um, that's a great question.",
    "start": "3632550",
    "end": "3638295"
  },
  {
    "text": "And the answer is, we saw a one-to-one correspondence between the definiteness and the spectrum, which means you can always calculate the spectrum",
    "start": "3638295",
    "end": "3645510"
  },
  {
    "text": "of a matrix by doing an eigendecomposition. We haven't, um, discussed how you actually do this decomposition, right?",
    "start": "3645510",
    "end": "3651960"
  },
  {
    "text": "And- and we'll- we- we'll probably see that later in the course, how you can, uh, uh, decompose it. But there are lots of methods to decompose a given matrix into its, you know,",
    "start": "3651960",
    "end": "3660855"
  },
  {
    "text": "eigenvalue decomposition and singular value decomposition, and then you can just inspect the eigenvalues.",
    "start": "3660855",
    "end": "3666330"
  },
  {
    "text": "Are they all positive? Are they all negative? And, you know, you get the corresponding definiteness. One more thing for the eigenvalue decomposition.",
    "start": "3666330",
    "end": "3675510"
  },
  {
    "text": "If you like to reorganize the- in the spectrum models and the [inaudible] model,",
    "start": "3675510",
    "end": "3682390"
  },
  {
    "text": "the aigen models, is it [inaudible]? Can you switch around, like, the columns of the,",
    "start": "3682390",
    "end": "3688050"
  },
  {
    "text": "uh, corresponding aigen with the columns- Mm-hmm. -of the corresponding [inaudible]? Yeah. You can. You can.",
    "start": "3688050",
    "end": "3694050"
  },
  {
    "text": "[inaudible]. Yeah. So- so the question is, um, can we swap these two, uh,",
    "start": "3694050",
    "end": "3700109"
  },
  {
    "text": "rows and columns and also the corresponding, you know, diagonals and also the corresponding rows? Yes, you can do that. Uh, but as a- as a convention,",
    "start": "3700110",
    "end": "3706590"
  },
  {
    "text": "you just write them in- in- in descending order. It's just a convention. Okay. So moving on to matrix calculus.",
    "start": "3706590",
    "end": "3716260"
  },
  {
    "text": "I should have probably swapped, right? [NOISE]",
    "start": "3726230",
    "end": "3770664"
  },
  {
    "text": "All right, so matrix calculus. Let's see. Yeah, I'm assuming all of you are familiar with- with calculus,",
    "start": "3770665",
    "end": "3778120"
  },
  {
    "text": "you know, with the concept of differentiation. Um, and as a prerequisite,",
    "start": "3778120",
    "end": "3784705"
  },
  {
    "text": "you should have already seen multivariate calculus in terms of, you know, um, um, differentiating,",
    "start": "3784705",
    "end": "3791035"
  },
  {
    "text": "say a scalar-valued function that has a vector-valued input, um, and such.",
    "start": "3791035",
    "end": "3796075"
  },
  {
    "text": "So, uh, in fact, everything that I'm, uh, covering today [inaudible].",
    "start": "3796075",
    "end": "3804609"
  },
  {
    "text": "You know, hopefully this is not your introduction to this subjects. Hopefully you- you should have already seen them and this should be like, um, a refresher or,",
    "start": "3804610",
    "end": "3811705"
  },
  {
    "text": "you know, just- just to, um, you know, recollect things if you've- if you've forgotten them. [NOISE] So, uh, I'll- I'll be skipping over a lot of, um,",
    "start": "3811705",
    "end": "3822400"
  },
  {
    "text": "I'll be skipping over a lot of, um, um, um, precise rigorous statements and just giving you the intuition so that,",
    "start": "3822400",
    "end": "3830155"
  },
  {
    "text": "you know, you just remember things that you've already studied. So hopefully this is not an introduction to these subjects for you. All right.",
    "start": "3830155",
    "end": "3836305"
  },
  {
    "text": "So, uh, [NOISE] matrix calculus- calculus.",
    "start": "3836305",
    "end": "3846880"
  },
  {
    "text": "Right. Let's see- let's- let's look at a few functions. All right.",
    "start": "3846880",
    "end": "3855474"
  },
  {
    "text": "So we write a function. [NOISE] Right?",
    "start": "3855475",
    "end": "3861710"
  },
  {
    "text": "Is this notation familiar to everyone? What this means is we have a function that",
    "start": "3862440",
    "end": "3869500"
  },
  {
    "text": "we're going to call as- as F. F is the name that we assign, and this is like the type signature of that function.",
    "start": "3869500",
    "end": "3875500"
  },
  {
    "text": "It takes as input a real-valued number and produces as output a real-valued number.",
    "start": "3875500",
    "end": "3881965"
  },
  {
    "text": "Right? Now, um, an example of this is,",
    "start": "3881965",
    "end": "3888385"
  },
  {
    "text": "you know, x square. Take a number x, that's going to be the input,",
    "start": "3888385",
    "end": "3893830"
  },
  {
    "text": "you square it, that's going to be the output, right? Real value input real value output.",
    "start": "3893830",
    "end": "3899335"
  },
  {
    "text": "And the- the value of this function is real value.",
    "start": "3899335",
    "end": "3910490"
  },
  {
    "text": "Okay? This is just an example, um, the- the first derivative of this function is going to have a value in,",
    "start": "3911070",
    "end": "3922855"
  },
  {
    "text": "also in R, right? And the second derivative is going to have",
    "start": "3922855",
    "end": "3931090"
  },
  {
    "text": "a value also in R. So if you have a function tha- that takes a real value input real value output, for example x squared,",
    "start": "3931090",
    "end": "3936970"
  },
  {
    "text": "its value is- is real value, right? Its first derivative, in this case 2x is also going to be real value.",
    "start": "3936970",
    "end": "3942744"
  },
  {
    "text": "Its second derivative, x2, is also going to be real value.",
    "start": "3942745",
    "end": "3949720"
  },
  {
    "text": "Okay? Now, consider a function f,",
    "start": "3949720",
    "end": "3954730"
  },
  {
    "text": "[NOISE] What does this mean?",
    "start": "3954730",
    "end": "3960970"
  },
  {
    "text": "It means it's a function that we can call this as f. The input of the function is going to be some vector,",
    "start": "3960970",
    "end": "3966880"
  },
  {
    "text": "a d-dimensional vector, right? And the output is going to be a real-valued number, right?",
    "start": "3966880",
    "end": "3973165"
  },
  {
    "text": "So vector input, scalar output. And we're going to encounter a lot of functions of this kind in this course, right?",
    "start": "3973165",
    "end": "3982825"
  },
  {
    "text": "And the most common one that we are going to, uh, encounter is, or what you call as a loss function. [NOISE] Right?",
    "start": "3982825",
    "end": "3991240"
  },
  {
    "text": "We haven't covered what a loss function is yet, but, you know, this- this- this is just to give you a sense of what, uh, what's the kind of things you're going to be, uh, looking at.",
    "start": "3991240",
    "end": "3998380"
  },
  {
    "text": "Uh, the loss function is, that's a value, uh, a scalar value, right?",
    "start": "3998380",
    "end": "4005115"
  },
  {
    "text": "And what's the first derivative of a vector? What's- what's- what's the type of the first derivative of a function like this?",
    "start": "4005115",
    "end": "4012630"
  },
  {
    "text": "[BACKGROUND] Gradient. It's called the gradient and the gradient is going to be in RD.",
    "start": "4012630",
    "end": "4019630"
  },
  {
    "text": "And the second deriv- derivative of a function like this is called the [BACKGROUND] Hessian.",
    "start": "4023360",
    "end": "4028695"
  },
  {
    "text": "And the Hessian is [NOISE]",
    "start": "4028695",
    "end": "4039119"
  },
  {
    "text": "and the question in this case, not only is it a squared matrix, it is also guaranteed to be symmetric, right?",
    "start": "4039120",
    "end": "4047115"
  },
  {
    "text": "And symmetric matrices are also written, um, as S with just a single D,",
    "start": "4047115",
    "end": "4054195"
  },
  {
    "text": "because it's implied that it's D along both the dimensions because it's- it's symmetric, right?",
    "start": "4054195",
    "end": "4060615"
  },
  {
    "text": "And again, further later in the course, we're also going to encounter functions of this type.",
    "start": "4060615",
    "end": "4068160"
  },
  {
    "text": "RD to RP, and its vector input,",
    "start": "4068160",
    "end": "4075089"
  },
  {
    "text": "vector output and- and of course the dimensions are different, it- they could be the same, but in general, the dimensions are different.",
    "start": "4075090",
    "end": "4081780"
  },
  {
    "text": "Um, can anybody think of functions like this?",
    "start": "4081780",
    "end": "4087955"
  },
  {
    "text": "Why you might use such functions in machine learning? I mean, it's- it's totally fine if- if, uh,",
    "start": "4087955",
    "end": "4095250"
  },
  {
    "text": "um [BACKGROUND] , bias. So a- a function that takes a d-dimensional vector and outputs a p-dimensional vector.",
    "start": "4095250",
    "end": "4105659"
  },
  {
    "text": "Projections, and- and, uh, another, uh, um, um, a commonly used- used,",
    "start": "4105660",
    "end": "4111734"
  },
  {
    "text": "um, a component are going to be like a neural network layer.",
    "start": "4111735",
    "end": "4117060"
  },
  {
    "text": "So you're going to take, um, the way neural networks work is you- you transform one vector to another in- in some way,",
    "start": "4117060",
    "end": "4124784"
  },
  {
    "text": "and it's, uh, you know, one layer of neural network can- can look like this. It takes a d-dimensional vector as input and produces",
    "start": "4124785",
    "end": "4130409"
  },
  {
    "text": "some other dimensional vectors as output. And, is there a question?",
    "start": "4130410",
    "end": "4136275"
  },
  {
    "text": "Uh, I was going to say discrete classification to see bigger image. Yeah- yeah, classification, you- you feed a- an image as input and,",
    "start": "4136275",
    "end": "4145020"
  },
  {
    "text": "uh, you know, output o- of a vector of probabilities. Yeah, that's- that's, you know, that totally falls into this,",
    "start": "4145020",
    "end": "4150089"
  },
  {
    "text": "uh, category as well. That's a good answer. And so the output here is going to be an RP.",
    "start": "4150090",
    "end": "4155880"
  },
  {
    "text": "Right? And what's the first derivative of this? Of this kind of, uh,",
    "start": "4155880",
    "end": "4161520"
  },
  {
    "text": "uh, uh, function going to look? What's- what's the type of, it's called a Jacobian, exactly,",
    "start": "4161520",
    "end": "4166994"
  },
  {
    "text": "and it's going to be R of D by P or is it P by D,",
    "start": "4166995",
    "end": "4172455"
  },
  {
    "text": "anyway, one of the two is called the Jacobian.",
    "start": "4172455",
    "end": "4177820"
  },
  {
    "text": "Um, and that is why if- if- if, uh, if some of you are already familiar with neural networks,",
    "start": "4178130",
    "end": "4184259"
  },
  {
    "text": "you're going to encounter Jacobians a lot in neural networks because, you know, that's how you, um, um, train them, uh, by,",
    "start": "4184260",
    "end": "4189855"
  },
  {
    "text": "um, evaluating the Jacobians. And, uh, the second derivative for this is going to be some kind of,",
    "start": "4189855",
    "end": "4196845"
  },
  {
    "text": "um, um, D by P by P, some kind of, uh, you know, let's just call it a higher-order tensor.",
    "start": "4196845",
    "end": "4202350"
  },
  {
    "text": "[NOISE] Not very interested. All right.",
    "start": "4202350",
    "end": "4209340"
  },
  {
    "text": "So, um, we see right away that, you know, vectors and matrices are going to show up when you do multivariate calculus, right?",
    "start": "4209340",
    "end": "4218010"
  },
  {
    "text": "And the tools that we saw above, uh, for example,",
    "start": "4218010",
    "end": "4223695"
  },
  {
    "text": "eigenvalue decomposition, can be applied to Hessians and that's going to tell you a lot of",
    "start": "4223695",
    "end": "4230220"
  },
  {
    "text": "information about the nature of the function, right? Uh, for example, if you- if you take the Hessian of a function,",
    "start": "4230220",
    "end": "4237030"
  },
  {
    "text": "and if the Hessian of a function is positive semi-definite at all input points x,",
    "start": "4237030",
    "end": "4244289"
  },
  {
    "text": "then it means the function is kind of bowl-shaped, it is convex. If you take some function and if you,",
    "start": "4244290",
    "end": "4252705"
  },
  {
    "text": "uh, uh, calculates its Hessian, and if it is negative definite or negative semi-definite,",
    "start": "4252705",
    "end": "4257880"
  },
  {
    "text": "then it's kind of an inverted bowl shape, right? And if the Hessian of a function is indefinite,",
    "start": "4257880",
    "end": "4265110"
  },
  {
    "text": "which means it's neither positive definite or, you know, semi-definite, then it is [BACKGROUND] exactly,",
    "start": "4265110",
    "end": "4270960"
  },
  {
    "text": "it- it has saddle points. What does a saddle point mean? It means along one axis, it- it- it is, you know,",
    "start": "4270960",
    "end": "4277245"
  },
  {
    "text": "cup-shaped and along a- another axes it is, it's kind of bowl shaped, just like the shape of a saddle that you place on a horse.",
    "start": "4277245",
    "end": "4283995"
  },
  {
    "text": "It- it- goes down along one axis and go- and goes up on another axis, right? So the- the- the connection between",
    "start": "4283995",
    "end": "4291000"
  },
  {
    "text": "multi-variable calculus and linear algebra is- are- are very deep and- and you're going",
    "start": "4291000",
    "end": "4296760"
  },
  {
    "text": "to- you're going to be using an analyzing Hessians of different loss functions to kind of characterize the convex,",
    "start": "4296760",
    "end": "4304919"
  },
  {
    "text": "which means if they are convex, you know, that is good news. It means the- when you minimize the loss function,",
    "start": "4304919",
    "end": "4310559"
  },
  {
    "text": "you reach a unique global minimum, because if, you know, any bowl-shaped function or, you know, take any bowl,",
    "start": "4310560",
    "end": "4317730"
  },
  {
    "text": "there's always like a unique, uh, global minimum, whereas if- if- if a function is- is not convex or if it has,",
    "start": "4317730",
    "end": "4325215"
  },
  {
    "text": "you know, uh, uh, saddle points, then, you know, optimization is- is- is a little harder, trying to minimize the loss function is going to be harder, right?",
    "start": "4325215",
    "end": "4333135"
  },
  {
    "text": "Now, some examples of, uh, how we actually go about calculating, uh, gradients.",
    "start": "4333135",
    "end": "4338640"
  },
  {
    "text": "Uh, for example, if you have a function x, the gradient of the function with respect to x is written like this.",
    "start": "4338640",
    "end": "4345809"
  },
  {
    "text": "This- this is the terminology that we generally use, the inverted, um, uh, triangle, um, is- is- denotes the gradient symbol.",
    "start": "4345810",
    "end": "4355215"
  },
  {
    "text": "And if you- if you are writing your homeworks in LaTeX, then this, you know,",
    "start": "4355215",
    "end": "4360525"
  },
  {
    "text": "to get the symbol in LaTeX, you use backslash nabla,",
    "start": "4360525",
    "end": "4366855"
  },
  {
    "text": "um, and, um, the- the- the, uh,",
    "start": "4366855",
    "end": "4372090"
  },
  {
    "text": "uh, uh, subscript for nabla indicates what is the variable with respect to which you are, um, um, taking the, um,",
    "start": "4372090",
    "end": "4378940"
  },
  {
    "text": "um, gradient and, you know, for gradients, this is going to be vector-valued, obviously, and the definition is just this.",
    "start": "4378940",
    "end": "4386730"
  },
  {
    "text": "Take the, um, uh, partial derivative of f with respect to x1 and evaluate it as x.",
    "start": "4386730",
    "end": "4397125"
  },
  {
    "text": "So I'm going to use a red x here to indicate that this is the value that we're feeding and this is the variable",
    "start": "4397125",
    "end": "4405840"
  },
  {
    "text": "with respect to which we are differentiating [NOISE] XD of.",
    "start": "4405840",
    "end": "4418330"
  },
  {
    "text": "All right? This is just the definition of a gradient, which means take every, uh, uh,",
    "start": "4421460",
    "end": "4427110"
  },
  {
    "text": "it- because this is a vector valued, you know you can also kind of think of this as x,",
    "start": "4427110",
    "end": "4433420"
  },
  {
    "text": "x_1, x_2, x_d, right?",
    "start": "4435140",
    "end": "4440430"
  },
  {
    "text": "And this is just a short for- short notation for- for this. So the, the function has multiple inputs because you're feeding in a vector of",
    "start": "4440430",
    "end": "4447150"
  },
  {
    "text": "values and the gray- and its output is scalar. And the gradient is then defined as the partial derivative of f with",
    "start": "4447150",
    "end": "4456900"
  },
  {
    "text": "respect to every input evaluated at the,",
    "start": "4456900",
    "end": "4461985"
  },
  {
    "text": "the, the vector that you're feeding in. And this collection of partial derivatives is called a gradient, right?",
    "start": "4461985",
    "end": "4469125"
  },
  {
    "text": "And the gradient also has an interpretation which is, it is the- it is the direction of steepest ascent. What does that mean?",
    "start": "4469125",
    "end": "4477675"
  },
  {
    "text": "If you have, uh, um,",
    "start": "4477675",
    "end": "4480280"
  },
  {
    "text": "so let's assume this is x_1 and this is actually, you know, x_2 and this is f of x.",
    "start": "4482810",
    "end": "4492900"
  },
  {
    "text": "And a, a, a scalar valued vector input, input function is going to be some surface along this,",
    "start": "4492900",
    "end": "4501300"
  },
  {
    "text": "you know, in, in, in such a graph. And the, the- I'm not good at drawing surfaces.",
    "start": "4501300",
    "end": "4508349"
  },
  {
    "text": "Uh, but, you know, imagine there is some kind of a surface here, um, and the height from any point up to that surface is the value of that function, right?",
    "start": "4508349",
    "end": "4517140"
  },
  {
    "text": "It's, it's, it's similar to, you know, f of x except now it's- it,",
    "start": "4517140",
    "end": "4524039"
  },
  {
    "text": "it has, you know, uh, multiple, uh, input dimensions. Now, along the surface, um,",
    "start": "4524040",
    "end": "4529905"
  },
  {
    "text": "the surface has got to have some kind of, you know, a shape. And the gradient of the function at this input",
    "start": "4529905",
    "end": "4536640"
  },
  {
    "text": "tells you the direction in which you want to move to, to increase the value of the function the most, right?",
    "start": "4536640",
    "end": "4544800"
  },
  {
    "text": "If you want to, um, um, the- I'm just going to try to draw a surface, right?",
    "start": "4544800",
    "end": "4553505"
  },
  {
    "text": "Imagine this is some kind of a surface. Now, if you are at this point and you want to adjust your input to",
    "start": "4553505",
    "end": "4559610"
  },
  {
    "text": "a different value such that the function evaluates to a larger value, then you calculate the gradient.",
    "start": "4559610",
    "end": "4566485"
  },
  {
    "text": "At this point, it's going to give you some direction because it is a vector. It's going to give- point you in some direction.",
    "start": "4566485",
    "end": "4571605"
  },
  {
    "text": "And if you take a very small step along the direction from- and from this new point,",
    "start": "4571605",
    "end": "4577650"
  },
  {
    "text": "if you evaluate the value of the function, it should be, it should be larger, right?",
    "start": "4577650",
    "end": "4583010"
  },
  {
    "text": "It gives you the, the direction in which you want to move your inputs such that the output value goes up, right?",
    "start": "4583010",
    "end": "4589594"
  },
  {
    "text": "And similarly, you can also calculate the, the gradient of a function that takes as a matrix as input with respect to the matrix.",
    "start": "4589595",
    "end": "4600285"
  },
  {
    "text": "Okay? It- that- this is just a generalization of this, right? And this is going to be again,",
    "start": "4600285",
    "end": "4607690"
  },
  {
    "text": "a whole collection of, of partial derivatives.",
    "start": "4608060",
    "end": "4614290"
  },
  {
    "text": "So even though this is kind of a- A is,",
    "start": "4646070",
    "end": "4651840"
  },
  {
    "text": "is two-dimensional, just pretend it's one long vector, kind of serialize it into one long vector, right?",
    "start": "4651840",
    "end": "4657900"
  },
  {
    "text": "Calculate the partial derivatives and then reshape it back into, you know, um, a, um, a matrix.",
    "start": "4657900",
    "end": "4665760"
  },
  {
    "text": "Any, any question? Yes? F is a function? Yes, F is a function that takes A as input,",
    "start": "4665760",
    "end": "4673260"
  },
  {
    "text": "a matrix as input, and produces a scalar as output, in this case.",
    "start": "4673260",
    "end": "4679185"
  },
  {
    "text": "In this case, we're looking at, uh, f as a function where it takes a vector as input and scalar as output. Yes?",
    "start": "4679185",
    "end": "4686740"
  },
  {
    "text": "So you reshape the output in the shape, uh, of the input? Yeah, yeah, exactly. So you know,",
    "start": "4689060",
    "end": "4695975"
  },
  {
    "text": "uh, A is going to be some kind of, you know, uh, um, some kind of a, um,",
    "start": "4695975",
    "end": "4702840"
  },
  {
    "text": "you know, A, it's going to have a_11, a_1, you know n,",
    "start": "4702890",
    "end": "4709770"
  },
  {
    "text": "assuming it is m by n, you know a_n1 till- a_m1 to a_mn, right?",
    "start": "4709770",
    "end": "4718655"
  },
  {
    "text": "So you- you're feeding, uh, you know, these many different inputs, uh, to f. The output is, is a scalar.",
    "start": "4718655",
    "end": "4725505"
  },
  {
    "text": "So take the partial derivative of the output of f with respect to a_11 and,",
    "start": "4725505",
    "end": "4731159"
  },
  {
    "text": "you know, and, and so on. Any questions about this? All right.",
    "start": "4731160",
    "end": "4740800"
  },
  {
    "text": "Now, um, so this was",
    "start": "4741230",
    "end": "4749850"
  },
  {
    "text": "f from R_m by n to R, right?",
    "start": "4749850",
    "end": "4758400"
  },
  {
    "text": "And over here f was from R. What could I use?",
    "start": "4758400",
    "end": "4766920"
  },
  {
    "text": "d to R. All right.",
    "start": "4766920",
    "end": "4772230"
  },
  {
    "text": "We could also look at, um, what does, what does the Hessian where x is again,",
    "start": "4772230",
    "end": "4781800"
  },
  {
    "text": "so f is from R_d to R. Now,",
    "start": "4781800",
    "end": "4788280"
  },
  {
    "text": "the Hessian is going to look like this.",
    "start": "4788280",
    "end": "4792699"
  },
  {
    "text": "Now, instead of taking the first partial derivative, we're going to take, you know, two partial derivatives,",
    "start": "4793550",
    "end": "4798840"
  },
  {
    "text": "which means it's going to be del square f by del x_1,",
    "start": "4798840",
    "end": "4804945"
  },
  {
    "text": "del x_2 of [NOISE] x",
    "start": "4804945",
    "end": "4815349"
  },
  {
    "text": "del square i, del x_d,",
    "start": "4820100",
    "end": "4825855"
  },
  {
    "text": "del x_d of f of x.",
    "start": "4825855",
    "end": "4834150"
  },
  {
    "text": "Similarly, del square of del x_1,",
    "start": "4834150",
    "end": "4842475"
  },
  {
    "text": "x_d, x_d, del x_d f of [NOISE] x.",
    "start": "4842475",
    "end": "4852280"
  },
  {
    "text": "Right? So here we take the second partial derivatives of f with respect to every input.",
    "start": "4853630",
    "end": "4860120"
  },
  {
    "text": "Yes, question? [BACKGROUND]. Oh, yeah, you are right. Thank you.",
    "start": "4860120",
    "end": "4865139"
  },
  {
    "text": "X_1, x_1, x_1, x_d, x_d, x_d, and this will be a squared",
    "start": "4866080",
    "end": "4871280"
  },
  {
    "text": "x_d x_1 f of x.",
    "start": "4871280",
    "end": "4880969"
  },
  {
    "text": "The reason I'm- I'm- I'm highlighting things in red is to tell you that, um, by performing the partial, um, um,",
    "start": "4880970",
    "end": "4888125"
  },
  {
    "text": "the- the partial derivative or two times, uh, partial derivatives, you're gonna end up with a function.",
    "start": "4888125",
    "end": "4893195"
  },
  {
    "text": "And that function is this function for which you feed any arbitrary x as- as the input, right?",
    "start": "4893195",
    "end": "4902525"
  },
  {
    "text": "Is this clear? Good. And a few examples.",
    "start": "4902525",
    "end": "4910370"
  },
  {
    "text": "So, um, now, so- so- some- a few- few things that you're gonna be using,",
    "start": "4910370",
    "end": "4917195"
  },
  {
    "text": "you know, very liberally throughout. What is the partial derivative, uh, or what's the gradient with respect to x of b transpose x,",
    "start": "4917195",
    "end": "4926719"
  },
  {
    "text": "where b is some vector, some constant vector. You- you take a dot product with x and we wanna",
    "start": "4926720",
    "end": "4933800"
  },
  {
    "text": "calculate the- the- this- this is scalar value, b transpose x is a scalar,",
    "start": "4933800",
    "end": "4939440"
  },
  {
    "text": "and we wanna calculate the gradient of b transpose x with respect to x where b is some constant, b, right?",
    "start": "4939440",
    "end": "4946805"
  },
  {
    "text": "And why is that? We apply this directly. So by definition, this is- I'm just gonna write for some ith element.",
    "start": "4946805",
    "end": "4958385"
  },
  {
    "text": "So partial, uh, of x_i of b transpose x, right?",
    "start": "4958385",
    "end": "4968300"
  },
  {
    "text": "I have just applied this here. And this is gonna be x_i of b_1 x_1",
    "start": "4968300",
    "end": "4980704"
  },
  {
    "text": "plus b_2 x_2 plus b_d x_d, right?",
    "start": "4980705",
    "end": "4990425"
  },
  {
    "text": "And when- when- when- when you calculate the partial derivative with respect to this, all the terms except b_i x_i are gonna cancel out,",
    "start": "4990425",
    "end": "4997655"
  },
  {
    "text": "um, because the rest of them are all constants with respect to i. So maybe let's call this b_i,",
    "start": "4997655",
    "end": "5004465"
  },
  {
    "text": "b_i,",
    "start": "5004465",
    "end": "5012030"
  },
  {
    "text": "x_i, okay? And this is gonna give you afterward b_i,",
    "start": "5014220",
    "end": "5022150"
  },
  {
    "text": "and this is just b, right?",
    "start": "5022150",
    "end": "5027640"
  },
  {
    "text": "Is this clear? The- the- the, uh, the- the function is simple,",
    "start": "5027640",
    "end": "5033490"
  },
  {
    "text": "but the methodology is the same for any- any arbitrary complex function. Where we- the gradient is just",
    "start": "5033490",
    "end": "5040360"
  },
  {
    "text": "defined as the collection of partial derivatives with respect to every input. And then you expand out the function,",
    "start": "5040360",
    "end": "5046389"
  },
  {
    "text": "calculate the partial derivative with respect to the corresponding input. And- and then you finally see c- you know,",
    "start": "5046390",
    "end": "5051985"
  },
  {
    "text": "is this a pattern that we recognize? And in this case it happens to be just b, otherwise your- your- your gradient is just some expression where, um,",
    "start": "5051985",
    "end": "5059005"
  },
  {
    "text": "in general it may not have, you know, a simple expression like this, right?",
    "start": "5059005",
    "end": "5065590"
  },
  {
    "text": "And so that's- that's how we calculate gradients for simple functions,",
    "start": "5065590",
    "end": "5071920"
  },
  {
    "text": "with respect to time.",
    "start": "5071920",
    "end": "5078100"
  },
  {
    "text": "[NOISE]",
    "start": "5078100",
    "end": "5083810"
  },
  {
    "text": "Okay?",
    "start": "5088200",
    "end": "5090380"
  },
  {
    "text": "And similarly, you can also apply, um, just like regular calculus,",
    "start": "5102720",
    "end": "5108280"
  },
  {
    "text": "you can apply the product rule. And we're gonna see just one example of the product rule.",
    "start": "5108280",
    "end": "5116364"
  },
  {
    "text": "So what does the, uh, gradient with respect to x of x transpose Ax, right?",
    "start": "5116365",
    "end": "5125575"
  },
  {
    "text": "We're gonna apply the product rule as we know it. So this is gonna be the gradient with respect to,",
    "start": "5125575",
    "end": "5134450"
  },
  {
    "text": "um, let me use two colors to- to,",
    "start": "5135360",
    "end": "5139010"
  },
  {
    "text": "um, right? Or precisely, right?",
    "start": "5156120",
    "end": "5164245"
  },
  {
    "text": "Treat it as a product of two things and, you know, um, ah, ah, by them, uh, uh, differently.",
    "start": "5164245",
    "end": "5170980"
  },
  {
    "text": "And this is going to, um, come out to be, um, um,",
    "start": "5170980",
    "end": "5177295"
  },
  {
    "text": "so here the- the gradient here is, um, you can think of this as gradient of x with,",
    "start": "5177295",
    "end": "5188330"
  },
  {
    "text": "sorry, I forgot a transpose here, right? So, yeah, so this is gonna be, um,",
    "start": "5193500",
    "end": "5200350"
  },
  {
    "text": "just Ax d",
    "start": "5200350",
    "end": "5205450"
  },
  {
    "text": "transpose- Ax,",
    "start": "5205450",
    "end": "5211045"
  },
  {
    "text": "and this is going to be, um, A transpose x, right? So, um, the- this is gonna be x times A plus A transpose.",
    "start": "5211045",
    "end": "5221080"
  },
  {
    "text": "And if A is- is, uh, uh, symmetric, this is going to be just 2Ax, right?",
    "start": "5221080",
    "end": "5227470"
  },
  {
    "text": "A few more, um, matrix derivates which are gonna be very useful.",
    "start": "5227470",
    "end": "5235645"
  },
  {
    "text": "Yeah. [BACKGROUND]. Uh, I'm sorry. What's the- what's the question.",
    "start": "5235645",
    "end": "5243880"
  },
  {
    "text": "[BACKGROUND]. Oh, this is the product rule, so for example, um,",
    "start": "5243880",
    "end": "5249280"
  },
  {
    "text": "um, when you are taking, um, so d by dx of f of x,",
    "start": "5249280",
    "end": "5259900"
  },
  {
    "text": "g of x is equal to d by dx of f of x times g of",
    "start": "5259900",
    "end": "5268975"
  },
  {
    "text": "x plus f of x times d by dx of g of x, right?",
    "start": "5268975",
    "end": "5277960"
  },
  {
    "text": "This is the product rule, and this is the multivariate version of that. You think of this as f of x and g of x?",
    "start": "5277960",
    "end": "5284590"
  },
  {
    "text": "[BACKGROUND]. Yeah. [BACKGROUND] Exactly.",
    "start": "5284590",
    "end": "5290860"
  },
  {
    "text": "Exactly. Exactly. You know, because this is- when I- when I- when I write it as, you know, uh, the gradient,",
    "start": "5290860",
    "end": "5296320"
  },
  {
    "text": "it's not clear to which parts I'm applying it to, right? So the red parts are the parts that are getting differentiated, the black parts,",
    "start": "5296320",
    "end": "5302725"
  },
  {
    "text": "you treat them as constants, you know, just like this. [BACKGROUND]. I'm sorry.",
    "start": "5302725",
    "end": "5308110"
  },
  {
    "text": "[BACKGROUND] You could- you could do that too, but, you know, generally,",
    "start": "5308110",
    "end": "5313525"
  },
  {
    "text": "you- yeah, you could- you could- you could do that too. Yeah, you could. [BACKGROUND].",
    "start": "5313525",
    "end": "5319989"
  },
  {
    "text": "Yeah, you could, you could, right? So that's, uh, you know, this is- this is the product rule.",
    "start": "5319990",
    "end": "5326635"
  },
  {
    "text": "Um, another, uh, very useful, um, identity that you're gonna be using is gonna be,",
    "start": "5326635",
    "end": "5335090"
  },
  {
    "text": "um, gradient of A of the log of the determinant of A.",
    "start": "5335430",
    "end": "5342340"
  },
  {
    "text": "And this looks pretty- pretty nasty. So what's happening here? A is a matrix. Take the determinant of the matrix,",
    "start": "5342340",
    "end": "5349119"
  },
  {
    "text": "and then you take the log of the determinant of the matrix and you wanna differentiate it with respect to A. Why would you want to do that?",
    "start": "5349120",
    "end": "5354715"
  },
  {
    "text": "I mean, [LAUGHTER] uh, but it- it- it turns out that this is gonna be a recurring,",
    "start": "5354715",
    "end": "5360534"
  },
  {
    "text": "um, uh, um, uh, pattern. This is gonna show up in multiple places, um, especially when you are- you are dealing with,",
    "start": "5360535",
    "end": "5367405"
  },
  {
    "text": "you know, Gaussian distributions and stuff. You remember the Gaussian has a determinant, in- in- in the- in the denominator, anyways.",
    "start": "5367405",
    "end": "5373239"
  },
  {
    "text": "But this is gonna be, um, uh, this is gonna show up many times in your homeworks,",
    "start": "5373240",
    "end": "5379050"
  },
  {
    "text": "possibly in your exams. Um, and the- the, um, the answer is just A inverse, right?",
    "start": "5379050",
    "end": "5385885"
  },
  {
    "text": "And the intuition is, you know, d by dt of- or d by dx of log x is x inverse.",
    "start": "5385885",
    "end": "5396825"
  },
  {
    "text": "It's- it's, you know, think of it like that.",
    "start": "5396825",
    "end": "5399880"
  },
  {
    "text": "All right, uh, we have another 20 minutes remaining.",
    "start": "5408270",
    "end": "5413275"
  },
  {
    "text": "Um, let's see here. I'm gonna do a quick review of probability in the meantime.",
    "start": "5413275",
    "end": "5418570"
  },
  {
    "text": "Uh, do you have any other questions? Yes, question. [BACKGROUND].",
    "start": "5418570",
    "end": "5436240"
  },
  {
    "text": "I'm sorry. [BACKGROUND] This one?",
    "start": "5436240",
    "end": "5442240"
  },
  {
    "text": "[BACKGROUND] The first thing? [BACKGROUND] Above it? [BACKGROUND]. Well, I mean,",
    "start": "5442240",
    "end": "5451660"
  },
  {
    "text": "do you- do you agree with this? I mean, so this is- this is just the same thing in a- in a- in a multivariate setting.",
    "start": "5451660",
    "end": "5459219"
  },
  {
    "text": "[NOISE].",
    "start": "5459220",
    "end": "5475410"
  },
  {
    "text": "[BACKGROUND]. It is pretty straightforward. It's in the- in the- in the lecture notes. I mean, in the- in the review notes,",
    "start": "5475410",
    "end": "5481160"
  },
  {
    "text": "steps posted online, you can go through the steps. [NOISE] All right, so this- we're gonna switch gears now and,",
    "start": "5481160",
    "end": "5487655"
  },
  {
    "text": "um, briefly review probability theory. Um, that was it about in terms of,",
    "start": "5487655",
    "end": "5494465"
  },
  {
    "text": "um, um, review of matrix calculus and linear algebra. Um, so we're gonna switch gears to",
    "start": "5494465",
    "end": "5501650"
  },
  {
    "text": "probability theory now and this is gonna be the last- um, um, last topic that we're gonna review and from next class,",
    "start": "5501650",
    "end": "5508219"
  },
  {
    "text": "we're gonna, you know, start machine learning and, you know, with linear regression. All right. Um, again, um,",
    "start": "5508220",
    "end": "5515510"
  },
  {
    "text": "treat this as a review and not as your introduction to the subject. We are not gonna teach it in the way it has to be taught to a student the first time.",
    "start": "5515510",
    "end": "5525680"
  },
  {
    "text": "We are just gonna review things so that it refreshes your memory. Okay, so first of all,",
    "start": "5525680",
    "end": "5532400"
  },
  {
    "text": "what are the basic elements of probability theory, right? So in probability theory, there is- um,",
    "start": "5532400",
    "end": "5538940"
  },
  {
    "text": "probability is- is basically the study of uncertainty about, um, things that can happen randomly, all right?",
    "start": "5538940",
    "end": "5545900"
  },
  {
    "text": "And whenever we are talking about probability, there is always an implicit sample space.",
    "start": "5545900",
    "end": "5553954"
  },
  {
    "text": "So sample space are- is- is basically the set of all- all outcomes that can happen,",
    "start": "5553955",
    "end": "5560480"
  },
  {
    "text": "random outcomes that can happen, all right? For example, if we- um,",
    "start": "5560480",
    "end": "5565970"
  },
  {
    "text": "your sample space can be what- what- what- what is the sequence of,",
    "start": "5565970",
    "end": "5571190"
  },
  {
    "text": "um, um, coin tosses you get if you- if you flip it twice, right? Is it gonna be heads heads or is it gonna be heads tails",
    "start": "5571190",
    "end": "5578195"
  },
  {
    "text": "is it going to be tails heads, tails tails, right? Now, if- if the experiment that you are performing is two coin tosses,",
    "start": "5578195",
    "end": "5586894"
  },
  {
    "text": "then any of these is a possible outcome. Right? And the set of all possible outcomes are",
    "start": "5586894",
    "end": "5595040"
  },
  {
    "text": "called- it- are- are- are- are- are- is basically called the sample space.",
    "start": "5595040",
    "end": "5600245"
  },
  {
    "text": "Right? Now, um, the thing that we assign probabilities to are called events. All right?",
    "start": "5600245",
    "end": "5609185"
  },
  {
    "text": "An event is a subset of the sample space. All right?",
    "start": "5609185",
    "end": "5614990"
  },
  {
    "text": "Uh, you want to think of- um, let's see, an event is- is a subset of- of- um, the, uh, uh,",
    "start": "5614990",
    "end": "5623360"
  },
  {
    "text": "sample space and in this case, you know, for example, if A is- is- is some- some event, uh,",
    "start": "5623360",
    "end": "5628449"
  },
  {
    "text": "where it- it includes, you know, heads heads and heads tails. Uh, basically, we are saying we are interested in the event that",
    "start": "5628450",
    "end": "5635679"
  },
  {
    "text": "the first coin toss turns out to be a- a heads. All right?",
    "start": "5635680",
    "end": "5640915"
  },
  {
    "text": "And the full- the entire probabili- sample space is also an event,",
    "start": "5640915",
    "end": "5646385"
  },
  {
    "text": "which means we are interested in, you know, anything that happens though it's not very interesting. All right? So that's- that's the event space,",
    "start": "5646385",
    "end": "5653929"
  },
  {
    "text": "the set of all possible subsets of your sample space, okay? And now if your sample space is- is finite,",
    "start": "5653930",
    "end": "5661730"
  },
  {
    "text": "the- the event space is basically the power set, which means the set of all subsets is- is the, um, event space.",
    "start": "5661730",
    "end": "5671285"
  },
  {
    "text": "And then we assign something called as a probability measure. Now, the probability measure takes us input an event, not an outcome,",
    "start": "5671285",
    "end": "5678935"
  },
  {
    "text": "an event, and assigns it a value between 0 and 1, right? That's- that's- that's something you wanna, um, um,",
    "start": "5678935",
    "end": "5685265"
  },
  {
    "text": "keep in mind that we don't assign probabilities to outcomes, but we actually assign probabilities to events in",
    "start": "5685265",
    "end": "5691489"
  },
  {
    "text": "the case of finite- um, finite sample space, the distinction is- is- is moot because you",
    "start": "5691490",
    "end": "5698570"
  },
  {
    "text": "can always create a subset that has only one- um, that has only one event and assign a probability of that subset.",
    "start": "5698570",
    "end": "5705560"
  },
  {
    "text": "Uh, but when we, you know, move on to, uh, continuous space- uh, uh,",
    "start": "5705560",
    "end": "5710930"
  },
  {
    "text": "or continuous-valued samples, then, you know, the distinction became- becomes, uh, more relevant.",
    "start": "5710930",
    "end": "5716630"
  },
  {
    "text": "Now the- the- the- the three main axioms- uh, the three axioms of probability are that the probability assigned",
    "start": "5716630",
    "end": "5724520"
  },
  {
    "text": "to an event is always greater than equal to 0 for all events. Again, the- we're not talking about outcomes,",
    "start": "5724520",
    "end": "5731390"
  },
  {
    "text": "we're talking about events. Events are- are sets of outcomes, are, you know, subset of- of the sample space.",
    "start": "5731390",
    "end": "5738455"
  },
  {
    "text": "The probability assigned to the entire event space or- or- or the entire sample space is always 1,",
    "start": "5738455",
    "end": "5746525"
  },
  {
    "text": "which means some event or the other in- from the sample space will always occur.",
    "start": "5746525",
    "end": "5752945"
  },
  {
    "text": "Right? And if you take disjoint events where the- the- the intersection between any two events are- is the null set,",
    "start": "5752945",
    "end": "5760750"
  },
  {
    "text": "then the probability assigned to the union of the- of- of those disjoint events is equal to just the sum of the individual probabilities, right?",
    "start": "5760750",
    "end": "5770515"
  },
  {
    "text": "All- all very intuitive. There's- there's- there's nothing, uh, fancy happening here. Then, uh, conditional probability, um,",
    "start": "5770515",
    "end": "5778014"
  },
  {
    "text": "let B be an event such that the probability of B is not zero. Again, B is an event, not an outcome,",
    "start": "5778015",
    "end": "5784290"
  },
  {
    "text": "which means that's we are talking about sets of outcomes, and probability of A given B is- is- this is just the definition.",
    "start": "5784290",
    "end": "5792980"
  },
  {
    "text": "Um, it's the probability assigned to the intersection of A and B. So the intersection of A and B is also a set and therefore also an event.",
    "start": "5792980",
    "end": "5802909"
  },
  {
    "text": "Uh, so it's the event of A intersection B divided by the probability of, uh, uh, the event of B.",
    "start": "5802910",
    "end": "5810665"
  },
  {
    "text": "Now, uh, A and B are independent. So the inverted T symbol means independent if the probability of",
    "start": "5810665",
    "end": "5818614"
  },
  {
    "text": "A intersection B is the same as the probability of A times the probability of B. All right?",
    "start": "5818615",
    "end": "5823640"
  },
  {
    "text": "And, uh, to- to- I mean, this should- this should make sense, right? A intersection B is always going to be a subset of A of subset of",
    "start": "5823640",
    "end": "5830930"
  },
  {
    "text": "B and probability of A and probability of B, are, you know, values between 0 and 1 and when you multiply",
    "start": "5830930",
    "end": "5836960"
  },
  {
    "text": "two numbers that are smaller than the- one, then you're going to get a value which is, you know, even- even- even smaller.",
    "start": "5836960",
    "end": "5844110"
  },
  {
    "text": "Again, uh, probability of, uh, uh, A and B are independent events if and only if probability of A given B, uh,",
    "start": "5845440",
    "end": "5855320"
  },
  {
    "text": "is- is equal to the probability of A, which means the probability of A does not change whether B occurred or not.",
    "start": "5855320",
    "end": "5863570"
  },
  {
    "text": "Right? So you- you- you- you- then A and B are independent.",
    "start": "5863570",
    "end": "5869045"
  },
  {
    "text": "Now, random variables. Now consider, um, um, an event where, you know,",
    "start": "5869045",
    "end": "5875824"
  },
  {
    "text": "the experiment is, say, 10- 10 coin tosses and, you know, this is one such event. Now a random variable is a function that maps outcomes to real values.",
    "start": "5875825",
    "end": "5887525"
  },
  {
    "text": "Now we're not talking about events, we are talking about outcomes. You know, the- the- the- um, the outcomes of- of- of,",
    "start": "5887525",
    "end": "5893690"
  },
  {
    "text": "uh, uh, what happens in- in a- a- a random- as a random occurrence, right? A random variable is a mapping from outcomes to real values.",
    "start": "5893690",
    "end": "5904114"
  },
  {
    "text": "This- this is just an example of, you know, a- a random variable here, we're given an outcome,",
    "start": "5904115",
    "end": "5910265"
  },
  {
    "text": "you know that sequence of heads and tails, the function just counts the number of heads in it. You know, that's- that's just a function and you know,",
    "start": "5910265",
    "end": "5917315"
  },
  {
    "text": "the- the output is- is a real value. So it- it- it's kind of useful to, uh, think about it like this.",
    "start": "5917315",
    "end": "5923405"
  },
  {
    "text": "Um, I'm gonna- I'm gonna- I'm gonna call this the- the,",
    "start": "5923405",
    "end": "5931039"
  },
  {
    "text": "uh, uh, outcome space and this to be the real line.",
    "start": "5931040",
    "end": "5940115"
  },
  {
    "text": "Right? Now, the outcome space has, you know, lots of different actual outcomes,",
    "start": "5940115",
    "end": "5948515"
  },
  {
    "text": "and then on top of these outcomes, we define events. Right? And events could be overlapping.",
    "start": "5948515",
    "end": "5956225"
  },
  {
    "text": "Right? And a random variable is a function that maps outcomes to real values and here, this is- you know,",
    "start": "5956225",
    "end": "5966245"
  },
  {
    "text": "this is straight line, which means- which means to say, you know, it's- you know, there's just the real line and I intentionally draw",
    "start": "5966245",
    "end": "5972200"
  },
  {
    "text": "the outcome space with a wiggly line because this is just a bag of things that happen. There is no natural order among these, right?",
    "start": "5972200",
    "end": "5979535"
  },
  {
    "text": "So if- if you- if you take a die, which is colored, for example, without the numbers 1, 2, 3 on it and you roll the die,",
    "start": "5979535",
    "end": "5986360"
  },
  {
    "text": "some colors are going to show up, right? And you can think of those as the different outcomes where colors are not ordered.",
    "start": "5986360",
    "end": "5992675"
  },
  {
    "text": "But then you can define a random variable which maps each side of the die to a number.",
    "start": "5992675",
    "end": "5998030"
  },
  {
    "text": "For example, you- you number them 1, 2, 3. You know, that's essentially you're defining a random variable where you are now mapping the random events to the real line.",
    "start": "5998030",
    "end": "6006580"
  },
  {
    "text": "right? So, you know, this could be a random variable.",
    "start": "6006580",
    "end": "6012324"
  },
  {
    "text": "A random variable is a function, you know, it's a- it's a very poorly chosen name. A random variable is neither random, neither is it a variable.",
    "start": "6012325",
    "end": "6018700"
  },
  {
    "text": "It's just a function. It's a function that maps outcomes to real values. I mean, it's- it's- uh, it's just like computer science. It's just a horrible name.",
    "start": "6018700",
    "end": "6027040"
  },
  {
    "text": "It's not about computers, it's not a science, but you still call it computer science, right? So, uh, the- a random variable is neither random nor,",
    "start": "6027040",
    "end": "6034840"
  },
  {
    "text": "uh, is it a variable. It's just a function that maps outcomes to- to the real line.",
    "start": "6034840",
    "end": "6041909"
  },
  {
    "text": "Right? However, we assign probabilities to events.",
    "start": "6041910",
    "end": "6049650"
  },
  {
    "text": "Right? We don't assign probabilities to outcomes but we assign probabilities to events. And the reason why we- we- we kind of,",
    "start": "6049650",
    "end": "6056625"
  },
  {
    "text": "are interested in random- random variables is because, uh, by defining a random variable,",
    "start": "6056625",
    "end": "6063574"
  },
  {
    "text": "we are now kind of, uh, bringing any experiment into a level playing field.",
    "start": "6063575",
    "end": "6069265"
  },
  {
    "text": "You know- you map them into the real line and you start the rest of your analysis starting with the real line, right?",
    "start": "6069265",
    "end": "6075550"
  },
  {
    "text": "You're kind of abstracting away what the specific details of the random event are, just map them onto the real line and now, you know,",
    "start": "6075550",
    "end": "6082105"
  },
  {
    "text": "you can- you can have like a unified theory of- of what you can, you know, um, do with, you know, random things happening on the real line.",
    "start": "6082105",
    "end": "6089690"
  },
  {
    "text": "Right? So, uh, by, uh, uh value of X, we mean, you know, the set of all values that X can- X can take.",
    "start": "6090030",
    "end": "6097315"
  },
  {
    "text": "So in this case, you know, um, this random variable can take any of these values.",
    "start": "6097315",
    "end": "6103789"
  },
  {
    "text": "And if it's a discrete random variable, then your function will look like this. Each thing gets mapped to a different number,",
    "start": "6105900",
    "end": "6114265"
  },
  {
    "text": "each outcome gets mapped to a different number, right? And this is going to be a discrete random variable, right?",
    "start": "6114265",
    "end": "6121405"
  },
  {
    "text": "And the val- value of that would be, you know, different values.",
    "start": "6121405",
    "end": "6127600"
  },
  {
    "text": "Random value can- the random variable can take. And then we have something called the cumulative distribution function.",
    "start": "6127600",
    "end": "6136090"
  },
  {
    "text": "Now the- the, as I said earlier, ah, probabilities are defined on events,",
    "start": "6136090",
    "end": "6145165"
  },
  {
    "text": "and that probability is being defined in this space on the events.",
    "start": "6145165",
    "end": "6150850"
  },
  {
    "text": "Now the cumulative distribution function is trying to map you know, these probabilities to probabilities along the real value, right?",
    "start": "6150850",
    "end": "6159655"
  },
  {
    "text": "Um, the definition of the cdf is fx of x is equal to the probability that X is less than x.",
    "start": "6159655",
    "end": "6170890"
  },
  {
    "text": "And what this actually means is what- what does this actually mean? It means the probability assigned to the set of all outcomes omega,",
    "start": "6170890",
    "end": "6183164"
  },
  {
    "text": "such that X of omega is less than,",
    "start": "6183165",
    "end": "6189300"
  },
  {
    "text": "maybe, let me call this t, just less than t, right? What does it mean?",
    "start": "6189300",
    "end": "6195985"
  },
  {
    "text": "So look at your random variable that is some choose a t",
    "start": "6195985",
    "end": "6201685"
  },
  {
    "text": "and get the set of all omegas such that x of t,",
    "start": "6201685",
    "end": "6209200"
  },
  {
    "text": "x- x of omega is less than t, right? So in this case it is this one,",
    "start": "6209200",
    "end": "6214915"
  },
  {
    "text": "this one, this one. And create a set out of those ah,",
    "start": "6214915",
    "end": "6223719"
  },
  {
    "text": "ah create a set out of- of- create an event out of those outcomes and measure the probability on that event.",
    "start": "6223720",
    "end": "6230560"
  },
  {
    "text": "Because we always measure the probability in- in the sample space or- or the event space, right?",
    "start": "6230560",
    "end": "6236034"
  },
  {
    "text": "And we pretend that the cdf is measuring probabilities on the real line.",
    "start": "6236035",
    "end": "6241480"
  },
  {
    "text": "But actually what it does is we map it back, find the pre-image corresponding to- ah,",
    "start": "6241480",
    "end": "6247675"
  },
  {
    "text": "corresponding to the- the set we are interested and measure the probability in the event space, right?",
    "start": "6247675",
    "end": "6255565"
  },
  {
    "text": "And the cdf looks like this. Um, a cdf looks like this because er,",
    "start": "6255565",
    "end": "6261520"
  },
  {
    "text": "the picture that we see- that we see is- is, you know, you gotta flip it over here.",
    "start": "6261520",
    "end": "6268210"
  },
  {
    "text": "We're mapping from the- the outcomes to the real line. But over here, you know,",
    "start": "6268210",
    "end": "6273940"
  },
  {
    "text": "once we are- once we are kind of comfortable with this agreement, you know, moving forward, we start with the real line as- as- as our basis and define probabilities there. Yep.",
    "start": "6273940",
    "end": "6285220"
  },
  {
    "text": "[inaudible]",
    "start": "6285220",
    "end": "6294940"
  },
  {
    "text": "Yeah. So omega is the sequence of heads tails, that the long sequence is omega.",
    "start": "6294940",
    "end": "6302530"
  },
  {
    "text": "It's just a string of heads, tails, whatever. And then given that as an input, you can count the number of heads in it.",
    "start": "6302530",
    "end": "6309055"
  },
  {
    "text": "And the function that takes the string as input and returns the number as output is the random variable.",
    "start": "6309055",
    "end": "6316370"
  },
  {
    "text": "Okay? And then you have discrete versus continuous random variables. Uh, we went through this again already.",
    "start": "6317700",
    "end": "6324370"
  },
  {
    "text": "Um, and I'm just gonna skip over these.",
    "start": "6324370",
    "end": "6328580"
  },
  {
    "text": "And ah, the way we ah- ah calculate the- so- so the cdf is defined this way, right?",
    "start": "6331680",
    "end": "6339715"
  },
  {
    "text": "And the cdf is essentially gives you- you know, it- it- it kind of decoupled you from",
    "start": "6339715",
    "end": "6346810"
  },
  {
    "text": "the event space and allows you to only think about the- the real line.",
    "start": "6346810",
    "end": "6352045"
  },
  {
    "text": "And once we define the random variable, we- we forget what the event space was, what the outcome space was,",
    "start": "6352045",
    "end": "6357369"
  },
  {
    "text": "and- and I only deal with the real thing. And so the, um, the cdf tells you what is the probability that the, uh,",
    "start": "6357370",
    "end": "6367660"
  },
  {
    "text": "that uh, your random variable is less than or equal to some value t. And the height of the cdf is,",
    "start": "6367660",
    "end": "6375155"
  },
  {
    "text": "um, um, gives you that- that value. And the cdf is always between 0 and 1, right?",
    "start": "6375155",
    "end": "6382335"
  },
  {
    "text": "At minus infinity, the cdf is 0, at plus infinity, the cdf reaches 1.",
    "start": "6382335",
    "end": "6388060"
  },
  {
    "text": "And the probability mass function is- is defined for discrete random variables",
    "start": "6390240",
    "end": "6397360"
  },
  {
    "text": "where there is a probability assigned to an- to uh a given number.",
    "start": "6397360",
    "end": "6403705"
  },
  {
    "text": "So a probability mass function would look like this. This is r, then at 0, you have some height.",
    "start": "6403705",
    "end": "6411220"
  },
  {
    "text": "Where the height describes the probability assigned, right?",
    "start": "6411220",
    "end": "6417310"
  },
  {
    "text": "And again, all of these will be between 0 and 1, and the sum of all these heights should add up to 1, right?",
    "start": "6417310",
    "end": "6424285"
  },
  {
    "text": "That's a probability mass function. Similarly, if it's ah, ah, continuous valued, then you have a probability density function.",
    "start": "6424285",
    "end": "6433520"
  },
  {
    "text": "And the probability density function is basically the derivative of the probability ah,",
    "start": "6434130",
    "end": "6440034"
  },
  {
    "text": "cumulative- ah, cumulative density function, or the cdf.",
    "start": "6440035",
    "end": "6445430"
  },
  {
    "text": "Expected value. So this is probably uh, the most important concept that we are going to encounter.",
    "start": "6447120",
    "end": "6454690"
  },
  {
    "text": "And I'm just going to finish expectation and then we can break for the day.",
    "start": "6454690",
    "end": "6459920"
  },
  {
    "text": "So what's the expected value? The definition of expected- uh, value is like this.",
    "start": "6462600",
    "end": "6469555"
  },
  {
    "text": "So let g be a function from R to R. Which means- let's assume g- you know,",
    "start": "6469555",
    "end": "6481010"
  },
  {
    "text": "this is some function g, this is R and this R. And",
    "start": "6481620",
    "end": "6489160"
  },
  {
    "text": "the expectation of g of x.",
    "start": "6489160",
    "end": "6497845"
  },
  {
    "text": "Now, if the input that we feed to g is a random variable, which means the actual event that can get fed is random,",
    "start": "6497845",
    "end": "6508375"
  },
  {
    "text": "that get fed as input as random, then the expectation of g of x is defined like this.",
    "start": "6508375",
    "end": "6514825"
  },
  {
    "text": "For- you know um, the sum of every possible um, x that can be fed as input.",
    "start": "6514825",
    "end": "6521530"
  },
  {
    "text": "Evaluate g at, for that input, multiplied by the probability that- that x can get fed according to the random variable.",
    "start": "6521530",
    "end": "6530410"
  },
  {
    "text": "And if x is continuous, then it is the integral. Now, one way to think of it is like this.",
    "start": "6530410",
    "end": "6538795"
  },
  {
    "text": "If this defines, if- if- if your x that you're feeding to g is random,",
    "start": "6538795",
    "end": "6547750"
  },
  {
    "text": "then your g of x- your X has some kind of a probability density, right?",
    "start": "6547750",
    "end": "6558595"
  },
  {
    "text": "So this is- this is x. This is the pdf of X,",
    "start": "6558595",
    "end": "6564850"
  },
  {
    "text": "the probability density function, and this is g of x, right? Now, what the expectation is telling you is if you were to sample your",
    "start": "6564850",
    "end": "6574300"
  },
  {
    "text": "x's according to this density- if you randomly sample x according to this density.",
    "start": "6574300",
    "end": "6581275"
  },
  {
    "text": "And then for each of those samples, evaluate g of x.",
    "start": "6581275",
    "end": "6586640"
  },
  {
    "text": "And then kind of average them.",
    "start": "6587460",
    "end": "6592280"
  },
  {
    "text": "You know, and take the average over here. It's going to be some- some value.",
    "start": "6592740",
    "end": "6598240"
  },
  {
    "text": "And this is expectation of g of x. Which means if your- if your sampling x's according to the random variable X,",
    "start": "6598240",
    "end": "6608005"
  },
  {
    "text": "evaluating g on those sample inputs, what is the average value of g you would get if you",
    "start": "6608005",
    "end": "6615099"
  },
  {
    "text": "were to repeat this experiment you know, indefinitely long, right? What's the expected value of the output of g if your inputs are sampled according to x?",
    "start": "6615100",
    "end": "6624790"
  },
  {
    "text": "So that's- that's ah the expectation of g of x, and this procedure of calculating the uh, expectation.",
    "start": "6624790",
    "end": "6631980"
  },
  {
    "text": "So the analytical uh, definition is here, that's the integral of g of x, f of x dx.",
    "start": "6631980",
    "end": "6638175"
  },
  {
    "text": "The, um the other interpretation of this is to um,",
    "start": "6638175",
    "end": "6645324"
  },
  {
    "text": "take the average 1 over n, g of some of equals 1 to n,",
    "start": "6645325",
    "end": "6653320"
  },
  {
    "text": "g of x i, where x i are random samples that you- that you- the random samples of x,",
    "start": "6653320",
    "end": "6662744"
  },
  {
    "text": "that is the input drawn according to the density, right? Evaluated at g of x.",
    "start": "6662745",
    "end": "6668215"
  },
  {
    "text": "And take the average, right? And what we basically know is that the limit of this as n tends to infinity,",
    "start": "6668215",
    "end": "6676073"
  },
  {
    "text": "which means that if- if you perform this procedure with larger and larger number of samples,",
    "start": "6676074",
    "end": "6682224"
  },
  {
    "text": "the limit of this, thanks to the integral of g of x and p of x dx,",
    "start": "6682225",
    "end": "6693910"
  },
  {
    "text": "where p of x is the probability density, g of x is the function that we're trying to calculate expectation over.",
    "start": "6693910",
    "end": "6699670"
  },
  {
    "text": "And this- this statement is also called the? Anybody? This is called the law of large numbers.",
    "start": "6699670",
    "end": "6709179"
  },
  {
    "text": "Large numbers, right? Super important. And this estimate of the expectation?",
    "start": "6712470",
    "end": "6721015"
  },
  {
    "text": "You know, so this- this is an approximate estimate of the expectation which the approximation gets better and better as you increase",
    "start": "6721015",
    "end": "6728980"
  },
  {
    "text": "the number of n. This estimate is also called the? Anybody? It's called the Monte Carlo",
    "start": "6728980",
    "end": "6734650"
  },
  {
    "text": "estimate, right?",
    "start": "6734650",
    "end": "6743890"
  },
  {
    "text": "I'm sorry. [BACKGROUND] Monte Carlo estimate. There's the Monte Carlo estimates.",
    "start": "6743890",
    "end": "6749290"
  },
  {
    "text": "And as you- as you increase n to infinity, your Monte Carlo estimate becomes the true expectation.",
    "start": "6749290",
    "end": "6755080"
  },
  {
    "text": "And that's- that's basically as a consequence of the law of large numbers. And we're going to be seeing the Monte Carlo estimates for various,",
    "start": "6755080",
    "end": "6763405"
  },
  {
    "text": "you know uh, uh, for various purposes in- in machine learning and- and in this course as well.",
    "start": "6763405",
    "end": "6768980"
  }
]