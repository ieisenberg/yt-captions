[
  {
    "start": "0",
    "end": "170000"
  },
  {
    "text": "We're now turning to least squares,",
    "start": "4340",
    "end": "6900"
  },
  {
    "text": "which is really the third part of the book,",
    "start": "6900",
    "end": "9030"
  },
  {
    "text": "um, and an important part of that course.",
    "start": "9030",
    "end": "12285"
  },
  {
    "text": "So we'll jump right in,",
    "start": "12285",
    "end": "14019"
  },
  {
    "text": "and I'll tell you what the least squares problem is,",
    "start": "14020",
    "end": "16139"
  },
  {
    "text": "and we'll get to how to solve it.",
    "start": "16140",
    "end": "17730"
  },
  {
    "text": "Actually, it's the math is not very complicated.",
    "start": "17730",
    "end": "20160"
  },
  {
    "text": "Uh, what's exciting about least squares are all the-",
    "start": "20160",
    "end": "22215"
  },
  {
    "text": "all the things you can do with it, the applications.",
    "start": "22215",
    "end": "25665"
  },
  {
    "text": "Okay. So here's the least squares problem.",
    "start": "25665",
    "end": "28890"
  },
  {
    "text": "We start with m by n matrix A, which is tall.",
    "start": "28890",
    "end": "32369"
  },
  {
    "text": "So that means that m is bigger than n. And that says as a set of equations,",
    "start": "32370",
    "end": "36510"
  },
  {
    "text": "Ax equals b is overdetermined.",
    "start": "36510",
    "end": "38730"
  },
  {
    "text": "You have more equations than you have unknowns.",
    "start": "38730",
    "end": "41445"
  },
  {
    "text": "And that implies that for most,",
    "start": "41445",
    "end": "44850"
  },
  {
    "text": "uh, choices of b,",
    "start": "44850",
    "end": "46725"
  },
  {
    "text": "the right hand side, there is no solution.",
    "start": "46725",
    "end": "49140"
  },
  {
    "text": "There's no x that satisfies Ax equals b.",
    "start": "49140",
    "end": "52410"
  },
  {
    "text": "So what we're going to do is we're going to talk about the residual r,",
    "start": "52410",
    "end": "57250"
  },
  {
    "text": "and that's Ax minus b.",
    "start": "57250",
    "end": "59420"
  },
  {
    "text": "It means it's the residual in the equation.",
    "start": "59420",
    "end": "61535"
  },
  {
    "text": "It means that if the amount by which the equation doesn't satisfy,",
    "start": "61535",
    "end": "65135"
  },
  {
    "text": "so if r equals 0, the residual,",
    "start": "65135",
    "end": "67595"
  },
  {
    "text": "that means x is actually a solution of Ax equals b.",
    "start": "67595",
    "end": "71200"
  },
  {
    "text": "Now, in this case,",
    "start": "71200",
    "end": "73170"
  },
  {
    "text": "when A is tall,",
    "start": "73170",
    "end": "74865"
  },
  {
    "text": "for most b, there isn't an x that makes the residual 0.",
    "start": "74865",
    "end": "78299"
  },
  {
    "text": "So what we're going to do is this.",
    "start": "78300",
    "end": "80510"
  },
  {
    "text": "We're going to choose x that makes the residual as small as possible,",
    "start": "80510",
    "end": "86880"
  },
  {
    "text": "and we'll use that,",
    "start": "86880",
    "end": "88439"
  },
  {
    "text": "will- will- it will be smallest possible in the norm or the norm squared.",
    "start": "88440",
    "end": "92500"
  },
  {
    "text": "Because if you minimize the norm,",
    "start": "92500",
    "end": "94010"
  },
  {
    "text": "it's the same as minimizing the norm squared.",
    "start": "94010",
    "end": "95840"
  },
  {
    "text": "So that's what we're going to do.",
    "start": "95840",
    "end": "97070"
  },
  {
    "text": "We're going to minimize the sum of squares of the residual.",
    "start": "97070",
    "end": "102405"
  },
  {
    "text": "That's how we're going to choose x. Um,",
    "start": "102405",
    "end": "104820"
  },
  {
    "text": "now, this is called the objective function.",
    "start": "104820",
    "end": "106920"
  },
  {
    "text": "It's a scaler, it's a nonnegative scalar.",
    "start": "106920",
    "end": "109725"
  },
  {
    "text": "Um, and it's the objective function,",
    "start": "109725",
    "end": "112549"
  },
  {
    "text": "and it's one that we want when we're going to choose x,",
    "start": "112550",
    "end": "114545"
  },
  {
    "text": "we want that to be as small as possible.",
    "start": "114545",
    "end": "116945"
  },
  {
    "text": "That- that's the, uh, that- that's what we want.",
    "start": "116945",
    "end": "119825"
  },
  {
    "text": "Um, and so we'll say that x hat is",
    "start": "119825",
    "end": "122524"
  },
  {
    "text": "a solution of the least squares problem if the following holds;",
    "start": "122525",
    "end": "126545"
  },
  {
    "text": "if the objective value for x hat is less than",
    "start": "126545",
    "end": "129849"
  },
  {
    "text": "or equal to the objective value for any other x.",
    "start": "129850",
    "end": "133305"
  },
  {
    "text": "That's it. So roughly speaking,",
    "start": "133305",
    "end": "134939"
  },
  {
    "text": "it says it's the best- it's the best- if it's- it's the",
    "start": "134940",
    "end": "137830"
  },
  {
    "text": "best you can do in terms of making the residual small.",
    "start": "137830",
    "end": "141335"
  },
  {
    "text": "Okay. So that's the - that's the condition.",
    "start": "141335",
    "end": "144305"
  },
  {
    "text": "Um, x hat minimizes the objective.",
    "start": "144305",
    "end": "147829"
  },
  {
    "text": "That's the right way, or you would say x hat minimizes the norm or norm squared of the residual.",
    "start": "147830",
    "end": "153520"
  },
  {
    "text": "Okay. Um, now this is- it's got other names too,",
    "start": "153520",
    "end": "158115"
  },
  {
    "text": "this is just least squares.",
    "start": "158115",
    "end": "159570"
  },
  {
    "text": "And it's got many, many other names depending on the application.",
    "start": "159570",
    "end": "162310"
  },
  {
    "text": "For example, in statistics,",
    "start": "162310",
    "end": "164020"
  },
  {
    "text": "people would call it a regression.",
    "start": "164020",
    "end": "166210"
  },
  {
    "text": "That's where you're fitting some data, and we'll see that later.",
    "start": "166210",
    "end": "170120"
  },
  {
    "start": "170000",
    "end": "280000"
  },
  {
    "text": "Okay. Now, x hat, um,",
    "start": "170240",
    "end": "174380"
  },
  {
    "text": "we will shortly get to how do you compute x hat or what is x hat?",
    "start": "174380",
    "end": "178045"
  },
  {
    "text": "There will be a formula for it,",
    "start": "178045",
    "end": "179110"
  },
  {
    "text": "and it will not be unsurprising, the formula.",
    "start": "179110",
    "end": "181180"
  },
  {
    "text": "Uh, but for now,",
    "start": "181180",
    "end": "182469"
  },
  {
    "text": "we'll say that if you choose x that minimizes the- the norm squared of the residual.",
    "start": "182470",
    "end": "188185"
  },
  {
    "text": "But we're- we're going to call that a least squares",
    "start": "188185",
    "end": "190405"
  },
  {
    "text": "approximate solution of Ax equals b. Um,",
    "start": "190405",
    "end": "193155"
  },
  {
    "text": "now, I- I have to tell you,",
    "start": "193155",
    "end": "195160"
  },
  {
    "text": "and that's because in general, it is false.",
    "start": "195160",
    "end": "198280"
  },
  {
    "text": "This is, in general,",
    "start": "198280",
    "end": "199660"
  },
  {
    "text": "we don't have this,",
    "start": "199660",
    "end": "201730"
  },
  {
    "text": "right, that generally does not occur.",
    "start": "201730",
    "end": "204175"
  },
  {
    "text": "Um, okay.",
    "start": "204175",
    "end": "205970"
  },
  {
    "text": "So some people in older terminology for a least squares approximate solution of",
    "start": "205970",
    "end": "211780"
  },
  {
    "text": "this is people say that x hat is sometimes",
    "start": "211780",
    "end": "214700"
  },
  {
    "text": "called a solution of Ax equals b in the least square sense.",
    "start": "214700",
    "end": "218455"
  },
  {
    "text": "Um, don't say that. I'll tell you why.",
    "start": "218455",
    "end": "222240"
  },
  {
    "text": "Because it's not a solution of Ax equals b in any sense,",
    "start": "222240",
    "end": "225870"
  },
  {
    "text": "it- a solution of Ax equals b is a vector that satisfies Ax equals b.",
    "start": "225870",
    "end": "229640"
  },
  {
    "text": "And x hat generally does not satisfy that,",
    "start": "229640",
    "end": "231870"
  },
  {
    "text": "so don't say that.",
    "start": "231870",
    "end": "233120"
  },
  {
    "text": "Um, it's very confusing.",
    "start": "233120",
    "end": "236185"
  },
  {
    "text": "Uh, please don't say it, and actually,",
    "start": "236185",
    "end": "238330"
  },
  {
    "text": "don't hang out with people who say stuff like that because it's just,",
    "start": "238330",
    "end": "241610"
  },
  {
    "text": "you know, it- it's not cool.",
    "start": "241610",
    "end": "243630"
  },
  {
    "text": "So- but I just wanted to warn you.",
    "start": "243630",
    "end": "245655"
  },
  {
    "text": "Some people say this,",
    "start": "245655",
    "end": "246950"
  },
  {
    "text": "it's that x hat is a solution of x in the least square sense,",
    "start": "246950",
    "end": "249860"
  },
  {
    "text": "which doesn't mean anything.",
    "start": "249860",
    "end": "251045"
  },
  {
    "text": "Okay. Fine. Now, as I've said a couple of times, uh,",
    "start": "251045",
    "end": "254550"
  },
  {
    "text": "x hat, the solution of the least squares problem,",
    "start": "254550",
    "end": "257090"
  },
  {
    "text": "generally doesn't satisfy Ax hat equals b.",
    "start": "257090",
    "end": "259264"
  },
  {
    "text": "It merely minimizes the norm of the residual.",
    "start": "259265",
    "end": "261709"
  },
  {
    "text": "It does not make it 0.",
    "start": "261710",
    "end": "263180"
  },
  {
    "text": "Um, now, if you have an x hat that does satisfy Ax equals b,",
    "start": "263180",
    "end": "268145"
  },
  {
    "text": "then I guarantee you, it's the solution because the norm squared of 0 is 0,",
    "start": "268145",
    "end": "272328"
  },
  {
    "text": "and norm squareds don't come any smaller than 0.",
    "start": "272329",
    "end": "274460"
  },
  {
    "text": "So it would- it is actually in that case, the minimum.",
    "start": "274460",
    "end": "277000"
  },
  {
    "text": "So it generalizes the idea of an actual solution.",
    "start": "277000",
    "end": "280800"
  },
  {
    "start": "280000",
    "end": "362000"
  },
  {
    "text": "I want to give a couple of, uh,",
    "start": "280850",
    "end": "282900"
  },
  {
    "text": "interpretations of least squares, uh,",
    "start": "282900",
    "end": "285180"
  },
  {
    "text": "before we get to how do you actually compute it or- or,",
    "start": "285180",
    "end": "288020"
  },
  {
    "text": "uh, what's the formula for it, which we'll see shortly.",
    "start": "288020",
    "end": "290460"
  },
  {
    "text": "Um, so let's let A1 through AN and be the columns of- of A, the matrix.",
    "start": "290460",
    "end": "295430"
  },
  {
    "text": "And then we're going to write out Ax in terms of its columns.",
    "start": "295430",
    "end": "298655"
  },
  {
    "text": "And so, um Ax is, well,",
    "start": "298655",
    "end": "301385"
  },
  {
    "text": "it's nothing more than a linear combination of the columns of A",
    "start": "301385",
    "end": "305090"
  },
  {
    "text": "that's written here with coefficients that are given by x, the x Is.",
    "start": "305090",
    "end": "309125"
  },
  {
    "text": "Um, so what it says is that the norm squared of Ax minus b, that's the residual.",
    "start": "309125",
    "end": "314480"
  },
  {
    "text": "Um, is this thing,",
    "start": "314480",
    "end": "317355"
  },
  {
    "text": "minus b, this is Ax but written out by columns.",
    "start": "317355",
    "end": "320705"
  },
  {
    "text": "Um, and what it says is",
    "start": "320705",
    "end": "322370"
  },
  {
    "text": "that the loose- least squares problem is trying",
    "start": "322370",
    "end": "324560"
  },
  {
    "text": "to find a linear combination of the columns of",
    "start": "324560",
    "end": "326825"
  },
  {
    "text": "A that is closest to b because this is",
    "start": "326825",
    "end": "330050"
  },
  {
    "text": "the distance between this thing and this thing squared.",
    "start": "330050",
    "end": "333305"
  },
  {
    "text": "So that's a- that's a geometric interpretation.",
    "start": "333305",
    "end": "335270"
  },
  {
    "text": "So it says, among all linear combinations of the columns of A,",
    "start": "335270",
    "end": "338919"
  },
  {
    "text": "find me one that is closest to b.",
    "start": "338920",
    "end": "341925"
  },
  {
    "text": "Okay. Um, now, if x hat is a solution to the least squares problem,",
    "start": "341925",
    "end": "347944"
  },
  {
    "text": "then the m vector Ax hat,",
    "start": "347945",
    "end": "350510"
  },
  {
    "text": "that's this thing, that would be this thing here.",
    "start": "350510",
    "end": "353240"
  },
  {
    "text": "That is the vector which is closest to",
    "start": "353240",
    "end": "355940"
  },
  {
    "text": "b among all linear combinations of the columns of A.",
    "start": "355940",
    "end": "359525"
  },
  {
    "text": "Okay. I mean, essentially by definition.",
    "start": "359525",
    "end": "362785"
  },
  {
    "start": "362000",
    "end": "461000"
  },
  {
    "text": "Now we have a row interpretation as well.",
    "start": "362785",
    "end": "365180"
  },
  {
    "text": "[NOISE] Let's let A1 tilde transpose up to AM tilde transpose b the rows of A.",
    "start": "365180",
    "end": "370970"
  },
  {
    "text": "A is here, it's m by n. Okay.",
    "start": "370970",
    "end": "375090"
  },
  {
    "text": "So we have, uh, these are the rows,",
    "start": "375090",
    "end": "377790"
  },
  {
    "text": "um, and the residuals,",
    "start": "377790",
    "end": "379755"
  },
  {
    "text": "now we can write it in terms of the rows.",
    "start": "379755",
    "end": "381825"
  },
  {
    "text": "Um, AI tilde transpose x is,",
    "start": "381825",
    "end": "386190"
  },
  {
    "text": "uh, is going to be Ax,",
    "start": "386190",
    "end": "388240"
  },
  {
    "text": "the ith component and then minus BI.",
    "start": "388240",
    "end": "390440"
  },
  {
    "text": "So this is- this is the ith component of the residual.",
    "start": "390440",
    "end": "393349"
  },
  {
    "text": "It's a tilde transpose x minus BI.",
    "start": "393349",
    "end": "396930"
  },
  {
    "text": "Um, now the least squares objective we can write,",
    "start": "396930",
    "end": "398960"
  },
  {
    "text": "well, it's the sum of the squares of the Rs.",
    "start": "398960",
    "end": "401240"
  },
  {
    "text": "So that means we just write it out this way, like that.",
    "start": "401240",
    "end": "404815"
  },
  {
    "text": "And what that says is it's the- it- it's the sum of the squares of- of- of the residuals.",
    "start": "404815",
    "end": "412455"
  },
  {
    "text": "Um, a given, and the residuals are each",
    "start": "412455",
    "end": "414740"
  },
  {
    "text": "given by the- these things involving the rows of A.",
    "start": "414740",
    "end": "417190"
  },
  {
    "text": "Okay. So when you choose",
    "start": "417190",
    "end": "420110"
  },
  {
    "text": "x to minimize the sum of the squares of residuals as you do with least squares,",
    "start": "420110",
    "end": "424629"
  },
  {
    "text": "what you're really doing is you're taking these sort of M numbers.",
    "start": "424630",
    "end": "428270"
  },
  {
    "text": "You're adding them up and making the sum as small as possible.",
    "start": "428270",
    "end": "431014"
  },
  {
    "text": "And that's sort of a way to make all of them small.",
    "start": "431015",
    "end": "433475"
  },
  {
    "text": "I mean, they won't- they won't all be 0 or you- maybe even none of them will be 0.",
    "start": "433475",
    "end": "436805"
  },
  {
    "text": "But that's kind of the idea here.",
    "start": "436805",
    "end": "438430"
  },
  {
    "text": "Okay. So, um, [NOISE] and we can say a couple things.",
    "start": "438430",
    "end": "441360"
  },
  {
    "text": "Actually solving the equation,",
    "start": "441360",
    "end": "443009"
  },
  {
    "text": "Ax equals b is when you make every residual 0.",
    "start": "443010",
    "end": "446335"
  },
  {
    "text": "Um, as I said, that's generally not possible when",
    "start": "446335",
    "end": "449569"
  },
  {
    "text": "there- the- when the system of equations is overdetermined.",
    "start": "449570",
    "end": "452960"
  },
  {
    "text": "So instead, what you're- we're going to do is as a compromise,",
    "start": "452960",
    "end": "456350"
  },
  {
    "text": "try to make the sum of the squares small, right?",
    "start": "456350",
    "end": "459515"
  },
  {
    "text": "Okay. That's the row interpretation.",
    "start": "459515",
    "end": "461524"
  },
  {
    "start": "461000",
    "end": "716000"
  },
  {
    "text": "Um, [NOISE] let's look at",
    "start": "461524",
    "end": "463580"
  },
  {
    "text": "a little example just to sort of get a picture for what it looks like.",
    "start": "463580",
    "end": "467095"
  },
  {
    "text": "Here's a matrix A. I mean, this is silly,",
    "start": "467095",
    "end": "469940"
  },
  {
    "text": "and it's so simple that you wouldn't do this,",
    "start": "469940",
    "end": "471560"
  },
  {
    "text": "but we'll- we'll- well, we'll get to that later.",
    "start": "471560",
    "end": "473705"
  },
  {
    "text": "So here's a matrix.",
    "start": "473705",
    "end": "475235"
  },
  {
    "text": "A is three by two, and here's b.",
    "start": "475235",
    "end": "478189"
  },
  {
    "text": "Uh, and we consider the set of equations, Ax equals b.",
    "start": "478190",
    "end": "482705"
  },
  {
    "text": "Well, there is no solution of that.",
    "start": "482705",
    "end": "484340"
  },
  {
    "text": "And we can quickly figure that out because if I told you that a times x_1,",
    "start": "484340",
    "end": "489785"
  },
  {
    "text": "x_2 is equal to b like that,",
    "start": "489785",
    "end": "493590"
  },
  {
    "text": "then we can work out some of what this implies, right.",
    "start": "493590",
    "end": "496790"
  },
  {
    "text": "From the first- the first entry of Ax is going to be 2_x1,",
    "start": "496790",
    "end": "501325"
  },
  {
    "text": "and that must equal 1.",
    "start": "501325",
    "end": "503370"
  },
  {
    "text": "So cool, x_1 is a half,",
    "start": "503370",
    "end": "505335"
  },
  {
    "text": "and the last entry says 2 x_2 is equal to minus 1.",
    "start": "505335",
    "end": "511335"
  },
  {
    "text": "And so that says that x_1 equals a half.",
    "start": "511335",
    "end": "514584"
  },
  {
    "text": "We just conclude that x_2 equals minus a half.",
    "start": "514585",
    "end": "517784"
  },
  {
    "text": "U, oh, so we know x, great.",
    "start": "517785",
    "end": "519719"
  },
  {
    "text": "This is if it were a solution,",
    "start": "519720",
    "end": "521240"
  },
  {
    "text": "there's that middle equation.",
    "start": "521240",
    "end": "523024"
  },
  {
    "text": "And unfortunately, in that case,",
    "start": "523025",
    "end": "525440"
  },
  {
    "text": "when we find out what it is,",
    "start": "525440",
    "end": "527165"
  },
  {
    "text": "it is going to be, uh,",
    "start": "527165",
    "end": "528839"
  },
  {
    "text": "the second entry is not going to be 0.",
    "start": "528840",
    "end": "531140"
  },
  {
    "text": "Unfortunately, it is going to be minus 1.",
    "start": "531140",
    "end": "533725"
  },
  {
    "text": "Okay. So- so with this choice, uh,",
    "start": "533725",
    "end": "537255"
  },
  {
    "text": "Ax- Ax going, uh,",
    "start": "537255",
    "end": "540000"
  },
  {
    "text": "minus b is going to be something like 0,",
    "start": "540000",
    "end": "542610"
  },
  {
    "text": "uh, minus 1 and 0.",
    "start": "542610",
    "end": "545220"
  },
  {
    "text": "So the good news is we solve the first and the third equation.",
    "start": "545220",
    "end": "549978"
  },
  {
    "text": "Uh, bad news, we missed on the middle equation.",
    "start": "549979",
    "end": "552515"
  },
  {
    "text": "Okay. But that's exactly kind of what least squares is supposed to help us with.",
    "start": "552515",
    "end": "555800"
  },
  {
    "text": "Okay. So, um, all right.",
    "start": "555800",
    "end": "560519"
  },
  {
    "text": "So least- the least squares problem is to choose x to minimize,",
    "start": "560520",
    "end": "563810"
  },
  {
    "text": "and here's norm Ax minus b squared.",
    "start": "563810",
    "end": "565570"
  },
  {
    "text": "And because it's just two entries, x_1 and x_2,",
    "start": "565570",
    "end": "567550"
  },
  {
    "text": "I've just written out completely here with no vector.",
    "start": "567550",
    "end": "569709"
  },
  {
    "text": "Uh, no- no vector matrix notation at all.",
    "start": "569710",
    "end": "572110"
  },
  {
    "text": "It's just- it's just this,",
    "start": "572110",
    "end": "573329"
  },
  {
    "text": "it's 2x minus 1 squared plus this thing, plus that thing.",
    "start": "573330",
    "end": "576430"
  },
  {
    "text": "And these are the residuals as for now.",
    "start": "576430",
    "end": "577980"
  },
  {
    "text": "[NOISE] Okay.",
    "start": "577980",
    "end": "579880"
  },
  {
    "text": "Now this function,",
    "start": "579880",
    "end": "581530"
  },
  {
    "text": "um, is plotted here.",
    "start": "581530",
    "end": "583890"
  },
  {
    "text": "Um, in the sense,",
    "start": "583890",
    "end": "585520"
  },
  {
    "text": "these are level curves.",
    "start": "585520",
    "end": "586585"
  },
  {
    "text": "So it turns out,",
    "start": "586585",
    "end": "588100"
  },
  {
    "text": "the minimum occurs when x hat is a third minus a third.",
    "start": "588100",
    "end": "592029"
  },
  {
    "text": "Now, you could get that using calculus.",
    "start": "592030",
    "end": "594010"
  },
  {
    "text": "If you like, just take this- this expression,",
    "start": "594010",
    "end": "596544"
  },
  {
    "text": "take the partial derivative with respect to x_1, set it equal to 0.",
    "start": "596544",
    "end": "599285"
  },
  {
    "text": "Take the partial derivative with respect to x_2, set it equal to 0.",
    "start": "599285",
    "end": "601860"
  },
  {
    "text": "And you'll find that x_1 and x_2 are third and minus a third.",
    "start": "601860",
    "end": "604935"
  },
  {
    "text": "Um, so that's what you get.",
    "start": "604935",
    "end": "608085"
  },
  {
    "text": "Um, and up- up here,",
    "start": "608085",
    "end": "609350"
  },
  {
    "text": "what I've plotted is the objective, uh,",
    "start": "609350",
    "end": "611660"
  },
  {
    "text": "which is f. So this is f of x here, um, like that.",
    "start": "611660",
    "end": "617355"
  },
  {
    "text": "And it has its absolute minimum value here at one third minus one third.",
    "start": "617355",
    "end": "622339"
  },
  {
    "text": "Um, then what these lines show is where it has other value.",
    "start": "622340",
    "end": "626780"
  },
  {
    "text": "So this is when the value is 1 is- is 1 bigger than the value at x hat and so on.",
    "start": "626780",
    "end": "633490"
  },
  {
    "text": "And so these are ellipsoids.",
    "start": "633490",
    "end": "635075"
  },
  {
    "text": "That's not an- that's- I mean,",
    "start": "635075",
    "end": "636460"
  },
  {
    "text": "that doesn't- these are ellipses.",
    "start": "636460",
    "end": "637540"
  },
  {
    "text": "It doesn't- that's not a surprise there.",
    "start": "637540",
    "end": "640024"
  },
  {
    "text": "Yeah. So, um, but the idea,",
    "start": "640025",
    "end": "642290"
  },
  {
    "text": "and you can see that it's getting steeper and steeper here.",
    "start": "642290",
    "end": "644389"
  },
  {
    "text": "But the small- the- the value where it's smallest is right there.",
    "start": "644390",
    "end": "647150"
  },
  {
    "text": "This is one third minus one third.",
    "start": "647150",
    "end": "648980"
  },
  {
    "text": "Um, and, uh, that smallest value is in fact",
    "start": "648980",
    "end": "653839"
  },
  {
    "text": "norm Ax minus b squared Ax hat minus b squared is actually two thirds.",
    "start": "653840",
    "end": "658655"
  },
  {
    "text": "Um, by the way, I want to notice something,",
    "start": "658655",
    "end": "660500"
  },
  {
    "text": "that beats our previous solu- right,",
    "start": "660500",
    "end": "662390"
  },
  {
    "text": "our previous idea was we would take x 1 equals a half,",
    "start": "662390",
    "end": "665390"
  },
  {
    "text": "x_2 equals minus a half.",
    "start": "665390",
    "end": "667030"
  },
  {
    "text": "That gives you this residual,",
    "start": "667030",
    "end": "669030"
  },
  {
    "text": "0 minus 1, 0,",
    "start": "669030",
    "end": "670680"
  },
  {
    "text": "and the sum of the squares of that is 1.",
    "start": "670680",
    "end": "672649"
  },
  {
    "text": "Okay. So, and two thirds is indeed, smaller than one.",
    "start": "672650",
    "end": "676220"
  },
  {
    "text": "So I mean, it had to be because this is- this- this",
    "start": "676220",
    "end": "680264"
  },
  {
    "text": "is the value that it- that gives the lowest value of the norm squared of the residual.",
    "start": "680265",
    "end": "685220"
  },
  {
    "text": "Okay. Um, and if we form Ax hat,",
    "start": "685220",
    "end": "688834"
  },
  {
    "text": "we get this vector here,",
    "start": "688835",
    "end": "689945"
  },
  {
    "text": "two thirds minus two thirds minus two thirds.",
    "start": "689945",
    "end": "692405"
  },
  {
    "text": "And that is the linear combination of",
    "start": "692405",
    "end": "695435"
  },
  {
    "text": "these two columns that is closest to the right hand side, b.",
    "start": "695435",
    "end": "700050"
  },
  {
    "text": "Okay. So this is just a picture.",
    "start": "700050",
    "end": "702105"
  },
  {
    "text": "Um, you can visualize it because it's in two dimensions.",
    "start": "702105",
    "end": "705010"
  },
  {
    "text": "Now we're not going to actually be interested in any problem with two variables.",
    "start": "705010",
    "end": "708360"
  },
  {
    "text": "We're going to interested in a problem with 20 or 20,000,",
    "start": "708360",
    "end": "711834"
  },
  {
    "text": "or something like that, a very large number of variables.",
    "start": "711835",
    "end": "714320"
  },
  {
    "text": "We'll see that that's all possible. Okay.",
    "start": "714320",
    "end": "718030"
  },
  {
    "start": "716000",
    "end": "930000"
  },
  {
    "text": "What we're gonna do now is actually,",
    "start": "718030",
    "end": "721199"
  },
  {
    "text": "uh, work out the solution of the least squares problem.",
    "start": "721199",
    "end": "724560"
  },
  {
    "text": "Uh, so- and it's gonna be not too surprising and it's gonna involve,",
    "start": "724560",
    "end": "729870"
  },
  {
    "text": "uh, the matrices and ideas that you have already been, uh, introduced to.",
    "start": "729870",
    "end": "735660"
  },
  {
    "text": "Okay. Now, we're gonna make one assumption,",
    "start": "735660",
    "end": "738029"
  },
  {
    "text": "and that is that A has linearly independent columns, right?",
    "start": "738030",
    "end": "742260"
  },
  {
    "text": "So that's gonna be our- our standing assumption for least squares.",
    "start": "742260",
    "end": "746460"
  },
  {
    "text": "Um, now, that implies that the Gram matrix is invertible.",
    "start": "746460",
    "end": "749325"
  },
  {
    "text": "Uh, that's something we saw, uh,",
    "start": "749325",
    "end": "751215"
  },
  {
    "text": "in the last chapter,",
    "start": "751215",
    "end": "752625"
  },
  {
    "text": "uh, that that's invertible.",
    "start": "752625",
    "end": "754350"
  },
  {
    "text": "Um, and then it turn s out-",
    "start": "754350",
    "end": "756165"
  },
  {
    "text": "I'll just tell you what the solution of the least squares problem is.",
    "start": "756165",
    "end": "758550"
  },
  {
    "text": "It's precisely this, it's actually A transpose A inverse, A transpose B.",
    "start": "758550",
    "end": "763620"
  },
  {
    "text": "You will recognize that as something you've already seen.",
    "start": "763620",
    "end": "766410"
  },
  {
    "text": "It's the pseudoinverse times B. That's it.",
    "start": "766410",
    "end": "770279"
  },
  {
    "text": "So the pseudoinverse is the matrix that maps",
    "start": "770280",
    "end": "773190"
  },
  {
    "text": "the right-hand side to the least squares solution.",
    "start": "773190",
    "end": "776940"
  },
  {
    "text": "That's- that's what- that's what- that's what- that- that's what the,",
    "start": "776940",
    "end": "780030"
  },
  {
    "text": "uh, pseudoinverse or A dagger is.",
    "start": "780030",
    "end": "781950"
  },
  {
    "text": "[NOISE] Now look, this is actually kind of cool.",
    "start": "781950",
    "end": "784905"
  },
  {
    "text": "It looks like A inverse B.",
    "start": "784905",
    "end": "787320"
  },
  {
    "text": "In fact, as you know, when A is square,",
    "start": "787320",
    "end": "789930"
  },
  {
    "text": "a dagger is A inverse.",
    "start": "789930",
    "end": "792570"
  },
  {
    "text": "So it's a very nice extension of the- uh,",
    "start": "792570",
    "end": "796860"
  },
  {
    "text": "of just solving a set of equations.",
    "start": "796860",
    "end": "798779"
  },
  {
    "text": "If you- if you- if you have a square matrix A and it's, uh,",
    "start": "798780",
    "end": "802200"
  },
  {
    "text": "its columns are inverted or its columns are linearly independent,",
    "start": "802200",
    "end": "804750"
  },
  {
    "text": "that means it's not singular, it's invertible.",
    "start": "804750",
    "end": "807330"
  },
  {
    "text": "And that means the solution is just A inverse B.",
    "start": "807330",
    "end": "810630"
  },
  {
    "text": "Now, you know that when A is tall and has, uh,",
    "start": "810630",
    "end": "815505"
  },
  {
    "text": "and has linearly independent columns,",
    "start": "815505",
    "end": "817575"
  },
  {
    "text": "the- the solution of the least squares problem is A dagger B.",
    "start": "817575",
    "end": "823410"
  },
  {
    "text": "So it looks the same. Um, so that's the solution.",
    "start": "823410",
    "end": "827069"
  },
  {
    "text": "I'm about to- I'll show it. We'll- we'll actually show it",
    "start": "827070",
    "end": "828990"
  },
  {
    "text": "two different ways from two different traditions.",
    "start": "828990",
    "end": "831404"
  },
  {
    "text": "Um, but I just want to mention something,",
    "start": "831405",
    "end": "834120"
  },
  {
    "text": "uh- oh, I should mention one thing.",
    "start": "834120",
    "end": "835935"
  },
  {
    "text": "It- it shows you that A dagger is very closely related to- to the inverse.",
    "start": "835935",
    "end": "840165"
  },
  {
    "text": "Um, and now you realize that what A dagger, uh,",
    "start": "840165",
    "end": "842774"
  },
  {
    "text": "B gives you is not a solution,",
    "start": "842775",
    "end": "844980"
  },
  {
    "text": "it's actually least-squares approximate solution, that's the correct way to say it.",
    "start": "844980",
    "end": "848440"
  },
  {
    "text": "Now, let me say a little bit about this in, um,",
    "start": "848440",
    "end": "851465"
  },
  {
    "text": "in, uh, uh, how this works in various computer languages.",
    "start": "851465",
    "end": "855035"
  },
  {
    "text": "In- in some languages and/or packages for linear algebra,",
    "start": "855035",
    "end": "859115"
  },
  {
    "text": "um, there is a symbol- this is not math.",
    "start": "859115",
    "end": "861785"
  },
  {
    "text": "So this is something that just appears in code,",
    "start": "861785",
    "end": "863839"
  },
  {
    "text": "um, and it's the backslash.",
    "start": "863840",
    "end": "865580"
  },
  {
    "text": "So- and the way people write that is you write x equals,",
    "start": "865580",
    "end": "868310"
  },
  {
    "text": "like, A backslash B.",
    "start": "868310",
    "end": "870440"
  },
  {
    "text": "Now, what I have to tell you is this,",
    "start": "870440",
    "end": "873245"
  },
  {
    "text": "everything here, this is math, okay?",
    "start": "873245",
    "end": "875000"
  },
  {
    "text": "This is all in math. This is- this is not math.",
    "start": "875000",
    "end": "877695"
  },
  {
    "text": "You walk over to the math department and say A backslash B,",
    "start": "877695",
    "end": "880020"
  },
  {
    "text": "no one will have a clue what you're talking about.",
    "start": "880020",
    "end": "882240"
  },
  {
    "text": "But in several packages for linear algebra,",
    "start": "882240",
    "end": "884910"
  },
  {
    "text": "backslash is a very cool thing.",
    "start": "884910",
    "end": "887220"
  },
  {
    "text": "What it means is if A is square and invertible,",
    "start": "887220",
    "end": "890399"
  },
  {
    "text": "it means A inverse B.",
    "start": "890400",
    "end": "891915"
  },
  {
    "text": "If A is tall and has linearly independent columns,",
    "start": "891915",
    "end": "896445"
  },
  {
    "text": "it means A dagger B.",
    "start": "896445",
    "end": "897495"
  },
  {
    "text": "So it's basically three characters to get the least squares solution,",
    "start": "897495",
    "end": "902490"
  },
  {
    "text": "that's what that looks like in a lot of computer languages.",
    "start": "902490",
    "end": "905024"
  },
  {
    "text": "But you have to be very careful and always distinguish between,",
    "start": "905025",
    "end": "908730"
  },
  {
    "text": "you know, you have two different dialects.",
    "start": "908730",
    "end": "910320"
  },
  {
    "text": "You have the- you have math,",
    "start": "910320",
    "end": "913290"
  },
  {
    "text": "this is math here.",
    "start": "913290",
    "end": "914685"
  },
  {
    "text": "Um, and then you also have how it's expressed in",
    "start": "914685",
    "end": "917820"
  },
  {
    "text": "various linear algebra packages or languages and things like that.",
    "start": "917820",
    "end": "921615"
  },
  {
    "text": "But backslash is- is common and you- you may see it,",
    "start": "921615",
    "end": "925515"
  },
  {
    "text": "uh, in- in- somewhere.",
    "start": "925515",
    "end": "926880"
  },
  {
    "text": "Depends on what languages you- you work with.",
    "start": "926880",
    "end": "929920"
  },
  {
    "start": "930000",
    "end": "1276000"
  },
  {
    "text": "Okay. So what we're gonna do now is we're gonna derive it, uh, via calculus.",
    "start": "930110",
    "end": "935130"
  },
  {
    "text": "I mean, after all, you did- well,",
    "start": "935130",
    "end": "937050"
  },
  {
    "text": "I didn't say did, you were forced to take calculus.",
    "start": "937050",
    "end": "939435"
  },
  {
    "text": "Um, so let's- uh, you know,",
    "start": "939435",
    "end": "942195"
  },
  {
    "text": "one of the few things you actually learn how to do in calculus is to, uh,",
    "start": "942195",
    "end": "945480"
  },
  {
    "text": "is to- to at least get sufficient conditions for- for a function being minimized.",
    "start": "945480",
    "end": "949920"
  },
  {
    "text": "So what we'll do is we're gonna write out f of x as, uh- actually, by entry.",
    "start": "949920",
    "end": "954660"
  },
  {
    "text": "So here it is. Uh, you can check.",
    "start": "954660",
    "end": "956865"
  },
  {
    "text": "Um, this parentheses here is actually ax minus b sub i.",
    "start": "956865",
    "end": "963839"
  },
  {
    "text": "It is the ith component of a x minus b.",
    "start": "963840",
    "end": "966825"
  },
  {
    "text": "This says, take the sum of the squares,",
    "start": "966825",
    "end": "968490"
  },
  {
    "text": "but there it is written out in all its glory.",
    "start": "968490",
    "end": "971024"
  },
  {
    "text": "Now, the solution x-hat must satisfy the following,",
    "start": "971025",
    "end": "976710"
  },
  {
    "text": "that the partial derivative with respect to each x has to be zero at the solution, okay?",
    "start": "976710",
    "end": "983915"
  },
  {
    "text": "Now, by the way, if this were false,",
    "start": "983915",
    "end": "986255"
  },
  {
    "text": "it would mean that if you moved a little bit, uh,",
    "start": "986255",
    "end": "988770"
  },
  {
    "text": "in- in a- in the direction of, let's say,",
    "start": "988770",
    "end": "991530"
  },
  {
    "text": "the negative gradient, um,",
    "start": "991530",
    "end": "993195"
  },
  {
    "text": "then what would happens is f would get smaller.",
    "start": "993195",
    "end": "994890"
  },
  {
    "text": "And that would show that you are a liar when you said",
    "start": "994890",
    "end": "997080"
  },
  {
    "text": "that x-hat actually minimizes f. Okay.",
    "start": "997080",
    "end": "999600"
  },
  {
    "text": "So I don't have to go into that,",
    "start": "999600",
    "end": "1000740"
  },
  {
    "text": "that's- that's- that's calculus.",
    "start": "1000740",
    "end": "1002720"
  },
  {
    "text": "That's- you're supposed to have seen that or something.",
    "start": "1002720",
    "end": "1005360"
  },
  {
    "text": "Okay. Now, um, we can take partial derivatives of- of this thing with respect to xk,",
    "start": "1005360",
    "end": "1012524"
  },
  {
    "text": "um, and it's kind of a pain I mean,",
    "start": "1012525",
    "end": "1014960"
  },
  {
    "text": "it's not a bad thing to do, uh, but you can do it.",
    "start": "1014960",
    "end": "1017270"
  },
  {
    "text": "Uh, the good news is there's a lot of terms in here that don't",
    "start": "1017270",
    "end": "1019550"
  },
  {
    "text": "contain xk and- and therefore they don't matter because they're constants,",
    "start": "1019550",
    "end": "1022880"
  },
  {
    "text": "and so the partial derivative zero.",
    "start": "1022880",
    "end": "1024735"
  },
  {
    "text": "However, when you work out what the gradient is,",
    "start": "1024735",
    "end": "1027324"
  },
  {
    "text": "I'm not gonna go through this derivation,",
    "start": "1027325",
    "end": "1028914"
  },
  {
    "text": "but it's in the book and you can take a look at it if you'd like,",
    "start": "1028915",
    "end": "1031435"
  },
  {
    "text": "um, is you end up finding that the gradient,",
    "start": "1031435",
    "end": "1034890"
  },
  {
    "text": "the kth entry of the gradient,",
    "start": "1034890",
    "end": "1036685"
  },
  {
    "text": "which is, in fact, this partial derivative,",
    "start": "1036685",
    "end": "1038589"
  },
  {
    "text": "is nothing but two A transpose, ax minus b. Um, in other words,",
    "start": "1038590",
    "end": "1042939"
  },
  {
    "text": "[NOISE] the gradient is two A transpose ax minus b.",
    "start": "1042940",
    "end": "1048304"
  },
  {
    "text": "Actually, that's kind of cool because it- it does generalize,",
    "start": "1048305",
    "end": "1051500"
  },
  {
    "text": "uh, what would be the case if A were a scalar, right?",
    "start": "1051500",
    "end": "1054680"
  },
  {
    "text": "If I walked up to you on the street and said,",
    "start": "1054680",
    "end": "1057560"
  },
  {
    "text": "\"Please minimize this,\" where everything involved is a scalar,",
    "start": "1057560",
    "end": "1062030"
  },
  {
    "text": "and a- a is a scalar,",
    "start": "1062030",
    "end": "1064850"
  },
  {
    "text": "x is a scalar, b is a scalar,",
    "start": "1064850",
    "end": "1066245"
  },
  {
    "text": "you would say, \"No problem,\" and you'd take the derivative.",
    "start": "1066245",
    "end": "1068510"
  },
  {
    "text": "And the derivative of this would be 2 times ax minus b times a,",
    "start": "1068510",
    "end": "1073055"
  },
  {
    "text": "and you would say that that's equal to 0, okay?",
    "start": "1073055",
    "end": "1075800"
  },
  {
    "text": "And, you know, you would, uh,",
    "start": "1075800",
    "end": "1077630"
  },
  {
    "text": "you would derive the solution, which is silly.",
    "start": "1077630",
    "end": "1079940"
  },
  {
    "text": "Uh, the solution [LAUGHTER] is gonna be x equals, in this case,",
    "start": "1079940",
    "end": "1083690"
  },
  {
    "text": "b over a, and you actually just solve the problem.",
    "start": "1083690",
    "end": "1087695"
  },
  {
    "text": "Exactly. So- okay.",
    "start": "1087695",
    "end": "1089690"
  },
  {
    "text": "All I want to point out is that this- this expression for a scalar, uh,",
    "start": "1089690",
    "end": "1094549"
  },
  {
    "text": "turns out the correct- the- the expression for a matrix is that,",
    "start": "1094550",
    "end": "1097805"
  },
  {
    "text": "and they look very similar,",
    "start": "1097805",
    "end": "1099200"
  },
  {
    "text": "but there's a big difference, uh, here.",
    "start": "1099200",
    "end": "1100985"
  },
  {
    "text": "The difference is, you know, first of all,",
    "start": "1100985",
    "end": "1102470"
  },
  {
    "text": "the a appears on the left and it's transposed, right?",
    "start": "1102470",
    "end": "1105200"
  },
  {
    "text": "So, uh, so here's actually my advice for how to handle things like this.",
    "start": "1105200",
    "end": "1109429"
  },
  {
    "text": "When you look at a matrix equation,",
    "start": "1109430",
    "end": "1111155"
  },
  {
    "text": "when there is an obvious analog of a scalar equation,",
    "start": "1111155",
    "end": "1114950"
  },
  {
    "text": "um, derive the scalar equation,",
    "start": "1114950",
    "end": "1117034"
  },
  {
    "text": "put it on some paper or something like that because you're",
    "start": "1117035",
    "end": "1118970"
  },
  {
    "text": "going to destroy the evidence later, okay?",
    "start": "1118970",
    "end": "1121039"
  },
  {
    "text": "So you- you- you- you work it out like this,",
    "start": "1121040",
    "end": "1123170"
  },
  {
    "text": "you work out the- the derivative,",
    "start": "1123170",
    "end": "1124985"
  },
  {
    "text": "the gradient is- that- that's the derivative, right?",
    "start": "1124985",
    "end": "1127580"
  },
  {
    "text": "Then what- but this just get- this just tells you, look,",
    "start": "1127580",
    "end": "1130370"
  },
  {
    "text": "it should look something like that,",
    "start": "1130370",
    "end": "1131720"
  },
  {
    "text": "but it has to be matrix appropriate, right?",
    "start": "1131720",
    "end": "1133835"
  },
  {
    "text": "So for example, here's what you should not do,",
    "start": "1133835",
    "end": "1136580"
  },
  {
    "text": "is say, oh, that's the derivative.",
    "start": "1136580",
    "end": "1138679"
  },
  {
    "text": "So for matrices it should be this, right? Times A.",
    "start": "1138680",
    "end": "1142955"
  },
  {
    "text": "Well, because, well, I mean,",
    "start": "1142955",
    "end": "1144260"
  },
  {
    "text": "that's the obvious analog of this.",
    "start": "1144260",
    "end": "1146300"
  },
  {
    "text": "Now, a couple of problems with this.",
    "start": "1146300",
    "end": "1148010"
  },
  {
    "text": "Number 1 is it- it doesn't even make sense syntactically, right?",
    "start": "1148010",
    "end": "1151385"
  },
  {
    "text": "That you cannot multiply ax minus b,",
    "start": "1151385",
    "end": "1153950"
  },
  {
    "text": "which is a vector on the right by this matrix A,",
    "start": "1153950",
    "end": "1156860"
  },
  {
    "text": "it doesn't make any- it just doesn't work.",
    "start": "1156860",
    "end": "1159080"
  },
  {
    "text": "So this is just a very wrong.",
    "start": "1159080",
    "end": "1160580"
  },
  {
    "text": "But what you do do is you keep this here,",
    "start": "1160580",
    "end": "1162200"
  },
  {
    "text": "and then when you- when you derive the correct matrix equation,",
    "start": "1162200",
    "end": "1166490"
  },
  {
    "text": "you'll look at it and you go, \"Uh-huh.",
    "start": "1166490",
    "end": "1167795"
  },
  {
    "text": "Okay.\" This scalar equation generalizes to that matrix vector equation this way.",
    "start": "1167795",
    "end": "1173480"
  },
  {
    "text": "And that's- this is just a very good way to do this.",
    "start": "1173480",
    "end": "1175625"
  },
  {
    "text": "Okay. So back to the gradient equal to zero.",
    "start": "1175625",
    "end": "1178850"
  },
  {
    "text": "When I expand this,",
    "start": "1178850",
    "end": "1180080"
  },
  {
    "text": "what I get is I get to 2A transpose",
    "start": "1180080",
    "end": "1183200"
  },
  {
    "text": "A times x-hat minus 2A transpose B equals 0.",
    "start": "1183200",
    "end": "1191149"
  },
  {
    "text": "When I move things around,",
    "start": "1191150",
    "end": "1193310"
  },
  {
    "text": "I move this to the other side,",
    "start": "1193310",
    "end": "1194480"
  },
  {
    "text": "divide by 2, I get these things.",
    "start": "1194480",
    "end": "1195919"
  },
  {
    "text": "And these equations are very famous and they have a name,",
    "start": "1195920",
    "end": "1198575"
  },
  {
    "text": "they're called the normal equations for this least squares problems.",
    "start": "1198575",
    "end": "1202880"
  },
  {
    "text": "So that's the normal equations.",
    "start": "1202880",
    "end": "1204680"
  },
  {
    "text": "Um, and you can see that the coefficient matrix is actually the Gram matrix,",
    "start": "1204680",
    "end": "1208940"
  },
  {
    "text": "uh, which we know about already. Okay. Um, so that's it.",
    "start": "1208940",
    "end": "1212480"
  },
  {
    "text": "And, of course, if a has linearly independent columns,",
    "start": "1212480",
    "end": "1217564"
  },
  {
    "text": "then that tells you that A transpose A,",
    "start": "1217564",
    "end": "1220279"
  },
  {
    "text": "the Gram matrix is invertible.",
    "start": "1220280",
    "end": "1222170"
  },
  {
    "text": "And so I get my solution like that,",
    "start": "1222170",
    "end": "1224060"
  },
  {
    "text": "which is just what we said it was gonna be.",
    "start": "1224060",
    "end": "1225785"
  },
  {
    "text": "It is- it is nothing more than A dagger B.",
    "start": "1225785",
    "end": "1230270"
  },
  {
    "text": "It's the pseudoinverse times B.",
    "start": "1230270",
    "end": "1232100"
  },
  {
    "text": "Okay. So that's a derivation from calculus.",
    "start": "1232100",
    "end": "1234500"
  },
  {
    "text": "[NOISE] I think there's a couple of weird things about the calculus thing.",
    "start": "1234500",
    "end": "1238220"
  },
  {
    "text": "Number 1, if you go back and read the fine print in your calculus book,",
    "start": "1238220",
    "end": "1241760"
  },
  {
    "text": "it's gonna tell you this,",
    "start": "1241760",
    "end": "1243020"
  },
  {
    "text": "it's gonna tell you that the minimizer has the gradient equal to zero.",
    "start": "1243020",
    "end": "1248190"
  },
  {
    "text": "Fine. But if you read the fine print, all  the legalese,",
    "start": "1248360",
    "end": "1252990"
  },
  {
    "text": "what you're going to find is that the gradient can be 0 and you might not be a",
    "start": "1252990",
    "end": "1257010"
  },
  {
    "text": "minimum. So you could be a maximum or what's called a Saddle point or something.",
    "start": "1257010",
    "end": "1261510"
  },
  {
    "text": "Now, in this particular case,",
    "start": "1261510",
    "end": "1263295"
  },
  {
    "text": "everything is cool because in fact there was only one solution of the gradient equals 0,",
    "start": "1263295",
    "end": "1268095"
  },
  {
    "text": "and that is, this one. It's this.",
    "start": "1268095",
    "end": "1269850"
  },
  {
    "text": "So it has to be the minimizer,",
    "start": "1269850",
    "end": "1272220"
  },
  {
    "text": "but it's a little bit weird because it could just as well have been the maximizer.",
    "start": "1272220",
    "end": "1275250"
  },
  {
    "text": "Okay. Now, the next derivation, um,",
    "start": "1275250",
    "end": "1278490"
  },
  {
    "start": "1276000",
    "end": "1548000"
  },
  {
    "text": "is a more direct verification, um, verification and I personally prefer it.",
    "start": "1278490",
    "end": "1283050"
  },
  {
    "text": "So, here it is,",
    "start": "1283050",
    "end": "1284370"
  },
  {
    "text": "[NOISE] and we'll directly show that x hat equals a dagger b,",
    "start": "1284370",
    "end": "1291270"
  },
  {
    "text": "minimizes the norm squared of ax minus b.",
    "start": "1291270",
    "end": "1293850"
  },
  {
    "text": "Let's see how that works. So, I'm going to define x hat to be this thing,",
    "start": "1293850",
    "end": "1297855"
  },
  {
    "text": "and then, well, you can just check from that,",
    "start": "1297855",
    "end": "1300705"
  },
  {
    "text": "it says that a transpose a,",
    "start": "1300705",
    "end": "1303495"
  },
  {
    "text": "x hat, I'm just going to use this equals a transpose b, and then from that,",
    "start": "1303495",
    "end": "1309690"
  },
  {
    "text": "I will rewrite that as this,",
    "start": "1309690",
    "end": "1311384"
  },
  {
    "text": "it says that a transpose times ax hat minus b, is 0.",
    "start": "1311385",
    "end": "1317340"
  },
  {
    "text": "By the way, it's pretty cool here.",
    "start": "1317340",
    "end": "1320460"
  },
  {
    "text": "Some people actually make a big deal about that equation there.",
    "start": "1320460",
    "end": "1324345"
  },
  {
    "text": "So it basically says that ax hat minus b is the optimal residual,",
    "start": "1324345",
    "end": "1329385"
  },
  {
    "text": "and what this says is the optimal residual,",
    "start": "1329385",
    "end": "1331980"
  },
  {
    "text": "is orthogonal to all the columns of a,",
    "start": "1331980",
    "end": "1334725"
  },
  {
    "text": "because that's what this equation says.",
    "start": "1334725",
    "end": "1336434"
  },
  {
    "text": "Right, so some people make a big deal about that, they even,",
    "start": "1336435",
    "end": "1339255"
  },
  {
    "text": "I think they even call it like the orthogonality principle or something,",
    "start": "1339255",
    "end": "1342510"
  },
  {
    "text": "which is fine, I don't object,",
    "start": "1342510",
    "end": "1343980"
  },
  {
    "text": "but the point is just this equation.",
    "start": "1343980",
    "end": "1345809"
  },
  {
    "text": "Okay. Now, [NOISE] what we're going to do is, now what I'm going",
    "start": "1345810",
    "end": "1350010"
  },
  {
    "text": "to do I'm going to directly show that if you take any other choice of x,",
    "start": "1350010",
    "end": "1354540"
  },
  {
    "text": "any choice of x at all,",
    "start": "1354540",
    "end": "1355965"
  },
  {
    "text": "you will have a norm squared residual,",
    "start": "1355965",
    "end": "1358034"
  },
  {
    "text": "which is at least as big as norm squared residual Ax hat,",
    "start": "1358035",
    "end": "1361350"
  },
  {
    "text": "I am sorry, associated with x hat.",
    "start": "1361350",
    "end": "1363030"
  },
  {
    "text": "So let's see how that works. Norm x minus b squared is this,",
    "start": "1363030",
    "end": "1367290"
  },
  {
    "text": "and what I have done in this equation is very silly.",
    "start": "1367290",
    "end": "1371100"
  },
  {
    "text": "What I've done is added and subtracted the same thing inside,",
    "start": "1371100",
    "end": "1375225"
  },
  {
    "text": "so here I've added ax hat,",
    "start": "1375225",
    "end": "1376695"
  },
  {
    "text": "and here I've subtracted it.",
    "start": "1376695",
    "end": "1378134"
  },
  {
    "text": "Well, that's weird, like why would you do that?",
    "start": "1378135",
    "end": "1380565"
  },
  {
    "text": "Then the answer is hang on and wait, and we'll see.",
    "start": "1380565",
    "end": "1383429"
  },
  {
    "text": "Now I'm going to use a formula for the norm squared of the sum of two vectors,",
    "start": "1383430",
    "end": "1391305"
  },
  {
    "text": "and that is always equal to the norm squared of the first vector plus the norm squared of",
    "start": "1391305",
    "end": "1395370"
  },
  {
    "text": "the second vector plus 2 times the inner product between the two vectors.",
    "start": "1395370",
    "end": "1399555"
  },
  {
    "text": "So, this I'm writing is this,",
    "start": "1399555",
    "end": "1401280"
  },
  {
    "text": "it's a norm squared of a times",
    "start": "1401280",
    "end": "1404520"
  },
  {
    "text": "x minus x hat plus norm square root of ax hat minus b squared norm squared,",
    "start": "1404520",
    "end": "1410205"
  },
  {
    "text": "sorry, plus, and then twice the inner product of this vector times that vector,",
    "start": "1410205",
    "end": "1414840"
  },
  {
    "text": "and that's this thing here.",
    "start": "1414840",
    "end": "1417510"
  },
  {
    "text": "Now, because what I'm going to do now,",
    "start": "1417510",
    "end": "1421695"
  },
  {
    "text": "is I'm going to do the transpose of a product.",
    "start": "1421695",
    "end": "1425130"
  },
  {
    "text": "So here I have a times the vector x minus ax hat transpose,",
    "start": "1425130",
    "end": "1428220"
  },
  {
    "text": "and I'm going to rewrite that,",
    "start": "1428220",
    "end": "1429855"
  },
  {
    "text": "I'm going to transpose each of them and reverse the order,",
    "start": "1429855",
    "end": "1432525"
  },
  {
    "text": "so that's x minus x hat transpose a transpose ax hat minus b.",
    "start": "1432525",
    "end": "1438585"
  },
  {
    "text": "Now we look up here,",
    "start": "1438585",
    "end": "1441570"
  },
  {
    "text": "we realize for our choice of x hat,",
    "start": "1441570",
    "end": "1444495"
  },
  {
    "text": "a transpose times ax hat minus b is 0,",
    "start": "1444495",
    "end": "1448575"
  },
  {
    "text": "and if you'd like, you could invoke the orthogonality principle or whatever you like.",
    "start": "1448575",
    "end": "1452669"
  },
  {
    "text": "This is 0 and this whole thing just goes away and you get this.",
    "start": "1452670",
    "end": "1458055"
  },
  {
    "text": "Now let's sit back and see what equation we have,",
    "start": "1458055",
    "end": "1460455"
  },
  {
    "text": "it's pretty awesome actually,",
    "start": "1460455",
    "end": "1461580"
  },
  {
    "text": "it says this, it says ax hat minus b,",
    "start": "1461580",
    "end": "1464985"
  },
  {
    "text": "sorry, ax minus b, x is anything at all,",
    "start": "1464985",
    "end": "1467160"
  },
  {
    "text": "any vector at all, norm squared is equal to ax hat minus b norm squared plus,",
    "start": "1467160",
    "end": "1475155"
  },
  {
    "text": "and then this thing, whatever it is, it's always positive.",
    "start": "1475155",
    "end": "1477855"
  },
  {
    "text": "That says that this is always bigger than that for any choice of x period.",
    "start": "1477855",
    "end": "1482490"
  },
  {
    "text": "That means we're done.",
    "start": "1482490",
    "end": "1484245"
  },
  {
    "text": "That shows you directly that x hat minimizes norm squared ax minus b.",
    "start": "1484245",
    "end": "1491655"
  },
  {
    "text": "Because here I've shown that for any other choice of x,",
    "start": "1491655",
    "end": "1494310"
  },
  {
    "text": "that norm squared is at least as big.",
    "start": "1494310",
    "end": "1496605"
  },
  {
    "text": "It's actually even interesting to see what it tells you.",
    "start": "1496605",
    "end": "1499260"
  },
  {
    "text": "It even says that, how big it is,",
    "start": "1499260",
    "end": "1501840"
  },
  {
    "text": "depends on how far you are from x hat,okay,",
    "start": "1501840",
    "end": "1503985"
  },
  {
    "text": "so, so that's it.",
    "start": "1503985",
    "end": "1508405"
  },
  {
    "text": "To see that it's unique,",
    "start": "1508405",
    "end": "1510635"
  },
  {
    "text": "we can figure that out too. Right, if I had two solutions,",
    "start": "1510635",
    "end": "1515675"
  },
  {
    "text": "x hat and x,",
    "start": "1515675",
    "end": "1517445"
  },
  {
    "text": "then they would both have to have the same value and then this has to be 0,",
    "start": "1517445",
    "end": "1521909"
  },
  {
    "text": "and that tells you x equals x hat,",
    "start": "1521910",
    "end": "1523650"
  },
  {
    "text": "so that gives you a uniqueness as well.",
    "start": "1523650",
    "end": "1526365"
  },
  {
    "text": "So that's the direct verification that the solution of",
    "start": "1526365",
    "end": "1531420"
  },
  {
    "text": "the approximate of- of the least squares problem is nothing more than a dagger times b.",
    "start": "1531420",
    "end": "1540090"
  },
  {
    "text": "The good news is, you've already seen it, you already know,",
    "start": "1540090",
    "end": "1543510"
  },
  {
    "text": "you've already been introduced to the pseudo-inverse, and so on.",
    "start": "1543510",
    "end": "1548670"
  },
  {
    "text": "Now it also tells us how we can compute it. So, um,",
    "start": "1548670",
    "end": "1552825"
  },
  {
    "text": "then the way we do this via the QR factorization,",
    "start": "1552825",
    "end": "1555750"
  },
  {
    "text": "you'll remember that the QR factorization",
    "start": "1555750",
    "end": "1558495"
  },
  {
    "text": "is, um, is one way that we had to express the pseudo-inverse,",
    "start": "1558495",
    "end": "1562920"
  },
  {
    "text": "and so we'll just use that. So [NOISE] we start by computing the QR factorization of a,",
    "start": "1562920",
    "end": "1568240"
  },
  {
    "text": "that's a equals QR and that costs you 2mn squared flops.",
    "start": "1568240",
    "end": "1572179"
  },
  {
    "text": "Let me point out one thing, um,",
    "start": "1572180",
    "end": "1574385"
  },
  {
    "text": "out here. Um, if this fails,",
    "start": "1574385",
    "end": "1579285"
  },
  {
    "text": "it's because a has dependent columns, but remember that's our, that's,",
    "start": "1579285",
    "end": "1582765"
  },
  {
    "text": "our assumption here, is that the columns of a are linearly independent.",
    "start": "1582765",
    "end": "1587790"
  },
  {
    "text": "Which implies by the way,",
    "start": "1587790",
    "end": "1589380"
  },
  {
    "text": "that Gram-Schmidt will terminate successfully,",
    "start": "1589380",
    "end": "1592515"
  },
  {
    "text": "and it also guarantees that a QR factorization exists.",
    "start": "1592515",
    "end": "1596115"
  },
  {
    "text": "Okay. Um, now, we're going to compute x hat,",
    "start": "1596115",
    "end": "1600195"
  },
  {
    "text": "which is a dagger b,",
    "start": "1600195",
    "end": "1601980"
  },
  {
    "text": "which is R inverse Q transpose b,",
    "start": "1601980",
    "end": "1604575"
  },
  {
    "text": "you'll remember that from last",
    "start": "1604575",
    "end": "1606404"
  },
  {
    "text": "lecture.Um,and the way we do that is we're going to form Q transpose b,",
    "start": "1606405",
    "end": "1610185"
  },
  {
    "text": "that's 2mn flops, and then we're going to compute x hat,",
    "start": "1610185",
    "end": "1613785"
  },
  {
    "text": "which is our inverse Q transpose.",
    "start": "1613785",
    "end": "1615300"
  },
  {
    "text": "We calculated this, and that's an upper triangular matrix, that's",
    "start": "1615300",
    "end": "1619440"
  },
  {
    "text": "solving an upper triangular system of",
    "start": "1619440",
    "end": "1621659"
  },
  {
    "text": "linear equations with an upper triangular coefficient matrix,",
    "start": "1621660",
    "end": "1624390"
  },
  {
    "text": "and that's just back substitution and that's n squared",
    "start": "1624390",
    "end": "1626760"
  },
  {
    "text": "flops. So both of these are actually negligible",
    "start": "1626760",
    "end": "1631545"
  },
  {
    "text": "compared to this, so the one that really counts is simply",
    "start": "1631545",
    "end": "1634200"
  },
  {
    "text": "the QR factorization is 2mn squared flops. And by the way, this is,",
    "start": "1634200",
    "end": "1639735"
  },
  {
    "text": "this is absolutely identical to the algorithm for",
    "start": "1639735",
    "end": "1643290"
  },
  {
    "text": "solving a x equals b for square invertible a.",
    "start": "1643290",
    "end": "1646806"
  },
  {
    "text": "Um, it's literally the same algorithm, right, you form a QR factorization,",
    "start": "1646806",
    "end": "1651960"
  },
  {
    "text": "you form Q transpose b and then you do back substitution with, with r. Now,",
    "start": "1651960",
    "end": "1657825"
  },
  {
    "text": "if a is square,",
    "start": "1657825",
    "end": "1659804"
  },
  {
    "text": "and of course this means it's invertible this just solves ax equals b,",
    "start": "1659805",
    "end": "1663795"
  },
  {
    "text": "but if a is tall,",
    "start": "1663795",
    "end": "1665445"
  },
  {
    "text": "then the exact same algorithm leads to the least squares solution.",
    "start": "1665445",
    "end": "1668940"
  },
  {
    "text": "So It's kind of cool. And by the way,",
    "start": "1668940",
    "end": "1671445"
  },
  {
    "text": "this is maybe why a lot of these packages use",
    "start": "1671445",
    "end": "1674850"
  },
  {
    "text": "this backslash notation to represent both",
    "start": "1674850",
    "end": "1677640"
  },
  {
    "text": "solving a set of linear equations and solving a least squares problem.",
    "start": "1677640",
    "end": "1682030"
  }
]