[
  {
    "start": "0",
    "end": "12000"
  },
  {
    "text": "So we are going to talk about random walk approaches,",
    "start": "4070",
    "end": "7950"
  },
  {
    "text": "uh, to node embeddings.",
    "start": "7950",
    "end": "9389"
  },
  {
    "text": "Um, and the idea here will be the following.",
    "start": "9390",
    "end": "12615"
  },
  {
    "start": "12000",
    "end": "207000"
  },
  {
    "text": "Uh, we are going to learn a vector z for every node",
    "start": "12615",
    "end": "16410"
  },
  {
    "text": "and this would be an embedding of the node and this is what we aim to find.",
    "start": "16410",
    "end": "20565"
  },
  {
    "text": "We are then going to, um,",
    "start": "20565",
    "end": "22770"
  },
  {
    "text": "also define a probability that, uh, basically will- uh,",
    "start": "22770",
    "end": "26745"
  },
  {
    "text": "will be the pre- predicted probability of how similar a given node u,",
    "start": "26745",
    "end": "31740"
  },
  {
    "text": "um, is uh, to some node, uh,",
    "start": "31740",
    "end": "34155"
  },
  {
    "text": "v. And given that we are going to use random walks to define this similarity,",
    "start": "34155",
    "end": "38519"
  },
  {
    "text": "this would be the probab- proba- uh,",
    "start": "38520",
    "end": "40410"
  },
  {
    "text": "predicted probability of visiting node v,",
    "start": "40410",
    "end": "42910"
  },
  {
    "text": "one random walks starting from node u.",
    "start": "42910",
    "end": "46030"
  },
  {
    "text": "Then we are also goin- need to, um,",
    "start": "46030",
    "end": "49980"
  },
  {
    "text": "nonlinear functions, uh, that will be used to,",
    "start": "49980",
    "end": "53579"
  },
  {
    "text": "uh, define or to produce these probabilities.",
    "start": "53580",
    "end": "55940"
  },
  {
    "text": "First, I'm going to define the notion of a softmax function,",
    "start": "55940",
    "end": "60070"
  },
  {
    "text": "which- which is- uh,",
    "start": "60070",
    "end": "61620"
  },
  {
    "text": "which returns a vector of k real values,",
    "start": "61620",
    "end": "65430"
  },
  {
    "text": "uh, um, and the- and these values sum to one.",
    "start": "65430",
    "end": "69320"
  },
  {
    "text": "So essentially, given a set of numbers z,",
    "start": "69320",
    "end": "72380"
  },
  {
    "text": "the- the softmax of that vector will be",
    "start": "72380",
    "end": "75229"
  },
  {
    "text": "a probability distribution over those values and the more likely that,",
    "start": "75230",
    "end": "80035"
  },
  {
    "text": "um, that- number is the maximum in the vector,",
    "start": "80035",
    "end": "83015"
  },
  {
    "text": "the higher the probability,",
    "start": "83015",
    "end": "84620"
  },
  {
    "text": "uh, it will have.",
    "start": "84620",
    "end": "85745"
  },
  {
    "text": "And essentially the way you can think of it, I take this, uh,",
    "start": "85745",
    "end": "88445"
  },
  {
    "text": "value z and I exponentiate them and then I normalize everything to sum to one.",
    "start": "88445",
    "end": "93650"
  },
  {
    "text": "So the idea is if the largest value, um,",
    "start": "93650",
    "end": "96810"
  },
  {
    "text": "in- in this vector z,",
    "start": "96810",
    "end": "98100"
  },
  {
    "text": "when I expo- exponentiate it,",
    "start": "98100",
    "end": "99869"
  },
  {
    "text": "it will be even larger than everything else.",
    "start": "99870",
    "end": "102105"
  },
  {
    "text": "So most probability mass,",
    "start": "102105",
    "end": "103950"
  },
  {
    "text": "um, will be concentrated on that value.",
    "start": "103950",
    "end": "106784"
  },
  {
    "text": "This is why this is called softmax because it's",
    "start": "106785",
    "end": "109070"
  },
  {
    "text": "a kind of a soft version of a maximum function.",
    "start": "109070",
    "end": "112040"
  },
  {
    "text": "Um, and then we are also going to define this notion of a sigmoid which is an",
    "start": "112040",
    "end": "116000"
  },
  {
    "text": "S shaped function that turns real values into,",
    "start": "116000",
    "end": "119960"
  },
  {
    "text": "uh, a range of- of 01,",
    "start": "119960",
    "end": "122195"
  },
  {
    "text": "um, and softmax is defined as 1 over 1 plus e to the minus x. Um,",
    "start": "122195",
    "end": "128410"
  },
  {
    "text": "and this is a nice way how to take something that lives on, uh,",
    "start": "128410",
    "end": "131535"
  },
  {
    "text": "minus infinity to plus infinity and kind of squish it,",
    "start": "131535",
    "end": "134750"
  },
  {
    "text": "uh, to, uh, value 01.",
    "start": "134750",
    "end": "137170"
  },
  {
    "text": "So that's- those are two important functions,",
    "start": "137170",
    "end": "139750"
  },
  {
    "text": "uh, I will- we will use.",
    "start": "139750",
    "end": "141500"
  },
  {
    "text": "Now let me define the notion of a random walk.",
    "start": "141500",
    "end": "144005"
  },
  {
    "text": "So a random walk is simply,",
    "start": "144005",
    "end": "146120"
  },
  {
    "text": "um, a process on top of the graph,",
    "start": "146120",
    "end": "148790"
  },
  {
    "text": "where we sa- let say start at some node and then out",
    "start": "148790",
    "end": "151700"
  },
  {
    "text": "of the outgoing neighbors of that node,",
    "start": "151700",
    "end": "154550"
  },
  {
    "text": "in this case it will be 1, 3, and 5,",
    "start": "154550",
    "end": "156460"
  },
  {
    "text": "we pick one at random and we move to it and this is one step of random- random walk.",
    "start": "156460",
    "end": "161870"
  },
  {
    "text": "Now we are in this,",
    "start": "161870",
    "end": "163114"
  },
  {
    "text": "uh, node 5, again,",
    "start": "163115",
    "end": "164390"
  },
  {
    "text": "we have four different ways in which we can go.",
    "start": "164390",
    "end": "166970"
  },
  {
    "text": "We can return back to four.",
    "start": "166970",
    "end": "168290"
  },
  {
    "text": "We can go to 8, 6 or 7 and we pick one of them at random and move there.",
    "start": "168290",
    "end": "173170"
  },
  {
    "text": "Um, and this process continues,",
    "start": "173170",
    "end": "175515"
  },
  {
    "text": "let say for a- for a- in this case for a fixed,",
    "start": "175515",
    "end": "178330"
  },
  {
    "text": "uh, number of steps.",
    "start": "178330",
    "end": "179940"
  },
  {
    "text": "So the way you can think of this is that we basically simulated this, uh,",
    "start": "179940",
    "end": "183530"
  },
  {
    "text": "random walk over- over this graph and let's say,",
    "start": "183530",
    "end": "186319"
  },
  {
    "text": "um, over our fixed, uh,",
    "start": "186320",
    "end": "188000"
  },
  {
    "text": "number of steps where the random walk can traverse the same edge multiple times,",
    "start": "188000",
    "end": "191840"
  },
  {
    "text": "can return, can go back and forth,",
    "start": "191840",
    "end": "194560"
  },
  {
    "text": "do, uh, whatever the random walk,",
    "start": "194560",
    "end": "196770"
  },
  {
    "text": "uh, wants to do.",
    "start": "196770",
    "end": "198345"
  },
  {
    "text": "All right. And this random walk is a seq- sequence of nodes visited this way,",
    "start": "198345",
    "end": "203480"
  },
  {
    "text": "uh, on a graph across the- across the edges.",
    "start": "203480",
    "end": "207430"
  },
  {
    "start": "207000",
    "end": "274000"
  },
  {
    "text": "So now how are we going to- to define",
    "start": "207430",
    "end": "210890"
  },
  {
    "text": "this notion of similarity and these probabilities that we talked about?",
    "start": "210890",
    "end": "214000"
  },
  {
    "text": "What we are going to do is to say we want to learn these",
    "start": "214000",
    "end": "216140"
  },
  {
    "text": "coordinate z such that the product of two,",
    "start": "216140",
    "end": "219170"
  },
  {
    "text": "uh, nodes u and v,",
    "start": "219170",
    "end": "221224"
  },
  {
    "text": "um, is similar or equals is, uh,",
    "start": "221225",
    "end": "224075"
  },
  {
    "text": "approximates the probability that u and v co-occur on a random walk, uh, over the graph.",
    "start": "224075",
    "end": "229465"
  },
  {
    "text": "So here is- here is the idea, right?",
    "start": "229465",
    "end": "231765"
  },
  {
    "text": "First we will need to estimate the probability of visiting",
    "start": "231765",
    "end": "234260"
  },
  {
    "text": "node v on a random walk starting,",
    "start": "234260",
    "end": "236790"
  },
  {
    "text": "uh, at some node u using some, let's say,",
    "start": "236790",
    "end": "239219"
  },
  {
    "text": "a random walk strategy R. I'm going to define",
    "start": "239220",
    "end": "241550"
  },
  {
    "text": "this notion of random walks strategy, uh, later,",
    "start": "241550",
    "end": "244220"
  },
  {
    "text": "but for now just think it's a simple random walk where we pick one of the, uh,",
    "start": "244220",
    "end": "247700"
  },
  {
    "text": "uh, neighbors uniformly at random and we move to it.",
    "start": "247700",
    "end": "251709"
  },
  {
    "text": "And then we wanna optimize the embeddings,",
    "start": "251710",
    "end": "254670"
  },
  {
    "text": "uh, in such a way to encode this random walk statistics.",
    "start": "254670",
    "end": "257980"
  },
  {
    "text": "Basically we want the- the cosine of the angle between the two vectors,",
    "start": "257980",
    "end": "262970"
  },
  {
    "text": "this is the dot product to be proportional or similar to the probability that,",
    "start": "262970",
    "end": "267950"
  },
  {
    "text": "uh, u and v are visited, uh,",
    "start": "267950",
    "end": "270590"
  },
  {
    "text": "uh, on the same random, uh, walk.",
    "start": "270590",
    "end": "274355"
  },
  {
    "start": "274000",
    "end": "318000"
  },
  {
    "text": "So why random walks?",
    "start": "274355",
    "end": "276240"
  },
  {
    "text": "We want to use random walks because they are expressive, they are flexible.",
    "start": "276240",
    "end": "280655"
  },
  {
    "text": "It gives us a flexible stochastic definition of node similarity that",
    "start": "280655",
    "end": "284570"
  },
  {
    "text": "incorporates both kind of local as well as higher order neighbor with information, right?",
    "start": "284570",
    "end": "289500"
  },
  {
    "text": "And the idea is that if a random walk, uh,",
    "start": "289500",
    "end": "291680"
  },
  {
    "text": "starting from node u visits v, um,",
    "start": "291680",
    "end": "294195"
  },
  {
    "text": "with high probability that u and v are similar, uh, uh,",
    "start": "294195",
    "end": "297690"
  },
  {
    "text": "they have kind of similar network neighborhood they are close together with each other,",
    "start": "297690",
    "end": "301530"
  },
  {
    "text": "there might be multiple paths between them and so on.",
    "start": "301530",
    "end": "304410"
  },
  {
    "text": "Um, and what is interesting is that this is in some sense also",
    "start": "304410",
    "end": "308930"
  },
  {
    "text": "efficient because we do not need to consider all the node pairs when- when training.",
    "start": "308930",
    "end": "313280"
  },
  {
    "text": "We only need to consider pairs that co-occur in random walks.",
    "start": "313280",
    "end": "317570"
  },
  {
    "text": "[BACKGROUND] So the supervised,",
    "start": "317570",
    "end": "320404"
  },
  {
    "start": "318000",
    "end": "367000"
  },
  {
    "text": "uh, feature learning, uh,",
    "start": "320404",
    "end": "321889"
  },
  {
    "text": "will work the following.",
    "start": "321890",
    "end": "323000"
  },
  {
    "text": "The intuition is that we find embedding of nodes in",
    "start": "323000",
    "end": "325325"
  },
  {
    "text": "d-dimensional space that preserves similarity.",
    "start": "325325",
    "end": "328375"
  },
  {
    "text": "Uh, the idea is that we want to learn node embedding such that nearby nodes in",
    "start": "328375",
    "end": "332520"
  },
  {
    "text": "the network are clo- are- are embedded close together in the embedding space.",
    "start": "332520",
    "end": "336979"
  },
  {
    "text": "Um, and given a node u the question is,",
    "start": "336980",
    "end": "339800"
  },
  {
    "text": "how do we define nearby?",
    "start": "339800",
    "end": "341604"
  },
  {
    "text": "And we are going to have these definition and,",
    "start": "341605",
    "end": "345210"
  },
  {
    "text": "uh, sub r of u, where, uh,",
    "start": "345210",
    "end": "347354"
  },
  {
    "text": "basically this is n labels the neighborhood,",
    "start": "347355",
    "end": "350820"
  },
  {
    "text": "uh, of u obtained by some random walk strategy or r, right?",
    "start": "350820",
    "end": "354750"
  },
  {
    "text": "So for a given node uh,",
    "start": "354750",
    "end": "356280"
  },
  {
    "text": "u we need to define what is the neighborhood?",
    "start": "356280",
    "end": "358830"
  },
  {
    "text": "And in our case, neighborhood will simply be a sequence of nodes that this,",
    "start": "358830",
    "end": "363080"
  },
  {
    "text": "um, uh, that the random walk starting at u has visited.",
    "start": "363080",
    "end": "368055"
  },
  {
    "start": "367000",
    "end": "432000"
  },
  {
    "text": "So now how are we setting this up as an optimization problem?",
    "start": "368055",
    "end": "372005"
  },
  {
    "text": "Given the graph, uh,",
    "start": "372005",
    "end": "373385"
  },
  {
    "text": "on nodes, uh, V and an edge set E,",
    "start": "373385",
    "end": "376810"
  },
  {
    "text": "our goal is to learn a mapping from the nodes, uh,",
    "start": "376810",
    "end": "380450"
  },
  {
    "text": "to their embeddings and we are going to maximize the following,",
    "start": "380450",
    "end": "384170"
  },
  {
    "text": "uh, maximum likelihood objective, right?",
    "start": "384170",
    "end": "386195"
  },
  {
    "text": "Our goal will be to find this function,",
    "start": "386195",
    "end": "388460"
  },
  {
    "text": "this mapping so basically find the coordinates z of the nodes such that,",
    "start": "388460",
    "end": "392729"
  },
  {
    "text": "uh, the summation over all the nodes, uh,",
    "start": "392730",
    "end": "395305"
  },
  {
    "text": "of log probabilities that given the node u, um, that, uh,",
    "start": "395305",
    "end": "399925"
  },
  {
    "text": "maximizes log probabilities of the nodes that appear in its- uh,",
    "start": "399925",
    "end": "404449"
  },
  {
    "text": "in its local uh, random-walk neighborhood, right?",
    "start": "404450",
    "end": "408210"
  },
  {
    "text": "So we want to basically to sum out- to maximize the sum,",
    "start": "408210",
    "end": "411375"
  },
  {
    "text": "which means we want to make nodes that are, um,",
    "start": "411375",
    "end": "414560"
  },
  {
    "text": "that are visited in the same random walk to be kind of embedded,",
    "start": "414560",
    "end": "418399"
  },
  {
    "text": "uh, close together, right?",
    "start": "418399",
    "end": "419909"
  },
  {
    "text": "So we wanna learn feature representations that are",
    "start": "419910",
    "end": "422330"
  },
  {
    "text": "predictive of the nodes in each, uh, uh, uh,",
    "start": "422330",
    "end": "425224"
  },
  {
    "text": "of the nodes that appear in it's, uh,",
    "start": "425225",
    "end": "427490"
  },
  {
    "text": "random walk neighborhood uh, uh,",
    "start": "427490",
    "end": "429365"
  },
  {
    "text": "N. That's the idea.",
    "start": "429365",
    "end": "432294"
  },
  {
    "start": "432000",
    "end": "667000"
  },
  {
    "text": "So how are we going to do this?",
    "start": "432295",
    "end": "434730"
  },
  {
    "text": "First, we are going to run short fixed length",
    "start": "434730",
    "end": "437560"
  },
  {
    "text": "random walk starting from each node u in the graph using,",
    "start": "437560",
    "end": "441500"
  },
  {
    "text": "uh, some random walks strategy R. Uh,",
    "start": "441500",
    "end": "443600"
  },
  {
    "text": "for each node u,",
    "start": "443600",
    "end": "445025"
  },
  {
    "text": "we are going to collect, uh,",
    "start": "445025",
    "end": "446570"
  },
  {
    "text": "N of u which is a multi set of nodes",
    "start": "446570",
    "end": "449300"
  },
  {
    "text": "visited in random walk starting from node u. Multi-set",
    "start": "449300",
    "end": "452509"
  },
  {
    "text": "meaning that a same node can appear",
    "start": "452510",
    "end": "455030"
  },
  {
    "text": "multiple times in the neighborhood because it may be visited multiple times.",
    "start": "455030",
    "end": "458620"
  },
  {
    "text": "Um, and then we are going to optimize- define",
    "start": "458620",
    "end": "461449"
  },
  {
    "text": "an optimization problem and optimize the embedding, so that, uh,",
    "start": "461450",
    "end": "465350"
  },
  {
    "text": "given node u we wanna be able to predict who are",
    "start": "465350",
    "end": "468350"
  },
  {
    "text": "the nodes that are in its neighborhood and defined again by the random walk.",
    "start": "468350",
    "end": "472280"
  },
  {
    "text": "So we are going to maximize, uh, uh, this,",
    "start": "472280",
    "end": "475460"
  },
  {
    "text": "uh, objective here, uh,",
    "start": "475460",
    "end": "476930"
  },
  {
    "text": "this maximum likelihood objective.",
    "start": "476930",
    "end": "479155"
  },
  {
    "text": "So how can we write this out?",
    "start": "479155",
    "end": "481190"
  },
  {
    "text": "We can write this out the following.",
    "start": "481190",
    "end": "482480"
  },
  {
    "text": "We- we- we write it as sum over all the starting nodes u,",
    "start": "482480",
    "end": "486470"
  },
  {
    "text": "sum over all the nodes that are in the neighborhood of u.",
    "start": "486470",
    "end": "490050"
  },
  {
    "text": "Let's call this nodes v and then we wanna maximize the log probability,",
    "start": "490050",
    "end": "494360"
  },
  {
    "text": "uh, that predicts, uh,",
    "start": "494360",
    "end": "495949"
  },
  {
    "text": "that node v, um,",
    "start": "495950",
    "end": "497540"
  },
  {
    "text": "is in the neighborhood of node U. Um,",
    "start": "497540",
    "end": "500640"
  },
  {
    "text": "and as I said opt- intuition is that we want to optimize embeddings to",
    "start": "500640",
    "end": "504230"
  },
  {
    "text": "maximize the likelihood of a random walk, uh, co-occurrence.",
    "start": "504230",
    "end": "507890"
  },
  {
    "text": "Um, how are we going to do this?",
    "start": "507890",
    "end": "510335"
  },
  {
    "text": "Um, we still need to define this, uh,",
    "start": "510335",
    "end": "513315"
  },
  {
    "text": "probability p and the way we are going to define it is we are going to use",
    "start": "513315",
    "end": "517370"
  },
  {
    "text": "the notion of softmax function that I have introduced a couple of slides ago.",
    "start": "517370",
    "end": "521914"
  },
  {
    "text": "So the idea here will be that what we wanna do is we wanna maximize",
    "start": "521915",
    "end": "526024"
  },
  {
    "text": "the dot product between the node u and node v. So node u is the starting node,",
    "start": "526025",
    "end": "532250"
  },
  {
    "text": "node v is the node,",
    "start": "532250",
    "end": "533630"
  },
  {
    "text": "um, in the neighborhood.",
    "start": "533630",
    "end": "535525"
  },
  {
    "text": "Random walk neighborhood of node, uh,",
    "start": "535525",
    "end": "537510"
  },
  {
    "text": "u we wanna maxima- we wanna to apply softf- softmax.",
    "start": "537510",
    "end": "541890"
  },
  {
    "text": "So this is the exponentiated value of the dot product of the node that is in",
    "start": "541890",
    "end": "546920"
  },
  {
    "text": "the neighborhood divided by sum of",
    "start": "546920",
    "end": "549170"
  },
  {
    "text": "the exponential dot product with all the other nodes in the network, right?",
    "start": "549170",
    "end": "553519"
  },
  {
    "text": "So the idea here is that we wanna assign as much probability mass to- uh,",
    "start": "553520",
    "end": "558180"
  },
  {
    "text": "to these dot product um,",
    "start": "558180",
    "end": "560550"
  },
  {
    "text": "and as little to all other, uh, dot products.",
    "start": "560550",
    "end": "564690"
  },
  {
    "text": "So now to write- to put it all together,",
    "start": "564690",
    "end": "567460"
  },
  {
    "text": "the way we can think of this is- this is- we are trying to optimize this function,",
    "start": "567460",
    "end": "571660"
  },
  {
    "text": "which is a sum over all the nodes for every node,",
    "start": "571660",
    "end": "574795"
  },
  {
    "text": "sum over all the nodes v that are seen on",
    "start": "574795",
    "end": "578410"
  },
  {
    "text": "random walks starting from this node u and then we wanna, uh, uh, uh,",
    "start": "578410",
    "end": "582745"
  },
  {
    "text": "optimize for a minus log probability of these softmax which says,",
    "start": "582745",
    "end": "588145"
  },
  {
    "text": "I want to, uh,",
    "start": "588145",
    "end": "589410"
  },
  {
    "text": "maximize the dot product between the- the starting node u and",
    "start": "589410",
    "end": "593829"
  },
  {
    "text": "the node v that is in the neighborhood and we- and we normalize this over all the nodes,",
    "start": "593830",
    "end": "598360"
  },
  {
    "text": "uh, in the network.",
    "start": "598360",
    "end": "599680"
  },
  {
    "text": "So now, um, you know what does it mean optimizing random walk embeddings,",
    "start": "599680",
    "end": "604140"
  },
  {
    "text": "it means finding this coordinates z, uh,",
    "start": "604140",
    "end": "607070"
  },
  {
    "text": "used here such that this likelihood function is, uh, minimized.",
    "start": "607070",
    "end": "611595"
  },
  {
    "text": "Now, uh, the question is,",
    "start": "611595",
    "end": "614035"
  },
  {
    "text": "how do we do this in practice?",
    "start": "614035",
    "end": "616735"
  },
  {
    "text": "And the problem is that this is very expensive because if you look at this,",
    "start": "616735",
    "end": "620725"
  },
  {
    "text": "we actually have to sum- two nested summations,",
    "start": "620725",
    "end": "623529"
  },
  {
    "text": "uh, over all nodes of the network.",
    "start": "623530",
    "end": "625450"
  },
  {
    "text": "We sum over all nodes in the- of- of the network",
    "start": "625450",
    "end": "627880"
  },
  {
    "text": "here for starting nodes of the random walks.",
    "start": "627880",
    "end": "630625"
  },
  {
    "text": "And here when we are normalizing softmax,",
    "start": "630625",
    "end": "633355"
  },
  {
    "text": "we are normalizing it over all of the nodes of the network again.",
    "start": "633355",
    "end": "637120"
  },
  {
    "text": "So this is a double summation,",
    "start": "637120",
    "end": "639205"
  },
  {
    "text": "which means that it will have complexity order, um, V squared.",
    "start": "639205",
    "end": "643030"
  },
  {
    "text": "So it will be, uh, quadratic in the number of nodes in the network,",
    "start": "643030",
    "end": "646060"
  },
  {
    "text": "and that's prohibitively expensive.",
    "start": "646060",
    "end": "647965"
  },
  {
    "text": "So let me tell you what, uh,",
    "start": "647965",
    "end": "649720"
  },
  {
    "text": "we do to make this, uh, practical.",
    "start": "649720",
    "end": "652584"
  },
  {
    "text": "And the- the issue here is that there is this problem with the softmax that, um,",
    "start": "652585",
    "end": "658330"
  },
  {
    "text": "we need to sum over all the nodes to basically",
    "start": "658330",
    "end": "661360"
  },
  {
    "text": "normalize it back to- to a distribution over the nodes.",
    "start": "661360",
    "end": "664795"
  },
  {
    "text": "So, um, can we approximate this theorem?",
    "start": "664795",
    "end": "667660"
  },
  {
    "start": "667000",
    "end": "817000"
  },
  {
    "text": "And the answer is yes.",
    "start": "667660",
    "end": "669310"
  },
  {
    "text": "And the solution to this is called negative sampling.",
    "start": "669310",
    "end": "672355"
  },
  {
    "text": "And intuitively, the idea is that rather than summing here over all the nodes,",
    "start": "672355",
    "end": "677740"
  },
  {
    "text": "uh, in the network, we are only going to sum over a subset of the nodes.",
    "start": "677740",
    "end": "682630"
  },
  {
    "text": "So essentially, we are going to sample a couple of",
    "start": "682630",
    "end": "685420"
  },
  {
    "text": "negative examples and sum, uh, over them.",
    "start": "685420",
    "end": "688660"
  },
  {
    "text": "So the way the approximation works out is that, um, we, um,",
    "start": "688660",
    "end": "693509"
  },
  {
    "text": "we can- we can view this as an approximation to the, um,",
    "start": "693510",
    "end": "697135"
  },
  {
    "text": "to the- to the softmax function, where we can,",
    "start": "697135",
    "end": "700630"
  },
  {
    "text": "uh, approximate it using,",
    "start": "700630",
    "end": "702340"
  },
  {
    "text": "uh, the following, uh, expression.",
    "start": "702340",
    "end": "704560"
  },
  {
    "text": "We are going to take log sigmoid function of the dot product between u and v. Uh,",
    "start": "704560",
    "end": "710020"
  },
  {
    "text": "this is for the, uh, for the theorem here.",
    "start": "710020",
    "end": "712480"
  },
  {
    "text": "And then we are going to say minus sum of i going from one to k,",
    "start": "712480",
    "end": "717399"
  },
  {
    "text": "so this is our k negative examples, logarithm, again,",
    "start": "717400",
    "end": "721120"
  },
  {
    "text": "of the sigmoid function between the, uh,",
    "start": "721120",
    "end": "723235"
  },
  {
    "text": "st- starting node u,",
    "start": "723235",
    "end": "725095"
  },
  {
    "text": "and the negative, um,",
    "start": "725095",
    "end": "727389"
  },
  {
    "text": "negative sample, uh, i,",
    "start": "727390",
    "end": "729160"
  },
  {
    "text": "where this negative samples,",
    "start": "729160",
    "end": "730690"
  },
  {
    "text": "this negative nodes will be, uh,",
    "start": "730690",
    "end": "732895"
  },
  {
    "text": "sampled at random, but not at ra- at uniform random,",
    "start": "732895",
    "end": "737470"
  },
  {
    "text": "but random in a biased way.",
    "start": "737470",
    "end": "739329"
  },
  {
    "text": "So the idea here is that,",
    "start": "739330",
    "end": "741110"
  },
  {
    "text": "instead of normalizing with respect to all nodes in the network,",
    "start": "741110",
    "end": "745125"
  },
  {
    "text": "we are going to normalize softmax against k random negative samples,",
    "start": "745125",
    "end": "750840"
  },
  {
    "text": "so negative nodes, uh, from the network.",
    "start": "750840",
    "end": "753195"
  },
  {
    "text": "And this negative samples will be carefully chosen.",
    "start": "753195",
    "end": "756795"
  },
  {
    "text": "So how do we choose negative samples?",
    "start": "756795",
    "end": "760394"
  },
  {
    "text": "We- we sample k negative nodes,",
    "start": "760395",
    "end": "763260"
  },
  {
    "text": "each with probabil- probability proportional to its degrees.",
    "start": "763260",
    "end": "766680"
  },
  {
    "text": "So it means that nodes that have higher degree,",
    "start": "766680",
    "end": "769399"
  },
  {
    "text": "uh, will be more likely to be chosen as a negative sample.",
    "start": "769400",
    "end": "772960"
  },
  {
    "text": "Um, there are two considerations for picking k in practice,",
    "start": "772960",
    "end": "777400"
  },
  {
    "text": "which means number of negative samples.",
    "start": "777400",
    "end": "779260"
  },
  {
    "text": "Higher values of k will give me more robust estimate.",
    "start": "779260",
    "end": "782965"
  },
  {
    "text": "Uh, but higher values of K also correspond, uh, to,",
    "start": "782965",
    "end": "786850"
  },
  {
    "text": "uh, to more, uh,",
    "start": "786850",
    "end": "788305"
  },
  {
    "text": "to more sampling again, to higher bias on negative events.",
    "start": "788305",
    "end": "791380"
  },
  {
    "text": "So what people tend to choose in practice is k between 5 and 20.",
    "start": "791380",
    "end": "795790"
  },
  {
    "text": "And if you think about it,",
    "start": "795790",
    "end": "796990"
  },
  {
    "text": "this is a very small number.",
    "start": "796990",
    "end": "798820"
  },
  {
    "text": "If you think network of a million nodes or 100,000 nodes,",
    "start": "798820",
    "end": "802330"
  },
  {
    "text": "rather than summing over 100,000,",
    "start": "802330",
    "end": "804730"
  },
  {
    "text": "uh, nodes, uh, every time here,",
    "start": "804730",
    "end": "807010"
  },
  {
    "text": "you are only summing over, you know,",
    "start": "807010",
    "end": "808960"
  },
  {
    "text": "5-20 nodes in this case.",
    "start": "808960",
    "end": "810880"
  },
  {
    "text": "And this way, your method and your estimation will be far,",
    "start": "810880",
    "end": "814030"
  },
  {
    "text": "um, much, much, much more, uh, efficient.",
    "start": "814030",
    "end": "817315"
  },
  {
    "start": "817000",
    "end": "959000"
  },
  {
    "text": "So, um, now how do we solve this optimization problem?",
    "start": "817315",
    "end": "821530"
  },
  {
    "text": "Uh, I won't go into too much detail,",
    "start": "821530",
    "end": "823930"
  },
  {
    "text": "but these things are today solved with,",
    "start": "823930",
    "end": "826585"
  },
  {
    "text": "uh, stochastic gradient descent.",
    "start": "826585",
    "end": "828415"
  },
  {
    "text": "And I just want to give you a quick introduction to stochastic gradient descent, uh,",
    "start": "828415",
    "end": "833274"
  },
  {
    "text": "two slides that are great lectures, uh,",
    "start": "833275",
    "end": "836800"
  },
  {
    "text": "a lot of really good tutorials on what is stochastic gradient descent,",
    "start": "836800",
    "end": "840790"
  },
  {
    "text": "how does it work and all the theoretical analysis of it.",
    "start": "840790",
    "end": "843910"
  },
  {
    "text": "But essentially, the idea is that if you have a smooth function,",
    "start": "843910",
    "end": "847194"
  },
  {
    "text": "then you can optimizing- optimize it by doing gradient descent,",
    "start": "847195",
    "end": "851260"
  },
  {
    "text": "by basically computing the gradient at- at a given point and then moving, um, uh, for,",
    "start": "851260",
    "end": "855880"
  },
  {
    "text": "uh, as a small step, um,",
    "start": "855880",
    "end": "857890"
  },
  {
    "text": "in- in the direction opposite of the gradient, right?",
    "start": "857890",
    "end": "860800"
  },
  {
    "text": "So this is the- this is the idea here, right?",
    "start": "860800",
    "end": "863290"
  },
  {
    "text": "Your start at some random point.",
    "start": "863290",
    "end": "865029"
  },
  {
    "text": "Um, in our case,",
    "start": "865030",
    "end": "866620"
  },
  {
    "text": "we can initialize embeddings of nodes,",
    "start": "866620",
    "end": "869050"
  },
  {
    "text": "uh, at- with random numbers,",
    "start": "869050",
    "end": "871555"
  },
  {
    "text": "and then we iterate until we converge.",
    "start": "871555",
    "end": "873595"
  },
  {
    "text": "We computed the derivative of",
    "start": "873595",
    "end": "875439"
  },
  {
    "text": "the likelihood function with respect to the embedding of a single node,",
    "start": "875440",
    "end": "879520"
  },
  {
    "text": "and now we find the direction of the derivative of the gradient.",
    "start": "879520",
    "end": "883180"
  },
  {
    "text": "And then we make, uh,",
    "start": "883180",
    "end": "884470"
  },
  {
    "text": "a step in the opposite direction of that gradient,",
    "start": "884470",
    "end": "887889"
  },
  {
    "text": "where, um, uh, this is the learning rate,",
    "start": "887890",
    "end": "890950"
  },
  {
    "text": "which means how big step do we take.",
    "start": "890950",
    "end": "892900"
  },
  {
    "text": "And we can actually even tune the step as we make,",
    "start": "892900",
    "end": "895240"
  },
  {
    "text": "uh, as we go and solve.",
    "start": "895240",
    "end": "896890"
  },
  {
    "text": "Uh, but essentially, this is what gradient descent is.",
    "start": "896890",
    "end": "900505"
  },
  {
    "text": "And in stochastic gradient descent,",
    "start": "900505",
    "end": "902740"
  },
  {
    "text": "we are approximating the gradient in a stochastic way.",
    "start": "902740",
    "end": "905770"
  },
  {
    "text": "So rather than evaluating the gradient over all the examples, we just do it, um,",
    "start": "905770",
    "end": "910570"
  },
  {
    "text": "uh, over, uh, a small batch of examples or over an individual examples.",
    "start": "910570",
    "end": "915640"
  },
  {
    "text": "So what does it mean? Is that rather than, um, uh, evaluating,",
    "start": "915640",
    "end": "919780"
  },
  {
    "text": "uh, the gradient over all the nodes- all the negative nodes, um,",
    "start": "919780",
    "end": "924280"
  },
  {
    "text": "and- or all the neighbors in the neighborhood of a given node,",
    "start": "924280",
    "end": "928450"
  },
  {
    "text": "and then make a- make a step,",
    "start": "928450",
    "end": "930205"
  },
  {
    "text": "we are going to do this only for a, uh,",
    "start": "930205",
    "end": "932335"
  },
  {
    "text": "for a given, uh,",
    "start": "932335",
    "end": "933670"
  },
  {
    "text": "for a given node in the neighborhood.",
    "start": "933670",
    "end": "935290"
  },
  {
    "text": "So basically, the idea is that, you know,",
    "start": "935290",
    "end": "936970"
  },
  {
    "text": "we'll sample node i,",
    "start": "936970",
    "end": "938274"
  },
  {
    "text": "and then for all, uh,",
    "start": "938275",
    "end": "939655"
  },
  {
    "text": "js that in the- in the,",
    "start": "939655",
    "end": "941740"
  },
  {
    "text": "uh, in the neighborhood,",
    "start": "941740",
    "end": "942880"
  },
  {
    "text": "we are going to compute the gradient, uh,",
    "start": "942880",
    "end": "945295"
  },
  {
    "text": "and then make a step and keep iterating this.",
    "start": "945295",
    "end": "948355"
  },
  {
    "text": "Uh, we- of course, we'll get",
    "start": "948355",
    "end": "949630"
  },
  {
    "text": "a stochastic estimates or kind of a random estimate of the gradient.",
    "start": "949630",
    "end": "952840"
  },
  {
    "text": "But we'll be able to up- to make updates much, much faster,",
    "start": "952840",
    "end": "955735"
  },
  {
    "text": "which in practice tends,",
    "start": "955735",
    "end": "957070"
  },
  {
    "text": "uh, to be much, uh, better.",
    "start": "957070",
    "end": "959305"
  },
  {
    "start": "959000",
    "end": "1009000"
  },
  {
    "text": "So let me summarize.",
    "start": "959305",
    "end": "961585"
  },
  {
    "text": "We are going to run th-",
    "start": "961585",
    "end": "963190"
  },
  {
    "text": "a short fixed-length random walks starting from each node on the graph.",
    "start": "963190",
    "end": "967210"
  },
  {
    "text": "Uh, for each node u,",
    "start": "967210",
    "end": "969025"
  },
  {
    "text": "we are going to collect, um,",
    "start": "969025",
    "end": "971245"
  },
  {
    "text": "its neighborhood and as a multiset of",
    "start": "971245",
    "end": "973960"
  },
  {
    "text": "nodes visited on the random walks starting from node u.",
    "start": "973960",
    "end": "976540"
  },
  {
    "text": "And then we are going to optimize this embeddings",
    "start": "976540",
    "end": "978940"
  },
  {
    "text": "used- using stochastic gradient descent, which means, uh,",
    "start": "978940",
    "end": "982030"
  },
  {
    "text": "we are going to, uh, uh,",
    "start": "982030",
    "end": "983575"
  },
  {
    "text": "find the coordinate Z that maximize,",
    "start": "983575",
    "end": "986380"
  },
  {
    "text": "uh, this particular expression.",
    "start": "986380",
    "end": "988360"
  },
  {
    "text": "And we are going to efficiently approximate this expression,",
    "start": "988360",
    "end": "991510"
  },
  {
    "text": "uh, using negative sampling,",
    "start": "991510",
    "end": "993580"
  },
  {
    "text": "where we, um, sample negative nodes of each probability proportional to their degree.",
    "start": "993580",
    "end": "999010"
  },
  {
    "text": "And in practice, we sample about 5-20 negative examples,",
    "start": "999010",
    "end": "1003195"
  },
  {
    "text": "uh, for, uh, for every node, for every step.",
    "start": "1003195",
    "end": "1006555"
  },
  {
    "text": "So, um, now, um,",
    "start": "1006555",
    "end": "1009330"
  },
  {
    "start": "1009000",
    "end": "1049000"
  },
  {
    "text": "the question that I wanna also talk is,",
    "start": "1009330",
    "end": "1012495"
  },
  {
    "text": "um, you know, how should we do this random walk?",
    "start": "1012495",
    "end": "1014790"
  },
  {
    "text": "Right? So far, I only described, uh,",
    "start": "1014790",
    "end": "1017130"
  },
  {
    "text": "how to optimize the embeddings,",
    "start": "1017130",
    "end": "1018914"
  },
  {
    "text": "uh, for a given the random walk, um, uh,",
    "start": "1018914",
    "end": "1021435"
  },
  {
    "text": "R. And we talked about this uniform random walk,",
    "start": "1021435",
    "end": "1024300"
  },
  {
    "text": "where basically we run fixed-length unbiased random walks starting at each node.",
    "start": "1024300",
    "end": "1029819"
  },
  {
    "text": "And the idea here is that, um,",
    "start": "1029820",
    "end": "1032880"
  },
  {
    "text": "there is the issue of this, uh,",
    "start": "1032880",
    "end": "1034530"
  },
  {
    "text": "type of similarity because in many cases,",
    "start": "1034530",
    "end": "1036915"
  },
  {
    "text": "it might be to constraint.",
    "start": "1036915",
    "end": "1038295"
  },
  {
    "text": "So the question is, can we use richer, um, random walks?",
    "start": "1038295",
    "end": "1042510"
  },
  {
    "text": "Can be made the- can be make random walks more",
    "start": "1042510",
    "end": "1045150"
  },
  {
    "text": "expressive so that we can tune these embeddings more?",
    "start": "1045150",
    "end": "1049035"
  },
  {
    "start": "1049000",
    "end": "1181000"
  },
  {
    "text": "And this is the idea of a method called, uh, node2vec,",
    "start": "1049035",
    "end": "1053280"
  },
  {
    "text": "where the idea is that we wanna, again,",
    "start": "1053280",
    "end": "1055440"
  },
  {
    "text": "embed nodes with similar network neighborhoods,",
    "start": "1055440",
    "end": "1057870"
  },
  {
    "text": "uh, closing the feature space.",
    "start": "1057870",
    "end": "1059565"
  },
  {
    "text": "Uh, we are going to frame.",
    "start": "1059565",
    "end": "1060825"
  },
  {
    "text": "The goal is again as, um,",
    "start": "1060825",
    "end": "1062669"
  },
  {
    "text": "maximum likelihood optimization problem,",
    "start": "1062670",
    "end": "1064950"
  },
  {
    "text": "uh, independent of the downstream prediction task.",
    "start": "1064950",
    "end": "1067679"
  },
  {
    "text": "Uh, and the key observation here is that",
    "start": "1067680",
    "end": "1070230"
  },
  {
    "text": "we have a flexible notion of network neighborhood,",
    "start": "1070230",
    "end": "1073635"
  },
  {
    "text": "um, which leads to much richer, uh, node embeddings.",
    "start": "1073635",
    "end": "1077115"
  },
  {
    "text": "And the extension of this simple random walk, uh,",
    "start": "1077115",
    "end": "1080370"
  },
  {
    "text": "here is that we are going to develop a second-order random",
    "start": "1080370",
    "end": "1083910"
  },
  {
    "text": "walk R to generate- to generate the network neighborhood,",
    "start": "1083910",
    "end": "1087900"
  },
  {
    "text": "uh, N, and then we are going to apply the same optimization problem.",
    "start": "1087900",
    "end": "1091890"
  },
  {
    "text": "So the only difference between deep walk and node2vec is how these, uh,",
    "start": "1091890",
    "end": "1096210"
  },
  {
    "text": "set of neighboring nodes, um,",
    "start": "1096210",
    "end": "1099135"
  },
  {
    "text": "is defined and how the random walk is defined.",
    "start": "1099135",
    "end": "1102315"
  },
  {
    "text": "So the idea is to use flexible, um,",
    "start": "1102315",
    "end": "1106169"
  },
  {
    "text": "biased random walks that can trade off between the local and global views,",
    "start": "1106170",
    "end": "1109770"
  },
  {
    "text": "uh, in the network.",
    "start": "1109770",
    "end": "1111495"
  },
  {
    "text": "Um, and what I mean by local and global,",
    "start": "1111495",
    "end": "1113865"
  },
  {
    "text": "when you are doing the random walk,",
    "start": "1113865",
    "end": "1115350"
  },
  {
    "text": "you can think of, uh, for example,",
    "start": "1115350",
    "end": "1117360"
  },
  {
    "text": "depth-first search, uh, as a way to explore as much of the network",
    "start": "1117360",
    "end": "1121380"
  },
  {
    "text": "as possible given a given budget of steps starting at node u.",
    "start": "1121380",
    "end": "1125775"
  },
  {
    "text": "But if you really want to get a good understanding how",
    "start": "1125775",
    "end": "1128130"
  },
  {
    "text": "the network looks like very locally around node u,",
    "start": "1128130",
    "end": "1130755"
  },
  {
    "text": "then perhaps you'd want to explore the network more in,",
    "start": "1130755",
    "end": "1133950"
  },
  {
    "text": "uh, um, breadth-first search, uh, fashion.",
    "start": "1133950",
    "end": "1137039"
  },
  {
    "text": "So this is really,",
    "start": "1137040",
    "end": "1138600"
  },
  {
    "text": "um, what this will allow us to do.",
    "start": "1138600",
    "end": "1140220"
  },
  {
    "text": "It will allow us to trade off or kind of extrapolate between breadth-first search, um,",
    "start": "1140220",
    "end": "1145049"
  },
  {
    "text": "and depth-first search, uh, type,",
    "start": "1145050",
    "end": "1147240"
  },
  {
    "text": "um, network exploration, right?",
    "start": "1147240",
    "end": "1150390"
  },
  {
    "text": "Um, uh, as I said, right, like, um,",
    "start": "1150390",
    "end": "1152805"
  },
  {
    "text": "in terms of strategies to explore the network neighborhood,",
    "start": "1152805",
    "end": "1156780"
  },
  {
    "text": "uh, and define the notion o- of, uh,",
    "start": "1156780",
    "end": "1159030"
  },
  {
    "text": "N from a given starting node,",
    "start": "1159030",
    "end": "1161235"
  },
  {
    "text": "you- you could imagine you wanna explore very",
    "start": "1161235",
    "end": "1163830"
  },
  {
    "text": "locally and would give you a very local view of the network,",
    "start": "1163830",
    "end": "1167159"
  },
  {
    "text": "and this will be just kind of breadth-first search exploration.",
    "start": "1167160",
    "end": "1169890"
  },
  {
    "text": "What you'd wanna look, perhaps a depth-first search explanation, right?",
    "start": "1169890",
    "end": "1173010"
  },
  {
    "text": "You wanna have these kind of global, uh,",
    "start": "1173010",
    "end": "1174885"
  },
  {
    "text": "macroscopic view of the network because you",
    "start": "1174885",
    "end": "1177405"
  },
  {
    "text": "capture much longer and larger, uh, distances.",
    "start": "1177405",
    "end": "1181035"
  },
  {
    "start": "1181000",
    "end": "1197000"
  },
  {
    "text": "All right. And that's essentially the intuition behind node2vec is",
    "start": "1181035",
    "end": "1185400"
  },
  {
    "text": "that you can- you can explore the network in different ways and you will get,",
    "start": "1185400",
    "end": "1189450"
  },
  {
    "text": "um, better resolution, uh, you know,",
    "start": "1189450",
    "end": "1192000"
  },
  {
    "text": "at more microscopic view versus more,",
    "start": "1192000",
    "end": "1194460"
  },
  {
    "text": "uh, macroscopic view, uh, of the network.",
    "start": "1194460",
    "end": "1197445"
  },
  {
    "start": "1197000",
    "end": "1252000"
  },
  {
    "text": "So how are we going to now do this in practice?",
    "start": "1197445",
    "end": "1201360"
  },
  {
    "text": "How are we going to define this random walk?",
    "start": "1201360",
    "end": "1203355"
  },
  {
    "text": "We are going to do biased fixed-length random walks are that,",
    "start": "1203355",
    "end": "1208725"
  },
  {
    "text": "that- so that a given node u generates its neighborhood,",
    "start": "1208725",
    "end": "1212025"
  },
  {
    "text": "uh, N, uh, of u.",
    "start": "1212025",
    "end": "1214020"
  },
  {
    "text": "And we are going to have two hyperparameters.",
    "start": "1214020",
    "end": "1216930"
  },
  {
    "text": "We'll have the return parameter p,",
    "start": "1216930",
    "end": "1219150"
  },
  {
    "text": "that will say how likely is the random walk maker step back,",
    "start": "1219150",
    "end": "1223050"
  },
  {
    "text": "backtrack to this- to the previous node.",
    "start": "1223050",
    "end": "1225435"
  },
  {
    "text": "And then we are going to have another parameter q that",
    "start": "1225435",
    "end": "1228900"
  },
  {
    "text": "are- we are going to call, uh, in-out parameter,",
    "start": "1228900",
    "end": "1232470"
  },
  {
    "text": "and it will allow us to trade off between moving outward,",
    "start": "1232470",
    "end": "1235919"
  },
  {
    "text": "kind of doing breadth-first search,",
    "start": "1235920",
    "end": "1237765"
  },
  {
    "text": "versus staying inwards, staying close to the starting node in this way,",
    "start": "1237765",
    "end": "1241860"
  },
  {
    "text": "mimicking, uh, breadth-first search.",
    "start": "1241860",
    "end": "1243975"
  },
  {
    "text": "And intuitively, we can think of q as",
    "start": "1243975",
    "end": "1246720"
  },
  {
    "text": "the ratio between the breadth-first and depth-first,",
    "start": "1246720",
    "end": "1250080"
  },
  {
    "text": "uh, exploration of the network.",
    "start": "1250080",
    "end": "1252149"
  },
  {
    "start": "1252000",
    "end": "1427000"
  },
  {
    "text": "To make this a bit more, uh, precise,",
    "start": "1252150",
    "end": "1254849"
  },
  {
    "text": "this is called a second-order random walk because it remembers where it came from.",
    "start": "1254849",
    "end": "1260835"
  },
  {
    "text": "Um, and then imagine for example in this- in this case that",
    "start": "1260835",
    "end": "1264720"
  },
  {
    "text": "the random walk just came from node S_1 to the node W. And now at W,",
    "start": "1264720",
    "end": "1269684"
  },
  {
    "text": "random walk needs to decide what to do, and there are- you know,",
    "start": "1269685",
    "end": "1273240"
  },
  {
    "text": "it needs to pick a node,",
    "start": "1273240",
    "end": "1274485"
  },
  {
    "text": "and there are actually three things that th- the- that the no- walker can do.",
    "start": "1274485",
    "end": "1278265"
  },
  {
    "text": "It can return back where it came from.",
    "start": "1278265",
    "end": "1280560"
  },
  {
    "text": "It can stay at the same distance,",
    "start": "1280560",
    "end": "1283905"
  },
  {
    "text": "um, uh, from, uh,",
    "start": "1283905",
    "end": "1285390"
  },
  {
    "text": "from where it came as it was before,",
    "start": "1285390",
    "end": "1287250"
  },
  {
    "text": "so you know, it's one hop,",
    "start": "1287250",
    "end": "1288600"
  },
  {
    "text": "our W is one hop from S_1,",
    "start": "1288600",
    "end": "1291000"
  },
  {
    "text": "so S_2 is also one hop from S_1.",
    "start": "1291000",
    "end": "1293580"
  },
  {
    "text": "So this means you stay at the same distance as from S_1 as you were,",
    "start": "1293580",
    "end": "1297779"
  },
  {
    "text": "or you can navigate farther out,",
    "start": "1297780",
    "end": "1300135"
  },
  {
    "text": "meaning navigate to someone- somebody who is at a distance 2 from the previous node S_1.",
    "start": "1300135",
    "end": "1306390"
  },
  {
    "text": "All right? So because we know where the random walker came,",
    "start": "1306390",
    "end": "1309960"
  },
  {
    "text": "the random walker needs to decide,",
    "start": "1309960",
    "end": "1311429"
  },
  {
    "text": "go back, stay at the same orbit,",
    "start": "1311430",
    "end": "1314310"
  },
  {
    "text": "at the same level, Or move one step further?",
    "start": "1314310",
    "end": "1317760"
  },
  {
    "text": "And the way we are going to parameterize this is using parameters p and q.",
    "start": "1317760",
    "end": "1322830"
  },
  {
    "text": "So we- di- if you think of this in terms of an unnormalized probabilities,",
    "start": "1322830",
    "end": "1327480"
  },
  {
    "text": "then we can think that, you know,",
    "start": "1327480",
    "end": "1328995"
  },
  {
    "text": "staying at the same distance,",
    "start": "1328995",
    "end": "1330600"
  },
  {
    "text": "we take this with probability proportional to some constant,",
    "start": "1330600",
    "end": "1334155"
  },
  {
    "text": "we return with probability 1 over p1,",
    "start": "1334155",
    "end": "1336930"
  },
  {
    "text": "and then we move farther away with probability one over",
    "start": "1336930",
    "end": "1340605"
  },
  {
    "text": "q one or proportional with- to- to 1 over q1, all right?",
    "start": "1340605",
    "end": "1344580"
  },
  {
    "text": "So here p is the return parameter,",
    "start": "1344580",
    "end": "1347355"
  },
  {
    "text": "and q is walk away type parameter.",
    "start": "1347355",
    "end": "1351165"
  },
  {
    "text": "So how are we going now to do this in- in practice is essentially,",
    "start": "1351165",
    "end": "1355844"
  },
  {
    "text": "as the random walker,",
    "start": "1355844",
    "end": "1357120"
  },
  {
    "text": "let's say goes from S_1 to the W,",
    "start": "1357120",
    "end": "1359309"
  },
  {
    "text": "now it needs to decide where to go.",
    "start": "1359310",
    "end": "1361275"
  },
  {
    "text": "We are going to have this unnormalized probability distribution of transitions,",
    "start": "1361275",
    "end": "1365940"
  },
  {
    "text": "which neighbor of W to navigate to.",
    "start": "1365940",
    "end": "1369120"
  },
  {
    "text": "We are going to normalize these to sum to one and then",
    "start": "1369120",
    "end": "1372360"
  },
  {
    "text": "flip a biased coin that will- that will navigate,",
    "start": "1372360",
    "end": "1375585"
  },
  {
    "text": "that will pick one of these four possible options, right?",
    "start": "1375585",
    "end": "1378434"
  },
  {
    "text": "Returning back, staying at the same distance, or navigating further.",
    "start": "1378435",
    "end": "1382980"
  },
  {
    "text": "And now, for example,",
    "start": "1382980",
    "end": "1384450"
  },
  {
    "text": "if I set a low value of p,",
    "start": "1384450",
    "end": "1387255"
  },
  {
    "text": "then this first term will be very high and the random walk will most likely return.",
    "start": "1387255",
    "end": "1392220"
  },
  {
    "text": "Uh, if we, uh,",
    "start": "1392220",
    "end": "1394215"
  },
  {
    "text": "want to navigate farther away,",
    "start": "1394215",
    "end": "1396255"
  },
  {
    "text": "we set a low value of q,",
    "start": "1396255",
    "end": "1398325"
  },
  {
    "text": "which means that S_3 and S_4,",
    "start": "1398325",
    "end": "1400365"
  },
  {
    "text": "will- will get a lot of, uh, probabilityness.",
    "start": "1400365",
    "end": "1403530"
  },
  {
    "text": "Um, and that's, uh, that's basically the idea.",
    "start": "1403530",
    "end": "1406575"
  },
  {
    "text": "And then again, the- the set n will be defined",
    "start": "1406575",
    "end": "1409440"
  },
  {
    "text": "by the nodes visited by this biased random walk",
    "start": "1409440",
    "end": "1412889"
  },
  {
    "text": "that is trading off the exploration of farther",
    "start": "1412890",
    "end": "1416130"
  },
  {
    "text": "out in the neigh- neighborhood versus exploration close,",
    "start": "1416130",
    "end": "1419430"
  },
  {
    "text": "uh, to the- to the- to the starting node,",
    "start": "1419430",
    "end": "1421935"
  },
  {
    "text": "um, S_1 in this case.",
    "start": "1421935",
    "end": "1423570"
  },
  {
    "text": "So that is, um,",
    "start": "1423570",
    "end": "1424904"
  },
  {
    "text": "that is the- that is the idea.",
    "start": "1424905",
    "end": "1427095"
  },
  {
    "start": "1427000",
    "end": "1490000"
  },
  {
    "text": "Um, so how does the algorithm work?",
    "start": "1427095",
    "end": "1429825"
  },
  {
    "text": "We are going to compute the random walk probabilities first.",
    "start": "1429825",
    "end": "1433274"
  },
  {
    "text": "Then we are going to simulate, the r, um,",
    "start": "1433275",
    "end": "1436695"
  },
  {
    "text": "biased random walks of some fixed length l starting from each node u.",
    "start": "1436695",
    "end": "1441825"
  },
  {
    "text": "And then we are going to, uh,",
    "start": "1441825",
    "end": "1443580"
  },
  {
    "text": "optimize the, uh, objective function, uh,",
    "start": "1443580",
    "end": "1446715"
  },
  {
    "text": "the same negative sampling objective function",
    "start": "1446715",
    "end": "1449039"
  },
  {
    "text": "that I- that I already discussed in DeepWalk,",
    "start": "1449040",
    "end": "1451410"
  },
  {
    "text": "uh, using stochastic gradient descent.",
    "start": "1451410",
    "end": "1453855"
  },
  {
    "text": "Um, the beauty in this,",
    "start": "1453855",
    "end": "1455955"
  },
  {
    "text": "is that there is linear time complexity in the optimization.",
    "start": "1455955",
    "end": "1459825"
  },
  {
    "text": "Because for every node,",
    "start": "1459825",
    "end": "1461220"
  },
  {
    "text": "we have a fixed set of random walks.",
    "start": "1461220",
    "end": "1463184"
  },
  {
    "text": "So it's linear, uh,",
    "start": "1463185",
    "end": "1464865"
  },
  {
    "text": "in the size o- of the graph.",
    "start": "1464865",
    "end": "1466950"
  },
  {
    "text": "And the- all these different three steps are also parallelizable.",
    "start": "1466950",
    "end": "1470894"
  },
  {
    "text": "So can- you can run them- uh, in parallel.",
    "start": "1470895",
    "end": "1473310"
  },
  {
    "text": "The drawback of this, uh,",
    "start": "1473310",
    "end": "1474930"
  },
  {
    "text": "no demanding approaches, uh,",
    "start": "1474930",
    "end": "1476880"
  },
  {
    "text": "is that we need to learn a separate,",
    "start": "1476880",
    "end": "1480000"
  },
  {
    "text": "uh, embedding, uh, for every node, uh, individually.",
    "start": "1480000",
    "end": "1483255"
  },
  {
    "text": "So with a bigger networks,",
    "start": "1483255",
    "end": "1485370"
  },
  {
    "text": "we need to learn, uh,",
    "start": "1485370",
    "end": "1486795"
  },
  {
    "text": "bigger, uh, embeddings, or more embeddings.",
    "start": "1486795",
    "end": "1490805"
  },
  {
    "start": "1490000",
    "end": "1546000"
  },
  {
    "text": "Um, of course, there's- there has been a lot of work, um,",
    "start": "1490805",
    "end": "1494705"
  },
  {
    "text": "after these, uh, these initial,",
    "start": "1494705",
    "end": "1497269"
  },
  {
    "text": "uh, papers that have proposed these ideas,",
    "start": "1497270",
    "end": "1499775"
  },
  {
    "text": "there are different kinds, uh,",
    "start": "1499775",
    "end": "1501440"
  },
  {
    "text": "of random walks that people kept proposed,",
    "start": "1501440",
    "end": "1503855"
  },
  {
    "text": "that our alternative optimization schemes, um,",
    "start": "1503855",
    "end": "1506960"
  },
  {
    "text": "and also different network pre-processing techniques, uh,",
    "start": "1506960",
    "end": "1510289"
  },
  {
    "text": "that allow us to define different notions, uh, of similarity.",
    "start": "1510290",
    "end": "1514310"
  },
  {
    "text": "Here are some papers that I linked,",
    "start": "1514310",
    "end": "1515930"
  },
  {
    "text": "uh, you know, if you are interested,",
    "start": "1515930",
    "end": "1517760"
  },
  {
    "text": "curious to learn more, um,",
    "start": "1517760",
    "end": "1519945"
  },
  {
    "text": "please, uh, please read them,",
    "start": "1519945",
    "end": "1521534"
  },
  {
    "text": "it will be a very good read.",
    "start": "1521535",
    "end": "1523320"
  },
  {
    "text": "So let me summarize what we have learned so far.",
    "start": "1523320",
    "end": "1527774"
  },
  {
    "text": "So the core idea was to embed nodes.",
    "start": "1527775",
    "end": "1530100"
  },
  {
    "text": "So the distances in the embedding space",
    "start": "1530100",
    "end": "1532169"
  },
  {
    "text": "reflect node similarities in the original network.",
    "start": "1532170",
    "end": "1535275"
  },
  {
    "text": "Um, and we talked about two different notions of node similarity.",
    "start": "1535275",
    "end": "1539250"
  },
  {
    "text": "Uh, first one was naive similarity where,",
    "start": "1539250",
    "end": "1541920"
  },
  {
    "text": "uh, if two node-,",
    "start": "1541920",
    "end": "1542940"
  },
  {
    "text": "if we could- for example,",
    "start": "1542940",
    "end": "1544440"
  },
  {
    "text": "do, uh, connect, uh,",
    "start": "1544440",
    "end": "1545669"
  },
  {
    "text": "make notes, uh, close together if they are simply connected by an edge.",
    "start": "1545670",
    "end": "1549135"
  },
  {
    "start": "1546000",
    "end": "1627000"
  },
  {
    "text": "We could, uh, do,",
    "start": "1549135",
    "end": "1550410"
  },
  {
    "text": "a neighborhood similarity, um,",
    "start": "1550410",
    "end": "1553110"
  },
  {
    "text": "and today we talked about random walk approaches, uh,",
    "start": "1553110",
    "end": "1556095"
  },
  {
    "text": "to, uh, node similarity where we said,",
    "start": "1556095",
    "end": "1558525"
  },
  {
    "text": "all the nodes visited on a random walk from a starting node,",
    "start": "1558525",
    "end": "1562080"
  },
  {
    "text": "those are, uh, similar to it.",
    "start": "1562080",
    "end": "1564299"
  },
  {
    "text": "So that's, uh, essentially the idea,",
    "start": "1564300",
    "end": "1566640"
  },
  {
    "text": "uh, for- uh, for today.",
    "start": "1566640",
    "end": "1569130"
  },
  {
    "text": "So, uh, now of course the question is,",
    "start": "1569130",
    "end": "1571830"
  },
  {
    "text": "which method should you use?",
    "start": "1571830",
    "end": "1573630"
  },
  {
    "text": "Um, and no method wins all cases.",
    "start": "1573630",
    "end": "1576585"
  },
  {
    "text": "So for example, node2vec performs better on a node classification,",
    "start": "1576585",
    "end": "1580815"
  },
  {
    "text": "while for example, a link prediction,",
    "start": "1580815",
    "end": "1582929"
  },
  {
    "text": "some alternative methods may perform better.",
    "start": "1582930",
    "end": "1585240"
  },
  {
    "text": "Uh, there is a very nice survey,",
    "start": "1585240",
    "end": "1587309"
  },
  {
    "text": "um, three years ago by Goyal and Ferrara.",
    "start": "1587310",
    "end": "1590520"
  },
  {
    "text": "That, um, surveyed many of these methods and compare them on a number of different tasks.",
    "start": "1590520",
    "end": "1596310"
  },
  {
    "text": "Um, er, and generally, you know,",
    "start": "1596310",
    "end": "1599340"
  },
  {
    "text": "random walk approaches are, uh,",
    "start": "1599340",
    "end": "1601304"
  },
  {
    "text": "quite efficient because you can simulate,",
    "start": "1601305",
    "end": "1603705"
  },
  {
    "text": "uh, a limited number of random walks.",
    "start": "1603705",
    "end": "1605669"
  },
  {
    "text": "They don't necessarily scale to the super big networks,",
    "start": "1605670",
    "end": "1608910"
  },
  {
    "text": "but they scale to lar- to rel- let say medium-size network.",
    "start": "1608910",
    "end": "1613020"
  },
  {
    "text": "Um , and, uh, in general, right?",
    "start": "1613020",
    "end": "1615750"
  },
  {
    "text": "You must choose the definition of node similarity",
    "start": "1615750",
    "end": "1618225"
  },
  {
    "text": "that best matches, uh, your application.",
    "start": "1618225",
    "end": "1622239"
  }
]