[
  {
    "start": "0",
    "end": "127000"
  },
  {
    "text": "So the next topic is\ncalled bagging or bootstrap",
    "start": "0",
    "end": "4190"
  },
  {
    "text": "aggregation, which is\na way of using trees",
    "start": "4190",
    "end": "7220"
  },
  {
    "text": "in an ensemble, an\naverage of trees",
    "start": "7220",
    "end": "8990"
  },
  {
    "text": "to improve their\nprediction error.",
    "start": "8990",
    "end": "10620"
  },
  {
    "text": "And as we'll see that\nthe bagging methods,",
    "start": "10620",
    "end": "12559"
  },
  {
    "text": "well, I'll mention the bagging\nmethod is due to Leo Breiman,",
    "start": "12560",
    "end": "15500"
  },
  {
    "text": "and it's a bootstrap process.",
    "start": "15500",
    "end": "17670"
  },
  {
    "text": "And I remember first\ntime I saw this,",
    "start": "17670",
    "end": "20090"
  },
  {
    "text": "what would that been maybe in\nthe mid-'90s, I knew a lot about",
    "start": "20090",
    "end": "23917"
  },
  {
    "text": "the bootstrap.",
    "start": "23917",
    "end": "24500"
  },
  {
    "text": "Actually, I was a\nstudent of Brad Efron",
    "start": "24500",
    "end": "26165"
  },
  {
    "text": "who invented the bootstrap.",
    "start": "26165",
    "end": "27290"
  },
  {
    "text": "And Brad and I wrote a book\ntogether on the bootstrap",
    "start": "27290",
    "end": "29498"
  },
  {
    "text": "in the early '90s.",
    "start": "29498",
    "end": "31220"
  },
  {
    "text": "And then when I saw the\nbagging from Leo, I thought,",
    "start": "31220",
    "end": "34670"
  },
  {
    "text": "this looks really crazy.",
    "start": "34670",
    "end": "36649"
  },
  {
    "text": "Usually the bootstrap is used to\nget an idea of standard errors",
    "start": "36650",
    "end": "39590"
  },
  {
    "text": "or bias, but Leo\nwants to use Bootstrap",
    "start": "39590",
    "end": "42260"
  },
  {
    "text": "to produce a whole\nbunch of trees",
    "start": "42260",
    "end": "44239"
  },
  {
    "text": "and to average them, which\nsounded really crazy to me.",
    "start": "44240",
    "end": "46620"
  },
  {
    "text": "And it was a reminder\nto me that, when",
    "start": "46620",
    "end": "48320"
  },
  {
    "text": "you see an idea that\nlooks really crazy,",
    "start": "48320",
    "end": "50180"
  },
  {
    "text": "it's got a reasonable chance\nof actually being really good.",
    "start": "50180",
    "end": "52650"
  },
  {
    "text": "If things look very\nfamiliar, they're not",
    "start": "52650",
    "end": "55220"
  },
  {
    "text": "likely to be big steps forward.",
    "start": "55220",
    "end": "56570"
  },
  {
    "text": "This was a big step forward,\nand it took me and others",
    "start": "56570",
    "end": "58820"
  },
  {
    "text": "a long time to realize that.",
    "start": "58820",
    "end": "60660"
  },
  {
    "text": "And bagging, and you'll\nsee random forests",
    "start": "60660",
    "end": "63420"
  },
  {
    "text": "that we're going to talk about\nas well, they used all over.",
    "start": "63420",
    "end": "66850"
  },
  {
    "text": "And I just was at a\ntalk the other day,",
    "start": "66850",
    "end": "69450"
  },
  {
    "text": "where random forest was the\nprimary tool used in a big image",
    "start": "69450",
    "end": "73770"
  },
  {
    "text": "classification problem, using\nstate-of-the-art methods,",
    "start": "73770",
    "end": "77430"
  },
  {
    "text": "and random forest was\nat the heart of it.",
    "start": "77430",
    "end": "80430"
  },
  {
    "text": "So let's see what\nthe bag idea is.",
    "start": "80430",
    "end": "81980"
  },
  {
    "text": "So it's a way of taking a bunch\nof independent observations,",
    "start": "81980",
    "end": "88920"
  },
  {
    "text": "and taking their average\nto reduce variance.",
    "start": "88920",
    "end": "91077"
  },
  {
    "text": "So just recall, if we had\na bunch of observation,",
    "start": "91078",
    "end": "93120"
  },
  {
    "text": "Z1 through Zn, the\nindependent observations",
    "start": "93120",
    "end": "95190"
  },
  {
    "text": "with variance sigma squared,\nthat if we take their average,",
    "start": "95190",
    "end": "98070"
  },
  {
    "text": "that the variance of the mean\ny bar is sigma squared over n.",
    "start": "98070",
    "end": "102780"
  },
  {
    "text": "So by taking an average\nof independent things,",
    "start": "102780",
    "end": "104970"
  },
  {
    "text": "we reduce the variance by n.",
    "start": "104970",
    "end": "107700"
  },
  {
    "text": "That's good.",
    "start": "107700",
    "end": "108689"
  },
  {
    "text": "So what does that say to us in\nterms of supervised learning,",
    "start": "108690",
    "end": "112000"
  },
  {
    "text": "it says if we had more\nthan one training set.",
    "start": "112000",
    "end": "114090"
  },
  {
    "text": "We could grow, for example,\na tree on each training set,",
    "start": "114090",
    "end": "117359"
  },
  {
    "text": "and take the average tree.",
    "start": "117360",
    "end": "118740"
  },
  {
    "text": "Now, that doesn't\nreally help us probably,",
    "start": "118740",
    "end": "120895"
  },
  {
    "text": "because we don't have\nmultiple training sets.",
    "start": "120895",
    "end": "122770"
  },
  {
    "text": "Let's assume we just have our\none available training set.",
    "start": "122770",
    "end": "126850"
  },
  {
    "text": "Well, bagging is going to\ntry to achieve this now",
    "start": "126850",
    "end": "132090"
  },
  {
    "start": "127000",
    "end": "283000"
  },
  {
    "text": "without new training\nsets, by taking bootstrap",
    "start": "132090",
    "end": "134190"
  },
  {
    "text": "samples to create\npseudo training sets,",
    "start": "134190",
    "end": "137460"
  },
  {
    "text": "grow a tree on each one,\nand then take their average.",
    "start": "137460",
    "end": "140190"
  },
  {
    "text": "That's the simple, but I think,\nvery clever idea of bagging.",
    "start": "140190",
    "end": "143880"
  },
  {
    "text": "So we're going to take bootstrap\nsamples from our training",
    "start": "143880",
    "end": "148025"
  },
  {
    "text": "set of the same size as\nour original sample, that's",
    "start": "148025",
    "end": "150150"
  },
  {
    "text": "the usual bootstrap.",
    "start": "150150",
    "end": "151870"
  },
  {
    "text": "And maybe draw a\nlittle picture here",
    "start": "151870",
    "end": "154599"
  },
  {
    "text": "just so we can think about it.",
    "start": "154600",
    "end": "156830"
  },
  {
    "text": "Here's our original\ntraining set.",
    "start": "156830",
    "end": "158470"
  },
  {
    "text": "We're going to draw\nbootstrap samples.",
    "start": "158470",
    "end": "160060"
  },
  {
    "text": "Again, the samples taken with\nreplacement of the same size",
    "start": "160060",
    "end": "163030"
  },
  {
    "text": "from the original data set.",
    "start": "163030",
    "end": "165040"
  },
  {
    "text": "And we'll take lots of\nthem, maybe a few hundred.",
    "start": "165040",
    "end": "168819"
  },
  {
    "text": "And for each one,\nwe're going to grow",
    "start": "168820",
    "end": "171100"
  },
  {
    "text": "a tree using, for example, the\nprocedure we just went over.",
    "start": "171100",
    "end": "177328"
  },
  {
    "text": "On each of these\ntraining sets. we're",
    "start": "177328",
    "end": "178870"
  },
  {
    "text": "going to get a little tree.",
    "start": "178870",
    "end": "180049"
  },
  {
    "text": "They're actually\nquite big trees.",
    "start": "180050",
    "end": "183900"
  },
  {
    "text": "Big trees, that's right.",
    "start": "183900",
    "end": "185010"
  },
  {
    "text": "So little tree, but\nactually, in fact,",
    "start": "185010",
    "end": "186790"
  },
  {
    "text": "turns out We don't need to\nworry about pruning the trees,",
    "start": "186790",
    "end": "189165"
  },
  {
    "text": "we can grow large trees\nand when we average them,",
    "start": "189165",
    "end": "191903"
  },
  {
    "text": "we'll reduce\nvariance, so we don't",
    "start": "191903",
    "end": "193320"
  },
  {
    "text": "need to prune even, that's\nanother happy coincidence.",
    "start": "193320",
    "end": "195528"
  },
  {
    "text": "So notation-wise,\nwe'll call the tree",
    "start": "195528",
    "end": "198690"
  },
  {
    "text": "that we grow on the B tree,\nagain, B going from maybe",
    "start": "198690",
    "end": "201510"
  },
  {
    "text": "1 to 200, f hat, star b of x.",
    "start": "201510",
    "end": "204599"
  },
  {
    "text": "So the prediction that\nfeature x for the B tree,",
    "start": "204600",
    "end": "209390"
  },
  {
    "text": "we'll call f hat star B of x.",
    "start": "209390",
    "end": "211520"
  },
  {
    "text": "Our overall bagged estimate\nprediction is just the average.",
    "start": "211520",
    "end": "215180"
  },
  {
    "text": "So in other words, we grow\nthese trees, we have a new x.",
    "start": "215180",
    "end": "218780"
  },
  {
    "text": "And we want a predicted x.",
    "start": "218780",
    "end": "220310"
  },
  {
    "text": "We ask each tree, what's\nyour prediction for x?",
    "start": "220310",
    "end": "224750"
  },
  {
    "text": "And that's f hat star B of x,\nand we average the predictions,",
    "start": "224750",
    "end": "228350"
  },
  {
    "text": "and that's called bagging.",
    "start": "228350",
    "end": "229790"
  },
  {
    "text": "It's a really clever idea.",
    "start": "229790",
    "end": "231430"
  },
  {
    "text": "So the old idea of\npruning a tree back,",
    "start": "231430",
    "end": "234650"
  },
  {
    "text": "was to reduce the variance,\nyou pruned it back.",
    "start": "234650",
    "end": "237045"
  },
  {
    "text": "But when you prune\nit back, you give it",
    "start": "237045",
    "end": "238670"
  },
  {
    "text": "a lot of bias, because\nyou make it necessarily",
    "start": "238670",
    "end": "241160"
  },
  {
    "text": "just a much coarser tree.",
    "start": "241160",
    "end": "242740"
  },
  {
    "text": "So Breiman's idea\nwas, don't prune back,",
    "start": "242740",
    "end": "246440"
  },
  {
    "text": "have bushy trees which\ntend to have low bias,",
    "start": "246440",
    "end": "248990"
  },
  {
    "text": "and get rid of the variance\nby doing this averaging.",
    "start": "248990",
    "end": "251702"
  },
  {
    "text": "And we can apply this to\nregression or classification",
    "start": "251702",
    "end": "253909"
  },
  {
    "text": "here.",
    "start": "253910",
    "end": "254190"
  },
  {
    "text": "I've basically implicitly\nassumed we're doing regression,",
    "start": "254190",
    "end": "255920"
  },
  {
    "text": "so that the\nprediction is actually",
    "start": "255920",
    "end": "257337"
  },
  {
    "text": "a quantitative variable.",
    "start": "257337",
    "end": "258560"
  },
  {
    "text": "But when doing classification,\nwe can just do a majority vote.",
    "start": "258560",
    "end": "261389"
  },
  {
    "text": "So we can have 200 trees.",
    "start": "261390",
    "end": "264890"
  },
  {
    "text": "So we have two classes, we\nmake a prediction at each x,",
    "start": "264890",
    "end": "268790"
  },
  {
    "text": "and we ask each tree, what's\nyour class prediction?",
    "start": "268790",
    "end": "271680"
  },
  {
    "text": "and suppose, of the 200, 150,\nsay class I, and other 50",
    "start": "271680",
    "end": "275120"
  },
  {
    "text": "say class 2, then our\nprediction is class 1.",
    "start": "275120",
    "end": "277802"
  },
  {
    "text": "So we can just take a majority\nvote among the B trees",
    "start": "277802",
    "end": "280010"
  },
  {
    "text": "that we grow.",
    "start": "280010",
    "end": "282330"
  },
  {
    "text": "So here's actually\nthe heart data.",
    "start": "282330",
    "end": "285150"
  },
  {
    "start": "283000",
    "end": "447000"
  },
  {
    "text": "This is the result of\nbagging the heart data.",
    "start": "285150",
    "end": "287040"
  },
  {
    "text": "So along the horizontal\naxis is the number",
    "start": "287040",
    "end": "290090"
  },
  {
    "text": "of trees, which we grew up\nto 300 trees in this case.",
    "start": "290090",
    "end": "292960"
  },
  {
    "text": "And what do we\nsee here, well, we",
    "start": "292960",
    "end": "297278"
  },
  {
    "text": "have the test error\nof bagging, first",
    "start": "297278",
    "end": "298820"
  },
  {
    "text": "of all, where we had a\ntest set we set aside.",
    "start": "298820",
    "end": "301190"
  },
  {
    "text": "And it's the black curve, and\nits error is around a little",
    "start": "301190",
    "end": "307700"
  },
  {
    "text": "over 0.25.",
    "start": "307700",
    "end": "309643"
  },
  {
    "text": "If we go over the\nleft here, we'll",
    "start": "309643",
    "end": "311060"
  },
  {
    "text": "see actually the result\nof a single tree.",
    "start": "311060",
    "end": "313220"
  },
  {
    "text": "So in this case, bagging\nimproved a single tree,",
    "start": "313220",
    "end": "315740"
  },
  {
    "text": "maybe just by 1% error.",
    "start": "315740",
    "end": "317639"
  },
  {
    "text": "Actually, the dotted\nline is a single tree.",
    "start": "317640",
    "end": "319550"
  },
  {
    "text": "OK, thank you.",
    "start": "319550",
    "end": "320190"
  },
  {
    "text": "So the [INAUDIBLE] tree.",
    "start": "320190",
    "end": "321030"
  },
  {
    "text": "So it looks like we may\nhave improved the prediction",
    "start": "321030",
    "end": "323197"
  },
  {
    "text": "error by perhaps 1, not a\ngreat win in this one case.",
    "start": "323197",
    "end": "327440"
  },
  {
    "text": "The random forest, which I'll\ntalk about in a few slides,",
    "start": "327440",
    "end": "331790"
  },
  {
    "text": "it's a more advanced\nform of bagging",
    "start": "331790",
    "end": "333920"
  },
  {
    "text": "where we do another\ntrick of Beiman's that's",
    "start": "333920",
    "end": "337760"
  },
  {
    "text": "going to prove things here by\nmaybe another couple of percent.",
    "start": "337760",
    "end": "340840"
  },
  {
    "text": "So in this figure, we've\nalso got two error curves",
    "start": "340840",
    "end": "343960"
  },
  {
    "text": "called out-of-bag, both for\nbagging and random forest.",
    "start": "343960",
    "end": "347930"
  },
  {
    "text": "We'll describe it\nfor bagging now.",
    "start": "347930",
    "end": "350810"
  },
  {
    "text": "So the out-of-bag area's\na very important part",
    "start": "350810",
    "end": "353560"
  },
  {
    "text": "of bagging and random forest.",
    "start": "353560",
    "end": "355250"
  },
  {
    "text": "And it turns out it's\na essentially free way",
    "start": "355250",
    "end": "358150"
  },
  {
    "text": "of computing leave-one-out\ncross-validation.",
    "start": "358150",
    "end": "360910"
  },
  {
    "text": "And it works as follows.",
    "start": "360910",
    "end": "362800"
  },
  {
    "text": "Each of this bootstrap\nsamples includes about 2/3",
    "start": "362800",
    "end": "368470"
  },
  {
    "text": "of the observations, and about\none-third of the observations",
    "start": "368470",
    "end": "371770"
  },
  {
    "text": "are left out.",
    "start": "371770",
    "end": "372940"
  },
  {
    "text": "And of course, for\neach bootstrap sample,",
    "start": "372940",
    "end": "374920"
  },
  {
    "text": "a different one\nthird is left out.",
    "start": "374920",
    "end": "376820"
  },
  {
    "text": "So suppose we want\nto compute the error",
    "start": "376820",
    "end": "378970"
  },
  {
    "text": "for a particular\nobservation, well, we",
    "start": "378970",
    "end": "381970"
  },
  {
    "text": "can take all the\nbootstrap samples that",
    "start": "381970",
    "end": "385000"
  },
  {
    "text": "didn't include that\nobservation, and we just",
    "start": "385000",
    "end": "387460"
  },
  {
    "text": "average their predictions\nfor that observation,",
    "start": "387460",
    "end": "390250"
  },
  {
    "text": "and use them to predict\nthat observation.",
    "start": "390250",
    "end": "392690"
  },
  {
    "text": "Well, that observation\nwouldn't have",
    "start": "392690",
    "end": "395140"
  },
  {
    "text": "been in any of those\ntraining samples,",
    "start": "395140",
    "end": "397880"
  },
  {
    "text": "and so that's a left\nout error estimate.",
    "start": "397880",
    "end": "402290"
  },
  {
    "text": "And so you can just\naccumulate those predictions",
    "start": "402290",
    "end": "405070"
  },
  {
    "text": "in that fashion for each\nof the observations,",
    "start": "405070",
    "end": "407450"
  },
  {
    "text": "and that's called\nthe out-of-bag error.",
    "start": "407450",
    "end": "409780"
  },
  {
    "text": "And if you think about it\ncarefully, if B is large,",
    "start": "409780",
    "end": "412790"
  },
  {
    "text": "the number of trees is\nlarge, that's essentially",
    "start": "412790",
    "end": "414790"
  },
  {
    "text": "leave-one-out cross-validation,\nand it just comes for free.",
    "start": "414790",
    "end": "417950"
  },
  {
    "text": "And so that's what was\ngiven in this slide here.",
    "start": "417950",
    "end": "421800"
  },
  {
    "text": "So for bagging, that's\nthe green curve.",
    "start": "421800",
    "end": "424629"
  },
  {
    "text": "And it's quite a bit\nlower than the black curve",
    "start": "424630",
    "end": "428860"
  },
  {
    "text": "above, which is the error on\nthe test set, the green curve.",
    "start": "428860",
    "end": "434167"
  },
  {
    "text": "And of course, there's\nvariability in these,",
    "start": "434167",
    "end": "436000"
  },
  {
    "text": "because these are just\nsamples of numbers,",
    "start": "436000",
    "end": "438440"
  },
  {
    "text": "and so that variability is just\ndue to the division of the test",
    "start": "438440",
    "end": "442610"
  },
  {
    "text": "samples and the\ntraining samples.",
    "start": "442610",
    "end": "443985"
  },
  {
    "start": "443985",
    "end": "446849"
  },
  {
    "text": "So the next idea\nthat Leo Breiman",
    "start": "446850",
    "end": "449460"
  },
  {
    "start": "447000",
    "end": "593000"
  },
  {
    "text": "had a few years after bagging,\nwas called random forests.",
    "start": "449460",
    "end": "453990"
  },
  {
    "text": "And the idea here is that\nwe're taking an average,",
    "start": "453990",
    "end": "456419"
  },
  {
    "text": "and an average would be lower\nif the things being averaged",
    "start": "456420",
    "end": "459270"
  },
  {
    "text": "have lower correlation.",
    "start": "459270",
    "end": "460759"
  },
  {
    "text": "So the idea of random forest\nis to build trees in such a way",
    "start": "460760",
    "end": "464580"
  },
  {
    "text": "as to actually make the\ncorrelation between trees",
    "start": "464580",
    "end": "467310"
  },
  {
    "text": "smaller.",
    "start": "467310",
    "end": "468360"
  },
  {
    "text": "Even smaller than you\nget from bootstrapping.",
    "start": "468360",
    "end": "471159"
  },
  {
    "text": "Exactly.",
    "start": "471160",
    "end": "472455"
  },
  {
    "text": "And, again, this is one\nof these ideas which,",
    "start": "472455",
    "end": "474330"
  },
  {
    "text": "when I first saw it, at\nleast look really bizarre,",
    "start": "474330",
    "end": "477090"
  },
  {
    "text": "but it's a very clever thing.",
    "start": "477090",
    "end": "479230"
  },
  {
    "text": "So what is that idea?",
    "start": "479230",
    "end": "480545"
  },
  {
    "text": "Well, the idea\nis, is we're going",
    "start": "480545",
    "end": "481920"
  },
  {
    "text": "to do bagging exactly\nthe way we did before,",
    "start": "481920",
    "end": "484170"
  },
  {
    "text": "except with one change.",
    "start": "484170",
    "end": "485592"
  },
  {
    "text": "When we build the\ntrees and we make",
    "start": "485592",
    "end": "487050"
  },
  {
    "text": "splits, every time we\nconsider a split in a tree,",
    "start": "487050",
    "end": "490770"
  },
  {
    "text": "we don't consider all\npossible predictors,",
    "start": "490770",
    "end": "493139"
  },
  {
    "text": "as we normally do with trees,\nbut rather we select at random m",
    "start": "493140",
    "end": "499200"
  },
  {
    "text": "predictors among the p.",
    "start": "499200",
    "end": "500520"
  },
  {
    "text": "And the m is say, typically,\nabout the square root of p.",
    "start": "500520",
    "end": "502952"
  },
  {
    "text": "So in other words, if there's\n100 predictors in our data m,",
    "start": "502952",
    "end": "505410"
  },
  {
    "text": "every time we go\nto make a split,",
    "start": "505410",
    "end": "507010"
  },
  {
    "text": "we don't consider all\n100, but rather we",
    "start": "507010",
    "end": "509110"
  },
  {
    "text": "take a random subset\nof 10 of them.",
    "start": "509110",
    "end": "511810"
  },
  {
    "text": "And the other 99\nconsidered, only",
    "start": "511810",
    "end": "514630"
  },
  {
    "text": "the 10 are even allowed to\nbe considered for the splits.",
    "start": "514630",
    "end": "517000"
  },
  {
    "text": "And a new selection is made\nat every possible split.",
    "start": "517000",
    "end": "523030"
  },
  {
    "text": "So again, it seems\ncrazy to throw away",
    "start": "523030",
    "end": "524920"
  },
  {
    "text": "most of the predictors\nat every split.",
    "start": "524920",
    "end": "527139"
  },
  {
    "text": "But the effect of\ndoing this, is that it",
    "start": "527140",
    "end": "530170"
  },
  {
    "text": "forces the trees to use\ndifferent predictors",
    "start": "530170",
    "end": "533649"
  },
  {
    "text": "to split at different times.",
    "start": "533650",
    "end": "535540"
  },
  {
    "text": "And since we're going\nto build lots of trees,",
    "start": "535540",
    "end": "538000"
  },
  {
    "text": "although if a good predictor\nis left out at a given split,",
    "start": "538000",
    "end": "540430"
  },
  {
    "text": "it's going to have lots\nof chances in that tree",
    "start": "540430",
    "end": "542410"
  },
  {
    "text": "or in other trees to\nmake an appearance.",
    "start": "542410",
    "end": "544730"
  },
  {
    "text": "So although it seems\ncrazy to throw away,",
    "start": "544730",
    "end": "547240"
  },
  {
    "text": "in the sense, most of your\npredictors at every stage,",
    "start": "547240",
    "end": "550750"
  },
  {
    "text": "since we're going to build\na large number of trees,",
    "start": "550750",
    "end": "553480"
  },
  {
    "text": "and then take the average, it\nactually works out very well.",
    "start": "553480",
    "end": "556100"
  },
  {
    "text": "So even with the same training\nsample, if you grow two trees,",
    "start": "556100",
    "end": "560680"
  },
  {
    "text": "you'll get two different\ntrees, because by chance it'll",
    "start": "560680",
    "end": "563230"
  },
  {
    "text": "pick different\nvariables each time.",
    "start": "563230",
    "end": "565480"
  },
  {
    "text": "So let's see how random\nforest does in the heart data.",
    "start": "565480",
    "end": "572060"
  },
  {
    "text": "So you remember, the bagging\ntest error was about here,",
    "start": "572060",
    "end": "574900"
  },
  {
    "text": "and random forest improves\nby maybe 1% or 2%,",
    "start": "574900",
    "end": "579380"
  },
  {
    "text": "So again, by doing this trick\nof throwing away predictors,",
    "start": "579380",
    "end": "583180"
  },
  {
    "text": "we've decorrelated the trees\nand the resulting average",
    "start": "583180",
    "end": "585430"
  },
  {
    "text": "seems to do a little better,\nand the out-of-bag estimate",
    "start": "585430",
    "end": "588279"
  },
  {
    "text": "is also a little better\nthan it was for bagging.",
    "start": "588280",
    "end": "592690"
  },
  {
    "text": "Our next example is a\nhigh-dimensional example",
    "start": "592690",
    "end": "595210"
  },
  {
    "start": "593000",
    "end": "826000"
  },
  {
    "text": "from Chapter 8 on\ngene expression.",
    "start": "595210",
    "end": "597110"
  },
  {
    "text": "So this is using the gene\nexpression from 349 patients,",
    "start": "597110",
    "end": "600980"
  },
  {
    "text": "these are cancer patients.",
    "start": "600980",
    "end": "603139"
  },
  {
    "text": "The gene expression of 4,718\ngenes to predict their cancer",
    "start": "603140",
    "end": "606110"
  },
  {
    "text": "class.",
    "start": "606110",
    "end": "607790"
  },
  {
    "text": "And this is a common activity\namong people in statistics",
    "start": "607790",
    "end": "610370"
  },
  {
    "text": "and genomics to try\nto do classification",
    "start": "610370",
    "end": "612980"
  },
  {
    "text": "into things like cancer\nclass, based on the gene",
    "start": "612980",
    "end": "615649"
  },
  {
    "text": "expression of genes in cells.",
    "start": "615650",
    "end": "617620"
  },
  {
    "text": "So in this case, there's\nactually 15 classes,",
    "start": "617620",
    "end": "619740"
  },
  {
    "text": "the one class is\nhealthy or normal,",
    "start": "619740",
    "end": "621740"
  },
  {
    "text": "and then there's 14\ndifferent types of cancer.",
    "start": "621740",
    "end": "624230"
  },
  {
    "text": "So we're going to\napply the random forest",
    "start": "624230",
    "end": "626389"
  },
  {
    "text": "method we just described to\nthis high-dimensional data.",
    "start": "626390",
    "end": "628940"
  },
  {
    "text": "But instead of using the\nentire set of 4,718 genes,",
    "start": "628940",
    "end": "633820"
  },
  {
    "text": "we're actually going to\npre-screen the genes.",
    "start": "633820",
    "end": "635695"
  },
  {
    "text": "We're going to choose the genes\nhaving the highest variance",
    "start": "635695",
    "end": "638153"
  },
  {
    "text": "across the training set.",
    "start": "638153",
    "end": "639260"
  },
  {
    "text": "And it makes sense if we want\nto reduce the number of genes",
    "start": "639260",
    "end": "642118"
  },
  {
    "text": "to use variance,\nbecause if something",
    "start": "642118",
    "end": "643660"
  },
  {
    "text": "has small variance\nacross the training set,",
    "start": "643660",
    "end": "646360"
  },
  {
    "text": "it's probably not\nvery predictive.",
    "start": "646360",
    "end": "648190"
  },
  {
    "text": "So choosing the one that\nhas the largest variance",
    "start": "648190",
    "end": "651220"
  },
  {
    "text": "are the ones that are most\nlikely to be predictive.",
    "start": "651220",
    "end": "654569"
  },
  {
    "text": "So is that cheating, Rob?",
    "start": "654570",
    "end": "656760"
  },
  {
    "text": "You went from 4,700\ngenes down to 500 genes.",
    "start": "656760",
    "end": "662592"
  },
  {
    "text": "That's a good question.",
    "start": "662592",
    "end": "663550"
  },
  {
    "text": "Is it cheating, is\nit biasing things?",
    "start": "663550",
    "end": "665350"
  },
  {
    "text": "Well, it's not because\nthe outcome the class",
    "start": "665350",
    "end": "668040"
  },
  {
    "text": "is not being used\nto choose the genes.",
    "start": "668040",
    "end": "670253"
  },
  {
    "text": "It would have been\na problem if we",
    "start": "670253",
    "end": "671670"
  },
  {
    "text": "chose the genes that varied the\nmost from one class to another.",
    "start": "671670",
    "end": "675720"
  },
  {
    "text": "In other words, if we use the\nclass label to choose the genes.",
    "start": "675720",
    "end": "680019"
  },
  {
    "text": "But because we're\njust doing this,",
    "start": "680020",
    "end": "682200"
  },
  {
    "text": "just looking at the\noverall variance,",
    "start": "682200",
    "end": "683910"
  },
  {
    "text": "without regard to the\nclass label, this is fine,",
    "start": "683910",
    "end": "686592"
  },
  {
    "text": "it's not going to\nbias our results.",
    "start": "686592",
    "end": "688050"
  },
  {
    "text": "So it's unsupervised screening.",
    "start": "688050",
    "end": "689470"
  },
  {
    "text": "Exactly.",
    "start": "689470",
    "end": "689970"
  },
  {
    "text": "So supervised screening\nwould create a bias",
    "start": "689970",
    "end": "694980"
  },
  {
    "text": "in our cross-validation or\nthe out-of-bag estimates.",
    "start": "694980",
    "end": "697930"
  },
  {
    "text": "But if we do it\nunsupervised, it's",
    "start": "697930",
    "end": "699570"
  },
  {
    "text": "no problem, that's\nwhat we're doing here.",
    "start": "699570",
    "end": "701790"
  },
  {
    "text": "So again, for\ncomparison purposes,",
    "start": "701790",
    "end": "705480"
  },
  {
    "text": "we divided the data into\na training and a test set,",
    "start": "705480",
    "end": "708430"
  },
  {
    "text": "we apply the random forest\nto the training set,",
    "start": "708430",
    "end": "710880"
  },
  {
    "text": "and for three different\nvalues of the m, which",
    "start": "710880",
    "end": "714813"
  },
  {
    "text": "is the number of\nvariables that we",
    "start": "714813",
    "end": "716230"
  },
  {
    "text": "choose at each split, the\nrandom number among which we're",
    "start": "716230",
    "end": "721149"
  },
  {
    "text": "going to choose at each split.",
    "start": "721150",
    "end": "722650"
  },
  {
    "text": "So let's see the results.",
    "start": "722650",
    "end": "724540"
  },
  {
    "text": "So again, a horizontal\naxis has the number",
    "start": "724540",
    "end": "726970"
  },
  {
    "text": "of trees, total number of\ntrees, we go up to about 500.",
    "start": "726970",
    "end": "730600"
  },
  {
    "text": "On the left, we have\nthe single tree results.",
    "start": "730600",
    "end": "732949"
  },
  {
    "text": "So remember, we've been saying\nquite often that the trees are",
    "start": "732950",
    "end": "735730"
  },
  {
    "text": "attractive, because\nthey're interpretable,",
    "start": "735730",
    "end": "737649"
  },
  {
    "text": "but they really don't\noften predict well.",
    "start": "737650",
    "end": "739600"
  },
  {
    "text": "And here we see a case where\nthey're predicting pretty badly.",
    "start": "739600",
    "end": "742141"
  },
  {
    "text": "Here's a single tree, and its\nerror is upwards of 50% or 60%.",
    "start": "742142",
    "end": "747180"
  },
  {
    "text": "Now remember,\nthere's 15 classes,",
    "start": "747180",
    "end": "748770"
  },
  {
    "text": "so an error of 60%\nor 70% isn't crazy,",
    "start": "748770",
    "end": "751040"
  },
  {
    "text": "because there's so many classes.",
    "start": "751040",
    "end": "753899"
  },
  {
    "text": "But still, it's not very good.",
    "start": "753900",
    "end": "756210"
  },
  {
    "text": "It's interesting,\nRob, how quickly",
    "start": "756210",
    "end": "757760"
  },
  {
    "text": "the error comes down with random\nforests, and then levels off.",
    "start": "757760",
    "end": "761910"
  },
  {
    "text": "It doesn't take long, by about\n100 trees it's leveled off,",
    "start": "761910",
    "end": "765800"
  },
  {
    "text": "and not really\nchanging that much.",
    "start": "765800",
    "end": "768050"
  },
  {
    "text": "Exactly.",
    "start": "768050",
    "end": "770670"
  },
  {
    "text": "So it looks like the\nbest is around, well,",
    "start": "770670",
    "end": "773870"
  },
  {
    "text": "if we use m equals p,\nwhich is just bagging,",
    "start": "773870",
    "end": "777747"
  },
  {
    "text": "that means at every split we're\nusing all possible predictors,",
    "start": "777747",
    "end": "780330"
  },
  {
    "text": "or considering all possible\npredictors, that's bagging,",
    "start": "780330",
    "end": "782920"
  },
  {
    "text": "those are usual trees.",
    "start": "782920",
    "end": "784410"
  },
  {
    "text": "That gives us the gold\ncurve random forest,",
    "start": "784410",
    "end": "788279"
  },
  {
    "text": "the green one with\nthe square root of p.",
    "start": "788280",
    "end": "790690"
  },
  {
    "text": "So we're retaining square root\np and throwing the rest away.",
    "start": "790690",
    "end": "796410"
  },
  {
    "text": "You see, because it gives us an\nimprovement of perhaps 3% or 4%",
    "start": "796410",
    "end": "799860"
  },
  {
    "text": "over bagging.",
    "start": "799860",
    "end": "800850"
  },
  {
    "text": "One nice thing about\nrandom forest and bagging,",
    "start": "800850",
    "end": "803069"
  },
  {
    "text": "is that you can't overfit by\nputting in too many trees.",
    "start": "803070",
    "end": "806580"
  },
  {
    "text": "The only benefit of\nadding more trees",
    "start": "806580",
    "end": "808500"
  },
  {
    "text": "is, it brings the\nvariance down more.",
    "start": "808500",
    "end": "810300"
  },
  {
    "text": "But at some point,\nthe variance just",
    "start": "810300",
    "end": "812610"
  },
  {
    "text": "stops decreasing, and\nadding more trees does",
    "start": "812610",
    "end": "815220"
  },
  {
    "text": "doesn't help you at all,\nbut it never will hurt you.",
    "start": "815220",
    "end": "817839"
  },
  {
    "text": "So by looking at, for example,\nthese out-of-bag errors,",
    "start": "817840",
    "end": "821340"
  },
  {
    "text": "you can just decide\nwhen you've done enough.",
    "start": "821340",
    "end": "824360"
  },
  {
    "start": "824360",
    "end": "826000"
  }
]