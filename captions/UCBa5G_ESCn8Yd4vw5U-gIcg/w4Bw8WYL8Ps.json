[
  {
    "start": "0",
    "end": "5080"
  },
  {
    "text": "So I'm excited to talk\ntoday about our recent work on using transformers for\nreinforcement learning.",
    "start": "5080",
    "end": "13360"
  },
  {
    "text": "And this is joint work with\na bunch of really exciting collaborators, most of\nthem at UC Berkeley,",
    "start": "13360",
    "end": "21220"
  },
  {
    "text": "and some of them at\nFacebook, and Google. I should mention, this\nwork was led by very two",
    "start": "21220",
    "end": "27970"
  },
  {
    "text": "talented undergrads-- Lili Chen and Kevin Lu. And I'm excited to present\nthe results we had.",
    "start": "27970",
    "end": "35020"
  },
  {
    "text": "So let's try to motivate why we\neven care about this problem.",
    "start": "35020",
    "end": "40280"
  },
  {
    "text": "So we have seen in the\nlast three or four years",
    "start": "40280",
    "end": "45430"
  },
  {
    "text": "that transformers, since\ntheir introduction in 2017, have taken over lots and\nlots of different fields",
    "start": "45430",
    "end": "52960"
  },
  {
    "text": "of artificial intelligence. So we saw them having a big\nimpact for language processing.",
    "start": "52960",
    "end": "59770"
  },
  {
    "text": "We saw them being used\nfor vision, in the vision transformer very recently.",
    "start": "59770",
    "end": "65440"
  },
  {
    "text": "They were in nature trying\nto solve protein folding. And very soon they might just\nreplace us computer scientists",
    "start": "65440",
    "end": "72550"
  },
  {
    "text": "by having automatically\ngenerate code. So with all of\nthese advances, it",
    "start": "72550",
    "end": "77830"
  },
  {
    "text": "seems like we are\ngetting closer to having a unified model\nfor decision making",
    "start": "77830",
    "end": "84369"
  },
  {
    "text": "for artificial intelligence. But artificial\nintelligence is much more about not just\nhaving perception but also",
    "start": "84370",
    "end": "92230"
  },
  {
    "text": "using the perception\nknowledge to make decisions. And this is what this\ntalk is going to be about.",
    "start": "92230",
    "end": "99080"
  },
  {
    "text": "But before I go into\nactually thinking about how we will use these\nmodels for decision making,",
    "start": "99080",
    "end": "104260"
  },
  {
    "text": "here is a motivation\nfor why I think it is important to ask this question.",
    "start": "104260",
    "end": "109750"
  },
  {
    "text": "So unlike models\nfor RL, when we look at transformers for\nperception modalities",
    "start": "109750",
    "end": "117610"
  },
  {
    "text": "like I showed in\nthe previous slide, we find that these\nmodels are very scalable",
    "start": "117610",
    "end": "122830"
  },
  {
    "text": "and have very stable\ntraining dynamics. So you can keep-- as long as\nyou have enough computation",
    "start": "122830",
    "end": "130119"
  },
  {
    "text": "and you have more and more\ndata that can be sourced. you can train bigger\nand bigger models.",
    "start": "130120",
    "end": "135310"
  },
  {
    "text": "And you'll see very smooth\nreductions in the loss. And the overall training\ndynamics are very, very stable.",
    "start": "135310",
    "end": "144250"
  },
  {
    "text": "And this makes it very easy for\npractitioners and researchers to build these models\nand learn richer",
    "start": "144250",
    "end": "151900"
  },
  {
    "text": "and richer distributions. So like I said, all\nof these advances have so far occurred\nin perception.",
    "start": "151900",
    "end": "159430"
  },
  {
    "text": "What we're interested\nin this talk is to think about how we can\ngo from perception, looking",
    "start": "159430",
    "end": "165010"
  },
  {
    "text": "at images, looking at\ntext, and all these kinds of sensory signals to then\ngoing into the field of actually",
    "start": "165010",
    "end": "171189"
  },
  {
    "text": "taking actions and\nmaking our agents do interesting\nthings in the world.",
    "start": "171190",
    "end": "176815"
  },
  {
    "text": " And here, throughout\nthe talk, we",
    "start": "176815",
    "end": "181840"
  },
  {
    "text": "should be thinking about why\nthis perspective is going to enable us to do scalable\nlearning, like I showed",
    "start": "181840",
    "end": "188860"
  },
  {
    "text": "in the previous slide,\notherwise bringing stability into the whole procedure. So sequential decision\nmaking is a very broad area.",
    "start": "188860",
    "end": "197080"
  },
  {
    "text": "And what I'm specifically\ngoing to be focusing on today, is one route to sequential\ndecision making,",
    "start": "197080",
    "end": "204070"
  },
  {
    "text": "that's reinforcement learning. So just as a brief background,\nwhat is reinforcement learning?",
    "start": "204070",
    "end": "211690"
  },
  {
    "text": "So we are given an agent\nwho is in a current state. And the agent is going to\ninteract with the environment",
    "start": "211690",
    "end": "220060"
  },
  {
    "text": "by taking actions. And by taking these\nactions, the environment",
    "start": "220060",
    "end": "225580"
  },
  {
    "text": "is going to return to\nit a reward for how good that action was, as well as\nthe next state into which",
    "start": "225580",
    "end": "232900"
  },
  {
    "text": "the agent will transition. And this whole feedback\nloop will continue on.",
    "start": "232900",
    "end": "238720"
  },
  {
    "text": "The goal here for\nan intelligent agent is to then, using\ntrial and error,",
    "start": "238720",
    "end": "244850"
  },
  {
    "text": "so try out different actions,\nsee what rewards it leads to, learn a policy, which maps\nyour states to actions such",
    "start": "244850",
    "end": "252520"
  },
  {
    "text": "that the policy maximizes the\nagent's cumulative rewards over time horizon.",
    "start": "252520",
    "end": "258620"
  },
  {
    "text": "So you take a\nsequence of actions. And then based on the\nreward you accumulate for that sequence\nof actions, we'll",
    "start": "258620",
    "end": "264789"
  },
  {
    "text": "judge how good your policy is. This talk is also going\nto be specifically focused",
    "start": "264790",
    "end": "271990"
  },
  {
    "text": "on a form of\nreinforcement learning that goes by the name of\noffline reinforcement learning.",
    "start": "271990",
    "end": "277330"
  },
  {
    "text": "So the idea here is\nthat what changes from the previous picture\nwhere I was talking",
    "start": "277330",
    "end": "282760"
  },
  {
    "text": "about online learning, online\nreinforcement learning is that here, now instead\nof doing actively",
    "start": "282760",
    "end": "289420"
  },
  {
    "text": "interacting with\nthe environment, you have a collection of\nlog data of interactions.",
    "start": "289420",
    "end": "295670"
  },
  {
    "text": "So think about some robot\nthat's going out in the fields. And it collects a\nbunch of sensory data.",
    "start": "295670",
    "end": "301180"
  },
  {
    "text": "And you have all logged it. And using that log data, you\nnow want to train another agent,",
    "start": "301180",
    "end": "307000"
  },
  {
    "text": "it could be another\nrobot, to then learn something interesting\nabout that environment",
    "start": "307000",
    "end": "312940"
  },
  {
    "text": "just by looking at log data. So there's no trial\nand error component,",
    "start": "312940",
    "end": "319670"
  },
  {
    "text": "which is currently one of the\nextensions of this framework,",
    "start": "319670",
    "end": "325040"
  },
  {
    "text": "which would be very exciting. So I'll talk about this towards\nthe end of the talk, why it's exciting to think about how\nwe can extend this framework",
    "start": "325040",
    "end": "333100"
  },
  {
    "text": "to include an\nexploration component and have trial and error. OK, so now to go more\nconcretely into what",
    "start": "333100",
    "end": "342280"
  },
  {
    "text": "the motivating challenge\nof this talk was now that we've introduced RL. So let's look at\nsome statistics.",
    "start": "342280",
    "end": "350110"
  },
  {
    "text": "So large language models, they\nhave billions of parameters. And they have roughly about\n100 layers in transformer.",
    "start": "350110",
    "end": "361490"
  },
  {
    "text": "They're very stable to train\nusing supervised learning style losses, which are\nthe building blocks",
    "start": "361490",
    "end": "368440"
  },
  {
    "text": "of autoregressive\ngeneration, for instance, or for masked language\nmodeling as input.",
    "start": "368440",
    "end": "375350"
  },
  {
    "text": "And this is like a field\nthat's growing every day. And there is a\ncourse at Stanford",
    "start": "375350",
    "end": "382539"
  },
  {
    "text": "that we're all taking\njust because it has had such a monumental impact on AI.",
    "start": "382540",
    "end": "389110"
  },
  {
    "text": "RL policies on the other hand,\nand I'm talking about deep RL.",
    "start": "389110",
    "end": "394409"
  },
  {
    "text": "The maximum they would\nextend to is maybe millions of parameters\nor 20 layers.",
    "start": "394410",
    "end": "401439"
  },
  {
    "text": "And what's really\nunnerving is that they're very unstable to train. So the current algorithms\nfor reinforcement learning,",
    "start": "401440",
    "end": "409480"
  },
  {
    "text": "they build on mostly\ndynamic programming, which involves following\nan inner loop optimization",
    "start": "409480",
    "end": "415240"
  },
  {
    "text": "problem that's very unstable. And it's very common\nto see practitioners",
    "start": "415240",
    "end": "420340"
  },
  {
    "text": "in RL looking at rewards\nthat look like this. So what I really\nwant you to see here",
    "start": "420340",
    "end": "426039"
  },
  {
    "text": "is the variance in the returns\nthat we tend to get in RL. It's really huge even\nafter doing multiple rounds",
    "start": "426040",
    "end": "433990"
  },
  {
    "text": "of experimentation. And that is really at\nthe core with the fact that our algorithms [INAUDIBLE]\nneed better improvements",
    "start": "433990",
    "end": "446080"
  },
  {
    "text": "so that the performance can\nbe stably achieved by agents in complex environments.",
    "start": "446080",
    "end": "453350"
  },
  {
    "text": "So what this work\nis hoping to do is it's going to\nintroduce transformers.",
    "start": "453350",
    "end": "460960"
  },
  {
    "text": "And I'll first show,\nin one slide, what exactly that model looks like. And we're going to go into\ndeeper details of each",
    "start": "460960",
    "end": "469030"
  },
  {
    "text": "of the components. I have a question.",
    "start": "469030",
    "end": "474230"
  },
  {
    "text": "Yeah, can I ask a\nquestion real quick? Yes. [INTERPOSING VOICES]",
    "start": "474230",
    "end": "480160"
  },
  {
    "text": "I'm curious to know what is\nthe cause for why RL typically has several orders of\nmagnitude fewer parameters?",
    "start": "480160",
    "end": "489840"
  },
  {
    "text": "That's a great question. So typically, when you think\nabout reinforcement learning algorithms, in deep\nRL in particular,",
    "start": "489840",
    "end": "498460"
  },
  {
    "text": "so the most common\nalgorithms, for example, have different networks playing\ndifferent roles in the task.",
    "start": "498460",
    "end": "509300"
  },
  {
    "text": "So you'll have a\nnetwork, for instance, playing the role of an actor. So it's trying to\nfigure out a policy.",
    "start": "509300",
    "end": "515309"
  },
  {
    "text": "And then there'll be a\ndifferent network that's playing the role of the critic. And these networks\nare trained on data",
    "start": "515309",
    "end": "523169"
  },
  {
    "text": "that's adaptively gathered. So unlike perception, where\nyou will have a huge data",
    "start": "523169",
    "end": "529020"
  },
  {
    "text": "set of interactions on which\nyou can train your models,",
    "start": "529020",
    "end": "534630"
  },
  {
    "text": "in this case, the architectures\nand even the environments to some extent are very\nsimplistic because of the fact",
    "start": "534630",
    "end": "543450"
  },
  {
    "text": "that we are trying to train very\nsmall components, the functions that we are training and then\nbringing them all together.",
    "start": "543450",
    "end": "550649"
  },
  {
    "text": "And these functions are often\ntrained in not super complex",
    "start": "550650",
    "end": "556350"
  },
  {
    "text": "environments. So it's a mix of\ndifferent issues. I wouldn't say it's purely\njust got to do with the fact",
    "start": "556350",
    "end": "563070"
  },
  {
    "text": "that the learning\nobjectives are at fault, but it's a combination of\nthe environments we use,",
    "start": "563070",
    "end": "569070"
  },
  {
    "text": "the combination of\nthe targets that each of the neural networks\nare predicting, which leads to\nnetworks which are",
    "start": "569070",
    "end": "577020"
  },
  {
    "text": "much bigger than\nwhat we currently see tending to all fit. And that's why it's very\ncommon to see neural networks",
    "start": "577020",
    "end": "584970"
  },
  {
    "text": "with much fewer layers\nbeing used in RL as opposed to perception.",
    "start": "584970",
    "end": "590010"
  },
  {
    "text": " Thank you. ",
    "start": "590010",
    "end": "596020"
  },
  {
    "text": "You want to ask a question? Yeah. Yeah, I was going to. Is there a reason why you chose\na offline RL versus online RL?",
    "start": "596020",
    "end": "606310"
  },
  {
    "text": "That's another great question. So the question is why offline\nRL as opposed to online RL.",
    "start": "606310",
    "end": "611650"
  },
  {
    "text": "And the plain reason\nis because this is the first work trying to\nlook at reinforcement learning.",
    "start": "611650",
    "end": "618460"
  },
  {
    "text": "So offline RL avoids this\nproblem of exploration. You are given a large\ndata set of interactions.",
    "start": "618460",
    "end": "626080"
  },
  {
    "text": "You're not allowed to further\ninteract with the environment. So just from this\ndata set, you are trying to unearth a policy of\nwhat the optimal agent would",
    "start": "626080",
    "end": "635290"
  },
  {
    "text": "look like. So it would be-- Like if you do online RL\nwouldn't that like just",
    "start": "635290",
    "end": "640960"
  },
  {
    "text": "give you this opportunity\nof exploration, basically? It would, it would.",
    "start": "640960",
    "end": "646360"
  },
  {
    "text": "But what it would also do, which\nis technically challenging here is that the exploration\nwould be harder to encode.",
    "start": "646360",
    "end": "654610"
  },
  {
    "text": "So offline RL is a first step. There's no reason why we should\nnot study RL online or cannot",
    "start": "654610",
    "end": "660490"
  },
  {
    "text": "be done. It's just that it provides\na more contained state of where ideas from transformers\nwill directly extend.",
    "start": "660490",
    "end": "667405"
  },
  {
    "start": "667405",
    "end": "673320"
  },
  {
    "text": "OK, sounds good. So let's look at the model. And it's really\nsimple on purpose.",
    "start": "673320",
    "end": "679330"
  },
  {
    "text": "So what we're going\nto do is we're going to look at our offline\ndata, which is essentially",
    "start": "679330",
    "end": "684960"
  },
  {
    "text": "in the form of trajectories. So offline data would look like\na sequence of states, actions,",
    "start": "684960",
    "end": "692339"
  },
  {
    "text": "returns, over\nmultiple time steps. It's a sequence. So it's natural to think\nof as directly feeding it",
    "start": "692340",
    "end": "700920"
  },
  {
    "text": "as input to a transformer. In this case, we use\na causal transformer",
    "start": "700920",
    "end": "706950"
  },
  {
    "text": "as it's common in GPT. So we go from left to right. And because this data set comes\nwith the notion of time step",
    "start": "706950",
    "end": "715019"
  },
  {
    "text": "causality, here it's\nmuch more well-intended than the general meaning\nthat used for perception.",
    "start": "715020",
    "end": "721560"
  },
  {
    "text": "This is really\ncausality, how it should be in the perspective of time.",
    "start": "721560",
    "end": "727020"
  },
  {
    "text": "What we predict out\nof this transformer are the actions\nconditioned on everything",
    "start": "727020",
    "end": "734160"
  },
  {
    "text": "that comes before that\ntoken in the sequence. So if you want to predict the\naction at this t minus 1 step,",
    "start": "734160",
    "end": "741330"
  },
  {
    "text": "we will use everything that\ncame at timestep t minus 2",
    "start": "741330",
    "end": "746640"
  },
  {
    "text": "as well as the returns and\nstates at timestep t minus 1.",
    "start": "746640",
    "end": "752970"
  },
  {
    "text": "OK, so I will go into the\ndetails of how exactly",
    "start": "752970",
    "end": "759180"
  },
  {
    "text": "each of these are encoded. But essentially, this\nis in a one liner.",
    "start": "759180",
    "end": "765149"
  },
  {
    "text": "It's taking the trajectory\ndata from the offline data, treating it as a\nsequence of tokens,",
    "start": "765150",
    "end": "770970"
  },
  {
    "text": "passing it through a\ncausal transformer, and getting a sequence\nof actions as the output.",
    "start": "770970",
    "end": "776850"
  },
  {
    "text": "OK, so how exactly do\nwe do the forward pass through the network?",
    "start": "776850",
    "end": "783580"
  },
  {
    "text": "So one important\naspect of this work, which is the usestate,\nactions, and this quantity",
    "start": "783580",
    "end": "792600"
  },
  {
    "text": "called returns-to-go. So these are not direct rewards. These are returns-to-go.",
    "start": "792600",
    "end": "798450"
  },
  {
    "text": "And let's see what\nthey really mean. So this is a trajectory that\ngoes as input to the system.",
    "start": "798450",
    "end": "808060"
  },
  {
    "text": "And the returns-to-go\nare the sum of rewards starting from\nthe current time step",
    "start": "808060",
    "end": "815470"
  },
  {
    "text": "until the end of the episode. So really what we want the\ntransformer is to get better",
    "start": "815470",
    "end": "822700"
  },
  {
    "text": "at using a target\nreturn, this is how you should think\nof returns-to-go,",
    "start": "822700",
    "end": "828580"
  },
  {
    "text": "as the input in deciding\nwhat action to take. This perspective is going\nto have multiple advantages.",
    "start": "828580",
    "end": "835520"
  },
  {
    "text": "It will allow us to actually\ndo much more than offline RL and generalizing to\ndifferent tasks by just",
    "start": "835520",
    "end": "841490"
  },
  {
    "text": "changing the returns-to-go. And here it's very important. So at time step 1, we will just\nhave the overall sum of rewards",
    "start": "841490",
    "end": "850940"
  },
  {
    "text": "for the entire trajectory. At time step 2, we\nsubtract the reward we get by taking\nthe first action",
    "start": "850940",
    "end": "857840"
  },
  {
    "text": "and then have the sum of\nrewards for the remainder of the trajectory.",
    "start": "857840",
    "end": "862950"
  },
  {
    "text": "OK, so that's why we\ncall it returns-to-go, like how many more\nrewards in accumulation",
    "start": "862950",
    "end": "870450"
  },
  {
    "text": "you need to acquire to fulfill\nyour reward, your return goal",
    "start": "870450",
    "end": "876360"
  },
  {
    "text": "that you set in the beginning. What is the output? The output is the sequence\nof predicted actions.",
    "start": "876360",
    "end": "884320"
  },
  {
    "text": "So as I showed in\nthe previous slide, we used a call to the\ntransformer so we'll predict,",
    "start": "884320",
    "end": "890670"
  },
  {
    "text": "in sequence, the\ndesired actions. The attention which is\ngoing to be computed",
    "start": "890670",
    "end": "901200"
  },
  {
    "text": "inside the transformer will take\nin an important hyperparameter K, which is the context length.",
    "start": "901200",
    "end": "907320"
  },
  {
    "text": "We see that in\nperception as well here. And for the rest\nof the talk, I'm going to use the notation K\nto denote how many tokens,",
    "start": "907320",
    "end": "914160"
  },
  {
    "text": "in the past, would\nbe attending over to predict the action\nand the current timestep.",
    "start": "914160",
    "end": "921980"
  },
  {
    "text": "OK, so again, digging a\nlittle bit deeper into code,",
    "start": "921980",
    "end": "928040"
  },
  {
    "text": "there are some\nsubtle differences with how a decision\ntransformer operates as opposed",
    "start": "928040",
    "end": "935570"
  },
  {
    "text": "to a normal transformer. The first is that here\nthe timestep notion",
    "start": "935570",
    "end": "941240"
  },
  {
    "text": "is going to have much\nbigger semantics that",
    "start": "941240",
    "end": "947029"
  },
  {
    "text": "extends across three tokens. So in perception, you just think\nabout the timestep for word,",
    "start": "947030",
    "end": "955850"
  },
  {
    "text": "for instance like in\nNLP or patch for vision. And in this case, we\nwill have a timestep",
    "start": "955850",
    "end": "962870"
  },
  {
    "text": "encapsulating three tokens. One for the states. One for the actions. And one for the rewards.",
    "start": "962870",
    "end": "968990"
  },
  {
    "text": "And then we'll embed\neach of these tokens. And then add the position\nembedding as it's",
    "start": "968990",
    "end": "975380"
  },
  {
    "text": "coming in the transformer. And we feed those inputs\nto the transformer.",
    "start": "975380",
    "end": "982850"
  },
  {
    "text": "At the output, we\nonly care about one of these feed tokens\nin this default setup.",
    "start": "982850",
    "end": "987950"
  },
  {
    "text": "I will show\nexperiments where even the other tokens might be of\ninterest as target predictions.",
    "start": "987950",
    "end": "993110"
  },
  {
    "text": "But for now, let's\nkeep it simple. We want to learn a policy,\na policy of just trying to predict actions.",
    "start": "993110",
    "end": "998639"
  },
  {
    "text": "So when we try to decode, we'll\nonly be looking at the actions",
    "start": "998640",
    "end": "1004750"
  },
  {
    "text": "from the hidden representation\nin the final layer. OK, so this is the forward pass.",
    "start": "1004750",
    "end": "1010180"
  },
  {
    "text": "Now what do we do\nwith this network? We train it. How do we train it?",
    "start": "1010180",
    "end": "1015993"
  },
  {
    "text": "Just a quick\nquestion on semantics there, if you go back one\nslide, the plus in this case,",
    "start": "1015993",
    "end": "1021350"
  },
  {
    "text": "the syntax means\nthat you are actually adding the values elementwise\nwise and not concatenating, is that right?",
    "start": "1021350",
    "end": "1027243"
  },
  {
    "text": "That is correct. OK, just wanted to check. ",
    "start": "1027243",
    "end": "1033640"
  },
  {
    "text": "OK, so what's the loss function? Hold up on that. I thought it was concatenating,\nwhy are we just adding it?",
    "start": "1033640",
    "end": "1041119"
  },
  {
    "text": "Sorry, can you go back? Yeah, I think it's\na design choice.",
    "start": "1041119",
    "end": "1046470"
  },
  {
    "text": "You can concatenate. You can add it. It leads to different\nfunctions being encoded.",
    "start": "1046470",
    "end": "1051629"
  },
  {
    "text": "In our case, it is addition. OK. ",
    "start": "1051630",
    "end": "1057770"
  },
  {
    "text": "Did you try the other one\nand it just didn't work or why is that? Because I think,\nintuitively, concatenating",
    "start": "1057770",
    "end": "1064970"
  },
  {
    "text": "would make more sense. So I think both of them\nhave different use cases",
    "start": "1064970",
    "end": "1073920"
  },
  {
    "text": "for the functional encoding. Like one is really mixing in\nthe embeddings for the state",
    "start": "1073920",
    "end": "1080290"
  },
  {
    "text": "and basically shifting it. So when you add\nsomething, if you think of the embedding\nof the states as a vector",
    "start": "1080290",
    "end": "1089250"
  },
  {
    "text": "and you add something, you\nare actually shifting it. Whereas in the\nconcatenation case,",
    "start": "1089250",
    "end": "1095010"
  },
  {
    "text": "you are actually increasing the\ndimensionality of this space. ",
    "start": "1095010",
    "end": "1100460"
  },
  {
    "text": "So those are different\nchoices which are doing very different things.",
    "start": "1100460",
    "end": "1105659"
  },
  {
    "text": "We found this one to be better. I'm not sure I remember if the\nresults was very significantly",
    "start": "1105660",
    "end": "1112190"
  },
  {
    "text": "different if you would\nconcatenate them, but this is the one\nwhich we operate with.",
    "start": "1112190",
    "end": "1117440"
  },
  {
    "text": "But wouldn't there-- because\nif you're shifting it, like if you have an\nembedding for a state, and let's say you\nperform certain actions,",
    "start": "1117440",
    "end": "1123710"
  },
  {
    "text": "and you like end up at\nthe same state again, you would want these\nembeddings to be the same.",
    "start": "1123710",
    "end": "1129587"
  },
  {
    "text": "However, now you're at\na different timestep, so like you shifted it. So wouldn't that be\nlike harder to learn?",
    "start": "1129587",
    "end": "1136340"
  },
  {
    "text": "So there's a bigger and\ninteresting question in that. What you said is basically are\nwe losing the Markov property?",
    "start": "1136340",
    "end": "1144540"
  },
  {
    "text": "Because as you said that if\nyou come back to the same state at a different\ntimestep, shouldn't we",
    "start": "1144540",
    "end": "1151590"
  },
  {
    "text": "be doing similar operations? And the answer here is, yes.",
    "start": "1151590",
    "end": "1157470"
  },
  {
    "text": "We are actually\nbeing non-Markov. And this might seem very\nnon-intuitive at first",
    "start": "1157470",
    "end": "1162660"
  },
  {
    "text": "that why is non-Markovness\nimportant here?",
    "start": "1162660",
    "end": "1169530"
  },
  {
    "text": "And I want to refer\nto another paper which came very much in conjunction\nwith this type of transformer",
    "start": "1169530",
    "end": "1176440"
  },
  {
    "text": "that actually shows\nin more detail. And it's basically\nsaying that if you were trying to predict\nthe transition dynamics,",
    "start": "1176440",
    "end": "1183090"
  },
  {
    "text": "then you could have actually\nhad a Markovian system built in here, which would\ndo just as good.",
    "start": "1183090",
    "end": "1190659"
  },
  {
    "text": "However, from the perspective\nof trying to actually predict actions, it does help to look\nat the previous timesteps,",
    "start": "1190660",
    "end": "1200730"
  },
  {
    "text": "even more so when you\nhave missing observations. So for instance, if you\nhave the observations being",
    "start": "1200730",
    "end": "1207240"
  },
  {
    "text": "a subset of the true state, so\nlooking at the previous states and actions helps you\nbetter fill in the missing",
    "start": "1207240",
    "end": "1216330"
  },
  {
    "text": "pieces in some sense. So this is commonly known as\npartial observability, where by looking at the\nprevious tokens,",
    "start": "1216330",
    "end": "1222540"
  },
  {
    "text": "you can do a better\njob at predicting the actions you should take\nat the current timestep.",
    "start": "1222540",
    "end": "1231230"
  },
  {
    "text": "So non-Markovness is on purpose. And it's not\nintuitive, but I think",
    "start": "1231230",
    "end": "1237920"
  },
  {
    "text": "it's one of the things that\nseparates this framework from existing ones.",
    "start": "1237920",
    "end": "1244650"
  },
  {
    "text": "So it will basically help you-- because like RL usually works\nbetter on infinite horizon",
    "start": "1244650",
    "end": "1251000"
  },
  {
    "text": "problems, right? So technically, the\nway you formulate it, it would work better on finite\nhorizon problems, I'm assuming.",
    "start": "1251000",
    "end": "1256830"
  },
  {
    "text": "Because you want to take\ndifferent actions based on a history, based\non given the fact that now you would look\nat a different time step.",
    "start": "1256830",
    "end": "1263850"
  },
  {
    "text": "Yeah. Yeah, so if you wanted to\nwork on infinite horizon, maybe something like this\nwould work just as well",
    "start": "1263850",
    "end": "1270740"
  },
  {
    "text": "to get that effect. In this case, we were using\na discount factor of 1",
    "start": "1270740",
    "end": "1276920"
  },
  {
    "text": "or basically like no\ndiscounting at all. But you're right,\nif I think we really want to extend it\nto infinite horizon,",
    "start": "1276920",
    "end": "1283610"
  },
  {
    "text": "we would need to change\nthe discount factor. OK, thanks. ",
    "start": "1283610",
    "end": "1292350"
  },
  {
    "text": "Quick question. I think it was just\nanswered in chat. But I'll ask it anyways.",
    "start": "1292350",
    "end": "1298140"
  },
  {
    "text": "I think I might have\nmissed this or maybe you're about to talk about it. The offline data that\nwas collected, what policy was used to collect it?",
    "start": "1298140",
    "end": "1306440"
  },
  {
    "text": "So this is a very\nimportant question. And it will be something I\nmention in the experiment.",
    "start": "1306440",
    "end": "1312630"
  },
  {
    "text": "So we were using the benchmarks\nthat exist for offline RL. Essentially, the\nvarious benchmarks",
    "start": "1312630",
    "end": "1319279"
  },
  {
    "text": "are constructed as you train\nan agent using online RL. And then you look at its\nreplay buffer at some timestep,",
    "start": "1319280",
    "end": "1327170"
  },
  {
    "text": "so while it's training. So while it's like a\nmedium sort of expert, you collect the transitions,\nits experience so far.",
    "start": "1327170",
    "end": "1335990"
  },
  {
    "text": "And that is the offline data. It's something which is-- our framework is\nvery agnostic to what",
    "start": "1335990",
    "end": "1342590"
  },
  {
    "text": "offline data that we use. So I'm not discussing so far. But something that\nin our experiments",
    "start": "1342590",
    "end": "1350420"
  },
  {
    "text": "is based on\ntraditional benchmarks. Got it. So the reason I ask is I'm\nsure that your framework can",
    "start": "1350420",
    "end": "1356210"
  },
  {
    "text": "accommodate any offline data. But it seems to me like\nthe results that you're about to present are going to\nbe heavily contingent on what",
    "start": "1356210",
    "end": "1362539"
  },
  {
    "text": "that data collection policy is. Indeed, indeed. ",
    "start": "1362540",
    "end": "1369918"
  },
  {
    "text": "And also we will-- I think I have a slide\nwhere we show an experiment where the amount of data\ncan make a difference in how",
    "start": "1369918",
    "end": "1377340"
  },
  {
    "text": "we compare with baselines. And essentially, we will see how\ndecision transformer especially",
    "start": "1377340",
    "end": "1382830"
  },
  {
    "text": "shines when there is small\namounts of offline data.",
    "start": "1382830",
    "end": "1388960"
  },
  {
    "text": "OK, cool. Thank you. OK, great questions. So let's go ahead.",
    "start": "1388960",
    "end": "1394230"
  },
  {
    "text": "So we have defined our\nmodel which is going to look at these trajectories.",
    "start": "1394230",
    "end": "1401039"
  },
  {
    "text": "And now, let's see\nhow we train it. So very simple. We are trying to\npredict actions.",
    "start": "1401040",
    "end": "1408000"
  },
  {
    "text": "We'll try to match them to the\nones we have in our data set. If they are continuous,\nusing the mean squared error.",
    "start": "1408000",
    "end": "1413429"
  },
  {
    "text": "If they are discrete then\nwe can use a cross-entropy. So but there is something very\ndeep in here for RL research,",
    "start": "1413430",
    "end": "1423400"
  },
  {
    "text": "which is that these objectives\nare very stable to train and easy to regularize\nbecause they've been developed",
    "start": "1423400",
    "end": "1429720"
  },
  {
    "text": "for supervised learning. In contrast, what\nRL is more used to is dynamic programming\nstyle objectives,",
    "start": "1429720",
    "end": "1437120"
  },
  {
    "text": "which are based on\nthe Bellman equation. And those end up being much\nharder to optimize and scale.",
    "start": "1437120",
    "end": "1444990"
  },
  {
    "text": "And that's why you see a lot\nof the variance in the results as well.",
    "start": "1444990",
    "end": "1451150"
  },
  {
    "text": "OK, so this is how\nwe train the model. Now, how do we use the model? And that's the\npoint about trying",
    "start": "1451150",
    "end": "1457360"
  },
  {
    "text": "to do rollout for the model. So here again, this is\ngoing to be similar to doing",
    "start": "1457360",
    "end": "1463750"
  },
  {
    "text": "an autoregressive generation. There is an\nimportant token here, which was the returns-to-go.",
    "start": "1463750",
    "end": "1470350"
  },
  {
    "text": "And what we need to\nset during evaluation, presumably, we want\nexpert level performance",
    "start": "1470350",
    "end": "1478059"
  },
  {
    "text": "because that will have\nthe highest returns. So we set the initial\nreturns-to-go not",
    "start": "1478060",
    "end": "1485170"
  },
  {
    "text": "based on that trajectory,\nbecause now we don't have a trajectory. We're in a generator trajectory. So this is at inference time.",
    "start": "1485170",
    "end": "1490960"
  },
  {
    "text": "So we will set it to the\nexpert return for instance. So in code, what this\nwhole procedure would",
    "start": "1490960",
    "end": "1496960"
  },
  {
    "text": "look like is basically you\nset this returns-to-go token to some target return.",
    "start": "1496960",
    "end": "1503409"
  },
  {
    "text": "And you set your\ninitial state to one",
    "start": "1503410",
    "end": "1508600"
  },
  {
    "text": "from the environment\ndistribution of initial states. And then you just roll out\nyour decision transformer.",
    "start": "1508600",
    "end": "1515890"
  },
  {
    "text": "So you get a new action. And this action will\nalso give you a state and reward from the environment.",
    "start": "1515890",
    "end": "1523300"
  },
  {
    "text": "You append them\nto your sequence. And you get a new returns-to-go.",
    "start": "1523300",
    "end": "1529210"
  },
  {
    "text": "And you take just\nthe context in K because that's what's used\nby the transformer to making predictions and\nthen feed it back",
    "start": "1529210",
    "end": "1536110"
  },
  {
    "text": "to the decision transformer. So it's regular\nautoregressive generation,",
    "start": "1536110",
    "end": "1541899"
  },
  {
    "text": "but the only key point to\nnotice is how you initialize the transformer for RL.",
    "start": "1541900",
    "end": "1550230"
  },
  {
    "text": "Sorry, I have one question here. So how much does the choice\nof the expert targeted return matter?",
    "start": "1550230",
    "end": "1555670"
  },
  {
    "text": "Does it have to be like\nthe mean expected reward or can it be like the maximum\nreward possible in [INAUDIBLE]?? Does the choice of the\nnumber really matter?",
    "start": "1555670",
    "end": "1564150"
  },
  {
    "text": "That's a very good question. So we generally would set it to\nbe slightly higher than the max",
    "start": "1564150",
    "end": "1573030"
  },
  {
    "text": "return in the data set. So I think the factor\nthat we use 1.1 times.",
    "start": "1573030",
    "end": "1579870"
  },
  {
    "text": "But I think we have done\na lot of experimentation",
    "start": "1579870",
    "end": "1585690"
  },
  {
    "text": "in the range. And it's fairly robust\nto what choice you use.",
    "start": "1585690",
    "end": "1591880"
  },
  {
    "text": "So for example we hope our\nexpert returns about 3,600. And we have found ways\nto improve performance",
    "start": "1591880",
    "end": "1597660"
  },
  {
    "text": "from all the way\nfrom like 3,500, 3,400 to even going to very high\nnumbers like 5,000, it works.",
    "start": "1597660",
    "end": "1609530"
  },
  {
    "text": "Yeah, so however, I\nwould want to point out that this is something which\nis not typically needed",
    "start": "1609530",
    "end": "1618170"
  },
  {
    "text": "in regular RL, like\nknowing the expert return. Here we are actually going\nbeyond regular RL in that we",
    "start": "1618170",
    "end": "1625159"
  },
  {
    "text": "can choose a return we want. So we also actually\nneed this information about what the expert return is.",
    "start": "1625160",
    "end": "1630920"
  },
  {
    "start": "1630920",
    "end": "1636120"
  },
  {
    "text": "There's another question? Yes, so just to kind of\nbe on the regular RL.",
    "start": "1636120",
    "end": "1644100"
  },
  {
    "text": "But I'm curious\nabout do you also like restrict this\nframework to only offline RL",
    "start": "1644100",
    "end": "1650850"
  },
  {
    "text": "because if you want to run\nthis kind of framework in RL, you have to determine the\nreturns-to-go a priori.",
    "start": "1650850",
    "end": "1660010"
  },
  {
    "text": "So this kind of\nframework I think is restricted to only\noffline RL, do you think so?",
    "start": "1660010",
    "end": "1667590"
  },
  {
    "text": "Yes, and I think asking this\nquestion as well earlier that, yes, I think for now,\nthis is the first step.",
    "start": "1667590",
    "end": "1675510"
  },
  {
    "text": "So we were focusing\non offline RL where this information can be\ngathered from the offline data",
    "start": "1675510",
    "end": "1681570"
  },
  {
    "text": "set. It is possible to\nthink about strategies",
    "start": "1681570",
    "end": "1688110"
  },
  {
    "text": "on how you can even\nget this online, what you'll need\nis a curriculum. So early on during training\nas you're gathering data, when",
    "start": "1688110",
    "end": "1697890"
  },
  {
    "text": "you're doing rollouts, you\nwill set your expert return to whatever you see in the data\nset, and then increment it as",
    "start": "1697890",
    "end": "1706080"
  },
  {
    "text": "and when you start seeing that\nthe transformer can actually exceed the performance. So you can think of\nspecifying a curriculum",
    "start": "1706080",
    "end": "1713400"
  },
  {
    "text": "from slow to high for what\nthat expert return could be for which you roll out\nthe decision transformer.",
    "start": "1713400",
    "end": "1724020"
  },
  {
    "text": "That's cool, thank you.  So yeah, this was\nabout the model.",
    "start": "1724020",
    "end": "1729900"
  },
  {
    "text": "So we discussed\nhow this model is,",
    "start": "1729900",
    "end": "1735080"
  },
  {
    "text": "what the input to this model\nare, what the outputs are, what the loss function is\nused for training this model,",
    "start": "1735080",
    "end": "1740870"
  },
  {
    "text": "and how do we use this\nmodel at test time. There is a connection\nto this framework",
    "start": "1740870",
    "end": "1749480"
  },
  {
    "text": "as being one way\nto instantiate what is often known as RL as\nprobabilistic inference.",
    "start": "1749480",
    "end": "1755670"
  },
  {
    "text": "So we can formulate RL as a\ngraphical model problem, where you have the states and\nactions being used to determine",
    "start": "1755670",
    "end": "1765350"
  },
  {
    "text": "what the next state is. And to encode a\nnotion of optimality, typically you would also have\nthese additional auxiliary",
    "start": "1765350",
    "end": "1772070"
  },
  {
    "text": "variables-- O1, O2, and so on,\nand forth, which are implicitly saying that\nencoding some notion of reward.",
    "start": "1772070",
    "end": "1780380"
  },
  {
    "text": "And conditioned on this\noptimality being true, RL is the task of\nlearning a policy which",
    "start": "1780380",
    "end": "1788300"
  },
  {
    "text": "is the mapping from\nstates to actions such that we get optimal behavior.",
    "start": "1788300",
    "end": "1796740"
  },
  {
    "text": "And if you really\nsquint your eyes, you can see that these\noptimality variables",
    "start": "1796740",
    "end": "1802309"
  },
  {
    "text": "in decision transformers,\nare actually being encoded by the returns-to-go.",
    "start": "1802310",
    "end": "1807390"
  },
  {
    "text": "So if when we give a value\nthat's high enough at test time during rollout, like\nthe expert return,",
    "start": "1807390",
    "end": "1815149"
  },
  {
    "text": "we are essentially\nsaying that conditioned on this being the\nmathematical form",
    "start": "1815150",
    "end": "1822530"
  },
  {
    "text": "quantification of\noptimality, roll out your decision transformer\nto hopefully satisfy",
    "start": "1822530",
    "end": "1830530"
  },
  {
    "text": "this condition.  So yeah, so this was all I\nwant to talk about the model--",
    "start": "1830530",
    "end": "1838290"
  },
  {
    "text": "Can you explain that, please? What do you mean by optimality\nvariables in the decision",
    "start": "1838290",
    "end": "1843480"
  },
  {
    "text": "transformer? And how do you mean\nlike returns-to-go? Right, so optimality\nvariables, we",
    "start": "1843480",
    "end": "1851850"
  },
  {
    "text": "can think in the most simplest\ncontext, as-- let's just say they were binary. So 1 is if you solve the goal.",
    "start": "1851850",
    "end": "1859890"
  },
  {
    "text": "And 0 is if you did\nnot solve the goal.  And basically in that\ncase, you could also",
    "start": "1859890",
    "end": "1867780"
  },
  {
    "text": "think of your\ndecision transformer as at test time when we\nencode the returns-to-go,",
    "start": "1867780",
    "end": "1876780"
  },
  {
    "text": "we could set it to 1. Which would basically mean\nthat conditioned on optimality",
    "start": "1876780",
    "end": "1883409"
  },
  {
    "text": "here means solving\nthe goal as 1. Generate me the\nsequence of actions",
    "start": "1883410",
    "end": "1891289"
  },
  {
    "text": "such that this would be true. Of course our learning\nis not perfect. So it's not guaranteed\nwe'll get that.",
    "start": "1891290",
    "end": "1899560"
  },
  {
    "text": "But we have trained\nthe transformer in a way to interpret\nthe returns-to-go",
    "start": "1899560",
    "end": "1904770"
  },
  {
    "text": "as some notion of optimality. ",
    "start": "1904770",
    "end": "1909957"
  },
  {
    "text": "So if I'm interpreting\nthis correctly, it's roughly like\nsaying, show me what an optimal\nsequence of transitions",
    "start": "1909957",
    "end": "1916909"
  },
  {
    "text": "look like because you've\nlearned-- the model has learned sort of both successful and\nunsuccessful transitions.",
    "start": "1916910",
    "end": "1923180"
  },
  {
    "text": "Exactly, exactly. And as we shall see\nin some experiments,",
    "start": "1923180",
    "end": "1928610"
  },
  {
    "text": "for the binary case, it's\neither optimal or non optimal. But rarely this can be\na continuous variable,",
    "start": "1928610",
    "end": "1935340"
  },
  {
    "text": "which it is in our experiments. So we can also see what happens\nin between experimentally. ",
    "start": "1935340",
    "end": "1942770"
  },
  {
    "text": "OK, so let's jump\ninto the experiments. So there are a bunch\nof experiments.",
    "start": "1942770",
    "end": "1948980"
  },
  {
    "text": "And I've picked\nout a few which I think are interesting and give\nthe key results in the paper.",
    "start": "1948980",
    "end": "1956840"
  },
  {
    "text": "But feel free to\nreference the paper for an even more\ndetailed analysis on some of the components of our model.",
    "start": "1956840",
    "end": "1962585"
  },
  {
    "text": " So first we can look at how well\nthey're doing in offline RL.",
    "start": "1962585",
    "end": "1969620"
  },
  {
    "text": "So there are benchmarks for\nthe Atari suite of environments",
    "start": "1969620",
    "end": "1974750"
  },
  {
    "text": "and the OpenAI Gym. And we have another\nenvironment Key-To-Door which is especially hard because\nit contains sparse rewards",
    "start": "1974750",
    "end": "1982398"
  },
  {
    "text": "and requires it to do\ncredit assignment that I'll talk about later. But across the board, we see\nthat decision transformer",
    "start": "1982398",
    "end": "1990230"
  },
  {
    "text": "is competitive with the state\nof the art model-free offline RL methods.",
    "start": "1990230",
    "end": "1995480"
  },
  {
    "text": "In this case, this was\na version of Q-learning designed for offline RL.",
    "start": "1995480",
    "end": "2001390"
  },
  {
    "text": "And it can do\nexcellent especially when there is long term\ncredit assignment where",
    "start": "2001390",
    "end": "2007930"
  },
  {
    "text": "traditional methods based\non TD learning would fail. Yeah, so the takeaway\nhere should not",
    "start": "2007930",
    "end": "2014140"
  },
  {
    "text": "be that we should\nbe at the stage where we can just\nsubstitute the existing",
    "start": "2014140",
    "end": "2020230"
  },
  {
    "text": "algorithms with the\ndecision transformer, but this is very strong\nevidence in favor that this paradigm which\nis building on transformers",
    "start": "2020230",
    "end": "2029680"
  },
  {
    "text": "will permit us to better\niterate and improve the models to hopefully\nsurpass the existing algorithms",
    "start": "2029680",
    "end": "2038350"
  },
  {
    "text": "uniformly. And there's some early evidence\nof that in hard environments which do require long\nterm credit assignment.",
    "start": "2038350",
    "end": "2046810"
  },
  {
    "text": "May I ask a question here about\nthe baseline, specifically, TD learning.",
    "start": "2046810",
    "end": "2051832"
  },
  {
    "text": "I'm curious to\nknow, because I know that a lot of TD learning\nagents are feedforward networks. Are these baselines?",
    "start": "2051832",
    "end": "2057699"
  },
  {
    "text": "Do they have recurrence? Yeah, yeah. So I think the conservative\nQ-learning baselines here",
    "start": "2057699",
    "end": "2065960"
  },
  {
    "text": "did have recurrence,\nbut I'm not very sure. So I can check back on this\noffline and get back to you",
    "start": "2065960",
    "end": "2072590"
  },
  {
    "text": "on this. OK, OK, thank you. .",
    "start": "2072590",
    "end": "2079849"
  },
  {
    "text": "Also another quick question. So just how exactly do\nyou evaluate the decision transformer here\nin the experiment,",
    "start": "2079850",
    "end": "2086790"
  },
  {
    "text": "because you need to\nsupply the returns-to-go, so you didn't use\nthe optimal policy",
    "start": "2086790",
    "end": "2093570"
  },
  {
    "text": "to get what's the optimal\nrewards, which feed that in? So here, we basically\nlook at the offline data",
    "start": "2093570",
    "end": "2099850"
  },
  {
    "text": "set that is used for training. And we said whatever\nwas the maximum return",
    "start": "2099850",
    "end": "2105570"
  },
  {
    "text": "in the offline data, we\nset the desired target return to go as slightly\nhigher than that.",
    "start": "2105570",
    "end": "2113980"
  },
  {
    "text": "So 1.1 I think was the\ncoefficient we used. I see.",
    "start": "2113980",
    "end": "2119339"
  },
  {
    "text": "And the performance sorry, I'm\nnot really well versed in RL. So how is the\nperformance defined here?",
    "start": "2119340",
    "end": "2125880"
  },
  {
    "text": "Is it how much reward\nyou get actually from-- Yes, yes. So you can specify a\ntarget return to go,",
    "start": "2125880",
    "end": "2132580"
  },
  {
    "text": "but there's no guarantee that\nthe actual actions that you take will achieve that return.",
    "start": "2132580",
    "end": "2138750"
  },
  {
    "text": " So you actually you measure\nthe true environmental return",
    "start": "2138750",
    "end": "2144660"
  },
  {
    "text": "based on that. Yeah, I see. But then just curiously,\nso are these performance",
    "start": "2144660",
    "end": "2150840"
  },
  {
    "text": "the percentage you get,\nlike how much, I guess, reward you recover from\nthe actual environments?",
    "start": "2150840",
    "end": "2158400"
  },
  {
    "text": "Yeah, so these are\nnot percentages. These are some way of\nnormalizing the returns so that everything\nfalls between 0 to 100.",
    "start": "2158400",
    "end": "2165869"
  },
  {
    "text": "Yeah, yeah. Just wondering if\nyou have a rough idea of how much reward\nactually is recovered",
    "start": "2165870",
    "end": "2173760"
  },
  {
    "text": "by decision transformers? Does it say like, if you specify\nI want to get 50 rewards,",
    "start": "2173760",
    "end": "2179010"
  },
  {
    "text": "does it get 49? Or is it even\nbetter sometimes or?",
    "start": "2179010",
    "end": "2184140"
  },
  {
    "text": "That's an excellent\nquestion and my next slide.  So here, we're going to\nanswer precisely this question",
    "start": "2184140",
    "end": "2192630"
  },
  {
    "text": "that we're asked is like,\nif you're feeding the target return, it could be expert or\nit could also not be expert.",
    "start": "2192630",
    "end": "2199110"
  },
  {
    "text": "How well does the model\nactually do in attaining it? So the x-axis is what we specify\nas the target return we want.",
    "start": "2199110",
    "end": "2209369"
  },
  {
    "text": "And the y-axis is basically\nhow well do we actually get?",
    "start": "2209370",
    "end": "2215580"
  },
  {
    "text": "For reference, we have this\ngreen line, which is the Oracle so which means\nwhatever you desire,",
    "start": "2215580",
    "end": "2221490"
  },
  {
    "text": "the decision transformer\ngives it to you. So this would have\nbeen the ideal case. So it's a diagonal.",
    "start": "2221490",
    "end": "2227970"
  },
  {
    "text": "We also have, because\nthis is offline RL, we have, in orange, what was\nthe best trajectory data set.",
    "start": "2227970",
    "end": "2236020"
  },
  {
    "text": "So the offline data\nis not perfect. So we just plot what is the\nupper bound on the offline data",
    "start": "2236020",
    "end": "2244350"
  },
  {
    "text": "performance. And here, we find that for the\nmajority of the environments,",
    "start": "2244350",
    "end": "2251760"
  },
  {
    "text": "there is a good fit between\nthe target return we feed in and the actual\nperformance of the model",
    "start": "2251760",
    "end": "2259620"
  },
  {
    "text": "And there are some\nother observations which I wanted to take from the\nslide is that because we can",
    "start": "2259620",
    "end": "2268500"
  },
  {
    "text": "vary this notion of reward,\nwe can, in some sense, do multi-task RL via\nreturn conditioning.",
    "start": "2268500",
    "end": "2277599"
  },
  {
    "text": "This is not the only\nway to do multitask RL, you can specify a task\nvia natural language, you can specify by a goal\nstate, and so on, and so forth.",
    "start": "2277600",
    "end": "2285180"
  },
  {
    "text": "But this is one notion\nwhere the notion of a task could be how much\nreward you want. ",
    "start": "2285180",
    "end": "2293140"
  },
  {
    "text": "And another thing to\nnotice is, occasionally, these models extrapolate. This is not a trend we have\nbeen seeing consistently,",
    "start": "2293140",
    "end": "2299430"
  },
  {
    "text": "but we do see some signs of it. So if you look at,\nfor example Seaquest.",
    "start": "2299430",
    "end": "2305070"
  },
  {
    "text": "Here, the highest return\ntrajectory in a data set was pretty low.",
    "start": "2305070",
    "end": "2311020"
  },
  {
    "text": "And if you specify\na return higher than that for a\ndecision transformer,",
    "start": "2311020",
    "end": "2316980"
  },
  {
    "text": "we do find that the\nmodel is able to achieve. So it is able to\ngenerate trajectories",
    "start": "2316980",
    "end": "2324720"
  },
  {
    "text": "with returns higher than\nit ever saw in the dataset. ",
    "start": "2324720",
    "end": "2329859"
  },
  {
    "text": "I do believe that future\nwork in this space trying to improve this model\nshould think about",
    "start": "2329860",
    "end": "2337040"
  },
  {
    "text": "how can the strain be more\nconsistent across environments because this would really\nachieve the goal of offline RL",
    "start": "2337040",
    "end": "2343730"
  },
  {
    "text": "which is given\nsuboptimal behavior, how do you get optimal\nbehavior out of it?",
    "start": "2343730",
    "end": "2350120"
  },
  {
    "text": "But it remains to be seen how\nwell this trend can be made consistent across environments.",
    "start": "2350120",
    "end": "2356150"
  },
  {
    "text": "Can I jump in with a question? Yes. So I think that last point\nis really interesting.",
    "start": "2356150",
    "end": "2361460"
  },
  {
    "text": "And it's cool that you\nguys occasionally see it. I'm curious to\nknow what happens.",
    "start": "2361460",
    "end": "2366507"
  },
  {
    "text": "So this is all condition. You give as an input what\nreturn you would like. And it tries to\nselect a sequence",
    "start": "2366508",
    "end": "2372028"
  },
  {
    "text": "of actions that gives it. I'm curious to know what\nhappens if you just give it ridiculous inputs,\nlike for example, here",
    "start": "2372028",
    "end": "2378620"
  },
  {
    "text": "the order of magnitude for\nthe return is like 50 to 100, what happens if\nyou put in 10,000?",
    "start": "2378620",
    "end": "2385150"
  },
  {
    "text": "Good question. And this is something\nwe tried early on. I don't want to say\nwe went up to 10,000,",
    "start": "2385150",
    "end": "2391570"
  },
  {
    "text": "but we tried like\nreally high returns that not even an\nexpert could get. And generally, we see\nthis leveling performance.",
    "start": "2391570",
    "end": "2397480"
  },
  {
    "text": "So you can see hints of it in\nHalfCheetah, and Pong as well,",
    "start": "2397480",
    "end": "2403540"
  },
  {
    "text": "or Walker to some extent. If you look at the very end,\nthings start saturating.",
    "start": "2403540",
    "end": "2409460"
  },
  {
    "text": "So if you exceed what is\nlike certain threshold,",
    "start": "2409460",
    "end": "2415160"
  },
  {
    "text": "which often corresponds with\nthe best trajectory threshold but not always, beyond that\neverything is similar returns.",
    "start": "2415160",
    "end": "2424060"
  },
  {
    "text": "So at least one good\nthing is it does not degrade in performance. So it would have\nbeen a little bit worrying if you specified\na return of 10,000",
    "start": "2424060",
    "end": "2431260"
  },
  {
    "text": "and it gives you a return which\nis 20 or something really low.",
    "start": "2431260",
    "end": "2437440"
  },
  {
    "text": "So it's good that it\nstabilizes, but it's not that it keeps\nincreasing on and on. So there would be a point\nwhere the performance would",
    "start": "2437440",
    "end": "2445690"
  },
  {
    "text": "get saturated. OK, thank you. I was just curious. So usually for transfer\nmodels you need a lot of data.",
    "start": "2445690",
    "end": "2452560"
  },
  {
    "text": "So do you know like how\nmuch data do you need, like how well does it scale\nwith data, the performance, of decision transformer?",
    "start": "2452560",
    "end": "2459310"
  },
  {
    "text": "Yeah, so here we use\nthe standard data",
    "start": "2459310",
    "end": "2466450"
  },
  {
    "text": "like the default RL\nbenchmarks for MuJoCo which I think have\nmillions or transitions",
    "start": "2466450",
    "end": "2471520"
  },
  {
    "text": "in the order of millions. For Atari, we used 1%\nof the replay buffer,",
    "start": "2471520",
    "end": "2480080"
  },
  {
    "text": "which is smaller than the one we\nused for the MuJoCo benchmarks.",
    "start": "2480080",
    "end": "2488020"
  },
  {
    "text": "And I actually have a result\nin the very next slide, which shows a decision transformer\nespecially being useful",
    "start": "2488020",
    "end": "2496990"
  },
  {
    "text": "when you have a little data. ",
    "start": "2496990",
    "end": "2503190"
  },
  {
    "text": "Yeah, so I guess one\nquestion to ask-- Before you move on,\nin the last slide,",
    "start": "2503190",
    "end": "2509940"
  },
  {
    "text": "what do you mean again by return\nconditioning for the multitask part?",
    "start": "2509940",
    "end": "2515190"
  },
  {
    "text": "Yes, if you think\nabout the returns-to-go at test time, the one\nyou have to feed in",
    "start": "2515190",
    "end": "2520290"
  },
  {
    "text": "as the starting token,\nas one way of specifying",
    "start": "2520290",
    "end": "2528810"
  },
  {
    "text": "what policy you want. [INTERPOSING VOICES] How is that multi-task?",
    "start": "2528810",
    "end": "2535650"
  },
  {
    "text": "So it is multitask\nin the sense that you can get different policies\nby changing your target",
    "start": "2535650",
    "end": "2541740"
  },
  {
    "text": "return-to-go. You're essentially getting\ndifferent behaviors included. So think about, for instance,\nthe Hopper and you specify",
    "start": "2541740",
    "end": "2549310"
  },
  {
    "text": "a return-to-go that's very low. So you're basically saying get\nme an agent which will just",
    "start": "2549310",
    "end": "2556320"
  },
  {
    "text": "stick around its\ninitial state and not go into uncharted territory.",
    "start": "2556320",
    "end": "2565930"
  },
  {
    "text": "And if you give it really,\nreally high, then you're asking it to do the traditional\ntask, which is to hop",
    "start": "2565930",
    "end": "2572760"
  },
  {
    "text": "and go as far as\npossible without falling. Can you qualify\nthose multitasks, because that\nbasically just means",
    "start": "2572760",
    "end": "2579299"
  },
  {
    "text": "that your return conditioning\nis a cue for it to memorize,",
    "start": "2579300",
    "end": "2584590"
  },
  {
    "text": "right? Which is usually one of\nthe pitfalls of multitask? ",
    "start": "2584590",
    "end": "2591837"
  },
  {
    "text": "I'm not sure-- You need a task identifier,\nthat's what I'm trying to say. ",
    "start": "2591837",
    "end": "2598360"
  },
  {
    "text": "I'm not sure if\nit's memorization-- ",
    "start": "2598360",
    "end": "2603370"
  },
  {
    "text": "I think the purpose\nof this, I mean",
    "start": "2603370",
    "end": "2608410"
  },
  {
    "text": "having an offline dataset\nthat makes it basically saying that it's very, very specific\nto if you had the same start",
    "start": "2608410",
    "end": "2616569"
  },
  {
    "text": "state, and you took\nthe same actions, and you had the same\ntarget returns, that",
    "start": "2616570",
    "end": "2622240"
  },
  {
    "text": "would qualify as memorization. But here at test time, we allow\nall of these things to change.",
    "start": "2622240",
    "end": "2628420"
  },
  {
    "text": "And in fact they do change. So your initial state\nwould be different. Your target return of it could\nbe a different scalar than one",
    "start": "2628420",
    "end": "2638477"
  },
  {
    "text": "you ever saw during training. ",
    "start": "2638477",
    "end": "2644020"
  },
  {
    "text": "So essentially, the\nmodel has to learn to generate that\nbehavior starting",
    "start": "2644020",
    "end": "2649090"
  },
  {
    "text": "from a different\ninitial state and maybe a different value\nof the target return than it saw during training.",
    "start": "2649090",
    "end": "2657910"
  },
  {
    "text": "If the dynamics are stochastic\nthat also makes it that. Even if you memorize\nthe actions, you're not guaranteed to\nget the same next state.",
    "start": "2657910",
    "end": "2665840"
  },
  {
    "text": "So you would actually\nhave a bad correlation with the performance if the\ndynamics were also stochastic.",
    "start": "2665840",
    "end": "2671610"
  },
  {
    "text": " I also [INAUDIBLE]\nhow much time does it",
    "start": "2671610",
    "end": "2677420"
  },
  {
    "text": "take to train decision\ntransformer in general? So it takes about a few hours.",
    "start": "2677420",
    "end": "2684390"
  },
  {
    "text": "So I want to say like\nabout four to five hours,",
    "start": "2684390",
    "end": "2692220"
  },
  {
    "text": "depending on what\nquality GPU you use. But yeah that's a\nreasonable estimate.",
    "start": "2692220",
    "end": "2697550"
  },
  {
    "text": "Yeah, got it. Thanks. ",
    "start": "2697550",
    "end": "2703720"
  },
  {
    "text": "OK, so actually while\ndoing this project, we thought of a\nbaseline which we",
    "start": "2703720",
    "end": "2711100"
  },
  {
    "text": "were surprised is not there\nin previous literature on offline RL but\nmakes very much sense.",
    "start": "2711100",
    "end": "2716710"
  },
  {
    "text": "And we thought we\nshould also think about whether decision\ntransformer they're actually doing something very\nsimilar to that baseline.",
    "start": "2716710",
    "end": "2723160"
  },
  {
    "text": "And the baseline is what we call\nas percent behavioral cloning. So behavioral\ncloning what it does",
    "start": "2723160",
    "end": "2728440"
  },
  {
    "text": "is basically it\nignores the returns and simply imitates the agent by\njust trying to map the actions",
    "start": "2728440",
    "end": "2739270"
  },
  {
    "text": "given the constraints. This is not a good idea\nwith an offline data",
    "start": "2739270",
    "end": "2745865"
  },
  {
    "text": "set which will have\ntrajectories of both low returns and high returns.",
    "start": "2745865",
    "end": "2750869"
  },
  {
    "text": "So traditional\nbehaviour cloning, it's common to see that as a\nbaseline in offline RL methods.",
    "start": "2750870",
    "end": "2756694"
  },
  {
    "text": " And unless you have a very\nhigh quality data set,",
    "start": "2756695",
    "end": "2762810"
  },
  {
    "text": "it is not a good\nbaseline for offline RL. However, there is\na version that we",
    "start": "2762810",
    "end": "2767990"
  },
  {
    "text": "call as percent\nBC, which actually makes quite a lot of sense. And in this version, we filter\nout the top trajectories",
    "start": "2767990",
    "end": "2776000"
  },
  {
    "text": "from our offline dataset\nand stop the ones that have the highest rewards. You know the rewards\nfor each transition.",
    "start": "2776000",
    "end": "2783140"
  },
  {
    "text": "You calculate the returns\non the trajectories. And we take the trajectories\nwith the highest returns",
    "start": "2783140",
    "end": "2789170"
  },
  {
    "text": "and keep a certain\npercentage of them, which is going to be\nhyperparameter here.",
    "start": "2789170",
    "end": "2795450"
  },
  {
    "text": "And once you keep those top\nfraction of your trajectories, you then just ask your\nmodel to imitate them.",
    "start": "2795450",
    "end": "2803330"
  },
  {
    "text": " So imitation learning\nalso uses, especially when",
    "start": "2803330",
    "end": "2809557"
  },
  {
    "text": "it's used in the form\nof behavioral cloning, it uses supervised\nlearning essentially, it's a supervised\nlearning problem.",
    "start": "2809557",
    "end": "2815360"
  },
  {
    "text": "So you could actually\nalso get supervised learning objective functions\nif you did this filtering step.",
    "start": "2815360",
    "end": "2821559"
  },
  {
    "text": " And what do you find?",
    "start": "2821560",
    "end": "2826720"
  },
  {
    "text": "Actually that for the moderate\nand high data regimes, decision transformers\nare actually very comparable to percent BC.",
    "start": "2826720",
    "end": "2833210"
  },
  {
    "text": "So it's a very strong\nbaseline, which I think all of future work\nin offline RL should include.",
    "start": "2833210",
    "end": "2838540"
  },
  {
    "text": "There's actually\nan ICLR submission from last week, which has a much\nmore detailed analysis on just",
    "start": "2838540",
    "end": "2845410"
  },
  {
    "text": "this baseline that we\nintroduced in this paper. And what we do find is\nthat for low data regimes,",
    "start": "2845410",
    "end": "2852430"
  },
  {
    "text": "the decision transformer\ndoes much better than percent behavior cloning. So this is for the\nAtari benchmarks",
    "start": "2852430",
    "end": "2858070"
  },
  {
    "text": "where, like I\npreviously mentioned, we have a much smaller data\nset as compared to the MuJoCo",
    "start": "2858070",
    "end": "2865809"
  },
  {
    "text": "environments. And here we find that even after\nvarying the different fraction",
    "start": "2865810",
    "end": "2871599"
  },
  {
    "text": "of the percentage\nhyperparameter here, we are generally not able to\nget the strong performance",
    "start": "2871600",
    "end": "2876970"
  },
  {
    "text": "the decision transformer gets. So 10% BC basically means that\nwe filter and keep the top 10%",
    "start": "2876970",
    "end": "2884770"
  },
  {
    "text": "of the trajectories. If you go even lower,\nthen the dataset becomes really small so the\nbaseline becomes meaningless.",
    "start": "2884770",
    "end": "2893050"
  },
  {
    "text": "But for even though\nreasonable ranges, we never find the\nperformance matching that of decision transformers\nfor these Atari benchmarks.",
    "start": "2893050",
    "end": "2903500"
  },
  {
    "text": "Aditya, if I may. So notice in table\n3, for example,",
    "start": "2903500",
    "end": "2908760"
  },
  {
    "text": "which is not this\ntable but the one just before it in the paper. There's a report on the CQL\nperformance, which to me also",
    "start": "2908760",
    "end": "2914810"
  },
  {
    "text": "feels intuitively pretty\nsimilar to the percent BC, in the sense of you\npick trajectories",
    "start": "2914810",
    "end": "2921162"
  },
  {
    "text": "you know are performing\nwell and you try and stay roughly within sort of\nthe same kind of kind of policy distribution and\nstate space distribution.",
    "start": "2921162",
    "end": "2931880"
  },
  {
    "text": "I was curious on this one. Do you have a sense of what\nthe CQL performance was relative to, say, the\npercent BC performance here?",
    "start": "2931880",
    "end": "2940640"
  },
  {
    "text": "So that's a great question. So the question is\nthat even for CQL you",
    "start": "2940640",
    "end": "2946410"
  },
  {
    "text": "rely on this notion\nof pessimism where you want to pick trajectories\nwhere you're more confident in.",
    "start": "2946410",
    "end": "2953339"
  },
  {
    "text": "And try to make sure policy\nremains in that region. So I don't have the numbers\nof CQL on this table.",
    "start": "2953340",
    "end": "2961420"
  },
  {
    "text": "But if you look at the\ndetailed results for Atari, then I think they should\nhave the CQL for sure,",
    "start": "2961420",
    "end": "2971790"
  },
  {
    "text": "because that's the numbers\nwe are reporting here. So I can tell you what the\nCQL performance is actually",
    "start": "2971790",
    "end": "2979560"
  },
  {
    "text": "pretty good and it's very\ncompetitive with the decision transformer for Atari.",
    "start": "2979560",
    "end": "2986690"
  },
  {
    "text": "So this TD learning\nbaseline here is CQL. So by naturally by\nextension, I would",
    "start": "2986690",
    "end": "2993500"
  },
  {
    "text": "imagine it's doing\nbetter than percent BC. Yeah, and I apologize\nif this was mentioned",
    "start": "2993500",
    "end": "2999615"
  },
  {
    "text": "and I just missed it. But do you have\nthe sense that this is basically like\na failure of CQL to be able to extrapolate well\nor sort of stitch together",
    "start": "2999615",
    "end": "3008350"
  },
  {
    "text": "different parts of trajectories,\nwhereas the decision transformer can sort of\nmake that extrapolation",
    "start": "3008350",
    "end": "3013390"
  },
  {
    "text": "between-- you have\nthe first half of one trajectory is really good. The second half of one\ntrajectory is really good. And so you can actually\npiece those together",
    "start": "3013390",
    "end": "3018990"
  },
  {
    "text": "with the decision transform,\nwhere you can't necessarily do that with CQL because\nthe path connecting those may not necessarily be well\ncovered by the behavior policy.",
    "start": "3018990",
    "end": "3028059"
  },
  {
    "text": "Yeah, yeah. And this actually goes to one of\nthe intuitions which I did not",
    "start": "3028060",
    "end": "3035650"
  },
  {
    "text": "emphasize too much, but we have\na discussion on that paper, where essentially\nwhy do we expect",
    "start": "3035650",
    "end": "3041559"
  },
  {
    "text": "a transformer or any\nmodel for that matter to look at offline\ndata that's suboptimal",
    "start": "3041560",
    "end": "3047110"
  },
  {
    "text": "and get something\nin a policy that generates optimal rollouts? The intuition is that,\nas Scott was mentioning,",
    "start": "3047110",
    "end": "3055420"
  },
  {
    "text": "you could perhaps stitch\ntogether good behaviors",
    "start": "3055420",
    "end": "3061329"
  },
  {
    "text": "from suboptimal trajectories. And that stitching\ncould perhaps lead to a behavior that was\nbetter than anything",
    "start": "3061330",
    "end": "3067450"
  },
  {
    "text": "you saw in individual\ntrajectories in a data set. It's something we find early\nevidence of in small scale",
    "start": "3067450",
    "end": "3077500"
  },
  {
    "text": "experiments for graphs. And that's really our\nhope also that something",
    "start": "3077500",
    "end": "3083560"
  },
  {
    "text": "that the transformer\nis very good at, because it can attend\nto very long sequences.",
    "start": "3083560",
    "end": "3089060"
  },
  {
    "text": "So it could identify\nthose segments of behavior which when stitched\ntogether would give you",
    "start": "3089060",
    "end": "3095890"
  },
  {
    "text": "optimal behavior. ",
    "start": "3095890",
    "end": "3104079"
  },
  {
    "text": "And it's very much possible. And that is something unique\nto decision transformers. And something like CQL would\nnot be able to do percent BC",
    "start": "3104080",
    "end": "3113410"
  },
  {
    "text": "because it's filtering out\nthe data is automatically being limited and not\nbeing able to do that,",
    "start": "3113410",
    "end": "3118750"
  },
  {
    "text": "because those segments\nof good behavior could be in trajectories\nwhich overall do not have a high return.",
    "start": "3118750",
    "end": "3125119"
  },
  {
    "text": "So if you filter them\nout, you are losing out on that information. ",
    "start": "3125120",
    "end": "3133560"
  },
  {
    "text": "OK, so I said there is a\nhyperparameter, the context in K. And like with\nmost of perception,",
    "start": "3133560",
    "end": "3141369"
  },
  {
    "text": "one of the big advantages\nof transformers as opposed to other sequence\nmodels like LSTMs",
    "start": "3141370",
    "end": "3146940"
  },
  {
    "text": "is that they can process\nfairly large sequences. And here at first glance, it\nmight seem that being Markovian",
    "start": "3146940",
    "end": "3156720"
  },
  {
    "text": "would have been helpful for RL,\nwhich also was a question that was raised earlier.",
    "start": "3156720",
    "end": "3162390"
  },
  {
    "text": "So we did this experiment where\nwe did compare performance with context in K plus 1.",
    "start": "3162390",
    "end": "3168060"
  },
  {
    "text": "And here, we had context\nlength between 30",
    "start": "3168060",
    "end": "3173190"
  },
  {
    "text": "for the environments\nand 50 for Pong. And we find that increasing\nthe context length",
    "start": "3173190",
    "end": "3179010"
  },
  {
    "text": "is very, very important\nto get good performance.",
    "start": "3179010",
    "end": "3184080"
  },
  {
    "start": "3184080",
    "end": "3194170"
  },
  {
    "text": "OK, now so far I've showed\nyou how decision transformer, which is very simple.",
    "start": "3194170",
    "end": "3201620"
  },
  {
    "text": "There was no slide\nI had, which was going into the details of\ndynamic programming, which is the crux of most RL.",
    "start": "3201620",
    "end": "3209080"
  },
  {
    "text": "This was just pure\nsupervised learning in an autoregressive\nframework that was getting us this good performance.",
    "start": "3209080",
    "end": "3215350"
  },
  {
    "text": " What about cases where\nthis approach actually",
    "start": "3215350",
    "end": "3222200"
  },
  {
    "text": "starts outperforming some of\nthe traditional methods RL? So to probe a\nlittle bit further,",
    "start": "3222200",
    "end": "3228830"
  },
  {
    "text": "we started looking at\nsparse reward environments. And basically, we just took our\nexisting MuJoCo environments.",
    "start": "3228830",
    "end": "3236210"
  },
  {
    "text": "And then instead of giving\nit the information for reward for every transition,\nwe Fed it the",
    "start": "3236210",
    "end": "3243050"
  },
  {
    "text": "cumulative reward at the\nend of the trajectory. So every transition will have\nzero reward except the very end",
    "start": "3243050",
    "end": "3248600"
  },
  {
    "text": "where you get the\nentire reward at once. It's a very sparse reward\nscenario for that reason.",
    "start": "3248600",
    "end": "3256170"
  },
  {
    "text": "And here we find that, compared\nto the original dense results,",
    "start": "3256170",
    "end": "3262309"
  },
  {
    "text": "the delayed results for DT will\ndeteriorate a little bit, which",
    "start": "3262310",
    "end": "3267770"
  },
  {
    "text": "was expected because\nnow you are withholding some of the more fine grained\ninformation at every time step.",
    "start": "3267770",
    "end": "3273950"
  },
  {
    "text": "But the drop is not too\nsignificant compared to the original DT\nperformance here.",
    "start": "3273950",
    "end": "3281329"
  },
  {
    "text": "Whereas for something\nlike CQL, there is a drastic drop\nin performance. So CQL suffers quite a lot\nin sparse reward scenarios,",
    "start": "3281330",
    "end": "3291450"
  },
  {
    "text": "but decision\ntransformer does not. And just for\ncompleteness, we also have performance of the\nbehavior cloning and percent",
    "start": "3291450",
    "end": "3297990"
  },
  {
    "text": "behavior cloning\nbecause they don't look at reward information,\nexcept maybe percent BC",
    "start": "3297990",
    "end": "3303820"
  },
  {
    "text": "looks at only for\npre-processing the data set. These are agnostic to\nwhether the environments",
    "start": "3303820",
    "end": "3309789"
  },
  {
    "text": "have sparse rewards or not.  Would you expect\nthis to be different",
    "start": "3309790",
    "end": "3316290"
  },
  {
    "text": "if you were doing online RL? ",
    "start": "3316290",
    "end": "3324480"
  },
  {
    "text": "What's the intuition\nfor it being different? A priori, I would\nsay no, but maybe I'm",
    "start": "3324480",
    "end": "3331320"
  },
  {
    "text": "missing out on a key piece of\nintuition behind that question. ",
    "start": "3331320",
    "end": "3337820"
  },
  {
    "text": "I think that because you're\ntraining like offline, the next input will always\nbe the correct action",
    "start": "3337820",
    "end": "3346310"
  },
  {
    "text": "in that sense. So you don't just deviate\nand let go off the rails technically, because\nyou just don't know.",
    "start": "3346310",
    "end": "3352650"
  },
  {
    "text": "So I could see how online would\nhave a really hard, cold start",
    "start": "3352650",
    "end": "3357980"
  },
  {
    "text": "basically because it\njust doesn't know. And it's just\ntapping in the dark until it maybe eventually\nhits the jackpot.",
    "start": "3357980",
    "end": "3366170"
  },
  {
    "text": "Right, right, I think I\nagree, that's a good piece of intuition out there.",
    "start": "3366170",
    "end": "3372170"
  },
  {
    "text": " I think here that offline\nRL is really getting rid",
    "start": "3372170",
    "end": "3378230"
  },
  {
    "text": "of the trial and\nerror aspect of it and for sparse reward\nenvironments that",
    "start": "3378230",
    "end": "3384260"
  },
  {
    "text": "would be harder. So the drop in DT performance\nshould be more prominent there.",
    "start": "3384260",
    "end": "3394730"
  },
  {
    "text": "I'm not sure how\nit would compare with the job performance\nfor other algorithms.",
    "start": "3394730",
    "end": "3401990"
  },
  {
    "text": "But it does seem like\nan interesting setup to test the gain.",
    "start": "3401990",
    "end": "3409930"
  },
  {
    "text": "Well, Aditya, maybe\nI'm wrong here. But my understanding with the\ndecision transformer as well as this is the critical\npiece that in the training",
    "start": "3409930",
    "end": "3417525"
  },
  {
    "text": "you use the rewards\nto go, right? So is it not the sense\nthat essentially like",
    "start": "3417525",
    "end": "3423130"
  },
  {
    "text": "for each trajectory, from\nthe initial state based on the training\nregime, the model",
    "start": "3423130",
    "end": "3428470"
  },
  {
    "text": "has access to whether or not\nthe final result was a success or failure, right?",
    "start": "3428470",
    "end": "3435610"
  },
  {
    "text": "But that's a unique aspect\nof the training regime for decision transformers. Whereas in CQL, my\nunderstanding is",
    "start": "3435610",
    "end": "3443920"
  },
  {
    "text": "that it's based on a per\ntransition training regime. So each transition\nis decoupled somewhat",
    "start": "3443920",
    "end": "3451090"
  },
  {
    "text": "to what the final reward\nwas, is that correct? Yes, although like\none difficulty which",
    "start": "3451090",
    "end": "3458410"
  },
  {
    "text": "at first glance you can imagine\nthe decision transformer having is that\ninitial token will not",
    "start": "3458410",
    "end": "3464950"
  },
  {
    "text": "change throughout\nthe trajectory, because it's a sparse\nreward scenario.",
    "start": "3464950",
    "end": "3471250"
  },
  {
    "text": "Except the very last token\nwhere it will drop down to 0 all of a sudden. This token remains\nthe same throughout",
    "start": "3471250",
    "end": "3479680"
  },
  {
    "text": "But I think you're right that\nmaybe just even at the start,",
    "start": "3479680",
    "end": "3484780"
  },
  {
    "text": "feeding it in a manner which\nlooks at the future rewards that you need to get to is\nperhaps one part of the reason",
    "start": "3484780",
    "end": "3492190"
  },
  {
    "text": "why the drop in performance\nis not noticeable.",
    "start": "3492190",
    "end": "3498140"
  },
  {
    "text": "Yeah, I mean, I guess, one\nsort of ablation experiment here would be if you\nchange the training regime so that only the last\ntrajectory had the reward.",
    "start": "3498140",
    "end": "3506589"
  },
  {
    "text": "But I'm trying to think about\nwhether or not that would just be compensated for by sort of\nthe attention mechanism anyway,",
    "start": "3506590",
    "end": "3515000"
  },
  {
    "text": "and vice versa, right, if\nyou embedded that reward information into the\nCQL training procedure as well, I'd be curious to\nsee what would happen there.",
    "start": "3515000",
    "end": "3523559"
  },
  {
    "text": "Yeah, good experiments. ",
    "start": "3523560",
    "end": "3531369"
  },
  {
    "text": "OK, related to this, there's\nanother environment we tested. I gave you a brief preview\nof the results in one",
    "start": "3531370",
    "end": "3538120"
  },
  {
    "text": "of the earlier slides. This is called a\nKey-To-Door environment. And it has three phases. So in the first phase, the agent\nis placed in a room with a key.",
    "start": "3538120",
    "end": "3548920"
  },
  {
    "text": "A good agent will\npick up the key. And then in phase two, it will\nbe placed in an empty room.",
    "start": "3548920",
    "end": "3554410"
  },
  {
    "text": "And in phase three, it will be\nplaced in a room with a door where it will\nactually use the key",
    "start": "3554410",
    "end": "3560500"
  },
  {
    "text": "that it collected in phase one,\nif it did, to open the door.",
    "start": "3560500",
    "end": "3566110"
  },
  {
    "text": "So essentially,\nthe agent is going to receive a binary reward\ncorresponding to whether it",
    "start": "3566110",
    "end": "3572410"
  },
  {
    "text": "reached and opened the door\nin phase three, conditioned on the fact that it did pick\nup the key in phase one.",
    "start": "3572410",
    "end": "3583130"
  },
  {
    "text": "So there is this notion on\nthat you want to assign credit to something that happened-- to\nan event that happened really",
    "start": "3583130",
    "end": "3589970"
  },
  {
    "text": "in the past. So it's a very challenging\nand sensible scenario",
    "start": "3589970",
    "end": "3595862"
  },
  {
    "text": "if you wanted to ask the\nmodels for how well they are at long term credit assignment. And here, we find that--",
    "start": "3595862",
    "end": "3603500"
  },
  {
    "text": "so we tested it for different\namounts of trajectories. So here the number\nof trajectories basically says how often\nwould you actually see",
    "start": "3603500",
    "end": "3610609"
  },
  {
    "text": "this kind of behavior? And the decision transformer\nand percent behavior cloning,",
    "start": "3610610",
    "end": "3618950"
  },
  {
    "text": "both of these actually\nbaselines do much better than other models which\nstruggle at this task.",
    "start": "3618950",
    "end": "3627109"
  },
  {
    "text": " There's a related\nexperiment there",
    "start": "3627110",
    "end": "3633089"
  },
  {
    "text": "which is also of interest. So generally a lot\nof other algorithms",
    "start": "3633090",
    "end": "3638940"
  },
  {
    "text": "have this notion of\nan actor and a critic. Actor is basically\nsomeone that takes",
    "start": "3638940",
    "end": "3644340"
  },
  {
    "text": "actions, conditioned on the\nstates and think of a policy. A critic is basically\nevaluating how good",
    "start": "3644340",
    "end": "3651630"
  },
  {
    "text": "these actions are in terms\nof achieving a long term-- in terms of the cumulative sum\nof rewards in the long term.",
    "start": "3651630",
    "end": "3661170"
  },
  {
    "text": "This is a good\nenvironment because we can see how well the\ndecision transformer would do",
    "start": "3661170",
    "end": "3668100"
  },
  {
    "text": "if it was trained as a critic. So here what we did\nis, instead of having",
    "start": "3668100",
    "end": "3673710"
  },
  {
    "text": "the actions of\nthe output target, what if we substituted\nthat with the rewards?",
    "start": "3673710",
    "end": "3681050"
  },
  {
    "text": "So that's very much possible. We can again use the same\ncausal transformer machinery",
    "start": "3681050",
    "end": "3687650"
  },
  {
    "text": "to only look at transitions\nin the previous timestep and try to pick the reward.",
    "start": "3687650",
    "end": "3692990"
  },
  {
    "text": "And here, we see this\ninteresting pattern where in the three\nphases that we",
    "start": "3692990",
    "end": "3698270"
  },
  {
    "text": "had in that Key-To-Door\nenvironment, we do see the reward\nprobability changing very much",
    "start": "3698270",
    "end": "3704480"
  },
  {
    "text": "in how we expect. So basically, the three\nscenarios are the agent.",
    "start": "3704480",
    "end": "3710089"
  },
  {
    "text": "The first scenario,\nlet's look at blue, in which the agent does not\npick up the key, in phase one.",
    "start": "3710090",
    "end": "3717020"
  },
  {
    "text": "So the reward probability\nthey all started on the same but as it becomes apparent\nthat the agent is not",
    "start": "3717020",
    "end": "3722840"
  },
  {
    "text": "going to pick up the key,\nthe reward starts going down. And then it stays\nvery much close to 0",
    "start": "3722840",
    "end": "3728870"
  },
  {
    "text": "throughout the\nepisode because there is no way you will\nhave the key to open",
    "start": "3728870",
    "end": "3734450"
  },
  {
    "text": "the door in the future phases. If you pick up the key,\nthere are two possibilities",
    "start": "3734450",
    "end": "3745060"
  },
  {
    "text": "which remain, which\nare essentially the same in phase two where\nyou are in an empty room, which",
    "start": "3745060",
    "end": "3751869"
  },
  {
    "text": "is just a distractor to make\nthe episode really long. But at the very end,\nthe two possibilities",
    "start": "3751870",
    "end": "3758473"
  },
  {
    "text": "are one that you take\nthe key and you actually reach the door, which is the\none we see in orange and brown",
    "start": "3758473",
    "end": "3765430"
  },
  {
    "text": "here, where you see that the\nreward probability goes up. And there's this\nother possibility",
    "start": "3765430",
    "end": "3770560"
  },
  {
    "text": "that you actually pick up the\nkey but do not reach the door. In which case, again,\nwe start seeing that the reward probability\nthat's predicted",
    "start": "3770560",
    "end": "3778420"
  },
  {
    "text": "starts going down. So the takeaway\nfrom this experiment",
    "start": "3778420",
    "end": "3784700"
  },
  {
    "text": "is that decision\ntransformers are not just great actors,\nwhich is what we've been seeing so\nfar in the results",
    "start": "3784700",
    "end": "3792830"
  },
  {
    "text": "from the optimized policy. But they're also very\nimpressive critics in doing this long\nterm assignment where",
    "start": "3792830",
    "end": "3800060"
  },
  {
    "text": "the reward is also very sparse. So Aditya, just\nto be correct, are they predicting the rewards\nto go at each timestep",
    "start": "3800060",
    "end": "3806953"
  },
  {
    "text": "or is this like\nthey have to make the rewards at each timestep,\nare they predicting?",
    "start": "3806953",
    "end": "3812890"
  },
  {
    "text": "So this is the rewards to go. And I can also check--\nmy impression was",
    "start": "3812890",
    "end": "3819783"
  },
  {
    "text": "in this particular\nexperiment it didn't really make a difference, whether you\nwere predicting rewards to go or the actual rewards.",
    "start": "3819783",
    "end": "3826710"
  },
  {
    "text": "But I think it was\nreturns-to-go for this one. So how do you get the\nprobability distribution",
    "start": "3826710",
    "end": "3832109"
  },
  {
    "text": "of the rewards? Is it just like\nyou just evaluate a lot of different episodes\nand just give the rewards? Or are you explicitly predicting\nsome sort of distribution?",
    "start": "3832110",
    "end": "3840750"
  },
  {
    "text": "So this is a binary\nreward, so you can have-- so you can have a\nprobabilistic outcome.",
    "start": "3840750",
    "end": "3847099"
  },
  {
    "text": "Got it.  [INAUDIBLE] you have a question.",
    "start": "3847100",
    "end": "3853090"
  },
  {
    "text": "Yes, so generally, we\nwill call something predicts a state value or\nstate action value as critic.",
    "start": "3853090",
    "end": "3860619"
  },
  {
    "text": "But in this case, you ask\na decision transformer to only predict rewards.",
    "start": "3860620",
    "end": "3866220"
  },
  {
    "text": "So why choose to\ncall it a critic? So I think the analogy\nhere gets a bit clearer",
    "start": "3866220",
    "end": "3874070"
  },
  {
    "text": "with returns-to-go. Like if you think\nabout returns-to-go, it's really capturing that\nessence that you want to see",
    "start": "3874070",
    "end": "3879740"
  },
  {
    "text": "the future rewards that-- So I'm assuming it's just\ngoing to predict the return",
    "start": "3879740",
    "end": "3885050"
  },
  {
    "text": "to go instead of single\nstate reward, right? Yeah, yeah.",
    "start": "3885050",
    "end": "3890320"
  },
  {
    "text": "OK, so if you're\ngoing to predict returns-to-go is kind\nof counterintuitive",
    "start": "3890320",
    "end": "3895450"
  },
  {
    "text": "to me, because in phase\none when the agent is still in the key room, I think it\nshould have high returns-to-go",
    "start": "3895450",
    "end": "3903100"
  },
  {
    "text": "if it picked up the key. But in the plot,\nin the key room,",
    "start": "3903100",
    "end": "3911859"
  },
  {
    "text": "the agent picked up\nthe key and the agent that didn't pick up key\nhas the same kind of level",
    "start": "3911860",
    "end": "3918575"
  },
  {
    "text": "of returns-to-go, so that's\nquite counterintuitive to me.",
    "start": "3918575",
    "end": "3924619"
  },
  {
    "text": "I think this is reflecting\non a good property, which",
    "start": "3924620",
    "end": "3930080"
  },
  {
    "text": "is that your distribution-- if you interpret returns-to-go\nin the right way, in phase one,",
    "start": "3930080",
    "end": "3937070"
  },
  {
    "text": "you don't know which\nof these three outcomes are really possible. And phase one, I'm talking about\nthe very beginning basically,",
    "start": "3937070",
    "end": "3943369"
  },
  {
    "text": "slowly we learn about it. But essentially, in phase one if\nyou see the returns-to-go as 1",
    "start": "3943370",
    "end": "3951800"
  },
  {
    "text": "or 0 for-- all three possibilities\nare equally likely.",
    "start": "3951800",
    "end": "3958760"
  },
  {
    "text": "And all three possibilities-- so if we try to evaluate\nthe predicted rewards",
    "start": "3958760",
    "end": "3965150"
  },
  {
    "text": "for these possibilities,\nit should be the same, because we really\nhaven't done or we",
    "start": "3965150",
    "end": "3970820"
  },
  {
    "text": "don't know what's going\nto happen in phase three.",
    "start": "3970820",
    "end": "3976250"
  },
  {
    "text": "Sorry, it's my mistake,\nbecause previously I thought a green line\nis the agent which",
    "start": "3976250",
    "end": "3982099"
  },
  {
    "text": "doesn't pick up the key. But it turns out the\nblue line is agent which doesn't pick up the key.",
    "start": "3982100",
    "end": "3988310"
  },
  {
    "text": "So yeah, it's my mistake. It makes sense to me. Thank you. I also must note pretty\nclear from the paper,",
    "start": "3988310",
    "end": "3995305"
  },
  {
    "text": "but did you do\nexperiments where you're predicting what actions and\nwhat are the rewards to go? Can it improve performance\nif it didn't work together?",
    "start": "3995305",
    "end": "4003780"
  },
  {
    "text": "So actually, we did some\npreliminary experiments on that. And it didn't help us much.",
    "start": "4003780",
    "end": "4009100"
  },
  {
    "text": "However, I do want\nto, again, put in a plug for a paper that\ncame concurrently, trajectory",
    "start": "4009100",
    "end": "4014130"
  },
  {
    "text": "transformer which tried\nto predict states, actions, and rewards,\nactually, all three of them.",
    "start": "4014130",
    "end": "4022800"
  },
  {
    "text": "They were in a model based\nsetup where it made sense also to try to learn each of the\ncomponents like the transition",
    "start": "4022800",
    "end": "4030780"
  },
  {
    "text": "dynamics, the policy, and maybe\neven the critic and their setup together.",
    "start": "4030780",
    "end": "4035880"
  },
  {
    "text": "We did not find any\nsignificant improvements. So in favor of simplicity\nand keeping it model free,",
    "start": "4035880",
    "end": "4042300"
  },
  {
    "text": "we did not try to\npredict them together. Got it. ",
    "start": "4042300",
    "end": "4051050"
  },
  {
    "text": "OK, so the summary, we\nshowed decision transformers which is a first work\nin trying to approach RL",
    "start": "4051050",
    "end": "4060900"
  },
  {
    "text": "based on sequence modeling. The main advantages\nover previous approaches",
    "start": "4060900",
    "end": "4066230"
  },
  {
    "text": "is it's simple by design. The hope is that\nfurther extensions",
    "start": "4066230",
    "end": "4071660"
  },
  {
    "text": "we will find it to scale\nmuch better than existing RL algorithms.",
    "start": "4071660",
    "end": "4076849"
  },
  {
    "text": "It is stable to train, because\nthe loss functions we are using have been tested and\niterated upon a lot",
    "start": "4076850",
    "end": "4084140"
  },
  {
    "text": "by research in perception. And in the future,\nwe will also hope",
    "start": "4084140",
    "end": "4090590"
  },
  {
    "text": "that because of\nthese similarities in the architecture\nand the training",
    "start": "4090590",
    "end": "4096979"
  },
  {
    "text": "with how perception based\ntasks are conducted, it would also be easy\nto integrate them",
    "start": "4096979",
    "end": "4102720"
  },
  {
    "text": "within this loop. So the states, the actions,\nor even the task of interest",
    "start": "4102720",
    "end": "4108950"
  },
  {
    "text": "they could be specified based\non perceptual based sensors.",
    "start": "4108950",
    "end": "4115490"
  },
  {
    "text": "So you could have\na target task being specified by a natural\nlanguage instruction.",
    "start": "4115490",
    "end": "4122278"
  },
  {
    "text": "And because these\nmodels can very well play with these kinds\nof inputs, the hope",
    "start": "4122279",
    "end": "4128210"
  },
  {
    "text": "is that they would be easy to\nintegrate within the decision making process.",
    "start": "4128210",
    "end": "4135890"
  },
  {
    "text": "And empirically saw\nstrong performance in a range of\noffline RL settings",
    "start": "4135890",
    "end": "4141290"
  },
  {
    "text": "and especially good\nperformance in scenarios which required us to do\nlong credit assignment.",
    "start": "4141290",
    "end": "4149410"
  },
  {
    "text": "So there's a lot of future work. This is definitely not the end.",
    "start": "4149410",
    "end": "4156139"
  },
  {
    "text": "This is a first\nwork in rethinking how do we build RL agents\nthat can scale and generalize.",
    "start": "4156140",
    "end": "4165759"
  },
  {
    "text": "A few things that I\npicked out which I feel would be very\nexciting to extend--",
    "start": "4165760",
    "end": "4172210"
  },
  {
    "text": "the first is multi modality. So really, one of\nour big motivations",
    "start": "4172210",
    "end": "4177729"
  },
  {
    "text": "with going after\nthese kinds of models is that we can combine\ndifferent kinds of inputs",
    "start": "4177729",
    "end": "4183939"
  },
  {
    "text": "both online and offline to\nreally build decision making",
    "start": "4183939",
    "end": "4189160"
  },
  {
    "text": "agents which work like humans. We process so many inputs around\nus in different modalities",
    "start": "4189160",
    "end": "4195550"
  },
  {
    "text": "and we act on them. So we do take decisions. And we want the same to\nhappen in artificial agents.",
    "start": "4195550",
    "end": "4202720"
  },
  {
    "text": "And maybe decision transformers\nis one important step in that route.",
    "start": "4202720",
    "end": "4208525"
  },
  {
    "text": " Multi-task, so I\nshowed or described",
    "start": "4208525",
    "end": "4215929"
  },
  {
    "text": "very limited form of\nmultitasking here, which was based on the\ndesired returns-to-go.",
    "start": "4215930",
    "end": "4223030"
  },
  {
    "text": "But it could be more richer in\nterms of specifying a command",
    "start": "4223030",
    "end": "4229059"
  },
  {
    "text": "to be a robot or a\ndesired goal state which could be, for\nexample, even visual.",
    "start": "4229060",
    "end": "4235389"
  },
  {
    "text": "So trying to better explore\nthe different multitask capabilities of\nthis model but also",
    "start": "4235390",
    "end": "4242650"
  },
  {
    "text": "be an interesting extension. Finally, multi-agent\nas human beings",
    "start": "4242650",
    "end": "4250960"
  },
  {
    "text": "we never act in isolation. We are always acting within\nan environment that involves",
    "start": "4250960",
    "end": "4258040"
  },
  {
    "text": "many, many more agents. Things become partially\nobservable in those scenarios, which plays to the strengths\nof decision transformers",
    "start": "4258040",
    "end": "4266590"
  },
  {
    "text": "being non-Markovian by design. So I think there is great\npossibilities of exploring even",
    "start": "4266590",
    "end": "4273250"
  },
  {
    "text": "multi-agent scenarios where\nthe fact that transformers can process very large sequences\ncompared to existing algorithms",
    "start": "4273250",
    "end": "4281199"
  },
  {
    "text": "could again help build\nbetter models of other agents in your environment and act.",
    "start": "4281200",
    "end": "4287050"
  },
  {
    "text": " Yeah, these are some\nuseful links in case",
    "start": "4287050",
    "end": "4292769"
  },
  {
    "text": "you're interested. The project website, the paper,\nand the code are all public.",
    "start": "4292770",
    "end": "4299430"
  },
  {
    "text": "And I'm happy to take\nany more questions. OK, so Aditya, thanks\nfor the good talk.",
    "start": "4299430",
    "end": "4305470"
  },
  {
    "text": "We really appreciate it. Everyone had a good time for it. I think we are like\nnear the class limit.",
    "start": "4305470",
    "end": "4313110"
  },
  {
    "text": "So usually, I have\na round of questions for the speaker, that is like\nthe students usually know.",
    "start": "4313110",
    "end": "4318480"
  },
  {
    "text": "but if someone is asking you\njust ask general questions",
    "start": "4318480",
    "end": "4326190"
  },
  {
    "text": "before we stop the recording. So if anyone wants to\nleave earlier at this time,",
    "start": "4326190",
    "end": "4331590"
  },
  {
    "text": "just feel free to\nask your questions. Otherwise, I would\njust like continue on.",
    "start": "4331590",
    "end": "4338068"
  },
  {
    "text": "So what do you think is the\nfuture of like transformers in RL? Do you think they\nwill take over--",
    "start": "4338068",
    "end": "4343090"
  },
  {
    "text": "they've already taken over\nlike language and vision, or like model based and\nmodel free learning, do you think we will see a lot\nmore like transformers pop up",
    "start": "4343090",
    "end": "4349690"
  },
  {
    "text": "in RL literature? Yes, I think we'll see a\nflurry of work, if not already.",
    "start": "4349690",
    "end": "4359500"
  },
  {
    "text": "There's so many works\nusing transformers at this year's ICLR conference.",
    "start": "4359500",
    "end": "4366360"
  },
  {
    "text": "Having said that, I feel\nthat an important piece of the puzzle that needs to\nbe solved is exploration.",
    "start": "4366360",
    "end": "4372420"
  },
  {
    "text": "It's non-trivial. And it will have to,\nmy guess is that, you",
    "start": "4372420",
    "end": "4380610"
  },
  {
    "text": "will have to forego\nsome of the advantages that I talked about\nfor transformers",
    "start": "4380610",
    "end": "4385860"
  },
  {
    "text": "in terms of loss functions to\nactually enable exploration. So it remains to be seen whether\nthose modified, those functions",
    "start": "4385860",
    "end": "4396900"
  },
  {
    "text": "for exploration actually hurt\nperformance significantly.",
    "start": "4396900",
    "end": "4402449"
  },
  {
    "text": "But as long as we cannot\ncross that bottleneck,",
    "start": "4402450",
    "end": "4409680"
  },
  {
    "text": "I do not want to commit that\nthis is indeed the future of RL",
    "start": "4409680",
    "end": "4414870"
  },
  {
    "text": "Got It. Also do you think something-- [INTERPOSING VOICES] Can I ask a follow up question?",
    "start": "4414870",
    "end": "4420870"
  },
  {
    "text": "Sure. I'm not sure I\nunderstood that point. So you're saying that in order\nto apply transformers in RL",
    "start": "4420870",
    "end": "4426810"
  },
  {
    "text": "to do exploration, there have\nto be particular loss functions and they're tricky\nfor some reason?",
    "start": "4426810",
    "end": "4433450"
  },
  {
    "text": "Could you explain\nmore, like what are the modified loss functions\nand why do they seem tricky?",
    "start": "4433450",
    "end": "4438600"
  },
  {
    "text": "So essentially in\nexploration you have to do the opposite\nof exploitation,",
    "start": "4438600",
    "end": "4444880"
  },
  {
    "text": "which is not greedy. And there is right now\nno nothing in-built",
    "start": "4444880",
    "end": "4450475"
  },
  {
    "text": "in the transformer\nright now which encourages that sort\nof random behavior",
    "start": "4450475",
    "end": "4456670"
  },
  {
    "text": "where you seek out unfamiliar\nparts of the state space.",
    "start": "4456670",
    "end": "4464590"
  },
  {
    "text": "That is something\nwhich is inbuilt into traditional algorithms.",
    "start": "4464590",
    "end": "4469820"
  },
  {
    "text": "So you usually have some\nsort of entropy bonus to encourage exploration. And those are the modifications\nwhich one will also",
    "start": "4469820",
    "end": "4478960"
  },
  {
    "text": "need to think about if\none were to use decision transformers for online RL.",
    "start": "4478960",
    "end": "4485690"
  },
  {
    "text": "So what happens if somebody,\nI mean just naively, suppose I have this\nexact same setup. And the way that I\nsample the action is I",
    "start": "4485690",
    "end": "4493060"
  },
  {
    "text": "sample epsilon greedily, or I\ncreate a Boltzmann distribution in my sample from that,\nI mean just what happens?",
    "start": "4493060",
    "end": "4499420"
  },
  {
    "text": "It seems that's what RL\ndoes, so does what happens? So RL does a little\nbit more than that.",
    "start": "4499420",
    "end": "4506349"
  },
  {
    "text": "It indeed does those\nkinds of things where it would change the\ndistribution, for example,",
    "start": "4506350",
    "end": "4512350"
  },
  {
    "text": "the Boltzmann distribution\nand sample from it. There are these, as I say,\nthe devil lies in the detail.",
    "start": "4512350",
    "end": "4519850"
  },
  {
    "text": "It's also about how it controls\nthat exploration component with the exploitation.",
    "start": "4519850",
    "end": "4526954"
  },
  {
    "text": "It remains to be\nseen whether that is compatible with\ndecision transformers.",
    "start": "4526954",
    "end": "4532222"
  },
  {
    "text": "I don't want to\njump the gun, but I would say preliminary evidence\nsuggests that it's not directly",
    "start": "4532222",
    "end": "4540070"
  },
  {
    "text": "transferable the exact same\nsetup to the online case, that's what we have found.",
    "start": "4540070",
    "end": "4546880"
  },
  {
    "text": "There has to be some\nadjustments to be made to make, which we are still\nfiguring it out.",
    "start": "4546880",
    "end": "4553420"
  },
  {
    "text": "So the reason why I\nask is as you said, the devil's in the details. And so someone naively like me\nmight just come along and try",
    "start": "4553420",
    "end": "4559150"
  },
  {
    "text": "doing what's in RL. I want to hear more about this. So you're saying that\nwhat works in RL may not work for decision transformers.",
    "start": "4559150",
    "end": "4565900"
  },
  {
    "text": "Can you tell us why, like\nwhat pathologies emerge?",
    "start": "4565900",
    "end": "4571239"
  },
  {
    "text": "What are those devils\nhiding in the details? Well, I'll tell you [INAUDIBLE]\nwe also over time so--",
    "start": "4571240",
    "end": "4579064"
  },
  {
    "text": "[INTERPOSING VOICES]  But like to me, that's\nreally exciting.",
    "start": "4579064",
    "end": "4584650"
  },
  {
    "text": "And I'm sure that it's tricky. Yeah, I'm just asking\ntwo more questions",
    "start": "4584650",
    "end": "4590390"
  },
  {
    "text": "and you can finish off of those. So when do you think something\nlike decision transformer",
    "start": "4590390",
    "end": "4596270"
  },
  {
    "text": "is able to solve the credit\nassignment problem in RL instead of using some sort\nof like discount factor? ",
    "start": "4596270",
    "end": "4604610"
  },
  {
    "text": "Sorry, can you\nrepeat the question? Sorry, I think\nusually in RL, we have to rely on some sort\nof discount factor",
    "start": "4604610",
    "end": "4609800"
  },
  {
    "text": "to encode the rewards to\ngo, but decision transformer is able to do this credit\nassignment without that.",
    "start": "4609800",
    "end": "4616700"
  },
  {
    "text": "So do you think\nsomething like this book is like the way\nyou should do it,",
    "start": "4616700",
    "end": "4621980"
  },
  {
    "text": "like instead of having\nsome discount try to directly predict rewards?",
    "start": "4621980",
    "end": "4627020"
  },
  {
    "text": "So I would go on\nto say that I feel a discount factor is an\nimportant consideration",
    "start": "4627020",
    "end": "4632420"
  },
  {
    "text": "in general. And it's not incompatible\nwith decision transformers.",
    "start": "4632420",
    "end": "4637530"
  },
  {
    "text": "So basically what would change,\nand I think the code actually gives that functionality\nwhere the returns-to-go would",
    "start": "4637530",
    "end": "4645230"
  },
  {
    "text": "be computed as the\ndiscounted sum of rewards. And so it is very\nmuch compatible.",
    "start": "4645230",
    "end": "4652039"
  },
  {
    "text": "So there are scenarios where\ncontext length is still is not",
    "start": "4652040",
    "end": "4657440"
  },
  {
    "text": "enough to actually capture the\nlong term behavior we really need for credit assignment.",
    "start": "4657440",
    "end": "4663560"
  },
  {
    "text": "Maybe traditional\ntricks that are used could be brought in back to\nsolve those kinds of problems.",
    "start": "4663560",
    "end": "4671790"
  },
  {
    "text": "Got it. Yeah, I also thought like when\nI was reading the decision transformer that the interesting\nthing is that you don't",
    "start": "4671790",
    "end": "4678072"
  },
  {
    "text": "have a fixed gamma,\nlike a gamma is that usually a hyperparameter. But you don't have\na fixed gamma,",
    "start": "4678072",
    "end": "4683340"
  },
  {
    "text": "can you also learn this thing? And could this also be like-- can you have a different gamma\nfor each timestep or something,",
    "start": "4683340",
    "end": "4688833"
  },
  {
    "text": "possibly? That would be\ninteresting actually. I had not thought\nof that, but maybe",
    "start": "4688833",
    "end": "4696710"
  },
  {
    "text": "learning to predict\nthe discount factor could be another\nextension of this work.",
    "start": "4696710",
    "end": "4702440"
  },
  {
    "text": "Also do you think like\ndecision transformer work, is it compatible\nwith Q-learning?",
    "start": "4702440",
    "end": "4707840"
  },
  {
    "text": "So if you have something\nlike CQL, stuff like that, can you also implement those\nsort of loss functions on top",
    "start": "4707840",
    "end": "4713210"
  },
  {
    "text": "of a decision transformer? ",
    "start": "4713210",
    "end": "4718690"
  },
  {
    "text": "Maybe I could\nimagine ways in which you could include pessimism\nin here as well, which",
    "start": "4718690",
    "end": "4726159"
  },
  {
    "text": "is key to how CQL works. And actually most of\noffline RL algorithms work,",
    "start": "4726160",
    "end": "4734810"
  },
  {
    "text": "including the model based ones. Our focus here deliberately\nwas to go after simplicity.",
    "start": "4734810",
    "end": "4743020"
  },
  {
    "text": "Because we feel that part of\nthe reason why RL literature has been so scattered, as well\nif you think about different",
    "start": "4743020",
    "end": "4749080"
  },
  {
    "text": "sub-problems that\neveryone tries to solve, has been because everyone's\ntried to pick up on ideas which",
    "start": "4749080",
    "end": "4758110"
  },
  {
    "text": "are very well suited for\nthat narrow problem like,",
    "start": "4758110",
    "end": "4763510"
  },
  {
    "text": "for example, you have-- whether you're doing offline\nRL, or you're doing online, or you're doing\nimitation, you're",
    "start": "4763510",
    "end": "4769025"
  },
  {
    "text": "doing multitask, and all\nthese different variants. And so by design,\nwe did not want",
    "start": "4769025",
    "end": "4777520"
  },
  {
    "text": "to incorporate exactly\nthe components that exist in the current algorithms,\nbecause then it just starts",
    "start": "4777520",
    "end": "4786250"
  },
  {
    "text": "looking more like\narchitecture change as opposed to a more\nconceptual change",
    "start": "4786250",
    "end": "4791560"
  },
  {
    "text": "into thinking about RL\nas sequence modeling very generally.",
    "start": "4791560",
    "end": "4797280"
  },
  {
    "text": "Got it. That sounds interesting. So do you think we\ncan use some sort of like Q-learning\nobjectives instead",
    "start": "4797280",
    "end": "4802533"
  },
  {
    "text": "of self-supervised learning?  It's possible.",
    "start": "4802533",
    "end": "4808320"
  },
  {
    "text": "And maybe like I'm\nsaying that for certain-- like for online RL it\nmight be necessary.",
    "start": "4808320",
    "end": "4815580"
  },
  {
    "text": "Offline RL, we were happy\nto see it was not necessary.",
    "start": "4815580",
    "end": "4821010"
  },
  {
    "text": "But it remains to be\nseen more generally for transformer model or any\nother model for that matter",
    "start": "4821010",
    "end": "4826889"
  },
  {
    "text": "encompassing RL more\nbroadly whether that becomes a necessity.",
    "start": "4826890",
    "end": "4833100"
  },
  {
    "text": "Yeah. Well, thanks for your time. This was great. ",
    "start": "4833100",
    "end": "4842240"
  }
]