[
  {
    "start": "0",
    "end": "5340"
  },
  {
    "text": "We'll finish up this\nlast additional topic. It's not an additional topic. You've already\nseen a bunch of it, but I thought it\nwould be fun to go",
    "start": "5340",
    "end": "11368"
  },
  {
    "text": "into a little bit more detail. And at the end of\ntoday's lecture, we'll talk about\njust summarizing",
    "start": "11368",
    "end": "17370"
  },
  {
    "text": "the whole class and what do you\ndo now, where to go from here.",
    "start": "17370",
    "end": "22780"
  },
  {
    "text": "So, OK. So jumping back into this.",
    "start": "22780",
    "end": "28150"
  },
  {
    "text": "So we were talking about\ncardinality constraint but the cardinality could be\napplied for example to the,",
    "start": "28150",
    "end": "34590"
  },
  {
    "text": "let's say, a first\ndifference in which case you would be doing\npiecewise constant fitting and that's obvious.",
    "start": "34590",
    "end": "40470"
  },
  {
    "text": "You take a first\ndifference matrix here. So you take a first\ndifference matrix D,",
    "start": "40470",
    "end": "47010"
  },
  {
    "text": "and then if Dx has\na small cardinality,",
    "start": "47010",
    "end": "52329"
  },
  {
    "text": "it means you have a small\nnumber of jumps in x. And you could just do this\nfor the second difference",
    "start": "52330",
    "end": "60540"
  },
  {
    "text": "and things like that. If you do second\ndifference, then you",
    "start": "60540",
    "end": "67480"
  },
  {
    "text": "get a second difference matrix. And if this matrix\nmultiplied by x",
    "start": "67480",
    "end": "72490"
  },
  {
    "text": "is sparse or has\ncardinality of less than k, then this is exactly when\nx is piecewise linear,",
    "start": "72490",
    "end": "79210"
  },
  {
    "text": "or I guess that's the way\npeople would normally say it. I might say piecewise\naffine with something",
    "start": "79210",
    "end": "84250"
  },
  {
    "text": "like k kinks in the change\nof slope and stuff like that.",
    "start": "84250",
    "end": "89800"
  },
  {
    "text": "So we can handle all these kinds\nof constraints in this convex",
    "start": "89800",
    "end": "95200"
  },
  {
    "text": "cardinality framework. OK. So now we get to the heuristic. You know it already.",
    "start": "95200",
    "end": "101020"
  },
  {
    "text": "You've seen it. It's really dumb. It's wherever you see the\ncardinality replace it",
    "start": "101020",
    "end": "106510"
  },
  {
    "text": "with a multiple of the\none norm or just add",
    "start": "106510",
    "end": "111610"
  },
  {
    "text": "some l1 regularization\nto the objective. And then gamma\nhere is a parameter",
    "start": "111610",
    "end": "120490"
  },
  {
    "text": "that you use to achieve\nthe desired sparsity. So that that's the heuristic.",
    "start": "120490",
    "end": "125799"
  },
  {
    "text": "You've seen this a lot. There are some other versions. You're going to see them. They're actually better.",
    "start": "125800",
    "end": "132820"
  },
  {
    "text": "That's a weighted l1 norm. And this one I've\nheard people call that an asymmetric l1 norm.",
    "start": "132820",
    "end": "138730"
  },
  {
    "text": "But I hate that because it's not\na norm if it's not symmetric. In other words, you actually\nwould penalize things",
    "start": "138730",
    "end": "146980"
  },
  {
    "text": "being small or less\nthan 0, bigger than 0 with different slopes.",
    "start": "146980",
    "end": "152020"
  },
  {
    "text": "By the way, we're\ngoing to see later exactly why you might do that. ",
    "start": "152020",
    "end": "157780"
  },
  {
    "text": "But generally speaking, it's\njust the l1 norm heuristic. That's used in a lot of fields.",
    "start": "157780",
    "end": "164030"
  },
  {
    "text": "So here's the minimum\ncardinality problem. And you start with this problem. It's hard.",
    "start": "164030",
    "end": "169120"
  },
  {
    "text": "And you replace it with\nthis one, which is convex. And so it's easy to solve. Now, of course, that's\njust a heuristic.",
    "start": "169120",
    "end": "175930"
  },
  {
    "text": "When you solve this problem. You have not solved\nthat problem. This is merely a heuristic.",
    "start": "175930",
    "end": "181720"
  },
  {
    "text": "A quite good one, in fact,\nfor solving the top problem.",
    "start": "181720",
    "end": "187590"
  },
  {
    "text": "OK. If you had cardinality\nconstraint problem, lots of ways to do that.",
    "start": "187590",
    "end": "194290"
  },
  {
    "text": "One way is to literally just\nreplace this with an l1 norm and you have something\nthat looks like this,",
    "start": "194290",
    "end": "199590"
  },
  {
    "text": "or you can make it\nan l1 regularization. You can actually add\nthis regularization term to the objective.",
    "start": "199590",
    "end": "205830"
  },
  {
    "text": "And then you adjust\nbeta and gamma until the cardinality\nis what you want.",
    "start": "205830",
    "end": "212069"
  },
  {
    "text": "So these are the most basic\nways that people might do this.",
    "start": "212070",
    "end": "218280"
  },
  {
    "text": "Something called polishing is\nactually super interesting.",
    "start": "218280",
    "end": "223800"
  },
  {
    "text": "That's this. What you do is you\nuse an l1 heuristic to guess merely the\nsparsity pattern of x if x",
    "start": "223800",
    "end": "233220"
  },
  {
    "text": "is the thing you want sparse. So you run an l1\nheuristic, adjust gamma",
    "start": "233220",
    "end": "239940"
  },
  {
    "text": "until you think they're\nsupposed to be 15 nonzeros or you feel like\nthat's right for you, then you get 15 nonzeros.",
    "start": "239940",
    "end": "246849"
  },
  {
    "text": "And what you simply\ndo is you record the locations or the indexes\nof the non-zero entries.",
    "start": "246850",
    "end": "253299"
  },
  {
    "text": "So this is phase one. Then you go back and\nyou resolve the problem with only those\nas the variables.",
    "start": "253300",
    "end": "260470"
  },
  {
    "text": "All the others are now 0. And you remove any\nregularization if you had that.",
    "start": "260470",
    "end": "266530"
  },
  {
    "text": "So that's called polishing. So this would be very,\nvery commonly done.",
    "start": "266530",
    "end": "272199"
  },
  {
    "text": "Actually, it's interesting. So if you ask two\ndifferent people",
    "start": "272200",
    "end": "277389"
  },
  {
    "text": "who do statistics is\nthis a good idea when you're doing model fitting? And most of my friends\nwill say, no, no, no.",
    "start": "277390",
    "end": "284050"
  },
  {
    "text": "You should never do\nthis in statistics because the whole point was\nnot only did you sparse, did you do feature selection\nbut you shrunk also",
    "start": "284050",
    "end": "291250"
  },
  {
    "text": "the coefficients. And then other people\nI know they would say, no, we always do this.",
    "start": "291250",
    "end": "296360"
  },
  {
    "text": "So I'm not quite sure. I think it depends\non the application. So an example of where\nthis is always done",
    "start": "296360",
    "end": "303520"
  },
  {
    "text": "is when people they\nhave gene response data.",
    "start": "303520",
    "end": "309759"
  },
  {
    "text": "So you have 20,000 gene\nexpression features and you're trying to\npredict something.",
    "start": "309760",
    "end": "315419"
  },
  {
    "text": "So first step is you do l1\nregularized logistic regression you find the 22 genes\nthat you believe",
    "start": "315420",
    "end": "322830"
  },
  {
    "text": "will give you a\ngood logistic model. Again, if you know\nwhat I'm talking about that's fine, if you\ndon't, that's also probably OK.",
    "start": "322830",
    "end": "328680"
  },
  {
    "text": "So you get these 22, you locate\nthose 22, then you go back and you just fit a\nlogistic regression model with just those 22 genes.",
    "start": "328680",
    "end": "335160"
  },
  {
    "text": "Everybody got that? Yeah. Is the sparsity\npattern dependent on the initialization?",
    "start": "335160",
    "end": "341180"
  },
  {
    "text": "Would you get a different\nsparsity pattern? Absolutely not. No. No, no.",
    "start": "341180",
    "end": "346280"
  },
  {
    "text": "So there's no such thing as\ninitialization, number one. And number two, if\nit turns out when",
    "start": "346280",
    "end": "352250"
  },
  {
    "text": "you solve that problem there's\nnot a unique solution, which is entirely possible,\nthen it's possible",
    "start": "352250",
    "end": "358245"
  },
  {
    "text": "that you could have\ndifferent solutions but they'll all have\nthe same optimal value.",
    "start": "358245",
    "end": "365120"
  },
  {
    "text": "But no, that's not an issue. If that's an issue we're\nin deep, deep trouble.",
    "start": "365120",
    "end": "371660"
  },
  {
    "text": "Because the general\nstory for convex optimization goes like this. I've done this. I've gone off and give\ntalks about something",
    "start": "371660",
    "end": "379039"
  },
  {
    "text": "and I'll say that's\na convex problem. And then they'll just\nsay, we can solve it. And then people will say,\nhow did you solve it?",
    "start": "379040",
    "end": "385040"
  },
  {
    "text": "Did you use a\nconjugate gradient? Did you use this? Interpoint method? I'd actually literally just\nsay, it's none of your business.",
    "start": "385040",
    "end": "392975"
  },
  {
    "text": "It's very good at\nirritating people. And then finally they'd say,\nbut you must have used a method. I said, \"Oh, I used a method.",
    "start": "392975",
    "end": "398220"
  },
  {
    "text": "I did. You're right. I did. But I'm not saying what it\nis because it's not relevant because it's a convex problem.\"",
    "start": "398220",
    "end": "403950"
  },
  {
    "text": "So what that means\nis basically-- I told them finally I can think\nof 50 methods all of which",
    "start": "403950",
    "end": "409740"
  },
  {
    "text": "would work, all of them. It's like if somebody\ndoes least squares",
    "start": "409740",
    "end": "415410"
  },
  {
    "text": "and you go, how'd you do it? How'd you do it? Do you use QR\nfactorization, SVD? Conjugate gradient, LSQR?",
    "start": "415410",
    "end": "421050"
  },
  {
    "text": "And the answer is unless\nthe problem is ginormous and you're under extreme\nreal-time constraints",
    "start": "421050",
    "end": "426240"
  },
  {
    "text": "or something like that,\nit's no one's business. They would always\nget the same answer. So that's why. And certainly, it has nothing to\ndo with the initial condition.",
    "start": "426240",
    "end": "433360"
  },
  {
    "text": "So that's just my\nreaction to that. OK. So this is polishing.",
    "start": "433360",
    "end": "440770"
  },
  {
    "text": "It's interesting. And it depends on\nthe application, whether or not you\nshould do polishing.",
    "start": "440770",
    "end": "447530"
  },
  {
    "text": "I think in a lot of\nstatistical fitting methods, you should not because the\nentire point was to regularize.",
    "start": "447530",
    "end": "453610"
  },
  {
    "text": "Well, it was two things. It was to do feature\nselection and to regularize. So you shouldn't do this.",
    "start": "453610",
    "end": "459280"
  },
  {
    "text": "Many other things like if\nyou're doing electronic design or mechanical\nengineering or something and you want a sparse\nthing because you",
    "start": "459280",
    "end": "466570"
  },
  {
    "text": "don't intend to build a space\nframe with a million bars but in fact, only\n236, then you do",
    "start": "466570",
    "end": "473259"
  },
  {
    "text": "do this because once you know\nwhich ones are actual bars then",
    "start": "473260",
    "end": "478570"
  },
  {
    "text": "you go back and\nsolve the problem. That make sense? ",
    "start": "478570",
    "end": "486320"
  },
  {
    "text": "So now we're going\nto give a couple of-- so we gave a couple\nof interpretations",
    "start": "486320",
    "end": "492730"
  },
  {
    "text": "as to why this l1\nheuristic would work. And one that is not\nbad is this idea",
    "start": "492730",
    "end": "498220"
  },
  {
    "text": "that compared to\na square penalty, an absolute value\npenalty keeps up",
    "start": "498220",
    "end": "503830"
  },
  {
    "text": "the pressure to make the thing\nsmall all the way up until you get to 0.",
    "start": "503830",
    "end": "508900"
  },
  {
    "text": "And by pressure, I\nmean, the derivative. It's either plus or minus 1. Plus 1 if you're negative.",
    "start": "508900",
    "end": "515740"
  },
  {
    "text": "Whereas for a square\npenalty, once you get small, it doesn't care anymore because\nthe penalty is basically small",
    "start": "515740",
    "end": "521469"
  },
  {
    "text": "squared. That's actually,\nbelieve it or not, a perfectly good explanation\nfor why an l1 norm.",
    "start": "521470",
    "end": "528500"
  },
  {
    "text": "And for that matter, any\npenalty with a kink at 0 is going to have this property.",
    "start": "528500",
    "end": "535570"
  },
  {
    "text": "So, in fact, there's a\npenalty that's very funny. Everyone knows about\nthe Huber penalty,",
    "start": "535570",
    "end": "540770"
  },
  {
    "text": "which is it's a\nFrankenstein penalty that's quadratic and then\ntransitions to linear.",
    "start": "540770",
    "end": "547100"
  },
  {
    "text": "So there's another one\nwhich is l1 for small things",
    "start": "547100",
    "end": "552660"
  },
  {
    "text": "and then transitions\nto quadratic. Everybody picture that? And you're going to get--",
    "start": "552660",
    "end": "558339"
  },
  {
    "text": "that's going to be a sparsifying\nregularizer for example if you use it as regularizer. And do you know\nwhat it's called?",
    "start": "558340",
    "end": "565040"
  },
  {
    "text": "This is kind of cutesy\nbut I don't care. That's called the BerHu penalty. Sorry.",
    "start": "565040",
    "end": "570400"
  },
  {
    "text": "It's Huber-like trans--\nanyway, it's stupid. Who cares anyway? But the point is that's an\nexample of a sparsifying",
    "start": "570400",
    "end": "576550"
  },
  {
    "text": "penalty. OK. All right. So we gave that as\nan example and I",
    "start": "576550",
    "end": "582558"
  },
  {
    "text": "think you've actually\nalready done problems where you have experienced\nthis sparsification.",
    "start": "582558",
    "end": "588190"
  },
  {
    "text": "But now we're going to\ngive a couple of other very cool interpretations. So one is let's\nsolve this problem.",
    "start": "588190",
    "end": "596110"
  },
  {
    "text": "Let's attempt to solve\nthe cardinality of x, minimize this. Subject x is in a\nconvex set and then",
    "start": "596110",
    "end": "603097"
  },
  {
    "text": "all of the x's are between plus\nand minus-- all the components of x are between plus n minus\nR. So that's this thing here.",
    "start": "603097",
    "end": "610160"
  },
  {
    "text": "OK. So what we're going\nto do is we're going to write it this way. We can introduce\nsome new variables zi and they're going to be Boolean.",
    "start": "610160",
    "end": "616223"
  },
  {
    "text": "They're going to\nbe either 0 or 1. And we're going to write the\nconstraint that xi is less than R is this way.",
    "start": "616223",
    "end": "622330"
  },
  {
    "text": "And what this means is if\nzi is 0, it means xi is 0. If zi is 1, we express\nthe idea that it's less",
    "start": "622330",
    "end": "629350"
  },
  {
    "text": "than R. So this\nproblem is completely equivalent to that one.",
    "start": "629350",
    "end": "635260"
  },
  {
    "text": "So they're identical. Not identical. Sorry, equivalent. Of course, they're not\nidentical or equivalent.",
    "start": "635260",
    "end": "642310"
  },
  {
    "text": "This one is no easier\nto solve than this. Actually, this one you could\npop in to what are called mixed",
    "start": "642310",
    "end": "650140"
  },
  {
    "text": "integer linear programming. So a MILP solver. And if you're lucky, you might\nget a solution or something",
    "start": "650140",
    "end": "657610"
  },
  {
    "text": "like that, who knows? If you're unlucky you might be\nwaiting an hour or something.",
    "start": "657610",
    "end": "662770"
  },
  {
    "text": "OK. So what we're going to do\nis we're going to relax. That makes perfect sense.",
    "start": "662770",
    "end": "668878"
  },
  {
    "text": "What we're going to\ndo is we're going to take these z's which\nare Boolean variables. So it's 0 or 1. We're going to relax them to\nthis weird quantum mechanical",
    "start": "668878",
    "end": "676550"
  },
  {
    "text": "state that can be\nanywhere between 0 and 1. So that's a relaxation. Now that gives you this\nproblem here, this top problem.",
    "start": "676550",
    "end": "686060"
  },
  {
    "text": "And let's see, let's\ntake a look at it. Oh, good news is that's convex. So that we can solve.",
    "start": "686060",
    "end": "691790"
  },
  {
    "text": "That's completely tractable. But if you stare\nat it carefully, you'll realize that if\nI divided things out,",
    "start": "691790",
    "end": "700010"
  },
  {
    "text": "this is actually the\nepigraph, the sum of z's here is the epigraph--",
    "start": "700010",
    "end": "705470"
  },
  {
    "text": "this is the epigraph\nrepresentation of the l1 norm of x divided\nby R, or just take R equals 1.",
    "start": "705470",
    "end": "713779"
  },
  {
    "text": "So this problem, which\nis literally it's",
    "start": "713780",
    "end": "718940"
  },
  {
    "text": "the relaxation of the original\nBoolean problem just simply becomes our l1 heuristic.",
    "start": "718940",
    "end": "724660"
  },
  {
    "text": "By the way, this l1\nnorm here thing was-- sorry the l infinity constraint\nwas critical to do this.",
    "start": "724660",
    "end": "731680"
  },
  {
    "text": "Without that, the\nrelaxation is nothing. It gives you no\ninformation at all.",
    "start": "731680",
    "end": "737440"
  },
  {
    "text": " This also suggests\nall sorts of things. If your variables have\ndifferent ranges or bounds,",
    "start": "737440",
    "end": "746440"
  },
  {
    "text": "then you probably shouldn't\nbe doing an l1 heuristic, you should probably\nbe a weighted l1 norm,",
    "start": "746440",
    "end": "752860"
  },
  {
    "text": "weighted by the inverse of the\nbounds or something like that. OK.",
    "start": "752860",
    "end": "758340"
  },
  {
    "text": "Now when you solve this\nproblem, these are the same. They get the same optimal value.",
    "start": "758340",
    "end": "763740"
  },
  {
    "text": "You get a lower bound\non the optimal value. It's not usually a\nparticularly good one. So this makes sense.",
    "start": "763740",
    "end": "771819"
  },
  {
    "text": "Now, you can also interpret\nthis in a really cool way as a convex envelope,\nwhich I will explain.",
    "start": "771820",
    "end": "778170"
  },
  {
    "text": "So if I have any\nfunction at all, here's a function non-convex. Then it's convex envelope.",
    "start": "778170",
    "end": "784980"
  },
  {
    "text": "I'll just draw it and\nthen I will write it down. I can write it down in all\nsorts of very cool ways.",
    "start": "784980",
    "end": "790290"
  },
  {
    "text": "There we go. And then I just do this. So this thing, that's the\nso-called convex envelope",
    "start": "790290",
    "end": "797040"
  },
  {
    "text": "of the original function. And it makes sense. So lots of ways to say it.",
    "start": "797040",
    "end": "802200"
  },
  {
    "text": "It's the largest convex\nfunction that is everywhere below your given function.",
    "start": "802200",
    "end": "808840"
  },
  {
    "text": "So that's another way to say it. Here's a super compact\ndescription of it, which is very cool.",
    "start": "808840",
    "end": "814420"
  },
  {
    "text": "It's f star star. So it's the conjugate\nof the conjugate.",
    "start": "814420",
    "end": "820320"
  },
  {
    "text": "Remember that the conjugate\nof any function convex or not is convex. ",
    "start": "820320",
    "end": "828010"
  },
  {
    "text": "Then when I take the\nconjugate of that, if the original\nfunction had been convex I would have recovered\nthe initial function.",
    "start": "828010",
    "end": "834070"
  },
  {
    "text": "But I'm definitely going to get\nsomething that is-- actually, you can easily show you're\ngoing to get something which is a lower bound on the\noriginal function, number one.",
    "start": "834070",
    "end": "841420"
  },
  {
    "text": "So f star star is less\nthan f is always true. But the other cool part\nis had it been convex,",
    "start": "841420",
    "end": "847810"
  },
  {
    "text": "you would have gotten f back. So that's actually another way\nto express the envelope, which I think is cool.",
    "start": "847810",
    "end": "853285"
  },
  {
    "text": " And in fact, you'll\nsee a lot of people",
    "start": "853285",
    "end": "858399"
  },
  {
    "text": "not even use fn but\njust literally write f star star in papers\nas the convex envelope.",
    "start": "858400",
    "end": "864279"
  },
  {
    "text": "So it makes sense. And you can work it out\nfor at least some things.",
    "start": "864280",
    "end": "869529"
  },
  {
    "text": "And we can actually\nwork it out for the-- we can work it out\nfor the card function",
    "start": "869530",
    "end": "875890"
  },
  {
    "text": "but restricted to an interval. So I'm going to write\ndown the card function.",
    "start": "875890",
    "end": "881410"
  },
  {
    "text": "This is 0. So, let's see. It looks like this-- oh, no, so that's filled.",
    "start": "881410",
    "end": "886613"
  },
  {
    "text": "And then it looks like that. That's the card function. By the way, what is\nthe convex envelope",
    "start": "886613",
    "end": "892220"
  },
  {
    "text": "of just card by itself? It has one and I'd like\nto know what it is. ",
    "start": "892220",
    "end": "899080"
  },
  {
    "text": "It is 0. Yeah. This is 0. It's the constant 0. And you go-- if you think about\nit, at A, it's convex, and B,",
    "start": "899080",
    "end": "907175"
  },
  {
    "text": "it's a lower bound\non the card function because a card function\nonly has the value 0 or 1. So congratulations.",
    "start": "907175",
    "end": "912280"
  },
  {
    "text": "It's completely useless.  But if we say,\nno, you know what?",
    "start": "912280",
    "end": "918610"
  },
  {
    "text": "But I happen to also know\nthat the variable is between, let's say, minus R and\nplus R, so the actual",
    "start": "918610",
    "end": "928340"
  },
  {
    "text": "function I'm taking the\ncard of looks like this. And it goes up to plus\ninfinity out there.",
    "start": "928340",
    "end": "933430"
  },
  {
    "text": "What is the convex\nenvelope of that function? This is actually very cool.",
    "start": "933430",
    "end": "939140"
  },
  {
    "text": "You can get it. What is it? [INAUDIBLE] Yeah. So you just draw and\nyou go like this.",
    "start": "939140",
    "end": "947110"
  },
  {
    "text": "Literally, you\ntake the epigraph-- the epigraph of the whole thing\nis this thing up here going up",
    "start": "947110",
    "end": "952540"
  },
  {
    "text": "to infinity, plus there's a\nlittle needle sticking down here. That's the epigraph. I take the convex hull of the\nepigraph that fills in this.",
    "start": "952540",
    "end": "960350"
  },
  {
    "text": "And then this is f star star. So, in this case,\nwhere f is card",
    "start": "960350",
    "end": "968980"
  },
  {
    "text": "restricted to\nabsolute value less than R. Everybody got this?",
    "start": "968980",
    "end": "974840"
  },
  {
    "text": "Actually, it's cool because it\nsuggests all sorts of things. If you had other bounds\non x that were asymmetric,",
    "start": "974840",
    "end": "981230"
  },
  {
    "text": "you would use an\nasymmetric thing here. And by the way, it would\nbe a better sparsifier.",
    "start": "981230",
    "end": "987200"
  },
  {
    "text": "So this is the idea. So, actually, you don't\nhave to worry about that.",
    "start": "987200",
    "end": "993660"
  },
  {
    "text": "So, for example,\nin the application of regressor selection\nin machine learning",
    "start": "993660",
    "end": "999110"
  },
  {
    "text": "and statistics, this is\ngenerally not a problem because the data have--",
    "start": "999110",
    "end": "1004570"
  },
  {
    "text": "I assume standard practice\nis to actually standardize the features. So they're all now\nbetween reasonable ranges.",
    "start": "1004570",
    "end": "1013190"
  },
  {
    "text": "In fact, for that\nmatter roughly speaking between plus and minus\n1, and so that justifies",
    "start": "1013190",
    "end": "1018370"
  },
  {
    "text": "the use of an l1 norm. If for whatever reason you did\nnot standardize your features",
    "start": "1018370",
    "end": "1025119"
  },
  {
    "text": "and you want to do\nsomething like Lasso, then you should--\nwell, number one, you should not do\nthat, number one.",
    "start": "1025119",
    "end": "1031000"
  },
  {
    "text": "But if you did do\nthat, you should be using a weird weighted l1\nnorm or an asymmetric thing",
    "start": "1031000",
    "end": "1038109"
  },
  {
    "text": "or something like that. Everybody following this? Good.",
    "start": "1038109",
    "end": "1043150"
  },
  {
    "text": "It's interesting. OK. So that's just this. If you knew, for example, that--",
    "start": "1043150",
    "end": "1052150"
  },
  {
    "text": "if you knew that over this\nconvex feasible set x--",
    "start": "1052150",
    "end": "1058120"
  },
  {
    "text": "if you calculated what's\ncalled the bounding box, which by the way I could\nsolve for any problem",
    "start": "1058120",
    "end": "1063309"
  },
  {
    "text": "if I don't mind solving\n2n convex problems. I just maximize xi subject to x\nand C, and then I minimize xi.",
    "start": "1063310",
    "end": "1069970"
  },
  {
    "text": "Those are both convex problems. And I will get the best\npossible values of l and u.",
    "start": "1069970",
    "end": "1075679"
  },
  {
    "text": "If x were-- let's see. If x has a dimension\nreally small like 10,",
    "start": "1075680",
    "end": "1081950"
  },
  {
    "text": "you should just consider\nsolving it brute force and just solve all 1,024\nconvex problems, then is 20.",
    "start": "1081950",
    "end": "1090980"
  },
  {
    "text": "This is actually\ninteresting right here. Because it's not going to kill\nyou to solve 40 problems to get",
    "start": "1090980",
    "end": "1096350"
  },
  {
    "text": "the bounds, which is going to\ngive you a better sparsifier than just an l1 norm.",
    "start": "1096350",
    "end": "1101900"
  },
  {
    "text": "So that would be one. What you get though is\nsomething really cool.",
    "start": "1101900",
    "end": "1107480"
  },
  {
    "text": "You get a function\nthat matches-- it's just what you think it is. I'll just draw the\npicture over here.",
    "start": "1107480",
    "end": "1112970"
  },
  {
    "text": "It just says, suppose\nthis is between-- what I call this? li or\nsomething and then this is ui.",
    "start": "1112970",
    "end": "1120860"
  },
  {
    "text": "I'll make it asymmetric. There's ui. I'll just call it\nu. i doesn't matter.",
    "start": "1120860",
    "end": "1127580"
  },
  {
    "text": "Then your-- I mean,\nthis is clear. But your convex envelope\nlooks like this.",
    "start": "1127580",
    "end": "1136560"
  },
  {
    "text": "And it's this tilted function. Actually, which I\nthink people now",
    "start": "1136560",
    "end": "1144570"
  },
  {
    "text": "call the pinball\nloss because I think it has to do with\npinball machines.",
    "start": "1144570",
    "end": "1150060"
  },
  {
    "text": "Don't ask. Anyway, fine. I think it's stuck. ",
    "start": "1150060",
    "end": "1155730"
  },
  {
    "text": "That's what this is called. So this is the idea. And if you do this, you'll\nget a better sparsifier,",
    "start": "1155730",
    "end": "1165765"
  },
  {
    "text": "its principle. Now we'll do some examples.",
    "start": "1165765",
    "end": "1171390"
  },
  {
    "text": "The first one is\nregressor selection. So here we're doing\nleast squares, but we want to choose here\nwhich of the entries--",
    "start": "1171390",
    "end": "1182200"
  },
  {
    "text": "so x represents our\nparameter in our model and we want it to\nhave just k nonzeros.",
    "start": "1182200",
    "end": "1188940"
  },
  {
    "text": "So it basically says,\nhere's an example. I'd say, here's 25,000\ngene expressions.",
    "start": "1188940",
    "end": "1195270"
  },
  {
    "text": "I'd like you to fit a model\nthat predicts some outcome for my 600 patients.",
    "start": "1195270",
    "end": "1200460"
  },
  {
    "text": "You'd say, please--\nor my 600 experiments with mice or something. And I'd say, you know what?",
    "start": "1200460",
    "end": "1207660"
  },
  {
    "text": "You can use 30 of them. Go ahead. And the whole point\nthere is 25,000 choose 30 is a\npretty big number.",
    "start": "1207660",
    "end": "1215010"
  },
  {
    "text": "So this would be an example\nof a regressor selection. OK.",
    "start": "1215010",
    "end": "1220260"
  },
  {
    "text": "There are lots of\nways to do this. And they're all related\nto Lasso and they have all sorts of\nother names and things like that like basis pursuit.",
    "start": "1220260",
    "end": "1226590"
  },
  {
    "text": "It depends on whether it's a\nconstraint or a regularizer and all that kind of stuff. But this is cool.",
    "start": "1226590",
    "end": "1233190"
  },
  {
    "text": "And here's just a\nquick baby example. ",
    "start": "1233190",
    "end": "1239010"
  },
  {
    "text": "Here's x and R20. So that means, of course,\nthat there's a million different possible values of x.",
    "start": "1239010",
    "end": "1245620"
  },
  {
    "text": "Sorry, sparsity patterns\nfor x is a million. So we just solved it globally\nbecause it's not a big deal.",
    "start": "1245620",
    "end": "1252549"
  },
  {
    "text": "And then you get a\ntrade-off curve of-- this is how many\nfeatures you use.",
    "start": "1252550",
    "end": "1258670"
  },
  {
    "text": "And then this is how small\nyou can make norm x minus b. This might be some kind of fit.",
    "start": "1258670",
    "end": "1263950"
  },
  {
    "text": "And there's a trade-off. This one shows you the--\nthis is the global optimal.",
    "start": "1263950",
    "end": "1269380"
  },
  {
    "text": "This would have been impossible\nhad this been our R50. I mean, not\nimpossible because it",
    "start": "1269380",
    "end": "1275440"
  },
  {
    "text": "would be outside the\nstuff we do in this class. So if you do the l1--",
    "start": "1275440",
    "end": "1282340"
  },
  {
    "text": "if you do the l1\nheuristic with polishing, you actually get this-- this thing.",
    "start": "1282340",
    "end": "1287710"
  },
  {
    "text": "And it's actually really cool. Often, you're just dead on. It's like here it's dead on. Well, that's silly, but\nhere you're dead on.",
    "start": "1287710",
    "end": "1294610"
  },
  {
    "text": "Here, now you're off by\n1, here you're off by 1.",
    "start": "1294610",
    "end": "1300460"
  },
  {
    "text": "It's actually interesting\nhow this works.",
    "start": "1300460",
    "end": "1305500"
  },
  {
    "text": "All right. So this is a baby example but\nit's just to illustrate this. A huge, big difference that\nthe approximate difference",
    "start": "1305500",
    "end": "1314130"
  },
  {
    "text": "in computation\neffort, in this case, we could have done\nit more efficiently. But, in this case,\nit's 1 million to 1.",
    "start": "1314130",
    "end": "1323130"
  },
  {
    "text": "Because each of this point,\nthis point was found--",
    "start": "1323130",
    "end": "1328265"
  },
  {
    "text": "maybe not a million\nbut whatever, this point was found-- actually, it is 1\nmillion to 1 roughly,",
    "start": "1328265",
    "end": "1333720"
  },
  {
    "text": "this point was found by\nactually checking all 20 choose for sparsity\npatterns and finding",
    "start": "1333720",
    "end": "1340950"
  },
  {
    "text": "the one that had the best fit. So it's not a small\nnumber, whatever it is.",
    "start": "1340950",
    "end": "1346290"
  },
  {
    "text": "So that's it. So the idea that\nyou get this close with such a stunning\nreduction in computation",
    "start": "1346290",
    "end": "1352690"
  },
  {
    "text": "that you can't claim\nthat you've actually solved the original cardinality\nproblem, but that's OK.",
    "start": "1352690",
    "end": "1359160"
  },
  {
    "text": "Yes. How do you know for sure that\nthe components that are not 0",
    "start": "1359160",
    "end": "1364289"
  },
  {
    "text": "are the same in both cases? Oh, we don't. In fact, they're not.",
    "start": "1364290",
    "end": "1369990"
  },
  {
    "text": "Because, let's see, which\none are you looking at? The last one when\nthey maps [INAUDIBLE]..",
    "start": "1369990",
    "end": "1377430"
  },
  {
    "text": "You mean here? Yeah. Yeah. Well, a was probably\njust chosen randomly.",
    "start": "1377430",
    "end": "1382500"
  },
  {
    "text": "So that has probability\n0 of happening. Of course, it is. I mean, it's not\nnecessarily true.",
    "start": "1382500",
    "end": "1388440"
  },
  {
    "text": "If a had two columns\nthat were identical, or if you want to translate\nit to machine learning",
    "start": "1388440",
    "end": "1395130"
  },
  {
    "text": "and fitting, you'd\nsay two features which are either 100% correlated\nor exactly the same,",
    "start": "1395130",
    "end": "1400260"
  },
  {
    "text": "then in fact, you\nget your choice. They might not be the same. ",
    "start": "1400260",
    "end": "1405380"
  },
  {
    "text": "And I don't think that matters. We're just saying\nwhat is the best-- this says you may pick\nfour features and it says,",
    "start": "1405380",
    "end": "1413130"
  },
  {
    "text": "what's the best fit you can get? Usually, there's a unique set\nof four features that does that.",
    "start": "1413130",
    "end": "1418697"
  },
  {
    "text": "But if there's not, then\nI don't think it matters. It's the same thing. ",
    "start": "1418697",
    "end": "1424370"
  },
  {
    "text": "I don't know if that made sense. Didn't totally make sense to\nme, but anyway, that's fine.",
    "start": "1424370",
    "end": "1430140"
  },
  {
    "text": " Here's sparse signal\nreconstruction.",
    "start": "1430140",
    "end": "1436210"
  },
  {
    "text": " So these are just\nthe standard things.",
    "start": "1436210",
    "end": "1442380"
  },
  {
    "text": "Actually, a lot of\npeople would square this. It has no effect it changes\nof course the parameterization",
    "start": "1442380",
    "end": "1448260"
  },
  {
    "text": "with the hyperparameter gamma,\nbut otherwise, it's the same. In fact, those are\ntwo scalarizations",
    "start": "1448260",
    "end": "1454470"
  },
  {
    "text": "of what's really a\nbicriterion problem. So these are one. We got all sorts of names.",
    "start": "1454470",
    "end": "1460380"
  },
  {
    "text": "Here's just a quick example. So there's a signal with a-- it's got a dimension of 1,000,\nbut there's only 30 nonzeros.",
    "start": "1460380",
    "end": "1468420"
  },
  {
    "text": "And here's the exact signal. This is just a\nsimulated example.",
    "start": "1468420",
    "end": "1473700"
  },
  {
    "text": "And then I give you 200\nrandom noisy measurements of these 1,000 numbers.",
    "start": "1473700",
    "end": "1481950"
  },
  {
    "text": "And so if you do an\nl1 reconstruction with an appropriately\nchosen gamma,",
    "start": "1481950",
    "end": "1489900"
  },
  {
    "text": "you basically get it exactly. So I think 20 years ago when\npeople were first seeing this,",
    "start": "1489900",
    "end": "1499020"
  },
  {
    "text": "this was pretty stunning. Nowadays, I think\na lot of people have already seen this in\nundergraduate machine learning",
    "start": "1499020",
    "end": "1505320"
  },
  {
    "text": "classes and they're\nlike yeah, so what? Who cares? But you really should stop\nand actually reflect on this",
    "start": "1505320",
    "end": "1512100"
  },
  {
    "text": "and think how cool--\nthis is basically somebody walks up to you\non the street and says, I have 1,000 numbers I'd\nlike to help you figure out.",
    "start": "1512100",
    "end": "1520580"
  },
  {
    "text": "You go, cool. I'll be happy to help. You tell me what\nmeasurements you have. You go, I have 200 measurements.",
    "start": "1520580",
    "end": "1527790"
  },
  {
    "text": "You're like, oh, I'm\nsorry, did you say 2,000? Or did you get the\ntwo numbers different?",
    "start": "1527790",
    "end": "1534090"
  },
  {
    "text": "Do you have 1,000\nmeasurements and something like that and 300 parameters\nyou want to estimate? You go, no, I would like to\nestimate 1,000 parameters based",
    "start": "1534090",
    "end": "1542430"
  },
  {
    "text": "on 300 noisy measurements. So the correct thing to do\nin a situation like that is to walk away\nfrom that person.",
    "start": "1542430",
    "end": "1549810"
  },
  {
    "text": "Maybe not turn your back on\nthem, just walk away carefully and avoid them from then on.",
    "start": "1549810",
    "end": "1554907"
  },
  {
    "text": "It's like someone walks\nup to the street and says, I'm thinking of four numbers\nthey add to 12, what are they? You should avoid\npeople like this",
    "start": "1554907",
    "end": "1561610"
  },
  {
    "text": "because it's a\nnonsensical problem. OK. Fine. But then they say, ah,\nbut only 30 are nonzero.",
    "start": "1561610",
    "end": "1568960"
  },
  {
    "text": "Then you'd say, that sounds\nlike a normal thing to me. Because you think\nof fitting and stuff",
    "start": "1568960",
    "end": "1575770"
  },
  {
    "text": "like that like data\ncompression and you go, cool. I have 300 noisy measurements\nto estimate, is that right?",
    "start": "1575770",
    "end": "1582340"
  },
  {
    "text": "Yeah, 200 noisy\nmeasurements estimate 30 things, that feels right. That's a sensible thing.",
    "start": "1582340",
    "end": "1587500"
  },
  {
    "text": "You'd say, what are you doing? I'm fitting a model. How many parameters\nare in your model? 30. How many measurements you got?",
    "start": "1587500",
    "end": "1592630"
  },
  {
    "text": "200. Sounds totally cool. OK. But there's a catch. No one tells you which\n30 of the 1,000 they are.",
    "start": "1592630",
    "end": "1603289"
  },
  {
    "text": "And 1,000 choose\n30 is a big number. Everybody following this? OK.",
    "start": "1603290",
    "end": "1608360"
  },
  {
    "text": "So what's amazing\nis it just works. And I think one of the first\npackages for this maybe 20",
    "start": "1608360",
    "end": "1613700"
  },
  {
    "text": "years ago, 25 years ago\nwas called L1-MAGIC, was the name for it, which\nwas now people don't care.",
    "start": "1613700",
    "end": "1620570"
  },
  {
    "text": "But it is good every now and\nthen think how crazy this is that this actually works.",
    "start": "1620570",
    "end": "1625850"
  },
  {
    "text": "OK. Just for comparison, that\nwould be the l2 reconstruction.",
    "start": "1625850",
    "end": "1631220"
  },
  {
    "text": "Well, duh. If I walk up to\nyou and say please help me guess\n1,000 numbers, I'll give you 300 noisy\nmeasurements, what",
    "start": "1631220",
    "end": "1637013"
  },
  {
    "text": "did you think you were\ngoing to get-- what did you think was going to happen? So this fits with that.",
    "start": "1637013",
    "end": "1642860"
  },
  {
    "text": "OK. I'm actually going to skip\nthis because, I don't know, these fussy things\nthat actually tell you",
    "start": "1642860",
    "end": "1650050"
  },
  {
    "text": "under some circumstances. This actually does with high\nprobability actually work.",
    "start": "1650050",
    "end": "1655850"
  },
  {
    "text": "I'm going to go on from that. We'll just go to the next part. ",
    "start": "1655850",
    "end": "1668900"
  },
  {
    "text": "OK. So we'll look at a couple more\nexamples that are pretty cool. So you could do the same thing.",
    "start": "1668900",
    "end": "1677139"
  },
  {
    "text": "We did total variation\nfor a scalar signal. So the total variation\nis essentially",
    "start": "1677140",
    "end": "1682659"
  },
  {
    "text": "the l1 norm of the derivative. That's rough, but\nthat's what it is. It tells you how much--",
    "start": "1682660",
    "end": "1689260"
  },
  {
    "text": "well, that's what it is. Let's just leave it that way. And the convex\ncardinality thing,",
    "start": "1689260",
    "end": "1696280"
  },
  {
    "text": "that's called a total variation,\nyou do the same thing. And we actually saw an example.",
    "start": "1696280",
    "end": "1702220"
  },
  {
    "text": "I think here's an\nexample where the top is some original signal which\nhas got smooth parts but also",
    "start": "1702220",
    "end": "1707799"
  },
  {
    "text": "these big jumps. The corrupted version is below.",
    "start": "1707800",
    "end": "1712985"
  },
  {
    "text": "And then when you\ndenoise it, you get things like this\nusing l1 reconstruction.",
    "start": "1712985",
    "end": "1718049"
  },
  {
    "text": "And you could, I\ndon't know, you could decide which value\nis right, which",
    "start": "1718050",
    "end": "1723210"
  },
  {
    "text": "you could do by some\ncross-validation or something. But roughly speaking, this\nlooks like it's overregularized.",
    "start": "1723210",
    "end": "1729750"
  },
  {
    "text": "That looks like maybe\nit's underregularized. And you might say that that's\nabout right or maybe somewhere",
    "start": "1729750",
    "end": "1735780"
  },
  {
    "text": "even in between these\ntwo might be about right. ",
    "start": "1735780",
    "end": "1740970"
  },
  {
    "text": "And so this is\nused in-- actually, there was a famous\nincident in the '90s. I think this was used\nto reconstruct actually",
    "start": "1740970",
    "end": "1749880"
  },
  {
    "text": "a lot of old wax recordings\nof Caruso, the opera singer.",
    "start": "1749880",
    "end": "1755430"
  },
  {
    "text": "They would have these wax\ndrums all scratched up and things like that.",
    "start": "1755430",
    "end": "1761460"
  },
  {
    "text": "And to extract-- what something\nbased on the total variation denoising would\ndo would actually",
    "start": "1761460",
    "end": "1767430"
  },
  {
    "text": "remove a lot of the\nnoise but actually preserve the sharp attacks.",
    "start": "1767430",
    "end": "1772570"
  },
  {
    "text": "Because, otherwise, if you did\nsome of the l2 version of it, it's just a low-pass filter. I'm now speaking\ndialect, but sorry.",
    "start": "1772570",
    "end": "1778760"
  },
  {
    "text": "And so basically, it would\njust come out muffled. Good news, noise is\nsubstantially reduced,",
    "start": "1778760",
    "end": "1784930"
  },
  {
    "text": "bad news, so is\nthe actual content. But when you do a\ntotal variation, you get something\nthat's actually cool.",
    "start": "1784930",
    "end": "1791880"
  },
  {
    "text": "The attack of a drum\nbeat when somebody leans into a snare\ndrum, it's right there.",
    "start": "1791880",
    "end": "1798160"
  },
  {
    "text": "So, anyway, that was in the\n'90s, I think, which was cool. OK.",
    "start": "1798160",
    "end": "1803440"
  },
  {
    "text": "I will skip this because\nI think we saw that. One more example,\nbut this is in 2D.",
    "start": "1803440",
    "end": "1810240"
  },
  {
    "text": "And even this is\na simplification. And I will make a comment\nabout this a bit later.",
    "start": "1810240",
    "end": "1816164"
  },
  {
    "text": " Let's say that I have\nan image or let's",
    "start": "1816165",
    "end": "1823750"
  },
  {
    "text": "say I'm doing MRI\nreconstruction. And so I have all\nthese values here. ",
    "start": "1823750",
    "end": "1831340"
  },
  {
    "text": "So here the total variation\nis actually given by-- I mean, in the continuous\nversion, it's actually this.",
    "start": "1831340",
    "end": "1838779"
  },
  {
    "text": "It's f of-- here,\nI can call this-- yeah, I'll do this.",
    "start": "1838780",
    "end": "1844750"
  },
  {
    "text": "It's this.  There you go.",
    "start": "1844750",
    "end": "1849961"
  },
  {
    "text": "That is what the\ntotal variation is when you do in the\ninfinite-dimensional case.",
    "start": "1849961",
    "end": "1858100"
  },
  {
    "text": "Very important here,\nthis is not squared. So as a matter of fact,\nthere's a weird thing",
    "start": "1858100",
    "end": "1863660"
  },
  {
    "text": "when you show this to someone. It's like if you\nshow someone d, d, they just don't\neven see it, often.",
    "start": "1863660",
    "end": "1870050"
  },
  {
    "text": "So many people when shown\nthis, their retina-- I didn't even know where it\nhappens in the optical cortex,",
    "start": "1870050",
    "end": "1877520"
  },
  {
    "text": "but a 2 appears here. And then you have to go-- so you\nhave to actually emphasize this when you show this to\npeople because they're",
    "start": "1877520",
    "end": "1884163"
  },
  {
    "text": "like, yeah, sure, sure. Then they go to the next one. And something paints in a\n2 here just because that's",
    "start": "1884163",
    "end": "1890343"
  },
  {
    "text": "the traditional thing. You don't paint it in here. So that's total variation. OK.",
    "start": "1890343",
    "end": "1895460"
  },
  {
    "text": "So simple version of that\nhere is to estimate this,",
    "start": "1895460",
    "end": "1901460"
  },
  {
    "text": "we make a two vector,\nwhich is the x gradient is just the difference here\nand the y gradient is this.",
    "start": "1901460",
    "end": "1910417"
  },
  {
    "text": "It turns out that's actually\nnot the way to do it. You get artifacts that\nalign with the axes. And there's actually\nway cooler ways",
    "start": "1910417",
    "end": "1916250"
  },
  {
    "text": "to estimate the gradient\nbased on, well, I don't know, if you study PDEs or something,\nyou get a little template.",
    "start": "1916250",
    "end": "1922380"
  },
  {
    "text": "And you make a gradient here\nthat depends on these things. I think they call that\na stencil or something.",
    "start": "1922380",
    "end": "1927890"
  },
  {
    "text": "And there's some\ncoefficients in there to map those five values\nto two gradient numbers.",
    "start": "1927890",
    "end": "1932900"
  },
  {
    "text": "This is the space x gradient\nand the space y gradient. Everybody got this? And you can make those so that\nthey are actually approximately",
    "start": "1932900",
    "end": "1941360"
  },
  {
    "text": "rotation invariant. So you don't get the artifacts. I only mention this. This is the main idea here.",
    "start": "1941360",
    "end": "1948420"
  },
  {
    "text": "This is the crudest possible\nstencil or approximation of the space\ngradient of an image",
    "start": "1948420",
    "end": "1956400"
  },
  {
    "text": "would be its x differences\nand its y differences. But you could do\nactually better. OK. So you get something that\nlooks like that with that very",
    "start": "1956400",
    "end": "1963000"
  },
  {
    "text": "crude approximation. You get something\nthat looks like this and then you end up with\nsomething like that.",
    "start": "1963000",
    "end": "1968940"
  },
  {
    "text": "OK. So we'll look at\nan example of this. So here's what\nhappens is I have--",
    "start": "1968940",
    "end": "1976950"
  },
  {
    "text": "this is probably\ntomography or something. So this is my\noriginal thing in 2D.",
    "start": "1976950",
    "end": "1983100"
  },
  {
    "text": "I think it's a\ntomography problem. So I calculate line\nintegrals through it",
    "start": "1983100",
    "end": "1989340"
  },
  {
    "text": "and then I reconstruct it. And if you use total\nvariation to do this--",
    "start": "1989340",
    "end": "1995415"
  },
  {
    "text": " and this is again\na case where you have 8 eight times\nmore variables",
    "start": "1995415",
    "end": "2001460"
  },
  {
    "text": "to predict than measurements. So it's the same\nas the last one. You just reproduce it like\npretty much perfectly. Actually, if you\nlook closely, you'll",
    "start": "2001460",
    "end": "2007973"
  },
  {
    "text": "see that there are\nweird artifacts, but that would be fixed\nif you did a fancier version of this approximation.",
    "start": "2007973",
    "end": "2013730"
  },
  {
    "text": "And again, if you just\nwanted to compare that, that's what the l2\nreconstruction would look like.",
    "start": "2013730",
    "end": "2020419"
  },
  {
    "text": "So it's not bad. It knows that there were\nsome line integrals that went through the main part of\nit that came out pretty big.",
    "start": "2020420",
    "end": "2027740"
  },
  {
    "text": "And from that,\nyou would conclude there's something\nthere in the middle and it's basically a blob.",
    "start": "2027740",
    "end": "2034010"
  },
  {
    "text": "So this is standard. A lot of people do\nother stuff now, but these were regularizers\nthat were a lot better than what",
    "start": "2034010",
    "end": "2044120"
  },
  {
    "text": "people had before. ",
    "start": "2044120",
    "end": "2050219"
  },
  {
    "text": "We're now going to mention\na little bit about-- just for fun, the idea of an\niterated weighted l1 heuristic.",
    "start": "2050219",
    "end": "2056429"
  },
  {
    "text": "And the way that\nworks is this is I start with all the weights\none here and I do l1.",
    "start": "2056429",
    "end": "2063510"
  },
  {
    "text": "Then what I do is\nactually really cool. ",
    "start": "2063510",
    "end": "2069270"
  },
  {
    "text": "If xi is 0 here, I set the\nweight to 1 over epsilon. Epsilon is a small number and\nthat means it's really high.",
    "start": "2069270",
    "end": "2075750"
  },
  {
    "text": "If x is big, I actually\nreduce the weight below 1.",
    "start": "2075750",
    "end": "2081419"
  },
  {
    "text": "So basically, what this\nis doing this update here is actually doing triage.",
    "start": "2081420",
    "end": "2087322"
  },
  {
    "text": "It's basically\nwho's going to live and who's not going to live. So if you're small or\n0, your weight goes up",
    "start": "2087322",
    "end": "2095790"
  },
  {
    "text": "and that means you're a prime\ntarget for being mapped to 0 in the next l1 minimization. If you're big, your\nweight is reduced,",
    "start": "2095790",
    "end": "2103480"
  },
  {
    "text": "which means that basically\nif I anthropomorphize that means you've\nconcluded that whatever,",
    "start": "2103480",
    "end": "2110500"
  },
  {
    "text": "x7 is going to be nonzero. And so you reduce the weight. This is actually\ncombines polishing",
    "start": "2110500",
    "end": "2116380"
  },
  {
    "text": "there because polishing is\nan extreme form of this. Polishing goes like this,\nsolve the l1 problem.",
    "start": "2116380",
    "end": "2122319"
  },
  {
    "text": "And you're zero, put an\ninfinite weight on you. And that means you're\nnever going to be nonzero. ",
    "start": "2122320",
    "end": "2129250"
  },
  {
    "text": "You're nonzero and you go\nput your weight to zero, meaning no regularization.",
    "start": "2129250",
    "end": "2134440"
  },
  {
    "text": "And then resolve. So polishing is a version,\nan extreme version of this. This is like iterative\nsoft polishing or something",
    "start": "2134440",
    "end": "2140740"
  },
  {
    "text": "weird like that. OK. So this actually works quite\nwell for a lot of things.",
    "start": "2140740",
    "end": "2148090"
  },
  {
    "text": "Very few people know about this. I don't know why. But it's true that many\npeople don't know about it.",
    "start": "2148090",
    "end": "2154020"
  },
  {
    "text": "And you get a beautiful\ninterpretation by doing something like\nthis, which is very cool.",
    "start": "2154020",
    "end": "2161220"
  },
  {
    "text": "So let's start with\nthe actual card function which looks like this.",
    "start": "2161220",
    "end": "2167350"
  },
  {
    "text": "There's the card function and\nsomeone comes along and says, well, I'm going to\nuse an l1 heuristic.",
    "start": "2167350",
    "end": "2175208"
  },
  {
    "text": "And they go, what's that? And you go, well,\ninstead of your function, I'm going to use this function.",
    "start": "2175208",
    "end": "2183520"
  },
  {
    "text": "How about that? And they go, that's not a\nvery good approximation. Yeah, but it seems\nto work pretty well.",
    "start": "2183520",
    "end": "2190319"
  },
  {
    "text": "So here what you do\ninstead is you write down.",
    "start": "2190320",
    "end": "2196140"
  },
  {
    "text": "You make a non-convex function\nthat it looks close to this and I'm going to write\ndown what that function is.",
    "start": "2196140",
    "end": "2202380"
  },
  {
    "text": "It's going to be this. I'm going to make a weird\nfunction that goes like this. Actually, over here\nit'll be minus--",
    "start": "2202380",
    "end": "2208020"
  },
  {
    "text": "I can subtract something\nto make it like this. It's going to look like that. ",
    "start": "2208020",
    "end": "2216880"
  },
  {
    "text": "And that you would say\nthat's a better approximation of this cardinality.",
    "start": "2216880",
    "end": "2222869"
  },
  {
    "text": "Oh, by the way,\npeople do this also with powers, which is cool. So you do the square root. That's another one you would--",
    "start": "2222870",
    "end": "2228900"
  },
  {
    "text": "here's the real thing. Sorry, this is like that. That's like that. And people would put\na square root here.",
    "start": "2228900",
    "end": "2236099"
  },
  {
    "text": "Then they would write extremely\nannoying papers about-- they would say lp approximation\nwith p equals 0.5,",
    "start": "2236100",
    "end": "2243790"
  },
  {
    "text": "which drives me nuts because\nthere's no such thing as an l0.5 norm. Oh, even worse, people refer\nto cardinality as the l0 norm.",
    "start": "2243790",
    "end": "2251940"
  },
  {
    "text": "So has anyone heard that? Don't let that stand. If you are in a social\nsituation and someone",
    "start": "2251940",
    "end": "2258660"
  },
  {
    "text": "says l1 norm, say\nsomething like, excuse me, that's not a norm.",
    "start": "2258660",
    "end": "2265400"
  },
  {
    "text": "And if they say,\noh, that's cool. Sorry, I thought we were just\nfriends here or whatever, just casual, then you go,\nfine, but just to check",
    "start": "2265400",
    "end": "2272180"
  },
  {
    "text": "that they know that. If they don't, then I don't\nleave the room or something because you should not hang\nout with people like that.",
    "start": "2272180",
    "end": "2278750"
  },
  {
    "text": "It's like people who talk\nabout an overcomplete basis, and you're like, what?",
    "start": "2278750",
    "end": "2283790"
  },
  {
    "text": "By the way, otherwise\nknown as a set of vectors. So just, anyway, all right.",
    "start": "2283790",
    "end": "2288980"
  },
  {
    "text": "Fine. All right. So it turns out here\nyou do something--",
    "start": "2288980",
    "end": "2296420"
  },
  {
    "text": "you just linearize this\nobjective at the current point and then you get exactly this\niterated reweighted heuristic.",
    "start": "2296420",
    "end": "2305990"
  },
  {
    "text": "So you can think\nof it as doing a-- people sometimes call that\nsequential convex optimization.",
    "start": "2305990",
    "end": "2311580"
  },
  {
    "text": "So that's when you have a\nterm, a constraint term, a function that's not convex.",
    "start": "2311580",
    "end": "2318599"
  },
  {
    "text": "And what you do is\nyou approximate it by a convex function at each\nstep and solve that problem.",
    "start": "2318600",
    "end": "2324170"
  },
  {
    "text": "And this is an example of that. And this works quite well.",
    "start": "2324170",
    "end": "2331010"
  },
  {
    "text": "So that's it. And I'll just show you some\nquick examples of that. So here would be a\nbaby example, right,",
    "start": "2331010",
    "end": "2337519"
  },
  {
    "text": "is we want to find\na sparse solution in a polyhedron of a set\nof linear inequalities.",
    "start": "2337520",
    "end": "2345020"
  },
  {
    "text": "So what we do is we start\nwith the l1 heuristic and you come back\nwith something that",
    "start": "2345020",
    "end": "2350330"
  },
  {
    "text": "has cardinality 44,\nwhatever, it doesn't matter.",
    "start": "2350330",
    "end": "2355480"
  },
  {
    "text": "So if you run this\niterated l1 heuristic,",
    "start": "2355480",
    "end": "2360685"
  },
  {
    "text": "these are the iterations,\nwhat you can see is like, I don't know, in\nthree, four steps, you've got in four steps,\nyou've got something",
    "start": "2360685",
    "end": "2367480"
  },
  {
    "text": "with a cardinality of 36. By the way, for this\nproblem, for fun, we just solved it globally and\nthe global solution is 32.",
    "start": "2367480",
    "end": "2376063"
  },
  {
    "text": "By the way, that\nworked perfectly. It was a randomly\ngenerated example. Because had the global\nsolution been 36,",
    "start": "2376063",
    "end": "2381619"
  },
  {
    "text": "this would be misleading. So this tells you everything. This example worked\nout perfectly",
    "start": "2381620",
    "end": "2387080"
  },
  {
    "text": "to say exactly the honest and\ncorrect story, which is this is a really good heuristic.",
    "start": "2387080",
    "end": "2392492"
  },
  {
    "text": "And if someone says does it\nalways get the global solution? The answer is no. It doesn't. And here's a perfectly\ngood example.",
    "start": "2392492",
    "end": "2398060"
  },
  {
    "text": " So this is just a\nquick example of that.",
    "start": "2398060",
    "end": "2404730"
  },
  {
    "text": "So you can use this\nin all sorts of-- I mean, once you understand\nabout these methods, you can use them\nin very cool ways.",
    "start": "2404730",
    "end": "2410750"
  },
  {
    "text": "So here's a quick example. I have an AR model. So an autoregressive model.",
    "start": "2410750",
    "end": "2415859"
  },
  {
    "text": "And what we're\ngoing to do is we're going to assume that the\ndynamics of the AR model,",
    "start": "2415860",
    "end": "2422640"
  },
  {
    "text": "which is the a and\nthe b, they changed. There's a lot of things for this\nwhere you want to fit a model",
    "start": "2422640",
    "end": "2429260"
  },
  {
    "text": "but it changes over time\nor something like that. And so what you do is actually\nsomething pretty cool.",
    "start": "2429260",
    "end": "2436260"
  },
  {
    "text": "It's a weird idea\nbut here it is. What you do is you\nstart by actually",
    "start": "2436260",
    "end": "2441500"
  },
  {
    "text": "assuming that you introduce\na number of variables. So if you were just\ndoing an AR 2 fit, a and b would be constant\nand they'd be two variables.",
    "start": "2441500",
    "end": "2448250"
  },
  {
    "text": "And you'd have a bunch\nof data because you'd have t equals 100 time\nsteps or a couple hundred and everything would make\nsense to fit an AR 2 model.",
    "start": "2448250",
    "end": "2456530"
  },
  {
    "text": "But here what you\ndo if someone says, no, no, my AR 2 model\nis time-varying. And you go, OK, cool.",
    "start": "2456530",
    "end": "2462380"
  },
  {
    "text": "So you actually have a different\na and b for each time period. Minor problem with that is\nyou're now hideously overfit,",
    "start": "2462380",
    "end": "2470779"
  },
  {
    "text": "at least 2 to 1,\nbecause you've got-- so it's not just that you\ncan match something exactly,",
    "start": "2470780",
    "end": "2477950"
  },
  {
    "text": "you have to put something,\nsome prior on a and b. You can have lots of priors.",
    "start": "2477950",
    "end": "2484700"
  },
  {
    "text": "Here would be one-- a\nand b are increasing. I just made that up. Here's one. They vary smoothly, meaning\nthe sum of the squares",
    "start": "2484700",
    "end": "2491990"
  },
  {
    "text": "of the differences is small. Everybody got that? That would be a smoothly\nvarying AR model.",
    "start": "2491990",
    "end": "2498830"
  },
  {
    "text": "Here's another one. You have a piecewise\nconstant AR model, meaning a and b are constants\nfor various periods,",
    "start": "2498830",
    "end": "2506750"
  },
  {
    "text": "then there's some shift and\nthey assume different values. Everybody got this? But by now, at this\npoint in the class,",
    "start": "2506750",
    "end": "2513290"
  },
  {
    "text": "you have all the\ntools to do like you could do all sorts of\ncrazy stuff with this.",
    "start": "2513290",
    "end": "2520010"
  },
  {
    "text": "So you do this. So this is your-- this is\nbasically the regularizer",
    "start": "2520010",
    "end": "2525770"
  },
  {
    "text": "and this is the loss. So this is just a traditional\nsquare loss or Gaussian,",
    "start": "2525770",
    "end": "2532820"
  },
  {
    "text": "whatever you want,\nv of t is Gaussian. So then this is a regularizer\nwhich if you read it,",
    "start": "2532820",
    "end": "2539550"
  },
  {
    "text": "you will understand\ncompletely is a sparsifier for differences\nchanges in a and b.",
    "start": "2539550",
    "end": "2548900"
  },
  {
    "text": "Everybody got that? That's a sparsifier\nfor the difference, which is basically saying I\nwant a piecewise constant.",
    "start": "2548900",
    "end": "2555290"
  },
  {
    "text": "So here's a quick example. So here's some data\nthat was generated.",
    "start": "2555290",
    "end": "2560525"
  },
  {
    "text": "Here's a and b. It doesn't matter. But actually, since\nyou see a and b, you can see they change at t\nequals 100 and t equals 200.",
    "start": "2560525",
    "end": "2567950"
  },
  {
    "text": "And if you look at your\neyeball and draw a line here, you can look at that and look\nat that and your eyeball,",
    "start": "2567950",
    "end": "2573830"
  },
  {
    "text": "your spectral analyzer\nin your eyeball says something happened. And then you can again, this\nis hard, but you could pretend,",
    "start": "2573830",
    "end": "2583910"
  },
  {
    "text": "especially after\nyou knew the truth, you could certainly\npretend that you'd say, oh, yeah, no, I can see that\nthe spectrum is different.",
    "start": "2583910",
    "end": "2591120"
  },
  {
    "text": "In fact, the\nautocorrelation changes. That's where people\nwho have the ability",
    "start": "2591120",
    "end": "2596910"
  },
  {
    "text": "to look at a plot\nof something and say what the autocorrelation\nis in their head.",
    "start": "2596910",
    "end": "2601980"
  },
  {
    "text": "So there are people like that. But anyway, so you\ncan see something happened in that three times. And the question is, from\nthe signal on the left,",
    "start": "2601980",
    "end": "2610440"
  },
  {
    "text": "how would we guess these\nchanging coefficients? And here's what happens.",
    "start": "2610440",
    "end": "2616869"
  },
  {
    "text": "If you just do\nthat l1 heuristic, you get this dashed line here. And it gets it.",
    "start": "2616870",
    "end": "2622810"
  },
  {
    "text": "It's pretty good. You can see things like-- it got pretty close to\nwhere this change was.",
    "start": "2622810",
    "end": "2629829"
  },
  {
    "text": "It was pretty good up there. But you see there's some\nweird little things like that and this is sad.",
    "start": "2629830",
    "end": "2635770"
  },
  {
    "text": "That's weird. If you do this\niterated l1 heuristic, you get that over there.",
    "start": "2635770",
    "end": "2641089"
  },
  {
    "text": "That's actually pretty good. That's good. And actually, it's good.",
    "start": "2641090",
    "end": "2648760"
  },
  {
    "text": "I swear I would have rerun this. If it turns out it had\nnailed the actual times",
    "start": "2648760",
    "end": "2654250"
  },
  {
    "text": "or the amplitudes\nexactly, I would have rerun it because\nthat would have been sending the wrong message. But fortunately, it did not.",
    "start": "2654250",
    "end": "2661100"
  },
  {
    "text": "It's fine. This is what you want to see. It does a pretty good\njob of doing this. But, by the way, for vector--",
    "start": "2661100",
    "end": "2669244"
  },
  {
    "text": "I'm sorry, for scalar AR\n2 models, no big deal, your eyeball can do it.",
    "start": "2669244",
    "end": "2674420"
  },
  {
    "text": "But what if I gave you\nan econometric time series with 50 numbers?",
    "start": "2674420",
    "end": "2680420"
  },
  {
    "text": "The dimension of 50 and I gave\nit to you monthly for 50 years and you wanted to do this?",
    "start": "2680420",
    "end": "2685630"
  },
  {
    "text": "Anyway, I guarantee your eyeball\ncould not do it, but you could.",
    "start": "2685630",
    "end": "2692369"
  },
  {
    "text": "It's like 16 lines of Python. So I'm just saying. I'm showing you baby\nexamples, but trust me,",
    "start": "2692370",
    "end": "2699579"
  },
  {
    "text": "these are the exact same methods\napply to much bigger and much more impressive examples.",
    "start": "2699580",
    "end": "2704900"
  },
  {
    "text": " Last topic is\nextension to matrices.",
    "start": "2704900",
    "end": "2712510"
  },
  {
    "text": "This is actually very cool. So rank is a natural analog\nof cardinality for matrices.",
    "start": "2712510",
    "end": "2719810"
  },
  {
    "text": "There's lots of\nways to say that. And a convex-rank\nproblem is a problem",
    "start": "2719810",
    "end": "2724928"
  },
  {
    "text": "that's convex, except\nthere are rank constraints. There's tons of these problems\nthat come up all the time.",
    "start": "2724928",
    "end": "2733030"
  },
  {
    "text": "And there's lots of\nconnections to them. But it turns out the analog of\nthe l1 norm is extremely cool.",
    "start": "2733030",
    "end": "2739720"
  },
  {
    "text": " This is the dual of\nthe spectral norm.",
    "start": "2739720",
    "end": "2745420"
  },
  {
    "text": "The spectral norm is the\nmaximum of the singular values. The dual of that--",
    "start": "2745420",
    "end": "2751450"
  },
  {
    "text": "surprise, surprise, it's the\nsum of the singular values. By now, you should be seeing\nsome of these patterns. I don't know when you\nstudy music or something",
    "start": "2751450",
    "end": "2758260"
  },
  {
    "text": "and you start hearing,\nyou see themes and things like that, you should\nstart seeing them.",
    "start": "2758260",
    "end": "2763369"
  },
  {
    "text": "So now you know the dual\nof the spectral norm is the sum of the singular\nvalues, which is cool.",
    "start": "2763370",
    "end": "2770270"
  },
  {
    "text": "And guess what? It's a low rank.",
    "start": "2770270",
    "end": "2775640"
  },
  {
    "text": "We don't have a verb\nlike sparsifying, but it's a low rankifying. That's not a verb.",
    "start": "2775640",
    "end": "2780650"
  },
  {
    "text": "But if it were a verb, that's\na low-rankifying regularizer.",
    "start": "2780650",
    "end": "2786740"
  },
  {
    "text": "So lots of cool stuff. I'll give you one example. And now this is research\nfrom only 10 years ago.",
    "start": "2786740",
    "end": "2794030"
  },
  {
    "text": "Suppose I give you some\nsamples from a Gaussian vector,",
    "start": "2794030",
    "end": "2800330"
  },
  {
    "text": "a Gaussian, but I'm\ngoing to give you the prior that the inverse of\nthe covariance matrix, which",
    "start": "2800330",
    "end": "2806330"
  },
  {
    "text": "is the precision matrix is\nsparse, which by the way, has a beautiful interpretation\nbecause sparsity",
    "start": "2806330",
    "end": "2813540"
  },
  {
    "text": "in the precision\nmatrix corresponds to conditional independence. So basically says,\nhere are these things.",
    "start": "2813540",
    "end": "2820310"
  },
  {
    "text": "My prior is that it comes\nfrom a sparse Bayesian model because that's\nexactly what that is.",
    "start": "2820310",
    "end": "2825815"
  },
  {
    "text": "And by the way, if you don't\nknow what I'm talking about, well, you should\nprobably learn this stuff because it's really cool.",
    "start": "2825815",
    "end": "2831207"
  },
  {
    "text": "If you do then that's fine, too.  Then you would know what to do.",
    "start": "2831207",
    "end": "2837880"
  },
  {
    "text": "You maximize the log-likelihood\nminus the dual norm of the precision matrix.",
    "start": "2837880",
    "end": "2845530"
  },
  {
    "text": "And you would be about 8 to\n10 years ago in research. So it's cool.",
    "start": "2845530",
    "end": "2852730"
  },
  {
    "text": "You're very close to the\nboundaries of research here. OK.",
    "start": "2852730",
    "end": "2858340"
  },
  {
    "text": "I think maybe I'll even skip\nfactor modeling as an example and I'll quit there.",
    "start": "2858340",
    "end": "2866090"
  },
  {
    "text": "And now we want to switch\nto something else, which is going to be--",
    "start": "2866090",
    "end": "2871737"
  },
  {
    "text": "oops, let's switch over to here. ",
    "start": "2871737",
    "end": "2877980"
  },
  {
    "text": "OK. So at this point,\nthe class is over. So we're done. I mean, not for you, actually.",
    "start": "2877980",
    "end": "2884225"
  },
  {
    "text": "Actually, for that matter or us. But you'll finish your\nhomework 8 by tomorrow.",
    "start": "2884225",
    "end": "2890662"
  },
  {
    "text": "Hopefully, you'll get a\nworking barrier method or something like that and then\nyou'll feel good about that. You should feel good about that.",
    "start": "2890663",
    "end": "2896810"
  },
  {
    "text": "Then you have a week to poke\nthrough all the material. You really should\ngo back for fun",
    "start": "2896810",
    "end": "2902240"
  },
  {
    "text": "and read a couple of sections\nin duality or convex functions because you've been living\nwith them for nine weeks.",
    "start": "2902240",
    "end": "2910160"
  },
  {
    "text": "So it should be like the\nfirst time you saw this and you thought log\ndeterminant or something",
    "start": "2910160",
    "end": "2915800"
  },
  {
    "text": "like that or the min\nof the square root, it probably looked pretty weird. It should look very\nfamiliar to you now, I hope.",
    "start": "2915800",
    "end": "2922714"
  },
  {
    "text": " So you're going to have a\nwhole week of just letting",
    "start": "2922715",
    "end": "2928460"
  },
  {
    "text": "all this stuff process and\nall the neural links formed and stuff like that.",
    "start": "2928460",
    "end": "2933539"
  },
  {
    "text": "So what I want to do now is just\nactually talk about just some of the super high-level\nmain ideas in the class. ",
    "start": "2933540",
    "end": "2944430"
  },
  {
    "text": "So, first of all,\nthere's a ton of problems in any mathematical area\nthat reduced to optimization.",
    "start": "2944430",
    "end": "2952470"
  },
  {
    "text": "So engineering design, data\nanalysis and statistics, economics, management,\nall of these things,",
    "start": "2952470",
    "end": "2959370"
  },
  {
    "text": "they're just\noptimization problems. Frankly, that's a tautology.",
    "start": "2959370",
    "end": "2968550"
  },
  {
    "text": "So, of course, they're\nall optimization problems. That's not an\ninteresting statement. I mean, it's an interesting\nway to think about things.",
    "start": "2968550",
    "end": "2976260"
  },
  {
    "text": "I think it's actually\nsuper clear way to do it. You go into a situation and\npeople are coming at you from four different ways\ntelling you, oh, there's this,",
    "start": "2976260",
    "end": "2982230"
  },
  {
    "text": "and we have to do that,\nand these constraints don't forget about, oh,\nthere's regulatory, blah, blah, and you're like, ah. And you just sit down and\nyou say, OK, wait, sorry.",
    "start": "2982230",
    "end": "2989640"
  },
  {
    "text": "What are the things you're\nallowed to mess with? These are the variables. What are the things\nthat are nonnegotiable?",
    "start": "2989640",
    "end": "2998853"
  },
  {
    "text": "And they would say, well, these,\nand those are your constraints. And then you'd say, what\nwould make you happy?",
    "start": "2998853",
    "end": "3004230"
  },
  {
    "text": "Now they probably are going to\nmention two or three things. And then you know you have\na multicriterion problem.",
    "start": "3004230",
    "end": "3009780"
  },
  {
    "text": "And you'd say, cool,\nI will scalarize it with a bunch of\nweights and we'll play with those later to see\nhow happy we can make you.",
    "start": "3009780",
    "end": "3016290"
  },
  {
    "text": "Everybody following\nwhat I just said? So it's actually not a bad\nway to organize your thoughts. When you get into\nsomething there's people coming at you\nfrom 50 different things",
    "start": "3016290",
    "end": "3022845"
  },
  {
    "text": "and they say, oh, whatever,\nremember rule 144B and remember the Basel requirements for\ncapital and you're like, wah.",
    "start": "3022845",
    "end": "3031109"
  },
  {
    "text": "Anyway, so it's\njust a very good way to actually just do anything. Not just there, but\nactually any field.",
    "start": "3031110",
    "end": "3036270"
  },
  {
    "text": " So basically, tractability.",
    "start": "3036270",
    "end": "3044220"
  },
  {
    "text": "Roughly speaking,\nbut some people get really weird about this. Tractability. The problems that are tractable\nare the ones which are convex.",
    "start": "3044220",
    "end": "3052890"
  },
  {
    "text": "And there's people that get\nweird and extreme about this. I'm not one of them.",
    "start": "3052890",
    "end": "3058109"
  },
  {
    "text": "So they would actually go\nthrough the undergrad CS course and they'd say, ha, you can find\nthe shortest path in a graph",
    "start": "3058110",
    "end": "3064620"
  },
  {
    "text": "from here to here. That's polynomial time. That's not a convex problem. Well, the answer is yes, it is. It turns out it's one where\nthe relaxation has zero duality",
    "start": "3064620",
    "end": "3072480"
  },
  {
    "text": "gap. Or they say, I can take the\nsingular value decomposition. That's not convex. And then it's even weirder.",
    "start": "3072480",
    "end": "3078480"
  },
  {
    "text": "They would say, oh,\nit is because that's a non-convex problem\nwith zero duality gap.",
    "start": "3078480",
    "end": "3083843"
  },
  {
    "text": "This is getting\nweird because it's just we know how to do that.",
    "start": "3083843",
    "end": "3089120"
  },
  {
    "text": "And, of course,\nthe other thing is we shouldn't get too bent\nout of shape about it because in 95% of\napplications when you're",
    "start": "3089120",
    "end": "3097880"
  },
  {
    "text": "solving an optimization\nproblem, that's just a surrogate for\nwhat you really want. And that's actually something\ngood to keep in mind.",
    "start": "3097880",
    "end": "3105770"
  },
  {
    "text": "So if you're doing\nmachine learning, you don't care about minimizing\na loss plus a regularizer.",
    "start": "3105770",
    "end": "3111400"
  },
  {
    "text": "You want to have good\npredictions on data you've never seen before.",
    "start": "3111400",
    "end": "3117550"
  },
  {
    "text": "If you're making\na drone trajectory and you're running MPC\nor something like that, you don't care about solving\nwhatever optimization problem",
    "start": "3117550",
    "end": "3125140"
  },
  {
    "text": "of somebody, you want it to\nfollow the trajectory well and use as little fuel or, in\nthat case, energy as possible.",
    "start": "3125140",
    "end": "3132340"
  },
  {
    "text": "Everybody see what I'm saying? If you're doing\nfinance, you just want to make money\nwithout taking risks.",
    "start": "3132340",
    "end": "3138550"
  },
  {
    "text": "If someone chooses to solve\nsome silly convex optimization problem to do it, good for\nthem, but you're not paid.",
    "start": "3138550",
    "end": "3146710"
  },
  {
    "text": "We should just all bear in mind\nthat in almost all applications the optimization\nproblem you're solving",
    "start": "3146710",
    "end": "3151720"
  },
  {
    "text": "is actually just a surrogate\nfor what you really want. So that's an important\nthing to remember.",
    "start": "3151720",
    "end": "3157569"
  },
  {
    "text": "And when you think about\nthat, then you realize, it's not that big a deal if you\ncan't solve a problem globally.",
    "start": "3157570",
    "end": "3166440"
  },
  {
    "text": "It's perfectly cool if you can. But if you can't, it's\njust not that big a deal.",
    "start": "3166440",
    "end": "3173480"
  },
  {
    "text": "So if someone took\nyour circuit design and reduced the dynamic\npower consumption by 20%,",
    "start": "3173480",
    "end": "3181010"
  },
  {
    "text": "you're happy. Maybe you think could\nsomeone do it by 30, but you're just happy you\njust reduced it to 25%.",
    "start": "3181010",
    "end": "3188430"
  },
  {
    "text": "So I'm just saying we shouldn't\nget too hung up on this. Nevertheless, tractable\nis not a bad thing.",
    "start": "3188430",
    "end": "3194600"
  },
  {
    "text": "[LAUGHS] So to be able to say, I\ncan solve this problem exactly, there's a lot of benefit.",
    "start": "3194600",
    "end": "3199980"
  },
  {
    "text": "I mean, one benefit is\nthere's no babysitting of the algorithm. When you close your eyes\nand think of any place",
    "start": "3199980",
    "end": "3208250"
  },
  {
    "text": "where people widely used\nnon-convex optimization, anyone here can think\nof a bunch of those.",
    "start": "3208250",
    "end": "3214040"
  },
  {
    "text": "It turns out actually\nlike quote solving those problems which\nyou're not actually doing is actually an art.",
    "start": "3214040",
    "end": "3219690"
  },
  {
    "text": "And somebody says,\noh, no, no, you should be using this\nand that and you should change the learning\nrate and blah, blah, blah.",
    "start": "3219690",
    "end": "3225540"
  },
  {
    "text": "And that's just all\nshort-circuited. You just don't have it. Or what's your initial--\nlike you asked, what's the initial condition?",
    "start": "3225540",
    "end": "3232710"
  },
  {
    "text": "That's a pretty\ngood circuit design, but what was your\ninitial design? And the point about this is\nthat's just short-circuited.",
    "start": "3232710",
    "end": "3238290"
  },
  {
    "text": "There's no such thing. You don't have any of that. It's just all gone. The other advantage\nof course being",
    "start": "3238290",
    "end": "3244859"
  },
  {
    "text": "the fact that they can be\nmade unbelievably reliable and therefore they\ncan be embedded.",
    "start": "3244860",
    "end": "3249940"
  },
  {
    "text": "So they are embedded\nin control systems where they solve\n1,000 QPs per second.",
    "start": "3249940",
    "end": "3256990"
  },
  {
    "text": "And they do that for hours and\nhours and days and days, just forever. And it just works.",
    "start": "3256990",
    "end": "3262480"
  },
  {
    "text": "And try doing that\nwith a non-convex thing because it's not going to\ntake long before something",
    "start": "3262480",
    "end": "3268630"
  },
  {
    "text": "fails to converge or\nsomething like that. OK.",
    "start": "3268630",
    "end": "3274060"
  },
  {
    "text": "So it turns out a\nlot of applications that you've seen in this\nclass they can be formulated as convex problems. We go a little bit\noverboard on it",
    "start": "3274060",
    "end": "3280390"
  },
  {
    "text": "because you're in a\nlittle sandbox now. But once you finish the\ncourse, you can venture out.",
    "start": "3280390",
    "end": "3287180"
  },
  {
    "text": "So it turns out most problems\naren't really convex. But it turns out once\nyou know these tools, you're going to be unbelievably\neffective at problems",
    "start": "3287180",
    "end": "3294880"
  },
  {
    "text": "that aren't convex. if someone will say, I\nwant to do a trade list. You say, oh, but there's a\nminimum non-zero trade size.",
    "start": "3294880",
    "end": "3302250"
  },
  {
    "text": "I'm sorry, don't come\nback and tell me to buy $300 worth of something. Minimum is $10,000.",
    "start": "3302250",
    "end": "3307990"
  },
  {
    "text": "Everybody see what I'm saying? It's either 0 or a\nminimum of 10,000. That actually\nhappens all the time.",
    "start": "3307990",
    "end": "3315700"
  },
  {
    "text": "But you know these things. All of you could figure\nout a heuristic for that",
    "start": "3315700",
    "end": "3321500"
  },
  {
    "text": "and it would be extremely good. So, in fact,\nsomebody just come up with a heuristic\nfor that right now.",
    "start": "3321500",
    "end": "3329565"
  },
  {
    "text": "Let's make it simple. I'm going to sell\na bunch of stuff. I have 6,000 assets,\nI'm going to sell some.",
    "start": "3329565",
    "end": "3334579"
  },
  {
    "text": "But here are the rules. Don't ask me to sell\n$300 worth of something. That's insulting.",
    "start": "3334580",
    "end": "3339710"
  },
  {
    "text": "Make it something--\nit's either you don't sell any or it's $10,000 worth. Somebody come up with\na heuristic right now.",
    "start": "3339710",
    "end": "3345800"
  },
  {
    "text": "And the rest of\nthe problem is I've got transaction cost,\nrisk, blah, blah, blah. I don't know. So somebody give me some--",
    "start": "3345800",
    "end": "3352007"
  },
  {
    "text": "well, how would you do it?  I say you could do it right now.",
    "start": "3352007",
    "end": "3359580"
  },
  {
    "text": "Well, maybe after\nsome reflection. What would you do? What's the first thing you\nwould do in that situation?",
    "start": "3359580",
    "end": "3367837"
  },
  {
    "text": "Let me describe it\nin a different way. Somebody comes into you and\nthey're ranting and raving and they say, here's\nwhat I want you to do.",
    "start": "3367837",
    "end": "3374110"
  },
  {
    "text": "And they list off 12\nconstraints and objectives which are convex and one that's not.",
    "start": "3374110",
    "end": "3381377"
  },
  {
    "text": "Do you have a suggestion for me?  The street fighting\nsuggestion, what is it?",
    "start": "3381377",
    "end": "3387050"
  },
  {
    "text": "What would you do? Relax the last one. Yeah, relax the last one. Which means? Let's be brutally honest.",
    "start": "3387050",
    "end": "3393151"
  },
  {
    "text": "[INAUDIBLE] How about this? How about just ignore it? [LAUGHTER] ",
    "start": "3393152",
    "end": "3399230"
  },
  {
    "text": "That's a strong\nform of relaxation. So you solve it without it.",
    "start": "3399230",
    "end": "3405214"
  },
  {
    "text": "You can put an l1 norm\nin there, that's fine. Let's put an l1 norm\nin there and then you look at all your trades,\nand then what do you do?",
    "start": "3405215",
    "end": "3413630"
  },
  {
    "text": "If they come out above\n$10,000, they're fine. If they're 0, they're fine.",
    "start": "3413630",
    "end": "3418978"
  },
  {
    "text": "If they're in that weird\ndead band in the middle, they're not cool according to\nyour trading desk or something. Everybody following this?",
    "start": "3418978",
    "end": "3425390"
  },
  {
    "text": "I could transpose this\nto any field there is. I guarantee you, exact\nidentical things happen.",
    "start": "3425390",
    "end": "3431059"
  },
  {
    "text": "Here's one, a generator is\neither off and generating 0 power, or the minimum\nis 10 megawatts.",
    "start": "3431060",
    "end": "3438900"
  },
  {
    "text": "So it's 0 or between\n10 and 30 megawatts. End of story. That's identical to this.",
    "start": "3438900",
    "end": "3445330"
  },
  {
    "text": "All right. So what you do now\nis anybody-- what you do is you solve it once\nthen you simply have a threshold",
    "start": "3445330",
    "end": "3451530"
  },
  {
    "text": "and you decide which generator\nis on and which is off. Now you resolve with\nthat as a constraint.",
    "start": "3451530",
    "end": "3456900"
  },
  {
    "text": "If you're on, you're bigger\nthan your minimum power. Let's make it 10 megawatts. If you're trading you'd\nsay I'm going to sell more.",
    "start": "3456900",
    "end": "3463530"
  },
  {
    "text": "Anybody that came if you're on-- I'm actually selling\nyou, I have to sell more than $10,000 worth.",
    "start": "3463530",
    "end": "3469049"
  },
  {
    "text": "Everybody got this? And everything else is 0. That's two convex problems. This would work beautifully.",
    "start": "3469050",
    "end": "3474880"
  },
  {
    "text": "It's heuristic. This is just an example\nto say that we do stuff",
    "start": "3474880",
    "end": "3480539"
  },
  {
    "text": "that's mostly just convex. But once you absorb\nall these tools, you will be extremely\neffective at things like that.",
    "start": "3480540",
    "end": "3487385"
  },
  {
    "text": " All right.",
    "start": "3487385",
    "end": "3492750"
  },
  {
    "text": "So the theoretical\nconsequences of convexity are local optima or global.",
    "start": "3492750",
    "end": "3499380"
  },
  {
    "text": "Duality theory is awesome. Basically, I think of it as\na completely systematic way",
    "start": "3499380",
    "end": "3505530"
  },
  {
    "text": "to derive sophisticated lower\nbounds on the optimal value of a problem, which is cool.",
    "start": "3505530",
    "end": "3512549"
  },
  {
    "text": "In the case of\nconvexity, they become necessary and sufficient.",
    "start": "3512550",
    "end": "3517769"
  },
  {
    "text": "I mean, not quite. There's a little asterisk. There's some constraint\nqualification, but that's fine.",
    "start": "3517770",
    "end": "3523560"
  },
  {
    "text": "And that's super useful. It allows you to do things\nsensitivity analysis.",
    "start": "3523560",
    "end": "3528630"
  },
  {
    "text": "It produces things like\ninfeasibility certificates. When you come back and you go, I\ncan't do it, I can't design it.",
    "start": "3528630",
    "end": "3535660"
  },
  {
    "text": "Someone says, why not? And you go, no one can. And they go, well,\nyou can't, but I'll fire you and your group\nand I'll get a new one",
    "start": "3535660",
    "end": "3541800"
  },
  {
    "text": "and they will be able to maybe. And you go, no, because I\ncan actually easily prove that no one can do it.",
    "start": "3541800",
    "end": "3546810"
  },
  {
    "text": "Everybody follow me? That's a certificate\nof infeasibility. So that's cool. ",
    "start": "3546810",
    "end": "3554100"
  },
  {
    "text": "If you care about the worst-case\ncomplexity theory then there is one. So that's fine.",
    "start": "3554100",
    "end": "3561160"
  },
  {
    "text": "The practical ones are actually\nI think much more interesting. So basically, if a problem\nis convex, you can solve it.",
    "start": "3561160",
    "end": "3570790"
  },
  {
    "text": "I'm aware of no situation\nwhere someone proposed a convex",
    "start": "3570790",
    "end": "3577090"
  },
  {
    "text": "problem that they\ncouldn't end up-- not all of them are\ngoing to work with CVXPY,",
    "start": "3577090",
    "end": "3585160"
  },
  {
    "text": "but people in machine learning\nthey were like, excuse me, I have a billion samples. And by the way, 100\nmillion parameters.",
    "start": "3585160",
    "end": "3591770"
  },
  {
    "text": "So it's not working. People figure it out. In that case, you use\nthese other methods like subgradient methods\nor something like that.",
    "start": "3591770",
    "end": "3600110"
  },
  {
    "text": "People for images. And you can do a lot\nof that stuff, too. It's all fine.",
    "start": "3600110",
    "end": "3606470"
  },
  {
    "text": "So anyway, this\nis the basic idea. But for normal-sized\nproblems, if you're not",
    "start": "3606470",
    "end": "3613760"
  },
  {
    "text": "doing images or giant things\nor handling video or something like that, then you're within\nstriking distance of these kind",
    "start": "3613760",
    "end": "3624559"
  },
  {
    "text": "of easy-to-use tools. So the implications of things\nlike CVXPY are immense.",
    "start": "3624560",
    "end": "3632090"
  },
  {
    "text": "And actually, I can\ntell you specifically why you should be extremely\ngrateful that you're",
    "start": "3632090",
    "end": "3637760"
  },
  {
    "text": "taking this in a\npost-CVXPY world because I'll tell you how\nit worked before that. Here's the way it worked.",
    "start": "3637760",
    "end": "3643849"
  },
  {
    "text": "I'd go into my\noffice, there'll be a bunch of grad students\nthere, and I'd say, I've got this\nunbelievably good idea.",
    "start": "3643850",
    "end": "3650150"
  },
  {
    "text": "Here's how we're going to--\nwe're solve this convex problem and it's going to work really\nwell for some application.",
    "start": "3650150",
    "end": "3659480"
  },
  {
    "text": "And they go, cool. Then here's what would happen. I would go home, have a nice\ndinner, that kind of thing.",
    "start": "3659480",
    "end": "3666270"
  },
  {
    "text": "Grad students would sit\naround and implement some-- and it was not--\nit's not E, I mean, they would have MOSEK or some\ncommercial solver, who knows,",
    "start": "3666270",
    "end": "3674130"
  },
  {
    "text": "or some other one. They'd be writing C++ and\nbasically stuffing matrices",
    "start": "3674130",
    "end": "3679140"
  },
  {
    "text": "by themselves. Everyone following this? It would not be fun. I would come in the next\nmorning all well-rested",
    "start": "3679140",
    "end": "3686170"
  },
  {
    "text": "and I'd say, hey, how'd it go? And they'd go, oh,\nwe implemented it. And I go, cool. How's does it work? And they go, not too well.",
    "start": "3686170",
    "end": "3691470"
  },
  {
    "text": "And I go, oh, well, I\ngot another great idea. [LAUGHTER]  So I think actually,\nit's basically",
    "start": "3691470",
    "end": "3698200"
  },
  {
    "text": "grad students who should be\nmost grateful for the existence of these high-level tools\nbecause they're the,",
    "start": "3698200",
    "end": "3704980"
  },
  {
    "text": "I believe, the\nchief beneficiaries. OK. So how do you use it?",
    "start": "3704980",
    "end": "3712099"
  },
  {
    "text": "People have different\nideas about this, but these are just mine. Basically, I'd start with\nthis rapid prototyping.",
    "start": "3712100",
    "end": "3721660"
  },
  {
    "text": "So you start with really\nsmall problem instances and you can use some\ninefficient method.",
    "start": "3721660",
    "end": "3728860"
  },
  {
    "text": "It doesn't matter. By the way, if you don't like\nthe results there, then you just--",
    "start": "3728860",
    "end": "3733980"
  },
  {
    "text": "if someone else is saying like,\nit's never going to scale, it's never going to\nscale, what are you doing? If you don't like what\nit does with problems",
    "start": "3733980",
    "end": "3740650"
  },
  {
    "text": "with 1,000 or 10,000\nvariables, then why on Earth would you think you're\ngoing to like what it does for 1 million or 100 million?",
    "start": "3740650",
    "end": "3747550"
  },
  {
    "text": "By the way, there's\nsome people-- I got yelled at by\nsomeone at Google who once said, \"That's so wrong.\"",
    "start": "3747550",
    "end": "3753740"
  },
  {
    "text": "He said, \"The things we work\non, the interesting things emerge at scale.\"",
    "start": "3753740",
    "end": "3759410"
  },
  {
    "text": "And that's supposed to sound\nreally deep and all that. And it's true. But it's fine. And they're like, no,\nwe implement at scale.",
    "start": "3759410",
    "end": "3766474"
  },
  {
    "text": "And I'm like,\nwell, good for you. So I'm just saying there's\ndiversity of opinions about how to do this. But, anyway, you heard mine,\nand there's others who say that.",
    "start": "3766475",
    "end": "3774830"
  },
  {
    "text": " So you can always do cool stuff.",
    "start": "3774830",
    "end": "3780560"
  },
  {
    "text": "You can work out and simplify\nthe optimality conditions and dual. Honestly, in most cases,\nthat doesn't actually",
    "start": "3780560",
    "end": "3787460"
  },
  {
    "text": "have any practical\nuse because you're going to solve it and stuff. So you don't need\nto worry about that.",
    "start": "3787460",
    "end": "3793770"
  },
  {
    "text": "And in some cases, there\nare some rare cases, but it's actually\ninteresting because it's",
    "start": "3793770",
    "end": "3799160"
  },
  {
    "text": "cool to say, hey, guess what? We get prices automatically. We get energy prices\nin 15-minute intervals",
    "start": "3799160",
    "end": "3806720"
  },
  {
    "text": "at all these different nodes. There you go. This is $87 per megawatt\nhour from 2:00 to 2:15 PM",
    "start": "3806720",
    "end": "3812869"
  },
  {
    "text": "at this node. So there are cases\nwhere it's actually really useful to understand\nthe dual and work it out.",
    "start": "3812870",
    "end": "3819730"
  },
  {
    "text": "In other cases, though,\nit's mostly there to interpret your-- I mean, actually, to\nimpress your friends, then",
    "start": "3819730",
    "end": "3826869"
  },
  {
    "text": "you can sound really-- you don't really care. You would say like, I'm\ndesigning this thing and you'd say what the\ndual is, and they go, oh,",
    "start": "3826870",
    "end": "3833290"
  },
  {
    "text": "that's awesome. That's cool. Did you use that to solve it? And you're like, no. But, anyway.",
    "start": "3833290",
    "end": "3839260"
  },
  {
    "text": "Now, so it turns out once you\nunderstand all the convex stuff and are pretty\nfluent with it, you",
    "start": "3839260",
    "end": "3846730"
  },
  {
    "text": "can do all sorts of crazy\nstuff by repeatedly forming and solving a convex\napproximation. We actually just\ndid that over here.",
    "start": "3846730",
    "end": "3853450"
  },
  {
    "text": "So these are just super\ngood methods to know about. And there's a lot of\nstreet fighting tricks.",
    "start": "3853450",
    "end": "3860950"
  },
  {
    "text": "And someday I'm actually going\nto put maybe a lecture together on street fighting tricks.",
    "start": "3860950",
    "end": "3866440"
  },
  {
    "text": "I'd had everyone\nsign an NDA saying that under no\ncircumstances could they divulge where they\nlearned these things",
    "start": "3866440",
    "end": "3873250"
  },
  {
    "text": "and that I would deny it\nif they pointed at me. Anyway, so I haven't done that\nyet, but that's really cool.",
    "start": "3873250",
    "end": "3879635"
  },
  {
    "text": "I just give you one for fun. Here's an example. You go on an internship\nand someone says,",
    "start": "3879635",
    "end": "3885260"
  },
  {
    "text": "generate me minimum\nfuel trajectories that move this thing\nfrom here to there. And you go, cool.",
    "start": "3885260",
    "end": "3890270"
  },
  {
    "text": "Please tell me\nabout the fuel use. And they go, oh,\nyeah, no problem because an intern last year. And then they dump something\nthis thick on your desk",
    "start": "3890270",
    "end": "3898940"
  },
  {
    "text": "or whatever it is. And they go, yeah, that's it. And then they start talking\nabout thermodynamics and n1",
    "start": "3898940",
    "end": "3904910"
  },
  {
    "text": "and n2 and these turbines\nand their reflux in the past and you're like, bah.",
    "start": "3904910",
    "end": "3909950"
  },
  {
    "text": "Anyway, then you say, do\nyou have a simulator for it? And they go, yeah, we do.",
    "start": "3909950",
    "end": "3915790"
  },
  {
    "text": "Then you quiet, set up\na script to simulate it. And you find out that\nthe fuel use looks",
    "start": "3915790",
    "end": "3923290"
  },
  {
    "text": "like this when you ask for-- this is the thrust\nyou're requesting.",
    "start": "3923290",
    "end": "3930970"
  },
  {
    "text": "And this is at idling and it\nlooks something like this. It goes like that,\nthen it's actually something weird like that,\nand then it goes like that,",
    "start": "3930970",
    "end": "3937450"
  },
  {
    "text": "and then it goes up like that. Everybody got this? And then someone who knows\nthe details would say,",
    "start": "3937450",
    "end": "3942890"
  },
  {
    "text": "well, I know why\nbecause when you get up to this amount of thrust\nthey open these valves and there's some-- I'm just making this up,\nthere's some gas that",
    "start": "3942890",
    "end": "3949327"
  },
  {
    "text": "flows over these four turbines. I don't know. Anyway, but it would\nlook like that. This would be the-- this\nyou got from a simulator.",
    "start": "3949327",
    "end": "3956770"
  },
  {
    "text": "Everybody following this? What would you do\nnow that now we're",
    "start": "3956770",
    "end": "3962260"
  },
  {
    "text": "in full street-fighting mode? What would you do? Well, first thing you do is\nyou'd say, it's not a convex.",
    "start": "3962260",
    "end": "3970240"
  },
  {
    "text": "I want a suggestion. Get a convex surrogate? Yeah. Get a convex surrogate.",
    "start": "3970240",
    "end": "3975569"
  },
  {
    "text": "Do you want me to\nmake one for you? Here's my convex surrogate. Ready?",
    "start": "3975570",
    "end": "3980960"
  },
  {
    "text": "You just go like\nthat or something. It's fine. Now you solve the\nproblem quickly.",
    "start": "3980960",
    "end": "3988170"
  },
  {
    "text": "You could even fit\nthis either by eyeball. If it's in one dimension,\nyou do it by eyeball. And then you just solve\nit with this thing.",
    "start": "3988170",
    "end": "3994800"
  },
  {
    "text": "It's fine. So everybody following this? So I'm just saying\nthat's not now,",
    "start": "3994800",
    "end": "4002810"
  },
  {
    "text": "not until you get your\nactual stamped certificate saying that you're now no\nlonger a trainee or something.",
    "start": "4002810",
    "end": "4011240"
  },
  {
    "text": "But when you get\nthat certificate, you'll be allowed to-- so don't\ndo this for the next 10 days. But after that, you have a\nlicense to do stuff like this.",
    "start": "4011240",
    "end": "4020030"
  },
  {
    "text": "Everybody following this? OK.",
    "start": "4020030",
    "end": "4025052"
  },
  {
    "text": "So there's a whole lot\nof stuff we didn't cover and that's actually\nthat's something I want to talk about what's next.",
    "start": "4025052",
    "end": "4031670"
  },
  {
    "text": "So we didn't talk\nabout this whole area of subgradient methods,\nstochastic subgradient methods. And you've probably seen this\nin a machine learning course",
    "start": "4031670",
    "end": "4038650"
  },
  {
    "text": "or statistics course,\nI would imagine. And it's absolutely fascinating.",
    "start": "4038650",
    "end": "4043730"
  },
  {
    "text": "So a lot of stuff there. Actually, all of these\nare taught in, I guess,",
    "start": "4043730",
    "end": "4049539"
  },
  {
    "text": "several courses. And that's the next thing\nwe're going to talk about. So there's 364B.",
    "start": "4049540",
    "end": "4056413"
  },
  {
    "text": "I think I actually have\na list of courses here but it's way out of date. So this is completely and\nhideously out of date,",
    "start": "4056413",
    "end": "4062380"
  },
  {
    "text": "but that's fine. I'll leave it up here anyway. So you have a couple of-- you may never want to take\na course in this area again.",
    "start": "4062380",
    "end": "4072010"
  },
  {
    "text": "But if you did, I'll\npoint you to some places.",
    "start": "4072010",
    "end": "4078160"
  },
  {
    "text": "So one is 364B, which is\nwe cover a lot of material",
    "start": "4078160",
    "end": "4084250"
  },
  {
    "text": "in that that's super useful. It's not as linear in its\ndependency graph as this class.",
    "start": "4084250",
    "end": "4089920"
  },
  {
    "text": "This class is you do\nthis, then you do that, then you do that,\nthen everything it's just very linear.",
    "start": "4089920",
    "end": "4095050"
  },
  {
    "text": "That one is once you\nknow about subgradients, then the dependency\ngraph gets wide. And there's a lot of\nchoices of topics,",
    "start": "4095050",
    "end": "4101149"
  },
  {
    "text": "but they're just\nsuper good things. Actually, for\nimplementing-- I mean, some of the best things\nyou would learn in 364B",
    "start": "4101149",
    "end": "4107589"
  },
  {
    "text": "would be how to solve relatively\nbig problems that would either",
    "start": "4107590",
    "end": "4114818"
  },
  {
    "text": "take way too long using\nCVXPY or just not even work. So if you were doing\nimage reconstruction",
    "start": "4114819",
    "end": "4121344"
  },
  {
    "text": "or if you're reproducing\nstuff from video data, it's just too big. I'm sorry, you can't plug\nthat in and stuff like that.",
    "start": "4121344",
    "end": "4129369"
  },
  {
    "text": "You'd learn how to do that. You'd probably write it in\nTorch or something like that, but you'd learn very effective,\nsimple methods for that.",
    "start": "4129370",
    "end": "4136990"
  },
  {
    "text": "So that's fine. You'll learn about cool\nthings like distributed convex optimization, which\nis super cool.",
    "start": "4136990",
    "end": "4144339"
  },
  {
    "text": "I'll tell you how that works. You're sitting on\nsome data, you're sitting on some data,\nand your hospitals,",
    "start": "4144340",
    "end": "4149408"
  },
  {
    "text": "and you have some data, too. And I want to fit a model. I don't care a logistic\nregression model of something.",
    "start": "4149408",
    "end": "4158324"
  },
  {
    "text": "What's the likelihood\nthis person is going to develop sepsis? There we go. So you have a bunch of data,\nyou have a bunch of data,",
    "start": "4158325",
    "end": "4163740"
  },
  {
    "text": "you have a bunch of data. The obvious thing\nto do is I'd say, could I please\nhave all your data? So I would collect all\nyour data and then fit",
    "start": "4163740",
    "end": "4171189"
  },
  {
    "text": "a model in distributed. The way it works in distributed\nconvex objects is very cool.",
    "start": "4171189",
    "end": "4176299"
  },
  {
    "text": "It works like this. I don't get your data,\nI don't get your data, I don't get your data,\nbut I coordinate you.",
    "start": "4176300",
    "end": "4184960"
  },
  {
    "text": "So I'd say you fit a logistic\nmodel on your own data. Everybody got that?",
    "start": "4184960",
    "end": "4190580"
  },
  {
    "text": "Then all you do is you\nsend your models to me. I do a very small\namount of computation.",
    "start": "4190580",
    "end": "4196870"
  },
  {
    "text": "And I send a-- oops,\nsorry about that, and I send a\nmessage back to you.",
    "start": "4196870",
    "end": "4202849"
  },
  {
    "text": "You plug another term\ninto your models and refit and this goes 10 steps\norder of magnitude.",
    "start": "4202850",
    "end": "4209880"
  },
  {
    "text": "And at that point, we've\nconverged on the same model. You actually have no idea\nhow many other hospitals",
    "start": "4209880",
    "end": "4217080"
  },
  {
    "text": "I'm talking to. But guess what? We just together\nsolved this problem",
    "start": "4217080",
    "end": "4223680"
  },
  {
    "text": "that's identical to had I\ncollected all of your data in one place and\ndone it all once. Anyway, everyone get that?",
    "start": "4223680",
    "end": "4230159"
  },
  {
    "text": "So that's in 364B. It's actually in a\ncouple of other things, but these are\npretty cool things. We didn't talk about\nit at all here.",
    "start": "4230160",
    "end": "4236010"
  },
  {
    "text": "Not surprisingly it uses\nideas like duality and stuff like that.",
    "start": "4236010",
    "end": "4241200"
  },
  {
    "text": "And the other one is 392F. Is that right? So that's taught by Ernest Ryu.",
    "start": "4241200",
    "end": "4250620"
  },
  {
    "text": "He's a professor at Seoul\nNational University. He just wrote a beautiful book\non monotone operator methods.",
    "start": "4250620",
    "end": "4257940"
  },
  {
    "text": "It'll be a little more\nmathematical than this one. But it's going to\nbe a weird thing because it will look like math.",
    "start": "4257940",
    "end": "4264599"
  },
  {
    "text": "But in fact, everything\nin it is it's very close to extremely\nuseful things.",
    "start": "4264600",
    "end": "4272469"
  },
  {
    "text": "If you took that\ncourse, actually stayed awake through the math. You're just one Torch\nimplementation away",
    "start": "4272470",
    "end": "4279969"
  },
  {
    "text": "from actually an extremely,\ngood very large-scale solver for something.",
    "start": "4279970",
    "end": "4285040"
  },
  {
    "text": "Everybody got that? So that's a cool thing. So I think-- let's see. What I'll do now is--",
    "start": "4285040",
    "end": "4292300"
  },
  {
    "text": "oh, so I have to thank the TAs. You probably-- I\ndon't know, if you",
    "start": "4292300",
    "end": "4298360"
  },
  {
    "text": "have any idea [LAUGHS] how much\nis going on behind the scenes. A lot of it is chaotic and\nyou wouldn't want to know.",
    "start": "4298360",
    "end": "4304210"
  },
  {
    "text": "But mostly, it's just\na whole lot of work just making sure\nscripts actually still",
    "start": "4304210",
    "end": "4311440"
  },
  {
    "text": "work with updates in CVXPY\nand all sorts of other stuff. So, anyway, it'd be impossible\nwithout tons of help from them.",
    "start": "4311440",
    "end": "4321790"
  },
  {
    "text": "And I think we're done. You've mostly survived, I think.",
    "start": "4321790",
    "end": "4328780"
  },
  {
    "text": "Yes. Do you want to mention A-222? Oh, sorry. I should have mentioned\nMichael's class. OK.",
    "start": "4328780",
    "end": "4333930"
  },
  {
    "text": "So other things I would point\nyou to that'd be very cool",
    "start": "4333930",
    "end": "4339060"
  },
  {
    "text": "is actually, I just point you\nto all of Mykel Kochenderfer classes, all of them, and books.",
    "start": "4339060",
    "end": "4345400"
  },
  {
    "text": "So it's simple. Just read them, take the\nclasses, whatever you want. They're going to\nbe extremely good.",
    "start": "4345400",
    "end": "4351810"
  },
  {
    "text": "They're quite different. So they're more-- we focused\non one idea like convexity",
    "start": "4351810",
    "end": "4358890"
  },
  {
    "text": "and for that matter very\nsmall number of methods, his courses are more like\nwe're going to do optimization.",
    "start": "4358890",
    "end": "4365072"
  },
  {
    "text": "We don't care if it's\nconvex, non-convex, and we're going to look at\nmultiple methods to do it. But just beautiful, clear\nbook, incredibly good classes.",
    "start": "4365072",
    "end": "4372960"
  },
  {
    "text": "So I would actually strongly\nurge you to look at those. Some people don't\nknow about them",
    "start": "4372960",
    "end": "4378180"
  },
  {
    "text": "because I guess they're\nlisted in AeroAstro, and then maybe\nthey're cross-listed in CS or something.",
    "start": "4378180",
    "end": "4383400"
  },
  {
    "text": "So people in CS know about it,\nbut I think many others don't. So I would strongly\nrecommend those.",
    "start": "4383400",
    "end": "4390550"
  },
  {
    "text": "All right. Cool? All right. So I actually think we're done.",
    "start": "4390550",
    "end": "4396310"
  },
  {
    "text": "So our experience by\nthe way is after you take the final, the nightmares\nrecede after a couple of weeks",
    "start": "4396310",
    "end": "4403630"
  },
  {
    "text": "after that. They recur for the next\nyear or so, but it's fine. We don't know of any\nlong-term problems with that.",
    "start": "4403630",
    "end": "4411890"
  },
  {
    "text": "So, good. Have fun and good\nluck and see you in office hours or something. Good. [APPLAUSE]",
    "start": "4411890",
    "end": "4419010"
  },
  {
    "start": "4419010",
    "end": "4423000"
  }
]