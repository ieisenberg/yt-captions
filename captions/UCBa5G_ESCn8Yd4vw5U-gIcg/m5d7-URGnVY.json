[
  {
    "start": "0",
    "end": "317000"
  },
  {
    "start": "0",
    "end": "2260"
  },
  {
    "text": "OK, so we've learnt about\nsupport vector machines",
    "start": "2260",
    "end": "4900"
  },
  {
    "text": "in their full generality.",
    "start": "4900",
    "end": "6620"
  },
  {
    "text": "Let's see how it works--",
    "start": "6620",
    "end": "8080"
  },
  {
    "text": "how they work on\na simple example,",
    "start": "8080",
    "end": "11139"
  },
  {
    "text": "which is the heart\ndata, where we've",
    "start": "11140",
    "end": "13240"
  },
  {
    "text": "got a bunch of variables, not\ntoo many, 10 or so variables.",
    "start": "13240",
    "end": "17890"
  },
  {
    "text": "And there's two classes, heart\ndisease or no heart disease.",
    "start": "17890",
    "end": "23689"
  },
  {
    "text": "And we use the support\nvector machines to classify.",
    "start": "23690",
    "end": "27550"
  },
  {
    "text": "So in these two panels,\nwe show the performance",
    "start": "27550",
    "end": "32020"
  },
  {
    "text": "on the training data.",
    "start": "32020",
    "end": "33970"
  },
  {
    "text": "And next slide, we'll\nshow on the test data.",
    "start": "33970",
    "end": "36940"
  },
  {
    "text": "So what we are showing\nare ROC curves.",
    "start": "36940",
    "end": "40219"
  },
  {
    "text": "So we've seen these before.",
    "start": "40220",
    "end": "41830"
  },
  {
    "text": "But just to remind you, once\nwe fit our function, which",
    "start": "41830",
    "end": "46960"
  },
  {
    "text": "is initially going to\nbe a linear function,",
    "start": "46960",
    "end": "50469"
  },
  {
    "text": "we thresholded zero.",
    "start": "50470",
    "end": "52000"
  },
  {
    "text": "And that's a decision boundary.",
    "start": "52000",
    "end": "53350"
  },
  {
    "text": "If the function\nis bigger than 0,",
    "start": "53350",
    "end": "55629"
  },
  {
    "text": "we'll classify it to one\nclass, otherwise less than 0",
    "start": "55630",
    "end": "58270"
  },
  {
    "text": "to the other class.",
    "start": "58270",
    "end": "60280"
  },
  {
    "text": "And you'll make a\nnumber of errors.",
    "start": "60280",
    "end": "62489"
  },
  {
    "text": "You'll make some false\npositives and false negatives.",
    "start": "62490",
    "end": "66670"
  },
  {
    "text": "Here, we actually look\nat the false positives",
    "start": "66670",
    "end": "68810"
  },
  {
    "text": "and the true positives.",
    "start": "68810",
    "end": "70579"
  },
  {
    "text": "Well, as you change\nthe threshold,",
    "start": "70580",
    "end": "73130"
  },
  {
    "text": "you're going to change the false\npositives and true positives.",
    "start": "73130",
    "end": "76759"
  },
  {
    "text": "And what the ROC curve\ndoes is trace out",
    "start": "76760",
    "end": "81860"
  },
  {
    "text": "the true positives versus\nthe false positives,",
    "start": "81860",
    "end": "84290"
  },
  {
    "text": "as you change the threshold.",
    "start": "84290",
    "end": "86250"
  },
  {
    "text": "And that gives you a curve.",
    "start": "86250",
    "end": "88218"
  },
  {
    "text": "And if you think about\nit, what you'd really like",
    "start": "88218",
    "end": "90260"
  },
  {
    "text": "is a curve that really\nhugs this Northwest corner.",
    "start": "90260",
    "end": "94080"
  },
  {
    "text": "This one seems to be\ndoing pretty well, OK.",
    "start": "94080",
    "end": "96935"
  },
  {
    "text": "And a way of\ncomparing classifiers",
    "start": "96935",
    "end": "98930"
  },
  {
    "text": "is to see which one gets\ncloser to the corner, OK.",
    "start": "98930",
    "end": "103840"
  },
  {
    "text": "So that's called the ROC curve.",
    "start": "103840",
    "end": "105920"
  },
  {
    "text": "The name comes from\nan engineering term,",
    "start": "105920",
    "end": "109060"
  },
  {
    "text": "but it's not really relevant in\nterms of how we use it today.",
    "start": "109060",
    "end": "112992"
  },
  {
    "text": "Remember, the so-called-- the\nAUC, the area under the curve",
    "start": "112992",
    "end": "115450"
  },
  {
    "text": "is the usual measure of\nhow close the curve gets",
    "start": "115450",
    "end": "118780"
  },
  {
    "text": "to the Northwest corner.",
    "start": "118780",
    "end": "120290"
  },
  {
    "text": "Oh, good point there, Rob.",
    "start": "120290",
    "end": "121840"
  },
  {
    "text": "So AUC, area under the\ncurve is a summary measure",
    "start": "121840",
    "end": "129418"
  },
  {
    "text": "of how close you get\nto the corner here.",
    "start": "129419",
    "end": "132920"
  },
  {
    "text": "If the area under the curve is\n1, notice this is a unit cube,",
    "start": "132920",
    "end": "136599"
  },
  {
    "text": "it means this curve exactly\ngets into the corner.",
    "start": "136600",
    "end": "139890"
  },
  {
    "text": "And there's a 45 degree line.",
    "start": "139890",
    "end": "142770"
  },
  {
    "text": "You should always be\nabove the 45 degree line.",
    "start": "142770",
    "end": "145690"
  },
  {
    "text": "So the AUC will be\nbetween 0.5 and 1.",
    "start": "145690",
    "end": "149490"
  },
  {
    "text": "So in this left panel, we've\ngot the linear support vector",
    "start": "149490",
    "end": "152880"
  },
  {
    "text": "classifier, that's\nthe red curve.",
    "start": "152880",
    "end": "154870"
  },
  {
    "text": "And we're comparing it to linear\ndiscriminant analysis here,",
    "start": "154870",
    "end": "157799"
  },
  {
    "text": "which is the blue curve.",
    "start": "157800",
    "end": "159070"
  },
  {
    "text": "And on the training data,\nthey do pretty much the same.",
    "start": "159070",
    "end": "161680"
  },
  {
    "text": "Maybe the support vector has got\na slight edge in a few places,",
    "start": "161680",
    "end": "166109"
  },
  {
    "text": "OK.",
    "start": "166110",
    "end": "168180"
  },
  {
    "text": "The left panel-- in the\nright panel-- sorry,",
    "start": "168180",
    "end": "170670"
  },
  {
    "text": "in the right panel, we compare\nthe linear support vector",
    "start": "170670",
    "end": "174900"
  },
  {
    "text": "classifier, which is the\nred curve to the SVM using",
    "start": "174900",
    "end": "178170"
  },
  {
    "text": "a radial kernel with different\nvalues of gamma, all right.",
    "start": "178170",
    "end": "182970"
  },
  {
    "text": "And you'll notice that\nit's not monotone.",
    "start": "182970",
    "end": "188890"
  },
  {
    "text": "So when gamma is 10 to the minus\n1, we seem to do really well.",
    "start": "188890",
    "end": "195550"
  },
  {
    "text": "When gamma is 10 to the minus\n2, that's the green curve,",
    "start": "195550",
    "end": "201110"
  },
  {
    "text": "we do worse than the red curve.",
    "start": "201110",
    "end": "202960"
  },
  {
    "text": "And 10 to the minus\n3, even worse again.",
    "start": "202960",
    "end": "206970"
  },
  {
    "text": "So remember the larger\ngamma, the more restrictive--",
    "start": "206970",
    "end": "212340"
  },
  {
    "text": "sorry, the more wiggly\nthe decision boundary.",
    "start": "212340",
    "end": "214950"
  },
  {
    "text": "And so this is\nordered in complexity.",
    "start": "214950",
    "end": "218379"
  },
  {
    "text": "So when gamma is large,\n10 to the minus 1,",
    "start": "218380",
    "end": "222000"
  },
  {
    "text": "we're doing the best.",
    "start": "222000",
    "end": "223240"
  },
  {
    "text": "And then as we decrease gamma,\nwe start doing worse and worse.",
    "start": "223240",
    "end": "227710"
  },
  {
    "text": "And the linear classifier\ncomes in between,",
    "start": "227710",
    "end": "230292"
  },
  {
    "text": "in the middle of that regime.",
    "start": "230292",
    "end": "231500"
  },
  {
    "start": "231500",
    "end": "234010"
  },
  {
    "text": "Which means gamma is another\ntuning parameter for the support",
    "start": "234010",
    "end": "237129"
  },
  {
    "text": "vector classifier.",
    "start": "237130",
    "end": "238062"
  },
  {
    "text": "And just keep in mind,\nwhat this is telling us,",
    "start": "238062",
    "end": "240019"
  },
  {
    "text": "it's not really a fair\ncomparison on the right panel",
    "start": "240020",
    "end": "242187"
  },
  {
    "text": "because the gamma is\nsmaller, has more complexity.",
    "start": "242187",
    "end": "246050"
  },
  {
    "text": "So it's going to fit more.",
    "start": "246050",
    "end": "247390"
  },
  {
    "text": "So this is bigger?",
    "start": "247390",
    "end": "248870"
  },
  {
    "text": "Yeah.",
    "start": "248870",
    "end": "249370"
  },
  {
    "text": "Yeah.",
    "start": "249370",
    "end": "249870"
  },
  {
    "text": "So a training error comparison\nisn't a fair comparison.",
    "start": "249870",
    "end": "254245"
  },
  {
    "text": "So we didn't do it here.",
    "start": "254245",
    "end": "255830"
  },
  {
    "text": "But if we made gamma\neven much bigger,",
    "start": "255830",
    "end": "259148"
  },
  {
    "text": "we'd probably get an area\nunder the curve of 1.",
    "start": "259149",
    "end": "262599"
  },
  {
    "text": "So of course, we know we\nshould look at test data.",
    "start": "262600",
    "end": "265460"
  },
  {
    "text": "And so we put aside\n80 observations",
    "start": "265460",
    "end": "268810"
  },
  {
    "text": "as test observations\nand fit the classifiers",
    "start": "268810",
    "end": "272620"
  },
  {
    "text": "on the training data.",
    "start": "272620",
    "end": "273590"
  },
  {
    "text": "And now compared to the--\nnow, we look at ROC curves",
    "start": "273590",
    "end": "276220"
  },
  {
    "text": "on the test data.",
    "start": "276220",
    "end": "278320"
  },
  {
    "text": "And again, linear versus LDA,\nit looks like the support vector",
    "start": "278320",
    "end": "283840"
  },
  {
    "text": "classifier does a little bit\nbetter just by a small amount,",
    "start": "283840",
    "end": "287440"
  },
  {
    "text": "OK.",
    "start": "287440",
    "end": "288340"
  },
  {
    "text": "But notice that this\nROC curve is not",
    "start": "288340",
    "end": "291790"
  },
  {
    "text": "as good as the previous\none on the training data",
    "start": "291790",
    "end": "295420"
  },
  {
    "text": "because that was\noverfitting a bit.",
    "start": "295420",
    "end": "298900"
  },
  {
    "text": "And in the right hand panel, we\nlook at the different support",
    "start": "298900",
    "end": "303729"
  },
  {
    "text": "vector machines again.",
    "start": "303730",
    "end": "305140"
  },
  {
    "text": "And now, the one with the\nbiggest value of gamma",
    "start": "305140",
    "end": "310420"
  },
  {
    "text": "actually does the worst.",
    "start": "310420",
    "end": "312090"
  },
  {
    "text": "So while it was doing the\nbest on the training data,",
    "start": "312090",
    "end": "315790"
  },
  {
    "text": "it actually does the\nworst on the test data.",
    "start": "315790",
    "end": "318640"
  },
  {
    "start": "317000",
    "end": "364000"
  },
  {
    "text": "And here, the linear\nsupport vector machine",
    "start": "318640",
    "end": "320800"
  },
  {
    "text": "does pretty much the best.",
    "start": "320800",
    "end": "322360"
  },
  {
    "text": "And the most regularized SVM\nis with gamma 10 to the minus--",
    "start": "322360",
    "end": "331180"
  },
  {
    "text": "see that number, but 3 is\ndoing about the same, OK.",
    "start": "331180",
    "end": "337729"
  },
  {
    "text": "So these are tuning parameters.",
    "start": "337730",
    "end": "339500"
  },
  {
    "text": "And we'd have to use\nall our usual tools",
    "start": "339500",
    "end": "343070"
  },
  {
    "text": "for deciding if we had to\npick a value for gamma,",
    "start": "343070",
    "end": "347420"
  },
  {
    "text": "we may use cross-validation or\nvalidation data set to pick it,",
    "start": "347420",
    "end": "352340"
  },
  {
    "text": "as well as the cost parameter\nC. So with a kernel,",
    "start": "352340",
    "end": "358430"
  },
  {
    "text": "we've got two tuning parameters.",
    "start": "358430",
    "end": "359870"
  },
  {
    "text": "With a linear support\nvector classifier,",
    "start": "359870",
    "end": "361639"
  },
  {
    "text": "we've just got one, which is C.",
    "start": "361640",
    "end": "366777"
  },
  {
    "start": "364000",
    "end": "494000"
  },
  {
    "text": "OK, so everything we've talked\nabout so far is for two classes,",
    "start": "366777",
    "end": "370760"
  },
  {
    "text": "separating hyperplane\ntwo classes.",
    "start": "370760",
    "end": "373140"
  },
  {
    "text": "So what do we do if we\nhave more than two classes?",
    "start": "373140",
    "end": "375910"
  },
  {
    "text": "Well, unfortunately, things\nget a little bit ad-hoc here.",
    "start": "375910",
    "end": "379560"
  },
  {
    "text": "And so there's two\ngeneral approaches.",
    "start": "379560",
    "end": "381950"
  },
  {
    "text": "And the one's called\nOVA, one versus all.",
    "start": "381950",
    "end": "387260"
  },
  {
    "text": "It's an acronym.",
    "start": "387260",
    "end": "388640"
  },
  {
    "text": "And so the idea is you\nfit k different support,",
    "start": "388640",
    "end": "391970"
  },
  {
    "text": "two class support vector\nmachine classifiers.",
    "start": "391970",
    "end": "395330"
  },
  {
    "text": "Each class versus the rest.",
    "start": "395330",
    "end": "397319"
  },
  {
    "text": "So you just relabel all the\nother classes as a mega minus",
    "start": "397320",
    "end": "400970"
  },
  {
    "text": "one class and say the target\nclass is class plus 1.",
    "start": "400970",
    "end": "405620"
  },
  {
    "text": "You fit a support\nvector classifier.",
    "start": "405620",
    "end": "407870"
  },
  {
    "text": "And you do that with each of\nthe classes as being the plus 1",
    "start": "407870",
    "end": "410900"
  },
  {
    "text": "and all the others\nbeing the minus 1.",
    "start": "410900",
    "end": "412710"
  },
  {
    "text": "So that means you'll fit k\ndifferent classifiers, which",
    "start": "412710",
    "end": "415639"
  },
  {
    "text": "means you'll have k\ndifferent functions.",
    "start": "415640",
    "end": "417750"
  },
  {
    "text": "And now, when you come\nto classify a new point,",
    "start": "417750",
    "end": "420140"
  },
  {
    "text": "you evaluate those k\ndifferent functions,",
    "start": "420140",
    "end": "422480"
  },
  {
    "text": "and you'll classify to the\nclass, which is the largest.",
    "start": "422480",
    "end": "426180"
  },
  {
    "text": "So that's the one approach.",
    "start": "426180",
    "end": "428460"
  },
  {
    "text": "And the other approach is\nOVO, which is one versus one.",
    "start": "428460",
    "end": "433050"
  },
  {
    "text": "Which means you do all k choose\ntwo pairwise classifiers.",
    "start": "433050",
    "end": "437699"
  },
  {
    "text": "So if k is, for\nexample, 10, that",
    "start": "437700",
    "end": "444040"
  },
  {
    "text": "means you have to fit k\nchoose two, which is--",
    "start": "444040",
    "end": "447610"
  },
  {
    "text": "45.",
    "start": "447610",
    "end": "448120"
  },
  {
    "text": "45, 45 classifiers.",
    "start": "448120",
    "end": "450900"
  },
  {
    "text": "And so obviously, that gets\nbig as the number of classes",
    "start": "450900",
    "end": "453490"
  },
  {
    "text": "gets big.",
    "start": "453490",
    "end": "454780"
  },
  {
    "text": "And so now, you've got all\nthese pairwise classifiers.",
    "start": "454780",
    "end": "458600"
  },
  {
    "text": "And to classify a new\npoint, what you do",
    "start": "458600",
    "end": "461170"
  },
  {
    "text": "is you evaluate every single\none of these classifiers,",
    "start": "461170",
    "end": "464380"
  },
  {
    "text": "and you see which class wins\nthe most pairwise competitions.",
    "start": "464380",
    "end": "469570"
  },
  {
    "text": "And that's the class\nto which you classify.",
    "start": "469570",
    "end": "472360"
  },
  {
    "text": "So these seem ad-hoc.",
    "start": "472360",
    "end": "474639"
  },
  {
    "text": "The actual mechanisms aren't\nquite as ad-hoc as they sound.",
    "start": "474640",
    "end": "478880"
  },
  {
    "text": "And so you can explain this from\na somewhat theoretical point",
    "start": "478880",
    "end": "481900"
  },
  {
    "text": "of view.",
    "start": "481900",
    "end": "482410"
  },
  {
    "text": "But that's what gets done in\npractice, one of these two.",
    "start": "482410",
    "end": "485470"
  },
  {
    "text": "The number of classes is\ntoo large, usually OVA.",
    "start": "485470",
    "end": "488410"
  },
  {
    "text": "Otherwise, OVO tends\nto get favored.",
    "start": "488410",
    "end": "490665"
  },
  {
    "start": "490665",
    "end": "494320"
  },
  {
    "start": "494000",
    "end": "720000"
  },
  {
    "text": "OK, we're near the\nend of this section.",
    "start": "494320",
    "end": "497690"
  },
  {
    "text": "We'll end up by comparing\nsupport vector machines",
    "start": "497690",
    "end": "500290"
  },
  {
    "text": "to logistic regression.",
    "start": "500290",
    "end": "501850"
  },
  {
    "text": "So remember, logistic\nregression did",
    "start": "501850",
    "end": "505870"
  },
  {
    "text": "solve classification problems\nby modeling the probabilities",
    "start": "505870",
    "end": "508630"
  },
  {
    "text": "of the classes.",
    "start": "508630",
    "end": "509860"
  },
  {
    "text": "With support vector\nmachines, we're",
    "start": "509860",
    "end": "512079"
  },
  {
    "text": "optimizing going directly\nfor the decision boundary.",
    "start": "512080",
    "end": "515710"
  },
  {
    "text": "They seem very different.",
    "start": "515710",
    "end": "517000"
  },
  {
    "text": "But it turns out they're not as\ndifferent as one might think.",
    "start": "517000",
    "end": "520210"
  },
  {
    "text": "So if we write the linear\nfunction in this form",
    "start": "520210",
    "end": "522849"
  },
  {
    "text": "as we've done\nbefore, it turns out",
    "start": "522850",
    "end": "526269"
  },
  {
    "text": "we can rephrase the optimization\nproblem for the support vector",
    "start": "526270",
    "end": "530860"
  },
  {
    "text": "classifier in the\nfollowing form.",
    "start": "530860",
    "end": "533950"
  },
  {
    "text": "Now, this is somewhat\ntechnical, but there's",
    "start": "533950",
    "end": "536980"
  },
  {
    "text": "a loss function between\ny and f of x summed over",
    "start": "536980",
    "end": "542260"
  },
  {
    "text": "the observations.",
    "start": "542260",
    "end": "543260"
  },
  {
    "text": "So this is similar to but not\nthe same as a log likelihood.",
    "start": "543260",
    "end": "547240"
  },
  {
    "text": "There's a penalty\non the coefficients.",
    "start": "547240",
    "end": "548899"
  },
  {
    "text": "It looks like it's a quadratic\npenalty, like a ridge penalty",
    "start": "548900",
    "end": "551410"
  },
  {
    "text": "here.",
    "start": "551410",
    "end": "551910"
  },
  {
    "text": "And there's a tuning\nparameter lambda.",
    "start": "551910",
    "end": "554480"
  },
  {
    "text": "Now, this loss function is\na somewhat strange beast.",
    "start": "554480",
    "end": "558050"
  },
  {
    "text": "It's known as the hinge loss.",
    "start": "558050",
    "end": "559580"
  },
  {
    "text": "And maybe it's better just\nto show you a picture.",
    "start": "559580",
    "end": "562450"
  },
  {
    "text": "So its primary argument\nis y times f of x.",
    "start": "562450",
    "end": "566180"
  },
  {
    "text": "Remember, this is what\nwe call now the margin.",
    "start": "566180",
    "end": "568850"
  },
  {
    "text": "This is the quantity\nwe'd like to be positive.",
    "start": "568850",
    "end": "571389"
  },
  {
    "text": "And the bigger it is,\nthe further a point",
    "start": "571390",
    "end": "574540"
  },
  {
    "text": "is away from its margin, OK.",
    "start": "574540",
    "end": "577313"
  },
  {
    "text": "So this is the loss that\nwe assign to that quantity.",
    "start": "577314",
    "end": "582950"
  },
  {
    "text": "In this formulation,\nif it's bigger than 1,",
    "start": "582950",
    "end": "585080"
  },
  {
    "text": "we sign zero loss.",
    "start": "585080",
    "end": "586230"
  },
  {
    "text": "So that's zero over here.",
    "start": "586230",
    "end": "587570"
  },
  {
    "text": "But if it's less than\n1 and particularly,",
    "start": "587570",
    "end": "590030"
  },
  {
    "text": "if it goes negative, it\nmeans in all these regimes,",
    "start": "590030",
    "end": "592940"
  },
  {
    "text": "we're on the wrong\nside of our margin.",
    "start": "592940",
    "end": "595640"
  },
  {
    "text": "We pay an increase\nin linear loss.",
    "start": "595640",
    "end": "599100"
  },
  {
    "text": "So that's called the hinge loss.",
    "start": "599100",
    "end": "601500"
  },
  {
    "text": "And if you optimize this\nproblem, this criterion",
    "start": "601500",
    "end": "604410"
  },
  {
    "text": "with respect to the\nbetas, the solution",
    "start": "604410",
    "end": "607019"
  },
  {
    "text": "is equivalent to the\nsupport vector machine.",
    "start": "607020",
    "end": "609150"
  },
  {
    "text": "So that's not obvious.",
    "start": "609150",
    "end": "610210"
  },
  {
    "text": "It's pretty hard.",
    "start": "610210",
    "end": "612240"
  },
  {
    "text": "The derivation is not so easy.",
    "start": "612240",
    "end": "613630"
  },
  {
    "text": "Really, Rob?",
    "start": "613630",
    "end": "614400"
  },
  {
    "text": "You can't see just from looking.",
    "start": "614400",
    "end": "616390"
  },
  {
    "text": "No, it's very hard.",
    "start": "616390",
    "end": "618390"
  },
  {
    "text": "Well, it's not very\nhard, but it's quite hard",
    "start": "618390",
    "end": "620730"
  },
  {
    "text": "to map this to the other one.",
    "start": "620730",
    "end": "622120"
  },
  {
    "text": "But what's more important,\nand it's not necessarily",
    "start": "622120",
    "end": "624420"
  },
  {
    "text": "that this is the\nway one does it,",
    "start": "624420",
    "end": "625889"
  },
  {
    "text": "it's just illustrative\nbecause it",
    "start": "625890",
    "end": "629460"
  },
  {
    "text": "allows us to compare\nto logistic regression.",
    "start": "629460",
    "end": "632760"
  },
  {
    "text": "Of course, with\nlogistic regression,",
    "start": "632760",
    "end": "635070"
  },
  {
    "text": "we also fit in a\nlinear function.",
    "start": "635070",
    "end": "637020"
  },
  {
    "text": "So the function looks the same.",
    "start": "637020",
    "end": "639120"
  },
  {
    "text": "Remember, there,\nthe linear function",
    "start": "639120",
    "end": "640770"
  },
  {
    "text": "was the logit of the probability\nof class 1 versus class",
    "start": "640770",
    "end": "644160"
  },
  {
    "text": "minus 1 in this case.",
    "start": "644160",
    "end": "646079"
  },
  {
    "text": "And for logistic regression,\nwe had a log likelihood plus--",
    "start": "646080",
    "end": "650130"
  },
  {
    "text": "we could have a log likelihood\nplus a rich penalty.",
    "start": "650130",
    "end": "653340"
  },
  {
    "text": "And the log likelihood is given\nby this green curve over here.",
    "start": "653340",
    "end": "658460"
  },
  {
    "text": "And you'll notice, it\nlooks very similar.",
    "start": "658460",
    "end": "661280"
  },
  {
    "text": "In this formulation, it tapers\noff to be horizontal over here.",
    "start": "661280",
    "end": "665960"
  },
  {
    "text": "It asymptotes to be\nlinear over here.",
    "start": "665960",
    "end": "668310"
  },
  {
    "text": "But instead of having the\nsharp corner over here,",
    "start": "668310",
    "end": "671029"
  },
  {
    "text": "it has a gentler corner.",
    "start": "671030",
    "end": "674210"
  },
  {
    "text": "Which means you can also\nthink of logistic regression",
    "start": "674210",
    "end": "676760"
  },
  {
    "text": "as having a soft\nmargin and that it",
    "start": "676760",
    "end": "680540"
  },
  {
    "text": "focuses more on points\nclose to the margin",
    "start": "680540",
    "end": "682579"
  },
  {
    "text": "than points further away.",
    "start": "682580",
    "end": "684170"
  },
  {
    "text": "So there's a lot of similarity\nbetween logistic regression",
    "start": "684170",
    "end": "687889"
  },
  {
    "text": "and support vector machines.",
    "start": "687890",
    "end": "690610"
  },
  {
    "text": "And if you want to\nlearn more about that,",
    "start": "690610",
    "end": "693940"
  },
  {
    "text": "you may want to look\nat our book, Elements",
    "start": "693940",
    "end": "696940"
  },
  {
    "text": "of Statistical Learning, where\nwe go into this in a little bit",
    "start": "696940",
    "end": "699970"
  },
  {
    "text": "more detail.",
    "start": "699970",
    "end": "700709"
  },
  {
    "text": "It's one thing to add the\nfact that the hinge loss has",
    "start": "700710",
    "end": "702960"
  },
  {
    "text": "that corner is what gives it\nthe support vector property.",
    "start": "702960",
    "end": "705795"
  },
  {
    "text": "This is the alphas\nthat are-- some",
    "start": "705795",
    "end": "708279"
  },
  {
    "text": "of which are 0, whereas\nthe smooth loss doesn't",
    "start": "708280",
    "end": "710608"
  },
  {
    "text": "have that property.",
    "start": "710608",
    "end": "711399"
  },
  {
    "text": "You don't get support points.",
    "start": "711400",
    "end": "712640"
  },
  {
    "text": "Good point, Rob.",
    "start": "712640",
    "end": "713590"
  },
  {
    "text": "And you may get points,\nwhose weight gets close to 0,",
    "start": "713590",
    "end": "717280"
  },
  {
    "text": "but not exactly 0.",
    "start": "717280",
    "end": "718620"
  },
  {
    "start": "718620",
    "end": "721870"
  },
  {
    "start": "720000",
    "end": "888000"
  },
  {
    "text": "OK, so which to use,\nsupport vector machine",
    "start": "721870",
    "end": "724480"
  },
  {
    "text": "or logistic regression?",
    "start": "724480",
    "end": "726190"
  },
  {
    "text": "We'll just sum up here.",
    "start": "726190",
    "end": "729040"
  },
  {
    "text": "So if the classes\nare nearly separable,",
    "start": "729040",
    "end": "731680"
  },
  {
    "text": "the support vector\nmachine tends to do better",
    "start": "731680",
    "end": "734050"
  },
  {
    "text": "than logistic\nregression and so does",
    "start": "734050",
    "end": "736330"
  },
  {
    "text": "linear discriminant analysis.",
    "start": "736330",
    "end": "737960"
  },
  {
    "text": "Logistic regression\nactually breaks down,",
    "start": "737960",
    "end": "740410"
  },
  {
    "text": "if the classes are\nexactly separable.",
    "start": "740410",
    "end": "743170"
  },
  {
    "text": "In that case, you'd have to\nuse some kind of regularization",
    "start": "743170",
    "end": "746290"
  },
  {
    "text": "with logistic regression.",
    "start": "746290",
    "end": "747920"
  },
  {
    "text": "But support vector-- that's\na regime where support vector",
    "start": "747920",
    "end": "750310"
  },
  {
    "text": "machines do well.",
    "start": "750310",
    "end": "752029"
  },
  {
    "text": "When the classes\naren't separable,",
    "start": "752030",
    "end": "753620"
  },
  {
    "text": "where there's somewhat--\nthere's quite a bit of overlap,",
    "start": "753620",
    "end": "756650"
  },
  {
    "text": "the logistic regression,\nperhaps with a ridge penalty",
    "start": "756650",
    "end": "759950"
  },
  {
    "text": "or with a Lasso penalty, tends\nto do better and is more useful.",
    "start": "759950",
    "end": "765930"
  },
  {
    "text": "They're similar.",
    "start": "765930",
    "end": "766668"
  },
  {
    "text": "The results will be similar,\nbut it's more useful",
    "start": "766668",
    "end": "768710"
  },
  {
    "text": "because it's actually estimating\nprobabilities for you.",
    "start": "768710",
    "end": "773290"
  },
  {
    "text": "And for non-linear\nboundaries, you",
    "start": "773290",
    "end": "775420"
  },
  {
    "text": "can use kernel support\nvector machines.",
    "start": "775420",
    "end": "777550"
  },
  {
    "text": "And they're popular.",
    "start": "777550",
    "end": "778730"
  },
  {
    "text": "You can use the same kernels\nwith logistic regression and LDA",
    "start": "778730",
    "end": "782139"
  },
  {
    "text": "as well, but the computations\ntend to be more expensive.",
    "start": "782140",
    "end": "785260"
  },
  {
    "text": "And so in those scenarios,\nsupport vector machines",
    "start": "785260",
    "end": "787480"
  },
  {
    "text": "tend to be used.",
    "start": "787480",
    "end": "789510"
  },
  {
    "text": "So here we are at the\nend of the session.",
    "start": "789510",
    "end": "791617"
  },
  {
    "text": "Rob, do you have\nanything to add here?",
    "start": "791617",
    "end": "793200"
  },
  {
    "text": "Well, I was going to add about--",
    "start": "793200",
    "end": "794533"
  },
  {
    "text": "there's sort of a no\nfree lunch principle.",
    "start": "794533",
    "end": "796470"
  },
  {
    "text": "With support vector machines,\nwe saw a way with kernels",
    "start": "796470",
    "end": "798761"
  },
  {
    "text": "to finesse the--",
    "start": "798762",
    "end": "800400"
  },
  {
    "text": "get a solution essentially\nin high dimensions for free.",
    "start": "800400",
    "end": "802960"
  },
  {
    "text": "But the one price you pay is you\ndon't get a feature selection",
    "start": "802960",
    "end": "806907"
  },
  {
    "text": "that you get for the\nLasso for L1 penalties,",
    "start": "806907",
    "end": "808740"
  },
  {
    "text": "where we explicitly put a\npenalty on the features.",
    "start": "808740",
    "end": "811920"
  },
  {
    "text": "And as a result, a lot of\nthe features are set to 0.",
    "start": "811920",
    "end": "814740"
  },
  {
    "text": "With support vector\nmachines, one disadvantage",
    "start": "814740",
    "end": "816990"
  },
  {
    "text": "is it uses all the features.",
    "start": "816990",
    "end": "818880"
  },
  {
    "text": "And it doesn't easily select\nwhich features are important.",
    "start": "818880",
    "end": "822190"
  },
  {
    "text": "So for high-dimensional\nproblems,",
    "start": "822190",
    "end": "824190"
  },
  {
    "text": "that can be a drawback, not\nso much for the classification",
    "start": "824190",
    "end": "827160"
  },
  {
    "text": "performance, but for the\ninterpretation of the solution.",
    "start": "827160",
    "end": "829998"
  },
  {
    "text": "The other point that\nTrevor mentioned",
    "start": "829998",
    "end": "831540"
  },
  {
    "text": "about probability, that's\na really important point.",
    "start": "831540",
    "end": "833730"
  },
  {
    "text": "If you're working in a cancer\ndiagnosis problem, you want--",
    "start": "833730",
    "end": "837540"
  },
  {
    "text": "very often, you don't\njust want to classify,",
    "start": "837540",
    "end": "841337"
  },
  {
    "text": "but you want to know the\nprobabilities of the estimated",
    "start": "841338",
    "end": "843630"
  },
  {
    "text": "class probabilities.",
    "start": "843630",
    "end": "845040"
  },
  {
    "text": "Because if something's got a\nprobability of being cancer",
    "start": "845040",
    "end": "848320"
  },
  {
    "text": "of 0.51 as opposed to 0.99,\nthe classification is the same,",
    "start": "848320",
    "end": "853160"
  },
  {
    "text": "but the implications\nin the actual situation",
    "start": "853160",
    "end": "855790"
  },
  {
    "text": "are very different.",
    "start": "855790",
    "end": "856730"
  },
  {
    "text": "So class probabilities\nare very important.",
    "start": "856730",
    "end": "858790"
  },
  {
    "text": "And support vector\nmachines don't provide",
    "start": "858790",
    "end": "860949"
  },
  {
    "text": "an easy way of getting those.",
    "start": "860950",
    "end": "862930"
  },
  {
    "text": "The community in\nsupport vector machines",
    "start": "862930",
    "end": "864940"
  },
  {
    "text": "have tried to address\nthese problems with post",
    "start": "864940",
    "end": "868270"
  },
  {
    "text": "ad-hoc add-ons, something called\nrecursive feature elimination.",
    "start": "868270",
    "end": "872690"
  },
  {
    "text": "And also, they have ways of\nactually getting probabilities",
    "start": "872690",
    "end": "875740"
  },
  {
    "text": "by fitting logistic regressions\nafter fitting support vector",
    "start": "875740",
    "end": "878529"
  },
  {
    "text": "machines.",
    "start": "878530",
    "end": "879200"
  },
  {
    "text": "But we can do that\ndirectly with, for example,",
    "start": "879200",
    "end": "883270"
  },
  {
    "text": "Lasso-regularized\nlogistic regression.",
    "start": "883270",
    "end": "887310"
  },
  {
    "start": "887310",
    "end": "888000"
  }
]