[
  {
    "start": "0",
    "end": "5990"
  },
  {
    "text": "Good afternoon, CS 109. How are you guys doing today? [CHEERS] OK, fantastic.",
    "start": "5990",
    "end": "11599"
  },
  {
    "text": "I'm glad to hear it. We have ourselves\nan exciting class. We're going to be talking\nabout deep learning.",
    "start": "11600",
    "end": "17750"
  },
  {
    "text": "There is no word of the day and\nthat kind of reflects the point that we're at a very\ninteresting part of the class",
    "start": "17750",
    "end": "24108"
  },
  {
    "text": "where we have covered a\nlot of the core material. We're going to be talking about\nfairness and ethics on Friday,",
    "start": "24108",
    "end": "29335"
  },
  {
    "text": "and that is\nsomething that you'll need for your problem sets. But the positioning that I\nencourage for you for today",
    "start": "29335",
    "end": "34970"
  },
  {
    "text": "is a little bit more of\na positioning of wonder than a positioning of I must\nmemorize every single detail here because this is a story\nthat's unfolding in real time.",
    "start": "34970",
    "end": "43219"
  },
  {
    "text": "And it's also one that I think\ntaking a step back and getting the big picture is what's\nreally critical here.",
    "start": "43220",
    "end": "50120"
  },
  {
    "text": "Now having said that, we\nhave an exciting class where we are going to go\ninto the details of what",
    "start": "50120",
    "end": "55820"
  },
  {
    "text": "is the mystery\nbehind deep learning. We've heard so much about it. And today in class, you get\nto learn how that works.",
    "start": "55820",
    "end": "61370"
  },
  {
    "text": " Now deep learning is a tool,\nbut the effects of the tool,",
    "start": "61370",
    "end": "69634"
  },
  {
    "text": "you've probably heard\nabout, and you've started to see in your life\nand in news around you.",
    "start": "69635",
    "end": "75520"
  },
  {
    "text": "So a bunch of results recently\nhave been pretty impressive. As you might have\nheard, computers",
    "start": "75520",
    "end": "82333"
  },
  {
    "text": "were getting better and\nbetter at board games, but there was this one board\ngame that computers could never play because it was thought\nto be so complicated,",
    "start": "82333",
    "end": "88452"
  },
  {
    "text": "you needed human creativity\nin order to play it, and it's this game of Go. And Go is complicated because\nit has a board that's 17 by 17.",
    "start": "88452",
    "end": "95890"
  },
  {
    "text": "Every square can have a white\ndot, a black dot, or no dots. And this just becomes\nso exponentially large",
    "start": "95890",
    "end": "102400"
  },
  {
    "text": "that traditional compute power\nisn't so helpful for being able to solve it. But then a team called--",
    "start": "102400",
    "end": "111270"
  },
  {
    "text": "or a team in the\nUnited Kingdom came up with an algorithm that\nwas able to beat the world",
    "start": "111270",
    "end": "117300"
  },
  {
    "text": "champion at this game. And this was perhaps the\nlast standing board game. Computers had got really\ngood at making art.",
    "start": "117300",
    "end": "124500"
  },
  {
    "text": "It started out a\nlittle bit modestly, computers doing these\nthings called deep dreaming.",
    "start": "124500",
    "end": "129610"
  },
  {
    "text": "It then transformed\ninto computers could recreate the styles\nof specific artwork, like this picture might\nlook like it's by Rembrandt,",
    "start": "129610",
    "end": "136573"
  },
  {
    "text": "but it's not actually. It's been created by a computer. And if you guys have been\npaying attention recently,",
    "start": "136573",
    "end": "142200"
  },
  {
    "text": "you realize that you can now\nopen a thing called Stable Diffusion and you can\nsay, write a text,",
    "start": "142200",
    "end": "148530"
  },
  {
    "text": "and the computer will generate\nartwork based off the text that you've used. Very controversial, but\ncertainly also very powerful.",
    "start": "148530",
    "end": "157150"
  },
  {
    "text": "And there are other\nnarratives that are perhaps more inspiring, like,\nfor example, algorithms",
    "start": "157150",
    "end": "162659"
  },
  {
    "text": "that can look at this photo of\nsomeone's skin, and at the end, decide that this is, in fact,\nsomething that looks cancerous.",
    "start": "162660",
    "end": "169630"
  },
  {
    "text": "So if you're far\nfrom a hospital, if you took a\nphoto of this skin, you put it through\nthe algorithm. It says, this is cancerous.",
    "start": "169630",
    "end": "175075"
  },
  {
    "text": "Maybe I need to go to\nhospital right now. And that seems like\nquite an inspiring use.",
    "start": "175075",
    "end": "180490"
  },
  {
    "text": "And the crazy thing about\nall of these examples is that they're using the\nexact same technology, which",
    "start": "180490",
    "end": "185620"
  },
  {
    "text": "is what we're going\nto be learning today. And just as a little\nbit of a warm up,",
    "start": "185620",
    "end": "191019"
  },
  {
    "text": "I wanted to start today by\ntraining a little critter. Everybody, this is going to be\nour little critter for class.",
    "start": "191020",
    "end": "197890"
  },
  {
    "text": "The little critter is trying\nto learn how to eat red dots and not to eat green dots.",
    "start": "197890",
    "end": "202900"
  },
  {
    "text": "And this little critter is going\nto have a small little deep learning algorithm. At first, our little critter is\ngoing to act pretty randomly.",
    "start": "202900",
    "end": "209980"
  },
  {
    "text": "But I'm just going to let this\nguy train throughout class. And by the end of\nclass, we'll be able to talk a little\nbit more about how",
    "start": "209980",
    "end": "215487"
  },
  {
    "text": "this little critter is working\nand gaining intelligence. So this is going to be exciting.",
    "start": "215487",
    "end": "221515"
  },
  {
    "text": "Certainly, this is a\njourney we should go on. In order to go on\nthis journey, let's make sure we bring\neveryone along, which means starting with\na little bit of review.",
    "start": "221515",
    "end": "228910"
  },
  {
    "text": "In CS 109, we've been learning\nabout machine learning. Particularly,\nwe've been learning about classification tasks. Classification\ntasks are ones where",
    "start": "228910",
    "end": "235629"
  },
  {
    "text": "you're given a\ntraining data set, and based on that\ntraining data set, you're going to build a machine\nthat can take inputs and learn",
    "start": "235630",
    "end": "242140"
  },
  {
    "text": "to predict outputs. And the outputs are always\nlabels in our problem sets that are either 0 or 1.",
    "start": "242140",
    "end": "247750"
  },
  {
    "text": "And the fact that those\noutputs are discrete is what makes something\na classification task.",
    "start": "247750",
    "end": "253519"
  },
  {
    "text": "The idea is you're going to\nuse that training data set and you're going to\nbuild a little black box. And that little\nblack box is going",
    "start": "253520",
    "end": "259324"
  },
  {
    "text": "to be governed by\nthese special numbers that we call parameters. And the metaphor we've been\nusing in CS 109 for parameters",
    "start": "259325",
    "end": "264340"
  },
  {
    "text": "are sliders. So once you build\nthis little black box, you should be able to put\nin a new set of inputs.",
    "start": "264340",
    "end": "270140"
  },
  {
    "text": "So maybe different regions\nof someone's heart, like you'll give me all of\nthese inputs and the machinery",
    "start": "270140",
    "end": "276580"
  },
  {
    "text": "will then be able to\npredict an output. That output, we call\ny, and sometimes we",
    "start": "276580",
    "end": "282760"
  },
  {
    "text": "think of the probability\nof it as y hat. An important thing\nis if you ever",
    "start": "282760",
    "end": "288990"
  },
  {
    "text": "want to deploy one\nof these models, you want to be able to make\nsure that it works beforehand.",
    "start": "288990",
    "end": "294330"
  },
  {
    "text": "So one thing that's\nworth noting and is relevant to your\nhomework is instead of just thinking about all\nyour data set as one piece,",
    "start": "294330",
    "end": "301320"
  },
  {
    "text": "we often take your\ndata set and split it. And we'll take some set of our\ndata set and call it training.",
    "start": "301320",
    "end": "306480"
  },
  {
    "text": "That's where you'll\nlearn your parameters. And then once you've\nbuilt your black box",
    "start": "306480",
    "end": "311883"
  },
  {
    "text": "based off the parameters learned\nfrom this part of the data set, we reserve some of your\ndata set for making sure",
    "start": "311883",
    "end": "318449"
  },
  {
    "text": "that the algorithm's working. So we'll have a\nlittle test data set. We'll evaluate how well\nthe algorithm works.",
    "start": "318450",
    "end": "323849"
  },
  {
    "text": "And if it seems to\nbe doing well enough and it seems to\nbe acting fairly, then we might\nconsider deploying it.",
    "start": "323850",
    "end": "330450"
  },
  {
    "text": "OK, questions? Feel free to just jump in. I have enough mandarins for a\nwhole bunch of questions today.",
    "start": "330450",
    "end": "339509"
  },
  {
    "text": "Now there's a whole\nbunch of ways you could approach this problem. We learned about naive\nBayes first, but then",
    "start": "339510",
    "end": "344807"
  },
  {
    "text": "on Monday's class, we learned\nabout logistic regression. That's really where we're\ngoing to be picking off today. Logistic regression\nis one version",
    "start": "344807",
    "end": "351580"
  },
  {
    "text": "of that black box algorithm. And if you were to look behind\nthe scenes for the black box algorithm, it would look\nsomething like this.",
    "start": "351580",
    "end": "357560"
  },
  {
    "text": "You take your inputs,\nevery input gets weighted. We sum up those weighted inputs. We then have to squash the sum.",
    "start": "357560",
    "end": "364210"
  },
  {
    "text": "We interpret the\nsquash as a probability that the output\ntakes on the value 1. And then based on\nthat probability,",
    "start": "364210",
    "end": "370240"
  },
  {
    "text": "we make our\nclassification prediction. This is really what it looks-- this is the mental\nmodel you should have,",
    "start": "370240",
    "end": "375819"
  },
  {
    "text": "but mathematically,\nit's doing this. This is the weighted sum. This is the squashing\nfunction, the sigmoid function,",
    "start": "375820",
    "end": "382030"
  },
  {
    "text": "not to be confused with the\nsigmoid from a Gaussian. And then at the end of the day,\nit gives you the probability",
    "start": "382030",
    "end": "388030"
  },
  {
    "text": "that y equals 1. It's not a true\nmodel of the world.",
    "start": "388030",
    "end": "393600"
  },
  {
    "text": "Nothing about the\nworld works like this. It is a model that is\nuseful because it ends up",
    "start": "393600",
    "end": "398850"
  },
  {
    "text": "making good\npredictions in general, but this whole machinery\nis a little bit imagined.",
    "start": "398850",
    "end": "404650"
  },
  {
    "text": "And so even though\nit's wrong and it's not capturing the way the\nworld really works, it ends up with a\nvery useful machine.",
    "start": "404650",
    "end": "411540"
  },
  {
    "text": "OK, and that is our review. Now it's time to take a\njourney from the mathematics",
    "start": "411540",
    "end": "416820"
  },
  {
    "text": "that we have for\nlogistic regression and get all the\nway to how could we use this for skin\ncancer detection.",
    "start": "416820",
    "end": "424260"
  },
  {
    "text": "I really like this\nidea, by the way, of thinking about\nlogistic regression as being a Harry\nPotter Sorting Hat.",
    "start": "424260",
    "end": "430530"
  },
  {
    "text": "You're going to take\nsome input and then you're going to make\nyour prediction. It's like the Harry Potter\nsorting hat looks at the input",
    "start": "430530",
    "end": "437220"
  },
  {
    "text": "and says, ah, this is a\n1, or ah, this is a 0. And the beautiful\nthing about CS 109",
    "start": "437220",
    "end": "443070"
  },
  {
    "text": "is while you might have\nread the Harry Potter books and figured that the\nHarry Potter Sorting Hat was interesting,\nCS 109, you can learn",
    "start": "443070",
    "end": "449100"
  },
  {
    "text": "how it actually could work. So how the Harry Potter\nsorting hat could work is just logistic\nregression perhaps.",
    "start": "449100",
    "end": "455530"
  },
  {
    "text": "But you imagine that might\nnot be powerful enough. ",
    "start": "455530",
    "end": "460870"
  },
  {
    "text": "Just to be clear, inputs. In the case of your problem\nset, sometimes it'll be things like movies\nthat a person liked.",
    "start": "460870",
    "end": "468430"
  },
  {
    "text": "And so each input\nindex would correspond to a particular movie.",
    "start": "468430",
    "end": "473620"
  },
  {
    "text": "When you have a particular user,\nthey'll either like or dislike those movies. Then you weight them, sum\nthem, and it will end up",
    "start": "473620",
    "end": "480430"
  },
  {
    "text": "producing your prediction. The intelligence comes from\nthe weights, good weights, and you make good\npredictions, random weights,",
    "start": "480430",
    "end": "487090"
  },
  {
    "text": "and you don't make\ngood predictions. So that was the theory. And that theory, though,\nled us to this wonderful set",
    "start": "487090",
    "end": "494490"
  },
  {
    "text": "of mathematics. And this wonderful\nset of mathematics says, OK, where are we going\nto get smart weights from?",
    "start": "494490",
    "end": "500099"
  },
  {
    "text": "We need smart weights. Where do they come from? Well, first let's write down\nthe logistic regression model.",
    "start": "500100",
    "end": "507300"
  },
  {
    "text": "Based off that logistic\nregression model, we can say, for\nany one data point based on the current\nvalues of thetas,",
    "start": "507300",
    "end": "514620"
  },
  {
    "text": "how likely is that data point? And this looked really\ncomplicated, but recall,",
    "start": "514620",
    "end": "520210"
  },
  {
    "text": "this was just the log\nlikelihood of a Bernoulli.",
    "start": "520210",
    "end": "525240"
  },
  {
    "text": "And because basically we\ninterpreted the output, this thing, as the P\nparameter of a Bernoulli,",
    "start": "525240",
    "end": "531840"
  },
  {
    "text": "this is just a Bernoulli where,\ninstead of the P parameter, you put in the output of\nyour logistic regression.",
    "start": "531840",
    "end": "538510"
  },
  {
    "text": "This log likelihood,\nit's a scoring function. It says, hey, how good are\nyour current parameters?",
    "start": "538510",
    "end": "543955"
  },
  {
    "text": "Great parameters will\nget a really high value on this score. Awful parameters get a very\nlow value on this score. A scoring function is helpful,\nbut it doesn't tell you",
    "start": "543955",
    "end": "551050"
  },
  {
    "text": "how to get smart parameters. In order to do that,\nyou need a derivative. So derive your scoring\nfunction with respect",
    "start": "551050",
    "end": "556149"
  },
  {
    "text": "to every single parameter. And once you do that,\nyou can do hill climbing.",
    "start": "556150",
    "end": "561410"
  },
  {
    "text": "Now we have a particular\nequation or algorithm",
    "start": "561410",
    "end": "566870"
  },
  {
    "text": "for hill climbing that\nwe use in CS 109, which is gradient ascent. And in gradient\nascent, you're going",
    "start": "566870",
    "end": "573950"
  },
  {
    "text": "to have a system for\nlearning all your parameters. We talked about this on Monday. But you start with\nrandom settings",
    "start": "573950",
    "end": "579110"
  },
  {
    "text": "to parameters, and\nthen over time, you improve each parameter based\noff of calculating its gradient",
    "start": "579110",
    "end": "585200"
  },
  {
    "text": "and then taking\na very small step in this direction of the\ngradient for each of your j",
    "start": "585200",
    "end": "591410"
  },
  {
    "text": "parameters. And the idea of\nthis gradient ascent is if you know the\nderivative of your scoring",
    "start": "591410",
    "end": "598330"
  },
  {
    "text": "function with respect\nto each parameter and if you keep going up\nhill, as you keep training,",
    "start": "598330",
    "end": "603519"
  },
  {
    "text": "you'll get higher and\nhigher and higher scores. And higher and higher\nscores, we think, lead to smarter\nand smarter models.",
    "start": "603520",
    "end": "610420"
  },
  {
    "text": "Before I move on to\nartificial neurons, let me just stop here and\ngive you guys a second to talk about this with\nthe person next to you.",
    "start": "610420",
    "end": "617140"
  },
  {
    "text": "Summarize what you've learned\nand see if you can come up with a good question. Take a minute and\nthen we're going to jump into deep learning\nbecause this is our launching",
    "start": "617140",
    "end": "625060"
  },
  {
    "text": "spot. OK, go for it. Have a good little conversation\nwith the person next to you. [SIDE CONVERSATIONS]",
    "start": "625060",
    "end": "630904"
  },
  {
    "start": "630905",
    "end": "723190"
  },
  {
    "text": "OK. As I said, high-level\nperspective.",
    "start": "723190",
    "end": "729020"
  },
  {
    "text": "But this would be a great time\nfor clarification questions. If you found\nsomething confusing, certainly other people in the\nclass found it confusing, too.",
    "start": "729020",
    "end": "735490"
  },
  {
    "text": "So what's confusing about this? What's interesting? What's curious? Yes?",
    "start": "735490",
    "end": "740500"
  },
  {
    "text": "Can you just elaborate on\nwhy logistics expression [INAUDIBLE]. Different and why you\nwould want to [INAUDIBLE]??",
    "start": "740500",
    "end": "746529"
  },
  {
    "text": "Such a good question. So at this point in class,\nwe have two totally different algorithms for\nmaking predictions",
    "start": "746530",
    "end": "753948"
  },
  {
    "text": "for classification tasks. We have logistic\nregression and naive Bayes. And let me tell you\nsomething very interesting.",
    "start": "753948",
    "end": "760130"
  },
  {
    "text": "They're quite similar. They have a lot in common. They have the same\nlevel of complexity",
    "start": "760130",
    "end": "766600"
  },
  {
    "text": "and they have similar\nnumber of parameters. So for similar\nproblems, they will both perform pretty similarly.",
    "start": "766600",
    "end": "773170"
  },
  {
    "text": "The question was, when would\nyou choose one or the other? I'd say if-- for example, the\nones you have in your problem",
    "start": "773170",
    "end": "779170"
  },
  {
    "text": "sets, it would be acceptable\nto go with either. So that's my first answer, is\nfor lots of similar problems,",
    "start": "779170",
    "end": "786670"
  },
  {
    "text": "either one will be just fine. They're different, but they're\nsimilar in their effectiveness. That's my first answer.",
    "start": "786670",
    "end": "792020"
  },
  {
    "text": "It's kind of lame. The second answer is\nyou could try both. If you had a problem, you\ncould run logistic regression,",
    "start": "792020",
    "end": "798220"
  },
  {
    "text": "you could run Naive Bayes, and\nsee which one is doing better on the testing data\nset and you say, that's the one I\nwant to go with.",
    "start": "798220",
    "end": "804240"
  },
  {
    "text": "And then there is another\nexample, or answer, which is all of our\nexamples in CS 109",
    "start": "804240",
    "end": "810079"
  },
  {
    "text": "have binary inputs\nand binary outputs. And if you were to change that,\nsay, your inputs would become",
    "start": "810080",
    "end": "815720"
  },
  {
    "text": "real valued, or your\noutputs become something like multiclass. It turns out inputs\nbecoming real valued,",
    "start": "815720",
    "end": "822500"
  },
  {
    "text": "logistic regression handles\nthat much more naturally, and outputs being\nmulticlass, it turns out naive Bayes handles that\nmuch more naturally.",
    "start": "822500",
    "end": "829050"
  },
  {
    "text": "So three separate answers,\nbut maybe the most satisfying is just try both of them. Whichever one works\nbest, that's your model.",
    "start": "829050",
    "end": "835868"
  },
  {
    "text": "Just to clarify that, I'm not\nsure if this is what you meant, but I [INAUDIBLE]\ndifferent uses of it.",
    "start": "835868",
    "end": "840920"
  },
  {
    "text": "I'm wondering how the\nactual process of doing it. I know that, for example,\nthe main point of naive Bayes",
    "start": "840920",
    "end": "847190"
  },
  {
    "text": "was that we're going to see\nwhich probability is bigger and we're going to\nchoose that one. Yes.",
    "start": "847190",
    "end": "852500"
  },
  {
    "text": "Using regression, what\nexactly is different about the sigmoid [INAUDIBLE]?",
    "start": "852500",
    "end": "857810"
  },
  {
    "text": "Yeah. The sigmoid doesn't even\nexist in naive Bayes.",
    "start": "857810",
    "end": "863140"
  },
  {
    "text": "Naive Bayes sees no\nsigmoid, because it's not trying to directly model the\nprobability of y given x.",
    "start": "863140",
    "end": "869170"
  },
  {
    "text": "Naive Bayes is trying to use\nBayesian calculations to get that, now of course,\nthat's almost impossible,",
    "start": "869170",
    "end": "875483"
  },
  {
    "text": "so we have to make\nthe naive assumption. So it's like, I'm going to\napproach this just using",
    "start": "875483",
    "end": "880540"
  },
  {
    "text": "pure Bayesian thinking. I get to some point\nwhere it's impossible, I make a big assumption,\nand that big assumption",
    "start": "880540",
    "end": "886660"
  },
  {
    "text": "leads to a\nclassification algorithm. Logistic regression\nhas a different path. It says, I'm not even\ngoing to try and use Bayes.",
    "start": "886660",
    "end": "892060"
  },
  {
    "text": "I'm not going to try and use\ncore ideas of probability. Instead, I'm going to build\na probability machine.",
    "start": "892060",
    "end": "897563"
  },
  {
    "text": "And this probability\nmachine has nothing to do with Bayes, nothing to do\nwith how probabilities actually work. It's just going to be a little\nmachine that you stuff in x's",
    "start": "897563",
    "end": "904540"
  },
  {
    "text": "and you get probabilities out. And it's going to use\nthis sigmoid mechanism. So they're pretty different\npaths, but they lead to--",
    "start": "904540",
    "end": "912010"
  },
  {
    "text": "they both lead to the\nsame functionality of I will make a prediction. Yes? Something I don't\nreally understand",
    "start": "912010",
    "end": "918209"
  },
  {
    "text": "is why we take log with respect\nto thetas, plural, and not just theta? Because when we did the\nalgorithm in the problem",
    "start": "918210",
    "end": "925709"
  },
  {
    "text": "set last week where we try\nto find the optimization, we just took one\nderivative [INAUDIBLE]..",
    "start": "925710",
    "end": "931470"
  },
  {
    "text": "Yeah, good question. So you do mean logs, you\nmeant derivatives I believe. Yes, OK. So the great question\nwas, why do you",
    "start": "931470",
    "end": "937470"
  },
  {
    "text": "have to do the derivative with\nrespect to every single theta? That's actually-- if you\nwant to do optimization",
    "start": "937470",
    "end": "944040"
  },
  {
    "text": "over parameters, you need\nevery partial derivative of all the movable pieces.",
    "start": "944040",
    "end": "949770"
  },
  {
    "text": "Hill climbing needs all of\nthose partial derivatives. It's not enough to just say,\nif I were to change theta 0,",
    "start": "949770",
    "end": "955560"
  },
  {
    "text": "how would my score change? I have to think about\nevery single theta. And if I were to change them,\nhow would my score change?",
    "start": "955560",
    "end": "961529"
  },
  {
    "text": "And that's the\nnecessary component for getting to hill climbing,\nAKA gradient ascent.",
    "start": "961530",
    "end": "966990"
  },
  {
    "text": "So gradient ascent wants to\nknow for every single parameter, if I were to change it,\nhow would the score change?",
    "start": "966990",
    "end": "972630"
  },
  {
    "text": "So we need lots more\npartial derivatives. Earlier in class, we'd only\nhave single parameters.",
    "start": "972630",
    "end": "978760"
  },
  {
    "text": "We did some assignments when\nwe looked at maximum likelihood estimation, where you'd\nhave a model with only one",
    "start": "978760",
    "end": "984730"
  },
  {
    "text": "theta, or only one parameter. In that case, you only need\none partial derivative. Such a good question.",
    "start": "984730",
    "end": "990550"
  },
  {
    "text": "Yes? If the data is convex, is\nthere a functional difference between gradient\ndescent and setting",
    "start": "990550",
    "end": "997300"
  },
  {
    "text": "all the partial\nderivatives equal to 0? Good question. If it's convex, would\nyou get different answers",
    "start": "997300",
    "end": "1002730"
  },
  {
    "text": "if you did gradient\nascent versus if you set the derivative equal to 0? First of all, I'd encourage you\nto try setting this equal to 0",
    "start": "1002730",
    "end": "1010110"
  },
  {
    "text": "and solving for it. It turns out to be very,\nvery, very difficult because of this sigmoid.",
    "start": "1010110",
    "end": "1015450"
  },
  {
    "text": "Having said that, if you were\nable to come up with an answer, they would be the same. Now there is a good reason we\ndo gradient ascent, because when",
    "start": "1015450",
    "end": "1024270"
  },
  {
    "text": "we get to deep learning, taking\na derivative, setting it to 0 as a method of\noptimization will not work.",
    "start": "1024270",
    "end": "1031439"
  },
  {
    "text": "We are going to need\nthis powerful tool of gradient ascent.",
    "start": "1031440",
    "end": "1036750"
  },
  {
    "text": "Oh, such good questions. OK, one more. So for logistic regression,\nlike we mentioned last class,",
    "start": "1036750",
    "end": "1044010"
  },
  {
    "text": "it gets its intelligence from\nmaximum likelihood estimation, which is basically just\noptimizing parameters",
    "start": "1044010",
    "end": "1050040"
  },
  {
    "text": "that we use? Yes. For that, we need to, at\nsome point or another, pick a step size so\nperform gradient ascent.",
    "start": "1050040",
    "end": "1057390"
  },
  {
    "text": "So I was just\nwondering, is there some way of methodically\npicking a good step size, or is there some sort\nof convention for us",
    "start": "1057390",
    "end": "1063780"
  },
  {
    "text": "to use that strikes a good\nbalance between running time and accuracy?",
    "start": "1063780",
    "end": "1069149"
  },
  {
    "text": "Or is it just purely\nan arbitrary choice? Oh my God, I love it. So there's this big\nmystery over the step size.",
    "start": "1069150",
    "end": "1075720"
  },
  {
    "text": "And the step size, we use this\nlittle symbol for step size. And here we can have super\nsmall and really large.",
    "start": "1075720",
    "end": "1083640"
  },
  {
    "text": " If you have really\nlarge step sizes--",
    "start": "1083640",
    "end": "1089670"
  },
  {
    "text": "imagine you're trying to\nget to the top of this hill, and you have huge step sizes.",
    "start": "1089670",
    "end": "1094890"
  },
  {
    "text": "There is this problem\nthat if you're here and your step size is\nhuge, and you go there,",
    "start": "1094890",
    "end": "1100059"
  },
  {
    "text": "and if your next\nstep size is huge, you could end up\nbouncing back and forth over the top of the mountain.",
    "start": "1100060",
    "end": "1106035"
  },
  {
    "text": "You can never reach the\ntop because your step sizes are too big. Imagine somebody\nclimbing a mountain. Instead of taking small\nsteps, they're just leaping,",
    "start": "1106035",
    "end": "1112080"
  },
  {
    "text": "and they just keep\nleaping over the top. So if you have really\nlarge step sizes, you hit this problem that\nyou won't actually converge,",
    "start": "1112080",
    "end": "1121100"
  },
  {
    "text": "just like this marker\nis not converging. So over here, you\nhave doesn't converge.",
    "start": "1121100",
    "end": "1127355"
  },
  {
    "text": " So when you choose\na step size, you",
    "start": "1127355",
    "end": "1132660"
  },
  {
    "text": "might start with a large number,\nand then you might run it, and it might just never end\nup getting to a point where gradients are 0.",
    "start": "1132660",
    "end": "1139710"
  },
  {
    "text": "If you have a super\ntiny step size, you're going to avoid\nthat problem entirely. If you're trying to get to\nthe top of the mountain taking",
    "start": "1139710",
    "end": "1146528"
  },
  {
    "text": "a really small step\nsize, you'll get there. But, as you said-- It's going to be [INAUDIBLE].",
    "start": "1146528",
    "end": "1152030"
  },
  {
    "text": "Yeah. If we had run time, this\nis going to be really long.",
    "start": "1152030",
    "end": "1158090"
  },
  {
    "text": "And as you get to\nsmaller step sizes, you'll be able to\nconverge quicker. We used to talk a lot about\nthe craft of deep learning.",
    "start": "1158090",
    "end": "1166785"
  },
  {
    "text": "And a lot of the\ncraft of deep learning were things like, how do\nyou choose good step sizes? You could do\nsomething like start",
    "start": "1166785",
    "end": "1172760"
  },
  {
    "text": "with a pretty medium sized one. See if you're in this territory.",
    "start": "1172760",
    "end": "1177809"
  },
  {
    "text": "And if you are, then just\nmake it a little bit smaller. Now people have gotten\nmore intelligent.",
    "start": "1177810",
    "end": "1184200"
  },
  {
    "text": "There are things like\nadaptive gradients where the computer is in\ncontrol of the step size,",
    "start": "1184200",
    "end": "1189530"
  },
  {
    "text": "and it's choosing at each\npoint what step size to do. That, you'll learn about\nin further courses.",
    "start": "1189530",
    "end": "1195150"
  },
  {
    "text": "So there's more\nsophisticated techniques. And there's the lay of the land. Good question.",
    "start": "1195150",
    "end": "1200390"
  },
  {
    "text": "And there's another one. That's what I was kind\nof going to ask about, is that there seems to be\nparameters that aren't just",
    "start": "1200390",
    "end": "1207170"
  },
  {
    "text": "within theta, like step size,\nor like the bias like theta 0.",
    "start": "1207170",
    "end": "1212700"
  },
  {
    "text": "Are those things all-- also can you do like a\nlikelihood function on those? OK, so you bring\nup a good point.",
    "start": "1212700",
    "end": "1219427"
  },
  {
    "text": "So there seems to\nbe other parameters, and particularly, the step\nsize is a good example that. Theta 0 actually is a\ntraditional parameter.",
    "start": "1219427",
    "end": "1225750"
  },
  {
    "text": "We think of it as a\nparameter, and we're going to have a\nderivative, and we're going to be doing our likelihood on--",
    "start": "1225750",
    "end": "1232080"
  },
  {
    "text": "it goes into the\nlikelihood function. Whereas step size doesn't show\nup in the likelihood function. And so this sort of parameter\ngets a special name.",
    "start": "1232080",
    "end": "1240130"
  },
  {
    "text": "It's not just a parameter,\nit's a hyperparameter.",
    "start": "1240130",
    "end": "1245250"
  },
  {
    "text": "It's super excitable. And that's just to be like\nthere's some parameters",
    "start": "1245250",
    "end": "1250590"
  },
  {
    "text": "that we're not going\nto try and learn. Instead, we're going\nto set them ourselves, step size being one of them.",
    "start": "1250590",
    "end": "1256570"
  },
  {
    "text": "And as I said, in\nCS 109, we just give you some step sizes\nthey work pretty well. You can play around with them.",
    "start": "1256570",
    "end": "1261590"
  },
  {
    "text": "And in further classes, you\ncan learn really cool ways of making those intelligent--\nor making decisions",
    "start": "1261590",
    "end": "1267179"
  },
  {
    "text": "intelligent decisions\nabout those. OK, yes? Potentially, if your\n[INAUDIBLE] is zero does that mean we're\ndisregarding that data",
    "start": "1267180",
    "end": "1274740"
  },
  {
    "text": "point because we're\nsaying [INAUDIBLE]?? Yes. It's not important [INAUDIBLE]? Yeah, exactly.",
    "start": "1274740",
    "end": "1280390"
  },
  {
    "text": "And your thetas can\nthen become positive, or they can become negative. And if it's positive,\nsaying, this one",
    "start": "1280390",
    "end": "1285720"
  },
  {
    "text": "is going to be shifting\nmy probability positively, or to be more likely,\nand if it's negative, saying, this data\npoint's making me think",
    "start": "1285720",
    "end": "1292020"
  },
  {
    "text": "the probability is smaller. So there you go. You can have 0, you\ncan have positive, you can negative for thetas.",
    "start": "1292020",
    "end": "1298735"
  },
  {
    "text": "And then you can put it\nthrough hill climbing and you get to a\npoint, fantastic.",
    "start": "1298735",
    "end": "1304780"
  },
  {
    "text": "There's such a simple\nidea behind deep learning. It's amazing how close you are.",
    "start": "1304780",
    "end": "1311019"
  },
  {
    "text": "The simple idea about\ndeep learning is, hey, you know that\nlogistic regression that we just spent a lot\nof time talking about?",
    "start": "1311020",
    "end": "1316900"
  },
  {
    "text": "That logistic regression\nis like a cartoon model of how a human neuron works.",
    "start": "1316900",
    "end": "1321950"
  },
  {
    "text": "Now, if you haven't\nstudied neuroscience, that's totally fine. I'm not going to give you a\nreally detailed idea of how",
    "start": "1321950",
    "end": "1327820"
  },
  {
    "text": "a neuron works, but I'm going\nto give you a cartoon model. So in a cartoon\nmodel, your brain is filled with these\nthings called neurons.",
    "start": "1327820",
    "end": "1334690"
  },
  {
    "text": "Did you know that? And neurons have inputs,\noften from other neurons, but they could be from\nthings like the retina, which",
    "start": "1334690",
    "end": "1341440"
  },
  {
    "text": "is getting light input. So it has all these inputs\nthat come into any neuron cell.",
    "start": "1341440",
    "end": "1347429"
  },
  {
    "text": "And for those inputs,\nsome of those inputs will be super important,\nlike if this neuron gets",
    "start": "1347430",
    "end": "1353730"
  },
  {
    "text": "an input from this\nparticular input, it's definitely going to fire. So the neuron gets\nall these inputs.",
    "start": "1353730",
    "end": "1360450"
  },
  {
    "text": "If the charge builds\nup high enough, the neuron then fires,\nand then any other neuron",
    "start": "1360450",
    "end": "1365460"
  },
  {
    "text": "it's connected to will\nthen get its input. So one of the ideas\nsomebody had is hey,",
    "start": "1365460",
    "end": "1370620"
  },
  {
    "text": "this logistic\nregression looks a lot like a cartoon model of\nwhat's going on in our brains.",
    "start": "1370620",
    "end": "1376440"
  },
  {
    "text": "But imagine you tried\nto learn to do a task and you only were\ngiven one human neuron.",
    "start": "1376440",
    "end": "1384180"
  },
  {
    "text": "It seems unlikely that one human\nneuron could become very smart. Instead, if you\nwanted to do a task,",
    "start": "1384180",
    "end": "1390000"
  },
  {
    "text": "you might want to think\nabout having a whole brain. And a brain, you can think of\nit as being a network of neurons",
    "start": "1390000",
    "end": "1396480"
  },
  {
    "text": "where this neuron would then\nconnect to other neurons, and this whole\nnetwork of neurons",
    "start": "1396480",
    "end": "1401490"
  },
  {
    "text": "would then be able to\noutput predictions. So the simple idea\nis, what if we just",
    "start": "1401490",
    "end": "1408519"
  },
  {
    "text": "took a bunch of these\nlogistic regressions. Think about every single\nlogistic regression as one tiny little LEGO piece.",
    "start": "1408520",
    "end": "1415930"
  },
  {
    "text": "And we can start stacking\nthese LEGO pieces on top of each other.",
    "start": "1415930",
    "end": "1421260"
  },
  {
    "text": "And that's the simple idea. So what you learned on\nMonday was that core unit.",
    "start": "1421260",
    "end": "1426519"
  },
  {
    "text": "And once you start putting\nthose units together, you get what we call a neural\nnetwork, AKA deep learning.",
    "start": "1426520",
    "end": "1431570"
  },
  {
    "text": "The term \"deep\"\ncomes from that you have depth in your layers\nof logistic regressions.",
    "start": "1431570",
    "end": "1438350"
  },
  {
    "text": "And that's the core idea\nbehind the revolution of AI. We could just walk\nout and be like, now you know deep learning.",
    "start": "1438350",
    "end": "1444710"
  },
  {
    "text": "But of course,\nthat's unsatisfying. Can I give you guys a\nlittle bit more details?",
    "start": "1444710",
    "end": "1450170"
  },
  {
    "text": "Anyways, this is\nthe idea that leads to AlphaGo, self-driving\ncars, computers making art. And at its core,\ndeep learning is just",
    "start": "1450170",
    "end": "1456470"
  },
  {
    "text": "many logistic regressions\npieced on top of each other. Can I give you guys an\nexample to show you this",
    "start": "1456470",
    "end": "1463650"
  },
  {
    "text": "in a little bit more detail? I'm going to use this running\nexample of computer vision. Computer vision is the hard\ntask of looking at an image",
    "start": "1463650",
    "end": "1472050"
  },
  {
    "text": "and deciding what's in it. So you guys are\nvery smart humans. What's in this image? 0.",
    "start": "1472050",
    "end": "1477660"
  },
  {
    "text": "Yeah, it's a hand-drawn 0. But if you had to predict\na 0 or 1, you would say 0. And how about this image?",
    "start": "1477660",
    "end": "1483650"
  },
  {
    "text": "A 1. Such smart humans we are. We have huge neural\nnetworks that are helping us with this prediction. Actually, it is very\nmisleading, because you",
    "start": "1483650",
    "end": "1490143"
  },
  {
    "text": "have billions of neurons\nhelping you make that decision. So your brain actually starts\nwith a very complicated",
    "start": "1490143",
    "end": "1497220"
  },
  {
    "text": "representation. When you see this, it's hitting\nthe back of your eyeball, a thing called the retina. And the retina is\nseeing amounts of light",
    "start": "1497220",
    "end": "1504540"
  },
  {
    "text": "that hit different parts. Another way of\nthinking about this is-- a very similar analogy\nis what the computer sees.",
    "start": "1504540",
    "end": "1510570"
  },
  {
    "text": "The computer doesn't\nsee a picture of a 0. Instead, it sees a whole\nbunch of pixels, which",
    "start": "1510570",
    "end": "1516210"
  },
  {
    "text": "either could be black or white. And that whole bunch\nof pixels to it just looks like a list\nor a grid of 0s and 1s.",
    "start": "1516210",
    "end": "1523544"
  },
  {
    "text": "Same thing with your brain. When you first see\nthis image it's a whole bunch of light\nresponses on the back of an eye.",
    "start": "1523545",
    "end": "1528780"
  },
  {
    "text": "It doesn't have meaning. Computer vision is\nthe particular type of neural network that\ncould take an image",
    "start": "1528780",
    "end": "1534840"
  },
  {
    "text": "and then make a prediction. And just to be clear, the reason\nthat you find this easy is you",
    "start": "1534840",
    "end": "1540210"
  },
  {
    "text": "have hundreds of\nmillions of neurons. In fact, visual neurons make\nup about 30% of your brain.",
    "start": "1540210",
    "end": "1546235"
  },
  {
    "text": "So when we look at\nthis problem, you're going to think of all\nthese tasks as quite easy. But that's because you have such\nan impressive neural network.",
    "start": "1546235",
    "end": "1552443"
  },
  {
    "text": "And actually, a fun\nthing is the first layer of your neural network\nactually happens in the back of your head, which\nis not that important right",
    "start": "1552443",
    "end": "1558118"
  },
  {
    "text": "now. Imagine you had to do this\ntask and I just gave you a single logistic\nregression unit.",
    "start": "1558118",
    "end": "1566110"
  },
  {
    "text": "It would be really difficult.\nThe x's, the features in this case, will correspond\nto each different pixel",
    "start": "1566110",
    "end": "1572590"
  },
  {
    "text": "in the image. If you took a\nparticular image, that would turn on all the\nx vectors, and then you",
    "start": "1572590",
    "end": "1578338"
  },
  {
    "text": "could have a logistic\nregression that could weight each of those, sum\nit, squash it, and then make a prediction.",
    "start": "1578338",
    "end": "1583900"
  },
  {
    "text": "But this would be a\nlittle bit similar to asking a single neuron\nto learn how to see.",
    "start": "1583900",
    "end": "1589060"
  },
  {
    "text": "You have 30% of\nyour brain to see. It seems unreasonable that\na single logistic regression",
    "start": "1589060",
    "end": "1594520"
  },
  {
    "text": "could possibly find good thetas. You're like, can't we just use\ngradient ascent and they'll just get better, and\nbetter, and better?",
    "start": "1594520",
    "end": "1601430"
  },
  {
    "text": "And the answer is no. This model of logistic\nregression is just too far-- it's too puny to tackle\nsuch a large task.",
    "start": "1601430",
    "end": "1610090"
  },
  {
    "text": "There's not enough parameters. So even though you can have a\ngreat optimization algorithm, we can write a\nlikelihood function,",
    "start": "1610090",
    "end": "1616660"
  },
  {
    "text": "we can try and optimize it. There's no set of\nparameters that will start to do a good task. Or a good job at this task.",
    "start": "1616660",
    "end": "1624279"
  },
  {
    "text": "Anything confusing about that? Interesting, curious. Yes? So can you go over again,\nlike what differentiates this",
    "start": "1624280",
    "end": "1630880"
  },
  {
    "text": "from [INAUDIBLE] like\nbeing able to tell if you like a movie or not?",
    "start": "1630880",
    "end": "1637000"
  },
  {
    "text": "Why is this not\ndoable [INAUDIBLE]?? So the question is,\nwhy is this not doable,",
    "start": "1637000",
    "end": "1642309"
  },
  {
    "text": "but the movie thing was doable? It's a good question. Part of it is that\nthe task is harder.",
    "start": "1642310",
    "end": "1647950"
  },
  {
    "text": "Part of it is think about\nthe number of inputs. How many pixels\ndo you have here?",
    "start": "1647950",
    "end": "1653500"
  },
  {
    "text": "And the number of inputs\nis really, really, really, really, really large.",
    "start": "1653500",
    "end": "1658750"
  },
  {
    "text": "And the combination\nof those inputs that leads to your prediction\nis very complicated.",
    "start": "1658750",
    "end": "1664190"
  },
  {
    "text": "So you have a huge\ndimension for x. And what makes for a\ny, the true methodology",
    "start": "1664190",
    "end": "1671740"
  },
  {
    "text": "for deciding if this is a 0\nor 1, is very complicated. It's not just, is\nthere a white here?",
    "start": "1671740",
    "end": "1677410"
  },
  {
    "text": "Because the white could be over\nhere when you draw your zero. Whereas if this was\na particular movie,",
    "start": "1677410",
    "end": "1684010"
  },
  {
    "text": "it's much easier because\nit doesn't change so much. If somebody likes\nthis movie, then it has a very consistent\nimpact on whether or not",
    "start": "1684010",
    "end": "1690940"
  },
  {
    "text": "they like your target movie. So two answers. One is much higher\ndimension, the second is much more complicated\ninteraction between the inputs",
    "start": "1690940",
    "end": "1699160"
  },
  {
    "text": "to making the output. So the dimension part can\nbe fixed with more computing power, right?",
    "start": "1699160",
    "end": "1705470"
  },
  {
    "text": "Yes. It could be fixed with\nmore computing power, but the other part can't be. Yes, fantastic question. Yes?",
    "start": "1705470",
    "end": "1711350"
  },
  {
    "text": "Do we understand\nexactly why it becomes really powerful\nto stack these up, or did we just kind of\ndo it and realize it?",
    "start": "1711350",
    "end": "1719610"
  },
  {
    "text": "Yeah. There are some people who would\nsay they have some guesses, but you're not completely\nwrong in that we did it,",
    "start": "1719610",
    "end": "1725000"
  },
  {
    "text": "it seemed kind of like a\ncartoon model of a brain, and then it worked. And then we started\nanalyzing it.",
    "start": "1725000",
    "end": "1730049"
  },
  {
    "text": "But now that we've\nanalyzed it, there are some cool theories\nthat say, oh, you could learn nonlinear functions.",
    "start": "1730050",
    "end": "1735679"
  },
  {
    "text": "There's actually no limit to\nthe functions you could learn. And people are still trying\nto tell more and more nuanced",
    "start": "1735680",
    "end": "1742250"
  },
  {
    "text": "stories for why this works. But your insight was correct. People basically\njust tried this. Wait, can I tell you\na hilarious thing?",
    "start": "1742250",
    "end": "1748790"
  },
  {
    "text": "Somebody proposed this,\nand they tried it, and it didn't work because\ntheir computers were too puny. And then no one researched\nit for a really long time.",
    "start": "1748790",
    "end": "1756470"
  },
  {
    "text": "And then a couple of people\nwere like, hey, computers are really good. Should we try that old idea? And then it worked and\nthey got Turing awards.",
    "start": "1756470",
    "end": "1763340"
  },
  {
    "text": "Really, all of them. But anyway, yeah. It turns out the\nfunctionality was really",
    "start": "1763340",
    "end": "1770330"
  },
  {
    "text": "what drove the research. I'm going to start drawing a\nlogistic regression like this,",
    "start": "1770330",
    "end": "1775940"
  },
  {
    "text": "if you guys don't mind. This is a logistic\nregression, but now I'm going to ignore all\nthe middle parts.",
    "start": "1775940",
    "end": "1781610"
  },
  {
    "text": "The squashing-- or the\nsumming and the squashing. And I'm just going to\nhave an arrow that says, all of these inputs\nget summed and squashed",
    "start": "1781610",
    "end": "1787852"
  },
  {
    "text": "and they lead to this output. Does that sound reasonable? Now logistic regression.",
    "start": "1787852",
    "end": "1794059"
  },
  {
    "text": "Maybe it could do a\ngood job sometimes, but it's not always\ngoing to do a good job. And so the very\nsimple, humble idea",
    "start": "1794060",
    "end": "1801188"
  },
  {
    "text": "that somebody\nproposed, and it didn't work because their\ncomputers were too puny, and then it was\na huge deal later is what if we had stacks\nof neural networks.",
    "start": "1801188",
    "end": "1810769"
  },
  {
    "text": "And to be clear, here I'm going\nto say, you have your inputs, and you have your outputs.",
    "start": "1810770",
    "end": "1816410"
  },
  {
    "text": "But I'm going to introduce a\nnew thing called a hidden layer. And every single thing\nin this hidden layer",
    "start": "1816410",
    "end": "1822230"
  },
  {
    "text": "is going to be a logistic\nregression connected to the inputs.",
    "start": "1822230",
    "end": "1828220"
  },
  {
    "text": "And then once I have\nthis hidden layer, I'll build a logistic\nregression that takes the hidden\nlayer as its input",
    "start": "1828220",
    "end": "1834670"
  },
  {
    "text": "and then predicts the output. I am going to-- just to\ngive you an idea of this.",
    "start": "1834670",
    "end": "1840809"
  },
  {
    "text": "As I said, every single\ncircle in this hidden layer is a logistic regression.",
    "start": "1840810",
    "end": "1846030"
  },
  {
    "text": "It's going to take\nevery input, weight it in its own way with its own\nparameters, sum it, squash it,",
    "start": "1846030",
    "end": "1851760"
  },
  {
    "text": "and then it will turn on or off. So this circle represents\none logistic regression. But so does that circle.",
    "start": "1851760",
    "end": "1858029"
  },
  {
    "text": "This circle is also\nlogistic regression. It has the same inputs, but\nit's got its own weights. So it may turn on or\noff in a different way.",
    "start": "1858030",
    "end": "1865680"
  },
  {
    "text": "Every single one\nof these circles is now a logistic regression. So you have a huge\namount more parameters.",
    "start": "1865680",
    "end": "1871980"
  },
  {
    "text": "And then finally,\nif we did this, we'd have a whole\nbunch of 0s and 1s. And that's not a prediction.",
    "start": "1871980",
    "end": "1877960"
  },
  {
    "text": "So we're going to take\nall of the outputs of these logistic\nregressions and call them inputs to a final\nlogistic regression, which",
    "start": "1877960",
    "end": "1885059"
  },
  {
    "text": "is going to predict our 0 or 1. Cool or what?",
    "start": "1885060",
    "end": "1890570"
  },
  {
    "text": "What a simple idea. If we can understand\nthis, that is-- this now counts as deep\nlearning because we have depth of our neurons.",
    "start": "1890570",
    "end": "1896820"
  },
  {
    "text": "You can imagine the depth\nis that at some point, you have a logistic regression\nwhose inputs were the outputs",
    "start": "1896820",
    "end": "1901890"
  },
  {
    "text": "of other logistic regressions.  There are so many\nparameters here.",
    "start": "1901890",
    "end": "1908973"
  },
  {
    "text": "If you imagine this final\nlogistic regression, we've got a weighting\nfor every single output. But how about over here?",
    "start": "1908973",
    "end": "1915000"
  },
  {
    "text": "There's a whole\nbunch of parameters. In fact, I'd encourage you to\nthink about how many parameters there could be.",
    "start": "1915000",
    "end": "1920470"
  },
  {
    "text": "But for now, I'm just going to\nsay we have way more parameters than we used to have. ",
    "start": "1920470",
    "end": "1928030"
  },
  {
    "text": "And just to be clear, what\nwe just described to you is a neural network, many\nlogistic regressions stacked",
    "start": "1928030",
    "end": "1934750"
  },
  {
    "text": "to each other, and\ndeep learning is the process of\nmaximizing likelihood for a neural network.",
    "start": "1934750",
    "end": "1942280"
  },
  {
    "text": "And at this point,\nyou can imagine it as it's just like\nlogistic regression, but now your input\nto output matching",
    "start": "1942280",
    "end": "1948970"
  },
  {
    "text": "is just going to be lots of\nlogistic regressions, LOL. There's a whole slide just\nfor that one awful joke.",
    "start": "1948970",
    "end": "1956290"
  },
  {
    "text": "Can I show you a demonstration? I feel like it's\ncool to talk about it and show it in a\nslide, but wouldn't it",
    "start": "1956290",
    "end": "1962230"
  },
  {
    "text": "be great to see a real,\nlive neural network? Let me show you one that\ndoes this task of predicting",
    "start": "1962230",
    "end": "1967570"
  },
  {
    "text": "0s and 1s. Oh, hey, our critter\nis still learning. This whole time that we've been\ntalking, it's been practicing.",
    "start": "1967570",
    "end": "1974570"
  },
  {
    "text": "Here is a logistic regression--\nor, sorry, a deep learning network, because it's not\njust one logistic regression.",
    "start": "1974570",
    "end": "1980659"
  },
  {
    "text": "At the bottom here, you're going\nto have inputs and at the top you're going to\nhave predictions. I'm going to hand draw a 0.",
    "start": "1980660",
    "end": "1987860"
  },
  {
    "text": "Here we go. And I'm going to be\nnot great about it. And our network's job is to\ntake all those pixel values",
    "start": "1987860",
    "end": "1995240"
  },
  {
    "text": "and predict if\nthis was a 0 or 1. And if you look at\nit, at the very bottom",
    "start": "1995240",
    "end": "2000480"
  },
  {
    "text": "of this neural network,\nwe have a bunch of cells that represent the\ninitial pixel values of what",
    "start": "2000480",
    "end": "2006940"
  },
  {
    "text": "I drew. Does that make sense? Then every single\ncube here is going to be its own\nlogistic regression.",
    "start": "2006940",
    "end": "2013020"
  },
  {
    "text": "We'll look into those\nin a little bit. But if you go to the very\ntop, at the very top, we have logistic\nregressions that represent--",
    "start": "2013020",
    "end": "2020250"
  },
  {
    "text": "that have numbers above them. This is a logistic\nregression that's associated with 0, 1,\n2, 3, 4, 5, 6, 7, 8, 9.",
    "start": "2020250",
    "end": "2026441"
  },
  {
    "text": "And did you notice\nthat when I drew that 0, the logistic regression\nassociated with the 0 is the one that turned on.",
    "start": "2026442",
    "end": "2033000"
  },
  {
    "text": "So overall, it's\ntaking pixel values and recognizing it's a 0.",
    "start": "2033000",
    "end": "2038310"
  },
  {
    "text": "And if we drew something\ndifferent, like a 7, it'll take pixel values\ndown here, just what I drew,",
    "start": "2038310",
    "end": "2045409"
  },
  {
    "text": "and then turn on this one\ncell that represents a 7. Or if I drew, let's\nsay, a 1, it could",
    "start": "2045410",
    "end": "2053239"
  },
  {
    "text": "take all these pixel\nvalues and then turn on that one cell\nthat represents a 1. That's the input and\nthat's the output.",
    "start": "2053239",
    "end": "2060279"
  },
  {
    "text": "But clearly, it doesn't just\nhave one logistic regression anymore. We have a whole neural network. So if you look at this\nfinal cell up here,",
    "start": "2060280",
    "end": "2068469"
  },
  {
    "text": "it is a logistic\nregression and its inputs are not coming from the pixels.",
    "start": "2068469",
    "end": "2073540"
  },
  {
    "text": "Its inputs are coming from\nall of these other logistic regressions, which are\nturned on in different ways.",
    "start": "2073540",
    "end": "2079460"
  },
  {
    "text": "And if you looked\nat any one of them, you could see that\nany one of these is a logistic regression\nwhose inputs come",
    "start": "2079460",
    "end": "2085540"
  },
  {
    "text": "from this previous layer. We call these things layers\nof logistic regressions.",
    "start": "2085540",
    "end": "2090544"
  },
  {
    "text": "And if you looked at a different\none, this one turned on, but it had the same inputs as\nthe other one we looked at.",
    "start": "2090545",
    "end": "2095619"
  },
  {
    "text": "It's just probably weighted\nthose inputs differently. So every single cube here\nis a logistic regression.",
    "start": "2095620",
    "end": "2101560"
  },
  {
    "text": "And as you get lower and lower,\ntheir logistic regression's pulling from different\nparts of the image.",
    "start": "2101560",
    "end": "2108590"
  },
  {
    "text": "OK, that's it. That is deep learning. And at this point, because you\nunderstand logistic regression,",
    "start": "2108590",
    "end": "2115310"
  },
  {
    "text": "you could just start\nstacking these things on top of each\nother and you would have your own neural network.",
    "start": "2115310",
    "end": "2121960"
  },
  {
    "text": "Now of course, the\ndifficulty will always be where do you get\nthose thetas from?",
    "start": "2121960",
    "end": "2128140"
  },
  {
    "text": "You imagine if somebody\nmade this neural network and every parameter was random? It wouldn't be able to\ntake pixels and predict",
    "start": "2128140",
    "end": "2136060"
  },
  {
    "text": "what's in the picture. All of its\nintelligence will still be coming from the parameters.",
    "start": "2136060",
    "end": "2141100"
  },
  {
    "text": "So the mystery still will be,\nI can make a neural network, but how could I learn\nthose parameters?",
    "start": "2141100",
    "end": "2147160"
  },
  {
    "text": "So is there any solution to\nwhere you start your parameters or do you really just\npick random [INAUDIBLE]??",
    "start": "2147160",
    "end": "2154029"
  },
  {
    "text": "You truly pick random. There's one thing that's\nvery different from what you do in your problem sets\nor logistic regression. In logistic regression, we\nstarted all our parameters",
    "start": "2154030",
    "end": "2161218"
  },
  {
    "text": "at 0. But for a neural\nnetwork, it turns out to be important that you\nstart all of your parameters randomly.",
    "start": "2161218",
    "end": "2166420"
  },
  {
    "text": "And that allows the\ndifferent neurons to learn different things. But no, really, we start random.",
    "start": "2166420",
    "end": "2172270"
  },
  {
    "text": "There's no smart starting point. It's just starting at\nrandom and then hill climb.",
    "start": "2172270",
    "end": "2177430"
  },
  {
    "text": "Of course, we'll get\ninto the details. Yeah, OK. Question? So intuitively, what are\nthese cubes in each layer?",
    "start": "2177430",
    "end": "2185480"
  },
  {
    "text": "Every single cube is its\nown logistic regression. It takes inputs, it\nweights those inputs,",
    "start": "2185480",
    "end": "2192980"
  },
  {
    "text": "it squashes those\ninputs, and then it comes up with its\nown value, which is the squashing of all\nthose weighted inputs.",
    "start": "2192980",
    "end": "2201080"
  },
  {
    "text": "So every single cube, it\nis one logistic regression. It will have its own\nparameters and then",
    "start": "2201080",
    "end": "2206359"
  },
  {
    "text": "it will do the summing-squashing\nbased on those parameters. Does that answer? Yeah.",
    "start": "2206360",
    "end": "2211790"
  },
  {
    "text": "Also what are the combination\nof those cubes then for each of these rectangles? Oh, like over\nhere, these layers?",
    "start": "2211790",
    "end": "2218370"
  },
  {
    "text": "Yeah. So like in each layer, we have\na bunch of different rectangles or a combination\nof different cubes. What are those?",
    "start": "2218370",
    "end": "2224630"
  },
  {
    "text": "I mean, I call\nthese things layers. When you get deeper in here,\nwe have these grid-like ones.",
    "start": "2224630",
    "end": "2230090"
  },
  {
    "text": "That has-- it's a particular\nway of doing images where we have a few logistic\nregressions that, as you",
    "start": "2230090",
    "end": "2238550"
  },
  {
    "text": "notice, have smaller\nnumber of inputs. They just take four inputs. That's a nice little\ntrick for images.",
    "start": "2238550",
    "end": "2245119"
  },
  {
    "text": "Not worth focusing\non too much now. We'll talk about that later. But these three layers are-- they're not fully connected.",
    "start": "2245120",
    "end": "2251880"
  },
  {
    "text": "It's not that every single cube\nis taking every single input as an x.",
    "start": "2251880",
    "end": "2256950"
  },
  {
    "text": "It's a nice little design trick. Yeah? So I know you said that\nthe intelligence comes from the weights,\nbut then also you",
    "start": "2256950",
    "end": "2263790"
  },
  {
    "text": "have to decide beforehand\nhow deep are you going to go and within each layer how\nmany [INAUDIBLE] going to be.",
    "start": "2263790",
    "end": "2271197"
  },
  {
    "text": "So how do you decide that? Is some intelligence\nin that, too? It's a lot like this. I would call those\nhyperparameters.",
    "start": "2271197",
    "end": "2277500"
  },
  {
    "text": "So this is a little bit\nmore like the art and craft of deep learning,\nis you construct",
    "start": "2277500",
    "end": "2283109"
  },
  {
    "text": "how many layers, you\nconstruct how many neurons for each layer. Maybe there is some\nneat little thing",
    "start": "2283110",
    "end": "2288282"
  },
  {
    "text": "you'll do at the beginning\nso that these neurons have fewer inputs. All those things are artistic\nchoices, I would say.",
    "start": "2288282",
    "end": "2296910"
  },
  {
    "text": "Of course, there was\none person who was like, can I have the computer build\na neural network that could",
    "start": "2296910",
    "end": "2301950"
  },
  {
    "text": "make those choices for me? And people have\ndefinitely done that. And yet still, I still see a\nlot of people just being like,",
    "start": "2301950",
    "end": "2307080"
  },
  {
    "text": "we'll make a neural network. It'll have 40 layers and each\nlayer will have 100 neurons.",
    "start": "2307080",
    "end": "2314350"
  },
  {
    "text": "So artistic choices. I'm just going to\nask a question. So if it's artistic,\nso you just have",
    "start": "2314350",
    "end": "2321030"
  },
  {
    "text": "to practice your own way until\nyour accuracy goes up, I think. Yeah, exactly.",
    "start": "2321030",
    "end": "2326970"
  },
  {
    "text": "So you could do something. Like if you didn't know, you\ncould make two neural networks, train both of them,\nand then see which",
    "start": "2326970",
    "end": "2333420"
  },
  {
    "text": "one starts to get\nsmarter, and then that's how you could make\nyour artistic choice. Sounds a little bit more\nlike optimization, then.",
    "start": "2333420",
    "end": "2340070"
  },
  {
    "start": "2340070",
    "end": "2345880"
  },
  {
    "text": "So at this point, I hope you're\nunderstanding this big picture that deep learning is a bunch\nof logistics pointed together.",
    "start": "2345880",
    "end": "2353300"
  },
  {
    "text": "You visualized it,\nhopefully you understand it. But I haven't told\nyou where we're going to set all those thetas.",
    "start": "2353300",
    "end": "2359030"
  },
  {
    "text": "I've now exploded\nour number of thetas without talking about how\nwe're going to set them. And that seems impossible.",
    "start": "2359030",
    "end": "2365180"
  },
  {
    "text": "Which leaves us with the\nfinal mystery for today, how do we train. And you guys are getting\nthat close to understanding",
    "start": "2365180",
    "end": "2371390"
  },
  {
    "text": "deep learning. It is such a simple idea,\nlogistics on top of each other, but you need to be able to\nanswer this question as well.",
    "start": "2371390",
    "end": "2377870"
  },
  {
    "text": "And the start of this journey is\ngoing to be maximum likelihood estimation.",
    "start": "2377870",
    "end": "2383390"
  },
  {
    "text": "We're going to say everything\nin our neural network is a parameter and we want\nto choose the parameters that",
    "start": "2383390",
    "end": "2389270"
  },
  {
    "text": "maximize the likelihood\nof some training data set. There we go.",
    "start": "2389270",
    "end": "2395059"
  },
  {
    "text": "First, a couple learning goals. I do want you to walk away from\ntoday's class understanding chain rule a little bit better,\nand understanding a chain rule",
    "start": "2395060",
    "end": "2402530"
  },
  {
    "text": "is the heart and soul of\nthe gaining of intelligence for a neural network,\nthe deep learning.",
    "start": "2402530",
    "end": "2408770"
  },
  {
    "text": "I do want to demystify\ndeep learning. You guys might have heard\nabout deep learning. You might have heard people\ntalk about it like it's",
    "start": "2408770",
    "end": "2415455"
  },
  {
    "text": "some sort of sentient\nbeing, but now I want you to understand\nno, it's something you could tangibly understand.",
    "start": "2415455",
    "end": "2422180"
  },
  {
    "text": "And then finally,\none of my goals is you walk out\ntoday so ready to get",
    "start": "2422180",
    "end": "2428260"
  },
  {
    "text": "the big picture of the\nlogistic regression that you have to do\nfor your problem set. And now it is time for\nsome math worth learning.",
    "start": "2428260",
    "end": "2436280"
  },
  {
    "text": "We are going to go through the\nmath that is how deep learning gets us intelligence.",
    "start": "2436280",
    "end": "2441800"
  },
  {
    "text": "And this might become\nsomething you could expect should become common knowledge.",
    "start": "2441800",
    "end": "2447650"
  },
  {
    "text": "As deep learning becomes\nmore and more pervasive, it impacts our\nlives more and more. Whether or not you\nbuild it or you just",
    "start": "2447650",
    "end": "2453140"
  },
  {
    "text": "want to understand this\ntool that exists in society, we should know this math.",
    "start": "2453140",
    "end": "2458380"
  },
  {
    "text": "A little bit of new notation. I am going to write it\non the board over here so that we can keep\ntrack of it over time.",
    "start": "2458380",
    "end": "2464619"
  },
  {
    "text": "I got x, that was our inputs\nof our little black box. I've still got y hat.",
    "start": "2464620",
    "end": "2471010"
  },
  {
    "text": "That's our output\nof our black box. But now I've got\na hidden layer, h. And I am going to talk about\neach of these things as xi's.",
    "start": "2471010",
    "end": "2483060"
  },
  {
    "text": "That's nothing too new. I'm going to refer to\neach of these neurons",
    "start": "2483060",
    "end": "2488200"
  },
  {
    "text": "in the hidden layer as h-- I think I call it j, right? Yeah.",
    "start": "2488200",
    "end": "2494090"
  },
  {
    "text": "So we're going to call\neach of these ones hj. And hj is going to have a whole\nbunch of parameters associated",
    "start": "2494090",
    "end": "2502760"
  },
  {
    "text": "with it. And the parameters\nassociated with hj, I'm going to be\ncalling the thetas.",
    "start": "2502760",
    "end": "2508595"
  },
  {
    "text": " And in superscript, I'm\ngoing to say it's for hj.",
    "start": "2508595",
    "end": "2516579"
  },
  {
    "text": "And that's going to\ntell me that I'm going to have a bunch more thetas-- oh, sorry. This is thetas for\nthe hidden layer.",
    "start": "2516580",
    "end": "2523780"
  },
  {
    "text": " And particularly for\nthis logistic regression,",
    "start": "2523780",
    "end": "2529820"
  },
  {
    "text": "it's going to have a theta for\nevery single one of the inputs. So this hj will have a\ntheta for x0, x1, x2, x3.",
    "start": "2529820",
    "end": "2538340"
  },
  {
    "text": "Does that make sense? So hj it's going to have\na theta for theta 0.",
    "start": "2538340",
    "end": "2546370"
  },
  {
    "text": "It'll have a theta for theta 1. It'll have a theta for theta\n2, a theta for theta for x3.",
    "start": "2546370",
    "end": "2553460"
  },
  {
    "text": "And so because of that,\nwe're going to think of-- oh, it looks like they're\nhanging out together. Reimagine there's going to\nbe a whole set of parameters",
    "start": "2553460",
    "end": "2561170"
  },
  {
    "text": "for this hidden layer. And for this hidden\nlayer, it will have two different subscripts. It will say I have a parameter\nfor in this hidden layer",
    "start": "2561170",
    "end": "2569450"
  },
  {
    "text": "every combination of some\ninput and some hidden neuron. Wow, that's a whole\nbunch of parameters.",
    "start": "2569450",
    "end": "2577430"
  },
  {
    "text": "Every single one of\nthese hidden neurons is a logistic regression. So hj is going to be\na logistic regression.",
    "start": "2577430",
    "end": "2585890"
  },
  {
    "text": "You guys ready for it? That means it's going\nto be the squashing function of the weighted\nsum of two things.",
    "start": "2585890",
    "end": "2594380"
  },
  {
    "text": "One of those things is\nx and the other thing is going to be theta--",
    "start": "2594380",
    "end": "2601420"
  },
  {
    "text": "is going to be the\ntheta h's for j. But this sum-- or this\ntranspose is the same",
    "start": "2601420",
    "end": "2608320"
  },
  {
    "text": "as saying, OK, we're going to\nsum over all the inputs in xi. We're going to take every xi\nand we're going to weight it.",
    "start": "2608320",
    "end": "2618650"
  },
  {
    "text": "It's going to be a\nweight, so it's a theta. It's coming from\nthis hidden layer, so it's going to have\nthis superscript h.",
    "start": "2618650",
    "end": "2625850"
  },
  {
    "text": "And particularly,\nthis is the one that's going to the\nj-th hidden neuron. So it's going to have\na j subscript there.",
    "start": "2625850",
    "end": "2633830"
  },
  {
    "text": "And we're going to loop\nover all the inputs. And there's going to\nbe a different theta for every single input here.",
    "start": "2633830",
    "end": "2641400"
  },
  {
    "text": "So this is my mathematical way\nof saying every single circle here in this hidden\nlayer is going",
    "start": "2641400",
    "end": "2647809"
  },
  {
    "text": "to be a logistic regression. It's going to be a\nlogistic regression, which has its own weights,\nand there'll",
    "start": "2647810",
    "end": "2652940"
  },
  {
    "text": "be one weight for every input\nhidden layer combination. If I gave you x's, and I\ngave you all the thetas,",
    "start": "2652940",
    "end": "2660470"
  },
  {
    "text": "you could use this formula to\ndecide if hj was close to 1",
    "start": "2660470",
    "end": "2665660"
  },
  {
    "text": "or not. When you actually evaluate\nthis, this will become a number. OK, that was complicated.",
    "start": "2665660",
    "end": "2672150"
  },
  {
    "text": "Let's take some questions. Yes? So how many thetas would there\nbe in, I guess, that example?",
    "start": "2672150",
    "end": "2679040"
  },
  {
    "text": "It's such a good\nquestion, I'm going to ask you in just a second. But I'm going to let you\nguys start thinking about it.",
    "start": "2679040",
    "end": "2684350"
  },
  {
    "text": "Imagine there's 20-- or 40\nthings here, and 10 things",
    "start": "2684350",
    "end": "2691280"
  },
  {
    "text": "here, and one thing here. You can start asking yourself,\nhow many thetas are there? But I will ask you\nin just a second.",
    "start": "2691280",
    "end": "2697079"
  },
  {
    "text": "So think about it for a moment. I do want to introduce just\na little bit more notation.",
    "start": "2697080",
    "end": "2702560"
  },
  {
    "text": "And the last piece of\nnotation I'd like to introduce is that all of\nthis just describes the logistic regressions\nthat go from x to h.",
    "start": "2702560",
    "end": "2710580"
  },
  {
    "text": "There is one final\nlogistic regression that takes all of\nthese outputs as inputs",
    "start": "2710580",
    "end": "2716640"
  },
  {
    "text": "and then predicts a y. And in that final\nlogistic regression, y hat is the output of\na logistic regression.",
    "start": "2716640",
    "end": "2723609"
  },
  {
    "text": "So again, there's a sum. This sum is going\nto go over and it's going to pull out each\nof the h's, and it's",
    "start": "2723610",
    "end": "2732070"
  },
  {
    "text": "going to weight them. And it'll be another weight. And to distinguish the\nweights in this part",
    "start": "2732070",
    "end": "2739247"
  },
  {
    "text": "of the neural network\nfrom the weights in this part in\nthe neural network, we're going to consider these\nweights to be theta with y",
    "start": "2739247",
    "end": "2745450"
  },
  {
    "text": "in the superscript. And that's just so that we\ncan tell apart these weights from these weights.",
    "start": "2745450",
    "end": "2750970"
  },
  {
    "text": "So these are the\nweights in the h layer and these are the\nweights in the y layer.",
    "start": "2750970",
    "end": "2756750"
  },
  {
    "text": "And then here it\nis on the screen for people who have trouble\nreading on the board. But I'm going to keep\nthis on the board",
    "start": "2756750",
    "end": "2762635"
  },
  {
    "text": "the whole class through. Yes? Does that mean every h value is\ndependent on every single value",
    "start": "2762635",
    "end": "2768202"
  },
  {
    "text": "of x and the only thing\nthat's different is the theta value that you choose? What you said was 100% correct.",
    "start": "2768202",
    "end": "2773505"
  },
  {
    "text": "That's awesome. So every single h gets\nthe exact same set of inputs and the only\nthing that's different",
    "start": "2773505",
    "end": "2778690"
  },
  {
    "text": "is that they will have\ndifferent thetas, so different parameters. OK, rock and rolling.",
    "start": "2778690",
    "end": "2784900"
  },
  {
    "text": "What I don't understand is you\nsaid that the little circle is",
    "start": "2784900",
    "end": "2791289"
  },
  {
    "text": "the regression thing? So how is it having\nthetas if everything's contained within the circle?",
    "start": "2791290",
    "end": "2798490"
  },
  {
    "text": "This little circle, it's\ngoing to be a computation. You'll start with\nthe x's and how",
    "start": "2798490",
    "end": "2803950"
  },
  {
    "text": "do you compute the value\nof this little circle. The way you compute the\nvalue of this little circle is you use this formula.",
    "start": "2803950",
    "end": "2809750"
  },
  {
    "text": "So you take all your\nx's and then you're going to be weighting\nthem by these thetas. These thetas are going to live\nin the memory of your computer.",
    "start": "2809750",
    "end": "2817060"
  },
  {
    "text": "And these thetas are\ngoing to tell you how you should be weighting\nthe x's for this particular h. So you weight each of\nyour x's, get that number,",
    "start": "2817060",
    "end": "2824380"
  },
  {
    "text": "throw it through our sigmoid\nfunction, and that will be hj. If you chose a different h, it\nwould use the same x's, but it",
    "start": "2824380",
    "end": "2830740"
  },
  {
    "text": "would have its own thetas. Again, this would live\nsomewhere in your Python memory. It would pull out\nthose thetas, it",
    "start": "2830740",
    "end": "2836523"
  },
  {
    "text": "would do the sum, the squash,\nand it would get its own value. ",
    "start": "2836523",
    "end": "2841619"
  },
  {
    "text": "Fantastic. So let's do a forward pass. If we take a picture\nof a 1, we don't",
    "start": "2841620",
    "end": "2849299"
  },
  {
    "text": "have to guess the values of x. We're told what they are. Each value of x corresponds\nto a pixel value.",
    "start": "2849300",
    "end": "2854538"
  },
  {
    "text": "This will be the pixel value\nin the bottom right corner and that will be the pixel\nvalue in the top left corner.",
    "start": "2854538",
    "end": "2859890"
  },
  {
    "text": "Then once we have\nx's, we can calculate every single hidden neuron. We can save every\nsingle hidden neuron,",
    "start": "2859890",
    "end": "2866550"
  },
  {
    "text": "calculate yourself\nbased on the x's. In order to do\nthat, they're going to use this formula,\nwhich is also over here,",
    "start": "2866550",
    "end": "2873390"
  },
  {
    "text": "and this formula is going to\ntake those x's and weight them. Every single thing\nin the hidden layer has its own set of weights.",
    "start": "2873390",
    "end": "2880600"
  },
  {
    "text": "So they will come up\nwith different answers. Even though they're all\ntaking the same x's, since each one is\nweighting its own weights,",
    "start": "2880600",
    "end": "2887130"
  },
  {
    "text": "they'll get different\nanswers, and then you'll activate the hidden layer. We call this the forward pass. This is the prediction pass.",
    "start": "2887130",
    "end": "2893277"
  },
  {
    "text": "So we took an image, we\nactivate the hidden layer, and then finally we're going\nto activate this layer. This layer is going\nto say, OK, I'm",
    "start": "2893277",
    "end": "2899910"
  },
  {
    "text": "going to take each of\nthese as my inputs, I'm going to weight each\nof those inputs using",
    "start": "2899910",
    "end": "2905019"
  },
  {
    "text": "the weights that are stored in\nthis part of my Python program, and then I'm going to\nget that sum, squash it, and I'm going to call\nthat my prediction.",
    "start": "2905020",
    "end": "2913380"
  },
  {
    "text": "OK, so that's forward pass. If we wanted to get to the\npoint of learning these thetas,",
    "start": "2913380",
    "end": "2921990"
  },
  {
    "text": "it's now time to score\nour forward pass. So we did our forward pass.",
    "start": "2921990",
    "end": "2928260"
  },
  {
    "text": "How good a job did we do? And the scoring\nfunction we want should",
    "start": "2928260",
    "end": "2933359"
  },
  {
    "text": "be derivable with respect\nto the different thetas. So when we say, how\ngood a job do we do?",
    "start": "2933360",
    "end": "2938610"
  },
  {
    "text": "We don't want to just give\nus a 100% if we got it right or a 0% if we got it wrong. Instead, we'd like\nto have something",
    "start": "2938610",
    "end": "2944550"
  },
  {
    "text": "that we could learn from. So again, the scoring function\nwe'd like to use at this point is going to be maximum\nlikelihood estimation.",
    "start": "2944550",
    "end": "2952760"
  },
  {
    "text": "We're going to say, hey, you're\npredicting something binary.",
    "start": "2952760",
    "end": "2957950"
  },
  {
    "text": "And if you're predicting\nsomething binary, you could imagine that there's\nsome Bernoulli parameter, P.",
    "start": "2957950",
    "end": "2966410"
  },
  {
    "text": "You'd come up-- this\nvery final thing is going to be interpreted\nas a probability. So we're going to call this y\nhat is actually a probability.",
    "start": "2966410",
    "end": "2974240"
  },
  {
    "text": " And we're interpreting this very\nfinal thing as the probability",
    "start": "2974240",
    "end": "2980360"
  },
  {
    "text": "that the label should be 1. We say, whether or not the label\nshould be 1, it's a Bernoulli.",
    "start": "2980360",
    "end": "2985670"
  },
  {
    "text": "And what came out of\nyour logistic regression was the parameter\nfor that Bernoulli. And we can say how likely\nis a true label of 1",
    "start": "2985670",
    "end": "2996230"
  },
  {
    "text": "if your probability was a 0.8? So if you're probably\nis a 0.8, you can say the likelihood\nhere is going",
    "start": "2996230",
    "end": "3001690"
  },
  {
    "text": "to be 0.8 to the power of 1\ntimes 0.2 to the power of 0. So 0.8.",
    "start": "3001690",
    "end": "3007900"
  },
  {
    "text": "If you guess-- if\nthe true label is a 1 and your final probability\nis a 0.8, we can score that.",
    "start": "3007900",
    "end": "3013690"
  },
  {
    "text": "And we score it in\nthe exact same way that we score\nlogistic regression. Which I appreciate\nis complicated.",
    "start": "3013690",
    "end": "3019030"
  },
  {
    "text": "We've got this whole\nBernoulli thing, we have this continuous\nlikelihood function. That over there is the log\nof this continuous likelihood",
    "start": "3019030",
    "end": "3026400"
  },
  {
    "text": "function. So this scoring function is\nnot very good for computers. We want the log version of it. And we end up with\nthis crazy equation.",
    "start": "3026400",
    "end": "3033500"
  },
  {
    "text": "But the high level\npicture is the same. You can make a prediction,\nend up with a probability,",
    "start": "3033500",
    "end": "3039530"
  },
  {
    "text": "and I can score that probability\nusing the MLE mindset. Questions,\ncuriosities, concerns?",
    "start": "3039530",
    "end": "3047110"
  },
  {
    "text": "Yeah? What's actually in layer h? Is it another set of thetas for\neach [INAUDIBLE] regressions,",
    "start": "3047110",
    "end": "3054099"
  },
  {
    "text": "or is it actually\na 0, or 1 value? Both. There will be thetas,\nand those thetas",
    "start": "3054100",
    "end": "3059529"
  },
  {
    "text": "will lead to actual values. They won't be 0 and\n1 because they're coming out of a sigmoid. They'll be something\nbetween 0 and 1.",
    "start": "3059530",
    "end": "3066830"
  },
  {
    "text": "But they'll be smushed to\nbe close to 0's or 1's. So they're both true. There are thetas. They'll have numbers.",
    "start": "3066830",
    "end": "3072490"
  },
  {
    "text": "There will be activations. If you put an input,\nyou can activate the hidden layer itself. So there's both a theta,\nyou can think of those",
    "start": "3072490",
    "end": "3079138"
  },
  {
    "text": "as living between the\nlayers, and then there's the layer itself, which\nwill end up getting-- taking on values.",
    "start": "3079138",
    "end": "3085200"
  },
  {
    "text": "Yeah? Could you repeat\n[INAUDIBLE] probability? I'm sorry, what was the-- I couldn't hear you.",
    "start": "3085200",
    "end": "3090650"
  },
  {
    "text": "How do score the probability? We scored the\nprobability using MLE. So we're going to say, how\nlikely does one data point look",
    "start": "3090650",
    "end": "3098120"
  },
  {
    "text": "like based on this prediction? The data point will\nhave a true label. What's the true label here?",
    "start": "3098120",
    "end": "3103710"
  },
  {
    "text": "So you end up with a probability\nout here, let's say it's a 0.8. How do you score the 0.8\nif the true label is one?",
    "start": "3103710",
    "end": "3109980"
  },
  {
    "text": "So y hat is going to be\nyour 0.8 and the true label is going to be y.",
    "start": "3109980",
    "end": "3115590"
  },
  {
    "text": "And so you could put that into\nthis little function here. It's 0.8, put that through\na log, times it by 1",
    "start": "3115590",
    "end": "3122130"
  },
  {
    "text": "because the true label was a 1. This is going to be times by 0,\nso we're going to forget that.",
    "start": "3122130",
    "end": "3127710"
  },
  {
    "text": "It'll just be 1 times log\nof 0.8 will be your score. So you have these two\nthings, this y hat,",
    "start": "3127710",
    "end": "3133930"
  },
  {
    "text": "which is the probability\nthat you just calculated. And if you had a true label\nbecause this is training data,",
    "start": "3133930",
    "end": "3140070"
  },
  {
    "text": "then you can put those two\nthings together into a score. It looks really confusing. I feel like this equation's\nincredibly confusing for what",
    "start": "3140070",
    "end": "3147300"
  },
  {
    "text": "it really is. It's just taking a\nprobability and scoring it based on the true label. And it's just using\nthis derivable version",
    "start": "3147300",
    "end": "3155430"
  },
  {
    "text": "of the Bernoulli\nprobability mass function. Insane, but very clever.",
    "start": "3155430",
    "end": "3162559"
  },
  {
    "text": "Good job, whoever\nfigured this out.  Yeah?",
    "start": "3162560",
    "end": "3167910"
  },
  {
    "text": "Just as a sanity\ncheck, I know you just said that layer h is basically a\nlist of outputs of the sigmoid.",
    "start": "3167910",
    "end": "3174270"
  },
  {
    "text": "But wouldn't we need to pass\non a list of binary inputs when we do the second round\nof logistic regression?",
    "start": "3174270",
    "end": "3180870"
  },
  {
    "text": "Ah, what a good question. Do you remember earlier somebody\nasked me, hey, when would you",
    "start": "3180870",
    "end": "3186090"
  },
  {
    "text": "use logistic regression? When would you use naive Bayes? I threw out a little bit\nof an interesting comment.",
    "start": "3186090",
    "end": "3191160"
  },
  {
    "text": "I said, hey, if your\ninputs weren't binary, if your inputs were\nsomething like real valued, that's totally fine for\nlogistic regression.",
    "start": "3191160",
    "end": "3196923"
  },
  {
    "text": "It doesn't care. But totally breaks Naive Bayes. It turns out logistic regression\ndoesn't rely on the fact",
    "start": "3196923",
    "end": "3203790"
  },
  {
    "text": "that x's are zeros or ones. Logistic regression is\ntotally fine if those are 0.7.",
    "start": "3203790",
    "end": "3209190"
  },
  {
    "text": "It's totally fine if\nthose are like 20. It doesn't even change\nthe probability analysis because the MLE is all just\nbased off of that final output",
    "start": "3209190",
    "end": "3217349"
  },
  {
    "text": "anyways. So there's no probability\nanalysis here. It's just a black box,\nand that black box",
    "start": "3217350",
    "end": "3223290"
  },
  {
    "text": "is totally fine with\nreal-valued inputs. Good question. Yes? Does that impact\nthe answer, though,",
    "start": "3223290",
    "end": "3228740"
  },
  {
    "text": "that you would get if you were\nputting in non-binary things? I'm thinking in terms of\nmaybe our movie example. Instead of doing they like\nor they don't like it,",
    "start": "3228740",
    "end": "3235342"
  },
  {
    "text": "it's like they 70% like it? You know what? Logistic regression would\njust learn different weights.",
    "start": "3235342",
    "end": "3241000"
  },
  {
    "text": "And it would probably do\nbetter if it had richer inputs. Such a good question. I love this. Yes?",
    "start": "3241000",
    "end": "3246190"
  },
  {
    "text": "Sorry, back to the\nlog likelihood thing. [INAUDIBLE] if your true value\nis a number from 0 to 10?",
    "start": "3246190",
    "end": "3254153"
  },
  {
    "text": "Well, in this case,\nlet's say it's 0, 1. We're still into the\npredicting 0's or 1's. Because if it wasn't\na 0 or 1, then we",
    "start": "3254153",
    "end": "3260770"
  },
  {
    "text": "couldn't use the Bernoulli\nprobability mass function. We'd have to use\na different one. I'll talk to you about that\nat the end of today's class.",
    "start": "3260770",
    "end": "3266440"
  },
  {
    "text": "But actually, yeah. We're just predicting hand-drawn\nzeros or hand-drawn ones. ",
    "start": "3266440",
    "end": "3272869"
  },
  {
    "text": "Fantastic. This is a crazy,\ncomplicated slide. It gets a little bit\neasier from here.",
    "start": "3272870",
    "end": "3278060"
  },
  {
    "text": "It looks scary at some\npoint, but it's just a ruse. So if you can understand\nthis, you're in a good place.",
    "start": "3278060",
    "end": "3284270"
  },
  {
    "start": "3284270",
    "end": "3290390"
  },
  {
    "text": "We put it all\ntogether and we start to write neural networks in much\nmore complicated-- or compact ways. We've got our x, we've got\nour h, and we've got our y.",
    "start": "3290390",
    "end": "3297545"
  },
  {
    "text": "Between the x and the\nh, we have some thetas. Between the h and the y,\nwe've got some more thetas. And then I have this question\nthat somebody asked me earlier,",
    "start": "3297545",
    "end": "3305060"
  },
  {
    "text": "and I was so cheeky, and\nI said, you figure it out. I'm just joking. So let's say our x is\nsize 40, our h is size 20.",
    "start": "3305060",
    "end": "3315210"
  },
  {
    "text": "In this case, my\nfirst question for you is, how many\nparameters are there",
    "start": "3315210",
    "end": "3322079"
  },
  {
    "text": "in this part of\nthe neural network? After I ask you\nthat, I will ask you",
    "start": "3322080",
    "end": "3327450"
  },
  {
    "text": "how many parameters\nare there in this part of the neural network. Why don't you talk about\nwhat the person next to you. Come up with a guess. There's no reason\nthat you should",
    "start": "3327450",
    "end": "3334072"
  },
  {
    "text": "be able to answer this at\nthis point, but if you can, fantastic. And if not, we'll talk about it.",
    "start": "3334072",
    "end": "3339211"
  },
  {
    "text": "[SIDE CONVERSATIONS] ",
    "start": "3339211",
    "end": "3362140"
  },
  {
    "text": "OK, let's do this. Who thinks it's A?",
    "start": "3362140",
    "end": "3367299"
  },
  {
    "text": "Yeah!  Who thinks it's B?",
    "start": "3367300",
    "end": "3372862"
  },
  {
    "text": "A bunch of folks. Who thinks it's C? Not so many. How about D? A couple folks.",
    "start": "3372862",
    "end": "3380010"
  },
  {
    "text": "OK. Fun fact, they're actually\nall a little bit wrong. No, no, they're very close.",
    "start": "3380010",
    "end": "3385470"
  },
  {
    "text": "I would say if I had to\nchoose between these, I'd probably choose B. Because\nthis final layer is actually just one logistic\nregression and its inputs",
    "start": "3385470",
    "end": "3393023"
  },
  {
    "text": "are the hidden layers. So if there's 20 elements\nin the hidden layers, it'll have 20 inputs. And each of those inputs need\nto be weighted exactly once.",
    "start": "3393023",
    "end": "3399760"
  },
  {
    "text": "So they'll need 20 parameters. The reason I say\nit's slightly wrong is because I think the\ntrue answer should be 21.",
    "start": "3399760",
    "end": "3405270"
  },
  {
    "text": "And the reason is-- we're not going to get too\nmuch into in this lecture, but you know when we\nhad that bias term when",
    "start": "3405270",
    "end": "3411375"
  },
  {
    "text": "we did logistic regression? We're still going to keep using\nthose in our neural networks. So there'll be one\nextra term for a bias.",
    "start": "3411375",
    "end": "3417090"
  },
  {
    "text": "But that's not important\nright now, really. The number of things in\nthis y is not too large.",
    "start": "3417090",
    "end": "3423010"
  },
  {
    "text": "How about this one? How many parameters\nare there in this chunk",
    "start": "3423010",
    "end": "3428369"
  },
  {
    "text": "of the neural network? So this is the part that\nconnects the x's to the h's.",
    "start": "3428370",
    "end": "3433470"
  },
  {
    "text": "Think about it. Think about it. It's time to vote. Imagine there's a plus\n1 to all of these.",
    "start": "3433470",
    "end": "3439500"
  },
  {
    "text": "Who wants to say 800? Bunch of folks. Who wants to say 20?",
    "start": "3439500",
    "end": "3446280"
  },
  {
    "text": "That would be nice. Who wants to say 820? That'd be nice, too. Who wants to say 16,000, just\nall the parameters of the--",
    "start": "3446280",
    "end": "3454170"
  },
  {
    "text": "Will again. In this case, there's actually\na little bit more than 800.",
    "start": "3454170",
    "end": "3460733"
  },
  {
    "text": "If you forget the\nbias term, there would be exactly 800\nbecause every single one of these hidden neurons\nwill have a parameter",
    "start": "3460733",
    "end": "3468150"
  },
  {
    "text": "for every single input. So each of these are going\nto have 40 parameters and there's 20 of them.",
    "start": "3468150",
    "end": "3474030"
  },
  {
    "text": "So there's 20 times 40\nparameters in this chunk. There'll be a few more\nonce you have bias terms.",
    "start": "3474030",
    "end": "3479550"
  },
  {
    "text": "And 40 times 20 is 800. Oh my God. I got it wrong.",
    "start": "3479550",
    "end": "3484600"
  },
  {
    "text": "It says how many\nparameters in total? 800 plus 20 is 820. Man, reading\ncomprehension, Chris.",
    "start": "3484600",
    "end": "3491609"
  },
  {
    "text": "Tsk-tsk. OK, so there's something\nlike 820 parameters if you have 20 hidden neurons\nand 40 things in your inputs.",
    "start": "3491610",
    "end": "3502810"
  },
  {
    "text": "All you need in order to\ntrain is a partial derivative",
    "start": "3502810",
    "end": "3508740"
  },
  {
    "text": "of that likelihood function for\nevery single one of those 820 parameters. So let's go do 820\npartial derivatives.",
    "start": "3508740",
    "end": "3515970"
  },
  {
    "text": "Who's ready? Yeah. Yeah! Let's try and do\nsomething brave. Let's talk about how we can\nget the partial derivatives",
    "start": "3515970",
    "end": "3522952"
  },
  {
    "text": "for all of these 820 parameters\nthat all need setting. And we only have\nto do three things.",
    "start": "3522952",
    "end": "3528880"
  },
  {
    "text": "We only have to write the\nlikelihood assumption, or get this likelihood equation.",
    "start": "3528880",
    "end": "3533922"
  },
  {
    "text": "And then once we\nhave that, we just have to derive it\nwith respect to all of the different components.",
    "start": "3533922",
    "end": "3539670"
  },
  {
    "text": "And why do you need this? Because if you want a\ngood neural network, you need good thetas.",
    "start": "3539670",
    "end": "3546150"
  },
  {
    "text": "If you want good\nthetas, you want to search for ones that\nmaximize likelihood.",
    "start": "3546150",
    "end": "3552210"
  },
  {
    "text": "You can maximize likelihood\nusing optimization techniques such as gradient ascent.",
    "start": "3552210",
    "end": "3558750"
  },
  {
    "text": "But for gradient\nascent, you need to have the partial derivatives\nof the scoring function you",
    "start": "3558750",
    "end": "3564970"
  },
  {
    "text": "have with respect to\nevery single movable part in your model. And all of our movable\nparts are the thetas.",
    "start": "3564970",
    "end": "3570790"
  },
  {
    "text": "So that is the story,\nand it basically says when you guys--\nactually, I just",
    "start": "3570790",
    "end": "3576930"
  },
  {
    "text": "want to have this meta thing,\nthat understanding MLE applied to deep learning is only hard\nbecause there's these four compounded steps to get there.",
    "start": "3576930",
    "end": "3585500"
  },
  {
    "text": "So-- oh. Before we jump into this,\nI think this is funny. This is a picture of a\nhuge deep learning model",
    "start": "3585500",
    "end": "3591490"
  },
  {
    "text": "and we've got Scooby-Doo\nsaying, hey, gang, let's see what deep\nlearning really is, because at the end of the\nepisodes, they always reveal.",
    "start": "3591490",
    "end": "3597788"
  },
  {
    "text": "And in this case, oh my God,\nit's just convex optimization. It's just a whole bunch of\nderivatives happening that can",
    "start": "3597788",
    "end": "3604180"
  },
  {
    "text": "make your theta smarter. OK, let's jump into it. This is exactly the same\nas logistic regression.",
    "start": "3604180",
    "end": "3610840"
  },
  {
    "text": "Our model is going to eventually\noutput a probability, which we're going to assume is the\nprobability that the class",
    "start": "3610840",
    "end": "3617230"
  },
  {
    "text": "label is 1. The probability the\nclass label is 0 is going to be 1 minus that. That's our assumption,\nis that this model is",
    "start": "3617230",
    "end": "3622570"
  },
  {
    "text": "creating those probabilities. Based off that, we can get a\nsuper sweet scoring function.",
    "start": "3622570",
    "end": "3628530"
  },
  {
    "text": "If you had a single data\npoint, the way you could score this is you could use\nyour Bernoulli probability mass",
    "start": "3628530",
    "end": "3634850"
  },
  {
    "text": "function. So y hat is the\nprobability that came out of your neural network.",
    "start": "3634850",
    "end": "3639950"
  },
  {
    "text": "If you want to get\nreally detailed into it, it's coming from this equation. But it is the\nprobability that-- we're",
    "start": "3639950",
    "end": "3644990"
  },
  {
    "text": "assuming is the probability\nthat y equals 1. And because of that, that's the\nparameter of your Bernoulli. Your data point has\na true value, y.",
    "start": "3644990",
    "end": "3652099"
  },
  {
    "text": "It'll be the probability\nraised to the 1 or 0, which is the true value y, and\nthis is the continuous version.",
    "start": "3652100",
    "end": "3658099"
  },
  {
    "text": "So great. y is a Bernoulli. We can use that as our\nlikelihood function. If we have many data points\nin our training sample,",
    "start": "3658100",
    "end": "3664980"
  },
  {
    "text": "we're just going to do the\nproduct of those many times. So you get a whole bunch\nof the outputs of my i-th--",
    "start": "3664980",
    "end": "3671397"
  },
  {
    "text": "the output of my\nlogistic regression-- or my deep learning for\nthe i-th training example raised to its label.",
    "start": "3671397",
    "end": "3677540"
  },
  {
    "text": "Now this is going to be\nnumerically unstable. So instead of maximizing\nthe likelihood, we're going to maximize\nthe log likelihood.",
    "start": "3677540",
    "end": "3683540"
  },
  {
    "text": "Gets the same\nanswer, but is going to be much easier for\nour computer friends. So instead of doing\nthis likelihood,",
    "start": "3683540",
    "end": "3688932"
  },
  {
    "text": "we're going to\ntake the log of it. And we take the log, you\nget the exact same equation from MLE of logistic regression.",
    "start": "3688933",
    "end": "3696320"
  },
  {
    "text": "This is exactly character\nfor character the same. If you have n data points,\nit has this inner sum",
    "start": "3696320",
    "end": "3702960"
  },
  {
    "text": "for each of those data points. Amazing. So this is the same as\nlogistic regression.",
    "start": "3702960",
    "end": "3710320"
  },
  {
    "text": "This is the same as\nlogistic regression. Finally, though,\nall we have to do is derive this with respect to\nevery single one of our thetas,",
    "start": "3710320",
    "end": "3717430"
  },
  {
    "text": "all the thetas here,\nall the thetas here. So we have 800\nhere and 20 there.",
    "start": "3717430",
    "end": "3722440"
  },
  {
    "text": "We're going to do\nthat derivative. Now our goal is the\nderivative with respect",
    "start": "3722440",
    "end": "3729130"
  },
  {
    "text": "to all of these\noutput parameters and the derivative with respect\nto all these hidden parameters. If you can calculate all of\nthose partial derivatives",
    "start": "3729130",
    "end": "3735148"
  },
  {
    "text": "as a function, you are done. Here is a bad approach.",
    "start": "3735148",
    "end": "3741160"
  },
  {
    "text": "Let's say you want to\ncalculate the derivative. You're like, I've got\nthis scoring function, and I want to get the\nderivative with respect",
    "start": "3741160",
    "end": "3747155"
  },
  {
    "text": "to, say, a theta here. A bad approach would\nbe you could say, oh, y hat is actually this.",
    "start": "3747155",
    "end": "3753070"
  },
  {
    "text": "This is the equation for\nhow y hat's calculated. So I can just substitute\nthat into my equation.",
    "start": "3753070",
    "end": "3759530"
  },
  {
    "text": "And so instead of having\nthis log likelihood here, I could put that\ninto my equation,",
    "start": "3759530",
    "end": "3764640"
  },
  {
    "text": "then I can keep\nchaining these as I go. That would be correct, but\nwill lead to a math bug.",
    "start": "3764640",
    "end": "3771099"
  },
  {
    "text": "Instead, what I'd\nlike to show you guys is derivatives\nwithout tears. How can we derivatives\nwithout tears?",
    "start": "3771100",
    "end": "3776290"
  },
  {
    "text": "Because this is going\nto be important, because deep networks often\ndon't just have one layer, they have multiple layers.",
    "start": "3776290",
    "end": "3781460"
  },
  {
    "text": "So we need to know how to do\nthis calculus without making us want to pull out our hairs. So no-tears calculus.",
    "start": "3781460",
    "end": "3788630"
  },
  {
    "text": "And it comes down\nto the chain rule. And Mr. Blanton was my\nhigh school math teacher and he taught me the chain rule.",
    "start": "3788630",
    "end": "3794600"
  },
  {
    "text": "And I told him, I don't think\nit was going to be useful. And he was right. It was, in fact,\nuseful, and it now governs most of\nartificial intelligence.",
    "start": "3794600",
    "end": "3800930"
  },
  {
    "text": "Chain rule says, if\nyou have a function, you can decompose it using\nsome intermediate step.",
    "start": "3800930",
    "end": "3806180"
  },
  {
    "text": "I find this a\nlittle bit abstract, but I find this really\neasy to understand. You want the derivative of\nlog likelihood with respect",
    "start": "3806180",
    "end": "3812660"
  },
  {
    "text": "to a theta. There is an intermediate result,\nwhich is the probability.",
    "start": "3812660",
    "end": "3817963"
  },
  {
    "text": "So get the derivative of\nlog likelihood with respect to that probability and\nthen you can multiply it by the derivative of that\nprobability with respect",
    "start": "3817963",
    "end": "3824390"
  },
  {
    "text": "to the thing that went towards\ncalculating the probability. You can decompose each\nstep in your calculation",
    "start": "3824390",
    "end": "3830060"
  },
  {
    "text": "of log likelihood and get the\nderivative piece by piece. Also, don't forget.",
    "start": "3830060",
    "end": "3835460"
  },
  {
    "text": "Sigmoid, we love it because it's\ngot this beautiful derivative. And another thing not to\nforget, the derivative of a sum",
    "start": "3835460",
    "end": "3843350"
  },
  {
    "text": "is the sum of the derivative. So often from now\non, I'll just be thinking about a\nlog likelihood that",
    "start": "3843350",
    "end": "3848450"
  },
  {
    "text": "doesn't have the\nsum, because you can do the derivative of this\nlog likelihood without the sum. Then you can just\nput a sum outside",
    "start": "3848450",
    "end": "3854575"
  },
  {
    "text": "and it will be the same.  In Monday's class when we\ndid logistic regression,",
    "start": "3854575",
    "end": "3862760"
  },
  {
    "text": "we had this practice thing. We said, what is the\nderivative of this expression,",
    "start": "3862760",
    "end": "3868820"
  },
  {
    "text": "sigmoid of theta\ntranspose x, where we know that sigmoid has\nthis beautiful slope?",
    "start": "3868820",
    "end": "3875350"
  },
  {
    "text": "We used the chain rule. We said, OK, take\nthat input, call it z. And then we'll get the\nderivative of sigmoid of z",
    "start": "3875350",
    "end": "3884140"
  },
  {
    "text": "with respect to z and multiply\nthat by the derivative of z with respect to each theta ij. And when we did all\nthat and plug and chug,",
    "start": "3884140",
    "end": "3890530"
  },
  {
    "text": "we ended up with this\nnice little equation. You can review Monday, just\na little bit of reminder",
    "start": "3890530",
    "end": "3897820"
  },
  {
    "text": "of what we have done. So let's do it,\nthis is Stanford.",
    "start": "3897820",
    "end": "3903200"
  },
  {
    "text": "We can do something\nbrave, derivative goals. Let's start out with\ngetting this derivative.",
    "start": "3903200",
    "end": "3908539"
  },
  {
    "text": "I want the derivative of this\nscoring function for one data point with respect\nto a parameter",
    "start": "3908540",
    "end": "3916190"
  },
  {
    "text": "here in the output layer. ",
    "start": "3916190",
    "end": "3921720"
  },
  {
    "text": "First, we have to decompose it. We want to use the chain rule\nto say, OK, there's a theta here and there's an output here.",
    "start": "3921720",
    "end": "3927760"
  },
  {
    "text": "But there's this\nintermediate computation, which is the computation\nof the probability. And let's split this up.",
    "start": "3927760",
    "end": "3933540"
  },
  {
    "text": "We say, the log likelihood\ncomes from this probability, and this probability\ncomes from this theta.",
    "start": "3933540",
    "end": "3940380"
  },
  {
    "text": "And we can do derivatives in\ntwo steps using the chain rule. We can find the derivative\nwith respect to y hat,",
    "start": "3940380",
    "end": "3946470"
  },
  {
    "text": "multiply that by the derivative\nof y hat with respect to this particular parameter. And that's fantastic.",
    "start": "3946470",
    "end": "3952170"
  },
  {
    "text": "What if we wanted to then get\nthe derivative with respect to one of these inner thetas? And this might be the\nmost important slide",
    "start": "3952170",
    "end": "3957640"
  },
  {
    "text": "of today's because\nthis is to show you how chain rule can make it easy\nto do calculations derivatives",
    "start": "3957640",
    "end": "3963059"
  },
  {
    "text": "when you go deeper and deeper. So now we have a scoring\nfunction over here,",
    "start": "3963060",
    "end": "3968310"
  },
  {
    "text": "and it is impacted by\na parameter over here. How can we figure\nout, if you were",
    "start": "3968310",
    "end": "3973410"
  },
  {
    "text": "to change a parameter here, how\nmuch the scoring function would change, AKA the derivative? Again, you could try and\nwrite the scoring function",
    "start": "3973410",
    "end": "3980670"
  },
  {
    "text": "in an equation that\nhas this parameter and then do the straight\nmath, but that would not work very well. Instead, you should\nuse the chain rule.",
    "start": "3980670",
    "end": "3987270"
  },
  {
    "text": "And to do the chain\nrule, you should recognize that this leads\nto these calculations. These calculations leads\nto this calculation,",
    "start": "3987270",
    "end": "3993700"
  },
  {
    "text": "and this calculation leads\nto your log likelihood. And the chain rule can allow you\nto do the derivatives of each of those steps on their own.",
    "start": "3993700",
    "end": "3999819"
  },
  {
    "text": "So the derivative\nwith respect the thing will become the derivative of\nlog likelihood with respect",
    "start": "3999820",
    "end": "4005690"
  },
  {
    "text": "to your output probability,\nthe derivative of your output probability with respect\nto your hidden activation,",
    "start": "4005690",
    "end": "4011567"
  },
  {
    "text": "and the derivative of\nyour hidden activation with respect to each of these. You can just chain each\nof these layers together.",
    "start": "4011567",
    "end": "4018140"
  },
  {
    "text": "And you don't have to do\none composed derivative. You can do each\nderivative on its own.",
    "start": "4018140",
    "end": "4024330"
  },
  {
    "text": "Decomposition. What a good time. So if we want to get that\nderivative of log likelihood",
    "start": "4024330",
    "end": "4031218"
  },
  {
    "text": "with respect to the\noutput parameter, we'd first need this, the\nderivative of log likelihood with respect to P\nor y hat, rather,",
    "start": "4031218",
    "end": "4039060"
  },
  {
    "text": "the probability that comes out\nof our deep learning algorithm. We know log likelihood\nis just going",
    "start": "4039060",
    "end": "4044400"
  },
  {
    "text": "to be this Bernoulli probability\nmass function logged. And when we do the derivative\nof this with respect to y hat,",
    "start": "4044400",
    "end": "4051090"
  },
  {
    "text": "it's really not that bad. You have this term. What's its derivative\nwith respect to y hat?",
    "start": "4051090",
    "end": "4056610"
  },
  {
    "text": "Well, that looks\nlike a constant. And derivative of\nlog of your variable is just going to be\none over your variable.",
    "start": "4056610",
    "end": "4062280"
  },
  {
    "text": "Something similar happens\non this right-hand side. We are having a good time. ",
    "start": "4062280",
    "end": "4068610"
  },
  {
    "text": "You could simplify\nthis if you wanted to and you end up with\na really nice expression for that first term.",
    "start": "4068610",
    "end": "4073880"
  },
  {
    "text": "OK, not so bad. That first term? Check, done. How about this\nderivative, the derivative",
    "start": "4073880",
    "end": "4080869"
  },
  {
    "text": "of the output of your neural\nnetwork with respect to one of the hidden parameters, yi.",
    "start": "4080870",
    "end": "4087260"
  },
  {
    "text": "Don't forget the output\nis defined in terms of this logistic regression.",
    "start": "4087260",
    "end": "4092390"
  },
  {
    "text": "You can think of\nthis as sigmoid of z, where z is the inner part\nthat goes into the sigmoid.",
    "start": "4092390",
    "end": "4098899"
  },
  {
    "text": "And just like we did\nbefore, you can say, OK, this derivative of y hat\nwith respect to that parameter",
    "start": "4098899",
    "end": "4105620"
  },
  {
    "text": "is going to be this\nderivative over there.",
    "start": "4105620",
    "end": "4110870"
  },
  {
    "text": " Oh, right, right, right.",
    "start": "4110870",
    "end": "4116790"
  },
  {
    "text": "This is the sigmoid. So the sigmoid's derivative is\njust the sigmoid itself times 1 minus the sigmoid\ntimes the derivative",
    "start": "4116790",
    "end": "4124229"
  },
  {
    "text": "of y hat with respect to h of\nthe xi's, and you get this. But this is the\nsame as what we had for maximum\nlikelihood estimation",
    "start": "4124229",
    "end": "4131339"
  },
  {
    "text": "of logistic regression. This is a very similar formula. You have the output of your\nlogistic regression times 1",
    "start": "4131340",
    "end": "4138120"
  },
  {
    "text": "minus the output of your\nlogistic regression times by what used to be\nxi but is now hi.",
    "start": "4138120",
    "end": "4143640"
  },
  {
    "text": "OK, not so bad. And not too scary. And this really\ncomes from the fact",
    "start": "4143640",
    "end": "4149220"
  },
  {
    "text": "that sigmoid has a nice slope. This is the derivative\nof a sigmoid.",
    "start": "4149220",
    "end": "4154463"
  },
  {
    "text": "And so that's just going\nto be sigmoid times 1 minus a sigmoid times the\nderivative of the inside",
    "start": "4154463",
    "end": "4161540"
  },
  {
    "text": "with respect to the\nparameter you cared about. And we get with\nsomething not so bad.",
    "start": "4161540",
    "end": "4166790"
  },
  {
    "text": "At this point, we figured\nout of log likelihood with respect to y hat,\nand we figured out",
    "start": "4166790",
    "end": "4172939"
  },
  {
    "text": "the derivative of y hat\nwith respect to this, and we just multiply\nthose two things together. And you would get the\nderivative of log likelihood",
    "start": "4172939",
    "end": "4179000"
  },
  {
    "text": "with respect to one of\nthese output parameters. And we're almost done. We've now figured out the\nderivative with respect",
    "start": "4179000",
    "end": "4185318"
  },
  {
    "text": "to one of these\noutput parameters, now let's talk about the\nderivative with respect to one of these\ninput parameters. You don't need to\nmemorize this, but I want to give you the flavor\nfor how chain rule is allowing",
    "start": "4185319",
    "end": "4192670"
  },
  {
    "text": "us to get bigger and bigger\nnetworks without the math becoming insane. The way we can get this\nderivative is just chain rule.",
    "start": "4192670",
    "end": "4199300"
  },
  {
    "text": "Get the derivative of this\ncalculation, with respect to-- times it by the derivative\nof this calculation and times it by the derivative\nof this calculation.",
    "start": "4199300",
    "end": "4206000"
  },
  {
    "text": "So you have each of\nthese parts on their own. This derivative,\nwe've already done. We already know the derivative\nof log likelihood with respect",
    "start": "4206000",
    "end": "4212382"
  },
  {
    "text": "to y hat. We haven't done this derivative. What's the derivative of y hat\nwith respect to one of the h's? OK, you'd have to derive this\nwith respect to one of the h's.",
    "start": "4212382",
    "end": "4220060"
  },
  {
    "text": "Luckily, this is the\nderivative of a sigmoid. And the sigmoid has\nthis nice derivative where it's just going to\nbe the sigmoid times 1",
    "start": "4220060",
    "end": "4226630"
  },
  {
    "text": "minus sigmoid times\nthe derivative of the inside with respect\nto what you cared about. What's the derivative of this\ninside with respect to hj?",
    "start": "4226630",
    "end": "4234070"
  },
  {
    "text": "Well, there's only one thing\nthat's multiplied to hj, and that's theta j. So this derivative, not so bad.",
    "start": "4234070",
    "end": "4240730"
  },
  {
    "text": "Hey, is it over? Almost. We've got this derivative. We've got the derivative of\ny hat with respect to hj.",
    "start": "4240730",
    "end": "4247640"
  },
  {
    "text": "And now we just need the\nderivative with respect to one of the parameters. What's hj? Here you go.",
    "start": "4247640",
    "end": "4254098"
  },
  {
    "text": "We need the derivative\nof this with respect to one of those thetas. Now this is a little bit\nnice because you're doing",
    "start": "4254098",
    "end": "4261017"
  },
  {
    "text": "the derivative of sigmoid. What's the derivative\nof the sigmoid? It's just going to be the\nsigmoid itself times 1",
    "start": "4261017",
    "end": "4267200"
  },
  {
    "text": "minus the sigmoid times the\nderivative of the inner part with respect to\nwhat we cared about. In this case, a theta ij.",
    "start": "4267200",
    "end": "4274280"
  },
  {
    "text": "Now there's only one theta ij-- or there's only\none number that's multiplied with theta ij.",
    "start": "4274280",
    "end": "4280039"
  },
  {
    "text": "So that's when k equals i. And when k equals\ni, x also equals i. So there's only one number\nthat's multiplied by theta ij.",
    "start": "4280040",
    "end": "4286280"
  },
  {
    "text": "So when you take the\nderivative of this with respect to theta ij. We just get an xi.",
    "start": "4286280",
    "end": "4292160"
  },
  {
    "text": "And we're done. And you just multiply each\nof these values together.",
    "start": "4292160",
    "end": "4297680"
  },
  {
    "text": "Your computer can\ncalculate this term, and multiply it with this term,\nand multiply with this term, and that will be the number\nthat is your derivative.",
    "start": "4297680",
    "end": "4305429"
  },
  {
    "text": "And that's all. Wait a second. You now know how to\nbuild a neural network.",
    "start": "4305430",
    "end": "4313400"
  },
  {
    "text": "And you now know\nthe training just requires you to get these\npartial derivatives. And chain rule will allow you\nto get partial derivatives,",
    "start": "4313400",
    "end": "4320480"
  },
  {
    "text": "no matter how big\nyour network ends up. You guys could both\nbuild a neural network, and if you really needed\nto, you could sit down,",
    "start": "4320480",
    "end": "4327080"
  },
  {
    "text": "you could implement how\nwe could do the training. The training would just\nuse gradient ascent. You'd have 820\nparameters and you now",
    "start": "4327080",
    "end": "4335390"
  },
  {
    "text": "have equations\nthat you could use to calculate the derivatives\nof each of those 820 parameters at any one point.",
    "start": "4335390",
    "end": "4341420"
  },
  {
    "text": "And that's it. You guys got it. Now of course, I said the\npositioning for today's lecture",
    "start": "4341420",
    "end": "4348906"
  },
  {
    "text": "should just be high level. I want you guys to see\nthe details so that you can appreciate the big picture. This isn't magic.",
    "start": "4348907",
    "end": "4355270"
  },
  {
    "text": "Chain rule really is the answer\nto where all this deep learning intelligence is coming from.",
    "start": "4355270",
    "end": "4360430"
  },
  {
    "text": "What a powerful\ntool, and it allows us to do intelligence with\nbigger and bigger neural networks. And so maybe we should just\nhave a moment of silence",
    "start": "4360430",
    "end": "4367390"
  },
  {
    "text": "to appreciate\nthose simple facts. Demystified deep learning OK. ",
    "start": "4367390",
    "end": "4374840"
  },
  {
    "text": "Fantastic. So just to be clear,\nwhat would you do if we had multiple\nhidden layers?",
    "start": "4374840",
    "end": "4381320"
  },
  {
    "text": "You would just do\nmore chain rule. If you wanted to have the\nderivative with respect to these thetas,\nthen you can have the derivative of loss\nwith respect to y,",
    "start": "4381320",
    "end": "4388489"
  },
  {
    "text": "respect of y with respect\nto h2, the derivative of h2 with respect to\nh1, and derivative of h1",
    "start": "4388490",
    "end": "4393530"
  },
  {
    "text": "with respect to the thetas. You could just keep\nadding chain rules to go deeper and deeper\nin your neural networks.",
    "start": "4393530",
    "end": "4402170"
  },
  {
    "text": "Now it's time to do a little\nbit of fun exploration. These neural networks, there's\nthis great little demo site",
    "start": "4402170",
    "end": "4409380"
  },
  {
    "text": "that I've got here, and\nit shows neural networks trying to learn more\ncomplicated functions than logistic\nregression could learn.",
    "start": "4409380",
    "end": "4415460"
  },
  {
    "text": "Someone asked what's the\ntheory behind neural networks and people would come\nup with these things where you have every point has\nan x1, x2 coordinate, and then",
    "start": "4415460",
    "end": "4423920"
  },
  {
    "text": "a label, either red\nfor 0 or green for 1. And a neural network\ncan learn a circle.",
    "start": "4423920",
    "end": "4429930"
  },
  {
    "text": "So if all of your 1's are\nin the circle of this grid and all your x's\nare in the outside, a neural network could\nlearn the circle.",
    "start": "4429930",
    "end": "4436380"
  },
  {
    "text": "Spatially, a neural network\ncould learn a spiral. People got deeper and\ndeeper into asking what",
    "start": "4436380",
    "end": "4441720"
  },
  {
    "text": "can a neural network learn? And now we feel that a\nneural network could learn",
    "start": "4441720",
    "end": "4446970"
  },
  {
    "text": "any function of any complexity. There is no function that's\nnot complicated enough that if I got infinite neural\nnetworks, hey, come on.",
    "start": "4446970",
    "end": "4453840"
  },
  {
    "text": "Learn. Sometimes they get stuck. But you can imagine my step\nsize is a little small here.",
    "start": "4453840",
    "end": "4460350"
  },
  {
    "text": "Oh, yeah, I got\nover the plateau. Keep going. Yes! Yes! Yes! Train, neural network, train.",
    "start": "4460350",
    "end": "4466320"
  },
  {
    "text": "Do your chain rule. OK, now my neural network is\nable to make good predictions. But people got deeper and\ndeeper into this, and as I said,",
    "start": "4466320",
    "end": "4473800"
  },
  {
    "text": "we now think that if you\nput enough neurons together, there is no function\nthat you couldn't learn if you were able to set\nall the parameters perfectly.",
    "start": "4473800",
    "end": "4483449"
  },
  {
    "text": "There's a couple things that\nI want to add as extra ideas. One of the extra ideas\nis we've been talking",
    "start": "4483450",
    "end": "4489690"
  },
  {
    "text": "about binary predictions,\nbut obviously, that hand-drawing\ndigit 1 wanted us to make not just\nbinary predictions,",
    "start": "4489690",
    "end": "4494850"
  },
  {
    "text": "but they want to make\nthose predictions that were more multinomial, so like\nyour prediction of 0, 1, 2, 3,",
    "start": "4494850",
    "end": "4500205"
  },
  {
    "text": "4, 5, 6, 7, 8, 9. We aren't going to push\nyou in that direction yet, but if you wanted to,\ninstead of using a sigmoid,",
    "start": "4500205",
    "end": "4505679"
  },
  {
    "text": "you'd use a version of\nsigmoid called softmax, and then your loss function\nwould be the probability mass",
    "start": "4505680",
    "end": "4511020"
  },
  {
    "text": "function of a multinomial,\nnot the probability mass function of a Bernoulli.",
    "start": "4511020",
    "end": "4516390"
  },
  {
    "text": "We saw in the hand-drawn\ndigit that not all layers were fully connected. At the end, we had these\nfully connected ones.",
    "start": "4516390",
    "end": "4522660"
  },
  {
    "text": "But earlier on, we\nhad these boxes, and those boxes were\nactually sharing weights. And sharing weights\nis a great idea",
    "start": "4522660",
    "end": "4528630"
  },
  {
    "text": "that people call convolution. And it turns out if\nyou want to do images, having convolution in the first\ncouple of layers just makes",
    "start": "4528630",
    "end": "4535410"
  },
  {
    "text": "for some nice, well-trainable\nneural networks.",
    "start": "4535410",
    "end": "4540870"
  },
  {
    "text": "Here's a crazy thing. People, when they're\nfirst starting to understand neural\nnetworks-- somebody",
    "start": "4540870",
    "end": "4547420"
  },
  {
    "text": "made this crazy neural network\nwhere you would put in a face and it would predict\nwhose face it was. And they started to put in lots\nand lots of faces into this.",
    "start": "4547420",
    "end": "4554690"
  },
  {
    "text": "And when they did, this the\ncraziest thing happened. They looked at what was\nlearned in the first layer,",
    "start": "4554690",
    "end": "4560567"
  },
  {
    "text": "and they looked at what was\nlearned in the middle layer, and they looked at what was\nlearned in the later layers, and they saw\nsomething incredible.",
    "start": "4560567",
    "end": "4567580"
  },
  {
    "text": "When they looked at what each\nneuron in the first layer was doing, they learned\nto do these things that looked like edge detectors.",
    "start": "4567580",
    "end": "4574300"
  },
  {
    "text": "And when they looked at what was\nhappening in the middle layer, these neurons would\nmost activated",
    "start": "4574300",
    "end": "4579752"
  },
  {
    "text": "when they saw things that\nlooked like parts of faces. One neuron would be\nlooking for a nose, one neuron would be\nlooking for an eye.",
    "start": "4579752",
    "end": "4585170"
  },
  {
    "text": "And when they looked\nat later neurons, they would be looking\nfor ghost faces. Each one of these neurons\nwould be most activated",
    "start": "4585170",
    "end": "4591770"
  },
  {
    "text": "by a face that looked\nlike this picture. You know what's\ninsane about that? If people look at human brains,\nwhen they look at human brains,",
    "start": "4591770",
    "end": "4599750"
  },
  {
    "text": "the first part of your visual\ncortex is called the V1 cortex. And guess what it looks like. It looks just like that.",
    "start": "4599750",
    "end": "4605780"
  },
  {
    "text": "The first part of our\nbrain does edge detection. If you look deeper into our\nbrain, the next chunk of brain",
    "start": "4605780",
    "end": "4611330"
  },
  {
    "text": "does things like finding parts. And then eventually, we get\nto higher-level concepts, like who are we seeing in this?",
    "start": "4611330",
    "end": "4617180"
  },
  {
    "text": "So a neural network with the\nmechanism I just told you, when it was trained\non data without being told how a human brain\nworks, replicated",
    "start": "4617180",
    "end": "4624200"
  },
  {
    "text": "some of these key properties. Insane. And at that point,\npeople were like, whoa. Turing awards for\nall of you guys.",
    "start": "4624200",
    "end": "4631730"
  },
  {
    "text": "Obviously, neural networks have\nmany more than 820 neurons. In fact, there's this\nGoogle Brain network which",
    "start": "4631730",
    "end": "4639030"
  },
  {
    "text": "does visual recognition, and\nthis is certainly outdated, but a few years ago it\nhad a trillion neurons.",
    "start": "4639030",
    "end": "4645512"
  },
  {
    "text": "And when you looked at\nthose trillion neurons, they'd be 22 layers deep,\nand somebody architected each",
    "start": "4645513",
    "end": "4650880"
  },
  {
    "text": "of these layers painstakingly. At the very, very, very,\nvery, very, very end, it's still producing a\nprobability of different class",
    "start": "4650880",
    "end": "4657990"
  },
  {
    "text": "labels. They trained it on a\nwhole bunch of things. They also trained it on like\nall frames from YouTube videos.",
    "start": "4657990",
    "end": "4664660"
  },
  {
    "text": "So it got super into cats,\nwhich is just hilarious. Anyways, if you looked\nat the different neurons",
    "start": "4664660",
    "end": "4671340"
  },
  {
    "text": "that this trillion-parameter\nneural network learned, they had semantic meanings. You could look at the best\nstimulus and you'd be like,",
    "start": "4671340",
    "end": "4676930"
  },
  {
    "text": "OK, here's a neuron-- and\nthis is really looking for things that are diagonal. But here's a neuron\nthat's looking for these circular grids.",
    "start": "4676930",
    "end": "4682949"
  },
  {
    "text": "But later on, you'd have\nthere's a duck neuron, there's the flower\nneuron, there's the knife neuron,\nstuff like that.",
    "start": "4682950",
    "end": "4690030"
  },
  {
    "text": "There's this really hard test. It says, I'm going to give you\nan image from 22,000 categories",
    "start": "4690030",
    "end": "4696287"
  },
  {
    "text": "and you have to figure out\nwhich category it's in. It was created by\nour own Fei-Fei Li. And these categories\nare pretty specific.",
    "start": "4696287",
    "end": "4701850"
  },
  {
    "text": "You could get a\npicture of a Stingray or you could get a\npicture of a manta ray and you'd have to be able\nto tell the difference",
    "start": "4701850",
    "end": "4707660"
  },
  {
    "text": "between those two. If you were to random guess,\nyou would do awful at this task. There's 22,000 classes.",
    "start": "4707660",
    "end": "4713850"
  },
  {
    "text": "Before neural\nnetworks, people spent a lot of time trying to\nget really good at this, and they got to\nabout 1.5% accuracy.",
    "start": "4713850",
    "end": "4719400"
  },
  {
    "text": "And people got papers\noff of 1.5% accuracy. And at some point,\npeople started using neural networks for this.",
    "start": "4719400",
    "end": "4725100"
  },
  {
    "text": "And they started to get\ncloser to like 44% accuracy. Humans can get close\nto 93% accuracy",
    "start": "4725100",
    "end": "4731550"
  },
  {
    "text": "and the most up-to-date neural\nnetworks can outperform humans on this task. And we know this.",
    "start": "4731550",
    "end": "4738960"
  },
  {
    "text": "How many parameters is too many? It turns out if you\nhave enough data and you have enough\ncomputer power, the scary thing\nis it doesn't seem",
    "start": "4738960",
    "end": "4745650"
  },
  {
    "text": "like there is a limit here. We're in this really\nweird world where if you throw more compute\npower that can do more gradient",
    "start": "4745650",
    "end": "4752220"
  },
  {
    "text": "descent, and you\nhave more parameters and you have more data, there\ndoesn't seem to be a ceiling. Their models just seem to\nget smarter and smarter,",
    "start": "4752220",
    "end": "4758010"
  },
  {
    "text": "and that's why there's such\na weird arms race going on between Google, and\nChatGPT 3, and all these folks to make the best neural network.",
    "start": "4758010",
    "end": "4765179"
  },
  {
    "text": "I'm going to talk about\nthis more on Friday and I'll bring this up, too.",
    "start": "4765180",
    "end": "4771030"
  },
  {
    "text": "I do want to lightly note\nthat not everything is classification. And I would like to\nleave on this note",
    "start": "4771030",
    "end": "4776190"
  },
  {
    "text": "of this little critter\nwe had training from the beginning of class. Do you guys want to go\ncheck in on our critter?",
    "start": "4776190",
    "end": "4781755"
  },
  {
    "text": "Aw, there's our critter. Here's our creator. And this critter has\nnow gotten pretty smart.",
    "start": "4781755",
    "end": "4788360"
  },
  {
    "text": "The critter has now figured\nout, OK, I want to go and I want to eat all the\nred dots without eating",
    "start": "4788360",
    "end": "4795020"
  },
  {
    "text": "all the green dots. How does this critter work? You can look at this\ncritter's neural network.",
    "start": "4795020",
    "end": "4801909"
  },
  {
    "text": "What's its inputs? It has all these lines\nand it knows a distance and a color of what it sees.",
    "start": "4801910",
    "end": "4807880"
  },
  {
    "text": "Based on those inputs, it then\nhas a whole neural network. But then it has a\nvery final output.",
    "start": "4807880",
    "end": "4813530"
  },
  {
    "text": "And that very final\noutput doesn't look like it's predicting\na 1 or 0, does it?",
    "start": "4813530",
    "end": "4819639"
  },
  {
    "text": "It's predicting\nan action to take. One of the actions is move left. One of the actions\nis move right.",
    "start": "4819640",
    "end": "4825400"
  },
  {
    "text": "One of the actions is\nmove really far left, and one of the action is\nmove really far right, and one of the actions\nis move forward.",
    "start": "4825400",
    "end": "4830810"
  },
  {
    "text": "And so at the very\nend, it's just predicting which of these\nactions it should take.",
    "start": "4830810",
    "end": "4836180"
  },
  {
    "text": "Its loss function, the\nthing that was optimizing, was based off of looking at\nits experience and saying,",
    "start": "4836180",
    "end": "4842810"
  },
  {
    "text": "was I getting good\nfood to eat or not? What a time to be alive. This is just the start.",
    "start": "4842810",
    "end": "4849600"
  },
  {
    "text": "We can talk a little bit more\nabout how far this has gone, but you guys have understood\nthat full picture.",
    "start": "4849600",
    "end": "4855620"
  },
  {
    "text": "This simple idea of putting\nlogistic regressions on top of each\nother, that's what's behind the algorithms\nthat can make decisions.",
    "start": "4855620",
    "end": "4862000"
  },
  {
    "text": "That's what's behind\nthese algorithms that can draw pictures. That's what's behind\nthe self-driving cars.",
    "start": "4862000",
    "end": "4867050"
  },
  {
    "text": "It all comes to logistic\nregressions put together, trains by chain rule. What a crazy world we live in.",
    "start": "4867050",
    "end": "4872480"
  },
  {
    "text": "Have a fantastic day. Come back on Friday. We'll talk about\nethics and fairness before our very\nfinal week of school.",
    "start": "4872480",
    "end": "4879380"
  },
  {
    "text": "Have a great day, CS 109. Oh, if you asked a question,\ncome get a mandarin up front.",
    "start": "4879380",
    "end": "4884469"
  },
  {
    "start": "4884470",
    "end": "4889000"
  }
]