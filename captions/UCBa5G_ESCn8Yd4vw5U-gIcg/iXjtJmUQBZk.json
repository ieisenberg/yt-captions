[
  {
    "start": "0",
    "end": "5490"
  },
  {
    "text": "Welcome to CS224N\nToday, I'm really excited that we're getting to\nhave the first of our guest",
    "start": "5490",
    "end": "13260"
  },
  {
    "text": "lectures. And so tonight,\nit's not tonight. Today, it's really great\nto have Maarten Sap.",
    "start": "13260",
    "end": "20730"
  },
  {
    "text": "So Maarten is currently a\nyoung investigator at the Allen Institute for AI in Seattle.",
    "start": "20730",
    "end": "26580"
  },
  {
    "text": "But pretty soon,\nnow, I guess he's going to be starting\nat Carnegie Mellon University as a professor.",
    "start": "26580",
    "end": "33960"
  },
  {
    "text": "So Maarten has\ndone a huge amount of really exciting and\ninteresting work looking",
    "start": "33960",
    "end": "39030"
  },
  {
    "text": "at socially intelligent\nNLP systems. And in particular, he's had\na really strong emphasis",
    "start": "39030",
    "end": "45390"
  },
  {
    "text": "at looking at issues of\nsocial inequality, bias, and toxicity in language.",
    "start": "45390",
    "end": "51510"
  },
  {
    "text": "And that work's\nbeen widely noticed. And he's had it covered in\nThe New York Times, Forbes,",
    "start": "51510",
    "end": "56760"
  },
  {
    "text": "Fortune, et cetera. So it'll be a great opportunity\ntoday to hear about some",
    "start": "56760",
    "end": "63750"
  },
  {
    "text": "of that interesting\nwork that he has done. OK, so notes, questions--",
    "start": "63750",
    "end": "69060"
  },
  {
    "text": "Maarten would be delighted\nto have live questions. So if you'd like to\nask a live question,",
    "start": "69060",
    "end": "75660"
  },
  {
    "text": "you can either hit\nthe Raise Hand thing, or you could put in the Q&A\n\"live Q\" or some shorthand",
    "start": "75660",
    "end": "81990"
  },
  {
    "text": "like that. And then we can make it so\nyou can ask live questions.",
    "start": "81990",
    "end": "87900"
  },
  {
    "text": "If you're feeling very\nshy, you can still put a question in the\nquestion and answer that we can relay for you. But live questions\nare much preferred.",
    "start": "87900",
    "end": "96678"
  },
  {
    "text": "I think that's the\nonly instructions. And otherwise, take\nit away, Maarten. Thanks. I'll just say that\nthere might be",
    "start": "96678",
    "end": "102780"
  },
  {
    "text": "a chance that I don't\nsee the raising hands or questions like that. So if any of the\nsort of coordinators",
    "start": "102780",
    "end": "108210"
  },
  {
    "text": "can let me know if there's\nsomething that's pressing, I can just totally\nanswer it as well.",
    "start": "108210",
    "end": "116610"
  },
  {
    "text": "Yeah, yeah, we'll do that. We'll stop you for questions. Awesome. Yeah, all right.",
    "start": "116610",
    "end": "121920"
  },
  {
    "text": "So thank you so much for\nhaving me here today. I'm really excited to talk\nabout some of the work",
    "start": "121920",
    "end": "127260"
  },
  {
    "text": "that I've done doing my PhD and\nthat I'm currently still doing. And today's talk is going\nto be focusing specifically",
    "start": "127260",
    "end": "134243"
  },
  {
    "text": "on the work that\nI've done related to detecting and rewriting\nsocially biased language.",
    "start": "134243",
    "end": "139950"
  },
  {
    "text": "And so I want to\nstart with a quote from Rita Mae Brown, who was\na feminist author and LGBTQ",
    "start": "139950",
    "end": "146580"
  },
  {
    "text": "activist from the '60s,\nwho said that language is the roadmap of a\nculture, and it tells you",
    "start": "146580",
    "end": "153630"
  },
  {
    "text": "where its people come from\nand where they are going. And so this is a known thing\nthat many other linguists",
    "start": "153630",
    "end": "159612"
  },
  {
    "text": "and philosophers have discussed,\nthat language and culture really cannot be disentangled\nfrom each other that much.",
    "start": "159613",
    "end": "166620"
  },
  {
    "text": "And this is\nparticularly interesting when we think\nabout understanding how inequality or biases or\nhatred can manifest themselves",
    "start": "166620",
    "end": "173540"
  },
  {
    "text": "in language. And so I like to talk\nabout this in a framework that I call the cycle of\nsocial inequality in text.",
    "start": "173540",
    "end": "180230"
  },
  {
    "text": "And so what I mean by that\nis that we know that there is social inequality between\nminority and majority",
    "start": "180230",
    "end": "185269"
  },
  {
    "text": "groups in the world-- for example, between\nmen and women. And because of how\nour language works,",
    "start": "185270",
    "end": "193830"
  },
  {
    "text": "the world is going to influence\nthe way that our language patterns are\ndifferent with respect to these demographic groups.",
    "start": "193830",
    "end": "199902"
  },
  {
    "text": "And so language is\ninherently going to reflect existing\nsocial inequality. And this is going to,\nfor example, show up",
    "start": "199902",
    "end": "206760"
  },
  {
    "text": "in the portrayals of\nminority characters, who are known to be more biased\nor stereotypical, typically,",
    "start": "206760",
    "end": "212610"
  },
  {
    "text": "than majority characters. And this also shows up in the\nfact that, for example, hate speech and toxicity are mostly\ngoing to target minority groups",
    "start": "212610",
    "end": "220950"
  },
  {
    "text": "and not really majority groups. All right, I'm already\nseeing some discussion. I think we're good.",
    "start": "220950",
    "end": "226502"
  },
  {
    "text": " And then in turn, the language\nthat we use or we read",
    "start": "226502",
    "end": "234560"
  },
  {
    "text": "is actually influencing\nthe world itself. And so for example,\nhate speech has been shown to be able\nto worsen relationships",
    "start": "234560",
    "end": "241819"
  },
  {
    "text": "between demographic groups. And we also know that\nthe portrayal of minority characters can shape the\nperceptions and stereotypes",
    "start": "241820",
    "end": "249740"
  },
  {
    "text": "that we have of those\nminority identities. And so there's this sort\nof cycle that happens.",
    "start": "249740",
    "end": "255690"
  },
  {
    "text": "And so in this talk,\nI'm going to try to talk about how we can\nmake NLP systems understand and mitigate social biases\nand toxicity in language.",
    "start": "255690",
    "end": "263490"
  },
  {
    "text": "And one of the reasons-- the crux of the reason why\nthis is super important is because, if any\nhuman-generated data is going",
    "start": "263490",
    "end": "270740"
  },
  {
    "text": "to inherently reflect social\ndynamics and inequality, and if our NLP systems\nare going to be trained on this human-generated\ndata, they",
    "start": "270740",
    "end": "276957"
  },
  {
    "text": "need to account\nfor these dynamics because otherwise, it can\nhave really harmful results.",
    "start": "276957",
    "end": "282800"
  },
  {
    "text": "And so to drive this point\nhome a little bit more, let me walk you through\na couple NLP tasks.",
    "start": "282800",
    "end": "288900"
  },
  {
    "text": "The first big task\nthat we can think about is conversational AI. And this is a very big task\nthat people are tackling in NLP",
    "start": "288900",
    "end": "295580"
  },
  {
    "text": "these days, creating digital\nassistants, chatbots, things like that. And you may remember\nthe really awful example",
    "start": "295580",
    "end": "303140"
  },
  {
    "text": "of Microsoft's Tay, which was\nan AI chatbot that they released on Twitter, and it turned\nracist in less than a day.",
    "start": "303140",
    "end": "311007"
  },
  {
    "text": "And this is because\nthey didn't really account for any of the social\ndynamics that maybe goes on",
    "start": "311007",
    "end": "316025"
  },
  {
    "text": "on the internet or went into\nthe training data of this model. And their bot ended\nup having really rude and offensive\nconversations in this case.",
    "start": "316025",
    "end": "324680"
  },
  {
    "text": "Another field or\nsubfield of NLP that has gotten a lot of\nattention recently is language generation, which\nis similar to conversational AI",
    "start": "324680",
    "end": "332870"
  },
  {
    "text": "but just more about\nautocompleting text, or continuing stories\nor news articles.",
    "start": "332870",
    "end": "337880"
  },
  {
    "text": "And here, if you don't\naccount for the inequality or the social dynamics that\ngo into your training data,",
    "start": "337880",
    "end": "343699"
  },
  {
    "text": "then you can end up with really\nincoherent, mindless, but also really biased or\noffensive generations.",
    "start": "343700",
    "end": "349159"
  },
  {
    "text": "And some of my own work\nhas shown that, actually, GPT-3, which is OpenAI's\nsort of most powerful text",
    "start": "349160",
    "end": "355190"
  },
  {
    "text": "generator out there, really\ncan delve into toxicity really quickly.",
    "start": "355190",
    "end": "361120"
  },
  {
    "text": "And then finally,\nalso related to NLP, there's this whole field or\nsubfield of text understanding.",
    "start": "361120",
    "end": "366485"
  },
  {
    "text": "And particularly,\nI've worked on things like hate speech detection\nand sentiment analysis. And here, if you don't account\nfor the underlying dynamics",
    "start": "366485",
    "end": "374199"
  },
  {
    "text": "that went into your data\ncollection, your data creation, you can end up with really worse\nperformance on minority user",
    "start": "374200",
    "end": "379600"
  },
  {
    "text": "input, or even worse,\nactually biased behavior against the minority users. And so some of my\nown work has actually",
    "start": "379600",
    "end": "386200"
  },
  {
    "text": "shown that, for example, hate\nspeech detection systems tend to be racially biased. And I'll talk about\nthis in a minute.",
    "start": "386200",
    "end": "393190"
  },
  {
    "text": "And this is the deep\nlearning NLP class. So we know that in\nrecent years, there's",
    "start": "393190",
    "end": "398680"
  },
  {
    "text": "been a lot of improvements in\nNLP tasks in general thanks to these pretrained\nlanguage models.",
    "start": "398680",
    "end": "404080"
  },
  {
    "text": "And for the sake of\nthis talk, I just want to highlight some\nof the key parts of why these pretrained language\nmodels are working so well.",
    "start": "404080",
    "end": "412182"
  },
  {
    "text": "So we know that they're\nlarge neural nets that are trained on large\namounts of text to predict which\nword comes next.",
    "start": "412183",
    "end": "417442"
  },
  {
    "text": "And they're using\nthis thing called the Transformer architecture,\nwhich is a neural network.",
    "start": "417442",
    "end": "423198"
  },
  {
    "text": "And so their basic recipe is to\ngather a large amount of text data, take a transformer model,\nand then train that transformer",
    "start": "423198",
    "end": "430120"
  },
  {
    "text": "model to predict, basically,\na word given its context. And there's a trend\nof naming these models",
    "start": "430120",
    "end": "437560"
  },
  {
    "text": "after Sesame Street characters\nfor some reason in NLP. But we also got some\nsort of text generators",
    "start": "437560",
    "end": "445390"
  },
  {
    "text": "that were named GPT-2,\nGPT-3, things like that. And we know that these models,\nlike GPT-3, are getting",
    "start": "445390",
    "end": "451930"
  },
  {
    "text": "bigger and bigger every day. But also getting bigger\nis the training data, or the pretraining corpora\nthat are used to create",
    "start": "451930",
    "end": "459100"
  },
  {
    "text": "these language models. And we started out\nwith using, quote unquote, \"only\" documents\nfrom English Wikipedia,",
    "start": "459100",
    "end": "466750"
  },
  {
    "text": "or maybe just a\nsmall set of books. GPT-2 was trained on\na really large set",
    "start": "466750",
    "end": "472990"
  },
  {
    "text": "of documents that were basically\noutbound links from Reddit. And T5 and GPT-3 were trained\non the Common Crawl data",
    "start": "472990",
    "end": "481759"
  },
  {
    "text": "set, which is a\nreally large archive of basically all\ndocuments on the internet that people could find.",
    "start": "481760",
    "end": "488943"
  },
  {
    "text": "And here, I want to pause\na little bit for a second. And thinking about the\nfact that our models",
    "start": "488943",
    "end": "495009"
  },
  {
    "text": "are learning language from\nrandom internet data, what could go wrong with that?",
    "start": "495010",
    "end": "500530"
  },
  {
    "text": "So what could go wrong\nand does go wrong is actually that these\nTransformer language models are really mindless\nand socially oblivious.",
    "start": "500530",
    "end": "507460"
  },
  {
    "text": "And so not only\nare they learning stereotypes and social biases\nfrom their training data-- some of my own work\nhas shown that they're",
    "start": "507460",
    "end": "513579"
  },
  {
    "text": "at risk for generating toxicity\nreally, really quickly, in less than 100 generations. So if you sampled 100\nsentences from GPT-3,",
    "start": "513580",
    "end": "522219"
  },
  {
    "text": "you're likely to find\nsomething really toxic within those 100 samples. So that's really\nquick and a problem.",
    "start": "522220",
    "end": "528980"
  },
  {
    "text": "And so the crux of the issue\nis that, as Professor Ruha Benjamin puts it, feeding AI\nsystems on the world's beauty,",
    "start": "528980",
    "end": "534940"
  },
  {
    "text": "ugliness, and cruelty, but\nexpecting it to reflect only the beauty is a fantasy.",
    "start": "534940",
    "end": "541540"
  },
  {
    "text": "And what we need is formalisms\nto represent and detect this ugliness and cruelty\nor these social biases",
    "start": "541540",
    "end": "550630"
  },
  {
    "text": "and algorithms to\nmitigate and avoid these social biases and this\ntoxicity and this ugliness.",
    "start": "550630",
    "end": "556930"
  },
  {
    "text": "And so that's what I'll\nbe talking about today. And specifically, I'm going\nto talk about three projects",
    "start": "556930",
    "end": "562399"
  },
  {
    "text": "today. The first two are going to\nbe about detecting toxicity and social biases in language.",
    "start": "562400",
    "end": "567519"
  },
  {
    "text": "And then I'll talk about\nrewriting and debiasing texts with a model called\nPowerTransformer.",
    "start": "567520",
    "end": "573880"
  },
  {
    "text": "And then I'll talk about some\nexciting future directions that we can go in towards\nhuman-centric social bias",
    "start": "573880",
    "end": "579970"
  },
  {
    "text": "detection and mitigation. So I'll start with\nthis first work,",
    "start": "579970",
    "end": "585938"
  },
  {
    "text": "which is called \"The Risk of\nRacial Bias in Hate Speech Detection.\" And this work is really tackling\nthe phenomenon of hate speech",
    "start": "585938",
    "end": "595010"
  },
  {
    "text": "online. And we know that this is a\nreally rampant problem that",
    "start": "595010",
    "end": "600620"
  },
  {
    "text": "causes people to quit social\nmedia because there's just too much hate, and\nthey can't take it.",
    "start": "600620",
    "end": "606410"
  },
  {
    "text": "Transgender people are reporting\nbeing treated inhumanely. People of color are saying that\nthey can't subject themselves",
    "start": "606410",
    "end": "612980"
  },
  {
    "text": "any longer to the hate. People are quitting. And there's increasing\ncalls for these platforms",
    "start": "612980",
    "end": "621320"
  },
  {
    "text": "to address this issue\nof hate speech online. ",
    "start": "621320",
    "end": "626397"
  },
  {
    "text": "Oh, I'm sorry. I'm just looking at\nthe questions more, OK. ",
    "start": "626397",
    "end": "634011"
  },
  {
    "text": "And one of the issues\nwith this-- so obviously, you know that platforms\nare struggling to moderate this content.",
    "start": "634012",
    "end": "639640"
  },
  {
    "text": "And one of the issues\nis that it's really challenging to rely\non humans solely to moderate this content.",
    "start": "639640",
    "end": "646020"
  },
  {
    "text": "There's really way too many\ntweets, or way too many posts, that are posted on these\nplatforms for humans",
    "start": "646020",
    "end": "651360"
  },
  {
    "text": "to just be able to sift through. Apparently, according\nto this source, there's 500 million tweets\nthat are sent in one day.",
    "start": "651360",
    "end": "657672"
  },
  {
    "text": "So there's no way\nthat we could get humans to do this all alone. And taking a sort of a\ncommunity-centric perspective,",
    "start": "657672",
    "end": "665160"
  },
  {
    "text": "like what Reddit has done,\nwhere, basically, you delegate the moderation to the\nsubcommunities that are naturally created,\ncan lead to, actually,",
    "start": "665160",
    "end": "672240"
  },
  {
    "text": "hate-endorsing communities,\nlike, if you remember, the famous case where a Reddit\nCEO stepped in and basically",
    "start": "672240",
    "end": "679080"
  },
  {
    "text": "deleted or quarantined several\nsuper misogynistic subreddits. ",
    "start": "679080",
    "end": "686315"
  },
  {
    "text": "OK, I'm looking\nat the questions.  So someone asked, are\nyou more of a proponent",
    "start": "686315",
    "end": "693840"
  },
  {
    "text": "for we should only feed the\nmodel good data to reduce toxicity, or we should try\nto filter model output, modify what the model has\nlearned from the data?",
    "start": "693840",
    "end": "701010"
  },
  {
    "text": "That is a good question. I think we should do both\nbecause there's no way",
    "start": "701010",
    "end": "706800"
  },
  {
    "text": "to say that something\nis inherently good and is free of biases. ",
    "start": "706800",
    "end": "713953"
  },
  {
    "text": "And it's easier to-- I mean, we should be really\nmindful of the data, still. And the decisions on\nwhich data we choose",
    "start": "713953",
    "end": "720180"
  },
  {
    "text": "is something that I\nthink most people tend to take just lightly and just\ntake whatever is available.",
    "start": "720180",
    "end": "725648"
  },
  {
    "text": "And we should be thinking\na little bit more about whose data it is. Are we ethically\nusing this data?",
    "start": "725648",
    "end": "730725"
  },
  {
    "text": "Was this the purpose of it? All this stuff. But at the same time, there's\nthis issue, fundamentally,",
    "start": "730725",
    "end": "737310"
  },
  {
    "text": "with machine learning and AI. And that is that we are trying\nto predict the future based",
    "start": "737310",
    "end": "742560"
  },
  {
    "text": "on data from the past. So even if we might not consider\nsomething biased or problematic",
    "start": "742560",
    "end": "748290"
  },
  {
    "text": "right now, maybe\nin a year or two, there will be some\nnew evidence that some of the stuff that we trained\nstuff on was problematic.",
    "start": "748290",
    "end": "754330"
  },
  {
    "text": "So I think having\na stop or a way to mitigate these\nat decoding time or at prediction time is also\nsomething that we should do.",
    "start": "754330",
    "end": "763062"
  },
  {
    "text": "And I'm actually really\nexcited about that direction, too, especially as we're\nin a new era of not being",
    "start": "763062",
    "end": "769110"
  },
  {
    "text": "able to train our models with\nthese pretrained language models being so big. So yeah, I'm excited about that.",
    "start": "769110",
    "end": "776589"
  },
  {
    "text": "Hopefully, that responds\nto that question. OK, so I was talking\nabout why platforms",
    "start": "776590",
    "end": "784200"
  },
  {
    "text": "are struggling to moderate\nthis harmful, hateful content online. And one of the other things\nthat is often overlooked",
    "start": "784200",
    "end": "790500"
  },
  {
    "text": "is that even if we ask humans\nto filter through this stuff, these people that are actually\nemployed to do this and stare",
    "start": "790500",
    "end": "796890"
  },
  {
    "text": "at awful, awful things all day\nsuffer really inhumane working conditions. They're often\noutsourced to countries",
    "start": "796890",
    "end": "803070"
  },
  {
    "text": "where the minimum wage is\nlower and the conditions are just worse. And the mental health of\nthese people is just--",
    "start": "803070",
    "end": "809370"
  },
  {
    "text": "it's been very documented\nthat it's really bad. So this is a place where AI\ncould actually help, right?",
    "start": "809370",
    "end": "815670"
  },
  {
    "text": "And so this has kind of led to\nthis field of automatic hate speech detection, which is\nall about trying to make",
    "start": "815670",
    "end": "820800"
  },
  {
    "text": "the internet less toxic. And there's a lot of work\nthat has come out of NLP",
    "start": "820800",
    "end": "826170"
  },
  {
    "text": "trying to tackle this task. There's even a workshop\non detecting online abuses and harms.",
    "start": "826170",
    "end": "832110"
  },
  {
    "text": "And people are developing\nAPIs that they can use-- that you can kind of just\nuse easily off the shelf.",
    "start": "832110",
    "end": "838470"
  },
  {
    "text": "For example, Google's\nsister company Jigsaw develops the Perspective API,\nwhich is a toxicity detection",
    "start": "838470",
    "end": "844470"
  },
  {
    "text": "system that is\nbeing used currently to moderate the New York Times\ncomment section, for example.",
    "start": "844470",
    "end": "850055"
  },
  {
    "text": "And so the first thing that\nI'm going to talk about is that there's a big problem\nof racial bias in hate speech detection.",
    "start": "850055",
    "end": "856620"
  },
  {
    "text": "And when I say\nracial bias, I'm not talking about bias\nagainst racial minorities",
    "start": "856620",
    "end": "862020"
  },
  {
    "text": "or hate speech against\nracial minorities. I'm actually talking\nabout the kind of bias",
    "start": "862020",
    "end": "867180"
  },
  {
    "text": "in which a harmless\ngreeting or a harmless tweet can be flagged as\ntoxic when written",
    "start": "867180",
    "end": "872579"
  },
  {
    "text": "by certain racial minorities\nbut not flagged as toxic or flagged as harmless when\nwritten by a white majority",
    "start": "872580",
    "end": "878339"
  },
  {
    "text": "person. And here, in this\ncase, this is happening because our toxicity\nmodels are trained",
    "start": "878340",
    "end": "883800"
  },
  {
    "text": "on text-only\nannotations, and so they have no idea who's speaking. And the issue specifically\nin this example",
    "start": "883800",
    "end": "891060"
  },
  {
    "text": "is that the N-word\nspoken by a white person is usually considered a lot\nmore offensive than the N-word",
    "start": "891060",
    "end": "896460"
  },
  {
    "text": "spoken by a Black person. And so this illustrates\nthe fact that data sets",
    "start": "896460",
    "end": "902618"
  },
  {
    "text": "are ignoring the underlying\nsocial dynamics of speech-- for example, the\nidentity of the speaker or the dialect of English.",
    "start": "902618",
    "end": "908600"
  },
  {
    "text": "And in this case,\nignoring these nuances really risks harming\nminority populations by suppressing\ninoffensive speech more.",
    "start": "908600",
    "end": "916069"
  },
  {
    "text": "And so we wanted to\ncharacterize and quantify the racial bias in hate\nspeech detection here.",
    "start": "916070",
    "end": "922570"
  },
  {
    "text": "And specifically,\nwhat we wanted to do was first investigate how\nmachine learning models acquire",
    "start": "922570",
    "end": "927730"
  },
  {
    "text": "this racial bias from data\nsets and also look at, stepping back, thinking\nabout the annotation",
    "start": "927730",
    "end": "934000"
  },
  {
    "text": "task for offensiveness\nor toxicity and asking, what about the\nannotation task design",
    "start": "934000",
    "end": "939160"
  },
  {
    "text": "actually affects these\nracial biases here? And you may be wondering, why\nare we looking at racial bias",
    "start": "939160",
    "end": "945260"
  },
  {
    "text": "specifically? Well, the answer is\nthat, as we know, minority populations\nare most often the target of hate\nspeech, compared",
    "start": "945260",
    "end": "951470"
  },
  {
    "text": "to majority populations. And racial bias\nhas actually been studied a lot less than\nother identity-based biases.",
    "start": "951470",
    "end": "957470"
  },
  {
    "text": "And specifically, gender has\ngotten a lot of attention in NLP.",
    "start": "957470",
    "end": "962810"
  },
  {
    "text": "And specifically\non Twitter, there's an actual big danger of\nsilencing Black folks disproportionately.",
    "start": "962810",
    "end": "969110"
  },
  {
    "text": "There's been some\nstudies that have shown the cultural\nimportance of Twitter, specifically, this phenomenon\ncalled Black Twitter.",
    "start": "969110",
    "end": "975260"
  },
  {
    "text": "And it's also a\nreally important space for activism-- for example,\nin the Black Lives Matter movement.",
    "start": "975260",
    "end": "980779"
  },
  {
    "text": "However, one challenge\nwith studying racial bias is that Twitter\nprofiles don't actually have any race data\nassociated with them.",
    "start": "980780",
    "end": "988360"
  },
  {
    "text": "And so what we're going\nto do is we're actually going to use dialect as a\nproxy for racial identity.",
    "start": "988360",
    "end": "994158"
  },
  {
    "text": "And we're operating\nunder the premise here that there are specific\nlexical indicators of minority",
    "start": "994158",
    "end": "1000200"
  },
  {
    "text": "identity in language. And specifically, we're going to\nbe looking for African American English, which is a dialect\nor a set of dialects,",
    "start": "1000200",
    "end": "1008660"
  },
  {
    "text": "a variety of English, that is\ncommon but not limited to Black or African-American\nfolks in the US.",
    "start": "1008660",
    "end": "1014750"
  },
  {
    "text": "And it's extensively\nbeen studied by linguists and shown to have its own\ngrammar and things like that.",
    "start": "1014750",
    "end": "1021170"
  },
  {
    "text": "And there's actually\nalso been shown to be presence of AAE variants\non Twitter, specifically.",
    "start": "1021170",
    "end": "1028369"
  },
  {
    "text": "And so specifically, we're\ngoing to use a lexical detector that Blodgett et al created\nto infer the presence of AAE.",
    "start": "1028369",
    "end": "1035800"
  },
  {
    "text": "But again, a caveat here is\nthat dialect and race are much more complex than this.",
    "start": "1035800",
    "end": "1040900"
  },
  {
    "text": "I'm seeing a question.  Is using dialect as a\nproxy for racial identity",
    "start": "1040900",
    "end": "1046839"
  },
  {
    "text": "susceptible to\npropagating stereotypes, especially if these lexical\nindicators evolve over time?",
    "start": "1046839",
    "end": "1052810"
  },
  {
    "text": "Yeah, I think I'm going\nto click Responding to It. Yeah, so that's\ndefinitely true, for sure.",
    "start": "1052810",
    "end": "1061000"
  },
  {
    "text": "And I'll talk about\nthis later as well. But focusing solely on\ndialect, again, as a static way",
    "start": "1061000",
    "end": "1067540"
  },
  {
    "text": "ignores the evolution that\nthis could have as well as-- we don't want to\ncategorize people's races",
    "start": "1067540",
    "end": "1074192"
  },
  {
    "text": "based on their dialect alone. And that's not what\nwe're doing in this work. We're looking at\nthe dialect which we know has correlates with race.",
    "start": "1074192",
    "end": "1080785"
  },
  {
    "text": "But it's also important\nto look at, actually, self-reported race\nas well, which I'll talk about in a second.",
    "start": "1080785",
    "end": "1086260"
  },
  {
    "text": " OK, so given this intro,\nlet's look at some results",
    "start": "1086260",
    "end": "1093172"
  },
  {
    "text": "and look at, how racially\nbiased are hate speech data sets actually? And so in this work, we focused\non two widely used data sets",
    "start": "1093172",
    "end": "1099847"
  },
  {
    "text": "that we'll call Twitter\nHateBase and Twitter Bootstrap, which are references\nto how they were collected.",
    "start": "1099847",
    "end": "1105010"
  },
  {
    "text": "And what we find is that\nthere's really big racial bias in both these data sets.",
    "start": "1105010",
    "end": "1110769"
  },
  {
    "text": "And so for example,\nin the first data set, about half of the tweets\nin white-aligned English, which is a label that the\ndialect model gives us--",
    "start": "1110770",
    "end": "1119290"
  },
  {
    "text": "only half are flagged as\noffensive by the annotators, whereas 92% of them in\nAfrican American English",
    "start": "1119290",
    "end": "1124930"
  },
  {
    "text": "are flagged as offensive. So there's a huge skew. And we see a similar skew where\n18% of white-aligned English",
    "start": "1124930",
    "end": "1130990"
  },
  {
    "text": "in the Twitter Bootstrap data\nset is labeled as abusive versus much higher\nrates of abusive tweets",
    "start": "1130990",
    "end": "1138039"
  },
  {
    "text": "that are in African\nAmerican English. I'm seeing another question.",
    "start": "1138040",
    "end": "1143260"
  },
  {
    "text": "Something I notice in a\nlot more online spaces is that people\nregrettably will use AAE despite not being Black,\nusually for humorous effect.",
    "start": "1143260",
    "end": "1149210"
  },
  {
    "text": "So under this system,\ncouldn't have trolls get away with saying the N-word by\ntrying to put an AAE spin on it?",
    "start": "1149210",
    "end": "1154227"
  },
  {
    "text": "Yeah, that's a thing\nthat people talk about a lot, actually,\nthe sort of appropriation of African American English\nor just those kinds of things.",
    "start": "1154228",
    "end": "1162639"
  },
  {
    "text": "And I think SNL\ncalled it Gen Z slang. But really, it was markers\nof AAE that they meant.",
    "start": "1162640",
    "end": "1169570"
  },
  {
    "text": "That's a broader question\nof how language sort of gets adopted by different groups and\nhow things evolve in that way",
    "start": "1169570",
    "end": "1178720"
  },
  {
    "text": "that I think social linguists\nare probably more equipped to talk about.",
    "start": "1178720",
    "end": "1184110"
  },
  {
    "text": "But yeah, it's definitely\nsure that if someone is sort of adopting an\nAAE identity online,",
    "start": "1184110",
    "end": "1189270"
  },
  {
    "text": "then they would also maybe fall\nprey to this overcensorship by toxicity detection\nsystems, for sure.",
    "start": "1189270",
    "end": "1196600"
  },
  {
    "text": "Oops, another question. Reflecting on a\nprecision recall tradeoff in classifying such\nproblems, which",
    "start": "1196600",
    "end": "1202570"
  },
  {
    "text": "would you-- do you recommend\nan approach to model building? Let me talk about the\nresults real quick first before we talk about that.",
    "start": "1202570",
    "end": "1208880"
  },
  {
    "text": "So given that there's\nevidence in the data that there's a lot\nof racial bias,",
    "start": "1208880",
    "end": "1214299"
  },
  {
    "text": "we wanted to see, how\ndo models actually acquire these racial\nbiases from data sets?",
    "start": "1214300",
    "end": "1219370"
  },
  {
    "text": "And maybe you're\nthinking, oh, god, maybe they're actually\naveraging those out because they learn to pick out\nthe right patterns in the data,",
    "start": "1219370",
    "end": "1224950"
  },
  {
    "text": "right? Because models can\ndo that, and we're really optimistic about it. Unfortunately, models\nactually not only acquire",
    "start": "1224950",
    "end": "1231940"
  },
  {
    "text": "these racial biases, but\nthey exacerbate them. So that's a problem. And in order to show\nthis, we basically",
    "start": "1231940",
    "end": "1238480"
  },
  {
    "text": "trained classifiers\non these two data sets that we were looking at-- HateBase and Bootstrap.",
    "start": "1238480",
    "end": "1244600"
  },
  {
    "text": "And we're going to look at rates\nof false flagging of toxicity. And specifically, we're going\nto break those false flagging",
    "start": "1244600",
    "end": "1251889"
  },
  {
    "text": "of toxicity rates\ndown by dialect group on our development set. And we're going to look for\nbias under the definition",
    "start": "1251890",
    "end": "1259270"
  },
  {
    "text": "of the equality of opportunity\ncriterion from Moritz Hardt.",
    "start": "1259270",
    "end": "1265060"
  },
  {
    "text": "And looking at the\nresults here, what we find is that both data sets are\nreally biased against AAE.",
    "start": "1265060",
    "end": "1272350"
  },
  {
    "text": "And specifically, they make\nmistakes towards mistaken AAE as offensive much\nmore often than",
    "start": "1272350",
    "end": "1278920"
  },
  {
    "text": "mistaken white-aligned English. So the first one, 46% of\nnonoffensive AAE tweets",
    "start": "1278920",
    "end": "1284440"
  },
  {
    "text": "are mistaken for\noffensive versus only 9% of white nonoffensive tweets. And in the second\nclassifier data set,",
    "start": "1284440",
    "end": "1291970"
  },
  {
    "text": "26% of nonabusive AAE tweets\nare mistaken for abusive versus only 5% white.",
    "start": "1291970",
    "end": "1297580"
  },
  {
    "text": "And the opposite is\ntrue for the second data set, where white tweets\nthat are actually abusive",
    "start": "1297580",
    "end": "1302830"
  },
  {
    "text": "are flagged as not abusive\nat a much higher rate. ",
    "start": "1302830",
    "end": "1308240"
  },
  {
    "text": "OK, does that answer\nyour question? I don't know if that's what you\nwere specifically asking about.",
    "start": "1308240",
    "end": "1316629"
  },
  {
    "text": "What's your recommended\napproach to model building? Yeah, so I think in this\ncase, maybe the question",
    "start": "1316630",
    "end": "1322460"
  },
  {
    "text": "will be answered later about-- actually, instead of thinking\nabout model building, we should be thinking\nabout different ways",
    "start": "1322460",
    "end": "1329090"
  },
  {
    "text": "to conceptualize this task. How do we distinguish\nbetween disproportionate hate",
    "start": "1329090",
    "end": "1335090"
  },
  {
    "text": "speech that's actually due to\ntrue positive, false positive? Hopefully, those results just\nanswered your question here.",
    "start": "1335090",
    "end": "1343430"
  },
  {
    "text": "Do you think asking users\nfor more personal details, such as race, is too much of\na risk for users' privacy?",
    "start": "1343430",
    "end": "1350030"
  },
  {
    "text": "That's an interesting\nethical question. I think, unfortunately,\nif we want to do research that involves\npeople, we need volunteers.",
    "start": "1350030",
    "end": "1358402"
  },
  {
    "text": "And we need people to\ngive us information because I think we don't\nhave enough evidence that-- without actually\nstudying these problems",
    "start": "1358402",
    "end": "1367640"
  },
  {
    "text": "and tackling and really\ndigging into the issues related to identity, we're\nnot really going",
    "start": "1367640",
    "end": "1373250"
  },
  {
    "text": "to be able to make something\nless biased, unfortunately.",
    "start": "1373250",
    "end": "1378645"
  },
  {
    "text": "OK, let me move on for a second\nbefore I answer some more questions. So what we've seen\nhere in the in domain,",
    "start": "1378645",
    "end": "1385160"
  },
  {
    "text": "in distribution setting\nis that there's really strong bias against AAE tweets\nfrom both these classifiers",
    "start": "1385160",
    "end": "1390170"
  },
  {
    "text": "in both these data sets. But you may be\nwondering, OK, maybe this is a data set problem, and\nthe racial bias doesn't really",
    "start": "1390170",
    "end": "1397190"
  },
  {
    "text": "generalize beyond\nthese data sets, right? Unfortunately, it really does. And so this is kind of\naddressing this question",
    "start": "1397190",
    "end": "1402920"
  },
  {
    "text": "that I just answered about race\nand asking people their race. ",
    "start": "1402920",
    "end": "1408380"
  },
  {
    "text": "So we wanted to\nsimulate a situation where these classifiers would\nbe released in the wild.",
    "start": "1408380",
    "end": "1413900"
  },
  {
    "text": "And we looked, basically,\nat prediction rates of offensiveness\nwhen you actually have more gold-standard dialect\nor race information available.",
    "start": "1413900",
    "end": "1423140"
  },
  {
    "text": "And specifically, we looked\nat one corpus that had dialect inferred based on\ngeolocation of tweets",
    "start": "1423140",
    "end": "1428570"
  },
  {
    "text": "and US Census demographic data. So it's not gold. It's maybe\nsilver-standard labels,",
    "start": "1428570",
    "end": "1434900"
  },
  {
    "text": "but sort of more informed by\nwhere the tweet came from. So the dialect label might\nbe a little bit more valid.",
    "start": "1434900",
    "end": "1441350"
  },
  {
    "text": "And what we find is that\none of our classifiers basically is twice\nas likely to predict",
    "start": "1441350",
    "end": "1447350"
  },
  {
    "text": "that an AAE tweet is\noffensive compared to a white-aligned tweet.",
    "start": "1447350",
    "end": "1453220"
  },
  {
    "text": "And then we also looked\nat another data set, where people actually\nparticipated in a survey",
    "start": "1453220",
    "end": "1459039"
  },
  {
    "text": "online, gave researchers their\nrace as well as their Twitter handle, and they give a bunch\nof other demographics too.",
    "start": "1459040",
    "end": "1465399"
  },
  {
    "text": "And those researchers then sort\nof scraped all their tweets to study--",
    "start": "1465400",
    "end": "1471182"
  },
  {
    "text": "I don't remember what\nthe purpose of this study was specifically online. But we can use this\ndata set to actually",
    "start": "1471182",
    "end": "1476380"
  },
  {
    "text": "see, regardless of dialect,\nwhat is actually happening when we look at self-reported race.",
    "start": "1476380",
    "end": "1482860"
  },
  {
    "text": "And what we find, unfortunately,\nis the same kind of biases that are there in dialect land. And the classifier\nhere is 1.5 times",
    "start": "1482860",
    "end": "1490780"
  },
  {
    "text": "as likely to flag a tweet by\nan African-American person as offensive compared to\na tweet by a white person.",
    "start": "1490780",
    "end": "1497510"
  },
  {
    "text": "And unfortunately,\nthis is basically the exact same pattern\nin the second classifier that we studied,\nwhere, basically, this",
    "start": "1497510",
    "end": "1505660"
  },
  {
    "text": "is showing that,\nnot just AAE tweets, but also tweets by Black folks\nare more often flagged as toxic compared to tweets\nby white people",
    "start": "1505660",
    "end": "1512860"
  },
  {
    "text": "or in white-aligned English. And this is really\nstrong evidence that this racial bias is really\ngeneralizing to other corpora",
    "start": "1512860",
    "end": "1518870"
  },
  {
    "text": "here.  Let me take a minute here to\nanswer some questions, OK.",
    "start": "1518870",
    "end": "1524820"
  },
  {
    "text": " In the past, there are\ncases where companies are saying that\ntheir model is biased",
    "start": "1524820",
    "end": "1530790"
  },
  {
    "text": "because the data is biased. But it seems impossible\nto have perfect data. It just seems that right\nnow, we have no one",
    "start": "1530790",
    "end": "1535977"
  },
  {
    "text": "taking direct responsibility on\nthe biased, toxic, problematic models. Do you think there's a\npractical way to address this?",
    "start": "1535977",
    "end": "1541410"
  },
  {
    "text": "And in your opinion,\nshould the burdens fall on researchers working\nin the area of bias detection, all NLP researchers,\nor companies",
    "start": "1541410",
    "end": "1547860"
  },
  {
    "text": "that use-- apply the\nmodels, or the society where the data comes from? ",
    "start": "1547860",
    "end": "1553670"
  },
  {
    "text": "This is a very astute question. Let me think about\nthis for a second.",
    "start": "1553670",
    "end": "1561509"
  },
  {
    "text": "So I think one\nthing that we forget with these systems\nis that AI isn't just",
    "start": "1561510",
    "end": "1567980"
  },
  {
    "text": "operating in a vacuum. It's operating in a full\nsocietal pipeline between users",
    "start": "1567980",
    "end": "1573350"
  },
  {
    "text": "and government and laws\nand companies and stuff. And so a lot of times,\nresearchers are like, oh, well,",
    "start": "1573350",
    "end": "1580070"
  },
  {
    "text": "I don't really know. I'm sort of turning\na blind eye to how my systems are being used.",
    "start": "1580070",
    "end": "1585590"
  },
  {
    "text": "But in order to really\naddress fair and equitable--",
    "start": "1585590",
    "end": "1590630"
  },
  {
    "text": "I'm just going to use this\nexample of toxic content detection-- it does require rules and laws\nand sort of a full pipeline",
    "start": "1590630",
    "end": "1602480"
  },
  {
    "text": "involvement for tackling this. So it's not necessarily--",
    "start": "1602480",
    "end": "1609360"
  },
  {
    "text": "yeah, maybe the companies\nthat are applying these models are the ones that should\nbe held responsible. I'm not really sure\nin terms of that where",
    "start": "1609360",
    "end": "1616289"
  },
  {
    "text": "the responsibility should lie. But I think that,\noperating under the premise of a democratic government,\nwe should have legislation",
    "start": "1616290",
    "end": "1625289"
  },
  {
    "text": "that actually dictates\nwhat can and can't be suppressed or removed\nthrough these algorithms.",
    "start": "1625290",
    "end": "1631638"
  },
  {
    "text": "And I think, currently,\nthe situation is that the companies\nhave all the power to do whatever they want. And I think that's led\nto a lot of frustration",
    "start": "1631638",
    "end": "1638550"
  },
  {
    "text": "from a lot of people. And so the answer might be that\nother companies that are more",
    "start": "1638550",
    "end": "1644340"
  },
  {
    "text": "designed with inclusivity\nfrom the get-go could be a solution there. But yeah, this is\nprobably a longer question",
    "start": "1644340",
    "end": "1651420"
  },
  {
    "text": "to answer directly. There's a live question. So let's have a second go\nand see if we can get someone",
    "start": "1651420",
    "end": "1657440"
  },
  {
    "text": "to ask one live.  Can you hear me? Yeah.",
    "start": "1657440",
    "end": "1662880"
  },
  {
    "text": "Mm-hmm. Wonderful. Yeah, I'm wondering,\nof course, I don't know if these\nmodels are open about how",
    "start": "1662880",
    "end": "1674700"
  },
  {
    "text": "they operationalize the term\n\"toxic\" or \"hate speech.\" But I'm curious if you have any\ninsight into that because that",
    "start": "1674700",
    "end": "1681000"
  },
  {
    "text": "seems like not only do you\nhave the issue of making the model accurate\nbased on your rule set,",
    "start": "1681000",
    "end": "1689039"
  },
  {
    "text": "but also defining a rule set\nbecause different people, like the NAACP, would say that\nthe N-word is never acceptable.",
    "start": "1689040",
    "end": "1698909"
  },
  {
    "text": "Other Black people\nmay feel differently. So it seems that there's\nalso, even within that model,",
    "start": "1698910",
    "end": "1707910"
  },
  {
    "text": "a level of interpretation. Yeah, for sure. I totally agree.",
    "start": "1707910",
    "end": "1714010"
  },
  {
    "text": "And I think that, again,\nkind of pointing forward to the rest of\nthis talk, I always",
    "start": "1714010",
    "end": "1719700"
  },
  {
    "text": "advocate for just not\nhaving AI systems determine offensiveness, period, and just\nmoving towards other approaches",
    "start": "1719700",
    "end": "1727140"
  },
  {
    "text": "to doing those kind of problems.  So yeah.",
    "start": "1727140",
    "end": "1732880"
  },
  {
    "text": "Let me keep going a little bit\nand maybe answer some questions after that, if that's OK. ",
    "start": "1732880",
    "end": "1739780"
  },
  {
    "text": "OK, so we know that there\nis a lot of racial bias in these classifiers. And it just seems really bleak.",
    "start": "1739780",
    "end": "1744850"
  },
  {
    "text": "So you probably are\nall wondering, OK, what can we actually do\nto reduce these biases?",
    "start": "1744850",
    "end": "1752493"
  },
  {
    "text": "One answer is that actually\nchanging the way that we do data collection helps. And so to test this out, we\ndid a small MTurk pilot study",
    "start": "1752493",
    "end": "1761409"
  },
  {
    "text": "where we took 350 AAE tweets\nfrom our two data sets, stratified by toxicity label,\nand we asked three annotators",
    "start": "1761410",
    "end": "1769840"
  },
  {
    "text": "to look at each tweet. And specifically, we wanted\nthem to answer a question about how or whether this tweet\ncould be offensive to anyone.",
    "start": "1769840",
    "end": "1779110"
  },
  {
    "text": "And we did sort of ABC\ntesting, basically, where we had three\nconditions in which people",
    "start": "1779110",
    "end": "1785500"
  },
  {
    "text": "were labeling these. The first one, the\ncontrol condition, was them sort of\nannotating just the text",
    "start": "1785500",
    "end": "1791230"
  },
  {
    "text": "of the tweet-- no\ncontext, no nothing. The second condition,\nwhere we basically",
    "start": "1791230",
    "end": "1798920"
  },
  {
    "text": "provided them with information\nabout the dialect of the tweet. So we were like,\noh, our AI thinks that this tweet is in\nAfrican American English,",
    "start": "1798920",
    "end": "1805492"
  },
  {
    "text": "basically highlighting\nthe fact that this may come from a user who was\nspeaking African American English.",
    "start": "1805492",
    "end": "1810860"
  },
  {
    "text": "And we see here that there's\nactually a significant decrease in the likelihood of\nlabeling these tweets as",
    "start": "1810860",
    "end": "1815960"
  },
  {
    "text": "offensive to anyone, which\nis really interesting. And then the third\ncondition that we had was,",
    "start": "1815960",
    "end": "1821688"
  },
  {
    "text": "basically, instead of\nthinking about the dialect, we made people think\nabout the race that is associated with the dialect.",
    "start": "1821688",
    "end": "1826850"
  },
  {
    "text": "And so our priming\ntext was basically, a Twitter user that is likely\nBlack or African American tweeted this thing.",
    "start": "1826850",
    "end": "1833750"
  },
  {
    "text": "And here, again, we see\na significant difference compared to the\ncontrolled condition.",
    "start": "1833750",
    "end": "1840019"
  },
  {
    "text": "We also asked a second\nquestion of offensiveness, which is, is this tweet\noffensive to you, which is",
    "start": "1840020",
    "end": "1845410"
  },
  {
    "text": "a different sort of labeling. And one of the\ninteresting things",
    "start": "1845410",
    "end": "1850750"
  },
  {
    "text": "here is that the\npropensity for people to label a tweet as\noffensive to anyone is much higher than\nlabeled offensive",
    "start": "1850750",
    "end": "1857380"
  },
  {
    "text": "to them or to themselves. And also, here, we found\nthat the only difference",
    "start": "1857380",
    "end": "1862929"
  },
  {
    "text": "in decreasing sort of\noffensive to themselves is if we highlighted the race\nassociated with the dialect",
    "start": "1862930",
    "end": "1868420"
  },
  {
    "text": "and not just the dialect\nitself compared to the control condition. So this shows that\npriming annotators",
    "start": "1868420",
    "end": "1874465"
  },
  {
    "text": "to think about dialects\nand race can actually influence the labels and maybe\nmitigate some of this bias.",
    "start": "1874465",
    "end": "1880390"
  },
  {
    "text": "But also, just even\nusing two questions, we can show that the\nannotations of offensiveness are highly subjective.",
    "start": "1880390",
    "end": "1885737"
  },
  {
    "text": "The mental processes that the\nlabelers are going through are very different.",
    "start": "1885737",
    "end": "1891289"
  },
  {
    "text": "So to quickly give some\ntakeaways of this work, overt toxicity really backfires\nagainst racial minorities",
    "start": "1891290",
    "end": "1898570"
  },
  {
    "text": "if we try to automate it. And we showed specifically\nthat there's really strong dialect-based\nracial bias.",
    "start": "1898570",
    "end": "1905440"
  },
  {
    "text": "We hypothesize that\nthis is probably due to the negative\nperception of race, and AAE is just\nsometimes thought",
    "start": "1905440",
    "end": "1910990"
  },
  {
    "text": "of as less good English,\nor things like that. And NLP models that are\ntrained on this biased data",
    "start": "1910990",
    "end": "1917920"
  },
  {
    "text": "will just exacerbate\nthose biases. And in our pilot\nstudy, we showed that highlighting the dialect\ncan actually influence",
    "start": "1917920",
    "end": "1924490"
  },
  {
    "text": "the labels of offensiveness. But kind of tackling\nsome of the themes that have been in the\nquestion so far,",
    "start": "1924490",
    "end": "1930400"
  },
  {
    "text": "maybe, given this\nsituation, we should rethink how we tackle hate\nspeech detection as a whole?",
    "start": "1930400",
    "end": "1937419"
  },
  {
    "text": "And to hammer on\nthis point even more, racial bias isn't the\nonly bias or issue",
    "start": "1937420",
    "end": "1945070"
  },
  {
    "text": "that is going on in toxicity\nclassification systems. There's also this\nthing that we like to call lexical biases,\nwhich is basically",
    "start": "1945070",
    "end": "1952090"
  },
  {
    "text": "that if you have a\nminority identity, your system is more\nlikely to flag it as toxic",
    "start": "1952090",
    "end": "1960370"
  },
  {
    "text": "compared to if you have a\nmajority identity in your text. There's also bias\nagainst swear words.",
    "start": "1960370",
    "end": "1966640"
  },
  {
    "text": "So if you say\nsomething positive, like, I fucking love\nthis, your model is going to flag\nit as super toxic.",
    "start": "1966640",
    "end": "1972530"
  },
  {
    "text": "But if you say something\nthat's really awful but doesn't have\nany swear words, your model might just not even\nrealize that it's a problem.",
    "start": "1972530",
    "end": "1979429"
  },
  {
    "text": "And just to highlight\nthis a little bit-- so in some recent work\nthat we presented at EACL,",
    "start": "1979430",
    "end": "1985150"
  },
  {
    "text": "we actually tried to see\nif we could automatically debias the racial biases and\nlexical biases in toxicity",
    "start": "1985150",
    "end": "1991660"
  },
  {
    "text": "language detection models. And specifically, we asked,\ncan automatic debiasing methods",
    "start": "1991660",
    "end": "1997330"
  },
  {
    "text": "from NLI, Natural\nLanguage Inference tasks mitigate these biases? Because there's\nbeen a lot of work",
    "start": "1997330",
    "end": "2003337"
  },
  {
    "text": "in debiasing in\nalignment systems. For example, there's been things\nlooking into ensemble model",
    "start": "2003337",
    "end": "2009269"
  },
  {
    "text": "learning as well as data\nfiltering and things like that. And I encourage you\nto read the paper. But the short\nanswer is that it's",
    "start": "2009270",
    "end": "2016620"
  },
  {
    "text": "not that easy to debias\nthese models, actually. And it's easier to debias\nif your biases are lexical,",
    "start": "2016620",
    "end": "2023730"
  },
  {
    "text": "so related to keywords. But it's actually a lot harder\nfor dialect-based biases",
    "start": "2023730",
    "end": "2028889"
  },
  {
    "text": "to be removed or mitigated\nin these systems. Hi, Maarten.",
    "start": "2028890",
    "end": "2034279"
  },
  {
    "text": "There is a question. Awesome. Hi, Maarten.",
    "start": "2034280",
    "end": "2040586"
  },
  {
    "text": "Hello. Hello. I'm wondering, so for the\ndifferent types of priming,",
    "start": "2040586",
    "end": "2045860"
  },
  {
    "text": "for dialect and\nrace priming, I'm wondering-- because\nthey seem quite similar to me in\nterms of conceptually",
    "start": "2045860",
    "end": "2055969"
  },
  {
    "text": "as part of the experiment. Also, the results seem to be\npretty similar between them.",
    "start": "2055969",
    "end": "2065250"
  },
  {
    "text": "So what is the\nsignificance of having both the dialect and the race? Shouldn't one kind\nof entail the other?",
    "start": "2065250",
    "end": "2072040"
  },
  {
    "text": "Yeah, so I mean, it's kind of\ngoing back to the question of, not everyone who is Black\nspeaks AAE, and not everyone who",
    "start": "2072040",
    "end": "2078620"
  },
  {
    "text": "speaks AAE is Black. And also, just in terms of\nlaypeople's knowledge of AAE--",
    "start": "2078620",
    "end": "2085280"
  },
  {
    "text": "I think most people in America\nknow what a Black person is. But I don't think that-- maybe 60%, 70%-- I don't know.",
    "start": "2085280",
    "end": "2092480"
  },
  {
    "text": "That's maybe too high. I don't know how\nmany people actually know what African American\nEnglish is or consider",
    "start": "2092480",
    "end": "2097730"
  },
  {
    "text": "it a valid form of language. And so when we're thinking\nabout these annotation tasks,",
    "start": "2097730",
    "end": "2104060"
  },
  {
    "text": "we should think about,\nalso, what is the knowledge that these annotators have? And so when we were doing the\ndialect priming condition,",
    "start": "2104060",
    "end": "2110060"
  },
  {
    "text": "we gave them information\nabout the dialects. But in the race\npriming condition,",
    "start": "2110060",
    "end": "2115608"
  },
  {
    "text": "we didn't have to give\nthem that much information because people know\nwhat race is, broadly.",
    "start": "2115608",
    "end": "2122309"
  },
  {
    "text": "So that's the difference. OK, yeah, thank you. That makes a lot more sense. Thank you.",
    "start": "2122310",
    "end": "2127680"
  },
  {
    "text": "Sure.  Should I answer\nsome more questions?",
    "start": "2127680",
    "end": "2133700"
  },
  {
    "text": "Or what do we think? How many years are\nwe away from a time where we may not need an\narmy of human moderators",
    "start": "2133700",
    "end": "2140502"
  },
  {
    "text": "for explicit content? I would advocate\nthat we should never be free of human moderators.",
    "start": "2140502",
    "end": "2145960"
  },
  {
    "text": "I think that letting AI systems\nremove or moderate things automatically is not\nthe greatest option",
    "start": "2145960",
    "end": "2153760"
  },
  {
    "text": "because that gives them\npower to silence us in a way that we don't want that.",
    "start": "2153760",
    "end": "2160567"
  },
  {
    "text": "Does the race, ethnicity of the\nannotator matter in these data sets? Since we are using these\nground truth labels, how do we ensure the\ncorrectness of label quality",
    "start": "2160567",
    "end": "2166330"
  },
  {
    "text": "with the perspective\nof different races? Thank you so much\nfor a great segue into my next slide, which\nis talking about who decides",
    "start": "2166330",
    "end": "2173500"
  },
  {
    "text": "what is offensive or not. And so I want a quick plug\nfor our recent preprint",
    "start": "2173500",
    "end": "2179020"
  },
  {
    "text": "that came out in November,\ncalled \"Annotators with Attitudes, How Annotator\nBeliefs and Identities Bias Toxic Language Detection.\"",
    "start": "2179020",
    "end": "2186220"
  },
  {
    "text": "And basically, what we\nstudied in this work is literally what\nyou just asked about, which is the who, why, and what\nbehind toxicity annotation.",
    "start": "2186220",
    "end": "2192690"
  },
  {
    "text": "So it's basically\nlooking at what is the effect of annotator\nidentities and beliefs",
    "start": "2192690",
    "end": "2198580"
  },
  {
    "text": "on their toxicity labeling\nbehavior as well as looking at what types of text\nthey're likely to label",
    "start": "2198580",
    "end": "2203829"
  },
  {
    "text": "as toxic more or not. And specifically, we did two\ncontrolled annotation studies where we collected attitudes\nabout different concepts,",
    "start": "2203830",
    "end": "2211270"
  },
  {
    "text": "like offensiveness, or we asked\nthem about sort of racism. We asked them about free speech.",
    "start": "2211270",
    "end": "2217240"
  },
  {
    "text": "We asked them about empathy\nand things like that. And we also collected their\ndemographic information.",
    "start": "2217240",
    "end": "2222280"
  },
  {
    "text": "And some things that are\nshowing up is that racist text--",
    "start": "2222280",
    "end": "2228970"
  },
  {
    "text": "so things that are really\nracist in meaning-- are less likely to be labeled\nas offensive by people who",
    "start": "2228970",
    "end": "2235960"
  },
  {
    "text": "already hold racist beliefs-- also, less likely to be\nlabeled as offensive by people",
    "start": "2235960",
    "end": "2241210"
  },
  {
    "text": "who don't think that censorship\nshould exist, for example. I don't fully remember what\nthe demographic correlates are.",
    "start": "2241210",
    "end": "2247990"
  },
  {
    "text": "But I'm going to assume,\nmaybe, for racist speech, there wasn't that\nmuch of a difference.",
    "start": "2247990",
    "end": "2255000"
  },
  {
    "text": "But we also found that African\nAmerican English tweets seem more racist\nto people who hold",
    "start": "2255000",
    "end": "2260467"
  },
  {
    "text": "racist beliefs, which\nis also a really interesting finding there. And then finally, we looked\nat sort of the swear word",
    "start": "2260467",
    "end": "2267930"
  },
  {
    "text": "piece of it as well. And what we found\nwas that swear words can seem much more\noffensive to people who are more conservative--",
    "start": "2267930",
    "end": "2273960"
  },
  {
    "text": "so politically conservative\nas well as more traditional, rated as using a psychological\ntraditionalism scale.",
    "start": "2273960",
    "end": "2284760"
  },
  {
    "text": " Should I be following up on\nthe discussion and the Q&A?",
    "start": "2284760",
    "end": "2290196"
  },
  {
    "text": "Or, I'm looking at-",
    "start": "2290196",
    "end": "2295600"
  },
  {
    "text": "I mean, I guess\nyou have to judge how many questions\nyou can answer and how much you want\nto make progress.",
    "start": "2295600",
    "end": "2300940"
  },
  {
    "text": "You're welcome to\nanswer more of them. Yeah. I mean, yeah, I think I could\nanswer a couple questions.",
    "start": "2300940",
    "end": "2308493"
  },
  {
    "text": "Are these models\nlearning toxicity that treat entries\nof a corpus that are labeled a priori\nor of any supervised learning any corpora?",
    "start": "2308493",
    "end": "2314798"
  },
  {
    "start": "2314799",
    "end": "2320313"
  },
  {
    "text": "I'm not sure I fully\nunderstand this question. ",
    "start": "2320313",
    "end": "2326680"
  },
  {
    "text": "Does identity of\nresearchers influence how data sets are created? And what's considered\ntoxic, since the mainstream view about the\nconstruction reflects",
    "start": "2326680",
    "end": "2332500"
  },
  {
    "text": "the majority demographic? That's an interesting\nquestion of,",
    "start": "2332500",
    "end": "2338344"
  },
  {
    "text": "what about the positionality\nof the researchers themselves?",
    "start": "2338344",
    "end": "2343640"
  },
  {
    "text": "I think, typically, probably,\nthat influences things.",
    "start": "2343640",
    "end": "2349240"
  },
  {
    "text": "But I think to sort of keep\ngoing with my talk here, I think that,\nreally, we should be rethinking automatic\noffensiveness",
    "start": "2349240",
    "end": "2355900"
  },
  {
    "text": "detection totally. And just to really\nrecap everything,",
    "start": "2355900",
    "end": "2362710"
  },
  {
    "text": "we know that there's a\nlot of labeling variation in these things. What is offensiveness to whom?",
    "start": "2362710",
    "end": "2367930"
  },
  {
    "text": "It's really different,\ndepending on your background, your attitudes, and everything. And even just the task phrasing\ncan make a big difference.",
    "start": "2367930",
    "end": "2376040"
  },
  {
    "text": "But also, when we think\nabout hate speech, there's some countries that\nactually have legal definitions of hate speech, right? And so we can't just go throwing\naround that term, being like,",
    "start": "2376040",
    "end": "2382810"
  },
  {
    "text": "this is hate\nspeech, when there's legal definitions of this and\nlegal implications of labeling something as hate speech.",
    "start": "2382810",
    "end": "2387910"
  },
  {
    "text": "And then, like the question\nasked, looking at the fact that annotators\nmight operationalize",
    "start": "2387910",
    "end": "2393069"
  },
  {
    "text": "a definition differently. NLP researchers might\nhave different views of what should go on in there.",
    "start": "2393070",
    "end": "2399700"
  },
  {
    "text": "And I think that\none big component is that no one's asking\nthe real question of,",
    "start": "2399700",
    "end": "2404859"
  },
  {
    "text": "why is something\nhateful or offensive? Everyone's just concerned\nwith labeling and noise and annotations.",
    "start": "2404860",
    "end": "2410230"
  },
  {
    "text": "But why aren't we focusing\non, what about the text is making it offensive or hateful?",
    "start": "2410230",
    "end": "2415432"
  },
  {
    "text": "And also, just to hammer this\npoint home that I've already mentioned, personally,\nmy position is that we",
    "start": "2415433",
    "end": "2422080"
  },
  {
    "text": "don't-- should we really have AI\nsystems making these decisions alone, like risking removing\nentire swaths of dialects",
    "start": "2422080",
    "end": "2429490"
  },
  {
    "text": "of content and moderating that? I'm not sure that that's\nreally what should be going on.",
    "start": "2429490",
    "end": "2435619"
  },
  {
    "text": "So what if AI\nsystems were designed to help humans\ndetermine toxicity by explaining why\nsomething might be toxic",
    "start": "2435620",
    "end": "2441609"
  },
  {
    "text": "or biased instead? And this is going to be\nthe next part of my talk. But let me look at some\nquestions real quick.",
    "start": "2441610",
    "end": "2450560"
  },
  {
    "text": "OK, for the biased\nevaluations, [INAUDIBLE] reannotated to not classify any\ntweets using slang such as much",
    "start": "2450560",
    "end": "2455870"
  },
  {
    "text": "as the N-word is not offensive. If models were trained on\nthis reannotated data set, does that mitigate the issue? In other words,\nwouldn't the NLP models",
    "start": "2455870",
    "end": "2462155"
  },
  {
    "text": "learn that the N-word is only\noffensive in non-AAE dialects? Yeah, so we actually did in\nthe \"Challenges in Automatic",
    "start": "2462155",
    "end": "2467960"
  },
  {
    "text": "Debiasing\" paper-- we did do\na translation experiment where we basically tried\nto translate the AAE",
    "start": "2467960",
    "end": "2474500"
  },
  {
    "text": "tweets into non-AAE English. And that was the most\npromising direction.",
    "start": "2474500",
    "end": "2480410"
  },
  {
    "text": "Basically, when you sort of\nremove the markers of AAE but try to preserve as much\nof the content and sort of use",
    "start": "2480410",
    "end": "2487850"
  },
  {
    "text": "those labels there--\nbut then the issue is that you have\nto relabel those",
    "start": "2487850",
    "end": "2495500"
  },
  {
    "text": "in sort of the context of\ntheir non-AAE versions. So that sort of adds data\nannotation to the challenge.",
    "start": "2495500",
    "end": "2503207"
  },
  {
    "text": " It seems like a lot of\nthe classifiers right",
    "start": "2503207",
    "end": "2508820"
  },
  {
    "text": "now are very lexically focused. Is there a way to make the\nmodel more contextually aware? Yes, and I think\nthat's something",
    "start": "2508820",
    "end": "2514310"
  },
  {
    "text": "that we should be\nfocusing on more and more. And I'll talk about that\nin some future work. Seems like the\nmodel is picking up",
    "start": "2514310",
    "end": "2519770"
  },
  {
    "text": "on raw tokens, such\nas swear words. what are your thoughts\nif I say, what does the sentence talk about? Yeah, so that's\nkind of also what I'm going to talk about\nin this next part.",
    "start": "2519770",
    "end": "2526505"
  },
  {
    "text": " I think I'm trying to take\ncare of these questions.",
    "start": "2526505",
    "end": "2533260"
  },
  {
    "text": "Let me rephrase my question. How are these toxicity\nclassifying algorithms learning what is\nand isn't toxic? Based on labels, right?",
    "start": "2533260",
    "end": "2539349"
  },
  {
    "text": "And that's kind of the\nissue, is if the humans are labeling things\nimproperly because they're flagging just words, then the\nmodels are just only going",
    "start": "2539350",
    "end": "2546099"
  },
  {
    "text": "to be able to recreate\nthat behavior. ",
    "start": "2546100",
    "end": "2553940"
  },
  {
    "text": "I hope that there is going to be\npotential regulations coming up towards this kind of regulating\nwhat AI systems can and can't",
    "start": "2553940",
    "end": "2561360"
  },
  {
    "text": "do. But that also requires\ntackling the fact that tech companies have a lot\nmore freedom in the US right",
    "start": "2561360",
    "end": "2566940"
  },
  {
    "text": "now.  OK, is it possible\nthat developing a model that detects a person's\nidentity through his tweets",
    "start": "2566940",
    "end": "2573948"
  },
  {
    "text": "has some potential\nprivacy issue? How to mitigate the\npotential harmful use? Yeah, so I think that language,\nunfortunately, kind of",
    "start": "2573948",
    "end": "2581930"
  },
  {
    "text": "is a way to communicate\nyour identity to people. And so that's what\nsocial linguists",
    "start": "2581930",
    "end": "2587840"
  },
  {
    "text": "talk about when you do code\nswitching between your family and your work friends,\nyou are asserting",
    "start": "2587840",
    "end": "2594619"
  },
  {
    "text": "a common shared identity\nor creating a common shared identity between your family and\nyou in that one sort of dialect",
    "start": "2594620",
    "end": "2600770"
  },
  {
    "text": "or way of speaking. And if you're speaking\nto your coworkers, maybe you're going to use\na different type of way",
    "start": "2600770",
    "end": "2605812"
  },
  {
    "text": "of speaking. And so sure, maybe no one\nshould be spying on you",
    "start": "2605812",
    "end": "2611612"
  },
  {
    "text": "and looking at,\noh, you're speaking to a coworker versus not. But if people are tweeting\nin these settings, then they're giving\naway a little bit",
    "start": "2611612",
    "end": "2618860"
  },
  {
    "text": "of who they're tweeting\nto and things like that. So yeah, when it comes to\nstudying these kinds of things,",
    "start": "2618860",
    "end": "2629530"
  },
  {
    "text": "it's hard to do without\nknowing, actually, who the identities of the\npeople are in the network. But it's just not\nas simple as sort",
    "start": "2629530",
    "end": "2637300"
  },
  {
    "text": "of your behavioral patterns\nmight not give away as much of your identity\nmarkers as language can.",
    "start": "2637300",
    "end": "2644210"
  },
  {
    "text": "All right, I think I'm going\nto walk through some more slides a little bit first. And then we can look at\nthe questions some more.",
    "start": "2644210",
    "end": "2650400"
  },
  {
    "text": "But if there's anything really\nimportant, please let me know. All right, so we talked\nabout the problems with classifying toxicity\nor toxic language.",
    "start": "2650400",
    "end": "2659810"
  },
  {
    "text": " So I want to talk about Social\nBias Frames, which is actually",
    "start": "2659810",
    "end": "2666890"
  },
  {
    "text": "what I would like to\ncall a new alternative way of looking at this problem,\nor the first iteration of that.",
    "start": "2666890",
    "end": "2673430"
  },
  {
    "text": "So Social Bias Frames\nis a new formalism to reason about the social and\npower implications of language.",
    "start": "2673430",
    "end": "2678433"
  },
  {
    "text": "And I know that we've been\ntalking about hate speech and stuff like that already. But I want to warn\nyou that the content in the rest of this talk may\nbe upsetting or offensive",
    "start": "2678433",
    "end": "2685610"
  },
  {
    "text": "because I'm going to show\nsome examples, unfortunately. And again, just to contextualize\nthis a little bit more,",
    "start": "2685610",
    "end": "2692060"
  },
  {
    "text": "we're approaching these\nproblems of social biases from a US sociocultural\nperspective in this project.",
    "start": "2692060",
    "end": "2697910"
  },
  {
    "text": "And this is actually work with\nanother Stanford professor, Dan Jurafsky.",
    "start": "2697910",
    "end": "2703340"
  },
  {
    "text": "OK, so when we think\nabout social biases, there's two ways that\nthese social biases can be expressed in language.",
    "start": "2703340",
    "end": "2709880"
  },
  {
    "text": "First one is, we should kill\nall XYZ demographic group. And this is super\neasily flagged as toxic by your off-the-shelf\ntoxicity detection systems.",
    "start": "2709880",
    "end": "2718380"
  },
  {
    "text": "But there's a much\nmore subtle way of expressing social biases-- for example, in\nthe statement, we shouldn't lower our standards\njust to hire more women.",
    "start": "2718380",
    "end": "2726540"
  },
  {
    "text": "And so we can all\nhere understand that this type of\nunconscious bias here--",
    "start": "2726540",
    "end": "2731750"
  },
  {
    "text": "because of how language\nimplicature works, this is implying that\nwomen are less qualified",
    "start": "2731750",
    "end": "2736849"
  },
  {
    "text": "than men, especially because\nthis \"just\" word here is really hammering that. And so we can understand that\nthis is an unconscious bias.",
    "start": "2736850",
    "end": "2743390"
  },
  {
    "text": "But it's not flagged as\ntoxic at all by these models, unfortunately. And so this motivated our\ncreation of Social Bias Frames",
    "start": "2743390",
    "end": "2750920"
  },
  {
    "text": "as a new structured formalism\nthat distills knowledge about the harmful or biased\napplications of language.",
    "start": "2750920",
    "end": "2757160"
  },
  {
    "text": "And so specifically, this is\nkind of a structured formalism, so let me walk you\nthrough the structure.",
    "start": "2757160",
    "end": "2762210"
  },
  {
    "text": "So if we have a post, we\nshouldn't lower our standards just to hire more\nwomen, we're going to ask annotators, is\nthis offensive or not,",
    "start": "2762210",
    "end": "2768950"
  },
  {
    "text": "as a sort of prescreening round. We're going to look\nat, do you think this was intended to\nbe offensive or not?",
    "start": "2768950",
    "end": "2774800"
  },
  {
    "text": "In this case, maybe. We're going to ask, is this a\nlewd statement or referencing",
    "start": "2774800",
    "end": "2780380"
  },
  {
    "text": "something sexual? Not really here. If it was annotated\nas offensive, we're going to look at, is this\ntargeting a group of people",
    "start": "2780380",
    "end": "2787915"
  },
  {
    "text": "or referencing a\ngroup of people? Or is this really just\nan individual insult?",
    "start": "2787915",
    "end": "2793345"
  },
  {
    "text": "If it is a group\nof people, we're going to ask for a\nfree-text explanation of who that group of people was. In this case, it's women.",
    "start": "2793345",
    "end": "2799520"
  },
  {
    "text": "And then we're going to ask\nfor a free-text explanation of, what is the implied\nstereotype here?",
    "start": "2799520",
    "end": "2804619"
  },
  {
    "text": "And this is the\nstereotype that women are less qualified than men. And then finally, the\nlast variable in our frame",
    "start": "2804620",
    "end": "2810260"
  },
  {
    "text": "is related to in-group\nlanguage, which is about sort of capturing\nwhether the statement is made by members of the same group as\nthe group that's targeted, kind",
    "start": "2810260",
    "end": "2819359"
  },
  {
    "text": "of trying to address the\nspeaker and listener identities. But here in this case,\nthat doesn't really",
    "start": "2819360",
    "end": "2824780"
  },
  {
    "text": "seem to be the case. So again, to remind\nyou, the motivation",
    "start": "2824780",
    "end": "2830130"
  },
  {
    "text": "for Social Bias Frames\nis really that if we want to be able to avoid\nproblematic-- or really,",
    "start": "2830130",
    "end": "2835470"
  },
  {
    "text": "if companies want to avoid PR\nproblems of their chat bots turning racist, they need\nan understanding of what",
    "start": "2835470",
    "end": "2842340"
  },
  {
    "text": "they actually want to avoid. And Social Bias Frames is\na new view on what to avoid",
    "start": "2842340",
    "end": "2847559"
  },
  {
    "text": "or these social biases. And it's more explainable\nand trustworthy because it comes baked in with\nexplanations of why something",
    "start": "2847560",
    "end": "2854550"
  },
  {
    "text": "could be biased. And it's more holistic than\nbinary hate speech detection because it gets around,\nare you offended",
    "start": "2854550",
    "end": "2862598"
  },
  {
    "text": "by this statement or not? Really, it's trying\nto distill, what is the offensive meaning behind\nthis, which is different.",
    "start": "2862598",
    "end": "2870650"
  },
  {
    "text": "I also want to\nhighlight that in order to study these social biases\nin the wild, we created",
    "start": "2870650",
    "end": "2875690"
  },
  {
    "text": "the Social Bias\nInference Corpus, which is 150,000\nannotated tuples that",
    "start": "2875690",
    "end": "2881089"
  },
  {
    "text": "were labeled from 44,000\nposts from social media,",
    "start": "2881090",
    "end": "2886530"
  },
  {
    "text": "including from Twitter, Reddit,\nthe neo-Nazi communities of Gab and Stormfront, as\nwell as some banned,",
    "start": "2886530",
    "end": "2893119"
  },
  {
    "text": "really misogynistic subreddits. And our corpus contains 34,000\nimplications about 3,000",
    "start": "2893120",
    "end": "2901640"
  },
  {
    "text": "different demographic groups. And I don't have time to\ngo into the details of how this was created. But because of how we trained\nour MTurkers and our annotators",
    "start": "2901640",
    "end": "2910778"
  },
  {
    "text": "and how we selected\nthem, we actually got pretty high pairwise\nagreement on these annotations.",
    "start": "2910778",
    "end": "2916970"
  },
  {
    "text": "And also, we really wanted\nto be able to capture the types of discrimination\nthat people are actually",
    "start": "2916970",
    "end": "2922940"
  },
  {
    "text": "reporting experiencing\noffline, or online, but in the real world.",
    "start": "2922940",
    "end": "2928289"
  },
  {
    "text": "And so we're not just capturing\nfandom wars or pop star fandom",
    "start": "2928290",
    "end": "2933680"
  },
  {
    "text": "wars, but we're\nactually capturing hatred towards\ndemographic groups that are reflective of\nreal-world discrimination.",
    "start": "2933680",
    "end": "2940900"
  },
  {
    "text": "Also, I wanted to\nhighlight the way that we designed\nthis frame inherently had interdisciplinary\nsort of knowledge in mind.",
    "start": "2940900",
    "end": "2948410"
  },
  {
    "text": "And so we really tried to\nground this in social science literature, specifically looking\nat literature of rudeness,",
    "start": "2948410",
    "end": "2953500"
  },
  {
    "text": "pragmatics, offensiveness, and\nhow people sort of perceive offensiveness and\nthings like that.",
    "start": "2953500",
    "end": "2959270"
  },
  {
    "text": "And this actually led to\nthe inclusion of this intent variable, which is not\nonly there because if you",
    "start": "2959270",
    "end": "2966330"
  },
  {
    "text": "sort of perceive someone\nas being well-intentioned, you might be more forgiving\ntowards what they're saying,",
    "start": "2966330",
    "end": "2971650"
  },
  {
    "text": "even though their\nbias is still there, but also, if we think\nabout implementing these tools to give feedback to\npeople who are writing texts--",
    "start": "2971650",
    "end": "2978700"
  },
  {
    "text": "and if your AI system tells you,\nthis is 80% toxic, versus, hey,",
    "start": "2978700",
    "end": "2983710"
  },
  {
    "text": "you might have not\nintended to be offensive, but here's what\nyour thing implies",
    "start": "2983710",
    "end": "2989200"
  },
  {
    "text": "about this group of people, that\ncould actually be a much softer feedback to an author.",
    "start": "2989200",
    "end": "2996099"
  },
  {
    "text": "Also, as we've discussed with\nthe AAE racial bias case, we wanted to include\nsituations where",
    "start": "2996100",
    "end": "3003750"
  },
  {
    "text": "things were cases of language\nthat's more in-group. So that's things\nlike self-deprecation or reclaimed slurs that\ncan appear offensive",
    "start": "3003750",
    "end": "3009630"
  },
  {
    "text": "if you're not a member of a\ngroup, but if you are a member, it could be less offensive. And then we also wanted to be\na little more intersectional.",
    "start": "3009630",
    "end": "3017220"
  },
  {
    "text": "So we wanted to have\nthe ability for posts to target multiple\ngroups at the same time or multiple identities.",
    "start": "3017220",
    "end": "3022260"
  },
  {
    "text": "And so we collected that. And we also collected\nmultiple implications to get more data for the kinds\nof stereotypes that are there.",
    "start": "3022260",
    "end": "3029972"
  },
  {
    "text": "And then, finally, the way that\nwe designed this frame also had in mind the\nsort of annotators",
    "start": "3029972",
    "end": "3035550"
  },
  {
    "text": "to see what can and\ncan't be done at scale.  I see a question.",
    "start": "3035550",
    "end": "3041925"
  },
  {
    "text": " How accurately can AI detect\nintent from text, if at all?",
    "start": "3041925",
    "end": "3047210"
  },
  {
    "text": "I will answer that in\na second because we're going to look at some results. What about AAE are\nmodels finding offensive?",
    "start": "3047210",
    "end": "3053880"
  },
  {
    "text": "Is it mainly use of the N-word? Actually, if you go-- Sorry, this was a\nquestion from before. But if you go to the appendix\nof the racial bias paper,",
    "start": "3053880",
    "end": "3059532"
  },
  {
    "text": "we have the most common features\nthat are used by the classifier to determine things and\nplotted by likelihood of AAE.",
    "start": "3059532",
    "end": "3066940"
  },
  {
    "text": "You should take a look at\nthat, and it'll answer that. So it's not just the\nN-word, but it also is suffixes that appear in\nwords and things like that.",
    "start": "3066940",
    "end": "3075887"
  },
  {
    "text": "OK, I think the other two are\nmaybe a little bit longer. So I'm going to\nanswer them later. OK, so given that we\nhave a corpus that",
    "start": "3075887",
    "end": "3082440"
  },
  {
    "text": "is annotated with\nSocial Bias Frames, we wanted to know, how good\nare NLP models actually making inferences using\nSocial Bias Frames?",
    "start": "3082440",
    "end": "3089080"
  },
  {
    "text": "And so we set up\na case study where the goal is to predict an\nentire Social Bias Frame",
    "start": "3089080",
    "end": "3094260"
  },
  {
    "text": "from a previously unseen post. And in order to\ndo this, our model requires classifying these\ncategorical variables of intent",
    "start": "3094260",
    "end": "3102535"
  },
  {
    "text": "and offensiveness\nand things like that. But also, it needs to be able to\ngenerate the groups and implied statements.",
    "start": "3102535",
    "end": "3108550"
  },
  {
    "text": "So not all models\nare able to do this. But GPT-style style\nmodels actually can do this if we set\nthem up correctly.",
    "start": "3108550",
    "end": "3115030"
  },
  {
    "text": "And so the way that we did\nthis is we took our Social Bias Frame-- and this is a cool\nanimation, so watch out--",
    "start": "3115030",
    "end": "3121920"
  },
  {
    "text": "and we linearized it,\nadding special tokens for each classification\nvariable. And then we passed them through\na Transformer-based conditional",
    "start": "3121920",
    "end": "3129300"
  },
  {
    "text": "language model that we had\ninitialized with GPT-2, in this case. And then we optimized the\nnegative log-likelihood",
    "start": "3129300",
    "end": "3135810"
  },
  {
    "text": "of all tokens for training. And then for predicting\nSocial Bias Frames,",
    "start": "3135810",
    "end": "3140970"
  },
  {
    "text": "we are going to frame this\nas a conditional generation setting of the\nlinearized frame, token",
    "start": "3140970",
    "end": "3146580"
  },
  {
    "text": "by token, sort of sampling it. But can anyone\ntell me if there's",
    "start": "3146580",
    "end": "3151725"
  },
  {
    "text": "anything wrong with the\ngenerated frame right here? ",
    "start": "3151725",
    "end": "3157080"
  },
  {
    "text": "So actually, there's\na big problem here-- that the post was predicted\nnot to be offensive but still having implications\nabout Black folks.",
    "start": "3157080",
    "end": "3165320"
  },
  {
    "text": "And this is a\nproblem, here, where generated frames\ncan sometimes not be consistent with\nthe frame structure.",
    "start": "3165320",
    "end": "3171891"
  },
  {
    "text": "And this is because when\nyou linearize your model, it can sometimes just not learn\nthe full structure properly.",
    "start": "3171892",
    "end": "3177780"
  },
  {
    "text": "And so what we need to do is\nenforce the structure post-hoc. And so I'm going to gloss\nover the details here.",
    "start": "3177780",
    "end": "3183237"
  },
  {
    "text": "But basically, you can either\njust kind of like top-down enforce it, or we can do\nsomething a little bit more global and let your\nfuture decisions correct",
    "start": "3183237",
    "end": "3191840"
  },
  {
    "text": "your past mistakes a little bit. And we see a little\nbit of difference that this constrained, sort of\nmore holistic inference helps.",
    "start": "3191840",
    "end": "3198973"
  },
  {
    "text": "But sort of answering\nthe question of, how well do models do at\nthese different classification",
    "start": "3198973",
    "end": "3204410"
  },
  {
    "text": "variables, well, it's\nmaybe OK to predict whether something was intended\nto be offensive or not.",
    "start": "3204410",
    "end": "3211540"
  },
  {
    "text": "But what is really\nmuch more challenging is predicting whether\nsomething is targeting a group or an individual as well\nas predicting whether something",
    "start": "3211540",
    "end": "3220090"
  },
  {
    "text": "is in-group language, which\nis just really hard to do, especially given the\ndata that we have.",
    "start": "3220090",
    "end": "3225609"
  },
  {
    "text": "But more interestingly,\nthe performance of how models are able to\ngenerate the implications",
    "start": "3225610",
    "end": "3230920"
  },
  {
    "text": "is actually\ninteresting to look at. And I'm going to gloss over\nthe automatic sort of metrics",
    "start": "3230920",
    "end": "3236950"
  },
  {
    "text": "of this here. But the model is\nable to identify the right targeted group,\nactually, pretty well.",
    "start": "3236950",
    "end": "3243130"
  },
  {
    "text": "But the biased\nimplications are a lot more challenging to generate, which\nI can illustrate with an example",
    "start": "3243130",
    "end": "3251200"
  },
  {
    "text": "after answering\nsome more questions because I'm seeing\nthings pop up. Posts from fandom/stan\nTwitter might be overly",
    "start": "3251200",
    "end": "3258040"
  },
  {
    "text": "offensive for comedic effect. I think that we\nshould be concerned about these types of posts. Yeah, so that's\none of the reasons why I think it's\nimportant to think",
    "start": "3258040",
    "end": "3264310"
  },
  {
    "text": "about who's being offended, or\nwhat is the implication here? And I'm a big believer\nthat we should",
    "start": "3264310",
    "end": "3270880"
  },
  {
    "text": "tackle important problems. Sorry, stan Twitter. But people aren't dying or\nbeing killed because they're",
    "start": "3270880",
    "end": "3280660"
  },
  {
    "text": "a Selena Gomez fan. However, people are dying\nbecause they're a trans woman.",
    "start": "3280660",
    "end": "3285940"
  },
  {
    "text": "That is one of the highest\nmurder rates in the country-- trans women of\ncolor, specifically.",
    "start": "3285940",
    "end": "3291250"
  },
  {
    "text": "So that's the kind of\nbiases that I think our AI system should tackle. And capturing arguments between\npolitical parties online",
    "start": "3291250",
    "end": "3297820"
  },
  {
    "text": "isn't that important in my view. But these type of informations\nof who's actually being",
    "start": "3297820",
    "end": "3306400"
  },
  {
    "text": "targeted, who's being harmed-- that wasn't there until\nSocial Bias Frames appeared. No one thought about this\nkind of stuff before.",
    "start": "3306400",
    "end": "3312877"
  },
  {
    "text": "They were just like, oh,\nthis is offensive or not. And so you end up just\nputting anti-Semitism",
    "start": "3312877",
    "end": "3318490"
  },
  {
    "text": "in the same bucket as anti\nsome pop star's fandom sort of arguments.",
    "start": "3318490",
    "end": "3323660"
  },
  {
    "text": "So I think that's a\nreally good point. How are the free-text responses\nprocessed in the model?",
    "start": "3323660",
    "end": "3330820"
  },
  {
    "text": "So it's literally\njust token by token. So it's kind of like language\nmodeling-- conditional language modeling.",
    "start": "3330820",
    "end": "3336130"
  },
  {
    "text": "I maybe would refer\nyou to the paper for a little bit more detail. But it's really\njust word by word. Why is a generative method used\nto detect the frames rather",
    "start": "3336130",
    "end": "3343390"
  },
  {
    "text": "than a classification model? So we have to generate\nthe implications. Those are literally\njust open-ended strings.",
    "start": "3343390",
    "end": "3349720"
  },
  {
    "text": "So we can't just\nuse classification. And we could just do separate-- classification first and\nthen add a different model",
    "start": "3349720",
    "end": "3355085"
  },
  {
    "text": "but that would just-- I mean, I encourage you\nto try if you want to. But that may work about\nthe same, potentially.",
    "start": "3355085",
    "end": "3363859"
  },
  {
    "text": "OK, so let's look\nat some examples. Again, so warning, there's\nsome offensive content here.",
    "start": "3363860",
    "end": "3370607"
  },
  {
    "text": "All right, so here's\nan example from the dev side of social bias\ninference corpus that says, I love gay guys.",
    "start": "3370607",
    "end": "3375680"
  },
  {
    "text": "They're so much fun. I would love to have\na gay guy best friend, but lesbians are just annoying.",
    "start": "3375680",
    "end": "3381050"
  },
  {
    "text": "So the model here predicts\nthat this post is offensive because it implies that\nlesbians are annoying, which is",
    "start": "3381050",
    "end": "3387440"
  },
  {
    "text": "basically just written there. And this is in line with\nwhat the annotators wrote,",
    "start": "3387440",
    "end": "3392860"
  },
  {
    "text": "which is that this post implies\nthat lesbians are annoying. But the annotators also\nwrote that this post",
    "start": "3392860",
    "end": "3398730"
  },
  {
    "text": "implies that all gay guys\nare fun to be around. And so this illustrates\nthe kind of mistakes",
    "start": "3398730",
    "end": "3404910"
  },
  {
    "text": "that this model\ntends to make, which is that it can\ntypically be successful when there's really\nverbatim cues,",
    "start": "3404910",
    "end": "3411180"
  },
  {
    "text": "but they struggle with\nmore subtle biases-- for example the positive\nstereotype that all gay guys",
    "start": "3411180",
    "end": "3417450"
  },
  {
    "text": "are fun to be around. And social psychologists tell us\nthat these positive stereotypes",
    "start": "3417450",
    "end": "3422580"
  },
  {
    "text": "can have, also, nefarious\neffects on people, even though they're not\nnegative in sentiment.",
    "start": "3422580",
    "end": "3429710"
  },
  {
    "text": "Another example here is about\na Black guy who's in class and throws a paper\nball into the trash,",
    "start": "3429710",
    "end": "3435470"
  },
  {
    "text": "and his teacher says that\nhe's a disgrace to his race. The model here\npredicts that this post",
    "start": "3435470",
    "end": "3441880"
  },
  {
    "text": "implies that Black people\nare trash, which is not what the annotators wrote.",
    "start": "3441880",
    "end": "3447850"
  },
  {
    "text": "And in fact, the annotators\nhere correctly flagged that this post implies\nthat Black men are defined",
    "start": "3447850",
    "end": "3452973"
  },
  {
    "text": "by their athletic skill or\nthat all Black men should be good at basketball. And again, this is\nan example of where",
    "start": "3452973",
    "end": "3458785"
  },
  {
    "text": "a model might be relying\non negative keywords more and finding the\nidentity-based words as well as the negative keywords\nand just combining",
    "start": "3458785",
    "end": "3464590"
  },
  {
    "text": "them and assuming that that's\nwhat the implication is. And so this kind of\nkeyword-based reliance",
    "start": "3464590",
    "end": "3473695"
  },
  {
    "text": "is sort of not only\nspecific to this task. There's a lot of places\nwhere models tend to rely",
    "start": "3473695",
    "end": "3480400"
  },
  {
    "text": "on lexical cues a lot more. But yeah, so we need some\nnew modeling advances to do",
    "start": "3480400",
    "end": "3485950"
  },
  {
    "text": "this task better, probably. So just to summarize\nthis real quick-- so Social Bias Frames\nis a new formalism",
    "start": "3485950",
    "end": "3492840"
  },
  {
    "text": "to distill the harmful or\nbiased implications of language. We introduced a new data\nset with annotations.",
    "start": "3492840",
    "end": "3498299"
  },
  {
    "text": "And then our experiments\nshow that models really struggle with these more subtle\nbiased implications, which",
    "start": "3498300",
    "end": "3504089"
  },
  {
    "text": "motivates, as I said, the need\nfor better structured reasoning about social biases in people\nand groups in language.",
    "start": "3504090",
    "end": "3512050"
  },
  {
    "text": "So at a higher level, though,\nthe goal of Social Bias Frames was basically to create an\ninterpretable or explainable",
    "start": "3512050",
    "end": "3518790"
  },
  {
    "text": "formalism that could represent\nsocial biases in language. And these explanations\ncan be really",
    "start": "3518790",
    "end": "3525410"
  },
  {
    "text": "useful for determining\nwhat's going on in already written text.",
    "start": "3525410",
    "end": "3531330"
  },
  {
    "text": "So for example, I'm\nimagining Social Bias Frames as being helpful for helping\ncontent moderators make",
    "start": "3531330",
    "end": "3536810"
  },
  {
    "text": "decisions-- so not just\nhaving the text, but also, if our models were good, having\nimplications of the posts",
    "start": "3536810",
    "end": "3544339"
  },
  {
    "text": "be associated with there. And people could make a\nmore informed decision",
    "start": "3544340",
    "end": "3549829"
  },
  {
    "text": "about moderation. It could also be really useful\nif you're sort of looking at a corpus of data that you're\ninterested in quantifying,",
    "start": "3549830",
    "end": "3556190"
  },
  {
    "text": "like, how much sexism is\nappearing in this corpus? Then you could do that with\nthese kinds of explanations.",
    "start": "3556190",
    "end": "3561920"
  },
  {
    "text": "But what I'm really\nexcited about, too, is that these explanations\ncould actually help authors",
    "start": "3561920",
    "end": "3567230"
  },
  {
    "text": "as they are writing\ntext by pointing out the maybe unintentional\nbiases in their text.",
    "start": "3567230",
    "end": "3573380"
  },
  {
    "text": "And so this opens the\ndoor for debiasing text through rewriting, which is\nthe next part of this talk",
    "start": "3573380",
    "end": "3579215"
  },
  {
    "text": "that I'm going to talk\nabout and the final project. But I will answer one\nmore question real quick.",
    "start": "3579215",
    "end": "3585745"
  },
  {
    "text": "This is not a\ntechnical question. But I tried to develop models\nto filter out offensive content, but I had to give up because it\nwas mentally exhausting to see",
    "start": "3585745",
    "end": "3591800"
  },
  {
    "text": "the offensive content myself. Yes. What do you think is the best\nway to protect the researcher and annotators themselves from\nthe effect of harmful target",
    "start": "3591800",
    "end": "3599509"
  },
  {
    "text": "data set during research? Could you share\nany personal tips? So I think that, personally,\nI am just kind of used",
    "start": "3599510",
    "end": "3608900"
  },
  {
    "text": "to staring at this data now. So I may be a little\nbit less affected by it",
    "start": "3608900",
    "end": "3614750"
  },
  {
    "text": "because I think that\nthere's something empowering about knowing that I'm doing\nsomething about the issue.",
    "start": "3614750",
    "end": "3621030"
  },
  {
    "text": "So that makes me a little bit\nless sort of affected by it. When it comes to\nannotators, I think",
    "start": "3621030",
    "end": "3628880"
  },
  {
    "text": "I always try to align\ntheir motivations with that same goal. We're trying to make\nthe internet less toxic. We're really trying to\nmake a difference here.",
    "start": "3628880",
    "end": "3635060"
  },
  {
    "text": "And that's why we're doing this. We're not just displaying\nthese awful things to you for a fun purpose.",
    "start": "3635060",
    "end": "3643160"
  },
  {
    "text": "Also, try to take breaks. I also have extended\nexperience being in therapy,",
    "start": "3643160",
    "end": "3649460"
  },
  {
    "text": "so that helps, having a\nsupport system and things like that, just really\nnot approaching this from a place where you're\nalready vulnerable.",
    "start": "3649460",
    "end": "3657140"
  },
  {
    "text": "But if you can have\na support system and take breaks and\nthings like that, that makes it a lot easier\nto do this kind of research.",
    "start": "3657140",
    "end": "3664093"
  },
  {
    "text": "But like I said, I think\nthere's something just kind of empowering about\nknowing that I'm trying to tackle an important problem.",
    "start": "3664093",
    "end": "3672773"
  },
  {
    "text": "OK, so let me talk about this\nlast part of the talk, which is PowerTransformer.",
    "start": "3672773",
    "end": "3678020"
  },
  {
    "text": "And so PowerTransformer is\nan unsupervised controllable revision model for biased\nlanguage correction.",
    "start": "3678020",
    "end": "3683110"
  },
  {
    "text": "And what I mean when I say\nbiased language correction here is we're looking at bias through\nthe lens of connotation frames",
    "start": "3683110",
    "end": "3688270"
  },
  {
    "text": "of power and agency. So what these words mean is,\nbasically, connotation frames",
    "start": "3688270",
    "end": "3694690"
  },
  {
    "text": "of power and agency are\na commonsense formalism that I introduced in\n2017 with my co-authors.",
    "start": "3694690",
    "end": "3702130"
  },
  {
    "text": "And it distills connotational\nknowledge related to verbs or verb predicates.",
    "start": "3702130",
    "end": "3709460"
  },
  {
    "text": "So for example, if you have,\nsomeone is pursuing something, this connotation frame is\ngoing to distill knowledge",
    "start": "3709460",
    "end": "3715510"
  },
  {
    "text": "about the power differential\nbetween the agent and the theme of the\nverb, so the object and the subject of the verb.",
    "start": "3715510",
    "end": "3721400"
  },
  {
    "text": "So in this case, when someone\nis pursuing something, it's kind of likely that the\nperson doing the pursuing",
    "start": "3721400",
    "end": "3727150"
  },
  {
    "text": "has less power than the person\nthat they're trying to pursue, that they're not able to get.",
    "start": "3727150",
    "end": "3733120"
  },
  {
    "text": "Also, the connotation frames\ncapture notions of agency that is attributed to the person\ndoing the action or the event.",
    "start": "3733120",
    "end": "3739869"
  },
  {
    "text": "And so in this case,\nsomeone who is high agency is someone who tends\nto be very decisive, active, driving change.",
    "start": "3739870",
    "end": "3746260"
  },
  {
    "text": "So someone who is\npursuing something is really driving that change\nof pursuing the thing, but also,",
    "start": "3746260",
    "end": "3751630"
  },
  {
    "text": "someone who is wiping, beating,\nfighting, shooting, scarring, things like that. Versus someone who's low\nagency tends to be very passive",
    "start": "3751630",
    "end": "3758770"
  },
  {
    "text": "and experiencing events. So that's maybe someone who's\ndoing more tripping on things, or sleeping, or viewing, or\ndozing, or dreaming, things",
    "start": "3758770",
    "end": "3766840"
  },
  {
    "text": "like that. And so we can actually use\nthese connotation frames to analyze all sorts of texts.",
    "start": "3766840",
    "end": "3773920"
  },
  {
    "text": "And specifically, in this\noriginal work in 2017, we analyzed the way\nthat characters were",
    "start": "3773920",
    "end": "3778960"
  },
  {
    "text": "portrayed in movie scripts-- modern movie scripts,\nI should say. And we wanted to\nstudy, basically,",
    "start": "3778960",
    "end": "3785170"
  },
  {
    "text": "the agency levels of characters\nwith respect to their gender.",
    "start": "3785170",
    "end": "3790408"
  },
  {
    "text": "And so just glossing\nover the details here. But what we found is that, in\nthese modern movie scripts, men were portrayed with\nmuch higher agency,",
    "start": "3790408",
    "end": "3797290"
  },
  {
    "text": "and women were portrayed\nwith much lower agency, unfortunately. And again, this is\nan example of what",
    "start": "3797290",
    "end": "3804570"
  },
  {
    "text": "I call the cycle of\nsocial inequality in text because in the world,\nmen unfortunately",
    "start": "3804570",
    "end": "3810030"
  },
  {
    "text": "have more societal\nand decision-making power than women. And this, in turn,\nthen transformed it.",
    "start": "3810030",
    "end": "3816330"
  },
  {
    "text": "And when we're\nwriting text, we tend to portray women as less\nagentic and more powerful than men because it's just\nkind of how the world works.",
    "start": "3816330",
    "end": "3823810"
  },
  {
    "text": "And so then, when\nwe read these texts, this reinforces our perceptions\nof gender roles and stereotypes",
    "start": "3823810",
    "end": "3829619"
  },
  {
    "text": "and we can just\ncontinue the cycle. But this motivated our new\ntask of controllable debiasing,",
    "start": "3829620",
    "end": "3836460"
  },
  {
    "text": "which basically is\nasking, can machines learn to revise text to\ndebias these portrayals and essentially break the cycle\nof social inequality in text?",
    "start": "3836460",
    "end": "3845829"
  },
  {
    "text": "And so specifically, the goal\nof controllable debiasing of our story sentences is to\ntake in a sentence, like, Mey",
    "start": "3845830",
    "end": "3852130"
  },
  {
    "text": "daydreams of being a doctor,\nwhere Mey is portrayed very passively, with low agency.",
    "start": "3852130",
    "end": "3858250"
  },
  {
    "text": "And we're going\nto rewrite it to, Mey pursues her dream\nto be a doctor, where Mey all of a sudden has much\nhigher agency than before.",
    "start": "3858250",
    "end": "3865600"
  },
  {
    "text": " So there's actually\ntwo challenges to doing this kind of thing.",
    "start": "3865600",
    "end": "3871310"
  },
  {
    "text": "The first one is that, contrary\nto some existing sort of work on rewriting, we cannot just\nparaphrase the sentences",
    "start": "3871310",
    "end": "3878420"
  },
  {
    "text": "because a lot of times, the\nbiases are actually not just in the framing of the\nactions that are attributed",
    "start": "3878420",
    "end": "3884810"
  },
  {
    "text": "to characters, but actually\nin the actions themselves.",
    "start": "3884810",
    "end": "3889830"
  },
  {
    "text": "And so on the\nother hand, we also want to avoid making\nunnecessary meaning changes and completely\nrewriting the sentence.",
    "start": "3889830",
    "end": "3895297"
  },
  {
    "text": "So we want to preserve as\nmuch of the meaning as we can while also still debiasing. And so what we want\nis targeted edits",
    "start": "3895298",
    "end": "3901760"
  },
  {
    "text": "with minimal meaning change.  A second challenge is\nthat this is essentially",
    "start": "3901760",
    "end": "3907890"
  },
  {
    "text": "an unsupervised task. And what I mean by\nthat is that there's no parallel input-output\npairs that can show a model,",
    "start": "3907890",
    "end": "3914700"
  },
  {
    "text": "this is exactly what you should\nrewrite this sentence to. And so the way that people have\ntackled this problem before",
    "start": "3914700",
    "end": "3920400"
  },
  {
    "text": "is using generator-discriminator\nmodels by basically setting up",
    "start": "3920400",
    "end": "3925522"
  },
  {
    "text": "a discriminator on top\nof your generation model and kind of like\nGAN-style things.",
    "start": "3925522",
    "end": "3930960"
  },
  {
    "text": "But oftentimes,\nresearch has shown that this leads to\nreally disfluent or less grammatical output text.",
    "start": "3930960",
    "end": "3937829"
  },
  {
    "text": "And so our approach\nis we're going to follow Li et al's approach\nof masking and reconstructing",
    "start": "3937830",
    "end": "3943349"
  },
  {
    "text": "sentences. But in order to\nreally do this fully, we had to add two novel\nmodeling aspects to this.",
    "start": "3943350",
    "end": "3949253"
  },
  {
    "text": "The first one is we added\nan additional paraphrasing training objective. And then at testing time\nor at generation time,",
    "start": "3949253",
    "end": "3956160"
  },
  {
    "text": "we're going to add a\nvocabulary boosting mechanism to reach the desired\nagency levels better.",
    "start": "3956160",
    "end": "3961315"
  },
  {
    "text": "All right, so let me\nwalk you through the way that PowerTransformer is set up. So the way this\nmodel works is we",
    "start": "3961315",
    "end": "3967260"
  },
  {
    "text": "start with an input\nsentence, like, Mey daydreams of being a doctor,\nand a desired agency level--",
    "start": "3967260",
    "end": "3972900"
  },
  {
    "text": "in this case, high agency. We're going to\nmask all the agency markers from the input sentence\nusing our connotation frames.",
    "start": "3972900",
    "end": "3981518"
  },
  {
    "text": "We're going to transform\nour desired agency level into a special token. We're going to feed those into\na Transformer-based conditional",
    "start": "3981518",
    "end": "3988770"
  },
  {
    "text": "language model, in this\ncase, initialized as GPT. And then at training\ntime, we're going",
    "start": "3988770",
    "end": "3994020"
  },
  {
    "text": "to use our joint reconstruction\nand paraphrasing objective. And then at test time\nor at generation time,",
    "start": "3994020",
    "end": "3999180"
  },
  {
    "text": "we're going to use our\nvocab-boosting mechanism on top of the probabilities that\nare given to us by the model",
    "start": "3999180",
    "end": "4004790"
  },
  {
    "text": "to even further reach\nthe desired agency level.",
    "start": "4004790",
    "end": "4010430"
  },
  {
    "text": "So to be a little\nbit more specific, the joint training\nobjective has two parts. The first one is an in-domain\nreconstruction objective,",
    "start": "4010430",
    "end": "4018990"
  },
  {
    "text": "which takes in sentences\nfrom our story corpus, masks the agency markers,\nand then optimizes",
    "start": "4018990",
    "end": "4024140"
  },
  {
    "text": "the log likelihood of the\nreconstructed sentence. And then the second one is\nan out-of-domain paraphrasing",
    "start": "4024140",
    "end": "4029540"
  },
  {
    "text": "objective, which basically uses\npairs of paraphrases from TV subtitles-- so a very different\ncorpus, very different domain--",
    "start": "4029540",
    "end": "4036770"
  },
  {
    "text": "and then optimizes the\nlikelihood of a sentence given its last paraphrase.",
    "start": "4036770",
    "end": "4042340"
  },
  {
    "text": "And then the way\nat decoding time that our vocab-boosting\nmechanism works is, essentially, it's\ngoing to increase",
    "start": "4042340",
    "end": "4048340"
  },
  {
    "text": "the likelihood of tokens that\nare connoting the right desired agency levels. And we're doing that\nby adding a vocab size",
    "start": "4048340",
    "end": "4055119"
  },
  {
    "text": "vector for the\nright agency levels at each decoding time step.",
    "start": "4055120",
    "end": "4060230"
  },
  {
    "text": "So basically, every time\nwe generate a new word, we're going to shift\nthe probabilities up",
    "start": "4060230",
    "end": "4065770"
  },
  {
    "text": "of the words that are kind\nof in the right direction of the right agency. And then we can do this\nshifting at different strengths",
    "start": "4065770",
    "end": "4073900"
  },
  {
    "text": "using a parameter beta here. So there's a lot of different\ncomponents in this model,",
    "start": "4073900",
    "end": "4079430"
  },
  {
    "text": "right? So we actually wanted to measure\nwhether all of these components actually mattered.",
    "start": "4079430",
    "end": "4085130"
  },
  {
    "text": "And just doing some ablation\nstudies where we looked at whether the vocab\nboosting actually helped,",
    "start": "4085130",
    "end": "4090670"
  },
  {
    "text": "or reconstruction, or\nthe joint objective, and things like\nthat, we actually do find a performance gain\nfrom both the vocab boosting",
    "start": "4090670",
    "end": "4096350"
  },
  {
    "text": "and the joint objective,\nlooking at whether or not we reach the desired\nagency levels, so just in terms of accuracy.",
    "start": "4096350",
    "end": "4103660"
  },
  {
    "text": "But this is a kind of\nopen-ended generation task where we have an input sentence and\nan output sentence that could",
    "start": "4103660",
    "end": "4109839"
  },
  {
    "text": "look like a lot of things. And so we don't really\nhave good automatic ways of evaluating this kind of\nopen-ended generation task,",
    "start": "4109840",
    "end": "4117099"
  },
  {
    "text": "unfortunately. As of today, it's really\nstill an open problem. If you're excited about\nevaluating text generation,",
    "start": "4117100",
    "end": "4122500"
  },
  {
    "text": "I encourage you to work on it\nbecause it's really exciting. But unfortunately, until we have\na really good text evaluation",
    "start": "4122500",
    "end": "4128560"
  },
  {
    "text": "system, we tend to rely on\nhuman evaluations instead to tell us how good\nthese outputs are.",
    "start": "4128560",
    "end": "4135649"
  },
  {
    "text": "And so that's what\nwe did in this paper. We designed a head-to-head\nevaluation task where we basically gave the\nraters the output of two",
    "start": "4135649",
    "end": "4143409"
  },
  {
    "text": "systems and then asked them\nwhich one better preserved the meaning and which one better\npreserved the agency compared to the original sentence.",
    "start": "4143410",
    "end": "4150339"
  },
  {
    "text": "And we actually compared our\nfull PowerTransformer model's outputs to the nonboosted\nversion of the model as well",
    "start": "4150340",
    "end": "4158170"
  },
  {
    "text": "as two baselines from related\ntasks, but different tasks. And what we find is that\nboth PowerTransformer models",
    "start": "4158170",
    "end": "4166750"
  },
  {
    "text": "actually are better at\npreserving the meaning than the previous baselines. And then the full\nPowerTransformer model",
    "start": "4166750",
    "end": "4173200"
  },
  {
    "text": "with the vocab boosting actually\nhas more accurate output agency levels compared to\nthe other baselines.",
    "start": "4173200",
    "end": "4182149"
  },
  {
    "text": "So given that we have evidence\nthat our model actually works well, we wanted in\nthis paper, in this project,",
    "start": "4182149",
    "end": "4187699"
  },
  {
    "text": "to circle back to\nthe movie scripts that I mentioned earlier and\nthen see how PowerTransformer",
    "start": "4187700",
    "end": "4192979"
  },
  {
    "text": "can help actually sort\nof mitigate these biases. ",
    "start": "4192979",
    "end": "4199570"
  },
  {
    "text": "So as you recall, in the\noriginal movie scripts, we found that men were portrayed\nwith higher positive agency",
    "start": "4199570",
    "end": "4204730"
  },
  {
    "text": "than women. And so we're going to\ndo a case study where we're going to rewrite\nthe lines that describe",
    "start": "4204730",
    "end": "4209987"
  },
  {
    "text": "female characters\nusing PowerTransformer, specifically trying to give\nthese characters more agency.",
    "start": "4209987",
    "end": "4215662"
  },
  {
    "text": "And so in the\nrevised scripts, what we find is that\nnot only do women have much higher\nagency than before,",
    "start": "4215662",
    "end": "4221860"
  },
  {
    "text": "but if we compare the gender\neffects, actually, all of a sudden, women\nhave much higher agency",
    "start": "4221860",
    "end": "4228770"
  },
  {
    "text": "than male characters,\nstatistically significantly so. So these coefficients\nlook really big.",
    "start": "4228770",
    "end": "4235520"
  },
  {
    "text": "And I want to sort\nof give a caveat here that this is a very broad-swath\napproach to debiasing language",
    "start": "4235520",
    "end": "4242630"
  },
  {
    "text": "because we rewrote every\nsingle character that was detected as being female. But I think, here,\nwhat this actually",
    "start": "4242630",
    "end": "4248930"
  },
  {
    "text": "illustrates is the\npromise for using human-AI collaborative writing\nsetups to help people maybe write less\nstereotypically, so kind",
    "start": "4248930",
    "end": "4255030"
  },
  {
    "text": "of enhancing these writing sort\nof platforms or these tools that people are using\nto write to give them",
    "start": "4255030",
    "end": "4261290"
  },
  {
    "text": "more information about how what\nthey write might be perceived. So glossing over\nthe contributions",
    "start": "4261290",
    "end": "4268713"
  },
  {
    "text": "of PowerTransformer here,\nwe introduced the new task of controllable debiasing. I talked about these\nconnotation frames,",
    "start": "4268713",
    "end": "4273910"
  },
  {
    "text": "which are a new\ncommonsense formalism. And then we introduced a\nnew model, PowerTransformer.",
    "start": "4273910",
    "end": "4279310"
  },
  {
    "text": "And we showed in the case\nstudy that we could actually automatically mitigate\nsome of the gender biases in movie scripts.",
    "start": "4279310",
    "end": "4286930"
  },
  {
    "text": "And so this concludes\nthe last sort of part of my talk\nabout projects.",
    "start": "4286930",
    "end": "4291990"
  },
  {
    "text": "And I wanted to briefly walk\nthrough some future directions. And hopefully, we can\nget some more discussion going as well on that.",
    "start": "4291990",
    "end": "4299110"
  },
  {
    "text": "I know that I think I have\nabout 15 minutes left. So to recap, in this talk,\nI talked about racial bias",
    "start": "4299110",
    "end": "4305158"
  },
  {
    "text": "in hate speech\ndetection and proposed some race-aware\nannotation strategies to maybe mitigate this. Then I talked about\nSocial Bias Frames",
    "start": "4305158",
    "end": "4311740"
  },
  {
    "text": "as a new way of even viewing\nthe problem of hate speech detection by looking at the\nbias and harmful implications",
    "start": "4311740",
    "end": "4317050"
  },
  {
    "text": "of language instead. And then, finally, I showed\nwith PowerTransformer that we can revise\nand debias text",
    "start": "4317050",
    "end": "4322840"
  },
  {
    "text": "through the lens of\nconnotation frames. But there's a lot\nmore to be done towards this sort of broader\ngoal of avoiding and mitigating",
    "start": "4322840",
    "end": "4329949"
  },
  {
    "text": "biases in language with\nmore human-centric models. And so I'm really excited\nabout furthering this research",
    "start": "4329950",
    "end": "4337270"
  },
  {
    "text": "and understanding how\nsocial biases show up in human-written language. So this requires\nlooking at how we",
    "start": "4337270",
    "end": "4343150"
  },
  {
    "text": "can do better at\ndetecting social biases and toxicity in language. So I'm really excited\nabout working more",
    "start": "4343150",
    "end": "4349059"
  },
  {
    "text": "on developing formalisms for\ncontextual bias representation. So harkening back to a question\nthat was there earlier--",
    "start": "4349060",
    "end": "4355960"
  },
  {
    "text": "so for example, a lot\nof these representations have ignored the conversational\ncontext in which things appear.",
    "start": "4355960",
    "end": "4362620"
  },
  {
    "text": "And actually, the\npragmatic implications of conversation\nin terms of biases are actually not\ntrivial to capture.",
    "start": "4362620",
    "end": "4368890"
  },
  {
    "text": "For example, you could say\nsomething really offensive, and then someone else\ncould say, oh, I agree. And then, of course, the\nfirst utterance is offensive.",
    "start": "4368890",
    "end": "4376034"
  },
  {
    "text": "But then what do you do\nabout the second utterance? Knowing the stance towards\noffensive utterances",
    "start": "4376035",
    "end": "4381370"
  },
  {
    "text": "is also really important. And so we studied that\na little bit in a work called ToxiChat that was\npublished at EMNLP 2021.",
    "start": "4381370",
    "end": "4390500"
  },
  {
    "text": "As I showed, there's\na lot of differences in terms of who the speaker\nand listener and annotators are",
    "start": "4390500",
    "end": "4396409"
  },
  {
    "text": "when it comes to\nthese kinds of sort of social biases in language. And so we need to\nkeep looking at how",
    "start": "4396410",
    "end": "4403340"
  },
  {
    "text": "those identities and\nthe power dynamics between those identities\naffect the implications.",
    "start": "4403340",
    "end": "4409370"
  },
  {
    "text": "Of course, our Social Bias\nFrame model wasn't perfect. And so we need better models\nto do better, deeper reasoning",
    "start": "4409370",
    "end": "4414950"
  },
  {
    "text": "about biases in text. And as I mentioned, I'm really\nexcited about this whole sort of application of rewriting\ntext to debias it,",
    "start": "4414950",
    "end": "4422750"
  },
  {
    "text": "and so potentially using other\ndimensions than the connotation",
    "start": "4422750",
    "end": "4428150"
  },
  {
    "text": "frames of power and agency, then\nmaybe using Social Bias Frames instead, things like that. And this also\nrequires studying how",
    "start": "4428150",
    "end": "4435199"
  },
  {
    "text": "humans would react to these\nkinds of inputs from models. Is someone going to be\nmore or less offended if a model says that\ntheir text might be biased",
    "start": "4435200",
    "end": "4442969"
  },
  {
    "text": "versus a human would say that? On the flip side, I'm\nalso really interested,",
    "start": "4442970",
    "end": "4448160"
  },
  {
    "text": "and I think it's\nreally important to keep looking at how we can\navoid biases and toxicities in machine-generated language.",
    "start": "4448160",
    "end": "4453989"
  },
  {
    "text": "And so I've done some\nwork on scrutinizing the biases and toxicity in these\npretrained language models--",
    "start": "4453990",
    "end": "4459740"
  },
  {
    "text": "for example, looking at how\nquickly a language model could generate toxicity,\nbut also how, what",
    "start": "4459740",
    "end": "4467239"
  },
  {
    "text": "about the pre-training\ndata and the way that pre-training\ndata was selected, affects the types of\nbiases and toxicity",
    "start": "4467240",
    "end": "4473360"
  },
  {
    "text": "that is in these machines. So for example, in some\nrecent work, we found that--",
    "start": "4473360",
    "end": "4478940"
  },
  {
    "text": "I think that was published\nat EMNLP 2021 as well-- we basically found that the\nAfrican American English",
    "start": "4478940",
    "end": "4484970"
  },
  {
    "text": "was really likely to be\nremoved from automatic data sort of quality filters that\nGoogle uses for their corpora.",
    "start": "4484970",
    "end": "4492000"
  },
  {
    "text": "So there's some problematic\nthings about that as well. Also really excited\nabout finding ways",
    "start": "4492000",
    "end": "4498220"
  },
  {
    "text": "to steer methods that\ncan help avoid toxicity. And so this is also\nkind of hearkening back",
    "start": "4498220",
    "end": "4503620"
  },
  {
    "text": "at the earlier question of,\nwhat about the pretraining data or the training data\nversus the test time",
    "start": "4503620",
    "end": "4508900"
  },
  {
    "text": "sort of behaviorial models? And like I said, I think\nwe should tackle both. And I'm really excited about\ntackling both these things.",
    "start": "4508900",
    "end": "4515050"
  },
  {
    "text": "So how can we steer models\nthat are already trained, and we know that they\nmay have toxic behavior? How can we steer them,\nnonetheless, to be less toxic?",
    "start": "4515050",
    "end": "4522880"
  },
  {
    "text": "So we have some work called\nDExperts, where we basically learned from the worst, in\na way, where we basically have a really,\nreally toxic LM that",
    "start": "4522880",
    "end": "4529540"
  },
  {
    "text": "is an anti-role model for\nthe standard language model. And so it's basically like your\nreally, really racist uncle.",
    "start": "4529540",
    "end": "4536428"
  },
  {
    "text": "And so if you know that your\nracist uncle would say it, then you know that you\nshouldn't say it, basically.",
    "start": "4536428",
    "end": "4542920"
  },
  {
    "text": "And I think, in terms of\nlooking at steering these models and avoiding toxicity in\nmachine-generated text,",
    "start": "4542920",
    "end": "4548080"
  },
  {
    "text": "we should think about\nexpanding what we should avoid and not just\navoiding swear words,",
    "start": "4548080",
    "end": "4553119"
  },
  {
    "text": "but also, maybe we want\nour models to be prosocial. And maybe we want them to not\nviolate social norms in general",
    "start": "4553120",
    "end": "4559270"
  },
  {
    "text": "and just follow\ncommunity norms more. And specifically, maybe we want\nto allow for personalization",
    "start": "4559270",
    "end": "4566440"
  },
  {
    "text": "so that it's not just\none culture that's being represented in our\nmachine learning models that",
    "start": "4566440",
    "end": "4572122"
  },
  {
    "text": "could be personalized\nto your own culture and things like that. And then, finally, I'm\nreally excited about bridging",
    "start": "4572122",
    "end": "4579020"
  },
  {
    "text": "the gap between NLP\nand social sciences in general, so asking\nwhat social sciences can",
    "start": "4579020",
    "end": "4584420"
  },
  {
    "text": "do for improving NLP systems-- so for example, looking at how\npsychology and sociolinguistics",
    "start": "4584420",
    "end": "4589670"
  },
  {
    "text": "can help us understand how\npeople label or perceive offensiveness in text. But also, in general, how can\ncognitive science findings",
    "start": "4589670",
    "end": "4598665"
  },
  {
    "text": "sort of help us understand\nhow cognitive biases might appear in crowdsourcing\ntasks in general?",
    "start": "4598665",
    "end": "4604340"
  },
  {
    "text": "And then the\nconverse of using NLP to answer social\nscience questions is also really interesting. So for example, we know\nthat misinformation in news",
    "start": "4604340",
    "end": "4611809"
  },
  {
    "text": "is another big issue that's\ngoing on online, where people are sharing\nmisinformation or propaganda.",
    "start": "4611810",
    "end": "4617660"
  },
  {
    "text": "But can we make machine\nlearning or NLP systems that can help us fight\nthis by, for example,",
    "start": "4617660",
    "end": "4622880"
  },
  {
    "text": "distilling common tropes\nthat are evoked by headlines or producing or generating the\nlikely reactions that someone",
    "start": "4622880",
    "end": "4629997"
  },
  {
    "text": "would have to a headline so\nthat we know, when someone's really afraid, that\nmaybe we should be careful about\nframing something",
    "start": "4629997",
    "end": "4637010"
  },
  {
    "text": "in a very fearmongery way? And in general, creating methods\nfor analyzing social phenomena",
    "start": "4637010",
    "end": "4642922"
  },
  {
    "text": "in text is also\nreally important so that we can answer social\nscience questions for that. All right, that\nconcludes my talk.",
    "start": "4642922",
    "end": "4649580"
  },
  {
    "text": "And I'm really excited to\nhave been able to share my work with you all. And I want to thank\nyou all for listening.",
    "start": "4649580",
    "end": "4655020"
  },
  {
    "text": "And I specifically also want\nto thank my collaborators because without them, none of\nthis work that I just talked about could have been possible.",
    "start": "4655020",
    "end": "4661080"
  },
  {
    "text": "And I'll take more\nquestions now. I'll answer questions.",
    "start": "4661080",
    "end": "4666130"
  },
  {
    "text": "So in places where keyword\ncensorship is enforced, I observe that people\nchange how they speak, using code words or\nacronyms to avoid censorship.",
    "start": "4666130",
    "end": "4672150"
  },
  {
    "text": "But fundamentally,\nit is not that effective in actually preventing\npeople from talking about it. Hate speech detection is,\nof course, more complicated",
    "start": "4672150",
    "end": "4678780"
  },
  {
    "text": "and harder to evade. Do you think that, over\ntime, maybe people will still find a way to\navoid it and render the whole system ineffective?",
    "start": "4678780",
    "end": "4684000"
  },
  {
    "text": "Yeah, absolutely. And this is kind of\na dual-use question, too, because some moderation is\nreally meant to protect people.",
    "start": "4684000",
    "end": "4692039"
  },
  {
    "text": "For example, there's work from-- I think it's HCI people-- that looked at the proanorexia\ncommunities online.",
    "start": "4692040",
    "end": "4700560"
  },
  {
    "text": "And there's a constant\nbattle of people that are sort of trying to get\npeople to become more anorexic,",
    "start": "4700560",
    "end": "4706170"
  },
  {
    "text": "learning new ways to promote the\nhashtags and things like that. And then the platforms\nand the moderators, sort of falling behind,\nare just trying to follow",
    "start": "4706170",
    "end": "4713190"
  },
  {
    "text": "and being like, OK, this\nhashtag now means this, and things like that. So in the grand scheme of\nthings, it's like, well,",
    "start": "4713190",
    "end": "4719830"
  },
  {
    "text": "shouldn't we moderate this? And so it's definitely\na tricky question there.",
    "start": "4719830",
    "end": "4725460"
  },
  {
    "text": "But yeah, I think\nif we can, if we can tackle-- if we can set\na rule around the biases",
    "start": "4725460",
    "end": "4731640"
  },
  {
    "text": "that we want to\nmoderate, there's always going to be people that are\ngoing to try to evade it,",
    "start": "4731640",
    "end": "4737140"
  },
  {
    "text": "for sure, yeah. Yeah, I think that-- I don't actually know\nif there's a good answer for this question.",
    "start": "4737140",
    "end": "4743198"
  },
  {
    "text": "But that's a really astute\nobservation, for sure.  OK, said, something--\nI'm curious",
    "start": "4743198",
    "end": "4749065"
  },
  {
    "text": "if there is such a thing as\nkid-friendly or kid-aware LM. Kids are spending record\ntime on devices online. This is certainly\na governance issue,",
    "start": "4749065",
    "end": "4754420"
  },
  {
    "text": "but how do you incorporate\nage into the model to make technology\ntruly user-friendly? Yeah, that's hard\nbecause who decides",
    "start": "4754420",
    "end": "4760800"
  },
  {
    "text": "what's appropriate for\nkids or not, right? There's an anti-gay\nbill in Florida that's trying to be pushed\nwhere people don't want",
    "start": "4760800",
    "end": "4768159"
  },
  {
    "text": "to talk about homosexuality. That's a very political\nstance because there are kids that are gay\nand struggling with that.",
    "start": "4768160",
    "end": "4774530"
  },
  {
    "text": "And so I just want to raise\nmore questions, honestly, because I'm not going to\npronounce myself on that.",
    "start": "4774530",
    "end": "4781610"
  },
  {
    "text": " I don't know that-- I think that protecting\nchildren is oftentimes",
    "start": "4781610",
    "end": "4789530"
  },
  {
    "text": "a reason that is\nused as an excuse to justify censorship\nand other things.",
    "start": "4789530",
    "end": "4794740"
  },
  {
    "text": "So I think we should be careful\nabout that kind of argument, though. Are we actually looking at what\nis actually harming children?",
    "start": "4794740",
    "end": "4800690"
  },
  {
    "text": "And what are the biases of\nthose studies versus people just being morally\noutraged by something",
    "start": "4800690",
    "end": "4806090"
  },
  {
    "text": "and saying that\ntheir children should be protected from it instead? So it's not clear.",
    "start": "4806090",
    "end": "4811525"
  },
  {
    "text": "And also, I think,\nin general, we should be really\nregulating the usage of AI and technology for\nchildren because we do",
    "start": "4811525",
    "end": "4817525"
  },
  {
    "text": "want to protect\nthem from things. That's for sure. ",
    "start": "4817525",
    "end": "4823639"
  },
  {
    "text": "What techniques can be used\nto detect social biases and hate speech in languages\nthat we don't have large, curated, labeled data sets for?",
    "start": "4823640",
    "end": "4829350"
  },
  {
    "text": "That's a really good question. I don't know that\nit's that possible.",
    "start": "4829350",
    "end": "4837570"
  },
  {
    "text": "I mean, there's this sort\nof low-resource language approaches that\npeople would use--",
    "start": "4837570",
    "end": "4843450"
  },
  {
    "text": "maybe multilingual\ntransfer-type things could help. But I think, with\nthese kinds of things,",
    "start": "4843450",
    "end": "4850870"
  },
  {
    "text": "I'm more interested in what the\nhumans that are being affected think. And so we would need\nsome sort of information",
    "start": "4850870",
    "end": "4856650"
  },
  {
    "text": "about what they like and\nwhat they don't like in order to be able to tackle\nthis undesirable language",
    "start": "4856650",
    "end": "4864390"
  },
  {
    "text": "that we're trying\nto remove or detect.",
    "start": "4864390",
    "end": "4869880"
  },
  {
    "text": "I wonder if you've thought about\nsome of the potential risks or ways in which this type\nof work could be misused.",
    "start": "4869880",
    "end": "4875370"
  },
  {
    "text": "Yeah, so people talk about\ndual use a lot in this case. So the issue is that\nPandora's box has been opened.",
    "start": "4875370",
    "end": "4884160"
  },
  {
    "text": "The internet exists. And it's just rampant with\nhate speech right now. And it's pushing people\nout in a lot of ways.",
    "start": "4884160",
    "end": "4890679"
  },
  {
    "text": "And so not doing anything\nabout that problem is kind of an implicit\nendorsement of the fact",
    "start": "4890680",
    "end": "4896190"
  },
  {
    "text": "that minority people are being\npushed out of these platforms or are being silenced\nby these algorithms.",
    "start": "4896190",
    "end": "4901290"
  },
  {
    "text": "So we don't really have\na choice to just go back to a world where\nthis isn't an issue, and we can just pretend like\nit doesn't exist anymore.",
    "start": "4901290",
    "end": "4908969"
  },
  {
    "text": "I like to work on making\nthings a little bit better because I think that\npeople are being censored,",
    "start": "4908970",
    "end": "4914610"
  },
  {
    "text": "and that's wrong. But it's totally valid\nif you would rather not work on it because you're\nafraid that your technology is",
    "start": "4914610",
    "end": "4921840"
  },
  {
    "text": "going to backfire even more. That's for sure. But I think there's\nno good sort of like--",
    "start": "4921840",
    "end": "4926880"
  },
  {
    "text": "we're already in an ethically\nbad situation, unfortunately. So I have definitely\nthought about this a lot.",
    "start": "4926880",
    "end": "4933300"
  },
  {
    "text": " OK, have you faced\nany opposition",
    "start": "4933300",
    "end": "4940360"
  },
  {
    "text": "in applying your research\nin toxicity detection? Yeah, so kind of\nthe same question. I think people get\nreally freaked out",
    "start": "4940360",
    "end": "4945810"
  },
  {
    "text": "that they're going\nto get censored. And it's like, the answer\nis that people are already being censored. Maybe it's not you.",
    "start": "4945810",
    "end": "4952560"
  },
  {
    "text": "But there's already\npeople being censored. So again, cat's out of the bag. I don't think that it's like,\nwe're developing technology",
    "start": "4952560",
    "end": "4959880"
  },
  {
    "text": "that is just not solving\na real-world problem and just creating more problems. The internet is\nalready existing,",
    "start": "4959880",
    "end": "4965710"
  },
  {
    "text": "and so social media is. And we should keep working\nwith legislators, as well, to prevent people from\noverstepping the power",
    "start": "4965710",
    "end": "4973170"
  },
  {
    "text": "that they have. So for example, I think\nconsidering social media platforms not what a company\ncan dictate everything on,",
    "start": "4973170",
    "end": "4980610"
  },
  {
    "text": "but considering them like\npublic forums and sort of having regulations\naround what can and can't",
    "start": "4980610",
    "end": "4985740"
  },
  {
    "text": "be said in public forums\nand looking at those and seeing what applies\nto social media platforms",
    "start": "4985740",
    "end": "4991200"
  },
  {
    "text": "is important. Would you rather have\nTurkers, labelers for offensive and\ninoffensive text in corpora",
    "start": "4991200",
    "end": "4997000"
  },
  {
    "text": "be more or less ideologically,\nculturally congruous? What metrics should we\nuse to disqualify someone from labeling text, if any?",
    "start": "4997000",
    "end": "5003350"
  },
  {
    "text": "Yeah, so again, that's where\npositionality of researchers comes in. I didn't want to\ncreate a data set that",
    "start": "5003350",
    "end": "5011400"
  },
  {
    "text": "was all about flagging\nanti-white statements because that's not\na technology that I",
    "start": "5011400",
    "end": "5017070"
  },
  {
    "text": "believe is reflecting the\nactual harms that people are experiencing. If a researcher wants to\ndo that, they're free to.",
    "start": "5017070",
    "end": "5024840"
  },
  {
    "text": "I can't control what they do. But I think, yeah, this\nis kind of a part where",
    "start": "5024840",
    "end": "5030630"
  },
  {
    "text": "any technology that\nwe develop comes from a place of our own biases\nand our own positionality.",
    "start": "5030630",
    "end": "5036320"
  },
  {
    "text": "So we have to recognize that\nno matter what we do., right? And it's not just because\nwe're tackling something that's",
    "start": "5036320",
    "end": "5043752"
  },
  {
    "text": "politically charged\nall of a sudden that that's the only\ncases that these kinds of positionality things\naffect our research.",
    "start": "5043752",
    "end": "5051489"
  },
  {
    "text": "And we have a live question.  Hi, thank you for the talk. It's really enlightening.",
    "start": "5051490",
    "end": "5057510"
  },
  {
    "text": "I was wondering--\nthere's always going to be a gap between the\nvalues of the model builders and the values of the people\nwho are using these models.",
    "start": "5057510",
    "end": "5065070"
  },
  {
    "text": "And what do you think we can do\nin creating some sort of system where--",
    "start": "5065070",
    "end": "5070610"
  },
  {
    "text": "yeah, these values\nwill constantly evolve. We're constantly\nchanging what we think is moral and not\nmoral, biased and not biased.",
    "start": "5070610",
    "end": "5077010"
  },
  {
    "text": "And these values have evolved\nover thousands of years. But now, with AI,\nthey're evolving",
    "start": "5077010",
    "end": "5082200"
  },
  {
    "text": "over minutes and seconds. Are there ways to\nmake sure we come up",
    "start": "5082200",
    "end": "5087540"
  },
  {
    "text": "with a good set of\nvalues but can adapt to kind of the changing\nvalues of people who are using the models as well?",
    "start": "5087540",
    "end": "5094800"
  },
  {
    "text": "Yeah, I mean, I think,\nultimately, our approaches should be more human-centric\nthan they currently are",
    "start": "5094800",
    "end": "5101790"
  },
  {
    "text": "and looking at who the\nstakeholders of this technology are. And we're just really starting\nto scratch the surface only",
    "start": "5101790",
    "end": "5108659"
  },
  {
    "text": "of bridging that gap between\nthe developers and the people that are being affected by this.",
    "start": "5108660",
    "end": "5114760"
  },
  {
    "text": "And so we need to keep doing\nwork, just uncovering that link and how people\nare being affected",
    "start": "5114760",
    "end": "5121050"
  },
  {
    "text": "and looking at what\nthose people want and doing that kind of\nvalue-sensitive design,",
    "start": "5121050",
    "end": "5126688"
  },
  {
    "text": "looking at what the\ncommunities that are actually going to be affected by this\ntechnology want out of it.",
    "start": "5126688",
    "end": "5132310"
  },
  {
    "text": "But just like any technology,\nnothing should be static. And if the world\nchanges, we should",
    "start": "5132310",
    "end": "5137940"
  },
  {
    "text": "be changing our technology with\nit as well to adapt with it. And so yeah, it's\nkind of a nonanswer",
    "start": "5137940",
    "end": "5145225"
  },
  {
    "text": "because it's like,\nyeah, we should just be talking to people and\nasking them and getting out of the just NLP part of it and\njust really going to the users",
    "start": "5145225",
    "end": "5153510"
  },
  {
    "text": "and asking them, what\nare they experiencing? Thank you.",
    "start": "5153510",
    "end": "5158740"
  },
  {
    "text": " So asked, how does agency\nin PowerTransformer",
    "start": "5158740",
    "end": "5164670"
  },
  {
    "text": "actually quantify a character\nto be positive or negative? Are these based on actions,\nintent, emotional strength",
    "start": "5164670",
    "end": "5169800"
  },
  {
    "text": "of characters being portrayed? So it's about the verbs\nthat are being used to describe these characters.",
    "start": "5169800",
    "end": "5175270"
  },
  {
    "text": "So that's how we measure\nthe agency of a character, basically. So are they doing\na lot of chewing, or are they doing a lot\nof sleeping, basically,",
    "start": "5175270",
    "end": "5182820"
  },
  {
    "text": "to simplify. And it's just a lexicon, so\nyou can download the lexicon",
    "start": "5182820",
    "end": "5188340"
  },
  {
    "text": "and look at it as well. In all the current\nsocial media platforms, images also play\na crucial role--",
    "start": "5188340",
    "end": "5194140"
  },
  {
    "text": "more like a combination\nof image and text-- very important for detecting\nhate speech or other biases. Do you plan to explore? Yes. Yeah, I'm really excited.",
    "start": "5194140",
    "end": "5199832"
  },
  {
    "text": "I'm actually meeting\nwith some OpenAI folks next week to talk about\nmultimodal toxicity detection",
    "start": "5199832",
    "end": "5206560"
  },
  {
    "text": "and things like that. That's a whole other ball game. Unfortunately, my training\nis in just language,",
    "start": "5206560",
    "end": "5213093"
  },
  {
    "text": "so I don't really have a\nlot of expertise in vision. But I'm really excited to\ntry seeing how that works.",
    "start": "5213093",
    "end": "5218237"
  },
  {
    "text": "There's still a lot of places\nwhere it's just text only, though, so I'm confident that\nI'll still have some work",
    "start": "5218237",
    "end": "5223599"
  },
  {
    "text": "to do, just in that.  Thank you for an amazing chat.",
    "start": "5223600",
    "end": "5229750"
  },
  {
    "text": "My question is how to\nkeep social bias models we've talked about this\nlecture updated to the social-- OK, so that's kind of what\nsomeone else asked as well.",
    "start": "5229750",
    "end": "5236290"
  },
  {
    "text": "I think that's the answer. We just need to\nkeep updating things and just not think\nof models as static.",
    "start": "5236290",
    "end": "5242650"
  },
  {
    "text": "And if we can make our models\nbe able to be conditioned upon a large set of rules\nor things that we know",
    "start": "5242650",
    "end": "5248830"
  },
  {
    "text": "are problematic, and we can\nupdate that, then our model can inherently be\nupdated just by having",
    "start": "5248830",
    "end": "5255550"
  },
  {
    "text": "the new data accessible. We have gone to 4:45, Maarten.",
    "start": "5255550",
    "end": "5260700"
  },
  {
    "text": "OK, sorry. I mean, it's up to you. If you're just powering\nthrough and want to answer a couple more\nquestions, you're welcome to.",
    "start": "5260700",
    "end": "5267470"
  },
  {
    "text": "But you're also welcome to\ntake a deep sigh and stop. I feel like I want to\nanswer everyone's questions",
    "start": "5267470",
    "end": "5274099"
  },
  {
    "text": "because they're all so good. But I should probably stop here,\njust in the interest of time and being respectful of\neveryone who can't listen",
    "start": "5274100",
    "end": "5281510"
  },
  {
    "text": "to all the other questions. But this was great. I'm really excited about all\nthe questions that we got.",
    "start": "5281510",
    "end": "5287630"
  },
  {
    "text": "It's been a minute\nsince I've been interrupted in the middle\nof talks for questions. So I wanted to\nanswer all of them",
    "start": "5287630",
    "end": "5293143"
  },
  {
    "text": "and try to get\nthrough everything. But this is awesome, yeah. Well, thank you so much\nfor giving this talk",
    "start": "5293143",
    "end": "5299329"
  },
  {
    "text": "and for your rich\ninsights into the area. Yeah, of course. ",
    "start": "5299330",
    "end": "5308010"
  }
]