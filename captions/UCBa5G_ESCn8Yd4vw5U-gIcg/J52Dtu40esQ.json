[
  {
    "start": "0",
    "end": "5500"
  },
  {
    "text": "All right, welcome\nback, everyone, day two. We got a lot we want\nto accomplish today.",
    "start": "5500",
    "end": "12590"
  },
  {
    "text": "What I have on the\nscreen right now is the home base for the course. This is our public website,\nand you could think of it",
    "start": "12590",
    "end": "20350"
  },
  {
    "text": "as kind of a hub for everything\nthat you'll need in the course. You can see along the top here\nwe've got some policy pages.",
    "start": "20350",
    "end": "28990"
  },
  {
    "text": "There's a whole\npage on projects. There's a page that provides an\nindex of background materials,",
    "start": "28990",
    "end": "36280"
  },
  {
    "text": "YouTube screencasts,\nslides, hands-on materials in case you need to fill\nin some background stuff.",
    "start": "36280",
    "end": "43640"
  },
  {
    "text": "Notice also I do a podcast that\nactually began in this course last year.",
    "start": "43640",
    "end": "48880"
  },
  {
    "text": "And I found it so\nrewarding that I just continued doing it all year. And so new episodes\ncontinue to appear.",
    "start": "48880",
    "end": "55180"
  },
  {
    "text": "If you have ideas for\nguests for this podcast, feel free to suggest them. I'm always looking for\nexciting people to interview.",
    "start": "55180",
    "end": "62860"
  },
  {
    "text": "And I think the back episodes\nare also really illuminating. That's along the top.",
    "start": "62860",
    "end": "67880"
  },
  {
    "text": "Then over here on\nthe left, you've got one-stop shopping\nfor the various systems",
    "start": "67880",
    "end": "73017"
  },
  {
    "text": "that we have to deal with. You've got our Ed\nforum for discussion. If you're not in\nthere, let us know.",
    "start": "73017",
    "end": "78080"
  },
  {
    "text": "We can get you signed up. Canvas is your home for\nthe screencasts and also",
    "start": "78080",
    "end": "84080"
  },
  {
    "text": "the quizzes and I guess,\nthere's some other stuff there. Gradescope is where you'll\nsubmit the main assignments,",
    "start": "84080",
    "end": "90659"
  },
  {
    "text": "including your project work\nand also enter the bake offs. And then we have\nour course GitHub.",
    "start": "90660",
    "end": "96290"
  },
  {
    "text": "And that is the\ncourse code that will be depending on\nfor the assignments and that I hope you can build\non for the original work",
    "start": "96290",
    "end": "102650"
  },
  {
    "text": "that you do. If you need to reach us, you\ncan use the discussion forum, but we also have this\nstaff email address.",
    "start": "102650",
    "end": "108740"
  },
  {
    "text": "That is vastly preferred to\nwriting to us individually.",
    "start": "108740",
    "end": "113840"
  },
  {
    "text": "It really helps us\nmanage the workload and know what's\nhappening if you either ping us on the discussion forum,\nprivate post, public post,",
    "start": "113840",
    "end": "122330"
  },
  {
    "text": "whatever, or use\nthat staff address. In the middle of\nthis page here, we've",
    "start": "122330",
    "end": "129820"
  },
  {
    "text": "got links to all the materials. The first column\nis slides and stuff",
    "start": "129820",
    "end": "135040"
  },
  {
    "text": "like that and also notebooks. The middle column, it's\ncore readings mostly.",
    "start": "135040",
    "end": "140349"
  },
  {
    "text": "I'm not presupposing\nthat you will manage to do all of this reading\nbecause there is a lot of it,",
    "start": "140350",
    "end": "146710"
  },
  {
    "text": "but these are important\nand rewarding papers. And so at some\npoint in your life,",
    "start": "146710",
    "end": "151788"
  },
  {
    "text": "you might want to\nimmerse yourself in them. But I'm hoping that\nI can be your trusted guide through that literature.",
    "start": "151788",
    "end": "158330"
  },
  {
    "text": "And then on the right,\nyou have the assignments. That's the website.",
    "start": "158330",
    "end": "165069"
  },
  {
    "text": "Questions, comments,\nanything I could clear up? I have a time budgeted\nlater to review the policies",
    "start": "165070",
    "end": "170800"
  },
  {
    "text": "and required work in\na bit more detail. But if there are questions\nnow, I'm happy to take them.",
    "start": "170800",
    "end": "176050"
  },
  {
    "text": "Yes. For the quizzes, are the\nquizzes doable on the day that they become\navailable or do we",
    "start": "176050",
    "end": "181989"
  },
  {
    "text": "need like the\ncourse material all the way up through the unit?",
    "start": "181990",
    "end": "186991"
  },
  {
    "text": "That is a good question. It's going to depend\non your background. But in the worst case, if\nthis is all brand new to you,",
    "start": "186991",
    "end": "194350"
  },
  {
    "text": "you might not feel like\nyou can confidently finish the quiz until that\nfinal lecture in the unit.",
    "start": "194350",
    "end": "201310"
  },
  {
    "text": "This one is all\nabout transformers. All of the answers are\nembedded in this handout here.",
    "start": "201310",
    "end": "207439"
  },
  {
    "text": "But if you want to\nhear it from me, you might not hear that\nuntil next Thursday. But that gives you\nanother five days.",
    "start": "207440",
    "end": "214099"
  },
  {
    "text": "Perfect. Thank you. Yes. You mentioned past\nprojects are available.",
    "start": "214100",
    "end": "220120"
  },
  {
    "text": "Where can we find them? That must be here. I think I've got an\nindex of past projects",
    "start": "220120",
    "end": "227170"
  },
  {
    "text": "behind a protected\nlink, which will depend on you being enrolled. If you're not enrolled,\nwe can get you past that little hurdle.",
    "start": "227170",
    "end": "233709"
  },
  {
    "text": "But I did get permission\nto release some of them. So somewhere on this\npage is a link to--",
    "start": "233710",
    "end": "238995"
  },
  {
    "text": "oh, there it is\nexemplary projects. There's another list at\nthe GitHubprojects.md",
    "start": "238995",
    "end": "247099"
  },
  {
    "text": "Page, which is also\nlinked somewhere in here of published work. And that's stuff\nyou could download. The private link gives you\nthe actual course submission.",
    "start": "247100",
    "end": "255020"
  },
  {
    "text": "That could be an\ninteresting exercise to compare the paper they did\nin here with the thing they",
    "start": "255020",
    "end": "260028"
  },
  {
    "text": "actually published. And I'll emphasize again\nthat will be interesting because of how much\nwork it typically",
    "start": "260029",
    "end": "265310"
  },
  {
    "text": "takes to go from a class project\nto something that makes it onto the ACL Anthology.",
    "start": "265310",
    "end": "270919"
  },
  {
    "text": "But that's of course\nan exciting journey. ",
    "start": "270920",
    "end": "277142"
  },
  {
    "text": "Oh, yeah, and if you haven't\nalready, do this course setup. It's very lightweight. You get your computing\nenvironment set up",
    "start": "277142",
    "end": "283050"
  },
  {
    "text": "to use our code. And actually, this is a\nsign of the changing times. I also exhort you to sign\nup for a bunch of services--",
    "start": "283050",
    "end": "290100"
  },
  {
    "text": "Colab and maybe consider\ngetting a pro account for $30 over the course\nof the entire quarter.",
    "start": "290100",
    "end": "296010"
  },
  {
    "text": "You could get a lot\nmore compute on Colab including GPUs, also the Amazon\nversions, SageMaker Studio,",
    "start": "296010",
    "end": "305100"
  },
  {
    "text": "and in addition OpenAI\naccount and Cohere account. Both of those have free tiers.",
    "start": "305100",
    "end": "312120"
  },
  {
    "text": "For Cohere, you get really\nrate limited and for OpenAI, they give you $5. You could consider\nspending a little bit more.",
    "start": "312120",
    "end": "318690"
  },
  {
    "text": "I do think you could\ndo all our coursework for under those amounts. And I think that for\nOpenAI, you could still",
    "start": "318690",
    "end": "324240"
  },
  {
    "text": "have lots of accounts if you\nwanted to each one getting $5. It used to be 18,\nand now it's 5.",
    "start": "324240",
    "end": "330720"
  },
  {
    "text": "So we know what's coming. But embrace it while you can. Also I'll say I'm pretty\nwell confident that we'll",
    "start": "330720",
    "end": "337810"
  },
  {
    "text": "get a bunch of credits\nfrom AWS Educate for you to use EC2 machines.",
    "start": "337810",
    "end": "343509"
  },
  {
    "text": "So more details about\nthat in a little bit. OK.  So if you want to follow\nalong, let's head to this one.",
    "start": "343510",
    "end": "350610"
  },
  {
    "text": "This is our slideshow\nfrom last time. And I do just want to\nreview some things. What we did last time\nis I tried to immerse us",
    "start": "350610",
    "end": "357120"
  },
  {
    "text": "in this weird and\nwonderful moment for AI and give you a sense\nfor how we got here.",
    "start": "357120",
    "end": "364420"
  },
  {
    "text": "And then we talked about\nthe first two units, transformers and retrieval\naugmented in-context learning.",
    "start": "364420",
    "end": "371520"
  },
  {
    "text": "I think that is all\nwonderful stuff. I expect you all to do lots\nof creative and cool things in that space.",
    "start": "371520",
    "end": "377500"
  },
  {
    "text": "But it's important for me\nto continue this slide show because there is more\nto our field than just",
    "start": "377500",
    "end": "382680"
  },
  {
    "text": "those big language\nmodels and prompting. And there are lots of important\nways to contribute beyond that.",
    "start": "382680",
    "end": "388060"
  },
  {
    "text": "So let me take a moment\nand just give you an indication of what\nI have in mind there. Our third main course\nunit I've called",
    "start": "388060",
    "end": "396630"
  },
  {
    "text": "compositional generalization. This is brand new. We're going to focus on\nthe COGS benchmark, which",
    "start": "396630",
    "end": "403620"
  },
  {
    "text": "is a relatively recent synthetic\ndata set that is designed to stress test whether\nmodels have really",
    "start": "403620",
    "end": "410940"
  },
  {
    "text": "learned systematic solutions\nto language problems. So the way COGS works is we have\nessentially a semantic parsing",
    "start": "410940",
    "end": "419139"
  },
  {
    "text": "task. The input is a sentence like,\nLina gave the bottle to John. And the task is to learn\nhow to map those sentences",
    "start": "419140",
    "end": "426970"
  },
  {
    "text": "to their logical form, which are\nthese logical representations down here. The interesting\nthing about COGS is",
    "start": "426970",
    "end": "435190"
  },
  {
    "text": "that they've posed hard\ngeneralization tasks. For example, in\ntraining, you might",
    "start": "435190",
    "end": "440410"
  },
  {
    "text": "get to see examples where Lina\nhere is in subject position and then at test time, you\nsee Lina in object position.",
    "start": "440410",
    "end": "449350"
  },
  {
    "text": "Or at train time, you\nmight see Paula as a name but in isolation,\nand the task is",
    "start": "449350",
    "end": "455110"
  },
  {
    "text": "to have the system\nlearn how to deal with Paula as a\nsubject of a sentence like, Paula painted a cake\nor object PP to subject PP.",
    "start": "455110",
    "end": "464950"
  },
  {
    "text": "So at train time you see,\nEmma ate the cake on the table where on the table is inside the\ndirect object of the sentence.",
    "start": "464950",
    "end": "472180"
  },
  {
    "text": "And then at test\ntime, you see the cake on the table burned where on\nthe table is now a subject.",
    "start": "472180",
    "end": "478449"
  },
  {
    "text": "These seem like dead simple\ngeneralization tasks, and the sentences are\nvery simple, right?",
    "start": "478450",
    "end": "484570"
  },
  {
    "text": "But here's the punch line. This is a kind of\naccumulated leaderboard of a lot of entries for COGS.",
    "start": "484570",
    "end": "491410"
  },
  {
    "text": "And if you look all\nthe way on the right, you can see systems\nare doing pretty well. It is impressive\nthat they can go",
    "start": "491410",
    "end": "496870"
  },
  {
    "text": "from these free-form\nsentences into those very ornate logical forms. OK. But look at this column,\nthis is a column of zeroes--",
    "start": "496870",
    "end": "504460"
  },
  {
    "text": "object PP to subject PP. It looked really simple, right? That's just the task\nof learning from Emma",
    "start": "504460",
    "end": "510460"
  },
  {
    "text": "ate the cake on the\ntable and predicting the cake on the table burned. Why are all these\nbrand new systems",
    "start": "510460",
    "end": "516879"
  },
  {
    "text": "getting 0 on this split? That shows, first of all,\nthat this is a hard problem.",
    "start": "516880",
    "end": "522700"
  },
  {
    "text": "Now we are going to\nwork with a variant that we created of\nCOGS called ReCOGS.",
    "start": "522700",
    "end": "527770"
  },
  {
    "text": "This was done with my student\nZheng Wu and Chris Manning. It's brand new work. We think that in\npart all those zeros",
    "start": "527770",
    "end": "534520"
  },
  {
    "text": "derive from there being\nsome artifacts in COGS. So it was made kind of\nartificially hard and also",
    "start": "534520",
    "end": "540700"
  },
  {
    "text": "artificially easy in some ways. So in this class,\nwe're going to work with ReCOGS which has done some\nsystematic meaning preserving",
    "start": "540700",
    "end": "548800"
  },
  {
    "text": "transformations to the original\nto create a new data set that we think is fairer. But it still remains\nincredibly hard.",
    "start": "548800",
    "end": "557139"
  },
  {
    "text": "Systems can get traction where\nbefore they were getting zero so we know their signal,\nand we have more confidence",
    "start": "557140",
    "end": "563680"
  },
  {
    "text": "that this is testing\nsomething about semantics. And then the punchline\nremains the same. This is incredibly hard for our\nsystems, even our best systems.",
    "start": "563680",
    "end": "573040"
  },
  {
    "text": "There needs to be some\nkind of breakthrough here for us to get\nour systems to do",
    "start": "573040",
    "end": "578140"
  },
  {
    "text": "well even on these\nincredibly simple sentences. So I am eager to see what\nyou all do with this problem.",
    "start": "578140",
    "end": "585759"
  },
  {
    "text": "You're seeing a picture here of\nthe kind of best we could do, which is a little\nbit better than what",
    "start": "585760",
    "end": "591430"
  },
  {
    "text": "was in the literature\npreviously but certainly not a solved task, right? ",
    "start": "591430",
    "end": "598430"
  },
  {
    "text": "So that will culminate\nin this homework and bake off, our third one. From there, the course work\nopens up into your projects.",
    "start": "598430",
    "end": "607710"
  },
  {
    "text": "We're done with the\nregular assignments, and we go through the\nrhythm of lit review; experiment protocol, which is\na special document that kind of",
    "start": "607710",
    "end": "615150"
  },
  {
    "text": "lays down the nuts and\nbolts of what you're going to do for your paper; and\nthen the final paper itself.",
    "start": "615150",
    "end": "620850"
  },
  {
    "text": "And in the spirit of that, what\nwe do in our course together is think about topics\nthat will supercharge",
    "start": "620850",
    "end": "627420"
  },
  {
    "text": "your own final project papers. And the first topic that\ncomes to mind for me there",
    "start": "627420",
    "end": "633000"
  },
  {
    "text": "is better and more\ndiverse benchmarks. We need measurement instruments\nto get reliable estimates",
    "start": "633000",
    "end": "640560"
  },
  {
    "text": "of how well our\nsystems are doing, and that implies\nhaving good benchmarks. In this context, I really like\nto invoke this famous quotation",
    "start": "640560",
    "end": "648330"
  },
  {
    "text": "from the Explorer\nJacques Cousteau. He said, \"Water and air,\nthe two essential fluids",
    "start": "648330",
    "end": "654300"
  },
  {
    "text": "on which all life depends.\" That's data sets for our field. And you can see here that\nCousteau did continue,",
    "start": "654300",
    "end": "660899"
  },
  {
    "text": "\"Have become global\ngarbage cans.\" That might concern\nus about what's happening with our data sets.",
    "start": "660900",
    "end": "666420"
  },
  {
    "text": "I don't think it's\nthat bad, but still you could have that in\nthe back of your mind that we need these data sets\nwe create to be reliable,",
    "start": "666420",
    "end": "675130"
  },
  {
    "text": "high quality instruments. And the reason for that is\nthat we ask so much of our data",
    "start": "675130",
    "end": "680440"
  },
  {
    "text": "sets. We use them to optimize\nmodels when we train on them. We use them crucially, and\nthis is increasingly important",
    "start": "680440",
    "end": "686730"
  },
  {
    "text": "to evaluate our models. Our biggest language models that\nare getting all the headline, how well are they\nactually doing?",
    "start": "686730",
    "end": "693180"
  },
  {
    "text": "We need data sets for that. We use it to compare\nmodels to enable new capabilities via\ntraining and testing,",
    "start": "693180",
    "end": "700110"
  },
  {
    "text": "to measure progress as a field-- it's kind of our\nfundamental barometer for this-- and of course,\nfor basic scientific inquiry",
    "start": "700110",
    "end": "707730"
  },
  {
    "text": "into language and the world. This is a long and\nimportant list. And it shows you that\ndata sets are really",
    "start": "707730",
    "end": "715260"
  },
  {
    "text": "central to what we're doing. And so I'm sort of exhorting\nyou, as you can tell, to think about data\nsets especially",
    "start": "715260",
    "end": "721530"
  },
  {
    "text": "ones that would be powerful\nas evaluation tools in the context of this course. I am genuinely worried\nabout the new dynamic",
    "start": "721530",
    "end": "729130"
  },
  {
    "text": "where we are evaluating these\nbig language models essentially on Twitter where people have\nscreenshots of some fun cases",
    "start": "729130",
    "end": "737139"
  },
  {
    "text": "that they saw. And we all know that we're not\nseeing a full representative sample of the inputs.",
    "start": "737140",
    "end": "742779"
  },
  {
    "text": "We're seeing the\nworst and the best. And it's impossible to piece\ntogether a scientific picture from that.",
    "start": "742780",
    "end": "748945"
  },
  {
    "text": "My student Omar Khattab\nrecently observed-- I think this is very wise-- that we have moved into this\nera in which designing systems",
    "start": "748945",
    "end": "756514"
  },
  {
    "text": "might be really easy. It might be a matter\nof writing a prompt. But figuring out whether\nit was a good system",
    "start": "756515",
    "end": "761649"
  },
  {
    "text": "is going to get\nharder and harder. And for that, we need lots of\nevaluation data sets, right?",
    "start": "761650",
    "end": "768850"
  },
  {
    "text": "You could think about this slide\nthat I showed you from before, we have this\nbenchmark saturation",
    "start": "768850",
    "end": "774730"
  },
  {
    "text": "with all of these systems now\nincreasingly quickly getting above our estimate\nof human performance.",
    "start": "774730",
    "end": "779750"
  },
  {
    "text": "But I asked you to\nbe cynical about that as a measure of human\nperformance, right? Another perspective\non this slide",
    "start": "779750",
    "end": "786250"
  },
  {
    "text": "could be that our\nbenchmarks are simply too easy because it is not\nlike if you interacted with one",
    "start": "786250",
    "end": "792400"
  },
  {
    "text": "of these systems, even\nthe most recent ones, it would feel superhuman to you.",
    "start": "792400",
    "end": "798190"
  },
  {
    "text": "Partly what we're seeing\nhere is a remnant of the fact that until very\nrecently, our evaluations",
    "start": "798190",
    "end": "804730"
  },
  {
    "text": "had to be essentially machine\ntasks, not human tasks. And we had humans do machine\ntasks to get a \"measure\"",
    "start": "804730",
    "end": "811210"
  },
  {
    "text": "of human performance. Maybe we're moving into a\nnew and more exciting era. We're going to talk about\nadversarial testing.",
    "start": "811210",
    "end": "818410"
  },
  {
    "text": "I've been involved with\nthe Dynabench effort. This is a kind of\nopen source effort to develop data\nsets that are going",
    "start": "818410",
    "end": "824950"
  },
  {
    "text": "to be really hard for\nthe best of our models. And I think that's a wonderful\ndynamic as well, right?",
    "start": "824950",
    "end": "831550"
  },
  {
    "text": "And that kind of leads into\nthis related topic of us having more meaningful evaluations.",
    "start": "831550",
    "end": "837610"
  },
  {
    "text": "Here's a fundamental\nthing that you might worry about throughout\nartificial intelligence.",
    "start": "837610",
    "end": "842650"
  },
  {
    "text": "All we care about is\nperformance for these systems, some notion of accuracy.",
    "start": "842650",
    "end": "847910"
  },
  {
    "text": "I've put this under the heading\nof Strathern's law, when a measure becomes a target, it\nceases to be a good measure.",
    "start": "847910",
    "end": "853630"
  },
  {
    "text": "If we have this consensus that\nall we care about is accuracy, we know what will happen. Everyone in the field will\nhill climb on accuracy.",
    "start": "853630",
    "end": "861220"
  },
  {
    "text": "And we know from\nStrathern's law that will distort the\nactual rate of progress",
    "start": "861220",
    "end": "867520"
  },
  {
    "text": "by diminishing\neverything else that could be important to thinking\nabout these AI systems.",
    "start": "867520",
    "end": "874210"
  },
  {
    "text": "Relatedly, like this\nis a wonderful study from Birhane et al. I've selected a few\nof the values encoded",
    "start": "874210",
    "end": "881440"
  },
  {
    "text": "in ML research, which they did\nvia very extensive literature survey. And kind of impressionistically,\nhere's the list.",
    "start": "881440",
    "end": "888340"
  },
  {
    "text": "At the top dominating\neverything else, you have an obsession with\nperformance as I said.",
    "start": "888340",
    "end": "894580"
  },
  {
    "text": "Then way down on the list\nthough in second place, you have efficiency and\nthings like explainability,",
    "start": "894580",
    "end": "901480"
  },
  {
    "text": "applicability in a\nreal-world robustness. As I go farther down\non the list here the ones that are colored\nthere, they actually",
    "start": "901480",
    "end": "907570"
  },
  {
    "text": "should be in the tiniest\nof type because if you think about the\nfield's actual values",
    "start": "907570",
    "end": "912790"
  },
  {
    "text": "as reflected in\nthe literature, you find that these things are\ngetting almost no play. I think things are looking\nup, but it's still the case",
    "start": "912790",
    "end": "921850"
  },
  {
    "text": "that it's wildly skewed\ntoward performance. But those things that\nI have down there in purple and orange--",
    "start": "921850",
    "end": "928780"
  },
  {
    "text": "beneficence, privacy,\nfairness, and justice-- those are incredibly important things\nand more and more important",
    "start": "928780",
    "end": "935200"
  },
  {
    "text": "as these systems are being\ndeployed more widely. And so we have to\nvia our practices",
    "start": "935200",
    "end": "941020"
  },
  {
    "text": "and what we hold to be valuable\nelevate these other principles.",
    "start": "941020",
    "end": "946030"
  },
  {
    "text": "And you all could\nstart to do that by thinking about\nproposing evaluations that would elevate them.",
    "start": "946030",
    "end": "952000"
  },
  {
    "text": "That could be\ntremendously exciting. And the final point here\nis that we could also",
    "start": "952000",
    "end": "957070"
  },
  {
    "text": "have kind of a move\ntoward leaderboards that embrace more aspects of this.",
    "start": "957070",
    "end": "962269"
  },
  {
    "text": "Again, to help us move away from\nthe obsession on performance, we should have\nleaderboards that kind of",
    "start": "962270",
    "end": "967390"
  },
  {
    "text": "score us along many dimensions. In this context, I've\nreally been inspired by work that Kawin did on what he\ncalls Dynascoring, which",
    "start": "967390",
    "end": "976000"
  },
  {
    "text": "is a proposal for\nhow to synthesize across a lot of different\nmeasurement dimensions.",
    "start": "976000",
    "end": "981580"
  },
  {
    "text": "To give you a\nquick illustration, here I have a table\nwhere the rows are question-answering systems\nand the columns are",
    "start": "981580",
    "end": "989043"
  },
  {
    "text": "different things\nwe could measure, just a sample of them--\nperformance, throughput, memory, fairness,\nrobustness, and we",
    "start": "989043",
    "end": "996700"
  },
  {
    "text": "could add other dimensions. With the current Dynascoring\nthat you're seeing here where most of the weight\nis put on performance,",
    "start": "996700",
    "end": "1003390"
  },
  {
    "text": "that DeBERTa system\nis the winner in this leaderboard competition. OK. But that's standard.",
    "start": "1003390",
    "end": "1009180"
  },
  {
    "text": "But what if we decided that we\ncared much more about fairness for these systems, so\nwe adjusted the scoring",
    "start": "1009180",
    "end": "1015180"
  },
  {
    "text": "here to put five on fairness,\nkeep a lot on performance, but diminish the kind\nof other measures there.",
    "start": "1015180",
    "end": "1021150"
  },
  {
    "text": "Well, now the ELECTRA-large\nsystem is in first place. Which one was the true winner?",
    "start": "1021150",
    "end": "1026890"
  },
  {
    "text": "I think the answer is that\nthere is no true winner here. What this shows is that\nall of our leaderboards",
    "start": "1026890",
    "end": "1033119"
  },
  {
    "text": "are reflecting some\nordering of our preferences. And when we pick one,\nwe are instilling",
    "start": "1033119",
    "end": "1038939"
  },
  {
    "text": "a particular set of values\non the whole enterprise. But this is also\ncreating space for us",
    "start": "1038940",
    "end": "1044880"
  },
  {
    "text": "this is I think part of\nKawin's vision for scoring that we could design\nleaderboards that",
    "start": "1044880",
    "end": "1050125"
  },
  {
    "text": "were attuned to the\nthings that we want to do in the world via the\nweighting and the columns that we chose and evaluate\nsystems on that basis.",
    "start": "1050125",
    "end": "1059220"
  },
  {
    "text": "Yeah. What does fairness to\nyou in this field mean? How do you measure\nsomething like that?",
    "start": "1059220",
    "end": "1064560"
  },
  {
    "text": "What is fairness? Well, that's a whole\nother aspect to this. So if we're going to start\nto measure these dimensions, like we're going to have\na column for fairness,",
    "start": "1064560",
    "end": "1071498"
  },
  {
    "text": "we better be sure that we\nknow what's behind that. And I can tell you\nthere needs to be a lot more work on our\nmeasurement devices,",
    "start": "1071498",
    "end": "1078779"
  },
  {
    "text": "our benchmarks for\nassessing fairness because all of those things\nare incredibly nuanced",
    "start": "1078780",
    "end": "1084720"
  },
  {
    "text": "multi-dimensional concepts. And so the idea would be\nto bring that in as well.",
    "start": "1084720",
    "end": "1089880"
  },
  {
    "text": "Throughput memory maybe\nthose are straightforward, but fairness is going\nto be a challenging one.",
    "start": "1089880",
    "end": "1095100"
  },
  {
    "text": "But that's not to say that\nit's not incredibly important. ",
    "start": "1095100",
    "end": "1102950"
  },
  {
    "text": "And then finally, to\nreally inspire you, I do feel like this\nis the first time I could say in this\ncourse, I think, we're",
    "start": "1102950",
    "end": "1109390"
  },
  {
    "text": "moving into an era in which our\nevaluations can be much more meaningful than they\never were before.",
    "start": "1109390",
    "end": "1114850"
  },
  {
    "text": "Assessment today or yesterday\nis really one dimensional. That's the performance\nthing I mentioned.",
    "start": "1114850",
    "end": "1120730"
  },
  {
    "text": "It's largely insensitive\nto the context. We always pick F1 or\nsomething as the only thing",
    "start": "1120730",
    "end": "1126220"
  },
  {
    "text": "regardless of what we're trying\nto accomplish in the world. The terms are largely\nset by us researchers.",
    "start": "1126220",
    "end": "1132130"
  },
  {
    "text": "We say it's F1, and everyone\nkind of follows suit because we're supposed to\nbe the experts on this.",
    "start": "1132130",
    "end": "1137390"
  },
  {
    "text": "And it's often very opaque\nand tailored to machine tasks. I've already\ncomplained about that,",
    "start": "1137390",
    "end": "1143230"
  },
  {
    "text": "our estimates of\nhuman performance being very different\nfrom what you would think that phrase means.",
    "start": "1143230",
    "end": "1148630"
  },
  {
    "text": "In this new future that\nwe could start right now, our assessments could certainly\nbe high dimensional and fluid.",
    "start": "1148630",
    "end": "1154360"
  },
  {
    "text": "I showed you a glimpse of\nthat with the dynascoring. I think that's incredible. They could also in turn be\nhighly sensitive to the context",
    "start": "1154360",
    "end": "1161410"
  },
  {
    "text": "that we're in. If we care about fairness\nand we care about efficiency and we put those\nabove performance,",
    "start": "1161410",
    "end": "1167150"
  },
  {
    "text": "we're going to get a very\ndifferent prioritization of the systems and so\nforth and so on, right?",
    "start": "1167150",
    "end": "1173179"
  },
  {
    "text": "And then in turn, the\nterms of these evaluations could be set not by\nus researchers who",
    "start": "1173180",
    "end": "1178490"
  },
  {
    "text": "are doing our very\nabstract thing, but rather the people who\nare trying to get value out of these systems-- the people\nwho have to interact with them.",
    "start": "1178490",
    "end": "1186260"
  },
  {
    "text": "And then the judgments\ncould ultimately be made by the users. They could decide\nwhich system they",
    "start": "1186260",
    "end": "1191570"
  },
  {
    "text": "want to choose based on their\nown expressed preferences. And then in turn, maybe we\ncould have our evaluations",
    "start": "1191570",
    "end": "1199130"
  },
  {
    "text": "be much more at the\nlevel of human tasks as opposed-- so right\nnow, for example,",
    "start": "1199130",
    "end": "1204660"
  },
  {
    "text": "we might insist that\nsome human labelers choose a particular label\nfor an ambiguous example. And then we assess\nhow much agreement",
    "start": "1204660",
    "end": "1212990"
  },
  {
    "text": "they have whereas\nthe human thing is to discuss and debate, right,\nto have a dialogue about what",
    "start": "1212990",
    "end": "1218690"
  },
  {
    "text": "the right label is in the\nface of ambiguity and context dependence. Well, now we could have that\nkind of evaluation, right?",
    "start": "1218690",
    "end": "1226039"
  },
  {
    "text": "Maybe we evaluate\nsystems on their ability to adjudicate in a human-like\nway on what the label should",
    "start": "1226040",
    "end": "1231980"
  },
  {
    "text": "be. Hard to imagine before,\nbut now probably something that you could toy around\nwith a little bit with one",
    "start": "1231980",
    "end": "1238760"
  },
  {
    "text": "of these large language model\nAPIs right now if you wanted. And I think we could\nreally embrace that.",
    "start": "1238760",
    "end": "1244565"
  },
  {
    "text": " I have a couple more topics,\nbut let me pause there.",
    "start": "1244565",
    "end": "1250570"
  },
  {
    "text": "Do people have\nthoughts, questions, insights about benchmarks\nand evaluation?",
    "start": "1250570",
    "end": "1256900"
  },
  {
    "text": " I hope you're seeing that\nit's a wide open area",
    "start": "1256900",
    "end": "1263370"
  },
  {
    "text": "for final projects. Yeah. Is there more of a move to get\nspecialists in other fields",
    "start": "1263370",
    "end": "1269190"
  },
  {
    "text": "like, for example, like\nlinguistics or related things to help make benchmarks?",
    "start": "1269190",
    "end": "1274680"
  },
  {
    "text": "What a wonderful question. You asked, is there a move to\nhave more experts participate",
    "start": "1274680",
    "end": "1279720"
  },
  {
    "text": "in evaluation? I hope the answer is yes. But let's make the answer yes. That would be the point,\nright, because what we want is",
    "start": "1279720",
    "end": "1286830"
  },
  {
    "text": "to provide the world with tools\nand concepts that would allow domain experts, people who\nactually know what's going",
    "start": "1286830",
    "end": "1293160"
  },
  {
    "text": "on in the domain we're trying to\nuse this AI technology to make these decisions and make\nadjustments and so forth based",
    "start": "1293160",
    "end": "1300600"
  },
  {
    "text": "on what's working\nand what isn't. That should be our goal. And then what we as\nresearchers can do",
    "start": "1300600",
    "end": "1307110"
  },
  {
    "text": "is provide things like\nwhat Kawin provided with dynascoring, which is the\nintellectual infrastructure",
    "start": "1307110",
    "end": "1312210"
  },
  {
    "text": "to allow them to do that. Yeah. And then you all probably\nhave lots of domain expertise",
    "start": "1312210",
    "end": "1319343"
  },
  {
    "text": "that kind of intersects\nwith what we're doing, but maybe it comes\nfrom other fields. And so you could participate\nas NLU researcher",
    "start": "1319343",
    "end": "1327560"
  },
  {
    "text": "and as a domain expert\nto do a paper that embraced both aspects of this.",
    "start": "1327560",
    "end": "1332960"
  },
  {
    "text": "And maybe you propose\na kind of metric that you think really works\nwell for your field of economics",
    "start": "1332960",
    "end": "1338630"
  },
  {
    "text": "or sociology or whatever\nyou're studying-- medicine, all these things\nincredibly important.",
    "start": "1338630",
    "end": "1345825"
  },
  {
    "text": "I saw another hand go up. Yeah. One of the challenges\nwe're going to face is it's really expensive\nto collect human",
    "start": "1345825",
    "end": "1353250"
  },
  {
    "text": "or more sophisticated labels. As an example, there's a\npaper that came out recently, [INAUDIBLE],, where\nthey trained--",
    "start": "1353250",
    "end": "1360960"
  },
  {
    "text": "actually really just\ntuned [INAUDIBLE] to respond to medical\nstudent or medical questions",
    "start": "1360960",
    "end": "1370380"
  },
  {
    "text": "from USMLE and other\nmedicine-related exams. They also had a section\nfor long-form answers.",
    "start": "1370380",
    "end": "1377220"
  },
  {
    "text": "The short-form answers\nit's a multiple choice. They can figure it out. The long-form\nanswers they actually",
    "start": "1377220",
    "end": "1382650"
  },
  {
    "text": "had doctors evaluate them. And that's really expensive. They could only\ncollect so many labels",
    "start": "1382650",
    "end": "1387870"
  },
  {
    "text": "even with a large\nstaff of doctors. So I think the balance\nbetween, I mean,",
    "start": "1387870",
    "end": "1392880"
  },
  {
    "text": "calculating clickthrough\nis super easy. It's just counting. But evaluating how valuable\na search result is,",
    "start": "1392880",
    "end": "1398938"
  },
  {
    "text": "that requires a human. And that's a little\nmore expensive. I'm curious how we\ncan balance the cost. ",
    "start": "1398938",
    "end": "1405760"
  },
  {
    "text": "The issue of cost is going\nto be unavoidable for us. I think we should\nconfront it as a group.",
    "start": "1405760",
    "end": "1411370"
  },
  {
    "text": "This research has just\ngotten more expensive. And that's obviously\ndistorting who can participate and what we value.",
    "start": "1411370",
    "end": "1417250"
  },
  {
    "text": "It's another thing I could\ndiscuss under this rubric. For your particular\nquestion, though, I",
    "start": "1417250",
    "end": "1423040"
  },
  {
    "text": "remain optimistic because I\nthink we are in an era now in which you could do\na meaningful evaluation",
    "start": "1423040",
    "end": "1428740"
  },
  {
    "text": "of a system with\nno training data and rather just a few\ndozen, let's say, 100",
    "start": "1428740",
    "end": "1436000"
  },
  {
    "text": "examples for\nassessment if you're careful about how you use it. That is if you don't develop\nyour system on it and so forth.",
    "start": "1436000",
    "end": "1443433"
  },
  {
    "text": "But even if you\nsay, OK, I'm going to have 100 for\ndevelopment, 100 that I keep for a final\nevaluation to really get",
    "start": "1443433",
    "end": "1449529"
  },
  {
    "text": "a sense for how my system\nperforms on new data, that's only 200 examples. And I feel like\nthat's manageable",
    "start": "1449530",
    "end": "1459130"
  },
  {
    "text": "even if we need an expert. And the point would be that\nmight be money well spent. It might be that if we can get\nsome experts to provide the 200",
    "start": "1459130",
    "end": "1467380"
  },
  {
    "text": "cases, we have a really\nreliable measurement tool.",
    "start": "1467380",
    "end": "1472510"
  },
  {
    "text": "I could never have said this 10\nyears ago because 10 years ago, the norm was to have 50,000\ntraining instances and 5,000",
    "start": "1472510",
    "end": "1481210"
  },
  {
    "text": "test instances. And now your cost\nconcern really kicks in. But for the present\nmoment, I feel",
    "start": "1481210",
    "end": "1487630"
  },
  {
    "text": "like a few meaningful\ncases could be worth a lot. And you all could\nconstruct those data sets.",
    "start": "1487630",
    "end": "1493240"
  },
  {
    "text": "Again, before I used\nto give the advice don't create your own\ndata set in this class. You'll run out of time.",
    "start": "1493240",
    "end": "1498310"
  },
  {
    "text": "But now I can give\nthe advice, no, if you have some domain\nexpertise in the life",
    "start": "1498310",
    "end": "1503530"
  },
  {
    "text": "sciences or something and you\nwant a data set, create one to use for assessment.",
    "start": "1503530",
    "end": "1509140"
  },
  {
    "text": "It'll shape your\nsystem design, but that could be healthy as well. ",
    "start": "1509140",
    "end": "1520050"
  },
  {
    "text": "Another big theme,\nexplainability. And this also relates\nto our increased impact.",
    "start": "1520050",
    "end": "1526250"
  },
  {
    "text": "If we're going to deploy\nthese models out in the world, it is really important\nthat we understand them.",
    "start": "1526250",
    "end": "1532070"
  },
  {
    "text": "Right now, we do a lot\nof behavioral testing. That is we come up\nwith these test cases,",
    "start": "1532070",
    "end": "1538309"
  },
  {
    "text": "and we see how well\nthe model does. But the problem, which is a deep\nproblem of scientific induction",
    "start": "1538310",
    "end": "1544159"
  },
  {
    "text": "is that you can never\ncome up with enough cases. And the world is a\ndiverse and complex place.",
    "start": "1544160",
    "end": "1549470"
  },
  {
    "text": "And no matter how many\nthings you dreamed up when you were doing the\nresearch, if you deploy your system, it will\nencounter things",
    "start": "1549470",
    "end": "1556220"
  },
  {
    "text": "that you never anticipated. And if all you've done\nis behavioral testing, you might feel very\nnervous about this",
    "start": "1556220",
    "end": "1562880"
  },
  {
    "text": "because you might have\nessentially no idea what it's going to do on new cases. The mission of\nexplainability research",
    "start": "1562880",
    "end": "1570350"
  },
  {
    "text": "should be to go one layer\ndeeper and understand what is happening\ninside these models",
    "start": "1570350",
    "end": "1575685"
  },
  {
    "text": "so that we have a\nsense for how they'll generalize to new cases. It's a very challenging\nthing because we're",
    "start": "1575685",
    "end": "1581370"
  },
  {
    "text": "thinking about these enormous\nand incredibly opaque models.",
    "start": "1581370",
    "end": "1586530"
  },
  {
    "text": "You can even find people\nsaying in the literature that they're skeptical that\nwe can ever understand what's happening with these systems.",
    "start": "1586530",
    "end": "1592510"
  },
  {
    "text": "But I am optimistic. They are closed\ndeterministic systems. They may be complex,\nbut we're smart.",
    "start": "1592510",
    "end": "1600120"
  },
  {
    "text": "We can figure out what\nthey have learned. I really have\nconfidence in this. And the importance\nof this is really",
    "start": "1600120",
    "end": "1606150"
  },
  {
    "text": "that we have these\nbroader societal goals. We want systems that are\nreliable and safe and",
    "start": "1606150",
    "end": "1611909"
  },
  {
    "text": "trustworthy. And we want to know\nwhere we can use them. And we want them to\nbe free from bias.",
    "start": "1611910",
    "end": "1616930"
  },
  {
    "text": "And it seems to me that\nall of these questions depend on us having some true\nanalytic guarantees about model",
    "start": "1616930",
    "end": "1624780"
  },
  {
    "text": "behaviors. It seems very hard for\nme to say, trust me, my model is not biased\nalong some dimension",
    "start": "1624780",
    "end": "1631860"
  },
  {
    "text": "if I don't even have\nany idea how it works. The best I could say is\nthat it wasn't biased",
    "start": "1631860",
    "end": "1636960"
  },
  {
    "text": "in some evaluations that I ran. But I just emphasize\nfor you that that's very different from being\nevaluated by the world where",
    "start": "1636960",
    "end": "1644310"
  },
  {
    "text": "a lot of things could happen\nthat you didn't anticipate. We'll talk about a lot of\ndifferent explanation methods.",
    "start": "1644310",
    "end": "1651940"
  },
  {
    "text": "I think that these methods\nshould be human interpretable. That is we don't want low\nlevel mathematical explanations",
    "start": "1651940",
    "end": "1657752"
  },
  {
    "text": "of how the models work. We want this expressed\nin human-level concepts so that we can reason\nabout these systems.",
    "start": "1657752",
    "end": "1665380"
  },
  {
    "text": "And we also want\nthem to be faithful to the underlying model. We don't want to fabricate human\ninterpretable but inaccurate",
    "start": "1665380",
    "end": "1672570"
  },
  {
    "text": "explanations of the model. We want them to be true\nto the underlying systems. These are two very difficult\nstandards to meet together.",
    "start": "1672570",
    "end": "1681240"
  },
  {
    "text": "I can make them\nhuman interpretable if I offer you no guarantees\nof faithfulness, right,",
    "start": "1681240",
    "end": "1686250"
  },
  {
    "text": "but then I'm just\ntricking myself and you. I can make them\nfaithful by making them very technical and low level.",
    "start": "1686250",
    "end": "1692640"
  },
  {
    "text": "We could just talk about all the\nmatrix multiplications we want, but that's not going to provide\na human-level insight into how",
    "start": "1692640",
    "end": "1699330"
  },
  {
    "text": "the models are working. So together, though,\nwe need to get methods that are good\nfor both of these,",
    "start": "1699330",
    "end": "1704360"
  },
  {
    "text": "r ight, concept-level\nunderstanding of the causal dynamics\nof these systems.",
    "start": "1704360",
    "end": "1710630"
  },
  {
    "text": "We'll talk about a lot of\ndifferent explanation methods. I'll just do this quickly. Train/tests, that is\nthe behavioral thing,",
    "start": "1710630",
    "end": "1716740"
  },
  {
    "text": "remains very important\nfor the field. We'll talk about\nprobing, which was an early and very influential\nand very ambitious",
    "start": "1716740",
    "end": "1723820"
  },
  {
    "text": "attempt to understand the hidden\nrepresentations of our models. We'll talk about\nattribution methods.",
    "start": "1723820",
    "end": "1730309"
  },
  {
    "text": "These are kind of ways\nto assign importance to different parts of\nthe representations of these models, both\ninput and output and also",
    "start": "1730310",
    "end": "1737740"
  },
  {
    "text": "their internal representations. And then we're going\nto talk about methods that depend on\nactive manipulations",
    "start": "1737740",
    "end": "1744670"
  },
  {
    "text": "of model internal states. And you'll be able to\ntell that I strongly favor the active manipulation\napproach because I think",
    "start": "1744670",
    "end": "1752500"
  },
  {
    "text": "that that's the kind of\napproach that can give us causal insights and\nalso richly characterize",
    "start": "1752500",
    "end": "1758350"
  },
  {
    "text": "what the models are doing. And that's more or\nless the two desiderata that I just mentioned\nfor these methods.",
    "start": "1758350",
    "end": "1764290"
  },
  {
    "text": "But there's value to\nall of these things. And we'll talk\nabout all of them. And you'll get hands\non with all of them.",
    "start": "1764290",
    "end": "1769390"
  },
  {
    "text": "And all of them can be\nwonderful for your analysis sections of your final papers.",
    "start": "1769390",
    "end": "1776020"
  },
  {
    "text": "We might even talk about\ninterchange intervention training, which is when you\nuse explainability methods",
    "start": "1776020",
    "end": "1781540"
  },
  {
    "text": "to actually push the models\nto become better, more systematic, more reliable maybe\nless biased along dimensions",
    "start": "1781540",
    "end": "1788732"
  },
  {
    "text": "that you care about.  That's my review of\nthe core things--",
    "start": "1788732",
    "end": "1796809"
  },
  {
    "text": "questions or comments. I have a few more low-level\nthings about the course to do now.",
    "start": "1796810",
    "end": "1802570"
  },
  {
    "text": "Yeah. I know we're going to get\ninto all of the explanation methods in a lot\nof detail later on,",
    "start": "1802570",
    "end": "1808060"
  },
  {
    "text": "but can you give a\nquick example just so that we have any imagination\nof what they are?",
    "start": "1808060",
    "end": "1814149"
  },
  {
    "text": "Probing is training\nsupervised classifiers on internal representations.",
    "start": "1814150",
    "end": "1819560"
  },
  {
    "text": "And this was just the\ncool thing to say, hey, I'll just look\nat layer three, column four of my\nBERT representation.",
    "start": "1819560",
    "end": "1825460"
  },
  {
    "text": "Does it encode information\nabout animacy or part of speech? And the answer seems to be yes.",
    "start": "1825460",
    "end": "1834220"
  },
  {
    "text": "And I think that was\nreally eye opening that even if your task\nwith sentiment analysis,",
    "start": "1834220",
    "end": "1839360"
  },
  {
    "text": "you might have learned latent\nstructure about animacy. That's getting closer to the\nhuman-level concept stuff.",
    "start": "1839360",
    "end": "1846130"
  },
  {
    "text": "The problem with\nprobing is that you have no guarantee that\ninformation about animacy here has any causal impact\non the model behavior.",
    "start": "1846130",
    "end": "1853910"
  },
  {
    "text": "It could be just\nkind of something that the model\nlearned by the by. Attribution methods have\nthe kind of reverse problem.",
    "start": "1853910",
    "end": "1861110"
  },
  {
    "text": "They can give you\nsome causal guarantees that this neuron plays\nthis particular role",
    "start": "1861110",
    "end": "1866720"
  },
  {
    "text": "in the input-output\nbehavior, but it's usually just a kind of scalar value. It's like 0.3.",
    "start": "1866720",
    "end": "1872163"
  },
  {
    "text": "And you say, well,\nwhat is the 0.3 mean? And you say, it means that it's\nthat much important but nothing",
    "start": "1872163",
    "end": "1877820"
  },
  {
    "text": "like, oh, is it animate or? None of those\nhuman-level things. And then I think the active\nmanipulation thing, which",
    "start": "1877820",
    "end": "1883430"
  },
  {
    "text": "is like doing lots of brain\nsurgeries on your model can provide the\nbenefits of both probing",
    "start": "1883430",
    "end": "1889730"
  },
  {
    "text": "and attribution--\ncausal insights but also a deep understanding\nof what the representations are.",
    "start": "1889730",
    "end": "1898630"
  },
  {
    "text": "And there's a whole\nfamily of those. It's a very exciting\npart of the literature. Yeah.",
    "start": "1898630",
    "end": "1904030"
  },
  {
    "text": "I have a question\ngoing back to COGS. So I guess, why would we want to\nuse the COGS data set if we're",
    "start": "1904030",
    "end": "1909430"
  },
  {
    "text": "testing for generalization? Like, why can't we just prompt\na language model with a word that we've never seen\nbefore and try and induce",
    "start": "1909430",
    "end": "1917140"
  },
  {
    "text": "some form of if you see it\nin the subject position, get it to position in the object\nand see how well it does there?",
    "start": "1917140",
    "end": "1923380"
  },
  {
    "text": "Oh, yeah, no. So for COGS for your\noriginal system, it could be that you try\nto prompt a language model.",
    "start": "1923380",
    "end": "1930250"
  },
  {
    "text": "Zheng did a bunch of that\nas part of the research, it was only OK. But maybe there's\na version of that",
    "start": "1930250",
    "end": "1937005"
  },
  {
    "text": "where you prompt\nin the right way with the right kind\nof instructions and then it does solve COGS. That would be\nwonderful because that",
    "start": "1937005",
    "end": "1943030"
  },
  {
    "text": "would suggest to me that those\nmodels, whatever model you prompted has internal\nrepresentations that",
    "start": "1943030",
    "end": "1949840"
  },
  {
    "text": "are systematic enough to have\nkind of a notion of subject and a notion of object\nand verb and all",
    "start": "1949840",
    "end": "1955540"
  },
  {
    "text": "of that linguistic stuff. And that would be very exciting. The cool thing\nabout COGS is that I",
    "start": "1955540",
    "end": "1961450"
  },
  {
    "text": "think it's a pretty\nreliable measurement device for making such claims.",
    "start": "1961450",
    "end": "1967470"
  },
  {
    "text": "Yeah. How transferable is this\ndiscussion to languages",
    "start": "1967470",
    "end": "1973470"
  },
  {
    "text": "other than English? I wonder if there we\nshould be concerned about the very tight coupling\nbetween the properties",
    "start": "1973470",
    "end": "1981419"
  },
  {
    "text": "of English as a language and all\nour advancement in [INAUDIBLE]??",
    "start": "1981420",
    "end": "1987150"
  },
  {
    "text": "Well, I mean, I hope\nthat a lot of you do projects on multilingual NLP,\nlow-resource settings, and so",
    "start": "1987150",
    "end": "1994470"
  },
  {
    "text": "forth. And I think in a way, we live in\na golden age for that research",
    "start": "1994470",
    "end": "1999900"
  },
  {
    "text": "as well. There's more research\non more languages than there were 10 years\nago, and that's certainly",
    "start": "1999900",
    "end": "2006169"
  },
  {
    "text": "a positive development. The downside is\nthat it's all done with multilingual\nrepresentations,",
    "start": "2006170",
    "end": "2011900"
  },
  {
    "text": "multilingual BERT, and so forth. And they tend to do much\nbetter on English tasks than every other task.",
    "start": "2011900",
    "end": "2017669"
  },
  {
    "text": "So that obviously\nfeels like suboptimal. But again, that's the same\nstory of like sudden progress",
    "start": "2017670",
    "end": "2025220"
  },
  {
    "text": "with a lot of mixed\nfeelings that I have about a lot of these topics. ",
    "start": "2025220",
    "end": "2031929"
  },
  {
    "text": "In the interest of time,\nlet's press on a little bit. I think I just wanted to\nskip to the course mechanics.",
    "start": "2031930",
    "end": "2038393"
  },
  {
    "text": "This is at the website,\nbut there it is. That's the breakdown\nof required pieces. And you can see that it has\na kind of strong emphasis",
    "start": "2038393",
    "end": "2046000"
  },
  {
    "text": "toward the three parts that are\nrelated to the final project. But the homeworks are\nalso really important",
    "start": "2046000",
    "end": "2051520"
  },
  {
    "text": "and the quizzes less so. But I think they're\nimportant enough that you'll take them seriously.",
    "start": "2051520",
    "end": "2058888"
  },
  {
    "text": "It's fully asynchronous. It's wonderful to see\nso many of you here. And I am eager to interact\nwith you here if possible",
    "start": "2058889",
    "end": "2066960"
  },
  {
    "text": "but also on Zoom for\noffice hours and stuff. Please attend office hours\nif you just want to chat. One of my favorite games\nto play in office hours",
    "start": "2066960",
    "end": "2074339"
  },
  {
    "text": "is a group comes with\nthree project ideas, and I rank them from least to\nmost or most to least viable",
    "start": "2074340",
    "end": "2081510"
  },
  {
    "text": "for the course. It's a fun game for me. And I think it\nalways illuminates some things about the problems.",
    "start": "2081510",
    "end": "2088929"
  },
  {
    "text": "All right. And then we have\ncontinuous evaluation. So you have the three\nassignments, the quizzes, and then the project work.",
    "start": "2088929",
    "end": "2094749"
  },
  {
    "text": "And there's no final\nexam, just that we want you to be focused on the\nfinal project at that point.",
    "start": "2094749",
    "end": "2101240"
  },
  {
    "text": "I think I'll leave this aside. We can talk about the grading\nof the original systems a bit later.",
    "start": "2101240",
    "end": "2106790"
  },
  {
    "text": "And then you have\nthe project work, some links here,\nexceptional final projects, and some guidance.",
    "start": "2106790",
    "end": "2112850"
  },
  {
    "text": "These are the two documents\nI mentioned before. I'll just say,\nagain, that this is",
    "start": "2112850",
    "end": "2117918"
  },
  {
    "text": "the most important part\nof the course to me and the thing that's special. And I'll say again, also we have\nthis incredibly accomplished",
    "start": "2117918",
    "end": "2124150"
  },
  {
    "text": "teaching team this\nyear, diverse interests, and they all have done\nincredible research",
    "start": "2124150",
    "end": "2129400"
  },
  {
    "text": "on their own. I've learned a ton from\nthem and from their work. And I encourage\nyou to do the same.",
    "start": "2129400",
    "end": "2135290"
  },
  {
    "text": "So seek them out\nin office hours, and take advantage of their\nmentorship for the work you do.",
    "start": "2135290",
    "end": "2143137"
  },
  {
    "text": "And then here are some\ncrucial course links. I covered that before. The quizzes, I think,\nI've covered as well.",
    "start": "2143137",
    "end": "2151309"
  },
  {
    "text": "And these policies are\nall at the website. ",
    "start": "2151310",
    "end": "2157610"
  },
  {
    "text": "And so the setup, do that,\nif you haven't already. Make sure you're in\nthe discussion forum. We want you to be connected\nwith the ongoing discourse",
    "start": "2157610",
    "end": "2165140"
  },
  {
    "text": "for the class. Do quiz 0 as soon as you can\nso that you know your rights and responsibilities.",
    "start": "2165140",
    "end": "2170390"
  },
  {
    "text": "And then, I think, right now we\nshould check out the sentiment homework to make sure you're\noriented around that before we",
    "start": "2170390",
    "end": "2178130"
  },
  {
    "text": "dive into transformers. Questions about that stuff?",
    "start": "2178130",
    "end": "2183920"
  },
  {
    "text": "It's all at the website, but\nI've kind of evoked it for you in case it raised any issues. ",
    "start": "2183920",
    "end": "2193839"
  },
  {
    "text": "All right. Let's look briefly at\nthe first homework.",
    "start": "2193840",
    "end": "2198910"
  },
  {
    "text": "I feel like we should\nkick it off somehow. And it is maybe an unusual\nmode for homeworks.",
    "start": "2198910",
    "end": "2204430"
  },
  {
    "text": "So, feel free to ask questions. This is kind of cool. So this link will take\nyou to the GitHub, which",
    "start": "2204430",
    "end": "2211090"
  },
  {
    "text": "I think you're probably all set\nup with on your home computers. But you might want to work\nwith this in the cloud.",
    "start": "2211090",
    "end": "2217550"
  },
  {
    "text": "And so this works well. So you could just click\nlike Open in Colab.",
    "start": "2217550",
    "end": "2223069"
  },
  {
    "text": "And I think I've done a\npretty good job of getting you so that it will set itself up\nwith the installs that you need",
    "start": "2223070",
    "end": "2232130"
  },
  {
    "text": "and the course\nrepo and so forth. I would actually be curious to\nknow whether there are bumps along the road to getting\nthis to just work out",
    "start": "2232130",
    "end": "2238280"
  },
  {
    "text": "of the box in Colab. I do encourage this because\nif you're ambitious, you'll probably want GPUs.",
    "start": "2238280",
    "end": "2243920"
  },
  {
    "text": "And this is a good\ninexpensive way to get them. It's also a pretty nice\nenvironment to do the work in.",
    "start": "2243920",
    "end": "2250670"
  },
  {
    "text": "Let's zoom in here. Along the left, you\ncan see the outline. And that's actually kind\nof a good place to start.",
    "start": "2250670",
    "end": "2257640"
  },
  {
    "text": "So we're doing\nmulti-domain sentiment. And what I mean by that\nis you're encouraged",
    "start": "2257640",
    "end": "2263750"
  },
  {
    "text": "to work with three data sets-- DynaSent round 1, DynaSent round\n2, and the Stanford Sentiment",
    "start": "2263750",
    "end": "2270380"
  },
  {
    "text": "Treebank. These are all sentiment\ntasks, and they all have ternary labels--",
    "start": "2270380",
    "end": "2275480"
  },
  {
    "text": "positive, negative, and neutral. But I am not guaranteeing\nyou that those labels are",
    "start": "2275480",
    "end": "2281090"
  },
  {
    "text": "aligned in the semantic sense. In fact, I think\nthat the SST labels are a bit different from\nthe DynaSent set ones.",
    "start": "2281090",
    "end": "2289349"
  },
  {
    "text": "But certainly, the\nunderlying data are different because DynaSent\nis like product reviews",
    "start": "2289350",
    "end": "2295230"
  },
  {
    "text": "and Stanford Sentiment\nTreebank is movie reviews. But there are further things. So DynaSent round\n1 is hard examples",
    "start": "2295230",
    "end": "2302970"
  },
  {
    "text": "that we harvested\nfrom the world, from the Yelp academic data\nset, whereas DynaSent round 2",
    "start": "2302970",
    "end": "2309480"
  },
  {
    "text": "is actually annotators working\non the Dynabench platform, which I mentioned just a\nminute ago trying to fool",
    "start": "2309480",
    "end": "2317700"
  },
  {
    "text": "a really good sentiment model. So the DynaSent round\n2 examples are hard.",
    "start": "2317700",
    "end": "2323230"
  },
  {
    "text": "They involve\nnon-literal language use and sarcasm and other\nthings that we know challenge current day models.",
    "start": "2323230",
    "end": "2330800"
  },
  {
    "text": "So you have these\nthree data sets. And then there are really\ntwo main questions.",
    "start": "2330800",
    "end": "2335990"
  },
  {
    "text": "For the first question,\nI'm just pushing you to develop a simple linear\nmodel with sparse feature",
    "start": "2335990",
    "end": "2343100"
  },
  {
    "text": "representations. This is a kind of more\ntraditional mode background. If you need a refresher on this,\nthis is a chance to get it.",
    "start": "2343100",
    "end": "2350810"
  },
  {
    "text": "If you feel stuck\non this question, I think, we should talk\nabout how to get you up to speed for the course.",
    "start": "2350810",
    "end": "2357750"
  },
  {
    "text": "But for a lot of you, especially\nif you've been immersed in NLP, this should be a pretty\nstraightforward question.",
    "start": "2357750",
    "end": "2363590"
  },
  {
    "text": "It leads to a\npretty good system. So you do a feature\nfunction, you write a function for training\nmodels and a function",
    "start": "2363590",
    "end": "2369950"
  },
  {
    "text": "for assessing models. For each one of these\nquestions, what you do",
    "start": "2369950",
    "end": "2375170"
  },
  {
    "text": "is complete a function\nthat I started. There is not a lot of coding.",
    "start": "2375170",
    "end": "2381050"
  },
  {
    "text": "This is mainly about starting to\nbuild your own original system. And for every single\none of these questions,",
    "start": "2381050",
    "end": "2388460"
  },
  {
    "text": "there's a test that you can run. I like unit tests a lot. I think we should all\nwrite more unit tests.",
    "start": "2388460",
    "end": "2395500"
  },
  {
    "text": "The advantage of\nthe test for me is that if there was any unclarity\nabout my own instructions the test probably clears\nthem up and you also",
    "start": "2395500",
    "end": "2403510"
  },
  {
    "text": "get a guarantee that if\nyour code passes the test, you're in good shape. More or less the same\ntests run on Gradescope.",
    "start": "2403510",
    "end": "2410570"
  },
  {
    "text": "So when you upload\nthe notebook, if you got a clean bill\nof health at home, you'll probably do\nfine on Gradescope.",
    "start": "2410570",
    "end": "2417783"
  },
  {
    "text": "And if you don't, it might\nbe because the Gradescope autograder has a bug. Let me know about that.",
    "start": "2417783",
    "end": "2423099"
  },
  {
    "text": "Those things always\nfeel like they're just barely functioning. But the idea is\nthat this is really",
    "start": "2423100",
    "end": "2429819"
  },
  {
    "text": "not about me evaluating you. This is about you exercising\nthe relevant muscles",
    "start": "2429820",
    "end": "2436060"
  },
  {
    "text": "and building up\nconcepts that will let you develop your own systems. And I'm just trying to be a\ntrusted guide for you on that.",
    "start": "2436060",
    "end": "2444710"
  },
  {
    "text": "So you do some coding, and\nyou have these three questions here. And the result of doing\nthose three questions",
    "start": "2444710",
    "end": "2450130"
  },
  {
    "text": "is that you have\nsomething that could be the basis for\nyour original system. And it'd be pretty\ncool by the way",
    "start": "2450130",
    "end": "2455720"
  },
  {
    "text": "if some people competed using\njust sparse linear models to show the transformers\nthat there's still",
    "start": "2455720",
    "end": "2460850"
  },
  {
    "text": "competition out there. So that's the first question. And then the second\none is the same way,",
    "start": "2460850",
    "end": "2468480"
  },
  {
    "text": "except now we're focused on\ntransformer fine tuning, which is our main focus for this unit.",
    "start": "2468480",
    "end": "2475260"
  },
  {
    "text": "I have a question here that\npushes you to understand how these models tokenize data.",
    "start": "2475260",
    "end": "2481260"
  },
  {
    "text": "It's really different\nfrom the old mode. You'll learn some\nHugging Face code, and you'll also\nlearn some concepts.",
    "start": "2481260",
    "end": "2487770"
  },
  {
    "text": "And then I have a question that\npushes you to understand what the representations are like. We're going to talk\nabout them abstractly.",
    "start": "2487770",
    "end": "2494340"
  },
  {
    "text": "Here, you'll be\nhands on with them. And then finally, you're\ngoing to finish up writing a PyTorch module\nwhere you fine tune BERT.",
    "start": "2494340",
    "end": "2504109"
  },
  {
    "text": "And that is step 1 to a really\nincredible system, I'm sure.",
    "start": "2504110",
    "end": "2511940"
  },
  {
    "text": "And I've actually written\nthe interface for you so that given the course\ncode and everything else,",
    "start": "2511940",
    "end": "2517030"
  },
  {
    "text": "the interfaces for\nthese things are pretty straightforward to write. All you have to do\nis write the module. And for completing the\nhomework questions,",
    "start": "2517030",
    "end": "2523760"
  },
  {
    "text": "you don't actually need\nheavy duty computing at all because you don't do\nanything heavy duty.",
    "start": "2523760",
    "end": "2528920"
  },
  {
    "text": "But when you get to\nthe original system, that might be where you want\nto train a big monster model",
    "start": "2528920",
    "end": "2534799"
  },
  {
    "text": "and figure out how to work with\nthe computational resources that you have to get that done.",
    "start": "2534800",
    "end": "2540109"
  },
  {
    "text": "This notebook is using\nTiny BERT, which is small, but you still need a\nGPU to do the work.",
    "start": "2540110",
    "end": "2546740"
  },
  {
    "text": "So you'll still want to be on\nColab or something like that. And then I don't\nknow how ambitious",
    "start": "2546740",
    "end": "2552470"
  },
  {
    "text": "you're going to get for\nyour original system. You can tell that I'm\ntrying to lead you toward using question\n1 and question",
    "start": "2552470",
    "end": "2558340"
  },
  {
    "text": "2 for your original system,\nbut it's not required. And if you want to do something\nwhere you just prompt GPT-4,",
    "start": "2558340",
    "end": "2564580"
  },
  {
    "text": "maybe you'll win. I don't know. I'm up for anything. It does need to be\nan original system.",
    "start": "2564580",
    "end": "2571310"
  },
  {
    "text": "So you can't just download\nsomebody else's code. And if all you did was a\nvery boring prompt structure,",
    "start": "2571310",
    "end": "2576730"
  },
  {
    "text": "you wouldn't get a high grade\non your original system. We're trying to encourage you\nto think creatively and explore.",
    "start": "2576730",
    "end": "2583670"
  },
  {
    "text": "And then the final thing is you\njust enter this in a bake off. And really that just means\ngrabbing an unlabeled data set",
    "start": "2583670",
    "end": "2589550"
  },
  {
    "text": "from the web and adding a\ncolumn with predictions in it. And then you upload\nthat when you",
    "start": "2589550",
    "end": "2595010"
  },
  {
    "text": "submit your work to Gradescope. And then when everyone's\nsubmissions are in, we'll reveal the scores.",
    "start": "2595010",
    "end": "2601140"
  },
  {
    "text": "And there'll be some winners. And we'll give out prizes. I'm optimistic that we're going\nto have EC2 codes as prizes.",
    "start": "2601140",
    "end": "2608832"
  },
  {
    "text": "That's always been fun\nbecause if you win a bake off, you get a little more\ncompute resources for your next big thing.",
    "start": "2608832",
    "end": "2616470"
  },
  {
    "text": "They've become they don't want\nto hand out these codes anymore like they used to because cloud\ncompute is so important now.",
    "start": "2616470",
    "end": "2622859"
  },
  {
    "text": "But I think I have an\narrangement in place to get some. And by the way, we give out\nprizes for the best systems",
    "start": "2622860",
    "end": "2630210"
  },
  {
    "text": "and the most creative systems. And we have even\ngiven out prizes for the lowest scoring\nsystem, because if that",
    "start": "2630210",
    "end": "2636150"
  },
  {
    "text": "was a cool thing that\nshould have worked and didn't, I feel like you\ndid a service to all of us by going down that route.",
    "start": "2636150",
    "end": "2642750"
  },
  {
    "text": "And that deserves\na prize as a trying to have a multi-dimensional\nleaderboard to your right",
    "start": "2642750",
    "end": "2648780"
  },
  {
    "text": "even as we rank all\nof you according to the performance\nof your systems. ",
    "start": "2648780",
    "end": "2656279"
  },
  {
    "text": "That's my overview. Questions or\ncomments or anything? ",
    "start": "2656280",
    "end": "2670640"
  },
  {
    "text": "All right. I propose then that\nwe go to transformers.",
    "start": "2670640",
    "end": "2678020"
  },
  {
    "text": "So download the handout. And by the way, these\nshould be really good.",
    "start": "2678020",
    "end": "2683180"
  },
  {
    "text": "So these slides, you'll get\na version with fewer overlays to make it more browsable. All of these things\nup here are links.",
    "start": "2683180",
    "end": "2690380"
  },
  {
    "text": "So oops! So if you click\non these bubbles,",
    "start": "2690380",
    "end": "2696690"
  },
  {
    "text": "you can go directly\nto that part. And you can see that this is a\nkind of outline of this unit. And then there's\nalso a good table",
    "start": "2696690",
    "end": "2703170"
  },
  {
    "text": "of contents with good labels. So if you need to find\nthings in what I admit is a very large\ndeck, that should",
    "start": "2703170",
    "end": "2709590"
  },
  {
    "text": "make it easier to do that. And you can also\ntrack our progress as we move through these things. ",
    "start": "2709590",
    "end": "2722619"
  },
  {
    "text": "Should we dive in? Guiding ideas. What is happening with these\ncontextual representations?",
    "start": "2722620",
    "end": "2731100"
  },
  {
    "text": "This one slide here used to\ntake two weeks for this course. And I've been trying\nto convey this.",
    "start": "2731100",
    "end": "2737300"
  },
  {
    "text": "We've stopped doing that. The background materials\nare still at the website. It was also the first\ntwo weeks of CS224N.",
    "start": "2737300",
    "end": "2745230"
  },
  {
    "text": "We did them before they\ndid them in CS224N. Back before, natural language\nunderstanding was all the rage.",
    "start": "2745230",
    "end": "2752590"
  },
  {
    "text": "But they get there first now,\nand it is a more basic course. I'm saying they do\nGloVe, Word2Vec,",
    "start": "2752590",
    "end": "2757619"
  },
  {
    "text": "and we're going to dive\nright into transformers. Here is my one slide\nsummary of this.",
    "start": "2757620",
    "end": "2763680"
  },
  {
    "text": "Back in the old, old days-- the dawning of the statistical\nrevolution in NLP--",
    "start": "2763680",
    "end": "2770140"
  },
  {
    "text": "the way we represented examples,\nlet's say, words for this case was with feature-based\nsparse representations.",
    "start": "2770140",
    "end": "2776910"
  },
  {
    "text": "And what I mean by that is\nthat if you wanted to represent a word of a language, you\nmight write a feature function",
    "start": "2776910",
    "end": "2782490"
  },
  {
    "text": "that says, OK, yes or\nno on it being referring to an animate thing,\nyes or no on it",
    "start": "2782490",
    "end": "2788610"
  },
  {
    "text": "ending in the characters\nING, yes or no on it mostly",
    "start": "2788610",
    "end": "2794440"
  },
  {
    "text": "being used as a verb,\nand so forth and so on. And so all these little\nfeature functions would end up giving you really\nlong vectors of essentially 1s",
    "start": "2794440",
    "end": "2802360"
  },
  {
    "text": "and 0s that were hand-designed\nand that would give you",
    "start": "2802360",
    "end": "2807640"
  },
  {
    "text": "a perspective on a bunch of\nthe dimensions of the word you were trying to represent.",
    "start": "2807640",
    "end": "2813640"
  },
  {
    "text": "That lasted for a while. And then it started to get\nreplaced pre Word2Vec and GloVe",
    "start": "2813640",
    "end": "2821260"
  },
  {
    "text": "with methods like point-wise\nmutual information or TF-IDF.",
    "start": "2821260",
    "end": "2826840"
  },
  {
    "text": "These methods had long been\nrecognized as fundamental in the field of\ninformation retrieval,",
    "start": "2826840",
    "end": "2833320"
  },
  {
    "text": "especially TF-IDF as\na main representation technique for finding relevant\ndocuments for queries.",
    "start": "2833320",
    "end": "2840310"
  },
  {
    "text": "It took a while for\nNLP people to realize that they would be valuable. But what you start\nto see here is",
    "start": "2840310",
    "end": "2847630"
  },
  {
    "text": "that instead of writing all\nthose feature functions, I'll just keep track of\nco-occurrence patterns",
    "start": "2847630",
    "end": "2853030"
  },
  {
    "text": "in large collections of text. And PMI and TF-IDF\ndo this essentially",
    "start": "2853030",
    "end": "2859550"
  },
  {
    "text": "just by counting and then\nreweighting some of the counts. But really, it is\nthe rawest form",
    "start": "2859550",
    "end": "2864859"
  },
  {
    "text": "of distributional\nrepresentation. That got replaced or this\nis sort of simultaneous",
    "start": "2864860",
    "end": "2871880"
  },
  {
    "text": "in an interesting\nway, but you have paired with PMI\nand TF-IDF methods like principal\ncomponents analysis;",
    "start": "2871880",
    "end": "2879200"
  },
  {
    "text": "SVD, which is sometimes called\nlatent semantic analysis; LDA, which is latent Dirichlet\nallocation a topic modeling",
    "start": "2879200",
    "end": "2887030"
  },
  {
    "text": "technique. It's a whole family of these\nthings that are essentially taking count data and giving\nyou reduced dimensional versions",
    "start": "2887030",
    "end": "2894710"
  },
  {
    "text": "of that count data. And the power of\ndoing that is really that you can capture higher\norder notions of co-occurrence,",
    "start": "2894710",
    "end": "2902330"
  },
  {
    "text": "not just what I as\na word co-occurred with but also the\nsense in which I might",
    "start": "2902330",
    "end": "2907609"
  },
  {
    "text": "co-occur with words that\nco-occur with the things I co-occur with. You're kind of\nsecond-order neighbors.",
    "start": "2907610",
    "end": "2914670"
  },
  {
    "text": "And you can imagine\ntraveling out into this representational\nneighborhood here. And that turns out\nto be very powerful",
    "start": "2914670",
    "end": "2921440"
  },
  {
    "text": "because a lot of semantic\naffinities come not from just being neighbors with\nsomething, but rather from that whole\nnetwork of things",
    "start": "2921440",
    "end": "2928920"
  },
  {
    "text": "co-occurring with each other. And what these methods do\nis take all that count data and compress it in a way\nthat loses some information",
    "start": "2928920",
    "end": "2936510"
  },
  {
    "text": "but also captures those\nnotions of similarity. And then the final step,\nwhich might actually",
    "start": "2936510",
    "end": "2943720"
  },
  {
    "text": "be the kind of final\nstep in this literature were learned dimensionality\nreduction things--",
    "start": "2943720",
    "end": "2949870"
  },
  {
    "text": "autoencoders,\nWord2Vec, and GloVe. And this is where you might\nstart with some count data,",
    "start": "2949870",
    "end": "2956290"
  },
  {
    "text": "but you have some machine\nlearning algorithm that learns how to compute\ndense learned representations",
    "start": "2956290",
    "end": "2964510"
  },
  {
    "text": "from that count data. So kind of like step\n3 infused with more",
    "start": "2964510",
    "end": "2970580"
  },
  {
    "text": "of what we know of as\nmachine learning now. And I say it might be the\nend because I think now",
    "start": "2970580",
    "end": "2978270"
  },
  {
    "text": "for anything that you\nwould do with this mode, you would probably just use\ncontextual representations.",
    "start": "2978270",
    "end": "2984099"
  },
  {
    "text": "So this is the\nfull story perhaps.  And then here's the\nreview, if you want.",
    "start": "2984100",
    "end": "2991242"
  },
  {
    "text": "And I think it is\nimportant to understand this both the history, but also\nthe technical details to really",
    "start": "2991242",
    "end": "2996460"
  },
  {
    "text": "deeply understand what\nI'm about to dive into. So you might want to circle\nback, if that was too fast.",
    "start": "2996460",
    "end": "3002360"
  },
  {
    "text": "Yeah. Is there any option\nto just like one hot encode your entire vocabulary?",
    "start": "3002360",
    "end": "3008050"
  },
  {
    "text": "I think this is my\nunderstanding of what modern transformer-based\nmodels do. To one hot encode\nthe whole vocabulary?",
    "start": "3008050",
    "end": "3016060"
  },
  {
    "text": "Yes. So I'll just say a bit more\nlike, what are you doing?",
    "start": "3016060",
    "end": "3021460"
  },
  {
    "text": "Like, my understanding of how\nlarge language models encode",
    "start": "3021460",
    "end": "3027730"
  },
  {
    "text": "individual words\nis they have a list of all of their possible tokens. They break it down into tokens.",
    "start": "3027730",
    "end": "3033980"
  },
  {
    "text": "And then if your token\nis 337, you're just like, you have a vector of\nlength, the number",
    "start": "3033980",
    "end": "3039070"
  },
  {
    "text": "of tokens you have like a\nvocabulary of like 50,000. And then you just one hot\nencode which token that is.",
    "start": "3039070",
    "end": "3046680"
  },
  {
    "text": "Well, we're about to do this. I'll show you how\nthey represent things, and let's see if it\nconnects with your question",
    "start": "3046680",
    "end": "3056290"
  },
  {
    "text": "because it is different. It is going to be\nvery different. And the notion of tokenization\nand the notion of type",
    "start": "3056290",
    "end": "3061870"
  },
  {
    "text": "is about to get complicated. Before we do the technical part,\njust a little bit of context",
    "start": "3061870",
    "end": "3069440"
  },
  {
    "text": "here about why I think\nthis is so exciting. I'm a linguist. And I was excited by these\nstatic vector representations",
    "start": "3069440",
    "end": "3076250"
  },
  {
    "text": "of words. But it was also\nvery annoying to me because they give you\none vector per word,",
    "start": "3076250",
    "end": "3083240"
  },
  {
    "text": "whereas my experience\nof language is that words have\nmultiple senses.",
    "start": "3083240",
    "end": "3088500"
  },
  {
    "text": "And it is hard to delimit\nwhere the senses begin and end. Consider a verb\nlike break, which",
    "start": "3088500",
    "end": "3094010"
  },
  {
    "text": "I've worked on with my PhD\nstudent Erica Peterson. The vase broke, that's\none sense maybe.",
    "start": "3094010",
    "end": "3099829"
  },
  {
    "text": "Dawn broke, that's\nthe same form broke, but that means something\ndifferent, entirely different.",
    "start": "3099830",
    "end": "3107839"
  },
  {
    "text": "The news broke, again,\nbroke as the form there, but this being something more\nlike was published or appeared.",
    "start": "3107840",
    "end": "3115400"
  },
  {
    "text": "Sandy broke the world record\nis very unlike the vase broke.",
    "start": "3115400",
    "end": "3120500"
  },
  {
    "text": "Now it's like\nsurpassing the limit. Sandy broke the law is a\ndifferent sense yet again.",
    "start": "3120500",
    "end": "3126599"
  },
  {
    "text": "That's some kind\nof transgression. The burglar broke\ninto the house, that's break again but\nnow with a particle.",
    "start": "3126600",
    "end": "3133770"
  },
  {
    "text": "And that means something\ndifferent still. The newscaster broke\ninto the movie broadcast, that means it was interrupted--",
    "start": "3133770",
    "end": "3140790"
  },
  {
    "text": "very different again. We broke even means-- I don't know-- we ended\nup back at the same amount",
    "start": "3140790",
    "end": "3145920"
  },
  {
    "text": "we started with. So, how many senses\nof break are here? If I was in the old mode\nof static representation,",
    "start": "3145920",
    "end": "3153480"
  },
  {
    "text": "would I survive with\none break vector for all of these\nexamples or would I",
    "start": "3153480",
    "end": "3158730"
  },
  {
    "text": "have one per example type? But then what about all the\nones that I didn't list here?",
    "start": "3158730",
    "end": "3164805"
  },
  {
    "text": "The number of senses\nfor break starts to feel impossible to\nenumerate, if you just",
    "start": "3164805",
    "end": "3170460"
  },
  {
    "text": "think about all the ways in\nwhich you encounter this verb. And there is some\nmetaphorical core",
    "start": "3170460",
    "end": "3175710"
  },
  {
    "text": "that seems to run through them. But in the details, these\nsenses are all very different.",
    "start": "3175710",
    "end": "3182080"
  },
  {
    "text": "And this tells me that the\nsense of a word like break is being modulated by the\ncontext it is appearing in.",
    "start": "3182080",
    "end": "3190630"
  },
  {
    "text": "And the idea that we would\nhave one fixed representation for it, even if it's\nlearned from data,",
    "start": "3190630",
    "end": "3195660"
  },
  {
    "text": "is just kind of wrong\nfrom the outset. Here's another example.",
    "start": "3195660",
    "end": "3201160"
  },
  {
    "text": "We have a flat tire. But what about flat beer,\nflat note, flat surface?",
    "start": "3201160",
    "end": "3207180"
  },
  {
    "text": "Maybe they have some\nmetaphorical core, but those feel like at least\ntwo to four different senses",
    "start": "3207180",
    "end": "3213570"
  },
  {
    "text": "for flat. Throw a party, throw a fight,\nthrow a ball, throw a fit,",
    "start": "3213570",
    "end": "3219090"
  },
  {
    "text": "all very different senses\nit's tragic to think we would have one\nthrow that was meant",
    "start": "3219090",
    "end": "3224970"
  },
  {
    "text": "to cover all of these examples. A crane caught a fish,\na crane picked up",
    "start": "3224970",
    "end": "3231480"
  },
  {
    "text": "the steel beam, that might\nfeel like a standard sort of lexical ambiguity.",
    "start": "3231480",
    "end": "3236530"
  },
  {
    "text": "And so maybe you\ncan imagine that we have one vector for crane\nas a bird and one for crane",
    "start": "3236530",
    "end": "3242460"
  },
  {
    "text": "is a machine. But is that going to work\nfor the entire vocabulary? I suspect not.",
    "start": "3242460",
    "end": "3248580"
  },
  {
    "text": "I saw a crane-- we wouldn't\neven know what vector we were dealing with there, right? Which one would we pick?",
    "start": "3248580",
    "end": "3253922"
  },
  {
    "text": "Now we have another\nproblem on our hands, which is selecting the static\nvector based on context, right?",
    "start": "3253922",
    "end": "3259753"
  },
  {
    "text": "How are we going to do that? And this is a really deep thing. It's not just about the local\nkind of morphosyntactic context",
    "start": "3259753",
    "end": "3266750"
  },
  {
    "text": "here. What about, are there typos? I didn't see any. So the sense of any there is\nlike any typos, right, versus",
    "start": "3266750",
    "end": "3275440"
  },
  {
    "text": "are there bookstores downtown? I didn't see any. Now the sense of any and\nthe kind of elliptical stuff",
    "start": "3275440",
    "end": "3280839"
  },
  {
    "text": "that comes after it\nis any bookstores. And now I hope you can see that\nthe sense that words can have",
    "start": "3280840",
    "end": "3288160"
  },
  {
    "text": "is modulated by context in\nthe most extended sense. And having fixed\nstatic representations",
    "start": "3288160",
    "end": "3295359"
  },
  {
    "text": "was never going to work in the\nface of all of this diversity. We were never\ngoing to figure out",
    "start": "3295360",
    "end": "3301640"
  },
  {
    "text": "how to cut up the senses\nin just the right way to get all of this\ndata handled correctly.",
    "start": "3301640",
    "end": "3309420"
  },
  {
    "text": "And the vision of contextual\nrepresentation models is that you're not\neven going to try to do",
    "start": "3309420",
    "end": "3315089"
  },
  {
    "text": "all that hard and boring stuff. Instead, you are just\ngoing to embrace the fact that every word could take\non a different sense, that",
    "start": "3315090",
    "end": "3322740"
  },
  {
    "text": "is have a different\nrepresentation depending on everything that is\nhappening around it.",
    "start": "3322740",
    "end": "3328260"
  },
  {
    "text": "And we won't have to decide\nthen which sense is in 1A and whether it's different\nfrom 1B and 1C and so forth.",
    "start": "3328260",
    "end": "3333880"
  },
  {
    "text": "We will just have all of these\ntoken-level representations.",
    "start": "3333880",
    "end": "3340019"
  },
  {
    "text": "It will be entirely\na theory that is based in words as\nthey appear in context.",
    "start": "3340020",
    "end": "3345070"
  },
  {
    "text": " For me as a linguist,\nit is not surprising",
    "start": "3345070",
    "end": "3350380"
  },
  {
    "text": "at all that this turns out to\nlead to lots of engineering successes because it feels so\ndeeply right to me about how",
    "start": "3350380",
    "end": "3357250"
  },
  {
    "text": "language works.  Brief history here, I just\nwant to be dutiful about this",
    "start": "3357250",
    "end": "3363835"
  },
  {
    "text": "and make sure people get\ncredit where it's due. November 2015 Dai and Le, that's\na foundational paper where they",
    "start": "3363835",
    "end": "3370840"
  },
  {
    "text": "really did what is probably the\nfirst example of language model pretraining.",
    "start": "3370840",
    "end": "3376690"
  },
  {
    "text": "It's a cool paper to look at. It's complicated in some ways\nthat are surprising to us now.",
    "start": "3376690",
    "end": "3382030"
  },
  {
    "text": "And it is certainly\na visionary paper. And then McCann et al, this is\na paper from Salesforce Research",
    "start": "3382030",
    "end": "3388030"
  },
  {
    "text": "that's led by-- at the time\nwas read by Richard Socher, who was a distinguished\nalum of this class--",
    "start": "3388030",
    "end": "3393730"
  },
  {
    "text": "proud of that. They developed a CoVe\nmodel where what they did is train machine\ntranslation models.",
    "start": "3393730",
    "end": "3400090"
  },
  {
    "text": "And then the inspired idea\nwas that the translation representations might be\nuseful for other tasks.",
    "start": "3400090",
    "end": "3407890"
  },
  {
    "text": "And again, that\nfeels like the dawn of the notion of pretraining\ncontextual representations.",
    "start": "3407890",
    "end": "3413890"
  },
  {
    "text": "And then ELMo came. I mentioned ELMo last\ntime-- huge breakthrough, massive bidirectional LSTMs.",
    "start": "3413890",
    "end": "3421120"
  },
  {
    "text": "And they really\nshowed that could lead to rich multi-purpose\nrepresentations.",
    "start": "3421120",
    "end": "3426549"
  },
  {
    "text": "And that's where you\nreally feel everyone reorienting their research\ntoward these kind of models.",
    "start": "3426550",
    "end": "3432760"
  },
  {
    "text": "That's not a transformer-based\none, though, that's by LSTMs. And then we get GPT in June\n2018 and BERT in October 2018.",
    "start": "3432760",
    "end": "3446109"
  },
  {
    "text": "The BERT paper was published\na long time after that. But as I said before,\nit had already achieved massive\nimpact by the time",
    "start": "3446110",
    "end": "3452500"
  },
  {
    "text": "it was published in\n2019 or whatever. So that's why I've been giving\nthe months here because you",
    "start": "3452500",
    "end": "3457510"
  },
  {
    "text": "can see it's really there\nwas this sudden uptake in the amount of\ninterest in these things",
    "start": "3457510",
    "end": "3463030"
  },
  {
    "text": "that happened around this time. And that led to\nwhere we are now.",
    "start": "3463030",
    "end": "3469690"
  },
  {
    "text": "Another kind of interesting\nthing to think about, if you step back for the context\nhere, is that we as a field",
    "start": "3469690",
    "end": "3476530"
  },
  {
    "text": "have been traveling from\nhigh-bias models, where we decide a lot about how\nthe data should look and be",
    "start": "3476530",
    "end": "3483100"
  },
  {
    "text": "processed toward models\nthat impose essentially nothing on the world. So if you go up into\nthe upper left here,",
    "start": "3483100",
    "end": "3489850"
  },
  {
    "text": "I'm just imagining\na model that's in the old mode, where you\nhave your GLoVe representations of these three words.",
    "start": "3489850",
    "end": "3496150"
  },
  {
    "text": "And to get a representation\nfor the sentence, you just add up those\nrepresentations. In doing that, you have decided\nahead of time a lot of stuff",
    "start": "3496150",
    "end": "3506050"
  },
  {
    "text": "about how those pieces are\ngoing to come together. I mean, you just\nsaid it was going to be addition, which is almost\ncertainly not correct about how",
    "start": "3506050",
    "end": "3514300"
  },
  {
    "text": "the world works. So that's a prototypical\ncase of a high-bias decision.",
    "start": "3514300",
    "end": "3520000"
  },
  {
    "text": "If you move over\nto the right here, that's a kind of\nrecurrent neural network. And here, I've kind of\ndecided that my data will",
    "start": "3520000",
    "end": "3528440"
  },
  {
    "text": "be processed left to right. I could learn a lot of different\nfunctions in that space.",
    "start": "3528440",
    "end": "3533700"
  },
  {
    "text": "So it's much more\nexpressive, much less biased in this\nmachine learning sense than this solution here.",
    "start": "3533700",
    "end": "3539250"
  },
  {
    "text": "But I have still\ndecided ahead of time that I'm going to\ngo left to right. And this is another example.",
    "start": "3539250",
    "end": "3545670"
  },
  {
    "text": "These are\ntree-structured networks. Richard Socher, who\nI just mentioned was truly a pioneer in\ntree-structured recursive",
    "start": "3545670",
    "end": "3552680"
  },
  {
    "text": "neural networks. Here, I make a lot of decisions\nabout how the pieces can possibly come together.",
    "start": "3552680",
    "end": "3558319"
  },
  {
    "text": "The Rock is a unit a\nconstituent separate from rules, which comes in later.",
    "start": "3558320",
    "end": "3563599"
  },
  {
    "text": "And I'm just saying,\nI know ahead of time that the data will\nwork that way. If I'm right, it's going to give\nme a huge boost because I don't",
    "start": "3563600",
    "end": "3571940"
  },
  {
    "text": "have to learn those details. If I'm wrong, though, I\nmight be wrong forever.",
    "start": "3571940",
    "end": "3577603"
  },
  {
    "text": "And I think that's actually\nthat feeling that you're wrong forever is what led to this\nkind of thing happening. So here, I've got kind of a\nbidirectional recurrent model.",
    "start": "3577603",
    "end": "3586329"
  },
  {
    "text": "So now you can go left\nto right and right to left and all these\nattention mechanisms that",
    "start": "3586330",
    "end": "3592320"
  },
  {
    "text": "are going to like jump\naround in the linear string. And this is a true\nprogression with what",
    "start": "3592320",
    "end": "3598680"
  },
  {
    "text": "happened with\nrecurrent recursive or sorry, recurrent\nneural networks. Go both directions and add a\nbunch of attention connections,",
    "start": "3598680",
    "end": "3606450"
  },
  {
    "text": "and that is kind of the\nthing that caused everyone to realize, oh, we should\njust connect everything",
    "start": "3606450",
    "end": "3613500"
  },
  {
    "text": "to everything else and go to\nthe maximally low-biased version of this, and just assume\nthat the data will teach us",
    "start": "3613500",
    "end": "3620640"
  },
  {
    "text": "about what's important\nto connect to what. We won't decide\nanything ahead of time. So a triumph of saying,\nI have no idea what",
    "start": "3620640",
    "end": "3628900"
  },
  {
    "text": "the world is going to be like. I just trust in my data\nand my optimization.",
    "start": "3628900",
    "end": "3635260"
  },
  {
    "text": "The attention piece is\nreally interesting to me. We used to talk a lot\nabout this in the course. Here I have a sequence\nreally not so good.",
    "start": "3635260",
    "end": "3643780"
  },
  {
    "text": "And a common mode\nstill common today is that I might fit a classifier\non this final representation",
    "start": "3643780",
    "end": "3650470"
  },
  {
    "text": "here to make a\nsentiment decision. But people went on\nthat journey I just",
    "start": "3650470",
    "end": "3656720"
  },
  {
    "text": "described where you\nthink, wait a second, if I'm just going\nto use this, won't I",
    "start": "3656720",
    "end": "3662140"
  },
  {
    "text": "lose a lot of information\nabout the earlier words? I should have some way\nof connecting back.",
    "start": "3662140",
    "end": "3668780"
  },
  {
    "text": "And so what they did is\ndot products essentially between the thing\nthat you're using for your classifier and\nthe previous states--",
    "start": "3668780",
    "end": "3676270"
  },
  {
    "text": "that's what I've depicted here-- just as a kind of way of scoring\nthis final thing with respect",
    "start": "3676270",
    "end": "3682210"
  },
  {
    "text": "to the previous thing. You might normalize\nthem a little bit and then form, what was\ncalled a context vector.",
    "start": "3682210",
    "end": "3688780"
  },
  {
    "text": "This is like the\nattention representation. And what I've done here\nis build these links back",
    "start": "3688780",
    "end": "3694660"
  },
  {
    "text": "to all these previous states. And that turned out to\nbe incredibly powerful. And when you read the\ntitle of the paper,",
    "start": "3694660",
    "end": "3701950"
  },
  {
    "text": "\"Attention is All you Need,\"\nwhat they are doing is saying, you don't need LSTM\nconnections and recurrent",
    "start": "3701950",
    "end": "3709480"
  },
  {
    "text": "connections and stuff. The sense in which\nattention is all you need is the sense in\nwhich they're saying,",
    "start": "3709480",
    "end": "3714520"
  },
  {
    "text": "all you needed was\nthese connections that you were adding onto the\ntop of your earlier models. ",
    "start": "3714520",
    "end": "3721920"
  },
  {
    "text": "And maybe they were right. But certainly, it has\ntaken over the field.",
    "start": "3721920",
    "end": "3728970"
  },
  {
    "text": "Another important idea here\nthat might often be overlooked is just this notion\nthat we should",
    "start": "3728970",
    "end": "3734190"
  },
  {
    "text": "model the subparts of words. And again, I can't resist\na historical note here.",
    "start": "3734190",
    "end": "3739599"
  },
  {
    "text": "If you look back at the ELMo\npaper, what they did to embrace this insight is incredible.",
    "start": "3739600",
    "end": "3746140"
  },
  {
    "text": "They had character-level\nrepresentations. And then they fit all\nthese convolutions",
    "start": "3746140",
    "end": "3752190"
  },
  {
    "text": "on top of all those\ncharacter-level representations,\nthose essentially like ways of pooling together\nsubparts of the word.",
    "start": "3752190",
    "end": "3759210"
  },
  {
    "text": "And then they form\na representation at the top that's like the\naverage plus the concatenation",
    "start": "3759210",
    "end": "3765180"
  },
  {
    "text": "of all of these different\nconvolutional layers. And the result of\nthis is a vocabulary",
    "start": "3765180",
    "end": "3770940"
  },
  {
    "text": "that does latently\nhave information about characters and subparts\nof words as well as words in it.",
    "start": "3770940",
    "end": "3778210"
  },
  {
    "text": "And I feel that\nthat's deeply right. And this is like\na space in which you could capture lots\nof things like how",
    "start": "3778210",
    "end": "3784660"
  },
  {
    "text": "talk is similar to talking\nand is similar to talked. And all that stuff that a simple\nunigram parsing would miss",
    "start": "3784660",
    "end": "3792610"
  },
  {
    "text": "is latently represented\nin this space. But the vocabulary for\nELMo is like 100,000 words.",
    "start": "3792610",
    "end": "3802070"
  },
  {
    "text": "So that's 100,000 embedding\nspace that I need to have. It's absolutely\ngargantuan, and it's still",
    "start": "3802070",
    "end": "3809510"
  },
  {
    "text": "the case that if you process\nreal data, you have to ink out, that is mark as unknown,\nmost of the words",
    "start": "3809510",
    "end": "3816349"
  },
  {
    "text": "you encounter because the\nlanguage is incredibly complicated. And 100,000 doesn't even\ncome close to covering",
    "start": "3816350",
    "end": "3823670"
  },
  {
    "text": "all the tokens that you\nencounter in the world. And so again, we have\nthis kind of galaxy brain",
    "start": "3823670",
    "end": "3829640"
  },
  {
    "text": "moment where I guess the\nfield says, forget all that. And what you do instead\nis tokenize your data",
    "start": "3829640",
    "end": "3837020"
  },
  {
    "text": "so that you just\nsplit apart words into their sub-word\ntokens if you need to.",
    "start": "3837020",
    "end": "3844500"
  },
  {
    "text": "So here I've got an example\nwith the BERT tokenizer. This isn't too\nsurprising that comes out looking kind of normal.",
    "start": "3844500",
    "end": "3850880"
  },
  {
    "text": "But if you do Encode me,\nnotice that the word encode has been split into two tokens.",
    "start": "3850880",
    "end": "3857990"
  },
  {
    "text": "And if you do Snuffleupagus,\nyou get 1, 2, 3, 4, 5, 6,",
    "start": "3857990",
    "end": "3863860"
  },
  {
    "text": "7 tokens-- 6 or 7 from that-- because it\ndoesn't know what the word is.",
    "start": "3863860",
    "end": "3868870"
  },
  {
    "text": "And so what it does is\nnot unk it out but rather split it apart into a\nbunch of different pieces.",
    "start": "3868870",
    "end": "3874820"
  },
  {
    "text": "And the result is the\nreally startling thing that BERT-like models\nhave only 30,000 words",
    "start": "3874820",
    "end": "3880600"
  },
  {
    "text": "in their vocabulary. But they're words in the sense\nthat these sub-word tokens.",
    "start": "3880600",
    "end": "3887840"
  },
  {
    "text": "Now this was going\nto be tragically bad in the realm where\nwe were doing static word",
    "start": "3887840",
    "end": "3894740"
  },
  {
    "text": "representations because I'm\ngoing to have a vector for NU and have no sense in which it\nwas participating in the larger",
    "start": "3894740",
    "end": "3902900"
  },
  {
    "text": "part of Snuffleupagus. But we're talking about\ncontextual models. So even if these are\nthe tokens, the model",
    "start": "3902900",
    "end": "3909200"
  },
  {
    "text": "is going to see\nthe full sequence. And we can hope that it\nreconstructs something",
    "start": "3909200",
    "end": "3914210"
  },
  {
    "text": "like the word from all the\npieces that it encountered. Certainly, we could hope that\nfor something like encode.",
    "start": "3914210",
    "end": "3921680"
  },
  {
    "text": "And we take this\nfor granted now, but it's deeply insightful\nto me and incredibly",
    "start": "3921680",
    "end": "3927530"
  },
  {
    "text": "freeing in terms of the\nengineering resources that you need. But it does depend on rich\ncontextual representations.",
    "start": "3927530",
    "end": "3935720"
  },
  {
    "text": "And then another notion,\npositional encoding. So we have all these tokens\nor maybe subparts of words.",
    "start": "3935720",
    "end": "3943870"
  },
  {
    "text": "In addition to\nrepresenting things using a traditional\nstatic embedding",
    "start": "3943870",
    "end": "3948900"
  },
  {
    "text": "space, like a GloVe 1-- that's what I put with these\nlight gray boxes here--",
    "start": "3948900",
    "end": "3954210"
  },
  {
    "text": "we'll also represent sequences\nwith positional encodings, which will just keep track\nof where the token appeared",
    "start": "3954210",
    "end": "3961289"
  },
  {
    "text": "in the sequence I'm processing. And what that means is that\nthe word rock here occurring",
    "start": "3961290",
    "end": "3967760"
  },
  {
    "text": "in position 2 will have a\ndifferent representation if Rock appears in\nposition 47 in the string.",
    "start": "3967760",
    "end": "3976550"
  },
  {
    "text": "It'll be kind of the\nsame word but also partly a different word.",
    "start": "3976550",
    "end": "3982339"
  },
  {
    "text": "And that's another\nway in which you're embracing the fact that all\nof these representations are going to be contextual.",
    "start": "3982340",
    "end": "3989660"
  },
  {
    "text": "This is an interesting\nstory for me because I've been\nslow to realize what may be the whole field\nalready knew that this",
    "start": "3989660",
    "end": "3995990"
  },
  {
    "text": "is incredibly important. How you do this\npositional encoding really matters for how models perform.",
    "start": "3995990",
    "end": "4002079"
  },
  {
    "text": "And that's why, in fact, it's\nlike one of these early topics here that we'll talk\nabout next time. ",
    "start": "4002080",
    "end": "4009940"
  },
  {
    "text": "And then, of course,\nanother guiding idea here is that we are going to\ndo massive scale pretraining.",
    "start": "4009940",
    "end": "4016800"
  },
  {
    "text": "I mentioned this before. We're going to have these\ncontextual models with all these tiny little\nparts of words in them",
    "start": "4016800",
    "end": "4023609"
  },
  {
    "text": "all in sequences with\npositional encodings, and we are going to train\nat an incredible scale.",
    "start": "4023610",
    "end": "4028950"
  },
  {
    "text": "That's that same story\nof Word2Vec, GloVe, through GPT, and\nthen on up to GPT 3.",
    "start": "4028950",
    "end": "4035100"
  },
  {
    "text": "I mentioned this before. And some magic happens as you\ndo this on more and more data",
    "start": "4035100",
    "end": "4042329"
  },
  {
    "text": "with larger and larger models. And then finally,\nthis is related,",
    "start": "4042330",
    "end": "4049600"
  },
  {
    "text": "this insight that instead\nof starting from scratch for my machine\nlearning models, I",
    "start": "4049600",
    "end": "4056350"
  },
  {
    "text": "should start with\na pre-trained one and fine tune it for\nparticular tasks.",
    "start": "4056350",
    "end": "4063130"
  },
  {
    "text": "We saw this a little bit\nin the pre-transformer era. The standard mode was to\ntake GloVe or Word2Vec",
    "start": "4063130",
    "end": "4069490"
  },
  {
    "text": "representations and have them\nbe the inputs to something like an RNN. And then the RNN would learn.",
    "start": "4069490",
    "end": "4076180"
  },
  {
    "text": "And instead of having to\nlearn the embedding space from scratch, it would start in\nthis really interesting space.",
    "start": "4076180",
    "end": "4082180"
  },
  {
    "text": "And that is actually\na kind of learning of contextual\nrepresentations because what",
    "start": "4082180",
    "end": "4088390"
  },
  {
    "text": "happens if the glove\nrepresentations are updated is that they all shift around. And the network kind\nof pushes them around",
    "start": "4088390",
    "end": "4095079"
  },
  {
    "text": "so that you get different\nsenses for them in context. And then, again, the\ntransformer thing",
    "start": "4095080",
    "end": "4101439"
  },
  {
    "text": "just takes that to the limit. And that is the mode\nthat you'll operate in for the first homework.",
    "start": "4101439",
    "end": "4107060"
  },
  {
    "text": "I've put this from 2018 onwards. So we have this thing where\nI hope you can make it out at the bottom.",
    "start": "4107060",
    "end": "4112460"
  },
  {
    "text": "You load in BERT, and you just\nput a classifier on top of it. And you learn that classifier\nfor your sentiment task say,",
    "start": "4112460",
    "end": "4120949"
  },
  {
    "text": "and that actually updates\nthe BERT parameters. And the BERT parameters help\nyou do better at your task.",
    "start": "4120950",
    "end": "4127790"
  },
  {
    "text": "And in particular,\nthey might help you generalize to things\nthat are sparsely represented",
    "start": "4127790",
    "end": "4133339"
  },
  {
    "text": "in your task-specific\ntraining data because they've learned\nso much about the world",
    "start": "4133340",
    "end": "4139729"
  },
  {
    "text": "in their pre-training phase.  I put that for 2018 onwards.",
    "start": "4139729",
    "end": "4145886"
  },
  {
    "text": "I'm a little worried\nthat we're moving into a future in which\nfine tuning is just, again, using an OpenAI API.",
    "start": "4145887",
    "end": "4152630"
  },
  {
    "text": "But you all will\ndefinitely learn how to do much more than\nthis even if you fall back",
    "start": "4152630",
    "end": "4157670"
  },
  {
    "text": "on doing this at some point\nwhere now what you're doing is some lightweight\nversion of fine",
    "start": "4157670",
    "end": "4163460"
  },
  {
    "text": "tuning a massive model like\nGPT 3, same mental model there.",
    "start": "4163460",
    "end": "4168540"
  },
  {
    "text": "It's just that the starting\npoint knows so much about language in the world\ncompared even to the BERT model",
    "start": "4168540",
    "end": "4174434"
  },
  {
    "text": "up here. ",
    "start": "4174434",
    "end": "4180270"
  },
  {
    "text": "Those are the guiding ideas. Applause, questions, comments--\nwhat's on your minds?",
    "start": "4180270",
    "end": "4186720"
  },
  {
    "text": "Yeah. Going back to the sub-word\nsplitting of the longer words,",
    "start": "4186720",
    "end": "4192759"
  },
  {
    "text": "is there an inclusion we are\nimposing on splitting them in a particular way? Or is that also part of what\nis driven by the [INAUDIBLE]??",
    "start": "4192760",
    "end": "4201640"
  },
  {
    "text": "Oh, yeah. So what gets imposed as a\nmodeling bias in that sense when you do the tokenization,\nis that the question?",
    "start": "4201640",
    "end": "4208239"
  },
  {
    "text": "Yeah, potentially a lot. I left this out for\nreasons of time, but there are a bunch\nof different schemes",
    "start": "4208240",
    "end": "4214170"
  },
  {
    "text": "that you can run for doing\nthat sub-word tokenization. You'll see this as you\nread papers and as I talk--",
    "start": "4214170",
    "end": "4219480"
  },
  {
    "text": "wordpiece, there's\nbyte pair encoding, there's a unigram\nlanguage model.",
    "start": "4219480",
    "end": "4224550"
  },
  {
    "text": "All of them are attempts to\nlearn kind of an optimal way to tokenize the data\nbased on things that tend",
    "start": "4224550",
    "end": "4232590"
  },
  {
    "text": "to co-occur a lot together. But it's definitely\na meaningful step.",
    "start": "4232590",
    "end": "4237960"
  },
  {
    "text": "And so like, for\nexample, as someone who's interested in the\nmorphology of languages word",
    "start": "4237960",
    "end": "4243090"
  },
  {
    "text": "forms, and you all\nmight-- this could be a cool multilingual angle\nif you think about languages",
    "start": "4243090",
    "end": "4248100"
  },
  {
    "text": "with very rich\nmorphology, you might have an intuition that you\nwant a tokenization scheme that reproduces the morphology of\nthat language, that splits",
    "start": "4248100",
    "end": "4255630"
  },
  {
    "text": "a big word with\nall its suffixes, say, down into things that\nlook like the actual pieces",
    "start": "4255630",
    "end": "4261420"
  },
  {
    "text": "as you recognize them. And then you could think,\nwell, the best of these schemes should come close to that.",
    "start": "4261420",
    "end": "4267719"
  },
  {
    "text": "And that can be an important\nand useful bias that you impose. ",
    "start": "4267720",
    "end": "4273730"
  },
  {
    "text": "Yeah, go. Can you elaborate\non what happens",
    "start": "4273730",
    "end": "4279570"
  },
  {
    "text": "when we do fine tuning\nto the original model? Like, does it change or just it\nadds additional layers to it?",
    "start": "4279570",
    "end": "4288390"
  },
  {
    "text": "Or like, what actually\nhappens with fine tuning? What happens when you fine tune?",
    "start": "4288390",
    "end": "4294337"
  },
  {
    "text": "As usual with these\nquestions, there's like an easy answer\nand a hard answer. So the easy answer is that you\nare simply back propagating",
    "start": "4294337",
    "end": "4301500"
  },
  {
    "text": "whatever error signal you got\nfrom your output comparison with the true label\nback through all",
    "start": "4301500",
    "end": "4307679"
  },
  {
    "text": "the parameters of the model. And in principle, that could\nmean that as you fine tune on your sentiment task, you are\nupdating all of the parameters",
    "start": "4307680",
    "end": "4314940"
  },
  {
    "text": "even the pre-trained BERT ones. And then, of course,\nthere are variants of that where you update just\nsome of the BERT parameters",
    "start": "4314940",
    "end": "4321270"
  },
  {
    "text": "while leaving others\nfrozen and so forth. But the idea is you have a\nsmart initialization, that's",
    "start": "4321270",
    "end": "4327539"
  },
  {
    "text": "the BERT initialization. And then you kind of\nadjust the whole thing to be really good at your task.",
    "start": "4327540",
    "end": "4335240"
  },
  {
    "text": "What really happens there,\nthat's a deep question. That could actually connect\nwith the explainability stuff",
    "start": "4335240",
    "end": "4341660"
  },
  {
    "text": "like, what adjustments are\nhappening to the network? And which ones are useful? Which ones could\neven be detrimental?",
    "start": "4341660",
    "end": "4348530"
  },
  {
    "text": "Which ones are causing\nyou to overfit? Are there lightweight versions\nof the fine tuning that",
    "start": "4348530",
    "end": "4354380"
  },
  {
    "text": "would be better and more\nrobust and get a better balance from the pretraining\nand the task specific thing?",
    "start": "4354380",
    "end": "4360440"
  },
  {
    "text": "And that just shows you fine\ntuning is an art and a science all at once.",
    "start": "4360440",
    "end": "4366578"
  },
  {
    "text": "Yeah. Just a quick one. Sure. So, do we also control like the\ninfluence of the fine tuning",
    "start": "4366578",
    "end": "4377190"
  },
  {
    "text": "data set over the\noriginal model? Like, can we control it\nin a way that I'll just",
    "start": "4377190",
    "end": "4383623"
  },
  {
    "text": "change the model a little bit\nor change the original model completely or? Let's see, what's\nthe right metaphor?",
    "start": "4383623",
    "end": "4389580"
  },
  {
    "text": "You could control it the same\nway that you could control a kind of out of control car\nwhen you have a steering wheel",
    "start": "4389580",
    "end": "4397350"
  },
  {
    "text": "and you have an\naccelerator and a brake. But if they're all kind of\nyou're not sure how they work,",
    "start": "4397350",
    "end": "4404340"
  },
  {
    "text": "you can try. And as you get\nbetter at the task, as you get better at\ndriving the vehicle, you have more fine control.",
    "start": "4404340",
    "end": "4410280"
  },
  {
    "text": "But it's an art and a\nscience at the same time. I'm actually hoping\nthat Sid, he's",
    "start": "4410280",
    "end": "4415920"
  },
  {
    "text": "going to do a hands-on\nsession with us next week and that he imparts some\nof his own hard won lessons",
    "start": "4415920",
    "end": "4421350"
  },
  {
    "text": "to you about how to drive these\nthings because he's really been in the trenches doing\nthis with large models.",
    "start": "4421350",
    "end": "4427096"
  },
  {
    "text": "But you have your learning\nrate and your optimizer and other things you can fiddle\nwith and hope that it steers",
    "start": "4427096",
    "end": "4433552"
  },
  {
    "text": "in the direction you want.  If we use tree structures\nright now to represent syntax,",
    "start": "4433552",
    "end": "4442030"
  },
  {
    "text": "I guess, my question\nis, why don't they work super well for language models? And like, I guess the\nsentiment that you had was",
    "start": "4442030",
    "end": "4449350"
  },
  {
    "text": "like I'll just like put\nattention towards anything and see what works. I guess, why is it that the\nsentiment in linguistics then",
    "start": "4449350",
    "end": "4456310"
  },
  {
    "text": "as well? Great question. My personal perspective\nis that probably,",
    "start": "4456310",
    "end": "4461980"
  },
  {
    "text": "all the trees that we have\ncome up with are kind of wrong. And as a result,\nwe were actually",
    "start": "4461980",
    "end": "4468489"
  },
  {
    "text": "making it harder for our models\nbecause we were putting them in this bad initial state. And so the mode\nI've moved into is",
    "start": "4468490",
    "end": "4475300"
  },
  {
    "text": "to think, let's use the\ntransformer something that's much more like this\ntotally free form, and then use explainability\nmethods to see",
    "start": "4475300",
    "end": "4481989"
  },
  {
    "text": "if we can see what tree\nstructures they've induced because those might be\ncloser to the true tree",
    "start": "4481990",
    "end": "4487570"
  },
  {
    "text": "structures of language. And another aspect\nof this is that I feel like there's not, even\nfor a given sentence, one",
    "start": "4487570",
    "end": "4494290"
  },
  {
    "text": "structure. There could be one for\nsemantics, one for syntax, one for other things.",
    "start": "4494290",
    "end": "4499620"
  },
  {
    "text": "And so we want those all\nsimultaneously represented. And again, these powerful\nmodels we're talking about",
    "start": "4499620",
    "end": "4505490"
  },
  {
    "text": "could do that. And so then they become\ndevices for helping us learn what the right\nstructures are as opposed",
    "start": "4505490",
    "end": "4510860"
  },
  {
    "text": "to us imposing them. Yeah. Numbers are an important\npart of language--",
    "start": "4510860",
    "end": "4517360"
  },
  {
    "text": "how tokenization works for\nnumbers because it's digits, it's words, the same\nmeaning, different meaning.",
    "start": "4517360",
    "end": "4524590"
  },
  {
    "text": "Is it in the same domain or\nit's something otherwise?",
    "start": "4524590",
    "end": "4530110"
  },
  {
    "text": "Oh, I love this. This is a great\nexample of something that sounds small but could\nbe a wonderful final paper",
    "start": "4530110",
    "end": "4535510"
  },
  {
    "text": "and turns out to\nbe hard and deep. How do you represent\nnumbers if you've got a wordpiece tokenizer?",
    "start": "4535510",
    "end": "4541220"
  },
  {
    "text": "Do you get it down\nto all the digits? Do you leave them as big chunks? Do you do it based\non a strong bias",
    "start": "4541220",
    "end": "4547690"
  },
  {
    "text": "that you have about\nthis is the tens place, this is the hundreds place? What do you do? Yeah.",
    "start": "4547690",
    "end": "4553870"
  },
  {
    "text": "Wonderful question to ask. And I am absolutely positive\nthat low-level tokenization",
    "start": "4553870",
    "end": "4559210"
  },
  {
    "text": "choice will influence whether\nor not your model can learn to do basic arithmetic, say. Yeah.",
    "start": "4559210",
    "end": "4566080"
  },
  {
    "text": "And so a paper that\nevaluated a bunch of schemes in a way that was\ninsightful and important on real mathematical\nabilities could really",
    "start": "4566080",
    "end": "4573650"
  },
  {
    "text": "help us understand which models\nwill be intrinsically limited and in turn, how to\ndevelop better ones.",
    "start": "4573650",
    "end": "4579320"
  },
  {
    "text": "I love it.  Yeah. On the slide that's\ntitled attention,",
    "start": "4579320",
    "end": "4588260"
  },
  {
    "text": "you've shown your\nscores as dot products with the final word against\nall the other previous ones.",
    "start": "4588260",
    "end": "4595670"
  },
  {
    "text": "So why pick the final one\nand why not the first one or second? Or all of them.",
    "start": "4595670",
    "end": "4600770"
  },
  {
    "text": "Attention is all you need. This is a perfect transition. So you should do all of them.",
    "start": "4600770",
    "end": "4606860"
  },
  {
    "text": "That's like what they mean\nby the title of the paper. Yes, do it all. Self attention means attending\neverything to everything else.",
    "start": "4606860",
    "end": "4614550"
  },
  {
    "text": "So this is a perfect transition. We're out of time, 4:20. Next time, we will dive\ninto the transformer.",
    "start": "4614550",
    "end": "4621295"
  },
  {
    "text": "We'll resolve the questions\nthat we got back there. You'll see much more of\nthese attention connections. Yeah.",
    "start": "4621295",
    "end": "4626660"
  },
  {
    "text": "We're really queued up now to\ndive into the technical parts. ",
    "start": "4626660",
    "end": "4635000"
  }
]