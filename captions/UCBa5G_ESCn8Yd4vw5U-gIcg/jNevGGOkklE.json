[
  {
    "start": "0",
    "end": "30000"
  },
  {
    "text": "Welcome back, everyone.",
    "start": "0",
    "end": "5895"
  },
  {
    "text": "So this is CS229, lecture 14. The topic for today, we'll be starting a new chapter,",
    "start": "5895",
    "end": "11925"
  },
  {
    "text": "is, ah, reinforcement learning. And today we'll be covering Markov decision processes,",
    "start": "11925",
    "end": "17380"
  },
  {
    "text": "value iteration, and, and policy iteration. Ah, which form probably the core of reinforcement learning.",
    "start": "17380",
    "end": "23820"
  },
  {
    "text": "And, um, in, in Friday's lecture, we'll see more complex extensions of this.",
    "start": "23820",
    "end": "30020"
  },
  {
    "start": "30000",
    "end": "383000"
  },
  {
    "text": "And before we jump into today's topics, ah, a few announcements.",
    "start": "30020",
    "end": "35495"
  },
  {
    "text": "So, ah, for those of you who've been, um, sending in, ah, feedback through the Google Forms that we submitted, thank you so much.",
    "start": "35495",
    "end": "43890"
  },
  {
    "text": "There are, um, we got a lot of good suggestions, a lot of, ah, um, points where we can improve.",
    "start": "43890",
    "end": "50000"
  },
  {
    "text": "And I would like to, ah, briefly summarize, you know, some of the feedback that we've got, and the kind of steps that will be taking to, um, to improve.",
    "start": "50000",
    "end": "59515"
  },
  {
    "text": "So, ah, one of the, ah, concerns raised was, um, office hours, ah, that there're- ah there were",
    "start": "59515",
    "end": "67060"
  },
  {
    "text": "a few too many last minute changes for office hours. Um, and, um, we will try",
    "start": "67060",
    "end": "74200"
  },
  {
    "text": "to do our best not to change the office hour schedule moving forward. Unfortunately, sometimes there may be",
    "start": "74200",
    "end": "79450"
  },
  {
    "text": "personal emergencies or things beyond the TAs' control, ah, when it's too late to bring in a replacement TA.",
    "start": "79450",
    "end": "85810"
  },
  {
    "text": "But, um, those exceptions apart, ah, we will do our best to keep the office hour schedule fixed.",
    "start": "85810",
    "end": "92045"
  },
  {
    "text": "There were also a few very specific comments and suggestions about, ah, the way the office hours,",
    "start": "92045",
    "end": "97225"
  },
  {
    "text": "um, were, ah, were conducted. And, um, we've, we've, we've, ah,",
    "start": "97225",
    "end": "102315"
  },
  {
    "text": "noticed what you- what you've said, and we'll be, ah, definitely taking steps to improve. And, ah, a few more, ah,",
    "start": "102315",
    "end": "109149"
  },
  {
    "text": "comments from, from, ah, you guys. So a few comments on the lectures. So some of you have, have asked for,",
    "start": "109150",
    "end": "115290"
  },
  {
    "text": "um, doing the lectures with the slides format, where we're projecting slides and, um, and, and, and doing the lectures around the slides.",
    "start": "115290",
    "end": "123210"
  },
  {
    "text": "Ah, however, um, this is a pretty mathy class, and, um, at least it's my opinion that, um, you know,",
    "start": "123210",
    "end": "131610"
  },
  {
    "text": "slides might not be the best approach because there'll be a lot of back and forth between going on the slide and going on the blackboard,",
    "start": "131610",
    "end": "136900"
  },
  {
    "text": "and that can be pretty distracting for somebody who's watching the videos. And also, ah, we just tend, you know,",
    "start": "136900",
    "end": "142855"
  },
  {
    "text": "um, probably math is better explained on a whiteboard than on, on, on, on slides.",
    "start": "142855",
    "end": "147980"
  },
  {
    "text": "Ah, several of you have asked that I write bigger on the whiteboard. Ah, thank you for that feedback.",
    "start": "147980",
    "end": "153410"
  },
  {
    "text": "I will make a conscious decision to write bigger. If any of you, ah, have any trouble, ah,",
    "start": "153410",
    "end": "159230"
  },
  {
    "text": "understanding what's written on the whiteboard, please, ah, stop me right, right then, and, and, you know, tell me that you cannot see it.",
    "start": "159230",
    "end": "165625"
  },
  {
    "text": "I will, I will, you know, um, I have no problem erasing it, and writing all, all over again.",
    "start": "165625",
    "end": "170720"
  },
  {
    "text": "And also, um, some of you have mentioned that the lectures are going too slow.",
    "start": "170720",
    "end": "176330"
  },
  {
    "text": "Some of you have mentioned it's going too fast. Um, so the, the, um,",
    "start": "176330",
    "end": "181885"
  },
  {
    "text": "so I guess one, one way to kind of accommodate both needs of, of going fast and going too slow, ah,",
    "start": "181885",
    "end": "188090"
  },
  {
    "text": "is probably that, um, I, I think a good, good compromise there would be to probably postpone some of the questions that,",
    "start": "188090",
    "end": "196775"
  },
  {
    "text": "that come from the audience to a few- in a batched way. So hold off till I,",
    "start": "196775",
    "end": "201875"
  },
  {
    "text": "you know, complete some section, and then ask your question in case your question gets answered, ah, right then. And if, if the questions are not directly related to the lecture itself,",
    "start": "201875",
    "end": "211295"
  },
  {
    "text": "then I might ask you to just post the questions on Piazza. And when I asked you to post it on Pia- on, on Piazza,",
    "start": "211295",
    "end": "217655"
  },
  {
    "text": "it would be, um, it's a sincere request that you actually do post it on Piazza because, you know, I wanna answer them, ah, and just not hold, you know,",
    "start": "217655",
    "end": "225735"
  },
  {
    "text": "the 100 or more of you, you know, waiting while I answer the questions that may be of interest to a few of you.",
    "start": "225735",
    "end": "231875"
  },
  {
    "text": "Um, and then, ah, some of you have also, um, ah, given some feedback,",
    "start": "231875",
    "end": "237345"
  },
  {
    "text": "ah, about the difficulty of homeworks. Ah, so some find the math in it too hard,",
    "start": "237345",
    "end": "242685"
  },
  {
    "text": "some of you find the programming in it too hard, and, um, I don't know if there is a clear action that we can take over there, ah, because, ah,",
    "start": "242685",
    "end": "252485"
  },
  {
    "text": "machine learning is, is indeed, um, you know, a, a, a field that, that spans computer science and statistics.",
    "start": "252485",
    "end": "259260"
  },
  {
    "text": "So it will be hard on math, it will be hard on programming, and it- that's just the nature of, of, you know, ah, machine learning.",
    "start": "259260",
    "end": "266045"
  },
  {
    "text": "And, um, the programming assignments, ah, or, or, the, the, ah, homework assignments might feel a little too long.",
    "start": "266045",
    "end": "274009"
  },
  {
    "text": "Um, however, ah, you know, ah, er, in this course we are just following the,",
    "start": "274010",
    "end": "280025"
  },
  {
    "text": "the, um, following the pattern that has been followed for this course in the previous quarters, right?",
    "start": "280025",
    "end": "286245"
  },
  {
    "text": "And, and the programming assignments- and the homework assignments, you know, in each homework is not more or less than what's been done before.",
    "start": "286245",
    "end": "293900"
  },
  {
    "text": "And, and that's just the way the course is structured. It is, it is a work- workload heavy course. Ah, in terms of,",
    "start": "293900",
    "end": "301415"
  },
  {
    "text": "um, in terms of, ah, help required for programming, ah, in general, in office hours,",
    "start": "301415",
    "end": "307310"
  },
  {
    "text": "we tend not to focus on helping students debug their code. Ah, we expect students to be comfortable with",
    "start": "307310",
    "end": "313700"
  },
  {
    "text": "programming and be able to just implement things on your own. However, ah, going forward, if, if, ah,",
    "start": "313700",
    "end": "319640"
  },
  {
    "text": "there is no backlog in the queue, you know, the TAs will- might probably help you a little bit if there is nobody else waiting.",
    "start": "319640",
    "end": "325340"
  },
  {
    "text": "Ah, but in general, the expectation for the course is that you come with, you know, ah, um, basic computer programming skills,",
    "start": "325340",
    "end": "332010"
  },
  {
    "text": "and you have no, no problems writing simple code given an algorithm. Ah, but at the same time,",
    "start": "332010",
    "end": "337220"
  },
  {
    "text": "we don't wanna, um, you know, um, kind of leave you stuck if you're having some trouble.",
    "start": "337220",
    "end": "342430"
  },
  {
    "text": "So, you know, try attending office- if you're- especially if you need help in programming, try attending office hours early in the homework phase",
    "start": "342430",
    "end": "350030"
  },
  {
    "text": "so that office hours are free and maybe TAs can help you out there. If you, if you, ah, attend office hours when the deadline to a homework is, is, you know,",
    "start": "350030",
    "end": "357550"
  },
  {
    "text": "close, then the TAs will have a hard time helping you with, with, ah, programming. Um, and for those of you who's, you know,",
    "start": "357550",
    "end": "367005"
  },
  {
    "text": "the, the feedback form is still open, ah, I st- I'm still gonna keep checking for it every once in a while.",
    "start": "367005",
    "end": "372790"
  },
  {
    "text": "If you have any feedbacks, you know, please feel free to give us candid feedbacks on how we can improve, and we'll do our best to, you know,",
    "start": "372790",
    "end": "378865"
  },
  {
    "text": "make changes and accommodate as many needs as possible. [NOISE] Okay. With that, ah,",
    "start": "378865",
    "end": "384345"
  },
  {
    "start": "383000",
    "end": "750000"
  },
  {
    "text": "let's start with a quick recap of, um, of, of, ah, the last lecture,",
    "start": "384345",
    "end": "391365"
  },
  {
    "text": "and also do a quick summary of where we are standing in terms of the overall course.",
    "start": "391365",
    "end": "396485"
  },
  {
    "text": "So in the last lecture or the last two lectures, we've been talking quite a bit about bias-variance tradeoff,",
    "start": "396485",
    "end": "402620"
  },
  {
    "text": "and this- understanding this bias-variance tradeoff in a deep way should probably be the highest priority",
    "start": "402620",
    "end": "409850"
  },
  {
    "text": "for you of what to take away from this course, all right? Bias-variance tradeoff is something that is so fundamental,",
    "start": "409850",
    "end": "415699"
  },
  {
    "text": "and understanding it in an intuitive and deep way will be- will probably serve you the longest.",
    "start": "415700",
    "end": "421095"
  },
  {
    "text": "It, it has probably the largest payoff into your future machine learning career, all right? So, um, to summarize, ah,",
    "start": "421095",
    "end": "427850"
  },
  {
    "text": "bias-variance tradeoff, in machine learning, we are interested in generalization error, all right?",
    "start": "427850",
    "end": "433400"
  },
  {
    "text": "How well our model performs on unseen data, not on the training data that we have. And the test error or the generalization error can",
    "start": "433400",
    "end": "441410"
  },
  {
    "text": "be decomposed into three parts, three additive parts. So one is irreducible error,",
    "start": "441410",
    "end": "447935"
  },
  {
    "text": "and this is just the noise in the test example, all right? Our training data is noisy and our test data is noisy.",
    "start": "447935",
    "end": "454070"
  },
  {
    "text": "And the noise in the, in the, in the test data is what contributes to irreducible error, and there's nothing we can do about it.",
    "start": "454070",
    "end": "459905"
  },
  {
    "text": "It's just a fact of life and we accept it. And then there are these two other components called bias and variance.",
    "start": "459905",
    "end": "466070"
  },
  {
    "text": "Bias, you can think of it as a systematic error of your model. Which means, if you average it across all the possible training sets that you can get,",
    "start": "466070",
    "end": "474430"
  },
  {
    "text": "the systematic error that your model makes, that's either you're underpredicting or overpredicting for certain examples,",
    "start": "474430",
    "end": "480485"
  },
  {
    "text": "or that your parameters are closer to 0 than what it should be, that systematic error is called bias.",
    "start": "480485",
    "end": "487760"
  },
  {
    "text": "And variance is how sensitive our model is to noise in the training data, all right?",
    "start": "487760",
    "end": "493850"
  },
  {
    "text": "And this is where the distinction between noise in the test data and the training data comes into picture. Noise in the test data contributes to irreducible error, and that's, you know,",
    "start": "493850",
    "end": "502080"
  },
  {
    "text": "one component of the generalization error, and noise in the training data contributes to the variance of, of your model [NOISE] , right?",
    "start": "502080",
    "end": "508789"
  },
  {
    "text": "And [NOISE] the steps that we can take to reduce,",
    "start": "508790",
    "end": "513919"
  },
  {
    "text": "um, the test error, the steps that we can take, there are way too many steps that you can take. For example, you can try a bigger model,",
    "start": "513920",
    "end": "520760"
  },
  {
    "text": "you can try- you can get more training data. There are many steps that you can take. However, not all the steps work all the time.",
    "start": "520760",
    "end": "527735"
  },
  {
    "text": "And in order to determine what steps, what, what is the step that's most effective for the problem, ah,",
    "start": "527735",
    "end": "533480"
  },
  {
    "text": "that is immediately in front of you, in order to make that decision, it's very crucial that you understand this bias-variance trade-off.",
    "start": "533480",
    "end": "540635"
  },
  {
    "text": "All right? Reducing training error is, is simple. You can always, you know, um, um, get a bigger model,",
    "start": "540635",
    "end": "547280"
  },
  {
    "text": "get a more complex model, and that'll reduce your training error, but our goal is to improve on test error.",
    "start": "547280",
    "end": "552975"
  },
  {
    "text": "Okay. And- and- and- and the steps that we take, as I said already,",
    "start": "552975",
    "end": "558279"
  },
  {
    "text": "is gonna- is gonna depend on whether the current problem that you're facing is mostly a bias problem,",
    "start": "558280",
    "end": "563620"
  },
  {
    "text": "or is it mostly a variance problem, right? And the bias variance trade off basically tells you that [NOISE] by taking some step,",
    "start": "563620",
    "end": "571764"
  },
  {
    "text": "you may be reducing, you know, one of them, either bias or variance, but that may also increase the other, right?",
    "start": "571765",
    "end": "578769"
  },
  {
    "text": "And which is why it's- it's very important that we- that we are able to characterize which of",
    "start": "578770",
    "end": "583960"
  },
  {
    "text": "the two problems is the bigger problem at hand and take a step to purposefully go after that particular problem,",
    "start": "583960",
    "end": "590050"
  },
  {
    "text": "either bias or variance, right? For example, we saw that if we- if we increa- increase the model capacity,",
    "start": "590050",
    "end": "597970"
  },
  {
    "text": "if you- if you move to a more complex model, that will reduce the bias, um, of- of- of your model,",
    "start": "597970",
    "end": "604360"
  },
  {
    "text": "but at the same time, it may increase the variance of your model, right? So if- if the problem that you're [NOISE] facing right now is high variance,",
    "start": "604360",
    "end": "610089"
  },
  {
    "text": "then you should almost certainly not use a bigger model because that is going to worsen the problem that you have, right?",
    "start": "610090",
    "end": "615745"
  },
  {
    "text": "And similarly, with regularization, if we increase regularization, that is going to reduce the variance in the model,",
    "start": "615745",
    "end": "621640"
  },
  {
    "text": "but it's gonna increase the, um, um, it's also gonna increase the bias in the, in, uh, in the model, [NOISE] which means if your problem right now is high bias,",
    "start": "621640",
    "end": "629545"
  },
  {
    "text": "then increasing regularization is gonna make it worse, whereas increasing regula- regularization would have",
    "start": "629545",
    "end": "634899"
  },
  {
    "text": "helped if the problem was high variance, right? Which is why it's super- super important for you to characterize and",
    "start": "634900",
    "end": "641095"
  },
  {
    "text": "get a heuristic estimate of what the contribution of bias is towards error,",
    "start": "641095",
    "end": "646435"
  },
  {
    "text": "and what the contribution of variance is towards the current, uh, test error. And unfortunately, there is no, um,",
    "start": "646435",
    "end": "654535"
  },
  {
    "text": "principled approach to decompose and get these estimates in practice, which is why we use heuristics, right?",
    "start": "654535",
    "end": "660985"
  },
  {
    "text": "The heuristics are, you can think of the training error itself as the bias,",
    "start": "660985",
    "end": "666070"
  },
  {
    "text": "and you can think of the gap between cross-validation error and training error as variance. And this is where cross-validation comes into picture.",
    "start": "666070",
    "end": "673420"
  },
  {
    "text": "Cross-validation is necessary, or the- the- the most important use of cross-validation",
    "start": "673420",
    "end": "680065"
  },
  {
    "text": "is that we can get this- this decomposition of variance- variance and bias from the current model, right?",
    "start": "680065",
    "end": "686875"
  },
  {
    "text": "And- and once we get this- get this, uh, breakdown, we can then make, uh,",
    "start": "686875",
    "end": "693459"
  },
  {
    "text": "a judgment about whether our model is currently facing a larger bias problem or a larger variance problem and take",
    "start": "693460",
    "end": "699370"
  },
  {
    "text": "some remedial step according to which of the two is larger, right? So the- the summary is that whenever you are",
    "start": "699370",
    "end": "705705"
  },
  {
    "text": "working on model development and you wanna improve your model performance, you should always, always, always look at the training error and cross validation error simultaneously, right?",
    "start": "705705",
    "end": "715555"
  },
  {
    "text": "We care about improving the cross-validation error or the- or- or the- or the test error,",
    "start": "715555",
    "end": "720565"
  },
  {
    "text": "but just looking at that alone will give you nothing actionable. In order to have something actionable,",
    "start": "720565",
    "end": "726370"
  },
  {
    "text": "you need to see both the training error and the cross validation error simultaneously, make a judgment about whether",
    "start": "726370",
    "end": "733420"
  },
  {
    "text": "the situation is a high bias situation or a high variance situation, and then accordingly, take some action to remedy the high bias or high variance, right?",
    "start": "733420",
    "end": "740964"
  },
  {
    "text": "And this- this is probably the- the- the most important thing that's going to serve you well for the rest of your machine-learning career, right?",
    "start": "740965",
    "end": "748795"
  },
  {
    "text": "So that's- that's the, uh, bias-variance trade-off. And to kind of take a larger look of where we",
    "start": "748795",
    "end": "754090"
  },
  {
    "start": "750000",
    "end": "870000"
  },
  {
    "text": "are standing in terms of the overall course, so here's- here's a, um, um,",
    "start": "754090",
    "end": "759324"
  },
  {
    "text": "a rough overview or- or a quick overview of what we've covered so far. So approximately, week 1 through 4,",
    "start": "759325",
    "end": "765910"
  },
  {
    "text": "we cover supervised learning. And what is supervised learning? Supervised learning is learning some function h,",
    "start": "765910",
    "end": "772735"
  },
  {
    "text": "which we call the hypothesis, that maps x to y, right? The- there is- there is this clear concept of input and output in supervised learning,",
    "start": "772735",
    "end": "781555"
  },
  {
    "text": "which will not be the case in unsupervised learning. In supervised learning, there's a very clear sense of what is input and what's output,",
    "start": "781555",
    "end": "787645"
  },
  {
    "text": "and we have noisy examples of input-output pairs, right? And most of the time, the noise is embedded in the label, what we call, you know,",
    "start": "787645",
    "end": "795880"
  },
  {
    "text": "in- in the output, and we get a training set of size n of,",
    "start": "795880",
    "end": "801145"
  },
  {
    "text": "you know, these- these x-y pairs. And from this training set, our goal is to learn this hypothesis that- that's be- a function that maps x to y,",
    "start": "801145",
    "end": "810610"
  },
  {
    "text": "such that it generalizes well to unseen data, right? There is some notion of a loss function,",
    "start": "810610",
    "end": "815980"
  },
  {
    "text": "and we want to minimize this loss function on unseen data, right? If- if the problem was just to minimize it on the given data that we have,",
    "start": "815980",
    "end": "823780"
  },
  {
    "text": "then we would just call it an optimization problem, right? But this is machine learning because we want to do well on unseen data, right?",
    "start": "823780",
    "end": "830620"
  },
  {
    "text": "And that's- that's where, you know, the concept of bias-variance comes into the picture. And we thought- we've been seeing,",
    "start": "830620",
    "end": "838615"
  },
  {
    "text": "you know, approximately, you know, two kinds of algorithms, classification versus regression.",
    "start": "838615",
    "end": "843714"
  },
  {
    "text": "And this- this categorization is based on the data type of the y-variable, right?",
    "start": "843715",
    "end": "849430"
  },
  {
    "text": "If the data type of the y variable is binary, you know, 0 or 1, then we call it classification.",
    "start": "849430",
    "end": "855250"
  },
  {
    "text": "And this could not be just binary, but it could be, you know, or- or multiple class, like, you know, um, for example, in the homework,",
    "start": "855250",
    "end": "861720"
  },
  {
    "text": "you're- you're- you're building a classifier for handwritten digits for MNIST, where y can take anything between, you know, 0 to 9.",
    "start": "861720",
    "end": "869959"
  },
  {
    "text": "And that's called classification, where y is discrete. And in cases where y is continuous,",
    "start": "869960",
    "end": "875620"
  },
  {
    "start": "870000",
    "end": "1350000"
  },
  {
    "text": "we call it regression, right? And we've seen, you know, a simple- two very simple,",
    "start": "875620",
    "end": "880704"
  },
  {
    "text": "uh, models that we started off with, that was linear regression and logistic regression for,",
    "start": "880705",
    "end": "887515"
  },
  {
    "text": "uh, for- for regression problems and classification problems. And then we, uh, after- after seeing those two,",
    "start": "887515",
    "end": "893770"
  },
  {
    "text": "we kind of generalized the- the datatype of y to not just, you know, be this strict regression versus classification,",
    "start": "893770",
    "end": "902110"
  },
  {
    "text": "but generalize them into something called GLMs, generalized linear models, where y given x is some distribution in the exponential family, right?",
    "start": "902110",
    "end": "910045"
  },
  {
    "text": "It could be discrete, it could be continuous, it could be Poisson, which is just integers, it could be anything. It could be, you know, positive only.",
    "start": "910045",
    "end": "916584"
  },
  {
    "text": "And as long as that y given x distribution is- belongs to the exponential family,",
    "start": "916585",
    "end": "921940"
  },
  {
    "text": "then we can solve it with- with, you know, common update rules, and, you know, there- there were other niceties that we saw.",
    "start": "921940",
    "end": "928000"
  },
  {
    "text": "So generalized linear models is not just for classification, or for regression, or for Poisson,",
    "start": "928000",
    "end": "934795"
  },
  {
    "text": "it generalizes the data type of y to any- any kind of,",
    "start": "934795",
    "end": "940165"
  },
  {
    "text": "uh, datatype essentially, right? And then we also saw two different, uh, kinds of approaches for building models, discriminative versus generative.",
    "start": "940165",
    "end": "949959"
  },
  {
    "text": "In discriminative models, we are directly learning y given x. So a generalized linear model is an example of a discriminative model,",
    "start": "949960",
    "end": "957430"
  },
  {
    "text": "where we are only interested in learning the probability distribution of y when the corresponding x is given to you.",
    "start": "957430",
    "end": "964404"
  },
  {
    "text": "And the- the, uh, other, um, approach for building models is called generative models,",
    "start": "964405",
    "end": "970270"
  },
  {
    "text": "where we want to learn how to generate new examples, how to generate a full pair, x-y pair, right?",
    "start": "970270",
    "end": "977320"
  },
  {
    "text": "And this can be generally broken down using the chain rule as p of y times p of x given y.",
    "start": "977320",
    "end": "984145"
  },
  {
    "text": "And we saw a few examples of- of generative models, so we saw a Naive Bayes where x was discrete,",
    "start": "984145",
    "end": "991315"
  },
  {
    "text": "and we saw GDA, Gaussian discriminant analysis, where x was real valued, right?",
    "start": "991315",
    "end": "996775"
  },
  {
    "text": "And we also saw that, uh, with- with- with GDA,",
    "start": "996775",
    "end": "1001920"
  },
  {
    "text": "there is a one-to-one correspondence between GDA and logistic regression.",
    "start": "1001920",
    "end": "1007635"
  },
  {
    "text": "In fact, for any x given y, where x belongs to exponential family,",
    "start": "1007635",
    "end": "1013125"
  },
  {
    "text": "p of y given x will always take a logistic regression form, right? And we saw that in the homework as well.",
    "start": "1013125",
    "end": "1019320"
  },
  {
    "text": "And then we move on to non-linear models, right? The- wa- we- we actually did",
    "start": "1019320",
    "end": "1024780"
  },
  {
    "text": "explore non-linearity using feature maps in your homework already, in the first homework, where you tried different feature maps,",
    "start": "1024780",
    "end": "1030735"
  },
  {
    "text": "we get nonlinear hypotheses, right? And the- the- the- a more formal approach to feature maps that we saw was kernels, right?",
    "start": "1030735",
    "end": "1040560"
  },
  {
    "text": "Kernels are- are these symmetric positive definite functions which have an implicit feature map embedded in them.",
    "start": "1040560",
    "end": "1047819"
  },
  {
    "text": "And using kernels, we can- we can build, um, you know, kernel-based methods, you know,",
    "start": "1047820",
    "end": "1053085"
  },
  {
    "text": "we call them kernel methods, and we saw two such examples, support vector machines for classification problems and Gaussian processes for regression problems.",
    "start": "1053085",
    "end": "1061110"
  },
  {
    "text": "Both of these methods use kernels, and SVM, in particular, was- was,",
    "start": "1061110",
    "end": "1068235"
  },
  {
    "text": "you know, particularly well-suited for a kernel method. And that was because with- with, um,",
    "start": "1068235",
    "end": "1075659"
  },
  {
    "text": "with the non-kernel-based approaches, um, the complexity of the model depends on the number of parameters that we have,",
    "start": "1075660",
    "end": "1082605"
  },
  {
    "text": "that depends on the number of features that we have. But as with kernel methods, the- the complexity depends on the number of examples that we have, right?",
    "start": "1082605",
    "end": "1089914"
  },
  {
    "text": "Those- those are the two- two kind of sides of your design matrix. With- with- with nonlinear models,",
    "start": "1089915",
    "end": "1095390"
  },
  {
    "text": "we learn one parameter per feature, that is per column, and with kernel methods, we learn one coefficient per example, you know, per row.",
    "start": "1095390",
    "end": "1103299"
  },
  {
    "text": "And so, um, kernel methods, in general, tend not to work so well when the number of examples is a lot,",
    "start": "1103300",
    "end": "1110190"
  },
  {
    "text": "when you have a very big data set, kernel methods tend to, um, um, tend to be more expensive.",
    "start": "1110190",
    "end": "1116140"
  },
  {
    "text": "And we saw that in SVMs in particular, the set of coefficients that we learn for, um,",
    "start": "1116140",
    "end": "1122164"
  },
  {
    "text": "the kernel approach happens to be sparse, which means you get the best of both worlds. You get good scalability in the number of features",
    "start": "1122165",
    "end": "1129060"
  },
  {
    "text": "because kernels can potentially give you infinite number of features, and you also get good scalability in terms of number of examples because the coefficients",
    "start": "1129060",
    "end": "1135660"
  },
  {
    "text": "are mostly 0 even with large datasets, therefore SVMs tend to work well.",
    "start": "1135660",
    "end": "1142120"
  },
  {
    "text": "And then we moved on to another kind of non-linearity. So kernel methods was one and the other approach that we saw was learnable features.",
    "start": "1142340",
    "end": "1151950"
  },
  {
    "text": "Where in the first approach, the feature map of the kernel is hand-designed.",
    "start": "1151950",
    "end": "1157110"
  },
  {
    "text": "You decide what the kernel you want to use is. The other approach is to again use data to learn what are good feature maps, okay?",
    "start": "1157110",
    "end": "1165300"
  },
  {
    "text": "That's when neural networks came into the picture, right? With neural networks, you can think of a neural network as a generalized linear model at",
    "start": "1165300",
    "end": "1172290"
  },
  {
    "text": "the last layer and everything before that as some kind of a learnable feature map, right?",
    "start": "1172290",
    "end": "1177490"
  },
  {
    "text": "And after that, we moved on to learning theory. And in learning theory, the main topics that we touched upon was regularization and",
    "start": "1177830",
    "end": "1187650"
  },
  {
    "text": "its Bayesian interpretation of how regularization corresponds to performing MAP estimation in the Bayesian setting.",
    "start": "1187650",
    "end": "1198220"
  },
  {
    "text": "We studied bias and variance and the bias-variance trade-off, which we covered already,",
    "start": "1199250",
    "end": "1205440"
  },
  {
    "text": "and we also studied uniform convergence, right? So uniform convergence is a more theoretical part of the course.",
    "start": "1205440",
    "end": "1214590"
  },
  {
    "text": "And in general, understanding the contents of the lecture notes on",
    "start": "1214590",
    "end": "1220080"
  },
  {
    "text": "uniform convergence and what we covered in the lecture can be very useful if you're interested in getting into machine learning research.",
    "start": "1220080",
    "end": "1226875"
  },
  {
    "text": "Because that's the common framework in which most of the machine learning theory problems are",
    "start": "1226875",
    "end": "1232110"
  },
  {
    "text": "posted and that's until the last lecture.",
    "start": "1232110",
    "end": "1237570"
  },
  {
    "text": "So we are here right now, right? And today, and in the next lecture,",
    "start": "1237570",
    "end": "1242820"
  },
  {
    "text": "we'll be covering reinforcement learning. It's going to be a very quick overview of reinforcement learning, we'll not be going very deep.",
    "start": "1242820",
    "end": "1248850"
  },
  {
    "text": "There are multiple courses offered, only on reinforcement learning,",
    "start": "1248850",
    "end": "1254235"
  },
  {
    "text": "it's a super vast field, but we'll only be covering just the basics and giving you kind of a big picture or",
    "start": "1254235",
    "end": "1264169"
  },
  {
    "text": "a mind map of how the different reinforcement learning algorithms that we are not going to cover, how they all fit together in one piece.",
    "start": "1264170",
    "end": "1271774"
  },
  {
    "text": "So that's the plan for reinforcement learning. And starting next week we'll be starting unsupervised learning.",
    "start": "1271775",
    "end": "1277975"
  },
  {
    "text": "For me personally, I like unsupervised learning the most, it's very exciting.",
    "start": "1277975",
    "end": "1285015"
  },
  {
    "text": "And once we kind of finish up with unsupervised learning, we're going to cover some general topics,",
    "start": "1285015",
    "end": "1291390"
  },
  {
    "text": "things like evaluation metrics, maybe a few, a few theoretical ideas like KL divergence,",
    "start": "1291390",
    "end": "1297300"
  },
  {
    "text": "or a few such things. Some general topics that don't kind of fit into this supervised versus unsupervised classification.",
    "start": "1297300",
    "end": "1304455"
  },
  {
    "text": "And finally, in the last week, on week 8, we'll have only two lectures on the Monday and the Wednesday,",
    "start": "1304455",
    "end": "1311370"
  },
  {
    "text": "and those two lectures we'll be doing a full review of the entire course again, with the hope that it prepares you well for the final exam, right?",
    "start": "1311370",
    "end": "1318460"
  },
  {
    "text": "So any questions before we move into today's topic of reinforcement learning? Yes, question.",
    "start": "1319190",
    "end": "1325980"
  },
  {
    "text": "Could you please explain the motivation of why the training error is approximately used of bias and the same as [inaudible] in cross-validation error?",
    "start": "1325980",
    "end": "1336120"
  },
  {
    "text": "So the question is, why are these two, why is training error an approximation of bias and cross-validation?",
    "start": "1336120",
    "end": "1342930"
  },
  {
    "text": "I would say post it on Piazza and I'll be happy to kind of give you more details on that.",
    "start": "1342930",
    "end": "1349050"
  },
  {
    "text": "All right, so let's start with reinforcement learning.",
    "start": "1349050",
    "end": "1356250"
  },
  {
    "start": "1350000",
    "end": "1710000"
  },
  {
    "text": "[NOISE] So in the examples that we've,",
    "start": "1356250",
    "end": "1373680"
  },
  {
    "text": "in the topics that we've covered so far, which has been mostly supervised learning, we were always told for",
    "start": "1373680",
    "end": "1380310"
  },
  {
    "text": "a given input or for a given situation what the correct answer is, right? So for every x, we had a corresponding y,",
    "start": "1380310",
    "end": "1387255"
  },
  {
    "text": "which was the correct answer or the answer that our model should learn to predict, and that was given directly to us, right?",
    "start": "1387255",
    "end": "1394125"
  },
  {
    "text": "And which is why we call it supervised learning, right? Whereas in reinforcement learning,",
    "start": "1394125",
    "end": "1400115"
  },
  {
    "text": "this kind of supervision is a little weak. Instead of telling us what to do in a given situation,",
    "start": "1400115",
    "end": "1407675"
  },
  {
    "text": "we are instead given some kind of a reward, right? So in RL, one way to think of it is replace y,",
    "start": "1407675",
    "end": "1419520"
  },
  {
    "text": "which is supervision with reward, right,",
    "start": "1419520",
    "end": "1422560"
  },
  {
    "text": "and reward is always real-valued. It could be positive, it could be negative.",
    "start": "1425600",
    "end": "1431100"
  },
  {
    "text": "And the way you want to think of reward is kind of how good a job you did, right?",
    "start": "1431100",
    "end": "1436934"
  },
  {
    "text": "There is no right reward, so to speak. There is no optimal reward as such,",
    "start": "1436935",
    "end": "1448320"
  },
  {
    "text": "and the goal in reinforcement learning is to kind of maximize our long-term reward,",
    "start": "1448320",
    "end": "1455445"
  },
  {
    "text": "which means we are dealing with situations which are changing over time, right? In supervised learning, we were dealing with fixed situations and",
    "start": "1455445",
    "end": "1463035"
  },
  {
    "text": "each example was what we call as IID, where each example was independent of the other.",
    "start": "1463035",
    "end": "1469035"
  },
  {
    "text": "Whereas in reinforcement learning, we are trying to program an agent that kind of lives in an environment or in the real world,",
    "start": "1469035",
    "end": "1478500"
  },
  {
    "text": "or in a simulator that performs well over time. And examples of such agents could be,",
    "start": "1478500",
    "end": "1486105"
  },
  {
    "text": "for example, it could be a robot which is learning how to walk. It could be a game-playing agent that plays chess or Go.",
    "start": "1486105",
    "end": "1494565"
  },
  {
    "text": "It could be even some kind of an automated trading agent that you use in a financial market, right?",
    "start": "1494565",
    "end": "1504165"
  },
  {
    "text": "Things where you're making multiple decisions over time and at each time, after you take every action,",
    "start": "1504165",
    "end": "1512190"
  },
  {
    "text": "you get some kind of a reward, right? And the goal with reinforcement learning is to maximize this accumulated reward over time, right?",
    "start": "1512190",
    "end": "1522554"
  },
  {
    "text": "Which means we need to take this long-term view of trying to optimize how much reward we're going to accumulate,",
    "start": "1522555",
    "end": "1531840"
  },
  {
    "text": "starting from now all the way into the future, right? Which is very different from the supervised learning setting, where in the supervised learning setting,",
    "start": "1531840",
    "end": "1538950"
  },
  {
    "text": "we were given an example and our only concern was to do well on that example alone, and the next example is completely independent, right?",
    "start": "1538950",
    "end": "1546880"
  },
  {
    "text": "And so it is this concept of time that makes reinforcement learning different from general supervised learning, right?",
    "start": "1547010",
    "end": "1555630"
  },
  {
    "text": "In reinforcement learning, you're making multiple decisions, one decision at a time,",
    "start": "1555630",
    "end": "1562050"
  },
  {
    "text": "and with each decision, we're earning some reward and our goal is to maximize this accumulated reward into the future.",
    "start": "1562050",
    "end": "1571665"
  },
  {
    "text": "And the way we kind of formalize reinforcement learning is through this formalism called MDP or Markov Decision Process.",
    "start": "1571665",
    "end": "1580929"
  },
  {
    "text": "Markov Decision Process,",
    "start": "1585230",
    "end": "1588760"
  },
  {
    "text": "MDP. So an MDP",
    "start": "1593720",
    "end": "1607100"
  },
  {
    "text": "is, 1, 2, 3, it's a 5-tuple. It's a tuple containing five things.",
    "start": "1607100",
    "end": "1613050"
  },
  {
    "text": "S, A,",
    "start": "1613050",
    "end": "1621495"
  },
  {
    "text": "P_sa, Gamma, and R where S is a set of states.",
    "start": "1621495",
    "end": "1633509"
  },
  {
    "text": "[NOISE] It is the set of all possible states that our agent can currently be in.",
    "start": "1633510",
    "end": "1643170"
  },
  {
    "text": "[NOISE] A is set of actions, right?",
    "start": "1643170",
    "end": "1652360"
  },
  {
    "text": "P_sa, are transition probabilities, [NOISE]",
    "start": "1657470",
    "end": "1671820"
  },
  {
    "text": "Gamma. This is 0, 1",
    "start": "1671820",
    "end": "1677880"
  },
  {
    "text": "discount factor and R,",
    "start": "1677880",
    "end": "1687780"
  },
  {
    "text": "which is S cross A",
    "start": "1687780",
    "end": "1701550"
  },
  {
    "text": "is called the reward function right?",
    "start": "1701550",
    "end": "1710535"
  },
  {
    "start": "1710000",
    "end": "2130000"
  },
  {
    "text": "Now, let's- let's look at an example of what all of these actually mean. Supposing, uh, this example comes from",
    "start": "1710535",
    "end": "1716760"
  },
  {
    "text": "a very famous text book on- on artificial intelligence. And it's also a very standard example used in many courses.",
    "start": "1716760",
    "end": "1723659"
  },
  {
    "text": "So you might have seen this before. So supposing there is this- supposing this is- that is agent that lives in some kind of a map,",
    "start": "1723660",
    "end": "1736575"
  },
  {
    "text": "some kind of, um, um, um, region where your agent at any given time can be in one of these grids.",
    "start": "1736575",
    "end": "1748120"
  },
  {
    "text": "And our goal is to reach here. And, uh- and you get a reward of plus 1 if you reach here and you get a reward of",
    "start": "1748280",
    "end": "1756990"
  },
  {
    "text": "minus 1 if you reach here and our goal is to- is to,",
    "start": "1756990",
    "end": "1762090"
  },
  {
    "text": "uh- is to be able to solve this. And by- what I mean by solving is,",
    "start": "1762090",
    "end": "1768135"
  },
  {
    "text": "so first of all, what is S? S is the set of all possible states where our agent can be. So states in this case,",
    "start": "1768135",
    "end": "1774930"
  },
  {
    "text": "S in this case is the set,",
    "start": "1774930",
    "end": "1780945"
  },
  {
    "text": "you know, 1, 1 1, 2 and so on.",
    "start": "1780945",
    "end": "1786870"
  },
  {
    "text": "And the size of- the size of, uh, our- our, uh, state-space in this case is 11,",
    "start": "1786870",
    "end": "1794325"
  },
  {
    "text": "it can be in 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, and, you know, this is blocked.",
    "start": "1794325",
    "end": "1799950"
  },
  {
    "text": "You cannot be in this state. The set of actions that we can take in this case are",
    "start": "1799950",
    "end": "1806145"
  },
  {
    "text": "to  move our agent one step at a time in any of the directions. So the set of actions we take can be move north,",
    "start": "1806145",
    "end": "1812910"
  },
  {
    "text": "move south, move west, or move east, right? Four simple actions that we can take.",
    "start": "1812910",
    "end": "1819885"
  },
  {
    "text": "And the probability PSA is also called the transition probability.",
    "start": "1819885",
    "end": "1824955"
  },
  {
    "text": "What this means is this is a probability distribution over the states.",
    "start": "1824955",
    "end": "1830779"
  },
  {
    "text": "And what this basically means is if we are in one particular state, that's indexed by s and we take an action indexed by a,",
    "start": "1830780",
    "end": "1839695"
  },
  {
    "text": "then what is the probability that we're going to end up in one of the other states, right?",
    "start": "1839695",
    "end": "1845610"
  },
  {
    "text": "So it's- it's a probability distribution over the state space and it has two indices,",
    "start": "1845610",
    "end": "1850860"
  },
  {
    "text": "the current state and the action that we're taking. So for example, if you're- if you're",
    "start": "1850860",
    "end": "1856020"
  },
  {
    "text": "building a robot that has to move in this kind of a map. If the robot is in,",
    "start": "1856020",
    "end": "1861450"
  },
  {
    "text": "uh,- in 1, 3, then p_1, 3 and let's say the action that we take is go north- north,",
    "start": "1861450",
    "end": "1871830"
  },
  {
    "text": "so this is P (s,a) state, is, you know, the state we are in, 1, 3 and the action we take is go north,",
    "start": "1871830",
    "end": "1878414"
  },
  {
    "text": "then this will be a probability distribution such that,",
    "start": "1878415",
    "end": "1883695"
  },
  {
    "text": "uh, let's say there is- there is, you know, some kind of uncertainty in the robot.",
    "start": "1883695",
    "end": "1889110"
  },
  {
    "text": "And whenever we ask it to go north with- let's say with 80% probability, it actually goes north and with a 10% probability,",
    "start": "1889110",
    "end": "1896730"
  },
  {
    "text": "it- it goes west, and with 10% probability it goes east, right? So this will be a probability distribution that has zeroes",
    "start": "1896730",
    "end": "1905910"
  },
  {
    "text": "everywhere, 0.1 that corresponds to the state 1, 4 right?",
    "start": "1905910",
    "end": "1915075"
  },
  {
    "text": "0.1 that corresponds to the state 1.2, 0.8 corresponding to the state 2, 3.",
    "start": "1915075",
    "end": "1923710"
  },
  {
    "text": "And zeros everywhere. Okay. Is this clear?",
    "start": "1923870",
    "end": "1929070"
  },
  {
    "text": "For- for every state, we take an action. And when we take an action, we can end up in a new state.",
    "start": "1929070",
    "end": "1935205"
  },
  {
    "text": "And that- that transition of moving to the next state is stochastic. And that stochasticity- that stochasticity is captured by this probability distribution,",
    "start": "1935205",
    "end": "1946004"
  },
  {
    "text": "which tells us with what probability we're going to end up in any given state, depending on what action we took and what state we were currently in. Yes, question?",
    "start": "1946004",
    "end": "1956130"
  },
  {
    "text": "So action is for the- the action said it's for the [inaudible] like you can't say that for certain.",
    "start": "1956130",
    "end": "1964065"
  },
  {
    "text": "Yeah, I'll come to that- I'll come to that. So, uh, uh- so let- let's for, you know, simplicity assume we can take any action in any state for now, right?",
    "start": "1964065",
    "end": "1973125"
  },
  {
    "text": "So, uh- so that's- that's called a, uh, uh, transition probability. And then there is something called the, uh,",
    "start": "1973125",
    "end": "1979830"
  },
  {
    "text": "discount factor Gamma, we'll come to that, and there is a reward function. So a reward function is basically, uh,",
    "start": "1979830",
    "end": "1987105"
  },
  {
    "text": "the reward that our agent earns immediately after, you know, when it is in some- some particular state.",
    "start": "1987105",
    "end": "1994304"
  },
  {
    "text": "For example, if- if our agent comes to this state where we, you know, want it to be, it earns a reward of plus 1.",
    "start": "1994305",
    "end": "2001340"
  },
  {
    "text": "And if we wanted to avoid the state, it earns a reward of minus 1, right? And let's say, you know,",
    "start": "2001340",
    "end": "2007895"
  },
  {
    "text": "the reward at all the other places is a- a small negative reward of 0.02 minus,",
    "start": "2007895",
    "end": "2014840"
  },
  {
    "text": "you know, 0.02, minus 0.02 and so on, right?",
    "start": "2014840",
    "end": "2021529"
  },
  {
    "text": "And these are the immediate rewards that the model gets or- or the agent gets for entering a particular state.",
    "start": "2021530",
    "end": "2028115"
  },
  {
    "text": "And most of the times, the reward function is represented as a function of the state,",
    "start": "2028115",
    "end": "2034985"
  },
  {
    "text": "you know, it gives you some, uh, uh, real valued reward. And sometimes, it is also modeled as R of s, a,",
    "start": "2034985",
    "end": "2044090"
  },
  {
    "text": "where when you are in a particular state and you take the next action a,",
    "start": "2044090",
    "end": "2049744"
  },
  {
    "text": "you get some particular reward. The reason why sometimes, uh,",
    "start": "2049745",
    "end": "2055040"
  },
  {
    "text": "we tend to model- model it as a state action pair rather than state alone is because taking,",
    "start": "2055040",
    "end": "2060815"
  },
  {
    "text": "uh- taking different actions may have different costs. So, uh, even though you earn the same reward for being in a state,",
    "start": "2060815",
    "end": "2067235"
  },
  {
    "text": "the fact that you took one action over another, you may be paying a different cost. So, you know, uh, to- to- to, uh,",
    "start": "2067235",
    "end": "2073159"
  },
  {
    "text": "account for such, uh, actions, specific costs, you know, it's- you know, sometimes you would- you model the reward function as-",
    "start": "2073160",
    "end": "2079520"
  },
  {
    "text": "as a function over the state action pair rather than just a state alone. But for, uh- in terms of what we're gonna cover, most of the times, we'll be, uh,",
    "start": "2079520",
    "end": "2087770"
  },
  {
    "text": "just focusing on- on reward functions as a function of state and,",
    "start": "2087770",
    "end": "2092870"
  },
  {
    "text": "uh, assume that all actions have equal cost. Yes, question. I'm sorry, how do we know what reward to use? Yeah. So in practice, how do we know?",
    "start": "2092870",
    "end": "2099319"
  },
  {
    "text": "We'll- we'll, uh, get to that,  we know it's- it's- it's part of how we define the problem, uh, uh, it- itself.",
    "start": "2099320",
    "end": "2105635"
  },
  {
    "text": "Um, yeah, so that's the, uh, reward function. And in terms of,",
    "start": "2105635",
    "end": "2115625"
  },
  {
    "text": "um- in terms of, uh, uh, what happens when we start an agent,",
    "start": "2115625",
    "end": "2121490"
  },
  {
    "text": "we assume that the agent is gonna start in some initial state, S- S naught.",
    "start": "2121490",
    "end": "2127730"
  },
  {
    "text": "Um, so let's assume that the agent starts at some initial state S naught,",
    "start": "2127730",
    "end": "2134555"
  },
  {
    "text": "and we take an action a or a naught. That's the first action taken by, uh,",
    "start": "2134555",
    "end": "2140674"
  },
  {
    "text": "the agent and by taking an action a at, uh, state S, we enter a new state,",
    "start": "2140675",
    "end": "2148205"
  },
  {
    "text": "S1, which is according to the transition probabilities.",
    "start": "2148205",
    "end": "2153890"
  },
  {
    "text": "So S_1 is now- S_1 is sampled from",
    "start": "2153890",
    "end": "2158990"
  },
  {
    "text": "P of S_0, a_0, right?",
    "start": "2158990",
    "end": "2164390"
  },
  {
    "text": "And once we are in S_1, we take action a_1 and enter state S_2,",
    "start": "2164390",
    "end": "2170330"
  },
  {
    "text": "where S_2 is sampled from P of S_2, S_1, a_1.",
    "start": "2170330",
    "end": "2177890"
  },
  {
    "text": "All right? And then we take action a_2 into S_3,",
    "start": "2177890",
    "end": "2183109"
  },
  {
    "text": "you know, action a_3, and so on, right? This sequence of states and actions",
    "start": "2183110",
    "end": "2192530"
  },
  {
    "text": "is commonly called- there are multiple names that you will see in- in- in- in, uh, different books or different resources.",
    "start": "2192530",
    "end": "2198829"
  },
  {
    "text": "You can either call it a trial, or you can call it an episode, or trajectory, right?",
    "start": "2198830",
    "end": "2209975"
  },
  {
    "text": "All these are, you know, synonymous for a sequence of states and actions, uh, taken by an agent,",
    "start": "2209975",
    "end": "2216710"
  },
  {
    "text": "where- where the action is taken by the agent and the environment lands you in the new state according to the transition probabilities. Yes, question?",
    "start": "2216710",
    "end": "2224000"
  },
  {
    "text": "[inaudible] the probability of going north when we are at 1, 3 is a vector?",
    "start": "2224000",
    "end": "2229470"
  },
  {
    "start": "2230000",
    "end": "2353000"
  },
  {
    "text": "So, uh, the question is why is, uh, the probability of going north when we are on 1, 3 a vector?",
    "start": "2230470",
    "end": "2239210"
  },
  {
    "text": "Yeah, so, uh, in general- so- so the way to think of reinforcement learning is you are in",
    "start": "2239210",
    "end": "2245300"
  },
  {
    "text": "an agent that lives in some kind of an environment, right? You are in one particular state and you take an action.",
    "start": "2245300",
    "end": "2251060"
  },
  {
    "text": "And by- by taking an action, you end up in another state. But the state that you end up in can be random, right?",
    "start": "2251060",
    "end": "2259430"
  },
  {
    "text": "So let's say you are building a robot and in your robot, you, uh, ask the robot to move north and for whatever reason,",
    "start": "2259430",
    "end": "2266600"
  },
  {
    "text": "maybe there's a stone under its wheel or maybe there is some- some things that are beyond our control. Your robot may end up going to",
    "start": "2266600",
    "end": "2272930"
  },
  {
    "text": "a different location than the one that you are- than you are, uh, actually interested in. Maybe you ask your robot to move,",
    "start": "2272930",
    "end": "2279005"
  },
  {
    "text": "you know, a meter in one direction, but in- in reality, it may end up moving, say, 95 centimeters or 105 centimeters. All right.",
    "start": "2279005",
    "end": "2285530"
  },
  {
    "text": "So there is some stochasticity in the environment due to which we always don't go",
    "start": "2285530",
    "end": "2291200"
  },
  {
    "text": "to the same state that we- that an action will not always take you to the same state all the time, right?",
    "start": "2291200",
    "end": "2296885"
  },
  {
    "text": "And that is why, here, if you are in, you know, 1, 3 and you- you know- uh, uh, and you are- when you go north, most of the times,",
    "start": "2296885",
    "end": "2304010"
  },
  {
    "text": "you will go to 2, 3 with probability, you know, 0.8. But sometimes you may go to you know 1, 2 or you may go to 1,",
    "start": "2304010",
    "end": "2309470"
  },
  {
    "text": "4, sorry, either here or here with probability 0.1. But, you know, most of the times, you end up going north. That's- that's the general idea. Yes, question.",
    "start": "2309470",
    "end": "2316965"
  },
  {
    "text": "Just to- just to understand the [inaudible] so would- would the probability be like,",
    "start": "2316965",
    "end": "2323540"
  },
  {
    "text": "uh, let's say we try to use this [inaudible] , would that stochasticity be like [inaudible]",
    "start": "2323540",
    "end": "2331850"
  },
  {
    "text": "I'm- I'm sorry. I didn't understand the question. [inaudible] Is that [inaudible] look like [inaudible] what actually maybe the opponent can make like-",
    "start": "2331850",
    "end": "2338869"
  },
  {
    "text": "Yeah. So- so I- I think you're talking about- [NOISE] about um, a game playing where there is an opponent.",
    "start": "2338870",
    "end": "2344494"
  },
  {
    "text": "Um, so- uh um, I'll- I'll push that question until later. Uh, for now, let's assume we are, you know,",
    "start": "2344495",
    "end": "2350330"
  },
  {
    "text": "the environment is stochastic for whatever reason, right? [NOISE] Yeah, so it could be",
    "start": "2350330",
    "end": "2355670"
  },
  {
    "start": "2353000",
    "end": "2605000"
  },
  {
    "text": "if you're in- if you're in- uh, in a- in a game playing scenario, it could be due to the opponent um, um- So um yeah,",
    "start": "2355670",
    "end": "2364430"
  },
  {
    "text": "so this- this, we call it as a- as a trial or an episode or a trajectory, and all three are- all three names are synonymous.",
    "start": "2364430",
    "end": "2370790"
  },
  {
    "text": "[NOISE] And now, what we- what we- uh when- when an agent goes through a particular trajectory [NOISE] at each states,",
    "start": "2370790",
    "end": "2379640"
  },
  {
    "text": "it is earning some reward. [NOISE] So over here, it earns R of S naught and it lands in state S_1.",
    "start": "2379640",
    "end": "2388070"
  },
  {
    "text": "The agent earns R of S_1, right? From here, R of S_2. [NOISE] So the agent is accumulating rewards at each step, right?",
    "start": "2388070",
    "end": "2401690"
  },
  {
    "text": "[NOISE] And [NOISE] the- and this is where the discount factor comes into the picture, right?",
    "start": "2401690",
    "end": "2407359"
  },
  {
    "text": "So the discount factor is basically a factor which we multiply all future rewards by.",
    "start": "2407360",
    "end": "2414215"
  },
  {
    "text": "So [NOISE] the reward for being in state S_1 [NOISE] will be discounted by a factor Gamma.",
    "start": "2414215",
    "end": "2421250"
  },
  {
    "text": "[NOISE] The rewarded state, S_2, will be discounted by a factor Gamma squared.",
    "start": "2421250",
    "end": "2426515"
  },
  {
    "text": "The reward from S_3 [NOISE] will be discounted by a factor Gamma q.",
    "start": "2426515",
    "end": "2432559"
  },
  {
    "text": "And what the agent- um [NOISE] what the agent um, um,",
    "start": "2432560",
    "end": "2438290"
  },
  {
    "text": "accumulates over the future is basically the sum of these discounted rewards, right?",
    "start": "2438290",
    "end": "2444275"
  },
  {
    "text": "And this is- [NOISE] and this sum of accumulated rewards is what we wanna maximize.",
    "start": "2444275",
    "end": "2451474"
  },
  {
    "text": "And the- the uh idea here, the reason why we call it discount factor is one way to think of it is",
    "start": "2451475",
    "end": "2458510"
  },
  {
    "text": "um, any positive reward that we get is better obtained sooner rather than later, right?",
    "start": "2458510",
    "end": "2467120"
  },
  {
    "text": "So the discount factor incentivizes the model to earn larger rewards sooner.",
    "start": "2467120",
    "end": "2473060"
  },
  {
    "text": "And it also incentivizes the model to um, postpone any negative rewards into the future,",
    "start": "2473060",
    "end": "2480361"
  },
  {
    "text": "right? That's one way to think of it. Another common interpretation of the discount factor, especially if you're in a finance uh, uh, setting,",
    "start": "2480361",
    "end": "2487750"
  },
  {
    "text": "is you can think of the discount factor as interest rate, right, uh, which means uh, you want to- you want to uh, Earn",
    "start": "2487750",
    "end": "2496480"
  },
  {
    "text": "your money sooner rather than later and you wanna postpone your losses into the future rather than- you know,",
    "start": "2496480",
    "end": "2503890"
  },
  {
    "text": "uh, rather than sooner, right? [NOISE] Yes, question. Uh, with respect to [inaudible] doesn't that you need to sub out the most [inaudible]",
    "start": "2503890",
    "end": "2511880"
  },
  {
    "text": "Well, so I'm no- uh let's- let's push game- game playing. You know, we'll cover game playing tomorrow. Uh, for now, let's assume only in the setting where we have",
    "start": "2511880",
    "end": "2519290"
  },
  {
    "text": "an agent that's working in some kind of an environment, right? We'll cover game - game playing. That's the [inaudible] that's about the [inaudible] like",
    "start": "2519290",
    "end": "2525550"
  },
  {
    "text": "if- if you- if you are trying to maximize your winning just short of that- uh [inaudible] suppose to let by the end of the game?",
    "start": "2525550",
    "end": "2533075"
  },
  {
    "text": "So we- we- we- we're- we're gonna look at- we're gonna touch upon exactly that- that- that- uh that topic of- of- uh,",
    "start": "2533075",
    "end": "2538130"
  },
  {
    "text": "you know, sub-optimality, right? So uh, this is - this is the uh, setting in which uh, the- the uh, agent operates.",
    "start": "2538130",
    "end": "2547595"
  },
  {
    "text": "So we want to design uh, an agent that, given this Markov decision process,",
    "start": "2547595",
    "end": "2553220"
  },
  {
    "text": "wants to or needs to maximize the sum of accumulated rewards into the future,  right?",
    "start": "2553220",
    "end": "2561170"
  },
  {
    "text": "And these rewards are discounted by a discount factor Gamma and this discount factor Gamma,",
    "start": "2561170",
    "end": "2567830"
  },
  {
    "text": "um, um, there are many, many um, um, uh, good reasons to have this discount factor Gamma,",
    "start": "2567830",
    "end": "2573005"
  },
  {
    "text": "but some of the interpretation is uh, you can think of this Gamma as, you know, interest rate or you can think of it as um, um, uh,",
    "start": "2573005",
    "end": "2580970"
  },
  {
    "text": "a motivation to make the agent um,  um,  get a reward sooner rather than later.  Yes, question.",
    "start": "2580970",
    "end": "2586730"
  },
  {
    "text": "[inaudible] the same Gamma? Yeah, it- it's- it's- uh the Gamma is the same Gamma that we use at all time steps.",
    "start": "2586730",
    "end": "2595170"
  },
  {
    "text": "Is there a particular reason? Yes, there's a particular reason. Uh, we'll- we'll- um, uh, we're gon- we'll- we'll cover this in-",
    "start": "2595330",
    "end": "2602110"
  },
  {
    "text": "in- in- shortly why- why we're gonna use the same Gamma, right? So now uh, given this- given this setting,",
    "start": "2602110",
    "end": "2610010"
  },
  {
    "start": "2605000",
    "end": "3380000"
  },
  {
    "text": "we can define two central concepts, right,",
    "start": "2610010",
    "end": "2617740"
  },
  {
    "text": "and these two concepts are gonna- [NOISE] are gonna stay with us for the rest of reinforcement learning.",
    "start": "2617740",
    "end": "2625630"
  },
  {
    "text": "[NOISE]",
    "start": "2625630",
    "end": "2649220"
  },
  {
    "text": "So, so we're gonna define something called as",
    "start": "2649220",
    "end": "2658490"
  },
  {
    "text": "a policy Pi",
    "start": "2658490",
    "end": "2663545"
  },
  {
    "text": "which is mapping from state to action.",
    "start": "2663545",
    "end": "2669290"
  },
  {
    "text": "So the policy tells you what action to take when you are in a given state,  right?",
    "start": "2669290",
    "end": "2676355"
  },
  {
    "text": "And this is the policy over which um, the agent has flexibility to learn, right?",
    "start": "2676355",
    "end": "2682220"
  },
  {
    "text": "Our goal is to learn a policy. We wanna learn this function that maps us from state to action and we want to",
    "start": "2682220",
    "end": "2689240"
  },
  {
    "text": "learn a policy that is going to maximize something called as the value,  right?",
    "start": "2689240",
    "end": "2695435"
  },
  {
    "text": "So the value associated with the policy Pi is a function",
    "start": "2695435",
    "end": "2701420"
  },
  {
    "text": "that takes us from a state to a real value,",
    "start": "2701420",
    "end": "2707660"
  },
  {
    "text": "where V_Pi of S is defined as expectation",
    "start": "2707660",
    "end": "2717470"
  },
  {
    "text": "of R of S naught plus Gamma times R of",
    "start": "2717470",
    "end": "2727595"
  },
  {
    "text": "S_1 plus Gamma squared times R of S_2",
    "start": "2727595",
    "end": "2732920"
  },
  {
    "text": "and so on until S naught equals S, comma.",
    "start": "2732920",
    "end": "2739309"
  },
  {
    "text": "So we define the value of a state by following a particular policy",
    "start": "2739310",
    "end": "2747320"
  },
  {
    "text": "to be the sum of all the expected rewards from- starting from the first state all the way,",
    "start": "2747320",
    "end": "2757099"
  },
  {
    "text": "you know, until- un- until infinity where uh, each reward at a particular time step is",
    "start": "2757100",
    "end": "2763820"
  },
  {
    "text": "discounted by the corresponding Gamma factor, right? And over- over here, uh, the- the S,",
    "start": "2763820",
    "end": "2771695"
  },
  {
    "text": "which is the parameter of the value function. Besides, what is the starting state will be, right?",
    "start": "2771695",
    "end": "2776885"
  },
  {
    "text": "So the value of the value of a particular state by following a policy Pi is the discounted sum of all future rewards.",
    "start": "2776885",
    "end": "2787265"
  },
  {
    "text": "Assuming we start at the given state and the actions that we take at",
    "start": "2787265",
    "end": "2793115"
  },
  {
    "text": "each step is according to the policy that- that's given to us, right?",
    "start": "2793115",
    "end": "2800195"
  },
  {
    "text": "You start at a given state, start taking actions according to a policy, the environment based on the transition probabilities is gonna keep throwing you",
    "start": "2800195",
    "end": "2806960"
  },
  {
    "text": "into different states and when you go to a new state, [refer to your policy and take the next action and, you know,",
    "start": "2806960",
    "end": "2814099"
  },
  {
    "text": "you keep going to new states and as you're going to- uh, as you're going from one state to another,",
    "start": "2814100",
    "end": "2820130"
  },
  {
    "text": "you're accumulating rewards and we discount each of those rewards by the corresponding Gamma factor, right?",
    "start": "2820130",
    "end": "2827690"
  },
  {
    "text": "And the value of a particular- uh, uh, value of a policy at a given state is this expected sum of future rewards.",
    "start": "2827690",
    "end": "2836135"
  },
  {
    "text": "And the reason why we use expectation here [NOISE] is because these rewards are random.",
    "start": "2836135",
    "end": "2843170"
  },
  {
    "text": "The r- reason why the rewards are random is because the state that we end up in is random,",
    "start": "2843170",
    "end": "2848570"
  },
  {
    "text": "and the reason why the state that we end up in uh, is random is because of the transition probabilities, right?",
    "start": "2848570",
    "end": "2855619"
  },
  {
    "text": "So if the transition probabilities were not stochastic, so if you always reach the same state,",
    "start": "2855620",
    "end": "2862280"
  },
  {
    "text": "when we took a particular action every time, then we would not need this expectation but because the transitions are stochastic,",
    "start": "2862280",
    "end": "2871595"
  },
  {
    "text": "we're- uh, the value is defined as the expected sum of future rewards or sum of future discounted rewards.",
    "start": "2871595",
    "end": "2879245"
  },
  {
    "text": "Any questions about this? Yes, question. [inaudible]",
    "start": "2879245",
    "end": "2885140"
  },
  {
    "text": "But this is a comma. All right.",
    "start": "2885140",
    "end": "2891305"
  },
  {
    "text": "So this is- this is- uh, this is called the value function and this is called the policy, right?",
    "start": "2891305",
    "end": "2898099"
  },
  {
    "text": "And as we will see probably tomorrow, you know, these two concepts are- are kind of- um they",
    "start": "2898100",
    "end": "2905030"
  },
  {
    "text": "are the heart of reinforcement learning because you can kind of um, classify the entire field of reinforcement learning into those that try to",
    "start": "2905030",
    "end": "2914570"
  },
  {
    "text": "optimi- or- or learn good value function versus those that directly try to learn policies, right?",
    "start": "2914570",
    "end": "2919940"
  },
  {
    "text": "So the- this- this- uh these two are central concepts that- that we're going to encounter over and over.",
    "start": "2919940",
    "end": "2926360"
  },
  {
    "text": "And it's this value function that is- um,",
    "start": "2926360",
    "end": "2931730"
  },
  {
    "text": "that's related to rewards but also it is different from rewards.",
    "start": "2931730",
    "end": "2937760"
  },
  {
    "text": "So reward is- you know, you can think of reward as like the- um reward is the immediate reward that you get by just entering into a state",
    "start": "2937760",
    "end": "2947420"
  },
  {
    "text": "whereas value is- you can think of value as the long-term benefit",
    "start": "2947420",
    "end": "2952880"
  },
  {
    "text": "or the sum of all future words that you're gonna accumulate by following a particular policy.",
    "start": "2952880",
    "end": "2958665"
  },
  {
    "text": "So  reward, you can think of it as short-sighted, not something that you're gonna get immediately",
    "start": "2958665",
    "end": "2963790"
  },
  {
    "text": "by going into a particular state, whereas, you know, value is- is like the long-term value by following a particular policy. Yes, question?",
    "start": "2963790",
    "end": "2972670"
  },
  {
    "text": "[inaudible] understand so the value keeps changing, right, um, depending on [inaudible] state. So even in the 1st state,  um,",
    "start": "2972670",
    "end": "2978950"
  },
  {
    "text": "S_0 is always thinking of where you are at the moment and definitely, the value is shifting all the time, right? [NOISE] It can always [inaudible] .",
    "start": "2978950",
    "end": "2985580"
  },
  {
    "text": "So, um, the- so- so the question is the states that you end up in are- you know, can be different because of,",
    "start": "2985580",
    "end": "2991625"
  },
  {
    "text": "you know, the- the dynamics are- are- are stochastic, which is why we have the expectation. Right?",
    "start": "2991625",
    "end": "2997880"
  },
  {
    "text": "Based on the notation, does it mean the S naught is not fixed? It depends on where it ends up [inaudible]. So- yes, so in this expression,",
    "start": "2997880",
    "end": "3005274"
  },
  {
    "text": "S naught is not random because we have conditioned S naught to be something. That S is S that is,",
    "start": "3005274",
    "end": "3011605"
  },
  {
    "text": "you know, parameter to the function. So S naught here is not random because it's given. All the future S_1, S_2 are random.",
    "start": "3011605",
    "end": "3019700"
  },
  {
    "text": "Okay. So we can now rewrite this in a slightly different way,",
    "start": "3021570",
    "end": "3028855"
  },
  {
    "text": "you know, by actually recognizing that S naught is, um, um, not random.",
    "start": "3028855",
    "end": "3035140"
  },
  {
    "text": "And so we can rewrite this as R of S, because S naught was just equal to S,",
    "start": "3035140",
    "end": "3041605"
  },
  {
    "text": "plus Gamma times sum",
    "start": "3041605",
    "end": "3048430"
  },
  {
    "text": "over S prime and S, P of S, Pi of S,",
    "start": "3048430",
    "end": "3059620"
  },
  {
    "text": "S prime,",
    "start": "3059620",
    "end": "3064960"
  },
  {
    "text": "V Pi of S prime.",
    "start": "3064960",
    "end": "3071349"
  },
  {
    "text": "How did we go from this equation to this equation? So the way we went from this equation to this equation is by",
    "start": "3071350",
    "end": "3080260"
  },
  {
    "text": "recognizing the fact that this- this expression",
    "start": "3080260",
    "end": "3085735"
  },
  {
    "text": "can be now written as R of S plus Gamma",
    "start": "3085735",
    "end": "3095110"
  },
  {
    "text": "times expectation of R",
    "start": "3095110",
    "end": "3100320"
  },
  {
    "text": "of S_1 plus- plus R of S_1 plus Gamma,",
    "start": "3100320",
    "end": "3107755"
  },
  {
    "text": "R of S_2 plus Gamma times Gamma squared times R of S_3, and so on.",
    "start": "3107755",
    "end": "3118075"
  },
  {
    "text": "Right. Because this was- this was not random, we took it out. And all of these terms have a Gamma,",
    "start": "3118075",
    "end": "3126010"
  },
  {
    "text": "so we take the Gamma factor out. And what we end up with is- is- is,",
    "start": "3126010",
    "end": "3132085"
  },
  {
    "text": "um, the expectation over all of these. And this- we again recognize that this whole thing over here is,",
    "start": "3132085",
    "end": "3144890"
  },
  {
    "text": "right, so this whole thing is just V Pi of S_1.",
    "start": "3154590",
    "end": "3160165"
  },
  {
    "text": "Right. And by making this- this- uh, by, uh, recognizing this recursive pattern,",
    "start": "3160165",
    "end": "3167065"
  },
  {
    "text": "we can define this recursive statement that V Pi of S is equal to",
    "start": "3167065",
    "end": "3172405"
  },
  {
    "text": "the immediate reward that we obtained by being at S plus the discounted or,",
    "start": "3172405",
    "end": "3178150"
  },
  {
    "text": "uh, you know, uh, the discount factor times, um, the expectation of the future values for- of each particular state,",
    "start": "3178150",
    "end": "3186445"
  },
  {
    "text": "you know, times the probability that we end up in that state by taking the action. Right. And the action that we take is according to the policy. Yes, question.",
    "start": "3186445",
    "end": "3196330"
  },
  {
    "text": "So the S primes are the successors to [inaudible] , immediate successor? Yes. So the S primes are the immediate successor states.",
    "start": "3196330",
    "end": "3201940"
  },
  {
    "text": "Right. So, um, we are add- um, just for the fact that we are in state S,",
    "start": "3201940",
    "end": "3207684"
  },
  {
    "text": "we earn R of S. And once we have R of S, we take some action according to Pi of S. The policy tells us what action to take.",
    "start": "3207685",
    "end": "3218635"
  },
  {
    "text": "And by taking this action, we can end up with one of the all possible, um, uh,",
    "start": "3218635",
    "end": "3225025"
  },
  {
    "text": "future states or the successor states that we denote by S prime, and the- and for landing in one particular S prime,",
    "start": "3225025",
    "end": "3233500"
  },
  {
    "text": "we earn, you know, V Pi of S prime as- uh, as the as- as the, uh, uh, future value.",
    "start": "3233500",
    "end": "3238780"
  },
  {
    "text": "Right. And so the overall value of a state can be decomposed into these two parts,",
    "start": "3238780",
    "end": "3245410"
  },
  {
    "text": "the immediate part and everything else that follows. Yes, question.",
    "start": "3245410",
    "end": "3250970"
  },
  {
    "text": "Uh, yes, can you, uh, clarify the notation V S [inaudible]? Yeah, so the notation here- this is,",
    "start": "3254730",
    "end": "3259869"
  },
  {
    "text": "uh, the transition probability, that's PSA, A here is Pi of S. You're following,",
    "start": "3259870",
    "end": "3266680"
  },
  {
    "text": "uh, the policy of what action to take, right? And so this is PSA, think of this as PSA,",
    "start": "3266680",
    "end": "3272649"
  },
  {
    "text": "and PSA is, uh, probability- uh, uh, uh, probability function over all the states.",
    "start": "3272649",
    "end": "3279205"
  },
  {
    "text": "So PSA of S prime tells you the probability of ending in S prime,",
    "start": "3279205",
    "end": "3284815"
  },
  {
    "text": "if you were in state S and took action Pi of S. Right.",
    "start": "3284815",
    "end": "3290470"
  },
  {
    "text": "And depending on which state we end up in, we get an appropriate value out of it. And so this is just the expected value of taking- uh,",
    "start": "3290470",
    "end": "3298964"
  },
  {
    "text": "expected future value of taking, uh, uh, action according to Pi of S. Yes, question.",
    "start": "3298965",
    "end": "3304960"
  },
  {
    "text": "So V Pi S, what's the expected value [inaudible]? Uh, can you- can you, uh, repeat the question again? With V Pi S, one is just a sum, not expected value, right?",
    "start": "3311390",
    "end": "3317720"
  },
  {
    "text": "So V Pi S is- so- uh, so there was this expectation here, right?",
    "start": "3318810",
    "end": "3325315"
  },
  {
    "text": "In this expectation here, we could take, uh, R of S naught out because this was not random, right?",
    "start": "3325315",
    "end": "3331704"
  },
  {
    "text": "And what we end up with, that is the rest over here, we're just calling this V Pi S.",
    "start": "3331705",
    "end": "3336805"
  },
  {
    "text": "It's the expected value? Yeah, it's the expected value, yeah. Yeah, the S- S_1 through,",
    "start": "3336805",
    "end": "3342595"
  },
  {
    "text": "you know, uh, S_1, S_2, S_3 are still random, and so we need to still have the expectation.",
    "start": "3342595",
    "end": "3348260"
  },
  {
    "text": "Right. And this equation, you know, V Pi S,",
    "start": "3348720",
    "end": "3354895"
  },
  {
    "text": "where V is rep- uh, is defined recursively, right, so we have V on the left hand and V on the right,",
    "start": "3354895",
    "end": "3361630"
  },
  {
    "text": "this recursive equation is also called the Bellman equation.",
    "start": "3361630",
    "end": "3366380"
  },
  {
    "text": "Right. And let's put a box because this is important. All right.",
    "start": "3373410",
    "end": "3383770"
  },
  {
    "start": "3380000",
    "end": "3599000"
  },
  {
    "text": "So to quickly summarize, we have these two central concepts called policy and value, right?",
    "start": "3383770",
    "end": "3391090"
  },
  {
    "text": "Policy is just what actions we wanna take when we are in a given state. It's a simple function, nothing fancy there.",
    "start": "3391090",
    "end": "3398515"
  },
  {
    "text": "Value is this long-term extension of reward, right? Reward is this short-term, short-sighted,",
    "start": "3398515",
    "end": "3405970"
  },
  {
    "text": "immediate- immediate gratifi- gratification that we're gonna get by,",
    "start": "3405970",
    "end": "3411474"
  },
  {
    "text": "you know, uh, being in a particular state. Whereas value is this, you know, long-term, long-sighted, you know,",
    "start": "3411475",
    "end": "3417790"
  },
  {
    "text": "what is the- what is the total sum of discounted rewards that we're gonna make in",
    "start": "3417790",
    "end": "3423700"
  },
  {
    "text": "the long-term by following a particular policy, and that is value. Right. So and- and this distinction between-",
    "start": "3423700",
    "end": "3430345"
  },
  {
    "text": "between immediate reward and long-term value is at the heart of reinforcement learning.",
    "start": "3430345",
    "end": "3436240"
  },
  {
    "text": "Because if our goal was to just maximize the immediate reward,",
    "start": "3436240",
    "end": "3441655"
  },
  {
    "text": "you would just be back in supervised learning setting. Right. It is because that we want to maximize",
    "start": "3441655",
    "end": "3448570"
  },
  {
    "text": "this long-term reward where we need to take this long-term view, we may need to make some sacrifices of",
    "start": "3448570",
    "end": "3455680"
  },
  {
    "text": "short-term rewards in order to maximize the long-term value. And that's what makes reinforcement learning",
    "start": "3455680",
    "end": "3461950"
  },
  {
    "text": "hard and makes it distinct from supervised learning. Right. Yes, question.",
    "start": "3461950",
    "end": "3471369"
  },
  {
    "text": "So, uh, in- in that sum, [inaudible] you have gone for, uh,",
    "start": "3471370",
    "end": "3477685"
  },
  {
    "text": "all- all possibilities from if we start from [inaudible].",
    "start": "3477685",
    "end": "3487765"
  },
  {
    "text": "Yes. Yeah. So- so the question is, does this expectation cover all possible future scenarios?",
    "start": "3487765",
    "end": "3493555"
  },
  {
    "text": "Yes, it covers all possible future scenarios where you may win, you may lose, your robot may crash right away,",
    "start": "3493555",
    "end": "3498940"
  },
  {
    "text": "the- your robot may land your goal, uh, you know, reach the particular goal. And, you know, all these possible scenarios",
    "start": "3498940",
    "end": "3505510"
  },
  {
    "text": "that may happen by following a particular policy Pi. So our goal, uh, is to find the, uh, Pi?",
    "start": "3505510",
    "end": "3510859"
  },
  {
    "text": "So our goal which- which- which- which we're going to, uh, talk about next, is to come up with a policy that is going to maximize our values for- for all states.",
    "start": "3512700",
    "end": "3523975"
  },
  {
    "text": "Any other question? All right. So",
    "start": "3523975",
    "end": "3529960"
  },
  {
    "text": "before we go into- into maximizing these value functions, first, let's solve, you know,",
    "start": "3529960",
    "end": "3537565"
  },
  {
    "text": "given a policy, how do we calculate V^Pi, right? So- so the Picture to have is-",
    "start": "3537565",
    "end": "3543520"
  },
  {
    "text": "[NOISE] so these two concepts,",
    "start": "3543520",
    "end": "3552400"
  },
  {
    "text": "you know, value and policy, um, you can, kind of, think of them as the duals of each other, right?",
    "start": "3552400",
    "end": "3559839"
  },
  {
    "text": "So- [NOISE] and there is this,",
    "start": "3559840",
    "end": "3565015"
  },
  {
    "text": "kind of, um, relation between them such that one gets induced given the other.",
    "start": "3565015",
    "end": "3571059"
  },
  {
    "text": "[NOISE]",
    "start": "3571060",
    "end": "3585760"
  },
  {
    "text": "I should have just erase that. Never mind. So, um- so we have",
    "start": "3585760",
    "end": "3591250"
  },
  {
    "text": "policy and we have value, right?",
    "start": "3591250",
    "end": "3598000"
  },
  {
    "text": "So policy takes us from S to A, right?",
    "start": "3598000",
    "end": "3603865"
  },
  {
    "text": "Value takes us from S to R, right?",
    "start": "3603865",
    "end": "3609535"
  },
  {
    "text": "And again, value is distinct from reward because reward is immediate gratification and value is long-term value.",
    "start": "3609535",
    "end": "3619630"
  },
  {
    "text": "Which means, in order to maximize value, you may have to sacrifice immediate rewards",
    "start": "3619630",
    "end": "3625210"
  },
  {
    "text": "if there is a prospect of doing very well in the long run, right? That's- that's the key distinction between reward and value.",
    "start": "3625210",
    "end": "3630640"
  },
  {
    "text": "[NOISE] And given a policy Pi,",
    "start": "3630640",
    "end": "3637339"
  },
  {
    "text": "we can calculate the corresponding value V^Pi, right? So Pi takes us from Pi to V^Pi,",
    "start": "3638700",
    "end": "3645745"
  },
  {
    "text": "and we'll see how we're going to do that. And this should be intuitive, right? If- if, um- if you start from a given state S and you follow a particular policy,",
    "start": "3645745",
    "end": "3654280"
  },
  {
    "text": "and you fo- follow the same policy over and over and you repeat such trials, you- you know,",
    "start": "3654280",
    "end": "3659500"
  },
  {
    "text": "and you average out the total value, uh, the- the- you average out all the values that you end up making,",
    "start": "3659500",
    "end": "3665740"
  },
  {
    "text": "then, you know, that is V^Pi, right? So the- so Pi implicitly defines a value function for each state, right?",
    "start": "3665740",
    "end": "3674545"
  },
  {
    "text": "And correspondingly from V^Pi or- or from any value function V,",
    "start": "3674545",
    "end": "3681025"
  },
  {
    "text": "you can construct a corresponding policy Pi,",
    "start": "3681025",
    "end": "3686755"
  },
  {
    "text": "where you say that the action that you're going to take at a given state is to- the action that you're gonna take at",
    "start": "3686755",
    "end": "3695785"
  },
  {
    "text": "each state will be to choose the action that gives you the highest probability of reaching the next state that has the highest value, right?",
    "start": "3695785",
    "end": "3705460"
  },
  {
    "text": "Try to reach the next state which has the highest value, right? And that will- that implicitly defines a policy if you start with a value function, right?",
    "start": "3705460",
    "end": "3713200"
  },
  {
    "text": "So you can go from Pi to V^Pi as we're going to see next and from V to Pi,",
    "start": "3713200",
    "end": "3718930"
  },
  {
    "text": "uh, with this- with this, um- in- in a somewhat natural way, right?",
    "start": "3718930",
    "end": "3724510"
  },
  {
    "text": "So, uh, given a Pi, if you- if you just follow it over and over, you can, you know, find out what V^Pi is.",
    "start": "3724510",
    "end": "3730510"
  },
  {
    "text": "And similarly, if you- if you are given a value function, if- if you're told that in this- in the- in the grid of your map,",
    "start": "3730510",
    "end": "3736165"
  },
  {
    "text": "what the long-term value of each one is, and you're in a particular state, try to take an action that is going to ta-, you know,",
    "start": "3736165",
    "end": "3742420"
  },
  {
    "text": "with high probability take you towards the next state that has the highest value, right?",
    "start": "3742420",
    "end": "3748510"
  },
  {
    "text": "So it's- it's, uh, they're, kind of, duals of each other. Yes, question?",
    "start": "3748510",
    "end": "3754690"
  },
  {
    "text": "So when you- [inaudible] take into account the time [inaudible] if it's time T or [inaudible]",
    "start": "3761390",
    "end": "3768570"
  },
  {
    "text": "Can you- can you please repeat the question? I think, it's about- something about time.",
    "start": "3768570",
    "end": "3772330"
  },
  {
    "text": "Uh, sure. If you're in a given state [inaudible] indicates that [inaudible] Yes. So the- so the- the policy asks",
    "start": "3781550",
    "end": "3788650"
  },
  {
    "text": "you to take a particular action A when you are in a particular state, no matter how far into the trajectory you are right now, right?",
    "start": "3788650",
    "end": "3796660"
  },
  {
    "text": "So you always follow, uh- so you always follow, uh, the same action when you are in a given state according to the policy,",
    "start": "3796660",
    "end": "3803610"
  },
  {
    "text": "ignoring all the previous- You know, you may have been- you may be entering that state for the first time, you may be, uh, entering it for the thousandth time,",
    "start": "3803610",
    "end": "3810165"
  },
  {
    "text": "but if you're following a policy, you always follow the same- same action. Yes. [inaudible] about, like, arrow that goes form V to Pi?",
    "start": "3810165",
    "end": "3818920"
  },
  {
    "text": "V to Pi, yeah. So V- V- let's say, V will- V is essentially like between the grid it's just a bunch of numbers, right?",
    "start": "3818920",
    "end": "3826089"
  },
  {
    "text": "It will tell you what value you have if you start from the state. Yeah. It- it doesn't really- so whereas, Pi tells you,",
    "start": "3826090",
    "end": "3832930"
  },
  {
    "text": "if you're at a- so Pi obviously tells you if you're here, what you should do. Yeah. Uh, but V^Pi will just tell you what the value is if you follow,",
    "start": "3832930",
    "end": "3841450"
  },
  {
    "text": "like, some unknown good trajectory, right? Like, the best possible, yeah? So- so no. V- V^Pi does not tell you the best possible value that you can get.",
    "start": "3841450",
    "end": "3849610"
  },
  {
    "text": "V^Pi tells you what is the value that you get by following this policy. It's not the best possible policy, it's not the best possible value.",
    "start": "3849610",
    "end": "3855595"
  },
  {
    "text": "So how do you deduce- uh, yeah, it'll tell you, like, the expected, uh- pick expected value that is like from [inaudible] But how do you go from,",
    "start": "3855595",
    "end": "3865810"
  },
  {
    "text": "like, a grid of numbers to a policy? Like, that arrow [inaudible] Yeah. So- so we'll- we'll come to this.",
    "start": "3865810",
    "end": "3871405"
  },
  {
    "text": "Uh, so, uh, the- the- the takeaway from here is that, you know, there is a relation from going from Pi to V^Pi,",
    "start": "3871405",
    "end": "3877285"
  },
  {
    "text": "and a relation from going from V to Pi. And- and, you know, uh, we're going to, uh, uh, clearly define how to do this, uh, next.",
    "start": "3877285",
    "end": "3885220"
  },
  {
    "text": "So let's tal-, first, let's talk about going from Pi to V^Pi, right?",
    "start": "3885220",
    "end": "3891655"
  },
  {
    "text": "And so for that, you know, from [NOISE] Pi to V^Pi.",
    "start": "3891655",
    "end": "3896920"
  },
  {
    "text": "For this, we start with the Bellman's equation, right? V^Pi of S equals",
    "start": "3896920",
    "end": "3903819"
  },
  {
    "text": "[NOISE] R of s plus Gamma times sum",
    "start": "3903819",
    "end": "3910885"
  },
  {
    "text": "over S prime in S, P_S Pi of S,",
    "start": "3910885",
    "end": "3919270"
  },
  {
    "text": "S prime times V^Pi of S, right?",
    "start": "3919270",
    "end": "3924625"
  },
  {
    "text": "So this is just the Bellman equation, I've just written it again. And over here what we see is that,",
    "start": "3924625",
    "end": "3932050"
  },
  {
    "text": "V^Pi of S, assuming we have a finite number of states, so V^Pi is a vector, right?",
    "start": "3932050",
    "end": "3937990"
  },
  {
    "text": "So V^Pi [NOISE] is some vector of numbers that will have real values for each,",
    "start": "3937990",
    "end": "3947365"
  },
  {
    "text": "you know, S_1, S_2, and so on, right? And this is, you know, uh, we- we've, kind of,",
    "start": "3947365",
    "end": "3954430"
  },
  {
    "text": "seen this- this view before, of switching between vectors and functions in the past, right?",
    "start": "3954430",
    "end": "3960580"
  },
  {
    "text": "So vectors and functions are essentially one and the same. So think of this V^Pi of S as a vector, uh,",
    "start": "3960580",
    "end": "3967105"
  },
  {
    "text": "having one, you know, value- value per state.",
    "start": "3967105",
    "end": "3972910"
  },
  {
    "text": "All right, so, uh, we have V^Pi- so we have V^Pi, uh,",
    "start": "3972910",
    "end": "3980020"
  },
  {
    "text": "of S_1 over here, V^Pi of S_2 over here,",
    "start": "3980020",
    "end": "3986440"
  },
  {
    "text": "uh, over here, and so on. And what this tells us is, now, we want to solve for V^Pi.",
    "start": "3986440",
    "end": "3992785"
  },
  {
    "text": "Let's assume we know R of S for every possible state S. And let's assume,",
    "start": "3992785",
    "end": "3998410"
  },
  {
    "text": "we know the transition probabilities from any state and any action to any other state, right? Let's assume we know R, P, and Gamma.",
    "start": "3998410",
    "end": "4007244"
  },
  {
    "text": "So now, this is basically for each S- you know, uh, for each possible value of S,",
    "start": "4007245",
    "end": "4013290"
  },
  {
    "text": "we get one such equation where everything is linear. And we can construct a set of linear equations.",
    "start": "4013290",
    "end": "4020660"
  },
  {
    "text": "So if this is S_1, S_1, S_1, S_1, similarly,",
    "start": "4020660",
    "end": "4028309"
  },
  {
    "text": "V^Pi of S_2 equals R of S_2 plus [NOISE] Gamma times sum over S prime in",
    "start": "4028310",
    "end": "4038205"
  },
  {
    "text": "S- P_S [NOISE] Pi of S_2 of S prime times V^Pi of S prime, right?",
    "start": "4038205",
    "end": "4048569"
  },
  {
    "text": "And so on. All the way until V^Pi of S S equals- [NOISE] So here,",
    "start": "4048570",
    "end": "4059640"
  },
  {
    "text": "each of these terms [NOISE] is a scalar variable that we want to solve.",
    "start": "4059640",
    "end": "4067700"
  },
  {
    "text": "[NOISE] And there are,",
    "start": "4067700",
    "end": "4072869"
  },
  {
    "text": "you know, um, um, um- there are capital S number of",
    "start": "4072870",
    "end": "4078210"
  },
  {
    "text": "such variables and there are capital S number of such equations. So we have S variables with S number of equations, right?",
    "start": "4078210",
    "end": "4086760"
  },
  {
    "text": "And they are related only with, you know, in- in- in a linear way. So all of these are- are known constants and scalars which we are multiplying, right?",
    "start": "4086760",
    "end": "4095955"
  },
  {
    "text": "And this can be solved, you know, with- with any kind of a linear solver. To make this more concrete, um,",
    "start": "4095955",
    "end": "4104279"
  },
  {
    "text": "let's define this matrix P^Pi- let me write it a little bigger.",
    "start": "4104280",
    "end": "4112659"
  },
  {
    "text": "So [NOISE] let's define P^Pi to be matrix S.",
    "start": "4114080",
    "end": "4124600"
  },
  {
    "text": "So you have one row per state, and one column per state, [NOISE], right?",
    "start": "4129630",
    "end": "4135190"
  },
  {
    "text": "And so P Pi, here,",
    "start": "4135190",
    "end": "4142569"
  },
  {
    "text": "each row is basically, P_s [NOISE] Pi of",
    "start": "4142570",
    "end": "4152829"
  },
  {
    "text": "s [NOISE] where s over here indicates the row number.",
    "start": "4152830",
    "end": "4159115"
  },
  {
    "text": "So let's call this s1, s1, this corresponds to s1.",
    "start": "4159115",
    "end": "4165080"
  },
  {
    "text": "Si will give you [NOISE] P_si [NOISE] Pi of si, right?",
    "start": "4165630",
    "end": "4175299"
  },
  {
    "text": "Basically, collect all the transition probabilities of every P_s1 and a, right?",
    "start": "4175300",
    "end": "4183684"
  },
  {
    "text": "And where, ah, a is chosen according to pi, and write it out like a matrix, right?",
    "start": "4183685",
    "end": "4189670"
  },
  {
    "text": "So this is just, um, um you- you- you- you're just selecting a few of",
    "start": "4189670",
    "end": "4195055"
  },
  {
    "text": "the transition probabilities where we're starting from a given, um, state s-,uh, si,",
    "start": "4195055",
    "end": "4202840"
  },
  {
    "text": "and taking an action according to, uh, Pi of si, let's call it ai.",
    "start": "4202840",
    "end": "4207895"
  },
  {
    "text": "And this is P_si ai, right? And arrange them into a matrix,",
    "start": "4207895",
    "end": "4213759"
  },
  {
    "text": "and call it, uh, P Pi. And with that matrix, we can rewrite this in a more compact form as V Pi equals",
    "start": "4213759",
    "end": "4224440"
  },
  {
    "text": "R plus Gamma times [NOISE] P Pi V Pi.",
    "start": "4224440",
    "end": "4232900"
  },
  {
    "text": "[BACKGROUND] Uh, question, what is? [BACKGROUND] I?",
    "start": "4232900",
    "end": "4238690"
  },
  {
    "text": "ISI? What does ISI stand for? No, so this is the size of the set S,",
    "start": "4238690",
    "end": "4245725"
  },
  {
    "text": "that's many numbers of states you have. All right? So we can write this in a compact way. Yes, question?",
    "start": "4245725",
    "end": "4252219"
  },
  {
    "text": "Yes. Is this factor elimination really useful because of the number of policies that we're gonna have?",
    "start": "4252220",
    "end": "4258060"
  },
  {
    "text": "Seems like that policy is just gonna be way, way more than whatever [inaudible]",
    "start": "4258060",
    "end": "4263440"
  },
  {
    "text": "So this is just, ah, ah, So the number of policies that we have are- uh,",
    "start": "4263440",
    "end": "4268510"
  },
  {
    "text": "uh you could have any number of policies, but we- you know, we are assuming that we have PSA for every possible SA already, right?",
    "start": "4268510",
    "end": "4276685"
  },
  {
    "text": "We- we are assuming we have the model, we- we know that transition probability already. [NOISE] Okay.",
    "start": "4276685",
    "end": "4283719"
  },
  {
    "text": "So all these- all these rows and all the rows, the transition probability that are not included here,",
    "start": "4283720",
    "end": "4289300"
  },
  {
    "text": "we assume we know PSA because that's part of the MDP definition. So we have PSA for all combinations of S and A. Yes, question.",
    "start": "4289300",
    "end": "4297170"
  },
  {
    "text": "So in the [inaudible] instead of [inaudible] [NOISE] Over here, should it be v- v pi S or S prime?",
    "start": "4303420",
    "end": "4310510"
  },
  {
    "text": "Over here it is S prime because we are taking the expectation over all future states",
    "start": "4310510",
    "end": "4316449"
  },
  {
    "text": "[BACKGROUND].",
    "start": "4316450",
    "end": "4325300"
  },
  {
    "text": "This should be- you're right, So here, sorry, this should be S prime. Yes. So the S1 only goes into the policy.",
    "start": "4325300",
    "end": "4333130"
  },
  {
    "text": "So this should be S prime. Thank you. All right, so this-",
    "start": "4333130",
    "end": "4338905"
  },
  {
    "text": "this set of equations can be compactly written in vector notation in this way.",
    "start": "4338905",
    "end": "4344829"
  },
  {
    "text": "And once we write it in this way, we immediately get uh- so move this over to the other side and invert it.",
    "start": "4344830",
    "end": "4354940"
  },
  {
    "text": "We get v pi equals I minus",
    "start": "4354940",
    "end": "4362935"
  },
  {
    "text": "gamma P pi inverse R. So this is a vector of",
    "start": "4362935",
    "end": "4372250"
  },
  {
    "text": "size s. This is a vector of size S. This is S cross S, it's a matrix.",
    "start": "4372250",
    "end": "4382105"
  },
  {
    "text": "This is the matrix that we constructed here. And v pi is again, it's the same v pi over here.",
    "start": "4382105",
    "end": "4388570"
  },
  {
    "text": "of size S. Alright. So take this to the other side, invert it, and we get this.",
    "start": "4388570",
    "end": "4397040"
  },
  {
    "text": "Right. So this gives us a way of starting from a policy pi",
    "start": "4398930",
    "end": "4406660"
  },
  {
    "text": "and calculating the long-term values for each of the states if we follow the policy at all times.",
    "start": "4406660",
    "end": "4415205"
  },
  {
    "text": "Okay? So this is this arrow,",
    "start": "4415205",
    "end": "4422055"
  },
  {
    "text": "so we'll call it 1. This is 1, right?",
    "start": "4422055",
    "end": "4426790"
  },
  {
    "text": "So accordingly, according to the policy that we have, you're going to end up with a different p- p pi and plug that p pi in here.",
    "start": "4428850",
    "end": "4438160"
  },
  {
    "text": "Rs are given, gamma is given. And, you know, calculate this,",
    "start": "4438160",
    "end": "4443200"
  },
  {
    "text": "we get v pi. Yes, question. So uh this [inaudible] consider i through of p Pi will be from, uh,",
    "start": "4443200",
    "end": "4451720"
  },
  {
    "text": "from the state i all the probability of every state in the [inaudible]",
    "start": "4451720",
    "end": "4457990"
  },
  {
    "text": "Yes. So each row over here tells you if we are at state SI [OVERLAPPING] and we take an action according to pi of SI.",
    "start": "4457990",
    "end": "4467800"
  },
  {
    "text": "What are the set of all possible states that we can go to with corresponding probabilities?",
    "start": "4467800",
    "end": "4473719"
  },
  {
    "text": "So this tells us how to go from policy to value. [BACKGROUND]",
    "start": "4474210",
    "end": "4484150"
  },
  {
    "text": "[NOISE] Yes [BACKGROUND].",
    "start": "4484150",
    "end": "4489429"
  },
  {
    "text": "No, we're not making an assumption that we cannot go back to the state. We can always come back to the same state again,",
    "start": "4489430",
    "end": "4494755"
  },
  {
    "text": "uh, there is no such restriction. [OVERLAPPING] So the question is, will we not converge?",
    "start": "4494755",
    "end": "4500890"
  },
  {
    "text": "We will converge and we will see why we will converge shortly. [NOISE] So this is,",
    "start": "4500890",
    "end": "4506635"
  },
  {
    "text": "this is, ah, right. This- So yeah, this is that. How to go from policy,",
    "start": "4506635",
    "end": "4513250"
  },
  {
    "text": "uh, to value i. So the policy according to different policies, we get a different p matrix because we choose",
    "start": "4513250",
    "end": "4519430"
  },
  {
    "text": "different rows according to the corresponding actions. And you plug that matrix in, construct this,",
    "start": "4519430",
    "end": "4527050"
  },
  {
    "text": "you know this related matrix, invert it multiplied with r, you get that v pi.",
    "start": "4527050",
    "end": "4532554"
  },
  {
    "text": "Right? So now. Um, we can also define what is called as this,",
    "start": "4532555",
    "end": "4541870"
  },
  {
    "text": "[NOISE] you know, recognizing the fact that we can now,",
    "start": "4541870",
    "end": "4547720"
  },
  {
    "text": "you know, given a policy, we can construct the value function. Let us move on to define something related.",
    "start": "4547720",
    "end": "4553840"
  },
  {
    "text": "[NOISE] We'll come to the other arrow shortly. All right. [NOISE]",
    "start": "4553840",
    "end": "4595710"
  },
  {
    "text": "So we also define what is called as the optimal value function.",
    "start": "4595710",
    "end": "4601840"
  },
  {
    "text": "So the optimal value function is defined as V star",
    "start": "4610100",
    "end": "4616320"
  },
  {
    "text": "of S is equal to max over Pi,",
    "start": "4616320",
    "end": "4624170"
  },
  {
    "text": "V Pi of S. Alright?",
    "start": "4624170",
    "end": "4629989"
  },
  {
    "text": "So the optimal value function is defined such that if we are at a particular state, right?",
    "start": "4629990",
    "end": "4638265"
  },
  {
    "text": "By choosing the best possible policy, what is the highest value that we can obtain?",
    "start": "4638265",
    "end": "4646425"
  },
  {
    "text": "So V Pi of S tells us what is the- the long-term pay off,",
    "start": "4646425",
    "end": "4651824"
  },
  {
    "text": "long-term discou- sum of discounted rewards, the expected sum of discounted rewards that we're gonna get if we follow",
    "start": "4651825",
    "end": "4658230"
  },
  {
    "text": "the policy Pi and V star of S is- it tells us, you know, what is the maximum possible value that you can get across",
    "start": "4658230",
    "end": "4666270"
  },
  {
    "text": "all possible policies Pi. Right?",
    "start": "4666270",
    "end": "4671925"
  },
  {
    "text": "Is S the initial state? Yes, you know, uh, here, the- so the question is here,",
    "start": "4671925",
    "end": "4677745"
  },
  {
    "text": "is- is S the initial state? Yes. So the definition of value of a function is, when- assuming you are at the current state,",
    "start": "4677745",
    "end": "4684540"
  },
  {
    "text": "it doesn't matter whether this is the initial state or you somehow landed here. If we start at a particular state S,",
    "start": "4684540",
    "end": "4692655"
  },
  {
    "text": "no matter what the history was, even if it's the first state, what is the expected sum of future,",
    "start": "4692655",
    "end": "4699045"
  },
  {
    "text": "uh- future rewards, right? So V star S is defined as the maximum possible value that you can",
    "start": "4699045",
    "end": "4706530"
  },
  {
    "text": "get by searching over the entire policy space. All right?",
    "start": "4706530",
    "end": "4711885"
  },
  {
    "text": "And V star of S can- is therefore is also written as R",
    "start": "4711885",
    "end": "4722349"
  },
  {
    "text": "of S plus max a and",
    "start": "4722350",
    "end": "4731415"
  },
  {
    "text": "E Gamma times S prime and",
    "start": "4731415",
    "end": "4737385"
  },
  {
    "text": "S P_Sa S prime times V star of S prime.",
    "start": "4737385",
    "end": "4748420"
  },
  {
    "text": "All right. So V star of S is defined as the max of V Pi of S,",
    "start": "4748970",
    "end": "4756875"
  },
  {
    "text": "where V Pi of S, we used this definition. We plugged in this definition into V Pi of S and note that R of S",
    "start": "4756875",
    "end": "4769364"
  },
  {
    "text": "is a constant that does not depend on a and it does not depend on Pi because the policy only tells us what's gonna happen in the future,",
    "start": "4769365",
    "end": "4777885"
  },
  {
    "text": "whereas the reward is what we already got. So it's gonna be R of S plus max of a in the action space,",
    "start": "4777885",
    "end": "4787034"
  },
  {
    "text": "what is the action that we need to take such that we maximize the,",
    "start": "4787035",
    "end": "4792375"
  },
  {
    "text": "uh, uh, sum of the future rewards. This equation looks pretty similar to the original Bellman equation,",
    "start": "4792375",
    "end": "4801390"
  },
  {
    "text": "except the- the difference is that here instead of Pi of S,",
    "start": "4801390",
    "end": "4806880"
  },
  {
    "text": "we are maximizing our a in place of P, uh,- Pi of S, we have P_s of a and you're maximizing over A.",
    "start": "4806880",
    "end": "4815925"
  },
  {
    "text": "And the other difference is that V Pi of S, uh, because this was for V Pi of S,",
    "start": "4815925",
    "end": "4821880"
  },
  {
    "text": "we had V Pi of S here. And over here, because this is where V star of S, you know, by the recursive definition,",
    "start": "4821880",
    "end": "4827790"
  },
  {
    "text": "we have V star of S here. Now, this- the Bellman's equation,",
    "start": "4827790",
    "end": "4833145"
  },
  {
    "text": "we saw that we can express this in a nice linear form and obtain a close form solution,",
    "start": "4833145",
    "end": "4841455"
  },
  {
    "text": "you know, to, uh, to- to- uh, to obtain V Pi.",
    "start": "4841455",
    "end": "4848295"
  },
  {
    "text": "However, V star of S, because of this max operator it's just not possible to solve it linearly, right?",
    "start": "4848295",
    "end": "4857505"
  },
  {
    "text": "So if you're given a policy, it's very easy to calculate the long-term value according to that policy.",
    "start": "4857505",
    "end": "4863340"
  },
  {
    "text": "But to calculate the best possible value or V star or the optimal value function, it's not- it's not as easy as solving a linear system, right?",
    "start": "4863340",
    "end": "4873710"
  },
  {
    "text": "And it is- it is, um- and we saw",
    "start": "4873710",
    "end": "4882840"
  },
  {
    "text": "how we can go from Pi- Pi policy to V Pi using this approach.",
    "start": "4882840",
    "end": "4888195"
  },
  {
    "text": "Now, here, you'll see how we can go from a value function back to a policy, right?",
    "start": "4888195",
    "end": "4893550"
  },
  {
    "text": "So this is the optimal value function that, you know, we are, um, um, interested in or strive to- to, uh, calculate.",
    "start": "4893550",
    "end": "4901815"
  },
  {
    "text": "And given this V Pi of S, we can define the corresponding Pi star of S to be the optimal policy to be equal",
    "start": "4901815",
    "end": "4913260"
  },
  {
    "text": "to arg max aEA, same thing over here.",
    "start": "4913260",
    "end": "4923550"
  },
  {
    "text": "Sum over S prime in capital S P_sa of S prime",
    "start": "4923550",
    "end": "4936959"
  },
  {
    "text": "times V star of S prime.",
    "start": "4936959",
    "end": "4941980"
  },
  {
    "text": "So the optimal policy tells us that the action that we need to take at state",
    "start": "4943580",
    "end": "4950760"
  },
  {
    "text": "S is basically the same action that achieved the optimal value for us.",
    "start": "4950760",
    "end": "4957840"
  },
  {
    "text": "Okay. Can you explain why the second question is not easy to solve because if you eliminate a finite set of A's,",
    "start": "4957840",
    "end": "4965415"
  },
  {
    "text": "you can eliminate all and just take the max and solve. So the question is, why is this hard to solve because, you know,",
    "start": "4965415",
    "end": "4972344"
  },
  {
    "text": "we have a limited number of- of, um, A's and- and, uh, you can- so, yes,",
    "start": "4972345",
    "end": "4977670"
  },
  {
    "text": "so in- in- in cases where, uh- in cases where you have,",
    "start": "4977670",
    "end": "4982740"
  },
  {
    "text": "um, uh, a finite state space and an action space, you can iteratively calculate this.",
    "start": "4982740",
    "end": "4987870"
  },
  {
    "text": "So this involves iteration. Whereas the- the, uh- for V Pi,",
    "start": "4987870",
    "end": "4993510"
  },
  {
    "text": "there was no iteration, you had a direct close form solution. By hard, I meant there is no close form solution, all right?",
    "start": "4993510",
    "end": "5001100"
  },
  {
    "text": "So this is the optimal value function and this is the optimal policy. [NOISE] All right.",
    "start": "5001100",
    "end": "5010700"
  },
  {
    "text": "So the optimal policy is defined as the policy that will achieve the optimal value function for us.",
    "start": "5010700",
    "end": "5018860"
  },
  {
    "text": "All right. So it's- it's- because, uh, it achieves the optimal value function because, you know,",
    "start": "5018860",
    "end": "5025265"
  },
  {
    "text": "we are- we are using the exact same, uh, action at each state that's gonna maximize, uh, the- the same right-hand side.",
    "start": "5025265",
    "end": "5032490"
  },
  {
    "text": "All right? So this is- uh, we started with the two concepts, policy and value.",
    "start": "5034450",
    "end": "5042710"
  },
  {
    "text": "[NOISE] We started with policy and value, right? These two are two very central concepts, all right?",
    "start": "5042710",
    "end": "5051305"
  },
  {
    "text": "And given a policy, we can calculate the corresponding value function that you get",
    "start": "5051305",
    "end": "5057410"
  },
  {
    "text": "by just following the policy with this closed form expression. And similarly, for any given value function,",
    "start": "5057410",
    "end": "5065239"
  },
  {
    "text": "[NOISE] we can define a policy that takes action that maximizes the- the,",
    "start": "5065240",
    "end": "5074675"
  },
  {
    "text": "uh, uh, expected value based on the given value function estimate, right? And when we use the optimal value in place,",
    "start": "5074675",
    "end": "5083570"
  },
  {
    "text": "we get the optimal policy. All right. So this is how we move from value function to policy.",
    "start": "5083570",
    "end": "5094110"
  },
  {
    "text": "Now, with these two observations, we can now come up with two algorithms",
    "start": "5096310",
    "end": "5102679"
  },
  {
    "text": "to- to actually calculate V star of S. This is just,",
    "start": "5102680",
    "end": "5107825"
  },
  {
    "text": "uh, um, you know, this is just a mathematical expression. This is not an algorithm of how to calculate, right?",
    "start": "5107825",
    "end": "5113260"
  },
  {
    "text": "So this is just giving you a relation between- um, uh, that V star needs to satisfy. So this is a necessary condition",
    "start": "5113260",
    "end": "5120860"
  },
  {
    "text": "that V star needs to satisfy. [NOISE] Uh, if- if V star is the optimal value function, right?",
    "start": "5125110",
    "end": "5132739"
  },
  {
    "text": "So there's a necessary condition. It is not telling you how to calculate, um, um, uh, V star of S and this is the optimal policy corresponding to V star.",
    "start": "5132740",
    "end": "5142925"
  },
  {
    "text": "So now we're gonna look at two algorithms, you know, one of them is- is called value iteration,",
    "start": "5142925",
    "end": "5147950"
  },
  {
    "text": "the other is called policy iteration, that will help us calculate V star of S and,",
    "start": "5147950",
    "end": "5153385"
  },
  {
    "text": "uh, Pi star of S. [NOISE]",
    "start": "5153385",
    "end": "5184775"
  },
  {
    "text": "Policy iteration, that's the next topic. [NOISE] Sorry, not policy iteration, value iteration.",
    "start": "5184775",
    "end": "5193220"
  },
  {
    "text": "[NOISE]",
    "start": "5193220",
    "end": "5207800"
  },
  {
    "text": "So the value iteration is an algorithm,",
    "start": "5207800",
    "end": "5212369"
  },
  {
    "text": "right? [NOISE] So this is an algorithm. It tells us how to compute. It's not just a mathematical relation which is a necessary condition, right?",
    "start": "5217630",
    "end": "5224810"
  },
  {
    "text": "So this just tells us how to compute. So step 1, [NOISE] for each state S,",
    "start": "5224810",
    "end": "5239780"
  },
  {
    "text": "[NOISE] initialize V of S equal to 0.",
    "start": "5239780",
    "end": "5245969"
  },
  {
    "text": "Two, repeat until convergence.",
    "start": "5248590",
    "end": "5256610"
  },
  {
    "text": "[NOISE]",
    "start": "5256610",
    "end": "5262730"
  },
  {
    "text": "For every state S,",
    "start": "5262730",
    "end": "5270695"
  },
  {
    "text": "update V of S equal to R",
    "start": "5270695",
    "end": "5280020"
  },
  {
    "text": "of S plus max",
    "start": "5280020",
    "end": "5286490"
  },
  {
    "text": "a in A sum over Gamma prime,",
    "start": "5286490",
    "end": "5292260"
  },
  {
    "text": "P_sa of S prime, V of S prime.",
    "start": "5292260",
    "end": "5299284"
  },
  {
    "text": "So initialize- initialize, uh, your value function to be 0 everywhere at all states, right?",
    "start": "5299285",
    "end": "5307565"
  },
  {
    "text": "And then, for every state S, update V of S according to this equation.",
    "start": "5307565",
    "end": "5314315"
  },
  {
    "text": "And this equation is the same necessary condition that we have according to,",
    "start": "5314315",
    "end": "5320344"
  },
  {
    "text": "uh, according to the optimal value condition, right? So if- if we have the optimal value, uh,",
    "start": "5320345",
    "end": "5325610"
  },
  {
    "text": "optimal value, then V of S would have satisfied this.",
    "start": "5325610",
    "end": "5332000"
  },
  {
    "text": "But we're gonna use- you know, uh, pretend that, um, um, V Pi of- of- V of S is- is the optimal value.",
    "start": "5332000",
    "end": "5340100"
  },
  {
    "text": "And- and -and -and, uh, by taking that- that, uh, necessary condition, you're gonna construct an update rule.",
    "start": "5340100",
    "end": "5348260"
  },
  {
    "text": "Now, the question is, why will this converge? Will this even converge, right?",
    "start": "5348260",
    "end": "5353810"
  },
  {
    "text": "We- we took a necessary condition, right? It's just a condition, and converted into an update rule where on the right-hand side we have the old value,",
    "start": "5353810",
    "end": "5361804"
  },
  {
    "text": "and you plug it in, you know, and we set it to be, you know, uh, to be equal to the, uh, uh,",
    "start": "5361805",
    "end": "5367370"
  },
  {
    "text": "left-hand side and update V of S. Why is this even expected to converge?",
    "start": "5367370",
    "end": "5372545"
  },
  {
    "text": "And the reason why it- I know the proof is- is beyond the scope,",
    "start": "5372545",
    "end": "5377675"
  },
  {
    "text": "but I can, you know, offer a quick sketch of how- how it works.",
    "start": "5377675",
    "end": "5382260"
  },
  {
    "text": "So this update rule that we have over here is also",
    "start": "5383920",
    "end": "5389420"
  },
  {
    "text": "called the Bellman update operator or the Bellman Backup operator.",
    "start": "5389420",
    "end": "5394489"
  },
  {
    "text": "[NOISE]",
    "start": "5394490",
    "end": "5405050"
  },
  {
    "text": "So the word operator is interesting. That's because, supposing we have S_1 and S_s, right?",
    "start": "5405050",
    "end": "5418159"
  },
  {
    "text": "So you might remember how- how we visualized functions as points in space.",
    "start": "5418160",
    "end": "5423245"
  },
  {
    "text": "You know, we've done it several times in the past, right? So supposing we have a function- we have a policy or- I'm sorry.",
    "start": "5423245",
    "end": "5429710"
  },
  {
    "text": "Supposing we have, um, some, uh, value function, V Pi,",
    "start": "5429710",
    "end": "5435935"
  },
  {
    "text": "that we represent as, you know, a point in a space where each axis corresponds to a different- uh,",
    "start": "5435935",
    "end": "5441005"
  },
  {
    "text": "to a state, right? And V Pi of S is basically the- uh, you know, the coordinate along S1 is V Pi of S1,",
    "start": "5441005",
    "end": "5448090"
  },
  {
    "text": "coordinate along S2 is V Pi S2. You know, the same way we've been, uh, viewing functions as points in space.",
    "start": "5448090",
    "end": "5454960"
  },
  {
    "text": "So supposing this is the value function, and we plug that V Pi of S into this operator on",
    "start": "5454960",
    "end": "5463490"
  },
  {
    "text": "the right-hand side and get something else on the left-hand side, right?",
    "start": "5463490",
    "end": "5469790"
  },
  {
    "text": "We take that V Pi, run it through this operator, and, you know, there- we obtain some output, right?",
    "start": "5469790",
    "end": "5475910"
  },
  {
    "text": "This is the update rule, right? Now, supposing we start here,",
    "start": "5475910",
    "end": "5481565"
  },
  {
    "text": "and this is the, uh, uh- so I'm gonna call this B of V Pi, which means,",
    "start": "5481565",
    "end": "5488255"
  },
  {
    "text": "this is the output of applying the Bellman Backup operator, right?",
    "start": "5488255",
    "end": "5493790"
  },
  {
    "text": "Now, there is a proof that is beyond the scope of this course that this Bellman operator is",
    "start": "5493790",
    "end": "5501785"
  },
  {
    "text": "what is called as- the technical term for it is called a contraction mapping. So what a contraction mapping means is,",
    "start": "5501785",
    "end": "5508745"
  },
  {
    "text": "take any two input points, you know,",
    "start": "5508745",
    "end": "5514385"
  },
  {
    "text": "let's call this V prime of Pi, or let's just call it, um, V Pi, and let's call it, uh,",
    "start": "5514385",
    "end": "5521210"
  },
  {
    "text": "V- V, um, um, V Pi 2. Let's call it V Pi 1 and V Pi 2.",
    "start": "5521210",
    "end": "5528020"
  },
  {
    "text": "And the Bellman update operator- uh, sorry, the Bellman Backup operator,",
    "start": "5528020",
    "end": "5534650"
  },
  {
    "text": "when you apply on the two of them, you get two different B of V Pi 2.",
    "start": "5534650",
    "end": "5543140"
  },
  {
    "text": "And the Bellman opera- uh, update operator is called a contraction mapping because, for any two inputs to the operator,",
    "start": "5543140",
    "end": "5550159"
  },
  {
    "text": "the corresponding outputs will be closer to each other than the corresponding two inputs.",
    "start": "5550160",
    "end": "5556040"
  },
  {
    "text": "So- and- and this can be- this can be, uh, you know, uh, shown mathematically,",
    "start": "5556040",
    "end": "5561725"
  },
  {
    "text": "uh, with- with, you know, some simple rules. You apply this and you can prove that the Bellman operat- uh, the Bellman Backup operator is a contraction mapping.",
    "start": "5561725",
    "end": "5568730"
  },
  {
    "text": "And when something is a contraction mapping, there exists something called as a fixed point.",
    "start": "5568730",
    "end": "5575285"
  },
  {
    "text": "So the intuition there is, for any two- any two points, you- you know, uh, uh,",
    "start": "5575285",
    "end": "5580520"
  },
  {
    "text": "run them through the Bellman operator. You know, you get two- you know, the two output points will always be closer than the two input points.",
    "start": "5580520",
    "end": "5586475"
  },
  {
    "text": "And that holds for any two points in the entire space, right? And when you have such a contraction mapping,",
    "start": "5586475",
    "end": "5593060"
  },
  {
    "text": "the points to which they are all converging to is called a fixed point, right?",
    "start": "5593060",
    "end": "5598535"
  },
  {
    "text": "And it's called a fixed point because, once you reach the fixed point and you apply the Bellman operator on it, you get the same point again, right?",
    "start": "5598535",
    "end": "5605420"
  },
  {
    "text": "And this fixed point is called V star- is- is V star, that's the optimal value function, right?",
    "start": "5605420",
    "end": "5610955"
  },
  {
    "text": "So start with any initialization. We started with 0, right? We started at 0. It does not have to be 0.",
    "start": "5610955",
    "end": "5617885"
  },
  {
    "text": "You start at any other point you want and you keep applying this Bellman Backup operator over and over recursively.",
    "start": "5617885",
    "end": "5625310"
  },
  {
    "text": "You take the- uh, you start from the initialization, apply the operator, you get an output, apply it back into Bellman operator,",
    "start": "5625310",
    "end": "5630905"
  },
  {
    "text": "you get an output, and so on. And we will keep following, you know, this- this kind of a path where,",
    "start": "5630905",
    "end": "5637594"
  },
  {
    "text": "eventually, you're gonna converge to V star. And the reason we are gonna converge to V star is because you",
    "start": "5637595",
    "end": "5643580"
  },
  {
    "text": "can prove that the Bellman Backup operator is what is called, technically, a contraction mapping, right?",
    "start": "5643580",
    "end": "5649040"
  },
  {
    "text": "So it- it- it- it- it kind of contracts the-the entire space towards a single fixed point.",
    "start": "5649040",
    "end": "5655550"
  },
  {
    "text": "Anyway, so the proof is beyond- beyond the scope, but it's good to have this intuition that what's happening over here,",
    "start": "5655550",
    "end": "5662300"
  },
  {
    "text": "this right-hand side, corresponds to this B mapping, this contraction mapping where the entire space is converging towards,",
    "start": "5662300",
    "end": "5669770"
  },
  {
    "text": "uh, V star, right? So this is the value iteration, right?",
    "start": "5669770",
    "end": "5675620"
  },
  {
    "text": "So start with some random initialization. We know P, uh- we know- we know all- all the- all the P values,",
    "start": "5675620",
    "end": "5682010"
  },
  {
    "text": "we know all the R values, and you keep applying this over and over until it converges, right?",
    "start": "5682010",
    "end": "5689165"
  },
  {
    "text": "And when you converge, you would have reached V star and the so-called fixed point.",
    "start": "5689165",
    "end": "5694355"
  },
  {
    "text": "After which, even if you apply the Bellman operator, the output is gonna be the same. Yes, question?",
    "start": "5694355",
    "end": "5700860"
  },
  {
    "text": "So the Bellman operator is defined to be, uh, one iter- one iteration of that,",
    "start": "5701770",
    "end": "5708139"
  },
  {
    "text": "uh, of that last equation that you wrote or is it, like, until convergence? So the Bellman operator takes you one step.",
    "start": "5708140",
    "end": "5716165"
  },
  {
    "text": "That's one iteration of that, one application of that zero? Yes. So one application of this for all the states is one Bellman operator.",
    "start": "5716165",
    "end": "5724264"
  },
  {
    "text": "And- and anywhere in the state, uh, space, it's almost like a convex problem. [OVERLAPPING]",
    "start": "5724265",
    "end": "5731390"
  },
  {
    "text": "Yes, it is a convex one exactly. So it is a convex problem. No matter where you start and you'll keep recursively applying the Bellman operator,",
    "start": "5731390",
    "end": "5737870"
  },
  {
    "text": "you will keep, you know, hopping and hopping and hopping and reach the same fixed point. Yeah. Yeah? All right,",
    "start": "5737870",
    "end": "5743810"
  },
  {
    "text": "so that's- that's, uh, value iteration. And- and- and this proof is beyond the- beyond the scope, but you need to know the,",
    "start": "5743810",
    "end": "5750305"
  },
  {
    "text": "you know, value iteration itself, the algorithm itself. And then there is, uh,",
    "start": "5750305",
    "end": "5756245"
  },
  {
    "text": "another algorithm that's called the policy iteration. [NOISE] And in the notes,",
    "start": "5756245",
    "end": "5766850"
  },
  {
    "text": "in fact, there are two variants of the, uh, value iteration, that's called the synchronous and asynchronous.",
    "start": "5766850",
    "end": "5771860"
  },
  {
    "text": "So in- in- in, uh, in the synchronous, we calculate the- the left-hand sides for all states as a temporary variable,",
    "start": "5771860",
    "end": "5779120"
  },
  {
    "text": "and, you know, replace it in one shot. Whereas, an asynchronous, um, you update it for one variable and use",
    "start": "5779120",
    "end": "5785690"
  },
  {
    "text": "the updated variable for updating the other variables, right? That's, uh, a- a slight variant.",
    "start": "5785690",
    "end": "5791059"
  },
  {
    "text": "And now, uh, we come to the- the second algorithm, called the policy iteration. So in policy iteration,",
    "start": "5791060",
    "end": "5797810"
  },
  {
    "text": "this is a different approach. Here we start with the policy, right? So initialize [NOISE] Pi randomly.",
    "start": "5797810",
    "end": "5808719"
  },
  {
    "text": "[NOISE] As I said, in RL, you have these two central concepts,",
    "start": "5808720",
    "end": "5815070"
  },
  {
    "text": "the value function, and the policy, right? So with this approach, use the value function and I've kept updating the value function,",
    "start": "5815070",
    "end": "5821895"
  },
  {
    "text": "kept hopping the value function. Here we start with a policy, right? The other central concept in RL.",
    "start": "5821895",
    "end": "5827550"
  },
  {
    "text": "And two, uh, so step 2, repeat until convergence [NOISE] a,",
    "start": "5827550",
    "end": "5840540"
  },
  {
    "text": "set V equal to V^Pi and b,",
    "start": "5840540",
    "end": "5854400"
  },
  {
    "text": "for each state, S set,",
    "start": "5855380",
    "end": "5867290"
  },
  {
    "text": "Pi of S equal to argmax in A,",
    "start": "5867290",
    "end": "5878885"
  },
  {
    "text": "sum over S prime, P_sa of S prime times V of S prime.",
    "start": "5878885",
    "end": "5890864"
  },
  {
    "text": "Right? So start with some initial estimate for Pi, right?",
    "start": "5890865",
    "end": "5896565"
  },
  {
    "text": "And probably erased it. We saw how given a Pi we can- we have a close form expression for,",
    "start": "5896565",
    "end": "5904935"
  },
  {
    "text": "uh, calculating V, right? You just solve a linear system of equations. So start with the random Pi,",
    "start": "5904935",
    "end": "5910890"
  },
  {
    "text": "calculate the corresponding value- value function, and then using that value function,",
    "start": "5910890",
    "end": "5916920"
  },
  {
    "text": "update your policy such that you're taking steps where you are maximizing your expected future rewards using the current value function estimate.",
    "start": "5916920",
    "end": "5926880"
  },
  {
    "text": "Okay? And then that gives us a new policy. Using that new policy, update V. And then using the- the updated V,",
    "start": "5926880",
    "end": "5934845"
  },
  {
    "text": "update Pi, and so on. Right? Now, what's the difference between these two methods?",
    "start": "5934845",
    "end": "5941715"
  },
  {
    "text": "So we can make a few observations. So first of all, in value iteration,",
    "start": "5941715",
    "end": "5949860"
  },
  {
    "text": "all we're doing is for each- for each state in the value function,",
    "start": "5949860",
    "end": "5956130"
  },
  {
    "text": "we are performing a max over, um, um, over a S number of, uh, items, right?",
    "start": "5956130",
    "end": "5965010"
  },
  {
    "text": "So you can think of the, um, computational complexity as this will be",
    "start": "5965010",
    "end": "5973770"
  },
  {
    "text": "order S square approximately.",
    "start": "5973770",
    "end": "5980280"
  },
  {
    "text": "Is that right? So for each state, we take a max over.",
    "start": "5980280",
    "end": "5986295"
  },
  {
    "text": "So I think this would be O S squared times A, action space, right?",
    "start": "5986295",
    "end": "5994590"
  },
  {
    "text": "Whereas over here, this step involves inverting,",
    "start": "5994590",
    "end": "6004130"
  },
  {
    "text": "uh, uh, S by S matrix, right? You remember there was an inverse where V equals identity minus",
    "start": "6004130",
    "end": "6013685"
  },
  {
    "text": "Gamma P Pi inverse of R. Right?",
    "start": "6013685",
    "end": "6021410"
  },
  {
    "text": "So this involves inverting an S by S matrix. And so, uh, this is order approximately S cubed.",
    "start": "6021410",
    "end": "6033659"
  },
  {
    "text": "And this- this step is approximately order O of,",
    "start": "6036160",
    "end": "6046085"
  },
  {
    "text": "you know, action space times state space.",
    "start": "6046085",
    "end": "6051364"
  },
  {
    "text": "Right? So for large state spaces, policy iteration can be much more expensive than value iteration.",
    "start": "6051365",
    "end": "6064865"
  },
  {
    "text": "However, with policy iteration, after a finite number of steps,",
    "start": "6064865",
    "end": "6071675"
  },
  {
    "text": "we get the exact optimal value, right?",
    "start": "6071675",
    "end": "6076954"
  },
  {
    "text": "Whereas with this, we keep co- getting closer and closer and, um, uh, we keep getting closer and closer but we- we never reach the exact V star.",
    "start": "6076955",
    "end": "6087085"
  },
  {
    "text": "But with policy iteration after a fixed number of steps, we will recover the exact V star,",
    "start": "6087085",
    "end": "6093480"
  },
  {
    "text": "after some number of steps. Yes, question? Are we converging in terms of V or in terms of policy?",
    "start": "6093480",
    "end": "6099095"
  },
  {
    "text": "So the question is, are we converging in terms of V or in terms of policy? Uh, both- both are,",
    "start": "6099095",
    "end": "6105005"
  },
  {
    "text": "you know, you can use both the conditions. Um, it is easier to check in terms of policy because it is discrete.",
    "start": "6105005",
    "end": "6111260"
  },
  {
    "text": "So how do we know it will never change in the future? As we can just see that in someday it might change if you do more iterations.",
    "start": "6111260",
    "end": "6119200"
  },
  {
    "text": "With this? Yes. So once- once your policy converges, it- it's going to stay converged. If you don't know if it has converged or not?",
    "start": "6119200",
    "end": "6126620"
  },
  {
    "text": "Because we are watching the arrows, like what action you should take? Yes, so- so the way to check convergence here is first, you know, um,",
    "start": "6126620",
    "end": "6133310"
  },
  {
    "text": "make a copy as, you know, Pi prime of the full policy.",
    "start": "6133310",
    "end": "6139445"
  },
  {
    "text": "You know, update the new policy and check if, you know, for every element-wise, does Pi prime equals Pi of S for every S?",
    "start": "6139445",
    "end": "6147469"
  },
  {
    "text": "And if it does, then, you know, your policy hasn't changed at all after a full update. And, you know, then you can declare yourself as converged.",
    "start": "6147470",
    "end": "6155135"
  },
  {
    "text": "Does it take 11 iterations? Yeah, it can take 11 iterations, it can take, you know, some number of iterations, but,",
    "start": "6155135",
    "end": "6161660"
  },
  {
    "text": "um, there is a proof that we're not going to go over it, that this will converge in a finite number of steps.",
    "start": "6161660",
    "end": "6167344"
  },
  {
    "text": "Whereas this has this, you know, uh, uh, this- this contraction mapping will take you closer and closer to an arbitrary,",
    "start": "6167345",
    "end": "6175355"
  },
  {
    "text": "um, um, um, degree of precision of your choice, but you may never actually reach V star.",
    "start": "6175355",
    "end": "6180440"
  },
  {
    "text": "But with this, once your Pi converges, the conversed Pi will be Pi star.",
    "start": "6180440",
    "end": "6187115"
  },
  {
    "text": "And once you have Pi star, you can recover the exact V star. Yes, question?",
    "start": "6187115",
    "end": "6195739"
  },
  {
    "text": "So if we use the value iteration then we can't find the converged value of Pi? So once we reach,",
    "start": "6195740",
    "end": "6202355"
  },
  {
    "text": "or- or if- if you use value iteration and we reach, um, uh, and- and- and we converge to some, you know,",
    "start": "6202355",
    "end": "6209360"
  },
  {
    "text": "V, let's not call it V star, let's call it V prime. Then you can use the V prime and recover the corresponding policy by using V prime here.",
    "start": "6209360",
    "end": "6220940"
  },
  {
    "text": "Right? So this is taking you from value function to policy.",
    "start": "6220940",
    "end": "6226010"
  },
  {
    "text": "Right? And this takes you from policy to value function. So in practice, it uses both matrices?",
    "start": "6226010",
    "end": "6232159"
  },
  {
    "text": "In practice, we will use whether you use policy iteration or value iteration.",
    "start": "6232160",
    "end": "6238340"
  },
  {
    "text": "If our end goal is to recover pol- the optimal policy, we will end up using both these equations,",
    "start": "6238340",
    "end": "6244370"
  },
  {
    "text": "uh, both these equations. In value iteration, we apply- we- we- we use this over and over and in the end,",
    "start": "6244370",
    "end": "6253715"
  },
  {
    "text": "extract the policy according to this equation. And in case of policy iteration,",
    "start": "6253715",
    "end": "6260045"
  },
  {
    "text": "um, we apply this over and over and, uh, the- the- the value function,",
    "start": "6260045",
    "end": "6267275"
  },
  {
    "text": "uh, uh, uh, Pi to V is- is used in step a, and in step b, we extract the, uh, the, uh,",
    "start": "6267275",
    "end": "6272555"
  },
  {
    "text": "policy according to that updated value function. Yes, question? Can you do value iteration then basically [inaudible]",
    "start": "6272555",
    "end": "6279470"
  },
  {
    "text": "step up this to get the V product exactly? Yeah, so the question is, can we- can we run value iteration for",
    "start": "6279470",
    "end": "6286820"
  },
  {
    "text": "some number of iterations and then switch over to a policy iteration. No, it has to converge first.",
    "start": "6286820",
    "end": "6293180"
  },
  {
    "text": "Let us converge then from that converge, we can extract Pi then do more iteration to get the V.",
    "start": "6293180",
    "end": "6298235"
  },
  {
    "text": "Yeah, so if it has mathematically converged to the right answer, then you- performing one value- er, er,",
    "start": "6298235",
    "end": "6305075"
  },
  {
    "text": "policy iteration and ex- uh, extracting, uh, V star will give you the same V star, right?",
    "start": "6305075",
    "end": "6310820"
  },
  {
    "text": "Both- both the algorithms take you to the same V star and same Pi star.",
    "start": "6310820",
    "end": "6315150"
  },
  {
    "text": "But then you don't have the exact V? In this case, after a finite number of steps,",
    "start": "6316390",
    "end": "6321995"
  },
  {
    "text": "you get an exact V star. You know, as soon as your policy stops changing, the corresponding value function is the exact V star.",
    "start": "6321995",
    "end": "6330545"
  },
  {
    "text": "With value iteration, you get arbitrary close to V star, you know,",
    "start": "6330545",
    "end": "6335705"
  },
  {
    "text": "depending on how many steps you have, but you may never get the exact V star by only following this approach.",
    "start": "6335705",
    "end": "6341765"
  },
  {
    "text": "Question. Yes, question. Can you explain like why we should expect them to converge?",
    "start": "6341765",
    "end": "6352390"
  },
  {
    "text": "Like why do you need that V update and V and why- why should it lead to convergence?",
    "start": "6352390",
    "end": "6360140"
  },
  {
    "text": "So the intuition of why it is expected to converge is right here. Right? So think of the Bellman update operator as some kind of a contraction mapping.",
    "start": "6360140",
    "end": "6367020"
  },
  {
    "text": "The proof of why it's a contraction mapping is- is beyond the scope and it is pretty easy to show actually,",
    "start": "6367020",
    "end": "6372715"
  },
  {
    "text": "you can- you can try it on your own. Um, and if you're interested, I can post a- a proof on Piazza.",
    "start": "6372715",
    "end": "6378040"
  },
  {
    "text": "But the Bellman update operator is a contraction mapping, and once you prove it, you know, you get the intuition that, uh,",
    "start": "6378040",
    "end": "6384875"
  },
  {
    "text": "contraction mapping, uh, contraction mappings have a fixed point. And if you keep repeatedly applying that operator over and over to the same input,",
    "start": "6384875",
    "end": "6393860"
  },
  {
    "text": "you know, and then applying it to the output and then applying to its output, it's going to take you towards the fixed point. And that's exactly why this will converge.",
    "start": "6393860",
    "end": "6400670"
  },
  {
    "text": "Now we gonna, uh, cover a few more simple extensions to the policy iteration and value iteration.",
    "start": "6400670",
    "end": "6407960"
  },
  {
    "text": "What is something called as the fitted value function. And we're gonna cover a- a quick overview of",
    "start": "6407960",
    "end": "6413915"
  },
  {
    "text": "the larger reinforcement learning field as a whole. We won't be getting into any other methods.",
    "start": "6413915",
    "end": "6419990"
  },
  {
    "text": "For your homework, these, uh, what we're going to cover, uh, until the middle of next Friday will be sufficient for",
    "start": "6419990",
    "end": "6427010"
  },
  {
    "text": "the reinforcement learning part of whatever is gonna to come in your homework. But the rest of the reinforcement learning will be just for,",
    "start": "6427010",
    "end": "6433025"
  },
  {
    "text": "you know, for your information. [NOISE] All right, thanks. [BACKGROUND]",
    "start": "6433025",
    "end": "6446000"
  }
]