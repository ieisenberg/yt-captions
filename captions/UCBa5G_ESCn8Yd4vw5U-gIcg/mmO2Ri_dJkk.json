[
  {
    "text": " All right, so let's get started.",
    "start": "0",
    "end": "6314"
  },
  {
    "text": " I'd like to bring this back up\nfrom last time really quick.",
    "start": "6315",
    "end": "13060"
  },
  {
    "text": "Remember, we closed\nlast Thursday on-- we were running this. We basically walked\nthrough a case study",
    "start": "13060",
    "end": "18550"
  },
  {
    "text": "of optimizing a program. And if you remember\nthe program, it was this numerical program\nwhere what we did was",
    "start": "18550",
    "end": "24520"
  },
  {
    "text": "we divided the program\ninto various phases, and I'll just page\nit back in for you. I said on phase one,\nwe're going to update",
    "start": "24520",
    "end": "32049"
  },
  {
    "text": "all of these black\ncells on a grid, and we updated every Black cell\nby looking at the neighbors.",
    "start": "32049",
    "end": "38080"
  },
  {
    "text": "And then we said, hold up, wait\ntill everybody does their work. Everybody's got to check\nto see if conversion,",
    "start": "38080",
    "end": "43790"
  },
  {
    "text": "so we're supposed to stop, and\nthen we're going to repeat back, repeat the next iteration\nif we haven't converged.",
    "start": "43790",
    "end": "49430"
  },
  {
    "text": "And the last thing that\nall of you did in class, our last three or four\nminutes of class last time,",
    "start": "49430",
    "end": "54730"
  },
  {
    "text": "was all told me why there were\nthree barriers in this code,",
    "start": "54730",
    "end": "60489"
  },
  {
    "text": "and it's just a quick review. Remember, all of the threads\ncomputed some elements",
    "start": "60490",
    "end": "67580"
  },
  {
    "text": "and computed how much did\nthey change the final output.",
    "start": "67580",
    "end": "73140"
  },
  {
    "text": "And then all of those threads\naccumulated their partial sums into a variable, and then\nif that variable exceeded",
    "start": "73140",
    "end": "80120"
  },
  {
    "text": "some value, if we\nhadn't converged, all the threads just went\nback to the top of the loop",
    "start": "80120",
    "end": "85760"
  },
  {
    "text": "again and continued. And we had a number\nof barriers in there. We said, well, this\nbarrier is here",
    "start": "85760",
    "end": "92060"
  },
  {
    "text": "because all the threads need to\nknow that all other threads have accumulated into this sum,\nso it's the final value",
    "start": "92060",
    "end": "98570"
  },
  {
    "text": "before we check if\nwe're going to continue. That was why we\nhad this barrier. And then we had this barrier\nbecause we were like, well,",
    "start": "98570",
    "end": "105600"
  },
  {
    "text": "wait a minute here. We need to make sure that\neverybody has checked the actual value to determine\nif they should continue",
    "start": "105600",
    "end": "113120"
  },
  {
    "text": "before any thread gets to\nthe top of the loop again and resets those values.",
    "start": "113120",
    "end": "118280"
  },
  {
    "text": "And then there was even a\nthird barrier which says, well, we need to prevent\nanybody from going through this section of\nthe code and actually",
    "start": "118280",
    "end": "124550"
  },
  {
    "text": "updating that value\nperhaps before it got reset, or\nsomething like that, or checked by someone else,\nso there these three barriers.",
    "start": "124550",
    "end": "131700"
  },
  {
    "text": "And I asked, did\nanybody-- and maybe you posted this on the\nwebsite, did anybody come up",
    "start": "131700",
    "end": "136940"
  },
  {
    "text": "with a way to do this with one? ",
    "start": "136940",
    "end": "143105"
  },
  {
    "text": "Nobody puzzled over this\nbecause it would be a good thing to talk about. So let me just give you--",
    "start": "143105",
    "end": "148239"
  },
  {
    "text": "what is the reason why we\nhave to keep waiting here?",
    "start": "148240",
    "end": "153910"
  },
  {
    "text": "There's a variable\nthat we keep checking, and what's that variable? myDiff. That's myDiff.",
    "start": "153910",
    "end": "160160"
  },
  {
    "text": "And what is myDiff do? MyDiff is a single\nvariable that holds",
    "start": "160160",
    "end": "166060"
  },
  {
    "text": "the value of the amount\nof change per iteration. And the problem that\nwe have is that this",
    "start": "166060",
    "end": "172120"
  },
  {
    "text": "is code with\nmultiple iterations, and that my diff value is\nbeing reused over and over",
    "start": "172120",
    "end": "178960"
  },
  {
    "text": "and again to represent\nthe change in each of these iterations.",
    "start": "178960",
    "end": "185230"
  },
  {
    "text": "So the problem is\nthat all these threads have to get out of\niteration i before anybody",
    "start": "185230",
    "end": "191470"
  },
  {
    "text": "can start doing any work with\nmyDiff on iteration i plus 1. ",
    "start": "191470",
    "end": "197909"
  },
  {
    "text": "So why not just have\ndifferent variables for all the different iterations?",
    "start": "197910",
    "end": "203586"
  },
  {
    "text": " If all we do is have\na different variable. If we had myDiff for iteration\ni, everybody in iteration i",
    "start": "203586",
    "end": "212069"
  },
  {
    "text": "would just accumulate into the\niteration i version of myDiff, check that and\nthen you don't have",
    "start": "212070",
    "end": "217897"
  },
  {
    "text": "to wait at all because when you\nmove on to the next iteration, you're starting with\na different variable,",
    "start": "217897",
    "end": "223000"
  },
  {
    "text": "or you're reading and writing\nto a different variable. So this was a case where\nI have kind of falsely created a dependency\nin the program that",
    "start": "223000",
    "end": "229920"
  },
  {
    "text": "didn't need to be there\nbecause I was reusing one variable for a couple\nof different purposes,",
    "start": "229920",
    "end": "235620"
  },
  {
    "text": "every iteration. So a very simple solution\nwould be to go to one barrier",
    "start": "235620",
    "end": "240960"
  },
  {
    "text": "here, and instead take this diff\nvariable and make it an array.",
    "start": "240960",
    "end": "249360"
  },
  {
    "text": "Now, conceptually,\nwhat I've done is, I could have made\na different array of--",
    "start": "249360",
    "end": "254910"
  },
  {
    "text": "diff is an array of size\nnumber of iterations, but really all I\nneed is I needed three different copies of diff.",
    "start": "254910",
    "end": "261540"
  },
  {
    "text": "I need to maintain a diff\nfrom the last iteration, the current iteration,\nand the next iteration",
    "start": "261540",
    "end": "267090"
  },
  {
    "text": "because all threads\nare only going to be in one of those\nthree iterations. They're either finishing\nup from last time,",
    "start": "267090",
    "end": "272940"
  },
  {
    "text": "working on the\npresent, or trying to move forward to the next. So what I did is\nI just said, I'm",
    "start": "272940",
    "end": "279150"
  },
  {
    "text": "just going to duplicate\nthat variable, and make sure that\nthe accumulation goes into the right copy of\nthe variable every time.",
    "start": "279150",
    "end": "287430"
  },
  {
    "text": "So this is like the\nsame idea as what we did right here,\nwhere we had that one",
    "start": "287430",
    "end": "293340"
  },
  {
    "text": "single variable, myDiff. And remember, we made\na local copy of it to accumulate our partial sum?",
    "start": "293340",
    "end": "300810"
  },
  {
    "text": "So that was, again,\nthe same technique. It's like we had one\ncopy of a variable and everybody was\nsynchronizing on it.",
    "start": "300810",
    "end": "306050"
  },
  {
    "text": "So we said,\nconceptually, everybody can write to a different\ncopy of the variable,",
    "start": "306050",
    "end": "311150"
  },
  {
    "text": "and we can get things\nall synced up at the end. So it's interesting,\nit's actually",
    "start": "311150",
    "end": "317949"
  },
  {
    "text": "the same trick just used\nin a different context. ",
    "start": "317950",
    "end": "329850"
  },
  {
    "text": "So at the beginning of\nweek 3, let's finally start talking about some\nprogram optimization techniques.",
    "start": "329850",
    "end": "335650"
  },
  {
    "text": "And the way this\nwork is going to go-- the way this week is\ngoing to go is today, I want to talk about\nassignment or ideas",
    "start": "335650",
    "end": "344169"
  },
  {
    "text": "for how to figure out\nwhat thread or what worker does each piece of work? So today is going to be\nabout workload assignment",
    "start": "344170",
    "end": "352750"
  },
  {
    "text": "and scheduling,\nand then Thursday is going to be about doing that\nscheduling in a manner that",
    "start": "352750",
    "end": "361270"
  },
  {
    "text": "reduces communication\ncosts, reduces stalls due to memory\nand stuff like that. So today, is going\nto be about we're not",
    "start": "361270",
    "end": "367540"
  },
  {
    "text": "going to think much\nabout memory at all, we're just going to\nthink about synchronizing a bunch of workers, and making\nsure everybody has good workload",
    "start": "367540",
    "end": "374470"
  },
  {
    "text": "balance. Thursday, it's going\nto be about let's make sure we're\ncommunicating and accessing memory efficiently.",
    "start": "374470",
    "end": "380800"
  },
  {
    "text": "OK, so here's where\nwe're going with this. And yeah, my first\ndisclaimer before we",
    "start": "380800",
    "end": "390130"
  },
  {
    "text": "get into any of\nthe work this week, and even next week,\nwe're going to now start telling you about strategies for\nscheduling programs efficiently.",
    "start": "390130",
    "end": "399430"
  },
  {
    "text": "And I cannot overemphasize and\ncase 1 being your assignments. And in particular\nthis is going to be",
    "start": "399430",
    "end": "405009"
  },
  {
    "text": "true in assignment\n3 is people go, oh, I heard about\nthese complex--",
    "start": "405010",
    "end": "410275"
  },
  {
    "text": "all these ideas in lecture. I'm going to think\nabout the problem that they give me on my\nprogramming assignments,",
    "start": "410275",
    "end": "416090"
  },
  {
    "text": "and I'm going to figure out\nhow to apply some of the more advanced techniques.",
    "start": "416090",
    "end": "421150"
  },
  {
    "text": "And without question, I want you\nto take a different approach. I want you to take the\nsimplest possible approach that",
    "start": "421150",
    "end": "429160"
  },
  {
    "text": "gets the program working,\nand then the simplest possible approach\nto paralyze it,",
    "start": "429160",
    "end": "435310"
  },
  {
    "text": "and I want you to\nmeasure your performance. I want you to do\nthe types of things that we forced you to\ndo on assignment 1.",
    "start": "435310",
    "end": "441160"
  },
  {
    "text": "Put in some timers,\nfigure out which thread is doing the most\nwork, those kinds of things,",
    "start": "441160",
    "end": "446270"
  },
  {
    "text": "and then only if performance is\nbad do you start doing something more sophisticated.",
    "start": "446270",
    "end": "452280"
  },
  {
    "text": "And this is the case. This happens every\nyear to, I would say, about 10% of students\nin the class.",
    "start": "452280",
    "end": "457680"
  },
  {
    "text": "They read the handout,\nthey read the lectures, they get on the whiteboard,\nand they come up with a pretty sophisticated\nscheme to solve the assignment,",
    "start": "457680",
    "end": "465660"
  },
  {
    "text": "and they do that\nfor multiple days. And then they say\nthree or four days before the assignment\nis due, they're like,",
    "start": "465660",
    "end": "471090"
  },
  {
    "text": "we're just going to code it up. As soon as we can get it\nto work, it'll be great. And it turns out\noften that there's complex scheme is slower\nthan a simple scheme.",
    "start": "471090",
    "end": "480440"
  },
  {
    "text": "So always do the simple thing\nfirst and use measurements, use data to govern\nwhat you do next.",
    "start": "480440",
    "end": "488180"
  },
  {
    "text": "OK, so I'm going to start\nwith the simplest thing here. And the next sequence\nof slides is going",
    "start": "488180",
    "end": "494150"
  },
  {
    "text": "to all be about balancing work. And the name of the game when\nyou need to balance work--",
    "start": "494150",
    "end": "499889"
  },
  {
    "text": "here's an example where I\nhave four processor cores, and for whatever reason, one of\nthese cores processor 4 here,",
    "start": "499890",
    "end": "506009"
  },
  {
    "text": "core 4 does a lot more\nwork than the others. And the reason why that\nlimits speed up is we",
    "start": "506010",
    "end": "511310"
  },
  {
    "text": "have effectively serialized\nparts of our program. A large part of execution\ntime, only one of these workers",
    "start": "511310",
    "end": "517880"
  },
  {
    "text": "is doing anything,\nit is effectively running as a serial program.",
    "start": "517880",
    "end": "523520"
  },
  {
    "text": "So if you have a bunch of work,\nand you have a bunch of cores, or threads, whatnot, what\nis probably the simplest way",
    "start": "523520",
    "end": "531590"
  },
  {
    "text": "to divide work\nonto those workers? Or in other words,\nwhere do we start",
    "start": "531590",
    "end": "537980"
  },
  {
    "text": "on program 1 of assignment 1? Just give each of them\na contiguous chunk of the workload. Just give everybody an equal.",
    "start": "537980",
    "end": "544930"
  },
  {
    "text": "I'm not even going to think\nabout what's contiguous or not, but we did something like this. We said, we're just\ngoing to break up",
    "start": "544930",
    "end": "551060"
  },
  {
    "text": "the work completely evenly. Now, we need to dig in a little\nbit on what we mean by even.",
    "start": "551060",
    "end": "558230"
  },
  {
    "text": "So on program 1, evenly\nmeant same number of pixels,",
    "start": "558230",
    "end": "565260"
  },
  {
    "text": "and so here are a couple\nof different strategies",
    "start": "565260",
    "end": "570920"
  },
  {
    "text": "for breaking that work up. So this was the original\nversion of assignment 1.",
    "start": "570920",
    "end": "577390"
  },
  {
    "text": "If these were actually\npixel sized rows, you might have ended\nup with a solution that looked a little bit like this.",
    "start": "577390",
    "end": "583690"
  },
  {
    "text": "If I didn't make the\nconstraint that said, your solution should probably\nwork for any viewpoint,",
    "start": "583690",
    "end": "589990"
  },
  {
    "text": "you might have done something\nlike this where you just manually said, OK, I know what\nthe expense of every pixel was,",
    "start": "589990",
    "end": "599990"
  },
  {
    "text": "and therefore I divided\nup the work equally. Now, in these\ndiagrams, actually,",
    "start": "599990",
    "end": "606290"
  },
  {
    "text": "how do I know how much work\nis associated with the pixel? One of the reasons why we use\nthis as a starting example",
    "start": "606290",
    "end": "612192"
  },
  {
    "text": "is that it's pretty clear. Is it the brightness? Yeah, the brightness\nof the pixel is kind of how expensive\nit is to compute.",
    "start": "612192",
    "end": "618230"
  },
  {
    "text": "So the regions down here\nare not a lot of work. The regions in here\nare a lot of work.",
    "start": "618230",
    "end": "624139"
  },
  {
    "text": "And as a result, this scheme\nyielded a significant amount of workload imbalance.",
    "start": "624140",
    "end": "631264"
  },
  {
    "text": "Now, some of you decided\nto come up with a solution where you interleaved\nthe assignment of pixels,",
    "start": "631265",
    "end": "639710"
  },
  {
    "text": "rows of pixels to the\noutput or to the threads.",
    "start": "639710",
    "end": "645255"
  },
  {
    "text": "And why is that\nactually an OK solution? Because each of those rows\nhas different amounts of work,",
    "start": "645255",
    "end": "652149"
  },
  {
    "text": "for sure. Different regions of the image\nhave very different amounts of work, so why did that\nactually work out pretty well?",
    "start": "652150",
    "end": "659475"
  },
  {
    "text": " Yeah. Because on average, one large--",
    "start": "659475",
    "end": "666800"
  },
  {
    "text": "one thread getting cited with\nmost of the work [? decreased ?] because they're [INAUDIBLE]? So yeah, and we're\nkind of assuming",
    "start": "666800",
    "end": "672980"
  },
  {
    "text": "that rows right next\nto each other kind of have the same amount\nof work because there's some continuity to the figure.",
    "start": "672980",
    "end": "679620"
  },
  {
    "text": "So if every really\nlocal piece of the image gets divided up into\nfour rows and go",
    "start": "679620",
    "end": "684870"
  },
  {
    "text": "into different\nprocessors, that means all the processors on\naverage will end up having a reasonable\namount of work.",
    "start": "684870",
    "end": "691020"
  },
  {
    "text": "Now, you could imagine\na different workload where that would have\nbeen a terrible strategy.",
    "start": "691020",
    "end": "697949"
  },
  {
    "text": "For example, imagine it\nwas all white at the top, and then slowly went down this\nway or something like that.",
    "start": "697950",
    "end": "704240"
  },
  {
    "text": "The top half was a triangle. That would mean that\nprocessor or thread 1 would always get a little bit\nmore work than thread 2, which",
    "start": "704240",
    "end": "711410"
  },
  {
    "text": "would always get a little\nbit more work than thread 3, but in this case, it actually\nturned out to be pretty good. So what are some\nsituations where you",
    "start": "711410",
    "end": "719540"
  },
  {
    "text": "think you can pull off this? And the title of the slide\nis static assignment.",
    "start": "719540",
    "end": "724810"
  },
  {
    "text": "In other words, you\ncan go, without ever like actually looking at the\nexecution characteristics",
    "start": "724810",
    "end": "729900"
  },
  {
    "text": "of the program, I can\njust say, if you give me the size of the image, if you\ngive me the number of workers I have, I'll give\nyou a scheme that's",
    "start": "729900",
    "end": "736290"
  },
  {
    "text": "always going to be pretty good. So what are the properties\nof this that we knew about?",
    "start": "736290",
    "end": "743237"
  },
  {
    "text": "I guess you have to know\nwhat parts of the program are going to be more\nintensive than other parts.",
    "start": "743237",
    "end": "748350"
  },
  {
    "text": "This would have been very\neasy if all pixels were the same cost. So you have to have some ability\nto predict the cost of things.",
    "start": "748350",
    "end": "756970"
  },
  {
    "text": "In this particular case,\nyou actually can't really predict the cost\nof any one pixel,",
    "start": "756970",
    "end": "762370"
  },
  {
    "text": "but you definitely can predict\non average that on average stuff",
    "start": "762370",
    "end": "768000"
  },
  {
    "text": "is going to be about the same. So there's this notion\nof predictability that kind of makes this\nproblem pretty easy, right?",
    "start": "768000",
    "end": "776580"
  },
  {
    "text": "You can just say, well,\nif I have 8 threads or so many threads, I\nhave this many pixels, here's a way I can just\ndescribe the policy up front",
    "start": "776580",
    "end": "784136"
  },
  {
    "text": "for who does what, and I'm going\nto get a pretty good workload balance. I'm going to schedule\nthis out onto the machine.",
    "start": "784137",
    "end": "791500"
  },
  {
    "text": "So this is really\napplicable when",
    "start": "791500",
    "end": "796545"
  },
  {
    "text": "the cost of the chunks of work,\nand here the cost of the work is maybe the vertical height\nof these bars is equal.",
    "start": "796545",
    "end": "802920"
  },
  {
    "text": "If I know I have\n12 pieces of work, and they all have\nequal time, and I have four different workers,\nfour processors, sure,",
    "start": "802920",
    "end": "808810"
  },
  {
    "text": "I might just divide\n12 by 3, or 12 by 4, and I get three pieces\nof work per worker.",
    "start": "808810",
    "end": "815430"
  },
  {
    "text": "Really simple way of doing it. And this is great because\nonce I tell P1 and P2,",
    "start": "815430",
    "end": "822150"
  },
  {
    "text": "you're responsible\nfor the first quarter, you're responsible for\nthe second quarter, these threads don't\nhave to synchronize.",
    "start": "822150",
    "end": "828010"
  },
  {
    "text": "There's no communication. They can compute\nwhat they have to do without knowing what\nanybody else is doing,",
    "start": "828010",
    "end": "833820"
  },
  {
    "text": "and that's very helpful because\nthat minimizes communication amongst the threads.",
    "start": "833820",
    "end": "839579"
  },
  {
    "text": "Static assignment, as we\nsaw in that fractal example, can also be applicable\nwhen you don't know the actual cost of\nany one piece of work,",
    "start": "839580",
    "end": "846760"
  },
  {
    "text": "but you know that on average,\nyou're going to end up with-- on average, everything\nis the same.",
    "start": "846760",
    "end": "853050"
  },
  {
    "text": "You might have some variance. And if I give enough\njobs to every worker, if I add up the length\nof the dark blue bars,",
    "start": "853050",
    "end": "859920"
  },
  {
    "text": "and plot them like\nthis, and stack them, all the different\nthreads end up getting about the same amount\nof work, so here's",
    "start": "859920",
    "end": "865890"
  },
  {
    "text": "an example where on average\neverything is the same. And just because\nthere's variance, that doesn't mean that I\ndon't get a good workload",
    "start": "865890",
    "end": "873389"
  },
  {
    "text": "balance from a simple just\nchop it all up strategy, and like that fractal\nwas a good example.",
    "start": "873390",
    "end": "883182"
  },
  {
    "text": "There are a lot of\napplications, especially if you-- anything that\nruns for a long time,",
    "start": "883182",
    "end": "889800"
  },
  {
    "text": "let's say you're a\nsupercomputing simulation, this is a figure from\na turbulent simulation.",
    "start": "889800",
    "end": "897730"
  },
  {
    "text": "What you see here in white is\nactually the profile of a wing, so you're actually looking\ndown at the edge of the wing,",
    "start": "897730",
    "end": "904170"
  },
  {
    "text": "towards the fuselage. And this is a mesh\nthat's discretized all of the fluid\nflow around the wing,",
    "start": "904170",
    "end": "911320"
  },
  {
    "text": "and so you could see that\nsuper high resolution around here, that's actually\nbecause you need high resolution around areas of high pressure.",
    "start": "911320",
    "end": "917860"
  },
  {
    "text": "And the coloring suggests\nwhat cells of the mesh are processed by what\nthread, so different colors.",
    "start": "917860",
    "end": "924490"
  },
  {
    "text": "So in space there are\nvery different amounts of space per thread, but\nprobably if we look carefully",
    "start": "924490",
    "end": "931290"
  },
  {
    "text": "and counted the number\nof cells per thread, it'd probably be about the same. And you can imagine\nthat if we're",
    "start": "931290",
    "end": "938070"
  },
  {
    "text": "simulating this airline, this\nwing for a long period of time,",
    "start": "938070",
    "end": "943450"
  },
  {
    "text": "maybe during\ntakeoff, or landing, or as the wing geometry changed,\nthe distribution might change.",
    "start": "943450",
    "end": "951720"
  },
  {
    "text": "So it would be perfectly\nfine if, for example, we decided to go with a static\nassignment of grid cells",
    "start": "951720",
    "end": "959160"
  },
  {
    "text": "to the threads at the\nbeginning of the program. Maybe we let it run\nfor a minute or two,",
    "start": "959160",
    "end": "964620"
  },
  {
    "text": "and then maybe the\npressure changes, and we have to update\nthe number of cells, and then we do another\nassignment again.",
    "start": "964620",
    "end": "971220"
  },
  {
    "text": "So I like to-- there's this\nterm semi-static assignment, as you can think about it as,\ngiven the state of the world,",
    "start": "971220",
    "end": "977440"
  },
  {
    "text": "I'm going to upfront\nassign work to workers, and then I'm not going\nto change that assignment",
    "start": "977440",
    "end": "982950"
  },
  {
    "text": "for some amount of time. So I just want people\nto not walk away going, static assignment means\nit's known at compile time",
    "start": "982950",
    "end": "990600"
  },
  {
    "text": "when you write the program\nor something like that. OK. Another good one would be\nany kind of long running",
    "start": "990600",
    "end": "996120"
  },
  {
    "text": "machine learning computation. You might run that\ntraining for a while, understand where you have\nsome workload imbalance,",
    "start": "996120",
    "end": "1003330"
  },
  {
    "text": "adjust that workload\nimbalance, and then run again for another hour,\nand so on, and so on.",
    "start": "1003330",
    "end": "1010115"
  },
  {
    "text": "Now, the flip side\nof static assignment would be to do the assignment\ndynamically as we go.",
    "start": "1010116",
    "end": "1017800"
  },
  {
    "text": "So let's say that we cannot\npredict the amount of time that a task might take. It might be very short.",
    "start": "1017800",
    "end": "1023662"
  },
  {
    "text": "It might be very, very long,\nso there's not really any way we can upfront say, OK, you\nare going to do all these",
    "start": "1023662",
    "end": "1028959"
  },
  {
    "text": "and you are going\nto do all these. OK, so here's an\nexample program. I wrote it in just\nkind of pseudocode.",
    "start": "1028960",
    "end": "1036949"
  },
  {
    "text": "Imagine there's a\nvoid main, and then here's a sequential program\nfor i equals 0 to n.",
    "start": "1036950",
    "end": "1043670"
  },
  {
    "text": "I want to compute whether or not\nsome number in the input array is prime, which is\nsomething that's",
    "start": "1043670",
    "end": "1049360"
  },
  {
    "text": "a little bit harder to predict\nhow expensive that would be, for example. And now imagine that I have\na version of this code that",
    "start": "1049360",
    "end": "1056740"
  },
  {
    "text": "runs in an SPMD fashion, not\nnecessarily written in ISPC,",
    "start": "1056740",
    "end": "1063710"
  },
  {
    "text": "but it runs in the SPMD\nprogramming model, which means however many threads\nI'm running in my program,",
    "start": "1063710",
    "end": "1069900"
  },
  {
    "text": "they're all running\nthis code, and they just have a different thread\nID or something like that.",
    "start": "1069900",
    "end": "1075470"
  },
  {
    "text": "So imagine that in the\nparallel version of this code, there are n threads\nrunning this code.",
    "start": "1075470",
    "end": "1082130"
  },
  {
    "text": "And the way my code works\nis all the threads continue. They just do their thing until\nall elements of the array",
    "start": "1082130",
    "end": "1090650"
  },
  {
    "text": "have been processed. And the key thing here\nis there's a counter, and there's a lock\non that counter.",
    "start": "1090650",
    "end": "1098149"
  },
  {
    "text": "So the counter starts\nat 0 and counts up to n, and then every thread says,\nwell, if I have nothing to do,",
    "start": "1098150",
    "end": "1104910"
  },
  {
    "text": "grab the lock,\nincrement the counter, and then whatever the\ncounter's value is, I'm",
    "start": "1104910",
    "end": "1110090"
  },
  {
    "text": "responsible for that number. So first of all, let's\ncheck to make sure that do all n elements of\nthe array get processed?",
    "start": "1110090",
    "end": "1123419"
  },
  {
    "text": "Yes, it would be bad if two\nthreads did the same work. Is there any way for two\nthreads to do the same work?",
    "start": "1123420",
    "end": "1131580"
  },
  {
    "text": "No, and what ensures\nthat that's the case?  This lock here.",
    "start": "1131580",
    "end": "1137730"
  },
  {
    "text": "So this lock says that every-- no one else is going to\nbe reading that counter",
    "start": "1137730",
    "end": "1143010"
  },
  {
    "text": "while I'm incrementing it. So I'm guaranteed\nthat I will perform that update,\nincrement the counter,",
    "start": "1143010",
    "end": "1148150"
  },
  {
    "text": "I'll take the current number. It's like basically a ticket at\nthe deli counter or something like that, and\nthen everybody come",
    "start": "1148150",
    "end": "1154230"
  },
  {
    "text": "in later that looks for more\nwork will get a later number, and we're all going to get\nunique numbers, and as a result,",
    "start": "1154230",
    "end": "1159549"
  },
  {
    "text": "we're going to do all the work. And does this code terminate?",
    "start": "1159550",
    "end": "1164790"
  },
  {
    "text": "It will terminate because if a\nthread fails to get a number, it will quit. And once all threads\nhave quit, we are done.",
    "start": "1164790",
    "end": "1174059"
  },
  {
    "text": "So even though this is\nsome basically some C code with a lock and a counter,\nwhat have I really done here?",
    "start": "1174060",
    "end": "1180340"
  },
  {
    "text": "If you were trying to explain\nthis in high level terms on what is the work\nassignment strategy?",
    "start": "1180340",
    "end": "1187570"
  },
  {
    "text": "Yeah. It's because we use whichever\nthread is available, and just give it more,\nand we keep doing that.",
    "start": "1187570",
    "end": "1192650"
  },
  {
    "text": "Yeah. So how does this program\ndefine the work to do? Basically, it defines\nit by whichever",
    "start": "1192650",
    "end": "1201159"
  },
  {
    "text": "way is available to it. That's how it's going to be--\nthat defines how a thread is",
    "start": "1201160",
    "end": "1206650"
  },
  {
    "text": "going to get assigned to work,\nbut I want-- what I'm looking for here is when you read\nthis program, first of all,",
    "start": "1206650",
    "end": "1212260"
  },
  {
    "text": "how would you-- if you looked\nat this program and talk to your neighbor sitting right\nnext to you, you would say like, what's the definition\nof the word here?",
    "start": "1212260",
    "end": "1218755"
  },
  {
    "text": " The work is to\ncompute the primality of all of these numbers.",
    "start": "1218755",
    "end": "1225254"
  },
  {
    "text": "And a single piece\nof work is what? Computing the primality\nof one number.",
    "start": "1225254",
    "end": "1231554"
  },
  {
    "text": "And how does this program define\nall the work we need to do? Basically, it defines\nit via the current--",
    "start": "1231554",
    "end": "1238720"
  },
  {
    "text": "basically the number n, the\narray and the number of n. So basically I've\nsaid, look, there's",
    "start": "1238720",
    "end": "1243850"
  },
  {
    "text": "a whole array of\nstuff to do, and we're going to divvy up\nthat array by indices.",
    "start": "1243850",
    "end": "1250360"
  },
  {
    "text": "And the way it's\ngoing to work is we're all just going to\nkeep grabbing the next index that nobody else has done yet\nuntil they've all been done.",
    "start": "1250360",
    "end": "1258020"
  },
  {
    "text": "And you might even want\nto say effectively, even though you don't see a\nqueue data structure or anything",
    "start": "1258020",
    "end": "1263110"
  },
  {
    "text": "here, effectively\nI've thrown all of the things I need\nto do into a big queue, and I've implemented the queue\nby an array and a counter.",
    "start": "1263110",
    "end": "1272750"
  },
  {
    "text": "So conceptually this is kind\nof I have all this work, I threw all the work\nin a shared work queue,",
    "start": "1272750",
    "end": "1278890"
  },
  {
    "text": "and I have a bunch of worker\nthreads going to the queue and say, pop me the next\none, pop me the next one.",
    "start": "1278890",
    "end": "1283980"
  },
  {
    "text": "Now in this case, I have a\nvery efficient implementation of that queue because I know all\nmy data is stored in this array",
    "start": "1283980",
    "end": "1289580"
  },
  {
    "text": "already, and I can implement-- going back to the thing,\nI can implement queue pop",
    "start": "1289580",
    "end": "1296990"
  },
  {
    "text": "by essentially just an atomic\nincrement of that counter. So I'd like to make\nsure everybody--",
    "start": "1296990",
    "end": "1302720"
  },
  {
    "text": "does that make sense? Conceptually, that's just a work\nqueue, and so on, and so on. OK.",
    "start": "1302720",
    "end": "1308390"
  },
  {
    "text": "So this is like your\ncanonical dynamic assignment thing is you just expose all of\nthe potentially parallel work",
    "start": "1308390",
    "end": "1316310"
  },
  {
    "text": "that can be done in\nparallel, and then you let the underlying system\ndynamically assign work",
    "start": "1316310",
    "end": "1322460"
  },
  {
    "text": "to your workers, to your threads\nbased on if anybody goes idle, they should probably\ndo more work.",
    "start": "1322460",
    "end": "1327580"
  },
  {
    "start": "1327580",
    "end": "1332971"
  },
  {
    "text": "Now, yeah, in program\n3, a lot of people",
    "start": "1332972",
    "end": "1341920"
  },
  {
    "text": "ask me this question about when\nI created these ICPC tasks,",
    "start": "1341920",
    "end": "1352910"
  },
  {
    "text": "shouldn't I just\ncreate eight tasks? I'm on a machine\nwith eight threads,",
    "start": "1352910",
    "end": "1359736"
  },
  {
    "text": "and people were\nlike, why the heck do I need to create more tasks?",
    "start": "1359736",
    "end": "1364920"
  },
  {
    "text": "Because it's imperative,\nnot imperative, so you're saying that\nyou can distribute it",
    "start": "1364920",
    "end": "1371420"
  },
  {
    "text": "into this many different\nparallel units of execution, and then the emitted\ncode can decide",
    "start": "1371420",
    "end": "1378200"
  },
  {
    "text": "to do that parallel to up to\nthat extent, whenever it wants.",
    "start": "1378200",
    "end": "1383510"
  },
  {
    "text": "And so if you just cut\nthis thing into tasks, and you cut it into\neight tasks, or let's",
    "start": "1383510",
    "end": "1390380"
  },
  {
    "text": "just say you had a\ncomputer with four cores, haven't you just divided\nthe work up into this?",
    "start": "1390380",
    "end": "1397169"
  },
  {
    "text": "And what do we know that's\nwrong about this scheme? You're talking about-- It's really unbalanced.",
    "start": "1397170",
    "end": "1403290"
  },
  {
    "text": "Now, what if instead I cut\nthis up into a bunch of pieces? Not that scheme\nover there, but just into a bunch of smaller\npieces, and then I",
    "start": "1403290",
    "end": "1412170"
  },
  {
    "text": "throw all those pieces\nin a work queue. Those pieces are all\nof different cost,",
    "start": "1412170",
    "end": "1418529"
  },
  {
    "text": "but why am I going to\nend up in a good place?  Because the program is better\nable to distribute the work.",
    "start": "1418530",
    "end": "1425846"
  },
  {
    "text": "You don't have the issue-- Because something like\nthat might happen, right? And as long as I have\nenough pieces of work,",
    "start": "1425847",
    "end": "1431745"
  },
  {
    "text": "and they're\nrelatively short, I'm just-- all those\nthreads are just going to keep taking the next\nthing until they're ready,",
    "start": "1431745",
    "end": "1437393"
  },
  {
    "text": "until everything is done. So what's the cost of this--",
    "start": "1437393",
    "end": "1442570"
  },
  {
    "text": "where am I? Of this dynamic scheme? ",
    "start": "1442570",
    "end": "1450160"
  },
  {
    "text": "Let's say under the hood I\nreplace n to be number of tasks,",
    "start": "1450160",
    "end": "1456880"
  },
  {
    "text": "and is prime to test\nprimality as run task i. What is the cost of the\ndynamic allocation here?",
    "start": "1456880",
    "end": "1464260"
  },
  {
    "text": " What do you not have to do\nin programming assignment 1",
    "start": "1464260",
    "end": "1471600"
  },
  {
    "text": "or program 1? We have to use them both, so-- You don't have to synchronize?",
    "start": "1471600",
    "end": "1476860"
  },
  {
    "text": "Yeah. Right. So the question is, is the good\nworkload balance, the benefits",
    "start": "1476860",
    "end": "1483210"
  },
  {
    "text": "of that, compared\nto the cost of this, does it overcome the\npotential workload imbalance",
    "start": "1483210",
    "end": "1490230"
  },
  {
    "text": "that you might have if you\nused a static allocation? And it turns out that in\nthis case, it certainly did.",
    "start": "1490230",
    "end": "1497500"
  },
  {
    "text": "OK. So that kind of gets to what\nconstitutes a piece of work? So in this program,\nthe unit of scheduling",
    "start": "1497500",
    "end": "1506340"
  },
  {
    "text": "or the unit of\nassignment is what? It's one number, right? And so if I plot this\nthing out, let's just",
    "start": "1506340",
    "end": "1515490"
  },
  {
    "text": "think about this being time,\nand the vertical blue bars are actually-- imagine this\nis a plot of one thread,",
    "start": "1515490",
    "end": "1523200"
  },
  {
    "text": "and the blue bars\nare time that it's spent testing primality\nor running the ISGC task",
    "start": "1523200",
    "end": "1528730"
  },
  {
    "text": "or something like that. And then the white bars are\ntime spent grabbing the lock,",
    "start": "1528730",
    "end": "1534110"
  },
  {
    "text": "incrementing that counter,\nand getting out of the lock. So the question is\ndo we have a problem?",
    "start": "1534110",
    "end": "1544065"
  },
  {
    "text": " How would you know?",
    "start": "1544065",
    "end": "1550789"
  },
  {
    "text": "Yeah. It would depend on\nhow long it takes to do the synchronization\ncompared to how long it takes to do each task.",
    "start": "1550790",
    "end": "1556440"
  },
  {
    "text": "Yeah, another way to think about\nit would be, imagine I did this. I gave you this\nprogram and I said, I",
    "start": "1556440",
    "end": "1564050"
  },
  {
    "text": "want you to make it go as\nfast as you can, are you done? What would be your next\nstep in thinking about this?",
    "start": "1564050",
    "end": "1569659"
  },
  {
    "text": "I gave you this program and I\nsaid, are you done or is there something better you could do? How would we go about\neven figuring out",
    "start": "1569660",
    "end": "1577279"
  },
  {
    "text": "the answer to that question? OK, good to start. I want to talk through\nthis debugging exercise. We would first get\ntimings for this program.",
    "start": "1577280",
    "end": "1584263"
  },
  {
    "text": "OK, so let's say\nthe first thing I do is I put a timer at\nthe beginning, at the end, and it says it took 5.9 seconds.",
    "start": "1584263",
    "end": "1591510"
  },
  {
    "text": "OK. I put a timer in the program. And then applying the\nserial potion of it",
    "start": "1591510",
    "end": "1596703"
  },
  {
    "text": "to see what percentage\nof the program time is spent in the lock or waiting\nfor the lock, as opposed to--",
    "start": "1596703",
    "end": "1601710"
  },
  {
    "text": "Yeah. So the next thing\nI would do is I would put my timers right\naround test primality,",
    "start": "1601710",
    "end": "1609230"
  },
  {
    "text": "and I would compute this. And then the difference between\nthe timer around the whole thing",
    "start": "1609230",
    "end": "1615560"
  },
  {
    "text": "and the sum of all of the\ntimings of test primality is the overhead.",
    "start": "1615560",
    "end": "1620990"
  },
  {
    "text": "I could also put my timer around\nthe lock and do the opposite, but you get the point. So if I know how much time\nmy program takes, and I",
    "start": "1620990",
    "end": "1629210"
  },
  {
    "text": "know how much time I've\nspent doing useful work, what do I do next?",
    "start": "1629210",
    "end": "1634700"
  },
  {
    "text": "I've got a lot of\nuseful information now. You basically want to\ncompare it with if you were to do this assignment,\nhow much time did it stay?",
    "start": "1634700",
    "end": "1643530"
  },
  {
    "text": "Yeah. There's a couple of\nquestions I could have. So first of all, I could\nimplement the static version of the program, and hopefully.",
    "start": "1643530",
    "end": "1648580"
  },
  {
    "text": "I might have done that first. And I said, well,\nthat's the speed I got. It was like, it's like 8\nseconds, so I'm like, OK.",
    "start": "1648580",
    "end": "1655890"
  },
  {
    "text": "I'm already like 5.9 seconds. I'm doing better. But now my question is,\ncan I do even better",
    "start": "1655890",
    "end": "1661039"
  },
  {
    "text": "than the 5.9 seconds? And the information I have\nis the total 5.9 seconds,",
    "start": "1661040",
    "end": "1666390"
  },
  {
    "text": "and let's say that\nI end up saying I spend 5.75 seconds in all\nof my calls to test primality.",
    "start": "1666390",
    "end": "1675570"
  },
  {
    "text": "What do I conclude? ",
    "start": "1675570",
    "end": "1682000"
  },
  {
    "text": "You probably can't\nreally do better. Yeah. You're like, well, I\ngave it 5.9 seconds is the whole\ncomputation, 5.75 seconds",
    "start": "1682000",
    "end": "1689950"
  },
  {
    "text": "is like all the useful work. The only thing I could do is\nshave off 0.1 or something",
    "start": "1689950",
    "end": "1695169"
  },
  {
    "text": "like that, and then I decide,\nif I'm a high frequency trader, maybe I have to win that\nrace, and maybe I go for it.",
    "start": "1695170",
    "end": "1701390"
  },
  {
    "text": "But if I'm working on an\nassignment, I say, screw it, I'm going on to the next class. There's not much more I can do.",
    "start": "1701390",
    "end": "1707020"
  },
  {
    "text": "Exactly. Now, if this timer around\ntest primality says, all calls to test primality\nare total maybe 2.5 seconds,",
    "start": "1707020",
    "end": "1716620"
  },
  {
    "text": "now I'm like, well, wait a\nminute here, more than half of my time is probably\nbeing spent elsewhere.",
    "start": "1716620",
    "end": "1722050"
  },
  {
    "text": "And since this\nprogram is so simple, I know exactly where\nit's being spent. It's being spent in this lock.",
    "start": "1722050",
    "end": "1727320"
  },
  {
    "text": "So what do I do next? Let's say my timers tell\nme, over 50% of the time",
    "start": "1727320",
    "end": "1733929"
  },
  {
    "text": "is spent in this lock. And I'm like, yeah,\nI wouldn't mind",
    "start": "1733930",
    "end": "1739218"
  },
  {
    "text": "trying to make this\nthing two times faster. ",
    "start": "1739218",
    "end": "1745820"
  },
  {
    "text": "Yeah. Maybe you could try a static\nassignment scheme for-- OK. So one conclusion like let's\nsay this was my first program,",
    "start": "1745820",
    "end": "1752587"
  },
  {
    "text": "and this would probably\nbe a really simple program to write first. You could say, maybe I don't\nneed this dynamic mechanism.",
    "start": "1752587",
    "end": "1760040"
  },
  {
    "text": "Maybe I should just back off\nand do a static assignment and remove that.",
    "start": "1760040",
    "end": "1765260"
  },
  {
    "text": "That would be a--\npotentially, that would be a very viable strategy. There's a few simpler\nthings I might",
    "start": "1765260",
    "end": "1770750"
  },
  {
    "text": "try first that don't require\nme to rewrite everything. Yes sir. Do you sign more\nthan one party number",
    "start": "1770750",
    "end": "1776750"
  },
  {
    "text": "every time they get along? OK. So what we did here\nis we said, we're going to break this work\nup into these smallest",
    "start": "1776750",
    "end": "1784340"
  },
  {
    "text": "pieces that are possible. Maybe OK, I'm sure you could\nparallelize inside the test",
    "start": "1784340",
    "end": "1789860"
  },
  {
    "text": "primality, but we're\nnot going to do that. Let's say this is a black box. And the reason why I divided\nthings into these smallest",
    "start": "1789860",
    "end": "1797480"
  },
  {
    "text": "pieces as possible was what? I didn't explicitly state that. Yeah.",
    "start": "1797480",
    "end": "1802830"
  },
  {
    "text": "It's like you want to have the\nsmallest increment, so that you have a better probability\nof getting an even workload",
    "start": "1802830",
    "end": "1809502"
  },
  {
    "text": "between the size. Correct. So the more little\npieces I have, the easier it's\ngoing to be for me",
    "start": "1809502",
    "end": "1814530"
  },
  {
    "text": "to come up with a partitioning\nwhere everybody has about the same amount of work.",
    "start": "1814530",
    "end": "1820290"
  },
  {
    "text": "Imagine I only had-- let's say there\nwere eight threads, and imagine I divided the\nwork into eight pieces,",
    "start": "1820290",
    "end": "1826200"
  },
  {
    "text": "and I used the dynamic\nscheduling scheme. Not that helpful, right?",
    "start": "1826200",
    "end": "1831510"
  },
  {
    "text": "So I do want a lot\nof pieces to give me the flexibility to arrive\nat a good workload balance.",
    "start": "1831510",
    "end": "1837240"
  },
  {
    "text": "But the smaller I make those\npieces, the larger this overhead potentially is.",
    "start": "1837240",
    "end": "1843669"
  },
  {
    "text": "So I like this\nsuggestion, let's just-- I mean, maybe if we just\ndouble the size of the pieces,",
    "start": "1843670",
    "end": "1849153"
  },
  {
    "text": "we still have enough\nstuff to do because maybe let's say n is like a million,\nand I only have eight threads.",
    "start": "1849153",
    "end": "1854159"
  },
  {
    "text": "So how would you implement\ntaking two pieces of data out of the queue at once?",
    "start": "1854160",
    "end": "1860010"
  },
  {
    "text": "Yeah. You just increment counter by 2. You just increment counter\nby 2, and what that's done",
    "start": "1860010",
    "end": "1866100"
  },
  {
    "text": "is, it's going to reduce\nthe number of white bars or white regions here by 2.",
    "start": "1866100",
    "end": "1872262"
  },
  {
    "text": "And I can just play\nwith that number like I didn't do it\nby 2 in my example, I did it by granularity.",
    "start": "1872262",
    "end": "1877809"
  },
  {
    "text": "I parameterized it because\nI'm a good software engineer, and I can mess with\nthat parameter right",
    "start": "1877810",
    "end": "1882900"
  },
  {
    "text": "before the deadline to\nget the best performance. And so I just-- I increase the granularity of\nmy task to reduce overhead.",
    "start": "1882900",
    "end": "1890549"
  },
  {
    "text": "And it's very likely on\na lot of modern systems like taking this small\nlittle increment, it's not that expensive.",
    "start": "1890550",
    "end": "1896790"
  },
  {
    "text": "So most likely, you probably can\nmake your workload granularity with just make it a\nlittle bit bigger,",
    "start": "1896790",
    "end": "1901840"
  },
  {
    "text": "can basically probably\nwash out the overheads in a lot of dynamic situations.",
    "start": "1901840",
    "end": "1907180"
  },
  {
    "text": "So that's the trade off. We want as many tasks\nas we can get by with, or we want enough tasks that we\ncan nicely schedule the work,",
    "start": "1907180",
    "end": "1915669"
  },
  {
    "text": "but we don't want unnecessarily\nmany tasks such that overhead got too high.",
    "start": "1915670",
    "end": "1921640"
  },
  {
    "text": "For example, I don't if any of\nyou tried actually doing one task per row, like\ncreating 1,000 tasks?",
    "start": "1921640",
    "end": "1928540"
  },
  {
    "text": "And at some point,\nyou might have started to see the\nspeedup curve fall off. You almost certainly\nwould have seen",
    "start": "1928540",
    "end": "1934230"
  },
  {
    "text": "it fall off if you made\none task be a pixel. Yeah. So here's an example.",
    "start": "1934230",
    "end": "1940179"
  },
  {
    "text": "Imagine I have these 16 tasks. So this is task 0, 1, 2, 3,\nall the to 15 over there,",
    "start": "1940180",
    "end": "1945909"
  },
  {
    "text": "and I just purposely\nmade the last task much, much longer than the rest.",
    "start": "1945910",
    "end": "1951390"
  },
  {
    "text": "OK, now imagine some\ndynamic scheduling approach, just like what we talked about.",
    "start": "1951390",
    "end": "1956710"
  },
  {
    "text": "And here's a possible\nfinal assignment as a result of the\ndynamic scheduling.",
    "start": "1956710",
    "end": "1964270"
  },
  {
    "text": "So notice that even though\nI did dynamic assignment, I got a little bit\nunlucky because the work",
    "start": "1964270",
    "end": "1971289"
  },
  {
    "text": "at the end of the queue was\nactually really big stuff. So like the last piece\nof work that I assigned",
    "start": "1971290",
    "end": "1978549"
  },
  {
    "text": "was like the longest\nrunning piece of work, and so I still ended up\nwith a long tail here.",
    "start": "1978550",
    "end": "1984200"
  },
  {
    "text": "And there are some\nstrategies you could try. Like if you happen to\nknow a little bit more about your tasks, you\nmight do something",
    "start": "1984200",
    "end": "1990520"
  },
  {
    "text": "where you assign the\nbiggest tasks first, so I'm to take this now. Look at the P4 got assigned\nthe big task right at the end.",
    "start": "1990520",
    "end": "1998860"
  },
  {
    "text": "If we actually decided\nto sort things by cost, maybe I'll assign P\nfor the biggest task",
    "start": "1998860",
    "end": "2004050"
  },
  {
    "text": "at the very beginning. And notice that P4\nonly does two tasks, but everybody kind\nof ends up finishing",
    "start": "2004050",
    "end": "2010020"
  },
  {
    "text": "at about the same time. So the more you know\nabout your tasks, the better you can\ndo some scheduling.",
    "start": "2010020",
    "end": "2015850"
  },
  {
    "start": "2015850",
    "end": "2023080"
  },
  {
    "text": "Now, the other strategy we've\ntalked about, and that was just, we just talked about it with\nthese barriers or that myDiff",
    "start": "2023080",
    "end": "2028510"
  },
  {
    "text": "variable was, if we have\na bunch of communication, what is being shared in\nthe work queue example?",
    "start": "2028510",
    "end": "2034540"
  },
  {
    "text": "So in the example\nthat I just showed you with the pool of worker\nthreads, the shared variable",
    "start": "2034540",
    "end": "2040059"
  },
  {
    "text": "is the counter, is the\nlock and the counter. And maybe if we\nwere running this",
    "start": "2040060",
    "end": "2046030"
  },
  {
    "text": "on a big computer with 128\nthreads or something like that, that contention for that\ncounter might actually",
    "start": "2046030",
    "end": "2053649"
  },
  {
    "text": "be pretty non-trivial\nfor especially if the work was small. OK.",
    "start": "2053650",
    "end": "2058908"
  },
  {
    "text": "So I want to get into a little\nbit more advanced mode on how you might alleviate the\ncosts of having a shared work",
    "start": "2058909",
    "end": "2068379"
  },
  {
    "text": "queue by actually distributing,\nmaking copies of that work queue for every thread.",
    "start": "2068380",
    "end": "2075119"
  },
  {
    "text": "And so this is going to be\nthe same idea as, remember, we took the single\nmy diff variable and made local copies of it.",
    "start": "2075120",
    "end": "2081960"
  },
  {
    "text": "So you worked on\nyour local myDiff, and then only aggregated\nthose results when you had to at the very end.",
    "start": "2081960",
    "end": "2087960"
  },
  {
    "text": "We're going to do the same\nthing here for work queues. Now, you're not going to do this\nfor your assignment, by the way.",
    "start": "2087960",
    "end": "2093510"
  },
  {
    "text": "This will be this is just\nfor your own information. But the other thing I\nwanted to just point out, something you will do\nfor your assignment",
    "start": "2093510",
    "end": "2100190"
  },
  {
    "text": "is that so far in this\nlecture, all the pieces of work that we threw into\nthe queue were independent.",
    "start": "2100190",
    "end": "2106340"
  },
  {
    "text": "You could process them in any\norder, and it didn't matter. Something that's very common\nin a lot of different systems",
    "start": "2106340",
    "end": "2112550"
  },
  {
    "text": "is those pieces of the work. Many of them might be\nexecuted in parallel, might be independent,\nbut they have",
    "start": "2112550",
    "end": "2117860"
  },
  {
    "text": "dependencies between these two. So here's an example\nwhere I wrote some code where\nI said, OK, let's",
    "start": "2117860",
    "end": "2123200"
  },
  {
    "text": "say I had an API to\ntell the system, here's a new piece of work. So I want you to do task foo.",
    "start": "2123200",
    "end": "2129980"
  },
  {
    "text": "And then we say, I want\nyou to then do task bar, but please only do bar\nafter you've done foo.",
    "start": "2129980",
    "end": "2137990"
  },
  {
    "text": "So I'm building up this\ndependency graph that constrain or limit what can run when.",
    "start": "2137990",
    "end": "2145910"
  },
  {
    "text": "So your assignment 2,\nthe final part of it will be I give you\na bunch of tasks",
    "start": "2145910",
    "end": "2151070"
  },
  {
    "text": "like this with the\ndependencies, and you have to implement a work\nqueue that runs those tasks,",
    "start": "2151070",
    "end": "2158060"
  },
  {
    "text": "but respects all\nthose dependencies. And so it can be a\nlittle tricky, right? Because like maybe\none task finishes,",
    "start": "2158060",
    "end": "2164940"
  },
  {
    "text": "and that means that\nyou can now start running a whole\nbunch of future tasks that you had already been given\nbut could not start just yet.",
    "start": "2164940",
    "end": "2171860"
  },
  {
    "text": "So this is a very, very\ncommon scheduling problem that exists in a bunch\nof different systems,",
    "start": "2171860",
    "end": "2177060"
  },
  {
    "text": "so that's what your\nassignment 2 is going to be. ",
    "start": "2177060",
    "end": "2183560"
  },
  {
    "text": "OK. So just a little bit of\nsummary that I probably stated a bunch of times.",
    "start": "2183560",
    "end": "2189830"
  },
  {
    "text": "Any questions so far? Yeah. A quick question. It's not clear to me\nwhy stealing a task",
    "start": "2189830",
    "end": "2194960"
  },
  {
    "text": "from another thread\nmakes it faster because then doesn't the word\nqueue still remain empty? ",
    "start": "2194960",
    "end": "2203980"
  },
  {
    "text": "Let's get into it. Let's get into it. So far in this class, because\nit's pretty easy to think about,",
    "start": "2203980",
    "end": "2212769"
  },
  {
    "text": "almost all of the code\nthat we have looked at, almost all of the\nprograms, the parallelism",
    "start": "2212770",
    "end": "2219130"
  },
  {
    "text": "comes from doing about the\nsame thing on different pieces of data from a collection.",
    "start": "2219130",
    "end": "2225760"
  },
  {
    "text": "So in the Mandelbrot fractal,\nit was compute the color of a pixel for all pixels. For many of the other\nexamples in assignment 1,",
    "start": "2225760",
    "end": "2233286"
  },
  {
    "text": "it was like compute the\nsine of this number, or so on, and so on. So all the code that we\nhave looked at kind of",
    "start": "2233287",
    "end": "2241060"
  },
  {
    "text": "had this structure for\neach element of the array, for each task.",
    "start": "2241060",
    "end": "2247440"
  },
  {
    "text": "Every task does\nsomething different. Later in the course, we'll talk\nabout functional abstractions",
    "start": "2247440",
    "end": "2253119"
  },
  {
    "text": "which says, run the function foo\non every element of the array A, and put the output in every\nelement of the array B,",
    "start": "2253120",
    "end": "2260260"
  },
  {
    "text": "so this is more of a\nPyTorch kind of way of thinking about things. There are other things that\nyou'll see when we get into CUDA",
    "start": "2260260",
    "end": "2266590"
  },
  {
    "text": "and other types of programming. There's a little C\nextension called OpenMP.",
    "start": "2266590",
    "end": "2271660"
  },
  {
    "text": "OpenMP, you put these little\npragmas in front of for loop, and basically this pragma\ntells the compiler,",
    "start": "2271660",
    "end": "2277599"
  },
  {
    "text": "this is a normal C4 loop, but\nI promise the loop iterations are independent. Under the hood,\nlaunch actual threads",
    "start": "2277600",
    "end": "2284050"
  },
  {
    "text": "and execute these\niterations in parallel. So the easiest way to\nget parallelism in C is just use these little\nOpenMP kind of things.",
    "start": "2284050",
    "end": "2291230"
  },
  {
    "text": "And when we get into\nprogramming in CUDA, it's going to be the same thing. It's like, I want you\nto run this function foo this many times, and I want\nyou to run on those arrays.",
    "start": "2291230",
    "end": "2299300"
  },
  {
    "text": "So everything that\nwe've seen so far has been like, let me just give\nyou some arrays or some tensors, and I want you to do the\nsame thing on everything.",
    "start": "2299300",
    "end": "2306940"
  },
  {
    "text": "And so that's one\nway of programming that we've talked about. The other way of programming\nthat you're probably",
    "start": "2306940",
    "end": "2313210"
  },
  {
    "text": "much more accustomed to is\nif you want parallelism, you don't describe independent\nthings, you actually",
    "start": "2313210",
    "end": "2319450"
  },
  {
    "text": "create workers, and\nhave them do stuff. So here's an example where\nI actually create a bunch",
    "start": "2319450",
    "end": "2324950"
  },
  {
    "text": "of threads or C++ threads,\nexcuse me in this case. So I am saying,\nas the programmer,",
    "start": "2324950",
    "end": "2331530"
  },
  {
    "text": "I want you to create\nthese n threads and this is what they will do. So in the first case I was\nthinking in terms of data.",
    "start": "2331530",
    "end": "2338990"
  },
  {
    "text": "For all of these pieces of data,\nyou can do this independently. When we're threaded programming,\nwe think a little bit more like,",
    "start": "2338990",
    "end": "2345830"
  },
  {
    "text": "here are my threads and\nhere's what they should do. And another way of\nthinking about programs",
    "start": "2345830",
    "end": "2351120"
  },
  {
    "text": "that's pretty elegant. There's another\nclass of algorithms that if you took 161\nor something like that,",
    "start": "2351120",
    "end": "2357545"
  },
  {
    "text": "you would think about\nthem in a very divide and conquer kind of way, so\nthe canonical example of that",
    "start": "2357545",
    "end": "2363290"
  },
  {
    "text": "is quicksort. So can I just quick check,\nquicksort is something",
    "start": "2363290",
    "end": "2368660"
  },
  {
    "text": "that everybody has seen? OK. All right, great. And so maybe just a little\nbit of review of quicksort.",
    "start": "2368660",
    "end": "2374940"
  },
  {
    "text": "Just I know you've seen it\nwould be, given an array, I want to sort it, and I sort\nit in a divide and conquer way.",
    "start": "2374940",
    "end": "2381030"
  },
  {
    "text": "And the summary of\nquicksort, remember is I pick some\nelement in that array, the details of which matter\nfor its asymptotic complexity.",
    "start": "2381030",
    "end": "2388770"
  },
  {
    "text": "But for the sake\nof this class, pick some elements of\nthe array at random, or even pick the first one. And then I'm going to partition.",
    "start": "2388770",
    "end": "2395130"
  },
  {
    "text": "I'm going to move all\nthe elements below less than that value to\none side of the array, move all the elements\nabove that value",
    "start": "2395130",
    "end": "2401450"
  },
  {
    "text": "to the other side of the array,\nand then I'm going to recurse, and that's what you see\nhere, so the recursive calls",
    "start": "2401450",
    "end": "2406849"
  },
  {
    "text": "to quicksort. So this partition is picking\nthe pivot, and then--",
    "start": "2406850",
    "end": "2412609"
  },
  {
    "text": "it's picking the pivot\nand moving everything less than the pivot to the\nleft side of the array, moving everything greater\nthan the pivot to the right",
    "start": "2412610",
    "end": "2418400"
  },
  {
    "text": "side of the array. Now, it means that everything\non the left side of the array can be sorted and everything\non the right side of the array",
    "start": "2418400",
    "end": "2424428"
  },
  {
    "text": "can be sorted, and recursively\nwe have a sorted array. Just a little\nreview of quicksort.",
    "start": "2424428",
    "end": "2429890"
  },
  {
    "text": "OK. So the skill that you're\nsupposed to be building up in this class is if\nyou look at this code,",
    "start": "2429890",
    "end": "2437390"
  },
  {
    "text": "step one in any programming,\nparallel programming exercise is identify potential\nparallelism,",
    "start": "2437390",
    "end": "2443630"
  },
  {
    "text": "and where is that\nin this program? Let's assume that\npartition is going to be done serially for now.",
    "start": "2443630",
    "end": "2449040"
  },
  {
    "text": "Let's treat that as a black box. Yeah. In recursive calls. The two recursive\ncalls to quicksort,",
    "start": "2449040",
    "end": "2454290"
  },
  {
    "text": "we can do the left side and\nthe right side in parallel. So we're kind of used\nto this data parallel thinking where we say, we got\nan array of 10 million elements.",
    "start": "2454290",
    "end": "2462270"
  },
  {
    "text": "Everything can be\ndone in parallel. Here we're saying,\nhey, functionally we have two things that\ncan be done in parallel.",
    "start": "2462270",
    "end": "2469950"
  },
  {
    "text": "Probably not enough to saturate\na big parallel computer with just this one\nrecursive call,",
    "start": "2469950",
    "end": "2475620"
  },
  {
    "text": "but where do I get a lot of\nmy lots of parallelism from?",
    "start": "2475620",
    "end": "2480880"
  },
  {
    "text": "Each of those calls is\ngoing to make more calls. Each of those calls is going\nto create two more things, and so on, and so on,\nso I have this tree",
    "start": "2480880",
    "end": "2487800"
  },
  {
    "text": "of potential parallelism. So this is independent work,\nand if I recurse, ultimately",
    "start": "2487800",
    "end": "2493110"
  },
  {
    "text": "at some point I get down to a\nlevel of the tree where I have a lot of parallel things to do.",
    "start": "2493110",
    "end": "2498440"
  },
  {
    "text": "So this is a recursive\nalgorithm that progressively reveals its parallelism. Unlike this data parallel\nstuff where I say,",
    "start": "2498440",
    "end": "2505260"
  },
  {
    "text": "here's a million things to do.  OK, so we're good on\nthe workload, right?",
    "start": "2505260",
    "end": "2512250"
  },
  {
    "text": "All right. So what I'm going\nto talk about now is a programming\nabstraction that",
    "start": "2512250",
    "end": "2517650"
  },
  {
    "text": "makes it a lot easier\nto write these divide",
    "start": "2517650",
    "end": "2522660"
  },
  {
    "text": "and conquer parallel programs. And you're going to have at\nyour disposal-- and by the way,",
    "start": "2522660",
    "end": "2528185"
  },
  {
    "text": "this is a programming\nsystem called Cilk. It exists in most modern\nC++ compilers today,",
    "start": "2528185",
    "end": "2534520"
  },
  {
    "text": "so you probably can on your math\nmachines write code like this if you wish. And there's only two\nchanges to C in principle.",
    "start": "2534520",
    "end": "2543960"
  },
  {
    "text": "So typically, we think about\na normal function call. What we're going to\ndo is we're going",
    "start": "2543960",
    "end": "2549869"
  },
  {
    "text": "to be able to put this\nlittle keyword spawn in front of a function call,\nand then we're",
    "start": "2549870",
    "end": "2557339"
  },
  {
    "text": "going to add one more construct\nto the language called sync. Now, let's talk and be really\nclear about what this means.",
    "start": "2557340",
    "end": "2567270"
  },
  {
    "text": "So what cilk_spawn a function\nmeans is please invoke foo,",
    "start": "2567270",
    "end": "2573610"
  },
  {
    "text": "just like any other\nfunction call would. But unlike a normal\nfunction call,",
    "start": "2573610",
    "end": "2579250"
  },
  {
    "text": "the caller may continue\nexecuting asynchronously",
    "start": "2579250",
    "end": "2584410"
  },
  {
    "text": "with the callee. This is one of\nthose cases where I want to be very clear\nabout, in your mind,",
    "start": "2584410",
    "end": "2590420"
  },
  {
    "text": "does the semantics of what\nthe programming model means, is that clear to you independent\nof how it might be implemented?",
    "start": "2590420",
    "end": "2600220"
  },
  {
    "text": "So in normal C, if\nwe call a function, the calling thread of control\ngoes into the function.",
    "start": "2600220",
    "end": "2608720"
  },
  {
    "text": "The function must return and\nthen the caller continues. In Cilk, if it is a\nspawn foo, what it means",
    "start": "2608720",
    "end": "2617230"
  },
  {
    "text": "is that the caller\ncan just continue and asynchronously foo may run.",
    "start": "2617230",
    "end": "2623430"
  },
  {
    "text": " And then sync just says, for\nany calls that I have spawned,",
    "start": "2623430",
    "end": "2632839"
  },
  {
    "text": "sync means that you are\nguaranteed that those calls have completed, have returned\nprior to continuing",
    "start": "2632840",
    "end": "2640130"
  },
  {
    "text": "from this function,\nso you can think about Cinque as a barrier\nfor all spawned functions.",
    "start": "2640130",
    "end": "2645220"
  },
  {
    "text": "OK. Questions? ",
    "start": "2645220",
    "end": "2651450"
  },
  {
    "text": "Notice that I haven't said\nanything about C+ threads implementation or\nanything like that.",
    "start": "2651450",
    "end": "2659310"
  },
  {
    "text": "So this all is\nclear to everybody? OK. All right. So you might be thinking this\ncilk_spawn foo feels a lot",
    "start": "2659310",
    "end": "2667530"
  },
  {
    "text": "like forking a thread, and\nthe sync feels a whole lot like joining a thread.",
    "start": "2667530",
    "end": "2674940"
  },
  {
    "text": "But there's a difference between\ncreating threads and spawning work.",
    "start": "2674940",
    "end": "2680070"
  },
  {
    "text": "You can think about this\nas there's some work to do that is execute the\nfunction foo on these arguments,",
    "start": "2680070",
    "end": "2688140"
  },
  {
    "text": "and you can do it whenever\nyou want at some future time.",
    "start": "2688140",
    "end": "2693990"
  },
  {
    "text": "That's different\nfrom forking a thread or creating a thread which says,\nhere is a thread of control.",
    "start": "2693990",
    "end": "2699760"
  },
  {
    "text": "It is now running, it\nneeds an execution context, processor is running it.",
    "start": "2699760",
    "end": "2706170"
  },
  {
    "text": "Let's see if further\nelaboration makes sense, but I'm expecting\nquestions soon.",
    "start": "2706170",
    "end": "2713050"
  },
  {
    "text": "So here's an example of my\nfunction that calls foo and bar, and I'm starting to adopt\nthis illustration here",
    "start": "2713050",
    "end": "2720400"
  },
  {
    "text": "on the right, which illustrates\nthe stack or the sequence of execution of this\nthread of control.",
    "start": "2720400",
    "end": "2728247"
  },
  {
    "text": "So in this thread\nof control, there's part A, the part of my_func\nbefore the call to foo.",
    "start": "2728248",
    "end": "2734920"
  },
  {
    "text": "Then there will be\nthe execution of foo, then the execution of bar,\nand then everything after bar.",
    "start": "2734920",
    "end": "2741010"
  },
  {
    "text": "That's just normal\nC function calls. In silk, spawning\nfoo is like creating",
    "start": "2741010",
    "end": "2750599"
  },
  {
    "text": "an asynchronous, concurrent,\nlogical instruction stream that may or may not\nbe run at the same time.",
    "start": "2750600",
    "end": "2759090"
  },
  {
    "text": "So you can think\nabout spawning foo is more like creating a piece of\nwork, some note that says you've got to run foo at some point.",
    "start": "2759090",
    "end": "2765990"
  },
  {
    "text": "And notice what\nI'm drawing here is I'm showing you the concurrency\nin the program using my arrows.",
    "start": "2765990",
    "end": "2773319"
  },
  {
    "text": "So spawn foo, and then\ncalling bar, and then syncing, notice that foo is spawned\noff to the side, bar",
    "start": "2773320",
    "end": "2781109"
  },
  {
    "text": "is actually executed as a\nnormal C function by the caller. That's why it's straight\ndown, and then the sync",
    "start": "2781110",
    "end": "2786840"
  },
  {
    "text": "means that the main caller\nhas to wait for the foo to complete before it continues.",
    "start": "2786840",
    "end": "2793170"
  },
  {
    "text": "So does that diagram make sense? And then just to check\nyour understanding, I can also spawn foo,\nspawn bar, and then sync,",
    "start": "2793170",
    "end": "2800010"
  },
  {
    "text": "and notice that the initial\nlogical thread doesn't do anything.",
    "start": "2800010",
    "end": "2805190"
  },
  {
    "text": "It just sits there and\njust waits for foo and bar to get to finish. Yes.",
    "start": "2805190",
    "end": "2810790"
  },
  {
    "text": "So is it always guaranteed\nthat, say, unlike the second example of foo and bar will\nnever [INAUDIBLE] cilk_spawn",
    "start": "2810790",
    "end": "2818510"
  },
  {
    "text": "[? keyword? ?] So there we should\nbe careful about--",
    "start": "2818510",
    "end": "2823630"
  },
  {
    "text": "I want you to think about\nthis as logical work.",
    "start": "2823630",
    "end": "2829450"
  },
  {
    "text": "So what is true in the semantics\nis that bar and foo are two pieces of work that\ncan run asynchronously,",
    "start": "2829450",
    "end": "2837920"
  },
  {
    "text": "that run asynchronously\nwith this. I am being very careful\nnot to telling you what--",
    "start": "2837920",
    "end": "2844690"
  },
  {
    "text": "if there's a thread\npool under the hood, what threads are actually\ngoing to run these things.",
    "start": "2844690",
    "end": "2849799"
  },
  {
    "text": "But logically, I have three\nlogical threads of control here",
    "start": "2849800",
    "end": "2855250"
  },
  {
    "text": "that are defining work\nthat needs to be done and can be done concurrently\nat the same time.",
    "start": "2855250",
    "end": "2861859"
  },
  {
    "text": "[INAUDIBLE] get copies\nof their parameters? For now, just think about it\nas a shared address space.",
    "start": "2861860",
    "end": "2867130"
  },
  {
    "text": "Just completely\nshared address space. And just one more check,\nhere's spawn, foo, bar, fizz. Buzz happens as a conventional\nC call, C function call.",
    "start": "2867130",
    "end": "2876290"
  },
  {
    "text": "Notice that it's right\nhere on the main thread, and then I sync to make sure\nall the asynchronous stuff",
    "start": "2876290",
    "end": "2881320"
  },
  {
    "text": "is done before continuing on. So oops.",
    "start": "2881320",
    "end": "2886990"
  },
  {
    "text": "So silk says nothing about\nwhen this asynchronous work is done other than it has to\nget done before sync happens,",
    "start": "2886990",
    "end": "2896420"
  },
  {
    "text": "before sync returns. And so a question to you is\nthis question here on the slide,",
    "start": "2896420",
    "end": "2903160"
  },
  {
    "text": "is an implementation\nof silk correct? Meaning does it get a\nvalid answer to the program",
    "start": "2903160",
    "end": "2909790"
  },
  {
    "text": "if I just took out all of the\nspawns and syncs from the code? If my implementation\nof the silk compiler",
    "start": "2909790",
    "end": "2917470"
  },
  {
    "text": "is take this text string, search\nfor silk spawn, remove it.",
    "start": "2917470",
    "end": "2924490"
  },
  {
    "text": "Search for sync, remove it,\nand then compile the resulting program as a normal C program.",
    "start": "2924490",
    "end": "2930009"
  },
  {
    "text": "Yes. It is a correct program,\nand the language was actually intentionally\ndesigned for this",
    "start": "2930010",
    "end": "2935440"
  },
  {
    "text": "to be the case.  Can you see that?",
    "start": "2935440",
    "end": "2942980"
  },
  {
    "text": "Now, of course, would\nit be a correct program if the implementation here would\nbe whenever there was a sync,",
    "start": "2942980",
    "end": "2949460"
  },
  {
    "text": "cilk_spawn it spawned\na pthread to run foo, spawned another C++\nthread to run bar,",
    "start": "2949460",
    "end": "2957410"
  },
  {
    "text": "spawned another C++\nthread to run fizz, and then cilk_sync was just a\njoin of all three of those p",
    "start": "2957410",
    "end": "2963170"
  },
  {
    "text": "threads. That's also a valid\nimplementation. Yeah.",
    "start": "2963170",
    "end": "2968530"
  },
  {
    "text": "Question. Yeah. Those are two valid\nimplementations of this programming model. There's like no constraints on\norder that have dependencies,",
    "start": "2968530",
    "end": "2977119"
  },
  {
    "text": "and we can guarantee that-- Notice that there's no\ndependency here between foo, bar, and fizz.",
    "start": "2977120",
    "end": "2982210"
  },
  {
    "text": "The only dependency is that\nfoo can run asynchronously with the main thread, and\nfoo must be done by the time",
    "start": "2982210",
    "end": "2990970"
  },
  {
    "text": "the main thread\nreturns from cilk_sync. Those are the only\nscheduling constraints.",
    "start": "2990970",
    "end": "2996430"
  },
  {
    "text": "So I could run foo,\nbar, fizz sequentially. I could run foo, bar,\nfizz on different threads.",
    "start": "2996430",
    "end": "3002850"
  },
  {
    "text": "I could run foo and bar on one\nthread, and fizz on another. Those are all valid potential\nschedules or implementations",
    "start": "3002850",
    "end": "3010500"
  },
  {
    "text": "of this program. They will all get\nthe same answer. But would fizz, bar, foo\nbe valid by that case?",
    "start": "3010500",
    "end": "3015900"
  },
  {
    "text": "Fizz, bar, foo in-- If you said spawned\nbecause it doesn't really-- would it not really matter?",
    "start": "3015900",
    "end": "3021960"
  },
  {
    "text": "Because this foo is asynchronous\nwith the main thing, bar is asynchronous\nwith the main thing,",
    "start": "3021960",
    "end": "3028940"
  },
  {
    "text": "so it must be\nasynchronous with foo. And if fizz is asynchronous\nwith the main thing, which is asynchronous with bar and\nfoo, yes, they're all different.",
    "start": "3028940",
    "end": "3036619"
  },
  {
    "text": "Do not think about this\nas a command buffer or something like that, and I'm\npushing things into a command. Yeah.",
    "start": "3036620",
    "end": "3041870"
  },
  {
    "text": "I'm just saying,\nfoo, bar, and fizz are all completely\nindependent things that can be scheduled in any order.",
    "start": "3041870",
    "end": "3047610"
  },
  {
    "text": "Just hey, you better\nbe done with them by the time this call returns.",
    "start": "3047610",
    "end": "3052780"
  },
  {
    "text": "That's the only guarantee\nI get as a programmer. So I'm just kicking off\nparallel work, basically.",
    "start": "3052780",
    "end": "3060850"
  },
  {
    "text": "Now, the difference is\nthat these foos and bars, they can be recursive functions,\nand they can actually kick off",
    "start": "3060850",
    "end": "3066070"
  },
  {
    "text": "their own parallel work also. OK. So here is quicksort implemented\nin this programming model.",
    "start": "3066070",
    "end": "3074650"
  },
  {
    "text": "Notice that the only thing I\ndid more or less was call spawn.",
    "start": "3074650",
    "end": "3081099"
  },
  {
    "text": "By the way, there's an implicit\nsync at the end of the function, so I didn't write it here. But when a function returns,\nall things that it spawned",
    "start": "3081100",
    "end": "3090000"
  },
  {
    "text": "have to sync at the\nend of the function, so that's why you\ndon't see a sync here. And the only other\nthing is I have this",
    "start": "3090000",
    "end": "3096760"
  },
  {
    "text": "if the size of the array\nis sufficiently small, just do it sequentially\nbecause at that point,",
    "start": "3096760",
    "end": "3103097"
  },
  {
    "text": "so I don't want any overhead\nof dealing with concurrency or anything like that. So there's a little bit\nof a base case there now,",
    "start": "3103097",
    "end": "3109339"
  },
  {
    "text": "but I didn't need to put that\nin there for correctness. And now look to see if you can\nconfirm that this is correct.",
    "start": "3109340",
    "end": "3116780"
  },
  {
    "text": "So we start with\nthe main thread. The main thread runs partition,\nwhich is a normal C function.",
    "start": "3116780",
    "end": "3123790"
  },
  {
    "text": "Based on the\npartition, it spawns-- well, first of all,\nit does a normal--",
    "start": "3123790",
    "end": "3130030"
  },
  {
    "text": "it spawns parallel work,\nso that's the edge off to the right, and then it\ncalls quicksort recursively",
    "start": "3130030",
    "end": "3137890"
  },
  {
    "text": "on the main, the current\nthread of control. I could have spawned\nboth of them,",
    "start": "3137890",
    "end": "3143000"
  },
  {
    "text": "but I just decided to do one\nof them on the main thread. They're essentially the same.",
    "start": "3143000",
    "end": "3148500"
  },
  {
    "text": "And then that's where I\nget these two gray boxes, and then we recurse.",
    "start": "3148500",
    "end": "3153690"
  },
  {
    "text": "Inside the gray boxes, there's\na partition call, and then another spawn, and\nso on, and so on,",
    "start": "3153690",
    "end": "3159208"
  },
  {
    "text": "all the way down to the\nleaves of this tree where the leaves are just\ncalling standard std sort, and that small little\nimplementation detail in my cut",
    "start": "3159208",
    "end": "3170790"
  },
  {
    "text": "off. Any questions about this program\nand corresponding diagram? ",
    "start": "3170790",
    "end": "3178109"
  },
  {
    "text": "Good. Cool.  So the main idea here is just\nthe programmer's responsibility",
    "start": "3178110",
    "end": "3185790"
  },
  {
    "text": "is just to reveal work,\nand it can reveal it in a recursive way.",
    "start": "3185790",
    "end": "3192070"
  },
  {
    "text": "It's silk's responsibility\nto schedule these programs.",
    "start": "3192070",
    "end": "3197530"
  },
  {
    "text": "So your goal is to make sure\nthat the recursion generates enough parallel tasks such\nthat a good scheduler can",
    "start": "3197530",
    "end": "3206250"
  },
  {
    "text": "schedule them and get\ngood workload balance. If I don't create\nenough parallel tasks, there's nothing anybody can do.",
    "start": "3206250",
    "end": "3211812"
  },
  {
    "text": " OK. All right.",
    "start": "3211812",
    "end": "3218100"
  },
  {
    "text": "And so as I've alluded\nto, one simple-- well, the simplest possible\nimplementation of silk is ignore all of the\nspawn and sync keywords",
    "start": "3218100",
    "end": "3225360"
  },
  {
    "text": "and run everything sequentially. The other crazy,\nsimple implementation is turn every spawn into an\nactual standard thread spawn,",
    "start": "3225360",
    "end": "3233760"
  },
  {
    "text": "and every sync into a join of\neverything that I've spawned, and that is a valid\nimplementation,",
    "start": "3233760",
    "end": "3239220"
  },
  {
    "text": "but it would be\na little bit slow because the cost of spawning\nthreads is pretty heavy.",
    "start": "3239220",
    "end": "3245530"
  },
  {
    "text": "Remember that demo I showed\nlast week it was like 300 times faster by using a\nthread pool versus-- ",
    "start": "3245530",
    "end": "3253069"
  },
  {
    "text": "so yeah, so this is\na little bit much. So what we're going\nto do is we're going to do what\neverybody does is we're",
    "start": "3253070",
    "end": "3259770"
  },
  {
    "text": "going to create a thread pool. Let's say I have\na processor that has eight execution contexts.",
    "start": "3259770",
    "end": "3264980"
  },
  {
    "text": "At the beginning\nof the program, I'm going to fire up eight threads,\nand all of those threads are going to be just\nrunning while there is still",
    "start": "3264980",
    "end": "3272490"
  },
  {
    "text": "work to do in the system. I'm going to go grab\nthe next piece of work. I'm going to go\ngrab a piece of work",
    "start": "3272490",
    "end": "3278140"
  },
  {
    "text": "and I'm going to get it done. That's the thing here. So all of these spawn\ncalls are actually",
    "start": "3278140",
    "end": "3285190"
  },
  {
    "text": "going to be like adding work\nto a list of things to do, and we need an algorithm\nthat each of the threads",
    "start": "3285190",
    "end": "3292059"
  },
  {
    "text": "are running that achieves\ngood workload balance, and minimizes\nsynchronization cost.",
    "start": "3292060",
    "end": "3301915"
  },
  {
    "text": "So let's think about\nan example here. Cilk_spawn foo, run bar on the\nmain thread, and then sync, that's what we have\noff to the side.",
    "start": "3301915",
    "end": "3308530"
  },
  {
    "text": "And just to give some\nthings, some names, I'm going to call the\nasynchronous call the child,",
    "start": "3308530",
    "end": "3315700"
  },
  {
    "text": "and I'm going to call the\nrest of the main thread, the continuation, just to\ngive things some names.",
    "start": "3315700",
    "end": "3322210"
  },
  {
    "text": "So if we only have one core,\nwe have a choice to make. We can either run\nthe child first",
    "start": "3322210",
    "end": "3329710"
  },
  {
    "text": "or we run the continuation. A normal C program does what? ",
    "start": "3329710",
    "end": "3336620"
  },
  {
    "text": "This is a normal C code, does\nit run the child first or does it run the continuation? Child first.",
    "start": "3336620",
    "end": "3341625"
  },
  {
    "text": "Runs the child first because\nwe go into the function call. Exactly. So now let's assume that we\nhave a pool of two threads.",
    "start": "3341625",
    "end": "3349609"
  },
  {
    "text": "OK. All right. Let's assume that we have\na pool of two threads. And I got ahead\nof myself, if this",
    "start": "3349610",
    "end": "3356120"
  },
  {
    "text": "was a single threaded\nimplementation, I just have a stack, right? And what would happen\nis when I call foo,",
    "start": "3356120",
    "end": "3362520"
  },
  {
    "text": "I push my current\nframe onto the stack. I go run foo and\nthen when I return,",
    "start": "3362520",
    "end": "3369539"
  },
  {
    "text": "I return back to the caller,\nand that's just how it works. So imagine I'm running--",
    "start": "3369540",
    "end": "3375120"
  },
  {
    "text": "in this case, I'm running\na foo, but while I'm running foo, which is\nasynchronous with the caller",
    "start": "3375120",
    "end": "3382190"
  },
  {
    "text": "and silk, my other\nthread is just idle.",
    "start": "3382190",
    "end": "3388213"
  },
  {
    "text": "So the name of the game\nhere is that thread 1 could be running bar, it could\nbe running the caller,",
    "start": "3388214",
    "end": "3394789"
  },
  {
    "text": "and so what's going to happen\nis when this thread calls foo,",
    "start": "3394790",
    "end": "3403060"
  },
  {
    "text": "it starts running foo,\nand then it puts bar. It reveals its stack\nto all other threads.",
    "start": "3403060",
    "end": "3411910"
  },
  {
    "text": "And if thread 1 over there is\nidle, and sees that thread 0 has bar in its work queue,\nthread 1 is just",
    "start": "3411910",
    "end": "3418260"
  },
  {
    "text": "going to rip that out of the\nstack and start running bar, or really it's just\ngoing to put bar",
    "start": "3418260",
    "end": "3424200"
  },
  {
    "text": "in its own stack, or\nits own work queue. So instead of the stack just\nbeing kind of sitting there",
    "start": "3424200",
    "end": "3431820"
  },
  {
    "text": "in the heap and uninterpretable\nto other threads, what's going to happen is\nwhen we spawn foo, we're actually going to\ncreate this data record that",
    "start": "3431820",
    "end": "3438750"
  },
  {
    "text": "says there's foo, and\nthere's the continuation, and there's bar. Both of these are\nin my work queue.",
    "start": "3438750",
    "end": "3445559"
  },
  {
    "text": "One of the threads\nwill take foo. The other thread will take bar. So in this multiple\nqueue situation,",
    "start": "3445560",
    "end": "3451930"
  },
  {
    "text": "every worker thread\nhas a queue of stuff that it's supposed to do next,\nand if my queue ever goes empty,",
    "start": "3451930",
    "end": "3457752"
  },
  {
    "text": "I'm going to go steal\nfrom somebody else. ",
    "start": "3457752",
    "end": "3463010"
  },
  {
    "text": "All right. So we have some options. When I look at this foo and\nbar, I can either start running.",
    "start": "3463010",
    "end": "3475910"
  },
  {
    "text": "Like the thread that\nactually runs this code, it can either start\nrunning bar, and put foo",
    "start": "3475910",
    "end": "3481880"
  },
  {
    "text": "on the stack for stealing,\nor it can start running foo, and put bar on the stack. Did I say the same thing twice?",
    "start": "3481880",
    "end": "3488060"
  },
  {
    "text": "I can't remember. I might have, but I\nthink you get the point. So running the\ncontinuation first",
    "start": "3488060",
    "end": "3494060"
  },
  {
    "text": "is queuing the function\ncall for later stealing. Running the child first is\nin queuing the continuation",
    "start": "3494060",
    "end": "3502730"
  },
  {
    "text": "for later stealing. This is the same order\nas the serial program. This is a completely\ndifferent order.",
    "start": "3502730",
    "end": "3509095"
  },
  {
    "text": " So it's kind of unclear\nwhich one is better,",
    "start": "3509095",
    "end": "3514430"
  },
  {
    "text": "so let's take a look\nat this program. I have a foo loop for i\nequals 0 to n spawn, spawn, spawn, spawn, spawn.",
    "start": "3514430",
    "end": "3520390"
  },
  {
    "text": "So the work looks like this. I get all these edges\noff to the right side. ",
    "start": "3520390",
    "end": "3528042"
  },
  {
    "text": "And if we run the continuation\nfirst, every one of those spawns",
    "start": "3528042",
    "end": "3535180"
  },
  {
    "text": "is just going to be adding\nfoo(0), foo(1), foo(2), foo(3)",
    "start": "3535180",
    "end": "3540369"
  },
  {
    "text": "into my work queue. And the running--\nthe existing thread",
    "start": "3540370",
    "end": "3545658"
  },
  {
    "text": "is just going to keep\nrunning that foo loop because that's the\ncontinuation, it just keeps running the caller, and\nthis queue just kind of fills up",
    "start": "3545658",
    "end": "3553240"
  },
  {
    "text": "with a bunch of work. Can you see how that works? What's going to happen if\nwe run the child first?",
    "start": "3553240",
    "end": "3561450"
  },
  {
    "text": " The thread gets\nto the first foo--",
    "start": "3561450",
    "end": "3567720"
  },
  {
    "text": "spawn of foo 0, and what\ndoes it start doing? Yeah. It's going to start doing\nthe work for that phone call,",
    "start": "3567720",
    "end": "3574329"
  },
  {
    "text": "and it's not going\nto make more work. So this thread is going\nto start doing foo(0), and it's going to put its\ncontinuation on its stack,",
    "start": "3574330",
    "end": "3582665"
  },
  {
    "text": "on its work queue. So it says, I'm\nworking on foo(0), and the rest of that foo\nloop is the task that I'm",
    "start": "3582665",
    "end": "3589050"
  },
  {
    "text": "going to put in my queue is\nsomething I have to do later, so it might look a\nlittle bit like this.",
    "start": "3589050",
    "end": "3594100"
  },
  {
    "text": "So in my indication,\nmy notation here is thread 0 is running foo,\nfoo(0), and thread 0's work",
    "start": "3594100",
    "end": "3601830"
  },
  {
    "text": "queue has in it the rest of the\nloop, which I noted there is the continuation of the loop\nstarting at the iteration i",
    "start": "3601830",
    "end": "3608610"
  },
  {
    "text": "equals 1, the next iteration. So I just start\ndoing what the child,",
    "start": "3608610",
    "end": "3614400"
  },
  {
    "text": "and I just shove everything\nelse onto the queue. Now, what happens\nin this scheme,",
    "start": "3614400",
    "end": "3620110"
  },
  {
    "text": "in this run child first\nscheme, if I have two threads?",
    "start": "3620110",
    "end": "3626110"
  },
  {
    "text": "So what is thread-- so thread 0 is running foo. ",
    "start": "3626110",
    "end": "3633490"
  },
  {
    "text": "It put the rest of the\ncontinuation into its stack. There are some other\nthread that steals",
    "start": "3633490",
    "end": "3640360"
  },
  {
    "text": "the rest of foo loop, starts\nrunning iteration 1, which",
    "start": "3640360",
    "end": "3646360"
  },
  {
    "text": "causes it to start running\niteration 1, and then I need--",
    "start": "3646360",
    "end": "3653770"
  },
  {
    "text": "sorry, I made one mistake. And then the rest\nof the work to steal is for loop beginning\nof iteration 2.",
    "start": "3653770",
    "end": "3659875"
  },
  {
    "text": " And then if thread 0 gets done,\nit steals the continuation back,",
    "start": "3659875",
    "end": "3667519"
  },
  {
    "text": "starts running foo(2), and puts\nthe continuation with iteration 3 into its queue.",
    "start": "3667520",
    "end": "3674411"
  },
  {
    "text": "So in the first example--  well, in this example,\nthe continuation",
    "start": "3674411",
    "end": "3680390"
  },
  {
    "text": "is just going to be bouncing\nbetween the threads. In the other example,\nthe caller just",
    "start": "3680390",
    "end": "3686420"
  },
  {
    "text": "created all of the iterations\nof the work in its own queue, and the second\nthread was just going to steal all these\nlittle pieces.",
    "start": "3686420",
    "end": "3693620"
  },
  {
    "text": "OK, this is a good\none to talk through in the future or offline. A good one just to make\nsure you work through.",
    "start": "3693620",
    "end": "3699540"
  },
  {
    "text": "So at this point,\nyou're like, I'm not-- you're probably not feeling\nvery good about this because works like bouncing\naround all over the place.",
    "start": "3699540",
    "end": "3705870"
  },
  {
    "text": "But let's think about this when\nyour algorithm is recursive. So let's go back to quicksort,\nand think about what",
    "start": "3705870",
    "end": "3713450"
  },
  {
    "text": "happens if we're\ndoing a run child first scheme with a recursive\ndivide and conquer algorithm.",
    "start": "3713450",
    "end": "3721160"
  },
  {
    "text": "Let's just say the size\nof the array was, I think,",
    "start": "3721160",
    "end": "3726930"
  },
  {
    "text": "200 elements. 200 elements, yeah. So the first thing\nthat happened would be,",
    "start": "3726930",
    "end": "3734039"
  },
  {
    "text": "I'm going to call quicksort\nrecursively on elements 0 through 100, and I'm going\nto put onto my future queue,",
    "start": "3734040",
    "end": "3743460"
  },
  {
    "text": "I need to do the\nquicksort for the other, the back half of the\narray, 100 to 200. So the first thing I\npushed onto my queue",
    "start": "3743460",
    "end": "3750600"
  },
  {
    "text": "was quicksort for the rest. And I started working\non quicksort 0 to 100,",
    "start": "3750600",
    "end": "3755700"
  },
  {
    "text": "and then what happens next? I have to push 50 to\n100 onto my stack,",
    "start": "3755700",
    "end": "3764180"
  },
  {
    "text": "and I start working on 0 to 50. Then what happens next? I push 26 to 50 onto my stack,\nand I start working on 0 to 25.",
    "start": "3764180",
    "end": "3773090"
  },
  {
    "text": "So all these recursive\ncalls are building up here. Notice that each piece\nof work that I threw in",
    "start": "3773090",
    "end": "3780320"
  },
  {
    "text": "is smaller than\nthe previous one. So the tiny tasks are down\nhere, the big tasks are up here,",
    "start": "3780320",
    "end": "3790680"
  },
  {
    "text": "so now all my tasks\nare different sizes because of the recursive\nnature of this program.",
    "start": "3790680",
    "end": "3795880"
  },
  {
    "text": "OK, so now let's think about\nwhat these idle threads 1 and 2 are going to do.",
    "start": "3795880",
    "end": "3801020"
  },
  {
    "text": "So they're idle, so what\nare they going to do? They need to still work because\nthey're not making any progress.",
    "start": "3801020",
    "end": "3808240"
  },
  {
    "text": "So now here's a question. There are three\npieces of work here, which one should thread 1 steal?",
    "start": "3808240",
    "end": "3814590"
  },
  {
    "text": " What are some thoughts? Yeah.",
    "start": "3814590",
    "end": "3820589"
  },
  {
    "text": "Steal from the top. OK, so one vote is for stealing\nfrom the top of the queue. Really these are decks,\nso you can actually",
    "start": "3820590",
    "end": "3826140"
  },
  {
    "text": "steal from the bottom, or the\ntop, or anywhere, so they're not actually queues, but-- I was also going to say top. Top, what is your justification\nfor stealing from the top?",
    "start": "3826140",
    "end": "3834090"
  },
  {
    "text": "I was going to say, we\nsaw earlier in lecture, if you do the big\nwork last, then you end up having one thread,\nworking at the very end.",
    "start": "3834090",
    "end": "3842320"
  },
  {
    "text": "OK. Yeah, that's a good reason. That's actually a pretty\ninteresting reason. I actually wasn't\neven looking for that.",
    "start": "3842320",
    "end": "3847990"
  },
  {
    "text": "There's even a\nmore obvious reason why you want to\nsteal from the top. You all are correct. Yeah. It's just going to break up into\na bunch of smaller chunks again,",
    "start": "3847990",
    "end": "3856740"
  },
  {
    "text": "which the other guy can steal. So this is a big task. This is quick\nsorting 100 elements.",
    "start": "3856740",
    "end": "3862390"
  },
  {
    "text": "This is a much smaller task,\nquick sorting 25 elements. If another thread is going to go\nthrough the overhead of stealing",
    "start": "3862390",
    "end": "3870090"
  },
  {
    "text": "some work to give\nthemselves something to do, they should go steal\nthe biggest thing",
    "start": "3870090",
    "end": "3875279"
  },
  {
    "text": "and get working\non it immediately. Actually for both of\nyour reasons, right? So to minimize overhead,\nit makes sense for me",
    "start": "3875280",
    "end": "3881319"
  },
  {
    "text": "to look at the\nstate of the world, and not go I'm going to take\nthis little tiny thing away from you, but I'm going to\ngo take a big thing from you.",
    "start": "3881320",
    "end": "3889210"
  },
  {
    "text": "So there's the advantage\nof this thread 0 is always going to grab the next task off\nthe bottom of its local queue",
    "start": "3889210",
    "end": "3897940"
  },
  {
    "text": "because this is like the same\norder as the sequential program in some sense. It's also good because you\nget data locality and a lot",
    "start": "3897940",
    "end": "3903940"
  },
  {
    "text": "of other nice properties. Remote threads\nare going to steal from the top of my queue for\nactually two cool reasons.",
    "start": "3903940",
    "end": "3911070"
  },
  {
    "text": "One is the one\nthat you mentioned. If you steal a lot of work, you\nwon't have to synchronize again for a long time.",
    "start": "3911070",
    "end": "3916850"
  },
  {
    "text": "And two, there are\nactually, if you make sure that thread 0\nis only touching this,",
    "start": "3916850",
    "end": "3922420"
  },
  {
    "text": "and all other threads\nare touching the top, there are very efficient\ndata structures that you can use that\ndon't require nearly as",
    "start": "3922420",
    "end": "3929020"
  },
  {
    "text": "much synchronization actually. So here, let's say thread\n1 steals the biggest item,",
    "start": "3929020",
    "end": "3935350"
  },
  {
    "text": "and thread 2 steals the\nnext small, or sorry, did I get that right? No. Thread 2 stole the big\nthing in this case, and thread 1 stole the\nmedium-sized thing.",
    "start": "3935350",
    "end": "3944440"
  },
  {
    "text": "Yeah. Exactly. And then things just\nstart getting to work, and now everybody's\nprocessing their own jobs,",
    "start": "3944440",
    "end": "3952490"
  },
  {
    "text": "and all of those\njobs, keep in mind, are creating recursive\ncalls, and now shoving data",
    "start": "3952490",
    "end": "3957550"
  },
  {
    "text": "into the various queues. So now, all these\nthreads are independently working on their subtasks,\nnot communicating at all,",
    "start": "3957550",
    "end": "3967240"
  },
  {
    "text": "and actually revealing more\nparallel work as they go.",
    "start": "3967240",
    "end": "3973060"
  },
  {
    "text": "So the idea is that the\nlocal thread pushes and pops from the bottom of this\nqueue, from the tail. Remote threads\nsteal from the top,",
    "start": "3973060",
    "end": "3979708"
  },
  {
    "text": "so they stay out of\neach other's way, and they steal big\namounts of work. And as time goes on, notice\nthat each of these threads,",
    "start": "3979708",
    "end": "3986802"
  },
  {
    "text": "just by doing the\nrecursion, are actually going to build up\ntheir own work queues. And so they're going to\nhave tons of stuff to do,",
    "start": "3986802",
    "end": "3992869"
  },
  {
    "text": "and until one of them goes idle,\nthere is no more synchronization again.",
    "start": "3992870",
    "end": "3998050"
  },
  {
    "text": "So compare this to a single\nwork queue implementation where every single\ntime I run out of work,",
    "start": "3998050",
    "end": "4003579"
  },
  {
    "text": "I have to go take\nthat shared variable to go get the next thing. Now, I just go look at my\nlocal, my own local, myDiff,",
    "start": "4003580",
    "end": "4010850"
  },
  {
    "text": "my own local queue, and I\ncan avoid synchronization with the other threads.",
    "start": "4010850",
    "end": "4015914"
  },
  {
    "text": "And let me just continue\nthis thread, this slide out a little bit, and\nwe've worked-- yeah, sorry, I shouldn't have built that.",
    "start": "4015915",
    "end": "4021530"
  },
  {
    "text": "OK. So does this makes sense? Cool.",
    "start": "4021530",
    "end": "4026740"
  },
  {
    "text": "Yeah. So [INAUDIBLE] so that's the--",
    "start": "4026740",
    "end": "4033080"
  },
  {
    "text": "So let's say that thread 0\nfinishes all of its work. No. Just the current task. Oh, the current task. Yeah.",
    "start": "4033080",
    "end": "4038290"
  },
  {
    "text": "And now it [INAUDIBLE]. It will just grab this. So to grab that it has to\nmake sure that no other stuff.",
    "start": "4038290",
    "end": "4044770"
  },
  {
    "text": "There are ways to be careful\nbecause I know that-- I thread 0 know\nthat I'm only going to be accessing this side\nof the data structure,",
    "start": "4044770",
    "end": "4051910"
  },
  {
    "text": "and other threads are\ngoing to be issuing the other side of\nthe data structure that are very fast\ndata structures where",
    "start": "4051910",
    "end": "4057670"
  },
  {
    "text": "that can help you with that. Correct. Yes. Yeah. So when thread 2 finishes\nwith you and completion.",
    "start": "4057670",
    "end": "4066850"
  },
  {
    "text": "Let's just say thread through\ngoes idle for any reason at all. It gets done. Yep. Because it starts\nstealing from 0 or 1.",
    "start": "4066850",
    "end": "4073000"
  },
  {
    "text": "Great question. So now there are\nmultiple queues, and we have some options.",
    "start": "4073000",
    "end": "4078050"
  },
  {
    "text": "So is it correct to steal\nfrom any one of them? Not necessarily. Why not?",
    "start": "4078050",
    "end": "4084307"
  },
  {
    "text": "Well, you get the right answer. What I mean by correct. You're actually going\nto get the right answer. As long as if you're idle is\nstill work and start doing it.",
    "start": "4084308",
    "end": "4091370"
  },
  {
    "text": "The question is, what's the most\nefficient thing to steal from? Will be whichever has\nthe most work in it.",
    "start": "4091370",
    "end": "4098189"
  },
  {
    "text": "You could say that there's a-- maybe you should steal\nfrom the busiest thing.",
    "start": "4098189",
    "end": "4103560"
  },
  {
    "text": "It turns out that\ntheoretically asymptotically it is optimal to randomly select\na queue and steal from it,",
    "start": "4103560",
    "end": "4109589"
  },
  {
    "text": "and that's been proven. There are proofs to say that\nif you just randomly select a victim, you are\nwithin a constant",
    "start": "4109590",
    "end": "4116700"
  },
  {
    "text": "of theoretically optimal. So you could probably\nimprove on that, but if you improve\non that, you're only improving your constants.",
    "start": "4116700",
    "end": "4123924"
  },
  {
    "text": "And it's actually a\nlittle bit trickier because if everybody\nmakes that same decision to steal from the same thread,\nall of a sudden that steal",
    "start": "4123925",
    "end": "4131520"
  },
  {
    "text": "might get a little bit\nmore expensive because it could be contention.",
    "start": "4131520",
    "end": "4137469"
  },
  {
    "text": "It also might mean that\nwhen I look at the system, I'm like, wow, this\nguy has a lot of work.",
    "start": "4137470",
    "end": "4142728"
  },
  {
    "text": "By the time other threads\nmake the same decision, some of that work\nmight be gone, and you might be stealing\nfrom somebody that has a very small amount of work.",
    "start": "4142728",
    "end": "4148520"
  },
  {
    "text": "So it turns out that\nthe simplest thing to do is randomly pick a queue that\nhas work and steal from it.",
    "start": "4148520",
    "end": "4155330"
  },
  {
    "text": "So just a few other things,\nsome elegance of this policy. ",
    "start": "4155330",
    "end": "4161299"
  },
  {
    "text": "Let's go back to\nfoo loop example that I started this with. For i equals 0 to\nn, essentially,",
    "start": "4161300",
    "end": "4167639"
  },
  {
    "text": "this is work that says for\nevery element of an array, do some stuff in parallel.",
    "start": "4167640",
    "end": "4172818"
  },
  {
    "text": "I'd like you to convince\nyourself offline that this code that I\nwrote will sequentially",
    "start": "4172819",
    "end": "4179179"
  },
  {
    "text": "generate asynchronous work. Either the main\nthread serially spawns",
    "start": "4179180",
    "end": "4186710"
  },
  {
    "text": "all these things or\nthe continuation, bounces back and forth, is\nconstantly getting stolen.",
    "start": "4186710",
    "end": "4193770"
  },
  {
    "text": "So this is what I want you\nto think about offline. That code there will\nserially create--",
    "start": "4193770",
    "end": "4199910"
  },
  {
    "text": "reveal the parallel work,\nwhich is actually not good because I have\nall these threads, and I'd like to really\nquickly reveal all the work.",
    "start": "4199910",
    "end": "4207830"
  },
  {
    "text": "A much better way to write this\nfoo loop is actually this divide and conquer while\nloop, which says,",
    "start": "4207830",
    "end": "4215320"
  },
  {
    "text": "here's an array\nof n things, let's recurse into doing the first\nhalf and the second half,",
    "start": "4215320",
    "end": "4220650"
  },
  {
    "text": "then let's recurse into\ndoing the first quadrant, the first quarter, and\nthe second quarter, and so on, and so on.",
    "start": "4220650",
    "end": "4225909"
  },
  {
    "text": "This allows half the\narray to get stolen, and then it splits\nup like quicksort, and you get parallel generation\nof the independent work.",
    "start": "4225910",
    "end": "4233820"
  },
  {
    "text": "So actually if you\nsee cilk actually has a Cilk 4 concept in it. So if you go like Cilk 4, i\nequals 0 to n, under the hood,",
    "start": "4233820",
    "end": "4241777"
  },
  {
    "text": "they're going to\nimplement it that way because it's the\nscheduler will ultimately be able to create\nwork faster and you",
    "start": "4241777",
    "end": "4247920"
  },
  {
    "text": "get to running in\nparallel much faster. It's actually kind of a\ncool artifact of stuff.",
    "start": "4247920",
    "end": "4253800"
  },
  {
    "text": "OK. So in the last three\nor four minutes, I'm just going to give\nyou a quick sketch of the last little\npiece of this.",
    "start": "4253800",
    "end": "4260670"
  },
  {
    "text": "How do we implement sync? That's the last piece of this.",
    "start": "4260670",
    "end": "4266000"
  },
  {
    "text": "Up until now, it's\nbeen pretty simple. But when sync means\nI have to go check to see if all this work\nthat's been spawned,",
    "start": "4266000",
    "end": "4273030"
  },
  {
    "text": "which might have been\nstolen by other threads, I need to know if it's\ncomplete, so there's some more",
    "start": "4273030",
    "end": "4278480"
  },
  {
    "text": "bookkeeping that I have to do. OK. So let's think about\nquicksort again,",
    "start": "4278480",
    "end": "4284090"
  },
  {
    "text": "or no, let's think\nabout foo loop, again, because it's easier\nto think about. Let's imagine that we're\nin some state where",
    "start": "4284090",
    "end": "4291950"
  },
  {
    "text": "I am working on foo\nnine, someone else is working on 6, 7, 8. We're basically at the\nend of the iteration",
    "start": "4291950",
    "end": "4297680"
  },
  {
    "text": "and the only thing left is\nlike the rest of the loop, basically finish the\nloop and go on bar.",
    "start": "4297680",
    "end": "4303740"
  },
  {
    "text": "So in this case, let's just\nsay that one of the workers finishes.",
    "start": "4303740",
    "end": "4309470"
  },
  {
    "text": "What do you think's going\nto happen the next time a worker finishes work?",
    "start": "4309470",
    "end": "4315670"
  },
  {
    "text": " Just apply the rules that\nwe've already talked about.",
    "start": "4315670",
    "end": "4322690"
  },
  {
    "text": "The next thread that\nfinishes will go idle. It will select\nsomething to steal.",
    "start": "4322690",
    "end": "4328092"
  },
  {
    "text": "In this case, it will\nsteal the thread 0. Continuation. That continuation basically\nsays it's done with for loop,",
    "start": "4328092",
    "end": "4334780"
  },
  {
    "text": "so what happens next? Yeah. It's going to try to join\nall of the other threads.",
    "start": "4334780",
    "end": "4341340"
  },
  {
    "text": "It'll join on all-- it needs to\njoin on all the other threads, right? So if this was\nimplemented sequentially,",
    "start": "4341340",
    "end": "4346800"
  },
  {
    "text": "the join is trivial. You don't have to do\nanything, but it needs to know that everybody's done.",
    "start": "4346800",
    "end": "4352270"
  },
  {
    "text": "OK. All right. So let's take a look at this. So what I'm going\nto do is I'm going to say that I'm\ngoing to call this,",
    "start": "4352270",
    "end": "4358590"
  },
  {
    "text": "all this area before the\nsink, just a basic block, and I'm just going\nto give it a name. I'm going to give it the name\nA. And then there's this.",
    "start": "4358590",
    "end": "4365600"
  },
  {
    "text": "So this is the sink for all\ncalls spawned within block A. Just to give things a name.",
    "start": "4365600",
    "end": "4372910"
  },
  {
    "text": "So what's going\non here is I have",
    "start": "4372910",
    "end": "4378130"
  },
  {
    "text": "thread 0 is working on foo 9,\nand it has continuation here.",
    "start": "4378130",
    "end": "4383270"
  },
  {
    "text": "It's the rest of block A. And if\nthis item has never been stolen,",
    "start": "4383270",
    "end": "4396140"
  },
  {
    "text": "then what do we do? ",
    "start": "4396140",
    "end": "4401710"
  },
  {
    "text": "Thread 0, I put\nthis on my queue. If it's never been\nstolen, I know that nobody else is doing\nanything related to this,",
    "start": "4401710",
    "end": "4408449"
  },
  {
    "text": "so sync is just a no op. You just continue. So the only thing you have\nto do something clever on",
    "start": "4408450",
    "end": "4415230"
  },
  {
    "text": "is when it actually gets stolen,\nso let's think about that. So let's go back to\nthe very beginning here and say, the\nvery beginning.",
    "start": "4415230",
    "end": "4422469"
  },
  {
    "text": "I'm working on foo(0)\nand the continuation is the rest of the loop for\nblock A. And at some point,",
    "start": "4422470",
    "end": "4430020"
  },
  {
    "text": "the thing may get stolen,\nand if it gets stolen, silk's going to have to\nkeep a note off to the side.",
    "start": "4430020",
    "end": "4436898"
  },
  {
    "text": "So there's this note of, here\nare the things that have gotten stolen from your work queue. OK. Now, what I'm not\ntalking about here",
    "start": "4436898",
    "end": "4443370"
  },
  {
    "text": "is how to manage the locking and\nunlocking of all these shared data structures. But there's this\nsaying, hey, look,",
    "start": "4443370",
    "end": "4450040"
  },
  {
    "text": "one worker has\nstolen work that's related to block A and 0 works\nremote workers have reported",
    "start": "4450040",
    "end": "4458010"
  },
  {
    "text": "that they have finished work\nfor block A. So in this case, this field is now just empty\nbecause it's been stolen,",
    "start": "4458010",
    "end": "4467020"
  },
  {
    "text": "and thread 1 has the\ncontinuation of block A. So thread 1 starts doing\nits job, and at some point,",
    "start": "4467020",
    "end": "4476480"
  },
  {
    "text": "so now thread 1 starts\nworking on foo(1), and my reference\ncount of spawn things is 2, and so on, and so on,\nand so on as more thread steel.",
    "start": "4476480",
    "end": "4487639"
  },
  {
    "text": "Now my reference count is 3. OK, now some thread finishes\nsome work from block",
    "start": "4487640",
    "end": "4494283"
  },
  {
    "text": "A. Block A has been\nstolen, so when we finish work we have to\nupdate the done counter.",
    "start": "4494283",
    "end": "4501489"
  },
  {
    "text": "So that means three different\nthreads are working on A, or haven't worked on\nsub work from A. One",
    "start": "4501490",
    "end": "4507429"
  },
  {
    "text": "of those things is done. And at this point, thread 0\nwill go steal some more work,",
    "start": "4507430",
    "end": "4513380"
  },
  {
    "text": "and so on, and so on, and\nstart working on foo(3),",
    "start": "4513380",
    "end": "4518409"
  },
  {
    "text": "and now let's get to the very\nend of that for loop one. We're on spawn 10, we're done\nwith nine of these things.",
    "start": "4518410",
    "end": "4525790"
  },
  {
    "text": "These threads have no work to do\nbecause they're stuck on-- oh, they have no work to do.",
    "start": "4525790",
    "end": "4531345"
  },
  {
    "text": " Thread 2 seems to be doing the\nlast iteration of for loop.",
    "start": "4531345",
    "end": "4538900"
  },
  {
    "text": "When it gets done it says,\noh, that spawn is complete",
    "start": "4538900",
    "end": "4543949"
  },
  {
    "text": "or that block is\ncomplete, and now it can move on to working on bar.",
    "start": "4543950",
    "end": "4550160"
  },
  {
    "text": "So there's just\nthe sync is going to basically-- it\nbasically has to keep a record of all of the\nspawns related to that block.",
    "start": "4550160",
    "end": "4557550"
  },
  {
    "text": "Whenever you complete work\nfrom some stolen block, you update your\nreference counts.",
    "start": "4557550",
    "end": "4563150"
  },
  {
    "text": "And whichever thread finishes\nthe last piece of work, will just continue on with\nthe continuation at that time.",
    "start": "4563150",
    "end": "4572690"
  },
  {
    "text": "So this is called\ngreedy join scheduling. It means all threads are\nalways attempting to steal,",
    "start": "4572690",
    "end": "4578990"
  },
  {
    "text": "so they're being greedy. Threads only go idle if\nthere's nothing in any queue, and the worker thread\nthat initiates the spawn,",
    "start": "4578990",
    "end": "4589380"
  },
  {
    "text": "so in that example\nI just gave you, the original spawn was\nstarted on thread 0, but the spawn was\nfinished up by thread 2",
    "start": "4589380",
    "end": "4598159"
  },
  {
    "text": "because it was the last thread\nto do work related to it. So it says, OK,\nI'll just keep going",
    "start": "4598160",
    "end": "4604016"
  },
  {
    "text": "with this thread of control. So Cilk is pretty elegant. It's actually a fun thing\nto play around with.",
    "start": "4604016",
    "end": "4609710"
  },
  {
    "text": "The algorithms for scheduling\nare based on some pretty sound, theoretical guarantees, so that\nif you do some things randomly,",
    "start": "4609710",
    "end": "4616440"
  },
  {
    "text": "you're guaranteed to be\nwithin a constant factor of the optimum schedule. It allows you to write,\ndivide, and conquer programs",
    "start": "4616440",
    "end": "4622460"
  },
  {
    "text": "almost as easily\nas if you weren't thinking about parallelism. And under the hood,\nthere's a scheduling policy",
    "start": "4622460",
    "end": "4628369"
  },
  {
    "text": "that maximizes locality,\nminimizes communication amongst all these threads,\nbut still achieves",
    "start": "4628370",
    "end": "4636230"
  },
  {
    "text": "a pretty good workload balance. So this is much more\nadvanced than what you're expected to implement\nin your assignment 2,",
    "start": "4636230",
    "end": "4642505"
  },
  {
    "text": "but I just thought usually,\npeople like to know about it. So this is pretty common in\ndistributed systems as well,",
    "start": "4642505",
    "end": "4649820"
  },
  {
    "text": "and other things, so cool. All right, we're done, and\nI'll see you on Thursday. ",
    "start": "4649820",
    "end": "4659000"
  }
]