[
  {
    "text": "So after logistic\nregression, we're",
    "start": "0",
    "end": "2150"
  },
  {
    "text": "going to start talking about--",
    "start": "2150",
    "end": "3930"
  },
  {
    "text": "which was using stats models.",
    "start": "3930",
    "end": "5720"
  },
  {
    "text": "We'll start using some of the\nclassifiers from scikit-learn.",
    "start": "5720",
    "end": "8490"
  },
  {
    "text": "So the first one we'll\nfit is this LDA model.",
    "start": "8490",
    "end": "13440"
  },
  {
    "text": "So the way scikit-learn works\nfor these prediction problems",
    "start": "13440",
    "end": "19039"
  },
  {
    "text": "is, there's some object--",
    "start": "19040",
    "end": "21650"
  },
  {
    "text": "in this case, we've\ncalled it LDA.",
    "start": "21650",
    "end": "23660"
  },
  {
    "text": "That was just a shorthand for\nLinearDiscriminantAnalysis.",
    "start": "23660",
    "end": "28040"
  },
  {
    "text": "That sort of tells we\ndon't give any data",
    "start": "28040",
    "end": "31280"
  },
  {
    "text": "to the classifying object here.",
    "start": "31280",
    "end": "34370"
  },
  {
    "text": "We just give some arguments.",
    "start": "34370",
    "end": "36434"
  },
  {
    "text": "In this case, we asked it to\nstore the covariance in case",
    "start": "36435",
    "end": "38810"
  },
  {
    "text": "we want to inspect the\ncovariance matrix in LDA.",
    "start": "38810",
    "end": "41490"
  },
  {
    "text": "But having formed the\nestimator this way,",
    "start": "41490",
    "end": "44210"
  },
  {
    "text": "we later fit on a matrix of\nfeatures and a response y.",
    "start": "44210",
    "end": "50030"
  },
  {
    "text": "And then we can\npredict on new features",
    "start": "50030",
    "end": "52850"
  },
  {
    "text": "or the original features.",
    "start": "52850",
    "end": "54989"
  },
  {
    "text": "So that's the general way that\nthese estimators all work.",
    "start": "54990",
    "end": "57957"
  },
  {
    "text": "OK.",
    "start": "57957",
    "end": "60020"
  },
  {
    "text": "So we're, again, going to\nsplit into test and training.",
    "start": "60020",
    "end": "66500"
  },
  {
    "text": "And so we'll make our training\nand test sets as follows.",
    "start": "66500",
    "end": "72230"
  },
  {
    "text": "And we're going to fit\non the training data.",
    "start": "72230",
    "end": "75180"
  },
  {
    "text": "So Jonathan, we're using the\nsame train and test design",
    "start": "75180",
    "end": "79430"
  },
  {
    "text": "matrices we had before,\nbut you just drop",
    "start": "79430",
    "end": "82250"
  },
  {
    "text": "in the intercept from those.",
    "start": "82250",
    "end": "83570"
  },
  {
    "text": "Is that right?",
    "start": "83570",
    "end": "84800"
  },
  {
    "text": "That's right.",
    "start": "84800",
    "end": "85880"
  },
  {
    "text": "Yes.",
    "start": "85880",
    "end": "87350"
  },
  {
    "text": "LDA doesn't require you to put\nit in the intercept columns.",
    "start": "87350",
    "end": "90000"
  },
  {
    "text": "Yes.",
    "start": "90000",
    "end": "90500"
  },
  {
    "text": "And I think it will actually\ncomplain because the covariance",
    "start": "90500",
    "end": "95330"
  },
  {
    "text": "will be 0.",
    "start": "95330",
    "end": "96500"
  },
  {
    "text": "It will have a\nnon-invertible covariance.",
    "start": "96500",
    "end": "98240"
  },
  {
    "start": "98240",
    "end": "101350"
  },
  {
    "text": "So as we saw in lecture, there\nare a few different parameters",
    "start": "101350",
    "end": "105100"
  },
  {
    "text": "that come from\nfitting the LDA model.",
    "start": "105100",
    "end": "107049"
  },
  {
    "text": "One is the common covariance.",
    "start": "107050",
    "end": "110140"
  },
  {
    "text": "There are within class means\nthat you can find this way.",
    "start": "110140",
    "end": "114970"
  },
  {
    "text": "There's the label of\nwhich class is which.",
    "start": "114970",
    "end": "117370"
  },
  {
    "text": "This just helps-- if down is\nthe first one, that tells us",
    "start": "117370",
    "end": "120190"
  },
  {
    "text": "that now, this is 2 by 2.",
    "start": "120190",
    "end": "123310"
  },
  {
    "text": "We'll have the\ndocumentation, again,",
    "start": "123310",
    "end": "125619"
  },
  {
    "text": "as-- scikit-learn has\ngreat documentation,",
    "start": "125620",
    "end": "127690"
  },
  {
    "text": "will tell us whether the second\nrow is the mean in the class,",
    "start": "127690",
    "end": "132010"
  },
  {
    "text": "or the second column\nis the mean of the --",
    "start": "132010",
    "end": "135610"
  },
  {
    "text": "If we had used\nthree features, we",
    "start": "135610",
    "end": "138550"
  },
  {
    "text": "would be able to tell here\nbecause we would have a 2 by 3",
    "start": "138550",
    "end": "141070"
  },
  {
    "text": "or a 3 by 2 matrix, but the\ndocumentation will sort it out.",
    "start": "141070",
    "end": "146200"
  },
  {
    "text": "My guess is that it goes\nby row as the major index.",
    "start": "146200",
    "end": "152800"
  },
  {
    "text": "So these quantities are\nparameters of the fitted model,",
    "start": "152800",
    "end": "159220"
  },
  {
    "text": "and this scalings matrix\nhere, this actually tells you",
    "start": "159220",
    "end": "161660"
  },
  {
    "text": "the discriminant function.",
    "start": "161660",
    "end": "162770"
  },
  {
    "text": "That's what's used\nto tell to make",
    "start": "162770",
    "end": "165590"
  },
  {
    "text": "the prediction between\nwhether you're in the up",
    "start": "165590",
    "end": "168260"
  },
  {
    "text": "class or the down class.",
    "start": "168260",
    "end": "169347"
  },
  {
    "text": "OK.",
    "start": "169347",
    "end": "169847"
  },
  {
    "start": "169847",
    "end": "172340"
  },
  {
    "text": "So we want to, again, evaluate\nhow well this classifier does",
    "start": "172340",
    "end": "177590"
  },
  {
    "text": "on the test data.",
    "start": "177590",
    "end": "179700"
  },
  {
    "text": "So we'll use now\nthe second method",
    "start": "179700",
    "end": "181580"
  },
  {
    "text": "that's commonly used for\nscikit-learn classifiers",
    "start": "181580",
    "end": "184790"
  },
  {
    "text": "in any case.",
    "start": "184790",
    "end": "185670"
  },
  {
    "text": "It's the predict method.",
    "start": "185670",
    "end": "186740"
  },
  {
    "text": "Having fit on x_train and\ny_train, we predict on x_test.",
    "start": "186740",
    "end": "191390"
  },
  {
    "text": "And we'll, again, form\nthe confusion matrix",
    "start": "191390",
    "end": "196460"
  },
  {
    "text": "using the same function.",
    "start": "196460",
    "end": "198110"
  },
  {
    "text": "We have the same test labels.",
    "start": "198110",
    "end": "200180"
  },
  {
    "text": "So we haven't printed out the--",
    "start": "200180",
    "end": "202704"
  },
  {
    "text": "well, let's see\nif we've printed.",
    "start": "202705",
    "end": "204080"
  },
  {
    "start": "204080",
    "end": "206720"
  },
  {
    "text": "We have not printed\nout the accuracy.",
    "start": "206720",
    "end": "211040"
  },
  {
    "text": "So let's move on.",
    "start": "211040",
    "end": "213094"
  },
  {
    "text": "OK.",
    "start": "213095",
    "end": "214850"
  },
  {
    "text": "So we'll just pause\nhere for a moment",
    "start": "214850",
    "end": "218600"
  },
  {
    "text": "to say that most of the\ntime, we've assigned points",
    "start": "218600",
    "end": "223910"
  },
  {
    "text": "to classes based\non thresholding,",
    "start": "223910",
    "end": "226370"
  },
  {
    "text": "the probability at 50%.",
    "start": "226370",
    "end": "228040"
  },
  {
    "start": "228040",
    "end": "232900"
  },
  {
    "text": "Though, sometimes we might\ndecide to change that,",
    "start": "232900",
    "end": "238720"
  },
  {
    "text": "and we can easily do that by\njust changing the threshold",
    "start": "238720",
    "end": "244960"
  },
  {
    "text": "we apply to this LDA\nprobability vector.",
    "start": "244960",
    "end": "249410"
  },
  {
    "text": "These are the estimated\nclass probabilities",
    "start": "249410",
    "end": "251950"
  },
  {
    "text": "for the test data.",
    "start": "251950",
    "end": "256458"
  },
  {
    "text": "So the first column is\nthe estimated probability",
    "start": "256459",
    "end": "259930"
  },
  {
    "text": "of being in the up class,\nand the second column",
    "start": "259930",
    "end": "261910"
  },
  {
    "text": "is the estimated probability\nof being in the down class.",
    "start": "261910",
    "end": "264950"
  },
  {
    "text": "So if you increase\nthe threshold,",
    "start": "264950",
    "end": "267610"
  },
  {
    "text": "you'll classify\nless observations",
    "start": "267610",
    "end": "269530"
  },
  {
    "text": "in the up class and\nmore in the down class.",
    "start": "269530",
    "end": "271600"
  },
  {
    "text": "Yes, that's right.",
    "start": "271600",
    "end": "272350"
  },
  {
    "text": "And that'll change those\ntwo off-diagonal errors",
    "start": "272350",
    "end": "275110"
  },
  {
    "text": "that you have in the\nconfusion matrix.",
    "start": "275110",
    "end": "277099"
  },
  {
    "text": "Yes.",
    "start": "277100",
    "end": "277600"
  },
  {
    "text": "And I just saw in the\ntext that it's actually--",
    "start": "277600",
    "end": "280720"
  },
  {
    "text": "the first column\nis the down class.",
    "start": "280720",
    "end": "282350"
  },
  {
    "text": "So the effect that will be is\nif we increase the threshold",
    "start": "282350",
    "end": "285910"
  },
  {
    "text": "to 90%, we'll just\npredict fewer in the down,",
    "start": "285910",
    "end": "288610"
  },
  {
    "text": "and the number in this\nrow will get smaller.",
    "start": "288610",
    "end": "290949"
  },
  {
    "text": "But it's possible that the\naccuracy will improve by--",
    "start": "290950",
    "end": "295900"
  },
  {
    "text": "well, depending on the\nbalance of how many",
    "start": "295900",
    "end": "298630"
  },
  {
    "text": "are up and down and\nwithin class accuracy.",
    "start": "298630",
    "end": "302440"
  },
  {
    "start": "302440",
    "end": "305380"
  },
  {
    "text": "OK.",
    "start": "305380",
    "end": "305880"
  },
  {
    "text": "So let's move on to\nour next classifier.",
    "start": "305880",
    "end": "308070"
  },
  {
    "text": "The next classifier is\nQuadraticDiscriminantAnalysis",
    "start": "308070",
    "end": "311430"
  },
  {
    "text": "or QDA, and remember that\nthis is very similar to LDA.",
    "start": "311430",
    "end": "315389"
  },
  {
    "text": "The main difference\nis that there",
    "start": "315390",
    "end": "317160"
  },
  {
    "text": "is a class-specific\ncovariance matrix.",
    "start": "317160",
    "end": "320200"
  },
  {
    "text": "So in terms of code,\nwe're going to fit it--",
    "start": "320200",
    "end": "323250"
  },
  {
    "text": "as promised, it fits\nalmost identical to LDA.",
    "start": "323250",
    "end": "326460"
  },
  {
    "text": "We have the same training\nfeatures and the same training",
    "start": "326460",
    "end": "330090"
  },
  {
    "text": "labels.",
    "start": "330090",
    "end": "330750"
  },
  {
    "text": "We'll call the\nfit method on QDA,",
    "start": "330750",
    "end": "333090"
  },
  {
    "text": "and this little QDA is an\ninstance of the QDA classifier.",
    "start": "333090",
    "end": "339330"
  },
  {
    "text": "And again, we've asked it\nto store the covariance.",
    "start": "339330",
    "end": "341680"
  },
  {
    "text": "So with this, we\ncan, for instance,",
    "start": "341680",
    "end": "343210"
  },
  {
    "text": "look within the down\nclass covariance.",
    "start": "343210",
    "end": "347520"
  },
  {
    "text": "That would be the\nfirst covariant entry",
    "start": "347520",
    "end": "350039"
  },
  {
    "text": "of the covariance returned.",
    "start": "350040",
    "end": "352510"
  },
  {
    "text": "So just to remind\nyou, because we now",
    "start": "352510",
    "end": "354870"
  },
  {
    "text": "our separate covariance\nmatrix in each of the classes,",
    "start": "354870",
    "end": "358229"
  },
  {
    "text": "two in this case, the\nclassification function",
    "start": "358230",
    "end": "362070"
  },
  {
    "text": "becomes a quadratic function.",
    "start": "362070",
    "end": "363810"
  },
  {
    "text": "That's right.",
    "start": "363810",
    "end": "364600"
  },
  {
    "text": "Whereas by having that\nconvenient assumption",
    "start": "364600",
    "end": "367480"
  },
  {
    "text": "that they were the same\nif they were the same.",
    "start": "367480",
    "end": "370270"
  },
  {
    "text": "That led to\nLinearDiscriminantAnalysis,",
    "start": "370270",
    "end": "372220"
  },
  {
    "text": "was a linear function.",
    "start": "372220",
    "end": "373688"
  },
  {
    "text": "That's right.",
    "start": "373688",
    "end": "374229"
  },
  {
    "text": "So the discriminant\nfunction in QDA",
    "start": "374230",
    "end": "376960"
  },
  {
    "text": "is not described just\nby the scalings matrix.",
    "start": "376960",
    "end": "378940"
  },
  {
    "text": "You need both the covariance\nand the difference in means.",
    "start": "378940",
    "end": "381370"
  },
  {
    "text": "The scalings in the LDA\nexample is essentially",
    "start": "381370",
    "end": "385120"
  },
  {
    "text": "the difference in means, well,\nmultiplied by the inverse",
    "start": "385120",
    "end": "387940"
  },
  {
    "text": "of the covariance matrix.",
    "start": "387940",
    "end": "390970"
  },
  {
    "text": "So again, in order to evaluate\nhow well this classifier is",
    "start": "390970",
    "end": "395110"
  },
  {
    "text": "doing, we use the predict\nmethod on our test features,",
    "start": "395110",
    "end": "399610"
  },
  {
    "text": "and then we use\nthe same confusion",
    "start": "399610",
    "end": "402159"
  },
  {
    "text": "matrix or the confusion\ntable function to produce",
    "start": "402160",
    "end": "405010"
  },
  {
    "text": "the confusion matrix.",
    "start": "405010",
    "end": "406330"
  },
  {
    "start": "406330",
    "end": "408664"
  },
  {
    "text": "OK.",
    "start": "408665",
    "end": "409165"
  },
  {
    "text": "And so here, we see the\nQDA is about 60% accurate.",
    "start": "409165",
    "end": "413470"
  },
  {
    "text": "That's even better than--",
    "start": "413470",
    "end": "415630"
  },
  {
    "text": "I think we had 55% accuracy\nwith these features",
    "start": "415630",
    "end": "420940"
  },
  {
    "text": "in the logistic regression.",
    "start": "420940",
    "end": "422210"
  },
  {
    "text": "So this is actually quite\ngood for the financial data.",
    "start": "422210",
    "end": "428069"
  },
  {
    "text": "OK.",
    "start": "428070",
    "end": "428570"
  },
  {
    "text": "So hopefully, you\nnow see the pattern,",
    "start": "428570",
    "end": "431470"
  },
  {
    "text": "having seen it twice for\ntwo different classifiers.",
    "start": "431470",
    "end": "434630"
  },
  {
    "text": "The sklearn, it's-- the pattern,\nit's quite easy to fit many",
    "start": "434630",
    "end": "439453"
  },
  {
    "text": "different classifiers.",
    "start": "439453",
    "end": "440370"
  },
  {
    "text": "So we have a few more to fit.",
    "start": "440370",
    "end": "443000"
  },
  {
    "text": "One is Naive Bayes.",
    "start": "443000",
    "end": "444560"
  },
  {
    "text": "And then we'll wrap up with\nk-nearest neighbors classifier.",
    "start": "444560",
    "end": "448740"
  },
  {
    "text": "So recall from the lectures,\nor you can read in the textbook",
    "start": "448740",
    "end": "454699"
  },
  {
    "text": "that the Naive Bayes-- when\nyou have continuous features,",
    "start": "454700",
    "end": "457280"
  },
  {
    "text": "Naive Bayes is essentially\nquite closely related to QDA.",
    "start": "457280",
    "end": "461870"
  },
  {
    "text": "It's a restricted version of\nQuadraticDiscriminantAnalysis.",
    "start": "461870",
    "end": "465770"
  },
  {
    "text": "In terms of the code,\nagain, it is fit identically",
    "start": "465770",
    "end": "472039"
  },
  {
    "text": "to QDA and LDA.",
    "start": "472040",
    "end": "473690"
  },
  {
    "text": "We call the fit method on our\ntraining features and training",
    "start": "473690",
    "end": "477470"
  },
  {
    "text": "labels.",
    "start": "477470",
    "end": "478100"
  },
  {
    "text": "And then we'll evaluate\nin a similar fashion.",
    "start": "478100",
    "end": "481410"
  },
  {
    "text": "So I mentioned, it's\nquite related to QDA.",
    "start": "481410",
    "end": "484355"
  },
  {
    "start": "484355",
    "end": "486838"
  },
  {
    "text": "Essentially, it just\nimposes the constraint",
    "start": "486838",
    "end": "488630"
  },
  {
    "text": "that the covariance matrices\nare diagonal for QDA,",
    "start": "488630",
    "end": "491450"
  },
  {
    "text": "that's basically the difference\nbetween Naive Bayes and QDA.",
    "start": "491450",
    "end": "494480"
  },
  {
    "text": "So it's a simpler model, has\nfewer parameters than QDA,",
    "start": "494480",
    "end": "498109"
  },
  {
    "text": "but otherwise, quite similar.",
    "start": "498109",
    "end": "499317"
  },
  {
    "text": "OK.",
    "start": "499317",
    "end": "499817"
  },
  {
    "start": "499817",
    "end": "502949"
  },
  {
    "text": "So just like QDA and LDA, there\nare within class parameters",
    "start": "502950",
    "end": "508650"
  },
  {
    "text": "that you can examine as\nthe output of Naive Bayes,",
    "start": "508650",
    "end": "515700"
  },
  {
    "text": "for instance.",
    "start": "515700",
    "end": "518320"
  },
  {
    "text": "The covariance matrix being\ndiagonal means within class,",
    "start": "518320",
    "end": "522299"
  },
  {
    "text": "you only have to store the\nvariance of each feature",
    "start": "522299",
    "end": "525240"
  },
  {
    "text": "to describe the diagonal matrix\nrather than the covariance.",
    "start": "525240",
    "end": "527920"
  },
  {
    "text": "So this variance\nunderscore attribute",
    "start": "527920",
    "end": "530760"
  },
  {
    "text": "has the-- within\nclass covariance",
    "start": "530760",
    "end": "533100"
  },
  {
    "text": "matrices-- the variances of\nthe Lag1 and Lag2 for class 0,",
    "start": "533100",
    "end": "537839"
  },
  {
    "text": "and class 1, or class\ndown, and class up.",
    "start": "537840",
    "end": "540495"
  },
  {
    "start": "540495",
    "end": "542890"
  },
  {
    "text": "OK.",
    "start": "542890",
    "end": "543390"
  },
  {
    "text": "And if you want to\nsee more about what",
    "start": "543390",
    "end": "545670"
  },
  {
    "text": "these attributes are, if you\nlook at the help for the Naive",
    "start": "545670",
    "end": "548850"
  },
  {
    "text": "Bayes object, it will have\nits docstring that describes",
    "start": "548850",
    "end": "553589"
  },
  {
    "text": "what these attributes are.",
    "start": "553590",
    "end": "555210"
  },
  {
    "text": "And that's similar for the\nLDA object and the QDA object",
    "start": "555210",
    "end": "558420"
  },
  {
    "text": "if you're ever curious about\nhow to find their names.",
    "start": "558420",
    "end": "560925"
  },
  {
    "start": "560925",
    "end": "564016"
  },
  {
    "text": "OK.",
    "start": "564017",
    "end": "564517"
  },
  {
    "text": "So let's see how well\nour Naive Bayes predicts.",
    "start": "564517",
    "end": "567870"
  },
  {
    "start": "567870",
    "end": "570630"
  },
  {
    "text": "It's form the confusion\ntable similar fashion,",
    "start": "570630",
    "end": "573810"
  },
  {
    "text": "and we get around 59%.",
    "start": "573810",
    "end": "575940"
  },
  {
    "text": "Very close to LDA.",
    "start": "575940",
    "end": "577230"
  },
  {
    "start": "577230",
    "end": "579810"
  },
  {
    "text": "And they all do.",
    "start": "579810",
    "end": "581310"
  },
  {
    "text": "All these Discriminant\nAnalysis methods",
    "start": "581310",
    "end": "584279"
  },
  {
    "text": "are doing reasonably well.",
    "start": "584280",
    "end": "585745"
  },
  {
    "text": "Yes.",
    "start": "585745",
    "end": "586245"
  },
  {
    "text": "OK.",
    "start": "586245",
    "end": "586745"
  },
  {
    "start": "586745",
    "end": "589410"
  },
  {
    "text": "So our next topic will\nbe k-nearest neighbors.",
    "start": "589410",
    "end": "593660"
  },
  {
    "start": "593660",
    "end": "598000"
  }
]