[
  {
    "text": "Welcome to the section on neural networks.",
    "start": "4700",
    "end": "10360"
  },
  {
    "text": "So a neural network is a nonlinear predictor,",
    "start": "12290",
    "end": "17970"
  },
  {
    "text": "y hat is g Theta of x, which has a particular layered form.",
    "start": "17970",
    "end": "24760"
  },
  {
    "text": "One way to think about a neural network is that it incorporates aspects of feature engineering into the predictor.",
    "start": "24890",
    "end": "34809"
  },
  {
    "text": "So one way of, uh, interpreting this is as automatic feature engineering.",
    "start": "35240",
    "end": "42280"
  },
  {
    "text": "That's one of the strong reasons why neural networks are so popular,",
    "start": "44260",
    "end": "50195"
  },
  {
    "text": "is that for very complicated classification and regression problems,",
    "start": "50195",
    "end": "55955"
  },
  {
    "text": "it's not obvious what the right choice of features is and having, uh,",
    "start": "55955",
    "end": "61610"
  },
  {
    "text": "a system which can automatically determine the right choice of features is very powerful.",
    "start": "61610",
    "end": "69460"
  },
  {
    "text": "Uh, as a consequence of this,",
    "start": "69460",
    "end": "74780"
  },
  {
    "text": "the number of parameters, uh, which, uh, specify the particular neural network can be very large.",
    "start": "74780",
    "end": "85140"
  },
  {
    "text": "In other words, the dimension of the variable Theta. Uh, and that means that p here could be in",
    "start": "85140",
    "end": "92030"
  },
  {
    "text": "the millions or tens or hundreds of millions and,",
    "start": "92030",
    "end": "97280"
  },
  {
    "text": "uh, that can make training difficult and, uh, time-consuming.",
    "start": "97280",
    "end": "105854"
  },
  {
    "text": "So neural networks can easily take weeks to train. Uh, but however, the other side of the coin is that they can perform very well,",
    "start": "105855",
    "end": "117905"
  },
  {
    "text": "uh, especially when you have a lot of data to train them. Uh, the resulting predictor that",
    "start": "117905",
    "end": "126530"
  },
  {
    "text": "you get out after training is extremely hard to interpret. Essentially, it's a- it's a black box.",
    "start": "126530",
    "end": "133605"
  },
  {
    "text": "It takes an x and gives you back an estimate y hat, and how exactly it did that is somewhat mysterious.",
    "start": "133605",
    "end": "143215"
  },
  {
    "text": "Uh, if you compare that with in particular linear predictor,",
    "start": "143215",
    "end": "148804"
  },
  {
    "text": "where y hat is Theta transpose x, then the meaning of the individual entries of Theta is very clear.",
    "start": "148804",
    "end": "157765"
  },
  {
    "text": "It tells us how much increasing certain components of x affects the prediction y hat.",
    "start": "157765",
    "end": "165475"
  },
  {
    "text": "For a neural network, it's extremely difficult to interpret what the particular parameters Theta i actually mean.",
    "start": "165475",
    "end": "176650"
  },
  {
    "text": "So in a, uh, neural network in particular, here we're talking about feedforward neural networks.",
    "start": "182180",
    "end": "189560"
  },
  {
    "text": "There are other types of neural networks which we will see later in the class.",
    "start": "189560",
    "end": "194650"
  },
  {
    "text": "So a feedforward neural network consists of a composition of functions. Y hat is g_3, composed with g_2,",
    "start": "194650",
    "end": "203210"
  },
  {
    "text": "composed with g_1 of x and that's in the case where we have three layers,",
    "start": "203210",
    "end": "208280"
  },
  {
    "text": "but we might have any number of such functions, and those functions are called layers.",
    "start": "208280",
    "end": "214890"
  },
  {
    "text": "We might write this using composition notation, g is g_3 composed with g_2 composed with g_1,",
    "start": "214940",
    "end": "221705"
  },
  {
    "text": "where the composition operator is denoted by a circle.",
    "start": "221705",
    "end": "226740"
  },
  {
    "text": "Uh, some people would call this such a neural network on multi-layer perceptron.",
    "start": "227390",
    "end": "235230"
  },
  {
    "text": "We often write the, uh, predictor composition in terms of individual variables.",
    "start": "239710",
    "end": "247610"
  },
  {
    "text": "We might say z_1 is g_1 of x, then z_2 is g_2 of z_1,",
    "start": "247610",
    "end": "252785"
  },
  {
    "text": "and then y hat is g_3 of z_2 for our three layer example. And each of these vectors z_i is called the activation of the output of layer i,",
    "start": "252785",
    "end": "265085"
  },
  {
    "text": "where layer i is just the function g_i and z_i is a vector.",
    "start": "265085",
    "end": "270255"
  },
  {
    "text": "It has a dimension d_i, which depends on the layer.",
    "start": "270255",
    "end": "276040"
  },
  {
    "text": "And those layer of dimensions need not all be the same and in particular,",
    "start": "276530",
    "end": "283130"
  },
  {
    "text": "sometimes they grow and sometimes they shrink, uh, depending on the application.",
    "start": "283130",
    "end": "288620"
  },
  {
    "text": "We, uh, um, sometimes write z_0 is x,",
    "start": "288620",
    "end": "296835"
  },
  {
    "text": "and d_0 is d, the dimension of the number of, uh, features which are being embedded from the raw data.",
    "start": "296835",
    "end": "305620"
  },
  {
    "text": "And so we still have, uh, a basic feature map which constructs x from u, x is Phi of u,",
    "start": "305620",
    "end": "312660"
  },
  {
    "text": "but then we don't do feature engineering typically with a neural network, instead we allow the layers to provide features which are used by subsequent layers.",
    "start": "312660",
    "end": "325725"
  },
  {
    "text": "In this game, the case of our three layer network, we have z_3 is y hat, the prediction,",
    "start": "325725",
    "end": "333850"
  },
  {
    "text": "and then d_3 is the number of components of y hat, which will be m. So in particular,",
    "start": "333850",
    "end": "341139"
  },
  {
    "text": "that means that the predictor input x and the predictor output y are also considered activations of layers.",
    "start": "341140",
    "end": "349645"
  },
  {
    "text": "And we might visualize this as a simple graph where we have x coming in on the left passing through the function g_1 to generate z_1,",
    "start": "349645",
    "end": "359565"
  },
  {
    "text": "which is passed into g_2 to generate z_2, which is passed into g_3 to generate y hat,",
    "start": "359565",
    "end": "366430"
  },
  {
    "text": "which is just z_3.",
    "start": "366430",
    "end": "368479"
  },
  {
    "text": "These layers have a particular form. Each layer is a composition of a scalar function h within an affine function.",
    "start": "373250",
    "end": "384430"
  },
  {
    "text": "So g_i of z_i minus 1 is h composed with Theta_i transpose 1,",
    "start": "384430",
    "end": "392520"
  },
  {
    "text": "z_i minus 1, and this 1, z_i minus 1. Remember that notation?",
    "start": "392520",
    "end": "398115"
  },
  {
    "text": "That notation means if I have two vectors, a, b,",
    "start": "398115",
    "end": "404120"
  },
  {
    "text": "that's the same as stacking them up on top of each other, and that means that Theta transpose 1,",
    "start": "404120",
    "end": "412815"
  },
  {
    "text": "z means Theta transpose 1_z.",
    "start": "412815",
    "end": "418360"
  },
  {
    "text": "The matrix Theta_i is- has dimensions d_i minus 1 plus 1 by d_i.",
    "start": "422420",
    "end": "430725"
  },
  {
    "text": "It's the parameter for layer i. And, uh, the output of Theta transpose 1_z is- is a vector.",
    "start": "430725",
    "end": "442485"
  },
  {
    "text": "h here is, uh, a scalar activation function. It takes real numbers to real numbers and so the way it is applied to the vector,",
    "start": "442485",
    "end": "451849"
  },
  {
    "text": "the meaning of that notation is that it's applied element-wise. So in particular, if z_1 is equal to h of q,",
    "start": "451850",
    "end": "466845"
  },
  {
    "text": "let me just call it z for the case of argument. Then that means that z_1 is h of q_1,",
    "start": "466845",
    "end": "478380"
  },
  {
    "text": "z_2 is h of q_2, and so on.",
    "start": "478380",
    "end": "483940"
  },
  {
    "text": "So when we have M layers in our neural network, we have m matrices, Theta_1 through Theta_n, and those are",
    "start": "488150",
    "end": "495710"
  },
  {
    "text": "the parameters that we need to choose when training. [NOISE]",
    "start": "495710",
    "end": "506210"
  },
  {
    "text": "And we can explicitly divide up the components of the matrix Theta i as follows.",
    "start": "506210",
    "end": "514125"
  },
  {
    "text": "We'll write Theta i is Alpha i Beta i, like this.",
    "start": "514125",
    "end": "519630"
  },
  {
    "text": "And here, Alpha i is, uh, uh, is a, uh,",
    "start": "519630",
    "end": "527295"
  },
  {
    "text": "is a- a row vector.",
    "start": "527295",
    "end": "532305"
  },
  {
    "text": "So that's Alpha i 1, Alpha i 2, all way up to Alpha i_ di,",
    "start": "532305",
    "end": "541345"
  },
  {
    "text": "where each of these entries are scalars. But Beta i is a matrix and its, uh,",
    "start": "541345",
    "end": "550720"
  },
  {
    "text": "its entries are Beta i 1 up to Beta i j,",
    "start": "550720",
    "end": "557394"
  },
  {
    "text": "and each one of these is a vector of dimension d i minus 1.",
    "start": "557395",
    "end": "563900"
  },
  {
    "text": "Now, if we take that notation and then we say, well, we're going to multiply Theta transpose by 1z,",
    "start": "563970",
    "end": "573160"
  },
  {
    "text": "so in particular, for the ith layer, we're gonna take Theta i transpose multiplied by 1 z i minus 1.",
    "start": "573160",
    "end": "582365"
  },
  {
    "text": "That works out to be this vector Alpha,",
    "start": "582365",
    "end": "587385"
  },
  {
    "text": "plus a vector whose components are simply each of the form Beta transpose z i minus 1.",
    "start": "587385",
    "end": "597730"
  },
  {
    "text": "So each one of these entries of that vector is the linear part,",
    "start": "597730",
    "end": "604524"
  },
  {
    "text": "and then there's the constant part here. So just as we might expect for an affine function.",
    "start": "604525",
    "end": "613570"
  },
  {
    "text": "An affine function in general takes the form A times z plus- let me not call them that,",
    "start": "613570",
    "end": "625570"
  },
  {
    "text": "let me call them, uh, c plus D times z,",
    "start": "625570",
    "end": "634180"
  },
  {
    "text": "where D is a matrix and c is a vector. And this is our c and this is D z.",
    "start": "634180",
    "end": "644150"
  },
  {
    "text": "So then the layer map z i is g i of z i minus 1,",
    "start": "645990",
    "end": "652089"
  },
  {
    "text": "means that we take each of the components of this affine function.",
    "start": "652090",
    "end": "657955"
  },
  {
    "text": "Here, Alpha i j plus Beta i j transpose z i minus 1,",
    "start": "657955",
    "end": "664015"
  },
  {
    "text": "and pass that through the activation function h. And this sort of function- this sort of function right here,",
    "start": "664015",
    "end": "676149"
  },
  {
    "text": "it takes a vector z i minus 1 as an input,",
    "start": "676150",
    "end": "682075"
  },
  {
    "text": "and returns a scalar which is the jth component of z i.",
    "start": "682075",
    "end": "687520"
  },
  {
    "text": "Let's call it a neuron. People often write those like this.",
    "start": "687520",
    "end": "696235"
  },
  {
    "text": "What's coming out here is z i j, and then we have coming in",
    "start": "696235",
    "end": "704180"
  },
  {
    "text": "components of z i minus 1 1,",
    "start": "705210",
    "end": "713170"
  },
  {
    "text": "z i minus 1 2, z i minus 1 3,",
    "start": "713170",
    "end": "719300"
  },
  {
    "text": "and that's a single neuron. And uh, in- our layer has many such neurons,",
    "start": "719310",
    "end": "727580"
  },
  {
    "text": "each one of which is giving out a different component. This will be z i 2, this would be z i 1.",
    "start": "728160",
    "end": "739300"
  },
  {
    "text": "And this is fed by the same inputs.",
    "start": "739300",
    "end": "748614"
  },
  {
    "text": "And so this would be uh, a layer which has two outputs and three inputs,",
    "start": "748614",
    "end": "755905"
  },
  {
    "text": "and it consists of two neurons. Now these constant terms in the affine functions are called the vector of constant terms,",
    "start": "755905",
    "end": "767260"
  },
  {
    "text": "is called the bias of the neuron. And so in this layer which has two neurons,",
    "start": "767260",
    "end": "774490"
  },
  {
    "text": "there are two scalars, Alpha, i 1 and Alpha i 2. Those are the two biases at that layer,",
    "start": "774490",
    "end": "781495"
  },
  {
    "text": "and the Betas are the input weights.",
    "start": "781495",
    "end": "786200"
  },
  {
    "text": "Now. Activation functions, there are two commonly used choices for activation functions.",
    "start": "791130",
    "end": "798174"
  },
  {
    "text": "Um, uh, they are chosen to be nonlinear. In particular, if they were linear,",
    "start": "798174",
    "end": "805509"
  },
  {
    "text": "then g Theta would just be an affine function of z, which will be a linear predictor for a single layer.",
    "start": "805510",
    "end": "812620"
  },
  {
    "text": "And in particular, if I compose a single layer g i,",
    "start": "812620",
    "end": "818805"
  },
  {
    "text": "which is linear with the next layer g i plus 1. Well, I'm going to end up with this one non-linear layer,",
    "start": "818805",
    "end": "829360"
  },
  {
    "text": "where the two with the- the affine part of the combined layer is simply a composition of the two affine parts.",
    "start": "829360",
    "end": "838630"
  },
  {
    "text": "If I put a nonlinear function in place, then there is a difference between g1 composure to g2,",
    "start": "838630",
    "end": "846279"
  },
  {
    "text": "which is a function which cannot be expressed as a single layer.",
    "start": "846279",
    "end": "852620"
  },
  {
    "text": "So the most common activation functions are called the ReLu. We've also used this notation,",
    "start": "852690",
    "end": "859945"
  },
  {
    "text": "a plus the positive part of the number a or the maximum a 0.",
    "start": "859945",
    "end": "866890"
  },
  {
    "text": "And it's- it's this function which we've seen before. It's 0 when the input is negative,",
    "start": "866890",
    "end": "874269"
  },
  {
    "text": "and a when the input is positive. The other very common activation function is this function,",
    "start": "874270",
    "end": "884485"
  },
  {
    "text": "e to the a on 1 plus e to the a, which is a smooth function,",
    "start": "884485",
    "end": "893035"
  },
  {
    "text": "which varies between 0 and 1. And 0, it's exactly a half.",
    "start": "893035",
    "end": "901645"
  },
  {
    "text": "As a tends to infinity, it tends to 1, as a tends to minus infinity,",
    "start": "901645",
    "end": "907750"
  },
  {
    "text": "it tends to 0. It's a scaled hyperbolic tangent function. [NOISE] We often draw",
    "start": "907750",
    "end": "921880"
  },
  {
    "text": "explicitly the neurons in a neural network as a graph.",
    "start": "921880",
    "end": "927250"
  },
  {
    "text": "Here's a- here's a- here's such a graph. Here we have, uh, a neural network which has three layers.",
    "start": "927250",
    "end": "933640"
  },
  {
    "text": "And um, we have uh, this is the output.",
    "start": "933640",
    "end": "939460"
  },
  {
    "text": "This vector here is uh,",
    "start": "939460",
    "end": "946975"
  },
  {
    "text": "the feature vector x. Then we have z1 is this vector.",
    "start": "946975",
    "end": "956410"
  },
  {
    "text": "And so this would be x which is equal to g0. We map that through- through g1 to get z1.",
    "start": "956410",
    "end": "968769"
  },
  {
    "text": "Here we have z2, which we map z1 through g2 to get z2.",
    "start": "968770",
    "end": "977725"
  },
  {
    "text": "And then we finally map through g3 to get Y hat.",
    "start": "977725",
    "end": "983480"
  },
  {
    "text": "Um, and we can interpret on this graph,",
    "start": "983580",
    "end": "988734"
  },
  {
    "text": "each of the edges,",
    "start": "988734",
    "end": "993744"
  },
  {
    "text": "the lines that connect vertices within the graph as uh,",
    "start": "993744",
    "end": "999640"
  },
  {
    "text": "having a corresponding parameter. So each one of them would have a corresponding entry in the Theta matrix.",
    "start": "999640",
    "end": "1009210"
  },
  {
    "text": "In particular, here, we're going to see, well, let's look at the one that's labeled Theta 1, 2,",
    "start": "1009210",
    "end": "1014980"
  },
  {
    "text": "4 relates the fourth component of z1 to the first component of z0, which is x.",
    "start": "1014980",
    "end": "1025709"
  },
  {
    "text": "And so each edge has a Theta associated with it. But there were also Thetas that live inside the nodes which provide the biases.",
    "start": "1025710",
    "end": "1035184"
  },
  {
    "text": "So it's only the weight terms that are associated with edges of this graph.",
    "start": "1035185",
    "end": "1041590"
  },
  {
    "text": "Um, so each vertex here is- it's a component of an activation,",
    "start": "1043610",
    "end": "1048920"
  },
  {
    "text": "and the edges are the individual weights.",
    "start": "1048920",
    "end": "1052680"
  },
  {
    "text": "Uh, here's an example of the sort of thing you see if you construct,",
    "start": "1055870",
    "end": "1060889"
  },
  {
    "text": "uh, randomly chosen set of parameters. You see one of the function that looks something like this.",
    "start": "1060889",
    "end": "1067130"
  },
  {
    "text": "This is, uh, right here is- we have, uh, a three-layer neural network, uh,",
    "start": "1067130",
    "end": "1072590"
  },
  {
    "text": "with two variables, x_1 and x_2 at the input. And, uh, as a result here we have Theta as,",
    "start": "1072590",
    "end": "1080360"
  },
  {
    "text": "three, uh, three rows. The first row here has the biases,",
    "start": "1080360",
    "end": "1090005"
  },
  {
    "text": "here we have some more biases, here are biases, and the remaining entries are weights.",
    "start": "1090005",
    "end": "1096845"
  },
  {
    "text": "So the second, uh- the output of the first layer,",
    "start": "1096845",
    "end": "1102289"
  },
  {
    "text": "the input to the second layer is a vector which has dimension four.",
    "start": "1102290",
    "end": "1107570"
  },
  {
    "text": "Then, uh, the output of the subsequent layer has dimension 2 here.",
    "start": "1107570",
    "end": "1117005"
  },
  {
    "text": "And then we have an output which has dimension 1, which is y-hat.",
    "start": "1117005",
    "end": "1122105"
  },
  {
    "text": "And so the resulting functions are mapped from the two dimensions here to the one dimension at the output.",
    "start": "1122105",
    "end": "1135450"
  },
  {
    "text": "Uh, it's the same terminology that's very commonly used for neural networks.",
    "start": "1145900",
    "end": "1151250"
  },
  {
    "text": "If you have an M layer network, then layers 1 to M minus 1 are called hidden layers.",
    "start": "1151250",
    "end": "1157940"
  },
  {
    "text": "Uh, layer M is called the output layer. Very often, if you're doing regression,",
    "start": "1157940",
    "end": "1164735"
  },
  {
    "text": "you do not use an activation function on the output layer,",
    "start": "1164735",
    "end": "1169790"
  },
  {
    "text": "or more specifically use the identity activation function in the output layer, h of a is a. Um,",
    "start": "1169790",
    "end": "1176690"
  },
  {
    "text": "in the case of regression you can see why that is if you look back at the activation functions we typically use.",
    "start": "1176690",
    "end": "1182075"
  },
  {
    "text": "If the output layer was using, for example, the sigmoid activation function,",
    "start": "1182075",
    "end": "1187730"
  },
  {
    "text": "then it would only be able to generate output values or predictions between 0 and 1,",
    "start": "1187730",
    "end": "1194179"
  },
  {
    "text": "which of course won't work if we were trying to predict y's which didn't behind that range.",
    "start": "1194180",
    "end": "1200000"
  },
  {
    "text": "Or if we were using ReLU, then, uh, we'd only be able to predict non-negative values of y-hat.",
    "start": "1200000",
    "end": "1208980"
  },
  {
    "text": "Uh, the number of layers M is called the depth of the network.",
    "start": "1210130",
    "end": "1219010"
  },
  {
    "text": "Uh, and people refer to networks which have a large M and large varies,",
    "start": "1219010",
    "end": "1225890"
  },
  {
    "text": "but at least 3 would be typical, um, as deep learning. Um, very often for neural networks which are used for say,",
    "start": "1225890",
    "end": "1235025"
  },
  {
    "text": "image classification, we may have 15, 20 layers or many more sometimes, um.",
    "start": "1235025",
    "end": "1243480"
  },
  {
    "text": "Now, when we are doing training with a neural network, we do regularized empirical risk minimization,",
    "start": "1248260",
    "end": "1256730"
  },
  {
    "text": "as we've seen before. We pick the layer parameters,",
    "start": "1256730",
    "end": "1263120"
  },
  {
    "text": "Theta_1 through Theta_m, to minimize the empirical risk,",
    "start": "1263120",
    "end": "1268970"
  },
  {
    "text": "with the regularization term added. So the empirical risk is, uh, as always,",
    "start": "1268970",
    "end": "1279275"
  },
  {
    "text": "the average loss function evaluated on the training data set, where here g_Theta is the neural network map from inputs x to output y-hat.",
    "start": "1279275",
    "end": "1290929"
  },
  {
    "text": "Now the regularization term, uh, doesn't regularize the bias parameters,",
    "start": "1290930",
    "end": "1297380"
  },
  {
    "text": "uh, the Alpha_i_j's, the constant terms in each of the layers. It only regularizes the,",
    "start": "1297380",
    "end": "1305105"
  },
  {
    "text": "uh, the weight terms. Um, and his is because, uh, we have, uh,",
    "start": "1305105",
    "end": "1311345"
  },
  {
    "text": "no need to regularize terms which don't affect the sensitivity of the network.",
    "start": "1311345",
    "end": "1317970"
  },
  {
    "text": "Uh, common regularizes if sum of squares is very common, l_1 is also very common.",
    "start": "1319170",
    "end": "1325510"
  },
  {
    "text": "Um, and in particular with l_1, we can expect to see sparsification of",
    "start": "1325510",
    "end": "1333310"
  },
  {
    "text": "the- of the neural network and some of the weights will end up being 0.",
    "start": "1333310",
    "end": "1338740"
  },
  {
    "text": "And that allows us to do what's called pruning the neural network, simply re- removing entries which have zero weight and then re-training.",
    "start": "1338740",
    "end": "1349155"
  },
  {
    "text": "That might correspond to removing entire neurons or just removing particular paths through the network.",
    "start": "1349155",
    "end": "1356720"
  },
  {
    "text": "Er, typically, for the- when you're using a neural network predictor,",
    "start": "1356720",
    "end": "1362840"
  },
  {
    "text": "you cannot minimize the regularized empirical risk exactly.",
    "start": "1362840",
    "end": "1368645"
  },
  {
    "text": "Um, uh, the- the stec- special case of a convex loss function,",
    "start": "1368645",
    "end": "1375560"
  },
  {
    "text": "a convex regularizer and a linear predictor. That's, uh, a very special case in which one can exactly find the optimal solution.",
    "start": "1375560",
    "end": "1388910"
  },
  {
    "text": "For the neural network predictor, then even if you have a- a quadratic loss and a quadratic regularizer,",
    "start": "1388910",
    "end": "1396260"
  },
  {
    "text": "there are algorithm- there are no algorithms what will solve the problem exactly.",
    "start": "1396260",
    "end": "1402845"
  },
  {
    "text": "Um, oh, so training algorithms instead find approximately optimal solutions.",
    "start": "1402845",
    "end": "1408544"
  },
  {
    "text": "Uh, and these might be local- locally optimal, or they might be close to locally optimal.",
    "start": "1408545",
    "end": "1414125"
  },
  {
    "text": "And we're gonna see what methods you use to do that. These are iterative training methods.",
    "start": "1414125",
    "end": "1419660"
  },
  {
    "text": "Uh, and these can take a long time.",
    "start": "1419660",
    "end": "1424980"
  },
  {
    "text": "Oh, and here's a particular example. Uh, this neural network has,",
    "start": "1425770",
    "end": "1431149"
  },
  {
    "text": "I think four, or five layers. It's just got two-dimensional input and one-dimensional output,",
    "start": "1431150",
    "end": "1437510"
  },
  {
    "text": "and it was trained on roughly 100 data points, some of which you can see in the plot. And you can see that it's done a interesting job at fitting it.",
    "start": "1437510",
    "end": "1445835"
  },
  {
    "text": "Um, of course, we can't see all the data points because some of the points are underneath the surface. And uh, uh, um, it's- uh,",
    "start": "1445835",
    "end": "1456350"
  },
  {
    "text": "it's hard to have any intuition for what either the meaning of the parameters is or how good a fit this is in the space of our,",
    "start": "1456350",
    "end": "1464990"
  },
  {
    "text": "uh, possibilities associated with a particular size of the neural network that we have.",
    "start": "1464990",
    "end": "1470059"
  },
  {
    "text": "Uh, all we can do to evaluate how good it is is validation.",
    "start": "1470060",
    "end": "1475830"
  },
  {
    "text": "Uh, in Julia, this,",
    "start": "1477430",
    "end": "1483440"
  },
  {
    "text": "uh, looks like this. Uh, this is a- a- a glimpse at",
    "start": "1483440",
    "end": "1488780"
  },
  {
    "text": "the neural network regression function that was used to train the previous example. Uh, some things to notice about it.",
    "start": "1488780",
    "end": "1497885"
  },
  {
    "text": "Um, uh, this here defines the network structure.",
    "start": "1497885",
    "end": "1503630"
  },
  {
    "text": "So it says that, um, uh, we have, uh, three layers.",
    "start": "1503630",
    "end": "1511715"
  },
  {
    "text": "The first layer, layer",
    "start": "1511715",
    "end": "1519820"
  },
  {
    "text": "1 has d inputs and 10 outputs,",
    "start": "1519820",
    "end": "1529534"
  },
  {
    "text": "and a ReLU activation.",
    "start": "1529535",
    "end": "1532530"
  },
  {
    "text": "The second layer has 10 inputs and 10 outputs,",
    "start": "1534910",
    "end": "1541970"
  },
  {
    "text": "and also a ReLU activation. And the third layer has 10 inputs and m outputs,",
    "start": "1541970",
    "end": "1550580"
  },
  {
    "text": "where m in this case is 1 and d in this case is 2. And it has identity activation because we're solving a regression problem.",
    "start": "1550580",
    "end": "1558870"
  },
  {
    "text": "Now, what- what these- these um, these functions here, the function Dense actually returns a layer function.",
    "start": "1560320",
    "end": "1571085"
  },
  {
    "text": "So when you call it Dense d, 10, the activation function relu,",
    "start": "1571085",
    "end": "1577115"
  },
  {
    "text": "it will return for you a function f. We could just call it g_1 equals.",
    "start": "1577115",
    "end": "1583400"
  },
  {
    "text": "And then we'll get g_2 is dense this, and g_3 is dense or whatever we called it.",
    "start": "1583400",
    "end": "1593019"
  },
  {
    "text": "Then we might say our overall neural network, let's call it, uh, the predicted g. We would like to be g_3 composed with g_2, composed with g_1.",
    "start": "1593020",
    "end": "1606065"
  },
  {
    "text": "Now, in Julia, you can actually type this, that's totally fine. If you're- if you happen to know where to get",
    "start": "1606065",
    "end": "1613850"
  },
  {
    "text": "the Unicode symbol for the circle on your keyboard, and if like me, you don't,",
    "start": "1613850",
    "end": "1619880"
  },
  {
    "text": "then you can just write chain of g_1, g_2, and g_3.",
    "start": "1619880",
    "end": "1629120"
  },
  {
    "text": "And that's exactly what we're doing here. So that's just composing functions. And that means that if we want to apply the resulting predictor to an input,",
    "start": "1629120",
    "end": "1640434"
  },
  {
    "text": "we just take g, which in this case we called model, let me just call it model.",
    "start": "1640435",
    "end": "1646640"
  },
  {
    "text": "And we could do model of a vector x, and that would be y hat.",
    "start": "1649430",
    "end": "1656970"
  },
  {
    "text": "And so y hat is model of x, will execute the neural network predictor and return for you that prediction.",
    "start": "1656970",
    "end": "1666760"
  },
  {
    "text": "Now, the parameters, the Thetas,",
    "start": "1666760",
    "end": "1671930"
  },
  {
    "text": "uh, they're not called Alpha and Beta in- in flux, and here we're using the flux library.",
    "start": "1671930",
    "end": "1678414"
  },
  {
    "text": "So before we run this code, we need to type using flux.",
    "start": "1678415",
    "end": "1684700"
  },
  {
    "text": "The top there, uh, the parameters are stored inside the layer functions,",
    "start": "1685130",
    "end": "1693270"
  },
  {
    "text": "and the way one accesses those is the following. So in particular, I can- once I start- once I've composed model is chain G_1,",
    "start": "1693270",
    "end": "1702480"
  },
  {
    "text": "G_2, G_3, I can get back the functions. I might get them back by saying g_1 is equal to model brackets 1.",
    "start": "1702480",
    "end": "1713415"
  },
  {
    "text": "And that accessing that- the first component of model,",
    "start": "1713415",
    "end": "1719310"
  },
  {
    "text": "even though model is not a vector model, it's really a composition of functions.",
    "start": "1719310",
    "end": "1724760"
  },
  {
    "text": "It's been set up inside Julia in such a way that when I access the first component model,",
    "start": "1724760",
    "end": "1730310"
  },
  {
    "text": "what I get back is the first function, the first layer of my neural network.",
    "start": "1730310",
    "end": "1735865"
  },
  {
    "text": "And then I can access its bias and its weights.",
    "start": "1735865",
    "end": "1740985"
  },
  {
    "text": "Its bias is called g_1.b and its weights are called g_1.w.",
    "start": "1740985",
    "end": "1746040"
  },
  {
    "text": "And so g_1.b is a vector, we call the vector Alpha on the slides,",
    "start": "1746040",
    "end": "1751710"
  },
  {
    "text": "and g_1.w is a matrix, we called the matrix Beta on the slides.",
    "start": "1751710",
    "end": "1757544"
  },
  {
    "text": "And so you can see here that, uh, we have a regularization function which is constructing,",
    "start": "1757545",
    "end": "1766425"
  },
  {
    "text": "taking the norm squared of the weights of each of the layers and adding those up.",
    "start": "1766425",
    "end": "1772725"
  },
  {
    "text": "And that, uh- that's our regularizer. When we want to construct a prediction,",
    "start": "1772725",
    "end": "1779684"
  },
  {
    "text": "we just call the model function on x that returns us back to the prediction.",
    "start": "1779685",
    "end": "1786180"
  },
  {
    "text": "In particular, if I have an x and a y, to- to, uh, correspond to a particular record,",
    "start": "1786180",
    "end": "1792960"
  },
  {
    "text": "then I can compute predict x- predict y- predict y of x minus y, and that's the error.",
    "start": "1792960",
    "end": "1801330"
  },
  {
    "text": "And I might take the norm squared of that to compute the quadratic loss function.",
    "start": "1801330",
    "end": "1806669"
  },
  {
    "text": "And here's my overall objective that I'd like to minimize. And this function train does the hard work of actually",
    "start": "1806670",
    "end": "1815130"
  },
  {
    "text": "minimizing over the parameters and over the data, the cost function.",
    "start": "1815130",
    "end": "1820200"
  },
  {
    "text": "Those are the things it takes. The cost function, the parameters, that's a special function in flux which",
    "start": "1820200",
    "end": "1826740"
  },
  {
    "text": "extracts the parameters out of each of the layers. And, uh, there's the data which it iterates over.",
    "start": "1826740",
    "end": "1834225"
  },
  {
    "text": "And here we return the model. And so we would call nnregression with an x and a y,",
    "start": "1834225",
    "end": "1843240"
  },
  {
    "text": "and a regularization parameter Lambda. It will return for us a model.",
    "start": "1843240",
    "end": "1848340"
  },
  {
    "text": "We can then compute the corresponding predictions ever our dataset.",
    "start": "1848340",
    "end": "1855330"
  },
  {
    "text": "We have a function here called predictall. It does that. You give it a model and an X,",
    "start": "1855330",
    "end": "1861870"
  },
  {
    "text": "which is a data matrix, and it iterates over each row of the matrix X,",
    "start": "1861870",
    "end": "1867600"
  },
  {
    "text": "calls tthat x, and calls model(x) on it to give us a corresponding y hat.",
    "start": "1867600",
    "end": "1874695"
  },
  {
    "text": "And then it stacks all those up to re- return for us a matrix which we might call capital",
    "start": "1874695",
    "end": "1880980"
  },
  {
    "text": "Y hat would be predictall- predictall",
    "start": "1880980",
    "end": "1888885"
  },
  {
    "text": "of model X.",
    "start": "1888885",
    "end": "1894240"
  },
  {
    "text": "And so if we want to compute the- the RMS error, for example, of our predictions, we call predictall on our model,",
    "start": "1894240",
    "end": "1901634"
  },
  {
    "text": "on our training data, and we compare that with our true training Y target variables and we compute the RMSE.",
    "start": "1901634",
    "end": "1912700"
  },
  {
    "text": "And so Julia's flux library is really quite efficient and clean at,",
    "start": "1913040",
    "end": "1919230"
  },
  {
    "text": "uh, allowing us to construct neural networks. And you can see that we can construct neural networks of any size we want",
    "start": "1919230",
    "end": "1926040"
  },
  {
    "text": "and, uh, uh- and train them.",
    "start": "1926040",
    "end": "1930940"
  },
  {
    "text": "Of course, I haven't told you yet how the training function works. And we're going to defer that to",
    "start": "1932150",
    "end": "1937860"
  },
  {
    "text": "discussion of exactly what the train- how the training function works until the last few lectures of the class.",
    "start": "1937860",
    "end": "1946710"
  },
  {
    "text": "And- but for now, we're going to use the training function provided with, um- provided with, uh,",
    "start": "1946710",
    "end": "1953325"
  },
  {
    "text": "Julia, provided with flux.",
    "start": "1953325",
    "end": "1956019"
  },
  {
    "text": "Now, one way to think about neural networks is that they really have a very similar form to the feature engineering pipeline.",
    "start": "1962600",
    "end": "1970154"
  },
  {
    "text": "We start with an x and we carry out a sequence of transformations or mappings to that x,",
    "start": "1970155",
    "end": "1976860"
  },
  {
    "text": "um, and eventually, we come out with a y hat.",
    "start": "1976860",
    "end": "1982635"
  },
  {
    "text": "Um, the di- distinction is- is that feature engineering mappings are chosen by hand.",
    "start": "1982635",
    "end": "1988275"
  },
  {
    "text": "They typically don't have any parameters or have very few parameters, and you can interpret what they mean.",
    "start": "1988275",
    "end": "1993975"
  },
  {
    "text": "Um, we might say predict, we might- can do feature engineering by taking the product of two features to construct a new one.",
    "start": "1993975",
    "end": "2001085"
  },
  {
    "text": "And we know very well that if we take the product of two features, then that product is going to be large when both of the individual components are large.",
    "start": "2001085",
    "end": "2010020"
  },
  {
    "text": "Um, in- in contrast, the neural network mappings, well, those are a specific form that,",
    "start": "2010120",
    "end": "2017240"
  },
  {
    "text": "uh, we're not choosing by hand. It's just a- it's just a category that we have,",
    "start": "2017240",
    "end": "2023240"
  },
  {
    "text": "um, and we have lots of parameters. Um, and we're not choosing those by hand at all.",
    "start": "2023240",
    "end": "2028760"
  },
  {
    "text": "The training processes is choosing the parameters, which is going to determine the stru- the resulting map.",
    "start": "2028760",
    "end": "2036215"
  },
  {
    "text": "Um, and so one way to think about neural networks is that they're doing data-driven automatic feature engineering.",
    "start": "2036215",
    "end": "2045180"
  },
  {
    "text": "So one of the very common ways nowadays of using neural networks is to use what's called pre- trained neural networks.",
    "start": "2047800",
    "end": "2056315"
  },
  {
    "text": "Um, one, uh, trains a neural network to predict some particular target variable.",
    "start": "2056315",
    "end": "2062524"
  },
  {
    "text": "Um, so say, for example, one is trying to distinguish, uh, different, uh, uh,",
    "start": "2062525",
    "end": "2070685"
  },
  {
    "text": "features in the road from images for a self-driving car, and one has trained a neural network to classify",
    "start": "2070685",
    "end": "2078304"
  },
  {
    "text": "a whole bunch of different road sign types and a bunch of different vehicle types, and bicycles, and construction,",
    "start": "2078305",
    "end": "2084740"
  },
  {
    "text": "and traffic cones, and all the other things one sees on the road. And that training process would be done with",
    "start": "2084740",
    "end": "2091220"
  },
  {
    "text": "a very large dataset and take some considerable time. Um, and then what you do is you say,",
    "start": "2091220",
    "end": "2097129"
  },
  {
    "text": "well, I've got this neural network. It's very large. We're going to fix the parameters and take that la- the output of",
    "start": "2097130",
    "end": "2104060"
  },
  {
    "text": "the last hidden layer and use those as features to do some other prediction.",
    "start": "2104060",
    "end": "2110750"
  },
  {
    "text": "So after doing all that training, we suddenly realize that actually what we need to do is we also need to be able to distinguish things we hadn't thought of before,",
    "start": "2110750",
    "end": "2118880"
  },
  {
    "text": "a particular type of road sign or a particular type of, uh, traffic obstacle.",
    "start": "2118880",
    "end": "2125089"
  },
  {
    "text": "Well, then we don't retrain the parameters in the neural network that we have.",
    "start": "2125090",
    "end": "2132980"
  },
  {
    "text": "Instead, what we do is we retrain only the last layer of the neural network,",
    "start": "2132980",
    "end": "2140900"
  },
  {
    "text": "or we simply take all of the outputs of the first n minus 1- m minus 1 layers of",
    "start": "2140900",
    "end": "2149630"
  },
  {
    "text": "our neural network and feed those in as features to some new neural network,",
    "start": "2149630",
    "end": "2156079"
  },
  {
    "text": "or some new predictor, or some other form. And this actually works quite well,",
    "start": "2156080",
    "end": "2163325"
  },
  {
    "text": "even if one is training for quite a different task. Um, if one has trained a neural network to do, for example,",
    "start": "2163325",
    "end": "2170180"
  },
  {
    "text": "one type of image classification, then we're gonna often need to do a different type of image classification. Uh, this is called pre-trained neural networks.",
    "start": "2170180",
    "end": "2179600"
  },
  {
    "text": "Uh, we saw two examples of these in the earlier sections when we looked at VGG 16 for images,",
    "start": "2179600",
    "end": "2187309"
  },
  {
    "text": "and we looked at Word2Vec for English words. Both of those were pre-trained neural networks and we viewed them there as feature maps.",
    "start": "2187310",
    "end": "2196175"
  },
  {
    "text": "And of course, if we're not training their parameters, that's precisely what they are.",
    "start": "2196175",
    "end": "2201090"
  },
  {
    "text": "So let's summarize. Um, neural networks,",
    "start": "2207910",
    "end": "2213200"
  },
  {
    "text": "training needs a lot of data. Uh, it can be difficult and it can take a great deal of time.",
    "start": "2213200",
    "end": "2219500"
  },
  {
    "text": "And the resulting neural networks are not interpretable. However, they often work really well, uh,",
    "start": "2219500",
    "end": "2228305"
  },
  {
    "text": "better than anything else for many problems, in particular, image classification.",
    "start": "2228305",
    "end": "2234180"
  },
  {
    "text": "Uh, the, uh, best neural networks far outstrip any other methods that we have.",
    "start": "2234180",
    "end": "2241619"
  },
  {
    "text": "Um, and, uh, one way to think about them is that they're, uh, automatic feature engineering.",
    "start": "2241840",
    "end": "2250710"
  }
]