[
  {
    "start": "0",
    "end": "5420"
  },
  {
    "text": "All right, welcome everyone. Again, we have a very full day. The plan is to finish up our\nreview of core information",
    "start": "5420",
    "end": "14030"
  },
  {
    "text": "retrieval stuff. The focus will be on neural\ninformation retrieval where a lot of the\naction is these days.",
    "start": "14030",
    "end": "20689"
  },
  {
    "text": "I have then a few\ndata sets to show you, and then I'm going to\nturn it over to Sid and Sid is going to\nhelp us talk again",
    "start": "20690",
    "end": "27800"
  },
  {
    "text": "about how to build\nfantastic language models. So let's dive in. We'll start by using\nour big handout here,",
    "start": "27800",
    "end": "36530"
  },
  {
    "text": "information retrieval. And so here we are. And we are going to\nskip-- ah, that's right.",
    "start": "36530",
    "end": "42019"
  },
  {
    "text": "Ahead, I have a\ncouple more metrics that I wanted to show you. So let's start there. So last time we talked about how\nassessment in the space of IR",
    "start": "42020",
    "end": "50570"
  },
  {
    "text": "should be multi-dimensional. And we've been\nfocused on accuracy, but I will make amends.",
    "start": "50570",
    "end": "56030"
  },
  {
    "text": "We are going to\ncircle back and talk about these other dimensions\nwhich I regard as absolutely",
    "start": "56030",
    "end": "61100"
  },
  {
    "text": "crucial in this space. But with that said, we did\ndive in to different metrics.",
    "start": "61100",
    "end": "66720"
  },
  {
    "text": "We talked about success\nand reciprocal rank. Success, you should think of\nas just saying for my chosen K,",
    "start": "66720",
    "end": "74720"
  },
  {
    "text": "is there a star above me? That is, is there a\nrelevant document above K?",
    "start": "74720",
    "end": "80270"
  },
  {
    "text": "So it's a very\ncoarse-grained measure. So this one here if\nwe set success at 2,",
    "start": "80270",
    "end": "85670"
  },
  {
    "text": "D1 has success because there\nis a star at 2 or above, D2,",
    "start": "85670",
    "end": "91280"
  },
  {
    "text": "that ranking also\nhas a success of 1 because there is a\nstar at 2 or above, and for D3 gets a success of 0.",
    "start": "91280",
    "end": "99990"
  },
  {
    "text": "And you can see already\nthat it's coarse-grained because D1 and D2\nare differentiated",
    "start": "99990",
    "end": "105330"
  },
  {
    "text": "in some intuitive\nsense, but here they both got a\nsuccess score of 1.",
    "start": "105330",
    "end": "110440"
  },
  {
    "text": "Reciprocal rank is a little\nbit better in the sense that it's more or less just\nregistering whether there's",
    "start": "110440",
    "end": "116980"
  },
  {
    "text": "a star at or above\nK, except now we are sensitive to the\ntopmost ranked one.",
    "start": "116980",
    "end": "122980"
  },
  {
    "text": "So for example, D1 here\nhas an RR at 2 of 1 because there is a\nstar in first place,",
    "start": "122980",
    "end": "129789"
  },
  {
    "text": "whereas D2 has 1 over 2\nbecause the first star is",
    "start": "129789",
    "end": "134830"
  },
  {
    "text": "in second place, and then\nD3 still gets its for 0.",
    "start": "134830",
    "end": "140290"
  },
  {
    "text": "So pretty coarse-grained\nbut very intuitive. And sometimes success\nand RR are good metrics",
    "start": "140290",
    "end": "146140"
  },
  {
    "text": "in the sense that you just\nwant to know for your chosen K whether you hit the mark,\nwhether you got a star.",
    "start": "146140",
    "end": "152410"
  },
  {
    "text": "And especially if you only\nhave one relevant document per query, you might as\nwell use these metrics,",
    "start": "152410",
    "end": "158560"
  },
  {
    "text": "and then RR will just be\na little bit more nuanced. We also talked about\nprecision and recall,",
    "start": "158560",
    "end": "164140"
  },
  {
    "text": "the classic accuracy style\nmetrics in this space. The differentiator here\nfrom the previous ones",
    "start": "164140",
    "end": "170590"
  },
  {
    "text": "is that these are going to be\nsensitive to multiple stars. So if you have more\nthan one document that's irrelevant\nto your query, you",
    "start": "170590",
    "end": "178360"
  },
  {
    "text": "will be able to detect that. So we have this notion\nof a return value that is just the set of\ndocuments K or above.",
    "start": "178360",
    "end": "185710"
  },
  {
    "text": "And then the relevant documents,\nthose are the ones with stars. And precision is\nsaying for my chosen K",
    "start": "185710",
    "end": "192550"
  },
  {
    "text": "what percentage of the things\nat or above K are relevant.",
    "start": "192550",
    "end": "197780"
  },
  {
    "text": "And that's precision in the\nsense that if you picked K you're looking at\nthe set of documents and you want to know how many\nof them have stars relative",
    "start": "197780",
    "end": "204709"
  },
  {
    "text": "to the total, or the\nreverse of precision would be like which ones\nare imprecise as predictions",
    "start": "204710",
    "end": "211400"
  },
  {
    "text": "because there's no star there. And then recall is kind\nof the dual of that. And it says for my chosen\nK how many of the stars",
    "start": "211400",
    "end": "219470"
  },
  {
    "text": "made it up to K or above? And the opposite of that\nwould be how many stars",
    "start": "219470",
    "end": "224750"
  },
  {
    "text": "are lingering down below. So you can see here\nbecause of the numerator that we're going to\ndifferentiate systems now",
    "start": "224750",
    "end": "231650"
  },
  {
    "text": "based on how many stars\nare at K or above. So it's sensitive\nto multiple stars.",
    "start": "231650",
    "end": "237989"
  },
  {
    "text": "So just to walk through again. Precision at 2 for D1 is 2 out\nof 2, for D2 it's 1 out of 2",
    "start": "237990",
    "end": "245569"
  },
  {
    "text": "because just half of them have\na star, and for 4D3, 0 out of 2.",
    "start": "245570",
    "end": "252060"
  },
  {
    "text": "Recall is very similar but\nnow the denominator changes. So the recall at 2 for this\nfirst one is 2 out of 3.",
    "start": "252060",
    "end": "258359"
  },
  {
    "text": "That is of the three-star\ndocuments, 2 are at K or above. Here it's 1 out of 3,\nand here it's 0 out of 3.",
    "start": "258360",
    "end": "266729"
  },
  {
    "text": "And just to round this\nout, 4D3 has not fared well in our rankings so far.",
    "start": "266730",
    "end": "273000"
  },
  {
    "text": "But in a surprise twist, if\nI change the value of K to 5, all of a sudden D3\nlooks pretty good",
    "start": "273000",
    "end": "280860"
  },
  {
    "text": "because now it's got all\nthree of its stars at 5 or above, whereas the other\ntwo, even though they've",
    "start": "280860",
    "end": "287699"
  },
  {
    "text": "got some high stars\nup there, were not sensitive to that precisely,\nand so now D3 has pulled ahead.",
    "start": "287700",
    "end": "295110"
  },
  {
    "text": "And that is maybe\nsomething that you want to watch out for\nbecause people innocently choose these K values when\nthey're evaluating systems.",
    "start": "295110",
    "end": "301470"
  },
  {
    "text": "And I just showed you\nthat that could really impact the ranking of systems. And in particular, it's\nhard to imagine since there",
    "start": "301470",
    "end": "311020"
  },
  {
    "text": "are only six documents. But if it was a lot of work\nto travel down to our chosen K if K was 1,000, this would\nobscure the fact that we might",
    "start": "311020",
    "end": "319940"
  },
  {
    "text": "pick as our winner a system that\nhad all the stars more or less at 1,000 and the\nother systems which",
    "start": "319940",
    "end": "326000"
  },
  {
    "text": "have their stars at\nthe top of this ranking and therefore\nthey're easy to find, those might be diminished\nwith such a high K.",
    "start": "326000",
    "end": "333560"
  },
  {
    "text": "And so that gets you\ninto the role of thinking what are my users\ntrying to do, what is the cost of them scanning\ndown a list of ranked results",
    "start": "333560",
    "end": "340160"
  },
  {
    "text": "and things like that. And that's where I\nwant you to be when you think about these metrics-- what are you trying\nto solve out there",
    "start": "340160",
    "end": "347060"
  },
  {
    "text": "in the world, what are\nyour users confronting, what is the cost of\nreviewing examples, and so forth and so on.",
    "start": "347060",
    "end": "353840"
  },
  {
    "text": "Yeah. For the neural IR\nmodels that we're going to solve this problem of--",
    "start": "353840",
    "end": "359289"
  },
  {
    "text": "because right now everything's\nbased on the presence or not of a word rather than\nmaybe either a longer meaning",
    "start": "359290",
    "end": "366460"
  },
  {
    "text": "or the quality of the\nrelevance however we define it. Maybe it only says the word once\nbut it actually has the best",
    "start": "366460",
    "end": "373930"
  },
  {
    "text": "information afterwards. Will that take care of\nthat, or is all of neural also going to be based on\npresence or not of words?",
    "start": "373930",
    "end": "380470"
  },
  {
    "text": "That's a great question. Wait, we should be careful. So, yeah, I think\nfor the first part of your question I want to\nsay the neural IR models",
    "start": "380470",
    "end": "387760"
  },
  {
    "text": "are overall going to be better. Because of what\nyou alluded to they have a very rich semantic space.",
    "start": "387760",
    "end": "394610"
  },
  {
    "text": "It won't directly impact\nthis because these stars after all\naren't about terms, this is about whether\na whole document",
    "start": "394610",
    "end": "400759"
  },
  {
    "text": "was relevant to a query. You should imagine that the\nbackground process is like some team of humans went\nthrough and said, OK,",
    "start": "400760",
    "end": "407210"
  },
  {
    "text": "you searched for BERT, and now\nI'm going through documents and saying yeah, this one\nis relevant, this one isn't.",
    "start": "407210",
    "end": "413480"
  },
  {
    "text": "That's what produced\nthese rankings. But I think you're right\nin your core intuition-- term-based models are going\nto be kind of brittle.",
    "start": "413480",
    "end": "421190"
  },
  {
    "text": "And if we have hard\nquery document pairs they might miss them. Actually, that reminds me--",
    "start": "421190",
    "end": "427910"
  },
  {
    "text": "for some reason it\ndidn't display before, let's see if it displays now. I had this nice example\nthat Omar created.",
    "start": "427910",
    "end": "433700"
  },
  {
    "text": "This is an example of why\nsearch is a hard NLU problem. Because this is a query,\nwhat compounds protect",
    "start": "433700",
    "end": "440780"
  },
  {
    "text": "the digestive system against\nviruses where the response is certainly relevant, but\nthere is zero relevant term",
    "start": "440780",
    "end": "447680"
  },
  {
    "text": "overlap between\nquery and document, all of the connections\nthat we want to make are deeply semantic connections.",
    "start": "447680",
    "end": "454620"
  },
  {
    "text": "And I do think that that is\nwhy neural IR models have pulled ahead for accuracy\nstyle assessments.",
    "start": "454620",
    "end": "461925"
  },
  {
    "text": "I'm trying to be\ncareful, as you'll see. ",
    "start": "461925",
    "end": "467490"
  },
  {
    "text": "I have one more metric,\nwhich is average precision. And this will be-- this is fair to say our\nmost nuanced metric.",
    "start": "467490",
    "end": "475530"
  },
  {
    "text": "So a little bit\nhard to think about, but I think it's intuitive. Average precision,\nnotice it has no K.",
    "start": "475530",
    "end": "482010"
  },
  {
    "text": "And the reason it has\nno K is that we're going to sum over\nall the precision values for different\nK's here where",
    "start": "482010",
    "end": "489030"
  },
  {
    "text": "there is a relevant document. Think back to our rankings. Wherever there was a star, we're\ngoing to choose that as a K.",
    "start": "489030",
    "end": "496590"
  },
  {
    "text": "And we're going to sum up\njust those precision values and divide it by the number\nof relevant documents.",
    "start": "496590",
    "end": "504120"
  },
  {
    "text": "Here's an example. Same three rankings that we had\nbefore, and what I'll show you are these precision\ncalculations.",
    "start": "504120",
    "end": "510639"
  },
  {
    "text": "So for the first one, we have\nstars at positions 1, 2, and 6, and so we accumulate\nthe precision",
    "start": "510640",
    "end": "517349"
  },
  {
    "text": "values for 1, 2, and 6. And those are I hope the\nones I've given there. That sums to 2.5, and\nthen we divide that",
    "start": "517350",
    "end": "525990"
  },
  {
    "text": "by 3, which is the number\nof relevant documents.",
    "start": "525990",
    "end": "531270"
  },
  {
    "text": "So we've abstracted away\nthe K, which is reassuring, and we're also checking\nat every level. So it's not going to have\nthat sensitivity I showed you",
    "start": "531270",
    "end": "538460"
  },
  {
    "text": "before where the choice of\nK dramatically impacts which rankings we favor\nbecause now we're",
    "start": "538460",
    "end": "544340"
  },
  {
    "text": "looking at all of the ones\nchosen by the ranking. So that's D1.",
    "start": "544340",
    "end": "549660"
  },
  {
    "text": "And then for D2, the same thing. But now we're checking\nat 2, 5, and 6 because that's where the stars\nare, and that sums to 1.4.",
    "start": "549660",
    "end": "557610"
  },
  {
    "text": "And then for D3, we do the same\nthing at positions 3, 4, and 5.",
    "start": "557610",
    "end": "562649"
  },
  {
    "text": "And notice, interestingly,\nthat D3 has pulled ahead of D2.",
    "start": "562650",
    "end": "569330"
  },
  {
    "text": "That's less surprising to\nme in the current context because D2 is kind of\ngood and kind of not.",
    "start": "569330",
    "end": "577940"
  },
  {
    "text": "It has that one star\nthat's near the top but the other two stars are way\nat the bottom of our ranking,",
    "start": "577940",
    "end": "583460"
  },
  {
    "text": "whereas at least D3 put them\nall at least not literally at the bottom, whereas D1 looks\nlike just a slam dunk winner",
    "start": "583460",
    "end": "592730"
  },
  {
    "text": "here. I mean, it simply has most of\nthe stars right at the top. It has that lonely\none at the bottom,",
    "start": "592730",
    "end": "599200"
  },
  {
    "text": "but on balance, D1 looks good. So if I just stepped back\nfrom this little example,",
    "start": "599200",
    "end": "604570"
  },
  {
    "text": "I would say that\naverage precision is nice in terms of\ngiving what looks to me like a pretty nuanced\npicture of these three",
    "start": "604570",
    "end": "611705"
  },
  {
    "text": "rankings.  I think that's all of the\naccuracy style metrics.",
    "start": "611705",
    "end": "618977"
  },
  {
    "text": "Of course, there are others\nthat you'll encounter. Some are sensitive to the\nnumerical-- like the float",
    "start": "618977",
    "end": "624120"
  },
  {
    "text": "value because sometimes you\nhave not just a 1 or a 0 or a star or not but rather\na float value for relevance.",
    "start": "624120",
    "end": "630150"
  },
  {
    "text": "There are lots of versions\nthat of course average these over sets of queries that\nwill be very common to see.",
    "start": "630150",
    "end": "635670"
  },
  {
    "text": "But underlyingly, that's\njust some arithmetic average of these scores. So I think this\nis a good sample.",
    "start": "635670",
    "end": "641970"
  },
  {
    "text": "Are there questions I can\nanswer about these metrics? ",
    "start": "641970",
    "end": "648500"
  },
  {
    "text": "Really quick. The float value 1\nfor relevance, how is that at a high level computed?",
    "start": "648500",
    "end": "655010"
  },
  {
    "text": "Well, it's called the\ndiscounted cumulative gain and it is the sum of all\nof the scores divided",
    "start": "655010",
    "end": "664070"
  },
  {
    "text": "by something or other. This must be in your\nhistory somewhere. It is something that you\ncould also just label.",
    "start": "664070",
    "end": "672600"
  },
  {
    "text": "You don't have given\nlabels and then you could take precision-\nnot precision, maybe just",
    "start": "672600",
    "end": "678605"
  },
  {
    "text": "a precision-weighted\ncombination of human labels and weighted metrics.",
    "start": "678605",
    "end": "684730"
  },
  {
    "text": "Oh, so you found the\ndiscounted cumulative gain. That's a metric I left\nout, and then you're",
    "start": "684730",
    "end": "690199"
  },
  {
    "text": "just observing that. Very often for\nthese datasets, we'd have humans do a\nbunch of labeling, and then average\nprecision is one way",
    "start": "690200",
    "end": "697310"
  },
  {
    "text": "of aggregating over the labels\nwe might have collected. ",
    "start": "697310",
    "end": "704260"
  },
  {
    "text": "I kind of alluded\nto this before, but here's a partial list of\nthings you could think about. Which metric?",
    "start": "704260",
    "end": "709570"
  },
  {
    "text": "Fundamentally, there\nis no single answer. Is the cost of scrolling\nthrough K passages low?",
    "start": "709570",
    "end": "716290"
  },
  {
    "text": "Then maybe success at K is fine. Because you don't care whether\nit was a position 9 or position",
    "start": "716290",
    "end": "721690"
  },
  {
    "text": "1, what you really care\nabout is that the user was confronted with the success\nthat they can easily find.",
    "start": "721690",
    "end": "728470"
  },
  {
    "text": "That's one scenario\nthat you could be in. Are there multiple relevant\ndocuments per query?",
    "start": "728470",
    "end": "733960"
  },
  {
    "text": "This is straightforward. If so, you probably shouldn't\nuse success at K or RR at K because they're only\nsensitive really to one star.",
    "start": "733960",
    "end": "742570"
  },
  {
    "text": "And if you went to the trouble\nof getting multiple stars, why have your metric\nbe insensitive to that?",
    "start": "742570",
    "end": "748630"
  },
  {
    "text": "So that seems clear. Is it more important to find\nevery relevant document? If so you should favor recall.",
    "start": "748630",
    "end": "754990"
  },
  {
    "text": "That would be a case where\nmaybe human review is cheap or the cost of missing an\nexample is hugely expensive.",
    "start": "754990",
    "end": "762970"
  },
  {
    "text": "In that case, you\nwant to favor recall. You can't miss anything. Conversely, if you just need\nto find some relevant things,",
    "start": "762970",
    "end": "770630"
  },
  {
    "text": "maybe in an ocean of examples\nbecause you want to label them or because it's just\ngood to know about them,",
    "start": "770630",
    "end": "776400"
  },
  {
    "text": "then you could favor\nprecision because then all you really care about\nis that near the top are some relevant things.",
    "start": "776400",
    "end": "784709"
  },
  {
    "text": "F1 at K is the harmonic mean\nof precision and recall, same thing as we do in NLP.",
    "start": "784710",
    "end": "790290"
  },
  {
    "text": "And that can be\nused where there are multiple relevant documents,\nbut maybe the relative order above K doesn't matter.",
    "start": "790290",
    "end": "796830"
  },
  {
    "text": "That's just one\nperspective on what I mean when I say we're\ncombining precision and recall.",
    "start": "796830",
    "end": "803260"
  },
  {
    "text": "And then finally\naverage precision. Of the ones I've showed\nyou will give you the most fine-grained\ndistinctions of the metrics,",
    "start": "803260",
    "end": "808900"
  },
  {
    "text": "right, because it's\nsensitive to rank and it's sensitive to\nprecision and recall. Precision because it\naggregates over those",
    "start": "808900",
    "end": "815660"
  },
  {
    "text": "values, and recall because\nthat's the denominator. So that looks like\nan awfully good way",
    "start": "815660",
    "end": "821510"
  },
  {
    "text": "to really get a fine-grained\nranking of systems. ",
    "start": "821510",
    "end": "827520"
  },
  {
    "text": "And then finally, I'm going to\ntalk about this a bit later, we have to move on\nbeyond accuracy. This is a paper that I did with\na team recently of researchers",
    "start": "827520",
    "end": "835500"
  },
  {
    "text": "here at IBM. What we're seeing here is\na post hoc leaderboard.",
    "start": "835500",
    "end": "840509"
  },
  {
    "text": "Not an actual leaderboard\nbecause part of our complaint is that there are\nno leaderboards that really do anything beyond\nmeasuring accuracy style",
    "start": "840510",
    "end": "847889"
  },
  {
    "text": "things. But if you did go through the\nliterature, as we did here, and find a lot of systems, you\ncan see that they vary widely",
    "start": "847890",
    "end": "856260"
  },
  {
    "text": "along other dimensions. Here is the mean reciprocal\nrank, one of our rankings. It goes from 19 to 37\nor 39 or something.",
    "start": "856260",
    "end": "864720"
  },
  {
    "text": "So you say OK. But then look just to the right\nof that at the query latency. To get to 37, look\nhow much time I",
    "start": "864720",
    "end": "872850"
  },
  {
    "text": "have to spend versus\ndown here where at 36 I spend a fraction of the time.",
    "start": "872850",
    "end": "878100"
  },
  {
    "text": "That is absolutely\nsomething that will matter to the search\nexperience of users. There is almost no way\nthey're waiting around",
    "start": "878100",
    "end": "885730"
  },
  {
    "text": "for 691 milliseconds,\nfor example. Or what about the\nindex size, right?",
    "start": "885730",
    "end": "890860"
  },
  {
    "text": "If you care about\nspace footprint and you will if you are\nindexing the web, some of these",
    "start": "890860",
    "end": "896140"
  },
  {
    "text": "have tiny little indices. And then-- uh-oh, that's our\nmodel ColBERTv1, 154 gigabytes,",
    "start": "896140",
    "end": "901978"
  },
  {
    "text": "right? So now if you need\nto hold it in memory your world just got\na lot more expensive.",
    "start": "901978",
    "end": "909640"
  },
  {
    "text": "And you see over here,\nRAM requirements. So BM25, it has no hardware\nrequirements at all.",
    "start": "909640",
    "end": "916150"
  },
  {
    "text": "You can run that on anything. Whereas these models\ndown here that have these really\nhigh MRR scores,",
    "start": "916150",
    "end": "922269"
  },
  {
    "text": "hugely expensive in terms\nof compute, a classic story of the neural age, right?",
    "start": "922270",
    "end": "928750"
  },
  {
    "text": "You have to pay somewhere. And then, of course, I hope\nyou're thinking about this.",
    "start": "928750",
    "end": "934330"
  },
  {
    "text": "So then what is the\nbest combination of all these things? Well, it depends on\nhow much money you have",
    "start": "934330",
    "end": "940209"
  },
  {
    "text": "and how much time you\nhave and how much you care about accuracy. And so the best pitch\nI can make to you",
    "start": "940210",
    "end": "946230"
  },
  {
    "text": "is that as you\nevaluate systems, you think about what you care about,\nwhat matters, and construct",
    "start": "946230",
    "end": "952680"
  },
  {
    "text": "your evaluations on that basis. That's going to be a big\ntheme of the course later on. And I'm hoping to time it so\nthat you all for your papers",
    "start": "952680",
    "end": "961470"
  },
  {
    "text": "are thinking about assessment\nand you think, oh, I should have a whole\nsection about my philosophy",
    "start": "961470",
    "end": "966600"
  },
  {
    "text": "of assessment here and\nnot just fall into F1 or fall into success at K\nor whatever is relevant.",
    "start": "966600",
    "end": "973230"
  },
  {
    "text": " This is kind of\ninteresting, too.",
    "start": "973230",
    "end": "979530"
  },
  {
    "text": "This is from the same paper. Here's BM25. It cost essentially nothing but\nit has very low performance.",
    "start": "979530",
    "end": "987250"
  },
  {
    "text": "If you travel straight\nup from there, look at these SPLADE\nmodels, also costing",
    "start": "987250",
    "end": "992829"
  },
  {
    "text": "essentially nothing\nbut vastly better in terms of their performance. That looks like a\nreal discovery to me.",
    "start": "992830",
    "end": "999130"
  },
  {
    "text": "This is like the\nPareto frontier, as they call it, these\nsystems where you just",
    "start": "999130",
    "end": "1004680"
  },
  {
    "text": "wouldn't choose any that\nare off the frontier no matter what your values. And obviously, you can see\nthat to favor this model.",
    "start": "1004680",
    "end": "1011635"
  },
  {
    "text": "They're going to have to be\nother dimensions that we care about beyond cost and MRR\nbecause otherwise that's just",
    "start": "1011635",
    "end": "1018150"
  },
  {
    "text": "not a choice you would make. But for all I know there\nare hidden dimensions that need to be teased out that would\nshow that that ANCE model is",
    "start": "1018150",
    "end": "1026339"
  },
  {
    "text": "the best relative to-- [LAUGHTER]",
    "start": "1026339",
    "end": "1031421"
  },
  {
    "text": " Let's dive into some\nof those models then. Neural IR.",
    "start": "1031421",
    "end": "1037829"
  },
  {
    "text": "First, we'll start\nwith cross-encoders. This will be very intuitive. OK?",
    "start": "1037829",
    "end": "1043470"
  },
  {
    "text": "Here just imagine I have a\nhuge transformer and four cross-encoders.",
    "start": "1043470",
    "end": "1048690"
  },
  {
    "text": "What I do is I just concatenate\nthe query and the document together, process them\nwith my Transformer model.",
    "start": "1048690",
    "end": "1055480"
  },
  {
    "text": "And then on the top here I\nput a little scoring function and the scoring\nfunction will just say for this query how\ngood is this document.",
    "start": "1055480",
    "end": "1064980"
  },
  {
    "text": "Enormously powerful. To this comment\nfrom before, we are making maximal use of\nthis say BERT model",
    "start": "1064980",
    "end": "1073380"
  },
  {
    "text": "here to get every possible\ninteraction between query and document. So this would be good\nin terms of accuracy,",
    "start": "1073380",
    "end": "1080790"
  },
  {
    "text": "but you might worry\nabout some other things. Here, let me walk\nthrough a bit more. In the background here I'm\nassuming that our dataset looks",
    "start": "1080790",
    "end": "1087750"
  },
  {
    "text": "like this. We have a query, one\npositive document, and a set of one or\nmore negative documents.",
    "start": "1087750",
    "end": "1095850"
  },
  {
    "text": "We could have multiple\nof the negatives. What I'm depicting\non the left here",
    "start": "1095850",
    "end": "1101710"
  },
  {
    "text": "is a model we could\nsummarize like this. This is the encoder. We concatenate the query and\nthe document and process them",
    "start": "1101710",
    "end": "1110390"
  },
  {
    "text": "and we retrieve this\nrepresentation here, layer N position 0.",
    "start": "1110390",
    "end": "1116180"
  },
  {
    "text": "We feed that through a dense\nlayer that does our scoring and that is the basis for\nessentially a classifier.",
    "start": "1116180",
    "end": "1124410"
  },
  {
    "text": "This is called the\nnegative log-likelihood of the positive passage. And if you squint or you\ndon't squint you just",
    "start": "1124410",
    "end": "1130790"
  },
  {
    "text": "let it go blurry, you will see\nthat it is a typical classifier loss. The only possible\ntwist is the numerator",
    "start": "1130790",
    "end": "1138380"
  },
  {
    "text": "is the positive passage score,\nand then on the denominator, I have the positive\npassage summed together",
    "start": "1138380",
    "end": "1144110"
  },
  {
    "text": "with all the negative passages\nthat I have in my example set. But fundamentally,\nit's a classifier.",
    "start": "1144110",
    "end": "1152610"
  },
  {
    "text": "And so that's why these examples\nlook like this because that's what's being used here to\noptimize all these parameters",
    "start": "1152610",
    "end": "1159919"
  },
  {
    "text": "to score documents. Final thing, I hope you're\nthinking about this.",
    "start": "1159920",
    "end": "1166710"
  },
  {
    "text": "It's going to be incredibly\nexpressive and powerful, but it just won't scale.",
    "start": "1166710",
    "end": "1171750"
  },
  {
    "text": "The cost of having the query and\ndocument interact at query time is that I can't process any of\nthese documents ahead of time.",
    "start": "1171750",
    "end": "1178929"
  },
  {
    "text": "So just imagine\nthis-- your query comes in on the web\nlike your Google and you're using\na cross-encoder,",
    "start": "1178930",
    "end": "1184500"
  },
  {
    "text": "the user queries, you need to\nprocess that query together with every single document\non the web to score them",
    "start": "1184500",
    "end": "1191340"
  },
  {
    "text": "and then on that basis, you\nwill get beautiful scores. But obviously, each query\ncould take years to serve.",
    "start": "1191340",
    "end": "1198460"
  },
  {
    "text": "[LAUGHTER]  So from this perspective, it\nis just not a practical choice.",
    "start": "1198460",
    "end": "1204070"
  },
  {
    "text": "Maybe we could use\nit for reranking. You see this sometimes\nwhere a cheap retriever gets",
    "start": "1204070",
    "end": "1209350"
  },
  {
    "text": "a lot of like 1,000\ndocuments and then this is done to rerank\nthe last 1,000, but we can't do\nthis at web scale.",
    "start": "1209350",
    "end": "1217120"
  },
  {
    "text": "So a question in the back. Yeah. Could you use this with multiple\npositive documents as well?",
    "start": "1217120",
    "end": "1223120"
  },
  {
    "text": " For example, for\nthe ranking thing,",
    "start": "1223120",
    "end": "1229130"
  },
  {
    "text": "theoretically, multiple of\nthose could be good, but-- ",
    "start": "1229130",
    "end": "1235540"
  },
  {
    "text": "Let's see. I don't see why not. The numerator could be\nthe sum of the positive and then the denominator could\njust include all of those.",
    "start": "1235540",
    "end": "1241880"
  },
  {
    "text": "So what you would be doing is-- well, I'm just trying\nto think through.",
    "start": "1241880",
    "end": "1251920"
  },
  {
    "text": "That would be one approach. The other approach would\nbe to just treat them as separate examples.",
    "start": "1251920",
    "end": "1257200"
  },
  {
    "text": "I think under some conditions\nthose will be identical. But I'd have to\nthink it through. But I don't see a problem.",
    "start": "1257200",
    "end": "1264299"
  },
  {
    "text": "I don't see a problem. That's worth thinking about. I'll get back to you on that. ",
    "start": "1264300",
    "end": "1272360"
  },
  {
    "text": "Let's improve on this. DPR, dense passage retriever. This will also be intuitive.",
    "start": "1272360",
    "end": "1278210"
  },
  {
    "text": "Here we go. Query and document,\nexcept notice now they are processed\nby separate models--",
    "start": "1278210",
    "end": "1283850"
  },
  {
    "text": "the query encoder and\nthe document encoder. It could be the same\nparameters, but the point is we process them separately.",
    "start": "1283850",
    "end": "1290310"
  },
  {
    "text": "And I've made lighter every\nstate, except the output tokens below the two class\ntokens because those are",
    "start": "1290310",
    "end": "1297960"
  },
  {
    "text": "the only ones that we need-- OK? these two. And then we do some scoring\non that basis like similarity.",
    "start": "1297960",
    "end": "1306230"
  },
  {
    "text": "So here are examples. They are the same. Now, the similarity\nfunction as I'm calling it",
    "start": "1306230",
    "end": "1312020"
  },
  {
    "text": "for a query in a document\nis we process the query, we get this guy, process the\ndocument and we get this guy,",
    "start": "1312020",
    "end": "1318740"
  },
  {
    "text": "and then we do\nscoring on that basis. There are no\nadditional parameters, we just score based on those\nrepresentations, their dot",
    "start": "1318740",
    "end": "1326900"
  },
  {
    "text": "product. So now we've got something that\nis highly scalable because we",
    "start": "1326900",
    "end": "1333100"
  },
  {
    "text": "can process every document\nin our entire web collection into a single vector--\nthis one, and it can just",
    "start": "1333100",
    "end": "1339130"
  },
  {
    "text": "sit there on disk. And then at query time, process\nthe query, get its vector, and do this super fast dot\nproduct comparison for scoring.",
    "start": "1339130",
    "end": "1348670"
  },
  {
    "text": "So now we've got something\nthat is probably even going to function as a full-ranking\nmodel, not just a reranker.",
    "start": "1348670",
    "end": "1355300"
  },
  {
    "text": "But the real gain is\nthat we can process all our documents offline. The cost was that we\nnow have almost no",
    "start": "1355300",
    "end": "1362080"
  },
  {
    "text": "interactions between the\nquery and the document. If you think about\ntoken identities, if you think about\nthe soft matching that",
    "start": "1362080",
    "end": "1369070"
  },
  {
    "text": "happens with\nTF-IDF, none of that is going to be able\nto happen here.",
    "start": "1369070",
    "end": "1374340"
  },
  {
    "text": "That was the cost.  Yeah.",
    "start": "1374340",
    "end": "1380160"
  },
  {
    "text": "So essentially, what\nwe've been comparing are two fixed-length vectors. I mean, a vector\nrepresenting the document,",
    "start": "1380160",
    "end": "1387390"
  },
  {
    "text": "and another is the query. So is there a\nlimit to the length",
    "start": "1387390",
    "end": "1392550"
  },
  {
    "text": "of the document that could be\nrepresented in that vector? Could it represent an\narbitrarily long document",
    "start": "1392550",
    "end": "1401070"
  },
  {
    "text": "that could lose context? That's a great question. Let me repeat them. For the first question,\nyes, you are right.",
    "start": "1401070",
    "end": "1406620"
  },
  {
    "text": "The one constraint\nwe need to impose on the query encoder\nand the document encoder is that they have the\nsame dimensionality so",
    "start": "1406620",
    "end": "1412590"
  },
  {
    "text": "that we can do the dot product. They can otherwise be\nseparate models if we want. And then length of\nquery and length",
    "start": "1412590",
    "end": "1418950"
  },
  {
    "text": "of document, that's\njust going to be imposed by whatever we choose for\nthe query and the document themselves.",
    "start": "1418950",
    "end": "1424240"
  },
  {
    "text": "So if you choose\nBERT, you're going to be stuck with 512\nas the longest document that you can process unless we\ndo some further manipulation",
    "start": "1424240",
    "end": "1432247"
  },
  {
    "text": "of these things.  Yeah.",
    "start": "1432248",
    "end": "1437490"
  },
  {
    "text": "If these models are trained to\nproject into shared embedding",
    "start": "1437490",
    "end": "1442620"
  },
  {
    "text": "space, so documents that\nare similar to a query you're going to fall\ninto a similar location",
    "start": "1442620",
    "end": "1448260"
  },
  {
    "text": "in embedding space,\ncould we have a system where we essentially\npre-process all the documents?",
    "start": "1448260",
    "end": "1455100"
  },
  {
    "text": "We take a query\nat inference time, project it into embedding space\nand then do a nearest neighbor",
    "start": "1455100",
    "end": "1460950"
  },
  {
    "text": "search or something like that. Well, yes. So some aspects of what\nyou're describing I think",
    "start": "1460950",
    "end": "1467430"
  },
  {
    "text": "are what DPR will\nbe optimized for. The other parts of\nwhat you're saying are going to be optimization\ntricks that I'll show you",
    "start": "1467430",
    "end": "1473940"
  },
  {
    "text": "in a second, I believe. Yes. Can you elaborate on what\nyou mean by limited query doc",
    "start": "1473940",
    "end": "1480520"
  },
  {
    "text": "interactions? Just that all we've\ngot in the end is this vector for\nthe whole query",
    "start": "1480520",
    "end": "1486760"
  },
  {
    "text": "and this vector for\nthe whole document. So token identities\nto the extent that they're\npreserved at all they",
    "start": "1486760",
    "end": "1492460"
  },
  {
    "text": "have to have been packed\ninto those vectors. Whereas over here, we had\nevery token-level interaction",
    "start": "1492460",
    "end": "1501870"
  },
  {
    "text": "you can imagine as a result\nof us using the Transformer. ",
    "start": "1501870",
    "end": "1508480"
  },
  {
    "text": "Yeah. Is there any room for training\nsome more clever synthesis of the two representations\nyou get at the end as opposed",
    "start": "1508480",
    "end": "1515200"
  },
  {
    "text": "to just dot producting them? Yeah. I think that's a\nnatural follow-on is that you might think I want\nto have in this layer here",
    "start": "1515200",
    "end": "1522580"
  },
  {
    "text": "some additional parameters. And you can kind of see\nhow that might work, right? So instead of just\nusing this vector",
    "start": "1522580",
    "end": "1528220"
  },
  {
    "text": "I would put some\nparameters on top and then this same\noptimization can be used.",
    "start": "1528220",
    "end": "1534610"
  },
  {
    "text": "They're going in the same\nembedding space, I mean, attention. ",
    "start": "1534610",
    "end": "1541540"
  },
  {
    "text": "Yeah. And this could be\ngood in the sense that we would pay a\nlittle bit of a cost by adding more parameters\nbut we might gain something",
    "start": "1541540",
    "end": "1548500"
  },
  {
    "text": "in terms of expressivity. Nice. ",
    "start": "1548500",
    "end": "1556010"
  },
  {
    "text": "Let me show you a\nhappy compromise. Oh yeah, I just wanted\nto point this out.",
    "start": "1556010",
    "end": "1561743"
  },
  {
    "text": "I've just showed you\ntwo loss functions. I showed you the\ncross-encoder and the DPR. And you can probably already\nsee that they are identical,",
    "start": "1561743",
    "end": "1568750"
  },
  {
    "text": "except for this function\nthat you might call Cmp here, and that's kind of freeing.",
    "start": "1568750",
    "end": "1574030"
  },
  {
    "text": "And as you think about these\ndifferent model architectures, probably what you're\nthinking about is simply changing\nthis Cmp function",
    "start": "1574030",
    "end": "1581350"
  },
  {
    "text": "and then using\nyour available data to train the model against\nthis negative log-likelihood of the positive passage.",
    "start": "1581350",
    "end": "1587889"
  },
  {
    "text": "There are other losses out\nthere in the literature, but this is the most\nwidely used and it",
    "start": "1587890",
    "end": "1593350"
  },
  {
    "text": "seems to be very effective. ",
    "start": "1593350",
    "end": "1598590"
  },
  {
    "text": "ColBERT. This stands for contextualized\nlate interaction with BERT.",
    "start": "1598590",
    "end": "1604380"
  },
  {
    "text": "It was invented by Omar\nand Matei, who are here. Omar is my student. I work closely with Matei.",
    "start": "1604380",
    "end": "1609870"
  },
  {
    "text": "And let's see. So Omar would want\nyou to know that this stands for contextualized\nlate interaction.",
    "start": "1609870",
    "end": "1616080"
  },
  {
    "text": "And he pronounces it Colbert\nbecause Stephen Colbert has a show full of contextualized\nlate-night interactions.",
    "start": "1616080",
    "end": "1624420"
  },
  {
    "text": "But you can also\npronounce it ColBERT. It's your choice. Because the BERT there\nis the BERT model.",
    "start": "1624420",
    "end": "1631800"
  },
  {
    "text": "And we are yes, still hoping\nthat Stephen Colbert will take notice of this. [LAUGHTER]",
    "start": "1631800",
    "end": "1637220"
  },
  {
    "text": "Maybe we just start\nprocessing the first pair. [LAUGHTER]  Have you tried asking Stephen\nColbert an announcement",
    "start": "1637220",
    "end": "1644384"
  },
  {
    "text": "to do this? I haven't been so bold but I\nwelcome you all to do that. [LAUGHTER] That's great. @ him\non Twitter, yes.",
    "start": "1644384",
    "end": "1651970"
  },
  {
    "text": "Here's how this will work. I've drawn the query encoder\non the side for reasons that you'll see, but\nit's the same thing.",
    "start": "1651970",
    "end": "1657560"
  },
  {
    "text": "So imagine BERT\nprocesses my query. And I've grayed out everything\nbut the final representations because crucially those are\nthe only ones that we actually",
    "start": "1657560",
    "end": "1664750"
  },
  {
    "text": "need. The same thing with\nthe document, and it could be the same encoder. Now what I'm going\nto do with ColBERT",
    "start": "1664750",
    "end": "1671350"
  },
  {
    "text": "is form a grid of scores. And this is going\nto essentially give the similarity value\nbetween every query token",
    "start": "1671350",
    "end": "1679960"
  },
  {
    "text": "and every document token. And then I will\nchoose the values",
    "start": "1679960",
    "end": "1686460"
  },
  {
    "text": "along the rows, that is for each\nquery the document token that maximizes that\nsimilarity comparison,",
    "start": "1686460",
    "end": "1692820"
  },
  {
    "text": "and the scoring function\nis essentially the sum of those three max values.",
    "start": "1692820",
    "end": "1698730"
  },
  {
    "text": "That is why you see MaxSim all\nover the place for ColBERT.",
    "start": "1698730",
    "end": "1704580"
  },
  {
    "text": "Examples are as before,\nlosses as before. And I wrote down\nhere what we would",
    "start": "1704580",
    "end": "1709830"
  },
  {
    "text": "think of as the Cmp function\nand I wrote it as MaxSim. For a query in a document, you\nsum over all the query tokens",
    "start": "1709830",
    "end": "1716580"
  },
  {
    "text": "and you get the max\nmatching document token. So you can see why it's\ncontextualized late interaction",
    "start": "1716580",
    "end": "1724090"
  },
  {
    "text": "because I'm using\nthe output states. But unlike DPR,\nI'm allowing them",
    "start": "1724090",
    "end": "1729549"
  },
  {
    "text": "all to interact with each other\nvia these very fast MaxSim calculations. I have token-level interactions.",
    "start": "1729550",
    "end": "1737409"
  },
  {
    "text": "So highly scalable\nand highly expressive. The only cost is that the\ninteractions happen only",
    "start": "1737410",
    "end": "1743020"
  },
  {
    "text": "in this very thin final layer. But this is really pleasing for\nIR that ColBERT because of this",
    "start": "1743020",
    "end": "1752270"
  },
  {
    "text": "brings us back to\ncommon intuitions in IR. We do genuinely achieve\nwith these MaxSim scores",
    "start": "1752270",
    "end": "1758210"
  },
  {
    "text": "intuitive soft alignments. Here I have the query. When did the Transformers\ncartoon series come out?",
    "start": "1758210",
    "end": "1763940"
  },
  {
    "text": "And the response document,\nthe animated Transformers was released in August 1986.",
    "start": "1763940",
    "end": "1769370"
  },
  {
    "text": "And these are proportional\nto actual MaxSim values query relative to document,\nthe thickness of that line.",
    "start": "1769370",
    "end": "1777240"
  },
  {
    "text": "And you can see that it is\ndoing something very intuitive and also something\nvery semantic,",
    "start": "1777240",
    "end": "1782420"
  },
  {
    "text": "because unlike\nterm-based models I don't have to do anything\nspecial to capture the fact that come in\nthe context of come out",
    "start": "1782420",
    "end": "1790220"
  },
  {
    "text": "is a lot like released,\nand similarly with when and that date. Here I'm showing the two\ntopmost MaxSim values,",
    "start": "1790220",
    "end": "1798010"
  },
  {
    "text": "and they're also very intuitive. And this is wonderful\nbecause IR has been so successful for so\nlong doing term matching",
    "start": "1798010",
    "end": "1805730"
  },
  {
    "text": "and it is nice to see that\nintuition carried forward into this more semantic\nspace in my view.",
    "start": "1805730",
    "end": "1813050"
  },
  {
    "text": "So a hand go up. Yeah. Is that matrix of query\ndocument mappings?",
    "start": "1813050",
    "end": "1819850"
  },
  {
    "text": "Is that why the memory\nindex is so big for ColBERT? Yes. So your question is why is\nthe index for Colbert so big?",
    "start": "1819850",
    "end": "1827770"
  },
  {
    "text": "It is because we have to\nstore every token-level representation. Yes.",
    "start": "1827770",
    "end": "1832780"
  },
  {
    "text": "I'm going to show you\nthat we can do better, but naively storing these\nfor our entire document store",
    "start": "1832780",
    "end": "1838540"
  },
  {
    "text": "is going to be a\nlot of vectors-- one per token. Not one per type, one per token.",
    "start": "1838540",
    "end": "1845270"
  },
  {
    "text": "[LAUGHTER]  I have to pay somewhere.",
    "start": "1845270",
    "end": "1850440"
  },
  {
    "text": "I guess that's the insight. Yeah, question. Maybe there's some\nintuition that",
    "start": "1850440",
    "end": "1855450"
  },
  {
    "text": "can make this a little clearer. If the document has multiple\nvariants of Transformers,",
    "start": "1855450",
    "end": "1861580"
  },
  {
    "text": "the Decepticon, Optimus\nPrime, or whatever, they're all related to the\noriginal token Transformers.",
    "start": "1861580",
    "end": "1868019"
  },
  {
    "text": "Would you be able to draw that\nlink to all those other tokens as well, or would you\nhave to pick, I guess,",
    "start": "1868020",
    "end": "1876000"
  },
  {
    "text": "one relationship? Transformers is\nrepresented once. I think that's a great question.",
    "start": "1876000",
    "end": "1882010"
  },
  {
    "text": "So Transformers that's\nwhy I picked it. It's amusingly ambiguous\nbetween our model and the animated cartoon\nseries of my youth.",
    "start": "1882010",
    "end": "1893680"
  },
  {
    "text": "But I think the point is\nthat because it's BERT, Transformers in\nthis context will",
    "start": "1893680",
    "end": "1899050"
  },
  {
    "text": "have a very different\nrepresentation from the one if we were talking\nabout NLP, and that's why it's so good that we are\nusing BERT because then we'll",
    "start": "1899050",
    "end": "1906250"
  },
  {
    "text": "get MaxSims that are\nappropriately semantic. That is the hope. Yeah. Whereas term-based models\nare really going to struggle.",
    "start": "1906250",
    "end": "1913540"
  },
  {
    "text": "The best they're\ngoing to be able to do is have n-grams that\ncapture the fact that this Transformers is\nthe cartoon series one.",
    "start": "1913540",
    "end": "1921010"
  },
  {
    "text": " Actually, that's\nanother argument in favor of being in\na more semantic space.",
    "start": "1921010",
    "end": "1927580"
  },
  {
    "text": " I want to just\nquickly talk with you",
    "start": "1927580",
    "end": "1933549"
  },
  {
    "text": "about how we have worked\nto optimize ColBERT because I think that this\nsuggests things that you would want to do if you developed your\nown neural retrieval models.",
    "start": "1933550",
    "end": "1940570"
  },
  {
    "text": "Because the hard truth\nhere is that BM25 is blazingly fast and scalable\nand these neural models",
    "start": "1940570",
    "end": "1948790"
  },
  {
    "text": "are not. You have to work much\nharder to get them to that point of being\nas performant in terms of other dimensions\nbeyond accuracy.",
    "start": "1948790",
    "end": "1956830"
  },
  {
    "text": "We could use ColBERT\nas a reranker, as I alluded to before. So here I have all these\ntoken-level representations,",
    "start": "1956830",
    "end": "1964120"
  },
  {
    "text": "which I do have to\nstore, and they're each connected to a document.",
    "start": "1964120",
    "end": "1969280"
  },
  {
    "text": "Now, if use naively this\nwill be not scalable. But I could do this. Given some query\nthat's represented",
    "start": "1969280",
    "end": "1976650"
  },
  {
    "text": "as a sequence of tokens, I\ncould get the top K documents for using BM25 and\nthen rerank that top K.",
    "start": "1976650",
    "end": "1986760"
  },
  {
    "text": "And so if K is small, I pay the\nfull price of the ColBERT model but only for K documents.",
    "start": "1986760",
    "end": "1993240"
  },
  {
    "text": "And you're hoping that BM25\ndid a good job of getting you to that initial point.",
    "start": "1993240",
    "end": "1998470"
  },
  {
    "text": "So a very common\napplication and it can be really\nmeaningful to rerank that final set of K documents.",
    "start": "1998470",
    "end": "2005540"
  },
  {
    "text": "But we could do a little better. If we wanted to use\nColBERT end to end, here's how it could work. We again store all those\ntoken-level vectors,",
    "start": "2005540",
    "end": "2012950"
  },
  {
    "text": "but now we're going\nto turn things around. We just need to keep\ntrack of those vectors and their associated documents.",
    "start": "2012950",
    "end": "2020120"
  },
  {
    "text": "For a query that we have encoded\nas a set of vectors using ColBERT, we take\neach query vector wi",
    "start": "2020120",
    "end": "2027890"
  },
  {
    "text": "and we retrieve the p most token\nvectors from this huge list that are similar to our target.",
    "start": "2027890",
    "end": "2034440"
  },
  {
    "text": "And that doesn't require\nthe full ColBERT model. That could just be a\nsimilarity calculation.",
    "start": "2034440",
    "end": "2039520"
  },
  {
    "text": "And you can do\nthose really fast. People have really\noptimized that. And then you get all\nthe documents associated",
    "start": "2039520",
    "end": "2046140"
  },
  {
    "text": "with this small set of\nvectors that you found and you score them. So again, the name\nof the game is",
    "start": "2046140",
    "end": "2051870"
  },
  {
    "text": "to use ColBERT only very\nsparingly in the final stage because it is so expensive.",
    "start": "2051870",
    "end": "2059429"
  },
  {
    "text": "And then a third step here, just\nquickly, we can do even better. And this is quite striking.",
    "start": "2059429",
    "end": "2064860"
  },
  {
    "text": "What we can do is cluster our\ntoken-level representations into their centroids using\nK-means clustering-- that's",
    "start": "2064860",
    "end": "2072359"
  },
  {
    "text": "what I've got in red\nhere, and then use them as the basis for search. So again, we encode our query\ninto a series of vectors,",
    "start": "2072360",
    "end": "2080280"
  },
  {
    "text": "and then for this\ntarget vector wi, we first get the centroids\nthat are closest to that.",
    "start": "2080280",
    "end": "2086280"
  },
  {
    "text": "And this is important\nbecause in practice we can collect only four\ncentroids per token vector",
    "start": "2086280",
    "end": "2093239"
  },
  {
    "text": "and do really well. That's a tiny number. Then we get the t most similar\ntoken vectors to that centroid,",
    "start": "2093239",
    "end": "2102140"
  },
  {
    "text": "and then we finally do scoring\non the associated documents. And so by leaps\nand bounds here, we",
    "start": "2102140",
    "end": "2109059"
  },
  {
    "text": "have reduced the\namount of compute we need to deal with this huge\nindex by using the centroids",
    "start": "2109060",
    "end": "2114730"
  },
  {
    "text": "and then using ColBERT\nagain very sparingly. Final thing and then\nI'll take some questions.",
    "start": "2114730",
    "end": "2122040"
  },
  {
    "text": "The team has worked very hard to\nreduce the latency of ColBERT. This is a latency analysis here.",
    "start": "2122040",
    "end": "2127483"
  },
  {
    "text": "And the thing I want\nto point out to you is that the ColBERT\nmodel steps actually for this second\nversion, the one I just",
    "start": "2127483",
    "end": "2133140"
  },
  {
    "text": "described to you with the\ncentroids, that was actually a relatively small part\nof the overall cost because it was being\nused so sparingly.",
    "start": "2133140",
    "end": "2140370"
  },
  {
    "text": "The big costs were dealing\nwith the huge index",
    "start": "2140370",
    "end": "2146730"
  },
  {
    "text": "and also doing work to\nquantize the vectors so that they were easier to store\non disk by making them smaller.",
    "start": "2146730",
    "end": "2153870"
  },
  {
    "text": "And so after a bunch of\nwork with this framework called PLAID, they were able\nto get rid of almost all",
    "start": "2153870",
    "end": "2159470"
  },
  {
    "text": "of that index lookup and\ndequantization or decompression steps for the vectors\nthat were costing so much",
    "start": "2159470",
    "end": "2165859"
  },
  {
    "text": "and they brought the latency\ndown to 58 milliseconds.",
    "start": "2165860",
    "end": "2171320"
  },
  {
    "text": "So it went from\nsomething that is impossible to imagine deploying\nindustrially to something that",
    "start": "2171320",
    "end": "2176930"
  },
  {
    "text": "is close to what\nyou might entertain as a possibility for deployment. And the details are\nin the PLAID paper.",
    "start": "2176930",
    "end": "2183890"
  },
  {
    "text": "We can talk about them offline. I just wanted to call\nout that I think this is an incredible achievement.",
    "start": "2183890",
    "end": "2189110"
  },
  {
    "text": "It is so clever\nthe set of things that they did to achieve\nthis enormous improvement.",
    "start": "2189110",
    "end": "2195540"
  },
  {
    "text": "So shout out to them. And it does mean that if you\nhad heard a rumor that ColBERT was impractical to use because\nthe index was too large",
    "start": "2195540",
    "end": "2202940"
  },
  {
    "text": "and the latency was too long,\nI think it's not true anymore. The indices are small\nbecause of quantization,",
    "start": "2202940",
    "end": "2209180"
  },
  {
    "text": "and this is that\npicture of latency. So give it a shot.",
    "start": "2209180",
    "end": "2215037"
  },
  {
    "text": "I have one more model,\nbut let me take questions. Yeah. Did you have a question? Sorry, I just had a question\nabout the [INAUDIBLE]..",
    "start": "2215038",
    "end": "2221910"
  },
  {
    "text": "OK. Cool. I'm happy to talk more\nabout the PLAID paper. It's full of tricks\nand things like that. But I don't want to\ntake up too much time.",
    "start": "2221910",
    "end": "2228500"
  },
  {
    "text": "I definitely want to\ngive Sid plenty of time to talk about models. So let me just\nshow you one more. This is SPLADE. This is also ingenious.",
    "start": "2228500",
    "end": "2234500"
  },
  {
    "text": "It'll get you\nthinking in a new way. So for SPLADE, I wrote\nsequence at the bottom",
    "start": "2234500",
    "end": "2240530"
  },
  {
    "text": "because we're going to\ndo this for both queries and documents, this process. And crucially, here I\nhave the vocabulary.",
    "start": "2240530",
    "end": "2248570"
  },
  {
    "text": "I've only represented seven\ntokens, but if it was BERT it would be like 30,000.",
    "start": "2248570",
    "end": "2253880"
  },
  {
    "text": "So we again process the\ntext into the output states, t1 through t3 there.",
    "start": "2253880",
    "end": "2261300"
  },
  {
    "text": "And then we form all these\nscores and the scores are determined by\nthis thing here.",
    "start": "2261300",
    "end": "2266930"
  },
  {
    "text": "That's Si sub j. So we're going to\napply a linear layer to the encoding,\nthose output states,",
    "start": "2266930",
    "end": "2273829"
  },
  {
    "text": "and we're going to combine\nit with the embedding for these vocabulary\nitems with a bias.",
    "start": "2273830",
    "end": "2279450"
  },
  {
    "text": "So if you strip\naway the details you can see that this is\nlike a dot product of these states with\nall of these values",
    "start": "2279450",
    "end": "2286040"
  },
  {
    "text": "here in our vocabulary. And then SPLADE is\nthe sum of that.",
    "start": "2286040",
    "end": "2292099"
  },
  {
    "text": "And so you can think of that as\nsumming across all the document tokens. And so what we've got\nin that orange column",
    "start": "2292100",
    "end": "2298150"
  },
  {
    "text": "there is probably\nvery sparse vector that represents this text\ndown here with respect",
    "start": "2298150",
    "end": "2306820"
  },
  {
    "text": "to our vocabulary. ",
    "start": "2306820",
    "end": "2312200"
  },
  {
    "text": "So this is a lot like\nterm-based work of old.",
    "start": "2312200",
    "end": "2318450"
  },
  {
    "text": "This is a lot like a\nTF-IDF representation, except it was done\nin the neural space so we should get the advantages\nof being with a semantic model.",
    "start": "2318450",
    "end": "2328960"
  },
  {
    "text": "And then the similarity value is\njust the SPLADE representation that is this representation\nhere for the query.product",
    "start": "2328960",
    "end": "2336730"
  },
  {
    "text": "with the document, and\nthe loss is the one that we've been using all along. ",
    "start": "2336730",
    "end": "2344420"
  },
  {
    "text": "So just to be clear, so\nyou do the SPLADE process both with the query and with the\ndocument and then [INAUDIBLE]..",
    "start": "2344420",
    "end": "2352910"
  },
  {
    "text": "Yeah. There's a bunch of it. It looks similar in my\ndocument, but this is great. Let me review what\nyou just said, there's a bunch of new things.",
    "start": "2352910",
    "end": "2358930"
  },
  {
    "text": "Sequence, not query\nor document because we do this for both kinds. And of course, we can do all\nthe documents ahead of time.",
    "start": "2358930",
    "end": "2366280"
  },
  {
    "text": "The big twist is that we're\nscoring these sequences with respect to the vocabulary. And we are essentially\ngetting in semantic space",
    "start": "2366280",
    "end": "2373870"
  },
  {
    "text": "because this is an\nembedding space here and this is a contextual\nembedding space here, scores for each query\nterm with respect",
    "start": "2373870",
    "end": "2382300"
  },
  {
    "text": "to the whole vocabulary. That gives us this big\npresumably pretty sparse vector",
    "start": "2382300",
    "end": "2387890"
  },
  {
    "text": "and their optimization\nfurther encourages sparsity. And then the similarity\nvalue is the dot product",
    "start": "2387890",
    "end": "2393800"
  },
  {
    "text": "of those for queries\nand for documents. So it has some hallmarks\nof late interaction,",
    "start": "2393800",
    "end": "2399980"
  },
  {
    "text": "except it is interacting\nthe text representations with the vocabulary, kind of\nlike what you get with TF-IDF.",
    "start": "2399980",
    "end": "2408619"
  },
  {
    "text": "And this model is outstanding. You saw it in some of my\nslides before, very impressive.",
    "start": "2408620",
    "end": "2416810"
  },
  {
    "text": "And it's also a new way of\nthinking, which I really like. ",
    "start": "2416810",
    "end": "2422650"
  },
  {
    "text": "Here's a bunch of more\nrecent developments. And one theme of them\nI won't go through them is just that people are\nworking hard finally",
    "start": "2422650",
    "end": "2430060"
  },
  {
    "text": "on making these\nmodels more efficient. So a big theme of this, it's\nnot just obsession with accuracy",
    "start": "2430060",
    "end": "2435310"
  },
  {
    "text": "but also obsession with\nespecially latency.",
    "start": "2435310",
    "end": "2440620"
  },
  {
    "text": "And then finally for that\npaper that I mentioned before, we did a bunch of\nsystematic investigations",
    "start": "2440620",
    "end": "2446500"
  },
  {
    "text": "of different approaches. You can see BM25, DPR, ColBERT,\nand some SPLADE models here.",
    "start": "2446500",
    "end": "2452500"
  },
  {
    "text": "And these are all kind of\nvariants of these models where people have worked\nhard to optimize them.",
    "start": "2452500",
    "end": "2457930"
  },
  {
    "text": "There's lots of tables\nlike this in the paper. Let me just draw out\na few comparisons. BM25 is the only solution that\ncould run on this tiny hardware",
    "start": "2457930",
    "end": "2467170"
  },
  {
    "text": "here. We couldn't even run\nthe other systems. That's why it's alone in\nits own little block there.",
    "start": "2467170",
    "end": "2472619"
  },
  {
    "text": "And it costs nothing. But it's not that\nsuccessful either.",
    "start": "2472620",
    "end": "2478260"
  },
  {
    "text": "Success at 10 is low\nrelative to the rest. When we move here,\nthis is interesting.",
    "start": "2478260",
    "end": "2484200"
  },
  {
    "text": "These two ColBERT models achieve\nvery similar performance, if you look all the way to\nthe right, except one of them",
    "start": "2484200",
    "end": "2492089"
  },
  {
    "text": "is double the latency of the\nother one for this hardware. And so you might\nwonder, do I really",
    "start": "2492090",
    "end": "2498420"
  },
  {
    "text": "need this extra\npoint of performance if I'm going to have\nto wait that long?",
    "start": "2498420",
    "end": "2504790"
  },
  {
    "text": "And then if you look to SPLADE,\nso SPLADE is below ColBERTv2 small, but its latency\nis a quarter or something",
    "start": "2504790",
    "end": "2514660"
  },
  {
    "text": "of the ColBERTv2 small. So maybe you care more\nabout that and not so much",
    "start": "2514660",
    "end": "2520140"
  },
  {
    "text": "about the success. And then if you compare\nthese two SPLADEs, they have the same performance.",
    "start": "2520140",
    "end": "2525700"
  },
  {
    "text": "But if you just jack up\nthe hardware a little bit, then you get much lower latency.",
    "start": "2525700",
    "end": "2533160"
  },
  {
    "text": "But look how much\nthe price went up. It went up for all of them\nwith this heavy-duty hardware.",
    "start": "2533160",
    "end": "2539790"
  },
  {
    "text": " So this is the space that\nyou're actually operating in.",
    "start": "2539790",
    "end": "2545280"
  },
  {
    "text": "We'll talk later about how\nwe might more systematically integrate all these scores. I think this is enough now\nto get you thinking about all",
    "start": "2545280",
    "end": "2552657"
  },
  {
    "text": "of these dimensions. So they are the PLAID ColBERTs? They are the PLAID\nColBERTs, yeah. Pretty expensive there.",
    "start": "2552657",
    "end": "2558900"
  },
  {
    "text": "Luckily, in the paper, we\nshow that you never need a GPU for ColBERT, I believe. You can always use\ncheaper hardware.",
    "start": "2558900",
    "end": "2564750"
  },
  {
    "text": " But those costs do look scary. [LAUGHTER]",
    "start": "2564750",
    "end": "2570028"
  },
  {
    "text": " The final section of this\nis just some datasets. I think I don't need to go\nthrough it because you have it",
    "start": "2570028",
    "end": "2575920"
  },
  {
    "text": "as a resource. If you want to get started in\nneural information retrieval you've got TREC, MS\nMARCO, and then there",
    "start": "2575920",
    "end": "2582610"
  },
  {
    "text": "are a bunch of new\nbenchmarks that are designed to\nassess systems out of the box that is zero-shot. BEIR is great, LoTTE is\ngreat for long-tailed",
    "start": "2582610",
    "end": "2591550"
  },
  {
    "text": "topic-stratified\nevaluation, and then this XOR TyDI is cool\nbecause this is multilingual.",
    "start": "2591550",
    "end": "2596973"
  },
  {
    "text": "And I know that\nyou have expressed interest in multilingual stuff. This could be a great\nplayground for doing that with QA and retrieval\nlike OpenQA as we've been doing",
    "start": "2596973",
    "end": "2607066"
  },
  {
    "text": "A bunch of other topics. I think the bottom line\nhere is just again,",
    "start": "2607066",
    "end": "2613210"
  },
  {
    "text": "this is like a\nrefrain in this class. NLU and IR are\nback together again",
    "start": "2613210",
    "end": "2618400"
  },
  {
    "text": "after being apart\nfor so long and this is having profound implications\nfor research and technology",
    "start": "2618400",
    "end": "2624100"
  },
  {
    "text": "development. So this is absolutely\na very exciting moment to participate in this\nresearch because there",
    "start": "2624100",
    "end": "2630380"
  },
  {
    "text": "is so much innovation\nyet to happen and it is having such an\nimpact on research and also",
    "start": "2630380",
    "end": "2637040"
  },
  {
    "text": "out in the wider world. Excellent. All right. Sid, do you want to take over?",
    "start": "2637040",
    "end": "2643450"
  },
  {
    "text": "So it's cool that\nretrieval isn't just heading NLU, it's\nheading everywhere, like vision and robotics\nas of this week.",
    "start": "2643450",
    "end": "2650815"
  },
  {
    "text": "We're starting to\nuse retrieval methods to do what's the best\nway to figure out how to do a new\ntask, maybe retrieve",
    "start": "2650815",
    "end": "2658140"
  },
  {
    "text": "some examples of a robot or\na human doing the same task, and then generating\nyour actions. So cool stuff.",
    "start": "2658140",
    "end": "2665790"
  },
  {
    "text": "Cool. All right. Let's see if this works.",
    "start": "2665790",
    "end": "2671020"
  },
  {
    "text": "Yeah. All right. So I'm going to pick up or try\nto pick up where I left off",
    "start": "2671020",
    "end": "2679040"
  },
  {
    "text": "last week and kind of give you\nthis evolution, this history lesson on how we got\nto the Transformer,",
    "start": "2679040",
    "end": "2685580"
  },
  {
    "text": "and then go from there\ninto tips and tricks for training big\nmodels generally,",
    "start": "2685580",
    "end": "2690800"
  },
  {
    "text": "and then end with a small\nlittle teaser on fine-tuning and parameter efficient\ntuning so you can use that",
    "start": "2690800",
    "end": "2696980"
  },
  {
    "text": "in your projects down the road. Cool. So just to kind of\nblaze past things, I'll get started by talking\nthrough where things were",
    "start": "2696980",
    "end": "2706609"
  },
  {
    "text": "pre-2017 when the\nTransformer paper came out on both the RNN and\nthe CNN side and tied",
    "start": "2706610",
    "end": "2712250"
  },
  {
    "text": "a lot of the innovation\naround the Transformer how modern convolutional\nneural nets, specifically",
    "start": "2712250",
    "end": "2720950"
  },
  {
    "text": "residual nets were working\nand the connections there are closer than the\nconnections to RNNs.",
    "start": "2720950",
    "end": "2726830"
  },
  {
    "text": "We walk through how we got\nto the self-attention block with this fancy code, which\nis basically just saying",
    "start": "2726830",
    "end": "2733175"
  },
  {
    "text": "you're splitting your\nheads and you can think of your heads in a\nself-attention block as the different kind of kernels\nor filters in a CNN layer,",
    "start": "2733175",
    "end": "2742079"
  },
  {
    "text": "and then kind of closing with\nthis full self-attention block where we're actually doing\nthe RNN-style attention.",
    "start": "2742080",
    "end": "2748799"
  },
  {
    "text": "And then this question\nof this non-linearity that we're adding\nat the end because without this non-linearity\nand this MLP that we're",
    "start": "2748800",
    "end": "2755910"
  },
  {
    "text": "adding to the end of\neach Transformer block, we're really just\ndoing weighted averages of linear transforms of values.",
    "start": "2755910",
    "end": "2764015"
  },
  {
    "text": "OK. So if we take this as\nground truth starting",
    "start": "2764015",
    "end": "2769680"
  },
  {
    "text": "point for what a\nTransformer block looks like very much inspired by\nthe ideas of CNNs and RNNs",
    "start": "2769680",
    "end": "2775590"
  },
  {
    "text": "with attention at the time, we\nhave this residual connection here, which is just\nkind of adding x over",
    "start": "2775590",
    "end": "2782910"
  },
  {
    "text": "and over again as we stack\nmore and more layers together. There's a problem. Can anyone spot the problem in\nthis implementation by itself?",
    "start": "2782910",
    "end": "2792180"
  },
  {
    "text": "So the problem is that\nthe activations blow up. We keep adding the same\ninput over and over again",
    "start": "2792180",
    "end": "2797779"
  },
  {
    "text": "as we go deeper. Eventually, specifically\nin the RNN attention layer",
    "start": "2797780",
    "end": "2803598"
  },
  {
    "text": "when we take this dot\nproduct between the queries and the keys, we're\ngoing to get overflow.",
    "start": "2803598",
    "end": "2808640"
  },
  {
    "text": "And so we need to do\nsomething about that. So while the first part of\nbuilding the Transformer",
    "start": "2808640",
    "end": "2814310"
  },
  {
    "text": "layer is very, very much\ninspired by history, the second part is\njust trying to make sure it doesn't fail\nand doesn't blow-up",
    "start": "2814310",
    "end": "2820370"
  },
  {
    "text": "and doesn't crash when\nwe try training it. So what's one thing that we\ncan do to avoid the blow up",
    "start": "2820370",
    "end": "2827720"
  },
  {
    "text": "of our activations? So layer normalization. So layer normalization, maybe\nbatch norm and layer norm",
    "start": "2827720",
    "end": "2834710"
  },
  {
    "text": "recovered earlier on, is\na very, very simple idea along each feature dimension. We're just going to normalize so\nthat each feature has a mean 0",
    "start": "2834710",
    "end": "2843350"
  },
  {
    "text": "standard deviation 1, which\nmeans that every time we add a residual\nconnection we're going",
    "start": "2843350",
    "end": "2848480"
  },
  {
    "text": "to normalize so that everything\ncomes back to a decent space. We're still able to learn the\nsame level of expressivity we",
    "start": "2848480",
    "end": "2854180"
  },
  {
    "text": "care about, we're\njust not necessarily going to keep blowing up\nor growing the magnitude",
    "start": "2854180",
    "end": "2859609"
  },
  {
    "text": "of our activations. And what that looks like is\ntwo calls to nn.LayerNorm,",
    "start": "2859610",
    "end": "2867170"
  },
  {
    "text": "with the dimensionality\nof our Transformer, and then adding\nthat for each block. We're just going\nto normalize each",
    "start": "2867170",
    "end": "2873320"
  },
  {
    "text": "x before we pass it into the\nattention and the MLP layers, respectively. Now there's a problem with this\nthat isn't obvious and actually",
    "start": "2873320",
    "end": "2881138"
  },
  {
    "text": "wasn't obvious to\nthe people building Transformers at the time. It wasn't really explained\nkind of till three years later,",
    "start": "2881138",
    "end": "2886980"
  },
  {
    "text": "which is that you have\noptimization issues when you do this.",
    "start": "2886980",
    "end": "2892310"
  },
  {
    "text": "Specifically, if you just try to\noptimize the naive Transformer with this layer norm in place\nwith conventional ML wisdom,",
    "start": "2892310",
    "end": "2898925"
  },
  {
    "text": "right, which is\nlike learning rate decay or a constant learning\nrate, bad things happen.",
    "start": "2898925",
    "end": "2905591"
  },
  {
    "text": "Specifically, I'm going\nto use the Hugging Face emoji as my stand-in for a\nTransformer, stuff happens.",
    "start": "2905592",
    "end": "2911810"
  },
  {
    "text": "The optimization crashes. It's either exploding\ngradients, it's either vanishing gradients. If you have someone in\n2018 or 2019 or 2020,",
    "start": "2911810",
    "end": "2918500"
  },
  {
    "text": "they would tell you one\nor the other is happening, but they're definitely\nno gradients that are stable throughout\nthe training process.",
    "start": "2918500",
    "end": "2924880"
  },
  {
    "text": "So you introduce this\nkind of weird thing that kind of comes out of almost\nnowhere, which is this warm-up",
    "start": "2924880",
    "end": "2932160"
  },
  {
    "text": "schedule that you\nsee a lot of the time in any code for training or\neven fine-tuning Transformers",
    "start": "2932160",
    "end": "2939690"
  },
  {
    "text": "these days. Now, this is actually\njust fun because I have the time to go through it.",
    "start": "2939690",
    "end": "2945839"
  },
  {
    "text": "Who came up with this? ",
    "start": "2945840",
    "end": "2956660"
  },
  {
    "text": "You're thinking? I'm thinking. I think I remember in\nthe Transformer paper they had a weird learning\nrate, but I don't remember.",
    "start": "2956660",
    "end": "2963620"
  },
  {
    "text": "So it is in the original\nTransformer paper. It's the main thing that\nthey to get this stably.",
    "start": "2963620",
    "end": "2968960"
  },
  {
    "text": "So it's one of the authors\nthat came up with it. But if you actually\nrun a Git blame on the first ever Transformer\ncode base from Google,",
    "start": "2968960",
    "end": "2975770"
  },
  {
    "text": "the tensor to tensor code\nbase, in the very first commit in the argparse flags for the\ndifferent optimizers you use,",
    "start": "2975770",
    "end": "2984660"
  },
  {
    "text": "there's one option that's\ncalled Noam, after Noam Shazeer. And in the annotated Transformer\nin the very first blog",
    "start": "2984660",
    "end": "2990220"
  },
  {
    "text": "post, that's what the\noptimizer is called, it's called Noam\nin Shazeer's code. And it's called\nthe Noam optimizer",
    "start": "2990220",
    "end": "2996290"
  },
  {
    "text": "for a really long\ntime until they just decided to call it just like\nlinear warmup cosine decay.",
    "start": "2996290",
    "end": "3002770"
  },
  {
    "text": "And so Noam Shazeer\ncame up with it. And if you were to\ngo back and think",
    "start": "3002770",
    "end": "3007990"
  },
  {
    "text": "about the problems and the\npapers he was working on at the time, he was actually\ndoing a lot of stuff",
    "start": "3007990",
    "end": "3013450"
  },
  {
    "text": "with different types\nof gradient descent",
    "start": "3013450",
    "end": "3019280"
  },
  {
    "text": "optimizers like RMSProp. Ada factor came out like a year\nafter the Transformer paper",
    "start": "3019280",
    "end": "3024290"
  },
  {
    "text": "came out. And he was really interested\nin looking at this problem, he'd like, OK,\nweights seemed to be",
    "start": "3024290",
    "end": "3030260"
  },
  {
    "text": "like if you just really\ninspect the gradients early on with this layer norm thing,\nvariant seems to be high",
    "start": "3030260",
    "end": "3036049"
  },
  {
    "text": "and you want to burn that in. And he's seeing this for LSTM,\nso he's doing this already for his LSTM work.",
    "start": "3036050",
    "end": "3043130"
  },
  {
    "text": "And then you're just\nlike, let's try this. It worked and no one\nreally questioned it.",
    "start": "3043130",
    "end": "3049100"
  },
  {
    "text": "It breaks conventional\nmachine learning wisdom. Why am I warming\nup my learning rate before I'm bringing it down? If I'm optimizing some surface\nI want to start kind of high,",
    "start": "3049100",
    "end": "3057740"
  },
  {
    "text": "move in, and then\nmaybe anneal it as I get closer to my minimum. But no one is able to explain\nwhy, till three years later",
    "start": "3057740",
    "end": "3066230"
  },
  {
    "text": "a paper comes out\nthat kind of steps through the specifics of\ntraining a Transformer",
    "start": "3066230",
    "end": "3071720"
  },
  {
    "text": "model on some data,\nlike synthetic data, with the Adam\noptimizer, and actually",
    "start": "3071720",
    "end": "3076940"
  },
  {
    "text": "tie it to the layer\nnormalization layers that we just added. We fixed one problem,\nwe added another.",
    "start": "3076940",
    "end": "3082310"
  },
  {
    "text": "So up top, we have\ngood gradients. So on the left here is\nthe gradient magnitude",
    "start": "3082310",
    "end": "3087797"
  },
  {
    "text": "and here's the update magnitude. So the gradients\nthat are computed and the updates\nthat are actually applied to the weights,\nwith warmup in blue.",
    "start": "3087797",
    "end": "3094250"
  },
  {
    "text": "In red, we have the same\nthing but without warmup. And what ends up happening\nis that gradients go",
    "start": "3094250",
    "end": "3099770"
  },
  {
    "text": "to zero, somehow, as you train. This is actually a weird\ngraph because as you're coming forward in\ntime, it's like",
    "start": "3099770",
    "end": "3105620"
  },
  {
    "text": "as you're training\nmore and more. So this is starting\nout and then this is--",
    "start": "3105620",
    "end": "3111150"
  },
  {
    "text": "yeah. And this is wherever\ntraining becomes unstable.",
    "start": "3111150",
    "end": "3117619"
  },
  {
    "text": "And then your updates also\nbecome super high variance. So they did some math\nand they bound the update",
    "start": "3117620",
    "end": "3124700"
  },
  {
    "text": "as a proportional\nto the square root",
    "start": "3124700",
    "end": "3132030"
  },
  {
    "text": "of the dimensionality\nof your Transformer over the input norm\nthat's coming in. So if your input norm like if\nthe size of your activation",
    "start": "3132030",
    "end": "3139800"
  },
  {
    "text": "is sufficiently large,\nyour layer norm gradient",
    "start": "3139800",
    "end": "3144930"
  },
  {
    "text": "is going to be completely,\ncompletely screwed. So what they end up\ndoing is like, OK,",
    "start": "3144930",
    "end": "3151790"
  },
  {
    "text": "so warmup is necessary because\nit helps you get the Adam optimizer to move slowly\nenough at the beginning",
    "start": "3151790",
    "end": "3161390"
  },
  {
    "text": "so that we're saturating the\ngradients, we're like OK. And then when we go full\nthrottle like things",
    "start": "3161390",
    "end": "3167059"
  },
  {
    "text": "are generally stable,\nthe activations norms aren't changing all too much.",
    "start": "3167060",
    "end": "3173588"
  },
  {
    "text": "They're changing in\na predictable way and we can start to\nhandle that and then conventional ML kicks in.",
    "start": "3173588",
    "end": "3179210"
  },
  {
    "text": "But it's weird. It's also weird that it\ntook three years later. And some people still\ndon't buy this explanation,",
    "start": "3179210",
    "end": "3184410"
  },
  {
    "text": "but it's the best\nexplanation I've got to why we need that warmup. So general wisdom, you're\nfine-tuning or pre-training",
    "start": "3184410",
    "end": "3191010"
  },
  {
    "text": "a transformer, warmup for at\nleast 5% of your full training, and then start to hang.",
    "start": "3191010",
    "end": "3196020"
  },
  {
    "text": "It just helps. Can I ask this-- I don't know this paper. So is there some data dependency\nor some assumption about what",
    "start": "3196020",
    "end": "3205290"
  },
  {
    "text": "the data will be like? Because it seems like\nyou said, hey, look, after a while we can relax. These updates are\nsmall or reasonable",
    "start": "3205290",
    "end": "3213090"
  },
  {
    "text": "but the world could\ndo a lot to you. And if you shifted\ngenres or data types,",
    "start": "3213090",
    "end": "3218250"
  },
  {
    "text": "they would go back into\nbeing very unstable. Yeah. So I think in this paper\nthey're looking at what",
    "start": "3218250",
    "end": "3223710"
  },
  {
    "text": "I'll call nice datasets. They're looking at the\nWikitext-2's of the world that are somewhat predictable. It's all Wikipedia-homogenized\nlanguage.",
    "start": "3223710",
    "end": "3231140"
  },
  {
    "text": "But even when you're training\nthe modern like the really big Transformers these days, even\nafter all of these tricks,",
    "start": "3231140",
    "end": "3236609"
  },
  {
    "text": "you're still going\nto have bad batches. Just like really, really\nunpredictable things",
    "start": "3236610",
    "end": "3241710"
  },
  {
    "text": "that are low likelihood\nunder your model that are going to\ncause big updates in the middle of\ntraining that are going to completely crash your run.",
    "start": "3241710",
    "end": "3247380"
  },
  {
    "text": "So this happened tons\nof times while we were training the 355\nmillion parameter models. This happened every\ntime you're training",
    "start": "3247380",
    "end": "3253890"
  },
  {
    "text": "any big model in the 1\nbillion plus parameter range. So the EleutherAI folks,\nthis happens all of the time.",
    "start": "3253890",
    "end": "3260200"
  },
  {
    "text": "The T5 models have this thing. In one of the notes in the\nGitHub repository for training",
    "start": "3260200",
    "end": "3267100"
  },
  {
    "text": "a T5, which is if\ntraining fails, rewind to the latest checkpoint,\nrerandomize your data order,",
    "start": "3267100",
    "end": "3272829"
  },
  {
    "text": "and then try again\nand it won't crash. And that's how kind of modern ML\nor most modern language models",
    "start": "3272830",
    "end": "3280720"
  },
  {
    "text": "are trained right now. We don't know how\nto avoid it yet. We think it's tied to the data. We can't isolate it, so why\nnot just reroll and try again?",
    "start": "3280720",
    "end": "3289480"
  },
  {
    "text": "Eventually, you'll just\nkeep making progress. Cool. So question.",
    "start": "3289480",
    "end": "3296080"
  },
  {
    "text": "Unpack the graphs once again. What do the different colors\nrepresent in every role? Yeah.",
    "start": "3296080",
    "end": "3301278"
  },
  {
    "text": "So the question was what the\ndifferent colors represent. So up top is blue with the\nkind of traditional Transformer",
    "start": "3301278",
    "end": "3307599"
  },
  {
    "text": "learning rate, so warm\nup, and then go down. Red is the no warmup let's\njust start the learning rate",
    "start": "3307600",
    "end": "3315430"
  },
  {
    "text": "high, and then taper. So red is bad, blue is good. ",
    "start": "3315430",
    "end": "3322010"
  },
  {
    "text": "And inside the graph, the\ndark red would be more-- Yeah.",
    "start": "3322010",
    "end": "3327020"
  },
  {
    "text": "So the way you interpret\nthis graph and the paper is linked in at the\nbottom of the slide, but you can think of\nlike the furthest back,",
    "start": "3327020",
    "end": "3335750"
  },
  {
    "text": "magnitude is this is\nbasically plotting the mean standard deviation\nof the updates across layers.",
    "start": "3335750",
    "end": "3341720"
  },
  {
    "text": "The furthest back\nis like batch 0. As you get further\nin, you get batch 100",
    "start": "3341720",
    "end": "3349369"
  },
  {
    "text": "or batch 400 or whatever.  Question.",
    "start": "3349370",
    "end": "3354760"
  },
  {
    "text": "I wonder to what extent the\nwarmup and the rate requirement",
    "start": "3354760",
    "end": "3362020"
  },
  {
    "text": "got to be related with the\nchoice of optimizer event. Because I ran into\nsome problems where",
    "start": "3362020",
    "end": "3369970"
  },
  {
    "text": "I found that using Adamax\nof the infinity norm won't work because I've got this\nlevel of dropout or whatever.",
    "start": "3369970",
    "end": "3375700"
  },
  {
    "text": "Is there any guidance of all\nthese big hyperparameters that go into this tune where\nif I pull one lever I should",
    "start": "3375700",
    "end": "3383770"
  },
  {
    "text": "be pushing one down or\nchoose this optimizer I should do another? Because it feels\nlike, I mean, taking",
    "start": "3383770",
    "end": "3390903"
  },
  {
    "text": "three years it feels a\nlittle bit like the Wild West which is what it is. Yeah.",
    "start": "3390903",
    "end": "3396580"
  },
  {
    "text": "So if I were to\nparaphrase your question, it's like if you decide\nto change anything about the current recipe,\nlike change your optimizer,",
    "start": "3396580",
    "end": "3403510"
  },
  {
    "text": "change dropout, change\nyour learning rate, are there rules of\nthumb for what else you need to change to\nget things to work?",
    "start": "3403510",
    "end": "3410875"
  },
  {
    "text": "No. [LAUGHTER]  So part of why I led\nwith how we got here",
    "start": "3410875",
    "end": "3420690"
  },
  {
    "text": "starting from the\nhistorical context was to unpack a lot of\nthis folk knowledge. Because it's still\nat the point where",
    "start": "3420690",
    "end": "3427350"
  },
  {
    "text": "optimizing these models\nespecially as we go bigger is it's still concentrated\nin the minds and experience",
    "start": "3427350",
    "end": "3434940"
  },
  {
    "text": "of a very small\nnumber of people. Because who's\ntrained a 7 billion parameter language model, or\nwho's trained a 100 billion",
    "start": "3434940",
    "end": "3440820"
  },
  {
    "text": "parameter language model? Where does that\nskill set come from? When you're talking\nabout a training run that cost millions of dollars\nto develop plus however",
    "start": "3440820",
    "end": "3449340"
  },
  {
    "text": "much the compute\ncosts, how many things are you really going to\nbe trying at the end? What things can you\nextrapolate from?",
    "start": "3449340",
    "end": "3456720"
  },
  {
    "text": "So folks at OpenAI\nhave definitely done scaling laws\nresearch where they're trying these different things\nin some bounded search space.",
    "start": "3456720",
    "end": "3463480"
  },
  {
    "text": "But if you were to invent\na brand new optimizer, it looks at maybe second,\nthird, fourth order",
    "start": "3463480",
    "end": "3469320"
  },
  {
    "text": "moments of your gradients. Maybe do something\nfancy relative to how",
    "start": "3469320",
    "end": "3474450"
  },
  {
    "text": "things are changing\nover time and you were to just try and apply\nit to the biggest language",
    "start": "3474450",
    "end": "3479670"
  },
  {
    "text": "model you could train. I have no idea what I would\ntell you in terms of what things you should change beyond\nsome obvious things.",
    "start": "3479670",
    "end": "3486540"
  },
  {
    "text": "Maybe don't set the learning\nrate to be ridiculously high when starting out. ",
    "start": "3486540",
    "end": "3493530"
  },
  {
    "text": "If you have a batch\nof data that's let's just say during training you\ncome across a bad batch of data",
    "start": "3493530",
    "end": "3499550"
  },
  {
    "text": "that happens to cause\nthe destabilization with the gradients and then you\nrewind back to your checkpoint",
    "start": "3499550",
    "end": "3505910"
  },
  {
    "text": "and you take that same\nexact batch of data but instead of running\nit through training",
    "start": "3505910",
    "end": "3511325"
  },
  {
    "text": "when gradients are\nenabled if you just run it through inference, will\nthat bad batch of data have caused anomalous behavior\nfrom the model during inference",
    "start": "3511325",
    "end": "3521000"
  },
  {
    "text": "or is it strictly just\na backpropagation issue? So when we debug-- or when we debug this,\nso we, a couple of years",
    "start": "3521000",
    "end": "3528650"
  },
  {
    "text": "ago, trained some, by\ntoday's standards, really, really small language models\nlike 124 million to 355 million",
    "start": "3528650",
    "end": "3536120"
  },
  {
    "text": "scale. We were noticing this problem. The way we debugged it was by\nlooking at the gradients, which",
    "start": "3536120",
    "end": "3542057"
  },
  {
    "text": "didn't tell us much,\nbut then we just looked at activation norms\nper layer and that's",
    "start": "3542057",
    "end": "3547220"
  },
  {
    "text": "how we actually debugged this. So looking at the\nforward pass, looking at the magnitudes\nof each layer where",
    "start": "3547220",
    "end": "3553440"
  },
  {
    "text": "we thought we could possibly\nbe overflowing or underflowing, that's exactly how\nwe debugged it.",
    "start": "3553440",
    "end": "3559109"
  },
  {
    "text": "But we didn't debug\nit at the batch level, we debugged it as\na function of time. Because a single batch isn't\ngoing to perturb everything.",
    "start": "3559110",
    "end": "3566670"
  },
  {
    "text": "A series of batches, maybe two\nor three, who knows how many. Eventually, you're going to\nfall under some trajectory",
    "start": "3566670",
    "end": "3572220"
  },
  {
    "text": "where things get bad,\nyour activations blow up. So we would be able\nto deterministically rewind to that\ncheckpoint then determine",
    "start": "3572220",
    "end": "3578640"
  },
  {
    "text": "with step-through training\nand log every activation, which is expensive,\nbut that's how",
    "start": "3578640",
    "end": "3584160"
  },
  {
    "text": "we were able to get to\nthe bottom of the problem. But I don't think we have\ntools for figuring out",
    "start": "3584160",
    "end": "3590039"
  },
  {
    "text": "which batch of data or which\nsequence of batches of data were the actual triggers\nfor that behavior.",
    "start": "3590040",
    "end": "3596250"
  },
  {
    "text": "I guess I was just\ncurious specifically about if the same data\nthat causes destabilization in training can cause\nanomalous behavior during just",
    "start": "3596250",
    "end": "3605850"
  },
  {
    "text": "a normal forward pass. It probably would. And there's some\nmore recent work",
    "start": "3605850",
    "end": "3611190"
  },
  {
    "text": "about how to quantize\nthese models, how to get a Transformer that's\ntrained with 16-bit precision",
    "start": "3611190",
    "end": "3618119"
  },
  {
    "text": "or to run with 8-bit precision\nby intelligently bucketing",
    "start": "3618120",
    "end": "3623490"
  },
  {
    "text": "floats from Tim Dettmers, who's\na PhD student up at U-Dub, who has this theory on something\ncalled outlier features that",
    "start": "3623490",
    "end": "3630420"
  },
  {
    "text": "show up in these really big\nmodels that kind of try and get at this, but more of an art\nthan a science right now.",
    "start": "3630420",
    "end": "3636450"
  },
  {
    "text": "Thank you very much. So are we done now that\nwe fixed this layer norm",
    "start": "3636450",
    "end": "3643670"
  },
  {
    "text": "stuff, the learning rate\nstuff, all of this stuff that you have to\ntransform the work? Kind of, right?",
    "start": "3643670",
    "end": "3648800"
  },
  {
    "text": "So over the last few years,\ntraining has been stable. People want to milk the\nmost of the Transformer,",
    "start": "3648800",
    "end": "3655940"
  },
  {
    "text": "especially as they scale up. So they do a couple of things. So one, when you're training\nan attention layer and you're",
    "start": "3655940",
    "end": "3663320"
  },
  {
    "text": "projecting two queries and keys. At sufficient scale, the bias\nterm in each linear layer",
    "start": "3663320",
    "end": "3669200"
  },
  {
    "text": "is wx plus b. You can get rid of the b's\nbecause it's not really doing anything and it saves\na little bit of compute.",
    "start": "3669200",
    "end": "3674510"
  },
  {
    "text": "So let's get rid of them. That's the first\nthing you throw out. There are different activations\nthat have been invented",
    "start": "3674510",
    "end": "3680150"
  },
  {
    "text": "and different types of cool ways\nto just better fit your data.",
    "start": "3680150",
    "end": "3685797"
  },
  {
    "text": "So there's this SwishGLU. So a gated linear unit. Actually, it defines a\nseparate weight matrix",
    "start": "3685798",
    "end": "3691610"
  },
  {
    "text": "as part of the activation. And then a Swish is like\na sigmoid activation",
    "start": "3691610",
    "end": "3699295"
  },
  {
    "text": "that applies to one part of\nthe weight and not the other. I have code for all of this. It works better. This is actually the\nactivation of choice",
    "start": "3699295",
    "end": "3707210"
  },
  {
    "text": "now in most Transformer\nimplementations. So LLAMA was trained with this,\nPaLM was trained with this.",
    "start": "3707210",
    "end": "3712745"
  },
  {
    "text": "It worked really well. One thing that folks noticed\nis that moving the layer norm",
    "start": "3712745",
    "end": "3719809"
  },
  {
    "text": "to happen before\nyou actually feed it",
    "start": "3719810",
    "end": "3724860"
  },
  {
    "text": "through the attention\nor the MLP layers instead of after is a\nmore stabilizing force.",
    "start": "3724860",
    "end": "3730410"
  },
  {
    "text": "It is actually important. Also, a layer norm\nhas trainable weights. So some papers\ndecide to be like,",
    "start": "3730410",
    "end": "3736589"
  },
  {
    "text": "you don't actually need\nthese trainable parameters for mean and variance,\nyou can actually just divide by the\nmean square or the RMS",
    "start": "3736590",
    "end": "3744690"
  },
  {
    "text": "of your entire activation\nfeature, all things to just get rid of irrelevant\nflops because we're",
    "start": "3744690",
    "end": "3750090"
  },
  {
    "text": "training massive\nmodels and we're trying to do the bigger model\non the compute that we have.",
    "start": "3750090",
    "end": "3756570"
  },
  {
    "text": "Oh, here's the code for\nSwishGLU activation and RMSNorm.",
    "start": "3756570",
    "end": "3762210"
  },
  {
    "text": "So SwishGLU is basically a\nsigmoid projection layer. It's basically saying let's\ntake this input feature,",
    "start": "3762210",
    "end": "3767940"
  },
  {
    "text": "project it into two\nseparate chunks. One chunk becomes a gating\nvalue in a gated recurrent",
    "start": "3767940",
    "end": "3775110"
  },
  {
    "text": "unit in the RNN literature and\none becomes the actual value. You apply the\nsigmoid to the gate",
    "start": "3775110",
    "end": "3781170"
  },
  {
    "text": "and then multiply it\nelement-wise with the value and you get your new thing. Works really well.",
    "start": "3781170",
    "end": "3787299"
  },
  {
    "text": "An RMSNorm is\nliterally just dividing by the norm of the\nvector instead of trying to learn anything fancy.",
    "start": "3787300",
    "end": "3794700"
  },
  {
    "text": "Cool. That's what the modern\nTransformer looks like. So that's it for the\nevolution of the Transformer.",
    "start": "3794700",
    "end": "3802080"
  },
  {
    "text": "As far as I know, nothing\nin the last two weeks have changed\ndrastically from this. In the last two\nweeks, to what extent",
    "start": "3802080",
    "end": "3809970"
  },
  {
    "text": "let's say we are doing\na fine-tune instead of the full train or we're\ndoing LoRA on top of it.",
    "start": "3809970",
    "end": "3817218"
  },
  {
    "text": "Would we still want to follow\nthese kinds of guidelines or this is specific to\njust doing all of the data,",
    "start": "3817218",
    "end": "3823680"
  },
  {
    "text": "doing a full pre-train? Yeah. So the question is, what\nof this do we really",
    "start": "3823680",
    "end": "3828990"
  },
  {
    "text": "need if we're\nfine-tuning or if we're kind of doing\nparameter-efficient fine-tuning? Is this only necessary\nfor pre-training?",
    "start": "3828990",
    "end": "3834420"
  },
  {
    "text": "So I've started using SwishGLU\ninstead of any other activation",
    "start": "3834420",
    "end": "3839880"
  },
  {
    "text": "everywhere, even\nfor a two-layer MLP. It tends to work better.",
    "start": "3839880",
    "end": "3845049"
  },
  {
    "text": "So take that with a\ngrain of salt. Everything else you can probably\nnot care about.",
    "start": "3845050",
    "end": "3851190"
  },
  {
    "text": "The RMSNorm. The pre-NORM is probably just\nlike a general rule of thumb if you're adding any\nTransformer layers just",
    "start": "3851190",
    "end": "3857380"
  },
  {
    "text": "because it is demonstrably more\nstable, but other than that, no.",
    "start": "3857380",
    "end": "3863840"
  },
  {
    "text": "Other questions\nhere before we move how to train on lots\nand lots of compute?",
    "start": "3863840",
    "end": "3871680"
  },
  {
    "text": "Cool. So let's talk about\ntraining at scale. So I'll start with a story--",
    "start": "3871680",
    "end": "3880020"
  },
  {
    "text": "my story. So I am not old but I have seen\na few things as far as language",
    "start": "3880020",
    "end": "3889470"
  },
  {
    "text": "models have gone, right? So 2018 is when I think did my\nfirst deep learning tutorial.",
    "start": "3889470",
    "end": "3895950"
  },
  {
    "text": "I trained a MNIST, the typical\ntwo-layer, four-layer MLP",
    "start": "3895950",
    "end": "3901320"
  },
  {
    "text": "for classification. There's actually a line there. It's the 100,000 parameter line.",
    "start": "3901320",
    "end": "3908220"
  },
  {
    "text": "That's 2018. As I kind of start my PhD in\n2019, I'm doing more NLP stuff.",
    "start": "3908220",
    "end": "3914160"
  },
  {
    "text": "I'm looking at\nword vectors, RNNs, some more sophisticated things. I'm getting up to a\nmillion parameters.",
    "start": "3914160",
    "end": "3921450"
  },
  {
    "text": "2020, I kind of branch out\nfrom the small NLP stuff. I'm doing more intensive NLPs.",
    "start": "3921450",
    "end": "3928310"
  },
  {
    "text": "So looking at tasks like\nsummarization, training models with 10 million parameters.",
    "start": "3928310",
    "end": "3933480"
  },
  {
    "text": "And then by 2021,\nthe biggest models",
    "start": "3933480",
    "end": "3938650"
  },
  {
    "text": "I was training when I switched\ninto multi-modality, robotics, looking at visual\nquestion answering, 18",
    "start": "3938650",
    "end": "3944119"
  },
  {
    "text": "million parameters. And at the time, the\nstandard pipeline for me and I think this\nwas the standard pipeline",
    "start": "3944120",
    "end": "3949362"
  },
  {
    "text": "for a lot of grad students\nthat I talked to then, is I'd be able to train\nmost of my things on one GPU",
    "start": "3949362",
    "end": "3956170"
  },
  {
    "text": "or even my laptop CPU for\na maximum of a few hours.",
    "start": "3956170",
    "end": "3961510"
  },
  {
    "text": "That is what a\ntraining run would take, at least for most of the\nthings I was doing on a day to day.",
    "start": "3961510",
    "end": "3968390"
  },
  {
    "text": "But in 2021, Percy's like, hey,\nthis GPT-3 thing seems cool.",
    "start": "3968390",
    "end": "3975002"
  },
  {
    "text": "Let's at least\nfigure out if we can get an academic lab to try\nand train a GPT-2, the earlier",
    "start": "3975002",
    "end": "3980750"
  },
  {
    "text": "generation. So clocking in at 124\nmillion parameters, which is notably an order of\nmagnitude bigger than anything",
    "start": "3980750",
    "end": "3987770"
  },
  {
    "text": "I trained at the time. So why I decided to do\nthis is still beyond me, but I learned a lot\nof useful things.",
    "start": "3987770",
    "end": "3993525"
  },
  {
    "text": "One of the useful\nthings that I learned is that training a 124 million\nparameter model on a decent GPU",
    "start": "3993525",
    "end": "4001809"
  },
  {
    "text": "that we had access\nto, at the time, with a batch size greater\nthan 4 would go out of memory, which is bad because\na batch size of 4 is small",
    "start": "4001810",
    "end": "4010300"
  },
  {
    "text": "and we wanted to ideally train\nwith a batch size of 512. So there was a\nsimple trick and it's",
    "start": "4010300",
    "end": "4016642"
  },
  {
    "text": "called gradient\naccumulation, which is just I'm going to run\nbatch sizes of 4 however",
    "start": "4016642",
    "end": "4021820"
  },
  {
    "text": "many times it takes to get\ninto 512, and then do an update after processing all of\nthose batches sequentially.",
    "start": "4021820",
    "end": "4027992"
  },
  {
    "text": "So I'm just going to keep\naccumulating the gradients. And PyTorch makes\nit really easy. It's just a for loop\nand an if statement.",
    "start": "4027992",
    "end": "4036030"
  },
  {
    "text": "But if you do the\nmath, it's 100 days to train on that single\nGPU for 400,000 steps.",
    "start": "4036030",
    "end": "4041099"
  },
  {
    "text": "So how do we go from\nthis clock of 100 days to something reasonable?",
    "start": "4041100",
    "end": "4046710"
  },
  {
    "text": "That's what we're\ngoing to talk about. So the scaling toolbox, at least\nas far as we were concerned,",
    "start": "4046710",
    "end": "4057140"
  },
  {
    "text": "it ended up looking like three\ndifferent parts across 16 GPUs",
    "start": "4057140",
    "end": "4063769"
  },
  {
    "text": "because Percy and Chris Rea and\nI think Chris and Dan and Chris Manning and the NLP group\ndecided to invest upfront",
    "start": "4063770",
    "end": "4070940"
  },
  {
    "text": "in really powerful GPU\nmachines so we could actually train on 16 GPUs at once.",
    "start": "4070940",
    "end": "4077840"
  },
  {
    "text": "For reference, 16 GPUs on AWS\nto just rent on an hourly basis",
    "start": "4077840",
    "end": "4084260"
  },
  {
    "text": "is $56 an hour now. $56 an hour if you want\nto just sit on them.",
    "start": "4084260",
    "end": "4091430"
  },
  {
    "text": "But if you're willing to\nlet anyone who has the money and wants to sit on\nthem like preempt you,",
    "start": "4091430",
    "end": "4098060"
  },
  {
    "text": "you can get them\nfor $16 an hour. So across four days,\nthat's not the worst.",
    "start": "4098060",
    "end": "4103189"
  },
  {
    "text": "It's not great,\nbut totally doable. So the scaling toolbox\nwe ended up looking at",
    "start": "4103189",
    "end": "4110549"
  },
  {
    "text": "was data parallelism. You can think about this\nas literally just divide and conquer. How do I just parallelize\nwork across all",
    "start": "4110550",
    "end": "4118649"
  },
  {
    "text": "of these GPUs instead of one? Mixed precision training. And we're going to talk a little\nbit about what that means.",
    "start": "4118649",
    "end": "4127120"
  },
  {
    "text": "And then this interesting\nidea called ZeRO redundancy, which is about minimizing the\nmemory footprint of training.",
    "start": "4127120",
    "end": "4133380"
  },
  {
    "text": "And then later on as\nyou want to scale up to hundreds of billions of\nparameters on 256, 512, 1024,",
    "start": "4133380",
    "end": "4142979"
  },
  {
    "text": "2048 GPUs, we'll\ntalk like there are things that come in handy\nlike model parallelism.",
    "start": "4142979",
    "end": "4149868"
  },
  {
    "text": "There are things to consider\nlike hardware and software limitations. But some of you\nmight be here looking",
    "start": "4149868",
    "end": "4157420"
  },
  {
    "text": "at me which is like, OK,\ndo I need any of this stuff if I'm not training\nreally big models if I'm just fine-tuning stuff?",
    "start": "4157420",
    "end": "4162520"
  },
  {
    "text": "A lot of these tips\nand tricks, you",
    "start": "4162520",
    "end": "4167920"
  },
  {
    "text": "may not have access\nto 100 GPUs or even 8, but you might have\naccess to 2 or 4.",
    "start": "4167920",
    "end": "4173859"
  },
  {
    "text": "Comes in handy. A lot of the ideas\nhere, so ideas that I'm using when I'm\ntraining stuff on my laptop",
    "start": "4173859",
    "end": "4180231"
  },
  {
    "text": "or when I'm trying\nto run inference with the latest big model\nthat is publicly released. So it's useful.",
    "start": "4180231",
    "end": "4186210"
  },
  {
    "text": "But please ask questions\nif things become too hazy or too not useful.",
    "start": "4186210",
    "end": "4192858"
  },
  {
    "text": "Can I just ask where\nyou're coming to this? But for people\nrelying on Colab data",
    "start": "4192858",
    "end": "4198390"
  },
  {
    "text": "parallelism might\nactually not help. Yeah, Colab, you're still\nlimited to a single GPU.",
    "start": "4198390",
    "end": "4204960"
  },
  {
    "text": "Mixed precision and I'm guessing\nZeRO redundancy might help. So mixed precision\nwould help, kind of,",
    "start": "4204960",
    "end": "4213660"
  },
  {
    "text": "definitely for\nrunning inference, and ZeRO redundancy would\nalso help running inference. What's the ZeRO redundancy heat?",
    "start": "4213660",
    "end": "4220320"
  },
  {
    "text": "So ZeRO redundancy has\nadd-on that they wrote up in a paper called\nZeRO-Infinity, which",
    "start": "4220320",
    "end": "4226020"
  },
  {
    "text": "is what if I didn't put all\nmy weights on the GPU at once? What if I just put some\nof them on my CPU, RAM,",
    "start": "4226020",
    "end": "4232050"
  },
  {
    "text": "or even in NVMe SSD storage? So actually turning your laptop\ninto a more powerful workhorse",
    "start": "4232050",
    "end": "4238410"
  },
  {
    "text": "than a Colab GPU.  Cool. So this is a toy example kind of\ngoing through data parallelism.",
    "start": "4238410",
    "end": "4249010"
  },
  {
    "text": "We're running low on time-ish. So this is MNIST with an MLP.",
    "start": "4249010",
    "end": "4254500"
  },
  {
    "text": "We're trying to\ndo classification. It's the typical\nPyTorch workflow. I'm going to define\nan end-up module.",
    "start": "4254500",
    "end": "4261400"
  },
  {
    "text": "I'm going to define a batch\nsize, a data loader that's going to load from the\ntorchvision dataset, and then I'm just\ngoing to run lots",
    "start": "4261400",
    "end": "4269500"
  },
  {
    "text": "and lots of gradient steps. The idea here is how\ndo we parallelize this across multiple\nworkers, multiple GPUs?",
    "start": "4269500",
    "end": "4278949"
  },
  {
    "text": "Well, that batch size you see\nthere is totally divisible, especially given that what\nwe're doing at the end",
    "start": "4278950",
    "end": "4285190"
  },
  {
    "text": "when we compute the loss\nis just take an average. An average of averages\nis still the average.",
    "start": "4285190",
    "end": "4291552"
  },
  {
    "text": "That's the idea we're\ngoing to work with. The mean of means is\nstill the global mean. So just like in\nCPU/LAN where you",
    "start": "4291552",
    "end": "4298750"
  },
  {
    "text": "can think about\nSIMD instructions, single instruction\nmultiple data. So this is how most graphics\nand media operations",
    "start": "4298750",
    "end": "4306400"
  },
  {
    "text": "work on your laptops. We're going to now think\nabout the SPMD paradigm. I'm going to write one\nprogram and it's just",
    "start": "4306400",
    "end": "4312730"
  },
  {
    "text": "going to automatically scale\nto being split across however many machines\nbecause we're going to split the data across\nmultiple machines.",
    "start": "4312730",
    "end": "4320500"
  },
  {
    "text": "It seems hard, but as of PyTorch\n1.4, a lot of the hard parts are taken care of for you.",
    "start": "4320500",
    "end": "4325938"
  },
  {
    "text": "These are the only\nlines you need to change in the implementation. Two of them are\nimport statements. So the first thing\nwe're going to do",
    "start": "4325938",
    "end": "4332800"
  },
  {
    "text": "is we're going to just\ncreate something called a DistributedSampler, which\nis going to automatically partition our data across\na number of workers",
    "start": "4332800",
    "end": "4339350"
  },
  {
    "text": "we define upfront. We're defining a\nworld size of 8. So that means we're\ntraining on eight GPUs.",
    "start": "4339350",
    "end": "4345402"
  },
  {
    "text": "So it's going to\npartition our data into eight different subsets\nthat each worker gets to go through.",
    "start": "4345402",
    "end": "4350880"
  },
  {
    "text": "We're going to\nwrap our nn.module with this nice little\nwrapper, this distributed data-parallel\nwrapper, which is just",
    "start": "4350880",
    "end": "4357320"
  },
  {
    "text": "going to sync the gradients\nfor us behind the scenes. And then we're going to run\nthis with a special command",
    "start": "4357320",
    "end": "4364310"
  },
  {
    "text": "called torchrun,\nwhich is just going to inject a bunch of\nenvironment variables so that we can get some\nstatistics about our local rank",
    "start": "4364310",
    "end": "4372830"
  },
  {
    "text": "who's the guy who\nshould be printing stuff to the screen, who's the guy who\nshould be logging stuff where",
    "start": "4372830",
    "end": "4378440"
  },
  {
    "text": "each worker lives. And that's about it.",
    "start": "4378440",
    "end": "4383870"
  },
  {
    "text": "And you can do all of this and\nyou just parallelize naively across 16 GPUs. You get not quite a 16x speedup\nbecause there is some overhead",
    "start": "4383870",
    "end": "4391940"
  },
  {
    "text": "from communication. It's like seven days. That's cool. It was not good\nenough because we",
    "start": "4391940",
    "end": "4398030"
  },
  {
    "text": "were trying to train lots\nof models, reproducibly five seeds for 10\ndifferent model types.",
    "start": "4398030",
    "end": "4403920"
  },
  {
    "text": "So 50 models. So we needed to go a\nlittle faster than this. So let's talk about\nmemory footprints.",
    "start": "4403920",
    "end": "4411860"
  },
  {
    "text": "When I am training any model\nwith an Adam optimizer, how much memory does\njust storing that model",
    "start": "4411860",
    "end": "4419449"
  },
  {
    "text": "and the optimizer\nweights take up?  So in 32-bit\nprecision, our model",
    "start": "4419450",
    "end": "4427750"
  },
  {
    "text": "is going to have parameters\nwhere each parameter is stored with 32 bits, that's\na float, gradients, 32 bits.",
    "start": "4427750",
    "end": "4435910"
  },
  {
    "text": "And now your optimizer\nis also going to do this weird\nthing where it's going to have its\nown separate copy of the parameters\nlike duplicating",
    "start": "4435910",
    "end": "4441490"
  },
  {
    "text": "a little bit of work there. That's also 32 bits. And then Adam tracks\nmomentum and variance,",
    "start": "4441490",
    "end": "4448030"
  },
  {
    "text": "the first and second\norder of the gradients. So that's another 64\nbytes or bits right there.",
    "start": "4448030",
    "end": "4454179"
  },
  {
    "text": "So the lower bound\non static memory just like storing\nthe stuff on a GPU is 20 bytes times the number\nof parameters that you have.",
    "start": "4454180",
    "end": "4463180"
  },
  {
    "text": "That doesn't include activations\nat all for these larger transformer models. But if I want to\nkeep around every",
    "start": "4463180",
    "end": "4468257"
  },
  {
    "text": "buffer every intermediate\nmatrix as I pass it through the network. That takes up way more\nspace, but this at least",
    "start": "4468257",
    "end": "4475420"
  },
  {
    "text": "gives us something that\nwe can reason about. The training\nimplication of this is that if I want to fit a model\nwith a billion parameters,",
    "start": "4475420",
    "end": "4482480"
  },
  {
    "text": "it's going to take about 18\ngigs resting, 31 gigs of GPU",
    "start": "4482480",
    "end": "4488450"
  },
  {
    "text": "RAM with a batch size of\n1, which is problematic because most GPUs\nthen cap at 24 gigs.",
    "start": "4488450",
    "end": "4495850"
  },
  {
    "text": "The really expensive\nones now have 40 or 80, but this is still bad.",
    "start": "4495850",
    "end": "4501390"
  },
  {
    "text": "175 billion\nparameters would take three terabytes of RAM,\nnot storage, just RAM,",
    "start": "4501390",
    "end": "4506910"
  },
  {
    "text": "without activations. With activations,\nyou're probably looking at 10 terabytes. Good luck. Is that batch size\nof 1 in there?",
    "start": "4506910",
    "end": "4513420"
  },
  {
    "text": "The numbers in bold\nare batch size of 1. Numbers not in bold are just\nputting it on the thing.",
    "start": "4513420",
    "end": "4521740"
  },
  {
    "text": "And things you should\nknow about floats, it was a standard defined\nin this IEEE document.",
    "start": "4521740",
    "end": "4529810"
  },
  {
    "text": "You have a 1-bit\nsign, 8-bit exponent, 23-bit scientific notation,\nall the stuff that",
    "start": "4529810",
    "end": "4535540"
  },
  {
    "text": "happens after the exponent. Wide range, up to 1e38. And the question is like,\ndo you need that range?",
    "start": "4535540",
    "end": "4541750"
  },
  {
    "text": "The answer is kind\nof, but not really. So the mixed precision\nmemory footprint.",
    "start": "4541750",
    "end": "4547030"
  },
  {
    "text": "If I'm training a model in\nmixed precision what that means is that I'm just going to run\neverything in a forward pass",
    "start": "4547030",
    "end": "4553300"
  },
  {
    "text": "and part of the backwards pass\nin 16-bit precision instead of 32-bit precision.",
    "start": "4553300",
    "end": "4559570"
  },
  {
    "text": "Notably what that means is now\nI'm storing my parameters in 16 bits and my\ngradients in 16 bits,",
    "start": "4559570",
    "end": "4565300"
  },
  {
    "text": "all of those intermediate\nactivations that take up lots and lots and\nlots of memory, especially as you go bigger,\nare halved, which is great.",
    "start": "4565300",
    "end": "4573430"
  },
  {
    "text": "But the weird part\nabout mixed precision is not everything\nis mixed precision. So your optimizer to stably\nupdate your model still",
    "start": "4573430",
    "end": "4580640"
  },
  {
    "text": "needs the 32-bit parameter\ncopies, your 32-bit momentum, and 32-bit variants.",
    "start": "4580640",
    "end": "4587870"
  },
  {
    "text": "But you've dropped 4 bytes. And those 4 bytes\nare kind of useful. Yet training with mixed\nprecision, at least",
    "start": "4587870",
    "end": "4593989"
  },
  {
    "text": "a couple of years ago and\nis still mostly true now, is still way faster than\ntraining with full precision.",
    "start": "4593990",
    "end": "4600350"
  },
  {
    "text": "And the reason for that is\nmost NVIDIA GPUs, starting",
    "start": "4600350",
    "end": "4605940"
  },
  {
    "text": "with the Volta cards, started\nshipping with these things called tensor cores.",
    "start": "4605940",
    "end": "4611520"
  },
  {
    "text": "Tensor cores are basically\nthe individual logical units on a GPU that are responsible\nfor matrix multiplies.",
    "start": "4611520",
    "end": "4618300"
  },
  {
    "text": "Your GPU is really good at\naccelerating neural network training because it's\nreally good at doing matrix multiplies.",
    "start": "4618300",
    "end": "4623670"
  },
  {
    "text": "These things are optimized for\n4 by 4 to 16 by 16 size charts.",
    "start": "4623670",
    "end": "4631260"
  },
  {
    "text": "If you're training\nin 16-bit precision you can actually end up\nusing way more tensor cores",
    "start": "4631260",
    "end": "4636330"
  },
  {
    "text": "than if you were using\n32-bit precision. And so you're able to\nget a ton of speedups",
    "start": "4636330",
    "end": "4641830"
  },
  {
    "text": "just because you're able to tap\ninto the underlying hardware of your system more frequently.",
    "start": "4641830",
    "end": "4647290"
  },
  {
    "text": "As of the Ampere style\ncards like the A100s or the more recent\n3090s 3090TIs, 4090s,",
    "start": "4647290",
    "end": "4655750"
  },
  {
    "text": "those start shipping\nwith these cores that are able to do float32 precision\nbut are still even faster",
    "start": "4655750",
    "end": "4662409"
  },
  {
    "text": "for 16-bit precision. So when you can, train\nwith 16-bit precision.",
    "start": "4662410",
    "end": "4668280"
  },
  {
    "text": "All right. This shaves a day off of\nour small-scale training.",
    "start": "4668280",
    "end": "4673890"
  },
  {
    "text": "This shaves off way more,\nespecially as you go bigger. And now the final bit is how do\nwe eliminate the redundancies,",
    "start": "4673890",
    "end": "4681675"
  },
  {
    "text": "right? So in standard\ndata parallelism-- Yeah. A question.",
    "start": "4681675",
    "end": "4686910"
  },
  {
    "text": "So why do you need the 32-bit\nprecision for the optimizer that you're using\nwith the model? So when you are estimating\nthe gradients precision",
    "start": "4686910",
    "end": "4695400"
  },
  {
    "text": "matters more. You want that full range. Specifically, you want those\n23 bits that kind of correspond",
    "start": "4695400",
    "end": "4704520"
  },
  {
    "text": "to everything that has\ndata to be meaningful, because while the full range of\nfloat32 is really, really big,",
    "start": "4704520",
    "end": "4712210"
  },
  {
    "text": "it actually can't be super\nprecise in the 01 range,",
    "start": "4712210",
    "end": "4718020"
  },
  {
    "text": "for example. So you want as much there as\npossible to ensure precision.",
    "start": "4718020",
    "end": "4725800"
  },
  {
    "text": "OK. So ZeRO redundancy,\nstandard data parallelism, you're basically storing\neverything on each GPU.",
    "start": "4725800",
    "end": "4731489"
  },
  {
    "text": "Key idea is I don't need to\nstore everything on every GPU, I just need to store\nsome things on every GPU.",
    "start": "4731490",
    "end": "4738690"
  },
  {
    "text": "So the model gets\nto say on each GPU because the model has\nto do all the work, but the gradients\nI can just split",
    "start": "4738690",
    "end": "4744420"
  },
  {
    "text": "across a number of\ndevices that I have. So half my gradients if I have\ntwo GPUs go on one device,",
    "start": "4744420",
    "end": "4749470"
  },
  {
    "text": "half of them go on\nthe other device. Same with the optimizer states. Half of them go one device, half\nof them go on the other device.",
    "start": "4749470",
    "end": "4755527"
  },
  {
    "text": "With this model of\nZeRO redundancy, you're actually not adding\nany extra communication cost, you just get free memory\nbecause you're just",
    "start": "4755527",
    "end": "4763120"
  },
  {
    "text": "intelligently partitioning\nthings across devices. Notice that these scales use\nless and less memory as you",
    "start": "4763120",
    "end": "4772690"
  },
  {
    "text": "add more and more machines. So this is the biggest trick to\nstart training 1 billion to 10",
    "start": "4772690",
    "end": "4778370"
  },
  {
    "text": "billion parameter plus models. And now when you add this,\nyou're at three days. ",
    "start": "4778370",
    "end": "4786330"
  },
  {
    "text": "So would you have\nto tell it what-- because this would require\nthings being slightly out",
    "start": "4786330",
    "end": "4791520"
  },
  {
    "text": "of sync to optimize. Do you have to-- [INTERPOSING VOICES] So this actually doesn't\nrequire anything out of sync because in a backwards pass\nwith distributed data-parallel,",
    "start": "4791520",
    "end": "4800661"
  },
  {
    "text": "the individual\nupdates already have to be synced across processes. If you take the\naverage across all",
    "start": "4800661",
    "end": "4808350"
  },
  {
    "text": "processes that means that\nthe gradients you apply have to be transferred as well.",
    "start": "4808350",
    "end": "4813820"
  },
  {
    "text": "So this just basically\ndoes that work for you.  We got to wrap up.",
    "start": "4813820",
    "end": "4819660"
  },
  {
    "text": "You hit a communication\nwall, matrix multiplies, stop fitting on a device,\nso you start charging them,",
    "start": "4819660",
    "end": "4824699"
  },
  {
    "text": "and then you have to start\nscheduling wisely, and yeah. Great. Fine-tuning inference,\nthere is a great library",
    "start": "4824700",
    "end": "4831239"
  },
  {
    "text": "that you should use called\nPEFT from Hugging Face. It's great. And that's it. ",
    "start": "4831240",
    "end": "4841000"
  }
]