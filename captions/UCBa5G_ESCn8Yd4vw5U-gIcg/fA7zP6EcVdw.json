[
  {
    "start": "0",
    "end": "53000"
  },
  {
    "start": "0",
    "end": "5390"
  },
  {
    "text": "Hi, in this module, I'm going\nto talk about Bayesian networks,",
    "start": "5390",
    "end": "8440"
  },
  {
    "text": "a new modeling paradigm.",
    "start": "8440",
    "end": "10670"
  },
  {
    "text": "So we have talked about two\ntypes of variable based models.",
    "start": "10670",
    "end": "13910"
  },
  {
    "text": "The first was constraint\nsatisfaction problems,",
    "start": "13910",
    "end": "16450"
  },
  {
    "text": "where the objective is to find\nthe maximum weight assignment,",
    "start": "16450",
    "end": "19300"
  },
  {
    "text": "given a factor graph.",
    "start": "19300",
    "end": "20980"
  },
  {
    "text": "Then we talked about\nMarkov networks,",
    "start": "20980",
    "end": "22539"
  },
  {
    "text": "where we used factor graphs\nto define a joint probability",
    "start": "22540",
    "end": "25660"
  },
  {
    "text": "distribution over assignments.",
    "start": "25660",
    "end": "27520"
  },
  {
    "text": "And we were computing\nmarginal probabilities.",
    "start": "27520",
    "end": "31180"
  },
  {
    "text": "Now I'm going to talk about\nBayesian networks, where",
    "start": "31180",
    "end": "33760"
  },
  {
    "text": "we still define a distribution\nover a set of random variables",
    "start": "33760",
    "end": "37809"
  },
  {
    "text": "using a factor graph.",
    "start": "37810",
    "end": "39130"
  },
  {
    "text": "But now the factors are going\nto have special meaning.",
    "start": "39130",
    "end": "42232"
  },
  {
    "text": "So Bayesian networks\nwere developed",
    "start": "42232",
    "end": "43690"
  },
  {
    "text": "by Judea Pearl in the mid\n1980s, and really have",
    "start": "43690",
    "end": "47469"
  },
  {
    "text": "evolved into the more general\nnotion of generative modeling",
    "start": "47470",
    "end": "50830"
  },
  {
    "text": "that we see today\nin machine learning.",
    "start": "50830",
    "end": "54150"
  },
  {
    "start": "53000",
    "end": "112000"
  },
  {
    "text": "So quickly, before diving\ninto Bayesian networks,",
    "start": "54150",
    "end": "57090"
  },
  {
    "text": "it's helpful to compare and\ncontrast with Markov networks.",
    "start": "57090",
    "end": "61230"
  },
  {
    "text": "So both are going to\ndefine a probability",
    "start": "61230",
    "end": "64050"
  },
  {
    "text": "distribution over assignments\nto a set of random variables.",
    "start": "64050",
    "end": "68370"
  },
  {
    "text": "But the way that each approaches\nthis is very different.",
    "start": "68370",
    "end": "71980"
  },
  {
    "text": "So if you're defining\na Markov network,",
    "start": "71980",
    "end": "74520"
  },
  {
    "text": "you tend to think in\nterms of specifying",
    "start": "74520",
    "end": "77250"
  },
  {
    "text": "a set of preferences.",
    "start": "77250",
    "end": "78750"
  },
  {
    "text": "And you throw these factors,\nencoding these preferences",
    "start": "78750",
    "end": "82080"
  },
  {
    "text": "into the Markov network.",
    "start": "82080",
    "end": "84420"
  },
  {
    "text": "So for example, the\nlast time we just threw",
    "start": "84420",
    "end": "86909"
  },
  {
    "text": "in the transition factor\nand observation factor",
    "start": "86910",
    "end": "89280"
  },
  {
    "text": "for the object tracking example.",
    "start": "89280",
    "end": "92240"
  },
  {
    "text": "So the Bayesian network\nis going to require",
    "start": "92240",
    "end": "94060"
  },
  {
    "text": "a more coordinated approach.",
    "start": "94060",
    "end": "96290"
  },
  {
    "text": "So in a Bayesian\nnetwork, the factors",
    "start": "96290",
    "end": "97840"
  },
  {
    "text": "are going to be local\nconditional distributions,",
    "start": "97840",
    "end": "100329"
  },
  {
    "text": "as we'll see later.",
    "start": "100330",
    "end": "101620"
  },
  {
    "text": "And we really think about\na generative process",
    "start": "101620",
    "end": "104980"
  },
  {
    "text": "by which each of these\nvariables is set, based",
    "start": "104980",
    "end": "108130"
  },
  {
    "text": "on other variables, in turn.",
    "start": "108130",
    "end": "109719"
  },
  {
    "start": "109720",
    "end": "112740"
  },
  {
    "start": "112000",
    "end": "348000"
  },
  {
    "text": "So there are many applications\nof Bayesian networks,",
    "start": "112740",
    "end": "116100"
  },
  {
    "text": "and more generally,\ngenerative models.",
    "start": "116100",
    "end": "118720"
  },
  {
    "text": "So I'll just go through\na couple of them here.",
    "start": "118720",
    "end": "120790"
  },
  {
    "text": "So the first one\nis topic modeling,",
    "start": "120790",
    "end": "123060"
  },
  {
    "text": "where the goal is to\ndiscover hidden structure",
    "start": "123060",
    "end": "125970"
  },
  {
    "text": "in a large collection\nof documents.",
    "start": "125970",
    "end": "128470"
  },
  {
    "text": "So an example of topic modeling\nis Latent Dirichlet Allocation",
    "start": "128470",
    "end": "131970"
  },
  {
    "text": "or LDA.",
    "start": "131970",
    "end": "133020"
  },
  {
    "text": "And the LDA posits\nthat each document",
    "start": "133020",
    "end": "136320"
  },
  {
    "text": "is generated by drawing\na mixture of topics",
    "start": "136320",
    "end": "139860"
  },
  {
    "text": "and then generating the\nwords, given those topics.",
    "start": "139860",
    "end": "144040"
  },
  {
    "text": "Another interesting\nexample is this idea",
    "start": "144040",
    "end": "147189"
  },
  {
    "text": "of vision as inverse graphics.",
    "start": "147190",
    "end": "149530"
  },
  {
    "text": "So, much of computer\nvision today",
    "start": "149530",
    "end": "151780"
  },
  {
    "text": "is taking images\nand processing them",
    "start": "151780",
    "end": "155740"
  },
  {
    "text": "in some way to generate semantic\ndescriptions, such as object",
    "start": "155740",
    "end": "159010"
  },
  {
    "text": "categories or\nscene descriptions.",
    "start": "159010",
    "end": "161799"
  },
  {
    "text": "So vision as inverse graphics\ntakes a very different",
    "start": "161800",
    "end": "165130"
  },
  {
    "text": "approach, where\nwe specify, using",
    "start": "165130",
    "end": "168520"
  },
  {
    "text": "laws of physics,\na graphics engine",
    "start": "168520",
    "end": "171160"
  },
  {
    "text": "that can generate\nan image, given",
    "start": "171160",
    "end": "173230"
  },
  {
    "text": "some semantic\ndescription-- for example,",
    "start": "173230",
    "end": "175459"
  },
  {
    "text": "a 3D model of an object.",
    "start": "175460",
    "end": "178060"
  },
  {
    "text": "And then, given this\nmodel, computer vision",
    "start": "178060",
    "end": "181120"
  },
  {
    "text": "is just inverse\ngraphics, where we're",
    "start": "181120",
    "end": "185230"
  },
  {
    "text": "trying to recover the\nsemantic description using",
    "start": "185230",
    "end": "189970"
  },
  {
    "text": "the image as input.",
    "start": "189970",
    "end": "191780"
  },
  {
    "text": "So this is an\nexample of inference",
    "start": "191780",
    "end": "194020"
  },
  {
    "text": "on this generative model.",
    "start": "194020",
    "end": "197070"
  },
  {
    "text": "So while this idea\nhasn't really been",
    "start": "197070",
    "end": "199610"
  },
  {
    "text": "able to be scaled past\nsome limited examples,",
    "start": "199610",
    "end": "203210"
  },
  {
    "text": "it's, I think, a very\ntantalizing idea, nonetheless.",
    "start": "203210",
    "end": "207070"
  },
  {
    "text": "So switching gears a\nlittle bit, let's talk",
    "start": "207070",
    "end": "209320"
  },
  {
    "text": "about communication networks.",
    "start": "209320",
    "end": "211640"
  },
  {
    "text": "So in the\ncommunication networks,",
    "start": "211640",
    "end": "214000"
  },
  {
    "text": "nodes must send messages, just a\nsequence of bits to each other.",
    "start": "214000",
    "end": "218630"
  },
  {
    "text": "But these bits can get corrupted\nalong the way, due to physics.",
    "start": "218630",
    "end": "223960"
  },
  {
    "text": "So the idea behind\nerror correcting codes--",
    "start": "223960",
    "end": "226990"
  },
  {
    "text": "in particular, these things\ncalled low density parity",
    "start": "226990",
    "end": "229360"
  },
  {
    "text": "codes--",
    "start": "229360",
    "end": "230050"
  },
  {
    "text": "is that the sender sends random\nparity checks on the data bits.",
    "start": "230050",
    "end": "235430"
  },
  {
    "text": "And then the receiver\nobtains a noisy version",
    "start": "235430",
    "end": "237609"
  },
  {
    "text": "of both the data\nand the parity bits.",
    "start": "237610",
    "end": "240550"
  },
  {
    "text": "The Bayesian network defines\nhow the original bits",
    "start": "240550",
    "end": "243850"
  },
  {
    "text": "are related to the noisy bits.",
    "start": "243850",
    "end": "246400"
  },
  {
    "text": "And then the receiver can use\nBayesian inference to compute",
    "start": "246400",
    "end": "250239"
  },
  {
    "text": "and recover the original bits.",
    "start": "250240",
    "end": "251810"
  },
  {
    "text": "So this is actually\na very effective idea",
    "start": "251810",
    "end": "253660"
  },
  {
    "text": "that's used in practice.",
    "start": "253660",
    "end": "256829"
  },
  {
    "text": "The final example is either\ncontroversial or a little bit",
    "start": "256829",
    "end": "261390"
  },
  {
    "text": "grim, which I'll explain later.",
    "start": "261390",
    "end": "263980"
  },
  {
    "text": "So this is the problem\nof DNA matching.",
    "start": "263980",
    "end": "267630"
  },
  {
    "text": "So there are two use cases\nof this, one is in forensics.",
    "start": "267630",
    "end": "272350"
  },
  {
    "text": "So, given the DNA\nfound at a crime site,",
    "start": "272350",
    "end": "275610"
  },
  {
    "text": "even if the suspect's DNA\nis not in the database,",
    "start": "275610",
    "end": "279300"
  },
  {
    "text": "one can still match this DNA\nagainst the family members",
    "start": "279300",
    "end": "283530"
  },
  {
    "text": "of a subject.",
    "start": "283530",
    "end": "284610"
  },
  {
    "text": "And here, the Bayesian network\nis structured along the family",
    "start": "284610",
    "end": "287729"
  },
  {
    "text": "tree and specifies\na relationship",
    "start": "287730",
    "end": "290880"
  },
  {
    "text": "between the family member's DNA,\nusing a Mendelian inheritance.",
    "start": "290880",
    "end": "297070"
  },
  {
    "text": "So now, while this\ntechnology has actually",
    "start": "297070",
    "end": "299070"
  },
  {
    "text": "been used to solve a\nnumber of crime cases,",
    "start": "299070",
    "end": "301590"
  },
  {
    "text": "there's definitely a lot\nof tricky ethical concerns",
    "start": "301590",
    "end": "304680"
  },
  {
    "text": "about this expanded DNA\nmatching, especially when",
    "start": "304680",
    "end": "308340"
  },
  {
    "text": "an individual's decision\nto release their own DNA",
    "start": "308340",
    "end": "311010"
  },
  {
    "text": "can impact the privacy\nof family members.",
    "start": "311010",
    "end": "315350"
  },
  {
    "text": "The second use case is in\ndisaster victim identification.",
    "start": "315350",
    "end": "319660"
  },
  {
    "text": "So after a big airplane\ncrash or some other disaster.",
    "start": "319660",
    "end": "323160"
  },
  {
    "text": "For example, Malaysia Airlines\ncrashed in Ukraine in 2014.",
    "start": "323160",
    "end": "327720"
  },
  {
    "text": "And victims' DNA is\nfound at the crash site",
    "start": "327720",
    "end": "330150"
  },
  {
    "text": "and is matched against\nthe family members,",
    "start": "330150",
    "end": "333330"
  },
  {
    "text": "using the same mechanism\nas I just described,",
    "start": "333330",
    "end": "335819"
  },
  {
    "text": "to help identify our victims.",
    "start": "335820",
    "end": "337770"
  },
  {
    "text": "And these methods\nare very scalable,",
    "start": "337770",
    "end": "340449"
  },
  {
    "text": "which allows them to deal with\nthese unfortunate large crash",
    "start": "340450",
    "end": "346320"
  },
  {
    "text": "sites.",
    "start": "346320",
    "end": "348860"
  },
  {
    "start": "348000",
    "end": "541000"
  },
  {
    "text": "So why Bayesian networks?",
    "start": "348860",
    "end": "351439"
  },
  {
    "text": "Well, these days,\nit's kind of hard",
    "start": "351440",
    "end": "353450"
  },
  {
    "text": "not to think about problems\nexclusively through the lens",
    "start": "353450",
    "end": "356990"
  },
  {
    "text": "of standard supervised\nlearning, such as, just",
    "start": "356990",
    "end": "359360"
  },
  {
    "text": "train a deep neural\nnetwork on a pile of data.",
    "start": "359360",
    "end": "362430"
  },
  {
    "text": "Bayesian networks really operate\nin a very different paradigm,",
    "start": "362430",
    "end": "365570"
  },
  {
    "text": "which offers several advantages\nthat I want to underscore here.",
    "start": "365570",
    "end": "370610"
  },
  {
    "text": "So the first is that it can\nhandle heterogeneously missing",
    "start": "370610",
    "end": "374979"
  },
  {
    "text": "information.",
    "start": "374980",
    "end": "376220"
  },
  {
    "text": "So normally, when you're doing\nstandard supervised learning,",
    "start": "376220",
    "end": "380860"
  },
  {
    "text": "your data is fairly homogeneous.",
    "start": "380860",
    "end": "383199"
  },
  {
    "text": "You have training\ninput and output pairs,",
    "start": "383200",
    "end": "385810"
  },
  {
    "text": "both at training and test time.",
    "start": "385810",
    "end": "388080"
  },
  {
    "text": "But in cases where you have\nmissing information, where",
    "start": "388080",
    "end": "391169"
  },
  {
    "text": "you have auxiliary\ninformation, Bayesian networks",
    "start": "391170",
    "end": "393540"
  },
  {
    "text": "can gracefully\nhandle this messiness",
    "start": "393540",
    "end": "396090"
  },
  {
    "text": "in a way that's\na little bit more",
    "start": "396090",
    "end": "398490"
  },
  {
    "text": "challenging for traditional\nsupervised methods.",
    "start": "398490",
    "end": "402520"
  },
  {
    "text": "The second is that\nBayesian networks",
    "start": "402520",
    "end": "404020"
  },
  {
    "text": "allow you to incorporate prior\nknowledge much more easily.",
    "start": "404020",
    "end": "407120"
  },
  {
    "text": "So when you have\nit, for example,",
    "start": "407120",
    "end": "408520"
  },
  {
    "text": "you understand how Mendelian\ninheritance works on DNA.",
    "start": "408520",
    "end": "412690"
  },
  {
    "text": "Or you understand\nthe laws of physics,",
    "start": "412690",
    "end": "414910"
  },
  {
    "text": "that Bayesian networks\nprovides a nice language",
    "start": "414910",
    "end": "417040"
  },
  {
    "text": "for incorporating this\ninformation into your model.",
    "start": "417040",
    "end": "420370"
  },
  {
    "text": "And now, using this\nmodel, you can actually",
    "start": "420370",
    "end": "422710"
  },
  {
    "text": "learn from very few samples and\nextrapolate beyond the training",
    "start": "422710",
    "end": "426250"
  },
  {
    "text": "distribution.",
    "start": "426250",
    "end": "427300"
  },
  {
    "text": "Whereas in contrast,\nmany kind of model,",
    "start": "427300",
    "end": "431020"
  },
  {
    "text": "agnostic, low\ninductive bias methods,",
    "start": "431020",
    "end": "434139"
  },
  {
    "text": "such as deep neural networks,\nrequire much more data",
    "start": "434140",
    "end": "436720"
  },
  {
    "text": "to be effective.",
    "start": "436720",
    "end": "439070"
  },
  {
    "text": "Because you're specifying\nprior knowledge,",
    "start": "439070",
    "end": "441390"
  },
  {
    "text": "you can also interpret the\nvariables inside the Bayesian",
    "start": "441390",
    "end": "445730"
  },
  {
    "text": "networks.",
    "start": "445730",
    "end": "446510"
  },
  {
    "text": "And this could be useful for\nunderstanding why a model is",
    "start": "446510",
    "end": "451220"
  },
  {
    "text": "making a certain decision.",
    "start": "451220",
    "end": "452450"
  },
  {
    "text": "And you can introspect\nand ask questions",
    "start": "452450",
    "end": "454520"
  },
  {
    "text": "about any of the\nintermediate variables.",
    "start": "454520",
    "end": "456620"
  },
  {
    "text": "And this just follows from\nthe laws of probability.",
    "start": "456620",
    "end": "460070"
  },
  {
    "text": "Finally, Bayesian networks\nare an important precursor",
    "start": "460070",
    "end": "463220"
  },
  {
    "text": "to causal models.",
    "start": "463220",
    "end": "464270"
  },
  {
    "text": "So these are beyond the\nscope of this course.",
    "start": "464270",
    "end": "466639"
  },
  {
    "text": "But they are extremely\nimportant, especially",
    "start": "466640",
    "end": "469670"
  },
  {
    "text": "these days.",
    "start": "469670",
    "end": "470480"
  },
  {
    "text": "They allow you to answer\nquestions about interventions.",
    "start": "470480",
    "end": "473870"
  },
  {
    "text": "For example, what\nwould happen if we",
    "start": "473870",
    "end": "476210"
  },
  {
    "text": "give this drug to this patient?",
    "start": "476210",
    "end": "478600"
  },
  {
    "text": "When the counterfactual is,\nwhat would have happened",
    "start": "478600",
    "end": "481280"
  },
  {
    "text": "if we have given this drug?",
    "start": "481280",
    "end": "483090"
  },
  {
    "text": "So these questions are\nextremely tricky and deep,",
    "start": "483090",
    "end": "487310"
  },
  {
    "text": "that standard machine\nlearning, or any methods that",
    "start": "487310",
    "end": "490880"
  },
  {
    "text": "view the world through just\nthe lens of predictions,",
    "start": "490880",
    "end": "493190"
  },
  {
    "text": "are really inadequate to answer.",
    "start": "493190",
    "end": "495580"
  },
  {
    "text": "So we're not going to\ntalk about in this course.",
    "start": "495580",
    "end": "497580"
  },
  {
    "text": "But I highly encourage you to\nexplore this topic on your own.",
    "start": "497580",
    "end": "501990"
  },
  {
    "text": "So finally, Bayesian\nnetworks, obviously,",
    "start": "501990",
    "end": "504169"
  },
  {
    "text": "aren't the panacea\nin many situations.",
    "start": "504170",
    "end": "507180"
  },
  {
    "text": "So often, in these\ncanonical AI applications,",
    "start": "507180",
    "end": "511550"
  },
  {
    "text": "such as vision,\nspeech, and language,",
    "start": "511550",
    "end": "514549"
  },
  {
    "text": "we actually have\nlarge data sets.",
    "start": "514549",
    "end": "516080"
  },
  {
    "text": "And we mostly care\nabout prediction.",
    "start": "516080",
    "end": "518090"
  },
  {
    "text": "And it's extremely hard to\nincorporate prior knowledge",
    "start": "518090",
    "end": "521570"
  },
  {
    "text": "into your models in these\nvery complex domains.",
    "start": "521570",
    "end": "524850"
  },
  {
    "text": "So in these cases,\nBayesian networks",
    "start": "524850",
    "end": "526730"
  },
  {
    "text": "haven't been as successful\nand have largely",
    "start": "526730",
    "end": "531139"
  },
  {
    "text": "been supplanted by deep\nlearning approaches.",
    "start": "531140",
    "end": "533690"
  },
  {
    "text": "But still, having Bayesian\nnetworks in your toolkit",
    "start": "533690",
    "end": "536540"
  },
  {
    "text": "will allow you to use\nit effectively when",
    "start": "536540",
    "end": "539089"
  },
  {
    "text": "you discover the right problem.",
    "start": "539090",
    "end": "542420"
  },
  {
    "text": "So in the remaining modules\non Bayesian networks,",
    "start": "542420",
    "end": "544600"
  },
  {
    "text": "I will first introduce Bayesian\nnetworks more formally.",
    "start": "544600",
    "end": "548860"
  },
  {
    "text": "And then I'll talk about\nprobabilistic programming,",
    "start": "548860",
    "end": "551480"
  },
  {
    "text": "which is a way to define\nBayesian networks using",
    "start": "551480",
    "end": "554680"
  },
  {
    "text": "probabilistic programs.",
    "start": "554680",
    "end": "556070"
  },
  {
    "text": "So this is a really cool\nway to think about modeling.",
    "start": "556070",
    "end": "561520"
  },
  {
    "text": "Then we'll turn to inference.",
    "start": "561520",
    "end": "563650"
  },
  {
    "text": "I'll talk about what\ninference means,",
    "start": "563650",
    "end": "566680"
  },
  {
    "text": "computing conditional and\nmarginal probabilities.",
    "start": "566680",
    "end": "569680"
  },
  {
    "text": "We're actually going\nto reduce the problem",
    "start": "569680",
    "end": "572110"
  },
  {
    "text": "in Bayesian networks\nto the same problem",
    "start": "572110",
    "end": "575440"
  },
  {
    "text": "of a bit probabilistic\ninference in Markov networks,",
    "start": "575440",
    "end": "578410"
  },
  {
    "text": "allowing to leverage the\nstuff that we talked about,",
    "start": "578410",
    "end": "581980"
  },
  {
    "text": "where we talked about\nMarkov networks.",
    "start": "581980",
    "end": "584769"
  },
  {
    "text": "Then we're going to specialize\nto hidden Markov models, HMMs,",
    "start": "584770",
    "end": "588010"
  },
  {
    "text": "an important special case\nof Bayesian networks.",
    "start": "588010",
    "end": "590560"
  },
  {
    "text": "We're going to show that the\nforward backward algorithm can",
    "start": "590560",
    "end": "593170"
  },
  {
    "text": "leverage the chain\nstructure of an HMM,",
    "start": "593170",
    "end": "595959"
  },
  {
    "text": "allowing you to do exact\nprobabilistic inference",
    "start": "595960",
    "end": "598360"
  },
  {
    "text": "efficiently.",
    "start": "598360",
    "end": "599747"
  },
  {
    "text": "Then we're going to talk about\nparticle filtering, which",
    "start": "599747",
    "end": "602080"
  },
  {
    "text": "allows you to do approximate\ninference and scale up",
    "start": "602080",
    "end": "605680"
  },
  {
    "text": "to HMMs, where variables\nhave larger domains.",
    "start": "605680",
    "end": "609850"
  },
  {
    "text": "Finally, we're going\nto talk about learning",
    "start": "609850",
    "end": "612009"
  },
  {
    "text": "in Bayesian networks.",
    "start": "612010",
    "end": "613030"
  },
  {
    "text": "We're just going to start with\nsupervised learning, where",
    "start": "613030",
    "end": "615820"
  },
  {
    "text": "all the variables are observed.",
    "start": "615820",
    "end": "617590"
  },
  {
    "text": "And this actually turns\nout to be quite easy.",
    "start": "617590",
    "end": "620920"
  },
  {
    "text": "You'll be pleasantly surprised.",
    "start": "620920",
    "end": "623016"
  },
  {
    "text": "Then we're going to show you how\nto guard against overfitting,",
    "start": "623017",
    "end": "625600"
  },
  {
    "text": "using a Laplace smoothing.",
    "start": "625600",
    "end": "627100"
  },
  {
    "text": "And finally, we're going to\nturn to cases where not all",
    "start": "627100",
    "end": "629769"
  },
  {
    "text": "the variables are observed.",
    "start": "629770",
    "end": "631240"
  },
  {
    "text": "And we introduce\nthe EM algorithm",
    "start": "631240",
    "end": "632950"
  },
  {
    "text": "that will help us learn\nin such Bayesian elements.",
    "start": "632950",
    "end": "635950"
  },
  {
    "text": "OK, so let's jump in.",
    "start": "635950",
    "end": "638370"
  },
  {
    "start": "638370",
    "end": "643000"
  }
]