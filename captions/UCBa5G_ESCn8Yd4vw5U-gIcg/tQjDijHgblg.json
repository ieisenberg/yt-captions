[
  {
    "start": "0",
    "end": "5022"
  },
  {
    "text": "So today, we're\ngoing to be talking about lifelong learning. And lifelong learning\nis an area that--",
    "start": "5022",
    "end": "11940"
  },
  {
    "text": "actually, I feel like the\nterm lifelong learning means a lot of different things. And so we're going\nto talk a little bit",
    "start": "11940",
    "end": "18120"
  },
  {
    "text": "about how the problem statement\nisn't super well defined in some ways. We'll also talk about\nsome basic approaches",
    "start": "18120",
    "end": "23789"
  },
  {
    "text": "to lifelong learning,\nwhether we can do better than those basic approaches, and\nrevisit the problem statement",
    "start": "23790",
    "end": "30168"
  },
  {
    "text": "from the perspective\nof meta learning and how we might leverage\nsome ideas for meta learning in the context\nof lifelong learning.",
    "start": "30168",
    "end": "38160"
  },
  {
    "text": "Cool. So in the course, we've\ntalked about a number of different problem statements.",
    "start": "38160",
    "end": "44130"
  },
  {
    "text": "But two that I'll focus on here\nis multitask learning and meta learning where in\nmultitask learning,",
    "start": "44130",
    "end": "49450"
  },
  {
    "text": "our goal is to solve\na set of tasks. And the train tasks\nand the test set tasks are the same set\nof tasks ultimately.",
    "start": "49450",
    "end": "58530"
  },
  {
    "text": "Whereas in meta\nlearning, our goal is to quickly learn a new\ntask after having experience",
    "start": "58530",
    "end": "64319"
  },
  {
    "text": "on a set of training tasks. And one thing that is maybe--",
    "start": "64319",
    "end": "73022"
  },
  {
    "text": "I think that we've seen like\nlots of examples in multitask learning and meta learning\nwhere a lot of scenarios",
    "start": "73023",
    "end": "78980"
  },
  {
    "text": "in the real world that are\ndefined by these problems statements. But there's also a number of\nscenarios in the real world",
    "start": "78980",
    "end": "86130"
  },
  {
    "text": "where the tasks aren't\ngiven in a batch. And instead, we see\nthe tasks one by one.",
    "start": "86130",
    "end": "93679"
  },
  {
    "text": "And in this case, it's a little\nbit different of a problem setting in the sense that\nyou aren't just given",
    "start": "93680",
    "end": "100250"
  },
  {
    "text": "a large batch of tasks\nor a large batch of data from different tasks\nall right off the bat.",
    "start": "100250",
    "end": "105770"
  },
  {
    "text": "And instead, you may want\nto be able to leverage previous experience in a\nmore sequential fashion.",
    "start": "105770",
    "end": "111965"
  },
  {
    "text": " So some examples\nof this, you might",
    "start": "111965",
    "end": "117290"
  },
  {
    "text": "have students that are\nlearning concepts in school. They'll learn those concepts\nsequentially over time",
    "start": "117290",
    "end": "122840"
  },
  {
    "text": "rather than being given\nall of them from a batch. Also, in that sort\nof scenario, there might be a form of a curricula.",
    "start": "122840",
    "end": "130347"
  },
  {
    "text": "And some of the tasks might\nget harder and harder. Or they might build upon\nsome of the previous tasks.",
    "start": "130347",
    "end": "136178"
  },
  {
    "text": "An image classification\nsystem that's learning from a stream of\nimages from different users but could also fall\ninto the setting",
    "start": "136178",
    "end": "142819"
  },
  {
    "text": "where different users\nwill join the platform. And we'll start uploading\nimages at different times.",
    "start": "142820",
    "end": "151100"
  },
  {
    "text": "You could possibly\nthink of receiving data from a sequence of users\nrather than receiving data",
    "start": "151100",
    "end": "156860"
  },
  {
    "text": "from a set of\nusers all up front. Another example\nis maybe you have",
    "start": "156860",
    "end": "162439"
  },
  {
    "text": "a robot that's trying\nto learn an increasingly large set of skills in\ndifferent environments.",
    "start": "162440",
    "end": "168290"
  },
  {
    "text": "It might continuously run\ninto new things that it needs to learn or new\nenvironments that it",
    "start": "168290",
    "end": "174019"
  },
  {
    "text": "needs to be able to learn in. And we don't want to be\nrestricted to the setting",
    "start": "174020",
    "end": "179510"
  },
  {
    "text": "where we only have\na batch of settings. You can also have\na virtual assistant that's trying to\nlearn how to help",
    "start": "179510",
    "end": "184610"
  },
  {
    "text": "different users\nwith different tasks at different point in time. And so at each point in\ntime, you're going to be--",
    "start": "184610",
    "end": "189770"
  },
  {
    "text": "a new user will arrive. They'll have a new task that\nyou might need to perform.",
    "start": "189770",
    "end": "194905"
  },
  {
    "text": "And you need to\nbe able to handle that regardless\nof the experience that you've had before.",
    "start": "194905",
    "end": "200310"
  },
  {
    "text": "And then lastly, you could\nhave maybe a doctor's assistant that's aiding in\nmedical decision-making.",
    "start": "200310",
    "end": "206000"
  },
  {
    "text": "And, again, you kind of see\ncases in a sequential manner rather than necessarily\nall up front.",
    "start": "206000",
    "end": "211069"
  },
  {
    "text": " Now, to talk a little bit\nabout some terminology,",
    "start": "211070",
    "end": "220050"
  },
  {
    "text": "I'll refer to what\nwe just discussed as a form of\nsequential learning.",
    "start": "220050",
    "end": "226050"
  },
  {
    "text": "And there's really a lot\nof terms in the literature to describe different forms\nof this kind of problem,",
    "start": "226050",
    "end": "235599"
  },
  {
    "text": "including online learning,\nlifelong learning, continual learning, incremental\nlearning, and this streaming",
    "start": "235600",
    "end": "241860"
  },
  {
    "text": "data setting. And unfortunately, there\nisn't really crisp definitions",
    "start": "241860",
    "end": "247830"
  },
  {
    "text": "for each one of these. And oftentimes,\nmany of these terms are used somewhat\ninterchangeably as well.",
    "start": "247830",
    "end": "253959"
  },
  {
    "text": "And so it means that basically,\nthe waters are a little bit",
    "start": "253960",
    "end": "261670"
  },
  {
    "text": "murky, I think, in terms\nof-- there isn't really a lot of consensus behind what\nnecessarily each of these terms",
    "start": "261670",
    "end": "266910"
  },
  {
    "text": "mean. The other thing\nthat I'll mention",
    "start": "266910",
    "end": "272130"
  },
  {
    "text": "is that the setting is distinct\nfrom having sequential data or sequential decision-making.",
    "start": "272130",
    "end": "279090"
  },
  {
    "text": "In particular, this\nmeans-- sequential learning means that we're going to be\nlearning as the data comes in.",
    "start": "279090",
    "end": "284910"
  },
  {
    "text": "The data will come in sequence. And we'll be learning\nfrom that data in sequence, whereas sequential\ndata is scenarios where",
    "start": "284910",
    "end": "292140"
  },
  {
    "text": "your data points has\na sequential structure rather than it's\ncoming in over time. And sequential\ndecision-making means",
    "start": "292140",
    "end": "297810"
  },
  {
    "text": "that your system is making\ndecisions, multiple decisions in sequence, whereas\nsequential learning, again,",
    "start": "297810",
    "end": "304710"
  },
  {
    "text": "is one where you're learning\nas data comes in over time. ",
    "start": "304710",
    "end": "311930"
  },
  {
    "text": "Cool. So because the problem\nstatement of lifelong learning",
    "start": "311930",
    "end": "317020"
  },
  {
    "text": "and the definition\nof lifelong learning isn't somewhat agreed\nupon, I'd actually",
    "start": "317020",
    "end": "323350"
  },
  {
    "text": "like to do something a little\nbit different in this lecture and give you a little\nbit of an exercise.",
    "start": "323350",
    "end": "329680"
  },
  {
    "text": "And in particular, I want you\nto pick an example setting. It could be a setting\nthat you came up with.",
    "start": "329680",
    "end": "335350"
  },
  {
    "text": "Or it could be one\nof the examples that we saw in\nthe previous slide and discuss the\nproblem statement",
    "start": "335350",
    "end": "341860"
  },
  {
    "text": "in a small group for\nthat example setting. And in particular,\nthere's three things",
    "start": "341860",
    "end": "347590"
  },
  {
    "text": "that I'd like you to discuss. The first is, how\nwould you set up an experiment to\ndevelop and test",
    "start": "347590",
    "end": "353680"
  },
  {
    "text": "an algorithm for that setting? The second is, what are the\ndesired or required properties",
    "start": "353680",
    "end": "360850"
  },
  {
    "text": "of an algorithm for\nthat particular problem?",
    "start": "360850",
    "end": "365910"
  },
  {
    "text": "And the third is, how\nwould you actually go about evaluating\nsuch a system?",
    "start": "365910",
    "end": "371370"
  },
  {
    "text": "So if it's helpful, I'll\ngive you the example settings for reference here. And then I'd encourage\nyou to split up",
    "start": "371370",
    "end": "377570"
  },
  {
    "text": "into small groups, maybe\nsmall groups of no more than five people.",
    "start": "377570",
    "end": "382669"
  },
  {
    "text": "And then I'll give you\nmaybe about five minutes to discuss this in a group. And then afterwards, we will\nhave one person from each group",
    "start": "382670",
    "end": "393800"
  },
  {
    "text": "describe what they came up\nwith in terms of A, B, and C, and the problem setting\nthat they were discussing.",
    "start": "393800",
    "end": "400100"
  },
  {
    "text": "And yeah, we'll start to\ndiscuss the problem statement. Does that sound good? Any questions?",
    "start": "400100",
    "end": "407020"
  },
  {
    "text": "Cool. I can also help you split up\ninto groups if that's helpful. ",
    "start": "407020",
    "end": "413010"
  },
  {
    "text": "Cool, OK. Go ahead. And it's going to be one\nproblem setting per group.",
    "start": "413010",
    "end": "418662"
  },
  {
    "text": "[CHATTER] ",
    "start": "418662",
    "end": "470450"
  },
  {
    "text": "Are folks good? Or do you need more time? Raise your hand if\nyou need more time. ",
    "start": "470450",
    "end": "477940"
  },
  {
    "text": "OK, I think you've\nbeen outvoted.  Cool. OK, let's come back.",
    "start": "477940",
    "end": "483890"
  },
  {
    "text": " Yeah, for each person--\nfor each group-- I think we had about\nseven different groups.",
    "start": "483890",
    "end": "490460"
  },
  {
    "text": "If one person could\nkind of briefly run through what\nproblem setting you were discussing and the\ndesirable properties",
    "start": "490460",
    "end": "497389"
  },
  {
    "text": "in the evaluation. So do you guys want to start? Yeah, go ahead. ",
    "start": "497390",
    "end": "502670"
  },
  {
    "text": "So are [INAUDIBLE]\nlimitations [INAUDIBLE]",
    "start": "502670",
    "end": "512789"
  },
  {
    "text": "So is that [INAUDIBLE]\nhave a distribution of user",
    "start": "512789",
    "end": "520849"
  },
  {
    "text": "[INAUDIBLE] or however\ndistribution [INAUDIBLE] are introduced.",
    "start": "520850",
    "end": "526269"
  },
  {
    "text": "And the desirable properties\nof the [INAUDIBLE] properties.",
    "start": "526270",
    "end": "534365"
  },
  {
    "text": "Firstly, the model\nshould achieve good performance or\ninfinitely good performance",
    "start": "534365",
    "end": "541730"
  },
  {
    "text": "as the number of images\n[INAUDIBLE] to infinity. And secondly, the\nloss of the model",
    "start": "541730",
    "end": "552154"
  },
  {
    "text": "should be, on average,\napproximately monotonically",
    "start": "552155",
    "end": "557270"
  },
  {
    "text": "decreasing as you put in\nmore images to the model.",
    "start": "557270",
    "end": "563750"
  },
  {
    "text": "And to evaluate such\na system, we just-- Sorry. To clarify, so you're\nsaying good performance",
    "start": "563750",
    "end": "569645"
  },
  {
    "text": "as the number of images grows. Is this as the number\nof users grows or as the number of images grows?",
    "start": "569645",
    "end": "575120"
  },
  {
    "text": "[INAUDIBLE] That's the\nnumber of introduced rules. If it is for that\nuser, it grows. Got it.",
    "start": "575120",
    "end": "580575"
  },
  {
    "text": "And then what was\nthe second thing? You said the loss. The second thing is our average,\nbecause we're [INAUDIBLE]",
    "start": "580575",
    "end": "585875"
  },
  {
    "text": "expectation over different\nusers, our average, the loss should be\napproximately monotonically",
    "start": "585875",
    "end": "594560"
  },
  {
    "text": "decreasing as you increase\nthe number of images. All right. Is that also per--",
    "start": "594560",
    "end": "600830"
  },
  {
    "text": "as you see more and more users? As you see more\nimages for each user. OK.",
    "start": "600830",
    "end": "607510"
  },
  {
    "text": "So is that very similar\nto good performance? I guess losses\ndown, is lower than,",
    "start": "607510",
    "end": "612820"
  },
  {
    "text": "doesn't that mean you\nhave good performance? The first point is actually\nthat the performance can be infinitely grouped\nas you increase the number",
    "start": "612820",
    "end": "619500"
  },
  {
    "text": "of [INAUDIBLE] to infinity. I see. OK. So you'd want to be\nmonotonically increasing in an asymptote to\nbe kind of perfect.",
    "start": "619500",
    "end": "626440"
  },
  {
    "text": "Got it.  Cool. And then evaluation?",
    "start": "626440",
    "end": "632690"
  },
  {
    "text": "[INAUDIBLE] sources\nfrom-- you could do [INAUDIBLE] you calculate the\nexpectation, our expectation,",
    "start": "632690",
    "end": "644195"
  },
  {
    "text": "of how many images will it take\nfor performance to [INAUDIBLE]",
    "start": "644195",
    "end": "652301"
  },
  {
    "text": "a specific threshold,\nlike [INAUDIBLE]..",
    "start": "652301",
    "end": "657540"
  },
  {
    "text": "So this is basically like\nlearning efficiency, basically like how many images\ndo you need for a user in order to get a good\naccuracy for that user?",
    "start": "657540",
    "end": "664430"
  },
  {
    "text": "In order to get a\nspecific performance. And the lower [INAUDIBLE],,\nthe higher the performance.",
    "start": "664430",
    "end": "671210"
  },
  {
    "text": "Cool.  Maybe we can move on\nto the next group.",
    "start": "671210",
    "end": "678450"
  },
  {
    "text": "Yeah, go ahead. OK. So we were doing, E, the\ndoctor's assistant aiding",
    "start": "678450",
    "end": "685410"
  },
  {
    "text": "a medical decision-making. Should we-- do we go through\nall of them, like A, B, and C?",
    "start": "685410",
    "end": "692453"
  },
  {
    "text": "You can do only B\nand C, or if you want to kind of briefly\nmention A, you could do that. Briefly mention\nA. We would just--",
    "start": "692453",
    "end": "698856"
  },
  {
    "text": "we would have patient\ntasks come in, like various patient tasks,\nlike recording breath",
    "start": "698856",
    "end": "704820"
  },
  {
    "text": "and how fast you're\nbreathing or whatever. And then that would\nbe the experiment. As more patients\ncome in, we would",
    "start": "704820",
    "end": "711090"
  },
  {
    "text": "have more tasks to evaluate. The desirable report\nproperties, we",
    "start": "711090",
    "end": "717329"
  },
  {
    "text": "would be increasing and\nimproving the, I would say, similar to improving\nthe performance,",
    "start": "717330",
    "end": "724139"
  },
  {
    "text": "as the number of\nunique task grows and the number of examples\nfor each task also increases.",
    "start": "724140",
    "end": "732165"
  },
  {
    "text": " So that would be a desired,\nor probably required,",
    "start": "732165",
    "end": "739095"
  },
  {
    "text": "property of the algorithm. As for evaluating the system,\nwe would evaluate it--",
    "start": "739095",
    "end": "745379"
  },
  {
    "text": "we would have the same\nrecurring, a test that we use at multiple time points.",
    "start": "745380",
    "end": "750582"
  },
  {
    "text": "At each time point,\nthe system will have encountered more examples\nfrom more patients it has seen.",
    "start": "750582",
    "end": "756029"
  },
  {
    "text": "And then, hopefully,\nthe performance of those tasks they've seen\nbefore and new tasks increase.",
    "start": "756030",
    "end": "764670"
  },
  {
    "text": "Because the task set will have\nthose examples it hasn't seen",
    "start": "764670",
    "end": "770579"
  },
  {
    "text": "and tasks it has seen. Cool. And so then, I guess, one thing\nthat's a little bit different",
    "start": "770580",
    "end": "777545"
  },
  {
    "text": "here from the\nother evaluation is you also want to be able to do\nwell on past tasks, as well. Yeah, you want to\nsee if it does well",
    "start": "777545",
    "end": "783600"
  },
  {
    "text": "on new examples of past tasks. ",
    "start": "783600",
    "end": "790480"
  },
  {
    "text": "Cool. Anything else? No, I think we're\nprobably [INAUDIBLE].. All right. Next group. OK.",
    "start": "790480",
    "end": "795640"
  },
  {
    "text": "So we chose the example\nsetting C, the robot in different environments.",
    "start": "795640",
    "end": "801610"
  },
  {
    "text": "So one of the things\nwe talked about was, actually, the problem\nof forgetting stuff. So if you constantly transfer\nthe robot into new environments",
    "start": "801610",
    "end": "808630"
  },
  {
    "text": "and learn new tasks,\nit could actually forget what it's\nlearned in the past.",
    "start": "808630",
    "end": "814120"
  },
  {
    "text": "So that's one of\nthe very desirable and required properties\nwe talked about.",
    "start": "814120",
    "end": "819730"
  },
  {
    "text": "And also, we talked\na bit about if there is some kind of expressiveness\nin the model that might limit",
    "start": "819730",
    "end": "826390"
  },
  {
    "text": "what it can learn, like the\noverall number of tasks, in general. So if it learns\nmore and more tasks, it could get worse\nin the performance",
    "start": "826390",
    "end": "834040"
  },
  {
    "text": "for a single task over time,\neven if it doesn't yet. Also, it's like B and\nfor C, we would kind of",
    "start": "834040",
    "end": "840130"
  },
  {
    "text": "keep a database of the\ntasks we had in the past and basically see\nthe more tasks we",
    "start": "840130",
    "end": "847330"
  },
  {
    "text": "learned that we haven't\nforgotten any old stuff that we learned. Cool.",
    "start": "847330",
    "end": "852400"
  },
  {
    "text": "So yeah, this is\npretty well covered. The back left group. ",
    "start": "852400",
    "end": "859167"
  },
  {
    "text": "Sure. So we did the [INAUDIBLE]\nmost individual [INAUDIBLE].. But also, I guess\nthe [INAUDIBLE],,",
    "start": "859167",
    "end": "866600"
  },
  {
    "text": "it would be like we can display\nthe different [INAUDIBLE] that we're trying which could\npossibly affect our algorithm.",
    "start": "866600",
    "end": "871790"
  },
  {
    "text": "Because I can represent\nthe control to see what's performing best. And then the desired properties\nthat we want from this task",
    "start": "871790",
    "end": "878500"
  },
  {
    "text": "is basically [INAUDIBLE]\nlike it's increasingly, progressively getting a\ndifficult level of [INAUDIBLE]..",
    "start": "878500",
    "end": "886340"
  },
  {
    "text": "So those things that we\nwant the student to focus on would be increasing\ntheir knowledge, breadth, and depth so they can more\n[INAUDIBLE],, like the depth",
    "start": "886340",
    "end": "894260"
  },
  {
    "text": "of their knowledge increasing\nand the breadth of their [INAUDIBLE] more difficult,\nlike something that [INAUDIBLE]",
    "start": "894260",
    "end": "901180"
  },
  {
    "text": "ends up [INAUDIBLE]. To progressively more difficult.",
    "start": "901180",
    "end": "906535"
  },
  {
    "text": "And I guess the way\nto evaluate would be how the algorithm\nor the method",
    "start": "906535",
    "end": "912470"
  },
  {
    "text": "is performing on tangentially\nrelated but kind of derived",
    "start": "912470",
    "end": "917472"
  },
  {
    "text": "from the system,\nso how evaluation works in exams, for example. Like, testing the\nconcept, that's",
    "start": "917473",
    "end": "922640"
  },
  {
    "text": "kind of covered in the\ncourse, but the agent needs to develop\nto those patterns, like how is this organically\nlearning and developing",
    "start": "922640",
    "end": "930170"
  },
  {
    "text": "concepts to improve the breadth\nand depth of the subject? Got it. And so your evaluation would\nbe through tests or through--",
    "start": "930170",
    "end": "937550"
  },
  {
    "text": "Yeah, kind of like a continuous\nevaluation, which is testing. So not all sort of dissimilar\nto what we're taught.",
    "start": "937550",
    "end": "944297"
  },
  {
    "text": "But it's basically just\nthe generalizability of the content.",
    "start": "944297",
    "end": "949730"
  },
  {
    "text": "Yeah, that makes sense. One thing that's, I\nthink, interesting to note is that when you do go towards\nmore and more complex tasks,",
    "start": "949730",
    "end": "959900"
  },
  {
    "text": "it may be that as you\nhave more and more tasks, your performance may not\nactually continue to go up.",
    "start": "959900",
    "end": "966920"
  },
  {
    "text": "It may actually stay\nconstant or even go down because the tasks\nare also just getting harder and harder over time.",
    "start": "966920",
    "end": "972237"
  },
  {
    "text": "So that, actually,\nmay be a little bit different in\ncomparison to if you have a sequence of\ntasks that are more",
    "start": "972237",
    "end": "977540"
  },
  {
    "text": "IID, or similar in difficulty. Cool.",
    "start": "977540",
    "end": "982830"
  },
  {
    "text": "The group in the front. We didn't cover a lot of\nthings that we talked about.",
    "start": "982830",
    "end": "989555"
  },
  {
    "text": "Our smarter, or our\ndesirable properties, included good\nperformance over time,",
    "start": "989555",
    "end": "997490"
  },
  {
    "text": "not forgetting and not\nremaking similar mistakes. ",
    "start": "997490",
    "end": "1005009"
  },
  {
    "text": "As to the evaluation,\nI think we have something slightly different. When we were\ncomparing evaluation",
    "start": "1005009",
    "end": "1011029"
  },
  {
    "text": "in terms of the time state,\nso is it better than it was?",
    "start": "1011030",
    "end": "1018425"
  },
  {
    "text": " OK.",
    "start": "1018425",
    "end": "1023462"
  },
  {
    "text": "Nice work, [INAUDIBLE]. But the [INAUDIBLE]\nand performance.",
    "start": "1023462",
    "end": "1029040"
  },
  {
    "text": "So is it getting-- do you care\nmore about is it getting better versus just the\nabsolute performance?",
    "start": "1029040",
    "end": "1034343"
  },
  {
    "text": " Cool. OK. The group there.",
    "start": "1034343",
    "end": "1040970"
  },
  {
    "text": "We talked mainly about the\nvirtual assistant task. And I think what's\ninteresting about",
    "start": "1040970",
    "end": "1046199"
  },
  {
    "text": "that one is different\nusers might have different ideas of\nwhat constitutes",
    "start": "1046200",
    "end": "1052380"
  },
  {
    "text": "successful help on a\nparticular query prompt. So the system should be\nable to learn what that is.",
    "start": "1052380",
    "end": "1061049"
  },
  {
    "text": "It should be able to adapt\nitself to the different users. And maybe it has to--",
    "start": "1061050",
    "end": "1067409"
  },
  {
    "text": "that's not so obvious, always-- but we talked about how you\nmight generate data for that",
    "start": "1067410",
    "end": "1072760"
  },
  {
    "text": "and then evaluating it by\nlooking at how successfully",
    "start": "1072760",
    "end": "1077940"
  },
  {
    "text": "it's able to help many different\nusers other than just a few.",
    "start": "1077940",
    "end": "1084210"
  },
  {
    "text": "Kind of like performance on,\nbasically, all of the tasks come together, yeah? For all of the people.",
    "start": "1084210",
    "end": "1089899"
  },
  {
    "text": "Yeah. Cool. And then I think we have\none more group, which",
    "start": "1089900",
    "end": "1095700"
  },
  {
    "text": "is in the back right. So we [INAUDIBLE] did the robot\nrecording on a [INAUDIBLE] And",
    "start": "1095700",
    "end": "1104190"
  },
  {
    "text": "the setting was that-- so the final goal that\nwe want is for that we wanted to cook a meal.",
    "start": "1104190",
    "end": "1110040"
  },
  {
    "text": "And we're training\nit by giving it some simple tasks, such as\nmoving things, and making",
    "start": "1110040",
    "end": "1116940"
  },
  {
    "text": "it more and more complex. So a desirable\nproperty is that it should be able to use the\nprevious tasks in learning",
    "start": "1116940",
    "end": "1125480"
  },
  {
    "text": "the new task. And so if we give it a\nmore number of simple tasks before we give it\nthe final task,",
    "start": "1125480",
    "end": "1133211"
  },
  {
    "text": "then it should be able to\nlearn the final task faster and better. So the value [INAUDIBLE]\nwe will start,",
    "start": "1133212",
    "end": "1138660"
  },
  {
    "text": "we will compare against\njust giving one simple task or giving a few\ndifferent simple tasks",
    "start": "1138660",
    "end": "1144420"
  },
  {
    "text": "and how well it is able to\nlearn with those k tasks, how well it is able to\ndo in the final task with an increasing\nnumber of simpler tasks.",
    "start": "1144420",
    "end": "1151980"
  },
  {
    "text": "And the other thing was\nthe same that many people have covered, that it\nshould not forget the past. ",
    "start": "1151980",
    "end": "1158519"
  },
  {
    "text": "Cool. That makes sense. So that was actually\na little bit different than the other ones\nin that you maybe only care about the\nperformance on the last task.",
    "start": "1158520",
    "end": "1166610"
  },
  {
    "text": "Perhaps the curriculum is only\nkind of a means to an end. It's not the-- you don't\ncare about the performance",
    "start": "1166610",
    "end": "1172160"
  },
  {
    "text": "on the intermediate\ntasks itself.  I think that the only thing\nthat is not mentioned over there",
    "start": "1172160",
    "end": "1183110"
  },
  {
    "text": "that we thought of was\nhow fast the agent, assuming that he solved\nour task before that,",
    "start": "1183110",
    "end": "1191070"
  },
  {
    "text": "he can learn a new task\nfaster, so the rate now for learning a new task.",
    "start": "1191070",
    "end": "1197040"
  },
  {
    "text": "Yeah, so ability to\nlearn a new task quickly. ",
    "start": "1197040",
    "end": "1203100"
  },
  {
    "text": "Interesting.  Cool. So in general, actually,\nthere's more consensus,",
    "start": "1203100",
    "end": "1211320"
  },
  {
    "text": "I think, across the\ndifferent examples than I've seen in the past-- generally, looking\nfor good performance",
    "start": "1211320",
    "end": "1216930"
  },
  {
    "text": "on past tasks and new tasks,\nalthough in some cases, we didn't really care about\nthe performance on past tasks. We only cared about\nthe performance",
    "start": "1216930",
    "end": "1223260"
  },
  {
    "text": "on future tasks and later tasks. In some cases, you might be\nlearning a single model that",
    "start": "1223260",
    "end": "1228600"
  },
  {
    "text": "can kind of do everything,\nwhereas in other cases, you might care more\nabout adaptability.",
    "start": "1228600",
    "end": "1234810"
  },
  {
    "text": "For example, different people-- the virtual assistant\nexample came up where maybe different people\ndo have different objectives.",
    "start": "1234810",
    "end": "1241890"
  },
  {
    "text": "Someone wants a virtual\nassistant that has a certain-- I don't know, is\nvery efficient, where",
    "start": "1241890",
    "end": "1246899"
  },
  {
    "text": "someone else cares a lot more\nabout being very thorough, for example. And so adaptability\nmight actually",
    "start": "1246900",
    "end": "1252540"
  },
  {
    "text": "be pretty different\nthan if you were to just try to learn\na single model that can do all of the tasks that\nit receives in sequence.",
    "start": "1252540",
    "end": "1259470"
  },
  {
    "text": "There's a couple of\nother things that I thought of that I think\nactually can also sometimes vary",
    "start": "1259470",
    "end": "1266130"
  },
  {
    "text": "between problem statements. I guess one thing\nthat we did see a little bit is the tasks might\nbe IID versus something that's",
    "start": "1266130",
    "end": "1274350"
  },
  {
    "text": "kind of predictable over time\nversus a curriculum of tasks. You could also-- we didn't see\nthis in any of the examples,",
    "start": "1274350",
    "end": "1280890"
  },
  {
    "text": "but you could also have an\nadversarial sequence of tasks where, for example, you're\ntrying to detect spam",
    "start": "1280890",
    "end": "1287190"
  },
  {
    "text": "over time, spammers\nmay try to create things that will get\nthrough the filter",
    "start": "1287190",
    "end": "1293790"
  },
  {
    "text": "that you're trying to create. And then, in that case, the\ntask that you see over time,",
    "start": "1293790",
    "end": "1299850"
  },
  {
    "text": "or the data points\nthat you see over time, may be more\nadversarial in nature.",
    "start": "1299850",
    "end": "1304875"
  },
  {
    "text": " Another variation\nthat can come up",
    "start": "1304875",
    "end": "1310020"
  },
  {
    "text": "is having discrete\ntask boundaries where you clearly have\na new user versus more",
    "start": "1310020",
    "end": "1315929"
  },
  {
    "text": "continuous shifts over time. I don't think we really\nsaw too many examples",
    "start": "1315930",
    "end": "1322170"
  },
  {
    "text": "of continuous shifts over time. But there's lots of\nexamples in practice where, for example, people's\nsentiment on certain topics",
    "start": "1322170",
    "end": "1329340"
  },
  {
    "text": "might change over time or\nthe seasons might change over time and so forth. ",
    "start": "1329340",
    "end": "1336539"
  },
  {
    "text": "There also may be\nscenarios where there are very clear\ndelineations between tasks and shifts versus\nnot knowing exactly",
    "start": "1336540",
    "end": "1343740"
  },
  {
    "text": "what those delineations are. And that can affect\ndifferent algorithms. And then there's also some\ndifferent considerations.",
    "start": "1343740",
    "end": "1350470"
  },
  {
    "text": "So everyone talked\nabout model performance. We may also care a lot\nabout data efficiency,",
    "start": "1350470",
    "end": "1357029"
  },
  {
    "text": "about the ability to learn tasks\nmore efficiently over time. ",
    "start": "1357030",
    "end": "1363940"
  },
  {
    "text": "Another thing that actually\ndidn't come up at all is computational resources. You might want to\nbe able to learn",
    "start": "1363940",
    "end": "1371760"
  },
  {
    "text": "a task with fewer computational\nresources than before.",
    "start": "1371760",
    "end": "1377040"
  },
  {
    "text": "It may also be that as you see\ndata over time, at some point,",
    "start": "1377040",
    "end": "1384307"
  },
  {
    "text": "you may not be able to\nstore all of the data that you've seen over the\ncourse of your lifetime. And you need to think\nabout the resources",
    "start": "1384307",
    "end": "1390820"
  },
  {
    "text": "that you have in terms\nof memory, as well. And in those instances, you\ndon't want to necessarily",
    "start": "1390820",
    "end": "1397900"
  },
  {
    "text": "have algorithms that\nwill require memory that is linear in\nthe number of tasks",
    "start": "1397900",
    "end": "1403420"
  },
  {
    "text": "that you've seen over time. Because that will\ngrow unbounded. And so oftentimes,\nanother evaluation metric",
    "start": "1403420",
    "end": "1409059"
  },
  {
    "text": "might be the number of\ncomputational resources that it's using or\nthe amount of memory that it's using, as well. ",
    "start": "1409060",
    "end": "1417630"
  },
  {
    "text": "I guess another\npoint about memory, as well, is in the\npatient example, which I think someone did,\nyou may also not",
    "start": "1417630",
    "end": "1424799"
  },
  {
    "text": "be allowed to store\ndata over the course. User data can be sensitive. And for ethical reasons\nor legal reasons,",
    "start": "1424800",
    "end": "1433080"
  },
  {
    "text": "you may not be\nable to store data for more than some\nperiod of time. And that can also\naffect these algorithms",
    "start": "1433080",
    "end": "1439710"
  },
  {
    "text": "pretty dramatically, as well. And this relates to trying\nnot to be able to forget",
    "start": "1439710",
    "end": "1445710"
  },
  {
    "text": "past tasks or past users. And so that relates to privacy. There may also be\nother metrics that you",
    "start": "1445710",
    "end": "1452580"
  },
  {
    "text": "care about in terms of\ninterpretability, fairness and test time\ncomputation and so forth.",
    "start": "1452580",
    "end": "1458140"
  },
  {
    "text": "So I guess part of the point of\nthis exercise was to, I think,",
    "start": "1458140",
    "end": "1464107"
  },
  {
    "text": "shed light on the\nfact that, depending on the problem setting\nthat you're in, you may have different\nconsiderations for what",
    "start": "1464107",
    "end": "1470520"
  },
  {
    "text": "is most important for\nthat problem setting. In some cases, you\nmight care a lot about the performance of\ncertain performance metrics",
    "start": "1470520",
    "end": "1477960"
  },
  {
    "text": "much more than others. And you may also have\ndifferent constraints or different resources in\nterms of these algorithms.",
    "start": "1477960",
    "end": "1485340"
  },
  {
    "text": "And as a result,\nbecause there is so much variation in the\ndifferent problems people",
    "start": "1485340",
    "end": "1491040"
  },
  {
    "text": "might care about, it\nmeans that some algorithms may be much better\nsuited for some problem",
    "start": "1491040",
    "end": "1497490"
  },
  {
    "text": "settings than others.  Cool.",
    "start": "1497490",
    "end": "1502590"
  },
  {
    "text": "Any questions up\nuntil this point? OK.",
    "start": "1502590",
    "end": "1508027"
  },
  {
    "text": "[INAUDIBLE] you can\nalways [INAUDIBLE],,",
    "start": "1508027",
    "end": "1515452"
  },
  {
    "text": "even if your results\nare [INAUDIBLE] like an equation, to\nsolve the equation.",
    "start": "1515452",
    "end": "1521425"
  },
  {
    "text": "If you get the [INAUDIBLE]. So the question\nis, theoretically, can't you just kind of\nretrieve the user data",
    "start": "1521425",
    "end": "1529610"
  },
  {
    "text": "even if you didn't\nstore it on disk? Is that what you're asking? [INAUDIBLE] So this relates to--",
    "start": "1529610",
    "end": "1536960"
  },
  {
    "text": "this depends on the\nmodel that you're using. You may have a\nmachine learning model that does have some sort of\nexplicit memory mechanism that",
    "start": "1536960",
    "end": "1543320"
  },
  {
    "text": "is kind of storing information. Or you have a model that\nhas a lot of capacity and it's possible to back\nout the data from that model.",
    "start": "1543320",
    "end": "1550847"
  },
  {
    "text": "And then there's other scenarios\nwhere you might train a model and it's actually not\npossible to back out the data from the model itself.",
    "start": "1550847",
    "end": "1557630"
  },
  {
    "text": "In general, if\nprivacy is a concern, then you want to use a method\nin which you can't back out the data from the model.",
    "start": "1557630",
    "end": "1563193"
  },
  {
    "text": "So there's a lot of\nliterature in what's called differential privacy that\ngets into how to train models",
    "start": "1563193",
    "end": "1568460"
  },
  {
    "text": "such that you can't back\nout the data from the model. And there's also\na lot of work that",
    "start": "1568460",
    "end": "1574130"
  },
  {
    "text": "has shown that if you train a\nmodel on a sequence of data, it will actually, especially\nif that sequence of data",
    "start": "1574130",
    "end": "1580940"
  },
  {
    "text": "is kind of changing\nover time, it will forget and start to do\npoorly on data from the past.",
    "start": "1580940",
    "end": "1589243"
  },
  {
    "text": "Does that answer your question? Yeah. Cool. Any other questions? ",
    "start": "1589243",
    "end": "1598710"
  },
  {
    "text": "Cool. Great. So there's a lot of variety in\nthis kind of lifelong learning",
    "start": "1598710",
    "end": "1603740"
  },
  {
    "text": "problem statement. Now I'd like to talk a\nlittle bit about algorithms",
    "start": "1603740",
    "end": "1609530"
  },
  {
    "text": "for solving it. I guess maybe to get\na little bit more into the problem statement,\nthere is a fairly simple way",
    "start": "1609530",
    "end": "1618740"
  },
  {
    "text": "to formalize the general online\nor lifelong learning problem.",
    "start": "1618740",
    "end": "1624780"
  },
  {
    "text": "And it's pretty simple. First, you have kind\nof time, and you're",
    "start": "1624780",
    "end": "1630740"
  },
  {
    "text": "progressing through time. And at each point in time,\nyou observe an input.",
    "start": "1630740",
    "end": "1636022"
  },
  {
    "text": "This could be an image.  It could be a patient record.",
    "start": "1636022",
    "end": "1643850"
  },
  {
    "text": "And your goal is to\npredict the label at that given point in time. And then after you\npredict the label,",
    "start": "1643850",
    "end": "1651620"
  },
  {
    "text": "you get to observe\nthe true label. And this process repeats.",
    "start": "1651620",
    "end": "1657930"
  },
  {
    "text": "This is a fairly typical kind\nof formalization of the problem. This doesn't cover all\npossible formulations.",
    "start": "1657930",
    "end": "1665730"
  },
  {
    "text": "For example, in some\nproblem statements, you may not be able to\nobserve the label right away. There might be some\nsort of time delay.",
    "start": "1665730",
    "end": "1672030"
  },
  {
    "text": "Maybe it takes, I\ndon't know, a year to label your data, or\nmaybe a month, or an hour.",
    "start": "1672030",
    "end": "1678760"
  },
  {
    "text": "And so you may have\nsome delay between when you make a prediction\nfor a data point and when you observe the label.",
    "start": "1678760",
    "end": "1684210"
  },
  {
    "text": "It may also be that you\ndon't label all of your data. And so in that\ncase, you may not be",
    "start": "1684210",
    "end": "1689940"
  },
  {
    "text": "able to observe the\nlabels for all of them. But this is a fairly\ngeneral statement",
    "start": "1689940",
    "end": "1697106"
  },
  {
    "text": "that's also pretty simple. And it makes it such\nthat you can tractably-- you can think about\nthis in theory.",
    "start": "1697107",
    "end": "1703140"
  },
  {
    "text": "And there's been a lot of\nkind of theoretical work that's looking at this\nproblem statement.",
    "start": "1703140",
    "end": "1710640"
  },
  {
    "text": "Yeah? With this approach,\nwouldn't it take a long time to build up a large enough\ndata set to get good accuracy?",
    "start": "1710640",
    "end": "1718710"
  },
  {
    "text": "Let's say in the\nintermediate section, we have a few examples that\nyou've seen [INAUDIBLE]",
    "start": "1718710",
    "end": "1725530"
  },
  {
    "text": "Yeah. So the question is,\nwouldn't it take a long time to accumulate a data\nset that has enough data",
    "start": "1725530",
    "end": "1730860"
  },
  {
    "text": "points, especially in\ndeep learning regimes, where we want millions\nof data points? Yeah, absolutely.",
    "start": "1730860",
    "end": "1736660"
  },
  {
    "text": "And so you wouldn't expect,\nin this problem setting, to be able to do very\nwell right off the bat, especially if you're looking at\nimage classification examples.",
    "start": "1736660",
    "end": "1744000"
  },
  {
    "text": "And so oftentimes,\nin practice, you'll warm start this process\nwith an existing data set that can help you\ninitialize your predictor",
    "start": "1744000",
    "end": "1756840"
  },
  {
    "text": "rather than starting\ncompletely from scratch. Yeah? Can we sort of think about\nthis as a modified Markov",
    "start": "1756840",
    "end": "1763380"
  },
  {
    "text": "chain or a transition from\n[INAUDIBLE] or optimizer",
    "start": "1763380",
    "end": "1770180"
  },
  {
    "text": "and the [INAUDIBLE] say, a\nmodel convergence to [INAUDIBLE]",
    "start": "1770180",
    "end": "1778820"
  },
  {
    "text": "the model parameters? Yeah, so I do think it's\npossible to formulate this as a Markov chain,\nespecially if it does converge.",
    "start": "1778820",
    "end": "1789230"
  },
  {
    "text": "Yeah.  So there's a couple different\nvariants on this problem",
    "start": "1789230",
    "end": "1801250"
  },
  {
    "text": "setting, as well. So the inputs, you could\nhave an IID setting where x and y are coming from\nthis stationary distribution",
    "start": "1801250",
    "end": "1809410"
  },
  {
    "text": "where the distributions\naren't a function of time. But you could also\nhave a setting",
    "start": "1809410",
    "end": "1816130"
  },
  {
    "text": "where these distributions are\nchanging over time, as well. ",
    "start": "1816130",
    "end": "1822330"
  },
  {
    "text": "There's also what's typically\nreferred to as a streaming setting where you're not allowed\nto actually store your data",
    "start": "1822330",
    "end": "1829160"
  },
  {
    "text": "points and you only-- you receive a stream. And the reason why\nthis is a setting",
    "start": "1829160",
    "end": "1835430"
  },
  {
    "text": "is that you might be receiving\nso much data that it's just impossible to\nstore it over time.",
    "start": "1835430",
    "end": "1844230"
  },
  {
    "text": "So for example, if you're\nstreaming video, for example, and you're a very\nlarge video platform,",
    "start": "1844230",
    "end": "1851510"
  },
  {
    "text": "you may not have time\nto actually store all of the frames\nthat you're receiving. ",
    "start": "1851510",
    "end": "1858480"
  },
  {
    "text": "So lack of memory is one reason. You could also have lack\nof computational resources",
    "start": "1858480",
    "end": "1863539"
  },
  {
    "text": "or privacy considerations. You may not be allowed\nto store the data,",
    "start": "1863540",
    "end": "1869120"
  },
  {
    "text": "or you want to create\na service in which you are promising to the user that\nyou don't store their data.",
    "start": "1869120",
    "end": "1876720"
  },
  {
    "text": "One other reason that the\nstreaming setting possibly could be interesting\nis if you're interested in studying\nanalogs to the brain.",
    "start": "1876720",
    "end": "1885950"
  },
  {
    "text": "We don't really have-- we don't have hard drives. We can't just access a hard\ndrive like a computer does.",
    "start": "1885950",
    "end": "1892680"
  },
  {
    "text": "And so there's some work that\ntries to understand and study how memory might\nwork in the brain.",
    "start": "1892680",
    "end": "1899150"
  },
  {
    "text": "And so in that sort of\nsetting, oftentimes, you want the model to not have\nany sort of external storage.",
    "start": "1899150",
    "end": "1905210"
  },
  {
    "text": "It instead needs to somehow\nstore it in its weights. ",
    "start": "1905210",
    "end": "1912450"
  },
  {
    "text": "Cool. This setting is\ntrue in some cases.",
    "start": "1912450",
    "end": "1918210"
  },
  {
    "text": "And you may also have-- maybe\nyou don't have zero memory, but you have limited\nmemory and you can only store a tiny bit of data.",
    "start": "1918210",
    "end": "1924159"
  },
  {
    "text": "There are also a\nlot of cases where it is actually quite practical\nto store a ton of data.",
    "start": "1924160",
    "end": "1929400"
  },
  {
    "text": "Hard drives are getting\npretty big these days. And oftentimes, we can\nactually store data",
    "start": "1929400",
    "end": "1934907"
  },
  {
    "text": "as part of the process.  Cool. So the last thing\nthat I'll mention",
    "start": "1934907",
    "end": "1942480"
  },
  {
    "text": "is we talked a little bit\nabout task boundaries. ",
    "start": "1942480",
    "end": "1947780"
  },
  {
    "text": "If you are kind of\nseeing these data points from a sequence\nof tasks, then instead of just\nobserving xt, you",
    "start": "1947780",
    "end": "1953670"
  },
  {
    "text": "can also observe\nthe task identifier that corresponds to that task.",
    "start": "1953670",
    "end": "1960990"
  },
  {
    "text": "Cool. So what do you actually\nwant from an algorithm that operates in this setting?",
    "start": "1960990",
    "end": "1967649"
  },
  {
    "text": "One quantity that often comes\nup in the online and lifelong",
    "start": "1967650",
    "end": "1972930"
  },
  {
    "text": "learning literature is this\nnotion of what's called regret. And the way that\nregret is defined",
    "start": "1972930",
    "end": "1980370"
  },
  {
    "text": "is you're looking at\nthe cumulative loss of the algorithm, of the\nalgorithm's learning over time,",
    "start": "1980370",
    "end": "1988590"
  },
  {
    "text": "minus the cumulative loss of\nthe best learner in hindsight. And so specifically,\nthis can be written",
    "start": "1988590",
    "end": "1996300"
  },
  {
    "text": "with this equation where\nthe regret at a given point of time,\ncorresponding to capital T,",
    "start": "1996300",
    "end": "2003080"
  },
  {
    "text": "will basically\njust say, OK, what was the loss that your algorithm\nachieved at each point in time",
    "start": "2003080",
    "end": "2011120"
  },
  {
    "text": "summed over time? So that's pretty simple. That's just the loss that your\nalgorithm is getting over time.",
    "start": "2011120",
    "end": "2018049"
  },
  {
    "text": "And then the second term is\nlooking at, in hindsight, if we were able to pick a\nsingle set of parameters",
    "start": "2018050",
    "end": "2024980"
  },
  {
    "text": "for all of those time periods,\nwhat's the best parameter",
    "start": "2024980",
    "end": "2030380"
  },
  {
    "text": "vector that you could have\ndone, given all of that data and evaluating the\nsum of the losses",
    "start": "2030380",
    "end": "2037700"
  },
  {
    "text": "for that particular\nparameter vector. ",
    "start": "2037700",
    "end": "2043670"
  },
  {
    "text": "Yeah? For that, the second\npoint, the parameter vector that you could have\ndone across the board,",
    "start": "2043670",
    "end": "2050179"
  },
  {
    "text": "is it just to launch the\nlatest set of parameters at your latest task,\nat your current task?",
    "start": "2050179",
    "end": "2057050"
  },
  {
    "text": "Is that the best one because\nit's seen the most data? Or could it be another one?",
    "start": "2057050",
    "end": "2062149"
  },
  {
    "text": "Yeah, so the question\nis, does data just correspond to the\nlast parameters, or could it be another one? So one important thing to note\nis that, in general, you can't",
    "start": "2062150",
    "end": "2070879"
  },
  {
    "text": "evaluate regret in practice. Because we don't\nnecessarily have access to that best\npossible parameters.",
    "start": "2070880",
    "end": "2076158"
  },
  {
    "text": "But it is pretty\nuseful for analysis. That said, if you did\nwant to approximate this",
    "start": "2076159",
    "end": "2081800"
  },
  {
    "text": "at the end of training,\nyou could basically take all of the data that\nyou had seen so far, train",
    "start": "2081800",
    "end": "2087230"
  },
  {
    "text": "a model on all of\nthat data, and that would give you a reasonable\nproxy for that second term.",
    "start": "2087230",
    "end": "2094370"
  },
  {
    "text": "Yeah? Couldn't this value [INAUDIBLE]\nbe negative, as well, since you're looking for\n[INAUDIBLE] different data",
    "start": "2094370",
    "end": "2102010"
  },
  {
    "text": "for-- Right. So typically, this value is\nlooked at in the IID setting,",
    "start": "2102010",
    "end": "2110700"
  },
  {
    "text": "where, basically, there\nexists a single model that can do well on all of the data. But if you were in\na non-IID setting,",
    "start": "2110700",
    "end": "2117269"
  },
  {
    "text": "you could actually\ncertainly do better. You could have a negative\nregret because you might be able to adapt\nto each point in time",
    "start": "2117270",
    "end": "2123000"
  },
  {
    "text": "and get a model that's better\nthan just a single model evaluated on all the data. ",
    "start": "2123000",
    "end": "2131790"
  },
  {
    "text": "Cool. Now, one thing that's worth\nalso mentioning about this is that this is a quantity that\nis a function of capital T.",
    "start": "2131790",
    "end": "2140840"
  },
  {
    "text": "And so oftentimes,\nwe think about how is the regret growing over time?",
    "start": "2140840",
    "end": "2148170"
  },
  {
    "text": "And what note about this is\nthat if the regret is growing",
    "start": "2148170",
    "end": "2155309"
  },
  {
    "text": "linearly with time, it's\nactually fairly trivial",
    "start": "2155310",
    "end": "2161330"
  },
  {
    "text": "to construct a algorithm\nthat has linear regret.",
    "start": "2161330",
    "end": "2167390"
  },
  {
    "text": "Does anyone have any idea\nfor why that's the case? Yeah? Don't you just have zeros,\nand then the minimum",
    "start": "2167390",
    "end": "2175240"
  },
  {
    "text": "of the second term will just\nincrease by the same amount every time step?",
    "start": "2175240",
    "end": "2180540"
  },
  {
    "text": "Is that what-- Sorry. Can you repeat that? I didn't quite get it. Maybe I'm misunderstanding--",
    "start": "2180540",
    "end": "2186859"
  },
  {
    "text": "No. I think-- well, I think\nyou were mostly right. But I didn't fully\nunderstand it.",
    "start": "2186860",
    "end": "2192190"
  },
  {
    "text": "If you just have both the\nparameter at each time step to be just zero. Because the second term of\nthis is just increasing.",
    "start": "2192190",
    "end": "2200140"
  },
  {
    "text": "You add one of the same\ncopy every time step. And then if you assume\nboth tasks over time",
    "start": "2200140",
    "end": "2207880"
  },
  {
    "text": "to be similar in terms\nof the last magnitude, you will just have something\nthat grows linearly.",
    "start": "2207880",
    "end": "2214460"
  },
  {
    "text": "Yeah. So basically, if you have just\na constant set of parameters that your algorithm is\nproducing for theta t,",
    "start": "2214460",
    "end": "2220870"
  },
  {
    "text": "then you can actually get\nlinear regret just with that.",
    "start": "2220870",
    "end": "2226570"
  },
  {
    "text": "And maybe we can walk\nthrough an example to see why that\nmight be the case. So we can look at a very\nsimple example, which is maybe",
    "start": "2226570",
    "end": "2237070"
  },
  {
    "text": "you're really into basketball. And you're interested\nin being able to predict the number of points that a\nplayer will score over time.",
    "start": "2237070",
    "end": "2245590"
  },
  {
    "text": "And at the first\npoint in time, you don't know much\nabout the player.",
    "start": "2245590",
    "end": "2251890"
  },
  {
    "text": "They're kind of\njoining for the-- they're playing\ntheir first game. And so maybe you\nguessed that they're",
    "start": "2251890",
    "end": "2257792"
  },
  {
    "text": "going to have 10 points,\nscore 10 points in a game. But then it turns out that they\nscore 30 points in that game.",
    "start": "2257792",
    "end": "2265900"
  },
  {
    "text": "And then at the next point\nin time, in the next game,",
    "start": "2265900",
    "end": "2271180"
  },
  {
    "text": "you also guess 10 because you\njust have your constant model. And then they actually\nscore 28 points.",
    "start": "2271180",
    "end": "2278109"
  },
  {
    "text": "And likewise, maybe the\nthird time you see them play, you also guess 10 because you're\nusing just a constant model.",
    "start": "2278110",
    "end": "2283780"
  },
  {
    "text": "And they actually\nscore 32 points. In this case, in hindsight,\nthe best model, if we--",
    "start": "2283780",
    "end": "2292663"
  },
  {
    "text": "well, I guess we could first\ncompute the first term. So the first term would be maybe\nyou're using an absolute loss,",
    "start": "2292663",
    "end": "2299890"
  },
  {
    "text": "so your loss would be 20\nfor the first time step",
    "start": "2299890",
    "end": "2305279"
  },
  {
    "text": "plus 18 for the second time\nstep plus 22 for the third time",
    "start": "2305280",
    "end": "2313320"
  },
  {
    "text": "step, whereas in\nhindsight, the best model,",
    "start": "2313320",
    "end": "2319470"
  },
  {
    "text": "in hindsight-- for\nthe first data point, it doesn't have anything. So maybe it would also\nhave a loss of 20.",
    "start": "2319470",
    "end": "2325079"
  },
  {
    "text": " But after that point, it\nwould probably guess--",
    "start": "2325080",
    "end": "2334550"
  },
  {
    "text": "it should guess-- the optimal\nthing should guess 30. The optimal thing in hindsight--\nso this is something where",
    "start": "2334550",
    "end": "2343560"
  },
  {
    "text": "the regret is growing\nby, basically, an average of 20\nat each time step-- or, sorry, the first\nterm is growing by 20",
    "start": "2343560",
    "end": "2350609"
  },
  {
    "text": "at each time step. The second term is\nthe best in hindsight.",
    "start": "2350610",
    "end": "2357840"
  },
  {
    "text": "And so that would get a\nloss that is fairly small.",
    "start": "2357840",
    "end": "2363100"
  },
  {
    "text": "So at the first time\nstep, it would guess 30.",
    "start": "2363100",
    "end": "2368500"
  },
  {
    "text": "At the second time\nstep, it would guess 29. And the third time\nstep, it would output 30",
    "start": "2368500",
    "end": "2377375"
  },
  {
    "text": "because that's the average. And so basically, the second\nterm is fairly close to 0,",
    "start": "2377375",
    "end": "2383200"
  },
  {
    "text": "whereas this first term is\ngrowing by 20 at each time step. ",
    "start": "2383200",
    "end": "2390720"
  },
  {
    "text": "In contrast, you can\ndo much better if, instead of guessing\n10 at every time step,",
    "start": "2390720",
    "end": "2396480"
  },
  {
    "text": "if you guess kind of the\naverage of the scores that it had seen previously. So at the second time\nstep, you could guess 30.",
    "start": "2396480",
    "end": "2402630"
  },
  {
    "text": "And then at the third time step,\nthe average of 30 and 28 is 29. And so you could guess 29.",
    "start": "2402630",
    "end": "2408690"
  },
  {
    "text": "And in that case, your regret\nis going to grow sublinearly.",
    "start": "2408690",
    "end": "2413910"
  },
  {
    "text": "Because then, instead\nof being 20, 18, 22,",
    "start": "2413910",
    "end": "2419280"
  },
  {
    "text": "your loss will be 20 at the\nfirst time step and then 2",
    "start": "2419280",
    "end": "2425410"
  },
  {
    "text": "at the second time step and\nthen 3 at the third time step. And so if you were to\nplot this and looked",
    "start": "2425410",
    "end": "2432849"
  },
  {
    "text": "at how this grows over time,\nthe first one would be, if this is grades of 20\nand this is each time step,",
    "start": "2432850",
    "end": "2441400"
  },
  {
    "text": "your regret would be linear,\nwhereas for the second one,",
    "start": "2441400",
    "end": "2447819"
  },
  {
    "text": "you would first go up\nto here, but then it would grow much more slowly\nat future time steps.",
    "start": "2447820",
    "end": "2454974"
  },
  {
    "text": " Cool. So I guess the takeaway\nhere is that in terms",
    "start": "2454975",
    "end": "2464450"
  },
  {
    "text": "of defining regret in\nthese online learning settings, linear,\nyou basically want to try to have sublinear\nregret, and you",
    "start": "2464450",
    "end": "2471800"
  },
  {
    "text": "want to grow it as slowly\nas possible over time. Because that means\nthat you're actually",
    "start": "2471800",
    "end": "2476960"
  },
  {
    "text": "getting better and better\nat making predictions on your online learning problem. ",
    "start": "2476960",
    "end": "2487380"
  },
  {
    "text": "Cool. One of the really important\nconcept in lifelong learning is this notion of positive\nand negative transfer",
    "start": "2487380",
    "end": "2495630"
  },
  {
    "text": "and also forward transfer\nand backward transfer. So we talked a lot\nabout trying to be",
    "start": "2495630",
    "end": "2501960"
  },
  {
    "text": "able to do better\non future tasks and also better on past tasks.",
    "start": "2501960",
    "end": "2507000"
  },
  {
    "text": "And so positive forward\ntransfer is this notion of being able to do better\non future tasks as a result",
    "start": "2507000",
    "end": "2513660"
  },
  {
    "text": "of seeing the previous tasks. And this can be\nmeasured in comparison",
    "start": "2513660",
    "end": "2519450"
  },
  {
    "text": "to learning the future\ntasks from scratch. ",
    "start": "2519450",
    "end": "2524940"
  },
  {
    "text": "Positive backward transfer\nis basically the ability to do well on past tasks.",
    "start": "2524940",
    "end": "2531570"
  },
  {
    "text": "And so if you have a\npositive backward transfer, that means that the\ncurrent tasks are actually",
    "start": "2531570",
    "end": "2536970"
  },
  {
    "text": "causing you to do better\non the previous tasks than if you had only seen the\nprevious tasks learned",
    "start": "2536970",
    "end": "2545490"
  },
  {
    "text": "from scratch. Yeah? Is there an example\nwhere this happens? Where positive backward\ntransfer happens?",
    "start": "2545490",
    "end": "2551940"
  },
  {
    "text": "Yeah. So one example is\nmaybe for each task, you don't have that much data.",
    "start": "2551940",
    "end": "2558360"
  },
  {
    "text": "Maybe you only have, I don't\nknow, 100 data points per task. And maybe once you basically\naccumulate enough data",
    "start": "2558360",
    "end": "2564900"
  },
  {
    "text": "from all the tasks\nyou've seen so far, you may actually do\nbetter on the first task than if you hadn't\nseen all of that data.",
    "start": "2564900",
    "end": "2571185"
  },
  {
    "text": " I think that that's probably one\nof the most common cases where",
    "start": "2571185",
    "end": "2576330"
  },
  {
    "text": "you might see this. And then you could\nalso, likewise, define negative forward transfer\nand negative backward transfer.",
    "start": "2576330",
    "end": "2583890"
  },
  {
    "text": "That's where, basically,\nyou're actually getting worse at future tasks\nor worse at previous tasks as a result of seeing\nthe other tasks.",
    "start": "2583890",
    "end": "2594820"
  },
  {
    "text": "And negative backward transfer\nis actually quite common, especially if you have a\nsmall amount of memory and you",
    "start": "2594820",
    "end": "2600579"
  },
  {
    "text": "can't-- and you basically start\nforgetting those past tasks.",
    "start": "2600580",
    "end": "2608920"
  },
  {
    "text": "Positive forward transfer\nis also fairly common because you're essentially\nkind of pretraining",
    "start": "2608920",
    "end": "2614470"
  },
  {
    "text": "on the previous tasks rather\nthan starting completely from scratch. Yeah? When negative backward\ntransfer occurs,",
    "start": "2614470",
    "end": "2623800"
  },
  {
    "text": "do you think it's mainly because\nof the limited memory thing, or it's impossible for there\nto be a model that can do well",
    "start": "2623800",
    "end": "2631660"
  },
  {
    "text": "on all of the tasks? Or is it just because of\nsomething about the training order? Like, if you processed\nthe same data differently,",
    "start": "2631660",
    "end": "2637539"
  },
  {
    "text": "you could get a model that\ndid well on all of them? Right. So the question is, when you\nsee negative backward transfer,",
    "start": "2637540",
    "end": "2643550"
  },
  {
    "text": "is it caused by memory\nor caused by ordering in which the tasks were seen?",
    "start": "2643550",
    "end": "2650890"
  },
  {
    "text": "Typically, people\nsee it the most when they have very,\nvery little memory",
    "start": "2650890",
    "end": "2657430"
  },
  {
    "text": "and they just kind of\ntrain on all the data-- basically train on the data\nas it comes and don't actually",
    "start": "2657430",
    "end": "2663280"
  },
  {
    "text": "continue to train\non the old data. That's really the\nmost common instance of negative backward transfer.",
    "start": "2663280",
    "end": "2668890"
  },
  {
    "text": "That said, you can also\nhave both negative forward and negative backward\ntransfer if your model just",
    "start": "2668890",
    "end": "2673900"
  },
  {
    "text": "doesn't have enough capacity. And so if you're trying to train\non a lot of tasks with a really",
    "start": "2673900",
    "end": "2681640"
  },
  {
    "text": "small-capacity model, that\ncan lead to worse performance than if you were to train only\non that task from scratch.",
    "start": "2681640",
    "end": "2687370"
  },
  {
    "text": "And in that case, you could\nactually see negative transfer both in the forward direction\nand in the backward direction.",
    "start": "2687370",
    "end": "2693890"
  },
  {
    "text": "Yeah? Is this taxonomy\nreally that clean? [INAUDIBLE] tasks\ncauses them to converge",
    "start": "2693890",
    "end": "2704125"
  },
  {
    "text": "backwards to a higher loss? ",
    "start": "2704125",
    "end": "2709550"
  },
  {
    "text": "So your first question is,\nis the taxonomy that clean? I do think that these are\npretty cleanly defined.",
    "start": "2709550",
    "end": "2716295"
  },
  {
    "text": "I think that you can\nkind of write down equations for each of these. Your second question was--",
    "start": "2716295",
    "end": "2721710"
  },
  {
    "text": "can you repeat your second\npart of the question? I recall that we discussed\nprevious tasks causing",
    "start": "2721710",
    "end": "2726990"
  },
  {
    "text": "to converge faster or\nconverge to a higher loss. ",
    "start": "2726990",
    "end": "2732880"
  },
  {
    "text": "Right, I see. So this is often measured in\nterms of final performance",
    "start": "2732880",
    "end": "2740599"
  },
  {
    "text": "on the task. But you're right in\nthat there may be-- there's more than one\ncriteria that you care about.",
    "start": "2740600",
    "end": "2747530"
  },
  {
    "text": "You may care about\nlearning efficiency. You may care about\nfinal performance. And you may actually get\nbetter efficiency but worse",
    "start": "2747530",
    "end": "2754520"
  },
  {
    "text": "final performance. And so it's important--",
    "start": "2754520",
    "end": "2760579"
  },
  {
    "text": "you can basically\nmeasure both of these in terms of performance\nand in terms of efficiency.",
    "start": "2760580",
    "end": "2765860"
  },
  {
    "text": "It depends on what your\nperformance metric is. And so if your metric\nis the number of images",
    "start": "2765860",
    "end": "2772430"
  },
  {
    "text": "it takes to get a certain\nlevel of performance, then you can measure these\nin terms of that metric.",
    "start": "2772430",
    "end": "2777530"
  },
  {
    "text": "You could also\nmeasure each of these in terms of the final\nperformance, as well. And so you basically just\nneed to define the metric you",
    "start": "2777530",
    "end": "2784549"
  },
  {
    "text": "care about. And then these terms\nmake sense in the context of a certain metric.",
    "start": "2784550",
    "end": "2790569"
  },
  {
    "text": "And actually, if we have time\nat the end of the lecture, we'll show some\nexperiments where we're measuring both\nlearning efficiency",
    "start": "2790570",
    "end": "2796643"
  },
  {
    "text": "and final performance. Because oftentimes,\nyou do care about both. Cool. So let's get into\nsome basic approaches.",
    "start": "2796643",
    "end": "2804290"
  },
  {
    "text": "And really, there's two\nvery simple approaches, which are probably the\nthings that all of you",
    "start": "2804290",
    "end": "2811730"
  },
  {
    "text": "could come up with if you\nwere to try to approach one of these problems. The first approach is to store\nall the data you've seen so far",
    "start": "2811730",
    "end": "2819470"
  },
  {
    "text": "and train on it. And so, in particular, up\nuntil time t, you have--",
    "start": "2819470",
    "end": "2829184"
  },
  {
    "text": " at time t, you have\nall of the data that's",
    "start": "2829185",
    "end": "2834900"
  },
  {
    "text": "basically the union of all the\ndata points that you've seen.",
    "start": "2834900",
    "end": "2844115"
  },
  {
    "text": " Whoops.",
    "start": "2844115",
    "end": "2849260"
  },
  {
    "text": "Maybe I'll use\ncapital T for this. So at time T, the union from\nt equals 1 to T. And you just",
    "start": "2849260",
    "end": "2859130"
  },
  {
    "text": "train a model that-- this is the data up\nuntil that time step--",
    "start": "2859130",
    "end": "2864840"
  },
  {
    "text": "and then you just\ntrain a model that tries to minimize your\nloss function on that data.",
    "start": "2864840",
    "end": "2873990"
  },
  {
    "text": "And you train this model\nat every single time step. Of course, if you were to\ntrain a model from scratch",
    "start": "2873990",
    "end": "2879840"
  },
  {
    "text": "on the data set at\nevery time step, that would be very expensive. And so typically, you'll\nwarm start this model",
    "start": "2879840",
    "end": "2885420"
  },
  {
    "text": "with a model trained at\nthe previous time step. And you won't\nnecessarily train this",
    "start": "2885420",
    "end": "2891240"
  },
  {
    "text": "all the way to convergence. ",
    "start": "2891240",
    "end": "2897040"
  },
  {
    "text": "This is referred to often as\nthe follow the leader algorithm and generally achieves\nvery strong performance.",
    "start": "2897040",
    "end": "2904790"
  },
  {
    "text": "It can be computationally\nintensive. But if you warm start, that can\nhelp with the amount of compute",
    "start": "2904790",
    "end": "2912069"
  },
  {
    "text": "that you need. So if you're basically\njust continuously fine tuning on the data you've\nseen so far, that can help.",
    "start": "2912070",
    "end": "2918408"
  },
  {
    "text": "It can also be memory\nintensive because you need to store all the data\nthat you've seen so far.",
    "start": "2918408",
    "end": "2924210"
  },
  {
    "text": "And for some applications,\nthis is a deal breaker. For some applications,\nthis is completely fine. ",
    "start": "2924210",
    "end": "2931340"
  },
  {
    "text": "Cool. And then the second approach\nis simply at every time step",
    "start": "2931340",
    "end": "2937180"
  },
  {
    "text": "to take a gradient step on the\ndata point that you observe. And so what this means is\ninstead of actually storing",
    "start": "2937180",
    "end": "2943690"
  },
  {
    "text": "all of this data,\nbasically, at time T,",
    "start": "2943690",
    "end": "2949319"
  },
  {
    "text": "you will just update your model\nby evaluating it on the data",
    "start": "2949320",
    "end": "2955620"
  },
  {
    "text": "that you received at that\nparticular point in time. And so this would\ncorrespond to just looking",
    "start": "2955620",
    "end": "2962010"
  },
  {
    "text": "at the data at\nthat period of time",
    "start": "2962010",
    "end": "2967330"
  },
  {
    "text": "and taking a gradient\nstep on that. You can also take a\ncouple of gradient steps if you think your\nmodel needs it.",
    "start": "2967330",
    "end": "2974079"
  },
  {
    "text": "This is basically just\nstochastic gradient descent. It's, computationally,\nvery cheap.",
    "start": "2974080",
    "end": "2980870"
  },
  {
    "text": "It also doesn't\nrequire any memory beyond the memory of one data\npoint and your parameters.",
    "start": "2980870",
    "end": "2988960"
  },
  {
    "text": "It is subject to negative\nbackward transfer because it might forget\nsome of the data points",
    "start": "2988960",
    "end": "2994329"
  },
  {
    "text": "that it saw before. Oftentimes, when we run SGD,\nwe need multiple epochs, multiple passes over the data.",
    "start": "2994330",
    "end": "2999500"
  },
  {
    "text": "And this is not really\nbeing given multiple passes over the data.",
    "start": "2999500",
    "end": "3005109"
  },
  {
    "text": "This is often referred\nto as forgetting, sometimes referred to as\ncatastrophic forgetting, although I think that's a little\nbit dramatic, in some cases.",
    "start": "3005110",
    "end": "3012714"
  },
  {
    "text": " It could also be somewhat slow\nto learn, as well, especially",
    "start": "3012715",
    "end": "3020010"
  },
  {
    "text": "if you only take\none gradient stop. And if you take\nmultiple gradient steps, then you might overfit\nto the current data point",
    "start": "3020010",
    "end": "3026010"
  },
  {
    "text": "that you are seeing. ",
    "start": "3026010",
    "end": "3031970"
  },
  {
    "text": "Any questions about\nthese two approaches? Yeah? [INAUDIBLE] forwarding.",
    "start": "3031970",
    "end": "3039380"
  },
  {
    "text": "But like any [INAUDIBLE]\nin this case, [INAUDIBLE] getting the data in\nan online fashion?",
    "start": "3039380",
    "end": "3045359"
  },
  {
    "text": "Yeah. So if you're in the IID\nsetting, this is not too bad. If you're in the\nsetting where you're not",
    "start": "3045360",
    "end": "3052010"
  },
  {
    "text": "getting data points\nin an IID setting, then this can be\nmore challenging",
    "start": "3052010",
    "end": "3057350"
  },
  {
    "text": "because you'll have correlations\nbetween neighboring data points. And that can cause--",
    "start": "3057350",
    "end": "3062990"
  },
  {
    "text": "that especially\ncan cause-- things like forgetting where\nit'll just memorize the kind of correlations\nthat it's seeing right now.",
    "start": "3062990",
    "end": "3070369"
  },
  {
    "text": "For example, if you see-- if you pass in data that\nhas correlated labels, it might just\nmemorize that label",
    "start": "3070370",
    "end": "3077180"
  },
  {
    "text": "and not even consider outputting\nlabels that it saw in the past. ",
    "start": "3077180",
    "end": "3085269"
  },
  {
    "text": "Cool. So these are the\nreally simple things. They're actually not\nthat terrible for things.",
    "start": "3085270",
    "end": "3093130"
  },
  {
    "text": "And that's why like things\nlike SGD are very common. There's also a question of\nwhether we can do better.",
    "start": "3093130",
    "end": "3099160"
  },
  {
    "text": "Before we talk about\nwhether we can do better, we can look at just applying one\nof these very simple algorithms",
    "start": "3099160",
    "end": "3104230"
  },
  {
    "text": "in a robotics scenario. And we talked a little\nbit about warm starting.",
    "start": "3104230",
    "end": "3109630"
  },
  {
    "text": "And so in this case, we're\ngoing to warm start it on a pretty large data\nset of robot grasping.",
    "start": "3109630",
    "end": "3116200"
  },
  {
    "text": "It corresponded to\n580,000 grasp attempts collected across seven robots.",
    "start": "3116200",
    "end": "3122050"
  },
  {
    "text": "This model was able to achieve\nan 86% grasp success rate on objects that look like this.",
    "start": "3122050",
    "end": "3128470"
  },
  {
    "text": "But then, if you gave it\nobjects that were a little bit different, like\nthese glass bottles, the performance\ndropped significantly.",
    "start": "3128470",
    "end": "3136680"
  },
  {
    "text": "And so the way\nthat this will work is it will be very similar\nto the first algorithm",
    "start": "3136680",
    "end": "3143630"
  },
  {
    "text": "that we saw where we want\nto continuously fine tune on the data.",
    "start": "3143630",
    "end": "3148790"
  },
  {
    "text": "And it's going to use a Q\nlearning algorithm to do this. It's not too important, the\nspecifics of the RL approach,",
    "start": "3148790",
    "end": "3156920"
  },
  {
    "text": "but basically, it\ntakes the initial data, it kind of trains an\ninitial model on that data,",
    "start": "3156920",
    "end": "3162020"
  },
  {
    "text": "and then uses that to\ninitialize a Q-function or initialize the\nneural network model.",
    "start": "3162020",
    "end": "3168559"
  },
  {
    "text": "And it adapts on a mix\nof the original data and the data in\nthe new scenario,",
    "start": "3168560",
    "end": "3174410"
  },
  {
    "text": "like with the glass bottles. And even just fine tuning does\nquite well in this scenario.",
    "start": "3174410",
    "end": "3180708"
  },
  {
    "text": "So if you just do\npretraining and then fine tuning to one scenario, you\ncould increase the success rate",
    "start": "3180708",
    "end": "3185920"
  },
  {
    "text": "from 32% to 63% in\nthis harsh lighting condition, from 49% to 66%\nwith transparent bottles,",
    "start": "3185920",
    "end": "3193450"
  },
  {
    "text": "and so on and so forth for\nthese different scenarios.",
    "start": "3193450",
    "end": "3198820"
  },
  {
    "text": "But this is just fine tuning. What about a lifelong\nlearning setting? So we can basically take\nthis same sort of approach",
    "start": "3198820",
    "end": "3206850"
  },
  {
    "text": "and apply it in this\nsequential learning scenario where for each new condition\nthat the robot encounters,",
    "start": "3206850",
    "end": "3213240"
  },
  {
    "text": "it will basically initialize\nthe neural network with the parameters from\nthe previous time step,",
    "start": "3213240",
    "end": "3222030"
  },
  {
    "text": "or the previous task. And then it will be\nfine tuned on a mix",
    "start": "3222030",
    "end": "3227350"
  },
  {
    "text": "of the data from\nthe current scenario and the original\npretraining data set.",
    "start": "3227350",
    "end": "3235180"
  },
  {
    "text": "And if you do this,\nyou can get performance that also looks quite good.",
    "start": "3235180",
    "end": "3240760"
  },
  {
    "text": "And so it's able to continuously\nfine tune on each new scenario that it experiences and is able\nto get much better performance",
    "start": "3240760",
    "end": "3249370"
  },
  {
    "text": "than if you just took the model\ntrained on the first data set. ",
    "start": "3249370",
    "end": "3256573"
  },
  {
    "text": "One thing that you\nmight notice is that these numbers are actually\nmonotonically increasing. I think that that\nhappened by chance.",
    "start": "3256573",
    "end": "3265009"
  },
  {
    "text": "There is no reason\nto necessarily expect it to get better and better\nas you continuously fine tune.",
    "start": "3265010",
    "end": "3270290"
  },
  {
    "text": "It may be that, for\nexample, the last task is maybe a little bit easier. Or it could be that\nit just benefited",
    "start": "3270290",
    "end": "3278390"
  },
  {
    "text": "from having more gradient\nsteps, for some reason, or something like that. Now, this is a scenario\nwhere we're keeping",
    "start": "3278390",
    "end": "3286100"
  },
  {
    "text": "around data from the first-- the initial data set. And so, as a result, I\nwould expect the models here",
    "start": "3286100",
    "end": "3294410"
  },
  {
    "text": "to be able to\nstill transfer well to the original environment. That said, if you didn't assume\nthat you had all that memory,",
    "start": "3294410",
    "end": "3302000"
  },
  {
    "text": "then it probably would still\nforget in that scenario. And so what I'd like to talk\nabout in this next section",
    "start": "3302000",
    "end": "3308780"
  },
  {
    "text": "is whether we can\ndesign an algorithm that uses only a very\nsmall amount of memory in order to avoid that sort\nof negative backward transfer.",
    "start": "3308780",
    "end": "3317540"
  },
  {
    "text": "So the case study\nhere will be this idea",
    "start": "3317540",
    "end": "3323120"
  },
  {
    "text": "of having an episodic memory. And we're going\nto assume that we can store a very small amount\nof data per task in memory.",
    "start": "3323120",
    "end": "3331440"
  },
  {
    "text": "And then, when making\nupdates for new tasks, we're going to try to ensure\nthat those updates don't",
    "start": "3331440",
    "end": "3339750"
  },
  {
    "text": "unlearn the previous tasks. So the first step is easy. The second step is a\nlittle bit more difficult.",
    "start": "3339750",
    "end": "3346020"
  },
  {
    "text": "That's kind of the\nmeat of the algorithm. And so we're going\nto assume that we're learning some predictor\nthat takes as input",
    "start": "3346020",
    "end": "3352420"
  },
  {
    "text": "the example and the task\nID and predicts the label.",
    "start": "3352420",
    "end": "3359049"
  },
  {
    "text": "And then we'll assume that we\nhave some memory for each task. This memory might have as few\nas five examples, for example.",
    "start": "3359050",
    "end": "3369130"
  },
  {
    "text": "And then at each\ntime step, we're going to minimize the loss\nof our predictor on the data",
    "start": "3369130",
    "end": "3377170"
  },
  {
    "text": "from that time\nstep for that task.  But the key part\nis that we're going",
    "start": "3377170",
    "end": "3383380"
  },
  {
    "text": "to add this constraint,\nwhich is that we don't want the predictor, the new predictor\nthat we're optimizing over,",
    "start": "3383380",
    "end": "3394180"
  },
  {
    "text": "on task k to be worse\nthan the predictor from the previous time step.",
    "start": "3394180",
    "end": "3401600"
  },
  {
    "text": "And we're going to try\nto apply this constraint for all of our previous tasks,\nnot just our current task.",
    "start": "3401600",
    "end": "3407135"
  },
  {
    "text": " And so the idea\nhere is that we want",
    "start": "3407135",
    "end": "3412460"
  },
  {
    "text": "to find a new\npredictor that does well on the new task, or\nthe new data point, such",
    "start": "3412460",
    "end": "3418820"
  },
  {
    "text": "that the loss on the previous\ntasks doesn't get worse. And this constraint\nwill use the memory",
    "start": "3418820",
    "end": "3424890"
  },
  {
    "text": "that we had stored for\nthe previous tasks. And so this isn't\nnecessarily-- this",
    "start": "3424890",
    "end": "3432029"
  },
  {
    "text": "is going to make the assumption\nthat that memory is at least somewhat representative. And we could imagine possibly\noverfitting to that memory",
    "start": "3432030",
    "end": "3440230"
  },
  {
    "text": "and just memorizing\nthose functions, although, in practice, if\nyou're making a small update",
    "start": "3440230",
    "end": "3449860"
  },
  {
    "text": "to the model, you would\nexpect that if it doesn't make",
    "start": "3449860",
    "end": "3456340"
  },
  {
    "text": "the predictions on some\nof the data points worse, it probably shouldn't also make\nother things a lot worse, as well.",
    "start": "3456340",
    "end": "3462590"
  },
  {
    "text": "Now, there's a question of,\nhow do we actually implement",
    "start": "3462590",
    "end": "3467960"
  },
  {
    "text": "this constraint in practice? And what we're going to do is we\ndon't want to make these worse.",
    "start": "3467960",
    "end": "3474720"
  },
  {
    "text": "And so we can add this kind\nof local linearity assumption.",
    "start": "3474720",
    "end": "3480950"
  },
  {
    "text": "And basically, instead\nof placing a constraint on the loss function, we\ncan place a constraint",
    "start": "3480950",
    "end": "3488120"
  },
  {
    "text": "on the gradients. And so what this looks\nlike is we'll basically try to make it such\nthat we ensure that",
    "start": "3488120",
    "end": "3494570"
  },
  {
    "text": "the gradient of the function--",
    "start": "3494570",
    "end": "3499960"
  },
  {
    "text": "basically, the\ngradient that we're trying to apply to\nour predictor has",
    "start": "3499960",
    "end": "3507029"
  },
  {
    "text": "points in the same\ndirection or is orthogonal to the gradient\nfor the previous task.",
    "start": "3507030",
    "end": "3514390"
  },
  {
    "text": "And if their\ngradients are pointing in the same direction\nor orthogonal,",
    "start": "3514390",
    "end": "3519730"
  },
  {
    "text": "then, assuming local\nlinearity, that means that the loss function\nfor the previous task",
    "start": "3519730",
    "end": "3526990"
  },
  {
    "text": "won't decrease.  And so if they're orthogonal, we\nwould expect the loss function",
    "start": "3526990",
    "end": "3533610"
  },
  {
    "text": "to stay the same. If they're actually pointing\nin the same direction or at least somewhat\nin the same direction,",
    "start": "3533610",
    "end": "3538769"
  },
  {
    "text": "then you may actually get\npositive backward transfer, where, actually,\nyou might actually improve on the previous tasks.",
    "start": "3538770",
    "end": "3545759"
  },
  {
    "start": "3545760",
    "end": "3553520"
  },
  {
    "text": "Cool. So we're basically going to try\nto improve on the current time steps while also ensuring that\nthe gradients are pointing",
    "start": "3553520",
    "end": "3561460"
  },
  {
    "text": "in the same direction\nas the gradients for the previous tasks.",
    "start": "3561460",
    "end": "3567460"
  },
  {
    "text": "It's worth noting\nthat it's possible that this constraint leads\nto an infeasible constraint",
    "start": "3567460",
    "end": "3573610"
  },
  {
    "text": "optimization. Because if your gradients\nfor your current task and your previous task\nare pointing in opposite--",
    "start": "3573610",
    "end": "3580090"
  },
  {
    "text": "it may be that it's not possible\nto improve on the current task while also going in the\nsame direction as all",
    "start": "3580090",
    "end": "3585700"
  },
  {
    "text": "these other past tasks. And this may become\nincreasingly infeasible",
    "start": "3585700",
    "end": "3591130"
  },
  {
    "text": "as you increase the\nnumber of past tasks. Because that's going\nto basically increase the number of constraints\non your optimization.",
    "start": "3591130",
    "end": "3598250"
  },
  {
    "text": "Yeah? [INAUDIBLE] if the number\nof your data points is smaller than the number\nof parameters in your model,",
    "start": "3598250",
    "end": "3605290"
  },
  {
    "text": "[INAUDIBLE] become infeasible\nfor every single time step?",
    "start": "3605290",
    "end": "3611338"
  },
  {
    "text": "If a number of data\npoints is greater than the number of\nparameters in your model, would it become infeasible?",
    "start": "3611338",
    "end": "3618760"
  },
  {
    "text": "It really depends on what the\ndata points are, certainly. ",
    "start": "3618760",
    "end": "3626099"
  },
  {
    "text": "In general, yes,\nalthough in this case, we're actually looking at this\naveraged across the data points",
    "start": "3626100",
    "end": "3634290"
  },
  {
    "text": "within a task. And so it's really if\nthe number of tasks is above the number\nof parameters.",
    "start": "3634290",
    "end": "3639720"
  },
  {
    "text": " And in practice, we usually\nhave a very large number",
    "start": "3639720",
    "end": "3645930"
  },
  {
    "text": "of parameters and a very-- not necessarily a\nsmall number of tasks but the number of\ntasks is far less",
    "start": "3645930",
    "end": "3651390"
  },
  {
    "text": "than the number of parameters. Yeah? [INAUDIBLE] doesn't necessarily\n[INAUDIBLE] the constraints",
    "start": "3651390",
    "end": "3660710"
  },
  {
    "text": "from [INAUDIBLE] on-- the main constraint\nthat we are [INAUDIBLE] doesn't guarantee\nthat [INAUDIBLE]??",
    "start": "3660710",
    "end": "3666090"
  },
  {
    "text": "You're asking, if we don't\nhave local linearity, then this doesn't imply this? Yeah.",
    "start": "3666090",
    "end": "3672090"
  },
  {
    "text": "In practice, this\ndoesn't exactly guarantee the\n[INAUDIBLE] constraint variable in that [INAUDIBLE].",
    "start": "3672090",
    "end": "3678779"
  },
  {
    "text": "So if you are locally\nlinear, then this, I believe, should imply this right here.",
    "start": "3678780",
    "end": "3685410"
  },
  {
    "text": "Basically, the\nlinearity is important because, basically,\nif your gradient points in a certain direction,\nyou want to ensure that the--",
    "start": "3685410",
    "end": "3696160"
  },
  {
    "text": "what's a good example of this? Say your loss function\nlooks like this,",
    "start": "3696160",
    "end": "3702019"
  },
  {
    "text": "which is like pretty\nheavily non-linear. Then if your gradient\nright here is",
    "start": "3702020",
    "end": "3709809"
  },
  {
    "text": "saying that it's OK\nto basically go right, because then you'll have\na smaller loss function,",
    "start": "3709810",
    "end": "3716050"
  },
  {
    "text": "and you take too big of a step,\nthen you'll actually end up--",
    "start": "3716050",
    "end": "3721080"
  },
  {
    "text": "if you go right and you\ntake too big of a step, you might actually end up here. And so that's a problem.",
    "start": "3721080",
    "end": "3726500"
  },
  {
    "text": "But if it is actually\nlocally linear, and so your step size\nis smaller than that,",
    "start": "3726500",
    "end": "3732862"
  },
  {
    "text": "then that means that you will\nactually ensure that it goes down or, at least, that it stays\nthe same if the equality holds",
    "start": "3732863",
    "end": "3741520"
  },
  {
    "text": "for the inequality. ",
    "start": "3741520",
    "end": "3748099"
  },
  {
    "text": "Cool. And you can formulate this\nas a quadratic program and solve it once you\nevaluate the gradients.",
    "start": "3748100",
    "end": "3756320"
  },
  {
    "text": " Cool. So they evaluated this approach\non a few different experiments.",
    "start": "3756320",
    "end": "3764970"
  },
  {
    "text": "They looked at three different\nlifelong learning problems. The one is a sequence of MNIST\ntasks, so digit classification,",
    "start": "3764970",
    "end": "3772980"
  },
  {
    "text": "where each task has a\ndifferent permutation of the pixels in the image.",
    "start": "3772980",
    "end": "3778260"
  },
  {
    "text": "The second task is different\nrotations of MNIST digits. And the third task is a\nCIFAR-100 image classification",
    "start": "3778260",
    "end": "3786060"
  },
  {
    "text": "task where each task is\nintroducing five new image classes. ",
    "start": "3786060",
    "end": "3792980"
  },
  {
    "text": "BWT is short for\nBackward Transfer. FWT is short for\nForward Transfer. And their total\nmemory size that they",
    "start": "3792980",
    "end": "3799340"
  },
  {
    "text": "assume that they had\navailable was 5,012 examples.",
    "start": "3799340",
    "end": "3806650"
  },
  {
    "text": "Cool. So in the left plot,\nwe can see that-- the method that we just\ndiscussed is called GEM,",
    "start": "3806650",
    "end": "3812730"
  },
  {
    "text": "or Gradient Episodic Memory-- we see that the average\naccuracy is similar to or higher",
    "start": "3812730",
    "end": "3819570"
  },
  {
    "text": "than training a\nsingle model, training independent neural networks\nfor each task from scratch,",
    "start": "3819570",
    "end": "3826140"
  },
  {
    "text": "and some of these\nother prior methods. We can also see that some\nof these prior methods have negative backward transfer,\nmeaning that they forget",
    "start": "3826140",
    "end": "3834150"
  },
  {
    "text": "the previous tasks, whereas\nthis approach is actually able to get a small amount\nof positive transfer.",
    "start": "3834150",
    "end": "3840215"
  },
  {
    "text": "And then we also\nsee that there's very little forward transfer. And then the right\nplot is actually",
    "start": "3840215",
    "end": "3845410"
  },
  {
    "text": "evaluating the\naccuracy on task one after you train on\neach additional task.",
    "start": "3845410",
    "end": "3850960"
  },
  {
    "text": "And so we see that\nthis approach is able to maintain a pretty\nhigh performance on task one,",
    "start": "3850960",
    "end": "3858280"
  },
  {
    "text": "whereas these other\nmethods, the performance starts to drop as you\nsee more and more tasks. ",
    "start": "3858280",
    "end": "3866356"
  },
  {
    "text": "We can similarly look at plots\nfor the second problem, which is these different\nMNIST rotation tasks.",
    "start": "3866356",
    "end": "3871630"
  },
  {
    "text": "And again, we see a\nsomewhat similar trend. One thing that's different\nhere is we actually see a lot more forward transfer.",
    "start": "3871630",
    "end": "3877480"
  },
  {
    "text": "And that's probably expected\nbecause it's probably pretty difficult to--",
    "start": "3877480",
    "end": "3882670"
  },
  {
    "text": "there isn't a lot shared\nbetween different permutations of the pixels, whereas if\nyou're rotating these images,",
    "start": "3882670",
    "end": "3890375"
  },
  {
    "text": "there's a lot more\nthat might be shared across the different tasks. And then for\nCIFAR-100, it's also",
    "start": "3890375",
    "end": "3897763"
  },
  {
    "text": "somewhat of a similar story. It's a little bit\nkind of noisier in terms of the performance.",
    "start": "3897763",
    "end": "3902920"
  },
  {
    "text": "But it's also able to get\nless negative backward",
    "start": "3902920",
    "end": "3908770"
  },
  {
    "text": "transfer and a higher\naccuracy than some of these other methods. ",
    "start": "3908770",
    "end": "3914525"
  },
  {
    "text": "The last thing that\nI'll point out here is if we take a\nstep back and think about these problems, these\nMNIST problems and so forth,",
    "start": "3914525",
    "end": "3927765"
  },
  {
    "text": "in practice, we can store\nall of MNIST on a hard drive, or even in RAM, in most cases.",
    "start": "3927766",
    "end": "3933410"
  },
  {
    "text": "And so these tasks are\ncertainly somewhat synthetic. And so one general thing\nto keep in mind when",
    "start": "3933410",
    "end": "3940510"
  },
  {
    "text": "working on lifelong learning\nis trying to make sure that the experimental domains\nthat you're looking at",
    "start": "3940510",
    "end": "3945910"
  },
  {
    "text": "are ones that are reflective\nof real-world problems that you ultimately care about. ",
    "start": "3945910",
    "end": "3954860"
  },
  {
    "text": "Cool. And then one other thing that\nI'll mention that I think is kind of cool--\nwe don't have time",
    "start": "3954860",
    "end": "3961422"
  },
  {
    "text": "to talk about it\nin this lecture-- but you could actually use\nmeta-learning to acquire",
    "start": "3961423",
    "end": "3967640"
  },
  {
    "text": "a learning procedure, an\nonline learning procedure, that can avoid forgetting. And so there's a\ncouple of works that",
    "start": "3967640",
    "end": "3973640"
  },
  {
    "text": "have looked into this topic and\nfairly successfully developed",
    "start": "3973640",
    "end": "3980059"
  },
  {
    "text": "update rules that don't\nhave backward transfer. And so if you're interested\nin learning more about that, you can take a look\nat these references.",
    "start": "3980060",
    "end": "3986537"
  },
  {
    "text": " Great. And then in the\nlast six minutes,",
    "start": "3986537",
    "end": "3994640"
  },
  {
    "text": "I'd like to talk about a\nslightly different variation on the online\nlearning formulation.",
    "start": "3994640",
    "end": "4000530"
  },
  {
    "text": "So so far, we looked\nat a formulation",
    "start": "4000530",
    "end": "4005690"
  },
  {
    "text": "where we're basically evaluated\non a sequence of data points as we receive it. And this problem setting\ncan make a lot of sense",
    "start": "4005690",
    "end": "4013759"
  },
  {
    "text": "in certain scenarios, especially\nif we have a stream of data. But if you do actually\nhave different tasks,",
    "start": "4013760",
    "end": "4023090"
  },
  {
    "text": "this formulation\nmay not necessarily make full sense from the\nstandpoint of the evaluation. Because when you see\na new task, you're",
    "start": "4023090",
    "end": "4030170"
  },
  {
    "text": "actually going to\nbe also evaluating its zero-shot performance. And it may actually be\nvery difficult to perform",
    "start": "4030170",
    "end": "4036980"
  },
  {
    "text": "well zero-shot on a\ncompletely new task. And in some cases, kind\nof more realistically,",
    "start": "4036980",
    "end": "4043870"
  },
  {
    "text": "you might be given\na small amount of data for each new task\nthat you're looking at. And so the picture\nmight look something",
    "start": "4043870",
    "end": "4050220"
  },
  {
    "text": "more like this, where\nyou are actually kind of learning each task\nand then being evaluated on that task rather than being\nevaluated on that task right",
    "start": "4050220",
    "end": "4057569"
  },
  {
    "text": "from the start. And what we might hope\nfor in a setting like this",
    "start": "4057570",
    "end": "4063560"
  },
  {
    "text": "would be something\nwhere the first task we're learning pretty slowly. And as we see more\nand more tasks,",
    "start": "4063560",
    "end": "4069700"
  },
  {
    "text": "we're able to learn more\nand more quickly over time. Basically, the\nthing that differs",
    "start": "4069700",
    "end": "4076170"
  },
  {
    "text": "in terms of the\nevaluation is instead of measuring performance\non every single data point that you see, you can\nevaluate the performance",
    "start": "4076170",
    "end": "4083010"
  },
  {
    "text": "only after seeing a small amount\nof data for each new task. ",
    "start": "4083010",
    "end": "4089510"
  },
  {
    "text": "And this is really primarily\na difference in the evaluation rather than the stream of data.",
    "start": "4089510",
    "end": "4095070"
  },
  {
    "text": "And so, in particular, what\nthis looks like is for each task that you see over\ntime, you observe",
    "start": "4095070",
    "end": "4102259"
  },
  {
    "text": "a small data set for that task. You use some update procedure,\nlike gradient descent or something else, to produce\nparameters for that task.",
    "start": "4102260",
    "end": "4111580"
  },
  {
    "text": "And then you observe\na data point. You're asked to make a\nprediction on that data point. And then you observe the label.",
    "start": "4111580",
    "end": "4118299"
  },
  {
    "text": "And so these last\nthree steps are identical to the standard\nonline learning setting. The thing that's\ndifferent is you're",
    "start": "4118300",
    "end": "4124660"
  },
  {
    "text": "given this initial period to\ntry to actually learn the task with a small amount of data.",
    "start": "4124660",
    "end": "4130450"
  },
  {
    "text": " And in this setting, you can\nactually create the analog",
    "start": "4130450",
    "end": "4136750"
  },
  {
    "text": "of regret from the online\nlearning setting in this online meta-learning scenario\nwhere it's exactly the same",
    "start": "4136750",
    "end": "4144500"
  },
  {
    "text": "as before, except instead\nof looking at the loss of the predictor, the predictor\nfirst gets to apply this update",
    "start": "4144500",
    "end": "4150490"
  },
  {
    "text": "procedure-- which could be one step\nof gradient descent, which could be something else--",
    "start": "4150490",
    "end": "4156068"
  },
  {
    "text": "applied to the training data\nset before it's actually evaluated on each task.",
    "start": "4156069",
    "end": "4161595"
  },
  {
    "text": "And again, the\ngoal here would be to try to get sublinear regret\nrather than linear regret.",
    "start": "4161595",
    "end": "4168289"
  },
  {
    "text": " Cool.",
    "start": "4168289",
    "end": "4173492"
  },
  {
    "text": "And you could think of this\nas the loss of the best update rule in course minus the\nloss of the best update",
    "start": "4173493",
    "end": "4180200"
  },
  {
    "text": "rule in hindsight. ",
    "start": "4180200",
    "end": "4185990"
  },
  {
    "text": "Cool. We can apply\nmeta-learning algorithms to this kind of setting.",
    "start": "4185990",
    "end": "4191359"
  },
  {
    "text": "And you can basically\ntake the same follow the leader algorithm. And instead of\nfollow the leader,",
    "start": "4191359",
    "end": "4197332"
  },
  {
    "text": "you could have something like\nfollow the meta-leader, where you store all the data\nthat you've seen so far, you meta-train on that data\nthat you've seen so far,",
    "start": "4197332",
    "end": "4204560"
  },
  {
    "text": "and then you apply the\nupdate procedure that you've meta-learned on the current\ntask, and repeat that process.",
    "start": "4204560",
    "end": "4213900"
  },
  {
    "text": "And you can basically--\nalso similar to follow the leader-- you can warm\nstart your metaparameters with the metaparameters\nfrom the previous time step.",
    "start": "4213900",
    "end": "4220970"
  },
  {
    "start": "4220970",
    "end": "4226340"
  },
  {
    "text": "If the tasks you're seeing in\nsequence are non-stationary, then it can be useful to\nuse optimization-based",
    "start": "4226340",
    "end": "4231440"
  },
  {
    "text": "meta-learners for this. Because you would still\nexpect the update procedure",
    "start": "4231440",
    "end": "4237050"
  },
  {
    "text": "to do well on tasks that are\npossibly out of distribution.",
    "start": "4237050",
    "end": "4243210"
  },
  {
    "text": "So that's also something to\nkeep in mind in this setting. We're running out of time,\nso I think I'll probably more",
    "start": "4243210",
    "end": "4250520"
  },
  {
    "text": "or less skip this. But kind of the takeaway\nhere is that if you measure the learning efficiency and\nthe learning proficiency--",
    "start": "4250520",
    "end": "4258980"
  },
  {
    "text": "so how fast it's learning and\nthe error or the performance it",
    "start": "4258980",
    "end": "4265130"
  },
  {
    "text": "has as you increase\nthe task index-- you see that, in general,\nthese algorithms are all",
    "start": "4265130",
    "end": "4272210"
  },
  {
    "text": "able to decrease the number\nof examples they need and decrease the error as\nthey see more and more tasks.",
    "start": "4272210",
    "end": "4277880"
  },
  {
    "text": "But if you use a meta-learning\nalgorithm versus some of these other algorithms, it's\nactually able to kind of better",
    "start": "4277880",
    "end": "4285830"
  },
  {
    "text": "learn more efficiently\nover time and also do better and better over time. And this suggests that some\nof the meta-learning things",
    "start": "4285830",
    "end": "4293330"
  },
  {
    "text": "that we've learned\nin this course are actually kind\nof quite well suited for this sort of online\nlearning setting.",
    "start": "4293330",
    "end": "4299060"
  },
  {
    "text": " Cool.",
    "start": "4299060",
    "end": "4304440"
  },
  {
    "text": "So the takeaways\nfrom the lecture is, first, there's lots\nof different flavors of lifelong learning.",
    "start": "4304440",
    "end": "4309570"
  },
  {
    "text": "Unfortunately, a lot of the\nwork out there puts them under this same name.",
    "start": "4309570",
    "end": "4315040"
  },
  {
    "text": "And so that means\nthat if you look up a paper on lifelong\nlearning, it might end up",
    "start": "4315040",
    "end": "4320138"
  },
  {
    "text": "being a very different problem\nsetting than a different paper that studies lifelong learning.",
    "start": "4320138",
    "end": "4325780"
  },
  {
    "text": "Defining the problem\nstatement is often one of the hardest\nparts of this. And so hopefully, the\nexercise at the beginning",
    "start": "4325780",
    "end": "4330880"
  },
  {
    "text": "got you thinking a\nlittle bit about how you might define\nproblem statements for different problems.",
    "start": "4330880",
    "end": "4337393"
  },
  {
    "text": "And lastly, you can sort\nof view meta-learning as one slice of the\nlifelong learning problem where you have some\nprevious experience,",
    "start": "4337393",
    "end": "4344410"
  },
  {
    "text": "and your goal is to very\nquickly learn something new at the current time step. And it's also a pretty\nopen area of research.",
    "start": "4344410",
    "end": "4351300"
  },
  {
    "start": "4351300",
    "end": "4356000"
  }
]