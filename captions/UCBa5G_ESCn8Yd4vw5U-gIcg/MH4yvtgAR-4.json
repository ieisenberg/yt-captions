[
  {
    "text": "Next, we are going to talk about deep learning for graphs and in particular, graph neural networks.",
    "start": "4610",
    "end": "10995"
  },
  {
    "text": "So now that we have kind of refreshed our notion of, uh, how general, uh,",
    "start": "10995",
    "end": "16020"
  },
  {
    "text": "neural networks- uh, deep neural networks work, let's now go and generalize neural networks so that they can be applicable to graphs.",
    "start": "16020",
    "end": "23160"
  },
  {
    "text": "That's what we are going to do next. So, uh, the content is that we are going to talk about local network neighborhoods.",
    "start": "23160",
    "end": "32360"
  },
  {
    "text": "And we are then going to describe aggregation strategies and define what is called, uh, computation graphs.",
    "start": "32360",
    "end": "39155"
  },
  {
    "text": "And then we are going to talk about how do we stack multiple layers of these neural networks, uh,",
    "start": "39155",
    "end": "45860"
  },
  {
    "text": "to talk about how do we describe the model parameters training- how do we fit the model,",
    "start": "45860",
    "end": "51770"
  },
  {
    "text": "and how do we se- how do we- and give a simple example for unsupervised and supervised training.",
    "start": "51770",
    "end": "57840"
  },
  {
    "text": "So this is what we are going to talk about and what we are going to learn, uh, in this part, uh, of the lecture.",
    "start": "57840",
    "end": "64390"
  },
  {
    "text": "So the setup is as follows. Um, we are going to assume we have a graph G that has a set of vertices,",
    "start": "64390",
    "end": "72830"
  },
  {
    "text": "a set of nodes. It has an adjacency matrix. Right now let's assume it's binary, so means unweighted graph.",
    "start": "72830",
    "end": "78980"
  },
  {
    "text": "Let's also, for simplicity, assume it's undirected, but, uh, everything will generalize to directed graphs as well.",
    "start": "78980",
    "end": "85985"
  },
  {
    "text": "And let's assume that every node, uh, also has a para- uh, a node feature vector X associated with it, right?",
    "start": "85985",
    "end": "94340"
  },
  {
    "text": "So, um, you know, what are node features? For example, in social network, this could be user profile,",
    "start": "94340",
    "end": "100050"
  },
  {
    "text": "user image, a user age. In biological network, it could be a gene expression profile,",
    "start": "100050",
    "end": "105229"
  },
  {
    "text": "uh, gene functional information. And, for example, if there is no, um, node feature information the- in the dataset, um,",
    "start": "105230",
    "end": "113415"
  },
  {
    "text": "what people like to do is either use indicator vectors, so one-hot encodings of nodes,",
    "start": "113415",
    "end": "118820"
  },
  {
    "text": "or just a vector of all constants, uh, value 1. Uh, those are two, uh, popular choices.",
    "start": "118820",
    "end": "125030"
  },
  {
    "text": "Sometimes people also use a degree, uh, as fe- a node degree as a feature, uh, of the node.",
    "start": "125030",
    "end": "131305"
  },
  {
    "text": "And then another piece of notation, we are going to- to denote N of v to be a set of neighbors of a given node,",
    "start": "131305",
    "end": "138920"
  },
  {
    "text": "uh, v. And that's basically the notation, the setup, uh, we are going to use,",
    "start": "138920",
    "end": "144800"
  },
  {
    "text": "uh, for this part of the lecture. Now, if you say, how could I go and apply deep neural networks to graphs?",
    "start": "144800",
    "end": "152689"
  },
  {
    "text": "Here is a naive, uh, simple approach. The idea you could have is to say, why don't I represent a network with adjacency matrix?",
    "start": "152690",
    "end": "160970"
  },
  {
    "text": "And then why don't I append the node features to the adjacency matrix? Now think of this as a training example and feed it through a deep neural network, right?",
    "start": "160970",
    "end": "171209"
  },
  {
    "text": "It seems very natural. I take my network, represent it as an adjacency matrix, I append node features to it.",
    "start": "171210",
    "end": "178195"
  },
  {
    "text": "Now, this is, you know, the input to the neural network, um, and I'm able to make predictions.",
    "start": "178195",
    "end": "183910"
  },
  {
    "text": "Uh, the issue with this idea is several. First is that the number of parameters of this neural network will be multiple times the number of nodes in the network.",
    "start": "183910",
    "end": "193130"
  },
  {
    "text": "Because number of inputs here is the number of nodes plus the number of features. And if you say how- what is the- how many training examples I have,",
    "start": "193130",
    "end": "200495"
  },
  {
    "text": "I have one training example per node. So it means you'll have more parameters than you have training examples",
    "start": "200495",
    "end": "206090"
  },
  {
    "text": "and training will be very unstable and it will easily overfit. Another issue with this is that this model",
    "start": "206090",
    "end": "212840"
  },
  {
    "text": "won't be applicable to graphs of different sizes. Because if now I have a graph that has a height of seven nodes,",
    "start": "212840",
    "end": "219670"
  },
  {
    "text": "it's unclear how to fit a graph of seven nodes into five different inputs, right?",
    "start": "219670",
    "end": "226160"
  },
  {
    "text": "Because here we have a graph of five nodes. So that's one, uh, big- uh, big issue.",
    "start": "226160",
    "end": "232895"
  },
  {
    "text": "And another big issue is subtle but very important is that this approach will be sensitive to know node ordering.",
    "start": "232895",
    "end": "239275"
  },
  {
    "text": "Meaning right now I number nodes as A, B, C, uh, D, and E in the following order so my adjacency matrix was like this.",
    "start": "239275",
    "end": "248280"
  },
  {
    "text": "But now if I were to call for example node E- to call it A, B, C, D, and E,",
    "start": "248280",
    "end": "254620"
  },
  {
    "text": "then the shape of my adjacency matrix would change, right? The nodes, uh, would be permuted.",
    "start": "254620",
    "end": "260900"
  },
  {
    "text": "The rows and columns of the matrix will be permuted even though the information is still the same.",
    "start": "260900",
    "end": "266245"
  },
  {
    "text": "And in that case, the mind would get totally confused and wouldn't know what to- what to output.",
    "start": "266245",
    "end": "272525"
  },
  {
    "text": "So the point is that in graphs there is no fixed node ordering so it's unclear how to sort the nodes of the graph that you could,",
    "start": "272525",
    "end": "280009"
  },
  {
    "text": "um, you know, put them as inputs in the matrix. That's much easier in images because you say I'll start with the top-left pixel,",
    "start": "280010",
    "end": "288245"
  },
  {
    "text": "and then I- I'll kind of go line-by-line. In- in the graph, the same graph can represented by- in- by many different adjacency matrices,",
    "start": "288245",
    "end": "297424"
  },
  {
    "text": "because it all depends on the ordering or- uh, in- in which you labeled or numbered the nodes.",
    "start": "297424",
    "end": "303935"
  },
  {
    "text": "So we have to be invariant to node ordering. So the idea of what we are going to do is we're going to kind of,",
    "start": "303935",
    "end": "312004"
  },
  {
    "text": "take- borrow some intuition from convolutional neural networks, from computer vision, but generalize them to graphs.",
    "start": "312005",
    "end": "319220"
  },
  {
    "text": "So here's a quick idea about what convolutional neural networks do. Like if you have an image here,",
    "start": "319220",
    "end": "325650"
  },
  {
    "text": "you know, represented as this grid, then you define this convolutional operator, basically, this sliding window that you are sliding over the image, right?",
    "start": "325650",
    "end": "334280"
  },
  {
    "text": "You start at the top-left. This is now a three-by-three operator. You compute something over this,",
    "start": "334280",
    "end": "339530"
  },
  {
    "text": "uh, area of the image, and then you slide the operator by, you know, some number of steps to the right and apply the same operator again.",
    "start": "339530",
    "end": "348025"
  },
  {
    "text": "And- and you can keep doing this and you can imagine that now this will give you a new image of different size, of different- uh, um, uh,",
    "start": "348025",
    "end": "355470"
  },
  {
    "text": "different number of rows and columns to which you can apply another, uh, convolutional operator,",
    "start": "355940",
    "end": "362075"
  },
  {
    "text": "another kind of sliding window type operator that kind of goes over the rows, uh, uh, of that- of that image.",
    "start": "362075",
    "end": "368340"
  },
  {
    "text": "And if you just chain this, uh, together, uh, you can then, uh, come up, uh, with convolutional neural networks and, uh, good predictions.",
    "start": "368340",
    "end": "376569"
  },
  {
    "text": "Our goal here is to generalize this notion of convolution between simple lattices,",
    "start": "376570",
    "end": "381860"
  },
  {
    "text": "between simple- uh, beyond simple matri- uh, matrices, uh, and also leverage node features and attributes.",
    "start": "381860",
    "end": "388555"
  },
  {
    "text": "Like, for example, text or images that might be attached to the nodes of the network. The issue is that our networks are much more complex.",
    "start": "388555",
    "end": "397504"
  },
  {
    "text": "So defining the node- the notion of a sliding window, let's- let's say, uh, you know, a three-by-three window,",
    "start": "397505",
    "end": "403389"
  },
  {
    "text": "it's- is- is very strange because maybe in some case, you know, the sliding window may only cover three nodes in other-",
    "start": "403390",
    "end": "409505"
  },
  {
    "text": "in other case the sliding window may cover many more nodes. And it's unclear how to define this notion of a window,",
    "start": "409505",
    "end": "416729"
  },
  {
    "text": "and then it's also unclear how to define the notion of sliding the window over the graph.",
    "start": "416730",
    "end": "421765"
  },
  {
    "text": "And this is a big, uh, complexity and a big challenge that, uh, graph neural networks have to do, uh,",
    "start": "421765",
    "end": "428660"
  },
  {
    "text": "to be able to be applied to complex network data. So the idea that makes this,",
    "start": "428660",
    "end": "434940"
  },
  {
    "text": "uh, work is the following step in intuition. So the idea is that single convolutional- single layer of a convolutional,",
    "start": "434940",
    "end": "442070"
  },
  {
    "text": "uh, neural network, basically what it does it, for example, takes- uh, takes the, uh, area of three-by-three pixels,",
    "start": "442070",
    "end": "448875"
  },
  {
    "text": "apply some transformation to them and creates a new pixel. And that's the way you can think of it, right?",
    "start": "448875",
    "end": "455389"
  },
  {
    "text": "And now we can take this operator and slide it across the image. What we'd like to do in the graph is something similar, but, you know,",
    "start": "455390",
    "end": "463760"
  },
  {
    "text": "if you wanna apply this operator in terms of a- uh, in terms of a, uh,",
    "start": "463760",
    "end": "468995"
  },
  {
    "text": "let's say like a sliding window, then we will have a center of the sliding window, which is a node, and this center is going to kind of borrow,",
    "start": "468995",
    "end": "475980"
  },
  {
    "text": "uh, aggregate information from its neighbors, right? The same way here you can imagine that this is the center of the operator and",
    "start": "475980",
    "end": "483080"
  },
  {
    "text": "it's kind of collecting information from- from its neighbors, denoted by arrows, takes its own value as well and creates a new value,",
    "start": "483080",
    "end": "492560"
  },
  {
    "text": "a new pixel, uh, for itself. So the idea is that- that what convolutional operators are certainly doing,",
    "start": "492560",
    "end": "499595"
  },
  {
    "text": "they are transforming information from the neighbors, combining it, and creating a new kind of a message.",
    "start": "499595",
    "end": "507109"
  },
  {
    "text": "So this is when today's lecture also relates to the, uh, last lecture when we talked about message passing, right?",
    "start": "507109",
    "end": "514729"
  },
  {
    "text": "So today here we can think about a node collecting information from its neighbors, collecting messages from its neighbors, aggregating them,",
    "start": "514730",
    "end": "521659"
  },
  {
    "text": "combining them, and creating a new message. So, uh, that is the- that is the idea.",
    "start": "521660",
    "end": "528410"
  },
  {
    "text": "So how graph convolutional neural networks are going to work. Um, the idea is that node's neighborhood defines the neural network architecture.",
    "start": "528410",
    "end": "538910"
  },
  {
    "text": "So basically the structure of the graph around a node of interest defines the structure of the neural network.",
    "start": "538910",
    "end": "546790"
  },
  {
    "text": "Um, so the idea is, if i wanna make a prediction for this red node, I, here, uh, in the network,",
    "start": "546790",
    "end": "553310"
  },
  {
    "text": "then the way we can think of this is that i is going to take information,",
    "start": "553310",
    "end": "558680"
  },
  {
    "text": "uh, from its neighbors and neighbors are going to take information from neighbors of neighbors.",
    "start": "558680",
    "end": "563829"
  },
  {
    "text": "And we are going to learn how to propagate this information, how to transform it along the edges of the network,",
    "start": "563830",
    "end": "569884"
  },
  {
    "text": "how to aggregate the heat, and how to create a new message that then the next node up the chain can again aggregate,",
    "start": "569885",
    "end": "577775"
  },
  {
    "text": "transform, um, and- and compute. So in some sense, the way we can think of graph neural networks is a two-step process.",
    "start": "577775",
    "end": "585845"
  },
  {
    "text": "In the first step process we determine the node computation graph. And in the second process,",
    "start": "585845",
    "end": "591500"
  },
  {
    "text": "we then propagate, um, the- the- the information, we propagate and transform it over this computation graph.",
    "start": "591500",
    "end": "599404"
  },
  {
    "text": "And this computation graph defines the architecture or the structure of the underlying,",
    "start": "599405",
    "end": "605930"
  },
  {
    "text": "uh, neural network, right? So, um, in this way, we learn how to propagate information across",
    "start": "605930",
    "end": "612079"
  },
  {
    "text": "the graph structure to compute node features or, uh, node, uh, embeddings.",
    "start": "612080",
    "end": "617615"
  },
  {
    "text": "So that's the intuition. Let me give you an example of what I mean by this.",
    "start": "617615",
    "end": "622915"
  },
  {
    "text": "Consider here a very small, um, input graph on six nodes. Um, and what we will want to do is,",
    "start": "622915",
    "end": "630250"
  },
  {
    "text": "the key idea would be that you want to generate base- node embeddings based on the local structure of the neighborhood around that target node.",
    "start": "630250",
    "end": "639130"
  },
  {
    "text": "So for this input graph and this is the target node, uh, here is the structure of the neural network that is going to",
    "start": "639130",
    "end": "646870"
  },
  {
    "text": "make computation to be able to make a prediction for node A. And let me explain you why is this neural network has this structure.",
    "start": "646870",
    "end": "654535"
  },
  {
    "text": "The reason is that node A is going to take information from its neighbors in the network; B, C,",
    "start": "654535",
    "end": "661000"
  },
  {
    "text": "and D. So here are B, C, and D. And then of course, this is kind of one layer of computation,",
    "start": "661000",
    "end": "667750"
  },
  {
    "text": "but we can unfold this for multiple layers. So if we unfold this for one more layer, then node D takes information from its neighbor A.",
    "start": "667750",
    "end": "676540"
  },
  {
    "text": "And that's why we have this edge here. Node C, for example, takes information from its neighbors A, B,",
    "start": "676540",
    "end": "683500"
  },
  {
    "text": "uh, E, and F. They are- they are here; A, B, and F. And then D takes information from A and C because it's connected to nodes,",
    "start": "683500",
    "end": "691405"
  },
  {
    "text": "uh, A and C. So now, what does this mean is that if this is the structure of the, uh, graph neural network, now what is- what we have to define,",
    "start": "691405",
    "end": "700000"
  },
  {
    "text": "what we have to learn is we have to learn the message transformation operators along the edges as well as the aggregation operator.",
    "start": "700000",
    "end": "708730"
  },
  {
    "text": "Say, because node B says, \"I will collect the message from A, I will collect the message from C. These messages will be transformed,",
    "start": "708730",
    "end": "716830"
  },
  {
    "text": "aggregated together in a single message, and then I'm going to pass it on.\"",
    "start": "716830",
    "end": "721885"
  },
  {
    "text": "So that now node A can again say, \"I'll take message from B, I will transform it.",
    "start": "721885",
    "end": "727060"
  },
  {
    "text": "I will take a message from C, transform it, message from D, transform it. Now I am going to aggregate these three messages into the new message and",
    "start": "727060",
    "end": "734710"
  },
  {
    "text": "pass it on to whoever is kind of in the layer, uh, above me.\" So that is essentially the idea.",
    "start": "734710",
    "end": "740965"
  },
  {
    "text": "And of course, these transformations here, uh, uh, aggregations and transformations will be learned.",
    "start": "740965",
    "end": "748060"
  },
  {
    "text": "They will be parameterized and distributed parameters of our model. What is interesting and fundamentally different from classical neural networks,",
    "start": "748060",
    "end": "758485"
  },
  {
    "text": "is that every node gets to define its own neural network architecture.",
    "start": "758485",
    "end": "764454"
  },
  {
    "text": "Or every node gets to define its own computation graph based on the structure of the network around it.",
    "start": "764455",
    "end": "772055"
  },
  {
    "text": "So what this means is, for example, that, uh, blue node, D here, its computation graph will be like this,",
    "start": "772055",
    "end": "778395"
  },
  {
    "text": "will very skinny because D takes information from A, and A takes it from B and C. So it's a- you know,",
    "start": "778395",
    "end": "785080"
  },
  {
    "text": "this is now a two-layer neural network, but it's very skinny, very narrow. While for example, node C has a much bigger,",
    "start": "785080",
    "end": "792730"
  },
  {
    "text": "much wider neural network, because C collects information from- from its four neighbors and",
    "start": "792730",
    "end": "798040"
  },
  {
    "text": "then each of the neighbors collect it from its own set of neighbors. So that architecture, structure of this green neural network corresponding to nodes,",
    "start": "798040",
    "end": "806725"
  },
  {
    "text": "target node C, is very different than the one from the node, uh, D. So what is interesting,",
    "start": "806725",
    "end": "814150"
  },
  {
    "text": "um, conceptually is that now, every node has its own, uh, computational graph or it has its own, um, architecture.",
    "start": "814150",
    "end": "822835"
  },
  {
    "text": "Um, some nodes, if the neighborhood of the network around them is similar, for example, these two nodes will have the same computation graphs.",
    "start": "822835",
    "end": "831699"
  },
  {
    "text": "Like this is E and F, you see the kind of the structure of these computation graphs, these neural network architecture trees, uh, is the same.",
    "start": "831700",
    "end": "839500"
  },
  {
    "text": "But in principle, every node can have its own computation graph, that's first thing. And then the second thing that's interesting,",
    "start": "839500",
    "end": "846055"
  },
  {
    "text": "now we are going to train or learn over multiple architectures simultaneously, right?",
    "start": "846055",
    "end": "851560"
  },
  {
    "text": "So it's not that we have one neural network on which we train now every node comes with its own neural network,",
    "start": "851560",
    "end": "858670"
  },
  {
    "text": "uh, architecture, neural network structure. And of course, the structure of the neural network for",
    "start": "858670",
    "end": "863830"
  },
  {
    "text": "a given node depends on the structure of the network around this node, because this is how we determine the computation graph.",
    "start": "863830",
    "end": "871615"
  },
  {
    "text": "And that's kind of a very important, er, kind of deep insight into how these things are",
    "start": "871615",
    "end": "877930"
  },
  {
    "text": "different than kind of classical, uh, deep learning. So now, how do- how does this work as we have multiple layers, right?",
    "start": "877930",
    "end": "886600"
  },
  {
    "text": "So the point is that the model can be of arbitrary depth. We can create an arbitrary number, uh, of, uh,",
    "start": "886600",
    "end": "892584"
  },
  {
    "text": "layers and nodes have embeddings at each layer. Um, and the embedding at layer 0 of a given node",
    "start": "892585",
    "end": "900175"
  },
  {
    "text": "is simply initialized as its input features X. And then the layer K embedding gets information",
    "start": "900175",
    "end": "907150"
  },
  {
    "text": "from nodes that are kind of K hops away, right? So the way this would work is, layer 0 embedding for nodes is simply their feature vectors,",
    "start": "907150",
    "end": "916210"
  },
  {
    "text": "so here, denoted by X. Then for example, embedding of, um,",
    "start": "916210",
    "end": "921639"
  },
  {
    "text": "node B at layer 1 would be sum aggregation of feature- feature vectors of, ah,",
    "start": "921640",
    "end": "927385"
  },
  {
    "text": "of neighbors, uh, A and C, uh, plus its own feature vector, and this is now embedding of this node at layer 1.",
    "start": "927385",
    "end": "935100"
  },
  {
    "text": "And then this will be passed on, uh, so that now node 2 can- node A can compute its embedding at layer 2.",
    "start": "935100",
    "end": "941824"
  },
  {
    "text": "So what it means is that the embedding of A at layer 0 is different than its embedding at layer 1- sorry, at layer 2.",
    "start": "941825",
    "end": "950214"
  },
  {
    "text": "So at every layer, a node will have a different, uh, embedding. And also, we are only going to run this for a limited number of steps.",
    "start": "950215",
    "end": "959005"
  },
  {
    "text": "We are not going to run this kind of infinitely long or until it converges as- as we did in the last lecture.",
    "start": "959005",
    "end": "965860"
  },
  {
    "text": "We don't have this notion of convergence. We'd only do this for a limited number of steps. Each step corresponds to one layer of the neural network,",
    "start": "965860",
    "end": "974110"
  },
  {
    "text": "corresponds to one hop, uh, in the underlying network. So if we want to collect information from K hops away from the starting node,",
    "start": "974110",
    "end": "982899"
  },
  {
    "text": "from the target node, we are going to need a K layer neural network. And because networks have final diameter,",
    "start": "982900",
    "end": "990430"
  },
  {
    "text": "it makes no sense to talk about, I know, 100 layer, uh, deep neural networks. Again, unless your network has diameter or that you know,",
    "start": "990430",
    "end": "999190"
  },
  {
    "text": "the longest, shortest path is of 100 hops. So now that we have, uh,",
    "start": "999190",
    "end": "1006570"
  },
  {
    "text": "defined the notion of how do we create this computation graph based on the structure of the neighborhood around a given node,",
    "start": "1006570",
    "end": "1013575"
  },
  {
    "text": "now we need to talk about these transformations that happen in the neural network.",
    "start": "1013575",
    "end": "1019320"
  },
  {
    "text": "And the key concept is neighborhood aggregation. Um, and the key distinction between different approaches,",
    "start": "1019320",
    "end": "1025709"
  },
  {
    "text": "different graph neural network architectures is how different, uh, how this aggregation, uh, is done.",
    "start": "1025710",
    "end": "1032415"
  },
  {
    "text": "How this information from, uh, children nodes is aggregated and combined,",
    "start": "1032415",
    "end": "1037679"
  },
  {
    "text": "uh, with the information or message of the pattern. One important thing to notice is because",
    "start": "1037680",
    "end": "1044100"
  },
  {
    "text": "the ordering of the nodes in a graph is arbitrary; this means that we have to have our aggregation operator to be,",
    "start": "1044100",
    "end": "1053385"
  },
  {
    "text": "um, uh, permutation invariant. Right? So it- it- it means that, um,",
    "start": "1053385",
    "end": "1059895"
  },
  {
    "text": "we- we can order the nodes in any order and if we aggregate them,",
    "start": "1059895",
    "end": "1064950"
  },
  {
    "text": "the aggregation will always be the same, right? That's the- that's the idea. It doesn't matter in what order we aggregate,",
    "start": "1064950",
    "end": "1070710"
  },
  {
    "text": "we want the- the result to be always the same because it doesn't matter whether, you know, node B has neighbors A- A and C,",
    "start": "1070710",
    "end": "1079455"
  },
  {
    "text": "or C and A, they are just- it's just a set of elements. So it doesn't matter whether we're aggregating, you know,",
    "start": "1079455",
    "end": "1085575"
  },
  {
    "text": "in that sense from A- A and C or C and A. You should always get the same result.",
    "start": "1085575",
    "end": "1091169"
  },
  {
    "text": "So neighborhood aggregation function has to be, uh, order invariant or permutation invariant.",
    "start": "1091170",
    "end": "1098055"
  },
  {
    "text": "So now, of course, the question here is- that we haven't yet answered is, what is happening in these boxes?",
    "start": "1098055",
    "end": "1104685"
  },
  {
    "text": "What do we put into these boxes? How do we define these transformations? How are they parameterized? How do we learn?",
    "start": "1104685",
    "end": "1110880"
  },
  {
    "text": "So the basic approach is, for example, is to simply average information from the neighbors,",
    "start": "1110880",
    "end": "1117090"
  },
  {
    "text": "uh, and apply a neural network. So average- so a summation, use permutation or that invariant because in any number,",
    "start": "1117090",
    "end": "1124830"
  },
  {
    "text": "you sum up the- the- the numbers, in any way you sum up the numbers, you will always get the same result.",
    "start": "1124830",
    "end": "1130664"
  },
  {
    "text": "So average or a summation is a, uh, permutation invariant aggregation function.",
    "start": "1130665",
    "end": "1136934"
  },
  {
    "text": "So for example, the idea here is that every- every of these operators here will simply take the messages from the children,",
    "start": "1136935",
    "end": "1144389"
  },
  {
    "text": "uh, and average node, and then decide to take this average and make a new message out of it.",
    "start": "1144390",
    "end": "1150570"
  },
  {
    "text": "So the way we can, um, do this is do the, er, we aggregate the messages and then we apply our neural network,",
    "start": "1150570",
    "end": "1157890"
  },
  {
    "text": "which means we apply some linear transformation followed by a non-linearity to create a next, uh, level message.",
    "start": "1157890",
    "end": "1165600"
  },
  {
    "text": "So let me, uh, give you an example. The basic approach is that we want to average messages coming from the children,",
    "start": "1165600",
    "end": "1173510"
  },
  {
    "text": "coming from the neighbors and apply a neural network to them. So here is how this- uh, how this looks like in equation. So let me explain.",
    "start": "1173510",
    "end": "1180775"
  },
  {
    "text": "So first, write H is a- our embedding and, uh, subscript means,",
    "start": "1180775",
    "end": "1186150"
  },
  {
    "text": "uh, the node and superscript denotes the neu- uh, the level of the neural network.",
    "start": "1186150",
    "end": "1191835"
  },
  {
    "text": "So at the beginning are the zeroth layer, the embedding of the node V is simply its feature representation.",
    "start": "1191835",
    "end": "1199395"
  },
  {
    "text": "And now we are going to create higher-order embeddings of nodes.",
    "start": "1199395",
    "end": "1205590"
  },
  {
    "text": "So right so this is a level zero so at level one, what are we going to do is- is getting an equation and let me explain.",
    "start": "1205590",
    "end": "1212880"
  },
  {
    "text": "So first we say, let's take the embeddings of the nodes from the previous uh,",
    "start": "1212880",
    "end": "1218595"
  },
  {
    "text": "from the previous layer. So this is the embedding of this node v from the previous layer. And let's multiply, transform it with some matrix B.",
    "start": "1218595",
    "end": "1228510"
  },
  {
    "text": "Then, what we are saying is let's also go over the neighbor's uh,",
    "start": "1228510",
    "end": "1233895"
  },
  {
    "text": "u of our node of interest v. And let's take the previous level embedding of every node u,",
    "start": "1233895",
    "end": "1242070"
  },
  {
    "text": "let's sum up these embeddings uh, and average them so this is the total number of neighbor's.",
    "start": "1242070",
    "end": "1248355"
  },
  {
    "text": "And then let's transform this uh, uh, aggregated average of embeddings of ma- of uh, children,",
    "start": "1248355",
    "end": "1256769"
  },
  {
    "text": "multiply it with another matrix and uh, send these through a non-linearity, right?",
    "start": "1256770",
    "end": "1262260"
  },
  {
    "text": "So basically we say, I have my own message- my own embedding if I'm node v, my own embedding from the previous layer, transform it.",
    "start": "1262260",
    "end": "1270225"
  },
  {
    "text": "I aggregate embeddings from my children, from my neighbor's from previous level,",
    "start": "1270225",
    "end": "1275265"
  },
  {
    "text": "I multiply that to be the different transformation matrix. I add these two together and send it through a non-linearity.",
    "start": "1275265",
    "end": "1282420"
  },
  {
    "text": "And this is a one layer of my neural network. And now of course I can run this or do this for several times, right?",
    "start": "1282420",
    "end": "1289890"
  },
  {
    "text": "So this is now how to go from level l, to level l plus 1, um, you know to- to- to compute the first layer embeddings,",
    "start": "1289890",
    "end": "1298125"
  },
  {
    "text": "I use the embeddings from level zero, which is just the node features. But now I can run this for several kinds of iterations.",
    "start": "1298125",
    "end": "1305039"
  },
  {
    "text": "Lets say several- several layers, maybe five, six, 10. And then whatever is the final um, eh, um,",
    "start": "1305040",
    "end": "1312270"
  },
  {
    "text": "hidden representation of the final layer uh, that is what I call uh, the embedding of the node, right?",
    "start": "1312270",
    "end": "1318450"
  },
  {
    "text": "So we have our total number of capital L layers and the final embedding of the node is simply h of v at the final uh,",
    "start": "1318450",
    "end": "1326640"
  },
  {
    "text": "at the final layer- level- at the final level of uh, neighborhood uh, aggregation. And this is what is called a deep encoder,",
    "start": "1326640",
    "end": "1333975"
  },
  {
    "text": "because it is encoding information from the previous layer- lay- lay- layer from a given node,",
    "start": "1333975",
    "end": "1340215"
  },
  {
    "text": "plus its neighbor's transforming it using matrices B and W, and then sending it through a non-linearity to obs- to obtain next level uh,",
    "start": "1340215",
    "end": "1349440"
  },
  {
    "text": "representation of the node. And we can basically now do this uh, for several iterations, for several uh, layers.",
    "start": "1349440",
    "end": "1356265"
  },
  {
    "text": "So how do we train the model if every node gets its own computational architecture,",
    "start": "1356265",
    "end": "1362925"
  },
  {
    "text": "and every node has its own transformation uh, parameters, basically this W and B?",
    "start": "1362925",
    "end": "1369030"
  },
  {
    "text": "The way we train the model is that we want to define what are the parameters of it? And parameters are this matrices W and B,",
    "start": "1369030",
    "end": "1377640"
  },
  {
    "text": "and they are indexed by l. So for every layer we have a different W, and for every layer we have a different uh, B.",
    "start": "1377640",
    "end": "1384540"
  },
  {
    "text": "And the idea here is that we can now uh, feed this embeddings into the loss function and we can",
    "start": "1384540",
    "end": "1390420"
  },
  {
    "text": "run stochastic gradient descent to train the rate parameters, meaning uh, B_l and uh, W_l.",
    "start": "1390420",
    "end": "1396720"
  },
  {
    "text": "Um, and these are the two parameters, right one is the weight matrix for uh, neighborhood aggregation,",
    "start": "1396720",
    "end": "1403725"
  },
  {
    "text": "and the other one is the weight matrix for transforming hidden um, hidden vector uh,",
    "start": "1403725",
    "end": "1409095"
  },
  {
    "text": "embedding of uh, of- of the node itself uh, to create then the next level uh, embedding.",
    "start": "1409095",
    "end": "1415095"
  },
  {
    "text": "So this is uh, how we- how we do this, what is important is that this weight matrices are shared across different nodes.",
    "start": "1415095",
    "end": "1424335"
  },
  {
    "text": "So what- what this means is that this l and v are not indexed by the node,",
    "start": "1424335",
    "end": "1431265"
  },
  {
    "text": "but all nodes in the network use the same transformation matrices. And this is an important detail.",
    "start": "1431265",
    "end": "1437880"
  },
  {
    "text": "Um, as I have written things out so far, I have written them out in terms of nodes aggregating from neighbor's.",
    "start": "1437880",
    "end": "1446340"
  },
  {
    "text": "But as we saw uh, earlier in graphs, many times you can also write things in the matrix form.",
    "start": "1446340",
    "end": "1453570"
  },
  {
    "text": "So let me explain you how to write things uh, in the matrix form, right? Many aggregations can be performed efficiently if you uh,",
    "start": "1453570",
    "end": "1461370"
  },
  {
    "text": "write them out in terms of matrix operations. So what you can do is you can take your um,",
    "start": "1461370",
    "end": "1467295"
  },
  {
    "text": "matrix H and simply stag the embeddings of the nodes together,",
    "start": "1467295",
    "end": "1472470"
  },
  {
    "text": "like here so every node is an embedding for a- for a- uh, given uh, node of a given layer.",
    "start": "1472470",
    "end": "1480375"
  },
  {
    "text": "So we can define this notion of a matrix H^l, and then write the way I can simply compute them basically by saying,",
    "start": "1480375",
    "end": "1490049"
  },
  {
    "text": "what is the sum o- of the embeddings- aggregation of embeddings um, of nodes u that are neighbor's of v. This is simply",
    "start": "1490050",
    "end": "1498240"
  },
  {
    "text": "taking the- taking the uh, the correct um, entry in the- in the adjacency matrix and multiplying it with the matrix uh,",
    "start": "1498240",
    "end": "1508050"
  },
  {
    "text": "H. And this way, I'm basically averaging or aggregating the embeddings coming from the neighbor' s. Um,",
    "start": "1508050",
    "end": "1514770"
  },
  {
    "text": "then I can also define this notion of a diagonal matrix, where basically this matrix is zero only on the diagonal of it.",
    "start": "1514770",
    "end": "1522255"
  },
  {
    "text": "We have uh, non-zero entries that are uh, the degrees of individual nodes, and then if you say what is the inverse of this diagonal matrix",
    "start": "1522255",
    "end": "1530520"
  },
  {
    "text": "D. It is another diagonal matrix where on the uh, edges where around the entries on the diagonal,",
    "start": "1530520",
    "end": "1537390"
  },
  {
    "text": "I have now one over the degree um- so that D- D times inverse of D is an identity matrix, right?",
    "start": "1537390",
    "end": "1545429"
  },
  {
    "text": "So now if you take this and combine it with this uh, D minus 1 then you can write the neighborhood aggregation,",
    "start": "1545430",
    "end": "1553259"
  },
  {
    "text": "basically averaging of the neighborhood embeddings simply has uh D to the minus 1.",
    "start": "1553260",
    "end": "1558540"
  },
  {
    "text": "So the inverse of D, times adjacency matrix, times the embeddings at level H,",
    "start": "1558540",
    "end": "1564210"
  },
  {
    "text": "at level l. So basically, what this means is that I can write things in terms of this summation and averaging,",
    "start": "1564210",
    "end": "1570450"
  },
  {
    "text": "or I can write it as a product of three matrices, a product of this diagonal matrix that has one over the diagonal uh,",
    "start": "1570450",
    "end": "1577260"
  },
  {
    "text": "one over the degree on the diagonal. So this corresponds to this theorem uh, A corresponds to summing over the neighbor's and H uh,",
    "start": "1577260",
    "end": "1586409"
  },
  {
    "text": "uh, superscript l are the embeddings of the nodes from the previous layer. So this means I can think of this in terms of this kind of matrix equation,",
    "start": "1586410",
    "end": "1595830"
  },
  {
    "text": "or I can write it basically as neighborhood uh, aggregation. So rewriting the update functioning matrix form then write- is- is like this, right?",
    "start": "1595830",
    "end": "1605460"
  },
  {
    "text": "It's basically take the- uh, your embedding and multiply it with B, uh, take the embeddings of the neighbor's from previous layer um,",
    "start": "1605460",
    "end": "1614310"
  },
  {
    "text": "and multiply them with W. Um, so red part of corresponds to neighborhood aggregation,",
    "start": "1614310",
    "end": "1619934"
  },
  {
    "text": "blue part corresponds to uh, to self-transformation um. And in practice what this implies is that uh,",
    "start": "1619934",
    "end": "1627585"
  },
  {
    "text": "efficient sparse matrix multiplication can be used to train these models very efficiently.",
    "start": "1627585",
    "end": "1634260"
  },
  {
    "text": "So you can basically represent everything goes matrices, and then you have matrix gradients uh,",
    "start": "1634260",
    "end": "1640365"
  },
  {
    "text": "and everything would work uh, very nicely. Now, in the last few minutes,",
    "start": "1640365",
    "end": "1645765"
  },
  {
    "text": "I wanna talk about how to train this thing. So the node embeddings z are- are a function of the input graph.",
    "start": "1645765",
    "end": "1653010"
  },
  {
    "text": "And we can train this in a supervised setting in a sense that we wanna minimize the loss, the same way as we discussed so far, right?",
    "start": "1653010",
    "end": "1659580"
  },
  {
    "text": "I wanna make a prediction based on the embedding, and I wanna minimize the discrepancy between the prediction and the truth.",
    "start": "1659580",
    "end": "1665550"
  },
  {
    "text": "Ur, where, you know, y could be, for example, an old label or a- or a scalar value. Um, or I could even apply this in an unsupervised setting,",
    "start": "1665550",
    "end": "1673740"
  },
  {
    "text": "where I would say I want, you know, the- the similar- where node labels are unavailable,",
    "start": "1673740",
    "end": "1678885"
  },
  {
    "text": "I can use the graph structure for supervision. So I could define the notion of similarity and say, you know, the dot product between the embeddings of",
    "start": "1678885",
    "end": "1685320"
  },
  {
    "text": "two nodes has to correspond to their similarity in the network. And I could use now deep encoder to come up with the embeddings of the nodes,",
    "start": "1685320",
    "end": "1693030"
  },
  {
    "text": "rather than using the shallow, uh, uh, shallow encoder, uh, but I could use the same decoder as in node to vet,",
    "start": "1693030",
    "end": "1700350"
  },
  {
    "text": "meaning the- the random walk and, um, similarity matching using the dot problem.",
    "start": "1700350",
    "end": "1706065"
  },
  {
    "text": "So that's you- I can apply this in both settings. Um, to explain how I could do,",
    "start": "1706065",
    "end": "1712695"
  },
  {
    "text": "uh, unsupervised training a bit more. So the idea would- would be that similar, uh, nodes have similar embeddings.",
    "start": "1712695",
    "end": "1719294"
  },
  {
    "text": "So the idea would be that, you know, let- let y- let y_u, v denote- be kept value one if node u and v are indeed similar,",
    "start": "1719295",
    "end": "1727875"
  },
  {
    "text": "for example, they- they code- they, uh, code according to the same random work as defined in nodes to work.",
    "start": "1727875",
    "end": "1733770"
  },
  {
    "text": "Decoder of the two embeddings, let's say, is a simple dot-product. It says embedding of one times the embedding of the other,",
    "start": "1733770",
    "end": "1740520"
  },
  {
    "text": "and you want the- the discrepancy between the similarity and the, uh, similarity in the graph versus similarity in the embedding space to be,",
    "start": "1740520",
    "end": "1749850"
  },
  {
    "text": "uh, small, so we can define this through the cross entropy. Um, and then we could again just basically run this, uh, optimization problem,",
    "start": "1749850",
    "end": "1758505"
  },
  {
    "text": "to come up with the graph neural network that makes, uh, the predictions, um,",
    "start": "1758505",
    "end": "1763799"
  },
  {
    "text": "as we- as we discussed. So, um, the way we are going to train this,",
    "start": "1763800",
    "end": "1769515"
  },
  {
    "text": "is that we are going to train this either, as I said, in this kind of unsupervised way, and of course we can also directly train this in a supervised way.",
    "start": "1769515",
    "end": "1777110"
  },
  {
    "text": "Which means that perhaps, you know, for a given node we have some, uh, we have- we have some label about a node,",
    "start": "1777110",
    "end": "1783110"
  },
  {
    "text": "maybe this is a drug-drug interaction network, and you know whether this drug is toxic, uh, or not, whether it's safe or toxic.",
    "start": "1783110",
    "end": "1789840"
  },
  {
    "text": "So we could then say, you know, given this neural network, predict at the end the label of the node.",
    "start": "1789840",
    "end": "1794880"
  },
  {
    "text": "Whether it's safe or toxic. And now, we can backpropagate based on label. So both- both are o- both are possible,",
    "start": "1794880",
    "end": "1802125"
  },
  {
    "text": "either we directly train to predict the labels, or we can train based on the network similarity,",
    "start": "1802125",
    "end": "1807959"
  },
  {
    "text": "where network similarity can be defined using random works, the same way as in, uh, node to work.",
    "start": "1807959",
    "end": "1814019"
  },
  {
    "text": "So for, uh, supervised training, uh, basically what we wanna do is,",
    "start": "1814020",
    "end": "1819030"
  },
  {
    "text": "we wanna define, uh, the loss, uh, in terms of let's say classification,",
    "start": "1819030",
    "end": "1824684"
  },
  {
    "text": "this is the cross entropy loss for a binary classification. Basically here is the prediction of the label for a- for a- uh,",
    "start": "1824685",
    "end": "1832890"
  },
  {
    "text": "for a given color whether it's toxic or not, this is whether it is truly toxic or not, um,",
    "start": "1832890",
    "end": "1838860"
  },
  {
    "text": "and then the way you can think of this is y takes value one if it's toxic, and zero if it's not.",
    "start": "1838860",
    "end": "1844065"
  },
  {
    "text": "If the true value is zero, then this term is going to survive and it's basically one minus the lock predicted- prob- uh,",
    "start": "1844065",
    "end": "1851370"
  },
  {
    "text": "one minus the predicted probability. So here we want this to be the predicted probability- to be as small as",
    "start": "1851370",
    "end": "1857309"
  },
  {
    "text": "possible so that one minus it becomes close to one because log of 1 is 0,",
    "start": "1857310",
    "end": "1862320"
  },
  {
    "text": "so that this discrepancy is small. And if the- if the, uh, class value is one,",
    "start": "1862320",
    "end": "1868110"
  },
  {
    "text": "then this term is going to survive because 1 plus 1- 1 minus 1 is 0 and this- this goes away.",
    "start": "1868110",
    "end": "1873660"
  },
  {
    "text": "So here we want this term to be as close to one as possible. Which again would say, if it's- uh,",
    "start": "1873660",
    "end": "1880605"
  },
  {
    "text": "if it's toxic, we want the probability to be high. If it's not toxic, we want the probability [NOISE] to be low- uh,",
    "start": "1880605",
    "end": "1887625"
  },
  {
    "text": "the predicted probability, uh, of it being toxic. And this is the cross, uh, entropy loss.",
    "start": "1887625",
    "end": "1893625"
  },
  {
    "text": "So this is the encoded input coming from node embeddings. Uh, these are the classification rates,",
    "start": "1893625",
    "end": "1900929"
  },
  {
    "text": "uh, for the final classification. Uh and these are the, uh, node labels, is it basically toxic, uh, or not.",
    "start": "1900930",
    "end": "1908385"
  },
  {
    "text": "Um, and I can optimize this loss function, uh, to basically come up with, uh, the- the parameters B and W that give me the embedding that then makes,",
    "start": "1908385",
    "end": "1918885"
  },
  {
    "text": "uh, good or accurate predictions. So let me just give an overview and,",
    "start": "1918885",
    "end": "1924735"
  },
  {
    "text": "uh, finish, uh, and complete the lecture. So the way we think of modern design is that given the graph,",
    "start": "1924735",
    "end": "1930404"
  },
  {
    "text": "we wanna compute the, uh, the embedding of a target node. First, we need to define the neighborhood aggregation function.",
    "start": "1930405",
    "end": "1938820"
  },
  {
    "text": "Um, uh, this is the first thing we have to do. The second thing we have to do then is to define the loss function on the embedding,",
    "start": "1938820",
    "end": "1946815"
  },
  {
    "text": "like the cross entropy loss I was just discussing, um, and then we need to train. And the way we can train the model is that we train it on a set",
    "start": "1946815",
    "end": "1954630"
  },
  {
    "text": "of nodes in a- on a batch of nodes. So we select a batch of nodes, create the computation graphs,",
    "start": "1954630",
    "end": "1960735"
  },
  {
    "text": "and this is a batch of nodes on which we train. Um, and then what is interesting is that,",
    "start": "1960735",
    "end": "1965789"
  },
  {
    "text": "after the model is trained, we can generate embeddings for any nodes, uh, as needed,",
    "start": "1965790",
    "end": "1970889"
  },
  {
    "text": "and simply apply [NOISE] our model to them by basically just doing the forward pass.",
    "start": "1970890",
    "end": "1976170"
  },
  {
    "text": "So this means that we can apply our model, even to nodes that we have never seen during training.",
    "start": "1976170",
    "end": "1981945"
  },
  {
    "text": "So this means that, uh, we cannot train on one graph and transfer the model to",
    "start": "1981945",
    "end": "1986970"
  },
  {
    "text": "the other graph because this particular set of, um, nodes could be used for training the parameters [NOISE] to be optimized here,",
    "start": "1986970",
    "end": "1994260"
  },
  {
    "text": "and then we could apply this to a new set of nodes, to a new set of, uh, computation graphs.",
    "start": "1994260",
    "end": "1999390"
  },
  {
    "text": "So this means that our model has inductive, uh, capability, which means that same aggregation parameters,",
    "start": "1999390",
    "end": "2006860"
  },
  {
    "text": "same W and B, are shared across all nodes. Uh, and the number of model parameters in",
    "start": "2006860",
    "end": "2012529"
  },
  {
    "text": "this case is sublinear with the size of the network, because W and B only depend on the embedding dimensionality and the size,",
    "start": "2012530",
    "end": "2019549"
  },
  {
    "text": "number of features, and not on the size of the graph. And this means that graph neural networks are able to generalize to unseen nodes.",
    "start": "2019550",
    "end": "2027695"
  },
  {
    "text": "And that's a super cool, uh, feature of them, because for example, this means that you can train your graph neural network on one graph,",
    "start": "2027695",
    "end": "2034670"
  },
  {
    "text": "and you can apply it to a new graph. Because you determine matrices [NOISE] W and B here,",
    "start": "2034670",
    "end": "2039755"
  },
  {
    "text": "and then you can transfer this, uh, to the new network. You can, uh, for example,",
    "start": "2039755",
    "end": "2045470"
  },
  {
    "text": "train on one organism, and transfer this to the new organism here. This is, let's say, a biological network.",
    "start": "2045470",
    "end": "2050720"
  },
  {
    "text": "This is also very useful for the networks that constantly evolve because you can take the snapshot of the network,",
    "start": "2050720",
    "end": "2057200"
  },
  {
    "text": "create the computation graphs here, and determined the parameters so that when then in production,",
    "start": "2057200",
    "end": "2064460"
  },
  {
    "text": "and you know, derives, you quickly create a computation graph for it. Just do the forward pass,",
    "start": "2064460",
    "end": "2070024"
  },
  {
    "text": "and here you have the embedding for it. So no- no retraining the model, um, is necessary.",
    "start": "2070025",
    "end": "2076565"
  },
  {
    "text": "And that is super cool because it means you can train- train on one graph, transfer to another one,",
    "start": "2076565",
    "end": "2082010"
  },
  {
    "text": "train on a small graph, transfer the model to a big, uh, graph, or to an evolving graph.",
    "start": "2082010",
    "end": "2087395"
  },
  {
    "text": "So let me summarize, uh, the lecture for today and finish here. What we did is, we generated node embeddings by",
    "start": "2087395",
    "end": "2094175"
  },
  {
    "text": "aggregating no- node neighborhood information. We saw our first basic variant of this idea that",
    "start": "2094175",
    "end": "2100340"
  },
  {
    "text": "simply averages up the messages coming from the neighbors. Um, and the key distinction between different architectures as we are going to see next,",
    "start": "2100340",
    "end": "2108635"
  },
  {
    "text": "is that- is how this aggregation process is being done. And, uh, what I'm going to discuss in",
    "start": "2108635",
    "end": "2115430"
  },
  {
    "text": "the next lecture will be the architecture called, um, um, GraphSAGE that generalizes this",
    "start": "2115430",
    "end": "2122390"
  },
  {
    "text": "into basically a framework where different types of aggregations, different kas- kinds of transformations, uh, can be used.",
    "start": "2122390",
    "end": "2129155"
  },
  {
    "text": "So, um, thank you very much, uh, for the lecture, um, and very happy, uh, to take questions now.",
    "start": "2129155",
    "end": "2136140"
  }
]