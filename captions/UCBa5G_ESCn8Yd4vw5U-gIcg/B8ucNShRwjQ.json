[
  {
    "start": "0",
    "end": "5480"
  },
  {
    "text": "OK. Let's get started. Welcome back. Hope you are\nenjoying homework 2.",
    "start": "5480",
    "end": "11495"
  },
  {
    "start": "11495",
    "end": "17940"
  },
  {
    "text": "So today, very exciting lecture,\nthis one and the next one, at least for me.",
    "start": "17940",
    "end": "23010"
  },
  {
    "text": "So we'll talk about two things. We'll continue the\ndiscussion from last time, put it into a more\npractical direction,",
    "start": "23010",
    "end": "30029"
  },
  {
    "text": "talk about achieving the entropy\nrate for Markov processes, stationary processes, and so on.",
    "start": "30030",
    "end": "37200"
  },
  {
    "text": "And we'll dive into\ncompressing text data. And then towards the\nend of today's lecture,",
    "start": "37200",
    "end": "44520"
  },
  {
    "text": "depending on how\nmuch time is left, I will start with\nLZ77, which is really",
    "start": "44520",
    "end": "51180"
  },
  {
    "text": "the most common compression\ntechnique you will encounter. And we will continue with\nthat in the next lecture.",
    "start": "51180",
    "end": "56760"
  },
  {
    "text": " So quick recap, we talked\nabout a lot of things.",
    "start": "56760",
    "end": "63870"
  },
  {
    "text": "So this is just a summary of\nwhat you will need really today. We talked about Markov chains. So if you remember Markov chains\nor Markov processes for a given",
    "start": "63870",
    "end": "72990"
  },
  {
    "text": "k, a k-th order\nMarkov process means that you need the previous\nk symbols to predict",
    "start": "72990",
    "end": "79170"
  },
  {
    "text": "the next symbol but no more. So looking at further\npast doesn't help you.",
    "start": "79170",
    "end": "84750"
  },
  {
    "text": "And in particular, a first\norder Markov processes, the current step depends\non the past step.",
    "start": "84750",
    "end": "91800"
  },
  {
    "text": "But given the past step, it\ndoesn't depend on anything before that.",
    "start": "91800",
    "end": "97680"
  },
  {
    "text": "Stationary processes are\nlike a generalization. They are any process which is\ntime stationary, time invariant,",
    "start": "97680",
    "end": "103290"
  },
  {
    "text": "which means that the statistics\ndon't change with time. We mostly will talk about\nstationary processes",
    "start": "103290",
    "end": "109950"
  },
  {
    "text": "only in the context of\ncertain theoretical results. But in practice, I\nguess, just think",
    "start": "109950",
    "end": "115870"
  },
  {
    "text": "of a process given the past,\nyou can predict something about the future. So that's the process in a way.",
    "start": "115870",
    "end": "124210"
  },
  {
    "text": "We talked about\nconditional entropy, which",
    "start": "124210",
    "end": "130000"
  },
  {
    "text": "answers the question, you\nhave two random variables u and v. And given v,\nhow much uncertainty",
    "start": "130000",
    "end": "137020"
  },
  {
    "text": "is left in u on average? And then we talked about\nthis specific one, u given",
    "start": "137020",
    "end": "143890"
  },
  {
    "text": "v equal to v, which\nis given you know that your random\nvariable v is equal to v,",
    "start": "143890",
    "end": "149769"
  },
  {
    "text": "how much uncertainty\nis left in u. Or you can talk in\nterms of bits used.",
    "start": "149770",
    "end": "156370"
  },
  {
    "text": "So this one says if I\nknow v, how many bits do I need to compress u?",
    "start": "156370",
    "end": "161440"
  },
  {
    "text": "And this one says if I know v\nequal to v, then how many bits do I need to compress u. ",
    "start": "161440",
    "end": "169645"
  },
  {
    "text": "And when we looked\nat a few properties. We looked at the chain\nrule, if you remember where we had the\njoint entropy of u",
    "start": "169645",
    "end": "177900"
  },
  {
    "text": "and v is simply the entropy of\nv less the entropy of u given",
    "start": "177900",
    "end": "183790"
  },
  {
    "text": "v. We looked at the inequality\nthat conditioning reduces",
    "start": "183790",
    "end": "190150"
  },
  {
    "text": "entropy. So just some results\nwe saw, which you will use in your homework.",
    "start": "190150",
    "end": "196670"
  },
  {
    "text": "And then we talked\nabout the limit. So you have a\nstationary process. And what you can do is, you\ntake your n plus 1 symbol,",
    "start": "196670",
    "end": "204200"
  },
  {
    "text": "you condition it\non its entire past, and then you keep increasing n.",
    "start": "204200",
    "end": "209420"
  },
  {
    "text": "So in the limit\nwhat will happen is you are trying to capture\ngiven I know the entire past,",
    "start": "209420",
    "end": "214670"
  },
  {
    "text": "is there still some uncertainty\nleft in the next symbol? Is each new symbol\nadding some entropy,",
    "start": "214670",
    "end": "220340"
  },
  {
    "text": "some randomness, some bits? And the limit is called\nthe entropy rate.",
    "start": "220340",
    "end": "225620"
  },
  {
    "text": "Another equivalent way\nof expressing it is you think in terms of a block.",
    "start": "225620",
    "end": "231530"
  },
  {
    "text": "So you, say, if I take\nthe first n symbols and I take the average entropy\nand then you keep increasing n",
    "start": "231530",
    "end": "238520"
  },
  {
    "text": "and then again you\ntake the limit.  And obviously if you\nhave done limits,",
    "start": "238520",
    "end": "244830"
  },
  {
    "text": "you will know that this will be\nat the end of the day dominated by the large terms.",
    "start": "244830",
    "end": "250319"
  },
  {
    "text": "The first few\nterms don't matter. So it's not very hard to prove\nthat these two are equal.",
    "start": "250320",
    "end": "257278"
  },
  {
    "text": "You just need to use\nstationarity and some properties about limits that you might\nhave seen in calculus.",
    "start": "257279",
    "end": "265280"
  },
  {
    "text": "But you don't need\nthat for this class. So with that in\nmind, let's quickly do the homework\nwhich will refresh",
    "start": "265280",
    "end": "272479"
  },
  {
    "text": "our concepts about conditional\nentropy, entropy rate, and so on. And then we'll move on.",
    "start": "272480",
    "end": "278780"
  },
  {
    "text": " So we are given a Markov\nchain in this question.",
    "start": "278780",
    "end": "285129"
  },
  {
    "text": "We don't say that it is a\nstationary Markov chain for now. So how is it described? We say U1 is Bernoulli 1/2.",
    "start": "285130",
    "end": "291895"
  },
  {
    "text": " And let me draw it in this\ndiagram way, which is easier,",
    "start": "291895",
    "end": "298220"
  },
  {
    "text": "I think, to understand. So if U1 is 0, we say\nthat U2 has to be 1.",
    "start": "298220",
    "end": "306405"
  },
  {
    "text": " Is that clear?",
    "start": "306405",
    "end": "311419"
  },
  {
    "text": "Basically, we are\nsaying that if U1 is 0, then there is no chance\nthat U2 is also 0",
    "start": "311420",
    "end": "316460"
  },
  {
    "text": "because this term is 0. And if U1 is 1,\nthen the next symbol",
    "start": "316460",
    "end": "324020"
  },
  {
    "text": "is either 1 or 0 with equal\nprobability, this sort of thing. This is 0.5.",
    "start": "324020",
    "end": "329360"
  },
  {
    "text": "This is 0.5. So the first question we\nask is, what is H of U1?",
    "start": "329360",
    "end": "335750"
  },
  {
    "text": "Anyone wants to guess\nthe entropy of U1.",
    "start": "335750",
    "end": "342520"
  },
  {
    "text": "This doesn't even\nneed the classroom. I told you U1 is Bernoulli 1/2.",
    "start": "342520",
    "end": "347830"
  },
  {
    "text": "So the entropy of\nU1 is just 1 bit because it's a Bernoulli\n1/2 random variable.",
    "start": "347830",
    "end": "353230"
  },
  {
    "text": " That's good. ",
    "start": "353230",
    "end": "366210"
  },
  {
    "text": "Now I ask you, what\nis entropy of U2? There was a question on ed I\nsaw where U2 is dependent on U1.",
    "start": "366210",
    "end": "373650"
  },
  {
    "text": "So what do you mean\nby entropy of U2? At the end of the\nday, all of these are random variables, whether or\nnot they depend on anybody else.",
    "start": "373650",
    "end": "381419"
  },
  {
    "text": "So any random variable\nyou can compute its marginal distribution,\nyou can compute its entropy.",
    "start": "381420",
    "end": "386639"
  },
  {
    "text": "Just follow the formulas. So what is, for example,\nwhat is P U2 equal to 0?",
    "start": "386640",
    "end": "395790"
  },
  {
    "text": "So we did this last time, so\nI won't spend too much time unless anybody has questions.",
    "start": "395790",
    "end": "401199"
  },
  {
    "text": "So you can divide\nit into two cases. You do either U1\nwas 0 or U1 was 1.",
    "start": "401200",
    "end": "408290"
  },
  {
    "text": "So you can just split\nit into two cases. ",
    "start": "408290",
    "end": "413520"
  },
  {
    "text": "And then you can\nsubstitute the things. ",
    "start": "413520",
    "end": "419300"
  },
  {
    "text": "So 1/2 times 0, 1/2 times 1. So I guess this is half. ",
    "start": "419300",
    "end": "426320"
  },
  {
    "text": "No.  So U1 is 1/2 2/2.",
    "start": "426320",
    "end": "434500"
  },
  {
    "text": "All right. This term is 0.5. So this is 1/2. This is 1/2. This is 1/2.",
    "start": "434500",
    "end": "440190"
  },
  {
    "text": "This is 0. So I guess you get 1 by 4. Is that what people got? OK. Good.",
    "start": "440190",
    "end": "446100"
  },
  {
    "text": "And then you can compute P\nU2 equal, which is just 1 minus this thing. So 3 by 4.",
    "start": "446100",
    "end": "451920"
  },
  {
    "text": "So you can compute the entropy. This is just the entropy of a\nBernoulli 1/4 random variable. ",
    "start": "451920",
    "end": "460390"
  },
  {
    "text": "Anybody remember the answer? ",
    "start": "460390",
    "end": "468060"
  },
  {
    "text": "OK. I will-- 0.8. 0.81. OK.",
    "start": "468060",
    "end": "473320"
  },
  {
    "text": "Thank you. Yes. Yes. So you have done\nthis a few times now.",
    "start": "473320",
    "end": "479770"
  },
  {
    "text": "Now is this process stationary? ",
    "start": "479770",
    "end": "489025"
  },
  {
    "text": "Anyone?  I thought it was because\nit's a Markov chain,",
    "start": "489025",
    "end": "498310"
  },
  {
    "text": "and so I guess the\nnumber is stationary",
    "start": "498310",
    "end": "504800"
  },
  {
    "text": "So is a Markov chain\nalways stationary? That's the question. Not necessarily. That's why many times\nin the lectures,",
    "start": "504800",
    "end": "510020"
  },
  {
    "text": "we specifically say a\nstationary Markov chain. In any case, in this\nparticular example, you see,",
    "start": "510020",
    "end": "516320"
  },
  {
    "text": "we said that stationarity is\na very, very vast property. All statistics stay\nconstant with time.",
    "start": "516320",
    "end": "522950"
  },
  {
    "text": "But in particular, the entropy\nshould stay constant with time. And you saw that\nthe entropy changed. Entropy of U1 was 1 bit.",
    "start": "522950",
    "end": "528720"
  },
  {
    "text": "Entropy of U2 was 0.81 bits. So there is no way\nthis can be stationary.",
    "start": "528720",
    "end": "533750"
  },
  {
    "text": "A Markov chain is\nstationary if you start it at a stationary distribution,\nwhich was, I think, question 4.",
    "start": "533750",
    "end": "541339"
  },
  {
    "text": "So this is no because this was\nactually because H U2 is not",
    "start": "541340",
    "end": "547790"
  },
  {
    "text": "equal to H U1. Is that clear that\nthis is not stationary? This was a-- don't\nworry too much about it,",
    "start": "547790",
    "end": "556110"
  },
  {
    "text": "but just to make sure the\nconcepts we are all on board.",
    "start": "556110",
    "end": "562250"
  },
  {
    "text": "Then we asked what\nis H U2 given U1? So let's do this quickly.",
    "start": "562250",
    "end": "567470"
  },
  {
    "text": "U1 equal to 0. Entropy U2 given U1 equal to 0.",
    "start": "567470",
    "end": "574700"
  },
  {
    "text": "I think we did an\nexample last lecture, so hopefully, we all remember. So this guy, if U1 equal\nto 0, U2 is deterministic.",
    "start": "574700",
    "end": "586410"
  },
  {
    "text": "If U1 is 0, U2 is deterministic. ",
    "start": "586410",
    "end": "593130"
  },
  {
    "text": "And this one is 1 because if U1\nis 1 then U2 is Bernoulli 1/2.",
    "start": "593130",
    "end": "602330"
  },
  {
    "text": "So after you do the\ncomputation, you will see that these two\nare 1/2, 1/2, obviously,",
    "start": "602330",
    "end": "607529"
  },
  {
    "text": "so this will be 1/2. ",
    "start": "607530",
    "end": "613269"
  },
  {
    "text": "If by any chance you didn't get\nthese, look at the last lecture,",
    "start": "613270",
    "end": "618490"
  },
  {
    "text": "we did an example, and just\nrefresh your probability textbook. And I think that's just\nfollowing the probability",
    "start": "618490",
    "end": "626225"
  },
  {
    "text": "formulas.  And now what we\ndid was we changed",
    "start": "626225",
    "end": "631480"
  },
  {
    "text": "the initial distribution,\nwhich does make it stationary. We didn't ask you to\nprove it's stationary.",
    "start": "631480",
    "end": "636490"
  },
  {
    "text": "You can trust us on that. And we said now you\ncompute the entropy rate.",
    "start": "636490",
    "end": "641680"
  },
  {
    "text": "And in the last lecture,\nwe saw a few things. We saw that for IID, the\nentropy rate was just",
    "start": "641680",
    "end": "649410"
  },
  {
    "text": "the entropy of the first one. For k-th order Markov,\nthe entropy rate",
    "start": "649410",
    "end": "659320"
  },
  {
    "text": "was just the entropy of Uk plus\n1 given all the past symbols.",
    "start": "659320",
    "end": "665560"
  },
  {
    "text": "So in particular for\na first order Markov, this is just going\nto be U2 given U1.",
    "start": "665560",
    "end": "671800"
  },
  {
    "text": "The entropy of U2\ngiven the past symbol. And now we can compute it\nusing the exact same formula.",
    "start": "671800",
    "end": "680200"
  },
  {
    "text": "So let me try to\ncopy-paste this one. ",
    "start": "680200",
    "end": "693830"
  },
  {
    "text": "So again, these stay the same. And now the initial\nprobability is change.",
    "start": "693830",
    "end": "701600"
  },
  {
    "text": "So I think you will get\n2 by 3 as the answer. ",
    "start": "701600",
    "end": "708990"
  },
  {
    "text": "Did people get to 2 by 3? Yep. I'm sorry. Are you asking a question or--",
    "start": "708990",
    "end": "715345"
  },
  {
    "text": "No. Good. ",
    "start": "715345",
    "end": "723380"
  },
  {
    "text": "So that's it. Yeah. Just make yourself\na bit comfortable. This is not just this class.",
    "start": "723380",
    "end": "729258"
  },
  {
    "text": "Generally, I think this\nconditional probabilities you will come across\nin many, many-- if you do machine\nlearning, for example,",
    "start": "729258",
    "end": "735240"
  },
  {
    "text": "or any sort of statistics\nor probability class. ",
    "start": "735240",
    "end": "740959"
  },
  {
    "text": "So moving on to the material,\nany questions so far",
    "start": "740960",
    "end": "746370"
  },
  {
    "text": "on the quiz on last lecture? So the question is, how do\nyou achieve the entropy rate?",
    "start": "746370",
    "end": "753149"
  },
  {
    "text": "And you will realize\nit's very, very simple.",
    "start": "753150",
    "end": "758350"
  },
  {
    "text": "So we will first start\nwith a very simple case, the first order Markov source. So what is the first\norder Markov source again?",
    "start": "758350",
    "end": "765410"
  },
  {
    "text": "The next symbol depends\non the previous symbol. That's it. So now if you recall\nthe entropy rate",
    "start": "765410",
    "end": "775450"
  },
  {
    "text": "was defined in\ntwo different ways when we looked at last time. There is this definition.",
    "start": "775450",
    "end": "781000"
  },
  {
    "text": "And then there is\nthis definition. And for a first\norder Markov source, the second one reduces to this.",
    "start": "781000",
    "end": "787480"
  },
  {
    "text": "So there are basically\ntwo definitions, this one. The first one says that we take\nthe joint entropy of n symbols",
    "start": "787480",
    "end": "795339"
  },
  {
    "text": "at a time and then\nwe divide it by n. So this just suggests\nyou do block coding. You just take n symbols at\na time, code them together,",
    "start": "795340",
    "end": "803800"
  },
  {
    "text": "and as you take bigger\nand bigger blocks, you are going to achieve\nthe entropy rate just by definition of this thing.",
    "start": "803800",
    "end": "809613"
  },
  {
    "text": "And we know how to achieve\nthe entropy of this thing by Huffman coding or\narithmetic, any of these codes.",
    "start": "809613",
    "end": "817620"
  },
  {
    "text": "The other solution\nis more interesting, and that's the one we'll\ndo most of the time today, is more incremental\napproach where you actually",
    "start": "817620",
    "end": "824490"
  },
  {
    "text": "try to do this conditionally. You take U1, you try\nto encode U2 given U1.",
    "start": "824490",
    "end": "829529"
  },
  {
    "text": "And then you try to encode\nU3 given U2 and so on. ",
    "start": "829530",
    "end": "839320"
  },
  {
    "text": "So the first one, which\nis the block coding is, you can just do Huffman\non blocks of length n",
    "start": "839320",
    "end": "844959"
  },
  {
    "text": "and then you make\nn bigger and bigger and it will achieve\nthe entropy rate. But it doesn't work very well.",
    "start": "844960",
    "end": "853023"
  },
  {
    "text": "First of all, you can't\nmake the block size too big, the complexity issues we\ntalked about last time.",
    "start": "853023",
    "end": "858260"
  },
  {
    "text": "And there is another\nissue, which hopefully you can understand, which is that\nwhen you had your usual IID",
    "start": "858260",
    "end": "866510"
  },
  {
    "text": "symbols, so you had U1,\nU2, U3, U4, and let's say you were coding in blocks.",
    "start": "866510",
    "end": "872480"
  },
  {
    "text": "You took U1 and U2 together. You took U3 and U4 together.",
    "start": "872480",
    "end": "877630"
  },
  {
    "text": "This was suboptimal because\nthe Huffman code is only optimal for a dyadic source. If you have symbols whose\nprobability is not a power of 2,",
    "start": "877630",
    "end": "884649"
  },
  {
    "text": "you don't get the perfect\ncode length and so on. It's not optimal. But here there is\nanother reason why",
    "start": "884650",
    "end": "891340"
  },
  {
    "text": "it's not optimal is that\nU2 and U3 are correlated.",
    "start": "891340",
    "end": "896440"
  },
  {
    "text": "There is some correlation\nbetween U2 and U3. So if you're encoding\nthem in blocks separately you just lose out\non the correlation",
    "start": "896440",
    "end": "903550"
  },
  {
    "text": "between consecutive symbols. So with that in mind,\ndo this exercise at home",
    "start": "903550",
    "end": "912000"
  },
  {
    "text": "just to convince\nyourself of these things. But let's not do it now.",
    "start": "912000",
    "end": "917280"
  },
  {
    "text": "Anyway, this technique is not\nsomething we are going to use. This is the one we\nare going to use.",
    "start": "917280",
    "end": "922940"
  },
  {
    "text": "So if everybody\nwould pay attention, let's recap arithmetic coding. This will be crucial for\nthe rest of this lecture.",
    "start": "922940",
    "end": "930440"
  },
  {
    "text": "This is all you need to\nknow about arithmetic coding in this lecture and just\nin generally, I think,",
    "start": "930440",
    "end": "936440"
  },
  {
    "text": "unless you're actually\ngoing to implement it. So arithmetic coding. You work with intervals.",
    "start": "936440",
    "end": "942440"
  },
  {
    "text": "You divide the intervals\ninto your symbols. And at every step, you split\nthe sub-interval corresponding",
    "start": "942440",
    "end": "948260"
  },
  {
    "text": "to your symbol. So if you have IID model, the\nthing we have already studied,",
    "start": "948260",
    "end": "955460"
  },
  {
    "text": "and you have 0, 1, 2, if\nthe first symbol is 2, you take the interval\ncorresponding to 2.",
    "start": "955460",
    "end": "961700"
  },
  {
    "text": "You split it into\nthree parts, 0, 1, 2, which are proportional\nto their probabilities.",
    "start": "961700",
    "end": "968390"
  },
  {
    "text": "And if the next\nsymbol is now 0, you take the interval\ncorresponding to 0, and you again split it into\nthree parts corresponding",
    "start": "968390",
    "end": "975140"
  },
  {
    "text": "to 0, 1, 2. Both sizes are\nagain proportional to their probabilities. And at the end of\nthis, very easy to see",
    "start": "975140",
    "end": "982200"
  },
  {
    "text": "that the length of the\ninterval that you get at the end of the\nencoding is simply the product of\nthe probabilities.",
    "start": "982200",
    "end": "990840"
  },
  {
    "text": "And the way we did\nthe encoding in bits was that the code length\nwas roughly logarithm of 1",
    "start": "990840",
    "end": "996060"
  },
  {
    "text": "by interval length, which was\nroughly this log 1 by Px1 Px2 Pxn sort of thing.",
    "start": "996060",
    "end": "1004060"
  },
  {
    "text": "Let me pause for a minute,\nmake sure everybody understands this because\nwe'll deal with this",
    "start": "1004060",
    "end": "1009372"
  },
  {
    "text": "throughout the lecture.  All right. This is arithmetic coding\nin a single slide basically.",
    "start": "1009372",
    "end": "1016790"
  },
  {
    "start": "1016790",
    "end": "1027359"
  },
  {
    "text": "That expected comeback. That's good. All [INAUDIBLE] Yes.",
    "start": "1027359",
    "end": "1032530"
  },
  {
    "text": "Yes. Yes. Yes. So this thing is for\nthe n block basically,",
    "start": "1032530",
    "end": "1039699"
  },
  {
    "text": "yeah, the entire block. ",
    "start": "1039700",
    "end": "1046030"
  },
  {
    "text": "And if you recall\nour thumb rule, we said that for any\nsymbol x, we want to pay basically log 1 by Px.",
    "start": "1046030",
    "end": "1052780"
  },
  {
    "text": "And you see that's exactly what\narithmetic coding is paying. It's paying exactly log\n1 over P for each symbol.",
    "start": "1052780",
    "end": "1058615"
  },
  {
    "text": " So if we are happy with this,\nand I hope we are, we will--",
    "start": "1058615",
    "end": "1066415"
  },
  {
    "start": "1066415",
    "end": "1096590"
  },
  {
    "text": "so we'll get this\nMarkov chain where every symbol can go\nto either itself or 1 plus that symbol modulo 3.",
    "start": "1096590",
    "end": "1105500"
  },
  {
    "text": "Look at this figure. We change something\nin the previous slide. We change the arithmetic\ncoding how it is done.",
    "start": "1105500",
    "end": "1112970"
  },
  {
    "text": "I will wait and ask you to tell\nme what did we do different now. ",
    "start": "1112970",
    "end": "1120550"
  },
  {
    "text": "So we are encoding 2, 0,\n1 is what we are encoding. ",
    "start": "1120550",
    "end": "1129169"
  },
  {
    "text": "Let's start first. Just for encoding 2, did\nwe do anything different when we are encoding 2 compared\nto the good old arithmetic",
    "start": "1129170",
    "end": "1135760"
  },
  {
    "text": "coding? I can answer that one. No, we did not\nbecause we just took--",
    "start": "1135760",
    "end": "1142630"
  },
  {
    "text": "we assume the initial thing\nis uniformly distributed, and we just made three\nequal length intervals.",
    "start": "1142630",
    "end": "1148660"
  },
  {
    "text": "For the next one, anybody wants\nto tell me, what did we do? What is different\nfrom the previous one?",
    "start": "1148660",
    "end": "1155410"
  },
  {
    "text": "Yep. Basically, I think because\n2 can never go to 1.",
    "start": "1155410",
    "end": "1161940"
  },
  {
    "text": "So we didn't have 1\nand then the next one,",
    "start": "1161940",
    "end": "1167789"
  },
  {
    "text": "the two in that case was gone\nbecause you cannot get to 2.",
    "start": "1167790",
    "end": "1173490"
  },
  {
    "text": "Yeah. Yeah. Yeah. Very good. Yes, so what did we change? What we changed was if you know\nthat the first symbol is 2,",
    "start": "1173490",
    "end": "1180030"
  },
  {
    "text": "now your probability\nmodel tells you that definitely the\nsecond symbol is either 0 or 2 with 1/2 1/2 probability.",
    "start": "1180030",
    "end": "1185850"
  },
  {
    "text": "So rather than just following\nblindly the old distribution,",
    "start": "1185850",
    "end": "1190919"
  },
  {
    "text": "you used a different\ndistribution here. You split the interval according\nto the conditional distribution.",
    "start": "1190920",
    "end": "1197279"
  },
  {
    "text": "You split the interval\naccording by using the knowledge that the previous symbol is 2. And similarly for the next\none, you use the knowledge",
    "start": "1197280",
    "end": "1204539"
  },
  {
    "text": "that the previous symbol was 0. So therefore, the next\nsymbol can be either 0 or 1. So it just made this\nslight simple modification",
    "start": "1204540",
    "end": "1211049"
  },
  {
    "text": "in arithmetic coding,\nwhich we will see is basically the thing to do.",
    "start": "1211050",
    "end": "1216810"
  },
  {
    "text": " One thing, I think, there is a\nquestion in the homework where",
    "start": "1216810",
    "end": "1225580"
  },
  {
    "text": "you will see a\nsimilar thing, the one",
    "start": "1225580",
    "end": "1231130"
  },
  {
    "text": "where you generate\nrandom variables. So just a hint for that. ",
    "start": "1231130",
    "end": "1238169"
  },
  {
    "text": "So if we are happy with that-- please let me know if you\nare not happy with that.",
    "start": "1238170",
    "end": "1243870"
  },
  {
    "text": "If you are not just\ntrust it's fine. You can follow the rest of\nthe lecture, nothing breaks.",
    "start": "1243870",
    "end": "1249630"
  },
  {
    "text": " So we just change the model. At every step instead of\nsplitting the interval",
    "start": "1249630",
    "end": "1255780"
  },
  {
    "text": "by just the marginal\nprobability, we split the interval according\nto the conditional probability",
    "start": "1255780",
    "end": "1261059"
  },
  {
    "text": "given the past. And in this case because it's\na first order Markov chain, you just need to look\nat the last symbol.",
    "start": "1261060",
    "end": "1266878"
  },
  {
    "text": "But in general, you can\nlook at the entire past. So whatever knowledge you\nhave about the distribution,",
    "start": "1266878",
    "end": "1273180"
  },
  {
    "text": "you have seen the entire past. The next time you\nsplit the intervals, you use that knowledge. ",
    "start": "1273180",
    "end": "1281170"
  },
  {
    "text": "And then you can do\nthe math, which I won't spend too much time on doing. But what you will get\nat the end of the day",
    "start": "1281170",
    "end": "1286570"
  },
  {
    "text": "is that the expected\nbits per symbol you get is actually equal to the\nconditional entropy which is the entropy rate in this case.",
    "start": "1286570",
    "end": "1292570"
  },
  {
    "text": " Maybe I'll spend a little time. So you remember that the length\nof the interval for arithmetic",
    "start": "1292570",
    "end": "1300650"
  },
  {
    "text": "coding was a product\nof the probabilities. And in this case, the\nproduct of the probabilities, you need to take the conditional\nprobabilities at every step",
    "start": "1300650",
    "end": "1307560"
  },
  {
    "text": "because that's how we\nsplit the intervals. And so the bits for\nencoding is roughly log of 1",
    "start": "1307560",
    "end": "1312860"
  },
  {
    "text": "over the-- everything\nelse stays the same. The way you convert intervals\nto bits stays the same. Just the way you split\nthe intervals changes.",
    "start": "1312860",
    "end": "1320000"
  },
  {
    "text": " Yeah. Then you do the math.",
    "start": "1320000",
    "end": "1325050"
  },
  {
    "text": "First time is\nspecial because there is no past for the first term,\nbut every term after that, there is a past.",
    "start": "1325050",
    "end": "1332409"
  },
  {
    "text": "You use your usual\nformulas and then you get roughly\nentropy of U2 given U1. ",
    "start": "1332410",
    "end": "1341640"
  },
  {
    "text": "So this is the\ngeneralized version of the thing we just saw.",
    "start": "1341640",
    "end": "1347370"
  },
  {
    "text": "So this is context\nbased arithmetic coding. This is the core\nof today's lecture.",
    "start": "1347370",
    "end": "1353490"
  },
  {
    "text": "You have data. You have a model. The model may or may not be\nthe true model for the data.",
    "start": "1353490",
    "end": "1359669"
  },
  {
    "text": "Your data might come from\na probability distribution. Maybe your model data does\nnot come from a probability",
    "start": "1359670",
    "end": "1365870"
  },
  {
    "text": "distribution. Just you are given\na sequence and you have a probability model. You have a way to say that\nif my past was A, B, C,",
    "start": "1365870",
    "end": "1373970"
  },
  {
    "text": "D then my next symbol\nis E with probability 50%, something like that. You have a model like\nthat, a prediction model.",
    "start": "1373970",
    "end": "1381350"
  },
  {
    "text": " First step, you just split your\nintervals according to the--",
    "start": "1381350",
    "end": "1388240"
  },
  {
    "text": "basically, the\nmodel has no past, so you just predict the first\nsymbol with the probabilities.",
    "start": "1388240",
    "end": "1393400"
  },
  {
    "text": "Second step, you have\none previous symbol. So you predict the probabilities\naccording to the past,",
    "start": "1393400",
    "end": "1399490"
  },
  {
    "text": "the previous symbol. And next step you basically\ndo the past two symbols,",
    "start": "1399490",
    "end": "1406040"
  },
  {
    "text": "and then the past three,\npast four, you just take the entire\npast at every step. So in this example,\nfor example, here you",
    "start": "1406040",
    "end": "1416120"
  },
  {
    "text": "see, for example, that if I\nknow the first two symbols are 2 and 0, my model\nthinks that 1 is more",
    "start": "1416120",
    "end": "1422450"
  },
  {
    "text": "likely to occur than 0 or 2. Is that clear?",
    "start": "1422450",
    "end": "1427460"
  },
  {
    "text": "This is what this\nmodel is doing. It's saying that if my previous\ntwo symbols are 2 and 0, then the next symbol\nis more likely to be 1",
    "start": "1427460",
    "end": "1433700"
  },
  {
    "text": "and less likely to be 0 or 2. So it's just splitting\nthe intervals happily according to its\nprobability prediction.",
    "start": "1433700",
    "end": "1440510"
  },
  {
    "text": " The code length will be the log\nof the 1 over interval length,",
    "start": "1440510",
    "end": "1446640"
  },
  {
    "text": "this didn't change, which\nis the log of the product of the probabilities. And really this is the core.",
    "start": "1446640",
    "end": "1453630"
  },
  {
    "text": "Better predictor gives\nyou higher probabilities. So if you have a\nvery good predictor,",
    "start": "1453630",
    "end": "1458820"
  },
  {
    "text": "then the thing you\nactually observe will have very\nhigh probabilities. If you have a good\npredictor for English,",
    "start": "1458820",
    "end": "1465029"
  },
  {
    "text": "then it will say that after a\nQ, you are most likely going to get a U. So better\npredictor will give you",
    "start": "1465030",
    "end": "1470580"
  },
  {
    "text": "higher probabilities for the\nthing you actually observe. Higher probabilities lead to\nlarger intervals in arithmetic",
    "start": "1470580",
    "end": "1476620"
  },
  {
    "text": "coding because the\ninterval length is a product of the probabilities,\nwhich gives you smaller code",
    "start": "1476620",
    "end": "1481930"
  },
  {
    "text": "length. So the chain is basically\nbetter prediction gives you smaller code length. So better prediction gives\nyou better compression.",
    "start": "1481930",
    "end": "1491540"
  },
  {
    "text": "I will again wait\nfor a second here. ",
    "start": "1491540",
    "end": "1499375"
  },
  {
    "text": "And we are reaching a\npoint where you can now forget about arithmetic coding. Just remember that better\nprediction gives you",
    "start": "1499375",
    "end": "1504759"
  },
  {
    "text": "better compression,\nand that's all you need for the rest of\nthe lecture basically. Arithmetic coding is just\na tool to get you there.",
    "start": "1504760",
    "end": "1513330"
  },
  {
    "text": "Any questions? ",
    "start": "1513330",
    "end": "1538290"
  },
  {
    "text": "So this is just the same\nthing written in text. So as long as we can\nestimate the probability of the next symbol\ngiven some context.",
    "start": "1538290",
    "end": "1544980"
  },
  {
    "text": "So context just means the past. Maybe you take the entire\npast, maybe you just look at the last symbol.",
    "start": "1544980",
    "end": "1550690"
  },
  {
    "text": "It depends. It depends on the model. The model can be anything\nas far as we are concerned. We will look at some\nspecific models shortly.",
    "start": "1550690",
    "end": "1557490"
  },
  {
    "text": "We can use then arithmetic\ncoding to encode the data. And the number of bits used to\nencode U is just log 1 over P",
    "start": "1557490",
    "end": "1566050"
  },
  {
    "text": "basically. This log rule, the thumb\nrule we have been seeing, this is what you pay. So if you are--",
    "start": "1566050",
    "end": "1572590"
  },
  {
    "text": "let's just do it\nmore specifically. So good model will\nhave P Un given past.",
    "start": "1572590",
    "end": "1581049"
  },
  {
    "text": "So if it's a very good model,\nthen this will be closer to 1. It will truly be able to\npredict strongly that this",
    "start": "1581050",
    "end": "1587440"
  },
  {
    "text": "should be your next thing. And so then you pay\nbasically 0 bits.",
    "start": "1587440",
    "end": "1593340"
  },
  {
    "text": "So that's really nice. And if it's a bad model,\nthen the probability--",
    "start": "1593340",
    "end": "1601890"
  },
  {
    "text": "a bad model would\nsay like, after a Q, you are never going to get a\nU. So it will be very, very",
    "start": "1601890",
    "end": "1609539"
  },
  {
    "text": "bad model for English. And then log of 1\nover very small number",
    "start": "1609540",
    "end": "1615300"
  },
  {
    "text": "is going to be large. So if you have a good model,\ngood compression, bad model,",
    "start": "1615300",
    "end": "1621399"
  },
  {
    "text": "bad compression.  This is a fun example.",
    "start": "1621400",
    "end": "1627000"
  },
  {
    "text": "So Llama is a language\nmodel released by Meta. So I was just trying yesterday.",
    "start": "1627000",
    "end": "1635780"
  },
  {
    "text": "If you take a word\nand you try to predict the next word or a next\ntoken, and what probability",
    "start": "1635780",
    "end": "1642407"
  },
  {
    "text": "distribution does it give? And I was just looking\nat the top five guesses. So it looked at \"than\".",
    "start": "1642407",
    "end": "1648830"
  },
  {
    "text": "And the top guess was \"thanks\",\nI guess, in some new lingo.",
    "start": "1648830",
    "end": "1654110"
  },
  {
    "text": "But I don't know what thane is.",
    "start": "1654110",
    "end": "1659450"
  },
  {
    "text": "I see thanking. So we see with 18% probability,\nit's thought that T-H-A-N would",
    "start": "1659450",
    "end": "1665660"
  },
  {
    "text": "be followed by X. Then I gave it more context.",
    "start": "1665660",
    "end": "1671210"
  },
  {
    "text": "I said, oh, it's\nactually \"louder than\". And now suddenly you see that\nlouder than words, bombs.",
    "start": "1671210",
    "end": "1681080"
  },
  {
    "text": "So words got 30.4%. So I gave it more\ncontext and you see the distribution changed.",
    "start": "1681080",
    "end": "1686300"
  },
  {
    "text": "Now it's thinks\nsomething different about the next next word. Then I said \"speak louder than\".",
    "start": "1686300",
    "end": "1693080"
  },
  {
    "text": "So you might already\nguess where we are going. And I got words. Words is increasing now,\nclose to 50% probability.",
    "start": "1693080",
    "end": "1701000"
  },
  {
    "text": "So in this case, if words\nis the true next symbol,",
    "start": "1701000",
    "end": "1711250"
  },
  {
    "text": "can somebody tell me\nhow many bits would I pay to encode words\nwith this model, roughly how many bits would I pay?",
    "start": "1711250",
    "end": "1717470"
  },
  {
    "text": "Yeah, I see one. Why? Because log of 1 over 1 over 2,\n1/2 is roughly the probability",
    "start": "1717470",
    "end": "1723870"
  },
  {
    "text": "here. This is 1 bit. So I'm at a point where\ngiven the past three words,",
    "start": "1723870",
    "end": "1729030"
  },
  {
    "text": "I'm able to encode the next\nword, a word if you just naively encode it would be\na lot of bits to encode.",
    "start": "1729030",
    "end": "1735990"
  },
  {
    "text": "You're just able to encode it\nwith one bit at this point. And then I just\nbasically gave it",
    "start": "1735990",
    "end": "1742410"
  },
  {
    "text": "the game like, \"Actions\nspeak louder than\" and now you see word is 96.5%.",
    "start": "1742410",
    "end": "1747419"
  },
  {
    "text": "It can be now encoded\nwith far fewer than 1 bit.",
    "start": "1747420",
    "end": "1755930"
  },
  {
    "text": "So what did we learn? We learned that the more\ncontext you give to a model,",
    "start": "1755930",
    "end": "1761150"
  },
  {
    "text": "if it's a good model,\nit's going to be able to predict the next symbol\nwith more and more confidence",
    "start": "1761150",
    "end": "1766970"
  },
  {
    "text": "and therefore it will incur\nless bits when you try to compress with that model.",
    "start": "1766970",
    "end": "1772130"
  },
  {
    "text": " Obviously, we can do this\nbecause these models are now",
    "start": "1772130",
    "end": "1777600"
  },
  {
    "text": "very good. If you take a silly model from\n20 years ago, just give it long,",
    "start": "1777600",
    "end": "1783340"
  },
  {
    "text": "it won't do anything useful. ",
    "start": "1783340",
    "end": "1788649"
  },
  {
    "text": "Then I tried this, just\nanother simple example, I tried \"Stanford's\ndata compression\".",
    "start": "1788650",
    "end": "1794650"
  },
  {
    "text": "And it suggested research,\ngroup, library, team.",
    "start": "1794650",
    "end": "1799930"
  },
  {
    "text": "Class was nowhere there. Course, class, very sad. But then I said \"Enrolling in\nStanford's data compression\",",
    "start": "1799930",
    "end": "1808180"
  },
  {
    "text": "and see how the\ndistribution changes. Course, class, program, so on. So the longer context\nto give, usually it",
    "start": "1808180",
    "end": "1815565"
  },
  {
    "text": "will give you a\nbetter prediction if you're using\na powerful model. So that's the lesson. ",
    "start": "1815565",
    "end": "1823230"
  },
  {
    "text": "So if you know that your data\ncomes from a k-th order Markov model, then the\nprevious k symbols",
    "start": "1823230",
    "end": "1828930"
  },
  {
    "text": "are sufficient to\npredict the next symbol. If it's a Markov model, then you\nonly need the previous symbol.",
    "start": "1828930",
    "end": "1834720"
  },
  {
    "text": "You don't need\nthis long history. But in general, for real life\ndata, it's not a Markov model.",
    "start": "1834720",
    "end": "1840150"
  },
  {
    "text": "It's not a k-th order\nMarkov model for any k. If you think, you're reading\na novel, for example,",
    "start": "1840150",
    "end": "1848160"
  },
  {
    "text": "and some character occurs\nin the first chapter. And then they might not occur\nfor the last five chapters",
    "start": "1848160",
    "end": "1856000"
  },
  {
    "text": "and then suddenly occur again. So then actually having the\ncontext of the past six chapters",
    "start": "1856000",
    "end": "1861520"
  },
  {
    "text": "is important to be able to\nguess the name of that character in the last chapter. So there is a limit.",
    "start": "1861520",
    "end": "1869350"
  },
  {
    "text": "Computationally, you can't just\nlook at everything in the past, but our brain does\nthat in a way. When you're reading a novel\neven if something happened",
    "start": "1869350",
    "end": "1876310"
  },
  {
    "text": "in the past, I guess,\nwe flip back the pages and look if you don't remember. ",
    "start": "1876310",
    "end": "1886450"
  },
  {
    "text": "We will come back to some\nfun things a bit later. Again, back to the grind.",
    "start": "1886450",
    "end": "1892600"
  },
  {
    "text": "Block diagrams. So everything we\njust said can be",
    "start": "1892600",
    "end": "1897940"
  },
  {
    "text": "summarized in this figure\nwhere you have a context. Context means past.",
    "start": "1897940",
    "end": "1903730"
  },
  {
    "text": "You have a model. Given the model--\ngiven the context, the model predicts the\nprobability of the next symbol.",
    "start": "1903730",
    "end": "1910840"
  },
  {
    "text": "And then the arithmetic\ncoder is able to encode it using these many\nbits for each symbol.",
    "start": "1910840",
    "end": "1919160"
  },
  {
    "text": "And therefore the total\nbits you use for encoding is this the sum of the log loss.",
    "start": "1919160",
    "end": "1926419"
  },
  {
    "text": "Anybody tell me, how\nwould the decoding work? Can the decoder-- what would\na decoder block diagram",
    "start": "1926420",
    "end": "1932390"
  },
  {
    "text": "look like for this thing? ",
    "start": "1932390",
    "end": "1942787"
  },
  {
    "text": "Does the decoder need the model? ",
    "start": "1942787",
    "end": "1949540"
  },
  {
    "text": "Yeah, I'm seeing some nods. So basically, you remember\nthat arithmetic decoding is very symmetric to\narithmetic encoding.",
    "start": "1949540",
    "end": "1957130"
  },
  {
    "text": "The decoder also needs to know\nhow to split the intervals. So the decoder needs the\nsame model basically.",
    "start": "1957130",
    "end": "1964480"
  },
  {
    "text": "Decoder needs to\nuse the same model. And at step i, it\nwill have the past. So decoder will sort\nof work step by step.",
    "start": "1964480",
    "end": "1971289"
  },
  {
    "text": "At step 0, it has nothing\nbut it has the model, so it's able to make\nthose intervals.",
    "start": "1971290",
    "end": "1976480"
  },
  {
    "text": "It decodes U1. Then it knows U1. It feeds U1 into the model.",
    "start": "1976480",
    "end": "1982150"
  },
  {
    "text": "It gets the next step\nprobabilities, decodes U2, and so on. So this is iteratively\nable to decode.",
    "start": "1982150",
    "end": "1991690"
  },
  {
    "text": "Is that somewhat clear? And we'll look at some\nspecific models which hopefully",
    "start": "1991690",
    "end": "1997159"
  },
  {
    "text": "will make it slightly clearer. And you will have a\nhomework question. ",
    "start": "1997160",
    "end": "2007629"
  },
  {
    "text": "So really very symmetric, if we\nwanted to-- maybe let's just-- ",
    "start": "2007630",
    "end": "2016340"
  },
  {
    "text": "let's try to draw it. ",
    "start": "2016340",
    "end": "2021420"
  },
  {
    "text": "So the decoder, again, it has\nthe past U1 want to Ui minus 1.",
    "start": "2021420",
    "end": "2034380"
  },
  {
    "text": "It has the model.  Model gives you the probability.",
    "start": "2034380",
    "end": "2040940"
  },
  {
    "text": " You do a decoding.",
    "start": "2040940",
    "end": "2046409"
  },
  {
    "text": "Obviously, there is\nanother input, the bit array, the compressed--",
    "start": "2046409",
    "end": "2051960"
  },
  {
    "text": "obviously, it uses\nthe compressed bits. And it then decodes Ui.",
    "start": "2051960",
    "end": "2058060"
  },
  {
    "text": "And then this Ui becomes\nyour past basically.",
    "start": "2058060",
    "end": "2063460"
  },
  {
    "text": "It goes to your past. So it's this decoding where\nthe decoder every step it decodes one symbol,\nthat goes into the past.",
    "start": "2063460",
    "end": "2070270"
  },
  {
    "text": "Then it predicts the next\nsymbol, decodes the next symbol, then so on. ",
    "start": "2070270",
    "end": "2080149"
  },
  {
    "text": "If you make a compressor, make\nsure to write a decompressor also. Some of these things, you can\nget compression of 0 bits,",
    "start": "2080150",
    "end": "2088270"
  },
  {
    "text": "everything is good, but\nthen you can't decode it. We will look at some pitfalls\nfor these sorts of models",
    "start": "2088270",
    "end": "2093969"
  },
  {
    "text": "very soon. ",
    "start": "2093969",
    "end": "2102330"
  },
  {
    "text": "I don't have a model. What can I do? I am just getting this data.",
    "start": "2102330",
    "end": "2107619"
  },
  {
    "text": "I don't have a model. Any suggestions? Can somebody explain what\nthey did in homework 1",
    "start": "2107620",
    "end": "2113770"
  },
  {
    "text": "when they didn't have a\nmodel, when they didn't know the probability distribution?",
    "start": "2113770",
    "end": "2119090"
  },
  {
    "text": "The fifth question. I think this did-- I think in the homework, we\nwould do like, I don't know,",
    "start": "2119090",
    "end": "2125150"
  },
  {
    "text": "50 kilobytes, and we\njust do blocks and look at the empirical distribution. Yep.",
    "start": "2125150",
    "end": "2130610"
  },
  {
    "text": "Yep. Yep. So the answer is in question 5\nin homework 1, what you did was, we were doing Huffman\ncoding back then,",
    "start": "2130610",
    "end": "2136400"
  },
  {
    "text": "you would look at the empirical\ndistribution of the input data.",
    "start": "2136400",
    "end": "2141559"
  },
  {
    "text": "You would just make a\nmodel from the data and use that to compress the data. We used Huffman there because\nyou hadn't done arithmetic,",
    "start": "2141560",
    "end": "2147560"
  },
  {
    "text": "but you could have done\narithmetic coding also. And what was the model\nyou used in that question?",
    "start": "2147560",
    "end": "2154339"
  },
  {
    "text": "Did you assume it\nwas a Markov model? ",
    "start": "2154340",
    "end": "2161450"
  },
  {
    "text": "No, because we\nused IID back then. So we just used a\nsimple IID model.",
    "start": "2161450",
    "end": "2167599"
  },
  {
    "text": "So there are two ways\nto go about this. The first way is you first\nbuild a model from data",
    "start": "2167600",
    "end": "2173630"
  },
  {
    "text": "and then encode\nusing that model. This is the thing\nyou did in homework.",
    "start": "2173630",
    "end": "2180790"
  },
  {
    "text": "The other way which will\nlook at more in this lecture is adaptive where you build\nthe model from the data",
    "start": "2180790",
    "end": "2186910"
  },
  {
    "text": "as you see the data. So as you see the data, you\nkeep on building a model.",
    "start": "2186910",
    "end": "2192452"
  },
  {
    "text": "I think this is the first\ntime we are doing something like this. ",
    "start": "2192452",
    "end": "2198079"
  },
  {
    "text": "Let me show a block\ndiagram before we look at the pros and cons\nof these two approaches.",
    "start": "2198080",
    "end": "2203359"
  },
  {
    "text": "So again, past context,\nyou have a model. But now the model is not fixed.",
    "start": "2203360",
    "end": "2210280"
  },
  {
    "text": "At every step, the model\ngives you the probability for the next symbol. You encode the next symbol,\nand then you update the model,",
    "start": "2210280",
    "end": "2220340"
  },
  {
    "text": "and then you encode. So basically as the model\nsees more and more data,",
    "start": "2220340",
    "end": "2227000"
  },
  {
    "text": "it keeps on updating its state. It keeps on updating\nits own model basically. The model keeps updating.",
    "start": "2227000",
    "end": "2232310"
  },
  {
    "text": "It keeps training. We will do an example of this.",
    "start": "2232310",
    "end": "2237800"
  },
  {
    "text": "But for now, can anybody\ntell me how would you decode in this setting?",
    "start": "2237800",
    "end": "2243829"
  },
  {
    "text": "Is decoding even possible\nwith this sort of thing? What would the\ndecoder need to do?",
    "start": "2243830",
    "end": "2249400"
  },
  {
    "text": " This is adaptive. You are updating the model at\nevery step based on all the data",
    "start": "2249400",
    "end": "2257417"
  },
  {
    "text": "you have seen in the past. ",
    "start": "2257417",
    "end": "2265870"
  },
  {
    "text": "Since the decoder is decoding\none symbol at a time, it just needs to update\nits model accordingly for each symbol it sees.",
    "start": "2265870",
    "end": "2271690"
  },
  {
    "text": "So in that way, it keeps it. Basically, it's the same state\nin the model as the encoder did.",
    "start": "2271690",
    "end": "2276980"
  },
  {
    "text": "Yep. Yep. Yep. So the answer is decoder will\ndecoder will decode a symbol, it will update.",
    "start": "2276980",
    "end": "2282580"
  },
  {
    "text": "So we can write it down as\na sequence of operations. So you take the decoder.",
    "start": "2282580",
    "end": "2288270"
  },
  {
    "text": "So it will first decode U1,\nthen it will update model.",
    "start": "2288270",
    "end": "2294850"
  },
  {
    "text": "It will need to update the\nmodel in exactly the same way the encoder did. At any time if the models\ndiverge, everything is lost.",
    "start": "2294850",
    "end": "2303340"
  },
  {
    "text": "Decode U2, then you\nagain update model. ",
    "start": "2303340",
    "end": "2310490"
  },
  {
    "text": "So it basically does exactly\nwhat the encoder does. They go in sync.",
    "start": "2310490",
    "end": "2316720"
  },
  {
    "text": "And this is very, very crucial. The encoder and decoder\nneeds to share the same state at every step. If at any step the\narithmetic coder",
    "start": "2316720",
    "end": "2323230"
  },
  {
    "text": "sees different distributions,\nit will just break down. At any step if it breaks\ndown, then all the other steps",
    "start": "2323230",
    "end": "2329859"
  },
  {
    "text": "are gone. This is fine for\nsimple models, but when",
    "start": "2329860",
    "end": "2335110"
  },
  {
    "text": "you try to do stuff\nwith neural networks and so on where all\nthese libraries have all sorts of randomness,\nthings just get very bad.",
    "start": "2335110",
    "end": "2344109"
  },
  {
    "text": "It's very hard. Because when you're working with\nmachine learning applications, for ChatGPT, it's not\nimportant for a given input",
    "start": "2344110",
    "end": "2351370"
  },
  {
    "text": "to always produce\nthe same output. But for a compression\nmodel, it's very important that\ngiven a certain input,",
    "start": "2351370",
    "end": "2357160"
  },
  {
    "text": "it should always give you\nthe same probabilities. There can't be any GPU\nlevel randomization.",
    "start": "2357160",
    "end": "2363369"
  },
  {
    "text": "There can't be any\nfloating point stuff. So it's hard.",
    "start": "2363370",
    "end": "2370343"
  },
  {
    "text": "People have to write\ntheir own libraries if they want to do compression\nwith neural nets basically.",
    "start": "2370343",
    "end": "2375820"
  },
  {
    "text": "Even if you write something,\nthen it works on one GPU, so you compress a\nfile on one GPU. You decode it on another\nGPU and that breaks.",
    "start": "2375820",
    "end": "2384589"
  },
  {
    "text": "So yes, very crucial.  This one is also very important.",
    "start": "2384590",
    "end": "2392400"
  },
  {
    "text": "During compression, you\nhave the entire input. But you shouldn't get too eager. Don't straight away\nupdate the model with Ui",
    "start": "2392400",
    "end": "2399600"
  },
  {
    "text": "before you encode Ui. Always think of the decoder. It's always important\nto think of the decoder.",
    "start": "2399600",
    "end": "2405480"
  },
  {
    "text": "Whenever each step you\nthink of the decoder, what does the decoder\nknow at this point?",
    "start": "2405480",
    "end": "2410760"
  },
  {
    "text": "So let's say the\nencoder does this. ",
    "start": "2410760",
    "end": "2416569"
  },
  {
    "text": "It first updates model with U1.",
    "start": "2416570",
    "end": "2422620"
  },
  {
    "text": "And then it encodes U1. What will happen now?",
    "start": "2422620",
    "end": "2428680"
  },
  {
    "text": "Can the decoder still work? I hear no. No clearly because\ndecoder doesn't have U1.",
    "start": "2428680",
    "end": "2435740"
  },
  {
    "text": "Encoder has the entire thing. Encoder can do whatever,\nbut the decoder can't start, so always, this is wrong, bad.",
    "start": "2435740",
    "end": "2443750"
  },
  {
    "text": "So you just need to be careful. This comes up in lossy\ncompression also. There are these adaptive type\nschemes, just be careful.",
    "start": "2443750",
    "end": "2451250"
  },
  {
    "text": "Last thing is never provide\nzero probability to any symbol because if your model says that\nthis symbol will never occur",
    "start": "2451250",
    "end": "2459869"
  },
  {
    "text": "and then it actually occurs,\nthen log of 1 over zero is infinity. So the arithmetic order\nwill just break basically.",
    "start": "2459870",
    "end": "2468410"
  },
  {
    "text": "So every symbol should,\nhowever, small it might be, but it should always have\na non-zero probability.",
    "start": "2468410",
    "end": "2473960"
  },
  {
    "text": "Even if you think\nthat a Q after a Q will never happen in\nEnglish, provide it",
    "start": "2473960",
    "end": "2479509"
  },
  {
    "text": "some small probability. Never provide it exactly\nzero probability. ",
    "start": "2479510",
    "end": "2486869"
  },
  {
    "text": "Any questions? ",
    "start": "2486870",
    "end": "2493540"
  },
  {
    "text": "So just like a pros and\ncons list, two-pass. So one is that we learn\nmodel from entire data.",
    "start": "2493540",
    "end": "2500650"
  },
  {
    "text": "So you look at the entire data. So maybe you get a better\nmodel at the end of the day. And you can parallelize\nthe entire process more",
    "start": "2500650",
    "end": "2506920"
  },
  {
    "text": "because adaptive is sequential. You're going through\nthe data one by one. In the two-parts, there\nare ways to parallelize it.",
    "start": "2506920",
    "end": "2513460"
  },
  {
    "text": "However, you need to store the\nmodel in the compressed file so that's not ideal. You need two passes\nover the data.",
    "start": "2513460",
    "end": "2520089"
  },
  {
    "text": "This is often a breaking\nthing in many applications where you can't\njust hold the data. You have to keep compressing\nthe data as it comes in.",
    "start": "2520090",
    "end": "2527350"
  },
  {
    "text": "You can't keep the\nentire data in memory. So this is usually a problem.",
    "start": "2527350",
    "end": "2533310"
  },
  {
    "text": "And if your data has statistics\nthat change over time,",
    "start": "2533310",
    "end": "2539000"
  },
  {
    "text": "if you have a file\nwhich start with a novel and then it suddenly turns\ninto a code or something,",
    "start": "2539000",
    "end": "2544910"
  },
  {
    "text": "then the model you learn will\nbe a mixture of the two models, and it won't do as well. Whereas adaptive might\nwork even in this case.",
    "start": "2544910",
    "end": "2553250"
  },
  {
    "text": "So adaptive, the\nnice thing is you don't need to store the model. It is good for streaming. You can compress as it goes.",
    "start": "2553250",
    "end": "2561950"
  },
  {
    "text": "One bad thing is that initially,\nthe model is very bad. You don't know anything. You don't assume\nanything about the fold.",
    "start": "2561950",
    "end": "2568110"
  },
  {
    "text": "So initially, you might get\nslightly bad compression. But it works well in practice. So we'll look at one\nexample of adaptive shortly.",
    "start": "2568110",
    "end": "2577000"
  },
  {
    "text": "Any questions on adaptive\narithmetic coding? Otherwise, I think we'll go to\nthe example in a few slides.",
    "start": "2577000",
    "end": "2586150"
  },
  {
    "text": " So just wrapping this\nup, this is the relation",
    "start": "2586150",
    "end": "2593980"
  },
  {
    "text": "between compression\nand prediction that we have been seeing. So if you have done any\nmachine learning class,",
    "start": "2593980",
    "end": "2602210"
  },
  {
    "text": "either neural networks or\nwithout neural networks or AI or whatever, you might have\nseen this cross-entropy loss,",
    "start": "2602210",
    "end": "2609830"
  },
  {
    "text": "also called log\nloss for prediction. For prediction your task is you\nhave a bunch of classes and then",
    "start": "2609830",
    "end": "2617589"
  },
  {
    "text": "you're trying to\npredict which is the-- you're given an\nimage of something",
    "start": "2617590",
    "end": "2623080"
  },
  {
    "text": "and you want to tell whether\nit's a cat or a dog or a cow. And most of the predictors,\nwhat they actually give you",
    "start": "2623080",
    "end": "2629079"
  },
  {
    "text": "is a probability. So they give that 90% cat,\n10% dog, 10% cow, let's say.",
    "start": "2629080",
    "end": "2636370"
  },
  {
    "text": "And then you evaluate the model\naccording to this loss function where you say that\nthis is a cat,",
    "start": "2636370",
    "end": "2645220"
  },
  {
    "text": "and the model gives you cat\nis 90%, dog is 5, cow is 5.",
    "start": "2645220",
    "end": "2654250"
  },
  {
    "text": "Then the loss you get\nis log 1 over 0.9, the probability of being a\ncat according to the model.",
    "start": "2654250",
    "end": "2662980"
  },
  {
    "text": "So this forces the model to\ngive a higher probability to the correct symbol\nand lower probability to the not correct image--",
    "start": "2662980",
    "end": "2669880"
  },
  {
    "text": "and not correct label. But this exactly matches the\nthing we were just doing.",
    "start": "2669880",
    "end": "2676877"
  },
  {
    "text": "So actually prediction\nand compression are exactly the same problem. ",
    "start": "2676877",
    "end": "2683310"
  },
  {
    "text": "Not much more to\nsay, you can see the loss function is the same. It's literally the\nsame loss function.",
    "start": "2683310",
    "end": "2689940"
  },
  {
    "text": "The cost and arithmetic\ncoding of encoding a thing is exactly the same\nas the prediction loss",
    "start": "2689940",
    "end": "2695579"
  },
  {
    "text": "that we always use in ML. So what this tells\nyou that if you",
    "start": "2695580",
    "end": "2701910"
  },
  {
    "text": "have a good predictor\nin the ML sense, then it's a good compressor. And compression is\nall about having",
    "start": "2701910",
    "end": "2708730"
  },
  {
    "text": "a good model for the data. If you have a good\nmodel for the data, then you automatically\nhave a good compressor. You don't need to\ndo anything special.",
    "start": "2708730",
    "end": "2716930"
  },
  {
    "text": "Many compressors don't\nactually model the data. We'll see some. LZ77, the one we'll see, it\ndoesn't actually model the data.",
    "start": "2716930",
    "end": "2723890"
  },
  {
    "text": "But the compressors we see today\nwill actually model the data. So it need not be explicit\nbut inherently, there",
    "start": "2723890",
    "end": "2731990"
  },
  {
    "text": "is a modeling going on. And final thing,\nthis is out of scope.",
    "start": "2731990",
    "end": "2737660"
  },
  {
    "text": "We won't go into the\ndetails, but we are just doing arithmetic coding. But in many cases, you can\nuse rANS with some tricks.",
    "start": "2737660",
    "end": "2744500"
  },
  {
    "text": "And people are slowly\ntrying to use rANS in a lot of these applications. There are some issues, but\nsome of them can be worked out.",
    "start": "2744500",
    "end": "2751954"
  },
  {
    "text": " There is the\nopposite angle also.",
    "start": "2751955",
    "end": "2757990"
  },
  {
    "text": "Not only does every predictor\ngive you a compressor, actually every compressor\ngives you a predictor.",
    "start": "2757990",
    "end": "2765560"
  },
  {
    "text": "This is a bit trickier\nbut it is true.",
    "start": "2765560",
    "end": "2771755"
  },
  {
    "text": " And here the idea is-- ",
    "start": "2771755",
    "end": "2779590"
  },
  {
    "text": "so if you have a\ncompressor, encodes",
    "start": "2779590",
    "end": "2784890"
  },
  {
    "text": "x1 through xn in l\nbits, then the predictor",
    "start": "2784890",
    "end": "2791920"
  },
  {
    "text": "that the compressor gives you is\nbasically given by this thing. ",
    "start": "2791920",
    "end": "2799300"
  },
  {
    "text": "So basically each\ncompressor is a predictor. Each predictor is a\ncompressor internally.",
    "start": "2799300",
    "end": "2805660"
  },
  {
    "text": "You remember in the prediction\nthing, you have log 1 by P. So it's just like you do\nthe formula, log 1 over P.",
    "start": "2805660",
    "end": "2811809"
  },
  {
    "text": "So P is equal to\n2 to the minus l. So from any compressor, you can\nactually create a probability",
    "start": "2811810",
    "end": "2816940"
  },
  {
    "text": "model for your data. This is slightly advanced. We will not worry too much\nabout it, but it is a thing.",
    "start": "2816940",
    "end": "2826000"
  },
  {
    "text": "There is a paper I will tell you\nabout, very recent paper, that actually took gzip, which\nis a very common compressor,",
    "start": "2826000",
    "end": "2832150"
  },
  {
    "text": "and turned it into a predictor. It didn't do very well, but\nyou can do these things.",
    "start": "2832150",
    "end": "2840780"
  },
  {
    "text": "So prediction models\nused for compression. So now we'll look at a few\nspecific prediction models, do some examples, and then do\na few very fun experiments.",
    "start": "2840780",
    "end": "2852390"
  },
  {
    "text": " So this is k-th order\nadaptive arithmetic coding.",
    "start": "2852390",
    "end": "2858760"
  },
  {
    "start": "2858760",
    "end": "2863855"
  },
  {
    "text": "So what you are\ntrying to do here is you are trying to learn a\nk-th order model for your data as you go through the data.",
    "start": "2863855",
    "end": "2869595"
  },
  {
    "text": " Let's just read the last line.",
    "start": "2869595",
    "end": "2875082"
  },
  {
    "text": "Don't look at the rest of it. I think you will understand\nfrom the example the rest of it. So if you have seen BANA\nin the past, several times,",
    "start": "2875082",
    "end": "2883859"
  },
  {
    "text": "and 90% of the times you saw\nit being followed by an N, so for a banana, for example.",
    "start": "2883860",
    "end": "2888900"
  },
  {
    "text": "And 10% of the time you\nsaw it followed by an L. That is your model basically.",
    "start": "2888900",
    "end": "2893970"
  },
  {
    "text": "The next time you\nsee BANA, you would say that maybe it's an\nN with probability 90% and L with a probability 10%.",
    "start": "2893970",
    "end": "2901590"
  },
  {
    "text": "You use the past to predict\nthe future, as simple as that. So let's do an example,\nwhich should make it clear.",
    "start": "2901590",
    "end": "2910619"
  },
  {
    "text": "So we are doing first order\nadaptive arithmetic coding. So this means you\nneed to actually keep",
    "start": "2910620",
    "end": "2915780"
  },
  {
    "text": "second order statistics. You will understand why. ",
    "start": "2915780",
    "end": "2922140"
  },
  {
    "text": "So initially-- and\nthis is your data. You are going to encode 101011.",
    "start": "2922140",
    "end": "2927540"
  },
  {
    "text": "So initially, you\nassume everything is 1. You have some\nfrequencies or counts.",
    "start": "2927540",
    "end": "2933300"
  },
  {
    "text": "So you assume that-- this is to avoid the\nproblem we talked about before where you\ndon't want anything",
    "start": "2933300",
    "end": "2938339"
  },
  {
    "text": "to have a 0 probability. So you assume that\nyou've already seen a 00. You have seen 01. You have seen a 10. You have seen a 11.",
    "start": "2938340",
    "end": "2944370"
  },
  {
    "text": "You have seen all four\none time in the past.",
    "start": "2944370",
    "end": "2949790"
  },
  {
    "text": "So the first symbol\nyou see is 1. And here you don't have a past. So what we will do is\nwe'll add a 0 at the front,",
    "start": "2949790",
    "end": "2956870"
  },
  {
    "text": "just as a fake zero, padding. Now looking at\nthese counts, what",
    "start": "2956870",
    "end": "2968620"
  },
  {
    "text": "is the probability that\na 0 is followed by a 1? If your past is 0,\nwhat is the probability that the next one is a 1?",
    "start": "2968620",
    "end": "2974590"
  },
  {
    "start": "2974590",
    "end": "2981280"
  },
  {
    "text": "How many think it's one, the\nprobability that a 0 is followed by a 1?",
    "start": "2981280",
    "end": "2987960"
  },
  {
    "text": "How many think it's 1/3? 1/2. ",
    "start": "2987960",
    "end": "2994320"
  },
  {
    "text": "Very few words for 1/2. 1/2 is the correct answer,\nso not very tricky.",
    "start": "2994320",
    "end": "3001069"
  },
  {
    "text": "So basically what\nwe are doing this-- this is the equation. The probability that\na 0 is followed by 1",
    "start": "3001070",
    "end": "3007815"
  },
  {
    "text": "is you take all the\ncases where you say 0, comma 1 in the numerator. And you take all the cases\nwhere you see 0, comma 0 plus 0,",
    "start": "3007815",
    "end": "3014510"
  },
  {
    "text": "comma 1 in the denominator,\nthat sort of thing. So basically, what\nthis is saying is you",
    "start": "3014510",
    "end": "3021290"
  },
  {
    "text": "saw 0, comma 1 n01.",
    "start": "3021290",
    "end": "3028080"
  },
  {
    "text": "Let's say you saw this 99 times. And you saw 0, comma 0 one time.",
    "start": "3028080",
    "end": "3037960"
  },
  {
    "text": "Then you would say that\nthe probability that 0 will be followed by 1 is 9 over 10.",
    "start": "3037960",
    "end": "3046330"
  },
  {
    "text": "So just 9 out of\n10 times, you're seeing that a 0 will be followed\nby 1, as simple as that.",
    "start": "3046330",
    "end": "3053290"
  },
  {
    "text": "And now you have\nseen a 0, comma 1. So you update your count. You update your count\nof 0, comma 1 by 1.",
    "start": "3053290",
    "end": "3058840"
  },
  {
    "text": "So you do 1 plus 1 equal to 2. Are we happy with this? And then we'll do the next.",
    "start": "3058840",
    "end": "3064885"
  },
  {
    "text": " Let me pause for a sec. ",
    "start": "3064885",
    "end": "3079950"
  },
  {
    "text": "And now we do the next symbol. So next symbol is 0. So now you need to and\nthe previous symbol is 1.",
    "start": "3079950",
    "end": "3087040"
  },
  {
    "text": "So next symbol is 0,\nprevious symbol is 1. So you do probability of 0\ngiven 1 based on this model.",
    "start": "3087040",
    "end": "3092650"
  },
  {
    "text": "Just follow the math. Again, you get one 1/2. Again, you update the model.",
    "start": "3092650",
    "end": "3097720"
  },
  {
    "text": "Let me do the next one,\nwhich is more interesting. So now you have\nyour previous symbol",
    "start": "3097720",
    "end": "3103880"
  },
  {
    "text": "is a 0 and next symbol is a 1. But remember that\ndue to the padding, you have already seen 0, 1.",
    "start": "3103880",
    "end": "3110890"
  },
  {
    "text": "So now your model should\nlean towards, oh, maybe a 0 is more often followed by 1\nand less often followed by a 0.",
    "start": "3110890",
    "end": "3117010"
  },
  {
    "text": "And yeah, that's what it does. It gives you a 2/3 probability\ninstead of a 1/2 probability",
    "start": "3117010",
    "end": "3122319"
  },
  {
    "text": "which it gave you last time. So as you see more\nand more data, it will learn the\nstatistics of your data",
    "start": "3122320",
    "end": "3129040"
  },
  {
    "text": "and start giving you\npredictions which are consistent with your data. And at every step, you\nkeep updating your model.",
    "start": "3129040",
    "end": "3136360"
  },
  {
    "text": "I think you will have a\nchance to play with this, actually make your own model in\none of the homework questions.",
    "start": "3136360",
    "end": "3142280"
  },
  {
    "text": "So that should be exciting.  So the idea is\nthat over time, you",
    "start": "3142280",
    "end": "3148180"
  },
  {
    "text": "learn the empirical\ndistribution of the data as you see the\nmore and more data.",
    "start": "3148180",
    "end": "3153310"
  },
  {
    "text": "And initially you start off\nwith a uniform distribution in the example we did. But if you know\nsomething about the data,",
    "start": "3153310",
    "end": "3159519"
  },
  {
    "text": "you can use that to\ninitialize your counts. Make sure encoder and\ndecoder both know, and also",
    "start": "3159520",
    "end": "3166840"
  },
  {
    "text": "make sure nothing\nis 0 basically. Never make anything 0 because\nif you make anything zero, then",
    "start": "3166840",
    "end": "3172930"
  },
  {
    "text": "everything breaks. And one final thing for that\nquestion from homework 1",
    "start": "3172930",
    "end": "3179470"
  },
  {
    "text": "that you all did, you could have\nused this with k equal to 0. You could have-- you\ncould set k equal to 0.",
    "start": "3179470",
    "end": "3186309"
  },
  {
    "text": "So what you would do is, let's\nsay you have seen 1011000111",
    "start": "3186310",
    "end": "3192580"
  },
  {
    "text": "and the next symbol\nis a 0, which is what you want to encode. So in the past, you\nhave seen 1, 2, 3, 4, 5,",
    "start": "3192580",
    "end": "3199270"
  },
  {
    "text": "6 so six 1's and four 0's. So you could just make a\nmodel where this is 4 by 10.",
    "start": "3199270",
    "end": "3208890"
  },
  {
    "text": "And then the next symbol, let's\nsay that's also 0, can anybody",
    "start": "3208890",
    "end": "3216579"
  },
  {
    "text": "tell me what will be the\npredicted probability of the next symbol if this guy\nhad a probability of 4 over 10?",
    "start": "3216580",
    "end": "3221980"
  },
  {
    "text": " 5 over 11, I hear.",
    "start": "3221980",
    "end": "3228410"
  },
  {
    "text": "Yes, I think, because the last\nsymbol will increase the count of 0's and increase\nthe count of 1's.",
    "start": "3228410",
    "end": "3234849"
  },
  {
    "text": "So just the counts,\nnothing special. So you can implement it.",
    "start": "3234850",
    "end": "3244100"
  },
  {
    "text": "We implemented it. We took The Hound of\nBaskerville, and we ran it.",
    "start": "3244100",
    "end": "3254370"
  },
  {
    "text": "So 0th order is basically\nassuming an IID model. First order assumes\na first order model,",
    "start": "3254370",
    "end": "3259440"
  },
  {
    "text": "second order, third order. We also ran two\nstandard compressors as benchmarks, gzip and bzip2.",
    "start": "3259440",
    "end": "3267090"
  },
  {
    "text": "So somebody tell me why\ndoes it improve when you go from zero to second order? ",
    "start": "3267090",
    "end": "3287390"
  },
  {
    "text": "Let's say your\ninput was actually. There was no dependence. Do you expect it to improve\nif you use a larger context?",
    "start": "3287390",
    "end": "3294280"
  },
  {
    "text": "No, right? So clearly, there\nis some relation. And you know that English has.",
    "start": "3294280",
    "end": "3300260"
  },
  {
    "text": "English, if the\npast two symbols, you can predict the\nnext symbol better. So therefore, it will improve\nas you increase the order.",
    "start": "3300260",
    "end": "3307280"
  },
  {
    "text": "Now can somebody tell\nme why did it not get better for third order. Why did it actually get worse?",
    "start": "3307280",
    "end": "3313740"
  },
  {
    "text": "Recall that the file\nhas a certain size. It's a few 100 kilobytes. It's not infinite.",
    "start": "3313740",
    "end": "3319650"
  },
  {
    "start": "3319650",
    "end": "3325960"
  },
  {
    "text": "In a smallish\nfile, do you expect to see a sequence A,\nB, C, D or something?",
    "start": "3325960",
    "end": "3334522"
  },
  {
    "text": "Do you expect to see\na sequence of length four many, many times\nin a very short file?",
    "start": "3334522",
    "end": "3339680"
  },
  {
    "text": "If you have a small\namount of data, you won't even get any counts. That is the issue.",
    "start": "3339680",
    "end": "3344960"
  },
  {
    "text": "You start with your\ninitial uniform model and you never get\nenough counts to make",
    "start": "3344960",
    "end": "3350420"
  },
  {
    "text": "that model into something\nuseful because you have such a small-- because if\nyou think about it, the count",
    "start": "3350420",
    "end": "3356660"
  },
  {
    "text": "array basically, so for\nfirst order, you need pairs.",
    "start": "3356660",
    "end": "3366539"
  },
  {
    "text": "You need Xi, comma Xi plus 1. And then for second order, you\nneed Xi, Xi plus 1, Xi plus 2.",
    "start": "3366540",
    "end": "3375980"
  },
  {
    "text": "You need three at\na time, and so on. So this grows exponentially\nin the order basically.",
    "start": "3375980",
    "end": "3382700"
  },
  {
    "text": "So if your alphabet\nsize is X and your-- so this is your order.",
    "start": "3382700",
    "end": "3388520"
  },
  {
    "text": "Then this is the number of\ncounts you need to maintain. So if you have a very\nsmall amount of data,",
    "start": "3388520",
    "end": "3394740"
  },
  {
    "text": "your counts will be very sparse. Most of them will be 0. You won't see most\nof the patterns.",
    "start": "3394740",
    "end": "3400545"
  },
  {
    "text": "And since you don't see\nmost of the patterns, you get a very poor predictor,\nand it doesn't do well.",
    "start": "3400545",
    "end": "3407010"
  },
  {
    "text": "If you don't have a lot of\ndata, then higher order models start breaking because we\nhave a very silly model.",
    "start": "3407010",
    "end": "3413740"
  },
  {
    "text": "It just looks at the past. And so for example, just as an\nexample, let's say you see THIN",
    "start": "3413740",
    "end": "3426880"
  },
  {
    "text": "and then you're trying to\npredict the next symbol. But if you have never\nseen THIN in the past,",
    "start": "3426880",
    "end": "3432220"
  },
  {
    "text": "you have no model for it. So it won't do anything useful. So unless you have a\nlarge enough text where",
    "start": "3432220",
    "end": "3437680"
  },
  {
    "text": "you see that thing many, many\ntimes, it won't learn anything. So that's why it got worse here.",
    "start": "3437680",
    "end": "3445350"
  },
  {
    "text": "And we see that gzip and bzip2\nactually beat these models. And we'll talk about\nthem in next lecture.",
    "start": "3445350",
    "end": "3452619"
  },
  {
    "text": "So we talked already\nabout why is order 3 to inverse than order 2. So there are limitations.",
    "start": "3452620",
    "end": "3458650"
  },
  {
    "text": "k-th order becomes slow. The memory grows\nexponentially in k. The counts become very\nsparse for large k.",
    "start": "3458650",
    "end": "3465160"
  },
  {
    "text": "As you increase k,\nthe count is just-- you don't have many counts and\nit leads to worse performance.",
    "start": "3465160",
    "end": "3472490"
  },
  {
    "text": "And this is an\ninteresting point where-- ",
    "start": "3472490",
    "end": "3481210"
  },
  {
    "text": "just an example. Let's say you have\n\"thi\" and capital THI.",
    "start": "3481210",
    "end": "3486410"
  },
  {
    "text": "If you had a smart model,\nit would know that OK these two just the\ncapitalization is different,",
    "start": "3486410",
    "end": "3491840"
  },
  {
    "text": "but there is some relation. If you have seen THIS many,\nmany times even for uppercase T,",
    "start": "3491840",
    "end": "3499010"
  },
  {
    "text": "it should predict that\nthis is-- maybe it's a S. But if you have that silly\nadaptive arithmetic coder",
    "start": "3499010",
    "end": "3505610"
  },
  {
    "text": "k-th order model,\nthat doesn't know. Everything looks\ndifferent to it. It has no understanding\nof English.",
    "start": "3505610",
    "end": "3511309"
  },
  {
    "text": "It has no understanding\nof anything. And it just treats any two\ncontexts, any two past sequence",
    "start": "3511310",
    "end": "3517100"
  },
  {
    "text": "as just distinct, just those two\nseparate counts for all of them.",
    "start": "3517100",
    "end": "3522890"
  },
  {
    "text": "So it doesn't do as well. We can improve them. ",
    "start": "3522890",
    "end": "3528780"
  },
  {
    "text": "I said a lot of bad things. It's not the end-- people use this. They are used in certain\nspecific scenarios,",
    "start": "3528780",
    "end": "3535109"
  },
  {
    "text": "but they're not the best models. But they are simple models,\nand you can try them.",
    "start": "3535110",
    "end": "3540944"
  },
  {
    "text": " Now did another thing. I said I did the adaptive thing.",
    "start": "3540945",
    "end": "3547740"
  },
  {
    "text": "But if I did a\ntwo-pass thing, I can compute the conditional\nentropy from the data.",
    "start": "3547740",
    "end": "3554550"
  },
  {
    "text": "And now I see, oh, for 0th\norder, they are very close. For first order,\nthere is a small gap.",
    "start": "3554550",
    "end": "3559920"
  },
  {
    "text": "For third order, the\nempirical conditional entropy keeps improving. ",
    "start": "3559920",
    "end": "3569060"
  },
  {
    "text": "So if I did a\ntwo-pass approach, it seems like I could do a\nreally, really good job. ",
    "start": "3569060",
    "end": "3579625"
  },
  {
    "text": "Again, the Huffman case,\nwe did a two pass approach if you remember. ",
    "start": "3579625",
    "end": "3584835"
  },
  {
    "text": "Would do you do a good job? What do you think? ",
    "start": "3584835",
    "end": "3593849"
  },
  {
    "text": "So if you are doing\na third order model, let's say in your entire\ntext you see \"thi\" only once",
    "start": "3593850",
    "end": "3603859"
  },
  {
    "text": "and it was followed by, S\nlet's say, in your entire text. So how many bits do\nyou spend to encode S?",
    "start": "3603860",
    "end": "3610850"
  },
  {
    "text": "If you've seen it only\nonce, and the one time it was followed by\nS. You basically--",
    "start": "3610850",
    "end": "3616880"
  },
  {
    "text": "can somebody tell me. None. You spend 0 bits.",
    "start": "3616880",
    "end": "3622540"
  },
  {
    "text": "You spend 0 bits to encode the\nnext symbol because you only saw it once. So the probability\ndistribution is deterministic.",
    "start": "3622540",
    "end": "3630400"
  },
  {
    "text": "But how does the decoder know? ",
    "start": "3630400",
    "end": "3637490"
  },
  {
    "text": "Yes. Yes. Yes. Yes. Basically, what you are\ndoing is the model basically",
    "start": "3637490",
    "end": "3644200"
  },
  {
    "text": "now starts storing the data. As you increase k, what happens\nis think of the extreme,",
    "start": "3644200",
    "end": "3654860"
  },
  {
    "text": "think of the very extreme. Let's say you have a text of\nlength 1,000 and if you take a 1,000th order model, then\nthere is only 1,000 line",
    "start": "3654860",
    "end": "3663350"
  },
  {
    "text": "sequence in the entire input. It has probability 1. You encode it in 0 bits.",
    "start": "3663350",
    "end": "3669230"
  },
  {
    "text": "You don't need any\nbits to encode it. So this is just a thought\nexperiment where you take--",
    "start": "3669230",
    "end": "3676230"
  },
  {
    "text": "so let's say your input is\n1,000 bits and 1,000 symbols",
    "start": "3676230",
    "end": "3686910"
  },
  {
    "text": "and you take 1,000 order model.",
    "start": "3686910",
    "end": "3692329"
  },
  {
    "text": "So the probability\nof the input is 1. Probability of\neverything else is 0.",
    "start": "3692330",
    "end": "3707420"
  },
  {
    "text": "You just store your\nentire data in your model and you say, oh, I\nencoded it in 0 bits",
    "start": "3707420",
    "end": "3712460"
  },
  {
    "text": "because my model is perfect at\npredicting my particular data.",
    "start": "3712460",
    "end": "3718160"
  },
  {
    "text": "You take a Sherlock\nHolmes novel, your model contains the\nentire Sherlock Holmes novel.",
    "start": "3718160",
    "end": "3725400"
  },
  {
    "text": "And you encode it in 0 bits. You don't need to send anything. The decoder knows it's\na Sherlock Holmes novel.",
    "start": "3725400",
    "end": "3731130"
  },
  {
    "text": "So this is silly. This is not how we should do it. ",
    "start": "3731130",
    "end": "3737940"
  },
  {
    "text": "So as the order increases,\nbasically knowing the empirical\ndistribution basically",
    "start": "3737940",
    "end": "3743280"
  },
  {
    "text": "becomes storing the\ndata in the model. So you're not doing anything\nsmart at that point. You're just storing\nyour data in the model.",
    "start": "3743280",
    "end": "3750660"
  },
  {
    "text": "But recall that the model size\nis exponential in the order.",
    "start": "3750660",
    "end": "3758020"
  },
  {
    "text": "So as the order becomes bigger,\nthe model overwhelms everything. And therefore, you\nneed to account",
    "start": "3758020",
    "end": "3763359"
  },
  {
    "text": "for the cost of\nstoring the model. And this is another reason\nwhere adaptive models are often",
    "start": "3763360",
    "end": "3768560"
  },
  {
    "text": "preferred because you don't\nneed two passes and so on.",
    "start": "3768560",
    "end": "3774040"
  },
  {
    "text": "There is this principle. We won't talk too\nmuch about it, but I think this will convey\nwhat I was trying to say.",
    "start": "3774040",
    "end": "3779500"
  },
  {
    "text": "As you make your model\nmore and more complex, your compressed size will\nreduce because your model will at some point start\nlearning your data.",
    "start": "3779500",
    "end": "3786520"
  },
  {
    "text": "But the model size will\nincrease exponentially. And so the total size\nis what you care about,",
    "start": "3786520",
    "end": "3792160"
  },
  {
    "text": "the size of the model\nplus the size of the data. And there is some point\nwhere it's optimal.",
    "start": "3792160",
    "end": "3797200"
  },
  {
    "text": "Like in this figure, it's here. At some intermediate\nmodel complexity, you will get the\nbest overall result.",
    "start": "3797200",
    "end": "3806320"
  },
  {
    "text": "This principle is called\nminimum description length by Rissanen, who is\none of the people who",
    "start": "3806320",
    "end": "3811420"
  },
  {
    "text": "did a lot of interesting\ntheory in compression. And he's one of the creators\nof arithmetic coding actually,",
    "start": "3811420",
    "end": "3818230"
  },
  {
    "text": "so good work.",
    "start": "3818230",
    "end": "3823380"
  },
  {
    "text": "any questions so far? I don't think we'll\nget to LZ today,",
    "start": "3823380",
    "end": "3828680"
  },
  {
    "text": "but we'll do some fun things.  Any questions?",
    "start": "3828680",
    "end": "3836820"
  },
  {
    "text": "So we saw adaptive\nk-th order modeling. We saw what happens\nif you do a two-pass",
    "start": "3836820",
    "end": "3842570"
  },
  {
    "text": "and you keep\nincreasing the order. And we thought that the k-th\norder thing has some issues. The counts becomes sparse.",
    "start": "3842570",
    "end": "3848380"
  },
  {
    "text": "It gets harder. So this is something we won't\ngo into too much detail, but it's a great\nproject idea basically",
    "start": "3848380",
    "end": "3855040"
  },
  {
    "text": "if you want to do any of these. So the k-th order adaptive is\nimplemented in the library.",
    "start": "3855040",
    "end": "3860950"
  },
  {
    "text": "Solving the sparse\ncount problem, there are a couple of ways where\nyou can actually partially solve",
    "start": "3860950",
    "end": "3867490"
  },
  {
    "text": "that issue. There is something called\nPPM, something called CTW. And what they do is that\nmaybe I have not seen TH",
    "start": "3867490",
    "end": "3875800"
  },
  {
    "text": "before but maybe\nI've seen HI before or maybe I've seen\njust I before, so you mix a bunch of things.",
    "start": "3875800",
    "end": "3881840"
  },
  {
    "text": "And you try to\ncompensate for the fact that maybe you have not\nseen longer context,",
    "start": "3881840",
    "end": "3888460"
  },
  {
    "text": "but you have at least\nseen shorter context. So at least use those. Don't throw away information\nthat you already got.",
    "start": "3888460",
    "end": "3896660"
  },
  {
    "text": "Again I'm not going\ninto details sadly. Then there are some\nadvanced prediction models.",
    "start": "3896660",
    "end": "3904160"
  },
  {
    "text": "I have links here. I will talk over\na couple of them. And really this book is--",
    "start": "3904160",
    "end": "3909470"
  },
  {
    "text": "if you want to really understand\nthese and implement these, this is an online textbook\nby Matt Mahoney, who",
    "start": "3909470",
    "end": "3916880"
  },
  {
    "text": "has done a lot of interesting\nwork on heavy duty compressors in particular.",
    "start": "3916880",
    "end": "3922819"
  },
  {
    "text": "So all of these are\ntypically the most powerful compressors around but very slow\nfor many practical applications.",
    "start": "3922820",
    "end": "3930690"
  },
  {
    "text": "So just very quickly two\nof these advanced models. Some of them are neural net\nbased where what they do is--",
    "start": "3930690",
    "end": "3936810"
  },
  {
    "text": " yeah, so it basically you\nhave a given the past symbols,",
    "start": "3936810",
    "end": "3945190"
  },
  {
    "text": "you have a neural\nnetwork predictor, and then that goes\ninto arithmetic encoder",
    "start": "3945190",
    "end": "3951770"
  },
  {
    "text": "and similarly for the decoder. So we just changed\nour adaptive thing to a neural network\npredictor, which will again",
    "start": "3951770",
    "end": "3957440"
  },
  {
    "text": "give you the probabilities. And you can do a\ntwo-pass approach, or you can do it an\nadaptive approach. So you can keep updating\nyour neural net.",
    "start": "3957440",
    "end": "3963260"
  },
  {
    "text": "Just like you had\nyour count thing, you can do a neural net thing\nbasically is what we're saying. ",
    "start": "3963260",
    "end": "3970070"
  },
  {
    "text": "For CMIX, the other\none I mentioned, this is very advanced\nideas where what they do",
    "start": "3970070",
    "end": "3976130"
  },
  {
    "text": "is they have thousands\nof different models. You predict based\non the last bit. You predict the last two bits,\nlast three bits, previous word,",
    "start": "3976130",
    "end": "3984380"
  },
  {
    "text": "first letter of previous word,\nlast letter of previous word, whether this word\nis the first letter,",
    "start": "3984380",
    "end": "3989569"
  },
  {
    "text": "first word in the sentence\nor the second word, just a bunch of\ndifferent contexts. And then they have a neural\nnet that mixes all those",
    "start": "3989570",
    "end": "3997250"
  },
  {
    "text": "into a single probability. There is a talk--",
    "start": "3997250",
    "end": "4002320"
  },
  {
    "text": "I know you won't appreciate\nfrom what I just said, but there is a talk from\nlast year's IT forum",
    "start": "4002320",
    "end": "4007570"
  },
  {
    "text": "where the author of\nthis CMIX came to talk. And this is a slide\nthat Pulkit created",
    "start": "4007570",
    "end": "4016540"
  },
  {
    "text": "at some point, which just shows\nover time how has compression for text improved.",
    "start": "4016540",
    "end": "4021700"
  },
  {
    "text": "And this is gzip, bzip2, 7zip.",
    "start": "4021700",
    "end": "4027070"
  },
  {
    "text": "And here we show if you assume\nthat the characters are uniform, it's very bad.",
    "start": "4027070",
    "end": "4032589"
  },
  {
    "text": "Bits per character\nis more than four. If you assume word level some\nmodeling, then it gets lower.",
    "start": "4032590",
    "end": "4041050"
  },
  {
    "text": "And last time, we saw\nsome estimate by Shannon where he did some\nhuman experiment to try to estimate the\nentropy rate of English.",
    "start": "4041050",
    "end": "4047410"
  },
  {
    "text": "And we have, as you see\non the far right side, we have actually\nbeaten that now. With the CMIX and\nNNCP models, we",
    "start": "4047410",
    "end": "4054640"
  },
  {
    "text": "are doing better than Shannon's\nestimate of the entropy rate. So I've really come very\nfar in compressing text.",
    "start": "4054640",
    "end": "4061120"
  },
  {
    "text": "Even though text in many\ncases is not something-- there isn't a lot of text. All the text created by humans\nis a few terabytes or something.",
    "start": "4061120",
    "end": "4070150"
  },
  {
    "text": "It's far from the\nbiggest data source, but I think we are just curious\nto compress text as well",
    "start": "4070150",
    "end": "4075579"
  },
  {
    "text": "as possible.  Any questions? ",
    "start": "4075580",
    "end": "4084430"
  },
  {
    "text": "So if you didn't\nlisten to anything, please listen to this because\nI'm very excited about this.",
    "start": "4084430",
    "end": "4090680"
  },
  {
    "text": "Now remember that\ngood prediction equal to good compression. So today the most powerful\npredictors are these LLM models.",
    "start": "4090680",
    "end": "4098330"
  },
  {
    "text": "So this is new material. We didn't have this last year. So now forget about\nthat MDL setting",
    "start": "4098330",
    "end": "4105080"
  },
  {
    "text": "that we have been working\nwith where we said that the model is very big. That's bad. No.",
    "start": "4105080",
    "end": "4110450"
  },
  {
    "text": "Now we don't worry\nabout storing the model. Just get the best compression,\nbring your biggest model.",
    "start": "4110450",
    "end": "4116750"
  },
  {
    "text": "So throw your most powerful\npredictor at a problem. Gigabytes, hundreds of\ngigabytes of model, that's fine.",
    "start": "4116750",
    "end": "4124930"
  },
  {
    "text": "So in a normal\nsetting like this, this is not useful because\nwhere would you store the model. And obviously if you store\nthat model with the file,",
    "start": "4124930",
    "end": "4131380"
  },
  {
    "text": "that doesn't make sense. Can anyone think of\nany domain or any sort",
    "start": "4131380",
    "end": "4136630"
  },
  {
    "text": "of setting where it's\nfine to have a very big model as a predictor?",
    "start": "4136630",
    "end": "4141729"
  },
  {
    "start": "4141729",
    "end": "4154810"
  },
  {
    "text": "So what is the issue with\nhaving a very big model as the predictor? We said we need\nto store the model as part of the compressed file.",
    "start": "4154810",
    "end": "4162370"
  },
  {
    "text": "But do we? Maybe we don't. Maybe there is a\nworld where everybody",
    "start": "4162370",
    "end": "4168239"
  },
  {
    "text": "has GPT-4 on their mobile or\non their laptop or whatever.",
    "start": "4168240",
    "end": "4174299"
  },
  {
    "text": "And then everybody assumes\nthat everybody else has that huge model\non their machine, and everybody uses\nthat single model",
    "start": "4174300",
    "end": "4180240"
  },
  {
    "text": "to compress things and\nsend it to each other. So there are settings.",
    "start": "4180240",
    "end": "4187259"
  },
  {
    "text": "So if you have a large amount of\ndata of the same type basically and you can afford to\ndeploy the model everywhere,",
    "start": "4187260",
    "end": "4193710"
  },
  {
    "text": "which might become a reality. There are two other\nreasons to do this. One is that we\nwant to understand",
    "start": "4193710",
    "end": "4198870"
  },
  {
    "text": "the limits of compression. How good can your text\nprediction become?",
    "start": "4198870",
    "end": "4203880"
  },
  {
    "text": "And other is just to have\nfun in a compression class. So for me, the\nthird reason really.",
    "start": "4203880",
    "end": "4209250"
  },
  {
    "text": " So basic model is simple. ",
    "start": "4209250",
    "end": "4217099"
  },
  {
    "text": "Predictor gives you compressor. Log loss, so basically\nif you think of LLMs,",
    "start": "4217100",
    "end": "4222320"
  },
  {
    "text": "LLMs are trained as predictors. And their loss function is the\ncross-entropy loss or 2 power the cross-entropy loss.",
    "start": "4222320",
    "end": "4229190"
  },
  {
    "text": "So in effect, they are\nbecoming compressors. They are actually compressors. And I took this recent\nwork by the same person",
    "start": "4229190",
    "end": "4238100"
  },
  {
    "text": "who made an NNCP, who made\nthis text compression using large language models where\nyou can download any model",
    "start": "4238100",
    "end": "4245510"
  },
  {
    "text": "and then use that to\ncompress your favorite text. And I did like a\ncouple of things.",
    "start": "4245510",
    "end": "4251780"
  },
  {
    "text": "So I took a 2023 novel. And I took second order\narithmetic coding.",
    "start": "4251780",
    "end": "4258330"
  },
  {
    "text": "I took bzip2. And I took two models. And I took different\ncontext lengths.",
    "start": "4258330",
    "end": "4263730"
  },
  {
    "text": "So context length is how much\nof the past number of tokens you see.",
    "start": "4263730",
    "end": "4269280"
  },
  {
    "text": "So you see that if your contact\nlength is not very large. If you just look at\nthe last 16 tokens, it does better than bzip2\nbut not very impressive.",
    "start": "4269280",
    "end": "4277489"
  },
  {
    "text": "And the medium size model, which\nis in darker shade of blue, does slightly better\nthan the small model.",
    "start": "4277490",
    "end": "4284480"
  },
  {
    "text": "But it really starts to shine\nwhen you take a long context. That's the thing with these\nlarge language models.",
    "start": "4284480",
    "end": "4290960"
  },
  {
    "text": "They can look at a\nvery big past history and make a very good\nprediction for the next token based on that.",
    "start": "4290960",
    "end": "4296150"
  },
  {
    "text": "We saw it in the earlier\npart of the class if you remember where the\nmore context you give, the better the\nprediction becomes.",
    "start": "4296150",
    "end": "4302090"
  },
  {
    "text": "So once you give it like\na long enough context, it starts getting\nto the point where it's below the Shannon bound,\nvery close to CMIX and NNCP.",
    "start": "4302090",
    "end": "4309889"
  },
  {
    "text": "So it basically the optimal. And this is a very small model. It's a 430 million\nparameter model",
    "start": "4309890",
    "end": "4315199"
  },
  {
    "text": "whereas the GPTs are\ntens of billions or more.",
    "start": "4315200",
    "end": "4320660"
  },
  {
    "text": "Any question on this? You generally\nunderstand the idea. Bigger model, better\ncompression, or I guess, better model,\nbetter compression.",
    "start": "4320660",
    "end": "4327260"
  },
  {
    "text": "More context,\nbetter compression, not counting the model size. ",
    "start": "4327260",
    "end": "4335620"
  },
  {
    "text": "Now it did this. Instead of having this English\nnovel released in 2023,",
    "start": "4335620",
    "end": "4341890"
  },
  {
    "text": "I took a Pali is an\nancient Indian language. I took this text, which was\ntranscribed in Roman script,",
    "start": "4341890",
    "end": "4347829"
  },
  {
    "text": "and I compressed it with,\nagain, all of these guys. Now you see actually\nthe models are not",
    "start": "4347830",
    "end": "4354560"
  },
  {
    "text": "able to beat bzip2 or even gzip. Can anybody tell me why? ",
    "start": "4354560",
    "end": "4362020"
  },
  {
    "text": "Pali is like Sanskrit if\nyou have heard of that. In these languages\nwe're trying to--",
    "start": "4362020",
    "end": "4367540"
  },
  {
    "text": " they're like a universal model. So they're not really trained\nfor this specific thing",
    "start": "4367540",
    "end": "4374500"
  },
  {
    "text": "where using this, we\nwill have the model based on the specific distribution.",
    "start": "4374500",
    "end": "4380969"
  },
  {
    "text": "Yeah. Yeah. Yeah. So the idea is that these LLM\nmodels oftentimes, the training data set is English, especially\nfor these smaller models,",
    "start": "4380970",
    "end": "4387540"
  },
  {
    "text": "the training data set is\nlimited to a certain corpus.",
    "start": "4387540",
    "end": "4393160"
  },
  {
    "text": "And didn't try GPT-4\nbecause I don't have it. It would just take too\nlong, I think, to do. But maybe that will do better.",
    "start": "4393160",
    "end": "4400760"
  },
  {
    "text": "But yes, so gzip, bzip2,\nsecond order arithmetic coding, they don't make any\nassumption about the data.",
    "start": "4400760",
    "end": "4405770"
  },
  {
    "text": "They will work for\nany type of data. So that is the beauty there\nthat they are in a sense gzip",
    "start": "4405770",
    "end": "4411680"
  },
  {
    "text": "and bzip2 are\nuniversal in a sense. They don't care about your data. Just for any type\nof data you give,",
    "start": "4411680",
    "end": "4418010"
  },
  {
    "text": "if it's a big enough\nsample of data, they will learn the distribution\nfrom the data and do a good job. These LLM models, they\nassume that the data",
    "start": "4418010",
    "end": "4424940"
  },
  {
    "text": "is of a certain distribution. And if your data is not from\nthat, then they do worse. ",
    "start": "4424940",
    "end": "4432720"
  },
  {
    "text": "Then Kedar did some\nexperiments where he took an even bigger model,\na Llama 13-billion parameter",
    "start": "4432720",
    "end": "4439740"
  },
  {
    "text": "model on a short story,\nagain, released in 2023. He tried different\ncontext lengths,",
    "start": "4439740",
    "end": "4445500"
  },
  {
    "text": "and he got different\nbits per byte, compressed bits per character.",
    "start": "4445500",
    "end": "4451260"
  },
  {
    "text": "Higher context length, better. Nothing changed until now. I guess, you can see the\nnumber 0.874 is a bit better",
    "start": "4451260",
    "end": "4457260"
  },
  {
    "text": "than the ones we\ngot here presumably because the model is stronger.",
    "start": "4457260",
    "end": "4462300"
  },
  {
    "text": "Llama is a much stronger\nmodel than these ones. Then we tried something else.",
    "start": "4462300",
    "end": "4467440"
  },
  {
    "text": "We tried the same model on\na Sherlock Holmes novel. And you see this number 0.2.",
    "start": "4467440",
    "end": "4474000"
  },
  {
    "text": "So earlier, we were always\ntalking around 1 bits per byte. Suddenly, you are getting\n0.2 bits, 5x more compressed.",
    "start": "4474000",
    "end": "4481470"
  },
  {
    "text": "So this means either two things. One, either Sherlock Holmes or\nArthur Conan Doyle's English",
    "start": "4481470",
    "end": "4486930"
  },
  {
    "text": "was much simpler than this\nmodern short story novels or something else is going on.",
    "start": "4486930",
    "end": "4492480"
  },
  {
    "text": "And any suggestions? Why is the compression ratio so\ngood for this Sherlock Holmes",
    "start": "4492480",
    "end": "4502349"
  },
  {
    "text": "thing on the Llama model? ",
    "start": "4502350",
    "end": "4510630"
  },
  {
    "text": "Do you know how the\nLlama model was created? What did they do to\ncreate the model? ",
    "start": "4510630",
    "end": "4525630"
  },
  {
    "text": "Anybody? Any guesses? They used old books.",
    "start": "4525630",
    "end": "4531170"
  },
  {
    "text": "Yeah, yeah, they used old books. Among other things, they\nlooked at Wikipedia. So Llama was trained on a bunch\nof things, including, I assume,",
    "start": "4531170",
    "end": "4539390"
  },
  {
    "text": "the Sherlock Holmes thing was\npart of the training data set. And therefore, I guess\nit memorized some of it.",
    "start": "4539390",
    "end": "4545580"
  },
  {
    "text": "And it's just doing\nan exceptional job at compressing the\nSherlock Holmes novel. So one needs to be\ncareful when evaluating.",
    "start": "4545580",
    "end": "4553015"
  },
  {
    "text": "That's why all of\nthe times we're using this 2023 thing,\nwhich we assumed was not part of the very\nrecently released things.",
    "start": "4553015",
    "end": "4558995"
  },
  {
    "text": " So last slide, so LLM-based\ncompression remarkable results",
    "start": "4558995",
    "end": "4569020"
  },
  {
    "text": "can be-- it's really we are\npushing it to the limit. So for a compression\nperspective,",
    "start": "4569020",
    "end": "4575350"
  },
  {
    "text": "it's just useful to know\neven if these are not going to be the most practical ones.",
    "start": "4575350",
    "end": "4581594"
  },
  {
    "text": "We need to be careful about\na couple things we saw. One is that the model and the\ndata, if they are not aligned,",
    "start": "4581595",
    "end": "4586630"
  },
  {
    "text": "then you get not\nso good results. And when you're\nevaluating things, just make sure to\nnot use something",
    "start": "4586630",
    "end": "4592960"
  },
  {
    "text": "from the training data. Otherwise, your results\nmight be misleading. Right now they're very slow\nand very compute intensive.",
    "start": "4592960",
    "end": "4599830"
  },
  {
    "text": "But actually the smaller\nmodel that we did that was reasonably fast on a GPU. So it is possible that\nfor some applications,",
    "start": "4599830",
    "end": "4607420"
  },
  {
    "text": "these might actually\nbe used in the future. ",
    "start": "4607420",
    "end": "4612620"
  },
  {
    "text": "Yep. Yep. So something to notice here\nis by LLM compressor's work,",
    "start": "4612620",
    "end": "4619540"
  },
  {
    "text": "irrespective of how\nin machine learning you want to be careful\nabout overfitting.",
    "start": "4619540",
    "end": "4624820"
  },
  {
    "text": "For the compression,\nsay, you come up with a model which is really\noverfitted on whatever you are compressing on and\nthe model is shared,",
    "start": "4624820",
    "end": "4633175"
  },
  {
    "text": "it's actually a good thing. It's not a bad thing. You might want to overfit and\nshare that model everywhere",
    "start": "4633175",
    "end": "4638800"
  },
  {
    "text": "if you can on\nthese specific data sets you would like to work.",
    "start": "4638800",
    "end": "4644260"
  },
  {
    "text": "Why in practice it\nbecomes a hard thing to do because at that point, you\nneed bigger and bigger models.",
    "start": "4644260",
    "end": "4649420"
  },
  {
    "text": "You need higher and\nhigher frequencies. But for compression's\nsake, it might not be bad to overfit and pre-share\nthat model on both sides",
    "start": "4649420",
    "end": "4657400"
  },
  {
    "text": "if you can. That's the context in which\nthis LLM compressors work. Yeah. Yeah. So Pulkit makes a good point\nwhere for compression, it's",
    "start": "4657400",
    "end": "4664800"
  },
  {
    "text": "not necessarily a bad thing. If you actually want to\nsend a Sherlock Holmes novel to another person,\ncompression is good.",
    "start": "4664800",
    "end": "4672030"
  },
  {
    "text": "It doesn't hurt you. Obviously, if your model is\ngoing to learn everything, then the model gets very big.",
    "start": "4672030",
    "end": "4677948"
  },
  {
    "text": "So there are issues there but-- I guess that's why they are\nso good because they know",
    "start": "4677948",
    "end": "4682990"
  },
  {
    "text": "everything from the\npast and still they are able to\ngeneralize very well. Even on the latest data sets,\nthey are state-of-the-art.",
    "start": "4682990",
    "end": "4691420"
  },
  {
    "text": "So a couple resources. One is this ts_zip. So the thing that this one--",
    "start": "4691420",
    "end": "4700550"
  },
  {
    "text": "Kedar run these experiments. It's a very nice\nnotebook with a--",
    "start": "4700550",
    "end": "4707880"
  },
  {
    "text": "you will see how we\ndid the thing where we tried to predict the next-- the thing we did at the start,\npredicting the next token.",
    "start": "4707880",
    "end": "4715469"
  },
  {
    "text": "That's also in this notebook\nand how to do compression. However, this one if you\nactually compress it--",
    "start": "4715470",
    "end": "4722380"
  },
  {
    "text": "oh, yeah, one thing,\njust quick note is that when you're trying\nto evaluate these things, you know that arithmetic\ncoding will give you",
    "start": "4722380",
    "end": "4728829"
  },
  {
    "text": "this log 1 by P thing. So oftentimes, we don't\neven do arithmetic coding. We just calculate\nsummation of log 1 by P,",
    "start": "4728830",
    "end": "4736660"
  },
  {
    "text": "and we assume that's what\narithmetic coding is going to give us because it has a\n2 byte overhead or something, 2 bit overhead.",
    "start": "4736660",
    "end": "4743320"
  },
  {
    "text": "So that's just a useful\nthing to in practice. You don't actually need\nto do arithmetic coding",
    "start": "4743320",
    "end": "4748600"
  },
  {
    "text": "to know how well your compressor\nor predictor is performing, just compute the log loss.",
    "start": "4748600",
    "end": "4754030"
  },
  {
    "text": "And DeepMind had a recent-- DeepMind Google, they\nhad a recent paper called Language Modeling is\nCompression, very nice title.",
    "start": "4754030",
    "end": "4762489"
  },
  {
    "text": "I think it's an\ninteresting read now I think with what you\nhave learned and if you know a little bit about LLM,\nyou will be able to understand",
    "start": "4762490",
    "end": "4768880"
  },
  {
    "text": "that entire paper. So I encourage you\nto take a look.",
    "start": "4768880",
    "end": "4773950"
  },
  {
    "text": "So next lecture,\nwe will do LZ77. And depending on time and\nwith some practical aspects",
    "start": "4773950",
    "end": "4782690"
  },
  {
    "text": "about lossless compression. Thank you. ",
    "start": "4782690",
    "end": "4791000"
  }
]