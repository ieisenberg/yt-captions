[
  {
    "start": "0",
    "end": "6440"
  },
  {
    "text": "Hello, hi, everyone. Welcome back to CS330. And today, we'll\nbe talking about hierarchical reinforcement\nlearning and skill discovery.",
    "start": "6440",
    "end": "13268"
  },
  {
    "text": " Let's see if this works.",
    "start": "13268",
    "end": "18835"
  },
  {
    "text": "All right, so the\nplan for the lecture is as following--\nfirst, we'll discuss why we should work on things\nsuch as hierarchical reinforced",
    "start": "18835",
    "end": "25980"
  },
  {
    "text": "learning and skill discovery. Then, we will talk\na little bit about some information-theoretic\nconcepts",
    "start": "25980",
    "end": "32080"
  },
  {
    "text": "that will come in\nhandy when discussing different approaches. And some of these concepts\nyou might be already",
    "start": "32080",
    "end": "38650"
  },
  {
    "text": "familiar with, such as\nKL-divergence or entropy, but we'll go over\nthem anyway just to make sure that we're\nall on the same page.",
    "start": "38650",
    "end": "46070"
  },
  {
    "text": "Then we'll discuss skill\ndiscovery algorithms. And once we kind of learn\nhow we can discover skills,",
    "start": "46070",
    "end": "52723"
  },
  {
    "text": "we'll also try to\nlearn how we can use them, and different options\nthat we should consider there.",
    "start": "52723",
    "end": "59520"
  },
  {
    "text": "And then, at the\nend, we'll also talk about hierarchical reinforcement\nlearning algorithms.",
    "start": "59520",
    "end": "66049"
  },
  {
    "text": "All right, so let's start with\ndiscussing why skill discovery is something that is important.",
    "start": "66050",
    "end": "71820"
  },
  {
    "text": "And so far in the course,\nwe have been mostly in a scenario where the task was\nsomething that was given to us,",
    "start": "71820",
    "end": "78390"
  },
  {
    "text": "and then we're trying\nto find a policy or come up with\nan algorithm that will be able to solve\nfor that specific task.",
    "start": "78390",
    "end": "87980"
  },
  {
    "text": "All right, and while\nthis is a useful setting, it doesn't really\ncover everything that we might want\nour agents to do.",
    "start": "87980",
    "end": "96650"
  },
  {
    "text": "So in particular, when\nyou consider how we learn, it's not necessarily\nthat we always get the goal, or\nsome kind of task,",
    "start": "96650",
    "end": "103010"
  },
  {
    "text": "that is given to\nus by somebody else that we are trying\nto accomplish. We are also really\ngood at coming up with our own goals\nand our own skills",
    "start": "103010",
    "end": "110420"
  },
  {
    "text": "that we might want to practice. So we want to\nchange the question that we're considering, and we\nwould want our agents to also",
    "start": "110420",
    "end": "118730"
  },
  {
    "text": "be able to discover\ninteresting behaviors without any supervision. So in particular, I\nwanted to show you",
    "start": "118730",
    "end": "125180"
  },
  {
    "text": "this little video of\na baby left completely unattended in a room. And you can see that the baby\nis not given any specific goals,",
    "start": "125180",
    "end": "134270"
  },
  {
    "text": "but it's still pretty good\nat coming up with the goals to practice. And for its own curiosity, it's\nnot only coming up with them,",
    "start": "134270",
    "end": "141830"
  },
  {
    "text": "but then it's also\ntrying to practice them and it's trying to get\nbetter at all of them. So we want to get\nto a situation where",
    "start": "141830",
    "end": "147650"
  },
  {
    "text": "we can leave a robot\nunattended in a room, and that robot will come\nup with anything that there",
    "start": "147650",
    "end": "152690"
  },
  {
    "text": "is to do in that room\nthat is interesting. And then we come\nback and the robot is capable of doing\nmany different skills.",
    "start": "152690",
    "end": "159676"
  },
  {
    "text": "On the right, there is also a\nslightly different motivation, which is based on some of\nthe research that comes more",
    "start": "159676",
    "end": "167740"
  },
  {
    "text": "from neuroscience and biology. And in this particular\npaper that I'm citing here,",
    "start": "167740",
    "end": "174790"
  },
  {
    "text": "researchers were doing\na study on frogs, and they were\ntrying to discover,",
    "start": "174790",
    "end": "181599"
  },
  {
    "text": "or trying to research\nwhat kind of-- how does the frog move and how\ndoes the spinal cord of a frog",
    "start": "181600",
    "end": "187450"
  },
  {
    "text": "modulate all\ndifferent behaviors? And what they found\nout is that there",
    "start": "187450",
    "end": "192610"
  },
  {
    "text": "is a very small\nnumber of core skills that the frog knows\nabout, and then the whole variety of\nmotions that the frog can do",
    "start": "192610",
    "end": "200050"
  },
  {
    "text": "comes from spinal\ncord just modulating these different core\nlevel skills, right?",
    "start": "200050",
    "end": "205090"
  },
  {
    "text": "So there is a small\nnumber of core skills, and these give rise to a\nlarge variety of behaviors.",
    "start": "205090",
    "end": "212440"
  },
  {
    "text": "There is also a similar\nresearch in that vein that was done on human subjects where\nthe researchers were asking",
    "start": "212440",
    "end": "219250"
  },
  {
    "text": "humans to grasp a large\nvariety of objects, and then they would record the\nhand positions of the humans",
    "start": "219250",
    "end": "225970"
  },
  {
    "text": "and try to do some\nanalysis on that data. So they collected this big data\nset with many different humans",
    "start": "225970",
    "end": "231970"
  },
  {
    "text": "grasping many different\nobjects, and then they performed principle component\nanalysis on the dataset.",
    "start": "231970",
    "end": "237250"
  },
  {
    "text": "And they noticed that with just\na few top principle components they can explain a huge\nmajority of the data.",
    "start": "237250",
    "end": "244580"
  },
  {
    "text": "So that also seems to\npoint to something similar where we have a small\nnumber of core skills,",
    "start": "244580",
    "end": "251050"
  },
  {
    "text": "or called behaviors\nthat can then give rise to a large variety of skills.",
    "start": "251050",
    "end": "258500"
  },
  {
    "text": "So one other motivation\nfor skill discovery is that maybe if we leave\nrobots unattended and allow them",
    "start": "258500",
    "end": "263680"
  },
  {
    "text": "to discover the skills\nthemselves rather than us defining what they\nshould be, maybe they will be able to come up\nwith these core skills that",
    "start": "263680",
    "end": "269800"
  },
  {
    "text": "then would allow us to\ngenerate all kinds of things. ",
    "start": "269800",
    "end": "275820"
  },
  {
    "text": "There is also a much\nmore practical motivation for skill discovery, or for\nskill discovery algorithms.",
    "start": "275820",
    "end": "282060"
  },
  {
    "text": "And this is that,\nso far, we talked a lot about multitask\nreinforcement learning,",
    "start": "282060",
    "end": "287370"
  },
  {
    "text": "or meta reinforcement\nlearning, and we always assume that we have a bag of\ntasks that we can pretrain on.",
    "start": "287370",
    "end": "294360"
  },
  {
    "text": "And once you start\nimplementing these things, you very quickly\nrealize that coming up with the task themselves\nis really tricky.",
    "start": "294360",
    "end": "303160"
  },
  {
    "text": "And there are many\ndifferent considerations that you have to think\nabout while you do this.",
    "start": "303160",
    "end": "309100"
  },
  {
    "text": "And very often, one of\nthe limiting factors is that we are having\ntrouble with coming up",
    "start": "309100",
    "end": "315220"
  },
  {
    "text": "with a big enough set of tasks. And this might seem a\nlittle counterintuitive,",
    "start": "315220",
    "end": "320740"
  },
  {
    "text": "so I thought maybe we\ncan do a short exercise. So I'll ask you to\nwrite in the chat--",
    "start": "320740",
    "end": "327250"
  },
  {
    "text": "also feel free to raise your\nhand if you have any questions. I would like you to write down\nsome task ideas for a tabletop",
    "start": "327250",
    "end": "334810"
  },
  {
    "text": "manipulation scenario. And just to make it a\nlittle bit more specific, it's a tabletop\nmanipulation scenario where we have a robot\nwith a single arm that",
    "start": "334810",
    "end": "342430"
  },
  {
    "text": "is in front of a table, and\nit can be given any objects. So can you just write down\nany task that comes to mind?",
    "start": "342430",
    "end": "350500"
  },
  {
    "text": "And just write as many\nas you can think of. ",
    "start": "350500",
    "end": "357639"
  },
  {
    "text": "Grab something, OK. Pick up, stacking object,\npicking up a spoon. OK, we got three pick ups.\nPush down object from a table,",
    "start": "357640",
    "end": "365530"
  },
  {
    "text": "pour milk into a cup, move\nthe arm, dismantle the table. Well, that sounds\nlike a good one.",
    "start": "365530",
    "end": "371110"
  },
  {
    "text": "Read the paper. That would be interesting. Move to a specific location,\npet a dog, rotate the arm,",
    "start": "371110",
    "end": "378670"
  },
  {
    "text": "solve chess. OK. Throw.",
    "start": "378670",
    "end": "383960"
  },
  {
    "text": "Do the Harlem Shake. That's a perfect one. Stand up using support.",
    "start": "383960",
    "end": "390050"
  },
  {
    "text": "I'm not sure what you\nmean by that exactly. ",
    "start": "390050",
    "end": "395740"
  },
  {
    "text": "Oh, I guess it's like a\nhumanoid robot that can stand up using some kind of support.",
    "start": "395740",
    "end": "400940"
  },
  {
    "text": "Take apart a table\njust-- oh, I see. OK, great. ",
    "start": "400940",
    "end": "406860"
  },
  {
    "text": "Roll an object. All right, this is actually\na lot of really good ideas. I might use some of\nthese, so thanks a lot.",
    "start": "406860",
    "end": "413130"
  },
  {
    "text": "These are great. All right, so I think one\nthing to consider here",
    "start": "413130",
    "end": "419070"
  },
  {
    "text": "is that we have a\nlot of things that are kind of close to\neach other, like picking objects or grabbing\nobjects, but we have also",
    "start": "419070",
    "end": "426599"
  },
  {
    "text": "a lot of things that are very\ndifferent levels of difficulty. We had, I think, someone wrote\nto dismantle this-- or assemble",
    "start": "426600",
    "end": "432960"
  },
  {
    "text": "a table. So the difficulty of\nthat task compared to just grasping something\nis very, very different.",
    "start": "432960",
    "end": "441470"
  },
  {
    "text": "And so when we come up with\nall the different tasks that we want the\nrobot to do, we also need to keep in mind the\npractical algorithms that",
    "start": "441470",
    "end": "449310"
  },
  {
    "text": "will try to learn these tasks. So we usually want to\ncome up with a task that are feasible with our\ncurrent RL algorithms that",
    "start": "449310",
    "end": "458210"
  },
  {
    "text": "are similar levels of\ndifficulty so that we can train all of them at the same time. And we also want\nto make them such",
    "start": "458210",
    "end": "464130"
  },
  {
    "text": "that they share a lot\nof similar structure so that if you\ntrain a lot of them, they can kind of benefit\nfrom all the other tasks.",
    "start": "464130",
    "end": "472190"
  },
  {
    "text": "So you're really good at this. I just wanted to\nmention that when we were going through that\nexercise, when we were",
    "start": "472190",
    "end": "478792"
  },
  {
    "text": "developing that\nbenchmark mental world, that was really, really painful. And it was really tricky to\ncome up with this many tasks",
    "start": "478792",
    "end": "485509"
  },
  {
    "text": "and do this such that\neach one of these tasks is feasible by a single\ntask reinforcement",
    "start": "485510",
    "end": "490940"
  },
  {
    "text": "learning algorithm. So especially when you\nwork on these projects, you use that creativity\nthat you just displayed",
    "start": "490940",
    "end": "499220"
  },
  {
    "text": "and come up with as\nmany tasks as you can. And I think if you do\nthis, the algorithms will become much better, too.",
    "start": "499220",
    "end": "505250"
  },
  {
    "text": " All right, so we\ntalked a little bit about why skill discovery\nmight be useful.",
    "start": "505250",
    "end": "510930"
  },
  {
    "text": "So why hierarchical\nreinforcement learning? So far when we were talking\nabout reinforcement learning",
    "start": "510930",
    "end": "516830"
  },
  {
    "text": "algorithms, we were considering\nevery single task to be at the same level\nof abstraction. .",
    "start": "516830",
    "end": "523289"
  },
  {
    "text": "So for example, if we're\ncontrolling the robot at a specific frequency,\nwe would be sending actions",
    "start": "523289",
    "end": "529250"
  },
  {
    "text": "at that frequency. But we might want\nto ask the robot to do tasks of very different\nlevel of abstractions, right?",
    "start": "529250",
    "end": "536600"
  },
  {
    "text": "For example, we can tell the\nrobot to go bake a cheesecake. But we can also be much\nmore specific than that",
    "start": "536600",
    "end": "542420"
  },
  {
    "text": "and say, well, first you\nneed to buy ingredients. And we want to treat these\ntwo a little bit differently.",
    "start": "542420",
    "end": "549110"
  },
  {
    "text": "Also the first step to do that\nwould be to go to the store, or to walk to the\ndoor, take a step,",
    "start": "549110",
    "end": "554390"
  },
  {
    "text": "or contract the specific muscle. And then we would\nwant our algorithms to be able to reason on all\nthese levels of abstractions",
    "start": "554390",
    "end": "560840"
  },
  {
    "text": "such that the lower levels\ncan inform the higher levels. And we don't\nnecessarily want to plan for baking a cheesecake by\nplanning every single muscle",
    "start": "560840",
    "end": "568490"
  },
  {
    "text": "contraction. We want to do it at\na much higher level. And these are the things that\nour hierarchical reinforcement",
    "start": "568490",
    "end": "574260"
  },
  {
    "text": "learning algorithms\nwould allow us to do. In addition to this, it can be\nvery useful with exploration.",
    "start": "574260",
    "end": "582080"
  },
  {
    "text": "So when we are considering some\ntasks that can be much more",
    "start": "582080",
    "end": "587330"
  },
  {
    "text": "long-term tasks-- for example, block stacking. We don't necessarily\nplan in terms",
    "start": "587330",
    "end": "592790"
  },
  {
    "text": "of contracting\nour muscles or how are we going to move\nour finger and so on, but we are thinking on a much\nhigher level of abstraction.",
    "start": "592790",
    "end": "600600"
  },
  {
    "text": "We were just thinking\nabout the objects and where the objects\nshould go, and this allows us to do this planning\nmuch more efficiently",
    "start": "600600",
    "end": "606260"
  },
  {
    "text": "and allows for\nbetter exploration. So we'll also use\nhierarchical RL algorithms",
    "start": "606260",
    "end": "611980"
  },
  {
    "text": "to help us with that.  All right, great.",
    "start": "611980",
    "end": "617450"
  },
  {
    "text": "So this was a little\nbit of a motivation. Are there any questions\nat this point? ",
    "start": "617450",
    "end": "625900"
  },
  {
    "text": "Please, raise your hand if there\nare any questions, then just speak up, all right?",
    "start": "625900",
    "end": "631550"
  },
  {
    "text": "If not, then-- There's a question in the chat,\n\"Can a human-imposed hierarchy",
    "start": "631550",
    "end": "640430"
  },
  {
    "text": "potentially be suboptimal?\" Yeah, definitely, I think so.",
    "start": "640430",
    "end": "646220"
  },
  {
    "text": "So we will also learn\nabout algorithms that try to learn how to\ndo the hierarchy itself.",
    "start": "646220",
    "end": "653310"
  },
  {
    "text": "So other than the low-level\nskills that it should learn and the high-level skills they\nshould learn together with.",
    "start": "653310",
    "end": "659300"
  },
  {
    "text": "So it will be kind of\nan entwined process that will learn both levels.",
    "start": "659300",
    "end": "664700"
  },
  {
    "text": "There is still a little bit\nof human-imposed hierarchy, or human-injected\nknowledge about this being",
    "start": "664700",
    "end": "671570"
  },
  {
    "text": "just two levels of hierarchy,\nas opposed to many more. Or maybe the task\nshouldn't have been",
    "start": "671570",
    "end": "677090"
  },
  {
    "text": "solved in a hierarchical\nfashion in the first place. And I think this is a\nreally interesting question",
    "start": "677090",
    "end": "682130"
  },
  {
    "text": "as to how much is this\nconstraining our algorithms as opposed to helping them. ",
    "start": "682130",
    "end": "691269"
  },
  {
    "text": "All right, so let's jump into\nsome information-theoretic concepts. So there will be a\nlittle bit of theory,",
    "start": "691270",
    "end": "696307"
  },
  {
    "text": "but it should be\nrelatively straightforward. And for some of you,\nthis might not be new,",
    "start": "696307",
    "end": "701458"
  },
  {
    "text": "and you might have\nheard about this before, but we'll go over\nthis really quickly.",
    "start": "701458",
    "end": "706570"
  },
  {
    "text": "All right, so first,\nwe'll talk about entropy. And in order to\nintroduce that concept,",
    "start": "706570",
    "end": "711910"
  },
  {
    "text": "we first need to talk\nabout the distribution. So hopefully, we are all\nfamiliar with distributions.",
    "start": "711910",
    "end": "719740"
  },
  {
    "text": "And here we'll be\nplotting a distribution of a one-dimensional\nvariable, x.",
    "start": "719740",
    "end": "725019"
  },
  {
    "text": "So on the x-axis, we\nhave our different values",
    "start": "725020",
    "end": "730300"
  },
  {
    "text": "that the random\nvariable can take. And then the y-axis,\nwe have the probability of the random variable\ntaking that value.",
    "start": "730300",
    "end": "738000"
  },
  {
    "text": "So then, we can observe\nwhat kind of values the random variable takes. And then we can fit\nthe distribution,",
    "start": "738000",
    "end": "743330"
  },
  {
    "text": "that has to sum up to 1, that\ncorresponds to these events. So for example, a\nGaussian, in this case.",
    "start": "743330",
    "end": "750970"
  },
  {
    "text": "So this is what the\nprobability distribution is. Now, we'll refer to\nentropy as this script h,",
    "start": "750970",
    "end": "758020"
  },
  {
    "text": "and it's an entropy of a\nparticular distribution. And mathematically, it's\ndefined as following--",
    "start": "758020",
    "end": "764780"
  },
  {
    "text": "it's a negative\nlog probability log p of x under the\nexpectation of itself.",
    "start": "764780",
    "end": "772310"
  },
  {
    "text": "So what does that mean? So let's try to think about\nthe terms that we have here.",
    "start": "772310",
    "end": "777510"
  },
  {
    "text": "So that expectation\nhere means that if we sample from our distribution,\nfrom our p of x, so we'll just",
    "start": "777510",
    "end": "785360"
  },
  {
    "text": "sample according to\nthe distribution, if the entropy is very\nhigh, then for the samples",
    "start": "785360",
    "end": "795170"
  },
  {
    "text": "we sample, the log p of x\nshould be quite low, right? So basically, if we want\nto maximize the entropy,",
    "start": "795170",
    "end": "802820"
  },
  {
    "text": "that means we need to minimize\nthis negative term here. So for the samples that we\nsample from our distribution,",
    "start": "802820",
    "end": "809750"
  },
  {
    "text": "the probability mass\nshouldn't be that big-- should be as low as possible. So what does that\nmean in practice?",
    "start": "809750",
    "end": "816210"
  },
  {
    "text": "So in practice that means\nthat if I sample something, that means that\nthat sample doesn't",
    "start": "816210",
    "end": "822529"
  },
  {
    "text": "carry a lot of\nprobability distribution. So basically, it measures how\nbroad our distribution is.",
    "start": "822530",
    "end": "828710"
  },
  {
    "text": "If our distribution is\nmuch broader than something as big as this Gaussian in\nthe picture, for example.",
    "start": "828710",
    "end": "835400"
  },
  {
    "text": "Something that, let's\nsay, looks like this.",
    "start": "835400",
    "end": "842870"
  },
  {
    "text": "That means that we will\nhave a lot of samples that we are likely to sample,\nfor example, a sample here,",
    "start": "842870",
    "end": "848300"
  },
  {
    "text": "or sample there, that don't\nhave as much probability mass as they would have\nhad if we sampled something",
    "start": "848300",
    "end": "854840"
  },
  {
    "text": "from a distribution\nthat is much more peaky. And so if I sample something\nfrom a Gaussian like this,",
    "start": "854840",
    "end": "860250"
  },
  {
    "text": "that means that I'm very\nlikely to get a sample that",
    "start": "860250",
    "end": "865970"
  },
  {
    "text": "is close to that\npeak, and that sample will carry a lot of\nprobability mass right here.",
    "start": "865970",
    "end": "872880"
  },
  {
    "text": "So basically, entropy is\na measure of how random our distribution is, or\nhow much randomness there was in the random variable.",
    "start": "872880",
    "end": "880660"
  },
  {
    "text": "The other way to\nthink about this is to think about it a little\nbit counterintuitively.",
    "start": "880660",
    "end": "886060"
  },
  {
    "text": "So count as our-- an entropy will tell\nus how easily can",
    "start": "886060",
    "end": "893020"
  },
  {
    "text": "you guess what the\nsample will be. So let me give you a little\nbit better example here.",
    "start": "893020",
    "end": "899570"
  },
  {
    "text": "So let's consider a\nBernoulli random variable. So as if we were just\ndoing a coin flip. And here on the x-axis,\nwe'll be plotting",
    "start": "899570",
    "end": "907240"
  },
  {
    "text": "the parameter of the\nBernoulli random variable. So 0.5 means that heads and\ntails are equally likely.",
    "start": "907240",
    "end": "913210"
  },
  {
    "text": "And on the y-axis, we are\nplotting the entropy itself. And here we can see\nthat the entropy is",
    "start": "913210",
    "end": "918870"
  },
  {
    "text": "the highest for the value 0.5. So what that means is that\nit's extremely hard for us",
    "start": "918870",
    "end": "925570"
  },
  {
    "text": "to guess whether the\noutcome of the random event will be tails or heads, if\nthe parameter of the Bernoulli",
    "start": "925570",
    "end": "934990"
  },
  {
    "text": "probability distribution\nwould be 0.5. However, if that probability\ndistribution, that parameter,",
    "start": "934990",
    "end": "940652"
  },
  {
    "text": "the probability of getting\ntails, for instance, is equal to 1.0. That means that\nentropy is very low.",
    "start": "940652",
    "end": "946240"
  },
  {
    "text": "I don't even have to\nperform the experiment to know what I will get, right? From the parameter itself I\ncan basically easily predict",
    "start": "946240",
    "end": "953680"
  },
  {
    "text": "what's going to happen. So it measures a little bit\nhow much randomness there is in this random variable. ",
    "start": "953680",
    "end": "960769"
  },
  {
    "text": "So this is a little\nbit about the entropy. Let's talk about\nthe KL-divergence So you've heard a little\nbit of a KL-divergence,",
    "start": "960770",
    "end": "967470"
  },
  {
    "text": "and we usually have\nbeen introducing it as a distance between\ntwo distributions.",
    "start": "967470",
    "end": "972510"
  },
  {
    "text": "So let's define\nit mathematically. So KL-divergence is DKL between\ntwo distributions, q and p.",
    "start": "972510",
    "end": "980640"
  },
  {
    "text": "It's another symmetric measure\nthat is defined like this-- it's an expectation under\none of those distributions,",
    "start": "980640",
    "end": "987240"
  },
  {
    "text": "under distribution q, of log\nof the ratio between the two distributions.",
    "start": "987240",
    "end": "992400"
  },
  {
    "text": "Log of the ratio between\nq of x and p of x. ",
    "start": "992400",
    "end": "997873"
  },
  {
    "text": "Now, because these\nare logarithms, we can change it\nto look like this.",
    "start": "997873",
    "end": "1003480"
  },
  {
    "text": "So it's the difference between\nthese two expectations. And then we can\nchange it a little bit",
    "start": "1003480",
    "end": "1008630"
  },
  {
    "text": "further and say that this\nis this negative expectation of the log p of x minus\nthe entropy of q of x.",
    "start": "1008630",
    "end": "1015600"
  },
  {
    "text": "So here. I just use the definition\nthat we just introduced, the motion of the entropy.",
    "start": "1015600",
    "end": "1020780"
  },
  {
    "text": "So I transformed this term,\nput the minus in front of it, so that it's entropy, right?",
    "start": "1020780",
    "end": "1026470"
  },
  {
    "text": "So since this is a\ndistance, let's first think about the\nsimplest case where",
    "start": "1026470",
    "end": "1031869"
  },
  {
    "text": "q and p are equal to each other,\nand whether that distance would be 0. We can easily see, for example,\nfrom this part of the equation,",
    "start": "1031869",
    "end": "1038740"
  },
  {
    "text": "here, that this\nwould be the case. So if p was equal to\nq, or q was equal to p,",
    "start": "1038740",
    "end": "1045819"
  },
  {
    "text": "then that expectation\nwould be equal exactly to those expectations,\nand then they",
    "start": "1045819",
    "end": "1051563"
  },
  {
    "text": "would cancel each other out. This would be equal to 0.  All right, so let's take a\nlook at that final formula",
    "start": "1051563",
    "end": "1058240"
  },
  {
    "text": "that we got here. And let's try to understand\nit a little bit better. And in order to\nunderstand this, we'll",
    "start": "1058240",
    "end": "1064630"
  },
  {
    "text": "first plot some probability\ndistribution, p of x. And this is, let's say,\nsome complex probability",
    "start": "1064630",
    "end": "1070510"
  },
  {
    "text": "distribution that\nlooks like this. And now what we'll\ntry to do is we'll",
    "start": "1070510",
    "end": "1075550"
  },
  {
    "text": "try to find probability\ndistribution, q-- that is a Gaussian. We'll try to find the\nparameters of this Gaussian",
    "start": "1075550",
    "end": "1082120"
  },
  {
    "text": "such that the KL-divergence\nbetween q and p is as small as possible. We're trying to find the q that\nminimizes that KL-divergence",
    "start": "1082120",
    "end": "1091630"
  },
  {
    "text": "that is defined here. So because we are\ntrying to minimize this, that means that we have\nto maximize the negative.",
    "start": "1091630",
    "end": "1098860"
  },
  {
    "text": "So we are first maximizing\nthis term right here. So let's just\nfocus on that term.",
    "start": "1098860",
    "end": "1105560"
  },
  {
    "text": "And for this term, in\norder to maximize it, we need to come up\nwith a distribution, q that is extremely peaky\nhere, so that if you",
    "start": "1105560",
    "end": "1115460"
  },
  {
    "text": "sample from the distribution,\nso if we sample from our q-- this is the\nexpectation under q-- the log probability of p of x\nwould be as high as possible.",
    "start": "1115460",
    "end": "1124559"
  },
  {
    "text": "So there's as much\nprobability mass as possible. So this basically\ncorresponds to a distribution",
    "start": "1124560",
    "end": "1129650"
  },
  {
    "text": "that is very, very\nnarrow that looks for a mode in the\ndistribution p of x.",
    "start": "1129650",
    "end": "1136190"
  },
  {
    "text": "It's a mode-seeking behavior. Because if we put\nany probability mass for the distribution q\nof x anywhere else, then",
    "start": "1136190",
    "end": "1143330"
  },
  {
    "text": "the log probability\ndistribution for that sample would be much smaller than it\nwould have been for the mode. ",
    "start": "1143330",
    "end": "1150630"
  },
  {
    "text": "So this is the first term. So it's a mode-seeking\nbehavior that produces an extremely\nnarrow distribution.",
    "start": "1150630",
    "end": "1156445"
  },
  {
    "text": "And then we have the\nsecond term, which is the entropy of our q of x. And we already learned a\nlittle bit about the entropy.",
    "start": "1156445",
    "end": "1163500"
  },
  {
    "text": "So we are trying to make this\nentropy as big as possible, so that the negative would\nbe as small as possible,",
    "start": "1163500",
    "end": "1168780"
  },
  {
    "text": "and that entropy will try to\nmake the distribution wider. So we just talked about how\nentropy measures how broad",
    "start": "1168780",
    "end": "1174270"
  },
  {
    "text": "the distribution is. So that entropy, if we try to\nmake it as big as possible, it will transform our\nmode-seeking distribution",
    "start": "1174270",
    "end": "1180630"
  },
  {
    "text": "into something that\nis a little bit wider. And here, from this picture,\nyou can see kind of the behavior",
    "start": "1180630",
    "end": "1186409"
  },
  {
    "text": "that you will get\nif you're trying to minimize the KL-divergence\nof this form with respect to q.",
    "start": "1186410",
    "end": "1193190"
  },
  {
    "text": "So you will try to find the\nmode of the distribution, and then you'll\ntry to blow it up a little bit so that it\ncovers as much probability",
    "start": "1193190",
    "end": "1199490"
  },
  {
    "text": "mass of p of x as possible.  All right, so this is a\nbit about KL-divergence.",
    "start": "1199490",
    "end": "1206370"
  },
  {
    "text": "So let's introduce the last\nterm that we might find useful in today's lecture, which\nis mutual information.",
    "start": "1206370",
    "end": "1213360"
  },
  {
    "text": "So mutual information we'll\ndenote with this script. I, and is between two random\nvariables, let's say x and y,",
    "start": "1213360",
    "end": "1222370"
  },
  {
    "text": "and mathematically it's\ndefined as following-- it's a KL-divergence between\nthe joint distribution and the product\nat the marginals.",
    "start": "1222370",
    "end": "1230550"
  },
  {
    "text": "So we can write it\nout fully like this. But instead of analyzing\nit in this form,",
    "start": "1230550",
    "end": "1236070"
  },
  {
    "text": "I'll just try to give\nyou two examples of what mutual information\nis, and then we'll",
    "start": "1236070",
    "end": "1242267"
  },
  {
    "text": "change the formula\na little bit to get more intuition of how we can\nthink about mutual information.",
    "start": "1242267",
    "end": "1249360"
  },
  {
    "text": "So here we have two\nrandom variables, x and y, and we're plotting\ndifferent samples",
    "start": "1249360",
    "end": "1256010"
  },
  {
    "text": "from these distributions. And now, what mutual\ninformation is trying to measure",
    "start": "1256010",
    "end": "1262549"
  },
  {
    "text": "is how much is x dependent\non y, and vice versa. So if you knew about x, how\nmuch would you know about y?",
    "start": "1262550",
    "end": "1269840"
  },
  {
    "text": "So in this plot right\nhere, what we can say is that if we knew\nwhat value of x we had,",
    "start": "1269840",
    "end": "1277940"
  },
  {
    "text": "then we can fairly\nconfidently predict what y we are going to get. The dependence between\nx and y is very high.",
    "start": "1277940",
    "end": "1285230"
  },
  {
    "text": "So this is an example of\na high mutual information where x and y are\ndependent on each other.",
    "start": "1285230",
    "end": "1292300"
  },
  {
    "text": "Here in the bottom\nplot, the plot looks a little bit differently,\nbut regardless of what",
    "start": "1292300",
    "end": "1298647"
  },
  {
    "text": "x-- what value of x the\nrandom variable it picks. y is not really changing. So even if you\nknew about x, that",
    "start": "1298647",
    "end": "1305080"
  },
  {
    "text": "wouldn't really\nhelp you in guessing what the y is going to be. So in this case, x\nand y are independent,",
    "start": "1305080",
    "end": "1311230"
  },
  {
    "text": "and therefore, there's\nlow mutual information between these two\nrandom variables.",
    "start": "1311230",
    "end": "1318110"
  },
  {
    "text": "All right, so let's change\nthat formula to be about right here, something that looks\na little bit closer to what",
    "start": "1318110",
    "end": "1323870"
  },
  {
    "text": "we've seen before. And in particular, this is\nactually a somewhat symmetric",
    "start": "1323870",
    "end": "1331490"
  },
  {
    "text": "measure, and it tells you\nthe mutual information is equal to the entropy\nof the first variable",
    "start": "1331490",
    "end": "1337580"
  },
  {
    "text": "minus the entropy of the\nconditional distribution of p of x given y.",
    "start": "1337580",
    "end": "1342919"
  },
  {
    "text": "So what does that mean? So we know intuitively\nthat mutual information measures the dependency\nbetween these two variables.",
    "start": "1342920",
    "end": "1350610"
  },
  {
    "text": "So let's try to analyze this. So mutual information would\nbe high if that term is high.",
    "start": "1350610",
    "end": "1356700"
  },
  {
    "text": "So that means that\nthe entropy of p of x is really high, so\nyou can't really tell what x you're\ngoing to get if you",
    "start": "1356700",
    "end": "1363110"
  },
  {
    "text": "sample from the distribution. It's a very random\ndistribution, it's very wide.",
    "start": "1363110",
    "end": "1368390"
  },
  {
    "text": "So x on itself has high entropy. But then, for the mutual\ninformation to be high,",
    "start": "1368390",
    "end": "1374000"
  },
  {
    "text": "this term needs to be very low. So that means that the\nentropy of p of x given y",
    "start": "1374000",
    "end": "1380160"
  },
  {
    "text": "needs to be very low. So if I just ask you to\npredict what x is going to be,",
    "start": "1380160",
    "end": "1385982"
  },
  {
    "text": "you would have no idea. It would be very, very hard. But if I told you what\ny you get to experience,",
    "start": "1385982",
    "end": "1392650"
  },
  {
    "text": "what y is, then you would be\nable to very confidently tell me what x is going to be-- there\nis a high dependence between y",
    "start": "1392650",
    "end": "1399370"
  },
  {
    "text": "and x. So x by itself is very\nrandom, but as soon as you know about y,\nyou know what x will be. ",
    "start": "1399370",
    "end": "1406539"
  },
  {
    "text": "All right, so let's go\nfor a little exercise to see whether this makes sense. So I'll give you an example of\ntwo random variables, x and y,",
    "start": "1406540",
    "end": "1413769"
  },
  {
    "text": "and I'll ask you whether they\nhave high mutual information or low mutual information. So in the chat, you can\njust write high or low.",
    "start": "1413770",
    "end": "1422730"
  },
  {
    "text": "All right, so here we go. We have one random variable\nthat says it will rain tomorrow, it rains tomorrow, and then\nanother random variable that",
    "start": "1422730",
    "end": "1430320"
  },
  {
    "text": "says streets are wet tomorrow. So is it high\nmutual information? OK, yeah, high, high, high.",
    "start": "1430320",
    "end": "1437750"
  },
  {
    "text": "Great, yes. These are very dependent\nrandom variables. And basically, you can say that\nthe entropy of whether it's",
    "start": "1437750",
    "end": "1445250"
  },
  {
    "text": "going to rain tomorrow\nor not is actually fairly high-- we have no idea\nwhether it's going to rain or not.",
    "start": "1445250",
    "end": "1451520"
  },
  {
    "text": "I guess, maybe in the Bay\nArea it's a little different. But still. But if we knew that\nstreets are wet tomorrow,",
    "start": "1451520",
    "end": "1458537"
  },
  {
    "text": "it will be very easy for us to\ntell whether it's going to rain or not. All right, let's go\nfor another example.",
    "start": "1458537",
    "end": "1465110"
  },
  {
    "text": "So x is still it rains tomorrow,\nand y is we find life on Mars tomorrow. Is it high or low\nmutual information?",
    "start": "1465110",
    "end": "1473422"
  },
  {
    "text": "All right, so it's fairly\nstraightforward, yep. It's low mutual\ninformation, because even",
    "start": "1473422",
    "end": "1479390"
  },
  {
    "text": "though the entropy of p\nof x is still fairly high, we can't really tell whether\nit's going to rain tomorrow",
    "start": "1479390",
    "end": "1484520"
  },
  {
    "text": "or not. The additional information\nthat we are getting about y, that we find life\non Mars tomorrow,",
    "start": "1484520",
    "end": "1490608"
  },
  {
    "text": "is not really going to\nhelp us guess whether it's going to rain tomorrow or not. ",
    "start": "1490608",
    "end": "1496967"
  },
  {
    "text": "All right, great. So this is a little\nbit more abstract, so I'll give you a particular\nexample of mutual information that is actually used in\nreinforcement learning,",
    "start": "1496967",
    "end": "1505890"
  },
  {
    "text": "and this is called empowerment. And the empowerment\nis mutual information",
    "start": "1505890",
    "end": "1512700"
  },
  {
    "text": "between the next state\nand the current action. So does anybody have any idea\nwhy this is called empowerment?",
    "start": "1512700",
    "end": "1521620"
  },
  {
    "text": "Please, just speak up, or\nyou can write in the chat, or raise your hand,\nwhatever works for you.",
    "start": "1521620",
    "end": "1526690"
  },
  {
    "start": "1526690",
    "end": "1536759"
  },
  {
    "text": "All right, any ideas? \"The current action basically\ndetermines next state, the impact of the action.\"",
    "start": "1536760",
    "end": "1542310"
  },
  {
    "text": "Yes, exactly. Right. So that means that\nthe empowerment would be very high\nif your actions have",
    "start": "1542310",
    "end": "1549007"
  },
  {
    "text": "a lot of control over\nthe state that you're going to observe next. So if your actions\nare very powerful, if they have a lot of\ninfluence on the next state,",
    "start": "1549008",
    "end": "1556289"
  },
  {
    "text": "that means that you are a\nhighly empowered individual. All right, great. And we-- a lot of people use\nthat measure as something",
    "start": "1556290",
    "end": "1564600"
  },
  {
    "text": "to optimize for, or\nsomething to measure how capable the agents are. ",
    "start": "1564600",
    "end": "1572740"
  },
  {
    "text": "All right, so we\ndid a little review of some information-theoretic\nconcepts. Are there any questions\nat this point?",
    "start": "1572740",
    "end": "1578440"
  },
  {
    "start": "1578440",
    "end": "1587425"
  },
  {
    "text": "All right, okay. So let's talk about some\nskill discovery algorithms.",
    "start": "1587426",
    "end": "1593220"
  },
  {
    "text": "So one thing that we would\nwant to do in skill discovery is to find algorithms\nthat don't necessarily",
    "start": "1593220",
    "end": "1600440"
  },
  {
    "text": "solve one particular\ntask, but algorithms that can solve many,\nmany different tasks, and they can come up with\nthe tasks themselves.",
    "start": "1600440",
    "end": "1608160"
  },
  {
    "text": "So one algorithm that goes\ntowards this direction, that we already talked about, is the\nsoft Q-learning algorithm.",
    "start": "1608160",
    "end": "1616320"
  },
  {
    "text": "And in soft Q-learning, the\nsetting we're considering was still where we had a\ntask that was given to us.",
    "start": "1616320",
    "end": "1623809"
  },
  {
    "text": "For example, a dog that\nhas to come to the owner. But its behavior is a\nlittle more stochastic-- it doesn't just go\nin a straight line,",
    "start": "1623810",
    "end": "1630200"
  },
  {
    "text": "but it's a little more\nstochastic than that. And in particular, the objective\nthat the agent was optimizing,",
    "start": "1630200",
    "end": "1638039"
  },
  {
    "text": "like this. And here we had the\nadditional entropy term that we just discussed.",
    "start": "1638040",
    "end": "1644790"
  },
  {
    "text": "And then we had value in Q\nfunctions, as we described. And then we had a standard\nQ-learning algorithm, and we made some\nmodifications to it,",
    "start": "1644790",
    "end": "1652670"
  },
  {
    "text": "resulting in a soft\nQ-learning algorithm that allowed us to be\na little bit less committed than a\nstandard algorithm would.",
    "start": "1652670",
    "end": "1661252"
  },
  {
    "text": "And then we looked at\nsome of the results here that allowed us\nto not be as committed to a single solution, as you\nare, for example, in this case",
    "start": "1661252",
    "end": "1669290"
  },
  {
    "text": "where our policy is just picking\nthe mode of our Q function and always trying\nto do those actions,",
    "start": "1669290",
    "end": "1674840"
  },
  {
    "text": "and instead will put a little\nbit of probability mass for our policy, even\nfor the actions that are not perfectly optimal.",
    "start": "1674840",
    "end": "1682700"
  },
  {
    "text": "So this is something that goes\na little bit in the direction we want, in that it can do\nmore things that just",
    "start": "1682700",
    "end": "1688490"
  },
  {
    "text": "one particular test, because\nat least it can do it in many different ways",
    "start": "1688490",
    "end": "1694260"
  },
  {
    "text": "But this is not\nexactly what we need. This is still a\nlittle bit different. So what we would need\nwould be a policy",
    "start": "1694260",
    "end": "1702539"
  },
  {
    "text": "that would have a form that\nwould look something like that. So it would be a policy\npi of a, given s,",
    "start": "1702540",
    "end": "1709710"
  },
  {
    "text": "as we usually do, but it\nwill be conditional on one additional random variable,\nand that random variable will",
    "start": "1709710",
    "end": "1716310"
  },
  {
    "text": "denote this task index, or\nsome kind of skill index. So we want the agent\nto do different things",
    "start": "1716310",
    "end": "1722460"
  },
  {
    "text": "depending on the value of z. And that z could\nbe just an integer.",
    "start": "1722460",
    "end": "1727960"
  },
  {
    "text": "So for instance, if\nwe have an agent that is performing different\ntasks in this little grid,",
    "start": "1727960",
    "end": "1735180"
  },
  {
    "text": "that the agent starts right\nhere in the middle of it,",
    "start": "1735180",
    "end": "1740528"
  },
  {
    "text": "with different z's, we\nwould want the agent to go to different places,\nor to visit different parts of the state-space.",
    "start": "1740528",
    "end": "1746950"
  },
  {
    "text": "So for example, if\nours z is equal to 0, maybe the agent would move up. If z is equal to 1, it\nwill move to the right.",
    "start": "1746950",
    "end": "1754200"
  },
  {
    "text": "If it's equal to\n2, then the agent would explore this\nbottom right corner.",
    "start": "1754200",
    "end": "1759210"
  },
  {
    "text": "3 would be a different\ntrajectory, 4 different trajectory,\nand 5 would be a different trajectory, too. ",
    "start": "1759210",
    "end": "1767760"
  },
  {
    "text": "We want a policy such that\nwith different task indices, that policy will do\ncompletely different things. ",
    "start": "1767760",
    "end": "1775230"
  },
  {
    "text": "And we can't really use just Max\nEntropy RL to learn something like this, because first\nof all, action entropy",
    "start": "1775230",
    "end": "1781669"
  },
  {
    "text": "is not the same\nas state entropy. So we can have an agent that\ncan act very differently",
    "start": "1781670",
    "end": "1788330"
  },
  {
    "text": "in a specific state, and may\ntake very different actions. But that doesn't\nnecessarily mean that it",
    "start": "1788330",
    "end": "1793519"
  },
  {
    "text": "will land in different states. It can land in very\nsimilar states.",
    "start": "1793520",
    "end": "1798780"
  },
  {
    "text": "And in addition to this,\nour max entropy policies, so the soft pillaring\npolicies, are stochastic,",
    "start": "1798780",
    "end": "1806070"
  },
  {
    "text": "but they're not controllable. So we can't very easily pick\nwhat kind of solution we want.",
    "start": "1806070",
    "end": "1811790"
  },
  {
    "text": "Versus here, we would want\nto have a full control for our z variable to tell\nthe agent what kind of skill,",
    "start": "1811790",
    "end": "1817683"
  },
  {
    "text": "what kind of test we\nwant it to perform.  So intuitively,\nwhat we would want",
    "start": "1817683",
    "end": "1824770"
  },
  {
    "text": "is something that results in\nlow diversity for a fixed scene. So if I told the agent what\nkind of task index I want,",
    "start": "1824770",
    "end": "1831880"
  },
  {
    "text": "we want the agent\nto know what to do. But high diversity\nacross different z's.",
    "start": "1831880",
    "end": "1838190"
  },
  {
    "text": "So if I didn't\ntell what z I want, we want the agent that can visit\nany part of this state-space.",
    "start": "1838190",
    "end": "1844342"
  },
  {
    "text": " So the intuition is that\ndifferent skills should visit",
    "start": "1844342",
    "end": "1849860"
  },
  {
    "text": "different state-space regions. So let's try to come up\nwith an algorithm that could do something like that.",
    "start": "1849860",
    "end": "1855860"
  },
  {
    "text": " So in order to\nlearn this policy,",
    "start": "1855860",
    "end": "1861420"
  },
  {
    "text": "we would need to learn some\nkind of reward function that is fully automated.",
    "start": "1861420",
    "end": "1866520"
  },
  {
    "text": "So that reward function\nwill be in addition to being conditional\non state, it",
    "start": "1866520",
    "end": "1871620"
  },
  {
    "text": "will be also conditional\non the skill itself. So given what skill\nyou are doing, the reward function will\nbe a little bit different,",
    "start": "1871620",
    "end": "1878640"
  },
  {
    "text": "but we want it to be\nautomated-- we don't really want to come up with a reward\nfunction for every single task that we have engineered.",
    "start": "1878640",
    "end": "1885960"
  },
  {
    "text": "But we want the agent\nto discover the z's itself, and then have\nthe automated reward function for that specific z.",
    "start": "1885960",
    "end": "1891960"
  },
  {
    "text": " The objective of\nthe agent would be",
    "start": "1891960",
    "end": "1897500"
  },
  {
    "text": "to just maximize the reward-- but ultimately a reward that\nwe haven't come up with yet-- across all the different z's.",
    "start": "1897500",
    "end": "1903590"
  },
  {
    "text": " So here's one idea how we can\ndo this, what we can propose",
    "start": "1903590",
    "end": "1911100"
  },
  {
    "text": "for the reward function-- so we'll reward the states that\nare unlikely for other z's.",
    "start": "1911100",
    "end": "1917460"
  },
  {
    "text": "And so what does\nthat mean, exactly? So if our reward is\nrewarding every state",
    "start": "1917460",
    "end": "1925170"
  },
  {
    "text": "that the current z can visit,\nthen none of the other z's can, that would mean that we are\ncovering the state-space fairly",
    "start": "1925170",
    "end": "1932910"
  },
  {
    "text": "well, and that skill is\nfairly distinct from all the other skills. And so for example, if\nour z was equal to 2,",
    "start": "1932910",
    "end": "1941315"
  },
  {
    "text": "let's say we would\nwant the agent to discover this part\nof the state-space, and this is a different\npart of the state-space",
    "start": "1941315",
    "end": "1946740"
  },
  {
    "text": "that the agent would\nhave discovered had we given it any different z. So either this one or that one,\nor so on So that idea, at least",
    "start": "1946740",
    "end": "1958360"
  },
  {
    "text": "on the surface, makes sense. So how could we formalize\nit a little bit more?",
    "start": "1958360",
    "end": "1966050"
  },
  {
    "text": "So we reward the states that\nare unlikely for other z's. So here's one proposal\nwhat we can do--",
    "start": "1966050",
    "end": "1972890"
  },
  {
    "text": "we can learn a classifier\nthat, given the state, tells us what z\ncould have resulted",
    "start": "1972890",
    "end": "1979760"
  },
  {
    "text": "in us visiting that state. So we can easily train\na classifier like this.",
    "start": "1979760",
    "end": "1985510"
  },
  {
    "text": "That classifier can be trained\nusing supervised learning, and it will be just\nlooking at the states",
    "start": "1985510",
    "end": "1991299"
  },
  {
    "text": "that a particular skill\nhas visited and will try to regress towards that skill.",
    "start": "1991300",
    "end": "1996600"
  },
  {
    "text": "And then we can have\nthe reward function that tries to help this\nclassifier as much as possible.",
    "start": "1996600",
    "end": "2002450"
  },
  {
    "text": "So it tries to make the skills\na little bit more diverse, such as this classifier\nhave a very easy job.",
    "start": "2002450",
    "end": "2010420"
  },
  {
    "text": "So let's go through\na little example so that it makes a\nlittle more sense. So in the standard\nRL setting, we'll",
    "start": "2010420",
    "end": "2017140"
  },
  {
    "text": "have the policy that produces\nactions in the environment, and then gets the state as\nan input, and keeps going.",
    "start": "2017140",
    "end": "2023140"
  },
  {
    "text": "Here, we'll have an\nadditional input, which is the skill that\nwe're currently doing.",
    "start": "2023140",
    "end": "2028630"
  },
  {
    "text": "But we'll also have\nthis discriminator, this classifier,\nthat, given a state, will try to tell us what z could\nhave accomplished that state,",
    "start": "2028630",
    "end": "2036400"
  },
  {
    "text": "could have ended\nup in that state. And these two will be\ninteracting with each other.",
    "start": "2036400",
    "end": "2043640"
  },
  {
    "text": "And so in practice, it\nwould work like this-- we would start with sampling\ntwo different z's and giving it",
    "start": "2043640",
    "end": "2049780"
  },
  {
    "text": "to our policy, and we'll get\ntwo different trajectories. So, let's say, the\ntrajectory in green",
    "start": "2049780",
    "end": "2055600"
  },
  {
    "text": "is what's produced for z equals\n0, and the trajectory in blue was produced for z equals 1.",
    "start": "2055600",
    "end": "2061540"
  },
  {
    "text": "And they're very\nclose to each other, but they are slightly different. So now, in the next\nstep, we'll try",
    "start": "2061540",
    "end": "2067988"
  },
  {
    "text": "to learn a classifier that tries\nto learn that decision boundary between these two trajectories.",
    "start": "2067989",
    "end": "2073460"
  },
  {
    "text": "So it's trying to do\nthe best job it can. These trajectories that\nare fairly entangled, so it's a little tricky.",
    "start": "2073460",
    "end": "2079089"
  },
  {
    "text": "But it's trying to separate\nthe green trajectory from the blue trajectory. And let's say it came\nup with a decision",
    "start": "2079090",
    "end": "2085419"
  },
  {
    "text": "boundary that looks like this. Well, that's not really\nperfect, but it's the best job it could do.",
    "start": "2085420",
    "end": "2091460"
  },
  {
    "text": "So now we have a policy\nthat, given that classifier, is now this discriminator,\nwe start trying to help it.",
    "start": "2091460",
    "end": "2098925"
  },
  {
    "text": "It's trying to make the\nnext iteration for it a little bit easier. So given the\ndecision boundary, it will try to produce\nthe trajectories",
    "start": "2098925",
    "end": "2105200"
  },
  {
    "text": "in the next generation that\nwill make the current classifier a little bit better,\nbecause we are rewarding",
    "start": "2105200",
    "end": "2111110"
  },
  {
    "text": "our policy for maximizing the\nlikelihood of that classifier,",
    "start": "2111110",
    "end": "2117680"
  },
  {
    "text": "so to produce two trajectories\nthat are more different. And then we'll do another\niteration of the classifier,",
    "start": "2117680",
    "end": "2124400"
  },
  {
    "text": "so the classifier will learn\na little bit better decision boundary. And then we will\ncontinue doing so,",
    "start": "2124400",
    "end": "2130430"
  },
  {
    "text": "so then our policy will try\nto help the classifier again, and will produce trajectories\nthat are even different. And then we'll get better\ndecision boundary, and so on.",
    "start": "2130430",
    "end": "2137960"
  },
  {
    "text": " So overall, we see that there\nis this two player game where",
    "start": "2137960",
    "end": "2145779"
  },
  {
    "text": "both players are going\ntowards the same goal, where",
    "start": "2145780",
    "end": "2150790"
  },
  {
    "text": "the discriminator is trying\nto do the best job possible trying to classify which skills\nwould produce certain states.",
    "start": "2150790",
    "end": "2159970"
  },
  {
    "text": "And then the policy is trying\nto make the classifier's job as easy as possible.",
    "start": "2159970",
    "end": "2165880"
  },
  {
    "text": "So does that make sense? Are there any questions\nat this point? ",
    "start": "2165880",
    "end": "2178540"
  },
  {
    "text": "So let's look at some\nexamples of an algorithm that does exactly this, and let's\ntry to see what kind of task",
    "start": "2178540",
    "end": "2184480"
  },
  {
    "text": "it will learn. So, first the authors\ntrained this algorithm on the cheetah task, where\nwe have a cheetah that--",
    "start": "2184480",
    "end": "2193140"
  },
  {
    "text": "usually, the task of the\ncheetah is to run forward as fast as possible. But here, the only\nreward of this optimizing",
    "start": "2193140",
    "end": "2198549"
  },
  {
    "text": "is to try to help\nthat classifier, so it's trying to\nproduce skills that are as diverse as possible,\nso that the classifier has",
    "start": "2198550",
    "end": "2204270"
  },
  {
    "text": "a very easy job. And here are some examples of\nthe skills that came up with. So one of them is\ncheetah running forward,",
    "start": "2204270",
    "end": "2211022"
  },
  {
    "text": "but then we have\nsome other skills, such as cheetah doing backflips,\nor cheetah running backwards.",
    "start": "2211022",
    "end": "2217570"
  },
  {
    "text": "There's also a\ndifferent environment using exactly the\nsame algorithm, where we have the ant that\nis trying to learn",
    "start": "2217570",
    "end": "2224190"
  },
  {
    "text": "a whole bunch of\ndifferent behaviors by optimizing this\ndiversity objective.",
    "start": "2224190",
    "end": "2229210"
  },
  {
    "text": "And here we have a particular\nexample of a mountain car, where one of the solutions\nthat just comes out",
    "start": "2229210",
    "end": "2235440"
  },
  {
    "text": "from this diversity\nobjective is actually to just solve the task,\nand this is the one on the other at the very top.",
    "start": "2235440",
    "end": "2242309"
  },
  {
    "text": " So we discuss a little\nbit the intuition",
    "start": "2242310",
    "end": "2248079"
  },
  {
    "text": "behind this, why we would want\nto come up with a classifier, and so on. But we also introduced this\nconcept of mutual information.",
    "start": "2248080",
    "end": "2256170"
  },
  {
    "text": "So let's try to see if\nthere's some connection between the algorithm\nthat we just developed",
    "start": "2256170",
    "end": "2261740"
  },
  {
    "text": "and the concept of\nmutual information.  What we've written\nhere is everything",
    "start": "2261740",
    "end": "2267860"
  },
  {
    "text": "that's been discussed so far. Let's consider a specific\nmutual information, which is the mutual information\nbetween our skill index,",
    "start": "2267860",
    "end": "2276190"
  },
  {
    "text": "or task index, and\nthe current state. ",
    "start": "2276190",
    "end": "2281310"
  },
  {
    "text": "Using the definition\nof mutual information that we talked about, it would\nlook something like this.",
    "start": "2281310",
    "end": "2288520"
  },
  {
    "text": "Now, the first term\nwill be maximized if we are just using a maximum\nuniform prior over our skills.",
    "start": "2288520",
    "end": "2294070"
  },
  {
    "text": "So basically, if we want to\nmaximize the entropy of our z, that means that we need to\nbe able to sample every skill",
    "start": "2294070",
    "end": "2300880"
  },
  {
    "text": "with the same probability. So if our z is just a\ncategorical random variable,",
    "start": "2300880",
    "end": "2307380"
  },
  {
    "text": "it should be distributed\nuniformly such that every skill has a chance to go.",
    "start": "2307380",
    "end": "2312940"
  },
  {
    "text": "And then, the\nsecond term here is minimized by maximizing our log\nprobability of p of z given s.",
    "start": "2312940",
    "end": "2323270"
  },
  {
    "text": "So this is by maximizing\nour classifier, by trying to help our classifier\nas much as possible, right?",
    "start": "2323270",
    "end": "2329539"
  },
  {
    "text": "So that basically says\nthat given the state, I should be able to tell what\nskill produced that state.",
    "start": "2329540",
    "end": "2336680"
  },
  {
    "text": "Our policy is trying\nto minimize this term by maximizing that log\nprobability of the classifier.",
    "start": "2336680",
    "end": "2342920"
  },
  {
    "text": " All right, so this is a skill\ndiscovery algorithm that",
    "start": "2342920",
    "end": "2348430"
  },
  {
    "text": "was trying to\ndiscover skills that are as diverse as possible, and\nthey might not be very useful,",
    "start": "2348430",
    "end": "2353735"
  },
  {
    "text": "but at least they try to cover\nas much of the state-space as possible. Are there any questions\nat this point?",
    "start": "2353735",
    "end": "2360297"
  },
  {
    "text": "There's a question\nin the chat that's asking, \"is there an\noptimal number of z that covers the entire\nstate-space while not",
    "start": "2360298",
    "end": "2367390"
  },
  {
    "text": "overlapping between\ndifferent z?\" Right, so the overlapping\npart is hopefully",
    "start": "2367390",
    "end": "2374170"
  },
  {
    "text": "taken care of by the\nalgorithm itself, so regardless of what\nthe number of z's is,",
    "start": "2374170",
    "end": "2380590"
  },
  {
    "text": "the algorithm will\ntry to-- as long as you sample them\nuniformly-- the algorithm will try to cover or\nmake sure that they",
    "start": "2380590",
    "end": "2388210"
  },
  {
    "text": "don't overlap with each other. Now, is there an optimal\nnumber of z's to cover",
    "start": "2388210",
    "end": "2393730"
  },
  {
    "text": "the entire state-space? I think this is a\nvery tricky question, and we don't really\nhave an answer to this.",
    "start": "2393730",
    "end": "2399955"
  },
  {
    "text": " So I think we would need\nto have some kind of way",
    "start": "2399955",
    "end": "2405970"
  },
  {
    "text": "of describing a\nstate-space in a way that we can discuss\nits coverage.",
    "start": "2405970",
    "end": "2411310"
  },
  {
    "text": " And overall, I don't think\nwe have an answer to this.",
    "start": "2411310",
    "end": "2417850"
  },
  {
    "text": "So in practice,\nwhat we would do is we would set z to be a\ncategorical random variable",
    "start": "2417850",
    "end": "2423670"
  },
  {
    "text": "that can take values\nfrom 1 to a big number, and then we'll see what\nkind of skill it produces.",
    "start": "2423670",
    "end": "2429970"
  },
  {
    "text": "But I don't think there is any-- or I haven't seen\nany research that has been done on\nhow big the z should",
    "start": "2429970",
    "end": "2437350"
  },
  {
    "text": "be to cover the state-space. I think this is a\nvery good question but very tricky to answer.",
    "start": "2437350",
    "end": "2444710"
  },
  {
    "text": "All right, thanks.  All right, so now,\nlet's talk a little bit",
    "start": "2444710",
    "end": "2451060"
  },
  {
    "text": "about how we can use these\ndiscovered skills, right? So far, we discussed how\nwe can learn a policy that,",
    "start": "2451060",
    "end": "2461020"
  },
  {
    "text": "given different z values\nthat could be just integers, will produce\ndifferent behaviors.",
    "start": "2461020",
    "end": "2466930"
  },
  {
    "text": "But we don't want to just\nlearn a diverse set of skills, we want to learn them such\nthat we can then later use them to do some kind of task",
    "start": "2466930",
    "end": "2474650"
  },
  {
    "text": "So for example, you\nmight want to learn some kind of way\nof using them such",
    "start": "2474650",
    "end": "2479780"
  },
  {
    "text": "as we can perform a\nnavigation task, where we have that ant that has to\nnavigate to different points. ",
    "start": "2479780",
    "end": "2487010"
  },
  {
    "text": "So one question is, how can\nwe use the learned skills to accomplish a new task that\nis given to us at test time?",
    "start": "2487010",
    "end": "2494470"
  },
  {
    "text": "Are there any ideas here? Raise your hand or\nwrite in the chat. ",
    "start": "2494470",
    "end": "2507890"
  },
  {
    "text": "So just to repeat\nthe question, we have a low-level policy\nthat can do different things dependent on the value of z.",
    "start": "2507890",
    "end": "2514970"
  },
  {
    "text": "So for different z's, it will\ndo very different things. Now, we want to\nfind the policy that",
    "start": "2514970",
    "end": "2521420"
  },
  {
    "text": "is able to reuse these skills\nto do a particular task. All right, you can\noptimize the z sequence.",
    "start": "2521420",
    "end": "2528230"
  },
  {
    "text": "Yeah. So we can learn another policy\non top of it that operates on z's.",
    "start": "2528230",
    "end": "2534290"
  },
  {
    "text": "So now, instead of the standard\nactions being our actions a, that policy will be optimizing\nfor the perfect z sequence",
    "start": "2534290",
    "end": "2542990"
  },
  {
    "text": "to accomplishing a task, right? And this is actually\nwhat's done in the paper",
    "start": "2542990",
    "end": "2548010"
  },
  {
    "text": "that we were just\ndiscussing, which is called Diversity\nis All You Need, that was written by\nBen Eysenbach et al.",
    "start": "2548010",
    "end": "2556380"
  },
  {
    "text": "And they try to then\nrepurpose the skills that they already\nlearned to learn tasks that are more useful.",
    "start": "2556380",
    "end": "2565320"
  },
  {
    "text": "And they showed results\nshowing, quite convincingly, that you can learn a policy, a\nhigh-level policy that operates",
    "start": "2565320",
    "end": "2572670"
  },
  {
    "text": "on those z's, and\nit can find what's the best way to use these\nskills to accomplish a particular task.",
    "start": "2572670",
    "end": "2580270"
  },
  {
    "text": "All right, so this seems like\na good idea, this could work. But can we do better than this?",
    "start": "2580270",
    "end": "2588490"
  },
  {
    "text": "And when we think about\nhow we could do better, let's try to think a\nlittle bit what are",
    "start": "2588490",
    "end": "2595060"
  },
  {
    "text": "the problems of this solution. So one problem\nthat we can see is",
    "start": "2595060",
    "end": "2600530"
  },
  {
    "text": "that the skills that the\nalgorithm comes up with might not be\nparticularly useful. So it might be just an ant\ndoing kind of random motions,",
    "start": "2600530",
    "end": "2608420"
  },
  {
    "text": "random motor babbling. But as long as that\nbehavior is different than the other behavior, it will\nstill be perfectly acceptable,",
    "start": "2608420",
    "end": "2616040"
  },
  {
    "text": "but it won't be really\nuseful for something that we might want the ant\nto do a little bit later. ",
    "start": "2616040",
    "end": "2623280"
  },
  {
    "text": "And in addition to this,\nif you think about learning an additional policy that\noperates on these latent",
    "start": "2623280",
    "end": "2628710"
  },
  {
    "text": "variable z's-- on these task indices, z-- it's not very easy to\nuse these learned skills.",
    "start": "2628710",
    "end": "2638100"
  },
  {
    "text": "We need to run another\nreinforcement learning algorithm that will\nlearn that policy, and that will take many samples,\nand RL is tricky in general.",
    "start": "2638100",
    "end": "2645069"
  },
  {
    "text": "So it's not that you\ncan just very quickly use these skills\nstraight away and kind of",
    "start": "2645070",
    "end": "2650280"
  },
  {
    "text": "be done with a new task. And so we'll start with\nfocusing on that first question,",
    "start": "2650280",
    "end": "2656550"
  },
  {
    "text": "right here, that skills might\nnot be particularly useful, and trying to think of ways how\nwe can make them more useful.",
    "start": "2656550",
    "end": "2663640"
  },
  {
    "text": "And then we can see\nhow that results in a partial answer to the\nsecond question, as well,",
    "start": "2663640",
    "end": "2668715"
  },
  {
    "text": "right? So here's a question to you--",
    "start": "2668715",
    "end": "2674420"
  },
  {
    "text": "what do you think\nmakes a useful skill? So right now, we're just coming\nup with very diverse skills.",
    "start": "2674420",
    "end": "2679910"
  },
  {
    "text": "How can we constrain\nthis a little bit more and say that skills\nhave to be useful?",
    "start": "2679910",
    "end": "2684990"
  },
  {
    "text": "And please remember that we\ndon't have access to any tasks, we're doing this\ncompletely unsupervised,",
    "start": "2684990",
    "end": "2690860"
  },
  {
    "text": "where the agent has to\ndiscover the skills themselves. Are there any ideas to\nwhat makes a useful skill?",
    "start": "2690860",
    "end": "2698060"
  },
  {
    "text": "Just write in the chat\nor raise your hand. ",
    "start": "2698060",
    "end": "2707279"
  },
  {
    "text": "And this is actually a\nvery tricky question, so don't feel bad if there's\nsomething that comes to mind.",
    "start": "2707280",
    "end": "2714930"
  },
  {
    "text": "But I will give you\nanother 30 seconds or so. ",
    "start": "2714930",
    "end": "2722208"
  },
  {
    "text": "State-action spaces are\ndifferent from other skills, right. Yes, so state-action spaces are\ndifferent from other skills.",
    "start": "2722208",
    "end": "2728730"
  },
  {
    "text": "This is similar to\nthe objective that we had before where it's optimizing\nfor diversity of skills, right?",
    "start": "2728730",
    "end": "2734670"
  },
  {
    "text": "So as long as they're diverse,\nthey should be useful. That's a very good\nobjective, and it",
    "start": "2734670",
    "end": "2740280"
  },
  {
    "text": "results in a useful\nalgorithm, as we just saw. But can we do a\nlittle bit better? Can we make the\nskills more useful?",
    "start": "2740280",
    "end": "2746040"
  },
  {
    "text": "It can be combined to something. OK, that's a really good idea. Maybe the skill is\nperformed over and over",
    "start": "2746040",
    "end": "2751260"
  },
  {
    "text": "during the optimization process. Skills that utilizes\ndifferent parts of motions.",
    "start": "2751260",
    "end": "2758359"
  },
  {
    "text": "OK, yeah, I think these\nare some good ideas here. Great, so let me\ntry to convince you",
    "start": "2758360",
    "end": "2765650"
  },
  {
    "text": "to one particular property that\nwould help us to make skills",
    "start": "2765650",
    "end": "2771410"
  },
  {
    "text": "a little bit more useful. And I want to show you two\nexamples of things that are not",
    "start": "2771410",
    "end": "2778290"
  },
  {
    "text": "necessarily very useful. So we have an ant that is\ndoing kind of random motions, and it's not really clear\nwhat's going to happen.",
    "start": "2778290",
    "end": "2784840"
  },
  {
    "text": "And then we have this kid\nthat is playing with Jenga,",
    "start": "2784840",
    "end": "2790290"
  },
  {
    "text": "and also the consequences\nare kind of catastrophic, but more importantly,\nit's kind of very tricky to predict what's\ngoing to happen.",
    "start": "2790290",
    "end": "2798650"
  },
  {
    "text": "And then, here are some examples\nof skills that are very useful. So at the bottom,\nwe can see the ant",
    "start": "2798650",
    "end": "2804349"
  },
  {
    "text": "that is just running\nforward as fast as possible. And up top, we can see\nthis cartoon character",
    "start": "2804350",
    "end": "2810080"
  },
  {
    "text": "juggling really well. So I will make an\nassumption that what",
    "start": "2810080",
    "end": "2815810"
  },
  {
    "text": "makes a useful skill\nis how hard or how easy it is to predict the\nconsequences of performing",
    "start": "2815810",
    "end": "2822440"
  },
  {
    "text": "that skill. So the skill won't be\nuseful if the consequences are very hard to predict.",
    "start": "2822440",
    "end": "2829160"
  },
  {
    "text": "And we'll assume that\nthe skill is useful, so you are more skillful\nif the consequences",
    "start": "2829160",
    "end": "2834530"
  },
  {
    "text": "of performing that skill\nare very easy to predict. So this might sound a little\nnot very straightforward by why",
    "start": "2834530",
    "end": "2844352"
  },
  {
    "text": "that is, but let's\njust run with it and kind of see what\nhappens if we do that. ",
    "start": "2844352",
    "end": "2850869"
  },
  {
    "text": "All right, so we want to bake\nin that notion of predictability to help our skills to be\na little bit more useful,",
    "start": "2850870",
    "end": "2857980"
  },
  {
    "text": "and in order to do this, we will\nintroduce a slightly different mutual information. So before, we are optimizing\nmutual information",
    "start": "2857980",
    "end": "2864160"
  },
  {
    "text": "between our task index or\nthe skill index and state, s.",
    "start": "2864160",
    "end": "2870230"
  },
  {
    "text": "Now, we'll be introducing\na slightly different mutual information, that is, a\nconditional mutual information.",
    "start": "2870230",
    "end": "2876230"
  },
  {
    "text": "And very quickly,\njust to tell you what conditional\nmutual information is, this was the definition of our\noriginal mutual information.",
    "start": "2876230",
    "end": "2882890"
  },
  {
    "text": "Now, the conditional\nmutual information is just we condition everything\non the conditional variable, z,",
    "start": "2882890",
    "end": "2888850"
  },
  {
    "text": "for instance, all right?  So the particular\nmutual information",
    "start": "2888850",
    "end": "2894587"
  },
  {
    "text": "that we'll be looking at is the\nconditional mutual information between the next state\ngiven the current state, and the z given the\ncurrent state, all right?",
    "start": "2894587",
    "end": "2903030"
  },
  {
    "text": "We can write it out like this. Now, what does this mean? So we have the first entropy\nterm that is telling us",
    "start": "2903030",
    "end": "2911340"
  },
  {
    "text": "that we want the entropy\nof s prime given s to be as high as possible.",
    "start": "2911340",
    "end": "2916350"
  },
  {
    "text": "And then we have the\nsecond entropy term that tells us that we want s\nprime given s and the skill that we are doing to be\nas small as possible.",
    "start": "2916350",
    "end": "2925210"
  },
  {
    "text": "So the first term\ncorresponds to a future that is very hard to predict\nfor many different skills.",
    "start": "2925210",
    "end": "2930430"
  },
  {
    "text": "So if we don't know what\nskill you're performing, if you start at a\ncertain state, s,",
    "start": "2930430",
    "end": "2935730"
  },
  {
    "text": "it will be very hard to\nguess which state you're going to go to next. Because we don't know what\nskill you're performing,",
    "start": "2935730",
    "end": "2941155"
  },
  {
    "text": "it's very tricky to tell\nwhat's going to happen. But as soon as we\nknow what z is,",
    "start": "2941155",
    "end": "2947040"
  },
  {
    "text": "it should be very\npredictable. it should be very easy to\ntell which state you are going to end up in for\na given skill, all right?",
    "start": "2947040",
    "end": "2952567"
  },
  {
    "text": " So this kind of includes that\nnotion of predictability,",
    "start": "2952567",
    "end": "2958160"
  },
  {
    "text": "because we have\nthat s prime here, so it tells us a little\nbit about how predictable",
    "start": "2958160",
    "end": "2963380"
  },
  {
    "text": "is the future. And then we can write\nit out mathematically, and doing a few approximations\nthat the result looks",
    "start": "2963380",
    "end": "2974420"
  },
  {
    "text": "like this, where we have the\nnumerator that we're trying to maximize, we're\ntrying to find",
    "start": "2974420",
    "end": "2980150"
  },
  {
    "text": "the log probability of this\nq phi, s prime given s, z.",
    "start": "2980150",
    "end": "2985190"
  },
  {
    "text": "So we want that log probability\nto be as high as possible. So given the skill, we want\nthe future to be predictable.",
    "start": "2985190",
    "end": "2991640"
  },
  {
    "text": "And then in the denominator,\nwe would have something that we're trying to make\nas small as possible,",
    "start": "2991640",
    "end": "2996839"
  },
  {
    "text": "and that means that for\nall the other skills, it should be very\nhard to predict what s prime is going to be. All right, so what\nyou can see here",
    "start": "2996840",
    "end": "3007090"
  },
  {
    "text": "is that we have this\nterm, and also this term here, that is a probability\nof s prime given s, z.",
    "start": "3007090",
    "end": "3015280"
  },
  {
    "text": "So this is something that\nwe need to learn here. So let's think a little\nbit about what this is.",
    "start": "3015280",
    "end": "3023560"
  },
  {
    "text": "So this is the term\nthat we are considering. And if you look at this, this\nactually reminds us of a term",
    "start": "3023560",
    "end": "3029790"
  },
  {
    "text": "that we've seen before,\nwhich looks like this. So this is a very\nsimilar looking term,",
    "start": "3029790",
    "end": "3035850"
  },
  {
    "text": "except instead of\nz's, we have a's. And this term was what we\ncall the transition model,",
    "start": "3035850",
    "end": "3043250"
  },
  {
    "text": "or dynamics model,\nthat we're trying to learn, for example, from\nmodel-based reinforcement learning.",
    "start": "3043250",
    "end": "3048770"
  },
  {
    "text": "Now, the term up\nhere is very similar, except it's not taking\nactions as input,",
    "start": "3048770",
    "end": "3054050"
  },
  {
    "text": "but it's taking the\nskill, z, as input. So this is kind of\nlike a dynamics model,",
    "start": "3054050",
    "end": "3059530"
  },
  {
    "text": "but not for an action,\nbut for an entire skill. So it's trying to tell us,\ngiven the specific skill, what is the next state that\nI'm going to end up in.",
    "start": "3059530",
    "end": "3068859"
  },
  {
    "text": "Another one particular\naspect of this that makes it a little bit\neasier which is our skills,",
    "start": "3068860",
    "end": "3075609"
  },
  {
    "text": "our z's itself are\nalso trying to help this model in a similar\nway to how we were trying",
    "start": "3075610",
    "end": "3081730"
  },
  {
    "text": "to help our classifier before. Now, our skills are\noptimized to help this model as much as possible.",
    "start": "3081730",
    "end": "3087880"
  },
  {
    "text": "So skills are\noptimized specifically to make the skill dynamic. So they're trying to make\nlearning of this model",
    "start": "3087880",
    "end": "3095950"
  },
  {
    "text": "as easy as possible. So what that means is that,\nhopefully, our skills will",
    "start": "3095950",
    "end": "3101427"
  },
  {
    "text": "avoid situations where it is\nvery hard to predict what's going to happen next,\nbecause if you're just",
    "start": "3101427",
    "end": "3106460"
  },
  {
    "text": "doing random motions, it might\nbe really hard to predict what's going to happen. And then we'll focus on things\nthat are very easy to predict,",
    "start": "3106460",
    "end": "3113840"
  },
  {
    "text": "so that learning of this\nq is as easy as possible. And that would\ncorrespond to skills that are a little\nbit more useful,",
    "start": "3113840",
    "end": "3119370"
  },
  {
    "text": "such as running forward.  All right, so summing\nall of this together,",
    "start": "3119370",
    "end": "3125510"
  },
  {
    "text": "we arrive at an algorithm\ncalled DADS, that was introduced",
    "start": "3125510",
    "end": "3131920"
  },
  {
    "text": "by Archit Sharma et al. And the algorithm\nworks as following--",
    "start": "3131920",
    "end": "3137920"
  },
  {
    "text": "so we first\ninitialize our policy and we initialize\nour previously what we discussed as\na classifier, now",
    "start": "3137920",
    "end": "3143640"
  },
  {
    "text": "is the skill\ndynamics model, q pi. Now, we sample a skill from\nsome prior distribution,",
    "start": "3143640",
    "end": "3150863"
  },
  {
    "text": "and that prior\ndistribution should be as uniform as possible. In this case, I'll\ndo it as a Gaussian, but it's going to be just\na uniform distribution.",
    "start": "3150863",
    "end": "3157830"
  },
  {
    "text": "We sample those skills, and then\nwe collect M on policy samples.",
    "start": "3157830",
    "end": "3162910"
  },
  {
    "text": "So if we're given every z,\nwe'll give it to a robot, and we will tell the robot to do\nsomething according to that z.",
    "start": "3162910",
    "end": "3170680"
  },
  {
    "text": "Now, once we have\nthese trajectories, we can now update our\nskill dynamics model, q,",
    "start": "3170680",
    "end": "3179869"
  },
  {
    "text": "given the data. So this is similar to learning\na dynamics model, just in the skill space.",
    "start": "3179870",
    "end": "3185240"
  },
  {
    "text": "And then, once we\nhave that model, we can now compute the\nreward, because that reward",
    "start": "3185240",
    "end": "3190310"
  },
  {
    "text": "is based on that model. So we compute the reward,\nand once we have the reward,",
    "start": "3190310",
    "end": "3198700"
  },
  {
    "text": "we can update the policy\ngiven the state transitions",
    "start": "3198700",
    "end": "3204339"
  },
  {
    "text": "that we got and the reward\nthat we receive for them. And we can do this\nusing any RL algorithm. And I believe that\nin this paper,",
    "start": "3204340",
    "end": "3211040"
  },
  {
    "text": "the authors use the CC\nalgorithm to do that. ",
    "start": "3211040",
    "end": "3216230"
  },
  {
    "text": "And then we do this repeatedly\nin the loop until it converges. So we get a policy, now, that\ndoes different predictable",
    "start": "3216230",
    "end": "3223880"
  },
  {
    "text": "things for different z's. And then we'll do\nanother set of rollouts, and we try to move towards\nthat direction again",
    "start": "3223880",
    "end": "3231660"
  },
  {
    "text": "So in terms of some\nresults, so first, if we look at the trajectories\nthat an ant robot is producing,",
    "start": "3231660",
    "end": "3240660"
  },
  {
    "text": "compared to Diversity\nIs All You Need, so here's a plot of\ntrajectories that you",
    "start": "3240660",
    "end": "3246120"
  },
  {
    "text": "get if you set different z's\nfor Diversity Is All You Need, or DIAYN. So they're a little bit\nmessy, but they still",
    "start": "3246120",
    "end": "3253440"
  },
  {
    "text": "result in different behaviors. However, if you do it for\nDADS, these trajectories",
    "start": "3253440",
    "end": "3259360"
  },
  {
    "text": "are kind of much\nmore consistent, and they go in very\ndifferent directions. So it's a little bit less messy.",
    "start": "3259360",
    "end": "3266030"
  },
  {
    "text": "And we can see very\nsimilar results on the humanoid where it's just\na much more complicated robot.",
    "start": "3266030",
    "end": "3271970"
  },
  {
    "text": "But it's still kind of very\nconsistent in what direction it's trying to\ngo, and the skills are a bit more useful,\nbecause they're",
    "start": "3271970",
    "end": "3277183"
  },
  {
    "text": "trying to be predictable.  All right, so far we've changed\nthe algorithm a little bit",
    "start": "3277183",
    "end": "3285380"
  },
  {
    "text": "so that we'll have skills\nthat are more predictable. So how can we use\nthese learned skills?",
    "start": "3285380",
    "end": "3293640"
  },
  {
    "text": "And as you've also learned\nabout this a little bit in the model-based RL lectures,\nwhere once you have the model,",
    "start": "3293640",
    "end": "3302520"
  },
  {
    "text": "you can use that model to\ndo model-based planning. So here we can do\nsomething very similar,",
    "start": "3302520",
    "end": "3308310"
  },
  {
    "text": "which is use skill dynamics for\nmodel-based planning, instead. And we can do this using any\nmodel-based planning technique.",
    "start": "3308310",
    "end": "3315860"
  },
  {
    "text": "And what that\nusually corresponds to is us sampling different z's,\ndifferent trajectories of z's,",
    "start": "3315860",
    "end": "3323780"
  },
  {
    "text": "then doing imaginary\nrollouts using our model, and then seeing\nwhat kind of reward",
    "start": "3323780",
    "end": "3329540"
  },
  {
    "text": "we are getting for each\none of these trajectories, and then updating our\nplanner based on that.",
    "start": "3329540",
    "end": "3337930"
  },
  {
    "text": "So that's really\nimportant, because what that allows us to do is that\nbecause we have that skill dynamics model that\nwe use for something",
    "start": "3337930",
    "end": "3345700"
  },
  {
    "text": "a little bit different,\nbut now this model is very useful to also learn\nsomething at test time",
    "start": "3345700",
    "end": "3351220"
  },
  {
    "text": "where we don't need any\nadditional rollouts, right? We can use just model-based\nplanning and the skill dynamics",
    "start": "3351220",
    "end": "3356890"
  },
  {
    "text": "to do imaginary rollouts. And based on this, we can figure\nout which skills we should use and how we should\ncompose them such",
    "start": "3356890",
    "end": "3363138"
  },
  {
    "text": "that we can\naccomplish a new task.  All right, but here, we are\nplanning for skills rather than",
    "start": "3363138",
    "end": "3370190"
  },
  {
    "text": "for actions, and that also\nmeans that our task can be learned zero-shot, so we\ndon't need any additional data.",
    "start": "3370190",
    "end": "3376680"
  },
  {
    "text": "And the others show\nsome results indicating that that's possible. So the result that\nI showed before,",
    "start": "3376680",
    "end": "3382098"
  },
  {
    "text": "of an ant navigating to\ndifferent points on that board, actually comes from this\npaper, and it was learned",
    "start": "3382098",
    "end": "3388290"
  },
  {
    "text": "without any additional data. And the same can be done\nfor a humanoid, as well. ",
    "start": "3388290",
    "end": "3395660"
  },
  {
    "text": "All right, so to\nsummarize that part, we talked about two\ndifferent skill discovery algorithms that used some\nform of mutual information.",
    "start": "3395660",
    "end": "3404110"
  },
  {
    "text": "One of them, the DADS\nalgorithm, uses predictability as a proxy for\nusefulness of a skill.",
    "start": "3404110",
    "end": "3411572"
  },
  {
    "text": "And then that particular\nmethod optimizes for both predictability\nof the skills, as well as for\ndiversity, and that",
    "start": "3411572",
    "end": "3416850"
  },
  {
    "text": "allows us to do model-based\nplanning in skill-space. And more importantly, the family\nof skill discovery algorithms",
    "start": "3416850",
    "end": "3424579"
  },
  {
    "text": "is actually quite large. And there are different\npapers that talk about this. And one exciting\npart about this is",
    "start": "3424580",
    "end": "3430760"
  },
  {
    "text": "that not only we can\ndiscover new skills, now, but we can also\napply to something like meta-reinforcement\nlearning,",
    "start": "3430760",
    "end": "3436520"
  },
  {
    "text": "where it opens up new avenues. So we can potentially do\nmeta-reinforcement learning",
    "start": "3436520",
    "end": "3442040"
  },
  {
    "text": "in a way that is completely\nunsupervised, where we first run a skill discovery\nalgorithm to discover skills",
    "start": "3442040",
    "end": "3447440"
  },
  {
    "text": "that are useful, that\nwe meta trained on this, and we keep going. And this is described a\nlittle bit more in this paper",
    "start": "3447440",
    "end": "3454520"
  },
  {
    "text": "by Abhishek Gupta et al on\nUnsupervised Meta-Learning for Reinforcement Learning.",
    "start": "3454520",
    "end": "3460890"
  },
  {
    "text": "All right, so that summarizes\nthe skill discovery part of the lecture. Are there any questions\nat this point?",
    "start": "3460890",
    "end": "3467031"
  },
  {
    "text": "There are a few\nquestions in the chat. The first question is,\nhow do you optimize q phi?",
    "start": "3467031",
    "end": "3474940"
  },
  {
    "text": "How do you optimize q phi? So this is a supervised\nlearning problem at this point,",
    "start": "3474940",
    "end": "3480310"
  },
  {
    "text": "similar to how we were\ntraining the classifier before. Now, q phi is the\nskill dynamics model",
    "start": "3480310",
    "end": "3488410"
  },
  {
    "text": "that is trying to predict\nthe next state given the current state and the z.",
    "start": "3488410",
    "end": "3493780"
  },
  {
    "text": "So we have all three\nof these, right? We have the next state from\nour rollouts, we have the z that we send to the robot,\nand we have the state",
    "start": "3493780",
    "end": "3501349"
  },
  {
    "text": "that the robot was in, so we\ncan apply supervised learning to this problem to learn that\nmodel as well as possible.",
    "start": "3501350",
    "end": "3510700"
  },
  {
    "text": "The next question\nis, is q of s prime given s, z similar to the\nmeasure of empowerment",
    "start": "3510700",
    "end": "3517270"
  },
  {
    "text": "but taken over a horizon\nwith actions induced by z? Right, yes.",
    "start": "3517270",
    "end": "3523559"
  },
  {
    "text": "So there is a very\nstrong connection between the empowerment\nand the measure that was introduced here,\nexcept here we are operating",
    "start": "3523560",
    "end": "3529380"
  },
  {
    "text": "on z's rather than on actions. And I encourage you to take\na look at the DADS paper. In the appendix of\nthe paper, there's",
    "start": "3529380",
    "end": "3535800"
  },
  {
    "text": "a little bit of\nmotivation behind this that describes the connection\nto the empowerment, as well.",
    "start": "3535800",
    "end": "3542670"
  },
  {
    "text": " We learn skills that are\nnot locomotion, for example,",
    "start": "3542670",
    "end": "3548450"
  },
  {
    "text": "for object manipulations. So it's a very\ninteresting question.",
    "start": "3548450",
    "end": "3554470"
  },
  {
    "text": "So usually these skill\ndiscovery algorithms have been shown on\nthings that are--",
    "start": "3554470",
    "end": "3560779"
  },
  {
    "text": "things such as locomotion, where\nit's relatively easy to come to a useful skill, right?",
    "start": "3560780",
    "end": "3565880"
  },
  {
    "text": "So for an ant, for\nexample, as soon as you start moving in a\ncertain direction, that produces some kind of diversity, and that\nskill is immediately useful,",
    "start": "3565880",
    "end": "3573170"
  },
  {
    "text": "because then you can\nuse it for navigation. With manipulation, the\nproblem becomes a little bit more tricky, because for us to\ndiscover a useful manipulation",
    "start": "3573170",
    "end": "3581750"
  },
  {
    "text": "skill, you probably need\nto be very targeted, and go to a specific object, and\ndo something with that object, and so on.",
    "start": "3581750",
    "end": "3587220"
  },
  {
    "text": "And these tend to be much\nmore difficult than something like the locomotion scenario.",
    "start": "3587220",
    "end": "3593070"
  },
  {
    "text": "So I haven't seen many examples\nof these algorithms being applied to manipulation\nskills, but I think this is a great\nopen avenue for research",
    "start": "3593070",
    "end": "3602090"
  },
  {
    "text": "or for future work.  Is there a way to reduce\nsimilarity of skills,",
    "start": "3602090",
    "end": "3608240"
  },
  {
    "text": "learn movement in\nrotation instead of many similar\ntypes of movement that work in slightly\ndifferent directions?",
    "start": "3608240",
    "end": "3613760"
  },
  {
    "text": " I see, is there a way to\nreduce similarity of skills?",
    "start": "3613760",
    "end": "3620980"
  },
  {
    "text": "Right, so if I just go back\nin the slides real quick,",
    "start": "3620980",
    "end": "3632280"
  },
  {
    "text": "when we were introducing\nthe DADS algorithm,",
    "start": "3632280",
    "end": "3638130"
  },
  {
    "text": "we were learning a\nparticular classifier that",
    "start": "3638130",
    "end": "3643319"
  },
  {
    "text": "was looking at this, right here,\nthis log of p of z given s,",
    "start": "3643320",
    "end": "3648570"
  },
  {
    "text": "and this was given\nthe current state. It turns out that\nyou can condition it on many different things.",
    "start": "3648570",
    "end": "3654680"
  },
  {
    "text": "It can be not only\nthe current state, but it could be the full\ntrajectory, for example, of all the states that\nthat skill visited,",
    "start": "3654680",
    "end": "3660589"
  },
  {
    "text": "or it could be just the final\nstate that it ended up in. And the resulting\ntrajectories-- the diversity",
    "start": "3660590",
    "end": "3666280"
  },
  {
    "text": "of these trajectories will\nbe a little bit different. So you would measure their\ndiversity with respect to the entire trajectory\nrather than the single state.",
    "start": "3666280",
    "end": "3674800"
  },
  {
    "text": "And one particular\nversion of this is introduced in this\nvery English paper called Variational Intrinsic\nControl, but I guess, overall,",
    "start": "3674800",
    "end": "3682270"
  },
  {
    "text": "by controlling what this\nclassifier takes as input, you can control the\ndiversity, and the resulting diversity of skills.",
    "start": "3682270",
    "end": "3688109"
  },
  {
    "start": "3688110",
    "end": "3695110"
  },
  {
    "text": "All right, so let me just\nget back to where we were.",
    "start": "3695110",
    "end": "3701130"
  },
  {
    "text": "And I wanted to also\ntalk a little bit about hierarchical\nreinforcement learning.",
    "start": "3701130",
    "end": "3706440"
  },
  {
    "text": "So far, we talked quite a bit\nabout how we can learn skills,",
    "start": "3706440",
    "end": "3712230"
  },
  {
    "text": "and then how we can use these\nskills via high-level policy, but we don't necessarily need to\ndiscover these skills in order",
    "start": "3712230",
    "end": "3718880"
  },
  {
    "text": "to have this hierarchical\nstructure, where the high-level policy is able\nto use some low-level skills. So let's talk about that.",
    "start": "3718880",
    "end": "3726060"
  },
  {
    "text": "So we talk a little bit about\nwhy hierarchical reinforcement learning might be useful, how it\nwill allow us to perform tasks",
    "start": "3726060",
    "end": "3732435"
  },
  {
    "text": "at various level\nof abstractions, and how it can be\nuseful for exploration. And hierarchical reinforcement\nlearning is a very vast field.",
    "start": "3732435",
    "end": "3740980"
  },
  {
    "text": "And it will be really\ntricky to kind of introduce all the different\nworks that people",
    "start": "3740980",
    "end": "3748690"
  },
  {
    "text": "wrote about hierarchical\nreinforcement learning. So instead, I wanted\nto show a few axes on how you can design a\nhierarchical reinforcement",
    "start": "3748690",
    "end": "3756400"
  },
  {
    "text": "learning algorithm, and then\nshow you examples of that. And then we'll based\non each example, we'll be trying to say\nwhat kind of design choices",
    "start": "3756400",
    "end": "3763330"
  },
  {
    "text": "the authors took. All right, so let's build\na standard hierarchical",
    "start": "3763330",
    "end": "3769000"
  },
  {
    "text": "reinforcement\nlearning algorithm. And it looks\nsomething like that. So we have a low-level\npolicy, pi l.",
    "start": "3769000",
    "end": "3777310"
  },
  {
    "text": "And this is a standard\npolicy that we are learning, a standard flat reinforcement\nlearning, as well. It interacts with\nthe environment,",
    "start": "3777310",
    "end": "3783650"
  },
  {
    "text": "it takes a state from\nit, and then produces a certain action that\ngoes into the environment, and gets the next\nstate, produces",
    "start": "3783650",
    "end": "3788820"
  },
  {
    "text": "the next action, and so on. So far, this is just a standard\nreinforcement learning.",
    "start": "3788820",
    "end": "3794920"
  },
  {
    "text": "Now, we'll introduce\nanother policy, a high-level policy, pi h.",
    "start": "3794920",
    "end": "3800140"
  },
  {
    "text": "That policy will take\nthe initial state, and that policy will command\nthe low-level policy.",
    "start": "3800140",
    "end": "3807190"
  },
  {
    "text": "So in particular, it will send\na command, z1, in this case. And that command\nwill stay constant",
    "start": "3807190",
    "end": "3813630"
  },
  {
    "text": "for a certain period\nof time, right? So the low-level policy\nis getting its goal",
    "start": "3813630",
    "end": "3818640"
  },
  {
    "text": "from the high-level policy,\nand it's the same goal for a number of time steps.",
    "start": "3818640",
    "end": "3824700"
  },
  {
    "text": "Then, after the\nlow-level policy is done, we'll take the next state,\nthe high-level policy",
    "start": "3824700",
    "end": "3830460"
  },
  {
    "text": "will receive that state, and\nthen it will do it again, but it will send a\ndifferent command to the low-level policy.",
    "start": "3830460",
    "end": "3835810"
  },
  {
    "text": " All right, so there are\nnow a lot of design choices",
    "start": "3835810",
    "end": "3841200"
  },
  {
    "text": "that can be made here. So first, the commands\nthat the high-level policy",
    "start": "3841200",
    "end": "3847260"
  },
  {
    "text": "is sending to the\nlow-level policy can be a goal, a goal\nthat is defined as part of the state-space or not.",
    "start": "3847260",
    "end": "3854290"
  },
  {
    "text": "So that means that the\nlow-level policy can be either goal-conditioned or not.",
    "start": "3854290",
    "end": "3859780"
  },
  {
    "text": "So if it's not\ngoal-conditioned, we can be sending kind\nof any commands that then the\nlow-level policy needs",
    "start": "3859780",
    "end": "3865559"
  },
  {
    "text": "to learn how to interpret, such\nas, maybe, a natural language command, or some\nkind of latent space",
    "start": "3865560",
    "end": "3871830"
  },
  {
    "text": "that the low-level policy\ninitially has no idea about, but over time it\ncan figure it out. Or we can send the goal that\nwe want the policy to reach,",
    "start": "3871830",
    "end": "3879720"
  },
  {
    "text": "and this is the goal\nthat is expressed in terms of the\nstate-space of the policy. And if we do that,\nwe can apply things",
    "start": "3879720",
    "end": "3885740"
  },
  {
    "text": "as such as hindsight\nexperience relabeling that we talked about before. All right, so this is one axis.",
    "start": "3885740",
    "end": "3894140"
  },
  {
    "text": "Then, the second design\nchoice that you can make is whether the low-level\npolicy is pretrained",
    "start": "3894140",
    "end": "3900230"
  },
  {
    "text": "or whether everything is\nlearned end-to-end, all right? So you can either train\nthe low-level policy first,",
    "start": "3900230",
    "end": "3907099"
  },
  {
    "text": "and this can be trained\nwithout any knowledge of the high-level\npolicy, or you can train all of this given\nthat hierarchical structure",
    "start": "3907100",
    "end": "3915080"
  },
  {
    "text": "all at once in an\nend-to-end fashion, and then you will get\nboth the low-level policy and a high-level policy. ",
    "start": "3915080",
    "end": "3923105"
  },
  {
    "text": "All right, so then we have\nthe third design choice, which is whether the low-level\npolicy is self-terminating",
    "start": "3923105",
    "end": "3928520"
  },
  {
    "text": "or whether it's commanded\nat a fixed rate. So in that example\nthat I showed here, we would maintain\nthe command that",
    "start": "3928520",
    "end": "3935240"
  },
  {
    "text": "is being sent from the\nhigh-level policy over three time steps, and after\nthis, that's it.",
    "start": "3935240",
    "end": "3940520"
  },
  {
    "text": "At this point, the\nlow-level policy ends, and the high-level policy\ngets to send the next command.",
    "start": "3940520",
    "end": "3947660"
  },
  {
    "text": "But we can also imagine\na different scenario where the low-level\npolicy keeps going, and it tries to\naccomplish this goal,",
    "start": "3947660",
    "end": "3953930"
  },
  {
    "text": "up until it decides\nthat it's done. It can self-terminate. ",
    "start": "3953930",
    "end": "3961240"
  },
  {
    "text": "And the fourth design\nchoice is that this is kind of similar\nand applicable to a standard\nreinforcement learning.",
    "start": "3961240",
    "end": "3967033"
  },
  {
    "text": "This can be learned\non or off policy.  All right, so we'll\ndiscuss-- we'll",
    "start": "3967033",
    "end": "3973079"
  },
  {
    "text": "see a few examples of\nhierarchical reinforcement learning algorithms, and I'll\ngo over them fairly quickly.",
    "start": "3973080",
    "end": "3979200"
  },
  {
    "text": "I'll show you some\nscreenshots from the papers, and discuss the results\nand the design decisions, and then I will ask you what\nwere the specific design",
    "start": "3979200",
    "end": "3986819"
  },
  {
    "text": "choices along these axes\nthat the authors took, and we'll see if you\ncan guess it right.",
    "start": "3986820",
    "end": "3992640"
  },
  {
    "text": " So first, let me do\nthis example, myself,",
    "start": "3992640",
    "end": "4001240"
  },
  {
    "text": "so that we can kind of see\nwhat this game is about. So here we have our algorithm\nthat we just discussed,",
    "start": "4001240",
    "end": "4007510"
  },
  {
    "text": "this is the DADS algorithm\nwhere we discovered the skills, and then we can use them\nusing model-based planning.",
    "start": "4007510",
    "end": "4013730"
  },
  {
    "text": "So in this case, the design\nchoices that the authors made were the low-level policy\nis not goal-conditioned,",
    "start": "4013730",
    "end": "4019089"
  },
  {
    "text": "we are learning that\nlatent space, z, that was not a specific\ngoal in the state-space.",
    "start": "4019090",
    "end": "4024776"
  },
  {
    "text": "The low-level policies\nwere pretrained We're particularly using\na different algorithm to learn these\nskills, and then we",
    "start": "4024776",
    "end": "4031510"
  },
  {
    "text": "can learn another\npolicy on top of it, or use model-based planning\nto do high-level policy.",
    "start": "4031510",
    "end": "4037780"
  },
  {
    "text": "The authors were\nactually performing them at a fixed rate, so they\nwere not self-terminating.",
    "start": "4037780",
    "end": "4044200"
  },
  {
    "text": "And everything was\nbeing learned on-policy. We didn't discuss that,\nbut if you read the paper,",
    "start": "4044200",
    "end": "4049400"
  },
  {
    "text": "you will see that. So let me introduce the first\nhierarchical RL work, which",
    "start": "4049400",
    "end": "4057880"
  },
  {
    "text": "is called Learning\nLocomotor Controllers, this was written by\nNicholas Heess et al.",
    "start": "4057880",
    "end": "4063730"
  },
  {
    "text": "And here is a screenshot\nfrom the paper that shows the structure of\ntheir hierarchical policy.",
    "start": "4063730",
    "end": "4070150"
  },
  {
    "text": "So let's look a little\nbit closer at this. So first, in this blue box, we\nhave the high-level controller.",
    "start": "4070150",
    "end": "4076038"
  },
  {
    "text": "And this is a recurrent\nneural network that maintains this the state is ET.",
    "start": "4076038",
    "end": "4083843"
  },
  {
    "text": "And then we have a\nlow-level controller, which is that policy in green, here.",
    "start": "4083843",
    "end": "4088920"
  },
  {
    "text": "I think I'm blocking that. Now, the high-level controller\nis sending a command,",
    "start": "4088920",
    "end": "4094730"
  },
  {
    "text": "called C, to the\nlow-level controller, and is doing it every K steps.",
    "start": "4094730",
    "end": "4099960"
  },
  {
    "text": "So in that particular\ndiagram, K is equal to 2. So it's sending\nthat command here, and then the same command is\nsent to the next time step,",
    "start": "4099960",
    "end": "4106699"
  },
  {
    "text": "and that's it. At this point, it ends and\ndoes it again in a second. ",
    "start": "4106700",
    "end": "4112818"
  },
  {
    "text": "Then, we have\nproprioceptive information that is given both\nto the low-level and the high-level policy.",
    "start": "4112819",
    "end": "4118270"
  },
  {
    "text": "But the high-level policy has\nthis additional privileged information about the task that\nthe low-level policy does not",
    "start": "4118270",
    "end": "4124149"
  },
  {
    "text": "get to see. All right, a few more\nfacts about this paper,",
    "start": "4124149",
    "end": "4129810"
  },
  {
    "text": "both the high-level policy\nand the low-level policy are trained separately. They first trained\nthe low-level to do",
    "start": "4129810",
    "end": "4135778"
  },
  {
    "text": "a whole bunch of\ndifferent things, and then the high-level\nto operate on those. They're both trained\nwith policy gradients,",
    "start": "4135779",
    "end": "4143350"
  },
  {
    "text": "and they use what they refer\nto as hierarchical noise to do exploration for\nthat high-level policy,",
    "start": "4143350",
    "end": "4148575"
  },
  {
    "text": "for the high-level\npolicy is just sending noise\ncommands, initially, to the pretrained\nlow-level policy",
    "start": "4148575",
    "end": "4153759"
  },
  {
    "text": "to do better exploration. And here's a video showing\nsome results, where",
    "start": "4153760",
    "end": "4161170"
  },
  {
    "text": "we have the snake-like robot,\nthat when you modulate it with this high-level noise\nfrom high-level policy,",
    "start": "4161170",
    "end": "4167799"
  },
  {
    "text": "kind of does sensible motion. And then if you send noise and\nit does something sensible,",
    "start": "4167800",
    "end": "4173859"
  },
  {
    "text": "you can learn a high-level\npolicy on top of it. And here's one example\nof that policy navigating to a specific--\nnavigating the snake",
    "start": "4173859",
    "end": "4181750"
  },
  {
    "text": "robot to a specific\nlocation on the board. And then, it can\nalso do something",
    "start": "4181750",
    "end": "4187330"
  },
  {
    "text": "a little bit different, such\nas this canyon traversal task. And then they show\nsome results showing",
    "start": "4187330",
    "end": "4192770"
  },
  {
    "text": "how important it is to pretrain\nthe low-level policies. And they show how much better\nthis algorithm is compared",
    "start": "4192770",
    "end": "4199969"
  },
  {
    "text": "to the baseline's. All right, so now the question\nto you about the design",
    "start": "4199970",
    "end": "4205070"
  },
  {
    "text": "choices that were taken-- that\nwere made for this algorithm.",
    "start": "4205070",
    "end": "4210079"
  },
  {
    "text": "So is the goal condition--\nis the low-level policy goal-conditioned or not? And please just say 1 or 2.",
    "start": "4210080",
    "end": "4216260"
  },
  {
    "text": "So 1 would mean it's\ngoal-conditioned, 2 would mean it's not. And you can just\nwrite in the chat. ",
    "start": "4216260",
    "end": "4223650"
  },
  {
    "text": "1, 2, 1, 2, all right. ",
    "start": "4223650",
    "end": "4230750"
  },
  {
    "text": "OK, so it looks like it\ncan be a little confusing. So in this case, this\nis not goal-conditioned.",
    "start": "4230750",
    "end": "4238160"
  },
  {
    "text": "So the high-level policy--\nwe didn't discuss it that much in\ndetail, so I totally understand that you might\nbe a little confused, here.",
    "start": "4238160",
    "end": "4244058"
  },
  {
    "text": "But the high-level policy\nis not sending goals, it's just sending some\nkind of command, C, that then the low-level\npolicy needs to interpret.",
    "start": "4244058",
    "end": "4252573"
  },
  {
    "text": "All right, is the low-level\npolicy pretrained, or are they both learned end-to-end? Again, write 1 or 2.",
    "start": "4252573",
    "end": "4258019"
  },
  {
    "text": "So 1 is pretrained, 2 is e2e. 1, mostly 1s, there was one 2.",
    "start": "4258020",
    "end": "4267329"
  },
  {
    "text": "Yes, they're trained separately. So low-level policy\nis pretrained. Next question, is the low-level\npolicy self-terminating,",
    "start": "4267330",
    "end": "4274410"
  },
  {
    "text": "or is it commanded\nat a fixed rate?  Fixed rate, 2, 2.",
    "start": "4274410",
    "end": "4280559"
  },
  {
    "text": "There's some 1s, OK. Yeah, it's commanded\nwith a fixed rate. We talked about K equal to\n2 for that specific diagram.",
    "start": "4280560",
    "end": "4287640"
  },
  {
    "text": "And is it trained\non or off-policy? Again, 1 or 2.",
    "start": "4287640",
    "end": "4293019"
  },
  {
    "text": "1, 1, 1. OK, this is fairly\nstraightforward. Yes, it was trained with\npolicy rather than non-policy.",
    "start": "4293020",
    "end": "4298120"
  },
  {
    "text": "All right, great, let's\njump to another paper. So this paper is called The\nOption-Critic Architecture,",
    "start": "4298120",
    "end": "4304780"
  },
  {
    "text": "and this is written\nby Bacon et al. And this introduces a notion-- so it actually\ndoesn't introduce it--",
    "start": "4304780",
    "end": "4311340"
  },
  {
    "text": "this concept was known\nfor quite a long time, but it applies it to this modern\ndeep-reinforcement learning",
    "start": "4311340",
    "end": "4318690"
  },
  {
    "text": "scenario. And it's a concept of an option. So an option, here\ndepicted by omega,",
    "start": "4318690",
    "end": "4324480"
  },
  {
    "text": "is a triple that consists\nof this variable i omega, pi omega, and beta omega,\nwhere i is the initiation",
    "start": "4324480",
    "end": "4332309"
  },
  {
    "text": "set of the options. So it tells us when the\noption is allowed to start.",
    "start": "4332310",
    "end": "4337700"
  },
  {
    "text": "Pi is an intra-option\npolicy, so it's a low-level policy that gets\nto act for a specific system",
    "start": "4337700",
    "end": "4346640"
  },
  {
    "text": "in that particular state. And then beta is a\ntermination function. So beta is a function that\ngiven a state can allow option",
    "start": "4346640",
    "end": "4356840"
  },
  {
    "text": "to tell whether it\nshould terminate or not. So the option itself\ndecides when to terminate.",
    "start": "4356840",
    "end": "4363790"
  },
  {
    "text": "And they use this\nparticular diagram that shows our standard MDP\nsetting where different actions",
    "start": "4363790",
    "end": "4370300"
  },
  {
    "text": "led to the next states. But then, it also\nshows how this works with options where\nat any given state,",
    "start": "4370300",
    "end": "4376540"
  },
  {
    "text": "an option can decide\nthat this is now a time for this option to go. And that option can\nmake this larger jumps,",
    "start": "4376540",
    "end": "4382780"
  },
  {
    "text": "so this is some low-level\npolicy that doesn't just go to the next state, but\nit can take a larger jump",
    "start": "4382780",
    "end": "4388570"
  },
  {
    "text": "and do a few actions to\nget to the state that's a little bit further\naway, and then do this over and over again.",
    "start": "4388570",
    "end": "4394240"
  },
  {
    "text": "And then it also decides\nwhen to terminate. They also shown in this diagram\nto show, more or less, how",
    "start": "4394240",
    "end": "4401320"
  },
  {
    "text": "this works. So we have an environment\nthat gives you a state, then that state is taken\nby different policies",
    "start": "4401320",
    "end": "4409450"
  },
  {
    "text": "for different options. And then we have an\nadditional policy, over options, that then decides\nwhich option gets to go.",
    "start": "4409450",
    "end": "4416590"
  },
  {
    "text": " All right, so a few other\nthings from this paper.",
    "start": "4416590",
    "end": "4423290"
  },
  {
    "text": "So option is a self\nterminating mini-policy. They train everything together\nwith a policy gradient",
    "start": "4423290",
    "end": "4429040"
  },
  {
    "text": "algorithm. And that's it. All right, so to\nshow a few results,",
    "start": "4429040",
    "end": "4438409"
  },
  {
    "text": "here they show the termination\nof state frequency,",
    "start": "4438410",
    "end": "4444680"
  },
  {
    "text": "so they basically try to\ndepict in which states do the options terminate. And you can see that\nin this grid scenario,",
    "start": "4444680",
    "end": "4451460"
  },
  {
    "text": "they very often terminate\nclose to the exit, which kind of seems to make sense. So that probably means that\nwe have an option per room,",
    "start": "4451460",
    "end": "4458000"
  },
  {
    "text": "and then this option\ndecides to terminate when it's close to the door. And then they show some results\non the Atari benchmark showing",
    "start": "4458000",
    "end": "4465980"
  },
  {
    "text": "that this hierarchical\noption-critic algorithm is better than a flat RL algorithm.",
    "start": "4465980",
    "end": "4471447"
  },
  {
    "text": "All right, so let's\ndo the quiz again. So what are the design choices\nthat the authors made here? So first, is the low-policy\ngoal-conditioned or not?",
    "start": "4471447",
    "end": "4479000"
  },
  {
    "text": "1 or 2.  2. Yep, it's not goal conditioned.",
    "start": "4479000",
    "end": "4484487"
  },
  {
    "text": "Are the low-level\npolicies pretrained, or are they trained end-to-end? ",
    "start": "4484487",
    "end": "4492199"
  },
  {
    "text": "2, 2, 2, 2. Yep, they're trained\nend-to-end, right? Is the low-level policy\nself-terminating or fixed rate?",
    "start": "4492200",
    "end": "4498139"
  },
  {
    "text": "1 or 2. 1, 1, 1. Yep, it's self-terminating. That's what the option is.",
    "start": "4498140",
    "end": "4503540"
  },
  {
    "text": "And is it trained\non-policy or off-policy? The answer is on the\nslide, yes, it's on-policy",
    "start": "4503540",
    "end": "4510760"
  },
  {
    "text": "using policy gradient. All right, let's go over two\nmore algorithms real quick.",
    "start": "4510760",
    "end": "4516679"
  },
  {
    "text": "So first, let's talk about\nrelay policy learning. This is an algorithm introduced\nby Abhishek Gupta et al.",
    "start": "4516680",
    "end": "4522280"
  },
  {
    "text": "And here, we have a\ndiagram that looks very similar to the diagram\nthat we see on the right, with one distinction, which is\nthat the high-level policy is",
    "start": "4522280",
    "end": "4529810"
  },
  {
    "text": "sending goal states as opposed\nto some high-level commands to the low-level\npolicy, all right?",
    "start": "4529810",
    "end": "4535045"
  },
  {
    "text": " In particular, the way that the\nauthors trained this algorithm",
    "start": "4535045",
    "end": "4541390"
  },
  {
    "text": "is by pretraining it\nwith user demonstrations. And they ask the user to do\nall kinds of different things",
    "start": "4541390",
    "end": "4547420"
  },
  {
    "text": "in a kitchen environment. And these are not\ntasks, they're just asking the users to kind of\nplay in that environment,",
    "start": "4547420",
    "end": "4553370"
  },
  {
    "text": "and that's it. Once they have these\ndemonstrations, they do heavy data relabeling.",
    "start": "4553370",
    "end": "4560100"
  },
  {
    "text": "And they have a goal-conditioned\nhigh-level policy, as well as goal-conditioned\nlow-level policy.",
    "start": "4560100",
    "end": "4565530"
  },
  {
    "text": "And now, you can do hindsight\nand experience relabeling for both of these levels\nusing the algorithm that you",
    "start": "4565530",
    "end": "4571260"
  },
  {
    "text": "learned about previously, here. Then they do imitation\nlearning using",
    "start": "4571260",
    "end": "4576369"
  },
  {
    "text": "those relabeled demonstrations,\nand then continue with reinforcement\nlearning fine-tuning.",
    "start": "4576370",
    "end": "4582199"
  },
  {
    "text": "So a few more facts-- these are\ngoal-conditioned policies, both on high-level\nand low-level,",
    "start": "4582200",
    "end": "4587300"
  },
  {
    "text": "that were trained\nwith relabeling. They use demonstrations\nto pretrain everything,",
    "start": "4587300",
    "end": "4592430"
  },
  {
    "text": "and everything is trained\nusing an on-policy algorithm. And some results, here was a\nlong horizon goal, on the left,",
    "start": "4592430",
    "end": "4601179"
  },
  {
    "text": "that they want the\npolicy to achieve. And they show how\ntheir policy is",
    "start": "4601180",
    "end": "4606820"
  },
  {
    "text": "pretty good at performing\nthis task compared to other baselines. And then they also\nshow results indicating",
    "start": "4606820",
    "end": "4612820"
  },
  {
    "text": "how useful it is to pretrain\nit from demonstrations. All right, so again, quiz.",
    "start": "4612820",
    "end": "4618400"
  },
  {
    "text": "Design choices. Is the low-level policy\ngoal-conditioned or not? 1 or 2?",
    "start": "4618400",
    "end": "4623889"
  },
  {
    "text": "1, 1, 1. Yeah, great, it's\ngoal-conditioned. Is the low-level policy\npretrained, or is everything",
    "start": "4623890",
    "end": "4630130"
  },
  {
    "text": "trained end-to-end? Now, this is a little tricky. ",
    "start": "4630130",
    "end": "4635870"
  },
  {
    "text": "Great, yeah. So there are a few 2s. But yeah, it's trained\nend-to-end, so both of them",
    "start": "4635870",
    "end": "4643790"
  },
  {
    "text": "are trained at the same time. It's pretrained in the sense\nthat we're using demonstrations first, but it's not\nthat we're training",
    "start": "4643790",
    "end": "4649489"
  },
  {
    "text": "the low-level and the\nhigh-level policy separately-- they're trained\nat the same time. And that's what we meant by\npretrained versus end-to-end.",
    "start": "4649490",
    "end": "4655760"
  },
  {
    "text": "Now, is it self-terminating,\nor are the goals commanded at a fixed rate? We didn't talk\nabout it explicitly,",
    "start": "4655760",
    "end": "4661040"
  },
  {
    "text": "so it might be a little tricky,\nbut take a guess, 1 or 2. We have a few 2s,\nwe have a few 1s.",
    "start": "4661040",
    "end": "4667637"
  },
  {
    "text": "In this case, they\nare being commanded at a fixed rate-- that's\nwhat we use for relabeling. And then, is it trained\non-policy or off-policy?",
    "start": "4667637",
    "end": "4675100"
  },
  {
    "text": " Right, everybody is\nsaying correctly. Yes, this is trained on-policy.",
    "start": "4675100",
    "end": "4681713"
  },
  {
    "text": "All right, so let's\nintroduce the last trained reinforcement learning\nalgorithm, called HIRO. And this is very similar to\nthe algorithms from before.",
    "start": "4681713",
    "end": "4689340"
  },
  {
    "text": "And here is a diagram\nfrom the paper. High-level policy\nis sending goals",
    "start": "4689340",
    "end": "4694460"
  },
  {
    "text": "to the low-level policy that are\ncommanded at a fixed frequency. And the only difference is\nthat this algorithm does",
    "start": "4694460",
    "end": "4703612"
  },
  {
    "text": "this hierarchical\nreinforcement learning, but it does\nadditional relabeling that allows it to perform this\nusing off-policy reinforcement",
    "start": "4703612",
    "end": "4711050"
  },
  {
    "text": "learning algorithm. So in particular, what it\ndoes it changes the way that the high-level\npolicy operates,",
    "start": "4711050",
    "end": "4717860"
  },
  {
    "text": "because when the high-level\npolicy commands a certain goal, the low-level policy\nwill try to achieve it, and then it will achieve a goal\nthat is slightly different.",
    "start": "4717860",
    "end": "4725180"
  },
  {
    "text": "Now, when you get\nthat experience, you need to accommodate\nfor the fact that the low-level policy\nhas changed in the meantime,",
    "start": "4725180",
    "end": "4731930"
  },
  {
    "text": "so we need to perform some\nkind of off policy correction. And that's what they\nintroduce in this paper.",
    "start": "4731930",
    "end": "4737660"
  },
  {
    "text": "So they work with\ngoal-conditioned policies with relabeling, they\ndo off-policy training",
    "start": "4737660",
    "end": "4743480"
  },
  {
    "text": "through off-policy\ncorrections, and then here are a few results. So here is a robot\nthat learns how to push",
    "start": "4743480",
    "end": "4749050"
  },
  {
    "text": "this block to a goal location. And they also perform,\nin the follow up paper, they perform similar\ntasks on a real robot.",
    "start": "4749050",
    "end": "4758830"
  },
  {
    "text": "And they show how\ntheir algorithm is able to beat most\nof the baselines.",
    "start": "4758830",
    "end": "4764170"
  },
  {
    "text": "All right, so the\nlast quiz of today, what were the design choices\nthat were made by the authors?",
    "start": "4764170",
    "end": "4769869"
  },
  {
    "text": "So is it a goal\ncondition or not? 1 or 2? Yeah, it's all goal condition.",
    "start": "4769870",
    "end": "4776120"
  },
  {
    "text": "Great. Is the low-level policy\npretrained, or is it all trained end-to-end?",
    "start": "4776120",
    "end": "4782920"
  },
  {
    "text": "Yeah, a lot of 2s, yes,\nthis is end-to-end. Now, is the policy\nself-terminating, or is it being commanded\nat the fixed rate?",
    "start": "4782920",
    "end": "4789670"
  },
  {
    "text": "1 or 2? 2s.  We saw at least one 2, a few 2s.",
    "start": "4789670",
    "end": "4796220"
  },
  {
    "text": "Yeah, great. It's commanded at a fixed rate. And is this an on-policy\nor an off-policy algorithm?",
    "start": "4796220",
    "end": "4803770"
  },
  {
    "text": "2, we see a lot of 2s. Yes, it's an\noff-policy algorithm, that introduces\noff-policy correction that allows to do this.",
    "start": "4803770",
    "end": "4809800"
  },
  {
    "text": " Great, so to summarize that\npart of the lecture, when you",
    "start": "4809800",
    "end": "4818220"
  },
  {
    "text": "do hierarchical\nreinforced learning, there are multiple design\nchoices and multiple frameworks to use, and these are\njust four particular axes",
    "start": "4818220",
    "end": "4825900"
  },
  {
    "text": "that I picked that we're\nkind of iterating over, but there are many more.",
    "start": "4825900",
    "end": "4832179"
  },
  {
    "text": "It usually helps\nwith exploration and temporarily extended tasks. It can be a little\ntricky to get it",
    "start": "4832180",
    "end": "4837460"
  },
  {
    "text": "to work-- to get hierarchical\nreinforcement learning to work. Because now, you kind of-- with\na standard flat reinforcement",
    "start": "4837460",
    "end": "4842588"
  },
  {
    "text": "learning algorithm it's\ndifficult to get to work, now you have to do\nkind of policies at two different levels,\nwhich can be quite tricky.",
    "start": "4842588",
    "end": "4849840"
  },
  {
    "text": "And it also seems like\na natural direction for harder reinforcement\nlearning problems, where you might want to deal\nwith longer horizons and so on.",
    "start": "4849840",
    "end": "4857330"
  },
  {
    "text": "And in particular, if you'd\nlike to get a little bit more common sense of how\nthese algorithms work,",
    "start": "4857330",
    "end": "4864340"
  },
  {
    "text": "and why they work, I recommend\nthis paper by Ofir Nachum et al on Why Does Hierarchy\nSometimes Work?",
    "start": "4864340",
    "end": "4870010"
  },
  {
    "text": "They show some kind of\nexperiments disentangling what makes it useful\nand what doesn't.",
    "start": "4870010",
    "end": "4876440"
  },
  {
    "text": "Right, so we're out of time. So we discussed today\ninformation-theoretic concepts.",
    "start": "4876440",
    "end": "4881697"
  },
  {
    "text": "We talked a little bit about\nskill discovery algorithms, how we can use these\nskills, and then how we can apply similar\nconcepts in a hierarchical RL",
    "start": "4881697",
    "end": "4888050"
  },
  {
    "text": "scenario. And next week, we'll talk\nabout agents that can learn--",
    "start": "4888050",
    "end": "4893130"
  },
  {
    "text": "sorry, not next\nweek, but Wednesday, we'll talk about agents\nthat can learn continuously over their lifetime.",
    "start": "4893130",
    "end": "4899500"
  },
  {
    "text": "And this is it. Are there any questions? ",
    "start": "4899500",
    "end": "4911830"
  },
  {
    "text": "There's a raised hand from-- Yeah. I have a question\nabout, it's like are",
    "start": "4911830",
    "end": "4918072"
  },
  {
    "text": "two levels are standard\nin hierarchical learning or like if you can explore\nmore levels [INAUDIBLE]",
    "start": "4918072",
    "end": "4927110"
  },
  {
    "text": "I think there are\nsome papers, to try to do more than two levels. I think, usually, in practice,\ntwo levels are already",
    "start": "4927110",
    "end": "4933990"
  },
  {
    "text": "pretty hard to get to work,\ngiven that the low-level is constantly changing,\nand the high-level has to kind of\naccommodate for this.",
    "start": "4933990",
    "end": "4940720"
  },
  {
    "text": "And because these\nalgorithms maybe haven't worked as\nwell as everybody expected, at least\nat this point,",
    "start": "4940720",
    "end": "4946920"
  },
  {
    "text": "people haven't really--\nit's not a very common thing to do to try more\nlevels than two.",
    "start": "4946920",
    "end": "4952469"
  },
  {
    "text": "But I think it could be an\ninteresting area to explore. ",
    "start": "4952470",
    "end": "4960010"
  },
  {
    "text": "Thank you. ",
    "start": "4960010",
    "end": "4969380"
  },
  {
    "text": "Are there any other questions? There's a question from-- So I have a question.",
    "start": "4969380",
    "end": "4974840"
  },
  {
    "text": "So I guess these kind\nof tuned level systems",
    "start": "4974840",
    "end": "4980480"
  },
  {
    "text": "also have the\nproblems when they're trained end-to-end\nthere is a problem that is the [INAUDIBLE] phase, which\nis the chicken and egg problem,",
    "start": "4980480",
    "end": "4989850"
  },
  {
    "text": "so the low-level depends\non a good high-level agent,",
    "start": "4989850",
    "end": "4995150"
  },
  {
    "text": "and good high-level\nagents depend on good low-level agents. So how is this able to\nprevent this problem?",
    "start": "4995150",
    "end": "5004820"
  },
  {
    "text": "Yeah, this is a\nreally good question. Thanks for asking.",
    "start": "5004820",
    "end": "5009909"
  },
  {
    "text": "Yeah, so this is a\ncommon failure mode in many algorithms,\nespecially for algorithms",
    "start": "5009910",
    "end": "5016030"
  },
  {
    "text": "that make design\ndecisions that make these problems more apparent. For example, in the\noption framework,",
    "start": "5016030",
    "end": "5023140"
  },
  {
    "text": "these low-level policies can\nalso learn to self-terminate, a common failure case is\nthat an option would only",
    "start": "5023140",
    "end": "5030610"
  },
  {
    "text": "act for one time step, in\nwhich case, it's not really an option. Or there will be only one\noption for the entire task,",
    "start": "5030610",
    "end": "5037660"
  },
  {
    "text": "in which case, it also\ndoesn't make that much sense, because we want to have this\nparticular structure where",
    "start": "5037660",
    "end": "5043330"
  },
  {
    "text": "we can split the task\ninto a few options, but not just have one task\nbe one option-- that doesn't really simplify the problem.",
    "start": "5043330",
    "end": "5051220"
  },
  {
    "text": "There are a few\ndesign decisions that make this problem a\nlittle bit easier. So for example, if you command\nagents at a fixed rate,",
    "start": "5051220",
    "end": "5056660"
  },
  {
    "text": "that problem goes away. The other one, with respect\nto the chicken and egg problem",
    "start": "5056660",
    "end": "5062980"
  },
  {
    "text": "that you discussed, is this\noff-policy correction that tries to kind of think about,\ngiven a trajectory that",
    "start": "5062980",
    "end": "5069220"
  },
  {
    "text": "you've seen-- or given the goal\nthat you commanded, what would be the\nlow-level policy",
    "start": "5069220",
    "end": "5077950"
  },
  {
    "text": "that would be able\nto get to that goal? So there are a few\nthings that you can do to address\nthese challenges,",
    "start": "5077950",
    "end": "5084250"
  },
  {
    "text": "but overall, this makes this\nproblem quite challenging. I think you're totally right.",
    "start": "5084250",
    "end": "5090590"
  },
  {
    "text": "That's it. Thank you. ",
    "start": "5090590",
    "end": "5100000"
  }
]