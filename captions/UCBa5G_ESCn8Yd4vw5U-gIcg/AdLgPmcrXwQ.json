[
  {
    "start": "0",
    "end": "5410"
  },
  {
    "text": "Today we're happy to have\nNathan Lambert, a research scientist at the Allen Institute\nfor AI, who focuses on RLHF",
    "start": "5410",
    "end": "15520"
  },
  {
    "text": "and the author of\ninterconnects.ai. He'll be presenting\na really cool talk",
    "start": "15520",
    "end": "21100"
  },
  {
    "text": "on aligning open\nlanguage models today. So thank you for\njoining us, Nathan.",
    "start": "21100",
    "end": "27180"
  },
  {
    "text": "Yeah, thanks for the intro. OK, this is a long time coming.",
    "start": "27180",
    "end": "32229"
  },
  {
    "text": "This is a brief intro\non why I'm doing this. I think generally,\nsince ChatGPT,",
    "start": "32229",
    "end": "37290"
  },
  {
    "text": "you'll see a lot has\nobviously happened. But I don't think it's\nbeen a blur for me",
    "start": "37290",
    "end": "42719"
  },
  {
    "text": "as much as anyone else. So taking the time\nto retell what has happened in this\nfine-tuning and alignment space",
    "start": "42720",
    "end": "50399"
  },
  {
    "text": "since ChatGPT\nhappened is something that I thought was a\nworthy undertaking. So this is not really\na one-on-one lecture,",
    "start": "50400",
    "end": "56940"
  },
  {
    "text": "but it will probably\ngive you a lot of context on why people are\nmentioning certain things and what still matters\nand what does not.",
    "start": "56940",
    "end": "63430"
  },
  {
    "text": "So hopefully this is fun. I can see the chat. I don't know\nexactly if questions",
    "start": "63430",
    "end": "69212"
  },
  {
    "text": "are going to come to me or if\nI will see it the whole time. I think clarifying\nquestions are good, maybe",
    "start": "69212",
    "end": "74240"
  },
  {
    "text": "not discussions the whole time. And I'll try to keep\nsure that there's time for questions at the end.",
    "start": "74240",
    "end": "80280"
  },
  {
    "text": "So let's get into it. ",
    "start": "80280",
    "end": "85960"
  },
  {
    "text": "Generally, we're going to\ntalk about language models, it's what everyone wants\nto talk about these days. I need to do some\nof the older history",
    "start": "85960",
    "end": "92230"
  },
  {
    "text": "so that I can talk\nabout recent history. The place that I like\nto start is actually with Claude Shannon, who\nhad this paper talking",
    "start": "92230",
    "end": "99219"
  },
  {
    "text": "about approximating-- arranging characters to\ncreate language models.",
    "start": "99220",
    "end": "105280"
  },
  {
    "text": "That's probably why Anthropic\ncalled their models Claude. That's pretty well-known.",
    "start": "105280",
    "end": "110900"
  },
  {
    "text": "And a lot has happened since\nthese very early papers on predicting sequences of text.",
    "start": "110900",
    "end": "118310"
  },
  {
    "text": "And this is largely built\non this loss function, which is called the\nautoregressive loss function.",
    "start": "118310",
    "end": "123888"
  },
  {
    "text": "So if you have this training\nexample where you have something like \"I saw a\" you're trying to\npredict what comes after this,",
    "start": "123888",
    "end": "130090"
  },
  {
    "text": "the whole idea is that there's\ngoing to be one correct token that is the correct label. And their training loss\nis going to increase",
    "start": "130090",
    "end": "136269"
  },
  {
    "text": "the probability of that token\nand decrease the probability of everything else. This very simple loss function\nclassifying which token to use",
    "start": "136270",
    "end": "144069"
  },
  {
    "text": "and actually predict\nhas enabled wild things. And this took\nanother turn in 2017",
    "start": "144070",
    "end": "150530"
  },
  {
    "text": "when this transformer\npaper was born. Attention is all you need. Everyone here has\nheard about this. It's a great exercise\nto actually dig",
    "start": "150530",
    "end": "156950"
  },
  {
    "text": "into what the attention\nmechanism is doing. Not the focus of this talk. We'll quickly kind\nof keep going.",
    "start": "156950",
    "end": "163530"
  },
  {
    "text": "In 2018, there was\nthree main things. These are slightly out of order. ELMo was the earliest\none, which was",
    "start": "163530",
    "end": "168740"
  },
  {
    "text": "contextualized word embeddings. And the same year, we also\nhad GPT-1 and BERT released,",
    "start": "168740",
    "end": "174200"
  },
  {
    "text": "which is the beginning of core\nideas on which modern language models and transformers were\ntrending towards and just",
    "start": "174200",
    "end": "181040"
  },
  {
    "text": "getting these better\nmodels, training on large-- large internet scaled corpora.",
    "start": "181040",
    "end": "186409"
  },
  {
    "text": "BERT was a classifier. GPT-1 was generating text. And we continue along these\ntrends through the years.",
    "start": "186410",
    "end": "193140"
  },
  {
    "text": "GPT-2 is when we started\nlearning about scaling laws. And if you use orders of\nmagnitude more compute,",
    "start": "193140",
    "end": "199430"
  },
  {
    "text": "the actual test\nloss will continue to decrease in a linear fashion\nwith respect to the log compute.",
    "start": "199430",
    "end": "205420"
  },
  {
    "text": "These ideas now are\ncommonplace when we talk about language models. GPT-2 also pioneered\na lot of discussions",
    "start": "205420",
    "end": "212440"
  },
  {
    "text": "on releasing language models. So GPT-2, when it\nwas first announced, they were holding access\nback because of the risks",
    "start": "212440",
    "end": "219850"
  },
  {
    "text": "of language models. And this started a lot\nof the conversations around what you\nshould or should not",
    "start": "219850",
    "end": "225612"
  },
  {
    "text": "release with language models. They eventually\nactually released GPT-2, and you can download the models\non Hugging Face and use them.",
    "start": "225612",
    "end": "231830"
  },
  {
    "text": "But this is where that kind\nof conversation around release strategies emerged. In 2020 is when\nlanguage models really",
    "start": "231830",
    "end": "237640"
  },
  {
    "text": "started to be noticeably good. So GPT-3 is when a\nlot of people were like, whoa, this can actually\ndo really interesting things.",
    "start": "237640",
    "end": "244609"
  },
  {
    "text": "If I create a really\nclever prompt, figure out how to give it\nmy information correctly,",
    "start": "244610",
    "end": "249670"
  },
  {
    "text": "and GPT-3 could\ndo a ton of things with this few-shot or\nmulti-shot learning, which is when you give it a few\nexamples in the prompt",
    "start": "249670",
    "end": "257380"
  },
  {
    "text": "and then ask it to do\nanother rendition of it. And with this power\ncame many harms.",
    "start": "257380",
    "end": "263330"
  },
  {
    "text": "And this is a\ndiscussion of, what are the risks of\nreleasing language models? What types of groups\nwill be hurt by this?",
    "start": "263330",
    "end": "270200"
  },
  {
    "text": "Very important problems\nthat culminated in 2021 with the stochastic parrots\npaper, which is arguing about",
    "start": "270200",
    "end": "277550"
  },
  {
    "text": "whether or not language models\ncan be too big, is in the title. But it's really a critique\non how we should be",
    "start": "277550",
    "end": "284360"
  },
  {
    "text": "thinking about language models. What are the limits of them? Are they actually\ndoing the things?",
    "start": "284360",
    "end": "289760"
  },
  {
    "text": "Like, are they actually\nthinking or doing any of these human things? Or are they just kind of\nfollowing patterns in the data?",
    "start": "289760",
    "end": "295699"
  },
  {
    "text": "And then just the year\nafter, this is like the-- it's like the tragedy\nof syntactic parrots",
    "start": "295700",
    "end": "301483"
  },
  {
    "text": "because no one\ntalks about it now, is that ChatGPT\ncame the year later and totally reshaped the whole\nnarrative around language",
    "start": "301483",
    "end": "308180"
  },
  {
    "text": "models one more time. And this is really\nwhere we start today's talk, is like, how\ndoes this idea of alignment",
    "start": "308180",
    "end": "316069"
  },
  {
    "text": "emerge in ChatGPT? And then what\nhappens after this? So the question that\nI ask myself is like--",
    "start": "316070",
    "end": "322070"
  },
  {
    "text": "or I tell a lot of people is,\ncan ChatGPT exist without RLHF? And what we saw in\nthe release day--",
    "start": "322070",
    "end": "329610"
  },
  {
    "text": "so if you go back and read the\nactual OpenAI blog about RLHF, they list all these limitations.",
    "start": "329610",
    "end": "334940"
  },
  {
    "text": "But they say that RLHF\nwas an important tool to launching ChatGPT. And the limitations\nthat they list",
    "start": "334940",
    "end": "340130"
  },
  {
    "text": "are really the things that\nwe're still researching and that we're talking\nabout in this talk. It's a great blog\npost to go back to. But a good way to frame it is\nthat RLHF seems to be necessary",
    "start": "340130",
    "end": "349520"
  },
  {
    "text": "but is not sufficient. You can't do something\nlike ChatGPT or Gemini or Claude with a\ntechnique that--",
    "start": "349520",
    "end": "355070"
  },
  {
    "text": "without something like RLHF. But it's not the thing--\nlike, pretraining is still most of the work, but the\nfact that RLHF is needed",
    "start": "355070",
    "end": "361820"
  },
  {
    "text": "is really important\nto contextualize all these improvements\nthat we've seen in the open in the\nlast 14 months or so.",
    "start": "361820",
    "end": "368490"
  },
  {
    "text": "Some examples that\nI like to cite on RLHF being relied upon-- you\ncan list many more models here",
    "start": "368490",
    "end": "373770"
  },
  {
    "text": "than I have. This figure from Anthropic's\n\"Constitutional AI\" paper is the single one that\nI go back to all the time,",
    "start": "373770",
    "end": "381810"
  },
  {
    "text": "showing how just using RLHF\ncan get these more desirable behaviors from their model\nin really dramatic ways.",
    "start": "381810",
    "end": "389080"
  },
  {
    "text": "So these Elo measurements\naren't calibrated, so we don't know how to\ncompare Llama 3 on this chart",
    "start": "389080",
    "end": "396750"
  },
  {
    "text": "compared to Anthropic's models. But the level of\ninvestment that Anthropic has had in these\nkind of techniques",
    "start": "396750",
    "end": "402570"
  },
  {
    "text": "and showing this kind of\nwide-ranging improvements of their models with\nRLHF is a kind flag",
    "start": "402570",
    "end": "408120"
  },
  {
    "text": "that we can follow to try\nto learn how to do alignment with this much precision and\nwith this much kind of impact",
    "start": "408120",
    "end": "413490"
  },
  {
    "text": "as places like Anthropic\nor OpenAI will have. One such example is\njust a simple quote",
    "start": "413490",
    "end": "419050"
  },
  {
    "text": "from the Llama 2 paper, which\nis like the colloquial way",
    "start": "419050",
    "end": "424751"
  },
  {
    "text": "of reading this quote,\nwhich I will read, is that, whoa, RLHF\nworked really easily. And what the quote\nis, is \"Meanwhile,",
    "start": "424752",
    "end": "431890"
  },
  {
    "text": "reinforcement learning,\nknown for its instability, seemed a somewhat shadowy field\nfor those in the NLP research",
    "start": "431890",
    "end": "438130"
  },
  {
    "text": "community. However, reinforcement learning\nproved highly effective, particularly given its cost\nand time effectiveness.\"",
    "start": "438130",
    "end": "444520"
  },
  {
    "text": "So this is one of the\nbiggest endorsements of RLHF. And it's always fun for me\nbecause I came from the RL side,",
    "start": "444520",
    "end": "449680"
  },
  {
    "text": "and then I've been learning NLP. But for NLP researchers\nto say these things, like, yes, reinforcement\nlearning is",
    "start": "449680",
    "end": "455590"
  },
  {
    "text": "known for instability, and\ngiven that it is cost effective and time effective for an\nRL person, that's shocking.",
    "start": "455590",
    "end": "462320"
  },
  {
    "text": "It's like, RL has never\nbeen particularly cost and time effective. But for in these language\nmodel domain where we're fine",
    "start": "462320",
    "end": "469030"
  },
  {
    "text": "tuning with it rather than\nlearning from scratch, to have people in NLP\nthat are saying this is just really striking for\nhow much impact it can have.",
    "start": "469030",
    "end": "478550"
  },
  {
    "text": "And the timeline of\nalignment and open alignment is really like, when do\nwe see these benefits? Like, these benefits\ndidn't show up in models",
    "start": "478550",
    "end": "485300"
  },
  {
    "text": "that people were playing\nwith for quite a bit of time. So this is a little Atlas\nthat I've thrown together.",
    "start": "485300",
    "end": "491460"
  },
  {
    "text": "I also made a Hugging\nFace collection, where I tried to add all the\nmodels that I talk about to it. So you can actually\nclick on the models",
    "start": "491460",
    "end": "497780"
  },
  {
    "text": "or try to use them if you're\nso inclined of actually running the models yourself. It's just another way of\ndocumenting the artifacts that I",
    "start": "497780",
    "end": "505507"
  },
  {
    "text": "talked about and the\nartifacts that-- for me, this is a good review on,\nlike, what mattered, what mattered in this really\nnoisy journey in the last year.",
    "start": "505507",
    "end": "513049"
  },
  {
    "text": "Of course, some disclaimers-- I'm not covering every\nmodel since ChatGPT.",
    "start": "513049",
    "end": "518298"
  },
  {
    "text": "This little plot of model\nicons could probably look more like an exponential\nthan this capped bar.",
    "start": "518299",
    "end": "526130"
  },
  {
    "text": "And there's so\nmuch history of NLP that people are building on\nin the alignment space that",
    "start": "526130",
    "end": "532370"
  },
  {
    "text": "is totally swept\nunder the rug here, a lot of academic and\ninfrastructure contributions",
    "start": "532370",
    "end": "538100"
  },
  {
    "text": "that I'm not talking\nabout, but are really important to this\nproliferation of fine-tuning models.",
    "start": "538100",
    "end": "544110"
  },
  {
    "text": "So just describing what this\nimage that I have here is, is summarized--",
    "start": "544110",
    "end": "552180"
  },
  {
    "text": "some of these are based models. I'm not going to\nfocus on base models as much as fine-tuned models.",
    "start": "552180",
    "end": "558030"
  },
  {
    "text": "The base models are\nextremely important. Like, none of this\nhappens without Llama. None of this happens\nwithout Llama 2.",
    "start": "558030",
    "end": "564120"
  },
  {
    "text": "The base model are the\nbedrock of this ecosystem. And then the alignment\nmodels are what people--",
    "start": "564120",
    "end": "571215"
  },
  {
    "text": "the aligned models are a lot of\ntimes what people can play with and what you can try out,\nwhat you could do yourself on much less computing\ninfrastructure and all",
    "start": "571215",
    "end": "578348"
  },
  {
    "text": "these things. So I'm going to talk more\nabout the aligned models, but everything matters. It's one big ecosystem.",
    "start": "578348",
    "end": "583646"
  },
  {
    "text": " Another thing that's\nnot fun but I'm",
    "start": "583646",
    "end": "589370"
  },
  {
    "text": "going to do for the sake\nof flag posting, no one really likes listening\nto definitions.",
    "start": "589370",
    "end": "595280"
  },
  {
    "text": "Here are some things that\nyou'll hear thrown around. This isn't even all of them when\ntalking about, quote unquote,",
    "start": "595280",
    "end": "600710"
  },
  {
    "text": "\"alignment.\" I have here, like,\nalignment I've defined as a general\nnotion of training a model",
    "start": "600710",
    "end": "605780"
  },
  {
    "text": "to mirror a user's desires. Really, with any loss\nfunction, it's not restricted.",
    "start": "605780",
    "end": "611708"
  },
  {
    "text": "So there's a difference\nbetween instruction fine-tuning and supervised fine-tuning. Instruction fine-tuning\nis about trying",
    "start": "611708",
    "end": "618079"
  },
  {
    "text": "to get a model that\nwill respond to queries, formatted instructions, while\nsupervised fine-tuning is",
    "start": "618080",
    "end": "623300"
  },
  {
    "text": "more about learning a\nspecific tasks capabilities. These get interchanged\nall the time. Like, that's OK.",
    "start": "623300",
    "end": "629790"
  },
  {
    "text": "It's good to know that\nthey're different. And then two more ones\nthat I need to touch on",
    "start": "629790",
    "end": "635670"
  },
  {
    "text": "and we could go on even\nlonger is reinforcement learning from human feedback. There's this\nmulti-stage process.",
    "start": "635670",
    "end": "641460"
  },
  {
    "text": "It's a specific tool\nfor aligning ML models to human data.",
    "start": "641460",
    "end": "646860"
  },
  {
    "text": "It's a class of tools, so it has\nsome learn a preference model. And then you extract\ninformation from it.",
    "start": "646860",
    "end": "652574"
  },
  {
    "text": "So there are so many\ndifferent ways to do it. It's really an approach. And then there's a\nterm that I'm trying to grow, which is preference\nfine-tuning, which",
    "start": "652575",
    "end": "660540"
  },
  {
    "text": "could encompass RLHF\nmethods, like PPO. But there's a question of, how\ndo we differentiate something",
    "start": "660540",
    "end": "666060"
  },
  {
    "text": "like direct preference\noptimization, which doesn't use an RL\noptimizer from all of RLHF?",
    "start": "666060",
    "end": "672329"
  },
  {
    "text": "And I'll come back\nto this, but it's good to have some common\ngrounds to build on because I might be going\nthrough some of these things",
    "start": "672330",
    "end": "678720"
  },
  {
    "text": "pretty quickly. ",
    "start": "678720",
    "end": "686939"
  },
  {
    "text": "This is a chapter that\nI cover in one slide because it's really\ntapping into a lot",
    "start": "686940",
    "end": "692370"
  },
  {
    "text": "of different personal stories. It's hard to retell\nhow crazy things were",
    "start": "692370",
    "end": "698640"
  },
  {
    "text": "when ChatGPT dropped. People were not really\nlosing their mind, but there was a\nlot of uncertainty",
    "start": "698640",
    "end": "705720"
  },
  {
    "text": "on what the future held,\nespecially that it was clear",
    "start": "705720",
    "end": "710970"
  },
  {
    "text": "that language models\nwere important. But it is not clear-- there's a lot of\narticles on like-- titled, we're going\nto reproduce open",
    "start": "710970",
    "end": "717690"
  },
  {
    "text": "ChatGPT, which you can't really\nhave an open model that does",
    "start": "717690",
    "end": "722850"
  },
  {
    "text": "what a closed product does. There's a difference between\nmodel weights and this product that ChatGPT represents, but\nthere's so much excitement",
    "start": "722850",
    "end": "730500"
  },
  {
    "text": "that everyone is saying they're\ngoing to do these things and trying to figure out the\nright coalitions for actually",
    "start": "730500",
    "end": "735630"
  },
  {
    "text": "doing so. And it's interesting. This delay is this\nland grab, where people",
    "start": "735630",
    "end": "741240"
  },
  {
    "text": "are learning the basic things. Like, what is red-teaming? What is the difference\nbetween a dialogue",
    "start": "741240",
    "end": "746529"
  },
  {
    "text": "agent and a predictive\nlanguage model? What tools should we use? And everything follows from here\nwith what people are building.",
    "start": "746530",
    "end": "754370"
  },
  {
    "text": "But like, personally, I just\nremember multiple meetings where people were like,\nyeah, you should do it. You should go try to\nbuild open ChatGPT.",
    "start": "754370",
    "end": "760400"
  },
  {
    "text": "And when you look\nback, that goal is just so wild that so many\npeople are just going like,",
    "start": "760400",
    "end": "765520"
  },
  {
    "text": "we need to build this\nthing in the open source. And it's just like,\nit doesn't even make sense because\nyou can't open source",
    "start": "765520",
    "end": "771700"
  },
  {
    "text": "a whole system that way. But there are some\nthings that make a bit--",
    "start": "771700",
    "end": "777533"
  },
  {
    "text": "like, this makes\na lot more sense, which is when things start to\nget grounded in actual models. So the first Llama suite\nwas released in, I think,",
    "start": "777533",
    "end": "785399"
  },
  {
    "text": "in February. I have the date in\nsome notes somewhere. And then these instruction\ntuned models started to show up",
    "start": "785400",
    "end": "791370"
  },
  {
    "text": "on this first Llama model. The first one to really\ncrack the narrative was this Alpaca\nmodel, and it did",
    "start": "791370",
    "end": "797850"
  },
  {
    "text": "a bunch of things that\nstill are used today. So this was trained on 52,000\nself-constructed style data",
    "start": "797850",
    "end": "805529"
  },
  {
    "text": "distilled from text-davinci-003. There's a lot in the sentence. I'll say what\nself-construct means. But this wasn't even data\ngenerated from ChatGPT.",
    "start": "805530",
    "end": "813660"
  },
  {
    "text": "It was generated from one\nof OpenAI's API models. So if we talk about--",
    "start": "813660",
    "end": "819060"
  },
  {
    "text": "this is all on how to apply\ninstruction fine-tuning. And this thing I mentioned\non the definition slide.",
    "start": "819060",
    "end": "824910"
  },
  {
    "text": "But really, it's about making\na model that will respond to specific styles of inputs.",
    "start": "824910",
    "end": "830320"
  },
  {
    "text": "What often happens at\na technical level here is that the model is learning\nto integrate something called a chat\ntemplate, the ability",
    "start": "830320",
    "end": "837390"
  },
  {
    "text": "to include system prompts. So you want to tell--\nyou want the model to know it is an agent. You want the model to\nknow what day it is.",
    "start": "837390",
    "end": "844240"
  },
  {
    "text": "Excuse me. You can do this in the system\nprompt, which is something that the user doesn't see. But it steers the\nbehavior of the model.",
    "start": "844240",
    "end": "850437"
  },
  {
    "text": "And instruction\ntuning is really where we make the model capable\nof having these behaviors.",
    "start": "850437",
    "end": "855500"
  },
  {
    "text": "But the question is\nlike, what data are we training this behavior on? So the most common example\nis you continue training",
    "start": "855500",
    "end": "865130"
  },
  {
    "text": "with this autoregressive\nloss function on question-answer pairs. So it's like, what\nis a transformer?",
    "start": "865130",
    "end": "871170"
  },
  {
    "text": "And then the language model\nwill predict an answer. Could be from stack overflow. It could be something else.",
    "start": "871170",
    "end": "876630"
  },
  {
    "text": "And this example is human data. But what made Alpaca and a\nlot of these early models,",
    "start": "876630",
    "end": "883010"
  },
  {
    "text": "and even today, really\npopular and accessible is by using data\nto answer questions",
    "start": "883010",
    "end": "888350"
  },
  {
    "text": "that is generated by an AI. So this is where the idea of\nself-instruct data comes in.",
    "start": "888350",
    "end": "895670"
  },
  {
    "text": "Self-instruct was a paper\nfrom Allen AI and UDub in 2022",
    "start": "895670",
    "end": "900990"
  },
  {
    "text": "before ChatGPT, where\nessentially the idea is, how do we expand on the\ndistribution and instruction",
    "start": "900990",
    "end": "908430"
  },
  {
    "text": "data that we have, this training\ndata for fine-tuning a language model without getting\nmore humans in the loop?",
    "start": "908430",
    "end": "914370"
  },
  {
    "text": "So what you really\nhave to do is you start with some high-quality,\noften human prompts.",
    "start": "914370",
    "end": "921150"
  },
  {
    "text": "And then what we now see as\nmore common practice today but was very new then is asking\na stronger language model.",
    "start": "921150",
    "end": "928660"
  },
  {
    "text": "Create a list of prompts\nthat are similar to this but still diverse. And then once you have\na list of prompts,",
    "start": "928660",
    "end": "935850"
  },
  {
    "text": "you can use ChatGPT\nor another model to actually generate\ncompletions. Because then what you\nhave is a really big list",
    "start": "935850",
    "end": "942900"
  },
  {
    "text": "of question-answer\npairs, but you don't need to go through the\nbottleneck of getting humans to sit down and\nwrite all of them.",
    "start": "942900",
    "end": "949410"
  },
  {
    "text": "So what Alpaca was really-- why Alpaca worked is because\nof realizing this and taking",
    "start": "949410",
    "end": "956250"
  },
  {
    "text": "in this better\nmodel from OpenAI. So you can see this\nfigure here on the right is from the Alpaca paper or\nblog post, one of the two.",
    "start": "956250",
    "end": "962850"
  },
  {
    "text": "They took this model from\nOpenAI and they asked it to generate more tasks.",
    "start": "962850",
    "end": "968620"
  },
  {
    "text": "So they had 175 to start, and\nthen they ended up with over 50,000 tasks.",
    "start": "968620",
    "end": "974272"
  },
  {
    "text": "And they also\ngenerated completions from this OpenAI model. And then what they did is they\ntook these Meta weights that",
    "start": "974272",
    "end": "981430"
  },
  {
    "text": "had just come out, and they\ninstruction fine-tuning them. And then you end up with Alpaca. This is a pattern.",
    "start": "981430",
    "end": "987667"
  },
  {
    "text": "This is a pattern that\nwe've seen many times with Alpaca, which is\nessentially, you take--",
    "start": "987667",
    "end": "992920"
  },
  {
    "text": "you generate some data from\na stronger language model, and you fine-tuning on it.",
    "start": "992920",
    "end": "998150"
  },
  {
    "text": "It sounds so obvious today,\nbut this was the first model to actually release this.",
    "start": "998150",
    "end": "1004160"
  },
  {
    "text": "I can now see\nquestions coming in. I'll answer the ones that are\nclarifying and stuff like this,",
    "start": "1004160",
    "end": "1009899"
  },
  {
    "text": "so Thanks for asking them. And we can come back\nto more at the end. Once Alpaca happened,\nit felt like there",
    "start": "1009900",
    "end": "1015940"
  },
  {
    "text": "was a new model every week. The second model was Vicuna. And really, what\nthey changed was",
    "start": "1015940",
    "end": "1021550"
  },
  {
    "text": "they added new sources of\nprompts to the distribution.",
    "start": "1021550",
    "end": "1027559"
  },
  {
    "text": "So you can see that\nI say ShareGPT. They also introduced the\nidea of LLM-as-a-judge,",
    "start": "1027560",
    "end": "1033339"
  },
  {
    "text": "which is now obvious from a lot\nof their later evaluation work. But let's talk about why\nShareGPT was so interesting.",
    "start": "1033339",
    "end": "1041319"
  },
  {
    "text": "So ShareGPT was one\nof the only data sets that got open\nlanguage model builders, so",
    "start": "1041319",
    "end": "1051040"
  },
  {
    "text": "people like me, prompts that\nwere similar to what people",
    "start": "1051040",
    "end": "1056110"
  },
  {
    "text": "were asking ChatGPT. So what was happening was you\nwould install this browser",
    "start": "1056110",
    "end": "1061870"
  },
  {
    "text": "plug-in, and it would let you\nshare your prompts from ChatGPT",
    "start": "1061870",
    "end": "1067300"
  },
  {
    "text": "on Twitter or whatever. So it was making it easier\nto share the prompts before-- in your conversations, before\nOpenAI made a tool to do this.",
    "start": "1067300",
    "end": "1075560"
  },
  {
    "text": "And now there's\nthis legal gray area over the data set because\nmost of these data sets",
    "start": "1075560",
    "end": "1081740"
  },
  {
    "text": "are unlicensed, and they\nwere created without consent. Or they were released\nwithout consent. So there's a legal question of\nwhether or not people should",
    "start": "1081740",
    "end": "1089900"
  },
  {
    "text": "be trading on this data. But the fact of the\nmatter is that ShareGPT was really important to this\nacceleration in progress",
    "start": "1089900",
    "end": "1096290"
  },
  {
    "text": "on fine-tuning models\nbecause the diversity of data is just so much\nstronger than what",
    "start": "1096290",
    "end": "1101600"
  },
  {
    "text": "people were going to get from\nthis Alpaca self-instruct idea. And it set the bar much higher.",
    "start": "1101600",
    "end": "1107330"
  },
  {
    "text": "It's only today in the last\nfew months or six months for some of them that were\ngetting data sets that",
    "start": "1107330",
    "end": "1113059"
  },
  {
    "text": "can replace these. So you see I mentioned\nLMSYS-Chat-1M, which is just a million\nconversations from ChatBotArena,",
    "start": "1113060",
    "end": "1120320"
  },
  {
    "text": "which took a lot of work to\nclean out personal information; and then a project from\nthe Allen Institute",
    "start": "1120320",
    "end": "1126290"
  },
  {
    "text": "of AI, which is WildChat, which\nis really similar to ShareGPT. But the users were giving\nconsent at the start",
    "start": "1126290",
    "end": "1133220"
  },
  {
    "text": "that their data was going\nto be collected and released in exchange for using a\nlanguage model for free.",
    "start": "1133220",
    "end": "1138680"
  },
  {
    "text": "So there's a lot of\nhappenstance in the story where something like this,\nwhich is legally gray--",
    "start": "1138680",
    "end": "1145250"
  },
  {
    "text": "and the data is still on Hugging\nface, but it looks kind of odd-- where these little things helped\nenable the ecosystem, even",
    "start": "1145250",
    "end": "1152990"
  },
  {
    "text": "though, looking\nback, it's like, oh, we don't know if that\nshould have happened. ",
    "start": "1152990",
    "end": "1160260"
  },
  {
    "text": "Following Vicuna is one called\nKoala, also from Berkeley. And look at the timeframes.",
    "start": "1160260",
    "end": "1167965"
  },
  {
    "text": "It's pretty obvious\nthat a lot of these were developed\nconcurrently, and then their release dates just happen\nto be slightly different.",
    "start": "1167965",
    "end": "1174120"
  },
  {
    "text": "Koala is mostly known for having\na different diverse set of data sets. They use some from Alpaca.",
    "start": "1174120",
    "end": "1180690"
  },
  {
    "text": "They use some from\nShareGPT again. They also use Anthropic\ndata that has been released,",
    "start": "1180690",
    "end": "1186420"
  },
  {
    "text": "and they had some human\nevaluation from grad students. So this just added\nmore data diversity, and the evaluations\nweren't necessarily better.",
    "start": "1186420",
    "end": "1193397"
  },
  {
    "text": "But it was an important\nmodel that a lot of people noticed just from bringing\nback up these new data",
    "start": "1193397",
    "end": "1198960"
  },
  {
    "text": "sets that had been in the\nliterature from years prior. Something that you might\nask looking at these slides",
    "start": "1198960",
    "end": "1205080"
  },
  {
    "text": "is that it's like, why\nweight differences? I have all these slides,\nlike, weight diff to LLaMA 7B,",
    "start": "1205080",
    "end": "1210480"
  },
  {
    "text": "weight diff. Essentially, when\nLlama was released, it was released\nas research only.",
    "start": "1210480",
    "end": "1215910"
  },
  {
    "text": "And you had to-- and it was\ndistributed to researchers upon request, and the\nlicense prohibited people",
    "start": "1215910",
    "end": "1221610"
  },
  {
    "text": "from updating Llama\n1 to Hugging Face. So it's this\nannoying phase where, in order to use a\nmodel on Hugging Face,",
    "start": "1221610",
    "end": "1227370"
  },
  {
    "text": "you had to clone it. And then you had to run a script\nto convert it with this delta",
    "start": "1227370",
    "end": "1232710"
  },
  {
    "text": "into the new model in\norder to actually use it. So this is a really frustrating\nphase from a user perspective",
    "start": "1232710",
    "end": "1239460"
  },
  {
    "text": "because it just made\nexperimentation have one more barrier to entry. And thankfully, it was\nchanged with Llama 2,",
    "start": "1239460",
    "end": "1245790"
  },
  {
    "text": "but it was really\nsomething that many people dealt with at the time. And we now today see\ndifferent license restrictions",
    "start": "1245790",
    "end": "1252420"
  },
  {
    "text": "on how Llama is used. I mean, the Llama\n3 released today--",
    "start": "1252420",
    "end": "1257460"
  },
  {
    "text": "essentially, if I fine-tuning\na model for my research and I release it, too, Llama\n3 needs to be in the name.",
    "start": "1257460",
    "end": "1264049"
  },
  {
    "text": "So if I want to release\na new Tulu model, it would have to be\nLlama 3, Tulu 4 DPO.",
    "start": "1264050",
    "end": "1270625"
  },
  {
    "text": "It's like, the namings\nare going to be crazy. But there's always\nbeen restrictions on using Llama weights\nor how you share them.",
    "start": "1270625",
    "end": "1278740"
  },
  {
    "text": "And the final model that\nI group into this batch of this real first\nswing was Dolly.",
    "start": "1278740",
    "end": "1284270"
  },
  {
    "text": "So Dolly was fine-tuning\nfrom a different base model. It was fine-tuning\nfrom the Pythia models from a Luther, which were\na suite of early scaling",
    "start": "1284270",
    "end": "1292059"
  },
  {
    "text": "experiments from EleutherAI,\nwhich is still used extensively. But they added some\nhuman-written data",
    "start": "1292060",
    "end": "1298000"
  },
  {
    "text": "to the loop, which is really\nimportant because almost all of the projects\nthat I'll mention",
    "start": "1298000",
    "end": "1303400"
  },
  {
    "text": "today talk about synthetic data\nor data derived from OpenAI. There's only a few of them that\nactually added new human data",
    "start": "1303400",
    "end": "1310090"
  },
  {
    "text": "to the loop, and this is what\neveryone remembered Dolly for. And a lot of its\nperformance limitations",
    "start": "1310090",
    "end": "1315250"
  },
  {
    "text": "were probably from\nthe base model Pythia, which is trained in a time\nwhere this type of inference",
    "start": "1315250",
    "end": "1320289"
  },
  {
    "text": "that people expect\nwasn't as popular. And it was before-- the scaling laws were\nthought of differently.",
    "start": "1320290",
    "end": "1328630"
  },
  {
    "text": "You can see through\nthese where we're going to start with different\nmodel sizes and different MT bench scores.",
    "start": "1328630",
    "end": "1334360"
  },
  {
    "text": "I'll talk about what MT\nbench is in a few slides. It's an evaluation tool. And this is really just to\nground you on what the--",
    "start": "1334360",
    "end": "1342610"
  },
  {
    "text": "how the scores would\nchange over time. So I have these\nthroughout the talk as we go through\ndifferent models",
    "start": "1342610",
    "end": "1348340"
  },
  {
    "text": "just to show how the scores\ncontinue to progress over time",
    "start": "1348340",
    "end": "1353679"
  },
  {
    "text": "from one small change to another\nas the community gets better at these things.",
    "start": "1353680",
    "end": "1358860"
  },
  {
    "text": "While I was talking\nabout human data, so remember, Dolly is\nall about human data. Probably still the single\nbusiest human coordination",
    "start": "1358860",
    "end": "1369680"
  },
  {
    "text": "project for data generation\nwas OpenAssistant. I think it's easy now if\nyou get into fine-tuning",
    "start": "1369680",
    "end": "1375470"
  },
  {
    "text": "to see the open system\ndata set and not realize how important it was\nto the process of alignment",
    "start": "1375470",
    "end": "1382940"
  },
  {
    "text": "in this whole summer. And it is still used today. So essentially, there's\nthis quote on the top. But the leaders ran\na community project",
    "start": "1382940",
    "end": "1391460"
  },
  {
    "text": "to generate\nhuman-written prompts and these\nhuman-written responses",
    "start": "1391460",
    "end": "1398090"
  },
  {
    "text": "in many different\nlanguages with rating them. So you could use it\nas preferences with-- the slide has a bug,\nwhere it's like,",
    "start": "1398090",
    "end": "1405320"
  },
  {
    "text": "it has over 10,000\nannotated trees and over 1,000 volunteers.",
    "start": "1405320",
    "end": "1410490"
  },
  {
    "text": "This is still used\nextensively today, will come up again in the talk. They also released models.",
    "start": "1410490",
    "end": "1415650"
  },
  {
    "text": "So the first one of the first\nmodel used on Hugging Chat, which I don't remember\nthe launch date of,",
    "start": "1415650",
    "end": "1421220"
  },
  {
    "text": "was an OpenAssistant model. So OpenAssistant was probably\nthe first majorly successful",
    "start": "1421220",
    "end": "1426929"
  },
  {
    "text": "project of the era, and the\ndata set is still used today. Where I will end\nthis talk is saying",
    "start": "1426930",
    "end": "1432780"
  },
  {
    "text": "that we need more\nthings like this, is really one of the\nmost important things, is we need more human\ndata in the open.",
    "start": "1432780",
    "end": "1441110"
  },
  {
    "text": "And this is a quick aside. It's kind of out of\nthe flow of the talk. But on April 28 of 2023--",
    "start": "1441110",
    "end": "1447710"
  },
  {
    "text": "typo on the slide-- of April\n28 of 2023, stable Vicuna was released from\nCarperAI, which looks now",
    "start": "1447710",
    "end": "1455360"
  },
  {
    "text": "like the style of\ntraining models, except for the data set,\nwhich is now popular.",
    "start": "1455360",
    "end": "1462980"
  },
  {
    "text": "They got PPO to work. They had some human\nevaluations that were solid. It was a good chat model.",
    "start": "1462980",
    "end": "1468840"
  },
  {
    "text": "It wasn't out of distribution,\nbut CarperAI was really ahead at the time.",
    "start": "1468840",
    "end": "1474720"
  },
  {
    "text": "And then it seems like\npriorities kind of shifted from stability. But it's important to know the\nextent by which there were still",
    "start": "1474720",
    "end": "1481610"
  },
  {
    "text": "some players who knew how to\ndo RLHF really early on, even though there were pretty rare.",
    "start": "1481610",
    "end": "1487000"
  },
  {
    "text": " This is the last--",
    "start": "1487000",
    "end": "1492682"
  },
  {
    "text": "this is the last\nslide of this kind of first chapter on\ninstruction tuning, was the idea of\nQLoRA, which unlocked",
    "start": "1492682",
    "end": "1499900"
  },
  {
    "text": "a whole new bunch of\nplayers into actually being able to fine-tune models.",
    "start": "1499900",
    "end": "1505250"
  },
  {
    "text": "So for the quick\n60-second overview, LoRA stands for Low\nRank Adaptation,",
    "start": "1505250",
    "end": "1511600"
  },
  {
    "text": "which is the idea of\ncan freeze some model-- you freeze most of\nthe model weights, and you add new weights to\nspecific layers that you can",
    "start": "1511600",
    "end": "1519490"
  },
  {
    "text": "then fine-tuning as if you were\nfine-tuning the whole model. You use the same approach\nof instruction data",
    "start": "1519490",
    "end": "1525790"
  },
  {
    "text": "with question answering, but\nit takes much less memory. QLoRA was a technique\nthat built upon this",
    "start": "1525790",
    "end": "1531820"
  },
  {
    "text": "by adding very specific\nquantization and GPU tricks to make it\nso that it memory",
    "start": "1531820",
    "end": "1536919"
  },
  {
    "text": "requirements to fine-tune\nmodels was even lower. Tim Dettmers and team also\nreleased this Guanaco model",
    "start": "1536920",
    "end": "1544060"
  },
  {
    "text": "with it, which was another\nbig step up in performance of these models.",
    "start": "1544060",
    "end": "1549780"
  },
  {
    "text": "I have a few more slides\non it, on the methods. So you can see on the right this\ndifference, full fine-tuning",
    "start": "1549780",
    "end": "1555440"
  },
  {
    "text": "and LoRA. They look similar, where LoRA\nyou have fewer parameters, as what the smaller shapes mean. And QLoRA, they\nquantize the base model",
    "start": "1555440",
    "end": "1563632"
  },
  {
    "text": "that you're propagating\ngradients through to save most of the memory. So this is an\napproximation of, if you're",
    "start": "1563632",
    "end": "1570770"
  },
  {
    "text": "fine-tuning different model\nsizes on the top, so 7 billion, 13 billion, 30 billion,\nwith full fine-tuning",
    "start": "1570770",
    "end": "1577130"
  },
  {
    "text": "different amount of bits\nbut full fine-tuning versus LoRa versus QLoRA. And you can see,\nfor reference, one",
    "start": "1577130",
    "end": "1585200"
  },
  {
    "text": "A100 GPU has about 80\ngigabytes of memory. And these are really\nhard GPUs to get.",
    "start": "1585200",
    "end": "1591350"
  },
  {
    "text": "Plenty of consumer\nGPUs will only have 24 to 32\ngigabytes of memory.",
    "start": "1591350",
    "end": "1596880"
  },
  {
    "text": "So you need to use these QLoRA\ntechniques to actually get the ability to fine-tune models\nat the 7 or 13 billion parameter",
    "start": "1596880",
    "end": "1605210"
  },
  {
    "text": "size.  And like, Guanaco\ndid this, and they",
    "start": "1605210",
    "end": "1610770"
  },
  {
    "text": "released 33 billion and\n65 billion parameter Llama fine-tunes, which\nwere clear steps up",
    "start": "1610770",
    "end": "1616380"
  },
  {
    "text": "in the kind of state\nof the art at the time. And they also figured out ways\nto filter this OpenAssistant",
    "start": "1616380",
    "end": "1622830"
  },
  {
    "text": "data set that I\nmentioned, and this kind of filtered version\nof OpenAssistant is what is still\nmost popular today.",
    "start": "1622830",
    "end": "1629910"
  },
  {
    "text": "I'm going to pause and\nskim through the questions and see if there's\nanything on that section. And if not, I'll save the\nrelevant ones for later.",
    "start": "1629910",
    "end": "1638270"
  },
  {
    "text": "OK, I'm going to keep going. They're great questions,\nand I appreciate them. But they're mostly\nnot specific enough",
    "start": "1638270",
    "end": "1645860"
  },
  {
    "text": "where it's worth the digression. This chapter 2 phase\nis really where",
    "start": "1645860",
    "end": "1651549"
  },
  {
    "text": "it seemed like things were a\nlittle bit slower on the ground. But when we look back at\na lot of the things that",
    "start": "1651550",
    "end": "1658360"
  },
  {
    "text": "came out of this time, like\nthe DPO paper was in this era. Everyone read it, but we didn't\nknow what to do with it yet.",
    "start": "1658360",
    "end": "1664490"
  },
  {
    "text": "And the new evaluations\nare still really used. Transitioning in,\nsetting the scene",
    "start": "1664490",
    "end": "1670890"
  },
  {
    "text": "for being not sure if\nthings work, a lot of people are continuing to try to build\non these LoRA methods and QLoRA",
    "start": "1670890",
    "end": "1678000"
  },
  {
    "text": "methods. I remember a lot of\nexcitement at Hugging Face, where we were setting\nup our RLHF pipeline,",
    "start": "1678000",
    "end": "1683340"
  },
  {
    "text": "where we could do RLHF on\n7 billion parameter models. And we could maybe do\nit on a consumer GPU.",
    "start": "1683340",
    "end": "1690010"
  },
  {
    "text": "It was really cool to\nsee the loss going down. It's great to bring more\npeople into the space.",
    "start": "1690010",
    "end": "1695340"
  },
  {
    "text": "But weeks and weeks would go by. And they're like, why\nhas no one picked up",
    "start": "1695340",
    "end": "1700500"
  },
  {
    "text": "what we released\nin the blog post and trained a really\ngood model with it? And the kind of consensus now\nis that these LoRA methods just",
    "start": "1700500",
    "end": "1710460"
  },
  {
    "text": "have some sort of weird\nlimitation in how you use them or how the gradients flow\nthat make it much, much harder",
    "start": "1710460",
    "end": "1717150"
  },
  {
    "text": "to get a really good model out. If you only have a\ncertain number of GPUs, such that LoRA is your only\noption, definitely use it.",
    "start": "1717150",
    "end": "1724360"
  },
  {
    "text": "But for people that have more\nGPUs, figuring out how to scale is normally a better\nsolution than just",
    "start": "1724360",
    "end": "1729520"
  },
  {
    "text": "using something like\nLoRA that fits in is easier in the short term.",
    "start": "1729520",
    "end": "1735299"
  },
  {
    "text": "Another defining moment of this\nera was the Llama 2 backlash. I'm guessing some\npeople remember",
    "start": "1735300",
    "end": "1741600"
  },
  {
    "text": "this, which is like, the famous\nline was people ask Llama how to kill a Python process.",
    "start": "1741600",
    "end": "1746909"
  },
  {
    "text": "And it would say, no. And this really started a\nwhole bunch of new discussions",
    "start": "1746910",
    "end": "1752010"
  },
  {
    "text": "around what kind\nof alignment means or what model should\nor should not do.",
    "start": "1752010",
    "end": "1757840"
  },
  {
    "text": "Here's an example from a\npaper for a safety evaluation test set called XSTest,\nand it's just like,",
    "start": "1757840",
    "end": "1765179"
  },
  {
    "text": "should chat models be safe? Or should they follow the\ninstructions that I want?",
    "start": "1765180",
    "end": "1770470"
  },
  {
    "text": "And this is a\nfundamental question. It'll differ by\norganization, and it will differ by individual.",
    "start": "1770470",
    "end": "1775570"
  },
  {
    "text": "And this is the point where\nthis became very serious and something that\npeople actually had to reckon with because there\nwere models that were actively--",
    "start": "1775570",
    "end": "1784960"
  },
  {
    "text": "people were really disagreeing\nwith the specific take. I don't have any\nclear solution to it.",
    "start": "1784960",
    "end": "1790830"
  },
  {
    "text": "But one of the\nthings that led to is this idea of\nuncensored models. It's a really popular\ncategory on Hugging Face",
    "start": "1790830",
    "end": "1801120"
  },
  {
    "text": "right now, where the idea is you\nremove filtering-- so if we're using synthetic data\nand ask a language model",
    "start": "1801120",
    "end": "1806610"
  },
  {
    "text": "a question-- like, if I ask\nChatGPT how to make a bomb, it's going to say, I'm sorry. I'm a language model.",
    "start": "1806610",
    "end": "1811630"
  },
  {
    "text": "I shouldn't make this. And the idea of\nuncensored models is to remove those\npoints from our--",
    "start": "1811630",
    "end": "1818300"
  },
  {
    "text": "remove those points from\nour fine-tuning data set. I think there's a lot of\nconfusion over the name",
    "start": "1818300",
    "end": "1823680"
  },
  {
    "text": "because language\nmodels were never-- at this stage, really aren't\ncensored to begin with.",
    "start": "1823680",
    "end": "1828870"
  },
  {
    "text": "But it's really\nthat the data set and the method for\ncreating these data sets needed more\nfiltering, or they needed",
    "start": "1828870",
    "end": "1836220"
  },
  {
    "text": "some way of becoming unbiased. So there's a lot\nof people now that only build models to try\nto make them unbiased",
    "start": "1836220",
    "end": "1844059"
  },
  {
    "text": "against any sort of refusal. A refusal is when you ask\na language model something, and it says, no. And this goes on today, and this\ncame out of this Llama 2 thing.",
    "start": "1844060",
    "end": "1854169"
  },
  {
    "text": "But otherwise, this\nis a transition period where there is a lot of good,\nsolid models being trained.",
    "start": "1854170",
    "end": "1860530"
  },
  {
    "text": "But either they didn't have\na lot of documentation. They didn't have the\nright release team to splash as big as\nthey should have.",
    "start": "1860530",
    "end": "1866840"
  },
  {
    "text": "The methods were\ncomplicated to implement or something like this. So I could run through\nthese, and I remember",
    "start": "1866840",
    "end": "1872380"
  },
  {
    "text": "all these models coming out. But none of them were really\nthings that are household names like Alpaca is today.",
    "start": "1872380",
    "end": "1878649"
  },
  {
    "text": "Like, the team from\nbehind WizardLM where they created this method\ncalled EvolInstruct, which",
    "start": "1878650",
    "end": "1884530"
  },
  {
    "text": "is a synthetic data method-- like, all these\nthings were clearly working for them based on the\nmodels they were generating.",
    "start": "1884530",
    "end": "1891490"
  },
  {
    "text": "But for whatever\nreason, the narrative wasn't actually changed. There's some new data\nsets from ultra--",
    "start": "1891490",
    "end": "1898630"
  },
  {
    "text": "like, UltraLM is from an\nopen BNB in China that is releasing new data sets, more\npeople training on ShareGPT.",
    "start": "1898630",
    "end": "1905770"
  },
  {
    "text": "The model called XwinLM\nwas the first one to be a similar\nballpark, and it's also",
    "start": "1905770",
    "end": "1913000"
  },
  {
    "text": "trained with RLHF, so not\njust that carbon model. But for whatever reason, it's\nlike these didn't really splash.",
    "start": "1913000",
    "end": "1920029"
  },
  {
    "text": "And that was this\nsummer after Llama 2, where fine-tuning\nwas chugging along.",
    "start": "1920030",
    "end": "1926000"
  },
  {
    "text": "But the narrative wasn't\nchanging all that much, at least from my perspective. But that's why I'm here.",
    "start": "1926000",
    "end": "1931500"
  },
  {
    "text": " But what was happening\nin the background while the models weren't\nseeming that different",
    "start": "1931500",
    "end": "1939710"
  },
  {
    "text": "is that new evaluation tools\nwere coming out that ended up being the standard of today.",
    "start": "1939710",
    "end": "1945399"
  },
  {
    "text": "So you can see the dates here. So May 3, ChatBotArena, June 8,\nAlpacaEval; June 22, MT Bench.",
    "start": "1945400",
    "end": "1951210"
  },
  {
    "text": "Sometime in early July,\nthe Open LLM Leaderboard. All of these things were created\nabout the same time, where there",
    "start": "1951210",
    "end": "1956930"
  },
  {
    "text": "was a desperate need to\nget some sort of signal on what our fine-tuned\nmodels are doing in the open.",
    "start": "1956930",
    "end": "1963140"
  },
  {
    "text": "We don't have the\ncapability of paying humans to compare our\nresponses, like they do at anthropic,\nwhere they're always",
    "start": "1963140",
    "end": "1970040"
  },
  {
    "text": "trying new models on humans. That's way too expensive. We need something that you\ncould sit down as an engineer",
    "start": "1970040",
    "end": "1975950"
  },
  {
    "text": "and get feedback in\n10 to 15 minutes. So I run through these in order.",
    "start": "1975950",
    "end": "1981570"
  },
  {
    "text": "And a lot of these\nare obvious, but it's important to take this\nfrom the perspective of, what can I use when I'm\ntrying to align models?",
    "start": "1981570",
    "end": "1989210"
  },
  {
    "text": "And what is an\nimmediate feedback versus what is this\nlong-term signal? So ChatBotArena is\nobviously fantastic.",
    "start": "1989210",
    "end": "1997560"
  },
  {
    "text": "Like, everyone looks at\nthis today as something that is defining--",
    "start": "1997560",
    "end": "2003070"
  },
  {
    "text": "it's like defining\ncorporate strategy. It's defining the biggest\nlanguage model players, like if Claude 3 is\nbetter than GPT-4.",
    "start": "2003070",
    "end": "2010760"
  },
  {
    "text": "But if I'm an engineer, A,\nmany small [INAUDIBLE] riders aren't going to get\ntheir models in.",
    "start": "2010760",
    "end": "2016480"
  },
  {
    "text": "And B, it takes-- especially previously,\nit used to take weeks to get your model's rating.",
    "start": "2016480",
    "end": "2022419"
  },
  {
    "text": "But now it takes days. Like, I need to know what\nmy models are before I decide to actually release it.",
    "start": "2022420",
    "end": "2029090"
  },
  {
    "text": "So that's the\nbiggest thing, where I know I need something\nbeyond ChatBotArena, just for my engineering development.",
    "start": "2029090",
    "end": "2036990"
  },
  {
    "text": "And this is where\nI like AlpacaEval and MT Bench really thrive. So AlpacaEval got-- slide\nformat and got changed,",
    "start": "2036990",
    "end": "2043860"
  },
  {
    "text": "but I'll just keep\nrolling through this. AlpacaEval is the idea, if\nyou have a list of promts,",
    "start": "2043860",
    "end": "2050100"
  },
  {
    "text": "that you compare to a\nstrong other base model, like OpenAI's\nDavinci-003 or GPT-4.",
    "start": "2050100",
    "end": "2058860"
  },
  {
    "text": "And then you ask a language\nmodel which is better. And the data set\nhere is compiled from all these popular data sets\nthat I have been talking about",
    "start": "2058860",
    "end": "2067860"
  },
  {
    "text": "so far, so data sets from\nOpen Assistant, Vicuna, Koala, Anthropic. All these data sets that\npeople have been using--",
    "start": "2067860",
    "end": "2075908"
  },
  {
    "text": "they took the test\nsets from those. And that's what\nAlpacaEval mirrors. It's a known thing.",
    "start": "2075909",
    "end": "2082138"
  },
  {
    "text": "It has some limitations because\nthere's only so many prompts. And it's like asking a language\nmodel to provide a rating,",
    "start": "2082139",
    "end": "2089969"
  },
  {
    "text": "is going to have\nsome ceiling where we don't know how to compare\ntwo really good models. So it has more samples than\nMT Bench, so there's more--",
    "start": "2089969",
    "end": "2099839"
  },
  {
    "text": "so there's just\nsmaller error bars, and it's easier to use because\nit's a single-turn generation.",
    "start": "2099840",
    "end": "2105610"
  },
  {
    "text": "But we've heard about the length\nbias for a really long time, and it's not clear how to\ninterpret these top results.",
    "start": "2105610",
    "end": "2111970"
  },
  {
    "text": "So this is an older\nscreenshot of the leaderboard. But what is beating a\nmodel 95% of the time",
    "start": "2111970",
    "end": "2119190"
  },
  {
    "text": "mean to another language model? That's the questions\nthat we can't really answer in the short term.",
    "start": "2119190",
    "end": "2125640"
  },
  {
    "text": "AlpacaEval 2 came out, which\ntakes steps to this, where it-- rate compares to GPT-4\nrather than Davinci-003.",
    "start": "2125640",
    "end": "2134280"
  },
  {
    "text": "Davinci-003 was an\nInstructGPT variant. But at the end of\nthe day, GPT-4 is",
    "start": "2134280",
    "end": "2142200"
  },
  {
    "text": "answering these questions in\nthe Alpaca style really well. So what does beating\nGPT-4 exactly mean?",
    "start": "2142200",
    "end": "2149730"
  },
  {
    "text": "And I think we need to get more\nspecific in our evaluations because I don't really know if I\ncare too much about a 20% or 30%",
    "start": "2149730",
    "end": "2157440"
  },
  {
    "text": "score on AlpacaEval 2 because\nI don't know what it means. And this is one this\nis the opaqueness",
    "start": "2157440",
    "end": "2162810"
  },
  {
    "text": "of all of our evaluations. We'll see this time\nand time again, where we don't know what\nan increase in score means.",
    "start": "2162810",
    "end": "2168730"
  },
  {
    "text": "That's like the next step after\nbeing able to do it easily. This update was pretty recent.",
    "start": "2168730",
    "end": "2176140"
  },
  {
    "text": "MT Bench is pretty\nsimilar, where, instead of comparing\nto one model,",
    "start": "2176140",
    "end": "2181420"
  },
  {
    "text": "you ask a language\nmodel to provide a score to a list of prompts. So if I have a\nmodel I'm training,",
    "start": "2181420",
    "end": "2187030"
  },
  {
    "text": "I generate the completion\nto 80 diverse prompts. And then I ask GPT-4,\nhey, from 0 to 10,",
    "start": "2187030",
    "end": "2193180"
  },
  {
    "text": "how good were each\nof those completions? And this is good, but it\nruns into the same problem",
    "start": "2193180",
    "end": "2199840"
  },
  {
    "text": "of like, what if our model\nis getting really good? If our model is getting\nreally good, it's just like--",
    "start": "2199840",
    "end": "2206380"
  },
  {
    "text": "it becomes saturated. GPT-4 only gets to\nabout 9, and there's",
    "start": "2206380",
    "end": "2211540"
  },
  {
    "text": "only about 80 prompts\nin the evaluation set and all of these things. And one of the nuanced points\nis that there's actually",
    "start": "2211540",
    "end": "2218760"
  },
  {
    "text": "a variance. So even if you set\nthe temperature to 0, GPT-4 versions change.",
    "start": "2218760",
    "end": "2223950"
  },
  {
    "text": "Your own generations from your\nmodel you're trying to train can change. And this makes it better,\nwhere it's like, OK,",
    "start": "2223950",
    "end": "2232539"
  },
  {
    "text": "I can tell if a\nmodel was really bad, if MT Bench and AlpacaEval\nhave really low scores. But it's hard to--",
    "start": "2232540",
    "end": "2239830"
  },
  {
    "text": "still, we have this goal\nfor a precise evaluation. So in pretraining, we have\nMT Bench and HellaSwag",
    "start": "2239830",
    "end": "2248529"
  },
  {
    "text": "and all-- or sorry. In pretraining, we have MMLU and\nHellaSwag and all these things",
    "start": "2248530",
    "end": "2253630"
  },
  {
    "text": "that people can look at\nand average over 10 tasks. And if you get a 2% improvement\non average, you're doing great.",
    "start": "2253630",
    "end": "2259700"
  },
  {
    "text": "But we don't have\nthis clear indicator in alignment evaluation. The Open LLM Leaderboard\nwas the same, where it's--",
    "start": "2259700",
    "end": "2268109"
  },
  {
    "text": "this came out of the team I\nwas working on at Hugging Face, where we were just trying\nto evaluate more models",
    "start": "2268110",
    "end": "2273930"
  },
  {
    "text": "to get more signal,\nwhich was that we needed to know what our\ncompetitors are doing and get some ballpark estimate.",
    "start": "2273930",
    "end": "2280240"
  },
  {
    "text": "And what this grew\ninto is this whole kind of ecosystem supporting\ndiscovery tool",
    "start": "2280240",
    "end": "2285329"
  },
  {
    "text": "just because getting\nany signal is so useful. So this is where we're\nstarting with evaluation, which was just no signal.",
    "start": "2285330",
    "end": "2291330"
  },
  {
    "text": "And why this leaderboard\nwas so successful is because it gave\neveryone access to a little bit more\nsignal on the models",
    "start": "2291330",
    "end": "2297270"
  },
  {
    "text": "that they're evaluation. But it didn't really solve any\nof the fundamental problems, and it didn't show us that,\nlike, doing offline models",
    "start": "2297270",
    "end": "2304740"
  },
  {
    "text": "would actually make\nthe scores go up. It's starting to\nget better today, but that's a year on from the\nlaunch of this leaderboard.",
    "start": "2304740",
    "end": "2312230"
  },
  {
    "text": "So this is like, these\nproblems are still-- this is talking about a\nsection from July of 2023.",
    "start": "2312230",
    "end": "2317996"
  },
  {
    "text": "And it seems like\nthe things that if I were to go into-- go into work\nand talk to people, what we're going to do with our\nnew models, like,",
    "start": "2317997",
    "end": "2324170"
  },
  {
    "text": "these are the same questions\nthat we're still asking, which is why these evaluation\nschools are still so useful.",
    "start": "2324170",
    "end": "2331220"
  },
  {
    "text": "And it's why people still\ntalk about AlpacaEval. But it shows how much of an\nopportunity there still is.",
    "start": "2331220",
    "end": "2336990"
  },
  {
    "text": "So this is a summary of\nwhat I was talking about. It's like, how easy is it\nto use these evaluations?",
    "start": "2336990",
    "end": "2343160"
  },
  {
    "text": "ChatBotArena is everything. Andre Karpathy tweets\nabout it, and it's great.",
    "start": "2343160",
    "end": "2348210"
  },
  {
    "text": "And you can go there,\nand you can use models. But like, I don't know\nhow to make sense of that,",
    "start": "2348210",
    "end": "2354109"
  },
  {
    "text": "as if I'm trying to sit down\nevery day and write code. And the AlpacaEval\nand MT Bench mostly",
    "start": "2354110",
    "end": "2360200"
  },
  {
    "text": "solve this by being cheap\nand pretty accessible. But I really, really think\nthere's a huge opportunity here",
    "start": "2360200",
    "end": "2365930"
  },
  {
    "text": "to come out with more. So like, a colleague at AI2\nlaunched WildBench, which is a good tool that fits in.",
    "start": "2365930",
    "end": "2373130"
  },
  {
    "text": "It's like a ChatBotArena\nAlpacaEval hybrid, and you can use it\na little bit faster. It's like, how are we going to\ncontinue to push this along?",
    "start": "2373130",
    "end": "2380360"
  },
  {
    "text": "is a great question, and I would\nlove to hear what people think. ",
    "start": "2380360",
    "end": "2386960"
  },
  {
    "text": "And take another pause. I think we're getting\ngood questions in the chat around RLHF and other things.",
    "start": "2386960",
    "end": "2395190"
  },
  {
    "text": "To what extent are-- do\naligned models actually reason about whether\nuser intent is malicious",
    "start": "2395190",
    "end": "2401670"
  },
  {
    "text": "rather than perform target\ndetection to detect-- to avoid unsafe topics? This is a question\nthat I wanted to read",
    "start": "2401670",
    "end": "2408360"
  },
  {
    "text": "because it gets at this\nmodel versus system topic. So when ChatGPT was\nreleased on day one,",
    "start": "2408360",
    "end": "2414539"
  },
  {
    "text": "it has an output filter\nthat does moderation. The language model that is\ninstruction [INAUDIBLE] RLHF",
    "start": "2414540",
    "end": "2420320"
  },
  {
    "text": "[? too ?] and generates\na bunch of text, and then a separate model says yes or no.",
    "start": "2420320",
    "end": "2425710"
  },
  {
    "text": "And that's where it\nactually does detection. And with the release\nof Llama 3, there's",
    "start": "2425710",
    "end": "2430890"
  },
  {
    "text": "another model that's\ncalled Llama Guard. And this is a classifier\nwhich will take this text,",
    "start": "2430890",
    "end": "2436230"
  },
  {
    "text": "do the moderation, and say which\ntype of unsafe topic it is. The actual model\nthat is generating",
    "start": "2436230",
    "end": "2442690"
  },
  {
    "text": "does no reasoning over what\nis actually an unsafe topic. ",
    "start": "2442690",
    "end": "2449700"
  },
  {
    "text": "So I'll come back\nto the other ones. I'm going to do some discussions\nabout RLHF right now.",
    "start": "2449700",
    "end": "2456240"
  },
  {
    "text": "So this will give good grounds\nfor where we can continue some of these discussions on.",
    "start": "2456240",
    "end": "2462589"
  },
  {
    "text": "There's ORPO or Reinforce. I don't cover all of\nthem in the lecture, but I lead on to why we\nwould talk about them.",
    "start": "2462590",
    "end": "2469270"
  },
  {
    "text": " So this chapter is when I\nstarted to get validation",
    "start": "2469270",
    "end": "2476590"
  },
  {
    "text": "as an RL researcher that\nbeing opportunistic and going to work in language models\nwas actually a good idea.",
    "start": "2476590",
    "end": "2483760"
  },
  {
    "text": "For a lot of this, there\nwas a lot of uncertainty over if the people--",
    "start": "2483760",
    "end": "2489160"
  },
  {
    "text": "if people in this open ecosystem\nwere even going to be able to use RLHF at all, or if\nbeing a, quote unquote,",
    "start": "2489160",
    "end": "2495730"
  },
  {
    "text": "\"RLHF researcher\" for me meant\nI was going to do instruction, fine-tuning, and talk about\n[INAUDIBLE] and never think",
    "start": "2495730",
    "end": "2501580"
  },
  {
    "text": "about RL again. It turned out to be wrong. ",
    "start": "2501580",
    "end": "2507490"
  },
  {
    "text": "I'm going to review\nsome fundamentals, just to make sure we're\ntalking the same language as we talk about this.",
    "start": "2507490",
    "end": "2512660"
  },
  {
    "text": "And this will lead into direct\npreference optimization. So there's a reason\nwhy I'm doing math. I know this is not\na normal lecture,",
    "start": "2512660",
    "end": "2519280"
  },
  {
    "text": "but here's the equation where-- you'll see this in RLHF papers. This is what we're optimizing\nwhen we're optimizing RLHF.",
    "start": "2519280",
    "end": "2526829"
  },
  {
    "text": "It looks nebulous here. I'll break it down. So on the left side, we're\nreally maximizing with respect",
    "start": "2526830",
    "end": "2533339"
  },
  {
    "text": "to some policy pi. This reward that is\nparametrized by a network phi,",
    "start": "2533340",
    "end": "2538680"
  },
  {
    "text": "and we're have a\npenalty that is this KL term, which is the distance from\nour policy to some reference.",
    "start": "2538680",
    "end": "2545789"
  },
  {
    "text": "We want to increase reward, but\nwe want to constrain the model so that it doesn't-- this kind\nof optimization doesn't go too",
    "start": "2545790",
    "end": "2552150"
  },
  {
    "text": "far. And the primary\nquestions when doing this is, how do we implement\na good reward function?",
    "start": "2552150",
    "end": "2559200"
  },
  {
    "text": "And how do we\noptimize the reward? This is a really\nRL-centric way of doing it, which is like, if you give me a\nreward function, I can optimize.",
    "start": "2559200",
    "end": "2567960"
  },
  {
    "text": "And the classic RL idea\nwas, I'm in an environment. That environment has the\nreward function built in.",
    "start": "2567960",
    "end": "2574290"
  },
  {
    "text": "In RLHF, we're designing\nour own reward function. So this adds a lot of weirdness\nto the actual optimization",
    "start": "2574290",
    "end": "2581970"
  },
  {
    "text": "that we're doing. And what we do is-- to get this reward\nfunction is we",
    "start": "2581970",
    "end": "2588280"
  },
  {
    "text": "learn what is called a\npreference or a reward model. And the most popular\nway to do this is to take a\nlanguage model that's",
    "start": "2588280",
    "end": "2596890"
  },
  {
    "text": "predicting the separation\nof two preferences. This is called a\nBradley-Terry model, which",
    "start": "2596890",
    "end": "2602260"
  },
  {
    "text": "goes back to some economics. But the key idea\nis that the reward will be proportional\nto the probability",
    "start": "2602260",
    "end": "2609040"
  },
  {
    "text": "that the text they\nhave would be chosen over any other arbitrary text. [? Quickly ?] sounds\nreally theory like,",
    "start": "2609040",
    "end": "2614980"
  },
  {
    "text": "but it outputs a scalar, which\nis now a reward function. And it's based on\nthis pairwise data. ",
    "start": "2614980",
    "end": "2622300"
  },
  {
    "text": "So the idea is,\nwith this equation, what if we just use gradient\nascent on this equation?",
    "start": "2622300",
    "end": "2628060"
  },
  {
    "text": "And instead of trying to\nlearn a preference model and learn this R,\nwhat if we just",
    "start": "2628060",
    "end": "2633760"
  },
  {
    "text": "use gradient ascent directly? This is really what direct\npreference optimization is doing. There's a bunch of math in\nhere to get what this R is.",
    "start": "2633760",
    "end": "2641570"
  },
  {
    "text": "But this was\nreleased back in May, so we've already\nmoved on months ahead. This chapter starts in\nlate September, October.",
    "start": "2641570",
    "end": "2648250"
  },
  {
    "text": "Back in May, when we're still\ntalking about OpenAssistant, this DPO paper came out. It's a fantastic paper.",
    "start": "2648250",
    "end": "2654593"
  },
  {
    "text": "If you haven't read\nit, it's a great way to learn about\nlanguage model math. It's worth reading.",
    "start": "2654593",
    "end": "2659960"
  },
  {
    "text": "But the core idea is like, why\nare we spending all this time learning a reward model when\nwe can just use gradient ascent",
    "start": "2659960",
    "end": "2667300"
  },
  {
    "text": "and solve for the loss function? Some key ideas to\nthink about with DPO",
    "start": "2667300",
    "end": "2672850"
  },
  {
    "text": "is that DPO is extremely\nsimple to implement. On the right here side\nis the example code",
    "start": "2672850",
    "end": "2678700"
  },
  {
    "text": "from the DPO paper, where it's\nlike, as long as you have access to the log probs\nfrom a model, which",
    "start": "2678700",
    "end": "2684440"
  },
  {
    "text": "is a very core thing for\ntraining language models, you can compute the DPO loss.",
    "start": "2684440",
    "end": "2689900"
  },
  {
    "text": "Because of this, because\nthe loss function is at a nice abstraction,\nit scales nicely",
    "start": "2689900",
    "end": "2695180"
  },
  {
    "text": "with existing libraries. And what it's actually doing\nis training an implicit reward",
    "start": "2695180",
    "end": "2700910"
  },
  {
    "text": "function. So the reward is a\nfunction of the log probs. I don't have the equation\nhere, because it quickly",
    "start": "2700910",
    "end": "2706070"
  },
  {
    "text": "becomes a rabbit hole. But whatever the whole DPO\nversus PPO debate means or--",
    "start": "2706070",
    "end": "2713900"
  },
  {
    "text": "or RPO, I don't even remember\nwhat the paper, it's title is, we're going to see\na lot of these things",
    "start": "2713900",
    "end": "2719990"
  },
  {
    "text": "because it's simple,\nand it scales well. That doesn't necessarily mean\nthe fundamental limits are higher, but sometimes\nit doesn't matter",
    "start": "2719990",
    "end": "2726740"
  },
  {
    "text": "if the limits are higher if\nit's easier to make progress on something because\nit feels better when progress is being made.",
    "start": "2726740",
    "end": "2733015"
  },
  {
    "text": "So that's really\na core thing, is we'll keep seeing these models. And we are. And there's this\nwhole debate that",
    "start": "2733015",
    "end": "2739160"
  },
  {
    "text": "has gone on crushing a whole\nbunch of these questions by redirecting them in\na very political manner.",
    "start": "2739160",
    "end": "2746180"
  },
  {
    "text": "But it's like, should\nwe use REINFORCE? What about other things?",
    "start": "2746180",
    "end": "2751330"
  },
  {
    "text": "They're very different\nstyles of optimization. So in one half, we're\nusing RL update rules,",
    "start": "2751330",
    "end": "2758320"
  },
  {
    "text": "which is ultimately about\nlearning a value function and then learning to update-- taking gradient steps with\nrespect to that value function.",
    "start": "2758320",
    "end": "2765650"
  },
  {
    "text": "In DPO we're taking\ngradient steps directly from the probabilities\nof the language model. They're very different\noptimization regimes.",
    "start": "2765650",
    "end": "2772400"
  },
  {
    "text": "And there's this\ngreat meme where all-- there was a month where the\nwhole NLP Twitter was just",
    "start": "2772400",
    "end": "2778359"
  },
  {
    "text": "arguing about this. But both of them are continuing\nto progress, and that is good.",
    "start": "2778360",
    "end": "2785530"
  },
  {
    "text": "Like, it will not just\nbe one or the other. ",
    "start": "2785530",
    "end": "2790579"
  },
  {
    "text": "So what really made this\ndebate kick into gear was this release of the Zephyr\nbeta model from Hugging Face.",
    "start": "2790580",
    "end": "2797250"
  },
  {
    "text": "It's after I left Hugging\nFace for the team was on, and it was the first model\nto make a splash with DPO.",
    "start": "2797250",
    "end": "2802460"
  },
  {
    "text": "And it was a big step up in\nhow models were perceived. This model was added to\nthe You search engine.",
    "start": "2802460",
    "end": "2807530"
  },
  {
    "text": "People are using all\nsorts of crazy things, so it just felt\nreally good to use. It was building on\nthis better base model.",
    "start": "2807530",
    "end": "2813299"
  },
  {
    "text": "Mistral had come out. And new data set, this\nUltraFeedback data set that I mentioned\nis still one",
    "start": "2813300",
    "end": "2818450"
  },
  {
    "text": "of the core data sets used today\nwhen we're practicing alignment. This was back in\nSeptember or October",
    "start": "2818450",
    "end": "2825140"
  },
  {
    "text": "that this model came back. One of the core things to\ngetting DPO to work was using",
    "start": "2825140",
    "end": "2830300"
  },
  {
    "text": "really low learning\nrates, like 5E minus 7. There's memes about 3E minus 4\nbeing the only learning rate you",
    "start": "2830300",
    "end": "2836510"
  },
  {
    "text": "need to do deep learning\nand changing it being kind of a joke. DP0 is the case where that\nis not even remotely true.",
    "start": "2836510",
    "end": "2843450"
  },
  {
    "text": "And then you can see the\nMT Bench scores, again, continuing to rise. So this is a validation\nproof that DPO works.",
    "start": "2843450",
    "end": "2850059"
  },
  {
    "text": "That came four months after\nthe paper was released. That delay is something\nthat nobody expected.",
    "start": "2850060",
    "end": "2855970"
  },
  {
    "text": "We were losing hope\non DPO at many times, and now look at where it is. And then when I joined\nAI2, they were already",
    "start": "2855970",
    "end": "2863220"
  },
  {
    "text": "working on this project. And I just helped kind of\nget it across the line. It's like the classic advisor\nthing, where sometimes it's",
    "start": "2863220",
    "end": "2869370"
  },
  {
    "text": "just easier. And it is the first model to\nscale to 70 billion parameters. The last question was, oh,\nyeah, DPO works on small models.",
    "start": "2869370",
    "end": "2877120"
  },
  {
    "text": "Will anyone ever use\nit on a big model? And answer is, yes. And it's built on\nthe same recipe",
    "start": "2877120",
    "end": "2882180"
  },
  {
    "text": "as Zephyr with a little bit\ndifferent instruction tuning data sets. But scores continue to climb.",
    "start": "2882180",
    "end": "2889800"
  },
  {
    "text": "This model was so close to\nbeating GPT-3.5 on Chatbot Arena. It was like a couple\nElo points below.",
    "start": "2889800",
    "end": "2896987"
  },
  {
    "text": "So we didn't get the title of\nbeing the first one to do that. But open models were starting to\nget that kind of chatty behavior",
    "start": "2896988",
    "end": "2904020"
  },
  {
    "text": "that for so long had eluded them\nbecause we hadn't figured out scale, because we hadn't\nfigured out these data sets.",
    "start": "2904020",
    "end": "2909670"
  },
  {
    "text": "So it was great progress, very\nimportant, and major transition in my career, where\nnow it's like, OK,",
    "start": "2909670",
    "end": "2915650"
  },
  {
    "text": "our RLHF methods\nreally can work. And these weren't just-- I was not the only one\ntouching things that did this.",
    "start": "2915650",
    "end": "2923250"
  },
  {
    "text": "A couple other projects that\nare really important-- so NVIDIA had SteerLM, where SteerLM was\ncollecting feedback data where",
    "start": "2923250",
    "end": "2931670"
  },
  {
    "text": "there was attributes on it, like\nhow helpful the message was, how concise the message was. And they did a\nbunch of fine-tuning",
    "start": "2931670",
    "end": "2937610"
  },
  {
    "text": "and released good,\nvery solid models. And they also showed\nthat PPO is better",
    "start": "2937610",
    "end": "2942890"
  },
  {
    "text": "than DPO, which is interesting. And then Berkeley came out\nwith this Starling-LM-alpha,",
    "start": "2942890",
    "end": "2948770"
  },
  {
    "text": "where they had a new\npreference data set, Nectar, which is still looked at today.",
    "start": "2948770",
    "end": "2954210"
  },
  {
    "text": "And then they also use\nthis kind of PPO method after training a reward model. And both of these came\nout about the same time.",
    "start": "2954210",
    "end": "2960170"
  },
  {
    "text": "And they're like, huh, DPO\nisn't doing as well for us. The models are really good. Recently, the second\nStarling model came out.",
    "start": "2960170",
    "end": "2968009"
  },
  {
    "text": "Its reward model is very\nstrong in my testing. It's a 7B model that's almost\nreaching ChatGPT levels",
    "start": "2968010",
    "end": "2974839"
  },
  {
    "text": "and Chatbot Arena. It's crazy how fast\nthese models are going, but we still get a\nlot of models that",
    "start": "2974840",
    "end": "2980160"
  },
  {
    "text": "are built with PPO or with DPO. It's really not one or\nthe other at this point. ",
    "start": "2980160",
    "end": "2992740"
  },
  {
    "text": "OK I think this is a\nreasonable time for me to take a couple of these questions. I might come back to\nthem in more slides.",
    "start": "2992740",
    "end": "2998380"
  },
  {
    "text": "But someone asks, is there a\nparticular alignment method that I use?",
    "start": "2998380",
    "end": "3005290"
  },
  {
    "text": "This is teasing a paper, but\nthere was a recent paper that came out where--",
    "start": "3005290",
    "end": "3010420"
  },
  {
    "text": "I don't remember the group. I can find it later. But they did what they call a\nsystematic study of PPO and DPO,",
    "start": "3010420",
    "end": "3017380"
  },
  {
    "text": "and they showed\nthat PPO is better. I will say that in the\nexperiments that I'm seeing, AllenAI.",
    "start": "3017380",
    "end": "3022840"
  },
  {
    "text": "I'm also seeing\nPPO to be stronger. And we hope to release\nthis stuff soon.",
    "start": "3022840",
    "end": "3028570"
  },
  {
    "text": "It's not a one\ncrushes the other. It's that we're seeing that\nthere's-- for some reason,",
    "start": "3028570",
    "end": "3034300"
  },
  {
    "text": "PPO is just getting a\nbit more performance. And then the logical question\nis, why not reinforce?",
    "start": "3034300",
    "end": "3040059"
  },
  {
    "text": "which is another one\nof these questions. I would love to try it. It's just like, we have\nthe code that we have.",
    "start": "3040060",
    "end": "3045160"
  },
  {
    "text": "And we don't want to touch\nthings that are working well. And there's just\nso few people that are working in this\nspace, which are--",
    "start": "3045160",
    "end": "3051220"
  },
  {
    "text": "I'm like, let's get more\npeople working on these things because there's\nso few people that",
    "start": "3051220",
    "end": "3057078"
  },
  {
    "text": "can answer all these questions. So there's another\nquestion that says, like, some say REINFORCE can\nwork as well as, if not better,",
    "start": "3057078",
    "end": "3064490"
  },
  {
    "text": "than PPO. It probably can. It comes down to\nyour infrastructure, carefully fine-tuning it,\nwhat people are excited about,",
    "start": "3064490",
    "end": "3072320"
  },
  {
    "text": "and a lot of luck. So we'll see these continue to\nplay out throughout the year,",
    "start": "3072320",
    "end": "3077880"
  },
  {
    "text": "but it's complicated. I'll come back to\nthe Llama 3 question.",
    "start": "3077880",
    "end": "3084510"
  },
  {
    "text": "I have one slide for\nthat in a little bit. But really this\nmodern ecosystem is",
    "start": "3084510",
    "end": "3091180"
  },
  {
    "text": "how investment in releasing\nopen models that people can use is continuing to grow into 2024.",
    "start": "3091180",
    "end": "3099349"
  },
  {
    "text": "I think there's always been\nthis tenuous period of like, there's only a few people\nreleasing these aligned models.",
    "start": "3099350",
    "end": "3104600"
  },
  {
    "text": "There's these important\npeople in the ecosystem that are just doing this because\nthey want to, and it's for fun.",
    "start": "3104600",
    "end": "3110167"
  },
  {
    "text": "They might have a day job. And it's like, how\nlong can this go on? Like, what are the\nlimitations on this? But in 2024, we've really\nseen more companies",
    "start": "3110167",
    "end": "3119080"
  },
  {
    "text": "come into the space. And someone drew Meta\nLlama 3 on the screen. I was like, I was talking\nto [? co-workers. ?]",
    "start": "3119080",
    "end": "3124660"
  },
  {
    "text": "And they're like, yeah, you're\ngoing to need to keep adding models. You're never going to be\nable to give this lecture. Yeah, it's a losing battle. I know.",
    "start": "3124660",
    "end": "3130630"
  },
  {
    "text": "But the modern-- there's just\nway more types of models. So I get away with\nnot having Llama 3",
    "start": "3130630",
    "end": "3137392"
  },
  {
    "text": "on this specific slide because\nI'm talking about diversity of players and models, not\njust the fact that there",
    "start": "3137393",
    "end": "3142570"
  },
  {
    "text": "are more great models. So there's interesting models,\nlike this one, Genstruct from NousResearch in\nthe last few months,",
    "start": "3142570",
    "end": "3148660"
  },
  {
    "text": "where it's a specifically fine\ntuned model for rephrasing any text into instructions.",
    "start": "3148660",
    "end": "3154980"
  },
  {
    "text": "So if you have a\nbook and you want to model to be able to\nanswer questions about this, why don't we just throw it\nat this rephrasing question--",
    "start": "3154980",
    "end": "3161180"
  },
  {
    "text": "this rephrasing model? And the teams that\nI work on, AI2, were trying to release\ninstruction models where",
    "start": "3161180",
    "end": "3168260"
  },
  {
    "text": "every single thing that we've\ndone to train it is documented and reproducible, from data\nto what compute it was.",
    "start": "3168260",
    "end": "3175200"
  },
  {
    "text": "There's just, these models are\ngetting new features in these little ways other than just\nbeing the, quote unquote,",
    "start": "3175200",
    "end": "3180380"
  },
  {
    "text": "\"best open model,\" such as these\ncorporate entities that are",
    "start": "3180380",
    "end": "3186289"
  },
  {
    "text": "going for really\nstanding out in the open. So there's Databricks\nDBRX model,",
    "start": "3186290",
    "end": "3192620"
  },
  {
    "text": "Cohere's command R+ model. I think people are mostly\nblindsided by Cohere releasing model weights, but it\nwas the first open model",
    "start": "3192620",
    "end": "3199310"
  },
  {
    "text": "to pass GPT-4 on Chatbot Arena. And that has been\na long time coming. I think beating GPT-4 on a\nhuman evaluation is not easy.",
    "start": "3199310",
    "end": "3209370"
  },
  {
    "text": "And yes, the open is still a\nyear behind, but that's fine. As long as we have a\nfunctioning ecosystem,",
    "start": "3209370",
    "end": "3216170"
  },
  {
    "text": "it will continue to grow. Then there's other things, like,\ninteresting research models.",
    "start": "3216170",
    "end": "3221180"
  },
  {
    "text": "Like, Rho came out, which\ndoes data weighting. We're finally starting to get\nmultilingual models with Aya,",
    "start": "3221180",
    "end": "3227660"
  },
  {
    "text": "which is also from Cohere. People are getting more\nmixture of expert models to train on, which\nis just a bit more",
    "start": "3227660",
    "end": "3235190"
  },
  {
    "text": "of an efficient\npretraining equation. State-space models\nare really taking off. They had this moment\nin December with Mamba",
    "start": "3235190",
    "end": "3242630"
  },
  {
    "text": "and now it's continuing in 2024. So there's just a lot going on. And this makes me feel\ngood because it's like, OK,",
    "start": "3242630",
    "end": "3250224"
  },
  {
    "text": "I just have to keep doing what\nI'm doing and encouraging people to participate. And we're going to keep\nbeing able to do this fun",
    "start": "3250225",
    "end": "3256720"
  },
  {
    "text": "thing of figuring out how to\nmake models and share them with people. This is my slide for Llama 3.",
    "start": "3256720",
    "end": "3263900"
  },
  {
    "text": "The reason why I didn't make a\nlot of slides about this all day is that Llama 3's\nrelease is more",
    "start": "3263900",
    "end": "3269960"
  },
  {
    "text": "about scaling of the\necosystem as a whole than it is about alignment.",
    "start": "3269960",
    "end": "3275160"
  },
  {
    "text": "The Llama 2 paper was extremely\ndetailed about alignment. And we're going to get\na Llama 3 paper soon,",
    "start": "3275160",
    "end": "3281840"
  },
  {
    "text": "if you can believe\nmultiple sources at Meta, which I choose to. And when the Llama\n3 paper comes out",
    "start": "3281840",
    "end": "3287690"
  },
  {
    "text": "is when we will learn all\nthe interesting alignment things that they have done. That being said, they're\nvery unlikely to release",
    "start": "3287690",
    "end": "3295940"
  },
  {
    "text": "the human preference\ndata that they did. I'm yet to succeed\nin getting them to release a reward\nmodel for Llama 2",
    "start": "3295940",
    "end": "3301820"
  },
  {
    "text": "or Llama 3 from alignment. So we have more work to do\non getting Meta to support",
    "start": "3301820",
    "end": "3308089"
  },
  {
    "text": "this kind of open alignment\necosystem to the same extent that they are supporting\nthe pretrainging ecosystem.",
    "start": "3308090",
    "end": "3313950"
  },
  {
    "text": "And this kind of scaling story\nthat I'm saying very much connects to the previous slide,\nwhere scaling and solving this",
    "start": "3313950",
    "end": "3324270"
  },
  {
    "text": "is very much determined by the\nmarkets and capital incentives. But so long as\nscaling is continuing",
    "start": "3324270",
    "end": "3331890"
  },
  {
    "text": "to happen in the open\necosystem, it just means that more players\nare going to stick around. And in some ways, it\nfeeds back into itself,",
    "start": "3331890",
    "end": "3339270"
  },
  {
    "text": "where if this Llama 3\nis rumored to have-- or they're training\na $400 billion parameter model, which we're not\n100% sure that the weights will",
    "start": "3339270",
    "end": "3347010"
  },
  {
    "text": "be released. But it seems like that's\nMark Zuckerberg's intent. And having that, which\nis about GPT-4 quality,",
    "start": "3347010",
    "end": "3354480"
  },
  {
    "text": "really changes what you can do\nto get language models running in your products.",
    "start": "3354480",
    "end": "3360190"
  },
  {
    "text": "So Llama 3 and how many people\nare playing in the open space right now goes to\nshow that we have",
    "start": "3360190",
    "end": "3365730"
  },
  {
    "text": "more of the same coming, which\nis interesting models coming on a weekly basis. And most people are just kind\nof accommodated to it now.",
    "start": "3365730",
    "end": "3373270"
  },
  {
    "text": "Like, people don't freak\nout when there's a new-- well, they like Mistral's model\nbecause there's a magnet link, and it's funny.",
    "start": "3373270",
    "end": "3378715"
  },
  {
    "text": "But we're used to\nit, and I still expect that to be the case\nfor the next year or two,",
    "start": "3378715",
    "end": "3384180"
  },
  {
    "text": "with this pace just\nbeing how it is. And it's really fun to follow.",
    "start": "3384180",
    "end": "3389980"
  },
  {
    "text": "And I just think\nthat it's not a time to be worried about\nscooping, being scooped, but to just keep figuring\nout where you can contribute,",
    "start": "3389980",
    "end": "3397770"
  },
  {
    "text": "whether it's on\nevaluation or some of these other alignment methods\nthat people have talked about.",
    "start": "3397770",
    "end": "3404310"
  },
  {
    "text": "So I have a quick thing on\ncurrent directions, which is-- we'll come back to\nsome of these data things",
    "start": "3404310",
    "end": "3409380"
  },
  {
    "text": "that I mentioned multiple\ntimes, and then we can get to questions. ",
    "start": "3409380",
    "end": "3415700"
  },
  {
    "text": "And the thing that\npeople want to know a lot is, are open models going to\ncatch up to closed models?",
    "start": "3415700",
    "end": "3421200"
  },
  {
    "text": "My answer is, probably\nnot ever completely. There will be some friction\nin the system by a time delay",
    "start": "3421200",
    "end": "3428150"
  },
  {
    "text": "by which open models are closed. And open model weights\nare not inherently unsafe.",
    "start": "3428150",
    "end": "3434190"
  },
  {
    "text": "The open versus closed\ndebate has mostly converged around this. But given the\nterritory that we're",
    "start": "3434190",
    "end": "3439730"
  },
  {
    "text": "going with in AI,\nwhere we're uncovering new capabilities\nwe've never seen, I think it's OK that\nif there's a few months",
    "start": "3439730",
    "end": "3446270"
  },
  {
    "text": "wait before you have\nopen weights so you can run on your laptop, as we're\ndiscovering what AI can do.",
    "start": "3446270",
    "end": "3452030"
  },
  {
    "text": "If you look at someone if you\nlook at Maxime's plot with trend lines showing them, it\nshows that open models",
    "start": "3452030",
    "end": "3458300"
  },
  {
    "text": "are getting closer. But we're not really sure if\nopen models will stay closer on Chatbot Arena\nin the long term.",
    "start": "3458300",
    "end": "3464980"
  },
  {
    "text": "There will always be an\nopen and closed category because there is demand to\nhave models that are tuned",
    "start": "3464980",
    "end": "3470360"
  },
  {
    "text": "to what you want them to do. So this leans into my\ncurrent directions. Data is the biggest\nlimitation to alignment,",
    "start": "3470360",
    "end": "3477780"
  },
  {
    "text": "which is we have\ntwo or three data sets that are driving all the\nresearch and open alignment. Anthropic HH data set\nfor my friend Deep, got",
    "start": "3477780",
    "end": "3486030"
  },
  {
    "text": "that uploaded back\nin 2022, I think, UltraFeedback from OpenBMB.",
    "start": "3486030",
    "end": "3492390"
  },
  {
    "text": "And Nectar from\nberkeley-nest/Nexusflow with the Starling models are what\nmost people are focusing on.",
    "start": "3492390",
    "end": "3498609"
  },
  {
    "text": "We need more, particularly\nif humans wrote it, to add more diversity to our\nmodels and more robustness.",
    "start": "3498610",
    "end": "3505905"
  },
  {
    "text": "DPO is continuing in\nan academic sense. There is a comedy of\npapers extending DPO.",
    "start": "3505905",
    "end": "3512100"
  },
  {
    "text": "So this is odds ratio\npreference optimization, which doesn't need a reference model,\nconstrained DPO, identity",
    "start": "3512100",
    "end": "3520289"
  },
  {
    "text": "preference optimization. I don't remember what BCO is,\nand then I can't pronounce",
    "start": "3520290",
    "end": "3526320"
  },
  {
    "text": "the KTO authors but like\n[? Kavertsky-something ?] optimization from Contextual\nand Stanford DNO, sDPO,",
    "start": "3526320",
    "end": "3534540"
  },
  {
    "text": "which is like sequential\nDPO, and Self-Reward. There are so many,\nand that's good. And that trend will continue.",
    "start": "3534540",
    "end": "3541119"
  },
  {
    "text": "And at the same time, we're\nseeing more model sizes. Most alignment happened\nat the 7 or 13B scale.",
    "start": "3541120",
    "end": "3547550"
  },
  {
    "text": "I think there's a large drive\nto make smaller models aligned. Google is releasing 1\nbillion parameter models,",
    "start": "3547550",
    "end": "3554610"
  },
  {
    "text": "but it's also an\nopportunity where there aren't that many\npeople playing in the space. But it's something that\na lot of people want.",
    "start": "3554610",
    "end": "3560400"
  },
  {
    "text": "Just because they run\nthese models locally, making them smaller\nmakes it way easier. And then running back to two\nthemes throughout this lecture",
    "start": "3560400",
    "end": "3567859"
  },
  {
    "text": "is, what are\nspecific evaluations that we should be building? And how do we\npersonalize these models?",
    "start": "3567860",
    "end": "3574710"
  },
  {
    "text": "They go hand in hand. These are the things\nthat I'm thinking about. I welcome feedback from them.",
    "start": "3574710",
    "end": "3581450"
  },
  {
    "text": "I identified some\npeople that I'm following to see where\nnew models come out.",
    "start": "3581450",
    "end": "3586980"
  },
  {
    "text": "So I try to release models\nAI2, Hugging Face quickly turns around new aligned\nmodels under the Zephyr brand,",
    "start": "3586980",
    "end": "3596132"
  },
  {
    "text": "these kind of Berkeley-Nest\nand Nexusflow slow people building data sets\nand Starling models. NousResearch is a kind of--\nthey started as just a guy.",
    "start": "3596132",
    "end": "3604200"
  },
  {
    "text": "Teknium was fine-tuning\nmodels and now is a company for fine-tuning models. OpenBMB in China has been doing\na lot of preference data sets.",
    "start": "3604200",
    "end": "3612630"
  },
  {
    "text": "They recently released some\ndata sets called UltraInteract, which is some math\npreference data for doing",
    "start": "3612630",
    "end": "3618640"
  },
  {
    "text": "RLHF and fine-tuning. Argilla is a startup around\nbuilding tools to annotate data,",
    "start": "3618640",
    "end": "3625200"
  },
  {
    "text": "is focused on preference data. And there's even\njust individuals that are driving this\nnarrative, so Maxime and Jon.",
    "start": "3625200",
    "end": "3631912"
  },
  {
    "text": "There's just a lot of people. Model merging is something\nI didn't talk about, but it's like TPO but\ntaking it even farther,",
    "start": "3631912",
    "end": "3638640"
  },
  {
    "text": "where it's-- model\nmerging is so accessible. You don't need a\nGPU to merge models.",
    "start": "3638640",
    "end": "3644200"
  },
  {
    "text": "It's a for loop. So people are going\nto try it and there's going to be iteration on it. So in this alignment\nspace, never",
    "start": "3644200",
    "end": "3650730"
  },
  {
    "text": "bet against people where\nthey can just try things and see what's better,\nand then eventually learn.",
    "start": "3650730",
    "end": "3657069"
  },
  {
    "text": "That's what model\nmerging is, and it's going to be here to stay. So thanks for listening.",
    "start": "3657070",
    "end": "3663869"
  },
  {
    "text": "I'm happy to take questions. And thanks to my many teammates\nat Hugging Face and AI2 that made it look like I\ndid so many of these things.",
    "start": "3663870",
    "end": "3671830"
  },
  {
    "text": "But there's a lot of great\ncontributors that underlie this. So I'll slow down and drink some\nwater and answer some questions,",
    "start": "3671830",
    "end": "3680109"
  },
  {
    "text": "but thanks for coming again. Yeah, so the top\nquestion on scores--",
    "start": "3680110",
    "end": "3685782"
  },
  {
    "text": "please rate them\nbecause it's easy for me to see them-- was about odds\nratio preference alignment.",
    "start": "3685782",
    "end": "3690880"
  },
  {
    "text": "I think being agnostic to\nthe method is the best thing, but you probably need to\nbe good at engineering",
    "start": "3690880",
    "end": "3695920"
  },
  {
    "text": "to get really good at one method\nto get a specific model out.",
    "start": "3695920",
    "end": "3702970"
  },
  {
    "text": "And getting these deliverables\nimportant to getting recognition. I don't know if people can\ntalk via microphone, which is",
    "start": "3702970",
    "end": "3710310"
  },
  {
    "text": "a much more natural experience. But I'm just going to\nkeep talking to myself. [CHUCKLES]",
    "start": "3710310",
    "end": "3717877"
  },
  {
    "text": "There's a question around\nthe future of alignment, given simple methods can\ncircumvent fine tuning.",
    "start": "3717877",
    "end": "3723260"
  },
  {
    "text": "I think that the\nfuture of alignment is like, safety is not the\nonly thing that matters. There's a lot of promise\nshowing that alignment helps",
    "start": "3723260",
    "end": "3731240"
  },
  {
    "text": "with how much people\nlike the model, so how much RLHF improves\nthe user experience and how much it improves\ncode and math abilities.",
    "start": "3731240",
    "end": "3738510"
  },
  {
    "text": "So like while\neveryone hates Q-Star, Q-Star has some things\nto guide towards,",
    "start": "3738510",
    "end": "3744410"
  },
  {
    "text": "which are using synthetic\ndata in RL search and stuff to improve the raw\ncapabilities, rather than",
    "start": "3744410",
    "end": "3750397"
  },
  {
    "text": "just talking about safety. ",
    "start": "3750397",
    "end": "3756150"
  },
  {
    "text": "OK, onwards. ",
    "start": "3756150",
    "end": "3764070"
  },
  {
    "text": "Yeah, people are asking about\nthe fact that Llama 3 uses-- Llama 3 said that they use\ninstruction fine-tuning,",
    "start": "3764070",
    "end": "3771300"
  },
  {
    "text": "rejection sampling, DPO, and PPO\nfor their aligned models, which I was like, I don't\nknow how they're",
    "start": "3771300",
    "end": "3777660"
  },
  {
    "text": "using all of these things. But I think they're shifting\nthe abilities incrementally to provide nice initialization\nfor the next method",
    "start": "3777660",
    "end": "3785460"
  },
  {
    "text": "and to keep being able\nto use new human data and make the metrics go up. I think over time, that\nwill become simpler.",
    "start": "3785460",
    "end": "3792390"
  },
  {
    "text": "In the future, Meta will not\nhave this convoluted five stage multimethod process. And we'll figure out a way to\ndistill that to one algorithm.",
    "start": "3792390",
    "end": "3802290"
  },
  {
    "text": "Pitfalls of synthetic data\nis repetitiveness and not robust distributions.",
    "start": "3802290",
    "end": "3807490"
  },
  {
    "text": "So most of the synthetic data\nsets out there are about--",
    "start": "3807490",
    "end": "3812590"
  },
  {
    "text": "they have very similar things\nin there, and that is-- like, the models are going\nto generalize less well",
    "start": "3812590",
    "end": "3820320"
  },
  {
    "text": "and probably get less boosts\nfrom alignment training if there's not this kind\nof general improvement",
    "start": "3820320",
    "end": "3826810"
  },
  {
    "text": "to capabilities. So we want to take some\nin-person questions. oh, yeah, that's much better.",
    "start": "3826810",
    "end": "3833280"
  },
  {
    "text": "[CHUCKLING] Does anyone have some in-person\nquestions to ask Nathan? ",
    "start": "3833280",
    "end": "3840570"
  },
  {
    "text": "OK.  Hi.",
    "start": "3840570",
    "end": "3846200"
  },
  {
    "text": "Thank you so much for the talk. What do you think are the\ngreatest hotspots of research",
    "start": "3846200",
    "end": "3851569"
  },
  {
    "text": "or work in terms of\npersonalized language models, and where do you see them\nhaving the most impact?",
    "start": "3851570",
    "end": "3857665"
  },
  {
    "text": " This is one of the things that\nI'm excited about, the local LLM",
    "start": "3857665",
    "end": "3863849"
  },
  {
    "text": "community. I'm not particularly\nideologically aligned with the effective\naccelerationist stuff,",
    "start": "3863850",
    "end": "3871230"
  },
  {
    "text": "but I do think that\nthey have a lot of drive to create a language model\nthat they like to use.",
    "start": "3871230",
    "end": "3876339"
  },
  {
    "text": "So therefore, there's going to\nbe things we learn from them. And it's kind of a\nclassic like, how",
    "start": "3876340",
    "end": "3882960"
  },
  {
    "text": "to integrate\nmultiple communities. So it's like academics\naren't used to looking there, but I'm sure there's\na lot to learn there.",
    "start": "3882960",
    "end": "3889050"
  },
  {
    "text": "Yeah. I guess there are\nmultiple questions about advice for the field,\nwhether it's grad school or--",
    "start": "3889050",
    "end": "3896340"
  },
  {
    "text": "I'll give my advice with\nthe caged advice that is that you should be very wary\nof listening to people's advice",
    "start": "3896340",
    "end": "3902520"
  },
  {
    "text": "because it based\non their situation. But I think that the\nmost important thing you can do in the\nfield that's crazy",
    "start": "3902520",
    "end": "3907830"
  },
  {
    "text": "is just keep trying\nto develop skills and keep trying\nto build something that you think matters.",
    "start": "3907830",
    "end": "3912930"
  },
  {
    "text": "Because it's like, at\nthe end of the day, that's where you're\nmaking progress on. And you'll never be able to\nkeep track of everything,",
    "start": "3912930",
    "end": "3918849"
  },
  {
    "text": "and that's OK. And I can't keep\ntrack of everything, and I'm still trying to train\nmodels and build data sets.",
    "start": "3918850",
    "end": "3924530"
  },
  {
    "text": "So it's just like,\ngrad school is about learning to do research. And that still has value.",
    "start": "3924530",
    "end": "3930050"
  },
  {
    "text": "But industry is also fun if\nyou want to do a startup. So there's not\nlike-- you just have to think about what\nyou want to do.",
    "start": "3930050",
    "end": "3936680"
  },
  {
    "text": "I think someone sent me-- you can hear me, right? Yeah. --sent me a question\nthrough Zoom.",
    "start": "3936680",
    "end": "3944170"
  },
  {
    "text": "A quick question, you indicated\nthat making lower methods work with reinforcement\nlearning is tricky. Do you think lower methods work\nwell with DPO or its variance?",
    "start": "3944170",
    "end": "3954110"
  },
  {
    "text": "I haven't seen it be\nparticularly successful. So that's my general\nrule of thumb,",
    "start": "3954110",
    "end": "3959210"
  },
  {
    "text": "is I really wait to\ngo deep into a method until there's been a\nmodel release that's",
    "start": "3959210",
    "end": "3964460"
  },
  {
    "text": "in the relevant ballpark\nwith that type of-- with that method.",
    "start": "3964460",
    "end": "3970619"
  },
  {
    "text": "So the fact that it's\nbeen around for so long and hasn't happened\ncould be a blind spot. But I think that there's\nsome weirdness that's",
    "start": "3970620",
    "end": "3977180"
  },
  {
    "text": "preventing it from happening. Great. OK, another one.",
    "start": "3977180",
    "end": "3984422"
  },
  {
    "text": "Thank you for the talk. You mentioned GPT-4 being\nused as an evaluation metric, but it causes data\ncontamination.",
    "start": "3984422",
    "end": "3990890"
  },
  {
    "text": "What are some ways\nto mitigate this?  Oh, man.",
    "start": "3990890",
    "end": "3996680"
  },
  {
    "text": "Yeah. I mean, this is why it's, like,\nnice to have human evaluation. But I don't know if\nI have an answer.",
    "start": "3996680",
    "end": "4002530"
  },
  {
    "text": "At this point, I'm kind of\nfried from reading Llama 3 stuff and giving this lecture. But that's the\nfundamental problem,",
    "start": "4002530",
    "end": "4009280"
  },
  {
    "text": "is how to disambiguate various\nevaluation, various biases and evaluation and still\nget the signal out of them.",
    "start": "4009280",
    "end": "4017530"
  },
  {
    "text": "Right. OK, one more. Give me a second.",
    "start": "4017530",
    "end": "4022580"
  },
  {
    "text": "Thanks. For stuff like Llama 3\ntraining on so many tokens, like 15 trillion,\nwould that actually",
    "start": "4022580",
    "end": "4029150"
  },
  {
    "text": "make it harder to\nalign this model without losing some capabilities\nlearned from this overtraining?",
    "start": "4029150",
    "end": "4038359"
  },
  {
    "text": "It's not technically\novertrained, but every model will\nhave a different point",
    "start": "4038360",
    "end": "4044540"
  },
  {
    "text": "by which they're released. So it's like, that's why you\nneed a different learning rate and batch size and\ndata sets for models.",
    "start": "4044540",
    "end": "4050430"
  },
  {
    "text": "So you will need a different\nway of continuing it. But that is a common\nconfusion on how--",
    "start": "4050430",
    "end": "4060720"
  },
  {
    "text": "like, I mean, I don't even\nhave an intuition for it, just to know that I have\nbought this thing in the past",
    "start": "4060720",
    "end": "4066260"
  },
  {
    "text": "and been proven wrong about it. But it's like, it's not that\nit's overtrained or harder to fine-tune.",
    "start": "4066260",
    "end": "4071480"
  },
  {
    "text": "It's just that there's more\ninformation into the model. And as you continue to do this,\nthe model can keep learning.",
    "start": "4071480",
    "end": "4077490"
  },
  {
    "text": "It just takes more and more data\nto get marginal improvements. So Meta is willing to invest\nmore money into the model",
    "start": "4077490",
    "end": "4083330"
  },
  {
    "text": "to make it just a bit better,\nbut that should only help. It shouldn't hurt.",
    "start": "4083330",
    "end": "4089300"
  },
  {
    "text": "All right, great. Here's another one. Do you think synthetic data\ngeneration, like Cosmopedia,",
    "start": "4089300",
    "end": "4095200"
  },
  {
    "text": "is the way to go for making\ncontrolled or trusted domain-specific models?",
    "start": "4095200",
    "end": "4102060"
  },
  {
    "text": "I think it would be very good. I also think it's a good\nway to get around the fact that Google is paying\nrent at $60 million a year",
    "start": "4102060",
    "end": "4110969"
  },
  {
    "text": "to use their data so that we can\nno longer train on the newest Reddit data. I think that Cosmopedia\nand synthetic data",
    "start": "4110970",
    "end": "4118619"
  },
  {
    "text": "sets at a large scale\ncan be a way around this. And there are\nrumors that industry is doing something similar.",
    "start": "4118620",
    "end": "4123705"
  },
  {
    "start": "4123705",
    "end": "4131390"
  },
  {
    "text": "Give me a second. I think there's\none that I missed. ",
    "start": "4131390",
    "end": "4140859"
  },
  {
    "text": "Could you please\nshare some insights on why you are finding\nPPO better than DPO?",
    "start": "4140859",
    "end": "4147060"
  },
  {
    "text": "It's mostly like it ends up\nextracting more from the data.",
    "start": "4147060",
    "end": "4152528"
  },
  {
    "text": "So it's like the\nbenchmarks end up being a little bit better if\nwe get it set up correctly with the same starting point.",
    "start": "4152529",
    "end": "4159699"
  },
  {
    "text": "So it's like, you choose\na set of evaluations that you care about. And you look at them. And through\nfine-tuning the-- it's",
    "start": "4159700",
    "end": "4166890"
  },
  {
    "text": "primarily a group of great\ngrad students doing this. It's just running a ton\nof models and trainings,",
    "start": "4166890",
    "end": "4172170"
  },
  {
    "text": "and they're seeing\nthat PPO reliably can be doing a little bit better. And it's like, this is the\nfine margins that a lot of AI",
    "start": "4172170",
    "end": "4179250"
  },
  {
    "text": "works on nowadays.  Great.",
    "start": "4179250",
    "end": "4186330"
  },
  {
    "text": "Do you foresee a better\nevaluation method to be determined by a stronger\nor more specialized model, which",
    "start": "4186330",
    "end": "4194400"
  },
  {
    "text": "means rule-based metrics\nare dead forever? ",
    "start": "4194400",
    "end": "4200860"
  },
  {
    "text": "[INAUDIBLE],, I try not\nto say no to things. This is becoming\nphilosophical, which is like, I'm trying not to say\nno to things in the language",
    "start": "4200860",
    "end": "4207250"
  },
  {
    "text": "model space with how fast\nthings are progressing. It's like, I should try not to\nbet against progress continuing.",
    "start": "4207250",
    "end": "4215060"
  },
  {
    "text": "This goes for pretraining\nand alignment. And it's like, at multiple\nstages in the last few months",
    "start": "4215060",
    "end": "4220900"
  },
  {
    "text": "come to benefit me. So it's like, if you just assume\nthat things will get better and they will work,\nit's like, just",
    "start": "4220900",
    "end": "4227200"
  },
  {
    "text": "makes it a little bit easier to\nwrap your head around things. ",
    "start": "4227200",
    "end": "4233670"
  },
  {
    "text": "One last one here from-- give me a sec. ",
    "start": "4233670",
    "end": "4240750"
  },
  {
    "text": "At its core, an LLM is\ntrying to approximate a complex distribution. Would you say that alignment\nis the process of squashing",
    "start": "4240750",
    "end": "4248580"
  },
  {
    "text": "specific parts of\nthis distribution according to what humans prefer? ",
    "start": "4248580",
    "end": "4258099"
  },
  {
    "text": "Yeah, I think that's\nphrased generally enough that I could get behind it. It is. It's like, alignment is about\nchanging the distribution,",
    "start": "4258100",
    "end": "4265930"
  },
  {
    "text": "and it can be multiple tokens. It's like a\nmultiterm prediction. RL is not just autoregressive.",
    "start": "4265930",
    "end": "4272710"
  },
  {
    "text": "It can be these multistring\ndifferent things that are getting shifted around, and\nit's a really different loss",
    "start": "4272710",
    "end": "4278845"
  },
  {
    "text": "function.  Here's one from--\nhow do you envision",
    "start": "4278845",
    "end": "4286090"
  },
  {
    "text": "the usage of watermark for\nboth open and closed language [AUDIO OUT]",
    "start": "4286090",
    "end": "4292120"
  },
  {
    "text": "I think it, a lot of times,\nfeels like a losing battle. I think that a practical\nsolution in the future",
    "start": "4292120",
    "end": "4298420"
  },
  {
    "text": "is that a lot of-- if you want\nto prove something that is human made, you can prove that\nit was generated by a human",
    "start": "4298420",
    "end": "4304720"
  },
  {
    "text": "by having a certain\ntool, rather than trying to understand if a specific\ncontent was made by an AI.",
    "start": "4304720",
    "end": "4312040"
  },
  {
    "text": "So the assumption will be that\nall content was made by an AI unless proven to be human.",
    "start": "4312040",
    "end": "4317470"
  },
  {
    "text": "It's not what I would consider\na sociologically good answer. It just seems like\na practical one.",
    "start": "4317470",
    "end": "4325850"
  },
  {
    "text": "Makes sense. I think we have a\nfew more minutes. So if anybody has any\nlast minute questions,",
    "start": "4325850",
    "end": "4331230"
  },
  {
    "text": "feel free to send them over\nto me on the Zoom chat. Yeah, this is-- that was much\nbetter than me half-reading",
    "start": "4331230",
    "end": "4338699"
  },
  {
    "text": "the question. [CHUCKLING]  All right, here's one.",
    "start": "4338700",
    "end": "4344140"
  },
  {
    "text": "What are your thoughts on\ndifferent optimization functions to train large language\nmodels rather than using MLE?",
    "start": "4344140",
    "end": "4351280"
  },
  {
    "text": "What could be good\nresearch directions there?",
    "start": "4351280",
    "end": "4356940"
  },
  {
    "text": "I think this is the whole\nidea of what RLHF represents. And that's why,\nif you ask people",
    "start": "4356940",
    "end": "4362300"
  },
  {
    "text": "who have been in NLP longer, one\nof the most compelling arguments for me is like you now have\nextreme flexibility on the loss",
    "start": "4362300",
    "end": "4370140"
  },
  {
    "text": "function, while we were\nkind of limited on what our regressive losses could do. So there's arguments that\nit's like, why is there",
    "start": "4370140",
    "end": "4376440"
  },
  {
    "text": "any limit if we could just keep\ndoing more and more tokens of RL training? It's a really general\nframing, but RL's loss",
    "start": "4376440",
    "end": "4384120"
  },
  {
    "text": "function-- you make it so that\nthe training of a language model can incorporate many\ndifferent things.",
    "start": "4384120",
    "end": "4389500"
  },
  {
    "text": "And that's very exciting. That could be like the\n10 year goal of RLHF. ",
    "start": "4389500",
    "end": "4395780"
  },
  {
    "text": "To what extent is training\non adversarial data effective for defending\nagainst crescendo",
    "start": "4395780",
    "end": "4402290"
  },
  {
    "text": "and other simple\nmulti-turn attacks. I haven't spent as much time\non safety as I would want to,",
    "start": "4402290",
    "end": "4409240"
  },
  {
    "text": "but I think it's like-- it'll be this everlasting dance\nwhere, if you have example data,",
    "start": "4409240",
    "end": "4414460"
  },
  {
    "text": "you can defend against it. But it will not be impossible\nto generate new data. So mostly, it comes\ndown to the use case",
    "start": "4414460",
    "end": "4420190"
  },
  {
    "text": "that you're looking\nat protecting. So if you want to protect\nsomething really important, you need to have\nlayers on that are not",
    "start": "4420190",
    "end": "4426310"
  },
  {
    "text": "just sensitive to a new\nprompting technique, but limit what the model can do. It's like a use-focused theme.",
    "start": "4426310",
    "end": "4433490"
  },
  {
    "text": "Well, the kind of whole-- security is a very\ncomplicated thing, otherwise. ",
    "start": "4433490",
    "end": "4441300"
  },
  {
    "text": "Here's one on quantization. ",
    "start": "4441300",
    "end": "4446350"
  },
  {
    "text": "Do you see potential\nin quantization methods such as BitNet, like 1.58 bit?",
    "start": "4446350",
    "end": "4452630"
  },
  {
    "text": "If so, do you think BitNet\nwill become popular? I have no idea.",
    "start": "4452630",
    "end": "4459140"
  },
  {
    "text": "I wouldn't-- this\nis what I mean. It's like, OK, sounds cool. Wouldn't rule it out. ",
    "start": "4459140",
    "end": "4466950"
  },
  {
    "text": "You think there\nis a need or a way to control large-scale data\nextraction from large language models like Cosmopedia?",
    "start": "4466950",
    "end": "4473630"
  },
  {
    "text": " I do think there's\na lot of wills",
    "start": "4473630",
    "end": "4479940"
  },
  {
    "text": "in a lot of ways\nto explore making the synthetic data better. I think it's very early. I have a project\nthat's going on it,",
    "start": "4479940",
    "end": "4485730"
  },
  {
    "text": "and it is one of the few\nways that can generate more tokens, which is like--",
    "start": "4485730",
    "end": "4491780"
  },
  {
    "text": "people are actually\nrunning out of tokens, especially if you try not to\ntrain on things that you're not supposed to train on.",
    "start": "4491780",
    "end": "4496900"
  },
  {
    "text": "It's like, then you can\njust generate more data. And as we've seen with Llama,\nif you have the compute,",
    "start": "4496900",
    "end": "4501930"
  },
  {
    "text": "more data will help you. ",
    "start": "4501930",
    "end": "4508280"
  },
  {
    "text": "Let's see, self-play-like\nmake things. Any chance you can expand\nupon or share your opinions",
    "start": "4508280",
    "end": "4515720"
  },
  {
    "text": "on self-play-like\nthings, like OpenAI Superalignment [? work. ?]",
    "start": "4515720",
    "end": "4522349"
  },
  {
    "text": "I think people will keep\nusing language models in the loop of training\nother language models, but it's a broad field\nthat doesn't have",
    "start": "4522350",
    "end": "4529820"
  },
  {
    "text": "full agreement on how to do it. ",
    "start": "4529820",
    "end": "4537100"
  },
  {
    "text": "OK, great. And I think we're\npretty much out of time. So if folks want to get in\ntouch or have more questions,",
    "start": "4537100",
    "end": "4543520"
  },
  {
    "text": "can they email you or-- Yeah. OK, great. But yeah, thanks so much\nagain for taking the time",
    "start": "4543520",
    "end": "4550480"
  },
  {
    "text": "and giving us such a great talk. So yeah, give it up for Nathan.",
    "start": "4550480",
    "end": "4555652"
  },
  {
    "text": "Thanks, everyone. And I think the slides, as well\nas the Hugging Face collection, are all posted on our\nwebsite, as well as Discord",
    "start": "4555652",
    "end": "4563090"
  },
  {
    "text": "so in case anybody\nwants to follow along. ",
    "start": "4563090",
    "end": "4569380"
  },
  {
    "text": "Sounds good. Thanks a lot for having me. Yeah, no worries. See around [? Stan. ?] Bye-bye.",
    "start": "4569380",
    "end": "4575790"
  },
  {
    "start": "4575790",
    "end": "4581000"
  }
]