[
  {
    "start": "0",
    "end": "17000"
  },
  {
    "start": "0",
    "end": "4168"
  },
  {
    "text": "CHRISTOPHER POTTS:\nWelcome, everyone.",
    "start": "4168",
    "end": "5710"
  },
  {
    "text": "This is part three in our\nseries on contextual word",
    "start": "5710",
    "end": "7860"
  },
  {
    "text": "representations.",
    "start": "7860",
    "end": "8582"
  },
  {
    "text": "We're going to be talking\nabout the BERT model.",
    "start": "8582",
    "end": "10540"
  },
  {
    "text": "Which is an innovative\nand powerful application",
    "start": "10540",
    "end": "12498"
  },
  {
    "text": "of the transformer\narchitecture which we covered",
    "start": "12498",
    "end": "15060"
  },
  {
    "text": "in the previous screencast.",
    "start": "15060",
    "end": "16980"
  },
  {
    "text": "Let's dive into the core\nmodel structure of BERT.",
    "start": "16980",
    "end": "19289"
  },
  {
    "start": "17000",
    "end": "107000"
  },
  {
    "text": "We'll begin with the inputs.",
    "start": "19290",
    "end": "20970"
  },
  {
    "text": "As usual we'll work with a\nsimple example; \"the Rock",
    "start": "20970",
    "end": "23430"
  },
  {
    "text": "rules\" that has three tokens.",
    "start": "23430",
    "end": "24840"
  },
  {
    "text": "But you'll notice\nthat for BERT, we",
    "start": "24840",
    "end": "26790"
  },
  {
    "text": "begin every sequence with\nthe designated class token.",
    "start": "26790",
    "end": "29970"
  },
  {
    "text": "And sequences end with\na designated SEP token.",
    "start": "29970",
    "end": "32729"
  },
  {
    "text": "And the SEP token\ncan also be used",
    "start": "32729",
    "end": "34380"
  },
  {
    "text": "as a boundary marker between\nsub-parts of an input sequence.",
    "start": "34380",
    "end": "38810"
  },
  {
    "text": "We'll learn\npositional embeddings",
    "start": "38810",
    "end": "40520"
  },
  {
    "text": "for each one of those tokens.",
    "start": "40520",
    "end": "42410"
  },
  {
    "text": "And in addition, as you\ncan see in maroon here,",
    "start": "42410",
    "end": "44390"
  },
  {
    "text": "we're going to learn a\nsecond notion of position.",
    "start": "44390",
    "end": "47180"
  },
  {
    "text": "It sent A for all\nof these tokens,",
    "start": "47180",
    "end": "49160"
  },
  {
    "text": "so it won't contribute\nin this particular case.",
    "start": "49160",
    "end": "51390"
  },
  {
    "text": "But if we had a problem like\nnatural language inference",
    "start": "51390",
    "end": "54290"
  },
  {
    "text": "where examples\nconsist of a premise",
    "start": "54290",
    "end": "56420"
  },
  {
    "text": "sentence and a\nhypothesis sentence,",
    "start": "56420",
    "end": "58850"
  },
  {
    "text": "we could learn\nseparate embeddings",
    "start": "58850",
    "end": "60800"
  },
  {
    "text": "for the premise\nand the hypothesis.",
    "start": "60800",
    "end": "62390"
  },
  {
    "text": "And thereby hope that we can\ncapture that second notion",
    "start": "62390",
    "end": "65630"
  },
  {
    "text": "of position within our input.",
    "start": "65630",
    "end": "68939"
  },
  {
    "text": "We're going to have\nlearned embeddings",
    "start": "68940",
    "end": "70560"
  },
  {
    "text": "for each one of these\nnotions for the word",
    "start": "70560",
    "end": "72600"
  },
  {
    "text": "and the two notions of position.",
    "start": "72600",
    "end": "74520"
  },
  {
    "text": "And as in the transformer,\nthe actual embeddings",
    "start": "74520",
    "end": "76920"
  },
  {
    "text": "given here in light green\nwill be additive combinations",
    "start": "76920",
    "end": "79979"
  },
  {
    "text": "of those three learned\nembedding representations.",
    "start": "79980",
    "end": "85230"
  },
  {
    "text": "From there we just do a lot\nof work with the transformer.",
    "start": "85230",
    "end": "88230"
  },
  {
    "text": "Have we have had repeated\ntransformer blocks,",
    "start": "88230",
    "end": "90140"
  },
  {
    "text": "it could be 12, it could be\n24, it could be even more.",
    "start": "90140",
    "end": "93360"
  },
  {
    "text": "And the output of all of those\ntransformer blocks in the end",
    "start": "93360",
    "end": "96420"
  },
  {
    "text": "is a sequence of\noutput representations.",
    "start": "96420",
    "end": "99100"
  },
  {
    "text": "These are vectors, and I've\ngiven that in dark green.",
    "start": "99100",
    "end": "101820"
  },
  {
    "text": "That is the core model\nstructure for BERT.",
    "start": "101820",
    "end": "105210"
  },
  {
    "text": "And that brings us to how\nthis model is trained.",
    "start": "105210",
    "end": "107549"
  },
  {
    "start": "107000",
    "end": "199000"
  },
  {
    "text": "The masked language\nmodeling objective",
    "start": "107550",
    "end": "109440"
  },
  {
    "text": "is the fundamental training\nobjective for this model.",
    "start": "109440",
    "end": "113060"
  },
  {
    "text": "Fundamentally, what the\nmodel is trying to do",
    "start": "113060",
    "end": "115280"
  },
  {
    "text": "is work as an auto-encoder\nand reproduce the entire input",
    "start": "115280",
    "end": "118490"
  },
  {
    "text": "sequences.",
    "start": "118490",
    "end": "119869"
  },
  {
    "text": "To make that problem\nnon-trivial though,",
    "start": "119870",
    "end": "121790"
  },
  {
    "text": "we're going to employ this\nmasked language modeling idea.",
    "start": "121790",
    "end": "124285"
  },
  {
    "text": "And what that\nmeans is that we're",
    "start": "124285",
    "end": "125660"
  },
  {
    "text": "going to go through\nthese input sequences,",
    "start": "125660",
    "end": "127410"
  },
  {
    "text": "and randomly replace some\nsmall percentage, 10% to 15%",
    "start": "127410",
    "end": "131570"
  },
  {
    "text": "of the input tokens, with\nthis designated mask token.",
    "start": "131570",
    "end": "135080"
  },
  {
    "text": "And then the job of\nthe model is to learn,",
    "start": "135080",
    "end": "137210"
  },
  {
    "text": "for those masked inputs\nto reconstruct what",
    "start": "137210",
    "end": "139790"
  },
  {
    "text": "was the actual input, right?",
    "start": "139790",
    "end": "141489"
  },
  {
    "text": "So in this case, we masked out\nrules and the job of the model",
    "start": "141490",
    "end": "145040"
  },
  {
    "text": "is to use this bidirectional\ncontext, that's",
    "start": "145040",
    "end": "147319"
  },
  {
    "text": "flowing in from all those\nattention mechanisms",
    "start": "147320",
    "end": "150020"
  },
  {
    "text": "to figure out that\n\"rules\" was actually",
    "start": "150020",
    "end": "152000"
  },
  {
    "text": "the token that belonged\nto that initial position.",
    "start": "152000",
    "end": "155820"
  },
  {
    "text": "The BERT team did\na variant of this",
    "start": "155820",
    "end": "157710"
  },
  {
    "text": "as well, which is\nmasking by a random word.",
    "start": "157710",
    "end": "160390"
  },
  {
    "text": "So in this case, we\nmight replace \"rules\"",
    "start": "160390",
    "end": "162630"
  },
  {
    "text": "with a word like \"every\" picked\nrandomly from our vocabulary.",
    "start": "162630",
    "end": "166080"
  },
  {
    "text": "But there again, the\nfundamental job of the model",
    "start": "166080",
    "end": "168480"
  },
  {
    "text": "is to learn to figure out that\n\"rules\" was the actual token",
    "start": "168480",
    "end": "172140"
  },
  {
    "text": "in that position.",
    "start": "172140",
    "end": "173620"
  },
  {
    "text": "So it's going to make some\nprediction in this case.",
    "start": "173620",
    "end": "175980"
  },
  {
    "text": "If it's different from\n\"rules,\" then the error signal",
    "start": "175980",
    "end": "178260"
  },
  {
    "text": "will flow back down through all\nthe parameters of this model.",
    "start": "178260",
    "end": "180930"
  },
  {
    "text": "Affecting, we hope,\nall the representations",
    "start": "180930",
    "end": "183180"
  },
  {
    "text": "because of that dense thicket\nof attention connections",
    "start": "183180",
    "end": "186239"
  },
  {
    "text": "that exists across\nthese timesteps.",
    "start": "186240",
    "end": "188310"
  },
  {
    "text": "And in that way the model\nwill learn to update itself,",
    "start": "188310",
    "end": "190980"
  },
  {
    "text": "effectively learning how\nto reconstruct the missing",
    "start": "190980",
    "end": "194430"
  },
  {
    "text": "pieces from these inputs that\nwe created during training.",
    "start": "194430",
    "end": "199189"
  },
  {
    "start": "199000",
    "end": "295000"
  },
  {
    "text": "So let's dive into that masked\nlanguage modeling objective,",
    "start": "199190",
    "end": "201970"
  },
  {
    "text": "a little more deeply.",
    "start": "201970",
    "end": "203650"
  },
  {
    "text": "For transformer\nparameters H theta,",
    "start": "203650",
    "end": "206709"
  },
  {
    "text": "and some sequence of tokens x,\nwith its corresponding masked",
    "start": "206710",
    "end": "209980"
  },
  {
    "text": "version x hat.",
    "start": "209980",
    "end": "211360"
  },
  {
    "text": "This is the objective here.",
    "start": "211360",
    "end": "212685"
  },
  {
    "text": "Well, let's zoom in\non some timestep t.",
    "start": "212685",
    "end": "215568"
  },
  {
    "text": "The fundamental scoring\nthing is that we're",
    "start": "215568",
    "end": "217360"
  },
  {
    "text": "going to look up the\nvector representation",
    "start": "217360",
    "end": "219490"
  },
  {
    "text": "and the embedding\nfor that timestep t.",
    "start": "219490",
    "end": "221980"
  },
  {
    "text": "And we'll take the dot product\nof that with the output",
    "start": "221980",
    "end": "224950"
  },
  {
    "text": "representation at time t, from\nthe entire transformer model.",
    "start": "224950",
    "end": "229660"
  },
  {
    "text": "That much there, that\nscoring procedure",
    "start": "229660",
    "end": "231700"
  },
  {
    "text": "looks a lot like what you\nget from conditional language",
    "start": "231700",
    "end": "234099"
  },
  {
    "text": "models.",
    "start": "234100",
    "end": "234600"
  },
  {
    "text": "You just have to remember that\nbecause of all those attention",
    "start": "234600",
    "end": "237730"
  },
  {
    "text": "mechanisms, connecting every\ntoken to every other token.",
    "start": "237730",
    "end": "240670"
  },
  {
    "text": "This is not just the preceding\ncontext before timestep t,",
    "start": "240670",
    "end": "244030"
  },
  {
    "text": "but rather the\nentire surrounding",
    "start": "244030",
    "end": "245890"
  },
  {
    "text": "context for this position.",
    "start": "245890",
    "end": "248870"
  },
  {
    "text": "And then as usual we\nnormalize that by considering",
    "start": "248870",
    "end": "251180"
  },
  {
    "text": "all the alternative\ntokens x prime,",
    "start": "251180",
    "end": "253129"
  },
  {
    "text": "that could be in this position.",
    "start": "253130",
    "end": "255153"
  },
  {
    "text": "Now, you'll notice over here\nthere's an indicator variable",
    "start": "255153",
    "end": "257570"
  },
  {
    "text": "mt, mt is 1 if token\nt was masked, else 0.",
    "start": "257570",
    "end": "260910"
  },
  {
    "text": "So that's like saying we're\ngoing to turn on this loss,",
    "start": "260910",
    "end": "263600"
  },
  {
    "text": "only for the tokens\nthat we have masked out.",
    "start": "263600",
    "end": "267320"
  },
  {
    "text": "And then the final thing is kind\nof not a definitional choice",
    "start": "267320",
    "end": "270055"
  },
  {
    "text": "about this model, but\nsomething worth noting.",
    "start": "270055",
    "end": "271930"
  },
  {
    "text": "You'll see that we're using\nthe embedding for this token,",
    "start": "271930",
    "end": "275110"
  },
  {
    "text": "effectively as the\nsoftmax parameters.",
    "start": "275110",
    "end": "277593"
  },
  {
    "text": "There could be\nseparate parameters",
    "start": "277593",
    "end": "279009"
  },
  {
    "text": "here that we learn for the\nclassifier part, that's",
    "start": "279010",
    "end": "281710"
  },
  {
    "text": "learning to be a language model.",
    "start": "281710",
    "end": "283673"
  },
  {
    "text": "But I think people\nhave found over time,",
    "start": "283673",
    "end": "285340"
  },
  {
    "text": "that by tying these\nparameters, by using",
    "start": "285340",
    "end": "287290"
  },
  {
    "text": "the transpose of\nthese parameters",
    "start": "287290",
    "end": "288880"
  },
  {
    "text": "to create the output space, we\nget some statistical strength",
    "start": "288880",
    "end": "292240"
  },
  {
    "text": "and more efficient learning.",
    "start": "292240",
    "end": "295550"
  },
  {
    "start": "295000",
    "end": "335000"
  },
  {
    "text": "There is a second\nobjective in the BERT model",
    "start": "295550",
    "end": "297650"
  },
  {
    "text": "and it is the binary next\nsentence prediction task.",
    "start": "297650",
    "end": "301310"
  },
  {
    "text": "And I think this was\nan attempt to find",
    "start": "301310",
    "end": "302960"
  },
  {
    "text": "some coherence beyond just the\nsimple sentence or sequence",
    "start": "302960",
    "end": "306259"
  },
  {
    "text": "level.",
    "start": "306260",
    "end": "307243"
  },
  {
    "text": "So this is pretty\nstraightforward.",
    "start": "307243",
    "end": "308660"
  },
  {
    "text": "For positive instances\nfor this class,",
    "start": "308660",
    "end": "310710"
  },
  {
    "text": "we're going to take actual\nsequences of sentences",
    "start": "310710",
    "end": "313069"
  },
  {
    "text": "in the corpus that we're\nusing for training.",
    "start": "313070",
    "end": "315127"
  },
  {
    "text": "So here you can see these\nactually occur together,",
    "start": "315127",
    "end": "317210"
  },
  {
    "text": "and they are labeled as next.",
    "start": "317210",
    "end": "318860"
  },
  {
    "text": "And for negative examples\nwe just randomly choose",
    "start": "318860",
    "end": "321449"
  },
  {
    "text": "the second sentence, and\nlabel that as not next.",
    "start": "321450",
    "end": "324212"
  },
  {
    "text": "And I think the\naspiration here was",
    "start": "324212",
    "end": "325670"
  },
  {
    "text": "that this would help the model\nlearn some notion of discourse",
    "start": "325670",
    "end": "329090"
  },
  {
    "text": "coherence beyond\nthe local coherence",
    "start": "329090",
    "end": "331639"
  },
  {
    "text": "of individual sequences.",
    "start": "331640",
    "end": "335010"
  },
  {
    "start": "335000",
    "end": "402000"
  },
  {
    "text": "Now what you probably want\nto do with the BERT model",
    "start": "335010",
    "end": "337770"
  },
  {
    "text": "is not train it from scratch,\nbut rather fine tune it",
    "start": "337770",
    "end": "340410"
  },
  {
    "text": "on a particular\ntask that you have.",
    "start": "340410",
    "end": "342760"
  },
  {
    "text": "There are many modes that you\ncan think about for doing this.",
    "start": "342760",
    "end": "346098"
  },
  {
    "text": "Kind of default choice, the\nstandard and simple choice,",
    "start": "346098",
    "end": "348389"
  },
  {
    "text": "would be to use the class token.",
    "start": "348390",
    "end": "350900"
  },
  {
    "text": "More specifically, its\noutput in the final layer",
    "start": "350900",
    "end": "353280"
  },
  {
    "text": "of the BERT model, as the basis\nfor setting some task specific",
    "start": "353280",
    "end": "357330"
  },
  {
    "text": "parameters.",
    "start": "357330",
    "end": "358139"
  },
  {
    "text": "And then using that,\nwhatever labels",
    "start": "358140",
    "end": "360240"
  },
  {
    "text": "you have for\nsupervision up here.",
    "start": "360240",
    "end": "362251"
  },
  {
    "text": "And that could be effective\nbecause the class token appears",
    "start": "362252",
    "end": "364710"
  },
  {
    "text": "in this position in every\nsingle one of these sequences.",
    "start": "364710",
    "end": "367319"
  },
  {
    "text": "And so you might think of it as\na good summary representation",
    "start": "367320",
    "end": "370290"
  },
  {
    "text": "of the entire sequence.",
    "start": "370290",
    "end": "371868"
  },
  {
    "text": "And then when you\ndo the fine tuning,",
    "start": "371868",
    "end": "373410"
  },
  {
    "text": "you'll of course be updating\nthese task parameters.",
    "start": "373410",
    "end": "375870"
  },
  {
    "text": "And then you could,\nif you wanted to,",
    "start": "375870",
    "end": "377610"
  },
  {
    "text": "also update some or all\nof the actual parameters",
    "start": "377610",
    "end": "380294"
  },
  {
    "text": "from the pre-trained model.",
    "start": "380295",
    "end": "381420"
  },
  {
    "text": "And that would be a true\nnotion of fine tuning.",
    "start": "381420",
    "end": "384293"
  },
  {
    "text": "Now you might worry\nthat the class",
    "start": "384293",
    "end": "385710"
  },
  {
    "text": "token is an insufficient\nsummary of the entire sequence.",
    "start": "385710",
    "end": "388490"
  },
  {
    "text": "And so you could of course think\nabout pooling all the output",
    "start": "388490",
    "end": "391470"
  },
  {
    "text": "states in the sequence.",
    "start": "391470",
    "end": "392700"
  },
  {
    "text": "Via some function like\nsum, or mean, or max,",
    "start": "392700",
    "end": "395550"
  },
  {
    "text": "and using those as\nthe input to whatever",
    "start": "395550",
    "end": "398190"
  },
  {
    "text": "task-specific parameters\nyou have up here at the top.",
    "start": "398190",
    "end": "402620"
  },
  {
    "start": "402000",
    "end": "428000"
  },
  {
    "text": "I just want to remind us\nthat tokenization in BERT",
    "start": "402620",
    "end": "405260"
  },
  {
    "text": "is a little unusual.",
    "start": "405260",
    "end": "406440"
  },
  {
    "text": "We've covered this\na few times before,",
    "start": "406440",
    "end": "408050"
  },
  {
    "text": "but just remember that we're\ngetting effectively not",
    "start": "408050",
    "end": "410216"
  },
  {
    "text": "full words, but word pieces.",
    "start": "410217",
    "end": "412550"
  },
  {
    "text": "So for cases like\n\"encode me\", you",
    "start": "412550",
    "end": "414595"
  },
  {
    "text": "can see that the word\n\"encode\" has been split apart",
    "start": "414595",
    "end": "416720"
  },
  {
    "text": "into two word pieces.",
    "start": "416720",
    "end": "417890"
  },
  {
    "text": "And we're hoping,\nimplicitly, that the model",
    "start": "417890",
    "end": "420320"
  },
  {
    "text": "can learn that that is in\nsome deep sense still a word.",
    "start": "420320",
    "end": "422930"
  },
  {
    "text": "Even though it has been\nsplit apart and that",
    "start": "422930",
    "end": "425120"
  },
  {
    "text": "should draw on the\ntruly contextual nature",
    "start": "425120",
    "end": "427070"
  },
  {
    "text": "of these models.",
    "start": "427070",
    "end": "428390"
  },
  {
    "start": "428000",
    "end": "485000"
  },
  {
    "text": "The BERT team did two\ninitial model releases.",
    "start": "428390",
    "end": "431000"
  },
  {
    "text": "BERT base consists of\n12 transformer layers",
    "start": "431000",
    "end": "433670"
  },
  {
    "text": "and has representations of\ndimension 768 with 12 attention",
    "start": "433670",
    "end": "437645"
  },
  {
    "text": "heads for a total of\n110 million parameters.",
    "start": "437645",
    "end": "440629"
  },
  {
    "text": "That's of course,\na very large model,",
    "start": "440630",
    "end": "442190"
  },
  {
    "text": "but this is manageable\nfor you to do local work.",
    "start": "442190",
    "end": "444620"
  },
  {
    "text": "Especially if you just want\nto do some simple fine tuning,",
    "start": "444620",
    "end": "447320"
  },
  {
    "text": "or use this model for inference.",
    "start": "447320",
    "end": "449930"
  },
  {
    "text": "The BERT large release is\nmuch larger, it has 24 layers.",
    "start": "449930",
    "end": "453979"
  },
  {
    "text": "Twice the dimensionality\nfor its representations,",
    "start": "453980",
    "end": "456350"
  },
  {
    "text": "and 16 attention\nheads for a total",
    "start": "456350",
    "end": "458330"
  },
  {
    "text": "of 340 million parameters.",
    "start": "458330",
    "end": "460723"
  },
  {
    "text": "This is large\nenough that it might",
    "start": "460723",
    "end": "462139"
  },
  {
    "text": "be difficult to do\nlocal work with.",
    "start": "462140",
    "end": "464180"
  },
  {
    "text": "But of course you might get\nmuch more representational power",
    "start": "464180",
    "end": "466910"
  },
  {
    "text": "from using it.",
    "start": "466910",
    "end": "468293"
  },
  {
    "text": "For both of these models,\nwe have a limitation",
    "start": "468293",
    "end": "470210"
  },
  {
    "text": "to 512 tokens.",
    "start": "470210",
    "end": "472009"
  },
  {
    "text": "And that is because\nthat is the size",
    "start": "472010",
    "end": "474140"
  },
  {
    "text": "of the positional embedding\nspace that they learned.",
    "start": "474140",
    "end": "476367"
  },
  {
    "text": "There are many new\nreleases of course.",
    "start": "476367",
    "end": "477950"
  },
  {
    "text": "You can find those\nat the project site",
    "start": "477950",
    "end": "479533"
  },
  {
    "text": "and Hugging Face has\nmade it very easy",
    "start": "479533",
    "end": "481520"
  },
  {
    "text": "to access these models, and\nthat's been very empowering.",
    "start": "481520",
    "end": "485270"
  },
  {
    "start": "485000",
    "end": "589000"
  },
  {
    "text": "To close this let me just\nmention a few known limitations",
    "start": "485270",
    "end": "487759"
  },
  {
    "text": "of BERT that we're\ngoing to return",
    "start": "487760",
    "end": "489177"
  },
  {
    "text": "to as we go through\nsome subsequent models",
    "start": "489177",
    "end": "490940"
  },
  {
    "text": "for this unit.",
    "start": "490940",
    "end": "492210"
  },
  {
    "text": "So first in the\noriginal paper, there",
    "start": "492210",
    "end": "493819"
  },
  {
    "text": "is a large, but still partial\nnumber of ablation studies",
    "start": "493820",
    "end": "497480"
  },
  {
    "text": "and optimization studies.",
    "start": "497480",
    "end": "499100"
  },
  {
    "text": "There's a huge landscape\nthat's in play here,",
    "start": "499100",
    "end": "501830"
  },
  {
    "text": "and only small parts of it are\nexplored in the original paper.",
    "start": "501830",
    "end": "504806"
  },
  {
    "text": "So we might worry that there\nare better choices we could",
    "start": "504807",
    "end": "507140"
  },
  {
    "text": "be making within this space.",
    "start": "507140",
    "end": "510470"
  },
  {
    "text": "The original paper\nalso points out",
    "start": "510470",
    "end": "512210"
  },
  {
    "text": "that there's some unnaturalness\nabout this MASK token.",
    "start": "512210",
    "end": "514669"
  },
  {
    "text": "They say, the first downside\nof the MLM objective",
    "start": "514669",
    "end": "517700"
  },
  {
    "text": "is that we are\ncreating a mismatch",
    "start": "517700",
    "end": "519289"
  },
  {
    "text": "between pre-training\nand fine tuning,",
    "start": "519289",
    "end": "521120"
  },
  {
    "text": "because the MASK token is\nnever seen during fine tuning.",
    "start": "521120",
    "end": "524330"
  },
  {
    "text": "So that's something we\nmight want to address.",
    "start": "524330",
    "end": "527158"
  },
  {
    "text": "They also point out\nthat there's a downside",
    "start": "527158",
    "end": "528950"
  },
  {
    "text": "to using the MLM objective\nwhich is to say that it's",
    "start": "528950",
    "end": "531117"
  },
  {
    "text": "kind of data inefficient.",
    "start": "531117",
    "end": "532940"
  },
  {
    "text": "We can only mask out a small\npercentage of the tokens.",
    "start": "532940",
    "end": "535580"
  },
  {
    "text": "Because we need the\nsurrounding context",
    "start": "535580",
    "end": "537350"
  },
  {
    "text": "that would ostensibly\nreproduce those tokens.",
    "start": "537350",
    "end": "540290"
  },
  {
    "text": "And that means that it's\nkind of data inefficient.",
    "start": "540290",
    "end": "543639"
  },
  {
    "text": "And finally this is\nfrom the XLNet paper,",
    "start": "543640",
    "end": "545590"
  },
  {
    "text": "I think this is\nquite perceptive.",
    "start": "545590",
    "end": "547240"
  },
  {
    "text": "BERT assumes that\nthe predicted tokens",
    "start": "547240",
    "end": "549130"
  },
  {
    "text": "are independent of each other\ngiven the unmasked tokens,",
    "start": "549130",
    "end": "552040"
  },
  {
    "text": "which is oversimplified\nas high-order,",
    "start": "552040",
    "end": "554019"
  },
  {
    "text": "long-range dependency is\nprevalent in natural language.",
    "start": "554020",
    "end": "556837"
  },
  {
    "text": "What they have in mind\nhere, is essentially,",
    "start": "556837",
    "end": "558670"
  },
  {
    "text": "if you have an idiom\nlike \"Out of this world\"",
    "start": "558670",
    "end": "561459"
  },
  {
    "text": "and it happens that both\nthe first and the last words",
    "start": "561460",
    "end": "563980"
  },
  {
    "text": "in that idiom are\nmasked out, then",
    "start": "563980",
    "end": "565990"
  },
  {
    "text": "BERT is going to try to\nreproduce them as though they",
    "start": "565990",
    "end": "568240"
  },
  {
    "text": "were independent of each other.",
    "start": "568240",
    "end": "569640"
  },
  {
    "text": "And in fact, we know that there\nis a statistical dependency",
    "start": "569640",
    "end": "572710"
  },
  {
    "text": "between them coming\nfrom the fact",
    "start": "572710",
    "end": "574480"
  },
  {
    "text": "that they are participating\nin this idiom.",
    "start": "574480",
    "end": "576790"
  },
  {
    "text": "So there's some notion of\nrepresentational coherence",
    "start": "576790",
    "end": "580810"
  },
  {
    "text": "that BERT is simply\nnot capturing",
    "start": "580810",
    "end": "582460"
  },
  {
    "text": "with its MLM objective.",
    "start": "582460",
    "end": "585060"
  },
  {
    "start": "585060",
    "end": "589000"
  }
]