[
  {
    "start": "0",
    "end": "40000"
  },
  {
    "text": "Hello. So welcome to lecture 3. This is going to be about\nclassification and regression.",
    "start": "5100",
    "end": "11340"
  },
  {
    "text": "This moves us from\nour first task, which we were doing last time, which\nwas this regression how we fit a line into a task that\nwill look really, really",
    "start": "11340",
    "end": "19630"
  },
  {
    "text": "similar at first, but we'll have\na couple of subtle differences. And we'll go through that,\nwhich is classification.",
    "start": "19630",
    "end": "24939"
  },
  {
    "text": "Remember we talked\nabout classification was for discrete objects\nlike, that what's the animal in this photo,\nis it a cat, a pig, a horse,",
    "start": "24940",
    "end": "31699"
  },
  {
    "text": "something like that. Those are the types of\nproblems that are pretty prevalent in machine learning. And so we'll talk through\nthose basic issues today.",
    "start": "31699",
    "end": "38940"
  },
  {
    "text": "What we're going\nto do, though, is we're first going to start\nwith this probabilistic view of linear regression.",
    "start": "38940",
    "end": "45820"
  },
  {
    "start": "40000",
    "end": "220000"
  },
  {
    "text": "And the reason we're\ngoing to do this is we're going to walk\nthrough again in that setting, where it's hopefully relatively\nfamiliar what's going on,",
    "start": "45820",
    "end": "51230"
  },
  {
    "text": "how we give a generative\nmodel, is the term for it, for this underlying\nkind of optimization",
    "start": "51230",
    "end": "58500"
  },
  {
    "text": "class for linear regression. So we're going to interpret\nit probabilistically. And that interpretation,\nwe're going to be able to use\nfor classification,",
    "start": "58500",
    "end": "64820"
  },
  {
    "text": "and then again on Wednesday, for\na much richer class of models, which are all these\nexponential family models.",
    "start": "64820",
    "end": "71400"
  },
  {
    "text": "And I will assert\nat a high level to try and keep in your head. These things all look the same. We're trying to get to\nthis, an abstraction that",
    "start": "71400",
    "end": "77850"
  },
  {
    "text": "lets us solve them, do\ninference with them, and reason about them\nin a similar way. So this is one key\nbuilding block.",
    "start": "77850",
    "end": "83610"
  },
  {
    "text": "So we'll start with\nthat probabilistic view of linear regression. Then we're going to talk\nabout classification.",
    "start": "83610",
    "end": "89000"
  },
  {
    "text": "And at first blush,\nclassification is going to look just\nlike something we could solve with linear regression.",
    "start": "89000",
    "end": "94070"
  },
  {
    "text": "So I just want to make sure\nit's really clear in your mind when we use classification and\nwhen we use linear regression and kind of what the little\nproblems are or challenges",
    "start": "94070",
    "end": "101159"
  },
  {
    "text": "are as you go through\nactually doing that. Then we're going to introduce\nthe workhorse of machine",
    "start": "101159",
    "end": "106830"
  },
  {
    "text": "learning, logistic regression. This is something\nthat I probably use every day in\nsome form or another.",
    "start": "106830",
    "end": "113070"
  },
  {
    "text": "It has different names\nin deep learning now and the way people\nuse it, sometimes called like the linear\nlayer and softmax.",
    "start": "113070",
    "end": "118370"
  },
  {
    "text": "If you're a deep\nlearning aficionado, don't worry about it. But this is kind of\nthe standard workhorse.",
    "start": "118370",
    "end": "123600"
  },
  {
    "text": "And we can say a ton about\nhow logistic regression works. And it was not invented by\nmachine learning people.",
    "start": "123600",
    "end": "128860"
  },
  {
    "text": "This is an old and\nclassical algorithm that our statistical\nfriends invented. We use it in slightly\ndifferent ways",
    "start": "128860",
    "end": "135040"
  },
  {
    "text": "than maybe they\noriginally intended, and I can get into\nthose in a little bit. Now, as I mentioned,\nthere's going",
    "start": "135040",
    "end": "140069"
  },
  {
    "text": "to be parallel structures. So we're going to talk\nabout logistic regression. We're going to talk about\nwhy not logistic regression--",
    "start": "140069",
    "end": "145420"
  },
  {
    "text": "I'm sorry-- linear\nregression, and we'll talk about logistic regression,\nwhich is confusingly enough a classification\nalgorithm, although it",
    "start": "145420",
    "end": "150660"
  },
  {
    "text": "says regression in there. Don't blame us. Blame the stats people. And then once we do\nthat, we're going to parallel exactly as we\ndid in the last lecture",
    "start": "150660",
    "end": "156709"
  },
  {
    "text": "and talk about how to solve it. OK? And when we solve\nit, you're going to be introduced to a method\ncalled Newton's method, which",
    "start": "156709",
    "end": "162330"
  },
  {
    "text": "maybe you've seen if you\ntook a kind of a stats course at some point, or\ncalculus course.",
    "start": "162330",
    "end": "168080"
  },
  {
    "text": "And we'll reintroduce\na way to solve it. And Newton's method,\nwhen it's applicable, is really, really fast, meaning\nit converges very quickly.",
    "start": "168080",
    "end": "175550"
  },
  {
    "text": "But each step of that algorithm,\nwe'll see is quite expensive. And so it's not really\nappropriate for a lot of the places that machine\nlearning people care to use it.",
    "start": "175550",
    "end": "183080"
  },
  {
    "text": "OK? So the message is\nfrom this lecture if you get is the what\nis classification, why does it differ\nfrom regression, what",
    "start": "183080",
    "end": "188860"
  },
  {
    "text": "is the workhorse model\nin logistic regression, and then a method to solve it. And then we're\ngoing to come back at the end and kind of\ncompare and contrast",
    "start": "188860",
    "end": "195260"
  },
  {
    "text": "the different ways that people\nsolve these different things, all right? As last time, there is a\nthread that started online,",
    "start": "195260",
    "end": "202010"
  },
  {
    "text": "if you feel more\ncomfortable asking your questions anonymously. Last time, we had a\nbunch of great questions. I'd love to keep that going.",
    "start": "202010",
    "end": "207310"
  },
  {
    "text": "Super happy to talk with you\nabout anything that's there. But I will keep the lecture",
    "start": "207310",
    "end": "213810"
  },
  {
    "text": "if you want. OK. Awesome. Let's get started. All right.",
    "start": "213810",
    "end": "219920"
  },
  {
    "text": "So we're going to start again\nwith our friendly squares, right? So just to give\nyou the format, you",
    "start": "219920",
    "end": "225230"
  },
  {
    "text": "should think about these\ntasks, you're given something, and your goal is to\ndo something with it. So we're given some xi and\nyi, for i equals 1 to n.",
    "start": "225230",
    "end": "237980"
  },
  {
    "text": "And recall, this is the\ntraining set, right? In this case, xi\nlives in some Rd or Rd",
    "start": "237980",
    "end": "249969"
  },
  {
    "text": "plus 1 as our\nconvention is, d plus 1 recall, because we had\nthat convention that there was a bias term, where every\nsingle entry had a 1 appended",
    "start": "249969",
    "end": "257930"
  },
  {
    "text": "to it if you remember\nthat from last time. If not, don't worry. Just remember like, why\nis there d plus 1 there?",
    "start": "257930",
    "end": "263240"
  },
  {
    "text": "It's by convention. And we had a target variable\ny, and this target variable,",
    "start": "263240",
    "end": "268780"
  },
  {
    "text": "which I'll highlight in\npurple, this target variable was a real valued number, right?",
    "start": "268780",
    "end": "275349"
  },
  {
    "text": "So this is-- picture a\nline for the moment, OK? And our goal was that we\nwanted to find some theta",
    "start": "275350",
    "end": "284190"
  },
  {
    "text": "element of also Rd plus",
    "start": "284190",
    "end": "292060"
  },
  {
    "text": "the argmin, or very, very\nclose to it because remember, argmin, we can't really\nsolve these exactly, even though we like it,\nover theta of sum i equals 1",
    "start": "292060",
    "end": "300650"
  },
  {
    "text": "to n, yi minus h of\ntheta x of i squared,",
    "start": "300650",
    "end": "310400"
  },
  {
    "text": "where h of theta x of\ni equals theta dot x.",
    "start": "310400",
    "end": "316880"
  },
  {
    "text": "Actually, I'm just\ngoing to remove the xi. I don't care. It's true for any example.",
    "start": "316880",
    "end": "323659"
  },
  {
    "text": "OK. I'll put transpose here just\nto make sure it's clear. I have a dot there. But awesome, OK?",
    "start": "323660",
    "end": "330250"
  },
  {
    "text": "Now, I'm using this slightly\nmore general notation, this h of theta. And that looks like a little bit\nof overkill for a dot product,",
    "start": "330250",
    "end": "336780"
  },
  {
    "text": "but we're going to use\nthat in several ways through the next\ncouple of lectures. So apologies for that. If you remember\nhere, we had this.",
    "start": "336780",
    "end": "342430"
  },
  {
    "text": "The way we define this\ntheta is we said, oh, it minimizes the losses\nor the residuals squared. And we didn't give any\njustification for this.",
    "start": "342430",
    "end": "349169"
  },
  {
    "text": "And what we want to do\nis in this next part go kind of one level deeper\nand ask this why question.",
    "start": "349169",
    "end": "355020"
  },
  {
    "text": "Why did we pick to minimize\nthe sum of squares? Now, this will introduce us\nto one of our favorite friends",
    "start": "355020",
    "end": "360990"
  },
  {
    "text": "in this course, which is\nthe Gaussian distribution, and we'll talk about why that's\na plausible thing to do, right?",
    "start": "360990",
    "end": "366800"
  },
  {
    "text": "You could ask why the\nGaussian distribution, and I can wax with\nyou philosophical. I'll give you a sense of\nit, but we'll come back to that in one second.",
    "start": "366800",
    "end": "372430"
  },
  {
    "text": "So our goal is, OK,\nwe have this equation, but where did it come from? And by thinking\nabout where does it come from, that's going to tell\nus how to generalize it, right?",
    "start": "372430",
    "end": "379990"
  },
  {
    "text": "That's the plan for\nwhat we're up to. All right. So this is our first\nmodel in the class,",
    "start": "379990",
    "end": "387910"
  },
  {
    "text": "really like generative model. We're going to assume\nthat yi looks as follows,",
    "start": "387910",
    "end": "395300"
  },
  {
    "text": "it's going to equal theta t xi. I'm going to unpack this in\none second plus some epsilon i.",
    "start": "395300",
    "end": "403039"
  },
  {
    "text": "And this character here, this\nis an error or a noise term. All right, so let's unpack\nthis, because the first time",
    "start": "403039",
    "end": "411550"
  },
  {
    "text": "we're seeing one\nof these things. OK. So this thing here, maybe\nthis makes sense to you. You're like, oh, there\nexists some true theta that's",
    "start": "411550",
    "end": "418349"
  },
  {
    "text": "out there. That's what this is saying. This model is saying. There's some theta, I'm\ngoing to call theta star to make sure it's clear.",
    "start": "418349",
    "end": "424330"
  },
  {
    "text": "There's some theta\nstar that's out there, one that's hidden from us. But all of my data was\ngenerated by the taking",
    "start": "424330",
    "end": "430830"
  },
  {
    "text": "that parameter with the\nfeatures and generating the y. OK? This would be a situation\nwhere all of your data",
    "start": "430830",
    "end": "436930"
  },
  {
    "text": "laid perfectly on a line, right? Because it's just saying\nthat given the features, I know exactly\nwhat y's value is.",
    "start": "436930",
    "end": "443560"
  },
  {
    "text": "Now what we're\nsaying is something that's not quite that strong. That would be\nnoiseless situation.",
    "start": "443560",
    "end": "448800"
  },
  {
    "text": "We're adding in a\nlittle bit of noise. And when machine\nlearners or statisticians say noise, what they mean is\nstuff we can't really explain.",
    "start": "448800",
    "end": "456050"
  },
  {
    "text": "That's the kind of\nthing to think about it. We're modeling it up to this. OK? And maybe we know a little\nbit about the noise,",
    "start": "456050",
    "end": "461888"
  },
  {
    "text": "and we'll talk\nabout what we might expect from noise in a second. But like we expect that there's\nsome kind of random gyration,",
    "start": "461889",
    "end": "468400"
  },
  {
    "text": "maybe this accounts for in\nkind of physical setting, some measurement error, right? Some classical jitter that's\nunderneath the covers.",
    "start": "468400",
    "end": "474639"
  },
  {
    "text": "Maybe there's something\nwhere if you're more kind of\nsophisticated Bayesian, you think it's subject\nto your information.",
    "start": "474639",
    "end": "480169"
  },
  {
    "text": "I only know the features x, and\nthere's some unmodified piece that's in the noise. And I'm willing to\nkind of minimize that.",
    "start": "480169",
    "end": "486520"
  },
  {
    "text": "OK? So what are the properties\nyou would want of this error, OK, right? So this is a forward\nmodel by the way.",
    "start": "486520",
    "end": "492190"
  },
  {
    "text": "It tells me, I don't know\ntheta, but I know how my data is generated, right? That's what I mean when\nI say generative models.",
    "start": "492190",
    "end": "497900"
  },
  {
    "text": "If you give me an\nXi and presumably, knew how to construct epsilon,\nyou could get a yi value, OK?",
    "start": "497900",
    "end": "503630"
  },
  {
    "text": "Now we don't get to see\nepsilon i by the way. It doesn't appear\nin the training set. It's just a mental model for\nhow the data are actually",
    "start": "503630",
    "end": "509669"
  },
  {
    "text": "linked together. And that's going to be\nfairly important when we start to generalize\nthese models, right? They have certain kinds of\nerrors we can characterize.",
    "start": "509669",
    "end": "516710"
  },
  {
    "text": "OK? So what are the properties that\nwe would expect of epsilon i?",
    "start": "516710",
    "end": "524110"
  },
  {
    "text": "So first notice that\nit's epsilon super i is that noise is\ndifferent per tuple. It's not like there's\nsome noise offset",
    "start": "524110",
    "end": "530620"
  },
  {
    "text": "that was just added to\nall our data and shifted. Every single point\nif you like, we're going to have a random model,\nyou get a random sample",
    "start": "530620",
    "end": "537209"
  },
  {
    "text": "from that noise. OK? And that's what\ndetermines what the yi is. OK? All right.",
    "start": "537209",
    "end": "542620"
  },
  {
    "text": "So what would you\nexpect from that? Well it's random. We probably want something\nwhere the expected value",
    "start": "542620",
    "end": "549110"
  },
  {
    "text": "over all the epsilon i's over\nthat random process is 0. This is sometimes\nmean it's unbiased.",
    "start": "549110",
    "end": "555930"
  },
  {
    "text": "OK? Now this is on one hand like a\ndeep philosophical statement,",
    "start": "555930",
    "end": "561899"
  },
  {
    "text": "and other hand kind of\na trivial statement. The deep philosophical\nstatement is we're kind of saying that these\nerrors if you think about it",
    "start": "561899",
    "end": "568200"
  },
  {
    "text": "is kind of unreasonable like\nif I average over infinitely many of them, they're not going\nto appreciably change what",
    "start": "568200",
    "end": "573820"
  },
  {
    "text": "the true y value is, right? They don't have any information\nthat's inside the model. That's really what it's saying.",
    "start": "573820",
    "end": "579150"
  },
  {
    "text": "I may still get a\nsample where epsilon i is 0.1 or 0.2 or negative",
    "start": "579150",
    "end": "585040"
  },
  {
    "text": "But on average, I'm just making\na statement in a population like I don't care\nabout this value. It's going to be averaged\naway to 0 in a precise sense.",
    "start": "585040",
    "end": "591820"
  },
  {
    "text": "Now on the other hand, if\nit were not a zero value, that would be kind\nof a strange thing",
    "start": "591820",
    "end": "596990"
  },
  {
    "text": "because we have a bias term. Suggests that you could actually\njust kind of incorporate that standing bias\nin the theta star.",
    "start": "596990",
    "end": "602529"
  },
  {
    "text": "OK? So I don't want to go\ntoo far down that path. But like operationally, this\nis not an unreasonable thing",
    "start": "602529",
    "end": "607620"
  },
  {
    "text": "to say, and I want\nyou to think about it as kind of a statement\nof information like I modeled all the\nfeatures in the X's, right?",
    "start": "607620",
    "end": "613889"
  },
  {
    "text": "Those are all my features,\nthe house price, remember? The lot size, all that\nstuff, and there's something I haven't modeled.",
    "start": "613890",
    "end": "620560"
  },
  {
    "text": "Maybe the house looks\na little bit nicer. Maybe it reminds people of\nwhere they grew up as children. Maybe it was like a famous\nhouse or something that probably",
    "start": "620560",
    "end": "628570"
  },
  {
    "text": "doesn't count for noise. But there's something\nabout it that's unmodeled as a statement\nof information.",
    "start": "628570",
    "end": "634810"
  },
  {
    "text": "The second thing is a\nlittle bit more subtle. The second thing is that\nthe errors are independent.",
    "start": "634810",
    "end": "640640"
  },
  {
    "text": "OK. Now we don't always\nmake this, but this is going to allow us to do some\npretty healthy mathematics.",
    "start": "640640",
    "end": "647740"
  },
  {
    "text": "And let's talk about what\nwould happen if it failed. And what this means\nformally is this I'm going to write\ndown a strong form, OK?",
    "start": "647740",
    "end": "657100"
  },
  {
    "text": "It says that these\ntwo things equal the expected value of\nepsilon i, and I mean it in a very strong sense.",
    "start": "657100",
    "end": "663000"
  },
  {
    "text": "If you know about\nvarious different notions of independence and\nuncorrelation, don't worry, OK?",
    "start": "663000",
    "end": "668660"
  },
  {
    "text": "All right. So this is for i not equal to j. That's what that\nlittle piece is here.",
    "start": "668660",
    "end": "673940"
  },
  {
    "text": "Maybe I'll write it bigger\nfor i not equal to j. OK, what does this mean, OK?",
    "start": "673940",
    "end": "680160"
  },
  {
    "text": "So if you remember your\nnotion of independence, what we're saying\nhere is that they're statistically independent. And I mean in a\nreally strong sense",
    "start": "680160",
    "end": "685970"
  },
  {
    "text": "like knowing the\nerror for one tuple doesn't tell me anything about\nthe error for another tuple. This is consistent with my\nearlier kind of interpretation",
    "start": "685970",
    "end": "693860"
  },
  {
    "text": "of errors like how much\ninformation I know about it. If I didn't know\nsomething about that error and I could model\nit, then I'd have",
    "start": "693860",
    "end": "699830"
  },
  {
    "text": "to have kind of a more\ncomplicated model here. All right. Any questions about this so far?",
    "start": "699830",
    "end": "707570"
  },
  {
    "text": "Awesome. OK. Now with this setup,\nthere's one other thing.",
    "start": "707570",
    "end": "713490"
  },
  {
    "start": "710000",
    "end": "955000"
  },
  {
    "text": "So so far, I made\ntwo assumptions. At this stage, I hope\nyou think they're like plausible to make progress. And that's one of the things\nby the way about machine",
    "start": "713490",
    "end": "719790"
  },
  {
    "text": "learning that I think\npeople kind of get uncomfortable about. I certainly was\nwhen I first started and kind of statistical modeling\nthat like you're like well",
    "start": "719790",
    "end": "725790"
  },
  {
    "text": "is that really true. Kind of not the right\nquestion to ask. It's like is it a useful\nassumption to make progress",
    "start": "725790",
    "end": "731620"
  },
  {
    "text": "like, what am I giving\nup by making it, which is a much harder\nthing to assess. It's not ever true like. If you look at real\nerrors, they're",
    "start": "731620",
    "end": "737571"
  },
  {
    "text": "very infrequently Gaussian\ndistributed, right? That's kind of terrifying,\nwhy do we use it everywhere? Well it still works pretty well\nbecause we're not assuming too",
    "start": "737571",
    "end": "745019"
  },
  {
    "text": "much about the underlying data. Now if you know something\nabout that data, we'll come back in\nthe next lecture and tell you how to put more\ninformation about the error.",
    "start": "745019",
    "end": "752130"
  },
  {
    "text": "But I just want you to get\na sense like these are kind of strange modeling constructs. Now one thing that\nwe'll care about,",
    "start": "752130",
    "end": "758250"
  },
  {
    "text": "which is a function of\nthis is how noisy is it. We need a measure of noise. And so a natural thing\nto assume is that--",
    "start": "758250",
    "end": "768500"
  },
  {
    "text": "and we can relax\nthis assumption. Let's imagine that\nthey're all kind of a uniform\nbackground of noise,",
    "start": "768500",
    "end": "773640"
  },
  {
    "text": "so everyone has\nthe same variance. This is a standard\nvariance assumption.",
    "start": "773640",
    "end": "778959"
  },
  {
    "text": "The sigma squared,\nso there's noise. We don't know anything about it. We know it's unbiased. And we know that it has\nabout the same magnitude.",
    "start": "778960",
    "end": "784850"
  },
  {
    "text": "It's not wildly\ndifferent on some piece of our data versus others. And again, it's\nlike a statement of, if you want to be really\nphilosophical like an ignorance",
    "start": "784850",
    "end": "791800"
  },
  {
    "text": "prior. We don't know\nanything, so how would we know that this\npart of our data has more noise than the other. It's just an assumption\nto make progress.",
    "start": "791800",
    "end": "798570"
  },
  {
    "text": "Please. Why does [INAUDIBLE] equal\nto the expected value of that",
    "start": "798570",
    "end": "805320"
  },
  {
    "text": "square? Oh, yeah. So this the variance here,\nthe variance formally is epsilon squared\nminus the mu, so",
    "start": "805320",
    "end": "810730"
  },
  {
    "text": "normally when you\nhave a variance. But I've elided that because\nthis first term is 0.",
    "start": "810730",
    "end": "816020"
  },
  {
    "text": "So it is actually the variance. I just wrote it as\nthe square there. So it is epsilon minus\nthe mu, the mu is just 0. Wonderful question.",
    "start": "816020",
    "end": "822430"
  },
  {
    "text": "Thank you so much for that. Please. [INAUDIBLE]",
    "start": "822430",
    "end": "828320"
  },
  {
    "text": "So the error here is\na sample from a value, so it's actually\na discrete value that's underneath the covers.",
    "start": "828320",
    "end": "833490"
  },
  {
    "text": "So the way to picture\nit is like imagine-- I don't know. I hate invoking deities,\nbut imagine there's god,",
    "start": "833490",
    "end": "838829"
  },
  {
    "text": "and she's got her table, right? And then he got the value of all\nthe x's and she has her theta 0 and she produces a y,\nthen for whatever reason,",
    "start": "838829",
    "end": "845500"
  },
  {
    "text": "she adds a little error to it. That error is a specific scalar\nthat changed y from 0.2 to 0.4.",
    "start": "845500",
    "end": "851600"
  },
  {
    "text": "And I'm just saying that's the\npiece that we want to model. And the reason we\nwant to model that is because what\nwe're coming to--",
    "start": "851600",
    "end": "857019"
  },
  {
    "text": "so does that make sense, the\ntypes check, it's a scalar, not a function there? The reason we want to\nget to that is we're",
    "start": "857020",
    "end": "863350"
  },
  {
    "text": "going to solve this\nproblem up to noise, and we're going to worry\na lot in this theory, we won't do it too\nmuch in this lecture,",
    "start": "863350",
    "end": "869350"
  },
  {
    "text": "but we worry a lot in\nthe theory of could we solve it up to the noise floor? If you're making decisions\nthat depend on how noisy it",
    "start": "869350",
    "end": "876779"
  },
  {
    "text": "is and your data has\na sigma squared of 1 and you're trying to make\ndecisions where your values are",
    "start": "876779",
    "end": "882290"
  },
  {
    "text": "like 0.1 apart,\nintuitively, something should be wrong there. You're reading into the noise. So we'll worry a lot about\nhow our procedure scale",
    "start": "882290",
    "end": "889380"
  },
  {
    "text": "with the noise. So you can think about\nthis as saying like, there's some average noise,\nand it's noisy or not.",
    "start": "889380",
    "end": "895450"
  },
  {
    "text": "If it were 0, then our\ndata is perfectly clean. That's the way to\nthink about it for now. And if my explanations about\nscaling and other stuff",
    "start": "895450",
    "end": "902639"
  },
  {
    "text": "seem like obtuse and weird,\nplease don't worry about it. We'll come back to our\npicture of the Gaussian, and it will be hopefully a\nlittle bit clearer in a second.",
    "start": "902639",
    "end": "912060"
  },
  {
    "text": "Other questions? All right. Now, here's the\nremarkable thing. Here's how the\nGaussian comes up.",
    "start": "912060",
    "end": "917649"
  },
  {
    "text": "It comes up for a really\ninteresting reason, which is, if I tell you that\nI want a distribution such that it's unbiased and it\nhas this sigma squared,",
    "start": "917649",
    "end": "926850"
  },
  {
    "text": "I know its variance, and I\nassume nothing else about it, the Bayesian used to make\na lot of noise about this,",
    "start": "926850",
    "end": "932990"
  },
  {
    "text": "then that distribution\nis uniquely the Gaussian. So you don't have to know that\nin some fundamental sense,",
    "start": "932990",
    "end": "938019"
  },
  {
    "text": "but it leads you\nto this conclusion. That is a distribution that\nhas these two properties and you're not assuming\nthings about how",
    "start": "938019",
    "end": "943959"
  },
  {
    "text": "it's third and fourth moments,\nlike if you wrote a 3 or 4 there. I don't have to assume\nwhat they are, right?",
    "start": "943959",
    "end": "949420"
  },
  {
    "text": "They're just given. So let's see. If I can skip to it.",
    "start": "949420",
    "end": "956949"
  },
  {
    "start": "955000",
    "end": "1220000"
  },
  {
    "text": "So it turns out this is\nthe unique distribution. This is unique in some sense. That doesn't really\nmatter too much. That's more like\nphilosophical of the above.",
    "start": "956949",
    "end": "963750"
  },
  {
    "text": "And I've been a little\nbit too imprecise to really appreciate\nthat but that you should kind of take away.",
    "start": "963750",
    "end": "968930"
  },
  {
    "text": "OK? So this is our\nfriend, the Gaussian. Let's go through the notation\nfirst of what we mean. So this epsilon i here,\ngetting to the great question",
    "start": "968930",
    "end": "975990"
  },
  {
    "text": "earlier, what it says\nis epsilon i is drawn. This is what the total\nmeans, drawn, distributed",
    "start": "975990",
    "end": "981350"
  },
  {
    "text": "to n of mu sigma squared. And I'm going to draw\nthat in a second. This is a normal distribution,\nmeans this is the mean.",
    "start": "981350",
    "end": "990110"
  },
  {
    "text": "This is the mean of\nthat normal distribution and this is the variance. OK?",
    "start": "990110",
    "end": "995940"
  },
  {
    "text": "And here's a picture of it. We'll get to it in one second. This is the mean\nright here and this",
    "start": "995940",
    "end": "1001819"
  },
  {
    "text": "is how the distribution looks. And so epsilon,\nthe way it's picked is, I will sample with\nprobability proportional",
    "start": "1001820",
    "end": "1007250"
  },
  {
    "text": "to the height of this\ncurve right here, may I pick a value here, and\nthat's how I get the epsilon i. And I'm repeatedly drawing from\nthis underlying distribution.",
    "start": "1007250",
    "end": "1015490"
  },
  {
    "text": "OK? That's the mental model\nI have of epsilon. Now, is that actually\nwhat's going on? I don't know.",
    "start": "1015490",
    "end": "1020819"
  },
  {
    "text": "Don't know god. Don't know how that works. But it's our model of\nhow the world is going. OK? Sound good?",
    "start": "1020819",
    "end": "1027970"
  },
  {
    "text": "All right. Now, as we go through,\nthere's a couple of things. This distribution is\nactually fairly peaked.",
    "start": "1027970",
    "end": "1035579"
  },
  {
    "text": "So maybe you've seen\nthis like a central limit theorem or something before in\nearlier classes at some point.",
    "start": "1035579",
    "end": "1041240"
  },
  {
    "text": "You've seen this idea that\nif I take a bunch of things and add them\ntogether, they kind of converge to this distribution. I won't make that\nstatement precise.",
    "start": "1041240",
    "end": "1047928"
  },
  {
    "text": "But there's a reason this\nthing kind of comes up. If you have a bunch\nof additive errors, they end up looking, when\nyou average over all of them,",
    "start": "1047929",
    "end": "1054039"
  },
  {
    "text": "they end up looking Gaussian. OK? So if you have a lot of\nlittle tiny additive errors, they end up looking Gaussian. There's too much philosophy\nfor why this thing shows up.",
    "start": "1054040",
    "end": "1061240"
  },
  {
    "text": "If none of that matters\nto you, don't worry. It shows up and we're\ngoing to use it. All right.",
    "start": "1061240",
    "end": "1067100"
  },
  {
    "text": "So just to make sure you\nunderstand this function and what it looks like,\nhere's the mean value of it. If you see the sigma\nversion, so this",
    "start": "1067100",
    "end": "1074630"
  },
  {
    "text": "is the square root of\nthat sigma squared, you see that within one sigma,\nyou have here 63% or 69%",
    "start": "1074630",
    "end": "1081960"
  },
  {
    "text": "of the mass. By the way, these notes, you\ncan go download the templates",
    "start": "1081960",
    "end": "1087210"
  },
  {
    "text": "and they will have those\nthings and this picture is from Wikipedia. So you can also just look there. Please. So technically,\nare we always going",
    "start": "1087210",
    "end": "1092929"
  },
  {
    "text": "to assume population\nstructure in this class? We will do a lot with\npopulation statistics. We will not do\nthings with sampling",
    "start": "1092929",
    "end": "1098570"
  },
  {
    "text": "until a couple of key points. And the difference\nbetween those two will be immaterial when\nwe do the actual solves.",
    "start": "1098570",
    "end": "1104408"
  },
  {
    "text": "But it's a great question. Yeah. And if that doesn't make\nsense to you, don't worry.",
    "start": "1104409",
    "end": "1110059"
  },
  {
    "text": "Awesome. Is mu 0 there? Yeah. So in our setting before\nwhen we had epsilon i,",
    "start": "1110059",
    "end": "1116730"
  },
  {
    "text": "that's exactly right. I should have written this. For us, great call. This should be 0\nfor our epsilon i. In general, this is the\nnotation, so mu is equal to 0.",
    "start": "1116730",
    "end": "1124400"
  },
  {
    "text": "Mu equals 0. Great point. Awesome. All right.",
    "start": "1124400",
    "end": "1131200"
  },
  {
    "text": "Another thing is, this is\nthe function right here that we're looking at. Now, when you look\nat this, you start",
    "start": "1131200",
    "end": "1136408"
  },
  {
    "text": "to see why least\nsquares may come up. There's this quadratic\nlooking thing in here.",
    "start": "1136409",
    "end": "1141980"
  },
  {
    "text": "OK? And there's a 1/2 and there's\na sigma squared and whatever. This is the\nnormalizing constant.",
    "start": "1141980",
    "end": "1147510"
  },
  {
    "text": "That's just if I integrate the\nentire area under this curve, this function. That just makes\nsure that it's 1.",
    "start": "1147510",
    "end": "1152549"
  },
  {
    "text": "That's a PDF. That's all that thing is. You'll see it a bunch of times\nin various different guises here.",
    "start": "1152549",
    "end": "1157990"
  },
  {
    "text": "OK? But this is the function. OK? Now, let me unpack\nthis notation for you. You may not have\nseen this before.",
    "start": "1157990",
    "end": "1163580"
  },
  {
    "text": "This is not conditional. We'll come back to this. I'll hammer on this\na couple of times. This is the probability\ndistribution density of z",
    "start": "1163580",
    "end": "1170100"
  },
  {
    "text": "and then the semicolon\nis not conditional. Formally, it means these are the\nparameters of the distribution. You don't condition\non the parameters.",
    "start": "1170100",
    "end": "1176190"
  },
  {
    "text": "This may be a\nlittle bit pedantic, but we will stick to\nthis in the class. You have mu and sigma squared. Those are like things\nyou plug in, right?",
    "start": "1176190",
    "end": "1182620"
  },
  {
    "text": "So why does this matter? When we reason about\nthe normal distribution, we're going to have these\nparameters, the mean",
    "start": "1182620",
    "end": "1188470"
  },
  {
    "text": "and the sigma squared. We just plug them in and then\nit gives us a distribution. And that's going to\ncome back in one second",
    "start": "1188470",
    "end": "1194049"
  },
  {
    "text": "when we get to what's called\na likelihood function. So these are our parameters. Let me write that down here. These are our parameters.",
    "start": "1194049",
    "end": "1202100"
  },
  {
    "text": "Is the notation clear? If you're familiar\nwith conditioning, this is not conditioning. You can still condition.",
    "start": "1202100",
    "end": "1207980"
  },
  {
    "text": "You can write it\nin a bar, and we'll do that in one or two steps. You should be familiar with\nconditional probability for this class.",
    "start": "1207980",
    "end": "1213100"
  },
  {
    "text": "Not the most advanced\nversions of it, but basically what it does. OK.",
    "start": "1213100",
    "end": "1218450"
  },
  {
    "text": "Awesome. All right. So now, let's write\nsomething that's conditional.",
    "start": "1218450",
    "end": "1225190"
  },
  {
    "start": "1220000",
    "end": "1717000"
  },
  {
    "text": "So what is the probability\nof yi given xi?",
    "start": "1225190",
    "end": "1231389"
  },
  {
    "text": "And as we said, I'll\nwrite here theta, I'll write here theta, which\nis our underlying parameter.",
    "start": "1231390",
    "end": "1238760"
  },
  {
    "text": "Well, it's going to equal what?",
    "start": "1238760",
    "end": "1245080"
  },
  {
    "text": "that's the normalizing bit. I'm going to move this down. Sorry.",
    "start": "1245080",
    "end": "1252490"
  },
  {
    "text": "X of, and then here, it's\nyi minus theta xi squared",
    "start": "1252490",
    "end": "1260539"
  },
  {
    "text": "over 2 sigma squared. OK. So far so good.",
    "start": "1260539",
    "end": "1266059"
  },
  {
    "text": "So this is the\nprobability distribution. What does this say? It says, given that\nI saw xi, I saw",
    "start": "1266059",
    "end": "1272408"
  },
  {
    "text": "the feature, what\nis the probability distribution over the yi? What value should I expect,\nOK, to come out of this, right?",
    "start": "1272409",
    "end": "1280759"
  },
  {
    "text": "That, I have this theta model,\nwhich is our parameter here, OK, so our parameter.",
    "start": "1280760",
    "end": "1288929"
  },
  {
    "text": "OK? You put sigma squared\nin there, too. Now, we'll write this\nin a more compact form.",
    "start": "1288930",
    "end": "1298250"
  },
  {
    "text": "xi, this is the bar. So this is now the\nconditional probability.",
    "start": "1298250",
    "end": "1303740"
  },
  {
    "text": "It says, given that I saw\nxi, a condition on all the probability\ndistributions under theta, this is the probability\ndistribution over yi,",
    "start": "1303740",
    "end": "1310230"
  },
  {
    "text": "and I'll often write that this\nconditional distribution is n of theta tx i sigma squared.",
    "start": "1310230",
    "end": "1320429"
  },
  {
    "text": "So this mouthful is the\nsame as this mouthful. Does that make sense? This is just notation\nat this point.",
    "start": "1320429",
    "end": "1325780"
  },
  {
    "text": "And hopefully, the fact that\nit's a generative model kind of adds up. Go ahead. OK.",
    "start": "1325780",
    "end": "1331200"
  },
  {
    "text": "What will [INAUDIBLE] the\ndistribution of the [INAUDIBLE] or the distribution of y? Yeah. So great question.",
    "start": "1331200",
    "end": "1336590"
  },
  {
    "text": "So here, what I'm saying is,\nI'm basically asserting by Fiat that because the\nonly random variable is epsilon i, that xi here--",
    "start": "1336590",
    "end": "1343789"
  },
  {
    "text": "so this really\nis, the difference between these characters right\nhere is epsilon i, right?",
    "start": "1343790",
    "end": "1348980"
  },
  {
    "text": "That was by Fiat in the model\nwhen I did it earlier, right? Oops, sorry to scroll. I really wish it didn't look\nso nauseating, but it does.",
    "start": "1348980",
    "end": "1356419"
  },
  {
    "text": "So because of this model,\nI could substitute in here. So this value here is nothing\nmore than epsilon super i",
    "start": "1356419",
    "end": "1362679"
  },
  {
    "text": "in just a different guise,\nwhich tells me all these pieces. But it has to be conditioned\non xi because I saw that.",
    "start": "1362679",
    "end": "1368960"
  },
  {
    "text": "I saw that variable when\nI had to add it in, right? So that's how I get a\ndistribution over yi.",
    "start": "1368960",
    "end": "1375299"
  },
  {
    "text": "Wonderful questions. Are there more? Yeah. Oh yeah, so someone\nasked on the thing,",
    "start": "1375300",
    "end": "1381520"
  },
  {
    "text": "is ei the product of the two? And the answer is yes. Yeah, so thanks\nfor the question.",
    "start": "1381520",
    "end": "1387950"
  },
  {
    "text": "Someone is asking if this\nis the product up here? And yes, this is the product. These are just multiplied\nby one another.",
    "start": "1387950",
    "end": "1393908"
  },
  {
    "text": "There's no hidden\noperation there. Yeah. Sorry, the graph paper makes\nsure that I write in a line,",
    "start": "1393909",
    "end": "1399100"
  },
  {
    "text": "otherwise like I'll end\nup writing all crazy, but I can understand it's\nnot awesome for rendering.",
    "start": "1399100",
    "end": "1404520"
  },
  {
    "text": "Other questions? OK.",
    "start": "1404520",
    "end": "1410300"
  },
  {
    "text": "So why did we do all this? Maybe I just like torturing\nyou with notation. The truth is I really\ndon't like notation,",
    "start": "1410300",
    "end": "1416100"
  },
  {
    "text": "but we're going to use this\nin several different ways. And so hopefully right now,\nyou can piece it together and say like, OK,\nI kind of could",
    "start": "1416100",
    "end": "1421480"
  },
  {
    "text": "see a model underneath here. And what we're\ngoing to do is we're going to try and justify\nthe optimization that we",
    "start": "1421480",
    "end": "1426850"
  },
  {
    "text": "did for least squares by picking\nthe most likely parameter. So let me explain what we mean.",
    "start": "1426850",
    "end": "1433039"
  },
  {
    "text": "So before I do that,\nnotice here one fact that I've hidden from\nyou a little bit.",
    "start": "1433039",
    "end": "1438390"
  },
  {
    "text": "Picking theta picks\na distribution. Let me make sure that claim\nis clear before I move on.",
    "start": "1438390",
    "end": "1444929"
  },
  {
    "text": "OK. What do I mean? Once you tell me theta\nand I have the data fixed,",
    "start": "1444929",
    "end": "1450789"
  },
  {
    "text": "then all the distribution\nover the yi's are fixed. Does that make sense? So in some sense,\nby picking a theta,",
    "start": "1450789",
    "end": "1458039"
  },
  {
    "text": "once I have the\nsigma squared fixed, I'm picking a distribution now\nover what the yi should be.",
    "start": "1458039",
    "end": "1463690"
  },
  {
    "text": "And that's going to be\ninteresting because what it means is as you\npick a different theta, I can compare how well does\nit line up with my data.",
    "start": "1463690",
    "end": "1469780"
  },
  {
    "text": "So intuitively, right,\nif I pick a theta, all my data lies\non a line, and I pick the theta that\nexactly fits the line, that",
    "start": "1469780",
    "end": "1476630"
  },
  {
    "text": "should be much more\nlikely than if I pick a different theta,\nwhere it's scattered, my predictions are scattered\nall over the place.",
    "start": "1476630",
    "end": "1482679"
  },
  {
    "text": "And they're really far away,\nso at the thin parts of this. So let's come down and\nwrite that a little bit more",
    "start": "1482679",
    "end": "1487919"
  },
  {
    "text": "precisely. But that's the intuition\nof what's going on here. So ask me a question about that.",
    "start": "1487919",
    "end": "1493830"
  },
  {
    "text": "So for this, we\nneed a notion which will be very much used\nin this class, which is the notion of likelihoods.",
    "start": "1493830",
    "end": "1499710"
  },
  {
    "text": "And this allows us to pick\namong many distributions. OK? So at first, that\nsounds pretty fancy,",
    "start": "1499710",
    "end": "1506080"
  },
  {
    "text": "like how are we going to pick\namong these distributions. It's a huge unmeasurable\nclass, if you know what that is, all this nasty stuff.",
    "start": "1506080",
    "end": "1511640"
  },
  {
    "text": "But we just have to\npick in our situation among the different thetas\nthat could fit our data.",
    "start": "1511640",
    "end": "1516950"
  },
  {
    "text": "OK? And we're going to pick the\none that is most likely.",
    "start": "1516950",
    "end": "1522570"
  },
  {
    "text": "So let's write that\ndown right now. So what is the\nlikelihood of theta?",
    "start": "1522570",
    "end": "1528730"
  },
  {
    "text": "It's going to be the\nprobability of all the y's given all the x's given\ntheta or with input theta.",
    "start": "1528730",
    "end": "1541549"
  },
  {
    "text": "This just says how\nlikely the data is. And clearly, as\nI vary theta, I'm going to get\ndifferent scores here for how probabilistically\nlikely all the y's are.",
    "start": "1541549",
    "end": "1548700"
  },
  {
    "text": "Let's break it apart. If it doesn't make perfect\nsense what that statement means, when I start to write\nit out mechanically, hopefully, you'll see\nhow it decomposes,",
    "start": "1548700",
    "end": "1554820"
  },
  {
    "text": "and then please\nask me a question. Now, I'm going to\nwrite something which at first\nmay look, actually right here, a bit unmotivated.",
    "start": "1554820",
    "end": "1564029"
  },
  {
    "text": "I can break this down into many\nsmaller assumptions or many smaller pieces.",
    "start": "1564029",
    "end": "1572600"
  },
  {
    "text": "Why is this the case? Why can I take the\nbig thing and turn it into a product of\nthe small things?",
    "start": "1572600",
    "end": "1578450"
  },
  {
    "text": "What am I using? Independence. Exactly. I'm using independence and the\nstrong form of independence, which I kept bringing up.",
    "start": "1578450",
    "end": "1584009"
  },
  {
    "text": "That told me that I could\nwrite this big product over all the vectors as this\nproduct among all of them.",
    "start": "1584010",
    "end": "1590158"
  },
  {
    "text": "And sometimes you'll\nhear this referred to as the iid assumption,\nindependent and identically",
    "start": "1590159",
    "end": "1595450"
  },
  {
    "text": "distributed. All right. Cool. Please. [INAUDIBLE]",
    "start": "1595450",
    "end": "1601080"
  },
  {
    "text": "Yeah. So theta, there should be\na 0 and a sigma squared.",
    "start": "1601080",
    "end": "1609549"
  },
  {
    "text": "I'm being a little bit glib\nabout what happens with the-- I could imagine a model--\nthis is a wonderful question. Thank you for\nletting me say this.",
    "start": "1609550",
    "end": "1615100"
  },
  {
    "text": "I could imagine a model,\nwhere because of the way I specified the model, it's\nimplicit that mu is always 0. But I haven't told you\nwhat sigma squared is.",
    "start": "1615100",
    "end": "1621799"
  },
  {
    "text": "For right now, imagine that\nsigma squared is fixed. I told it to you\nahead of time, so I don't have to plug it in here.",
    "start": "1621799",
    "end": "1627140"
  },
  {
    "text": "I could also fit it, right? I could look at my data and\nsee among all the thetas that are there and all\nthe noise levels what's",
    "start": "1627140",
    "end": "1633080"
  },
  {
    "text": "the most likely one. And that's actually a\nslightly different model. But here, I'm imagining\nthat sigma squared is fixed, but it should go\nunder this rubric.",
    "start": "1633080",
    "end": "1639620"
  },
  {
    "text": "And you're like, why are you\nbeing so sloppy about that? And the reason is\nbecause later, we're going to be much\nsloppier about it because we're going to introduce\nnotation that says it's all",
    "start": "1639620",
    "end": "1646030"
  },
  {
    "text": "the parameters in the problem. But wonderful question. You're exactly on\ntarget for this.",
    "start": "1646030",
    "end": "1652740"
  },
  {
    "text": "Oh, please. [INAUDIBLE] No, no.",
    "start": "1652740",
    "end": "1659690"
  },
  {
    "text": "So here, we have\nthis probability. But because it's\nconditioned on x, we've removed all the\ndependence on the data,",
    "start": "1659690",
    "end": "1664710"
  },
  {
    "text": "so everyone gets to\nsee all the data. And now all that's left that's\nunspecified in the model is the epsilon i's.",
    "start": "1664710",
    "end": "1670049"
  },
  {
    "text": "If they were all 0's, you'd be\nable to get the yi's exactly. But the only randomness\nthat's left is that epsilon i.",
    "start": "1670049",
    "end": "1675320"
  },
  {
    "text": "That's what we're\ndoing and that's what we kind of cheated on here\nwhen we said this guy is really epsilon i as well.",
    "start": "1675320",
    "end": "1681130"
  },
  {
    "text": "Wonderful questions. You folks are really\non top of this. Any other questions?",
    "start": "1681130",
    "end": "1687620"
  },
  {
    "text": "Please. Yeah. Can you [INAUDIBLE]?",
    "start": "1687620",
    "end": "1692679"
  },
  {
    "text": "No, no. So there's really\nnot much to say here.",
    "start": "1692679",
    "end": "1701240"
  },
  {
    "text": "All that there is we went\nthrough this model, where we said yi's are of this form. Now that we've conditioned\non xi, we know this,",
    "start": "1701240",
    "end": "1708330"
  },
  {
    "text": "and we're plugging in theta. So you're giving me a\nparticular theta to evaluate. And now epsilon i is\na random variable.",
    "start": "1708330",
    "end": "1714860"
  },
  {
    "text": "It has yet to be determined. So there's a\ndistribution over that. The distribution of epsilon\ni is given by this equation",
    "start": "1714860",
    "end": "1720480"
  },
  {
    "text": "because this is exactly\nequal to epsilon i. And so this now\ngives me and say, I don't know what epsilon\nis, but it has a distribution",
    "start": "1720480",
    "end": "1725898"
  },
  {
    "text": "that looks like this. So if I sampled it,\nthat's a weird statement. I want to be clear, that's\na really weird statement. It means that if I\npicked enough epsilon",
    "start": "1725899",
    "end": "1732269"
  },
  {
    "text": "i's, I'd expect its\nmean to be here, whatever mu was, in this case 0,\nand I'd expect the scatterplot",
    "start": "1732269",
    "end": "1738398"
  },
  {
    "text": "to look like it was inside\nhere or actually the histogram to look like this, if I binned\nhow many were in each thing",
    "start": "1738399",
    "end": "1745039"
  },
  {
    "text": "and eventually converging\nto this distribution. That's exactly what I mean. Awesome questions. These are great.",
    "start": "1745039",
    "end": "1750779"
  },
  {
    "text": "Oops. All right. Oh, man, I even had it\ndown here written nicely.",
    "start": "1750779",
    "end": "1756950"
  },
  {
    "text": "Sorry. We'll go back to\nmy messy version. So here we've gone from\nthis piece to this piece.",
    "start": "1756950",
    "end": "1763580"
  },
  {
    "text": "And then here, all I'm\ndoing is because these are the epsilon\ni's, which I we've assumed independence\nin the strongest way, I can move to a product.",
    "start": "1763580",
    "end": "1770649"
  },
  {
    "text": "We will do this\nthroughout the course. A lot of machine\nlearning is based on iid because it's an OK assumption.",
    "start": "1770649",
    "end": "1776110"
  },
  {
    "text": "Does that mean the errors\nare not correlated? No, no. Of course, they're correlated,\nbut we're not modeling it. That's all it means.",
    "start": "1776110",
    "end": "1782039"
  },
  {
    "text": "It's not true. It's just a good model. OK, great. Now, I substitute in more thing.",
    "start": "1782039",
    "end": "1789450"
  },
  {
    "text": "Equals product i 1\nto n, and then I'm just going to write out the\ndistribution sigma 2pi times",
    "start": "1789450",
    "end": "1801149"
  },
  {
    "text": "x of-- so y, oh sorry. I maybe forget the minus\nsign, yi minus theta xi",
    "start": "1801149",
    "end": "1812880"
  },
  {
    "text": "squared over 2 sigma squared. Why did I do this? All right.",
    "start": "1812880",
    "end": "1818309"
  },
  {
    "text": "So I just wrote this\nwhole thing out. Whoops. I just wrote this\nwhole thing out.",
    "start": "1818309",
    "end": "1823389"
  },
  {
    "text": "Now, the reason is we don't\nuse this, so minimizing this seems like a nightmare.",
    "start": "1823390",
    "end": "1829260"
  },
  {
    "text": "And so what we do instead is\nwe use a simple transformation of this, which will make\nit nice and additive,",
    "start": "1829260",
    "end": "1834490"
  },
  {
    "text": "which is called\nthe log likelihood. All right.",
    "start": "1834490",
    "end": "1839820"
  },
  {
    "text": "Now, I want to make\nsure of one thing. Let me go back. There should be a minus here. I messed that up.",
    "start": "1839820",
    "end": "1845610"
  },
  {
    "text": "This has to be a\npositive square, otherwise it will\nspiral off to infinity. I'm sorry, it's in\nmy lizard brain.",
    "start": "1845610",
    "end": "1851340"
  },
  {
    "text": "I messed that up. Clear enough? Epsilon here. All right. Otherwise, it's the wrong sheet.",
    "start": "1851340",
    "end": "1857340"
  },
  {
    "text": "Yeah, please? Two questions. One, is there a negative in\nthe original formula also?",
    "start": "1857340",
    "end": "1865138"
  },
  {
    "text": "Yes. It's here. What about the one above it? Oh, yes. I made a mistake.",
    "start": "1865139",
    "end": "1871529"
  },
  {
    "text": "Not in the notes. Secondly, what does ex mean?",
    "start": "1871529",
    "end": "1878370"
  },
  {
    "text": "Oh, exponential function. So it just means\nthis character here. Sorry about this, x of x.",
    "start": "1878370",
    "end": "1884630"
  },
  {
    "text": "It may be more familiar\nto you as e to the x. That's the e to the power of x.",
    "start": "1884630",
    "end": "1890970"
  },
  {
    "text": "And x is everything\nin the bracket? Everything in the bracket. Yes. But I thought the original\nfunction was appropriate",
    "start": "1890970",
    "end": "1896888"
  },
  {
    "text": "because effectively,\nwe're competing in a negative [INAUDIBLE]\nby doing the real minus the predicted rather.",
    "start": "1896889",
    "end": "1902650"
  },
  {
    "text": "No, it's squared. So this character's squared. Oh, yeah. Great point. It's my mistake.",
    "start": "1902650",
    "end": "1909120"
  },
  {
    "text": "Awesome. Is that clear? I want make sure that's clear. It's a small detail, and\nit is in the notes maybe.",
    "start": "1909120",
    "end": "1915450"
  },
  {
    "text": "I don't know. All right. OK. So far so good.",
    "start": "1915450",
    "end": "1922660"
  },
  {
    "text": "Wonderful. So we have this function\nwith all of my bugs",
    "start": "1922660",
    "end": "1928750"
  },
  {
    "text": "that I've introduced\nthat we're catching on the fly, which is awesome. And we're going to\nintroduce a new function. It's going to be the\nlog of our old function.",
    "start": "1928750",
    "end": "1935120"
  },
  {
    "text": "Say, why are you doing that? And the reason is what does\nlog do for x functions? Well, it brings the contents\ndown, which is nice.",
    "start": "1935120",
    "end": "1943960"
  },
  {
    "text": "And it also separates out\nthings that are product. So we have a big\nproduct, we take a log, we turn into a big sum.",
    "start": "1943960",
    "end": "1949440"
  },
  {
    "text": "That some looks\na little bit more like what we were\nexpecting intuitively from our least squares.",
    "start": "1949440",
    "end": "1957039"
  },
  {
    "text": "Let me write it here. So it's sum i equals 1 to n\nbecause we turn this product",
    "start": "1957039",
    "end": "1962680"
  },
  {
    "text": "into a sum 1 over sigma",
    "start": "1962680",
    "end": "1971110"
  },
  {
    "text": "xi squared over 2 sigma.",
    "start": "1971110",
    "end": "1978080"
  },
  {
    "text": "OK. I just took the log\nof the exponential. Please. [INAUDIBLE] log over sigma--",
    "start": "1978080",
    "end": "1986250"
  },
  {
    "text": "Oh, log sigma. Excellent point. Let me move this.",
    "start": "1986250",
    "end": "1993650"
  },
  {
    "text": "Oops. This should be logs again. You can see where there\nare typos and things I don't care about.",
    "start": "1993650",
    "end": "1999120"
  },
  {
    "text": "That term's going to disappear\nin a second, but awesome find. Yeah. OK, cool.",
    "start": "1999120",
    "end": "2006120"
  },
  {
    "text": "So what do I care about here? The thing I was\njust about to say is, this term doesn't depend\nin any way on theta, right?",
    "start": "2006120",
    "end": "2015990"
  },
  {
    "text": "And so remember when we talked\nabout minimizing the loss function, we're like, oh,\nif I added a constant,",
    "start": "2015990",
    "end": "2021019"
  },
  {
    "text": "it didn't matter. And this is a\nconstant, the sigma squared that doesn't\ndepend on my data anyway. So I can just toss it away.",
    "start": "2021019",
    "end": "2028080"
  },
  {
    "text": "In contrast, this\nthing very much does depend on my\ndata and theta. On data and theta.",
    "start": "2028080",
    "end": "2037100"
  },
  {
    "text": "Is that clear? Please. Do you have a\nsummation [INAUDIBLE]??",
    "start": "2037100",
    "end": "2044860"
  },
  {
    "text": "So think about it like this. I'm sorry.",
    "start": "2044860",
    "end": "2051559"
  },
  {
    "text": "Awesome questions. Yeah, wonderful. So now, what does that mean?",
    "start": "2051560",
    "end": "2059830"
  },
  {
    "text": "If I want to find the\nmost likely function, that corresponds to doing what?",
    "start": "2059830",
    "end": "2065040"
  },
  {
    "text": "I claim it corresponds to\nmax over theta of theta. Why is that the case?",
    "start": "2065040",
    "end": "2072190"
  },
  {
    "text": "Well, log is a monotone\ntransformation, right? So this original thing, I wanted\nto maximize the probability,",
    "start": "2072190",
    "end": "2078128"
  },
  {
    "text": "log is monotone. Looks like this\nand all the rest. And so log of theta is the\nsame as maximizing that.",
    "start": "2078129",
    "end": "2084790"
  },
  {
    "text": "Then this term is\njust a constant we talked about how that\ndoesn't really matter too much. So I can drop that term.",
    "start": "2084790",
    "end": "2090908"
  },
  {
    "text": "And then I have a\nminus here, so it's the same as minimizing over\ntheta 1 over 2 sum i 1 to n",
    "start": "2090909",
    "end": "2103481"
  },
  {
    "text": "yi minus theta xi squared.",
    "start": "2103481",
    "end": "2110900"
  },
  {
    "text": "And then you say, well, what\nabout that sigma squared, what happened to it? Well, as we talked\nabout last time, it doesn't matter if you\nscale the loss by a constant.",
    "start": "2110900",
    "end": "2118349"
  },
  {
    "text": "It's still the same minimizer,\nwe don't care about the value. And what is this character? That's least squares.",
    "start": "2118349",
    "end": "2124559"
  },
  {
    "text": "And we call this thing j theta. This was j theta in\nthe last lecture.",
    "start": "2124560",
    "end": "2129800"
  },
  {
    "text": "OK. So what was important here? I walked through\nthis fairly slowly.",
    "start": "2129800",
    "end": "2135410"
  },
  {
    "text": "And the reason I wanted to walk\nthrough it like this and you should run through this is\nbecause we're going to run this same playbook again and again,\nwe're going to talk about what",
    "start": "2135410",
    "end": "2142770"
  },
  {
    "text": "the error is for\nthese linear models, then we're going to try and\nreduce them to this likelihood",
    "start": "2142770",
    "end": "2148599"
  },
  {
    "text": "computation-- whoops-- this likelihood computation. We'll almost always use\nthe log because it turns it",
    "start": "2148599",
    "end": "2155420"
  },
  {
    "text": "into an additive problem. And remember stochastic\ngradient descent likes to work on\nadditive problems.",
    "start": "2155420",
    "end": "2160480"
  },
  {
    "text": "This is of a nice form\nthat we like to deal with and then we solve the\nunderlying equation. And so there'll be\nkind of this mapping",
    "start": "2160480",
    "end": "2166089"
  },
  {
    "text": "that I give you a distribution\nand then out comes a loss function. And that's going to be nearly\nautomatic after this lecture",
    "start": "2166089",
    "end": "2171790"
  },
  {
    "text": "and the next lecture to be\nable to do that for a pretty wide class of models. And then how do we solve them? It turns out we're going to\nsolve them all the same way.",
    "start": "2171790",
    "end": "2179680"
  },
  {
    "text": "Awesome. OK. So is that clear? Is the probabilistic\ninterpretation",
    "start": "2179680",
    "end": "2186750"
  },
  {
    "text": "of least squares or\nfitting a line clear? Please just go ahead.",
    "start": "2186750",
    "end": "2195839"
  },
  {
    "text": "[INAUDIBLE] Yeah, great question. So if you had\nsigma squared here,",
    "start": "2195839",
    "end": "2201819"
  },
  {
    "text": "I'm not going to show you\nhow to fit this right now. But there's another\nmodel where you have it. This is called with\nknown variance.",
    "start": "2201819",
    "end": "2207099"
  },
  {
    "text": "This is what I called\nfixed design with known variance linear regression. If you also don't know\nthe sigma squared,",
    "start": "2207100",
    "end": "2212740"
  },
  {
    "text": "you have to learn that, too. And there's a parameter. Sigma comes out. It doesn't come out quite\nnicely to the least squared",
    "start": "2212740",
    "end": "2218789"
  },
  {
    "text": "formulation. You have to do a little bit of\nextra work to estimate sigma, but you can do it. And I think it may be\na homework problem.",
    "start": "2218790",
    "end": "2224588"
  },
  {
    "text": "So I'm not going to tell\nyou too much more about it. But it's not it's\nnot complicated. Yeah. It shouldn't be complicated.",
    "start": "2224589",
    "end": "2229660"
  },
  {
    "text": "But great question. For now, we're assuming\nsigma squared is given. You do not need to\nmake that assumption. Please.",
    "start": "2229660",
    "end": "2235320"
  },
  {
    "text": "Oh, sorry. I'm all set. Either way. All right. All good. All right.",
    "start": "2235320",
    "end": "2241320"
  },
  {
    "text": "So at this point, we've gone\nthrough that interpretation. Let me make sure if\nthere's anything else. Fantastic.",
    "start": "2241320",
    "end": "2247070"
  },
  {
    "text": "All right. Let's talk about classification. A little primer\non classification.",
    "start": "2247070",
    "end": "2252319"
  },
  {
    "start": "2248000",
    "end": "2403000"
  },
  {
    "text": "This is where we are. We're going to talk about\nhow classification works, why regression isn't the thing\nthat we would necessarily",
    "start": "2252319",
    "end": "2257869"
  },
  {
    "text": "want to do in this\nscenario, and then we're going to run the same\nplaybook I assert to be able to solve the model.",
    "start": "2257870",
    "end": "2265700"
  },
  {
    "text": "That is estimate the theta\nunderneath the covers. All right. So here what is classification? What are we given?",
    "start": "2265700",
    "end": "2271069"
  },
  {
    "text": "We're given, not surprisingly,\nxi's and yi's, no change so far.",
    "start": "2271069",
    "end": "2277790"
  },
  {
    "text": "For i equal 1 to n. OK? But yi, we're going to work on\nbinary classification in 0, 1.",
    "start": "2277790",
    "end": "2287880"
  },
  {
    "text": "And the values of 0 and You could have minus 1 and 1. I actually prefer\nthat because it makes some of the math\na little bit nicer,",
    "start": "2287880",
    "end": "2293910"
  },
  {
    "text": "but it's not what we're\ndoing in the course. You could have just\ncategorical values. They're discrete encodings\nof the variables.",
    "start": "2293910",
    "end": "2300319"
  },
  {
    "text": "OK? Now, we often think\nin terminology, why I also like the minus 1. We call this often\nthe negative class.",
    "start": "2300320",
    "end": "2308260"
  },
  {
    "text": "This is just convention. There's no intrinsic\nmeaning to these things. This is our model and this\nis the positive class.",
    "start": "2308260",
    "end": "2319170"
  },
  {
    "text": "So a positive class could\nbe we found the tumor. There is a tumor in this\nimage versus a negative class",
    "start": "2319170",
    "end": "2326180"
  },
  {
    "text": "is there's no tumor, or this\nis a cat, this is not a cat. We're doing binary right now.",
    "start": "2326180",
    "end": "2331210"
  },
  {
    "text": "You can do multiclass,\nwhich we'll come to later, which is there's\na cat, a dog, a pig, a horse. Right now, we're just doing two.",
    "start": "2331210",
    "end": "2338990"
  },
  {
    "text": "Great. So you look at this\ndata and you're like, oh OK, I plotted it. You told me there's 0,",
    "start": "2338990",
    "end": "2346010"
  },
  {
    "text": "so we can use linear\nalgebra and vector techniques and all the rest. So 0, 1, here are some. And you're like, oh, why\ndon't you just fit a line.",
    "start": "2346010",
    "end": "2353390"
  },
  {
    "text": "I should just fit a line, maybe\nthe line kind of, I don't know, goes through here or\nsomething like this, and it's fine, right?",
    "start": "2353390",
    "end": "2360070"
  },
  {
    "text": "And indeed for a\nlot of problems, if you run linear regression\nand just kind of say like, is it closer to 1 than",
    "start": "2360070",
    "end": "2367010"
  },
  {
    "text": "you can get out a classifier. But it kind of feels a\nlittle bit weird, especially because your data,\nthere's no reason",
    "start": "2367010",
    "end": "2372360"
  },
  {
    "text": "it should be nicely clustered. What if there were a blue\npoint all the way over here, or over here, or way over here?",
    "start": "2372360",
    "end": "2378980"
  },
  {
    "text": "What's going to\nhappen to your line? Well, it's going to\nstart because it's fitting those residuals to\ngo crazier and crazier if you",
    "start": "2378980",
    "end": "2384619"
  },
  {
    "text": "like in this direction, right? Naturally, it's going\nto skew more and more towards more of the data.",
    "start": "2384620",
    "end": "2390240"
  },
  {
    "text": "And so whatever decision\nboundary you kind of put there, you're going to get\ninto kind of stranger",
    "start": "2390240",
    "end": "2395270"
  },
  {
    "text": "and stranger situations. Now, that's just a\nmotivation for why you want to treat something\nthat's natively categorical.",
    "start": "2395270",
    "end": "2401880"
  },
  {
    "text": "So let's go through\nthe function here. And maybe you've seen this\nin a stats course already.",
    "start": "2401880",
    "end": "2407869"
  },
  {
    "start": "2403000",
    "end": "2925000"
  },
  {
    "text": "This is logistic regression. So we're going to do one trick\nhere over linear regression. Our hypothesis is going\nto generate something",
    "start": "2407869",
    "end": "2415730"
  },
  {
    "text": "of x is going to live in 0, 1. So this is a graph\nhere of 0, 1 that we're",
    "start": "2415730",
    "end": "2423120"
  },
  {
    "text": "going to get to in one second. An h of theta of x is\ngoing to be written",
    "start": "2423120",
    "end": "2428700"
  },
  {
    "text": "as g of theta tx, where\nwhich will equal 1 plus e",
    "start": "2428700",
    "end": "2438910"
  },
  {
    "text": "to the minus theta tx over 1. OK?",
    "start": "2438910",
    "end": "2443970"
  },
  {
    "text": "Now this function here g of z\nequals 1 over 1 plus e of z.",
    "start": "2443970",
    "end": "2450890"
  },
  {
    "text": "This is called a link function. Sometimes it's also called\nan inverse link function.",
    "start": "2450890",
    "end": "2458880"
  },
  {
    "text": "The literature goes\nback and forth. Doesn't really matter. So you say, why did you do this? Well, our model is still going\nto be linear in our features,",
    "start": "2458880",
    "end": "2465329"
  },
  {
    "text": "but we're going to feed it\nthrough this nonlinearity. And that nonlinearity is all\nof a sudden going to make sure that it kind of\nsaturates when it",
    "start": "2465330",
    "end": "2470869"
  },
  {
    "text": "gets too big and saturates\nwhen it gets too small. So it's not going to\nhave the behavior if we looked at our old data, right?",
    "start": "2470870",
    "end": "2477210"
  },
  {
    "text": "Let's go back up\nto our old data. It's going to kind of\nhave a function that looks more like this. Does that make sense?",
    "start": "2477210",
    "end": "2483670"
  },
  {
    "text": "But at least at a high level. OK? So that's the intuition. OK/ And this function\nhere has a special name.",
    "start": "2483670",
    "end": "2490599"
  },
  {
    "text": "It's the sigmoid. So you may see that. If you use modern deep\nlearning packages, you'll see sigmoids or\nthings floating around.",
    "start": "2490600",
    "end": "2497230"
  },
  {
    "text": "That's what they are. They're just this function\nthat smooths it over. Now, you may ask, why don't I\nuse a different link function?",
    "start": "2497230",
    "end": "2502750"
  },
  {
    "text": "You could. There are lots of different\nlink functions to use. This is by far the most popular\nfor a variety of reasons.",
    "start": "2502750",
    "end": "2508910"
  },
  {
    "text": "One is that you can\nturn it into kind of what they call probabilistic\nestimates, which we'll get to a little bit later.",
    "start": "2508910",
    "end": "2514009"
  },
  {
    "text": "Please. What do you do when\nyou have [INAUDIBLE]?? Yeah, let's get through\na great question.",
    "start": "2514010",
    "end": "2521020"
  },
  {
    "text": "How do you do multiclass? Let's first get through how we\nactually do the binary class. It's a great question.",
    "start": "2521020",
    "end": "2526250"
  },
  {
    "text": "You can think about\na standard way to do multiclass is to do\nwhat's called one versus all, if it bothers you, where\nyou say, am I in class 1",
    "start": "2526250",
    "end": "2533760"
  },
  {
    "text": "or any other class, class and you can put them that way. There are more\nsophisticated schemes.",
    "start": "2533760",
    "end": "2538910"
  },
  {
    "text": "There's a wonderful\npaper from 2024 that talks about how those more\nsophisticated schemes don't",
    "start": "2538910",
    "end": "2545510"
  },
  {
    "text": "always pan out. And it's written in a very\naggressive style, which I find interesting\nand entertaining,",
    "start": "2545510",
    "end": "2550750"
  },
  {
    "text": "anyway, by a guy\nfrom the Media Lab. OK, cool. So at first, this looks\nweirdly motivated,",
    "start": "2550750",
    "end": "2558299"
  },
  {
    "text": "but there's some\nmotivation for it, which is just that it\nhas this nice property. And it's smooth, and it looks\nclose to a threshold function.",
    "start": "2558300",
    "end": "2566780"
  },
  {
    "text": "Now, the other thing\nwhen I say it's smooth is we could also imagine\nthe function that was like a step function,\nthat you could use that. That seems like a natural thing.",
    "start": "2566780",
    "end": "2572800"
  },
  {
    "text": "When you're below 0,\nthen when you're negative return 0, when you're\na positive return 1.",
    "start": "2572800",
    "end": "2580470"
  },
  {
    "text": "That would be a\nthing you could do. And deep learning these\nare sometimes called-- it's a sine function.",
    "start": "2580470",
    "end": "2587010"
  },
  {
    "text": "The problem is the derivatives\nwould give you no information here if they were flat. So this is smooth, so it tells\nyou a nice smooth transition,",
    "start": "2587010",
    "end": "2594579"
  },
  {
    "text": "and that will work better\nwith modern optimization. That's one thing\nwe want out of it. Please.",
    "start": "2594579",
    "end": "2599650"
  },
  {
    "text": "Can you please explain\nwhat h of x and g of z",
    "start": "2599650",
    "end": "2606359"
  },
  {
    "text": "are with this function thing. Recall this is the same\nnotation we had earlier. This is the\nhypothesis sub theta. This is how do we do prediction.",
    "start": "2606360",
    "end": "2611690"
  },
  {
    "text": "So the way I do\nprediction in this model, in the logistic\nregression model is, you've given me theta, which is\nsome parameters that you have.",
    "start": "2611690",
    "end": "2618859"
  },
  {
    "text": "That chooses your\nmodel, then you give me some x, which recall\nis your data point, and then what I'm\ngoing to do is I'm",
    "start": "2618860",
    "end": "2624300"
  },
  {
    "text": "going to produce a\nnumber between 0 and 1. And the way I'm going to produce\nit, is I'm going to run-- I'm going to take their dot\nproduct as I was doing before,",
    "start": "2624300",
    "end": "2631109"
  },
  {
    "text": "and then I'm going to run\nit through this function. And I'll come in one second\nhow we interpret those scores, but you can think about those\nscores as being closer to 1,",
    "start": "2631109",
    "end": "2638760"
  },
  {
    "text": "means I'm confident it's in\nthe class and closer to 0, means I'm not\nconfident in the class. And I was saying is, this\nfunction looks like it was",
    "start": "2638760",
    "end": "2644770"
  },
  {
    "text": "picked out of a hat,\nand it really wasn't. The reason it wasn't is, it\nhas a couple of properties. It's smooth and it transitions\nnicely between 0 and 1.",
    "start": "2644770",
    "end": "2652510"
  },
  {
    "text": "And I was trying to explain\nwhy those properties were important. And so that's where h\ntheta links to this image.",
    "start": "2652510",
    "end": "2658240"
  },
  {
    "text": "Does that make sense? Yeah. [INAUDIBLE] the g of\ntheta transpose times",
    "start": "2658240",
    "end": "2666180"
  },
  {
    "text": "x isn't actually equal to\nthe thing to the right of it in the parentheses?",
    "start": "2666180",
    "end": "2672119"
  },
  {
    "text": "It is. So if you look, g is\nthis function here, but z is a scalar, right?",
    "start": "2672119",
    "end": "2677950"
  },
  {
    "text": "And so it's just substituting\nin theta tx for z. Yeah, I just wrote it this\nway, so I'd have more room",
    "start": "2677950",
    "end": "2683280"
  },
  {
    "text": "to write the numerator. And then here I wrote it 1\nover because that's the more standard way to write it.",
    "start": "2683280",
    "end": "2688338"
  },
  {
    "text": "They're equivalent. Great questions, please. What's the difference between\nsigmoid and a weak function?",
    "start": "2688339",
    "end": "2695190"
  },
  {
    "text": "A link function is\na general class of g that you could apply,\nthat's some kind of nonlinearity, one that\nyou may have seen if you ever",
    "start": "2695190",
    "end": "2701480"
  },
  {
    "text": "played with a deep learning\npackage or something called ReLU or rectified linear. It looks like this.",
    "start": "2701480",
    "end": "2707640"
  },
  {
    "text": "So there are other functions\nthat they're out there. There's probits, and logits,\nand all kinds of things. We're going to use this one. But I want you to be\naware of it because I",
    "start": "2707640",
    "end": "2713900"
  },
  {
    "text": "think you have to try one other\nlink function on a homework. And the phrase is used\nin the literature, and it's very mysterious\nif you don't hear it first.",
    "start": "2713900",
    "end": "2721160"
  },
  {
    "text": "Yeah. Very good questions. And sometimes it's also called\nan inverse link function. That's a separate issue.",
    "start": "2721160",
    "end": "2727359"
  },
  {
    "text": "Cool. Awesome. All right. So how do we interpret\nthose scores? Now, this is the\ntwist that gets us",
    "start": "2727359",
    "end": "2734890"
  },
  {
    "text": "into probabilistic\nmodeling, which we're going to generalize. So we say the probability that y\nequals 1 according to the model",
    "start": "2734890",
    "end": "2745029"
  },
  {
    "text": "is equal to h theta of x. Now, this is a\ntestable statement.",
    "start": "2745030",
    "end": "2754740"
  },
  {
    "text": "Now just to complete\nit also, what's the probability y\nequals zero, this is going to be proportional to\nx theta of 1 minus h theta of xy",
    "start": "2754740",
    "end": "2762970"
  },
  {
    "text": "because probability's sum to 1. We only have two classes. Now, this is actually testable.",
    "start": "2762970",
    "end": "2769328"
  },
  {
    "text": "If you took a bunch of data\nand put it through and look at the probabilities\nand binned them,",
    "start": "2769329",
    "end": "2776318"
  },
  {
    "text": "right, so you took\nall the predictions that were between 0.5\nand 0.6 and 0.6 and 0.7, and you counted them\nup, and every bucket,",
    "start": "2776319",
    "end": "2782609"
  },
  {
    "text": "how many were accurate? This is testable. You can see if the model is\nwhat's called calibrated. And that's a very useful that\nthe errors are meaningful.",
    "start": "2782610",
    "end": "2790410"
  },
  {
    "text": "And so you can check that. It's more of a condition\nthan the right or wrong. Now in modern machine learning,\nthat's less important,",
    "start": "2790410",
    "end": "2796030"
  },
  {
    "text": "but you will hear people\ntalk about the probabilities or the scores that come\nout of these models and using those\nscores for something.",
    "start": "2796030",
    "end": "2802329"
  },
  {
    "text": "And this is what they mean. They'll sometimes\nuse the log of this, which is called the logit.",
    "start": "2802329",
    "end": "2807790"
  },
  {
    "text": "OK. But that's how we interpret\nwhat the model tells us. That's why this link\nfunction is important that it's between 0 and 1.",
    "start": "2807790",
    "end": "2816329"
  },
  {
    "text": "It doesn't damage optimization. It's not obvious, but it\ndoesn't damage optimization, which is the other major thing.",
    "start": "2816330",
    "end": "2821490"
  },
  {
    "text": "OK. So let's use that information to\nwrite our likelihood function.",
    "start": "2821490",
    "end": "2826839"
  },
  {
    "text": "The probability of y,\nI'll emphasize that it's a vector this time, x-- I don't really like that\nnotation, but it's OK--",
    "start": "2826839",
    "end": "2833710"
  },
  {
    "text": "is, well, why did we get here?",
    "start": "2833710",
    "end": "2839810"
  },
  {
    "text": "Oops. Well, this is again the\nindependence assumption rearing",
    "start": "2839810",
    "end": "2851599"
  },
  {
    "text": "its ugly or not head, right? We're able to go from the entire\ndata set to a product over all",
    "start": "2851599",
    "end": "2857020"
  },
  {
    "text": "the terms. Nothing surprising there. And then we'll write this\nin one form which hopefully",
    "start": "2857020",
    "end": "2863810"
  },
  {
    "text": "makes a little bit of sense. xi, this seems like a cheat,\nbut it's actually OK, 1 minus h",
    "start": "2863810",
    "end": "2873510"
  },
  {
    "text": "of theta. So why does this\nseem like a cheat? It's a weird way to do it.",
    "start": "2873510",
    "end": "2879560"
  },
  {
    "text": "But it'll become\nnice in a second. So what I'm writing\nhere is I'm saying the probability is\nthe probability I",
    "start": "2879560",
    "end": "2885529"
  },
  {
    "text": "said that it was true. Now, what is yi? When y is 1, I select this\nterm because this term is 0.",
    "start": "2885530",
    "end": "2893060"
  },
  {
    "text": "So think about when y is 1. This character is 1,\nthis character is 0. So this goes to 1, and this\nis the only term that matters.",
    "start": "2893060",
    "end": "2899660"
  },
  {
    "text": "When the true label is When it's 0, I should say\nwhen it's 0, this term is 1",
    "start": "2899660",
    "end": "2906770"
  },
  {
    "text": "and then this term goes away. Does that make sense? Just think through the\ncases y as 0 or y as 1.",
    "start": "2906770",
    "end": "2913130"
  },
  {
    "text": "So far so good? So it's like encoding\nboth simultaneously. I get to see yi, so really,\nonly one is present.",
    "start": "2913130",
    "end": "2920420"
  },
  {
    "text": "But it just makes my arithmetic\na little bit cleaner below. So that encoding makes sense? Cool. Great.",
    "start": "2920420",
    "end": "2926349"
  },
  {
    "start": "2925000",
    "end": "3402000"
  },
  {
    "text": "All right. So let's take the\nlog of l theta.",
    "start": "2926349",
    "end": "2931410"
  },
  {
    "text": "We're doing exactly\nthe same thing that we did before, 1 to n. Now, I'm going to\nwrite this out.",
    "start": "2931410",
    "end": "2938480"
  },
  {
    "text": "It's 1iy log h theta xi\nplus 1 minus yi log 1",
    "start": "2938480",
    "end": "2949970"
  },
  {
    "text": "minus h theta xi.",
    "start": "2949970",
    "end": "2957020"
  },
  {
    "text": "OK. So so far so good.",
    "start": "2957020",
    "end": "2962170"
  },
  {
    "text": "But now notice, this\nis in exactly the form that I need for SGD to run.",
    "start": "2962170",
    "end": "2971099"
  },
  {
    "text": "That's pretty wild. This is just a sum\nover everything. I can just write gradient\ndescent or anything",
    "start": "2971099",
    "end": "2976670"
  },
  {
    "text": "else I want in terms of the\nthetas, and I'm all good. These are functions\nunderneath the covers. Now, one other thing,\nwhich I won't arrive,",
    "start": "2976670",
    "end": "2983250"
  },
  {
    "text": "but you can see very\neasily from this--",
    "start": "2983250",
    "end": "2989270"
  },
  {
    "text": "so just to be\nclear, same recipe. I want to write down what\nI mean by same recipe.",
    "start": "2989270",
    "end": "2995730"
  },
  {
    "text": "We have theta t plus 1 equals\ntheta t minus alpha theta i t",
    "start": "2995730",
    "end": "3007160"
  },
  {
    "text": "theta. Now, when we do this,\nsomething-- oh, so right now,",
    "start": "3007160",
    "end": "3012500"
  },
  {
    "text": "actually, we're sorry. We're going to do gradient\nascent because this is still maximizing probability. We haven't pulled\nout a negative term. Sorry about that.",
    "start": "3012500",
    "end": "3018650"
  },
  {
    "text": "But one interesting\nthing pops out. So you should verify this. We'll see if we can do it.",
    "start": "3018650",
    "end": "3025190"
  },
  {
    "text": "We won't do it in class, but you\nshould see if you can do this. Theta j of l theta equals\nthe sum, i goes from 1 to n,",
    "start": "3025190",
    "end": "3041670"
  },
  {
    "text": "of yi minus h theta\nxi times xi j.",
    "start": "3041670",
    "end": "3052500"
  },
  {
    "text": "So this is pretty miraculous,\nif you look at this. What it says is,\nif I look actually at this underlying function\nand I take the derivative,",
    "start": "3052500",
    "end": "3060470"
  },
  {
    "text": "it comes out in\nexactly the same form that we had when we were\ndoing linear regression.",
    "start": "3060470",
    "end": "3066180"
  },
  {
    "text": "It's your prediction\nerror times the xi. Now the prediction itself\nis different, right?",
    "start": "3066180",
    "end": "3071359"
  },
  {
    "text": "Before, the prediction\nwas just theta dot xi. Now it's this h function.",
    "start": "3071359",
    "end": "3076870"
  },
  {
    "text": "But this gives me something\nthat-- these models are very, very similar, right? They're like, how much\nerror do I make, then",
    "start": "3076870",
    "end": "3082230"
  },
  {
    "text": "take a gradient step with\nrespect to the data that tries to minimize that error. And so this is the\nsense in which I mean.",
    "start": "3082230",
    "end": "3088509"
  },
  {
    "text": "These models really all\nare like all the same. We're twisting these\npieces at the edge for how we model things.",
    "start": "3088510",
    "end": "3095770"
  },
  {
    "text": "And so that means actually after\nyou pop all this stuff out, you can use exactly\nthe same rule.",
    "start": "3095770",
    "end": "3100800"
  },
  {
    "text": "And this rule is\nextremely general.",
    "start": "3100800",
    "end": "3107430"
  },
  {
    "text": "And that's surprising. This rule of-- I just take my\npredictor, and I do",
    "start": "3107430",
    "end": "3112619"
  },
  {
    "text": "the derivative, that's shocking\nthat a large class of models. And in fact, that's\nwhat we're going to generalize in\nthe next lecture",
    "start": "3112619",
    "end": "3118559"
  },
  {
    "text": "to make sure that we understand\nexactly the breadth of that. Any questions to this part\nof what we're talking about?",
    "start": "3118559",
    "end": "3126570"
  },
  {
    "text": "Please. [INAUDIBLE] Oh, right, great question. So remember, the reason that\nwe got this minus sign here?",
    "start": "3126570",
    "end": "3134180"
  },
  {
    "text": "Sorry to go back. Look. The minus sign was our nemesis\nthe last time that popped out.",
    "start": "3134180",
    "end": "3142299"
  },
  {
    "text": "And so this turned our\nmax into a min, right? We didn't have a minus sign here\nand we were maximizing the loss",
    "start": "3142299",
    "end": "3150359"
  },
  {
    "text": "that we put in, and\nI didn't-- oops-- I didn't change anything. So we never had a minus\nsign pop out of here.",
    "start": "3150359",
    "end": "3158068"
  },
  {
    "text": "But when you actually\ngo through and see it, a minus sign does pop out. You have to take my\nword for it, or you just do the calculations here.",
    "start": "3158069",
    "end": "3163170"
  },
  {
    "text": "But here, that's why we\nhave gradient descent, because we're maximizing\nthe loss, not minimizing",
    "start": "3163170",
    "end": "3168290"
  },
  {
    "text": "the underlying function. Please. What was the subscript\non the [INAUDIBLE]??",
    "start": "3168290",
    "end": "3175270"
  },
  {
    "text": "Oh, this is xj is\nthe jth component. So here, I did this\nj's are the same.",
    "start": "3175270",
    "end": "3180710"
  },
  {
    "text": "So I was clicking the\nderivative with respect to the jth component of theta. And so that's the\nunderlying derivative.",
    "start": "3180710",
    "end": "3186690"
  },
  {
    "text": "Same way if you remember\nin the linear regression. We calculated the derivative\nwith respect to each component",
    "start": "3186690",
    "end": "3192548"
  },
  {
    "text": "independently. That's the j component\nof the i-th sample. Exactly right. Well, this is exactly right.",
    "start": "3192549",
    "end": "3199640"
  },
  {
    "text": "Yeah, I don't have\nanything to add.",
    "start": "3199640",
    "end": "3209130"
  },
  {
    "text": "Yeah. [INAUDIBLE] Oh, great question.",
    "start": "3209130",
    "end": "3215900"
  },
  {
    "text": "yi is a label. And here, that's\nwhat I was saying.",
    "start": "3215900",
    "end": "3221200"
  },
  {
    "text": "I said it in an extremely\nconfusing way for some reason. So yi is fixed.",
    "start": "3221200",
    "end": "3226869"
  },
  {
    "text": "I know yi. I get to see yi, it's a label. So when I have yi is equal\nto 0, then this term is 0,",
    "start": "3226869",
    "end": "3233349"
  },
  {
    "text": "so this thing is just 1. And this is the\nterm that's inside. It's like a switch statement. When yi is 1, which\nI get to see, right,",
    "start": "3233349",
    "end": "3240020"
  },
  {
    "text": "for the values that it's 1,\nthen this statement goes away. And I have only\nthis character, if I",
    "start": "3240020",
    "end": "3245470"
  },
  {
    "text": "could draw faster with colors. That's a terrible color. Does that make sense? So it switches between\nboth based on what yi is.",
    "start": "3245470",
    "end": "3252460"
  },
  {
    "text": "It's just a compact\nencoding of both cases. That's why it's a\nlittle bit awkward. Yeah, great question.",
    "start": "3252460",
    "end": "3258780"
  },
  {
    "text": "Please. [INAUDIBLE] So we compute it. So the thing here is this\nderivative here, this log,",
    "start": "3258780",
    "end": "3265780"
  },
  {
    "text": "and I'll make sure\nthis is clear. We want to-- this I'm asserting,\nI haven't shown you this. But the way you compute it\nis you take the derivative,",
    "start": "3265780",
    "end": "3272530"
  },
  {
    "text": "you put it inside, you say,\nOK, yi doesn't depend on theta. This term does. You compute the derivative\nof this character internally,",
    "start": "3272530",
    "end": "3279570"
  },
  {
    "text": "and then that is what I'm\nsaying you can simplify it to down here. But you compute it. That's up to you to do.",
    "start": "3279570",
    "end": "3285099"
  },
  {
    "text": "Once the model is\nin this form, you just use the rules of\ncalculus to compute it. Nothing fancy. Yeah, please.",
    "start": "3285099",
    "end": "3291130"
  },
  {
    "text": "There's a few\nproblems like this. Obviously, it's all [INAUDIBLE].",
    "start": "3291130",
    "end": "3296380"
  },
  {
    "text": "Yeah, exactly. This follows this notation\nI will use reflexively",
    "start": "3296380",
    "end": "3302680"
  },
  {
    "text": "without thinking. This is the log likelihood. Let me change colors.",
    "start": "3302680",
    "end": "3307930"
  },
  {
    "text": "This is the log likelihood. And this is the likelihood.",
    "start": "3307930",
    "end": "3313510"
  },
  {
    "text": "OK? This is on the probabilities. This is on the logs of them. Great questions.",
    "start": "3313510",
    "end": "3319310"
  },
  {
    "text": "Cool. Please. [INAUDIBLE] It's the log likelihood.",
    "start": "3319310",
    "end": "3325470"
  },
  {
    "text": "It's a little l. So we'll almost always\ndo gradient descent. So one rule of the course,\nor not rule, but one thing",
    "start": "3325470",
    "end": "3330900"
  },
  {
    "text": "is, you typically use\nthe log likelihoods for a variety of reasons. But one is that they're\nnice for optimization.",
    "start": "3330900",
    "end": "3336200"
  },
  {
    "text": "And so that's what this\nlink is meant to show you. Very cool.",
    "start": "3336200",
    "end": "3342770"
  },
  {
    "text": "Awesome. I will pause a second. All right.",
    "start": "3342770",
    "end": "3348950"
  },
  {
    "text": "So at this phase right now,\nwe've seen another model. What we're going\nto see next lecture",
    "start": "3348950",
    "end": "3354710"
  },
  {
    "text": "is we're going to\ngeneralize this with a little bit more math. And so the thing is, maybe it's\na terrible, terrible metaphor,",
    "start": "3354710",
    "end": "3360430"
  },
  {
    "text": "but a frog with boiling\nwater, or whatever. But you're getting\nmore complexity and you're not\nnoticing it, right?",
    "start": "3360430",
    "end": "3365760"
  },
  {
    "text": "We started at lines. I know how to fit a line,\nthen we had some probability distributions. OK, they came in, and then we\nstarted these predictors that",
    "start": "3365760",
    "end": "3372599"
  },
  {
    "text": "were actually, instead\nof just giving you a value of regression,\nthey were actually giving you a probability. We interpreted those\nas log likelihoods.",
    "start": "3372599",
    "end": "3378070"
  },
  {
    "text": "Now we're going to\nmake-- next lecture, we're going to make the\nprobabilities more complex. And that's going to\nallow us to generalize.",
    "start": "3378070",
    "end": "3384950"
  },
  {
    "text": "Before we do that,\nI want to show you one other thing, which is\nthe Newton's method, which",
    "start": "3384950",
    "end": "3391109"
  },
  {
    "text": "is another solution\nmethod so that we can compare and contrast with\nstochastic gradient descent and give you a chance\nto ask questions.",
    "start": "3391109",
    "end": "3396920"
  },
  {
    "text": "So we were a little rushed\nat the end of last lecture because I screwed up. OK. Sound good?",
    "start": "3396920",
    "end": "3402470"
  },
  {
    "start": "3402000",
    "end": "3599000"
  },
  {
    "text": "All right. OK. So let's talk about\nNewton's method. We're now talking about--\nforget about your modeling side, now we're talking about\noptimization, right?",
    "start": "3402470",
    "end": "3409620"
  },
  {
    "text": "So Newton's method\nis the following. We're going to be given\nsome f from Rd to d.",
    "start": "3409620",
    "end": "3419788"
  },
  {
    "text": "It's got to be a scalar out\nat this point, actually. And what we want\nto do is we want",
    "start": "3419789",
    "end": "3425829"
  },
  {
    "text": "to find f of x equals to This is in general a hard\nand intractable problem.",
    "start": "3425830",
    "end": "3432630"
  },
  {
    "text": "Sometimes it will work,\nsometimes it won't work. If it were really nasty\nfunction, if it's continuous, great.",
    "start": "3432630",
    "end": "3437890"
  },
  {
    "text": "So why does this have anything\nto do with what we care about? Just as an aside. Remember your thinking here.",
    "start": "3437890",
    "end": "3444770"
  },
  {
    "text": "If you want to minimize,\nsay, l theta and it's convex or has a nice shape, that's\nthe same as l prime theta",
    "start": "3444770",
    "end": "3450109"
  },
  {
    "text": "equals 0, sorry if\nthat's too small, right? So if I want to\nminimize something, it's the same as finding the\nroots of its derivative, right?",
    "start": "3450109",
    "end": "3460600"
  },
  {
    "text": "Assuming it's convex both shape. So they're related clearly. All right.",
    "start": "3460600",
    "end": "3465690"
  },
  {
    "text": "So how does this\nthing actually work? So the idea here is-- and probably you've seen\nthis method at some point.",
    "start": "3465690",
    "end": "3472119"
  },
  {
    "text": "Maybe in a calculus\nclass or somewhere, and it's a good method. But it has trouble\nwith machine learning,",
    "start": "3472119",
    "end": "3477690"
  },
  {
    "text": "and I want to talk about why. OK. So here we have theta 0. We take our guess f of theta",
    "start": "3477690",
    "end": "3482780"
  },
  {
    "text": "OK? Remember the\nderivative in this case because it's one-dimensional\nis the direction",
    "start": "3482780",
    "end": "3488170"
  },
  {
    "text": "of maximal increase, right? That's the way the\nfunction is increasing. That's what the derivative\nis actually giving us.",
    "start": "3488170",
    "end": "3494190"
  },
  {
    "text": "Now, what we're\ngoing to do is we're going to follow the derivative\nto where it crosses the axis.",
    "start": "3494190",
    "end": "3502799"
  },
  {
    "text": "So our guess is\ngoing to be theta 1. Now, you should kind\nof convince yourself--",
    "start": "3502799",
    "end": "3508400"
  },
  {
    "text": "it's not true everywhere,\nbut almost always, if you think about picture\na function in your head that crosses 0, this is going\nto be a pretty interesting way",
    "start": "3508400",
    "end": "3517119"
  },
  {
    "text": "to find the 0's\nand to get closer. In fact, this method is\ninsanely fast for a large class of functions.",
    "start": "3517119",
    "end": "3522910"
  },
  {
    "text": "It's what's called\nquadratically faster, which we'll emphasize again. But it means you get twice the\nnumber of digits of precision",
    "start": "3522910",
    "end": "3528490"
  },
  {
    "text": "as you run. It's wildly fast, OK, when\nit runs in terms of steps that it takes.",
    "start": "3528490",
    "end": "3533520"
  },
  {
    "text": "OK. So this distance we\nwant to call delta. So theta 1 is going to be\nequal to theta 0 minus delta.",
    "start": "3533520",
    "end": "3542930"
  },
  {
    "text": "What is delta? How far do we step? Then we have an\nalgorithm here, right? And then we'll repeat it,\nright, just to be clear.",
    "start": "3542930",
    "end": "3548130"
  },
  {
    "text": "We would go up here, we would\ncompute another derivative,",
    "start": "3548130",
    "end": "3553210"
  },
  {
    "text": "and so on. And we would zoom in on this. This would be our\ntheta 1 or theta 2.",
    "start": "3553210",
    "end": "3558280"
  },
  {
    "text": "OK? So we have to solve\nthis key step. So how big is this? Well, if we look\nat it, f of theta 0",
    "start": "3558280",
    "end": "3569110"
  },
  {
    "text": "equals f prime of\ntheta 0 times delta. It's just a triangle.",
    "start": "3569110",
    "end": "3576539"
  },
  {
    "text": "Rise over run. It's all I'm doing. That means delta equals\nf prime of theta 0, which",
    "start": "3576539",
    "end": "3589548"
  },
  {
    "text": "I'll write in an obfuscated\nway, times f theta of 0. Oops, that looks terrible.",
    "start": "3589549",
    "end": "3595760"
  },
  {
    "text": "Let me erase that. There you go. Inverse.",
    "start": "3595760",
    "end": "3601520"
  },
  {
    "text": "OK. So I have to do an inversion. OK?",
    "start": "3601520",
    "end": "3607460"
  },
  {
    "text": "So this gives us\nthe rule theta t plus 1 equals theta\nt minus f of theta t",
    "start": "3607460",
    "end": "3617230"
  },
  {
    "text": "over f of theta t prime. This is our route\nfinding algorithm.",
    "start": "3617230",
    "end": "3622920"
  },
  {
    "text": "And as I said, this thing\nconverges crazy, crazy fast, right? If you go to 0.1, then the\nnext iteration will be 0.01.",
    "start": "3622920",
    "end": "3631510"
  },
  {
    "text": "This is error\ngoing to be 0.0001. This is the error. This is what\nquadratic speed means.",
    "start": "3631510",
    "end": "3636588"
  },
  {
    "text": "That's insane. You don't have that many\ndigits on your device. It'll get to machine\nprecision very, very quickly.",
    "start": "3636589",
    "end": "3644789"
  },
  {
    "text": "Now, this algorithm looks great. In one dimensions,\nit's quite good. But there's a problem\nwith it when we scale up",
    "start": "3644789",
    "end": "3650980"
  },
  {
    "text": "to higher dimensions. And the problem when we\nscale up to higher dimensions is right here. The way that you write\nthe higher dimensional",
    "start": "3650980",
    "end": "3657039"
  },
  {
    "text": "version of this rule is theta\nt plus 1 equals theta t minus, and I'm going to write the\ntypical way we do this,",
    "start": "3657039",
    "end": "3666010"
  },
  {
    "text": "h inverse gradient, and I'm\ngoing to put it in the way that we would use it.",
    "start": "3666010",
    "end": "3672049"
  },
  {
    "text": "So what have I done here? Let me unpack this. It's a little bit obtuse. So when we want to\ngeneralize to vectors,",
    "start": "3672050",
    "end": "3678588"
  },
  {
    "text": "when we want to generalize\nand use for minimization,",
    "start": "3678589",
    "end": "3687940"
  },
  {
    "text": "we get here. So theta is remember\nour friend in Rd plus 1.",
    "start": "3687940",
    "end": "3693088"
  },
  {
    "text": "L theta becomes our f theta as\nwe were using it above, right?",
    "start": "3693089",
    "end": "3700000"
  },
  {
    "text": "I've written it as a gradient\nwith respect to this. This thing here, if you\nremember your calculus,",
    "start": "3700000",
    "end": "3707650"
  },
  {
    "text": "this is the Hessian. How big is that thing? Well, it's in d plus",
    "start": "3707650",
    "end": "3714260"
  },
  {
    "text": "All right. This thing is small.",
    "start": "3714260",
    "end": "3719569"
  },
  {
    "text": "This thing is not small. This thing is small. This is an Rd.",
    "start": "3719569",
    "end": "3728820"
  },
  {
    "text": "Now, one thing that is thing\ndefinitely-- and by the way, if you remember what the Hessian\nis, Hij equals, in this case,",
    "start": "3728820",
    "end": "3737700"
  },
  {
    "text": "I'll write it as l\ntheta, is the matrix of second partial derivatives.",
    "start": "3737700",
    "end": "3746329"
  },
  {
    "text": "All right.",
    "start": "3746329",
    "end": "3752829"
  },
  {
    "text": "So it's all the mixed\npartial derivatives. Do you remember this\nfrom your calculus class? If not, don't worry, I'm sure\na brief refresh will be fine.",
    "start": "3752829",
    "end": "3758818"
  },
  {
    "text": "A couple of things that are\ngreat about this algorithm. First is, notice\nthere's nothing there. There's no step size. There's no alpha.",
    "start": "3758819",
    "end": "3765059"
  },
  {
    "text": "This thing just runs. This is a great algorithm. In machine learning\nantiquity, 2003 and '04, 2006,",
    "start": "3765059",
    "end": "3773750"
  },
  {
    "text": "people use this algorithm\nbecause it carried over from statisticians. This is how statisticians would\nsolve logistic regression.",
    "start": "3773750",
    "end": "3779920"
  },
  {
    "text": "So if you go into R, I think,\nup until very recently, maybe even still, and you say\nsolve logistic regression,",
    "start": "3779920",
    "end": "3785460"
  },
  {
    "text": "it will use this algorithm\nunder the coverage. And the reason is, it will get\nsuper, super accurate, right? It'll get all that--\nfill up all your digits",
    "start": "3785460",
    "end": "3792029"
  },
  {
    "text": "and be very, very efficient\nin terms of how many steps it takes. But each one of those steps\nfor a machine learning problem",
    "start": "3792029",
    "end": "3798130"
  },
  {
    "text": "could blow out your memory. If you imagine you\nhave a machine learning model that has a\nbillion parameters,",
    "start": "3798130",
    "end": "3803930"
  },
  {
    "text": "a billion squared is a lot. It's huge. It will blow out your system.",
    "start": "3803930",
    "end": "3809000"
  },
  {
    "text": "And so people have ways\nof relaxing this over time that they try to get\nmore information in,",
    "start": "3809000",
    "end": "3814049"
  },
  {
    "text": "but it hasn't historically\nbeen worth it. OK. So does this\nalgorithm make sense?",
    "start": "3814049",
    "end": "3819849"
  },
  {
    "text": "You recall this algorithm? Happy to answer\nquestions about it. Cool. All right.",
    "start": "3819849",
    "end": "3825160"
  },
  {
    "text": "So let's do a rough comparison. And if you want, please ask\nquestions about anything, I guess, but relevant fine.",
    "start": "3825160",
    "end": "3831359"
  },
  {
    "text": "I think I have a chart for this. OK. So let's look back\nat the methods",
    "start": "3831359",
    "end": "3836920"
  },
  {
    "text": "we've seen because I want\nto put them in context. We saw this SGD\nalgorithm, pure SGD.",
    "start": "3836920",
    "end": "3843170"
  },
  {
    "text": "Every iteration\ntook one data point. We looked.",
    "start": "3843170",
    "end": "3848589"
  },
  {
    "text": "So I want to compare\nthe method just so we're clear on the method name. How much they cost\nper iteration? That is every time I take a\nstep and change the model.",
    "start": "3848589",
    "end": "3856029"
  },
  {
    "text": "That's what I mean\nby per iteration. How much compute do they do as\na result of that per iteration?",
    "start": "3856030",
    "end": "3861319"
  },
  {
    "text": "And how many steps do\nthey take to the error, to get the error, right? This is kind of the\nconvergence tradeoff. So SGD has a pretty bad estimate\nof the underlying gradient,",
    "start": "3861319",
    "end": "3871099"
  },
  {
    "text": "but you can go super\nfast relative to the size of the model, so you take many\nof them, and you make up for it in some situations.",
    "start": "3871099",
    "end": "3877750"
  },
  {
    "text": "So let's see this. So the compute here, this\nis proportional to d. It does not depend on the\nsize of your data set.",
    "start": "3877750",
    "end": "3883670"
  },
  {
    "text": "There are situations, where\nyou can train these models, you don't even see\nall of your data. You only sample a\nsmall-- actually,",
    "start": "3883670",
    "end": "3889660"
  },
  {
    "text": "there are models that people\npay money for that they have huge, huge collections\nof data, and they only",
    "start": "3889660",
    "end": "3894710"
  },
  {
    "text": "ran on the first 30% of it, and\nthey released the model there. They're like, hey,\nwe sampled from it. It was fine.",
    "start": "3894710",
    "end": "3900420"
  },
  {
    "text": "It was fast enough. If you ran even a single episode\nof batch gradient descent,",
    "start": "3900420",
    "end": "3907420"
  },
  {
    "text": "batch gradient\ndescent, you would have to look at all the data\npoints, which would potentially",
    "start": "3907420",
    "end": "3915539"
  },
  {
    "text": "be much, much slower. OK? So it takes time at least-- oh, and put theta here,\nalthough, I don't really",
    "start": "3915539",
    "end": "3921538"
  },
  {
    "text": "mean them formally, OK? And then there was\nNewton's method.",
    "start": "3921539",
    "end": "3930010"
  },
  {
    "text": "Newton's method also\nlooks at all data points.",
    "start": "3930010",
    "end": "3935990"
  },
  {
    "text": "It's extremely expensive. We won't talk about how\nexpensive-- you can get it slightly down from this,\nand I've written papers",
    "start": "3935990",
    "end": "3941880"
  },
  {
    "text": "with other people\nand lots of people have tried to\nimprove this method. But it literally--\nit's huge, it has this d squared is\ngoing to kill you, OK?",
    "start": "3941880",
    "end": "3948539"
  },
  {
    "text": "You can try and get\naround it because you have to compute the interaction. Remember these partial\ndifferentials here?",
    "start": "3948539",
    "end": "3954680"
  },
  {
    "text": "These are the interactions\nbetween every pair of variables that you have. That's a lot of information\nquadratic anymore.",
    "start": "3954680",
    "end": "3960990"
  },
  {
    "text": "So that's where their\nd squared is, OK? Now, these things\nare super fast, and I'll be a little\nbit glib here,",
    "start": "3960990",
    "end": "3966589"
  },
  {
    "text": "because I'm not going to state\nthe true precise running time. This thing is really fast. If you want to get\nto epsilon error,",
    "start": "3966590",
    "end": "3971660"
  },
  {
    "text": "you take log 1\nover epsilon steps, alright potentially a little\nbit less than that, too, but it's fine.",
    "start": "3971660",
    "end": "3977369"
  },
  {
    "text": "That's super fast, OK/ You have an epsilon\nof 10 to the minus 16, log of 10 to the minus 16 is\ntake a couple of hundred steps",
    "start": "3977369",
    "end": "3983690"
  },
  {
    "text": "and you're done. That's wild. SGD, a couple of\nhundred steps, it's likely still spitting\nout random values. In contrast, at\nthe other extreme,",
    "start": "3983690",
    "end": "3992328"
  },
  {
    "text": "this is epsilon to the minus These are very vague notions.",
    "start": "3992329",
    "end": "3997990"
  },
  {
    "text": "This is only under\nsome situations. I just want to give\nyou engineers intuition of how well these work. And the point is that\nthere's a clear tradeoff here",
    "start": "3997990",
    "end": "4005760"
  },
  {
    "text": "of how expensive each\none of these points are versus how many\nsteps you have to take.",
    "start": "4005760",
    "end": "4012910"
  },
  {
    "text": "So if you had a\ncomputing device that made it absolutely instantaneous\nto look at all N data points simultaneously, then maybe\nbatch gradient descent",
    "start": "4012910",
    "end": "4020119"
  },
  {
    "text": "makes sense because you\nwould just take steps really, really fast. If you had an Oracle that\ncould compute Hessians, right,",
    "start": "4020119",
    "end": "4025680"
  },
  {
    "text": "which is what people\ntried to do for a while, and compute them\nreally, really quickly, then you would prefer\nthis algorithm, right?",
    "start": "4025680",
    "end": "4031730"
  },
  {
    "text": "So it's a trade-off\nbetween size and speed. Now, we tend to operate as\nmachine learners in situations",
    "start": "4031730",
    "end": "4037280"
  },
  {
    "text": "increasingly, whether\nit's a good idea or not. I happen to like the idea\nof huge, huge models,",
    "start": "4037280",
    "end": "4043560"
  },
  {
    "text": "trillions of parameters are the\nnew thing people care about. It's wild. Computing a trillion\nsquared-- if you",
    "start": "4043560",
    "end": "4049099"
  },
  {
    "text": "thought a billion\nsquared was big, a trillion squared is\nbigger, but about a factor of a million. It's huge.",
    "start": "4049099",
    "end": "4054480"
  },
  {
    "text": "So we can't run on\nthose kinds of models. And we tend to train on\ndata sets that are much, much larger over time.",
    "start": "4054480",
    "end": "4060328"
  },
  {
    "text": "Now that how much larger\nmeans changes every generation of hardware every\ntwo to four years, what we mean by that changes.",
    "start": "4060329",
    "end": "4066880"
  },
  {
    "text": "But we train on the web, all\nemissions, or all of the-- we still can't train\non all the video.",
    "start": "4066880",
    "end": "4072320"
  },
  {
    "text": "There's more video\nthat's put out there than we can possibly train on. We would like to\nbe able to do that. But eventually,\nhardware will catch up,",
    "start": "4072320",
    "end": "4078240"
  },
  {
    "text": "and hopefully, the same\ndumb algorithms will work. That's what we're\npraying for right now.",
    "start": "4078240",
    "end": "4083650"
  },
  {
    "text": "We have no justification\nfor that statement. OK? Now, the one thing that I\nelided last time, this is g,",
    "start": "4083650",
    "end": "4090570"
  },
  {
    "text": "the one thing I alighted\nlast time was there's a little character that squeezes\nin here called mini batch.",
    "start": "4090570",
    "end": "4095828"
  },
  {
    "text": "We talked about\nthis very briefly. What mini batch does is instead\nof selecting one points, it randomly selects B points.",
    "start": "4095829",
    "end": "4102080"
  },
  {
    "text": "Now, its estimate is\nsomehow better than SGD, but not kind of theoretically.",
    "start": "4102080",
    "end": "4108318"
  },
  {
    "text": "It doesn't change the curve. The point is for modern\nmachine learning, you can do a sample of B things\nin parallel in the same wall",
    "start": "4108319",
    "end": "4116680"
  },
  {
    "text": "clock time that you can do one. And that's what's distorted\nus to use these batch methods really candidly.",
    "start": "4116680",
    "end": "4122279"
  },
  {
    "text": "There's a little bit of\nerror reduction in the noise. You get a better\nestimate of the gradient. But really, it's because it's\nfree for the compute device.",
    "start": "4122280",
    "end": "4128470"
  },
  {
    "text": "The way a GPU works or any of\nthese kind of batch parallel systems, you put\nin d points, they",
    "start": "4128470",
    "end": "4134178"
  },
  {
    "text": "can do them all in parallel. So that's the thing that's\nlurking under the covers. Because after the lecture\npeople ask me, well, why would you prefer that?",
    "start": "4134179",
    "end": "4139909"
  },
  {
    "text": "You're still taking the same\nnumber of steps and batch gradient versus SGD. And it's because\nof this parallelism",
    "start": "4139909",
    "end": "4145258"
  },
  {
    "text": "that's underneath the covers. We've built big parallel\nmachines humanity, right? Your phone has an ungodly\nnumber of teraflops in it,",
    "start": "4145259",
    "end": "4153469"
  },
  {
    "text": "supercomputer level teraflops\nsome number of years ago. And we'll continue on\nthat thing so that you can get your photos tagged.",
    "start": "4153469",
    "end": "4158830"
  },
  {
    "text": "I mean, I don't know. That's how it works. Anyway. So those are why\nwe do mini batch.",
    "start": "4158830",
    "end": "4166009"
  },
  {
    "text": "So far so good? All right. Any other questions? All right.",
    "start": "4166010",
    "end": "4171750"
  },
  {
    "text": "So the last thing I'll\njust put on the thing here is in classical stats,\nthese were all things that people cared about.",
    "start": "4171750",
    "end": "4178658"
  },
  {
    "text": "Classical stats d\nwas really small. And n was moderate size.",
    "start": "4178659",
    "end": "4186319"
  },
  {
    "text": "If you look at\nwhere-- if you talk to your friends and the social\nscientists who are maybe they're not doing the same\nthing now, but for a while ago",
    "start": "4186320",
    "end": "4191968"
  },
  {
    "text": "when they would solve\nthese models, when they would solve these\nmodels, d would be 100, right? And they really cared what\nthe responses were down",
    "start": "4191969",
    "end": "4198869"
  },
  {
    "text": "to very, very fine levels. You have to run for\na really long time",
    "start": "4198870",
    "end": "4204590"
  },
  {
    "text": "to get that level\nof accuracy for SGD. What machine learning is about,\nin a really fundamental way is taking these bigger models\nand solving them approximately.",
    "start": "4204590",
    "end": "4214130"
  },
  {
    "text": "And weirdly enough, they\nend up pretty robust, which is something horrifying. We don't understand it. Please.",
    "start": "4214130",
    "end": "4220030"
  },
  {
    "text": "What does the omega field mean? Oh, yeah. So it means at least. So I mean the big\nO style notation. It means asymptotically, it\ngrows at least this fast.",
    "start": "4220030",
    "end": "4227239"
  },
  {
    "text": "It's a little bit slower, but\nI don't want to get into it. Wonderful question. Please.",
    "start": "4227239",
    "end": "4232260"
  },
  {
    "text": "What about all this a few\nalgorithms [INAUDIBLE]??",
    "start": "4232260",
    "end": "4237719"
  },
  {
    "text": "Awesome question. So one access that you\ncould also look on here is, how well do they handle\nnoise in the data?",
    "start": "4237720",
    "end": "4244670"
  },
  {
    "text": "And SGD turns out to\nbe remarkably robust. And there are some versions\nwhere you can prove this.",
    "start": "4244670",
    "end": "4250760"
  },
  {
    "text": "So when you're optimizing-- So there's folklore around this. We'll talk a little\nbit about this.",
    "start": "4250760",
    "end": "4256330"
  },
  {
    "text": "But SGD, because\nit's noisy, there's some belief that it doesn't\nget stuck in local minima as frequently as some of\nthe other algorithms do.",
    "start": "4256330",
    "end": "4263250"
  },
  {
    "text": "If you imagine this\npicture right here, imagine that it\nwent back up, then",
    "start": "4263250",
    "end": "4269408"
  },
  {
    "text": "somehow you're using all\nthe second order information to race you down to the\nclosest local minima, that's",
    "start": "4269409",
    "end": "4275710"
  },
  {
    "text": "potentially not what you\nwanted the entire time. So there's some folklore\ntheory that says SGD is a little bit better. Now I say folklore because\nwe can only nail that down",
    "start": "4275710",
    "end": "4282909"
  },
  {
    "text": "in some cases. They're basically theorems\nthat say of the form like, if your data looks\nlike this, then this happens.",
    "start": "4282909",
    "end": "4288130"
  },
  {
    "text": "Or for certain things, I'll\nshow you in a couple of weeks, this happens for solving\ncertain matrix equations.",
    "start": "4288130",
    "end": "4293660"
  },
  {
    "text": "You can prove that\nit happens there. So that's another axis. The other axis, which\nyou may think about if you're an optimizer\nis, how numerically stable",
    "start": "4293660",
    "end": "4301790"
  },
  {
    "text": "is the underlying algorithm? And here's the\nthing that's pretty wild about machine learning. The trend has been not to make\nmore numerically stable things.",
    "start": "4301790",
    "end": "4309400"
  },
  {
    "text": "So if you care about\nhow a computer works, you have doubles inside double\nprecisions, floating point numbers.",
    "start": "4309400",
    "end": "4314900"
  },
  {
    "text": "Now if you saw NVIDIA's\nlast announcement, they're going down\nto FP8 which, means instead of 64 bits for a number,\nthey're using only 8 bits.",
    "start": "4314900",
    "end": "4321320"
  },
  {
    "text": "There are people right now\ntraining with integers. Those methods, we\nreally only know how to do over SGD,\nbecause these methods,",
    "start": "4321320",
    "end": "4327678"
  },
  {
    "text": "you can't get enough meaningful\ninformation in there. So there's another argument\nabout how statistically robust",
    "start": "4327679",
    "end": "4333510"
  },
  {
    "text": "they are and how\nnumerically stable they are. They're not very\nnumerically stable. There's a lot of tricks.",
    "start": "4333510",
    "end": "4339179"
  },
  {
    "text": "We're pretty primitive\nthere compared to the optimizers of the world. But yeah, wonderful questions.",
    "start": "4339179",
    "end": "4344928"
  },
  {
    "text": "Awesome. Fantastic. Any other questions?",
    "start": "4344929",
    "end": "4349950"
  },
  {
    "text": "OK, great. So we're going to end a\nlittle bit early today. What we're going to do on\nthe and know in general,",
    "start": "4349950",
    "end": "4357490"
  },
  {
    "text": "you have to stay till 4:45. But today, we got through it. Next time, we're going to talk\nabout our exponential models.",
    "start": "4357490",
    "end": "4363260"
  },
  {
    "text": "These are going to be models\nthat have a more complex link function and allow\nus to model more of the world that's around us\nand interesting noise things.",
    "start": "4363260",
    "end": "4370120"
  },
  {
    "text": "See you. Have fun. I'll stick around for\na couple of questions.",
    "start": "4370120",
    "end": "4373160"
  }
]