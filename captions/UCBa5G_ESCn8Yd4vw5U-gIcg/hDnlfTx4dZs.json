[
  {
    "start": "0",
    "end": "5340"
  },
  {
    "text": "Hi, everyone. Welcome to the class. I hope the exam went well.",
    "start": "5340",
    "end": "12150"
  },
  {
    "text": "And what is going to happen in\nthe class for the last three",
    "start": "12150",
    "end": "17520"
  },
  {
    "text": "lectures is that we\nare going to look at some advanced\ntopics in, let's say,",
    "start": "17520",
    "end": "22680"
  },
  {
    "text": "graph representation learning. And we are going to do a\nseries of guest lectures.",
    "start": "22680",
    "end": "29260"
  },
  {
    "text": "And I'm going to talk about\ntoday's guest lecture next. So the lecture today will be\nabout geometric graph learning",
    "start": "29260",
    "end": "37500"
  },
  {
    "text": "or geometric deep learning. And the core concept\nof today's lecture",
    "start": "37500",
    "end": "42960"
  },
  {
    "text": "will be being able\nto develop models that are invariant to symmetry,\nso that account for symmetries",
    "start": "42960",
    "end": "52230"
  },
  {
    "text": "in the data and because\nof this able to learn better with less data more\nefficiently and so on.",
    "start": "52230",
    "end": "58980"
  },
  {
    "text": "And the applications that\nwe are going to talk today about is chemistry,\nis basically how",
    "start": "58980",
    "end": "65740"
  },
  {
    "text": "do we capture\nstructure of molecules using machine learning.",
    "start": "65740",
    "end": "71320"
  },
  {
    "text": "And the lecture will\nbe given by Minkai Xu. He's a PhD student here in the\ncomputer science department",
    "start": "71320",
    "end": "78310"
  },
  {
    "text": "at Stanford and actually\none of the leaders in this area of using geometric\ndeep learning diffusion",
    "start": "78310",
    "end": "84909"
  },
  {
    "text": "models, things like that,\nfor molecular applications.",
    "start": "84910",
    "end": "90650"
  },
  {
    "text": "So I'll hand it\noff to Minkai now. ",
    "start": "90650",
    "end": "100350"
  },
  {
    "text": "Yeah. Thank you, Jure for-- thanks to Jure for\nhaving me today here.",
    "start": "100350",
    "end": "105400"
  },
  {
    "text": "Thank you. To introduce also both of\nthis promising direction",
    "start": "105400",
    "end": "111240"
  },
  {
    "text": "in the advanced graph\nrepresentation learning. And also I can incorporate\nour most recent progress",
    "start": "111240",
    "end": "119010"
  },
  {
    "text": "about graph generation or\ngeometry generation using diffusion models. ",
    "start": "119010",
    "end": "127130"
  },
  {
    "text": "OK, yeah, I think Jure\nhas nicely introduced me. So I'm fifth year student at\nStanford at computer science,",
    "start": "127130",
    "end": "135110"
  },
  {
    "text": "working mainly on intersection\nof general model and graph representation learning.",
    "start": "135110",
    "end": "140609"
  },
  {
    "text": "So the application will mainly\nabout graph generation models on molecular structure,\non drug discovery,",
    "start": "140610",
    "end": "148460"
  },
  {
    "text": "on protein design,\nsomething like this. ",
    "start": "148460",
    "end": "155770"
  },
  {
    "text": "OK. So this will be the\noutline of today's lecture. So first, I will\nintroduce what is--",
    "start": "155770",
    "end": "163530"
  },
  {
    "text": "when we talk about geometric\ngraph, what we are talking about, what it is, and\nwhat is the key difficulty",
    "start": "163530",
    "end": "172200"
  },
  {
    "text": "of developing machine learning\nmodels for geometric graphs?",
    "start": "172200",
    "end": "177270"
  },
  {
    "text": "Significantly different\nto your previous content for typical graph learning.",
    "start": "177270",
    "end": "182700"
  },
  {
    "text": "And the second\npart is, OK, now we know what is geometric graph.",
    "start": "182700",
    "end": "188670"
  },
  {
    "text": "We know what is the difficulty\nof learning geometric graph. Then how we develop effective\nmodel for geometric graph.",
    "start": "188670",
    "end": "196450"
  },
  {
    "text": "So we introduce some-- mainly two class of graph neural\nnetwork for geometric graph.",
    "start": "196450",
    "end": "203400"
  },
  {
    "text": "And the last part is,\nconsidering we have already",
    "start": "203400",
    "end": "208650"
  },
  {
    "text": "defined the data,\nwe have already developed some\nmodern architecture,",
    "start": "208650",
    "end": "214200"
  },
  {
    "text": "like a graph neural network,\nhow to use this graph neural network to tackle\nreal world challenges.",
    "start": "214200",
    "end": "220239"
  },
  {
    "text": "And I will introduce\nour most recent progress of geometric\ndiffusion model, which",
    "start": "220240",
    "end": "226000"
  },
  {
    "text": "is capable to\ngenerate a very, very realistic molecular structure.",
    "start": "226000",
    "end": "232159"
  },
  {
    "text": "OK. So first part will\nbe geometric graph. And maybe we can have\na very brief recap",
    "start": "232160",
    "end": "239269"
  },
  {
    "text": "of a very typical graph. So what is graph? I think you have learned\nit for the whole quarter.",
    "start": "239270",
    "end": "247190"
  },
  {
    "text": "It's adjacency matrix and\nalso a collection of nodes. The node have node\nfeature, and they",
    "start": "247190",
    "end": "252920"
  },
  {
    "text": "are defined by some connection\nof the adjacency matrix.",
    "start": "252920",
    "end": "257930"
  },
  {
    "text": "And occasionally, in\nthe adjacency matrix, the aij also can contain\nsome edge feature.",
    "start": "257930",
    "end": "266360"
  },
  {
    "text": "So we want to learn\nover such graph. Yeah, very typical thing.",
    "start": "266360",
    "end": "271680"
  },
  {
    "text": "And I think, yeah, these\ntwo equations, you also have learned it for many times.",
    "start": "271680",
    "end": "277780"
  },
  {
    "text": "So how to do learning\non graph structure? We actually use the\ngraph neural network.",
    "start": "277780",
    "end": "284550"
  },
  {
    "text": "And it mainly-- it is mainly\ncomposed of two steps. So one step is aggregation.",
    "start": "284550",
    "end": "290490"
  },
  {
    "text": "Like currently we\nare on one node. And we have multiple neighbor.",
    "start": "290490",
    "end": "296310"
  },
  {
    "text": "The neighbor will\ngenerate or predict-- or we want to learn some\nmessage from the neighborhood.",
    "start": "296310",
    "end": "305310"
  },
  {
    "text": "And the neighborhood will\npass their message toward you. You will learn\naggregation function",
    "start": "305310",
    "end": "313440"
  },
  {
    "text": "to aggregate the information. And after you aggregated\nthe information, you will also have\nupdated function",
    "start": "313440",
    "end": "319889"
  },
  {
    "text": "to learn how to update\ncurrent node embedding. So multiple perspective\nof understanding",
    "start": "319890",
    "end": "329330"
  },
  {
    "text": "the traditional\ngraph neural network. So these are two most,\nI think, well known",
    "start": "329330",
    "end": "337640"
  },
  {
    "text": "or accepted perspective to\nunderstand the graph neural network.",
    "start": "337640",
    "end": "343099"
  },
  {
    "text": "One direction is just\nlook at one layer. And you see it is one node,\nand there is neighbor.",
    "start": "343100",
    "end": "351860"
  },
  {
    "text": "You repeatedly pass the message\nand aggregate the message and update node embedding.",
    "start": "351860",
    "end": "357720"
  },
  {
    "text": "And I think the computational\ntree has also been incorporated in the lecture.",
    "start": "357720",
    "end": "362940"
  },
  {
    "text": "So when we are considering\nmultiple graph neural network, you can see, in current layer,\nyou aggregate the information",
    "start": "362940",
    "end": "370099"
  },
  {
    "text": "from your neighbor. And if you look at\nthe before layer, you can see your\nneighborhood's embedding.",
    "start": "370100",
    "end": "376970"
  },
  {
    "text": "They actually aggregate\nthe information of their neighborhood. So by stacking multiple graph\nneural network, actually,",
    "start": "376970",
    "end": "385970"
  },
  {
    "text": "you just exponentially\nexpand your perceptual field",
    "start": "385970",
    "end": "391850"
  },
  {
    "text": "and learn more and more\nfeature from the neighborhood's neighborhood's neighborhoods. Something like this.",
    "start": "391850",
    "end": "398789"
  },
  {
    "text": "OK. And before I\nintroduce-- finally, before I introduce\nwhat is graph--",
    "start": "398790",
    "end": "404790"
  },
  {
    "text": "what is geometric graph,\nI need to also make",
    "start": "404790",
    "end": "410040"
  },
  {
    "text": "a brief recap of the\ngraph-level prediction task.",
    "start": "410040",
    "end": "416210"
  },
  {
    "text": "I know for graph\nrepresentation learning, I think Jure concentrated on\ntwo or three kinds of task.",
    "start": "416210",
    "end": "425770"
  },
  {
    "text": "One kind is\nnode-level prediction. We want to predict\nsome feature of a node.",
    "start": "425770",
    "end": "433720"
  },
  {
    "text": "And another similar application\nmay be edge prediction, so predict some\nfeature on the edge.",
    "start": "433720",
    "end": "440770"
  },
  {
    "text": "But today, maybe we first\nconsider the problem on graph-level prediction.",
    "start": "440770",
    "end": "447790"
  },
  {
    "text": "So this is very useful\nfor-- especially for a molecular graph,\nfor small graphs.",
    "start": "447790",
    "end": "455160"
  },
  {
    "text": "So we have a molecular graph. And we want to\npredict some feature",
    "start": "455160",
    "end": "460770"
  },
  {
    "text": "or property of this molecule. So what do we do? In addition to the typical\nmessage passing neural network,",
    "start": "460770",
    "end": "469300"
  },
  {
    "text": "we call it permutation\nequivariant neural network. But never mind, just a\ntypical graph neural network. ",
    "start": "469300",
    "end": "477050"
  },
  {
    "text": "Between different layers\nof graph neural network, we also have some advanced\nlayer like pooling.",
    "start": "477050",
    "end": "483680"
  },
  {
    "text": "And yeah, I think I also\nsee the previous syllabus.",
    "start": "483680",
    "end": "489440"
  },
  {
    "text": "I think differentiable pooling\nhas been also introduced before. So in addition to just learn\nthe embedding on node level,",
    "start": "489440",
    "end": "499190"
  },
  {
    "text": "we also can use some pooling\nto pool multiple nodes together as a super node.",
    "start": "499190",
    "end": "504650"
  },
  {
    "text": "So this kind of formulation\ncan significantly reduce the number of nodes\nduring message passing.",
    "start": "504650",
    "end": "512659"
  },
  {
    "text": "And also, it can help to\nenable some message passing between multiple group of node.",
    "start": "512659",
    "end": "518479"
  },
  {
    "text": "Or in other words,\nit is more makes-- it makes more sense for molecules.",
    "start": "518480",
    "end": "523729"
  },
  {
    "text": "Because in molecules, there\nare chemical rings or something like this. They are very predefined\nlocal structure.",
    "start": "523730",
    "end": "531000"
  },
  {
    "text": "So this kind of pooling\ncan learn to pool the local structure towards a single--\ntowards as represented--",
    "start": "531000",
    "end": "538860"
  },
  {
    "text": "represented as some\npseudo single node. So enabling some message\npassing between the local graphs",
    "start": "538860",
    "end": "545820"
  },
  {
    "text": "makes the information\ntransformation or message passing more effective,\nsignificantly improve",
    "start": "545820",
    "end": "553560"
  },
  {
    "text": "many tasks for molecular\nproperty prediction.",
    "start": "553560",
    "end": "559680"
  },
  {
    "text": "OK. So by last slide, I just\nintroduced-- also introduced,",
    "start": "559680",
    "end": "566450"
  },
  {
    "text": "like mentioned\nhow to learn graph neural network for molecules.",
    "start": "566450",
    "end": "571950"
  },
  {
    "text": "Now maybe we can\nhave a look again of when we do graph\nrepresentation learning",
    "start": "571950",
    "end": "579510"
  },
  {
    "text": "for molecule, actually\nwhat we have as the input. So this figure is\npretty typical, right?",
    "start": "579510",
    "end": "588519"
  },
  {
    "text": "So it is, again, a node. Different node are labeled\nwith like item type,",
    "start": "588520",
    "end": "595060"
  },
  {
    "text": "and they are connected\nby valence bond. And the edge can also be\nlabeled with some chemical type.",
    "start": "595060",
    "end": "602050"
  },
  {
    "text": "So something like this. But most importantly, actually,\nwe also, in many times,",
    "start": "602050",
    "end": "608410"
  },
  {
    "text": "have the 3D structure\nof molecules. And in many real\nworld applications,",
    "start": "608410",
    "end": "613690"
  },
  {
    "text": "we are very interested\nin this kind of 3D structure of molecules.",
    "start": "613690",
    "end": "619920"
  },
  {
    "text": "For example, when we want\nto model how a molecule acts",
    "start": "619920",
    "end": "627950"
  },
  {
    "text": "in some solvent or in our human\nbody, how a protein, if you have some disease,\nthere is some protein,",
    "start": "627950",
    "end": "635120"
  },
  {
    "text": "there is some functional\nproblem of your protein and you eat a drug, how the\ndrug will solve your problem.",
    "start": "635120",
    "end": "642050"
  },
  {
    "text": "Actually, they are\neffective in the 3D space. We want to model\nthem in the 3D space.",
    "start": "642050",
    "end": "648140"
  },
  {
    "text": "So it is very informative. And another thing is,\nconsidering for one molecule,",
    "start": "648140",
    "end": "656460"
  },
  {
    "text": "actually even one molecule will\nhave a different 3D structure in different temperature,\nin different solvent.",
    "start": "656460",
    "end": "662970"
  },
  {
    "text": "Something like this. And when they are in\ndifferent condition, they will have different 3D.",
    "start": "662970",
    "end": "669089"
  },
  {
    "text": "And then they have\ndifferent property. So it never makes sense to-- for some property, never\nmake sense to input",
    "start": "669090",
    "end": "677160"
  },
  {
    "text": "just the adjacency matrix\ngraph and make some property prediction because\nthe property actually",
    "start": "677160",
    "end": "683519"
  },
  {
    "text": "is more-- is directly related\nwith the 3D structure instead",
    "start": "683520",
    "end": "688800"
  },
  {
    "text": "of the 2D structure. OK. So now this is the\ntheme of this lecture,",
    "start": "688800",
    "end": "696730"
  },
  {
    "text": "how we do representation\nlearning on the 3D structures. And molecule is the most\nexciting and promising area",
    "start": "696730",
    "end": "708380"
  },
  {
    "text": "of this direction,\nso geometric graph.",
    "start": "708380",
    "end": "714410"
  },
  {
    "text": "I mean, it seems\nlike there will be some killer application of\ngeometric graph neural network.",
    "start": "714410",
    "end": "721040"
  },
  {
    "text": "But more broadly, there are\nalso many other scenarios. They are also interested\nin geometric graph.",
    "start": "721040",
    "end": "728640"
  },
  {
    "text": "So in addition to\nsmall molecule, there are also a little\nbit larger molecule like protein DNA.",
    "start": "728640",
    "end": "734930"
  },
  {
    "text": "And these three are\norganic structure. So for inorganic\nstructure, there",
    "start": "734930",
    "end": "740000"
  },
  {
    "text": "are also exciting research\non crystal and catalyst. And also those are\nin macro world.",
    "start": "740000",
    "end": "747930"
  },
  {
    "text": "But in our physical\nmacro world, there are also applications like\ntransportation, the city",
    "start": "747930",
    "end": "754740"
  },
  {
    "text": "layout, and vehicle\nand robotics.",
    "start": "754740",
    "end": "759990"
  },
  {
    "text": "So this is really\nan exciting area.",
    "start": "759990",
    "end": "765200"
  },
  {
    "text": "OK. So now we can formally define\nwhat is geometric graph.",
    "start": "765200",
    "end": "770930"
  },
  {
    "text": "From your last feedback,\nI feel like you are-- all of you are very\nfamiliar with what is graph.",
    "start": "770930",
    "end": "777360"
  },
  {
    "text": "So now I can just say, in\naddition to typical graph, so what's new is just every\nnode is additionally embedded",
    "start": "777360",
    "end": "787970"
  },
  {
    "text": "in our Euclidean space. So every node is associated\nwith some 3D coordinates.",
    "start": "787970",
    "end": "795150"
  },
  {
    "text": "So more formally,\nso for this lecture,",
    "start": "795150",
    "end": "800670"
  },
  {
    "text": "I will name the\ntraditional node feature",
    "start": "800670",
    "end": "805820"
  },
  {
    "text": "like item type as\na scalar feature. But for the coordinates, I\nwill name it as tensor feature.",
    "start": "805820",
    "end": "812839"
  },
  {
    "text": "I think this is some terminology\nused in physics course. So scalar means that they\ndon't have direction.",
    "start": "812840",
    "end": "821360"
  },
  {
    "text": "They are just some\nnumber or some value. But for tensor, they are-- in addition to the--",
    "start": "821360",
    "end": "828019"
  },
  {
    "text": " like in physics, we usually make\nnotation of tensor as an arrow.",
    "start": "828020",
    "end": "839630"
  },
  {
    "text": "So in addition to the\nlength of the arrow, we also have the direction. So this is the\nintrinsic difference.",
    "start": "839630",
    "end": "847130"
  },
  {
    "text": "It is directional. ",
    "start": "847130",
    "end": "852399"
  },
  {
    "text": "OK. So before I introduce what is-- what we are concerned about,\ninvariant and equivariant,",
    "start": "852400",
    "end": "860560"
  },
  {
    "text": "maybe I can provide some\nvery intuitive understanding of why we want to do invariant\nprediction or equivariant",
    "start": "860560",
    "end": "869230"
  },
  {
    "text": "prediction, OK? So for geometric graph, let me\njust take a molecular structure",
    "start": "869230",
    "end": "876940"
  },
  {
    "text": "as an example. So firstly, maybe\nlet me just introduce",
    "start": "876940",
    "end": "882970"
  },
  {
    "text": "the supervised learning task. This slide does show\na supervisor task",
    "start": "882970",
    "end": "889660"
  },
  {
    "text": "of prediction of some\nproperty of this structure.",
    "start": "889660",
    "end": "894670"
  },
  {
    "text": " There's some functional property\nof this protein or binding",
    "start": "894670",
    "end": "900550"
  },
  {
    "text": "affinity of this protein. So a brief introduction\nof binding affinity.",
    "start": "900550",
    "end": "906700"
  },
  {
    "text": "So when we have some\ndisease, I think the majority of our\ndisease is because there",
    "start": "906700",
    "end": "913780"
  },
  {
    "text": "is some protein with-- either some functional\nprotein was disabled,",
    "start": "913780",
    "end": "919450"
  },
  {
    "text": "or there is some like\nvirus protein in our body. So when we eat a\ndrug, actually, we",
    "start": "919450",
    "end": "926470"
  },
  {
    "text": "hope the drug to be\nlike in this figure. Maybe this is our drug. So when we use a drug, the\ndrug will combine or bind",
    "start": "926470",
    "end": "934570"
  },
  {
    "text": "on the protein structure. If the protein is\ngood, maybe we want to reactivate the function.",
    "start": "934570",
    "end": "941680"
  },
  {
    "text": "Or if it is virus protein, maybe\nour hope is to deactivate it.",
    "start": "941680",
    "end": "948649"
  },
  {
    "text": "But anyway, so I just want\nto see, when we eat a drug, the true effectiveness\nis we hope",
    "start": "948650",
    "end": "957190"
  },
  {
    "text": "they can bind together, OK? But anyway, they are like\na graph-level prediction.",
    "start": "957190",
    "end": "963130"
  },
  {
    "text": "When we have a 3D\nstructure, we want to have some prediction of the\nproperty of this 3D structure.",
    "start": "963130",
    "end": "971980"
  },
  {
    "text": "And one interesting here-- one interesting thing\nhere is when-- how",
    "start": "971980",
    "end": "979360"
  },
  {
    "text": "we represent the 3D structure. So it has the n by 3. So when we have a node,\nit has n by 3 matrix",
    "start": "979360",
    "end": "987580"
  },
  {
    "text": "to represent the coordinates. So every node will\nhave a coordinate. So a node have n by 3 matrix\nto represent the coordinates.",
    "start": "987580",
    "end": "996500"
  },
  {
    "text": "But when we do some rotation or\ntranslation of this structure, this [INAUDIBLE] tensor\ndefinitely will change.",
    "start": "996500",
    "end": "1004548"
  },
  {
    "text": "But no matter how it\nchange or how it rotate or how it translate, it is\nstill the identical structure.",
    "start": "1004548",
    "end": "1012130"
  },
  {
    "text": "And we really hope the\nprediction should be the same. So this is the high level idea\nof what we are concerned about",
    "start": "1012130",
    "end": "1020740"
  },
  {
    "text": "to develop our neural network. So it should capture\nsuch symmetry",
    "start": "1020740",
    "end": "1025780"
  },
  {
    "text": "of this physical world, OK? So this one is for\nsingle number prediction.",
    "start": "1025780",
    "end": "1033750"
  },
  {
    "text": "For example, some property,\nso we just predict a value. Here we also have some\nother application like--",
    "start": "1033750",
    "end": "1043230"
  },
  {
    "text": "this one, like in\ncomputer vision. We usually call this\nkind of application",
    "start": "1043230",
    "end": "1048630"
  },
  {
    "text": "as this application. I mean, for example, if we\ndo image classification, we just want to output\nwhich class it is.",
    "start": "1048630",
    "end": "1057179"
  },
  {
    "text": "By analogy, when we want\nto do like a segmentation of this image, we will output\na very dense output of all",
    "start": "1057180",
    "end": "1065220"
  },
  {
    "text": "the pixels to show the segment. So this is kind of similar.",
    "start": "1065220",
    "end": "1070679"
  },
  {
    "text": "I call it a dense output\nor dense prediction. So for molecular\nsimulation, I think",
    "start": "1070680",
    "end": "1079330"
  },
  {
    "text": "it should be covered in high\nschool physics or university physics.",
    "start": "1079330",
    "end": "1085030"
  },
  {
    "text": "So something like, we can start\nfrom a very unstable structure.",
    "start": "1085030",
    "end": "1090400"
  },
  {
    "text": "And molecular simulation\nmeans we really want to know what is the stable\nstructure of this molecule.",
    "start": "1090400",
    "end": "1097520"
  },
  {
    "text": "And what we do is\nwe have some method to calculate the electric\nforce between different atoms.",
    "start": "1097520",
    "end": "1104980"
  },
  {
    "text": "And when we have\nthis kind of force, the force can tell us how to\nmove this kind of molecules.",
    "start": "1104980",
    "end": "1112510"
  },
  {
    "text": "And we move again, move\nagain, calculate the force, move the structure,\ncalculate the force,",
    "start": "1112510",
    "end": "1119590"
  },
  {
    "text": "update the structure. And finally, we hope that we can\nhave a very stable structure,",
    "start": "1119590",
    "end": "1125450"
  },
  {
    "text": "which means the-- when we calculate\nthe force again,",
    "start": "1125450",
    "end": "1131150"
  },
  {
    "text": "we hopefully-- hopefully,\nthe force is 0 now finally. So which means we get\na stable structure.",
    "start": "1131150",
    "end": "1139529"
  },
  {
    "text": "And here, so very\ntraditional simulator. So it need--\nactually, I think they",
    "start": "1139530",
    "end": "1148520"
  },
  {
    "text": "saw something like a\nSchrodinger equation and calculate the force.",
    "start": "1148520",
    "end": "1153740"
  },
  {
    "text": "So really expensive. And here, what we want to do\nis like a Schrodinger equation",
    "start": "1153740",
    "end": "1161120"
  },
  {
    "text": "to collect many structure\nand their corresponding force structure and the\ncorresponding force.",
    "start": "1161120",
    "end": "1167720"
  },
  {
    "text": "And we want to\nuse neural network to learn what's the force\non current structure.",
    "start": "1167720",
    "end": "1175580"
  },
  {
    "text": "So it has this prediction. I mean, the output\nis also n by 3.",
    "start": "1175580",
    "end": "1181159"
  },
  {
    "text": "So if we have n nodes,\nwe have n by 3 dimension",
    "start": "1181160",
    "end": "1186200"
  },
  {
    "text": "of forces acting on every node,\nevery atom, and move this atom. So here is another\ninteresting symmetry, right?",
    "start": "1186200",
    "end": "1194899"
  },
  {
    "text": "So considering we do\nthis kind of simulation,",
    "start": "1194900",
    "end": "1200150"
  },
  {
    "text": "we can also rotate and translate\nthe input molecule structure.",
    "start": "1200150",
    "end": "1205700"
  },
  {
    "text": "And how should the force move? So it should rotate\ntogether, right?",
    "start": "1205700",
    "end": "1212780"
  },
  {
    "text": "So when we rotate\nan input structure, the force is mainly\nby like interactions.",
    "start": "1212780",
    "end": "1222520"
  },
  {
    "text": "So the output should\nrotate together. And considering translation,\nno matter how we translate,",
    "start": "1222520",
    "end": "1229250"
  },
  {
    "text": "force will stay the same. So this is another symmetry\nof this kind of task, OK?",
    "start": "1229250",
    "end": "1237929"
  },
  {
    "text": "And finally, a very\nbrief idea of what",
    "start": "1237930",
    "end": "1243330"
  },
  {
    "text": "we can do in most modern\nmachine learning, especially in the era of AIGC.",
    "start": "1243330",
    "end": "1249570"
  },
  {
    "text": "So every day, we can see\nStable Diffusion generate some fancy images,\nand also ChatGPT",
    "start": "1249570",
    "end": "1255900"
  },
  {
    "text": "generate some fancy stories. So there is also this kind\nof very exciting progress",
    "start": "1255900",
    "end": "1263250"
  },
  {
    "text": "in graphs. So every day we see many\npharma company, drug company. They actually seem like want\nto solve this drug development",
    "start": "1263250",
    "end": "1272700"
  },
  {
    "text": "problem. They want the-- you give like\nyour COVID virus protein,",
    "start": "1272700",
    "end": "1280380"
  },
  {
    "text": "and they can generate\nsome molecule. Probably you can find\nit on the protein.",
    "start": "1280380",
    "end": "1285840"
  },
  {
    "text": "And they can also-- another promising direction is\nlike they also have some humans",
    "start": "1285840",
    "end": "1291780"
  },
  {
    "text": "antibody on this virus protein. And they can modify the antibody\nto make it more effective",
    "start": "1291780",
    "end": "1298030"
  },
  {
    "text": "and develop method to\nsynthesize that antibody. But anyway, just built on\nthe geometric deep learning,",
    "start": "1298030",
    "end": "1308710"
  },
  {
    "text": "develop new generative\nmodel for these geometries. ",
    "start": "1308710",
    "end": "1315010"
  },
  {
    "text": "OK. So here I want to\nformally like chat",
    "start": "1315010",
    "end": "1322060"
  },
  {
    "text": "about the obstacle or\ndifficulty of this area. So formally, we can see,\nwhen we have a structure--",
    "start": "1322060",
    "end": "1331510"
  },
  {
    "text": "OK, this structure is here. But we don't know\nhow to construct our coordinate system.",
    "start": "1331510",
    "end": "1337490"
  },
  {
    "text": "We have multiple\ndifferent way to construct our coordinate system. And then the coordinates\nof current geometry",
    "start": "1337490",
    "end": "1345760"
  },
  {
    "text": "will be different. But the atom or the--",
    "start": "1345760",
    "end": "1352681"
  },
  {
    "text": "not the atom-- the objective\nor the molecule, no matter in which coordinate system,\nit should be the same one.",
    "start": "1352681",
    "end": "1360730"
  },
  {
    "text": "Or in other words, if we\nfix the coordinate system, it can be seen as like we\nhave a molecule structure.",
    "start": "1360730",
    "end": "1369880"
  },
  {
    "text": "And we do multiple like\nrotation and translation to move it all around\nthe [INAUDIBLE] world.",
    "start": "1369880",
    "end": "1377930"
  },
  {
    "text": "But it is still the same. It should have\nthe same property. Something like this.",
    "start": "1377930",
    "end": "1385000"
  },
  {
    "text": "However, consider how we\nuse traditional graph neural",
    "start": "1385000",
    "end": "1391370"
  },
  {
    "text": "network to process\nour node feature. A naive way to process the--",
    "start": "1391370",
    "end": "1397970"
  },
  {
    "text": "as I introduced before,\nfor the geometric graph, the only additional feature\nis the n by 3 coordinates.",
    "start": "1397970",
    "end": "1405240"
  },
  {
    "text": "So how to process this\ninformation coordinates? A very naive way is just, oh,\nit is node feature, right?",
    "start": "1405240",
    "end": "1412880"
  },
  {
    "text": "Every node has coordinates. Just concatenate it\nto node embedding and make it together\nas a node embedding",
    "start": "1412880",
    "end": "1421400"
  },
  {
    "text": "and do message passing. The problem seems\nlike it is solved.",
    "start": "1421400",
    "end": "1427570"
  },
  {
    "text": "Is it really solved? Definitely not, right? So we can translate and rotate.",
    "start": "1427571",
    "end": "1432830"
  },
  {
    "text": "And when we\ntranslate and rotate, the coordinate will change. And when the coordinate\nchange after we",
    "start": "1432830",
    "end": "1439100"
  },
  {
    "text": "input into a traditional\ngraph neural network, I would know the\nembedding will change.",
    "start": "1439100",
    "end": "1444559"
  },
  {
    "text": "And the output will change. The prediction will change. This is definitely\nnot what we want.",
    "start": "1444560",
    "end": "1450220"
  },
  {
    "text": "So yeah, this is the problem. Traditional GNN, when\nthey see a different",
    "start": "1450220",
    "end": "1459390"
  },
  {
    "text": "like location or orientation\nof an identical geometry,",
    "start": "1459390",
    "end": "1464460"
  },
  {
    "text": "they don't know\nthey are the same. They say, oh, they\nare different.",
    "start": "1464460",
    "end": "1469705"
  },
  {
    "text": "OK.  So what we want to do is, let's\ntry to think about some novel",
    "start": "1469705",
    "end": "1477539"
  },
  {
    "text": "method to make our\ngraph neural network can see, oh, these\ntwo geometry, I",
    "start": "1477540",
    "end": "1484410"
  },
  {
    "text": "know they are\ndescribed differently, but they should be the same. And when we make prediction, the\nprediction should be the same.",
    "start": "1484410",
    "end": "1491250"
  },
  {
    "text": " OK. And the before slide is\nmainly about the input space.",
    "start": "1491250",
    "end": "1501340"
  },
  {
    "text": "So when input space\nchange the symmetry, rotated or translated\nhere, we want",
    "start": "1501340",
    "end": "1509400"
  },
  {
    "text": "to make the prediction\nstill the same. But for the output space,\nthere is also some symmetry",
    "start": "1509400",
    "end": "1514799"
  },
  {
    "text": "which I have mentioned before. Like when we do\noptimization or simulation",
    "start": "1514800",
    "end": "1521340"
  },
  {
    "text": "of the molecular structure,\nwhen we rotate the input,",
    "start": "1521340",
    "end": "1526350"
  },
  {
    "text": "we want to make the predicted\nforce also rotated together.",
    "start": "1526350",
    "end": "1532059"
  },
  {
    "text": "But when we do translation, we\nhope, oh, the predicted force should stay the same.",
    "start": "1532060",
    "end": "1538240"
  },
  {
    "text": "So there is also symmetry\nconsideration of the output space, so which means--",
    "start": "1538240",
    "end": "1544950"
  },
  {
    "text": "which we use the\nword equivariant. So input change,\noutput change together.",
    "start": "1544950",
    "end": "1550750"
  },
  {
    "text": "So yeah, this is formally\nnamed as equivariance.",
    "start": "1550750",
    "end": "1555770"
  },
  {
    "text": "OK. So this is the formal\ndefinition of equivariance.",
    "start": "1555770",
    "end": "1561490"
  },
  {
    "text": "So firstly, we can abstract\nour graph neural network as a function, so\nthe function F.",
    "start": "1561490",
    "end": "1567820"
  },
  {
    "text": "We have the input space\nX and the output space Y. So in the example here, so our\ninput space is the molecule",
    "start": "1567820",
    "end": "1577450"
  },
  {
    "text": "structure and the output\nspace is the force. Or more formally\nhere, we're only--",
    "start": "1577450",
    "end": "1583610"
  },
  {
    "text": "we are mainly interested\nin the 3D structure. So maybe you can\nabstract the problem as input, coordinates,\nand output, the force.",
    "start": "1583610",
    "end": "1592149"
  },
  {
    "text": "So equivariance means when we\nhave this kind of function,",
    "start": "1592150",
    "end": "1598560"
  },
  {
    "text": "we can apply some transformation\nof the input space. We name it like row\nX And we similarly",
    "start": "1598560",
    "end": "1607860"
  },
  {
    "text": "want to make some transformation\nin the output space, we name it row Y. So equivariance formally means\nfor every action in the X",
    "start": "1607860",
    "end": "1616590"
  },
  {
    "text": "space, there is a corresponding\naction in the Y space. They are bijective.",
    "start": "1616590",
    "end": "1622770"
  },
  {
    "text": "So a very natural choice is\nthey should be rotated together",
    "start": "1622770",
    "end": "1627990"
  },
  {
    "text": "and translated-- oh, they are not\ntranslated together. OK. Just think about rotation first.",
    "start": "1627990",
    "end": "1634410"
  },
  {
    "text": "They are-- the rotation\nis input how rotate. Output should rotate in\nexactly the same way.",
    "start": "1634410",
    "end": "1642610"
  },
  {
    "text": "So this is the\nformal introduction of equivariance, yeah.",
    "start": "1642610",
    "end": "1648030"
  },
  {
    "text": "But-- yes, so translation\nis not the case. But still call it equivariance.",
    "start": "1648030",
    "end": "1654090"
  },
  {
    "text": "We will present it later. So here is some\nvery intuitive way",
    "start": "1654090",
    "end": "1661740"
  },
  {
    "text": "to have a sense of\nwhat is equivariance. So maybe look at this first.",
    "start": "1661740",
    "end": "1668910"
  },
  {
    "text": "We have such structure, and\nwe make false prediction. OK, we have all the directions.",
    "start": "1668910",
    "end": "1674740"
  },
  {
    "text": "And if we do a row, so it\nis some transformation. Or in practice, it is\nrotation of this system.",
    "start": "1674740",
    "end": "1682020"
  },
  {
    "text": "And then the forces should\nrotate in the same way. And here is an even\nmore informative way",
    "start": "1682020",
    "end": "1690640"
  },
  {
    "text": "to illustrate the equivariance. When we rotate the\nstructure, the two arrows",
    "start": "1690640",
    "end": "1697750"
  },
  {
    "text": "rotate together, which indicates\nthe force rotate together. ",
    "start": "1697750",
    "end": "1706159"
  },
  {
    "text": "OK. So beyond the\nequivariance, here we also define what is invariance.",
    "start": "1706160",
    "end": "1713910"
  },
  {
    "text": "Invariance is kind of similar. So we have a function,\nsimilar function. Here, input an image.",
    "start": "1713910",
    "end": "1720480"
  },
  {
    "text": "And we input-- oh, who\nis this famous guy? OK. So here-- this one, OK, yeah,\nit's Professor Leskovec, right?",
    "start": "1720480",
    "end": "1731370"
  },
  {
    "text": "And then there is some\nrotation and translation, but you can still know this\nis still Professor Leskovec.",
    "start": "1731370",
    "end": "1738420"
  },
  {
    "text": "This is not Minkai, right? And yeah, this is the formal\ndefinition of invariance.",
    "start": "1738420",
    "end": "1747030"
  },
  {
    "text": "So still the space X,\nmaybe it is an image. And we do some transformation\nof the image and the input",
    "start": "1747030",
    "end": "1755159"
  },
  {
    "text": "to the function F, which\nis some neural network. And we hope that no matter\nhow we apply the row",
    "start": "1755160",
    "end": "1761669"
  },
  {
    "text": "X we see the same prediction. This is invariance.",
    "start": "1761670",
    "end": "1767290"
  },
  {
    "text": "And this is a lecture, so I\nneed to present everything",
    "start": "1767290",
    "end": "1772570"
  },
  {
    "text": "in a rigorous way. So formally, invariance is a\nspecial case of equivariance.",
    "start": "1772570",
    "end": "1778720"
  },
  {
    "text": "So look like invariant\nand equivariant.",
    "start": "1778720",
    "end": "1785890"
  },
  {
    "text": "Actually, invariant\nis just a special case of the equivariance,\nwhich means row y is",
    "start": "1785890",
    "end": "1791800"
  },
  {
    "text": "defined as like a network. Apply any action. So here means no matter how you\nrotate or translate your input,",
    "start": "1791800",
    "end": "1801070"
  },
  {
    "text": "the output space transformation\nis no transformation.",
    "start": "1801070",
    "end": "1807500"
  },
  {
    "text": "Sorry? This is the bijectivity though\nthat you defined before. You said that equivariant\nfunctions have to be--",
    "start": "1807500",
    "end": "1814790"
  },
  {
    "text": "have a bijective relation\nbetween PX and PY. Yes. Indeed. I think-- I realized I make some\nwrong statement of bijective.",
    "start": "1814790",
    "end": "1823730"
  },
  {
    "text": "Yeah. Sorry. But yeah, thank you. You are listening\nvery carefully, yeah.",
    "start": "1823730",
    "end": "1831230"
  },
  {
    "text": "So the row Y here is defined\nas no transformation.",
    "start": "1831230",
    "end": "1836549"
  },
  {
    "text": "So formally, invariance is a\nspecial case of equivariance.",
    "start": "1836550",
    "end": "1841880"
  },
  {
    "text": "But in this lecture,\nI will just-- I don't want to lose the\nmeaning of these two words.",
    "start": "1841880",
    "end": "1849180"
  },
  {
    "text": "So when I mention\nequivariance, I just always want to see, oh,\nthey changed together.",
    "start": "1849180",
    "end": "1854490"
  },
  {
    "text": "And when I mention\ninvariance later, I just want to see there will\nbe no change of output space.",
    "start": "1854490",
    "end": "1860640"
  },
  {
    "text": "OK. But formally or\ntheoretically, invariance is a special case\nof equivariance.",
    "start": "1860640",
    "end": "1866090"
  },
  {
    "text": " OK. And here I just have\na brief introduction",
    "start": "1866090",
    "end": "1874789"
  },
  {
    "text": "of all the tasks we will\nconcentrate on later for all",
    "start": "1874790",
    "end": "1880160"
  },
  {
    "text": "the neural network design. So what we talk about\ntoday is a special--",
    "start": "1880160",
    "end": "1887660"
  },
  {
    "text": "mathematically, it's a\nspecial group in our world.",
    "start": "1887660",
    "end": "1894510"
  },
  {
    "text": "So it is called a Special\nEuclidean group 3, SE 3 group.",
    "start": "1894510",
    "end": "1900260"
  },
  {
    "text": "SE 3 group means we\nconsider the transformation as a combination of\nrotation and translation.",
    "start": "1900260",
    "end": "1906900"
  },
  {
    "text": "So a little bit more. So there are also just a typical\nEuclidean space, E3 space.",
    "start": "1906900",
    "end": "1915510"
  },
  {
    "text": "So in E3 space, the additional\naction is a reflection.",
    "start": "1915510",
    "end": "1920585"
  },
  {
    "text": "But in molecule, when\nyou reflect a molecule, actually the\nproperty will change.",
    "start": "1920585",
    "end": "1925800"
  },
  {
    "text": "So we never consider reflection. We only consider\nrotation and translation. But anyway, I just want\nto have an introduction",
    "start": "1925800",
    "end": "1935030"
  },
  {
    "text": "of what actions-- what\ntransformations we will consider today. And the two main task are\nalso actually introduced.",
    "start": "1935030",
    "end": "1944270"
  },
  {
    "text": "So a very classic problem\nis to energy prediction.",
    "start": "1944270",
    "end": "1950160"
  },
  {
    "text": "So every structure, we\nhave a potential energy. So how many potential\nenergy it has?",
    "start": "1950160",
    "end": "1956670"
  },
  {
    "text": "So no matter how you\nrotate or translate, it should have the same energy. So it is a very typical\ninvariant prediction task.",
    "start": "1956670",
    "end": "1964549"
  },
  {
    "text": "And another task is we want\nto do force prediction. And as introduced\nbefore, it should be",
    "start": "1964550",
    "end": "1971240"
  },
  {
    "text": "equivalent to prediction task.  By the equivariant,\nactually, it means",
    "start": "1971240",
    "end": "1977870"
  },
  {
    "text": "equivariant through\nrotation, but the invariant through translation. Yeah, we want to do\nthis kind of prediction.",
    "start": "1977870",
    "end": "1983900"
  },
  {
    "text": " There is some analogy\nin image domain.",
    "start": "1983900",
    "end": "1990420"
  },
  {
    "text": "What is invariant\nand equivariant? So when we do like\na classification,",
    "start": "1990420",
    "end": "1997680"
  },
  {
    "text": "no matter how you\nchange, stay the same. But when we do\nsegmentation, what we",
    "start": "1997680",
    "end": "2004220"
  },
  {
    "text": "output is on every\npixel coordinates or every pixel position. So they will also\nchange together",
    "start": "2004220",
    "end": "2011450"
  },
  {
    "text": "considering rotation\nand translation. So this is equivariance in\nthe computer vision domain.",
    "start": "2011450",
    "end": "2017240"
  },
  {
    "text": " OK. A summary of all the\ndata and the symmetry",
    "start": "2017240",
    "end": "2025720"
  },
  {
    "text": "we need to consider. And here is a lot of content. But maybe I can\nconcentrate on a graph",
    "start": "2025720",
    "end": "2034059"
  },
  {
    "text": "to make some analogy\non the graph. So graph actually is a\npermutation symmetry group.",
    "start": "2034060",
    "end": "2041120"
  },
  {
    "text": "So by permutation\nsymmetry, we can also consider two types of\nprediction, a scalar prediction",
    "start": "2041120",
    "end": "2050020"
  },
  {
    "text": "and a dense prediction. OK. Scalar prediction means we want\nto do graph-level prediction.",
    "start": "2050020",
    "end": "2057310"
  },
  {
    "text": "So we predefine\norder of this graph. And in PyTorch or TensorFlow,\nit is stored as a matrix.",
    "start": "2057310",
    "end": "2067219"
  },
  {
    "text": "So different nodes, they are\ncombined together as a matrix.",
    "start": "2067219",
    "end": "2072869"
  },
  {
    "text": "But actually, we can change\nthe order of different nodes. They are arbitrarily stored.",
    "start": "2072870",
    "end": "2078120"
  },
  {
    "text": "And when we do a\ngraph-level prediction, no matter what order\nis the different node,",
    "start": "2078120",
    "end": "2085230"
  },
  {
    "text": "we should do the\nsame prediction. So it means the graph never\nchange with different order",
    "start": "2085230",
    "end": "2091770"
  },
  {
    "text": "of the nodes. So this is permutation\ninvariant when we do graph-level prediction.",
    "start": "2091770",
    "end": "2097109"
  },
  {
    "text": "We are also sometimes interested\nin prediction for every node, so node-level prediction.",
    "start": "2097110",
    "end": "2104230"
  },
  {
    "text": "For node-level\nprediction, when we change the order of the\ninput of different node, the output should\nchange accordingly.",
    "start": "2104230",
    "end": "2111790"
  },
  {
    "text": " So this is called\npermutation equivariant.",
    "start": "2111790",
    "end": "2117910"
  },
  {
    "text": "So they should change\nin the same way. So I think this\nis also the design",
    "start": "2117910",
    "end": "2124720"
  },
  {
    "text": "philosophy of graph neural\nnetwork to some perspective. The message passing, when\nyou do message passing,",
    "start": "2124720",
    "end": "2131830"
  },
  {
    "text": "you never consider the\norder of your neighborhood. So the message passing network\nis permutation invariant.",
    "start": "2131830",
    "end": "2137890"
  },
  {
    "text": "And I think this is also one\nof the reason of the success",
    "start": "2137890",
    "end": "2143319"
  },
  {
    "text": "of graph neural network. OK. So now let me\nintroduce how to do",
    "start": "2143320",
    "end": "2150849"
  },
  {
    "text": "graph neural network for this\nkind of symmetry or symmetric data.",
    "start": "2150850",
    "end": "2155905"
  },
  {
    "text": " OK.",
    "start": "2155905",
    "end": "2160990"
  },
  {
    "text": "Although our typical\ngraph neural network couldn't capture\nthe symmetry, we can do some data\naugmentation, right?",
    "start": "2160990",
    "end": "2166750"
  },
  {
    "text": "This is also, I think, used\na lot in computer vision. When they do like\na classification,",
    "start": "2166750",
    "end": "2175810"
  },
  {
    "text": "they randomly rotate\nand translate the image and force the model to\nlearn-- force the model",
    "start": "2175810",
    "end": "2182320"
  },
  {
    "text": "to learn the same prediction. Here we can do the same thing. With a single data, we can just\nenumerate all the positions",
    "start": "2182320",
    "end": "2190360"
  },
  {
    "text": "and force the model to\nmake the same prediction. But this is very expensive,\nvery time consuming, and also",
    "start": "2190360",
    "end": "2200589"
  },
  {
    "text": "not very natural and elegant. And here, what we want to\ndo is we should develop",
    "start": "2200590",
    "end": "2206410"
  },
  {
    "text": "a GNN aware of symmetry. Just one data is enough. Here is some intuition.",
    "start": "2206410",
    "end": "2215010"
  },
  {
    "text": "So when you network design the\ngeometric graph neural network, you allow yourself to explore\nin the whole learnable function",
    "start": "2215010",
    "end": "2223950"
  },
  {
    "text": "space. And finally, your data\nconstrain your model",
    "start": "2223950",
    "end": "2229650"
  },
  {
    "text": "to be some subspace\nbut still very large. But here, what we\nwant to do is you",
    "start": "2229650",
    "end": "2235590"
  },
  {
    "text": "need to constrain your neural\nnetwork to make it to be",
    "start": "2235590",
    "end": "2241200"
  },
  {
    "text": "just the invariant functions. And you significantly constrain\nthe function space you explore.",
    "start": "2241200",
    "end": "2248700"
  },
  {
    "text": "And hopefully, you\ncan effectively learn what you want. ",
    "start": "2248700",
    "end": "2255960"
  },
  {
    "text": "OK. So today I will introduce two\nkind of graph neural network.",
    "start": "2255960",
    "end": "2261809"
  },
  {
    "text": "One type is invariant\ngraph neural network. And another type is\nequivariant graph--",
    "start": "2261810",
    "end": "2267270"
  },
  {
    "text": "sorry. You go back to the\nprevious slide. Here, just clarifying, are\nyou using the phrase geometric function to mean the same\nthing as equivariant function?",
    "start": "2267270",
    "end": "2274575"
  },
  {
    "text": " Geometric function, equivariant\nfunction, not the same.",
    "start": "2274575",
    "end": "2283790"
  },
  {
    "text": "So geometric graph\nneural network has many, many\ndifferent classes. So we will introduce later.",
    "start": "2283790",
    "end": "2292000"
  },
  {
    "text": "So many different\ngraph neural network. And our ideal goal is to\ndevelop very effective",
    "start": "2292000",
    "end": "2299290"
  },
  {
    "text": "geometric functions. But currently, we still\ndon't have enough knowledge",
    "start": "2299290",
    "end": "2305440"
  },
  {
    "text": "to fully capture all the\ngeometry information. So later, I will introduce more.",
    "start": "2305440",
    "end": "2312460"
  },
  {
    "text": "OK. Invariant graph neural network\nand equivariant graph neural network. OK.",
    "start": "2312460",
    "end": "2317660"
  },
  {
    "text": "So here are mainly two classes\nof graph neural network.",
    "start": "2317660",
    "end": "2323480"
  },
  {
    "text": "Invariant means we\nmake some-- we only act on some invariant\nfeature, while",
    "start": "2323480",
    "end": "2330369"
  },
  {
    "text": "equivariant neural\nnetwork act also on some directional feature. I will introduce\nthem more later.",
    "start": "2330370",
    "end": "2336740"
  },
  {
    "text": " Before formally introduce\nthe neural network, I can--",
    "start": "2336740",
    "end": "2344690"
  },
  {
    "text": " actually, I think I have\nintroduced all the information",
    "start": "2344690",
    "end": "2351170"
  },
  {
    "text": "before, like\nsimulation needed to-- what is molecular simulation?",
    "start": "2351170",
    "end": "2358230"
  },
  {
    "text": "Oh, maybe another\nknowledge is when we do molecular\nsimulation, I just",
    "start": "2358230",
    "end": "2364710"
  },
  {
    "text": "explained from the force view. Actually, you can\nalso understand it",
    "start": "2364710",
    "end": "2370650"
  },
  {
    "text": "from the energy view. So when we do\nmolecular simulation, we actually want to find the\nlocal minimum of the energy",
    "start": "2370650",
    "end": "2378510"
  },
  {
    "text": "surface. So we want to find the\nstructure of the lowest energy. And what we do is\nusing the force",
    "start": "2378510",
    "end": "2385799"
  },
  {
    "text": "to optimize the structure. And the force\nactually can be viewed as the gradient of energy,\nso negative gradient.",
    "start": "2385800",
    "end": "2396030"
  },
  {
    "text": "So it will drive the structure\ntowards lower and lower energy. And finally, we can\nget the local minimum",
    "start": "2396030",
    "end": "2404190"
  },
  {
    "text": "of the energy function. And it is a stable structure. ",
    "start": "2404190",
    "end": "2409730"
  },
  {
    "text": "OK. Yeah, we can use force to\noptimize the structure. ",
    "start": "2409730",
    "end": "2416590"
  },
  {
    "text": "OK. So let's see the task, how\nwe train this kind of model.",
    "start": "2416590",
    "end": "2422340"
  },
  {
    "text": "We have item type. And we have their positions,\nthe 3D coordinates.",
    "start": "2422340",
    "end": "2428350"
  },
  {
    "text": "And what we want to learn is\nwe want to predict the energy. And we want to\npredict their force.",
    "start": "2428350",
    "end": "2434970"
  },
  {
    "text": "And the first class model\nwe will introduce today, they just learn how\nto predict the energy.",
    "start": "2434970",
    "end": "2442020"
  },
  {
    "text": "And for force, although\nit should be equivariant, it can actually\nbe calculated by--",
    "start": "2442020",
    "end": "2449580"
  },
  {
    "text": "if energy is learned\nby some neural network, we can just take\nthis neural network,",
    "start": "2449580",
    "end": "2454740"
  },
  {
    "text": "calculate the gradient of\nthe output towards input. And this gradient can be viewed\nas the forced prediction.",
    "start": "2454740",
    "end": "2462240"
  },
  {
    "text": " OK. Now introduce the first class\nof geometric function, which is",
    "start": "2462240",
    "end": "2471000"
  },
  {
    "text": "invariant graph neural network. This is SchNet, I think has been\nmany years, five years, yeah.",
    "start": "2471000",
    "end": "2479700"
  },
  {
    "text": "This is the first-- I think one of the first graph\nneural network on 3D structure.",
    "start": "2479700",
    "end": "2486040"
  },
  {
    "text": "So it is also still the\nmessage passing framework. So let's see the equation.",
    "start": "2486040",
    "end": "2492060"
  },
  {
    "text": "On the L layer, how we\nupdate the node embedding. The node embedding is\nupdated at-- maybe just",
    "start": "2492060",
    "end": "2498690"
  },
  {
    "text": "look at this equation. W is some neural network\nyou can understand.",
    "start": "2498690",
    "end": "2504690"
  },
  {
    "text": "And what we do is when we went\nto the embedding of this node,",
    "start": "2504690",
    "end": "2510720"
  },
  {
    "text": "we first calculate\nall the direction from the neighborhood\nto this node.",
    "start": "2510720",
    "end": "2517100"
  },
  {
    "text": "And this direction is input\nto the neural network, WL.",
    "start": "2517100",
    "end": "2523115"
  },
  {
    "text": "And W will output some-- like a controller\non this edge which",
    "start": "2523115",
    "end": "2528770"
  },
  {
    "text": "controls how the information\nwill propagate from this node to this node.",
    "start": "2528770",
    "end": "2534040"
  },
  {
    "text": "Something like this. Yeah. OK? So formally, we have WL\nas a graph neural network.",
    "start": "2534040",
    "end": "2543040"
  },
  {
    "text": "This graph neural\nnetwork, the input is the direction from the\nneighborhood to current node.",
    "start": "2543040",
    "end": "2550010"
  },
  {
    "text": "And the output will be some-- like a filter acting on the\nneighborhood's embedding.",
    "start": "2550010",
    "end": "2558320"
  },
  {
    "text": "And finally, all\nthe embedding will be summed together as a message\naggregation, simple message",
    "start": "2558320",
    "end": "2564950"
  },
  {
    "text": "aggregation, just by sum. And this will be used for\nupdating our current node",
    "start": "2564950",
    "end": "2572599"
  },
  {
    "text": "embedding. OK. So let me introduce how\nwe can make it invariant.",
    "start": "2572600",
    "end": "2582380"
  },
  {
    "text": "Yeah, actually, this is very\nsimilar to our previous graph neural network.",
    "start": "2582380",
    "end": "2587910"
  },
  {
    "text": "So the only difference\nis we use some edge feature, which is the direction\nto control the message.",
    "start": "2587910",
    "end": "2596880"
  },
  {
    "text": "And the message just\naggregated together. And how they achieve\ninvariance, this neural network",
    "start": "2596880",
    "end": "2603860"
  },
  {
    "text": "do a very simple thing. So OK, here, the input is\ndirectional thing, right,",
    "start": "2603860",
    "end": "2610820"
  },
  {
    "text": "from neighbor to this node. OK, I can do some\nvery simple thing just",
    "start": "2610820",
    "end": "2617430"
  },
  {
    "text": "to make this tensor to a scalar. So what they do is\ncalled scalarizing.",
    "start": "2617430",
    "end": "2623990"
  },
  {
    "text": "So actually, when the neural\nnetwork input is the direction,",
    "start": "2623990",
    "end": "2630060"
  },
  {
    "text": "they first calculate the\nlength of this direction. So they only use--",
    "start": "2630060",
    "end": "2636569"
  },
  {
    "text": "in other words, only use\nthe distance information of current bond. And the input is bond as the\ncontroller of the message",
    "start": "2636570",
    "end": "2644215"
  },
  {
    "text": "passing. ",
    "start": "2644215",
    "end": "2650700"
  },
  {
    "text": "OK. Yeah. So from this figure\nto the next figure, we actually do some\nrotation of the molecule.",
    "start": "2650700",
    "end": "2657560"
  },
  {
    "text": "But no matter how we\nrotate this molecule, their bond distance\nstay the same.",
    "start": "2657560",
    "end": "2663300"
  },
  {
    "text": "So considering bond distance\nstay the same, so look",
    "start": "2663300",
    "end": "2668758"
  },
  {
    "text": "at the equation.  This input is the\ninvariant distance.",
    "start": "2668758",
    "end": "2675620"
  },
  {
    "text": "And naturally, the output\nof this W is also invariant. And naturally, the message\npassing is invariant.",
    "start": "2675620",
    "end": "2683059"
  },
  {
    "text": "And naturally,\neverything is invariant. So the high level\nidea is, in one word,",
    "start": "2683060",
    "end": "2690170"
  },
  {
    "text": "so no matter how we\nrotate and translate our current structure, their\ninter distance never change.",
    "start": "2690170",
    "end": "2698060"
  },
  {
    "text": "So this kind of\ngraph neural network, the high level idea is just to\ntake the distance as the edge",
    "start": "2698060",
    "end": "2706130"
  },
  {
    "text": "feature, so which\nnever change consider rotation and translation and--",
    "start": "2706130",
    "end": "2711140"
  },
  {
    "text": "yes? How can this network take\ninto account the angle between the two bonds? Yes.",
    "start": "2711140",
    "end": "2717140"
  },
  {
    "text": "This is improved work. So I will introduce later.",
    "start": "2717140",
    "end": "2722180"
  },
  {
    "text": "Yeah, so very simple. Is this just equivalent to\ndropping the X matrix entirely",
    "start": "2722180",
    "end": "2729012"
  },
  {
    "text": "and then augmenting the\nfeature matrix and one column is just the distance?",
    "start": "2729012",
    "end": "2734070"
  },
  {
    "text": "You mean, give up the\ncoordinates tensor and just augment\nthe edge feature?",
    "start": "2734070",
    "end": "2740800"
  },
  {
    "text": "Yeah. Is it equivalent to\nwhat we're doing here? Exactly the same. Actually, yeah.",
    "start": "2740800",
    "end": "2745910"
  },
  {
    "text": "So very intuitive\nunderstanding, so.  Yeah.",
    "start": "2745910",
    "end": "2751070"
  },
  {
    "text": "So yeah, I think the\nidea is most important. So maybe we can skip\nthis kind of details.",
    "start": "2751070",
    "end": "2758870"
  },
  {
    "text": "So for this sense, it\nis just a single value. So they make some method called\nRBF to expand the distance.",
    "start": "2758870",
    "end": "2768800"
  },
  {
    "text": "It is a single value\nto higher dimension and make it a more\ninformative edge embedding.",
    "start": "2768800",
    "end": "2774950"
  },
  {
    "text": "But whatever, just\nsome invariant feature",
    "start": "2774950",
    "end": "2779960"
  },
  {
    "text": "depend on the edge embedding--\non the distance, sorry. ",
    "start": "2779960",
    "end": "2786380"
  },
  {
    "text": "OK. And this is so defined. I think considering\nthe time, maybe stick",
    "start": "2786380",
    "end": "2794990"
  },
  {
    "text": "for this kind of\nnetwork architecture. Yeah, we can skip it.",
    "start": "2794990",
    "end": "2801549"
  },
  {
    "text": "This is just the message\npassing framework we have introduced\nbefore, so cfconv.",
    "start": "2801550",
    "end": "2808310"
  },
  {
    "text": "And what they do is, in\naddition to message passing,",
    "start": "2808310",
    "end": "2813320"
  },
  {
    "text": "they first do message passing\nto aggregate the information and then do some\nitem-wise like updates",
    "start": "2813320",
    "end": "2819950"
  },
  {
    "text": "of the aggregated\ninformation and add it to the node embedding.",
    "start": "2819950",
    "end": "2825380"
  },
  {
    "text": "Something like this. Yeah. Seems like should not\nbe very interesting.",
    "start": "2825380",
    "end": "2831019"
  },
  {
    "text": " OK. And oh--",
    "start": "2831020",
    "end": "2838640"
  },
  {
    "text": "I have a question,\nbut feel free to-- yeah. So a lot of molecules don't\nhave one stable structure.",
    "start": "2838640",
    "end": "2847620"
  },
  {
    "text": "Yes. Right? And they might spontaneously\nisomerize or whatever. And so when you have\nthis representation where",
    "start": "2847620",
    "end": "2856280"
  },
  {
    "text": "you say this is the\ncoordinate of each atom, and then we're going\nto minimize it, and then that's going to\nbe the final structure,",
    "start": "2856280",
    "end": "2861620"
  },
  {
    "text": "can be misleading\nabout what actually is going on chemically. And I guess, I\nwondered if there are",
    "start": "2861620",
    "end": "2866870"
  },
  {
    "text": "alternate representations that\nconsider that more explicitly? ",
    "start": "2866870",
    "end": "2874300"
  },
  {
    "text": "[INAUDIBLE] Like for example, if you\nknow that a molecule can",
    "start": "2874300",
    "end": "2879550"
  },
  {
    "text": "convert between two different-- Yes. 3D structures. Yes. Can we represent both\nof them simultaneously",
    "start": "2879550",
    "end": "2887170"
  },
  {
    "text": "rather than just one? ",
    "start": "2887170",
    "end": "2897210"
  },
  {
    "text": "You mean, you want to\nhave a representation, but it'll capture\nall the information of different stable structure?",
    "start": "2897210",
    "end": "2903390"
  },
  {
    "text": "Yeah. Oh, this would be, I\nthink, difficult. Are you",
    "start": "2903390",
    "end": "2909000"
  },
  {
    "text": "from chemistry department? I know a little bit, yeah. I think they are interested\nin something like--",
    "start": "2909000",
    "end": "2915029"
  },
  {
    "text": "this is why they are interested\nin some generative model because generative model can\ndo unbiased sampling of all",
    "start": "2915030",
    "end": "2921210"
  },
  {
    "text": "these structure. And they do some statistics\nover all the structures. But for only one representation,\nI think it's kind of difficult.",
    "start": "2921210",
    "end": "2930540"
  },
  {
    "text": "It is always some statistical\nthings in chemistry, so always multiple structure\nand do some aggregation.",
    "start": "2930540",
    "end": "2939359"
  },
  {
    "text": "Yeah. OK. Yeah. And finally, we can make some\nprediction on every node,",
    "start": "2939360",
    "end": "2949410"
  },
  {
    "text": "and sum them together\nas an energy prediction. Yeah, very intuitive.",
    "start": "2949410",
    "end": "2955059"
  },
  {
    "text": "And this is the\ntraining objective. So maybe you can take\na look in this model. So there is a predicted\nenergy, and we",
    "start": "2955060",
    "end": "2962550"
  },
  {
    "text": "do the mean squared error loss\nof the ground truth energy. And we can also, since\nthis is a graph--",
    "start": "2962550",
    "end": "2969450"
  },
  {
    "text": "this is a neural network, we\ncan take some back propagation to calculate the gradient\nwith respect to the input",
    "start": "2969450",
    "end": "2977970"
  },
  {
    "text": "and take it as the force to\ndo some supervised learning on the force. ",
    "start": "2977970",
    "end": "2985540"
  },
  {
    "text": "OK. Yeah. And for the improved\nversion of DimeNet, I think I don't want to\nintroduce the detail.",
    "start": "2985540",
    "end": "2993190"
  },
  {
    "text": "But yeah, the same motivation\nas [INAUDIBLE],, right?",
    "start": "2993190",
    "end": "3000210"
  },
  {
    "text": "I forgot your name. Hi, you are concerned\nabout the angles?",
    "start": "3000210",
    "end": "3007620"
  },
  {
    "text": "Yeah. So previously, the\nmessage passing is only conducted by taking the\ndistance as the edge feature.",
    "start": "3007620",
    "end": "3018990"
  },
  {
    "text": "But very obviously,\nthe distance is not enough for describing\nthe energy.",
    "start": "3018990",
    "end": "3026490"
  },
  {
    "text": "So chemically, energy have\nmultiple different components, like not only the two atom.",
    "start": "3026490",
    "end": "3035970"
  },
  {
    "text": "The pairwise distance count. Also, the angle and torsion\nwill make some difference",
    "start": "3035970",
    "end": "3041820"
  },
  {
    "text": "to the energy. So the high-level\nidea of DimeNet is, OK, in addition\nto distance, I also",
    "start": "3041820",
    "end": "3047730"
  },
  {
    "text": "need to consider some\nother quantities. And they incorporate the angle\ninto the message passing.",
    "start": "3047730",
    "end": "3054930"
  },
  {
    "text": "But I don't want to really\ndelve into the details. You can just think, OK,\nthere is also some angle.",
    "start": "3054930",
    "end": "3063030"
  },
  {
    "text": "I can somehow make them\ninto the edge embedding for message passing, yes. And the output is\nstill invariant.",
    "start": "3063030",
    "end": "3070650"
  },
  {
    "text": "Because no matter how\nyou rotate and translate, distance and angles\nall stay the same.",
    "start": "3070650",
    "end": "3076860"
  },
  {
    "text": "We already call them\ninternal coordinates, so they never change.",
    "start": "3076860",
    "end": "3082090"
  },
  {
    "text": "However, all these\nkind of methods relied on scalarization.",
    "start": "3082090",
    "end": "3090400"
  },
  {
    "text": "So we reduce the coordinates\ninto angles and distance. Is them sufficient\ndescription of the geometry?",
    "start": "3090400",
    "end": "3099579"
  },
  {
    "text": "Actually, definitely not. So consider these two\nmolecules' structure.",
    "start": "3099580",
    "end": "3105520"
  },
  {
    "text": "So they have identical\ndistance and identical angles. So when we do that kind\nof invariant prediction--",
    "start": "3105520",
    "end": "3117010"
  },
  {
    "text": "for invariant\nprediction, it's fine. So we can ensure that\nprediction is invariant.",
    "start": "3117010",
    "end": "3122710"
  },
  {
    "text": "But as you can see,\nsome information is lost in this\nkind of structure.",
    "start": "3122710",
    "end": "3127970"
  },
  {
    "text": "So these two, our prediction\nwill also be the same. This is wrong.",
    "start": "3127970",
    "end": "3133150"
  },
  {
    "text": "The invariant is a little\nbit over invariant. But consider if we can truly\nincorporate some direction",
    "start": "3133150",
    "end": "3142850"
  },
  {
    "text": "or tensor into the framework. Like when we add some more\ndescription of these two node",
    "start": "3142850",
    "end": "3149120"
  },
  {
    "text": "with their middle direction\nof their two bonds,",
    "start": "3149120",
    "end": "3154590"
  },
  {
    "text": "they can be distinguished now. So we have some tensor feature. So the next large\nclass of geometric GNN",
    "start": "3154590",
    "end": "3162710"
  },
  {
    "text": "is how to incorporate this\nkind of tensor feature. And I call it--",
    "start": "3162710",
    "end": "3168200"
  },
  {
    "text": "I will call it an equivariant\ngraph neural network because this kind of\ndirectional tensor",
    "start": "3168200",
    "end": "3174290"
  },
  {
    "text": "will be equivariant with\nthe input coordinates. So it always store some\nequivariant feature",
    "start": "3174290",
    "end": "3181910"
  },
  {
    "text": "during message passing.  OK. So another motivation for now\nwhy we are not satisfied just",
    "start": "3181910",
    "end": "3193840"
  },
  {
    "text": "with the invariant\ngraph neural network.  So we have a class of\nall the neural network",
    "start": "3193840",
    "end": "3201500"
  },
  {
    "text": "can fully capture all the\ngeometric information. And what we want\nis the orange part.",
    "start": "3201500",
    "end": "3207500"
  },
  {
    "text": "But now, when we constrain our-- when we constrain to\nonly invariant features",
    "start": "3207500",
    "end": "3213440"
  },
  {
    "text": "like distance and\nangles, actually, we constrain ourselves\nto really a subspace.",
    "start": "3213440",
    "end": "3219950"
  },
  {
    "text": "And maybe we couldn't find\nall the optimal solution. Or in neural network,\nwe usually say,",
    "start": "3219950",
    "end": "3227270"
  },
  {
    "text": "we want to define\nour function as some universal approximation. But here, we constrain\nour function too much.",
    "start": "3227270",
    "end": "3236580"
  },
  {
    "text": "We want to release some\nspace of our neural network. ",
    "start": "3236580",
    "end": "3244069"
  },
  {
    "text": "OK. So high-level idea of the recent\nprogress of equivariant GNN.",
    "start": "3244070",
    "end": "3252900"
  },
  {
    "text": "It is called PaiNN. Yeah, interesting.",
    "start": "3252900",
    "end": "3258349"
  },
  {
    "text": "Yeah, this neural network,\nsome part is same,",
    "start": "3258350",
    "end": "3263570"
  },
  {
    "text": "but some part is different. So the same part is we still\nhave for every-- for every edge",
    "start": "3263570",
    "end": "3270500"
  },
  {
    "text": "feature, we will have a matrix. And the matrix\ncontrol how message is passed through this bond.",
    "start": "3270500",
    "end": "3277160"
  },
  {
    "text": "And this filter or\nthis weight matrix W",
    "start": "3277160",
    "end": "3282890"
  },
  {
    "text": "is still conditional\non just the distance. But what is different\nis, for every node now,",
    "start": "3282890",
    "end": "3289880"
  },
  {
    "text": "we not only have just a\nregular node embedding, as you learned before,\nas I introduced",
    "start": "3289880",
    "end": "3296150"
  },
  {
    "text": "before, the invariant\nnode embedding. We also, for every\nnode after every layer,",
    "start": "3296150",
    "end": "3302930"
  },
  {
    "text": "we give them an\nadditional vector feature. The vector feature will change\ntogether with the input.",
    "start": "3302930",
    "end": "3311380"
  },
  {
    "text": "So just a quick\nintroduction of what",
    "start": "3311380",
    "end": "3317369"
  },
  {
    "text": "we do for the scalar\nfeature and vector feature. So the scalar feature will be\ninitialized in atom embedding",
    "start": "3317370",
    "end": "3325620"
  },
  {
    "text": "or node embedding. So it is very typical. Input one [INAUDIBLE] and\nthere is some embedding.",
    "start": "3325620",
    "end": "3332430"
  },
  {
    "text": "And the vector feature,\nwe will begin with 0. So set them as 0.",
    "start": "3332430",
    "end": "3338850"
  },
  {
    "text": "They are updated with\nsome residual updates. So in every layer, we will learn\nsome updates of both S and V,",
    "start": "3338850",
    "end": "3347940"
  },
  {
    "text": "both the invariant feature\nand the equivariant feature. And these two\nfeature will be used",
    "start": "3347940",
    "end": "3353880"
  },
  {
    "text": "to update either in the\nResNet view, ResNet fashion,",
    "start": "3353880",
    "end": "3359769"
  },
  {
    "text": "so just a plus. But whatever. So now we are interested\nin how, in each layer,",
    "start": "3359770",
    "end": "3367619"
  },
  {
    "text": "we update the S and the V, the\nscalar embedding and the tensor",
    "start": "3367620",
    "end": "3372660"
  },
  {
    "text": "embedding, by delta S and delta\nV So here is the equation, but I will introduce\nmore details later.",
    "start": "3372660",
    "end": "3382329"
  },
  {
    "text": "OK. So first, I'll see how we update\nthe scalar node embedding.",
    "start": "3382330",
    "end": "3387728"
  },
  {
    "text": " I will see exactly\nthe same as SchNet, so",
    "start": "3387728",
    "end": "3395500"
  },
  {
    "text": "the invariant neural network. So maybe let me still\nintroduce this slide.",
    "start": "3395500",
    "end": "3401760"
  },
  {
    "text": "So yeah, distance input\nto a matrix size filter.",
    "start": "3401760",
    "end": "3406820"
  },
  {
    "text": "And a filter can be used by-- can be used for like\nphi S learn some feature",
    "start": "3406820",
    "end": "3415820"
  },
  {
    "text": "of current node of\na neighborhood node. And the neighborhood node\nembedding combine with the--",
    "start": "3415820",
    "end": "3423020"
  },
  {
    "text": "into filter W. They\ncombine together as a message in that bond. And for that node,\nall the messages",
    "start": "3423020",
    "end": "3431030"
  },
  {
    "text": "phi S, multiply with WS,\ncombine together as a message. And they aggregate the message.",
    "start": "3431030",
    "end": "3437450"
  },
  {
    "text": "Everything here is\ninvariant because only depends on distance. Still the same.",
    "start": "3437450",
    "end": "3444780"
  },
  {
    "text": "But here, we have something\ndifferent as the vector tensor.",
    "start": "3444780",
    "end": "3449820"
  },
  {
    "text": "So how we update\nthe vector feature?",
    "start": "3449820",
    "end": "3455860"
  },
  {
    "text": "So we input the distance. And we have the\noutput with matrix",
    "start": "3455860",
    "end": "3462100"
  },
  {
    "text": "to control the message passing. And similarly, we also,\nfor all the neighborhood,",
    "start": "3462100",
    "end": "3467650"
  },
  {
    "text": "we use neural network of\nphi S to learn some message from that node.",
    "start": "3467650",
    "end": "3473450"
  },
  {
    "text": "However, for the\nmessage passing, it is not only to\nscatter multiplied--",
    "start": "3473450",
    "end": "3479560"
  },
  {
    "text": "to scatter tensor multiplied. After the\nmultiplication, they also multiply with the\nradius direction.",
    "start": "3479560",
    "end": "3488150"
  },
  {
    "text": "Now radius direction,\nall the neighborhood to current node direction. So it is multiplied\nwith this tensor.",
    "start": "3488150",
    "end": "3495800"
  },
  {
    "text": "And our [INAUDIBLE],,\nour tensor embedding will be updated by a\nweighted sum of all",
    "start": "3495800",
    "end": "3503060"
  },
  {
    "text": "the relative direction,\nso relative direction from neighborhood\nto current node. So think about this thing.",
    "start": "3503060",
    "end": "3511220"
  },
  {
    "text": "When we do translation-- when we do a rotation of\nthe molecule structure,",
    "start": "3511220",
    "end": "3517490"
  },
  {
    "text": "like from this structure\nto this structure,",
    "start": "3517490",
    "end": "3522800"
  },
  {
    "text": "the relative direction\nwill change together. So here are the\nrelative direction. It's like this direction.",
    "start": "3522800",
    "end": "3529130"
  },
  {
    "text": "But after you rotate,\nit will change together. And all the relative direction\nwill change together.",
    "start": "3529130",
    "end": "3536340"
  },
  {
    "text": "So naturally, a weighted sum\nof all the relative direction",
    "start": "3536340",
    "end": "3542310"
  },
  {
    "text": "will change together. So I call it a rotation\nequivariant feature. So this is the way to learn\nthe equivariant feature.",
    "start": "3542310",
    "end": "3552609"
  },
  {
    "text": "And what I just introduced\nis a single layer",
    "start": "3552610",
    "end": "3558040"
  },
  {
    "text": "for updating the\ninvariant node embedding and the equivariant\nnode embedding. And by stacking\nmultiple layer, we",
    "start": "3558040",
    "end": "3566350"
  },
  {
    "text": "can have the output to be still\nequivariant for the input.",
    "start": "3566350",
    "end": "3571730"
  },
  {
    "text": "So which means when we do a\nrotation of the structure, the output force\nwill rotate together",
    "start": "3571730",
    "end": "3578960"
  },
  {
    "text": "and same as what I\nhave introduced before. And also, the\ntensor feature helps",
    "start": "3578960",
    "end": "3584890"
  },
  {
    "text": "to capture more information\nin our before example to show that the distance\nand the angle is not enough.",
    "start": "3584890",
    "end": "3593930"
  },
  {
    "text": "OK? Yeah, this is PaiNN. And yeah, considering the\ntime, maybe skip this summary",
    "start": "3593930",
    "end": "3602090"
  },
  {
    "text": "and begin to introduce\nthe most recent progress of fancy demos of the\ngeometry generative model.",
    "start": "3602090",
    "end": "3611660"
  },
  {
    "text": "Yeah, there are really\nmany, many exciting progress",
    "start": "3611660",
    "end": "3616819"
  },
  {
    "text": "of protein design\nin pharma company. And generative biomedicine\nis actually the same founder,",
    "start": "3616820",
    "end": "3627140"
  },
  {
    "text": "I think, of Moderna. So they opened another company\nof AI-motivated drug discovery",
    "start": "3627140",
    "end": "3635150"
  },
  {
    "text": "company. So all these kind of task\nis modern diffusion model.",
    "start": "3635150",
    "end": "3641299"
  },
  {
    "text": "OK. So today, the problem I\nwould like to concentrate,",
    "start": "3641300",
    "end": "3647170"
  },
  {
    "text": "so we need to have a\ntask to concentrate on to introduce our model. So what I do is, given\nthe molecule graph,",
    "start": "3647170",
    "end": "3656140"
  },
  {
    "text": "we want to predict\nthe 3D structure and, yeah, as this\nfriend just say.",
    "start": "3656140",
    "end": "3663280"
  },
  {
    "text": "So every molecule will have\ndifferent-- multiple different structure.",
    "start": "3663280",
    "end": "3668330"
  },
  {
    "text": "So naturally, this is\nnot a prediction task. Prediction task\nmeans [INAUDIBLE]..",
    "start": "3668330",
    "end": "3673780"
  },
  {
    "text": "But actually, it has\nmultiple stable structure. So naturally, this\nis a generation task.",
    "start": "3673780",
    "end": "3681910"
  },
  {
    "text": "And physically, there\nis some distribution",
    "start": "3681910",
    "end": "3687270"
  },
  {
    "text": "of this 3D structure. It's a Boltzmann distribution\ngiven some temperature.",
    "start": "3687270",
    "end": "3693540"
  },
  {
    "text": "But never mind, so just want\nto see there is a distribution.",
    "start": "3693540",
    "end": "3700110"
  },
  {
    "text": "Why does the molecular\ngraph have to be 2D? Can you have crossover bonds?",
    "start": "3700110",
    "end": "3706410"
  },
  {
    "text": "Why is that the\nmolecular graph is 2D? 2D. Why does it have to be 2D? ",
    "start": "3706410",
    "end": "3713830"
  },
  {
    "text": "Why it is-- Why did it have to be 2D? You have crossover bonds. Crossover bonds.",
    "start": "3713830",
    "end": "3719910"
  },
  {
    "text": "The graph on the\nleft, that's 2D. Like, for example,\nwould you draw two bonds that are overlapping?",
    "start": "3719910",
    "end": "3727930"
  },
  {
    "text": "Or you are looking at it from\na perspective that would be-- that wouldn't\nrequire a 3D graph?",
    "start": "3727930",
    "end": "3735160"
  },
  {
    "text": "3D graph. ",
    "start": "3735160",
    "end": "3740550"
  },
  {
    "text": "You mean a 3D profile? [INAUDIBLE] Oh, you mean define some bond\nto have the direction right.",
    "start": "3740550",
    "end": "3746549"
  },
  {
    "text": "So-- I mean the bond can overlap,\nso why does it have to be a 2D [INAUDIBLE] on the\nsecond [INAUDIBLE]??",
    "start": "3746550",
    "end": "3755440"
  },
  {
    "text": "The specific graphic is 2D. It didn't have to be 2D\nrepresentation to be--",
    "start": "3755440",
    "end": "3760780"
  },
  {
    "text": "Oh, I see. For this task, what we\nwant to do is, given 2D,",
    "start": "3760780",
    "end": "3766180"
  },
  {
    "text": "we want to generate a 3D. But yeah, graph can\nbe represented in 3D.",
    "start": "3766180",
    "end": "3773740"
  },
  {
    "text": "That's fine. When you say 2D graph,\nyou really just mean an-- Oh, adjacency matrix.",
    "start": "3773740",
    "end": "3779420"
  },
  {
    "text": "Yeah, yeah, yeah. There we go. Sorry. Yeah. I think-- yeah. Sorry.",
    "start": "3779420",
    "end": "3784840"
  },
  {
    "text": "Sorry. I think I still missed\nsome detail, yeah. Molecule, you already have a\nthree level of representation.",
    "start": "3784840",
    "end": "3792430"
  },
  {
    "text": "The most naive way is to-- actually, they have a sequence\nrepresentation of molecule.",
    "start": "3792430",
    "end": "3798460"
  },
  {
    "text": "Just some atom and some-- it is called SMILES and-- yeah.",
    "start": "3798460",
    "end": "3804520"
  },
  {
    "text": "So application on 1D\nrepresentation is very natural. All the task can apply on it.",
    "start": "3804520",
    "end": "3810610"
  },
  {
    "text": "And then graph neural\nnetwork and everyone work on, I usually call 2D. So it means adjacency\nmatrix and node feature.",
    "start": "3810610",
    "end": "3819040"
  },
  {
    "text": "And now we concentrate on 3D. But actually, it is still\ngraph representation, yes.",
    "start": "3819040",
    "end": "3825460"
  },
  {
    "text": "So every node, we have\nan additional coordinates for every node.",
    "start": "3825460",
    "end": "3833290"
  },
  {
    "text": "But the task is 3D generation. That's all what I want to\nsay for this slide, yeah.",
    "start": "3833290",
    "end": "3841045"
  },
  {
    "text": " And similar to the learning--",
    "start": "3841045",
    "end": "3851059"
  },
  {
    "text": "similar to the representation\nlearning for 3D graph, which need to be capture some\ninvariant and equivariant,",
    "start": "3851060",
    "end": "3859069"
  },
  {
    "text": "here, it's the same. So for the generative\nmodel, we also",
    "start": "3859070",
    "end": "3864590"
  },
  {
    "text": "want to incorporate some\nequivariance into the model.",
    "start": "3864590",
    "end": "3869640"
  },
  {
    "text": "And yeah, I know if I\nintroduce the full information",
    "start": "3869640",
    "end": "3875490"
  },
  {
    "text": "of diffusion model, it\nwill be too time consuming, so I deleted something. But formally, what\nwe are concerned",
    "start": "3875490",
    "end": "3883020"
  },
  {
    "text": "about developing a\ngenerative model for 3D is, generative model\nessentially want",
    "start": "3883020",
    "end": "3888180"
  },
  {
    "text": "to estimate the\nlikelihood of data and do something like a\nmaximum likelihood training.",
    "start": "3888180",
    "end": "3895540"
  },
  {
    "text": "And here you can think about\nlikelihood is probability, right? So no matter how you rotate and\ntranslate your data or your 3D",
    "start": "3895540",
    "end": "3904890"
  },
  {
    "text": "data, when you input it\ninto a generative model,",
    "start": "3904890",
    "end": "3910140"
  },
  {
    "text": "it will tell you a likelihood. So the likelihood should never\nchange considering the rotation",
    "start": "3910140",
    "end": "3916119"
  },
  {
    "text": "and translation. So this is theoretically\nthe consideration of developing geometric\ngenerative model.",
    "start": "3916120",
    "end": "3922950"
  },
  {
    "text": "But here I just-- yeah, so just give\nsome technical detail for generative model.",
    "start": "3922950",
    "end": "3930609"
  },
  {
    "text": "And some very intuitive\nunderstanding of diffusion model. So diffusion model means when\nwe have this kind of image,",
    "start": "3930610",
    "end": "3939700"
  },
  {
    "text": "we will add a noise on\nthe image to destroy it to different level. And like here, our data, we\nusually denote the data as X0.",
    "start": "3939700",
    "end": "3950140"
  },
  {
    "text": "We will have multiple\ndifferent level of noise to destroy it until\nrandom Gaussian.",
    "start": "3950140",
    "end": "3957109"
  },
  {
    "text": "And what we want to learn\nis, for every timestep,",
    "start": "3957110",
    "end": "3962470"
  },
  {
    "text": "we first perturb the data. And it will be a noisy data. And we want our model, we\nwill input XT into our model",
    "start": "3962470",
    "end": "3973540"
  },
  {
    "text": "and hope our model can\npredict what is XT minus 1. So how to denoise.",
    "start": "3973540",
    "end": "3979799"
  },
  {
    "text": "Similar to here. So we predefine how to perturb\nor how to diffuse the data.",
    "start": "3979800",
    "end": "3986430"
  },
  {
    "text": "And what we learn is the reverse\ndirection, how to denoise. So diffusion model,\nthe general idea",
    "start": "3986430",
    "end": "3993450"
  },
  {
    "text": "is predefined diffusion\nprocess and to learn a model to generate by denoising.",
    "start": "3993450",
    "end": "3999570"
  },
  {
    "text": "So this is a high level idea. And a very brief introduction\nof diffusion model.",
    "start": "3999570",
    "end": "4007780"
  },
  {
    "text": "There are many, many\nfancy mathematics for diffusion model. But here, I think I summarized\nthe least requirement",
    "start": "4007780",
    "end": "4017310"
  },
  {
    "text": "of the diffusion model. So you can think about, every\ntime we just randomly sample",
    "start": "4017310",
    "end": "4024210"
  },
  {
    "text": "a random noise or still\nmade some information, the random noise is sampled\nfrom normal distribution.",
    "start": "4024210",
    "end": "4031120"
  },
  {
    "text": "So mean 0 and variance\nas a unit variance.",
    "start": "4031120",
    "end": "4036310"
  },
  {
    "text": "And we destroy the data by-- to the T step diffusion data\nby multiplying Mu t and plus",
    "start": "4036310",
    "end": "4046830"
  },
  {
    "text": "sigma. Sigma weighted the noise. And the high level idea\nhere is in the first step,",
    "start": "4046830",
    "end": "4055380"
  },
  {
    "text": "in the first few step,\nby step I mean the t, so when t is very low, we hope\nthat the Mu t is nearly 1.",
    "start": "4055380",
    "end": "4065350"
  },
  {
    "text": "So Mu 0 is 1. And we hope-- not we hope,\nwe set, we define, yeah.",
    "start": "4065350",
    "end": "4071830"
  },
  {
    "text": "And we define sigma 0 as 0. So which means we\nnever destroy the data.",
    "start": "4071830",
    "end": "4076990"
  },
  {
    "text": "And with very low\ntimestep t, we just destroy the data a little bit.",
    "start": "4076990",
    "end": "4083349"
  },
  {
    "text": "And what we define for the\nfinal step is, I think,",
    "start": "4083350",
    "end": "4089140"
  },
  {
    "text": "shouldn't be 1. It shouldn't be\ncapital T, sorry. ",
    "start": "4089140",
    "end": "4094789"
  },
  {
    "text": "Yeah. So Mu capital T is nearly 0. And Mu capital T is nearly\n1, which means just totally",
    "start": "4094790",
    "end": "4103549"
  },
  {
    "text": "destroy the data\ninto random noise. Yeah, you multiply 0\nand destroy the data",
    "start": "4103550",
    "end": "4110089"
  },
  {
    "text": "and make it-- this is 1, so\nmake it just a random noise. And what we want to do is,\nevery time we give us a data,",
    "start": "4110090",
    "end": "4121790"
  },
  {
    "text": "we perturb the data\nto some timestep. And we put this noisy\ndata and the timestep",
    "start": "4121790",
    "end": "4127130"
  },
  {
    "text": "into the neural network. And we hope our neural\nnetwork learns how",
    "start": "4127130",
    "end": "4132140"
  },
  {
    "text": "to predict the noise vector. And if this can be learned very\nprecisely, you can know here.",
    "start": "4132140",
    "end": "4142580"
  },
  {
    "text": "If we know XT, and we know the\nepsilon, and Mu t and sigma t",
    "start": "4142580",
    "end": "4149120"
  },
  {
    "text": "are defined by\n[INAUDIBLE],, so we always know this kind of parameter. So we can recover\nthe clean data.",
    "start": "4149120",
    "end": "4158120"
  },
  {
    "text": "So this is the high-level\nidea of diffusion. And yeah, this is also\nthe sampling procedure.",
    "start": "4158120",
    "end": "4166589"
  },
  {
    "text": "So we just always sample a very\nnoisy data from Gaussian noise.",
    "start": "4166590",
    "end": "4173028"
  },
  {
    "text": "And we repeatedly--\nfrom timestep capital T to time step capital 0,\nrepeatedly input the model,",
    "start": "4173029",
    "end": "4181009"
  },
  {
    "text": "predict the noise vector. And you subtract\nthe noise vector. You can progressively\ndenoise the data",
    "start": "4181010",
    "end": "4187818"
  },
  {
    "text": "and hopefully recover\na very realistic data. So yeah, I think most\nof you have played",
    "start": "4187819",
    "end": "4195320"
  },
  {
    "text": "with like Stable Diffusion. Give some prompt and generate\na really fancy image. So really impressive\nprogress of this field.",
    "start": "4195320",
    "end": "4205460"
  },
  {
    "text": "So yeah, this is\nalso from Stanford.",
    "start": "4205460",
    "end": "4211030"
  },
  {
    "text": "So Stefano Ermon,\nanother professor working on general model. ",
    "start": "4211030",
    "end": "4217550"
  },
  {
    "text": "And yeah, this is\nsome really fancy",
    "start": "4217550",
    "end": "4224050"
  },
  {
    "text": "[INAUDIBLE] for\ndiffusion process and the reverse\ndenoising process. They recently formulated this\nas a stochastic differentiable",
    "start": "4224050",
    "end": "4234060"
  },
  {
    "text": "experience. Something like this. Yeah. ",
    "start": "4234060",
    "end": "4239579"
  },
  {
    "text": "And this is our paper. So we extend the idea\nof geometric learning",
    "start": "4239580",
    "end": "4246630"
  },
  {
    "text": "of a diffusion model\nfrom a typical image to also the geometry. And yeah, very happy.",
    "start": "4246630",
    "end": "4253830"
  },
  {
    "text": "It's top 50 most cited\npaper in 2022, yeah. So I just want to highlight\nits promising direction",
    "start": "4253830",
    "end": "4263280"
  },
  {
    "text": "and exciting research field. And yeah, this is what we do. So given data C0,\nexactly the same idea.",
    "start": "4263280",
    "end": "4274960"
  },
  {
    "text": "And the noise to destroy it\nand learn the reverse process to progressively denoising\nto generate data.",
    "start": "4274960",
    "end": "4283960"
  },
  {
    "text": "However, when you\nhave a look, I want to introduce the work\nby a very intuitive way,",
    "start": "4283960",
    "end": "4290640"
  },
  {
    "text": "so I just skipped all\nthe math in the paper. So when you just have\nobservation of this figure,",
    "start": "4290640",
    "end": "4298659"
  },
  {
    "text": "you will feel\nlike, oh, actually, this is progressively\ndenoising by the noise vector.",
    "start": "4298660",
    "end": "4306150"
  },
  {
    "text": "Actually, this is similar\nto the molecular simulation. So molecular simulation, when\nwe do molecular simulation,",
    "start": "4306150",
    "end": "4315860"
  },
  {
    "text": "we always have either a\nclassical Schrodinger solver",
    "start": "4315860",
    "end": "4321170"
  },
  {
    "text": "or a graph neural network. It will tell you\nwhat's the force. And the force will be used\nto optimize the structure.",
    "start": "4321170",
    "end": "4329970"
  },
  {
    "text": "And here what you\ndo is you always also have a structure, your\ninput to the neural network.",
    "start": "4329970",
    "end": "4337010"
  },
  {
    "text": "And it tells you\ndenoising direction. So you follow this\ndenoising direction",
    "start": "4337010",
    "end": "4342590"
  },
  {
    "text": "to update your structure. So this is kind of similar,\nright, output dense matrix",
    "start": "4342590",
    "end": "4348260"
  },
  {
    "text": "and optimize your structure. And what's the insight\nof this observation",
    "start": "4348260",
    "end": "4354350"
  },
  {
    "text": "is similar to force. We also need to give the\nneural network the power",
    "start": "4354350",
    "end": "4360650"
  },
  {
    "text": "of capture the equivariance. So how to denoise? The denoising vector\nshould also be equivariant",
    "start": "4360650",
    "end": "4367130"
  },
  {
    "text": "with the input structure. No matter how you\nrotate your structure,",
    "start": "4367130",
    "end": "4372350"
  },
  {
    "text": "the denoising direction\nshould rotate together. So this gives us the motivation\nto impose this inductive bias",
    "start": "4372350",
    "end": "4384350"
  },
  {
    "text": "into our model. And what we do is parameterizing\nthe denoising network",
    "start": "4384350",
    "end": "4390619"
  },
  {
    "text": "with equivariant\ngraph neural network. ",
    "start": "4390620",
    "end": "4395659"
  },
  {
    "text": "Yeah. And just this kind\nof thing, this is a visualization\nfrom my paper.",
    "start": "4395660",
    "end": "4403580"
  },
  {
    "text": "And it's a generator. Although there is some-- the ring is not\non surface, right?",
    "start": "4403580",
    "end": "4409340"
  },
  {
    "text": "But still fine, still\nlearn something. And it gives the community\nthe first version",
    "start": "4409340",
    "end": "4414710"
  },
  {
    "text": "of how to define diffusion\nmodel for geometries.",
    "start": "4414710",
    "end": "4420320"
  },
  {
    "text": "And yeah, I suppose\nmaybe some of you are familiar with diffusion.",
    "start": "4420320",
    "end": "4425780"
  },
  {
    "text": "Considering if you are\nfamiliar, maybe you think this is trivial. And I can speak a\nlittle bit more.",
    "start": "4425780",
    "end": "4432200"
  },
  {
    "text": "So for diffusion\nprocess, as you can look, diffusion process actually is--",
    "start": "4432200",
    "end": "4439080"
  },
  {
    "text": "if you couldn't\nunderstand, never mind. So it's not important. So diffusion process\ndefine Markov process",
    "start": "4439080",
    "end": "4446639"
  },
  {
    "text": "of generating from prior\ndistribution towards the data distribution.",
    "start": "4446640",
    "end": "4451890"
  },
  {
    "text": "And we have a prior\ndistribution P of CT. And we have all\nthe Markov kernels.",
    "start": "4451890",
    "end": "4458400"
  },
  {
    "text": "And we multiply them\ntogether and do some integral of all the hidden\nlatent vector, we can",
    "start": "4458400",
    "end": "4466410"
  },
  {
    "text": "have a probability of the C0. So I just want to see,\nby diffusion model,",
    "start": "4466410",
    "end": "4472650"
  },
  {
    "text": "we can define probability of P\nof data or likelihood of data.",
    "start": "4472650",
    "end": "4477929"
  },
  {
    "text": "And imposing equivariance\ninto the neural network,",
    "start": "4477930",
    "end": "4484320"
  },
  {
    "text": "here, the motivation seems\nlike some observation. But the true\nmotivation for my paper",
    "start": "4484320",
    "end": "4489870"
  },
  {
    "text": "is, I need to make the\nlikelihood estimation to be invariant to the translation.",
    "start": "4489870",
    "end": "4496210"
  },
  {
    "text": "So no matter how you rotate\nor translate your structure,",
    "start": "4496210",
    "end": "4501300"
  },
  {
    "text": "diffusion model can enable\nto calculate some likelihood. And this model can actually\nmake the likelihood invariant",
    "start": "4501300",
    "end": "4507690"
  },
  {
    "text": "to rotation and translation. So this is some\ntheoretical motivation. And if you are interested,\nyou can see the paper.",
    "start": "4507690",
    "end": "4516100"
  },
  {
    "text": "But here, maybe too heavy\nto introduce all the math behind this model. ",
    "start": "4516100",
    "end": "4523660"
  },
  {
    "text": "Let me click on the figure. This is the most recent exciting\nprogress of diffusion model.",
    "start": "4523660",
    "end": "4532220"
  },
  {
    "text": "So this is now very, very\nrealistic problem now.",
    "start": "4532220",
    "end": "4537760"
  },
  {
    "text": "So we have some\nprotein in our body. So here is, also, we have some\nscaffold protein in our body.",
    "start": "4537760",
    "end": "4546820"
  },
  {
    "text": "And we really want to\nmake it more stable or want to make some\nantibody to bind with them",
    "start": "4546820",
    "end": "4553000"
  },
  {
    "text": "or something like this. And these two figures show that\nwe can already use diffusion",
    "start": "4553000",
    "end": "4558040"
  },
  {
    "text": "to generate some in [INAUDIBLE]\ngenerate some promising drug",
    "start": "4558040",
    "end": "4563140"
  },
  {
    "text": "or antibody for some\ndisease, although they still",
    "start": "4563140",
    "end": "4568270"
  },
  {
    "text": "need some clinical trial to\nverify the effectiveness. But yeah, already\npretty promising now.",
    "start": "4568270",
    "end": "4575769"
  },
  {
    "text": " OK. I think, yeah, it's\nalso the time now.",
    "start": "4575770",
    "end": "4582260"
  },
  {
    "text": "So maybe just one or\ntwo minute to introduce some future direction.",
    "start": "4582260",
    "end": "4588860"
  },
  {
    "text": "Also what I'm working on-- what I'm working on recently\nis, if we have a pocket,",
    "start": "4588860",
    "end": "4594260"
  },
  {
    "text": "if we have some\nill-behaved protein, how to design small drug\nfor this protein target.",
    "start": "4594260",
    "end": "4602000"
  },
  {
    "text": "And also this is\nantibody design. So we already know\nhuman can generate",
    "start": "4602000",
    "end": "4607430"
  },
  {
    "text": "some antibody for\nsome antigen. And we want to do some mutation,\nmeaningful mutation",
    "start": "4607430",
    "end": "4614630"
  },
  {
    "text": "on the antibody to\nmake it more effective. And we can synthesize it. ",
    "start": "4614630",
    "end": "4621420"
  },
  {
    "text": "OK. Yeah, this slide is\nfrom many people. And I think the links are\nalso meaningful resources,",
    "start": "4621420",
    "end": "4629000"
  },
  {
    "text": "if you want to learn\na little bit more. So Jure is mainly\nfor general graph. And Tess is in PhD\nin physics, I think.",
    "start": "4629000",
    "end": "4638960"
  },
  {
    "text": "And Jian is my previous\nmaster advisor. We worked together\nfor the GRD paper.",
    "start": "4638960",
    "end": "4645980"
  },
  {
    "text": "And also Stefano Ermon. Yang Song, now a\nprofessor at Caltech now. And Ron is, anyway, a\nbiologist, biology professor.",
    "start": "4645980",
    "end": "4656690"
  },
  {
    "text": "So all kind of resources. And that's all. Sorry for one\nadditional minute, yeah.",
    "start": "4656690",
    "end": "4664925"
  },
  {
    "text": " And thank you.",
    "start": "4664925",
    "end": "4670969"
  },
  {
    "text": "Thank you. ",
    "start": "4670970",
    "end": "4678000"
  }
]