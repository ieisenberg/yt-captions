[
  {
    "start": "0",
    "end": "5330"
  },
  {
    "text": "--up here asking you\nabout DPO and RLHF. ",
    "start": "5330",
    "end": "72619"
  },
  {
    "text": "OK, great. Why don't you turn to somebody\nand compare your answers? ",
    "start": "72620",
    "end": "109200"
  },
  {
    "text": "[SIDE CONVERSATION] ",
    "start": "109200",
    "end": "144886"
  },
  {
    "text": "OK, so there's\nstill pretty good-- there's a lot of\ndisagreement on one of these.",
    "start": "144886",
    "end": "151530"
  },
  {
    "text": "For the first one, it's false. Does somebody want to\ntell me which one does not learn an explicit representation\nof the reward function?",
    "start": "151530",
    "end": "159250"
  },
  {
    "text": "So they do not both learn one. One of them does, and\none of them doesn't. Which one does?",
    "start": "159250",
    "end": "164730"
  },
  {
    "text": "Yeah. RLHF learns, and then\nDPO doesn't learn. That's right. Yeah, that's exactly right.",
    "start": "164730",
    "end": "170650"
  },
  {
    "text": "So this one does learn. ",
    "start": "170650",
    "end": "178060"
  },
  {
    "text": "OK, so this is false. Now, it's true that DPO\nassumes a particular parametric",
    "start": "178060",
    "end": "183180"
  },
  {
    "text": "representation for\nthe award model. Both of them do. But DPO then inverts that.",
    "start": "183180",
    "end": "188615"
  },
  {
    "text": "So you can directly\ndo policy learning. It never has to explicitly\nlearn a reward function in the same way that RLHF does.",
    "start": "188615",
    "end": "195300"
  },
  {
    "text": "What about the second one? What do you think? Is it constrained to\nbe as good as the best examples in the pairwise\npreference data?",
    "start": "195300",
    "end": "201725"
  },
  {
    "start": "201725",
    "end": "208060"
  },
  {
    "text": "So I think this is false. Does somebody who\nalso said false want to say, why is this false?",
    "start": "208060",
    "end": "215055"
  },
  {
    "text": " Yeah.",
    "start": "215055",
    "end": "220950"
  },
  {
    "text": "Maybe because we're using policy\napproximate, using a function to approximate it.",
    "start": "220950",
    "end": "226210"
  },
  {
    "text": "So it could become-- Yeah, it could take a step,\nwhich is more positive than.",
    "start": "226210",
    "end": "236130"
  },
  {
    "text": "Yeah, exactly\n[INAUDIBLE] I said. So you're going to-- at least if\nwe think about the RLHF case--",
    "start": "236130",
    "end": "242140"
  },
  {
    "text": "we are using this information\nto learn a reward model. If that reward\nmodel is good even",
    "start": "242140",
    "end": "247950"
  },
  {
    "text": "and can extrapolate beyond and\ngeneralize beyond the samples that we have, when you do\nPPO using that reward model,",
    "start": "247950",
    "end": "254350"
  },
  {
    "text": "you can learn a policy that's\nbetter than your demonstrations. So this can, in fact, go beyond\nthe best sort of performance",
    "start": "254350",
    "end": "261359"
  },
  {
    "text": "that's inside your data. Or if you think of it\nin terms of the reward, maybe some of the\nexamples you're showing",
    "start": "261360",
    "end": "267030"
  },
  {
    "text": "aren't that great,\nbut then you can use that to actually\nget a better policy. And in fact, you might\nthink that's probably",
    "start": "267030",
    "end": "272280"
  },
  {
    "text": "exactly what's\nhappening with ChatGPT, because for ChatGPT,\nthey initially got the fine-tuned model\nfrom supervised learning,",
    "start": "272280",
    "end": "281560"
  },
  {
    "text": "and then they showed\nthose examples to people. And people would\npick between them. And then it learned\na reward model,",
    "start": "281560",
    "end": "287850"
  },
  {
    "text": "and then they got a policy\nthat was better at generating those sort of responses. So you could argue that ChatGPT\nis an example that suggests,",
    "start": "287850",
    "end": "295640"
  },
  {
    "text": "yes, this often can be true. We can learn a good\nenough reward model such that if we do PPO, at\nleast a little bit of it,",
    "start": "295640",
    "end": "301100"
  },
  {
    "text": "we can actually outperform\nthe training examples. PPO and DPO does use\na reference policy.",
    "start": "301100",
    "end": "308150"
  },
  {
    "text": "Both of them do. And this idea will come up. We've seen it a\nfew times already,",
    "start": "308150",
    "end": "314150"
  },
  {
    "text": "and it'll continue\nto come up today. This idea of\nthinking essentially of how far can we\nextrapolate, or how far can we",
    "start": "314150",
    "end": "319850"
  },
  {
    "text": "interpolate from our data? And when do we need\nto constrain ourselves to be fairly close either in the\npolicy space or something else",
    "start": "319850",
    "end": "328770"
  },
  {
    "text": "so that we don't generalize\nto parts of the domain where we might have\nreally bad performance? We saw that in\nimitation learning.",
    "start": "328770",
    "end": "335340"
  },
  {
    "text": "We saw that in DPO. We've seen that in PPO. In all of these cases where\nwe're thinking, given the data",
    "start": "335340",
    "end": "340550"
  },
  {
    "text": "that we have, how can we\ngeneralize as much as possible, but not further? ",
    "start": "340550",
    "end": "347870"
  },
  {
    "text": "All right. OK, so we're getting into a part\nof the class, which is probably",
    "start": "347870",
    "end": "354485"
  },
  {
    "text": "my favorite part of the class,\nthough I like-- of course, I'm biased. I like all of it. But we've been\ntalking about learning",
    "start": "354485",
    "end": "361320"
  },
  {
    "text": "from past human preferences. We first saw that\nsort of learning from past human demonstrations.",
    "start": "361320",
    "end": "366850"
  },
  {
    "text": "Then we saw learning from\npast human preferences. And today, we're going\nto think just generally about learning from past data.",
    "start": "366850",
    "end": "372790"
  },
  {
    "text": "So that could be\ngenerated by humans, or it could be generated by\nyour robot or something else. And then next time,\nwe're going to start",
    "start": "372790",
    "end": "378930"
  },
  {
    "text": "talking about fast or\ndata-efficient learning, and that's going to be useful\nfor doing homework 3 as",
    "start": "378930",
    "end": "384240"
  },
  {
    "text": "well, because the theory\nquestion for homework 3 is focused on\ndata-efficient learning.",
    "start": "384240",
    "end": "389590"
  },
  {
    "text": "All right, so we'll\nfocus on that now. So in particular, for today,\nwe're going to discuss,",
    "start": "389590",
    "end": "394840"
  },
  {
    "text": "like we often do, thinking of\nseparating things into a policy evaluation question\nand then a policy",
    "start": "394840",
    "end": "400530"
  },
  {
    "text": "learning question, because\nwe've seen repeatedly that if we think\nabout can we evaluate how good a particular policy\nis, that we can often combine",
    "start": "400530",
    "end": "408030"
  },
  {
    "text": "that as a way to bootstrap\nimproving our policy optimization.",
    "start": "408030",
    "end": "413290"
  },
  {
    "text": "All right. But I want to start with just\na question, which is, can we do better than imitation learning?",
    "start": "413290",
    "end": "418689"
  },
  {
    "text": "And of course, this\nrelates to the question I just asked you in the\nrefresh your understanding. So I'm just going to give\nup sort of an example.",
    "start": "418690",
    "end": "425160"
  },
  {
    "text": "In my lab, we often think about\neducation data or health care data or other cases where\ndecisions are being generated",
    "start": "425160",
    "end": "431250"
  },
  {
    "text": "by humans or automated\nsystems where you might have, say, a series of patients.",
    "start": "431250",
    "end": "436450"
  },
  {
    "text": "You could think of this\nas medical record data. And each of those\npeople are getting a series of interventions.",
    "start": "436450",
    "end": "442570"
  },
  {
    "text": "Maybe it's some medication. Maybe it's a medical checkup. Maybe it's a vaccine. And then we observe\nsome sort of outcome.",
    "start": "442570",
    "end": "450150"
  },
  {
    "text": "And in imitation learning, we\nsaw the idea of saying, well, could we try to\nmimic the best human,",
    "start": "450150",
    "end": "455400"
  },
  {
    "text": "or could we try to\nmimic expert data? And so an important\nquestion is whether or not we can go beyond that.",
    "start": "455400",
    "end": "461880"
  },
  {
    "text": "And we just thought\nabout one example where we might be able\nto go beyond that. But I think that there's\na huge number of places",
    "start": "461880",
    "end": "467490"
  },
  {
    "text": "we'd love to be able to go\nbeyond the limits of at least the average human performance.",
    "start": "467490",
    "end": "472720"
  },
  {
    "text": "Health care is\ncertainly one of them. In America, we pay a\nlot for our health care, and we don't have particularly\ngood outcomes compared",
    "start": "472720",
    "end": "478650"
  },
  {
    "text": "to how much we are paying. So you would hope\nthat maybe we could learn through reinforcement\nlearning or others.",
    "start": "478650",
    "end": "484600"
  },
  {
    "text": "Are there better\nsequences of decisions we could make in order to better\nassist, say, a new patient?",
    "start": "484600",
    "end": "490910"
  },
  {
    "text": "So I'll just give a little bit\nof backstory of why I started",
    "start": "490910",
    "end": "497000"
  },
  {
    "text": "thinking about this question. So about a decade ago,\nI was collaborating",
    "start": "497000",
    "end": "502160"
  },
  {
    "text": "with Zoran Popovic and his\nlab and my grad students. He's at University\nof Washington, and he had this game\ncalled Refraction.",
    "start": "502160",
    "end": "509180"
  },
  {
    "text": "And Refraction helps teach\nkids about fractions. It's one of the\nconcepts kids typically",
    "start": "509180",
    "end": "514669"
  },
  {
    "text": "find really challenging when\nthey start to learn math. And so in it, you\nhave a spaceship,",
    "start": "514669",
    "end": "520640"
  },
  {
    "text": "and you're trying to fuel\na spaceship by splitting laser beams in certain ways. So that you create fractions\nor subparts of laser",
    "start": "520640",
    "end": "527090"
  },
  {
    "text": "beams to fuel spaceships and\nto save the-- save the agents.",
    "start": "527090",
    "end": "532370"
  },
  {
    "text": "And in this case, so I think\nroughly around maybe 500,000 kids have played this game.",
    "start": "532370",
    "end": "537720"
  },
  {
    "text": "And what we were thinking\nabout is, how could we customize it to make it more\npersonalized and adaptive",
    "start": "537720",
    "end": "542839"
  },
  {
    "text": "to students? So in particular, there are\nall these different sort of game activities\nand game levels.",
    "start": "542840",
    "end": "548759"
  },
  {
    "text": "And we wanted to understand,\nhow could we use information about how the student was\nworking in one of the activities",
    "start": "548760",
    "end": "555900"
  },
  {
    "text": "to adaptively select\nwhich next activity to do? So this is a decision policy\nand you can imagine conditioning",
    "start": "555900",
    "end": "563250"
  },
  {
    "text": "on all sorts of state features. So state features could be\nlike how long they took. But it also could be things\nlike where did they put down",
    "start": "563250",
    "end": "570180"
  },
  {
    "text": "laser beams, or what\nseries of mistakes did they make, you imagine? It could be generally a\nreally, really rich context",
    "start": "570180",
    "end": "575850"
  },
  {
    "text": "or state space. And then there were lots\nof different next levels we could do. So that was the question\nwe were interested in.",
    "start": "575850",
    "end": "583290"
  },
  {
    "text": "And in particular, in this case,\nwe had access to about 11,000 learners who had been giving\nactivities in a random order.",
    "start": "583290",
    "end": "591660"
  },
  {
    "text": "Now, that was because there\nwas a human designer who had designed a specific\nsequence through the game,",
    "start": "591660",
    "end": "597760"
  },
  {
    "text": "but we weren't sure\nif that was actually optimal or close to optimal. And what we wanted to do\nis to see whether or not",
    "start": "597760",
    "end": "604440"
  },
  {
    "text": "we could find\nusing reinforcement learning an adaptive\npolicy to help students persist at the game for longer.",
    "start": "604440",
    "end": "610650"
  },
  {
    "text": "So this game was\noffered on something called BrainPOP, which some of\nyou guys might have seen before. It offers lots of\neducational games for kids.",
    "start": "610650",
    "end": "618209"
  },
  {
    "text": "And a lot of kids use\nit for a little while, and then they stop. So it's an optional game.",
    "start": "618210",
    "end": "623439"
  },
  {
    "text": "And we had some\nevidence that suggested that if kids played\nthe game, they were likely to learn things.",
    "start": "623440",
    "end": "628933"
  },
  {
    "text": "But if they don't play\nthe game, they are not. So we wanted to think about\nincreasing student persistence in terms of the\nnumber of levels.",
    "start": "628933",
    "end": "636270"
  },
  {
    "text": "And so we really wanted to\ngo beyond expert performance in this case, like beyond\nwhat the experts had done.",
    "start": "636270",
    "end": "642570"
  },
  {
    "text": "And so what we did is we\nused reinforcement learning, and we wanted to see if we could\noutperform essentially behavior",
    "start": "642570",
    "end": "648060"
  },
  {
    "text": "cloning. And to give a spoiler\nof the types of ideas we're going to see\ntoday, in this case,",
    "start": "648060",
    "end": "653200"
  },
  {
    "text": "we found we could learn a policy\nthat increased persistence by about 30%. And so that suggests that\nin some of these domains,",
    "start": "653200",
    "end": "660130"
  },
  {
    "text": "there may be essentially\nbe enough data and evidence to find new decision\npolicies that",
    "start": "660130",
    "end": "667410"
  },
  {
    "text": "are substantially better than\nwhat is being currently done. And so that's what inspires\nme in my lab a lot,",
    "start": "667410",
    "end": "673600"
  },
  {
    "text": "is to think about where can\nwe use natural variation in the decisions\nthat are being made or past experiments that\nwere run in order to find",
    "start": "673600",
    "end": "681120"
  },
  {
    "text": "substantially better decision\npolicies than are currently being used. Yeah, not a super relevant\nquestion to the subject matter,",
    "start": "681120",
    "end": "688750"
  },
  {
    "text": "but just out of curiosity, was\nthe 30% distributed uniformly?",
    "start": "688750",
    "end": "693820"
  },
  {
    "text": "Or was it just like the\npeople who already played played longer, or the\nones that stopped early",
    "start": "693820",
    "end": "699630"
  },
  {
    "text": "would actually continue? This is a great question. So this is a really\nbig challenge often is whether or not who\nare you moving inside",
    "start": "699630",
    "end": "706560"
  },
  {
    "text": "of this distribution? So this is just an\nexpectation, like most of what we've been doing\nfor Q-learning and others. We did not analyze that\ntoo much in this case.",
    "start": "706560",
    "end": "714180"
  },
  {
    "text": "In another much more\nrecent paper we have, which I think came out\nin January or something, we did exactly that\nanalysis to try to see",
    "start": "714180",
    "end": "721350"
  },
  {
    "text": "who was actually impacted. And there, we were\nreally excited that it was the\nlowest performers that were most impacted.",
    "start": "721350",
    "end": "726930"
  },
  {
    "text": "And that was exciting because\none of the big concerns is that a lot of\nthese systems are just increasing the inequity gap.",
    "start": "726930",
    "end": "733830"
  },
  {
    "text": "And this is particularly a\nproblem in these optional ones because it's normally the\nkids that are furthest ahead",
    "start": "733830",
    "end": "740040"
  },
  {
    "text": "that have the highest usage. So great question. But it also raises in terms\nof on the technical side,",
    "start": "740040",
    "end": "746200"
  },
  {
    "text": "these questions\naround understanding-- sort of predicting estimates\nfor subparts of the population",
    "start": "746200",
    "end": "751410"
  },
  {
    "text": "and doing heterogeneous\ntreatment effect analysis to figure out which\ngroupings of contexts have different\nforms of Q values.",
    "start": "751410",
    "end": "758100"
  },
  {
    "text": "Yeah. In terms of the\npolicy, what was sort of being changed, like\nthe simplest level,",
    "start": "758100",
    "end": "764339"
  },
  {
    "text": "is it like the difficulty\nof the fractions? Is it how hard it goes\nas they're going up? Yeah, it's a great question.",
    "start": "764340",
    "end": "770260"
  },
  {
    "text": "So in this case, I can't\nremember what the final exact policy was using, but the type\nof things that we're varying in this case is things\naround the fraction--",
    "start": "770260",
    "end": "777310"
  },
  {
    "text": "so like changing the numbers-- as well as different\nthings of how tricky it is graphically to do that.",
    "start": "777310",
    "end": "783332"
  },
  {
    "text": "So there was a couple\nof different things that we could\nmanipulate as well as-- you can see, just\nlike visually here,",
    "start": "783332",
    "end": "790300"
  },
  {
    "text": "these look quite different. So one thing that we found\nin some other work in a game called Battleship\nNumberline, which",
    "start": "790300",
    "end": "796350"
  },
  {
    "text": "I was excited because\nrecently, my son uses BrainPOP. And it just popped up. And I was like,\nwe worked on that.",
    "start": "796350",
    "end": "801630"
  },
  {
    "text": "So that was exciting. In Battleship Numberline,\nwhich is another thing to do with fractions,\nwe found there",
    "start": "801630",
    "end": "808050"
  },
  {
    "text": "that variability was incredibly\nimportant for persistence. And so just changing how\nthings look in that case,",
    "start": "808050",
    "end": "814240"
  },
  {
    "text": "how big the battleships were,\nalso makes a very big difference to persistence and engagement. I think that's actually\nan interesting question",
    "start": "814240",
    "end": "820830"
  },
  {
    "text": "too, in terms of\nincluding the history and the state features to try\nto capture stuff like people",
    "start": "820830",
    "end": "828000"
  },
  {
    "text": "caring about variability. So great questions. We'll talk a little bit more\nabout this example later",
    "start": "828000",
    "end": "834780"
  },
  {
    "text": "to talk about some\nthings that we tried or that didn't work\nin this domain. But this is just to\nhighlight that, I guess,",
    "start": "834780",
    "end": "841530"
  },
  {
    "text": "we shouldn't set our\nexpectations too low. So I think that imitation\nlearning is amazing. And of course, if\nyou're trying to imitate",
    "start": "841530",
    "end": "848170"
  },
  {
    "text": "the best surgeons in the\nworld, that's incredible. But there are many\ncases where we think we can go beyond\nhuman performance,",
    "start": "848170",
    "end": "853670"
  },
  {
    "text": "particularly in cases where\nour high-level principles don't inform what we should do\nat a more micro level.",
    "start": "853670",
    "end": "859550"
  },
  {
    "text": "So, for example, here we\nmight have general principles of learning science. But it doesn't say which\nactivity to exactly do when,",
    "start": "859550",
    "end": "866320"
  },
  {
    "text": "and that's where it being data\ndriven can be really helpful. OK, let me give you\nanother example.",
    "start": "866320",
    "end": "872180"
  },
  {
    "text": "Another place thing that\nwe think about a lot is health care. We've collaborated a lot\nwith Finale Doshi-Velez at Harvard and her lab.",
    "start": "872180",
    "end": "878480"
  },
  {
    "text": "This is an example,\nthinking about hypertension and trying to optimize\ndifferent policies for that.",
    "start": "878480",
    "end": "884540"
  },
  {
    "text": "There is a really\namazing data set called Mimic that comes out\nof, I think, MIT and MGH,",
    "start": "884540",
    "end": "889730"
  },
  {
    "text": "Mass General Hospital,\nwhich has lots and lots of electronic medical\nrecord systems. And so what these guys did\nin this particular paper is",
    "start": "889730",
    "end": "898700"
  },
  {
    "text": "to look at behavior policy-- so that's this flat line-- and to see if they\ncould learn policies",
    "start": "898700",
    "end": "904310"
  },
  {
    "text": "using a method called\npopcorn that they thought would be much better. And again, here the results\ndepend on the method",
    "start": "904310",
    "end": "910925"
  },
  {
    "text": "and some of the hyperparameters\nthey're looking at. But the important\nthing just to notice here is that a number\nof these policies",
    "start": "910925",
    "end": "916310"
  },
  {
    "text": "are substantially better than\nbaseline, suggesting again, that there may be\ndomains where we",
    "start": "916310",
    "end": "921350"
  },
  {
    "text": "can leverage the intrinsic\nvariability in the data and identify things that are\nworking much more successfully in this systematic way.",
    "start": "921350",
    "end": "929779"
  },
  {
    "text": "So when we think\nabout doing this, generally, I would call this\nsort of offline or batch or counterfactual. And it's counterfactual\nbecause what we're trying to do",
    "start": "929780",
    "end": "937430"
  },
  {
    "text": "is to estimate or\nlearn policies that don't exist in the actual\ndata collection strategy.",
    "start": "937430",
    "end": "942950"
  },
  {
    "text": "So we have this setting\nnow where we'll assume, like in imitation\nlearning, that we have a data set of n trajectories.",
    "start": "942950",
    "end": "949205"
  },
  {
    "text": "So we're going to assume\nnow we're going back to the standard MDP setting. And it's not\npairwise preferences.",
    "start": "949205",
    "end": "954690"
  },
  {
    "text": "We're just back to having\nsequences of states and actions and rewards. So all right, so in particular,\nwe may have things like this",
    "start": "954690",
    "end": "964300"
  },
  {
    "text": "where we have data\nfrom one policy and data from another\npolicy, and we want to think about how we\ncan learn from that, thinking",
    "start": "964300",
    "end": "970360"
  },
  {
    "text": "about the state distribution\nof what's actually best. Now, I'll just highlight here\ntwo reasons why this is hard.",
    "start": "970360",
    "end": "979560"
  },
  {
    "text": "So we're always trying to\nestimate a counterfactual here over what might have\nhappened that wasn't tried.",
    "start": "979560",
    "end": "987330"
  },
  {
    "text": "So in this case, we don't know\nfor this patient group what would have happened if we gave\nthem that treatment or vice",
    "start": "987330",
    "end": "992550"
  },
  {
    "text": "versa. So just a reminder, this\nis the fundamental problem of causal inference.",
    "start": "992550",
    "end": "998860"
  },
  {
    "text": "And this is going to be a\nbig challenge for us here, particularly when we try to\ngo beyond the performance of the policy we\nsaw in the past.",
    "start": "998860",
    "end": "1006410"
  },
  {
    "text": "So data is censored. And of course, in\ngeneral, again, we're going to need\ngeneralization because we",
    "start": "1006410",
    "end": "1011995"
  },
  {
    "text": "don't want to have to enumerate\nall the possible policies. And I do just want\nto highlight here",
    "start": "1011995",
    "end": "1019180"
  },
  {
    "text": "that in addition to\neducation and health care, you want to think about climate\nchange or many other areas. There's just a huge\nnumber of scenarios,",
    "start": "1019180",
    "end": "1026037"
  },
  {
    "text": "including robotics, because\nit's often really expensive to do robotics experiments\nwhere these types of ideas",
    "start": "1026037",
    "end": "1031089"
  },
  {
    "text": "are helpful. Now, one thing you\nmight be wondering about is, when I'm talking\nabout this and I'm",
    "start": "1031089",
    "end": "1037480"
  },
  {
    "text": "talking about\ntrying to understand the performance of a new\ndecision policy that was not used to gather the\ndata, you might start",
    "start": "1037480",
    "end": "1044140"
  },
  {
    "text": "to think back to Q-learning. There's a lot of work on\noff-policy reinforcement learning from really the very\nbeginning of reinforcement",
    "start": "1044140",
    "end": "1050919"
  },
  {
    "text": "learning. So you might say, why don't\nwe already have the tools that we need to try to tackle\nthis problem of learning better",
    "start": "1050920",
    "end": "1057340"
  },
  {
    "text": "policies? And in fact, as we saw, ChatGPT,\nif we learn a reward function",
    "start": "1057340",
    "end": "1062809"
  },
  {
    "text": "and we do PPO, that is\ndoing off-policy learning. So that's one example.",
    "start": "1062810",
    "end": "1069130"
  },
  {
    "text": "So why can't we do this? Why can we just do Q-learning or\nsome of the other methods we've seen?",
    "start": "1069130",
    "end": "1074830"
  },
  {
    "text": "One thing to remember\nis a little while ago, I said, sometimes we\nhave this deadly triad of bootstrapping function,\napproximation and off-policy",
    "start": "1074830",
    "end": "1082480"
  },
  {
    "text": "learning. That sometimes when we combine\nall three of these things. Things can fail.",
    "start": "1082480",
    "end": "1087960"
  },
  {
    "text": "And that was part of\nthe motivation for PPO, is that we don't want to go\ntoo far from the distribution.",
    "start": "1087960",
    "end": "1093290"
  },
  {
    "text": "Let me just talk a\nlittle bit about what can happen here in the\ncontext of Q-learning sort of model-free learning.",
    "start": "1093290",
    "end": "1100559"
  },
  {
    "text": "So this is the BCQ, Behavior\nConstrained Q-learning,",
    "start": "1100560",
    "end": "1105690"
  },
  {
    "text": "from Scott Fujimoto. And what this shows here\nis that-- so these are",
    "start": "1105690",
    "end": "1111091"
  },
  {
    "text": "a bunch of different methods. This is DQN, Deep Q-learning. This is behavior cloning.",
    "start": "1111092",
    "end": "1117600"
  },
  {
    "text": "This is the behavioral policy. So what they did is\nthey gathered some data, and then they tried to\nuse different methods",
    "start": "1117600",
    "end": "1124020"
  },
  {
    "text": "to learn a policy from it. And this is DPG. And what they found with this\nis that some of the methods",
    "start": "1124020",
    "end": "1131070"
  },
  {
    "text": "did really bad, even\ngiven the behavior data. DQN does about the same\nas the behavior data.",
    "start": "1131070",
    "end": "1137409"
  },
  {
    "text": "But what they found here is\nthat by being a bit more careful and using methods that\nwere explicitly designed",
    "start": "1137410",
    "end": "1144420"
  },
  {
    "text": "to handle this offline data-- in this case, BCQ\nbehavior, constrained Q-learning-- they could\ndo substantially better.",
    "start": "1144420",
    "end": "1152620"
  },
  {
    "text": "And so that suggests\nthat we don't probably just want to use-- if we\nknow our data is fixed, and we know we're not going\nto get additional data,",
    "start": "1152620",
    "end": "1158938"
  },
  {
    "text": "that it may be worth\nit for us to use different types of\nalgorithms in order to handle the fact that\nour data is constrained",
    "start": "1158938",
    "end": "1164470"
  },
  {
    "text": "and we're not going to be\ncontinuing to get fresh data. So that motivates why we're\ngoing to need new methods.",
    "start": "1164470",
    "end": "1171910"
  },
  {
    "text": "All right. So now what we're going to do\nis dive into policy evaluation, and then we'll talk about\npolicy optimization afterwards.",
    "start": "1171910",
    "end": "1179210"
  },
  {
    "text": "So in batch policy\nevaluation, what we're going to be\nthinking about is",
    "start": "1179210",
    "end": "1184679"
  },
  {
    "text": "we have a particular\npolicy of interest. And we have a data\nset, and we'd like to be able to use that\ndata set to estimate",
    "start": "1184680",
    "end": "1190530"
  },
  {
    "text": "how good that policy\nis for one state or on average over a\nset of starting states--",
    "start": "1190530",
    "end": "1197010"
  },
  {
    "text": "so similar to what we've seen\nfor policy evaluation before. One thing I want to highlight--",
    "start": "1197010",
    "end": "1205430"
  },
  {
    "text": "this is by Phil Thomas, who\nI had the privilege of having as my postdoc a few years ago. He is a professor\nat UMass Amherst.",
    "start": "1205430",
    "end": "1213680"
  },
  {
    "text": "We generally want to think\nabout sample efficient methods for doing this. So in this case, he\nwas working with Adobe,",
    "start": "1213680",
    "end": "1220640"
  },
  {
    "text": "and they have 10 million\nto 20 million trajectories. It doesn't matter too\nmuch what these lines are.",
    "start": "1220640",
    "end": "1226769"
  },
  {
    "text": "The key thing here is that\nthis is the behavior policy, and you want to be\nlearning policies",
    "start": "1226770",
    "end": "1232100"
  },
  {
    "text": "which you're confident are\nbetter than your behavior policy. And this is just to highlight\nthat depending on the methods",
    "start": "1232100",
    "end": "1237710"
  },
  {
    "text": "you use, you may be confident\nat very different points, so just meaning\nthat data efficiency and having good algorithms\nis going to matter a lot.",
    "start": "1237710",
    "end": "1244690"
  },
  {
    "text": "Yeah. By behavior, you mean\nthe policy that was observed in the current set?",
    "start": "1244690",
    "end": "1250910"
  },
  {
    "text": "Exactly-- behavior policy. Great, great\nclarification question. When I say behavior\npolicy today, what I mean is the policy that\nwas used to gather",
    "start": "1250910",
    "end": "1257360"
  },
  {
    "text": "the data set that you have. So I'll just write\nthat out on here. ",
    "start": "1257360",
    "end": "1263030"
  },
  {
    "text": "So we're going to assume\nbehavior policy is the one that",
    "start": "1263030",
    "end": "1268760"
  },
  {
    "text": "was used to gather your data. ",
    "start": "1268760",
    "end": "1275040"
  },
  {
    "text": "All right. Let's first think\nabout using models. So this is actually\nthe first thing we tried to do with Refraction.",
    "start": "1275040",
    "end": "1280250"
  },
  {
    "text": "We thought, OK, great. Travis Mendel, who is the grad\nstudent leading the project, we have all this\nhistorical data.",
    "start": "1280250",
    "end": "1286880"
  },
  {
    "text": "Let's just try to\nlearn models from it. So we're going to\nlook at, and we're going to represent the\nstate space in some way.",
    "start": "1286880",
    "end": "1294370"
  },
  {
    "text": "And then under\ndifferent actions-- here in this case, that's\njust different levels. And there's only a finite\nnumber of levels and activities.",
    "start": "1294370",
    "end": "1300397"
  },
  {
    "text": "Let's learn a dynamics model. So in this case, the idea is\nthat we have that existing",
    "start": "1300397",
    "end": "1306309"
  },
  {
    "text": "data set, and we're going to\nlearn an explicit dynamics model. And we can learn an\nexplicit reward model.",
    "start": "1306310",
    "end": "1311450"
  },
  {
    "text": "Now, in our case,\nour reward model was known because\nit's persistence. So we could essentially\nget a reward every time",
    "start": "1311450",
    "end": "1318040"
  },
  {
    "text": "the student didn't\nquit the game, but we didn't know\nthe dynamics model. And so that's what we're\nusing the data to learn.",
    "start": "1318040",
    "end": "1324520"
  },
  {
    "text": "Now, as you might imagine, we\nhad to make a lot of choices here about what state\nrepresentations you would use.",
    "start": "1324520",
    "end": "1330601"
  },
  {
    "text": "And so we thought about lots and\nlots and lots of different state representations.",
    "start": "1330602",
    "end": "1335950"
  },
  {
    "text": "But once you have that, then you\ncan treat this as a simulator. So now you have your\nsimulator of the world",
    "start": "1335950",
    "end": "1341953"
  },
  {
    "text": "because you have a dynamics\nmodel or a reward model. Either you can do\nthis analytically, like in some of\nthe methods we saw",
    "start": "1341953",
    "end": "1347470"
  },
  {
    "text": "in some of the\nfirst few classes, or you can use dynamic\nprogramming or Q-learning,",
    "start": "1347470",
    "end": "1353910"
  },
  {
    "text": "Q-evaluation with those\nto explicitly learn what the value is.",
    "start": "1353910",
    "end": "1359590"
  },
  {
    "text": "But really, can use anything. You can even use like\nMonte Carlo methods because you can try to\nlearn from this simulator",
    "start": "1359590",
    "end": "1367430"
  },
  {
    "text": "an optimal policy. So you can either try to\nlearn an optimal policy or you can evaluate\na specific one.",
    "start": "1367430",
    "end": "1372945"
  },
  {
    "text": "You can do either of those. So I'll just write that here. So you can either evaluation\nor learn a new policy",
    "start": "1372945",
    "end": "1382860"
  },
  {
    "text": "with any other oral\nmethod because now you",
    "start": "1382860",
    "end": "1389860"
  },
  {
    "text": "have a simulator. OK, let me show\nyou what happens.",
    "start": "1389860",
    "end": "1395890"
  },
  {
    "text": "All right. So the first thing I'm going\nto show you is the following. What we have on\nthe x-axis here is",
    "start": "1395890",
    "end": "1402650"
  },
  {
    "text": "different state representations\nof this environment. So these are obviously\nreally small state spaces.",
    "start": "1402650",
    "end": "1408630"
  },
  {
    "text": "Like we don't actually\nthink that human learning is encapsulated in terms of\nfive states or 10 states,",
    "start": "1408630",
    "end": "1414030"
  },
  {
    "text": "but you can just\nimagine sweeping this. So these are some\nof the state spaces we consider where we use really,\nreally condensed state spaces",
    "start": "1414030",
    "end": "1420440"
  },
  {
    "text": "or much more complicated ones. What this is showing\nhere is normalized score, and this is log likelihood.",
    "start": "1420440",
    "end": "1426500"
  },
  {
    "text": "And this is held out. So what this is saying\nis as you might expect,",
    "start": "1426500",
    "end": "1431580"
  },
  {
    "text": "as you increase your\nstate space complexity, you get a better\nfit on the data.",
    "start": "1431580",
    "end": "1437490"
  },
  {
    "text": "You can better predict the\nnext state of the student if you use a more\ncomplex state space.",
    "start": "1437490",
    "end": "1442940"
  },
  {
    "text": "And that's not totally\nsurprising, right. Because we think that human\nlearning is complicated, and so we really think we are\ngetting a better dynamics model.",
    "start": "1442940",
    "end": "1450240"
  },
  {
    "text": "And again, just to emphasize\nhere, this is cross validation. So this is on a held-out set.",
    "start": "1450240",
    "end": "1456649"
  },
  {
    "text": "It's a not training error. So we're doing better\nin terms of this.",
    "start": "1456650",
    "end": "1461912"
  },
  {
    "text": "Now, what are we doing with\nthese once we have these? Yeah, go ahead. [INAUDIBLE] Yes. Yeah.",
    "start": "1461912",
    "end": "1467340"
  },
  {
    "text": "So the data set\nsize is fixed here. What we're trying to\ndo is given the data that you've seen before,\nthere are all different ways.",
    "start": "1467340",
    "end": "1473010"
  },
  {
    "text": "We just have clickstream data. There's tons of ways to\nmodel that as state space. And so we're just\ndoing model selection.",
    "start": "1473010",
    "end": "1478880"
  },
  {
    "text": "Now what we were doing here then\nis once we had that simulator, we were trying to\nlearn a good policy,",
    "start": "1478880",
    "end": "1484549"
  },
  {
    "text": "and then we were\nevaluating the performance of that actual policy. Now I'll tell you\nhow we actually",
    "start": "1484550",
    "end": "1489860"
  },
  {
    "text": "evaluated that policy shortly,\nbut this is the important thing. So this is saying that the\nmodels that we're getting",
    "start": "1489860",
    "end": "1496430"
  },
  {
    "text": "are actually better. But here's the problem. If I take this\npolicy, which really",
    "start": "1496430",
    "end": "1504852"
  },
  {
    "text": "if I take this model, which\nreally is a better model, it really does fit\nthe data better. And then I do say dynamic\nprogramming with it,",
    "start": "1504852",
    "end": "1510950"
  },
  {
    "text": "and I extract an\noptimal pi star. So that's the procedure. I take my model. I learn an optimal\npolicy, and now I",
    "start": "1510950",
    "end": "1517500"
  },
  {
    "text": "want to know how good that\nactually is in the real world. If I evaluate that\nin the real world,",
    "start": "1517500",
    "end": "1522850"
  },
  {
    "text": "even though the model\nitself was actually better, what you can see is the\nactual value of that policy",
    "start": "1522850",
    "end": "1528330"
  },
  {
    "text": "is getting worse. OK, so I've got a\nbetter simulator, but the policy I get by\noptimizing for that better",
    "start": "1528330",
    "end": "1535850"
  },
  {
    "text": "simulator is worse. OK, so this is the actual\nunbiased reward estimator.",
    "start": "1535850",
    "end": "1542300"
  },
  {
    "text": "And I'll tell you shortly how\nwe do that because of course, under the model's opinion,\nthe model thinks it's--",
    "start": "1542300",
    "end": "1548380"
  },
  {
    "text": "the policy it's helping\nproduce is great. Let me just make sure\nthat the pipeline of what we're doing there is clear.",
    "start": "1548380",
    "end": "1553945"
  },
  {
    "text": "So what we do as\nwe are getting-- we're going from data to a\nmodel of the dynamics model.",
    "start": "1553945",
    "end": "1563630"
  },
  {
    "text": " And then we add in\na reward function.",
    "start": "1563630",
    "end": "1569539"
  },
  {
    "text": "And we extract a pi star for\nthat estimated dynamics model.",
    "start": "1569540",
    "end": "1575170"
  },
  {
    "text": "But that's just\nunder the simulator. And then what I want\nto know actually is what the true value is of\nthat policy I've computed.",
    "start": "1575170",
    "end": "1584740"
  },
  {
    "text": "And what this\ngraph is showing is that even though my\nmodel is getting better,",
    "start": "1584740",
    "end": "1591610"
  },
  {
    "text": "the actual performance of\nthe value I'm getting out is getting worse. Now, when we first saw this,\nwe were kind of confused.",
    "start": "1591610",
    "end": "1598260"
  },
  {
    "text": "We weren't quite sure\nwhy this was happening. And in fact, there had been\nsome work a few years prior",
    "start": "1598260",
    "end": "1603660"
  },
  {
    "text": "to this in the educational data\nmining community that suggested doing exactly what we were doing\nhere, which was build a model,",
    "start": "1603660",
    "end": "1611049"
  },
  {
    "text": "then use it to simulate\nand learn a good policy, and then deploy the\npolicy that looked best.",
    "start": "1611050",
    "end": "1616940"
  },
  {
    "text": "But what our work here suggested\nis that it was not a good idea. Now, the reason for that\nis because the model",
    "start": "1616940",
    "end": "1623850"
  },
  {
    "text": "is misspecified. Now that means that under\nthis model misspecification,",
    "start": "1623850",
    "end": "1631980"
  },
  {
    "text": "the value it's getting when it\ncomputes the optimal policy. ",
    "start": "1631980",
    "end": "1638230"
  },
  {
    "text": "So you can think of there\nbeing two things here. There is one thing which is\nv hat of pi hat star, which",
    "start": "1638230",
    "end": "1644920"
  },
  {
    "text": "is its own estimate of\nhow good its value is. And then there is\nthe true value of it.",
    "start": "1644920",
    "end": "1651800"
  },
  {
    "text": "And these in general are\ngoing to be different, and these in particular\nare going to be different if your estimated model is bad.",
    "start": "1651800",
    "end": "1659090"
  },
  {
    "text": "So it's going to\nthink I'm doing great. This is going to help students\npersist till the end of time. But if the model\nis misspecified,",
    "start": "1659090",
    "end": "1666419"
  },
  {
    "text": "meaning that even\nwith infinite data, it will not converge to the\ntrue model of student learning,",
    "start": "1666420",
    "end": "1672020"
  },
  {
    "text": "then that estimate\nwill be wrong. And as you might imagine here,\n20 state model of human learning",
    "start": "1672020",
    "end": "1678620"
  },
  {
    "text": "is not that great. Yeah. [INAUDIBLE] 10 state.",
    "start": "1678620",
    "end": "1684120"
  },
  {
    "text": "Yeah. Yeah. So it's not saying that-- it's not saying that\nsome of these policies",
    "start": "1684120",
    "end": "1689660"
  },
  {
    "text": "might not be good policies. What this was arguing to us-- so it's a great question.",
    "start": "1689660",
    "end": "1694730"
  },
  {
    "text": "It's not that inside of\nthese there might not be pretty decent policy classes. You could argue that education\nworks because there's",
    "start": "1694730",
    "end": "1702050"
  },
  {
    "text": "decentish policies. I mean, I don't have\nperfect models of all of you guys' learning, but\nit's still sufficient for us",
    "start": "1702050",
    "end": "1707930"
  },
  {
    "text": "to be able to learn\nand communicate. What I'm arguing here is\nthat we should not just use the accuracy of\nthe dynamics model",
    "start": "1707930",
    "end": "1717170"
  },
  {
    "text": "as like a proxy for which of the\nvalues or which of the policies",
    "start": "1717170",
    "end": "1722270"
  },
  {
    "text": "to pick. This is arguing that we need\nseparate independent estimates of really-- we want to\nbasically in some ways",
    "start": "1722270",
    "end": "1728990"
  },
  {
    "text": "kind of like what we saw\nwith PPO and policy learning. We would like to\ndirectly evaluate the performance of\na policy instead",
    "start": "1728990",
    "end": "1735770"
  },
  {
    "text": "of using as a proxy, how much\nour Q-function is changing or how accurate we think\nour dynamics model is.",
    "start": "1735770",
    "end": "1743720"
  },
  {
    "text": "Yes. So when we evaluate\nthe policy, we execute it under\nthe real environment",
    "start": "1743720",
    "end": "1750799"
  },
  {
    "text": "or estimate the policy\nperformance using our estimate. So there's two things.",
    "start": "1750800",
    "end": "1757310"
  },
  {
    "text": "We can do it under\nour simulated model, or we can do it\nunder our real model.",
    "start": "1757310",
    "end": "1762320"
  },
  {
    "text": "We don't want to have to\ndo it under our real model because we want to know\nwhich policy to deploy before we actually deploy it.",
    "start": "1762320",
    "end": "1768387"
  },
  {
    "text": "Otherwise, we could\nkind of be doing online. So what I'll shortly\nbe giving you is a way to get an accurate\nestimate of how good the policy",
    "start": "1768387",
    "end": "1775190"
  },
  {
    "text": "is before we deploy it. I haven't said how\nto do that yet. I've just argued that using\nmodels alone might not be good.",
    "start": "1775190",
    "end": "1780892"
  },
  {
    "text": "Do you have a question? OK, yeah. So about model\nmisspecification is one way",
    "start": "1780892",
    "end": "1787110"
  },
  {
    "text": "to think about this\njust like you're kind of overfitting\nyour dynamics",
    "start": "1787110",
    "end": "1793230"
  },
  {
    "text": "model by increasing the number\nof states that you represented? It's a question.",
    "start": "1793230",
    "end": "1799290"
  },
  {
    "text": "We're not overfitting\nhere because it really is a better fit. It's still just\nnot a perfect fit.",
    "start": "1799290",
    "end": "1805360"
  },
  {
    "text": "In other ways, you might say\nthis is it's not realizable. This is not the real\nmodel of student learning. And under this, that\nmeans that there's",
    "start": "1805360",
    "end": "1811650"
  },
  {
    "text": "still essentially significant\nbias when we do this learning. ",
    "start": "1811650",
    "end": "1819679"
  },
  {
    "text": "Now, one thing I\njust want to note is model-based learning\ncan still be helpful. One thing that we may\nwant to do in this case",
    "start": "1819680",
    "end": "1826700"
  },
  {
    "text": "is explicitly build\ndifferent models when we know we want to\nevaluate different policies. So normally, when\nwe fit a model,",
    "start": "1826700",
    "end": "1833370"
  },
  {
    "text": "we try to minimize the loss\nunder the data distribution of the behavior policy. So if you have a bunch of data\nand you fit your dynamics model,",
    "start": "1833370",
    "end": "1841019"
  },
  {
    "text": "you're essentially trying\nto optimize for the accuracy over your behavior policy. But if you know that the\npolicy you want to evaluate",
    "start": "1841020",
    "end": "1847430"
  },
  {
    "text": "is different, you\ncan actually change. So you can weigh your\nerrors separately.",
    "start": "1847430",
    "end": "1852570"
  },
  {
    "text": "So this is a paper that\nwe did a few years ago with [INAUDIBLE]\nand Omer Gottesman",
    "start": "1852570",
    "end": "1858320"
  },
  {
    "text": "and others, which\njust highlighted this, that you could change\nyour loss function and essentially up weigh\nyour accuracy over the state",
    "start": "1858320",
    "end": "1864726"
  },
  {
    "text": "and action pairs that you\nthink you will encounter under a different policy. And that can help a lot.",
    "start": "1864727",
    "end": "1869940"
  },
  {
    "text": "So you can see here this\nwas for a medical domain. And what you can see is that\nthis green here is ground truth.",
    "start": "1869940",
    "end": "1877290"
  },
  {
    "text": "And what we found in this case--\nso ours was our model here. And this is just if you fit\nfor the behavior policy.",
    "start": "1877290",
    "end": "1883787"
  },
  {
    "text": "And what you can see is\nby essentially reweighting your data, you can fit dynamics\nmodels that are much better fit the type of dynamics\nyou'd see in the future.",
    "start": "1883787",
    "end": "1894000"
  },
  {
    "text": "But now I'm going to introduce\nsort of model-free methods, and then we're going to get\ninto importance sampling",
    "start": "1894000",
    "end": "1899309"
  },
  {
    "text": "as other ways to try to do this\npolicy evaluation that hopefully have different limitations\nor less limitations",
    "start": "1899310",
    "end": "1904920"
  },
  {
    "text": "compared to the\nmodel-based method.  So one of the first methods\nthat I'll talk about here",
    "start": "1904920",
    "end": "1911960"
  },
  {
    "text": "is fitted Q evaluation. So fitted Q evaluation\nis going to look pretty similar to deep Q-learning,\nbut there's just",
    "start": "1911960",
    "end": "1919130"
  },
  {
    "text": "going to be a couple\nimportant differences. So our data set here is a\nbunch of just different tuples",
    "start": "1919130",
    "end": "1924860"
  },
  {
    "text": "of state action\nreward next state. Recall that our Q-function\nQ pi is just going to be",
    "start": "1924860",
    "end": "1933070"
  },
  {
    "text": "the immediate reward we got\nfrom being in that si, a tuple-- so whatever we saw\nin our data set--",
    "start": "1933070",
    "end": "1938830"
  },
  {
    "text": "and then we'll put in plus\ngamma times V pi of S I plus 1.",
    "start": "1938830",
    "end": "1945049"
  },
  {
    "text": "And then what we do\nis we try to minimize the difference between this\nunder a parameterized function,",
    "start": "1945050",
    "end": "1950810"
  },
  {
    "text": "just like what we saw with deep\nQ-learning versus the observed data tuples. So you can think of\nthis as our target.",
    "start": "1950810",
    "end": "1957130"
  },
  {
    "text": " And this is called\nfitted Q evaluation.",
    "start": "1957130",
    "end": "1964150"
  },
  {
    "text": "It's closely related to\nsomething called FQI, which is fitted Q iteration,\nwhich I think was around 2015,",
    "start": "1964150",
    "end": "1978630"
  },
  {
    "text": "2005ish. And so this is very\nsimilar to what we've seen",
    "start": "1978630",
    "end": "1985420"
  },
  {
    "text": "with deep Q-learning before. We just fit this function. The key thing here is\nthat we want it to be",
    "start": "1985420",
    "end": "1991000"
  },
  {
    "text": "for just a single policy pi. So we're not doing an argmax.",
    "start": "1991000",
    "end": "1996640"
  },
  {
    "text": "So this is how the algorithm\nworks for fitted Q evaluation. We sort of initialize\nour Q function randomly.",
    "start": "1996640",
    "end": "2003450"
  },
  {
    "text": "It could be a deep Q Network. It could be something else. We compute the targets where\nwhen we put in the next Q,",
    "start": "2003450",
    "end": "2011630"
  },
  {
    "text": "we have to use the policy\nwe're interested in evaluating.",
    "start": "2011630",
    "end": "2017150"
  },
  {
    "text": "So we're only doing this for\nthe actions we would take under the policy we care about. We build our training set of\nXs, actions, and our output Q",
    "start": "2017150",
    "end": "2026990"
  },
  {
    "text": "target, and then we\nfit our Q function. ",
    "start": "2026990",
    "end": "2032789"
  },
  {
    "text": "And so, again, the key\ndifference here compared to DQN is there's no max.",
    "start": "2032790",
    "end": "2038700"
  },
  {
    "text": "We are fixing this part\nonly for a fixed pi.",
    "start": "2038700",
    "end": "2050473"
  },
  {
    "text": " But aside from that, it\nshould look really similar.",
    "start": "2050474",
    "end": "2056638"
  },
  {
    "text": "And so one of the--\nso this was something that was very closely\nrelated to a common algorithm",
    "start": "2056639",
    "end": "2062879"
  },
  {
    "text": "for doing off-policy\nlearning, which is fitted Q iteration, excuse\nme, which is very related to deep Q learning.",
    "start": "2062880",
    "end": "2069688"
  },
  {
    "text": "And one of the things\npeople wanted to understand is whether this thing that was\nworking in practice actually",
    "start": "2069688",
    "end": "2075179"
  },
  {
    "text": "had some theoretical\ngrounding behind it. Like, could we say\nanything formal about how good\nthis approach was.",
    "start": "2075179",
    "end": "2080419"
  },
  {
    "text": " So just to give\nyou an illustration of the types of\nguarantees that we",
    "start": "2080420",
    "end": "2086449"
  },
  {
    "text": "can get in this\ncase, what we want to look at in this situation\nis to think about what",
    "start": "2086449",
    "end": "2093980"
  },
  {
    "text": "is the generalization error. OK, let me put this in here. So I won't go through\nthe whole paper.",
    "start": "2093980",
    "end": "2101545"
  },
  {
    "text": "I just want to give\nyou an illustration of the types of guarantees that\nyou might get in this setting. What they would like\nto know in this case",
    "start": "2101545",
    "end": "2108000"
  },
  {
    "text": "is to compare the\ndifference between the value that you will compute under this\nprocedure versus the true value",
    "start": "2108000",
    "end": "2113940"
  },
  {
    "text": "of the policy. This is your normal\ndiscount factor. And then there's a whole bunch\nof things that are additional.",
    "start": "2113940",
    "end": "2120550"
  },
  {
    "text": "Let me highlight some\nimportant things here. And here is the number\nof samples you need. So n tells you\nabout how much data",
    "start": "2120550",
    "end": "2127170"
  },
  {
    "text": "you're going to need\nin order to do this. So this is how much data-- ",
    "start": "2127170",
    "end": "2133170"
  },
  {
    "text": "much data. OK, epsilon is sort of your\ndesired target accuracy.",
    "start": "2133170",
    "end": "2144376"
  },
  {
    "text": " This is one of the\nreally important things.",
    "start": "2144376",
    "end": "2150750"
  },
  {
    "text": "So we're going to\nhave something called a concentratability coefficient.",
    "start": "2150750",
    "end": "2158136"
  },
  {
    "text": " Concentratability\ncoefficient is going",
    "start": "2158136",
    "end": "2164460"
  },
  {
    "text": "to be the difference essentially\nbetween the distribution of state action pairs that\nyou have in your data set and the distribution\nof state action",
    "start": "2164460",
    "end": "2170940"
  },
  {
    "text": "pairs you would get under\nyour desired policy. So we saw this before\nwith PPO of thinking about these divergence in the\nstate action distributions--",
    "start": "2170940",
    "end": "2179910"
  },
  {
    "text": "state action distributions. And it's also related to what\nwe'll call overlap later.",
    "start": "2179910",
    "end": "2189859"
  },
  {
    "text": "So I won't go through all\nthe details in this case, but I want to just give an\nillustration that people often think about trying to\nunderstand if you have a data",
    "start": "2189860",
    "end": "2197450"
  },
  {
    "text": "set of some behavior data,\nhow accurate you can hope to be of evaluating the\nperformance of the policy",
    "start": "2197450",
    "end": "2203750"
  },
  {
    "text": "depends on your discount\nfactor, because that says how accurate you want to\nbe and how much you care",
    "start": "2203750",
    "end": "2209119"
  },
  {
    "text": "about long-term\nrewards, how much data you have in terms of your target\nerror, and how closely related",
    "start": "2209120",
    "end": "2215270"
  },
  {
    "text": "your state action\ndistributions are from your training set to your\ntest set or your desired policy.",
    "start": "2215270",
    "end": "2221460"
  },
  {
    "text": "Now one of the challenges\nabout this approach is that it generally still\nrelies on the Markov assumption.",
    "start": "2221460",
    "end": "2228492"
  },
  {
    "text": "So we're still assuming\nour data is all Markov, and it relies on our models. In these case, the sort of Q\nfunctions being well specified.",
    "start": "2228492",
    "end": "2235819"
  },
  {
    "text": "So what do I mean by that? It means that we really\ncan fit the Q function. There's some existing Q function\nin the world for our policy,",
    "start": "2235820",
    "end": "2242660"
  },
  {
    "text": "and we can really fit it. And if you say, for example--",
    "start": "2242660",
    "end": "2249370"
  },
  {
    "text": "let's say that this\nis your state space. It's just one-dimensional.",
    "start": "2249370",
    "end": "2255090"
  },
  {
    "text": "And this is what your\ntrue function looks like. You can imagine that it\nlooks something like this.",
    "start": "2255090",
    "end": "2260920"
  },
  {
    "text": "And let's say that\nyou are restricting yourself to fit a line like\nthat with just two parameters.",
    "start": "2260920",
    "end": "2268500"
  },
  {
    "text": "So in that case, even if you\nhad infinite amounts of data, you're still going to\nhave a lot of error. You're not going to be\nable to fit the Q function.",
    "start": "2268500",
    "end": "2274350"
  },
  {
    "text": "So these methods assume\ntypically realizability, meaning that if you\nhad infinite data, you could fit the function.",
    "start": "2274350",
    "end": "2279480"
  },
  {
    "text": "The problem is that you\ndon't have infinite data. OK, all right. So now we're going to see a\nreally beautiful method called",
    "start": "2279480",
    "end": "2285359"
  },
  {
    "text": "importance sampling, which\nallows us to deal with this. We've seen sort of brief\nideas about this before,",
    "start": "2285360",
    "end": "2290410"
  },
  {
    "text": "but I'm curious if anybody who's\nseen this in other classes. Who's seen importance\nsampling before?",
    "start": "2290410",
    "end": "2295890"
  },
  {
    "text": "So just a couple of people. This is one of the\nfavorite ideas in CS234",
    "start": "2295890",
    "end": "2301110"
  },
  {
    "text": "according to some past people. All right. So the idea what\nis the motivation? So importance sampling is\nan idea from statistics",
    "start": "2301110",
    "end": "2308670"
  },
  {
    "text": "that we have imported over\ninto reinforcement learning. Why would we like to do this? Well, we want a method that\ndoesn't rely on the models being",
    "start": "2308670",
    "end": "2315740"
  },
  {
    "text": "correct, meaning that we\ncan actually fit things with a two-layer deep\nneural network or stuff,",
    "start": "2315740",
    "end": "2321318"
  },
  {
    "text": "and that we don't have to\nrely on the Markov assumption in the state space we're using. We saw before that we\ncould use Monte Carlo",
    "start": "2321318",
    "end": "2327869"
  },
  {
    "text": "methods to accomplish this\nfor online policy evaluation. And now we want to do\nthis for offline data,",
    "start": "2327870",
    "end": "2333198"
  },
  {
    "text": "meaning that we have data\nfrom a different distribution from the policy we\nwant to evaluate. And the key challenge,\nas has often been,",
    "start": "2333198",
    "end": "2339880"
  },
  {
    "text": "is data distribution mismatch. OK, so here's how\nimportance sampling works.",
    "start": "2339880",
    "end": "2345880"
  },
  {
    "text": "Let me just specify\nwhat this means. Let's say we want to try to\nunderstand the expected reward",
    "start": "2345880",
    "end": "2351360"
  },
  {
    "text": "over a distribution of states. So for this part, you can just\nthink of x is equal to states. ",
    "start": "2351360",
    "end": "2359480"
  },
  {
    "text": "And R of x is equal to\nthe reward of a state. This works for very, very\ngeneral distributions.",
    "start": "2359480",
    "end": "2366340"
  },
  {
    "text": "But you can think of that here\nas just being a [INAUDIBLE]. All right. What we're going to\ndo is the following.",
    "start": "2366340",
    "end": "2372610"
  },
  {
    "text": "This is what we would\nlike to evaluate. So you could think\nof this here as maybe being p of x could be equal\nto the probability of reaching",
    "start": "2372610",
    "end": "2383940"
  },
  {
    "text": "x under policy.",
    "start": "2383940",
    "end": "2390329"
  },
  {
    "text": "So you might really want this. You might want to know what\nis the expected reward I'm going to get under this policy\nwhere I know what my reward is",
    "start": "2390330",
    "end": "2396910"
  },
  {
    "text": "for each state or I\nhave samples of it, and then I have this\nprobability distribution. The problem is that you\ndon't have data from that.",
    "start": "2396910",
    "end": "2403040"
  },
  {
    "text": "So we put no data from p of x.",
    "start": "2403040",
    "end": "2409020"
  },
  {
    "text": "So that's the general\nchallenge we're in. We want to see how well our\nalternative policy would",
    "start": "2409020",
    "end": "2414060"
  },
  {
    "text": "work for helping\nstudents persist, but we have no data from that. So here's the trick.",
    "start": "2414060",
    "end": "2420630"
  },
  {
    "text": "Let's multiply and\ndivide by the same thing. I'm going to introduce a new\npolicy and its distribution Q",
    "start": "2420630",
    "end": "2431529"
  },
  {
    "text": "OK. So q of x is a different policy. ",
    "start": "2431530",
    "end": "2442065"
  },
  {
    "text": "This is a different policy. Maybe it's going to end\nup in different states with different probabilities. OK, so let's rewrite this.",
    "start": "2442065",
    "end": "2448810"
  },
  {
    "text": "This is going to be\nequal to q of x times p",
    "start": "2448810",
    "end": "2453840"
  },
  {
    "text": "of x over r over q of x r of x.",
    "start": "2453840",
    "end": "2460620"
  },
  {
    "text": "OK, I haven't\nchanged anything yet. This is exactly equal. But if I have data\nfrom q of x, I",
    "start": "2460620",
    "end": "2467100"
  },
  {
    "text": "can approximate this\nexcitation with samples. So this is approximately\nequal to 1 over n sum",
    "start": "2467100",
    "end": "2474630"
  },
  {
    "text": "over I equals 1\nto n of x sampled",
    "start": "2474630",
    "end": "2479849"
  },
  {
    "text": "according to q of x, p\nof xi, q of xi, r of xi.",
    "start": "2479850",
    "end": "2488820"
  },
  {
    "text": " This is super beautiful. What we've said here\nis that I really",
    "start": "2488820",
    "end": "2494950"
  },
  {
    "text": "want to estimate the expectation\nof something over, say, policy, this policy p. I don't have any samples from p.",
    "start": "2494950",
    "end": "2501370"
  },
  {
    "text": "What I can do is I can just\ntake samples from my policy q, and I can reweigh them.",
    "start": "2501370",
    "end": "2508480"
  },
  {
    "text": "So it says, if I was\nreally likely to take-- to reach a particular\nx under a policy q",
    "start": "2508480",
    "end": "2514900"
  },
  {
    "text": "but less likely under this\none, I'll weigh that data less. If I'm much more likely\nto get to a state xi",
    "start": "2514900",
    "end": "2520510"
  },
  {
    "text": "than I was under here, I'm\ngoing to upweight those samples. ",
    "start": "2520510",
    "end": "2525760"
  },
  {
    "text": "So this is beautiful,\nand it's unbiased. So this is an unbiased estimate.",
    "start": "2525760",
    "end": "2532900"
  },
  {
    "text": "We'll extend it in a second to\nthink about multi time steps, but just for single\ntime step right now,",
    "start": "2532900",
    "end": "2538090"
  },
  {
    "text": "this is how we can do this. It gives us an\nunbiased estimate. And as we'll see shortly, we can\nextend this to multi time steps.",
    "start": "2538090",
    "end": "2544120"
  },
  {
    "text": "And we don't have to\nmake a Markov assumption. OK, so this is a\nreally lovely idea.",
    "start": "2544120",
    "end": "2550880"
  },
  {
    "text": "So we can compute\nthis expected value under an alternative\ndistribution. ",
    "start": "2550880",
    "end": "2557910"
  },
  {
    "text": "And it is generally\nan unbiased estimator under a couple of assumptions. The first is that the\nsampling distribution Q--",
    "start": "2557910",
    "end": "2565020"
  },
  {
    "text": "so our alternative policy-- has to be greater than or equal\nto 0 for all x such that P of x",
    "start": "2565020",
    "end": "2571560"
  },
  {
    "text": "would be greater than zero. What does that mean in practice? That means that if you could\nreach a state under your policy",
    "start": "2571560",
    "end": "2578490"
  },
  {
    "text": "you care about with a\nnon-zero probability. So let's say, I don't know. Your student could get\nto this particular level",
    "start": "2578490",
    "end": "2584220"
  },
  {
    "text": "with non-zero probability\nunder your target policy, then there has to be some\nprobability you'd also get there",
    "start": "2584220",
    "end": "2590730"
  },
  {
    "text": "under your training data set. This is sort of\nreasonable, right.",
    "start": "2590730",
    "end": "2595750"
  },
  {
    "text": "So this says that if I\nwant to think about-- I don't know-- a policy that\nlike recommends restaurants",
    "start": "2595750",
    "end": "2603960"
  },
  {
    "text": "versus coffee shops, I can't\nuse that data to then estimate how good it would be\nto go to the movies,",
    "start": "2603960",
    "end": "2610060"
  },
  {
    "text": "because I've never done that. For anything that we're\ntrying to estimate here, we have to have non-zero\nprobability for that x.",
    "start": "2610060",
    "end": "2618109"
  },
  {
    "text": "The second thing is a\nlittle bit more subtle, but it comes up a lot in\nreal empirical data, which is called no hidden confounding.",
    "start": "2618110",
    "end": "2624240"
  },
  {
    "text": "And that means that\nessentially you have to know all of the\nfeatures that were used to define this distribution.",
    "start": "2624240",
    "end": "2632790"
  },
  {
    "text": "So this doesn't-- may not\nseem as clear in this part, but I think once we start\ngetting into multi time steps and the sequences, it\nbecomes really relevant.",
    "start": "2632790",
    "end": "2639960"
  },
  {
    "text": "So let me give an example. ",
    "start": "2639960",
    "end": "2674250"
  },
  {
    "text": "OK, so imagine like a\nhealth care setting. So if we go back to that\nelectronic medical record setting, we often are\ninterested in what",
    "start": "2674250",
    "end": "2680760"
  },
  {
    "text": "would have happened to a patient\nif we did a different action. So we want to know what\nthat counterfactual is.",
    "start": "2680760",
    "end": "2685920"
  },
  {
    "text": "One of the challenges\nthere is that we will have certain\nfeatures that are in our electronic\nmedical record system.",
    "start": "2685920",
    "end": "2692680"
  },
  {
    "text": "We will see an action, like\nsomeone was taken to surgery or some drug was administered,\nand then we see the outcome.",
    "start": "2692680",
    "end": "2698760"
  },
  {
    "text": "In order for importance\nsampling to work, all of the features that were\nused to make that decision",
    "start": "2698760",
    "end": "2704400"
  },
  {
    "text": "or pick that action\nhave to be known. And that's called no\nhidden confounding.",
    "start": "2704400",
    "end": "2711030"
  },
  {
    "text": "Now, why is that? Well, it might be,\nfor example, you might see that there are\ncertain patients that are sick,",
    "start": "2711030",
    "end": "2717870"
  },
  {
    "text": "and then a particular\naction is taken. And maybe they die. And you might see\nother patients that look like they have\nthe same features,",
    "start": "2717870",
    "end": "2724110"
  },
  {
    "text": "and a different action is taken. And they live. And in that case, you might\nthink, oh, maybe the decision",
    "start": "2724110",
    "end": "2730170"
  },
  {
    "text": "was just bad. That's possible. But it's also possible\nthat there are just hidden additional features that\nyou don't have in your data.",
    "start": "2730170",
    "end": "2736760"
  },
  {
    "text": "And that meant that the first\nperson was much more sick, and that's why they got\nthat particular treatment versus the other person.",
    "start": "2736760",
    "end": "2743590"
  },
  {
    "text": "So it might be that there's\nimportant reasons that are not part of x that are being\nused and used to define what",
    "start": "2743590",
    "end": "2751359"
  },
  {
    "text": "the action is in the data set. Excuse me. And in those sorts of\nconfounding scenarios,",
    "start": "2751360",
    "end": "2756600"
  },
  {
    "text": "and if you try to use\nimportance sampling, you will not get an\nunbiased estimator. This is really important\nand really hard in practice.",
    "start": "2756600",
    "end": "2763810"
  },
  {
    "text": "It comes up all the time. And in fact, one of\nthe things we were just doing on a paper\nwe just put online,",
    "start": "2763810",
    "end": "2768930"
  },
  {
    "text": "we were trying to think\nreally, really carefully about whether or not there would\nbe additional confounding beyond the features that\nwe had in our data set.",
    "start": "2768930",
    "end": "2777509"
  },
  {
    "text": "So in that case, we\nhad done an experiment to see whether or not offering\nstudents access to GPT-4",
    "start": "2777510",
    "end": "2783330"
  },
  {
    "text": "would increase or\ndecrease participation in the class and exam scores.",
    "start": "2783330",
    "end": "2788970"
  },
  {
    "text": "And only some people used GPT-4. A lot of people that were given\naccess to it did not use it.",
    "start": "2788970",
    "end": "2795240"
  },
  {
    "text": "And so an important question\nthere then is, well, is there something intrinsically\ndifferent about those students who were using it that also\nwould confound their test",
    "start": "2795240",
    "end": "2804270"
  },
  {
    "text": "scores? And so this issue of hidden\nconfounding comes up a lot,",
    "start": "2804270",
    "end": "2810410"
  },
  {
    "text": "particularly when\nactions are optional or being made by humans.",
    "start": "2810410",
    "end": "2815500"
  },
  {
    "text": "Now, if you're in\nMuJoCo or something, this is easy because if you\nhave control over the simulator, you don't have to\nworry about it.",
    "start": "2815500",
    "end": "2820905"
  },
  {
    "text": "But it's important\nto know in practice. All right. Let's take a second and\ncheck your understanding.",
    "start": "2820905",
    "end": "2826460"
  },
  {
    "text": "So we haven't really\ntalked about bandits yet. Don't worry about exactly this. We're going to be doing\npolicy evaluation.",
    "start": "2826460",
    "end": "2833099"
  },
  {
    "text": "So let's say we have\na data set for-- we'll just say samples-- for samples from three actions.",
    "start": "2833100",
    "end": "2840020"
  },
  {
    "text": "OK, action one is a Bernoulli\nvariable with probability 0.02. You get a really\nhigh reward of 0.",
    "start": "2840020",
    "end": "2847160"
  },
  {
    "text": "The second one,\nyou get probability with probability 0.55. You get reward of 2 else 0, and\nthe third one with probability",
    "start": "2847160",
    "end": "2854000"
  },
  {
    "text": "0.5 get a reward of 1 else 0. Your data is going to be sampled\nfrom a particular behavior",
    "start": "2854000",
    "end": "2860390"
  },
  {
    "text": "policy. So this is what we've been\ncalling a behavior policy where",
    "start": "2860390",
    "end": "2869980"
  },
  {
    "text": "with probability 0.8,\nit pulls this action. Else it pulls action two.",
    "start": "2869980",
    "end": "2875140"
  },
  {
    "text": "The policy we want to evaluate\npi 2 pulls action two. Excuse me, it pulls action one.",
    "start": "2875140",
    "end": "2881910"
  },
  {
    "text": "This question asks you\nto think about what are true about the\nperformance of those policies,",
    "start": "2881910",
    "end": "2888549"
  },
  {
    "text": "whether or not we could\nuse the data from pi 1 to get an unbiased\nestimator of pi 2",
    "start": "2888550",
    "end": "2894905"
  },
  {
    "text": "and whether or not the rewards\nbeing positive or negative might impact that. ",
    "start": "2894905",
    "end": "2943270"
  },
  {
    "text": "The third one is kind\nof hard and might require looking back\nat the equations on the previous slides.",
    "start": "2943270",
    "end": "2948280"
  },
  {
    "start": "2948280",
    "end": "2953610"
  },
  {
    "text": "Water for a second\nand then [INAUDIBLE]. ",
    "start": "2953610",
    "end": "2985890"
  },
  {
    "text": "All right. Why don't you turn to a\nneighbor and see what you got? [SIDE CONVERSATONS]",
    "start": "2985890",
    "end": "2993580"
  },
  {
    "start": "2993580",
    "end": "3213175"
  },
  {
    "text": "All right, let's\ngo through this. So the first one requires a\ncouple of nested expectations.",
    "start": "3213175",
    "end": "3218180"
  },
  {
    "text": "So let's go through those and\nmake sure I get my math right. So for the first\none, pi 1, so there's",
    "start": "3218180",
    "end": "3225310"
  },
  {
    "text": "two levels of\nstochasticity here. We have a stochastic\npolicy, and we have-- we have rewards with\nstochastic actions--",
    "start": "3225310",
    "end": "3232599"
  },
  {
    "text": "or stochastic rewards. So let's first just figure\nout what the expected reward is for action one.",
    "start": "3232600",
    "end": "3238630"
  },
  {
    "text": "This is equal to\n0.02 with reward 100. So that is 2 plus else\nyou get a reward of 0.",
    "start": "3238630",
    "end": "3246380"
  },
  {
    "text": "So the expected reward\nfor action a1 is two. I'll just write that as here. ",
    "start": "3246380",
    "end": "3254910"
  },
  {
    "text": "Expected reward for action one. We can do the same\ncalculation here. So the expected\nreward for A2 is just",
    "start": "3254910",
    "end": "3263530"
  },
  {
    "text": "going to be equal to 0.55 times\n2, which is just equal to 1.1. And the expected reward for\nA3 is just equal to 0.5.",
    "start": "3263530",
    "end": "3273849"
  },
  {
    "text": "So generally, policies that\nput more weight on action one are going to be better. Now let's see.",
    "start": "3273850",
    "end": "3279970"
  },
  {
    "text": "Look at what the expected value\nis of pi 1-- so what pi 1 is.",
    "start": "3279970",
    "end": "3284980"
  },
  {
    "text": "It's going to say\nwith probability 0.8, we're going to get the\nreward of a3 plus 0.2.",
    "start": "3284980",
    "end": "3293160"
  },
  {
    "text": "We get the reward of a2. The reward of a3 is 0.5. So it's 0.8 times 0.5\nplus 0.2 times 1.1.",
    "start": "3293160",
    "end": "3302730"
  },
  {
    "text": "So that's about how\nmuch that one is. So this is like approximately--",
    "start": "3302730",
    "end": "3310760"
  },
  {
    "text": "yeah, OK. So this one is like\napproximately like 0.42ish.",
    "start": "3310760",
    "end": "3317140"
  },
  {
    "text": "I'll double-check\nthe exact math. It's roughly like that. Now let's do it for-- so this is the reward for pi 1.",
    "start": "3317140",
    "end": "3324440"
  },
  {
    "text": "We'll do the same\nthing for pi 2. This says with 0.5, it gets\nthe reward, the expected reward",
    "start": "3324440",
    "end": "3331900"
  },
  {
    "text": "for r of a2 plus 0.5. It gets the reward of a1.",
    "start": "3331900",
    "end": "3338410"
  },
  {
    "text": "So that's equal to 0.5\ntimes 1.1 plus 0.5 times 2.",
    "start": "3338410",
    "end": "3344530"
  },
  {
    "text": "So it's approximately\nequal to 1.5ish.",
    "start": "3344530",
    "end": "3350615"
  },
  {
    "text": "I think I was off by 2 when\nI was chatting to some people before. Yeah.",
    "start": "3350615",
    "end": "3356420"
  },
  {
    "text": "I got 162 for first one,\n0.65 for the second one.",
    "start": "3356420",
    "end": "3361849"
  },
  {
    "text": "So I do my math wrong. I think this is-- I think it's going to be more\nthan that because they expect your word for this one is 2.",
    "start": "3361850",
    "end": "3367640"
  },
  {
    "text": "And so this one has to be-- Oh, 2-- OK. I thought you said it's 0.2.",
    "start": "3367640",
    "end": "3373730"
  },
  {
    "text": "Yeah. Yeah. So I think this ends\nup being roughly 1.5. I can double-check my math,\nbut I think that's right.",
    "start": "3373730",
    "end": "3380300"
  },
  {
    "text": "So pi 2 does have\ntrue higher reward. So this is true. ",
    "start": "3380300",
    "end": "3386450"
  },
  {
    "text": "The second is we\ncan't use pi 1 to get an unbiased estimate of pi 2. Why is that?",
    "start": "3386450",
    "end": "3393170"
  },
  {
    "text": "So this is true also.  Why can't we use pi\none, data from pi 1?",
    "start": "3393170",
    "end": "3402060"
  },
  {
    "text": "Because it never pulls. It never does action one. That's right. So it never does action one. So it's like saying\nyou have data",
    "start": "3402060",
    "end": "3408513"
  },
  {
    "text": "about all these restaurants. And then you ask\nit, OK, I also have a policy that's now going to\ngo to this new restaurant, and you have no data from that.",
    "start": "3408513",
    "end": "3414790"
  },
  {
    "text": "So we can't get an unbiased\nestimate of the average award. This one's hard.",
    "start": "3414790",
    "end": "3422039"
  },
  {
    "text": "This is false. ",
    "start": "3422040",
    "end": "3427170"
  },
  {
    "text": "It turns out that you can still\nget an unbiased-- you can still",
    "start": "3427170",
    "end": "3434530"
  },
  {
    "text": "get a lower bound on the\nperformance of a policy using another policy\nwhich doesn't have",
    "start": "3434530",
    "end": "3439599"
  },
  {
    "text": "complete overlap if the\nrewards are strictly positive. So if the rewards are always\ngreater than or equal to 0,",
    "start": "3439600",
    "end": "3450380"
  },
  {
    "text": "you can do this. Why is this? So we have a paper on\nthis from a few years",
    "start": "3450380",
    "end": "3457119"
  },
  {
    "text": "ago now, just for\nwhy this happens. Essentially, you can think\nof it as if your behavior",
    "start": "3457120",
    "end": "3463090"
  },
  {
    "text": "policy doesn't include\nsome of the actions that you want to evaluate, it's\nlike putting 0 mass on those.",
    "start": "3463090",
    "end": "3470180"
  },
  {
    "text": "OK, because if you think back\nto what is happening here, it's like you never sample them.",
    "start": "3470180",
    "end": "3478580"
  },
  {
    "text": "So you have zero probability\nmass on some things that you want to evaluate. You want to include a policy\nthat sometimes recommends",
    "start": "3478580",
    "end": "3485599"
  },
  {
    "text": "movies, and you never do. So it's like putting\n0 mass on that. If all your reward is positive,\nthat's essentially just lowering",
    "start": "3485600",
    "end": "3493490"
  },
  {
    "text": "your estimated value. So it turns out that if all\nyour rewards are positive,",
    "start": "3493490",
    "end": "3498660"
  },
  {
    "text": "you can use a\nbehavior policy that doesn't have complete coverage\nwith your target policy,",
    "start": "3498660",
    "end": "3504089"
  },
  {
    "text": "but it will be a lower bound. The reason why that\nmight be useful is because if it's\nstill the case",
    "start": "3504090",
    "end": "3510109"
  },
  {
    "text": "that your target new\nevaluation policy is better than your\nbehavior policy, even though it might\nnot have full coverage,",
    "start": "3510110",
    "end": "3517020"
  },
  {
    "text": "you may still want to use it. So you're like, oh,\nit doesn't matter whether those recommendations\nit makes for those new movies",
    "start": "3517020",
    "end": "3522530"
  },
  {
    "text": "is good or not. It's already a better policy. So we can do that. OK, great.",
    "start": "3522530",
    "end": "3529460"
  },
  {
    "text": "All right. So it turns out\nthat we can also do this for RL policy evaluation. So I just showed you a much\nmore simple setting of this.",
    "start": "3529460",
    "end": "3536200"
  },
  {
    "text": "And I'll highlight too here\nthat importance, sampling like many things in stats\nand math, et cetera,",
    "start": "3536200",
    "end": "3541600"
  },
  {
    "text": "goes by many different names. You'll often see things like\ninverse propensity weighting.",
    "start": "3541600",
    "end": "3550339"
  },
  {
    "text": "So if you take econ\nclasses, people often refer to these things more as\nlike IPR or inverse propensity",
    "start": "3550340",
    "end": "3557450"
  },
  {
    "text": "weighting. What I learned about\nthem, I learned about them as importance\nsampling often also depends whether\nyou're using these",
    "start": "3557450",
    "end": "3563300"
  },
  {
    "text": "to design ways to gather\ndata or whether you have historical data. OK, let's see how we can do\nthis for reinforcement learning.",
    "start": "3563300",
    "end": "3570190"
  },
  {
    "text": "So in reinforcement learning, we\ncan do exactly the same thing. So I have what I want to have.",
    "start": "3570190",
    "end": "3575710"
  },
  {
    "text": "These are now my trajectories. And as we've seen before, we can\nthink of the value of a policy",
    "start": "3575710",
    "end": "3581690"
  },
  {
    "text": "as just being an expectation\nover all the trajectories that could be generated\nby that policy",
    "start": "3581690",
    "end": "3587599"
  },
  {
    "text": "from initial start\nstate times the reward of those trajectories. So this is the reward\nof a trajectory tau.",
    "start": "3587600",
    "end": "3598910"
  },
  {
    "text": "This is the probability of a\ntrajectory under the desired",
    "start": "3598910",
    "end": "3604230"
  },
  {
    "text": "policy. So what we can do in this\ncase is the following. We can just multiply and\ndivide by the same thing,",
    "start": "3604230",
    "end": "3610340"
  },
  {
    "text": "like what we saw before. So we're going to\nimagine that we have data from a different policy.",
    "start": "3610340",
    "end": "3616714"
  },
  {
    "text": " So I'm going to call this pi B.",
    "start": "3616715",
    "end": "3625270"
  },
  {
    "text": "So I've now introduced\nmy behavior policy. OK, so I'm just going to\nrewrite that so I can just",
    "start": "3625270",
    "end": "3639760"
  },
  {
    "text": "have this weight. This is just reweighing what's\nthe probability of me getting",
    "start": "3639760",
    "end": "3645880"
  },
  {
    "text": "a particular trajectory under my\nbehavior policy versus my target policy. ",
    "start": "3645880",
    "end": "3653960"
  },
  {
    "text": "OK, so we have that here. Write it out here first.",
    "start": "3653960",
    "end": "3659160"
  },
  {
    "text": "So now we know. Let me put this from here. We know from before that if we\nhave samples from our behavior",
    "start": "3659160",
    "end": "3668059"
  },
  {
    "text": "policy, we can approximate\nthis expectation by a sampled expectation. And we reweight these.",
    "start": "3668060",
    "end": "3673140"
  },
  {
    "text": "The next thing is to make sure\nthat we can compute what's the probability of a trajectory\nunder our target policy",
    "start": "3673140",
    "end": "3678740"
  },
  {
    "text": "versus our evaluation policy. And we've seen things\nlike this before. So just remember, what\nwe can do in this case is",
    "start": "3678740",
    "end": "3685370"
  },
  {
    "text": "that the probability\nof a trajectory, given a policy in action, is\nequal to the product over equals",
    "start": "3685370",
    "end": "3694910"
  },
  {
    "text": "1 to the length of the\ntrajectory, the probability, the transition probability.",
    "start": "3694910",
    "end": "3703269"
  },
  {
    "text": "I'm just going to write it\nas a deterministic policy for simplicity--\ndeterministic for simplicity.",
    "start": "3703270",
    "end": "3711310"
  },
  {
    "text": "But you can extend all of\nthis times the probability that you would take\nthat action s--",
    "start": "3711310",
    "end": "3719180"
  },
  {
    "text": "oops.  Actually, yeah,\nI'll rewrite that.",
    "start": "3719180",
    "end": "3727430"
  },
  {
    "text": "I don't want it to\nbe deterministic. That will be misleading. I put it here. ",
    "start": "3727430",
    "end": "3744480"
  },
  {
    "text": "OK all right. So this is just the probability\nof us taking the action given",
    "start": "3744480",
    "end": "3750180"
  },
  {
    "text": "the state under our policy\nand the transition probability for every single time step. The nice thing that we\ncan see in this case,",
    "start": "3750180",
    "end": "3757507"
  },
  {
    "text": "we can write that out for\nboth the behavior policy and the target policy. And as we've seen in some\nother cases, this will cancel.",
    "start": "3757507",
    "end": "3766190"
  },
  {
    "text": "So you don't need to\nknow the dynamics model. So this is beautiful\nand incredibly helpful",
    "start": "3766190",
    "end": "3772460"
  },
  {
    "text": "under similar conditions\nto what we just saw as long as you\nhave coverage, which means that you will visit the\nsame sort of trajectories,",
    "start": "3772460",
    "end": "3778440"
  },
  {
    "text": "maybe with differing\nprobabilities. All you have to do\nis reweight them so they look more\nlike the policy",
    "start": "3778440",
    "end": "3784880"
  },
  {
    "text": "that you want to evaluate. And we assume that\nthis because this is just your policy probability.",
    "start": "3784880",
    "end": "3790878"
  },
  {
    "text": "It just says, what action\nwould you take in this state. And so this is known if you're\ndoing policy evaluation.",
    "start": "3790878",
    "end": "3796530"
  },
  {
    "text": "So this first introduced\nfor RL, to my knowledge by Doina Precup, Richard Sutton,\nand Satinder Singh in 2000.",
    "start": "3796530",
    "end": "3803577"
  },
  {
    "text": "Then there's been a lot of\nfollow-up work and leveraging of this. It's super helpful.",
    "start": "3803577",
    "end": "3808580"
  },
  {
    "text": "We don't need the Markov\nassumption or anything. OK, requires\nfundamental assumptions.",
    "start": "3808580",
    "end": "3814589"
  },
  {
    "text": "It's unbiased, and it corrects\nfor distribution mismatch-- so extremely helpful.",
    "start": "3814590",
    "end": "3820090"
  },
  {
    "text": "I won't do this now, but you\nmight want to look through this later just to think about, given\neverything you know about Monte",
    "start": "3820090",
    "end": "3826220"
  },
  {
    "text": "Carlo methods, et\ncetera, like what might be some of the\nlimitations of doing this?",
    "start": "3826220",
    "end": "3832090"
  },
  {
    "text": "I'll just briefly\nsay there's been a whole bunch of extensions. One thing is called per\ndecision importance sampling.",
    "start": "3832090",
    "end": "3837610"
  },
  {
    "text": "Similar to a policy gradient,\nwe think about the fact that in terms of the\nreward and the decisions,",
    "start": "3837610",
    "end": "3844809"
  },
  {
    "text": "later decisions that are made\ncan't affect earlier rewards. So you can reduce\nthe variance by being",
    "start": "3844810",
    "end": "3850090"
  },
  {
    "text": "a little bit more strategic\nin where you put your weights. And we saw similar ideas\nto this in policy gradient.",
    "start": "3850090",
    "end": "3857290"
  },
  {
    "text": "So this is called the per\ndecision importance sampling, and it helps to have better\nproperties in terms of--",
    "start": "3857290",
    "end": "3863860"
  },
  {
    "text": "particularly for long sequences. In general, the\nvariance is pretty high,",
    "start": "3863860",
    "end": "3869599"
  },
  {
    "text": "like for most Monte\nCarlo methods.  One thing to know is there's\nconcentration inequalities,",
    "start": "3869600",
    "end": "3876744"
  },
  {
    "text": "like the Hoeffding\ninequality, that you can use that will generally\nscale with the largest range of the variable if you\nwant to start to get confidence",
    "start": "3876745",
    "end": "3883320"
  },
  {
    "text": "intervals over these values. And this can start to be pretty\nterrible for long horizons",
    "start": "3883320",
    "end": "3889349"
  },
  {
    "text": "for importance sampling. So I'll post afterwards\nwhat the solutions are for both of these check\nyour understandings,",
    "start": "3889350",
    "end": "3894670"
  },
  {
    "text": "but it's pretty informative\nto think about exactly how bad this can become. OK to deal with this, there's\na lot of different extensions.",
    "start": "3894670",
    "end": "3902930"
  },
  {
    "text": "One thing is that you can if\nyou do have Markov structure, you can think about state\ndistributions instead",
    "start": "3902930",
    "end": "3909290"
  },
  {
    "text": "of trajectories, and\nthat can be very helpful. There's been a bunch of\nwork in that direction. One work that we've done\nin others is called--",
    "start": "3909290",
    "end": "3917000"
  },
  {
    "text": "is taking ideas from statistics\non doubly robust estimation and using these to reduce the\nvariance in these methods,",
    "start": "3917000",
    "end": "3924420"
  },
  {
    "text": "as well as trying to blend\nbetween methods that make a Markov assumption\nand methods that don't.",
    "start": "3924420",
    "end": "3931250"
  },
  {
    "text": "All right. I want to finish now by\ntalking a bit about how we can use these ideas\nand others to think",
    "start": "3931250",
    "end": "3937810"
  },
  {
    "text": "about offline policy learning. So I think there's a couple\nimportant ideas we went through so far today.",
    "start": "3937810",
    "end": "3943260"
  },
  {
    "text": "One is that you can just build a\nsimulator from historical data, and you can use that to learn.",
    "start": "3943260",
    "end": "3948440"
  },
  {
    "text": "But it may be\nbiased, and that bias may be substantial\nwhen you're trying to use it to pick policies.",
    "start": "3948440",
    "end": "3955359"
  },
  {
    "text": "We can do model-free\nmethods, but we're going to want to be\ncareful about those. And we're going to see\nmore of that later.",
    "start": "3955360",
    "end": "3961460"
  },
  {
    "text": "And you can use\nimportance sampling to get an unbiased estimate,\nbut it might be high variance. We're now going to think\nabout those sorts of ideas",
    "start": "3961460",
    "end": "3968050"
  },
  {
    "text": "in the context of when we're\nactually trying to pick a policy and do optimization.",
    "start": "3968050",
    "end": "3973260"
  },
  {
    "text": "So I'm going to go back\nto this issue of coverage because I think it's\nimportant to emphasize. So let's imagine that\nyou have antibiotics,",
    "start": "3973260",
    "end": "3980214"
  },
  {
    "text": "mechanical ventilator, and a\nvasopressor, all things that are often used in an\nintensive care unit.",
    "start": "3980215",
    "end": "3986810"
  },
  {
    "text": "And you might have\ndifferent probabilities of these interventions. And let's say you\nwant to evaluate",
    "start": "3986810",
    "end": "3992130"
  },
  {
    "text": "a policy that frequently\ndoes mechanical ventilation. As we've been talking\nabout, your data",
    "start": "3992130",
    "end": "3999500"
  },
  {
    "text": "has to support the policy\nyou want to evaluate. So if this is your\nbehavior policy, that",
    "start": "3999500",
    "end": "4005157"
  },
  {
    "text": "works because every single\naction you want to try, you have a non-zero probability\nof drawing that in the data.",
    "start": "4005157",
    "end": "4012070"
  },
  {
    "text": "If you have this policy\nthat doesn't work, that's the same as the\nexample that we saw before. So if you never use a\nvasopressor in your behavior",
    "start": "4012070",
    "end": "4018910"
  },
  {
    "text": "data, you cannot evaluate\nhow good that would be in the future. Now, when I draw like\nthis, or in the example",
    "start": "4018910",
    "end": "4024850"
  },
  {
    "text": "that I gave in the check\nyour understanding, it's pretty obvious\nbecause there's a finite number of actions,\nand it's pretty clear,",
    "start": "4024850",
    "end": "4030800"
  },
  {
    "text": "if we didn't take action, the\nvasopressor action, that we can't evaluate it. But in real data\nsets, it often gets",
    "start": "4030800",
    "end": "4036460"
  },
  {
    "text": "really hard to understand\nwhat does it mean to have sufficient coverage?",
    "start": "4036460",
    "end": "4041772"
  },
  {
    "text": "So in general, this is going\nto be hard because we're going to want to say, well, is it OK? If it's 0, I\ndefinitely can't do it.",
    "start": "4041772",
    "end": "4048049"
  },
  {
    "text": "But if it was like right\nhere, is that sufficient? If like one in a million\ntimes I use a vasopressor,",
    "start": "4048050",
    "end": "4053570"
  },
  {
    "text": "is that going to be OK? Does it have to be in\nmy actual data set, or does it just have to be there\nwas a chance of me doing this?",
    "start": "4053570",
    "end": "4060070"
  },
  {
    "text": "So all these issues are\nkind of exactly how much data support you need come up.",
    "start": "4060070",
    "end": "4065810"
  },
  {
    "text": "So up to around 2020,\nmost of the methods for doing off-policy\nevaluation, kind of model based",
    "start": "4065810",
    "end": "4072810"
  },
  {
    "text": "or model free, assumed overlap. So if you're doing\noff-policy estimation,",
    "start": "4072810",
    "end": "4078100"
  },
  {
    "text": "it means for your\npolicy of interest. But for off-policy optimization,\nit often assumed all policies.",
    "start": "4078100",
    "end": "4083560"
  },
  {
    "text": "So every single policy you\ncan imagine in your domain had to have coverage with\nyour behavior policy.",
    "start": "4083560",
    "end": "4090000"
  },
  {
    "text": "Now, if your behavior policy\nis random, that's fine. But if your behavior policy\nis, say, how physicians operate",
    "start": "4090000",
    "end": "4096720"
  },
  {
    "text": "or how teachers operate\nor some sort of policy that's not completely\nrandom, that",
    "start": "4096720",
    "end": "4101729"
  },
  {
    "text": "wouldn't always be satisfied. And in general,\nmany, many real data",
    "start": "4101729",
    "end": "4106910"
  },
  {
    "text": "sets don't involve complete\nrandom exploration. And this means if you assume\nthis and use these methods",
    "start": "4106910",
    "end": "4113068"
  },
  {
    "text": "and it's not true,\nthen you might end up sort of going into\nparts of the domain or ending up taking policies\nthat go into parts of the domain",
    "start": "4113069",
    "end": "4120330"
  },
  {
    "text": "where you have very\nlittle coverage.  So I'm going to\nintroduce an idea.",
    "start": "4120330",
    "end": "4127259"
  },
  {
    "text": "And it turns out this idea,\nthere was a number of groups that all started thinking\nabout this at the same time, and I'll cite a few others\nof them in a minute.",
    "start": "4127260",
    "end": "4135020"
  },
  {
    "text": "We call this doing the\nbest with what you've got. So the idea was, how can we\nleverage data sets where we only",
    "start": "4135020",
    "end": "4140479"
  },
  {
    "text": "have partial coverage? Like we still want\nto do as well as we can, but within the\nsupport of the data.",
    "start": "4140479",
    "end": "4147200"
  },
  {
    "text": "And this is similar to\nthe KL constraint or PPO clipping that we've seen\nbefore, but this is all going to be entirely\nin the offline case",
    "start": "4147200",
    "end": "4153816"
  },
  {
    "text": "where we don't manage to\nget any additional data. And the key idea that we're\ngoing to think about here is just being\npessimistic so that when",
    "start": "4153816",
    "end": "4161359"
  },
  {
    "text": "we don't think we have\nsufficient coverage, or we have high uncertainty\nover what the reward might",
    "start": "4161359",
    "end": "4166370"
  },
  {
    "text": "be in a particular\nstate or action, we want to be pessimistic with\nrespect to that uncertainty.",
    "start": "4166370",
    "end": "4172509"
  },
  {
    "text": "I want to highlight that just\neven when our paper came out here, there was\nsort of increasing interest in offline RL.",
    "start": "4172510",
    "end": "4178910"
  },
  {
    "text": "But what we noted is\nthat there was still quite a few\nchallenges, and I just want to illustrate that with\na really simple example.",
    "start": "4178910",
    "end": "4185380"
  },
  {
    "text": "So this is known\nas the chain MDP, and we might talk about it\nmore when we talk about data efficient exploration.",
    "start": "4185380",
    "end": "4191649"
  },
  {
    "text": "This is not exactly the\nsame as all the chain MDPs, but there's a number of them. And they're used to\nillustrate the hardness",
    "start": "4191649",
    "end": "4198130"
  },
  {
    "text": "of learning good policies. So the idea in\nthis setting is you have an initial start\nstate S1 and then--",
    "start": "4198130",
    "end": "4206260"
  },
  {
    "text": "or sorry, S0. And then under one\npolicy, mu, you have a probability of\ngoing to S1, S2, et cetera.",
    "start": "4206260",
    "end": "4216250"
  },
  {
    "text": "And also with\nanother probability, you have a probability\nof transitioning to S10. It's a really, really small\nMDP, just a very small number",
    "start": "4216250",
    "end": "4224080"
  },
  {
    "text": "of states. The important thing to note\nhere is that all of these states",
    "start": "4224080",
    "end": "4229360"
  },
  {
    "text": "have deterministic reward\nexcept for this one. So in reality, this has\nan expected reward of 0.8.",
    "start": "4229360",
    "end": "4240560"
  },
  {
    "text": "You always get 0.8 when\nyou get to that state. And this one has an\nexpected reward of 0.5.",
    "start": "4240560",
    "end": "4246710"
  },
  {
    "text": "So it's a worse state. But if you go there because of\nstochasticity, some of the time,",
    "start": "4246710",
    "end": "4252150"
  },
  {
    "text": "you'll get a 1 there, which\nmeans when you have finite data, you might think that state\ns9 is better than S10.",
    "start": "4252150",
    "end": "4257960"
  },
  {
    "text": "That will just happen\nwith your data. So what we could\nshow in this case",
    "start": "4257960",
    "end": "4263060"
  },
  {
    "text": "is that a bunch of the\nother algorithms for doing conservative batch RL-- and I won't go through all of\nthem, but happy to talk to them",
    "start": "4263060",
    "end": "4269260"
  },
  {
    "text": "offline-- had this weird behavior where as\nyou this is your behavior data set.",
    "start": "4269260",
    "end": "4274960"
  },
  {
    "text": "So as you increase the\namount of your behavior data, you would hope in\ngeneral that you get a better, better\nestimate of a new policy,",
    "start": "4274960",
    "end": "4281240"
  },
  {
    "text": "and you get actually better\nand better performance. This is the success rate. And we found we had this\nweird behavior where",
    "start": "4281240",
    "end": "4288620"
  },
  {
    "text": "for a lot of the\nother algorithms, they would start\noff, and they would learn the optimal policy\nfor this domain, which",
    "start": "4288620",
    "end": "4294440"
  },
  {
    "text": "is to go to S10. But then as you got more data\nfrom your behavior data set,",
    "start": "4294440",
    "end": "4300420"
  },
  {
    "text": "they would get misled because\nsometime you would have seen S9, and it would have\ngiven you a one.",
    "start": "4300420",
    "end": "4305900"
  },
  {
    "text": "And if it saw that, it would say\nno, I don't want to go to S10. I want to go to S9 instead. So with intermediate\namounts of data,",
    "start": "4305900",
    "end": "4312210"
  },
  {
    "text": "these other methods\nwould get confused and learned a bad policy. And it was only as you started\nto get a lot more data that they",
    "start": "4312210",
    "end": "4317630"
  },
  {
    "text": "would end up getting\nback and realizing what the best policy was. And so that was\nsomewhat concerning,",
    "start": "4317630",
    "end": "4323349"
  },
  {
    "text": "because you would\ngenerally hope that you get some sort of\nmonotonic improvement as you get more and more data\nfrom your behavior data set.",
    "start": "4323350",
    "end": "4330990"
  },
  {
    "text": "But here we were seeing that\nsome of the previous methods had this sort of\nunfortunate behavior.",
    "start": "4330990",
    "end": "4337350"
  },
  {
    "text": "And it turned out it\ndidn't just happen in for these particular\nexamples, but we could show some other\ntypes of examples",
    "start": "4337350",
    "end": "4343650"
  },
  {
    "text": "where we got very similar\ntypes of performance challenges for other methods. So the key idea\nis pretty simple,",
    "start": "4343650",
    "end": "4352810"
  },
  {
    "text": "which is just be pessimistic\nif you haven't seen a state action very much. So we defined a filtration\nfunction, which is just",
    "start": "4352810",
    "end": "4360510"
  },
  {
    "text": "a simple threshold\nthat says, let me check for this state and action pair. It's kind of what my density\nis, how much I've seen it.",
    "start": "4360510",
    "end": "4367210"
  },
  {
    "text": "If it's greater\nthan a threshold, this is going to be 1. OK, so what this\nthreshold is doing",
    "start": "4367210",
    "end": "4374067"
  },
  {
    "text": "is trying to account for your\nstatistical uncertainty you have if you have finite\namounts of data. So if you haven't\nseen things very much,",
    "start": "4374067",
    "end": "4380200"
  },
  {
    "text": "this is going to become 0. If you've seen things a\nlot, it's going to be one. That's all we're doing.",
    "start": "4380200",
    "end": "4385980"
  },
  {
    "text": "And then we can just combine\nthis with Bellman backups. So just like for DQN or Belmont\noperator, we can just apply it.",
    "start": "4385980",
    "end": "4392770"
  },
  {
    "text": "So that when we are\nlooking at your reward plus gamma times your expected\ndiscounted sum of rewards,",
    "start": "4392770",
    "end": "4398020"
  },
  {
    "text": "we look at the states\nyou might get into. And if those states we don't\nhave very much data for,",
    "start": "4398020",
    "end": "4403270"
  },
  {
    "text": "then this whole thing becomes 0. And so it's like\nsaying if I transition",
    "start": "4403270",
    "end": "4408810"
  },
  {
    "text": "to a next state for which\nI don't have much data, I just pretend its reward is 0. And then I back up from\nthere, which essentially",
    "start": "4408810",
    "end": "4416340"
  },
  {
    "text": "means I don't want\nto take actions that transition to states for\nwhich I don't have enough data--",
    "start": "4416340",
    "end": "4421970"
  },
  {
    "text": "just pessimistic. And it's going to\nbe a lower bound. If your rewards are\nall bounded by 0,",
    "start": "4421970",
    "end": "4428340"
  },
  {
    "text": "it's just going to be a lower\nbound on your potential reward. So since we assume that our\nrewards are all positive",
    "start": "4428340",
    "end": "4436000"
  },
  {
    "text": "and you can always\njust shift them, this is going to become\na pessimistic estimate for all of those tuples. ",
    "start": "4436000",
    "end": "4443969"
  },
  {
    "text": "And you can do this for either\npolicy evaluation or for-- so you can use this in like\npolicy gradient type approaches",
    "start": "4443970",
    "end": "4450570"
  },
  {
    "text": "or for Q-learning type methods. And it turns out\nthis helps a lot.",
    "start": "4450570",
    "end": "4457090"
  },
  {
    "text": "So let me just-- we call this\nmarginalized behavior supported policy optimization.",
    "start": "4457090",
    "end": "4462730"
  },
  {
    "text": "I'll just highlight what was-- because one of the key\nthings of this paper was the theory that\nwe showed with it.",
    "start": "4462730",
    "end": "4468310"
  },
  {
    "text": "As I said, a lot of\nthe previous methods had to make assumptions over\ncoverage that like your data,",
    "start": "4468310",
    "end": "4474770"
  },
  {
    "text": "covered any possible policy\nthat you might want to evaluate. And under that, you could ensure\nthat the policy that you learn",
    "start": "4474770",
    "end": "4482260"
  },
  {
    "text": "is close to optimal. Ours does not make\nthat guarantee. It only says, let's\nthink about all policies",
    "start": "4482260",
    "end": "4490060"
  },
  {
    "text": "that we could reasonably\nevaluate that have sort of enough coverage. We are guaranteed to find the\nbest policy within that class.",
    "start": "4490060",
    "end": "4497025"
  },
  {
    "text": " I'll skip through\nthis now due to time.",
    "start": "4497025",
    "end": "4502110"
  },
  {
    "text": "But under some assumptions\nthat we can also give these kind of\nfinite sample guarantees",
    "start": "4502110",
    "end": "4507350"
  },
  {
    "text": "similar to what we saw for\nthe fitted Q evaluation. All right.",
    "start": "4507350",
    "end": "4512750"
  },
  {
    "text": "And I'll just highlight that\nthose do include the function approximation. So these aren't for tabular.",
    "start": "4512750",
    "end": "4518760"
  },
  {
    "text": "So this is what's\npretty cool to see. So in this case, this is hopper. This is the behavior policy.",
    "start": "4518760",
    "end": "4524910"
  },
  {
    "text": "So this is the behavior policy\nused to gather the data. What you can see here is if you\nuse DDPG, that actually does",
    "start": "4524910",
    "end": "4532490"
  },
  {
    "text": "worse than the behavior policy. If you use behavior cloning,\nit was a little bit better,",
    "start": "4532490",
    "end": "4537920"
  },
  {
    "text": "about the same. It used a particular Vi. We then compare it to BCQ, which\nI mentioned briefly before,",
    "start": "4537920",
    "end": "4544580"
  },
  {
    "text": "Scott Fujimoto's work, and you\ncan see that and our approach in green both do\nsubstantially better.",
    "start": "4544580",
    "end": "4550670"
  },
  {
    "text": "Again, highly in\nsome of these cases, the data does support you\nlearning a much better policy, and you should do-- you\nshould try to uncover that",
    "start": "4550670",
    "end": "4557840"
  },
  {
    "text": "by using these methods\nthat explicitly think about your uncertainty. ",
    "start": "4557840",
    "end": "4563810"
  },
  {
    "text": "Now, I'll skip this\njust due to time. There's some interesting\ntheoretical reasons why model based\nmight be even better.",
    "start": "4563810",
    "end": "4571250"
  },
  {
    "text": "At the same time,\nthere were three papers that all came out at NeurIPS--\nours was one of them-- the same year with all\nbasically very related ideas.",
    "start": "4571250",
    "end": "4578940"
  },
  {
    "text": "Ours was a model-free\nbased approach and work by some of my\ncolleagues, Chelsea Finn,",
    "start": "4578940",
    "end": "4584180"
  },
  {
    "text": "and [INAUDIBLE] and others,\nlearned a model-based approach where they penalize model\nuncertainty during planning.",
    "start": "4584180",
    "end": "4591900"
  },
  {
    "text": "And they had some very\nnice results in D4RL cases. Ours is a bit more\ntheoretical and model free. Theirs was a little more\nalgorithmic and empirical",
    "start": "4591900",
    "end": "4599700"
  },
  {
    "text": "and also had some really nice-- and was focused on\nmodel-based approaches.",
    "start": "4599700",
    "end": "4605100"
  },
  {
    "text": "I'll just highlight that another\nmethod that came out similarly around the same time was\nconservative Q-learning,",
    "start": "4605100",
    "end": "4611050"
  },
  {
    "text": "and that has also continued\nto be very popular since. So that's another way to think\nabout being conservative.",
    "start": "4611050",
    "end": "4618922"
  },
  {
    "text": "We're almost out\nof time, so I just want to do share how do these\ndifferent approaches compare.",
    "start": "4618922",
    "end": "4625679"
  },
  {
    "text": "Pessimistic\napproaches in general do better than alternatives. All of these have some\nform of pessimism.",
    "start": "4625680",
    "end": "4633150"
  },
  {
    "text": "These are model based. This is the sort of behavior\nconstrained Q-learning,",
    "start": "4633150",
    "end": "4638219"
  },
  {
    "text": "some nice work there from\nSergey Levine's group from Berkeley and CQL,\nwhich is also from Berkeley.",
    "start": "4638220",
    "end": "4645960"
  },
  {
    "text": "The different methods\ntend to do better or worse in different settings. I think that in\ngeneral, the key thing",
    "start": "4645960",
    "end": "4653760"
  },
  {
    "text": "to understand from this\npart is that it really can be beneficial to think\nexplicitly about uncertainty",
    "start": "4653760",
    "end": "4658860"
  },
  {
    "text": "and use that to\npenalize and constrain your function to be in\nthe parts of the domain",
    "start": "4658860",
    "end": "4664289"
  },
  {
    "text": "where you have support. And again, this\nis pretty similar. It should definitely make\nyou think back to PPO",
    "start": "4664290",
    "end": "4674800"
  },
  {
    "text": "instead of having\nconstrained updates.",
    "start": "4674800",
    "end": "4682040"
  },
  {
    "text": "So many of these\ndifferent settings-- we're really trying to think\nexplicitly about coverage",
    "start": "4682040",
    "end": "4687620"
  },
  {
    "text": "and how far we can use\nthe existing data we have. But particularly,\nhere where we assume you don't get any\nadditional data,",
    "start": "4687620",
    "end": "4693690"
  },
  {
    "text": "you're just going to\ndeploy a policy at the end, we want to think about exactly\nhow much support we have.",
    "start": "4693690",
    "end": "4700680"
  },
  {
    "text": "OK all right. I will skip the last\npart because we're going to be out of time. If you're interested,\nI just want",
    "start": "4700680",
    "end": "4706710"
  },
  {
    "text": "to highlight that you can\nextend these ideas to think about there being constraints.",
    "start": "4706710",
    "end": "4712119"
  },
  {
    "text": "So we had a science paper\na few years ago thinking about what if you\nwant to make sure your performance is improving\ncompared to baselines.",
    "start": "4712120",
    "end": "4718590"
  },
  {
    "text": "And in particular, we used like\na diabetes insulin measurement simulator.",
    "start": "4718590",
    "end": "4724082"
  },
  {
    "text": "It's a really cool simulator. It was approved by the FDA\nto replace early-stage animal trials. And you can learn new ways\nto do insulin delivery.",
    "start": "4724082",
    "end": "4732340"
  },
  {
    "text": "And what we wanted to\nillustrate in this case is that by thinking explicitly\nabout your uncertainty",
    "start": "4732340",
    "end": "4737940"
  },
  {
    "text": "over the performance of\nnew decision policies, you could quickly\nlearn a policy that you were confident would be better\nthan the existing policy.",
    "start": "4737940",
    "end": "4745940"
  },
  {
    "text": "So I just highlight\nthat to say that there are lots of cases where you'd\nlike to do this offline policy learning, but do\nso in a way where",
    "start": "4745940",
    "end": "4751447"
  },
  {
    "text": "you have safety\nconstraints or constraints over the performance. All right. Let me just summarize this part.",
    "start": "4751447",
    "end": "4757850"
  },
  {
    "text": "So in terms of things that you\nshould know or be able to do-- excuse me. You should be able to\ndefine and apply importance",
    "start": "4757850",
    "end": "4763580"
  },
  {
    "text": "sampling for off policy,\npolicy evaluation and understand some\nof the limitations of these prior works.",
    "start": "4763580",
    "end": "4769550"
  },
  {
    "text": "You should understand\nwhy offline RL might be able to outperform\nimitation learning. You should know this idea of\npessimism under uncertainty",
    "start": "4769550",
    "end": "4776630"
  },
  {
    "text": "and be able to have some\napplication areas where you might want to be doing\noffline RL or offline policy",
    "start": "4776630",
    "end": "4782540"
  },
  {
    "text": "evaluation. So particularly in high-risk\nsettings, that can be important. What we'll be doing next is\ngoing to start to talk about how",
    "start": "4782540",
    "end": "4789980"
  },
  {
    "text": "if we can gather\nour data, how we should gather our data in order\nto really efficiently learn policies?",
    "start": "4789980",
    "end": "4795270"
  },
  {
    "text": "I'll see you on Wednesday. ",
    "start": "4795270",
    "end": "4802000"
  }
]