[
  {
    "start": "0",
    "end": "12000"
  },
  {
    "text": "This is Lecture 3 of our class and we are going to talk today about node embeddings.",
    "start": "4160",
    "end": "10620"
  },
  {
    "text": "So the way we think of this is the following.",
    "start": "10620",
    "end": "13020"
  },
  {
    "start": "12000",
    "end": "91000"
  },
  {
    "text": "Um, the inter- what we talked",
    "start": "13020",
    "end": "15810"
  },
  {
    "text": "on last week was about traditional machine learning in graphs,",
    "start": "15810",
    "end": "19310"
  },
  {
    "text": "where the idea was that given an input graph,",
    "start": "19310",
    "end": "21555"
  },
  {
    "text": "we are going to extract some node link or graph level",
    "start": "21555",
    "end": "24840"
  },
  {
    "text": "features that basically describe the topological structure,",
    "start": "24840",
    "end": "29040"
  },
  {
    "text": "uh, of the network,",
    "start": "29040",
    "end": "30510"
  },
  {
    "text": "either around the node,",
    "start": "30510",
    "end": "31650"
  },
  {
    "text": "around a particular link or the entire graph.",
    "start": "31650",
    "end": "34260"
  },
  {
    "text": "And then we can take that topological information,",
    "start": "34260",
    "end": "36780"
  },
  {
    "text": "compare, um, er, uh, um,",
    "start": "36780",
    "end": "38684"
  },
  {
    "text": "combine it with the attribute-based information to",
    "start": "38685",
    "end": "43760"
  },
  {
    "text": "then train a classical machine learning model",
    "start": "43760",
    "end": "46039"
  },
  {
    "text": "like a support vector machine or a logistic regression,",
    "start": "46040",
    "end": "48850"
  },
  {
    "text": "uh, to be able to make predictions.",
    "start": "48850",
    "end": "51370"
  },
  {
    "text": "So, um, in this sense, right,",
    "start": "51370",
    "end": "53820"
  },
  {
    "text": "the way we are thinking of this is that we are given an input graph here.",
    "start": "53820",
    "end": "57079"
  },
  {
    "text": "We are then, uh,",
    "start": "57080",
    "end": "58385"
  },
  {
    "text": "creating structure- structured features or structural features,",
    "start": "58385",
    "end": "61579"
  },
  {
    "text": "uh, of this graph so that then we can apply",
    "start": "61580",
    "end": "63920"
  },
  {
    "text": "our learning algorithm and make, uh, prediction.",
    "start": "63920",
    "end": "66490"
  },
  {
    "text": "And generally most of the effort goes here into the feature engineering,",
    "start": "66490",
    "end": "70729"
  },
  {
    "text": "uh, where, you know, we are as,",
    "start": "70730",
    "end": "72605"
  },
  {
    "text": "uh, engineers, humans, scientists,",
    "start": "72605",
    "end": "74510"
  },
  {
    "text": "we are trying to figure out how to best describe,",
    "start": "74510",
    "end": "76835"
  },
  {
    "text": "uh, this particular, um,",
    "start": "76835",
    "end": "78275"
  },
  {
    "text": "network so that, uh,",
    "start": "78275",
    "end": "79685"
  },
  {
    "text": "it would be most useful, uh,",
    "start": "79685",
    "end": "81570"
  },
  {
    "text": "for, uh, downstream prediction task.",
    "start": "81570",
    "end": "83955"
  },
  {
    "text": "Um, and, uh, the question then becomes,",
    "start": "83955",
    "end": "86670"
  },
  {
    "text": "uh, can we do this automatically?",
    "start": "86670",
    "end": "88140"
  },
  {
    "text": "Can we kind of get away from, uh, feature engineer?",
    "start": "88140",
    "end": "91034"
  },
  {
    "start": "91000",
    "end": "160000"
  },
  {
    "text": "So the idea behind graph representation learning is that we wanna",
    "start": "91035",
    "end": "95465"
  },
  {
    "text": "alleviate this need to do manual feature engineering every single time, every time for,",
    "start": "95465",
    "end": "100009"
  },
  {
    "text": "uh, every different task,",
    "start": "100010",
    "end": "101600"
  },
  {
    "text": "and we wanna kind of automatically learn the features,",
    "start": "101600",
    "end": "105409"
  },
  {
    "text": "the structure of the network,",
    "start": "105410",
    "end": "106850"
  },
  {
    "text": "um, in- that we are interested in.",
    "start": "106850",
    "end": "109310"
  },
  {
    "text": "And this is what is called, uh,",
    "start": "109310",
    "end": "110750"
  },
  {
    "text": "representation learning so that no manual, uh,",
    "start": "110750",
    "end": "113510"
  },
  {
    "text": "feature engineering is, uh, necessary, uh, anymore.",
    "start": "113510",
    "end": "117055"
  },
  {
    "text": "So the idea will be to do",
    "start": "117055",
    "end": "119000"
  },
  {
    "text": "efficient task-independent feature learning for machine learning with the graphs.",
    "start": "119000",
    "end": "123800"
  },
  {
    "text": "Um, the idea is that for example,",
    "start": "123800",
    "end": "125930"
  },
  {
    "text": "if we are doing this at the level of individual nodes,",
    "start": "125930",
    "end": "128270"
  },
  {
    "text": "that for every node,",
    "start": "128270",
    "end": "129410"
  },
  {
    "text": "we wanna learn how to map this node in a d-dimensional",
    "start": "129410",
    "end": "133160"
  },
  {
    "text": "space ha- and represent it as a vector of d numbers.",
    "start": "133160",
    "end": "137600"
  },
  {
    "text": "And we will call this vector of d numbers as feature representation,",
    "start": "137600",
    "end": "142190"
  },
  {
    "text": "or we will call it, um, an embeding.",
    "start": "142190",
    "end": "144590"
  },
  {
    "text": "And the goal will be that this, uh, mapping, um,",
    "start": "144590",
    "end": "148385"
  },
  {
    "text": "happens automatically and that this vector",
    "start": "148385",
    "end": "150890"
  },
  {
    "text": "captures the structure of the underlying network that,",
    "start": "150890",
    "end": "154640"
  },
  {
    "text": "uh, we are, uh, interested in,",
    "start": "154640",
    "end": "156815"
  },
  {
    "text": "uh, analyzing or making predictions over.",
    "start": "156815",
    "end": "159470"
  },
  {
    "text": "So why would you wanna do this?",
    "start": "159470",
    "end": "161570"
  },
  {
    "start": "160000",
    "end": "206000"
  },
  {
    "text": "Why create these embeddings?",
    "start": "161570",
    "end": "163115"
  },
  {
    "text": "Right. The task is to map nodes into an- into an embedding space.",
    "start": "163115",
    "end": "167240"
  },
  {
    "text": "Um, and the idea is that similarity, uh,",
    "start": "167240",
    "end": "169970"
  },
  {
    "text": "of the embeddings between nodes indicates their similarity in the network.",
    "start": "169970",
    "end": "175100"
  },
  {
    "text": "Uh, for example, you know,",
    "start": "175100",
    "end": "176640"
  },
  {
    "text": "if bo- nodes that are close to each other in the network,",
    "start": "176640",
    "end": "179585"
  },
  {
    "text": "perhaps they should be embedded close together in the embedding space.",
    "start": "179585",
    "end": "183095"
  },
  {
    "text": "Um, and the goal of this is that kind of en-",
    "start": "183095",
    "end": "185810"
  },
  {
    "text": "automatically encodes the network, uh, structure information.",
    "start": "185810",
    "end": "190075"
  },
  {
    "text": "Um, and then, you know,",
    "start": "190075",
    "end": "191550"
  },
  {
    "text": "it can be used for many kinds of different downstream prediction tasks.",
    "start": "191550",
    "end": "195995"
  },
  {
    "text": "For example, you can do any kind of node classification, link prediction,",
    "start": "195995",
    "end": "199655"
  },
  {
    "text": "graph classification, you can do anomaly detection,",
    "start": "199655",
    "end": "202280"
  },
  {
    "text": "you can do clustering,",
    "start": "202280",
    "end": "203465"
  },
  {
    "text": "a lot of different things.",
    "start": "203465",
    "end": "205364"
  },
  {
    "text": "So to give you an example, uh,",
    "start": "205365",
    "end": "208080"
  },
  {
    "start": "206000",
    "end": "303000"
  },
  {
    "text": "here is- here is a plot from a- a paper that came up with",
    "start": "208080",
    "end": "211070"
  },
  {
    "text": "this idea back in 2014, fe- 2015.",
    "start": "211070",
    "end": "214685"
  },
  {
    "text": "The method is called DeepWalk.",
    "start": "214685",
    "end": "216575"
  },
  {
    "text": "Um, and they take this, uh, small, uh,",
    "start": "216575",
    "end": "219080"
  },
  {
    "text": "small network that you see here,",
    "start": "219080",
    "end": "220895"
  },
  {
    "text": "and then they show how the embedding of nodes would look like in two-dimensions.",
    "start": "220895",
    "end": "224690"
  },
  {
    "text": "And- and here the nodes are,",
    "start": "224690",
    "end": "226145"
  },
  {
    "text": "uh, colored by different colors.",
    "start": "226145",
    "end": "227930"
  },
  {
    "text": "Uh, they have different numbers.",
    "start": "227930",
    "end": "229579"
  },
  {
    "text": "And here in the, um, in this example, uh,",
    "start": "229580",
    "end": "232550"
  },
  {
    "text": "you can also see how, um, uh,",
    "start": "232550",
    "end": "235600"
  },
  {
    "text": "how different nodes get mapped into different parts of the embedding space.",
    "start": "235600",
    "end": "238850"
  },
  {
    "text": "For example, all these light blue nodes end up here,",
    "start": "238850",
    "end": "241835"
  },
  {
    "text": "the violet nodes, uh,",
    "start": "241835",
    "end": "243350"
  },
  {
    "text": "from this part of the network end up here,",
    "start": "243350",
    "end": "245750"
  },
  {
    "text": "you know, the green nodes are here,",
    "start": "245750",
    "end": "247820"
  },
  {
    "text": "the bottom two nodes here,",
    "start": "247820",
    "end": "249230"
  },
  {
    "text": "get kind of set, uh,",
    "start": "249230",
    "end": "250459"
  },
  {
    "text": "uh on a different pa- uh,",
    "start": "250460",
    "end": "252050"
  },
  {
    "text": "in a different place.",
    "start": "252050",
    "end": "253400"
  },
  {
    "text": "And basically what you see is that in some sense,",
    "start": "253400",
    "end": "255709"
  },
  {
    "text": "this visualization of the network and",
    "start": "255710",
    "end": "257690"
  },
  {
    "text": "the underlying embedding correspond to each other quite well in two-dimensional space.",
    "start": "257690",
    "end": "262370"
  },
  {
    "text": "And of course, this is a small network.",
    "start": "262370",
    "end": "263900"
  },
  {
    "text": "It's a small kind of toy- toy network,",
    "start": "263900",
    "end": "266525"
  },
  {
    "text": "but you can get an idea about, uh,",
    "start": "266525",
    "end": "268775"
  },
  {
    "text": "how this would look like in,",
    "start": "268775",
    "end": "270514"
  },
  {
    "text": "uh, uh- in, uh,",
    "start": "270515",
    "end": "271805"
  },
  {
    "text": "more interesting, uh, larger,",
    "start": "271805",
    "end": "273680"
  },
  {
    "text": "uh- in larger dimensions.",
    "start": "273680",
    "end": "275509"
  },
  {
    "text": "So that's basically the,",
    "start": "275510",
    "end": "277010"
  },
  {
    "text": "uh- that's basically the, uh, idea.",
    "start": "277010",
    "end": "279465"
  },
  {
    "text": "So what I wanna now do is to tell you about how do we formulate this as a task, uh,",
    "start": "279465",
    "end": "284590"
  },
  {
    "text": "how do we view it in this, uh,",
    "start": "284590",
    "end": "286300"
  },
  {
    "text": "encoder, decoder, uh, view or a definition?",
    "start": "286300",
    "end": "289735"
  },
  {
    "text": "And then what kind of practical methods, um,",
    "start": "289735",
    "end": "291819"
  },
  {
    "text": "exist there, uh for us to be able, uh, to do this.",
    "start": "291820",
    "end": "295605"
  },
  {
    "text": "So the way we are going to do this, um,",
    "start": "295605",
    "end": "298630"
  },
  {
    "text": "is that we are going to represent, uh,",
    "start": "298630",
    "end": "300580"
  },
  {
    "text": "a graph, as a- as a- with an adjacency matrix.",
    "start": "300580",
    "end": "303960"
  },
  {
    "start": "303000",
    "end": "335000"
  },
  {
    "text": "Um, and we are going to think of this,",
    "start": "303960",
    "end": "306509"
  },
  {
    "text": "um, in terms of its adjacency matrix,",
    "start": "306510",
    "end": "309060"
  },
  {
    "text": "and we are not going to assume any feature, uh, uh,",
    "start": "309060",
    "end": "312490"
  },
  {
    "text": "represe- features or attributes,",
    "start": "312490",
    "end": "314800"
  },
  {
    "text": "uh, on the nodes, uh, of the network.",
    "start": "314800",
    "end": "316659"
  },
  {
    "text": "So we are just going to- to think of this as a- as a- as a set of,",
    "start": "316660",
    "end": "322410"
  },
  {
    "text": "um, as a- as an adjacency matrix that we wanna- that we wanna analyze.",
    "start": "322410",
    "end": "326240"
  },
  {
    "text": "Um, we are going to have a graph, as I showed here,",
    "start": "326240",
    "end": "328625"
  },
  {
    "text": "and the corresponding adjacency matrix A.",
    "start": "328625",
    "end": "330920"
  },
  {
    "text": "And for simplicity, we are going to think of these as undirected graphs.",
    "start": "330920",
    "end": "334895"
  },
  {
    "text": "So the goal is to encode nodes so that similarity in the embedding space- uh,",
    "start": "334895",
    "end": "340104"
  },
  {
    "start": "335000",
    "end": "439000"
  },
  {
    "text": "similarity in the embedding space,",
    "start": "340105",
    "end": "341660"
  },
  {
    "text": "you can think of it as distance or as a dot product,",
    "start": "341660",
    "end": "344840"
  },
  {
    "text": "as an inner product of the coordinates of two nodes",
    "start": "344840",
    "end": "347435"
  },
  {
    "text": "approximates the similarity in the graph space, right?",
    "start": "347435",
    "end": "350389"
  },
  {
    "text": "So the idea will be that in- or in the original network,",
    "start": "350390",
    "end": "353345"
  },
  {
    "text": "I wanna to take the nodes,",
    "start": "353345",
    "end": "354560"
  },
  {
    "text": "I wanna map them into the embedding space.",
    "start": "354560",
    "end": "357290"
  },
  {
    "text": "I'm going to use the letter Z to denote the coordinates,",
    "start": "357290",
    "end": "360960"
  },
  {
    "text": "uh, of that- of that embedding,",
    "start": "360960",
    "end": "362910"
  },
  {
    "text": "uh, of a given node.",
    "start": "362910",
    "end": "364605"
  },
  {
    "text": "Um, and the idea is that, you know,",
    "start": "364605",
    "end": "366575"
  },
  {
    "text": "some notion of similarity here",
    "start": "366575",
    "end": "368345"
  },
  {
    "text": "corresponds to some notion of similarity in the embedding space.",
    "start": "368345",
    "end": "371240"
  },
  {
    "text": "And the goal is to learn this encoder that encodes",
    "start": "371240",
    "end": "374060"
  },
  {
    "text": "the original network as a set of, uh, node embeddings.",
    "start": "374060",
    "end": "377905"
  },
  {
    "text": "So the goal is to- to define the similarity in the original network, um,",
    "start": "377905",
    "end": "383780"
  },
  {
    "text": "and to map nodes into the coordinates in the embedding space such that, uh,",
    "start": "383780",
    "end": "387815"
  },
  {
    "text": "similarity of their embeddings corresponds to the similarity in the network.",
    "start": "387815",
    "end": "392120"
  },
  {
    "text": "Uh, and as a similarity metric in the embedding space, uh,",
    "start": "392120",
    "end": "395360"
  },
  {
    "text": "people usually, uh, select, um, dot product.",
    "start": "395360",
    "end": "399300"
  },
  {
    "text": "And dot product is simply the angle,",
    "start": "399300",
    "end": "401849"
  },
  {
    "text": "uh, between the two vectors, right?",
    "start": "401850",
    "end": "403670"
  },
  {
    "text": "So when you do the dot product,",
    "start": "403670",
    "end": "404870"
  },
  {
    "text": "it's the cosine of the- of the angle.",
    "start": "404870",
    "end": "406820"
  },
  {
    "text": "So if the two points are close together or in the same direction from the origin,",
    "start": "406820",
    "end": "411575"
  },
  {
    "text": "they have, um, um,",
    "start": "411575",
    "end": "413120"
  },
  {
    "text": "high, uh, uh, dot product.",
    "start": "413120",
    "end": "414979"
  },
  {
    "text": "And if they are orthogonal,",
    "start": "414980",
    "end": "416420"
  },
  {
    "text": "so there is kind of a 90-degree angle, uh,",
    "start": "416420",
    "end": "418550"
  },
  {
    "text": "then- then they are as- as dissimilar as",
    "start": "418550",
    "end": "420740"
  },
  {
    "text": "possible because the dot product will be, uh, zero.",
    "start": "420740",
    "end": "423919"
  },
  {
    "text": "So that's the idea. So now what do we need to",
    "start": "423920",
    "end": "426560"
  },
  {
    "text": "define is we need to define this notion of, uh,",
    "start": "426560",
    "end": "428810"
  },
  {
    "text": "ori- similarity in the original network and we need to define then",
    "start": "428810",
    "end": "432320"
  },
  {
    "text": "an objective function that will connect the similarity with the, uh, embeddings.",
    "start": "432320",
    "end": "436850"
  },
  {
    "text": "And this is really what we are going to do ,uh, in this lecture.",
    "start": "436850",
    "end": "439985"
  },
  {
    "start": "439000",
    "end": "473000"
  },
  {
    "text": "So, uh, to summarize a bit, right?",
    "start": "439985",
    "end": "442219"
  },
  {
    "text": "Encoder maps nodes, uh, to embeddings.",
    "start": "442220",
    "end": "445070"
  },
  {
    "text": "We need to define a node similarity function,",
    "start": "445070",
    "end": "447680"
  },
  {
    "text": "a measure of similarity in the original network.",
    "start": "447680",
    "end": "450815"
  },
  {
    "text": "And then the decoder, right,",
    "start": "450815",
    "end": "453110"
  },
  {
    "text": "maps from the embeddings to the similarity score.",
    "start": "453110",
    "end": "456305"
  },
  {
    "text": "Uh, and then we can optimize the parameters such that",
    "start": "456305",
    "end": "459949"
  },
  {
    "text": "the decoded similarity corresponds as closely as",
    "start": "459950",
    "end": "463760"
  },
  {
    "text": "possible to the underlying definition of the network similarity.",
    "start": "463760",
    "end": "467720"
  },
  {
    "text": "Where here we're using a very simple decoder,",
    "start": "467720",
    "end": "470300"
  },
  {
    "text": "as I said, just the dot-product.",
    "start": "470300",
    "end": "472520"
  },
  {
    "text": "So, uh, encoder will map notes into low-dimensional vectors.",
    "start": "472520",
    "end": "477995"
  },
  {
    "start": "473000",
    "end": "527000"
  },
  {
    "text": "So encoder of a given node will simply be the coordinates or the embedding of that node.",
    "start": "477995",
    "end": "483470"
  },
  {
    "text": "Um, we talked about how we are going to define the similarity",
    "start": "483470",
    "end": "486439"
  },
  {
    "text": "in the embedding space in terms of the decoder,",
    "start": "486439",
    "end": "489260"
  },
  {
    "text": "in terms of the dot product.",
    "start": "489260",
    "end": "491360"
  },
  {
    "text": "Um, and as I said, uh,",
    "start": "491360",
    "end": "493280"
  },
  {
    "text": "the embeddings will be in some d-dimensional space.",
    "start": "493280",
    "end": "496925"
  },
  {
    "text": "You can think of d, you know, between,",
    "start": "496925",
    "end": "499580"
  },
  {
    "text": "let's say 64 up to about 1,000,",
    "start": "499580",
    "end": "502460"
  },
  {
    "text": "this is usually how- how,",
    "start": "502460",
    "end": "504694"
  },
  {
    "text": "uh, how many dimensions people, uh, choose,",
    "start": "504695",
    "end": "506870"
  },
  {
    "text": "but of course, it depends a bit on the size of the network,",
    "start": "506870",
    "end": "509750"
  },
  {
    "text": "uh, and other factors as well.",
    "start": "509750",
    "end": "511715"
  },
  {
    "text": "Um, and then as I said,",
    "start": "511715",
    "end": "513349"
  },
  {
    "text": "the similarity function specifies how the relationship in",
    "start": "513350",
    "end": "516469"
  },
  {
    "text": "the- in the vector space map to the relationship in the,",
    "start": "516470",
    "end": "520115"
  },
  {
    "text": "uh, original ,uh, in the original network.",
    "start": "520115",
    "end": "522979"
  },
  {
    "text": "And this is what I'm trying to ,uh,",
    "start": "522980",
    "end": "524779"
  },
  {
    "text": "show an example of, uh, here.",
    "start": "524780",
    "end": "527210"
  },
  {
    "start": "527000",
    "end": "687000"
  },
  {
    "text": "So the simplest encoding approach is that an encoder is just an embedding-lookup.",
    "start": "527210",
    "end": "533000"
  },
  {
    "text": "So what- what do I mean by this that- is that an encoded- an",
    "start": "533000",
    "end": "536270"
  },
  {
    "text": "encoding of a given node is simply a vector of numbers.",
    "start": "536270",
    "end": "539780"
  },
  {
    "text": "And this is just a lookup in some big matrix.",
    "start": "539780",
    "end": "542645"
  },
  {
    "text": "So what I mean by this is that our goal will be to learn this matrix Z,",
    "start": "542645",
    "end": "546995"
  },
  {
    "text": "whose dimensionalities is d,",
    "start": "546995",
    "end": "548810"
  },
  {
    "text": "the embedding dimension times the number of nodes,",
    "start": "548810",
    "end": "551390"
  },
  {
    "text": "uh, in the network.",
    "start": "551390",
    "end": "552590"
  },
  {
    "text": "So this means that for every node we will have a column",
    "start": "552590",
    "end": "555680"
  },
  {
    "text": "that is reserved to store the embedding for that node.",
    "start": "555680",
    "end": "559190"
  },
  {
    "text": "And this is what we are going to learn,",
    "start": "559190",
    "end": "561350"
  },
  {
    "text": "this is what we are going to estimate.",
    "start": "561350",
    "end": "563180"
  },
  {
    "text": "And then in this kind of notation,",
    "start": "563180",
    "end": "564860"
  },
  {
    "text": "you can think of v simply as an indicator vector that has all zeros,",
    "start": "564860",
    "end": "569510"
  },
  {
    "text": "except the value of one in the column",
    "start": "569510",
    "end": "571865"
  },
  {
    "text": "indicating the- the ID of that node v. And- and what this",
    "start": "571865",
    "end": "575899"
  },
  {
    "text": "will do pictorially is that basically you can think of",
    "start": "575900",
    "end": "578630"
  },
  {
    "text": "Z as this matrix that has one column per node,",
    "start": "578630",
    "end": "582440"
  },
  {
    "text": "um, and the column store- a given column stores the embedding of that given node.",
    "start": "582440",
    "end": "587690"
  },
  {
    "text": "So the size of this matrix will be number of nodes times the embedding dimension.",
    "start": "587690",
    "end": "592295"
  },
  {
    "text": "And people now who are, for example,",
    "start": "592295",
    "end": "594110"
  },
  {
    "text": "thinking about large graphs may already have a question.",
    "start": "594110",
    "end": "598010"
  },
  {
    "text": "You know, won't these to be a lot of parameters to estimate?",
    "start": "598010",
    "end": "600725"
  },
  {
    "text": "Because the number of parameters in this model is basically the number of entries, uh,",
    "start": "600725",
    "end": "605524"
  },
  {
    "text": "of this matrix, and this matrix gets very large because",
    "start": "605525",
    "end": "609080"
  },
  {
    "text": "it dep- the size of the matrix depends on the number of nodes in the network.",
    "start": "609080",
    "end": "612560"
  },
  {
    "text": "So if you want to do a network or one billion nodes,",
    "start": "612560",
    "end": "616175"
  },
  {
    "text": "then the dimensionality of this matrix would be one billion times,",
    "start": "616175",
    "end": "619880"
  },
  {
    "text": "let's say thousandth, uh,",
    "start": "619880",
    "end": "620990"
  },
  {
    "text": "and embedding dimension and that's- that's,",
    "start": "620990",
    "end": "623135"
  },
  {
    "text": "uh, that's a lot of parameters.",
    "start": "623135",
    "end": "624860"
  },
  {
    "text": "So these methods won't necessarily be most scalable, you can scale them,",
    "start": "624860",
    "end": "629420"
  },
  {
    "text": "let's say up to millions or a million nodes or, uh,",
    "start": "629420",
    "end": "633035"
  },
  {
    "text": "something like that if you- if you really try,",
    "start": "633035",
    "end": "635105"
  },
  {
    "text": "but they will be slow because for every node",
    "start": "635105",
    "end": "637760"
  },
  {
    "text": "we essentially have to estimate the parameters.",
    "start": "637760",
    "end": "640655"
  },
  {
    "text": "Basically for every node we have to estimate its embedding- embedding vector,",
    "start": "640655",
    "end": "644990"
  },
  {
    "text": "which is described by the d-numbers d-parameters,",
    "start": "644990",
    "end": "648649"
  },
  {
    "text": "d-coordinates that we have to estimate.",
    "start": "648650",
    "end": "650990"
  },
  {
    "text": "So, um, but this means that once we have estimated this embeddings,",
    "start": "650990",
    "end": "655100"
  },
  {
    "text": "getting them is very easy.",
    "start": "655100",
    "end": "656300"
  },
  {
    "text": "Is just, uh, lookup in this matrix where everything is stored.",
    "start": "656300",
    "end": "660410"
  },
  {
    "text": "So this means, as I said,",
    "start": "660410",
    "end": "662870"
  },
  {
    "text": "each node is assigned a unique embedding vector.",
    "start": "662870",
    "end": "665555"
  },
  {
    "text": "And the goal of our methods will be to directly optimize or ,uh,",
    "start": "665555",
    "end": "670820"
  },
  {
    "text": "learn the embedding of each node separately in some sense.",
    "start": "670820",
    "end": "675305"
  },
  {
    "text": "Um, and this means that, uh,",
    "start": "675305",
    "end": "677045"
  },
  {
    "text": "there are many methods that will allow us to do this.",
    "start": "677045",
    "end": "679700"
  },
  {
    "text": "In particular, we are going to look at two methods.",
    "start": "679700",
    "end": "682130"
  },
  {
    "text": "One is called, uh,",
    "start": "682130",
    "end": "683270"
  },
  {
    "text": "DeepWalk and the other one is called node2vec.",
    "start": "683270",
    "end": "687105"
  },
  {
    "start": "687000",
    "end": "746000"
  },
  {
    "text": "So let me- let me summarize.",
    "start": "687105",
    "end": "689904"
  },
  {
    "text": "In this view we talked about an encoder ,uh, decoder, uh,",
    "start": "689905",
    "end": "694060"
  },
  {
    "text": "framework where we have what we call a shallow encoder because it's",
    "start": "694060",
    "end": "697600"
  },
  {
    "text": "just an embedding-lookup the parameters to optimize ,um, are- are,",
    "start": "697600",
    "end": "702110"
  },
  {
    "text": "uh, very simple, it is just this embedding matrix Z. Um,",
    "start": "702110",
    "end": "706714"
  },
  {
    "text": "and for every node we want to identify the embedding z_u.",
    "start": "706715",
    "end": "711375"
  },
  {
    "text": "And v are going to cover in the future lectures is we are",
    "start": "711375",
    "end": "715460"
  },
  {
    "text": "going to cover deep encoders like graph neural networks that- that,",
    "start": "715460",
    "end": "719360"
  },
  {
    "text": "uh, are a very different approach to computing, uh, node embeddings.",
    "start": "719360",
    "end": "724370"
  },
  {
    "text": "In terms of a decoder,",
    "start": "724370",
    "end": "726080"
  },
  {
    "text": "decoder for us would be something very similar- simple.",
    "start": "726080",
    "end": "728765"
  },
  {
    "text": "It'd be simple- simply based on the node similarity based on the dot product.",
    "start": "728765",
    "end": "734510"
  },
  {
    "text": "And our objective function that we are going to try to",
    "start": "734510",
    "end": "737000"
  },
  {
    "text": "learn is to maximize the dot product",
    "start": "737000",
    "end": "739955"
  },
  {
    "text": "of node pairs that are similar according to our node similarity function.",
    "start": "739955",
    "end": "746765"
  },
  {
    "start": "746000",
    "end": "804000"
  },
  {
    "text": "So then the question is,",
    "start": "746765",
    "end": "749180"
  },
  {
    "text": "how do we define the similarity, right?",
    "start": "749180",
    "end": "751100"
  },
  {
    "text": "I've been talking about it,",
    "start": "751100",
    "end": "752285"
  },
  {
    "text": "but I've never really defined it.",
    "start": "752285",
    "end": "754310"
  },
  {
    "text": "And really this is how these methods are going to differ between each other,",
    "start": "754310",
    "end": "758165"
  },
  {
    "text": "is how do they define the node similarity notion?",
    "start": "758165",
    "end": "761810"
  },
  {
    "text": "Um, and you could ask a lot of different ways how- how to do this, right?",
    "start": "761810",
    "end": "765950"
  },
  {
    "text": "You could chay- say,",
    "start": "765950",
    "end": "766970"
  },
  {
    "text": "\"Should two nodes have similar embedding if they are perhaps linked by an edge?\"",
    "start": "766970",
    "end": "771545"
  },
  {
    "text": "Perhaps they share many neighbors in common,",
    "start": "771545",
    "end": "774185"
  },
  {
    "text": "perhaps they have something else in common or they are in similar part of",
    "start": "774185",
    "end": "778220"
  },
  {
    "text": "the network or the structure of the network around them, uh, look similar.",
    "start": "778220",
    "end": "782524"
  },
  {
    "text": "And the idea that allow- that- that started all this area of",
    "start": "782525",
    "end": "787505"
  },
  {
    "text": "learning node embeddings was that we are going to def- define a similarity,",
    "start": "787505",
    "end": "793055"
  },
  {
    "text": "um, of nodes based on random walks.",
    "start": "793055",
    "end": "795860"
  },
  {
    "text": "And we are going to ,uh,",
    "start": "795860",
    "end": "797899"
  },
  {
    "text": "optimize node embedding for this random-walk similarity measure.",
    "start": "797900",
    "end": "802445"
  },
  {
    "text": "So, uh, let me explain what- what I mean by that.",
    "start": "802445",
    "end": "806255"
  },
  {
    "start": "804000",
    "end": "884000"
  },
  {
    "text": "So, uh, it is important to know that this method",
    "start": "806255",
    "end": "810755"
  },
  {
    "text": "is what is called unsupervised or self-supervised,",
    "start": "810755",
    "end": "814850"
  },
  {
    "text": "in a way that when we are learning the node embeddings,",
    "start": "814850",
    "end": "817745"
  },
  {
    "text": "we are not utilizing any node labels.",
    "start": "817745",
    "end": "820714"
  },
  {
    "text": "Um, will only be basically trying to",
    "start": "820715",
    "end": "823550"
  },
  {
    "text": "learn embedding so that they capture some notion of network similarity,",
    "start": "823550",
    "end": "827404"
  },
  {
    "text": "but they don't need to capture the- the notion of labels of the nodes.",
    "start": "827405",
    "end": "831500"
  },
  {
    "text": "Uh, and we are also not- not utilizing any node features",
    "start": "831500",
    "end": "835100"
  },
  {
    "text": "or node attributes in a sense that if nodes are humans,",
    "start": "835100",
    "end": "838490"
  },
  {
    "text": "perhaps, you know, their interest,",
    "start": "838490",
    "end": "840380"
  },
  {
    "text": "location, gender, age would be attached to the node.",
    "start": "840380",
    "end": "843860"
  },
  {
    "text": "So we are not using any data,",
    "start": "843860",
    "end": "845779"
  },
  {
    "text": "any information attached to every node or attached to every link.",
    "start": "845780",
    "end": "850130"
  },
  {
    "text": "And the goal here is to directly estimate a set of coordinates of node so",
    "start": "850130",
    "end": "854840"
  },
  {
    "text": "that some aspect of the network structure is preserved.",
    "start": "854840",
    "end": "860300"
  },
  {
    "text": "And in- in this sense,",
    "start": "860300",
    "end": "862220"
  },
  {
    "text": "these embeddings will be task-independent because they are",
    "start": "862220",
    "end": "865279"
  },
  {
    "text": "not trained on a given prediction task, um,",
    "start": "865280",
    "end": "868400"
  },
  {
    "text": "or a given specific, you know,",
    "start": "868400",
    "end": "870410"
  },
  {
    "text": "labelings of the nodes or are given specific subset of links,",
    "start": "870410",
    "end": "874384"
  },
  {
    "text": "it is trained just given the network itself.",
    "start": "874385",
    "end": "879270"
  }
]