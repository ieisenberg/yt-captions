[
  {
    "start": "0",
    "end": "5760"
  },
  {
    "text": "Hey, guys. Welcome to our last\nlecture of this quarter.",
    "start": "5760",
    "end": "11030"
  },
  {
    "text": "And we're very happy\nto have Douwe here. He's the CEO of Contextual AI,\nthe enterprise LLM company,",
    "start": "11030",
    "end": "20450"
  },
  {
    "text": "as well as an adjunct\nprofessor in symbolic systems here at Stanford. And previously, he was the head\nof research at Hugging Face;",
    "start": "20450",
    "end": "27860"
  },
  {
    "text": "and before that, a research\nscientist at Facebook AI Research. He received his PhD and master's\nfrom the University of Cambridge",
    "start": "27860",
    "end": "35977"
  },
  {
    "text": "as well as a master's in\nlogic from the University of Amsterdam and studied\nphilosophy and cognitive",
    "start": "35977",
    "end": "41090"
  },
  {
    "text": "AI in undergrad. And his work focuses on machine\nlearning, as well as NLP,",
    "start": "41090",
    "end": "46430"
  },
  {
    "text": "specifically on developing\nbetter models for language understanding and generation,\nand better tools for evaluation",
    "start": "46430",
    "end": "53239"
  },
  {
    "text": "and management. OK, I give it up for Douwe. All right.",
    "start": "53240",
    "end": "59250"
  },
  {
    "text": "Thank you. So I guess I have to\nstand here in the corner so people can see me\non the Zoom as well.",
    "start": "59250",
    "end": "66060"
  },
  {
    "text": "Yeah, thanks so much\nfor having me here. So I asked Stephen what\nI should talk about.",
    "start": "66060",
    "end": "72270"
  },
  {
    "text": "There were a couple of things I\ncould talk about-- multimodality or evaluation. And this was the\npreferred topic, I guess,",
    "start": "72270",
    "end": "80010"
  },
  {
    "text": "because the others\nwere already covered. So yeah, I'm very happy to\ntalk to you about everything",
    "start": "80010",
    "end": "86040"
  },
  {
    "text": "retrieval augmentation. I think this is really\none of the coolest topics right now in our field.",
    "start": "86040",
    "end": "92820"
  },
  {
    "text": "So I'll just give\nyou an overview of what's been\nhappening and what I think are the interesting\nquestions to think about.",
    "start": "92820",
    "end": "100600"
  },
  {
    "text": "So first of all, obviously,\nin case you've missed it, we are in the age\nof language models.",
    "start": "100600",
    "end": "106200"
  },
  {
    "text": "And I just wanted to\ndo a quick poll here in this not super big audience. I guess there's more\npeople on the Zoom.",
    "start": "106200",
    "end": "112470"
  },
  {
    "text": "But who invented\nlanguage models? If you thought OpenAI,\nthen I'm angry with you.",
    "start": "112470",
    "end": "120740"
  },
  {
    "text": "So actually, this is\na very, very old idea. So the idea is just\nyou take a sequence,",
    "start": "120740",
    "end": "126320"
  },
  {
    "text": "and you vectorize out\nthe token probabilities. And so it wasn't\ninvented by OpenAI.",
    "start": "126320",
    "end": "132830"
  },
  {
    "text": "It's not like a few years old. It's actually\nseveral decades' old. So I'm bringing this up because\nI was talking to someone,",
    "start": "132830",
    "end": "139910"
  },
  {
    "text": "and they were like OpenAI\ninvented language models. And I was like, you're\nkidding me, right?",
    "start": "139910",
    "end": "145220"
  },
  {
    "text": "So I went back to\nthe literature. And this is the oldest one\nI could find, actually.",
    "start": "145220",
    "end": "150260"
  },
  {
    "text": "1991, first neural\nlanguage model. There's a very nice paper\nfrom 2003 from Bengio,",
    "start": "150260",
    "end": "157430"
  },
  {
    "text": "where they actually have word\nembeddings and everything already in there. So, obviously, these\nare LMs, not LLMs.",
    "start": "157430",
    "end": "165530"
  },
  {
    "text": "And as it turns out, if\nyou make them really big and you parameterize them with\nthese massive neural nets,",
    "start": "165530",
    "end": "171417"
  },
  {
    "text": "then you get something\nreally powerful that really shows\nemergent properties. And that's why we're all\nso excited in this stuff.",
    "start": "171417",
    "end": "179430"
  },
  {
    "text": "So if we think about this\nfrom a classic CS perspective, there's input/output. There's this thing\nin the middle.",
    "start": "179430",
    "end": "186090"
  },
  {
    "text": "It's the generator. So we take a sequence,\nthe input sequence. And then the task of the model\nis to predict the next token.",
    "start": "186090",
    "end": "194189"
  },
  {
    "text": "Very, very simple model. And so that's why it was so easy\nto come up with this in 1991",
    "start": "194190",
    "end": "200970"
  },
  {
    "text": "already, because the\nidea is very intuitive. But for a long time, what\nwas really broken with",
    "start": "200970",
    "end": "206760"
  },
  {
    "text": "this was the user interface. And this, I think,\na lot of people",
    "start": "206760",
    "end": "212160"
  },
  {
    "text": "misunderstand what\nChatGPT was about. That's really what\nChatGPT fixed. So initially, you had to come\nup with these very weird prompts",
    "start": "212160",
    "end": "220980"
  },
  {
    "text": "in order to get\nyour language model to do what you wanted it to do. And humans are terrible at this.",
    "start": "220980",
    "end": "226740"
  },
  {
    "text": "So we're much better at telling\npeople or things around us what we want. So if we have a dog, we say sit.",
    "start": "226740",
    "end": "232799"
  },
  {
    "text": "We don't prompt it in a very\nweird way so that it sits. And it's the same with\nthe language model.",
    "start": "232800",
    "end": "238380"
  },
  {
    "text": "If you wanted to\ngenerate some rap lyrics in the style of a pirate\nor Shakespeare or something,",
    "start": "238380",
    "end": "244200"
  },
  {
    "text": "then you tell it,\ngenerate some rap lyrics in the style of pirates. So that kind of\ninstruction data actually",
    "start": "244200",
    "end": "250530"
  },
  {
    "text": "turns out to be super,\nsuper rare in just web data. So what you need to do is you\nneed to fix the user interface",
    "start": "250530",
    "end": "256890"
  },
  {
    "text": "to the language model. And the classic recipe for doing\nthat is the sequence basically",
    "start": "256890",
    "end": "262109"
  },
  {
    "text": "the ChatGPT used. So you prompt the model\nin a specific way. You instruction\nfine tune the model, and you do some alignment RLHF,\nwhatever you do on top of that.",
    "start": "262110",
    "end": "271657"
  },
  {
    "text": "So that's the first thing. So now you have a\nworking language model with a working user interface.",
    "start": "271657",
    "end": "277509"
  },
  {
    "text": "So are we done then? Obviously, we're not. So right now, language models\nare taking the world by storm.",
    "start": "277510",
    "end": "285010"
  },
  {
    "text": "But if you talk to anyone,\nespecially in an enterprise, for example, where they\nhave very strict accuracy",
    "start": "285010",
    "end": "290950"
  },
  {
    "text": "requirements, they will tell\nyou that they can't really productionize this yet. And the reason is because there\nare all these familiar problems.",
    "start": "290950",
    "end": "297880"
  },
  {
    "text": "Probably a bunch of you are\nworking on these problems right now around hallucination.",
    "start": "297880",
    "end": "303190"
  },
  {
    "text": "So these models,\nthey make up stuff very often with very\nhigh confidence, which is even more scary in a way.",
    "start": "303190",
    "end": "310569"
  },
  {
    "text": "Attribution-- so we don't\nreally know why these models are saying what they're saying. Stillness-- they go out of date.",
    "start": "310570",
    "end": "316509"
  },
  {
    "text": "So this was a big\nproblem with ChatGPT not knowing anything that\nhappened after a certain cutoff date.",
    "start": "316510",
    "end": "321880"
  },
  {
    "text": "And they keep updating\nit every once in a while. But you want to\nhave a system that's always completely up to\ndate, that never goes stale.",
    "start": "321880",
    "end": "328840"
  },
  {
    "text": "You want to be able to revise\nthe information in the system. So if you're a\nEuropean organization,",
    "start": "328840",
    "end": "334639"
  },
  {
    "text": "you have to worry\nabout GDPR, which means that you need to be\nable to remove information from the language model or\nmaybe revise facts, which",
    "start": "334640",
    "end": "342410"
  },
  {
    "text": "we don't really know how to do. So again, this is a\nvery interesting area of study for a lot of\nfolks model editing.",
    "start": "342410",
    "end": "350240"
  },
  {
    "text": "So this is something that we\nreally want to be able to fix. And then there's\nthis big question of,",
    "start": "350240",
    "end": "355250"
  },
  {
    "text": "how do you customize\nthese models? So different people have\ndifferent use cases. You have different data.",
    "start": "355250",
    "end": "360860"
  },
  {
    "text": "If you're a company\nor if you want to have a language\nmodel on your own data, how do you make it\nwork on your own data?",
    "start": "360860",
    "end": "366500"
  },
  {
    "text": "So one of the solutions\nthat everybody has started using right\nnow is to couple it",
    "start": "366500",
    "end": "372140"
  },
  {
    "text": "to an external memory. So that's really just RAG. So we can-- this whole lecture\nis basically about RAG.",
    "start": "372140",
    "end": "379550"
  },
  {
    "text": "But the way to understand\nwhat is going on here is we have a generator\njust like before.",
    "start": "379550",
    "end": "385740"
  },
  {
    "text": "We have the input and a\nprompt just like before. But now, instead of just\ngiving those two things,",
    "start": "385740",
    "end": "390860"
  },
  {
    "text": "we give this additional context. So we contextualize the language\nmodel using things we retrieved.",
    "start": "390860",
    "end": "397220"
  },
  {
    "text": "And the retriever is\nvery often pretty simple. It's just a query and\na document encoder.",
    "start": "397220",
    "end": "403190"
  },
  {
    "text": "And then you get a\nbunch of documents. You give them as context\nthrough the model. So super simple architecture.",
    "start": "403190",
    "end": "410870"
  },
  {
    "text": "And I think it's useful\nto think about it from the perspective of\nthese two separate paradigms.",
    "start": "410870",
    "end": "417680"
  },
  {
    "text": "So if you've ever\ntaken an exam-- I'm sure you have-- you can have a closed\nbook exam, where you have to memorize\nall of this or you",
    "start": "417680",
    "end": "424285"
  },
  {
    "text": "have to cram all the knowledge\ninto your parameters, your neurons. Or you have an open\nbook exam, where",
    "start": "424285",
    "end": "429980"
  },
  {
    "text": "you have all of this\ninformation in the book that you can access\nwhen you do the exam. So it's a very similar\nthing with RAG.",
    "start": "429980",
    "end": "436732"
  },
  {
    "text": "You can just make it an open\nbook setting where you give it access to this external\ninformation Wikipedia",
    "start": "436732",
    "end": "442010"
  },
  {
    "text": "or something else or\nbasically the entire internet and then have the\nlanguage model do",
    "start": "442010",
    "end": "447110"
  },
  {
    "text": "its job without having\nto memorize all of it in its parameters. So the other, I think,\nuseful distinction",
    "start": "447110",
    "end": "453560"
  },
  {
    "text": "here is that cramming everything\ninto your parameters, that's the parametric approach.",
    "start": "453560",
    "end": "458660"
  },
  {
    "text": "So what we're doing\nwith RAG is we're adding this non-parametric\nretrieval component.",
    "start": "458660",
    "end": "464090"
  },
  {
    "text": "So you might call\nthis semi-parametric if you want to give this a name.",
    "start": "464090",
    "end": "470509"
  },
  {
    "text": "All right, so why does that\nactually solve these issues? And so the answer is\nbasically that if you",
    "start": "470510",
    "end": "477380"
  },
  {
    "text": "have this separate index,\nthis separate retriever, you can swap it in. You can swap it out. You can replace it\nwith a new index",
    "start": "477380",
    "end": "484460"
  },
  {
    "text": "so you can really customize it. And so you can customize\nyour language model system for what the user\nreally wants to see.",
    "start": "484460",
    "end": "492461"
  },
  {
    "text": "And then, obviously, you\ncan update this index. So it doesn't really\ngo stale, and you can revise it if everything goes\nwrong-- if anything goes wrong.",
    "start": "492462",
    "end": "501270"
  },
  {
    "text": "The other thing you\nget is grounding. So that's initially\nwhy I became interested in this kind of\narchitecture, because I",
    "start": "501270",
    "end": "507360"
  },
  {
    "text": "was thinking a lot about\ngrounding and multimodality and things like that. And actually, one really\nnice way to ground things",
    "start": "507360",
    "end": "512820"
  },
  {
    "text": "is to find some other\ninformation that you can ground your generation in. So you really want the language\nmodel to only say things",
    "start": "512820",
    "end": "519840"
  },
  {
    "text": "that it has evidence for\nin this other piece of text or even multimodal data that\nit retrieves separately.",
    "start": "519840",
    "end": "526350"
  },
  {
    "text": "So if you do that, then\nyou get less hallucination, because you can always\npoint back to your source. It's always grounded\nin your source.",
    "start": "526350",
    "end": "532849"
  },
  {
    "text": "And you get\nattribution because you know why the model is\nsaying what it's saying is because it found\nthat this thing here.",
    "start": "532849",
    "end": "538830"
  },
  {
    "text": "Is that clear? All right, so for the\nrest of this lecture,",
    "start": "538830",
    "end": "545450"
  },
  {
    "text": "we're going to talk about\nthis basic architecture. And so it kind looks like\na pretty simple thing,",
    "start": "545450",
    "end": "552590"
  },
  {
    "text": "but there are\nactually lots and lots of questions you can ask about\nwhat this system should really look like.",
    "start": "552590",
    "end": "558650"
  },
  {
    "text": "And this doesn't even cover\nhalf the questions you can ask. So it really is about how do\nwe optimize this entire system.",
    "start": "558650",
    "end": "567777"
  },
  {
    "text": "So we have these\nseparate components-- the retriever, the generator. And then there are things\nlike this query encoder.",
    "start": "567777",
    "end": "574610"
  },
  {
    "text": "How do we encode queries? How do we do the retrieval? Do we update the\ndocument encoder?",
    "start": "574610",
    "end": "580220"
  },
  {
    "text": "How do we actually\ndefine the document? Is it like a full\ndocument, or is it a paragraph or a chunk or a\nsentence or a couple of words?",
    "start": "580220",
    "end": "588410"
  },
  {
    "text": "So there are lots\nof questions to ask. And as you'll see, there\nare lots of possible answers",
    "start": "588410",
    "end": "594649"
  },
  {
    "text": "to these questions as well. So this is what we'll cover.",
    "start": "594650",
    "end": "600649"
  },
  {
    "text": "So there are lots\nof architectures going into these questions.",
    "start": "600650",
    "end": "605990"
  },
  {
    "text": "And I think as we go through\nthem, it's useful for you to think about what happens\nduring training time",
    "start": "605990",
    "end": "612020"
  },
  {
    "text": "and what happens\nduring test time. So during training time,\nit's really, OK, we have this language model.",
    "start": "612020",
    "end": "617899"
  },
  {
    "text": "We have this retriever. Which one do we update? How do we update them?",
    "start": "617900",
    "end": "622910"
  },
  {
    "text": "How do we train\nthis entire system? Do we maybe not train it at all? Do we pre-train it from scratch?",
    "start": "622910",
    "end": "628460"
  },
  {
    "text": "Do we initialize it with\ncomponents that were already separately trained? These are the kinds\nof questions that you",
    "start": "628460",
    "end": "634460"
  },
  {
    "text": "have to answer if you want\nto design a system like this. And then during test time,\nyou have this entire system.",
    "start": "634460",
    "end": "641630"
  },
  {
    "text": "So actually multiple models in\na way that are working together.",
    "start": "641630",
    "end": "647540"
  },
  {
    "text": "So there's also different\nthings you can do there. So give it different\nindices during test time or manipulate how you're\nsampling, things like that.",
    "start": "647540",
    "end": "656370"
  },
  {
    "text": "So the starting point\nfor all of this stuff, I think if you ask someone\nnow, like, What is RAG?",
    "start": "656370",
    "end": "662100"
  },
  {
    "text": "they will think of this thing. So this is frozen\nRAG, basically.",
    "start": "662100",
    "end": "668129"
  },
  {
    "text": "There's no training here at all. So going back to this question\nof train time, test time, there's only test time here.",
    "start": "668130",
    "end": "673380"
  },
  {
    "text": "Train time happens separately\nwith these black box models that we don't\nnecessarily have control over.",
    "start": "673380",
    "end": "679020"
  },
  {
    "text": "So there's this\ndocument embedding model is whatever is\ncurrently at the top of some open-source leaderboard.",
    "start": "679020",
    "end": "686790"
  },
  {
    "text": "You use that-- oops, sorry-- to get some vectors\nthat you then use",
    "start": "686790",
    "end": "692100"
  },
  {
    "text": "to create this vector database. And then the vector\ndatabase just does search. And it gives the information\nfrom the search to the language",
    "start": "692100",
    "end": "699279"
  },
  {
    "text": "model. And it just passes\nit as the context. So this only works because\nof in-context learning.",
    "start": "699280",
    "end": "707730"
  },
  {
    "text": "And I think as a\nmachine learner myself, this feels very inelegant.",
    "start": "707730",
    "end": "714120"
  },
  {
    "text": "So what this\nlecture is about is, can we do better than\nthis frozen thing?",
    "start": "714120",
    "end": "721110"
  },
  {
    "text": "So let's start from\nthe left side of this. OK, if we want to outperform\nthis frozen thing itself",
    "start": "721110",
    "end": "727920"
  },
  {
    "text": "with just the vector\ndatabase, what would that look like from\na retrieval perspective?",
    "start": "727920",
    "end": "734029"
  },
  {
    "text": "And the starting point for\neverything retrieval is TF-IDF. Does everybody know\nwhat TF-IDF is?",
    "start": "734030",
    "end": "741140"
  },
  {
    "text": "No? OK, so TF-IDF is basically\na sparse retrieval method,",
    "start": "741140",
    "end": "746320"
  },
  {
    "text": "where you have a\nscore function that looks at documents and queries.",
    "start": "746320",
    "end": "751569"
  },
  {
    "text": "So D and Q. And then there are\nbasically two terms that matter. One is the TF, the\nTerm Frequency.",
    "start": "751570",
    "end": "757209"
  },
  {
    "text": "And the other is the IDF, the\nInverse Document Frequency. So this inverse\ndocument frequency is actually a really nice\nidea from Karen Sparck Jones,",
    "start": "757210",
    "end": "765100"
  },
  {
    "text": "really underrated researcher. She's done some amazing work. But the basic idea\nis that you want",
    "start": "765100",
    "end": "771250"
  },
  {
    "text": "to look at the words\nthat are very special. So they don't occur in lots\nof different documents. And so the overlap between\nthe word \"the\" doesn't really",
    "start": "771250",
    "end": "778840"
  },
  {
    "text": "matter. \"The\" occurs everywhere. So you want to have\nthe special words. So that's what TF-IDF does\nin a nutshell-- it gives you",
    "start": "778840",
    "end": "786760"
  },
  {
    "text": "a score for document\nquery overlap. And then you can do\nall kinds of things",
    "start": "786760",
    "end": "792130"
  },
  {
    "text": "here with how you weight it. So there's all these\nweird different parameters like this b and\nthings like that,",
    "start": "792130",
    "end": "797480"
  },
  {
    "text": "that allow you to make\nit better than just having the TF-IDF score. So there's a couple of\ntweaks you can do there.",
    "start": "797480",
    "end": "804410"
  },
  {
    "text": "So BM25, actually, in\ncase you're wondering, stands for Best Match 25. So I tried to discover where\ndoes the 25 actually come from.",
    "start": "804410",
    "end": "813860"
  },
  {
    "text": "That's because the preceding\n24 experiments failed.",
    "start": "813860",
    "end": "819140"
  },
  {
    "text": "So it's literally the 25th\none that seemed to work, and that's why it's called BM25. It's bizarre.",
    "start": "819140",
    "end": "826700"
  },
  {
    "text": "So this is sparse retrieval. It's just counting words. So you have this\nmassive, massive vector",
    "start": "826700",
    "end": "832130"
  },
  {
    "text": "of all these word occurrences. It's sparse because\nmost words never occur. So it's sort of like a vector\nof vocabulary size dimensions.",
    "start": "832130",
    "end": "842120"
  },
  {
    "text": "So most of that\nis obviously zero. So that's actually\na nice property",
    "start": "842120",
    "end": "847160"
  },
  {
    "text": "if you want to do\nfast search on a CPU. Because on a CPU,\nsparse dot product",
    "start": "847160",
    "end": "852500"
  },
  {
    "text": "is fairly easy to compute. So this is used in the system\ncalled DrQA, which is really",
    "start": "852500",
    "end": "860029"
  },
  {
    "text": "one of the first neural\ninstances of this open domain, open book question\nanswering paradigm.",
    "start": "860030",
    "end": "867260"
  },
  {
    "text": "So you have a question like,\nhow many of Warsaw's inhabitants blah, blah, blah. So you want to ask\nbasically Wikipedia",
    "start": "867260",
    "end": "874163"
  },
  {
    "text": "what the answer is for this. So then you have this\ndocument retriever based on these sparse-- so BM25, I think, in this case--",
    "start": "874163",
    "end": "881630"
  },
  {
    "text": "retrieval methods. You pass that to this-- I think this was still\nby LSTM at the time--",
    "start": "881630",
    "end": "889220"
  },
  {
    "text": "a document reader model. And then that model\ngives you the answer. So this, I think, is\nreally the first instance",
    "start": "889220",
    "end": "896540"
  },
  {
    "text": "of having this separation\nbetween a retrieval and a generator\nsystem that you use",
    "start": "896540",
    "end": "901880"
  },
  {
    "text": "for answering complicated\nquestions based on open domain knowledge.",
    "start": "901880",
    "end": "907100"
  },
  {
    "text": "So after the sparse\nstuff, there was a bunch of work on dense retrieval.",
    "start": "907100",
    "end": "912650"
  },
  {
    "text": "And so the advantage\nof dense retrieval-- so this is just like word\nembeddings, basically vectors.",
    "start": "912650",
    "end": "918290"
  },
  {
    "text": "They're dense now,\nno longer sparse. So they're much smaller in\nterms of dimensionality.",
    "start": "918290",
    "end": "923890"
  },
  {
    "text": "And the nice advantage\nof dense retrieval is that it's not really\nabout specific words.",
    "start": "923890",
    "end": "928959"
  },
  {
    "text": "So if there are\nsynonyms, you can still find the relevant document,\nwhich you couldn't really do",
    "start": "928960",
    "end": "936010"
  },
  {
    "text": "with a sparse representation. So that's really the\nadvantage of dense is that you get\nsemantic similarity.",
    "start": "936010",
    "end": "943150"
  },
  {
    "text": "So you can do this\nover word embeddings that doesn't really\nwork all that well. But at the time that people\nstarted thinking about this,",
    "start": "943150",
    "end": "950230"
  },
  {
    "text": "BERT was already out there. And BERT is really\ngreat for giving you a vector representation for\nan entire sequence of words.",
    "start": "950230",
    "end": "956290"
  },
  {
    "text": "So a sentence representation\nor a passage representation. So there are all these cool\nsystems like ORCA and DPR,",
    "start": "956290",
    "end": "963100"
  },
  {
    "text": "the Dense Passage Retriever,\nwhere they essentially use the retrieval as a latent\nvariable in the system.",
    "start": "963100",
    "end": "971620"
  },
  {
    "text": "And the way to get\nthe latent variable to work to be good\nenough, essentially to train the entire\nsystem is to pre-train",
    "start": "971620",
    "end": "979060"
  },
  {
    "text": "the retriever on\nrelevant information. So for archive, they do\nsomething called inverse close.",
    "start": "979060",
    "end": "985640"
  },
  {
    "text": "So they do a close\ntask where you want to find passages that\nare relevant to the preceding",
    "start": "985640",
    "end": "992450"
  },
  {
    "text": "passage. And then DPR, they just train\nit on the supervised thing. But really, the\ncore idea here is",
    "start": "992450",
    "end": "998210"
  },
  {
    "text": "that, as you can see\nin this graph here, you can do better than BM25\nif you had lots of documents.",
    "start": "998210",
    "end": "1004600"
  },
  {
    "text": "And the way you compute the\nscore function is much simpler. It's just the dot product. ",
    "start": "1004600",
    "end": "1010690"
  },
  {
    "text": "So the nice thing\nabout dot products is that you can do them very,\nvery efficiently on the GPU",
    "start": "1010690",
    "end": "1017170"
  },
  {
    "text": "as well if you know\nwhat you're doing. So what you really\nwant to get at",
    "start": "1017170",
    "end": "1022899"
  },
  {
    "text": "is Maximum Inner\nProduct Search, MIPS. So this is one of the core\nideas of a lot of this stuff.",
    "start": "1022900",
    "end": "1028990"
  },
  {
    "text": "And you can do MIPS with ANN,\nApproximate Nearest Neighbor search.",
    "start": "1028990",
    "end": "1034490"
  },
  {
    "text": "And so there's this really\nbrilliant piece of work out of FAIR for my\ncolleagues at the time",
    "start": "1034490",
    "end": "1040959"
  },
  {
    "text": "called FAISS, which\nreally underlies all of these modern\nvector databases.",
    "start": "1040960",
    "end": "1046089"
  },
  {
    "text": "So all the popular ones that\nare of re-implementations of this FAISS idea one is\nin like Rust one, is in Go.",
    "start": "1046089",
    "end": "1052078"
  },
  {
    "text": "But it's all basically\nthe same idea. It's just FAISS. And so FAISS really powers\na lot of this stuff.",
    "start": "1052078",
    "end": "1059380"
  },
  {
    "text": "And whenever somebody\ntells you something about a vector database,\njust think about FAISS. Very fast dot product.",
    "start": "1059380",
    "end": "1065425"
  },
  {
    "text": " So, obviously, you can\ngo beyond dot product.",
    "start": "1065425",
    "end": "1070970"
  },
  {
    "text": "Yes. What is it? What's FAISS? What is FAISS? Yeah. So it's an open-source library,\nFacebook AI Similarity Search.",
    "start": "1070970",
    "end": "1078935"
  },
  {
    "text": "Yeah, [INAUDIBLE]. No, so it's just basically\noff the shelf and algorithms.",
    "start": "1078935",
    "end": "1086590"
  },
  {
    "text": " Yeah, so there are all\nkinds of different--",
    "start": "1086590",
    "end": "1093090"
  },
  {
    "text": "I don't know if you do you know\nwhat product quantization is and things like that. So there are basically-- so\nyou have a bunch of vectors.",
    "start": "1093090",
    "end": "1100200"
  },
  {
    "text": "And you can just compute\nthe full dot product, which is sort of inefficient. So what you can do is try to\ncompress subspaces of the vector",
    "start": "1100200",
    "end": "1109289"
  },
  {
    "text": "and then just look at\nthe kind of centroids. So you can quantize\nsubvectors of the full vector",
    "start": "1109290",
    "end": "1116490"
  },
  {
    "text": "and then do much faster search\nover just the centroids. ",
    "start": "1116490",
    "end": "1122035"
  },
  {
    "text": "It's a good question. Any other questions? ",
    "start": "1122035",
    "end": "1127520"
  },
  {
    "text": "All right, so about\nthis dot product idea. So what we have\nhere is some people",
    "start": "1127520",
    "end": "1133610"
  },
  {
    "text": "call this a Siamese network. I guess it is. So you have two different\nbird models or whatever",
    "start": "1133610",
    "end": "1139640"
  },
  {
    "text": "your encoder is here. And then at the end, you\nget these two vectors. And then you just\ndo dot product. So you get one single score.",
    "start": "1139640",
    "end": "1145550"
  },
  {
    "text": "But you can do all kinds\nof much fancier things if you're willing to give up\non this bi encoder approach.",
    "start": "1145550",
    "end": "1153170"
  },
  {
    "text": "So a really nice example\nfrom one of your colleagues here at Stanford is ColBert.",
    "start": "1153170",
    "end": "1159290"
  },
  {
    "text": "So what this does is\nlay the interaction. So instead of just having\nthis dot product here,",
    "start": "1159290",
    "end": "1165680"
  },
  {
    "text": "you have a more complicated\nversion of computing a score where you aggregate\nover maximum similarity",
    "start": "1165680",
    "end": "1173090"
  },
  {
    "text": "scores between different words. So I only recently\nactually discovered that this is called ColBert\nbecause of the late night show",
    "start": "1173090",
    "end": "1179540"
  },
  {
    "text": "Colbert. So it's Omar's joke,\nactually, this name. But just so you know,\nif you run into it.",
    "start": "1179540",
    "end": "1186650"
  },
  {
    "text": " But I think if we look at\nwhere the state of the art",
    "start": "1186650",
    "end": "1193640"
  },
  {
    "text": "has been going now, one of the\nnice things about these vector databases is that\nthey're super efficient.",
    "start": "1193640",
    "end": "1198930"
  },
  {
    "text": "So dot product is\nmuch more efficient than this late\ninteraction stuff, especially if you do the\napproximate nearest neighbor",
    "start": "1198930",
    "end": "1204480"
  },
  {
    "text": "search. But there's been some\nreally cool work. So things like\nSPLADE, they basically",
    "start": "1204480",
    "end": "1211740"
  },
  {
    "text": "have sparse meat dense in a way. So one of the big problems\nas I said with sparse is that you can't really handle\nsynonyms and things like that.",
    "start": "1211740",
    "end": "1219300"
  },
  {
    "text": "But what you could do is take a\ndense model like a BERT model, look at this one word\nin your sequence,",
    "start": "1219300",
    "end": "1226620"
  },
  {
    "text": "try to see which other\nwords fit in the same slot. So that gives you the synonyms. So now you can give all these\nsynonyms to a sparse vector.",
    "start": "1226620",
    "end": "1235860"
  },
  {
    "text": "And then you can just\ndo sparse dot product, and so have a much, much more\nefficient way to do search",
    "start": "1235860",
    "end": "1241800"
  },
  {
    "text": "without giving up on\nall the cool stuff that you get from a\ndense representation.",
    "start": "1241800",
    "end": "1248010"
  },
  {
    "text": "So that's one thing. And this other idea I really\nlike is called DRAGON.",
    "start": "1248010",
    "end": "1253679"
  },
  {
    "text": "So this, I think, is\nreally the best generalized dense retriever. So if you want to take something\noff the shelf right now",
    "start": "1253680",
    "end": "1259840"
  },
  {
    "text": "and just go to Hugging\nFace or something, then this DRAGON or\nDragonPlus is probably the thing you want to\nuse for dense retriever.",
    "start": "1259840",
    "end": "1266429"
  },
  {
    "text": "And the way they train this is\nthrough this progressive data augmentation strategy to make\nthe model better and better",
    "start": "1266430",
    "end": "1273060"
  },
  {
    "text": "over time by sampling\nvery difficult negatives. And that gives you very\ngood representations.",
    "start": "1273060",
    "end": "1280530"
  },
  {
    "text": "And so the other\nthing about this I think is the only final point\nabout retrieval in general",
    "start": "1280530",
    "end": "1287070"
  },
  {
    "text": "is that what we see happening\nright now if you look at the developer\ncommunity around RAG",
    "start": "1287070",
    "end": "1292080"
  },
  {
    "text": "is that they're all doing\nhybrid search right now. So you can actually just\ncombine the search results",
    "start": "1292080",
    "end": "1297179"
  },
  {
    "text": "from your sparse BM25 or\nwhatever thing or SPLAYED. And you can combine\nthem with your DRAGON.",
    "start": "1297180",
    "end": "1303840"
  },
  {
    "text": "And then you get this ranking\nthat works even better. So then you get\nbest of both worlds, but then you get\nall these questions",
    "start": "1303840",
    "end": "1310150"
  },
  {
    "text": "about how do you\ncombine the results. Any questions on this part?",
    "start": "1310150",
    "end": "1317140"
  },
  {
    "text": "Oh, can you hear me? Yes. Oh, sorry. On the earlier\nslide, has there been",
    "start": "1317140",
    "end": "1323780"
  },
  {
    "text": "any work on benchmark, how much\nless hallucination RAG incurs over a closed book\nquestion answering?",
    "start": "1323780",
    "end": "1330950"
  },
  {
    "text": "For example, directly asking\nthe large language model the question, has there been any\nbenchmarking studies in this?",
    "start": "1330950",
    "end": "1337280"
  },
  {
    "text": "Yeah, so there's a great\npaper if I can say so myself on the fact that\nretrieval augmentation",
    "start": "1337280",
    "end": "1342560"
  },
  {
    "text": "reduces hallucination. It's from 2021, I think. So yeah, you can just--",
    "start": "1342560",
    "end": "1348679"
  },
  {
    "text": "if you literally look for\nretrieval augmentation reduces hallucination, then\nyou'll find the paper.",
    "start": "1348680",
    "end": "1354030"
  },
  {
    "text": "Oh, thank you.  [INAUDIBLE],, what's\nthe [INAUDIBLE]??",
    "start": "1354030",
    "end": "1361035"
  },
  {
    "text": " Yeah, so very often, you want to\nhave very precise word overlap",
    "start": "1361035",
    "end": "1371220"
  },
  {
    "text": "for things where you don't\nwant to have the synonyms or the nearest neighbors. So if there's a brand name or\nsomething like that, then--",
    "start": "1371220",
    "end": "1381150"
  },
  {
    "text": "let's say the brand is Apple. You don't want to find\nstuff about pears. So that's what you would\ndo with a dense retriever.",
    "start": "1381150",
    "end": "1388740"
  },
  {
    "text": "So it really depends on\nwhat you want to use it for. That's why hybrid is\nprobably the way to go.",
    "start": "1388740",
    "end": "1395919"
  },
  {
    "text": "It's a good question. Wait, with the dense things,\nit's contextualized embeddings.",
    "start": "1395920",
    "end": "1403980"
  },
  {
    "text": "But shouldn't it realize\nApple, the company, would be different from-- No, so if they were actually\ncontextualized, then yes.",
    "start": "1403980",
    "end": "1411450"
  },
  {
    "text": "But very often, it's a\nfrozen retrieval system. That's one of the problems\nwith all the frozen RAG stuff.",
    "start": "1411450",
    "end": "1417735"
  },
  {
    "text": " I might be missing\nsomething very basic here.",
    "start": "1417735",
    "end": "1424252"
  },
  {
    "text": "[INAUDIBLE] referring\nto the vectors that you're getting\nthe [INAUDIBLE]..",
    "start": "1424252",
    "end": "1429370"
  },
  {
    "text": "Vectors that you're using is\neither the [INAUDIBLE] query or the [INAUDIBLE].",
    "start": "1429370",
    "end": "1434929"
  },
  {
    "text": "No. And so the document and the\nquery, they're the same.",
    "start": "1434930",
    "end": "1441260"
  },
  {
    "text": "So they're either\nsparse, or they're dense. So if they're sparse, the\ncomponents of the vector",
    "start": "1441260",
    "end": "1446420"
  },
  {
    "text": "are literally the other words. So-- You just one\npenalized when you're",
    "start": "1446420",
    "end": "1451980"
  },
  {
    "text": "[INAUDIBLE] to the thing\nthat creates the embeddings?",
    "start": "1451980",
    "end": "1457950"
  },
  {
    "text": "How are you getting [INAUDIBLE]? So it's literally counts. So basically, it's one big\nmatrix of documents as rows.",
    "start": "1457950",
    "end": "1466650"
  },
  {
    "text": "And the columns are the\nwords in the documents. And then you just\ncount how often a word occurs in a document.",
    "start": "1466650",
    "end": "1472650"
  },
  {
    "text": "Oh. So that's a sparse. [INAUDIBLE] property of sparse. You're also referring\nto [INAUDIBLE]..",
    "start": "1472650",
    "end": "1480429"
  },
  {
    "text": "Yeah, and so in\nthe field, we call them sparse embeddings\nor sparse retrieval",
    "start": "1480430",
    "end": "1486309"
  },
  {
    "text": "because most of that vector\nis 0 because most words don't occur in that document.",
    "start": "1486310",
    "end": "1494220"
  },
  {
    "text": "Does that make sense? Yeah.  Cool.",
    "start": "1494220",
    "end": "1500300"
  },
  {
    "text": "So let's talk about\ndoing slightly better. So going back to\nSteven's question about,",
    "start": "1500300",
    "end": "1506950"
  },
  {
    "text": "OK, we have this\nretrieval thing. But how do we actually make this\nretrieval good for the context",
    "start": "1506950",
    "end": "1512890"
  },
  {
    "text": "that it's going to be used in? So can we contextualize the\nretrieval for the generator,",
    "start": "1512890",
    "end": "1518169"
  },
  {
    "text": "even if it's a generator\nwhere we might not have access to the weights? So it could be a GPT4 model.",
    "start": "1518170",
    "end": "1524080"
  },
  {
    "text": "We just send it to some API. We get some stuff back. And so one paper I really\nlike is called \"RePlug.\"",
    "start": "1524080",
    "end": "1532300"
  },
  {
    "text": "So just to explain\nwhat this looks like. So you have this context. You have a retriever that\nwe do the standard retrieval",
    "start": "1532300",
    "end": "1539330"
  },
  {
    "text": "step with. This is a dense retriever. Sorry. And now you compute\nthe likelihood.",
    "start": "1539330",
    "end": "1546880"
  },
  {
    "text": "So you basically just\nnormalize the scores that you get for\nthe top k documents to get a distribution here.",
    "start": "1546880",
    "end": "1553030"
  },
  {
    "text": "And then you give each one\nof the retrieved documents separately to this generator,\nto your language model.",
    "start": "1553030",
    "end": "1560570"
  },
  {
    "text": "So you can look at the\nperplexity of the correct answer for that language model.",
    "start": "1560570",
    "end": "1566270"
  },
  {
    "text": "So now we have these two\nprobability distributions or two likelihoods, essentially. And we can minimize\nthe kl divergence",
    "start": "1566270",
    "end": "1572720"
  },
  {
    "text": "to make sure that\nwe can actually retrieve the documents\nthat lead to the lowest perplexity on the right\nanswer for the language model.",
    "start": "1572720",
    "end": "1581330"
  },
  {
    "text": "So super simple idea. Works really, really well.",
    "start": "1581330",
    "end": "1586400"
  },
  {
    "text": "And the nice thing about this\nis completely agnostic of what happens upstream. So this will work for any\nsort of encoder-decoder.",
    "start": "1586400",
    "end": "1593630"
  },
  {
    "text": "For any language model, what\nyou need is a perplexity score.",
    "start": "1593630",
    "end": "1598880"
  },
  {
    "text": "But for most\nlanguage models, you can get that, not\nnecessarily all of them. So that's one thing.",
    "start": "1598880",
    "end": "1604070"
  },
  {
    "text": "And then there's this other\nreally nice approach on-- Well, what parameters are\n[INAUDIBLE] in the retriever?",
    "start": "1604070",
    "end": "1612320"
  },
  {
    "text": "So in the retriever,\nyou're literally updating the dense\nrepresentations.",
    "start": "1612320",
    "end": "1618440"
  },
  {
    "text": "So your encoder basically for\nyour dense representation. That's a good question. We'll get into that a bit more.",
    "start": "1618440",
    "end": "1624460"
  },
  {
    "text": "So there's another paper on\nin-context retrieval augmented language models, where the\nwhole paper is basically",
    "start": "1624460",
    "end": "1631720"
  },
  {
    "text": "about just doing BM25 and\njust giving stuff directly to the context of\nthe language model.",
    "start": "1631720",
    "end": "1637030"
  },
  {
    "text": "And things work. So it's frozen RAG but even\nmore primitive in a way",
    "start": "1637030",
    "end": "1642490"
  },
  {
    "text": "where the retriever is this\nvery old sparse algorithm. But it works\nreally, really well.",
    "start": "1642490",
    "end": "1648880"
  },
  {
    "text": "But then they have this\nreally awesome section where they show that you can\njust have this reranker on top",
    "start": "1648880",
    "end": "1655510"
  },
  {
    "text": "of the BM25 results. And you can backprop\ninto this reranker. So now you still keep the\nlanguage model completely fixed.",
    "start": "1655510",
    "end": "1663220"
  },
  {
    "text": "So that's this part\nof the loss here. So you have a stop gradient\non the parameters theta.",
    "start": "1663220",
    "end": "1669429"
  },
  {
    "text": "That's just your language model. But now you have\nthis rank function",
    "start": "1669430",
    "end": "1674799"
  },
  {
    "text": "here that you can backprop into. So that's your\nreranker is basically-- it can be a BERT model\nor anything like that",
    "start": "1674800",
    "end": "1680783"
  },
  {
    "text": "works on top of the\nthings you initially retrieved from your BM25. And now you have\nthis BERT reranker",
    "start": "1680783",
    "end": "1686050"
  },
  {
    "text": "that you can backprop into. So this also works\nreally, really nice.",
    "start": "1686050",
    "end": "1691360"
  },
  {
    "text": "So we're slowly progressing\ntowards having a system that is much more optimized for being\nproperly retrieval augmented",
    "start": "1691360",
    "end": "1699250"
  },
  {
    "text": "in a way where it's useful\nand contextualized for what you want to use it for.",
    "start": "1699250",
    "end": "1704650"
  },
  {
    "text": "So yeah, just to point\nout what that looks like with this reranker. So you just have this\nextra step, essentially.",
    "start": "1704650",
    "end": "1710680"
  },
  {
    "text": "So we have our retriever,\nthen we have our reranker, then we have our\ngenerator and our output.",
    "start": "1710680",
    "end": "1716820"
  },
  {
    "text": "[INAUDIBLE] No, not necessarily.",
    "start": "1716820",
    "end": "1723570"
  },
  {
    "text": "So for this one, you do, yeah. So for RePlug, you don't.",
    "start": "1723570",
    "end": "1728800"
  },
  {
    "text": "Yeah, [INAUDIBLE]. Got it. Yeah, yeah, yeah. So basically, you need to get-- [INAUDIBLE]",
    "start": "1728800",
    "end": "1735400"
  },
  {
    "text": "Not all of them. Some of them do. But yeah, there are\nall kinds of tricks you can do on top of that, yeah.",
    "start": "1735400",
    "end": "1741460"
  },
  {
    "text": " So basically, the\nquestion is, how do we get",
    "start": "1741460",
    "end": "1747760"
  },
  {
    "text": "gradients flowing into this? So if you don't\nactually have access to the full parameters\nof the model",
    "start": "1747760",
    "end": "1752870"
  },
  {
    "text": "so that you can backprop\nall the way through it, then you can do a reinforce\nstyle loss on the retrieval.",
    "start": "1752870",
    "end": "1760090"
  },
  {
    "text": "And then you just pass\nthe log likelihood if you have access to that or\nsome other black box function.",
    "start": "1760090",
    "end": "1766045"
  },
  {
    "start": "1766045",
    "end": "1771550"
  },
  {
    "text": "All right, the next\nthing, you can do is to optimize both the\nretriever and the generator.",
    "start": "1771550",
    "end": "1780100"
  },
  {
    "text": "And so this really\nstarts getting to the proper contextualization\nof the entire architecture",
    "start": "1780100",
    "end": "1786669"
  },
  {
    "text": "where you want everything\nto work together. So rather than having\nthis frozen thing where everything is\nbasically not aware",
    "start": "1786670",
    "end": "1792730"
  },
  {
    "text": "that the other part exists. It's like two\nhalves of the brain. They're not talking\nto each other. One is your retriever.",
    "start": "1792730",
    "end": "1797770"
  },
  {
    "text": "That is your language model. There's no connection. They're just sort\nof like something is thrown over the fence, and\nthen you hope for the best.",
    "start": "1797770",
    "end": "1803920"
  },
  {
    "text": "So instead of that, we\nhave everything much closer and learning together. So one of the first ways of\ndoing this with a generator",
    "start": "1803920",
    "end": "1814810"
  },
  {
    "text": "was RAG, Retrieval\nAugmented Generation, which we did at FAIR in 2020.",
    "start": "1814810",
    "end": "1820179"
  },
  {
    "text": "And it's very similar to\nwhat we've already seen. We basically have\nthis retriever here",
    "start": "1820180",
    "end": "1825700"
  },
  {
    "text": "that works over\ndifferent documents. You get some score\nfunction that gets given to this generator\nthat generates the answer.",
    "start": "1825700",
    "end": "1833620"
  },
  {
    "text": "And now you want to\nbackprop all the way and update your\ngenerator as well. So in the previous\ntwo architectures,",
    "start": "1833620",
    "end": "1840010"
  },
  {
    "text": "we saw you keep the\ngenerator fixed. You backprop into\nyour retriever. But here we update everything.",
    "start": "1840010",
    "end": "1846400"
  },
  {
    "text": "Well, not exactly\neverything, as you'll see, but we'll also update\nthe part of the retriever",
    "start": "1846400",
    "end": "1852279"
  },
  {
    "text": "and the generator. So in this RAG\nmodel, we actually",
    "start": "1852280",
    "end": "1857470"
  },
  {
    "text": "have two different\nways of doing this. And this is probably\nsomething that when we talk about this, if you think\nabout this long enough, then",
    "start": "1857470",
    "end": "1865150"
  },
  {
    "text": "you'll think like, OK, but when\nactually do I need to retrieve? Do I retrieve every time\nI generate a new token?",
    "start": "1865150",
    "end": "1872169"
  },
  {
    "text": "Or do I just retrieve\nonce and then generate an entire sequence? Or maybe I want to\nretrieve every n tokens?",
    "start": "1872170",
    "end": "1880502"
  },
  {
    "text": "So these are hyperparameters. Or maybe I want to\nlearn when to retrieve as well as we'll see that's\nalso something people have done.",
    "start": "1880502",
    "end": "1886720"
  },
  {
    "text": "So these are two\ndifferent ways to do it. And what we do in this\npaper-- basically,",
    "start": "1886720",
    "end": "1892580"
  },
  {
    "text": "the whole point of the paper is\nthat this frozen thing doesn't really work all that well. So I think what people\ncall RAG now is usually",
    "start": "1892580",
    "end": "1901730"
  },
  {
    "text": "refers to the frozen thing,\nbut the whole paper basically would never have been accepted\nanywhere if we had just",
    "start": "1901730",
    "end": "1907220"
  },
  {
    "text": "done the frozen thing. The whole point of the paper is\nthat you want to optimize it.",
    "start": "1907220",
    "end": "1912420"
  },
  {
    "text": "And so at my\ncompany, Contextual, we call this frozen thing\nFrankenstein's monster, because it's really\nlike you cobble together",
    "start": "1912420",
    "end": "1918950"
  },
  {
    "text": "these different pieces. It's really like Frankenstein. You just put it together,\nand then it walks.",
    "start": "1918950",
    "end": "1925843"
  },
  {
    "text": "But it doesn't\nreally have a soul. It doesn't really actually work. It's not the real thing.",
    "start": "1925843",
    "end": "1930860"
  },
  {
    "text": "So that's great for\neveryone here, I think, because there are so\nmany opportunities to do better than what most\npeople are using right now.",
    "start": "1930860",
    "end": "1940100"
  },
  {
    "text": "So one of the limitations of\nthe original RAG architecture is that it only\nsupports a very small k.",
    "start": "1940100",
    "end": "1947560"
  },
  {
    "text": "So if you have lots\nand lots of documents, then the problem is that\nyou have to fit all of them",
    "start": "1947560",
    "end": "1953320"
  },
  {
    "text": "into context. But how do you really\nget that to fit? So one thing you can do is\nyou first encode things so",
    "start": "1953320",
    "end": "1963340"
  },
  {
    "text": "that you get one single\nrepresentation or only the few top-level\nrepresentations. Then you concatenate.",
    "start": "1963340",
    "end": "1969159"
  },
  {
    "text": "Those and then you just\nfeed them to the decoder. So this is FiD,\nFusion in Decoder.",
    "start": "1969160",
    "end": "1974770"
  },
  {
    "text": "And as you can see, this\nscales to a much higher number of passages, and that leads\nto corresponding improvements",
    "start": "1974770",
    "end": "1983170"
  },
  {
    "text": "in the scores that\nyou care about. So that's a really cool idea.",
    "start": "1983170",
    "end": "1988529"
  },
  {
    "text": "And so we're slowly\nmoving towards more decoder-only architectures. So in RAG, we have\nthis BART model.",
    "start": "1988530",
    "end": "1995270"
  },
  {
    "text": "It's an encoder-decoder\narchitecture. But here, you just\nhave this decoder that does some fancy\nattention over stuff",
    "start": "1995270",
    "end": "2002020"
  },
  {
    "text": "that you retrieved before. And so another pure decoder\nlanguage model architecture",
    "start": "2002020",
    "end": "2011200"
  },
  {
    "text": "is this one, kNN-LM,\nwhich I think is very elegant\nin its simplicity.",
    "start": "2011200",
    "end": "2016660"
  },
  {
    "text": "So it's basically you just\nhave a normal language model. But you interpolate the\nnormal language model weights",
    "start": "2016660",
    "end": "2023710"
  },
  {
    "text": "with things that you retrieved. So basically, you have\nsome sort of prompt.",
    "start": "2023710",
    "end": "2029290"
  },
  {
    "text": "So like Obama's birthplace is. You go to your big corpus. You find similar things.",
    "start": "2029290",
    "end": "2034570"
  },
  {
    "text": "You look at the words that come\nnext to the similar things. You rank that thing.",
    "start": "2034570",
    "end": "2040330"
  },
  {
    "text": "You sample your top k. You renormalize that. So now you have a\nbunch of scores,",
    "start": "2040330",
    "end": "2045580"
  },
  {
    "text": "and now you can just interpolate\nbetween your retrieved non-parametric memory scores and\nyour parametric language model",
    "start": "2045580",
    "end": "2052980"
  },
  {
    "text": "scores. So this is very late\nfusion in a sense. At the very end, you\ncombine these two,",
    "start": "2052980",
    "end": "2058429"
  },
  {
    "text": "and it allows you to reweight\nthe pure language model probabilities or likelihoods.",
    "start": "2058429",
    "end": "2064099"
  },
  {
    "text": "So this works really well. And it scales especially well\nif you have a huge retrieval",
    "start": "2064100",
    "end": "2069530"
  },
  {
    "text": "corpus. So if you have trillions and\ntrillions of tokens in there, you can have a much\nsmaller language model",
    "start": "2069530",
    "end": "2075469"
  },
  {
    "text": "that does not that much heavy\nlifting because you can really rely on this big source corpus\nthat you're working from.",
    "start": "2075469",
    "end": "2081800"
  },
  {
    "text": "And so that idea was exploited\nby this paper called \"Retro\" out of DeepMind, where they\nshowed that you can have a 25",
    "start": "2081800",
    "end": "2090860"
  },
  {
    "text": "times smaller retrieval\naugmented language model trained from scratch. So really pre-trained\nentirely from scratch",
    "start": "2090860",
    "end": "2097609"
  },
  {
    "text": "that outperforms this\n25 times bigger language model on the same data\nin terms of perplexity,",
    "start": "2097610",
    "end": "2103460"
  },
  {
    "text": "which is pretty impressive. So this architecture\nis much more efficient than a parametric\nmodel because you can",
    "start": "2103460",
    "end": "2110080"
  },
  {
    "text": "rely on this external memory. So if your external\nmemory is big enough, you can get pretty huge gains.",
    "start": "2110080",
    "end": "2117280"
  },
  {
    "text": "So there was a lot of\nexcitement about RETRO when it was announced,\nbut it's a DeepMind paper.",
    "start": "2117280",
    "end": "2122529"
  },
  {
    "text": "So there's really\nno open-source. Nothing really to validate\nthat this actually works.",
    "start": "2122530",
    "end": "2128380"
  },
  {
    "text": "And so very recently, there has\nbeen a bit of work from NVIDIA called Retro++, where they have\nthis hybrid between the RETRO",
    "start": "2128380",
    "end": "2138160"
  },
  {
    "text": "architecture, and then\nthey do basically RAG. They put the top one\nor the top k results",
    "start": "2138160",
    "end": "2143890"
  },
  {
    "text": "in the context of the\nlanguage model after all. So it's a crossover\nbetween RAG and RETRO.",
    "start": "2143890",
    "end": "2149980"
  },
  {
    "text": "And they showed some\nreally nice results here. But I think it's pointing\nto this big flaw, I think,",
    "start": "2149980",
    "end": "2156490"
  },
  {
    "text": "is that, why is there still no\ngood open-source RETRO model? That probably tells you\nsomething about whether it",
    "start": "2156490",
    "end": "2163329"
  },
  {
    "text": "actually really works. I spent a lot of\ntime in my career trying to reproduce\nDeepMind papers that",
    "start": "2163330",
    "end": "2168730"
  },
  {
    "text": "didn't necessarily always work. And so I think the\nsame is true for RETRO.",
    "start": "2168730",
    "end": "2175900"
  },
  {
    "text": "And that's why we need\nto do this in context RAG on top of RETRO to\nactually get it to work.",
    "start": "2175900",
    "end": "2182160"
  },
  {
    "text": "But could it just be\na true [INAUDIBLE]?? Because you're searching\na trillion [INAUDIBLE]??",
    "start": "2182160",
    "end": "2188579"
  },
  {
    "text": "Yeah, so-- Deploying that\n[INAUDIBLE] stuff? Yeah, so the doing retrieval\nover that big corpus",
    "start": "2188580",
    "end": "2196539"
  },
  {
    "text": "is not that difficult,\nactually, yeah. So there are even like\ndistributed FAISS packages.",
    "start": "2196540",
    "end": "2203079"
  },
  {
    "text": "You can just do\neverything yourself. So-- [INAUDIBLE] Yeah, so in terms\nof compute, it's",
    "start": "2203080",
    "end": "2208540"
  },
  {
    "text": "actually not that hard\nanymore to reproduce something like this. But I tried several times, and\nit is not really reproducible.",
    "start": "2208540",
    "end": "2217099"
  },
  {
    "text": "So the only way\nto get it to work is if you do this in context\nRAG on top of the RETRO thing. And then as you can see\nhere in the results,",
    "start": "2217100",
    "end": "2223490"
  },
  {
    "text": "then it actually gives you a\ngain over the pure GPT model. So it starts from\na GPT, and then they retrofit as they\ncall it the GPT model.",
    "start": "2223490",
    "end": "2232900"
  },
  {
    "text": "So in short, I think\nthere are still a lot of work to be done in\npre-training these systems really from scratch.",
    "start": "2232900",
    "end": "2238660"
  },
  {
    "text": "And RETRO showed that\nit might be possible, but we don't necessarily\nknow exactly how to do it the right way.",
    "start": "2238660",
    "end": "2244270"
  },
  {
    "text": "And this is really one of the\ninteresting open questions. Any questions on that?",
    "start": "2244270",
    "end": "2250300"
  },
  {
    "text": " Online? ",
    "start": "2250300",
    "end": "2259730"
  },
  {
    "text": "No? OK, then we'll move on. So let's go all the way\nwith the contextualization.",
    "start": "2259730",
    "end": "2269610"
  },
  {
    "text": "So with RETRO and with RAG,\nwhat we actually did is we only updated\nthe query encoder.",
    "start": "2269610",
    "end": "2276720"
  },
  {
    "text": "So updating the document\nencoder is very expensive. So one of the first\npapers, actually,",
    "start": "2276720",
    "end": "2283589"
  },
  {
    "text": "the OG of the non-frozen dense\nretrieval augmented methods, is this paper called \"REALM.\"",
    "start": "2283590",
    "end": "2290280"
  },
  {
    "text": "This is really like\nvisionary work. This was basically the first\nversion that did this properly",
    "start": "2290280",
    "end": "2297750"
  },
  {
    "text": "where they updated\nit all the way, including the document encoder. So can someone\nexplain to me why it's",
    "start": "2297750",
    "end": "2304740"
  },
  {
    "text": "expensive to update\nthe document encoder? ",
    "start": "2304740",
    "end": "2310970"
  },
  {
    "text": "So let's say we have a\ntrillion tokens in our corpus. So now we go all the way.",
    "start": "2310970",
    "end": "2317150"
  },
  {
    "text": "So we basically\ndo a forward pass. We get a gradient at the end. Now we back propagate the\ngradient through the retriever.",
    "start": "2317150",
    "end": "2323780"
  },
  {
    "text": "We update the query encoder. Now we have to update\nthe document encoder. So what do we then\nneed to do after we've",
    "start": "2323780",
    "end": "2329720"
  },
  {
    "text": "updated the document encoder? We need to re-encode\nthe entire internet.",
    "start": "2329720",
    "end": "2334730"
  },
  {
    "text": "So basically, every\nsingle gradient update, we have to re-encode\nwhatever our index is.",
    "start": "2334730",
    "end": "2340099"
  },
  {
    "text": "And so if this is like\ntrillions of tokens, it's like re-encoding the\ninternet after every batch update.",
    "start": "2340100",
    "end": "2345950"
  },
  {
    "text": "So that's not very efficient. [INAUDIBLE] or other stuff\nthat if you [INAUDIBLE]",
    "start": "2345950",
    "end": "2357675"
  },
  {
    "text": "take care of all the activations\nand have some [INAUDIBLE] unpredictable thing. So [INAUDIBLE] this?",
    "start": "2357675",
    "end": "2363800"
  },
  {
    "text": "Yeah.  Yeah, that's one way to do it.",
    "start": "2363800",
    "end": "2369140"
  },
  {
    "text": "So there are a bunch\nof different ways to update the document encoder. So what they do in REALM is they\nbasically do it for t batches.",
    "start": "2369140",
    "end": "2378380"
  },
  {
    "text": "Then they stop. They re-encode the\nentire internet, and then they train again.",
    "start": "2378380",
    "end": "2383520"
  },
  {
    "text": "So it's sort of\nasynchronous updates. They have this very fancy\nsharding mechanisms, where they take down certain\nparts of their entire index",
    "start": "2383520",
    "end": "2392690"
  },
  {
    "text": "and then update them on the fly. So you can do it. It's just very expensive.",
    "start": "2392690",
    "end": "2397760"
  },
  {
    "text": "So one of the things\nthat a lot of people have been thinking about,\nnot exactly the LoRa idea, but similar versions of\nthat are around like,",
    "start": "2397760",
    "end": "2406290"
  },
  {
    "text": "can you make it more\nefficient so that you don't have to do this asynchronously?",
    "start": "2406290",
    "end": "2412740"
  },
  {
    "text": "So one of the downsides\nof this REALM architecture is that it's really\njust a bird model,",
    "start": "2412740",
    "end": "2418328"
  },
  {
    "text": "but then you do this\nretrieval augmentation on a bird model with\nother bird models. So it's not really generative. It's not really gen AI\nin the modern paradigm.",
    "start": "2418328",
    "end": "2426120"
  },
  {
    "text": "But if you want to read\none paper on this topic, this is a very good one to read.",
    "start": "2426120",
    "end": "2432990"
  },
  {
    "text": "The other one that is really,\nreally good to read is this paper called \"Atlas.\"",
    "start": "2432990",
    "end": "2438510"
  },
  {
    "text": "So Atlas is-- so this is out\nof FAIR with a bunch of folks,",
    "start": "2438510",
    "end": "2444630"
  },
  {
    "text": "the folks who did RAG and the\nfolks who did FID and really a brilliant set of people.",
    "start": "2444630",
    "end": "2450930"
  },
  {
    "text": "And this is really a\ncomprehensive analysis of everything that's happening\nin this architecture.",
    "start": "2450930",
    "end": "2457050"
  },
  {
    "text": "So the first question\nthey really look at is, how do we train\nthis retriever? So we've seen a couple\nof versions of this,",
    "start": "2457050",
    "end": "2463830"
  },
  {
    "text": "but which one\nactually works better? They haven't really\nbeen compared in a head-to-head setting.",
    "start": "2463830",
    "end": "2469500"
  },
  {
    "text": "So one thing is we have this FID\nstyle attention distillation. So that's really too complicated\nto go into detail here.",
    "start": "2469500",
    "end": "2477700"
  },
  {
    "text": "But the others are\nactually very simple. So one is this loss we've\nbasically seen before.",
    "start": "2477700",
    "end": "2484660"
  },
  {
    "text": "So we've seen this, I think,\nwith the in-context RAG one. So we have a stop gradient\non the language model,",
    "start": "2484660",
    "end": "2490390"
  },
  {
    "text": "and then we update\nthe retriever. The other one is what\nwe've seen with REPLUG. So this is basically\nexactly the REPLUG loss.",
    "start": "2490390",
    "end": "2497680"
  },
  {
    "text": "So we have the KL\ndivergence of the documents and the improvement that you see\nwhen you give it that document.",
    "start": "2497680",
    "end": "2507070"
  },
  {
    "text": "The other thing they\nhave is basically the inverse of that one. So if I take this\none document out,",
    "start": "2507070",
    "end": "2513160"
  },
  {
    "text": "how does that\naffect my perplexity of the language model?",
    "start": "2513160",
    "end": "2519010"
  },
  {
    "text": "And so this one, I think,\nis actually quite elegant because that really\ngets to how valuable",
    "start": "2519010",
    "end": "2524770"
  },
  {
    "text": "is this one single\ndocument for me answering this question correctly.",
    "start": "2524770",
    "end": "2529900"
  },
  {
    "text": "So they compare all of\nthese different versions. And what you can see is that the\nREPLUG-style loss and this leave",
    "start": "2529900",
    "end": "2539080"
  },
  {
    "text": "one out loss, they\nperformed a lot better than all of these others. So this fixed retriever or\nno joint pre-training, these",
    "start": "2539080",
    "end": "2545500"
  },
  {
    "text": "are really the baseline frozen\nRAG models or close book. And as you can see, you\ncan do really a lot better",
    "start": "2545500",
    "end": "2553089"
  },
  {
    "text": "if you optimize things. And so just leave one out\nthing is probably the best,",
    "start": "2553090",
    "end": "2558430"
  },
  {
    "text": "I would say. So then the other question\nis, how do you actually train that entire system?",
    "start": "2558430",
    "end": "2564460"
  },
  {
    "text": "What data or what tasks\ndo you train this on? So they also experiment with\na bunch of different versions.",
    "start": "2564460",
    "end": "2570670"
  },
  {
    "text": "So one is doing PrefixLM, if\nyou're familiar with that. So they basically take a\nchunk that occurs somewhere",
    "start": "2570670",
    "end": "2578830"
  },
  {
    "text": "on the internet, and then\nthey predict the next chunk from that chunk. So it's really like\nsentence to sentence.",
    "start": "2578830",
    "end": "2585820"
  },
  {
    "text": "So maybe like Skip-Thought\nback in the day, but now you have this\nretrieval step where you predict the next sentence.",
    "start": "2585820",
    "end": "2591650"
  },
  {
    "text": "And then they just do\nT5 styles or denoising. So there's mass\nlanguage modeling if you're familiar with T5.",
    "start": "2591650",
    "end": "2598270"
  },
  {
    "text": "And then they have this title\nto section generation piece. So I think the takeaway\nfrom this table",
    "start": "2598270",
    "end": "2603940"
  },
  {
    "text": "is basically that\nwhatever you do here-- so they're using T5 models. So whatever you do here needs to\nbe the same that your language",
    "start": "2603940",
    "end": "2611230"
  },
  {
    "text": "model expects. So for T5, that's T5 style loss.",
    "start": "2611230",
    "end": "2616930"
  },
  {
    "text": "And then the next final\nquestion that they look into going back to what\nwe talked about, how exactly do",
    "start": "2616930",
    "end": "2624250"
  },
  {
    "text": "we update this retriever? So do we have to update\nthe document encoder? Or do we maybe have\nto do some reranking?",
    "start": "2624250",
    "end": "2632230"
  },
  {
    "text": "Or do we maybe just\nupdate the query? And quite surprisingly,\nI think they find",
    "start": "2632230",
    "end": "2637270"
  },
  {
    "text": "that just updating the query. So in the original\nRAG paper, it's actually already basically\ngood enough in many cases.",
    "start": "2637270",
    "end": "2645520"
  },
  {
    "text": "So that's nice because\nit's much more efficient if you don't have to update\nyour documents all the time.",
    "start": "2645520",
    "end": "2651550"
  },
  {
    "text": "I think the real question\nhere, though, is like, how good is your document\nrepresentation to begin with?",
    "start": "2651550",
    "end": "2658390"
  },
  {
    "text": "So you need to have very, very\nhigh quality embedding model for this to work. If you don't have that,\nthen this will not work.",
    "start": "2658390",
    "end": "2663940"
  },
  {
    "text": "But if you do have that, then\nyou get a very nice query side fine-tuning thing.",
    "start": "2663940",
    "end": "2668980"
  },
  {
    "text": "What's [INAUDIBLE]?  So the Atlas paper is about\ntrying to do few shot language",
    "start": "2668980",
    "end": "2678170"
  },
  {
    "text": "modeling tasks. So it's how many examples\nare given in the context. ",
    "start": "2678170",
    "end": "2686609"
  },
  {
    "text": "Yeah, so the main\ntakeaway here is that if you compare\nthe close book,",
    "start": "2686610",
    "end": "2691830"
  },
  {
    "text": "equivalent model to the\nretrieval augmented model, you see very big improvements.",
    "start": "2691830",
    "end": "2699720"
  },
  {
    "text": "That's really the only takeaway\nof this entire section.",
    "start": "2699720",
    "end": "2704730"
  },
  {
    "text": "But I think that that's\nreally saying something in terms of what we\nshould be thinking about.",
    "start": "2704730",
    "end": "2711840"
  },
  {
    "text": "How much time do\nI have in total? There's still plenty of time.",
    "start": "2711840",
    "end": "2717020"
  },
  {
    "text": "OK, OK. I think. All right. All right. Other questions? Yeah, other documents\nin the training",
    "start": "2717020",
    "end": "2724960"
  },
  {
    "text": "set same as [INAUDIBLE] when you\nchange the setup [INAUDIBLE]?? Yeah, so they can be different.",
    "start": "2724960",
    "end": "2732820"
  },
  {
    "text": "So in Atlas, Atlas\nbasically tries everything. So they also try to\nsee, what happens",
    "start": "2732820",
    "end": "2738280"
  },
  {
    "text": "if I train this on Wikipedia,\nbut I swap in a common crawl index?",
    "start": "2738280",
    "end": "2744400"
  },
  {
    "text": "And I think-- so in Atlas but\nalso in RETRO, the main finding is just the more, the better.",
    "start": "2744400",
    "end": "2750700"
  },
  {
    "text": "So it's really just like\nthe bigger your index, the more likely you are to\nfind the exact right thing",
    "start": "2750700",
    "end": "2757539"
  },
  {
    "text": "and then make the\nright prediction. ",
    "start": "2757540",
    "end": "2765100"
  },
  {
    "text": "Any other questions on this? Oh, yeah. Sorry, this is a question about\nthe generator in, I guess,",
    "start": "2765100",
    "end": "2772360"
  },
  {
    "text": "the RAG system. So recently, I saw a\npaper on Mistral 7B.",
    "start": "2772360",
    "end": "2778900"
  },
  {
    "text": "So it introduces a lot of\nthese new architectural changes like the sliding\nwindow attention",
    "start": "2778900",
    "end": "2784540"
  },
  {
    "text": "to handle longer sequences at a\nsmaller cost and the group query attention for faster inference.",
    "start": "2784540",
    "end": "2790600"
  },
  {
    "text": "I'd like to know your thoughts\non designing a generator specifically for RAG\nleveraging, for example,",
    "start": "2790600",
    "end": "2798430"
  },
  {
    "text": "where Mistral 7B currently\nis because, for example, the sliding window attention,\nI could see how that could be",
    "start": "2798430",
    "end": "2804820"
  },
  {
    "text": "adapted to the RAG case. Yeah, so maybe your read on\nwhat makes Mistrals special",
    "start": "2804820",
    "end": "2811672"
  },
  {
    "text": "is a bit different from mine. So I don't think that the\nsliding attention window thing is actually that interesting.",
    "start": "2811672",
    "end": "2816780"
  },
  {
    "text": "The reason Mistral works\nso well is because it's trained on a lot of data. And you can do that more\nefficiently because you",
    "start": "2816780",
    "end": "2822450"
  },
  {
    "text": "have sliding window attention. So you don't need to\nattend to everything. ",
    "start": "2822450",
    "end": "2828330"
  },
  {
    "text": "So to answer your question-- I guess you're asking about the\narchitecture of the generator",
    "start": "2828330",
    "end": "2834150"
  },
  {
    "text": "if you know that there's\ngoing to be a retriever. So I think that's basically\nwhat RETRO tried to do.",
    "start": "2834150",
    "end": "2842490"
  },
  {
    "text": "Actually, some of the\npeople on the RETRO paper are at Mistral now. So they have this chunk\ncross-attention idea here.",
    "start": "2842490",
    "end": "2852119"
  },
  {
    "text": "So you basically have\na language model, but the way it does\nattention over the things you retrieve in your\nRETRO architecture.",
    "start": "2852120",
    "end": "2861450"
  },
  {
    "text": "They get integrated\ninto a model, not using the standard\nattention mechanism,",
    "start": "2861450",
    "end": "2866820"
  },
  {
    "text": "but using this slightly\ndifferent chunk cross-attention. OK, so I think the\nsliding window attention",
    "start": "2866820",
    "end": "2874400"
  },
  {
    "text": "point I was trying to get at was\nthat it uses a fixed window so that whenever you're doing\nthe query key computation",
    "start": "2874400",
    "end": "2883130"
  },
  {
    "text": "with the query vectors\nand the key vectors, you're using a fixed\nwindow attention. So I think my idea\nwas to actually, one,",
    "start": "2883130",
    "end": "2892160"
  },
  {
    "text": "use a dynamic window. Because for example,\nthe RAG case, if you use a fixed window\nwhen you're doing attention,",
    "start": "2892160",
    "end": "2899930"
  },
  {
    "text": "it is possible that you\nactually are leaving-- you're only looking at a\nfixed span of information.",
    "start": "2899930",
    "end": "2906859"
  },
  {
    "text": "So if you could\nmaybe adapt Mistral so that you could make it\nbetter for the RAG case",
    "start": "2906860",
    "end": "2912230"
  },
  {
    "text": "in, for example,\nmaking the fixed window size the dynamic window.",
    "start": "2912230",
    "end": "2917330"
  },
  {
    "text": "Yeah. Yeah, I think it's\nan interesting idea. So for me, what Mistral is\ndoing with the sliding window,",
    "start": "2917330",
    "end": "2925340"
  },
  {
    "text": "that's basically like a ConvNet. So we had all these\nconvolutional-- light ConvNets,",
    "start": "2925340",
    "end": "2930840"
  },
  {
    "text": "where we would have\nword embeddings, and you would do convolutions\nover it and then pull. And then you would still\nget the information out.",
    "start": "2930840",
    "end": "2937380"
  },
  {
    "text": "So it's not that the\nsliding window prohibits you from looking earlier. It's just that happens higher\nup in your transformer, sort of.",
    "start": "2937380",
    "end": "2945450"
  },
  {
    "text": "Yeah, yeah. OK. So I think that definitely\nis an interesting direction",
    "start": "2945450",
    "end": "2951350"
  },
  {
    "text": "to think in, yeah. Yeah, so I think it's\nnot too crazy to say,",
    "start": "2951350",
    "end": "2957020"
  },
  {
    "text": "are there any\narchitectural changes that we can introduce into\nthis 7 billion parameter models",
    "start": "2957020",
    "end": "2962420"
  },
  {
    "text": "so that they could be better\nadapted to the RAG case? Yeah, so there might be.",
    "start": "2962420",
    "end": "2969630"
  },
  {
    "text": "Yeah, I think one question\nis just, How do you-- how do you do the attention\nover things you've retrieved?",
    "start": "2969630",
    "end": "2975210"
  },
  {
    "text": "which I think is what you're-- Yeah, thanks.",
    "start": "2975210",
    "end": "2980670"
  },
  {
    "text": "So just to make\nsure I understand. So I mean, in this RETRO model,\nyou are retrieving each block.",
    "start": "2980670",
    "end": "2988859"
  },
  {
    "text": "And when you talk about putting\nthe retrieval in the context, are you saying that you\nonly do it at the beginning,",
    "start": "2988860",
    "end": "2994980"
  },
  {
    "text": "and you don't do\nit at each block? Yeah, so in context-- so this is-- it's not\nexactly every layer, sort of.",
    "start": "2994980",
    "end": "3001760"
  },
  {
    "text": "So it's every token. So every step, basically,\nnot every block.",
    "start": "3001760",
    "end": "3007490"
  },
  {
    "text": "So it doesn't make sense. So it's not every layer\nthat you do the retrieval.",
    "start": "3007490",
    "end": "3014330"
  },
  {
    "text": "Yeah, so every step. So this is kind of\nlike what RAG token is.",
    "start": "3014330",
    "end": "3020290"
  },
  {
    "text": "So you retrieve every token. So you generate, and then\nyou can retrieve again.",
    "start": "3020290",
    "end": "3025690"
  },
  {
    "text": "Or in the case of retro,\nyou, can generate a chunk, and then you retrieve\nchunks again.",
    "start": "3025690",
    "end": "3030910"
  },
  {
    "text": "If you look at the\nin-context case, you retrieve once at the\nbeginning, and then you give it.",
    "start": "3030910",
    "end": "3036790"
  },
  {
    "text": "So [INAUDIBLE],,\nyou're saying that doing this multiple\nretrieval, [INAUDIBLE]..",
    "start": "3036790",
    "end": "3042310"
  },
  {
    "text": "Yeah, so the in-context thing-- so here, you don't actually\ngive it as context at all",
    "start": "3042310",
    "end": "3050650"
  },
  {
    "text": "directly to the model. So here, you let the\ndecoder attend over it.",
    "start": "3050650",
    "end": "3056230"
  },
  {
    "text": "Like computation. Yeah. And that nobody [INAUDIBLE].",
    "start": "3056230",
    "end": "3063530"
  },
  {
    "text": "So I don't think cross-attention\nreally works, yeah. ",
    "start": "3063530",
    "end": "3071930"
  },
  {
    "text": "Other questions? Deep inside that in some cases,\nthe training of the retriever",
    "start": "3071930",
    "end": "3078300"
  },
  {
    "text": "is not so necessary\nbecause a large lost. So I'm wondering your\ninsights of the cases.",
    "start": "3078300",
    "end": "3085590"
  },
  {
    "text": "Like what cases are\nreally necessarily need to asynchronous update or\nany updates or documents?",
    "start": "3085590",
    "end": "3094930"
  },
  {
    "text": "Yeah. So you do want to update\nthe retriever, right? But only part of\nthe retriever is necessary to be updated\nfor a lot of these cases.",
    "start": "3094930",
    "end": "3104520"
  },
  {
    "text": "But so I think it-- so these are very\nspecific data sets, right?",
    "start": "3104520",
    "end": "3109680"
  },
  {
    "text": "Natural questions, wizard\nof Wikipedia and fever, so they're really very\nknowledge-intensive tasks.",
    "start": "3109680",
    "end": "3116700"
  },
  {
    "text": "So in that case, if you already\nhave a very good system like DPR that is specifically\npre-trained for those tasks,",
    "start": "3116700",
    "end": "3123690"
  },
  {
    "text": "then you only need to\nupdate the query encoder. So I would expect\nthat if you move",
    "start": "3123690",
    "end": "3128730"
  },
  {
    "text": "beyond this to general language\nmodeling things like retro then you probably do want\nto update the document",
    "start": "3128730",
    "end": "3135310"
  },
  {
    "text": "encoder at least in a way\nwhere you can scale it. So everything that's in this\ntask is very knowledge-intensive",
    "start": "3135310",
    "end": "3146160"
  },
  {
    "text": "and actually decouples\nthe [INAUDIBLE] and the generator\n[INAUDIBLE] as long",
    "start": "3146160",
    "end": "3154049"
  },
  {
    "text": "as we have a good\nknowledge of the documents",
    "start": "3154050",
    "end": "3159570"
  },
  {
    "text": "by those good\nmodels [INAUDIBLE].. Yeah. But so you need to learn how\nto query into that index.",
    "start": "3159570",
    "end": "3168300"
  },
  {
    "text": "So if you don't do\nthat, then yeah, you don't get really\ngood performance.",
    "start": "3168300",
    "end": "3173970"
  },
  {
    "text": "So that's like your closed\nbook performance, right? If you just have the language\nmodel and you're just like,",
    "start": "3173970",
    "end": "3179700"
  },
  {
    "text": "what does the parametric\nmodel on its own without the retrieval? What does it actually know?",
    "start": "3179700",
    "end": "3184890"
  },
  {
    "text": "As you can see, they're\npretty big gaps there. ",
    "start": "3184890",
    "end": "3192619"
  },
  {
    "text": "Other questions? Otherwise I will\ncover other questions. ",
    "start": "3192620",
    "end": "3198900"
  },
  {
    "text": "No? Hello? Yeah. Go for it. Oh. Quick question. So what about more\ntheoretical retrieval?",
    "start": "3198900",
    "end": "3205980"
  },
  {
    "text": "Like I suppose\nthere'll be message trying to not just\nretrieve a single chunk but some groups of\nchunks or something",
    "start": "3205980",
    "end": "3212430"
  },
  {
    "text": "or summarized versions. There's been some interesting\nwork on doing that where",
    "start": "3212430",
    "end": "3217470"
  },
  {
    "text": "you first try to find. So you can have multiple\nindices and they can cascade. So first, you want to find\nthe relevant document.",
    "start": "3217470",
    "end": "3223710"
  },
  {
    "text": "So you have some\ndocument representation. And then within\nthat document, you want to find the relevant chunk,\nso you can do that direction.",
    "start": "3223710",
    "end": "3231330"
  },
  {
    "text": "You can also do it in reverse. I think I have\nsomething on the slide there where you\ncan find the chunk and then expand the\ncontext around it",
    "start": "3231330",
    "end": "3239579"
  },
  {
    "text": "and then give that to\nthe language model. And so I think-- yeah, there are all kinds\nof interesting things",
    "start": "3239580",
    "end": "3245580"
  },
  {
    "text": "you can do there. Cool. Thanks. I guess another thing,\njust like can you",
    "start": "3245580",
    "end": "3252220"
  },
  {
    "text": "compare RAG versus like\nlong context efforts. So there are lots of things\nlike around just having",
    "start": "3252220",
    "end": "3258880"
  },
  {
    "text": "really long context and the\nextreme, it could replace RAG. But I don't know, like\ngive me your takes.",
    "start": "3258880",
    "end": "3264339"
  },
  {
    "text": "Yeah. So everybody understands\nthis question, right? So there's a trend where we\nwant to have very long context",
    "start": "3264340",
    "end": "3272470"
  },
  {
    "text": "language models so\nthat basically you can take Harry Potter\nor something just put it in the context and then\nask a question like what",
    "start": "3272470",
    "end": "3279160"
  },
  {
    "text": "is the name of like Harry\nPotter's owl or something. And then it can just attend\nover the entire thing.",
    "start": "3279160",
    "end": "3285280"
  },
  {
    "text": "So attending over\nall of Harry Potter to answer that one question\nis super inefficient, right?",
    "start": "3285280",
    "end": "3291759"
  },
  {
    "text": "So most of Harry Potter has\nnothing to do with the owl. So but you are still\nreading it if you do it",
    "start": "3291760",
    "end": "3297789"
  },
  {
    "text": "with the long context window. So that's why I think doing\nit the right way where you have this nonparametric\ncomponent is a much more",
    "start": "3297790",
    "end": "3305710"
  },
  {
    "text": "efficient way to\nsolve this problem. And if you actually look at\nthe literature on long context windows, the way they solve\nthe problem of scaling",
    "start": "3305710",
    "end": "3314900"
  },
  {
    "text": "the attention mechanism is\nby making it very sparse. So they're basically\nturning it--",
    "start": "3314900",
    "end": "3320348"
  },
  {
    "text": "so that's a different\nkind of sparse, but they're turning it into a\nnon-parametric retrieval problem",
    "start": "3320348",
    "end": "3325849"
  },
  {
    "text": "behind-the-scenes. They're not actually\nall that different. If you want to\nscale long context, then you're going to\nmove towards a RAG style",
    "start": "3325850",
    "end": "3332600"
  },
  {
    "text": "architecture. Good. Thanks. ",
    "start": "3332600",
    "end": "3339130"
  },
  {
    "text": "All right. So let's talk about some\nother interesting questions. So one thing and\nI already alluded",
    "start": "3339130",
    "end": "3345970"
  },
  {
    "text": "to this is when do\nwe actually retrieve? So if we're doing-- if we\nwant to retrieve every token,",
    "start": "3345970",
    "end": "3353590"
  },
  {
    "text": "that's also very inefficient\nbecause I probably don't have to retrieve to generate the--",
    "start": "3353590",
    "end": "3359109"
  },
  {
    "text": "right? I can probably do that on my\nown with a language model. It's a waste to go\nand retrieve stuff.",
    "start": "3359110",
    "end": "3364480"
  },
  {
    "text": "But if I only retrieve once at\nthe beginning of the sequence, that's probably also not great.",
    "start": "3364480",
    "end": "3370200"
  },
  {
    "text": "So what we ideally want to\nbe able to do is to say, OK, sometimes, I\nwant to retrieve, sometimes, I don't\nwant to retrieve.",
    "start": "3370200",
    "end": "3375840"
  },
  {
    "text": "And I'm going to\nlearn when I want to expand the compute budget\non doing the retrieval.",
    "start": "3375840",
    "end": "3382560"
  },
  {
    "text": "So a nice paper where\nthey have a stab at this is called FLARE for active\nretrieval augmentation",
    "start": "3382560",
    "end": "3388290"
  },
  {
    "text": "where they basically have\nthe language model decide when it should do a search and\nwhat it should do to search for.",
    "start": "3388290",
    "end": "3396990"
  },
  {
    "text": "So I think this fits\nin a general trend that you can see in the\nfield around agents.",
    "start": "3396990",
    "end": "3402540"
  },
  {
    "text": "So we can talk a little\nbit more about that too. So this other\nquestion that I think",
    "start": "3402540",
    "end": "3408869"
  },
  {
    "text": "we also covered already here is\nhow do we train this at scale? So we can do these\nasynchronous updates.",
    "start": "3408870",
    "end": "3414210"
  },
  {
    "text": "We can do rerankers. We can do query side only. There's this really\nnice paper which",
    "start": "3414210",
    "end": "3419550"
  },
  {
    "text": "is quite close I think to\nthe idea you proposed where you first use BM25 to create a\nbatch basically where everything",
    "start": "3419550",
    "end": "3428190"
  },
  {
    "text": "is very similar in terms\nof what you retrieved and now you have\nthis in-batch update.",
    "start": "3428190",
    "end": "3435960"
  },
  {
    "text": "So it's like a reranker where\nyou encode the information that is just in your batch\nusing this other model",
    "start": "3435960",
    "end": "3441869"
  },
  {
    "text": "and now you can update\nthis model on the fly, so you don't have to\nworry too much about doing the full document side update.",
    "start": "3441870",
    "end": "3449430"
  },
  {
    "text": "And again, here,\nwhat really matters is like how big is your index. If you have an\namazing index, you can basically solve any\nproblem just by looking it up.",
    "start": "3449430",
    "end": "3457470"
  },
  {
    "text": "So rather than cramming\nit into your parameters, you can just find it. ",
    "start": "3457470",
    "end": "3463960"
  },
  {
    "text": "This is a really nice\npaper called SILO. So one of the\ninteresting things I",
    "start": "3463960",
    "end": "3469599"
  },
  {
    "text": "think that's going to happen\nin the next year or two around language models is there,\nand you've seen this already",
    "start": "3469600",
    "end": "3475090"
  },
  {
    "text": "there's a bunch of\nlawsuits against OpenAI and other places around where\ndoes the data exactly come from.",
    "start": "3475090",
    "end": "3481839"
  },
  {
    "text": "So one very elegant\nsolution I think is to have a system\nthat you train",
    "start": "3481840",
    "end": "3487000"
  },
  {
    "text": "on data that is safe so you can\ntrain that thing on Wikipedia. But now during test\ntime, you can give it",
    "start": "3487000",
    "end": "3494200"
  },
  {
    "text": "a data store that has\nmaybe slightly riskier information in it so\nthis massive index",
    "start": "3494200",
    "end": "3500049"
  },
  {
    "text": "of all the stuff on the internet\nincluding some things that are maybe higher risk.",
    "start": "3500050",
    "end": "3506020"
  },
  {
    "text": "You can still have them in your\nindex but your language model, your retrieval augmented\nlanguage model, I should say,",
    "start": "3506020",
    "end": "3511918"
  },
  {
    "text": "you know that that thing is\nsafe because it was trained on data that is public domain. So that's what they do\nin SILO and they show",
    "start": "3511918",
    "end": "3518230"
  },
  {
    "text": "that that works really well. So that's one possible solution\nto a lot of the compliance",
    "start": "3518230",
    "end": "3525049"
  },
  {
    "text": "and legal risk around\nlanguage model deployments. There's a great paper\non also from one",
    "start": "3525050",
    "end": "3533080"
  },
  {
    "text": "of your colleagues\naround context getting lost in the middle. I think this is also a\nfascinating phenomenon.",
    "start": "3533080",
    "end": "3539680"
  },
  {
    "text": "This is on a frozen RAG system. But language models are very\nsimilar to humans in what things",
    "start": "3539680",
    "end": "3547840"
  },
  {
    "text": "they pay attention to. So if you give them a bunch\nof things that you retrieve, what they will look at are\nthe first things you list",
    "start": "3547840",
    "end": "3555070"
  },
  {
    "text": "and the last things you list. And they will ignore the middle. And so if it actually\nrespected the rank function,",
    "start": "3555070",
    "end": "3561430"
  },
  {
    "text": "then this curve would go\ndown all the way, right? But it goes up. So I think that's a very\ninteresting observation which",
    "start": "3561430",
    "end": "3570460"
  },
  {
    "text": "shows that how brittle\nthese systems can be. So if you have a\nfrozen RAG system,",
    "start": "3570460",
    "end": "3576370"
  },
  {
    "text": "it can be very,\nvery brittle where like the order of\nthe retrieve context matters a lot in whether you\nget the right answer or not.",
    "start": "3576370",
    "end": "3584582"
  },
  {
    "text": "Can you then work on reading\nthis as a reinforcement learning problem in a sense\nof place and vector",
    "start": "3584582",
    "end": "3590789"
  },
  {
    "text": "as opposed to specifically\ngoing for interpretation, try to output a vector\nthat's going to inner product",
    "start": "3590790",
    "end": "3597360"
  },
  {
    "text": "with just the right-- maybe you can tune for\na particular database?",
    "start": "3597360",
    "end": "3602490"
  },
  {
    "text": "Yeah. I don't know exactly. Yeah. So what I just described\nsomebody asked,",
    "start": "3602490",
    "end": "3607799"
  },
  {
    "text": "like how do you\nactually-- so I said there are other ways to do this\nand then the question was how do you do that?",
    "start": "3607800",
    "end": "3612850"
  },
  {
    "text": "So the way you do that\nis using reinforce. So yeah, there has been\nwork on doing that.",
    "start": "3612850",
    "end": "3619650"
  },
  {
    "text": "So some of the older papers\nwere playing with this. But one of the big\nproblems with--",
    "start": "3619650",
    "end": "3625290"
  },
  {
    "text": "so I think the\nREPLUG solution is more elegant for\nsolving that problem",
    "start": "3625290",
    "end": "3631049"
  },
  {
    "text": "because you actually use\nsignal from the language model. And if you just do reinforce,\nit's very high variance.",
    "start": "3631050",
    "end": "3636420"
  },
  {
    "text": "So you're it's going to be\nsuper finicky if you don't want to destroy your index.",
    "start": "3636420",
    "end": "3643250"
  },
  {
    "text": "But people have tried it, yeah. ",
    "start": "3643250",
    "end": "3648740"
  },
  {
    "text": "So there are some really\nnice work from OpenAI where they basically show--",
    "start": "3648740",
    "end": "3655460"
  },
  {
    "text": "and again, we're like thinking\nmore and more about agents here, right? Where they show something very\nsimilar to the FLARE results",
    "start": "3655460",
    "end": "3662870"
  },
  {
    "text": "from earlier with\nactive retrieval that doesn't necessarily have\nto be some index that you own. It can be just some\nweb search, right?",
    "start": "3662870",
    "end": "3670487"
  },
  {
    "text": "And obviously in this\ncase, you don't really have access to the web\nsearch necessarily, so Bing or whatever they use here is not\ngoing to update its parameters.",
    "start": "3670487",
    "end": "3678440"
  },
  {
    "text": "But I just wanted to\nput this in your mind, like this is another\nthing you can do, right?",
    "start": "3678440",
    "end": "3683900"
  },
  {
    "text": "And if we take this really\nto the general form, then you can think of language\nmodels as just tool users.",
    "start": "3683900",
    "end": "3690920"
  },
  {
    "text": "So rather than just retrieval\naugmenting language models, we can tool augment language\nmodels and retrieval is just",
    "start": "3690920",
    "end": "3697190"
  },
  {
    "text": "one of the many tools that\nlanguage models have access to. We can have rerankers\nand things on top",
    "start": "3697190",
    "end": "3702950"
  },
  {
    "text": "of the outputs of these tools. And so one of the big\nquestions, I think, is, how do you actually get this\nsystem to learn stuff, right?",
    "start": "3702950",
    "end": "3711620"
  },
  {
    "text": "So we're going to need RL if we\nwant this system to really learn how to take these\nactions properly. ",
    "start": "3711620",
    "end": "3719780"
  },
  {
    "text": "And so yeah, this has\nbeen taken to the extreme and is self RAG architecture\nwhere they have this retrieval",
    "start": "3719780",
    "end": "3727400"
  },
  {
    "text": "step and it's active and\nthen you criticize it and then you basically do some\nnatural language inference",
    "start": "3727400",
    "end": "3733280"
  },
  {
    "text": "and all of that just\nwith one language model to answer the questions.",
    "start": "3733280",
    "end": "3738630"
  },
  {
    "text": "So the other missing\npiece-- so I'm just going through a bunch\nof open questions that people have\nlooked at but feel",
    "start": "3738630",
    "end": "3746269"
  },
  {
    "text": "free to interrupt me if there's\nanything you want to know. But so instruction\ntuning, we established",
    "start": "3746270",
    "end": "3751852"
  },
  {
    "text": "at the beginning of\nthe lecture that this is pretty important for\ngetting things to work, so fixing the user interface.",
    "start": "3751852",
    "end": "3758240"
  },
  {
    "text": "But the instruction\ntuning has almost always only happened on\nthe language model",
    "start": "3758240",
    "end": "3763250"
  },
  {
    "text": "and not on the entire system. So I think one of the\ninteresting things that people are looking at\nnow with things like raw data",
    "start": "3763250",
    "end": "3769970"
  },
  {
    "text": "and InstructRetro is how can\nwe instruct and fine tune an entire retrieval\naugmented system. So all the way into\nthe retrieval step",
    "start": "3769970",
    "end": "3777079"
  },
  {
    "text": "can we generate\ndata so that also follows the\ninstructions properly which currently doesn't\nhappen in any of these model",
    "start": "3777080",
    "end": "3783119"
  },
  {
    "text": "architectures. And then finally, I\nthink I would be remiss",
    "start": "3783120",
    "end": "3788430"
  },
  {
    "text": "if I didn't really talk about\nwhat people call advanced RAGs. So like the developer\ncommunity has been really doing",
    "start": "3788430",
    "end": "3794910"
  },
  {
    "text": "some awesome stuff,\nso like frameworks, like LlamaIndex, and LangChain. And there's all these open\nsource vector databases",
    "start": "3794910",
    "end": "3801870"
  },
  {
    "text": "like Chroma and Weaviate. And they're all about\nmaking RAGs really easy. But this is all\nfrozen RAG, right?",
    "start": "3801870",
    "end": "3808590"
  },
  {
    "text": "But even with frozen RAG, you\ncan really do incredible things. So we mentioned some\nof these already.",
    "start": "3808590",
    "end": "3814799"
  },
  {
    "text": "So child-parent\nrecursive retriever. So you find small parts and then\nyou give the big parts around it",
    "start": "3814800",
    "end": "3820260"
  },
  {
    "text": "to the language model. You can do hybrid search where\nwe use reciprocal rank fusion, so we have different\nsearch results",
    "start": "3820260",
    "end": "3826680"
  },
  {
    "text": "that we didn't combine before\nwe give the final thing to the language model. There's zero shot like large\nlanguage model reranker,",
    "start": "3826680",
    "end": "3833820"
  },
  {
    "text": "so basically the\nscore function is not-- doesn't come\nfrom your retrieval, it comes directly from\nthe language model.",
    "start": "3833820",
    "end": "3839880"
  },
  {
    "text": "And then hypothetical document\nembeddings which I think is a really cool idea. So you just basically\nyou fix hallucination",
    "start": "3839880",
    "end": "3847860"
  },
  {
    "text": "through hallucination. So you get a\nquestion, then you let the language model hallucinate\na bunch of possible answers,",
    "start": "3847860",
    "end": "3854250"
  },
  {
    "text": "then you go and search\nfor nearest neighbors to the possible answers, and\nyou give those as context",
    "start": "3854250",
    "end": "3859290"
  },
  {
    "text": "and then it gives the\nright answer based on that. So it was really like\nhallucinating answers.",
    "start": "3859290",
    "end": "3864300"
  },
  {
    "text": "And I think it's a\nbrilliant solution. So there's a lot\nof stuff happening",
    "start": "3864300",
    "end": "3869369"
  },
  {
    "text": "in the frozen RAG\ncommunity too that I think is very interesting to look at.",
    "start": "3869370",
    "end": "3875579"
  },
  {
    "text": "So just to wrap up, looking\nat the future of this stuff.",
    "start": "3875580",
    "end": "3881490"
  },
  {
    "text": "There are still lots of very\ninteresting open questions. So if you're a student thinking\nabout how to solve any of these,",
    "start": "3881490",
    "end": "3888270"
  },
  {
    "text": "I think you can have\nquite a lot of impact. So how exactly do we\ndo like pre-training",
    "start": "3888270",
    "end": "3895050"
  },
  {
    "text": "of this architecture? And do we even\nneed to pre-train? I think even retro shows\nthat you don't necessarily",
    "start": "3895050",
    "end": "3900535"
  },
  {
    "text": "have to pre-train,\nso but maybe there's something wrong\nwith how we do that. What do scaling laws look like?",
    "start": "3900535",
    "end": "3907240"
  },
  {
    "text": "So I think there's a really\ninteresting question here around if I have a huge index and\na very rich encoder of all",
    "start": "3907240",
    "end": "3913000"
  },
  {
    "text": "the information in that\nindex, maybe I can move, so basically decouple all the\nmemorization to this index.",
    "start": "3913000",
    "end": "3919510"
  },
  {
    "text": "So I have a language model\nthat doesn't know anything, it just speaks English, it\njust reasons on top of it.",
    "start": "3919510",
    "end": "3924940"
  },
  {
    "text": "It has no knowledge\nbecause that always comes from this retriever. If you can do\nsomething like that, then you get very interesting\nscaling trade offs, right?",
    "start": "3924940",
    "end": "3931833"
  },
  {
    "text": "So you can have a\ntiny language model and do your retrieval to do\na lot of the heavy lifting",
    "start": "3931833",
    "end": "3937570"
  },
  {
    "text": "with your retrieval which\nis nice because that's a cached computation, right? So you can just-- you\nalready have the embeddings,",
    "start": "3937570",
    "end": "3943900"
  },
  {
    "text": "you just need to\ndo the dot product, so it's much more efficient than\nself-attention in the language",
    "start": "3943900",
    "end": "3949119"
  },
  {
    "text": "model. Can we move beyond by encoders? So vector databases,\nI like people",
    "start": "3949120",
    "end": "3956410"
  },
  {
    "text": "who build vector\ndatabases, but I'm not sure how long we're going to\nkeep vector databases.",
    "start": "3956410",
    "end": "3961660"
  },
  {
    "text": "Because I think reranker is\nprobably work just as well. And BM25 is much more efficient\nthan a vector database.",
    "start": "3961660",
    "end": "3970780"
  },
  {
    "text": "So I don't really see why we\nneed dedicated vector databases.",
    "start": "3970780",
    "end": "3976300"
  },
  {
    "text": "And so what we're\nseeing but maybe this is a bit of a critique of\nmaybe Silicon Valley investment",
    "start": "3976300",
    "end": "3982110"
  },
  {
    "text": "strategies and things\nlike that, but a lot of these vector\ndatabase companies",
    "start": "3982110",
    "end": "3987253"
  },
  {
    "text": "are basically becoming\ndatabase companies now. So they are adding\nall this spark stuff because the dense\nthing is not enough.",
    "start": "3987253",
    "end": "3993700"
  },
  {
    "text": "And as it turns out, there are\na lot of pretty good sparse databases out there already like\nPostgres and things like that.",
    "start": "3993700",
    "end": "4000600"
  },
  {
    "text": "And they're also all adding\nvectors to their databases so I think that's all going\nto coalesce into databases.",
    "start": "4000600",
    "end": "4009445"
  },
  {
    "text": "[INAUDIBLE] So I think there are\nsome interesting things",
    "start": "4009445",
    "end": "4014760"
  },
  {
    "text": "to look at for the data. So alluding to this\ninstruction problem, can we generate much better\ndata for training RAG systems",
    "start": "4014760",
    "end": "4023280"
  },
  {
    "text": "synthetically? And then I think there's\nthis massive open question around how we actually measure\nwhether the system is any good.",
    "start": "4023280",
    "end": "4029970"
  },
  {
    "text": "So right now, we just look\nat downstream performance which is OK.",
    "start": "4029970",
    "end": "4035434"
  },
  {
    "text": "But if you mess\nup the retrieval, it's very hard to measure. But how to measure\nwhether your retrieval is",
    "start": "4035435",
    "end": "4041940"
  },
  {
    "text": "right is also very\ndifficult. So there are some frameworks\nwhere they try to take like the harmonic\nmean of your retrieval",
    "start": "4041940",
    "end": "4047520"
  },
  {
    "text": "accuracy and your\nlanguage model accuracy. But I think those are also very\nshoddy because we don't really",
    "start": "4047520",
    "end": "4053069"
  },
  {
    "text": "have very good data\nsets to measure that on. So I think that's a very cool\nproblem to work on as well.",
    "start": "4053070",
    "end": "4060460"
  },
  {
    "text": "So the other problem\nthat I personally am always very excited\nabout is multi-modality.",
    "start": "4060460",
    "end": "4066900"
  },
  {
    "text": "And so why would we stop with\nRAG systems with just text? Right? So you can do the same\nthing with images.",
    "start": "4066900",
    "end": "4074339"
  },
  {
    "text": "You can augment language\nmodels with vision. So we did this\nwork on LENS where we have a language\nmodel enhanced",
    "start": "4074340",
    "end": "4080700"
  },
  {
    "text": "to see where you can just give\na computer vision pipeline just",
    "start": "4080700",
    "end": "4086040"
  },
  {
    "text": "like a retrieval pipeline and\ngive that to a frozen language model and pass it\nthrough the context and that system actually is\nan amazing visual question",
    "start": "4086040",
    "end": "4093420"
  },
  {
    "text": "answering system. It's close to state-of-the-art\nflamingo from DeepMind which is",
    "start": "4093420",
    "end": "4099253"
  },
  {
    "text": "also very hard to reproduce\nbecause there's no open source version of that.",
    "start": "4099253",
    "end": "4104880"
  },
  {
    "text": "So we've done some early\nwork on this in 2021 where we have this\ncross-modal retrieval",
    "start": "4104880",
    "end": "4110640"
  },
  {
    "text": "and there are some more recent\nwork out of fair where they also look at this. So I think that's\nreally like, if you",
    "start": "4110640",
    "end": "4116609"
  },
  {
    "text": "look at the trend in the field\nlike multi-modality with GPT 4 and things like that\nis really a hot topic.",
    "start": "4116609",
    "end": "4121949"
  },
  {
    "text": "So everything is going\nin that direction. So it's an interesting\nthing to think about.",
    "start": "4121950",
    "end": "4128549"
  },
  {
    "text": "So overall, I think\nit would be nice if everybody moves away from\nRAG 1.0 to frozen Frankenstein",
    "start": "4128550",
    "end": "4136369"
  },
  {
    "text": "RAG and moves towards this\nmuch more optimized version RAG 2.0, so it's really about\nsystems over models.",
    "start": "4136370",
    "end": "4143870"
  },
  {
    "text": "It's not just your language\nmodel and your retriever and they're separate. It's about thinking\nfrom a systems",
    "start": "4143870",
    "end": "4149389"
  },
  {
    "text": "perspective about the\nentire thing and the problem you're trying to solve. And so I think that really is\nthe way that in deep learning",
    "start": "4149390",
    "end": "4156410"
  },
  {
    "text": "things have always\nprogressed, where if you optimize the\nsystem end-to-end, that's always going to win out.",
    "start": "4156410",
    "end": "4161540"
  },
  {
    "text": "Like back in the day in\ncomputer vision or NLP, we have parsers and scene\nparsers and all this stuff,",
    "start": "4161540",
    "end": "4166639"
  },
  {
    "text": "and all of that just\ndoesn't exist anymore now because we optimize\nthe system end to end.",
    "start": "4166640",
    "end": "4172350"
  },
  {
    "text": "So that's what's going\nto happen here too. So if we take that to the\nextreme, like versus chunker",
    "start": "4172350",
    "end": "4177470"
  },
  {
    "text": "thing in your documents, like\ncutting it up into pieces, you could backprop into that. Why not?",
    "start": "4177470",
    "end": "4183509"
  },
  {
    "text": "Somebody should really do that. So I think like trading off\ncosts and quality and zero shot",
    "start": "4183510",
    "end": "4191040"
  },
  {
    "text": "domain generalization,\nthat's really where this stuff is going to come in. So language models, right\nnow, they're amazing.",
    "start": "4191040",
    "end": "4196320"
  },
  {
    "text": "But very often, they're way too\nexpensive for being deployed somewhere where you can\nactually make money from them",
    "start": "4196320",
    "end": "4201720"
  },
  {
    "text": "if you're in a company. So what you want to do is\nmake it much more efficient and have the right\ncost quality trade off.",
    "start": "4201720",
    "end": "4208170"
  },
  {
    "text": "And the easiest\nway I can think of is to do it through\nretrieval augmentation. But obviously, I'm very biased.",
    "start": "4208170",
    "end": "4215489"
  },
  {
    "text": "So yeah, that was\nall I had actually. So if you're interested\nin this, I'm at Stanford.",
    "start": "4215490",
    "end": "4220840"
  },
  {
    "text": "So I can work with you\non research projects on these topics or\nif you want, you can also join Contextual because\nwe work on this stuff every day.",
    "start": "4220840",
    "end": "4229480"
  },
  {
    "text": "Thank you. Well, sorry, I had a\nquestion from earlier.",
    "start": "4229480",
    "end": "4235760"
  },
  {
    "text": "Yeah. I think you said\nsomething really, really I think really super\nhelpful earlier about Mistral",
    "start": "4235760",
    "end": "4241670"
  },
  {
    "text": "7b. You talked about-- you compared\nthe sliding window attention to convolutional\nneural networks.",
    "start": "4241670",
    "end": "4246920"
  },
  {
    "text": "And I do see the\nparallel because with convolutional\nneural networks you have several layers of--\nseveral different layers",
    "start": "4246920",
    "end": "4252500"
  },
  {
    "text": "of convolutional layers. And the top\nconvolutional layers are able to see a larger\nreceptive field",
    "start": "4252500",
    "end": "4258350"
  },
  {
    "text": "in the bottom\nconvolutional layers. And with convolutional\nlayers, you're able to tune the filter\nsizes and the stride,",
    "start": "4258350",
    "end": "4267020"
  },
  {
    "text": "so you're able to see a\ndifferent receptive field. And I was wondering if you\ncould see that same innovation",
    "start": "4267020",
    "end": "4272510"
  },
  {
    "text": "in Mistral 7b by tuning-- because you have different\ntransformer layers, and each transformer\nlayer will have a span",
    "start": "4272510",
    "end": "4279228"
  },
  {
    "text": "over a different set of tokens. And if you can tune, I guess\nthe transformer architecture, the way you tune those\nconvolutional layers the filter",
    "start": "4279228",
    "end": "4286070"
  },
  {
    "text": "sizes the receptive\nfield, perhaps we can do some optimization\nin the transformer realm that we have already done\nin convolutional layers.",
    "start": "4286070",
    "end": "4293630"
  },
  {
    "text": "Yeah. I think that-- so\nthat's a good idea. There's a great paper\non light convolutions,",
    "start": "4293630",
    "end": "4299070"
  },
  {
    "text": "I think, from Michael\nAuli and David Grangier and a bunch of people,\nwhere it's basically--",
    "start": "4299070",
    "end": "4304890"
  },
  {
    "text": "this came out at exactly the\nsame time as the transformer, and the transformer is\nlikely more optimized",
    "start": "4304890",
    "end": "4310470"
  },
  {
    "text": "for GPU computation. But the convolutional model\nwas actually slightly better than the transformer.",
    "start": "4310470",
    "end": "4316349"
  },
  {
    "text": " It's definitely worth exploring. OK, cool, thanks.",
    "start": "4316350",
    "end": "4321570"
  },
  {
    "text": " The advantages of the\nreranker were implied.",
    "start": "4321570",
    "end": "4327990"
  },
  {
    "text": "But does that give us\na lot of the advantages of the semantic search? Or is the trade not that great?",
    "start": "4327990",
    "end": "4334580"
  },
  {
    "text": "Yeah. So it depends on the problem. I think what you\nprobably want to do is cast a wide net with BM25\nand then just narrow it down",
    "start": "4334580",
    "end": "4343539"
  },
  {
    "text": "with dense search. So you often see that\nas a two-stage process where the first one is noisy.",
    "start": "4343540",
    "end": "4349540"
  },
  {
    "text": "You can add noise\nactually to your retrieval and then you use the dense\none to filter it down. ",
    "start": "4349540",
    "end": "4356920"
  },
  {
    "text": "Yeah. Everyone's trying to\nmaybe adapt their models to own domain specific areas.",
    "start": "4356920",
    "end": "4365290"
  },
  {
    "text": "Like I think there are mainly\ntwo ways to approach it. One way is to use construction\nthrough many things",
    "start": "4365290",
    "end": "4371230"
  },
  {
    "text": "like learning way or fine\ntuning like tuning method. And another way is just the\nmain topic of this lecture",
    "start": "4371230",
    "end": "4377800"
  },
  {
    "text": "is using visual automatic way. So I'm wondering besides the low\ncost advantage of the retrieval",
    "start": "4377800",
    "end": "4385510"
  },
  {
    "text": "automatic way. Do you think that our\ncapacity or the quality of visual augmented way can be\nmatched with those two context",
    "start": "4385510",
    "end": "4394780"
  },
  {
    "text": "learning? Yeah. So I think actually what's going\nto happen is that all of this",
    "start": "4394780",
    "end": "4400119"
  },
  {
    "text": "will come together, right? So if you actually train things\nlike end-to-end RAG 2.0 style,",
    "start": "4400120",
    "end": "4406600"
  },
  {
    "text": "then you can also fine tune\nthat system on some use case end-to-end, right?",
    "start": "4406600",
    "end": "4411730"
  },
  {
    "text": "So why would you just take\nthe retrieval augmented system if you can also fine tune it\non the thing you care about.",
    "start": "4411730",
    "end": "4417883"
  },
  {
    "text": "So I think in the\nend, everybody's going to do all of those things. And then there's questions like\nhow do you do that efficiently,",
    "start": "4417883",
    "end": "4423380"
  },
  {
    "text": "so that's why you would use\nadapters or things like that. ",
    "start": "4423380",
    "end": "4429440"
  },
  {
    "text": "If there was another question.  Just about hardware,\nyou say it's",
    "start": "4429440",
    "end": "4435100"
  },
  {
    "text": "going to be a database thing. But what about retrieval\nhardware and you",
    "start": "4435100",
    "end": "4443920"
  },
  {
    "text": "smile because you've got so\nmuch of the running part.",
    "start": "4443920",
    "end": "4449489"
  },
  {
    "text": "But what about--\nbecause this is huge-- Yeah. Yeah. Trillions. So you have any ideas?",
    "start": "4449490",
    "end": "4455370"
  },
  {
    "text": "It's just a database problem-- So I don't know if I'm allowed\nto say this exactly actually. But so one of the biggest\nchip manufacturers",
    "start": "4455370",
    "end": "4465060"
  },
  {
    "text": "that recently-- their\nstock has done really well, they have some dedicated\nretrieval hardware coming out",
    "start": "4465060",
    "end": "4470910"
  },
  {
    "text": "I think soon or it\nmight already be out. So yeah. So yeah.",
    "start": "4470910",
    "end": "4476490"
  },
  {
    "text": "Very efficient dense retrieval\nis a very big business.",
    "start": "4476490",
    "end": "4481960"
  },
  {
    "start": "4481960",
    "end": "4487719"
  },
  {
    "text": "Other questions?  What we think\neverything solving RAG",
    "start": "4487720",
    "end": "4493910"
  },
  {
    "text": "that will solve\nthose [INAUDIBLE].. ",
    "start": "4493910",
    "end": "4498980"
  },
  {
    "text": "Yes. I think so if you take\nit to the extreme. So one of the big\nproblems right now is",
    "start": "4498980",
    "end": "4504290"
  },
  {
    "text": "that if you contextualize an\nexisting language model that already hallucinates,\nthen it's going",
    "start": "4504290",
    "end": "4510170"
  },
  {
    "text": "to be hard to get rid of\nthe hallucination, right? So if you do REPLUG on GPT 4,\nGPT 4 might still hallucinate.",
    "start": "4510170",
    "end": "4517660"
  },
  {
    "text": "So it could\nbasically just ignore all the stuff you've\nretrieved and just do whatever it wants anyway.",
    "start": "4517660",
    "end": "4522699"
  },
  {
    "text": "So that's one of the\nreasons why you want to train the system end-to-end. And if you take that to the\nextreme where like I said,",
    "start": "4522700",
    "end": "4528200"
  },
  {
    "text": "right? If you can just have the\nlanguage model only reasoning and speak so it knows\nEnglish and reasoning",
    "start": "4528200",
    "end": "4534250"
  },
  {
    "text": "but it has no knowledge which\nall comes from somewhere else, then you can't hallucinate.",
    "start": "4534250",
    "end": "4539530"
  },
  {
    "text": "So it's really all grounded\nin whatever is in your index. ",
    "start": "4539530",
    "end": "4548510"
  },
  {
    "text": "But so they're--\nabout hallucination, I'm frustrated that a lot\nof people in the field",
    "start": "4548510",
    "end": "4553580"
  },
  {
    "text": "misunderstand what\nhallucination even means. So a lot of people are\nconflating hallucination with correctness\nor incorrectness.",
    "start": "4553580",
    "end": "4560570"
  },
  {
    "text": "So they're like, oh, the model\nmade a mistake, it hallucinated. It's like, no, it\nmade a mistake. That's different\nfrom hallucination.",
    "start": "4560570",
    "end": "4566870"
  },
  {
    "text": "Hallucination, I think,\nis a very specific retrieved something\nso I have some sort of counterfactual ground truth.",
    "start": "4566870",
    "end": "4573650"
  },
  {
    "text": "And what I'm saying does not\ncorrespond to that ground truth.",
    "start": "4573650",
    "end": "4579260"
  },
  {
    "text": "And so yeah, I think there's a\nbunch of folks at Stanford also working on better\nmeasurement of hallucination",
    "start": "4579260",
    "end": "4586010"
  },
  {
    "text": "and definitions and\nthings like that. ",
    "start": "4586010",
    "end": "4591049"
  },
  {
    "text": "If I'm understanding correctly\nyour definition of hallucination only makes sense in a\ncontext with ground truth.",
    "start": "4591049",
    "end": "4597460"
  },
  {
    "text": "Yeah. Of some ground truth, right? So hallucination is\nreally like-- there is",
    "start": "4597460",
    "end": "4603430"
  },
  {
    "text": "something that is true, right? So if we're talking\nabout like hallucination, and yeah, so if we're talking\nabout just general parametric",
    "start": "4603430",
    "end": "4610420"
  },
  {
    "text": "language models then\nthe ground truth is whatever we\nconsider to be true. Right? ",
    "start": "4610420",
    "end": "4617980"
  },
  {
    "text": "But we had to work for\nlike language models making mistakes before it\nwas called making mistakes. ",
    "start": "4617980",
    "end": "4626950"
  },
  {
    "text": "Yeah, on the ground,\nI guess, you're solving the housemates question\n[INAUDIBLE] on that path.",
    "start": "4626950",
    "end": "4634900"
  },
  {
    "text": "Are you working on\nground truth per se in a document saying Obama\nI've never been a president",
    "start": "4634900",
    "end": "4641920"
  },
  {
    "text": "and everything falls apart. Are you saying work\non that on this trial?",
    "start": "4641920",
    "end": "4647560"
  },
  {
    "text": "Yeah. So I like the SILO\nmentioned there as well. So I think the whole\npoint is that you",
    "start": "4647560",
    "end": "4654849"
  },
  {
    "text": "can have different indices and\ndifferent definitions of ground truth. And so I think you could say\nI only trust archive or I",
    "start": "4654850",
    "end": "4662949"
  },
  {
    "text": "only trust like peer-reviewed\npapers and not just archive. So you can make decisions\nin your architecture",
    "start": "4662950",
    "end": "4669639"
  },
  {
    "text": "during test time about what\nyou define as ground truth. And I also think actually that\nand there's a bunch of work",
    "start": "4669640",
    "end": "4677630"
  },
  {
    "text": "I think happening\non this right now. You can control for\nhow grounded you want to be in your ground truth.",
    "start": "4677630",
    "end": "4683719"
  },
  {
    "text": "So that's another\nkind of misconception about hallucinations. Like sometimes hallucinations\nare actually good.",
    "start": "4683720",
    "end": "4690020"
  },
  {
    "text": "If you have a creative\nwriting assistant and you wanted to come up\nwith some cool new ideas, you want the language\nmodel to hallucinate.",
    "start": "4690020",
    "end": "4696679"
  },
  {
    "text": "So I think what you want to\nhave is a tunable knob where you say like now\nyou can hallucinate",
    "start": "4696680",
    "end": "4701703"
  },
  {
    "text": "and now maybe you should like\nreally tell me the truth on. ",
    "start": "4701703",
    "end": "4711760"
  },
  {
    "text": "Anything else? But as [INAUDIBLE] temperatures\ngain, some form of control--",
    "start": "4711760",
    "end": "4720460"
  },
  {
    "text": "Yeah. Hallucination. Yeah. So but the temperature,\nthat's just about how you sample, right?",
    "start": "4720460",
    "end": "4726120"
  },
  {
    "text": "So how flat your distribution,\nis that you sample from. [INAUDIBLE] Yeah.",
    "start": "4726120",
    "end": "4731280"
  },
  {
    "text": "[INAUDIBLE] Yes. So even if you have\na low temperature,",
    "start": "4731280",
    "end": "4736530"
  },
  {
    "text": "it can still come up\nwith random stuff, right? So it just says that then\nyou're very likely to do like greedy sampling.",
    "start": "4736530",
    "end": "4744090"
  },
  {
    "text": "So I think what you want\nto get at is something more sophisticated than that. ",
    "start": "4744090",
    "end": "4755040"
  },
  {
    "text": "Great. Lots of interesting question. Yeah. I like their question. Thanks, Douwe, again,\nfor the great talk. [INAUDIBLE]",
    "start": "4755040",
    "end": "4763000"
  },
  {
    "start": "4763000",
    "end": "4766000"
  }
]