[
  {
    "start": "0",
    "end": "14000"
  },
  {
    "start": "0",
    "end": "4058"
  },
  {
    "text": "CHRISTOPHER POTTS:\nWelcome, everyone.",
    "start": "4058",
    "end": "5600"
  },
  {
    "text": "This is Part 5 in our series\non methods and metrics.",
    "start": "5600",
    "end": "8080"
  },
  {
    "text": "We're going to be talking\nabout essential selected",
    "start": "8080",
    "end": "10240"
  },
  {
    "text": "topics in model\nevaluation in our field.",
    "start": "10240",
    "end": "14790"
  },
  {
    "start": "14000",
    "end": "157000"
  },
  {
    "text": "Here's our overview.",
    "start": "14790",
    "end": "15800"
  },
  {
    "text": "I'd like to start by talking\nabout baselines and their role",
    "start": "15800",
    "end": "19230"
  },
  {
    "text": "in experimental comparisons.",
    "start": "19230",
    "end": "21390"
  },
  {
    "text": "Then we'll discuss\nhyperparameter optimization,",
    "start": "21390",
    "end": "23520"
  },
  {
    "text": "both the process\nand the motivations,",
    "start": "23520",
    "end": "25740"
  },
  {
    "text": "as well as compromises\nthat you might",
    "start": "25740",
    "end": "28170"
  },
  {
    "text": "have to make due to\nresource constraints",
    "start": "28170",
    "end": "30000"
  },
  {
    "text": "and other constraints.",
    "start": "30000",
    "end": "31670"
  },
  {
    "text": "We'll touch briefly on\nclassifier comparison,",
    "start": "31670",
    "end": "33820"
  },
  {
    "text": "which is a topic we covered in\nthe sentiment analysis unit.",
    "start": "33820",
    "end": "37320"
  },
  {
    "text": "And then we'll close with\ntwo topics that are really",
    "start": "37320",
    "end": "39540"
  },
  {
    "text": "pressing for deep learning\nmodels, which is assessing",
    "start": "39540",
    "end": "42930"
  },
  {
    "text": "models without convergence and\nthe role of random parameter",
    "start": "42930",
    "end": "45990"
  },
  {
    "text": "initialization in shaping\nexperimental results.",
    "start": "45990",
    "end": "48930"
  },
  {
    "start": "48930",
    "end": "51480"
  },
  {
    "text": "So let's begin with baselines.",
    "start": "51480",
    "end": "54020"
  },
  {
    "text": "The fundamental insight\nhere is that in our field",
    "start": "54020",
    "end": "56660"
  },
  {
    "text": "evaluation numbers can\nnever be understood properly",
    "start": "56660",
    "end": "59540"
  },
  {
    "text": "in isolation.",
    "start": "59540",
    "end": "60380"
  },
  {
    "text": "Let's consider\ntwo extreme cases.",
    "start": "60380",
    "end": "62670"
  },
  {
    "text": "Suppose your system\ngets 0.95 F1,",
    "start": "62670",
    "end": "65150"
  },
  {
    "text": "then you might feel like you can\ndeclare victory at that point.",
    "start": "65150",
    "end": "68210"
  },
  {
    "text": "But it will be\nnatural for people",
    "start": "68210",
    "end": "70460"
  },
  {
    "text": "who are consuming your\nresults to ask, well,",
    "start": "70460",
    "end": "73140"
  },
  {
    "text": "is the task too easy?",
    "start": "73140",
    "end": "74510"
  },
  {
    "text": "Is it really an achievement\nthat you've got 0.95?",
    "start": "74510",
    "end": "77360"
  },
  {
    "text": "Or would even\nsimpler systems have",
    "start": "77360",
    "end": "79790"
  },
  {
    "text": "achieved something similar?",
    "start": "79790",
    "end": "81595"
  },
  {
    "text": "At the other end\nof the spectrum,",
    "start": "81595",
    "end": "82970"
  },
  {
    "text": "suppose your system\ngets 0.6 F1, you",
    "start": "82970",
    "end": "85945"
  },
  {
    "text": "might think that means you\nhaven't gotten traction.",
    "start": "85945",
    "end": "88070"
  },
  {
    "text": "But we should ask two questions.",
    "start": "88070",
    "end": "89520"
  },
  {
    "text": "First, what do humans get\nas a kind of upper bound?",
    "start": "89520",
    "end": "92869"
  },
  {
    "text": "And also, what would a\nrandom classifier get?",
    "start": "92870",
    "end": "95090"
  },
  {
    "text": "And if your 0.6 is\nreally different",
    "start": "95090",
    "end": "96979"
  },
  {
    "text": "from the random classifier\nand human performance",
    "start": "96980",
    "end": "99740"
  },
  {
    "text": "is kind of low, we might\nthen see that this 0.6",
    "start": "99740",
    "end": "102680"
  },
  {
    "text": "F1 is a real achievement.",
    "start": "102680",
    "end": "105875"
  },
  {
    "text": "Now it kind of shows\nyou that baselines",
    "start": "105875",
    "end": "107500"
  },
  {
    "text": "are just crucial for strong\nexperiments in our field.",
    "start": "107500",
    "end": "110690"
  },
  {
    "text": "So defining baselines\nfor you should not",
    "start": "110690",
    "end": "112390"
  },
  {
    "text": "be an afterthought, but\nrather central to how you",
    "start": "112390",
    "end": "114790"
  },
  {
    "text": "define your overall hypothesis.",
    "start": "114790",
    "end": "117640"
  },
  {
    "text": "Baselines are really important\nfor building a persuasive case.",
    "start": "117640",
    "end": "121000"
  },
  {
    "text": "And they can be used to\nilluminate specific aspects",
    "start": "121000",
    "end": "123790"
  },
  {
    "text": "of the problem that\nyou're tackling",
    "start": "123790",
    "end": "125560"
  },
  {
    "text": "and specific virtues of\nyour proposed system.",
    "start": "125560",
    "end": "128457"
  },
  {
    "text": "But this really comes down\nto is, right from the start,",
    "start": "128458",
    "end": "130750"
  },
  {
    "text": "you might be saying,\nfor example, here's",
    "start": "130750",
    "end": "132880"
  },
  {
    "text": "a baseline model, here's my\nproposed modification of it.",
    "start": "132880",
    "end": "136790"
  },
  {
    "text": "And the way we test\nyour hypothesis",
    "start": "136790",
    "end": "138640"
  },
  {
    "text": "is by comparing the performance\nof those two systems.",
    "start": "138640",
    "end": "141700"
  },
  {
    "text": "In that context, you can see\nthat the baseline is playing",
    "start": "141700",
    "end": "144160"
  },
  {
    "text": "a crucial role in quantifying\nthe extent to which",
    "start": "144160",
    "end": "147100"
  },
  {
    "text": "your hypothesis is true.",
    "start": "147100",
    "end": "148750"
  },
  {
    "text": "And therefore, careful model\ncomparisons at that level",
    "start": "148750",
    "end": "152020"
  },
  {
    "text": "are going to be\nreally fundamental",
    "start": "152020",
    "end": "153460"
  },
  {
    "text": "to successful pursuit\nof the hypothesis.",
    "start": "153460",
    "end": "158000"
  },
  {
    "start": "157000",
    "end": "197000"
  },
  {
    "text": "When in doubt, you could\ninclude random baselines",
    "start": "158000",
    "end": "160640"
  },
  {
    "text": "in your results table.",
    "start": "160640",
    "end": "161810"
  },
  {
    "text": "They are very easy\nto set up and can",
    "start": "161810",
    "end": "163485"
  },
  {
    "text": "illuminate what it's\nlike if we're just",
    "start": "163485",
    "end": "165110"
  },
  {
    "text": "making random predictions.",
    "start": "165110",
    "end": "166940"
  },
  {
    "text": "And here I'm showing you\nthat scikit-learn kind of",
    "start": "166940",
    "end": "169040"
  },
  {
    "text": "has you covered on this point.",
    "start": "169040",
    "end": "170430"
  },
  {
    "text": "They have two classes,\nDummyClassifier",
    "start": "170430",
    "end": "172819"
  },
  {
    "text": "and DummyRegressor, each with\na wide range of different ways",
    "start": "172820",
    "end": "176210"
  },
  {
    "text": "that they can make random\nguesses based on the data.",
    "start": "176210",
    "end": "179355"
  },
  {
    "text": "And I would encourage you to\nuse these classes because it",
    "start": "179355",
    "end": "181730"
  },
  {
    "text": "will make it easy for you\nto fit the random baselines",
    "start": "181730",
    "end": "184670"
  },
  {
    "text": "into your overall experimental\npipeline, which will reduce",
    "start": "184670",
    "end": "187840"
  },
  {
    "text": "the amount of code that you\nhave to write and possibly avoid",
    "start": "187840",
    "end": "190340"
  },
  {
    "text": "bugs that might come from\nimplementing these baselines",
    "start": "190340",
    "end": "193160"
  },
  {
    "text": "yourself.",
    "start": "193160",
    "end": "194000"
  },
  {
    "text": "So strongly encouraged.",
    "start": "194000",
    "end": "196245"
  },
  {
    "text": "And kind of at the other\nend of the spectrum,",
    "start": "196245",
    "end": "198120"
  },
  {
    "start": "197000",
    "end": "411000"
  },
  {
    "text": "you might think, for\nyour task, whether there",
    "start": "198120",
    "end": "199995"
  },
  {
    "text": "are task-specific baselines\nthat you should be considering.",
    "start": "199995",
    "end": "203342"
  },
  {
    "text": "Because they might\nreveal something",
    "start": "203342",
    "end": "204799"
  },
  {
    "text": "about the dataset, or\nthe problem, or the way",
    "start": "204800",
    "end": "207050"
  },
  {
    "text": "people are going about\nmodeling the problem.",
    "start": "207050",
    "end": "209540"
  },
  {
    "text": "We saw an example of this\nbefore in the context",
    "start": "209540",
    "end": "211799"
  },
  {
    "text": "of natural language inference.",
    "start": "211800",
    "end": "213050"
  },
  {
    "text": "We saw that hypothesis-only\nbaselines tended to make",
    "start": "213050",
    "end": "217280"
  },
  {
    "text": "predictions that were as\ngood as 0.65 to 0.70 F1,",
    "start": "217280",
    "end": "221480"
  },
  {
    "text": "which is substantially better\nthan the baseline random chance",
    "start": "221480",
    "end": "225080"
  },
  {
    "text": "which would be at about 0.33.",
    "start": "225080",
    "end": "227600"
  },
  {
    "text": "And that's revealing to us that\nwhen we measure performance,",
    "start": "227600",
    "end": "230720"
  },
  {
    "text": "we should really be\nthinking about gains",
    "start": "230720",
    "end": "232430"
  },
  {
    "text": "above that\nhypothesis-only baseline.",
    "start": "232430",
    "end": "235040"
  },
  {
    "text": "Comparisons against\nrandom chance",
    "start": "235040",
    "end": "236780"
  },
  {
    "text": "are going to vastly overstate\nthe extent to which we",
    "start": "236780",
    "end": "239180"
  },
  {
    "text": "have made meaningful\nprogress on those datasets.",
    "start": "239180",
    "end": "244069"
  },
  {
    "text": "The story of the Story Cloze\ntask is somewhat similar.",
    "start": "244070",
    "end": "246980"
  },
  {
    "text": "Here the task is to distinguish\nbetween a coherent and",
    "start": "246980",
    "end": "249709"
  },
  {
    "text": "incoherent ending for a story.",
    "start": "249710",
    "end": "252020"
  },
  {
    "text": "And people observed\nthat systems that",
    "start": "252020",
    "end": "253700"
  },
  {
    "text": "looked only at\nthe ending options",
    "start": "253700",
    "end": "255532"
  },
  {
    "text": "were able to do really well.",
    "start": "255533",
    "end": "256700"
  },
  {
    "text": "There is some bias in coherent\nand incoherent continuations",
    "start": "256700",
    "end": "261350"
  },
  {
    "text": "that leads them to be pretty\ngood evidence for making",
    "start": "261350",
    "end": "264140"
  },
  {
    "text": "this classification decision.",
    "start": "264140",
    "end": "266180"
  },
  {
    "text": "Again, you might\nthink that reveals",
    "start": "266180",
    "end": "267889"
  },
  {
    "text": "that there's a fundamental\nproblem with the data set.",
    "start": "267890",
    "end": "270140"
  },
  {
    "text": "And that might be true.",
    "start": "270140",
    "end": "271140"
  },
  {
    "text": "But another\nperspective is simply",
    "start": "271140",
    "end": "273218"
  },
  {
    "text": "that when we do comparisons\nand think about the model's",
    "start": "273218",
    "end": "275509"
  },
  {
    "text": "performance, it should be\nwith this as the baseline",
    "start": "275510",
    "end": "278360"
  },
  {
    "text": "and not random guessing.",
    "start": "278360",
    "end": "279620"
  },
  {
    "start": "279620",
    "end": "283300"
  },
  {
    "text": "OK, let's talk about\nhyperparameter optimization.",
    "start": "283300",
    "end": "285970"
  },
  {
    "text": "We discussed this in our\nunit on sentiment analysis.",
    "start": "285970",
    "end": "288800"
  },
  {
    "text": "And we walked through\nsome of the rationale.",
    "start": "288800",
    "end": "290740"
  },
  {
    "text": "Let me quickly reiterate\nthe full case for this.",
    "start": "290740",
    "end": "293650"
  },
  {
    "text": "First, hyperparameter\noptimization",
    "start": "293650",
    "end": "295630"
  },
  {
    "text": "might be crucial for obtaining\nthe best version of your model",
    "start": "295630",
    "end": "298480"
  },
  {
    "text": "that you can, which might\nbe your fundamental goal.",
    "start": "298480",
    "end": "301450"
  },
  {
    "text": "Probably for any modern model\nthat you're looking at there",
    "start": "301450",
    "end": "304420"
  },
  {
    "text": "is a wide range of\nhyperparameters.",
    "start": "304420",
    "end": "306550"
  },
  {
    "text": "And we might know that\ndifferent settings of them",
    "start": "306550",
    "end": "308800"
  },
  {
    "text": "lead to very different outcomes.",
    "start": "308800",
    "end": "310819"
  },
  {
    "text": "So it's in your best interest\nto do hyperparameter search",
    "start": "310820",
    "end": "313300"
  },
  {
    "text": "to put your model in\nthe very best light.",
    "start": "313300",
    "end": "316360"
  },
  {
    "text": "We also talked at\nlength about how",
    "start": "316360",
    "end": "317949"
  },
  {
    "text": "this is a crucial step in\nconducting fair comparisons",
    "start": "317950",
    "end": "321520"
  },
  {
    "text": "between models.",
    "start": "321520",
    "end": "322720"
  },
  {
    "text": "It's really important that\nwhen you conduct a comparison",
    "start": "322720",
    "end": "325330"
  },
  {
    "text": "you not put one model in\nits best light with its best",
    "start": "325330",
    "end": "328509"
  },
  {
    "text": "hyperparameter settings and\nhave all the other models",
    "start": "328510",
    "end": "331390"
  },
  {
    "text": "be kind of randomly chosen\nor even poorly chosen",
    "start": "331390",
    "end": "334420"
  },
  {
    "text": "hyperparameter\nsettings, because that",
    "start": "334420",
    "end": "336130"
  },
  {
    "text": "would lead to unfair comparisons\nand exaggerate differences",
    "start": "336130",
    "end": "339520"
  },
  {
    "text": "between the models.",
    "start": "339520",
    "end": "340819"
  },
  {
    "text": "What we want to do\nis compare the models",
    "start": "340820",
    "end": "343030"
  },
  {
    "text": "all with their best possible\nhyperparameter settings.",
    "start": "343030",
    "end": "345850"
  },
  {
    "text": "And that implies doing extensive\nsearch to find those settings.",
    "start": "345850",
    "end": "350075"
  },
  {
    "text": "And the third motivation\nyou might have",
    "start": "350075",
    "end": "351700"
  },
  {
    "text": "is just to understand the\nstability of your architecture.",
    "start": "351700",
    "end": "354310"
  },
  {
    "text": "We might want to know for some\nlarge space of hyperparameters",
    "start": "354310",
    "end": "357520"
  },
  {
    "text": "which ones really matter\nfor final performance,",
    "start": "357520",
    "end": "360250"
  },
  {
    "text": "maybe which ones lead to\nreally degenerate solutions,",
    "start": "360250",
    "end": "363220"
  },
  {
    "text": "and which space\nof hyperparameters",
    "start": "363220",
    "end": "365410"
  },
  {
    "text": "overall perform the best.",
    "start": "365410",
    "end": "366790"
  },
  {
    "text": "So that we have more than just\na single set of parameters",
    "start": "366790",
    "end": "369370"
  },
  {
    "text": "that work well, but\nmaybe real insights",
    "start": "369370",
    "end": "371530"
  },
  {
    "text": "into the overall settings of\nthe models that are really good.",
    "start": "371530",
    "end": "376060"
  },
  {
    "text": "There's one more rule that\nI need to reiterate here.",
    "start": "376060",
    "end": "378970"
  },
  {
    "text": "All hyperparameter\ntuning must be done only",
    "start": "378970",
    "end": "381610"
  },
  {
    "text": "on train and development data.",
    "start": "381610",
    "end": "383530"
  },
  {
    "text": "It is a sin in our field to\ndo any kind of hyperparameter",
    "start": "383530",
    "end": "386980"
  },
  {
    "text": "tuning on a test set.",
    "start": "386980",
    "end": "388570"
  },
  {
    "text": "All of that tuning should\nhappen outside of the test set.",
    "start": "388570",
    "end": "391420"
  },
  {
    "text": "And then as usual,\nyou get one run",
    "start": "391420",
    "end": "393310"
  },
  {
    "text": "on the test set with\nyour chosen parameters",
    "start": "393310",
    "end": "395380"
  },
  {
    "text": "and that is the number that\nyou report as performance",
    "start": "395380",
    "end": "398500"
  },
  {
    "text": "on the test data.",
    "start": "398500",
    "end": "399940"
  },
  {
    "text": "That's the only way\nthat we can really",
    "start": "399940",
    "end": "402100"
  },
  {
    "text": "get a look at how\nthese systems behave",
    "start": "402100",
    "end": "404260"
  },
  {
    "text": "on completely unseen data.",
    "start": "404260",
    "end": "405920"
  },
  {
    "text": "So this is really crucial\nfor understanding progress",
    "start": "405920",
    "end": "409000"
  },
  {
    "text": "in our field.",
    "start": "409000",
    "end": "411700"
  },
  {
    "start": "411000",
    "end": "449000"
  },
  {
    "text": "Now hyperparameter optimization,\nas you can imagine,",
    "start": "411700",
    "end": "415180"
  },
  {
    "text": "can get very expensive.",
    "start": "415180",
    "end": "416397"
  },
  {
    "text": "And let's review that and then\ntalk about some compromises,",
    "start": "416397",
    "end": "418855"
  },
  {
    "text": "right?",
    "start": "418855",
    "end": "419740"
  },
  {
    "text": "The ideal for\nhyperparameter optimization",
    "start": "419740",
    "end": "422229"
  },
  {
    "text": "is that you identify a large\nset of values for your model.",
    "start": "422230",
    "end": "426020"
  },
  {
    "text": "You create a list of all the\ncombinations of those values.",
    "start": "426020",
    "end": "428800"
  },
  {
    "text": "This will be the cross\nproduct of all the values",
    "start": "428800",
    "end": "431180"
  },
  {
    "text": "of the features\nthat you identified.",
    "start": "431180",
    "end": "433880"
  },
  {
    "text": "And for each of\nthe settings, you",
    "start": "433880",
    "end": "435310"
  },
  {
    "text": "should cross validate it on\nthe available training data.",
    "start": "435310",
    "end": "438730"
  },
  {
    "text": "And then choose the settings\nthat are the best at step",
    "start": "438730",
    "end": "441580"
  },
  {
    "text": "three, train on all the training\ndata using those settings.",
    "start": "441580",
    "end": "445250"
  },
  {
    "text": "And then finally,\nevaluate on the test data.",
    "start": "445250",
    "end": "448060"
  },
  {
    "text": "That is the ideal here.",
    "start": "448060",
    "end": "449098"
  },
  {
    "start": "449000",
    "end": "499000"
  },
  {
    "text": "And let's just think about how\nthat's actually going to work.",
    "start": "449098",
    "end": "451639"
  },
  {
    "text": "Suppose for our example that\nwe have one hyperparameter",
    "start": "451640",
    "end": "454240"
  },
  {
    "text": "and it has five values.",
    "start": "454240",
    "end": "456599"
  },
  {
    "text": "And we have a second\nhyperparameter with ten values.",
    "start": "456600",
    "end": "459415"
  },
  {
    "text": "Then the cross product\nis going to lead",
    "start": "459415",
    "end": "461040"
  },
  {
    "text": "us to have 50 total settings\nfor those hyperparameters.",
    "start": "461040",
    "end": "464910"
  },
  {
    "text": "Suppose we add a third\nhyperparameter with two values.",
    "start": "464910",
    "end": "467520"
  },
  {
    "text": "Now the number of settings that\nwe have has jumped up to 100.",
    "start": "467520",
    "end": "472620"
  },
  {
    "text": "If we want to do\n5-fold cross-validation",
    "start": "472620",
    "end": "474840"
  },
  {
    "text": "to select those\noptimal parameters,",
    "start": "474840",
    "end": "476820"
  },
  {
    "text": "then we are talking about doing\n500 different experiments.",
    "start": "476820",
    "end": "481320"
  },
  {
    "text": "That's probably\nperfectly fine if you're",
    "start": "481320",
    "end": "483150"
  },
  {
    "text": "dealing with a\nsmall linear model",
    "start": "483150",
    "end": "484979"
  },
  {
    "text": "with some hand-built features.",
    "start": "484980",
    "end": "486720"
  },
  {
    "text": "But if you are fitting a\nlarge transformer-based model",
    "start": "486720",
    "end": "489750"
  },
  {
    "text": "where each experiment\ntakes you up to one day,",
    "start": "489750",
    "end": "492540"
  },
  {
    "text": "this is going to be\nprohibitively expensive",
    "start": "492540",
    "end": "494760"
  },
  {
    "text": "in terms of time or\ncompute resources.",
    "start": "494760",
    "end": "497220"
  },
  {
    "text": "And that's going to compel\nus to make some compromises.",
    "start": "497220",
    "end": "499830"
  },
  {
    "start": "499000",
    "end": "551000"
  },
  {
    "text": "This is the bottom line here.",
    "start": "499830",
    "end": "501720"
  },
  {
    "text": "The above picture,\nthat ideal, is",
    "start": "501720",
    "end": "503910"
  },
  {
    "text": "untenable as a set of laws\nfor our scientific community.",
    "start": "503910",
    "end": "507450"
  },
  {
    "text": "If we adopted it,\nthen complex models",
    "start": "507450",
    "end": "510270"
  },
  {
    "text": "trained on large data sets\nwould end up disfavored",
    "start": "510270",
    "end": "513089"
  },
  {
    "text": "and only the very wealthy\nwould be able to participate.",
    "start": "513090",
    "end": "516580"
  },
  {
    "text": "And just to give you a\nglimpse of just how expensive",
    "start": "516580",
    "end": "518932"
  },
  {
    "text": "this could get,\nhere's a quotation",
    "start": "518933",
    "end": "520349"
  },
  {
    "text": "from this nice paper\non NLP and machine",
    "start": "520350",
    "end": "522750"
  },
  {
    "text": "learning for health care.",
    "start": "522750",
    "end": "524099"
  },
  {
    "text": "In their supplementary\nmaterials,",
    "start": "524100",
    "end": "525839"
  },
  {
    "text": "they report that\nperformance on all",
    "start": "525840",
    "end": "528180"
  },
  {
    "text": "of the above neural networks\nwas tuned automatically using",
    "start": "528180",
    "end": "531180"
  },
  {
    "text": "Google Vizier with a total\nof over 200,000 GPU hours.",
    "start": "531180",
    "end": "537000"
  },
  {
    "text": "For me as a private\ncitizen, that",
    "start": "537000",
    "end": "538500"
  },
  {
    "text": "could easily cost a\nhalf a million dollars",
    "start": "538500",
    "end": "541100"
  },
  {
    "text": "just for the process of\nhyperparameter optimization.",
    "start": "541100",
    "end": "545009"
  },
  {
    "text": "And that's what I mean by this\nbeing kind of fundamentally",
    "start": "545010",
    "end": "547500"
  },
  {
    "text": "untenable for us.",
    "start": "547500",
    "end": "549180"
  },
  {
    "text": "So what should we\ndo in response?",
    "start": "549180",
    "end": "550990"
  },
  {
    "text": "We need a pragmatic\nresponse here.",
    "start": "550990",
    "end": "553410"
  },
  {
    "start": "551000",
    "end": "698000"
  },
  {
    "text": "Here are some\nsteps that you take",
    "start": "553410",
    "end": "554879"
  },
  {
    "text": "to alleviate the problem in what\nI view as kind of descending",
    "start": "554880",
    "end": "558210"
  },
  {
    "text": "order of attractiveness.",
    "start": "558210",
    "end": "559920"
  },
  {
    "text": "So starting with\nthe best option,",
    "start": "559920",
    "end": "561930"
  },
  {
    "text": "you could do some\nrandom sampling",
    "start": "561930",
    "end": "563730"
  },
  {
    "text": "and maybe guided\nsampling to explore",
    "start": "563730",
    "end": "566160"
  },
  {
    "text": "a large space of hyperparameters\non a fixed computational",
    "start": "566160",
    "end": "569519"
  },
  {
    "text": "budget.",
    "start": "569520",
    "end": "572000"
  },
  {
    "text": "You could do search based\non just a few epochs",
    "start": "572000",
    "end": "574340"
  },
  {
    "text": "of training, right, rather\nthan allowing your model",
    "start": "574340",
    "end": "576480"
  },
  {
    "text": "to run for many epochs,\nwhich could take a whole day.",
    "start": "576480",
    "end": "580010"
  },
  {
    "text": "You might select hyperparameters\nbased on one or two epochs,",
    "start": "580010",
    "end": "583370"
  },
  {
    "text": "on the assumption that settings\nthat are good at the start",
    "start": "583370",
    "end": "587089"
  },
  {
    "text": "will remain good and settings\nthat are bad at the start",
    "start": "587090",
    "end": "589550"
  },
  {
    "text": "will remain bad.",
    "start": "589550",
    "end": "590450"
  },
  {
    "text": "That's a heuristic assumption.",
    "start": "590450",
    "end": "591830"
  },
  {
    "text": "But it seems reasonable.",
    "start": "591830",
    "end": "593360"
  },
  {
    "text": "You could possibly bolster\nit with some learning curves",
    "start": "593360",
    "end": "595700"
  },
  {
    "text": "and so forth.",
    "start": "595700",
    "end": "596600"
  },
  {
    "text": "And that could vastly\ncut down on the amount",
    "start": "596600",
    "end": "598910"
  },
  {
    "text": "that you have to spend\nin this search process.",
    "start": "598910",
    "end": "602870"
  },
  {
    "text": "You can also search based\non subsets of the data.",
    "start": "602870",
    "end": "605270"
  },
  {
    "text": "This would be another\nkind of compromise.",
    "start": "605270",
    "end": "607350"
  },
  {
    "text": "However because a lot of\nhyperparameters are dependent",
    "start": "607350",
    "end": "610310"
  },
  {
    "text": "on dataset size,z think\nof regularization terms.",
    "start": "610310",
    "end": "613920"
  },
  {
    "text": "This might be riskier\nthan the version in 2",
    "start": "613920",
    "end": "616610"
  },
  {
    "text": "there where we just\ntrained for a few epochs.",
    "start": "616610",
    "end": "620560"
  },
  {
    "text": "You also might do some\nheuristic search, maybe",
    "start": "620560",
    "end": "623320"
  },
  {
    "text": "by defining which\nhyperparameters matter less",
    "start": "623320",
    "end": "625450"
  },
  {
    "text": "and then set those by hand\nbased on this heuristic search.",
    "start": "625450",
    "end": "628450"
  },
  {
    "text": "And then you might just describe\nthat process in the paper",
    "start": "628450",
    "end": "630910"
  },
  {
    "text": "that via few observations\nyou made some guesses",
    "start": "630910",
    "end": "634149"
  },
  {
    "text": "about parameters\nthat you could fix",
    "start": "634150",
    "end": "636280"
  },
  {
    "text": "and therefore explored a smaller\nsubset of the space that you",
    "start": "636280",
    "end": "639100"
  },
  {
    "text": "might have liked to explore.",
    "start": "639100",
    "end": "640660"
  },
  {
    "text": "Again, I think if\nyou make the case",
    "start": "640660",
    "end": "642129"
  },
  {
    "text": "and you're clear\nabout this, readers",
    "start": "642130",
    "end": "643930"
  },
  {
    "text": "will be receptive because\nwe're aware of the costs.",
    "start": "643930",
    "end": "647770"
  },
  {
    "text": "You can also find\noptimal hyperparameters",
    "start": "647770",
    "end": "649720"
  },
  {
    "text": "via a single split of\nyour data and use them",
    "start": "649720",
    "end": "652060"
  },
  {
    "text": "for all subsequent splits.",
    "start": "652060",
    "end": "654220"
  },
  {
    "text": "That would be justified if\nthe splits are very similar",
    "start": "654220",
    "end": "656709"
  },
  {
    "text": "and your model performance\nis very stable.",
    "start": "656710",
    "end": "659080"
  },
  {
    "text": "And that would reduce all\nthat cross-validation that",
    "start": "659080",
    "end": "661450"
  },
  {
    "text": "did cause the number\nof experiments",
    "start": "661450",
    "end": "663310"
  },
  {
    "text": "we had to run to jump\nup by a large amount.",
    "start": "663310",
    "end": "667450"
  },
  {
    "text": "And finally, you might\nadopt other's choices.",
    "start": "667450",
    "end": "670150"
  },
  {
    "text": "Now the skeptic will complain\nthat these findings don't",
    "start": "670150",
    "end": "672850"
  },
  {
    "text": "translate to new data sets,\nbut it could be the only option",
    "start": "672850",
    "end": "676420"
  },
  {
    "text": "that you just\nobserve for example",
    "start": "676420",
    "end": "678100"
  },
  {
    "text": "that for some very large\nmodel, the original authors use",
    "start": "678100",
    "end": "681220"
  },
  {
    "text": "settings X, Y, and Z,\nthen you might simply",
    "start": "681220",
    "end": "683589"
  },
  {
    "text": "adopt them even knowing that\nyour dataset or your test",
    "start": "683590",
    "end": "687610"
  },
  {
    "text": "might call for different\noptimal settings.",
    "start": "687610",
    "end": "690550"
  },
  {
    "text": "It isn't the best.",
    "start": "690550",
    "end": "691700"
  },
  {
    "text": "But if it's the only\nthing that you can afford,",
    "start": "691700",
    "end": "694700"
  },
  {
    "text": "it's certainly a\nreasonable case to make.",
    "start": "694700",
    "end": "698940"
  },
  {
    "start": "698000",
    "end": "734000"
  },
  {
    "text": "Finally some tools for\nhyperparameter search.",
    "start": "698940",
    "end": "701190"
  },
  {
    "text": "As usual scikit-learn\nhas a bunch",
    "start": "701190",
    "end": "702930"
  },
  {
    "text": "of great tools for this,\nGridSearch, RandomizedSearch,",
    "start": "702930",
    "end": "706170"
  },
  {
    "text": "and HalvingGridSearch.",
    "start": "706170",
    "end": "708180"
  },
  {
    "text": "GridSearch will be\nthe most expensive,",
    "start": "708180",
    "end": "709920"
  },
  {
    "text": "RandomizedSearch\nthe least expensive,",
    "start": "709920",
    "end": "711630"
  },
  {
    "text": "and HalvingGridSearch\nwill help you",
    "start": "711630",
    "end": "713130"
  },
  {
    "text": "kind of strategically\nnavigate through the space",
    "start": "713130",
    "end": "716420"
  },
  {
    "text": "of hyperparameters.",
    "start": "716420",
    "end": "717779"
  },
  {
    "text": "And if you want to go even\nfurther in that direction,",
    "start": "717780",
    "end": "720090"
  },
  {
    "text": "the scikit-optimize\npackage offers a bunch",
    "start": "720090",
    "end": "723840"
  },
  {
    "text": "of tools for doing model-based\nperformance-driven exploration",
    "start": "723840",
    "end": "728040"
  },
  {
    "text": "of a space of hyperparameters.",
    "start": "728040",
    "end": "729470"
  },
  {
    "text": "And that could be\nvery effective indeed.",
    "start": "729470",
    "end": "733600"
  },
  {
    "text": "All right, let's talk briefly\nabout classifier comparison.",
    "start": "733600",
    "end": "736370"
  },
  {
    "start": "734000",
    "end": "811000"
  },
  {
    "text": "We've-- it's a topic\nwe've reviewed before,",
    "start": "736370",
    "end": "738529"
  },
  {
    "text": "but I'll just briefly recap.",
    "start": "738530",
    "end": "740230"
  },
  {
    "text": "The scenario is this.",
    "start": "740230",
    "end": "741380"
  },
  {
    "text": "Suppose you have assessed\ntwo classifier models.",
    "start": "741380",
    "end": "743710"
  },
  {
    "text": "Their performance is probably\ndifferent to some degree",
    "start": "743710",
    "end": "746200"
  },
  {
    "text": "numerically.",
    "start": "746200",
    "end": "747430"
  },
  {
    "text": "What can be done to establish\nwhether those models are",
    "start": "747430",
    "end": "749830"
  },
  {
    "text": "different in some\nmeaningful sense?",
    "start": "749830",
    "end": "752310"
  },
  {
    "text": "As we've discussed, I think\nguidance from the literature",
    "start": "752310",
    "end": "755140"
  },
  {
    "text": "is that first we could\ncover practical differences.",
    "start": "755140",
    "end": "757630"
  },
  {
    "text": "If you just observed that one\nmodel makes 10,000 more highly",
    "start": "757630",
    "end": "761260"
  },
  {
    "text": "important predictions\nthan another,",
    "start": "761260",
    "end": "763160"
  },
  {
    "text": "then that might be\nsufficient to make the case",
    "start": "763160",
    "end": "765129"
  },
  {
    "text": "that it's the better model.",
    "start": "765130",
    "end": "767260"
  },
  {
    "text": "For differences\nthat are narrower,",
    "start": "767260",
    "end": "769000"
  },
  {
    "text": "again the guidance is that we\nmight use confidence intervals",
    "start": "769000",
    "end": "772870"
  },
  {
    "text": "on repeated runs.",
    "start": "772870",
    "end": "774040"
  },
  {
    "text": "Or the Wilcoxon signed-rank\ntest to give a single summary",
    "start": "774040",
    "end": "777339"
  },
  {
    "text": "statistic of whether or\nnot the different runs are",
    "start": "777340",
    "end": "780250"
  },
  {
    "text": "truly different in their\nmeans and variance.",
    "start": "780250",
    "end": "783810"
  },
  {
    "text": "You could use McNemar's\ntest if you can only",
    "start": "783810",
    "end": "786600"
  },
  {
    "text": "afford to run one experiment,\nwhereas the Wilcoxon",
    "start": "786600",
    "end": "789899"
  },
  {
    "text": "and confidence\nintervals will require",
    "start": "789900",
    "end": "791910"
  },
  {
    "text": "you to run 10 to 20 different\nexperiments, which again, could",
    "start": "791910",
    "end": "795569"
  },
  {
    "text": "be prohibitively expensive.",
    "start": "795570",
    "end": "797010"
  },
  {
    "text": "And in those situations,\nyou might fall back",
    "start": "797010",
    "end": "799560"
  },
  {
    "text": "to McNemar's test, because it's\nless expensive and arguably",
    "start": "799560",
    "end": "803250"
  },
  {
    "text": "better than nothing,\nespecially in scenarios",
    "start": "803250",
    "end": "805800"
  },
  {
    "text": "where it's hard to tell whether\nthere are practical differences",
    "start": "805800",
    "end": "808987"
  },
  {
    "text": "between the systems.",
    "start": "808987",
    "end": "809820"
  },
  {
    "start": "809820",
    "end": "812910"
  },
  {
    "start": "811000",
    "end": "873000"
  },
  {
    "text": "Finally, let's talk\nabout two topics",
    "start": "812910",
    "end": "814579"
  },
  {
    "text": "that seem especially pressing\nin the context of large scale",
    "start": "814580",
    "end": "817490"
  },
  {
    "text": "deep learning models.",
    "start": "817490",
    "end": "818390"
  },
  {
    "text": "And the first is assessing\nmodels without convergence,",
    "start": "818390",
    "end": "821105"
  },
  {
    "text": "right?",
    "start": "821105",
    "end": "822320"
  },
  {
    "text": "When working with linear\nmodels, convergence issues",
    "start": "822320",
    "end": "825080"
  },
  {
    "text": "rarely arise, because\nthe models seem",
    "start": "825080",
    "end": "828110"
  },
  {
    "text": "to converge quickly based on\nwhatever threshold you've set.",
    "start": "828110",
    "end": "831380"
  },
  {
    "text": "And convergence implies kind\nof maximized performance",
    "start": "831380",
    "end": "834290"
  },
  {
    "text": "in a wide range of cases.",
    "start": "834290",
    "end": "836750"
  },
  {
    "text": "With neural networks\nhowever, convergence issues",
    "start": "836750",
    "end": "839270"
  },
  {
    "text": "really take center stage.",
    "start": "839270",
    "end": "840600"
  },
  {
    "text": "The models rarely converge even\nbased on liberal thresholds",
    "start": "840600",
    "end": "843920"
  },
  {
    "text": "that you might set.",
    "start": "843920",
    "end": "845880"
  },
  {
    "text": "They converge at different\nrates between runs,",
    "start": "845880",
    "end": "848040"
  },
  {
    "text": "so it's hard to predict.",
    "start": "848040",
    "end": "849329"
  },
  {
    "text": "And their performance\non the test data",
    "start": "849330",
    "end": "851520"
  },
  {
    "text": "is often heavily dependent\non these differences, right?",
    "start": "851520",
    "end": "856560"
  },
  {
    "text": "Sometimes a model\nwith a low final error",
    "start": "856560",
    "end": "858779"
  },
  {
    "text": "turns out to be great.",
    "start": "858780",
    "end": "860370"
  },
  {
    "text": "And sometimes it turns out\nto be worse than one that",
    "start": "860370",
    "end": "863279"
  },
  {
    "text": "finished with a higher error.",
    "start": "863280",
    "end": "864660"
  },
  {
    "text": "Who really knows\nwhat's going on?",
    "start": "864660",
    "end": "866879"
  },
  {
    "text": "Our only fallback\nin these situations",
    "start": "866880",
    "end": "868890"
  },
  {
    "text": "is just do experiments\nand observe",
    "start": "868890",
    "end": "871230"
  },
  {
    "text": "what seems to work the best.",
    "start": "871230",
    "end": "873970"
  },
  {
    "start": "873000",
    "end": "933000"
  },
  {
    "text": "So I think a very natural\nand easy to implement",
    "start": "873970",
    "end": "876660"
  },
  {
    "text": "response to this that\nproves highly effective",
    "start": "876660",
    "end": "878850"
  },
  {
    "text": "is what I'm calling your\nincremental dev-set testing.",
    "start": "878850",
    "end": "882310"
  },
  {
    "text": "This is just the idea\nthat as training proceeds,",
    "start": "882310",
    "end": "885240"
  },
  {
    "text": "we will regularly collect\ninformation about performance",
    "start": "885240",
    "end": "888420"
  },
  {
    "text": "on some held out dev-set as\npart of the training process.",
    "start": "888420",
    "end": "892269"
  },
  {
    "text": "For example, at every\n100th iteration,",
    "start": "892270",
    "end": "894720"
  },
  {
    "text": "you could make predictions\non that dev-set",
    "start": "894720",
    "end": "896790"
  },
  {
    "text": "and store those predictions\nfor some kind of assessment.",
    "start": "896790",
    "end": "900990"
  },
  {
    "text": "All the PyTorch\nmodels for our course",
    "start": "900990",
    "end": "903149"
  },
  {
    "text": "have an early stopping\nparameter that",
    "start": "903150",
    "end": "905130"
  },
  {
    "text": "will allow you to conduct\nexperiments in this way",
    "start": "905130",
    "end": "907800"
  },
  {
    "text": "and keep hold of what\nseemed to be the best",
    "start": "907800",
    "end": "910200"
  },
  {
    "text": "model, performance-wise,\nand then",
    "start": "910200",
    "end": "912240"
  },
  {
    "text": "report that based on\nthe stopping criteria",
    "start": "912240",
    "end": "915149"
  },
  {
    "text": "that you've set up.",
    "start": "915150",
    "end": "916230"
  },
  {
    "text": "And with luck,\nheuristically, that",
    "start": "916230",
    "end": "918029"
  },
  {
    "text": "will give you the best\nmodel in the fewest epochs.",
    "start": "918030",
    "end": "921410"
  },
  {
    "text": "And the early_stopping\nparameter has a bunch",
    "start": "921410",
    "end": "923490"
  },
  {
    "text": "of different other\nsettings that you",
    "start": "923490",
    "end": "924990"
  },
  {
    "text": "can use to control\nexactly how it behaves,",
    "start": "924990",
    "end": "927750"
  },
  {
    "text": "which might be important for\nparticular models structures",
    "start": "927750",
    "end": "930840"
  },
  {
    "text": "or data sets.",
    "start": "930840",
    "end": "934000"
  },
  {
    "start": "933000",
    "end": "985000"
  },
  {
    "text": "Here's a bit of our\nmotivation for early stopping.",
    "start": "934000",
    "end": "936622"
  },
  {
    "text": "You might be\nthinking, why not just",
    "start": "936622",
    "end": "938080"
  },
  {
    "text": "let my model run to\nconvergence if I possibly can.",
    "start": "938080",
    "end": "941350"
  },
  {
    "text": "In the context of these large,\nvery difficult optimization",
    "start": "941350",
    "end": "945639"
  },
  {
    "text": "processes, that could lead\nyou really far astray, right?",
    "start": "945640",
    "end": "948760"
  },
  {
    "text": "So here is a picture of\na deep learning model.",
    "start": "948760",
    "end": "951590"
  },
  {
    "text": "And you can see its error\ngoing down very quickly",
    "start": "951590",
    "end": "953710"
  },
  {
    "text": "over many iterations.",
    "start": "953710",
    "end": "955062"
  },
  {
    "text": "And it looks like you\nmight want to iterate out",
    "start": "955062",
    "end": "957020"
  },
  {
    "text": "to even to 80\nepochs of training.",
    "start": "957020",
    "end": "959840"
  },
  {
    "text": "However, if you\nlook at performance",
    "start": "959840",
    "end": "961420"
  },
  {
    "text": "on that held-out dev set you\nsee that this model actually",
    "start": "961420",
    "end": "964209"
  },
  {
    "text": "very quickly reached\nits peak of performance.",
    "start": "964210",
    "end": "967360"
  },
  {
    "text": "And then all that\nremaining training",
    "start": "967360",
    "end": "968890"
  },
  {
    "text": "was just either wasting time\nor eroding the performance",
    "start": "968890",
    "end": "973930"
  },
  {
    "text": "that you saw early\non in the process.",
    "start": "973930",
    "end": "975790"
  },
  {
    "text": "And this is exactly why since\nthis is our real goal here,",
    "start": "975790",
    "end": "978940"
  },
  {
    "text": "you might want to do some\nkind of dev-set testing",
    "start": "978940",
    "end": "982930"
  },
  {
    "text": "with early stopping.",
    "start": "982930",
    "end": "986029"
  },
  {
    "start": "985000",
    "end": "1078000"
  },
  {
    "text": "The final thing I'd want to\nsay here is that all of this",
    "start": "986030",
    "end": "988370"
  },
  {
    "text": "might lead us to get out\nof the mode of assuming",
    "start": "988370",
    "end": "990589"
  },
  {
    "text": "that we should always\nbe recording one",
    "start": "990590",
    "end": "992270"
  },
  {
    "text": "number to summarize our models.",
    "start": "992270",
    "end": "994320"
  },
  {
    "text": "We're dealing with\nvery powerful models.",
    "start": "994320",
    "end": "996530"
  },
  {
    "text": "In the limit, they\nmight be able to learn",
    "start": "996530",
    "end": "998390"
  },
  {
    "text": "very complicated things.",
    "start": "998390",
    "end": "999830"
  },
  {
    "text": "And we might want to ask\ndifferent questions like how",
    "start": "999830",
    "end": "1002920"
  },
  {
    "text": "quickly can they learn, and how\neffectively, and how robustly.",
    "start": "1002920",
    "end": "1006250"
  },
  {
    "text": "And that might imply that\nwhat we really want to do",
    "start": "1006250",
    "end": "1008710"
  },
  {
    "text": "is not report summary\ntables of statistics,",
    "start": "1008710",
    "end": "1011740"
  },
  {
    "text": "but rather full learning curves\nwith confidence intervals.",
    "start": "1011740",
    "end": "1015130"
  },
  {
    "text": "This is a picture from a paper\nthat I was involved with.",
    "start": "1015130",
    "end": "1017770"
  },
  {
    "text": "And I think it's\nilluminating to see",
    "start": "1017770",
    "end": "1019390"
  },
  {
    "text": "a by category breakdown of\nhow the model is performing",
    "start": "1019390",
    "end": "1022840"
  },
  {
    "text": "in addition to the\noverall average.",
    "start": "1022840",
    "end": "1024910"
  },
  {
    "text": "Because you can see that\nwhile this red model",
    "start": "1024910",
    "end": "1027310"
  },
  {
    "text": "is arguably much better than\nthe yellow and the gray overall.",
    "start": "1027310",
    "end": "1031569"
  },
  {
    "text": "It's kind of hard to\ndistinguish it globally",
    "start": "1031569",
    "end": "1033819"
  },
  {
    "text": "from this blue model.",
    "start": "1033819",
    "end": "1035409"
  },
  {
    "text": "But for various of\nthe subcategories,",
    "start": "1035410",
    "end": "1037240"
  },
  {
    "text": "you do see some differences.",
    "start": "1037240",
    "end": "1038838"
  },
  {
    "text": "Whereas for others,\nyou do see that they're",
    "start": "1038838",
    "end": "1040630"
  },
  {
    "text": "kind of indistinguishable.",
    "start": "1040630",
    "end": "1042310"
  },
  {
    "text": "It's a very rich picture.",
    "start": "1042310",
    "end": "1043740"
  },
  {
    "text": "You can also see that early on\nfor some of these categories",
    "start": "1043740",
    "end": "1046730"
  },
  {
    "text": "some of these models are\nreally differentiated.",
    "start": "1046730",
    "end": "1048700"
  },
  {
    "text": "They learn more efficiently.",
    "start": "1048700",
    "end": "1050679"
  },
  {
    "text": "Whereas by the time you've\nrun out to 100,000 epochs,",
    "start": "1050680",
    "end": "1053890"
  },
  {
    "text": "many of the model\ndistinctions have disappeared.",
    "start": "1053890",
    "end": "1056770"
  },
  {
    "text": "That's the kind of rich picture\nthat is already giving us",
    "start": "1056770",
    "end": "1059350"
  },
  {
    "text": "a sense for how different\nvalues and different goals",
    "start": "1059350",
    "end": "1062330"
  },
  {
    "text": "we have might guide\ndifferent choices about which",
    "start": "1062330",
    "end": "1065260"
  },
  {
    "text": "model to use and different\nchoices about how to optimize.",
    "start": "1065260",
    "end": "1068908"
  },
  {
    "text": "And I would just\nlove it if our field",
    "start": "1068908",
    "end": "1070450"
  },
  {
    "text": "got into the habit of reporting\nthis very full picture, as",
    "start": "1070450",
    "end": "1074230"
  },
  {
    "text": "opposed to reducing\neverything to a single number.",
    "start": "1074230",
    "end": "1078419"
  },
  {
    "start": "1078000",
    "end": "1209000"
  },
  {
    "text": "The final topic is the\nrole of random parameter",
    "start": "1078420",
    "end": "1080850"
  },
  {
    "text": "initialization.",
    "start": "1080850",
    "end": "1081780"
  },
  {
    "text": "This is kind of yet another\nhyperparameter that's",
    "start": "1081780",
    "end": "1084390"
  },
  {
    "text": "in the background that's much\nmore difficult to think about.",
    "start": "1084390",
    "end": "1087960"
  },
  {
    "text": "Most deep learning models have\ntheir parameters initialized",
    "start": "1087960",
    "end": "1090779"
  },
  {
    "text": "randomly, or many\nof those parameters",
    "start": "1090780",
    "end": "1093150"
  },
  {
    "text": "are initialized randomly.",
    "start": "1093150",
    "end": "1095170"
  },
  {
    "text": "This is clearly meaningful for\nthese non-convex optimization",
    "start": "1095170",
    "end": "1098640"
  },
  {
    "text": "problems that we're posing.",
    "start": "1098640",
    "end": "1100200"
  },
  {
    "text": "But even simple\nmodels can also be",
    "start": "1100200",
    "end": "1101940"
  },
  {
    "text": "impacted if you're dealing\nwith very small data sets",
    "start": "1101940",
    "end": "1104309"
  },
  {
    "text": "with very large feature spaces.",
    "start": "1104310",
    "end": "1107480"
  },
  {
    "text": "In this classic paper\nhere, these authors",
    "start": "1107480",
    "end": "1109669"
  },
  {
    "text": "just observed that\ndifferent initializations",
    "start": "1109670",
    "end": "1111800"
  },
  {
    "text": "for neural sequence\nmodels that were doing",
    "start": "1111800",
    "end": "1113600"
  },
  {
    "text": "named entity recognition led\nto statistically significantly",
    "start": "1113600",
    "end": "1117830"
  },
  {
    "text": "different results.",
    "start": "1117830",
    "end": "1118909"
  },
  {
    "text": "That is one and the same model\nwith a different random seed",
    "start": "1118910",
    "end": "1122210"
  },
  {
    "text": "was performing in ways that\nlooked significantly different",
    "start": "1122210",
    "end": "1125029"
  },
  {
    "text": "on these data sets.",
    "start": "1125030",
    "end": "1126470"
  },
  {
    "text": "And a number of recent\nsystems actually",
    "start": "1126470",
    "end": "1128630"
  },
  {
    "text": "turned out to be\nindistinguishable in terms",
    "start": "1128630",
    "end": "1130850"
  },
  {
    "text": "of their raw performance\nonce this source of variation",
    "start": "1130850",
    "end": "1134030"
  },
  {
    "text": "was taken into account.",
    "start": "1134030",
    "end": "1135440"
  },
  {
    "text": "That's just a powerful example\nof how much a random seed could",
    "start": "1135440",
    "end": "1140029"
  },
  {
    "text": "shape final performance in the\ncontext of models like this.",
    "start": "1140030",
    "end": "1144140"
  },
  {
    "text": "Relatedly, at the other\nend of the spectrum,",
    "start": "1144140",
    "end": "1146160"
  },
  {
    "text": "you can see catastrophic\nfailure as a result",
    "start": "1146160",
    "end": "1149040"
  },
  {
    "text": "of unlucky initialization.",
    "start": "1149040",
    "end": "1151010"
  },
  {
    "text": "Some settings are great and\nsome can be miserable failures.",
    "start": "1151010",
    "end": "1154790"
  },
  {
    "text": "We don't really know ahead\nof time which will be which.",
    "start": "1154790",
    "end": "1157460"
  },
  {
    "text": "And that means that we just have\nto be really attentive to how",
    "start": "1157460",
    "end": "1160250"
  },
  {
    "text": "we're initializing these systems\nin a wide range of settings.",
    "start": "1160250",
    "end": "1164280"
  },
  {
    "text": "And you'll notice that\nin the evaluation methods",
    "start": "1164280",
    "end": "1166700"
  },
  {
    "text": "notebook that I've distributed\nas a companion to this lecture,",
    "start": "1166700",
    "end": "1170100"
  },
  {
    "text": "I fit a simple feed-forward\nnetwork, a very small one,",
    "start": "1170100",
    "end": "1172669"
  },
  {
    "text": "on the classic\nXOR problem, which",
    "start": "1172670",
    "end": "1174530"
  },
  {
    "text": "is one of the original\nmotivating problems",
    "start": "1174530",
    "end": "1176930"
  },
  {
    "text": "for using deep\nlearning models at all.",
    "start": "1176930",
    "end": "1179390"
  },
  {
    "text": "And what you see is that it\nsucceeds about 8 out of 10",
    "start": "1179390",
    "end": "1182150"
  },
  {
    "text": "times, where the\nonly thing that we're",
    "start": "1182150",
    "end": "1184280"
  },
  {
    "text": "changing across these\nmodels is the way they",
    "start": "1184280",
    "end": "1187310"
  },
  {
    "text": "are randomly initialized.",
    "start": "1187310",
    "end": "1188720"
  },
  {
    "text": "And that again just\nshows you that this",
    "start": "1188720",
    "end": "1190820"
  },
  {
    "text": "can be powerfully shaping final\nperformance for our systems.",
    "start": "1190820",
    "end": "1194659"
  },
  {
    "text": "And probably what\nwe need to do is",
    "start": "1194660",
    "end": "1196340"
  },
  {
    "text": "be thinking about this as\nyet another hyperparameter",
    "start": "1196340",
    "end": "1199730"
  },
  {
    "text": "that we need to tune\nand optimize along",
    "start": "1199730",
    "end": "1202580"
  },
  {
    "text": "with all the rest.",
    "start": "1202580",
    "end": "1204549"
  },
  {
    "start": "1204550",
    "end": "1208427"
  }
]