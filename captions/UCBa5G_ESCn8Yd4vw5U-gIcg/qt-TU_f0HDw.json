[
  {
    "start": "0",
    "end": "118000"
  },
  {
    "start": "0",
    "end": "4662"
  },
  {
    "text": "CHRISTOPHER POTTS:\nHello, everyone,",
    "start": "4662",
    "end": "6120"
  },
  {
    "text": "welcome to part 2 in our\nseries on supervised sentiment",
    "start": "6120",
    "end": "8580"
  },
  {
    "text": "analysis.",
    "start": "8580",
    "end": "9180"
  },
  {
    "text": "The screencast is going to focus\non some general practical tips",
    "start": "9180",
    "end": "12330"
  },
  {
    "text": "for doing work in this\nspace, especially focused",
    "start": "12330",
    "end": "14639"
  },
  {
    "text": "on setting up a\nproject and doing kind",
    "start": "14640",
    "end": "16850"
  },
  {
    "text": "of pre-processing of your data.",
    "start": "16850",
    "end": "19830"
  },
  {
    "text": "So first I just wanted\nto give you links",
    "start": "19830",
    "end": "21770"
  },
  {
    "text": "to a whole bunch of benchmark\ndata sets in the space.",
    "start": "21770",
    "end": "24650"
  },
  {
    "text": "We're going to concentrate\non the SST and DynaSent,",
    "start": "24650",
    "end": "27650"
  },
  {
    "text": "but there are a lot\nof other choices",
    "start": "27650",
    "end": "29150"
  },
  {
    "text": "you could make both for\ndeveloping original systems",
    "start": "29150",
    "end": "32180"
  },
  {
    "text": "and also supplementing\ntraining data",
    "start": "32180",
    "end": "34100"
  },
  {
    "text": "that you've got for a\nparticular application.",
    "start": "34100",
    "end": "36090"
  },
  {
    "text": "Some of these data sets\nare really, really large,",
    "start": "36090",
    "end": "38210"
  },
  {
    "text": "and they cover a\ndiversity of domains.",
    "start": "38210",
    "end": "40079"
  },
  {
    "text": "So these could be\nimportant assets for you.",
    "start": "40080",
    "end": "42980"
  },
  {
    "text": "In a similar spirit, there\nare lots of sentiment lexicons",
    "start": "42980",
    "end": "45559"
  },
  {
    "text": "out there.",
    "start": "45560",
    "end": "46310"
  },
  {
    "text": "They cover different\nemotional dimensions",
    "start": "46310",
    "end": "48320"
  },
  {
    "text": "and different aspects\nof the problem.",
    "start": "48320",
    "end": "50097"
  },
  {
    "text": "And they too could\nbe used to help you",
    "start": "50097",
    "end": "51680"
  },
  {
    "text": "with powerful featurization.",
    "start": "51680",
    "end": "53287"
  },
  {
    "text": "They could supplement\nfeatures that you've created",
    "start": "53287",
    "end": "55370"
  },
  {
    "text": "or help you group\nyour vocabulary",
    "start": "55370",
    "end": "57200"
  },
  {
    "text": "into interesting\nsubcategories that",
    "start": "57200",
    "end": "59540"
  },
  {
    "text": "would be powerful for making\nsentiment predictions.",
    "start": "59540",
    "end": "62070"
  },
  {
    "text": "And these range from\nsimple word lists up",
    "start": "62070",
    "end": "64250"
  },
  {
    "text": "to highly structured\nmulti-dimensional lexicons.",
    "start": "64250",
    "end": "68700"
  },
  {
    "text": "Now for a first\npre-processing step,",
    "start": "68700",
    "end": "70310"
  },
  {
    "text": "I thought we would just talk a\nlittle bit about tokenization",
    "start": "70310",
    "end": "72810"
  },
  {
    "text": "because I think that this can\nbe a definitional choice that",
    "start": "72810",
    "end": "75780"
  },
  {
    "text": "really affects\ndownstream success.",
    "start": "75780",
    "end": "78100"
  },
  {
    "text": "So just as a running\nexample here,",
    "start": "78100",
    "end": "79650"
  },
  {
    "text": "let's imagine we start\nwith this raw text, which",
    "start": "79650",
    "end": "81780"
  },
  {
    "text": "is a kind of imagined tweet.",
    "start": "81780",
    "end": "83850"
  },
  {
    "text": "We have an @ mention here.",
    "start": "83850",
    "end": "85466"
  },
  {
    "text": "And then you can see\nthat some of the markup",
    "start": "85467",
    "end": "87300"
  },
  {
    "text": "has gotten a little bit garbled.",
    "start": "87300",
    "end": "88650"
  },
  {
    "text": "We have an emoticon that\nlooks sort of obscured",
    "start": "88650",
    "end": "91590"
  },
  {
    "text": "and a link at the end.",
    "start": "91590",
    "end": "93539"
  },
  {
    "text": "I think as a very\npreliminary step,",
    "start": "93540",
    "end": "95640"
  },
  {
    "text": "even before we\ntokenize it, we might",
    "start": "95640",
    "end": "97200"
  },
  {
    "text": "want to isolate\nsome of that markup",
    "start": "97200",
    "end": "98700"
  },
  {
    "text": "and replace the HTML entities.",
    "start": "98700",
    "end": "100774"
  },
  {
    "text": "It's a pretty easy\nthing that you",
    "start": "100775",
    "end": "102150"
  },
  {
    "text": "can do that could really\nmake a difference.",
    "start": "102150",
    "end": "103942"
  },
  {
    "text": "Now we've got an apostrophe.",
    "start": "103942",
    "end": "105660"
  },
  {
    "text": "We've got our emoticon intact.",
    "start": "105660",
    "end": "107460"
  },
  {
    "text": "And we still have the link\nand other things in here.",
    "start": "107460",
    "end": "110507"
  },
  {
    "text": "So even before you\ndo that, you might",
    "start": "110508",
    "end": "112050"
  },
  {
    "text": "check to see whether a simple\nreplacement of the HTML",
    "start": "112050",
    "end": "115350"
  },
  {
    "text": "entities would make a\ndifference in your data.",
    "start": "115350",
    "end": "118290"
  },
  {
    "start": "118000",
    "end": "164000"
  },
  {
    "text": "Now we begin the\ntokenization question.",
    "start": "118290",
    "end": "120115"
  },
  {
    "text": "And I think a good\nbaseline choice here",
    "start": "120115",
    "end": "121740"
  },
  {
    "text": "would be simply\nwhitespace tokenizing.",
    "start": "121740",
    "end": "123350"
  },
  {
    "text": "I think we're going\nto split on whitespace",
    "start": "123350",
    "end": "125100"
  },
  {
    "text": "and treat all the resulting\nstrings as tokens.",
    "start": "125100",
    "end": "127799"
  },
  {
    "text": "So that would take our raw\ntext up here and split it up.",
    "start": "127800",
    "end": "130580"
  },
  {
    "text": "As you can see on these\nindependent lines,",
    "start": "130580",
    "end": "133080"
  },
  {
    "text": "this looks OK to me.",
    "start": "133080",
    "end": "134483"
  },
  {
    "text": "So we're going to have a\nproblem with our @ mention,",
    "start": "134483",
    "end": "136650"
  },
  {
    "text": "because it has this\ncolon on the end.",
    "start": "136650",
    "end": "138269"
  },
  {
    "text": "So we might miss the fact that\nthis is the actual @ mention.",
    "start": "138270",
    "end": "142500"
  },
  {
    "text": "The unigrams look OK, although\nthe date has been split apart.",
    "start": "142500",
    "end": "145900"
  },
  {
    "text": "We've preserved our hashtag.",
    "start": "145900",
    "end": "147659"
  },
  {
    "text": "We've got this token that\nmight appear only once even",
    "start": "147660",
    "end": "150450"
  },
  {
    "text": "though there's a clear\nconsistent signal there.",
    "start": "150450",
    "end": "152940"
  },
  {
    "text": "We do have our emoticon.",
    "start": "152940",
    "end": "154380"
  },
  {
    "text": "And our link is mostly\nintact although this period",
    "start": "154380",
    "end": "157620"
  },
  {
    "text": "could be disruptive\nif we actually",
    "start": "157620",
    "end": "159060"
  },
  {
    "text": "want to follow the link,\nbecause it could still",
    "start": "159060",
    "end": "161017"
  },
  {
    "text": "go on onto the end of the URL.",
    "start": "161018",
    "end": "164340"
  },
  {
    "start": "164000",
    "end": "215000"
  },
  {
    "text": "Treebank tokenizing is another\nvery common scheme in NLP",
    "start": "164340",
    "end": "167099"
  },
  {
    "text": "I would say at this point\nlargely for historical reasons.",
    "start": "167100",
    "end": "170700"
  },
  {
    "text": "The way treebank tokenizing\nworks is it takes this raw text",
    "start": "170700",
    "end": "173489"
  },
  {
    "text": "and splits it up into a\nwhole lot of tokens, right?",
    "start": "173490",
    "end": "175865"
  },
  {
    "text": "In comparison with\nwhitespace, we",
    "start": "175865",
    "end": "177240"
  },
  {
    "text": "have a lot of\ndistinct pieces here.",
    "start": "177240",
    "end": "179220"
  },
  {
    "text": "And this really looks kind\nof problematic, right?",
    "start": "179220",
    "end": "181660"
  },
  {
    "text": "So we have destroyed\nour @ mention,",
    "start": "181660",
    "end": "183840"
  },
  {
    "text": "we don't have that\nusername anymore.",
    "start": "183840",
    "end": "186269"
  },
  {
    "text": "It does this interesting\nthing with words like \"can\"",
    "start": "186270",
    "end": "188910"
  },
  {
    "text": "that they get split\napart into two tokens.",
    "start": "188910",
    "end": "191970"
  },
  {
    "text": "We've lost our date.",
    "start": "191970",
    "end": "193140"
  },
  {
    "text": "We have lost our hashtag.",
    "start": "193140",
    "end": "195420"
  },
  {
    "text": "This is possibly good.",
    "start": "195420",
    "end": "196440"
  },
  {
    "text": "So YAAAAAAY has been split up\naccording to its punctuation.",
    "start": "196440",
    "end": "199530"
  },
  {
    "text": "So we have now four\nexclamation marks",
    "start": "199530",
    "end": "202110"
  },
  {
    "text": "separated out from\nthis word here.",
    "start": "202110",
    "end": "204480"
  },
  {
    "text": "But our emoticon\nis completely lost",
    "start": "204480",
    "end": "206405"
  },
  {
    "text": "and our link has been\nreally destroyed.",
    "start": "206405",
    "end": "208030"
  },
  {
    "text": "So this looks problematic\nfrom the point of view",
    "start": "208030",
    "end": "210090"
  },
  {
    "text": "of accurate featurization\nand also doing things",
    "start": "210090",
    "end": "213569"
  },
  {
    "text": "with social media.",
    "start": "213570",
    "end": "215705"
  },
  {
    "start": "215000",
    "end": "337000"
  },
  {
    "text": "So that kind of\nbrings me to what",
    "start": "215705",
    "end": "217080"
  },
  {
    "text": "we might want from\nwhat I've called",
    "start": "217080",
    "end": "218538"
  },
  {
    "text": "the sentiment-aware tokenizer.",
    "start": "218538",
    "end": "220260"
  },
  {
    "text": "We would like to isolate\nemoticons clearly because they",
    "start": "220260",
    "end": "223019"
  },
  {
    "text": "can be really sentimentally--",
    "start": "223020",
    "end": "225300"
  },
  {
    "text": "We want to probably respect\nTwitter and other domain",
    "start": "225300",
    "end": "227760"
  },
  {
    "text": "specific markup because that's\noften the space in which--",
    "start": "227760",
    "end": "230819"
  },
  {
    "text": "our data come from,\nthe kind of place",
    "start": "230820",
    "end": "232560"
  },
  {
    "text": "we want to make predictions in.",
    "start": "232560",
    "end": "234522"
  },
  {
    "text": "In a similar spirit,\nyou might take",
    "start": "234522",
    "end": "235980"
  },
  {
    "text": "advantage of underlying markup.",
    "start": "235980",
    "end": "237510"
  },
  {
    "text": "Maybe don't filter off the\nHTML, because there could",
    "start": "237510",
    "end": "239760"
  },
  {
    "text": "be an important signal there.",
    "start": "239760",
    "end": "241680"
  },
  {
    "text": "You might be aware that the\nwebsite or data producer might",
    "start": "241680",
    "end": "245239"
  },
  {
    "text": "have done some\npre-processing of their own",
    "start": "245240",
    "end": "246990"
  },
  {
    "text": "that might disrupt\nthings like curses which",
    "start": "246990",
    "end": "249350"
  },
  {
    "text": "could of course carry a\nlot of important sentiment",
    "start": "249350",
    "end": "252000"
  },
  {
    "text": "information.",
    "start": "252000",
    "end": "253320"
  },
  {
    "text": "You might want to\npreserve capitalization",
    "start": "253320",
    "end": "255120"
  },
  {
    "text": "because of course that\ncould be used for emphasis.",
    "start": "255120",
    "end": "257591"
  },
  {
    "text": "In a similar spirit,\nyou might want",
    "start": "257592",
    "end": "259049"
  },
  {
    "text": "to regularize emotive\nlengthening like YAAAAAY down",
    "start": "259050",
    "end": "262770"
  },
  {
    "text": "to just three characters\nhere to capture",
    "start": "262770",
    "end": "264840"
  },
  {
    "text": "that it is an\nemotive lengthening",
    "start": "264840",
    "end": "266520"
  },
  {
    "text": "but also regularize all\nthose distinct tokens.",
    "start": "266520",
    "end": "269729"
  },
  {
    "text": "And then there's a stretch goal.",
    "start": "269730",
    "end": "271120"
  },
  {
    "text": "Although this might be\nless important in the era",
    "start": "271120",
    "end": "273120"
  },
  {
    "text": "of contextual models,\nyou might think",
    "start": "273120",
    "end": "275160"
  },
  {
    "text": "about capturing multiword\nexpressions that",
    "start": "275160",
    "end": "277410"
  },
  {
    "text": "carry sentiment.",
    "start": "277410",
    "end": "278370"
  },
  {
    "text": "Just think of an example\nlike \"out of this world,\"",
    "start": "278370",
    "end": "280949"
  },
  {
    "text": "which is positive.",
    "start": "280950",
    "end": "282340"
  },
  {
    "text": "But none of its component\npieces are positive.",
    "start": "282340",
    "end": "284400"
  },
  {
    "text": "So many models\nwill miss that that",
    "start": "284400",
    "end": "285990"
  },
  {
    "text": "is conveying clear\nsentiment, whereas",
    "start": "285990",
    "end": "288030"
  },
  {
    "text": "with a clever\ntokenization scheme,",
    "start": "288030",
    "end": "290190"
  },
  {
    "text": "you might capture that\nas one single token.",
    "start": "290190",
    "end": "294300"
  },
  {
    "text": "So here's a simple example\nthat meets a lot of those goals",
    "start": "294300",
    "end": "297330"
  },
  {
    "text": "here for a\nsentiment-aware tokenizer.",
    "start": "297330",
    "end": "299370"
  },
  {
    "text": "We begin from our\nusual raw text.",
    "start": "299370",
    "end": "301110"
  },
  {
    "text": "We normalize and\npreserve the @ mention.",
    "start": "301110",
    "end": "304050"
  },
  {
    "text": "We keep most of these words\nintact and we kind of capture",
    "start": "304050",
    "end": "306750"
  },
  {
    "text": "that June 9 thing was a date.",
    "start": "306750",
    "end": "309180"
  },
  {
    "text": "Preserve the hashtag, of course.",
    "start": "309180",
    "end": "311400"
  },
  {
    "text": "We're treating all these\npotentially emotion-laden",
    "start": "311400",
    "end": "314669"
  },
  {
    "text": "punctuation marks as\nseparate unigrams.",
    "start": "314670",
    "end": "316740"
  },
  {
    "text": "I think that could be good.",
    "start": "316740",
    "end": "318360"
  },
  {
    "text": "Of course capture the emoticon\nand capture the link, right?",
    "start": "318360",
    "end": "321629"
  },
  {
    "text": "And if you want something\nthat meets more or less all",
    "start": "321630",
    "end": "323850"
  },
  {
    "text": "these criteria, except I\nthink the date normalization,",
    "start": "323850",
    "end": "326850"
  },
  {
    "text": "you could just use the\nnltk TweetTokenizer.",
    "start": "326850",
    "end": "329970"
  },
  {
    "text": "It's a good simple\nchoice that you",
    "start": "329970",
    "end": "331710"
  },
  {
    "text": "could make that I think will be\nuseful for sentiment analysis.",
    "start": "331710",
    "end": "335699"
  },
  {
    "text": "And to quantify that\na little bit, here's",
    "start": "335700",
    "end": "337885"
  },
  {
    "start": "337000",
    "end": "431000"
  },
  {
    "text": "some experimental\nevidence that I think",
    "start": "337885",
    "end": "339510"
  },
  {
    "text": "is going to be relevant\nto the kind of work",
    "start": "339510",
    "end": "341302"
  },
  {
    "text": "that you all are doing.",
    "start": "341302",
    "end": "342730"
  },
  {
    "text": "So my data is OpenTable.",
    "start": "342730",
    "end": "344370"
  },
  {
    "text": "That's restaurant\nreview, short ones.",
    "start": "344370",
    "end": "346590"
  },
  {
    "text": "I've got 6,000 reviews\nin my test set.",
    "start": "346590",
    "end": "349500"
  },
  {
    "text": "And what I'm doing\nalong the x-axis",
    "start": "349500",
    "end": "351420"
  },
  {
    "text": "here is varying the\namount of training data",
    "start": "351420",
    "end": "353460"
  },
  {
    "text": "that these systems can see.",
    "start": "353460",
    "end": "355289"
  },
  {
    "text": "It's simply a Softmax classifier\nand my primary manipulation",
    "start": "355290",
    "end": "358950"
  },
  {
    "text": "is I have the sentiment-aware\ntokenizer in orange,",
    "start": "358950",
    "end": "361950"
  },
  {
    "text": "treebank in green, and\nwhitespace in gray.",
    "start": "361950",
    "end": "365430"
  },
  {
    "text": "And the picture is pretty clear.",
    "start": "365430",
    "end": "367229"
  },
  {
    "text": "Right along the x-axis,\nwe have accuracy.",
    "start": "367230",
    "end": "369750"
  },
  {
    "text": "It's a balanced problem.",
    "start": "369750",
    "end": "371650"
  },
  {
    "text": "And what you can see is that\nthe sentiment-aware tokenizer",
    "start": "371650",
    "end": "374550"
  },
  {
    "text": "is the clear winner\nhere, especially where",
    "start": "374550",
    "end": "377169"
  },
  {
    "text": "training data are sparse.",
    "start": "377170",
    "end": "379200"
  },
  {
    "text": "In the limit of adding\nlots of training data,",
    "start": "379200",
    "end": "381280"
  },
  {
    "text": "I think we can make up for a lot\nof shortcomings of tokenizers",
    "start": "381280",
    "end": "384257"
  },
  {
    "text": "because we see a lot of\nredundancy in the training",
    "start": "384257",
    "end": "386340"
  },
  {
    "text": "data.",
    "start": "386340",
    "end": "387419"
  },
  {
    "text": "But where data are sparse,\nthe sentiment-aware tokenizer",
    "start": "387420",
    "end": "390090"
  },
  {
    "text": "is clearly a good choice.",
    "start": "390090",
    "end": "391230"
  },
  {
    "text": "And another thing I would add\nis that because it produces",
    "start": "391230",
    "end": "394230"
  },
  {
    "text": "more intuitive tokens,\nthe sentiment-aware models",
    "start": "394230",
    "end": "396690"
  },
  {
    "text": "might be more interpretable\nin some sense.",
    "start": "396690",
    "end": "400800"
  },
  {
    "text": "And to really connect\nwith the homework",
    "start": "400800",
    "end": "402509"
  },
  {
    "text": "that you all are\ndoing in the bakeoff,",
    "start": "402510",
    "end": "404400"
  },
  {
    "text": "this is what happens when\nwe go across domains.",
    "start": "404400",
    "end": "406630"
  },
  {
    "text": "So here I'm training on\nOpenTable restaurant reviews,",
    "start": "406630",
    "end": "409530"
  },
  {
    "text": "but I'm going to test on\nmovie review sentences here.",
    "start": "409530",
    "end": "412230"
  },
  {
    "text": "Otherwise, this is the\nsame experimental paradigm.",
    "start": "412230",
    "end": "414960"
  },
  {
    "text": "Because of the\ncross domain thing,",
    "start": "414960",
    "end": "416490"
  },
  {
    "text": "the results are a\nlittle bit more chaotic.",
    "start": "416490",
    "end": "419340"
  },
  {
    "text": "But I think, again, the\nsentiment-aware tokenizer",
    "start": "419340",
    "end": "421740"
  },
  {
    "text": "is a clear winner with the\nlargest gains where training",
    "start": "421740",
    "end": "424860"
  },
  {
    "text": "data are a little bit sparse.",
    "start": "424860",
    "end": "426479"
  },
  {
    "text": "And that's the expected picture.",
    "start": "426480",
    "end": "429490"
  },
  {
    "text": "So be thoughtful\nabout tokenizing.",
    "start": "429490",
    "end": "431067"
  },
  {
    "start": "431000",
    "end": "530000"
  },
  {
    "text": "As a counterpoint to that,\nI've called the section",
    "start": "431067",
    "end": "433150"
  },
  {
    "text": "on stemming the\ndangers of stemming",
    "start": "433150",
    "end": "434770"
  },
  {
    "text": "because what I want to try\nto do is convince you not",
    "start": "434770",
    "end": "436960"
  },
  {
    "text": "to stem your data, the\nfirst word is stemming.",
    "start": "436960",
    "end": "439669"
  },
  {
    "text": "So stemming is a kind of\npre-processing technique",
    "start": "439670",
    "end": "442120"
  },
  {
    "text": "that would collapse\ndistinct word forms.",
    "start": "442120",
    "end": "445180"
  },
  {
    "text": "There are three common\nalgorithms for this.",
    "start": "445180",
    "end": "447520"
  },
  {
    "text": "Easy to use, the Porter stemmer.",
    "start": "447520",
    "end": "449740"
  },
  {
    "text": "The Lancaster stemmer,\nand the WordNet stemmer.",
    "start": "449740",
    "end": "452169"
  },
  {
    "text": "And my criticisms\nare largely leveled",
    "start": "452170",
    "end": "454390"
  },
  {
    "text": "at Porter and Lancaster.",
    "start": "454390",
    "end": "456340"
  },
  {
    "text": "Here is the bottom line.",
    "start": "456340",
    "end": "458050"
  },
  {
    "text": "In doing this kind\nof stemming, you",
    "start": "458050",
    "end": "459879"
  },
  {
    "text": "are apt to destroy many\nimportant sentiment",
    "start": "459880",
    "end": "462220"
  },
  {
    "text": "distinctions, making\nthis a counterproductive",
    "start": "462220",
    "end": "464950"
  },
  {
    "text": "pre-processing step.",
    "start": "464950",
    "end": "466840"
  },
  {
    "text": "On the other hand,\nthe WordNet stemmer",
    "start": "466840",
    "end": "468490"
  },
  {
    "text": "does not have this problem.",
    "start": "468490",
    "end": "470509"
  },
  {
    "text": "It's much more conservative,\nbut it also doesn't really",
    "start": "470510",
    "end": "473470"
  },
  {
    "text": "do enough to make it worthwhile.",
    "start": "473470",
    "end": "475630"
  },
  {
    "text": "It's costly to run and has some\nrequirements that might make it",
    "start": "475630",
    "end": "479140"
  },
  {
    "text": "simply not worth it.",
    "start": "479140",
    "end": "480430"
  },
  {
    "text": "And I would say that the\nbottom line here for stemming",
    "start": "480430",
    "end": "482680"
  },
  {
    "text": "is that in an era where we have\nvery large sentiment data sets,",
    "start": "482680",
    "end": "486460"
  },
  {
    "text": "the function of\nstemming would be",
    "start": "486460",
    "end": "487840"
  },
  {
    "text": "to collapse the size\nof your vocabulary",
    "start": "487840",
    "end": "490449"
  },
  {
    "text": "and make learning more\neasier in small domains,",
    "start": "490450",
    "end": "493450"
  },
  {
    "text": "but we mostly don't confront\nthat problem anymore.",
    "start": "493450",
    "end": "496730"
  },
  {
    "text": "But just to drive\nhome this point here,",
    "start": "496730",
    "end": "498910"
  },
  {
    "text": "here are some examples\nfocused on the Porter stemmer,",
    "start": "498910",
    "end": "501970"
  },
  {
    "text": "of cases where running\nthe Porter stemmer",
    "start": "501970",
    "end": "504070"
  },
  {
    "text": "actually collapses clear\nsentiment distinctions",
    "start": "504070",
    "end": "506620"
  },
  {
    "text": "according to the\nHarvard Inquirer, which",
    "start": "506620",
    "end": "508540"
  },
  {
    "text": "is one of those lexicons\nI mentioned before.",
    "start": "508540",
    "end": "511270"
  },
  {
    "text": "I got defense and defensive.",
    "start": "511270",
    "end": "513020"
  },
  {
    "text": "They get collapsed down into\nthis funny, non-word defens.",
    "start": "513020",
    "end": "516340"
  },
  {
    "text": "Extravagance and extravagant,\ndifferent sentiment",
    "start": "516340",
    "end": "519250"
  },
  {
    "text": "collapsed down into this\nword fragment, and so forth",
    "start": "519250",
    "end": "522700"
  },
  {
    "text": "for these other examples.",
    "start": "522700",
    "end": "523830"
  },
  {
    "text": "I think this is showing that\nin pre-processing your data,",
    "start": "523830",
    "end": "526450"
  },
  {
    "text": "you might be removing some\nimportant sentiment signals.",
    "start": "526450",
    "end": "530170"
  },
  {
    "start": "530000",
    "end": "600000"
  },
  {
    "text": "The Lancaster stemmer uses\na very similar strategy",
    "start": "530170",
    "end": "532810"
  },
  {
    "text": "and has arguably even more\nproblems in this space.",
    "start": "532810",
    "end": "535810"
  },
  {
    "text": "Here we've got the positive\nword complement and complicate.",
    "start": "535810",
    "end": "539890"
  },
  {
    "text": "According to the\nHarvard Inquirer again,",
    "start": "539890",
    "end": "541563"
  },
  {
    "text": "they could both\nget collapsed down",
    "start": "541563",
    "end": "542980"
  },
  {
    "text": "into what is a completely\ndistinct word, comply.",
    "start": "542980",
    "end": "546250"
  },
  {
    "text": "That should be concerning\nfor many reasons.",
    "start": "546250",
    "end": "548450"
  },
  {
    "text": "And the other examples\nmake a very similar point.",
    "start": "548450",
    "end": "552568"
  },
  {
    "text": "The WordNet stemmer I mentioned\nbefore, I think this actually",
    "start": "552568",
    "end": "555110"
  },
  {
    "text": "has something going for it.",
    "start": "555110",
    "end": "556620"
  },
  {
    "text": "There might be cases where\nyou'd want to use it.",
    "start": "556620",
    "end": "558620"
  },
  {
    "text": "It's high precision.",
    "start": "558620",
    "end": "559880"
  },
  {
    "text": "It requires word,\npart of speech pairs.",
    "start": "559880",
    "end": "563210"
  },
  {
    "text": "And the general issue\nis just that it removes",
    "start": "563210",
    "end": "565340"
  },
  {
    "text": "some comparative morphology.",
    "start": "565340",
    "end": "566673"
  },
  {
    "text": "That's the only thing you might\nworry about for sentiment.",
    "start": "566673",
    "end": "569089"
  },
  {
    "text": "But otherwise it's going to\ntake like exclaims, explained,",
    "start": "569090",
    "end": "571850"
  },
  {
    "text": "and exclaiming, and\ncollapse them down.",
    "start": "571850",
    "end": "573680"
  },
  {
    "text": "That could be a\nuseful compression",
    "start": "573680",
    "end": "575480"
  },
  {
    "text": "of your feature space.",
    "start": "575480",
    "end": "576829"
  },
  {
    "text": "It will leave exclamation\nalone, which I think is good.",
    "start": "576830",
    "end": "580110"
  },
  {
    "text": "Similarly for these\nthings, they all",
    "start": "580110",
    "end": "581810"
  },
  {
    "text": "get preserved across\nthe two verb forms",
    "start": "581810",
    "end": "584120"
  },
  {
    "text": "but we preserve the\nadjective is different.",
    "start": "584120",
    "end": "585930"
  },
  {
    "text": "I think that could be good.",
    "start": "585930",
    "end": "587490"
  },
  {
    "text": "And as I said, the only concern\nwould be that happy, happier,",
    "start": "587490",
    "end": "590180"
  },
  {
    "text": "and happiest all go down\ninto their base form",
    "start": "590180",
    "end": "592910"
  },
  {
    "text": "whereas I think\nthese could encode",
    "start": "592910",
    "end": "594829"
  },
  {
    "text": "different gradations\nof sentiment",
    "start": "594830",
    "end": "596510"
  },
  {
    "text": "that you might want to preserve.",
    "start": "596510",
    "end": "598710"
  },
  {
    "text": "That's worth some\nthought, but overall I",
    "start": "598710",
    "end": "600560"
  },
  {
    "start": "600000",
    "end": "638000"
  },
  {
    "text": "think you probably want\nto avoid doing stemming.",
    "start": "600560",
    "end": "602840"
  },
  {
    "text": "And to bring that\nhome, let's return",
    "start": "602840",
    "end": "604340"
  },
  {
    "text": "to my experimental\nparadigm using",
    "start": "604340",
    "end": "606590"
  },
  {
    "text": "a Softmax classifier, OpenTable\nreviews, 6,000 of them",
    "start": "606590",
    "end": "610550"
  },
  {
    "text": "in my test set, and\nhere along the x-axis,",
    "start": "610550",
    "end": "613040"
  },
  {
    "text": "I'm varying the amount\nof training data I have.",
    "start": "613040",
    "end": "615740"
  },
  {
    "text": "And I think what you see is\nthat the Porter and Lancaster",
    "start": "615740",
    "end": "619040"
  },
  {
    "text": "stemmer in purple and\nblack respectively",
    "start": "619040",
    "end": "621350"
  },
  {
    "text": "are kind of forever behind,\nright, versus just simply",
    "start": "621350",
    "end": "624680"
  },
  {
    "text": "sentiment-aware tokenizing.",
    "start": "624680",
    "end": "626210"
  },
  {
    "text": "It gives you a lead.",
    "start": "626210",
    "end": "627530"
  },
  {
    "text": "The lead is especially\nclear as you",
    "start": "627530",
    "end": "629300"
  },
  {
    "text": "get out of this very\nsparse domain here",
    "start": "629300",
    "end": "632060"
  },
  {
    "text": "with very few\ntraining instances.",
    "start": "632060",
    "end": "634850"
  },
  {
    "text": "To close, just a few\nother pre-processing",
    "start": "634850",
    "end": "637316"
  },
  {
    "text": "techniques that you\nmight think about.",
    "start": "637317",
    "end": "638900"
  },
  {
    "start": "638000",
    "end": "679000"
  },
  {
    "text": "So you could part-of-speech\ntag your data",
    "start": "638900",
    "end": "641150"
  },
  {
    "text": "in the spirit of trying\nto capture more sentiment",
    "start": "641150",
    "end": "643730"
  },
  {
    "text": "distinctions that you\nmight capture otherwise.",
    "start": "643730",
    "end": "645810"
  },
  {
    "text": "So just for example,\narrest, like",
    "start": "645810",
    "end": "648020"
  },
  {
    "text": "arresting as an adjective is\npositive, but arrest as a verb",
    "start": "648020",
    "end": "650990"
  },
  {
    "text": "is typically negative.",
    "start": "650990",
    "end": "652910"
  },
  {
    "text": "Fine as an adjective\nis positive,",
    "start": "652910",
    "end": "655339"
  },
  {
    "text": "but to incur a fine as a noun\nis negative, and so forth.",
    "start": "655340",
    "end": "658800"
  },
  {
    "text": "You can see that some\nsentiment distinctions actually",
    "start": "658800",
    "end": "661040"
  },
  {
    "text": "do turn on the part\nof speech of the word.",
    "start": "661040",
    "end": "663920"
  },
  {
    "text": "So treating all of your unigram\nfeatures as based in word",
    "start": "663920",
    "end": "668389"
  },
  {
    "text": "part-of-speech tag pairs\ncould be useful for preserving",
    "start": "668390",
    "end": "671720"
  },
  {
    "text": "some of these distinctions.",
    "start": "671720",
    "end": "673009"
  },
  {
    "text": "Again as a pre-processing\nstep to help your model",
    "start": "673010",
    "end": "675680"
  },
  {
    "text": "be more attuned to these points\nof variation in comparison.",
    "start": "675680",
    "end": "679830"
  },
  {
    "start": "679000",
    "end": "725000"
  },
  {
    "text": "But there are limits\neven to this, right?",
    "start": "679830",
    "end": "681740"
  },
  {
    "text": "So there is just some\ncases on the slides",
    "start": "681740",
    "end": "683600"
  },
  {
    "text": "where even within the\nsame part of speech,",
    "start": "683600",
    "end": "685850"
  },
  {
    "text": "we have an adjective that\nin one sense is positive",
    "start": "685850",
    "end": "688880"
  },
  {
    "text": "and another negative.",
    "start": "688880",
    "end": "690240"
  },
  {
    "text": "For example, the adjective\nmean can mean hateful,",
    "start": "690240",
    "end": "693649"
  },
  {
    "text": "but it can also mean excellent\nas in they make a mean apple",
    "start": "693650",
    "end": "696890"
  },
  {
    "text": "pie.",
    "start": "696890",
    "end": "698750"
  },
  {
    "text": "Smart as an adjective could be\nboth painful and also bright",
    "start": "698750",
    "end": "704510"
  },
  {
    "text": "and brilliant, and\nso forth like that.",
    "start": "704510",
    "end": "706230"
  },
  {
    "text": "And similarly for serious\nand fantastic and sneer,",
    "start": "706230",
    "end": "709050"
  },
  {
    "text": "depending on the context and\nthe intention of the speaker,",
    "start": "709050",
    "end": "711740"
  },
  {
    "text": "they can kind of cut in\ndifferent directions.",
    "start": "711740",
    "end": "714895"
  },
  {
    "text": "So even part-of-speech\ntagging is",
    "start": "714895",
    "end": "716270"
  },
  {
    "text": "going to be limiting when it\ncomes to really recovering",
    "start": "716270",
    "end": "719150"
  },
  {
    "text": "the underlying word\nsense, even for something",
    "start": "719150",
    "end": "721910"
  },
  {
    "text": "as low dimensional as a\nsentiment distinction.",
    "start": "721910",
    "end": "725940"
  },
  {
    "start": "725000",
    "end": "842000"
  },
  {
    "text": "Finally, this is another\npowerful technique",
    "start": "725940",
    "end": "728550"
  },
  {
    "text": "that you might use\nand think about",
    "start": "728550",
    "end": "729990"
  },
  {
    "text": "as you select and\nevaluate different models.",
    "start": "729990",
    "end": "733270"
  },
  {
    "text": "This is what I've called\nsimple negation mark.",
    "start": "733270",
    "end": "735210"
  },
  {
    "text": "And the phenomenon\nis just that if I",
    "start": "735210",
    "end": "737310"
  },
  {
    "text": "have a verb like enjoy, which\nsounds positive in isolation,",
    "start": "737310",
    "end": "741570"
  },
  {
    "text": "of course its contribution\nto the overall sentiment",
    "start": "741570",
    "end": "743850"
  },
  {
    "text": "will change depending\non whether it's",
    "start": "743850",
    "end": "745769"
  },
  {
    "text": "in the scope of a negation.",
    "start": "745770",
    "end": "747480"
  },
  {
    "text": "\"I didn't enjoy\" is negative.",
    "start": "747480",
    "end": "749610"
  },
  {
    "text": "A negation can be\nexpressed in many ways",
    "start": "749610",
    "end": "752700"
  },
  {
    "text": "as this modifier of\nauxiliaries, like not.",
    "start": "752700",
    "end": "755670"
  },
  {
    "text": "But as an adverb like never,\nit could be in the subject",
    "start": "755670",
    "end": "758730"
  },
  {
    "text": "like \"No one.\"",
    "start": "758730",
    "end": "759449"
  },
  {
    "text": "And it could even be\nreally encoded for things",
    "start": "759450",
    "end": "761610"
  },
  {
    "text": "like \"I have yet to enjoy it,\"\nwhich is a kind of negation.",
    "start": "761610",
    "end": "765390"
  },
  {
    "text": "And then of course the\nnegation in 5 here is very far.",
    "start": "765390",
    "end": "768330"
  },
  {
    "text": "\"I don't think I will enjoy\nit,\" is probably negative,",
    "start": "768330",
    "end": "771270"
  },
  {
    "text": "but the negation is way far away\nfrom the verb that we want to--",
    "start": "771270",
    "end": "777260"
  },
  {
    "text": "with sentiment we want to\nmodulate with a negation.",
    "start": "777260",
    "end": "781110"
  },
  {
    "text": "So here's a very simple\nmethod that I think was first",
    "start": "781110",
    "end": "784019"
  },
  {
    "text": "explored by Das and Chen.",
    "start": "784020",
    "end": "785280"
  },
  {
    "text": "It's also used in Pang et al.",
    "start": "785280",
    "end": "786760"
  },
  {
    "text": "These are classic early\nsentiment analysis papers.",
    "start": "786760",
    "end": "789390"
  },
  {
    "text": "And the idea is simply to append\na _NEG suffix to every word",
    "start": "789390",
    "end": "793410"
  },
  {
    "text": "in the sequence that appears\nbetween the negation and some",
    "start": "793410",
    "end": "796529"
  },
  {
    "text": "clause level mark\nof punctuation,",
    "start": "796530",
    "end": "798390"
  },
  {
    "text": "to sort of roughly indicate the\nsemantic scope of the negation.",
    "start": "798390",
    "end": "802530"
  },
  {
    "text": "This is a simple pre-processing\nstep, highly heuristic.",
    "start": "802530",
    "end": "805650"
  },
  {
    "text": "It would take a sentence\nlike \"No one enjoys it,\"",
    "start": "805650",
    "end": "808050"
  },
  {
    "text": "and literally turn the\nunigrams one, enjoys,",
    "start": "808050",
    "end": "811529"
  },
  {
    "text": "and it into variant\nforms of them",
    "start": "811530",
    "end": "814125"
  },
  {
    "text": "where one has a NEG\nappended to it, and so does",
    "start": "814125",
    "end": "816390"
  },
  {
    "text": "enjoys, and so does it.",
    "start": "816390",
    "end": "817752"
  },
  {
    "text": "And the idea is\nthat in doing this,",
    "start": "817752",
    "end": "819210"
  },
  {
    "text": "we're giving our\nmodel the opportunity",
    "start": "819210",
    "end": "821070"
  },
  {
    "text": "to discover that enjoys in\nthis context is actually",
    "start": "821070",
    "end": "824610"
  },
  {
    "text": "a different token in some\nsense than enjoys when it's not",
    "start": "824610",
    "end": "828000"
  },
  {
    "text": "in the scope of negation.",
    "start": "828000",
    "end": "829470"
  },
  {
    "text": "And for many of the\nlinear models with handout",
    "start": "829470",
    "end": "831750"
  },
  {
    "text": "features that we explore, simply\nmaking that initial distinction",
    "start": "831750",
    "end": "835200"
  },
  {
    "text": "might create some\nspace for your model",
    "start": "835200",
    "end": "837420"
  },
  {
    "text": "to learn the\ninteraction of negation",
    "start": "837420",
    "end": "839790"
  },
  {
    "text": "with these other features.",
    "start": "839790",
    "end": "841980"
  },
  {
    "text": "And just to quantify it a little\nbit by way of rounding this",
    "start": "841980",
    "end": "844769"
  },
  {
    "start": "842000",
    "end": "921000"
  },
  {
    "text": "out, I think this slide shows\nthe impact that this can",
    "start": "844770",
    "end": "847110"
  },
  {
    "text": "have despite its simplicity.",
    "start": "847110",
    "end": "849000"
  },
  {
    "text": "So similar, we have\nOpenTable as our test set.",
    "start": "849000",
    "end": "852720"
  },
  {
    "text": "We're using a\nSoftmax classifier.",
    "start": "852720",
    "end": "854399"
  },
  {
    "text": "And the x-axis is again varying\nthe amount of training data",
    "start": "854400",
    "end": "857190"
  },
  {
    "text": "that we have.",
    "start": "857190",
    "end": "859230"
  },
  {
    "text": "The whitespace\ntokenizer is in gray.",
    "start": "859230",
    "end": "861120"
  },
  {
    "text": "It's the worst, followed\nby Treebank in green.",
    "start": "861120",
    "end": "863880"
  },
  {
    "text": "Then we have that\nsentiment-aware tokenizer",
    "start": "863880",
    "end": "865920"
  },
  {
    "text": "in orange.",
    "start": "865920",
    "end": "867240"
  },
  {
    "text": "And then way above\nthem, consistently",
    "start": "867240",
    "end": "869520"
  },
  {
    "text": "for all parts of the data\nhere are sentiment-aware",
    "start": "869520",
    "end": "872100"
  },
  {
    "text": "plus that negation marking.",
    "start": "872100",
    "end": "874230"
  },
  {
    "text": "That is obviously\nthe superior model",
    "start": "874230",
    "end": "876480"
  },
  {
    "text": "for all kinds of amounts\nof training data.",
    "start": "876480",
    "end": "878753"
  },
  {
    "text": "And I think what\nthat's showing is",
    "start": "878753",
    "end": "880170"
  },
  {
    "text": "that the influence of\nnegation is actually",
    "start": "880170",
    "end": "882810"
  },
  {
    "text": "really real and severe in a\nlot of sentiment datasets.",
    "start": "882810",
    "end": "885730"
  },
  {
    "text": "It's just very common to\ncombine sentiment words,",
    "start": "885730",
    "end": "888959"
  },
  {
    "text": "positive or negative,\nwith negation",
    "start": "888960",
    "end": "891270"
  },
  {
    "text": "and it has this\npredictable effect",
    "start": "891270",
    "end": "892890"
  },
  {
    "text": "of kind of flipping the value.",
    "start": "892890",
    "end": "895000"
  },
  {
    "text": "So in doing this sentiment, and\ndoing this negation marking,",
    "start": "895000",
    "end": "897810"
  },
  {
    "text": "we're giving our\nmodel a better chance",
    "start": "897810",
    "end": "900300"
  },
  {
    "text": "at discovering exactly\nthose distinctions.",
    "start": "900300",
    "end": "903029"
  },
  {
    "text": "And here's a similar set\nof results for cross-domain",
    "start": "903030",
    "end": "905700"
  },
  {
    "text": "where I'm starting on\nOpenTable and testing on IMDB.",
    "start": "905700",
    "end": "909070"
  },
  {
    "text": "Again the results are a\nlittle bit more chaotic,",
    "start": "909070",
    "end": "911070"
  },
  {
    "text": "but I think it's a clear win\nfor the sentiment-aware plus",
    "start": "911070",
    "end": "914490"
  },
  {
    "text": "negation marking model.",
    "start": "914490",
    "end": "916850"
  },
  {
    "start": "916850",
    "end": "921000"
  }
]