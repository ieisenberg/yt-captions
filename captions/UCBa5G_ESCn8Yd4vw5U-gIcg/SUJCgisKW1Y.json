[
  {
    "start": "0",
    "end": "5670"
  },
  {
    "text": "So our plan for\ntoday is primarily we're going to\nfocus on what I'll refer to as non-parametric\nfew-shot short learning",
    "start": "5670",
    "end": "11400"
  },
  {
    "text": "methods. This is a pretty\ncool class of methods that actually seems\nto work really, really well for few-shot\nclassification problems.",
    "start": "11400",
    "end": "20040"
  },
  {
    "text": "And it will also be\npart of homework 2, in addition to\nsome of the topics that we had covered on Monday.",
    "start": "20040",
    "end": "26250"
  },
  {
    "text": "We'll also look at a\ncase study in education, where we actually\ndeployed these systems",
    "start": "26250",
    "end": "31620"
  },
  {
    "text": "for a real live application,\nwhich is pretty exciting. And then once we talk about\nthis class of methods,",
    "start": "31620",
    "end": "39670"
  },
  {
    "text": "we'll also talk a\nlittle bit, start to wrap up this module on\nmeta learning algorithms,",
    "start": "39670",
    "end": "44900"
  },
  {
    "text": "by talking about a comparison\nof the three different classes of approaches that\nwe've talked about, and also give a few different\nexample applications.",
    "start": "44900",
    "end": "53465"
  },
  {
    "text": "So really, the\ngoal is by the end of the lecture are to get\nan understanding for what this third class of\nmeta learning methods",
    "start": "53465",
    "end": "59310"
  },
  {
    "text": "is, and understand\nhow to implement these non-parametric\nmeta learning methods,",
    "start": "59310",
    "end": "64805"
  },
  {
    "text": "and then also\nstart to understand the trade-offs between different\nmeta learning algorithms and some of the applications\nof these algorithms.",
    "start": "64805",
    "end": "73690"
  },
  {
    "text": "Cool. So to briefly\nrecap, the last two lectures we've talked about\nblack box meta learning",
    "start": "73690",
    "end": "79090"
  },
  {
    "text": "and optimization\nbased meta learning. And in black-box\nmeta learning, we",
    "start": "79090",
    "end": "84220"
  },
  {
    "text": "dramatize the learning\nprocess with something like a big black box for\ncurrent neural network by passing in a trained data\nset into that neural network,",
    "start": "84220",
    "end": "91720"
  },
  {
    "text": "having it output a\nset of parameters, and having a new example\nbe passed into that,",
    "start": "91720",
    "end": "97689"
  },
  {
    "text": "and training this\nwhole system end to end with respect to\nthe ability to generalize to new data points.",
    "start": "97690",
    "end": "105049"
  },
  {
    "text": "This is what you've been\nimplementing in homework 1. It's a very expressive\napproach in that it can represent lots of\ndifferent learning procedures.",
    "start": "105050",
    "end": "112965"
  },
  {
    "text": "But it can also be somewhat\nchallenging to optimize and somewhat sensitive\nto hyper parameters.",
    "start": "112965",
    "end": "119070"
  },
  {
    "text": "Then on the lecture on Monday,\nwe talked about optimization based meta learning\nalgorithms that embed the structure\nof gradient descent",
    "start": "119070",
    "end": "126300"
  },
  {
    "text": "into the inner loop\nlearning process. And we saw that these\nalgorithms, because they're",
    "start": "126300",
    "end": "136315"
  },
  {
    "text": "embedding the structure\nof optimization inside this learning process,\nyou get that nice structure. And so at initialization,\nyou're already",
    "start": "136315",
    "end": "142673"
  },
  {
    "text": "going to be getting\nsomething that can do at least a little bit\nof learning from the examples that you give it.",
    "start": "142673",
    "end": "149530"
  },
  {
    "text": "But it does require a\nsecond order optimization. And this can be computationally\nmore heavyweight, especially",
    "start": "149530",
    "end": "155605"
  },
  {
    "text": "in comparison to some\nof the approaches that we'll be\ntalking about today.",
    "start": "155605",
    "end": "160980"
  },
  {
    "text": "Cool. So really what we'd like\nto be able to do today is we'd like to be able to\ntake a learning procedure",
    "start": "160980",
    "end": "167280"
  },
  {
    "text": "and embed that\ninside the inner loop process of these meta learning\nalgorithms without requiring",
    "start": "167280",
    "end": "174150"
  },
  {
    "text": "a second order optimization. And the way that\nwe're going to do that is instead\nof trying to embed",
    "start": "174150",
    "end": "179700"
  },
  {
    "text": "gradient descent into\nthese algorithms, we're going to\nlook at algorithms",
    "start": "179700",
    "end": "185010"
  },
  {
    "text": "like nearest neighbors. And in particular,\nyou might think, OK, well nearest neighbors is not\na very powerful algorithm.",
    "start": "185010",
    "end": "192299"
  },
  {
    "text": "So why might we\nactually do anything like nearest neighbors? But I think nearest neighbors,\nthese non-parametric machine",
    "start": "192300",
    "end": "199200"
  },
  {
    "text": "learning methods actually\nwork pretty well if you are in a low data regime.",
    "start": "199200",
    "end": "205122"
  },
  {
    "text": "If you have a small\namount of data, then these algorithms\nare computationally efficient because you don't\nhave that many comparisons to be",
    "start": "205122",
    "end": "211170"
  },
  {
    "text": "making. And they're also very simple. And meta test time when,\nwe're trying to do something",
    "start": "211170",
    "end": "218260"
  },
  {
    "text": "like few-shot learning, we're\nactually in a low data regime. And so things like\nnearest neighbors may actually make\na lot of sense.",
    "start": "218260",
    "end": "225890"
  },
  {
    "text": "However, during\nmeta training time, we actually potentially\nhave a large number of tasks and we want to be able to\nlearn good representations",
    "start": "225890",
    "end": "233255"
  },
  {
    "text": "from the data that we have. And so we still want to be\nparametric during meta training time.",
    "start": "233255",
    "end": "240110"
  },
  {
    "text": "And so really the\nidea behind the class of methods that we'll\ntalk about today is trying to use parametric\nmeta learners that",
    "start": "240110",
    "end": "248140"
  },
  {
    "text": "produce effective\nnon-parametric learners. ",
    "start": "248140",
    "end": "253970"
  },
  {
    "text": "So it might be a\nlittle bit mysterious what that exactly means. So let's get into\nit a little bit. So say we want to do the\ncanonical few-shot image",
    "start": "253970",
    "end": "263150"
  },
  {
    "text": "classification problem that\nwe've been looking at before. If we want to do something\nlike nearest neighbors",
    "start": "263150",
    "end": "268430"
  },
  {
    "text": "with this example,\nwhat we would do is we would take\nour test data point and we would compare it to\neach of our training examples.",
    "start": "268430",
    "end": "276810"
  },
  {
    "text": "And once we figured out which\nof the training examples it was most similar to,\nthen we can output the label",
    "start": "276810",
    "end": "283590"
  },
  {
    "text": "of that training example. So this is pretty simple.",
    "start": "283590",
    "end": "289750"
  },
  {
    "text": "We're just going to be comparing\nour test image with each of the images in the training\ndata set for our given task.",
    "start": "289750",
    "end": "298290"
  },
  {
    "text": "Now, the key question is when we\nmake these comparisons, in what",
    "start": "298290",
    "end": "303480"
  },
  {
    "text": "space do we compare,\nor what distance metric are we going to be using\nto compare these images.",
    "start": "303480",
    "end": "310004"
  },
  {
    "text": " Now, one thing you\ncould imagine doing is doing L2 distance\nin pixel space,",
    "start": "310005",
    "end": "316650"
  },
  {
    "text": "so just doing Euclidean\ndistance in the original space of the images.",
    "start": "316650",
    "end": "323030"
  },
  {
    "text": "And so if you do\nsomething like that, say you were to compare this\nimage on the right with the two images on the left just\nwith this L2 distance.",
    "start": "323030",
    "end": "331460"
  },
  {
    "text": "I'm curious what\nyou think would be the closer of the two images.",
    "start": "331460",
    "end": "336810"
  },
  {
    "text": "How many people think that\nthe left image would be closer in terms of L2 distance?",
    "start": "336810",
    "end": "341840"
  },
  {
    "text": "And how many people\nthink the right image? So maybe people are trying to--",
    "start": "341840",
    "end": "348525"
  },
  {
    "text": "but in general, there wasn't\nany sort of consensus. But it turns out that this\nleft image is actually, in L2 space what's\ngoing to be closer",
    "start": "348525",
    "end": "355090"
  },
  {
    "text": "to this image on the right. And at least\nperceptually, in terms of when we see these images,\nat least the image on the right",
    "start": "355090",
    "end": "362122"
  },
  {
    "text": "is something that\nwe would probably want to be closer in terms\nof our distance metric.",
    "start": "362122",
    "end": "369099"
  },
  {
    "text": "So things like L2\ndistance are actually really terrible for comparing\nin the original space of things",
    "start": "369100",
    "end": "375580"
  },
  {
    "text": "like images. So we don't want\nto use L2 distance.",
    "start": "375580",
    "end": "381520"
  },
  {
    "text": "Does anyone have ideas\nfor what distance metrics we might consider\nusing instead of L2 distance?",
    "start": "381520",
    "end": "387220"
  },
  {
    "text": "Yeah? Hold the first five\nlayers of VGG-19, and you use [INAUDIBLE]\nfor this image.",
    "start": "387220",
    "end": "393100"
  },
  {
    "text": "That's very specific. So you could use it\nas the embeddings from kind of a trained\nneural network,",
    "start": "393100",
    "end": "398860"
  },
  {
    "text": "perhaps the fifth layer\nof the VGG network. Yeah?",
    "start": "398860",
    "end": "404719"
  },
  {
    "text": "Learn the metrics,\nlike the training data. Yeah, so maybe you can learn the\nmetric with the training data.",
    "start": "404720",
    "end": "412750"
  },
  {
    "text": "Any other thoughts?  Yeah. Extract key points.",
    "start": "412750",
    "end": "419787"
  },
  {
    "text": "So you could try to extract\nsome key points in the image and compare those, the key\npoints across the images.",
    "start": "419787",
    "end": "424960"
  },
  {
    "start": "424960",
    "end": "430789"
  },
  {
    "text": "Yeah. [INAUDIBLE] so if\nyou know your pocket",
    "start": "430790",
    "end": "437030"
  },
  {
    "text": "that you could pull out of\nthe [INAUDIBLE],, and maybe do cosine similarities in\nthis huge feature space.",
    "start": "437030",
    "end": "445310"
  },
  {
    "text": "Yeah, so you could do something\nlike cosine similarity in a certain feature space.",
    "start": "445310",
    "end": "451070"
  },
  {
    "text": "Yeah? This isn't a\nsuggestion, but I just feel like I'm missing something. If we're talking about\nchoosing a distance metric,",
    "start": "451070",
    "end": "457460"
  },
  {
    "text": "isn't that a parameter? Like in what sense is this\na non-parametric type of--",
    "start": "457460",
    "end": "462590"
  },
  {
    "text": "Yeah. So the question was, if we're\nchoosing the distance metric, isn't that a parameter of the\nmethod, and so in what sense",
    "start": "462590",
    "end": "469759"
  },
  {
    "text": "is this a non-parametric method. And so yeah, that's\nexactly right.",
    "start": "469760",
    "end": "475055"
  },
  {
    "text": "The choice of distance function\nis going to be parametric. And in this case,\nwe're actually going to be learning a distance metric\nwhich is what was suggested.",
    "start": "475055",
    "end": "483350"
  },
  {
    "text": "That's the parametric\npart of it. But once you have\nthat distance metric, the rest is non-parametric.",
    "start": "483350",
    "end": "488600"
  },
  {
    "text": "So you're going to be\ncomparing in that space. So once you embed into some\nembedding space, everything",
    "start": "488600",
    "end": "494750"
  },
  {
    "text": "after that is non-parametric. And so it is going to\nbe something that's a little bit more hybrid. The meta learning process\nwill be parametric",
    "start": "494750",
    "end": "501263"
  },
  {
    "text": "and have some parameters\nthat you're optimizing. And then once you optimize\nthose parameters, at test time",
    "start": "501263",
    "end": "506810"
  },
  {
    "text": "there isn't going to be any kind\nof notion of task parameters. And we'll see that\na little bit when",
    "start": "506810",
    "end": "512000"
  },
  {
    "text": "we get into some of the math. Cool. So yeah, the key idea\nbehind this class of methods",
    "start": "512000",
    "end": "519909"
  },
  {
    "text": "is maybe we can learn\nhow to compare examples, using the meta training data.",
    "start": "519909",
    "end": "527200"
  },
  {
    "text": "So there's three specific\nmethods that we'll go into, and we'll start with the\nsimplest one, which is referred",
    "start": "527200",
    "end": "533500"
  },
  {
    "text": "to as a Siamese network. So a Siamese network is a\npretty simple neural network architecture where\nyou have two inputs,",
    "start": "533500",
    "end": "541690"
  },
  {
    "text": "and you pass those two inputs\ninto the same exact neural",
    "start": "541690",
    "end": "546905"
  },
  {
    "text": "network.  And it's called\nthe Siamese network",
    "start": "546905",
    "end": "552680"
  },
  {
    "text": "because these two\nneural networks have the same exact parameters. So the parameters are\nshared across the two.",
    "start": "552680",
    "end": "559010"
  },
  {
    "text": " And what we'll do is\nwe'll pass in two images into this Siamese network.",
    "start": "559010",
    "end": "566589"
  },
  {
    "text": "And then once we\nhave the output, we'll then compare these the\nresulting representation.",
    "start": "566590",
    "end": "573070"
  },
  {
    "text": "And we'll train the Siamese\nneural network to output whether or not the two\nimages we're passing in",
    "start": "573070",
    "end": "579070"
  },
  {
    "text": "have the same class or not. And so in this example,\nthese two images",
    "start": "579070",
    "end": "585242"
  },
  {
    "text": "have a different class. One is of a lion one\nis of cups or bowls, and so the label will\nbe 0, because these",
    "start": "585242",
    "end": "591480"
  },
  {
    "text": "are a different class. Likewise, we could pass\nit these two images which are both images of bowls, and\nwe would train it to output 1,",
    "start": "591480",
    "end": "601380"
  },
  {
    "text": "and so on and so forth. So whenever we give it to images\nthat our different classes, we train it to output 0. Whenever they're both the same\nclass, we train to output 1.",
    "start": "601380",
    "end": "609720"
  },
  {
    "text": "So the training process\nis a very simple binary classification problem. And you can do this with all\nof the available meta training",
    "start": "609720",
    "end": "617160"
  },
  {
    "text": "data that you have.  Cool.",
    "start": "617160",
    "end": "622500"
  },
  {
    "text": "And then once you have this\nSiamese neural network, what you can do is use this as\nessentially your similarity",
    "start": "622500",
    "end": "630060"
  },
  {
    "text": "metric or your distance metric. So you can compare your test\nimage to each of the examples",
    "start": "630060",
    "end": "637260"
  },
  {
    "text": "in your training data set,\nand ask this network, which of the images is the closest.",
    "start": "637260",
    "end": "644110"
  },
  {
    "text": "And so in particular,\nif we go back to the example you\nhad before, you would pass in all pairs of\ntraining example, comma, test",
    "start": "644110",
    "end": "653310"
  },
  {
    "text": "data point, and ask it which\nhas the highest probability of being the same\nclass, and then output the corresponding label.",
    "start": "653310",
    "end": "659310"
  },
  {
    "text": " Any questions on how that works?",
    "start": "659310",
    "end": "665195"
  },
  {
    "text": " Cool, so training will be binary\nclassification, and test time,",
    "start": "665195",
    "end": "673310"
  },
  {
    "text": "you're running these\npairwise comparisons. And so if you have n times\nk examples in your training",
    "start": "673310",
    "end": "678650"
  },
  {
    "text": "data set, you're going\nto be actually doing n times k forward passes\nof this neural network,",
    "start": "678650",
    "end": "684733"
  },
  {
    "text": "and then finding the\nprobability that's the highest, and then not putting\nthe corresponding label. ",
    "start": "684733",
    "end": "691380"
  },
  {
    "text": "So meta training is\nbinary classification, and then meta test\ntime will actually be a form of N-way\nclassification.",
    "start": "691380",
    "end": "697290"
  },
  {
    "text": "Yeah? [INAUDIBLE] on one-shot\nlearning then kind of thing.",
    "start": "697290",
    "end": "703520"
  },
  {
    "text": "So you're asking, this can't\nwork for one-shot learning? Why can't it work for\none-shot learning? If you only have one\nexample of each class,",
    "start": "703520",
    "end": "710540"
  },
  {
    "text": "then if you compare it\nto just [INAUDIBLE],, the unit will just learn\nto predict identity only,",
    "start": "710540",
    "end": "716269"
  },
  {
    "text": "kind of like that. So, similar to what we saw\nin the previous two lectures,",
    "start": "716270",
    "end": "721760"
  },
  {
    "text": "you need to have at least\none example per class in your training data, in\nyour meta training data set.",
    "start": "721760",
    "end": "728127"
  },
  {
    "text": "And so in your meta\ntraining data set, you need to make sure\nthat, for example, you sample two images per character,\nor two images per class.",
    "start": "728127",
    "end": "736220"
  },
  {
    "text": "And that ensures that\nyou're actually going to train it to generalize. Versus, for example, if you\npass in the same exact image",
    "start": "736220",
    "end": "742988"
  },
  {
    "text": "into the network,\nit would just learn to memorize, and learn how\nto predict whether or not they're exactly the same\nimage, or whether they're",
    "start": "742988",
    "end": "748907"
  },
  {
    "text": "slightly different images. ",
    "start": "748907",
    "end": "754870"
  },
  {
    "text": "Cool, so now one\nthing that's not super appealing about\nthis kind of approach",
    "start": "754870",
    "end": "760450"
  },
  {
    "text": "is that we actually have\nthis mismatch between what's happening at meta\ntraining time and what's happening at meta test time.",
    "start": "760450",
    "end": "766910"
  },
  {
    "text": "And so, in particular,\nif you were actually to write out what happens\nat meta test time,",
    "start": "766910",
    "end": "772149"
  },
  {
    "text": "you're going to get something\nwhere basically you're taking-- we'll call this neural\nnetwork f, or f data.",
    "start": "772150",
    "end": "779320"
  },
  {
    "text": "You're going to be comparing\nthe test example with each",
    "start": "779320",
    "end": "786580"
  },
  {
    "text": "of your training examples. So you'll be looking at this for\neach of the xk in your training data set.",
    "start": "786580",
    "end": "791660"
  },
  {
    "text": "And you can\nessentially view what's happening at meta test\ntime as, well, asking",
    "start": "791660",
    "end": "800889"
  },
  {
    "text": "this neural network whether\nor not the probability",
    "start": "800890",
    "end": "806110"
  },
  {
    "text": "that these match is\ngreater than, say, 0.5. And if it is greater than\n0.5, outputting the label",
    "start": "806110",
    "end": "814060"
  },
  {
    "text": "corresponding to that\ntraining example. So one way that you\ncould write what's",
    "start": "814060",
    "end": "819759"
  },
  {
    "text": "going to be happening\nat meta test time would be something like this,\nwhere you're summing over",
    "start": "819760",
    "end": "826450"
  },
  {
    "text": "all of the examples in your\ntraining data set, comparing",
    "start": "826450",
    "end": "831790"
  },
  {
    "text": "the test input to each of\nthose training samples, and for the one that has\nthe highest probability,",
    "start": "831790",
    "end": "837430"
  },
  {
    "text": "then outputting the\ncorresponding label. This isn't exactly correct. Because the network\nmight actually",
    "start": "837430",
    "end": "842500"
  },
  {
    "text": "output greater than 0.5 for\nmore than one of the examples. But this is approximately\nwhat it is doing.",
    "start": "842500",
    "end": "849430"
  },
  {
    "text": "And this will correspond\nto y hat k, your prediction",
    "start": "849430",
    "end": "856720"
  },
  {
    "text": "for the test example. So y hat test.",
    "start": "856720",
    "end": "863380"
  },
  {
    "text": "So now from this\nstandpoint, if you view this as what's happening at\nmeta test time, what you could",
    "start": "863380",
    "end": "872589"
  },
  {
    "text": "imagine doing instead of\ndoing binary classification during train time is formulating\nan equation like this,",
    "start": "872590",
    "end": "878439"
  },
  {
    "text": "and simply just back\npropagating into theta. And if we did\nsomething like that,",
    "start": "878440",
    "end": "884300"
  },
  {
    "text": "then we could actually match\nwhat happens at meta training time and meta test time. Now, we can't do exactly that\nbecause this operator right",
    "start": "884300",
    "end": "892980"
  },
  {
    "text": "here that's basically is\nkind of an indicator variable for whether or not the\nprobability is above 0.5, it's going to be hard to\ndifferentiate through something",
    "start": "892980",
    "end": "899850"
  },
  {
    "text": "like that, because\nit's a hard operation. And so what you can do instead\nis do something softer,",
    "start": "899850",
    "end": "905460"
  },
  {
    "text": "where you simply multiply\nthis probability by yk,",
    "start": "905460",
    "end": "918285"
  },
  {
    "text": "and sum over your\nexamples in your training",
    "start": "918285",
    "end": "924870"
  },
  {
    "text": "data set for a given task. And if we do something\nlike this, this here,",
    "start": "924870",
    "end": "930209"
  },
  {
    "text": "we can actually back\npropagate through this loss. So this is going to be\nequal to y hat test.",
    "start": "930210",
    "end": "937830"
  },
  {
    "text": "And you can actually optimize\nthe parameters of this network",
    "start": "937830",
    "end": "943740"
  },
  {
    "text": "right here with respect\nto how accurate you are on test examples.",
    "start": "943740",
    "end": "950130"
  },
  {
    "text": "And so in particular,\nthe way this algorithm",
    "start": "950130",
    "end": "955610"
  },
  {
    "text": "would look like is very similar\nto the existing meta training algorithms that we\nhave done before,",
    "start": "955610",
    "end": "960769"
  },
  {
    "text": "where first you sample a task.",
    "start": "960770",
    "end": "965955"
  },
  {
    "text": "So this might, if we were\ndoing three-way classification, this might correspond to,\nsay, three characters.",
    "start": "965955",
    "end": "971030"
  },
  {
    "text": "Second, we sample two\nimages per character.",
    "start": "971030",
    "end": "977180"
  },
  {
    "text": "And then for the training\ndata set, this basically,",
    "start": "977180",
    "end": "983029"
  },
  {
    "text": "it gives us our\ntraining data set and our test set for that task.",
    "start": "983030",
    "end": "989160"
  },
  {
    "text": "We plug in our training data\nset into this equation here to then get an estimate,\nas well as our test",
    "start": "989160",
    "end": "999899"
  },
  {
    "text": "example from the test set to\nget an estimate for the label. And then when we actually\ngo to update theta,",
    "start": "999900",
    "end": "1005670"
  },
  {
    "text": "we're going to be looking\nat comparing the--",
    "start": "1005670",
    "end": "1011519"
  },
  {
    "text": "I guess I should write this\nmore as cross entropy loss. So we'll be looking at\nwhat's the best way?",
    "start": "1011520",
    "end": "1020780"
  },
  {
    "text": "Something like y hat\ntest log y tests.",
    "start": "1020780",
    "end": "1026579"
  },
  {
    "text": "I should have written\nthis down before. I think the cross\nentropy loss is actually",
    "start": "1026579",
    "end": "1032030"
  },
  {
    "text": "the opposite of this. ",
    "start": "1032030",
    "end": "1037554"
  },
  {
    "text": "Is this correct for\ncross entropy loss? ",
    "start": "1037555",
    "end": "1047449"
  },
  {
    "text": "Something like this. And so then you'll be minimizing\nthe parameters of basically",
    "start": "1047450",
    "end": "1058280"
  },
  {
    "text": "our comparator function\nwith respect to how accurate your predictions are. ",
    "start": "1058280",
    "end": "1067620"
  },
  {
    "text": "So more specifically,\nwhat this will look like is this\ncorresponds to something called matching networks.",
    "start": "1067620",
    "end": "1073799"
  },
  {
    "text": "And what we're\ngoing to do is we're going to encode our training\nexamples into some embedding",
    "start": "1073800",
    "end": "1081240"
  },
  {
    "text": "space. And then for each\nof those embeddings, we will compute this\nfunction that you",
    "start": "1081240",
    "end": "1088012"
  },
  {
    "text": "can think of this as\npotentially taking the dot product between your\nembedding for x test and you're betting for xk.",
    "start": "1088012",
    "end": "1096230"
  },
  {
    "text": "And so in this\ndiagram, you can think of each of these black dots\nas this function right here.",
    "start": "1096230",
    "end": "1104440"
  },
  {
    "text": "And then each of\nthese colored squares corresponds to the label\nfor that training example.",
    "start": "1104440",
    "end": "1112279"
  },
  {
    "text": "We take the dot product between\nthe black dots and the colored",
    "start": "1112280",
    "end": "1120010"
  },
  {
    "text": "squares, sum over\neach of these examples to ultimately get the\nprediction for our best example.",
    "start": "1120010",
    "end": "1130300"
  },
  {
    "text": " Yeah?",
    "start": "1130300",
    "end": "1135679"
  },
  {
    "text": "How well do these methods\nwork on unseen classes at inference time?",
    "start": "1135680",
    "end": "1143100"
  },
  {
    "text": "So how will these methods\nwork on unseen examples at inference time? So in general, as\nlong as you train it",
    "start": "1143100",
    "end": "1150919"
  },
  {
    "text": "on enough of the kinds\nof meditating scenarios that we've had\nbefore where you have",
    "start": "1150920",
    "end": "1156380"
  },
  {
    "text": "enough characters that\nyou're training it on, it will actually generalize\nwell to new image classes",
    "start": "1156380",
    "end": "1162860"
  },
  {
    "text": "at test time. If you only train\nit on a few classes, then it will struggle\nto generalize.",
    "start": "1162860",
    "end": "1169340"
  },
  {
    "text": "Yeah? [INAUDIBLE] parametric\nmethods that constrain to classification tasks,\nonly because you cannot learn",
    "start": "1169340",
    "end": "1176710"
  },
  {
    "text": "anything else [INAUDIBLE]. Yeah, so that's a\ngreat observation. So these methods are restricted\nto classification tasks.",
    "start": "1176710",
    "end": "1185860"
  },
  {
    "text": "Things like nearest neighbors\nare specific to classification,",
    "start": "1185860",
    "end": "1191049"
  },
  {
    "text": "and it's not trivial to try\nto extend these kinds of ideas to regression problems.",
    "start": "1191050",
    "end": "1196195"
  },
  {
    "text": " Yeah? Why is the D test not going\nthrough the same embedding",
    "start": "1196195",
    "end": "1204995"
  },
  {
    "text": "space as the training? Yeah, that's a great question. So one thing you\nmight know is that",
    "start": "1204995",
    "end": "1210518"
  },
  {
    "text": "in this particular\narchitecture, they use a different encoder\nfor the test example than for the training examples.",
    "start": "1210518",
    "end": "1216320"
  },
  {
    "text": "That's mostly just a choice\nof their architecture. You could choose\nto put it through",
    "start": "1216320",
    "end": "1221570"
  },
  {
    "text": "the same exact embedding space. And actually, in the method\nthat we'll look at next, they do actually choose to put\nit in the same exact embedding",
    "start": "1221570",
    "end": "1227960"
  },
  {
    "text": "space. One other note that I'll\nmake about this architecture is that when they embed\nthe training examples",
    "start": "1227960",
    "end": "1235910"
  },
  {
    "text": "they actually use-- this is a somewhat old paper. So they use a\nbidirectional LSTM. If they were designing\nthis more recently,",
    "start": "1235910",
    "end": "1242809"
  },
  {
    "text": "they'd probably use a\nbidirectional transformer model. And that means that when\nit actually computes this,",
    "start": "1242810",
    "end": "1250430"
  },
  {
    "text": "it can actually take into\naccount not just one example. But it's also implicitly taking\ninto account the other examples",
    "start": "1250430",
    "end": "1257330"
  },
  {
    "text": "in your task training set.  But basically at a high level,\nyou can think of this method",
    "start": "1257330",
    "end": "1264110"
  },
  {
    "text": "as embedding things,\ndoing nearest neighbors in that embedding space,\nand then back propagating all the way into the parameters\nof your embedding function.",
    "start": "1264110",
    "end": "1273368"
  },
  {
    "text": "Of course, we need to do that\nsort of nearest neighbors function in the\nsoft way, so that we could differentiate through it.",
    "start": "1273368",
    "end": "1279180"
  },
  {
    "text": "And that's why we\nget a function that looks like this, where we\nare rather than taking kind",
    "start": "1279180",
    "end": "1284750"
  },
  {
    "text": "of this hardmax, we're going\nto be taking more of something like a softmax. ",
    "start": "1284750",
    "end": "1295132"
  },
  {
    "text": "So the whole thing is\ntrained end to end.  One thing that's nice about\nthis paper is it actually",
    "start": "1295132",
    "end": "1301430"
  },
  {
    "text": "really emphasizes\nhow you should really try to match what's happening\nat meta training time and meta test time. And they were able to\nget substantially better",
    "start": "1301430",
    "end": "1308210"
  },
  {
    "text": "performance than something\nlike Siamese networks. ",
    "start": "1308210",
    "end": "1317400"
  },
  {
    "text": "Cool. So if we walk through\nthe algorithm, we also did this\nsomewhat on the board,",
    "start": "1317400",
    "end": "1323040"
  },
  {
    "text": "but more formally it's very\nsimilar to the algorithms that we've seen\nfor the black box",
    "start": "1323040",
    "end": "1328830"
  },
  {
    "text": "approach and the\noptimization based approach. And what's different is just\nprimarily just the third step,",
    "start": "1328830",
    "end": "1336270"
  },
  {
    "text": "although also the fourth step. So in the third step, instead\nof actually explicitly computing parameters\nfor the task,",
    "start": "1336270",
    "end": "1344520"
  },
  {
    "text": "we're actually going to\nbe skipping that step, and directly making predictions\nfor the test examples.",
    "start": "1344520",
    "end": "1353070"
  },
  {
    "text": "You can view this as kind of\nintegrating out the parameters, the test parameters. And hence, why we're referring\nto these as non-parametric",
    "start": "1353070",
    "end": "1359970"
  },
  {
    "text": "few-shot learning methods. And then of course,\nbecause we're just",
    "start": "1359970",
    "end": "1366930"
  },
  {
    "text": "computing test\npredictions directly instead of using a loss function\nin the form of step four,",
    "start": "1366930",
    "end": "1373920"
  },
  {
    "text": "we're going to be updating\nthe meta parameters simply using a loss function\nthat compares",
    "start": "1373920",
    "end": "1380399"
  },
  {
    "text": "the predictions and the labels. How does this\ngeneralize in the case",
    "start": "1380400",
    "end": "1385892"
  },
  {
    "text": "where you have more than\none data point [INAUDIBLE] of the same [INAUDIBLE]?",
    "start": "1385892",
    "end": "1391440"
  },
  {
    "text": "Yeah, that's a great question. And that was actually the\nnext question on my side. So in particular,\nthe question is,",
    "start": "1391440",
    "end": "1399400"
  },
  {
    "text": "what happens if you have\nthan one example per class.",
    "start": "1399400",
    "end": "1408150"
  },
  {
    "text": "So I guess I was\nactually thinking about posing this to you guys.",
    "start": "1408150",
    "end": "1413700"
  },
  {
    "text": "So maybe, does anyone want\nto answer that question? Yeah? You can compare to\nthe average embedding",
    "start": "1413700",
    "end": "1419907"
  },
  {
    "text": "over the different\nimages of the same class. Yeah, so one thing\nthat you can do, is you can compare\nthe average embedding.",
    "start": "1419907",
    "end": "1425618"
  },
  {
    "text": "You're foreshadowing\nwhat happens next. Yeah? We could take the\nscore for each class,",
    "start": "1425618",
    "end": "1432380"
  },
  {
    "text": "count how many [INAUDIBLE]\nhave probably greater than 0.5, and the class which has\nthe highest count, that",
    "start": "1432380",
    "end": "1439110"
  },
  {
    "text": "could be the prediction. Yeah, so you could do some\nsort of voting scheme. And in practice,\nthat's actually what matching networks tends to do.",
    "start": "1439110",
    "end": "1444880"
  },
  {
    "text": "So basically you're\ngoing to be-- this equation is still\nvalid for scenarios",
    "start": "1444880",
    "end": "1449990"
  },
  {
    "text": "where you have more than\none example per class. And essentially, each\nof these is going",
    "start": "1449990",
    "end": "1455360"
  },
  {
    "text": "to vote on what the label is. And if you accumulate\nenough votes,",
    "start": "1455360",
    "end": "1461509"
  },
  {
    "text": "the thing with the highest\naccumulation of those scores will then win the vote\nand get to be able to make",
    "start": "1461510",
    "end": "1470390"
  },
  {
    "text": "the prediction for the label. Now, there's a downside to\nthis voting approach, which",
    "start": "1470390",
    "end": "1477380"
  },
  {
    "text": "is that all the votes are cast\nsomewhat independently of one another. And this means that you might\nhave scenarios where the test",
    "start": "1477380",
    "end": "1488630"
  },
  {
    "text": "example is, maybe\nthere is one example with the incorrect label that\nactually has a very high vote,",
    "start": "1488630",
    "end": "1495350"
  },
  {
    "text": "but everything else has a very\nlow vote for that example. And you might get\nexamples essentially",
    "start": "1495350",
    "end": "1508360"
  },
  {
    "text": "where that overpowers\nthe actual correct label and doesn't give you\nexactly what you want.",
    "start": "1508360",
    "end": "1515250"
  },
  {
    "text": "As one maybe rough example\nfor what this might look like,",
    "start": "1515250",
    "end": "1520770"
  },
  {
    "text": "you could think of this as\nmimicking something a lot like nearest neighbors. And there's a failure mode for\nnearest neighbors, where if,",
    "start": "1520770",
    "end": "1528990"
  },
  {
    "text": "for example, you have data\nthat maybe looks something like this, where you have just a\ntwo-way classification problem.",
    "start": "1528990",
    "end": "1536700"
  },
  {
    "text": "You're trying to classify\nbetween positive and negative examples.",
    "start": "1536700",
    "end": "1541890"
  },
  {
    "text": "If you end up getting an\nexample that's say, right here for example, something like\nnearest neighbors will give you",
    "start": "1541890",
    "end": "1550320"
  },
  {
    "text": "a negative prediction,\neven though there's a lot of positives in\nthis general vicinity.",
    "start": "1550320",
    "end": "1556800"
  },
  {
    "text": "And if instead of\ndoing nearest neighbors you aggregated information\nacross the examples",
    "start": "1556800",
    "end": "1563399"
  },
  {
    "text": "into what we'll refer to as\na prototype for each class, then you can start to\nmitigate that issue.",
    "start": "1563400",
    "end": "1571360"
  },
  {
    "text": "So if this is your\nembedding space, and you average the\nembedding per class,",
    "start": "1571360",
    "end": "1580919"
  },
  {
    "text": "you'll get kind of one prototype\nthat is maybe somewhere right here for the positive class.",
    "start": "1580920",
    "end": "1587520"
  },
  {
    "text": "And you'll get one\nprototype that's somewhere maybe around here\nfor the negative class.",
    "start": "1587520",
    "end": "1594179"
  },
  {
    "text": "And then if you compare your\nexample to these prototypes, you'll then see\nthat the distance",
    "start": "1594180",
    "end": "1602130"
  },
  {
    "text": "to the positive prototype\nis less than the distance to the negative prototype. And then you'll get\nsomething that's",
    "start": "1602130",
    "end": "1608430"
  },
  {
    "text": "closer to the right answer. And so that's the idea\nbehind the last approach",
    "start": "1608430",
    "end": "1615730"
  },
  {
    "text": "that we'll talk about, which\nis that instead of trying to perform these\ncomparisons independently, we'll try to aggregate\nclass information",
    "start": "1615730",
    "end": "1622780"
  },
  {
    "text": "to create a prototypical\nembedding for each class, and then compare\nthe test example",
    "start": "1622780",
    "end": "1628840"
  },
  {
    "text": "to those prototypical examples. So we'll just do\nnearest neighbors to the prototypes instead\nof nearest neighbors",
    "start": "1628840",
    "end": "1634930"
  },
  {
    "text": "to the individual\ntraining examples. ",
    "start": "1634930",
    "end": "1640769"
  },
  {
    "text": "Cool. So what that looks\nlike is basically exactly what's in this picture.",
    "start": "1640770",
    "end": "1646350"
  },
  {
    "text": "If you were actually to\nwrite it out in math,",
    "start": "1646350",
    "end": "1653309"
  },
  {
    "text": "you'll compute your prototype. We'll refer to this as\nCn to match the picture.",
    "start": "1653310",
    "end": "1659710"
  },
  {
    "text": "This will be basically\njust an average over an embedding of the\nexamples for that class.",
    "start": "1659710",
    "end": "1668010"
  },
  {
    "text": "And so if we take example-- ",
    "start": "1668010",
    "end": "1674580"
  },
  {
    "text": "I'll use xk here. We will only want to\naverage over the embeddings that correspond to class n.",
    "start": "1674580",
    "end": "1681840"
  },
  {
    "text": "And so we're only going\nto do this summation over the examples\nwhere yk is equal to n.",
    "start": "1681840",
    "end": "1688679"
  },
  {
    "text": "And this is a summation\nover all of the examples in your training data set.",
    "start": "1688680",
    "end": "1695640"
  },
  {
    "text": "So this is basically just\ngoing from these blue pluses to the purple plus,\nand the blue negatives",
    "start": "1695640",
    "end": "1705230"
  },
  {
    "text": "to the purple negative. So this is a very\nsimple average. And then once we have\nthese prototypes, we can compute the distance\nbetween the prototype",
    "start": "1705230",
    "end": "1716040"
  },
  {
    "text": "and the test example,\nrather than looking at the distance between\nthe individual examples.",
    "start": "1716040",
    "end": "1722985"
  },
  {
    "text": "And in practice, what\nthis algorithm will do, is once you compute\nthese distances. Then you'll just\nnegate the distances",
    "start": "1722985",
    "end": "1729270"
  },
  {
    "text": "to get a similarity score. And then take a softmax to\nultimately get the probability",
    "start": "1729270",
    "end": "1737850"
  },
  {
    "text": "that y hat for the test\nexample is equal to class n.",
    "start": "1737850",
    "end": "1743370"
  },
  {
    "start": "1743370",
    "end": "1752420"
  },
  {
    "text": "So written out on the slides\nwe embed all of our examples",
    "start": "1752420",
    "end": "1758210"
  },
  {
    "text": "using the function f,\naverage those embeddings. And so unlike the\nprevious slide, we're actually just going to be\nusing the same exact embedding",
    "start": "1758210",
    "end": "1765683"
  },
  {
    "text": "function for our training\nexamples and our test example.",
    "start": "1765683",
    "end": "1771860"
  },
  {
    "text": "And then this is grinding out\nthe full softmax equation, where we take the distance,\nwe compute the distance",
    "start": "1771860",
    "end": "1777380"
  },
  {
    "text": "between the example\nand the prototype, and then kind of exponentially\nand normalize in order to compute a probability.",
    "start": "1777380",
    "end": "1785890"
  },
  {
    "text": "Can you [INAUDIBLE] the distance\nfunction as well [INAUDIBLE] Yeah, so in the\noriginal paper, they",
    "start": "1785890",
    "end": "1793730"
  },
  {
    "text": "use Euclidean distance\nor cosine distance as the distance function. In practice, you could actually\nalso learn a distance function,",
    "start": "1793730",
    "end": "1801139"
  },
  {
    "text": "somewhat similar to what\nmatching networks does here as well. And so instead of using--",
    "start": "1801140",
    "end": "1807919"
  },
  {
    "text": "this could be basically\na learned network. If it is learned, then it was\ngoing to have its parameters",
    "start": "1807920",
    "end": "1815330"
  },
  {
    "text": "will be kind of a part. It will maybe have some\nadditional parameters. And when you run the\nmeta training process,",
    "start": "1815330",
    "end": "1821600"
  },
  {
    "text": "you're going to optimize-- it would both be\nthe parameters of f,",
    "start": "1821600",
    "end": "1827419"
  },
  {
    "text": "as well as the parameters of\nthis distance function right here.",
    "start": "1827420",
    "end": "1832990"
  },
  {
    "text": "Yeah? Can you explain why there's\nthe minus d on the top",
    "start": "1832990",
    "end": "1839990"
  },
  {
    "text": "and at the bottom\nthere's no minus d? Why there's a minus d\nhere, and why-- oh that's",
    "start": "1839990",
    "end": "1847030"
  },
  {
    "text": "a typo on the slide. Definitely. Yeah. That's a great catch. So on the slide, that should be\na minus d in the denominator.",
    "start": "1847030",
    "end": "1855910"
  },
  {
    "text": "Yeah? In practice, how\nwill [INAUDIBLE]",
    "start": "1855910",
    "end": "1862419"
  },
  {
    "text": "Yeah, so there's\na paper that I'll mention a few slides from\nnow called relation networks.",
    "start": "1862420",
    "end": "1868960"
  },
  {
    "text": "And they basically meta learned\nthe distance function as well. I think that on the\nbenchmark problems,",
    "start": "1868960",
    "end": "1875950"
  },
  {
    "text": "they weren't seeing significant\nbenefits from using a distance function over just\nusing something like cosine similarity\nor Euclidean distance.",
    "start": "1875950",
    "end": "1884140"
  },
  {
    "text": "And one reason to\npotentially expect to not see too many\nbenefits is that f",
    "start": "1884140",
    "end": "1889300"
  },
  {
    "text": "itself has a lot\nof expressive power to embed into whatever\nspace you might want. And that's going to impose\ndifferent distance metrics.",
    "start": "1889300",
    "end": "1897760"
  },
  {
    "text": "And so you may not\nactually need something on top of actually\njust embedding",
    "start": "1897760",
    "end": "1903100"
  },
  {
    "text": "into this other space. But it's possible that there\nare some applications where it's difficult to impose\na certain metric space,",
    "start": "1903100",
    "end": "1910900"
  },
  {
    "text": "and where we're\nlearning something, the output probability\nis helpful.",
    "start": "1910900",
    "end": "1917215"
  },
  {
    "start": "1917215",
    "end": "1925850"
  },
  {
    "text": "Cool. Actually, no. So it's actually on\nthe [INAUDIBLE] slide. So this is the\nrelation network paper. They also do this embedding,\nand then you compare to them.",
    "start": "1925850",
    "end": "1934760"
  },
  {
    "text": "But then they\nlastly do this have this kind of relation\nscore that corresponds",
    "start": "1934760",
    "end": "1940490"
  },
  {
    "text": "to this learned\ndistance function.  Now there are a\ncouple of other things",
    "start": "1940490",
    "end": "1946440"
  },
  {
    "text": "that you could do as well. So there might be\nsome examples where",
    "start": "1946440",
    "end": "1951600"
  },
  {
    "text": "some of your positive\nand negative examples are not easy to cluster nicely. So for example, if you're\ntrying to classify between cats",
    "start": "1951600",
    "end": "1958290"
  },
  {
    "text": "and dogs, you may have\nsome breeds of cats that look a lot like dogs. And if you have\nsomething like that, then",
    "start": "1958290",
    "end": "1964320"
  },
  {
    "text": "maybe you actually\nhave kind of a cluster over here of negatives. And if your algorithm is\nhaving a lot of trouble finding",
    "start": "1964320",
    "end": "1971910"
  },
  {
    "text": "a metric space where everything\nis clustered together nicely, you could also use\nsomething where you actually",
    "start": "1971910",
    "end": "1977520"
  },
  {
    "text": "have a mixture of prototypes. And that's what this paper did\nhere is instead of actually",
    "start": "1977520",
    "end": "1983039"
  },
  {
    "text": "just having one\nprototype per class, they actually had multiple\nprototypes per class.",
    "start": "1983040",
    "end": "1988440"
  },
  {
    "text": "In a lot of examples, this\nisn't critical for getting good performance. But it's something\nthat's worth mentioning.",
    "start": "1988440",
    "end": "1996512"
  },
  {
    "text": "And then the last\nthing that I'll mention is that instead of doing\nsomething as simple",
    "start": "1996512",
    "end": "2002240"
  },
  {
    "text": "as nearest neighbors,\nthere are also approaches that do\nsomething more complicated, where you formulate this\ngraph, and do message passing",
    "start": "2002240",
    "end": "2008390"
  },
  {
    "text": "on that graph in order\nto ultimately figure out the relationship between\ndifferent examples.",
    "start": "2008390",
    "end": "2014075"
  },
  {
    "start": "2014075",
    "end": "2020820"
  },
  {
    "text": "Cool. Yeah, so that's\nthe gist of it for these non-parametric methods.",
    "start": "2020820",
    "end": "2026820"
  },
  {
    "text": "I guess to summarize, unlike\nthe previous examples, unlike black box meta\nlearning and optimization",
    "start": "2026820",
    "end": "2032904"
  },
  {
    "text": "based meta learning,\nwe're not actually ever going to be getting\nan estimate for the task parameters.",
    "start": "2032905",
    "end": "2038730"
  },
  {
    "text": "Instead, we're going to\nbe typically computing some embedding space and\nthen doing nearest neighbors",
    "start": "2038730",
    "end": "2043920"
  },
  {
    "text": "in that embedding space.  Yeah?",
    "start": "2043920",
    "end": "2049100"
  },
  {
    "text": "Does it converge quickly\ninto the [INAUDIBLE]?? Yeah, so one of\nthe things that's",
    "start": "2049100",
    "end": "2054710"
  },
  {
    "text": "really nice about\nthis class of methods is that this process is\nentirely feedforward.",
    "start": "2054710",
    "end": "2063169"
  },
  {
    "text": "So you compute your embeddings\nin a feedforward manner.",
    "start": "2063170",
    "end": "2068197"
  },
  {
    "text": "And then you compute\nyour prototypes. And then you do this\ndistance function.",
    "start": "2068197",
    "end": "2074399"
  },
  {
    "text": "So it ends up being,\nyou don't have to do any sort of\ngradient descent process.",
    "start": "2074400",
    "end": "2079908"
  },
  {
    "text": "It ends up being pretty\nlightweight and pretty fast to train. And so you'll see in homework\n2 that this class of methods",
    "start": "2079909",
    "end": "2088309"
  },
  {
    "text": "can actually be quite nice\nfor classification problems. ",
    "start": "2088310",
    "end": "2093929"
  },
  {
    "text": "Yeah? What's message\npassing, the last idea?",
    "start": "2093929",
    "end": "2099482"
  },
  {
    "text": "So the question is,\nwhat is message passing.  Going into that in depth\nis beyond the scope",
    "start": "2099482",
    "end": "2105980"
  },
  {
    "text": "of what I'll cover in lecture. Message passing is,\nI guess very briefly,",
    "start": "2105980",
    "end": "2114440"
  },
  {
    "text": "you can think of message\npassing as if you have a graph, in this case a graph\nof different examples,",
    "start": "2114440",
    "end": "2121880"
  },
  {
    "text": "and you have some notion\nof how these examples are related to each other.",
    "start": "2121880",
    "end": "2127500"
  },
  {
    "text": "Like some of the\nexamples maybe have a very strong relationship,\nsome of the examples have a weaker relationship. Message passing\nalgorithms basically",
    "start": "2127500",
    "end": "2134300"
  },
  {
    "text": "try to pass messages along\nthe edges of that graph.",
    "start": "2134300",
    "end": "2140210"
  },
  {
    "text": "And you iteratively\npass messages in order to ultimately\ntry to converge",
    "start": "2140210",
    "end": "2145640"
  },
  {
    "text": "to some notion about\nthe true relation",
    "start": "2145640",
    "end": "2152420"
  },
  {
    "text": "between these examples. But I encourage you. You can read this\npaper for more detail. There's also, if you take\nlike Stefano's course",
    "start": "2152420",
    "end": "2160190"
  },
  {
    "text": "on deep generative\nmodels, he also I'm pretty sure it covers\nundirected graphical models, and things like\nmessage passing there.",
    "start": "2160190",
    "end": "2165763"
  },
  {
    "text": " Yeah? Sorry, which one\nwas it that you said",
    "start": "2165763",
    "end": "2170930"
  },
  {
    "text": "didn't require gradient descent? So none of these methods\nrequire gradient descent",
    "start": "2170930",
    "end": "2177480"
  },
  {
    "text": "at meta test time. All of them require a\nform of gradient descent",
    "start": "2177480",
    "end": "2182789"
  },
  {
    "text": "in order to optimize\nyour objective, to optimize your\nembedded [INAUDIBLE].. ",
    "start": "2182790",
    "end": "2192990"
  },
  {
    "text": "Cool. So I'd like to run through a\ncase study of actually using this algorithm in practice.",
    "start": "2192990",
    "end": "2199920"
  },
  {
    "text": "I put one example of a\nprevious years' case study on the slides. That was actually in a pretty\ncool healthcare setting,",
    "start": "2199920",
    "end": "2206819"
  },
  {
    "text": "where they're looking at\nimage classification of trying to classify different skin\nconditions and skin diseases.",
    "start": "2206820",
    "end": "2214408"
  },
  {
    "text": "But the case study that\nI want to cover this year is an example in education\nthat I'm pretty excited about.",
    "start": "2214408",
    "end": "2220437"
  },
  {
    "text": "I'm a little bit biased, because\nI was involved in the effort. But I think education is a\nreally cool application area.",
    "start": "2220437",
    "end": "2228330"
  },
  {
    "text": "And I should also mention\nkind of at the start of this project, we\nreally had no idea",
    "start": "2228330",
    "end": "2234990"
  },
  {
    "text": "how hard or easy the\nproblem would be. And so, yeah, it was kind of\nrewarding to see how it went.",
    "start": "2234990",
    "end": "2241770"
  },
  {
    "text": "So the problem that\nwe were looking at is the feedback problem.",
    "start": "2241770",
    "end": "2248510"
  },
  {
    "text": "And as one instance\nof this problem, there was a course\noffered by some folks at Stanford called\nCode in Place,",
    "start": "2248510",
    "end": "2255170"
  },
  {
    "text": "similar to Shelter in Place. While you're sheltering in\nplace, you could code in place.",
    "start": "2255170",
    "end": "2261110"
  },
  {
    "text": "And it was a free course,\nfree intro to computer science course. And in the second\niteration of the course,",
    "start": "2261110",
    "end": "2267380"
  },
  {
    "text": "it had more than 12,000\nstudents from more than 150 different countries\nacross the world.",
    "start": "2267380",
    "end": "2274730"
  },
  {
    "text": "And in the course, they\ngave a diagnostic exam to help students\nunderstand where",
    "start": "2274730",
    "end": "2279770"
  },
  {
    "text": "they're at with the material. And they wanted to be\nable to give feedback on the diagnostic. And what the diagnostic\nwas, is students submitted",
    "start": "2279770",
    "end": "2287810"
  },
  {
    "text": "open ended Python code\nsnippets that was trying to solve different problems. And they estimated\nthat if you were",
    "start": "2287810",
    "end": "2294349"
  },
  {
    "text": "to try to give feedback on\nall of the student's code, it would take a very\nlong time, in particular,",
    "start": "2294350",
    "end": "2301040"
  },
  {
    "text": "take you more than eight\nmonths to try to do that. And so for this reason\nactually in the first iteration",
    "start": "2301040",
    "end": "2306380"
  },
  {
    "text": "of the course, they\ndidn't give any feedback to students on this diagnostic. They just took it,\nand then they got",
    "start": "2306380",
    "end": "2311630"
  },
  {
    "text": "to see what the solutions were. We tried to see if we could\ngive feedback to the students. ",
    "start": "2311630",
    "end": "2318490"
  },
  {
    "text": "Now, there's a way to formulate\nthis as a classification problem. And in particular, in many\nof the courses that you've",
    "start": "2318490",
    "end": "2324819"
  },
  {
    "text": "probably taken at\nStanford, when you submit an assignment or an\nexam, you get feedback",
    "start": "2324820",
    "end": "2330220"
  },
  {
    "text": "through a rubric which tells\nyou what kinds of misconceptions you made on a given question.",
    "start": "2330220",
    "end": "2336790"
  },
  {
    "text": "And we can formulate\nrubric grading as a classification problem,\nwhere the input, the x example",
    "start": "2336790",
    "end": "2343089"
  },
  {
    "text": "is the open ended Python code. And the label corresponds\nto basically filling out",
    "start": "2343090",
    "end": "2350410"
  },
  {
    "text": "the rubric. And so each rubric will be\na multi-label classification problem where you need to be\nable to predict whether or not",
    "start": "2350410",
    "end": "2357160"
  },
  {
    "text": "the student has that\nparticular misconception. Now, you might think, well,\nOK, if this is a classification",
    "start": "2357160",
    "end": "2363580"
  },
  {
    "text": "problem, it must be\npretty easy to solve. Because classification is pretty\nstraightforward with machine",
    "start": "2363580",
    "end": "2369260"
  },
  {
    "text": "learning.  But the challenge with it\nis that, first, we don't",
    "start": "2369260",
    "end": "2375998"
  },
  {
    "text": "have that much annotation data. We don't have tons of data\non the internet that tells us feedback on student programs.",
    "start": "2375998",
    "end": "2382860"
  },
  {
    "text": "There's also these\nlong-tail distributions where students\nwill solve problems in many, many different ways.",
    "start": "2382860",
    "end": "2388700"
  },
  {
    "text": "And then lastly, and\nperhaps most importantly, every time you give an\nassignment or an exam,",
    "start": "2388700",
    "end": "2395140"
  },
  {
    "text": "usually it's somewhat different\nfrom the previous time that exam or\nassignment was given,",
    "start": "2395140",
    "end": "2402332"
  },
  {
    "text": "and then also oftentimes\nthe TAs are different, the instructors are\ndifferent, and so forth. And so you don't have a static\ndata set or a static problem.",
    "start": "2402332",
    "end": "2412000"
  },
  {
    "text": "You actually have lots\nof different assignments, exams, rubrics, student\nsolutions, and so forth.",
    "start": "2412000",
    "end": "2419440"
  },
  {
    "text": "Cool. So does anyone have any ideas\nfor how you could frame this as a meta learning problem? ",
    "start": "2419440",
    "end": "2432890"
  },
  {
    "text": "I guess to start off,\nthe meta training data set that we have available\nto us is four final exams and four midterms from CS106a.",
    "start": "2432890",
    "end": "2439460"
  },
  {
    "text": "This has a number of\ndifferent questions and each student solution\nhas feedback from a rubric. ",
    "start": "2439460",
    "end": "2449530"
  },
  {
    "text": "Yeah? I guess you can look at\nit question by question. And then for each question,\nyou have a rubric, and I guess each type of\ntask within the rubric,",
    "start": "2449530",
    "end": "2459220"
  },
  {
    "text": "would be task for the model. Yeah, exactly. So you could go\nquestion by question.",
    "start": "2459220",
    "end": "2464230"
  },
  {
    "text": "And essentially each\nitem on the rubric can correspond to a different\ntask, where you can formulate.",
    "start": "2464230",
    "end": "2471974"
  },
  {
    "text": "For each rubric,\nfor each question will basically be\na different task, and your goal is\nto very quickly be",
    "start": "2471975",
    "end": "2478410"
  },
  {
    "text": "able to give feedback on that\nrubric item with a small amount of label data. ",
    "start": "2478410",
    "end": "2485690"
  },
  {
    "text": "And so each rubric\nhas several items. And every question has\npossibly its own set",
    "start": "2485690",
    "end": "2493310"
  },
  {
    "text": "of rubric items and options. And so for that reason,\nyou can basically have each rubric item\nas a different task.",
    "start": "2493310",
    "end": "2500300"
  },
  {
    "text": "So if you had a string\ninsertion problem, and your rubric looks something\nlike this, where maybe one",
    "start": "2500300",
    "end": "2506121"
  },
  {
    "text": "option is for the\nsolution to be perfect, another option is for the\nstudent to incorrectly insert",
    "start": "2506122",
    "end": "2515030"
  },
  {
    "text": "the wrong character,\nand so on and so forth, then each item here can\nbe just a different task",
    "start": "2515030",
    "end": "2521000"
  },
  {
    "text": "for running meta training. And then once you\nhave all these tasks, you can apply the meta\nlearning algorithms",
    "start": "2521000",
    "end": "2528180"
  },
  {
    "text": "that we've looked\nat in the class. And so what we're\ngoing to do is we're going to apply prototypical\nnetworks, which is what we had",
    "start": "2528180",
    "end": "2534839"
  },
  {
    "text": "talked about on the last slide. There's also a typo in\nthis equation again. And x corresponds to a\nsequence of discrete tokens.",
    "start": "2534840",
    "end": "2543760"
  },
  {
    "text": "So it's going to be in\nthis case, Python code, or pseudo Python code, depending\non how good the student was.",
    "start": "2543760",
    "end": "2551380"
  },
  {
    "text": "And the embedding function,\nbecause it's going to be text, we're going to use a BERT based\nmodel called the RoBERTa model.",
    "start": "2551380",
    "end": "2559090"
  },
  {
    "text": "That's going to take as input\nthe Python code and output an embedding of\nthat Python program.",
    "start": "2559090",
    "end": "2564460"
  },
  {
    "text": " And then unfortunately, when we\nuse Prototypical Networks out",
    "start": "2564460",
    "end": "2572650"
  },
  {
    "text": "of the box with a RoBERTa\nmodel, it doesn't do very well. And so attention isn't\nquite all you need.",
    "start": "2572650",
    "end": "2580150"
  },
  {
    "text": "And there are some\ntricks that we needed to get it to work a lot better. The first was that instead\nof only using the tasks",
    "start": "2580150",
    "end": "2587830"
  },
  {
    "text": "from the rubrics\nin our data set, we can augment those tasks\nwith other tasks that can be constructed in a\nself-supervised manner.",
    "start": "2587830",
    "end": "2595660"
  },
  {
    "text": "So we can construct\na task, for example, by predicting the compilation\nerror that Python returns",
    "start": "2595660",
    "end": "2602140"
  },
  {
    "text": "if you try to run the program. We can also construct tasks\nsimilar to masked language",
    "start": "2602140",
    "end": "2608560"
  },
  {
    "text": "modeling where you are trying\nto predict the token that has been masked out.",
    "start": "2608560",
    "end": "2614381"
  },
  {
    "text": "And that can also be formulated\nas a classification problem where you want to\nclassify whether or not that token is a for token,\nor an in token, and so forth.",
    "start": "2614382",
    "end": "2622380"
  },
  {
    "text": "So that's going to augment\nour data set significantly. Another thing that we can do\nis instead of only giving it",
    "start": "2622380",
    "end": "2629119"
  },
  {
    "text": "a few examples, we can also\nincorporate side information. So we have information about\nthe name of the rubric option,",
    "start": "2629120",
    "end": "2636020"
  },
  {
    "text": "as well as the text\nof the question. And we can basically pass this\ninto the embedding function",
    "start": "2636020",
    "end": "2643820"
  },
  {
    "text": "to inform the network\nwhat the task might be.",
    "start": "2643820",
    "end": "2648950"
  },
  {
    "text": "And so when we encode\nthe program here, we're also going to prepend\nthe side information which",
    "start": "2648950",
    "end": "2655510"
  },
  {
    "text": "will also be encoded\nwith a BERT-like model. ",
    "start": "2655510",
    "end": "2661270"
  },
  {
    "text": "Great, and then the last trick\nis instead of only using data from the exams and assignments\nthat I mentioned before, we can",
    "start": "2661270",
    "end": "2668260"
  },
  {
    "text": "also use a pretrained model. So instead of randomly\ninitializing the RoBERTa model,",
    "start": "2668260",
    "end": "2673360"
  },
  {
    "text": "we'll pre-train it on with\na model called CodeBERT that was pretrained on a\nton of unlabeled Python code",
    "start": "2673360",
    "end": "2680860"
  },
  {
    "text": "from the internet.  Cool. So the gist of the method is\nstill Prototypical Networks.",
    "start": "2680860",
    "end": "2688020"
  },
  {
    "text": "It's just is that there's\nthree small changes to it. One is that we are using\nthese additional tasks.",
    "start": "2688020",
    "end": "2693250"
  },
  {
    "text": "The second is that we are\nincorporating the side information into the embedding. And the third is that\nwe're going to pretrain",
    "start": "2693250",
    "end": "2698730"
  },
  {
    "text": "the weights of the encoder. As a whole, the model\nlooks like this, where we have our side\ninformation that's",
    "start": "2698730",
    "end": "2705360"
  },
  {
    "text": "encoded and prepended into\nthe transformer layers. We embed the student\ncode into embeddings.",
    "start": "2705360",
    "end": "2711540"
  },
  {
    "text": "And we'll then average\nthose embeddings to form a prototype for each\nlabel for each rubric option.",
    "start": "2711540",
    "end": "2719340"
  },
  {
    "text": "And then we'll train\nthe whole network end to end, initialized\nwith the CodeBERT model. ",
    "start": "2719340",
    "end": "2727900"
  },
  {
    "text": "Cool. So how will all this work? Our first results were\nactually just offline,",
    "start": "2727900",
    "end": "2734109"
  },
  {
    "text": "on held out exams and held out\nrubrics from Stanford data.",
    "start": "2734110",
    "end": "2739960"
  },
  {
    "text": "We found that first\nit outperformed supervised learning\nby 8% to 17%,",
    "start": "2739960",
    "end": "2744970"
  },
  {
    "text": "which is fairly significant-- 8% in the held-out\nexam case, and 17% in the held-out rubric case.",
    "start": "2744970",
    "end": "2752455"
  },
  {
    "text": "In the case where you\nhave a held-out rubric, it's actually more\naccurate than a human TA. It turns out human\nTAs are actually not",
    "start": "2752455",
    "end": "2758410"
  },
  {
    "text": "that good at grading. Grading code actually\njust involves",
    "start": "2758410",
    "end": "2763780"
  },
  {
    "text": "debugging the code itself. And so, yeah, it's\nactually pretty hard. But there's also\nstill a lot of room",
    "start": "2763780",
    "end": "2770140"
  },
  {
    "text": "to grow in terms of\nthe held-out exam. Helds-out exams are harder\nthan held-out rubrics, because they might involve\ntokens that were unseen",
    "start": "2770140",
    "end": "2776560"
  },
  {
    "text": "during training because of\nthe questions and things",
    "start": "2776560",
    "end": "2781840"
  },
  {
    "text": "being entirely new.  The more exciting\nthing was actually",
    "start": "2781840",
    "end": "2788120"
  },
  {
    "text": "trying to actually deploy\nthis, in Code in Place. In particular, a lot of\nthis was in collaboration",
    "start": "2788120",
    "end": "2794694"
  },
  {
    "text": "with Chris Piech, and Chris\nPiech promised the Code in Place people that\nwe would give them feedback on their diagnostic.",
    "start": "2794695",
    "end": "2802100"
  },
  {
    "text": "On May 10th, the students\ntook the diagnostic. And I think we\nhad about one week to give feedback on\nall the assignments,",
    "start": "2802100",
    "end": "2808580"
  },
  {
    "text": "or on all the solutions. Alan and Chris\nmade a cool UI that",
    "start": "2808580",
    "end": "2814220"
  },
  {
    "text": "looks like this, where we paired\nthe predicted rubric option",
    "start": "2814220",
    "end": "2819650"
  },
  {
    "text": "with text that\ndescribes the feedback for that rubric option. And that was presented to the\nstudent in this purple box",
    "start": "2819650",
    "end": "2827359"
  },
  {
    "text": "here. The students then evaluated\nthe feedback, whether or not they agreed with the feedback\nor disagreed with the feedback.",
    "start": "2827360",
    "end": "2834590"
  },
  {
    "text": "And we also tried\nto use attention to highlight where the algorithm\nthought the error might be arising from.",
    "start": "2834590",
    "end": "2842158"
  },
  {
    "text": "And it's also worth mentioning\nthat things like syntax errors can prevent unit tests\nfrom being useful and giving feedback.",
    "start": "2842158",
    "end": "2848030"
  },
  {
    "text": "And so there are some solutions\nwe were able to give feedback",
    "start": "2848030",
    "end": "2853760"
  },
  {
    "text": "automatically to. But the bulk of the\nsolutions we were not able to give\nautomatic feedback on.",
    "start": "2853760",
    "end": "2860260"
  },
  {
    "text": "Yeah? Is there any\nadditional things you need to do in order\nto get the model",
    "start": "2860260",
    "end": "2865720"
  },
  {
    "text": "to pay attention to places where\nthe student might have gotten it wrong?",
    "start": "2865720",
    "end": "2871060"
  },
  {
    "text": "Specifically for\nthe highlighting? Yeah. Yeah, so the\nhighlighting, I think",
    "start": "2871060",
    "end": "2876618"
  },
  {
    "text": "it worked a pretty good\namount of the time. But it didn't work\n100% of the time. The way that we did that,\nI don't know if I would",
    "start": "2876618",
    "end": "2883494"
  },
  {
    "text": "necessarily recommend this. But the way that we did it,\nis we randomly masked out",
    "start": "2883495",
    "end": "2888520"
  },
  {
    "text": "part of the input,\nand then tried to see if the prediction\nof the model changed. And if it did change,\nthat's an indication",
    "start": "2888520",
    "end": "2894850"
  },
  {
    "text": "that the error may have occurred\nin that part of the program. But it wasn't the most reliable. And in general, interpretability\nis a very open area",
    "start": "2894850",
    "end": "2902710"
  },
  {
    "text": "of research in deep learning. ",
    "start": "2902710",
    "end": "2907779"
  },
  {
    "text": "Yeah? Was there an ablation study\nto see which one of the tricks helps the most?",
    "start": "2907780",
    "end": "2913450"
  },
  {
    "text": "Yeah. So there's a pretty\ndetailed ablation study. I don't have them\nin the slides here. But you can take a look\nat the paper to see it.",
    "start": "2913450",
    "end": "2920450"
  },
  {
    "text": "The gist is that actually\nall of them helped a lot. And the three that\nI covered, I think",
    "start": "2920450",
    "end": "2925750"
  },
  {
    "text": "that they're helping by\nupwards of 10% accuracy.",
    "start": "2925750",
    "end": "2931760"
  },
  {
    "text": "Yeah? So can this model [INAUDIBLE]\nlogic in edge cases, other than [INAUDIBLE],, like\nwhen a student makes an error,",
    "start": "2931760",
    "end": "2939230"
  },
  {
    "text": "with an edge case or\nsomething like that? Yeah, so the question\nis, can this model also understand if students--",
    "start": "2939230",
    "end": "2945710"
  },
  {
    "text": "like kind of edge\ncases and so forth. So it's only going to understand\nthe rubric options you give it.",
    "start": "2945710",
    "end": "2954522"
  },
  {
    "text": "And so if there's something\nthat's not on the rubric, it's not going to\nunderstand that.",
    "start": "2954522",
    "end": "2960040"
  },
  {
    "text": "And it also still\nneeds a few examples of test time for the rubric.",
    "start": "2960040",
    "end": "2965325"
  },
  {
    "text": " Cool. So let's get into\nsome of the results",
    "start": "2965325",
    "end": "2970562"
  },
  {
    "text": "from actually the\nlive deployment. So we actually gave this\nto the students in the Code",
    "start": "2970562",
    "end": "2976100"
  },
  {
    "text": "in Place course. We got humans to\nactually volunteer to give 1,000 feedback,\nor feedback on 1,000",
    "start": "2976100",
    "end": "2983630"
  },
  {
    "text": "different student solutions. And then we had the system\ngive feedback on the remaining 15,000 solutions.",
    "start": "2983630",
    "end": "2990280"
  },
  {
    "text": "And then around 2,000 of the\nsolutions could be auto graded, and then they weren't included\nin any of the analysis.",
    "start": "2990280",
    "end": "2995870"
  },
  {
    "text": " And then in terms of giving\nthe feedback, for the 15,000",
    "start": "2995870",
    "end": "3004150"
  },
  {
    "text": "that were graded by the\nsystem, that's the feedback that the students got. For the 1,000, they\ngot the human feedback.",
    "start": "3004150",
    "end": "3010380"
  },
  {
    "text": "The students didn't know if\nthey were getting human feedback or AI based feedback. Although they did know\nthat we were running",
    "start": "3010380",
    "end": "3016300"
  },
  {
    "text": "a study as part of the process.  First, we looked at how\nmuch the students agreed",
    "start": "3016300",
    "end": "3023150"
  },
  {
    "text": "with the feedback from the\nPrototypical Networks based system versus the\nhuman feedback. And we were actually\nsomewhat surprised",
    "start": "3023150",
    "end": "3030740"
  },
  {
    "text": "to see that they actually\nagreed with the feedback from the system slightly more.",
    "start": "3030740",
    "end": "3036650"
  },
  {
    "text": "And also in general, they\nagreed with the feedback a lot. So in general, it was like-- I think it was 97% versus\n98%, or maybe 96% versus 97%.",
    "start": "3036650",
    "end": "3045270"
  },
  {
    "text": "So it seemed to work\nactually quite well. Now, they might agree\nwith the feedback,",
    "start": "3045270",
    "end": "3051080"
  },
  {
    "text": "but not actually find it useful. For example, if you\nalways said good job, they might kind of\nagree with that, but not find it very useful.",
    "start": "3051080",
    "end": "3056820"
  },
  {
    "text": "And so we asked them how useful\nthey found the feedback out of a rating from 1 to 5. And they gave it a 4.6 out of\n5 in terms of usefulness, which",
    "start": "3056820",
    "end": "3065660"
  },
  {
    "text": "suggests that they actually\nfind it useful in pointing out their misconceptions.",
    "start": "3065660",
    "end": "3070920"
  },
  {
    "text": "Yeah? So [INAUDIBLE] not\nall of those students would have responded\nwith yes or no.",
    "start": "3070920",
    "end": "3076620"
  },
  {
    "text": "Some of the students would\nnot have responded at all. So is it the same\ncomponent of bias? Was is [INAUDIBLE] and\nfor the 1,000 students,",
    "start": "3076620",
    "end": "3087660"
  },
  {
    "text": "that would like\nget the score to-- give the feedback properly?",
    "start": "3087660",
    "end": "3092752"
  },
  {
    "text": "So you're asking what\nabout students who didn't give feedback on the feedback? Is it effective of\n[INAUDIBLE] experience",
    "start": "3092752",
    "end": "3098850"
  },
  {
    "text": "giving or not giving the\nfeedback, if [INAUDIBLE] Yeah, that's a great question. I mean so first the\nstudents didn't know if it",
    "start": "3098850",
    "end": "3104550"
  },
  {
    "text": "was human feedback or not. And so this comparison\nshould very much hold water, because they're\nnot going to like abstain",
    "start": "3104550",
    "end": "3110790"
  },
  {
    "text": "at different rates. The other thing that we\ndid is in the interface we gave the feedback one by\none, and to go to the next one,",
    "start": "3110790",
    "end": "3118085"
  },
  {
    "text": "to actually see the feedback\nfor the next question, they had to give feedback\non the first question.",
    "start": "3118085",
    "end": "3123130"
  },
  {
    "text": "And so we required that\nthey stated whether or not they agreed with it or not.",
    "start": "3123130",
    "end": "3129000"
  },
  {
    "text": "It's possible that not\nall students actually looked at the feedback. I don't know exactly what\nthe rate of that was.",
    "start": "3129000",
    "end": "3135660"
  },
  {
    "text": "Yeah? Did experts also grade this,\napart from the students by any chance?",
    "start": "3135660",
    "end": "3141270"
  },
  {
    "text": "The humans here were actually\nkind of basically volunteer TAs for the course.",
    "start": "3141270",
    "end": "3146790"
  },
  {
    "text": "I meant the AI feedback. Did any experts also\nreview the feedback",
    "start": "3146790",
    "end": "3151830"
  },
  {
    "text": "and said whether the feedback\nwould be useful or not? Got it. Yeah, we did not have\nexperts look at the feedback.",
    "start": "3151830",
    "end": "3158692"
  },
  {
    "text": "So these were the only\nevaluations that we ran. ",
    "start": "3158692",
    "end": "3164589"
  },
  {
    "text": "Yeah? Was there a difference\nin the usefulness found between the AI feedback\nand the human feedback?",
    "start": "3164590",
    "end": "3171390"
  },
  {
    "text": "That's a good question. I don't know off\nthe top of my head. But I think the answer was no.",
    "start": "3171390",
    "end": "3176650"
  },
  {
    "text": "And I should mention\nthat in some ways this is kind of a hybrid\nhuman AI system in some ways.",
    "start": "3176650",
    "end": "3181680"
  },
  {
    "text": "Because this text was\nwritten by Chris actually.",
    "start": "3181680",
    "end": "3187450"
  },
  {
    "text": "And so it was doing the\nbulk of the work, which was to figure out which\nrubric item it was.",
    "start": "3187450",
    "end": "3192973"
  },
  {
    "text": "And then for each of\nthose rubric items, Chris wrote that text. And the text from Chris\nwas put in the box.",
    "start": "3192973",
    "end": "3199529"
  },
  {
    "text": "This still saved like almost\nall of the work needed to give feedback. Because Chris just\nneeded to enter",
    "start": "3199530",
    "end": "3205307"
  },
  {
    "text": "that text for each of the\nitems in the rubric once. But I think that because\nit's very similar,",
    "start": "3205307",
    "end": "3211240"
  },
  {
    "text": "I wouldn't expect there to\nbe a significant difference in usefulness. ",
    "start": "3211240",
    "end": "3218869"
  },
  {
    "text": "Cool. And then the last\nthing that we checked, just as a sanity check, we\nweren't really expecting to see any bias. But we wanted to check that\nthe model wasn't picking up",
    "start": "3218870",
    "end": "3228040"
  },
  {
    "text": "on something that we\nwouldn't be aware of. And so we looked at the\nagreement by the different--",
    "start": "3228040",
    "end": "3233800"
  },
  {
    "text": "the two most common\ngender demographics, and the three most common\ncountries in the data set.",
    "start": "3233800",
    "end": "3240730"
  },
  {
    "text": "And saw that there\nweren't any signs of bias. This was expected, because\nwe removed all the comments",
    "start": "3240730",
    "end": "3247420"
  },
  {
    "text": "from the code. And really there probably\nisn't that much information in the code that reveals\nthis sort of thing.",
    "start": "3247420",
    "end": "3254150"
  },
  {
    "text": "But it's always good to\ndo these sorts of checks in these kinds of\nstudies as well. ",
    "start": "3254150",
    "end": "3261930"
  },
  {
    "text": "Cool. Yeah, so that was a case study\nof actually using Prototypical Networks in a real application.",
    "start": "3261930",
    "end": "3269040"
  },
  {
    "text": "We're also trying to\ncontinue to improve on it. We actually have a\nlittle bit more data now. And so we're hoping to improve\nthat bar a little bit more.",
    "start": "3269040",
    "end": "3277115"
  },
  {
    "text": " Now, I'd like to\ntalk a little bit about how the different\napproaches compare.",
    "start": "3277115",
    "end": "3283673"
  },
  {
    "text": "So we've talked about\nblack box meta learning. We talked about optimization\nbased meta learning. And today we talked about these\nmore non-parametric approaches.",
    "start": "3283673",
    "end": "3290480"
  },
  {
    "text": "And I think it's useful to take\na high level view to understand when should you use one\nalgorithm versus the other,",
    "start": "3290480",
    "end": "3295730"
  },
  {
    "text": "and how do they\ncompare to each other. First, we can compare them\nat a conceptual level.",
    "start": "3295730",
    "end": "3302260"
  },
  {
    "text": "So we walked through this\ncomputation graph perspective",
    "start": "3302260",
    "end": "3307570"
  },
  {
    "text": "on meta learning in\nMonday's lecture. And we saw that both\nblack box meta learning and optimization\nbased meta learning",
    "start": "3307570",
    "end": "3314050"
  },
  {
    "text": "are both a\ncomputation graph that takes as input the training\ndata set and the test example, and makes a prediction\nfor that test example.",
    "start": "3314050",
    "end": "3323950"
  },
  {
    "text": "Non-parametric approaches can\nbe viewed in the same exact way. So you can also view something\nlike prototypical networks",
    "start": "3323950",
    "end": "3330190"
  },
  {
    "text": "as having the same form\nof computation graph. It's just what happens on the\ninside and the inner loop that",
    "start": "3330190",
    "end": "3336160"
  },
  {
    "text": "differs between it. And so in particular,\nit makes its predictions",
    "start": "3336160",
    "end": "3344560"
  },
  {
    "text": "for test inputs using something\nlike nearest neighbors or nearest neighbors\nto prototypes,",
    "start": "3344560",
    "end": "3350109"
  },
  {
    "text": "where the equation\nfor the prototype is the average embedding. So as a whole, you can view\nall of these approaches",
    "start": "3350110",
    "end": "3357160"
  },
  {
    "text": "as being the same family of\nmeta learning algorithms. And they're all optimized end\nto end with respect to some meta",
    "start": "3357160",
    "end": "3364570"
  },
  {
    "text": "learning objective. The difference is\njust whether or not you don't give it\nany structure at all and you represent it as a\nneural network, versus embedding",
    "start": "3364570",
    "end": "3371710"
  },
  {
    "text": "gradient descent, versus\nembedding something like nearest neighbors. This also suggests\nthat it's possible",
    "start": "3371710",
    "end": "3377319"
  },
  {
    "text": "that there may be a fourth\nclass of approach that comes up that embeds something\nthat looks a little bit different from\nthese other methods.",
    "start": "3377320",
    "end": "3385150"
  },
  {
    "text": "And like we mentioned\nbefore, you can also, again, mix and match different aspects\nof this computation graph,",
    "start": "3385150",
    "end": "3391360"
  },
  {
    "text": "and get algorithms\nthat aren't clearly in one of these three buckets.",
    "start": "3391360",
    "end": "3397810"
  },
  {
    "text": "So for example, there are\nalgorithms that both condition the network on the data\nand run gradient descent.",
    "start": "3397810",
    "end": "3405309"
  },
  {
    "text": "So that would be a hybrid\nbetween a black box approach and an optimization\nbased approach. There is also an algorithm\nthat does something",
    "start": "3405310",
    "end": "3412780"
  },
  {
    "text": "like relation networks, which we\nhad talked about here where you learn this distance\nfunction, and actually run",
    "start": "3412780",
    "end": "3420190"
  },
  {
    "text": "gradient descent on that\nrelation network embedding. And so that would be a\nhybrid between optimization based and non-parametric based.",
    "start": "3420190",
    "end": "3427128"
  },
  {
    "text": "And then there's also approaches\nthat do something like MAML, but initialize the last layer\nas a Prototypical Network.",
    "start": "3427128",
    "end": "3437799"
  },
  {
    "text": "So I guess in\ngeneral I think it's useful and pedagogical to\nthink about these three different categories.",
    "start": "3437800",
    "end": "3443200"
  },
  {
    "text": "But there really is a\nspectrum in between these and there's lots\nof algorithms that don't fall into one of these\nthree categories super cleanly.",
    "start": "3443200",
    "end": "3450475"
  },
  {
    "start": "3450475",
    "end": "3456870"
  },
  {
    "text": "Then beyond the\nconceptual similarities",
    "start": "3456870",
    "end": "3462930"
  },
  {
    "text": "between these algorithms, we can\nalso think about the properties that they have. One property that we've\ntalked about a little bit",
    "start": "3462930",
    "end": "3469260"
  },
  {
    "text": "is expressive power. And by that, I mean the\nability for the learner to represent a wide range\nof learning procedures.",
    "start": "3469260",
    "end": "3479010"
  },
  {
    "text": "And we want to have expressive\npower, because it means that we might be able\nto scale to scenarios where we have lots of\nmeta training data,",
    "start": "3479010",
    "end": "3485130"
  },
  {
    "text": "and we want to learn a really\ngood learning algorithm. And it also means that it\nmight be applicable to a wider",
    "start": "3485130",
    "end": "3491905"
  },
  {
    "text": "range of domains, where maybe\nwe don't have good learning algorithms.",
    "start": "3491905",
    "end": "3497530"
  },
  {
    "text": "Beyond expressive power,\nthere's a second property that I think is useful, which\nI'll refer to as consistency,",
    "start": "3497530",
    "end": "3505590"
  },
  {
    "text": "which is that regardless of\nwhat you do in the meta training process, it would be nice\nif your learning procedure",
    "start": "3505590",
    "end": "3513470"
  },
  {
    "text": "monotonically improved as you\ngave it more training data.",
    "start": "3513470",
    "end": "3518940"
  },
  {
    "text": "And this is useful\nbecause you might get a test task that's pretty\ndifferent from your meta",
    "start": "3518940",
    "end": "3524400"
  },
  {
    "text": "training tasks. And if you do get an out\nof distribution task, it'd be nice if your algorithm\nstill does something somewhat",
    "start": "3524400",
    "end": "3530400"
  },
  {
    "text": "reasonable on those tasks. And so this property\nwill reduce the reliance on having a large number\nof meta training tasks.",
    "start": "3530400",
    "end": "3537210"
  },
  {
    "text": "And it should give you, in\nprinciple, somewhat better OOD task performance.",
    "start": "3537210",
    "end": "3543327"
  },
  {
    "text": " And you can remember\nthe performance",
    "start": "3543327",
    "end": "3548890"
  },
  {
    "text": "that we looked at when we\nmade the task more and more out of distribution, and we saw\nthat an algorithm like MAML,",
    "start": "3548890",
    "end": "3555250"
  },
  {
    "text": "which is actually\nconsistent, it does better than algorithms that\nare not consistent.",
    "start": "3555250",
    "end": "3560950"
  },
  {
    "text": "Yeah? Is consistency the same\nconcept as generalizability?",
    "start": "3560950",
    "end": "3567020"
  },
  {
    "text": "The question is, is\nconsistency the same concept as generalizability. I think that consistency implies\nthat the algorithm should",
    "start": "3567020",
    "end": "3575770"
  },
  {
    "text": "generalize better,\nbut not vice versa. You could have something that\ndoes seem to generalize well,",
    "start": "3575770",
    "end": "3583119"
  },
  {
    "text": "but isn't guaranteed to improve\nwith more data in expectation.",
    "start": "3583120",
    "end": "3590320"
  },
  {
    "text": "Yeah, they're certainly related. ",
    "start": "3590320",
    "end": "3596930"
  },
  {
    "text": "Cool. So I think these properties\nare pretty important for a lot of applications. And we can think of\neach of these algorithms",
    "start": "3596930",
    "end": "3603230"
  },
  {
    "text": "within the context\nof those properties, and black box methods have\ncomplete expressive power,",
    "start": "3603230",
    "end": "3609590"
  },
  {
    "text": "but are not consistent. If you give them more data, they\nwon't necessarily get better.",
    "start": "3609590",
    "end": "3615560"
  },
  {
    "text": "Optimization based methods\nare consistent in that they reduce to gradient descent. And they're expressive\nif you give them",
    "start": "3615560",
    "end": "3622250"
  },
  {
    "text": "a very deep model for\nsupervised learning settings. ",
    "start": "3622250",
    "end": "3628710"
  },
  {
    "text": "And then non-parametric\nmethods are expressive for most\narchitectures that you give it.",
    "start": "3628710",
    "end": "3637200"
  },
  {
    "text": "And under certain conditions,\nthey can also be consistent. Essentially, you can expect\nthem to be consistent",
    "start": "3637200",
    "end": "3645720"
  },
  {
    "text": "if you're embedding\nfunction doesn't compress too much about the input. ",
    "start": "3645720",
    "end": "3652110"
  },
  {
    "text": "If it compresses\ntoo much, then it may no longer be\nconsistent because you may have two examples that are\nvery similar to each other,",
    "start": "3652110",
    "end": "3658832"
  },
  {
    "text": "but your embedding\nspace doesn't actually put them as similar to\neach other, because it kind of compressed away\nsome of the details.",
    "start": "3658832",
    "end": "3664530"
  },
  {
    "text": " And then beyond\nthese properties,",
    "start": "3664530",
    "end": "3671119"
  },
  {
    "text": "I think there's other pros\nand cons of these algorithms as well. We talked a little bit\nabout the pros and cons",
    "start": "3671120",
    "end": "3677030"
  },
  {
    "text": "of back box and\noptimization based methods in the previous lectures. And these were that black\nbox is easy to combine",
    "start": "3677030",
    "end": "3683120"
  },
  {
    "text": "with a variety of\nlearning problems, but can be challenging to\noptimize and as a result, be data inefficient.",
    "start": "3683120",
    "end": "3689520"
  },
  {
    "text": "Optimization based meta\nlearning algorithms have a positive inductive bias at\nthe start of meta learning, and they also are pretty good\nat handling varying amounts of K",
    "start": "3689520",
    "end": "3698030"
  },
  {
    "text": "and large numbers\nof examples well, because you're averaging\nacross those examples when you compute the gradient.",
    "start": "3698030",
    "end": "3705410"
  },
  {
    "text": "But they involve a\nsecond order optimization that can be compute\nand memory intensive.",
    "start": "3705410",
    "end": "3712930"
  },
  {
    "text": "Now for non-parametric\napproaches, they're an entirely\nfeedforward process.",
    "start": "3712930",
    "end": "3718599"
  },
  {
    "text": "You never have this\ngradient descent step. And so as a result, they end\nup being computationally very",
    "start": "3718600",
    "end": "3723790"
  },
  {
    "text": "fast, and usually\npretty easy to optimize.",
    "start": "3723790",
    "end": "3728940"
  },
  {
    "text": "In practice, people have found\nthat if you have varying K, It's somewhat\nharder to generalize to varying K. I don't honestly\nhave great intuition for why",
    "start": "3728940",
    "end": "3737022"
  },
  {
    "text": "this is the case. But it's kind of an\nempirical observation that people have\nmade in the past,",
    "start": "3737022",
    "end": "3742589"
  },
  {
    "text": "and also that it can\nbe difficult to scale to very large K. The reason why it's difficult\nto scale to very large K",
    "start": "3742590",
    "end": "3748500"
  },
  {
    "text": "is because you have\nto make comparisons to all the examples\nin your training set.",
    "start": "3748500",
    "end": "3754650"
  },
  {
    "text": "And in general,\nnearest neighbors and non-parametric methods don't\nscale to very large data sets",
    "start": "3754650",
    "end": "3761310"
  },
  {
    "text": "because the runtime of\nyour algorithm is O of n,",
    "start": "3761310",
    "end": "3767340"
  },
  {
    "text": "or O of n times K in this case. And then the last thing\nwhich was mentioned before is that these\nmethods are entirely limited",
    "start": "3767340",
    "end": "3774839"
  },
  {
    "text": "to classification. ",
    "start": "3774840",
    "end": "3780730"
  },
  {
    "text": "Cool. And then I guess the other\nthing that I'll mention here is that on many few-shot\nlearning benchmarks,",
    "start": "3780730",
    "end": "3788170"
  },
  {
    "text": "if you tune these\nalgorithms well, oftentimes you'll see\nthat all three of these can perform quite well on\nthose benchmark problems.",
    "start": "3788170",
    "end": "3795202"
  },
  {
    "text": "I think this says\na little bit more about the benchmarks\nthan the methods. I think the benchmarks may\nbe a little bit too easy.",
    "start": "3795202",
    "end": "3802150"
  },
  {
    "text": "And I think that\nwhich method you use will depend on your use case. So in general, my\nrecommendation is",
    "start": "3802150",
    "end": "3808990"
  },
  {
    "text": "if you have a supervised\nclassification problem, I think that non-parametric\nmethods would generally",
    "start": "3808990",
    "end": "3815548"
  },
  {
    "text": "be my recommendation,\nbecause they're very fast and easy to optimize. And they tend to work\nwell for classification.",
    "start": "3815548",
    "end": "3822272"
  },
  {
    "text": "But if you don't have a\nclassification problem, then you need to use one of\nthe other two approaches.",
    "start": "3822272",
    "end": "3827560"
  },
  {
    "text": "And usually in that case, I\nmight recommend an optimization based meta learning approach. And there are a few\ninstances where I might",
    "start": "3827560",
    "end": "3835960"
  },
  {
    "text": "recommend a black box approach. Black box approaches\nI think are actually quite useful in reinforcement\nlearning scenarios, in part",
    "start": "3835960",
    "end": "3845230"
  },
  {
    "text": "because we don't actually have\ngood inner loop optimization methods for\nreinforcement learning.",
    "start": "3845230",
    "end": "3850790"
  },
  {
    "text": "And then lastly, if you\nhave a ton and ton of data, things like black box\napproaches can make sense",
    "start": "3850790",
    "end": "3856030"
  },
  {
    "text": "because they're so expressive,\nand because you don't care as much about data efficiency. And that's why we've\nseen things like GPT-3",
    "start": "3856030",
    "end": "3863319"
  },
  {
    "text": "I think be very effective\nwith a black box model.",
    "start": "3863320",
    "end": "3868820"
  },
  {
    "text": "Yeah? What did you exactly here\nmean by entirely feedforward?",
    "start": "3868820",
    "end": "3874470"
  },
  {
    "text": "We are doing training, correct? Right, so by\nentirely feedforward I mean that at meta test time,\nit's entirely feedforward.",
    "start": "3874470",
    "end": "3881700"
  },
  {
    "text": "Like the inner loop process is\na feedforward computation graph. ",
    "start": "3881700",
    "end": "3888119"
  },
  {
    "text": "But actually the same could\nbe true for a black box model as well. Although black box\nmodels often may",
    "start": "3888120",
    "end": "3893220"
  },
  {
    "text": "have some amount of recurrence. But yeah, so I mean in contrast\nto optimization based methods",
    "start": "3893220",
    "end": "3899315"
  },
  {
    "text": "where you're running gradient\ndescent at test time, these approaches are\njust a forward pass",
    "start": "3899315",
    "end": "3904932"
  },
  {
    "text": "through a neural network. Yeah? So for all of these meta\nlearning algorithms,",
    "start": "3904932",
    "end": "3912740"
  },
  {
    "text": "we still need to have a very\nlarge number of tasks for them to sample from ID.",
    "start": "3912740",
    "end": "3919880"
  },
  {
    "text": "If we have a small\nnumber of tasks, where generating a new\ntask is still expensive,",
    "start": "3919880",
    "end": "3925290"
  },
  {
    "text": "so we only have dozens of\ntasks instead of hundreds or thousands, would\nyou still recommend",
    "start": "3925290",
    "end": "3931099"
  },
  {
    "text": "using multi-task learning\ninstead of meta learning or other ways to modify them? Yeah, so the question\nis, all of these methods",
    "start": "3931100",
    "end": "3938450"
  },
  {
    "text": "require a fairly\nlarge number of tasks. And if you have a\nsmall number of tasks, would I recommend using\nmulti-task learning",
    "start": "3938450",
    "end": "3944690"
  },
  {
    "text": "instead of learning? Yeah. So first off, I would\nsay that in general,",
    "start": "3944690",
    "end": "3950329"
  },
  {
    "text": "in settings where you have\na small number of tasks, that's where consistency\ncan be very important.",
    "start": "3950330",
    "end": "3955940"
  },
  {
    "text": "Because if it is still\nrunning something like gradient descent\non that new task, then it is less reliant on\nhaving a large number of meta",
    "start": "3955940",
    "end": "3965320"
  },
  {
    "text": "training tasks. ",
    "start": "3965320",
    "end": "3970572"
  },
  {
    "text": "And then there's I\nthink two other things that you can do when you\nhave a small number of tasks. The first is that you can do\ntask augmentation, which we",
    "start": "3970572",
    "end": "3977410"
  },
  {
    "text": "saw in the education example. In some scenarios, it's\npossible to come up",
    "start": "3977410",
    "end": "3982420"
  },
  {
    "text": "with those kinds of tasks. In other scenarios, it might\nbe more difficult. And then whether or not to use multi-task\nlearning or meta learning,",
    "start": "3982420",
    "end": "3989170"
  },
  {
    "text": "I think it depends a lot on\nthe particulars of the problem.",
    "start": "3989170",
    "end": "3996842"
  },
  {
    "text": "In general, if there's\na small number of tasks, I do think that multi-task\nlearning can be a better",
    "start": "3996842",
    "end": "4002760"
  },
  {
    "text": "approach, especially if\nyou don't mind training on all the tasks at once.",
    "start": "4002760",
    "end": "4010349"
  },
  {
    "text": "Whereas in meta\nlearning, one thing that's nice about these\nmeta learning algorithms is you can train on all your\nmeta training tasks first, then",
    "start": "4010350",
    "end": "4018120"
  },
  {
    "text": "encapsulate that into a model,\nthrow away your meta training task data, and then apply\nthat to the meta test task.",
    "start": "4018120",
    "end": "4023859"
  },
  {
    "text": "So I think that generally,\nwhether to use meta learning",
    "start": "4023860",
    "end": "4029058"
  },
  {
    "text": "or whether to use\nmulti-task learning will depend a bit on the\nconsiderations like that.",
    "start": "4029058",
    "end": "4035100"
  },
  {
    "text": " Yeah?",
    "start": "4035100",
    "end": "4040650"
  },
  {
    "text": "What is [INAUDIBLE]\nor why do you",
    "start": "4040650",
    "end": "4047200"
  },
  {
    "text": "prefer non-parametric\nover optimized [INAUDIBLE] because I think [INAUDIBLE]. Is it just because compute, or\nis there's another reason as to",
    "start": "4047200",
    "end": "4055220"
  },
  {
    "text": "[INAUDIBLE]? Yeah, so the question was\nfor classification tasks, is there a reason to prefer\nnon-parametric methods",
    "start": "4055220",
    "end": "4061950"
  },
  {
    "text": "over optimization based methods. And generally, my recommendation\nis just because of compute.",
    "start": "4061950",
    "end": "4067289"
  },
  {
    "text": "These generally will\ntypically require less compute during the meta\ntraining process.",
    "start": "4067290",
    "end": "4072660"
  },
  {
    "text": "They'll also be a little\nbit lighter weight. And so in the\neducation example, when",
    "start": "4072660",
    "end": "4079530"
  },
  {
    "text": "we were processing Python code\nwith these BERT based models, trying to do a bilevel\noptimization with a BERT model",
    "start": "4079530",
    "end": "4085829"
  },
  {
    "text": "gets computationally\nexpensive fairly quickly. And if you have a\nfeedforward process,",
    "start": "4085830",
    "end": "4091800"
  },
  {
    "text": "these computational\nbenefits are quite nice. But again, that's my default\nor rough recommendation.",
    "start": "4091800",
    "end": "4098729"
  },
  {
    "text": "And there may be applications\nwhere an optimization based method may be\npreferable, especially",
    "start": "4098729",
    "end": "4104880"
  },
  {
    "text": "for example, if you have a\nlarger K, or a variant K, or something like that. ",
    "start": "4104880",
    "end": "4112560"
  },
  {
    "text": "Yeah? You said there\nwere intermediates, or there was a\nspectrum between these. What would be a combination\nbetween a non-parametric method",
    "start": "4112560",
    "end": "4120509"
  },
  {
    "text": "and one of the other two? Yeah, so I mentioned\none combination",
    "start": "4120510",
    "end": "4128016"
  },
  {
    "text": "on the previous slide,\nand particularly what it was doing is it was--",
    "start": "4128016",
    "end": "4134551"
  },
  {
    "text": "oh, I didn't mention\non this slide. Which slide was it? So these second two\nmethods are examples",
    "start": "4134552",
    "end": "4141549"
  },
  {
    "text": "of hybrids of non-parametric\nand optimization based methods. For example, the\nsecond one, it actually",
    "start": "4141550",
    "end": "4149080"
  },
  {
    "text": "learns this embedding space,\nand then does gradient descent on that embedding space. And so that's a little\nbit of a hybrid.",
    "start": "4149080",
    "end": "4154960"
  },
  {
    "text": "I also think that matching\nnetworks itself is somewhat of a hybrid between\nnon-parametric methods and black box methods, because\nit uses this bidirectional LSTM",
    "start": "4154960",
    "end": "4163189"
  },
  {
    "text": "to encode the examples\nin the training set. ",
    "start": "4163189",
    "end": "4171500"
  },
  {
    "text": "Cool. Great. And then in the last eight\nminutes, or actually before we",
    "start": "4171500",
    "end": "4178310"
  },
  {
    "text": "do that, first, there's also\na third property which we'll talk about in a couple of weeks,\nwhich is uncertainty awareness.",
    "start": "4178310",
    "end": "4185660"
  },
  {
    "text": "In general, if you're in a\nfew-shot learning regime, there may be some ambiguity with\nrespect to what the task is.",
    "start": "4185660",
    "end": "4191568"
  },
  {
    "text": "And it'd be nice if the\nnetwork would tell you, given the few\nexamples that I have,",
    "start": "4191569",
    "end": "4196650"
  },
  {
    "text": "I need more data in order to\nfigure out a good classifier for this task.",
    "start": "4196650",
    "end": "4201760"
  },
  {
    "text": "And this can be useful in active\nlearning settings and settings where you need calibrated\nuncertainty estimates, like in safety\ncritical settings,",
    "start": "4201760",
    "end": "4208470"
  },
  {
    "text": "also in reinforcement learning. Or if you care about driving\nthings from first principles",
    "start": "4208470",
    "end": "4214409"
  },
  {
    "text": "from a Bayesian standpoint,\nthis can also be useful. And we'll talk about\nuncertainty where",
    "start": "4214410",
    "end": "4220320"
  },
  {
    "text": "meta learning algorithms in-- actually, it might\nbe three weeks. It's either two\nweeks or three weeks,",
    "start": "4220320",
    "end": "4225590"
  },
  {
    "text": "in Bayesian meta\nlearning lecture.  Cool.",
    "start": "4225590",
    "end": "4230990"
  },
  {
    "text": "So in the last\nseven minutes, I'd like to just briefly run\nthrough a few applications, just to give you even more of\nan idea of the kinds of problems",
    "start": "4230990",
    "end": "4239510"
  },
  {
    "text": "that these methods\ncan be applied to. And also in some\nways some of these",
    "start": "4239510",
    "end": "4246200"
  },
  {
    "text": "are actually somewhat\ncreative in actually how they use meta learning algorithms. So in the first\nlecture, I showed you",
    "start": "4246200",
    "end": "4251600"
  },
  {
    "text": "this video of doing\none-shot imitation learning. And the different\ntasks corresponded",
    "start": "4251600",
    "end": "4257030"
  },
  {
    "text": "to manipulating\ndifferent objects. And the training\nexample corresponded",
    "start": "4257030",
    "end": "4262430"
  },
  {
    "text": "to a video of a human. And the test\nexample corresponded to a teleoperated\ndemonstration of that task.",
    "start": "4262430",
    "end": "4271370"
  },
  {
    "text": "The model here was\nan optimization based meta learning algorithm. So it was something like MAML.",
    "start": "4271370",
    "end": "4276559"
  },
  {
    "text": "And one thing that\nwas cool about this is that when you have\na video of a human, you don't actually have labels.",
    "start": "4276560",
    "end": "4284000"
  },
  {
    "text": "You just have input images. You don't know what actions\nthe robot should take. And so this approach actually\nused a learned inner loss",
    "start": "4284000",
    "end": "4291800"
  },
  {
    "text": "function. And so instead of only\nlearning the initialization of the model, it\nalso learned a loss",
    "start": "4291800",
    "end": "4297380"
  },
  {
    "text": "function that was used\nto run gradient descent in the inner loop.",
    "start": "4297380",
    "end": "4303427"
  },
  {
    "text": "And so this is an\nexample of something where instead of\nonly meta learning",
    "start": "4303427",
    "end": "4309991"
  },
  {
    "text": "one component of the\nsystem, you can meta learn other components\nof the system as well.",
    "start": "4309992",
    "end": "4315590"
  },
  {
    "text": "And so the way that this ended\nup working is that at test time you give it a video of\na human doing the task.",
    "start": "4315590",
    "end": "4322389"
  },
  {
    "text": "It runs gradient\ndescent on this video with the learned\ninner loss function",
    "start": "4322390",
    "end": "4327730"
  },
  {
    "text": "to get a policy for the task. And then the result of running\nthat policy on the robot",
    "start": "4327730",
    "end": "4333490"
  },
  {
    "text": "is something like this, where\nit can successfully figure out that it should be\ngoing to the red bowl.",
    "start": "4333490",
    "end": "4339310"
  },
  {
    "text": " A second example is\nlooking at predicting",
    "start": "4339310",
    "end": "4348400"
  },
  {
    "text": "the properties of molecules. This is potentially useful in\ndrug discovery problems, where you only have a small amount of\nexperimental data for a given",
    "start": "4348400",
    "end": "4356530"
  },
  {
    "text": "molecule.  The task is to predict the\nproperties and activities",
    "start": "4356530",
    "end": "4363010"
  },
  {
    "text": "of different molecules. For example, you might\nhave some experiments that are cheaper or easier\nto run, and you",
    "start": "4363010",
    "end": "4368740"
  },
  {
    "text": "want to predict activities\nthat are more expensive to run. And here they used\noptimization based methods.",
    "start": "4368740",
    "end": "4375849"
  },
  {
    "text": "They actually looked at MAML,\nfirst order MAML, and a variant of MAML that only\nupdates the last layer",
    "start": "4375850",
    "end": "4383650"
  },
  {
    "text": "of the model in the inner loop. And they're using a graph neural\nnetwork as the base model.",
    "start": "4383650",
    "end": "4390267"
  },
  {
    "text": "And they found that these\nmeta learning algorithms were able to do better than fine\ntuning or K nearest neighbors.",
    "start": "4390267",
    "end": "4399550"
  },
  {
    "text": "And then the last application\nthat I'll mention here is few-shot motion prediction.",
    "start": "4399550",
    "end": "4405858"
  },
  {
    "text": "This could potentially be useful\nfor human robot interaction, where you want to predict\nthe motion of people, or in autonomous\ndriving where you",
    "start": "4405858",
    "end": "4412210"
  },
  {
    "text": "want to predict the\nmotion of other cars. And the task corresponds\nto different users and different\nunderlying motions,",
    "start": "4412210",
    "end": "4418960"
  },
  {
    "text": "basically different\ntrajectories. And the training data set\ncorresponded to the past K time steps of motion,\nand the test set",
    "start": "4418960",
    "end": "4425650"
  },
  {
    "text": "corresponded to the\nfuture seconds of motion.  They use kind of a hybrid of\nan optimization and a black box",
    "start": "4425650",
    "end": "4432820"
  },
  {
    "text": "approach here. So they had this learned\nupdate rule in the inner loop.",
    "start": "4432820",
    "end": "4438160"
  },
  {
    "text": "And they were able\nto, in this case, predict the motion of\nthese human skeletons",
    "start": "4438160",
    "end": "4445720"
  },
  {
    "text": "fairly accurately, and were able\nto do so much more effectively than a multi-task\nlearning approach",
    "start": "4445720",
    "end": "4451240"
  },
  {
    "text": "or a transfer learning approach.  Cool.",
    "start": "4451240",
    "end": "4456898"
  },
  {
    "text": "So those were just\nthree applications that I think survey the\nspectrum of problems",
    "start": "4456898",
    "end": "4464390"
  },
  {
    "text": "that you might look at in\nmeta learning examples. There's lastly, one\nclosing note that I'd",
    "start": "4464390",
    "end": "4470300"
  },
  {
    "text": "like to mention which\ngets at one of the things that we saw in the\nfirst application.",
    "start": "4470300",
    "end": "4476540"
  },
  {
    "text": "Which is that so far whenever\nwe sample a train set and a test set, we've been looking at\nexamples where we sample them",
    "start": "4476540",
    "end": "4484730"
  },
  {
    "text": "by ID from the\nsame distribution, and they don't actually need\nto be sampled independently from one another.",
    "start": "4484730",
    "end": "4490118"
  },
  {
    "text": " The inner loop training\ndata that you're learning from, it could\nhave noisy labels, it",
    "start": "4490118",
    "end": "4497150"
  },
  {
    "text": "could be weakly\nsupervised, and not actually have exactly\nthe label that you want.",
    "start": "4497150",
    "end": "4502855"
  },
  {
    "text": "It could have\ndomain shift that's different from what you see in\nthe outer loop test examples.",
    "start": "4502855",
    "end": "4510260"
  },
  {
    "text": "And this is cool, because it\nmeans that you can actually kind of metal learn for\nlearning procedures that",
    "start": "4510260",
    "end": "4517332"
  },
  {
    "text": "are more robust to noisy\nlabels, that can learn from weak supervision,\nthat can learn from other forms of supervision\nthan typical forms of machine",
    "start": "4517332",
    "end": "4524390"
  },
  {
    "text": "learning. And so in general, the\ninner and outer loop don't have to be the same.",
    "start": "4524390",
    "end": "4532130"
  },
  {
    "text": "On this note, it is really\nimportant for the test set to be a well formed\nmachine learning objective, because that's going to\ndrive the meta learning",
    "start": "4532130",
    "end": "4538618"
  },
  {
    "text": "process and optimize for\nthat learning procedure. But it's really\nthe inner loop that can be almost anything that\nyou want in some sense,",
    "start": "4538618",
    "end": "4547002"
  },
  {
    "text": "although you do need to\nhave enough supervision and enough information in\nthat inner loop for the model to be able to infer what\nit's supposed to be doing.",
    "start": "4547002",
    "end": "4554860"
  },
  {
    "text": "Yeah? Is this true for black box? Yeah, this is true\nfor black box too. So when you pass in your\nexamples, your x train and y",
    "start": "4554860",
    "end": "4563870"
  },
  {
    "text": "train, you could\nhave noisy examples, like noisy labels that\nare being passed in. Or you could have\nsupervision that's",
    "start": "4563870",
    "end": "4570080"
  },
  {
    "text": "a little bit different than a\none hot vector, for example. The reason I ask the question\nis, does the domain shift",
    "start": "4570080",
    "end": "4575810"
  },
  {
    "text": "mean the scaling of the shifting\nthat was done for the MAML? Yeah, so for domain\nshift, what I had in mind",
    "start": "4575810",
    "end": "4581960"
  },
  {
    "text": "there is, you want to learn\na classifier between-- maybe you want to learn\nan image classifier.",
    "start": "4581960",
    "end": "4588230"
  },
  {
    "text": "And you want to be able to\ntrain an image classifier with sketches of things. So that if you give it\nlike a sketch of a cat",
    "start": "4588230",
    "end": "4594350"
  },
  {
    "text": "and a sketch of a dog,\nit learns a good cat versus dog classifier. Then D train could be\nsketches, and D test",
    "start": "4594350",
    "end": "4599870"
  },
  {
    "text": "could be neutral images. And Black box methods\ncould work well for that, or optimization based\nmethods, or possibly things",
    "start": "4599870",
    "end": "4607130"
  },
  {
    "text": "like Prototypical Networks. I guess, in that\nexample I would guess",
    "start": "4607130",
    "end": "4612200"
  },
  {
    "text": "that in general, black\nbox methods could actually be a better choice in\nsome of those scenarios, because it is very expressive\nin the learning procedures",
    "start": "4612200",
    "end": "4619563"
  },
  {
    "text": "it can learn. ",
    "start": "4619563",
    "end": "4624685"
  },
  {
    "text": "But yeah, in\ngeneral, like expect kind of all the\nalgorithms that we've covered to be able to handle\nthese kinds of things.",
    "start": "4624685",
    "end": "4631140"
  },
  {
    "start": "4631140",
    "end": "4636720"
  },
  {
    "text": "Cool. So that's it for today. We talked about non-parametric\nfew-shot learning algorithms.",
    "start": "4636720",
    "end": "4642300"
  },
  {
    "text": "We compared the\ndifferent approaches. This is the last lecture on\nmeta learning algorithms. And next week we'll talk\nabout unsupervised pretraining",
    "start": "4642300",
    "end": "4649890"
  },
  {
    "text": "methods, and also a little\nbit into how these relate to meta learning algorithms, and\nhow you can view some of them",
    "start": "4649890",
    "end": "4654900"
  },
  {
    "text": "in a similar light. And then there's\nalso some reminders about coursework on the right.",
    "start": "4654900",
    "end": "4660320"
  },
  {
    "start": "4660320",
    "end": "4664000"
  }
]