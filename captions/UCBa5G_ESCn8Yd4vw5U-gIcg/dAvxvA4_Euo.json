[
  {
    "text": " We'll jump in,\nand we'll continue",
    "start": "0",
    "end": "6130"
  },
  {
    "text": "where we are last time. And weirdly, this picture\ntells you everything.",
    "start": "6130",
    "end": "11210"
  },
  {
    "text": "This gives you all the\ninsight you need to know. It's just weird and hilarious. Whoops. ",
    "start": "11210",
    "end": "18560"
  },
  {
    "text": "Let me fix this. ",
    "start": "18560",
    "end": "23640"
  },
  {
    "text": "Yeah. So here we're just\nminimizing a quadratic. We know exactly\nwhat the minimum is. It's just x equals 0.",
    "start": "23640",
    "end": "29970"
  },
  {
    "text": "So we just try something\nlike the steepest-- here we're using gradient descent.",
    "start": "29970",
    "end": "36239"
  },
  {
    "text": "It's just simple enough. You can actually work out\nwhat the iterations are. And what you'll find-- I mean, you have\na formula for it,",
    "start": "36240",
    "end": "42150"
  },
  {
    "text": "but the picture\nis basically this. ",
    "start": "42150",
    "end": "47212"
  },
  {
    "text": "These curves show\nyou the level sets. And what you can see\npretty clear-- you're beginning to see\nit already here,",
    "start": "47212",
    "end": "53430"
  },
  {
    "text": "is if the level sets\nare poorly conditioned, meaning they're anisotropic.",
    "start": "53430",
    "end": "59820"
  },
  {
    "text": "Isotropic means you're the\nsame in all directions. Anisotropic means\nyou're not the same. You're different in\ndifferent directions.",
    "start": "59820",
    "end": "66460"
  },
  {
    "text": "So if your curvature\nis different in different directions,\nor another way to say it, if your sublevel sets\nare not nearly spherical,",
    "start": "66460",
    "end": "74729"
  },
  {
    "text": "then gradient descent\nworks quite poorly. And you can see why. It does this weird zigzagging.",
    "start": "74730",
    "end": "80640"
  },
  {
    "text": "OK. And then the flip\nside of this is how well does gradient descent\ndo when the sublevel sets are",
    "start": "80640",
    "end": "88720"
  },
  {
    "text": "actually literally spherical\nor balls, Euclidean balls? Here the answer is\njust-- it's one step.",
    "start": "88720",
    "end": "95600"
  },
  {
    "text": "OK. So takeaway message is that\nthe efficiency or speed",
    "start": "95600",
    "end": "104710"
  },
  {
    "text": "of gradient descent\nis going to depend on how aspherical or anisotropic\nthe sublevel sets are.",
    "start": "104710",
    "end": "113080"
  },
  {
    "text": "And here is just an\nexample to show it works. I mean, it's an example\nin two dimensions. It's silly just to show that\nyou are getting zigzagging",
    "start": "113080",
    "end": "123340"
  },
  {
    "text": "or something like that. And I think we looked\nat this last time.",
    "start": "123340",
    "end": "129340"
  },
  {
    "text": "And now it comes to-- so the conclusion is,\ndepending on the problem,",
    "start": "129340",
    "end": "135340"
  },
  {
    "text": "if your sublevel sets are very\nwell conditioned, or spherical, or symmetric, or however you\nwant to say it, or isotropic,",
    "start": "135340",
    "end": "144069"
  },
  {
    "text": "or something like that, then\ngradient descent is fine.",
    "start": "144070",
    "end": "149260"
  },
  {
    "text": "So what this hints\nthough is that there's something strange about it. And it's actually a bit weird.",
    "start": "149260",
    "end": "155180"
  },
  {
    "text": "Because when you\nthink about it, why would you not use the\nthe negative gradient",
    "start": "155180",
    "end": "162850"
  },
  {
    "text": "as your descent direction? Just something seems\nvery weird about it because you arrive at\na point, and then you'd",
    "start": "162850",
    "end": "168550"
  },
  {
    "text": "say, hey, if I was to move a\nsmall amount in any direction, which would be the direction\nwhere my objective would",
    "start": "168550",
    "end": "174310"
  },
  {
    "text": "go down the fastest? And it seems like\nyou'd be an idiot to take anything other\nthan that direction.",
    "start": "174310",
    "end": "179900"
  },
  {
    "text": "And so we had said, well,\nit's the negative gradient. When you analyze that\nstatement further,",
    "start": "179900",
    "end": "185049"
  },
  {
    "text": "it turns out that depends\non the metric you're using.",
    "start": "185050",
    "end": "190430"
  },
  {
    "text": "So it actually depends\non the norm you use. And so the way you\nthink of that-- one way to do this in a\nprincipled way is to say, well,",
    "start": "190430",
    "end": "198439"
  },
  {
    "text": "the amount-- if you go in a\nsmall direction v, then the amount f\nis going to decrease",
    "start": "198440",
    "end": "206510"
  },
  {
    "text": "is about equal to the\ngradient of f transpose v. You want that negative.",
    "start": "206510",
    "end": "211640"
  },
  {
    "text": "So then you'd think,\nwell, OK, take v equals minus the gradient. That's an obvious choice.",
    "start": "211640",
    "end": "218340"
  },
  {
    "text": "You can't just say\nchoose v to minimize grad f of x transpose v. You\ncan't do that because it's",
    "start": "218340",
    "end": "225769"
  },
  {
    "text": "unbounded below. It's silly. And here's where\nthe metric comes in. You have to give a\nnorm to limit that.",
    "start": "225770",
    "end": "232580"
  },
  {
    "text": "You have to say, no, I\nwant the norm of v to be, let's say, equal to 1 or\nless than or equal to 1.",
    "start": "232580",
    "end": "238970"
  },
  {
    "text": "It would be the same thing. So you do that. And here-- now, notice\nthat this norm up here on v",
    "start": "238970",
    "end": "245570"
  },
  {
    "text": "is a general norm. It is not the\ntwo-norm necessarily, just a general norm. So this is referred to as the\nsteepest descent direction.",
    "start": "245570",
    "end": "255440"
  },
  {
    "text": "And it's going to\ndepend on the norm. And then you can unnormalize it. It's actually convenient\nto normalize it",
    "start": "255440",
    "end": "262100"
  },
  {
    "text": "by the dual norm\nof the gradient. And then you get something\nthat looks like that.",
    "start": "262100",
    "end": "269840"
  },
  {
    "text": "So the steepest descent\nmethod is basically-- and it's specified by a norm. You give a norm.",
    "start": "269840",
    "end": "275630"
  },
  {
    "text": "So once you give a norm, then\nthe steepest descent method is completely specified because\nthat's the direction you're",
    "start": "275630",
    "end": "283370"
  },
  {
    "text": "going to use. And you get that the convergence\nis similar to this descent.",
    "start": "283370",
    "end": "288770"
  },
  {
    "text": "But we'll look at some\nexamples to understand this. So the first one\nis if you just--",
    "start": "288770",
    "end": "294919"
  },
  {
    "text": "what is steepest descent\nin the Euclidean norm? It's precisely gradient descent.",
    "start": "294920",
    "end": "301650"
  },
  {
    "text": "Yeah? Why do you normalize it by\nthe dual norm of the gradient? You actually don't have to.",
    "start": "301650",
    "end": "307820"
  },
  {
    "text": "There's no reason\nto normalize it. It's just so that a\nlot of the equations get simpler if you do\nthe convergence analysis.",
    "start": "307820",
    "end": "314670"
  },
  {
    "text": "Yeah. So that's the only reason. Also that way, like for example,\nthe steepest descent direction",
    "start": "314670",
    "end": "320780"
  },
  {
    "text": "with the L2 norm is literally\nthe negative gradient. Otherwise, it would be the\nnegative gradient scaled.",
    "start": "320780",
    "end": "326880"
  },
  {
    "text": "It'd be a unit vector. It actually would be\nthe negative gradient divided by the norm of the\ngradient, so unit vector.",
    "start": "326880",
    "end": "335640"
  },
  {
    "text": "So if you use a quadratic norm-- so a quadratic norm is\nyou take a quadratic form with a positive definite matrix.",
    "start": "335640",
    "end": "342565"
  },
  {
    "text": "Take the square root. That's a quadratic norm. And here's an example of a--",
    "start": "342565",
    "end": "348275"
  },
  {
    "text": "I guess this is a unit ball\nin a quadratic norm, which is an ellipsoid. And then it says here's\nthe negative gradient.",
    "start": "348275",
    "end": "355870"
  },
  {
    "text": "And it says you should\nminimize the inner product of-- or sorry, you should-- yeah,\nyou should minimize the inner",
    "start": "355870",
    "end": "361530"
  },
  {
    "text": "product of a point in\nthis set with the--",
    "start": "361530",
    "end": "366750"
  },
  {
    "text": "I guess minimize it\nwith the gradient, which means maximize\nthe inner product with the negative gradient. So basically, it says these\nare the hyperplanes that",
    "start": "366750",
    "end": "378360"
  },
  {
    "text": "are going to be-- this is the outward normal. These are the level curves.",
    "start": "378360",
    "end": "384180"
  },
  {
    "text": "And it says you want to go\nas far as you can this way. And you end up here. And so this is, in fact, the--",
    "start": "384180",
    "end": "392083"
  },
  {
    "text": "this is going to be\nthe steepest descent direction in the norm\nwhere this is the unit",
    "start": "392083",
    "end": "397110"
  },
  {
    "text": "ball we're showing you, OK? Now, you can work things out\nlike you're never going to--",
    "start": "397110",
    "end": "402610"
  },
  {
    "text": "so actually, it does\nchange the direction. It's very weird. It changes the\ndirection, never rotates",
    "start": "402610",
    "end": "409000"
  },
  {
    "text": "it more than 90 degrees. It cannot, obviously, because\nthen you wouldn't have a--",
    "start": "409000",
    "end": "414670"
  },
  {
    "text": "you'd have a positive inner\nproduct with the gradient. So this is it. So weirdly-- so\nwhat the real answer",
    "start": "414670",
    "end": "422290"
  },
  {
    "text": "is it says that when you stop\nand you ask for directions, and you say what direction would\nbe the direction where you go",
    "start": "422290",
    "end": "428530"
  },
  {
    "text": "downhill the fastest, the\nperson should ask you, yeah, in what norm? Because what you select as\na norm is going to change",
    "start": "428530",
    "end": "436420"
  },
  {
    "text": "the directions they give you. And you can see that it\ndistorts the directions.",
    "start": "436420",
    "end": "444650"
  },
  {
    "text": "OK. It's actually super interesting. If you do the L1 norm,\nwhat you'll work out",
    "start": "444650",
    "end": "451820"
  },
  {
    "text": "is actually also a\nvery simple thing. If you take the inner\nproduct of a gradient",
    "start": "451820",
    "end": "458720"
  },
  {
    "text": "with a vector whose L1\nnorm is less than 1, you want to minimize that. What you will do is you will\nfind the entry, the axis",
    "start": "458720",
    "end": "467690"
  },
  {
    "text": "along with where the\npartial derivative or entry of the gradient\nis most negative.",
    "start": "467690",
    "end": "473750"
  },
  {
    "text": "And then you'll go-- sorry. You find the largest one\nand then go in, obviously, the decreasing direction.",
    "start": "473750",
    "end": "480634"
  },
  {
    "text": "And so that's actually\ngot a-- that's a named method from the '40s and '50s. And it basically--",
    "start": "480635",
    "end": "486650"
  },
  {
    "text": "I mean, that one has got\na very simple story to it. You stop where you\nare, and you say--",
    "start": "486650",
    "end": "492440"
  },
  {
    "text": "then you ask not which direction\nis the steepest direction to go down-- I mean, you can.",
    "start": "492440",
    "end": "497750"
  },
  {
    "text": "It's in the L1. But what you really ask is\nyou ask, if I go along axis 1,",
    "start": "497750",
    "end": "503130"
  },
  {
    "text": "how much am I going to go down? If I go along axis 2, how\nmuch is it going to go down? And you find the\none where you're",
    "start": "503130",
    "end": "508902"
  },
  {
    "text": "going to go down\nthe most, and then you simply go along the axis. So that's a so-called\naxis-aligned step,",
    "start": "508902",
    "end": "514020"
  },
  {
    "text": "people would call it. OK. And again, the picture\nshows how you actually",
    "start": "514020",
    "end": "519839"
  },
  {
    "text": "get a different direction. OK. Now, the choice of the metric\nor the norm for steepest descent",
    "start": "519840",
    "end": "528930"
  },
  {
    "text": "is going to make\na big difference. And here's just this\nbaby example, again,",
    "start": "528930",
    "end": "534149"
  },
  {
    "text": "where this is-- the first one\nshows you steepest descent with a norm whose unit\nball looks like this.",
    "start": "534150",
    "end": "539820"
  },
  {
    "text": "And I don't know. You just eyeball it. You can see some\nnumber of iterations. And here we take\nsteepest descent",
    "start": "539820",
    "end": "547950"
  },
  {
    "text": "in using a norm where the\nunit ball is skewed this way.",
    "start": "547950",
    "end": "553380"
  },
  {
    "text": "And what you see is\nthat has exacerbated the convergence horribly.",
    "start": "553380",
    "end": "558630"
  },
  {
    "text": "And so what this\ntells us is the-- it tells you the obvious thing.",
    "start": "558630",
    "end": "564780"
  },
  {
    "text": "It says that what\nyou really want is you want a norm, or\na metric, or something",
    "start": "564780",
    "end": "571320"
  },
  {
    "text": "like that that's aligned\nwith the sublevel sets of your function.",
    "start": "571320",
    "end": "579029"
  },
  {
    "text": "In other words, it's one which\nhas roughly the same shape. So this is just a very\nsimple example to show this.",
    "start": "579030",
    "end": "585130"
  },
  {
    "text": "I'm just pretending that\nthis function looks fatter than it does look tall.",
    "start": "585130",
    "end": "591940"
  },
  {
    "text": "And so over here this use of\nnorm, this selection of norm",
    "start": "591940",
    "end": "597730"
  },
  {
    "text": "actually improves\nthe convergence. This one exacerbates it, OK?",
    "start": "597730",
    "end": "602830"
  },
  {
    "text": "Does this make sense? These are just ideas just to--",
    "start": "602830",
    "end": "608199"
  },
  {
    "text": "I mean, but it's the essence\nof where we're going. Because now we go to the--",
    "start": "608200",
    "end": "616390"
  },
  {
    "text": "if you keep going\nwith this logic you end up at something\ncalled the Newton step.",
    "start": "616390",
    "end": "621440"
  },
  {
    "text": "So the Newton step,\nyou actually will use-- what you'll do is\nyou take the Hessian",
    "start": "621440",
    "end": "630520"
  },
  {
    "text": "of the second derivative of the\nfunction at the current point. And you then use\nthat as your metric.",
    "start": "630520",
    "end": "640360"
  },
  {
    "text": "And so the argument now\nmakes perfect sense. Because if I said-- if you say, OK, I accept the\nidea that I should choose,",
    "start": "640360",
    "end": "647170"
  },
  {
    "text": "let's say, a metric\nthat captures the shape of my function,\nthat's what I should do.",
    "start": "647170",
    "end": "653564"
  },
  {
    "text": "Actually, let's\nhave a discussion about smooth functions\nnear their minimum. Let's have that\ndiscussion just for fun. What do smooth functions look\nlike, smooth convex functions",
    "start": "653565",
    "end": "661000"
  },
  {
    "text": "look like near their minimum?  What do they look like?",
    "start": "661000",
    "end": "667709"
  },
  {
    "text": "They're just quadratic. And they're quadratic with-- what's the Hessian\nthere of the--",
    "start": "667710",
    "end": "673830"
  },
  {
    "text": "I mean, well that\nthat gives it away. That's the Hessian. The sublevel sets, if\nthis is a smooth convex",
    "start": "673830",
    "end": "680399"
  },
  {
    "text": "function, that's the minimizer. And then I say what do\nthe level sets look like? They look like this.",
    "start": "680400",
    "end": "685770"
  },
  {
    "text": "Now, these are not exactly\nellipsoids, I mean, unless the function\nis exactly quadratic. But this is what they look like.",
    "start": "685770",
    "end": "692850"
  },
  {
    "text": "I mean, eventually they start\nlooking not like ellipsoids. They start looking like that.",
    "start": "692850",
    "end": "698250"
  },
  {
    "text": "Because who knows? But down near the minimum,\nthey look really very much like ellipsoids.",
    "start": "698250",
    "end": "705600"
  },
  {
    "text": "And the shape of\nthe ellipsoid is determined by the Hessian of\nthe function at this point.",
    "start": "705600",
    "end": "711140"
  },
  {
    "text": "OK. By the way, if\nyou're in statistics or if you're not in statistics\nbut know some statistics,",
    "start": "711140",
    "end": "716959"
  },
  {
    "text": "you'd recognize\nthis immediately. If you look at the\nnegative log likelihood, that's going to be--",
    "start": "716960",
    "end": "724160"
  },
  {
    "text": "and this is in your\nparameter space. This point is going to be\nyour maximum likelihood or minimum negative\nlog likelihood point.",
    "start": "724160",
    "end": "732110"
  },
  {
    "text": "And then if I ask you, well,\ntell me about models that have a likelihood almost as high\nas my maximum likelihood--",
    "start": "732110",
    "end": "739910"
  },
  {
    "text": "everybody following this-- then basically I'm looking at-- I'm looking exactly\nat level curves",
    "start": "739910",
    "end": "745100"
  },
  {
    "text": "that are very close to\nthe maximum likelihood. I pulled down from the maximum\nlikelihood, and I get sets.",
    "start": "745100",
    "end": "751340"
  },
  {
    "text": "Now, these are\nparameters and models that fit the data you've seen.",
    "start": "751340",
    "end": "756830"
  },
  {
    "text": "What do they look like? Well, if the negative\nlog likelihood",
    "start": "756830",
    "end": "761870"
  },
  {
    "text": "is convex and smooth,\nthen they look-- they're",
    "start": "761870",
    "end": "767540"
  },
  {
    "text": "approximately ellipsoidal. And so actually--\nand the ellipsoid",
    "start": "767540",
    "end": "773210"
  },
  {
    "text": "is determined by the Hessian\nof the log likelihood at the maximum likelihood point,\nwhich is, of course, the Fisher",
    "start": "773210",
    "end": "778339"
  },
  {
    "text": "information. Everybody got that? And so this tells\nyou very good ways to get sets of model\nparameters which",
    "start": "778340",
    "end": "785990"
  },
  {
    "text": "give you almost the maximum\nlikelihood on the data you've seen. Again, if you know\nabout this, fine.",
    "start": "785990",
    "end": "792550"
  },
  {
    "text": "And if you don't, that's OK too. I'm just saying that\nthis idea is everywhere.",
    "start": "792550",
    "end": "797610"
  },
  {
    "text": "OK. So now we connect it back to\nthis conversation and said, here's what you should do.",
    "start": "797610",
    "end": "804010"
  },
  {
    "text": "You should use steepest\ndescent, but what you should do is you need to get\na metric that looks",
    "start": "804010",
    "end": "811710"
  },
  {
    "text": "the curvature of your function. So this would be a\nstunningly good method.",
    "start": "811710",
    "end": "819570"
  },
  {
    "text": "Ready for my method? It says do steepest descent. And someone says, yeah,\nwhich quadratic norm",
    "start": "819570",
    "end": "827170"
  },
  {
    "text": "are you going to use, which p? And I would say, oh, the\nHessian of your function at the optimum.",
    "start": "827170",
    "end": "832800"
  },
  {
    "text": "Everybody follow this? Boy, would that work well. Everybody got that? That would work\nunbelievably well.",
    "start": "832800",
    "end": "840930"
  },
  {
    "text": "Only minor problem is\nsomeone says, OK, well what is the Hessian at the optimum? And then the answer would be\nsomething like, well, first",
    "start": "840930",
    "end": "848040"
  },
  {
    "text": "minimize it, and\nthen I'll tell you the Hessian, which is kind\nof like this is stupid because that's what I\nwant to do in the first-- everybody follow this?",
    "start": "848040",
    "end": "854010"
  },
  {
    "text": "OK, so instead\nwe're simply going to use the Hessian\nat the current point",
    "start": "854010",
    "end": "859980"
  },
  {
    "text": "as the best approximation\nwe can make of the Hessian at the optimal point. Everybody following this?",
    "start": "859980",
    "end": "866950"
  },
  {
    "text": "That's one way to explain\nwhat Newton method is. ",
    "start": "866950",
    "end": "872320"
  },
  {
    "text": "And if we take the Newton\nstep and each time we redo it,",
    "start": "872320",
    "end": "877442"
  },
  {
    "text": "we're actually\nchanging the metric. And so sometimes people\nrefer to this-- this is a variable metric method.",
    "start": "877442",
    "end": "884440"
  },
  {
    "text": "Because you're actually--\nyou're changing the metric when you stop and ask for\ndirections and say, hey,",
    "start": "884440",
    "end": "890300"
  },
  {
    "text": "what's the fastest way\nto go down from here? And then someone on their toes\nsays, yeah, in what metric?",
    "start": "890300",
    "end": "898060"
  },
  {
    "text": "Every step you\ntake, you change it. OK.",
    "start": "898060",
    "end": "903360"
  },
  {
    "text": "I mean, these are\njust ideas or ways you can think about\nwhat the Newton step is. There are lots of other\nways to think of it too.",
    "start": "903360",
    "end": "910290"
  },
  {
    "text": "So here's another\ncompletely beautiful method.",
    "start": "910290",
    "end": "915300"
  },
  {
    "text": "It says let's have a\nsecond-order approximation of our function.",
    "start": "915300",
    "end": "921180"
  },
  {
    "text": "And that's, of course,\njust nothing but that. And you'd say, all right,\nlet's minimize that.",
    "start": "921180",
    "end": "927810"
  },
  {
    "text": "If you form a second-order\napproximation of a function at a point, minimize it,\nyou get the Newton step.",
    "start": "927810",
    "end": "933450"
  },
  {
    "text": "Everybody got that? So that also just\nmakes perfect sense.",
    "start": "933450",
    "end": "938670"
  },
  {
    "text": "Yeah. Another way is you\ncould think about the optimality conditions,\nand that looks like this.",
    "start": "938670",
    "end": "944320"
  },
  {
    "text": "So this would be here. Here's a picture\nof the first one. So here's the function f.",
    "start": "944320",
    "end": "949650"
  },
  {
    "text": "We're at x here. Now, what I do is I get the\nsecond-order approximation",
    "start": "949650",
    "end": "955830"
  },
  {
    "text": "here. This is f hat of f. By the way, the\nfirst-order approximation",
    "start": "955830",
    "end": "961180"
  },
  {
    "text": "has to be an\nunderestimator of f. The second order, we don't know. Could be above or below.",
    "start": "961180",
    "end": "966640"
  },
  {
    "text": "You don't know. OK, so here you are here.",
    "start": "966640",
    "end": "972910"
  },
  {
    "text": "Here you are here. We get a second order. And what it means to be a\nsecond-order approximation",
    "start": "972910",
    "end": "978190"
  },
  {
    "text": "is that f and f\nhat right near here are extremely close, which is\nmy way of saying close cubed.",
    "start": "978190",
    "end": "986500"
  },
  {
    "text": "It's small cubed,\nsomething like that.",
    "start": "986500",
    "end": "991600"
  },
  {
    "text": "They agree to second order here. So it's a super good\napproximation right here. OK, so we form that.",
    "start": "991600",
    "end": "997908"
  },
  {
    "text": "That's this dashed thing. And you can see over here\nit's on the wrong side. It's a lower bound up here.",
    "start": "997908",
    "end": "1002910"
  },
  {
    "text": "It's an upper bound. And the minimizer\nof this is this. And so this is the Newton\nstep, the difference between here and here.",
    "start": "1002910",
    "end": "1009120"
  },
  {
    "text": "OK. Another one is you linearize\nthe optimality condition. And you'd say, I'm at a point x.",
    "start": "1009120",
    "end": "1016560"
  },
  {
    "text": "I want a point where\nthe gradient of f is 0. And you go, cool, then\nwhat are you going to do?",
    "start": "1016560",
    "end": "1022123"
  },
  {
    "text": "And you say, well, I'm\ngoing to perturb x by-- I'm going to take a step v. But what you'd like is\nyou'd like the gradient",
    "start": "1022123",
    "end": "1028209"
  },
  {
    "text": "of f of x plus v to equal 0. But you don't know how\nto solve that in general.",
    "start": "1028210",
    "end": "1034780"
  },
  {
    "text": "There's one case where you do. We'll get to that in a minute. Instead you'd say,\nwell, you know what, I'll approximate\nthis using calculus.",
    "start": "1034780",
    "end": "1041151"
  },
  {
    "text": "So what this gradient is-- this is the first-order\nexpansion of the gradient.",
    "start": "1041151",
    "end": "1046829"
  },
  {
    "text": "The gradient here--\nthis is going to be the first-order thing. It's actually the following.",
    "start": "1046829",
    "end": "1052990"
  },
  {
    "text": "It's the gradient plus the\nsecond derivative times the displacement. And then what you\ndo is you say, OK, I",
    "start": "1052990",
    "end": "1059640"
  },
  {
    "text": "don't know what's actually\ngoing to happen to the gradient. But this I know.",
    "start": "1059640",
    "end": "1065530"
  },
  {
    "text": "This I get. I can compute these two. And I'll just solve\nthis equation. And that gives us,\nagain, the Newton step.",
    "start": "1065530",
    "end": "1071950"
  },
  {
    "text": "So that all make sense? I mean, I hate to give-- there are even\nmore explanations,",
    "start": "1071950",
    "end": "1078790"
  },
  {
    "text": "but these are just\ninterpretations of what it is. OK, so that's the Newton step.",
    "start": "1078790",
    "end": "1084820"
  },
  {
    "text": "Oh, and here's a picture\nof the second one in 1D. So actually, Newton would\nhave been quite familiar",
    "start": "1084820",
    "end": "1091150"
  },
  {
    "text": "with this picture. So here's the derivative. It's increasing because\nthe function is convex.",
    "start": "1091150",
    "end": "1099460"
  },
  {
    "text": "So here it is. And what we want to do is we\nwant to find the zero-crossing like this. So we want to find\nthat point there.",
    "start": "1099460",
    "end": "1105049"
  },
  {
    "text": "So how do you do that? Well, you're at a point\nhere, and you simply here take a linear,\nI guess you would",
    "start": "1105050",
    "end": "1112270"
  },
  {
    "text": "say, or affine approximation of\nthe derivative at that point. That's this dashed line here.",
    "start": "1112270",
    "end": "1118299"
  },
  {
    "text": "And then wherever that is\n0, that's our next point. By the way, if I were to do\nit again, what would happen?",
    "start": "1118300",
    "end": "1125750"
  },
  {
    "text": "So if I went to this point,\ncalculated the first-order",
    "start": "1125750",
    "end": "1131290"
  },
  {
    "text": "approximation, and looked at the\n0 of it, what would your next-- if I iterated the\nNewton step twice,",
    "start": "1131290",
    "end": "1137049"
  },
  {
    "text": "I think your eyeball is\ngoing to tell you what? ",
    "start": "1137050",
    "end": "1143220"
  },
  {
    "text": "Are you going to get\na good approximation? Yeah, good. And then if you do\nit a third time,",
    "start": "1143220",
    "end": "1149039"
  },
  {
    "text": "it's going to be\nreally, really good. Why? Because if I zoom in on a\nsmooth increasing function right",
    "start": "1149040",
    "end": "1155639"
  },
  {
    "text": "as it goes through 0, if I Zoom\nin it just looks like a line. That's all it looks like.",
    "start": "1155640",
    "end": "1161190"
  },
  {
    "text": "The same is true over here. If I take this point,\nfit a quadratic to it, and minimize it-- but suppose\nwhen I get near the bottom here",
    "start": "1161190",
    "end": "1169799"
  },
  {
    "text": "and I fit a quadratic to it,\nhow good an approximation is the quadratic nearby?",
    "start": "1169800",
    "end": "1177909"
  },
  {
    "text": "It's really good. And so all of this is giving you\nthis intuition that this method",
    "start": "1177910",
    "end": "1186400"
  },
  {
    "text": "should have some amazingly good\nterminal convergence, which",
    "start": "1186400",
    "end": "1191620"
  },
  {
    "text": "we'll get to in a minute, OK? And it's all true.",
    "start": "1191620",
    "end": "1196710"
  },
  {
    "text": "OK. And here's another\nvery cool way to do it.",
    "start": "1196710",
    "end": "1201970"
  },
  {
    "text": "It turns out it's the steepest\ndescent direction in the metric given by the Hessian.",
    "start": "1201970",
    "end": "1209670"
  },
  {
    "text": "And that makes total sense\nbecause, again, you land at x.",
    "start": "1209670",
    "end": "1215220"
  },
  {
    "text": "You ask someone, hey, what's the\nsteepest way down the mountain or whatever? And then they ask you, in\nwhat norm or what metric?",
    "start": "1215220",
    "end": "1223830"
  },
  {
    "text": "And you say, well,\ndo you happen to know the Hessian of the function\nat the minimizer down",
    "start": "1223830",
    "end": "1230733"
  },
  {
    "text": "at the bottom of the valley? And they'll say, no, I don't. I just live here or\nsomething like that.",
    "start": "1230733",
    "end": "1237550"
  },
  {
    "text": "Then you'd say, fine. Do the Hessian at\nthe current point? And they would say,\noh, that I do know.",
    "start": "1237550",
    "end": "1242590"
  },
  {
    "text": "OK. You go, fine, we'll use that. Everybody following this? And that would give you this.",
    "start": "1242590",
    "end": "1248850"
  },
  {
    "text": "So it's very cool,\nand you can actually plot pictures that shows\nwhat a Newton direction is",
    "start": "1248850",
    "end": "1254169"
  },
  {
    "text": "going to look like. And this is an example\nshowing that you're going to get a pretty\ngood Newton step, at least",
    "start": "1254170",
    "end": "1261159"
  },
  {
    "text": "for this thing. OK. So this is the Newton step. ",
    "start": "1261160",
    "end": "1268480"
  },
  {
    "text": "Then you have something cool\ncalled the Newton decrement. The Newton decrement, you\ncan think of it this way. ",
    "start": "1268480",
    "end": "1276940"
  },
  {
    "text": "A very good way to\nthink about it is this. Actually, it is the predicted\ndecrease when you take a Newton",
    "start": "1276940",
    "end": "1283120"
  },
  {
    "text": "step of the function,\na court predicted by your quadratic\nmodel of the function.",
    "start": "1283120",
    "end": "1289240"
  },
  {
    "text": "So that's what it is. Yeah? Can you define again what the\nmetric defined by the Hessian",
    "start": "1289240",
    "end": "1294690"
  },
  {
    "text": "means? Oh, yeah. It means I'm using a\nquadratic metric, which is going to be this.",
    "start": "1294690",
    "end": "1300539"
  },
  {
    "text": "When someone says,\nwhat's your norm? And I'd say, oh, that's easy.",
    "start": "1300540",
    "end": "1305700"
  },
  {
    "text": "I'll take the norm to be this. This is my current point. So here, this is the norm of z.",
    "start": "1305700",
    "end": "1310980"
  },
  {
    "text": " There you go.",
    "start": "1310980",
    "end": "1316330"
  },
  {
    "text": "That's it. Yeah. That's what it means. ",
    "start": "1316330",
    "end": "1322070"
  },
  {
    "text": "By the way, if you\nhave other ways to get a rough idea of the\nshape of your sublevel sets,",
    "start": "1322070",
    "end": "1328490"
  },
  {
    "text": "by all means use it. I mean, there's lots of other\nways you might be able-- there are plenty of cases where\nyou might have some other--",
    "start": "1328490",
    "end": "1335180"
  },
  {
    "text": "be able to get\nanother estimate which is not the Hessian or something\nlike that to get this. So OK.",
    "start": "1335180",
    "end": "1342140"
  },
  {
    "text": "All right. So one thing that's very\ncool about this metric",
    "start": "1342140",
    "end": "1348020"
  },
  {
    "text": "is that it turns out\nit's lots of things, but one thing that's\nvery important is it's actually\naffine invariant.",
    "start": "1348020",
    "end": "1355320"
  },
  {
    "text": "So that's actually going to\nbe a big, important theme. So if I change coordinates,\nif I apply an affine transform",
    "start": "1355320",
    "end": "1364669"
  },
  {
    "text": "to my problem and solve that-- by the way, if you do\nthat with gradient method,",
    "start": "1364670",
    "end": "1370730"
  },
  {
    "text": "that's actually completely\nequivalent to using steepest descent with a\ndifferent quadratic norm.",
    "start": "1370730",
    "end": "1376772"
  },
  {
    "text": "So you can even think\nof steepest descent as two steps, where the\nquadratic norm is two steps.",
    "start": "1376772",
    "end": "1382900"
  },
  {
    "text": "Step one, apply a matrix so\nthat your unit ball becomes just",
    "start": "1382900",
    "end": "1392910"
  },
  {
    "text": "a Euclidean ball. Another way to say it is-- actually, some people\nuse a great verb",
    "start": "1392910",
    "end": "1398490"
  },
  {
    "text": "for that, which is you round the\nproblem, which is a great term.",
    "start": "1398490",
    "end": "1403980"
  },
  {
    "text": "Because to round it means\nif your sublevel sets look like this and you apply\na coordinate transform,",
    "start": "1403980",
    "end": "1410310"
  },
  {
    "text": "and then you just want\nit to look isotropic. You want the curvature\nto be about the same",
    "start": "1410310",
    "end": "1415890"
  },
  {
    "text": "in all directions. And someone says, why? You'd say because\nwhen the curvature is",
    "start": "1415890",
    "end": "1421020"
  },
  {
    "text": "the same in all directions,\ngradient method is outstanding. Everybody got this?",
    "start": "1421020",
    "end": "1427050"
  },
  {
    "text": "So OK.  So here, if you\nchange coordinates",
    "start": "1427050",
    "end": "1434850"
  },
  {
    "text": "by a linear mapping,\nthis doesn't change. It's the same number.",
    "start": "1434850",
    "end": "1440830"
  },
  {
    "text": "So affine invariance is\ngoing to come up later. OK. So now we have Newton's method.",
    "start": "1440830",
    "end": "1447160"
  },
  {
    "text": "So I mean, it just\nuses the Newton step. So you compute the Newton\nstep and the gradient.",
    "start": "1447160",
    "end": "1452940"
  },
  {
    "text": "You quit if this\nso-called Newton decrement is small enough.",
    "start": "1452940",
    "end": "1459940"
  },
  {
    "text": "Then you use a backtracking line\nsearch, for example, whatever. And this entire algorithm\nis basically independent.",
    "start": "1459940",
    "end": "1470549"
  },
  {
    "text": "It commutes with\nchanges of coordinates. So in other words, if someone\nsays, instead of minimizing f,",
    "start": "1470550",
    "end": "1481680"
  },
  {
    "text": "I'm going to minimize\nthis f tilde, which multiplies its argument by\na non-singular matrix T.",
    "start": "1481680",
    "end": "1487410"
  },
  {
    "text": "In gradient method, you get\ncompletely different iterates. And all that might be\nfaster, might be way slower. I don't know.",
    "start": "1487410",
    "end": "1492929"
  },
  {
    "text": "It just depends. But you'll get a\ncompletely different thing. Actually, up here\nit all goes away.",
    "start": "1492930",
    "end": "1498570"
  },
  {
    "text": "And you can check\nbecause when you do this, the gradient gets\na T in front of it,",
    "start": "1498570",
    "end": "1503760"
  },
  {
    "text": "and then the Hessian gets a T\non the left and a T transpose on the right or\nsomething like that.",
    "start": "1503760",
    "end": "1509510"
  },
  {
    "text": "And when you\nassemble everything, it all just goes away. So you get the\nidentical steps here.",
    "start": "1509510",
    "end": "1515230"
  },
  {
    "text": "So another way to say\nthis, Newton's method is independent of\nchanges of coordinate, of any linear change\nof coordinates.",
    "start": "1515230",
    "end": "1523539"
  },
  {
    "text": " That's an extremely\ngood attribute to have.",
    "start": "1523540",
    "end": "1531070"
  },
  {
    "text": "Actually, in theory it's\ncool, but it's actually super useful in practice.",
    "start": "1531070",
    "end": "1536320"
  },
  {
    "text": "What it means is to\nfirst order, scaling doesn't matter for something\nlike a Newton method.",
    "start": "1536320",
    "end": "1541800"
  },
  {
    "text": "It doesn't make any\ndifference at all. I mean, if you were going to do\nthe linear algebra and infinite",
    "start": "1541800",
    "end": "1547700"
  },
  {
    "text": "precision, it literally wouldn't\nmatter at all, whereas that's completely false for something\nlike a gradient method.",
    "start": "1547700",
    "end": "1555140"
  },
  {
    "text": "There you change coordinates. And if you change\ncoordinates, you either",
    "start": "1555140",
    "end": "1561350"
  },
  {
    "text": "get something faster or,\nactually much more likely, slower. OK.",
    "start": "1561350",
    "end": "1566510"
  },
  {
    "text": "So this is Newton's method. So I'm going to give you the\nclassical convergence analysis.",
    "start": "1566510",
    "end": "1572630"
  },
  {
    "text": "So this is actually interesting. This is actually from the\nSoviet Union around the 1930s.",
    "start": "1572630",
    "end": "1578020"
  },
  {
    "text": "But here it is. It's actually super interesting. Oh, and actually\nbefore we start,",
    "start": "1578020",
    "end": "1586179"
  },
  {
    "text": "I have to ask you a\nbunch of questions. How well does Newton's method\nwork on a quadratic function?",
    "start": "1586180",
    "end": "1591985"
  },
  {
    "text": " Yeah, quite well.",
    "start": "1591985",
    "end": "1598409"
  },
  {
    "text": "Yeah, how well? It converges in one step. Why?",
    "start": "1598410",
    "end": "1603750"
  },
  {
    "text": "Because one way to understand\nwhat Newton's method is you say you're at x, and you\nsay, please develop for me",
    "start": "1603750",
    "end": "1609930"
  },
  {
    "text": "a quadratic approximation\nof your function. What's a quadratic approximation\nof a quadratic function? Itself.",
    "start": "1609930",
    "end": "1615640"
  },
  {
    "text": "And then the next step\nis it says minimize that. So actually, while you're\ncomputing the Newton step, you didn't know it,\nbut you're actually",
    "start": "1615640",
    "end": "1621665"
  },
  {
    "text": "solving the entire problem. Oh, and by the way-- yeah.",
    "start": "1621665",
    "end": "1627039"
  },
  {
    "text": "Yeah. Notice the other\nthing is that when I ask you to minimize a\nquadratic convex function,",
    "start": "1627040",
    "end": "1632920"
  },
  {
    "text": "what does that turn into? It's linear algebra. It's nothing else because the\nHessian is affine in the point.",
    "start": "1632920",
    "end": "1643010"
  },
  {
    "text": "So it's just linear algebra. And so therefore, that means\nI can pull in all the stuff we talked about last week.",
    "start": "1643010",
    "end": "1648799"
  },
  {
    "text": "And we will. We'll do that later. That's another thread. But we'll be talking about\nwhat does it mean for a Hessian",
    "start": "1648800",
    "end": "1654320"
  },
  {
    "text": "to be diagonal, or\nbanded, or sparse? And we'll talk about\nall of that later.",
    "start": "1654320",
    "end": "1661580"
  },
  {
    "text": "So what it says is--\nso I mean, one way to say it is computing a\nNewton step is basically",
    "start": "1661580",
    "end": "1668690"
  },
  {
    "text": "linear algebra. You know a fair amount\nof linear algebra by now. You know at least\nthe beginnings of it.",
    "start": "1668690",
    "end": "1675980"
  },
  {
    "text": "And it says it's\njust linear algebra. So we've reduced solving\na problem when you're doing Newton's method to\na sequence of problems",
    "start": "1675980",
    "end": "1682850"
  },
  {
    "text": "in each step you\nuse linear algebra. It's cool. OK.",
    "start": "1682850",
    "end": "1688117"
  },
  {
    "text": "Let's get back to-- OK, so we established\nthat Newton's method works shockingly well\nfor quadratic functions.",
    "start": "1688118",
    "end": "1694940"
  },
  {
    "text": "So what's really going to\naffect Newton's method actually",
    "start": "1694940",
    "end": "1700909"
  },
  {
    "text": "is how well-- remember\nhow this all went. It all went this way. You wanted to use\nthe metric which was",
    "start": "1700910",
    "end": "1707270"
  },
  {
    "text": "the Hessian at the solution. Instead you use the\nHessian where you are.",
    "start": "1707270",
    "end": "1715450"
  },
  {
    "text": "And by the way,\nthat also explains why this is going\nto do really well as you get closer and closer\nbecause that approximation is",
    "start": "1715450",
    "end": "1721289"
  },
  {
    "text": "going to get better and better. OK. So when you're here, when\nyou're right close to,",
    "start": "1721290",
    "end": "1729125"
  },
  {
    "text": "let's say, if this is a\nmaximum likelihood problem, if you're right close\nto the parameters, then the Newton method\nis going to give you",
    "start": "1729125",
    "end": "1735870"
  },
  {
    "text": "essentially an extremely good\nestimate of the parameter on the next step.",
    "start": "1735870",
    "end": "1742230"
  },
  {
    "text": "Everybody got this? OK. All right. So what it depends on is\nactually the third derivative,",
    "start": "1742230",
    "end": "1754049"
  },
  {
    "text": "if you think about it carefully. It's the third\nderivative that matters.",
    "start": "1754050",
    "end": "1759600"
  },
  {
    "text": "Because the third\nderivative tells you how much the second derivative\nchanges when you move.",
    "start": "1759600",
    "end": "1765460"
  },
  {
    "text": "That's what the\nthird derivative is. So if the third derivative\nof a function is small,",
    "start": "1765460",
    "end": "1770889"
  },
  {
    "text": "then our intuition is\nthat Newton's method should work unbelievably well. And someone would say, why? And you'd say,\nwell, because it's",
    "start": "1770890",
    "end": "1777943"
  },
  {
    "text": "getting close to what\nyou really wanted to do, which was to change\ncoordinates or how",
    "start": "1777943",
    "end": "1783540"
  },
  {
    "text": "you use a metric\nwhich was aligned with the Hessian defined by\nthe Hessian at the optimum. And you'd say, but you're\nnot at the optimum.",
    "start": "1783540",
    "end": "1789585"
  },
  {
    "text": "Yeah, but the Hessian\nchanges very slowly. So I mean, there's\nnot more to what I'm saying than this is not deep.",
    "start": "1789585",
    "end": "1796230"
  },
  {
    "text": "Everybody got this? Here's an extreme case of a\nthird derivative being small. It's 0.",
    "start": "1796230",
    "end": "1802700"
  },
  {
    "text": "What does it mean if a\nthird derivative is 0? It's quadratic. So all of us should be expecting\nit's a third derivative.",
    "start": "1802700",
    "end": "1810799"
  },
  {
    "text": "Now, the third\nderivative of a function on r is something none of\nus should be afraid of. Third derivative of a\nfunction on rn is something--",
    "start": "1810800",
    "end": "1821283"
  },
  {
    "text": "this has to do\nwith weirdness, has to do with our education in\nmathma-- we should all avoid.",
    "start": "1821283",
    "end": "1826659"
  },
  {
    "text": "It's a trilinear form. So if you're cool with tensors--",
    "start": "1826660",
    "end": "1832497"
  },
  {
    "text": "people in mechanical engineering\ncan do this very well. A lot of people in\nphysics can do it because they're used to working\nwith higher-order tensors.",
    "start": "1832498",
    "end": "1838407"
  },
  {
    "text": "They don't have any friends. And they're like,\nthey don't scare me. What do you mean? And I'm like, there's\nthree indices.",
    "start": "1838407",
    "end": "1844510"
  },
  {
    "text": "And they're like, yeah, and so? Anyway, I'm just\nsaying, unless you have it come from\nsome weird fields",
    "start": "1844510",
    "end": "1852460"
  },
  {
    "text": "where you do handle\nthese kinds of things, then it's complicated.",
    "start": "1852460",
    "end": "1857560"
  },
  {
    "text": "For the rest of us who have this\nweird education where we were trained about matrices--\nthat would include me--",
    "start": "1857560",
    "end": "1864610"
  },
  {
    "text": "then we get very uncomfortable\nwhen we go above two indices. Everybody following this?",
    "start": "1864610",
    "end": "1870220"
  },
  {
    "text": "So if you're in that-- are\nyou in one of these fields? Yeah. Which one?",
    "start": "1870220",
    "end": "1875410"
  },
  {
    "text": "Physics. There you go, physics. See? Yeah, they laugh at\nus, and they say, well, that's contravariant. That's covariant.",
    "start": "1875410",
    "end": "1880590"
  },
  {
    "text": "I don't care how many of\nthese Einstein-- it's fine. It's totally fine. It's fine. So wouldn't bother you to take--",
    "start": "1880590",
    "end": "1886980"
  },
  {
    "text": "yeah, how about this? How about the third derivative\nof the log determinant of a positive definite matrix?",
    "start": "1886980",
    "end": "1895040"
  },
  {
    "text": "You can pretend to visualize it. Yeah. OK, good.",
    "start": "1895040",
    "end": "1900110"
  },
  {
    "text": "OK. So yeah. I mean, that's some\ncrazy trilinear form that's got six indices\nbecause the original thing is",
    "start": "1900110",
    "end": "1906759"
  },
  {
    "text": "indexed by two. Anyway, fine. All right. All right, so actually it turns\nout all we need to express",
    "start": "1906760",
    "end": "1913547"
  },
  {
    "text": "is that the third\nderivative is small. So we're not going to actually\nwork with third derivatives",
    "start": "1913547",
    "end": "1918790"
  },
  {
    "text": "and trilinear forms. And we're certainly not going to\ntalk about the third derivative of, let's say, the\nnegative log determinant",
    "start": "1918790",
    "end": "1925480"
  },
  {
    "text": "of a positive definite\nmatrix, which, I mean, I could write it out. I can write out all the\npartial derivatives.",
    "start": "1925480",
    "end": "1930879"
  },
  {
    "text": "I know what they are. I just don't want to write it\ndown because of my training. So that's fine. OK.",
    "start": "1930880",
    "end": "1936620"
  },
  {
    "text": "OK. So instead this\nis often expressed as a Lipschitz constant. So this is cool.",
    "start": "1936620",
    "end": "1942970"
  },
  {
    "text": "This we can do. If I take the two-norm of the\ndifference of the two Hessians",
    "start": "1942970",
    "end": "1949779"
  },
  {
    "text": "and I say that\nthat's less than L, so it has a Lipschitz\nconstant of L, then that's basically\nthe same as saying",
    "start": "1949780",
    "end": "1957730"
  },
  {
    "text": "that the norm of\nthis trilinear form, which is the third\nderivative, is less than L.",
    "start": "1957730",
    "end": "1965290"
  },
  {
    "text": "And this avoids us having to\nget a headache because there's three indices and all\nthat kind of stuff.",
    "start": "1965290",
    "end": "1970620"
  },
  {
    "text": "Everybody following this? And besides, this\nis what you want. So you should imagine that\nif that condition holds and L",
    "start": "1970620",
    "end": "1978139"
  },
  {
    "text": "is small, Newton's method\nshould work really well. Because it says--\nactually, that literally",
    "start": "1978140",
    "end": "1984530"
  },
  {
    "text": "says the Hessian doesn't\nchange much as you move. And that's the key to Newton's\nmethod working really well.",
    "start": "1984530",
    "end": "1991970"
  },
  {
    "text": "Everybody following this? OK. So here you have some\nLipschitz constant there.",
    "start": "1991970",
    "end": "1998930"
  },
  {
    "text": "We'll assume that. And then the way-- I won't go through the--",
    "start": "1998930",
    "end": "2007235"
  },
  {
    "text": "I mean, if you read\nthe book-- it's not even that long, but it's\nnot the kind of thing, I think, that should\nbe done in public,",
    "start": "2007235",
    "end": "2012550"
  },
  {
    "text": "is to walk through a proof\nof this kind of stuff because I don't get it. The main point is-- the outline\nis actually pretty cool.",
    "start": "2012550",
    "end": "2019240"
  },
  {
    "text": "It basically says this. If the norm of the\ngradient-- there's a number. And it says if the\nnorm of the gradient",
    "start": "2019240",
    "end": "2025150"
  },
  {
    "text": "is bigger than that\nnumber, then it guarantees, it absolutely guarantees a\nfixed decrease in the function.",
    "start": "2025150",
    "end": "2032370"
  },
  {
    "text": "That's cool. Once the gradient gets\nless than that number,",
    "start": "2032370",
    "end": "2037530"
  },
  {
    "text": "then you get something\ncrazy, which is that-- I mean, ignore the\nconstants, but basically",
    "start": "2037530",
    "end": "2044480"
  },
  {
    "text": "the norm of the\ngradient, which is how close are you to optimal,\nthat is less than or equal to--",
    "start": "2044480",
    "end": "2052080"
  },
  {
    "text": "it actually goes like\nthe square at most. Now, to visualize what\nhappens, once the error",
    "start": "2052080",
    "end": "2060059"
  },
  {
    "text": "is 0.1, on the next\nstep it's 0.01. On the next step, it's 0.001.",
    "start": "2060060",
    "end": "2067060"
  },
  {
    "text": "Wait, did I do that right? No, 0.0001. Next time, it's 1e minus 8.",
    "start": "2067060",
    "end": "2072960"
  },
  {
    "text": "And on the next step,\nit's one 1e minus 16. And now we're at floating\npoint precision, and we stop. Everybody follow this?",
    "start": "2072960",
    "end": "2080429"
  },
  {
    "text": "And if you remember\nlinear convergence from the gradient\nmethod, linear-- I mean, it's a weird name.",
    "start": "2080429",
    "end": "2085860"
  },
  {
    "text": "Linear was on a log\naccuracy vertical axis and number of iterations.",
    "start": "2085860",
    "end": "2091379"
  },
  {
    "text": "What that says is you get a\nconstant improvement each step. And the improvement is\nusually rather pathetic.",
    "start": "2091380",
    "end": "2099310"
  },
  {
    "text": "You multiply your error by 0.99. You have to multiply things\nby 0.99 for a long time",
    "start": "2099310",
    "end": "2105940"
  },
  {
    "text": "until you get 1e minus 6. Everybody following this? This accelerates.",
    "start": "2105940",
    "end": "2111849"
  },
  {
    "text": "The accuracy doubles every step\nonce you get into this region.",
    "start": "2111850",
    "end": "2116860"
  },
  {
    "text": "And this is going to be\ncalled quadratic convergence because on a semi-log plot\nit's going to be a quadratic.",
    "start": "2116860",
    "end": "2124900"
  },
  {
    "text": "OK. And it's going to be the\nway to understand that this accelerates and gets--",
    "start": "2124900",
    "end": "2130547"
  },
  {
    "text": "it's the thing we observed\njust by looking at silly baby examples that once you're\nclose to the optimum,",
    "start": "2130547",
    "end": "2136029"
  },
  {
    "text": "Newton's method is going\nto work unbelievably well. So OK.",
    "start": "2136030",
    "end": "2141915"
  },
  {
    "text": "All right.  So the first part is called\nthe damped Newton phase,",
    "start": "2141915",
    "end": "2148730"
  },
  {
    "text": "and the second is called the\nquadratically convergent phase. And in these cases,\nmost of the iterations",
    "start": "2148730",
    "end": "2156740"
  },
  {
    "text": "require backtracking or damping. Yeah? So I have a probably\nnaive question.",
    "start": "2156740",
    "end": "2164559"
  },
  {
    "text": "Can we understand that if the\nproperty of the function f is closer to the\nquadratic function,",
    "start": "2164560",
    "end": "2172040"
  },
  {
    "text": "then the better the\nconvergence result?",
    "start": "2172040",
    "end": "2178300"
  },
  {
    "text": "That is a way to say that. We expect here\nthat if L is small,",
    "start": "2178300",
    "end": "2184270"
  },
  {
    "text": "then that's one way\nof saying the Hessian doesn't change as you\nmove around that much. That's what it says.",
    "start": "2184270",
    "end": "2189820"
  },
  {
    "text": "Yeah. Then I would say that function\nis closer to quadratic than if L was really big.",
    "start": "2189820",
    "end": "2195050"
  },
  {
    "text": "And so yes, we should\nexpect everything to work. When L gets small, things\nshould work pretty well.",
    "start": "2195050",
    "end": "2201910"
  },
  {
    "text": "So yeah. I mean, it's\nactually pretty cool. If L gets small, this\neta gets pretty big.",
    "start": "2201910",
    "end": "2210010"
  },
  {
    "text": "And that means that\nyou get into the region of quadratic convergence faster.",
    "start": "2210010",
    "end": "2215820"
  },
  {
    "text": "So indeed, it does go that way. But yeah, that's a\nperfectly good-- how far is the function from quadratic?",
    "start": "2215820",
    "end": "2223480"
  },
  {
    "text": "Because someone says, why? You say, well, for\nquadratic function it's game over in one step. And then this is a measure of\nhow far you are from quadratic.",
    "start": "2223480",
    "end": "2232490"
  },
  {
    "text": "OK. All right. So this first bit, it says since\nyou're guaranteed in this case",
    "start": "2232490",
    "end": "2243520"
  },
  {
    "text": "that you're going to go\ndown by this gamma thing, then f of x0 minus p\nstar divided by eta",
    "start": "2243520",
    "end": "2249340"
  },
  {
    "text": "is the maximum\nnumber of steps you could be in the\ndamped Newton step. Once you're here, you get\nsomething that looks like this.",
    "start": "2249340",
    "end": "2257240"
  },
  {
    "text": "And I'm just going to point\nout this is 1/2 to the 2 to the L minus k. So what happens is it\njust keeps going down.",
    "start": "2257240",
    "end": "2265210"
  },
  {
    "text": "It accelerates. It's this thing where you\nget four digits of accuracy, then eight, then 16.",
    "start": "2265210",
    "end": "2270940"
  },
  {
    "text": "As a practical matter, you\ndon't do more than three or four steps in\nquadratically convergent phase",
    "start": "2270940",
    "end": "2275950"
  },
  {
    "text": "because then you just hit\nmachine precision and stop.  And I'll tell you a story\nabout that a little bit later.",
    "start": "2275950",
    "end": "2283040"
  },
  {
    "text": "OK, so this is the idea\nbehind the analysis here.",
    "start": "2283040",
    "end": "2289145"
  },
  {
    "text": " And the conclusion is\nthe number of iterations",
    "start": "2289145",
    "end": "2296110"
  },
  {
    "text": "until you get something that's\nlike epsilon suboptimal is no more than the\ncurrent function",
    "start": "2296110",
    "end": "2301810"
  },
  {
    "text": "value minus the\noptimal value divided by this number gamma,\nwhich we could just calculate in some number.",
    "start": "2301810",
    "end": "2307930"
  },
  {
    "text": "And then the quadratic\nthing, since you're doubling the number of basically\ndigits of accuracy each step,",
    "start": "2307930",
    "end": "2314350"
  },
  {
    "text": "it's going to be\nproportional to log. It's actually going to be\nlog log 1 over epsilon.",
    "start": "2314350",
    "end": "2320109"
  },
  {
    "text": "And you should read that\nin a reasonable way. There's a very, very\ngood way to think-- bisection is like\nlog 1 over epsilon",
    "start": "2320110",
    "end": "2328029"
  },
  {
    "text": "because you divide your-- and then even something\nthat multiplies your error",
    "start": "2328030",
    "end": "2333400"
  },
  {
    "text": "by 0.99, that's also like log 1\nover epsilon because it just-- I mean, it's got a\ndifferent constant in front,",
    "start": "2333400",
    "end": "2339070"
  },
  {
    "text": "but that's how long it takes. You get a constant fraction\nimprovement each step.",
    "start": "2339070",
    "end": "2344650"
  },
  {
    "text": "The log log is because\nwhen you get close, you start accelerating, and\nyou get faster and faster. OK.",
    "start": "2344650",
    "end": "2350980"
  },
  {
    "text": "OK, so this is the-- and\nthat second term there, I think a normal person, you\njust read that second term",
    "start": "2350980",
    "end": "2356950"
  },
  {
    "text": "as 3 or 5. 5 is generous.",
    "start": "2356950",
    "end": "2362589"
  },
  {
    "text": "It's silly because\nit's like once you're in the region of quadratic\nconvergence, it's three steps",
    "start": "2362590",
    "end": "2368829"
  },
  {
    "text": "and you're at double\nprecision stationary. OK. So OK.",
    "start": "2368830",
    "end": "2374080"
  },
  {
    "text": "Now, there's a few\nproblems with this. And this is the\nclassical analysis.",
    "start": "2374080",
    "end": "2380140"
  },
  {
    "text": "OK. There's a few problems\nwith this analysis. Let's start with one.",
    "start": "2380140",
    "end": "2386200"
  },
  {
    "text": "Let's see, gamma and epsilon 0. And they depend on m, L,\nand the initial point.",
    "start": "2386200",
    "end": "2391839"
  },
  {
    "text": "Now, do you imagine in\nany practical problem you would ever know\nany of these things?",
    "start": "2391840",
    "end": "2397420"
  },
  {
    "text": "The answer is no. Are there cases where you do? Yes. If I'm doing logistic\nregression or something",
    "start": "2397420",
    "end": "2404230"
  },
  {
    "text": "like that with some\nL2 regularization, I do have some values\non some of these things.",
    "start": "2404230",
    "end": "2409869"
  },
  {
    "text": "So that's the first thing. OK. And the other minor\npoint is p star",
    "start": "2409870",
    "end": "2418430"
  },
  {
    "text": "appears in my\ncomplexity analysis. So you're sitting\nthere, and you say, I'm going to minimize this\nfunction by Newton's method.",
    "start": "2418430",
    "end": "2424490"
  },
  {
    "text": "You say, but I'd like to\nknow how many steps it's going to take. Or actually, what I really\nneed to know is an upper bound. And the person\nsays, yeah, awesome.",
    "start": "2424490",
    "end": "2430820"
  },
  {
    "text": "It's that, OK? It's f of x0 minus\np star divided by gamma plus log log epsilon. And then you'd say,\nOK, what's gamma?",
    "start": "2430820",
    "end": "2439500"
  },
  {
    "text": "No one. You're not going to know gamma. You're not going to know that. And then you say, what's p star? And you go, oh, that's the\noptimal value of the problem.",
    "start": "2439500",
    "end": "2447050"
  },
  {
    "text": "You're like, wait a minute. So anyway. So then what this suggests\nis to actually use this--",
    "start": "2447050",
    "end": "2454520"
  },
  {
    "text": "I mean, I'm making fun of it-- the answer is you should\nfirst use Newton's method to minimize the function,\nwhereupon you know p star.",
    "start": "2454520",
    "end": "2462080"
  },
  {
    "text": "Now you can actually go back\nand evaluate this and go back in a time machine and tell\nyourself an upper bound",
    "start": "2462080",
    "end": "2467780"
  },
  {
    "text": "on the number of-- it's completely ridiculous. So when you make fun of it-- by the way, people taught\nthis like a crowning--",
    "start": "2467780",
    "end": "2475279"
  },
  {
    "text": "after two lectures of horrible\ncalculations you get to this, and it's supposed to be\nthe crowning achievement. Everybody following this?",
    "start": "2475280",
    "end": "2481220"
  },
  {
    "text": "So honestly to me, many\nclassical Western optimization",
    "start": "2481220",
    "end": "2487410"
  },
  {
    "text": "convergence results\nsound like this. Here's what they all\nsound like to me. It says, if-- and there's a\nlong hypothesis filled with all",
    "start": "2487410",
    "end": "2495660"
  },
  {
    "text": "sorts of stuff and parameters\nthat you will never know under any circum-- there's no way.",
    "start": "2495660",
    "end": "2501030"
  },
  {
    "text": "If this is Lipschitz,\nand there's L, and there's m, and strong-- blah, blah, blah, blah, blah-- ",
    "start": "2501030",
    "end": "2507720"
  },
  {
    "text": "then it will give you a bound\non the number of steps for that algorithm, which, by the way,\nif you actually evaluated it",
    "start": "2507720",
    "end": "2514710"
  },
  {
    "text": "with real numbers would be\nnumbers like 10 million or 10 billion. It's completely useless. And you go, yeah, thanks.",
    "start": "2514710",
    "end": "2521160"
  },
  {
    "text": "Actually, my observation\nis that this converges in around 20 steps always. So thanks for the upper\nbound of 150 million.",
    "start": "2521160",
    "end": "2528890"
  },
  {
    "text": "Thank you. Anyway, it's just so it's-- they\nall sound like to me if a bunch",
    "start": "2528890",
    "end": "2534000"
  },
  {
    "text": "of complete, utterly\nunknowable hypotheses hold, then a completely useless\nconclusion will hold.",
    "start": "2534000",
    "end": "2541950"
  },
  {
    "text": "That's how I hear basically\nWestern convergence analysis. And then when I confront\npeople and say this,",
    "start": "2541950",
    "end": "2549515"
  },
  {
    "text": "they get very\nangry, as you might imagine if that's all they\ndid their whole life was develop these.",
    "start": "2549515",
    "end": "2554760"
  },
  {
    "text": "And then they go,\nlook, you don't get it. I didn't want the number. You say, this is conceptual.",
    "start": "2554760",
    "end": "2561030"
  },
  {
    "text": "God, this guy is so dumb. The whole point of this-- this is conceptual. This says if you\nknew these numbers,",
    "start": "2561030",
    "end": "2568290"
  },
  {
    "text": "then I can prove it'll\ntake less than 150 million. Anyway, I don't buy it.",
    "start": "2568290",
    "end": "2573330"
  },
  {
    "text": "I mean, it's fine. It's better than knowing\nit's not going to work. It's better than some fields\nwhere you say, I don't know.",
    "start": "2573330",
    "end": "2580140"
  },
  {
    "text": "Try it. Although might have a little bit\nmore respect for some of those fields anyway than the others.",
    "start": "2580140",
    "end": "2585610"
  },
  {
    "text": "Anyway. OK. Everybody following this? OK. Yes? If you can compute the\nHessian of your objective,",
    "start": "2585610",
    "end": "2592859"
  },
  {
    "text": "can you still adapt this method? Perhaps you could iteratively-- Absolutely. OK, so yeah.",
    "start": "2592860",
    "end": "2598650"
  },
  {
    "text": "So if you can't compute\nthe Hessian at the point, what can--",
    "start": "2598650",
    "end": "2604500"
  },
  {
    "text": "OK. Yeah, there's a lot of other--\nthere's whole fields you can take, entire courses on that.",
    "start": "2604500",
    "end": "2611200"
  },
  {
    "text": "So there are quasi-Newton\nmethods where you approximate. You build up an approximation\nof the Hessian using only,",
    "start": "2611200",
    "end": "2618150"
  },
  {
    "text": "let's say, gradient information. That's one. There's a ton of\nmethods like that.",
    "start": "2618150",
    "end": "2623358"
  },
  {
    "text": "It's a whole field. Actually, some of those\nare really useful. So yeah, it's a good question.",
    "start": "2623358",
    "end": "2628870"
  },
  {
    "text": "There's also a lot\nof other variations. Here's one that\nactually is useful.",
    "start": "2628870",
    "end": "2634020"
  },
  {
    "text": "You evaluate the Hessian\nonly every 10 steps. There you go. 10 was the parameter,\nbut whatever.",
    "start": "2634020",
    "end": "2640890"
  },
  {
    "text": "Then that actually makes perfect\nsense, works unbelievably well.",
    "start": "2640890",
    "end": "2646026"
  },
  {
    "text": "Now that you know\nsome linear algebra, you know the following. You compute the\nHessian, factor it,",
    "start": "2646027",
    "end": "2651059"
  },
  {
    "text": "and then when you're\ncomputing Newton's steps it's just a back solve,\nand the cost is really low. Everybody following?",
    "start": "2651060",
    "end": "2656795"
  },
  {
    "text": "So there are a\nlot of variations, but, yes, you're\nabsolutely right. There's that, yeah. And actually, some methods now\nusing Autograd and autodiff",
    "start": "2656795",
    "end": "2667710"
  },
  {
    "text": "things, you just call dot\nbackward, dot backward. OK. And there are\nmethods that do that.",
    "start": "2667710",
    "end": "2674680"
  },
  {
    "text": "So that's even more radical. That's like computer Hessian. Why would you do that?",
    "start": "2674680",
    "end": "2682135"
  },
  {
    "text": "I know how to do it with\nautodifferentiation. So there's a lot of methods\nthat would be based on that.",
    "start": "2682135",
    "end": "2687610"
  },
  {
    "text": "You had a question? I'm just curious-- where do\nthe optimization methods come from that we do respect or use?",
    "start": "2687610",
    "end": "2696500"
  },
  {
    "text": "Oh. If all the ones are jokes,\nthen what are we using? Oh, no. Sorry. The method is not a joke.",
    "start": "2696500",
    "end": "2702730"
  },
  {
    "text": "Oh, boy. No, no, no. The analysis is a joke. To me, it's funny anyway. Let's put it that way.",
    "start": "2702730",
    "end": "2708525"
  },
  {
    "text": "I'm not making\nfun of the method. Oh, no. Oh, boy. This works really\nwell in practice.",
    "start": "2708525",
    "end": "2713900"
  },
  {
    "text": "So the convergence analysis. That's a joke to me personally. That's a subjective thing.",
    "start": "2713900",
    "end": "2719260"
  },
  {
    "text": "I mean, not to people\nwho this is all they do. But to me it's a little\nbit of a joke, yeah.",
    "start": "2719260",
    "end": "2726640"
  },
  {
    "text": "But the method is not. Oh, no, no. Never confuse the method. Yeah, these are\ndifferent things.",
    "start": "2726640",
    "end": "2731720"
  },
  {
    "text": "OK. OK. Now, there's something\neven worse here, which is an aesthetic. So I've made fun of this enough,\nbut I'm going to actually--",
    "start": "2731720",
    "end": "2739400"
  },
  {
    "text": "I'm going to make\nit even worse now. So here it is. The actual Newton's method\nis affine invariant,",
    "start": "2739400",
    "end": "2748140"
  },
  {
    "text": "which is an amazing thing. The entire algorithm, if you\nscale change coordinates and it",
    "start": "2748140",
    "end": "2755090"
  },
  {
    "text": "takes 27 iterations,\nyou change coordinates, it takes 27 iterations, which\nis an incredibly beautiful",
    "start": "2755090",
    "end": "2762290"
  },
  {
    "text": "property. It says basically that it is not\nsensitive in any way whatsoever",
    "start": "2762290",
    "end": "2769430"
  },
  {
    "text": "to bad problem scaling and\nall sorts of other stuff. Everybody following this? Beautiful, super useful in\npractice attribute to have.",
    "start": "2769430",
    "end": "2776839"
  },
  {
    "text": "OK. OK, now you say, OK, this\nis a beautiful algorithm. Could you please\nanalyze it for me?",
    "start": "2776840",
    "end": "2782780"
  },
  {
    "text": "And they come back. And they say, yeah, no problem. You need to tell me L,\nand you need to tell me m.",
    "start": "2782780",
    "end": "2792093"
  },
  {
    "text": "Yeah, that's it. OK. Everybody following this? And you'd say, excuse me,\nif I change coordinates,",
    "start": "2792093",
    "end": "2798789"
  },
  {
    "text": "do the values of L and m change? And the answer is yes.",
    "start": "2798790",
    "end": "2804000"
  },
  {
    "text": "And now you have, aesthetically,\na deeply embarrassing situation.",
    "start": "2804000",
    "end": "2809490"
  },
  {
    "text": "Because somebody\nwho just wrote up the code in Rust\nor C or something has this incredibly beautiful\nthing, and the actual code",
    "start": "2809490",
    "end": "2818849"
  },
  {
    "text": "itself is now more elegant\nthan the mathematical proof.",
    "start": "2818850",
    "end": "2824160"
  },
  {
    "text": "Because the actual\ncode, it actually is invariant up to\nnumerical issues.",
    "start": "2824160",
    "end": "2830310"
  },
  {
    "text": "It's actually invariant\nunder a scaling or change of coordinates. But the proof is not. Everybody following this?",
    "start": "2830310",
    "end": "2835770"
  },
  {
    "text": "So basically, what\nwould happen is I'd say, how long will it take\nme to minimize this? And they'll say, oh, I\ndon't know, couple million",
    "start": "2835770",
    "end": "2841529"
  },
  {
    "text": "iterations, no more than that. Of course, then it takes 12. So then you're like, OK. And then you'd say the\nexact same function.",
    "start": "2841530",
    "end": "2848100"
  },
  {
    "text": "And then you'd say,\nif you don't mind, I'm going to change coordinates. Well, the algorithm once again\nsolves it in 12 iterations",
    "start": "2848100",
    "end": "2855089"
  },
  {
    "text": "because it's actually\naffine invariant. And you go, what's\nyour upper bound now? And they go, oh, now? Oh, boy.",
    "start": "2855090",
    "end": "2860890"
  },
  {
    "text": "15 million iterations for sure. I can prove it will take less\nthan 15 million iterations.",
    "start": "2860890",
    "end": "2866597"
  },
  {
    "text": "And you go, well,\nthank you very much. Just PS, it just took 12 again. Everybody following?",
    "start": "2866597",
    "end": "2871720"
  },
  {
    "text": "So this is bad. This is not how the\nworld should be. The world should be like this.",
    "start": "2871720",
    "end": "2876849"
  },
  {
    "text": "The math is beautiful and clean. That's how it should work. And then you go and you dig\ndeep into the source, which",
    "start": "2876850",
    "end": "2882847"
  },
  {
    "text": "you probably shouldn't do. But if you do\nthis, you should be emotionally prepared for\nwhat you're going to find. And you're going\nto find all sorts",
    "start": "2882847",
    "end": "2888017"
  },
  {
    "text": "of constants buried in there,\nlike 0.001 and this and that. And you're going to say, I\ndidn't see that in the LaTeX.",
    "start": "2888017",
    "end": "2893950"
  },
  {
    "text": "I didn't see a point, a buried-- so that's the way it should be. And you say, just be quiet.",
    "start": "2893950",
    "end": "2899440"
  },
  {
    "text": "We've tested it. It works fine in practice. Go away. That's the way the\nworld should be. But the math should be\nbeautiful and all that.",
    "start": "2899440",
    "end": "2905827"
  },
  {
    "text": "Anyway, this is the reverse. And to me, that's just\naesthetically very bad. Yes?",
    "start": "2905827",
    "end": "2911349"
  },
  {
    "text": "Why are they doing\nthis [INAUDIBLE] in machine learning? Well, I mean, yeah,\nignorance is one reason.",
    "start": "2911350",
    "end": "2918880"
  },
  {
    "text": "I mean, actually\npeople sometimes do use the methods you were\nhinting at or asking about,",
    "start": "2918880",
    "end": "2925510"
  },
  {
    "text": "which are quasi-Newton methods. So those are actually--\nshould be quite widely used.",
    "start": "2925510",
    "end": "2931270"
  },
  {
    "text": "Yeah. A quick answer is this\nis for smooth problems,",
    "start": "2931270",
    "end": "2937060"
  },
  {
    "text": "smooth functions. So this is going to rule\nout a lot of things. But if it's smooth, they\nshould use something like this.",
    "start": "2937060",
    "end": "2945099"
  },
  {
    "text": "And my suspicion is\nif you look at things like logistic regression or\nif you look at multinomi--",
    "start": "2945100",
    "end": "2951519"
  },
  {
    "text": "if you look at problems\nwhere you really could, if you look at what\nthe R implementation is",
    "start": "2951520",
    "end": "2956619"
  },
  {
    "text": "or whatever, it's probably this. It's fine.",
    "start": "2956620",
    "end": "2961930"
  },
  {
    "text": "OK. There's another\nbig reason, which we'll get to later, which is\nactually how to compute it.",
    "start": "2961930",
    "end": "2968500"
  },
  {
    "text": "It's a super interesting reason. It's the reason it\ndropped out of-- people stopped looking\nat this in the '70s",
    "start": "2968500",
    "end": "2974770"
  },
  {
    "text": "and stuff like that because\nthey were like, whoa, you actually have to invert a\nmatrix, which we all know now",
    "start": "2974770",
    "end": "2980440"
  },
  {
    "text": "means solve a set of equations. And then someone said,\nthat costs n cubed. And at that time,\nit would be like,",
    "start": "2980440",
    "end": "2986880"
  },
  {
    "text": "it's really slow if\nyou have to do it for 100 variables and 100\nequations, which is hilarious now because that's\nmicrosecond time for us.",
    "start": "2986880",
    "end": "2994710"
  },
  {
    "text": "And that was the other reason. But now linear you'd say,\nwell, now you're smarter.",
    "start": "2994710",
    "end": "2999757"
  },
  {
    "text": "After last week, you're\nmore sophisticated. And you'd say, yeah,\nin general that's true. But if there was some\nstructure in your problem, then",
    "start": "2999758",
    "end": "3007440"
  },
  {
    "text": "maybe we could do it faster. And then guess what? Smooth optimal control problems,\nsmooth signal processing",
    "start": "3007440",
    "end": "3012990"
  },
  {
    "text": "problems, you could\ndo it in linear time. But again, you\nhave to know that.",
    "start": "3012990",
    "end": "3018600"
  },
  {
    "text": "So OK. All right.",
    "start": "3018600",
    "end": "3024070"
  },
  {
    "text": "All right, so this is-- so\nI've made fun of this, I think, sufficiently. But now we're going to\nget to something that's--",
    "start": "3024070",
    "end": "3030910"
  },
  {
    "text": "actually, first, we're going\nto look at some examples. Because I mean, I'm\nsure you guessed this, but it works unbelievably well.",
    "start": "3030910",
    "end": "3038119"
  },
  {
    "text": "So I mean, here's the\nnumber of iterations. Note that it's five. I don't know if you\ncan see the power here.",
    "start": "3038120",
    "end": "3044800"
  },
  {
    "text": "So basically, it's just you\nget double precision, accuracy, and whatever.",
    "start": "3044800",
    "end": "3050410"
  },
  {
    "text": "I mean, it's a silly\nproblem in two dimensions.",
    "start": "3050410",
    "end": "3056740"
  },
  {
    "text": "Everybody follow? Oh, by the way, on a\nsemi-log plot, so iterations",
    "start": "3056740",
    "end": "3061750"
  },
  {
    "text": "and log accuracy, remember\nthat linear convergence is a straight line. And that means you're basically\nmaking a constant factor",
    "start": "3061750",
    "end": "3070720"
  },
  {
    "text": "improvement in your residual\nor your error in each step. Here it's accelerating.",
    "start": "3070720",
    "end": "3079870"
  },
  {
    "text": "And in fact, well,\nas a matter of fact you will be implementing\na Newton method next week,",
    "start": "3079870",
    "end": "3086210"
  },
  {
    "text": "I think. Is that true? Homework 8 [INAUDIBLE]\nhomework 7 Newton.",
    "start": "3086210",
    "end": "3092210"
  },
  {
    "text": "Yeah. OK, yeah, homework 7. That's tomorrow. Is homework 7 posted?",
    "start": "3092210",
    "end": "3098510"
  },
  {
    "text": "Not yet. It will be tomorrow. OK. So yeah, so tomorrow. Anyway, you're looking for--\nthat's what you're looking",
    "start": "3098510",
    "end": "3105260"
  },
  {
    "text": "for right there,\nsomething like-- this is a tiny problem, so\nit won't look that dramatic, but I'll show you\nwhat it looks like.",
    "start": "3105260",
    "end": "3112319"
  },
  {
    "text": "So here's an example\nin 100 dimensions or whatever,\nsomething like that.",
    "start": "3112320",
    "end": "3118510"
  },
  {
    "text": "And it's fine. I mean, once again you\nsee very small numbers",
    "start": "3118510",
    "end": "3125000"
  },
  {
    "text": "of iterations and\nshocking accuracies. So make sense?",
    "start": "3125000",
    "end": "3131600"
  },
  {
    "text": "Yeah? [INAUDIBLE] the values? We do. That's right. Yeah.",
    "start": "3131600",
    "end": "3137150"
  },
  {
    "text": "So you will make this\nplot after it converges. Yeah. Just compare it to\nwhatever you've got.",
    "start": "3137150",
    "end": "3143360"
  },
  {
    "text": "Yeah, that's one way to do it. Yeah. I mean, there's\nanother way to do it. Instead here you could put\na norm of the gradient,",
    "start": "3143360",
    "end": "3150170"
  },
  {
    "text": "and that would also work. That you do know\nbefore it finishes. So these would be--",
    "start": "3150170",
    "end": "3157160"
  },
  {
    "text": "but the other one-- these\nare made after you finish. Otherwise, it sounds\nlike the story before.",
    "start": "3157160",
    "end": "3163550"
  },
  {
    "text": "Like, what's the bound on\nthe number of iterations? You go, no problem.",
    "start": "3163550",
    "end": "3168769"
  },
  {
    "text": "What's the optimal value? And you're like, dude,\nI haven't even started. I was asking you how many\nsteps it's going to take me.",
    "start": "3168770",
    "end": "3175040"
  },
  {
    "text": "OK, yeah. And this makes sense. And you can go to a\nproblem that's bigger.",
    "start": "3175040",
    "end": "3181357"
  },
  {
    "text": "And actually, this\nwill be a first one where we get something. So here it's a problem\nin 10,000 variables.",
    "start": "3181357",
    "end": "3188180"
  },
  {
    "text": "It's fine. And it's the same story. And you see this\nthing accelerating,",
    "start": "3188180",
    "end": "3194510"
  },
  {
    "text": "and that tells you\nyou are in the region of quadratic convergence.",
    "start": "3194510",
    "end": "3200480"
  },
  {
    "text": "Now, I have a question. And this will be\nfun, but let me--",
    "start": "3200480",
    "end": "3207550"
  },
  {
    "text": "let's see. What is it? No, this is fine. OK. So in this one, you can see\nthat it's some big log barrier",
    "start": "3207550",
    "end": "3215490"
  },
  {
    "text": "problem or something like that. I'll just leave it. I'll leave it there. [INAUDIBLE]",
    "start": "3215490",
    "end": "3221400"
  },
  {
    "text": " I'm suspicious of\nsomething here. I'm deeply suspicious of\nthe 100,000 over there.",
    "start": "3221400",
    "end": "3229535"
  },
  {
    "text": "Do you mind cross-referencing\nthat against the book or something like that? Yeah, I'll check that. OK. It's fine. But anyway, it looks\nlike it's too much.",
    "start": "3229535",
    "end": "3237520"
  },
  {
    "text": "Anyway, so the idea is-- and\nI mean, this is ridiculous. So this is a function in--",
    "start": "3237520",
    "end": "3244200"
  },
  {
    "text": "what is it-- in R 10,000. Just for the record, R\n10,000 is a pretty big place.",
    "start": "3244200",
    "end": "3249750"
  },
  {
    "text": "And then this says\nyou stop and you ask for directions 18 times.",
    "start": "3249750",
    "end": "3255990"
  },
  {
    "text": "You stop, and you\nsay, here's my x. And you'd say, hey-- asking for directions\nis basically calculating",
    "start": "3255990",
    "end": "3261300"
  },
  {
    "text": "the gradient and the Hessian. So you ask for\ndirections 18 times. You go, OK, thanks, thanks.",
    "start": "3261300",
    "end": "3267240"
  },
  {
    "text": "Then you do a\nbacktracking line search, which is not very crude. How is it that in 18--",
    "start": "3267240",
    "end": "3274320"
  },
  {
    "text": "you stop, ask for\ndirections 18 times, and you end up\nfinding the minimum",
    "start": "3274320",
    "end": "3280319"
  },
  {
    "text": "of this thing in relatively high\ndimensions to high accuracy? I mean, I think you have to\nbe impressed by this, I think.",
    "start": "3280320",
    "end": "3287650"
  },
  {
    "text": "You should be, anyway. OK. So we're going to\nactually now talk about--",
    "start": "3287650",
    "end": "3296010"
  },
  {
    "text": "actually, it's very cool. It is a theory I can get\nbehind and don't make fun of. Yes?",
    "start": "3296010",
    "end": "3301230"
  },
  {
    "text": "So for R 10,000, how long does\nit take to compute the Hessian? Ah, OK.",
    "start": "3301230",
    "end": "3306450"
  },
  {
    "text": "So that's why I have\na suspicion here. ",
    "start": "3306450",
    "end": "3313770"
  },
  {
    "text": "Yeah. So I have a weird feeling. Did you cross check that? I'm checking. I don't think that's\n100,000 over there.",
    "start": "3313770",
    "end": "3320510"
  },
  {
    "text": "[INAUDIBLE] It is? I have 100,000. Oh, it's 100,000. OK, that's fine. Well, we could actually\ntalk about how to--",
    "start": "3320510",
    "end": "3327620"
  },
  {
    "text": "what the Hessian would be here. Ah, sorry, sorry, sorry.",
    "start": "3327620",
    "end": "3332910"
  },
  {
    "text": "Sorry, I was missing something. It is 100,000. It says sparse ai. OK.",
    "start": "3332910",
    "end": "3338280"
  },
  {
    "text": "So that's a great question. Let let's address it. So if you knew nothing, you\ncould calculate the Hessian.",
    "start": "3338280",
    "end": "3347580"
  },
  {
    "text": "You get a\n10,000-by-10,000 matrix, and you'd have to solve\nlinear equations with that. Actually, that's\nborderline fine.",
    "start": "3347580",
    "end": "3354270"
  },
  {
    "text": "I mean, right now that\nwould be just totally fine. On GPU, it would be 0. You wouldn't even\nthink about it, OK?",
    "start": "3354270",
    "end": "3360510"
  },
  {
    "text": "I mean, it's not small. 10,000-by-10,000 matrix\nis 10 to the 8 numbers.",
    "start": "3360510",
    "end": "3367470"
  },
  {
    "text": "Multiply about 10\nto get doubles. And so you're talking a gig. It's not that big.",
    "start": "3367470",
    "end": "3374849"
  },
  {
    "text": "Current GPUs can handle\na lot more than that. Everybody following this?",
    "start": "3374850",
    "end": "3381180"
  },
  {
    "text": "It's just not that hard. It wouldn't take that long\nto factorize it and so on. OK.",
    "start": "3381180",
    "end": "3386250"
  },
  {
    "text": "But the key is here that\nthe ai's are sparse here.",
    "start": "3386250",
    "end": "3393930"
  },
  {
    "text": "And so not only that,\nthe first term-- I wasn't going to do this now,\nbut we'll do it a little bit",
    "start": "3393930",
    "end": "3400730"
  },
  {
    "text": "now. So basically, what it is is to\nsolve for the Newton direction. We're actually going to\nuse the linear algebra",
    "start": "3400730",
    "end": "3406640"
  },
  {
    "text": "we learned last week.  What is the Hessian\nof that first term?",
    "start": "3406640",
    "end": "3412460"
  },
  {
    "text": " I just want to know does\nit have a structure?",
    "start": "3412460",
    "end": "3418830"
  },
  {
    "text": " What is it?",
    "start": "3418830",
    "end": "3426338"
  },
  {
    "text": "It's got a super\nimportant structure, one that should be after last\nweek burned into your mind.",
    "start": "3426338",
    "end": "3431944"
  },
  {
    "text": "It's what? [INAUDIBLE] It's what? Diagonal. It's diagonal.",
    "start": "3431945",
    "end": "3437700"
  },
  {
    "text": "Yeah, yeah. OK, so a function\nis separable if it's a sum of functions of the\nindividual components.",
    "start": "3437700",
    "end": "3446430"
  },
  {
    "text": "If I take the gradient, then\nthe fourth component-- or here, just take the second-- let's\ndo the second derivatives",
    "start": "3446430",
    "end": "3451950"
  },
  {
    "text": "in our head. If my f of x is sum\nover i of fi of xi",
    "start": "3451950",
    "end": "3457680"
  },
  {
    "text": "and I take the partial\nsquared of that with respect to partial xi, partial xj,\nit's 0 unless i equals j.",
    "start": "3457680",
    "end": "3467400"
  },
  {
    "text": "So that first one is diagonal. And then it's going to turn\nout the second is sparse. So the Hessian here is\ndiagonal plus sparse.",
    "start": "3467400",
    "end": "3476122"
  },
  {
    "text": "I mean, I guess the\ndiagonal is already sparse, so it's just sparse. So that should answer the\nquestion as to how we did this.",
    "start": "3476122",
    "end": "3482099"
  },
  {
    "text": " We'll get to that later\nbecause you're going to-- then",
    "start": "3482100",
    "end": "3487680"
  },
  {
    "text": "you're going to get\nthe full connection. Actually, you'll be really cool\nwhen you get the connection all",
    "start": "3487680",
    "end": "3492810"
  },
  {
    "text": "the way from the application to\nthe sparsity to how solve it, and then it's not a problem.",
    "start": "3492810",
    "end": "3498375"
  },
  {
    "text": " And that'll typically\ngo like this. I have this problem. You go, well, use\nNewton's method.",
    "start": "3498375",
    "end": "3504684"
  },
  {
    "text": "And then someone, let's say,\ntrained in machine learning, just to carry on my insult from\nearlier, would say like, oh,",
    "start": "3504685",
    "end": "3511270"
  },
  {
    "text": "are you kidding? There's 100,000 variables. Actually, they\nwouldn't even know.",
    "start": "3511270",
    "end": "3516390"
  },
  {
    "text": "But let's suppose there's\none who heard about it, went into Wikipedia and looked\nat it or something like that, and said, you can't do that.",
    "start": "3516390",
    "end": "3522120"
  },
  {
    "text": "You can't form. You can neither form nor store\n100,000-by-100,000 matrices. And even if you could,\nyou sure as hell",
    "start": "3522120",
    "end": "3528550"
  },
  {
    "text": "couldn't compute this Newton. You couldn't solve\nthese equations. And then you'd say, chill.",
    "start": "3528550",
    "end": "3534177"
  },
  {
    "text": "Because as a matter of\nfact, from your description of the problem, I've\ndetermined that the Hessian is banded arrow.",
    "start": "3534177",
    "end": "3542612"
  },
  {
    "text": "And they'll say, what's that? And you go, just\ndon't worry about it. What it means is I\ncan form that Hessian,",
    "start": "3542612",
    "end": "3547690"
  },
  {
    "text": "and I can compute the\nNewton step really fast. Everybody got that? So that would be\nhow that would work.",
    "start": "3547690",
    "end": "3554898"
  },
  {
    "text": "That's how it would work. OK. OK. Now, the next topic-- and\nit's actually really cool.",
    "start": "3554898",
    "end": "3563390"
  },
  {
    "text": "It addresses this question,\nthis embarrassment really of the classical\nanalysis of Newton's method,",
    "start": "3563390",
    "end": "3570470"
  },
  {
    "text": "which was less sophisticated\nthan the actual algorithm itself. Because the actual algorithm\nitself is affine independent,",
    "start": "3570470",
    "end": "3577569"
  },
  {
    "text": "but then the analysis,\nyou change coordinates and they give you a different\nnumber for their upper bound.",
    "start": "3577570",
    "end": "3584020"
  },
  {
    "text": "I mean, still useless, but they\ngive you a different number. OK. So it turns out there's a\nweird variation on this.",
    "start": "3584020",
    "end": "3590575"
  },
  {
    "text": "This comes from the late '80s\nand '90s from also Moscow. And it's developed by\nNesterov and Nemirovski.",
    "start": "3590575",
    "end": "3598569"
  },
  {
    "text": "They have this 500-page--\nno, 600, 700-page book.",
    "start": "3598570",
    "end": "3603580"
  },
  {
    "text": "It's not for the faint of heart. They do look at third\nderivatives of things",
    "start": "3603580",
    "end": "3612370"
  },
  {
    "text": "like-- they look at the\nthird derivative of things like log determinant\nof a matrix,",
    "start": "3612370",
    "end": "3617750"
  },
  {
    "text": "positive definite matrix. That's the classical\nSoviet math training.",
    "start": "3617750",
    "end": "3625370"
  },
  {
    "text": "One of them-- Nemirovski, actually\nonce told me-- he said, yes, we don't learn matrices.",
    "start": "3625370",
    "end": "3631579"
  },
  {
    "text": "He said, for you it is a crutch. He said, it leads to\nsloppy and weak thinking.",
    "start": "3631580",
    "end": "3638610"
  },
  {
    "text": "And he said, we don't do\nthat until much older. He says, we just do\neverything with indices.",
    "start": "3638610",
    "end": "3644460"
  },
  {
    "text": "I was like, whoa, that's cool. So anyway, it's very-- anyway, pretty crazy.",
    "start": "3644460",
    "end": "3649730"
  },
  {
    "text": "OK, fine. So here it is. And actually, it's\na hilarious thing. It actually replaces-- actually,\nit is only the following.",
    "start": "3649730",
    "end": "3658925"
  },
  {
    "text": "And then by the time I\ntell the story enough, the solution is\ngoing to be obvious, although this is just stunning\nin the early '90s or whatever.",
    "start": "3658925",
    "end": "3666740"
  },
  {
    "text": "Basically, the issue with\nthe classical analysis is that we had to say the\nthird derivative is small,",
    "start": "3666740",
    "end": "3674480"
  },
  {
    "text": "but we said it in a way that\nwas not affine independent. That was the issue.",
    "start": "3674480",
    "end": "3681680"
  },
  {
    "text": "Let's say it's a function\non just one variable so we don't have to worry\nabout tensors and three forms.",
    "start": "3681680",
    "end": "3688410"
  },
  {
    "text": "OK.  How do you say the\nderivative is small? And here's the obvious one.",
    "start": "3688410",
    "end": "3695750"
  },
  {
    "text": "You would say, how\nabout like this? ",
    "start": "3695750",
    "end": "3703240"
  },
  {
    "text": "There. That looks to me\nlike a statement that the third\nderivative is small. And you go, cool.",
    "start": "3703240",
    "end": "3709890"
  },
  {
    "text": "Now you say, let's do an\naffine change of coordinates. And actually, this scales.",
    "start": "3709890",
    "end": "3715230"
  },
  {
    "text": "So this is not a way-- this is not an acceptable way. I guess if I was going to use\nthe actual notation we were",
    "start": "3715230",
    "end": "3721565"
  },
  {
    "text": "using before, it would be that. So this is not a way to say\nthe third derivative is small",
    "start": "3721565",
    "end": "3732900"
  },
  {
    "text": "that is invariant under\nchanges of coordinates, OK? So now you have to\nfigure out how would you say the third derivative\nis small in a way",
    "start": "3732900",
    "end": "3740850"
  },
  {
    "text": "that when I change-- if I do a linear change of\ncoordinates it's invariant?",
    "start": "3740850",
    "end": "3745920"
  },
  {
    "text": "So that's actually what\nwe're going to get. Actually, what happens\nwhen you do this",
    "start": "3745920",
    "end": "3750930"
  },
  {
    "text": "is the most stunning thing. Instead of a silly theorem\nthat looks like this-- if,",
    "start": "3750930",
    "end": "3756060"
  },
  {
    "text": "and then a whole bunch\nof absolutely unknowable hypotheses were to occur. And then you'd say then you'd\nhave a completely useless",
    "start": "3756060",
    "end": "3764160"
  },
  {
    "text": "conclusion. That's my template for a\ntraditional convergence result. Then it's actually\ngoing to be very weird.",
    "start": "3764160",
    "end": "3771480"
  },
  {
    "text": "It's actually going to be if\na small number of actually weirdly knowable\nthings hold, then--",
    "start": "3771480",
    "end": "3777880"
  },
  {
    "text": "and then the conclusions\nare actually going to be-- they're not going to\nbe hilariously wrong.",
    "start": "3777880",
    "end": "3784720"
  },
  {
    "text": "They're not going to\nbe hilariously useless. So OK. So that's the idea. So we'll look at this.",
    "start": "3784720",
    "end": "3792170"
  },
  {
    "text": "OK. And actually, we need to\nuse it later in the class. So OK.",
    "start": "3792170",
    "end": "3797630"
  },
  {
    "text": "And they call this\nself-concordance. So here it is. It's this.",
    "start": "3797630",
    "end": "3803289"
  },
  {
    "text": "It says a function is\nself-concordant if the absolute",
    "start": "3803290",
    "end": "3808670"
  },
  {
    "text": "value is just from R to R. If\nthe absolute value of the third derivative is less than the--",
    "start": "3808670",
    "end": "3816190"
  },
  {
    "text": "2 doesn't matter. That's just for convenience. The second derivative\nto the 3/2 power.",
    "start": "3816190",
    "end": "3822190"
  },
  {
    "text": "OK. So that's it. And the reason is this, is\nthis is affine invariant.",
    "start": "3822190",
    "end": "3828910"
  },
  {
    "text": " If I replace this, if I\nhave f tilde of y is f of ay",
    "start": "3828910",
    "end": "3837580"
  },
  {
    "text": "plus b, then what happens is\nthe third derivative scales like the cube of a and the\nsecond like the square.",
    "start": "3837580",
    "end": "3845710"
  },
  {
    "text": "If you take those numbers and\nyou plug it into that formula, the a's drop away.",
    "start": "3845710",
    "end": "3851050"
  },
  {
    "text": "Everybody got this? So as a matter of\nfact, as an exercise, if I just said right\naway write down",
    "start": "3851050",
    "end": "3859329"
  },
  {
    "text": "something that says the\nthird derivative is small, but it has to be invariant to\nany affine change of variable,",
    "start": "3859330",
    "end": "3866500"
  },
  {
    "text": "you would have to\ncome up with that. Everybody see that?",
    "start": "3866500",
    "end": "3872050"
  },
  {
    "text": "You just have to. So I don't know if\nthis makes sense.",
    "start": "3872050",
    "end": "3877350"
  },
  {
    "text": "Now, on the other hand, if\nsomeone just walks up to me on the street and\nsays, yeah, I was thinking about Newton's method. I have a new\ndefinition of small--",
    "start": "3877350",
    "end": "3883780"
  },
  {
    "text": "sorry, I have a new definition\nof the third derivative of small, or I have\na new definition of what it means to\nbe nearly quadratic.",
    "start": "3883780",
    "end": "3891070"
  },
  {
    "text": "It's that. You would go like, where\ndid that come from? I mean, it's a bit shocking.",
    "start": "3891070",
    "end": "3897220"
  },
  {
    "text": "So OK. Everybody got the idea? ",
    "start": "3897220",
    "end": "3902810"
  },
  {
    "text": "So some weird things happen. It turns out that a lot-- certainly not all\nconvex functions",
    "start": "3902810",
    "end": "3913930"
  },
  {
    "text": "are so-called self-concordant,\nbut a whole lot of the ones that\nwe encounter are.",
    "start": "3913930",
    "end": "3919930"
  },
  {
    "text": "And anyway, so that's that. And you'd say, OK,\nwell, that's fine. You would expect a Newton's\nmethod based on this to be--",
    "start": "3919930",
    "end": "3927890"
  },
  {
    "text": "an analysis of Newton's method\nbased on self-concordance be-- you'd imagine it to be simpler. And you would be right.",
    "start": "3927890",
    "end": "3935810"
  },
  {
    "text": "And there's a lot of\nweird calculus stuff here you can actually\nwork about various things.",
    "start": "3935810",
    "end": "3941690"
  },
  {
    "text": "But actually, it's\ncool because you end up showing that a lot of the\nthings that we care about-- here's one right here.",
    "start": "3941690",
    "end": "3947660"
  },
  {
    "text": "So weirdly, it's\nself-concordant, which would not be fun to show.",
    "start": "3947660",
    "end": "3953351"
  },
  {
    "text": "It's not that hard, actually. To tell you the truth,\nit's not that bad. And a bunch of other functions\nare and things like that.",
    "start": "3953352",
    "end": "3960120"
  },
  {
    "text": "So again, I'm not going to-- this is not--",
    "start": "3960120",
    "end": "3965180"
  },
  {
    "text": "I mean, it actually elucidates\nwhy a lot of practical things actually do work well.",
    "start": "3965180",
    "end": "3971468"
  },
  {
    "text": "But I would say that\nas a practical matter, it's not that important. But we can-- we'll go--",
    "start": "3971468",
    "end": "3978230"
  },
  {
    "text": "keep going. So when you, in the sense\nof Nesterov and Nemirovski",
    "start": "3978230",
    "end": "3983390"
  },
  {
    "text": "do their analysis, they end up\nwith something pretty crazy. ",
    "start": "3983390",
    "end": "3989690"
  },
  {
    "text": "You actually end up again--\nit has exactly the same form. It says now you're using\nthe Newton decrement.",
    "start": "3989690",
    "end": "3996087"
  },
  {
    "text": "In the classical analysis, we're\nusing a norm of the gradient. Norm of the gradient is\nnot affine independent.",
    "start": "3996087",
    "end": "4002270"
  },
  {
    "text": "If I change coordinates,\nthen the norm of the gradient changes here.",
    "start": "4002270",
    "end": "4008329"
  },
  {
    "text": "But remember, the Newton\ndecrement does not. That's an affine\ninvariant quantity. And so it says if\nthe Newton decrement",
    "start": "4008330",
    "end": "4015890"
  },
  {
    "text": "is bigger than some number, then\nyou get a guaranteed decrease. Once on the other side, you\nget something like that.",
    "start": "4015890",
    "end": "4023220"
  },
  {
    "text": "So you get this very-- this is a complexity bound\nthat looks like that. And yeah, these things depend\non the backtracking parameters.",
    "start": "4023220",
    "end": "4031940"
  },
  {
    "text": "But a sloppy analysis tells\nyou that it ends up saying that the maximum number of iterations\nis less than 375 times f of x0",
    "start": "4031940",
    "end": "4040370"
  },
  {
    "text": "minus p star-- yes, that\nissue is still there-- ",
    "start": "4040370",
    "end": "4046800"
  },
  {
    "text": "plus 6. 6 is the way I say log\nlog 1 over epsilon.",
    "start": "4046800",
    "end": "4054980"
  },
  {
    "text": "In fact, I used\nto give talks when I knew there were going\nto be a bunch of theorists in the crowd. I replaced log log with\n4 and would do this just",
    "start": "4054980",
    "end": "4064579"
  },
  {
    "text": "to taunt them. And I'd wait. And sure enough, I'd\nsee somebody itching, and they're twitching.",
    "start": "4064580",
    "end": "4069787"
  },
  {
    "text": "And then they're like, wait,\nthis is an iterative algorithm. I was like, oh, I'm sorry. By 4, I meant log\nlog 1 over epsilon.",
    "start": "4069787",
    "end": "4077810"
  },
  {
    "text": "Sorry, that's a macro for me. Anyway, it works pretty-- I recommend this. If you know people like\nthat, you could try this too.",
    "start": "4077810",
    "end": "4085310"
  },
  {
    "text": "So OK. All right, there it is. There you go.",
    "start": "4085310",
    "end": "4090690"
  },
  {
    "text": "So you get actually\nsomething that's weird. What's weird is that\nthese are actually not-- I'll show you just\nsome examples here,",
    "start": "4090690",
    "end": "4096859"
  },
  {
    "text": "but here's your minimizing a-- you are minimizing a--",
    "start": "4096859",
    "end": "4104285"
  },
  {
    "text": "it's a log barrier for a\nset of linear inequalities or something. And then what we\ndid was we just--",
    "start": "4104285",
    "end": "4111920"
  },
  {
    "text": "so to explain what we\ndid is we generated a sample, solved it\nusing Newton's method, noted the number of\niterations, and we also",
    "start": "4111920",
    "end": "4119278"
  },
  {
    "text": "noted at that point p star. Then we could note this,\nwhich is the difference.",
    "start": "4119279",
    "end": "4126359"
  },
  {
    "text": "And then you just\nscatter plot it. And you get something like this. Now, by the way,\nthe bound is a line",
    "start": "4126359",
    "end": "4131430"
  },
  {
    "text": "that would be\nalmost vertical here because it's got this\ncoefficient 375 in there.",
    "start": "4131430",
    "end": "4137009"
  },
  {
    "text": "But still, what's\ninteresting is-- actually, it's really interesting. It looks like if you're\ndeposed in a math courtroom",
    "start": "4137010",
    "end": "4146729"
  },
  {
    "text": "or something like\nthat with scary people all around you\nlistening to everything",
    "start": "4146729",
    "end": "4153149"
  },
  {
    "text": "you say, hoping you're going\nto say something wrong, you use the number 375. It's fine.",
    "start": "4153149",
    "end": "4158189"
  },
  {
    "text": "And then they would\nsay, is that true? And you go, oh,\nthat's actually true. But what's very\ninteresting here is",
    "start": "4158189",
    "end": "4166528"
  },
  {
    "text": "that if you replace the\n375 with 1/2, for example, it actually gets a pretty\ngood estimate of actually how",
    "start": "4166529",
    "end": "4173399"
  },
  {
    "text": "many steps it's going to take. So this make sense?",
    "start": "4173399",
    "end": "4180149"
  },
  {
    "text": "And it's actually pretty cool. So you can actually give\na talk to two audiences,",
    "start": "4180149",
    "end": "4185818"
  },
  {
    "text": "and they can even\nbe mixed together. Because you just put\na constant in front. You just say it's this.",
    "start": "4185819",
    "end": "4192060"
  },
  {
    "text": "And then if someone says,\nwhat is a true statement, you'd say, well, to make\nthis a true statement I'd have to say 375.",
    "start": "4192060",
    "end": "4198360"
  },
  {
    "text": "But if you wanted to say, here's\nsomething that is probably true and a good approximation,\nyou just change c to 0.5,",
    "start": "4198360",
    "end": "4204900"
  },
  {
    "text": "and you're done. So OK. It's cool.",
    "start": "4204900",
    "end": "4210930"
  },
  {
    "text": "So this is how that works. Oh, by the way, can\nsomeone explain this one?",
    "start": "4210930",
    "end": "4220890"
  },
  {
    "text": "So here, this difference was-- I don't know. Let's say it was 12.",
    "start": "4220890",
    "end": "4226790"
  },
  {
    "text": "But it converged in\neight iterations.",
    "start": "4226790",
    "end": "4232370"
  },
  {
    "text": "What word would you say if\nsomeone said, explain that? ",
    "start": "4232370",
    "end": "4238865"
  },
  {
    "text": "What happened there? ",
    "start": "4238865",
    "end": "4244350"
  },
  {
    "text": "How about just dumb luck? There you are in some--",
    "start": "4244350",
    "end": "4249360"
  },
  {
    "text": "I don't even know what the\ndimension of these things is. OK. I have no idea. It's either 50 variables\nor 500 variables.",
    "start": "4249360",
    "end": "4255610"
  },
  {
    "text": "It doesn't matter. But the point was you asked\nfor directions, went that way. You asked again, seventh time\nyou asked for directions.",
    "start": "4255610",
    "end": "4262409"
  },
  {
    "text": "And then by dumb\nluck it just happened to point exactly\ntowards the minimum. so So that's what those are.",
    "start": "4262410",
    "end": "4269796"
  },
  {
    "text": "Those are just dumb luck. Yeah? On the previous slide,\nwhat was lambda of x?",
    "start": "4269796",
    "end": "4277769"
  },
  {
    "text": "What's lambda? Lambda is this Newton decrement. So that's actually-- it's this.",
    "start": "4277770",
    "end": "4284610"
  },
  {
    "text": "It's the gradient here. ",
    "start": "4284610",
    "end": "4292679"
  },
  {
    "text": "Actually, it's something\nlike this divided by 2. Actually, lambda is the\nsquare root of this.",
    "start": "4292680",
    "end": "4298215"
  },
  {
    "text": "This is very close. Lambda-- here, let\nme put it-- yeah, OK. Here. Lambda is something like that.",
    "start": "4298215",
    "end": "4305230"
  },
  {
    "text": "And there's a factor\nof 2 in there, roughly. I can't remember where\nthe square root of 2 goes,",
    "start": "4305230",
    "end": "4311860"
  },
  {
    "text": "but that's what it is. Actually, it's\nsuper interesting. It is the size of the gradient\nin the current Hessian",
    "start": "4311860",
    "end": "4320380"
  },
  {
    "text": "norm, which is pretty-- and that's how you\ncan guess that it's",
    "start": "4320380",
    "end": "4326500"
  },
  {
    "text": "going to be affine invariant. So OK.",
    "start": "4326500",
    "end": "4332210"
  },
  {
    "text": "So we looked at this a bit. And now we'll say a little\nbit about implementation. This is actually\nfar more important.",
    "start": "4332210",
    "end": "4338020"
  },
  {
    "text": "And this is interesting. Because now we're\ngoing to tie it to last week, which is\nnumerical linear algebra.",
    "start": "4338020",
    "end": "4345800"
  },
  {
    "text": "So basically, what it says\nis let's review where we are. At a high level you\nwould say this--",
    "start": "4345800",
    "end": "4351110"
  },
  {
    "text": "how do you minimize a\nsmooth convex function? And the answer is\nNewton's method. You say, well,\nwhat does that do?",
    "start": "4351110",
    "end": "4356225"
  },
  {
    "text": "You say that converts\nthe solution. How do you solve a\nsmooth convex function",
    "start": "4356225",
    "end": "4361849"
  },
  {
    "text": "is we don't know how\nto do that, but we do know how to minimize a\nconvex quadratic function.",
    "start": "4361850",
    "end": "4369170"
  },
  {
    "text": "That's easy. That's actually linear algebra. That's solving a set\nof linear equations.",
    "start": "4369170",
    "end": "4374640"
  },
  {
    "text": "So you could say that\nNewton's method reduces the solution of the\nminimization of a smooth convex",
    "start": "4374640",
    "end": "4381050"
  },
  {
    "text": "function to\nminimizing a sequence of quadratic functions. And someone says, OK, how do you\nminimize a quadratic function?",
    "start": "4381050",
    "end": "4389150"
  },
  {
    "text": "You go, oh, that's\nlinear algebra. That's called linear algebra. Everybody following this? So when you roll it\nup, it says that we",
    "start": "4389150",
    "end": "4395760"
  },
  {
    "text": "can minimize a smooth convex\nfunction by an iteration that's going to take some number of\ntens or whatever of iterations.",
    "start": "4395760",
    "end": "4403139"
  },
  {
    "text": "Each iteration is\nlinear algebra. And in fact, the linear\nalgebra is very simple.",
    "start": "4403140",
    "end": "4409360"
  },
  {
    "text": "You're just solving\nthis Newton system, H times delta x equals minus g.",
    "start": "4409360",
    "end": "4414705"
  },
  {
    "text": " One way to do it is just\nby Cholesky factorization.",
    "start": "4414705",
    "end": "4421770"
  },
  {
    "text": "So you do a Cholesky\nfactorization. And that'll cost\nyou n cubed flops for an unstructured system. ",
    "start": "4421770",
    "end": "4430110"
  },
  {
    "text": "OK. But here's the interesting part. If the Hessian is\nstructured and you",
    "start": "4430110",
    "end": "4438300"
  },
  {
    "text": "know how to exploit it,\nor even better basically you know how to call the\nright libraries to exploit",
    "start": "4438300",
    "end": "4447162"
  },
  {
    "text": "it or something\nlike that, which is what-- that's what I mean when I\nsay you know how to exploit it. If that's the case, then this\nis going to work really well.",
    "start": "4447162",
    "end": "4456719"
  },
  {
    "text": "Let me just give a couple of\nexamples about how that works. ",
    "start": "4456720",
    "end": "4464880"
  },
  {
    "text": "We could do-- I mean, it doesn't matter,\nbut we could do some kind of--",
    "start": "4464880",
    "end": "4470750"
  },
  {
    "text": "we'll just do some kind of\nsignal processing problem or optimal control problem where\nyou're estimating something,",
    "start": "4470750",
    "end": "4477180"
  },
  {
    "text": "but it's a-- in that case, the variables\nonly interact locally",
    "start": "4477180",
    "end": "4484469"
  },
  {
    "text": "in these types of-- like in a control problem\nor something like that. What'll happen\nthen is the Hessian",
    "start": "4484470",
    "end": "4491940"
  },
  {
    "text": "is then going to be banded\nbecause variable x doesn't",
    "start": "4491940",
    "end": "4498860"
  },
  {
    "text": "interact with\nvariable xi and xj. They only interact if i\nminus j is small enough, OK?",
    "start": "4498860",
    "end": "4507350"
  },
  {
    "text": "That means, if you go back\nto last week, that says, oh, we can solve those\nsystems in linear time.",
    "start": "4507350",
    "end": "4513320"
  },
  {
    "text": "I would not be afraid to solve-- I can compute the Newton\ndirection if H was banded,",
    "start": "4513320",
    "end": "4518900"
  },
  {
    "text": "let's say, with a\nbandwidth of 20. If it's in a million dimensions,\nI could solve that problem.",
    "start": "4518900",
    "end": "4524657"
  },
  {
    "text": "This, I guarantee you,\npeople in machine learning have absolutely not\nthe slightest idea of.",
    "start": "4524658",
    "end": "4530060"
  },
  {
    "text": "There might be one or two, but\nI doubt any more than that. OK, maybe a few more,\nbut not that many.",
    "start": "4530060",
    "end": "4537650"
  },
  {
    "text": "And that's actually\npart of the reason-- it is very interesting\nwhen people say, oh, you",
    "start": "4537650",
    "end": "4542652"
  },
  {
    "text": "couldn't use Newton's method. Why not? You go, well, first of all,\nyou have to form this Hessian. And second, then you\nhave to solve it.",
    "start": "4542652",
    "end": "4549020"
  },
  {
    "text": "And everybody knows\nthat's n cubed. But the point is if it's\nstructured, it's not. So OK.",
    "start": "4549020",
    "end": "4554212"
  },
  {
    "text": "And then you'd say, how\noften does structure come up? And the answer is a\nlot more frequently than you would imagine.",
    "start": "4554212",
    "end": "4560790"
  },
  {
    "text": "I mean, that also makes sense. OK. So this is Newton method,\nhow you would actually",
    "start": "4560790",
    "end": "4568920"
  },
  {
    "text": "implement it. Oh, by the way, I\nmentioned this other method where you update\nbasically the quadratic.",
    "start": "4568920",
    "end": "4576990"
  },
  {
    "text": "What you do is you\ncalculate the Hessian once. And you use that as your--\nyou use that for the next,",
    "start": "4576990",
    "end": "4583210"
  },
  {
    "text": "let's say, 10 or 20 steps. You don't update the Hessian. You update it every 20 steps.",
    "start": "4583210",
    "end": "4588260"
  },
  {
    "text": "It's got a name for\nit, and I forgot what that method is called. How do you do that here?",
    "start": "4588260",
    "end": "4594800"
  },
  {
    "text": "And what's the cost of that? This is completely dense,\nno structure in the Hessian.",
    "start": "4594800",
    "end": "4600650"
  },
  {
    "text": "What do you do? Every 20 steps, you evaluate\nthe Hessian, factor it.",
    "start": "4600650",
    "end": "4605719"
  },
  {
    "text": "That costs you n cubed. Then how much do actually\ncomputing the Newton steps when you have different\ngradients here?",
    "start": "4605720",
    "end": "4613070"
  },
  {
    "text": " What's the number? N squared. It's n squared, exactly.",
    "start": "4613070",
    "end": "4619219"
  },
  {
    "text": "So if n is big,\nthat's a big savings. By the way, that method\nworks perfectly well.",
    "start": "4619220",
    "end": "4624620"
  },
  {
    "text": "People don't\nimplement it because I think they're just lazy, so\nthey just factor every time. But it's an old method\nfrom the '70s or '80s,",
    "start": "4624620",
    "end": "4631280"
  },
  {
    "text": "and it's perfectly good. Probably people should use it.",
    "start": "4631280",
    "end": "4636300"
  },
  {
    "text": "OK. So here's a quick example. But I think what we're going to\ndo is we're going to quit here.",
    "start": "4636300",
    "end": "4643343"
  },
  {
    "text": "And we'll quit here for now. And then I think there's a--",
    "start": "4643343",
    "end": "4648539"
  },
  {
    "text": "let me just mention one thing. You're doing something\non the homework. And you're actually--\nwe're only going to cover the material\nfor that next Tuesday.",
    "start": "4648540",
    "end": "4655160"
  },
  {
    "text": "So you'll have to read\nahead to Newton method with equality constraints\nif you want to start on that",
    "start": "4655160",
    "end": "4660949"
  },
  {
    "text": "earlier than next Tuesday. This is a heads up. ",
    "start": "4660950",
    "end": "4670000"
  }
]