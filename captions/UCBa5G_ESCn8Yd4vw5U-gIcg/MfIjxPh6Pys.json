[
  {
    "text": "Hello everyone. Uh, welcome to CS229. Um, today we're going to talk about,",
    "start": "3500",
    "end": "10559"
  },
  {
    "text": "um, deep learning and neural networks. Um, we're going to have two lectures on that,",
    "start": "10560",
    "end": "16980"
  },
  {
    "text": "one today and a little bit more of it on, ah, Monday. Um, don't hesitate to ask questions during the lecture.",
    "start": "16980",
    "end": "24869"
  },
  {
    "text": "Ah, so stop me if you don't understand something and we'll try to build intuition around neural networks together.",
    "start": "24870",
    "end": "30689"
  },
  {
    "text": "We will actually start with an algorithm that you guys have seen, uh, previously called logistic regression.",
    "start": "30690",
    "end": "36015"
  },
  {
    "text": "Everybody remembers logistic regression? Yes. Okay. Remember it's a classification algorithm.",
    "start": "36015",
    "end": "41030"
  },
  {
    "text": "Um, we're going to do that. Explain how logistic regression can be interpreted as",
    "start": "41030",
    "end": "46940"
  },
  {
    "text": "a neural network- specific case of a neural network and then, we will go to neural networks. Sounds good?",
    "start": "46940",
    "end": "54114"
  },
  {
    "text": "So the quick intro on deep learning.",
    "start": "54115",
    "end": "57300"
  },
  {
    "text": "So deep learning is a- is a set of techniques that is let's say a subset of",
    "start": "62240",
    "end": "69290"
  },
  {
    "text": "machine learning and it's one of the growing techniques that have been used in the industry specifically for problems in computer vision,",
    "start": "69290",
    "end": "76159"
  },
  {
    "text": "natural language processing and speech recognition. So you guys have a lot of different tools and,",
    "start": "76159",
    "end": "81840"
  },
  {
    "text": "and uh, plug-ins on your smartphones that uses this type of algorithm. Ah, the reason it came, uh,",
    "start": "81840",
    "end": "88880"
  },
  {
    "text": "to work very well is primarily the, the new computational methods. So one thing we're going to see to- today, um,",
    "start": "88880",
    "end": "97460"
  },
  {
    "text": "is that deep learning is really really computationally expensive and we- people had to",
    "start": "97460",
    "end": "104630"
  },
  {
    "text": "find techniques in order to parallelize the code and use GPUs specifically in order to graphical processing units,",
    "start": "104630",
    "end": "111830"
  },
  {
    "text": "in order to be able to compute, uh, the, the, the, the computations in deep learning.",
    "start": "111830",
    "end": "116870"
  },
  {
    "text": "Ah, the second part is the data available has been growing after,",
    "start": "116870",
    "end": "125460"
  },
  {
    "text": "after the Internet bubble, the digitalization of the world. So now people have access to large amounts of data and this type of algorithm has",
    "start": "125460",
    "end": "133310"
  },
  {
    "text": "the specificity of being able to learn a lot when there is a lot of data. So these models are very flexible and the more you give them data,",
    "start": "133310",
    "end": "141170"
  },
  {
    "text": "the more they will be able to understand the salient feature of the data. And finally algorithms.",
    "start": "141170",
    "end": "148805"
  },
  {
    "text": "So people have come up with, with new techniques, uh, in order to use the data,",
    "start": "148805",
    "end": "155079"
  },
  {
    "text": "use the computation power and build models. So we are going to touch a little bit on all of that,",
    "start": "155080",
    "end": "160519"
  },
  {
    "text": "but let's go with logistic regression first.",
    "start": "160519",
    "end": "163709"
  },
  {
    "text": "Can you guys see in the back? Yeah? Okay, perfect.",
    "start": "168410",
    "end": "174105"
  },
  {
    "text": "So you remember, uh, what logistic regression is? What- we are going to fix a goal for us,",
    "start": "174105",
    "end": "181840"
  },
  {
    "text": "uh, that, uh, is a classification goal. So let's try to,",
    "start": "181840",
    "end": "187320"
  },
  {
    "text": "to find cats in images. So find cats in images.",
    "start": "187320",
    "end": "193870"
  },
  {
    "text": "Meaning binary classification. If there is a cat in the image,",
    "start": "195180",
    "end": "201290"
  },
  {
    "text": "we want to output a number that is close to 1, presence of the cat,",
    "start": "202080",
    "end": "208010"
  },
  {
    "text": "and if there is no cat in the image, we wanna output 0.",
    "start": "208010",
    "end": "217245"
  },
  {
    "text": "Let- let's say for now, ah, we're constrained to the fact that there is maximum one cat per image, there's no more.",
    "start": "217245",
    "end": "223145"
  },
  {
    "text": "If you are to draw the logistic regression model, that's what you would do. You would take a cat.",
    "start": "223145",
    "end": "229220"
  },
  {
    "text": "So this is an image of a cat. I'm very bad at that. Um, sorry.",
    "start": "229220",
    "end": "238489"
  },
  {
    "text": "In computer science, you know that images can be represented as 3D matrices.",
    "start": "238490",
    "end": "244610"
  },
  {
    "text": "So if I tell you that this is a color image of size 64 by 64,",
    "start": "244610",
    "end": "251410"
  },
  {
    "text": "how many numbers do I have to represent those pixels? [BACKGROUND] Yeah, I heard it,",
    "start": "251410",
    "end": "261840"
  },
  {
    "text": "64 by 64 by 3. Three for the RGB channel, red, green, blue.",
    "start": "261840",
    "end": "269500"
  },
  {
    "text": "Every pixel in an image can be represented by three numbers. One representing the red filter, the green filter, and the, and the blue filter.",
    "start": "269500",
    "end": "277000"
  },
  {
    "text": "So actually this image is of size 64 times 64 times 3.",
    "start": "277000",
    "end": "283160"
  },
  {
    "text": "That makes sense? So the first thing we will do in order to use logistic regression to find if there is a cat in this image,",
    "start": "283160",
    "end": "290035"
  },
  {
    "text": "we're going to flatten th- this into a vector.",
    "start": "290035",
    "end": "293300"
  },
  {
    "text": "So I'm going to take all the numbers in this matrix and flatten them in a vector.",
    "start": "295190",
    "end": "300415"
  },
  {
    "text": "Just an image to vector operation, nothing more. And now I can use my logistic regression because I have a vector input.",
    "start": "300415",
    "end": "309065"
  },
  {
    "text": "So I'm going to, to take all of these and push them in an operation that",
    "start": "309065",
    "end": "316520"
  },
  {
    "text": "we call th- the logistic operation which has one part that is wx plus b,",
    "start": "316520",
    "end": "324319"
  },
  {
    "text": "where x is going to be the image. So wx plus b,",
    "start": "324320",
    "end": "331635"
  },
  {
    "text": "and the second part is going to be the sigmoid. Everybody is familiar with the sigmoid function?",
    "start": "331635",
    "end": "338280"
  },
  {
    "text": "Function that takes a number between minus infinity and plus infinity and maps it between 0 and 1. It is very convenient for classification problems.",
    "start": "338280",
    "end": "345250"
  },
  {
    "text": "And this we are going to call it y hat, which is sigmoid of what you've seen in class previously,",
    "start": "345250",
    "end": "351155"
  },
  {
    "text": "I think it's Theta transpose x. But here we will just separate the notation into w and b.",
    "start": "351155",
    "end": "357590"
  },
  {
    "text": "So can someone tell me what's the shape of w? The matrix W, vector matrix.",
    "start": "363510",
    "end": "373370"
  },
  {
    "text": "Um, what?",
    "start": "374750",
    "end": "379540"
  },
  {
    "text": "Yes, 64 by 64 by 3 as a- yeah.",
    "start": "384240",
    "end": "389395"
  },
  {
    "text": "So you know that this guy here is a vector of 64 by 64 by 3, a column vector.",
    "start": "389395",
    "end": "397565"
  },
  {
    "text": "So the shape of x is going to be 64 by 64 by 3 times 1.",
    "start": "397565",
    "end": "406320"
  },
  {
    "text": "This is the shape and this, I think it's- that if I don't know,",
    "start": "406320",
    "end": "412005"
  },
  {
    "text": "12,288 and this indeed because we want y-hat to be one-by-one,",
    "start": "412005",
    "end": "420395"
  },
  {
    "text": "this w has to be 1 by 12,288.",
    "start": "420395",
    "end": "426879"
  },
  {
    "text": "That makes sense? So we have a row vector as our parameter. We're just changing the notations of the logistic regression that you guys have seen.",
    "start": "426880",
    "end": "436919"
  },
  {
    "text": "And so once we have this model, we need to train it as you know. And the process of training is that first,",
    "start": "436920",
    "end": "443240"
  },
  {
    "text": "we will initialize our parameters.",
    "start": "443240",
    "end": "449840"
  },
  {
    "text": "These are what we call parameters. We will use the specific vocabulary of weights and bias.",
    "start": "449840",
    "end": "458450"
  },
  {
    "text": "I believe you guys have heard this vocabulary before, weights and biases.",
    "start": "458450",
    "end": "464315"
  },
  {
    "text": "So we're going to find the right w and the right b in order to be able,",
    "start": "464315",
    "end": "470750"
  },
  {
    "text": "ah, to use this model properly. Once we initialized them, what we will do is that we will optimize them,",
    "start": "470750",
    "end": "478810"
  },
  {
    "text": "find the optimal w and b,",
    "start": "478810",
    "end": "485595"
  },
  {
    "text": "and after we found the optimal w and b, we will use them to predict.",
    "start": "485595",
    "end": "491250"
  },
  {
    "text": "Does this process make sense? This training process? And I think the important part is to understand what this is.",
    "start": "500760",
    "end": "508780"
  },
  {
    "text": "Find the optimal w and b means defining your loss function which is the objective.",
    "start": "508780",
    "end": "513985"
  },
  {
    "text": "And in machine learning, we often have this, this, this specific problem where you have a function that you know you want to find,",
    "start": "513985",
    "end": "521740"
  },
  {
    "text": "the network function, but you don't know the values of its parameters. In order to find them, you're going to use",
    "start": "521740",
    "end": "526900"
  },
  {
    "text": "a proxy that is going to be your loss function. If you manage to minimize the loss function, you will find the right parameters.",
    "start": "526900",
    "end": "533485"
  },
  {
    "text": "So you define a loss function, that is the logistic loss.",
    "start": "533485",
    "end": "540230"
  },
  {
    "text": "Y log of y hat plus 1 minus y log of 1 minus y hat up.",
    "start": "541050",
    "end": "551214"
  },
  {
    "text": "You guys have seen this one. You remember where it comes from? Comes from a maximum likelihood estimation,",
    "start": "551215",
    "end": "558880"
  },
  {
    "text": "starting from a probabilistic model. And so the idea is how can I minimize this function.",
    "start": "558880",
    "end": "566230"
  },
  {
    "text": "Minimize, because I've put the minus sign here. I want to find w and b that",
    "start": "566230",
    "end": "572350"
  },
  {
    "text": "minimize this function and I'm going to use a gradient descent algorithm. Which means I'm going to",
    "start": "572350",
    "end": "578245"
  },
  {
    "text": "iteratively compute the derivative of the loss with respect to my parameters.",
    "start": "578245",
    "end": "584779"
  },
  {
    "text": "And at every step, I will update them to make this loss function go a little down at every iterative step.",
    "start": "588870",
    "end": "595779"
  },
  {
    "text": "So in terms of implementation, this is a for loop. You will loop over a certain number of iteration and at every point,",
    "start": "595780",
    "end": "602440"
  },
  {
    "text": "you will compute the derivative of the loss with respect to your parameters. Everybody remembers how to compute this number?",
    "start": "602440",
    "end": "609650"
  },
  {
    "text": "Take the derivative here, you use the fact that the sigmoid function has a derivative that is sigmoid times 1 minus sigmoid,",
    "start": "609840",
    "end": "618640"
  },
  {
    "text": "and you will compute the results. We- we're going to do some derivative later today. But just to set up the problem here.",
    "start": "618640",
    "end": "626350"
  },
  {
    "text": "So, the few things that I wanna- that I wanna touch on here is,",
    "start": "626350",
    "end": "631720"
  },
  {
    "text": "first, how many parameters does this model have? This logistic regression? If you have to, count them.",
    "start": "631720",
    "end": "638230"
  },
  {
    "text": "So this is the numb- 089 yeah, correct. So 12,288 weights and 1 bias. That makes sense?",
    "start": "645510",
    "end": "653769"
  },
  {
    "text": "So, actually, it's funny because you can quickly count it by just counting the number of edges on the- on the- on the drawing plus 1.",
    "start": "653770",
    "end": "661325"
  },
  {
    "text": "Every circle has a bias. Every edge has a weight because ultimately this",
    "start": "661325",
    "end": "668019"
  },
  {
    "text": "operation you can rewrite it like that, right? It means every weight has- every weight corresponds to an edge.",
    "start": "668020",
    "end": "677620"
  },
  {
    "text": "So that's another way to count it, we are going to use it a little further. So we're starting with not too many parameters actually.",
    "start": "677620",
    "end": "683665"
  },
  {
    "text": "And one thing that we notice is that the number of parameters of our model depends on the size of the input.",
    "start": "683665",
    "end": "689035"
  },
  {
    "text": "We probably don't want that at some point, so we are going to change it later. So two equations that I want you to remember is,",
    "start": "689035",
    "end": "698170"
  },
  {
    "text": "the first one is neuron equals linear plus activation.",
    "start": "698170",
    "end": "704260"
  },
  {
    "text": "So this is the vocabulary we will use in neural networks. We define a neuron as an operation that has two parts,",
    "start": "704260",
    "end": "711910"
  },
  {
    "text": "one linear part, and one activation part and it's exactly that. This is actually a neuron.",
    "start": "711910",
    "end": "717470"
  },
  {
    "text": "We have a linear part, wx plus b and then we take the output of this linear part and we put it in an activation,",
    "start": "718710",
    "end": "727210"
  },
  {
    "text": "that in this case, is the sigmoid function. It can be other functions, okay?",
    "start": "727210",
    "end": "732490"
  },
  {
    "text": "So this is the first equation, not too hard. The second equation that I wanna set now is the model",
    "start": "732490",
    "end": "740904"
  },
  {
    "text": "equals architecture plus parameters.",
    "start": "740905",
    "end": "748700"
  },
  {
    "text": "What does that mean? It means here we're, we're trying to train a logistic regression in order to,",
    "start": "750180",
    "end": "757015"
  },
  {
    "text": "to be able to use it. We need an architecture which is the following,",
    "start": "757015",
    "end": "762070"
  },
  {
    "text": "a one neuron neural network and the parameters w and b.",
    "start": "762070",
    "end": "767785"
  },
  {
    "text": "So basically, when people say we've shipped a model, like in the industry, what they're saying is that they found the right parameters,",
    "start": "767785",
    "end": "775765"
  },
  {
    "text": "with the right architecture. They have two files and these two files are predicting a bunch of things, okay?",
    "start": "775765",
    "end": "782480"
  },
  {
    "text": "One parameter file and one architecture file. The architecture will be modified a lot today.",
    "start": "782480",
    "end": "788245"
  },
  {
    "text": "We will add neurons all over and the parameters will always be called w and b,",
    "start": "788245",
    "end": "793885"
  },
  {
    "text": "but they will become bigger and bigger. Because we have more data, we want to be able to understand it.",
    "start": "793885",
    "end": "799060"
  },
  {
    "text": "You can get that it's going to be hard to understand what a cat is with only that, that, that many parameters.",
    "start": "799060",
    "end": "805430"
  },
  {
    "text": "We want to have more parameters. Any questions so far?",
    "start": "805430",
    "end": "811449"
  },
  {
    "text": "So this was just to set up the problem with logistic regression. Let's try to set a new goal,",
    "start": "811450",
    "end": "818260"
  },
  {
    "text": "after the first goal we have set prior to that. So the second goal would be, find cat,",
    "start": "818260",
    "end": "829810"
  },
  {
    "text": "a lion, iguana in images.",
    "start": "829810",
    "end": "836860"
  },
  {
    "text": "So a little different than before, only thing we changed is that we want to now to detect three types of animals.",
    "start": "836860",
    "end": "844270"
  },
  {
    "text": "Either if there's a cat in the image, I wanna know there is a cat. If there's an iguana in the image, I wanna know there is an iguana.",
    "start": "844270",
    "end": "849430"
  },
  {
    "text": "If there is a lion in the image, I wanna know it as well. So how would you modify the network",
    "start": "849430",
    "end": "855565"
  },
  {
    "text": "that we previously had in order to take this into account?",
    "start": "855565",
    "end": "859550"
  },
  {
    "text": "Yeah? Yeah, good idea.",
    "start": "862950",
    "end": "867970"
  },
  {
    "text": "So put two more circles, so neurons, and do the same thing. So we have our picture here with the cats.",
    "start": "867970",
    "end": "876010"
  },
  {
    "text": "So the cat is going to the right.",
    "start": "876010",
    "end": "879920"
  },
  {
    "text": "64 by 64 by 3 we flatten it, from x1 to xn.",
    "start": "881700",
    "end": "887770"
  },
  {
    "text": "Let's say n represents 64, 64 by 3 and what I will do, is that I will use three neurons that are all computing the same thing.",
    "start": "887770",
    "end": "897490"
  },
  {
    "text": "They're all connected to all these inputs, okay?",
    "start": "897490",
    "end": "906700"
  },
  {
    "text": "I connect all my inputs x1 to xn to each of these neurons, and I will use a specific set of notation here. Okay.",
    "start": "906700",
    "end": "922639"
  },
  {
    "text": "Y_2 hat equals a_2_1 sigmoid of",
    "start": "943220",
    "end": "948870"
  },
  {
    "text": "w_ 2_1 plus x plus b_2_1.",
    "start": "948870",
    "end": "956145"
  },
  {
    "text": "And similarly, y_3 hat equals a_3_1,",
    "start": "956145",
    "end": "961670"
  },
  {
    "text": "which is sigmoid of w_ 3_1 x plus b_3_1.",
    "start": "961670",
    "end": "969160"
  },
  {
    "text": "So I'm introducing a few notations here and we'll, we'll get used to it, don't worry.",
    "start": "970650",
    "end": "976075"
  },
  {
    "text": "So just, just write this down and we're going to go over it. So [NOISE] the square brackets here represent what we will call later on a layer.",
    "start": "976075",
    "end": "986905"
  },
  {
    "text": "If you look at this network, it looks like there is one layer here. There's one layer in which neurons don't communicate with each other.",
    "start": "986905",
    "end": "995200"
  },
  {
    "text": "We could add up to it, and we will do it later on, more neurons in other layers. We will denote with square brackets the index of the layer.",
    "start": "995200",
    "end": "1003450"
  },
  {
    "text": "The index, that is, the subscript to this a is the number identifying the neuron inside the layer.",
    "start": "1003450",
    "end": "1010980"
  },
  {
    "text": "So here we have one layer. We have a_1, a_2, and a_3 with square brackets one to identify the layer. Does that make sense?",
    "start": "1010980",
    "end": "1018960"
  },
  {
    "text": "And then we have our y-hat that instead of being a single number as it was before,",
    "start": "1018960",
    "end": "1024675"
  },
  {
    "text": "is now a vector of size three. So how many parameters does this network have?",
    "start": "1024675",
    "end": "1031650"
  },
  {
    "text": "[NOISE]",
    "start": "1031650",
    "end": "1045569"
  },
  {
    "text": "How much? [BACKGROUND] Okay. How did you come up with that?",
    "start": "1045570",
    "end": "1050669"
  },
  {
    "text": "[BACKGROUND]. Okay. Yeah, correct. So we just have three times the, the thing we had before because we added",
    "start": "1050670",
    "end": "1057180"
  },
  {
    "text": "two more neurons and they all have their own set of parameters. Look like this edge is a separate edge as this one.",
    "start": "1057180",
    "end": "1063615"
  },
  {
    "text": "So we, we have to replicate parameters for each of these. So w_1_1 would be the equivalent of what we had for the cat,",
    "start": "1063615",
    "end": "1069809"
  },
  {
    "text": "but we have to add two more, ah, parameter vectors and biases.",
    "start": "1069810",
    "end": "1074940"
  },
  {
    "text": "[NOISE] So other question, when you had to train this logistic regression,",
    "start": "1074940",
    "end": "1081185"
  },
  {
    "text": "what dataset did you need? [NOISE]",
    "start": "1081185",
    "end": "1094040"
  },
  {
    "text": "Can someone try to describe the dataset. Yeah.",
    "start": "1094040",
    "end": "1099450"
  },
  {
    "text": "[BACKGROUND] Yeah, correct. So we need images and labels with it labeled as cat,",
    "start": "1099450",
    "end": "1108075"
  },
  {
    "text": "1 or no cat, 0. So it is a binary classification with images and labels. Now, what do you think should be the dataset to train this network? Yes.",
    "start": "1108075",
    "end": "1122100"
  },
  {
    "text": "[BACKGROUND]",
    "start": "1122100",
    "end": "1129600"
  },
  {
    "text": "That's a good idea. So just to repeat. Uh, a label for an image that has a cat would probably be",
    "start": "1129600",
    "end": "1139215"
  },
  {
    "text": "a vector with a 1 and two 0s where the 1 should represent the prese- the presence of a cat.",
    "start": "1139215",
    "end": "1147180"
  },
  {
    "text": "This one should represent the presence of a lion and this one should represent the presence of an iguana.",
    "start": "1147180",
    "end": "1153159"
  },
  {
    "text": "So let's assume I use this scheme to label my dataset. I train this network using the same techniques here.",
    "start": "1154760",
    "end": "1162240"
  },
  {
    "text": "Initialize all my weights and biases with a value, a starting value,",
    "start": "1162240",
    "end": "1167309"
  },
  {
    "text": "optimize a loss function by using gradient descent, and then use y-hat equals, uh, log to predict.",
    "start": "1167310",
    "end": "1176170"
  },
  {
    "text": "What do you think this neuron is going to be responsible for?",
    "start": "1177380",
    "end": "1182350"
  },
  {
    "text": "If you had to describe the responsibility of this neuron. [BACKGROUND]",
    "start": "1186590",
    "end": "1193830"
  },
  {
    "text": "Yes. Well, this one. Lion. Yeah, lion and this one iguana.",
    "start": "1193830",
    "end": "1200415"
  },
  {
    "text": "So basically the, the, the way you- yeah, go for it. [BACKGROUND]",
    "start": "1200415",
    "end": "1207260"
  },
  {
    "text": "That's a good question. We're going to talk about that now. Multiple image contain different animals or not.",
    "start": "1207260",
    "end": "1212345"
  },
  {
    "text": "So going back on what you said, because we decided to label our dataset like that,",
    "start": "1212345",
    "end": "1217505"
  },
  {
    "text": "after training, this neuron is naturally going to be there to detect cats. If we had changed the labeling scheme and I said",
    "start": "1217505",
    "end": "1224190"
  },
  {
    "text": "that the second entry would correspond to the cat, the presence of a cat, then after training,",
    "start": "1224190",
    "end": "1229980"
  },
  {
    "text": "you will detect that this neuron is responsible for detecting a cat. So the network is going to evolve depending on the way you label your dataset.",
    "start": "1229980",
    "end": "1237309"
  },
  {
    "text": "Now, do you think that this network can still be robust to different animals in the same picture?",
    "start": "1237310",
    "end": "1244770"
  },
  {
    "text": "So this cat now has, uh, a friend [NOISE] that is a lion.",
    "start": "1244770",
    "end": "1249975"
  },
  {
    "text": "Okay, I have no idea how to draw a lion, but let's say there is a lion here and because there is a lion,",
    "start": "1249975",
    "end": "1258285"
  },
  {
    "text": "I will add a 1 here. Do you think this network is robust to this type of labeling?",
    "start": "1258285",
    "end": "1263520"
  },
  {
    "text": "[BACKGROUND]",
    "start": "1263520",
    "end": "1277470"
  },
  {
    "text": "It should be. The neurons aren't talking to each other. That's a good answer actually. Another answer. [BACKGROUND]",
    "start": "1277470",
    "end": "1291659"
  },
  {
    "text": "That's a good, uh, intuition because the network, what it sees is just 1, 1, 0, and an image.",
    "start": "1291660",
    "end": "1297300"
  },
  {
    "text": "It doesn't see that this one corresponds to- the cat corresponds to the first one and the second- and the lion corresponds to the second one.",
    "start": "1297300",
    "end": "1304544"
  },
  {
    "text": "So [NOISE] this is a property of neural networks, it's the fact that you don't need to tell them everything.",
    "start": "1304545",
    "end": "1309945"
  },
  {
    "text": "If you have enough data, they're going to figure it out. So because you will have also cats with iguanas,",
    "start": "1309945",
    "end": "1315120"
  },
  {
    "text": "cats alone, lions with iguanas, lions alone, ultimately, this neuron will understand what it's looking for,",
    "start": "1315120",
    "end": "1322035"
  },
  {
    "text": "and it will understand that this one corresponds to this lion. Just needs a lot of data.",
    "start": "1322035",
    "end": "1328245"
  },
  {
    "text": "So yes, it's going to be robust. And that's the reason you mentioned. It's going to be robust to that because the three neurons aren't communicating together.",
    "start": "1328245",
    "end": "1337050"
  },
  {
    "text": "So we can totally train them independent- independently from each other. And in fact, the sigmoid here,",
    "start": "1337050",
    "end": "1343125"
  },
  {
    "text": "doesn't depend on the sigmoid here and doesn't depend on the sigmoid here. It means we can have one, one,",
    "start": "1343125",
    "end": "1348149"
  },
  {
    "text": "and one as an output. [NOISE] Yes, question. [BACKGROUND]",
    "start": "1348150",
    "end": "1355830"
  },
  {
    "text": "You could, you could, you could think about it as three logistic regressions. So we wouldn't call that a neural network yet.",
    "start": "1355830",
    "end": "1361695"
  },
  {
    "text": "It's not ready yet, but it's a three neural network or three logistic regression with each other.",
    "start": "1361695",
    "end": "1368940"
  },
  {
    "text": "[NOISE]. Now, following up on that, uh, yeah, go for it. A question. [BACKGROUND]",
    "start": "1368940",
    "end": "1378780"
  },
  {
    "text": "W and b are related to what? [BACKGROUND] Oh, yeah. Yeah. So, so usually you would have Theta transpose x,",
    "start": "1378780",
    "end": "1387764"
  },
  {
    "text": "which is sum of Theta_i_x_i, correct? And what I will split it is,",
    "start": "1387765",
    "end": "1394125"
  },
  {
    "text": "I will spit it in sum of Theta_i_x_i plus Theta_0 times 1.",
    "start": "1394125",
    "end": "1401205"
  },
  {
    "text": "I'll split it like that. Theta_0 would correspond to b and these Theta_is would correspond to w_is, make sense?",
    "start": "1401205",
    "end": "1408915"
  },
  {
    "text": "Okay. One more question and then we move on. [BACKGROUND]",
    "start": "1408915",
    "end": "1425930"
  },
  {
    "text": "Good question. That's the next thing we're going to see. So the question is a follow up on this,",
    "start": "1425930",
    "end": "1432350"
  },
  {
    "text": "is there cases where we have a constraint where there is only one possible outcome?",
    "start": "1432350",
    "end": "1439475"
  },
  {
    "text": "It means there is no cat and lion, there is either a cat or a lion, there is no iguana and lion,",
    "start": "1439475",
    "end": "1444710"
  },
  {
    "text": "there's either an iguana or a lion. Think about health care. There are many, there are many models that are made to detect,",
    "start": "1444710",
    "end": "1454745"
  },
  {
    "text": "uh, if a disease, skin disease is present on- based on cell microscopic images.",
    "start": "1454745",
    "end": "1460309"
  },
  {
    "text": "Usually, there is no overlap between these, it means, you want to classify a specific disease among a large number of diseases.",
    "start": "1460310",
    "end": "1467254"
  },
  {
    "text": "So this model would still work but would not be optimal because it's longer to train.",
    "start": "1467255",
    "end": "1472400"
  },
  {
    "text": "Maybe one disease is super, super rare and one of the neurons is never going to be trained. Let's say you're working in a zoo where there is",
    "start": "1472400",
    "end": "1479059"
  },
  {
    "text": "only one iguana and there are thousands of lions and thousands of cats. This guy will never train almost,",
    "start": "1479060",
    "end": "1485570"
  },
  {
    "text": "you know, it would be super hard to train this one. So you want to start with another model that- where you put the constraint, that's, okay,",
    "start": "1485570",
    "end": "1491465"
  },
  {
    "text": "there is only one disease that we want to predict and let the model learn, with all the neurons learn together by creating interaction between them.",
    "start": "1491465",
    "end": "1500855"
  },
  {
    "text": "Have you guys heard of softmax? Yes? Somebody, ah, I see that in the back.",
    "start": "1500855",
    "end": "1507140"
  },
  {
    "text": "[LAUGHTER] Okay. So let's look at softmax a little bit together. So we set a new goal now,",
    "start": "1507140",
    "end": "1513030"
  },
  {
    "text": "which is we add a constraint which is an unique animal on an image.",
    "start": "1515140",
    "end": "1527909"
  },
  {
    "text": "So at most one animal on an image. So I'm going to modify the network a little bit.",
    "start": "1530350",
    "end": "1538595"
  },
  {
    "text": "We're go- we have our cat and there is no lion on the image, we flatten it,",
    "start": "1538595",
    "end": "1544860"
  },
  {
    "text": "and now I'm going to use the same scheme with the three neurons, a_1, a_2, a_3.",
    "start": "1545110",
    "end": "1555390"
  },
  {
    "text": "But as an output, what I am going to use is an exponent, a softmax function.",
    "start": "1562330",
    "end": "1571475"
  },
  {
    "text": "So let me be more precise, let me, let me actually introduce another notation to make it easier.",
    "start": "1571475",
    "end": "1578805"
  },
  {
    "text": "As you know, the neuron is a linear part plus an activation.",
    "start": "1578805",
    "end": "1584090"
  },
  {
    "text": "So we are going to introduce a notation for the linear part,",
    "start": "1584090",
    "end": "1589505"
  },
  {
    "text": "I'm going to introduce Z_11 to represent the linear part of the first neuron.",
    "start": "1589505",
    "end": "1595680"
  },
  {
    "text": "Z_112 to introduce the linear part of the second neuron.",
    "start": "1596170",
    "end": "1603365"
  },
  {
    "text": "So now our neuron has two parts, one which computes Z and one which computes a, equals sigmoid of Z.",
    "start": "1603365",
    "end": "1609290"
  },
  {
    "text": "Now, I'm going to remove all the activations and make these Zs and I'm going to use the specific formula.",
    "start": "1609290",
    "end": "1621900"
  },
  {
    "text": "So this, if you recall, is exactly the softmax formula.",
    "start": "1643810",
    "end": "1650059"
  },
  {
    "text": "[NOISE] Yeah.",
    "start": "1650060",
    "end": "1653670"
  },
  {
    "text": "Okay. So now the network we have, can you guys see or it's too small? Too small?",
    "start": "1670090",
    "end": "1677764"
  },
  {
    "text": "Okay, I'm going to just write this formula bigger and then you can figure out the others, I guess, because,",
    "start": "1677765",
    "end": "1684395"
  },
  {
    "text": "e of Z_3, 1 divided by sum from",
    "start": "1684395",
    "end": "1690020"
  },
  {
    "text": "k equals 1 to 3 of e, exponential of ZK_1.",
    "start": "1690020",
    "end": "1695465"
  },
  {
    "text": "Okay, can you see this one? So here is for the third one.",
    "start": "1695465",
    "end": "1701570"
  },
  {
    "text": "If you are doing it for the first one, you will add- you'll just change this into a 2, into a 1 and for the second one into a 2.",
    "start": "1701570",
    "end": "1707975"
  },
  {
    "text": "So why is this formula interesting and why is it not robust to this labeling scheme anymore?",
    "start": "1707975",
    "end": "1713480"
  },
  {
    "text": "It's because the sum of the outputs of this network have to sum up to 1. You can try it.",
    "start": "1713480",
    "end": "1720679"
  },
  {
    "text": "If you sum the three outputs, you get the same thing in the numerator and on the denominator and you get 1. That makes sense?",
    "start": "1720680",
    "end": "1729945"
  },
  {
    "text": "So instead of getting a probabilistic output for each,",
    "start": "1729945",
    "end": "1736220"
  },
  {
    "text": "each of y, if, each of y hat 1, y hat 2, y hat 3, we will get a probability distribution over all the classes.",
    "start": "1736450",
    "end": "1744934"
  },
  {
    "text": "So that means we cannot get 0.7, 0.6, 0.1, telling us roughly that there is probably a cat and a lion but no iguana.",
    "start": "1744935",
    "end": "1753710"
  },
  {
    "text": "We have to sum these to 1. So it means, if there is no cat and no lion,",
    "start": "1753710",
    "end": "1759424"
  },
  {
    "text": "it means there's very likely an iguana. The three probabilities are dependent on each",
    "start": "1759425",
    "end": "1764570"
  },
  {
    "text": "other and for this one we have to label the following way,",
    "start": "1764570",
    "end": "1771605"
  },
  {
    "text": "1, 1, 0 for a cat, 0, 1, 0 for a lion or 0,",
    "start": "1771605",
    "end": "1776720"
  },
  {
    "text": "0, 1 for an iguana. So this is called a softmax multi-class network.",
    "start": "1776720",
    "end": "1786740"
  },
  {
    "text": "[inaudible].",
    "start": "1786740",
    "end": "1803929"
  },
  {
    "text": "You assume there is at least one of the three classes, otherwise you have to add a fourth input that will represent an absence of an animal.",
    "start": "1803930",
    "end": "1810904"
  },
  {
    "text": "But this way, you assume there is always one of these three animals on every picture.",
    "start": "1810905",
    "end": "1816960"
  },
  {
    "text": "And how many parameters does the network have? The same as the second one.",
    "start": "1821950",
    "end": "1828035"
  },
  {
    "text": "We still have three neurons and although I didn't write it, this Z_1 is equal to w_11,",
    "start": "1828035",
    "end": "1834095"
  },
  {
    "text": "x plus b_1, Z_2 same, Z_3 same. So there's 3n plus 3 parameters.",
    "start": "1834095",
    "end": "1841800"
  },
  {
    "text": "So one question that we didn't talk about is, how do we train these parameters?",
    "start": "1843640",
    "end": "1851010"
  },
  {
    "text": "These, these parameters, the 3n plus 3 parameters, how do we train them? You think this scheme will work or no?",
    "start": "1854560",
    "end": "1861665"
  },
  {
    "text": "What's wrong, what's wrong with this scheme?",
    "start": "1861665",
    "end": "1864420"
  },
  {
    "text": "What's wrong with the loss function specifically?",
    "start": "1866800",
    "end": "1871320"
  },
  {
    "text": "There's only two outcomes. So in this loss function, y is a number between 0 and 1,",
    "start": "1875410",
    "end": "1883144"
  },
  {
    "text": "y hat same is the probability, y is either a 0 or 1, y hat is between 0 and 1,",
    "start": "1883145",
    "end": "1888799"
  },
  {
    "text": "so it cannot match this labeling. So we need to modify the loss function.",
    "start": "1888800",
    "end": "1894390"
  },
  {
    "text": "So let's call it loss three neuron.",
    "start": "1894730",
    "end": "1899970"
  },
  {
    "text": "What I'm going to do is I'm going to just sum it up for the three neurons.",
    "start": "1900760",
    "end": "1908670"
  },
  {
    "text": "Does this make sense? So I am just doing three times this loss for each of the neurons.",
    "start": "1925220",
    "end": "1934095"
  },
  {
    "text": "So we have exactly three times this. We sum them together.",
    "start": "1934095",
    "end": "1939345"
  },
  {
    "text": "And if you train this loss function, you should be able to train the three neurons that you have.",
    "start": "1939345",
    "end": "1945540"
  },
  {
    "text": "And again, talking about scarcity of one of the classes. If there is not many iguana,",
    "start": "1945540",
    "end": "1951090"
  },
  {
    "text": "then the third term of this sum is not going to help this neuron train towards detecting an iguana.",
    "start": "1951090",
    "end": "1960435"
  },
  {
    "text": "It's going to push it to detect no iguana.",
    "start": "1960435",
    "end": "1963850"
  },
  {
    "text": "Any question on the loss function? Does this one make sense? Yeah? [inaudible]",
    "start": "1965930",
    "end": "1981810"
  },
  {
    "text": "Yeah. Usually, that's what will happen is that the output of this network once it's trained, is going to be a probability distribution.",
    "start": "1981810",
    "end": "1988169"
  },
  {
    "text": "You will pick the maximum of those, and you will set it to 1 and the others to 0 as your prediction. One more question, yeah.",
    "start": "1988170",
    "end": "1997890"
  },
  {
    "text": "[inaudible]",
    "start": "1997890",
    "end": "2009230"
  },
  {
    "text": "If you use the 2-1. If you use this labeling scheme like 1-1-0 for this network,",
    "start": "2009230",
    "end": "2015920"
  },
  {
    "text": "what do you think it will happen? It will probably not work.",
    "start": "2015920",
    "end": "2023150"
  },
  {
    "text": "And the reason is this sum is equal to 2, the sum of these entries,",
    "start": "2023150",
    "end": "2028400"
  },
  {
    "text": "while the sum of these entries is equal to 1. So you will never be able to match the output to the input to the label. That makes sense?",
    "start": "2028400",
    "end": "2036725"
  },
  {
    "text": "So what the network is probably going to do is it's probably going to send this one to one-half,",
    "start": "2036725",
    "end": "2041750"
  },
  {
    "text": "this one to one-half, and this one to 0 probably, which is not what you want.",
    "start": "2041750",
    "end": "2046050"
  },
  {
    "text": "Okay. Let's talk about the loss function for this softmax regression.",
    "start": "2047020",
    "end": "2052520"
  },
  {
    "text": "[NOISE] Because you know",
    "start": "2052520",
    "end": "2062360"
  },
  {
    "text": "what's interesting about this loss is if I take this derivative,",
    "start": "2062360",
    "end": "2067835"
  },
  {
    "text": "derivative of the loss 3N with respect to W2_1.",
    "start": "2067835",
    "end": "2074070"
  },
  {
    "text": "You think is going to be harder than this derivative, than this one or no?",
    "start": "2075430",
    "end": "2080915"
  },
  {
    "text": "It's going to be exactly the same. Because only one of these three terms depends on W12.",
    "start": "2080915",
    "end": "2086149"
  },
  {
    "text": "It means the derivative of the two others are 0. So we are exactly at the same complexity during the derivation.",
    "start": "2086150",
    "end": "2092855"
  },
  {
    "text": "But this one, do you think if you try to compute,",
    "start": "2092855",
    "end": "2098044"
  },
  {
    "text": "let's say we define a loss function that corresponds roughly to that. If you try to compute the derivative of the loss with respect to W2,",
    "start": "2098044",
    "end": "2105545"
  },
  {
    "text": "it will become much more complex. Because this number, the output here that is going to impact the loss function directly,",
    "start": "2105545",
    "end": "2114215"
  },
  {
    "text": "not only depends on the parameters of W2, it also depends on the parameters of W1 and W3.",
    "start": "2114215",
    "end": "2120065"
  },
  {
    "text": "And same for this output. This output also depends on the parameters W2. Does this makes sense? Because of this denominator.",
    "start": "2120065",
    "end": "2128420"
  },
  {
    "text": "So the softmax regression needs a different loss function and a different derivative.",
    "start": "2128420",
    "end": "2134359"
  },
  {
    "text": "So the loss function we'll define is a very common one in deep learning, it's called the softmax cross entropy.",
    "start": "2134360",
    "end": "2141620"
  },
  {
    "text": "Cross entropy loss.",
    "start": "2141620",
    "end": "2147120"
  },
  {
    "text": "I'm not going to- to- into the details of where it comes from but you can get the intuition of yklog.",
    "start": "2147550",
    "end": "2157230"
  },
  {
    "text": "So it, it surprisingly looks like the binary croissant,",
    "start": "2174400",
    "end": "2179615"
  },
  {
    "text": "the binary, uh, the logistic loss function. The only difference is that we will sum it up on all the- on all the classes.",
    "start": "2179615",
    "end": "2192270"
  },
  {
    "text": "Now, we will take a derivative of something that looks like that later. But I'd say if you can try it at home on this one,",
    "start": "2193240",
    "end": "2201530"
  },
  {
    "text": "uh, it would be a good exercise as well. So this binary cross entropy loss is very",
    "start": "2201530",
    "end": "2207890"
  },
  {
    "text": "likely to be used in classification problems that are multi-class.",
    "start": "2207890",
    "end": "2212789"
  },
  {
    "text": "Okay. So this was the first part on logistic regression types of networks.",
    "start": "2214860",
    "end": "2220345"
  },
  {
    "text": "And I think we're ready now with the notation that we introduced to jump on to neural networks.",
    "start": "2220345",
    "end": "2226484"
  },
  {
    "text": "Any question on this first part before we move on?",
    "start": "2226485",
    "end": "2230130"
  },
  {
    "text": "So one- one question I would have for you. Let's say instead of trying to predict if there is a cat or no cat,",
    "start": "2234060",
    "end": "2242260"
  },
  {
    "text": "we were trying to predict the age of the cat based on the image. What would you change? This network.",
    "start": "2242260",
    "end": "2252150"
  },
  {
    "text": "Instead of predicting 1-0, you wanna predict the age of the cat.",
    "start": "2252150",
    "end": "2256460"
  },
  {
    "text": "What are the things you would change? Yes.",
    "start": "2257680",
    "end": "2264650"
  },
  {
    "text": "[inaudible].",
    "start": "2264650",
    "end": "2273890"
  },
  {
    "text": "Okay. So I repeat. I, I basically make several output nodes where each of them corresponds to one age of cats.",
    "start": "2273890",
    "end": "2282230"
  },
  {
    "text": "So would you use this network or the third one? Would you use the three neurons neural network or the softmax regression?",
    "start": "2282230",
    "end": "2290930"
  },
  {
    "text": "Third one. The third one. Why? You have a unique age. You have a unique age,",
    "start": "2290930",
    "end": "2296735"
  },
  {
    "text": "you cannot have two ages, right. So we would use a softmax one because we want the probability distribution along the edge, the ages.",
    "start": "2296735",
    "end": "2304880"
  },
  {
    "text": "Okay. That makes sense. That's a good approach. There is also another approach which is using directly a regression to predict an age.",
    "start": "2304880",
    "end": "2313775"
  },
  {
    "text": "An age can be between zero and plus infi- not plus infinity- [LAUGHTER]. -zero and a certain number.",
    "start": "2313775",
    "end": "2319609"
  },
  {
    "text": "[LAUGHTER] And, uh, so let's say you wanna do a regression,",
    "start": "2319610",
    "end": "2325955"
  },
  {
    "text": "how would you modify your network? Change the Sigmoid.",
    "start": "2325955",
    "end": "2331280"
  },
  {
    "text": "The Sigmoid puts the Z between 0 and 1. We don't want this to happen. So I'd say we will change the Sigmoid.",
    "start": "2331280",
    "end": "2337580"
  },
  {
    "text": "Into what function would you change the Sigmoid? [inaudible]",
    "start": "2337580",
    "end": "2350600"
  },
  {
    "text": "Yeah. So the second one you said was? [inaudible] Oh, to get a Poisson type of distribution.",
    "start": "2350600",
    "end": "2356765"
  },
  {
    "text": "Okay. So let's, let's go with linear. You mentioned linear. We could just use a linear function,",
    "start": "2356765",
    "end": "2363409"
  },
  {
    "text": "right, for the Sigmoid. But this becomes a linear regression. The whole network becomes a linear regression.",
    "start": "2363409",
    "end": "2369800"
  },
  {
    "text": "Another one that is very common in, in deep learning is called the Rayleigh function. It's a function that is almost linear,",
    "start": "2369800",
    "end": "2376100"
  },
  {
    "text": "but for every input that is negative, it's equal to 0. Because we cannot have negative h,",
    "start": "2376100",
    "end": "2381905"
  },
  {
    "text": "it makes sense to use this one. Okay. So this is called rectified linear units, ReLU.",
    "start": "2381905",
    "end": "2391295"
  },
  {
    "text": "It's a very common one in deep learning. Now, what else would you change? We talked about linear regression.",
    "start": "2391295",
    "end": "2398150"
  },
  {
    "text": "Do you remember the loss function you are using in linear regression? What was it? [inaudible]",
    "start": "2398150",
    "end": "2406040"
  },
  {
    "text": "It was probably one of these two; y hat minus y. This comparison between the output label and y-hat,",
    "start": "2406040",
    "end": "2414455"
  },
  {
    "text": "the prediction, or it was the L2 loss; y-hat minus y in L2 norm.",
    "start": "2414455",
    "end": "2420244"
  },
  {
    "text": "So that's what we would use. We would modify our loss function to fit the regression type of problem.",
    "start": "2420245",
    "end": "2425525"
  },
  {
    "text": "And the reason we would use this loss instead of the one we have for a regression task is because in optimization,",
    "start": "2425525",
    "end": "2433039"
  },
  {
    "text": "the shape of this loss is much easier to optimize for a regression task than it is for a classification task and vice versa.",
    "start": "2433040",
    "end": "2439460"
  },
  {
    "text": "I'm not going to go into the details of that but that's the intuition. [NOISE] Okay.",
    "start": "2439460",
    "end": "2445040"
  },
  {
    "text": "Let's go have fun with neural networks. [NOISE]",
    "start": "2445040",
    "end": "2471800"
  },
  {
    "text": "So we, we stick to our first goal. Given an image, tell us if there is cat or no cat.",
    "start": "2471800",
    "end": "2484320"
  },
  {
    "text": "This is 1, this is 0. But now we're going to make a network a little more complex.",
    "start": "2484600",
    "end": "2490340"
  },
  {
    "text": "We're going to add some parameters. So I get my picture of the cat. [NOISE] The cat is moving.",
    "start": "2490340",
    "end": "2500520"
  },
  {
    "text": "Okay. And what I'm going to do is that I'm going to put more neurons than before.",
    "start": "2503890",
    "end": "2509819"
  },
  {
    "text": "Maybe something like that. [NOISE]",
    "start": "2510270",
    "end": "2542380"
  },
  {
    "text": "So using the same notation you see that",
    "start": "2542380",
    "end": "2544220"
  },
  {
    "text": "my square bracket- So using the same notation, you see that my square bracket here is two",
    "start": "2554220",
    "end": "2560480"
  },
  {
    "text": "indicating that there is a layer here which is the second layer [NOISE] while this one is the first layer and this one is the third layer.",
    "start": "2560480",
    "end": "2574700"
  },
  {
    "text": "Everybody's, er, up to speed with the notations? Cool. So now notice that when you make a choice of architecture,",
    "start": "2576120",
    "end": "2586945"
  },
  {
    "text": "you have to be careful of one thing, is that the output layer has to have the same number of neurons as you",
    "start": "2586945",
    "end": "2594790"
  },
  {
    "text": "want, the number of classes to be for reclassification and one for a regression.",
    "start": "2594790",
    "end": "2601040"
  },
  {
    "text": "So, er, how many parameters does this - this network have?",
    "start": "2605160",
    "end": "2611005"
  },
  {
    "text": "Can someone quickly give me the thought process?",
    "start": "2611005",
    "end": "2614900"
  },
  {
    "text": "So how much here?",
    "start": "2616530",
    "end": "2619580"
  },
  {
    "text": "Yeah, like 3n plus 3 let's say. [inaudible].",
    "start": "2621570",
    "end": "2639910"
  },
  {
    "text": "Yeah, correct. So here you would have 3n weights plus 3 biases.",
    "start": "2639910",
    "end": "2645670"
  },
  {
    "text": "Here you would have 2 times 3 weights plus 2 biases because you have",
    "start": "2645670",
    "end": "2650829"
  },
  {
    "text": "three neurons connected to two neurons and here you will have 2 times 1 plus 1 bias.",
    "start": "2650830",
    "end": "2656500"
  },
  {
    "text": "Makes sense. So this is the total number of parameters. So you see that we didn't add too much parameters.",
    "start": "2656500",
    "end": "2663205"
  },
  {
    "text": "Most of the parameters are still in the input layer. Um, let's define some vocabulary.",
    "start": "2663205",
    "end": "2671005"
  },
  {
    "text": "The first word is Layer. Layer denotes neurons that are not connected to each other.",
    "start": "2671005",
    "end": "2676465"
  },
  {
    "text": "These two neurons are not connected to each other. These two neurons are not connected to each other. We call this cluster of neurons a layer.",
    "start": "2676465",
    "end": "2682570"
  },
  {
    "text": "And then this has three layers. So we would use input layer to define the first layer,",
    "start": "2682570",
    "end": "2688880"
  },
  {
    "text": "output layer to define the third layer because it directly sees the outputs and we would call the second layer a hidden layer.",
    "start": "2688880",
    "end": "2697450"
  },
  {
    "text": "And the reason we call it hidden is because the inputs and the outputs are hidden from this layer.",
    "start": "2698910",
    "end": "2704980"
  },
  {
    "text": "It means the only thing that this layer sees as input is what's the previous layer gave it.",
    "start": "2704980",
    "end": "2710995"
  },
  {
    "text": "So it's an abstraction of the inputs but it's not the inputs. Does that make sense? And same, it doesn't see the output,",
    "start": "2710995",
    "end": "2719335"
  },
  {
    "text": "it just gives what it understood to the last neuron that will compare the output to the ground truth.",
    "start": "2719335",
    "end": "2726250"
  },
  {
    "text": "So now, why are neural networks interesting? And why do we call this hidden layer?",
    "start": "2726250",
    "end": "2731755"
  },
  {
    "text": "Um, it's because if you train this network on cat classification with a lot of images of cats,",
    "start": "2731755",
    "end": "2739585"
  },
  {
    "text": "you would notice that the first layers are going to understand the fundamental concepts of the image,",
    "start": "2739585",
    "end": "2745810"
  },
  {
    "text": "which is the edges. This neuron is going to be able to detect this type of edges.",
    "start": "2745810",
    "end": "2752080"
  },
  {
    "text": "This neuron is probably going to detect some other type of edge. This neuron, maybe this type of edge.",
    "start": "2752080",
    "end": "2759235"
  },
  {
    "text": "Then what's gonna happen, is that these neurons are going to communicate what they found on the image to the next layer's neuron.",
    "start": "2759235",
    "end": "2765385"
  },
  {
    "text": "And this neuron is going to use the edges that these guys found to figure out that, oh, there is a - their ears.",
    "start": "2765385",
    "end": "2772810"
  },
  {
    "text": "While this one is going to figure out, oh, there is a mouth and so on if you have",
    "start": "2772810",
    "end": "2778000"
  },
  {
    "text": "several neurons and they're going to communicate what they understood to the output neuron that is going to construct the face of",
    "start": "2778000",
    "end": "2784900"
  },
  {
    "text": "the cat based on what it received and be able to tell if there is a cat or not. So the reason it's called hidden layer is because we - we",
    "start": "2784900",
    "end": "2793210"
  },
  {
    "text": "don't really know what it's going to figure out but with enough data, it should understand very complex information about the data.",
    "start": "2793210",
    "end": "2799690"
  },
  {
    "text": "The deeper you go, the more complex information the neurons are able to understand. Let me give you another example which is a house prediction example.",
    "start": "2799690",
    "end": "2809755"
  },
  {
    "text": "House price prediction. [NOISE]",
    "start": "2809755",
    "end": "2833650"
  },
  {
    "text": "So let's assume that our inputs are number of bedrooms,",
    "start": "2833650",
    "end": "2839900"
  },
  {
    "text": "size of the house, zip code, and wealth of the neighborhood, let's say.",
    "start": "2840540",
    "end": "2852050"
  },
  {
    "text": "What we will build is a network that has three neurons in the first layer and one neuron in the output layer.",
    "start": "2852050",
    "end": "2860240"
  },
  {
    "text": "So what's interesting is that as a human if you were to build, uh,",
    "start": "2860820",
    "end": "2865825"
  },
  {
    "text": "this network and like hand engineer it, you would say that, uh, okay zip codes and wealth or - or sorry.",
    "start": "2865825",
    "end": "2874480"
  },
  {
    "text": "Let's do that. Zip code and wealth are able to tell us about the school quality in the neighborhood.",
    "start": "2874480",
    "end": "2883645"
  },
  {
    "text": "The quality of the school that is next to the house probably.",
    "start": "2883645",
    "end": "2890155"
  },
  {
    "text": "As a human you would say these are probably good features to predict that. The zip code is going to tell us if the neighborhood is walkable or not, probably.",
    "start": "2890155",
    "end": "2903110"
  },
  {
    "text": "The size and the number of bedrooms is going to tell us what's the size of the family that can fit in this house.",
    "start": "2903360",
    "end": "2913015"
  },
  {
    "text": "And these three are probably better information than these in order to finally predict the price.",
    "start": "2913015",
    "end": "2920125"
  },
  {
    "text": "So that's a way to hand engineer that by hand, as a human in order",
    "start": "2920125",
    "end": "2925180"
  },
  {
    "text": "to give human knowledge to the network to figure out the price.",
    "start": "2925180",
    "end": "2931119"
  },
  {
    "text": "In practice what we do here is that we use a fully-connected layer - fully-connected.",
    "start": "2931120",
    "end": "2942640"
  },
  {
    "text": "What does that mean? It means that we connect every input of a layer,",
    "start": "2942640",
    "end": "2948055"
  },
  {
    "text": "every - every input to the first layer, every output of the first layer to the input of the third layer and so on.",
    "start": "2948055",
    "end": "2955075"
  },
  {
    "text": "So all the neurons among lay - from one layer to another are connected with each other.",
    "start": "2955075",
    "end": "2960145"
  },
  {
    "text": "What we're saying is that we will let the network figure these out. We will let the neurons of the first layer figure out",
    "start": "2960145",
    "end": "2967630"
  },
  {
    "text": "what's interesting for the second layer to make the price prediction. So we will not tell these to the network,",
    "start": "2967630",
    "end": "2973255"
  },
  {
    "text": "instead we will fully connect the network [NOISE] and so on.",
    "start": "2973255",
    "end": "2981019"
  },
  {
    "text": "Okay. We'll fully connect the network and let it figure out what are the interesting features.",
    "start": "2981300",
    "end": "2987820"
  },
  {
    "text": "And oftentimes, the network is going to be able better than the humans to find these - what are the features that are representative.",
    "start": "2987820",
    "end": "2993835"
  },
  {
    "text": "Sometimes you may hear neural networks referred as, uh, black box models.",
    "start": "2993835",
    "end": "2999744"
  },
  {
    "text": "The reason is we will not understand what this edge will correspond to. It's - it's hard to figure out that this neuron is",
    "start": "2999745",
    "end": "3007680"
  },
  {
    "text": "detecting a weighted average of the input features. Does that make sense?",
    "start": "3007680",
    "end": "3014770"
  },
  {
    "text": "Another word you might hear is end-to-end learning. The reason we talked about end-to-end learning is because we have an input,",
    "start": "3016610",
    "end": "3025605"
  },
  {
    "text": "a ground truth, and we don't constrain the network in the middle.",
    "start": "3025605",
    "end": "3030780"
  },
  {
    "text": "We let it learn whatever it has to learn and we call it end-to-end learning because we are just training based on the input and the output.",
    "start": "3030780",
    "end": "3037530"
  },
  {
    "text": "[NOISE]",
    "start": "3037530",
    "end": "3075600"
  },
  {
    "text": "Let's delve more into the math of this network. The neural network that we have here which has an input layer,",
    "start": "3075600",
    "end": "3082320"
  },
  {
    "text": "a hidden layer and an output layer. Let's try to write down the equations that run the inputs and pro - propagate it to the output.",
    "start": "3082320",
    "end": "3091420"
  },
  {
    "text": "We first have Z_1, that is the linear part of the first layer,",
    "start": "3091670",
    "end": "3096805"
  },
  {
    "text": "that is computed using W_1 times x plus b_1.",
    "start": "3096805",
    "end": "3103050"
  },
  {
    "text": "Then this Z_1 is given to an activation, let's say sigmoid, which is sigmoid of Z_1.",
    "start": "3104000",
    "end": "3112560"
  },
  {
    "text": "Z_2 is then the linear part of the second neuron which is going to",
    "start": "3112560",
    "end": "3118920"
  },
  {
    "text": "take the output of the previous layer, multiply it by its weights and add the bias.",
    "start": "3118920",
    "end": "3127210"
  },
  {
    "text": "The second activation is going to take the sigmoid of Z_2.",
    "start": "3127310",
    "end": "3133210"
  },
  {
    "text": "And finally, we have the third layer which is going to multiply its weights,",
    "start": "3134270",
    "end": "3140425"
  },
  {
    "text": "with the output of the layer presenting it and add its bias.",
    "start": "3140425",
    "end": "3146760"
  },
  {
    "text": "And finally, we have the third activation which is simply the sigmoid of the three.",
    "start": "3146760",
    "end": "3154390"
  },
  {
    "text": "So what is interesting to notice between these equations and the equations that we wrote here,",
    "start": "3158230",
    "end": "3167280"
  },
  {
    "text": "is that we put everything in matrices. So it means this a_3 that I have here, sorry,",
    "start": "3167860",
    "end": "3177500"
  },
  {
    "text": "this here for three neurons I wrote three equations, here for three neurons",
    "start": "3177500",
    "end": "3185630"
  },
  {
    "text": "in the second layer I just wrote a single equation to summarize it. But the shape of these things are going to be vectors.",
    "start": "3185630",
    "end": "3193160"
  },
  {
    "text": "So let's go over the shapes, let's try to define them. Z_1 is going to be x which is n by 1 times",
    "start": "3193160",
    "end": "3201980"
  },
  {
    "text": "w which has to be 3 by n because it connects three neurons to the input.",
    "start": "3201980",
    "end": "3209435"
  },
  {
    "text": "So this z has to be 3 by 1. It makes sense because we have three neurons.",
    "start": "3209435",
    "end": "3216780"
  },
  {
    "text": "Now let's go, let's go deeper. A_1 is just the sigmoid of z_1 so it doesn't change the shape.",
    "start": "3217270",
    "end": "3225260"
  },
  {
    "text": "It keeps the 3 by 1. Z_2 we know it,",
    "start": "3225260",
    "end": "3230330"
  },
  {
    "text": "it has to be 2 by 1 because there are two neurons in the second layer and it helps us figure out what w_2 would be.",
    "start": "3230330",
    "end": "3238280"
  },
  {
    "text": "We know a_1 is 3 by 1. It means that w_2 has to be 2 by 3.",
    "start": "3238280",
    "end": "3244460"
  },
  {
    "text": "And if you count the edges between the first and the second layer here you will find six edges, 2 times 3.",
    "start": "3244460",
    "end": "3252890"
  },
  {
    "text": "A_2, same shape as z_2. Z_3, 1 by 1,",
    "start": "3252890",
    "end": "3258155"
  },
  {
    "text": "a_3, 1 by 1, w_3, it has to be 1 by 2,",
    "start": "3258155",
    "end": "3263809"
  },
  {
    "text": "because a_2 is 2 by 1 and same for b. B is going to be the number of neurons so 3 by 1,",
    "start": "3263810",
    "end": "3272704"
  },
  {
    "text": "2 by 1, and finally 1 by 1. So I think it's usually very helpful,",
    "start": "3272705",
    "end": "3279710"
  },
  {
    "text": "even when coding these type of equations, to know all the shapes that are involved. Are you guys, like, totally okay with the shapes,",
    "start": "3279710",
    "end": "3287569"
  },
  {
    "text": "super-easy to figure out? Okay, cool. So now what is interesting is that we will try to vectorize the code even more.",
    "start": "3287570",
    "end": "3299000"
  },
  {
    "text": "Does someone remember the difference between stochastic gradient descent and gradient descent. What's the difference?",
    "start": "3299000",
    "end": "3306170"
  },
  {
    "text": "[inaudible]",
    "start": "3306170",
    "end": "3314299"
  },
  {
    "text": "Exactly. Stochastic gradient descent is updates, the weights and the bias after you see every example.",
    "start": "3314300",
    "end": "3321785"
  },
  {
    "text": "So the direction of the gradient is quite noisy. It doesn't represent very well the entire batch,",
    "start": "3321785",
    "end": "3327215"
  },
  {
    "text": "while gradient descent or batch gradient descent is updates after you've seen the whole batch of examples.",
    "start": "3327215",
    "end": "3333755"
  },
  {
    "text": "And the gradient is much more precise. It points to the direction you want to go to.",
    "start": "3333755",
    "end": "3339960"
  },
  {
    "text": "So what we're trying to do now is to write down these equations if",
    "start": "3340630",
    "end": "3348019"
  },
  {
    "text": "instead of giving one single cat image we had given a bunch of images that either have a cat or not a cat.",
    "start": "3348020",
    "end": "3354950"
  },
  {
    "text": "So now our input x. So what happens for",
    "start": "3354950",
    "end": "3366890"
  },
  {
    "text": "an input batch of m examples?",
    "start": "3366890",
    "end": "3376140"
  },
  {
    "text": "So now our x is not anymore a single column vector,",
    "start": "3380860",
    "end": "3388399"
  },
  {
    "text": "it's a matrix with the first image corresponding to x_1,",
    "start": "3388399",
    "end": "3394520"
  },
  {
    "text": "the second image corresponding to x_2 and so on until the nth image corresponding to x_n.",
    "start": "3394520",
    "end": "3402455"
  },
  {
    "text": "And I'm introducing a new notation which is the parentheses superscript corresponding to the ID of the example.",
    "start": "3402455",
    "end": "3413610"
  },
  {
    "text": "So square brackets for the layer, round brackets for the idea of the example we are talking about.",
    "start": "3415750",
    "end": "3425010"
  },
  {
    "text": "So just to give more context on what we're trying to do. We know that this is a bunch of operations.",
    "start": "3425490",
    "end": "3432474"
  },
  {
    "text": "We just have a, a network with inputs, hidden, and output layer.",
    "start": "3432475",
    "end": "3437510"
  },
  {
    "text": "We could have a network with 1,000 layer. The more layers we have the more computation and it quickly goes up.",
    "start": "3437510",
    "end": "3444694"
  },
  {
    "text": "So what we wanna do is to be able to parallelize our code or, or our computation as much as possible by giving",
    "start": "3444695",
    "end": "3451250"
  },
  {
    "text": "batches of inputs and parallelizing these equations. So let's see how these equations are modified when we give it a batch of m inputs.",
    "start": "3451250",
    "end": "3459599"
  },
  {
    "text": "I will use capital letters to denote the equivalent of the lowercase letters but for a batch of input.",
    "start": "3460810",
    "end": "3471860"
  },
  {
    "text": "So Z_1 as an example would be W_1,",
    "start": "3471860",
    "end": "3477455"
  },
  {
    "text": "let's use the same actually, W_1 times X plus B_1.",
    "start": "3477455",
    "end": "3483890"
  },
  {
    "text": "So let's analyze what Z_1 would look like. Z_1 we know that for every,",
    "start": "3483890",
    "end": "3491190"
  },
  {
    "text": "for every input example of the batch we will get one Z_1 which should look like this.",
    "start": "3491350",
    "end": "3499050"
  },
  {
    "text": "Then we have to figure out what have to be the shapes of this equation in order to end up with this.",
    "start": "3508960",
    "end": "3514385"
  },
  {
    "text": "We know that Z_1 was 3 by 1. It mean, it means capital Z_1 has to be 3 by",
    "start": "3514385",
    "end": "3522950"
  },
  {
    "text": "m because each of these column vectors are 3 by 1 and we have m of them.",
    "start": "3522950",
    "end": "3530105"
  },
  {
    "text": "Because for each input we forward propagate through the network, we get these equations. So for the first cat image we get these equations,",
    "start": "3530105",
    "end": "3536975"
  },
  {
    "text": "for the second cat image we get again equations like that and so on.",
    "start": "3536975",
    "end": "3541710"
  },
  {
    "text": "So what is the shape of x? We have it above. We know that it's n by n. What is the shape of w_1?",
    "start": "3546340",
    "end": "3557735"
  },
  {
    "text": "It didn't change. W_1 doesn't change. It's not because I will give 1,000 inputs to",
    "start": "3557735",
    "end": "3563660"
  },
  {
    "text": "my network that the parameters are going to be more. So the parameter number stays the same even if I give more inputs.",
    "start": "3563660",
    "end": "3571865"
  },
  {
    "text": "And so this has to be 3 by n in order to match Z_1. Now the interesting thing is that there is an algebraic problem here.",
    "start": "3571865",
    "end": "3582830"
  },
  {
    "text": "What is the algebraic problem? We said that the number of parameters doesn't change.",
    "start": "3582830",
    "end": "3587910"
  },
  {
    "text": "It means that w has the same shape as it has before, as it had before.",
    "start": "3588100",
    "end": "3594545"
  },
  {
    "text": "B should have the same shape as it had before, right? It should be 3 by 1. What's the problem of this equation?",
    "start": "3594545",
    "end": "3603690"
  },
  {
    "text": "Exactly. We're summing a 3 by m matrix to a 3 by 1 vector.",
    "start": "3606190",
    "end": "3614569"
  },
  {
    "text": "This is not possible in math. It doesn't work. It doesn't match. When you do some summations or subtraction,",
    "start": "3614570",
    "end": "3620885"
  },
  {
    "text": "you need the two terms to be the same shape because you will do an element-wise addition or an ele- element-wise subtraction.",
    "start": "3620885",
    "end": "3629030"
  },
  {
    "text": "So what's the trick that is used here? It's a, it's a technique called broadcasting.",
    "start": "3629030",
    "end": "3634470"
  },
  {
    "text": "Broadcasting is that- is the fact that we don't want to change the number of parameters, it should stay the same.",
    "start": "3641230",
    "end": "3647945"
  },
  {
    "text": "But we still want this operation to be able to be written in parallel version.",
    "start": "3647945",
    "end": "3653750"
  },
  {
    "text": "So we still want to write this equation because we want to parallelize our code, but we don't want to add more parameters, it doesn't make sense.",
    "start": "3653750",
    "end": "3659885"
  },
  {
    "text": "So what we're going to do is that we are going to create a vector b tilde",
    "start": "3659885",
    "end": "3665045"
  },
  {
    "text": "1 which is going to be b_1 repeated three times.",
    "start": "3665045",
    "end": "3671525"
  },
  {
    "text": "Sorry, repeated m times.",
    "start": "3671525",
    "end": "3675000"
  },
  {
    "text": "So we just keep the same number of parameters but just repeat them in order to be able to write my code in parallel.",
    "start": "3683030",
    "end": "3691780"
  },
  {
    "text": "This is called broadcasting. And what is convenient is that for those of you who, uh, the homeworks are in Matlab or Python?",
    "start": "3691940",
    "end": "3700635"
  },
  {
    "text": "Matlab. Okay. So in Matlab, no Python? [LAUGHTER]. [NOISE] Thank you. Um, Python. So in Python,",
    "start": "3700635",
    "end": "3708990"
  },
  {
    "text": "there is a package that is often used to to code these equations. It's numPy. Some people call it numPy, I'm not sure why.",
    "start": "3708990",
    "end": "3715830"
  },
  {
    "text": "So numPy, basically numerical Python,",
    "start": "3715830",
    "end": "3721250"
  },
  {
    "text": "we directly do the broadcasting. It means if you sum this 3 by m matrix with a 3 by 1 parameter vector,",
    "start": "3721250",
    "end": "3732105"
  },
  {
    "text": "it's going to automatically reproduce the parameter vector m times so that the equation works. It's called broadcasting. Does that make sense?",
    "start": "3732105",
    "end": "3740325"
  },
  {
    "text": "So because we're using this technique, we're able to rewrite all these equations with capital letters.",
    "start": "3740325",
    "end": "3747105"
  },
  {
    "text": "Do you wanna do it together or do you want to do it on your own? Who wants to do it on their own?",
    "start": "3747105",
    "end": "3754780"
  },
  {
    "text": "Okay. So let's do it on their own [LAUGHTER] on your own.",
    "start": "3755690",
    "end": "3760905"
  },
  {
    "text": "So rewrite these with capital letters and figure out the shapes. I think you can do it at home, wherever,",
    "start": "3760905",
    "end": "3766980"
  },
  {
    "text": "we're not going to do here, but make sure you understand all the shapes. Yeah. [inaudible] How how is this",
    "start": "3766980",
    "end": "3776550"
  },
  {
    "text": "[inaudible]?",
    "start": "3776550",
    "end": "3785700"
  },
  {
    "text": "So the question is how is this different from principal component analysis? This is a supervised learning algorithm that",
    "start": "3785700",
    "end": "3792119"
  },
  {
    "text": "will be used to predict the price of a house. Principal component analysis doesn't predict anything.",
    "start": "3792120",
    "end": "3797445"
  },
  {
    "text": "It gets an input matrix X normalizes it, ah, computes the covariance matrix and then figures out what are",
    "start": "3797445",
    "end": "3805200"
  },
  {
    "text": "the pri- principal components by doing the the eigenvalue decomposition. But the outcome of PCA is,",
    "start": "3805200",
    "end": "3811994"
  },
  {
    "text": "you know that the most important features of your dataset X are going to be these features.",
    "start": "3811995",
    "end": "3819090"
  },
  {
    "text": "Here we're not looking at the features. We're only looking at the output. That is what is important to us. Yes.",
    "start": "3819090",
    "end": "3826600"
  },
  {
    "text": "In the first lecture when did you say that the first layers is the edges in an [inaudible].",
    "start": "3828170",
    "end": "3838140"
  },
  {
    "text": "So the question is, can you explain why the first layer would see the edges? Is there an intuition behind it? It's not always going to see the edges,",
    "start": "3838140",
    "end": "3845205"
  },
  {
    "text": "but it's oftentimes going to see edges because um, in order to detect a human face,",
    "start": "3845205",
    "end": "3851340"
  },
  {
    "text": "let's say, you will train an algorithm to find out whose face it is. So it has to understand the faces very well.",
    "start": "3851340",
    "end": "3857715"
  },
  {
    "text": "Um, you need the network to be complex enough to understand very detailed features of the face.",
    "start": "3857715",
    "end": "3863010"
  },
  {
    "text": "And usually, this neuron, what it sees as input are pixels.",
    "start": "3863010",
    "end": "3869040"
  },
  {
    "text": "So it means every edge here is the multiplication of the weight by a pixel.",
    "start": "3869040",
    "end": "3874560"
  },
  {
    "text": "So it sees pixels. It cannot understand the face as a whole because it sees only pixels.",
    "start": "3874560",
    "end": "3882255"
  },
  {
    "text": "It's very granular information for it. So it's going to check if pixels nearby have",
    "start": "3882255",
    "end": "3888420"
  },
  {
    "text": "the same color and understand that there is an edge there, okay? But it's too complicated to understand the whole face in the first layer.",
    "start": "3888420",
    "end": "3895890"
  },
  {
    "text": "However, if it understands a little more than a pixel information, it can give it to the next neuron.",
    "start": "3895890",
    "end": "3902055"
  },
  {
    "text": "This neuron will receive more than pixel information. It would receive a little more complex-like edges,",
    "start": "3902055",
    "end": "3909060"
  },
  {
    "text": "and then it will use this information to build on top of it and build the features of the face.",
    "start": "3909060",
    "end": "3914235"
  },
  {
    "text": "So what I'm trying to sum up is that these neurons only see the pixels, so they're not able to build more than the edges.",
    "start": "3914235",
    "end": "3919800"
  },
  {
    "text": "That's the minimum thing that they can- the maximum thing they can do. And it's it's a complex topic,",
    "start": "3919800",
    "end": "3925785"
  },
  {
    "text": "like interpretation of neural network is a highly researched topic, it's a big research topic. So nobody figured out exactly how all the neurons evolve.",
    "start": "3925785",
    "end": "3936165"
  },
  {
    "text": "Yeah. One more question and then we move on.",
    "start": "3936165",
    "end": "3940000"
  },
  {
    "text": "Ah, how [inaudible].",
    "start": "3943970",
    "end": "3951359"
  },
  {
    "text": "So the question is how [OVERLAPPING]. [inaudible]. -how do you decide how many neurons per layer? How many layers?",
    "start": "3951360",
    "end": "3957330"
  },
  {
    "text": "What's the architecture of your neural network? There are two things to take into consideration I would say. First and nobody knows the right answer, so you have to test it.",
    "start": "3957330",
    "end": "3965190"
  },
  {
    "text": "So you you guys talked about training set, validation set, and test set. So what we would do is,",
    "start": "3965190",
    "end": "3971400"
  },
  {
    "text": "we would try ten different architectures, train it, train the network on these,",
    "start": "3971400",
    "end": "3976605"
  },
  {
    "text": "looking at the validation set accuracy of all these, and decide which one seems to be the best. That's how we figure out what's the right network size.",
    "start": "3976605",
    "end": "3984300"
  },
  {
    "text": "On top of that, using experience is often valuable. So if you give me a problem,",
    "start": "3984300",
    "end": "3989864"
  },
  {
    "text": "I try always to gauge how complex is the problem. Like cat classification, do you",
    "start": "3989864",
    "end": "3996990"
  },
  {
    "text": "think it's easier or harder than day and night classification? So day and night classification is I give you an image,",
    "start": "3996990",
    "end": "4003215"
  },
  {
    "text": "I asked you to predict if it was taken during the day or during the night, and on the other hand you want there's a cat on the image or not.",
    "start": "4003215",
    "end": "4009175"
  },
  {
    "text": "Which one is easier, which one is harder?",
    "start": "4009175",
    "end": "4012020"
  },
  {
    "text": "Who thinks cat classification is harder? Okay. I think people agree.",
    "start": "4014650",
    "end": "4020465"
  },
  {
    "text": "Cat classification seems harder, why? Because there are many breeds of cats. Can look like different things.",
    "start": "4020465",
    "end": "4025760"
  },
  {
    "text": "There's not many breeds of nights. um, I guess. [LAUGHTER] Um, one thing that might be challenging in the day and night classification,",
    "start": "4025760",
    "end": "4032975"
  },
  {
    "text": "is if you want also to figure it out in house like i- inside, you know maybe there is a tiny window there and I'm able to tell that is the day",
    "start": "4032975",
    "end": "4042005"
  },
  {
    "text": "but for a network to understand it you will need a lot more data than if only you wanted to work outside, different.",
    "start": "4042005",
    "end": "4047570"
  },
  {
    "text": "So these problems all have their own complexity. Based on their complexity, I think the network should be deeper.",
    "start": "4047570",
    "end": "4054080"
  },
  {
    "text": "The comp- the more complex usually is the problem, the more data you need in order to figure out the output,",
    "start": "4054080",
    "end": "4059390"
  },
  {
    "text": "the more deeper should be the network. That's an intuition, let's say. Okay. Let's move on guys because I think we have about what, 12 more minutes?",
    "start": "4059390",
    "end": "4070290"
  },
  {
    "text": "Okay. Let's try to write the loss function",
    "start": "4077440",
    "end": "4082260"
  },
  {
    "text": "for this problem. [NOISE].",
    "start": "4082720",
    "end": "4099020"
  },
  {
    "text": "So now that we have our network, we have written this propagation equation and I we call it forward propagation,",
    "start": "4099020",
    "end": "4106430"
  },
  {
    "text": "because it's going forward, it's going from the input to the output. Later on when we will, we will derive these equations,",
    "start": "4106430",
    "end": "4113690"
  },
  {
    "text": "we will call them backward propagation, because we are starting from the loss and going backwards.",
    "start": "4113690",
    "end": "4119359"
  },
  {
    "text": "So let's let's talk about the optimization problem. Optimizing w_1, w_2, w_3, b_1, b_2, b_3.",
    "start": "4119360",
    "end": "4137270"
  },
  {
    "text": "We have a lot of stuff to optimize, right? We have to find the right values for these and remember model equals architecture plus parameter.",
    "start": "4137270",
    "end": "4143210"
  },
  {
    "text": "We have our architecture, if we have our parameters we're done. So in order to do that, we have to define an objective function.",
    "start": "4143210",
    "end": "4153029"
  },
  {
    "text": "Sometimes called loss, sometimes called cost function.",
    "start": "4153040",
    "end": "4157740"
  },
  {
    "text": "So usually we would call it loss if there is only one example in the batch,",
    "start": "4158350",
    "end": "4163625"
  },
  {
    "text": "and cost, if there is multiple examples in a batch.",
    "start": "4163625",
    "end": "4170310"
  },
  {
    "text": "So the loss function that, let- let's define the cost function.",
    "start": "4172300",
    "end": "4178085"
  },
  {
    "text": "The cost function J depends on y hat n_y. Okay. So y hat,",
    "start": "4178085",
    "end": "4186154"
  },
  {
    "text": "y hat is a_3. Okay. It depends on y hat n_y,",
    "start": "4186155",
    "end": "4197525"
  },
  {
    "text": "and we will set it to be the sum of the loss functions L_i,",
    "start": "4197525",
    "end": "4205820"
  },
  {
    "text": "and I will normalize it. It's not mandatory, but normalize it with 1/n. So what does this mean?",
    "start": "4205820",
    "end": "4214910"
  },
  {
    "text": "It's that we are going for batch gradient descent. We wanna compute the loss function for the whole batch, parallelize our code,",
    "start": "4214910",
    "end": "4223485"
  },
  {
    "text": "and then calculate the cost function that will be then derived to give us the direction of the gradients.",
    "start": "4223485",
    "end": "4231665"
  },
  {
    "text": "That is, the average direction of all the de-de- derivation with respect to the whole input batch.",
    "start": "4231665",
    "end": "4238520"
  },
  {
    "text": "And L_i will be the loss function corresponding to one parameter.",
    "start": "4238520",
    "end": "4245945"
  },
  {
    "text": "So what's the error on this specific one input, sorry not parameter, and it will be the logistic loss.",
    "start": "4245945",
    "end": "4254940"
  },
  {
    "text": "You've already seen these equations, I believe. So now, is it more complex to take",
    "start": "4268750",
    "end": "4276890"
  },
  {
    "text": "a derivative with respect to J like of J with respect to the parameters or of L?",
    "start": "4276890",
    "end": "4282390"
  },
  {
    "text": "What's the most complex between this one, let's say we're taking the derivative with respect to w_2, compared to this one?",
    "start": "4282390",
    "end": "4293699"
  },
  {
    "text": "Which one is the hardest? Who thinks J is the hardest?",
    "start": "4298660",
    "end": "4307460"
  },
  {
    "text": "Who think it doesn't matter? Yeah, it doesn't matter because derivation is is a linear operation, right?",
    "start": "4307500",
    "end": "4316385"
  },
  {
    "text": "So you can just take the derivative inside and you will see that if you know this, you just have to take the sum over this.",
    "start": "4316385",
    "end": "4323675"
  },
  {
    "text": "So instead of computing all derivatives on J, we will com- compute them on L, but it's totally equivalent.",
    "start": "4323675",
    "end": "4329960"
  },
  {
    "text": "There's just one more step at the end. Okay. So now we defined our loss function, super.",
    "start": "4329960",
    "end": "4338850"
  },
  {
    "text": "We defined our loss function and the next step is optimize. So we have to compute a lot of derivatives. [NOISE]",
    "start": "4339610",
    "end": "4361699"
  },
  {
    "text": "And that's called backward propagation. [NOISE] So the question",
    "start": "4361700",
    "end": "4373190"
  },
  {
    "text": "is why is it called backward propagation? It's because what we want to do ultimately is this.",
    "start": "4373190",
    "end": "4379655"
  },
  {
    "text": "For any l equals 1-3,",
    "start": "4379655",
    "end": "4385264"
  },
  {
    "text": "we want to do that, wl equals wl minus Alpha derivative of j with respect to wl,",
    "start": "4385265",
    "end": "4401110"
  },
  {
    "text": "and bl equals bl minus Alpha derivative of j with respect to bl.",
    "start": "4401110",
    "end": "4408920"
  },
  {
    "text": "So we want to do that for every parameter in layer 1, 2, and 3.",
    "start": "4409000",
    "end": "4415910"
  },
  {
    "text": "So it means, we have to compute all these derivatives, we have to compute derivative of the cost with respect to w1,",
    "start": "4415910",
    "end": "4421970"
  },
  {
    "text": "w2, w3, b1, b2, b3. You've done it with logistic regression,",
    "start": "4421970",
    "end": "4427114"
  },
  {
    "text": "we're going to do it with a neural network, and you're going to understand why it's called backward propagation.",
    "start": "4427115",
    "end": "4432335"
  },
  {
    "text": "Which one do you want to start with? Which derivative? You wanna start with the derivative with respect to w1,",
    "start": "4432335",
    "end": "4438590"
  },
  {
    "text": "w2, or w3, let's say. Assuming we'll do the bias later. W what?",
    "start": "4438590",
    "end": "4446225"
  },
  {
    "text": "W1? You think w1 is a good idea. I do- don't wanna do w1.",
    "start": "4446225",
    "end": "4452610"
  },
  {
    "text": "I think we should do w3, and the reason is because if you look at this loss function,",
    "start": "4453280",
    "end": "4461070"
  },
  {
    "text": "do you think the relation between w3 and this loss function is easier to understand or",
    "start": "4462490",
    "end": "4468230"
  },
  {
    "text": "the relation between w1 and this loss function? It's the relation between w3 and this loss function.",
    "start": "4468230",
    "end": "4474380"
  },
  {
    "text": "Because w3 happens much later in the- in the network. So if you want to understand how much should we move w1 in order to make the loss move?",
    "start": "4474380",
    "end": "4482239"
  },
  {
    "text": "It's much more complicated than answering the question how much should w3 move to move the loss.",
    "start": "4482240",
    "end": "4487655"
  },
  {
    "text": "Because there's much more connections if you wanna compete with w1.",
    "start": "4487655",
    "end": "4492920"
  },
  {
    "text": "So that's why we call it backward propagation is because we will start with the top layer, the one that's the closest to the loss function,",
    "start": "4492920",
    "end": "4498740"
  },
  {
    "text": "derive the derivative of j with respect to w1.",
    "start": "4498740",
    "end": "4507900"
  },
  {
    "text": "Once we've computed this derivative which we are going to do next week,",
    "start": "4508600",
    "end": "4514520"
  },
  {
    "text": "once we computed this number, we can then tackle this one.",
    "start": "4514520",
    "end": "4519660"
  },
  {
    "text": "Oh, sorry. Yeah. Thanks. Yeah. Once we computed this number,",
    "start": "4522460",
    "end": "4528065"
  },
  {
    "text": "we will be able to compute this one very easily. Why very easily?",
    "start": "4528065",
    "end": "4533795"
  },
  {
    "text": "Because we can use the chain rule of calculus. So let's see how it works. What we're- I'm just going to give you, uh,",
    "start": "4533795",
    "end": "4540815"
  },
  {
    "text": "the one-minute pitch on- on backprop, but, uh, we'll do it next week together. So if we had to compute this derivative,",
    "start": "4540815",
    "end": "4548165"
  },
  {
    "text": "what I will do is that I will separate it into several derivatives that are easier.",
    "start": "4548165",
    "end": "4553310"
  },
  {
    "text": "I will separate it into the derivative of j with respect to something, with the something, with respect to w3.",
    "start": "4553310",
    "end": "4560825"
  },
  {
    "text": "And the question is, what should this something be? I will look at my equations.",
    "start": "4560825",
    "end": "4567275"
  },
  {
    "text": "I know that j depends on Y-hat, and I know that Y-hat depends on z3.",
    "start": "4567275",
    "end": "4573680"
  },
  {
    "text": "Y-hat is the same thing as a3, I know it depends on z3. So why don't- why don't I include z3 in my equation?",
    "start": "4573680",
    "end": "4581165"
  },
  {
    "text": "I also know that z3 depends on w3, and the derivative of z3 with respect to w2 is super easy,",
    "start": "4581165",
    "end": "4586760"
  },
  {
    "text": "it's just a2 transpose. So I will just make a quick hack and say that",
    "start": "4586760",
    "end": "4592820"
  },
  {
    "text": "this derivative is the same as taking it with respect to a3,",
    "start": "4592820",
    "end": "4597949"
  },
  {
    "text": "taking derivative of a3 with respect to z3, and taking the derivative of z3 with respect to w3.",
    "start": "4597950",
    "end": "4609455"
  },
  {
    "text": "So you see? Same, same derivative, calculated in different ways.",
    "start": "4609455",
    "end": "4615050"
  },
  {
    "text": "And I know this, I know these are pretty easy to compute.",
    "start": "4615050",
    "end": "4621510"
  },
  {
    "text": "So that's why we call it backpropagation, it's because I will use the chain rule to compute the derivative of w3,",
    "start": "4621510",
    "end": "4627620"
  },
  {
    "text": "and then when I want to do it for w2, I'm going to insert, I'm going to insert the derivative with z3 times the derivative of",
    "start": "4627620",
    "end": "4639740"
  },
  {
    "text": "z3 with respect to a2 times the derivative of a2 with respect to z2,",
    "start": "4639740",
    "end": "4648290"
  },
  {
    "text": "and derivative of z2 with respect to w2.",
    "start": "4648290",
    "end": "4653435"
  },
  {
    "text": "Does this make sense that this thing here is the same thing as this?",
    "start": "4653435",
    "end": "4661980"
  },
  {
    "text": "It means, if I wanna compute the derivative of w2, I don't need to compute this anymore,",
    "start": "4662890",
    "end": "4668165"
  },
  {
    "text": "I already did this for w3. I just need to compute those which are easy ones, and so on.",
    "start": "4668165",
    "end": "4674244"
  },
  {
    "text": "If I wanna compute the derivative of j with respect to w1,",
    "start": "4674245",
    "end": "4679870"
  },
  {
    "text": "I'm going to- I'm not going to decompose all the thing again, I'm just going to take the derivative of j with respect to",
    "start": "4679870",
    "end": "4686320"
  },
  {
    "text": "z2 which is equal to this whole thing. And then I'm gonna multiply it by the derivative of",
    "start": "4686320",
    "end": "4693080"
  },
  {
    "text": "z2 with respect to a1 times derivative of a1 with respect to z1 times the derivative of z1 with respect to w1.",
    "start": "4693080",
    "end": "4706040"
  },
  {
    "text": "And again, this thing I know it already, I computed it previously just for this one.",
    "start": "4706040",
    "end": "4712350"
  },
  {
    "text": "So what's, what's interesting about it is that I'm not gonna redo the work I did,",
    "start": "4712870",
    "end": "4718010"
  },
  {
    "text": "I'm just gonna store the right values while back-propagating and continue to derivate. One thing that you need to notice though is that, look,",
    "start": "4718010",
    "end": "4726905"
  },
  {
    "text": "you need this forward propagation equation in order to remember what should be the path to take in",
    "start": "4726905",
    "end": "4733010"
  },
  {
    "text": "your chain rule because you know that this derivative of j with respect to w3,",
    "start": "4733010",
    "end": "4739175"
  },
  {
    "text": "I cannot use it as it is because w3 is not connected to the previous layer. If you look at this equation,",
    "start": "4739175",
    "end": "4745760"
  },
  {
    "text": "a2 doesn't depend on w3, it depends on z3. Sorry, like, uh, my bad,",
    "start": "4745760",
    "end": "4752255"
  },
  {
    "text": "it depends- no, sorry, what I wanted to say is that z2 is connected to w2,",
    "start": "4752255",
    "end": "4760354"
  },
  {
    "text": "but a1 is not connected to w2. So you wanna choose the path that you're going",
    "start": "4760354",
    "end": "4768320"
  },
  {
    "text": "through in the proper way so that there's no cancellation in these derivatives.",
    "start": "4768320",
    "end": "4773494"
  },
  {
    "text": "You- you cannot compute derivative of w2 with",
    "start": "4773495",
    "end": "4779150"
  },
  {
    "text": "respect to- to a1, right?",
    "start": "4779150",
    "end": "4785074"
  },
  {
    "text": "You cannot compute that, you don't know it. Okay. So I think we're done for today.",
    "start": "4785075",
    "end": "4791585"
  },
  {
    "text": "So one thing that I'd like you to do if you have time is just think about the things that can be tweaked in a neural network.",
    "start": "4791585",
    "end": "4798860"
  },
  {
    "text": "When you build a neural network, you are not done, you have to tweak it, you have to tweak the activations, you have to tweak the loss function.",
    "start": "4798860",
    "end": "4805310"
  },
  {
    "text": "There's many things you can tweak, and that's what we're going to see next week. Okay. Thanks.",
    "start": "4805310",
    "end": "4810059"
  }
]