[
  {
    "start": "0",
    "end": "28000"
  },
  {
    "text": "OK. Hello, everyone. Welcome to this lecture. So this is Huaxiu I'm a postdoc in\nChelsea's lab.",
    "start": "5490",
    "end": "11750"
  },
  {
    "text": "Today, I will give a lecture\nabout domain generalization. So before we start,\nI will first go",
    "start": "11750",
    "end": "18650"
  },
  {
    "text": "through some logistical things. So the project milestone\nis on Wednesday, and the homework four, which\nis a optional homework,",
    "start": "18650",
    "end": "26180"
  },
  {
    "text": "is due on Monday. Here is the plan for today. Today, I will introduce a\nnew concept called the domain",
    "start": "26180",
    "end": "32970"
  },
  {
    "start": "28000",
    "end": "200000"
  },
  {
    "text": "generalization. I will first do some problem\nstatement and problem formulation, and then\nintroduce two kinds",
    "start": "32970",
    "end": "39410"
  },
  {
    "text": "of representative algorithms\nas the first one is to adding explicit regularizers\nto solve this domain",
    "start": "39410",
    "end": "45260"
  },
  {
    "text": "generalization problem,\nand the second one is we can leverage\nsome data augmentation to handle this problem.",
    "start": "45260",
    "end": "52470"
  },
  {
    "text": "And the goal for this\nlecture is that I will first let you know how to understand\nthe intuition behind the domain",
    "start": "52470",
    "end": "59370"
  },
  {
    "text": "generalization and this\nproblem formulation, and I will also see that you can\nbe familiar with the mainstream",
    "start": "59370",
    "end": "65463"
  },
  {
    "text": "domain generalization approach. So the regularization-based\napproach or our augmentation-based\napproach.",
    "start": "65464",
    "end": "73650"
  },
  {
    "text": "So let's first do some recap\nabout the domain augmentation that we learned from\nour last lecture. So in the domain\nadaptation, we aim",
    "start": "73650",
    "end": "80299"
  },
  {
    "text": "to use some training data\nfrom the source domains and to make it perform\nwell on the target domain.",
    "start": "80300",
    "end": "85610"
  },
  {
    "text": "This is a kind of a form\nof transfer learning, but we can also access\nthe target domain",
    "start": "85610",
    "end": "90920"
  },
  {
    "text": "data during the\ntraining process, so it is transductive learning. So basically, we have\nthe trained domain data",
    "start": "90920",
    "end": "98360"
  },
  {
    "text": "and the unlabeled\ntarget domain data, and we want to perform\nwell on the target domain.",
    "start": "98360",
    "end": "103840"
  },
  {
    "text": "So there are two assumptions\nfor the domain adaptation, and the first one\nis a source domain",
    "start": "103840",
    "end": "109530"
  },
  {
    "text": "and a target\ndomain-- only differ in domain of the function. So it means the\ncondition distribution py",
    "start": "109530",
    "end": "115890"
  },
  {
    "text": "given x is the same between\nthe source and target domain. And there are also exist\na single hypothesis",
    "start": "115890",
    "end": "122610"
  },
  {
    "text": "with a low error on both\nsource and target domains. We also do some revisiting\nto see that a domain is",
    "start": "122610",
    "end": "130440"
  },
  {
    "text": "a special case of task. So a task is composed\nof three components.",
    "start": "130440",
    "end": "135460"
  },
  {
    "text": "The first one is pi x. It's for the margin distribution\nor feature distribution, and then y is for\nthe distribution",
    "start": "135460",
    "end": "142290"
  },
  {
    "text": "of the domain of the function,\nthe py given x and also the loss function. So for one task that we\nlearned from the multitask",
    "start": "142290",
    "end": "149760"
  },
  {
    "text": "learning or meta-learning--\nall these three components can be changed from\ndifferent tasks. But in the domain,\nonly the px can",
    "start": "149760",
    "end": "157140"
  },
  {
    "text": "change over different domains. So before we start--\nso then we will",
    "start": "157140",
    "end": "163650"
  },
  {
    "text": "mention that can we always\naccess unlabeled data from the target domain?",
    "start": "163650",
    "end": "170190"
  },
  {
    "text": "However, in some real\nworld applications, because of the following two\nreasons that we cannot always",
    "start": "170190",
    "end": "175370"
  },
  {
    "text": "assess this target domain. The first one is\nsometimes we want to do real time deployment,\nand we do not have time",
    "start": "175370",
    "end": "182480"
  },
  {
    "text": "to collect enough\ntarget domain data and to do the adaptation,\nincluding domain adaptation",
    "start": "182480",
    "end": "188390"
  },
  {
    "text": "or collect the label data to\ndo some meta-learning or fusion learning adaptation, and in the\ncase that obtaining the target",
    "start": "188390",
    "end": "196250"
  },
  {
    "text": "data may be restricted\nby the privacy policy.",
    "start": "196250",
    "end": "198845"
  },
  {
    "start": "200000",
    "end": "232000"
  },
  {
    "text": "And I would provide\nsome examples for each of these reasons. The first one is for the\nreal-time deployment,",
    "start": "201960",
    "end": "207810"
  },
  {
    "text": "and in this case,\nfor example, if we want to change an\nautonomous driving system,",
    "start": "207810",
    "end": "213370"
  },
  {
    "text": "and we want to let\nthese models trained on three types of\nroads, and then",
    "start": "213370",
    "end": "218730"
  },
  {
    "text": "we want to deploy this model\nto a new road-- for example, we want to deploy this on\nthe night, and in this case,",
    "start": "218730",
    "end": "226390"
  },
  {
    "text": "we want to do some\nreal-time deployment, and we do not have sufficient\ntime to collect enough data.",
    "start": "226390",
    "end": "233280"
  },
  {
    "start": "232000",
    "end": "284000"
  },
  {
    "text": "And another case is about\nthe-- another example is about the privacy concerns. So there are a lot of policies\nabout the privacy concerns.",
    "start": "233280",
    "end": "243640"
  },
  {
    "text": "So for example,\nthe most famous one is about General Data\nProtection Regulation,",
    "start": "243640",
    "end": "249510"
  },
  {
    "text": "which is from Europe. And in this case, we cannot\nshare the data between different institutions or\nhospitals or any other stuff.",
    "start": "249510",
    "end": "257458"
  },
  {
    "text": "And for example, if we\nwant to build a disease predictive model-- so this disease\npredictive model we",
    "start": "257459",
    "end": "264060"
  },
  {
    "text": "trained on the three hospitals. For example, hospital\none, two, three, and then",
    "start": "264060",
    "end": "269460"
  },
  {
    "text": "we did want to deploy this\nmodel to a new hospital. And in this case, to deploy\nthis model to a new hospital,",
    "start": "269460",
    "end": "276800"
  },
  {
    "text": "we cannot access the training\ndata due to some privacy concerns.",
    "start": "276800",
    "end": "280415"
  },
  {
    "start": "284000",
    "end": "333000"
  },
  {
    "text": "Based on these two things, our\nfirst-- to mention why we need domain generalization and first\nto give us some formal problem",
    "start": "284860",
    "end": "292090"
  },
  {
    "text": "formulations for this. In domain\ngeneralization, so assume",
    "start": "292090",
    "end": "298440"
  },
  {
    "text": "we have a bunch\nof source domains. For example, there are\nthree domains-- a clipart, painting, and a sketch.",
    "start": "298440",
    "end": "304470"
  },
  {
    "text": "We want to recognize different\nobjects for each domain, and we will train a model.",
    "start": "304470",
    "end": "310800"
  },
  {
    "text": "And to get such a\nneural network to see that we will train the model. And then we will\ndeploy this model",
    "start": "310800",
    "end": "317130"
  },
  {
    "text": "to some unseen target domains. Here is a domain\nfor the real image.",
    "start": "317130",
    "end": "322600"
  },
  {
    "text": "And we want to generalize\nsome common knowledge from these source domains and to\nmake those models perform well",
    "start": "322600",
    "end": "329070"
  },
  {
    "text": "on this target domain.",
    "start": "329070",
    "end": "330120"
  },
  {
    "start": "333000",
    "end": "644000"
  },
  {
    "text": "Mathematically, the domain\ngeneralization problem could be formulated as we given\na bunch of source domains--",
    "start": "334358",
    "end": "340060"
  },
  {
    "text": "p1 x1 to pn x1. We're aiming to solve\nunseen target domain, pt xy,",
    "start": "340060",
    "end": "345220"
  },
  {
    "text": "without accessing\nthe data from it. So they are-- similarly,\nthere are also",
    "start": "345220",
    "end": "352610"
  },
  {
    "text": "two common assumptions. The first one is all\ndomains only differ in domain of the function.",
    "start": "352610",
    "end": "359010"
  },
  {
    "text": "It means that the\nconstitutional distribution py given x is the same\nacross different domains.",
    "start": "359010",
    "end": "365010"
  },
  {
    "text": "In our previous definition\nof domain adaptation, we will make the\nconditional distributions",
    "start": "365010",
    "end": "371310"
  },
  {
    "text": "to be the same between the\nsource and target domain. And but here, it would be-- is exactly the same across all\ndomains, so only px can change.",
    "start": "371310",
    "end": "381790"
  },
  {
    "text": "So another assumption\nis that they are is a single hypothesis\nwith low error in all domains.",
    "start": "381790",
    "end": "388570"
  },
  {
    "text": "So that-- which\nguarantees that we can learn a model that can\nperform well across different",
    "start": "388570",
    "end": "393655"
  },
  {
    "text": "domains. OK. So this is also elevating\nfor the domain--",
    "start": "393655",
    "end": "399740"
  },
  {
    "text": "the special case our task. So it's also how [INAUDIBLE]\ndomain generalization problem.",
    "start": "399740",
    "end": "403930"
  },
  {
    "text": "Based on this\ndefinition, I will try to do some comparison\nbetween the meta-learning",
    "start": "407220",
    "end": "412259"
  },
  {
    "text": "that we learned in a lot of\nlectures before and the domain generalization.",
    "start": "412260",
    "end": "417569"
  },
  {
    "text": "And in meta-learning, it\nis like transfer learning, and we want to transfer some\nknowledge with many source",
    "start": "417570",
    "end": "424620"
  },
  {
    "text": "tasks. So given the data\nfrom task 1 to n, we want to solve a new task\nt more quickly, proficiently,",
    "start": "424620",
    "end": "432300"
  },
  {
    "text": "and stably. And in domain\ngeneralization, this is a special case\nof the meta-learning",
    "start": "432300",
    "end": "439629"
  },
  {
    "text": "that we learned before. So given the data\nfrom domain d1 to dn, we aim to perform well\nunder the new domain dt.",
    "start": "439630",
    "end": "448780"
  },
  {
    "text": "So but there are two\nkinds of difference between the domain\ngeneralization and the meta-learning. The first one is only pi x\nchanges across different tasks,",
    "start": "448780",
    "end": "457380"
  },
  {
    "text": "but in the meta-learning, we\ncan change a lot of things-- so about three components.",
    "start": "457380",
    "end": "463199"
  },
  {
    "text": "And in domain\ngeneralization, it's we want to directly\ngeneralize to the new domain,",
    "start": "463200",
    "end": "468750"
  },
  {
    "text": "instead of doing any\nkinds of adaptation.",
    "start": "468750",
    "end": "471450"
  },
  {
    "text": "Yeah. This is basically the\ncomparison between meta-learning and domain generalization.",
    "start": "474980",
    "end": "480320"
  },
  {
    "text": "The second comparison is\nbetween the domain adaptation that we learned from our\nlast lecture and domain",
    "start": "482920",
    "end": "488610"
  },
  {
    "text": "generalization. In the domain adaptation,\nwe can access some--",
    "start": "488610",
    "end": "495880"
  },
  {
    "text": "we can access label data\nfrom all-source domains, and we can also use unlabeled\ndata from the target domain,",
    "start": "495880",
    "end": "502639"
  },
  {
    "text": "so we aim to make the\nmodel perform well on the target domain. And in this case,\nthe targeted data--",
    "start": "502640",
    "end": "509170"
  },
  {
    "text": "we can access the target data\nduring the training process. So we can access the unlabeled\ndata, and which also there",
    "start": "509170",
    "end": "516219"
  },
  {
    "text": "is still some information\nof the target domain. And in domain adaptation, there\nare only one source domains",
    "start": "516220",
    "end": "524209"
  },
  {
    "text": "typically, but people can\nalso use more source domains, but we can only rely\non one source domains",
    "start": "524210",
    "end": "529580"
  },
  {
    "text": "to achieve success\ndomain adaptation. And the model trained\nfor the adaptation",
    "start": "529580",
    "end": "536360"
  },
  {
    "text": "is only specialized\nfor the target domain. So it means, we only care\nabout the performance",
    "start": "536360",
    "end": "542150"
  },
  {
    "text": "at the specific target\ndomains that we can access, instead of considering this\nproblem on a more general case",
    "start": "542150",
    "end": "548763"
  },
  {
    "text": "on a bunch of domains. So domain adaptation is a one\nkind of transductive learning",
    "start": "548763",
    "end": "556625"
  },
  {
    "text": "setting. In domain generalization,\nthese things",
    "start": "556625",
    "end": "561910"
  },
  {
    "text": "would be a little bit different. So in domain generalization--\nso given labeled data from a set",
    "start": "561910",
    "end": "568000"
  },
  {
    "text": "of source terms, py x1-- p1 xy to pn xy. And we aim to make\nthe model perform",
    "start": "568000",
    "end": "574839"
  },
  {
    "text": "well on the target domain--\na bunch of target domain actually. So we cannot access the test\ndata during the training",
    "start": "574840",
    "end": "582550"
  },
  {
    "text": "process, so this is\nthe first difference. And the second one is domain\ntranslation usually needs",
    "start": "582550",
    "end": "589330"
  },
  {
    "text": "more than one source domain. So if we only-- when you have\na bunch of source domains, you can capture the common\nknowledge behind these source",
    "start": "589330",
    "end": "596710"
  },
  {
    "text": "domains and then to generalize\nthis common knowledge to benefit the performance\nof the target domain.",
    "start": "596710",
    "end": "603250"
  },
  {
    "text": "And the third difference\nis that these models can be applied to all domains,\nincluding the source domains",
    "start": "605860",
    "end": "612250"
  },
  {
    "text": "and target domains\nand even some domains that we didn't captured. So this is why we needed to lend\nsome well-generalized knowledge",
    "start": "612250",
    "end": "621390"
  },
  {
    "text": "and can be generalized\nto a bunch of domains.",
    "start": "621390",
    "end": "624210"
  },
  {
    "text": "Yes. This is the key difference\nbetween the domain adaptation and the domain generalization. In this lecture, we want to\nmention-- domain generalization",
    "start": "629060",
    "end": "636579"
  },
  {
    "text": "is an inductive setting.",
    "start": "636580",
    "end": "637855"
  },
  {
    "text": "OK. So based on this definition\nand based on some comparison,",
    "start": "643650",
    "end": "650370"
  },
  {
    "start": "644000",
    "end": "776000"
  },
  {
    "text": "I would like to show you\nsome real world applications of domain generalization.",
    "start": "650370",
    "end": "655860"
  },
  {
    "text": "So first application is about\nsustainability and the two. We want to do some\nwildlife recognition.",
    "start": "655860",
    "end": "663060"
  },
  {
    "text": "We would like to recognize\ndifferent animals on different locations. So here, we will use--",
    "start": "663060",
    "end": "669240"
  },
  {
    "text": "for them here, we\nhave 245 locations, and we want to train a\nmodel for these locations",
    "start": "669240",
    "end": "675600"
  },
  {
    "text": "and to generalize these\nmodels to new locations.",
    "start": "675600",
    "end": "679290"
  },
  {
    "text": "Under the exam policy,\nyou may familiar with because in\nthe last lecture, we also see the adaptation\nversion of this tissue",
    "start": "682530",
    "end": "691080"
  },
  {
    "text": "classification\nexample, so we aim to classify one tissue\nimage, whether it's normal",
    "start": "691080",
    "end": "697500"
  },
  {
    "text": "or is tumor. So basically, we learn a model\nfrom a bunch of hospitals,",
    "start": "697500",
    "end": "703510"
  },
  {
    "text": "and then we train this\nmodel to some new hospitals. Here is hospital\n4 and hospital 5.",
    "start": "703510",
    "end": "710880"
  },
  {
    "text": "And in the domain adaptation,\ninstead, we probably only have two hospitals, and we want\nto learn some common knowledge",
    "start": "710880",
    "end": "716370"
  },
  {
    "text": "from these two hospitals. The third application is\nmolecule property prediction.",
    "start": "716370",
    "end": "723380"
  },
  {
    "text": "The molecule property prediction\nis quite important in the drug discovery field, so they want to\npredict some toxic [AUDIO OUT]",
    "start": "723380",
    "end": "732110"
  },
  {
    "text": "of giving a small molecule\nthat predicts the toxic. And we train a model\non different states",
    "start": "732110",
    "end": "738050"
  },
  {
    "text": "of these molecules and\ndifferent scale force. And then we aim to generalize\nsome common knowledge",
    "start": "738050",
    "end": "745340"
  },
  {
    "text": "and to make it work on\nsome unseen scale force.",
    "start": "745340",
    "end": "750570"
  },
  {
    "text": "And the last one is\nabout code competition. This is also very\nimportant applications",
    "start": "750570",
    "end": "756600"
  },
  {
    "text": "for implications in the field\nof programming language. So we have a bunch of\nreports, and then we",
    "start": "756600",
    "end": "762750"
  },
  {
    "text": "train a model to\npredict the next tokens in the context of\nsource code, and then",
    "start": "762750",
    "end": "769530"
  },
  {
    "text": "we aim to generalize this model\nto some test distribution.",
    "start": "769530",
    "end": "773056"
  },
  {
    "start": "776000",
    "end": "981000"
  },
  {
    "text": "OK. So this is the\nbasic introduction of what is domain generalization\nand do some comparison",
    "start": "777440",
    "end": "784450"
  },
  {
    "text": "and give some applications. Next, I will introduce\nsome specific algorithms",
    "start": "784450",
    "end": "790600"
  },
  {
    "text": "for domain generalization. The first one is we aim to\nadd some explicit regularizers",
    "start": "790600",
    "end": "797200"
  },
  {
    "text": "to handle this problem. So before we dive into\nthe specific algorithm, so",
    "start": "797200",
    "end": "804470"
  },
  {
    "text": "let's rethink one problem-- how to learn such agendas\nabout representations.",
    "start": "804470",
    "end": "810180"
  },
  {
    "text": "And to answer this\nquestion, a natural way is we want to think\nanother problem is",
    "start": "810180",
    "end": "815520"
  },
  {
    "text": "about why do machine learning\nmodels fail to generalize?",
    "start": "815520",
    "end": "819090"
  },
  {
    "text": "And here is one\nvery simple example. So our goal is to\nclassify dog versus cat.",
    "start": "823420",
    "end": "829300"
  },
  {
    "text": "So there are two domains. So domain one is water and\nthen domain two is grass. And we actually\nhave four groups.",
    "start": "829300",
    "end": "838579"
  },
  {
    "text": "So the first one is dog in the\nwater and a cat in the water, and dog in grass\nand a cat in grass.",
    "start": "838580",
    "end": "845259"
  },
  {
    "text": "So for dog in the water and\nthe cat in grass, two majority classes--",
    "start": "845260",
    "end": "850870"
  },
  {
    "text": "two majority groups. And otherwise, cat in the\nwater and dog in grass",
    "start": "850870",
    "end": "856360"
  },
  {
    "text": "are minority groups. Based on this, we train a model\non this two source domains.",
    "start": "856360",
    "end": "862839"
  },
  {
    "text": "And the cat has\ntrained the model, and then they train the model\nto a new domain-- for example,",
    "start": "862840",
    "end": "868150"
  },
  {
    "text": "as a dog in the forest. And our question\nis, is this a dog?",
    "start": "868150",
    "end": "874870"
  },
  {
    "text": "A human can easily\nrecognize this as dog, but for computer\nit is very hard.",
    "start": "874870",
    "end": "881440"
  },
  {
    "text": "So usually in combination,\nit's very easy to recognize, but in this case, it's hard\nto recognize this is a dog.",
    "start": "881440",
    "end": "888100"
  },
  {
    "text": "And the computer will\nmake the wrong prediction. So why is this happened?",
    "start": "888100",
    "end": "894573"
  },
  {
    "text": "So there are some similarities\nin the training data. We can see the dog. In the training data,\nthe dog is usually",
    "start": "894573",
    "end": "900550"
  },
  {
    "text": "in the water, and the\ncat usually in grass. And then it sees some\nsimilar environments",
    "start": "900550",
    "end": "907390"
  },
  {
    "text": "compared with grass, and then-- so grass with\nspurious information.",
    "start": "907390",
    "end": "914030"
  },
  {
    "text": "It means that these\nmodels will spuriously associate the cat information\nand the grass information.",
    "start": "914030",
    "end": "923180"
  },
  {
    "text": "And that's when the computer\nsee some similar environments, it will make wrong prediction.",
    "start": "923180",
    "end": "930610"
  },
  {
    "text": "So our goal is to cancel out\nsuch spurious information here.",
    "start": "930610",
    "end": "935700"
  },
  {
    "text": "And to do this, we aim to\ntrain a neural network to learn",
    "start": "935700",
    "end": "941760"
  },
  {
    "text": "some domain environments. So this is another concept\nI want to mention here.",
    "start": "941760",
    "end": "948460"
  },
  {
    "text": "So the domain\nenvironment is that we want to learn a\nbunch of features",
    "start": "948460",
    "end": "954510"
  },
  {
    "text": "via the neural network\nthat don't change across different domains.",
    "start": "954510",
    "end": "957975"
  },
  {
    "text": "In case that we can learn such\ndomain environment information, for example, we can associate\nthe animals with the labels,",
    "start": "962390",
    "end": "969770"
  },
  {
    "text": "and the computer can make\nthe right prediction.",
    "start": "969770",
    "end": "972440"
  },
  {
    "text": "OK. Yes, this is something\nabout domain environments, and based on this definition,\nI will send details",
    "start": "977670",
    "end": "987185"
  },
  {
    "start": "981000",
    "end": "1113000"
  },
  {
    "text": "of regularization based method. The key idea of\nregularization based methods",
    "start": "987185",
    "end": "993750"
  },
  {
    "text": "is that we want to\nuse a regularizer to align representations\nacross different domains.",
    "start": "993750",
    "end": "999435"
  },
  {
    "text": "And the two cats domain\nenvironment representation.",
    "start": "1002120",
    "end": "1004610"
  },
  {
    "text": "Let's go back to this example. So we have two domains\nand the two classes and the two classifiers\nof cat and dog.",
    "start": "1007320",
    "end": "1014940"
  },
  {
    "text": "And in this case, we can get the\nrepresentations, for example, for domain one. And for domain one, we can\ncancel representations, which",
    "start": "1014940",
    "end": "1022260"
  },
  {
    "text": "is composed of two information. The first one is animal,\nand the second one is water.",
    "start": "1022260",
    "end": "1028619"
  },
  {
    "text": "So we only cover some major\ninformation in these images.",
    "start": "1028619",
    "end": "1034109"
  },
  {
    "text": "And similarly, we can get the\nrepresentation for domain two, so it's also animal and grass.",
    "start": "1034109",
    "end": "1039240"
  },
  {
    "text": "And I hope we can\nalign these two representations to reinforce\nthe time to be very similar.",
    "start": "1042839",
    "end": "1049290"
  },
  {
    "text": "The simplest way\nto get a small loss is that finally, this\nnew network can only",
    "start": "1049290",
    "end": "1056309"
  },
  {
    "text": "learn the animal information\nbecause the animal information is shared\nacross different domains.",
    "start": "1056310",
    "end": "1060419"
  },
  {
    "text": "OK. So based on this example,\nI would then mathematically define some generic\nloss function for that.",
    "start": "1066250",
    "end": "1075460"
  },
  {
    "text": "So this is a typical\nloss function for the regulation-based method.",
    "start": "1075460",
    "end": "1080700"
  },
  {
    "text": "In the first time, it's for\nlabel classification loss. For example, in this, we\nclassify dog versus cat hair,",
    "start": "1080700",
    "end": "1090160"
  },
  {
    "text": "and then we will average\nthe loss over all training examples.",
    "start": "1090160",
    "end": "1093345"
  },
  {
    "text": "And then we will define\nexplicit regularizer to learn the domain\nvalue interpretation.",
    "start": "1096320",
    "end": "1102600"
  },
  {
    "text": "This is the key part for this\nregularization based method-- so how to define\nsuch a regularizer.",
    "start": "1102600",
    "end": "1108111"
  },
  {
    "start": "1113000",
    "end": "1877000"
  },
  {
    "text": "OK. So before we dive into this to\ngive some specific algorithms,",
    "start": "1114010",
    "end": "1119530"
  },
  {
    "text": "I will first recap the\ndomain adversarial training in domain adaptation that we\nlearned from the last lecture.",
    "start": "1119530",
    "end": "1127630"
  },
  {
    "text": "The key idea is that the\nprediction must be made based on the features that cannot\nbe discriminated between",
    "start": "1127630",
    "end": "1133900"
  },
  {
    "text": "the domains. For example, given one\nimage, this input x,",
    "start": "1133900",
    "end": "1139450"
  },
  {
    "text": "we will feed this image\nto our feature extractor and catch the features.",
    "start": "1139450",
    "end": "1145390"
  },
  {
    "text": "And then we have two\nbranches, and the first branch is to do the label\npredictor, and we",
    "start": "1145390",
    "end": "1151950"
  },
  {
    "text": "aim to make accurate\nlabel prediction. So in this case,\nbecause in my adaptation",
    "start": "1151950",
    "end": "1157289"
  },
  {
    "text": "we only have the labels\nfor the source domains-- so only source domain-- only data from source domain\nare feeding to this branch.",
    "start": "1157290",
    "end": "1165420"
  },
  {
    "text": "Second branch is our\ndomain classifier. So domain classifier, we aim\nto make this image cannot--",
    "start": "1165420",
    "end": "1173070"
  },
  {
    "text": "we cannot predict the domain\nfor this image based on these features. So in this case, the features--",
    "start": "1173070",
    "end": "1180690"
  },
  {
    "text": "we can learn some domain\ninvariant features. So both data from\nsource and target domain",
    "start": "1180690",
    "end": "1187560"
  },
  {
    "text": "can feed into this\nbranch because we want to do some domain\nclassification here",
    "start": "1187560",
    "end": "1194130"
  },
  {
    "text": "to classify whether these\nfeatures is from the source domain or from\nthe target domain.",
    "start": "1194130",
    "end": "1199710"
  },
  {
    "text": "OK. I now have one question. So this is a very simple version\nfor the adversarial training",
    "start": "1203490",
    "end": "1210000"
  },
  {
    "text": "in domain adaptation. So does anyone have ideas\non how to use the domain",
    "start": "1210000",
    "end": "1216720"
  },
  {
    "text": "adversarial training in the\ndomain generalization setting? So any volunteers?",
    "start": "1216720",
    "end": "1221370"
  },
  {
    "text": "One [INAUDIBLE] to predict\nwhich domain are real?",
    "start": "1232660",
    "end": "1237960"
  },
  {
    "text": "Are real? Or-- for each image?",
    "start": "1237960",
    "end": "1240405"
  },
  {
    "text": "Yeah. You want to predict its\ncorresponding domains, you mean?",
    "start": "1244400",
    "end": "1251080"
  },
  {
    "text": "You want to predict if this is\nthe one or not, for example. Oh, yeah, yeah, yeah.",
    "start": "1251080",
    "end": "1257050"
  },
  {
    "text": "Yeah. Yeah, that's right. So yeah, something that\nwe will mention here",
    "start": "1257050",
    "end": "1263710"
  },
  {
    "text": "is we have the label predictor\nand the domain classifier. And instead of we\nonly feed the data",
    "start": "1263710",
    "end": "1270790"
  },
  {
    "text": "from the source domain\nto the label predictor. In the domain\ngeneralization setting, we will fit data from\nall source domains",
    "start": "1270790",
    "end": "1278380"
  },
  {
    "text": "into this labor\npredictor so to predict its corresponding labels.",
    "start": "1278380",
    "end": "1282280"
  },
  {
    "text": "And similarly, the data\nfrom all source domains will be feed into the\ndomain classifier.",
    "start": "1285970",
    "end": "1293530"
  },
  {
    "text": "And instead of to\nclassify whether it's from source domain\nor target domain because there are no such\nhigh definition in the domain",
    "start": "1293530",
    "end": "1300730"
  },
  {
    "text": "generalization settings,\nwell, to predict those domain labels for every image.",
    "start": "1300730",
    "end": "1306309"
  },
  {
    "text": "Based on this definition-- so I\nwill first mathematically give",
    "start": "1313460",
    "end": "1318529"
  },
  {
    "text": "some definitions for\nthis label predictor and the domain classifier\nand its corresponding losses.",
    "start": "1318530",
    "end": "1325530"
  },
  {
    "text": "And in the label\nprediction, we aim to do-- such a label\nprediction gives an input x,",
    "start": "1325530",
    "end": "1332730"
  },
  {
    "text": "and we will extract some\nfeatures from f theta x, and then we will do the\nlabel predictor to get",
    "start": "1332730",
    "end": "1338500"
  },
  {
    "text": "its predictive value\nwith the function g, and we will submit\nfor every examples,",
    "start": "1338500",
    "end": "1346270"
  },
  {
    "text": "finally, to optimize the\nfunction-- the parameters",
    "start": "1346270",
    "end": "1351360"
  },
  {
    "text": "of label predictor\nand feature extractor. And here, the smaller\nis better because we",
    "start": "1351360",
    "end": "1357390"
  },
  {
    "text": "want to make accurate prediction\nin the source domains.",
    "start": "1357390",
    "end": "1360390"
  },
  {
    "text": "For domain prediction,\ninstead, we want to predict these\ndomains, but we actually",
    "start": "1364940",
    "end": "1370130"
  },
  {
    "text": "want to maximize the loss. So the larger loss is better\nbecause the larger loss",
    "start": "1370130",
    "end": "1376130"
  },
  {
    "text": "means it is harder to\ndistinguish domains for every input examples.",
    "start": "1376130",
    "end": "1382700"
  },
  {
    "text": "In case where we cannot\ndistinguish domains, we can learn some domain\ninvariant features and to make",
    "start": "1382700",
    "end": "1389570"
  },
  {
    "text": "it generalizable.",
    "start": "1389570",
    "end": "1390909"
  },
  {
    "text": "OK. And then we will bridge\nfrom the generic formulation",
    "start": "1398410",
    "end": "1405669"
  },
  {
    "text": "and as a new formulation for\nthe domain adversarial training in domain generalization.",
    "start": "1405670",
    "end": "1411440"
  },
  {
    "text": "This is a loss for the end\nloss in domain generalization. So the first term would be\nlabel classification loss,",
    "start": "1411440",
    "end": "1422500"
  },
  {
    "text": "and the second term\nwould be we design one regularizer to learn\nsuch a domain environment",
    "start": "1422500",
    "end": "1428799"
  },
  {
    "text": "representation.",
    "start": "1428800",
    "end": "1430036"
  },
  {
    "text": "And the full algorithm\nhas four steps. The first one is\nthat we will randomly",
    "start": "1435390",
    "end": "1441840"
  },
  {
    "text": "initialize the encoder,\nthe label classifier, and the domain classifier.",
    "start": "1441840",
    "end": "1447990"
  },
  {
    "text": "Then we will try to use the\ndomain classifier loss, the ld, to optimize the\ndomain classifier.",
    "start": "1447990",
    "end": "1456450"
  },
  {
    "text": "And we will-- based\non these results, we will update the label\nclassifier and the encoder",
    "start": "1456450",
    "end": "1464010"
  },
  {
    "text": "by considering both\nlabel classification loss and the domain\nclassification loss.",
    "start": "1464010",
    "end": "1469660"
  },
  {
    "text": "We will finally repeat\nstep two and step three until convergence.",
    "start": "1469660",
    "end": "1474070"
  },
  {
    "text": "In the second step. Are we trying to optimize\nthe domain classifier",
    "start": "1479820",
    "end": "1485970"
  },
  {
    "text": "to perform very well\nor perform very poor? Perform very poor.",
    "start": "1485970",
    "end": "1491122"
  },
  {
    "text": "I'm thinking about you want\nto do the [INAUDIBLE] training right, you should have a\nvery good domain classifier,",
    "start": "1491123",
    "end": "1497230"
  },
  {
    "text": "so that you will force\nthe representation to be domain [INAUDIBLE]. Yes. Then in this case should then\nwe like make the classifier very",
    "start": "1497230",
    "end": "1504044"
  },
  {
    "text": "good, so that it forces the-- You could. It's like in case you cannot\nforce a classifier-- yeah.",
    "start": "1504044",
    "end": "1511540"
  },
  {
    "text": "Yeah. Yes, this is good, yeah. So in case you can\nminimize the loss,",
    "start": "1511540",
    "end": "1517679"
  },
  {
    "text": "so you can learn a\ngood domain classifier, and its image can force\na domain classifier. [INAUDIBLE]",
    "start": "1517680",
    "end": "1524420"
  },
  {
    "text": "If we want to maximize this. So this is to minimize, yeah.",
    "start": "1527130",
    "end": "1530929"
  },
  {
    "text": "Any other questions? Do you think this would be more\nexpensive or less expensive",
    "start": "1534850",
    "end": "1542140"
  },
  {
    "text": "than say learning\nfeatures for a domain and then so like a multitask\nsetting, and then conditioning",
    "start": "1542140",
    "end": "1552340"
  },
  {
    "text": "on the domain with the feature\nextraction for the label?",
    "start": "1552340",
    "end": "1559390"
  },
  {
    "text": "So your question is you want to\nlearn a separate directions or?",
    "start": "1559390",
    "end": "1565180"
  },
  {
    "text": "So you learn some\nfeatures for each domain, and then you condition\non those features",
    "start": "1567880",
    "end": "1575500"
  },
  {
    "text": "for the label classification. Would that be\nequally expressive?",
    "start": "1575500",
    "end": "1583520"
  },
  {
    "text": "OK. I see. So you mean you will\nuse separate encoders and every domains have\none encoder, right?",
    "start": "1583520",
    "end": "1591760"
  },
  {
    "text": "Yes. Sort of like extracting\nfeatures for the domain,",
    "start": "1591760",
    "end": "1597100"
  },
  {
    "text": "and then creating\nthe label classifier. Possibly-- in some cases,\nit's more expressive-- in case",
    "start": "1597100",
    "end": "1602900"
  },
  {
    "text": "where you have a lot of\ndata from every domains. So you can learn\nabout the encoder",
    "start": "1602900",
    "end": "1608260"
  },
  {
    "text": "without sharing encoder\nwith other domains. But in some cases, you cannot\ncollect so many data for every",
    "start": "1608260",
    "end": "1613330"
  },
  {
    "text": "domain. So in case you would like to\nshare some representations and to have a shadow\nencoder, yeah.",
    "start": "1613330",
    "end": "1621520"
  },
  {
    "text": "And what happens if\nyou don't know the-- don't have information\nabout the domains?",
    "start": "1621520",
    "end": "1626740"
  },
  {
    "text": "Like you have a data\nset, and it's not labeled all the domains? Yes, this is a\nvery good question.",
    "start": "1626740",
    "end": "1633149"
  },
  {
    "text": "So this is something\nthat we also try to learn without considering\nthe domain information.",
    "start": "1633150",
    "end": "1638710"
  },
  {
    "text": "There are a few solutions. For instance, one is you can\ntry to predict the domain information based on the data.",
    "start": "1638710",
    "end": "1645730"
  },
  {
    "text": "And in some cases, if you change\nEIM, empirical recommendation, you can also just use\nsome domain information",
    "start": "1645730",
    "end": "1651370"
  },
  {
    "text": "to see whether you\ncan predict it well. Other cases, you can find\nthere are some misclassified",
    "start": "1651370",
    "end": "1658360"
  },
  {
    "text": "examples. So this may be from\nthe minority domains, and you can upweight these\nexamples to make it work.",
    "start": "1658360",
    "end": "1667840"
  },
  {
    "text": "Thank you. OK. Yeah.",
    "start": "1667840",
    "end": "1673260"
  },
  {
    "text": "So my question was\nasking people to try to learn shared\nwritten representation",
    "start": "1673260",
    "end": "1678330"
  },
  {
    "text": "between the tasks [INAUDIBLE]. And so you learn task\nspecific, like domain specific,",
    "start": "1678330",
    "end": "1684090"
  },
  {
    "text": "and the domain [INAUDIBLE]\nshared feature space.",
    "start": "1684090",
    "end": "1689140"
  },
  {
    "text": "So how does it relate to the--\nso would that perform better, or would this kind of\nalgorithm perform better?",
    "start": "1689140",
    "end": "1696029"
  },
  {
    "text": "To repeat the question,\nso you mean we-- Separate domains.",
    "start": "1696030",
    "end": "1701250"
  },
  {
    "text": "Different domains, each\ndomain has its own encoder? Yeah. So you learn by\ndomain-specific parameters,",
    "start": "1701250",
    "end": "1708450"
  },
  {
    "text": "and those shared parameters\nacross the domains. By maximizing information\nbetween the different domains",
    "start": "1708450",
    "end": "1718950"
  },
  {
    "text": "present between the\nsamples that you have. So you can learn the common\n[INAUDIBLE] presentation,",
    "start": "1718950",
    "end": "1724529"
  },
  {
    "text": "and also you have two branches. Yeah, two branches\nto learn the--",
    "start": "1724530",
    "end": "1730050"
  },
  {
    "text": "two samples to have some\ndomain-specific layers. Yeah. Yeah, I think this is\npossibly more expressive.",
    "start": "1730050",
    "end": "1735629"
  },
  {
    "text": "It's the same question and-- I think-- wouldn't it give\nyou more expressive power in the testing time?",
    "start": "1735630",
    "end": "1741370"
  },
  {
    "text": "Like let's say if you\njust want to output a particular feature\nor particular domain?",
    "start": "1741370",
    "end": "1746910"
  },
  {
    "text": "But in the test time, you will\ndefinitely get one new domain, so you do not have its\ncorresponding branch.",
    "start": "1746910",
    "end": "1753077"
  },
  {
    "text": "We could use the commonly\nwritten representation-- Yes. Yes. Yes.",
    "start": "1753077",
    "end": "1758190"
  },
  {
    "text": "In case you mean you want to\n[INAUDIBLE] the domain-specific branches--",
    "start": "1758190",
    "end": "1764820"
  },
  {
    "text": "We could use the common-- [INTERPOSING VOICES] --common ones, and how to do it\nfor the domain-specific ones--",
    "start": "1764820",
    "end": "1771200"
  },
  {
    "text": "I just want to\nclarify your question. So probably I can\nuse some whiteboard.",
    "start": "1771200",
    "end": "1776980"
  },
  {
    "text": "So you mean there are one\nshared representations. This is shared,\nand then you have",
    "start": "1776980",
    "end": "1783330"
  },
  {
    "text": "two branches for two domains,\nand how to do it for the test.",
    "start": "1783330",
    "end": "1789414"
  },
  {
    "text": "So in the testing,\nyou definitely need to go out some branches. Yeah, but you do not have the\ninformation for the test time,",
    "start": "1789415",
    "end": "1797640"
  },
  {
    "text": "which branch do you want to go. With [INAUDIBLE]---- I\nwasn't thinking about the--",
    "start": "1797640",
    "end": "1804559"
  },
  {
    "text": "I was talking about learning\ncommon written representation for the samples themselves. For this one?",
    "start": "1804560",
    "end": "1810200"
  },
  {
    "text": "Yeah. So assuming that\nthat's a latent space, we could learn common latent\nspace and the shared latent",
    "start": "1810200",
    "end": "1815950"
  },
  {
    "text": "space. And then we could use\nthe common latent space to then generate\nfor any domains.",
    "start": "1815950",
    "end": "1824170"
  },
  {
    "text": "Yeah, I think this\nis what we learned right now to use our shared\nencoder for every domain.",
    "start": "1824170",
    "end": "1830750"
  },
  {
    "text": "So let's go back to this one. So we have a shared\nfeature encoder and to go to every domain.",
    "start": "1830750",
    "end": "1837934"
  },
  {
    "text": "[INAUDIBLE] or the\nlatents [INAUDIBLE].. Oh, we currently do\nnot add any conditions,",
    "start": "1837934",
    "end": "1843790"
  },
  {
    "text": "but possibly add some\nconditions to enforce. This would be helpful.",
    "start": "1843790",
    "end": "1851325"
  },
  {
    "text": "Well, maybe that's\nplenty helpful. For the multitask\nleaning, the key point",
    "start": "1851326",
    "end": "1856900"
  },
  {
    "text": "is that multitask\nleaning only solves the-- when the training domain is\nexactly the same from the test",
    "start": "1856900",
    "end": "1863350"
  },
  {
    "text": "domains, right, but we want\nto solve some unseen domains.",
    "start": "1863350",
    "end": "1866860"
  },
  {
    "text": "OK.",
    "start": "1871270",
    "end": "1871770"
  },
  {
    "start": "1877000",
    "end": "1957000"
  },
  {
    "text": "So any other questions?",
    "start": "1880910",
    "end": "1882260"
  },
  {
    "text": "What if you don't have a-- sorry. What if you don't have a\nbunch of domain labeled data? I mean, maybe with the\ncat dog example earlier,",
    "start": "1886540",
    "end": "1894550"
  },
  {
    "text": "you don't know that grass\nand water are really two different domains\nthat are going to be your problem until you are--",
    "start": "1894550",
    "end": "1900640"
  },
  {
    "text": "until after you try to do it? Yeah. I think, this problem, as I\nmentioned, so probably you",
    "start": "1900640",
    "end": "1906130"
  },
  {
    "text": "can see, try to distinguish\nthe domain information and to do that\ndomain prediction. And then in other cases,\nyou can see that how to,",
    "start": "1906130",
    "end": "1914230"
  },
  {
    "text": "for example, how to see that\nsome miss-- have a train-- feeding model into\n[INAUDIBLE] once,",
    "start": "1914230",
    "end": "1920514"
  },
  {
    "text": "and to identify some\nmisclassified [INAUDIBLE] ones. These ones may be from\nthe majority-- oh, sorry,",
    "start": "1920515",
    "end": "1925900"
  },
  {
    "text": "from the minority domains\nor from the minority groups. Yeah.",
    "start": "1925900",
    "end": "1930040"
  },
  {
    "text": "OK, OK. So let's continue. So the virtual\ntraining, so leverage",
    "start": "1937020",
    "end": "1943970"
  },
  {
    "text": "the adversarial optimization\nto learn the domain invariant features.",
    "start": "1943970",
    "end": "1949799"
  },
  {
    "text": "So you may ask, are there\nany other ways to do that?",
    "start": "1949800",
    "end": "1955120"
  },
  {
    "text": "Then I will introduce\none alternative approach called the CORAL. In the CORAL, so the key\nidea is that it can directly",
    "start": "1955120",
    "end": "1963090"
  },
  {
    "start": "1957000",
    "end": "2367000"
  },
  {
    "text": "align the representations\nbetween the different domains with some similarity metrics.",
    "start": "1963090",
    "end": "1968460"
  },
  {
    "text": "And in CORAL, so which is\ncalled the Correlation Alignment for Domain Adaptation,\nalthough the name is from the auto adaptation.",
    "start": "1972340",
    "end": "1978910"
  },
  {
    "text": "And this method is originally\nfrom the domain adaptation. But is usually also used\nin domain generation",
    "start": "1978910",
    "end": "1985540"
  },
  {
    "text": "in recent years. So here I would only\nchoose the dimension regime",
    "start": "1985540",
    "end": "1991590"
  },
  {
    "text": "version of this algorithm. So in this one, assume\nwe have two domains.",
    "start": "1991590",
    "end": "1996929"
  },
  {
    "text": "For them, here, we want to\nrecognize different objects. We have some shared layers.",
    "start": "1996930",
    "end": "2002830"
  },
  {
    "text": "And this could be the\ncorrelation feature extractor. And then we will use features\nto feed into some classifier",
    "start": "2002830",
    "end": "2008920"
  },
  {
    "text": "to get the classification\nloss for every domain. So for the domain one we\nhave one classification loss.",
    "start": "2008920",
    "end": "2014140"
  },
  {
    "text": "And the domain two has\nanother classification loss.. Then we will have\nthe CORAL loss.",
    "start": "2014140",
    "end": "2021720"
  },
  {
    "text": "In the CORAL loss, we want\nto directly makes aligns these both representations.",
    "start": "2021720",
    "end": "2029340"
  },
  {
    "text": "Before we dive into\nthe CORAL loss, I will first give\nsome notations. So the notation X1\nis a feature metric.",
    "start": "2029340",
    "end": "2037050"
  },
  {
    "text": "And for the representation and\nthis is with dimension n1 times",
    "start": "2037050",
    "end": "2042420"
  },
  {
    "text": "k for the domain one. And the exterior, similarly, for\nthe domain two is n2 times k.",
    "start": "2042420",
    "end": "2048388"
  },
  {
    "text": "And the k is the number\nof features here. And we get the mean value\nwith this formulation",
    "start": "2048389",
    "end": "2055320"
  },
  {
    "text": "and with dimension 1\ntimes k and similarly to the mean value of\nthe alignment domain",
    "start": "2055320",
    "end": "2062800"
  },
  {
    "text": "over all examples. Then we will try to calculate\nsome covariance metrics",
    "start": "2062800",
    "end": "2069638"
  },
  {
    "text": "in CORAL. The goal for CORAL is to make\nthis covariance matrix exactly similar to contrast\nthe similarity",
    "start": "2069639",
    "end": "2077110"
  },
  {
    "text": "between different\ncovariance matrix. So in the covariance\nmatrix, this is something that we learned\nfrom some math courses.",
    "start": "2077110",
    "end": "2084860"
  },
  {
    "text": "So to get this\ncovariance matrix. And finally, to\nfind one CORAL laws",
    "start": "2084860",
    "end": "2090250"
  },
  {
    "text": "to make this covariance\nof our features would be very exactly the same.",
    "start": "2090250",
    "end": "2094840"
  },
  {
    "text": "And this is also the loss\nfor the CORAL algorithm. And we will combine\nthe classification laws",
    "start": "2099860",
    "end": "2107300"
  },
  {
    "text": "for the first time over\nall different examples.",
    "start": "2107300",
    "end": "2112370"
  },
  {
    "text": "And we will also\ndesign one CORAL laws to explain it to\nregularize to learn",
    "start": "2112370",
    "end": "2117980"
  },
  {
    "text": "the domain-invariant\nrepresentation. So the key idea for the\nCORAL-- behind the CORAL is to make the covariance matrix\nsimilar for every domains.",
    "start": "2117980",
    "end": "2127235"
  },
  {
    "text": "Is this extendable to\nmore than two domains? Yeah. It's extendable. So you can directly\nadd the more domains,",
    "start": "2132900",
    "end": "2139710"
  },
  {
    "text": "for example, domains 3, 4, 5\nwith one classification loss and the use pairwise\nCORAL loss to do that.",
    "start": "2139710",
    "end": "2147150"
  },
  {
    "text": "So do we have individual\nencoders for different domains? Typically, people will\nshare the encoders.",
    "start": "2147150",
    "end": "2153310"
  },
  {
    "text": "Yeah.",
    "start": "2153310",
    "end": "2153810"
  },
  {
    "text": "Any other questions?",
    "start": "2165850",
    "end": "2167200"
  },
  {
    "text": "So one question was in\ngeneral about how much does model architecture and\nour mathematical formulation",
    "start": "2174800",
    "end": "2182980"
  },
  {
    "text": "of the problem matter in\nthis kind of-- in approaching this kind of problems?",
    "start": "2182980",
    "end": "2188319"
  },
  {
    "text": "So you mean for the generic\nversion of regularization? [INAUDIBLE] for domain\ngeneration for--",
    "start": "2188320",
    "end": "2195610"
  },
  {
    "text": "or any particular\nthis kind of problems, while approaching how\nthis choosing the model architecture and\nmathematical formulation",
    "start": "2195610",
    "end": "2203260"
  },
  {
    "text": "that the problem has? So it depends. So you know we can increase the\nmodel architecture from RESNet",
    "start": "2203260",
    "end": "2211600"
  },
  {
    "text": "or even for visual transformer,\nyou will have more parameters. And these models will\nbe more expressed--",
    "start": "2211600",
    "end": "2216910"
  },
  {
    "text": "have strong express power and\nto represent more information. And the design of\nsome regularizers",
    "start": "2216910",
    "end": "2223510"
  },
  {
    "text": "is also very sensitive in these\nspecific domain generation problems. In case you have\nvery large models--",
    "start": "2223510",
    "end": "2231010"
  },
  {
    "text": "in case you have the same\nmodels, same background, and we can compare different\nkinds of regularizers.",
    "start": "2231010",
    "end": "2236470"
  },
  {
    "text": "And with the same\nregularizers we can compare different\nbackgrounds.",
    "start": "2236470",
    "end": "2242010"
  },
  {
    "text": "Maybe I want to understand\nthe limits of each of these. So how much the model\narchitecture selection affects",
    "start": "2242010",
    "end": "2249059"
  },
  {
    "text": "your performance\nand how much can the mathematical formulation\nof loss term or any of the term",
    "start": "2249060",
    "end": "2255720"
  },
  {
    "text": "would affect the\nperformance in general? Or how much power do they\nhave to while [INAUDIBLE]..",
    "start": "2255720",
    "end": "2262380"
  },
  {
    "text": "So for each of\nthem, so for model architectures,\nfor the comparison between model architecture\nand the loss function.",
    "start": "2262380",
    "end": "2268982"
  },
  {
    "text": "For different data sets, I\nthink increasing the model--",
    "start": "2271980",
    "end": "2277890"
  },
  {
    "text": "the express power or\nmodel architecture is not a bad choice. So you can definitely\nincrease-- improve the results.",
    "start": "2277890",
    "end": "2286140"
  },
  {
    "text": "But typically, we try\nto add one regularizer.",
    "start": "2286140",
    "end": "2294089"
  },
  {
    "text": "And this regularizer,\nchosen regularizer can affect the problem-- the performance more.",
    "start": "2294090",
    "end": "2299130"
  },
  {
    "text": "If we found that bad one, so\nit even hurts the performance.",
    "start": "2299130",
    "end": "2305220"
  },
  {
    "text": "Yeah. Because if you increase\nthe model, express power, so the performance of\nERM will also increase.",
    "start": "2305220",
    "end": "2312705"
  },
  {
    "text": "Yeah.",
    "start": "2312705",
    "end": "2313205"
  },
  {
    "text": "[INAUDIBLE]",
    "start": "2320680",
    "end": "2321430"
  },
  {
    "text": "In case if we can't\nalign different kinds of different order\nof the information, it would be helpful.",
    "start": "2327610",
    "end": "2333260"
  },
  {
    "text": "So the simplest one,\nyou can directly align the representations. And in these papers, they\nconsider the second order",
    "start": "2333260",
    "end": "2338890"
  },
  {
    "text": "versions of covariance. So if you can do more. But if you could do more,\nyou will have more cost--",
    "start": "2338890",
    "end": "2344710"
  },
  {
    "text": "computational cost to do more. Yeah. [LAUGHS]",
    "start": "2344710",
    "end": "2348825"
  },
  {
    "text": "So we can continue.",
    "start": "2360780",
    "end": "2362100"
  },
  {
    "text": "And therefore, the\nresults, I just choose some results from\nthe OfficeHome DomainNet, and iWildCAM.",
    "start": "2366080",
    "end": "2371990"
  },
  {
    "start": "2367000",
    "end": "2417000"
  },
  {
    "text": "And in the OfficeHome,\nwe have four domains. And we will hold one\ndomains as a test",
    "start": "2371990",
    "end": "2377510"
  },
  {
    "text": "and use another three training-- another three\ndomains as training, and then to generalize one\nmodel to the test ones.",
    "start": "2377510",
    "end": "2385010"
  },
  {
    "text": "And we will repeat the\nprogress for fourth. So for every domains\nto do some evaluation.",
    "start": "2385010",
    "end": "2390320"
  },
  {
    "text": "And also, we will do the similar\nthings for the DomainNet. And for the iWildCAM,\nas I mentioned before,",
    "start": "2390320",
    "end": "2395930"
  },
  {
    "text": "is basically we\nwant to generalize the model to new locations to\ndo the wildlife recognition.",
    "start": "2395930",
    "end": "2402650"
  },
  {
    "text": "And we can see the\nCORAL performance well compared with the ERM. And then sometimes\nhurt the performance",
    "start": "2402650",
    "end": "2409940"
  },
  {
    "text": "compared with the ERM. I think this is\nalways a question that designing a suitable\nregularizer is very important.",
    "start": "2409940",
    "end": "2417840"
  },
  {
    "start": "2417000",
    "end": "2557000"
  },
  {
    "text": "Finally, for these\nkinds of methods, I would mention\nsome pros and cons. And the good things\nis that the first one",
    "start": "2417840",
    "end": "2425599"
  },
  {
    "text": "is, this method can generalize\nwell to all kinds of data in the networks.",
    "start": "2425600",
    "end": "2430799"
  },
  {
    "text": "So for example, if we want to\ntranslate to graph data or text data, we only need to change the\nshared layers, the background",
    "start": "2430800",
    "end": "2437600"
  },
  {
    "text": "from bird and to graph neural\nnetwork or any other layers.",
    "start": "2437600",
    "end": "2442760"
  },
  {
    "text": "And there are also some\ntheoretical guarantees. Here, in these kinds of methods,\nI will not dive into this one.",
    "start": "2442760",
    "end": "2448610"
  },
  {
    "text": "But if anyone are interested\nin any theoretical results, please email me\nand I can send you",
    "start": "2448610",
    "end": "2454100"
  },
  {
    "text": "some papers to discuss this. But as a bad thing is\nthat this regularizers",
    "start": "2454100",
    "end": "2460670"
  },
  {
    "text": "sometimes being too\nharsh or too constraining on the representations. And we can see that the DN\ncannot work very well in some",
    "start": "2460670",
    "end": "2469220"
  },
  {
    "text": "data sets. And let's also go back\nto see why regularizer",
    "start": "2469220",
    "end": "2474290"
  },
  {
    "text": "are being too harsh. For example, you\nsee, in this one, we can go back to this dog\nversus cat classification.",
    "start": "2474290",
    "end": "2481230"
  },
  {
    "text": "And this is the loss function. And if we directly add\none explicit regularizer,",
    "start": "2481230",
    "end": "2487910"
  },
  {
    "text": "that it will encourage the\ninternal representations to only contains the no\ninfo about the background.",
    "start": "2487910",
    "end": "2493820"
  },
  {
    "text": "But sometimes, it's\nin the mixture. So we also need\nsome informations",
    "start": "2493820",
    "end": "2499640"
  },
  {
    "text": "to do this\nclassification, or to-- if we do not have\nany backgrounds,",
    "start": "2499640",
    "end": "2505190"
  },
  {
    "text": "it even hurt us to\nlearn a better domain image representations, or\neven hurt the expressed power",
    "start": "2505190",
    "end": "2512450"
  },
  {
    "text": "of the neural network.",
    "start": "2512450",
    "end": "2513619"
  },
  {
    "text": "Any questions?",
    "start": "2519000",
    "end": "2520050"
  },
  {
    "text": "OK, so based on\nthis, we can see that in some real-world\napplications, like iWildCAM,",
    "start": "2528810",
    "end": "2534132"
  },
  {
    "text": "the color improves\nthe performance. But in some other\ndata sets, like R1 they say the medical\nimage classification task.",
    "start": "2534132",
    "end": "2541360"
  },
  {
    "text": "So the color even\nhurt the performance. So they also hurts\nthe performance, as I mentioned before.",
    "start": "2541360",
    "end": "2546765"
  },
  {
    "text": "You may ask, are\nthere any approaches to relax the dependency\nof the regularizer?",
    "start": "2549270",
    "end": "2554520"
  },
  {
    "start": "2557000",
    "end": "2787000"
  },
  {
    "text": "Then we'll let's move to our\nnext part of the algorithm design. So to do the data augmentation.",
    "start": "2558510",
    "end": "2564720"
  },
  {
    "text": "Before we're detailing\nthe data augmentation, let's first recap the\nspurious correlation. In the spurious\ncorrelation, our goal",
    "start": "2567980",
    "end": "2573770"
  },
  {
    "text": "is to classify the dog and cat. And the background is sometimes\nspuriously correlated with",
    "start": "2573770",
    "end": "2578869"
  },
  {
    "text": "the animal, and though some\nmodels cannot make a right prediction.",
    "start": "2578870",
    "end": "2583640"
  },
  {
    "text": "However, there is one question. In case we can\ncollect more data, so we have the grass data,\nthe water data, and the data--",
    "start": "2586700",
    "end": "2594130"
  },
  {
    "text": "the car domain and the keyboard\ndomain from both dog and cat.",
    "start": "2594130",
    "end": "2599420"
  },
  {
    "text": "So we want to recognize--\nstill recognize this dog. So the question would\nbe, will the network",
    "start": "2599420",
    "end": "2605630"
  },
  {
    "text": "still associate dog with water\nbackground in some source domains?",
    "start": "2605630",
    "end": "2611160"
  },
  {
    "text": "So any guys want to\nanswer this question?",
    "start": "2611160",
    "end": "2613530"
  },
  {
    "text": "The [INAUDIBLE]\ncannot associate.",
    "start": "2624490",
    "end": "2627700"
  },
  {
    "text": "[INAUDIBLE] If you still mainly\nstill just have",
    "start": "2633380",
    "end": "2639769"
  },
  {
    "text": "dogs in the water\nbackground, then it might still just associate\ndroplet water background,",
    "start": "2639770",
    "end": "2646250"
  },
  {
    "text": "even after you collect the data. Yeah. If we do not consider\nthis scenario.",
    "start": "2646250",
    "end": "2649850"
  },
  {
    "text": "Yeah. I would consider whether\nthere is uniform distribution",
    "start": "2652120",
    "end": "2657605"
  },
  {
    "text": "across a background. Across different backgrounds? In case we consider\nall backgrounds with a similar weight or--",
    "start": "2657605",
    "end": "2665839"
  },
  {
    "text": "Then maybe [INAUDIBLE].",
    "start": "2669130",
    "end": "2670940"
  },
  {
    "text": "Yeah. That's [INAUDIBLE]. So because there are\nmany more backgrounds, so we cannot recognize dogs\nonly with the grass background.",
    "start": "2674620",
    "end": "2684740"
  },
  {
    "text": "And in this case,\nsome models can produce the right prediction.",
    "start": "2684740",
    "end": "2688892"
  },
  {
    "text": "However, there is one challenge. In case that we cannot always\ncollect model data from",
    "start": "2691670",
    "end": "2697340"
  },
  {
    "text": "the different domains. So what we want to do? We can aim to generate more\ndatum and to train this model.",
    "start": "2697340",
    "end": "2705395"
  },
  {
    "text": "And here we want to\nintroduce some methods for data augmentation. For example, there are\nsome simple operators",
    "start": "2709480",
    "end": "2716530"
  },
  {
    "text": "that can help us to\ngenerate more data. And in the conditioning\nfor example,",
    "start": "2716530",
    "end": "2722210"
  },
  {
    "text": "we are given one image. This is the original image. We can use different operators\nto generate the data,",
    "start": "2722210",
    "end": "2728180"
  },
  {
    "text": "like flipping, rotating,\ncropping, or like a PCA, or edge enhancement,\nor any other methods",
    "start": "2728180",
    "end": "2734270"
  },
  {
    "text": "that we can generate some\nimage data based on one image.",
    "start": "2734270",
    "end": "2738005"
  },
  {
    "text": "And in the augmented domain, we\ncan do some back-translation. In the back-translation, we\nhave one sentence, originally",
    "start": "2740860",
    "end": "2747190"
  },
  {
    "text": "from English. And then we want to\ntranslate it to French and then translate it back.",
    "start": "2747190",
    "end": "2752860"
  },
  {
    "text": "So that augmented\nexample would be slightly different\nfrom the original one.",
    "start": "2752860",
    "end": "2758320"
  },
  {
    "text": "And this data\naugmentations can typically help us to do the\ndomain translation can",
    "start": "2758320",
    "end": "2764290"
  },
  {
    "text": "benefit the performance. However, this simple\noperators requires a knowledge",
    "start": "2764290",
    "end": "2770890"
  },
  {
    "text": "of the public domain. For example, we need to see\nhow this is the image data or from--",
    "start": "2770890",
    "end": "2776380"
  },
  {
    "text": "or text data, or to adopt\ndifferent strategies to do the data augmentation.",
    "start": "2776380",
    "end": "2782859"
  },
  {
    "text": "Our question would be,\nany general approaches?",
    "start": "2782860",
    "end": "2785630"
  },
  {
    "start": "2787000",
    "end": "2933000"
  },
  {
    "text": "And I will introduce one general\napproach for data augmentation",
    "start": "2788850",
    "end": "2793860"
  },
  {
    "text": "in this lecture. It's called the Mixup. In the Mixup, the keep idea is\nto interpret training examples.",
    "start": "2793860",
    "end": "2803220"
  },
  {
    "text": "Assuming we have a learned\nmodel with our training data. So the training data\nhas any examples",
    "start": "2803220",
    "end": "2809190"
  },
  {
    "text": "with the input and\nthe labeled pair, so xi, yi, and then from i to-- equal to n to--",
    "start": "2809190",
    "end": "2815640"
  },
  {
    "text": "equal to 1 to n. And we will train classifier\nbased on this training data.",
    "start": "2815640",
    "end": "2822150"
  },
  {
    "text": "And then, Mixup aim to\nreplace this original training data from a mixed version.",
    "start": "2822150",
    "end": "2830590"
  },
  {
    "text": "So in this case, we\nwill have two examples, xi, yi, and xj, yj.",
    "start": "2830590",
    "end": "2837780"
  },
  {
    "text": "And then, we will do some\nlinear interpolation. So also like a\nconvex combination",
    "start": "2837780",
    "end": "2843839"
  },
  {
    "text": "between the xi, yi-- ah, between xi and the\nxj and the yi and the yj.",
    "start": "2843840",
    "end": "2852060"
  },
  {
    "text": "So in this case, we can\ngenerate some virtual examples called xi tilde and the\ny tilde for both features",
    "start": "2852060",
    "end": "2860080"
  },
  {
    "text": "and the labels-- for both input and the labels. And then, Mixup will use\nthis generated examples",
    "start": "2860080",
    "end": "2868900"
  },
  {
    "text": "to replace the original examples\nand use the new training data to build a classifier.",
    "start": "2868900",
    "end": "2877420"
  },
  {
    "text": "So here's one example\nfor the Mixup. We can generate some virtual\nexamples between two classes.",
    "start": "2877420",
    "end": "2883920"
  },
  {
    "text": "So the first image in the\nleft-hand side is for the cat.",
    "start": "2883920",
    "end": "2888930"
  },
  {
    "text": "And then next one is for dog. And then we can combine\nthem to generate some images",
    "start": "2888930",
    "end": "2895740"
  },
  {
    "text": "in between. So from cat and dog. So this image has around\nthe 70% probability",
    "start": "2895740",
    "end": "2901800"
  },
  {
    "text": "to be classified as\na cat and the 30% to be classified as a dog.",
    "start": "2901800",
    "end": "2907760"
  },
  {
    "text": "So this is a very common\nand a useful ways to do data augmentation.",
    "start": "2907760",
    "end": "2915109"
  },
  {
    "text": "Any questions about the\nprocess for the Mixup?",
    "start": "2915110",
    "end": "2917660"
  },
  {
    "text": "So in domain\ngeneration, Mixup itself can improve the performance\nof domain generations.",
    "start": "2931500",
    "end": "2940779"
  },
  {
    "text": "So let's see. We here, we want to do\nthe tissue classification from Camelyon and also to do\nsome learned type prediction.",
    "start": "2940780",
    "end": "2949026"
  },
  {
    "text": "In the learned type\nprediction example we call-- this data set\nhere we call the FMoW. And we want to given\none satellite image,",
    "start": "2949027",
    "end": "2955920"
  },
  {
    "text": "we want to classify\nas land types. And we want to generalize\nfrom some year and the region",
    "start": "2955920",
    "end": "2961650"
  },
  {
    "text": "combination to other\nyear region combinations. And compared with the\nERM, and the Mixup",
    "start": "2961650",
    "end": "2968250"
  },
  {
    "text": "can improve the performance. However, it is not always good.",
    "start": "2968250",
    "end": "2973340"
  },
  {
    "text": "For them it's we will still\nuse these R1 examples, these are quite\nchallenging data set. So Mixup even hurts\nthe performance",
    "start": "2973340",
    "end": "2980450"
  },
  {
    "text": "compared with the ERM. The key job here for\nthe original Mixup",
    "start": "2980450",
    "end": "2987460"
  },
  {
    "text": "is that this method\nonly focuses on the data augmentation instead of\ntrying to learn some domain",
    "start": "2987460",
    "end": "2994570"
  },
  {
    "text": "invariance.",
    "start": "2994570",
    "end": "2995683"
  },
  {
    "text": "So the question is, how to\nimprove the original Mixup.",
    "start": "3001480",
    "end": "3006980"
  },
  {
    "text": "To introduce the\nnext algorithm, I will first give\na simple examples with spurious correlation.",
    "start": "3006980",
    "end": "3013250"
  },
  {
    "text": "So this example can help\nyou to understand what happens in the next algorithm.",
    "start": "3013250",
    "end": "3018589"
  },
  {
    "text": "And the example is quite\nsimilar to the examples from the real-world images. So this is a toy case.",
    "start": "3018590",
    "end": "3025250"
  },
  {
    "text": "In the toy case, we have a\nlot of twos and red fives.",
    "start": "3025250",
    "end": "3031100"
  },
  {
    "text": "And our goal is the label one\nis to see this digit is smaller than five, and the\nlabel two is this digit",
    "start": "3031100",
    "end": "3036230"
  },
  {
    "text": "is larger or equal to five. And then we have a target\ndomain with similar colors,",
    "start": "3036230",
    "end": "3043280"
  },
  {
    "text": "or with the green five,\nbut as the green five is from a different ways,\nits background is green.",
    "start": "3043280",
    "end": "3050810"
  },
  {
    "text": "And the feeding to\nour neural network, and this neural network cannot\nproduce the correct prediction.",
    "start": "3050810",
    "end": "3058460"
  },
  {
    "text": "Because in the training\npart, the models spuriously associated the\ncolor with the labels.",
    "start": "3058460",
    "end": "3065690"
  },
  {
    "text": "And so, these examples are\nvery similar to the examples I gave before for the dog\nand the cat classification.",
    "start": "3068710",
    "end": "3075369"
  },
  {
    "start": "3079000",
    "end": "3599000"
  },
  {
    "text": "OK. So based on this\nexample, I would like to introduce LISA to\nimprove the original Mixup.",
    "start": "3081490",
    "end": "3089119"
  },
  {
    "text": "The key idea behind LISA is-- LISA want to selectively\ninterpret examples",
    "start": "3089120",
    "end": "3095870"
  },
  {
    "text": "and to emphasize some\ninvariant information, not only on the data\naugmentation, like Mixup did.",
    "start": "3095870",
    "end": "3103215"
  },
  {
    "text": "So this is the examples. We called it colored MNIST. So we have some two majority\ngroups and the two minority",
    "start": "3105990",
    "end": "3113160"
  },
  {
    "text": "groups. And based on the\nbuild upon the Mixup,",
    "start": "3113160",
    "end": "3118200"
  },
  {
    "text": "there is one variant called\nthe intra-label LISA. In the intra-label LISA\nwe aim to interpret",
    "start": "3118200",
    "end": "3123960"
  },
  {
    "text": "examples with the same label\nbut from different domains. We just add one\nsimple constraint",
    "start": "3123960",
    "end": "3130560"
  },
  {
    "text": "on the original Mixup. So to make it the\ndomain quite different. So from di not equal to dj.",
    "start": "3130560",
    "end": "3136589"
  },
  {
    "text": "But we will make sure\nthe labels are same. So yi equal to yj.",
    "start": "3136590",
    "end": "3141809"
  },
  {
    "text": "And we can see in these\nfive images, so first one and the last one. I mean, the left one\nand the right one",
    "start": "3144360",
    "end": "3150540"
  },
  {
    "text": "is from the original data set. So lambda equal to zero\nor lambda equal to one.",
    "start": "3150540",
    "end": "3156420"
  },
  {
    "text": "And then, we can\nshare some images with different backgrounds. We can see these three\nimages in between.",
    "start": "3156420",
    "end": "3163560"
  },
  {
    "text": "And all images are associated\nwith the same label. So with digit larger\nor equal to five.",
    "start": "3163560",
    "end": "3170609"
  },
  {
    "text": "And we can see that\neven when we generate a lot of different\nbackgrounds, all of them",
    "start": "3170610",
    "end": "3177755"
  },
  {
    "text": "are associated with\nthe same label. And in this case, the\nmodels will finally",
    "start": "3177755",
    "end": "3183569"
  },
  {
    "text": "ignore this color\ninformation and only focus on the digit information\nand to capture this invariance.",
    "start": "3183570",
    "end": "3190410"
  },
  {
    "text": "Any questions for this? I think I have a Mixup\nquestion in general.",
    "start": "3195060",
    "end": "3203230"
  },
  {
    "text": "So is it possible-- have people been trying to\napply Mixup to the text domain?",
    "start": "3203230",
    "end": "3209440"
  },
  {
    "text": "Yeah. Yeah.\nYeah. I will mention it later. Yeah. Any other questions?",
    "start": "3209440",
    "end": "3216619"
  },
  {
    "text": "[INAUDIBLE] --certain of the labels\nof the triangle domain?",
    "start": "3216620",
    "end": "3223610"
  },
  {
    "text": "You mean domain labels? Yeah. Yeah. I mean, if we do not\nhave domain labels, it's a little bit hard\nto apply this algorithm.",
    "start": "3223610",
    "end": "3231690"
  },
  {
    "text": "So but one simple\nsolution is that we can try to see\nwhether we can only mix the examples\nwith the same label",
    "start": "3231690",
    "end": "3238460"
  },
  {
    "text": "without considering\nthe domain information. In this case, you can also\ngenerate some similar things for this variant.",
    "start": "3238460",
    "end": "3245254"
  },
  {
    "text": "Could you do that\nif the model is, let's say, confident\non some target samples?",
    "start": "3245254",
    "end": "3250790"
  },
  {
    "text": "Could we use the tools\nas a label control? You mean we want to train our\nmodel on the target samples?",
    "start": "3250790",
    "end": "3257135"
  },
  {
    "text": "Yeah. And so we do inference\non the target samples-- Yeah. --for which we\ndon't have labels.",
    "start": "3257135",
    "end": "3263340"
  },
  {
    "text": "But the predictions, if\nthose have good certainty, if the model is much more\nconfident on the samples--",
    "start": "3263340",
    "end": "3270560"
  },
  {
    "text": "of some samples-- On test domain? On the test domain, yeah. Could we use them\nas a ground truth-- [INTERPOSING VOICES]",
    "start": "3270560",
    "end": "3276640"
  },
  {
    "text": "The question is, we can\nuse labels from-- you can some examples with\nsome pseudo-labels",
    "start": "3276640",
    "end": "3283549"
  },
  {
    "text": "in the target domain. Yeah. This is another\nquestion would be close to some domain\nadaptation or semi",
    "start": "3283550",
    "end": "3290090"
  },
  {
    "text": "supervise the linear settings. Yeah. We will not consider\nthese settings here.",
    "start": "3290090",
    "end": "3294320"
  },
  {
    "text": "Is there data on how\nthis performs on, I mean, out of either\ndomain distribution?",
    "start": "3299479",
    "end": "3306680"
  },
  {
    "text": "So if you then showed a\nblue five and a blue two. Does it work better on those\nthan if you had not done",
    "start": "3306680",
    "end": "3314390"
  },
  {
    "text": "any red-green augmentation? Your question is that\nif we have any blue ones",
    "start": "3314390",
    "end": "3319550"
  },
  {
    "text": "or any auto-distribution,\nsomething like that? Yeah, it definitely works. So for example, we can\nput this example here,",
    "start": "3319550",
    "end": "3326660"
  },
  {
    "text": "is we change the background\nfrom green to blue.",
    "start": "3326660",
    "end": "3331920"
  },
  {
    "text": "So it can still work very well. So because in case you will\nlearn some digit information",
    "start": "3331920",
    "end": "3337040"
  },
  {
    "text": "and ignore this\ndomain information. So another variant of LISA\nis called intra-domain LISA.",
    "start": "3337040",
    "end": "3345320"
  },
  {
    "text": "Intra-domain LISA, it may\nbe we add some constraint on the domain level. So we want to interpret\nexamples with a different label",
    "start": "3345320",
    "end": "3353900"
  },
  {
    "text": "but same domain. So di is equal to dj, but\nyi is not equal to yj.",
    "start": "3353900",
    "end": "3361610"
  },
  {
    "text": "And in this case, the\nleft and the right ones are also from the\noriginal data set. All these are red\ntwo and red five.",
    "start": "3361610",
    "end": "3369680"
  },
  {
    "text": "And we can generate\na lot of images in between but with\ndifferent labels.",
    "start": "3369680",
    "end": "3375390"
  },
  {
    "text": "So in this case, we can see\neven the domain information is the same, but all of them\nare either associated",
    "start": "3375390",
    "end": "3382010"
  },
  {
    "text": "with the different labels. So the domain information\nis not the reason for the label changes.",
    "start": "3382010",
    "end": "3388400"
  },
  {
    "text": "So we can also make these models\nexactly ignore this domain information and only focus\non the environments--",
    "start": "3388400",
    "end": "3394880"
  },
  {
    "text": "focus on the digit information.",
    "start": "3394880",
    "end": "3397237"
  },
  {
    "text": "Any questions for this variant?",
    "start": "3401920",
    "end": "3403710"
  },
  {
    "text": "Let's move on. In practice, LISA aim to\ncombine this both of them",
    "start": "3410010",
    "end": "3418120"
  },
  {
    "text": "because each of them has\nits own applicable scopes.",
    "start": "3418120",
    "end": "3424852"
  },
  {
    "text": "The first one, for\nthe intra-label, it works better when\nthere are more domains.",
    "start": "3424852",
    "end": "3428680"
  },
  {
    "text": "Especially if you\nhave more domains, you can have more combination\nto apply the intra-label LISA.",
    "start": "3431260",
    "end": "3440650"
  },
  {
    "text": "And another thing is\nthe spurious correlation are not very strong.",
    "start": "3440650",
    "end": "3446260"
  },
  {
    "text": "In our practice, that\nintra-label LISA works better. And instead, the\nintra-domain LISA",
    "start": "3446260",
    "end": "3452369"
  },
  {
    "text": "works better when the\ndomain information is highly spuriously\ncorrelated with the label.",
    "start": "3452370",
    "end": "3459680"
  },
  {
    "text": "So in case, for example,\nthis data distribution is quite imbalanced. And there are much more even\n99% of green fives and red twos.",
    "start": "3459680",
    "end": "3472089"
  },
  {
    "text": "So in this case, such\nintra-domain LISA works better.",
    "start": "3472090",
    "end": "3475225"
  },
  {
    "text": "And in practice, LISA use\none type of parameter P select to determine when\nto use intra-label LISA",
    "start": "3478110",
    "end": "3485580"
  },
  {
    "text": "or when to use intra-domain\nLISA at each iteration.",
    "start": "3485580",
    "end": "3488910"
  },
  {
    "text": "What happens if you say you\ncannot apply either of these? So supposing you only have\nred twos and green fives?",
    "start": "3494550",
    "end": "3505030"
  },
  {
    "text": "Yeah. This is a very\ninteresting question. We already investigate\nin our other settings.",
    "start": "3505030",
    "end": "3511140"
  },
  {
    "text": "This is, in this case, you\ncan see that we call it under specification things.",
    "start": "3511140",
    "end": "3516570"
  },
  {
    "text": "Because these models\ncan infer the labels based on the background\ninformation only, or in informing the labels\nbased on the digit information.",
    "start": "3516570",
    "end": "3525060"
  },
  {
    "text": "We typically want to have--\nbuild a models with two heads. And each head represent\none kinds of things.",
    "start": "3525060",
    "end": "3531700"
  },
  {
    "text": "So first head maybe represent\nthe domain information, I mean, the color here. And also, another head would\nbe the label information",
    "start": "3531700",
    "end": "3540119"
  },
  {
    "text": "by sharing some layers. And we will use some unlabeled\ndata to make two heads produce",
    "start": "3540120",
    "end": "3547170"
  },
  {
    "text": "quite a different output. And in this case, we can--",
    "start": "3547170",
    "end": "3553090"
  },
  {
    "text": "when we have new\ndomains and the models aim to classify\nthe labels, we can",
    "start": "3553090",
    "end": "3558250"
  },
  {
    "text": "try to collect a few\nexamples to pick which heads that we want to use.",
    "start": "3558250",
    "end": "3565400"
  },
  {
    "text": "Yeah. If you want to see the\npaper, please email me, and I can send our paper to you.",
    "start": "3565400",
    "end": "3572930"
  },
  {
    "text": "So from the previous\nlecture we saw cycle there. Would something like\nthat be useful here",
    "start": "3572930",
    "end": "3579776"
  },
  {
    "text": "for data augmentation? A CycleGAN you could\npossibly use here.",
    "start": "3579776",
    "end": "3585920"
  },
  {
    "text": "It is a powerful data\naugmentation method.",
    "start": "3585920",
    "end": "3591349"
  },
  {
    "text": "You can even use some to\ndisentangle the representations and to shift something\nbetween different domains",
    "start": "3591350",
    "end": "3598010"
  },
  {
    "text": "and can even improve\nthe performance. Yeah. But in applying CycleGAN\nwould be a little bit complex.",
    "start": "3598010",
    "end": "3605490"
  },
  {
    "text": "So and need more efforts to\ntrain all the hyper parameters.",
    "start": "3605490",
    "end": "3609380"
  },
  {
    "text": "Is there a reason\nthat you choose either intra-label or\nintra-domain [INAUDIBLE]",
    "start": "3612775",
    "end": "3618470"
  },
  {
    "text": "in the same iteration? We choose either ones, because\neach one have its own pros",
    "start": "3618470",
    "end": "3627950"
  },
  {
    "text": "and cons. So in intra-- we actually,\nin practice, which chose intra-label LISA more.",
    "start": "3627950",
    "end": "3634040"
  },
  {
    "text": "But in some cases, intra-domain\nLISA also works very well.",
    "start": "3634040",
    "end": "3637250"
  },
  {
    "text": "Any other questions?",
    "start": "3653110",
    "end": "3654150"
  },
  {
    "text": "So for intra-domain,\ndoes it matter which label there you choose?",
    "start": "3662361",
    "end": "3668760"
  },
  {
    "text": "Yes. Right? Yes. Yeah. So how do you--\nhow do we decide?",
    "start": "3668760",
    "end": "3674427"
  },
  {
    "text": "To choose which labels? Yeah. You randomly pick some examples. I will choose a\nfull algorithm here.",
    "start": "3674427",
    "end": "3680089"
  },
  {
    "text": "So maybe you can take a\nlook what is the process.",
    "start": "3680090",
    "end": "3685310"
  },
  {
    "text": "So in the full algorithm\nof LISA is at first stage we randomly initialize\nthe model parameter theta.",
    "start": "3685310",
    "end": "3690970"
  },
  {
    "text": "And then we will\nsample one strategy from a Bernoulli distribution\nand with the high parameter",
    "start": "3690970",
    "end": "3697450"
  },
  {
    "text": "P select. And we will sample a\nbatch of examples, b.",
    "start": "3697450",
    "end": "3703934"
  },
  {
    "text": "And here, this stage we\nwill-- in this stage we will choose either intra-label\nLISA or intra-domain LISA.",
    "start": "3703934",
    "end": "3710980"
  },
  {
    "text": "So if s is equal to zero, and\nwe will use intra-label LISA. And for every examples\nwithin this batch,",
    "start": "3710980",
    "end": "3718299"
  },
  {
    "text": "we will sample another example\nthat satisfies yi equal to yj and the di not equal to dj.",
    "start": "3718300",
    "end": "3725170"
  },
  {
    "text": "And similarly, in\nintra-domain LISA, for every example\nin the batch, we will sample one\nexample that satisfies",
    "start": "3725170",
    "end": "3733150"
  },
  {
    "text": "the label is different\nbut the domain the same.",
    "start": "3733150",
    "end": "3736029"
  },
  {
    "text": "Then we will use\nsome interpreted-- we will interpolate\nthese examples and use the interpolated\nexamples to update the model.",
    "start": "3739720",
    "end": "3749220"
  },
  {
    "text": "Finally, we will repeat\nthe steps three and four several times until converge. So for every iterations, we will\nchoose either intra-label LISA",
    "start": "3749220",
    "end": "3756920"
  },
  {
    "text": "or intra-domain LISA. Could this be extended\nfrom, say, two label--",
    "start": "3756920",
    "end": "3763530"
  },
  {
    "text": "two classes or two domains? Yeah. We can use in a bunch\nof domains or classes.",
    "start": "3763530",
    "end": "3769335"
  },
  {
    "text": "[INAUDIBLE] for one label\nyou take all domains",
    "start": "3769335",
    "end": "3774420"
  },
  {
    "text": "and average all that? You mean for the\nintra-label one? For the Intra-Domain ones.",
    "start": "3774420",
    "end": "3780690"
  },
  {
    "text": "Oh, for the intra-domain\nones, we will use-- if we have more than one or\ntwo domains, and we will,",
    "start": "3780690",
    "end": "3786730"
  },
  {
    "text": "for each examples, we\nwill only pick examples with the same domain but without\nconsidering the labels only",
    "start": "3786730",
    "end": "3793535"
  },
  {
    "text": "because the labels\nare different. Yeah.",
    "start": "3793535",
    "end": "3795410"
  },
  {
    "text": "Any other questions for\nthe process of the-- for algorithm?",
    "start": "3804700",
    "end": "3809260"
  },
  {
    "text": "Let's move on. And here we choose some results\nto compare ERM, empirical risk",
    "start": "3817650",
    "end": "3824700"
  },
  {
    "text": "minimization. And the CORAL we mentioned\nas our regularization-based approach. And also LISA we mentioned\nhere the augmentation-based",
    "start": "3824700",
    "end": "3833220"
  },
  {
    "text": "approach. So the Camelyon is a two\nclass classification, is a binary class classification.",
    "start": "3833220",
    "end": "3838740"
  },
  {
    "text": "And with the\nsolution in domains. And the FMoW is\naround the 32 classes.",
    "start": "3838740",
    "end": "3846030"
  },
  {
    "text": "And a bunch of domains, R1\nis around the 33 classes,",
    "start": "3846030",
    "end": "3853350"
  },
  {
    "text": "if I remember correctly. And the Amazon has-- is also a binary classification. Amazon is a text data.",
    "start": "3853350",
    "end": "3859950"
  },
  {
    "text": "And in these four\ndata sets, LISA works better than the\nboth CORAL and the ERM.",
    "start": "3859950",
    "end": "3865140"
  },
  {
    "text": "And in the iWildCAM,\nthe CORAL works better than other methods.",
    "start": "3865140",
    "end": "3870900"
  },
  {
    "text": "Also in the OGB to use a\nmolecule property predictions, the ERM even works best\ncompared with other methods.",
    "start": "3870900",
    "end": "3880390"
  },
  {
    "text": "So in this case, we can\nsee different algorithms have its own advantages\nand disadvantages.",
    "start": "3880390",
    "end": "3888260"
  },
  {
    "text": "And you may choose\nwhich one you want to use based on the experimental\nresults in your practice.",
    "start": "3888260",
    "end": "3895060"
  },
  {
    "text": "So you may also see\nthat LISA do works well, and the image data also works\nwell, and the text data.",
    "start": "3901160",
    "end": "3908150"
  },
  {
    "text": "So how to apply Mixup\nin the text data? Because it's a little\nbit weird if you apply the Mixup on\nthe input level,",
    "start": "3908150",
    "end": "3916039"
  },
  {
    "text": "so on the original text data. So here, I will briefly\nmention the Manifold Mixup.",
    "start": "3916040",
    "end": "3924260"
  },
  {
    "text": "In the original Mixup, we\napply the Mixup on the input, and then catch the mixed image,\nand feeding into the feature",
    "start": "3924260",
    "end": "3931520"
  },
  {
    "text": "extractor to get the\nfeature representations, and then feed this\nfeature representation",
    "start": "3931520",
    "end": "3937280"
  },
  {
    "text": "to the classifier to\ndo the classification.",
    "start": "3937280",
    "end": "3942840"
  },
  {
    "text": "And in the Manifold\nMixup, we instead doing-- apply the Mixup on\nthe input, we use",
    "start": "3942840",
    "end": "3949250"
  },
  {
    "text": "a Mixup on the feature level. For example, in\nthe text domains, we typically do it on the top\noutput, the output output.",
    "start": "3949250",
    "end": "3957560"
  },
  {
    "text": "So we will change the\nfeature extractor at bird and apply the Mixup on the\nfeatures on the output output.",
    "start": "3957560",
    "end": "3964250"
  },
  {
    "text": "And but in image domain\nI use this example here. We have the dog features\nand the cat features.",
    "start": "3964250",
    "end": "3970310"
  },
  {
    "text": "And we apply Mixup here\nand get the mixed features,",
    "start": "3970310",
    "end": "3976470"
  },
  {
    "text": "and then feed these mixed\nfeatures to the classifier.",
    "start": "3976470",
    "end": "3980160"
  },
  {
    "text": "Any questions for this one?",
    "start": "3983020",
    "end": "3984400"
  },
  {
    "text": "So the final results I\nwould like to show you some invariance analysis.",
    "start": "3997230",
    "end": "4004339"
  },
  {
    "text": "So how to measure\nthe invariance. So the direct way to\nmeasure the invariance",
    "start": "4004340",
    "end": "4010000"
  },
  {
    "text": "is that when we get\nthe representations-- the environmental\nrepresentations-- we can try to use\nthis representation",
    "start": "4010000",
    "end": "4016960"
  },
  {
    "text": "to build a model, and\nlike a logistic regression or any other models,\nto predict the domains.",
    "start": "4016960",
    "end": "4024730"
  },
  {
    "text": "And we will use the accuracy\nof the domain prediction to measure whether we can find\na better invariance or not.",
    "start": "4024730",
    "end": "4032760"
  },
  {
    "text": "Another metrics\nthat we want to show is that we will measure the\ndivergence of the predictions",
    "start": "4032760",
    "end": "4038250"
  },
  {
    "text": "among the domains. So we will leverage\nthe representations of the predictors. The predictor means\nthe logits to see",
    "start": "4038250",
    "end": "4046289"
  },
  {
    "text": "that whether for\nevery classes we can see that if divergence\nbetween every domains",
    "start": "4046290",
    "end": "4051390"
  },
  {
    "text": "we will have one distribution\nabout the output, the representation distribution. And we will try to do\nsome pairwise comparison",
    "start": "4051390",
    "end": "4058260"
  },
  {
    "text": "between different--\namong different domains. And then, we will sum the\nresults for every classes.",
    "start": "4058260",
    "end": "4065670"
  },
  {
    "text": "And so, we compare\nLISA with ERM, also",
    "start": "4065670",
    "end": "4071130"
  },
  {
    "text": "with the Vanilla Mixup. The IRM, IB-IRM, REx\nis other methods-- other kinds of methods for\nregularization-based methods.",
    "start": "4071130",
    "end": "4079350"
  },
  {
    "text": "And we do have-- see that LISA\ncan lead to greater domain invariance than\nprior methods with",
    "start": "4079350",
    "end": "4086579"
  },
  {
    "text": "some explicit regularizers.",
    "start": "4086580",
    "end": "4089708"
  },
  {
    "text": "So both metrics, smaller value\nrepresents greater domain",
    "start": "4093380",
    "end": "4099580"
  },
  {
    "text": "invariance. Any questions for this?",
    "start": "4099580",
    "end": "4105029"
  },
  {
    "text": "And finally, I would\nlike to also do some comparison between\nregularizer-based",
    "start": "4116760",
    "end": "4122100"
  },
  {
    "text": "versus augmentation-based\nmethods. And in the\nregularizer-based methods,",
    "start": "4122100",
    "end": "4128250"
  },
  {
    "text": "I will recap the pros and cons. The first advantage\nhere is that it can generalize to all\nkinds of data networks",
    "start": "4128250",
    "end": "4135250"
  },
  {
    "text": "and has some\ntheoretical guarantees. And but it relies on\nthe design regularizers.",
    "start": "4135250",
    "end": "4141120"
  },
  {
    "text": "Also, sometimes the\nregularizers are too harsh. And in the\naugmentation-based method,",
    "start": "4141120",
    "end": "4147660"
  },
  {
    "text": "it is easier to understand\nand simple to implement. And we do not need to worry\nabout how to design very well",
    "start": "4147660",
    "end": "4155160"
  },
  {
    "text": "generalizable regularizers. And but sometimes, it's largely\nlimited to the classification",
    "start": "4155160",
    "end": "4161100"
  },
  {
    "text": "problems, also\nsome problem types if we want to apply some simple\naugmentation operators for text",
    "start": "4161100",
    "end": "4167910"
  },
  {
    "text": "or the image data.",
    "start": "4167910",
    "end": "4169859"
  },
  {
    "text": "Any other questions? Yeah. This is the plan for today. So we introduce the\ndomain translation problem",
    "start": "4174960",
    "end": "4183899"
  },
  {
    "text": "and gave the\ndefinition for this. And we introduce two\nkinds of algorithms.",
    "start": "4183899",
    "end": "4189068"
  },
  {
    "text": "The first one to adding explicit\nregularizers in the loss function to align\nthe representations.",
    "start": "4189069",
    "end": "4195000"
  },
  {
    "text": "And the second one, we\ndo some data augmentation to generate more datum and\nto learn domain invariance.",
    "start": "4195000",
    "end": "4203650"
  },
  {
    "text": "So hope we can reach the\ngoal and to understand what the domain generalization\nis, and also",
    "start": "4203650",
    "end": "4209310"
  },
  {
    "text": "to be familiar with the major\ndomain generation approaches. Yeah.",
    "start": "4209310",
    "end": "4214880"
  },
  {
    "text": "Any other questions for\nthe entire lecture, or?",
    "start": "4214880",
    "end": "4218215"
  },
  {
    "text": "What about domain generalization\nfor generative problems?",
    "start": "4223680",
    "end": "4229340"
  },
  {
    "text": "For generative problems,\nthere are not too much domain generation problems. But in cases you have\nmultiple domains,",
    "start": "4229340",
    "end": "4236280"
  },
  {
    "text": "you want to generate, for\nexample, some images in one",
    "start": "4236280",
    "end": "4242219"
  },
  {
    "text": "domains and some image\nin other domains, you can also apply some domain\ngeneration problems directly",
    "start": "4242220",
    "end": "4247770"
  },
  {
    "text": "to change the background\nof the generating models. But they do not have too\nmany advanced technologies",
    "start": "4247770",
    "end": "4254400"
  },
  {
    "text": "to apply this. Yeah.",
    "start": "4254400",
    "end": "4256810"
  },
  {
    "text": "Any other questions?",
    "start": "4267560",
    "end": "4268850"
  },
  {
    "text": "So I'm not sure\nif I missed this. But how do the\ninterpolation techniques generalize to text data?",
    "start": "4276220",
    "end": "4281719"
  },
  {
    "text": "Yeah. We can go back to this here. So this is for the image data? Yeah. For the text data, if you\nwant to apply the Mixup,",
    "start": "4281720",
    "end": "4291280"
  },
  {
    "text": "we will change it to text. Oh, right. The input to text. And the feature extractor\nwould be sometimes bird.",
    "start": "4291280",
    "end": "4297217"
  },
  {
    "text": "And you look at the\nfeature representations. You can choose this\nfeature representation and other kinds of input. Yeah.",
    "start": "4297217",
    "end": "4302620"
  },
  {
    "text": "And that to apply\ninterpolation here. That makes sense. Any other questions? OK.",
    "start": "4302620",
    "end": "4307945"
  },
  {
    "text": "If we do not have\nany other questions, so there are also\nsome reminders. So the project milestones\nare on Wednesday.",
    "start": "4313990",
    "end": "4321039"
  },
  {
    "text": "And that the next time\nis the lifelong learning. Lifelong learning is something\nto combine the techniques that we learned before.",
    "start": "4321040",
    "end": "4327260"
  },
  {
    "text": "So hope you can try\nit for the next time.",
    "start": "4327260",
    "end": "4331199"
  }
]