[
  {
    "start": "0",
    "end": "6210"
  },
  {
    "text": "OK. I guess, let's get started. Let's see. Is this working? Yes.",
    "start": "6210",
    "end": "13630"
  },
  {
    "text": "So I guess last\ntime, we have talked about unsupervised learning. And today, we're\ngoing to continue",
    "start": "13630",
    "end": "19600"
  },
  {
    "text": "with unsupervised learning. ",
    "start": "19600",
    "end": "26130"
  },
  {
    "text": "And first, we're going to\ncontinue with the moment method. ",
    "start": "26130",
    "end": "32619"
  },
  {
    "text": "And here we're going to talk\nabout higher order moments. ",
    "start": "32619",
    "end": "39850"
  },
  {
    "text": "And then, next, we're going\nto talk about something called clustering, or\nspectral clustering",
    "start": "39850",
    "end": "45230"
  },
  {
    "text": "in more technical words. ",
    "start": "45230",
    "end": "50783"
  },
  {
    "text": "So these are different type\nof unsupervised learning algorithms. So I guess just to\ncontinue with what",
    "start": "50783",
    "end": "57810"
  },
  {
    "text": "we had last time,\nlast time, we ended up with this mixture of Gaussians. ",
    "start": "57810",
    "end": "65950"
  },
  {
    "text": "The setup was that\nyou have some x which is sampled from a\nmixture of k Gaussians",
    "start": "65950",
    "end": "74580"
  },
  {
    "text": "with mean mu i and\ncovariance identity. And so last time, I\nthink, at the beginning,",
    "start": "74580",
    "end": "81689"
  },
  {
    "text": "we talked about case\n2, where you have a mixture of two Gaussians. And in that special case, you\ncan just take the second moment",
    "start": "81690",
    "end": "89190"
  },
  {
    "text": "of the Gaussian to\nrecover the mu i's. And then we moved down to talk\nabout cases bigger than two.",
    "start": "89190",
    "end": "96210"
  },
  {
    "text": "And in that case, we\nhave argued that if you take the second mixture, if you\ntake the second moment, then",
    "start": "96210",
    "end": "103590"
  },
  {
    "text": "this is something like 1\nover k times sum of mu i mu i transposed. And this is not\nenough to recover mu,",
    "start": "103590",
    "end": "115390"
  },
  {
    "text": "mu i's, because, given\nthis second moment, you still cannot identify the mu\ni's precisely because there are",
    "start": "115390",
    "end": "122230"
  },
  {
    "text": "multiple mu i's that can\nhave the same second moment. So that motivates us to\nconsider this third moment.",
    "start": "122230",
    "end": "131920"
  },
  {
    "text": "So the third moment is--  as we discussed, it\nis the expectation",
    "start": "131920",
    "end": "139510"
  },
  {
    "text": "of x tensor x tensor x. So this is the\nthird-order tensor in dimension d by d by d.",
    "start": "139510",
    "end": "148049"
  },
  {
    "text": "And let's compute what's the\nthird moment with the hope that the third moment will tell\nus enough about mu i's where",
    "start": "148050",
    "end": "153480"
  },
  {
    "text": "we can recover mu i's\nfrom the third moment. And that's indeed the case. So what we do here\nis the following.",
    "start": "153480",
    "end": "160030"
  },
  {
    "text": "So we compute the third moment. And I guess the initial step\nis always the same because you",
    "start": "160030",
    "end": "165990"
  },
  {
    "text": "have a mixture of k clusters. So what you do is\nyou have 1 over k times the sum of the\nmoment conditioned",
    "start": "165990",
    "end": "174950"
  },
  {
    "text": "on each of the cluster i\nwhere i is the cluster ID.",
    "start": "174950",
    "end": "180890"
  },
  {
    "text": "And now the question\nbecomes that if you have a Gaussian\ndrawn from-- if you have an x drawn from Gaussian,\nthen what's the third moment?",
    "start": "180890",
    "end": "188780"
  },
  {
    "text": "What's the expectation\nof the third moment? So what is this\nexpectation of x tensor",
    "start": "188780",
    "end": "193790"
  },
  {
    "text": "x tensor x conditioned on i? So let's do some kind of\nsimplification just to--",
    "start": "193790",
    "end": "202280"
  },
  {
    "text": "this is an abstraction\nin some sense so that we can make\nthe notation simpler. So suppose z is from\nsome Gaussian with mu--",
    "start": "202280",
    "end": "212720"
  },
  {
    "text": "let's call it a just to\ndistinguish it from the-- a and covariance identity.",
    "start": "212720",
    "end": "220760"
  },
  {
    "text": "And the question is, what is-- you have this lemma.",
    "start": "220760",
    "end": "226660"
  },
  {
    "text": "I guess the condition is this. And then our question is, what\nis the expectation of z tensor",
    "start": "226660",
    "end": "233350"
  },
  {
    "text": "z tensor z? And the claim is that\nthis is pretty much",
    "start": "233350",
    "end": "240400"
  },
  {
    "text": "equal to a tensor a tensor\na but with some caveats.",
    "start": "240400",
    "end": "245500"
  },
  {
    "text": "There are some other terms\nwhich are something like this. Let me write it\ndown and explain.",
    "start": "245500",
    "end": "251270"
  },
  {
    "text": "So this is from 1 to d,\nexpectation of x tensor el",
    "start": "251270",
    "end": "260519"
  },
  {
    "text": "tensor el plus the expectation\nel tensor expectation x.",
    "start": "260519",
    "end": "271830"
  },
  {
    "start": "271830",
    "end": "282500"
  },
  {
    "text": "So this is a formula-- oh, sorry, my bad. This is not x. This is z. There's no x in this lemma.",
    "start": "282500",
    "end": "288660"
  },
  {
    "text": "We already changed\nour notation to z. ",
    "start": "288660",
    "end": "296750"
  },
  {
    "text": "So note that expectation\nz is really literally a.",
    "start": "296750",
    "end": "306980"
  },
  {
    "text": "So, basically, you\nalready have a formula that expresses the third moment\nof z into a function of a.",
    "start": "306980",
    "end": "315380"
  },
  {
    "text": "That makes sense because\na decides everything. So everything at the end of\na should be a function of a.",
    "start": "315380",
    "end": "320930"
  },
  {
    "text": "So the reason why we still\nuse ez in this formula is because we want to implicitly\nsay that this is something",
    "start": "320930",
    "end": "329800"
  },
  {
    "text": "that is about the first moment. So maybe the more\nimportant thing is that this means\nthat you can compute--",
    "start": "329800",
    "end": "340840"
  },
  {
    "text": "so we can compute\na tensor a tensor",
    "start": "340840",
    "end": "346800"
  },
  {
    "text": "a from linear combinations\nof the third moment",
    "start": "346800",
    "end": "357409"
  },
  {
    "text": "and this first moment.  Why is it useful to get this?",
    "start": "357410",
    "end": "363210"
  },
  {
    "text": "I think this will\nbe clearer, why it's useful to get a tensor\na tensor a, it's useful--",
    "start": "363210",
    "end": "370160"
  },
  {
    "text": "or we'll see it in a moment. But this lemma\ntells us that if you",
    "start": "370160",
    "end": "375550"
  },
  {
    "text": "know the first moment\nand third moment, you can get a tensor\na tensor a from--",
    "start": "375550",
    "end": "381600"
  },
  {
    "text": "sorry. I'm messing up this letter here. So here is the z. ",
    "start": "381600",
    "end": "389710"
  },
  {
    "text": "OK.  And let's see.",
    "start": "389710",
    "end": "396875"
  },
  {
    "text": " I think I would-- any questions so far?",
    "start": "396875",
    "end": "404010"
  },
  {
    "text": "I guess it's not exactly\nclear why this lemma is useful at the current point.",
    "start": "404010",
    "end": "409380"
  },
  {
    "text": "I guess the main point is\nthat you can compute out what's the third moment\nwhen z is just a Gaussian.",
    "start": "409380",
    "end": "416155"
  },
  {
    "text": "And I think the proof-- and\nI'm going to show the proof-- I think the proof is\nnothing super interesting, but it tells you how to do\nthis kind of derivations",
    "start": "416155",
    "end": "424020"
  },
  {
    "text": "for the moment. And once you see it\nonce, then all the others become kind of trivial.",
    "start": "424020",
    "end": "429270"
  },
  {
    "text": "So how do you compute\nthe third moment? So what you do is you\ndo it for every entry.",
    "start": "429270",
    "end": "435280"
  },
  {
    "text": "So you say, look at the\nijk entry of this thing. Then this is just\nexpectation zi zj",
    "start": "435280",
    "end": "442390"
  },
  {
    "text": "zk, where zi denotes\nthe coordinates, the i's coordinates. And I think there's something.",
    "start": "442390",
    "end": "456000"
  },
  {
    "text": "Sorry for the note takers. I think I changed my rotation\nto v here just to be consistent.",
    "start": "456000",
    "end": "463669"
  },
  {
    "text": "Let me go back,\nchange this to v.",
    "start": "463670",
    "end": "471020"
  },
  {
    "text": "It's just a generic variable. It's just somehow, later, I used\nv. So let's change this to v. So what we do is that we--\njust to compute this moment,",
    "start": "471020",
    "end": "483720"
  },
  {
    "text": "we just, in some sense, do\nthis in a brute-force way. So what is zi zj zk?",
    "start": "483720",
    "end": "490139"
  },
  {
    "text": "You can write zi, which\nwould be vi plus some xi.",
    "start": "490140",
    "end": "495222"
  },
  {
    "text": "And vj and cj can\nbe vj plus xi j. And zk will be k plus xi ck.",
    "start": "495222",
    "end": "506509"
  },
  {
    "text": "We're using the fact\nthat z as a vector is equal to v plus some\nksi where ksi is from--",
    "start": "506510",
    "end": "512573"
  },
  {
    "text": "it's a spherical Gaussian. That's my definition\nof ksi, in some sense. Because ksi is\nequal to z minus v,",
    "start": "512574",
    "end": "518960"
  },
  {
    "text": "which has a spherical\nGaussian distribution. And then you can--",
    "start": "518960",
    "end": "525300"
  },
  {
    "text": "and, by the way, vi is the ith\ncoordinate, just to be clear. ",
    "start": "525300",
    "end": "531950"
  },
  {
    "text": "And then we can expand\nthis so there are eight terms in this product. So what are them? So one of the term is vi vj vk.",
    "start": "531950",
    "end": "539149"
  },
  {
    "text": "That's easy. v is deterministic\nbecause c is random. And some of the terms look\nlike expectation vi vj xi k.",
    "start": "539150",
    "end": "550400"
  },
  {
    "text": "One of the terms is this. Another term is expectation vi\nvk xi j plus expectation vj vk",
    "start": "550400",
    "end": "561200"
  },
  {
    "text": "xi i. And these terms\nwill be equal to 0",
    "start": "561200",
    "end": "567830"
  },
  {
    "text": "because the expectation\nof ksi is 0, and v is a\ndeterministic quantity. So that's why they\nare going to be 0.",
    "start": "567830",
    "end": "575329"
  },
  {
    "text": "And then we have the\nother three terms that looks like expectation vi\nksi j ksi k plus expectation vj",
    "start": "575330",
    "end": "585300"
  },
  {
    "text": "ksi i ksi k plus\nexpectation vk ksi i ksi j.",
    "start": "585300",
    "end": "593899"
  },
  {
    "text": "So these terms is a\nlittle bit different. Let me deal with it in a moment.",
    "start": "593900",
    "end": "599440"
  },
  {
    "text": "And the last type of term is\nthe product of the three xis.",
    "start": "599440",
    "end": "604520"
  },
  {
    "text": "So how do we deal with the\nrest of the four terms?",
    "start": "604520",
    "end": "609860"
  },
  {
    "text": "So the thing is that if you\nlook at expectation xi i xi k,",
    "start": "609860",
    "end": "615680"
  },
  {
    "text": "this is equal to what? This is equal to 0 if\ni is not equal to k",
    "start": "615680",
    "end": "621190"
  },
  {
    "text": "because if i is not equal to\nk, ksi i and ksi k are two independent random variables.",
    "start": "621190",
    "end": "626650"
  },
  {
    "text": "And you can factorize it to\nget expectation of ksi i ksi k. They're both 0, so you get 0.",
    "start": "626650",
    "end": "632350"
  },
  {
    "text": "And this is 1 if i is\nequal to k because-- OK, maybe let's have more steps.",
    "start": "632350",
    "end": "639140"
  },
  {
    "text": "So this is equal to expectation\nksi i squared, which is equal to 1, if i is equal to k.",
    "start": "639140",
    "end": "647170"
  },
  {
    "text": "So in summary,\nexpectation of ksi i ksi k is equal to indicator\nthat i is equal to k.",
    "start": "647170",
    "end": "659210"
  },
  {
    "text": "And you can also try to do\nthis with ksi i ksi j ksi k. And here you can still\ntry to do the same thing,",
    "start": "659210",
    "end": "666830"
  },
  {
    "text": "try to divide into\ndifferent cases, whether i, j, k are all the\nsame, or maybe two of the i",
    "start": "666830",
    "end": "672080"
  },
  {
    "text": "and j's are the same,\nand k is different. There are a few cases. And actually, if you\nenumerate all of those cases,",
    "start": "672080",
    "end": "677810"
  },
  {
    "text": "it turns out that it's always 0,\nregardless of the choice of i,",
    "start": "677810",
    "end": "685698"
  },
  {
    "text": "j, k, but for different reasons. For example, when i,\nj, k are all the same,",
    "start": "685698",
    "end": "692660"
  },
  {
    "text": "then this is the\nthird power of xi i. So it equals to\nexpectation xi i q. And that's 0 because the\nthird moment of Gaussian is 0.",
    "start": "692660",
    "end": "702250"
  },
  {
    "text": "And when i is equal to\nj but not equal to k, you can do another\ndifferent calculation.",
    "start": "702250",
    "end": "707800"
  },
  {
    "text": "But generally, you can\ndo all of calculations, and they're all equal to 0. Sometimes, I think the\nreason-- fundamental reason--",
    "start": "707800",
    "end": "713770"
  },
  {
    "text": "is that as long as you have--  even if you have all\ndegree polynomial,",
    "start": "713770",
    "end": "721240"
  },
  {
    "text": "all degree monomial of these\nksi i's it doesn't matter. It's always going to be 0.",
    "start": "721240",
    "end": "727100"
  },
  {
    "text": "The expectation is\nalways going to be 0. So these are all, in some\nsense, elementary calculations.",
    "start": "727100",
    "end": "733160"
  },
  {
    "text": "And then if you use this,\nthen you can continue here. You can get-- this\nis expect equals",
    "start": "733160",
    "end": "738470"
  },
  {
    "text": "to vi vj vk plus vi\ntimes the indicator j is equal to k\nplus vj indicator",
    "start": "738470",
    "end": "745970"
  },
  {
    "text": "i is equal to k plus vk\nindicator i is equal to j.",
    "start": "745970",
    "end": "752720"
  },
  {
    "text": "And this pretty much\ncompletes the proof. So then you just have to\nrewrite this in a tensor form.",
    "start": "752720",
    "end": "759960"
  },
  {
    "text": "So if you verify, I guess,\nhow do you write it? I guess if you\nverify this equation,",
    "start": "759960",
    "end": "766519"
  },
  {
    "text": "the target equation,\nentry by entry, then you see that this\nis actually exactly--",
    "start": "766520",
    "end": "772966"
  },
  {
    "text": "let's see. So this is vl. That's an el.",
    "start": "772966",
    "end": "778760"
  },
  {
    "text": "Sorry, v. l plus.",
    "start": "778760",
    "end": "787082"
  },
  {
    "start": "787082",
    "end": "797470"
  },
  {
    "text": "All right. So this is our target equation. This is what we got\nfor every entry.",
    "start": "797470",
    "end": "805149"
  },
  {
    "text": "So let's just verify\nthese two, verify that they are the same thing.",
    "start": "805150",
    "end": "810880"
  },
  {
    "text": "It's just a reorganization. So how do you verify that?",
    "start": "810880",
    "end": "815920"
  },
  {
    "text": "It's kind of like if you\ntake the i, j, k coordinates, do you see that-- so the question is, what\nis the i, j, k coordinate",
    "start": "815920",
    "end": "822563"
  },
  {
    "text": "of this guy is, right? So v tensor el tensor el--",
    "start": "822563",
    "end": "828490"
  },
  {
    "text": "the ijk coordinate is equal to-- it always has a vi there\nbecause vi is always there.",
    "start": "828490",
    "end": "838339"
  },
  {
    "text": "But the jk coordinate of\nthis el and el depends",
    "start": "838340",
    "end": "844610"
  },
  {
    "text": "on-- so, basically,\nyou have to-- the only possibility\nis that this is e-- ",
    "start": "844610",
    "end": "852140"
  },
  {
    "text": "so the only possibility that--\nso, basically, this is-- I guess, maybe, one way to\nwrite this is that if you really",
    "start": "852140",
    "end": "858589"
  },
  {
    "text": "do it, this is vi times el,\nthe jth coordinate of el,",
    "start": "858590",
    "end": "867020"
  },
  {
    "text": "and el the kth coordinate of el. ",
    "start": "867020",
    "end": "873610"
  },
  {
    "text": "And in what case is\nthe jth coordinate of el and kth coordinate\nof el-- are both non-zero?",
    "start": "873610",
    "end": "879430"
  },
  {
    "text": "The only case is that l is\nequal to j and l is equal to k.",
    "start": "879430",
    "end": "884790"
  },
  {
    "text": "That's the only case that\nthis can be non-zero. So that's why this is\nequal to vi times 1",
    "start": "884790",
    "end": "890430"
  },
  {
    "text": "when l is equal to\nj. l is equal to k. And the only way\nthis can happen is",
    "start": "890430",
    "end": "896990"
  },
  {
    "text": "that j is equal to k,\nwhen j is equal to k. Otherwise, it's going to be 0.",
    "start": "896990",
    "end": "904100"
  },
  {
    "text": "So that's how you verify. I don't expect you to verify\ncompletely on the fly.",
    "start": "904100",
    "end": "911990"
  },
  {
    "text": "But in some sense,\nthe exact formula doesn't matter that\nmuch either way. You only need to\nhave a formula that",
    "start": "911990",
    "end": "918079"
  },
  {
    "text": "depends on the third movement\nof v and also some formula--",
    "start": "918080",
    "end": "923510"
  },
  {
    "text": "basically, you just need\na formula for v. OK? So any questions so far?",
    "start": "923510",
    "end": "931260"
  },
  {
    "text": "So now let's see how we use it. So how we use it\nis the following, and you can kind of see what\nkind of things we exactly need.",
    "start": "931260",
    "end": "939650"
  },
  {
    "text": "So you need-- so now\nyou look at an x. x is a mixture of Gaussians.\nz was only a single Gaussian.",
    "start": "939650",
    "end": "946190"
  },
  {
    "text": "And you use the single\nGaussian on the--",
    "start": "946190",
    "end": "953414"
  },
  {
    "text": "the single Gaussian\nhas a building block to complete the moment of\nthe mixture of Gaussians.",
    "start": "953414",
    "end": "958635"
  },
  {
    "text": "And what you will\ndo is that, OK, when condition i\nbecomes a Gaussian and you apply the lemma,\nand you get 1 over k times",
    "start": "958635",
    "end": "967030"
  },
  {
    "text": "sum over i from 1 to k, and\nthen this is mu i tensor mu i",
    "start": "967030",
    "end": "972880"
  },
  {
    "text": "tensor mu i because mu i\nis taking the place of v.",
    "start": "972880",
    "end": "978880"
  },
  {
    "text": "And then you have the\nadditional three terms--",
    "start": "978880",
    "end": "985544"
  },
  {
    "text": "mu i tensor el tensor el,\nand el tensor mu i tensor",
    "start": "985544",
    "end": "995210"
  },
  {
    "text": "el plus el tensor el mu i.",
    "start": "995210",
    "end": "1003110"
  },
  {
    "text": " OK. So-- parentheses.",
    "start": "1003110",
    "end": "1012279"
  },
  {
    "text": "OK, and so, basically, the third\nmoment of x is a function of mu",
    "start": "1012280",
    "end": "1017860"
  },
  {
    "text": "i's. It's still a little bit messy. So what you do is\nyou say, I'm going to get rid of all of these\nterms by using the first moment,",
    "start": "1017860",
    "end": "1026829"
  },
  {
    "text": "the first in the moment of x. So what you do is that you\nfirst reorganize a little bit.",
    "start": "1026829",
    "end": "1032050"
  },
  {
    "text": "You get this somewhat\ncleanly-looking term, mu i tensor mu i tensor mu i.",
    "start": "1032050",
    "end": "1041260"
  },
  {
    "text": "And then you switch the\nk with the two sums. For the rest of three\nterms, you get sum",
    "start": "1041260",
    "end": "1047859"
  },
  {
    "text": "over l from 1 to d, 1 over\nk times sum over i from 1",
    "start": "1047859",
    "end": "1055330"
  },
  {
    "text": "to d mu i tensor el tensor el.",
    "start": "1055330",
    "end": "1060820"
  },
  {
    "text": "And you have the two\nother terms which-- I guess in theory.",
    "start": "1060820",
    "end": "1066539"
  },
  {
    "text": "You can imagine\nwhat they look like. They just are permutations. They are rotations of these\nterms in some changing order.",
    "start": "1066540",
    "end": "1072870"
  },
  {
    "text": "And now this one becomes\nthe first moment of x.",
    "start": "1072870",
    "end": "1078880"
  },
  {
    "text": "So you get 1 over k. You will get the same-- this is i, sorry.",
    "start": "1078880",
    "end": "1084400"
  },
  {
    "text": "This is i-- i plus\nsomething that depends",
    "start": "1084400",
    "end": "1099130"
  },
  {
    "text": "on the first moment of x. ",
    "start": "1099130",
    "end": "1105600"
  },
  {
    "text": "So what does this mean? It's that you can\nmove those three things to the left-hand side.",
    "start": "1105600",
    "end": "1112155"
  },
  {
    "text": " So, basically,\nthis means that we",
    "start": "1112155",
    "end": "1117310"
  },
  {
    "text": "can compute this tensor\nfrom the third moment",
    "start": "1117310",
    "end": "1135920"
  },
  {
    "text": "and the first moment. So that's basically\nour interface.",
    "start": "1135920",
    "end": "1143140"
  },
  {
    "text": "Once you have this tensor,\nthen the next step will be-- next step, so\nwe'll go from here.",
    "start": "1143140",
    "end": "1150390"
  },
  {
    "text": "So from this thing-- I guess let me write this.",
    "start": "1150390",
    "end": "1156410"
  },
  {
    "text": "I think I should have\nintroduced this, too, mu i's. And just for the notational\npurpose, so a to tensor 3",
    "start": "1156410",
    "end": "1165309"
  },
  {
    "text": "is just a shorthand for\na tensor a tensor a.",
    "start": "1165310",
    "end": "1171070"
  },
  {
    "text": "So, basically, what this\nwhole computation is saying is that now you can\ncompute this expectation--",
    "start": "1171070",
    "end": "1179470"
  },
  {
    "text": "sum of the third moment\nof mu i, and then you need to design an algorithm to\ncompute the mu i's from this.",
    "start": "1179470",
    "end": "1188390"
  },
  {
    "text": "And if you can do this question\nmark, then you're done. The whole thing is solved\nbecause you can first use",
    "start": "1188390",
    "end": "1195340"
  },
  {
    "text": "the moment to complete the\nthird tensor for mu i's. And then you can\nrun this algorithm.",
    "start": "1195340",
    "end": "1202227"
  },
  {
    "text": " There are actually some\ncleaner ways to deal with this. You don't have to deal\nwith these additional terms",
    "start": "1202228",
    "end": "1209170"
  },
  {
    "text": "in order to get-- there\nare some other ways to get this exact set of tensors\nas directly in a cleaner way.",
    "start": "1209170",
    "end": "1215559"
  },
  {
    "text": "But that requires a\nlot of other machinery. So that's why I'm only using\nthis relatively brute-force way",
    "start": "1215560",
    "end": "1223420"
  },
  {
    "text": "to get a circular tensor. But the point is that you can\nalways get something like this.",
    "start": "1223420",
    "end": "1228520"
  },
  {
    "text": "So now the problem becomes this\nso-called tensor decomposition problem. ",
    "start": "1228520",
    "end": "1237799"
  },
  {
    "text": "So, abstractly speaking, this\ntensor decomposition problem is something like you--",
    "start": "1237800",
    "end": "1244060"
  },
  {
    "text": "so, abstractly, you have\na sequence of vectors,",
    "start": "1244060",
    "end": "1250390"
  },
  {
    "text": "a1 up to an. These are all in\ndimension-- ak-- in our dimension d. So these are unknown.",
    "start": "1250390",
    "end": "1257590"
  },
  {
    "text": "And what you're given\nis a vector that looks like this from 1 to k.",
    "start": "1257590",
    "end": "1265100"
  },
  {
    "text": "And then your goal is\nto reconstruct ai's.",
    "start": "1265100",
    "end": "1272789"
  },
  {
    "text": " And you can also\nask about this-- for different orders\nof tensors, you",
    "start": "1272790",
    "end": "1279460"
  },
  {
    "text": "can also ask the same question. So questions--\nalso, for example,",
    "start": "1279460",
    "end": "1287390"
  },
  {
    "text": "you can have some other rth\norder tensor for some r that is bigger-- possibly bigger than 3.",
    "start": "1287390",
    "end": "1293668"
  },
  {
    "text": "And it turns out that you\ncan also get the fourth-order tensor-- the fourth-order power\nfrom this moment method.",
    "start": "1293668",
    "end": "1300232"
  },
  {
    "text": "You can take the fourth\nmoment of a beta, and you can get ax to tensor\n4 with some rearrangement",
    "start": "1300232",
    "end": "1305259"
  },
  {
    "text": "like we have done. So, basically, this is\na kind of interface.",
    "start": "1305260",
    "end": "1314770"
  },
  {
    "text": "It's where you basically\nreduce the moment. You reduce the [INAUDIBLE]\nproblem to the so-called tensor decomposition problem.",
    "start": "1314770",
    "end": "1321059"
  },
  {
    "text": "And this tensor decomposition\nproblem also has certain-- somewhat kind of like a--",
    "start": "1321060",
    "end": "1326730"
  },
  {
    "text": "let me also introduce some\nnotions for this, so-- notations. So the rank of the tensor--",
    "start": "1326730",
    "end": "1339280"
  },
  {
    "text": "so some basic notion\nfor the rank--",
    "start": "1339280",
    "end": "1344835"
  },
  {
    "text": " so I guess let's say a tensor\nb tensor c is a rank-1 tensor.",
    "start": "1344835",
    "end": "1356519"
  },
  {
    "text": "This is the definition\nof a rank-1 tensor. And then the rank\nof a tensor k--",
    "start": "1356520",
    "end": "1363370"
  },
  {
    "text": "tensor T-- is the\nminimum k such that T",
    "start": "1363370",
    "end": "1371940"
  },
  {
    "text": "can be written as a sum\nof rank-1, a sum of k",
    "start": "1371940",
    "end": "1382190"
  },
  {
    "text": "rank-1 tensors. Sometimes, this is also\ncalled CP decomposition.",
    "start": "1382190",
    "end": "1388549"
  },
  {
    "text": "So in some sense, the reason\nwhy this is called decomposition is that you observe this--",
    "start": "1388550",
    "end": "1394585"
  },
  {
    "text": "some of these\nrank-1 tensors, you want to decompose\nit into components. And each component is rank-1.",
    "start": "1394585",
    "end": "1402140"
  },
  {
    "text": "And this question is\nalso sometimes called CP decomposition because there\nare some other decompositions",
    "start": "1402140",
    "end": "1407180"
  },
  {
    "text": "for tensors that could also\nbe meaningful in other cases. ",
    "start": "1407180",
    "end": "1414744"
  },
  {
    "text": "But actually, it's\nalso fine to just call it tensor decomposition\nbecause this is the most popular\ndecomposition for tensors.",
    "start": "1414744",
    "end": "1423929"
  },
  {
    "text": "OK. So I guess now it becomes a\nvery modularized question.",
    "start": "1423930",
    "end": "1432320"
  },
  {
    "text": "It's an algorithmic question\nwhere, how do you figure out the components from--",
    "start": "1432320",
    "end": "1437779"
  },
  {
    "text": "given a lower tensor,\nhow do you figure out the lower components? So what I'm going to do is that\nI'm going to basically list",
    "start": "1437780",
    "end": "1447960"
  },
  {
    "text": "some of the existing results\nbut not really talking about details,\nbecause, actually, what",
    "start": "1447960",
    "end": "1453149"
  },
  {
    "text": "happens in this area-- I think this area becomes\nkind of very popular",
    "start": "1453150",
    "end": "1458400"
  },
  {
    "text": "around 2013, 2012. In the very beginning,\nI think, a few papers",
    "start": "1458400",
    "end": "1464970"
  },
  {
    "text": "kind of lay out the framework\nfor this whole thing. So how do you compute a moment? How do you convert it into a\ntensor decomposition problem?",
    "start": "1464970",
    "end": "1471120"
  },
  {
    "text": "And then those papers provide\nsome somewhat easy tensor decomposition problems,\nor they actually invoke some of the existing\ntensor decomposition",
    "start": "1471120",
    "end": "1477960"
  },
  {
    "text": "problems in those early papers. And then this field,\nsomewhat kind of like--",
    "start": "1477960",
    "end": "1485279"
  },
  {
    "text": "because this question\nbecomes two parts. One part is about, how\ndo you do the moment?",
    "start": "1485280",
    "end": "1490290"
  },
  {
    "text": "How do you turn the\nmoment into a tensor? And then the second part is,\nhow do you decompose the tensor?",
    "start": "1490290",
    "end": "1495670"
  },
  {
    "text": "So so people have-- there are a lot of papers\ninvolving some of my works as well.",
    "start": "1495670",
    "end": "1501100"
  },
  {
    "text": "But there are actually\na lot of works that tries to understand\nhow do you decompose",
    "start": "1501100",
    "end": "1506370"
  },
  {
    "text": "all different kinds of tensors,\nunder what conditions you can decompose. So what I'm going\nto do is I'm going",
    "start": "1506370",
    "end": "1514440"
  },
  {
    "text": "to list a few\nconditions that you can decompose these tensors\ncomputationally efficiently.",
    "start": "1514440",
    "end": "1522150"
  },
  {
    "text": "And those conditions, you\nwill turn into a condition for the upstream problem. For example, in the mixture\nof Gaussians problem,",
    "start": "1522150",
    "end": "1528420"
  },
  {
    "text": "you're going to have\nsome conditions. So just to set up\nkind of the basis,",
    "start": "1528420",
    "end": "1543309"
  },
  {
    "text": "let me see where I wrote this. ",
    "start": "1543310",
    "end": "1554910"
  },
  {
    "text": "Somehow I didn't notice this. So maybe the number 0 is that,\nin the most general case,",
    "start": "1554910",
    "end": "1563980"
  },
  {
    "text": "in the worst case in the more\nTCS language-- so we're calling it the worst case-- or\nin the most general case,",
    "start": "1563980",
    "end": "1569659"
  },
  {
    "text": "this problem is not solvable. So finding the ai's are\ncomputationally hard.",
    "start": "1569660",
    "end": "1580620"
  },
  {
    "text": " Actually, there are\nseveral layers here as well",
    "start": "1580620",
    "end": "1586740"
  },
  {
    "text": "if you want to\ndiscuss the details. In the very worst\ncase, actually, the ai's are not unique. You don't have a\nunique decomposition.",
    "start": "1586740",
    "end": "1593760"
  },
  {
    "text": "And when the\ndecomposition is unique, there are also cases where\nthe decomposition is unique,",
    "start": "1593760",
    "end": "1599040"
  },
  {
    "text": "but you cannot find them in a\ncomputationally efficient way. I think there's a question. So [INAUDIBLE] you\ncan put [INAUDIBLE]??",
    "start": "1599040",
    "end": "1608990"
  },
  {
    "start": "1608990",
    "end": "1614630"
  },
  {
    "text": "So if you take 3, you\nreplace 3 to be 2,",
    "start": "1614630",
    "end": "1620690"
  },
  {
    "text": "then it's pretty\nmuch like symmetric. This here is symmetric, but you\ncan also make it asymmetric.",
    "start": "1620690",
    "end": "1629629"
  },
  {
    "text": "But, yes, you are right. It's basically linear\nalgebraic stuff like FE. ",
    "start": "1629630",
    "end": "1637010"
  },
  {
    "text": "And this is a very\ngood question. So I think, in\nsome sense, as you will see in some\nof these questions",
    "start": "1637010",
    "end": "1642650"
  },
  {
    "text": "below, in some aspect,\nthe tensor decomposition is kind of closed to\nmatrix decomposition.",
    "start": "1642650",
    "end": "1649640"
  },
  {
    "text": "But there is one\nfundamental difference. So that fundamental\ndifference is what enables--",
    "start": "1649640",
    "end": "1658100"
  },
  {
    "text": "that makes these kind of tools\npowerful but also challenging. It's powerful in the sense that\nit's fundamentally powerful",
    "start": "1658100",
    "end": "1665150"
  },
  {
    "text": "because here, there is no\nrotational environment.",
    "start": "1665150",
    "end": "1673960"
  },
  {
    "start": "1673960",
    "end": "1679559"
  },
  {
    "text": "I guess this no rotational\nenvironment, also, you have to interpret\nit in a careful way. So what I mean is that\nsome of ai tensor 3",
    "start": "1679560",
    "end": "1690940"
  },
  {
    "text": "is not the same as some of\nthe rotation of ai tensor 3.",
    "start": "1690940",
    "end": "1699049"
  },
  {
    "text": "However, this is\ntrue for matrices. So if you have some\nof ai transposed,",
    "start": "1699050",
    "end": "1706820"
  },
  {
    "text": "this is the same as\nsome of r times ai. r is the rotation matrices. ",
    "start": "1706820",
    "end": "1714190"
  },
  {
    "text": "I guess it depends\non how you rotate it. My bad.",
    "start": "1714190",
    "end": "1719950"
  },
  {
    "text": "I think this--\nhow do I say this? I probably shouldn't\nsay this on the fly",
    "start": "1719950",
    "end": "1725020"
  },
  {
    "text": "without thinking about\nwhat's the best way to. ",
    "start": "1725020",
    "end": "1730169"
  },
  {
    "text": "I guess, technically, I\nshould rotate on the right. So maybe-- let me\nnot make it precise.",
    "start": "1730170",
    "end": "1739230"
  },
  {
    "text": "But I think maybe\none thing to realize is that if you have matrices,\nyou have a times a transposed, something like this, which\nis kind of like a sum of ai",
    "start": "1739230",
    "end": "1746725"
  },
  {
    "text": "ai transposed if you put all the\nai's as columns of calculating So then this is equal to a times\nr times r a transposed if r",
    "start": "1746725",
    "end": "1754230"
  },
  {
    "text": "is a rotation matrix. And you just cannot do this\nfor the tensors that often.",
    "start": "1754230",
    "end": "1762205"
  },
  {
    "text": " But what happens here\nis that if you permute-- if you have ai,\nand you permute it,",
    "start": "1762205",
    "end": "1770940"
  },
  {
    "text": "permuted indices to ai prime,\nwhere ai primes are just",
    "start": "1770940",
    "end": "1777000"
  },
  {
    "text": "permutations of ai,\nthen the resulting sum,",
    "start": "1777000",
    "end": "1782130"
  },
  {
    "text": "the third tensor,\nis still the same. So you only have the\nrotation symmetry, but no--",
    "start": "1782130",
    "end": "1790140"
  },
  {
    "text": "you only have permutation\nsymmetry, but no rotation symmetry. And this actually makes\nit somewhat powerful",
    "start": "1790140",
    "end": "1795210"
  },
  {
    "text": "because, in many cases,\nthis is the case. For a mixture of Gaussians, you\ncan permute all the centers,",
    "start": "1795210",
    "end": "1801750"
  },
  {
    "text": "and there still is\nthe same Gaussian. But you cannot rotate the\ncoordinate systems to make",
    "start": "1801750",
    "end": "1807990"
  },
  {
    "text": "the same-- you cannot rotate the-- at least you cannot take linear\ncombinations of the centers",
    "start": "1807990",
    "end": "1814470"
  },
  {
    "text": "to still maintain the\nsame nature of Gaussian. And I think this also\napplies to neural networks. I think, for neural\nnetworks, you",
    "start": "1814470",
    "end": "1820255"
  },
  {
    "text": "have the permutation\nsymmetry where you can permute the neurons in\nintermediate layers, and also",
    "start": "1820255",
    "end": "1827400"
  },
  {
    "text": "the associated edges. And you can still\nmaintain the functionality of the neural network\nexactly the same.",
    "start": "1827400",
    "end": "1833220"
  },
  {
    "text": "But you cannot do arbitrary\nrotations in it because you have the nonlinearity\nwith activations.",
    "start": "1833220",
    "end": "1839890"
  },
  {
    "text": "So, yeah, but I guess\nthis part is supposed to be somewhat abstract because\nif you see a lot of math,",
    "start": "1839890",
    "end": "1848010"
  },
  {
    "text": "then you can probably understand\nthis a little more better. But anyway, so there are\nsome fundamental differences",
    "start": "1848010",
    "end": "1854010"
  },
  {
    "text": "between this and linear algebra. So that's why tensor\ndecompositions becomes difficult--",
    "start": "1854010",
    "end": "1860537"
  },
  {
    "text": "especially the work is. OK. Going back to the list\nof questions, as I said,",
    "start": "1860537",
    "end": "1868660"
  },
  {
    "text": "the starting point is\nin the general case, you cannot hope to do anything. But there are many cases\nwhere you can do something.",
    "start": "1868660",
    "end": "1875610"
  },
  {
    "text": "So the easiest case is\nthe orthogonal case.  So orthogonal case means that\nif a1 up to ak are orthogonal--",
    "start": "1875610",
    "end": "1886260"
  },
  {
    "start": "1886260",
    "end": "1891450"
  },
  {
    "text": "and in this case, actually,\nthis is the closest to the eigenvector case. So here you can say that then\nai is actually the global--",
    "start": "1891450",
    "end": "1901485"
  },
  {
    "text": " each of these ai--",
    "start": "1901485",
    "end": "1906630"
  },
  {
    "text": "each of ai is the\nglobal minimizer.",
    "start": "1906630",
    "end": "1917650"
  },
  {
    "text": "There are multiple\nglobal minimizers. So that's why each of them is\na global minimizer-- maximizer, actually--",
    "start": "1917650",
    "end": "1923440"
  },
  {
    "text": "of this objective function\nwhere you maximize the l2 norm--",
    "start": "1923440",
    "end": "1929799"
  },
  {
    "text": "maximize this tensor\npicked by a rank-1 tensor. So I guess if you're not\nfamiliar with the notation,",
    "start": "1929800",
    "end": "1938130"
  },
  {
    "text": "then what I really mean is that\ntake this sum of Tijk times xi xj xk.",
    "start": "1938130",
    "end": "1945270"
  },
  {
    "text": "So this is the extension of the\nquadratic form for matrices. So suppose you have a matrix.",
    "start": "1945270",
    "end": "1950970"
  },
  {
    "text": "Then this is the quadratic form. And for tensor, this\nis this tensor form. ",
    "start": "1950970",
    "end": "1958200"
  },
  {
    "text": "So eigenvectors can\nbe defined in this way if you change the\ntensor to the matrix because an eigenvector is what's\nmaximizing the quadratic form",
    "start": "1958200",
    "end": "1966240"
  },
  {
    "text": "for the matrix. So in some sense, in this\nsense, the components are some kind of eigenvector.",
    "start": "1966240",
    "end": "1973409"
  },
  {
    "text": "And then you can find this. So this is an\ninteresting property. So this is saying that ai is\nkind of like eigenvectors of T.",
    "start": "1973410",
    "end": "1988230"
  },
  {
    "text": "And also, we can find it. It's not trivial to find it. But we can find ai's\nin polynomial time.",
    "start": "1988230",
    "end": "1998840"
  },
  {
    "text": "And actually, the way\nto find it is that you try to solve this optimization. And it's one way to\nfind it is that you",
    "start": "1998840",
    "end": "2004095"
  },
  {
    "text": "try to solve this optimization\nproblem back when you use that. So that's one way, one case. And another case is that--",
    "start": "2004095",
    "end": "2011755"
  },
  {
    "text": "a more general case that you can\nhave is the independent case. So it turns out\nthat if a1 up to ak",
    "start": "2011755",
    "end": "2018850"
  },
  {
    "text": "are linearly independent,\nthen this is also a good case.",
    "start": "2018850",
    "end": "2027260"
  },
  {
    "text": "You can find this\nin polynomial time. I think the algorithm is\ncalled Jenrich's algo.",
    "start": "2027260",
    "end": "2033580"
  },
  {
    "text": "I'm not going to describe\nall of this algorithm, just because it will\ntake too much time.",
    "start": "2033580",
    "end": "2038968"
  },
  {
    "text": "And then, sometimes,\nthese are things that you can-- as\nlong as you have some kind of basic\nknowledge, you",
    "start": "2038968",
    "end": "2044350"
  },
  {
    "text": "can search over the\nliterature, and there are many papers about this. ",
    "start": "2044350",
    "end": "2052330"
  },
  {
    "text": "But these are-- so 1\nand 2 are both about so-called undercomplete case.",
    "start": "2052330",
    "end": "2058869"
  },
  {
    "text": "1, 2 are the so-called\nundercomplete case,",
    "start": "2058870",
    "end": "2069800"
  },
  {
    "text": "which just really means that\nk, the number of components, is less than d. You can see that\nnumber 1 and number 2",
    "start": "2069800",
    "end": "2077719"
  },
  {
    "text": "can only happen when\nk is less than d because if k is\nbigger than d, there is no way that a1 up to an\nare linearly independent.",
    "start": "2077719",
    "end": "2085408"
  },
  {
    "text": "It's because your\nnumber of components is bigger than dimension. So they cannot be\nlinearly independent.",
    "start": "2085409",
    "end": "2090619"
  },
  {
    "text": "But actually, you can also do\nthis for overcomplete case. Overcomplete case\nare still possible--",
    "start": "2090620",
    "end": "2097520"
  },
  {
    "text": "are still possible\nin certain cases. So there are several\ndifferent ways",
    "start": "2097520",
    "end": "2103950"
  },
  {
    "text": "to deal with overcomplete case,\nwhich means k is bigger than d. So the first one is that you can\nlook at higher-order tensors.",
    "start": "2103950",
    "end": "2110714"
  },
  {
    "start": "2110715",
    "end": "2115970"
  },
  {
    "text": "So you can say that suppose\na1 tensor 2 up to ak tensor 2",
    "start": "2115970",
    "end": "2126830"
  },
  {
    "text": "are independent. This is a much relaxed\ncondition that a1 up to ak",
    "start": "2126830",
    "end": "2132563"
  },
  {
    "text": "are linearly independent\nbecause now you have a higher dimension. So now this only requires\nk needs to be less than d",
    "start": "2132563",
    "end": "2140780"
  },
  {
    "text": "squared to make this\npossible to happen. And suppose this is true.",
    "start": "2140780",
    "end": "2147200"
  },
  {
    "text": "Then you can just replace\nai by ai tensor 2.",
    "start": "2147200",
    "end": "2152589"
  },
  {
    "text": "So you can recover ai from\nthe sixth-order tensor.",
    "start": "2152590",
    "end": "2160070"
  },
  {
    "text": "So you recover from\na1 tensor power 2 to the tensor power\n3, i from 1 to k,",
    "start": "2160070",
    "end": "2168440"
  },
  {
    "text": "which is still the same\nas the sixth-order tensor. And how do you do it? You just invoke the third-order\ntensor on ai to the power 2.",
    "start": "2168440",
    "end": "2177069"
  },
  {
    "text": "And then, after you\nget ai to the power 2, you can get ai by just\ntaking the square root.",
    "start": "2177070",
    "end": "2183000"
  },
  {
    "text": "So this relaxes the\nrestriction on the k",
    "start": "2183000",
    "end": "2188240"
  },
  {
    "text": "but with the cost of\nestimating the sixth moment,",
    "start": "2188240",
    "end": "2193490"
  },
  {
    "text": "because how do you get this? This is the thing with\nr to the d to the sixth. So you have to\nsomehow do something",
    "start": "2193490",
    "end": "2199670"
  },
  {
    "text": "with the sixth moment. And it will be less\nsimple and efficient. And, well, another slightly\nclever way to do this",
    "start": "2199670",
    "end": "2206480"
  },
  {
    "text": "is that you can do fourth-order\ntensor with the same condition.",
    "start": "2206480",
    "end": "2214490"
  },
  {
    "text": "So you say that-- fourth-order generic tensor.",
    "start": "2214490",
    "end": "2220220"
  },
  {
    "text": "And what does generic\ntensor really mean? It means that you exclude--",
    "start": "2220220",
    "end": "2225455"
  },
  {
    "text": " excluding algebraic\nset of measure 0.",
    "start": "2225455",
    "end": "2238180"
  },
  {
    "text": "So you exclude a small set of-- a measured 0 set of tensors. And except those kind of\ntensors, you can do this.",
    "start": "2238180",
    "end": "2246460"
  },
  {
    "text": "And this is saying that when\nk is less than d squared,",
    "start": "2246460",
    "end": "2253119"
  },
  {
    "text": "you can recover ai from\nthe fourth tensor, right?",
    "start": "2253120",
    "end": "2262240"
  },
  {
    "text": " So before, if you do\na trivial reduction,",
    "start": "2262240",
    "end": "2267430"
  },
  {
    "text": "you get the third-- you need\nto use the sixth-order tensor. But now you only have to\nuse the fourth-order tensor.",
    "start": "2267430",
    "end": "2274040"
  },
  {
    "text": "And this algorithm\nis called FOOBI. ",
    "start": "2274040",
    "end": "2282340"
  },
  {
    "text": "And you can also have a\nrobust version of this. This algorithm by\nitself is not robust.",
    "start": "2282340",
    "end": "2287350"
  },
  {
    "text": "You can also have\nrobust versions of this. I guess let me not write\ndown these references.",
    "start": "2287350",
    "end": "2292480"
  },
  {
    "text": "I'll add the references\nlater, I guess. If I could just\nget the initials,",
    "start": "2292480",
    "end": "2299130"
  },
  {
    "text": "I think these are\nsome references like this, where you\ncan get a robust version of these algorithms.",
    "start": "2299130",
    "end": "2305540"
  },
  {
    "text": "And if you want to be more\nambitious-- so you want to say,",
    "start": "2305540",
    "end": "2311708"
  },
  {
    "text": "that I want to even deal\nwith third-order tensor, then what you can\ndo is you can say you can have random tensors.",
    "start": "2311708",
    "end": "2320680"
  },
  {
    "text": "And by random, it means\nthat if you assume ai's are randomly\ngenerated unit vectors.",
    "start": "2320680",
    "end": "2338137"
  },
  {
    "text": "I guess whether it's unit\nvectors is not that important. But for convenience, let's\nsay they are all unit vectors, and they're all randomly\ndistributed on a sphere.",
    "start": "2338137",
    "end": "2346850"
  },
  {
    "text": "And then, for even\nthird-order tensor,",
    "start": "2346850",
    "end": "2359100"
  },
  {
    "text": "k can be as large as d to 1.5.",
    "start": "2359100",
    "end": "2366660"
  },
  {
    "text": "So you can have kind\nof overcomplete case even with third-order tensor. ",
    "start": "2366660",
    "end": "2372930"
  },
  {
    "text": "And there are some references\nhere, which, I guess, I'll add it to the\nnotes eventually.",
    "start": "2372930",
    "end": "2379869"
  },
  {
    "text": "OK. OK cool. So this is just a\nvery quick list, kind of probably a little\nboring list of references.",
    "start": "2379870",
    "end": "2387300"
  },
  {
    "text": "But I guess you see\nthe rough idea, right? So you can, for various\nconditions for the component",
    "start": "2387300",
    "end": "2392710"
  },
  {
    "text": "ai's, you can have\nvarious kind of algorithms and different results.",
    "start": "2392710",
    "end": "2399230"
  },
  {
    "text": "So, typically, if you have\nmore restrictions on ai's, you get stronger results, right? So the strongest one would be\nyou assume they are random.",
    "start": "2399230",
    "end": "2406960"
  },
  {
    "text": "Then you can even decompose\novercomplete answers",
    "start": "2406960",
    "end": "2413380"
  },
  {
    "text": "when the order is only 3. But if you don't have\nthat strong assumption, you have to go with the\nfourth-order tensor or even",
    "start": "2413380",
    "end": "2420460"
  },
  {
    "text": "sixth-other tensor if you don't\nuse the right [INAUDIBLE].. So this is basically what's\ngoing on in this area.",
    "start": "2420460",
    "end": "2427240"
  },
  {
    "text": "And you can see, there\nare many, many papers that deal with different\nkind of setups.",
    "start": "2427240",
    "end": "2432490"
  },
  {
    "text": "So I will add some references\nto the lecture notes. But generally, this\nis something you",
    "start": "2432490",
    "end": "2437770"
  },
  {
    "text": "can kind of search on internet. And they are just-- before we conclude\nthis part, there",
    "start": "2437770",
    "end": "2444250"
  },
  {
    "text": "are other latent variables\nthat can be done-- ",
    "start": "2444250",
    "end": "2452910"
  },
  {
    "text": "can be done by moment\nmethod, or method of moment,",
    "start": "2452910",
    "end": "2460542"
  },
  {
    "text": "using the same strategy, where\nyou first complete a moment, you turn it into a tensor\ndecomposition problem so you can do the so-called ICA,\nindependent component analysis,",
    "start": "2460542",
    "end": "2468970"
  },
  {
    "text": "you can do the\nhidden Markov models, and you can also\ndo topic models.",
    "start": "2468970",
    "end": "2475140"
  },
  {
    "text": "I think there are\neven more than this. And I'm just listing a few\nthat are most prominent.",
    "start": "2475140",
    "end": "2480720"
  },
  {
    "text": "So these are all viable models\nfor unsupervised learning. And for each of\nthese, you can try",
    "start": "2480720",
    "end": "2486922"
  },
  {
    "text": "to compute certain\nkind of moments and rearrange your moments\nso that you get a tensor and then decompose the tensor\nto construct the true pattern.",
    "start": "2486923",
    "end": "2494490"
  },
  {
    "start": "2494490",
    "end": "2499690"
  },
  {
    "text": "Any questions so far?  What do you get if,\nsay, for example, it's",
    "start": "2499690",
    "end": "2507616"
  },
  {
    "text": "a third-order tensor?  So you want to activate\nit based on [INAUDIBLE]..",
    "start": "2507616",
    "end": "2514570"
  },
  {
    "text": "Right.  I guess it would be more\ngeneral [INAUDIBLE] tensor.",
    "start": "2514570",
    "end": "2522600"
  },
  {
    "text": "It's more-- [INAUDIBLE] So [INAUDIBLE],, is\nthere, say, [INAUDIBLE]..",
    "start": "2522600",
    "end": "2534900"
  },
  {
    "text": "I don't [INAUDIBLE]. What is the first [INAUDIBLE]? ",
    "start": "2534900",
    "end": "2543060"
  },
  {
    "text": "I think-- let me-- maybe I didn't-- let\nme try to answer, and then you can clarify if\nI didn't answer the question.",
    "start": "2543060",
    "end": "2549089"
  },
  {
    "text": "So I guess the flow just\nis something like you first start with the data. You compute some tensor, maybe\nthis, or maybe fourth-order--",
    "start": "2549090",
    "end": "2558330"
  },
  {
    "text": "maybe I said fourth-- here. And, of course, you cannot\ncompute this exactly. You compute this approximately.",
    "start": "2558330",
    "end": "2565320"
  },
  {
    "text": "You have some error in\nestimating this fourth moment. And you know that if you\ndon't have any error, then",
    "start": "2565320",
    "end": "2571050"
  },
  {
    "text": "this will be something\nlike some of ai to the tensor 4, i from 1 to k. And then you decompose.",
    "start": "2571050",
    "end": "2578370"
  },
  {
    "text": "You get ai's. And I guess-- how does\nthe dependency kind of-- so I guess one\nthing is whether it's",
    "start": "2578370",
    "end": "2585750"
  },
  {
    "text": "overcomplete or\nundercomplete, right? So why does that matter? That matters because this k is--",
    "start": "2585750",
    "end": "2593250"
  },
  {
    "text": "what is k? In a mixture of Gaussians,\nk is number of mixtures. So if you can handle\novercomplete tensor",
    "start": "2593250",
    "end": "2599339"
  },
  {
    "text": "decomposition, that means\nthat for the original problem, you can handle more\nthan d mixture.",
    "start": "2599340",
    "end": "2605760"
  },
  {
    "text": "The number of mixtures you can\nhandle is more than dimension. And if you can only do\nundercomplete tensors,",
    "start": "2605760",
    "end": "2611830"
  },
  {
    "text": "then your number of mixtures\nhas to be less than the matrix. That's why people care\nabout overcomplete tensors.",
    "start": "2611830",
    "end": "2617980"
  },
  {
    "text": "My question is, [INAUDIBLE]\nexpectation [INAUDIBLE]",
    "start": "2617980",
    "end": "2627560"
  },
  {
    "text": "with [INAUDIBLE]\nlarger k [INAUDIBLE].. With larger k?",
    "start": "2627560",
    "end": "2632790"
  },
  {
    "text": "The k here is something fixed. It's not about--\nso I guess there is another thing, which is\nk is the number of mixtures",
    "start": "2632790",
    "end": "2642045"
  },
  {
    "text": "in our data. It's something fixed. Consider. So I guess maybe what you're\nasking is this empirical thing.",
    "start": "2642045",
    "end": "2652760"
  },
  {
    "text": " So the real thing is\nthat you work on this.",
    "start": "2652760",
    "end": "2659880"
  },
  {
    "text": "And then you say this is\napproximately equal to the sum of ax to tensor 4.",
    "start": "2659880",
    "end": "2665760"
  },
  {
    "text": "And then you decompose\nthat approximate version. So you also need your algorithm,\nyour decomposition algorithm,",
    "start": "2665760",
    "end": "2671640"
  },
  {
    "text": "to be robust to some\nerrors because you don't know exactly this thing,\nthis lower tensor, exactly.",
    "start": "2671640",
    "end": "2679080"
  },
  {
    "text": "You only know approximate\nversion of it. ",
    "start": "2679080",
    "end": "2684800"
  },
  {
    "text": "Am I answering? I'm not answering the question? Go ahead. Maybe I'm not answering\nthe right question.",
    "start": "2684800",
    "end": "2691060"
  },
  {
    "text": "[INAUDIBLE]  Right. [INAUDIBLE]",
    "start": "2691060",
    "end": "2697410"
  },
  {
    "text": "This is the tensor\ndecomposition. Right. You can think of\ntensor decomposition",
    "start": "2697410",
    "end": "2702980"
  },
  {
    "text": "as a low-rank approximation\nfor the tensors. Yes. So [INAUDIBLE].",
    "start": "2702980",
    "end": "2710085"
  },
  {
    "text": "So it's a [INAUDIBLE] best\napproximation [INAUDIBLE]..",
    "start": "2710085",
    "end": "2717830"
  },
  {
    "text": "So all of these\ntheorems so far I listed, they all work\nfor approximate version,",
    "start": "2717830",
    "end": "2723920"
  },
  {
    "text": "even though I didn't really talk\nabout the approximate version yet. I didn't talk about\napproximation explicitly.",
    "start": "2723920",
    "end": "2731900"
  },
  {
    "text": "So in some sense,\nthe first-order bet is that even you don't\nhave an approximation. You get exactly a\nlow-rank tensor.",
    "start": "2731900",
    "end": "2738940"
  },
  {
    "text": "You have to be able\nto decompose it. Even that's nontrivial, right? So for matrices, it's trivial\nbecause you just take SVD.",
    "start": "2738940",
    "end": "2746410"
  },
  {
    "text": "But for tensors,\nit's not trivial. So that's why the\nfirst-order bet is to say, I get exact low-rank\ntensor, I can decompose it.",
    "start": "2746410",
    "end": "2752470"
  },
  {
    "text": "And then the second question\nis the so-called robustness, which means that you get\napproximately low-rank tensor,",
    "start": "2752470",
    "end": "2757630"
  },
  {
    "text": "how do you decompose it? I think all of these\nalgorithms, I think, are robust.",
    "start": "2757630",
    "end": "2763270"
  },
  {
    "text": "And there are some\nrobust version of them. And typically, if you don't\ncare about the optimal sample",
    "start": "2763270",
    "end": "2770260"
  },
  {
    "text": "efficiency, then\nthey're all robust just for trivial reasons.",
    "start": "2770260",
    "end": "2776093"
  },
  {
    "text": "But if you really\ncare about exactly how many samples and\nhow robust they are, it becomes a little\ntricky because you",
    "start": "2776093",
    "end": "2781390"
  },
  {
    "text": "have to talk about\nsample efficiency, how does the\nconcentration work, so.",
    "start": "2781390",
    "end": "2788168"
  },
  {
    "text": "[INAUDIBLE] find the\nlargest [INAUDIBLE].. ",
    "start": "2788168",
    "end": "2798674"
  },
  {
    "text": "Yeah, you can kind\nof think of the ai's as the largest eigenvectors. Largest eigenvectors.",
    "start": "2798675",
    "end": "2803839"
  },
  {
    "text": "Yes. You can roughly\nthink of that, yeah. ",
    "start": "2803840",
    "end": "2811434"
  },
  {
    "text": "OK? That's good? OK, cool. OK, sounds good. So I guess-- OK, cool.",
    "start": "2811434",
    "end": "2817010"
  },
  {
    "text": "So then I'm going to move\non to the last subtopic in this course, I guess.",
    "start": "2817010",
    "end": "2823125"
  },
  {
    "text": "It's still about\nunsupervised learning, but it's about a\nslightly different type of unsupervised learning,\nwhich is more like class III.",
    "start": "2823125",
    "end": "2830030"
  },
  {
    "text": "And you can see that we are\nstill doing spectral methods. We're still doing some kind\nof spectral decomposition.",
    "start": "2830030",
    "end": "2835970"
  },
  {
    "text": "But it's decomposing a\nslightly a different way.",
    "start": "2835970",
    "end": "2841650"
  },
  {
    "text": "I guess you will see once\nI formulate a problem, and then you can\nsee that before--",
    "start": "2841650",
    "end": "2848550"
  },
  {
    "text": "with the tensor method,\nyou are building some pairwise\ninformation between the coordinates-- or\nthreewise information",
    "start": "2848550",
    "end": "2856080"
  },
  {
    "text": "between the coordinates\nof the data, right? So here, from now\non, I'm going to talk",
    "start": "2856080",
    "end": "2861660"
  },
  {
    "text": "about a different\ntype of approach where you build pairwise\ninformation between the data points.",
    "start": "2861660",
    "end": "2867589"
  },
  {
    "text": "And then you do\nsomething on top of that. ",
    "start": "2867590",
    "end": "2872950"
  },
  {
    "text": "So I guess I'll\nspecify more clearly. So spectral clustering--",
    "start": "2872950",
    "end": "2882200"
  },
  {
    "text": "So I'm going to\ndiscuss, actually, a bunch of different\nalgorithmal setups",
    "start": "2882200",
    "end": "2887380"
  },
  {
    "text": "under this broad framework. This whole spectral cluster\nkind of framework, I think,",
    "start": "2887380",
    "end": "2892630"
  },
  {
    "text": "is proposed by Shi\nand Malik around 2000.",
    "start": "2892630",
    "end": "2901309"
  },
  {
    "text": "I think also Andrew Ng, Michael\nJordan, and Weiss in 2001.",
    "start": "2901310",
    "end": "2914190"
  },
  {
    "text": "Maybe this is 2001\nand this is 2000. I don't have the references\nin the lecture notes.",
    "start": "2914190",
    "end": "2920160"
  },
  {
    "text": "So it has been,\nlike, 20 years old. So I'm going to kind\nof discuss a bunch",
    "start": "2920160",
    "end": "2925230"
  },
  {
    "text": "of classical things about this. And also, next\nlecture, I'm going to talk about my own work, which\nkind of built on top of this",
    "start": "2925230",
    "end": "2933480"
  },
  {
    "text": "to get it to a deep\nlearning case-- to extend it to the\ndeep learning case.",
    "start": "2933480",
    "end": "2940230"
  },
  {
    "text": "So the general idea is\nthat suppose you have-- so we are given n data points.",
    "start": "2940230",
    "end": "2946115"
  },
  {
    "text": " Let's call them x 1 up to x n.",
    "start": "2946115",
    "end": "2953609"
  },
  {
    "text": "And let's say we are given-- for the moment, we are\ngiven a similarity matrix.",
    "start": "2953610",
    "end": "2958940"
  },
  {
    "text": "And don't ask me\nhow to get this. Let's just assume that we\nhave a similarity matrix G which is of dimension n by n.",
    "start": "2958940",
    "end": "2967910"
  },
  {
    "text": "Actually, it's going\nto be a problem to construct a similarity\nmatrix to some extent. But for the moment, I\nsay we will have this.",
    "start": "2967910",
    "end": "2973640"
  },
  {
    "text": "In some cases, we do have this\nsimilarity matrix and this G,",
    "start": "2973640",
    "end": "2978769"
  },
  {
    "text": "where each of these\nentries of this matrix is doing some similarity--",
    "start": "2978770",
    "end": "2986780"
  },
  {
    "text": "is capturing a similarity\nbetween two data points, x i and x j.",
    "start": "2986780",
    "end": "2991981"
  },
  {
    "start": "2991981",
    "end": "2999349"
  },
  {
    "text": "Here you can interpret this as\nsimilarity or something like-- or just generally\nsome matrix that",
    "start": "2999350",
    "end": "3004960"
  },
  {
    "text": "captures some relationship\nbetween data points. I think it's reasonable to\nthink of them as similarity.",
    "start": "3004960",
    "end": "3012700"
  },
  {
    "text": "And the larger they are,\nthe more similar, I say. But this is not that important.",
    "start": "3012700",
    "end": "3017793"
  },
  {
    "text": " So I guess you can\nsee that this is",
    "start": "3017793",
    "end": "3023690"
  },
  {
    "text": "what I call the pairwise\ninformation between the data points but not\npairwise information between the coordinates.",
    "start": "3023690",
    "end": "3030170"
  },
  {
    "text": "Actually, if you do-- in certain cases, they\nare kind of the same.",
    "start": "3030170",
    "end": "3035359"
  },
  {
    "text": "But in some other cases,\nthey are not the same. So, for example,\none example could be",
    "start": "3035360",
    "end": "3043190"
  },
  {
    "text": "that you have xi's are images. ",
    "start": "3043190",
    "end": "3050650"
  },
  {
    "text": "And then rho xi xj measures\nthe semantic similarity",
    "start": "3050650",
    "end": "3061750"
  },
  {
    "text": "of the two images.  How do you get this? I think it's a little bit kind\nof tricky because typically,",
    "start": "3061750",
    "end": "3069680"
  },
  {
    "text": "you cannot just take an l2\nnorm to match the semantic similarity because there could\nbe two images that looks pretty",
    "start": "3069680",
    "end": "3075950"
  },
  {
    "text": "different but are\nsemantically similar. But for the moment,\nlet's assume we're given such a matrix,\nsuch a similarity matrix.",
    "start": "3075950",
    "end": "3085990"
  },
  {
    "text": "Example 2, which is\nprobably more kind of like classical usage\nof these kind of models,",
    "start": "3085990",
    "end": "3092680"
  },
  {
    "text": "so where you can\nsay, think of x i is our users of social\nnetwork, and rho",
    "start": "3092680",
    "end": "3103640"
  },
  {
    "text": "of x i x j is equal to\n1 if they are friends,",
    "start": "3103640",
    "end": "3113700"
  },
  {
    "text": "like on Facebook, I say. And when they are\nfriends, it means",
    "start": "3113700",
    "end": "3118860"
  },
  {
    "text": "that they share some\nkind of similarities, maybe similarity in jobs or\ninterests or some other things.",
    "start": "3118860",
    "end": "3125650"
  },
  {
    "text": "So you can think of\nthis as a similarity measure between two users.",
    "start": "3125650",
    "end": "3131327"
  },
  {
    "text": "And eventually, I want to\nclassify-- in this case, you want to-- eventually,\nyou're going to classify the users into groups.",
    "start": "3131327",
    "end": "3136500"
  },
  {
    "text": "So you want to say, I can\ndetect hidden communities between users from\nthis unlabeled graph.",
    "start": "3136500",
    "end": "3144525"
  },
  {
    "text": " And so basically, the goal\nis to do some clustering",
    "start": "3144525",
    "end": "3154750"
  },
  {
    "text": "with some kind of clustering--  I guess maybe I should just--\njust clustering, clustering",
    "start": "3154750",
    "end": "3169300"
  },
  {
    "text": "the data points.  I guess, in the social\nnetwork example, maybe",
    "start": "3169300",
    "end": "3176770"
  },
  {
    "text": "you have all of these users\nwhere you have, let's say, so many users. And there's some\nfriendship relationship",
    "start": "3176770",
    "end": "3182650"
  },
  {
    "text": "between them-- something\nlike this, maybe. And then what you\nwant to do is you want to detect some\nso-called hidden community.",
    "start": "3182650",
    "end": "3190100"
  },
  {
    "text": "So, for example, you can\nsay this is a cluster. This is another cluster. And maybe this\ncluster corresponds",
    "start": "3190100",
    "end": "3195160"
  },
  {
    "text": "to people at Stanford,\nand this cluster corresponds to\npeople at Berkeley. And so, of course,\nbetween Stanford students",
    "start": "3195160",
    "end": "3202930"
  },
  {
    "text": "you have more connections,\nand between Berkeley students, they have more connections. And there are some connections\nacross the groups and so forth.",
    "start": "3202930",
    "end": "3211880"
  },
  {
    "text": "And in this case-- and also, for example,\nfor this example 2,",
    "start": "3211880",
    "end": "3218150"
  },
  {
    "text": "you can think of this,\nalso, G, as a graph. ",
    "start": "3218150",
    "end": "3224190"
  },
  {
    "text": "I think, even in general case,\nyou can view G as a graph. But it will be a weighted graph. Here G, in this\nsocial network case,",
    "start": "3224190",
    "end": "3232109"
  },
  {
    "text": "G is binary because\nGij is binary. So you can view G as a graph.",
    "start": "3232110",
    "end": "3237480"
  },
  {
    "text": "And Gij is an edge. ",
    "start": "3237480",
    "end": "3243599"
  },
  {
    "text": "And your goal is to\nkind of partition-- there are many ways to\nsay what your goal is.",
    "start": "3243600",
    "end": "3249160"
  },
  {
    "text": "So you can say you are\nclustering data points, or you can say you\nare partitioning the graph into different\nparts so that each part has",
    "start": "3249160",
    "end": "3257430"
  },
  {
    "text": "more connections. Within each part, you have\nmore connections compared to across different parts.",
    "start": "3257430",
    "end": "3263440"
  },
  {
    "text": "So in some sense,\nyou can all view it",
    "start": "3263440",
    "end": "3269640"
  },
  {
    "text": "as partitioning the graph\ninto kind of components that",
    "start": "3269640",
    "end": "3282289"
  },
  {
    "text": "are separated from each other,\nthat separate from each other,",
    "start": "3282290",
    "end": "3287860"
  },
  {
    "text": "to some extent. There's no way you can\ncompletely decompose that",
    "start": "3287860",
    "end": "3293030"
  },
  {
    "text": "into completely disjoint parts. But you can somewhat\ndecompose them, kind of",
    "start": "3293030",
    "end": "3299750"
  },
  {
    "text": "partition a graph into more\nor less disjoint parts. And so this is kind of\nthe general type of setup.",
    "start": "3299750",
    "end": "3307850"
  },
  {
    "text": "I'm going to kind of\ndiscuss probably one or two instantiations about this.",
    "start": "3307850",
    "end": "3313070"
  },
  {
    "text": " So I guess the general\ntheme is the following.",
    "start": "3313070",
    "end": "3328480"
  },
  {
    "text": "So I feel like this is a\npretty deep kind of observation in math.",
    "start": "3328480",
    "end": "3334890"
  },
  {
    "text": "And the general kind\nof way to think about-- to say this is that\neigendecomposition",
    "start": "3334890",
    "end": "3341840"
  },
  {
    "text": "of this graph G really relates\na lot to the graph partitioning. So, again, decomposition of\nthis adjacency matrix G--",
    "start": "3341840",
    "end": "3350510"
  },
  {
    "text": "here, by G, I mean\nan adjacency matrix-- relate very well to the\ngraph partition problem.",
    "start": "3350510",
    "end": "3359075"
  },
  {
    "start": "3359075",
    "end": "3365670"
  },
  {
    "text": "So you see that in all of the\nexamples I'm going to give, the main approach is to do\nsome eigendecomposition.",
    "start": "3365670",
    "end": "3373310"
  },
  {
    "text": "And actually, sometimes, it's\nnot eigendecomposition of T. It's eigendecomposition of\nsome transformation of T.",
    "start": "3373310",
    "end": "3380000"
  },
  {
    "text": "But the key point is\nthat eigendecomposition seems to relate so much to\npartitioning and clustering.",
    "start": "3380000",
    "end": "3385670"
  },
  {
    "text": "And it's not that obvious. But eigendecomposition is a\nvery linear algebra thing. And graph partition is a\nvery combinatorial thing.",
    "start": "3385670",
    "end": "3392869"
  },
  {
    "text": "And this is why\nit's kind of useful because when you deal with\ncombinatorial stuff, right, typically, the way\nI might kind of--",
    "start": "3392870",
    "end": "3400520"
  },
  {
    "text": "I'm not a really\ncombinatorics person. But my way to think about it is\nthat many combinatorial stuff,",
    "start": "3400520",
    "end": "3406280"
  },
  {
    "text": "once you can relate it to\nalgebraic or linear algebraic or other kind of\npolynomials, then",
    "start": "3406280",
    "end": "3412880"
  },
  {
    "text": "you get exposed to\ndifferent type of tools. And you can do, sometimes, a lot\nmore things than you expected.",
    "start": "3412880",
    "end": "3422242"
  },
  {
    "text": "So this is the general thing. And we're going to see\nprobably two or three examples to see why this is the case.",
    "start": "3422242",
    "end": "3430680"
  },
  {
    "text": "So now I'm going to do\nsomething more concrete. So this is called\na stochastic model.",
    "start": "3430680",
    "end": "3437970"
  },
  {
    "text": "This is a very concrete\nsetup where you can do math, and you can say what-- you can instantiate\nwhat I mean clearly.",
    "start": "3437970",
    "end": "3447090"
  },
  {
    "text": "So the stochastic block model-- I can just abbreviate\nit to SBM--",
    "start": "3447090",
    "end": "3454830"
  },
  {
    "text": "and so G is assumed to be\ngenerated randomly from two--",
    "start": "3454830",
    "end": "3468480"
  },
  {
    "text": " sometimes it can be more, but\nI'm doing only two groups--",
    "start": "3468480",
    "end": "3474540"
  },
  {
    "text": "two hidden\ncommunities or groups.",
    "start": "3474540",
    "end": "3480510"
  },
  {
    "text": " So the setting\nwould be something like you have n\nvertices or n users.",
    "start": "3480510",
    "end": "3489710"
  },
  {
    "text": "And you assume that there are\ntwo hidden groups, S and S bar.",
    "start": "3489710",
    "end": "3495680"
  },
  {
    "text": "And this is the partition,\nmeaning S and S bar are disjoint. ",
    "start": "3495680",
    "end": "3504900"
  },
  {
    "text": "And then you assume\nthat if you are from-- two users are from the\nsame hidden community,",
    "start": "3504900",
    "end": "3511290"
  },
  {
    "text": "then they are more likely\nto be connected via an edge. So if i and j are both from S,\nor i and j are both from S bar,",
    "start": "3511290",
    "end": "3524070"
  },
  {
    "text": "then the probability that\nGij is 1 with probability p",
    "start": "3524070",
    "end": "3532890"
  },
  {
    "text": "and 0 with\nprobability 1 minus p. And then if i and\nj and otherwise--",
    "start": "3532890",
    "end": "3540840"
  },
  {
    "text": "if i and j are from different-- otherwise, which\nmeans that they are from a different community--",
    "start": "3540840",
    "end": "3547660"
  },
  {
    "text": "then Gij is 1 with probability\nq and 0 with probability",
    "start": "3547660",
    "end": "3553470"
  },
  {
    "text": "1 minus q. And here, importantly,\np is much larger than q. ",
    "start": "3553470",
    "end": "3561089"
  },
  {
    "text": "Maybe that's much\nlarger for the moment. How much larger? We'll quantify it in a moment.",
    "start": "3561090",
    "end": "3566220"
  },
  {
    "text": "But you need p to\nbe larger than q. I guess maybe I'll just write\nlarger but not much larger.",
    "start": "3566220",
    "end": "3572140"
  },
  {
    "text": "So, basically, from\nthe same hidden group, you have a higher\nchance to be connected",
    "start": "3572140",
    "end": "3577330"
  },
  {
    "text": "by an edge compared to from\na different kind of group. ",
    "start": "3577330",
    "end": "3587660"
  },
  {
    "text": "If you draw this, I guess-- I don't know how to draw\nsomething, a random graph. But I think you can\nthink of there is an S,",
    "start": "3587660",
    "end": "3593480"
  },
  {
    "text": "and there is an S\nbar, some edges. And then, if p is something\nprobably close to 1,",
    "start": "3593480",
    "end": "3599040"
  },
  {
    "text": "you're going to have\nsomething like this. Within the group, you\nhave high probability to connect each other.",
    "start": "3599040",
    "end": "3604680"
  },
  {
    "text": "And across the group, you\nhave some sparse edges, maybe just some little edges. ",
    "start": "3604680",
    "end": "3613370"
  },
  {
    "text": "OK. And now the goal becomes--",
    "start": "3613370",
    "end": "3618410"
  },
  {
    "text": "so the goal is to\nrecover S and S bar--",
    "start": "3618410",
    "end": "3626119"
  },
  {
    "text": "if you recover S, you\ncan recover S bar-- from the graph G. All right. ",
    "start": "3626120",
    "end": "3634340"
  },
  {
    "text": "So this is a well-defined\ndata generation model. And basically, want to discover\nthe hidden groups where",
    "start": "3634340",
    "end": "3639960"
  },
  {
    "text": "you want to do the clustering. And our approach is going\nto be eigendecomposition.",
    "start": "3639960",
    "end": "3645420"
  },
  {
    "text": "Decomposition. ",
    "start": "3645420",
    "end": "3658640"
  },
  {
    "text": "So maybe, before talking\nabout eigendecomposition, for some extreme\ncases, you don't have to do\neigendecomposition, right?",
    "start": "3658640",
    "end": "3664329"
  },
  {
    "text": "So suppose-- let's\njust do some kind of somewhat trivial warmup. So suppose p is 0.5 and q is 0.",
    "start": "3664330",
    "end": "3673490"
  },
  {
    "text": "Then you don't have\nto do any kind of-- I think, almost, you\ndon't have to do anything,",
    "start": "3673490",
    "end": "3678950"
  },
  {
    "text": "because you're going to see\ntwo disconnected parts, right? So if p is 0.5, and\nq is 0, you basically",
    "start": "3678950",
    "end": "3685280"
  },
  {
    "text": "have some S and S bar. And you have some ideas--\nnot complete connections.",
    "start": "3685280",
    "end": "3690920"
  },
  {
    "text": "You have some ideas here. And then there is a\nclear two subgraphs. You can just basically kind of--",
    "start": "3690920",
    "end": "3697460"
  },
  {
    "text": "for example, you can\nsay, I start from this. I look at all my neighbors\nand then put them all in S. And then-- because\nif you see the edge,",
    "start": "3697460",
    "end": "3704347"
  },
  {
    "text": "you know that they are\nfrom the same group, right, because if they're not\nfrom the same group, you have zero chance to\nsee an edge, al right?",
    "start": "3704347",
    "end": "3709970"
  },
  {
    "text": "So, basically, you\njust need to see all the points you can reach\nfrom this single point,",
    "start": "3709970",
    "end": "3715070"
  },
  {
    "text": "and you've got all of\nthese three points. And then you declare that\nto be S. And the same thing,",
    "start": "3715070",
    "end": "3720660"
  },
  {
    "text": "you can do it for the other. You can do some kind of-- you can just try to-- does that make sense?",
    "start": "3720660",
    "end": "3726230"
  },
  {
    "text": "I saw some confusions. I don't know. Basically, the algorithm I'm\ngoing to do is the following. I start with a node and then see\nwhat this node can reduce to.",
    "start": "3726230",
    "end": "3736190"
  },
  {
    "text": "And I put it into my set. And then I do this\nrepeatedly to see what other nodes I can reach to.",
    "start": "3736190",
    "end": "3742040"
  },
  {
    "text": "And at some point, I\nreach the boundary. I reach a closure. I cannot reach any new nodes.",
    "start": "3742040",
    "end": "3749240"
  },
  {
    "text": "And then I declare this to\nbe S. And the rest of them, I declare to be S bar.",
    "start": "3749240",
    "end": "3755329"
  },
  {
    "text": "That would work pretty\nreasonably well for p is 0.5 and q is 0. And that's just\nbecause, first of all,",
    "start": "3755330",
    "end": "3762570"
  },
  {
    "text": "you don't have\nany false positive because all the\nnodes you discover",
    "start": "3762570",
    "end": "3768000"
  },
  {
    "text": "should belong to the\nsame group because if-- and secondly, I think you\ncan also try to show that--",
    "start": "3768000",
    "end": "3773550"
  },
  {
    "text": "you can find all the\nnodes because if somebody is in your same group,\nit should connect to someone, someone you know.",
    "start": "3773550",
    "end": "3781500"
  },
  {
    "text": "This is the so-called small\nworld phenomenon, right? If this other user is\nfrom the same group,",
    "start": "3781500",
    "end": "3788400"
  },
  {
    "text": "they should be connected with\nyou by some paths, some path. So anyway, but you can see,\neven for me to convince you",
    "start": "3788400",
    "end": "3798510"
  },
  {
    "text": "this algorithm is\nworking for p is 0.5 and q is 0 is not that trivial. You have to do something right.",
    "start": "3798510",
    "end": "3804420"
  },
  {
    "text": "And this is a\ncombinatorial algorithm. And what we're going\nto do is that we are going to do a more linear\nalgebraic type of algorithm.",
    "start": "3804420",
    "end": "3814720"
  },
  {
    "text": "And you can see everything\nbecomes kind of even clearer, and this is a more\npowerful algorithm.",
    "start": "3814720",
    "end": "3819960"
  },
  {
    "text": "And you don't need this\ncombinatorial reasoning. So what do we do?",
    "start": "3819960",
    "end": "3825640"
  },
  {
    "text": "So we basically just\ndo eigendecomposition. And as a warmup,\nwhat we're going to do is that we're going\nto do eigendecomposition--",
    "start": "3825640",
    "end": "3832680"
  },
  {
    "text": " because how do I simplify\neigendecomposition?",
    "start": "3832680",
    "end": "3842380"
  },
  {
    "text": "What's the right acronym\nfor eigendecomposition? Eigendecomposition for G bar,\nwhich is the expectation for G.",
    "start": "3842380",
    "end": "3854130"
  },
  {
    "text": "So clearly-- so what is G bar? G bar is the expectation\nof G. So you have a weight.",
    "start": "3854130",
    "end": "3860040"
  },
  {
    "text": "And a weight is the\nexpectation of-- just the expectation of Gij.",
    "start": "3860040",
    "end": "3867540"
  },
  {
    "text": "So clearly, you don't\nhave this phi bar. But just for the starters, let's\nlook at this expanded version.",
    "start": "3867540",
    "end": "3874980"
  },
  {
    "text": "And what is expectation? What is this G bar ij? This is going to be\nequal to p if i and j are",
    "start": "3874980",
    "end": "3882300"
  },
  {
    "text": "from the same class,\nfrom the same community",
    "start": "3882300",
    "end": "3888050"
  },
  {
    "text": "and equal to q otherwise. So that means that, basically,\nif you look at this G bar--",
    "start": "3888050",
    "end": "3896000"
  },
  {
    "text": "so suppose this is\nthe indices for S. This is the indices for S bar.",
    "start": "3896000",
    "end": "3901080"
  },
  {
    "text": "This is indices for S,\nand this is for S bar. So when you have both i and\nj from S, you will get P.",
    "start": "3901080",
    "end": "3906869"
  },
  {
    "text": "So you get p, p like this. And here you get q\nSo this is G bar.",
    "start": "3906870",
    "end": "3921860"
  },
  {
    "text": "And my claim is\nthat in this case,",
    "start": "3921860",
    "end": "3927270"
  },
  {
    "text": "suppose you have\nthe axis to G bar. So the top eigenvector of\nG bar is the L1 vector.",
    "start": "3927270",
    "end": "3943480"
  },
  {
    "text": "And the second eigenvector\nof G bar is interesting--",
    "start": "3943480",
    "end": "3955280"
  },
  {
    "text": "is this vector where you have\n1's on the coordinate in S",
    "start": "3955280",
    "end": "3962570"
  },
  {
    "text": "and minus 1 on the\ncoordinate in S bar. So, basically, if you've got\nthe second eigenvector of G bar,",
    "start": "3962570",
    "end": "3968930"
  },
  {
    "text": "you've solved the problem\nbecause you can just read off the\ncommunity membership from this eigenvector.",
    "start": "3968930",
    "end": "3975638"
  },
  {
    "text": "That's the k. ",
    "start": "3975638",
    "end": "3982550"
  },
  {
    "text": "OK. So it sounds a little kind\nof interesting, right?",
    "start": "3982550",
    "end": "3987890"
  },
  {
    "text": "The proof-- I guess,\nwhat's the intuition here? So I guess the intuition\nprobably comes from the proof.",
    "start": "3987890",
    "end": "3994950"
  },
  {
    "text": "So let's first do number 1. Number one is almost always\ntrue for any finite cases.",
    "start": "3994950",
    "end": "4002260"
  },
  {
    "text": "It doesn't even have to\nbe such a special G bar. So what you do is you just\nsay you get G times L1 vector.",
    "start": "4002260",
    "end": "4010599"
  },
  {
    "text": "And what is G times L1 vector? Basically, you multiply\nG with L1 vector here, something like this.",
    "start": "4010600",
    "end": "4015640"
  },
  {
    "text": "And you modify it. Basically, you are just\nthe looking at the low sum,",
    "start": "4015640",
    "end": "4021080"
  },
  {
    "text": "looking at some of the\nentries in each of the rows. That's what's G times 1.",
    "start": "4021080",
    "end": "4026240"
  },
  {
    "text": "So what's the sum of the\nentries of each of the rows? The sum of the first row is\nsomething like p times n over 2",
    "start": "4026240",
    "end": "4032720"
  },
  {
    "text": "plus q times n over\n2 because there are n over 2 entries with p and\nn over 2 entries with value q.",
    "start": "4032720",
    "end": "4039575"
  },
  {
    "text": "And every row has\nthe same thing. ",
    "start": "4039575",
    "end": "4047390"
  },
  {
    "text": "So this is equal to-- basically\nsimplify this-- p plus q over 2 times n times L1 vector.",
    "start": "4047390",
    "end": "4056550"
  },
  {
    "text": "So you can kind of see that\n1 is the top eigenvector.",
    "start": "4056550",
    "end": "4063840"
  },
  {
    "text": "Actually, even\nfor more general-- for general kind of\ngraphs, weighted graphs. So for any matrix\nwith fixed row sum",
    "start": "4063840",
    "end": "4080220"
  },
  {
    "text": "or for any graph with a\nso-called uniform degree.",
    "start": "4080220",
    "end": "4088025"
  },
  {
    "text": " The degree of a graph\nis really, literally,",
    "start": "4088025",
    "end": "4094070"
  },
  {
    "text": "the row sum of the\nadjacency matrix, so how many items do\nyou have connecting to each of the vertices,\nthat's basically the row sum.",
    "start": "4094070",
    "end": "4102778"
  },
  {
    "text": "So if all the degree of all\nthe vertices is the same,",
    "start": "4102779",
    "end": "4108318"
  },
  {
    "text": "that means the row sum of the\nadjacency matrix is constant. And that means that the L1\nvector is the top eigenvector.",
    "start": "4108319",
    "end": "4117290"
  },
  {
    "text": "So this is a very\ninteresting fact. So, basically, the\ntop eigenvector",
    "start": "4117290",
    "end": "4123620"
  },
  {
    "text": "doesn't really tell you much. You have to go to the\nsecond eigenvector to see the interesting thing.",
    "start": "4123620",
    "end": "4129720"
  },
  {
    "text": "So now let's look at\na second eigenvector. So what I'm going to do is--\nthere are many ways to evaluate",
    "start": "4129720",
    "end": "4135770"
  },
  {
    "text": "whether this-- I think let's call\nthis vector u. There are many ways to\nverify u is an eigenvector.",
    "start": "4135770",
    "end": "4143420"
  },
  {
    "text": "You can directly multiply it,\nand see what's the eigenvalue. I think, probably,\nthe most intuitive way",
    "start": "4143420",
    "end": "4150380"
  },
  {
    "text": "to think about it\nis the following. So let's look at G bar,\nsubtract from G bar--",
    "start": "4150380",
    "end": "4156589"
  },
  {
    "text": "a background thing. [INAUDIBLE]",
    "start": "4156590",
    "end": "4163640"
  },
  {
    "text": " Where?",
    "start": "4163640",
    "end": "4169034"
  },
  {
    "text": "Which one you were\ntalking about? Sorry. The expression [INAUDIBLE].",
    "start": "4169035",
    "end": "4174200"
  },
  {
    "text": "Oh, sure, sure. But the negative\nof S is n over 2.",
    "start": "4174200",
    "end": "4180170"
  },
  {
    "text": "Oh, I guess, sorry, I\ndidn't assume that-- my bad. I didn't assume that this\nis an equal partition.",
    "start": "4180170",
    "end": "4186259"
  },
  {
    "text": "I should assume that. So this is also\npossibly-- assume, also assume S is n over 2.",
    "start": "4186260",
    "end": "4194585"
  },
  {
    "text": "S bar is n over 2.  If they are not\nequally weighted,",
    "start": "4194585",
    "end": "4201160"
  },
  {
    "text": "I think you have to do a\nlittle bit other things to deal with it-- not\nsuper important, but yeah.",
    "start": "4201160",
    "end": "4208840"
  },
  {
    "text": "So if the S and S bar\nare not exactly same, I think L1 vector is not\nthe eigenvector anymore.",
    "start": "4208840",
    "end": "4215949"
  },
  {
    "text": "So you have to re-weight--\nyou have to kind of massage this matrix G a little\nbit to make it still true.",
    "start": "4215950",
    "end": "4221140"
  },
  {
    "text": "But we'll get to that in\na moment in next section, I guess.",
    "start": "4221140",
    "end": "4227890"
  },
  {
    "text": "But so far, OK. So let's assume S and\nS bar are balanced. And now how do we see that\nthe second eigenvector",
    "start": "4227890",
    "end": "4233920"
  },
  {
    "text": "is this vector u that we want\nto look at, we're looking for? So the way to think about it\nis that you subtract from G bar",
    "start": "4233920",
    "end": "4245560"
  },
  {
    "text": "this background matrix,\n1, 1 transposed, times q. So, basically, the\nstructure q from--",
    "start": "4245560",
    "end": "4252070"
  },
  {
    "text": "every entry of this matrix. 1, 1 transposed,\ntimes q is really just a matrix with\nall entries being q.",
    "start": "4252070",
    "end": "4259179"
  },
  {
    "text": "And then what's\nleft is this matrix. Let's call it-- let's say\nr is equal to p minus q.",
    "start": "4259180",
    "end": "4264790"
  },
  {
    "text": "You get r-- something like this. ",
    "start": "4264790",
    "end": "4283510"
  },
  {
    "text": "So this is S. This\nis S, S bar, S bar. And here we have 0.",
    "start": "4283510",
    "end": "4289150"
  },
  {
    "text": " OK? So now you can see that\nthis matrix becomes nice",
    "start": "4289150",
    "end": "4297640"
  },
  {
    "text": "because it's a block\ndiagonal matrix. So for this matrix,\nif you want to verify,",
    "start": "4297640",
    "end": "4303790"
  },
  {
    "text": "maybe let's call\nthis matrix G prime. So we can verify G prime\ntimes u is equal to u.",
    "start": "4303790",
    "end": "4312150"
  },
  {
    "text": "And how do you verify it's\ngoing to multiply off u? How do you verify this? This is just because you can\ndo this for the two blocks",
    "start": "4312150",
    "end": "4320300"
  },
  {
    "text": "separately. So this is really just--",
    "start": "4320300",
    "end": "4325596"
  },
  {
    "text": "so I'm going to go r, r\ntimes 1, 1, minus 1, minus 1.",
    "start": "4325596",
    "end": "4333795"
  },
  {
    "text": " All right. So you do these two\nthings separately,",
    "start": "4333795",
    "end": "4340920"
  },
  {
    "text": "and basically, you get r times\nn over 2 for the first of two",
    "start": "4340920",
    "end": "4347380"
  },
  {
    "text": "coordinates. And you get minus r times\nn over 2 for the second set",
    "start": "4347380",
    "end": "4352560"
  },
  {
    "text": "of coordinates. So this is r times\nover 2 times u itself. ",
    "start": "4352560",
    "end": "4359300"
  },
  {
    "text": "And also, u is\northogonal to L1 vector",
    "start": "4359300",
    "end": "4366932"
  },
  {
    "text": "just because half of them\nare positive, half of them negative. So you take the inner product. It becomes 0.",
    "start": "4366933",
    "end": "4372790"
  },
  {
    "text": "So that's why, if you even\nlook at G bar times u, this is equal to G prime times\nu because the background you",
    "start": "4372790",
    "end": "4381640"
  },
  {
    "text": "subtract off is orthogonal to u. So that's why this is equal\nto r times n over 2 times u,",
    "start": "4381640",
    "end": "4388480"
  },
  {
    "text": "which is p minus q\nover 2 times n times u. ",
    "start": "4388480",
    "end": "4395910"
  },
  {
    "text": "OK. So that's why u has eigenvalue\np minus q over 2 times n.",
    "start": "4395911",
    "end": "4406960"
  },
  {
    "text": "So this is the-- so I think the main point is\nthat after you subtract off this background thing, then\nthis G prime is block diagonal.",
    "start": "4406960",
    "end": "4416730"
  },
  {
    "text": " And this means that\nthe eigenvector",
    "start": "4416730",
    "end": "4427700"
  },
  {
    "text": "aligns with the blocks.",
    "start": "4427700",
    "end": "4433663"
  },
  {
    "text": "I think this is kind of\nthe fundamental things that we are looking for.",
    "start": "4433663",
    "end": "4439016"
  },
  {
    "text": "Maybe, just to generalize this\nto make it look a little bit",
    "start": "4439016",
    "end": "4446932"
  },
  {
    "text": "more convincing, so suppose\nyou have a matrix A which looks like this. Suppose you have some--",
    "start": "4446932",
    "end": "4453580"
  },
  {
    "text": "and just 1, 1, 1\nhere in this block, and you have a lot of 1's.",
    "start": "4453580",
    "end": "4459901"
  },
  {
    "text": "And you have a lot of 1's here. Suppose you have three blocks\nnow instead of two blocks.",
    "start": "4459901",
    "end": "4466520"
  },
  {
    "text": "Then because you\nhave block diagonals, so we know that for every block,\nyou can do your own thing,",
    "start": "4466520",
    "end": "4472587"
  },
  {
    "text": "right? So then, if you look\nat the eigenvectors,",
    "start": "4472587",
    "end": "4479960"
  },
  {
    "text": "you can see that-- ",
    "start": "4479960",
    "end": "4486437"
  },
  {
    "text": "you can see that this-- ",
    "start": "4486437",
    "end": "4493600"
  },
  {
    "text": "so I guess each of\nthese three vectors",
    "start": "4493600",
    "end": "4508520"
  },
  {
    "text": "are eigenvectors\nbecause you can do each of the blocks in\na separate way, right?",
    "start": "4508520",
    "end": "4515440"
  },
  {
    "text": "So I guess there is-- ",
    "start": "4515440",
    "end": "4521590"
  },
  {
    "text": "so, basically,\nyou can say that-- ",
    "start": "4521590",
    "end": "4527125"
  },
  {
    "text": "so you can say that\nI'm going to choose the L1 vector for the\nfirst block and then 0 at all the other places. That's still eigenvector.",
    "start": "4527125",
    "end": "4535470"
  },
  {
    "text": "And then, when you have\nthis, then you have that--",
    "start": "4535470",
    "end": "4541410"
  },
  {
    "text": "if you have these three\neigenvectors, then the rows-- if you look at every\nrow here, so this is 0.",
    "start": "4541410",
    "end": "4550630"
  },
  {
    "text": "This is 0. This is 1, right? So this row gives the\ncluster ID of the vertex.",
    "start": "4550630",
    "end": "4565090"
  },
  {
    "text": "So each row gives us the\ncluster ID of vertex. So I think this is the\nfundamental intuition about why",
    "start": "4565090",
    "end": "4572170"
  },
  {
    "text": "eigenvectors are useful for\ncapturing the clustering",
    "start": "4572170",
    "end": "4580510"
  },
  {
    "text": "structure in the graph. It's just because\nin the extreme case, when you have extreme\nclustering-- so every block,",
    "start": "4580510",
    "end": "4588640"
  },
  {
    "text": "every subset-- in these three blocks,\nor three subsets, they have, really, just\nstrong interconnections,",
    "start": "4588640",
    "end": "4596800"
  },
  {
    "text": "and know any other\ncross-group connections. In that case, the\neigenvector just strongly",
    "start": "4596800",
    "end": "4607190"
  },
  {
    "text": "aligns with the block structure. And here I think--",
    "start": "4607190",
    "end": "4615890"
  },
  {
    "text": "but it was complex. What makes things a\nlittle bit more complex is that you have\nsome background. You have some more\nthings here and here--",
    "start": "4615890",
    "end": "4622550"
  },
  {
    "text": "so some random entries,\nsmall entries other places-- then it would change\nthe-- it would",
    "start": "4622550",
    "end": "4627580"
  },
  {
    "text": "elevate the entire matrix\na little bit, right? But it wouldn't change the\neigenspace fundamentally.",
    "start": "4627580",
    "end": "4634190"
  },
  {
    "text": "That's pretty much\nthe intuition. Any questions?",
    "start": "4634190",
    "end": "4640880"
  },
  {
    "start": "4640880",
    "end": "4647712"
  },
  {
    "text": "So it seems like here you have\nthe [INAUDIBLE] structurally.",
    "start": "4647712",
    "end": "4653582"
  },
  {
    "text": "And then, as you have some\npermutation, [INAUDIBLE]----",
    "start": "4653582",
    "end": "4659402"
  },
  {
    "text": "Right. --you would have to [INAUDIBLE]. Right. Right. So how would you permute this?",
    "start": "4659402",
    "end": "4666590"
  },
  {
    "text": "That's a great question. Actually, that's exactly why\nthis is working because-- so the question is, what\nif you permute this, right?",
    "start": "4666590",
    "end": "4673469"
  },
  {
    "text": "So if you permute it, it's\nkind of like you're permuting--",
    "start": "4673470",
    "end": "4678550"
  },
  {
    "text": "the eigenvector will\npermute accordingly. So suppose you have a--",
    "start": "4678550",
    "end": "4684570"
  },
  {
    "text": "I'm not sure whether\nthat makes sense. So, for example,\nsuppose you have--",
    "start": "4684570",
    "end": "4689985"
  },
  {
    "text": " you declare this part and this\npart to be the first block.",
    "start": "4689985",
    "end": "4697930"
  },
  {
    "text": "And then this part and this\npart will be the second block.",
    "start": "4697930",
    "end": "4703300"
  },
  {
    "text": "I think your eigenvectors\nwill permute-- the coordinates\nof the eigenvector will permute accordingly. And that's why it's\naligned with the--",
    "start": "4703300",
    "end": "4709780"
  },
  {
    "text": "that's why the\nalignment is maintained, and you can discover\nthe hidden structure.",
    "start": "4709780",
    "end": "4715160"
  },
  {
    "start": "4715160",
    "end": "4731540"
  },
  {
    "text": "OK. Sounds good. So I guess maybe another thing-- I'm not sure whether this\nis a confusion for you.",
    "start": "4731540",
    "end": "4736960"
  },
  {
    "text": "It could be confusion,\ncould it not? So here, the eigenvectors-- there's no negative values\nin this construction, right?",
    "start": "4736960",
    "end": "4745550"
  },
  {
    "text": "But the reason why I\ndidn't have negative values is because it makes\nit simpler because--",
    "start": "4745550",
    "end": "4753800"
  },
  {
    "text": "so, for example, let's say even\nthis matrix, this vector, this",
    "start": "4753800",
    "end": "4759491"
  },
  {
    "text": "is also an eigenvector\nbecause it's the sum of two eigenvectors. And all of these are going\nto have the same eigenvalue.",
    "start": "4759492",
    "end": "4764730"
  },
  {
    "text": "So any combination\nof the eigenvalues is still an eigenvalue. That's how you get\nthe negative values.",
    "start": "4764730",
    "end": "4770925"
  },
  {
    "text": " And there's something special\nabout L1 vectors because--",
    "start": "4770925",
    "end": "4777960"
  },
  {
    "text": "so here, in this example,\nin this A example, there is nothing\nspecial about L1 vectors",
    "start": "4777960",
    "end": "4783000"
  },
  {
    "text": "because there is this\nbackground noise. But when you-- so what happens\nis that when you add kind",
    "start": "4783000",
    "end": "4789660"
  },
  {
    "text": "of a background noise to it,\nthen the L1 vectors becomes-- stands out.",
    "start": "4789660",
    "end": "4794910"
  },
  {
    "text": "So here you have\nthree eigenvectors that are equalizing. But when you-- an L1 vector is\nin a subspace of these three",
    "start": "4794910",
    "end": "4801600"
  },
  {
    "text": "eigenvectors, right? So L1 vectors is indeed\na linear combination of these three things.",
    "start": "4801600",
    "end": "4807636"
  },
  {
    "text": "And when you add the\nbackground noise, the L1 vector\ndirection stands out. And then you are left\nwith two other directions",
    "start": "4807637",
    "end": "4813630"
  },
  {
    "text": "which are still the same. And those two other\ndirections will tell you the block structure. ",
    "start": "4813630",
    "end": "4822682"
  },
  {
    "text": "So maybe another way\nto think about this is if you have two blocks--\nsuppose you have two blocks. ",
    "start": "4822683",
    "end": "4830260"
  },
  {
    "text": "So if you don't have\nany background noise, then the eigenvectors\nwill be 1, 1, 1, 0, 0, 0. ",
    "start": "4830260",
    "end": "4840322"
  },
  {
    "text": "These will be our\ntwo eigenvectors. But then, when you\nadd it back on-- and you can represent\nthese eigenvectors in two different ways.",
    "start": "4840322",
    "end": "4846227"
  },
  {
    "text": "This is an eigenvector\neigensystem. You could also\nwrite it like this,",
    "start": "4846227",
    "end": "4853510"
  },
  {
    "text": "just because you\nhave different ways to represent a two-dimensional\nsubspace of eigenvectors. But when you add the\nbackground noise,",
    "start": "4853510",
    "end": "4859900"
  },
  {
    "text": "then this one will stand out. So that's why you can only\nuse this system to see it,",
    "start": "4859900",
    "end": "4866110"
  },
  {
    "text": "but not here, because-- I'm not sure whether\nthat make sense.",
    "start": "4866110",
    "end": "4872280"
  },
  {
    "text": "No? So, basically, without\nadding the background noise, you have this direction, which\nis this 1, 1, 1, 0, 0, 0.",
    "start": "4872280",
    "end": "4881700"
  },
  {
    "text": "And there's this direction-- 0, 0, 0, 1, 1, 1.",
    "start": "4881700",
    "end": "4887070"
  },
  {
    "text": "And also, you can have\nthis direction, which is the L1 thing, and\nthis direction, which",
    "start": "4887070",
    "end": "4892110"
  },
  {
    "text": "is the 1, 1, 1 minus\n1, minus 1, minus 1. So they're both-- so you\nhave these two different sets",
    "start": "4892110",
    "end": "4901170"
  },
  {
    "text": "of coordinate systems. And when you add background\nnoise, you're going to elevate,",
    "start": "4901170",
    "end": "4906600"
  },
  {
    "text": "or you're going to increase\nthe strength in this direction. But the subspace\ndoesn't really change.",
    "start": "4906600",
    "end": "4912690"
  },
  {
    "text": "This direction becomes\nthe top eigenvector, and this becomes the\nsecond eigenvector.",
    "start": "4912690",
    "end": "4918840"
  },
  {
    "text": "But fundamentally, nothing\nreally changed that. I hope this only clarifies\nthis, not confuses.",
    "start": "4918840",
    "end": "4926000"
  },
  {
    "text": "OK. So I guess I'm\nrunning out of time. ",
    "start": "4926000",
    "end": "4932220"
  },
  {
    "text": "Let's see. ",
    "start": "4932220",
    "end": "4944930"
  },
  {
    "text": "So I guess I'll take two minutes\nto give a quick overview,",
    "start": "4944930",
    "end": "4952100"
  },
  {
    "text": "wrap up, and give a quick\noverview of what we do next. So, basically, you can also\nactually-- if you really",
    "start": "4952100",
    "end": "4962590"
  },
  {
    "text": "want, you can verify\nthat G is actually equal to p plus q over 2\ntimes 1, 1 transposed, plus p",
    "start": "4962590",
    "end": "4972400"
  },
  {
    "text": "minus q over 2 times\n1, u transposed.",
    "start": "4972400",
    "end": "4977409"
  },
  {
    "text": "This is the eigendecomposition\nof this matrix. ",
    "start": "4977410",
    "end": "4982715"
  },
  {
    "text": "And-- OK, T bar. So next, what happens is\nthat we only have access--",
    "start": "4982715",
    "end": "4990580"
  },
  {
    "text": "so in reality, we only\nhave G. So what do we do?",
    "start": "4990580",
    "end": "4998940"
  },
  {
    "text": "What we do is we just\nsay the intuition is just that G is approximately\nequal to expectation",
    "start": "4998940",
    "end": "5005570"
  },
  {
    "text": "of G in certain aspects. It's not true that\nevery entry of G is close to every\nentry of expected G,",
    "start": "5005570",
    "end": "5012020"
  },
  {
    "text": "because you take one\nentry, G is binary, and the expectation\nof G is p of q. There is no way they are close.",
    "start": "5012020",
    "end": "5018530"
  },
  {
    "text": "But this is in terms\nof the spectrum.",
    "start": "5018530",
    "end": "5023920"
  },
  {
    "text": " So, essentially, we want\nto show that, essentially--",
    "start": "5023920",
    "end": "5030079"
  },
  {
    "text": "even though we need a little\ntrick to make this work nicely-- essentially, we want to show the\noperator norm, the difference",
    "start": "5030080",
    "end": "5036800"
  },
  {
    "text": "between these two, is small. Then, even if you use G\nto do the decomposition--",
    "start": "5036800",
    "end": "5043329"
  },
  {
    "text": "so this means that\ndecomposing G is the same--",
    "start": "5043330",
    "end": "5050880"
  },
  {
    "text": "is similar-- to decomposing\nexpectation of G.",
    "start": "5050880",
    "end": "5059540"
  },
  {
    "text": "That's pretty much it. And now you can see the\nconcentration inequality that we discussed in the earlier\nlectures, in this course,",
    "start": "5059540",
    "end": "5066810"
  },
  {
    "text": "becomes useful. So concretely, what you\ndo is the following. ",
    "start": "5066810",
    "end": "5073850"
  },
  {
    "text": "So you write G is G minus\nexpectation G plus expectation",
    "start": "5073850",
    "end": "5085720"
  },
  {
    "text": "G. Expectation G is just G bar. So this is G minus expectation\nG plus p plus q over 2,",
    "start": "5085720",
    "end": "5096670"
  },
  {
    "text": "1, 1 transposed, plus p minus\nq over 2, 1, 1, transposed--",
    "start": "5096670",
    "end": "5103530"
  },
  {
    "text": "sorry, u, u transposed, right?  So you also say that this part\ndoesn't matter too much, right?",
    "start": "5103530",
    "end": "5110370"
  },
  {
    "text": "It doesn't really change\nyour eigenspectrum. To make it cleaner,\nwhat you can do",
    "start": "5110370",
    "end": "5115840"
  },
  {
    "text": "is that you subtract\nthis part because u is something you want to discover.",
    "start": "5115840",
    "end": "5122100"
  },
  {
    "text": "The top eigenvector is\nsomething you already know. So we probably shouldn't\nask the top eigenvector. You should just directly look\nfor the second eigenvector.",
    "start": "5122100",
    "end": "5129510"
  },
  {
    "text": "What you do is you move\nthis to the left-hand side. So you get this p--",
    "start": "5129510",
    "end": "5135699"
  },
  {
    "text": "you look at this matrix. This is something\nyou know because G is something you know. And then this matrix is equal\nto this plus p minus q over 2.",
    "start": "5135700",
    "end": "5148130"
  },
  {
    "text": "So you can view this\nas a perturbation. And this is something you\nare really looking for.",
    "start": "5148130",
    "end": "5154458"
  },
  {
    "text": "So, basically, you start\nfrom the left-hand side. You take an eigendecomposition. So you do eigendecomposition\nof G minus p plus q over 2,",
    "start": "5154458",
    "end": "5164890"
  },
  {
    "text": "1, 1 transposed. And you hope that the top\neigenvector of this matrix",
    "start": "5164890",
    "end": "5169960"
  },
  {
    "text": "is close to--  and the hope is the top\neigenvector is close to u.",
    "start": "5169960",
    "end": "5178860"
  },
  {
    "text": "That's basically our goal. And how do you make\nsure that this is true?",
    "start": "5178860",
    "end": "5184680"
  },
  {
    "text": "The only thing-- so it suffices\nto show this G minus EG",
    "start": "5184680",
    "end": "5194590"
  },
  {
    "text": "in terms of the spectrum norm\nis much, much smaller than--",
    "start": "5194590",
    "end": "5200020"
  },
  {
    "text": "the noise is much\nsmaller than that signal",
    "start": "5200020",
    "end": "5205710"
  },
  {
    "text": "in terms of the operator norm. And so, in some sense,\nyou need some robustness",
    "start": "5205710",
    "end": "5211770"
  },
  {
    "text": "of eigendecomposition. I guess I didn't really discuss\nany of the existing theorems. But essentially,\nif you have this,",
    "start": "5211770",
    "end": "5218110"
  },
  {
    "text": "you can prove that eigenvectors\nof the sum of these two matrices is very similar\nto the eigenvector of one",
    "start": "5218110",
    "end": "5223610"
  },
  {
    "text": "of these matrix. And this is called\nDavis-Kahan theorem. I guess I wouldn't have time\nto talk about all of this.",
    "start": "5223610",
    "end": "5230640"
  },
  {
    "text": "But this intuitively\nmakes sense. So if the noise is\nsmall enough in terms of the spectrum, the operator\nnorm, then you get the signal.",
    "start": "5230640",
    "end": "5239800"
  },
  {
    "text": "And so how do you get this? How do you show this is true?",
    "start": "5239800",
    "end": "5244860"
  },
  {
    "text": "I think I'm going\nto discuss that in the beginning of\nthe next lecture. It's essentially you just have\nto prove some concentration",
    "start": "5244860",
    "end": "5253110"
  },
  {
    "text": "inequality using some of the\ntools we had in lecture 3 or 4 of this course.",
    "start": "5253110",
    "end": "5258525"
  },
  {
    "text": " OK, any questions? ",
    "start": "5258525",
    "end": "5270790"
  },
  {
    "text": "What about multiple\nclusters where the noise was [INAUDIBLE]\ndecomposition [INAUDIBLE]??",
    "start": "5270790",
    "end": "5280070"
  },
  {
    "text": "So if you have more\nclusters, the noise will hurt the entire spectrum. ",
    "start": "5280070",
    "end": "5287018"
  },
  {
    "text": "And it becomes a\nlittle more complex. So first of all, if\nyou have no noise, then you can still prove\nthat the eigenvectors",
    "start": "5287018",
    "end": "5292930"
  },
  {
    "text": "are enough for you to\nrecover the blocks. But this robustness thing\nwill be a little tricky",
    "start": "5292930",
    "end": "5298370"
  },
  {
    "text": "because now you have\nmore eigenvectors. And the noise has an\ninfluence to each of them. And you have to, again, control\nsome noise-to-signal ratio",
    "start": "5298370",
    "end": "5308300"
  },
  {
    "text": "using a little more\nadvanced techniques. But essentially,\nthe-- yeah, I think",
    "start": "5308300",
    "end": "5314365"
  },
  {
    "text": "it's just really the\nmathematical part that's a little bit more complicated. But fundamentally, it's\ndoing the same thing. ",
    "start": "5314365",
    "end": "5323410"
  },
  {
    "text": "[INAUDIBLE] question. Is it sufficient to-- ",
    "start": "5323410",
    "end": "5330270"
  },
  {
    "text": "my impression is [INAUDIBLE] of\nthis noise, this eigenvector, you must always do [INAUDIBLE].",
    "start": "5330270",
    "end": "5336120"
  },
  {
    "text": "The new second\neigenvector, we want to show that it's\nclose to u and what",
    "start": "5336120",
    "end": "5344030"
  },
  {
    "text": "I guess the eigenvector of the\nnew matrix is supposed to be.",
    "start": "5344030",
    "end": "5349130"
  },
  {
    "text": "When we analyze the\noperator norm of G minus [INAUDIBLE]\nG vector, it feels like we're trying to bounds how\nmuch of all of the eigenvectors",
    "start": "5349130",
    "end": "5358514"
  },
  {
    "text": "we need. Right. Right. Do we really need to do that? Or is there a way we can go\naround and just sort of argue",
    "start": "5358515",
    "end": "5363590"
  },
  {
    "text": "about how the [INAUDIBLE]? Yeah. So I think that's\na great question. I guess, just to\nrephrase your question,",
    "start": "5363590",
    "end": "5370090"
  },
  {
    "text": "you are saying\nthat we really need to say that G minus EG is\nsmall in all directions.",
    "start": "5370090",
    "end": "5377900"
  },
  {
    "text": "So you just have to say\nthat G minus EG is only not maxing up with the direction u.",
    "start": "5377900",
    "end": "5383090"
  },
  {
    "text": "I think you do have to\nsay, to some extent, G minus EG is small\nin all directions",
    "start": "5383090",
    "end": "5389060"
  },
  {
    "text": "because if G minus EG is very,\nvery big in one direction, even if that direction\nis completely orthogonal",
    "start": "5389060",
    "end": "5395150"
  },
  {
    "text": "to u, then that direction\nwill be a new top eigenvector, right, because you're\ntalking about the max.",
    "start": "5395150",
    "end": "5404210"
  },
  {
    "text": "But I think, like, how do\nyou exactly measure this? There is still some\nroom to negotiate. But you do have\nto, in some sense,",
    "start": "5404210",
    "end": "5412159"
  },
  {
    "text": "say something about all\ndirections of the noise. ",
    "start": "5412160",
    "end": "5419930"
  },
  {
    "text": "OK. OK. Thanks. I guess, see you\nMonday-- or Wednesday.",
    "start": "5419930",
    "end": "5425410"
  },
  {
    "start": "5425410",
    "end": "5430000"
  }
]