[
  {
    "start": "0",
    "end": "58000"
  },
  {
    "text": "Right. We're going to get started. This is the last lecture for the term. Um, uh, just a few logistics things that we're getting at the beginning.",
    "start": "4100",
    "end": "12210"
  },
  {
    "text": "Um, so just a friendly reminder that the project write up is due on the 20th at 11:59 PM.",
    "start": "12210",
    "end": "17865"
  },
  {
    "text": "There are no late days, and then the poster presentations are on Friday at 8:30 AM.",
    "start": "17865",
    "end": "23340"
  },
  {
    "text": "[NOISE] Uh, you need to submit your poster as well and that should be submitted online to Gradescope by the same time.",
    "start": "23340",
    "end": "29550"
  },
  {
    "text": "Um, we'll open up a submission for that in advance. Um, and there's also no late dates for that.",
    "start": "29550",
    "end": "34935"
  },
  {
    "text": "Uh, you should have received an email with some details about the poster session. Any questions? Reach out to us. Does anybody have any questions right now?",
    "start": "34935",
    "end": "41745"
  },
  {
    "text": "It's the last week of office hours. We won't have office hours next week. It's finals week for most people.",
    "start": "41745",
    "end": "47969"
  },
  {
    "text": "Um, but you can reach out to us on Piazza or if you have extra questions, you know, we're happy to find the time.",
    "start": "47970",
    "end": "55240"
  },
  {
    "text": "Okay. All right. So what we're going to do today is, uh, so last time, of course, was the quiz.",
    "start": "55240",
    "end": "61464"
  },
  {
    "start": "58000",
    "end": "86000"
  },
  {
    "text": "Uh, and we're gonna be sending out- our goal is to send out grades for everybody that took it on Monday to send them out today.",
    "start": "61465",
    "end": "67300"
  },
  {
    "text": "We're almost done grading those. Um, [NOISE] and for the- there are a few people who are still taking that late so though- uh,",
    "start": "67300",
    "end": "73225"
  },
  {
    "text": "we have to grade the SCPDs. But everybody else should get their quiz scores who took it Monday, should get back today.",
    "start": "73225",
    "end": "78805"
  },
  {
    "text": "And then today what we're gonna do is we're going to talk just a little bit about Monte-Carlo tree search as well as discuss some end-of-course stuff.",
    "start": "78805",
    "end": "85400"
  },
  {
    "text": "So why Monte-Carlo tree search? Um, who here has heard of AlphaGo? All right.",
    "start": "85400",
    "end": "91305"
  },
  {
    "start": "86000",
    "end": "222000"
  },
  {
    "text": "Yes. So I mean AlphaGo, you could argue is one of the major AI achievements of the last, you know, 10 to 20 years, um,",
    "start": "91305",
    "end": "98130"
  },
  {
    "text": "and it really has been a spectacular achievement that was achieved much faster than was anticipated,",
    "start": "98130",
    "end": "103160"
  },
  {
    "text": "uh, to beat humans on the board game Go, which is considered an extremely hard game.",
    "start": "103160",
    "end": "108710"
  },
  {
    "text": "So the Monte Carlo tree search was a critical part of, you know, to achieve this success plus a lot of other additional things.",
    "start": "108710",
    "end": "115040"
  },
  {
    "text": "But it's one of the aspects that we have not talked about very much so far in class. So I think talking about Monte-Carlo tree search,",
    "start": "115040",
    "end": "120260"
  },
  {
    "text": "so you're familiar with some of the ideas behind that and therefore some of the ideas behind AlphaGo, um, are useful, uh, to be aware of.",
    "start": "120260",
    "end": "126890"
  },
  {
    "text": "Uh, and then also because when we start to think about Monte-Carlo tree search, it's a way for us to think about model-based reinforcement learning,",
    "start": "126890",
    "end": "133819"
  },
  {
    "text": "which is a very powerful tool that we haven't talked about as much in part because we haven't seen as much success in the deep learning case with models.",
    "start": "133820",
    "end": "143194"
  },
  {
    "text": "And I'm happy to talk more about that either today or offline. But I think that going forward it's likely to be a really productive avenue of research.",
    "start": "143195",
    "end": "150379"
  },
  {
    "text": "And we can talk about why that might be particularly useful in alpha in AlphaGo. Okay. So what we're gonna do first is we're gonna sort",
    "start": "150380",
    "end": "158569"
  },
  {
    "text": "of talk a little bit again about sort of model-based reinforcement learning. And then we'll talk about simulates- simulation-based search,",
    "start": "158570",
    "end": "164549"
  },
  {
    "text": "which is where Monte Carlo tree search comes up. Actually, just because everyone takes different classes and I'm curious,",
    "start": "164550",
    "end": "169595"
  },
  {
    "text": "who here has covered Monte Carlo tree search in a, in another class? Okay. Just two. What class was it?",
    "start": "169595",
    "end": "176420"
  },
  {
    "text": "[inaudible]. 238. Yeah. Yeah. Same?",
    "start": "176420",
    "end": "181430"
  },
  {
    "text": "Same one. It was mentioned a little bit like [NOISE] It was mentioned briefly. Ah, yeah. Very brief.",
    "start": "181430",
    "end": "186645"
  },
  {
    "text": "Yeah. And yeah? 217. Which is? General game play. Oh, yeah. General game play would be a good one to come and bring it in.",
    "start": "186645",
    "end": "193260"
  },
  {
    "text": "Okay. Cool. Awesome. Oh, and also I just think people are interested. At the end, I'll also mention some other classes",
    "start": "193260",
    "end": "198920"
  },
  {
    "text": "where you can learn more about reinforcement learning. All right. So model-based reinforcement learning. Um, [NOISE], most of what we've talked about this term though not all of it,",
    "start": "198920",
    "end": "206450"
  },
  {
    "text": "but most of what we've talked about this term [BACKGROUND] particularly when we're talking about learning, which means we don't know how the world works, um,",
    "start": "206450",
    "end": "211700"
  },
  {
    "text": "is that we're thinking about either learning a policy or a value function or both directly from data.",
    "start": "211700",
    "end": "217355"
  },
  {
    "text": "Um, and what we're gonna talk about more today is talking about learning a specific model. So just to remind ourselves because it has been a little while.",
    "start": "217355",
    "end": "224470"
  },
  {
    "start": "222000",
    "end": "259000"
  },
  {
    "text": "We're gonna be talking about learning the transition and/or reward model,",
    "start": "224470",
    "end": "232470"
  },
  {
    "text": "and we talked about this a little bit maybe, I don't know, a number of- ah, a few weeks ago,",
    "start": "233300",
    "end": "238730"
  },
  {
    "text": "It came up also in exploration. But once you have a model, you can use planning with that. And just to refresh our memories,",
    "start": "238730",
    "end": "245180"
  },
  {
    "text": "planning is where we take a known model of the world and then we use value iteration or policy iteration or dynamic programming in general",
    "start": "245180",
    "end": "253085"
  },
  {
    "text": "to try to compute a policy for those given models.",
    "start": "253085",
    "end": "257430"
  },
  {
    "text": "In contrast to that, of course, we've talked a lot about Model-Free RL, where there's no model and we just directly learn a value function from experience.",
    "start": "258320",
    "end": "266185"
  },
  {
    "start": "259000",
    "end": "424000"
  },
  {
    "text": "And now, we are going to learn a model from experience and then plan using that. Now, the planning that we do in addition to sort of",
    "start": "266185",
    "end": "273755"
  },
  {
    "text": "the approaches that are known from classical decision, uh, like dy- dynamic programming also can be",
    "start": "273755",
    "end": "279500"
  },
  {
    "text": "any of the other techniques that we've talked about so far in class. So, you know, once we have this, this is a- this can act as a simulator.",
    "start": "279500",
    "end": "287000"
  },
  {
    "text": "And so once you have that, you can do model-free RL using that simulator,",
    "start": "287000",
    "end": "294414"
  },
  {
    "text": "or you can do policy search or anything else you would like to do given that you have a model of the world.",
    "start": "294415",
    "end": "300320"
  },
  {
    "text": "It basically just acts as a simulator, and you can use it to generate experience as well as do things like dynamic programming.",
    "start": "300320",
    "end": "308354"
  },
  {
    "text": "Okay. You can do sort of all of those things. So once you have a simulator of the world, that's great. The downside of course can be if that simulator is not very good.",
    "start": "308355",
    "end": "315979"
  },
  {
    "text": "What does that do in terms of the resulting estimates? Okay. So just to think of it again.",
    "start": "315980",
    "end": "322940"
  },
  {
    "text": "We have our world. It's generating actions and rewards and states. Um, and now we're going to think about sort of explicitly trying to model those.",
    "start": "322940",
    "end": "331625"
  },
  {
    "text": "So in a lot of cases you may know the reward function, not always.",
    "start": "331625",
    "end": "336740"
  },
  {
    "text": "But in a lot of practical applications, you'll know the reward function. So if you're designing like a reinforcement learning based system for something like customer service,",
    "start": "336740",
    "end": "344340"
  },
  {
    "text": "you probably have a reward function in mind, like engagement or purchases or things like that.",
    "start": "344340",
    "end": "350240"
  },
  {
    "text": "But you may not have a very good model of the customer. So there are a lot of practical examples where you'll need to",
    "start": "350240",
    "end": "356030"
  },
  {
    "text": "learn the dynamics model either implicitly or explicitly. But the reward function itself might be known.",
    "start": "356030",
    "end": "362639"
  },
  {
    "text": "All right. So how do we think about this in terms of a loop? We think about having,",
    "start": "365540",
    "end": "371110"
  },
  {
    "text": "um, [NOISE] some experience. So this could be things like, you know, state, action, reward,",
    "start": "371110",
    "end": "376390"
  },
  {
    "text": "next state tuples that we then feed into a model, and this is going to output either a reward or a transition model.",
    "start": "376390",
    "end": "383515"
  },
  {
    "text": "We do planning with that, which can be dynamic programming or Q learning",
    "start": "383515",
    "end": "388705"
  },
  {
    "text": "or many of the other techniques we've seen here, policy search. And then that has to give us a way to pick an action.",
    "start": "388705",
    "end": "395574"
  },
  {
    "text": "So that has to give us an action that we can then use over here, and we don't have to necessarily compute a full value function.",
    "start": "395575",
    "end": "403765"
  },
  {
    "text": "All we need to know is what is the next action to take, and we're going to exploit that when we get to Monte Carlo tree search.",
    "start": "403765",
    "end": "409384"
  },
  {
    "text": "That we don't necessarily have to compute a full value function for the world nor do we have to have a complete policy.",
    "start": "409385",
    "end": "414605"
  },
  {
    "text": "All we have to know is what should we do for this particular action next.",
    "start": "414605",
    "end": "421620"
  },
  {
    "text": "So some of the advantages about this is, um, we've a lot of supervised learning methods including from deep,",
    "start": "423850",
    "end": "431060"
  },
  {
    "start": "424000",
    "end": "657000"
  },
  {
    "text": "uh, deep learning which we can use to learn models. Some of them are better or worse super, uh, suited.",
    "start": "431060",
    "end": "437164"
  },
  {
    "text": "So our transition dynamics, we're generally going to think of a stochastic. So we're going to need supervised learning methods that can predict distributions.",
    "start": "437165",
    "end": "444710"
  },
  {
    "text": "For reward models we can often treat them as scalars. So then we can use very classic regression-based approaches.",
    "start": "444710",
    "end": "452074"
  },
  {
    "text": "And the, the other nice thing about model-based reinforcement learning is like what we talked about for exploration,",
    "start": "452075",
    "end": "457264"
  },
  {
    "text": "um, that we can often have explicit models over our uncertainty of how good are our models.",
    "start": "457265",
    "end": "462965"
  },
  {
    "text": "And once we have uncertainty over our models of the world, we can use that to propagate into uncertainty over the decisions we make.",
    "start": "462965",
    "end": "469985"
  },
  {
    "text": "So in the bandit case that was pretty direct, because in the bandit case- so for bandits,",
    "start": "469985",
    "end": "475700"
  },
  {
    "text": "we had uncertainty over the reward of an arm,",
    "start": "475700",
    "end": "481265"
  },
  {
    "text": "and that just directly represented our uncertainty over the value because it was only a single timestep.",
    "start": "481265",
    "end": "486670"
  },
  {
    "text": "In the case of MDPs, we could represent uncertainty over the,",
    "start": "486670",
    "end": "491885"
  },
  {
    "text": "the reward and the dynamics model and forms of these sort of bonuses, and then propagate the- that information during planning.",
    "start": "491885",
    "end": "499010"
  },
  {
    "text": "And that again allowed us to think about sort of how well do we know the value of different states and actions and what could it be,",
    "start": "499010",
    "end": "505940"
  },
  {
    "text": "what sort of could it be optimistically. Now the downsides is that, you know, first we're gonna learn a model and then we're gonna construct a value function,",
    "start": "505940",
    "end": "513770"
  },
  {
    "text": "and there could be two sources of approximation error there. Because we're going to get an approximate model and then we're gonna do",
    "start": "513770",
    "end": "521719"
  },
  {
    "text": "approximate planning in general for large state spaces, and so we can get compounding errors in that case.",
    "start": "521720",
    "end": "528760"
  },
  {
    "text": "Now, another place that we saw compounding errors earlier in this course was when we talked about imitation learning. And we talked about if you had a trajectory and then you tried to",
    "start": "528760",
    "end": "537010"
  },
  {
    "text": "do behavior cloning and learn mappings from states to actions, and how if you then got that policy and followed it in the real world,",
    "start": "537010",
    "end": "543760"
  },
  {
    "text": "you might end up in parts of the state space where you didn't have much data and you could have sort of these escalating errors because,",
    "start": "543760",
    "end": "551045"
  },
  {
    "text": "um, again it could compound. Once you get in parts of the state space where you don't have much data, and then you're extrapolating,",
    "start": "551045",
    "end": "556470"
  },
  {
    "text": "then things can go badly. So similarly in this case, if you build a model and you compute a policy that ends up getting you to",
    "start": "556470",
    "end": "563110"
  },
  {
    "text": "parts of the world where you don't have very much data and where your model estimate is poor, then again your resulting value function in your policy might be bad.",
    "start": "563110",
    "end": "572680"
  },
  {
    "text": "I guess I'll just mention one other big advantage I think of with model-based reinforcement learning is that it can also be very powerful for transfer.",
    "start": "573890",
    "end": "582005"
  },
  {
    "text": "So when Chelsea was here and talking about meta-learning, one of the nice benefits of model-based RL is",
    "start": "582005",
    "end": "588019"
  },
  {
    "text": "that if you learn a dynamics model of the world, then if someone changes the reward function,",
    "start": "588020",
    "end": "594050"
  },
  {
    "text": "implicitly you can just do zero shot transfer, because you can just take your learned model of the dynamics and then your reward function,",
    "start": "594050",
    "end": "601274"
  },
  {
    "text": "you can just compute a new plan. So like if I'm a robot and I learned how to navigate in this room, and so like now I know like, you know,",
    "start": "601275",
    "end": "607579"
  },
  {
    "text": "what it's like to turn and what it's like to go forward, etc., and before I was always trying to get to that exit. But now I know what dynamic- I know my dynamics model in general for this room.",
    "start": "607580",
    "end": "616535"
  },
  {
    "text": "And then someone says, \"No. No. No. I don't want to go to that exit because that one's, you know, closed or something. So go to that other exit.\"",
    "start": "616535",
    "end": "622010"
  },
  {
    "text": "And they tell me the reward function. They say, you know, there's a +1 for that exit now instead of there.",
    "start": "622010",
    "end": "627170"
  },
  {
    "text": "Then I can just re-plan with my, my dynamics model. So I don't need any more experience.",
    "start": "627170",
    "end": "633080"
  },
  {
    "text": "I can get zero shot transfer. So that can be really useful. So that's one of the other reasons why you",
    "start": "633080",
    "end": "638630"
  },
  {
    "text": "might want to just build models of the world in general. And there's some interesting evidence that when people play Atari games,",
    "start": "638630",
    "end": "644980"
  },
  {
    "text": "that they are probably systematically building models. What happens when I move the iceberg next to the polar bear?",
    "start": "644980",
    "end": "650960"
  },
  {
    "text": "And because then you can generalize those models to other experience. Okay. So how are we gonna write,",
    "start": "650960",
    "end": "657630"
  },
  {
    "start": "657000",
    "end": "791000"
  },
  {
    "text": "write down our model in this case. We're again just gonna have our normal state, action, transition, dynamics and reward,",
    "start": "657630",
    "end": "663310"
  },
  {
    "text": "and we're gonna assume that our model approximately represents our,",
    "start": "663310",
    "end": "668670"
  },
  {
    "text": "our transition model and our reward model. So we're assuming the Markov assumption here.",
    "start": "668670",
    "end": "675220"
  },
  {
    "text": "So we can represent our next state is just the previous state and action in a distribution over that,",
    "start": "675220",
    "end": "680615"
  },
  {
    "text": "and we'll similarly have that for the- for the reward. And we, we typically assume things are conditionally independent,",
    "start": "680615",
    "end": "688149"
  },
  {
    "text": "like what we've done before. So we just have a particular dynamics model that's conditioned on the state and action and a reward that is conditioned on the previous state and action.",
    "start": "688150",
    "end": "697220"
  },
  {
    "text": "And so if we wanted to do model learning, then we have the supervised learning problem that we've talked about a little bit before of,",
    "start": "698670",
    "end": "705190"
  },
  {
    "text": "uh, you have the state and action, and you want to predict your reward in next state. And so we have this regression problem and this density estimation problem,",
    "start": "705190",
    "end": "713290"
  },
  {
    "text": "then you can do it in all sorts of ways. You can, uh, you know, use mean squared error, you can use different forms of losses.",
    "start": "713290",
    "end": "720910"
  },
  {
    "text": "Um, and in fact, one of the ways we've recently made progress on our off-policy reinforcement learning is by using",
    "start": "720910",
    "end": "726970"
  },
  {
    "text": "different forms of losses than standard sort of maximum likelihood losses.",
    "start": "726970",
    "end": "732279"
  },
  {
    "text": "Uh, but generally here, we're gonna talk about maximum likelihood losses. So we can just do this, and of course,",
    "start": "732280",
    "end": "737650"
  },
  {
    "text": "in the- in the tabular case this is just [NOISE] counting. So if you just have a discrete set of states and actions,",
    "start": "737650",
    "end": "743740"
  },
  {
    "text": "you can just count how many times did I start in this state and action, and go to state one, versus how many times they start in this state and action and go to state two.",
    "start": "743740",
    "end": "751660"
  },
  {
    "text": "And so you just count those up and then normalize. And in general, there's a huge number of different ways that you can represent these.",
    "start": "751660",
    "end": "760764"
  },
  {
    "text": "Uh, and I think one of the ones that I think is particularly interesting is Bayesian, not the- Bayesian Deep Neural Networks.",
    "start": "760765",
    "end": "767035"
  },
  {
    "text": "They've been pretty hard to tune so far. Oh, another policy, you know, Bayesian deep neural networks.",
    "start": "767035",
    "end": "775030"
  },
  {
    "text": "[NOISE] Um, I think one of the reasons those could be really powerful is they can explicitly represent uncertainty,",
    "start": "775030",
    "end": "780070"
  },
  {
    "text": "but so far they've been pretty hard to train. But I think that there's, you know, a lot of really- there's some really simple models we can use as",
    "start": "780070",
    "end": "786190"
  },
  {
    "text": "well as some really rich function approximators for these models. Okay. So if we're in the table lookup case,",
    "start": "786190",
    "end": "793779"
  },
  {
    "start": "791000",
    "end": "1200000"
  },
  {
    "text": "we're just averaging counts. So we're just counting as I said this state action, next state tuples dividing by the number of times we've taken,",
    "start": "793780",
    "end": "801760"
  },
  {
    "text": "uh, that action in that state, and we similarly just average all the rewards,",
    "start": "801760",
    "end": "807130"
  },
  {
    "text": "so this should be the reward scene, for the times that we were in that state, took that action,",
    "start": "807130",
    "end": "813790"
  },
  {
    "text": "and what was the reward we got on that time step.",
    "start": "813790",
    "end": "816769"
  },
  {
    "text": "So let's think about an example for what that looks like here. So a long time ago, we introduced this AB example,",
    "start": "819780",
    "end": "826540"
  },
  {
    "text": "where we have, um, a state that goes to the- a state A that goes to action in state B, and then after that,",
    "start": "826540",
    "end": "833200"
  },
  {
    "text": "it either goes to a terminal state where it got a reward of 1 with 75% probability,",
    "start": "833200",
    "end": "838990"
  },
  {
    "text": "or it goes to a terminal state where it gets a reward of 0 with 25% probability.",
    "start": "838990",
    "end": "844060"
  },
  {
    "text": "And imagine that we've experienced something in this world, so there's no actions here, there's a single action.",
    "start": "844060",
    "end": "850315"
  },
  {
    "text": "It's really a Markov reward process rather than a decision process,",
    "start": "850315",
    "end": "855715"
  },
  {
    "text": "but we can get our observations. So let's say, we start in state A, and then we got a reward of 0,",
    "start": "855715",
    "end": "861100"
  },
  {
    "text": "and went to B and got a 0. And then we had a whole set of times, we have 6 times, where we started at B,",
    "start": "861100",
    "end": "867385"
  },
  {
    "text": "and we've got a reward of 1, and then we got started in state B and got a reward of 0.",
    "start": "867385",
    "end": "872770"
  },
  {
    "text": "And now we can construct a table lookup model from this. And just to refresh our memories, um,",
    "start": "872770",
    "end": "879790"
  },
  {
    "text": "so we talked about the fact that if you do temporal difference learning in this problem with a tabular representation,",
    "start": "879790",
    "end": "886165"
  },
  {
    "text": "meaning one row for each state, so that's just two states; A and B. That if you do infinite replay on this set of experience that it's equivalent if",
    "start": "886165",
    "end": "895779"
  },
  {
    "text": "you took this data and estimated a Markov decision process model with it,",
    "start": "895780",
    "end": "902770"
  },
  {
    "text": "and then did planning with that to evaluate the optimal policy, or the policy that you're using to gather the data in this case.",
    "start": "902770",
    "end": "911680"
  },
  {
    "text": "So that was an interesting equivalence that the TD is, um, giving you exactly the same solution as what if you",
    "start": "911680",
    "end": "917890"
  },
  {
    "text": "compute what's often called a certainty equivalence model, because you take your data, you estimate, you take the empirical as average of that data.",
    "start": "917890",
    "end": "926650"
  },
  {
    "text": "So you can say, \"If this was all the data in the world, what would be the model that could be associated with that,",
    "start": "926650",
    "end": "931810"
  },
  {
    "text": "with a maximum likelihood estimate.\" And then we do planning. So TD makes that assumption.",
    "start": "931810",
    "end": "937540"
  },
  {
    "text": "Let's just do a quick check of memory. Do Monte-Carlo methods converge to the same solution on this data?",
    "start": "937540",
    "end": "943970"
  },
  {
    "text": "So maybe take a minute, turn to somebody next to you, and decide whether or not they do, and why or why not.",
    "start": "947160",
    "end": "958360"
  },
  {
    "text": "Do you have a question?",
    "start": "958360",
    "end": "973839"
  },
  {
    "text": "Uh, as an offering, yes. [LAUGHTER]. Okay. I think that Monte-Carlo methods will converge to solution with",
    "start": "973840",
    "end": "982315"
  },
  {
    "text": "the minimum MSE's opposed to have MLE effect? Correct. The Monte-Carlo methods do not make an assumption of Markovian.",
    "start": "982315",
    "end": "991315"
  },
  {
    "text": "Um, so the- they are suitable in cases where the domain is not Markovian.",
    "start": "991315",
    "end": "997030"
  },
  {
    "text": "So in this case, they will converge to the- well in all cases for this policy evaluation. They're gonna converge to the minimum mean squared error.",
    "start": "997030",
    "end": "1003660"
  },
  {
    "text": "Yeah, question? So you're saying that if you used an ML model you probably converge them into MSE,",
    "start": "1003660",
    "end": "1009315"
  },
  {
    "text": "what if you are using a different loss? [OVERLAPPING] Good question. This is- this is- this is going to converge to the minimum MSE not the MLE.",
    "start": "1009315",
    "end": "1015540"
  },
  {
    "text": "[inaudible] If you are using a different loss [inaudible].",
    "start": "1015540",
    "end": "1024030"
  },
  {
    "text": "Would the Monte Carlo methods converge to the- I mean, depending on the loss or if you regularize. It's a great question, if you regularize it may converge to",
    "start": "1024030",
    "end": "1031050"
  },
  {
    "text": "different solutions than the minimum mean squared error depending on how you regularize or the loss you use. But in general, it will not converge to the same thing as if",
    "start": "1031050",
    "end": "1037980"
  },
  {
    "text": "you got the maximum likelihood estimate model, and then did planning with that or policy evaluation.",
    "start": "1037980",
    "end": "1044025"
  },
  {
    "text": "And the key difference is Monte Carlo is not making a Markovian assumption. So it does- it does not assume Markov.",
    "start": "1044025",
    "end": "1054179"
  },
  {
    "text": "And so in particular in this case, um, because I may have a guess of what the value of A will be under the Monte Carlo estimate.",
    "start": "1054180",
    "end": "1063760"
  },
  {
    "text": "There's only one sample of it. 0. Yeah, um, yes. So there's only- for Monte Carlo here,",
    "start": "1067460",
    "end": "1074295"
  },
  {
    "text": "we'll say I- I'm only looking at full returns that started with this particular state, and there's no bootstrapping.",
    "start": "1074295",
    "end": "1080490"
  },
  {
    "text": "So, um, the only time we saw A was when the return from A was 0.",
    "start": "1080490",
    "end": "1085620"
  },
  {
    "text": "Uh, but, you know, if the system is really Markov, that's not a very good solution because we have all this other evidence",
    "start": "1085620",
    "end": "1090930"
  },
  {
    "text": "that B is actually normally has a higher value, and we're not able to take advantage of that, um, whereas TD does.",
    "start": "1090930",
    "end": "1097335"
  },
  {
    "text": "So TD can say, \"Well, I know that V of A was 0 this one time.\" But in general, we think that V of A is equal to the immediate reward plus,",
    "start": "1097335",
    "end": "1107070"
  },
  {
    "text": "in this case there's no discounting, so value of B. And I have all this other- other evidence that the value of B is, in this case,",
    "start": "1107070",
    "end": "1114540"
  },
  {
    "text": "actually exactly equal to 0.75, um, because we have six examples of it being 1,",
    "start": "1114540",
    "end": "1120120"
  },
  {
    "text": "and two examples of it being 0. So we would, uh, have V of B is equal to 0.75,",
    "start": "1120120",
    "end": "1126600"
  },
  {
    "text": "and both Monte Carlo and, um, TD would agree on that. Because for- if you look at every time you started on B,",
    "start": "1126600",
    "end": "1135285"
  },
  {
    "text": "75% or how- you know, 75% of time you got a 1, the rest of the time you got a 0. So the- the value of that is 0.75.",
    "start": "1135285",
    "end": "1143130"
  },
  {
    "text": "So the TD estimate would say also V of A is equal to 0.75, the TD estimate.",
    "start": "1143130",
    "end": "1153490"
  },
  {
    "text": "So one of the reasons this comes up here, a- and notice this is a- this is not due to a sort of finite number of backup,",
    "start": "1153590",
    "end": "1161115"
  },
  {
    "text": "or sorry, I'll be careful, a finite amount of use of the data. So this is saying, if you sort of run this through TD many,",
    "start": "1161115",
    "end": "1168030"
  },
  {
    "text": "many times, and the Monte Carlo estimate is also getting access to all of the data. It's just saying this is all the data there is.",
    "start": "1168030",
    "end": "1174375"
  },
  {
    "text": "So an alternative would be, if you take this data and you build a model. So now we have a model that says,",
    "start": "1174375",
    "end": "1180929"
  },
  {
    "text": "the probability of going to B given you started in A is 1, you always go from- from, um, A to B.",
    "start": "1180930",
    "end": "1188190"
  },
  {
    "text": "In fact, you've only ever seen this once, but the one time you saw it, you went to B, uh, and we can use this to try to get simulated data.",
    "start": "1188190",
    "end": "1196830"
  },
  {
    "text": "So let me just- well, I'll go a couple more. So the idea in this case is that once you have your simulator,",
    "start": "1196830",
    "end": "1204179"
  },
  {
    "start": "1200000",
    "end": "1271000"
  },
  {
    "text": "you can use it to simulate samples, and then you can plan using that simulated data.",
    "start": "1204179",
    "end": "1209625"
  },
  {
    "text": "Now, initially, that might sound like why would you do that because you hide your previous data and maybe you could have directly, you know,",
    "start": "1209625",
    "end": "1215970"
  },
  {
    "text": "put it through a model-free based approach, like why would you first build a model,",
    "start": "1215970",
    "end": "1221025"
  },
  {
    "text": "and then generate data from it. But we'll see an example right now from that sort of AB example of why that might be beneficial.",
    "start": "1221025",
    "end": "1228900"
  },
  {
    "text": "So you- what we can do is we can get this- we can get the maximum likelihood estimate of the model or other estimates you might want to use,",
    "start": "1228900",
    "end": "1236880"
  },
  {
    "text": "and then you can sample from it. So in that example we just had here, our estimated transition model,",
    "start": "1236880",
    "end": "1245435"
  },
  {
    "text": "is that whenever we're in A we go to B. So when I'm in A, I can sample and I will go to B,",
    "start": "1245435",
    "end": "1252740"
  },
  {
    "text": "and that generates me a fake data point. Okay? And I could do this a whole bunch of times,",
    "start": "1252740",
    "end": "1258720"
  },
  {
    "text": "we get lots of fake data. Now, this fake data may or may not look like what the real-world does, it depends how good my model is.",
    "start": "1258720",
    "end": "1264585"
  },
  {
    "text": "But it's certainly data I can train with, and- and we'll see, in a minute, in a second why that's beneficial.",
    "start": "1264585",
    "end": "1270105"
  },
  {
    "text": "Okay? So if we go back to here, this is the real experience on the left.",
    "start": "1270105",
    "end": "1275760"
  },
  {
    "start": "1271000",
    "end": "2326000"
  },
  {
    "text": "So on the left-hand side we had all this real experience, and then what we did is we built a model from that,",
    "start": "1275760",
    "end": "1283455"
  },
  {
    "text": "and then we could sample from it. So we could have experience that looks very similar to the data that we've actually seen,",
    "start": "1283455",
    "end": "1288885"
  },
  {
    "text": "but we could also have experience like this.",
    "start": "1288885",
    "end": "1292450"
  },
  {
    "text": "Now, why could we have that, because we now have a simulator, and, uh, in our simulated- In our model,",
    "start": "1295190",
    "end": "1301230"
  },
  {
    "text": "we've seen cases where we started in A and we went to B. And in our model there have",
    "start": "1301230",
    "end": "1308549"
  },
  {
    "text": "been other times where we've started in B, and we've got a 1. So essentially we can kind of chain that experience together,",
    "start": "1308550",
    "end": "1313905"
  },
  {
    "text": "and simulate something that we never observed in the real world. And we're leveraging the fact here that it's Markov.",
    "start": "1313905",
    "end": "1320140"
  },
  {
    "text": "So if the domain isn't really Markov, we could end up getting data that looks very very different than what you could ever get in the real world.",
    "start": "1320720",
    "end": "1328004"
  },
  {
    "text": "But if it's Markov, then, um, it may still be an approximate model because",
    "start": "1328005",
    "end": "1333389"
  },
  {
    "text": "we only had a limited amount of data training our model. But now we can start to see conjunctions of states and actions,",
    "start": "1333390",
    "end": "1339945"
  },
  {
    "text": "uh, that we maybe never saw in our data. But we could update [NOISE] our model as we sample?",
    "start": "1339945",
    "end": "1345120"
  },
  {
    "text": "Uh, well, okay great question. Could you update your model as you sample? You could, but right now we're just sampling from our model.",
    "start": "1345120",
    "end": "1352139"
  },
  {
    "text": "So this, this is not real-world experience. So that could lead to confirmation bias, because it's like your model is giving you data,",
    "start": "1352140",
    "end": "1358920"
  },
  {
    "text": "and if you treat that as real data, and put that like into your model. It's not from the real world.",
    "start": "1358920",
    "end": "1364005"
  },
  {
    "text": "So you could end up sort of being overly confident, in, um, uh, because you're generating fake data and then treating it as if it's real.",
    "start": "1364005",
    "end": "1371355"
  },
  {
    "text": "How do we judge how confident we would be in our sample's experience? I guess like relative to how much training data we'd have to put in the model.",
    "start": "1371355",
    "end": "1377750"
  },
  {
    "text": "Exactly. So that's like, how would be, how do we know how confident to be and, And in general this is the issue of your models are gonna be pretty bad.",
    "start": "1377750",
    "end": "1384500"
  },
  {
    "text": "Sometimes if you have limited amounts of data. So some of the techniques we talked about for exploration, where we could, uh, drive these confidence intervals over how good the models are,",
    "start": "1384500",
    "end": "1391669"
  },
  {
    "text": "they apply here as well. So, um, if you only have a little bit of data you can use things like Hoeffding to say,",
    "start": "1391670",
    "end": "1397005"
  },
  {
    "text": "how sure am I about this reward model for example. Um, for most of today, we're not gonna talk about that that much,",
    "start": "1397005",
    "end": "1403260"
  },
  {
    "text": "but you can use that information to try to quantify how uncertain should you be, and how would that error kind of propagate.",
    "start": "1403260",
    "end": "1409230"
  },
  {
    "text": "Yeah. Um, so I guess I'm trying to like conceptually think about the next step is that we're,",
    "start": "1409230",
    "end": "1417150"
  },
  {
    "text": "we're building this model. We're gonna use a method to learn some sort of a policy or some sort of like,",
    "start": "1417150",
    "end": "1423404"
  },
  {
    "text": "way to act in the real world. If we have the model, can we just used a model when you are acting and",
    "start": "1423405",
    "end": "1429600"
  },
  {
    "text": "just basically run our state through the model and get, maybe like a distribution and just take the maximum action,",
    "start": "1429600",
    "end": "1436634"
  },
  {
    "text": "the m- The action that maximizes our reward? So, I guess, once you have the model",
    "start": "1436634",
    "end": "1442820"
  },
  {
    "text": "you could use it in lots of different ways to do planning. So one is you could do, if it's a small case, like here it's a table.",
    "start": "1442820",
    "end": "1449090"
  },
  {
    "text": "So you could use value iteration and solve this exactly. There's no reason to simulate data.",
    "start": "1449090",
    "end": "1455255"
  },
  {
    "text": "Um, but when you start to think about doing like Atari or other really high-dimensional problems, the planning problem alone is super expensive.",
    "start": "1455255",
    "end": "1462600"
  },
  {
    "text": "And so you might still want to do model-free methods for your planning with your simulated data.",
    "start": "1462600",
    "end": "1467730"
  },
  {
    "text": "And one of the reasons you might want to do that is because, um, we've, we've talked about different ways in Q learning to be more sample",
    "start": "1467730",
    "end": "1474090"
  },
  {
    "text": "efficient like you have a replay buffer and you can do episodic replay. But another alternative is you just generate a lot of data from your simulated model,",
    "start": "1474090",
    "end": "1481440"
  },
  {
    "text": "and then you replay over that a ton of times. And so that's another way to kind of, um, make more use out of your data. Yeah, question in the back.",
    "start": "1481440",
    "end": "1489345"
  },
  {
    "text": "If, um, if you don't want to make the Markov assumption,",
    "start": "1489345",
    "end": "1495090"
  },
  {
    "text": "can you so do the same but, uh, condition on the past of [inaudible]?",
    "start": "1495090",
    "end": "1501480"
  },
  {
    "text": "Yeah. Question, what if you don't wanna make the Markov assumption? Yes, and can you condition on the past, you absolutely can.",
    "start": "1501480",
    "end": "1506850"
  },
  {
    "text": "Um, that means you would build models that are a full function of the history. The problem with that is, you don't have very much data.",
    "start": "1506850",
    "end": "1512295"
  },
  {
    "text": "So you have to, if you want to condition on the entire history as essentially your state, you're always fine in terms of the Markov assumption,",
    "start": "1512295",
    "end": "1519150"
  },
  {
    "text": "but you'll just have really really little data to estimate the transition models. Particularly as the horizon goes on.",
    "start": "1519150",
    "end": "1524400"
  },
  {
    "text": "So it's often a trade off, like do you want to have better models? Well, it depends on your domain. Maybe it's really Markov. If it's not really Markov,",
    "start": "1524400",
    "end": "1530220"
  },
  {
    "text": "do you want better models with very little data? So, um, in general this sort of gets to",
    "start": "1530220",
    "end": "1535500"
  },
  {
    "text": "the function approximation error versus the sample estimation error. You can have error due to the fact that you don't have much data.",
    "start": "1535500",
    "end": "1541260"
  },
  {
    "text": "Or you can have error due to the fact that your function approximation is wrong. And often you're going to want to trade off between those two.",
    "start": "1541260",
    "end": "1547365"
  },
  {
    "text": "So, so this example, you know, you can end up getting this sampled experience that looks different than anything you've seen in reality.",
    "start": "1547365",
    "end": "1553960"
  },
  {
    "text": "Um, and then in this case if you now run Monte Carlo on the new data,",
    "start": "1553960",
    "end": "1560419"
  },
  {
    "text": "you may, you can get something that looks very similar if you've done TD on the original data.",
    "start": "1560420",
    "end": "1565680"
  },
  {
    "text": "So basically, instead of taking our real experience and then doing Monte Carlo using that for policy evaluation.",
    "start": "1566030",
    "end": "1574890"
  },
  {
    "text": "An alternative is to say, we really think that this is a Markov system, let's simulate a bunch of data and then you could run Monte Carlo learning on this, or TD learning on it.",
    "start": "1574890",
    "end": "1583725"
  },
  {
    "text": "Um, and you would get probably the same answer. So this is, you know, in contrast to what Monte Carlo would've converged to",
    "start": "1583725",
    "end": "1590730"
  },
  {
    "text": "before which was V(A) = 0 and V(B) = 0.75. Now again you might say, okay but do I really want to do this, like if,",
    "start": "1590730",
    "end": "1598409"
  },
  {
    "text": "I, if I really didn't think the system was Markov, I wouldn't have run Monte Carlo on my fake data either.",
    "start": "1598410",
    "end": "1605130"
  },
  {
    "text": "But I think this illustrates, um, what you can do once you have this sampling and just",
    "start": "1605130",
    "end": "1610920"
  },
  {
    "text": "shows that allows you to make all sorts of choices. So maybe there you wanna have sort of a two-step Markov process or you want to do different,",
    "start": "1610920",
    "end": "1618179"
  },
  {
    "text": "make different assumptions in your model. And then after that you wanna do a lot of planning with it. So that just allows you to first take your data, compute a model,",
    "start": "1618180",
    "end": "1625755"
  },
  {
    "text": "and then you can decide how you're going to use that to try to do planning. And we'll see a particular way to do that shortly.",
    "start": "1625755",
    "end": "1632595"
  },
  {
    "text": "Now as you guys were just asking me about, um, if we have a bad model then we're gonna compute a sub-optimal policy in general.",
    "start": "1632595",
    "end": "1641895"
  },
  {
    "text": "You know, we might be incredibly lucky. Um, because ultimately for your decisions, we only need to decide whether,",
    "start": "1641895",
    "end": "1648000"
  },
  {
    "text": "you know, V(s,a1) is greater than V(s,a2).",
    "start": "1648000",
    "end": "1653325"
  },
  {
    "text": "So ultimately we're gonna be making comparison, pairwise comparison decisions. So you might be, have a really bad model and still end up with a good policy.",
    "start": "1653325",
    "end": "1661170"
  },
  {
    "text": "Um, but in general, if your model is bad and you do planning in general, your policy is also gonna be sub-optimal.",
    "start": "1661170",
    "end": "1667635"
  },
  {
    "text": "Um, and so one approach if the model is wrong,",
    "start": "1667635",
    "end": "1672765"
  },
  {
    "text": "um, is to do model-free reinforcement learning on the original data. So not to do model-based planning.",
    "start": "1672765",
    "end": "1680775"
  },
  {
    "text": "It's not clear that always solves the issue. So it depends why your model is wrong.",
    "start": "1680775",
    "end": "1686190"
  },
  {
    "text": "Um, if your model is wrong because you've picked the wrong parametric class or because the system is not Markov,",
    "start": "1686190",
    "end": "1692565"
  },
  {
    "text": "you're doing Q learning that's not gonna solve your problem because Q learning also assumes the world is Markov.",
    "start": "1692565",
    "end": "1697590"
  },
  {
    "text": "So model-free, you know, it depends on why, you know, why is it wrong? It depends on why? Whether or not this helps.",
    "start": "1697590",
    "end": "1705970"
  },
  {
    "text": "Um, another is to reason explicitly about your model uncertainty. And this is goes back to the exploration, exploitation.",
    "start": "1708260",
    "end": "1715965"
  },
  {
    "text": "Now again this only deals with a particular form of wrongness. Um, it deals with the fact that you might have sampling estimation error.",
    "start": "1715965",
    "end": "1723045"
  },
  {
    "text": "But it's still making the basic assumption that your model class is correct. So for example, if you're modeling the world as, um,",
    "start": "1723045",
    "end": "1731039"
  },
  {
    "text": "a multinomial distribution, and you don't have very much data then your prior metric estimates will be off. But if the world really isn't a multinomial,",
    "start": "1731040",
    "end": "1738269"
  },
  {
    "text": "um, then all bets are off. So it's always good to know sort of what assumptions we're making in",
    "start": "1738270",
    "end": "1744780"
  },
  {
    "text": "our algorithms and then what sort of forms of uncertainty we're accounting for. Now I guess I'll just say, say one other thing which is a",
    "start": "1744780",
    "end": "1752400"
  },
  {
    "text": "little bit subtle which I find super interesting which is, um, if you have a really good model, it is, a generally said it or if you have a perfect model,",
    "start": "1752400",
    "end": "1761910"
  },
  {
    "text": "and perfect estimate of the parameters, that is sufficient to make good decisions. If you were trying to train your model and you have",
    "start": "1761910",
    "end": "1768090"
  },
  {
    "text": "a restricted amount of data or restricted sort of expressivity of your model, um, then a model that has higher predictive accuracy can",
    "start": "1768090",
    "end": "1776010"
  },
  {
    "text": "in fact be worse when you use it to make decisions. And the intuition I like to think of is this, we,",
    "start": "1776010",
    "end": "1782250"
  },
  {
    "text": "we discovered this a few years ago, um, others have discovered it too, we were thinking about it for an intelligent tutoring system.",
    "start": "1782250",
    "end": "1787290"
  },
  {
    "text": "Um, the challenges, let's imagine that you have like a really complicated state-space. Say, um, you're trying to model what a kitchen looks like when someone's making tea,",
    "start": "1787290",
    "end": "1796455"
  },
  {
    "text": "and there's all sorts of features, you know, like there is steam, maybe it's a sunset outside or when there's also the temperature of the water.",
    "start": "1796455",
    "end": "1802920"
  },
  {
    "text": "And in order to make tea you need the water to be over 100 degrees. And in fact that's the only feature you need to pay",
    "start": "1802920",
    "end": "1808679"
  },
  {
    "text": "attention to in order to successfully make tea. But if you were trying to just do, um, build a model of the world,",
    "start": "1808680",
    "end": "1815190"
  },
  {
    "text": "you're trying to model the sunset, you're trying to model the steam etc. And there can be a huge number of features that you're trying to capture in your sort of,",
    "start": "1815190",
    "end": "1821880"
  },
  {
    "text": "you know, maybe slightly improvisional network. And so if you just try to fit the best maximum-likelihood estimate model,",
    "start": "1821880",
    "end": "1829005"
  },
  {
    "text": "it might not be the one that captures the features that you need to make decisions. And so models that look better in terms of",
    "start": "1829005",
    "end": "1836450"
  },
  {
    "text": "predictive accuracy may not always be better in terms of decision making. So that was this issue we encountered a few years ago,",
    "start": "1836450",
    "end": "1842975"
  },
  {
    "text": "and I think it just illustrates why the models that we need to build when we want to make decisions are not necessarily the same models we need for predictive accuracy.",
    "start": "1842975",
    "end": "1850660"
  },
  {
    "text": "So it's important where possible to know which of your features do you care about in terms of utility and value.",
    "start": "1850660",
    "end": "1857020"
  },
  {
    "text": "Okay. So let's talk a little bit about simulation-based search. Um, who here has seen forward search before in one of their classes?",
    "start": "1857810",
    "end": "1864810"
  },
  {
    "text": "Okay, a number people, but not everybody. Um, so forward search algorithms. What we're going to talk about now is different ways instead of",
    "start": "1864810",
    "end": "1871800"
  },
  {
    "text": "doing Q learning on our simulated data. Okay, we've got a model, what's another way we can use it to try to make decisions?",
    "start": "1871800",
    "end": "1877784"
  },
  {
    "text": "One way is to do forward search. So how does forward search work? The idea in forward search is that we're gonna think about all the actions we could take.",
    "start": "1877785",
    "end": "1885990"
  },
  {
    "text": "So let's say here we just have two actions, A1 and A2. And then we're going to think about all the possible next states we might get to.",
    "start": "1885990",
    "end": "1892860"
  },
  {
    "text": "So let's say it's a fairly small world, so we just have S1 and S2.",
    "start": "1892860",
    "end": "1897669"
  },
  {
    "text": "So in this current state, I could either take action one or action two and after I",
    "start": "1898130",
    "end": "1905090"
  },
  {
    "text": "take those actions I could either transition to state one or state two. And then after I get to whatever I state- I get state,",
    "start": "1905090",
    "end": "1910940"
  },
  {
    "text": "I get to, I again can make a decision A1 or A2. Because that's my action space here.",
    "start": "1910940",
    "end": "1917580"
  },
  {
    "text": "And then after I take that action, I again my transition or maybe sometimes I terminate it.",
    "start": "1917580",
    "end": "1923010"
  },
  {
    "text": "So this is a terminal state. Maybe my robot falls off or falls down or things like that,",
    "start": "1923010",
    "end": "1930435"
  },
  {
    "text": "or maybe else I go to another state. [NOISE] And I can think of sort of like these branching set of futures,",
    "start": "1930435",
    "end": "1937979"
  },
  {
    "text": "and I can roll them out as much as I want to. Let's say I want to think about the next h steps for example. And then I halt.",
    "start": "1937979",
    "end": "1945960"
  },
  {
    "text": "And once I have that, I can use that information and the- my sort of reward- well let me just say one more thing which is,",
    "start": "1945960",
    "end": "1952784"
  },
  {
    "text": "um, as I do these sort of like simulated features, I can think about what the reward would I get under these different features.",
    "start": "1952785",
    "end": "1960990"
  },
  {
    "text": "Because right now I'm assuming I have a model. So this is given a model. So I can think about if I took this state as t and",
    "start": "1960990",
    "end": "1969450"
  },
  {
    "text": "took action a2 what reward would I get? And then down here, I can think about if I got what reward I would get in s2, a2.",
    "start": "1969450",
    "end": "1978360"
  },
  {
    "text": "So I can think of sort of generating different features and then summing up the rewards, um, it will give me a series of reward along any of those different paths.",
    "start": "1978360",
    "end": "1987240"
  },
  {
    "text": "And then in order to figure out the value of these different sort of actions or the best action I should take,",
    "start": "1987240",
    "end": "1993210"
  },
  {
    "text": "what I can do is I can take a max over actions and I can take an expectation over states.",
    "start": "1993210",
    "end": "2002120"
  },
  {
    "text": "And I always know how to take that expectation because I'm assuming I have a model. So I can always think about what would be the probability of me",
    "start": "2002120",
    "end": "2010280"
  },
  {
    "text": "getting to that particular state given my parent action and the state I'm coming from.",
    "start": "2010280",
    "end": "2015900"
  },
  {
    "text": "So what happens, in this case, is as I roll out all these potential futures up until a certain depth.",
    "start": "2017290",
    "end": "2023779"
  },
  {
    "text": "In the case of Go or something like that, it would be until you win the game or lose the game. And then I want to back up.",
    "start": "2023780",
    "end": "2029450"
  },
  {
    "text": "So you can think of this [BACKGROUND] as sort of doing a not very efficient form of dynamic programming [NOISE]. Because, why is it not so efficient?",
    "start": "2029450",
    "end": "2035795"
  },
  {
    "text": "Because there might be many states in here which are identical. And I'm not- I'm not aliasing those or I'm not treating them as identical.",
    "start": "2035795",
    "end": "2044735"
  },
  {
    "text": "I'm saying I'm going to separately think about for each of those dates I would reach what would be the future reward I would get from that state under",
    "start": "2044735",
    "end": "2050870"
  },
  {
    "text": "different sequences of actions and resulting states. And then if I want to figure out how to act,",
    "start": "2050870",
    "end": "2056105"
  },
  {
    "text": "I go from my leaves and I take a max over all the, so let's say in this case I just made up for like a symbol small one.",
    "start": "2056105",
    "end": "2067550"
  },
  {
    "text": "This is a1, a2, and let's say at that point I terminate.",
    "start": "2067550",
    "end": "2073885"
  },
  {
    "text": "So I got like r(s, a1) and r(s, a2).",
    "start": "2073885",
    "end": "2079450"
  },
  {
    "text": "So anytime I have a structure that looks like that, what I do is I take a max and the reward here is equal to whichever of those was bigger.",
    "start": "2079450",
    "end": "2087724"
  },
  {
    "text": "Let's imagine that it was a2 that was bigger. So if I want to compute I basically roll all of these out,",
    "start": "2087725",
    "end": "2094849"
  },
  {
    "text": "computing like getting a sample of the reward, and the next state is I go all the way out.",
    "start": "2094850",
    "end": "2100085"
  },
  {
    "text": "And then to get the value at the root, whenever I see two- two, like a series of action nodes,",
    "start": "2100085",
    "end": "2107329"
  },
  {
    "text": "I take a max over all of them, which was just like in our Bellman backup we're taking a max over the action that allows us to have the best future rewards.",
    "start": "2107330",
    "end": "2115760"
  },
  {
    "text": "And then whenever I get to states- I'll do another one here. So let's say now I have two states s1 and s2,",
    "start": "2115760",
    "end": "2124100"
  },
  {
    "text": "one of them has this value s1 and this one has value s2.",
    "start": "2124100",
    "end": "2129125"
  },
  {
    "text": "And I want to figure out for this action, what my new value is then this is going to be equal to the probability of s1 given.",
    "start": "2129125",
    "end": "2137464"
  },
  {
    "text": "Let's say I came from s0, s0, a1 times V(s1) plus probability of s2,",
    "start": "2137465",
    "end": "2144830"
  },
  {
    "text": "s0, a1 times V(s2). This is exactly like, uh,",
    "start": "2144830",
    "end": "2151579"
  },
  {
    "text": "um, when we do a Bellman backup, that we think about all the next states I could get two under the current action and current state times the value of each of those.",
    "start": "2151580",
    "end": "2159020"
  },
  {
    "text": "Does that make sense? So we construct this tree out, and then in order to compute the value,",
    "start": "2159020",
    "end": "2165530"
  },
  {
    "text": "we do one of two operations for either we take the max over the actions, if it's, uh, um, an action nodes, or we take an expectation.",
    "start": "2165530",
    "end": "2175670"
  },
  {
    "text": "So these are called expecting max trees. Some of you guys might have seen these before in AI.",
    "start": "2175670",
    "end": "2184640"
  },
  {
    "text": "Sometimes people often talk about minimax trees. So if you're playing game theory where the other agent gets",
    "start": "2184640",
    "end": "2189680"
  },
  {
    "text": "to try to minimize your value and you get to maximize it. This is very similar except for we're doing an expectation over",
    "start": "2189680",
    "end": "2196100"
  },
  {
    "text": "next states and a max over actions, okay? And it's exactly like dynamic program but more inefficient.",
    "start": "2196100",
    "end": "2203900"
  },
  {
    "text": "But we're gonna see why we would want to do that in a second. So does anyone have questions about this?",
    "start": "2203900",
    "end": "2208920"
  },
  {
    "text": "Okay. All right. So this is all- and the way we do this is that we have to have a model, because if we don't have a model right now we can't, uh,",
    "start": "2209290",
    "end": "2217115"
  },
  {
    "text": "compute this expecting max exact- exactly because we're using that we know- like we're only expanding two states here.",
    "start": "2217115",
    "end": "2223865"
  },
  {
    "text": "Um, and we- we- in order to figure out how much weight we want to put on each of those two states, we need to know the probability of getting to each of them.",
    "start": "2223865",
    "end": "2231395"
  },
  {
    "text": "And so that's where we- we're using the model here, and we're using the model to get the rewards.",
    "start": "2231395",
    "end": "2236495"
  },
  {
    "text": "So simulation-based search is similar, um, [NOISE] except for we're just going to simulate out with a model.",
    "start": "2236495",
    "end": "2242825"
  },
  {
    "text": "We're not going to compute all of these sort of, um, exponentially growing number of futures.",
    "start": "2242825",
    "end": "2248710"
  },
  {
    "text": "Instead, we're just gonna say I'm gonna start here, and I have a model and I need to have some policy here.",
    "start": "2248710",
    "end": "2254995"
  },
  {
    "text": "But let's say I have a policy pi, and then I just use that. So I look at my policy for the current state and it tells me something to do.",
    "start": "2254995",
    "end": "2262130"
  },
  {
    "text": "So I followed that action, and then I go into my model and I sample in s prime.",
    "start": "2262130",
    "end": "2268385"
  },
  {
    "text": "So I look up my model and I say, \"What would be the next state, given that I was in",
    "start": "2268385",
    "end": "2274340"
  },
  {
    "text": "this particular state and took that action and I just simulate one next state?\" This is just like how we could have simulated data from our models before.",
    "start": "2274340",
    "end": "2282200"
  },
  {
    "text": "So let's say that got me to here, which was state s1. And then I look up again. I look up to my policy and I say, \"What is the policy for s1?\"",
    "start": "2282200",
    "end": "2290135"
  },
  {
    "text": "Let's say that's a2 and then I also- then I follow it down to here. So just simulate out a trajectory.",
    "start": "2290135",
    "end": "2296809"
  },
  {
    "text": "Just following my policy, simulating it out and I go until it terminates. And that gives me one return of how good that policy is.",
    "start": "2296810",
    "end": "2305700"
  },
  {
    "text": "So in these sort of cases, we can just simulate complete trajectories with the model.",
    "start": "2308290",
    "end": "2314270"
  },
  {
    "text": "Uh, and once we have those you could do something like model-free RL over those simulated trajectories,",
    "start": "2314270",
    "end": "2320120"
  },
  {
    "text": "which either can be Monte Carlo or it could be something like TD learning.",
    "start": "2320120",
    "end": "2324690"
  },
  {
    "text": "So if we think of this as sort of doing- like, in order to do that simulation we need some sort of policy.",
    "start": "2325510",
    "end": "2333335"
  },
  {
    "start": "2326000",
    "end": "2540000"
  },
  {
    "text": "So you have to have some way to pick actions in our sort of simulated world when we think about being in the state and picking an action,",
    "start": "2333335",
    "end": "2339035"
  },
  {
    "text": "how do we know what action to take? We follow our current simulation policy. So let's say we wanted to do effectively one step of policy improvement.",
    "start": "2339035",
    "end": "2347375"
  },
  {
    "text": "So you have a policy, you have your model, and then you start off in a state and for each",
    "start": "2347375",
    "end": "2352730"
  },
  {
    "text": "of the possible actions you simulate out trajectories. So this is like doing Monte Carlo rollouts.",
    "start": "2352730",
    "end": "2359130"
  },
  {
    "text": "So I started my state. So this is, let's say I'm really in a state s_t,",
    "start": "2360760",
    "end": "2365935"
  },
  {
    "text": "and I will need to figure out what to do next. So I start in that state s_t, and then in my head before I take an action in the real world,",
    "start": "2365935",
    "end": "2371980"
  },
  {
    "text": "I think about all the actions I could take, and then I just do roll outs from each of them under a behavior policy pi,",
    "start": "2371980",
    "end": "2377934"
  },
  {
    "text": "then I pick the max over those. So I'm really in state s_t, and then in my brain,",
    "start": "2377935",
    "end": "2383310"
  },
  {
    "text": "I think about doing a_1, and then I do lots of roll outs from that under my policy pi. And then I do a_2 and do lots of roll outs out of that.",
    "start": "2383310",
    "end": "2391510"
  },
  {
    "text": "a_3, this is all in my head, and that basically gives me now an estimate of Q s_t, a_1 under pi.",
    "start": "2391510",
    "end": "2402529"
  },
  {
    "text": "So it's as if I was to take this action then follow pi, what would be my estimated Q function, then I do that for each of the actions, and then I can take a max.",
    "start": "2404120",
    "end": "2412145"
  },
  {
    "text": "So this is sort of like doing one step of policy improvement, because this is going to depend on whatever my simulation policy is, does that make sense?",
    "start": "2412145",
    "end": "2420190"
  },
  {
    "text": "So we have some existing simulation policy, I haven't told you how we get it, and then we use it to simulate out experience.",
    "start": "2420190",
    "end": "2427290"
  },
  {
    "text": "Okay. So the question is whether or not we can actually do better than one step of policy improvement,",
    "start": "2428040",
    "end": "2433474"
  },
  {
    "text": "because like how do we get these simulation policies? Like, okay, if we had a simulation policy and it was good, we could do this one step,",
    "start": "2433475",
    "end": "2439575"
  },
  {
    "text": "but how could we do this in a more general setting? Well, the idea is that, um, if you have this model,",
    "start": "2439575",
    "end": "2446335"
  },
  {
    "text": "you could actually compute the optimal values by doing this Expectimax Tree. So like I was in this state St,",
    "start": "2446335",
    "end": "2452795"
  },
  {
    "text": "and instead of just thinking about- so remember in simulation based search we're just going to follow out one trajectory,",
    "start": "2452795",
    "end": "2459745"
  },
  {
    "text": "but in the Expectimax tree we can think of, well, what if I did a1 or a2, and after that, whether I went to S1 or S2,",
    "start": "2459745",
    "end": "2468550"
  },
  {
    "text": "and then what action should I do there? And I can think of basically trying to compute",
    "start": "2468550",
    "end": "2474790"
  },
  {
    "text": "the optimal Q function under my approximate model for the current state, okay?",
    "start": "2474790",
    "end": "2480540"
  },
  {
    "text": "The problem with that is that this tree gets really big. So in general, um, the size of the tree is going to scale at least with",
    "start": "2480540",
    "end": "2489635"
  },
  {
    "text": "S times A to the H. If H is the horizon, because at each step, this is why it's not as efficient at dynamic programming,",
    "start": "2489635",
    "end": "2498540"
  },
  {
    "text": "at- at each step you're going to think about all the possible next states, and then all the possible next actions. And so this tree is growing exponentially with the horizon.",
    "start": "2498540",
    "end": "2505670"
  },
  {
    "text": "And if you think of something like AlphaGo, um, that are playing, you know, for a number of time steps before someone wins or loses,",
    "start": "2505670",
    "end": "2512545"
  },
  {
    "text": "this H might be somewhere between 50 to 200. So if you have anything larger than an extremely small state space like one,",
    "start": "2512545",
    "end": "2519590"
  },
  {
    "text": "then this is not gonna be, not gonna be feasible, okay? So the idea with a Monte Carlo Tree Search is that,",
    "start": "2519590",
    "end": "2527640"
  },
  {
    "text": "okay we'd like to do better. In any case we need some sort of simulation policy if we're going to do this at all, and we can't be as",
    "start": "2527640",
    "end": "2534140"
  },
  {
    "text": "computationally intractable as full Expectimax. So how do we do something in between?",
    "start": "2534140",
    "end": "2539560"
  },
  {
    "text": "So- so the idea with Monte-Carlo Tree Search is to try to kind of get the best of both worlds,",
    "start": "2539740",
    "end": "2544980"
  },
  {
    "start": "2540000",
    "end": "2928000"
  },
  {
    "text": "where what we'd really like is the Expectimax Tree where we think about all possible futures and take a max over those,",
    "start": "2544980",
    "end": "2550755"
  },
  {
    "text": "um, but instead, we need to do that in a little bit more computationally tractable way, and why might that be possible?",
    "start": "2550755",
    "end": "2557080"
  },
  {
    "text": "Well, let's think about this. If we have our initial starting state, let's say this is our general tree, that's going to all of these nodes.",
    "start": "2557080",
    "end": "2565610"
  },
  {
    "text": "Some of these potential ways of acting might be really really bad. So some of these that might be clear, very early,",
    "start": "2565610",
    "end": "2572089"
  },
  {
    "text": "like with very little amounts of data that, or very little amounts of roll outs that in fact,",
    "start": "2572090",
    "end": "2577925"
  },
  {
    "text": "these are ways you would never want to play Go, because you're going to immediately lose. And so then, you don't need to bother sort of continuing to spend a lot of",
    "start": "2577925",
    "end": "2585375"
  },
  {
    "text": "computational effort fleshing out that tree when something else might look much better. So that's kinda the intuition here,",
    "start": "2585375",
    "end": "2591840"
  },
  {
    "text": "is that what we're gonna do is we're going to get us, construct a partial search tree. So we're going to start at the current state,",
    "start": "2591840",
    "end": "2599570"
  },
  {
    "text": "and we can sample actions in next state, just like in simulation based search. So maybe we first sample A1,",
    "start": "2599570",
    "end": "2606010"
  },
  {
    "text": "and then we sample S1 again, and then we sample A2. So we start off and we're kind of,",
    "start": "2606010",
    "end": "2611650"
  },
  {
    "text": "the very first round it looks exactly like simulation based search, but the idea is that then we can do this multiple times and slowly fill out the tree.",
    "start": "2611650",
    "end": "2618800"
  },
  {
    "text": "So maybe next time we happen to sample A2, and then maybe we sample S2,",
    "start": "2618800",
    "end": "2624125"
  },
  {
    "text": "and then sample A1, and so you can think of sort of slowly filling in this Expectimax Tree.",
    "start": "2624125",
    "end": "2629790"
  },
  {
    "text": "And in the limit, um, you will fill in the entire Expectimax tree. It's just that in practice you almost never will",
    "start": "2629790",
    "end": "2636250"
  },
  {
    "text": "because it's computationally intractable. So what we're gonna do is do this, and we'll do this for,",
    "start": "2636250",
    "end": "2643640"
  },
  {
    "text": "sort of a number of simulation episodes, each simulation episode can be thought of is you start at the root.",
    "start": "2643640",
    "end": "2648800"
  },
  {
    "text": "This is the root node and current state you're really at, and then you roll out until you reach a terminal state or a horizon H,",
    "start": "2648800",
    "end": "2656355"
  },
  {
    "text": "and then you go back to the start state and you again make another trajectory. And when you're done with all of this,",
    "start": "2656355",
    "end": "2662339"
  },
  {
    "text": "you can do the same thing you would do in Expectimax, in that you're always gonna take a max over actions and an expectation over states.",
    "start": "2662340",
    "end": "2671700"
  },
  {
    "text": "You only will have filled in part of the tree, so part of the tree might be missing.",
    "start": "2673060",
    "end": "2677970"
  },
  {
    "text": "So in order to do this, there's two key aspects. One is, what do you do in parts of the tree where you already have some data?",
    "start": "2680310",
    "end": "2688260"
  },
  {
    "text": "So like if you already have tried both actions in a state, which action should you pick again, and then what should you do when you reach like a node where there's",
    "start": "2688260",
    "end": "2696590"
  },
  {
    "text": "nothing else there or like there's only been one thing tried so far? So this is often called the tree policy and the roll out policy.",
    "start": "2696590",
    "end": "2703550"
  },
  {
    "text": "The roll out policy is for when you get to a node where, you know, you've only tried one thing,",
    "start": "2703550",
    "end": "2708920"
  },
  {
    "text": "or there's no more data, or you've never been there before. So for example, maybe you sample a state that you'd never reached before,",
    "start": "2708920",
    "end": "2714400"
  },
  {
    "text": "and so that's now a new node, and from then on you do a roll out policy. We'll show an example of this in a second.",
    "start": "2714400",
    "end": "2720590"
  },
  {
    "text": "And then the idea is, when we're thinking about computing the Q function, we're just gonna average over all rewards that are received from that node onwards.",
    "start": "2720600",
    "end": "2729430"
  },
  {
    "text": "This should seem a little bit weird, because we're not talking about maxes anymore, and we're not talking about doing- considering",
    "start": "2730130",
    "end": "2737380"
  },
  {
    "text": "explicitly like the expectation over states  in a formal way, we're just gonna average this.",
    "start": "2737380",
    "end": "2743329"
  },
  {
    "text": "The reason why this is okay is because, um, we're gonna, sort of sample actions in a way such that over time,",
    "start": "2743330",
    "end": "2751750"
  },
  {
    "text": "we're gonna sample actions that look better much more, and so we expect that, uh, eventually, the distribution of data is",
    "start": "2751750",
    "end": "2758440"
  },
  {
    "text": "gonna converge to the true Q. Yeah. Just to confirm, is it [inaudible] the simulation before, um,",
    "start": "2758440",
    "end": "2766200"
  },
  {
    "text": "there's different kind of averaging and moving parts because it seemed before we were also doing a bunch of roll outs and then combining this,",
    "start": "2766200",
    "end": "2773215"
  },
  {
    "text": "so that part is still the same, yes? Yes, great question, in many cases it's very similar though.",
    "start": "2773215",
    "end": "2778590"
  },
  {
    "text": "We're still gonna be sort of doing a bunch of simulations where we're gonna be averaging them. The question is, what is the policy we're using to do the, the, uh,",
    "start": "2778590",
    "end": "2785890"
  },
  {
    "text": "the roll outs is generally going to be changing for each roll out, instead of being identical across all roll outs,",
    "start": "2785890",
    "end": "2791160"
  },
  {
    "text": "and then the way we are gonna average them is also different. And really the key part I think is that, instead of using,",
    "start": "2791160",
    "end": "2798420"
  },
  {
    "text": "um, like when I said for, um, simple Monte Carlo search,",
    "start": "2798420",
    "end": "2804010"
  },
  {
    "text": "that assumes that you fix a policy and get a whole bunch of roll outs from that policy, just starting with different initial actions,",
    "start": "2804010",
    "end": "2810474"
  },
  {
    "text": "but then always following it. When you do Monte Carlo Tree Search and you also do say k roll outs, generally,",
    "start": "2810475",
    "end": "2816050"
  },
  {
    "text": "the policy will be different for each of the k roll outs, and that's on purpose so that you can hopefully get to a better policy.",
    "start": "2816050",
    "end": "2822500"
  },
  {
    "text": "So again, just to, and just to step back again, what are, you know, what's the whole loop of happening here?",
    "start": "2822680",
    "end": "2827750"
  },
  {
    "text": "So what's happening in this case is like you have your agent, is our robot and it's trying to figure out an action to take,",
    "start": "2827750",
    "end": "2835785"
  },
  {
    "text": "and then the real-world gives back a S prime and an R prime, and then what we're talking about right now is everything it needs to do in its head,",
    "start": "2835785",
    "end": "2843819"
  },
  {
    "text": "like all these roll outs in order to decide the next action to take.",
    "start": "2843820",
    "end": "2849780"
  },
  {
    "text": "So it's going to do a whole bunch of planning before it takes its next action, generally it may then throw that whole tree away,",
    "start": "2849780",
    "end": "2856410"
  },
  {
    "text": "and then the world is gonna give it a new state and a new reward and then it's gonna do this whole process together again.",
    "start": "2856410",
    "end": "2862105"
  },
  {
    "text": "And so the key thing is that we need to decide what action to take next.",
    "start": "2862105",
    "end": "2866490"
  },
  {
    "text": "And we want to do so in a way that we're gonna get the best expected value, given the information we have so far.",
    "start": "2867600",
    "end": "2874890"
  },
  {
    "text": "Now in general, um, in Monte Carlo Tree Search, you might also have another step here where you might compute a new model.",
    "start": "2874890",
    "end": "2883170"
  },
  {
    "text": "So if you're doing this online, you might take your most recent data, retrain your model, given",
    "start": "2884160",
    "end": "2890200"
  },
  {
    "text": "that model rerun Monte Carlo Tree Search and now decide what you're gonna do on the next time step.",
    "start": "2890200",
    "end": "2895410"
  },
  {
    "text": "All right. So the key thing is really this tree policy and roll out, both of them make a difference.",
    "start": "2897550",
    "end": "2902885"
  },
  {
    "text": "Um, the roll out often is done randomly at least in the most basic vanilla version, there's tons of extensions for,",
    "start": "2902885",
    "end": "2908420"
  },
  {
    "text": "for this and the key part is that tree policy. So one of the really common ways to do this is called upper confidence tree search,",
    "start": "2908420",
    "end": "2918295"
  },
  {
    "text": "and this relates to what we're talking about for the last few weeks with exploration. So the idea is, is that when we're rolling out, so let's say,",
    "start": "2918295",
    "end": "2927055"
  },
  {
    "text": "we're in s_t and we're using our simulated models to think about the next actions and states we would be in.",
    "start": "2927055",
    "end": "2935119"
  },
  {
    "start": "2928000",
    "end": "3163000"
  },
  {
    "text": "So let's say, um, I get to state one. And at this point, I want to, let's say,",
    "start": "2935290",
    "end": "2941480"
  },
  {
    "text": "I've taken a1 and a2 in the past and I've ended up with, you know, I've done lots of roll outs from these and a number of roll outs from here.",
    "start": "2941480",
    "end": "2948050"
  },
  {
    "text": "And let's say, three times I got, let's say, I won the game. So three 1s and I got one 1 and one 0.",
    "start": "2948050",
    "end": "2953970"
  },
  {
    "text": "These guys. So, so the key is what, what action should I take next time I encounter s1 when I'm doing, where I roll",
    "start": "2953970",
    "end": "2961480"
  },
  {
    "text": "out and the idea is to be optimistic with respect to the data you have so far.",
    "start": "2961480",
    "end": "2966600"
  },
  {
    "text": "So essentially, we're going to treat this as a bandit. Uh, think of each decision point as its own bandit,",
    "start": "2966600",
    "end": "2974100"
  },
  {
    "text": "its own independent bandit and say, well, if I've taken all the actions I could for my current state,",
    "start": "2974100",
    "end": "2979130"
  },
  {
    "text": "what is my average reward I got from each of those actions under any roll out that I've taken this from that particular node in the graph and action,",
    "start": "2979130",
    "end": "2987175"
  },
  {
    "text": "and how many times did I take it? So you can get your empirical average for that state one in the,",
    "start": "2987175",
    "end": "2996400"
  },
  {
    "text": "in the graph a1 plus a discount factor that often looks like the number of times you've been in that particular node.",
    "start": "2996400",
    "end": "3004500"
  },
  {
    "text": "Okay. This is really a node in the graph and this is also for that node.",
    "start": "3004500",
    "end": "3012530"
  },
  {
    "text": "So we think about sort of, every time I've been in that node what, and taken that particular action,",
    "start": "3013830",
    "end": "3019020"
  },
  {
    "text": "what's the average reward I've gotten plus how many times if I done that? And it just allows you to be, um, uses optimism again.",
    "start": "3019020",
    "end": "3024609"
  },
  {
    "text": "Says, well, when I've reached different parts of the graph before using my simulated model, what things look good?",
    "start": "3024610",
    "end": "3030040"
  },
  {
    "text": "I'm gonna focus on making sure that that's the part of the tree that I flesh out more. Because that's the one where I think I'm gonna get",
    "start": "3030040",
    "end": "3035900"
  },
  {
    "text": "likely reach policies that have higher value. And so hopefully, that's gonna mean that I have le- need",
    "start": "3035900",
    "end": "3041070"
  },
  {
    "text": "less computation in order to compute a good policy. Does that make sense? So like if you had an oracle,",
    "start": "3041070",
    "end": "3047960"
  },
  {
    "text": "that could tell you what the optimal policy was, then, you would only need to fill in that part of the tree.",
    "start": "3047960",
    "end": "3053255"
  },
  {
    "text": "And what we're using here is we're saying well, given the data we've seen so far, we, kind of, only, you know,",
    "start": "3053255",
    "end": "3058785"
  },
  {
    "text": "focus on the parts of the tree that seem like they're going to be the ones that when we take our max over our actions, are gonna be the ones that we end up, uh,",
    "start": "3058785",
    "end": "3066150"
  },
  {
    "text": "propagating the values back up to the root. All right. So we've maintained an upper confidence bound over the reward of each arm,",
    "start": "3066150",
    "end": "3075950"
  },
  {
    "text": "um, using, sort of, exactly the same thing as what we've done before. And we're treating each of the nodes,",
    "start": "3075950",
    "end": "3082280"
  },
  {
    "text": "so each state node as a separate bandit.",
    "start": "3082280",
    "end": "3085970"
  },
  {
    "text": "And so that means essentially that, you know, the next time we reach that same node in the tree, we might do something different because the accounts will change.",
    "start": "3089060",
    "end": "3099335"
  },
  {
    "text": "TD, uh, uh, [inaudible] this is kind of like TD in, in the sense that [NOISE] if you reward for the bandit problem,",
    "start": "3099335",
    "end": "3104605"
  },
  {
    "text": "the value of that, of the, of the state or if the, um, existing action pair or is it the reward for just that transition?",
    "start": "3104605",
    "end": "3112420"
  },
  {
    "text": "It's a great question. So, and what is the reward for this bandit? We are gonna treat it as, um,",
    "start": "3112420",
    "end": "3117605"
  },
  {
    "text": "essentially the full roll out from that node, um, because that's what we're averaging over and we're cou- doing counts.",
    "start": "3117605",
    "end": "3124305"
  },
  {
    "text": "It's not like TD in the sense that we're doing this per node. So as I mentioned before, you know, you might have s1,",
    "start": "3124305",
    "end": "3130795"
  },
  {
    "text": "a1 appear in this part of the graph and s1 a1 appear over here, and we're not combining their accounts. We're treating every node as if it's totally distinct.",
    "start": "3130795",
    "end": "3137945"
  },
  {
    "text": "Even though often the model we'll be using to simulate will be Markov. But in ter- in sort of the tree, you can do it,",
    "start": "3137945",
    "end": "3143790"
  },
  {
    "text": "it's mostly that it becomes a lot more complicated for implementation. If you wanted to basically treat it as a graph instead of a tree.",
    "start": "3143790",
    "end": "3150785"
  },
  {
    "text": "Now, I think the other point that you're bringing up, we'll come back to in a second which is, is this a good idea [LAUGHTER] is,",
    "start": "3150785",
    "end": "3157680"
  },
  {
    "text": "is well, what are the limitations of the bandit setting? So we'll come back to that in a second. All right, so let's talk about this in the context of Go.",
    "start": "3157680",
    "end": "3165020"
  },
  {
    "start": "3163000",
    "end": "3286000"
  },
  {
    "text": "For those of you that haven't played Go or aren't too familiar with it. It's at least 2,500 years old. It's considered the classic hardest board game.",
    "start": "3165020",
    "end": "3171455"
  },
  {
    "text": "Um, and it's been known as a grand challenge task in AI for a very long period of time. Um, [NOISE] just to remind ourselves, um,",
    "start": "3171455",
    "end": "3180195"
  },
  {
    "text": "this does not involve decision-making in the case where the dynamics and reward model are unknown, but the rules of Go are known.",
    "start": "3180195",
    "end": "3186310"
  },
  {
    "text": "The reward structure is known, but it's an incredibly large search space. So if we think about the combinatorics of the number of possible,",
    "start": "3186310",
    "end": "3194234"
  },
  {
    "text": "um, boards that you can see, it's, it's extremely large. So uh, just briefly there's two different types of stones.",
    "start": "3194235",
    "end": "3204570"
  },
  {
    "text": "Um, most people probably know this. Uh, and typically it's played on a 19 by 19 board though people have also thought about,",
    "start": "3204570",
    "end": "3210020"
  },
  {
    "text": "you know, some people play on smaller boards. And you want to capture the most territory, and it's a finite game because there's",
    "start": "3210020",
    "end": "3216680"
  },
  {
    "text": "a finite number of places to put stones on the board. So it's a finite horizon [NOISE].",
    "start": "3216680",
    "end": "3227555"
  },
  {
    "text": "I'm gonna go through this part sort of briefly there's different ways to write down. You can write down the reward function, um, for this game in terms of,",
    "start": "3227555",
    "end": "3234030"
  },
  {
    "text": "uh, you know, different, you could do different features. The simplest one is just to look at whether or not white wins or black wins on",
    "start": "3234030",
    "end": "3240670"
  },
  {
    "text": "the game and in that case it's a very sparse reward signal and you only get reward at the very end. You just have to play out all the way and see which ga- which,",
    "start": "3240670",
    "end": "3248035"
  },
  {
    "text": "uh, which person won. And then your value function is essentially what's the expected probability of you winning in the current state.",
    "start": "3248035",
    "end": "3256840"
  },
  {
    "text": "So how does this work if we do Monte  Carlo evaluation? So let's imagine this is your current board.",
    "start": "3258680",
    "end": "3264425"
  },
  {
    "text": "And you have a particular policy. Then you play out against,",
    "start": "3264425",
    "end": "3271330"
  },
  {
    "text": "um, a stationary opponent is normally assumed. And then, at the end you see your outcomes and maybe you won twice and you lost twice.",
    "start": "3271330",
    "end": "3278365"
  },
  {
    "text": "And so then, the value for this current start state is a half. Okay, so how would we do Monte-Carlo tree search in this case?",
    "start": "3278365",
    "end": "3288030"
  },
  {
    "start": "3286000",
    "end": "3374000"
  },
  {
    "text": "So you start off and you have one single start state. So at this point you haven't simulated any part of the tree.",
    "start": "3288030",
    "end": "3294325"
  },
  {
    "text": "And so you've never taken any action, so you just sample randomly. So maybe you take a1. And then you follow your default policy and this is often random.",
    "start": "3294325",
    "end": "3303380"
  },
  {
    "text": "Though for things like AlphaGo you often want much better policies but, um,",
    "start": "3303380",
    "end": "3308549"
  },
  {
    "text": "you can just use a random policy and you'll just take random actions and get to next states and you do this until the end and you see you either win or lose.",
    "start": "3308550",
    "end": "3316160"
  },
  {
    "text": "Now, um, in this case it's a two player game. So we do a minimax tree instead of expectimax,",
    "start": "3316160",
    "end": "3322450"
  },
  {
    "text": "but the basic ideas are exactly the same. So that's my first roll out.",
    "start": "3322450",
    "end": "3328025"
  },
  {
    "text": "I'm gonna do lots of these roll outs before I figure out how I'm going to actually place my piece. All right. So the next time, so this is my second roll out.",
    "start": "3328025",
    "end": "3335809"
  },
  {
    "text": "This is the second roll out of my head [NOISE]. And say okay well, last time, um, you know, I, I took this action.",
    "start": "3335810",
    "end": "3342710"
  },
  {
    "text": "So now I have my default policy. So this time I'm gonna take the other action. Generally, you want to fill in all,",
    "start": "3342710",
    "end": "3348460"
  },
  {
    "text": "try all the actions at least once from a current node. Now, the particular order in which you try actions can make a big difference and, um,",
    "start": "3348460",
    "end": "3355975"
  },
  {
    "text": "early on there were significant savings by putting in action heuristics of what order to try actions in. For right now let's just imagine you have to try all actions, um, equally.",
    "start": "3355975",
    "end": "3365109"
  },
  {
    "text": "So in this case, now, you take the other action and then after that you've never tried anything from that action. So you just do a roll out.",
    "start": "3365110",
    "end": "3370890"
  },
  {
    "text": "Again, just acting randomly. Okay, and then, you repeat this.",
    "start": "3370890",
    "end": "3376255"
  },
  {
    "start": "3374000",
    "end": "4031000"
  },
  {
    "text": "So now, when you get to here, so let's say I've tried this before. So now, when I get to this node,",
    "start": "3376255",
    "end": "3382305"
  },
  {
    "text": "I have to pick, um, I have to do a max maybe using my UCT.",
    "start": "3382305",
    "end": "3387785"
  },
  {
    "text": "So I look at what was the reward for this one and the reward for this one, plus you know something about the counts.",
    "start": "3387785",
    "end": "3395130"
  },
  {
    "text": "All right. This is Q to make it clear. Okay. And so I pick whichever action happened to have looked better",
    "start": "3395310",
    "end": "3401470"
  },
  {
    "text": "in the roll outs I've done from it so far. And then, I'm gonna focus on expanding that part of the tree. And you keep doing this, and you're gonna slowly sort of build out",
    "start": "3401470",
    "end": "3410175"
  },
  {
    "text": "the tree and you do this until your computational budget expires. And then, you go to the bottom of the tree and you go all the way back",
    "start": "3410175",
    "end": "3416660"
  },
  {
    "text": "up where for each of the action nodes you're taking a max. And each of this state nodes you're doing an expecti- or you're doing a,",
    "start": "3416660",
    "end": "3423640"
  },
  {
    "text": "in this case minimax. You just construct a [inaudible].",
    "start": "3423640",
    "end": "3429490"
  },
  {
    "text": "So, so what does the opponent do, is it like a stationary opponent, like, what does that mean? Great question. Okay. So, um,",
    "start": "3429490",
    "end": "3435295"
  },
  {
    "text": "in reality, I think one of the other really big insights to why people got Go to work is self play.",
    "start": "3435295",
    "end": "3441415"
  },
  {
    "text": "So typically in this, you would use the current agent as the opponent.",
    "start": "3441415",
    "end": "3446210"
  },
  {
    "text": "So I take whatever policy I just computed for the other.",
    "start": "3446760",
    "end": "3451930"
  },
  {
    "text": "It- Look, particularly, let's imagine that like, I kept that tree. So it already knows what it could do. So, um, at each point, I would look at I would have",
    "start": "3451930",
    "end": "3458050"
  },
  {
    "text": "the other agent tell me what action it would do in that state. But one of the really so,",
    "start": "3458050",
    "end": "3463825"
  },
  {
    "text": "so I think self play was an incredibly important, um, insight for this. And why is it so important? Because if I play against a grand-master in Go,",
    "start": "3463825",
    "end": "3471579"
  },
  {
    "text": "I get no reward for a really long period of time. Um, and that's an incredibly hard thing for an agent to learn from,",
    "start": "3471580",
    "end": "3477460"
  },
  {
    "text": "because there's no other reward signal. And so basically, you're just playing these tons and tons and tons of games and like, there's just no signal for a really, really long time.",
    "start": "3477460",
    "end": "3484480"
  },
  {
    "text": "And so you need some sort of signal so you can start to bootstrap and actually get to a good policy. If I play against me like,",
    "start": "3484480",
    "end": "3490750"
  },
  {
    "text": "five minutes ago, I'm probably gonna beat them [LAUGHTER]. Um, and or at least half the time maybe I'll beat them. And so that allows because you can have two players",
    "start": "3490750",
    "end": "3499210"
  },
  {
    "text": "that are both bad and one of them's gonna win and one of them is gonna lose and you start to get signal. And so the self play idea has been",
    "start": "3499210",
    "end": "3507700"
  },
  {
    "text": "hugely helpful in the context of games, like, two-player games. Because it can mean that you can start to get",
    "start": "3507700",
    "end": "3513520"
  },
  {
    "text": "some reward signal about what things are successful or not and then you- It's both so, like, it both gives you, helps with",
    "start": "3513520",
    "end": "3519820"
  },
  {
    "text": "this sparse reward problem and it gives you a curriculum. Because you're always kind of, only playing in an environment that's a little bit",
    "start": "3519820",
    "end": "3526990"
  },
  {
    "text": "more difficult than perhaps what you can tolerate, can manage. And actually I think, um, it would be really,",
    "start": "3526990",
    "end": "3532630"
  },
  {
    "text": "really cool if we could figure out how to take the same ideas to a lot of other domains. Like if there are other ways to essentially make self play for things like medicine or,",
    "start": "3532630",
    "end": "3540910"
  },
  {
    "text": "[LAUGHTER] um, uh, customer relations or things like that. It would be really great because it's often really hard to get the sort of reward signal you want.",
    "start": "3540910",
    "end": "3548095"
  },
  {
    "text": "And that's one of the really nice things here. Self-play, what if we get stuck in some local extrema?",
    "start": "3548095",
    "end": "3554260"
  },
  {
    "text": "Absolutely can happen, yes. So what in self play, how does it arrive if you get stuck in, you could but you always try to max.",
    "start": "3554260",
    "end": "3560335"
  },
  {
    "text": "So it's a little bit like policy improvement. You're always trying to do a little bit better. You're still trying to win. Um, so it's",
    "start": "3560335",
    "end": "3565690"
  },
  {
    "text": "possible you could get stuck in a case where you're both just, you know, winning half the time, but then there should be something that you can exploit.",
    "start": "3565690",
    "end": "3571930"
  },
  {
    "text": "And if there's something you could exploit, if you do enough planning you should be able to identify it. Yeah.",
    "start": "3571930",
    "end": "3577440"
  },
  {
    "text": "Do you imagine that there would be this kind of transition point where you might get added benefit from transitioning to a more expert player to play against versus yourself?",
    "start": "3577440",
    "end": "3587825"
  },
  {
    "text": "You need to kind of start slowly and ease into it but then you might actually do learn faster by playing against somebody harder modes.",
    "start": "3587825",
    "end": "3594359"
  },
  {
    "text": "Yeah, question is, like, you know, would you also always wanna kinda just, like, self play against, you know, yourself five minutes ago or maybe at some point you- It will be",
    "start": "3594360",
    "end": "3600880"
  },
  {
    "text": "more efficient to go at someone harder. I think it's a great question. I think probably that's the case. Like, probably there will be cases where you could do bigger curriculum jumps,",
    "start": "3600880",
    "end": "3609400"
  },
  {
    "text": "um, and that might accelerate learning. But I think it's a tricky sweet spot there if like, you still need to have enough reward signal to bootstrap from.",
    "start": "3609400",
    "end": "3616420"
  },
  {
    "text": "Absolutely. All right. So, you know, the benefits of doing this is it",
    "start": "3616420",
    "end": "3621670"
  },
  {
    "text": "becomes this highly selective best-first search, because you're sort of constructing part of the tree but you're constructing it in a very specific way.",
    "start": "3621670",
    "end": "3628240"
  },
  {
    "text": "And the goal is that you should be way more sample efficient than doing Expectimax, making the whole tree but you're gonna be much better than",
    "start": "3628240",
    "end": "3635110"
  },
  {
    "text": "doing just a single step of policy improvement, with some fixed, um, you know, simulation-based policy.",
    "start": "3635110",
    "end": "3642040"
  },
  {
    "text": "And it's also, you know, parallelizable anytime, anytime in the sense that like, whether you have one minute or you have three hours to compute the next action.",
    "start": "3642040",
    "end": "3650785"
  },
  {
    "text": "And you know, three hours can be very realistic if it's something like, you know, a customer recommendation article or a thing that's gonna make or maybe you're",
    "start": "3650785",
    "end": "3658360"
  },
  {
    "text": "gonna make one decision per day and so you can run it overnight for eight hours and then compute that one decision. So, um, it allows you to,",
    "start": "3658360",
    "end": "3664570"
  },
  {
    "text": "to take advantage of the computation you have but then always provide an answer no matter how quickly you need to do that,",
    "start": "3664570",
    "end": "3670180"
  },
  {
    "text": "cause you just do less roll outs. Okay, um, I'm gonna skip this for now.",
    "start": "3670180",
    "end": "3675369"
  },
  {
    "text": "I just want to mention briefly, um, I think it was a question. It's a little weird that we do bandits in each of the nodes.",
    "start": "3675370",
    "end": "3681985"
  },
  {
    "text": "And intuitively, the reason it's a little bit weird is because in bandits, why do we do optimism under uncertainty?",
    "start": "3681985",
    "end": "3688720"
  },
  {
    "text": "We do it because we're actually incurring the pain of making bad decisions. And so the idea with optimism under uncertainty is that",
    "start": "3688720",
    "end": "3695815"
  },
  {
    "text": "either you really do get high reward or you learn something. The weird thing about doing that for planning is",
    "start": "3695815",
    "end": "3700840"
  },
  {
    "text": "that we're not suffering if we make bad decisions in our head. Um, essentially, we're just trying to figure out",
    "start": "3700840",
    "end": "3707005"
  },
  {
    "text": "what actions can I take as quickly as possible in terms of, like, value of information so that I know what the right action is to get the route.",
    "start": "3707005",
    "end": "3714369"
  },
  {
    "text": "And so it doesn't actually matter if I simulate out bad actions. If it allows me to get a better decision of the route.",
    "start": "3714370",
    "end": "3721089"
  },
  {
    "text": "We just give a really quick example of where that could be different. So if you have something like this.",
    "start": "3721090",
    "end": "3727730"
  },
  {
    "text": "Let's say this is the potential value of Q. Okay. So this is the value of A1 and this is the value of A2 and this is our uncertainty.",
    "start": "3729060",
    "end": "3739015"
  },
  {
    "text": "Well, if you're being optimistic, you're always gonna select this, because it's got a higher value. But if you wanna be really confident that A1 is better,",
    "start": "3739015",
    "end": "3747250"
  },
  {
    "text": "you should select A2 because likely when you do that, you're gonna update your confidence intervals and now you're",
    "start": "3747250",
    "end": "3752410"
  },
  {
    "text": "gonna be totally sure that A1 is best. But this approach won't do that.",
    "start": "3752410",
    "end": "3758440"
  },
  {
    "text": "Okay, because it's, it's, um, it's like, no I'm gonna suffer the cost in my head of taking the wrong action so I'm gonna take A1.",
    "start": "3758440",
    "end": "3764440"
  },
  {
    "text": "But if ultimately, you just need to know what the right action is to do, then sometimes in terms of computation you should take A2 because now,",
    "start": "3764440",
    "end": "3771369"
  },
  {
    "text": "your confidence intervals will likely separate. You don't need to do any more computation. Um, so it's not clear that the bandits,",
    "start": "3771370",
    "end": "3778165"
  },
  {
    "text": "um, at each node is the optimal thing to do but it's pretty efficient. All right. So that's basically all we're g- I'm gonna say about Go.",
    "start": "3778165",
    "end": "3786369"
  },
  {
    "text": "There's, um, some really beautiful papers, uh, about this including the new recent extensions. Um, and they have applications to chess as well as, you know,",
    "start": "3786370",
    "end": "3794275"
  },
  {
    "text": "a number of other games, uh, which I think are really, they're, they're amazing results. So I highly encourage you to look at some of those papers.",
    "start": "3794275",
    "end": "3800110"
  },
  {
    "text": "Let me just briefly talk about sort of popping back up to the end of the course, um, because it's the last lecture. All right.",
    "start": "3800110",
    "end": "3806335"
  },
  {
    "text": "So I just wanted to sort of refresh on, you know, what were the goals of the course, um, as we end and some of these you guys had a chance to practice on, on Monday.",
    "start": "3806335",
    "end": "3814180"
  },
  {
    "text": "Um, but I just wanted to say what I think of as kinda the key things that I hope you got out of it. So then one is sort of, what is the key features of RL versus everything else?",
    "start": "3814180",
    "end": "3822670"
  },
  {
    "text": "Um, both AI and supervised learning. And to me is that really this key, uh, issue of, um,",
    "start": "3822670",
    "end": "3828280"
  },
  {
    "text": "the agents being able to gather their own data and make decisions in the world. And so it's the censored data",
    "start": "3828280",
    "end": "3834430"
  },
  {
    "text": "that's very different than the IID assumption for supervised learning. And it's also very different from planning because you are",
    "start": "3834430",
    "end": "3840760"
  },
  {
    "text": "reliant on the data you get about the world in order to make decisions. Um, this is probably- The second thing is",
    "start": "3840760",
    "end": "3848140"
  },
  {
    "text": "probably the thing that for many of you might end up being the most useful, which is just how do you figure out given a problem, um,",
    "start": "3848140",
    "end": "3854170"
  },
  {
    "text": "whether you should even write it down as an RL problem and how to formulate it. And we had you practice this a little bit on, uh,",
    "start": "3854170",
    "end": "3859720"
  },
  {
    "text": "Monday and also it's a chance to think about this a lot in some of your projects. But I think this is often one of the really hard parts.",
    "start": "3859720",
    "end": "3865464"
  },
  {
    "text": "It has huge implications for how easy or hard it is to solve the problem. Um, and it's often unclear.",
    "start": "3865465",
    "end": "3870985"
  },
  {
    "text": "There's lots of ways to write down the state space of describing a patient or describing a student or describing a customer.",
    "start": "3870985",
    "end": "3876789"
  },
  {
    "text": "And in some ways it goes back to this issue of function approximation versus sample efficiency.",
    "start": "3876790",
    "end": "3882040"
  },
  {
    "text": "If I treat all customers as the same, I have a lot of data. That's probably a pretty bad model. So there's a lot of different trade-offs that come up",
    "start": "3882040",
    "end": "3888700"
  },
  {
    "text": "in these cases and I'm sure all of you guys will, um, think about interesting, exciting new ways to address that.",
    "start": "3888700",
    "end": "3894234"
  },
  {
    "text": "And then the other three things were, you know, to be very familiar with a number of common RL algorithms,",
    "start": "3894235",
    "end": "3899349"
  },
  {
    "text": "which you guys have implemented a lot. Um, to understand how we should even decide if an RL algorithm is good,",
    "start": "3899350",
    "end": "3905065"
  },
  {
    "text": "whether it's empirically, compu- you know, in terms of its computational complexity or things like how much data it takes or its performance guarantees,",
    "start": "3905065",
    "end": "3912805"
  },
  {
    "text": "um, and to understand this exploration exploitation challenge, which really is quite unique to RL.",
    "start": "3912805",
    "end": "3918370"
  },
  {
    "text": "It doesn't come up in planning, doesn't come up in ML. Um, and again, it's this critical issue of like, how do you",
    "start": "3918370",
    "end": "3923710"
  },
  {
    "text": "gather data quickly in order to make good decisions. Um, if you wanna learn more about reinforcement learning,",
    "start": "3923710",
    "end": "3930220"
  },
  {
    "text": "there's a bunch of other classes, uh, particularly, Mykel Kochenderfer has some really nice ones. And also Ben Van Roy does some nice ones",
    "start": "3930220",
    "end": "3936280"
  },
  {
    "text": "particularly looking at some of the more theoretical aspects of it. And then I do an advanced survey of it where we",
    "start": "3936280",
    "end": "3941410"
  },
  {
    "text": "do current topics and it's a project-based class. Um, and I'll just I guess I'll, um, two more things.",
    "start": "3941410",
    "end": "3947650"
  },
  {
    "text": "One is that I think, you know, we see some really amazing results, uh, Go is one example. Uh, and we're seeing-starting to see some really exciting results in",
    "start": "3947650",
    "end": "3954850"
  },
  {
    "text": "robotics but I think we're missing, most of us do not have RL on our phone yet in the way that we have face recognition on our phone.",
    "start": "3954850",
    "end": "3961690"
  },
  {
    "text": "And so I think that the potential of using these types of ideas for a lot of other types of applications is still enormous.",
    "start": "3961690",
    "end": "3967540"
  },
  {
    "text": "Um, and so if you go off and you do some of that I would love to hear back about it. Um, uh, in my lab we think a lot about these other forms of applications.",
    "start": "3967540",
    "end": "3974485"
  },
  {
    "text": "And I think another really critical aspect of this is thinking about when we do these RL agents, um,",
    "start": "3974485",
    "end": "3980125"
  },
  {
    "text": "how do we do it in sort of safe, fair and accountable ways because typically, these systems are going to be part of, you know, a human in the loop system.",
    "start": "3980125",
    "end": "3986845"
  },
  {
    "text": "And so allowing the agents to sort of expose their reasoning and expose,",
    "start": "3986845",
    "end": "3992020"
  },
  {
    "text": "um, their limitations will be critical. So the final thing is that, um, it's really helpful to get you guys this feedback.",
    "start": "3992020",
    "end": "3998695"
  },
  {
    "text": "Um, it allows us to improve the class for future years, um, either to make sure we continue to do things that you found",
    "start": "3998695",
    "end": "4004260"
  },
  {
    "text": "helpful or that we stopped doing things that you didn't find helpful. So I'd really appreciate it if we could take about 10 minutes now",
    "start": "4004260",
    "end": "4009375"
  },
  {
    "text": "to go through the course evaluations, um, and just feed it, uh, let us know what helped you learn,",
    "start": "4009375",
    "end": "4015105"
  },
  {
    "text": "what things we could do even better next year. Thanks. [APPLAUSE]",
    "start": "4015105",
    "end": "4032000"
  }
]