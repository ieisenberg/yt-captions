[
  {
    "start": "0",
    "end": "412000"
  },
  {
    "text": "Hello, so welcome to our\nlecture on the EM algorithm.",
    "start": "5299",
    "end": "10910"
  },
  {
    "text": "So just to figure out\nwhere we are in the flow, because we kind\nof have this flow of looking through a bunch of\nthese unsupervised algorithms.",
    "start": "10910",
    "end": "18539"
  },
  {
    "text": "We've kind of got our hands\ndirty with k-means and GMM. And these were our first\ntwo unsupervised algorithms.",
    "start": "18539",
    "end": "24189"
  },
  {
    "text": "And what we're\ngoing to try and do is kind of generalize\nwhat happened there so that we can use it in\nmany different settings",
    "start": "24189",
    "end": "30750"
  },
  {
    "text": "and move on from there. So last time we saw these\ntwo algorithms k-means GMM, if you don't remember,\nthe GMM algorithm",
    "start": "30750",
    "end": "37539"
  },
  {
    "text": "was the one that we had these\nphotons that we were trying to fit Gaussians to, maybe three\nGaussians that look like that.",
    "start": "37539",
    "end": "44120"
  },
  {
    "text": "Don't worry about if you\ndon't remember the details, just roughly what\nwe're dealing with. And the big idea\nwe encountered was",
    "start": "44120",
    "end": "49950"
  },
  {
    "text": "this idea of a latent variable. And the latent variable in\nthis setting, if you remember, was this fraction of points\nthat come from a source.",
    "start": "49950",
    "end": "57250"
  },
  {
    "text": "So we didn't know\nhow many points were coming from each one\nof those light sources that were out there. We had to estimate that.",
    "start": "57250",
    "end": "63100"
  },
  {
    "text": "Once we estimated\nthat, then we would be able to go back and fit all\nthe different parameters that",
    "start": "63100",
    "end": "68380"
  },
  {
    "text": "are there. Awesome. So-- and the fraction\nof points, we also had to figure out the\nlinkage, the probability",
    "start": "68380",
    "end": "75320"
  },
  {
    "text": "that every source was\ncoming from a point. And then we could\ndo the estimation. And the main thing that I wanted\nyou to take from both K-means",
    "start": "75320",
    "end": "81370"
  },
  {
    "text": "and GMM was this main\nidea that we kind of guess the latent variable. This is a great way to put it\nand how we have in the notes.",
    "start": "81370",
    "end": "88850"
  },
  {
    "text": "You guess where they're\nprobabilistically linked, that is, what's the\nprobability of these points",
    "start": "88850",
    "end": "93930"
  },
  {
    "text": "belong to cluster one, this\npoint belongs to cluster two, so on? And then once you\nhave that, you then solve some estimation\nproblem that",
    "start": "93930",
    "end": "100078"
  },
  {
    "text": "looks like a traditional\nsupervised learning. So the decomposition\nis quite important. And we're going to try and\nkind of abstract that away.",
    "start": "100079",
    "end": "107260"
  },
  {
    "text": "And then we would estimate\nthe other parameters. That's what I mean by a kind of\ntraditional supervised thing. Now, today what\nwe're going to do",
    "start": "107260",
    "end": "113718"
  },
  {
    "text": "is we're going to take a tour\nof EM in latent variable models and try and cast them\non a little bit more",
    "start": "113719",
    "end": "119350"
  },
  {
    "text": "principled footing because last\ntime, the calculations were, yeah, it kind of makes sense\nthat that's the average,",
    "start": "119350",
    "end": "125350"
  },
  {
    "text": "the weights in a cluster. We'll try to derive\nthe action that we're doing there from a more\nprincipled framework, which",
    "start": "125350",
    "end": "131380"
  },
  {
    "text": "is MLE. It doesn't mean that it's right. Maximal likelihood\nis just a framework. It happens to be, though,\nthe framework that we use",
    "start": "131380",
    "end": "137370"
  },
  {
    "text": "throughout most of the class. There are others in machine\nlearning by the way. But this is the one\nwe're going to use.",
    "start": "137370",
    "end": "142900"
  },
  {
    "text": "So before I get started on the\nrundown, any questions there? I'll start to write. Oh, please.",
    "start": "142900",
    "end": "148260"
  },
  {
    "text": "Yeah, for things we start\nby casting the views,",
    "start": "148260",
    "end": "154110"
  },
  {
    "text": "what is the first step in GMM? What do we guess?",
    "start": "154110",
    "end": "159440"
  },
  {
    "text": "We could guess randomly an\nassignment of every point to the cluster, the probability. Remember, there was this z(i)j.",
    "start": "159440",
    "end": "164670"
  },
  {
    "text": "If you don't recall,\nwe'll bring that up later. Don't worry about now. But we had to guess,\nfor every point x, i, which of the k\nclusters it belonged",
    "start": "164670",
    "end": "171409"
  },
  {
    "text": "to and with what probability. We could, for example,\ninitialize that to uniform. We don't know anything,\nand that's something.",
    "start": "171409",
    "end": "178370"
  },
  {
    "text": "We may have some\nother heuristic guess. That was what was going\non in the k-means++. We have a smarter\ninitialization.",
    "start": "178370",
    "end": "184020"
  },
  {
    "text": "But that's how we get\nthe process started. Once the process\nis started, we just keep running those two\nloops again and again,",
    "start": "184020",
    "end": "189540"
  },
  {
    "text": "and hopefully, it will improve. And we'll capture in what\nsentence it improves. You'll see this weird picture\nof a curve that we go up,",
    "start": "189540",
    "end": "195459"
  },
  {
    "text": "and that's going to\nbe the loss function. Awesome. OK, so we're going to look\nat the EM for latent variable",
    "start": "195459",
    "end": "202849"
  },
  {
    "text": "algorithms, and this\nis where it applies. This is what it's for is dealing\nwith various different notions of latent variables.",
    "start": "202849",
    "end": "207879"
  },
  {
    "text": "And I'll say this right now--\nmay be a little bit cryptic, and I'll come back to it either\nat the end of today's lecture and the next lecture--",
    "start": "207879",
    "end": "213890"
  },
  {
    "text": "when we pick these\nlatent variables, there's a little bit of an art. What they're doing,\nbasically, is they have that\ndecoupling property.",
    "start": "213890",
    "end": "219790"
  },
  {
    "text": "If we knew this thing that we\ncouldn't have observed, then, all of a sudden, it\nbecomes a really standard statistical estimation problem.",
    "start": "219790",
    "end": "226390"
  },
  {
    "text": "And somehow, we are\nassuming structure, and that's what we're putting\ninto the latent variable. So we're to see walking through\nthis today that structure--",
    "start": "226390",
    "end": "233000"
  },
  {
    "text": "we're assuming there's this\nprobabilistic map out there that says how often, how\nlikely every point is",
    "start": "233000",
    "end": "239860"
  },
  {
    "text": "to go to every cluster. In other cases, we'll see\nmore sophisticated variants of this idea, but it's\nactually fairly profound.",
    "start": "239860",
    "end": "247400"
  },
  {
    "text": "That's the real key idea. We can abstract all\nthe algorithmic details into EM same way we did for the\nexponential family stuff, OK?",
    "start": "247400",
    "end": "256199"
  },
  {
    "text": "Now, before we get\nstarted, I want to take a technical detour. And so it's really important\nthat we have signposting here",
    "start": "256200",
    "end": "261518"
  },
  {
    "text": "because you'll say,\nwhy is this guy drawing these weird pictures? The technical detail\nis I want to make sure that you understand this key\nresult, which is convexity",
    "start": "261519",
    "end": "270350"
  },
  {
    "text": "and Jensen's inequality. And the reason is-- I'll refer to this\nthing as we go through.",
    "start": "270350",
    "end": "276310"
  },
  {
    "text": "We're going to use it. It's not like I'm just teaching\nyou something for your health. Like, this is actually going\nto be used in the next step. It will actually, in some\nsense, be the entire algorithm.",
    "start": "276310",
    "end": "283480"
  },
  {
    "text": "Like, if you understand\nthis in the simplest way, then understanding the algorithm\nwill make a lot of sense. So it'll become a\nclear point where",
    "start": "283480",
    "end": "289520"
  },
  {
    "text": "we apply Jensen's inequality,\nwhere we make it tight. Those are the things that\nwe're going to think about as we go through it, OK?",
    "start": "289520",
    "end": "295290"
  },
  {
    "text": "So we're going to do this\ntechnical detail, right? I'm going to try and show\nit to you in pictures because I think it's\nthe most intuitive way",
    "start": "295290",
    "end": "300960"
  },
  {
    "text": "to understand the basic cases. If you already know\nit, don't worry. It's just another\nproof that you'll see.",
    "start": "300960",
    "end": "306319"
  },
  {
    "text": "Then this will allow us to go to\ndoing the EM algorithm as MLE. And what I mean is\nwe're going to be",
    "start": "306320",
    "end": "311870"
  },
  {
    "text": "able to write down a formal\nloss function, a likelihood function, right? That's what MLE is. We write down this\nloss function.",
    "start": "311870",
    "end": "318000"
  },
  {
    "text": "Then we maximize the likelihood. And we're going to show that\nthis actual algorithm is actually under the covers\nmaximizing a likelihood",
    "start": "318000",
    "end": "323750"
  },
  {
    "text": "function, all right? Then I'm going to\ncome back, and I'm going to put GMM\ninto this framework.",
    "start": "323750",
    "end": "329230"
  },
  {
    "text": "And this will answer\nsome of the questions that we kind of intuitively,\nkind of heuristically answered.",
    "start": "329230",
    "end": "334650"
  },
  {
    "text": "Like, why are we estimating\nthose parameters in such a way? And that will allow us to say,\nyep, GMM is an EM algorithm,",
    "start": "334650",
    "end": "340980"
  },
  {
    "text": "and so it would\ngive us a principle to solve for all the weights. If you remember, there was\nthose cluster centers, the mus",
    "start": "340980",
    "end": "346130"
  },
  {
    "text": "and the sigmas, the\nsource centers, mus and sigmas and fractions,\nand we were going to solve for all of those. And this gives us\na principle way",
    "start": "346130",
    "end": "351430"
  },
  {
    "text": "to do it because we're in\nthis MLE framework, OK? So we'll exercise it, basically\nexercise the notation.",
    "start": "351430",
    "end": "357290"
  },
  {
    "text": "And then we almost\ncertainly will not have time for this today,\nbut I combine the notes, and we'll go continue to go\nthrough them on Wednesday.",
    "start": "357290",
    "end": "364530"
  },
  {
    "text": "We'll go through what we\ncall factor analysis, OK? And factor analysis\nis another model.",
    "start": "364530",
    "end": "370240"
  },
  {
    "text": "The reason I want\nto show it to you is it's different than GMMs, so\nit occupies a different space,",
    "start": "370240",
    "end": "375650"
  },
  {
    "text": "and it will kind of force you\nto look at the kind of decisions you're making, right? What are you modeling here?",
    "start": "375650",
    "end": "380820"
  },
  {
    "text": "And in particular,\nwe'll model a situation where traditional\nGaussians couldn't fit the bill because we're\nmodeling something that's",
    "start": "380820",
    "end": "386660"
  },
  {
    "text": "huge and high dimensional. And we have to\nassume some structure to be able to get the\nwhole program to work.",
    "start": "386660",
    "end": "391919"
  },
  {
    "text": "And by comparing these two\nand what's similar to them, hopefully, you get a pretty\ngood sense of what EM is and all the different\nplaces that it runs.",
    "start": "391919",
    "end": "400030"
  },
  {
    "text": "All right, OK, so far, so good? So if there are no\nquestions, we're",
    "start": "400030",
    "end": "405160"
  },
  {
    "text": "going to go right into our\ntechnical detour, which will lead, then, into\nthe EM algorithm as MLE.",
    "start": "405160",
    "end": "410960"
  },
  {
    "text": "All right, so here's our detour.",
    "start": "410960",
    "end": "416900"
  },
  {
    "start": "412000",
    "end": "1381000"
  },
  {
    "text": "This is convexity and Jensen.",
    "start": "416900",
    "end": "422460"
  },
  {
    "text": "So this is a\nclassical inequality. And what I want to show you is\nthat Jensen's inequality really",
    "start": "422460",
    "end": "428350"
  },
  {
    "text": "is like convexity in\nanother guise, OK? And it's a key result,\nso I want to go slowly.",
    "start": "428350",
    "end": "433860"
  },
  {
    "text": "That's the only reason\nwe're doing this, OK? So don't think that there's\nsomething super mysterious going on. There isn't.",
    "start": "433860",
    "end": "439490"
  },
  {
    "text": "Hopefully, if I do\nmy job well, you'll just look at the pictures\nand be like, oh, yeah, that makes sense. Here's the line. Let's see what's going on.",
    "start": "439490",
    "end": "445000"
  },
  {
    "text": "OK, so first, we're going\nto define a set is convex. We did this last\ntime, but just recall",
    "start": "445000",
    "end": "451940"
  },
  {
    "text": "a set is convex if for any\na and b element of omega,",
    "start": "451940",
    "end": "466840"
  },
  {
    "text": "the line between\nthem is in omega.",
    "start": "466840",
    "end": "473520"
  },
  {
    "text": "So I'll write this in\nmath so it's precise. Oops, is in omega, OK?",
    "start": "473520",
    "end": "479190"
  },
  {
    "text": "So what does that mean? So let's draw the picture\nfirst, and we'll draw the math. Here's the convex set. So it means no matter\nhow I pick a-- here's a--",
    "start": "479190",
    "end": "485820"
  },
  {
    "text": "and no matter how I pick b,\nthe straight line between them,",
    "start": "485821",
    "end": "491220"
  },
  {
    "text": "the geodesic between them, the\nstraight line, is in a set. This is convex. OK?",
    "start": "491220",
    "end": "497490"
  },
  {
    "text": "In contrast, just to see it's\nnot a trivial definition,",
    "start": "497490",
    "end": "502960"
  },
  {
    "text": "this thing here-- which I drew\nvery crappily, but that's OK. I'll draw it like this\nbecause, actually, you'll see why the bottom makes\nmore sense to me in a second.",
    "start": "502960",
    "end": "511250"
  },
  {
    "text": "Here, we have one example. So if I picked a here\nand I picked b here, yep, the line is in the set, but\nthat doesn't prove it's convex.",
    "start": "511250",
    "end": "518959"
  },
  {
    "text": "It has to be convex for\nall of those choices. And here, if I put it\nfor b, lo and behold,",
    "start": "518959",
    "end": "525520"
  },
  {
    "text": "this would not be convex, OK? So let me write the math\nwhile I have the picture.",
    "start": "525520",
    "end": "530830"
  },
  {
    "text": "So this is in symbols. For all lambda,\nelement of 0, 1-- so this is how I parameterize\ngoing on the line--",
    "start": "530830",
    "end": "536700"
  },
  {
    "text": "a, b element of omega lambda\na plus 1 minus lambda-- oops,",
    "start": "536700",
    "end": "545730"
  },
  {
    "text": "accidental stroke-- lambda b is an\nelement of omega, OK?",
    "start": "545730",
    "end": "552470"
  },
  {
    "text": "Clear enough what that means? This is just the line\nbetween a and b, right? I'm just saying that no matter\nhow I pick a and b and lambda,",
    "start": "552470",
    "end": "561300"
  },
  {
    "text": "this thing still remains\nin the set, which is just capturing this picture. Cool?",
    "start": "561300",
    "end": "566580"
  },
  {
    "text": "All right, now, we're going\nto apply this to functions.",
    "start": "566580",
    "end": "573300"
  },
  {
    "text": "So given a function--",
    "start": "573300",
    "end": "580019"
  },
  {
    "text": "for right now, we make\nit one dimensional, Gf--",
    "start": "580020",
    "end": "586240"
  },
  {
    "text": "we're going to find the graph of\nthat function as a set of x, y such that y is greater\nthan f of x, OK?",
    "start": "586240",
    "end": "598670"
  },
  {
    "text": "This is my definition. So a function is going to be\nconvex ",
    "start": "598670",
    "end": "607140"
  },
  {
    "text": "if its graph is, OK? As I said.",
    "start": "607140",
    "end": "616589"
  },
  {
    "text": "OK, so let's draw\nan example of this. So here's my function.",
    "start": "616589",
    "end": "621700"
  },
  {
    "text": "Here's 0. Here's minus 1. Here's 1, and I draw\nthis character, OK?",
    "start": "621700",
    "end": "628850"
  },
  {
    "text": "So this, OK? So the shaded region here--",
    "start": "628850",
    "end": "635200"
  },
  {
    "text": "so this function, by the way,\nthe one that's in my head, is going to be f is\nequal to x squared. So if you're trying to correct\nfor my artistic shortcomings,",
    "start": "635200",
    "end": "643519"
  },
  {
    "text": "this is f equals x squared. It's a parabola, kind of a\nbowl-shaped function, right?",
    "start": "643519",
    "end": "648550"
  },
  {
    "text": "Now, no matter how I pick\nthe points-- and clearly, I should really\nonly have to worry about picking on the edge. So if I pick a point here, a,\nand I pick a point, f of a,",
    "start": "648550",
    "end": "656389"
  },
  {
    "text": "pick a point b and pick a point,\nf of b, the line between them",
    "start": "656390",
    "end": "662240"
  },
  {
    "text": "goes here, all right? It's not necessarily a\nstraight line across.",
    "start": "662240",
    "end": "668120"
  },
  {
    "text": "I just happened to pick it that\nway-- go up it, go down, do whatever it wants, OK?",
    "start": "668120",
    "end": "673329"
  },
  {
    "text": "Now this function here,\nwe'll imagine-- we'll talk about a point z later. So I'll just say\nthere's a point z that's",
    "start": "673329",
    "end": "679150"
  },
  {
    "text": "going to live in the middle,\nand this is b lives here. Let me erase 0 and 1 because\nwe don't really need them.",
    "start": "679150",
    "end": "684829"
  },
  {
    "text": "Their values are kind\nof unimportant to us. It was just so you knew\nwhat I was drawing.",
    "start": "684830",
    "end": "689959"
  },
  {
    "text": "We'll draw a. All right, awesome. All right, now, what\nis this function? Well, this is a--",
    "start": "689959",
    "end": "695770"
  },
  {
    "text": "I think it's x minus looks like this. And then its graph is\neverything up here.",
    "start": "695770",
    "end": "707279"
  },
  {
    "text": "And this is not convex\nfor the same reason. I could pick a point here-- I pick a here. I pick b here, and the line\nbetween them is below the set.",
    "start": "707279",
    "end": "715890"
  },
  {
    "text": "So this is a convex function. This is not convex.",
    "start": "715890",
    "end": "721790"
  },
  {
    "text": "OK, so let's look\nat this in symbols. So clear enough? Hopefully like trivia.",
    "start": "721790",
    "end": "727019"
  },
  {
    "text": "Like, oh, you just drew two\npictures twice, and one, you said was a function graph,\nand the other one, you didn't. The function graph\nwas open to the top,",
    "start": "727019",
    "end": "732740"
  },
  {
    "text": "but that shouldn't\nbe really disturbing. So far, so good. All right, so what\ndoes this mean?",
    "start": "732740",
    "end": "738389"
  },
  {
    "text": "Means for all lambda\nelement of 0, 1, lambda a, f of a plus 1 minus\nlambda-- these are as tuples--",
    "start": "738389",
    "end": "748060"
  },
  {
    "text": "b, f of b is an\nelement of omega.",
    "start": "748060",
    "end": "754000"
  },
  {
    "text": "What does it mean to\nbe an element of omega? It means that if I take any z\nthat's on the path, lambda a,",
    "start": "754000",
    "end": "760839"
  },
  {
    "text": "to 1 minus lambda b-- so any character that comes\nin between here and here,",
    "start": "760839",
    "end": "767889"
  },
  {
    "text": "then it had better be the case\nthat lambda f of a plus 1 minus",
    "start": "767889",
    "end": "773310"
  },
  {
    "text": "lambda f of b is greater\nthan f of z, all right?",
    "start": "773310",
    "end": "779080"
  },
  {
    "text": "So this is now z, f of z. This is z. This is f of z.",
    "start": "779080",
    "end": "784740"
  },
  {
    "text": "Does that make sense? Just translating the\ndefinitions directly.",
    "start": "784740",
    "end": "791330"
  },
  {
    "text": "In more cryptic language,\nwe usually just tell you every chord is\nbelow the function--",
    "start": "791330",
    "end": "797279"
  },
  {
    "text": "or, sorry, is\nabove the function. Sorry, I'm drawing\nthe wrong way--",
    "start": "797279",
    "end": "804070"
  },
  {
    "text": "above the function. What does that mean? Well, a chord is just anything\nthat connects two points here.",
    "start": "804070",
    "end": "812220"
  },
  {
    "text": "So this character\nwould be another chord. It lies entirely above the\ngraph of the function, where the function actually lives.",
    "start": "812220",
    "end": "818660"
  },
  {
    "text": "Here, that's not case. I just found two points so\nthat the chord between them",
    "start": "818660",
    "end": "824280"
  },
  {
    "text": "is actually below the function. So it's not convex. And intuitively,\nthe reason I drew",
    "start": "824280",
    "end": "829670"
  },
  {
    "text": "these shapes is that\nconvexity for shapes probably makes more\nintuitive sense, 2D shapes.",
    "start": "829670",
    "end": "834680"
  },
  {
    "text": "But now, hopefully, you see\nthey're really the same thing. So your geometric intuition\nand the function intuition are the same, modulo that we\nchange this definition here.",
    "start": "834680",
    "end": "843600"
  },
  {
    "text": "OK? All right, now, let me see.",
    "start": "843600",
    "end": "853910"
  },
  {
    "text": "All right, so one\nother bit here.",
    "start": "853910",
    "end": "860190"
  },
  {
    "text": "So we're going to-- we'll\nactually prove this, I think, why not? Sounds like a fun\nthing to prove. If f is twice differentiable\nand, for all x,",
    "start": "860190",
    "end": "874339"
  },
  {
    "text": "f double prime of x is greater\nthan 0, then f is convex, OK?",
    "start": "874340",
    "end": "881600"
  },
  {
    "text": "So this says these functions\nreally are bowl-shaped, right? Second derivative\nbeing positive means",
    "start": "881600",
    "end": "887399"
  },
  {
    "text": "that they have this kind\nof positive curvature that looks like the U's, right?",
    "start": "887400",
    "end": "892440"
  },
  {
    "text": "Their first dimension-- first\nderivative goes up and down, but they're kind\nof always trending. That first derivative is always\ngetting more positive, right?",
    "start": "892440",
    "end": "898130"
  },
  {
    "text": "It's negative on\nthe left-hand side, positive on the right-hand side. That's what it means\nby bowl-shaped. OK, so this isn't\nsuper hard to prove,",
    "start": "898130",
    "end": "908510"
  },
  {
    "text": "but just because I think it will\nstall for a little bit, in case you want to ask me questions.",
    "start": "908510",
    "end": "914180"
  },
  {
    "text": "I'm writing out a\nTaylor series for this. f double prime-- see\nthe a, a minus z square.",
    "start": "914180",
    "end": "922610"
  },
  {
    "text": "OK. Oop, plus. Let me drag that guy\nso it's a little clear.",
    "start": "922610",
    "end": "934769"
  },
  {
    "text": "OK? And this a to a is\njust something in a, b. So maybe you remember this\nfrom your Taylor series.",
    "start": "934769",
    "end": "941529"
  },
  {
    "text": "All I'm saying is I can write\nf of a at some point f of z plus some first\nderivative information",
    "start": "941529",
    "end": "947000"
  },
  {
    "text": "plus some second\nderivative information. And then I'm using the\nsecond derivative remainder. So I'm saying there's some\npoint on the interval where",
    "start": "947000",
    "end": "953209"
  },
  {
    "text": "this is true, OK? Same thing for b.",
    "start": "953209",
    "end": "960399"
  },
  {
    "text": "This isn't super important for\nyour conceptual understanding, by the way. Like, this is just\nto show that you",
    "start": "960399",
    "end": "970740"
  },
  {
    "text": "can do what you want to do here,\nthat this makes sense to you. OK, minus z squared,\nhandled it, OK?",
    "start": "970740",
    "end": "979490"
  },
  {
    "text": "Now, I claim it's convex. So I just take what is\nthe obvious thing to do. I'm going to multiply\nthis by lambda.",
    "start": "979490",
    "end": "984879"
  },
  {
    "text": "I'm going to multiply this by I have to make a statement\nabout this, right?",
    "start": "984880",
    "end": "991560"
  },
  {
    "text": "That's what's in my\ndefinition above, OK? Well, that's just the same\nas adding f of z for lambda,",
    "start": "991560",
    "end": "998310"
  },
  {
    "text": "plus 1 minus lambda--\nthat's just f of z. So that's good. That appears. Notice here that I get 1\nplus a times lambda plus 1",
    "start": "998310",
    "end": "1008209"
  },
  {
    "text": "minus the lambda times b-- well,\nthat's just equal to z exactly, right? So I get 0 here plus 0, right?",
    "start": "1008209",
    "end": "1016440"
  },
  {
    "text": "This is just because lambda a\nplus 1 minus lambda b equals z. Plus-- these things\nare all positive.",
    "start": "1016440",
    "end": "1027490"
  },
  {
    "text": "Plus some constant\nthat's greater than 0. So that shows that this thing,\nthis inequality, holds, f of z.",
    "start": "1027490",
    "end": "1038168"
  },
  {
    "text": "Please. Oh, so you've seen the\ndouble [INAUDIBLE],,",
    "start": "1038169",
    "end": "1044529"
  },
  {
    "text": "is that [INAUDIBLE]? Yeah, this is Taylor's theorem. Great question. So what's going on here if you\nremember Taylor's theorem is you can keep\nexpanding, and then you",
    "start": "1044530",
    "end": "1050730"
  },
  {
    "text": "have the last term, which\nis the remainder term. And the remainder\nterm says, there exists some point that\nlives in a to b such",
    "start": "1050730",
    "end": "1057140"
  },
  {
    "text": "that this holds with equality. And I'm just using the remainder\nform of Taylor's theorem. By the way, this is\nreally not important for",
    "start": "1057140",
    "end": "1063211"
  },
  {
    "text": "your conceptual understanding. I just want to show that it's\none line, that this statement, which is sometimes\nmysterious about derivatives and causes people's\nheads to explode--",
    "start": "1063211",
    "end": "1069649"
  },
  {
    "text": "like, why are the derivatives\nconnected to the convexity? It's because of this. This is all that's going on.",
    "start": "1069650",
    "end": "1075320"
  },
  {
    "text": "Awesome. You can freely forget this and\njust use the fact, this fact,",
    "start": "1075320",
    "end": "1080380"
  },
  {
    "text": "in the course, OK? OK, stalling done.",
    "start": "1080380",
    "end": "1086549"
  },
  {
    "text": "Any more questions? Awesome. The real reason we want to go\nthrough the derivative thing",
    "start": "1086549",
    "end": "1092980"
  },
  {
    "text": "is, otherwise, this thing,\nwhich we actually do care about, is strongly convex.",
    "start": "1092980",
    "end": "1101970"
  },
  {
    "text": "This definition\nfeels like it comes from space aliens otherwise,\nif for on a domain,",
    "start": "1101970",
    "end": "1108350"
  },
  {
    "text": "is if f prime of x\nis greater than 0, strictly greater than 0. This is strict\nequality here, OK?",
    "start": "1108350",
    "end": "1116620"
  },
  {
    "text": "That's where the strongly\nconvex comes from, whereas this is actually strictly convex. Doesn't really matter, but OK.",
    "start": "1116620",
    "end": "1121820"
  },
  {
    "text": "So for example, f of x\nequals x squared, which I told you was in my head. Well, this gives me\na simple test, right?",
    "start": "1121820",
    "end": "1128090"
  },
  {
    "text": "Its second derivative is 2. That's greater than 0. It is the prototypical\nstrongly convex function, OK?",
    "start": "1128090",
    "end": "1135510"
  },
  {
    "text": "You also saw those This is to make this\nparameter sometimes with the curvature one. Doesn't really matter, OK?",
    "start": "1135510",
    "end": "1142690"
  },
  {
    "text": "The other function\nthat I had, which you can check and\ngraph yourself, is x squared x minus 1 squared.",
    "start": "1142690",
    "end": "1148320"
  },
  {
    "text": "This is not convex. Compute the derivative. You'll see. But it's the one that looks\nlike the two bumps, right? It's a quartix.",
    "start": "1148320",
    "end": "1154059"
  },
  {
    "text": "So that's what it looks\nlike, has two bumps, positive discriminant, OK?",
    "start": "1154059",
    "end": "1159890"
  },
  {
    "text": "Awesome. So at this point, what\ndo I care that you know? Not too much,\nhonestly, about this.",
    "start": "1159890",
    "end": "1165040"
  },
  {
    "text": "What I care that you\nknow is that there's some way that you are\nfamiliar with geometrically what convexity means.",
    "start": "1165040",
    "end": "1171700"
  },
  {
    "text": "And you know that\nthere are these tests in terms of the derivative. The second derivative\nbeing nonnegative is a good test for convexity.",
    "start": "1171700",
    "end": "1177040"
  },
  {
    "text": "And if you have a\nstronger condition, you can get this strong\nor strict convexity, OK?",
    "start": "1177040",
    "end": "1182240"
  },
  {
    "text": "All good. All right, now, what\nwe actually need--",
    "start": "1182240",
    "end": "1187908"
  },
  {
    "text": "Jensen's inequality. Now, if I've done my job\nwell, this mysterious-looking statement, once I show you the\nconnection, you go, oh, OK,",
    "start": "1187909",
    "end": "1196370"
  },
  {
    "text": "that makes sense. It's because it's\nactually just saying something about convexity,\nbut it's got a fancy name,",
    "start": "1196370",
    "end": "1202450"
  },
  {
    "text": "and it's so useful, and it's\nthe following statement-- the expected value\nof f of x is greater",
    "start": "1202450",
    "end": "1209970"
  },
  {
    "text": "than f of the expected value of\nx so long as f is convex, OK?",
    "start": "1209970",
    "end": "1222898"
  },
  {
    "text": "Why the heck would this happen? Let's take one example.",
    "start": "1222899",
    "end": "1230090"
  },
  {
    "text": "Suppose x takes value\na ",
    "start": "1230090",
    "end": "1236640"
  },
  {
    "text": "with prob of lambda. Then x takes value b\nwith prob 1 minus lambda.",
    "start": "1236640",
    "end": "1250370"
  },
  {
    "text": "Then what is it saying? It's saying the\nexpected value of f of x is equal to lambda times\nf of a plus 1 minus lambda",
    "start": "1250370",
    "end": "1262620"
  },
  {
    "text": "f of b. What is f of the\nexpected value of x?",
    "start": "1262620",
    "end": "1270760"
  },
  {
    "text": "Well, it's f of lambda\na plus 1 minus lambda b.",
    "start": "1270760",
    "end": "1281380"
  },
  {
    "text": "That's exactly the definition\nof convex, the inequality. This is convexity, OK?",
    "start": "1281380",
    "end": "1290039"
  },
  {
    "text": "Now, the other thing I\nwant to say for this is,",
    "start": "1290039",
    "end": "1295370"
  },
  {
    "text": "notice that this does not\nmatter how I pick lambda. Later, I'm going to\ndefine a curve, one way",
    "start": "1295370",
    "end": "1301110"
  },
  {
    "text": "to define a curve. And that curve is\ngoing to be as a result of sweeping some parameters in\na high-dimensional weird space.",
    "start": "1301110",
    "end": "1306230"
  },
  {
    "text": "But basically, it\nsays, no matter how I pick the\nparameters of that curve, anywhere that lives on this\nthing, that's a probability distribution, a\nbunch of numbers that",
    "start": "1306230",
    "end": "1312389"
  },
  {
    "text": "sum to 1 in the discrete case. This inequality holds. And that's going to\nallow me to build a lower",
    "start": "1312390",
    "end": "1317950"
  },
  {
    "text": "bound for my function, and I'm\ngoing to hillclimb using it. We'll see that in just a minute. That will become clear. Are there any questions\nabout this piece here?",
    "start": "1317950",
    "end": "1326740"
  },
  {
    "text": "All right.",
    "start": "1326740",
    "end": "1332090"
  },
  {
    "text": "Now, you may look\nand say, OK, well, this is only in the case when\nthere are two probabilities.",
    "start": "1332090",
    "end": "1338990"
  },
  {
    "text": "What happens when\nthere are more? You can just repeat\nby induction. You have to do\nsomething fancier if you want something that's a full\nprobability distribution.",
    "start": "1338990",
    "end": "1344820"
  },
  {
    "text": "This holds even if E is a\ncontinuous distribution. I won't show you that\nbecause we're not going to go too far off.",
    "start": "1344820",
    "end": "1350380"
  },
  {
    "text": "We'll stop at kind of\nhigh school calculus. Sound good? All right.",
    "start": "1350380",
    "end": "1356270"
  },
  {
    "text": "All right, so now you\nknow Jensen's theorem, and hopefully, you'll always get\nthe inequality the right way.",
    "start": "1356270",
    "end": "1362970"
  },
  {
    "text": "And the reason you'll always get\nthe inequality the right way is you'll draw the picture of\nthe function and see the chord",
    "start": "1362970",
    "end": "1369190"
  },
  {
    "text": "is always above it. Which one must be z? Which one must be\nf of z? f of z must be below the chord\nof the function,",
    "start": "1369190",
    "end": "1376240"
  },
  {
    "text": "and that's exactly this. Cool. All right.",
    "start": "1376240",
    "end": "1381419"
  },
  {
    "start": "1381000",
    "end": "1481000"
  },
  {
    "text": "Now, everything is\ndefined in the literature traditionally for convex. If you take convex analysis,\nit's the way we define things.",
    "start": "1381419",
    "end": "1387419"
  },
  {
    "text": "We actually don't want to\nuse a convex function here because we're\nmaximizing likelihood. And this is just\nnotational pain, right?",
    "start": "1387419",
    "end": "1393880"
  },
  {
    "text": "Like, if we were-- maybe we should have\nminimized unlikelihood. I don't know what we should have\ndone, but this is where we are.",
    "start": "1393880",
    "end": "1400590"
  },
  {
    "text": "So we need concave functions. And what are concave functions? g is concave if and only\nf minus g is convex.",
    "start": "1400590",
    "end": "1412490"
  },
  {
    "text": "All right? So we flip it upside down. OK?",
    "start": "1412490",
    "end": "1418370"
  },
  {
    "text": "The prototypical one that we'll\nuse if g of x, for example, is equal to log of x--",
    "start": "1418370",
    "end": "1425399"
  },
  {
    "text": "here's my picture of log of\nx, probably not very good-- is that's going to look\nsomething like this,",
    "start": "1425400",
    "end": "1432049"
  },
  {
    "text": "go off this way. And notice that if I take\na chord of this function--",
    "start": "1432049",
    "end": "1437150"
  },
  {
    "text": "that's a chord-- it's below-- chord is below, right? Which is what I\nshould hope, right?",
    "start": "1437150",
    "end": "1442559"
  },
  {
    "text": "If I flipped it upside down,\nthe chord would be above. Cool. Now, also, there\nare functions that",
    "start": "1442559",
    "end": "1451140"
  },
  {
    "text": "are concave and convex, right? So what if h of x is\nequal to a times x plus b?",
    "start": "1451140",
    "end": "1456549"
  },
  {
    "text": "It's a line. Chords are both no\nlonger above and below.",
    "start": "1456549",
    "end": "1462490"
  },
  {
    "text": "It's actually\nconcave and convex. Linear functions are\nconcave and convex. OK, that ends the detour.",
    "start": "1462490",
    "end": "1469710"
  },
  {
    "text": "Let's get back to\nmachine learning. So now we have the tools.",
    "start": "1469710",
    "end": "1475460"
  },
  {
    "text": "Just to make sure, what do\nI care that you got there? You got this way\nthat as long as we were dealing with probability\ndistributions, no matter which",
    "start": "1475460",
    "end": "1480740"
  },
  {
    "text": "probability distribution we\ntook, we have this inequality. We can get lower bounds.",
    "start": "1480740",
    "end": "1486140"
  },
  {
    "start": "1481000",
    "end": "1778000"
  },
  {
    "text": "We're going to use\nthat in a second to draw some curves of\na likelihood function that will hopefully\nbe easier to optimize",
    "start": "1486140",
    "end": "1491559"
  },
  {
    "text": "than the original function. And we'll try an\niterative algorithm that will look exactly like\nwe talked about before.",
    "start": "1491559",
    "end": "1496570"
  },
  {
    "text": "And then we will\nconceptualize it is we solve for some hidden parameter. We solve, and that gives\nus an entire family",
    "start": "1496570",
    "end": "1503559"
  },
  {
    "text": "of possible solutions. We solve on that,\nand we iterate. Let me draw the picture after\nI give you the formal set, OK?",
    "start": "1503559",
    "end": "1510950"
  },
  {
    "text": "Oops. All right, so EM algorithm\nhas max likelihood--",
    "start": "1510950",
    "end": "1519340"
  },
  {
    "text": "I'll actually put MLE. All right, so remember, this is\nthe max likelihood formulation.",
    "start": "1519340",
    "end": "1530480"
  },
  {
    "text": "There's some theta\nthat lives out there. We have some data,\ni from 1 to n. These are our data points.",
    "start": "1530480",
    "end": "1536650"
  },
  {
    "text": "We take a log of the\nprobability that we assign to the data\ngiven our parameters.",
    "start": "1536650",
    "end": "1543750"
  },
  {
    "text": "So this is a way for us to\ncompare different parameters. And recall, these\nare the parameters,",
    "start": "1543750",
    "end": "1553020"
  },
  {
    "text": "params, and this is the data.",
    "start": "1553020",
    "end": "1562730"
  },
  {
    "text": "So far, so good? All right, now, we're working\nwith latent variable models.",
    "start": "1562730",
    "end": "1568340"
  },
  {
    "text": "So latent variable models\nmean that P has a little bit of extra structure. P(x; theta)-- this is\na generic term, right?",
    "start": "1568340",
    "end": "1574900"
  },
  {
    "text": "This is just one\nof the i terms-- says the function\nfactors this way.",
    "start": "1574900",
    "end": "1580059"
  },
  {
    "text": "Looks like a sum over z, where\nz is our hidden or latent variable, P(x, z; theta).",
    "start": "1580059",
    "end": "1588440"
  },
  {
    "text": "This is a latent\nvariable, right? So remember, z was our GMM\nlatent variable, the cluster",
    "start": "1588440",
    "end": "1595929"
  },
  {
    "text": "probability, right? So we have to sum or marginalize\nover all the possible choices of z. This is basically saying,\nI don't know what z is.",
    "start": "1595929",
    "end": "1602610"
  },
  {
    "text": "I have some probability\ndistribution that I can compute over\nmy data and z given theta.",
    "start": "1602610",
    "end": "1609640"
  },
  {
    "text": "And since I don't know\nwhat z is, the term is-- I marginalize it\nout-- means I sum over all the possible values.",
    "start": "1609640",
    "end": "1614799"
  },
  {
    "text": "And this will get me back\na probability for x, right? This is a sum over\nall possible z's. This will leave me\nwith a probability",
    "start": "1614799",
    "end": "1620630"
  },
  {
    "text": "for x, or sorry,\nprobability for x. Is that clear? Yeah, ask a question, please. So where is the z\ngoing to go again?",
    "start": "1620630",
    "end": "1626100"
  },
  {
    "text": "Like, is that property of\nthe parameters [INAUDIBLE]??",
    "start": "1626100",
    "end": "1632260"
  },
  {
    "text": "Yeah, wonderful question. It's a property of the model. In a real sense, when we make a\nmodeling decision, and we say,",
    "start": "1632260",
    "end": "1637840"
  },
  {
    "text": "there exists some\nstructure out there. Like, there exists a\nprobabilistic assignment between photons\nand point sources.",
    "start": "1637840",
    "end": "1645270"
  },
  {
    "text": "One version of the prior\nis, I tell you exactly where every photon comes from. That's clearly a\nvery strong prior.",
    "start": "1645270",
    "end": "1650590"
  },
  {
    "text": "If you knew that, Godspeed. Go do it. You just solved GDA. If instead what you\nknow is there exists",
    "start": "1650590",
    "end": "1657279"
  },
  {
    "text": "some mapping that's out there,\nthen that structure that you're putting into your function--\nand what I'm saying is that mathematically comes\ndown to baking exactly this in,",
    "start": "1657279",
    "end": "1665000"
  },
  {
    "text": "OK? And this is the\nmathematical form for all of those\nlatent variable models. So when we have that idea\nabout latent structure,",
    "start": "1665000",
    "end": "1670830"
  },
  {
    "text": "we'll eventually put it\ninto this mathematical form. And we'll see a\ncouple more examples. Wonderful question.",
    "start": "1670830",
    "end": "1676330"
  },
  {
    "text": "In GMM, this was\nexactly the z there. The notation isn't an accident. It's the same z. [INAUDIBLE]-- Go ahead.",
    "start": "1676330",
    "end": "1682250"
  },
  {
    "text": "--example of this? Yeah, so the example that we had\nwas basically the whole lecture where z was the\nprobabilistic linking",
    "start": "1682250",
    "end": "1688440"
  },
  {
    "text": "between sources and photons. Yeah, yeah. So that's one. We'll have more examples later.",
    "start": "1688440",
    "end": "1694399"
  },
  {
    "text": "But I want to get\nthrough the algorithm in this abstract form, and\nwe can shoehorn more things into it. And what I'll do afterwards\nis put GMM right down",
    "start": "1694399",
    "end": "1700870"
  },
  {
    "text": "in this language. We need a couple more things. Please. Yeah, correct me if I'm not\nunderstanding this correctly, but-- so z is the probability\nthat a point is coming",
    "start": "1700870",
    "end": "1708549"
  },
  {
    "text": "from a particular cluster. What is it? Like, what is probability\nof x parameterized by theta",
    "start": "1708549",
    "end": "1713799"
  },
  {
    "text": "actually represent in this\ncase, in that photon example? Yeah, exactly. So remember, if you--",
    "start": "1713800",
    "end": "1719590"
  },
  {
    "text": "I think it was said\nyesterday by someone here on that side of the room. So I don't know if that's\nspatial recognition helps you in the last lecture.",
    "start": "1719590",
    "end": "1725559"
  },
  {
    "text": "But it was like, imagine I\nwas guessing all the photon models that were out there. So each one was parameterized\nby some choice of z(i).",
    "start": "1725559",
    "end": "1732440"
  },
  {
    "text": "And then what I'm\nthinking about is what I want over that is\nthat, across all those thetas, no matter how I instantiate\nz, each one gives me",
    "start": "1732440",
    "end": "1738870"
  },
  {
    "text": "a different probability\ndistribution. I can sum them up,\nand that tells me, given this theta, no matter how\nz is assigned or marginalized",
    "start": "1738870",
    "end": "1746559"
  },
  {
    "text": "across all ways\nthat it's assigned, how likely is the data? So P(x; theta), we've\nbeen using forever. We used that from\nthe supervised days.",
    "start": "1746559",
    "end": "1753169"
  },
  {
    "text": "We just inserted z and\nsaid, well, there's this wild z that\nwe can't observe, but it somehow constrains x.",
    "start": "1753169",
    "end": "1758908"
  },
  {
    "text": "It means that x--\nlike, the relationship between theta and x. And that's what the model does.",
    "start": "1758909",
    "end": "1764220"
  },
  {
    "text": "Awesome question. Very cool. These are wonderful questions.",
    "start": "1764220",
    "end": "1769490"
  },
  {
    "text": "I'd much rather answer\nthese than badly draw the pictures that come next. We're going to get\nto those pictures no matter what, so there's\nreally no saving us.",
    "start": "1769490",
    "end": "1776150"
  },
  {
    "text": "All right, let's get\nto the bad pictures. All right, so I'll try and\nleave this more or less on the screen.",
    "start": "1776150",
    "end": "1782539"
  },
  {
    "start": "1778000",
    "end": "2069000"
  },
  {
    "text": "Here's the algorithm. This is a picture,\nwhich maybe won't make perfect sense to start\nwith, but we'll get there.",
    "start": "1782539",
    "end": "1789820"
  },
  {
    "text": "I'll go. All right, so remember what\na loss function looks like. I'm drawing everything in",
    "start": "1789820",
    "end": "1795158"
  },
  {
    "text": "is in horrible high dimensions. My axis is theta.",
    "start": "1795159",
    "end": "1800289"
  },
  {
    "text": "And then what I have-- and I apologize. I will use a bunch of colors. I hope this is OK\nfor people to see.",
    "start": "1800290",
    "end": "1806640"
  },
  {
    "text": "If not, let me know. Doesn't look like the\nmost visible color but doesn't look like the least\nvisible, and I need a couple.",
    "start": "1806640",
    "end": "1813980"
  },
  {
    "text": "This is my loss\nfunction, l(theta), OK? So this is-- I'll write\nthat in black there.",
    "start": "1813980",
    "end": "1819260"
  },
  {
    "text": "This is l(theta), OK? So this is my loss curve, OK? This is l(theta) here.",
    "start": "1819260",
    "end": "1825130"
  },
  {
    "text": "Now, remember, it's not a nice\nconcave or convex function, right? We wouldn't expect it to be. We would hope, because\nwe're going to minimize it,",
    "start": "1825130",
    "end": "1831179"
  },
  {
    "text": "that it's concave. That would be nice. If it just looked like this-- like, oh, that'd be so great. We would just climb to the top. But we saw in a\nlot of the problems",
    "start": "1831180",
    "end": "1837360"
  },
  {
    "text": "that we were after, it\ndoesn't look like that. It has these kind\nof weird bends. So we had to settle-- that's\nanother way of copping",
    "start": "1837360",
    "end": "1842370"
  },
  {
    "text": "out and saying, we had to\nsettle for these local iterative solutions. That's all we're after. We settled for that\nin KMM, for k means,",
    "start": "1842370",
    "end": "1848740"
  },
  {
    "text": "and we're going to settle\nfor that in GM, OK? So how does the algorithm work?",
    "start": "1848740",
    "end": "1854500"
  },
  {
    "text": "We start with an initial guess. Now, again, you could ask-- these colors seem\nharder to read.",
    "start": "1854500",
    "end": "1859880"
  },
  {
    "text": "You start with an initial guess. So theta, let's say, at time t--\nso it could be time 0, right?",
    "start": "1859880",
    "end": "1865290"
  },
  {
    "text": "This is just the initial\nguess, whatever it is. Then what happens\nis this is mapped up to here, which is l of theta t.",
    "start": "1865290",
    "end": "1872789"
  },
  {
    "text": "I haven't written anything. I'm just giving notation. This is just the value of the\nloss that I currently have. I suspect there's something\nup this way I'd like to get.",
    "start": "1872789",
    "end": "1881940"
  },
  {
    "text": "So how do I do it? What's the algorithmic piece? What I'm going to do\nis I'm going to form--",
    "start": "1881940",
    "end": "1886960"
  },
  {
    "text": "so the problem is optimizing\nover all those z's seems daunting, directly\noptimizing the l's.",
    "start": "1886960",
    "end": "1892350"
  },
  {
    "text": "So instead, what\nI'm going to do is I'm going to come up\nwith a local curve, OK,",
    "start": "1892350",
    "end": "1899509"
  },
  {
    "text": "and I'm going to call\nthis curve Lt of theta. It's another function. I'm only drawing a piece of it,\nbut it goes the whole distance,",
    "start": "1899510",
    "end": "1905710"
  },
  {
    "text": "right? It's some curve. Now we'll pick Lt, usually,\nto be some nice convex",
    "start": "1905710",
    "end": "1910898"
  },
  {
    "text": "function, something that's\neasy to optimize, right? So we're going to\ntry and get that kind of easy-to-optimize function.",
    "start": "1910899",
    "end": "1915980"
  },
  {
    "text": "And then what we're\ngoing to do is we're going to\noptimize that function. We're going to find\nits local maximum. So it's local maximum, for the\nsake of writing, is, let's say,",
    "start": "1915980",
    "end": "1923770"
  },
  {
    "text": "here. And then we're going to set\nthat to be theta t plus 1, OK?",
    "start": "1923770",
    "end": "1929850"
  },
  {
    "text": "And this is now l\nof theta t plus 1. And we're going\nto, again, create",
    "start": "1929850",
    "end": "1935520"
  },
  {
    "text": "some new curve, Lt plus 1 of\ntheta, based on that point, OK? And the key aspects of the point\nthat I'll write in a second",
    "start": "1935520",
    "end": "1942179"
  },
  {
    "text": "is this point is a lower bound. This curve is a lower bound. It's always below the loss,\nso it's kind of a surrogate",
    "start": "1942179",
    "end": "1947580"
  },
  {
    "text": "that I'm not overestimating\nmy progress, and it's tight. It meets at exactly that point.",
    "start": "1947580",
    "end": "1952908"
  },
  {
    "text": "So if I did happen to have\nthe actual optimal value, it would meet at that point. So I wouldn't think\nand get fooled",
    "start": "1952909",
    "end": "1957940"
  },
  {
    "text": "that there was a higher loss\nfunction somewhere else. Let me write those\ntwo things, OK?",
    "start": "1957940",
    "end": "1965518"
  },
  {
    "text": "So first, Lt of theta is going\nto be less than l(theta).",
    "start": "1965519",
    "end": "1970870"
  },
  {
    "text": "We'll call this the\nlower bound property. Lt of theta t is going to\nbe equal to L of theta t--",
    "start": "1970870",
    "end": "1983700"
  },
  {
    "text": "sometimes call this\nthe tight property, OK? Our hope is Lt is easier\nto optimize than l.",
    "start": "1983700",
    "end": "1999179"
  },
  {
    "text": "So this picture-- the content\nis we're picking these-- like, that was a really\nbad drawing of one, but these picking these\nconcave kinds of functions,",
    "start": "1999179",
    "end": "2005630"
  },
  {
    "text": "which are easy to\nmaximize, right-- that's what I mean by it kind of\nlooks like a supervised thing. Then we maximize\nthat, and this is",
    "start": "2005630",
    "end": "2011800"
  },
  {
    "text": "formalizing the back and forth. We take that new\nmaximum that we have, which is our new best\neffort of parameters.",
    "start": "2011800",
    "end": "2017309"
  },
  {
    "text": "And then, [MOUTH POP]\nwe then do it again and create another curve. Now, the way we're going to\ncreate that curve-- you're",
    "start": "2017309",
    "end": "2023380"
  },
  {
    "text": "going to see in one minute. It's going to be Jensen's, and\nthat's the whole algorithm. So I'll sketch the algorithm.",
    "start": "2023380",
    "end": "2028769"
  },
  {
    "text": "You don't have math to\ntalk about the algorithm, but hopefully, it's\nclear what's going on. Easy-to-train\nsurrogate, and we kind",
    "start": "2028769",
    "end": "2035429"
  },
  {
    "text": "of slowly hillclimb with\nthat easy-to-train surrogate, alternating back and forth. And this is what we\nwere doing in K means. This is what we were\ndoing in GMMs as well.",
    "start": "2035429",
    "end": "2042450"
  },
  {
    "text": "And just so it's super clear,\nI want to make clear here,",
    "start": "2042450",
    "end": "2051230"
  },
  {
    "text": "phi t plus 1-- this is nothing more than\nthe argmax over theta",
    "start": "2051230",
    "end": "2057960"
  },
  {
    "text": "of Lt of theta-- means\nI do the optimization on the surrogate\ncurve that I created.",
    "start": "2057960",
    "end": "2066290"
  },
  {
    "text": "Cool. All right. I think that this description,\nhopefully, gives you",
    "start": "2066290",
    "end": "2072610"
  },
  {
    "start": "2069000",
    "end": "2198000"
  },
  {
    "text": "some intuition of what's going\non because, otherwise, the math is kind of bizarre-looking.",
    "start": "2072610",
    "end": "2079550"
  },
  {
    "text": "But we'll see. So this is the rough algo. I'm just restating\nwhat's on the thing. I'm not giving you enough math.",
    "start": "2079550",
    "end": "2084710"
  },
  {
    "text": "This is going to be called,\nnot surprisingly, E-step. This says, given phi t,\nfind this curve, L of t.",
    "start": "2084710",
    "end": "2098180"
  },
  {
    "text": "And then the M-step, and\ntogether, EM, given L of t,",
    "start": "2098180",
    "end": "2108260"
  },
  {
    "text": "set phi t plus 1 equal\nto argmax phi Lt(phi).",
    "start": "2108260",
    "end": "2114940"
  },
  {
    "text": "Cool. Please. Just could you reiterate? Like, why are we\nnot using gradients",
    "start": "2114940",
    "end": "2124680"
  },
  {
    "text": "on the original turbulence? Right, so we could imagine doing\nsome kind of gradient descent",
    "start": "2124680",
    "end": "2129710"
  },
  {
    "text": "here, but it's not clear how to\ndeal with this marginalization that happens in the middle. So if we did some\nmarginalization or some sampling, we\ncould do something",
    "start": "2129710",
    "end": "2136000"
  },
  {
    "text": "that looked like that,\nbut it's because we have this decomposition. Note we have-- you\ncan also imagine",
    "start": "2136000",
    "end": "2141200"
  },
  {
    "text": "that we have a decent\nsolver for the inner loop because it's this\nnice-to-solve thing. I would say, over time, this\nsplit of what's nice to solve",
    "start": "2141200",
    "end": "2149250"
  },
  {
    "text": "and what's not-- right\nnow, I'm pitching it to you as, it must be concave,\nand so it's nice.",
    "start": "2149250",
    "end": "2154520"
  },
  {
    "text": "But this kind of just means I\nhave an internal solver that's fast and I kind of\ntrust, and I have something on the outside\nthat's a latent variable",
    "start": "2154520",
    "end": "2160960"
  },
  {
    "text": "that I'm like splitting\nup the modeling. It's one of a number of\ndecomposition strategies. Doesn't mean it's the only\nway to solve it, though.",
    "start": "2160960",
    "end": "2166880"
  },
  {
    "text": "Wonderful question. Cool. All right, so the question is,\nhow do we construct L of t?",
    "start": "2166880",
    "end": "2178120"
  },
  {
    "text": "And I claim we know\neverything else. So we'll come back to\nthat claim in a second. OK?",
    "start": "2178120",
    "end": "2185039"
  },
  {
    "text": "So let's look. It's going to go term by term. So let's look at a single\nterm in our equation, OK?",
    "start": "2185040",
    "end": "2190190"
  },
  {
    "text": "All right, so I'm going to grab\none of these characters, just",
    "start": "2190190",
    "end": "2196170"
  },
  {
    "text": "one, and work with that, OK? So how do we construct it? So right now, we're trying\nto understand how to create",
    "start": "2196170",
    "end": "2204130"
  },
  {
    "start": "2198000",
    "end": "2647000"
  },
  {
    "text": "this L of t from this function. And you should\nroughly be thinking because I told you that\nJensen's will have something",
    "start": "2204130",
    "end": "2212210"
  },
  {
    "text": "to do with this. Now, what we're going to\ndo to put it in the form where Jensen's could be used\nlooks wholly unmotivated, OK,",
    "start": "2212210",
    "end": "2220510"
  },
  {
    "text": "totally unmotivated. But it's to shoehorn\ninto what we're doing, and there's some motivation, but\nit's kind of opaque, let's say.",
    "start": "2220510",
    "end": "2227450"
  },
  {
    "text": "What I'm going to do\nis something which, at first glance, seems strange.",
    "start": "2227450",
    "end": "2233940"
  },
  {
    "text": "Now, this is true, formally,\nfor any Q(z) that I pick, OK?",
    "start": "2233940",
    "end": "2241750"
  },
  {
    "text": "Please. Would you just go a\nlevel [INAUDIBLE]??",
    "start": "2241750",
    "end": "2247079"
  },
  {
    "text": "Oh. Right? So here, I'm just introducing Q.\nThis is true for any Q, right?",
    "start": "2247079",
    "end": "2254140"
  },
  {
    "text": "Let's not worry\nabout support issues, but I'm just putting in\nsomething that divides by 1-- seems sort of\nunmotivated to do this.",
    "start": "2254140",
    "end": "2260710"
  },
  {
    "text": "Now, I'm going to\nonly consider-- we get to pick Q's. So I'm going to pick Q's, so\nthat I can use Jensen, such",
    "start": "2260710",
    "end": "2269240"
  },
  {
    "text": "that it's a probability\ndistribution over the states such that the sum\nover Q(z) equals 1,",
    "start": "2269240",
    "end": "2277050"
  },
  {
    "text": "and Q(z) is greater\nthan or equal to 0. OK? And I'm going to call\nthis property star, OK?",
    "start": "2277050",
    "end": "2285470"
  },
  {
    "text": "So I'm going to pick Q as\na probability distribution. I'll write that in\na different color.",
    "start": "2285470",
    "end": "2292039"
  },
  {
    "text": "OK, why? Because now I can make\nmy argument one line. That's the real reason.",
    "start": "2292040",
    "end": "2300190"
  },
  {
    "text": "So how does it work? Yeah, good.",
    "start": "2300190",
    "end": "2305700"
  },
  {
    "text": "So we have this character-- copy-- in here.",
    "start": "2305700",
    "end": "2313079"
  },
  {
    "text": "This can also be written-- oops, I don't want to use blue. This can also be written\nas an expected value, where",
    "start": "2313079",
    "end": "2319630"
  },
  {
    "text": "z is distributed like Q of\nthis weird-looking quantity.",
    "start": "2319631",
    "end": "2329690"
  },
  {
    "text": "Why is that? Well, it's just the\ndefinition of expectation. This is just symbol pushing. There is nothing deep going on.",
    "start": "2329690",
    "end": "2338849"
  },
  {
    "text": "But it's important\nsymbol pushing because it means Jensen's applies. Oops, log of this thing, sorry.",
    "start": "2338849",
    "end": "2345790"
  },
  {
    "text": "Dammit, I forgot a log. OK. I'm just transforming this thing\ninternally into this notation.",
    "start": "2345790",
    "end": "2353119"
  },
  {
    "text": "Yeah, please. What's Q? Q is this function\nthat we picked up here. So Q is just some\nprobability distribution.",
    "start": "2353120",
    "end": "2358880"
  },
  {
    "text": "And this is going\nto define our curve. Just getting a little\nbit ahead of ourselves, we're going to\nallow-- the curve is going to be\nparameterized by whatever",
    "start": "2358880",
    "end": "2364400"
  },
  {
    "text": "probability\ndistribution we want. So it's our degree of freedom. I'm just telling\nyou something that's going to hold no matter how\nI select the probability",
    "start": "2364400",
    "end": "2370810"
  },
  {
    "text": "distribution. The tool that I have in\nmy arsenal to do that is Jensen's inequality. Now I've turned this into an\nexpectation, and in one line,",
    "start": "2370810",
    "end": "2377390"
  },
  {
    "text": "I'm going to be able to turn it\ninto a lower bound that works no matter how I pick it up. Graphically, what I'm doing--\nsorry to confuse folks who are",
    "start": "2377390",
    "end": "2383620"
  },
  {
    "text": "copying-- is basically show how to\nconstruct this Lt that's always a lower bound\neverywhere, and that's where I'm going to use\nJensen's inequality.",
    "start": "2383620",
    "end": "2390500"
  },
  {
    "text": "So let's see that next line. We'll come back to this. So this is less than.",
    "start": "2390500",
    "end": "2396250"
  },
  {
    "text": "I can pull the expectation out. P(x, z; theta) over Q(z).",
    "start": "2396250",
    "end": "2407970"
  },
  {
    "text": "This is Jensen, OK? Log is concave.",
    "start": "2407970",
    "end": "2419560"
  },
  {
    "text": "This is equal to some Q(z)\ntimes log P(x, z; theta) Q(z)--",
    "start": "2419560",
    "end": "2429609"
  },
  {
    "text": "again, just symbol pushing, OK? So there's only one\ncontent line here, OK?",
    "start": "2429609",
    "end": "2437900"
  },
  {
    "text": "The key holds for any\nQ satisfying star, OK?",
    "start": "2437900",
    "end": "2453150"
  },
  {
    "text": "OK? No matter how I pick the\nprobability distribution,",
    "start": "2453150",
    "end": "2461059"
  },
  {
    "text": "this chain of\nreasoning goes through. Please. [INAUDIBLE] always the first--",
    "start": "2461060",
    "end": "2471009"
  },
  {
    "text": "the second value? Like, how did you convert\nthe lower bound, that thing, and do certainly\nthe expectations? Yeah, so this was exactly\nJensen's inequality.",
    "start": "2471010",
    "end": "2477260"
  },
  {
    "text": "So if I scroll back up, this\nwas Jensen's inequality. But because I was applying\nit to the negative of it,",
    "start": "2477260",
    "end": "2483510"
  },
  {
    "text": "it's exactly the same piece,\nbut it reverses the inequality. And so I'm just directly\napplying that reasoning.",
    "start": "2483510",
    "end": "2490160"
  },
  {
    "text": "Well, I mean, like before, I was\nlike, how are you converging? Oh, this thing into this thing?",
    "start": "2490160",
    "end": "2496020"
  },
  {
    "text": "Yeah. Yeah, sorry. So this is just because Q(z)\nis a discrete distribution, and the definition\nof expectation",
    "start": "2496020",
    "end": "2501520"
  },
  {
    "text": "is, this is a bunch of\nnumbers that sum to 1. So this is an expectation with\nrespect to some distribution,",
    "start": "2501520",
    "end": "2506940"
  },
  {
    "text": "in particular, the\none where z(i) occurs with probability Q of z, z(i).",
    "start": "2506940",
    "end": "2512099"
  },
  {
    "text": "That's it. It's, again, just\nsymbol pushing. Please. [INAUDIBLE] distribution\nof z, it is going too far.",
    "start": "2512100",
    "end": "2519000"
  },
  {
    "text": "Therefore, 1 plus Yeah, so you want to know how\nwe ground it into an example. Is that what you're asking?",
    "start": "2519000",
    "end": "2525200"
  },
  {
    "text": "But isn't phi\ncompletely [INAUDIBLE]?? No, no. So there's no phi here. So apologies if there's\nsomething difficult to read.",
    "start": "2525200",
    "end": "2530700"
  },
  {
    "text": "There's a theta here. There's this new Q\nthat I've introduced. Q is something that I've\nartificially introduced.",
    "start": "2530700",
    "end": "2536170"
  },
  {
    "text": "And I'm just saying\nthat all I've shown here is that I have a way of--",
    "start": "2536170",
    "end": "2541710"
  },
  {
    "text": "if you pick a Q\nthat satisfies this, I have a way of lower\nbounding this function,",
    "start": "2541710",
    "end": "2547660"
  },
  {
    "text": "getting a family of\nlower bounds to it. And I'm trying to\ngive you the intuition of why I might want to do it. It's so that I can construct\nthose curves that come later,",
    "start": "2547660",
    "end": "2554359"
  },
  {
    "text": "because now this\nfunction is going to be much nicer to optimize,\nbut we haven't quite gotten there yet.",
    "start": "2554360",
    "end": "2560170"
  },
  {
    "text": "OK? So this whole thing is--\nthis gives a family.",
    "start": "2560170",
    "end": "2565400"
  },
  {
    "text": "This is just what\nI was saying there. So you're right ahead of it. This gives a family\nof lower bounds.",
    "start": "2565400",
    "end": "2574210"
  },
  {
    "text": "Namely, this is how I get\nLt theta less than l(theta)",
    "start": "2574210",
    "end": "2582060"
  },
  {
    "text": "because, term by term,\nit's going to be less than or equal to. Now, it doesn't satisfy\nall our requirements,",
    "start": "2582060",
    "end": "2589329"
  },
  {
    "text": "because we have\nto make it tight. So how do we make it tight? That's the next piece.",
    "start": "2589329",
    "end": "2596480"
  },
  {
    "text": "But right now, I have\na way of going term by term from the\nlikelihood function",
    "start": "2596480",
    "end": "2602299"
  },
  {
    "text": "and getting lower bounds\nat a particular spot. And it'll be a lower bound\nno matter where I am, OK?",
    "start": "2602300",
    "end": "2607359"
  },
  {
    "text": "But I have to pick a certain\nQ to make this operational. That's the piece. So I have freedom\nto pick Q, and I'm going to pick a very\nspecific Q, and that's",
    "start": "2607359",
    "end": "2613950"
  },
  {
    "text": "going to give me a lower bound. And that's going to allow\nme to get the curve. Go ahead. Would then start and then\nQ has to be greater than 0?",
    "start": "2613950",
    "end": "2620279"
  },
  {
    "text": "Yeah, yeah. So I said I was going\nto ignore the support. We can imagine just for\nthe sake of this lecture that it's strictly\ngreater than 0, so I don't run into\nweird things about what",
    "start": "2620280",
    "end": "2626410"
  },
  {
    "text": "I mean by divide by 0. Here, because I'm controlling\nthe multiplication ahead-- ahead of time, it\ndoes make sense,",
    "start": "2626410",
    "end": "2631470"
  },
  {
    "text": "but you're right\nto point that out. So just think about\nit as greater than 0. Yeah, wonderful question.",
    "start": "2631470",
    "end": "2637700"
  },
  {
    "text": "Cool, all right. All right, so now how\ndo we make it tight?",
    "start": "2637700",
    "end": "2647960"
  },
  {
    "start": "2647000",
    "end": "2849000"
  },
  {
    "text": "So what we have to do-- the intuition here\nis that we want",
    "start": "2647960",
    "end": "2653161"
  },
  {
    "text": "to make Jensen's\ninequality tight. And the idea is if what's\ninside is constant-- imagine there was\na constant inside,",
    "start": "2653161",
    "end": "2659230"
  },
  {
    "text": "that this term was constant for\nall the different values of z-- then the expectation clearly\ndoesn't matter, right?",
    "start": "2659230",
    "end": "2664980"
  },
  {
    "text": "So if they were all the\nsame, then these two would actually be equal\nto one another, right? This is some value,\nalpha, and then",
    "start": "2664980",
    "end": "2670961"
  },
  {
    "text": "you would get a sum over all\nthe alphas that were there. They would sum to 1. Boom, done. If they all have the\nsame value here, alpha,",
    "start": "2670961",
    "end": "2678318"
  },
  {
    "text": "they would be here in the\nlog, and they would also sum in the same exact way. So as long as this term\nis a constant-- that is,",
    "start": "2678319",
    "end": "2684609"
  },
  {
    "text": "it doesn't depend on z-- I'm in business, all right? So what that means is I want to\npick Q such that log P of x, z;",
    "start": "2684609",
    "end": "2696109"
  },
  {
    "text": "theta over Q(z) equals\nC. Now, before, I had all kinds of freedoms\nto pick whatever Q I wanted.",
    "start": "2696109",
    "end": "2703810"
  },
  {
    "text": "Now this is where the\nprobability comes in. So Q(z) has to be related in\nsome way to P x of z for this",
    "start": "2703810",
    "end": "2710369"
  },
  {
    "text": "to work. Go ahead. What is c? Ah, what is z or c? Oh, c is some constant.",
    "start": "2710369",
    "end": "2715950"
  },
  {
    "text": "This is a constant\nindependent of the-- just for some constant c.",
    "start": "2715950",
    "end": "2722680"
  },
  {
    "text": "It does not depend on\nz, independent of z.",
    "start": "2722680",
    "end": "2730160"
  },
  {
    "text": "We don't care what its value is. We just care that it doesn't\ndepend on z in any way, and then it will\nbe exact equality. Then Jensen's will\nbe equality, OK?",
    "start": "2730160",
    "end": "2740390"
  },
  {
    "text": "All right, so what is\nthe natural choice? Well it's that Q(z) should\nequal P of z given x; theta.",
    "start": "2740390",
    "end": "2753400"
  },
  {
    "text": "Why is that? Well, this is also equal to--",
    "start": "2753400",
    "end": "2763130"
  },
  {
    "text": "so this is because P of x of\nz of theta equals p of z x;",
    "start": "2763130",
    "end": "2772828"
  },
  {
    "text": "theta P of x; theta. So if I plug these\nin, they cancel out.",
    "start": "2772829",
    "end": "2781250"
  },
  {
    "text": "And c is equal to\nlog P x of theta, OK?",
    "start": "2781250",
    "end": "2790670"
  },
  {
    "text": "So let me make\nsure this is clear. Note-- this just means\n\"note well,\" and B-- I just use it reflexively\njust to signal.",
    "start": "2790670",
    "end": "2797930"
  },
  {
    "text": "Q(z) does depend on theta and x.",
    "start": "2797930",
    "end": "2806910"
  },
  {
    "text": "So we're going to have\nthis notation, Q(i) of z because it depends on\neach different value. So each data point is going to\nget its own different Q, which",
    "start": "2806910",
    "end": "2815480"
  },
  {
    "text": "is the log of how likely\nthis thing is, OK?",
    "start": "2815480",
    "end": "2821170"
  },
  {
    "text": "And we picked those for each i. So because we did this term\nby term, we can pick that Q--",
    "start": "2821170",
    "end": "2828570"
  },
  {
    "text": "Q1, Q2, Q3, all different. And we pick them all so\nthey satisfy this equation.",
    "start": "2828570",
    "end": "2840349"
  },
  {
    "text": "OK?",
    "start": "2840350",
    "end": "2846640"
  },
  {
    "text": "This thing has a\nvery famous name, so I'll write that while I kind\nof stall for more questions. So what we've defined here is\ncalled the Evidence-based Lower",
    "start": "2846640",
    "end": "2857820"
  },
  {
    "start": "2849000",
    "end": "2954000"
  },
  {
    "text": "BOund, or the ELBO, which\nactually is a fairly-- like, if you say ELBO to\na machine learning person,",
    "start": "2857820",
    "end": "2864000"
  },
  {
    "text": "they actually know what it is. It's not something\nwe're making up.",
    "start": "2864000",
    "end": "2870588"
  },
  {
    "text": "It's a real thing. So the ELBO of x, Q, z equals\nthe sum over z Q(z) log P(x, z;",
    "start": "2870589",
    "end": "2891440"
  },
  {
    "text": "theta) over Q(z), OK?",
    "start": "2891440",
    "end": "2897020"
  },
  {
    "text": "And what we've shown is\nthat l(theta) is less than",
    "start": "2897020",
    "end": "2903559"
  },
  {
    "text": "or equal to-- or is greater\nthan or equal to the sum, because we did this term by\nterm, of the ELBO of x of i,",
    "start": "2903559",
    "end": "2915570"
  },
  {
    "text": "Q of i, theta. Sorry, this is\nincorrect notation.",
    "start": "2915570",
    "end": "2923470"
  },
  {
    "text": "This is theta. Sorry, the z is\nmarginalized away. The z can't appear there. Only thing that\nappears there, OK--",
    "start": "2923470",
    "end": "2932338"
  },
  {
    "text": "for any Q(i) satisfying star.",
    "start": "2932339",
    "end": "2937740"
  },
  {
    "text": "Sound good? That was just restating\nthe lower bound.",
    "start": "2937740",
    "end": "2945578"
  },
  {
    "text": "All I said is we went term\nby term through this thing, so it holds for every term\nthat we can pick Q(i). As long as it's a\nprobability distribution,",
    "start": "2945579",
    "end": "2951700"
  },
  {
    "text": "it's a lower bound. And then we also\nshowed that l(theta) t",
    "start": "2951700",
    "end": "2958940"
  },
  {
    "start": "2954000",
    "end": "3021000"
  },
  {
    "text": "equals some i 1 to n ELBO x(i),\nQ(i), theta(t) for the choice",
    "start": "2958940",
    "end": "2974328"
  },
  {
    "text": "of Q(i) above, OK?",
    "start": "2974329",
    "end": "2983930"
  },
  {
    "text": "So hopefully, that\npicture makes sense. Again, just to recap\nwhat's going on here, we have this opportunity\nto pick these bounds,",
    "start": "2983930",
    "end": "2991059"
  },
  {
    "text": "and we'll use them in a\nsecond, so it'll hopefully become more clear\nexactly what we're kind of optimizing for here.",
    "start": "2991059",
    "end": "2996930"
  },
  {
    "text": "What we're going\nto do is we'll see how we pick the Q(i)'s and\nall the rest in a second. But this is basically saying\nthat it satisfies the two",
    "start": "2996930",
    "end": "3003069"
  },
  {
    "text": "properties that we had before. We're going to pick where\nwe are on the curve. We're going to find this\nupside-down bowl-shaped thing.",
    "start": "3003069",
    "end": "3010510"
  },
  {
    "text": "We're going to then optimize\nthat thing in a second, pick our new\ntheta(t) then repeat",
    "start": "3010510",
    "end": "3016380"
  },
  {
    "text": "and do another curve\nthat's present. All right, so let's do\nthe wrap-up and state",
    "start": "3016380",
    "end": "3023119"
  },
  {
    "start": "3021000",
    "end": "3204000"
  },
  {
    "text": "the algorithm now with our\nnewly hard-earned language. Yeah. [INAUDIBLE]? Then we need to\nfind a lower bound.",
    "start": "3023119",
    "end": "3032430"
  },
  {
    "text": "Oh, no. So these are both on\nthe original loss.",
    "start": "3032430",
    "end": "3038220"
  },
  {
    "text": "These are just saying,\nthis is the Lt here, capital L. Each one of these\nis capital L, basically, right?",
    "start": "3038220",
    "end": "3045450"
  },
  {
    "text": "And then this one here is saying\nthat, at that particular point for that t-th instantiation,\nthis is where we are. OK.",
    "start": "3045450",
    "end": "3051260"
  },
  {
    "text": "Yeah. All right, so the\nwrap-up is as follows-- this is how-- we\ncan now write down",
    "start": "3051260",
    "end": "3056650"
  },
  {
    "text": "the algorithm and the\nkind of full generality with mathematical precision,\nalthough it may still be a little bit opaque.",
    "start": "3056650",
    "end": "3063298"
  },
  {
    "text": "We set Q(i) of z in the E-step\nequal to the probability that z(i) given x(i) and theta\nfor i equals 1 to n, OK?",
    "start": "3063299",
    "end": "3076370"
  },
  {
    "text": "So this says that\nyou're going to pick the Q(i) distribution that says,\nwhat's the probability that's",
    "start": "3076370",
    "end": "3083640"
  },
  {
    "text": "most informed or the\nexact probability that comes from your model\nknowing the data and the current guess of\nyour parameters, right?",
    "start": "3083640",
    "end": "3089859"
  },
  {
    "text": "So you have some\ntheta at some time. You plug it in. You know the data point\nthat you're looking at. You condition on that.",
    "start": "3089859",
    "end": "3095690"
  },
  {
    "text": "And you say, what are the most\nlikely values of the cluster linkage-- as we\nwere talking about before, the source linkage--\nfor this particular point?",
    "start": "3095690",
    "end": "3102480"
  },
  {
    "text": "You get a probability\ndistribution over those. You set them to Q(i)z. That's really what's going on. It's your estimate of\nhow likely that is.",
    "start": "3102480",
    "end": "3109910"
  },
  {
    "text": "Then you take an M-step.",
    "start": "3109910",
    "end": "3116980"
  },
  {
    "text": "Theta t plus 1 equals argmax\nover theta of Lt(theta),",
    "start": "3116980",
    "end": "3126840"
  },
  {
    "text": "which equals-- so Lt(theta)-- sorry,\nI'll write it like this.",
    "start": "3126840",
    "end": "3133630"
  },
  {
    "text": "Lt(theta) equals this ELBO sum. x(i), Q(i), theta, OK?",
    "start": "3133630",
    "end": "3146720"
  },
  {
    "text": "Your current guess\nof parameters. So basically, what\nit's saying is, you give me a current\nguess of parameters. I get the lower bound that's\nunderneath the covers.",
    "start": "3146720",
    "end": "3153220"
  },
  {
    "text": "Then I optimize that\nlower bound surrogate. I get the theta t plus 1. That gives me a new\nguess of parameters,",
    "start": "3153220",
    "end": "3159838"
  },
  {
    "text": "which defines-- you\nget a new curve, a Q(i) for each one of what's\ngoing on, then I go back. Yeah, please.",
    "start": "3159839",
    "end": "3164963"
  },
  {
    "text": "Is it a ELBO down\nthere-- is that the Q(i)? Q-- oh, sorry.",
    "start": "3164963",
    "end": "3170609"
  },
  {
    "text": "Yeah, good call. This is Q(i), and this is theta. And I'm inconsistent\nwith the semicolons too.",
    "start": "3170609",
    "end": "3177870"
  },
  {
    "text": "So you move this. So there's a good\nvisual distance. This is an x. This is a Q. This is a theta.",
    "start": "3177870",
    "end": "3184380"
  },
  {
    "text": "Awesome. Please. More a notation\nquestion, I guess. Do you use that\nequation [INAUDIBLE]??",
    "start": "3184380",
    "end": "3190900"
  },
  {
    "text": "Right. Yeah. So it's just as\nbefore. t starts at 0. We have that initial guess,\nand then we go from there.",
    "start": "3190900",
    "end": "3196490"
  },
  {
    "text": "Theta is our current guess. Cool. All right, why does\nthis terminate?",
    "start": "3196490",
    "end": "3207750"
  },
  {
    "start": "3204000",
    "end": "3599000"
  },
  {
    "text": "And it's basically for\nsomething that's kind of not very interesting or\nsatisfying, but it does.",
    "start": "3207750",
    "end": "3217299"
  },
  {
    "text": "This gives you a sequence that\nis monotonically increasing or nondecreasing, OK?",
    "start": "3217299",
    "end": "3224380"
  },
  {
    "text": "So it's possible that it\nwould grind to a halt. But eventually, it\nhas to be strict.",
    "start": "3224380",
    "end": "3230410"
  },
  {
    "text": "There are some other things\nabout how fast it terminates. But it's a monotone\nsequence, so we'll have a convergence\nof subsequences.",
    "start": "3230410",
    "end": "3236170"
  },
  {
    "text": "That's really all\nthat matters, OK? We don't say how\nfast it converges. It's a separate issue.",
    "start": "3236170",
    "end": "3242240"
  },
  {
    "text": "Is it globally optimal? Well, no. No.",
    "start": "3242240",
    "end": "3247700"
  },
  {
    "text": "Just look at the picture.",
    "start": "3247700",
    "end": "3254329"
  },
  {
    "text": "And so to derive\na counterexample, you would just find\na likelihood function that had those two bumps. And you would run it in that\nparticular lower bound setting.",
    "start": "3254330",
    "end": "3262359"
  },
  {
    "text": "And what it will do is it\nwill gradually hillclimb. And this is actually not great. Like, it can't go\nback downhill, right?",
    "start": "3262359",
    "end": "3267880"
  },
  {
    "text": "It's got to just\ncontinue to go up. If it gets locked inside\none of those bumps,",
    "start": "3267880",
    "end": "3273170"
  },
  {
    "text": "it's kind of toast. OK, so in summary, what we saw\nhere is we derived EM as MLE",
    "start": "3273170",
    "end": "3286700"
  },
  {
    "text": "as promised, OK? So just to recap\nwhat happened here,",
    "start": "3286700",
    "end": "3294730"
  },
  {
    "text": "we started with this notion\naround Jensen's and convexity. So we looked at a\npictures of convexity, and we got an intuition\nof what sets are convex",
    "start": "3294730",
    "end": "3300700"
  },
  {
    "text": "and what sets are not. We wanted to use\nconcave functions, which are these kind of\ndownward-facing things.",
    "start": "3300700",
    "end": "3305760"
  },
  {
    "text": "The chords are always\nbelow them, OK? Those are the loss\nfunctions because we wanted to maximize them.",
    "start": "3305760",
    "end": "3311020"
  },
  {
    "text": "The reason that\nwas important is we had to do this\nback-and-forth iteration. Given a set of parameters, we\nwere going to find a surrogate.",
    "start": "3311020",
    "end": "3316640"
  },
  {
    "text": "That surrogate was going to\nbe concave in our setting. It's going to be one\nof those nice functions that we were after. We would use Jensen's inequality\nas a way of constructing",
    "start": "3316640",
    "end": "3324280"
  },
  {
    "text": "that entire curve. We needed the entire\ncurve because we wanted to optimize it. So it wasn't enough to find\na point in a lower bound.",
    "start": "3324280",
    "end": "3330030"
  },
  {
    "text": "We needed to find\nthe whole thing that was underneath it so we\ncould run our argmax step. And that was the\nsetting where we",
    "start": "3330030",
    "end": "3335560"
  },
  {
    "text": "would learn all\nof the parameters and estimate that in a\nway that was hopefully nice and easy to\ndo, which was like,",
    "start": "3335560",
    "end": "3343000"
  },
  {
    "text": "estimate the means and\nthe variance of the data that we're given. We'll run through\nan example of that. This is a necessarily kind\nof abstract and confusing",
    "start": "3343000",
    "end": "3350328"
  },
  {
    "text": "algorithm. The best way to\nunderstand it is just to run it through a couple\nof the different examples. EM and the next one-factor\nanalysis-- and by the end,",
    "start": "3350329",
    "end": "3356430"
  },
  {
    "text": "you'd be like, oh,\nOK, that makes sense. It's a lot of\nnotation because we're abstracting out a huge number\nof things that we're doing.",
    "start": "3356430",
    "end": "3362710"
  },
  {
    "text": "But in the end, it's\nnot so bad, right? You take the Q(i)'s. And this way, set the\nthetas, do a descent on them,",
    "start": "3362710",
    "end": "3372558"
  },
  {
    "text": "or ascent in this case. Do argmax. OK.",
    "start": "3372559",
    "end": "3377920"
  },
  {
    "text": "All right, so let's see it for\nour Gaussian mixture model. Please.",
    "start": "3377920",
    "end": "3383920"
  },
  {
    "text": "For this, [INAUDIBLE] this\ntermination condition event. Oh, so the termination condition\nis not really important,",
    "start": "3383920",
    "end": "3391210"
  },
  {
    "text": "or in the classical sense. The thing is that\nit's nondecreasing",
    "start": "3391210",
    "end": "3396230"
  },
  {
    "text": "so that, eventually, there's a\nconvergent subsequence of it. It's not telling you how fast\nit converges, for example.",
    "start": "3396230",
    "end": "3401700"
  },
  {
    "text": "And it converges\nfor the same reasons that GMMs converge that we\nwere kind of going downhill, if you remember it, every time. Like, there was some loss\nfunctional that was decreasing.",
    "start": "3401700",
    "end": "3408349"
  },
  {
    "text": "And this is just saying\nthere's something that's continually increasing. When do you terminate\nit operationally, like you're running this\nalgorithm, when do you decide?",
    "start": "3408349",
    "end": "3415450"
  },
  {
    "text": "You look and see if the loss\nof the likelihood function is not changing too much. What does too much depend on? Depends on your data,\ndepends on the problem.",
    "start": "3415450",
    "end": "3422170"
  },
  {
    "text": "Like, sometimes if you have a huge\namount of data, and you're averaging over\nbillions of examples.",
    "start": "3422170",
    "end": "3427660"
  },
  {
    "text": "Sometimes if you have only\na small amount of data, you want to get to\nmachine precision and 10",
    "start": "3427660",
    "end": "3432799"
  },
  {
    "text": "to the minus 16. And so that's the way\nyou decide when to do it. This just says that it's not\ngoing to oscillate wildly.",
    "start": "3432800",
    "end": "3438119"
  },
  {
    "text": "It's a very weak\nstatement I'm making. Yeah, please. Can you explain what\nspecific part of this",
    "start": "3438119",
    "end": "3444900"
  },
  {
    "text": "is linked to the MLE\nsort of aspect of it? Oh, awesome. Yeah, so we're going to see\nthe MLE when we actually",
    "start": "3444900",
    "end": "3451230"
  },
  {
    "text": "do the computation here. The reason it's linked to MLE\ncomes from a very simple piece,",
    "start": "3451230",
    "end": "3456720"
  },
  {
    "text": "which is we started in this\nmodel, where we were saying, the way we're going to\nthink about the world was to maximize the likelihood.",
    "start": "3456720",
    "end": "3462900"
  },
  {
    "text": "And that was how we\nthink about our data. That's less disturbing to this\ngroup than it is to, I guess,",
    "start": "3462900",
    "end": "3468160"
  },
  {
    "text": "generally worldwide who think\nabout this, because this is the only framework\nwe've used in the course, but that's what I mean.",
    "start": "3468160",
    "end": "3473829"
  },
  {
    "text": "We started with l(theta)\nas what we were optimizing, and then we derived this\nas a set of concerns.",
    "start": "3473829",
    "end": "3479030"
  },
  {
    "text": "We didn't get to\na global optimum. So I don't mean\nthat we definitely guaranteed that we got\nthe maximum likelihood",
    "start": "3479030",
    "end": "3484760"
  },
  {
    "text": "estimation, just that you can\nphrase what's going on as MLE. And so when you get into\nother estimation problems and the subproblems, you\njust apply the MLE stuff",
    "start": "3484760",
    "end": "3492410"
  },
  {
    "text": "you learned from the\nfirst half of the class. And we'll see that\nin an example. Does that make sense? Yeah, yeah.",
    "start": "3492410",
    "end": "3497750"
  },
  {
    "text": "Awesome. Thank you for the question. Please. Why is this tight? Which one? Why is this tight, the\nELBO you constructed?",
    "start": "3497750",
    "end": "3504940"
  },
  {
    "text": "Oh, it's tight because we\nwent through this small piece here, which was that\nif we selected it",
    "start": "3504940",
    "end": "3510050"
  },
  {
    "text": "as a constant in\nthis particular way-- so before we could pick any\nQ and it was a lower bound, as long as we did this,\nthen actually this line",
    "start": "3510050",
    "end": "3517230"
  },
  {
    "text": "was no longer an inequality but\nwas actually exact equality. Oh. And it depended, though--\nthat selection of Q",
    "start": "3517230",
    "end": "3523630"
  },
  {
    "text": "depends on theta and x. [INAUDIBLE] Awesome. Great question.",
    "start": "3523630",
    "end": "3529020"
  },
  {
    "text": "Yeah, and that's\njust making sure that the picture in your\nhead is exactly right. We go up to the loss curve.",
    "start": "3529020",
    "end": "3534860"
  },
  {
    "text": "We get something\nthat's underneath it that touches at that one point. And then any optimization\nwe do there is actually also optimization on\nthe loss curve itself.",
    "start": "3534860",
    "end": "3543670"
  },
  {
    "text": "Cool. All right.",
    "start": "3543670",
    "end": "3550049"
  },
  {
    "text": "EM for mixtures of Gaussians,\nor we call them GMMs, sorry.",
    "start": "3550049",
    "end": "3558230"
  },
  {
    "text": "All right. All right, so what's the E-step? Huh?",
    "start": "3558230",
    "end": "3563349"
  },
  {
    "text": "Yeah, I'm just going\nto copy down the thing.",
    "start": "3563349",
    "end": "3569310"
  },
  {
    "text": "So let's get the\ngeneric algorithms. Let me get the\ngeneric algorithm. All right.",
    "start": "3569310",
    "end": "3578210"
  },
  {
    "text": "All right, just so we\nhave it on the screen.",
    "start": "3578210",
    "end": "3586440"
  },
  {
    "text": "So here's our warm-up-- not really a warm-up because\nwe're almost out of time,",
    "start": "3586440",
    "end": "3593200"
  },
  {
    "text": "but here's-- remember, if we saw\nhow this worked-- P x(i) and z(i).",
    "start": "3593200",
    "end": "3598750"
  },
  {
    "text": "And remember, we factored it\nas the following-- this is just Bayes' rule, nothing crafty\ngoing on here, not tricky.",
    "start": "3598750",
    "end": "3608619"
  },
  {
    "text": "z(i) was a multinomial.",
    "start": "3608619",
    "end": "3614480"
  },
  {
    "text": "OK? This means phi i greater than",
    "start": "3614480",
    "end": "3626760"
  },
  {
    "text": "And this was, remember,\nour N cluster j. And so then once we knew--",
    "start": "3626760",
    "end": "3631828"
  },
  {
    "text": "given z(i) equals j-- that every cluster had\na different shape-- so we had a different mean,\nmu j, and a different size",
    "start": "3631829",
    "end": "3639930"
  },
  {
    "text": "or variance, mu j. And I'm doing everything\nin one dimension, but in two dimensions,\nyou would have actually the whole covariance\nwould be different.",
    "start": "3639930",
    "end": "3645319"
  },
  {
    "text": "These are the cluster\nsize descriptions, cluster means, OK?",
    "start": "3645319",
    "end": "3651170"
  },
  {
    "text": "All right, z(i) is\nour latent variable.",
    "start": "3651170",
    "end": "3658599"
  },
  {
    "text": "All right.",
    "start": "3658599",
    "end": "3666599"
  },
  {
    "text": "So let's take a look. What does EM actually do here?",
    "start": "3666599",
    "end": "3674290"
  },
  {
    "text": "So what is EM?",
    "start": "3674290",
    "end": "3680530"
  },
  {
    "text": "EM is very general. You can instantiate it, right? So what does it mean here?",
    "start": "3680530",
    "end": "3687059"
  },
  {
    "text": "So Q(i) of z is going to\nbe equal to P z(i) equals",
    "start": "3687059",
    "end": "3696600"
  },
  {
    "text": "j given x(i) and so forth, OK?",
    "start": "3696600",
    "end": "3705099"
  },
  {
    "text": "Now, what actually happened here\nwhen we wanted to understand--",
    "start": "3705099",
    "end": "3710230"
  },
  {
    "text": "what was the probability? This says, the probability\nthat i, the i-th component, belongs in j given what\nwe've observed about x(i)",
    "start": "3710230",
    "end": "3717690"
  },
  {
    "text": "and what we know\nabout the cluster shapes and their frequencies. So if you remember,\nwe had this diagram",
    "start": "3717690",
    "end": "3724700"
  },
  {
    "text": "that I drew quite poorly\nthe last time that said we had these two bumps,\nwhich were our two Gaussians,",
    "start": "3724700",
    "end": "3731890"
  },
  {
    "text": "let's say, in one dimensions\nthat looked like this. This was mu 2 sigma 2 square.",
    "start": "3731890",
    "end": "3740930"
  },
  {
    "text": "This was mu 1 sigma 1 square. And the question is, you\ngive me a point here.",
    "start": "3740930",
    "end": "3749880"
  },
  {
    "text": "This is my x(i). How likely is it to belong\nto 1 or 2, to cluster 1 or 2?",
    "start": "3749880",
    "end": "3764569"
  },
  {
    "text": "Right? That's basically\nwhat we're asking. What's the probability\nthat at this point, this i-th point here,\ncomes from 1 or 2?",
    "start": "3764569",
    "end": "3772710"
  },
  {
    "text": "Now, remember, if we\njust looked at this, and these two\ndistributions were--",
    "start": "3772710",
    "end": "3778311"
  },
  {
    "text": "or the phis were equal--\nthat is, both sources were generating the same\namount of information--",
    "start": "3778311",
    "end": "3783920"
  },
  {
    "text": "then we would say,\noh, it's probably much more likely it belongs\nto this function, cluster 1, than cluster 2.",
    "start": "3783920",
    "end": "3790400"
  },
  {
    "text": "But if we knew, say,\non the other hand--",
    "start": "3790400",
    "end": "3798338"
  },
  {
    "text": "if we knew phi 2 was hugely\nbigger than phi 1, right--",
    "start": "3798339",
    "end": "3803359"
  },
  {
    "text": "a billion points came\nfrom the second source, and only one point came\nfrom the first source--",
    "start": "3803359",
    "end": "3809390"
  },
  {
    "text": "we'd probably say that\nit's more likely that it would go to this, right? It would certainly\nboost its probability.",
    "start": "3809390",
    "end": "3815940"
  },
  {
    "text": "So now the question is, how\ndo we automate that reasoning? And that is Bayes' rule.",
    "start": "3815940",
    "end": "3821140"
  },
  {
    "text": "More likely in 2.",
    "start": "3821140",
    "end": "3826460"
  },
  {
    "text": "So to automate this,\nthis is Bayes' rule. This is all we did\nlast time, Bayes' rule.",
    "start": "3826460",
    "end": "3832380"
  },
  {
    "text": "It just weighs those\ntwo probabilities and tells us what should happen. That's it.",
    "start": "3832380",
    "end": "3839299"
  },
  {
    "text": "We ran through exactly those\ncalculations last time.",
    "start": "3839300",
    "end": "3844349"
  },
  {
    "text": "All right, let's take a\nlook at the M-step now. In the M-step, we have\nto compute derivatives.",
    "start": "3844349",
    "end": "3853819"
  },
  {
    "text": "I want to highlight\nonly one thing here because it's something that\ncauses people pain when",
    "start": "3853819",
    "end": "3859548"
  },
  {
    "text": "they do their homeworks. We have to compute derivatives. So we're maximizing here\nover all the parameters, phi",
    "start": "3859549",
    "end": "3867299"
  },
  {
    "text": "and mu and sigma, sigmas,\nsorry, all the covariants. So these are the\nsigmas, lowercase.",
    "start": "3867299",
    "end": "3872410"
  },
  {
    "text": "And the notation above-- these are all theta\nor all theta, right?",
    "start": "3872410",
    "end": "3879099"
  },
  {
    "text": "So theta refers to all the\nparameters of the problem. We were breaking it out into\nmus and sigmas and phis. So those are all the\nthings we're observing.",
    "start": "3879100",
    "end": "3885210"
  },
  {
    "text": "Everything that's\nnonlatent, that's observed, not hidden to us. And what we are maximizing\nover from our ELBO",
    "start": "3885210",
    "end": "3891240"
  },
  {
    "text": "lower bound was this, sum\nover z(i) Q(i) z(i) log P",
    "start": "3891240",
    "end": "3905440"
  },
  {
    "text": "x of i, z(i), theta\nover Q(i) of z(i), OK?",
    "start": "3905440",
    "end": "3918220"
  },
  {
    "text": "This whole thing,\nwe're going to call fi.",
    "start": "3918220",
    "end": "3926660"
  },
  {
    "text": "This is fi of theta. It hides a ton in our\nnotation, all right?",
    "start": "3926660",
    "end": "3935390"
  },
  {
    "text": "So this thing is-- let's write it out because\nthe gory details will help us.",
    "start": "3935390",
    "end": "3940900"
  },
  {
    "text": "Oh, please. You have a question. Do you mind defining what\nis latent and what is not?",
    "start": "3940900",
    "end": "3947160"
  },
  {
    "text": "Yeah, so in our terminology,\nz is just latent. So I'm giving you the\nintuition that it's something that's hidden or not observed. But formally, it's just going\nto be anything that's a z.",
    "start": "3947160",
    "end": "3954339"
  },
  {
    "text": "z is latent. That's our definition. Yeah. Please. So the fi is just like\nthe ELBO [INAUDIBLE]..",
    "start": "3954339",
    "end": "3959440"
  },
  {
    "text": "Exactly right. Yeah, this is exactly\nthe instantiation of what we had above. We reasoned about this through\nad-hoc reasons last time,",
    "start": "3959440",
    "end": "3966140"
  },
  {
    "text": "but it is exactly the\nELBO that we're now going to minimize with derivatives. And to make it\nconcrete, I am either",
    "start": "3966140",
    "end": "3971520"
  },
  {
    "text": "going to waste a bunch of\nyour time or something will snap in your head and see how\nthese things put together. I'm going to write out\nexactly what fi(theta)",
    "start": "3971520",
    "end": "3977910"
  },
  {
    "text": "is so that you can see what the\nderivatives are that you will compute on this thing, because\nright now, it's probably pretty mysterious to you.",
    "start": "3977910",
    "end": "3983970"
  },
  {
    "text": "Like, there's ELBOs, and\nthere's P's, and there's Q's. And you can just\nwrite this thing down and compute its derivatives,\nand that's what you do.",
    "start": "3983970",
    "end": "3990140"
  },
  {
    "text": "I mean, that's how this\nwhole method works, just abstracted three\norders of magnitude more than it should be, OK?",
    "start": "3990140",
    "end": "3997279"
  },
  {
    "text": "So let's see that piece. Oh, please. Sorry. The z(i) that said\nwe're summing over?",
    "start": "3997279",
    "end": "4003190"
  },
  {
    "text": "Yeah, that's going to be-- so\nI'm just using that notation to make sure it's clear\nthat it depends on the i.",
    "start": "4003190",
    "end": "4009150"
  },
  {
    "text": "It's actually just a z\nthat you're summing over. And it's summing\nover, for example-- like, we're imagining that it's\ndiscrete to make our notation",
    "start": "4009150",
    "end": "4016270"
  },
  {
    "text": "a little bit nicer. It would be summing over all\nof the different clusters that are possible there, all\nthe different sources.",
    "start": "4016270",
    "end": "4021660"
  },
  {
    "text": "How likely are you to be in\ncluster 1, 2, 3, 4, 5, so on? You could also--\nwe'll see later-- replace it with an integral if\nyou had something really fancy",
    "start": "4021660",
    "end": "4028609"
  },
  {
    "text": "that was there, like if you\nhad a continuous distribution over the hidden states. Yeah.",
    "start": "4028609",
    "end": "4034190"
  },
  {
    "text": "[INAUDIBLE] z, to\nsimilar to what phi does, like if phi is greater\nthan [INAUDIBLE],, what",
    "start": "4034190",
    "end": "4043640"
  },
  {
    "text": "sorts of [INAUDIBLE]? Ah, yeah. So it is, in fact,\nbecause of this right here, which I kind\nof glossed over.",
    "start": "4043640",
    "end": "4049529"
  },
  {
    "text": "Q(i) is exactly setting that-- is setting this function. So I glossed over this\nreally, really quickly",
    "start": "4049529",
    "end": "4054890"
  },
  {
    "text": "because it was the same\ncalculation we did last time. P z(i) j-- to compute\nthat, remember, we expanded it by Bayes' rule.",
    "start": "4054890",
    "end": "4061579"
  },
  {
    "text": "We had two different components. We had, if you knew you\nwere in a cluster, how likely is the data point?",
    "start": "4061579",
    "end": "4066589"
  },
  {
    "text": "And then we had\na term that said, how likely is the cluster? And those were the two functions\nthat we put in and broke down by Bayes' rule.",
    "start": "4066589",
    "end": "4072410"
  },
  {
    "text": "It's exactly the same. You've got it perfectly. Yeah. All right, so let me\nwrite out this monstrosity",
    "start": "4072410",
    "end": "4079088"
  },
  {
    "text": "just because it will\nbe potentially-- it has been in the\npast educational. Who knows if it's\neducational in the future,",
    "start": "4079089",
    "end": "4087680"
  },
  {
    "text": "and the future being,\nlike, two seconds from now? All right, I'm going\nto use a notation, and hopefully it\ndoesn't confuse you--",
    "start": "4087680",
    "end": "4094099"
  },
  {
    "text": "Q(i) equals z j. So this is the piece there. So this is the weight. This w(i) is the same\nw(i) we had before.",
    "start": "4094099",
    "end": "4100870"
  },
  {
    "text": "I'm sure you're intimately\nfamiliar with all the notation I used in the GMM lecture, but\nit's the same w(i)j that we had",
    "start": "4100870",
    "end": "4106798"
  },
  {
    "text": "before. It's the weight that\nsummarizes this probability, just so I don't have to write\nthat whole thing out, OK?",
    "start": "4106799",
    "end": "4112778"
  },
  {
    "text": "All right, so fi\nof theta is going to be equal to the sum over j--",
    "start": "4112779",
    "end": "4120060"
  },
  {
    "text": "because now I'm summing over\nthe cluster centers, right? The z(i) notation was still\nvery abstract-- wj(i),",
    "start": "4120060",
    "end": "4126100"
  },
  {
    "text": "which was summing over\nthis part here, log--",
    "start": "4126100",
    "end": "4131778"
  },
  {
    "text": "and help us all, 1 over 2 pi-- this is a covariance, 1/2.",
    "start": "4131779",
    "end": "4139089"
  },
  {
    "text": "This is the exp of",
    "start": "4139090",
    "end": "4146219"
  },
  {
    "text": "Oh, I decided to write this\nin four general things.",
    "start": "4146220",
    "end": "4153170"
  },
  {
    "text": "Why do I care about that? Oh, I see why.",
    "start": "4153170",
    "end": "4158359"
  },
  {
    "text": "OK. Transpose sigma inverse\nx(i) mu j times phi j.",
    "start": "4158359",
    "end": "4168750"
  },
  {
    "text": "Oh, I just missed it. Oh, that hurts. All right, let me scoot.",
    "start": "4168750",
    "end": "4174810"
  },
  {
    "text": "So much better here. On a whiteboard, that's\nreally catastrophic-- phi j, OK?",
    "start": "4174810",
    "end": "4180700"
  },
  {
    "text": "Let me make sure the\nbrackets are clear. I'm going to\nhighlight the brackets like it's a syntax editor. So make sure they're all there\nwhere they're supposed to be.",
    "start": "4180700",
    "end": "4187528"
  },
  {
    "text": "This blue goes with\nthat one, so, OK, great. Ah, and that means\nI'm missing a log. Perfect.",
    "start": "4187529",
    "end": "4193278"
  },
  {
    "text": "All right. Not so bad. Oh, and this whole\nthing is unfortunately-- snap 2?",
    "start": "4193279",
    "end": "4199340"
  },
  {
    "text": "Yeah-- over w(i)j, right? That's just this\npiece is this piece.",
    "start": "4199340",
    "end": "4207610"
  },
  {
    "text": "This piece here-- this\nis the probability. This is the Gaussian,\nremember, from our model. Let's go back up here.",
    "start": "4207610",
    "end": "4213000"
  },
  {
    "text": "Sorry for all the scrolling. This is our Gaussian here. This is a Gaussian\ndistribution with center j.",
    "start": "4213000",
    "end": "4218690"
  },
  {
    "text": "I did use a higher\ndimensional covariance because it's something you're\ngoing to have to compute. So I've gone from 1D\nto higher dimensions.",
    "start": "4218690",
    "end": "4225190"
  },
  {
    "text": "The notation doesn't\nchange except for this is what the Gaussian looks\nlike instead of a square. You know that, right?",
    "start": "4225190",
    "end": "4230600"
  },
  {
    "text": "And then there's the phi\nj, which is just multiplied times this horrible expression. And this exp parentheses is\nso I don't have to write it",
    "start": "4230600",
    "end": "4237850"
  },
  {
    "text": "in superscript, right? Just expo the function, just\na bad habit that I always use brackets for this.",
    "start": "4237850",
    "end": "4243960"
  },
  {
    "text": "It's historical, and I would\nlove to beat it out of myself if it were possible.",
    "start": "4243960",
    "end": "4249980"
  },
  {
    "text": "Please. Does the covariance depend on j? Right now, the covariance\ndoes not depend on z. In our model, the covariance\nhere is something that--",
    "start": "4249980",
    "end": "4258940"
  },
  {
    "text": "it depends on which\ncluster, right? So it depends on j. Sorry, I just want make\nsure I understand what you-- That's it. Yeah, so I think if you--\nit means it depends on j,",
    "start": "4258940",
    "end": "4265010"
  },
  {
    "text": "yes, the covariance could\nhave different shapes. Some could be long and skinny. Some could be shorten and round. Yeah, those depend on j.",
    "start": "4265010",
    "end": "4270730"
  },
  {
    "text": "So this thing here is a\nvery polite way of saying, this guy here\nshould depend on j.",
    "start": "4270730",
    "end": "4277100"
  },
  {
    "text": "Yeah, good catch. All right, so now we can compute\nsome fun derivatives, OK?",
    "start": "4277100",
    "end": "4291330"
  },
  {
    "text": "So let's compute mu\nj of fi of theta.",
    "start": "4291330",
    "end": "4298409"
  },
  {
    "text": "We have to estimate\nthe mean, right?",
    "start": "4298409",
    "end": "4304520"
  },
  {
    "text": "Now. And I'm going to\ndo it-- actually, I'm going to do something\nslightly harder. So apologies if you\nwrote that down.",
    "start": "4304520",
    "end": "4310739"
  },
  {
    "text": "Let's do this. It'll be just one extra line\nbecause it's all linear. I'm going to sum over\nall the data, 1 to n.",
    "start": "4310740",
    "end": "4317239"
  },
  {
    "text": "OK. So what this becomes\nis sum equals 1 to n. This is over all the data.",
    "start": "4317239",
    "end": "4322750"
  },
  {
    "text": "I get mu j here, mu j, times--",
    "start": "4322750",
    "end": "4329010"
  },
  {
    "text": "and then it's going\nto be wj, and I'm going to drop terms inside the\nlog that obviously have nothing to do with mu j.",
    "start": "4329010",
    "end": "4338010"
  },
  {
    "text": "sorry. T sigma inverse j x(i) mu j.",
    "start": "4338010",
    "end": "4348440"
  },
  {
    "text": "All right, and so just so you're\nclear what's going on here, the log turns these\nmultiplications into additions.",
    "start": "4348440",
    "end": "4356219"
  },
  {
    "text": "So when I take\nderivatives, this doesn't show up anywhere because\nsigma doesn't depend on it-- sorry it doesn't depend on mu.",
    "start": "4356219",
    "end": "4362500"
  },
  {
    "text": "And this doesn't\ndepend on mu either, so I'm left with these terms. Please, go ahead. Oh, what is the physical\nmeaning of the fi here?",
    "start": "4362500",
    "end": "4371333"
  },
  {
    "text": "fi? fi is just the term\nhere in a a function. It is the likelihood\nfunction after we've picked",
    "start": "4371333",
    "end": "4377170"
  },
  {
    "text": "Q at the particular iteration. So it's just notation\nso I don't have to write this monstrosity every time.",
    "start": "4377170",
    "end": "4383199"
  },
  {
    "text": "Yeah, but here, it's-- It's the ELBO. OK. It is exactly the ELBO. fi is the i-th ELBO.",
    "start": "4383200",
    "end": "4389620"
  },
  {
    "text": "OK. So when wj has\nsomething to do with mu?",
    "start": "4389620",
    "end": "4398199"
  },
  {
    "text": "Actually, it doesn't. I don't know why I kept it. Yeah, w doesn't have\nanything to do with lambda j.",
    "start": "4398199",
    "end": "4405810"
  },
  {
    "text": "So it should be crossed out. Is that true? Let me show-- let me see\nsomething crazy here.",
    "start": "4405810",
    "end": "4412850"
  },
  {
    "text": "No, it shouldn't have\nanything to do with it.",
    "start": "4412850",
    "end": "4418590"
  },
  {
    "text": "But it will be-- sorry, I see what's going on. This is what's going on.",
    "start": "4418590",
    "end": "4423840"
  },
  {
    "text": "This is 1/2, and\nthis is a minus. w(i)j is multiplied by it.\nw(i)j is take the derivative",
    "start": "4423840",
    "end": "4429320"
  },
  {
    "text": "of the log. It's going to be this\ntimes this thing plus. Yeah. Yeah, sorry. Thank you for the\nnotational issue.",
    "start": "4429320",
    "end": "4435900"
  },
  {
    "text": "Yeah. Cool. All right, we're in business.",
    "start": "4435900",
    "end": "4441219"
  },
  {
    "text": "So what happens now? Well, some mechanics that almost\ncertainly will introduce bugs and you will catch,\nand it'll be great.",
    "start": "4441219",
    "end": "4447360"
  },
  {
    "text": "That's learning happening there\nand me making mistakes, OK?",
    "start": "4447360",
    "end": "4452920"
  },
  {
    "text": "So when we actually\ncompute this, this is going to be\nsigma j x(i) minus mu j.",
    "start": "4452920",
    "end": "4462250"
  },
  {
    "text": "You computed this a bunch\nof times, all right? So, yeah, all good.",
    "start": "4462250",
    "end": "4468880"
  },
  {
    "text": "So when can we pull this\nthing out that's repeated? Because it's full rank, we can\npull it out, and it's linear,",
    "start": "4468880",
    "end": "4476429"
  },
  {
    "text": "and it doesn't change anything. So we want to set this to 0\nand use that sigma j inverse.",
    "start": "4476429",
    "end": "4488980"
  },
  {
    "text": "Sigma j is full rank. And that will become\nclear in a second why that matters so much,\nbecause when we pull it out,",
    "start": "4488980",
    "end": "4494370"
  },
  {
    "text": "what do we get? We get here sigma j\ninverse times sum,",
    "start": "4494370",
    "end": "4502730"
  },
  {
    "text": "which is an\nunfortunate collision, i equals 1 to n w(i)j x(i)\nminus mu j equals 0, OK?",
    "start": "4502730",
    "end": "4513800"
  },
  {
    "text": "But then because\nthis is full rank, the only way that\nthis thing is 0 is if it's identically 0, right?",
    "start": "4513800",
    "end": "4519909"
  },
  {
    "text": "If this were non-full rank-- sorry, the j is\nin the wrong spot. That's extraordinarily\nconfusing.",
    "start": "4519910",
    "end": "4527300"
  },
  {
    "text": "Since this matrix is full\nrank, for this thing to be 0 means that this blue\npart is identically 0.",
    "start": "4527300",
    "end": "4533860"
  },
  {
    "text": "And so what that tells us is\nmu j should be equal to sum i",
    "start": "4533860",
    "end": "4544830"
  },
  {
    "text": "w(i)j x(i) over sum w(i)j.",
    "start": "4544830",
    "end": "4551270"
  },
  {
    "text": "Yeah. That was before, OK?",
    "start": "4551270",
    "end": "4562440"
  },
  {
    "text": "So, so far, nothing happened. We estimated the means\nby simply averaging",
    "start": "4562440",
    "end": "4567900"
  },
  {
    "text": "their weighted averages,\nand we computed this before, and it's just a matter of\ncomputing the derivatives. The one that I actually care\nabout showing you, by the way,",
    "start": "4567900",
    "end": "4574490"
  },
  {
    "text": "is phi j, so let me just\njump to that because we only have a minute or two left. And I want to show you\nwhat happens in phi j.",
    "start": "4574490",
    "end": "4579590"
  },
  {
    "text": "So phi j is constrained. [INAUDIBLE] Please. Would you mind showing\n[INAUDIBLE] scrolling up?",
    "start": "4579590",
    "end": "4588440"
  },
  {
    "text": "Sure. No, wait. I just want the last one. OK, sure. Also I would say, ahead of time,\nI do post all the notes online.",
    "start": "4588440",
    "end": "4596980"
  },
  {
    "text": "Please feel free to take our\nreference to those notes too. They will have\npotentially fewer typos than me trying to\nanswer questions, draw,",
    "start": "4596980",
    "end": "4603199"
  },
  {
    "text": "and generally be distracted. Can't focus that long. I have to read the notes. They still do have\ntypos, though,",
    "start": "4603199",
    "end": "4609120"
  },
  {
    "text": "so always look at the notes. All right, let me just\nshow this one thing, phi j is constrained, OK?",
    "start": "4609120",
    "end": "4614900"
  },
  {
    "text": "So phi j is\nconstrained, and I just want to remind you of\nsomething that you probably learned in high school or\nfreshman year in calculus.",
    "start": "4614900",
    "end": "4623610"
  },
  {
    "text": "I don't actually know when\nanyone learns anything. Anytime I say\nsomething like that, my students always get upset\nwith me, so I should just stop. But I assume you've seen\nit before this moment,",
    "start": "4623610",
    "end": "4630100"
  },
  {
    "text": "how about that? You need a Lagrangian, OK? No, you haven't seen it?",
    "start": "4630100",
    "end": "4635940"
  },
  {
    "text": "That's fine too. If you want, I'll\npost notes about how to compute Lagrangians as well. If you haven't seen\nthis before, this",
    "start": "4635940",
    "end": "4641110"
  },
  {
    "text": "will trip you up in some way. So when you compute the\nderivative with respect to phi j, what happens is you're going\nto get something that says you",
    "start": "4641110",
    "end": "4650170"
  },
  {
    "text": "have this weighted sum w(i)j\ntimes the derivative of phi j",
    "start": "4650170",
    "end": "4658020"
  },
  {
    "text": "log phi j plus-- so if you just take this\nand compute the derivative,",
    "start": "4658020",
    "end": "4665980"
  },
  {
    "text": "it doesn't account\nfor the constraint. So you have a bunch of\nnumbers that must sum to 1. So if you think about\nyou're on a line--",
    "start": "4665980",
    "end": "4673320"
  },
  {
    "text": "let's say that you're\noptimizing on a line, right? If the gradient--\nlike, let's say",
    "start": "4673320",
    "end": "4678520"
  },
  {
    "text": "that your points are on this\nline, and you're saying, I want to optimize here. This condition that you could\nimagine for an optimal solution",
    "start": "4678520",
    "end": "4685900"
  },
  {
    "text": "is the gradient's\nidentically 0, right? It vanishes. That's good.",
    "start": "4685900",
    "end": "4691020"
  },
  {
    "text": "That's a point. But what if the gradient is\nperpendicular to the line? Like, its wants to push\nyou only perpendicular and has no component moving\nyou along the line, right?",
    "start": "4691020",
    "end": "4699050"
  },
  {
    "text": "In that case, this is\nstill a critical point. It's still\npotentially a minimum. Does that make sense?",
    "start": "4699050",
    "end": "4704170"
  },
  {
    "text": "Because it's not\ntelling you that there's a minimum to your\nleft and right. It's along the line, OK?",
    "start": "4704170",
    "end": "4711150"
  },
  {
    "text": "So the question is, how do\nyou encode that information that you want to kind of\nscreen off information",
    "start": "4711150",
    "end": "4717210"
  },
  {
    "text": "that's orthogonal to the line? And I'll write up a little\nnote to show this whole thing. What you do is you\nintroduce this thing called",
    "start": "4717210",
    "end": "4723020"
  },
  {
    "text": "Lagrange multipliers. And Lagrange multipliers--\nand if you haven't seen them,",
    "start": "4723020",
    "end": "4728151"
  },
  {
    "text": "don't worry. These are super easy to teach. Just say this-- it's\njust an extra term here.",
    "start": "4728151",
    "end": "4735610"
  },
  {
    "text": "And this multiplier-- it's not\nobvious in this formulation what it's doing, but this\nmultiplier is basically",
    "start": "4735610",
    "end": "4740920"
  },
  {
    "text": "the thing that\nscreening off things that are orthogonal to\nthese constraints, OK?",
    "start": "4740920",
    "end": "4746310"
  },
  {
    "text": "So this constraint here\nsays, theta j is equal to 1. And you set this\nconstraint equal to 0,",
    "start": "4746310",
    "end": "4751620"
  },
  {
    "text": "this term equal to 0. And it says, if you're going off\nin a direction that would not change any of their\nvalues, that's OK.",
    "start": "4751620",
    "end": "4757989"
  },
  {
    "text": "You get to screen that off. And I'll make that\ngeometric intuition. I'll just post a one-page\nwrite-up for you.",
    "start": "4757989",
    "end": "4763000"
  },
  {
    "text": "Please remind me in the thread,\nand I will definitely do that. If you don't do that,\nyou'll get the wrong answer.",
    "start": "4763000",
    "end": "4768329"
  },
  {
    "text": "That's also a\nmotivation to learn it. And so what ends up happening\nhere is you get something that says, I get sum i goes from 1 to\nn w(i)j over phi j plus lambda",
    "start": "4768330",
    "end": "4781960"
  },
  {
    "text": "equals 0. And this implies that phi of j\nis equal to 1 over lambda sum i",
    "start": "4781960",
    "end": "4789820"
  },
  {
    "text": "equals 1 to n of w(i)j, OK? And the lambda is playing\na very simple role here.",
    "start": "4789820",
    "end": "4796980"
  },
  {
    "text": "It's just telling you, you have\nto normalize them in some way, right?",
    "start": "4796980",
    "end": "4802290"
  },
  {
    "text": "Now, since-- in this case, we\ncan do it in an ad-hoc way. Since phi j is equal to 1, this\nimplies the sum of phi j is",
    "start": "4802290",
    "end": "4811949"
  },
  {
    "text": "equal to negative 1 over\nlambda sum i, j w(i)j,",
    "start": "4811949",
    "end": "4822020"
  },
  {
    "text": "and this equals\nnegative n/lambda, OK? And that's the correct\nnormalization, right?",
    "start": "4822020",
    "end": "4828230"
  },
  {
    "text": "Oh, sorry, this is\nequal to negative n. Oh, sorry, n/lambda, right?",
    "start": "4828230",
    "end": "4836260"
  },
  {
    "text": "And so now you can then go\nback and normalize and cancel out the-- divide everything by 1 since\nthis is just-- this sum",
    "start": "4836260",
    "end": "4842760"
  },
  {
    "text": "is equal to 1. I divide everything by 1\nhere, and that tells me that lambda must be equal\nto 1 over negative n.",
    "start": "4842760",
    "end": "4852550"
  },
  {
    "text": "This is equal to 1. This implies lambda\nequals negative 1/n.",
    "start": "4852550",
    "end": "4858050"
  },
  {
    "text": "It's just normalizing. It's just doing\nthe average, which was weird to look at before. But that allows us to compute\nall the things in the way",
    "start": "4858050",
    "end": "4864750"
  },
  {
    "text": "we would expect. Here, it's totally natural. So if you don't get\nthe general rule that I'm trying to\ntell you, the reason I'm trying to tell\nyou is I think",
    "start": "4864750",
    "end": "4870158"
  },
  {
    "text": "we make you use it at\nsome point, this rule. So just check. It'll come up on a homework. I don't think comes on an exam.",
    "start": "4870159",
    "end": "4876039"
  },
  {
    "text": "But just flag something\nwhen you have a constraint. When you have a constrained\nprobability distribution, you have to use a\nLagrange multiplier.",
    "start": "4876040",
    "end": "4882960"
  },
  {
    "text": "That's all I care about\nthat you understand. In this case, it makes\ntotal sense, though, because these numbers\nhave to sum to 1.",
    "start": "4882960",
    "end": "4888210"
  },
  {
    "text": "So if you don't have a\nnormalization constant here, you're adding up a bunch of\nnumbers which individually sum up to n, right?",
    "start": "4888210",
    "end": "4893540"
  },
  {
    "text": "The sum over all of them is n. You better normalize\nthem in some way. And this is just the\nprinciple that tells you,",
    "start": "4893540",
    "end": "4898900"
  },
  {
    "text": "you have to normalize\nthem by this n factor, OK? So all I care that\nyou take away,",
    "start": "4898900",
    "end": "4904140"
  },
  {
    "text": "if you've seen this a thousand\ntimes before, don't worry. I hope you had a nice rest. If you've never\nseen this before, I just want to\nflag for you, when",
    "start": "4904140",
    "end": "4910561"
  },
  {
    "text": "you minimize a function\nthat's constrained to make sure you use\nLagrange multipliers. I will put up a little\ntutorial about them.",
    "start": "4910561",
    "end": "4916130"
  },
  {
    "text": "You do not need to spend\na bunch of time on them. It's just if you see some\nchallenge where you're actually supposed-- some problem where\nyou're supposed to do it, just",
    "start": "4916130",
    "end": "4922739"
  },
  {
    "text": "have a little light bulb\nto go off that says, OK, I've got to look up\nhow to do it in this case. That's all I care about, OK? And you'll trace\nthrough it in the notes.",
    "start": "4922739",
    "end": "4929850"
  },
  {
    "text": "Please. So I have two questions. [INAUDIBLE] minus\n[INAUDIBLE] equals to 1",
    "start": "4929850",
    "end": "4936250"
  },
  {
    "text": "because that is lambda\nequals to minus [INAUDIBLE]..",
    "start": "4936250",
    "end": "4941850"
  },
  {
    "text": "Yeah, it equals to minus--\nsorry, equals minus 1/n because there's a\nnegative sign everywhere.",
    "start": "4941850",
    "end": "4946960"
  },
  {
    "text": "So this minus lambda is\ngoing to be equal to 1/n. So I'm just going to write\nthe final expression.",
    "start": "4946960",
    "end": "4952389"
  },
  {
    "text": "Maybe that would be less-- yeah. So [INAUDIBLE].",
    "start": "4952389",
    "end": "4958440"
  },
  {
    "text": "Oh, sorry, sorry, sorry, sorry. Oh, this arithmetic is\nwhat's bothering you.",
    "start": "4958440",
    "end": "4964650"
  },
  {
    "text": "Sorry, sorry. This is true. Oh, OK. Yeah, sorry. I just swapped them,\nand even that's backwards from how\nI should do it.",
    "start": "4964650",
    "end": "4970650"
  },
  {
    "text": "I didn't see that. Sorry about that. Thank you for the catch. It's just supposed to average\nout so it looks the same.",
    "start": "4970650",
    "end": "4980380"
  },
  {
    "text": "Awesome. Any questions about this? All right. So also, why is it [INAUDIBLE]?",
    "start": "4980380",
    "end": "4990250"
  },
  {
    "text": "Because it's a\nprobability distribution. So again, the issue here is phi\nj is constrained by the model. So if we go back to this model,\nthis is a constraint on phi j.",
    "start": "4990250",
    "end": "4998500"
  },
  {
    "text": "So whenever you have a\nprobability distribution, a multinomial\nprobability distribution, it's not just that the\nphi i's are nonnegative,",
    "start": "4998500",
    "end": "5004810"
  },
  {
    "text": "which the constraint--\nwe're almost ignoring-- but it's that the\nphi i's equal 1. And so you couldn't, for\nexample, set your probabilities",
    "start": "5004810",
    "end": "5010650"
  },
  {
    "text": "to be 0.5 and 0.8, right? They have to add up to 1 here\nbecause it's a multinomial.",
    "start": "5010650",
    "end": "5016220"
  },
  {
    "text": "So when we do the optimization,\nwe could, for example, prefer an optimization where we\nmake all the probabilities 1,",
    "start": "5016220",
    "end": "5021550"
  },
  {
    "text": "but that would be\nan invalid setting. And so that means these phi\nj's-- we have constrained them",
    "start": "5021550",
    "end": "5027190"
  },
  {
    "text": "to equal to 1. So now the question is, when we\ndo the gradient descent, which is exactly the gradient\ndescent we did before,",
    "start": "5027190",
    "end": "5032770"
  },
  {
    "text": "where does that show up? And it shows up\nin this extra term here, which is the\nLagrange multiplier. This thing is called\na Lagrange multiplier.",
    "start": "5032770",
    "end": "5042660"
  },
  {
    "text": "The multiplier itself\nis called that, OK? And this is the constraint\nput into the normal form. If you haven't seen this before,\nit'll look quite mysterious.",
    "start": "5042660",
    "end": "5049800"
  },
  {
    "text": "But what I was trying\nto do is I'm not going to teach you the Lagrange\nmultipliers in this class. I'll put up something. But the piece is here\nthat it gets you back",
    "start": "5049800",
    "end": "5056179"
  },
  {
    "text": "to an expression which\nmakes sense in this setting. And you needed something\nto average over because these numbers sum up\nto something that looks like n.",
    "start": "5056179",
    "end": "5062830"
  },
  {
    "text": "If you just compute\nit naively, you'll get something that\ndoesn't make any sense. Yeah, please. I think the problem is that, in\na lot of sense, phi j equals 1,",
    "start": "5062830",
    "end": "5072030"
  },
  {
    "text": "then the [INAUDIBLE] sigma\n[INAUDIBLE] of phi j's. No, no, there's no sigmoid here. A sigma for summation.",
    "start": "5072030",
    "end": "5078840"
  },
  {
    "text": "No, in this setting,\nthe sigma is out here. Yeah, but the other one,\nlike since phi j equals 1--",
    "start": "5078840",
    "end": "5085610"
  },
  {
    "text": "I think what's confusing\nyou maybe is this, that these are not the same j's. No, but [INAUDIBLE].",
    "start": "5085610",
    "end": "5091370"
  },
  {
    "text": "Which line? Just says phi j i. Oh, oh, oh, I see. I see, I see, I see. Sorry, sorry, sorry.",
    "start": "5091370",
    "end": "5097429"
  },
  {
    "text": "Yeah, I was talking\nwhile I was saying. Thank you for the clarification. That was really helpful.",
    "start": "5097429",
    "end": "5102448"
  },
  {
    "text": "Apologies for that. Yes, it's this constraint here. Sorry, this is the constraint\nthat was in our head.",
    "start": "5102449",
    "end": "5110330"
  },
  {
    "text": "Yeah, and it just makes a\nmysterious reappearance here,",
    "start": "5110330",
    "end": "5117150"
  },
  {
    "text": "all right? All right, awesome.",
    "start": "5117150",
    "end": "5122270"
  },
  {
    "text": "OK, so what is the\nmessage that I want you to take away from this? Two things.",
    "start": "5122270",
    "end": "5127880"
  },
  {
    "text": "First message is GMM\nis an EM algorithm, OK?",
    "start": "5127880",
    "end": "5140520"
  },
  {
    "text": "That's really one of the\npieces that is there. And this is interesting\nbecause we're going to see-- in\nthe next lecture,",
    "start": "5140520",
    "end": "5147050"
  },
  {
    "text": "we're going to see a\ndifferent example, which is called factor analysis, where\nz has a very different form. And it's meant to constrain\nthe problem in a different way.",
    "start": "5147050",
    "end": "5155050"
  },
  {
    "text": "We went through in this lecture\na couple of different steps. We started with\nthat convexity piece so we could get an intuition for\nwhat these functions look like.",
    "start": "5155050",
    "end": "5161388"
  },
  {
    "text": "We didn't want to use convexity. We used concavity. And then we went through\nthe EM algorithm, which we formalized as\nkind of back and forth",
    "start": "5161389",
    "end": "5167150"
  },
  {
    "text": "with using these\ncurves over time. Once we had those curves,\nwhat was happening was",
    "start": "5167150",
    "end": "5172770"
  },
  {
    "text": "we would pick and\noptimize on those curves, and we were getting\nthese Q(i)'s, right? The Q(i)'s played\na starring role.",
    "start": "5172770",
    "end": "5177960"
  },
  {
    "text": "Those became our w's\nhere, and they kind of add nastiness to\nall of the equations but not a tremendous\namount of nastiness, right?",
    "start": "5177960",
    "end": "5185090"
  },
  {
    "text": "They just add little weights\nand expectations everywhere. And then we ran exactly the kind\nof standard supervised machine",
    "start": "5185090",
    "end": "5192250"
  },
  {
    "text": "learning, if you\nlike, or the stuff that you've been doing for\nMLE for the entire quarter on those properties.",
    "start": "5192250",
    "end": "5198100"
  },
  {
    "text": "Then we introduced a ton of\ntypos to keep you on your toes. No, I introduced a\nton of typos because I was talking while I was writing,\nand I shouldn't have done that.",
    "start": "5198100",
    "end": "5204170"
  },
  {
    "text": "And so then we\nsaw the two things that I cared about to highlight. One is you know\nhow to find means, and these are just\nweighted means,",
    "start": "5204170",
    "end": "5210310"
  },
  {
    "text": "and you should run\nthrough that calculation to make sure you\nknow how to do it because I'm almost\npositive we will ask you to do it at some\npoint in the near future.",
    "start": "5210310",
    "end": "5216658"
  },
  {
    "text": "And then the second thing\nthat I would tell you to do is when you have\nconstraints, you have to know how\nto optimize them.",
    "start": "5216659",
    "end": "5222150"
  },
  {
    "text": "You don't need to know\nthe general theory of how you optimize against\nnonlinear constraints, but you should review\nhow to do this when you",
    "start": "5222150",
    "end": "5228400"
  },
  {
    "text": "have something that sums to 1. It's not more\ncomplicated than what I wrote here, but make sure\nindependently you go through it",
    "start": "5228400",
    "end": "5234619"
  },
  {
    "text": "and ask questions. I'm happy to ask questions. I'm happy to point you\nto different resources.",
    "start": "5234619",
    "end": "5241118"
  },
  {
    "text": "In the next class, as I\nsaid, what we're going to see is this notion of\nfactor analysis. And that is going to\ntell us how to apply EM",
    "start": "5241119",
    "end": "5247219"
  },
  {
    "text": "to a different kind of setting,\nwhich, at first glance, will look kind of impossible\nto do without a latent",
    "start": "5247219",
    "end": "5253340"
  },
  {
    "text": "variable model, and it's a\npretty interesting scenario. And I think that's\nall I want to say.",
    "start": "5253340",
    "end": "5259639"
  },
  {
    "text": "Any last questions\nbefore we head out? I'll stick around for a\ncouple of minutes as usual. Thanks so much for your\ntime and attention.",
    "start": "5259640",
    "end": "5265500"
  }
]