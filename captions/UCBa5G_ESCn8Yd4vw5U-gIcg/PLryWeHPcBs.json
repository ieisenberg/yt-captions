[
  {
    "start": "0",
    "end": "165000"
  },
  {
    "text": " So we're now starting in\nweek three with lecture five.",
    "start": "0",
    "end": "11740"
  },
  {
    "text": "So unfortunately, the\nlast class, I guess, I really got behind\nand went a bit slowly.",
    "start": "11740",
    "end": "18400"
  },
  {
    "text": "I guess, I must just enjoy\ntalking about natural languages too much and so I\nnever really got",
    "start": "18400",
    "end": "23650"
  },
  {
    "text": "to the punchline\nof showing how you could do good things with\nthe neural dependency parser. So today for the first\npiece I'll in some sense",
    "start": "23650",
    "end": "31420"
  },
  {
    "text": "be finishing the\ncontent of last time and talk about neural dependency\nparsing, which also gives us",
    "start": "31420",
    "end": "37059"
  },
  {
    "text": "the opportunity to introduce\na simple feed forward neural net classifier.",
    "start": "37060",
    "end": "43510"
  },
  {
    "text": "That will then lead into a\nlittle bit of just background things that you need to know\nabout neural networks content",
    "start": "43510",
    "end": "50265"
  },
  {
    "text": "because the fact\nof the matter is there is a bunch\nof stuff you need to know about neural networks. And then after both\nof those things,",
    "start": "50265",
    "end": "57280"
  },
  {
    "text": "I'll get into\nwhat's really meant to be the topic of\ntoday's lecture, which is looking at language modeling\nand recurrent neural networks.",
    "start": "57280",
    "end": "66040"
  },
  {
    "text": "And that's then going to\nlead into those two things are important topics that we'll\nthen be talking about, really,",
    "start": "66040",
    "end": "74360"
  },
  {
    "text": "for the whole of\nnext week as well. So the couple of reminders\nbefore we get underway,",
    "start": "74360",
    "end": "79600"
  },
  {
    "text": "the first is that you should\nhave handed in assignment 2 before you joined this class\nand in turn assignment 3",
    "start": "79600",
    "end": "87700"
  },
  {
    "text": "is out today. And this is an\nassignment where you're going to build essentially\nthe neural dependency parser",
    "start": "87700",
    "end": "96039"
  },
  {
    "text": "that I'm just about\nto present in PyTorch. So part of the role\nof this assignment is actually to get you\nup to speed with PyTorch.",
    "start": "96040",
    "end": "104080"
  },
  {
    "text": "So this assignment is\nhighly scaffolded with lots of comments and hints\nabout what to do.",
    "start": "104080",
    "end": "111080"
  },
  {
    "text": "And so the hope is that by the\ntime you come to the end of it, you'll feel fairly familiar\nand comfortable with PyTorch.",
    "start": "111080",
    "end": "119610"
  },
  {
    "text": "Don't forget there was also a\ntutorial on PyTorch last week. If you didn't catch\nthat at the time, you might want to go back\nand look at the video.",
    "start": "119610",
    "end": "128590"
  },
  {
    "text": "Another thing to mention\nabout the assignments is that assignment 3 is\nthe last assignment where",
    "start": "128590",
    "end": "135210"
  },
  {
    "text": "our great team of TAs are\nhappy to look at your code and sort out your bugs for\nyou, so maybe take advantage",
    "start": "135210",
    "end": "143340"
  },
  {
    "text": "of that but not too much. But starting in assignment\n4 for assignments 4, 5 and the final\nproject, the TAs are",
    "start": "143340",
    "end": "150660"
  },
  {
    "text": "very happy to help in\ngeneral but it's just not going to be their job\nto be actually sorting",
    "start": "150660",
    "end": "155849"
  },
  {
    "text": "out bugs for you. You should be\nlooking at your code and discussing ideas\nand concepts and reasons",
    "start": "155850",
    "end": "162209"
  },
  {
    "text": "why things might\nnot work with them. OK, so if you remember\nwhere we were last time,",
    "start": "162210",
    "end": "169050"
  },
  {
    "start": "165000",
    "end": "342000"
  },
  {
    "text": "I'd introduced this\nidea of transition based dependency\nparsers and that these",
    "start": "169050",
    "end": "175020"
  },
  {
    "text": "were an efficient linear\ntime method for giving the syntactic structure\nof natural language text.",
    "start": "175020",
    "end": "182160"
  },
  {
    "text": "And that they worked pretty well\nbefore neural nets came along and took over NLP again.",
    "start": "182160",
    "end": "188670"
  },
  {
    "text": "But they had some disadvantages,\nand their biggest disadvantage is that like most machine\nlearning models of that time,",
    "start": "188670",
    "end": "196710"
  },
  {
    "text": "they worked with\nindicator features. So that means that you are\nspecifying some condition",
    "start": "196710",
    "end": "203849"
  },
  {
    "text": "and then checking whether it\nwas true of a configuration. So something like the word on\nthe top of the stack is good",
    "start": "203850",
    "end": "210209"
  },
  {
    "text": "and its part of\nspeech is adjective or the next word coming\nup is a personal pronoun.",
    "start": "210210",
    "end": "216720"
  },
  {
    "text": "That those are\nconditions that would be features in a\nconventional transition based",
    "start": "216720",
    "end": "224010"
  },
  {
    "text": "dependency parser. And so what are the\nproblems with doing that? Well, one problem is that\nthose features are very sparse.",
    "start": "224010",
    "end": "233550"
  },
  {
    "text": "A second problem is the\nfeatures are incomplete. Well, what I mean\nby that is depending",
    "start": "233550",
    "end": "240360"
  },
  {
    "text": "on what words and configurations\noccurred in the training data,",
    "start": "240360",
    "end": "245880"
  },
  {
    "text": "there are certain\nfeatures that will exist because you sort of\nsaw a certain word preceding",
    "start": "245880",
    "end": "251670"
  },
  {
    "text": "a verb and certain features\nthat just won't exist because that word never\noccurred before a verb",
    "start": "251670",
    "end": "257010"
  },
  {
    "text": "in the training data. But perhaps the biggest\nproblem and opportunity",
    "start": "257010",
    "end": "262079"
  },
  {
    "text": "for doing better with the\nneural dependency parser is that it turns out that in\na symbolic dependency parser,",
    "start": "262079",
    "end": "269759"
  },
  {
    "text": "computing all these features\njust turns out to actually be pretty expensive. That although the\nactual transition system",
    "start": "269760",
    "end": "276030"
  },
  {
    "text": "that I showed last time is\nfast and efficient to run, you actually have to compute\nall of these features.",
    "start": "276030",
    "end": "283830"
  },
  {
    "text": "And what you found\nwas that about 95% of the parsing time\nof one of these models",
    "start": "283830",
    "end": "289830"
  },
  {
    "text": "was spent just computing\nall of the features of every configuration.",
    "start": "289830",
    "end": "295900"
  },
  {
    "text": "So that suggests that\nperhaps we can do better with a neural\napproach where we're going to learn a dense and\ncompact feature representation.",
    "start": "295900",
    "end": "304130"
  },
  {
    "text": "And so that's what I\nwant to go through now. So this time we're\nstill going to have",
    "start": "304130",
    "end": "309370"
  },
  {
    "text": "exactly the same configuration\nof a stack and a buffer",
    "start": "309370",
    "end": "314949"
  },
  {
    "text": "and running exactly the\nsame transition sequence. Except this time, rather than\nrepresenting the configuration,",
    "start": "314950",
    "end": "322419"
  },
  {
    "text": "the stack and the buffer\nby having several million symbolic features,\nwe're instead going",
    "start": "322420",
    "end": "328000"
  },
  {
    "text": "to summarize this\nconfiguration as a dense vector of dimensionality, perhaps\napproximately 1,000.",
    "start": "328000",
    "end": "337009"
  },
  {
    "text": "And our neural approach\nis going to learn this dense compact\nfeature representation.",
    "start": "337010",
    "end": "342750"
  },
  {
    "start": "342000",
    "end": "494000"
  },
  {
    "text": "And so quite\nexplicitly, what I'm going to show you\nnow briefly and what",
    "start": "342750",
    "end": "348199"
  },
  {
    "text": "you're going to\nimplement is essentially the neural dependency\nparser that was developed",
    "start": "348200",
    "end": "354710"
  },
  {
    "text": "by Danqi Chen in 2014. And to skip to the\nadvertisement right",
    "start": "354710",
    "end": "361250"
  },
  {
    "text": "at the beginning as to\nhow this works so well, these are the\nresults that you got",
    "start": "361250",
    "end": "367190"
  },
  {
    "text": "from it using the measures that\nI introduced at the last time. The unlabeled attachments score,\nwhether you attach dependencies",
    "start": "367190",
    "end": "374330"
  },
  {
    "text": "correctly to the right word and\nthe labeled attachment score as to whether you also get the\ntype of grammatical relation",
    "start": "374330",
    "end": "382460"
  },
  {
    "text": "of that dependency correct. And so essentially, this\nChen and Manning parser",
    "start": "382460",
    "end": "388940"
  },
  {
    "text": "gave a natural\nversion of something like a transition\nbased dependency parser",
    "start": "388940",
    "end": "395270"
  },
  {
    "text": "like MaltParser in yellow. And the interesting\nthing was that taking advantage of a\nneural classifier in ways",
    "start": "395270",
    "end": "403280"
  },
  {
    "text": "that I'm about to explain that\nthat could produce something that was about 2% more accurate\nthan the symbolic dependency",
    "start": "403280",
    "end": "412039"
  },
  {
    "text": "parser. And because of the\nfact that it's not doing all of the symbolic\nfeature computation",
    "start": "412040",
    "end": "418310"
  },
  {
    "text": "despite the fact that you might\nthink at first that there's a lot of real number math\nand matrix vector multiplies",
    "start": "418310",
    "end": "425120"
  },
  {
    "text": "in a neural dependency\nparser, it actually ran noticeably faster than\nthe symbolic dependency parser",
    "start": "425120",
    "end": "431510"
  },
  {
    "text": "because it didn't have all\nof feature computation. The other major\napproach to dependency",
    "start": "431510",
    "end": "437810"
  },
  {
    "text": "parsing that I'm\nalso showing here and I'll get back to at the end\nis what's referred to as graph",
    "start": "437810",
    "end": "443900"
  },
  {
    "text": "based dependency parsing. And so that's a different\napproach to dependency parsing.",
    "start": "443900",
    "end": "448970"
  },
  {
    "text": "And so these are two symbolic\ngraph based dependency parsers. And in the pre-neural\nworld, they",
    "start": "448970",
    "end": "456470"
  },
  {
    "text": "were somewhat more accurate than\nthe transition based parsers as you could see.",
    "start": "456470",
    "end": "461790"
  },
  {
    "text": "But on the other hand, they\nwere close to two orders of magnitude slower.",
    "start": "461790",
    "end": "466950"
  },
  {
    "text": "And so essentially, with\nthe Chen and Manning parser, we were able to provide\nsomething that was basically",
    "start": "466950",
    "end": "472700"
  },
  {
    "text": "as accurate as the best graph\nbased dependency parsers which were the best dependency\nparsers while operating",
    "start": "472700",
    "end": "479300"
  },
  {
    "text": "about two orders of\nmagnitude more quickly. So how did we do it? It was actually a very\nstraightforward implementation,",
    "start": "479300",
    "end": "489390"
  },
  {
    "text": "which is part of what makes it\ngreat for doing assignment 3.",
    "start": "489390",
    "end": "494930"
  },
  {
    "start": "494000",
    "end": "679000"
  },
  {
    "text": "But this is how we\ndid it and we got win. So the first win which\nis what we've already talked about extensively\nstarting in week one,",
    "start": "494930",
    "end": "503030"
  },
  {
    "text": "is to make use of\ndistributed representations. So we represent each\nword as a word embedding.",
    "start": "503030",
    "end": "509490"
  },
  {
    "text": "And you've had a lot of\nexperience with that already. And so that means\nwhen words weren't",
    "start": "509490",
    "end": "516110"
  },
  {
    "text": "seen in a particular\nconfiguration, we still know what\nthey're like because we'll",
    "start": "516110",
    "end": "521360"
  },
  {
    "text": "have seen similar words in\nthe correct configuration.",
    "start": "521360",
    "end": "526370"
  },
  {
    "text": "But we don't stop only\nwith word embeddings. The other things\nthat are central",
    "start": "526370",
    "end": "531920"
  },
  {
    "text": "to our dependency\nparser, are the parts of speech of words and\nthe dependency labels.",
    "start": "531920",
    "end": "538130"
  },
  {
    "text": "And so what we decided to do\nis that although those are much smaller sets, so the dependency\nlabels are about 40 in number",
    "start": "538130",
    "end": "547610"
  },
  {
    "text": "and the parts of speech are of\naround that order of magnitude, sometimes less,\nsometimes more, that even",
    "start": "547610",
    "end": "553700"
  },
  {
    "text": "within those sets\nof categories, there are ones that are\nvery strongly related.",
    "start": "553700",
    "end": "559220"
  },
  {
    "text": "So we also adopted distributed\nrepresentations for them.",
    "start": "559220",
    "end": "564329"
  },
  {
    "text": "So for example,\nthere might be parts of speech for singular\nnouns and plural nouns. And basically, most of the\ntime, they behave similarly",
    "start": "564330",
    "end": "572030"
  },
  {
    "text": "and there are\nadjectival modifiers and numerical modifiers\nthat these are just numbers like 3, 4, or 5.",
    "start": "572030",
    "end": "578690"
  },
  {
    "text": "And again, a lot\nof the time they behave the same that you have\nboth three cows and brown cows.",
    "start": "578690",
    "end": "585530"
  },
  {
    "text": " OK, so everything is going to be\nrepresented in the distributed",
    "start": "585530",
    "end": "591770"
  },
  {
    "text": "representation. So at that point, we have\nexactly the same kind of configuration where we\nhave our stack, our buffer,",
    "start": "591770",
    "end": "600950"
  },
  {
    "text": "and we've started\nto build some arcs. And so the\nclassification decisions",
    "start": "600950",
    "end": "607670"
  },
  {
    "text": "of the next transition\nare going to be made out of a few elements of\nthis configuration,",
    "start": "607670",
    "end": "612774"
  },
  {
    "text": "so we're looking\nat the top thing on the stack, the\nsecond on the stack, the first word on the buffer.",
    "start": "612775",
    "end": "619440"
  },
  {
    "text": "And then we actually added\nin some additional features that are then-- to the extent that we've\nalready built arcs for words",
    "start": "619440",
    "end": "626480"
  },
  {
    "text": "on the stack that\nwe can be looking at the dependents on the\nleft and right of those words",
    "start": "626480",
    "end": "632750"
  },
  {
    "text": "that are on the stack that are\nalready in the sets of arcs. And so for each of those\nthings, there is a word,",
    "start": "632750",
    "end": "640550"
  },
  {
    "text": "there is a part of speech. And for some of them there is\na dependency where it's already",
    "start": "640550",
    "end": "648440"
  },
  {
    "text": "connected up to something else. So for example, the\nleft corner of S2",
    "start": "648440",
    "end": "653450"
  },
  {
    "text": "here has an in\nsub-dependency back to the second\nthing on the stack.",
    "start": "653450",
    "end": "658590"
  },
  {
    "text": "So we can take these\nelements of the configuration and look up the\nembedding of each one.",
    "start": "658590",
    "end": "665600"
  },
  {
    "text": "So we have word embeddings,\npart of speech embeddings, and dependency embeddings\nand just concatenate them all together, kind of like\nwe did before with the window",
    "start": "665600",
    "end": "673670"
  },
  {
    "text": "classifier and that will give\nus a neural representation of the configuration.",
    "start": "673670",
    "end": "679750"
  },
  {
    "start": "679000",
    "end": "891000"
  },
  {
    "text": "Now, there's a\nsecond reason why we can hope to win by using a deep\nlearning classifier to predict",
    "start": "679750",
    "end": "686110"
  },
  {
    "text": "the next transition. And we haven't really\nsaid much about that yet. So I just wanted to detour\nand say a little bit more",
    "start": "686110",
    "end": "692890"
  },
  {
    "text": "about that. So the simplest\nkind of classifier that's close to what we've been\ntalking about in neural models",
    "start": "692890",
    "end": "702390"
  },
  {
    "text": "is a softmax classifier. So that if we have d dimensional\nvectors x and we have",
    "start": "702390",
    "end": "709470"
  },
  {
    "text": "y classes to assign\nthings to, also y",
    "start": "709470",
    "end": "715019"
  },
  {
    "text": "is an element of a set of c\nclasses to assign things to,",
    "start": "715020",
    "end": "721170"
  },
  {
    "text": "then we can build a softmax\nclassifier using the softmax distribution that\nwe've seen before,",
    "start": "721170",
    "end": "727020"
  },
  {
    "text": "where we decide\nthe classes based on having a weight\nmatrix that's c by d.",
    "start": "727020",
    "end": "733950"
  },
  {
    "text": "And we train on supervised data\nthe values of this W, weight matrix to minimize our\nnegative log likelihood",
    "start": "733950",
    "end": "742890"
  },
  {
    "text": "loss that we've seen before. A loss is also commonly referred\nto as cross-entropy loss,",
    "start": "742890",
    "end": "748160"
  },
  {
    "text": "a term that you'll see in\nPyTorch among other places. So that is a straightforward\nmachine learning classifier.",
    "start": "748160",
    "end": "756029"
  },
  {
    "text": "And if you've done 229, you've\nseen softmax classifiers.",
    "start": "756030",
    "end": "761040"
  },
  {
    "text": "But a simple softmax\nclassifier like this shares with most traditional\nmachine learning",
    "start": "761040",
    "end": "768150"
  },
  {
    "text": "classifiers some\nmodels that include Naive Bayes model,\nsupport vector machines, logistic regression.",
    "start": "768150",
    "end": "774839"
  },
  {
    "text": "That at the end of\nthe day, they're not very powerful\nclassifiers, they're classifiers that only give\nlinear decision boundaries.",
    "start": "774840",
    "end": "783279"
  },
  {
    "text": "And so this can\nbe quite limiting. So if you have a\ndifficult problem like the one I'm indicating in\nthe picture in the bottom left,",
    "start": "783280",
    "end": "791130"
  },
  {
    "text": "well, there's just no way you\ncan divide the green points from the red points by simply\ndrawing a straight line.",
    "start": "791130",
    "end": "798430"
  },
  {
    "text": "So you're going to have a\nquite imperfect classifier. So the second big win\nof neural classifiers",
    "start": "798430",
    "end": "805649"
  },
  {
    "text": "is that they can be much\nmore powerful because they can provide nonlinear\nclassification.",
    "start": "805650",
    "end": "812070"
  },
  {
    "text": "So rather than only being\nable to do something like in the left\npicture, we can come up with classifiers\nthat do something",
    "start": "812070",
    "end": "818970"
  },
  {
    "text": "like in the right picture\nand therefore can separate the green and the red points.",
    "start": "818970",
    "end": "825180"
  },
  {
    "text": "As an aside, these pictures I've\ntaken from Andrej Karpathy's ConvNetJS software, which\nis kind of a fun little tool",
    "start": "825180",
    "end": "832470"
  },
  {
    "text": "to play around with if you've\ngot a bit of spare time. And so there's something\nsubtle going on here",
    "start": "832470",
    "end": "839790"
  },
  {
    "text": "is because our more powerful\nneural net classifiers",
    "start": "839790",
    "end": "845130"
  },
  {
    "text": "at the end of the day, what\nthey have at the top of them is the softmax layer.",
    "start": "845130",
    "end": "850560"
  },
  {
    "text": "And so this softmax layer is\nindeed a linear classifier",
    "start": "850560",
    "end": "855779"
  },
  {
    "text": "and it's still a\nlinear classifier. But what they have below that\nis other layers of neural net.",
    "start": "855780",
    "end": "862950"
  },
  {
    "text": "And so effectively what happens\nis that the classification",
    "start": "862950",
    "end": "868080"
  },
  {
    "text": "decisions are linear as\nfar as the top softmax is concerned but nonlinear in the\noriginal representation space.",
    "start": "868080",
    "end": "876510"
  },
  {
    "text": "So precisely what a neural net\ncan do is warp the space around",
    "start": "876510",
    "end": "881700"
  },
  {
    "text": "and move the representation\nof data points to provide something that\nat the end of the day",
    "start": "881700",
    "end": "887639"
  },
  {
    "text": "can be classified by\na linear classifier. And so that's what a\nsimple feed forward",
    "start": "887640",
    "end": "894020"
  },
  {
    "start": "891000",
    "end": "1035000"
  },
  {
    "text": "neural network multi-class\nclassifier does. So it starts with an\ninput representation.",
    "start": "894020",
    "end": "900890"
  },
  {
    "text": "So these are some dense\nrepresentation of the input. It puts it through a hidden\nlayer h with a matrix multiply",
    "start": "900890",
    "end": "911500"
  },
  {
    "text": "followed by a non-linearity. So that matrix multiply can\ntransform the space and map",
    "start": "911500",
    "end": "916750"
  },
  {
    "text": "things around. And so then the\noutput of that, we can then put into\na softmax layer",
    "start": "916750",
    "end": "922839"
  },
  {
    "text": "and get out softmax\nprobabilities from which we make our\nclassification decisions.",
    "start": "922840",
    "end": "929920"
  },
  {
    "text": "And to the extent that our\nprobabilities don't assign one to the correct class, we\nthen get some log loss",
    "start": "929920",
    "end": "936970"
  },
  {
    "text": "or cross entropy\nerror, which we back propagate towards the parameters\nand embeddings of our model.",
    "start": "936970",
    "end": "944300"
  },
  {
    "text": "And as the learning that\ngoes on via back propagation, we increasingly will\nlearn parameters",
    "start": "944300",
    "end": "952180"
  },
  {
    "text": "of this hidden layer of\nthe model, which learn to re-represent the input.",
    "start": "952180",
    "end": "957250"
  },
  {
    "text": "They move the inputs around in\nan intermediate hidden vector space so it can be\neasily classified",
    "start": "957250",
    "end": "964569"
  },
  {
    "text": "with what at the end of the\nday is the linear softmax. So this is basically the\nwhole of a simple feed forward",
    "start": "964570",
    "end": "973120"
  },
  {
    "text": "neural network\nmulti-class classifier. But and if we had something\nlike a visual signal,",
    "start": "973120",
    "end": "980140"
  },
  {
    "text": "we'll just sort of feed\nstraight in here real numbers and we'll be done. But normally with human\nlanguage material,",
    "start": "980140",
    "end": "987820"
  },
  {
    "text": "we actually effectively\nhave one more layer that we're feeding in\nbefore that because really",
    "start": "987820",
    "end": "993730"
  },
  {
    "text": "below this dense input\nlayer we actually have one hot vectors for\nwhat words or parts of speech",
    "start": "993730",
    "end": "1000660"
  },
  {
    "text": "were involved. And then we're doing\na lookup process, which you can think of as\none more matrix multiply",
    "start": "1000660",
    "end": "1007080"
  },
  {
    "text": "to convert the one hot features\ninto our dense input layer.",
    "start": "1007080",
    "end": "1012720"
  },
  {
    "text": "OK, in my picture here, the one\nother thing that's different is I've introduced a\ndifferent non-linearity",
    "start": "1012720",
    "end": "1018270"
  },
  {
    "text": "in the hidden layer, which\nis a rectified linear unit. And that's what we'll be\nusing in our neural dependency",
    "start": "1018270",
    "end": "1024869"
  },
  {
    "text": "parses. It looks like the picture\nin the bottom right and I'll come back to\nthat in a few minutes.",
    "start": "1024869",
    "end": "1031079"
  },
  {
    "text": "That's one of the extra neural\nnet things to talk about.",
    "start": "1031079",
    "end": "1036260"
  },
  {
    "start": "1035000",
    "end": "1137000"
  },
  {
    "text": "OK, So our neural net dependency\nparser model architecture is essentially exactly\nthat but applied",
    "start": "1036260",
    "end": "1045020"
  },
  {
    "text": "to the configuration\nof our transition based dependency parser.",
    "start": "1045020",
    "end": "1050629"
  },
  {
    "text": "So based on our transition\nbased dependency parser configuration, we\nconstruct an input layer",
    "start": "1050630",
    "end": "1058460"
  },
  {
    "text": "embedding by looking\nup the various elements as I discussed previously.",
    "start": "1058460",
    "end": "1063540"
  },
  {
    "text": "And then we feed it through\nthis hidden layer to the softmax",
    "start": "1063540",
    "end": "1069050"
  },
  {
    "text": "layer to get probabilities\nout of which we can choose what the next action is.",
    "start": "1069050",
    "end": "1076009"
  },
  {
    "text": "And it's no more\ncomplicated than that. But what we found is that\njust simply in some sense",
    "start": "1076010",
    "end": "1087790"
  },
  {
    "text": "using the simplest\nkind of feed forward, neural classifier could\nprovide a very accurate",
    "start": "1087790",
    "end": "1098080"
  },
  {
    "text": "dependency parser\nthat determines the structure of\nsentences supporting",
    "start": "1098080",
    "end": "1103540"
  },
  {
    "text": "meaning interpretation\nthe kind of way that I suggested last time. Indeed despite the fact that it\nwas quite simple architecture,",
    "start": "1103540",
    "end": "1113350"
  },
  {
    "text": "in 2014 this was the first\nsuccessful neural dependency parser.",
    "start": "1113350",
    "end": "1118690"
  },
  {
    "text": "And the dense representations\nespecially, but also partly the non-linearity\nof the classifier",
    "start": "1118690",
    "end": "1126130"
  },
  {
    "text": "gave us this good result\nthat it could both outperform symbolic parsers in\nterms of accuracy",
    "start": "1126130",
    "end": "1132970"
  },
  {
    "text": "and it could outperform\nthem in terms of speed. ",
    "start": "1132970",
    "end": "1138259"
  },
  {
    "start": "1137000",
    "end": "1206000"
  },
  {
    "text": "So that was 2014. Just quickly here a\ncouple more slides",
    "start": "1138260",
    "end": "1143360"
  },
  {
    "text": "on what's happened since then. So lots of people got\nexcited by the success",
    "start": "1143360",
    "end": "1149690"
  },
  {
    "text": "of this neural dependency\nparser And a number of people, particularly at Google,\nthen set about building",
    "start": "1149690",
    "end": "1156440"
  },
  {
    "text": "a bigger fancier\ntransition-based neural dependency parser. So they explored\nbigger, deeper networks.",
    "start": "1156440",
    "end": "1163220"
  },
  {
    "text": "There's no reason to only\nhave one hidden layer. You can have two hidden layers. You can do beam search that I\nbriefly mentioned last time.",
    "start": "1163220",
    "end": "1171457"
  },
  {
    "text": "Another thing that I'm not\ngoing to talk about now is adding conditional\nrandom field style inference",
    "start": "1171457",
    "end": "1177590"
  },
  {
    "text": "over the decision sequences. And that then led\nin 2016 for a model",
    "start": "1177590",
    "end": "1183590"
  },
  {
    "text": "that they called Parsey\nMcParseface which is hard to say with\na straight face.",
    "start": "1183590",
    "end": "1189770"
  },
  {
    "text": "Which was then about 2 and 1/2,\n3% more accurate than the model",
    "start": "1189770",
    "end": "1195350"
  },
  {
    "text": "that we had produced. But still in basically\nthe same family of transition-based\nparser with the neural net",
    "start": "1195350",
    "end": "1202730"
  },
  {
    "text": "classifier to choose\nthe next transition. ",
    "start": "1202730",
    "end": "1208720"
  },
  {
    "start": "1206000",
    "end": "1341000"
  },
  {
    "text": "The alternative to\ntransition-based parsers is graph-based\ndependency parsers.",
    "start": "1208720",
    "end": "1214030"
  },
  {
    "text": "And for a graph-based dependency\nparser, what you're doing is effectively considering\nevery pair of words",
    "start": "1214030",
    "end": "1221950"
  },
  {
    "text": "and considering a word\nas a dependent of root. And you're coming\nup with a score",
    "start": "1221950",
    "end": "1227320"
  },
  {
    "text": "as to how likely is that\nbig is a dependent of root",
    "start": "1227320",
    "end": "1232450"
  },
  {
    "text": "or how likely is big to\nbe a dependent of cat. And similarly for\nevery other word. For the word sat,\nhow likely is it",
    "start": "1232450",
    "end": "1241330"
  },
  {
    "text": "to be a dependent of root or\na dependent of the, et cetera. Well, to do that well, you need\nto know more than just what",
    "start": "1241330",
    "end": "1249460"
  },
  {
    "text": "the two words involved are. And so what you want to do\nis understand the context.",
    "start": "1249460",
    "end": "1256549"
  },
  {
    "text": "So you want to have\nan understanding of the context of big,\nwhat's to the left or what's to the right of it to\nunderstand how you might get up",
    "start": "1256550",
    "end": "1264370"
  },
  {
    "text": "into the dependency\nrepresentations of the sentence.",
    "start": "1264370",
    "end": "1269600"
  },
  {
    "text": "And so while there\nhave been previous work in graph-based\ndependency parsing, like the MST parser I showed\non the earlier results slide,",
    "start": "1269600",
    "end": "1278120"
  },
  {
    "text": "it seemed appealing\nthat we could come up with a much better\nrepresentation of context",
    "start": "1278120",
    "end": "1284000"
  },
  {
    "text": "using neural nets\nthat look at context. And how we do that\nis actually what I'll be talking about in\nthe end part of the lecture.",
    "start": "1284000",
    "end": "1290930"
  },
  {
    "text": "And so at Stanford, we\nbecame interested in trying",
    "start": "1290930",
    "end": "1296150"
  },
  {
    "text": "to work out how to come up with\na better graph-based dependency parser using context.",
    "start": "1296150",
    "end": "1301780"
  },
  {
    "text": "Sorry, I forgot this\nwas showing that. If we can score each\npairwise dependency, we can simply\nchoose the best one.",
    "start": "1301780",
    "end": "1309090"
  },
  {
    "text": "So we can say probably\nbig is a dependent of cat. And your first\napproximation, we're",
    "start": "1309090",
    "end": "1315920"
  },
  {
    "text": "going to want to\nchoose for each word that it is a dependent\nof the word that seems",
    "start": "1315920",
    "end": "1322370"
  },
  {
    "text": "most likely to be a dependent. But we want to do that\nwith some constraints, because we want to\nget out something",
    "start": "1322370",
    "end": "1328040"
  },
  {
    "text": "that is the tree with a single\nroot as I discussed last time. And you can do that by making\nuse of a minimum spanning tree",
    "start": "1328040",
    "end": "1335660"
  },
  {
    "text": "algorithm that uses\nthese scores of how likely different\ndependencies are.",
    "start": "1335660",
    "end": "1341830"
  },
  {
    "start": "1341000",
    "end": "1409000"
  },
  {
    "text": "OK. So then in 2017, another\nstudent, Tim Dozat",
    "start": "1341830",
    "end": "1347000"
  },
  {
    "text": "and me then worked\non saying, well, can we now also build\na much better neural",
    "start": "1347000",
    "end": "1354020"
  },
  {
    "text": "graph-based dependency parser? And we developed a novel method\nfor scoring dependency parsers",
    "start": "1354020",
    "end": "1363059"
  },
  {
    "text": "in a graph-based\nmodel, which I'm not going to get into the\ndetails of right now.",
    "start": "1363060",
    "end": "1368120"
  },
  {
    "text": "But' that also had\na very nice result. Because getting back to\ngraph-based parsing we could",
    "start": "1368120",
    "end": "1373850"
  },
  {
    "text": "then build a graph-based parser\nthat performed about a percent better than the best of the\nGoogle transition-based neural",
    "start": "1373850",
    "end": "1381830"
  },
  {
    "text": "dependency parses. But I should point out\nthat this is a mixed win.",
    "start": "1381830",
    "end": "1386930"
  },
  {
    "text": "Because although its\naccuracy is better, these graph-based\nparses are just",
    "start": "1386930",
    "end": "1392660"
  },
  {
    "text": "n squared in performance\nrather than linear time. So kind of like the earlier\nresults they showed.",
    "start": "1392660",
    "end": "1398930"
  },
  {
    "text": "They don't operate nearly\nas quickly when you're wanting to parse\nlarge amounts of text",
    "start": "1398930",
    "end": "1405410"
  },
  {
    "text": "with complex long sentences. OK. So that's everything you need\nto know about dependency parsers",
    "start": "1405410",
    "end": "1413250"
  },
  {
    "text": "and to do assignment 3. So grab it this evening\nand start to work.",
    "start": "1413250",
    "end": "1418560"
  },
  {
    "text": "But I did want to sort of before\ngoing on to the next topic just mention a few more\nthings about neural networks",
    "start": "1418560",
    "end": "1426770"
  },
  {
    "text": "since some of you know\nthis well already. Some of you have\nseen less of it, but you know there just\nare a bunch of things",
    "start": "1426770",
    "end": "1433560"
  },
  {
    "text": "you have to be aware of for\nbuilding neural networks. Now again for assignment\n3, essentially we",
    "start": "1433560",
    "end": "1440700"
  },
  {
    "text": "give you everything. And if you follow the recipe,\nyour parser should work well.",
    "start": "1440700",
    "end": "1446260"
  },
  {
    "text": "But what you should\nhave minimally do is actually look carefully\nat some of the things",
    "start": "1446260",
    "end": "1454170"
  },
  {
    "text": "that this parser does,\nwhich is questions like, how do we initialize our\nmatrices of our neural network?",
    "start": "1454170",
    "end": "1463809"
  },
  {
    "text": "What kind of\noptimizers do we use? And things like that. Because these are all\nimportant decisions.",
    "start": "1463810",
    "end": "1471190"
  },
  {
    "text": "And so I wanted to say just\na few words about that. OK. So the first thing that we\nhaven't discussed at all",
    "start": "1471190",
    "end": "1479010"
  },
  {
    "start": "1475000",
    "end": "1679000"
  },
  {
    "text": "is the concept of\nregularization. So when we're building\nthese neural nets,",
    "start": "1479010",
    "end": "1485160"
  },
  {
    "text": "we're now building models with\na huge number of parameters.",
    "start": "1485160",
    "end": "1490480"
  },
  {
    "text": "So essentially just about\nall neural net models that work well, actually\ntheir full loss function",
    "start": "1490480",
    "end": "1500820"
  },
  {
    "text": "is a regularized loss function. So for this loss\nfunction here of J, well,",
    "start": "1500820",
    "end": "1508560"
  },
  {
    "text": "this part here is\nthe part that we've seen before where we're\nusing the softmax classifier",
    "start": "1508560",
    "end": "1516480"
  },
  {
    "text": "and then taking a\nnegative log likelihood loss, which we're then averaging\nover the different examples.",
    "start": "1516480",
    "end": "1521549"
  },
  {
    "text": "But actually, we then\nstick on the end of it this regularization term.",
    "start": "1521550",
    "end": "1527550"
  },
  {
    "text": "And so this\nregularization term sums the square of every\nparameter in the model.",
    "start": "1527550",
    "end": "1534910"
  },
  {
    "text": "And so what that\neffectively says is you only want to make parameters\nnon-zero if they're",
    "start": "1534910",
    "end": "1544529"
  },
  {
    "text": "really useful, right? So to the extent that the\nparameters don't help much,",
    "start": "1544530",
    "end": "1549600"
  },
  {
    "text": "you're just being penalized\nhere by making them non-zero. But to the extent that\nthe parameters do help,",
    "start": "1549600",
    "end": "1556650"
  },
  {
    "text": "you'll gain in your\nestimation of likelihood. And therefore, it's OK\nfor them to be non-zero.",
    "start": "1556650",
    "end": "1562440"
  },
  {
    "text": "In particular, notice that this\npenalty is assessed only once per parameter.",
    "start": "1562440",
    "end": "1568200"
  },
  {
    "text": "It's not being assessed\nseparately for each example, OK? And having this kind\nof regularization",
    "start": "1568200",
    "end": "1575950"
  },
  {
    "text": "is essential to build neural\nnet models that regularize well.",
    "start": "1575950",
    "end": "1581409"
  },
  {
    "text": "So the classic problem is\nreferred to as overfitting. And what overfitting\nmeans is that if you",
    "start": "1581410",
    "end": "1587649"
  },
  {
    "text": "have a particular\ntraining data set and you start training your\nmodel, your error",
    "start": "1587650",
    "end": "1593860"
  },
  {
    "text": "will go down because you'll\nshift the parameters so they better predict the\ncorrect answer for data",
    "start": "1593860",
    "end": "1601840"
  },
  {
    "text": "points in the model. And you can keep on doing\nthat and it will keep",
    "start": "1601840",
    "end": "1606940"
  },
  {
    "text": "on reducing your error rate. But if you then look at your\npartially trained classifier",
    "start": "1606940",
    "end": "1613630"
  },
  {
    "text": "and say, how well does\nthis classifier classify",
    "start": "1613630",
    "end": "1618640"
  },
  {
    "text": "independent data, different\ntest data that you weren't training the model on?",
    "start": "1618640",
    "end": "1624070"
  },
  {
    "text": "What you'll find is up\nuntil a certain point, you'll get better at classifying\nindependent test examples",
    "start": "1624070",
    "end": "1631690"
  },
  {
    "text": "as well. And after that, commonly\nwhat will happen is you'll actually\nstart to get worse",
    "start": "1631690",
    "end": "1638590"
  },
  {
    "text": "of classifying\nindependent test examples, even though you're continuing\nto get better at predicting",
    "start": "1638590",
    "end": "1644650"
  },
  {
    "text": "the training examples. And so this was then referred\nto as your overfitting",
    "start": "1644650",
    "end": "1650140"
  },
  {
    "text": "the training\nexamples, that you're fiddling the\nparameters of the model so they're really good at\npredicting the training",
    "start": "1650140",
    "end": "1656140"
  },
  {
    "text": "examples, which aren't\nuseful things that can then predict on independent examples\nthat you'd come to at run time.",
    "start": "1656140",
    "end": "1667340"
  },
  {
    "text": "OK. That classic view\nof regularization is sort of actually\noutmoded and wrong",
    "start": "1667340",
    "end": "1675580"
  },
  {
    "text": "for modern neural networks. So the right way to\nthink of it for the kind",
    "start": "1675580",
    "end": "1682899"
  },
  {
    "start": "1679000",
    "end": "1818000"
  },
  {
    "text": "of modern big neural\nnetworks that we build is that overfitting on the\ntraining data isn't a problem.",
    "start": "1682900",
    "end": "1692350"
  },
  {
    "text": "But nevertheless, you need\nregularization to make sure",
    "start": "1692350",
    "end": "1698200"
  },
  {
    "text": "that your models generalize\nwell to independent test data.",
    "start": "1698200",
    "end": "1703519"
  },
  {
    "text": "So what you would\nlike is for your graph not to look like this example\nwith test error starting",
    "start": "1703520",
    "end": "1711190"
  },
  {
    "text": "to head up. You'd like to have it\nat worst case flat line",
    "start": "1711190",
    "end": "1717190"
  },
  {
    "text": "and best case still\nbe gradually dropping. It will always be higher\nthan the training error,",
    "start": "1717190",
    "end": "1723080"
  },
  {
    "text": "but it's not actually showing\na failure to generalize. So when we train big\nneural nets these days,",
    "start": "1723080",
    "end": "1732640"
  },
  {
    "text": "our big neural nets always\noverfit on the training data.",
    "start": "1732640",
    "end": "1739180"
  },
  {
    "text": "They hugely overfit\non the training data. In fact, in many\ncircumstances our neural nets",
    "start": "1739180",
    "end": "1744760"
  },
  {
    "text": "have so many parameters\nthat you can continue to train them on\nthe training data",
    "start": "1744760",
    "end": "1749889"
  },
  {
    "text": "until the error on the\ntraining data is zero. They get every\nsingle example right because they can just\nmemorize enough stuff about it",
    "start": "1749890",
    "end": "1757360"
  },
  {
    "text": "to predict the right answer. But in general, providing the\nmodels are regularized well,",
    "start": "1757360",
    "end": "1763630"
  },
  {
    "text": "those models will still also\ngeneralize well and predict well in independent data.",
    "start": "1763630",
    "end": "1770500"
  },
  {
    "text": "And so for part of what\nwe want to do for that is to work out how\nmuch to regularize.",
    "start": "1770500",
    "end": "1776590"
  },
  {
    "text": "And so this Lambda\nparameter here is the strength\nof regularization.",
    "start": "1776590",
    "end": "1781780"
  },
  {
    "text": "So if you're making\nthat Lambda number big, you're getting more\nregularization.",
    "start": "1781780",
    "end": "1787299"
  },
  {
    "text": "And if you make it small,\nyou're getting less. And you don't want to have\nit be too big or else you won't fit the data well.",
    "start": "1787300",
    "end": "1793510"
  },
  {
    "text": "And you don't want\nto be too small, or else you have the problem\nthat you don't generalize well.",
    "start": "1793510",
    "end": "1801500"
  },
  {
    "text": "OK. So this is classic\nL2 regularization and it's a starting point.",
    "start": "1801500",
    "end": "1806830"
  },
  {
    "text": "That our big neural nets\nare sufficiently complex and have sufficiently\nmany parameters that essentially L2\nregularization doesn't cut it.",
    "start": "1806830",
    "end": "1815710"
  },
  {
    "text": "So the next thing that\nyou should know about and is a very\nstandard good feature",
    "start": "1815710",
    "end": "1821920"
  },
  {
    "start": "1818000",
    "end": "1977000"
  },
  {
    "text": "for building neural nets is\na technique called dropout.",
    "start": "1821920",
    "end": "1826990"
  },
  {
    "text": "So dropout is\ngenerally introduced",
    "start": "1826990",
    "end": "1832120"
  },
  {
    "text": "as a sort of a\nslightly funny process that you do when training to\navoid feature co-adaptation.",
    "start": "1832120",
    "end": "1840890"
  },
  {
    "text": "So in dropout what you do\nis that the time that you're training your model,\nthat for each instance",
    "start": "1840890",
    "end": "1850960"
  },
  {
    "text": "or for each batch\nin your training, then for each\nneuron in the model,",
    "start": "1850960",
    "end": "1858430"
  },
  {
    "text": "you drop 50% of its inputs. You just treat them as zero. And so that you can do\nby sort of zeroing out",
    "start": "1858430",
    "end": "1866110"
  },
  {
    "text": "elements of the sort of layers.",
    "start": "1866110",
    "end": "1872070"
  },
  {
    "text": "And then at test time, you don't\ndrop any of the model weight.",
    "start": "1872070",
    "end": "1877110"
  },
  {
    "text": "You keep them all. But actually you have\nall the model weights because you're now keeping\ntwice as many things",
    "start": "1877110",
    "end": "1883500"
  },
  {
    "text": "as you used at training data. And so effectively,\nthat little recipe",
    "start": "1883500",
    "end": "1890490"
  },
  {
    "text": "prevents what's called\nfeature co-adaptation. So you can't have\nfeatures that are only",
    "start": "1890490",
    "end": "1899880"
  },
  {
    "text": "useful in the presence of\nparticular other features, because the model\ncan't guarantee",
    "start": "1899880",
    "end": "1905700"
  },
  {
    "text": "which features are going to be\npresent for different examples. Because different features\nare being randomly dropped",
    "start": "1905700",
    "end": "1911250"
  },
  {
    "text": "all of the time. And so effectively\ndropout gives you a kind of a middle ground\nbetween Naive Bayes",
    "start": "1911250",
    "end": "1918090"
  },
  {
    "text": "and a logistic regression model. In Naive Bayes models, all the\nweights are set independently.",
    "start": "1918090",
    "end": "1923460"
  },
  {
    "text": "In a logistic regression\nmodel, all the weights are set in the context\nof all the others. And here you are aware\nof other weights,",
    "start": "1923460",
    "end": "1930720"
  },
  {
    "text": "but they can randomly\ndisappear from you. It's also related to ensemble\nmodels like model bagging,",
    "start": "1930720",
    "end": "1937200"
  },
  {
    "text": "because you're using different\nsubsets of the features every time. But after all of\nthose explanations,",
    "start": "1937200",
    "end": "1946030"
  },
  {
    "text": "there's actually\nanother way of thinking about dropout which was actually\ndeveloped here at Stanford,",
    "start": "1946030",
    "end": "1951710"
  },
  {
    "text": "this paper by Percy\nLiang and students, which is to argue that really\nwhat dropout gives",
    "start": "1951710",
    "end": "1958480"
  },
  {
    "text": "you is a strong\nregularizer that isn't a uniform regularizer like L2\nthat regularizes everything",
    "start": "1958480",
    "end": "1965740"
  },
  {
    "text": "with an L2 loss, but\ncan learn a feature dependent regularization. And so that dropout\nhas just emerged",
    "start": "1965740",
    "end": "1972400"
  },
  {
    "text": "as in general the best way to do\nregularization for neural nets.",
    "start": "1972400",
    "end": "1977810"
  },
  {
    "text": "I think you've already\nseen and heard this one, but just have it\non my slides once.",
    "start": "1977810",
    "end": "1985360"
  },
  {
    "text": "If you want to have your\nneural networks go fast, it's really essential\nthat you make",
    "start": "1985360",
    "end": "1991210"
  },
  {
    "text": "use of vectors,\nmatrices, tensors and you don't do things with for loops.",
    "start": "1991210",
    "end": "1996860"
  },
  {
    "text": "So here's a teeny\nexample, where I'm using timeit, which is a useful\nthing that you can use too",
    "start": "1996860",
    "end": "2003914"
  },
  {
    "text": "to see how fast\nyour neural nets run in different ways of writing it. And so when I'm doing\nthese dot products here,",
    "start": "2003915",
    "end": "2016860"
  },
  {
    "text": "I can either do the dot product\nin a for loop against each word",
    "start": "2016860",
    "end": "2023790"
  },
  {
    "text": "vector, or I can do the dot\nproduct with a single word vector matrix.",
    "start": "2023790",
    "end": "2029429"
  },
  {
    "text": "And if I do it in a for\nloop, doing each loop",
    "start": "2029430",
    "end": "2034740"
  },
  {
    "text": "takes me almost a second. Whereas if I do it\nwith a matrix multiply,",
    "start": "2034740",
    "end": "2042840"
  },
  {
    "text": "it takes me an order\nof magnitude less time. So you should always be looking\nto use vectors and matrices,",
    "start": "2042840",
    "end": "2049110"
  },
  {
    "text": "not for loops. And this is a speed\nup of about 10 times when you're doing\nthings on a CPU.",
    "start": "2049110",
    "end": "2056250"
  },
  {
    "text": "Heading forward, we're\ngoing to be using GPUs and they only further\nexaggerate the advantages",
    "start": "2056250",
    "end": "2063120"
  },
  {
    "text": "of using vectors and matrices,\nwhere you'll commonly get two orders of\nmagnitude speed up by doing things that way.",
    "start": "2063120",
    "end": "2071039"
  },
  {
    "start": "2070000",
    "end": "2181000"
  },
  {
    "text": "Yeah, so for the\nbackward parse, you are running backward\nparses before on the",
    "start": "2071040",
    "end": "2079469"
  },
  {
    "text": "dropped out examples, right? So for the things\nthat were dropped out",
    "start": "2079469",
    "end": "2085590"
  },
  {
    "text": "no gradient is\ngoing through them because they weren't present. They're not affecting things.",
    "start": "2085590",
    "end": "2091530"
  },
  {
    "text": "So in a particular\nbatch, you're only",
    "start": "2091530",
    "end": "2096810"
  },
  {
    "text": "training weights for the\nthings that aren't dropped out. But then since for\neach successive batch",
    "start": "2096810",
    "end": "2103230"
  },
  {
    "text": "you drop out different things\nthat over a bunch of batches, you're then training all of\nthe weights of the model.",
    "start": "2103230",
    "end": "2111240"
  },
  {
    "text": "And so feature\ndependent regularizer is meaning that the\ndifferent features can",
    "start": "2111240",
    "end": "2121610"
  },
  {
    "text": "be regularized different\namounts to maximize performance.",
    "start": "2121610",
    "end": "2129930"
  },
  {
    "text": "So back in this\nmodel, every feature was just sort of being penalized\nby taking Lambda times that",
    "start": "2129930",
    "end": "2138740"
  },
  {
    "text": "squared value. So this is sort of uniform\nregularization, where",
    "start": "2138740",
    "end": "2144290"
  },
  {
    "text": "the end result of this\ndropout style training is that you end up with some\nfeatures being regularized much",
    "start": "2144290",
    "end": "2153109"
  },
  {
    "text": "more strongly and some other\nfeatures being regularized",
    "start": "2153110",
    "end": "2158480"
  },
  {
    "text": "less strongly. And how much they\nare regularized depends on how much\nthey're being used.",
    "start": "2158480",
    "end": "2165300"
  },
  {
    "text": "So you're regularizing\nmore features that are being used less. But I'm not going to get through\ninto the details of how you can",
    "start": "2165300",
    "end": "2173900"
  },
  {
    "text": "understand that perspective. That's outside of the\ncontext of what I'm",
    "start": "2173900",
    "end": "2179299"
  },
  {
    "text": "going to get through right now. So the final bit\nis I just wanted to give a little\nbit of perspective",
    "start": "2179300",
    "end": "2186060"
  },
  {
    "start": "2181000",
    "end": "2607000"
  },
  {
    "text": "on non-linearities\nin our neural nets. So the first thing\nto remember is",
    "start": "2186060",
    "end": "2193620"
  },
  {
    "text": "you have to have non-linearity. So if you're building a\nmulti-layer neural net",
    "start": "2193620",
    "end": "2199080"
  },
  {
    "text": "and you've just got W1x plus\nb1 then you put it through W2x",
    "start": "2199080",
    "end": "2204960"
  },
  {
    "text": "plus b2 and then\nput it through W3x-- ",
    "start": "2204960",
    "end": "2209970"
  },
  {
    "text": "Well, I guess they're\ndifferent hidden layers. So I shouldn't say x. That should be hidden\n1, hidden 2, hidden 3.",
    "start": "2209970",
    "end": "2215079"
  },
  {
    "text": "W3 hidden 3 plus b3. That multiple linear\ntransformations",
    "start": "2215080",
    "end": "2222869"
  },
  {
    "text": "composed so they can\nbe just collapsed down into a single linear\ntransformation. So you don't get any power\nas a data representation",
    "start": "2222870",
    "end": "2234480"
  },
  {
    "text": "by having multiple\nlinear layers. There's a slightly longer story\nthere because you actually",
    "start": "2234480",
    "end": "2240300"
  },
  {
    "text": "do get some interesting\nlearning effects, but I'm not going to\ntalk about that now. But standardly, we have to\nhave some kind of non-linearity",
    "start": "2240300",
    "end": "2251070"
  },
  {
    "text": "to do something interesting\nin a deep neural network. OK.",
    "start": "2251070",
    "end": "2256140"
  },
  {
    "text": "So a starting point is the\nmost classic non-linearity",
    "start": "2256140",
    "end": "2261809"
  },
  {
    "text": "is the logistic. Often just called the\nsigmoid non-linearity because of its S\nshape, which we've seen",
    "start": "2261810",
    "end": "2269250"
  },
  {
    "text": "before in previous lectures. So this will take any\nreal number and method",
    "start": "2269250",
    "end": "2274740"
  },
  {
    "text": "on to the range of 0, 1. And that was sort of\nbasically what people used",
    "start": "2274740",
    "end": "2282029"
  },
  {
    "text": "in sort of 1980s neural nets. Now one disadvantage\nof this non-linearity",
    "start": "2282030",
    "end": "2289680"
  },
  {
    "text": "is that it's moving everything\ninto the positive space because the output is\nalways between 0 and 1.",
    "start": "2289680",
    "end": "2297390"
  },
  {
    "text": "So people then decided\nthat for many purposes, it was useful to have\nthis variant sigmoid shape",
    "start": "2297390",
    "end": "2304800"
  },
  {
    "text": "of hyperbolic tan,\nwhich is then being shown in the second picture.",
    "start": "2304800",
    "end": "2310619"
  },
  {
    "text": "Now you know logistic\nand hyperbolic tan, they sound like they're\nvery different things.",
    "start": "2310620",
    "end": "2316980"
  },
  {
    "text": "But actually, as you maybe\nremember from a math class, hyperbolic tan can be\nrepresented in terms",
    "start": "2316980",
    "end": "2322950"
  },
  {
    "text": "of exponentials as well. And if you do a bit of\nmath, which possibly we might make you do\non an assignment,",
    "start": "2322950",
    "end": "2329520"
  },
  {
    "text": "it's actually the case that\na hyperbolic tangent is just a rescaled and shifted\nversion of the logistic.",
    "start": "2329520",
    "end": "2335770"
  },
  {
    "text": "So it's really exactly the same\ncurve, just squeezed a bit. So it goes now symmetrically\nbetween minus 1 and 1.",
    "start": "2335770",
    "end": "2343950"
  },
  {
    "text": "Well, these kind of\ntranscendental functions like hyperbolic\ntangent, they're kind of",
    "start": "2343950",
    "end": "2351509"
  },
  {
    "text": "slow and expensive\nto compute, right, even on our fast\ncomputers, calculating exponentials is a bit slow.",
    "start": "2351510",
    "end": "2358170"
  },
  {
    "text": "So it's something people\nbecame interested in was well, could we do things with much\nsimpler non-linearities?",
    "start": "2358170",
    "end": "2365330"
  },
  {
    "text": "So what if we used a\nso-called hard tanh? So the hard tanh,\nup to some point",
    "start": "2365330",
    "end": "2374310"
  },
  {
    "text": "it just flatlines\nat minus 1, then it is y equals x up until 1.",
    "start": "2374310",
    "end": "2382140"
  },
  {
    "text": "And then it just\nflat lines again. And that seems a\nslightly weird thing",
    "start": "2382140",
    "end": "2388140"
  },
  {
    "text": "to use because if your\ninput is over on the left or over on the right,\nyou're sort of not",
    "start": "2388140",
    "end": "2395850"
  },
  {
    "text": "getting any discrimination. And everything's\ngiving the same output.",
    "start": "2395850",
    "end": "2400859"
  },
  {
    "text": "And somewhat\nsurprisingly, I mean, I was surprised when\npeople started doing this.",
    "start": "2400860",
    "end": "2406062"
  },
  {
    "text": "These kind of models proved\nto be very successful.",
    "start": "2406062",
    "end": "2411640"
  },
  {
    "text": "And so that then led\ninto what's proven to be kind of the most\nsuccessful and generally widely",
    "start": "2411640",
    "end": "2418290"
  },
  {
    "text": "used non-linearity in a\nlot of recent deep learning work, which is what was being\nused in the dependency powers",
    "start": "2418290",
    "end": "2426850"
  },
  {
    "text": "model I showed is what's\ncalled the Rectified Linear Unit or ReLU.",
    "start": "2426850",
    "end": "2431859"
  },
  {
    "text": "So a ReLU is the simplest\nkind of non-linearity that you can imagine. So if the value of x is\nnegative, its value is 0.",
    "start": "2431860",
    "end": "2440490"
  },
  {
    "text": "So effectively it's just dead. It's not doing anything\nin the computation. And if its value of x is greater\nthan 0, then it's just simply y",
    "start": "2440490",
    "end": "2451920"
  },
  {
    "text": "equals x. The value is being\npassed through.",
    "start": "2451920",
    "end": "2457380"
  },
  {
    "text": "And at first sight, this might\nseem really, really weird and how could this be\nuseful as a non-linearity?",
    "start": "2457380",
    "end": "2464100"
  },
  {
    "text": "But if you sort of think a bit\nabout how you can approximate things with piecewise linear\nfunctions very accurately,",
    "start": "2464100",
    "end": "2471870"
  },
  {
    "text": "you might kind of\nstart to see how you could use this to\ndo accurate function",
    "start": "2471870",
    "end": "2477000"
  },
  {
    "text": "approximation with\npiecewise linear functions. And that's what\nReLU units have been",
    "start": "2477000",
    "end": "2482970"
  },
  {
    "text": "found to do extremely,\nextremely successfully. So logistic and tanh are\nstill used in various places.",
    "start": "2482970",
    "end": "2491740"
  },
  {
    "text": "You use logistic when you\nwant a probability output. We'll see tanh's again\nvery soon when we get",
    "start": "2491740",
    "end": "2497790"
  },
  {
    "text": "to recurrent neural networks. But they're no\nlonger the default when making deep networks.",
    "start": "2497790",
    "end": "2503340"
  },
  {
    "text": "That in a lot of\nplaces, the first thing you should think about trying\nis ReLU nonlinearities.",
    "start": "2503340",
    "end": "2509040"
  },
  {
    "text": "And so in particular\npart of why they're good",
    "start": "2509040",
    "end": "2515880"
  },
  {
    "text": "is that ReLU networks\ntrain very quickly, because you get this sort\nof very straightforward",
    "start": "2515880",
    "end": "2523260"
  },
  {
    "text": "gradient back flow. Because providing you're on\nthe right hand side of it, you're then just getting this\nsort of constant gradient back",
    "start": "2523260",
    "end": "2531270"
  },
  {
    "text": "flow from the slope 1. And so they train very quickly. The somewhat surprising fact\nis that almost the simplest",
    "start": "2531270",
    "end": "2540329"
  },
  {
    "text": "non-linearity\nimaginable is still enough to have a very good\nneural network, but it just is.",
    "start": "2540330",
    "end": "2547859"
  },
  {
    "text": "People have played around\nwith variants of that. So people have\nthen played around with leaky ReLUs, where rather\nthan the left-hand side just",
    "start": "2547860",
    "end": "2557760"
  },
  {
    "text": "going completely to 0, it goes\nslightly negative on a much",
    "start": "2557760",
    "end": "2564510"
  },
  {
    "text": "shallower slope. And then there's been\na parametric ReLU where you have an extra\nparameter where you learn",
    "start": "2564510",
    "end": "2570630"
  },
  {
    "text": "the slope of the negative part. Another thing that's\nbeen used recently",
    "start": "2570630",
    "end": "2576270"
  },
  {
    "text": "is this swish non-linearity,\nwhich looks almost like a ReLU, but it sort of curves down\njust a little bit there",
    "start": "2576270",
    "end": "2584730"
  },
  {
    "text": "and starts to go up. I mean, I think it's fair\nto say that none of these",
    "start": "2584730",
    "end": "2589770"
  },
  {
    "text": "have really proven\nthemselves vastly superior. There are papers saying I can\nget better results by using",
    "start": "2589770",
    "end": "2595500"
  },
  {
    "text": "one of these and maybe you can. But it's not night and\nday and the vast majority",
    "start": "2595500",
    "end": "2601079"
  },
  {
    "text": "of work that you see\naround is still just using ReLUs in many places.",
    "start": "2601080",
    "end": "2607829"
  },
  {
    "start": "2607000",
    "end": "2751000"
  },
  {
    "text": "OK. Couple more things. Parameter initialization.",
    "start": "2607830",
    "end": "2613470"
  },
  {
    "text": "So in almost all\ncases, you must, must, must initialize the\nmatrices of your neural nets",
    "start": "2613470",
    "end": "2623700"
  },
  {
    "text": "with small random values. Neural nets just don't work\nif you start the matrices",
    "start": "2623700",
    "end": "2630210"
  },
  {
    "text": "off as zero, because effectively\nthen everything is symmetric.",
    "start": "2630210",
    "end": "2635980"
  },
  {
    "text": "Nothing can specialize\nin different ways. And you just don't have an\nability for a neural net",
    "start": "2635980",
    "end": "2645960"
  },
  {
    "text": "to learn. You sort of get this\ndefective solution. So standardly, you\nare using some methods",
    "start": "2645960",
    "end": "2653700"
  },
  {
    "text": "such as drawing random numbers\nuniformly between minus r and R",
    "start": "2653700",
    "end": "2658920"
  },
  {
    "text": "for a small value\nr and just filling in all the parameters with that.",
    "start": "2658920",
    "end": "2664260"
  },
  {
    "text": "Exception is with bias weights. It's fine to set bias weights\nto 0 and in some sense that's",
    "start": "2664260",
    "end": "2670200"
  },
  {
    "text": "better. In terms of choosing\nwhat the r value",
    "start": "2670200",
    "end": "2675580"
  },
  {
    "text": "is, essentially for\ntraditional neural nets,",
    "start": "2675580",
    "end": "2680830"
  },
  {
    "text": "what we want to set\nthat r range for is so that the numbers\nin our neural network",
    "start": "2680830",
    "end": "2687340"
  },
  {
    "text": "stay of a reasonable size. They don't get too big and\nthey don't get too small.",
    "start": "2687340",
    "end": "2693400"
  },
  {
    "text": "And whether they kind of\nblow up or not depends on how many connections there\nare in the neural networks.",
    "start": "2693400",
    "end": "2701110"
  },
  {
    "text": "I'm looking at the\nfan in and fan out of connections in\nthe neural network.",
    "start": "2701110",
    "end": "2706720"
  },
  {
    "text": "And so a very common\ninitialization",
    "start": "2706720",
    "end": "2711960"
  },
  {
    "text": "that you'll see in\nPyTorch is what's called Xavier initialization,\nnamed after a person who",
    "start": "2711960",
    "end": "2718799"
  },
  {
    "text": "suggested that. And it's working\nout a value based",
    "start": "2718800",
    "end": "2724320"
  },
  {
    "text": "on this fan in fan\nout of the layers that you can just\nsort of ask for it,",
    "start": "2724320",
    "end": "2730850"
  },
  {
    "text": "say initialize with this\ninitialization and it will. This is another area\nwhere there have been",
    "start": "2730850",
    "end": "2736640"
  },
  {
    "text": "some subsequent developments. So around week 5,\nwill start talking",
    "start": "2736640",
    "end": "2742970"
  },
  {
    "text": "about layer normalization. And if you're using\nlayer normalization, then it sort of\ndoesn't matter the same",
    "start": "2742970",
    "end": "2748339"
  },
  {
    "text": "how you initialize the weights.  So finally, we have\nto train our models.",
    "start": "2748340",
    "end": "2756110"
  },
  {
    "start": "2751000",
    "end": "2859000"
  },
  {
    "text": "And I've briefly introduced\nthe idea of stochastic gradient descent. And the good news is that\nmost of the time that",
    "start": "2756110",
    "end": "2765829"
  },
  {
    "text": "training neural networks with\nstochastic gradient descent works just fine, use it and\nyou will get good results.",
    "start": "2765830",
    "end": "2776260"
  },
  {
    "text": "However, often that requires\nchoosing a suitable learning rate, which is my final slide\nof tips on the next slide.",
    "start": "2776260",
    "end": "2785260"
  },
  {
    "text": "But there's been an\nenormous amount of work on optimization\nof neural networks",
    "start": "2785260",
    "end": "2791309"
  },
  {
    "text": "and people have come up\nwith a whole series of more sophisticated optimizers.",
    "start": "2791310",
    "end": "2797920"
  },
  {
    "text": "And I'm not going to get into\nthe details of optimization in this class. But the very loose idea\nis that these optimizers",
    "start": "2797920",
    "end": "2806220"
  },
  {
    "text": "are adaptive in that they\ncan kind of keep track of how much slope there\nwas, how much gradient there",
    "start": "2806220",
    "end": "2814140"
  },
  {
    "text": "is for different parameters. And therefore,\nbased on that, make decisions as to how much\nto adjust the weights when",
    "start": "2814140",
    "end": "2822000"
  },
  {
    "text": "doing the gradient update\nrather than adjusting it by a constant amount. And so in that\nfamily of methods,",
    "start": "2822000",
    "end": "2829170"
  },
  {
    "text": "there are methods that include\nAdagrad, RMSprop, Adam. And then a variance of Adam\nincluding SparseAdam, AdamW, et",
    "start": "2829170",
    "end": "2838890"
  },
  {
    "text": "cetera. The one called Adam is a\npretty good place to start.",
    "start": "2838890",
    "end": "2844509"
  },
  {
    "text": "And a lot of the time,\nthat's a good one to use. And again from the\nperspective of PyTorch,",
    "start": "2844510",
    "end": "2850140"
  },
  {
    "text": "when you're initializing\nan optimizer, you can just say please use\nAdam and you don't actually",
    "start": "2850140",
    "end": "2855810"
  },
  {
    "text": "need to know much more\nabout it than that. If you are using simple\nstochastic gradient descent,",
    "start": "2855810",
    "end": "2864650"
  },
  {
    "start": "2859000",
    "end": "3000000"
  },
  {
    "text": "you have to choose\nthe learning rate. So that was the eta value that\nyou multiplied the gradient",
    "start": "2864650",
    "end": "2871579"
  },
  {
    "text": "by for how much to\nadjust the weights. And so I talked\nabout that slightly",
    "start": "2871580",
    "end": "2876920"
  },
  {
    "text": "how you didn't want\nit to be too big or your model could\ndiverge or bounce around.",
    "start": "2876920",
    "end": "2882650"
  },
  {
    "text": "You didn't want it to be too\nsmall or else the training could take place\nexceedingly slowly",
    "start": "2882650",
    "end": "2889760"
  },
  {
    "text": "and you'll miss the\nassignment deadline. How big it should\nbe, depends on all",
    "start": "2889760",
    "end": "2895340"
  },
  {
    "text": "sorts of details of the model. And so you want\nto sort of try out some different order\nof magnitude numbers",
    "start": "2895340",
    "end": "2902930"
  },
  {
    "text": "to see what numbers seem to\nwork well for it training stably but reasonably quickly.",
    "start": "2902930",
    "end": "2908900"
  },
  {
    "text": "Something around 10 to the\nminus 3 or 10 to the minus 4 isn't a crazy place to start.",
    "start": "2908900",
    "end": "2914510"
  },
  {
    "text": "In principle, you\ncan do fine just using a constant\nlearning rate in SGD. In practice, people\ngenerally find",
    "start": "2914510",
    "end": "2922190"
  },
  {
    "text": "they can get better results\nby decreasing learning rates as you train. So a very common recipe is that\nyou halve the learning rate",
    "start": "2922190",
    "end": "2931609"
  },
  {
    "text": "after every k epochs,\nwhere an epoch means that you've made a pass through\nthe entire set of training",
    "start": "2931610",
    "end": "2938270"
  },
  {
    "text": "data. So perhaps something\nlike every three epochs, you have the learning rate.",
    "start": "2938270",
    "end": "2945050"
  },
  {
    "text": "And a final little\nnote there in purple is when you make a\npass through the data, you don't want to go through\nthe data items in the same order",
    "start": "2945050",
    "end": "2952700"
  },
  {
    "text": "each time. Because that leads\nyou to to kind of have a sort of patterning\nof the training examples",
    "start": "2952700",
    "end": "2960770"
  },
  {
    "text": "that the model will sort of\nfall into that periodicity of those patterns. So it's best to shuffle the data\nbefore each pass through it.",
    "start": "2960770",
    "end": "2970130"
  },
  {
    "text": "OK. There are more sophisticated\nways to set learning rates and I won't really\nget into those now.",
    "start": "2970130",
    "end": "2978320"
  },
  {
    "text": "Fancier optimizers like Adam\nalso have a learning rate. So you still have to choose\na learning rate value.",
    "start": "2978320",
    "end": "2985069"
  },
  {
    "text": "But it's effectively\nit's an initial learning rate, which typically the\noptimizer shrinks as it runs.",
    "start": "2985070",
    "end": "2990920"
  },
  {
    "text": "And so you commonly want to\nhave the number it starts off with beyond the larger\nsize because it'll",
    "start": "2990920",
    "end": "2996620"
  },
  {
    "text": "be shrinking as it goes. OK. So that's all by\nway of introduction,",
    "start": "2996620",
    "end": "3003900"
  },
  {
    "start": "3000000",
    "end": "3178000"
  },
  {
    "text": "and I'm now ready to start\non language models and RNNs. So what is language modeling?",
    "start": "3003900",
    "end": "3010140"
  },
  {
    "text": "I mean, as two words of\nEnglish, language modeling could mean just about anything. But in the natural language\nprocessing literature,",
    "start": "3010140",
    "end": "3018780"
  },
  {
    "text": "language modeling has a very\nprecise technical definition, which you should know.",
    "start": "3018780",
    "end": "3024300"
  },
  {
    "text": "So language modeling is\nthe task of predicting the word that comes next.",
    "start": "3024300",
    "end": "3031870"
  },
  {
    "text": "So if you have some context\nlike the students opened there,",
    "start": "3031870",
    "end": "3037050"
  },
  {
    "text": "you want to be able to predict\nwhat words will come next. Is that their books, their\nlaptops, their exams,",
    "start": "3037050",
    "end": "3044550"
  },
  {
    "text": "their minds. And so in particular,\nwhat you want to be doing",
    "start": "3044550",
    "end": "3050940"
  },
  {
    "text": "is being able to\ngive a probability that different words will\noccur in this context.",
    "start": "3050940",
    "end": "3058480"
  },
  {
    "text": "So a language model is a\nprobability distribution over next words given\na preceding context.",
    "start": "3058480",
    "end": "3066735"
  },
  {
    "text": " And a system that does that\nis called a language model.",
    "start": "3066735",
    "end": "3075060"
  },
  {
    "text": "So as a result of that, you can\nalso think of a language model as a system that\nassigns a probability",
    "start": "3075060",
    "end": "3082530"
  },
  {
    "text": "score to a piece of text. So if we have a piece\nof text, then we can just work out\nits probability",
    "start": "3082530",
    "end": "3088950"
  },
  {
    "text": "according to a language model. So the probability of\na sequence of tokens, we can decompose via\nthe chain of probability",
    "start": "3088950",
    "end": "3097440"
  },
  {
    "text": "of the first times probability\nof the second given the first et cetera, et cetera. And then we can work that\nout using what our language",
    "start": "3097440",
    "end": "3105660"
  },
  {
    "text": "model provides as a product of\neach probability of predicting the next word. ",
    "start": "3105660",
    "end": "3113290"
  },
  {
    "text": "OK. Language models are\nreally the cornerstone of human language technology.",
    "start": "3113290",
    "end": "3120460"
  },
  {
    "text": "Everything that you\ndo with computers that involves human language,\nyou are using language models.",
    "start": "3120460",
    "end": "3129320"
  },
  {
    "text": "So when you're using\nyour phone and it's suggesting, whether\nwell or badly,",
    "start": "3129320",
    "end": "3134650"
  },
  {
    "text": "what the next word that you\nprobably want to type is, that's the language model\nworking to try and predict",
    "start": "3134650",
    "end": "3140680"
  },
  {
    "text": "the likely next words. When the same thing\nhappens in a Google Doc",
    "start": "3140680",
    "end": "3145990"
  },
  {
    "text": "and it's suggesting a next\nword or a next few words, that's a language model.",
    "start": "3145990",
    "end": "3152260"
  },
  {
    "text": "The main reason why\nthe one in Google Docs works much better than\nthe one on your phone",
    "start": "3152260",
    "end": "3157960"
  },
  {
    "text": "is that for the\nkeyboard phone models, they have to be very compact\nso that they can run quickly",
    "start": "3157960",
    "end": "3164349"
  },
  {
    "text": "on not much memory. So they're sort of only\nmediocre language models, where",
    "start": "3164350",
    "end": "3169360"
  },
  {
    "text": "something like Google Docs\ncan do a much better language modeling job.",
    "start": "3169360",
    "end": "3174519"
  },
  {
    "text": "Query completion, same thing. There's a language model. And so then the\nquestion is, well, how",
    "start": "3174520",
    "end": "3181400"
  },
  {
    "start": "3178000",
    "end": "3256000"
  },
  {
    "text": "do we build language models? And so I briefly wanted\nto first again give",
    "start": "3181400",
    "end": "3188930"
  },
  {
    "text": "the traditional answer\nsince you should have at least some\nunderstanding of how NLP was",
    "start": "3188930",
    "end": "3195380"
  },
  {
    "text": "done without a neural network. And the traditional answer\nthat powered speech recognition",
    "start": "3195380",
    "end": "3202309"
  },
  {
    "text": "and other applications\nfor at least two decades, three decades really, was what\nwere called n-gram language",
    "start": "3202310",
    "end": "3209450"
  },
  {
    "text": "models. And these were a very simple,\nbut still quite effective idea.",
    "start": "3209450",
    "end": "3214880"
  },
  {
    "text": "So we want to give\nprobabilities of next words.",
    "start": "3214880",
    "end": "3219930"
  },
  {
    "text": "So what we're gonna\nwork with is what are referred to as n-grams. And so n-grams is just a\nchunk of n consecutive words",
    "start": "3219930",
    "end": "3229520"
  },
  {
    "text": "which are usually referred to\nas unigrams, bigrams, trigrams. And then 4-grams and 5-grams.",
    "start": "3229520",
    "end": "3235940"
  },
  {
    "text": "A horrible set of names, which\nwould offend any humanist but that's what\npeople normally say.",
    "start": "3235940",
    "end": "3243590"
  },
  {
    "text": "And so effectively what\nwe do is just collect statistics about how often\ndifferent n-grams occur",
    "start": "3243590",
    "end": "3251269"
  },
  {
    "text": "in a large amount\nof text and then use those to build\na probability model.",
    "start": "3251270",
    "end": "3256940"
  },
  {
    "start": "3256000",
    "end": "3337000"
  },
  {
    "text": "So the first thing we\ndo is what's referred to as making a Markov assumption. So these are also referred\nto as Markov models.",
    "start": "3256940",
    "end": "3264670"
  },
  {
    "text": "And we decide that the word\nin position t plus 1 only depends on the preceding\nn minus 1 words.",
    "start": "3264670",
    "end": "3272694"
  },
  {
    "text": " So if we want to\npredict t plus 1",
    "start": "3272695",
    "end": "3278750"
  },
  {
    "text": "given the entire preceding\ntext, we actually throw away the early words\nand just use the preceding n",
    "start": "3278750",
    "end": "3286850"
  },
  {
    "text": "minus 1 words as context. Well, once we made\nthat simplification,",
    "start": "3286850",
    "end": "3292160"
  },
  {
    "text": "we can then just\nuse the definition of conditional\nprobability and say all that conditional probability\nis the probability",
    "start": "3292160",
    "end": "3299870"
  },
  {
    "text": "of n words divided by the\npreceding n minus 1 words.",
    "start": "3299870",
    "end": "3307050"
  },
  {
    "text": "And so we have the\nprobability of an n-gram over the probability\nof an n minus 1 gram.",
    "start": "3307050",
    "end": "3314040"
  },
  {
    "text": "And so then how do we\nget these n-gram and n minus 1 gram probabilities? We simply take a large amount\nof text in some language",
    "start": "3314040",
    "end": "3322320"
  },
  {
    "text": "and we count how often the\ndifferent n-grams occur.",
    "start": "3322320",
    "end": "3327460"
  },
  {
    "text": "And so our crude\nstatistical approximation starts off as the count of\nthe n-gram over the count",
    "start": "3327460",
    "end": "3334545"
  },
  {
    "text": "of the n minus 1 gram. So here's an example of that. Suppose we are learning\na 4-gram language model.",
    "start": "3334545",
    "end": "3341839"
  },
  {
    "start": "3337000",
    "end": "3523000"
  },
  {
    "text": "OK. So we throw away all words\napart from the last three words",
    "start": "3341840",
    "end": "3346860"
  },
  {
    "text": "and they're our conditioning. ",
    "start": "3346860",
    "end": "3353477"
  },
  {
    "text": "We look in some large-- We use the counts from\nsome large training corpus and we see how\noften did students",
    "start": "3353477",
    "end": "3359700"
  },
  {
    "text": "open their books occur,\nhow often did students open their minds occur.",
    "start": "3359700",
    "end": "3364769"
  },
  {
    "text": "And then for each\nof those counts, we divide through by the\ncount of how often students",
    "start": "3364770",
    "end": "3369810"
  },
  {
    "text": "open their occurred and that\ngives us our probability estimates.",
    "start": "3369810",
    "end": "3376020"
  },
  {
    "text": "So for example,\nif in the corpus, students open their\noccurred 1,000 times,",
    "start": "3376020",
    "end": "3381950"
  },
  {
    "text": "students opened their\nbooks occurred 400 times, we'd get a probability\nestimate of 0.4 for books.",
    "start": "3381950",
    "end": "3389030"
  },
  {
    "text": "If exams occurred 100 times,\nwe'd get 0.1 for exams. And we sort of see here\nalready the disadvantage",
    "start": "3389030",
    "end": "3397099"
  },
  {
    "text": "of having made the\nMarkov assumption and have gotten rid of all of\nthis earlier context, which",
    "start": "3397100",
    "end": "3403760"
  },
  {
    "text": "would have been useful\nfor helping us to predict.  The one other point\nthat I'll just",
    "start": "3403760",
    "end": "3411560"
  },
  {
    "text": "mention that I\nconfused myself on is this count of the\nn-gram language model.",
    "start": "3411560",
    "end": "3418350"
  },
  {
    "text": "So for a 4-gram\nlanguage model, it's called a 4-gram language model. Because in its\nestimation, you're",
    "start": "3418350",
    "end": "3425570"
  },
  {
    "text": "using 4 grams in the\nnumerator and trigrams in the denominator.",
    "start": "3425570",
    "end": "3431500"
  },
  {
    "text": "So you use the size\nof the numerator. So that terminology is\ndifferent to the terminology",
    "start": "3431500",
    "end": "3438830"
  },
  {
    "text": "that's used in Markov models. So when people talk about\nthe order of a Markov model,",
    "start": "3438830",
    "end": "3444590"
  },
  {
    "text": "that refers to the amount\nof context you're using. So this would correspond to\na third order Markov model.",
    "start": "3444590",
    "end": "3452015"
  },
  {
    "text": " Yeah, so someone said is this\nsimilar to a Naive Bayes model?",
    "start": "3452015",
    "end": "3460680"
  },
  {
    "text": "Sort of. Naive Bayes models, you also\nestimate the probabilities just by counting.",
    "start": "3460680",
    "end": "3467640"
  },
  {
    "text": "So they're related and\nthey're sort of in some sense",
    "start": "3467640",
    "end": "3473039"
  },
  {
    "text": "two differences. The first difference\nor specialization",
    "start": "3473040",
    "end": "3479220"
  },
  {
    "text": "is that Naive Bayes\nmodels work out probabilities of words\nindependent of their neighbors.",
    "start": "3479220",
    "end": "3487529"
  },
  {
    "text": "So what in one part that a\nNaive Bayes language model is a unigram brand language model.",
    "start": "3487530",
    "end": "3494160"
  },
  {
    "text": "So you're just using the\ncounts of individual words. But the other part of\na Naive Bayes model",
    "start": "3494160",
    "end": "3500250"
  },
  {
    "text": "is you're learning a different\nset of unigram counts for every class for\nyour classifier.",
    "start": "3500250",
    "end": "3507309"
  },
  {
    "start": "3507310",
    "end": "3512910"
  },
  {
    "text": "And so effectively\na Naive Bayes model is you've got class specific\nunigram language models.",
    "start": "3512910",
    "end": "3521310"
  },
  {
    "text": " Okay, I gave this as a\nsimple statistical model",
    "start": "3521310",
    "end": "3528070"
  },
  {
    "start": "3523000",
    "end": "3599000"
  },
  {
    "text": "for estimating\nyour probabilities with an n-gram model. You can't actually get away\nwith just doing that because you",
    "start": "3528070",
    "end": "3535330"
  },
  {
    "text": "have sparsity problems. So it often will be the\ncase that for many words,",
    "start": "3535330",
    "end": "3541330"
  },
  {
    "text": "students open their\nbooks or students opened their backpacks just never\noccurred in the training data.",
    "start": "3541330",
    "end": "3549490"
  },
  {
    "text": "That if you think about it,\nif you have something like 10 to the 5th different words\neven and you want to have then",
    "start": "3549490",
    "end": "3556240"
  },
  {
    "text": "a sequence of four\nwords and there are 10 to the 5th\nof each, there's sort of 10 to the 20th\ndifferent combinations.",
    "start": "3556240",
    "end": "3564290"
  },
  {
    "text": "So unless you're seeing this\ntruly astronomical amount of data, most four word\nsequences you've never seen.",
    "start": "3564290",
    "end": "3571630"
  },
  {
    "text": "So then your numerator will be\n0 and your probability estimate will be 0. And so that's bad.",
    "start": "3571630",
    "end": "3577960"
  },
  {
    "text": "And so the commonest\nway of solving that is just to add a little\ndelta to every count and then",
    "start": "3577960",
    "end": "3583390"
  },
  {
    "text": "everything is non-zero. And that's called smoothing. But well, sometimes\nit's worse than that",
    "start": "3583390",
    "end": "3589690"
  },
  {
    "text": "because sometimes you won't even\nhave seen students open theirs. And that's more\nproblematic, because that",
    "start": "3589690",
    "end": "3595240"
  },
  {
    "text": "means our denominator here\nis 0 and so the division",
    "start": "3595240",
    "end": "3600250"
  },
  {
    "text": "will be ill-defined. And we can't usefully calculate\nany probabilities in a context",
    "start": "3600250",
    "end": "3605830"
  },
  {
    "text": "that we've never seen. And so the standard solution to\nthat is to shorten the context",
    "start": "3605830",
    "end": "3611530"
  },
  {
    "text": "and that's called back off. So we condition only\non opened their. Or if we still haven't\nseen the opened their,",
    "start": "3611530",
    "end": "3619990"
  },
  {
    "text": "we'll condition only on their. Or we could just\nforget all conditioning",
    "start": "3619990",
    "end": "3625119"
  },
  {
    "text": "and actually use a unigram\nmodel for our probabilities. ",
    "start": "3625120",
    "end": "3633940"
  },
  {
    "text": "Yeah, and so as you\nincrease the order n of the n-gram language model,\nthese sparsity problems become",
    "start": "3633940",
    "end": "3641200"
  },
  {
    "text": "worse and worse. So in the early days\npeople normally worked with trigram models. As it became easier to collect\nbillions of words of text,",
    "start": "3641200",
    "end": "3650470"
  },
  {
    "text": "people commonly moved\nto 5-gram models. But every time you go up\nan order of conditioning,",
    "start": "3650470",
    "end": "3658600"
  },
  {
    "text": "you effectively need\nto be collecting orders of magnitude\nmore data because",
    "start": "3658600",
    "end": "3663640"
  },
  {
    "text": "of the size of the vocabularies\nof human languages. ",
    "start": "3663640",
    "end": "3669590"
  },
  {
    "text": "There's also a problem\nthat these models are huge. So you basically\nhave to be caught",
    "start": "3669590",
    "end": "3675980"
  },
  {
    "text": "storing counts of all\nof these word sequences so you can work out\nthese probabilities.",
    "start": "3675980",
    "end": "3681980"
  },
  {
    "text": "And I mean, that's\nactually had a big effect on what terms of what\ntechnology is available.",
    "start": "3681980",
    "end": "3687630"
  },
  {
    "text": "So in the 2000s decade up until\nthat, whenever it was, 2014,",
    "start": "3687630",
    "end": "3694640"
  },
  {
    "text": "that there was already\nGoogle Translate using probabilistic models that\nincluded language models",
    "start": "3694640",
    "end": "3703130"
  },
  {
    "text": "of the n-gram\nlanguage model sort. But the only way they could\npossibly be run is in the cloud",
    "start": "3703130",
    "end": "3709520"
  },
  {
    "text": "because you needed to have these\nhuge tables of probabilities.",
    "start": "3709520",
    "end": "3714560"
  },
  {
    "text": "But now we have\nneural nets and you can have Google Translate just\nactually run on your phone.",
    "start": "3714560",
    "end": "3719660"
  },
  {
    "text": "And that's possible,\nbecause neural net models can be massively more compact\nthan these old n-gram language",
    "start": "3719660",
    "end": "3727320"
  },
  {
    "text": "models. ",
    "start": "3727320",
    "end": "3733300"
  },
  {
    "text": "But nevertheless, before we\nget onto the neural models, let's just sort of look at\nan example of how these work.",
    "start": "3733300",
    "end": "3744420"
  },
  {
    "text": "So it's trivial to\ntrain an n-gram language model because you\nreally just count how often word sequences\noccur in a corpus,",
    "start": "3744420",
    "end": "3752150"
  },
  {
    "text": "and you're ready to go. So these models can\nbe trained in seconds. That's really good. That's not like sitting around\nfor training neural networks.",
    "start": "3752150",
    "end": "3759570"
  },
  {
    "text": "So if I train on my laptop a\nsmall language model, about 1.7",
    "start": "3759570",
    "end": "3768050"
  },
  {
    "text": "million words as\na trigram model, I can then ask it\nto generate text.",
    "start": "3768050",
    "end": "3773360"
  },
  {
    "text": "If I give it a couple\nof words, today the, I can then get it\nto sort of suggest",
    "start": "3773360",
    "end": "3778880"
  },
  {
    "text": "a word that might come next. And the way I do that\nis the language model knows the probability\ndistribution",
    "start": "3778880",
    "end": "3786050"
  },
  {
    "text": "of things that can come next. Know that there's a kind of a\ncrude probability distribution.",
    "start": "3786050",
    "end": "3792109"
  },
  {
    "text": "I mean, because effectively over\nthis relatively small corpus, there were things that occurred\nonce, Italian and emirate.",
    "start": "3792110",
    "end": "3800120"
  },
  {
    "text": "There are things that\noccurred twice, price. There were things that occurred\nfour times, company and bank.",
    "start": "3800120",
    "end": "3806359"
  },
  {
    "text": "It's sort of fairly\ncrude and rough. But I nevertheless get\nprobability estimates. I can then say,\nOK, based on this,",
    "start": "3806360",
    "end": "3816290"
  },
  {
    "text": "let's take this\nprobability distribution and then we'll just\nsample the next word.",
    "start": "3816290",
    "end": "3822000"
  },
  {
    "text": "So the two most likely words\nto sample are company or bank. But we're rolling\nthe dice and we",
    "start": "3822000",
    "end": "3828289"
  },
  {
    "text": "might get any of the\nwords that had come next. So maybe I sample price.",
    "start": "3828290",
    "end": "3833750"
  },
  {
    "text": "Now I will condition\non the price and look up the\nprobability distribution",
    "start": "3833750",
    "end": "3840440"
  },
  {
    "text": "of what comes next. The most likely thing is of. And so again, I'll sample\nand maybe this time I'll",
    "start": "3840440",
    "end": "3846920"
  },
  {
    "text": "pick up of. And then I will now\ncondition on price of,",
    "start": "3846920",
    "end": "3852799"
  },
  {
    "text": "and I will look up the\nprobability distribution of words following that.",
    "start": "3852800",
    "end": "3857990"
  },
  {
    "text": "And I get this\nprobability distribution. And I'll sample randomly\nsome word from it",
    "start": "3857990",
    "end": "3864259"
  },
  {
    "text": "and maybe this time I'll\nsample a rare but possible one like gold.",
    "start": "3864260",
    "end": "3869569"
  },
  {
    "text": "And I can keep on\ngoing, and I'll get out something like this. Today the price of gold per ton,\nwhile production of shoe lasts",
    "start": "3869570",
    "end": "3878210"
  },
  {
    "text": "and shoe industry,\nthe bank intervened just after it considered\nand rejected the IMF demand",
    "start": "3878210",
    "end": "3883700"
  },
  {
    "text": "to rebuild depleted\nEuropean stocks, Sep 30 end primary $0.76 a share.",
    "start": "3883700",
    "end": "3890150"
  },
  {
    "text": "So what just a\nsimple trigram model can produce over\nnot very much text",
    "start": "3890150",
    "end": "3896510"
  },
  {
    "text": "is actually already\nkind of interesting. It's actually surprisingly\ngrammatical, right?",
    "start": "3896510",
    "end": "3902119"
  },
  {
    "text": "There are whole pieces of\nit while production of shoe lasts and shoe\nindustry, the bank",
    "start": "3902120",
    "end": "3907819"
  },
  {
    "text": "intervened just after it\nconsidered and rejected an IMF demand is really actually\npretty good grammatical text.",
    "start": "3907820",
    "end": "3914360"
  },
  {
    "text": "So it's sort of amazing that\nthese simple n-gram models actually can model a\nlot of human language.",
    "start": "3914360",
    "end": "3921930"
  },
  {
    "text": "On the other hand, it's not\na very good piece of text. It's completely incoherent\nand makes no sense.",
    "start": "3921930",
    "end": "3929140"
  },
  {
    "text": "And so to actually\nbe able to generate text that seems\nlike it makes sense,",
    "start": "3929140",
    "end": "3935450"
  },
  {
    "text": "we're going to need a\nconsiderably better language model. And that's precisely what neural\nlanguage models have allowed",
    "start": "3935450",
    "end": "3942980"
  },
  {
    "text": "us to build as we'll see later. OK. So how can we build a\nneural language model?",
    "start": "3942980",
    "end": "3950230"
  },
  {
    "text": "And so first of all, we're\ngoing to do a simple one and then we'll see where we get.",
    "start": "3950230",
    "end": "3956180"
  },
  {
    "text": "But to move into\nrecurrent neural nets might still take us\nto the next time.",
    "start": "3956180",
    "end": "3962349"
  },
  {
    "text": "So we can have input\nsequence of words and we want a\nprobability distribution",
    "start": "3962350",
    "end": "3967780"
  },
  {
    "text": "over the next word. The simplest thing\nthat we could try is to say, well,\nkind of the only tool",
    "start": "3967780",
    "end": "3975820"
  },
  {
    "text": "we have so far is a\nwindow based classifier. ",
    "start": "3975820",
    "end": "3982330"
  },
  {
    "text": "So what we've done previously,\neither for our named entity recognizer in lecture three\nor what I just showed you",
    "start": "3982330",
    "end": "3989740"
  },
  {
    "text": "for the dependency parser is\nwe have some context window, we put it through a neural\nnet and we predict something",
    "start": "3989740",
    "end": "3996910"
  },
  {
    "text": "as a classifier. So before we were\npredicting a location. But maybe instead, we can reuse\nexactly the same technology.",
    "start": "3996910",
    "end": "4007320"
  },
  {
    "text": "And say we're going to have\na window based classifier. So we're discarding\nthe further away",
    "start": "4007320",
    "end": "4012900"
  },
  {
    "text": "words just like in a\nn-gram language model, but we'll feed this fixed\nwindow into a neural net.",
    "start": "4012900",
    "end": "4022930"
  },
  {
    "text": "So we concatenate the\nword embeddings, we put it through a hidden layer and then\nwe have a softmax classifier",
    "start": "4022930",
    "end": "4031590"
  },
  {
    "text": "over our vocabulary. And so now rather than\npredicting something",
    "start": "4031590",
    "end": "4037020"
  },
  {
    "text": "like location or left arc\nin the dependency parser, we're going to have a softmax\nover the entire vocabulary.",
    "start": "4037020",
    "end": "4045660"
  },
  {
    "text": "Sort of like we did with the\nskip gram negative sampling model in the first two lectures.",
    "start": "4045660",
    "end": "4051750"
  },
  {
    "text": "And so we're going\nto see this choice as predicting what\nword that comes next,",
    "start": "4051750",
    "end": "4057750"
  },
  {
    "text": "whether it produces laptops,\nminds, books, et cetera.",
    "start": "4057750",
    "end": "4063470"
  },
  {
    "text": "OK, so this is a fairly\nsimple fixed window neural net classifier.",
    "start": "4063470",
    "end": "4069980"
  },
  {
    "text": "But this is essentially\na famous early model",
    "start": "4069980",
    "end": "4075380"
  },
  {
    "text": "in the use of neural nets\nfor NLP applications. So first a 2000 conference\npaper and then somewhat later,",
    "start": "4075380",
    "end": "4084619"
  },
  {
    "text": "journal paper. Yoshua Bengio and colleagues\nintroduced precisely this model",
    "start": "4084620",
    "end": "4090710"
  },
  {
    "text": "as the neural probabilistic\nlanguage model. And they were\nalready able to show",
    "start": "4090710",
    "end": "4097009"
  },
  {
    "text": "that this could give\ninteresting, good results for language modeling. And so it wasn't a great\nsolution for neural language",
    "start": "4097010",
    "end": "4106339"
  },
  {
    "text": "modeling, but it\nstill had value. So it didn't solve the\nproblem of allowing",
    "start": "4106340",
    "end": "4112159"
  },
  {
    "text": "us to have bigger\ncontexts to predict what words are going to come next. It's in that way limited\nexactly like an n-gram language",
    "start": "4112160",
    "end": "4121880"
  },
  {
    "text": "model is, but it does\nhave all the advantages of distributed representations.",
    "start": "4121880",
    "end": "4127818"
  },
  {
    "text": "So rather than\nhaving these counts for words sequences that are\nvery sparse and very crude,",
    "start": "4127819",
    "end": "4136549"
  },
  {
    "text": "we can use distributed\nrepresentations of words which then make predictions\nthat semantically similar words",
    "start": "4136550",
    "end": "4145189"
  },
  {
    "text": "should give similar\nprobability distributions. So the idea of that is if\nwe use some other word here,",
    "start": "4145189",
    "end": "4153229"
  },
  {
    "text": "like maybe the pupils\nopened their, well, maybe in our training data we'd\nseen sentences about students,",
    "start": "4153229",
    "end": "4161778"
  },
  {
    "text": "but we've never seen\nsentences about pupils. An n-gram language\nmodel then would sort of",
    "start": "4161779",
    "end": "4166818"
  },
  {
    "text": "have no idea what\nprobabilities to use. Whereas a neural\nlanguage model can say,",
    "start": "4166819",
    "end": "4172920"
  },
  {
    "text": "well pupils is kind of\nsimilar to students. Therefore, I can\npredict similarly to what I would have\npredicted for students.",
    "start": "4172920",
    "end": "4181039"
  },
  {
    "text": "OK. So there's now no\nsparsity problem. We don't need to store\nbillions of n-gram counts,",
    "start": "4181040",
    "end": "4191449"
  },
  {
    "text": "we simply need to\nstore our word vectors and our W and new matrices.",
    "start": "4191450",
    "end": "4199280"
  },
  {
    "text": "But we still have the remaining\nproblems that our fixed window is too small.",
    "start": "4199280",
    "end": "4204619"
  },
  {
    "text": "We can try and make\nthe window larger. If we do that W, the\nW matrix gets bigger.",
    "start": "4204620",
    "end": "4211280"
  },
  {
    "text": "But that also points out\nanother problem with this model. Not only can the\nwindow never be large",
    "start": "4211280",
    "end": "4217429"
  },
  {
    "text": "enough, but W is just\na trained matrix.",
    "start": "4217430",
    "end": "4223070"
  },
  {
    "text": "And so therefore, we're learning\ncompletely different weights for each position of context,\nthe word minus 1 position,",
    "start": "4223070",
    "end": "4231440"
  },
  {
    "text": "the word minus 2, the word\nminus 3, and the word minus 4. So that there's no sharing in\nthe model as to how it treats",
    "start": "4231440",
    "end": "4240590"
  },
  {
    "text": "words in different positions,\neven though in some sense they will contribute\nsemantic components",
    "start": "4240590",
    "end": "4248180"
  },
  {
    "text": "that are at least somewhat\nposition independent. So again, if you\nsort of think back",
    "start": "4248180",
    "end": "4255320"
  },
  {
    "text": "to either a Naive\nBayes model or what we saw with the Word2Vec\nmodel at the beginning,",
    "start": "4255320",
    "end": "4261680"
  },
  {
    "text": "the Word2Vec model\nor Naive Bayes model completely\nignores word order. So it has one set of\nparameters regardless of what",
    "start": "4261680",
    "end": "4269450"
  },
  {
    "text": "position things occur in. That doesn't work well\nfor language modeling, because word order is really\nimportant in language modeling.",
    "start": "4269450",
    "end": "4276830"
  },
  {
    "text": "If the last word is the,\nthat's a really good predictor of there being an adjective\nor noun following where",
    "start": "4276830",
    "end": "4283190"
  },
  {
    "text": "the word four back is\nthe, it doesn't give you the same information.",
    "start": "4283190",
    "end": "4288240"
  },
  {
    "text": "So you do want to somewhat\nmake use of word order.",
    "start": "4288240",
    "end": "4293760"
  },
  {
    "text": "But this model is\nat the opposite extreme that each\nposition is being modeled",
    "start": "4293760",
    "end": "4299070"
  },
  {
    "text": "completely independently. So what we'd like to have\nis a neural architecture",
    "start": "4299070",
    "end": "4306000"
  },
  {
    "text": "that can process an\narbitrary amount of context",
    "start": "4306000",
    "end": "4311400"
  },
  {
    "text": "and have more sharing\nof the parameters while still be\nsensitive to proximity.",
    "start": "4311400",
    "end": "4318390"
  },
  {
    "text": "And so that's the idea of\nrecurrent neural networks. And I'll say about five\nminutes about these today.",
    "start": "4318390",
    "end": "4326350"
  },
  {
    "text": "And then next time we'll return\nand do more about recurrent neural networks.",
    "start": "4326350",
    "end": "4331930"
  },
  {
    "text": "So for the recurrent\nneural network, rather than having\na single hidden",
    "start": "4331930",
    "end": "4338400"
  },
  {
    "text": "layer inside our classifier\nhere that we compute each time,",
    "start": "4338400",
    "end": "4344730"
  },
  {
    "text": "for the recurrent\nneural network we have the hidden layer,\nwhich is often referred",
    "start": "4344730",
    "end": "4350159"
  },
  {
    "text": "to as the hidden state. But we maintain it over time\nand we feed it back into itself.",
    "start": "4350160",
    "end": "4358180"
  },
  {
    "text": "So that's what\nthe word recurrent is meaning, that you're sort of\nfeeding the hidden layer back",
    "start": "4358180",
    "end": "4363720"
  },
  {
    "text": "into itself. So what we do is based\non the first word",
    "start": "4363720",
    "end": "4370620"
  },
  {
    "text": "we compute a hidden\nrepresentation, like before, which can be used to\npredict the next word.",
    "start": "4370620",
    "end": "4378820"
  },
  {
    "text": "But then for when\nwe want to predict what comes after\nthe second word,",
    "start": "4378820",
    "end": "4384780"
  },
  {
    "text": "we not only feed\nin the second word, we feed in the hidden layer\nfrom the previous word",
    "start": "4384780",
    "end": "4393750"
  },
  {
    "text": "to have it help predict\nthe hidden layer above the second word. And so formally the way\nwe're doing that is we're",
    "start": "4393750",
    "end": "4400980"
  },
  {
    "text": "taking the hidden layer\nabove the first word, multiplying it by a\nmatrix W. And then",
    "start": "4400980",
    "end": "4408930"
  },
  {
    "text": "that's going to be going in\ntogether with x2 to generate the next hidden step.",
    "start": "4408930",
    "end": "4415530"
  },
  {
    "text": "And so we keep on\ndoing that at each time step so that we're repeating\na pattern of creating",
    "start": "4415530",
    "end": "4423090"
  },
  {
    "text": "a next hidden layer\nbased on the next input word and the\nprevious hidden state",
    "start": "4423090",
    "end": "4430980"
  },
  {
    "text": "by updating it by\nmultiplying it by a matrix W. OK. So on my slide here.",
    "start": "4430980",
    "end": "4436000"
  },
  {
    "text": "I've still only got\nfour words of context because it's nice for my slide. But in principle, there\ncould be any number",
    "start": "4436000",
    "end": "4443070"
  },
  {
    "text": "of words of context now. OK. So what we're doing\nis that we start off",
    "start": "4443070",
    "end": "4451620"
  },
  {
    "text": "by having input\nvectors, which can be our word vectors that\nwe've looked up for each word.",
    "start": "4451620",
    "end": "4457440"
  },
  {
    "text": " So sorry, yeah, so we can\nhave the one hot vectors",
    "start": "4457440",
    "end": "4463360"
  },
  {
    "text": "for word identity. We look up our word embedding,\nso then we got word embeddings",
    "start": "4463360",
    "end": "4468540"
  },
  {
    "text": "for each word. And then we want to\ncompute hidden states. So we need to start\nfrom somewhere.",
    "start": "4468540",
    "end": "4475950"
  },
  {
    "text": "h0 is the initial hidden\nstate, and h0 is normally taken as a 0 vector.",
    "start": "4475950",
    "end": "4482590"
  },
  {
    "text": "So this is actually\njust initialized to 0s. And so for working out\nthe first hidden state,",
    "start": "4482590",
    "end": "4489300"
  },
  {
    "text": "we calculated based\non the first word's embedding by multiplying this\nembedding by a matrix, W_e,",
    "start": "4489300",
    "end": "4499200"
  },
  {
    "text": "and that gives us the\nfirst hidden state. But then as we go on, we want\nto apply the same formula over",
    "start": "4499200",
    "end": "4508960"
  },
  {
    "text": "again. So we have just two\nparameter matrices",
    "start": "4508960",
    "end": "4514000"
  },
  {
    "text": "in the recurrent neural network. One matrix for multiplying\ninput embeddings and one matrix",
    "start": "4514000",
    "end": "4521560"
  },
  {
    "text": "for updating the hidden\nstate of the network. And so for the second word\nfrom its word embedding,",
    "start": "4521560",
    "end": "4529250"
  },
  {
    "text": "we multiply it by\nthe W_e matrix. We take the previous\ntime steps in state",
    "start": "4529250",
    "end": "4536870"
  },
  {
    "text": "and multiply it\nby the W_h matrix. And we use the two of those to\ngenerate the new hidden state.",
    "start": "4536870",
    "end": "4545660"
  },
  {
    "text": "And precisely how we generate\nthe new hidden state is then be shown on this\nequation on the left.",
    "start": "4545660",
    "end": "4552380"
  },
  {
    "text": "So we take the previous hidden\nstate, multiply it by W_h. We take the input embedding,\nmultiply it by W_e.",
    "start": "4552380",
    "end": "4560000"
  },
  {
    "text": "We sum those two, we add\non a learn bias weight.",
    "start": "4560000",
    "end": "4566690"
  },
  {
    "text": "And then we put that\nthrough a non-linearity. And although on this\nslide that non-linearity",
    "start": "4566690",
    "end": "4573650"
  },
  {
    "text": "is written as sigma by far\nthe most common non-linearity to use here actually is\na tanh non-linearity.",
    "start": "4573650",
    "end": "4582260"
  },
  {
    "text": "And so this is the core equation\nfor a simple recurrent neural",
    "start": "4582260",
    "end": "4587960"
  },
  {
    "text": "network. And for each\nsuccessive time step, we're just going\nto keep on applying",
    "start": "4587960",
    "end": "4593510"
  },
  {
    "text": "that to work out hidden states. And then from those\nhidden states,",
    "start": "4593510",
    "end": "4598579"
  },
  {
    "text": "we can use them just like\nin our window classifier to predict what would\nbe the next word.",
    "start": "4598580",
    "end": "4605489"
  },
  {
    "text": "So at any position, we can\ntake this hidden vector, put it",
    "start": "4605490",
    "end": "4611000"
  },
  {
    "text": "through a softmax layer, which\nis multiplying by u matrix and adding on another bias\nand then making a softmax",
    "start": "4611000",
    "end": "4617480"
  },
  {
    "text": "distribution out of that. And that will then give the\nprobability distribution over next words.",
    "start": "4617480",
    "end": "4623989"
  },
  {
    "text": "What we saw here, right,\nthis is the entire math of a simple recurrent\nneural network.",
    "start": "4623990",
    "end": "4631760"
  },
  {
    "text": "And next time I'll come back\nand say more about them, but this is the\nentirety of what you",
    "start": "4631760",
    "end": "4639920"
  },
  {
    "text": "need to know in some\nsense for the computation of the forward model of a\nsimple recurrent neural network.",
    "start": "4639920",
    "end": "4646199"
  },
  {
    "text": "So the advantages we have\nnow as it can process a text input of any length.",
    "start": "4646200",
    "end": "4654320"
  },
  {
    "text": "In theory at least,\nit can use information from any number of steps back.",
    "start": "4654320",
    "end": "4659360"
  },
  {
    "text": "We'll talk more\nabout in practice how well that actually works. The model size is fixed.",
    "start": "4659360",
    "end": "4665730"
  },
  {
    "text": "It doesn't matter how much\nof a past context there is. All we have is that\nW_h and W_e parameters.",
    "start": "4665730",
    "end": "4673880"
  },
  {
    "text": "And at each time step, we\nuse exactly the same weights to update our hidden state.",
    "start": "4673880",
    "end": "4680330"
  },
  {
    "text": "So there's asymmetry in\nhow different inputs are processed in producing\nour predictions.",
    "start": "4680330",
    "end": "4688010"
  },
  {
    "text": "RNNs in practice though,\nthe simple RNNs in practice aren't perfect.",
    "start": "4688010",
    "end": "4693770"
  },
  {
    "text": "So a disadvantage is that\nthey're actually kind of slow. Because with this recurrent\ncomputation, in some sense",
    "start": "4693770",
    "end": "4702000"
  },
  {
    "text": "we are sort of stuck\nwith having to have on the outside of for loop. So we can do vector matrix\nmultiplies on the inside here,",
    "start": "4702000",
    "end": "4710489"
  },
  {
    "text": "but really we have to do\nfor time step equals 1 to n calculate the\nsuccess of hidden states.",
    "start": "4710490",
    "end": "4719240"
  },
  {
    "text": "And so that's not a perfect\nneural net architecture and we'll discuss\nalternatives to that later.",
    "start": "4719240",
    "end": "4727400"
  },
  {
    "text": "And although in\ntheory this model can access information\nany number of steps",
    "start": "4727400",
    "end": "4732440"
  },
  {
    "text": "back, in practice\nwe find that it's pretty imperfect at doing that.",
    "start": "4732440",
    "end": "4737660"
  },
  {
    "text": "And that will then lead\nto more advanced forms of recurrent neural\nnetwork that I'll talk about next time that are\nable to more effectively access",
    "start": "4737660",
    "end": "4748010"
  },
  {
    "text": "past context. OK. I think I'll stop\nthere for the day.",
    "start": "4748010",
    "end": "4753190"
  },
  {
    "start": "4753190",
    "end": "4757120"
  }
]