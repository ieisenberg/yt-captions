[
  {
    "text": "So today, we're going to talk\nabout something a little bit different, more of an algorithms\nlecture than anything else,",
    "start": "4790",
    "end": "12389"
  },
  {
    "text": "actually, not a systems lecture. And a reminder when we get\ntogether next week, we're",
    "start": "12390",
    "end": "17990"
  },
  {
    "text": "basically moving the course back\nto hardware for a little bit, so Kunle it's going\nto take over next week,",
    "start": "17990",
    "end": "23250"
  },
  {
    "text": "we're going to talk about\ncache coherence, I think, is the next lecture. Is that right? Yeah.\nNo, no. Sorry you're going to do spark. Sorry, actually, you're\ngoing to do spark first.",
    "start": "23250",
    "end": "29910"
  },
  {
    "text": "Yeah. Yeah. Because it follows the-- yeah. So sorry, we're not\ngoing to actually talk about hardware just\nyet, but Kunle",
    "start": "29910",
    "end": "37460"
  },
  {
    "text": "will be lecturing next time. OK, but everybody's doing OK? All right, let's get to it then.",
    "start": "37460",
    "end": "42840"
  },
  {
    "text": "So this lecture, sometimes\npeople come up to me and go, I've never thought\nabout things that way.",
    "start": "42840",
    "end": "48090"
  },
  {
    "text": "So what we're going\nto talk about today, about everything in\nthe course so far has been thinking about parallel\ncomputation in terms of threads,",
    "start": "48090",
    "end": "60200"
  },
  {
    "text": "or in terms of a very, very\nsimple organization of let's do",
    "start": "60200",
    "end": "65370"
  },
  {
    "text": "something to every\npiece of data in array. That's basically all\nwe've done so far is I've given you\narrays of stuff",
    "start": "65370",
    "end": "70860"
  },
  {
    "text": "and you've essentially performed\na loop body on those arrays, or you've thought about, OK, I'm\ngoing to create these threads,",
    "start": "70860",
    "end": "78119"
  },
  {
    "text": "and this is what this\nthread is going to do. So today, I want to\nelaborate on and go",
    "start": "78120",
    "end": "84090"
  },
  {
    "text": "into more detail about\nthis way of thinking of, I have a big array and\nwe're going to do something",
    "start": "84090",
    "end": "89400"
  },
  {
    "text": "for everything on the array. Except instead of\njust run a function on every element\nof an array, we're",
    "start": "89400",
    "end": "94530"
  },
  {
    "text": "going to start\nexpressing computation in terms of a richer array of--",
    "start": "94530",
    "end": "99690"
  },
  {
    "text": "a richer set of primitives on\narrays or collections of data.",
    "start": "99690",
    "end": "105060"
  },
  {
    "text": "So first of all,\njust to ground this, remember last time when we\ntalked about GPUs and I said",
    "start": "105060",
    "end": "110940"
  },
  {
    "text": "something about the total number\nof execution contexts is about 163,000 on one chip?",
    "start": "110940",
    "end": "117040"
  },
  {
    "text": "So that means that\nyou probably want to be working on data sets\nof at least that size,",
    "start": "117040",
    "end": "122873"
  },
  {
    "text": "otherwise, you're\nnot going to get the full complement of\nparallelism or latency hiding. So hopefully, at this\npoint in the class,",
    "start": "122873",
    "end": "130330"
  },
  {
    "text": "I have established that even if\nyou're working on a single chip,",
    "start": "130330",
    "end": "135870"
  },
  {
    "text": "you need a lot of parallelism. You need to actually be working\non programs that have hundreds",
    "start": "135870",
    "end": "141750"
  },
  {
    "text": "of thousands or more pieces\nof parallel things to do, otherwise, you're going\nto be in a bit of trouble",
    "start": "141750",
    "end": "148088"
  },
  {
    "text": "in actually using these things. So what we're talking about\ntoday is not going to be outlandish at all, so if\nyou're just going to write code",
    "start": "148088",
    "end": "154800"
  },
  {
    "text": "for a reasonable sized\nGPU, we're talking about, you've got to have parallelism\nin the 200,000 regime.",
    "start": "154800",
    "end": "162030"
  },
  {
    "text": "OK. All right. So also so far in this course,\na little bit more of background.",
    "start": "162030",
    "end": "169350"
  },
  {
    "text": "I think that whenever we throw\na piece of code up on the board or I ask you to do a\nprogramming assignment,",
    "start": "169350",
    "end": "175090"
  },
  {
    "text": "step one is almost\nalways where the heck are the dependencies in my\nprogram, because if I",
    "start": "175090",
    "end": "180629"
  },
  {
    "text": "know the dependencies or\nequivalently if I know where there are not\ndependencies, that's where I'm going to\nget the parallelism.",
    "start": "180630",
    "end": "187280"
  },
  {
    "text": "So here's an example of just\na simple set of expressions, and I just wrote out the\nfundamental dependencies here",
    "start": "187280",
    "end": "192989"
  },
  {
    "text": "in this graph. I cannot execute an operation\nthat's a node in that graph until I have executed\nthe prior ones.",
    "start": "192990",
    "end": "199840"
  },
  {
    "text": "And in part, or I guess\npart B of your assignment, that's in fact your\nunderstanding dependencies",
    "start": "199840",
    "end": "206070"
  },
  {
    "text": "and respecting them. So everything we're\ngoing to do today",
    "start": "206070",
    "end": "212610"
  },
  {
    "text": "is about writing code in\nsome operations where we're",
    "start": "212610",
    "end": "219030"
  },
  {
    "text": "going to just\nassume that someone has really good widely parallel\nimplementations of those",
    "start": "219030",
    "end": "226739"
  },
  {
    "text": "operations. So instead of\nthinking necessarily about the dependencies\nat a fine grain,",
    "start": "226740",
    "end": "233362"
  },
  {
    "text": "I'm just going to say\nI'm going to try and boil my program, my algorithm, down\ninto calls to these functions.",
    "start": "233362",
    "end": "239650"
  },
  {
    "text": "And these functions are known\nto be written in a manner that they can execute\nhighly, highly parallel.",
    "start": "239650",
    "end": "246900"
  },
  {
    "text": "So the thought process\nis, well, if all my code calls these functions,\nand on all these functions are highly parallel,\nthen any of my programs",
    "start": "246900",
    "end": "254360"
  },
  {
    "text": "are also highly parallel. That's the basic gist of things. And you do do this all the time.",
    "start": "254360",
    "end": "261329"
  },
  {
    "text": "For any of you writing\nNumPy code, or pick your favorite tensor language,\nor something like that,",
    "start": "261329",
    "end": "266465"
  },
  {
    "text": "you allocate variables like some\nbig array A or some big array B,",
    "start": "266465",
    "end": "272419"
  },
  {
    "text": "and you use operations\nthat are operating on those arrays,\nthose collections.",
    "start": "272420",
    "end": "278960"
  },
  {
    "text": "Like in this class,\nin this example, if A, B, and C are\nthese big NumPy vectors,",
    "start": "278960",
    "end": "284849"
  },
  {
    "text": "then the operation\nplus is an operation that you hope is implemented\nas fast as possible on those two arrays.",
    "start": "284850",
    "end": "290733"
  },
  {
    "text": "You don't think\nabout threads, you don't think about dependencies,\nor anything like that. OK.",
    "start": "290733",
    "end": "296490"
  },
  {
    "text": "So I want to just generalize\nthis notion of a NumPy array a little bit, and to give\nit a name, I'm going to call--",
    "start": "296490",
    "end": "302460"
  },
  {
    "text": "I'm not going to call it tensors\nor vectors because a vector has a specific meaning in C++,\nI like the term sequences,",
    "start": "302460",
    "end": "309920"
  },
  {
    "text": "which I've brought-- adopted from other researchers\nand other colleagues of mine.",
    "start": "309920",
    "end": "317759"
  },
  {
    "text": "And so I want you to just\nthink about a sequence as an ordered\ncollection of elements.",
    "start": "317760",
    "end": "322800"
  },
  {
    "text": "So it's not a set, it's ordered. And different languages have\ndifferent data types that",
    "start": "322800",
    "end": "329930"
  },
  {
    "text": "basically embody this concept,\nlike in C++ there's actually a sequence.",
    "start": "329930",
    "end": "335120"
  },
  {
    "text": "In Scala, it's list. In Python, you can use a\ndata frame or a NumPy array.",
    "start": "335120",
    "end": "340490"
  },
  {
    "text": "Arguably in PyTorch or\nTensorFlow, you have tensors. The important thing is\nthat unlike arrays--",
    "start": "340490",
    "end": "347009"
  },
  {
    "text": "so in an array you can just say,\nI want the element 43 at this position , and you go,\nyou get access to it.",
    "start": "347010",
    "end": "355560"
  },
  {
    "text": "Unlike arrays, programs can only\naccess elements of sequences",
    "start": "355560",
    "end": "360889"
  },
  {
    "text": "through very\nspecific operations, and that's the big difference. So when you write your ICPC\ncode, you write your CUDA code,",
    "start": "360890",
    "end": "368130"
  },
  {
    "text": "your C code, if I\ngive you an array, even though you can\nthink about it as for every element in the array\ndo X, you can-- actually,",
    "start": "368130",
    "end": "375380"
  },
  {
    "text": "in your code you\ncould write a sub I, which means you could access\nany element at any time.",
    "start": "375380",
    "end": "381080"
  },
  {
    "text": "Now, we're going\nto restrict that. And the ability to\naccess a at any time is the way you create\ndependencies that can really",
    "start": "381080",
    "end": "389000"
  },
  {
    "text": "blow up your program. You're like, oh, this\niteration of the loop accessed a sub i, the next one\naccess a sub i, again,",
    "start": "389000",
    "end": "395880"
  },
  {
    "text": "maybe there's a\ndependency there. So we're just not going to let\nyou do that so that your code is free of dependencies.",
    "start": "395880",
    "end": "403100"
  },
  {
    "text": "So let's talk about the first\nof these specific operations that you are very familiar with.",
    "start": "403100",
    "end": "409259"
  },
  {
    "text": "We just didn't really\ncall it as such, so that's an\noperation called map. Almost every piece of code\nwe've written in this class",
    "start": "409260",
    "end": "419920"
  },
  {
    "text": "so far was basically a map. It's taking a function\nand applying it to every element\nof an input array",
    "start": "419920",
    "end": "427200"
  },
  {
    "text": "to produce an output array. Now, how many people have seen\nthis more functional language syntax like the a or b syntax?",
    "start": "427200",
    "end": "433948"
  },
  {
    "text": "Have you seen that? So a couple of people. Has everybody seen it? How many have not seen it? Just OK. So plenty of people\nhave not seen it.",
    "start": "433948",
    "end": "440259"
  },
  {
    "text": "So map is a higher\norder function because it takes\ninput variables.",
    "start": "440260",
    "end": "446950"
  },
  {
    "text": "So in this case,\nthe arguments to map are the input array a and maybe\nb, and its output array b.",
    "start": "446950",
    "end": "454410"
  },
  {
    "text": "And map also takes another\nargument, which is a function. So let's just say the input\ncollection, excuse me,",
    "start": "454410",
    "end": "462400"
  },
  {
    "text": "the input collection is\na collection of integers. So then, and now\nimagine a function",
    "start": "462400",
    "end": "469629"
  },
  {
    "text": "that adds 2 to an integer. So it takes as input an integer,\nf of x, and it returns x plus 2.",
    "start": "469630",
    "end": "478599"
  },
  {
    "text": "So that function has\nsignature int to int. Input argument is an\ninteger, the output argument",
    "start": "478600",
    "end": "484840"
  },
  {
    "text": "is an integer. So this notation here,\nthis a arrow b just says, it's a function that takes as\ninput, something of type a,",
    "start": "484840",
    "end": "492680"
  },
  {
    "text": "like an integer, and\noutputs something of type b, which in my example\nhere is also an integer.",
    "start": "492680",
    "end": "499940"
  },
  {
    "text": "So if we look at the actual\nsignature of the map function. The map function\ntakes two arguments.",
    "start": "499940",
    "end": "506300"
  },
  {
    "text": "It takes a function\nlike add 2 to something, which in this case,\nlet's say is in my--",
    "start": "506300",
    "end": "515620"
  },
  {
    "text": "so sorry, in this program,\nwhat is my program doing? It's adding 10.",
    "start": "515620",
    "end": "520949"
  },
  {
    "text": "Excuse me. So in this case, my\nfunction is adding 10, so the function takes an\ninteger to an integer.",
    "start": "520950",
    "end": "527150"
  },
  {
    "text": "And so map takes that function,\nwhich is integer to integer, plus a sequence of\nintegers and produces what?",
    "start": "527150",
    "end": "535050"
  },
  {
    "text": "Output sequence. An output sequence which is\nof type sequence of integers.",
    "start": "537785",
    "end": "542820"
  },
  {
    "text": "If I gave map a function\nthat took integers to strings and applied it to a\ncollection of integers,",
    "start": "542820",
    "end": "548630"
  },
  {
    "text": "what would my output type be? The collection of strings. Exactly. So all I'm doing here is this\nis a functional notation that",
    "start": "548630",
    "end": "555550"
  },
  {
    "text": "says map takes two arguments. The first argument\nis a function of a to b, the second argument\nis a sequence of type a,",
    "start": "555550",
    "end": "562730"
  },
  {
    "text": "and the output is a\nsequence of type b, so if you take\nprogramming languages or a bunch of other\nclasses, you'll see that.",
    "start": "562730",
    "end": "569275"
  },
  {
    "text": "And it's not quite so elegant\nin other languages like C++, but we have all that\nfunctionality like, OK,",
    "start": "569275",
    "end": "575840"
  },
  {
    "text": "there's no actual-- I have a sequence here,\nwhich I'm actually representing as an array,\nand I have a function",
    "start": "575840",
    "end": "581650"
  },
  {
    "text": "from the standard\nlibrary transform, which takes in some sense\nthis is a pointer,",
    "start": "581650",
    "end": "587050"
  },
  {
    "text": "this is an iterator starting\nat the beginning of a, and this is a plus\n8, so 1, 2, 3, 4, 5. This is the end of the\narray, so basically this",
    "start": "587050",
    "end": "593638"
  },
  {
    "text": "is just defining a sequence. And it says, please\napply the function f",
    "start": "593638",
    "end": "598639"
  },
  {
    "text": "to all elements\nof this sequence, and put your results here in\npointed to by this iterator.",
    "start": "598640",
    "end": "605310"
  },
  {
    "text": "So it's the same idea, just like\neverything in C++ in a little bit janky syntax.",
    "start": "605310",
    "end": "611690"
  },
  {
    "text": "A lot more clean in a functional\nlanguage like Haskell. I have a sequence a,\nI have a function x,",
    "start": "611690",
    "end": "617550"
  },
  {
    "text": "which takes x to x plus 10. And then if I map f onto\na, I get a new sequence b.",
    "start": "617550",
    "end": "626350"
  },
  {
    "text": "OK. Despite me ripping on C++, I\nwould still write this code.",
    "start": "626350",
    "end": "631420"
  },
  {
    "text": "OK, so here's a question. Now, put your head as\nthe implementer of map.",
    "start": "631420",
    "end": "637720"
  },
  {
    "text": "So now you're implementing\nthe map library. Is this something\nthat you feel very confident about being parallel?",
    "start": "637720",
    "end": "645850"
  },
  {
    "text": "Absolutely, because by the\ndefinition of the function,",
    "start": "645850",
    "end": "651310"
  },
  {
    "text": "every invocation of the\nargument f is parallel. And by the way, f\nis only given access",
    "start": "651310",
    "end": "659500"
  },
  {
    "text": "to individual\nelements of the array. f is just x plus 10.",
    "start": "659500",
    "end": "664520"
  },
  {
    "text": "The only thing f\nknows about is x, so there's nothing f\ncan do accidentally,",
    "start": "664520",
    "end": "670490"
  },
  {
    "text": "for example, to\ncreate a dependency. And not only that,\nthe implementer of f",
    "start": "670490",
    "end": "675910"
  },
  {
    "text": "doesn't even have to think\nabout operating on collections. The implementer f just says,\njust give me one element",
    "start": "675910",
    "end": "681639"
  },
  {
    "text": "and I'll tell you\nhow to process it, and that's a nice,\nclean way to think.",
    "start": "681640",
    "end": "687560"
  },
  {
    "text": "So how would you--\nlet's say I gave you-- I told you to implement\nmap and I said,",
    "start": "687560",
    "end": "693680"
  },
  {
    "text": "it needs to work on sequences\nof arbitrary length, and you're going to get some\nblack box function f of x.",
    "start": "693680",
    "end": "701630"
  },
  {
    "text": "How would you implement map if\nyou want to do it in parallel?",
    "start": "701630",
    "end": "704830"
  },
  {
    "text": "Yeah. Go for it. Yeah. We can do some processing,\nso if we have an array.",
    "start": "709040",
    "end": "715250"
  },
  {
    "text": "OK. But yeah, so you\ncould actually say,",
    "start": "715250",
    "end": "720580"
  },
  {
    "text": "if I had the source to f I could\nrecompile f and a SIMD mode,",
    "start": "720580",
    "end": "726490"
  },
  {
    "text": "that'd be pretty cool. But that SIMD\nthing is only going to let say-- let's\nsay it's only going to do eight things at once.",
    "start": "726490",
    "end": "732270"
  },
  {
    "text": "So what are you going to do\nabout the fact that I could give you a collection that might\nhave 100,000 things in it?",
    "start": "732270",
    "end": "740620"
  },
  {
    "text": "Multi-thread. OK. I can spawn threads. I can spawn like-- I could create a\nworker pool of threads,",
    "start": "740620",
    "end": "746589"
  },
  {
    "text": "and those threads could all\ncall execute this thing. When I first asked the question\nI was thinking about is imagine",
    "start": "746590",
    "end": "752320"
  },
  {
    "text": "I gave you a C++ function\nalready, a header file, so you didn't necessarily even\nhave to recompile the thing.",
    "start": "752320",
    "end": "759203"
  },
  {
    "text": "You would just call f, and you\ncould call it sequentially, or you could spawn a bunch of\nthreads and call it in parallel.",
    "start": "759203",
    "end": "764440"
  },
  {
    "text": "In general, here's my\nlittle implementation of map given f onto sequence\nS would be partition sequence",
    "start": "764440",
    "end": "771410"
  },
  {
    "text": "S into p smaller sequences. Let's say if I had P\nprocessors or P threads, and for each subsequence\nsi in parallel,",
    "start": "771410",
    "end": "779650"
  },
  {
    "text": "sequentially just apply map\nto produce a partial output,",
    "start": "779650",
    "end": "784970"
  },
  {
    "text": "and now I can catenate all the\noutputs, which is basically like I'm going to create\na worker thread pool, and those threads are going\nto process different elements",
    "start": "784970",
    "end": "791955"
  },
  {
    "text": "of the sequence. Nothing too fancy here,\nso this is pretty easy.",
    "start": "791955",
    "end": "798209"
  },
  {
    "text": "All right. So now, let's go to some\nmore interesting operations.",
    "start": "798210",
    "end": "803665"
  },
  {
    "text": "So here's another operation\nthat's extremely important. Used all over the place. It's called fold.",
    "start": "803665",
    "end": "809530"
  },
  {
    "text": "So fold, if you read\nthat signature, takes b--",
    "start": "809530",
    "end": "814650"
  },
  {
    "text": "takes a function, and\nthat function f now, instead of taking a\nto b takes a pair.",
    "start": "814650",
    "end": "822240"
  },
  {
    "text": "It takes an a and a\nb and produces a b. OK, so I see people\nnodding, so that's good.",
    "start": "822240",
    "end": "828139"
  },
  {
    "text": "And so fold takes\na starting element, b, that's not all that\nimportant right now,",
    "start": "828140",
    "end": "834199"
  },
  {
    "text": "but it takes that function f in\na sequence and produces what?",
    "start": "834200",
    "end": "838810"
  },
  {
    "text": "Single elements. Single element. I'm having a little\nbit of a problem here. One second.",
    "start": "843523",
    "end": "849234"
  },
  {
    "text": "So if we write this in Scala,\nhere's an example of fold left, takes an a and a b.",
    "start": "849234",
    "end": "855960"
  },
  {
    "text": "Yeah, exactly. So it takes a sequence of a's.",
    "start": "855960",
    "end": "859149"
  },
  {
    "text": "A function that takes an a\nand a b produces b, and then",
    "start": "861690",
    "end": "867570"
  },
  {
    "text": "essentially iteratively\napplies this. So hopefully, just\nmake sure that you can confirm that fold\nleft with the function 10,",
    "start": "867570",
    "end": "876810"
  },
  {
    "text": "sorry, with the function\nplus, not the function 10. The function plus takes\nan a and an a to an a.",
    "start": "876810",
    "end": "882750"
  },
  {
    "text": "Int and int, to an int. So this function is expecting\na sequence of integers",
    "start": "882750",
    "end": "888750"
  },
  {
    "text": "and produces a single integer as\noutput, and the result is what? The sum of everything\nin the array.",
    "start": "888750",
    "end": "896120"
  },
  {
    "text": "So in this case fold of\n10 on the sequence is 53.",
    "start": "896120",
    "end": "901800"
  },
  {
    "text": "So here's an\ninteresting question. Can you paralyze fold?",
    "start": "901800",
    "end": "906200"
  },
  {
    "text": "So I see some\ndisagreement, actually. Some people are like, no way. Other people are, yes, we can.",
    "start": "908840",
    "end": "916050"
  },
  {
    "text": "It's like the first problem\nof the first lecture. We can use some\nsmall sums, and then",
    "start": "916050",
    "end": "923430"
  },
  {
    "text": "sum the small sums, and so on. OK, so good point. So we have definitely\ncomputed the sum",
    "start": "923430",
    "end": "929880"
  },
  {
    "text": "of a bunch of integers in\nparallel in this class. Absolutely. So anybody-- does\neverybody agree with this?",
    "start": "929880",
    "end": "936540"
  },
  {
    "text": "Anybody disagree with this? We have a little\nbit of disagreement. What's the-- We don't know that the\nfunction we're getting past",
    "start": "936540",
    "end": "942690"
  },
  {
    "text": "is associated. So we don't know that the\norder in which we apply it will be the same every time.",
    "start": "942690",
    "end": "948330"
  },
  {
    "text": "If we have a list\nof true and false under a function of exclusive\nor, then we could get--",
    "start": "948330",
    "end": "955529"
  },
  {
    "text": "Right. So if you interpreted\nmy question as, can fold plus be parallelized?",
    "start": "955530",
    "end": "964529"
  },
  {
    "text": "Yeah. We can think of\nsome ways to do it. As you pointed\nout, we've done it. But this is general fold on\nany arbitrary function f,",
    "start": "964530",
    "end": "972730"
  },
  {
    "text": "and we don't know\nthe properties of f. And a parallel\nimplementation, for example,",
    "start": "972730",
    "end": "978410"
  },
  {
    "text": "that chop this sequence up into\nsubsequences in parallel folded,",
    "start": "978410",
    "end": "985819"
  },
  {
    "text": "maybe sequentially\nthe subsequences, and then folded\nthose results is not guaranteed to get the same\nanswer unless you know",
    "start": "985820",
    "end": "993100"
  },
  {
    "text": "certain properties about f,\nand in particular in this case f needs to be associative.",
    "start": "993100",
    "end": "1005220"
  },
  {
    "text": "So there's some other\nversions of fold where you give fold a\nlittle bit more information.",
    "start": "1005220",
    "end": "1013140"
  },
  {
    "text": "Where you say, well, I'm going\nto give you a combiner function also. And in this case, the combiner--\nwell, not in this case.",
    "start": "1013140",
    "end": "1020140"
  },
  {
    "text": "The combiner function\nactually needs to take b to b.",
    "start": "1020140",
    "end": "1022415"
  },
  {
    "text": "And so now, I can split\nup my input sequence, do all of these sequential\nbut parallel versions of this,",
    "start": "1025627",
    "end": "1033160"
  },
  {
    "text": "and then on each\nthread I'm going to get a single b\nas output., and then",
    "start": "1033160",
    "end": "1039010"
  },
  {
    "text": "I can combine those if I want. But I've pushed this off\nto the user a little bit.",
    "start": "1039010",
    "end": "1047319"
  },
  {
    "text": "Often it's the\ncase that you care about the f being b to b to\nb like the plus operator.",
    "start": "1047319",
    "end": "1053800"
  },
  {
    "text": "And if f is b, a\npair of b's to a b, then you basically\nalso can specify f",
    "start": "1053800",
    "end": "1059260"
  },
  {
    "text": "and combine with one function. Does that make sense? And so as long as\nthat f is associative,",
    "start": "1059260",
    "end": "1065360"
  },
  {
    "text": "you can perform a parallel\nfold by just giving a single f, and the way it\nwill be implemented",
    "start": "1065360",
    "end": "1071110"
  },
  {
    "text": "is that it will\napply f, f, f, f, f, And then it'll use f\nto combine everything.",
    "start": "1071110",
    "end": "1077320"
  },
  {
    "text": "Note that f does not need to be\ncommutative because I actually am still doing all.",
    "start": "1077320",
    "end": "1083080"
  },
  {
    "text": "If I implement it\ncorrectly, I'm still doing all the math\nin the same order,",
    "start": "1083080",
    "end": "1088650"
  },
  {
    "text": "just needs to be associative. So all of a sudden,\nfor some f's--",
    "start": "1088650",
    "end": "1095220"
  },
  {
    "text": "so for some potential\nf's like plus, we've got a parallel\nimplementation of this thing.",
    "start": "1095220",
    "end": "1101140"
  },
  {
    "text": "And now we have the ability\nto map data in parallel, and we have the ability to do\nforms of data reduction, sums,",
    "start": "1104120",
    "end": "1111650"
  },
  {
    "text": "and averages, and\nstuff like that. Now, there's actually\nsomething that's",
    "start": "1111650",
    "end": "1117900"
  },
  {
    "text": "cool that I don't have on the\nslides or in this lecture, but think about the following. Imagine you wanted to do a dot\nproduct or something like that.",
    "start": "1117900",
    "end": "1127720"
  },
  {
    "text": "So you first had a map,\nand the map actually took the pair, a\npair of elements,",
    "start": "1127720",
    "end": "1135269"
  },
  {
    "text": "or that's a bad example. But imagine you want to\nmultiply all numbers by like 10, that's a map, and\nthen compute the sum.",
    "start": "1135270",
    "end": "1141870"
  },
  {
    "text": "So you would write that in\nthese primitives as map times 10",
    "start": "1141870",
    "end": "1147330"
  },
  {
    "text": "on the sequence. You get out what? A new sequence\nand then you apply fold with the plus operator\nto compute the sum.",
    "start": "1147330",
    "end": "1155045"
  },
  {
    "text": "But if I know the definition\nof these operators, and I saw a program that\nwas map and then fold,",
    "start": "1157820",
    "end": "1165960"
  },
  {
    "text": "you could imagine that I\ncould perform a transformation to that function, which\nturned it into something",
    "start": "1165960",
    "end": "1172230"
  },
  {
    "text": "kind of interesting. I would turn it into\na fold that actually did the multiply by some number\nand then immediately fold it",
    "start": "1172230",
    "end": "1182410"
  },
  {
    "text": "in the result. So you could start thinking\nabout transformations that I could do,\nbut don't actually require two passes over all\nthe data provided that you know",
    "start": "1182410",
    "end": "1189940"
  },
  {
    "text": "the definition of map and\nfold, and that's actually what these more sophisticated\nJIT compilers are actually",
    "start": "1189940",
    "end": "1195880"
  },
  {
    "text": "doing on your PyTorch\ncode and stuff like that, just so you know. So let me give you another one.",
    "start": "1195880",
    "end": "1201970"
  },
  {
    "text": "So we have fold, which\nis sequence to Scalr. Now, let's think about scan.",
    "start": "1201970",
    "end": "1208210"
  },
  {
    "text": "So scan is sequence to sequence\nfor an associated binary",
    "start": "1208210",
    "end": "1213640"
  },
  {
    "text": "operator that takes an\na, and an a, to an a. So let's just simplify it here,\nthat operator-- let's just",
    "start": "1213640",
    "end": "1219190"
  },
  {
    "text": "say that operators plus again. And so what scan computes\nis the repeated application",
    "start": "1219190",
    "end": "1226390"
  },
  {
    "text": "of that operator up to and\nincluding the current element.",
    "start": "1226390",
    "end": "1231800"
  },
  {
    "text": "So if we look at\nthis result here, the result of scan\nwith the plus operator is the first element\nof the output",
    "start": "1231800",
    "end": "1238660"
  },
  {
    "text": "is the sum of the all\nelements up to and including this element in the input.",
    "start": "1238660",
    "end": "1243909"
  },
  {
    "text": "The second element output-- the second element of\nthe output sequence is the sum over\nall elements up to",
    "start": "1243910",
    "end": "1249790"
  },
  {
    "text": "and including the same\nposition in the input element. So the last element of the\nscan is going to be the fold,",
    "start": "1249790",
    "end": "1259510"
  },
  {
    "text": "but now I'm producing\nall of the partials. So first of all, do\nyou see the difference",
    "start": "1259510",
    "end": "1264580"
  },
  {
    "text": "between fold and scan. Fold produces a single value,\nscan gives you all the partials.",
    "start": "1264580",
    "end": "1270710"
  },
  {
    "text": "And if I had to implement\nscan sequentially, it might look a\nlittle bit like this.",
    "start": "1270710",
    "end": "1275960"
  },
  {
    "text": "You would do this in a second. The first element of the\noutput is the first element of the input, and\nfor i equals 0 to n,",
    "start": "1275960",
    "end": "1282320"
  },
  {
    "text": "the output element is the\nprevious output element plus the new input element.",
    "start": "1282320",
    "end": "1287159"
  },
  {
    "text": "So the question is going\nto be, is this parallel? And things start getting\na little bit trickier now.",
    "start": "1291260",
    "end": "1298130"
  },
  {
    "text": "Before we move on,\njust as a detail, sometimes in some libraries, a\nscan will be an exclusive scan,",
    "start": "1298130",
    "end": "1303890"
  },
  {
    "text": "and sometimes it will\nbe an inclusive scan. I have inclusive here,\nwhich is up to and including",
    "start": "1303890",
    "end": "1310670"
  },
  {
    "text": "the current element. An exclusive scan is up\nto but not including, so it excludes the\ncurrent element.",
    "start": "1310670",
    "end": "1317490"
  },
  {
    "text": "If you have an exclusive\nscan, how do you compute an inclusive scan?",
    "start": "1317490",
    "end": "1322860"
  },
  {
    "text": "Subtract it from-- Or add, I guess. Yeah, exactly. You just take the output\nof the exclusive scan",
    "start": "1322860",
    "end": "1327900"
  },
  {
    "text": "and you would add\nthe current element. Cool. OK, so you probably have a\nsense of where we're going here.",
    "start": "1327900",
    "end": "1335070"
  },
  {
    "text": "Now, we need to figure out\nhow to parallelize something like scan. All right. So let's think about\nthis a little bit.",
    "start": "1335070",
    "end": "1341450"
  },
  {
    "text": "And again, let me get a\nlittle bit more formal. Here are two different\ndefinitions of scan, inclusive and exclusive.",
    "start": "1341450",
    "end": "1348370"
  },
  {
    "text": "The output element\nis always just going to be the repeated application\nof that binary operator",
    "start": "1348370",
    "end": "1356766"
  },
  {
    "text": "to everything that I've gotten\nto so far, including me. OK. All right.",
    "start": "1356766",
    "end": "1363610"
  },
  {
    "text": "So ignore my broken build,\nso let's just think about it.",
    "start": "1363610",
    "end": "1369410"
  },
  {
    "text": "So how would you go about this? This is going to be an\ninclusive scan, by the way.",
    "start": "1369410",
    "end": "1375790"
  },
  {
    "text": "And like you said, we can\nalways just minus the element if you want the exclusive.",
    "start": "1375790",
    "end": "1381010"
  },
  {
    "text": "So let's think about\nthis, how would you do it? Any thoughts on how\nyou go about it?",
    "start": "1381010",
    "end": "1389020"
  },
  {
    "text": "I would like to start with\nfinding the total sum by doing divide and conquer. So you're going to\nfind the total sum.",
    "start": "1389020",
    "end": "1397012"
  },
  {
    "text": "And how are you--\nwell, we actually already have a really good\nimplementation of total sum. We know how to do that. You don't even have to do well.",
    "start": "1397012",
    "end": "1403360"
  },
  {
    "text": "You do divide and conquer onto\nyour threads and add them up. OK, so let's just say that\nwe have a subroutine that computes the total sum.",
    "start": "1403360",
    "end": "1409490"
  },
  {
    "text": "That's fine. And that would take cost what? That would be basically,\nif this is of length n,",
    "start": "1409490",
    "end": "1415340"
  },
  {
    "text": "it would be n over\nnumber of threads would be the total time, right? Because I would just break\nthe array up into t pieces.",
    "start": "1415340",
    "end": "1422630"
  },
  {
    "text": "Everybody computes\na partial sum. I combine the results. Pretty easily parallelizable,\nso yes, we can do that.",
    "start": "1422630",
    "end": "1431179"
  },
  {
    "text": "Does that help? I think so. Well, it's a reasonable\nstarting point, but.",
    "start": "1431180",
    "end": "1437100"
  },
  {
    "text": "OK, let's keep going. Any other ideas? Yeah. I was thinking one\nof the steps you could do is like as you script\nthe sequence up to half, add",
    "start": "1437100",
    "end": "1444750"
  },
  {
    "text": "the first half [INAUDIBLE]\nbe one step closer. OK, so one thing we could\ndo is we could just say,",
    "start": "1444750",
    "end": "1451370"
  },
  {
    "text": "let's just say we magically\nhad the sum of the first half. Is that right? Is that what you're saying?",
    "start": "1451370",
    "end": "1457230"
  },
  {
    "text": "And if we knew the sum of\nelements a0 through a7, I could definitely\napply that sum",
    "start": "1457230",
    "end": "1463159"
  },
  {
    "text": "to all of these\nelements in parallel. It's kind of interesting.",
    "start": "1463160",
    "end": "1467070"
  },
  {
    "text": "Yeah. Perform scan where you can do\nsomething with [INAUDIBLE].",
    "start": "1471120",
    "end": "1478110"
  },
  {
    "text": "So basically that's\nyou're just taking this idea of dividing the thing\nin half because essentially",
    "start": "1478110",
    "end": "1483150"
  },
  {
    "text": "if we performed a scan\nover the first half, then we could just\ntake that result and then we add it in to the\nscan results for this half.",
    "start": "1483150",
    "end": "1490830"
  },
  {
    "text": "That's correct, and\nwe could break that up into as many pieces as we want.",
    "start": "1490830",
    "end": "1496559"
  },
  {
    "text": "Yep. Can I call for on the first\none and the first two was 3? Sorry.",
    "start": "1496560",
    "end": "1502030"
  },
  {
    "text": "Say that again. Can I call for on like a0-- You can do whatever you want. Yeah. And then that stores\nthe value there?",
    "start": "1502030",
    "end": "1508570"
  },
  {
    "text": "Yeah. That fold is basically this\nway to do the parallel sum. OK.",
    "start": "1508570",
    "end": "1515240"
  },
  {
    "text": "I like this. I think there's some\ngood answers here.",
    "start": "1515240",
    "end": "1523360"
  },
  {
    "text": "What if I make it\neven harder and say, let's imagine we're\nrunning this on a GPU,",
    "start": "1523360",
    "end": "1532987"
  },
  {
    "text": "and I'm going to come back\nto some of those ideas that you just told me, and we\nhave the same number of threads as we have elements.",
    "start": "1532988",
    "end": "1538413"
  },
  {
    "text": "Let's think about this in the\nmassive parallelism machine.",
    "start": "1541823",
    "end": "1544240"
  },
  {
    "text": "I want a really\nparallel solution. So let me just start\nhinting at a few things.",
    "start": "1549420",
    "end": "1555785"
  },
  {
    "text": "What did I do? So parallelism that's\nnow this o of n,",
    "start": "1560830",
    "end": "1567660"
  },
  {
    "text": "and the I just added\nup every neighbor.",
    "start": "1567660",
    "end": "1570080"
  },
  {
    "text": "Now, what did I do? So the notation in\nmy slide is a value.",
    "start": "1574680",
    "end": "1581400"
  },
  {
    "text": "I'm showing you the value-- all the values that go into\nthe sum at a current location.",
    "start": "1581400",
    "end": "1586870"
  },
  {
    "text": "So after the first step,\nthe sum of a0 and a1 is in the second element. And after the second step,\nthe sum of a0, 1, and 2",
    "start": "1586870",
    "end": "1597048"
  },
  {
    "text": "is in the second element, or\nreally I should look here. After the first of the two\nsteps, the sum of a0 through 3",
    "start": "1597048",
    "end": "1602625"
  },
  {
    "text": "is there.",
    "start": "1602625",
    "end": "1603125"
  },
  {
    "text": "And if I keep going, at\nsome point, I have my scan.",
    "start": "1608990",
    "end": "1616800"
  },
  {
    "text": "So first of all, what is the-- if I did a scan sequentially,\nhow much work do I have to do?",
    "start": "1616800",
    "end": "1626380"
  },
  {
    "text": "Remember that C code I\njust put on the slide? How much work did I do if\nthe size of the array was n,",
    "start": "1626380",
    "end": "1631750"
  },
  {
    "text": "the cost of the algorithm was? All of that. I just walked down my array. What's the cost of\nthis algorithm in terms",
    "start": "1631750",
    "end": "1639030"
  },
  {
    "text": "of the amount of work done? N log n. N log n because the first\nstep I do n work or n/2 work,",
    "start": "1639030",
    "end": "1646929"
  },
  {
    "text": "then I do n/4 work, then I do-- so oh, no, sorry. Sorry.",
    "start": "1646930",
    "end": "1652360"
  },
  {
    "text": "I don't n/4 work. I basically do n/2 work,\nand then I do similar work.",
    "start": "1652360",
    "end": "1657590"
  },
  {
    "text": "Yeah. So it's n log n cost. I have o of n work,\nand I have n steps.",
    "start": "1657590",
    "end": "1664389"
  },
  {
    "text": "OK, now, what's the longest\nchain of sequential steps, which is often something we call\nspan in a parallel algorithm?",
    "start": "1664390",
    "end": "1670360"
  },
  {
    "text": "If you had an infinite\nnumber of processors, this would take log n time. The total amount of\ncomputation you do is n log n.",
    "start": "1670360",
    "end": "1678435"
  },
  {
    "text": "If I'm doing an o\nof n work, n steps.",
    "start": "1682147",
    "end": "1684610"
  },
  {
    "text": "I don't like the fact that I\nasymptotically increased work that I'm doing because I mean,\nif I'm a parallel machine,",
    "start": "1688290",
    "end": "1697140"
  },
  {
    "text": "and I have n is a million,\nlog n is non-trivial.",
    "start": "1697140",
    "end": "1701260"
  },
  {
    "text": "So it turns out\nthat there's a way, and this is actually a pretty\nclever algorithm that came up",
    "start": "1704140",
    "end": "1710950"
  },
  {
    "text": "with a guy, I think, this\nwas guy bloke that did this, where I can do-- I can keep my log n span\nand I can be more clever",
    "start": "1710950",
    "end": "1721390"
  },
  {
    "text": "and get it in o of n work. And here's a little bit what\nthe algorithm looks like.",
    "start": "1721390",
    "end": "1727000"
  },
  {
    "text": "And this is something that\nI think is a lot easier just to stare at on your own.",
    "start": "1727000",
    "end": "1733600"
  },
  {
    "text": "But the gist of this is\nthat there's two phases now. There's like if you think about\nthere being a combining tree",
    "start": "1733600",
    "end": "1740380"
  },
  {
    "text": "in phase 1, and then a splitting\ntree an inverse tree on phase 2, there's--",
    "start": "1740380",
    "end": "1746800"
  },
  {
    "text": "and if you read documentation\nof this on the internet, people will say that there's an\nupsweep phase and a downsweep",
    "start": "1746800",
    "end": "1752470"
  },
  {
    "text": "phase. So the upsweep phase is\ncomputing these partial sums in this tree, where to the point\nthat you all are alluding at,",
    "start": "1752470",
    "end": "1761660"
  },
  {
    "text": "look at this, here is the sum\nof the first, the second half of the array.",
    "start": "1761660",
    "end": "1767330"
  },
  {
    "text": "Here is the sum of the\nfirst half of the array. And then along the way, we've\ncomputed some partial sums.",
    "start": "1767330",
    "end": "1775519"
  },
  {
    "text": "So if you look at it, I have\nthe first half here, sorry, the second half there, the\nfirst half there, and then",
    "start": "1775520",
    "end": "1781070"
  },
  {
    "text": "I still have these\ninteresting partials throughout the result\nof my combining tree.",
    "start": "1781070",
    "end": "1787640"
  },
  {
    "text": "And then very much to\nyour point earlier where you said what we're\ngoing do is we're going to take that partial sum,\nthe back half of the array.",
    "start": "1787640",
    "end": "1795929"
  },
  {
    "text": "And if we just had\nsome way to apply it to the appropriate\nelements or the partial sum from the front half of\nthe array for that matter,",
    "start": "1795930",
    "end": "1802740"
  },
  {
    "text": "and apply it to the\nappropriate elements, we could just kind\nof rebase everything.",
    "start": "1802740",
    "end": "1808190"
  },
  {
    "text": "So look what happens, after you\ndo this upsweep, first of all, we're almost done here.",
    "start": "1808190",
    "end": "1814280"
  },
  {
    "text": "We're done here, and so\nwe take that partial sum and shove it over here.",
    "start": "1814280",
    "end": "1821170"
  },
  {
    "text": "Now, at this step, I\njust overwrite, and then it basically becomes\ntake your partial sum, and bring it back,\nand then propagate it.",
    "start": "1821170",
    "end": "1830910"
  },
  {
    "text": "That's kind of the way\nthe combining tree works. And if you look at\nthis, this actually",
    "start": "1830910",
    "end": "1836190"
  },
  {
    "text": "only does a total\nof o of n operations because the first step is n.",
    "start": "1836190",
    "end": "1841840"
  },
  {
    "text": "The next step is n/2. The next step is n/4,\nand so on, and so on, and that telescoping\nseries, as you",
    "start": "1841840",
    "end": "1848370"
  },
  {
    "text": "might know from 161 or\nsomething like that is o of n. Now, there's an\nextra coefficient,",
    "start": "1848370",
    "end": "1854820"
  },
  {
    "text": "and it's actually two times\nthat because I take twice as many steps. So it's still log n in\nsteps, but it's actually",
    "start": "1854820",
    "end": "1861360"
  },
  {
    "text": "there's a constant\nthere, which is 2.",
    "start": "1861360",
    "end": "1863945"
  },
  {
    "text": "So this is something we're\ngoing to have you implement as your warm up CUDA\nprogramming assignment before we get into the\nmeat of assignment 3,",
    "start": "1866610",
    "end": "1873430"
  },
  {
    "text": "so you'll be intimately\nfamiliar with this. Now, yeah, so I think some\ninteresting things to point out",
    "start": "1873430",
    "end": "1883350"
  },
  {
    "text": "is like, it's awesome if you're\na theoretician. o of n work, log n steps.",
    "start": "1883350",
    "end": "1890040"
  },
  {
    "text": "But if you look closely,\nthere's an extra--",
    "start": "1890040",
    "end": "1896160"
  },
  {
    "text": "there's a factor of 2 here. There's a factor of 2\nhere, and there's also some other things\nthat are kind of not",
    "start": "1896160",
    "end": "1902880"
  },
  {
    "text": "great like you are not\nusing all your processors every single moment.",
    "start": "1902880",
    "end": "1908195"
  },
  {
    "text": "So even though I\nsaid, let's make use of an infinite\nnumber of processors, as the series goes down, you're\nusing fewer and fewer of them",
    "start": "1908195",
    "end": "1914820"
  },
  {
    "text": "every step, and\nalso data is kind of moving around\nall over the place. So there are some\nreasons why if I actually",
    "start": "1914820",
    "end": "1921360"
  },
  {
    "text": "ask you to implement this in\npractice, in your homework, you're actually just going\nto do this algorithm.",
    "start": "1921360",
    "end": "1927429"
  },
  {
    "text": "You're not meant to\nmake it really fast. It's just more of a\nprogramming exercise. If you were actually\nworking at NVIDIA",
    "start": "1927430",
    "end": "1933419"
  },
  {
    "text": "and trying to make\nthe library for scan, you would have to work\na little bit harder.",
    "start": "1933420",
    "end": "1939295"
  },
  {
    "text": "And now, let's talk about\nthat just a little bit. So now let's think about a\nsimpler problem actually.",
    "start": "1939295",
    "end": "1944740"
  },
  {
    "text": "Parallelizing this\nonto two cores. So let's just say\nwe cut it in half.",
    "start": "1944740",
    "end": "1951750"
  },
  {
    "text": "We only had two processors and\nwe were running this algorithm. Well, first of all, you have\nperfect workload balance,",
    "start": "1951750",
    "end": "1957429"
  },
  {
    "text": "that's not a problem. You actually don't have\na lot of communication between those processors,\nwhich is pretty cool.",
    "start": "1957430",
    "end": "1964110"
  },
  {
    "text": "If you look carefully, look at\nthose red arrows, the only data that ever moves\nare two elements,",
    "start": "1964110",
    "end": "1973890"
  },
  {
    "text": "but you are bouncing around\nmemory all over the place. So even though one\nprocessor is doing this side",
    "start": "1973890",
    "end": "1979730"
  },
  {
    "text": "and the other processor\nis doing the other side, you're bouncing\naround memory, and I think as you're starting to\nget in this class, that's",
    "start": "1979730",
    "end": "1985608"
  },
  {
    "text": "probably not such a great idea. And so in the spirit of doing\nthe simplest thing first,",
    "start": "1985608",
    "end": "1991929"
  },
  {
    "text": "if you gave me two threads\nand said compute a scan, I would probably do\nsomething like this.",
    "start": "1991930",
    "end": "1998190"
  },
  {
    "text": "I would divide\nthe array in half. I would do two\nsequential scans, which is o of n running\nstraight through memory.",
    "start": "1998190",
    "end": "2005520"
  },
  {
    "text": "I would get the base\nfor the first half, and then I would paralyze\nthe application of the base",
    "start": "2005520",
    "end": "2012380"
  },
  {
    "text": "to the second half of the array. If we're on a shared\nmemory multiprocessor,",
    "start": "2012380",
    "end": "2017900"
  },
  {
    "text": "the fact that information\ncomputed by P2 has to get over to P1 is\nnot that big of a deal.",
    "start": "2017900",
    "end": "2023070"
  },
  {
    "text": "I'm just reading from\nthe same memory system. So the very, very--",
    "start": "2023070",
    "end": "2028130"
  },
  {
    "text": "the thing that you would\ndo probably in one hour if I gave you this\nas an assignment would probably work pretty well.",
    "start": "2028130",
    "end": "2033193"
  },
  {
    "text": "Yeah. Oh, sorry, I didn't\ndraw my line. Now it's clear. OK.",
    "start": "2039530",
    "end": "2044740"
  },
  {
    "text": "So let's think about a\ndifferent type of machine. Let's imagine that you were\ntrying to compute a scan in ISPC",
    "start": "2044740",
    "end": "2054388"
  },
  {
    "text": "or actually a scan in CUDA,\nwhere that all of your threads,",
    "start": "2054389",
    "end": "2060460"
  },
  {
    "text": "your workers are actually\noperating in SIMD. So instead of saying\nI have two threads",
    "start": "2060460",
    "end": "2066629"
  },
  {
    "text": "and they're going to\nbe on different cores, or eight threads are\ngoing on different cores, just imagine if your\nworkers were SIMD lanes.",
    "start": "2066630",
    "end": "2073440"
  },
  {
    "text": "So let's take a look\nat this piece of code. It's written in CUDA, but\nyou can think about it just as ISPC if you want,\nreally doesn't matter.",
    "start": "2073440",
    "end": "2079645"
  },
  {
    "text": "This is pretty interesting. Look at this. So this is scan. It's called scan warp\nbecause this is something",
    "start": "2079645",
    "end": "2086730"
  },
  {
    "text": "that's called by all-- every CUDA thread individually\nin all the threads in a warp, which share the\nsame instruction stream.",
    "start": "2086730",
    "end": "2094810"
  },
  {
    "text": "Or you can think about this as\nif I wrote this as ISPC code, it would be scan of\na program, a gang,",
    "start": "2094810",
    "end": "2101860"
  },
  {
    "text": "and so every instance in a gang\nwould be calling this function. OK, so for now, just imagine\nthat a bunch of threads",
    "start": "2101860",
    "end": "2109570"
  },
  {
    "text": "are calling this function. And so first of\nall, every thread computes its lane, which\nis basically its program",
    "start": "2109570",
    "end": "2116830"
  },
  {
    "text": "ID or your program index. And since-- remember last\ntime your thread index in CUDA",
    "start": "2116830",
    "end": "2122560"
  },
  {
    "text": "could be whatever, anything\nin your thread block. You can make a thread block of\n2,000 threads or 256 threads,",
    "start": "2122560",
    "end": "2128180"
  },
  {
    "text": "but those 2,000 threads are\ngoing to be running in these little groups of 32 warps.",
    "start": "2128180",
    "end": "2133480"
  },
  {
    "text": "So what I do here, and this is\npretty low level hacky code is,",
    "start": "2133480",
    "end": "2138940"
  },
  {
    "text": "I first compute my\nID in the warp, which is to take my thread\nID, mod it by 32",
    "start": "2138940",
    "end": "2146080"
  },
  {
    "text": "to get my actual\nlocation in the warp, so now, I'm making assumptions\nabout the underlying",
    "start": "2146080",
    "end": "2152860"
  },
  {
    "text": "implementation. And so then I write\nbasically this five lines of code, which says, if I am--",
    "start": "2152860",
    "end": "2160520"
  },
  {
    "text": "if my lane is 0, do this. If my lane is 1, do this,\nand so on, and so on.",
    "start": "2160520",
    "end": "2166130"
  },
  {
    "text": "And can you confirm that\nthis code takes five steps? So first of all, why\nis there five steps?",
    "start": "2166130",
    "end": "2173570"
  },
  {
    "text": "Log of 32. So there's five steps\nand every step is just",
    "start": "2173570",
    "end": "2179530"
  },
  {
    "text": "an addition of two numbers. And based on my thread ID\nor my index in this case,",
    "start": "2179530",
    "end": "2187456"
  },
  {
    "text": "some threads stop\nparticipating after a while. OK, so how much\nwork does this do?",
    "start": "2187456",
    "end": "2195045"
  },
  {
    "text": "n log n, same thing. In other words, there\nare five instructions. Let's ignore the if statements.",
    "start": "2199170",
    "end": "2204790"
  },
  {
    "text": "Let's just say they're\nfree for a second. There's five\ninstructions of actually doing math, that's log 32.",
    "start": "2204790",
    "end": "2211450"
  },
  {
    "text": "And every single one\nof these lines of code is doing how much\nwork in a warp?",
    "start": "2211450",
    "end": "2216845"
  },
  {
    "text": "It's one thing per thread\nand there's 32 threads, so n of n total, right?",
    "start": "2219450",
    "end": "2224468"
  },
  {
    "text": "But every thread is doing one\nthing, but there are n threads. So I'm doing overall n log\nn work, and my span is 5.",
    "start": "2224468",
    "end": "2234270"
  },
  {
    "text": "Or in other words, if you put\na stopwatch on this program, you would say it gets\ndone in five steps.",
    "start": "2234270",
    "end": "2238598"
  },
  {
    "text": "OK. So what would you-- what\nwould this code look",
    "start": "2241790",
    "end": "2247350"
  },
  {
    "text": "like if I changed it to use the\nmore advanced o of n algorithm?",
    "start": "2247350",
    "end": "2254400"
  },
  {
    "text": "How many lines would it be? There'll be five\nlines for the upsweep,",
    "start": "2254400",
    "end": "2260150"
  },
  {
    "text": "and then another five\nlines for the down sweep. So wait a minute.",
    "start": "2260150",
    "end": "2266400"
  },
  {
    "text": "That's weird, right? So my n log n algorithm\ntook five cycles.",
    "start": "2266400",
    "end": "2272570"
  },
  {
    "text": "My o of n algorithm\ntakes how many cycles? 10. 10.",
    "start": "2272570",
    "end": "2277700"
  },
  {
    "text": "How can that be the case? Coefficients.",
    "start": "2277700",
    "end": "2283170"
  },
  {
    "text": "Well, yeah, the coefficients\ntoo, but where did the-- what happened? I'm using the same\nnumber of resources.",
    "start": "2283170",
    "end": "2290712"
  },
  {
    "text": "I have this-- in some sense,\nI have this SIMD block of execution units that\ncan do 32 things at once.",
    "start": "2290712",
    "end": "2299330"
  },
  {
    "text": "And that 32 things at\nonce takes five cycles to do n log n work, and 10\ncycles to do o of n work.",
    "start": "2299330",
    "end": "2310140"
  },
  {
    "text": "How are you doing [INAUDIBLE]? So the fact that the only\nthing this processor can do",
    "start": "2310140",
    "end": "2315150"
  },
  {
    "text": "is the same instruction. So the fact that I'm like\ndoing some redundant work",
    "start": "2315150",
    "end": "2320940"
  },
  {
    "text": "is actually kind of OK. The fact that I did\nbetter would mean that I would have all\nthis underutilization of my SIMD lanes, and\nso I do less work,",
    "start": "2320940",
    "end": "2329260"
  },
  {
    "text": "but I spread it out over\nmultiple highly incoherent instructions, so it's a loss.",
    "start": "2329260",
    "end": "2336000"
  },
  {
    "text": "So you really have to think,\nif you're implementing a library for scan, is depending\non how the parallel work gets",
    "start": "2336000",
    "end": "2344760"
  },
  {
    "text": "mapped to a machine,\ndifferent approaches may be very different. If I'm running on a\nmachine that has thousands",
    "start": "2344760",
    "end": "2352020"
  },
  {
    "text": "of independent processors,\nI might use that work efficient formulation. If I'm running on a machine\nwith just two processors,",
    "start": "2352020",
    "end": "2358240"
  },
  {
    "text": "I'd probably do the simple\nthing that you all suggested, divide it in half, compute\na scan, compute a scan, move the data over, and\nthen finish this up.",
    "start": "2358240",
    "end": "2364990"
  },
  {
    "text": "If I'm running on\na SIMD processor, this n log n thing is\nactually the best thing to do.",
    "start": "2364990",
    "end": "2370450"
  },
  {
    "text": "Yeah. So maybe instead of separating\ninto chunks with the big up",
    "start": "2370450",
    "end": "2376780"
  },
  {
    "text": "and competition,\nthat's right here. No. OK, let's go back to it. Yeah. OK. You split it into the eight\nfirst and the eight [INAUDIBLE]",
    "start": "2376780",
    "end": "2385460"
  },
  {
    "text": "If instead we\nintervened so that you do 80 and eight at the same\ntime, it's the same operation.",
    "start": "2385460",
    "end": "2392740"
  },
  {
    "text": "And as long as we have a barrier\nat the end of each iteration, two threads don't\nhave the same data.",
    "start": "2392740",
    "end": "2399490"
  },
  {
    "text": "Well, I think that's-- the problem that you're\nsolving was, is not--",
    "start": "2399490",
    "end": "2404500"
  },
  {
    "text": "well, the problem\nthat I just discussed is a different problem. The problem is the following. Imagine that-- OK, so\nthis is 16 wide, right?",
    "start": "2404500",
    "end": "2412150"
  },
  {
    "text": "Imagine you had a\n16 wide 70 unit. What are the lanes doing here?",
    "start": "2412150",
    "end": "2419680"
  },
  {
    "text": "There's nothing to do. That's the actual problem. Whereas if I had a machine that\nonly had two execution, two",
    "start": "2419680",
    "end": "2426790"
  },
  {
    "text": "threads or something\nlike that, the fact that we're not doing\nmuch in every step is great because the processor\ngets done with the step,",
    "start": "2426790",
    "end": "2433819"
  },
  {
    "text": "and then moves on\nreally quickly. So your work efficiency\nactually, whether or not",
    "start": "2433820",
    "end": "2439700"
  },
  {
    "text": "you care about work\nefficiency or raw parallelism can actually kind of change\nbased on how the machine works.",
    "start": "2439700",
    "end": "2445385"
  },
  {
    "text": "And if I was to do this for\nreal, let me go back to-- where were we? If I was to do this for real\non an NVIDIA chip, for example,",
    "start": "2445385",
    "end": "2453927"
  },
  {
    "text": "I would take that\nlittle piece of code that I gave you, this\none right here, that can compute the scan of 32\nelements in five SIMD steps.",
    "start": "2453927",
    "end": "2463520"
  },
  {
    "text": "And then if I had to do\nsomething for let's say, 128",
    "start": "2463520",
    "end": "2470690"
  },
  {
    "text": "elements, how would you do it?",
    "start": "2470690",
    "end": "2472410"
  },
  {
    "text": "70 steps. We could do the n log n\nthing, but I'm actually--",
    "start": "2476610",
    "end": "2482660"
  },
  {
    "text": "I would rather not\ntake this n log n. I only want to take\nthat n log n asymptotic",
    "start": "2482660",
    "end": "2488250"
  },
  {
    "text": "if I don't have to pay\nfor it in some sense. So what I would do\nis, here's what I do.",
    "start": "2488250",
    "end": "2494880"
  },
  {
    "text": "If I have a sequence that one\nwarp can get something done in five cycles and I get 128--",
    "start": "2494880",
    "end": "2500070"
  },
  {
    "text": "give me 128 things to do, I'll\njust have four different warps do a 32-wide scan, and\nnow I have four partials.",
    "start": "2500070",
    "end": "2508790"
  },
  {
    "text": "Let me just scan\nthose, and then I push the basses back\nout, and then in parallel",
    "start": "2508790",
    "end": "2516560"
  },
  {
    "text": "I update everything. So I did this for 128, but\nimagine you had a 32 squared",
    "start": "2516560",
    "end": "2523190"
  },
  {
    "text": "scan, 1,024 elements, you would\ndo in parallel a bunch of warps,",
    "start": "2523190",
    "end": "2528450"
  },
  {
    "text": "produce 32 scans in five cycles. Then you stick those partials\ntogether into 132-wide block.",
    "start": "2528450",
    "end": "2535160"
  },
  {
    "text": "Five more cycles per scan that,\nand then take those 32 partials and put them back to the blocks,\nand then in parallel do that.",
    "start": "2535160",
    "end": "2542345"
  },
  {
    "text": "So it takes five cycles\nto do the original scan, five cycles to scan the scans,\nand then one cycle for every",
    "start": "2544870",
    "end": "2551560"
  },
  {
    "text": "thread to add the\npartial back to itself, so I could do it in 11\ncycles for 1,024 elements.",
    "start": "2551560",
    "end": "2561550"
  },
  {
    "text": "And just for kicks, that's\nthe code for doing it. I actually give you this code,\nyou call it as a subroutine",
    "start": "2561550",
    "end": "2568480"
  },
  {
    "text": "if you wish in assignment\n3, you don't have to understand how it works. And then if I wanted\nan even larger scan,",
    "start": "2568480",
    "end": "2575300"
  },
  {
    "text": "I would take my blocks of 32,\nthen do a 1,024 by breaking it into blocks of 32, and then\nI distribute my blocks of 32",
    "start": "2575300",
    "end": "2583900"
  },
  {
    "text": "across all of the\nCUDA thread blocks, and I would do that in\nparallel and then scan those.",
    "start": "2583900",
    "end": "2589849"
  },
  {
    "text": "So I'm actually mixing this\ndata parallel scan with a more conventional\nsequential algorithm,",
    "start": "2589850",
    "end": "2595130"
  },
  {
    "text": "so I'm doing data\nparallel when it helps, the SIMD when it helps, and then\nwhen I have far more parallelism",
    "start": "2595130",
    "end": "2600875"
  },
  {
    "text": "than I have processors, I'm\nkind of backing off and doing the more conventional thing. So I just wanted to go\nthrough that sequence just",
    "start": "2600875",
    "end": "2607610"
  },
  {
    "text": "to let people know that there's\nsome real sophistication in implementing these\ndata parallel primitives very efficiently on a machine.",
    "start": "2607610",
    "end": "2614190"
  },
  {
    "text": "And I encourage\nyou in assignment 3 to basically we\nsay, hey, learn how",
    "start": "2614190",
    "end": "2619760"
  },
  {
    "text": "to program some CUDA by\ndoing the o of n algorithm, and then you move on to\nthe actual assignment.",
    "start": "2619760",
    "end": "2625230"
  },
  {
    "text": "And I say, by the way, if\nyou have some extra time, why don't you try and make\nyour scan really fast, and here's the CUDA\nlibrary for scan,",
    "start": "2625230",
    "end": "2631820"
  },
  {
    "text": "and see how close\nyou can get to it. And I have a feeling\nunder the hood they're doing this kind of stuff. I don't know, but it'll\nbe interesting to see",
    "start": "2631820",
    "end": "2637640"
  },
  {
    "text": "if anybody can beat\nCUDA's standard library function for scan. But as a programmer, you\njust think, if I call scan,",
    "start": "2637640",
    "end": "2643410"
  },
  {
    "text": "it'll be as fast as a good\nprogrammer can make it. Yeah. Question. So if whenever there's an\nidle leak or an idle score,",
    "start": "2643410",
    "end": "2651180"
  },
  {
    "text": "[INAUDIBLE] energy. Probably not so much within\na tiny little SIMD block.",
    "start": "2651180",
    "end": "2657290"
  },
  {
    "text": "May I ask Conely,\ndo you think there's any value to if we were\nrunning a SIMD instruction",
    "start": "2657290",
    "end": "2663200"
  },
  {
    "text": "and we knew a lane was dead? Do we gate it dynamically?",
    "start": "2663200",
    "end": "2669290"
  },
  {
    "text": "I mean, according to\nBill Dally these days, with the sparse\ntensor stuff, yes. You could.",
    "start": "2669290",
    "end": "2675740"
  },
  {
    "text": "So I think it's not\nastronomical savings, but if you're probably\ntrying to save a factor of 1.5 of power\nor something like that,",
    "start": "2675740",
    "end": "2682840"
  },
  {
    "text": "you'd probably do. I'm sure NVIDIA chases those\ntypes of optimizations. Yeah.",
    "start": "2682840",
    "end": "2688777"
  },
  {
    "text": "Probably the bigger\nthe SIMD width is, probably the more you consider\ndoing something like that, but--",
    "start": "2688777",
    "end": "2693920"
  },
  {
    "text": "You're better off optimizing\ntaking advantage of the wall. Yeah. [INAUDIBLE] to optimize\nthe use of this.",
    "start": "2693920",
    "end": "2701160"
  },
  {
    "text": "So let's just ramp up the\ncomplexity a little bit more because that's\nfun, and we're now in the back half of the class.",
    "start": "2705080",
    "end": "2711240"
  },
  {
    "text": "So here's a more\ninteresting primitive, so let's say we have scan.",
    "start": "2711240",
    "end": "2717230"
  },
  {
    "text": "A very common thing that\nyou might do in applications is you don't deal\nwith sequences,",
    "start": "2717230",
    "end": "2723120"
  },
  {
    "text": "but you might deal with\nsequences of sequences. So let me give\nyou some examples. For every vertex in a graph,\nfor every edge of that vertex,",
    "start": "2723120",
    "end": "2731910"
  },
  {
    "text": "so I have a sequence of\nvertices, and for every vertex I have a sequence of edges,\nstandard graph representation.",
    "start": "2731910",
    "end": "2739710"
  },
  {
    "text": "Or if you're in scientific\ncomputing, for every particle in a simulation,\nfor every particle",
    "start": "2739710",
    "end": "2745550"
  },
  {
    "text": "within some range of it. So it's like a sequence\nof sequences, right?",
    "start": "2745550",
    "end": "2751230"
  },
  {
    "text": "For every document D in a\ncollection, for each word in D. So I have a sequence\nof sequences",
    "start": "2751230",
    "end": "2756930"
  },
  {
    "text": "and the subsequences are\nall of different length. So there's two levels of\nparallelism in these problems.",
    "start": "2756930",
    "end": "2763780"
  },
  {
    "text": "There's the parallelism\nof the outermost loop, and then there's potentially,\nif you care, the parallelism over the innermost loop.",
    "start": "2763780",
    "end": "2770438"
  },
  {
    "text": "So imagine, for example, you're\nrunning on a GPU and you need 200,000 things. Imagine if you had a graph\nwith 500 vertices or 1,000.",
    "start": "2770438",
    "end": "2780850"
  },
  {
    "text": "No, that's too small,\nlike 10,000 vertices, but each vertex had on\naverage like 10 to 20 edges.",
    "start": "2780850",
    "end": "2789150"
  },
  {
    "text": "So if you parallelize\njust over the vertices, you don't have\nenough parallelism.",
    "start": "2789150",
    "end": "2794380"
  },
  {
    "text": "You need to actually find that\nparallelism over the edges. So these are the\ntypes of problems",
    "start": "2794380",
    "end": "2800900"
  },
  {
    "text": "that fit into this fold. So a segmented scan takes as\ninput, a sequence of sequences,",
    "start": "2800900",
    "end": "2809090"
  },
  {
    "text": "and then applies\nthe scan operator in parallel across all\nof the outer sequences,",
    "start": "2809090",
    "end": "2815579"
  },
  {
    "text": "but applies scan\nindividually to each of them. So what's going on here?",
    "start": "2815580",
    "end": "2820950"
  },
  {
    "text": "This is the plus, so\nlet's take a look. Let's confirm that\nthis is correct. My first sequence is 1 and\n2, so the scan exclusive",
    "start": "2820950",
    "end": "2830299"
  },
  {
    "text": "should be 0 and 1,\nand that's correct because the exclusive first\nelement is nothing, that's 0.",
    "start": "2830300",
    "end": "2835980"
  },
  {
    "text": "And the exclusive second\nelement is just everything up to and including-- and\nexcluding the second element,",
    "start": "2835980",
    "end": "2841860"
  },
  {
    "text": "so it should just be 1. The exclusive scan here is 0,\nand then the exclusive scan here",
    "start": "2841860",
    "end": "2847069"
  },
  {
    "text": "is 0, 1. 1 plus 2 is 3, and 3 plus 3 is\n6, so that's segmented scan.",
    "start": "2847070",
    "end": "2858859"
  },
  {
    "text": "And you can think\nabout implementing segmented scan often is you get\nas input a sequence of sequences",
    "start": "2858860",
    "end": "2865490"
  },
  {
    "text": "might be encoded\nas two sequences. The first sequence is just a\nlist of a regular flat list",
    "start": "2865490",
    "end": "2872490"
  },
  {
    "text": "of numbers, and\nthe second sequence might be some binary flags\non which one to start,",
    "start": "2872490",
    "end": "2878430"
  },
  {
    "text": "on where the sequences start. So in this case,\nthe first element is the start of a\nsequence, and then",
    "start": "2878430",
    "end": "2884730"
  },
  {
    "text": "1, 2, 3, third or\nthe fourth element is the start of the sequence. So imagine that this was just\ngiven to you as an array,",
    "start": "2884730",
    "end": "2892680"
  },
  {
    "text": "and you got that bit flag. You could say I know the\nfirst subsequence starts here, and the second\nsubsequence starts here.",
    "start": "2892680",
    "end": "2899730"
  },
  {
    "text": "It's a very compact\nrepresentation of a sequence of sequences. If you care about\nperformance and you're doing any graph processing, this\nis how you implement a graph.",
    "start": "2899730",
    "end": "2909470"
  },
  {
    "text": "It's like these numbers would\njust be the list of edges, and then you just basically get\na number of edges per vertex",
    "start": "2909470",
    "end": "2916370"
  },
  {
    "text": "here in this very common thing. So we're not going to go\nover how we implement it,",
    "start": "2916370",
    "end": "2922039"
  },
  {
    "text": "but there is an\nalgorithm, which is really fun to read on your own that\nadapts the work efficient",
    "start": "2922040",
    "end": "2930710"
  },
  {
    "text": "scan that I just showed\nyou, and makes it work on sequences of sequences.",
    "start": "2930710",
    "end": "2936070"
  },
  {
    "text": "So you just give it a\nsequence, and you give it these flags, and the output\nin o of n time and log",
    "start": "2936070",
    "end": "2942050"
  },
  {
    "text": "n span is that result. The\nsequence of the sequences. And the work efficient scan\nlooks a little bit like this.",
    "start": "2942050",
    "end": "2950520"
  },
  {
    "text": "Segmented scan looks a\nlittle bit like this. The way to understand\nthe algorithm is this figure, which\nyou can think of.",
    "start": "2950520",
    "end": "2956190"
  },
  {
    "text": "I mean, it looks really\ncomplicated, but really all it is, it's the old algorithm,\nexcept actually you're just propagating those\nsubsequent start flags along",
    "start": "2956190",
    "end": "2964970"
  },
  {
    "text": "with the information. And what you'll see\nhere is that whenever you need to have a back edge,\nif there is a start flag,",
    "start": "2964970",
    "end": "2972280"
  },
  {
    "text": "you just skip the\npropagation of the back edge. So if you go into\nthe code that I had,",
    "start": "2972280",
    "end": "2978220"
  },
  {
    "text": "it basically is there's\nsome if statements which say, if the flag is\none, do something else,",
    "start": "2978220",
    "end": "2984430"
  },
  {
    "text": "don't do something. So it's, if you understand\nthe segmented scan, or sorry, the regular scan, you\nactually will understand",
    "start": "2984430",
    "end": "2991350"
  },
  {
    "text": "the segmented scan\nif you just think about it as with the information\nI need to pass these flags.",
    "start": "2991350",
    "end": "2996880"
  },
  {
    "text": "And if I ever see a\nflag that should stop me from propagating\ninformation over a boundary, so it's actually not\ntoo big of a deal.",
    "start": "2996880",
    "end": "3004120"
  },
  {
    "text": "Now, why do we care\nabout segmented scan? Here's a great example. Sparse matrix\nmultiplication, something",
    "start": "3004120",
    "end": "3010610"
  },
  {
    "text": "that we do all the time. And these guys are\nnodding their heads because they're a big fan of\nsparse matrix multiplication.",
    "start": "3010610",
    "end": "3017390"
  },
  {
    "text": "So here's a matrix. So here I have a dense\nvector, my X's, and I",
    "start": "3017390",
    "end": "3022880"
  },
  {
    "text": "have a sparse matrix. And by sparse, I mean, most\nof these values are zeros.",
    "start": "3022880",
    "end": "3028910"
  },
  {
    "text": "And this is pretty important,\nif this was an n squared matrix, and 99% of the\nvalues are zeros, I",
    "start": "3028910",
    "end": "3036020"
  },
  {
    "text": "can store it much,\nmuch more compactly. And sparse matrices appear\neverywhere in the world whenever you have\nsparse connectivity.",
    "start": "3036020",
    "end": "3042050"
  },
  {
    "text": "For example, the way\nAmazon might store customer recommendation\ninformation, I have me,",
    "start": "3042050",
    "end": "3047570"
  },
  {
    "text": "and I have all the products\nthat I have bought, but the cross product\nof all their users",
    "start": "3047570",
    "end": "3052970"
  },
  {
    "text": "and all of their products\nwould be a very big matrix. But in practice, very few\npeople buy very few products,",
    "start": "3052970",
    "end": "3058620"
  },
  {
    "text": "so they might implement it\nor represent it sparsely. So one way we can express\nthis sparse matrix",
    "start": "3058620",
    "end": "3066079"
  },
  {
    "text": "is in a format called\ncompressed sparse row, so let's break this down.",
    "start": "3066080",
    "end": "3071339"
  },
  {
    "text": "First of all, it's\ncompressed because we're not going to store all the zeros\nexplicitly, and it's sparse row.",
    "start": "3071340",
    "end": "3077312"
  },
  {
    "text": "So the way this is\ngoing to work is I'm going to store\nall my non-zero values as a sequence of sequences.",
    "start": "3077312",
    "end": "3084980"
  },
  {
    "text": "So if you look carefully, the\nfirst row has non-zeros 3 and 1. That's my first subsequence.",
    "start": "3084980",
    "end": "3090660"
  },
  {
    "text": "The next row has a non-zero 2,\nthe next row has a non-zero 4, the last row has\nthree non-zeros.",
    "start": "3090660",
    "end": "3096470"
  },
  {
    "text": "So I have my rows are stored\nas sequences of sequences.",
    "start": "3096470",
    "end": "3101599"
  },
  {
    "text": "Now, this is not a sufficient\nencoding of the matrix because for every\nnon-zero I need",
    "start": "3101600",
    "end": "3106670"
  },
  {
    "text": "to know what column it's in. So I also have for\nevery non-zero,",
    "start": "3106670",
    "end": "3112910"
  },
  {
    "text": "I have another\nsequence of sequences that is what column it's in.",
    "start": "3112910",
    "end": "3116580"
  },
  {
    "text": "Or equivalently, I'm\nstoring for every non-zero, I'm storing a\nvalue and a column.",
    "start": "3119190",
    "end": "3122855"
  },
  {
    "text": "And then I have\nbasically an array, which tells me in\nthis array where is",
    "start": "3125850",
    "end": "3132359"
  },
  {
    "text": "the-- instead of a bit vector. So before I gave\nyou a-- let me--",
    "start": "3132360",
    "end": "3137490"
  },
  {
    "text": "I'm showing I noticed I just\nmade a skip, a big jump. So here, I use\nthese Boolean flags",
    "start": "3137490",
    "end": "3144600"
  },
  {
    "text": "to tell you where the\nstart of the sequences are. So if you gave me--",
    "start": "3144600",
    "end": "3150720"
  },
  {
    "text": "if I just scan\nthrough this array, I get all the starting points.",
    "start": "3150720",
    "end": "3155730"
  },
  {
    "text": "Another way to encode\nthe same information would be if it's a\nmatrix, for every row,",
    "start": "3155730",
    "end": "3162279"
  },
  {
    "text": "just give me, the\nstarting point of the row. So here's-- what I did here\nwas since this is a 2D matrix,",
    "start": "3162280",
    "end": "3170789"
  },
  {
    "text": "where is the start of row\n0 in this big flat array?",
    "start": "3170790",
    "end": "3176340"
  },
  {
    "text": "The subsequence of\nrow 0 starts at 0. The subsequence of row\n1 starts at index 2,",
    "start": "3176340",
    "end": "3183339"
  },
  {
    "text": "subsequence of row\nthree starts at index 4, subsequence of row\nfour starts at index 5. OK, so I'm encoding my sparse\nmatrix as a list of non-zeros.",
    "start": "3183340",
    "end": "3193480"
  },
  {
    "text": "The column index of those\nnon-zeros, and for every row, where does it start\nin these arrays?",
    "start": "3193480",
    "end": "3200920"
  },
  {
    "text": "So this is a commonly called a\ncompressed sparse row format. Compressed sparse row is\nbecause it's row major,",
    "start": "3200920",
    "end": "3207250"
  },
  {
    "text": "all these data structures\nare where the rows start. OK, so if you had\nsegmented scan, and map,",
    "start": "3207250",
    "end": "3214510"
  },
  {
    "text": "and some other\nthings, let's see how we can do this in\nterms of our primitives that we have in\nthe lecture so far.",
    "start": "3214510",
    "end": "3219760"
  },
  {
    "text": "So first of all, given\nall the non-zeros,",
    "start": "3219760",
    "end": "3226490"
  },
  {
    "text": "I need to multiply\nthem by some x. So for every non-zero, I know\nthe column of the non-zero",
    "start": "3226490",
    "end": "3235180"
  },
  {
    "text": "because it was down\nin my columns array. And so I first need to\nperform an operation that",
    "start": "3235180",
    "end": "3241890"
  },
  {
    "text": "takes all of the x's\nand produces a list or a sequence that's same length\nas my sequence of non-zeros.",
    "start": "3241890",
    "end": "3251880"
  },
  {
    "text": "So I actually haven't told\nyou about an operation called gather, but you should\njust-- let's just assume for now that I have the ability\nto basically for every column",
    "start": "3251880",
    "end": "3259710"
  },
  {
    "text": "here, I need to make\na new sequence that is pulling elements given by\nthis piece of data, the column",
    "start": "3259710",
    "end": "3268710"
  },
  {
    "text": "index. I pull x sub column index, and\nI make a new array of x values.",
    "start": "3268710",
    "end": "3275070"
  },
  {
    "text": "So that means that I'm going to\nbe duplicating, oops, elements from x into this array.",
    "start": "3275070",
    "end": "3284340"
  },
  {
    "text": "And then I just do a map\nwith the operation multiply. So I need to map every\nnon-zero in values",
    "start": "3284340",
    "end": "3291240"
  },
  {
    "text": "against my gathered array\nfrom x, so now I have a product of every non-value.",
    "start": "3291240",
    "end": "3297840"
  },
  {
    "text": "Now, I need to sum\nacross the rows. So that's a segmented scan\nwith a plus operation.",
    "start": "3297840",
    "end": "3304760"
  },
  {
    "text": "So I have all my products. I can create the\nflags array that we need to say where is the\nstart of all of the rows?",
    "start": "3304760",
    "end": "3312350"
  },
  {
    "text": "Then segmented scan of this\ndata with this start array gives me all of these partials.",
    "start": "3312350",
    "end": "3316800"
  },
  {
    "text": "And then the last\nelement of every segment is the sum of all of\neverything in the row.",
    "start": "3321500",
    "end": "3329030"
  },
  {
    "text": "So if I go grab the appropriate\nvalues out of this array, I get the sum of\neverything in the row.",
    "start": "3329030",
    "end": "3335670"
  },
  {
    "text": "So I've done a sparse\nmatrix multiplication that has parallelism\nproportional",
    "start": "3335670",
    "end": "3341720"
  },
  {
    "text": "to the number of\nnon-zeros, not proportional to the number of rows. And that can be quite cool if\nmy total number of non-zeros",
    "start": "3341720",
    "end": "3350750"
  },
  {
    "text": "is a lot higher than the\ntotal number of rows.",
    "start": "3350750",
    "end": "3355880"
  },
  {
    "text": "And this is going to\nbasically paralyze about as fast as map does,\nno problem, and about",
    "start": "3355880",
    "end": "3362150"
  },
  {
    "text": "as fast as segmented\nscan does, which is also going to be no\nproblem if we have good implementations\nof those things.",
    "start": "3362150",
    "end": "3367619"
  },
  {
    "text": "Yeah. How fast is the gather? That could get you a little bit. That could get you a\nlittle bit, but you",
    "start": "3367620",
    "end": "3374060"
  },
  {
    "text": "have to do the gather some-- like if you wrote\nthis in normal C code, you would have to\ngo get that gather.",
    "start": "3374060",
    "end": "3380960"
  },
  {
    "text": "The only difference here\nis that we are actually materializing that gather\nas a length n dense array,",
    "start": "3380960",
    "end": "3387380"
  },
  {
    "text": "and then iterating\nthrough the dense array. Whereas if we did it\nwith a memory access, we would never\ncreate a dense array.",
    "start": "3387380",
    "end": "3393060"
  },
  {
    "text": "We would just go\nget the data, go get the data, again and again. So this just hopefully\ngives you a taste",
    "start": "3393060",
    "end": "3399020"
  },
  {
    "text": "of how fun some of\nthese primitives can be, that you can boil\ndown irregular parallelism.",
    "start": "3399020",
    "end": "3406279"
  },
  {
    "text": "Now I can have different rows\nwith wildly different numbers of non-zeros per row. I flatten the whole\nthing, and treat it",
    "start": "3406280",
    "end": "3413630"
  },
  {
    "text": "as a big data\nparallel computation, and that can be pretty cool. That could be pretty cool.",
    "start": "3413630",
    "end": "3418270"
  },
  {
    "text": "Yeah. So I talked about this\nfictitious gathering. Really I should have thrown\nthis up earlier in the slide.",
    "start": "3421530",
    "end": "3427530"
  },
  {
    "text": "Two other really\nuseful operations are gather and scatter. And gather and scatter are\nyour data movement operations",
    "start": "3427530",
    "end": "3433440"
  },
  {
    "text": "when you're thinking\ndata parallel, so let me illustrate these for you. So gather takes an index\nsequence and a source data",
    "start": "3433440",
    "end": "3441869"
  },
  {
    "text": "sequence and produces\nan output by giving-- for every input element\nin the input sequence,",
    "start": "3441870",
    "end": "3448890"
  },
  {
    "text": "uses that as the index value,\nand grabs the data in order to densify data\nall over the place.",
    "start": "3448890",
    "end": "3455230"
  },
  {
    "text": "That's what we did for\nthat matrix, the vector x just a second ago. And then scatter is\njust the opposite.",
    "start": "3455230",
    "end": "3461140"
  },
  {
    "text": "Given a dense\nsequence in memory, and a list of indices\non where to put it,",
    "start": "3461140",
    "end": "3466740"
  },
  {
    "text": "scatter potentially sparsely\nthe data out to a bigger array.",
    "start": "3466740",
    "end": "3471990"
  },
  {
    "text": "And these can be very\ncostly operations because on one end of the\nfence, you are definitely moving data to some\nunpredictable data-dependent",
    "start": "3471990",
    "end": "3479770"
  },
  {
    "text": "place. These days, actually\non CPUs, you",
    "start": "3479770",
    "end": "3485820"
  },
  {
    "text": "have an actual AVX2,\na SIMD instruction which performs a gather, so that\nSIMD instruction takes a index",
    "start": "3485820",
    "end": "3493290"
  },
  {
    "text": "vector, so here's a vector\nregister with indices in it. It takes a pointer mem\nbase, and the result",
    "start": "3493290",
    "end": "3500580"
  },
  {
    "text": "of executing the gather is that\nit will dereference mem base sub",
    "start": "3500580",
    "end": "3505890"
  },
  {
    "text": "index element, and bring\nthat data into the result here in the vector register.",
    "start": "3505890",
    "end": "3514330"
  },
  {
    "text": "So in ISPC, when your\nprogram just says",
    "start": "3514330",
    "end": "3519900"
  },
  {
    "text": "program index food does\na sub something that's",
    "start": "3519900",
    "end": "3525390"
  },
  {
    "text": "potentially a gather operation. And if you think about\nhow this can be tricky,",
    "start": "3525390",
    "end": "3530890"
  },
  {
    "text": "these are arbitrary indices\ncan be whatever they want, so it's very possible for\nevery single lane of the vector",
    "start": "3530890",
    "end": "3538560"
  },
  {
    "text": "to trigger a cache miss\non a different cache line. It's very possible for\nevery lane of the vector",
    "start": "3538560",
    "end": "3544030"
  },
  {
    "text": "to trigger a different\npage fault handler, so this can be a nasty little\noperation to get right.",
    "start": "3544030",
    "end": "3550869"
  },
  {
    "text": "So that's why it's very nice\nthat when all of those program instances in ISPC or in CUDA\naccess the adjacent elements",
    "start": "3550870",
    "end": "3560500"
  },
  {
    "text": "of the array, you\ndon't need a gather, you can have a vector load\nwhich just says load the 8",
    "start": "3560500",
    "end": "3565630"
  },
  {
    "text": "elements starting at\nthis base pointer, so there's a big difference\nbetween vector loads and non vector loads.",
    "start": "3565630",
    "end": "3572110"
  },
  {
    "text": "There are some fun\ntricks on like-- I always when I\nlearned all this stuff, I would go through\na lot of this stuff.",
    "start": "3572110",
    "end": "3579250"
  },
  {
    "text": "If you have gather, can\nyou turn it into a scatter?",
    "start": "3579250",
    "end": "3584830"
  },
  {
    "text": "Well, it's pretty easy if\nthe scatter is actually a permutation, right? So if the scatter\nis a permutation,",
    "start": "3584830",
    "end": "3590300"
  },
  {
    "text": "meaning that all of\nthe indices are unique, and they cover\neverything, and they cover all elements of the array.",
    "start": "3590300",
    "end": "3596329"
  },
  {
    "text": "Well, scattering\naccording to an index is actually just\nsorting the data according to the index array.",
    "start": "3596330",
    "end": "3603316"
  },
  {
    "text": "So you might find\nyourself on platforms where you have a\ngather but no scatter, and so a common trick is\nto turn one into the other.",
    "start": "3603316",
    "end": "3612210"
  },
  {
    "text": "Another version if you don't\nhave scatter is pretty fun, but let's say you\ndon't have scatter, but you do have map\nand segmented scan.",
    "start": "3612210",
    "end": "3622769"
  },
  {
    "text": "So here's a scatter\nop, so what it means is scatter the value to this\nlocation, and perform an op.",
    "start": "3622770",
    "end": "3629670"
  },
  {
    "text": "It's actually pretty common if\nyou're doing like a histogram, you compute a bin that you\nneed to add in an increment one",
    "start": "3629670",
    "end": "3635490"
  },
  {
    "text": "to the bin. So look at this. OK, so for all elements\nin the sequence, what we--",
    "start": "3635490",
    "end": "3642135"
  },
  {
    "text": "the operation we want to\ndo is for all elements in the sequence, compute--",
    "start": "3642135",
    "end": "3647140"
  },
  {
    "text": "take the index, and I want to\nput the value into that target",
    "start": "3647140",
    "end": "3652410"
  },
  {
    "text": "location, but not just\nstore it there, do some op, take the current value\nand add the new value",
    "start": "3652410",
    "end": "3657450"
  },
  {
    "text": "into it or something like that. OK, so let's look at-- so the\nfirst thing I decided to do here",
    "start": "3657450",
    "end": "3662770"
  },
  {
    "text": "is I decided to sort\nthe index array.",
    "start": "3662770",
    "end": "3668290"
  },
  {
    "text": "So I have the list\nof indices where I need to put those things,\nand I decided to sort them,",
    "start": "3668290",
    "end": "3673570"
  },
  {
    "text": "and notice that those\nindices are not unique, so I'm pushing multiple\nvalues to the same location.",
    "start": "3673570",
    "end": "3679010"
  },
  {
    "text": "So you can imagine, we\nhave a parallel sort. We didn't talk about\nparallel sort today, but you can imagine that the\ndata parallel library has sort.",
    "start": "3679010",
    "end": "3685240"
  },
  {
    "text": "So now I have the input\nsorted by the index. So I did a sort of the input\ndata according to those indices.",
    "start": "3685240",
    "end": "3694380"
  },
  {
    "text": "Now, the next thing\nis I can compute the start of each\nrange of values",
    "start": "3694380",
    "end": "3700240"
  },
  {
    "text": "with the same index number. So this is my\nsorted index array,",
    "start": "3700240",
    "end": "3707619"
  },
  {
    "text": "and you could think about\ndoing some form of a map that said, OK, if I am the same\nas this one, I'm not a start.",
    "start": "3707620",
    "end": "3714740"
  },
  {
    "text": "But if I'm different from\nthis one, I am a start. So you can think\nabout in parallel,",
    "start": "3714740",
    "end": "3719920"
  },
  {
    "text": "figuring out every single\nelement of the array says, am I the start of\na new sequence?",
    "start": "3719920",
    "end": "3726010"
  },
  {
    "text": "I just made that bit vector\ncompletely in parallel. So imagine a CUDA program which\nfor every element in the vector",
    "start": "3726010",
    "end": "3734380"
  },
  {
    "text": "or in the sequence\nsays, take my thread ID, get the value in\nmy position, get",
    "start": "3734380",
    "end": "3740950"
  },
  {
    "text": "the value in the\nprevious position, and if they're different, I'm\nthe start of the sequence, so return 1, else return 0.",
    "start": "3740950",
    "end": "3746750"
  },
  {
    "text": "Perfectly parallelizable\noperation. Now, given this sequence,\nand given this sorted, now,",
    "start": "3746750",
    "end": "3753820"
  },
  {
    "text": "perform a segmented\nscan on that sequence,",
    "start": "3753820",
    "end": "3760290"
  },
  {
    "text": "and now I have all of\nthose operations inserted",
    "start": "3760290",
    "end": "3765300"
  },
  {
    "text": "into the appropriate\nmemory address. Kind of interesting. And then I guess the\nlast thing I have to do",
    "start": "3765300",
    "end": "3771350"
  },
  {
    "text": "is I would have to scatter\nthe results back out to the target locations. So now, let's try to do some\npretty interesting stuff",
    "start": "3771350",
    "end": "3777560"
  },
  {
    "text": "with just these basic\nparallel operations. And there are other\nparallel operations like filter, which\nis take a sequence,",
    "start": "3777560",
    "end": "3785339"
  },
  {
    "text": "give me a-- there's a\nfunction that's a predicate, and remove everything from the\nsequence that fit that property,",
    "start": "3785340",
    "end": "3792770"
  },
  {
    "text": "or a common operation in a lot\nof database kind of systems is given a sequence, create\na sequence of sequences.",
    "start": "3792770",
    "end": "3800730"
  },
  {
    "text": "So given a sequence of pairs,\ncreate a sequence where the--",
    "start": "3800730",
    "end": "3806450"
  },
  {
    "text": "a sequence of sequences where\nthe sequences are the key, followed by all values\nat the same key.",
    "start": "3806450",
    "end": "3813830"
  },
  {
    "text": "So imagine the key is a\ndocument and the value is a bunch of words. This would be sort all\nthe words by document,",
    "start": "3813830",
    "end": "3821317"
  },
  {
    "text": "so it's a very common operation\nin a lot of data processing stuff. OK, so let's talk\nabout why the heck I--",
    "start": "3821317",
    "end": "3827530"
  },
  {
    "text": "why I've been able\nto think in terms of reducing complex things\ninto these primitives",
    "start": "3830645",
    "end": "3835920"
  },
  {
    "text": "can be quite useful. And I'm going to give\nyou an example that is going to be shockingly\nsimilar to your homework",
    "start": "3835920",
    "end": "3841710"
  },
  {
    "text": "assignment. So here's a problem that I took\nfrom a physics application.",
    "start": "3841710",
    "end": "3848200"
  },
  {
    "text": "So imagine I have\na particle, I'm trying to do some\ngalaxy simulation or something like\nthat, and I have",
    "start": "3848200",
    "end": "3853470"
  },
  {
    "text": "all these particles\nrepresenting stars, or doing a fluid simulation. I'm representing\nfluid by particles.",
    "start": "3853470",
    "end": "3858880"
  },
  {
    "text": "So in this case, I have a bunch\nof particles that are red dots,",
    "start": "3858880",
    "end": "3863910"
  },
  {
    "text": "and the name of the game\nis, I want to create a data structure, where--",
    "start": "3863910",
    "end": "3869580"
  },
  {
    "text": "OK, and then I've divided\nspace up into 16 cells. So there's a grid over all of\nspace, 16 cells, 4 by 4 grid,",
    "start": "3869580",
    "end": "3879060"
  },
  {
    "text": "and then I have all these\nparticles at arbitrary locations x, y.",
    "start": "3879060",
    "end": "3884940"
  },
  {
    "text": "And what I want to do is I want\nto create a data structure where for every grid cell, basically,\nthis table over here,",
    "start": "3884940",
    "end": "3892450"
  },
  {
    "text": "for every grid cell, I\ncreate a list of particles that are in that cell.",
    "start": "3892450",
    "end": "3898830"
  },
  {
    "text": "So in some sense, this is a\nsequence of sequences, right? The outer sequence is of\nlink 16 and has cells.",
    "start": "3898830",
    "end": "3905020"
  },
  {
    "text": "The inner sequences\nare the number of the particle\nIDs in each cell,",
    "start": "3905020",
    "end": "3910125"
  },
  {
    "text": "so that's what we\nwant to create. I want to create a sequence of\nsequences or a grid of lists that's the data structure.",
    "start": "3910125",
    "end": "3916210"
  },
  {
    "text": "And I'd like to create this\ndata structure completely in parallel,\nassuming that I have tons of processors,\na lot of particles,",
    "start": "3916210",
    "end": "3925270"
  },
  {
    "text": "but maybe not a\nlot of grid cells. The reason why this is a cool\nproblem is this data structure",
    "start": "3925270",
    "end": "3933140"
  },
  {
    "text": "is super helpful for any\nkind of physics simulation when you want to say, I\nwant to compute the forces on this particle based\non nearby particles.",
    "start": "3933140",
    "end": "3940285"
  },
  {
    "text": "And then what you\nmight do is you say, oh, well, if\nI'm in this cell, give me only the\nneighboring cells,",
    "start": "3940285",
    "end": "3945395"
  },
  {
    "text": "and I'm only going to\niterate over those particles, so it's a common n-body\nsimulation kind of task,",
    "start": "3945395",
    "end": "3950880"
  },
  {
    "text": "so they have to make these\ndata structures all the time. OK. So let's think about\nthe dumbest, not dumb,",
    "start": "3950880",
    "end": "3958020"
  },
  {
    "text": "that's probably exactly\nwhat I would start with. Probably the smartest\nfirst implementation, which is this for\nevery particle in p,",
    "start": "3958020",
    "end": "3965880"
  },
  {
    "text": "let's assume there's\nmillions of particles, compute the cell\ncontaining p, and let's",
    "start": "3965880",
    "end": "3972020"
  },
  {
    "text": "just say I have a lock\nor something like that. Take the lock on the cell,\nor take the global lock",
    "start": "3972020",
    "end": "3978740"
  },
  {
    "text": "and append p to the cell list c. A pretty straightforward\nsolution.",
    "start": "3978740",
    "end": "3984290"
  },
  {
    "text": "What are problems, performance\nproblems with my solution?",
    "start": "3984290",
    "end": "3990970"
  },
  {
    "text": "First of all, do I have\na lot of parallelism? I have tons of them. Well, no. I mean I have tons of\nparallelism over particles.",
    "start": "3990970",
    "end": "3998520"
  },
  {
    "text": "That's true. I can parallelize that\nfor loop, but do I really have a lot of parallelism? People are saying no.",
    "start": "3998520",
    "end": "4004619"
  },
  {
    "text": "Why not? Because this parallel\nloop is basically going to synchronize\non this lock.",
    "start": "4004620",
    "end": "4010340"
  },
  {
    "text": "Absolutely. OK, how do I make\nthings a little better?",
    "start": "4010340",
    "end": "4016550"
  },
  {
    "text": "So I have contention\non a shared thing. It's the shared lock.",
    "start": "4016550",
    "end": "4022130"
  },
  {
    "text": "How do we alleviate contention? Yeah. We can just duplicate that\ncell list, like in demention",
    "start": "4022130",
    "end": "4028550"
  },
  {
    "text": "that like of you're\ngoing to re-use it. OK, so the one\nsuggestion would be,",
    "start": "4028550",
    "end": "4033700"
  },
  {
    "text": "I'm going to make a different\ncell list for every particle, for every thread, excuse me.",
    "start": "4033700",
    "end": "4040930"
  },
  {
    "text": "And then we'll just in parallel\ndeal with all the different cell lists.",
    "start": "4040930",
    "end": "4046110"
  },
  {
    "text": "Now, in this case, this is\nfor each particle in parallel, so are you going to\nmake a different cell list per particle? No.",
    "start": "4046110",
    "end": "4051460"
  },
  {
    "text": "I was saying you just\nmake, like, [INAUDIBLE]. For every worker\nthread on the machine,",
    "start": "4051460",
    "end": "4058070"
  },
  {
    "text": "let's make a\ndifferent cell list. That's one. That's a good approach, so\nwhat are the cost of that?",
    "start": "4058070",
    "end": "4063789"
  },
  {
    "text": "You have to make out all\nthose servers, and you still-- I have to allocate\nall the steroids, and then I'm going to some\nmerge process at the end.",
    "start": "4063790",
    "end": "4070520"
  },
  {
    "text": "And so if we are running\non a machine that only had a small\nnumber of processors or a small number of threads,\nI love your solution.",
    "start": "4070520",
    "end": "4076109"
  },
  {
    "text": "Looking good. If we were on a machine that\nhad a ton of processors, maybe creating 10,000 cell\nlists is not tractable,",
    "start": "4076110",
    "end": "4084015"
  },
  {
    "text": "and then maybe actually even\nmerging those is going to be a good fraction\nof execution time. Without dramatically\nchanging the code,",
    "start": "4084015",
    "end": "4090230"
  },
  {
    "text": "are there any other options? We have contention\nfor a shared lock. Have a lock for\nindividual lock cell.",
    "start": "4090230",
    "end": "4095780"
  },
  {
    "text": "So one thing we could\ndo is have a lock for every individual cell. Because if I want to update\nthe data structure in the cell,",
    "start": "4095780",
    "end": "4102317"
  },
  {
    "text": "I should really-- I'm not going to collide with\npeople touching other cells, so I could at least get 16\ntimes less contention here.",
    "start": "4102317",
    "end": "4109970"
  },
  {
    "text": "Maybe that will work, maybe\nit won't, or it certainly work with small\nnumber of processors. For a large number\nof processors,",
    "start": "4109970",
    "end": "4116370"
  },
  {
    "text": "it may not work\nbecause there's still be contention for these locks. Let's say I have 100,000 CUDA\nthreads trying to grab 16 locks,",
    "start": "4116370",
    "end": "4124399"
  },
  {
    "text": "that could be a problem. So we have massive contention,\nwhich you saw, and you told me",
    "start": "4124399",
    "end": "4131600"
  },
  {
    "text": "already that we\ncould actually move to a list of different\nlocks or start to multiple different locks. You also suggested that if we\nhave a small number of threads,",
    "start": "4131600",
    "end": "4140219"
  },
  {
    "text": "I don't quite have a-- I do have a slide\non that, but it comes later is we could just\nduplicate the data structure.",
    "start": "4140220",
    "end": "4146699"
  },
  {
    "text": "But again, both\nof these solutions are probably not going to\nscale to hundreds of thousands of threads.",
    "start": "4146700",
    "end": "4152812"
  },
  {
    "text": "If we wanted to scale\nto hundreds of thousands of threads, any other ideas?",
    "start": "4152812",
    "end": "4156025"
  },
  {
    "text": "Probably something\nlike we did earlier, like sequences of sequences. Yeah. We're Going to have to go\ndata parallel, but I want to--",
    "start": "4161125",
    "end": "4166750"
  },
  {
    "text": "See if they're all like\nequally, can we just statically divvy up the cells?",
    "start": "4166750",
    "end": "4172830"
  },
  {
    "text": "Well, there's only\n16 cell lists. So one interesting take here, by\nthe way, not a great solution,",
    "start": "4172830",
    "end": "4179859"
  },
  {
    "text": "but I actually could change\nthe axis of parallelism. I could say, I'm going to\nparalyze over all the cell",
    "start": "4179859",
    "end": "4186930"
  },
  {
    "text": "lists, and for\neach cell list, I'm going to iterate over\nall the particles. And if that particle\nis within me,",
    "start": "4186930",
    "end": "4193120"
  },
  {
    "text": "I'm just going to add\nto my own cell list. Notice how there's no\ncontention at all now.",
    "start": "4193120",
    "end": "4200300"
  },
  {
    "text": "What's my problem here? Well, one problem is\nI've defined this thing to say there's only 16 cells, so\nthat's not enough parallelism,",
    "start": "4200300",
    "end": "4207390"
  },
  {
    "text": "but there's actually an\neven more frightening thing. What if I actually\nhad 10,000 cells? I might be feeling good about\nmyself, but is there a problem?",
    "start": "4207390",
    "end": "4217961"
  },
  {
    "text": "[INAUDIBLE] Every thread on a\nunique cell is basically doing the whole problem.",
    "start": "4217961",
    "end": "4223850"
  },
  {
    "text": "It's looping over all p\nparticles, so that's trouble.",
    "start": "4223850",
    "end": "4230360"
  },
  {
    "text": "And then a fourth\nanswer was the one that you all gave\nme, which was we could duplicate the cell\nlist into multiple cell lists",
    "start": "4230360",
    "end": "4237020"
  },
  {
    "text": "and merge them at\nthe end, which I also thought was a good solution. But again, I don't think any of\nthese, the ones you're offering,",
    "start": "4237020",
    "end": "4243420"
  },
  {
    "text": "the ones I offered, none of us\nis going to get us to a good solution with 100,000 threads.",
    "start": "4243420",
    "end": "4249130"
  },
  {
    "text": "All right. So let's think about how to do\nthis in terms of data parallel concepts. So let's start here\nand let me just show",
    "start": "4249130",
    "end": "4256700"
  },
  {
    "text": "you some data representations. Let's say that we have\na particle indices,",
    "start": "4256700",
    "end": "4264130"
  },
  {
    "text": "those are my red numbers. So that's like\nparticle 0, 1, 2, 3, 4, 5, that's why I'm right here.",
    "start": "4264130",
    "end": "4270420"
  },
  {
    "text": "Now, imagine I run a\nmap on every particle, and compute what\ncell it lies in.",
    "start": "4270420",
    "end": "4277170"
  },
  {
    "text": "That's an easy\ncomputation, right? If you give me the\nx, y of a particle, I can give you the cell,\nand the results of that map",
    "start": "4277170",
    "end": "4284340"
  },
  {
    "text": "are now represented here\nin the sequence grid cell. So now I know I have\nthese particles need",
    "start": "4284340",
    "end": "4290520"
  },
  {
    "text": "to go into these cells. No communication,\nthat's pretty trivial.",
    "start": "4290520",
    "end": "4295560"
  },
  {
    "text": "And this group found that? Yeah so we're getting there. Yeah, so how are we going\nto implement this group buy? And now I'm not going\nto give you group buy,",
    "start": "4295560",
    "end": "4302559"
  },
  {
    "text": "I'm thinking about-- well, you\ncould just group buy on this, but now I want to do\nit really, really fast,",
    "start": "4302560",
    "end": "4307570"
  },
  {
    "text": "so let's think about what is\na really good implementation of group buy. OK. And to do a really good\nimplementation of group buy,",
    "start": "4307570",
    "end": "4313677"
  },
  {
    "text": "we need that to be parallel. So grouping and sorting\nare kind of the same thing,",
    "start": "4313677",
    "end": "4321220"
  },
  {
    "text": "so let's invoke a really\nhigh performance sort. OK, so now we're going to\nsort all of the indices",
    "start": "4321220",
    "end": "4328860"
  },
  {
    "text": "by the grid cell. So I did a sort here based on--",
    "start": "4328860",
    "end": "4334889"
  },
  {
    "text": "I sorted the array\ngrid cells, which if you look at the grid\ncell, you get this.",
    "start": "4334890",
    "end": "4341110"
  },
  {
    "text": "But also notice that I kept\nthe particle index with it because I need to know that this\nparticle 3 goes into grid cell",
    "start": "4341110",
    "end": "4348719"
  },
  {
    "text": "4, and particle 5\ngoes into grid cell, so I've essentially done\na group buy via sort.",
    "start": "4348720",
    "end": "4355200"
  },
  {
    "text": "So my goal is to have this\ndata structure, not just kind of there.",
    "start": "4355200",
    "end": "4360330"
  },
  {
    "text": "So now I need to\nfigure out where are the starts of the bins?",
    "start": "4360330",
    "end": "4365970"
  },
  {
    "text": "Because I don't have a sequence\nof sequences right now, I just have a sorted sequence.",
    "start": "4365970",
    "end": "4371310"
  },
  {
    "text": "So I need to find the\nstarts of the bins or the starts of\nthe subsequences.",
    "start": "4371310",
    "end": "4377190"
  },
  {
    "text": "How do I do that? I've told you. Checking like-- Yeah.",
    "start": "4377190",
    "end": "4382230"
  },
  {
    "text": "So I can run a map\non this code and say, if my value is different\nfrom my neighbors,",
    "start": "4382230",
    "end": "4388950"
  },
  {
    "text": "I'm the start of a sequence. There you go. All right.",
    "start": "4388950",
    "end": "4394410"
  },
  {
    "text": "So I'm going to run\nthat code, and just trust me that's what\nthis is because it's starting to run out of time.",
    "start": "4394410",
    "end": "4400550"
  },
  {
    "text": "And so I'm writing\nbasically, if I at index foo,",
    "start": "4400550",
    "end": "4407219"
  },
  {
    "text": "if that value is\ndifferent from the left, then my position in the array is\nwhere the start of bin foo is.",
    "start": "4407220",
    "end": "4416720"
  },
  {
    "text": "So I'll let you take a\nlook at this maybe offline, but at the end of the day, I\nend up with these cell starts",
    "start": "4416720",
    "end": "4423650"
  },
  {
    "text": "and cell ends. So my goal here this, this\nis an array of 16 values,",
    "start": "4423650",
    "end": "4431030"
  },
  {
    "text": "and this code updates some\nof those values based on whether or not the\ncurrent thread is",
    "start": "4431030",
    "end": "4439790"
  },
  {
    "text": "looking at an element that's\nthe start of the bin or not. So what I've done is I've done\na map, a sort, another map,",
    "start": "4439790",
    "end": "4448849"
  },
  {
    "text": "and I have my data\nstructure, so just confirm that this is\nthe data structure. So this is a data structure\nthat says, if you give me",
    "start": "4448850",
    "end": "4455250"
  },
  {
    "text": "a cell 0, 1, 2, 3, 4, 5,\nwhatever, if I look up there, I look up the cell\nstart and the cell end",
    "start": "4455250",
    "end": "4462210"
  },
  {
    "text": "into the original\nparticle array. And just from the diagram, the\nreason why most of these cells",
    "start": "4462210",
    "end": "4467550"
  },
  {
    "text": "are empty are because in my\nproblem, most of the cells are empty.",
    "start": "4467550",
    "end": "4473550"
  },
  {
    "text": "So this is a completely\ndata parallel way of making a uniform grid\nwhere you're parallelism",
    "start": "4473550",
    "end": "4481890"
  },
  {
    "text": "is proportional to the\nnumber of particles, and not the number of cells.",
    "start": "4481890",
    "end": "4487420"
  },
  {
    "text": "[INAUDIBLE] in the back\nprobably had his students do this a bunch of\ntimes because he loves to have particle\nlists hanging around.",
    "start": "4487420",
    "end": "4494070"
  },
  {
    "text": "Give me a particle, tell\nme what's next to it. Now, I'm going to stop. We're basically done. I'll actually give you\na couple of minutes",
    "start": "4494070",
    "end": "4499920"
  },
  {
    "text": "back, but the rest\nof this lecture, which I did not plan\nto get to, these slides are here only for your\nreference is some fun",
    "start": "4499920",
    "end": "4505980"
  },
  {
    "text": "stuff of how do you make\na histogram with only these primitives? And so here's a way\nto make a histogram?",
    "start": "4505980",
    "end": "4514500"
  },
  {
    "text": "And I think going over\nthis histogram example is pretty helpful\nactually because you",
    "start": "4514500",
    "end": "4519780"
  },
  {
    "text": "have to count the bins\nand do other things. Yeah. It seems like [INAUDIBLE]\nbefore it would be a pseudo?",
    "start": "4519780",
    "end": "4526170"
  },
  {
    "text": "Pretty close. Pretty close. Yeah. Histogram is about the same.",
    "start": "4526170",
    "end": "4531640"
  },
  {
    "text": "Yeah. But you'll use this in your--",
    "start": "4531640",
    "end": "4536850"
  },
  {
    "text": "well, the only difference\nbetween the histogram and what I just\ndid, what I just did is I found all of the particles\nthat go into the same bin.",
    "start": "4536850",
    "end": "4544380"
  },
  {
    "text": "The histogram needs to\nactually finish it off with a segmented sum\nto actually compute the total sum of those things.",
    "start": "4544380",
    "end": "4549880"
  },
  {
    "text": "It's more like the sparse matrix\nmultiply example I gave you. And when you're doing\nthe segmented scan,",
    "start": "4549880",
    "end": "4558997"
  },
  {
    "text": "you got to be a little careful\nbecause some of your bins can be empty, so there's a\nspecial case in here for what",
    "start": "4558997",
    "end": "4564660"
  },
  {
    "text": "happens when you\nhave empty bins. OK. Anyways, point of the\nday is there's just",
    "start": "4564660",
    "end": "4571637"
  },
  {
    "text": "kind of a whole other way to\nthink about parallel algorithms, which is how do I reduce\nirregular data structures,",
    "start": "4571638",
    "end": "4577690"
  },
  {
    "text": "irregular parallelism to regular\nparallelism that's embedded,",
    "start": "4577690",
    "end": "4583920"
  },
  {
    "text": "encapsulated in some library of\nthese data parallel operators? And depending on\nyour platform, you",
    "start": "4583920",
    "end": "4590820"
  },
  {
    "text": "will have different libraries. So for example, if\nyou're programming a GPU, NVIDIA has implemented this\nlibrary called Thrust, and just",
    "start": "4590820",
    "end": "4598020"
  },
  {
    "text": "go Google it, and go look\nat the API documentation, you will find things that\nlook very familiar given",
    "start": "4598020",
    "end": "4603060"
  },
  {
    "text": "these lectures. It's map, it's sort, it's\nscan, it's segmented scan, it's all of these things.",
    "start": "4603060",
    "end": "4609720"
  },
  {
    "text": "And if you don't really want\nto do crazy CUDA hacking, you can just express\nyour algorithm in terms of those\nhigh level primitives.",
    "start": "4609720",
    "end": "4616800"
  },
  {
    "text": "All of Apache\nSpark, which you may have heard of for\ndistributed programming, is based on the idea that\nall of your spark programs",
    "start": "4616800",
    "end": "4624030"
  },
  {
    "text": "are done in terms of\nthese sequence operators. Their sequences are\ncalled these RDDs.",
    "start": "4624030",
    "end": "4629580"
  },
  {
    "text": "An RDD is a sequence,\nand they only give you this set of operations\nto do things on RDDs.",
    "start": "4629580",
    "end": "4635860"
  },
  {
    "text": "And the whole premise of spark\nis that if you use these RDDs, you get parallelism\nacross a cluster,",
    "start": "4635860",
    "end": "4641290"
  },
  {
    "text": "you get fault tolerance\nand redundancy, and these operations are enough\nto do a whole bunch of cool data",
    "start": "4641290",
    "end": "4646950"
  },
  {
    "text": "analytics, and so that's why\nSpark got so popular, but just different\nimplementation, basically the same idea as\nThrust, so you'll",
    "start": "4646950",
    "end": "4654300"
  },
  {
    "text": "get a little bit used to this\nin programming assignment 3. All right. OK. Good luck finishing\nup 2, and we'll",
    "start": "4654300",
    "end": "4661260"
  },
  {
    "text": "be shipping you\nassignment on Monday.",
    "start": "4661260",
    "end": "4663739"
  }
]