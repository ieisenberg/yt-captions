[
  {
    "start": "0",
    "end": "18000"
  },
  {
    "text": "Okay. Let's get started again. Um, so before I, uh, proceed to Bayesian networks,",
    "start": "5510",
    "end": "11520"
  },
  {
    "text": "I'm gonna talk about a few announcements. So as you probably know, car is due tomorrow, um, p-progress is, uh,",
    "start": "11520",
    "end": "18915"
  },
  {
    "start": "18000",
    "end": "120000"
  },
  {
    "text": "due Thursday, um, and then finally the, the kind of the big one, uh, is the exam which is next Tuesday.",
    "start": "18915",
    "end": "26145"
  },
  {
    "text": "So everyone, if you're in the room or if you're watching from home, please remember to go to the exam.",
    "start": "26145",
    "end": "32535"
  },
  {
    "text": "Um, um, the exam will cover all the material from the beginning of the class up until and including today's lecture.",
    "start": "32535",
    "end": "40280"
  },
  {
    "text": "So all Bayesian networks, no logic. Um, it doesn't mean you shouldn't use logic on the exam.",
    "start": "40280",
    "end": "46340"
  },
  {
    "text": "Uh, reviews are going to be in section, uh, this Thursday and Saturday.",
    "start": "46340",
    "end": "52460"
  },
  {
    "text": "So you've got not one, but two review sessions. The first one we'll talk about, uh, reflex and state-based models and the second we'll talk about variable risk models.",
    "start": "52460",
    "end": "60980"
  },
  {
    "text": "Um, at this point, all the alternative exams have been, uh, scheduled.",
    "start": "60980",
    "end": "66200"
  },
  {
    "text": "Um, if you can't make the exam for some reason, then, uh, really please come talk to us right now.",
    "start": "66200",
    "end": "73935"
  },
  {
    "text": "Um, and just a final note is that the exam is, uh, the problems are a little bit different than the homework problems.",
    "start": "73935",
    "end": "81695"
  },
  {
    "text": "They require more kind of problem-solving, and the best way to prepare for the exam is to",
    "start": "81695",
    "end": "87170"
  },
  {
    "text": "really go through the old exams and get a sense for what kind of, uh, uh, problems and questions you're gonna be asked. Question?",
    "start": "87170",
    "end": "96200"
  },
  {
    "text": "Where is the Saturday session? Where is the Saturday session, does anyone know? We don't know yet. We don't know yet. It will be posted on Piazza.",
    "start": "96200",
    "end": "104290"
  },
  {
    "text": "Any other questions about anything? I know this is, uh, a lot of stuff, um,",
    "start": "105050",
    "end": "113455"
  },
  {
    "text": "but this is probably going to be the busiest week, and then you get Thanksgiving break. So that will be great.",
    "start": "113455",
    "end": "119735"
  },
  {
    "text": "Okay. So let's jump in. So two lectures ago,",
    "start": "119735",
    "end": "124860"
  },
  {
    "start": "120000",
    "end": "177000"
  },
  {
    "text": "we started introducing Bayesian networks. So Bayesian networks is a modeling paradigm where you define a set of variables,",
    "start": "124860",
    "end": "131480"
  },
  {
    "text": "which capture the state of the world, and you specify dependencies depicted as directed edges between these variables,",
    "start": "131480",
    "end": "139504"
  },
  {
    "text": "and given the graph structure, then you proceed to define distribution. So first, you define a co- local conditional distribution for every variable given the parents.",
    "start": "139505",
    "end": "149630"
  },
  {
    "text": "So for example, H given C and A and I given A, and then you slam all these local conditional distributions, aka factors together,",
    "start": "149630",
    "end": "159830"
  },
  {
    "text": "and it defines a glorious joint distribution over all the random variables, okay?",
    "start": "159830",
    "end": "165510"
  },
  {
    "text": "And you think about the joint distribution as the source of all truth. It is like a probabilistic database,",
    "start": "165510",
    "end": "170855"
  },
  {
    "text": "a guru or oracle that can be used to answer queries. So this- in the last lecture,",
    "start": "170855",
    "end": "176965"
  },
  {
    "text": "we talked about algorithms for doing probabilistic inference where you're given a Bayesian network,",
    "start": "176965",
    "end": "182195"
  },
  {
    "start": "177000",
    "end": "253000"
  },
  {
    "text": "which defines this glorious joint distribution, and you're asked a number of questions, and questions looked like this: Condition on some evidence,",
    "start": "182195",
    "end": "190974"
  },
  {
    "text": "which is a subset of the variables that you have observed, what is the distribution over some other subset of the variables which you didn't observe?",
    "start": "190974",
    "end": "199190"
  },
  {
    "text": "Then we look at a number of different algorithms in the last lecture. There is the forward-backward algorithm,",
    "start": "199190",
    "end": "205600"
  },
  {
    "text": "which was useful for doing filtering and smoothing queries in HMMs, this was an exact algorithm,",
    "start": "205600",
    "end": "211595"
  },
  {
    "text": "then we looked at particle filtering which happens, uh, to be useful in a case where your state space,",
    "start": "211595",
    "end": "216970"
  },
  {
    "text": "the number of varia- uh, the values that a variable can take on, it can be very large. Um, and this is an approximate,",
    "start": "216970",
    "end": "223330"
  },
  {
    "text": "but in practice it tends to be a good approximation. Um, and then finally, particle filtering which- sorry,",
    "start": "223330",
    "end": "229459"
  },
  {
    "text": "and Gibbs sampling, which is this much more general, uh, framework for doing probabilistic inference in, um,",
    "start": "229460",
    "end": "235715"
  },
  {
    "text": "arbitrary, uh, factor graphs, and this was again approximate, okay? So so far, we've, uh, bit off two pieces of this,",
    "start": "235715",
    "end": "244350"
  },
  {
    "text": "uh, modeling inference learning triangle. We've talked about models, we've talked about inference algorithms,",
    "start": "244350",
    "end": "249680"
  },
  {
    "text": "and finally today we're gonna talk about learning. And so the question in learning,",
    "start": "249680",
    "end": "255405"
  },
  {
    "start": "253000",
    "end": "277000"
  },
  {
    "text": "as we've seen repeatedly throughout this, uh, course, is: Where do all the parameters come from?",
    "start": "255405",
    "end": "260664"
  },
  {
    "text": "So so far, we have assumed that someone just hands you this, these local conditional distributions which,",
    "start": "260665",
    "end": "266360"
  },
  {
    "text": "uh, have numbers filled out. But in the real world, where do you get these? And we're gonna consider a setting where all of these parameters are",
    "start": "266360",
    "end": "273730"
  },
  {
    "text": "unknown and we have to figure out what they are from data, okay? So the roadmap, we're gonna start- first start with supervised learning,",
    "start": "273730",
    "end": "281280"
  },
  {
    "start": "277000",
    "end": "302000"
  },
  {
    "text": "which is going to be the kind of the easiest, easiest case, and then we're gonna move to unsupervised learning where some of the,",
    "start": "281280",
    "end": "288340"
  },
  {
    "text": "uh, variables are gonna be unobserved, okay? Any questions before we dive in?",
    "start": "288340",
    "end": "294270"
  },
  {
    "text": "[NOISE] Okay.",
    "start": "294270",
    "end": "301830"
  },
  {
    "text": "Let's do it. So the problem of learning is as follows. We're given training data and we want to turn this into parameters, okay?",
    "start": "301830",
    "end": "312060"
  },
  {
    "start": "302000",
    "end": "389000"
  },
  {
    "text": "So for the specific instance of, uh, Bayesian networks, training data looks like an example is an assignment to all the variables, okay?",
    "start": "312060",
    "end": "323020"
  },
  {
    "text": "This will become clear in an example. And the parameters are all those local conditional probabilities that",
    "start": "323020",
    "end": "329290"
  },
  {
    "text": "we now assume we don't know, okay? So here's a question: Which is more computationally more expensive for Bayesian networks?",
    "start": "329290",
    "end": "339625"
  },
  {
    "text": "Um, is it probabilistic inference given, uh, the parameters or learning the parameters given data?",
    "start": "339625",
    "end": "346000"
  },
  {
    "text": "Um, so how many of you- just use your intuition, how many of you think it's the former, probabilistic inference is more expensive?",
    "start": "346000",
    "end": "353290"
  },
  {
    "text": "Okay. One maybe. How about how many of you think learning is more expensive?",
    "start": "353780",
    "end": "359650"
  },
  {
    "text": "Yeah. That's probably what you would think, right? Because learning seems like there's just more unknowns,",
    "start": "359650",
    "end": "364715"
  },
  {
    "text": "um, and it can't- how can possibly it be, um, easier. It turns out that it's actually the opposite. Yeah. So good job.",
    "start": "364715",
    "end": "371430"
  },
  {
    "text": "[LAUGHTER] Um, and, this will hopefully be a relief to many of you, um, because, uh, I know probabilistic inference gets a little bit, uh,",
    "start": "371430",
    "end": "379850"
  },
  {
    "text": "quite intense sometimes and learning will actually be, um, maybe not quite a walk in the park, but, uh,",
    "start": "379850",
    "end": "385740"
  },
  {
    "text": "it will be, um, uh, a brisk stroll in the park. Um, and then when we come back to unsupervised learning, it's gonna get hard again.",
    "start": "385740",
    "end": "393410"
  },
  {
    "start": "389000",
    "end": "726000"
  },
  {
    "text": "So, um, at least in the fully supervised setting, it should be easy and intuitive.",
    "start": "393410",
    "end": "398525"
  },
  {
    "text": "So what I'm gonna do now is going to build up to the general algorithm by a series of examples of increasingly complex Bayesian networks.",
    "start": "398525",
    "end": "408230"
  },
  {
    "text": "So here's the world's simplest Bayesian network. It has one variable in it. [NOISE] Let's assume that variable represents the rating of a movie.",
    "start": "408230",
    "end": "416590"
  },
  {
    "text": "So it takes on values 1 through 5. And to specify the local conditional distribution,",
    "start": "416590",
    "end": "423520"
  },
  {
    "text": "you just have to specify the probability of 1, the probability of 2, the probability of 3,",
    "start": "423520",
    "end": "428860"
  },
  {
    "text": "probability of 4, and probability of 5, okay? So these five numbers represents the parameters of the Bayesian network, okay?",
    "start": "428860",
    "end": "437370"
  },
  {
    "text": "By fiddling these, I can get different distributions. Okay. So suppose someone hands you this training data.",
    "start": "437370",
    "end": "444310"
  },
  {
    "text": "So it's fully observed, which means I observed- one time I observed the value of R to be 1,",
    "start": "444310",
    "end": "450785"
  },
  {
    "text": "um, the second day I observed the value to be 3, third day I observed it to be 4, and so on, okay?",
    "start": "450785",
    "end": "456650"
  },
  {
    "text": "So this is your training data, okay? So now the question is: How do you go from the training data",
    "start": "456650",
    "end": "463070"
  },
  {
    "text": "here to the parameters, okay? Any ideas? Just use your gut, what does your gut say?",
    "start": "463070",
    "end": "470490"
  },
  {
    "text": "Count the number of [NOISE] and then divide them. Yeah, count and then divide or normalize, okay?",
    "start": "470490",
    "end": "477530"
  },
  {
    "text": "So this seems a very natural intuition. Um, later, I'll justify why this is a sensible thing to do.",
    "start": "477530",
    "end": "484340"
  },
  {
    "text": "But for now, let's just use your gut and see how far we can get. Okay. So the intuition is that, um,",
    "start": "484340",
    "end": "490550"
  },
  {
    "text": "the probability of some value of r should be proportional to the number of times it occurs in the training data.",
    "start": "490550",
    "end": "496310"
  },
  {
    "text": "So in this particular example, I look at all the possible values of r 1 through 5 and I just see 1 shows up once,",
    "start": "496310",
    "end": "503435"
  },
  {
    "text": "2 shows up 0 times, 4 shows up 5 times, and so on. So these are the counts of the particular values of r,",
    "start": "503435",
    "end": "511264"
  },
  {
    "text": "and then I simply normalize, okay? So normalization means adding the counts together,",
    "start": "511265",
    "end": "517219"
  },
  {
    "text": "which gives you 10, and dividing by 10, which gives you actual probabilities, okay?",
    "start": "517220",
    "end": "526255"
  },
  {
    "text": "It's pretty easy, huh? Yeah? Good. Okay. So let's go to two variables.",
    "start": "526255",
    "end": "532285"
  },
  {
    "text": "So now you improve your model of, uh, ratings. So now you take into account the genre.",
    "start": "532285",
    "end": "539050"
  },
  {
    "text": "So genre is a variable G which can take on two values, drama or comedy, and the rating is the same variable as before.",
    "start": "539050",
    "end": "545365"
  },
  {
    "text": "And now let's draw the simple Bayesian network which has two variables, um,",
    "start": "545365",
    "end": "550580"
  },
  {
    "text": "and the local conditional distributions are P of G and P of R given G, okay?",
    "start": "550580",
    "end": "555870"
  },
  {
    "text": "So again, I'm gonna give you some training data which specif- each training example specifies a full assignment to all the variables.",
    "start": "555870",
    "end": "562555"
  },
  {
    "text": "So in this case, it's d, 4, d- and this one it's d, 4 again, uh, this one is c, 1 and so on.",
    "start": "562555",
    "end": "568985"
  },
  {
    "text": "And the parameters here are, uh, the local conditional distributions,",
    "start": "568985",
    "end": "574470"
  },
  {
    "text": "which is P of G and P of R given G, okay?",
    "start": "574470",
    "end": "579569"
  },
  {
    "text": "Okay. So let's proceed to do this, um, and here following our nose again.",
    "start": "579570",
    "end": "585525"
  },
  {
    "text": "Uh, we're going to estimate each local conditional distribution separately, okay?",
    "start": "585525",
    "end": "591725"
  },
  {
    "text": "So this is may not be obvious why separate- doing it separately is the right thing to do.",
    "start": "591725",
    "end": "598220"
  },
  {
    "text": "But trust me, it is the right thing to do and it certainly is the easy thing to do. So let's, uh, just do that for now.",
    "start": "598220",
    "end": "603235"
  },
  {
    "text": "Okay. So again, loo- if we look at P of G, so we have to look at only this data restricted",
    "start": "603235",
    "end": "610730"
  },
  {
    "text": "to G and we see that d shows up three times, c shows up twice so I get these counts and I normalize,",
    "start": "610730",
    "end": "618529"
  },
  {
    "text": "and that's my estimate of the probability of G, okay? And now let's look at the conditional distribution R given G. Again,",
    "start": "618530",
    "end": "628610"
  },
  {
    "text": "I'm going to count up the number of times that these variables, uh, G and R show up.",
    "start": "628610",
    "end": "633890"
  },
  {
    "text": "So d, 4 shows up twice; d, 5 shows up once and so on;",
    "start": "633890",
    "end": "639055"
  },
  {
    "text": "count and normalize. Question? So if, if instead of R given G had-",
    "start": "639055",
    "end": "647165"
  },
  {
    "text": "The probability R of r, would that cause any differences in the results, like, if the order [NOISE] were changing?",
    "start": "647165",
    "end": "653710"
  },
  {
    "text": "Yeah. That's a good question. So the question is, what happens if we define our model in",
    "start": "653710",
    "end": "659080"
  },
  {
    "text": "the opposite direction where we have P of R and P of, uh, uh, G given R?",
    "start": "659080",
    "end": "664480"
  },
  {
    "text": "So then you would be estimating different probabilities. You would be estimating P of R, which contains 1 through 5,",
    "start": "664480",
    "end": "670089"
  },
  {
    "text": "and then P of, uh, G given R, which would be, you know, a different table.",
    "start": "670090",
    "end": "675400"
  },
  {
    "text": "Do you get the same results? Uh, so the question is, If you do inference,",
    "start": "675400",
    "end": "680709"
  },
  {
    "text": "do you get the same results? Um, in this case, um, you will get the same results.",
    "start": "680710",
    "end": "687070"
  },
  {
    "text": "In general, you're not. Uh, you're- depending on the, the model you define,",
    "start": "687070",
    "end": "692709"
  },
  {
    "text": "you're actually gonna get a different, uh, distribution, um, which will lead to different inference results.",
    "start": "692710",
    "end": "698935"
  },
  {
    "text": "Yeah. This happens when you have two variables and you couple them together and you do things like this way.",
    "start": "698935",
    "end": "705190"
  },
  {
    "text": "You're effectively estimating from the space of all joint distributions G and R. But if you have, uh,",
    "start": "705190",
    "end": "712090"
  },
  {
    "text": "conditional independence and you have, for example, HMM, the order, how you write the model definitely affects what inferences can return.",
    "start": "712090",
    "end": "720590"
  },
  {
    "text": "Okay. All right. So, so far, so good.",
    "start": "722550",
    "end": "728769"
  },
  {
    "start": "726000",
    "end": "888000"
  },
  {
    "text": "So now let's add another variable, okay? So, uh, so now we have a variable A,",
    "start": "728770",
    "end": "734574"
  },
  {
    "text": "which represents whether a movie won an award or not. Um, we're going to draw this Bayesian network,",
    "start": "734575",
    "end": "741430"
  },
  {
    "text": "um, and the joint distribution is P_G, P_A, P_R given A and G, okay?",
    "start": "741430",
    "end": "747595"
  },
  {
    "text": "So this type of structure is called a V structure, uh, for obvious reasons, and this remember was the thing that was tricky.",
    "start": "747595",
    "end": "755980"
  },
  {
    "text": "This is the thing that leads to explaining away and, um, all sorts of tricky things in Bayesian networks.",
    "start": "755980",
    "end": "761800"
  },
  {
    "text": "It turns out that when you're doing learning in them, um, it's actually the- all other trickery goes away.",
    "start": "761800",
    "end": "767680"
  },
  {
    "text": "Okay. So, um, let's just do, uh, the same thing as before. So we get this data.",
    "start": "767680",
    "end": "773980"
  },
  {
    "text": "Each data point, full assignment to all the variables. So this is d, 0, 3 corresponds to G equals d and, um,",
    "start": "773980",
    "end": "782500"
  },
  {
    "text": "A equals 0, and R equals 3, and the parameters are all again all the local conditional distributions.",
    "start": "782500",
    "end": "788800"
  },
  {
    "text": "Um, and now I'm gonna count and normalize. So this part was the same as before, counts in, uh,",
    "start": "788800",
    "end": "795055"
  },
  {
    "text": "the genres, d shows up three times, c shows up twice, normalize. Um, A is treated very much the same way.",
    "start": "795055",
    "end": "802089"
  },
  {
    "text": "So, um, three movies have won no awards, two movies have won one award.",
    "start": "802090",
    "end": "808375"
  },
  {
    "text": "So the probability of a movie winning award is 2 out of 5. And then finally, um,",
    "start": "808375",
    "end": "815305"
  },
  {
    "text": "this is the local conditional distribution of R given G and A, and here we have to specify for all combinations",
    "start": "815305",
    "end": "823780"
  },
  {
    "text": "of all the variables mentioned in that local conditional distribution. I'm gonna count the number of times that configuration shows up.",
    "start": "823780",
    "end": "831040"
  },
  {
    "text": "Um, so d, 0, 1 shows up once right here; d, 0, 3 shows up once,",
    "start": "831040",
    "end": "837310"
  },
  {
    "text": "right here and so on, okay? And now when you normalize, you just have to be careful that you're normalizing only over the variable,",
    "start": "837310",
    "end": "846430"
  },
  {
    "text": "uh, that you're, uh, the local distribution is on; in this case, R. So for every possible unique setting of G and A,",
    "start": "846430",
    "end": "854275"
  },
  {
    "text": "I have a different distribution, okay? So d, 0, that's a distribution over 1 and 3.",
    "start": "854275",
    "end": "861519"
  },
  {
    "text": "And if I normalize, I get half and a half and each of these other ones are completely separate because they have different values of G and A.",
    "start": "861520",
    "end": "870500"
  },
  {
    "text": "Okay. Any questions?",
    "start": "875220",
    "end": "878449"
  },
  {
    "text": "All good? All right. So that wasn't too bad.",
    "start": "883500",
    "end": "888565"
  },
  {
    "start": "888000",
    "end": "1228000"
  },
  {
    "text": "Um, so now let's invert the V and look at this structure, where now we have genre,",
    "start": "888565",
    "end": "895015"
  },
  {
    "text": "there's no award variable, but instead we have two people rating a movie and the Bayesian network looks like this.",
    "start": "895015",
    "end": "901644"
  },
  {
    "text": "The genre and we have R_1 given G and R_2 given G. And for now,",
    "start": "901645",
    "end": "909340"
  },
  {
    "text": "I'm going to assume that these are different, uh, local conditional distributions. So we'll have PR_1 and P of R_2.",
    "start": "909340",
    "end": "916420"
  },
  {
    "text": "So notice that in this lecture, I'm being very explicit about the local conditional distribution.",
    "start": "916420",
    "end": "922960"
  },
  {
    "text": "Here, I'm instead of just writing P of G, which can be ambiguous, I'm writing P of G to refer to the fact that this is the",
    "start": "922960",
    "end": "930010"
  },
  {
    "text": "local to conditional distribution for variable G. Um, this one's the one for R_1,",
    "start": "930010",
    "end": "935110"
  },
  {
    "text": "this one for R_2, and those are different. And you'll see later why this, uh, this matters.",
    "start": "935110",
    "end": "940840"
  },
  {
    "text": "Okay. So for now, we're gonna go through the same motions. Hopefully, this should be, um,",
    "start": "940840",
    "end": "947410"
  },
  {
    "text": "fairly, um, intuitive by now. You simply count, normalize,",
    "start": "947410",
    "end": "953245"
  },
  {
    "text": "um, for the P of G and then for R_1, I'm just going to look at the first, uh,",
    "start": "953245",
    "end": "960010"
  },
  {
    "text": "or the second element here which corresponds to R_1 and ignore R_2, right?",
    "start": "960010",
    "end": "965155"
  },
  {
    "text": "So G R_1. So d, 4 shows up twice. So I have d, 4, uh, and d,",
    "start": "965155",
    "end": "971709"
  },
  {
    "text": "4 that shows up twice; d, 5 shows up once, that's the d, 5; c, 1 shows up once;",
    "start": "971710",
    "end": "977500"
  },
  {
    "text": "and c, 5 shows up once, okay? And then normalize, get my distribution, and then same for R_2.",
    "start": "977500",
    "end": "985075"
  },
  {
    "text": "Now ignoring R_1, I'm gonna look at how many times did G equals d and R_2 equals 3, okay?",
    "start": "985075",
    "end": "994825"
  },
  {
    "text": "So you can think about each of these as a kind of a pattern that you sweep over the training set. So, um, you have d,",
    "start": "994825",
    "end": "1001740"
  },
  {
    "text": "5 so that's a 1 here; and d, 4 that's a 1 here; and d,",
    "start": "1001740",
    "end": "1008400"
  },
  {
    "text": "3 that's a 1 here and so on and so forth, okay? And then you normalize, okay?",
    "start": "1008400",
    "end": "1018370"
  },
  {
    "text": "How many of you are following this? Okay. Cool. All right.",
    "start": "1019820",
    "end": "1026415"
  },
  {
    "text": "So now things- um, I'm gonna make things a little bit more interesting. [NOISE] So here I've defined",
    "start": "1026415",
    "end": "1034740"
  },
  {
    "text": "different local conditional distributions for each rating variable R_1 and R_2, right?",
    "start": "1034740",
    "end": "1039990"
  },
  {
    "text": "But in general, maybe I have R_3, and R_4, and R_5, and I have maybe a thousand people who are rating this movie.",
    "start": "1039990",
    "end": "1047640"
  },
  {
    "text": "I don't really want to have a separate var- distribution for each person.",
    "start": "1047640",
    "end": "1052785"
  },
  {
    "text": "So in this model, I'm going to define a single distribution over rating conditioned on genre called PR,",
    "start": "1052785",
    "end": "1061215"
  },
  {
    "text": "and this is where subscripting with the actual identity of",
    "start": "1061215",
    "end": "1066570"
  },
  {
    "text": "the local conditional distribution becomes useful and this allows us to distinguish PR from this case,",
    "start": "1066570",
    "end": "1072990"
  },
  {
    "text": "which is PR_1 and PR_2, okay? Notice that the, the structure of the graph remains the same.",
    "start": "1072990",
    "end": "1082500"
  },
  {
    "text": "So you can't tell from just looking at the Bayesian network. Um, you have to look at carefully at the parameterization.",
    "start": "1082500",
    "end": "1089980"
  },
  {
    "text": "Okay. So if I just have one PR, what I'm going to do is,",
    "start": "1090440",
    "end": "1099015"
  },
  {
    "text": "um, what do you think the right thing should to do with this if you're just following your nose? [inaudible].",
    "start": "1099015",
    "end": "1109410"
  },
  {
    "text": "I'm sorry? [inaudible]. Yeah, so count both of them, I think is what you're saying.",
    "start": "1109410",
    "end": "1116630"
  },
  {
    "text": "So you combine them, right? So, um, so P of G is the same,",
    "start": "1116630",
    "end": "1122990"
  },
  {
    "text": "and now I'm going- I only have one distribution I need to estimate, uh, here r given g. Um,",
    "start": "1122990",
    "end": "1130070"
  },
  {
    "text": "and I'm gonna count the number of times in the data where I'm using- uh,",
    "start": "1130070",
    "end": "1135710"
  },
  {
    "text": "I have a particular value of g and a particular value of r. Okay? And I'm going to look at both R_1 and R_2 now.",
    "start": "1135710",
    "end": "1143720"
  },
  {
    "text": "Okay? So D_3 shows up once here. So that's, uh, R_2,",
    "start": "1143720",
    "end": "1149660"
  },
  {
    "text": "D_4 shows up three times. So once here with R_1, once here with R_1,",
    "start": "1149660",
    "end": "1155570"
  },
  {
    "text": "and once here with R_2, and so on. I'm not gonna go through all the details,",
    "start": "1155570",
    "end": "1160865"
  },
  {
    "text": "and then you count and normalize. Um, another way you can see this is that if I take the counts from,",
    "start": "1160865",
    "end": "1167045"
  },
  {
    "text": "um, um, these two tables, I'm just kind of merging them, adding them together, um, and then,",
    "start": "1167045",
    "end": "1175684"
  },
  {
    "text": "um, normalizing. Question? [BACKGROUND].",
    "start": "1175685",
    "end": "1181310"
  },
  {
    "text": "If the Rs are IID, are they drawn from the same conclusion, or?",
    "start": "1181310",
    "end": "1186550"
  },
  {
    "text": "Yeah, so is this assuming something about independence here? Um, so here when I am doing- I am assuming the data points are independent first of all,",
    "start": "1186550",
    "end": "1198184"
  },
  {
    "text": "and moreover, I'm also assuming the conditional independent structure of the Bayesian network is true.",
    "start": "1198185",
    "end": "1204259"
  },
  {
    "text": "So conditioned on g, R_1 and R_2 are independent. [BACKGROUND]",
    "start": "1204260",
    "end": "1211940"
  },
  {
    "text": "Yes, so here I'm also assuming that R_1 given g has the same distribution as R_2 given g,",
    "start": "1211940",
    "end": "1218105"
  },
  {
    "text": "and this is part of the-when I define the model that way I'm, you know, making that assumption.",
    "start": "1218105",
    "end": "1223159"
  },
  {
    "text": "[NOISE] Okay, so this is a general idea which I want to call out,",
    "start": "1223160",
    "end": "1230210"
  },
  {
    "start": "1228000",
    "end": "1295000"
  },
  {
    "text": "which is called, uh, parameter sharing. Um, and parameter sharing is when the local conditional distribution of different variables use the same parameters.",
    "start": "1230210",
    "end": "1239600"
  },
  {
    "text": "So the way to think about, uh, parameter sharing is in terms of, uh, powering.",
    "start": "1239600",
    "end": "1245815"
  },
  {
    "text": "So you have this Bayesian network, it's hanging out here. And behind the scenes,",
    "start": "1245815",
    "end": "1250915"
  },
  {
    "text": "the parameters are kind of driving it, right? And you can think about the, the parameters as these little tables sitting out there,",
    "start": "1250915",
    "end": "1259035"
  },
  {
    "text": "and you connect a table up to a variable if you say this table is going to power, uh, this node.",
    "start": "1259035",
    "end": "1266540"
  },
  {
    "text": "Okay? So, uh, what parameter sharing in this particular example was saying, this distribution of G powers this node and these two variables,",
    "start": "1266540",
    "end": "1276740"
  },
  {
    "text": "R_1 and R_2, are going to be powered by the same co-local conditional distribution.",
    "start": "1276740",
    "end": "1282720"
  },
  {
    "text": "Okay?",
    "start": "1283210",
    "end": "1285480"
  },
  {
    "text": "So, um, okay,",
    "start": "1289330",
    "end": "1294365"
  },
  {
    "text": "I'm gonna go to two [NOISE] more examples. Maybe I'll draw them on the board to make this a little bit more clear. So remember the naive based model,",
    "start": "1294365",
    "end": "1301520"
  },
  {
    "start": "1295000",
    "end": "1565000"
  },
  {
    "text": "uh, from, uh, two lectures ago. So this is a model that has a variable that represents- adapted to,",
    "start": "1301520",
    "end": "1309125"
  },
  {
    "text": "uh, movie genre setting. We have a genre variable which takes on var-values comedy and drama,",
    "start": "1309125",
    "end": "1315485"
  },
  {
    "text": "and then I have a movie review which represents a document with L words in it and each word is,",
    "start": "1315485",
    "end": "1323960"
  },
  {
    "text": "um, drawn independently given, uh, y. So I have said the joint probability is therefore going to",
    "start": "1323960",
    "end": "1331250"
  },
  {
    "text": "be probability of genre y times the probability of, uh, w_j given y for all, uh,",
    "start": "1331250",
    "end": "1338480"
  },
  {
    "text": "words j from 1 to L. Okay? So, so the way to think about this, um,",
    "start": "1338480",
    "end": "1346110"
  },
  {
    "text": "graph the Bayesian networks is, um, you have a variable Y here,",
    "start": "1346720",
    "end": "1352985"
  },
  {
    "text": "and so I'm just gonna draw W_1, W_2, W_3,",
    "start": "1352985",
    "end": "1360155"
  },
  {
    "text": "and, um, I'm going to have a local conditional distribution here which is y P genre of y.",
    "start": "1360155",
    "end": "1371600"
  },
  {
    "text": "So that's some table that's powering this node, and then I have a separate one single other of variable, um, sorry,",
    "start": "1371600",
    "end": "1381035"
  },
  {
    "text": "local conditional distribution of, uh, w, uh, y and w, and, um,",
    "start": "1381035",
    "end": "1388445"
  },
  {
    "text": "probability of what I call it word, word, um,",
    "start": "1388445",
    "end": "1394460"
  },
  {
    "text": "W given y, and",
    "start": "1394460",
    "end": "1399730"
  },
  {
    "text": "this distribution powers, um, these variables.",
    "start": "1399730",
    "end": "1405309"
  },
  {
    "text": "Okay? So notice that, um, here there's two, uh, local conditional distributions,",
    "start": "1405310",
    "end": "1413270"
  },
  {
    "text": "we have genre and we have word, even though there are l plus 1, uh,",
    "start": "1413270",
    "end": "1418415"
  },
  {
    "text": "variables in the Bayesian network. Yeah.",
    "start": "1418415",
    "end": "1425930"
  },
  {
    "text": "[BACKGROUND]",
    "start": "1425930",
    "end": "1433520"
  },
  {
    "text": "Yeah, so the input, what are- so the question is what is the input to, uh, P word W given y, uh,",
    "start": "1433520",
    "end": "1441320"
  },
  {
    "text": "so when you apply this to a particular variable, um,",
    "start": "1441320",
    "end": "1447440"
  },
  {
    "text": "in some sense you bind Y to Y and you bind W to a particular Wi at hand.",
    "start": "1447440",
    "end": "1454460"
  },
  {
    "text": "[BACKGROUND] [inaudible]. Exactly. And you can see this kind of mathematically,",
    "start": "1454460",
    "end": "1461660"
  },
  {
    "text": "where you hear we have, um, you pass into the P word, W j given y and j ranges from 1 to l. [BACKGROUND].",
    "start": "1461660",
    "end": "1476270"
  },
  {
    "text": "Okay. Um, just to kind of solidify understanding,",
    "start": "1476270",
    "end": "1483680"
  },
  {
    "text": "uh, let me ask the following question. If, um, y can take on two values as in here and each word can take on,",
    "start": "1483680",
    "end": "1493460"
  },
  {
    "text": "um, d values, how many parameters are there? In other words, how many numbers are in these,",
    "start": "1493460",
    "end": "1500240"
  },
  {
    "text": "uh, two tables on the board? So shout out the answer [BACKGROUND]",
    "start": "1500240",
    "end": "1507380"
  },
  {
    "text": "to the [inaudible] right.",
    "start": "1507380",
    "end": "1519065"
  },
  {
    "text": "Okay, okay. So 2D plus 2, so there's two here, right, so there's C_B.",
    "start": "1519065",
    "end": "1524300"
  },
  {
    "text": "So there's two numbers, and then there are for every C,",
    "start": "1524300",
    "end": "1530330"
  },
  {
    "text": "there's d possible values, for d there're d possible values. So there's 2 plus, uh,",
    "start": "1530330",
    "end": "1537304"
  },
  {
    "text": "so there's two here, and then there's, uh, d plus d here.",
    "start": "1537305",
    "end": "1542420"
  },
  {
    "text": "Okay? Now if you really want to be fancy and count the number of parameters you really need,",
    "start": "1542420",
    "end": "1549080"
  },
  {
    "text": "it should be less and less because if I specify the probability of C, then you can, uh,",
    "start": "1549080",
    "end": "1554255"
  },
  {
    "text": "1 minus that probability is probability of D, but let's not worry about that. [BACKGROUND]",
    "start": "1554255",
    "end": "1562304"
  },
  {
    "text": "Okay, let's do the same thing for HMMs, um, just to make sure we're on the same page here.",
    "start": "1562305",
    "end": "1569085"
  },
  {
    "start": "1565000",
    "end": "2020000"
  },
  {
    "text": "Since we all love HMMs, um, okay. So in HMM, remember there is a sequence of hidden variables H_1, H_2, H_3.",
    "start": "1569085",
    "end": "1580274"
  },
  {
    "text": "Um, and for each hidden variable, we have E_1, um,",
    "start": "1580275",
    "end": "1585420"
  },
  {
    "text": "E_2, and E_3 which are the observed variables. And, um, there are three types of,",
    "start": "1585420",
    "end": "1594855"
  },
  {
    "text": "uh, distributions for HMMS. There is, um, the probability of, um,",
    "start": "1594855",
    "end": "1603660"
  },
  {
    "text": "the- I'm gonna call it Pstart of h, which is gonna specify with the initial distribution over H_1.",
    "start": "1603660",
    "end": "1611865"
  },
  {
    "text": "I'm gonna have the transition probabilities, which I'm gonna denote, um, h-, um, it's called h prime.",
    "start": "1611865",
    "end": "1621150"
  },
  {
    "text": "Probability of h prime. Oh, I should write down Ptrans here.",
    "start": "1621150",
    "end": "1627070"
  },
  {
    "text": "H prime given h. So this is going to be another distribution which powers each, uh, transition.",
    "start": "1628100",
    "end": "1639180"
  },
  {
    "text": "Uh, each non-initial hidden variable, okay. Remember, the- these are pointing to variables, not edges or anything else.",
    "start": "1639180",
    "end": "1649185"
  },
  {
    "text": "And finally, I'm going to have, um, a distribution, the emission distribution of e given h. Um,",
    "start": "1649185",
    "end": "1658950"
  },
  {
    "text": "that table is going to power, um, each of the, uh,",
    "start": "1658950",
    "end": "1665355"
  },
  {
    "text": "observed variables e, okay. And here, [NOISE] again, there are three types of distributions: start,",
    "start": "1665355",
    "end": "1675179"
  },
  {
    "text": "transition, and emission, even though there could be any number of, uh, variables in a natural Bayesian network.",
    "start": "1675180",
    "end": "1681299"
  },
  {
    "text": "Okay. And just to be very clear about this, when I apply this table to this node, H binds to,",
    "start": "1681300",
    "end": "1691679"
  },
  {
    "text": "um, H_1 and H_2- h prime binds to H_2.",
    "start": "1691679",
    "end": "1697380"
  },
  {
    "text": "And then when I apply it to this node, H_2 binds to h and h prime binds to H_3.",
    "start": "1697380",
    "end": "1706635"
  },
  {
    "text": "And again, you can see this from, you know, formulas where I'm passing in to Ptrans,",
    "start": "1706635",
    "end": "1713535"
  },
  {
    "text": "hi given hi minus 1 as i sweeps from 2 to, um, n. Okay, so you can think about this as like a little function with local variables,",
    "start": "1713535",
    "end": "1725610"
  },
  {
    "text": "the argument to that, um, that function r, h, and h prime. But when I actually called this function so to speak,",
    "start": "1725610",
    "end": "1733290"
  },
  {
    "text": "when defining the joint distribution, I'm passing in the actual variables of the Bayesian network.",
    "start": "1733290",
    "end": "1740020"
  },
  {
    "text": "Okay, any questions about this?",
    "start": "1744620",
    "end": "1748630"
  },
  {
    "text": "Okay, maybe just to summarize some concepts first. Okay. Um, so we talked about learning",
    "start": "1751910",
    "end": "1761715"
  },
  {
    "text": "as the generically the problem of how we go from data to parameters.",
    "start": "1761715",
    "end": "1768825"
  },
  {
    "text": "And data in this case is full assignments to all the random variables.",
    "start": "1768825",
    "end": "1774659"
  },
  {
    "text": "In HMM case, it's, a data point is, an assignment to all the hidden variables and the observed variables.",
    "start": "1774660",
    "end": "1782910"
  },
  {
    "text": "And parameters usually denoted Theta is all the local conditional distributions, which are these, uh,",
    "start": "1782910",
    "end": "1788940"
  },
  {
    "text": "three tables in the case of HMM. Okay. Um, the key, um,",
    "start": "1788940",
    "end": "1797580"
  },
  {
    "text": "intuition is count and normalize, um, which is intuitive and later I'll justify why this",
    "start": "1797580",
    "end": "1805559"
  },
  {
    "text": "is an appropriate way to do parameter estimation. Um, and then finally parameter sharing is this idea which allows you to define huge, uh,",
    "start": "1805560",
    "end": "1817035"
  },
  {
    "text": "Bayesian networks but, um, not have to blow up the number of parameters because you can share",
    "start": "1817035",
    "end": "1822600"
  },
  {
    "text": "the parameters between the different variables, okay.",
    "start": "1822600",
    "end": "1828554"
  },
  {
    "text": "So now, let's talk about the general case. Um, so hopefully, you kind of understand the basic intuition.",
    "start": "1828555",
    "end": "1836475"
  },
  {
    "text": "So this is just going to be, um, some abstract notation that's gonna kind of sum it up, um.",
    "start": "1836475",
    "end": "1842850"
  },
  {
    "text": "So in a general Bayesian network, we have variables x_1 through xn. And we're gonna say that parameters are a collection of distributions.",
    "start": "1842850",
    "end": "1851490"
  },
  {
    "text": "So Theta equals P_d, okay. And so big D is going to be,",
    "start": "1851490",
    "end": "1857700"
  },
  {
    "text": "um, the set of types of distributions. So in the HMM case, it's going to be three types: start, trans, and emit.",
    "start": "1857700",
    "end": "1865020"
  },
  {
    "text": "So basically, that's the number of the- these little boxes that I have on the board there. Um, and the joint distribution when you define a Bayesian network is",
    "start": "1865020",
    "end": "1875415"
  },
  {
    "text": "going to be the product of P of X_i given x parents of i.",
    "start": "1875415",
    "end": "1881055"
  },
  {
    "text": "So this is the same as we had before. But now notice the crucial difference which I've out-",
    "start": "1881055",
    "end": "1886860"
  },
  {
    "text": "outlined in red here is that I'm subscripting this p with, um, a d_i, okay.",
    "start": "1886860",
    "end": "1893294"
  },
  {
    "text": "So what d_i for the i'th variable says is which of these distributions is powering that variable, okay?",
    "start": "1893295",
    "end": "1902865"
  },
  {
    "text": "So d of, uh, this variable is emit, d of this variable is, uh,",
    "start": "1902865",
    "end": "1909330"
  },
  {
    "text": "transition, and d of this variable is start, okay. So this looks maybe a little bit abstract, um, um, notationally,",
    "start": "1909330",
    "end": "1916680"
  },
  {
    "text": "but the idea is just to multiply in the probability is,",
    "start": "1916680",
    "end": "1921870"
  },
  {
    "text": "uh, that, uh, was used to generate that variable or power that variable.",
    "start": "1921870",
    "end": "1928030"
  },
  {
    "text": "Okay. And parameter sharing just means that the d_i could be the same for multiple i's. Yep?",
    "start": "1928430",
    "end": "1936000"
  },
  {
    "text": "In a Markov model case where the emission probabilities are all the same for all variables.",
    "start": "1936000",
    "end": "1942315"
  },
  {
    "text": "Like why do we need multiple emission distributions, like, wouldn't be the same as just drawing a emission distribution different?",
    "start": "1942315",
    "end": "1948645"
  },
  {
    "text": "Yes, so the question is if we only have one emission, uh, distribution, why do we need so many of these replica copies?",
    "start": "1948645",
    "end": "1957420"
  },
  {
    "text": "Um, and the reason is that, these variables represent, um,",
    "start": "1957420",
    "end": "1963225"
  },
  {
    "text": "the objects' locations at a particular time. So the value of this is gonna be different based on what time step you're at.",
    "start": "1963225",
    "end": "1969960"
  },
  {
    "text": "But the mechanism for generating that variable is the same. Just like if I flipped,",
    "start": "1969960",
    "end": "1975420"
  },
  {
    "text": "a coin, you know, 10 times, I only have one distribution that represents the probability of heads,",
    "start": "1975420",
    "end": "1981795"
  },
  {
    "text": "but I have 10 realizations of that random variable. Um, another analogy that might be helpful is think of probability of these as,",
    "start": "1981795",
    "end": "1991170"
  },
  {
    "text": "like, a parameter- of, like, functions in your program that you can call, right. This is like a sorting function.",
    "start": "1991170",
    "end": "1997230"
  },
  {
    "text": "And sorting is just used in a whole bunch of different places, but it's kind of the same- kind of local function that, uh,",
    "start": "1997230",
    "end": "2005809"
  },
  {
    "text": "powers a bunch of, uh, different, um, use cases which are specific to the context.",
    "start": "2005810",
    "end": "2012330"
  },
  {
    "text": "Yeah. Okay, so in this general notation,",
    "start": "2014590",
    "end": "2019895"
  },
  {
    "text": "what does learning look like? So the training data is, um, a set of full assignments and I want to output the parameters.",
    "start": "2019895",
    "end": "2030200"
  },
  {
    "start": "2020000",
    "end": "2174000"
  },
  {
    "text": "So here's the, the basic form, it's count and normalize. So in counting, um,",
    "start": "2030200",
    "end": "2035660"
  },
  {
    "text": "there's just a bunch of for loops. So for each, um, training example, which is x is a full assignment,",
    "start": "2035660",
    "end": "2041390"
  },
  {
    "text": "I'm gonna look at every, um, variable. I'm going to just increment a counter which,",
    "start": "2041390",
    "end": "2049445"
  },
  {
    "text": "uh, of the di'th, um, uh, distribution of this particular configuration, uh,",
    "start": "2049445",
    "end": "2057455"
  },
  {
    "text": "x parents i and xi, okay. And then in the normalization step,",
    "start": "2057455",
    "end": "2064385"
  },
  {
    "text": "I'm going to consider all the different types of, um, distributions.",
    "start": "2064385",
    "end": "2070669"
  },
  {
    "text": "And then I'm gonna consider all the possible local assignments to the parents,",
    "start": "2070670",
    "end": "2076339"
  },
  {
    "text": "and I'm going to normalize, um, that distribution. Yeah?",
    "start": "2076340",
    "end": "2081710"
  },
  {
    "text": "[inaudible] of d_i the right table we're looking, correct? Yeah, so the d_i, uh, refers for the i'th variable,",
    "start": "2081710",
    "end": "2089839"
  },
  {
    "text": "which red table I'm looking at. [NOISE].",
    "start": "2089840",
    "end": "2097510"
  },
  {
    "text": "Okay. So, um, I've given you already a bunch of examples.",
    "start": "2097510",
    "end": "2102730"
  },
  {
    "text": "So hopefully this, um, the notation might be a little bit abstruse, but hopefully you guys already have the, you know, intuition.",
    "start": "2102730",
    "end": "2109885"
  },
  {
    "text": "Um, any questions? We're moving on.",
    "start": "2109885",
    "end": "2116780"
  },
  {
    "text": "The main point of this slide is just to say that this is actually very general and I just didn't do it for",
    "start": "2118140",
    "end": "2124405"
  },
  {
    "text": "hidden Markov models and naive Bayesian doesn't work for anything else. Okay. So now let me come back to the question of,",
    "start": "2124405",
    "end": "2133420"
  },
  {
    "text": "you know, why does count and normalize make sense, right? So count and normalize is just like,",
    "start": "2133420",
    "end": "2139480"
  },
  {
    "text": "you know, some made up procedure. So why is it a reasonable thing to do? And it turns out this is actually based on very firm kind of foundational principles,",
    "start": "2139480",
    "end": "2148795"
  },
  {
    "text": "this idea of maximum likelihood, which is an idea from statistics, um, that says if I see some data and I have,",
    "start": "2148795",
    "end": "2157705"
  },
  {
    "text": "uh, a model over that data, I want to tweak the parameters to make the probability of that data as high as possible, okay?",
    "start": "2157705",
    "end": "2165400"
  },
  {
    "text": "Uh, so in symbols, what this looks like is I want to find the parameters Theta.",
    "start": "2165400",
    "end": "2172615"
  },
  {
    "text": "So remember parameters are probabilities in the, uh, red tables on the board, um,",
    "start": "2172615",
    "end": "2179320"
  },
  {
    "start": "2174000",
    "end": "2465000"
  },
  {
    "text": "and then I'm going look at- make the product of all over all the training examples the probability of that assignment.",
    "start": "2179320",
    "end": "2186325"
  },
  {
    "text": "So for every possible setting of the parameters, I can, that assigns particular probabilities to, um,",
    "start": "2186325",
    "end": "2194079"
  },
  {
    "text": "my training examples and I want to make that number as high as possible, okay?",
    "start": "2194080",
    "end": "2199940"
  },
  {
    "text": "And the point is that the algorithm on the previous slide exactly computes this maximum likelihood through,",
    "start": "2200970",
    "end": "2206770"
  },
  {
    "text": "uh, parameters in closed form, so which is really nice. If you think about when we talk about machine learning,",
    "start": "2206770",
    "end": "2213280"
  },
  {
    "text": "we define a loss function and you cannot compute anything in closed form except for maybe linear regression,",
    "start": "2213280",
    "end": "2219220"
  },
  {
    "text": "and you have to use stochastic gradient descent to optimize it. Here, it's actually much simpler because of the way that the model is setup.",
    "start": "2219220",
    "end": "2228595"
  },
  {
    "text": "You just count and normalize, and that is actually, uh, the optimal answer. So just because you write down a max,",
    "start": "2228595",
    "end": "2234930"
  },
  {
    "text": "it doesn't always mean you have to use gradient descent. Is the, is the lesson here. Um, okay.",
    "start": "2234930",
    "end": "2241300"
  },
  {
    "text": "So let me- I'm not gonna prove this in generality, but I want to give you some intuition why, uh,",
    "start": "2241300",
    "end": "2248665"
  },
  {
    "text": "this is true and hopefully connect the, the kind of the, the abstract principle with, uh,",
    "start": "2248665",
    "end": "2254620"
  },
  {
    "text": "on the ground, um, algorithm that you- of count and normalize. So suppose we have, um, uh,",
    "start": "2254620",
    "end": "2261730"
  },
  {
    "text": "two variable Bayesian network with genre and rating. So I have three data points here and I have this maximum likelihood principle,",
    "start": "2261730",
    "end": "2270550"
  },
  {
    "text": "which I'm, you know, going to follow and let's do some algebra here.",
    "start": "2270550",
    "end": "2275710"
  },
  {
    "text": "So I'm gonna expand the joint distribution, and remember joint distribution is just the product of all the local conditional distributions,",
    "start": "2275710",
    "end": "2284125"
  },
  {
    "text": "um, and I've also expanded this, you know, the product over these three instances.",
    "start": "2284125",
    "end": "2289240"
  },
  {
    "text": "So I have the probability of d, probably of 4 given d, um, probability of d here,",
    "start": "2289240",
    "end": "2295255"
  },
  {
    "text": "probably of 5 given d, um, probably of c, and probably of 5 given c, okay?",
    "start": "2295255",
    "end": "2301405"
  },
  {
    "text": "And what I'm maximizing over here right now is a distribution over genre and I have a distribution of",
    "start": "2301405",
    "end": "2308650"
  },
  {
    "text": "rating conditioned on genres c and the distribution of rating conditions genre equals d. Okay.",
    "start": "2308650",
    "end": "2316779"
  },
  {
    "text": "So, um, I've color-coded these in a certain way to emphasize, um,",
    "start": "2316780",
    "end": "2324145"
  },
  {
    "text": "that the- all the red touches is, is the local conditional distribution correspond to genre,",
    "start": "2324145",
    "end": "2331465"
  },
  {
    "text": "the blue is corresponds to, um, probability of rating given genre equals d and green",
    "start": "2331465",
    "end": "2340180"
  },
  {
    "text": "is probability of rating given genre equals c, okay? So now I can shuffle things around, okay?",
    "start": "2340180",
    "end": "2348715"
  },
  {
    "text": "And I notice, um, that these factors don't actually depend on probability of g at all.",
    "start": "2348715",
    "end": "2357180"
  },
  {
    "text": "So they can just hang out over here. And I've essent- and the- and this likewise, um,",
    "start": "2357180",
    "end": "2362940"
  },
  {
    "text": "if I'm thinking about the maximum over, uh, you know, argument c,",
    "start": "2362940",
    "end": "2368960"
  },
  {
    "text": "these other factors are just constants. So they don't really matter either. So I've basically reduced this as a problem of three independent maximization problems, okay?",
    "start": "2368960",
    "end": "2383230"
  },
  {
    "text": "And this is why I could take each local conditional distribution in turn and do count and normalize on each one separately. Um, okay.",
    "start": "2383230",
    "end": "2393100"
  },
  {
    "text": "And then the rest is actually, um, you know, to do- to do do it actually properly,",
    "start": "2393100",
    "end": "2398785"
  },
  {
    "text": "you have to use Lagrange multipliers, um, to, to solve it.",
    "start": "2398785",
    "end": "2404470"
  },
  {
    "text": "But, um, intuitively, uh, hopefully you can either do that or just believe that if I ask you,",
    "start": "2404470",
    "end": "2411320"
  },
  {
    "text": "um, what is the probability, best way to set probabilities so that this quantity is maximized,",
    "start": "2411320",
    "end": "2418905"
  },
  {
    "text": "I'm going to set, um, probability of d equals two-thirds and probability of c equals one-third.",
    "start": "2418905",
    "end": "2424619"
  },
  {
    "text": "[NOISE] This is similar to one of the questions on, uh, the foundations homework if you, uh,",
    "start": "2424620",
    "end": "2430059"
  },
  {
    "text": "remember, but only for probability of, um, uh, a coin flip essentially.",
    "start": "2430060",
    "end": "2437960"
  },
  {
    "text": "Okay. So hopefully some of you can now, uh, rest, uh, you know,",
    "start": "2441510",
    "end": "2446575"
  },
  {
    "text": "sleep at night thinking that if you do count and normalize, you're actually obeying some, um, high-minded principle of maximum likelihood.",
    "start": "2446575",
    "end": "2454609"
  },
  {
    "text": "Okay. Um, so let's talk about Laplace smoothing.",
    "start": "2457560",
    "end": "2464665"
  },
  {
    "text": "So here's a scenario. If I have given you a coin and I said I don't know what's the probability of heads,",
    "start": "2464665",
    "end": "2471280"
  },
  {
    "start": "2465000",
    "end": "2565000"
  },
  {
    "text": "but you flip it 100 times and 23 times it's heads and 77 times it's tails.",
    "start": "2471280",
    "end": "2477430"
  },
  {
    "text": "What is the maximum likelihood, uh, estimate going to be?",
    "start": "2477430",
    "end": "2482029"
  },
  {
    "text": "Yeah. So it's going to be probability of heads is, you know, count and normalize, uh,",
    "start": "2484860",
    "end": "2491365"
  },
  {
    "text": "it's 23 over 100, which is 0.23, probability of tails is 0.77, okay?",
    "start": "2491365",
    "end": "2497875"
  },
  {
    "text": "So it seems reasonable, right? Um, so what about this?",
    "start": "2497875",
    "end": "2503560"
  },
  {
    "text": "So you flip a coin once and you get heads, what's the probability of heads? So the maximum likelihood says 1 and 0.",
    "start": "2503560",
    "end": "2513370"
  },
  {
    "text": "So, you know, some of you are probably thinking, you know, smiling and you're probably thinking, \"This is a very, uh,",
    "start": "2513370",
    "end": "2519355"
  },
  {
    "text": "closed-minded thing to do, right?\" Just because you saw one heads, it's like, \"Oh, okay. The probability of heads must be 1.",
    "start": "2519355",
    "end": "2525190"
  },
  {
    "text": "Tails is impossible because I never saw it.\" So it seems pretty, um, you know, foolish, um,",
    "start": "2525190",
    "end": "2533020"
  },
  {
    "text": "and intuitively you feel like, \"Well, tails might happen, you know, sometimes.",
    "start": "2533020",
    "end": "2538885"
  },
  {
    "text": "Um, so it shouldn't be as stark as 1, 0.\" Okay. And this is an example of overfitting,",
    "start": "2538885",
    "end": "2546370"
  },
  {
    "text": "which we talked about in the machine learning lecture, where maximum likelihood will tend to just maximize the probability,",
    "start": "2546370",
    "end": "2552550"
  },
  {
    "text": "and in here it does maximize the probability because the probability of data is now 1 and you can't do better than that.",
    "start": "2552550",
    "end": "2558309"
  },
  {
    "text": "But this is definitely overfitting. So we want, uh, a more reasonable estimate.",
    "start": "2558310",
    "end": "2564685"
  },
  {
    "text": "So this is where Laplace smoothing comes in, and again I'm gonna introduce Laplace smoothing, uh,",
    "start": "2564685",
    "end": "2570474"
  },
  {
    "start": "2565000",
    "end": "2654000"
  },
  {
    "text": "from kind of a follow your nose kind of framework and then,",
    "start": "2570475",
    "end": "2575500"
  },
  {
    "text": "um, I'll talk about why it might be a good idea. Okay. So here's maximum likelihood, um,",
    "start": "2575500",
    "end": "2582190"
  },
  {
    "text": "just the number of times heads occurs over the total number of trials you have. So Laplace smoothing is, um,",
    "start": "2582190",
    "end": "2588895"
  },
  {
    "text": "just adding some numbers. Um, so, uh, La- this is Laplace named after the famous French mathematician who,",
    "start": "2588895",
    "end": "2598225"
  },
  {
    "text": "um, did a lot more than add numbers, um, like the Laplace transform and Laplace distribution.",
    "start": "2598225",
    "end": "2603670"
  },
  {
    "text": "But we're only going to use or talk about his, um, adding numbers, um, invention I guess.",
    "start": "2603670",
    "end": "2609460"
  },
  {
    "text": "Um, so, so here in red, I'm shown that for this probab- estimate,",
    "start": "2609460",
    "end": "2616810"
  },
  {
    "text": "no matter what the data is, I'm just gonna add a 1 here. I don't care what the data looks. I'm just gonna add a 1 and we're gonna divide by the total number of values,",
    "start": "2616810",
    "end": "2625405"
  },
  {
    "text": "uh, which are possible, which is 2, heads or tails. And for tails, I'm also gonna add a 1,",
    "start": "2625405",
    "end": "2630615"
  },
  {
    "text": "I don't care what the data says, um, and I'm gonna divide by 2, okay? So now I get two-thirds and one-third,",
    "start": "2630615",
    "end": "2637605"
  },
  {
    "text": "which should be a more, you know, sensib- intuitively sensible estimate if you're gonna come up with any sort of estimate.",
    "start": "2637605",
    "end": "2645099"
  },
  {
    "text": "It says, \"Well, I saw heads, so probably more than 50% is gonna be heads.",
    "start": "2645100",
    "end": "2650110"
  },
  {
    "text": "But, um, it's probably not, you know, 100%.\" Um, okay.",
    "start": "2650110",
    "end": "2656440"
  },
  {
    "start": "2654000",
    "end": "2949000"
  },
  {
    "text": "So let's look at it in a slightly, uh, more complicated setting. So here I have two variables and, um,",
    "start": "2656440",
    "end": "2665380"
  },
  {
    "text": "Laplace smoothing is driven by a parameter Lambda, um, which by default is going to be 1,",
    "start": "2665380",
    "end": "2671545"
  },
  {
    "text": "but it can be any number, um, uh, non-negative number. Um, and what Laplace smoothing amounts to is",
    "start": "2671545",
    "end": "2681130"
  },
  {
    "text": "saying start out with your tables and instead of filling them up with 0,",
    "start": "2681130",
    "end": "2687299"
  },
  {
    "text": "fill them in with Lambda, okay? So here's my table. Before I even look at the data,",
    "start": "2687300",
    "end": "2693300"
  },
  {
    "text": "I'm just gonna put 1s down. And then when I look at my data, I'm just going to add to that counter.",
    "start": "2693300",
    "end": "2698920"
  },
  {
    "text": "So d shows up twice, c shows up once, and then count and normalize, okay? Same over here.",
    "start": "2698920",
    "end": "2705670"
  },
  {
    "text": "Before I look at any data, I'm just gonna populate with ones, which is Lambda,",
    "start": "2705670",
    "end": "2710714"
  },
  {
    "text": "and then I get my three data points and I add the three counters,",
    "start": "2710715",
    "end": "2715995"
  },
  {
    "text": "uh, which are shown here, and I count and normalize. So by construction, no,",
    "start": "2715995",
    "end": "2724480"
  },
  {
    "text": "uh, events should have probability of 0 unless Lambda is 0. Because I already start with 1,",
    "start": "2724480",
    "end": "2730869"
  },
  {
    "text": "and 1 divided by some positive number is not 0, okay?",
    "start": "2730870",
    "end": "2738970"
  },
  {
    "text": "So the general idea is, uh, for every distribution and partial assignment, um,",
    "start": "2738970",
    "end": "2744805"
  },
  {
    "text": "I'm going to add Lambda to that count, um, you know,",
    "start": "2744805",
    "end": "2750460"
  },
  {
    "text": "and then I'm just going to normalize, uh, to get the probability estimates, okay?",
    "start": "2750460",
    "end": "2756970"
  },
  {
    "text": "So an interpretation of Laplace smoothing is you're essentially, you know, um,",
    "start": "2756970",
    "end": "2762535"
  },
  {
    "text": "pretending that you saw Lambda occurrences of every local assignment even though you didn't see in your data.",
    "start": "2762535",
    "end": "2769165"
  },
  {
    "text": "You're hallucinating this, this data, sometimes it's called pseudocounts or virtual counts.",
    "start": "2769165",
    "end": "2774445"
  },
  {
    "text": "Um, and the more, uh, higher Lambda is, the more hallucination or more smoothing you're doing,",
    "start": "2774445",
    "end": "2783145"
  },
  {
    "text": "and that will draw the probabilities closer to the uniform, you know, distribution and the other extreme Lambda equals 0 is simply maximum likelihood.",
    "start": "2783145",
    "end": "2794240"
  },
  {
    "text": "But in the end, you know, data, uh, wins out. So if you had, um,",
    "start": "2795830",
    "end": "2802590"
  },
  {
    "text": "Laplace smoothing and you saw one head and, uh, now you're saying estimated the probability of two-thirds.",
    "start": "2802590",
    "end": "2810225"
  },
  {
    "text": "But suppose you keep on flipping this coin and it keeps on coming up heads 998 times,",
    "start": "2810225",
    "end": "2818174"
  },
  {
    "text": "then by doing the same Laplace smoothing, you're going to eventually update your probability to 0.999.",
    "start": "2818174",
    "end": "2825990"
  },
  {
    "text": "You're never gonna reach 1 exactly because no probabilities are going to be ever 0 or 1.",
    "start": "2825990",
    "end": "2833445"
  },
  {
    "text": "But increasingly, you're gonna be much more confident that this is a very, very rich coin.",
    "start": "2833445",
    "end": "2839020"
  },
  {
    "text": "Okay. Any questions about Laplace smoothing? [NOISE]",
    "start": "2842420",
    "end": "2854130"
  },
  {
    "text": "So Laplace smoothing is [NOISE] just, um, add Lambda to everything essentially.",
    "start": "2854130",
    "end": "2862500"
  },
  {
    "text": "And oh, so, so I forgot the principle behind Laplace smoothing is if you think in terms of,",
    "start": "2862500",
    "end": "2868410"
  },
  {
    "text": "um, this is, kind of, beyond the scope of this course but you can think about a prior distribution over your parameters,",
    "start": "2868410",
    "end": "2875924"
  },
  {
    "text": "um, which is, um, some uniform distribution. And, um, instead of doing maximum likelihood,",
    "start": "2875925",
    "end": "2883244"
  },
  {
    "text": "you're doing, um, map or a mac- maximum a posteriori estimation.",
    "start": "2883245",
    "end": "2888585"
  },
  {
    "text": "So there is another principle but you don't have to worry about it for this class. Yeah?",
    "start": "2888585",
    "end": "2894450"
  },
  {
    "text": "[inaudible] make Lambda? The question is how big you make Lambda?",
    "start": "2894450",
    "end": "2900360"
  },
  {
    "text": "Um, it's a good question so one is- Lambda should be small.",
    "start": "2900360",
    "end": "2906855"
  },
  {
    "text": "It probably shouldn't be like 100. Um, probably should be more like 1 or even- you can make it less than 1,",
    "start": "2906855",
    "end": "2913125"
  },
  {
    "text": "maybe it should be like 0.01 or something. It depends on, um, you know,",
    "start": "2913125",
    "end": "2919125"
  },
  {
    "text": "how, I guess, uncertain, you know, you are. So in general, that these priors are meant to capture what you know about the,",
    "start": "2919125",
    "end": "2928050"
  },
  {
    "text": "um, the, kind of, the distribution at hand.",
    "start": "2928050",
    "end": "2932170"
  },
  {
    "text": "Okay. All right. So now we get to the fun part, this is going to- in some sense,",
    "start": "2933440",
    "end": "2942960"
  },
  {
    "text": "combined learning which we'd done with probabilistic inference, uh, which we did before.",
    "start": "2942960",
    "end": "2948569"
  },
  {
    "text": "And the motivation here is that, what happens if you don't observe some of the variables?",
    "start": "2948570",
    "end": "2955250"
  },
  {
    "start": "2949000",
    "end": "2989000"
  },
  {
    "text": "So far, learning has assumed that we observe complete assignments to all the random variables,",
    "start": "2955250",
    "end": "2961160"
  },
  {
    "text": "but in practice, this is just not true, right? The whole reason people call them Hidden Markov models is that",
    "start": "2961160",
    "end": "2967650"
  },
  {
    "text": "you don't actually observe the, the hidden states. So what happens if we only get the observations?",
    "start": "2967650",
    "end": "2973665"
  },
  {
    "text": "Or the simpler example, what happens if the data looks like this where we observe the ratings but we don't observe the genres?",
    "start": "2973665",
    "end": "2979650"
  },
  {
    "text": "So what can you do? So obviously, the counter normalized thing doesn't work anymore because you can't count with question marks.",
    "start": "2979650",
    "end": "2987970"
  },
  {
    "text": "So, um, there is, again, two ways to think about what to do here.",
    "start": "2988790",
    "end": "2996720"
  },
  {
    "start": "2989000",
    "end": "3179000"
  },
  {
    "text": "The high-minded principle is appeal to maximum likelihood and make it work for, uh, unobserved variables.",
    "start": "2996720",
    "end": "3005030"
  },
  {
    "text": "Um, and the other way to think about it which I'll come to later",
    "start": "3005030",
    "end": "3010220"
  },
  {
    "text": "is simply guess what the latent variables are and then do counter normalize.",
    "start": "3010220",
    "end": "3015785"
  },
  {
    "text": "Okay? And those- these two are gonna be equivalent. So let's be high-minded for now and think about,",
    "start": "3015785",
    "end": "3021920"
  },
  {
    "text": "um, what maximum marginal likelihood. Okay? So in general,",
    "start": "3021920",
    "end": "3027380"
  },
  {
    "text": "we're going to think about H as the hidden variables and E as observed variables. In this case, G is hidden,",
    "start": "3027380",
    "end": "3034849"
  },
  {
    "text": "and R1 and R2 are observed. And suppose I see some evidence E, so I see R_1 equals 1 and R_2 equals 2 but I don't see the value of G. And again,",
    "start": "3034850",
    "end": "3044930"
  },
  {
    "text": "the parameters are all the same, the same as before. I just have less data or information to estimate the parameters.",
    "start": "3044930",
    "end": "3052025"
  },
  {
    "text": "Okay. So if you're following maximum likelihood,",
    "start": "3052025",
    "end": "3057289"
  },
  {
    "text": "what does maximum likelihood say? It says tweak the parameters so that the likelihood of your data is as large as possible.",
    "start": "3057290",
    "end": "3065599"
  },
  {
    "text": "So what is the likelihood of the data here? It's simply instead of H and E,",
    "start": "3065600",
    "end": "3071675"
  },
  {
    "text": "I have probability of E. Okay? So this seems like a, kind of, a very natural sound extension of maximum likelihood,",
    "start": "3071675",
    "end": "3080288"
  },
  {
    "text": "um, and it's called, um, maximum marginal likelihood. [NOISE] Um, because, um,",
    "start": "3080289",
    "end": "3089345"
  },
  {
    "text": "this [NOISE] quantity is a marginal likelihood, right? It's not a joint likelihood or a joint distribution, it's",
    "start": "3089345",
    "end": "3095930"
  },
  {
    "text": "a marginal distribution over only a subset of the variables. Okay. So now, to unpack this a bit,",
    "start": "3095930",
    "end": "3103445"
  },
  {
    "text": "um, [NOISE] what is this marginal distribution? It's actually, uh, by the axioms of probability,",
    "start": "3103445",
    "end": "3111560"
  },
  {
    "text": "it's the summation over all possible values of the hidden variables of the joint distribution.",
    "start": "3111560",
    "end": "3118910"
  },
  {
    "text": "So in other words, you can think about maximum marginal likelihood as saying I",
    "start": "3118910",
    "end": "3124369"
  },
  {
    "text": "wanna change the parameters in such a way that such that, um,",
    "start": "3124370",
    "end": "3129455"
  },
  {
    "text": "the probability of, um, what I see as high as possible,",
    "start": "3129455",
    "end": "3134660"
  },
  {
    "text": "but what that really requires me to do is think about all the possible values that,",
    "start": "3134660",
    "end": "3140645"
  },
  {
    "text": "um, H could take on. I don't see it, so I have to consider what if's,",
    "start": "3140645",
    "end": "3146000"
  },
  {
    "text": "what if it were C? What if it were D and so on? Okay? So in other words, fundamentally,",
    "start": "3146000",
    "end": "3152840"
  },
  {
    "text": "if you don't observe a variable, you have to consider possible values on it.",
    "start": "3152840",
    "end": "3158069"
  },
  {
    "text": "All right. So now, let's, uh, skip to the other side of the,",
    "start": "3159370",
    "end": "3164510"
  },
  {
    "text": "kind of, scrappy way and think about what is a reasonable algorithm that makes sense. And I'm not gonna have time, uh,",
    "start": "3164510",
    "end": "3171080"
  },
  {
    "text": "in this course to, kind of, show the formal connection, but if you take, you know, CS 228 or a graphical models class,",
    "start": "3171080",
    "end": "3176840"
  },
  {
    "text": "um, you'll go into this in much more detail. Um, so the intuition here is, um,",
    "start": "3176840",
    "end": "3183724"
  },
  {
    "start": "3179000",
    "end": "5025000"
  },
  {
    "text": "the same as what we had for k-means. So remember in k-means, you tried to do clustering,",
    "start": "3183725",
    "end": "3189545"
  },
  {
    "text": "you don't know the, the centroids, and you also don't know the assignments.",
    "start": "3189545",
    "end": "3194765"
  },
  {
    "text": "So it's a chicken and egg problem. So you go back and forth between, uh,",
    "start": "3194765",
    "end": "3200900"
  },
  {
    "text": "figuring out what the centroids are and figuring out what the, the assignments are.",
    "start": "3200900",
    "end": "3206164"
  },
  {
    "text": "And the centroids are going to be an analog of parameters here, and the assignments are going to be the analog of the hidden variables.",
    "start": "3206165",
    "end": "3215795"
  },
  {
    "text": "Okay? So here's, here's the algorithm. Um, it's called Expectation-Maximization.",
    "start": "3215795",
    "end": "3222200"
  },
  {
    "text": "It was, you know, formalized, um, in its generality in the- in the 70's,",
    "start": "3222200",
    "end": "3227585"
  },
  {
    "text": "and you start out with some parameters,",
    "start": "3227585",
    "end": "3232729"
  },
  {
    "text": "um, maybe initializing to uniform or uniform plus a little bit of, uh, noise.",
    "start": "3232729",
    "end": "3237890"
  },
  {
    "text": "And then it's gonna alter- we're gonna alternate between two steps: the E-step and the M-step.",
    "start": "3237890",
    "end": "3243215"
  },
  {
    "text": "Um, and if you- it's useful to think about the k-means in your head while you're going through this algorithm.",
    "start": "3243215",
    "end": "3248810"
  },
  {
    "text": "So, um, in the E-step, we're going to guess what",
    "start": "3248810",
    "end": "3254690"
  },
  {
    "text": "the hidden variables are or what the values of the hidden variables take on. So we're going to define this q of h which is going to be,",
    "start": "3254690",
    "end": "3263915"
  },
  {
    "text": "uh, or represent our guess. And since we're in probability land, we're going to consider the distribution over, uh,",
    "start": "3263915",
    "end": "3270830"
  },
  {
    "text": "possible values of h. And this guess is going to be given by our current setting of parameters,",
    "start": "3270830",
    "end": "3279934"
  },
  {
    "text": "and the, the evidence that we've observed in our fixing. So we're asking the question what is the probability",
    "start": "3279935",
    "end": "3287195"
  },
  {
    "text": "of the hidden variables taking on particular values of h, given my data and given my parameters?",
    "start": "3287195",
    "end": "3295130"
  },
  {
    "text": "So this should- this should look, kind of, familiar to you, right? What is this? This is just probabilistic inference, right?",
    "start": "3295130",
    "end": "3305795"
  },
  {
    "text": "Which we've been doing for, um, the last lecture. Which means that, you can use any probabilistic inference algorithm here,",
    "start": "3305795",
    "end": "3314780"
  },
  {
    "text": "you can do forward backward, you can do Gibbs sampling, um, and that's, kind of, some module that you need to do, um, EM.",
    "start": "3314780",
    "end": "3323300"
  },
  {
    "text": "Okay? Okay so now what happens if we have our,",
    "start": "3323300",
    "end": "3329240"
  },
  {
    "text": "uh, setting of- or guess over the possible values H?",
    "start": "3329240",
    "end": "3334340"
  },
  {
    "text": "Now, we, um, make up data. Um, so we create weighted points, um,",
    "start": "3334340",
    "end": "3342080"
  },
  {
    "text": "of- with particular values of H and E, and, um,",
    "start": "3342080",
    "end": "3348035"
  },
  {
    "text": "each of them gets some weight, uh, q of h. Um, and then finally,",
    "start": "3348035",
    "end": "3353494"
  },
  {
    "text": "once we're given these weighted points, now we're just going to use, uh, do counter normalize and do maximum likelihood.",
    "start": "3353495",
    "end": "3360410"
  },
  {
    "text": "[NOISE] Okay? So I'm gonna walk through an example, um, to make this a little bit more grounded out.",
    "start": "3360410",
    "end": "3366045"
  },
  {
    "text": "Um, so I'm gonna do this on a board because it's [NOISE] gonna get a little bit,",
    "start": "3366045",
    "end": "3371065"
  },
  {
    "text": "um, maybe a little bit hairy. Um, okay. Let's, let's do this.",
    "start": "3371065",
    "end": "3377744"
  },
  {
    "text": "So here, we have a three variable, uh, Bayesian network.",
    "start": "3377745",
    "end": "3385170"
  },
  {
    "text": "So we have- um, let's draw it over here. Actually, I might need a space.",
    "start": "3387460",
    "end": "3393290"
  },
  {
    "text": "So we have G, we have R_1 and [NOISE] R_2. Okay? And our data that we see is, ah,",
    "start": "3393290",
    "end": "3402170"
  },
  {
    "text": "we get [NOISE] data which is, um, 2, 2 and 1, 2.",
    "start": "3402170",
    "end": "3407900"
  },
  {
    "text": "Okay? So I observed, um, 2, 2, that's one data point and,",
    "start": "3407900",
    "end": "3413030"
  },
  {
    "text": "uh, 1, 2, that's another data point. Okay? Um, so initially, what I'm going to do is,",
    "start": "3413030",
    "end": "3420304"
  },
  {
    "text": "um, start with some setting of parameters. Okay? So I'm going to start with my parameters Theta,",
    "start": "3420304",
    "end": "3430430"
  },
  {
    "text": "[NOISE] which specify a distribution over, uh, g. And for,",
    "start": "3430430",
    "end": "3437420"
  },
  {
    "text": "[NOISE] um, lack of information [NOISE] I'm going to consider [NOISE] just half and half.",
    "start": "3437420",
    "end": "3444079"
  },
  {
    "text": "Let me write 0.5 to be consistent. [NOISE] So I'm initializing it with a uniform distribution,",
    "start": "3444080",
    "end": "3450575"
  },
  {
    "text": "and my other table here is going to be, um,",
    "start": "3450575",
    "end": "3455720"
  },
  {
    "text": "g and r. So [NOISE] probability of, um, [NOISE] uh, r given g. [NOISE] And here,",
    "start": "3455720",
    "end": "3463099"
  },
  {
    "text": "I have c, [NOISE] uh, the possible values of g and the possible [NOISE] values of r",
    "start": "3463100",
    "end": "3468320"
  },
  {
    "text": "which for simplicity I'm gonna assume to just be 1 and 2. Um, and then for this, I have- [NOISE] for these numbers, 0.4,",
    "start": "3468320",
    "end": "3476030"
  },
  {
    "text": "0.6, um, [NOISE] and 0.6, 0.4.",
    "start": "3476030",
    "end": "3481190"
  },
  {
    "text": "So I'm- [NOISE] I'm not setting them to uniform, um, but adding a little bit of noise.",
    "start": "3481190",
    "end": "3487910"
  },
  {
    "text": "So hopefully, people can check that, um, the numbers on the board are the same as the numbers on the slides.",
    "start": "3487910",
    "end": "3494600"
  },
  {
    "text": "Okay. So- [NOISE] so that's my initial parameter setting. So now, in the E step,",
    "start": "3494600",
    "end": "3500990"
  },
  {
    "text": "[NOISE] I'm gonna use these parameters and to guess what g is. [NOISE] Okay? So what does this look like?",
    "start": "3500990",
    "end": "3508910"
  },
  {
    "text": "Um, so [NOISE] for [NOISE] each possible datapoint.",
    "start": "3508910",
    "end": "3514954"
  },
  {
    "text": "So for every example, so I have 2, 2, that's one datapoint. I'm gonna try to guess what the value of g is.",
    "start": "3514955",
    "end": "3522214"
  },
  {
    "text": "So it could be c, [NOISE] um, it could be d. And for the other datapoint: 1, 2, [NOISE] it also could be c or it could be d. Okay?",
    "start": "3522215",
    "end": "3530950"
  },
  {
    "text": "[NOISE] And now, I'm gonna try to, um, compute a weight for each of these datapoints [NOISE] based on the model.",
    "start": "3530950",
    "end": "3539545"
  },
  {
    "text": "Okay? So the weight is- not now, look- look at this. This is cool, right?",
    "start": "3539545",
    "end": "3545560"
  },
  {
    "text": "Because I have a fu- a complete assignment and you know what to do with complete assignments. I can just evaluate the probability of that assignment.",
    "start": "3545560",
    "end": "3552985"
  },
  {
    "text": "Okay? So now, um, ju- just to make things concrete,",
    "start": "3552985",
    "end": "3558425"
  },
  {
    "text": "I have probability of g times [NOISE] probability of r_1 given g [NOISE] probability of r_2 given g. [NOISE] Um,",
    "start": "3558425",
    "end": "3565460"
  },
  {
    "text": "this is by definition the probability of g [NOISE], um, 2 equals, uh [NOISE].",
    "start": "3565460",
    "end": "3575900"
  },
  {
    "text": "Sorry, that's a little bit messy but- [NOISE] So this is joint distribution by definition of this Bayesian network, it's this.",
    "start": "3575900",
    "end": "3584480"
  },
  {
    "text": "Um, okay? So how do I compute the probability of this configuration?",
    "start": "3584480",
    "end": "3591275"
  },
  {
    "text": "Well, it's a probability of g equals c. So I look at this table. That can be a 0.5 [NOISE] times the probability of r_1 given g, that's,",
    "start": "3591275",
    "end": "3601400"
  },
  {
    "text": "uh, 2 and c. So if we look over here, that's, ah, 0.6.",
    "start": "3601400",
    "end": "3607279"
  },
  {
    "text": "[NOISE] And then another r_2 given g, that's also a 2 and a c. So [NOISE] that's,",
    "start": "3607280",
    "end": "3613130"
  },
  {
    "text": "ah, 0.6 because of parameter sharing. [NOISE] And that's, ah, 0.18, right? [NOISE] Um, let me give you guys a slide so you guys can check my work. Um, Okay.",
    "start": "3613130",
    "end": "3624470"
  },
  {
    "text": "So now, I look at this table. So probability of g equals d is 0.5.",
    "start": "3624470",
    "end": "3630290"
  },
  {
    "text": "[NOISE] Um, and then the probability of r_1 equals 2 given g equals d. Or if we look in this table [NOISE], that's 0.4.",
    "start": "3630290",
    "end": "3638660"
  },
  {
    "text": "[NOISE] And then I have another 0.4 from the other two, and that is [NOISE] 0.08.",
    "start": "3638660",
    "end": "3645690"
  },
  {
    "text": "Um, and then I can normalize this. Question?",
    "start": "3645850",
    "end": "3651980"
  },
  {
    "text": "[NOISE] [inaudible] versus 0.6 for c2? So why is this 0.4 versus, uh, 0.6 here?",
    "start": "3651980",
    "end": "3661790"
  },
  {
    "text": "Um, this is because I initialize the parameters so that the probability of r equals 2 givens g equals d is 0.4.",
    "start": "3661790",
    "end": "3670190"
  },
  {
    "text": "So I- I guess I'm wondering why did you use that initialization? Ah, why did I use this initialization? Um, just for fun. I just made it up.",
    "start": "3670190",
    "end": "3677210"
  },
  {
    "text": "[LAUGHTER] Just so that if I had put 0.5's here, then you couldn't tell what was going on.",
    "start": "3677210",
    "end": "3682865"
  },
  {
    "text": "And also, um, as a side mark it wouldn't, um, you know, work, ah, yeah. [NOISE].",
    "start": "3682865",
    "end": "3688900"
  },
  {
    "text": "So initialization is always random? Initialization is generally going to be",
    "start": "3688900",
    "end": "3695109"
  },
  {
    "text": "random but as close to uniform as, as possible. Yeah. Thank you.",
    "start": "3695110",
    "end": "3701600"
  },
  {
    "text": "Okay. So this is- this column is going to be the probability of, um,",
    "start": "3701720",
    "end": "3709705"
  },
  {
    "text": "basically G equals [NOISE] g, uh, r- R_1 equals R_1,",
    "start": "3709705",
    "end": "3716465"
  },
  {
    "text": "R_2 equals R_2 [NOISE]. Okay? [NOISE] And then the q, um,",
    "start": "3716465",
    "end": "3721655"
  },
  {
    "text": "is going to just be [NOISE],",
    "start": "3721655",
    "end": "3727580"
  },
  {
    "text": "um, is just going to be the weight of this.",
    "start": "3727580",
    "end": "3732995"
  },
  {
    "text": "And normalizing these two, which means that you add them up and you divide, [NOISE] and then you get, uh,",
    "start": "3732995",
    "end": "3738845"
  },
  {
    "text": "was it 0.69 and 0.31? [NOISE] Sorry, the numbers are a little bit kind of,",
    "start": "3738845",
    "end": "3744770"
  },
  {
    "text": "um, awkward but that's what you get there. Ah, I'm not gonna do the second row but it's,",
    "start": "3744770",
    "end": "3750215"
  },
  {
    "text": "it's basically the same type of calculation. Question? [inaudible] kind of like particle filtering in that like in [NOISE] each step,",
    "start": "3750215",
    "end": "3759560"
  },
  {
    "text": "you do like the proposal and then then waiting, and then the end step is like you're res- resampling, like finding the maximum?",
    "start": "3759560",
    "end": "3766250"
  },
  {
    "text": "Um, so the question [NOISE] is, is you use, um, Expectation-Maximization is [NOISE] kind of like particle filtering because you have this proposal and you're reweighting.",
    "start": "3766250",
    "end": "3774725"
  },
  {
    "text": "Um, structurally, it's quite different. I mean, there is a sense like there's some,",
    "start": "3774725",
    "end": "3781265"
  },
  {
    "text": "um, you're proposing different options. But certainly, the two algorithms are meant to solve very different tasks.",
    "start": "3781265",
    "end": "3787730"
  },
  {
    "text": "One is for learning and one is for, um, probabilistic inference. Yeah. Okay. So let's look at the M-step now.",
    "start": "3787730",
    "end": "3798995"
  },
  {
    "text": "Okay. So the M-step. Uh, I'll let you guys fill this in.",
    "start": "3798995",
    "end": "3804425"
  },
  {
    "text": "So the M-step- [NOISE] um, I actually wanna keep this up.",
    "start": "3804425",
    "end": "3809960"
  },
  {
    "text": "[NOISE] Let me do the M-step over here. [NOISE] Is going to- now,",
    "start": "3809960",
    "end": "3816440"
  },
  {
    "text": "just take, ah, these examples. [NOISE] So these weights are 0.5. So it's, it's like someone handed you fully labeled data.",
    "start": "3816440",
    "end": "3824885"
  },
  {
    "text": "Complete observations, four points but each of observation has a weight.",
    "start": "3824885",
    "end": "3831530"
  },
  {
    "text": "So now, the only difference when counter normalized. Instead of just adding 1 whenever you see a particular configuration,",
    "start": "3831530",
    "end": "3839030"
  },
  {
    "text": "you are gonna add the weight. Okay. So [NOISE] for each of the parameters, I'm going to look at,",
    "start": "3839030",
    "end": "3845059"
  },
  {
    "text": "um, so this is gonna be [NOISE], uh, g probability of g. [NOISE] So this can be either c or d. Um,",
    "start": "3845060",
    "end": "3853700"
  },
  {
    "text": "[NOISE] and to get this I have- let me first do the count I guess.",
    "start": "3853700",
    "end": "3862310"
  },
  {
    "text": "Well, okay, let me just add it here. So I look at my data. [NOISE] So let me mark this.",
    "start": "3862310",
    "end": "3868820"
  },
  {
    "text": "So this is now my kind of, um, I'm gonna put data in quotes because I didn't actually observe it,",
    "start": "3868820",
    "end": "3875480"
  },
  {
    "text": "I just hallucinated It. And these are the weights [NOISE] which are associated with the data points.",
    "start": "3875480",
    "end": "3881900"
  },
  {
    "text": "So now, I'm going to look at g- uh, look at g equals c. So I see c here,",
    "start": "3881900",
    "end": "3887150"
  },
  {
    "text": "c here, and I have, um, the weights 0.69, um, [NOISE] 0.69 plus",
    "start": "3887150",
    "end": "3899910"
  },
  {
    "text": "0.5 probability of- sorry, I just count. [NOISE] And then for d. I see a d here and I see a d here.",
    "start": "3899910",
    "end": "3913190"
  },
  {
    "text": "That's 0.31 [NOISE] plus 0.5. [NOISE] And then I normalize this and I get my estimate.",
    "start": "3913190",
    "end": "3921785"
  },
  {
    "text": "Okay. So on the slide, this is exactly what I did here. I count and normalize.",
    "start": "3921785",
    "end": "3927740"
  },
  {
    "text": "And I'm gonna- not gonna do this table, um, but you, hopefully get the idea. Yeah.",
    "start": "3927740",
    "end": "3935150"
  },
  {
    "text": "[inaudible] [NOISE] estimate for all possible assignments of h? Yeah. So in each step,",
    "start": "3935150",
    "end": "3941750"
  },
  {
    "text": "you have to estimate for all possible assignments to h. So in this case,",
    "start": "3941750",
    "end": "3948005"
  },
  {
    "text": "h is only one variable. Um, in the next example, h is going to be in a hidden Markov model,",
    "start": "3948005",
    "end": "3953480"
  },
  {
    "text": "all the variables, and we're gonna see how we can deal with that. Yeah? Can you explain briefly how we get the values in the second table?",
    "start": "3953480",
    "end": "3960080"
  },
  {
    "text": "[NOISE] The second table with this one? [inaudible]. This one? Yes. Yeah. Sure. Just, uh,",
    "start": "3960080",
    "end": "3967160"
  },
  {
    "text": "briefly- so you look at these datapoints, um, and you see, ah, c1.",
    "start": "3967160",
    "end": "3974765"
  },
  {
    "text": "Okay. So where does c1 show up? So g equals c here, r_1 equals 1 here,",
    "start": "3974765",
    "end": "3979849"
  },
  {
    "text": "so that's a 0.5 here. And then what about c2? Where does c2 show up?",
    "start": "3979850",
    "end": "3985250"
  },
  {
    "text": "c2 shows up twice here with r1 and r2. So that's why there's two of, ah, 0.69's here.",
    "start": "3985250",
    "end": "3992420"
  },
  {
    "text": "And, uh, c2 shows up once here with a weight of 0.5.",
    "start": "3992420",
    "end": "3997595"
  },
  {
    "text": "So everything in the [inaudible]. Yeah. Everything here- these counts are based on the table from,",
    "start": "3997595",
    "end": "4005020"
  },
  {
    "text": "ah, the a step there.  [NOISE].",
    "start": "4005020",
    "end": "4012240"
  },
  {
    "text": "Okay? All right. So let's do something a little bit fun now. [NOISE] Um, so the Copiale cipher is",
    "start": "4012240",
    "end": "4020790"
  },
  {
    "text": "this 105 page encrypted volume that was discovered and people date it back from the 1730s.",
    "start": "4020790",
    "end": "4027450"
  },
  {
    "text": "So it looks like this. So it's unreadable because it's actually a cipher, it's not meant to be just read.",
    "start": "4027450",
    "end": "4033885"
  },
  {
    "text": "Um, and for decades, people were trying to figure out, you know, what is the actual, uh,",
    "start": "4033885",
    "end": "4039360"
  },
  {
    "text": "message that was inside this text. It's a lot of text. Um, and, and finally in 2011, um, some researchers,",
    "start": "4039360",
    "end": "4048540"
  },
  {
    "text": "um, actually cracked this code [NOISE] and they actually used, um, EM to help do this.",
    "start": "4048540",
    "end": "4055235"
  },
  {
    "text": "So I'm gonna give you a kind of a toy version of using EM to do decipherment.",
    "start": "4055235",
    "end": "4060335"
  },
  {
    "text": "And it turns out that this code is, um, uh, basically some book from a secret society,",
    "start": "4060335",
    "end": "4066150"
  },
  {
    "text": "which you can go read about on Wikipedia if you want. Um, so substitution ciphers.",
    "start": "4066150",
    "end": "4073545"
  },
  {
    "text": "A substitution cipher consists of a substitution table which specifies for every letter,",
    "start": "4073545",
    "end": "4079905"
  },
  {
    "text": "assume we have only 26 right now, um, a cipher letter, okay?",
    "start": "4079905",
    "end": "4085815"
  },
  {
    "text": "And the way you apply a substitution table is you take a plaintext which generally is unknown,",
    "start": "4085815",
    "end": "4091965"
  },
  {
    "text": "if you're trying to decipher like, you know, let's say hello world and you look up each letter",
    "start": "4091965",
    "end": "4098085"
  },
  {
    "text": "in the substitution table and you write down the corresponding thing. So h maps to n. So you write n,",
    "start": "4098085",
    "end": "4105164"
  },
  {
    "text": "e maps to m, so you write down m and so on. And so someone did this and then they obviously didn't give you the plaintext.",
    "start": "4105165",
    "end": "4113730"
  },
  {
    "text": "They give you the ciphertext. And so now all you have is a ciphertext. And obviously, he didn't give you the substitution table either.",
    "start": "4113730",
    "end": "4122040"
  },
  {
    "text": "So you- all you have is the ciphertext and you're trying to figure out both the cipher,",
    "start": "4122040",
    "end": "4127575"
  },
  {
    "text": "a, a substitution table, and also the plaintext, okay? So hopefully this, uh, pattern matches something.",
    "start": "4127575",
    "end": "4136185"
  },
  {
    "text": "And let's try to model this as a Hidden Markov Model, okay? So here, the hidden variables are going to be the characters in the plaintext.",
    "start": "4136185",
    "end": "4146120"
  },
  {
    "text": "This is the actual sequence that the hello world. And then observations are the characters of the ciphertext.",
    "start": "4146120",
    "end": "4154525"
  },
  {
    "text": "So familiar equation, the joint distribution of the Hidden Markov Model is given by this equation,",
    "start": "4154525",
    "end": "4161459"
  },
  {
    "text": "and we want to estimate the parameters which include p_start, p_trans, and p_emit.",
    "start": "4161459",
    "end": "4166890"
  },
  {
    "text": "Okay. So how do we go about, um, doing this? So we're gonna approach this in a, um,",
    "start": "4166890",
    "end": "4176025"
  },
  {
    "text": "in a slightly different way, I guess, than, uh,",
    "start": "4176025",
    "end": "4181830"
  },
  {
    "text": "simply just running the algorithm because we have, um, additional structure here, right?",
    "start": "4181830",
    "end": "4189435"
  },
  {
    "text": "So the probability of start, I've no idea, So we just set it to uniform.",
    "start": "4189435",
    "end": "4195150"
  },
  {
    "text": "The, the probability of transition. So this is interesting. So normally if you're doing EM,",
    "start": "4195150",
    "end": "4201180"
  },
  {
    "text": "you don't know the probability of transition. But because we know- let's say we know that,",
    "start": "4201180",
    "end": "4206640"
  },
  {
    "text": "uh, the underlying text was English, we can actually estimate the probability of a wo- of",
    "start": "4206640",
    "end": "4213000"
  },
  {
    "text": "a character given a previous English character from just English text lying around.",
    "start": "4213000",
    "end": "4218190"
  },
  {
    "text": "So this is cool because it allows us to hopefully simplify the problem. Um, I, I should maybe comment that in general,",
    "start": "4218190",
    "end": "4226620"
  },
  {
    "text": "unsupervised learning is really, really hard. And just because you can write down a bunch of equations doesn't mean that will work.",
    "start": "4226620",
    "end": "4232410"
  },
  {
    "text": "And, and, and it's very hard to get it to work. So you want to get as much, um,",
    "start": "4232410",
    "end": "4237989"
  },
  {
    "text": "kind of supervision or, uh, information as you can. Okay. So finally, e- p_emit",
    "start": "4237990",
    "end": "4245655"
  },
  {
    "text": "is the substitution table which we're gonna derive from EM, okay?",
    "start": "4245655",
    "end": "4250875"
  },
  {
    "text": "So now the, the- this might not type check in your head because a substitution specifies for every letter, one other letter,",
    "start": "4250875",
    "end": "4259215"
  },
  {
    "text": "the cipher letter, but in order to fit into this framework, we're going to think about,",
    "start": "4259215",
    "end": "4264690"
  },
  {
    "text": "uh, a distribution over possible cipher, uh, letters given a plaintext letter, okay?",
    "start": "4264690",
    "end": "4270840"
  },
  {
    "text": "With the intention that well, you know, if it's doing its job, it can always, uh, put a probability 1 on the actual cipher letter, okay?",
    "start": "4270840",
    "end": "4281985"
  },
  {
    "text": "So just, you know, stepping back away from the formulas, um, why might you think this could work?",
    "start": "4281985",
    "end": "4288179"
  },
  {
    "text": "And in general, this is not obvious that it should work. But, but what, what you have here is, um, a,",
    "start": "4288180",
    "end": "4295425"
  },
  {
    "text": "a language model p_transition, which tells you kind of what plain text looks like, right?",
    "start": "4295425",
    "end": "4301620"
  },
  {
    "text": "If you gen- generated a cipher and you say, ah, this is my cipher and you get some garbage, then it's probably not right.",
    "start": "4301620",
    "end": "4307515"
  },
  {
    "text": "So we have some information about what we're looking for. Like, so like when you solve a puzzle, you know when you kind of solved it.",
    "start": "4307515",
    "end": "4313860"
  },
  {
    "text": "Um, and then finally we also have this emission that tells us that each,",
    "start": "4313860",
    "end": "4320744"
  },
  {
    "text": "uh, letter that E has to go to kind of be substituted in the same way.",
    "start": "4320745",
    "end": "4325875"
  },
  {
    "text": "So you can't have E going to, you know, different- completely different things at different, uh, points at a time.",
    "start": "4325875",
    "end": "4333070"
  },
  {
    "text": "Okay. So, um, so for doing estimation in HMM,",
    "start": "4334220",
    "end": "4340260"
  },
  {
    "text": "we're going to use the EM algorithm. And remember, in the E-step, we're just doing probabilistic inference.",
    "start": "4340260",
    "end": "4345989"
  },
  {
    "text": "And we saw that forward backward gives us probabilistic inference. In particular, it gives you- for every, um, position,",
    "start": "4345990",
    "end": "4353415"
  },
  {
    "text": "I'm going to give you, uh, my guess which is a distribution of, uh, possible hidden variables.",
    "start": "4353415",
    "end": "4359835"
  },
  {
    "text": "So remember at each position, I observed a cipher letter, and I'm going to guess a distribution over the plain text letter, okay?",
    "start": "4359835",
    "end": "4368670"
  },
  {
    "text": "And then the M-step, I'm just going to count and normalize as we did before.",
    "start": "4368670",
    "end": "4375239"
  },
  {
    "text": "So once I've guessed the cipher letters, I can just, um, compute the probability of some cipher letter given a plain text letter, okay?",
    "start": "4375240",
    "end": "4385635"
  },
  {
    "text": "So I'm actually going to code this up, um, and see, um, it in- so you can see it in action.",
    "start": "4385635",
    "end": "4391079"
  },
  {
    "text": "Okay. I only have five minutes here. So- to make this quick. Um, okay. So here we have ciphertext, okay?",
    "start": "4391080",
    "end": "4401085"
  },
  {
    "text": "Looks pretty good. Um, we're gonna decrypt this or, or decipher it. [NOISE] Um, and then we also have our, uh,",
    "start": "4401085",
    "end": "4408600"
  },
  {
    "text": "text, uh, which is just, uh, some English text that we, uh, found.",
    "start": "4408600",
    "end": "4413790"
  },
  {
    "text": "Um, and then I'm going to- whoops, I wanna decipher and not encipher.",
    "start": "4413790",
    "end": "4419220"
  },
  {
    "text": "Okay. Um, so there's some utilities which are gonna be useful. So things for reading text,",
    "start": "4419220",
    "end": "4425880"
  },
  {
    "text": "converting them to things- into integers, um, normalization of, uh, weight,",
    "start": "4425880",
    "end": "4431190"
  },
  {
    "text": "and then most importantly the s- implementation of a forward-backward algorithm which we're gonna use.",
    "start": "4431190",
    "end": "4436364"
  },
  {
    "text": "Um, [NOISE] okay, because I'm not gonna try to do that in five minutes. [NOISE] Um, okay.",
    "start": "4436365",
    "end": "4441510"
  },
  {
    "text": "So let's initialize the HMM. And then later, we're gonna run EM on it, okay?",
    "start": "4441510",
    "end": "4448395"
  },
  {
    "text": "So the HMM parameters are- there's gonna be start, um, probability which is p in our notation, probability of start,",
    "start": "4448395",
    "end": "4459300"
  },
  {
    "text": "and this is going to be one over the number of possible,",
    "start": "4459300",
    "end": "4465029"
  },
  {
    "text": "uh, letters here, [NOISE] Um, for this is just of- a uniform distribution,",
    "start": "4465029",
    "end": "4471630"
  },
  {
    "text": "uh, 1 over k. And I should say k is 26 plus 1. So this is, uh, lowercase letters plus space.",
    "start": "4471630",
    "end": "4479369"
  },
  {
    "text": "[NOISE] Okay. So now we have our, um, transition probabilities.",
    "start": "4479370",
    "end": "4487440"
  },
  {
    "text": "Um, and I'm going to- this is gonna define a distribution over, uh,",
    "start": "4487440",
    "end": "4493290"
  },
  {
    "text": "um, hidden variable, [NOISE] uh, our next hidden variable given previous hidden variable.",
    "start": "4493290",
    "end": "4499770"
  },
  {
    "text": "And notice that the order is reversed here because I wanna first condition on each one. And then this the- thing is gonna be a distribution.",
    "start": "4499770",
    "end": "4506640"
  },
  {
    "text": "[NOISE] Um, so to do this, I'm going to first, um, remember my strategies.",
    "start": "4506640",
    "end": "4514110"
  },
  {
    "text": "I'm gonna use the language model data to, to count. So I'm going to have,",
    "start": "4514110",
    "end": "4519375"
  },
  {
    "text": "um, again, set things to, uh, 0 for h_2 and range k for h_1 and range k. So this basically gives me a,",
    "start": "4519375",
    "end": "4529920"
  },
  {
    "text": "a matrix of k by k matrix of all 0s. [NOISE] Um, and now I'm going to get my raw text, um,",
    "start": "4529920",
    "end": "4537595"
  },
  {
    "text": "and I'm gonna read it from, um, lm.train which, remember, looks like this.",
    "start": "4537595",
    "end": "4545435"
  },
  {
    "text": "Um, and I'm gonna convert this into a sequence of integers where,",
    "start": "4545435",
    "end": "4552275"
  },
  {
    "text": "um, they're gonna be 0 to k minus 1. Um, and then I'm going to- how do I estimate the transition?",
    "start": "4552275",
    "end": "4559520"
  },
  {
    "text": "Uh, again, counter normalize. So I'm going to go through this sequence and, um,",
    "start": "4559520",
    "end": "4568140"
  },
  {
    "text": "[NOISE] and I'm going to count every successive transition from one character to another.",
    "start": "4568140",
    "end": "4575730"
  },
  {
    "text": "And I'm going to, um, so I'm going to- at position I,",
    "start": "4575730",
    "end": "4580760"
  },
  {
    "text": "I have a character which is rawText of i at position i plus 1,",
    "start": "4580760",
    "end": "4586264"
  },
  {
    "text": "I have another character h_2 [NOISE] and I'm just going to increment this count.",
    "start": "4586265",
    "end": "4592130"
  },
  {
    "text": "[NOISE] Okay? And finally I'm gonna normalize this distribution.",
    "start": "4592130",
    "end": "4598765"
  },
  {
    "text": "So, um, transitionProbs equals, um, so for every,",
    "start": "4598765",
    "end": "4605305"
  },
  {
    "text": "um, h1, um, I have- I have a transition counts of h1,",
    "start": "4605305",
    "end": "4615385"
  },
  {
    "text": "and I can call the helpful normalize function which takes this distribution and normalizes it.",
    "start": "4615385",
    "end": "4622960"
  },
  {
    "text": "Okay? So this is just doing fully observed maximum likelihood count normalized on just the,",
    "start": "4622960",
    "end": "4629635"
  },
  {
    "text": "um, the plain text. Okay? [NOISE] Um, all right.",
    "start": "4629635",
    "end": "4635875"
  },
  {
    "text": "So now emissionProbs, um, this is going to be probability of emits of e given h,",
    "start": "4635875",
    "end": "4642715"
  },
  {
    "text": "and I'm going to just initialize these to uniform because I don't know any better. Um, so 1 over K",
    "start": "4642715",
    "end": "4652390"
  },
  {
    "text": "for e in range K for range K. So it just so happens that both the,",
    "start": "4652390",
    "end": "4659815"
  },
  {
    "text": "um, hidden variables and observed variables have the same domain, um, that's not generally the case.",
    "start": "4659815",
    "end": "4665185"
  },
  {
    "text": "Okay? So now, I'm ready to run EM. So, um, gonna do 200 iterations just- just put in a number.",
    "start": "4665185",
    "end": "4675505"
  },
  {
    "text": "Um, so have the E-step and the M-step. So in E-step, remember I'm gonna do probabilistic inference.",
    "start": "4675505",
    "end": "4681790"
  },
  {
    "text": "I'm gonna call forward backward. And remember this is, um,",
    "start": "4681790",
    "end": "4688525"
  },
  {
    "text": "this is going to be basically in our notation at the position I- what is my distribution over hidden states?",
    "start": "4688525",
    "end": "4697855"
  },
  {
    "text": "So this is actually forward backward, um, oh, I need a, um,",
    "start": "4697855",
    "end": "4705040"
  },
  {
    "text": "read some observations and read my ciphertext, uh,",
    "start": "4705040",
    "end": "4710380"
  },
  {
    "text": "ciphertext, and I'm going to convert this again into an integer array.",
    "start": "4710380",
    "end": "4716830"
  },
  {
    "text": "Um, and then I have observations, and then I pass in the parameters of my,",
    "start": "4716830",
    "end": "4721989"
  },
  {
    "text": "um, hidden Markov model, um, and then, I have my guess.",
    "start": "4721990",
    "end": "4728920"
  },
  {
    "text": "So let me print out what that guess is. Okay? So what I'm gonna do here is for every position,",
    "start": "4728920",
    "end": "4734215"
  },
  {
    "text": "I'm going to get the most, uh, likely outcome of h. So util of argmax of q i,",
    "start": "4734215",
    "end": "4744160"
  },
  {
    "text": "um, for i in range, um, length of observations.",
    "start": "4744160",
    "end": "4750440"
  },
  {
    "text": "Okay? So that gives me an array of guesses for each position, um,",
    "start": "4750780",
    "end": "4758034"
  },
  {
    "text": "and then I'm gonna convert that into a string so it's easier to look at, and put them in line.",
    "start": "4758035",
    "end": "4764800"
  },
  {
    "text": "[NOISE] So this is printing on the best guess.",
    "start": "4764800",
    "end": "4771385"
  },
  {
    "text": "Okay. So finally for the M-step, um, I'm go- I have my,",
    "start": "4771385",
    "end": "4776650"
  },
  {
    "text": "uh, q which gives me counts. So now I pretend I have a lot of, you know, data which are weighted by q.",
    "start": "4776650",
    "end": "4782920"
  },
  {
    "text": "So I'm gonna have emission, um, counts equals, uh, same as before.",
    "start": "4782920",
    "end": "4790810"
  },
  {
    "text": "I'm just gonna get a matrix of 0s, um, e in range K for h in range K,",
    "start": "4790810",
    "end": "4797665"
  },
  {
    "text": "um, and then I'm gonna go through the sequence, from i equals range of through the length of the observation sequence,",
    "start": "4797665",
    "end": "4807220"
  },
  {
    "text": "um, and for each, uh, position in the sequence, I'm gonna consider all the possible,",
    "start": "4807220",
    "end": "4813985"
  },
  {
    "text": "um, possible plaintext hidden values, [NOISE] and I'm going to increment, um,",
    "start": "4813985",
    "end": "4821394"
  },
  {
    "text": "h, observations of i, so this is the observation that I actually saw.",
    "start": "4821395",
    "end": "4828340"
  },
  {
    "text": "Um, and this should be qih which is the weight of,",
    "start": "4828340",
    "end": "4834295"
  },
  {
    "text": "uh, eight h at position i. Okay? And then finally I'm just going to normalize.",
    "start": "4834295",
    "end": "4843175"
  },
  {
    "text": "So, um, like what I did, uh, well, okay I'll just normalize it here.",
    "start": "4843175",
    "end": "4849280"
  },
  {
    "text": "So emission probabilities is util dot, um, normalize of the accounts, um,",
    "start": "4849280",
    "end": "4858145"
  },
  {
    "text": "for h in range K. Okay?",
    "start": "4858145",
    "end": "4864095"
  },
  {
    "text": "And I think that's, uh, pretty much it. Um, lemme me see if this- okay.",
    "start": "4864095",
    "end": "4871135"
  },
  {
    "text": "Um, this should be [NOISE] for for e but- right?",
    "start": "4871135",
    "end": "4883345"
  },
  {
    "text": "Okay. Okay. Good. Um, okay. So let's ru- run this and then I'll go back to",
    "start": "4883345",
    "end": "4888520"
  },
  {
    "text": "the- lemme just go over that code just one more time. Okay. So, [LAUGHTER] uh,",
    "start": "4888520",
    "end": "4893800"
  },
  {
    "text": "we initialize the, the probabilities, to uniform. For transition probabilities we can use our observations,",
    "start": "4893800",
    "end": "4901410"
  },
  {
    "text": "uh, or the- just a plaintext. We just count and normalize. In emissions, um, we initialize with uniform,",
    "start": "4901410",
    "end": "4910215"
  },
  {
    "text": "and then while running EM, we read the ciphertext, and then in the E-step, we're going to use the current prob- parameters to guess where the ciphertext is.",
    "start": "4910215",
    "end": "4921969"
  },
  {
    "text": "And then we're going to update our estimate of what the parameters are given our guess of what the ciphertext is.",
    "start": "4921970",
    "end": "4929200"
  },
  {
    "text": "Okay. So here's the final moment of truth. Uh, the ciphertext- and so",
    "start": "4929200",
    "end": "4935890"
  },
  {
    "text": "remember each iteration it's going to print out the best guess. So hope, it'll look like gibberish for a little bit.",
    "start": "4935890",
    "end": "4944005"
  },
  {
    "text": "It's not gonna be perfect, um, but this is- it starts to look somewhat like English.",
    "start": "4944005",
    "end": "4953170"
  },
  {
    "text": "Um, there's an and, and in my in there, um, anyone can read this?",
    "start": "4953170",
    "end": "4959920"
  },
  {
    "text": "[BACKGROUND] Alone without- okay, that looks like English.",
    "start": "4959920",
    "end": "4966115"
  },
  {
    "text": "Can be this like anyone that I could. Anyone to guess what this text is?",
    "start": "4966115",
    "end": "4974060"
  },
  {
    "text": "So here's the plain text. So I've lived my life alone without anyone that I could",
    "start": "4975000",
    "end": "4983409"
  },
  {
    "text": "really talk to until I had an accident with my plane and the,",
    "start": "4983410",
    "end": "4989905"
  },
  {
    "text": "um, does with my- whatever something. [LAUGHTER] But so, again,",
    "start": "4989905",
    "end": "4997870"
  },
  {
    "text": "unsupervised learning doesn't- it's not magic. It doesn't always work, but here you at least see some signal, and in the actual application,",
    "start": "4997870",
    "end": "5005250"
  },
  {
    "text": "they got partway there and then, you can iterate on this man- kind of in a manual way. [BACKGROUND] Okay.",
    "start": "5005250",
    "end": "5011070"
  },
  {
    "text": "[BACKGROUND] Oops. All right. So that's for it for Bayesian networks.",
    "start": "5011070",
    "end": "5017580"
  },
  {
    "text": "On Wednesday we'll do [NOISE] logic.",
    "start": "5017580",
    "end": "5020170"
  }
]