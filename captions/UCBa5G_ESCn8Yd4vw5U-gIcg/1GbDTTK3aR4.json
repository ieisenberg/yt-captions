[
  {
    "start": "0",
    "end": "6980"
  },
  {
    "text": "Do folks know what this is? There's [INAUDIBLE] Stanford.",
    "start": "6980",
    "end": "12200"
  },
  {
    "text": "That's Stanford [INAUDIBLE]. You know which one? Old person.",
    "start": "12200",
    "end": "17670"
  },
  {
    "text": "What is this? What's going on here? The first Dartmouth [INAUDIBLE].",
    "start": "17670",
    "end": "23573"
  },
  {
    "text": "That's right. And then-- What is the association\nto Stanford here? ",
    "start": "23573",
    "end": "30480"
  },
  {
    "text": "I believe this is [INAUDIBLE]. Yeah. He started Sail, if I\nunderstand correctly.",
    "start": "30480",
    "end": "36570"
  },
  {
    "text": "Is that right? He started Sail? Yeah, I think he did. But anyways.",
    "start": "36570",
    "end": "42570"
  },
  {
    "text": "So what's interesting is-- So it's amusing to actually look\nat what they wrote in there.",
    "start": "42570",
    "end": "50738"
  },
  {
    "text": "I don't know if it was\na brochure, or what. But they wrote in\nthere their goals.",
    "start": "50738",
    "end": "56370"
  },
  {
    "text": "The font is a bit small. So the study is to proceed on\nthe basis of the conjecture that every aspect of\nlearning or any other feature",
    "start": "56370",
    "end": "64739"
  },
  {
    "text": "of intelligence can, in\nprinciple, be so precisely described that a machine\ncan be made to simulate it.",
    "start": "64739",
    "end": "70920"
  },
  {
    "text": "Fantastic. So single machine. You want to simulate all\nof human intelligence.",
    "start": "70920",
    "end": "76020"
  },
  {
    "text": "And a carefully selected\ngroup of scientists-- and we think that we\ncan make, actually,",
    "start": "76020",
    "end": "81270"
  },
  {
    "text": "the paragraph right before the\nsecond set of red underlines,",
    "start": "81270",
    "end": "87000"
  },
  {
    "text": "is-- we think that a\nsignificant advance can be made in one or\ntwo of these problems if a carefully selected\ngroup of scientists",
    "start": "87000",
    "end": "96180"
  },
  {
    "text": "work together for a summer. I don't think they knew of\nAI winters then, actually.",
    "start": "96180",
    "end": "101490"
  },
  {
    "text": "They didn't know of it then. And the third thing I'm using\nis-- but the major obstacle",
    "start": "101490",
    "end": "107159"
  },
  {
    "text": "is not lack of machine\ncapacity, but our inability to write programs taking full\nadvantage of what we have.",
    "start": "107160",
    "end": "114102"
  },
  {
    "text": "So while the goals\nwere noble, it's surprising how wrong\nyou can be in-- with some of the smartest\npeople in the room.",
    "start": "114102",
    "end": "120060"
  },
  {
    "text": "So Selfridge, a\nneural network OG that wrote the\noriginal pandemoniums--",
    "start": "120060",
    "end": "125160"
  },
  {
    "text": "I think he got\neverything, basically, except problems with\nblack box optimization. Then Minsky.",
    "start": "125160",
    "end": "131700"
  },
  {
    "text": "Of course, Shannon. Solomonoff-- I think\nthis is Solomonoff MDL. In many ways, you can argue\nthat's the underpinning",
    "start": "131700",
    "end": "139740"
  },
  {
    "text": "of self-supervised\nlearning today. But it's really amazing\nto see the first--",
    "start": "139740",
    "end": "148080"
  },
  {
    "text": "at least I don't\nknow if we'll be able to characterize\nor write down",
    "start": "148080",
    "end": "154347"
  },
  {
    "text": "all the rules for intelligence. So you can imagine\nthe approaches they were taking were all\nthese rule-based systems.",
    "start": "154348",
    "end": "159780"
  },
  {
    "text": "And they couldn't be more\nwrong on machine capacity. Today's transformers are-- they\ndon't-- they're data centers.",
    "start": "159780",
    "end": "169590"
  },
  {
    "text": "And I guess they needed a\nreally, really long summer",
    "start": "169590",
    "end": "174599"
  },
  {
    "text": "to solve this one. But yeah. So it's 1955, so\nabout, yeah, 60 years--",
    "start": "174600",
    "end": "183870"
  },
  {
    "text": "no, no. Getting close to 70 years. And we're basically talking\nabout the same problems",
    "start": "183870",
    "end": "191010"
  },
  {
    "text": "again, except maybe\nsome things work. Some things don't work.",
    "start": "191010",
    "end": "197220"
  },
  {
    "text": "And this talk is\nabout some of the-- one of the pieces that has made\nthis larger enterprise work.",
    "start": "197220",
    "end": "203760"
  },
  {
    "text": "And we're getting closer to the\noriginal goals of the Dartmouth Conference. Yeah.",
    "start": "203760",
    "end": "209610"
  },
  {
    "text": "Again, so this is the big gaps. So what eventually happened\nin the field was that",
    "start": "209610",
    "end": "215400"
  },
  {
    "text": "their goal of having a single\nsystem that explained most of--",
    "start": "215400",
    "end": "221250"
  },
  {
    "text": "that was able to mimic our\ncognitive abilities, which would definitely mean\nimage processing, or image understanding\nand language processing",
    "start": "221250",
    "end": "227850"
  },
  {
    "text": "as well, that-- the field got-- a single\nmodel or a single approach",
    "start": "227850",
    "end": "234240"
  },
  {
    "text": "to do all these things\nwas shattered by thousands of different research projects. So there was no consolidation.",
    "start": "234240",
    "end": "241439"
  },
  {
    "text": "But here's another. This is going to\nbe a harder one. Can you tell what\nis a-- this is 2009.",
    "start": "241440",
    "end": "249000"
  },
  {
    "text": "And this a-- this is\nnot a single system. This is a complicated\nmachine translation system. So when I started my PhD,\nmachine translation systems",
    "start": "249000",
    "end": "256320"
  },
  {
    "text": "used to be a bit more\ncomplicated than this, actually. Thousands of pipeline systems.",
    "start": "256320",
    "end": "261690"
  },
  {
    "text": "You had to first\nextract-- you had to first do word alignments that\nactually looked like attention. It's like [INAUDIBLE] attention.",
    "start": "261690",
    "end": "268530"
  },
  {
    "text": "Then, based on that, you\nextracted how larger phrases aligned with other phrases. Then you had to figure out\nhow to-- how they-- then you",
    "start": "268530",
    "end": "274380"
  },
  {
    "text": "had to teach. There was some machine\nlearning there. You had to teach your\nmodel how to score them connecting with each other.",
    "start": "274380",
    "end": "280005"
  },
  {
    "text": "So can you-- does\nanybody know where a neural network is in this? ",
    "start": "280005",
    "end": "291340"
  },
  {
    "text": "All right. So this is a machine\ntranslation system from 2009.",
    "start": "291340",
    "end": "297160"
  },
  {
    "text": "And CSLM is a Continuous\nState Language Model that they would\nuse for re-scoring.",
    "start": "297160",
    "end": "303280"
  },
  {
    "text": "So the world was\nso discrete then that you had to call these\nmodels Continuous State Language Models.",
    "start": "303280",
    "end": "308310"
  },
  {
    "text": "And it was largely inspired\nby the Neural Probabilistic Language Model by-- oh, it doesn't appear.",
    "start": "308310",
    "end": "314966"
  },
  {
    "text": "Huh. Sorry.",
    "start": "314966",
    "end": "320250"
  },
  {
    "text": "Oh, there, the Neural\nProbabilistic Language Model by Bengio, I think\nit was, in 2003.",
    "start": "320250",
    "end": "325960"
  },
  {
    "text": "And so we were-- even in 2013, when I published a\npaper on neural network language",
    "start": "325960",
    "end": "333960"
  },
  {
    "text": "models, these models were still\nbeing put into the feed forward.",
    "start": "333960",
    "end": "339509"
  },
  {
    "text": "Neural network language\nmodels were still re-scoring. And now, it's incredible\nif you think about it, just",
    "start": "339510",
    "end": "346229"
  },
  {
    "text": "in terms of consolidation, how\nall of these complicated systems that have now been\nreplaced by just",
    "start": "346230",
    "end": "351330"
  },
  {
    "text": "neurons that talk to each other. And you just learn\nthe rules from-- you just learn the rules\nfrom data automatically.",
    "start": "351330",
    "end": "358370"
  },
  {
    "text": "So it's fun to-- it's interesting to see. And since then-- so this is\nwhat the NLP 2013 conference",
    "start": "358370",
    "end": "369729"
  },
  {
    "text": "was like. You see these different-- these-- you can\ncall it verticalized",
    "start": "369730",
    "end": "374790"
  },
  {
    "text": "NLPs, different areas\nlike morphology, dialogue, and discourse.",
    "start": "374790",
    "end": "379840"
  },
  {
    "text": "I don't even know if people\ntalk about [INAUDIBLE] this talk tomorrow. I don't know if there's\na research track anymore.",
    "start": "379840",
    "end": "385750"
  },
  {
    "text": "Then there is a\nmachine translation. So there's opinion mining\nand sentiment analysis. Now models make\nyou angry or upset.",
    "start": "385750",
    "end": "394540"
  },
  {
    "text": "And so you could see\nthat, just in 2013, the field-- even research was\ndivided into these smaller",
    "start": "394540",
    "end": "400160"
  },
  {
    "text": "tracks. And everybody had\ntheir own specific-- they were bringing their own\nspecific domain information.",
    "start": "400160",
    "end": "407162"
  },
  {
    "text": "And they had to specialize\nin a domain in order to solve some task. And we solved tasks\nto some degree. Machine translation-- probably\nbecause of a lot of government",
    "start": "407162",
    "end": "415940"
  },
  {
    "text": "funding as well, we have\nmade a lot of progress. And we were making practical\ntranslation systems that were being deployed in the wild.",
    "start": "415940",
    "end": "421840"
  },
  {
    "text": "Google Translate was a\ngreat example of that. And so since then, you\nhave this-- we started to--",
    "start": "421840",
    "end": "431860"
  },
  {
    "text": "first, we all agreed we\nneed distributed word representations. And you saw this.",
    "start": "431860",
    "end": "437050"
  },
  {
    "text": "People probably\nremember this funky embedding algebra, King minus\nMan plus Woman equals Queen,",
    "start": "437050",
    "end": "443919"
  },
  {
    "text": "from Word2Vec.  And we had a--",
    "start": "443920",
    "end": "450370"
  },
  {
    "text": "there was a big industry\nof models that actually-- that just-- that learned\nword representations.",
    "start": "450370",
    "end": "456817"
  },
  {
    "text": "And these word\nrepresentations were actually useful in downstream tasks. And then came another step\nin this process, where now we",
    "start": "456817",
    "end": "467620"
  },
  {
    "text": "started saying, OK,\nthese representations-- they're only helpful if\nthey're learned in context.",
    "start": "467620",
    "end": "474430"
  },
  {
    "text": "So the King should change based\non context, the King of Persia,",
    "start": "474430",
    "end": "479830"
  },
  {
    "text": "or the king has no clothes,\nor the emperor has no clothes. And so we saw these--\nwe saw approaches",
    "start": "479830",
    "end": "487690"
  },
  {
    "text": "like sequence-to-sequence\nto sequence learning, where we started to formulate. We started to create these\ngeneral formulations of how",
    "start": "487690",
    "end": "495310"
  },
  {
    "text": "to solve any task in NLP. So sequence-to-sequence\nformulation-- you can formulate\nmany tasks in language",
    "start": "495310",
    "end": "503437"
  },
  {
    "text": "as sequence-to-sequence,\nquestion answering, machine translation, dialogue. And then, of course, we had--",
    "start": "503437",
    "end": "510310"
  },
  {
    "text": "then we developed\nattention, which was a very effective,\ncontent-based way to summarize information.",
    "start": "510310",
    "end": "517719"
  },
  {
    "text": "Typically, you have these\nencoder-decoder architectures. Everybody is probably,\nI'm guessing, familiar with encoder-decoder\narchitectures, right?",
    "start": "517720",
    "end": "524229"
  },
  {
    "text": "So encoder-decoder architecture,\nin a position on the decoder side, could summarize,\nbased on its content,",
    "start": "524230",
    "end": "530530"
  },
  {
    "text": "all the information on\nthe source sentence. And this is a really\neffective, content-based way of summarizing information.",
    "start": "530530",
    "end": "535990"
  },
  {
    "text": "And what started\nhappening was we started-- these general paradigms\nstarted coming up.",
    "start": "535990",
    "end": "543610"
  },
  {
    "text": "Sequence-to-sequence\nlearning can solve-- it can install most\nlanguage problems because most language\nproblems have",
    "start": "543610",
    "end": "549280"
  },
  {
    "text": "to deal with learning\nrepresentations of variables length. The goal is to learn\nrepresentations of variable-length sequences.",
    "start": "549280",
    "end": "554688"
  },
  {
    "text": "And if you do that successfully,\nyou can then potentially solve that problem. And then attention\nwas an excellent way--",
    "start": "554688",
    "end": "559930"
  },
  {
    "text": "a content-based way to\nactually summarize information from some neighborhood. ",
    "start": "559930",
    "end": "568269"
  },
  {
    "text": "And the major\nworkhorse until then were these recurrent\nmodels, or LSTMs,",
    "start": "568270",
    "end": "573520"
  },
  {
    "text": "where, basically, the method\nwas typically the same. You had a sentence, and\nyou crushed the sentence",
    "start": "573520",
    "end": "580960"
  },
  {
    "text": "into a set of vectors that are\nrepresentations, one typically-- typically, one\nfor each position.",
    "start": "580960",
    "end": "587230"
  },
  {
    "text": "And the way LSTMs\ndid it was where they walked along the sentence. They ate up a\nword, and then they",
    "start": "587230",
    "end": "593649"
  },
  {
    "text": "summarized the entire history\ninto one fixed bottleneck. And that bottleneck\nwas then transmitted--",
    "start": "593650",
    "end": "599110"
  },
  {
    "text": "was updated based\non the next word. And if you were successfully\nable to learn representations,",
    "start": "599110",
    "end": "606756"
  },
  {
    "text": "then we could solve these tasks\nlike translation summarization dialogue. So an important movement. And in 2020, I--",
    "start": "606757",
    "end": "613815"
  },
  {
    "text": "I guess, when was the\nsequence-to-sequence learning papers? 2015 NeurIPS. Then we saw the attention\npaper in-- around 2015, 2016.",
    "start": "613815",
    "end": "621658"
  },
  {
    "text": "And the machine\ntranslation community was the first to\nrespond and say, hey, machine translation is a\nclassic sequence-to-sequence",
    "start": "621658",
    "end": "628389"
  },
  {
    "text": "learning problem. Why don't we now first\nstart re-scoring? And then can we\nstill build native",
    "start": "628390",
    "end": "633405"
  },
  {
    "text": "to really rethink\nmachine translation with these\nsequence-to-sequence models? And these are fantastic models.",
    "start": "633405",
    "end": "639830"
  },
  {
    "text": "I don't know if you guys have\never done these exercises on-- LSTMs can count.",
    "start": "639830",
    "end": "645850"
  },
  {
    "text": "For example, if you train\nan encoder-decoder on-- to model a to the n, b to\nthe n-- so you feed in n a's.",
    "start": "645850",
    "end": "653650"
  },
  {
    "text": "And you ask the decoder\nto predict n b's. And you-- actually just\na single-cell LSTM--",
    "start": "653650",
    "end": "659230"
  },
  {
    "text": "if you know the\nstructure of an LSTM, there's a cell that\nbasically keeps-- so it's a notion of state.",
    "start": "659230",
    "end": "664779"
  },
  {
    "text": "And just a single cell\nis able to actually just do trivial counting. It counts how many\na's you consumed,",
    "start": "664780",
    "end": "671680"
  },
  {
    "text": "and then it decrements it. And then when you\nconsume all the-- exactly the same number of b's as\nthe number of a's, something",
    "start": "671680",
    "end": "677132"
  },
  {
    "text": "lights up and says, I'm done. I've recognized this language. So you can train trivial\nlike a to the n, b to the n. And here, you have a--",
    "start": "677132",
    "end": "683210"
  },
  {
    "text": "I'm sorry this is not clear. But you have somewhat of a-- you have a grammar here. And you can see that\nthese are different cells.",
    "start": "683210",
    "end": "688808"
  },
  {
    "text": "This is about eight cells here. And each one of\nthese cells actually increments its counter once\nit reads a particular symbol.",
    "start": "688808",
    "end": "694490"
  },
  {
    "text": "And it's able to actually\ntrack how deep you are in this hierarchy\nand this grammar.",
    "start": "694490",
    "end": "700370"
  },
  {
    "text": "And Google, of course-- the crowning\nachievement, perhaps, of sequence-to-sequence models,\nwhich I was actually right--",
    "start": "700370",
    "end": "709980"
  },
  {
    "text": "I was fortunate to be\nin the same cubicle as it was being-- as\nthis work was being done,",
    "start": "709980",
    "end": "715940"
  },
  {
    "text": "was the Google Neural\nMachine Translation System, where they took LSTMs. They added many\nadvancements towards a lot",
    "start": "715940",
    "end": "722720"
  },
  {
    "text": "of systems, improvements to a\nlot of data that Google had. And they produced\nwhat you might-- at that time, the\nstate of the art",
    "start": "722720",
    "end": "729285"
  },
  {
    "text": "neural machine\ntranslation system and sequence-to-sequence models. So now this big, consolidated--\nthis big, complicated system,",
    "start": "729285",
    "end": "736820"
  },
  {
    "text": "which looked much\nmore complicated, had now become a homogeneous--\njust a single homogeneous neural",
    "start": "736820",
    "end": "742160"
  },
  {
    "text": "network. So at that time, the biggest\nfrustration we had was--",
    "start": "742160",
    "end": "749000"
  },
  {
    "text": "the LSTMs were the\nprimary workhorse. And the biggest frustration\nwe had was, they--",
    "start": "749000",
    "end": "755990"
  },
  {
    "text": "Not only were we producing. Not only were we--\ndid we produce the output autoregressively.",
    "start": "755990",
    "end": "761708"
  },
  {
    "text": "We were sequentially decoding\nthe output left-to-right. But also, we were reading\nthe input sequentially. So you had to--",
    "start": "761708",
    "end": "768470"
  },
  {
    "text": "In order to produce the\nrepresentation for the 10th word, you had to eat up the\nfirst word, the second word,",
    "start": "768470",
    "end": "773840"
  },
  {
    "text": "the third word. So that was really slow,\nand not the whole--",
    "start": "773840",
    "end": "779540"
  },
  {
    "text": "and another big\nproblem with LSTMs were that you have\nthis bottleneck that basically-- that\ncontains all the information",
    "start": "779540",
    "end": "787160"
  },
  {
    "text": "about your past. So you have to now crush-- you have to pack both\nlong-distance interactions",
    "start": "787160",
    "end": "793550"
  },
  {
    "text": "that you might have\nand local interactions through the single fixed vector\nthat you need to transmit.",
    "start": "793550",
    "end": "799160"
  },
  {
    "text": "And sequentiality doesn't--\ninhibits parallelism, which means that you\ncouldn't even read.",
    "start": "799160",
    "end": "804946"
  },
  {
    "text": "The encoder couldn't even\nread the sentence in parallel. And of course, decoding\nwas auto-regressive. So you couldn't even\nwrite in parallel.",
    "start": "804947",
    "end": "812830"
  },
  {
    "text": "And convolutions\nwere-- they were starting to emerge as\na solution, largely.",
    "start": "812830",
    "end": "820448"
  },
  {
    "text": "They had been very successful\nin computer vision. They had also figured out\nhow to optimize them while-- how to make them--\nhow to train--",
    "start": "820448",
    "end": "826120"
  },
  {
    "text": "how to make them really\nfast on GPUs, where-- because they're just basically\nmatrix multiplications.",
    "start": "826120",
    "end": "832240"
  },
  {
    "text": "And matrix multiplication\nis largely-- it's parallelizable. So convolutions were a\nsolution to this problem",
    "start": "832240",
    "end": "839889"
  },
  {
    "text": "of not being able to read in\nparallel because you could-- in parallel, every word,\nbasically, produced",
    "start": "839890",
    "end": "846610"
  },
  {
    "text": "its representations by\nlooking at its neighbors, its local neighbors.",
    "start": "846610",
    "end": "852010"
  },
  {
    "text": "And there were some very-- there were some breakthrough\npapers, such as ByteNet,",
    "start": "852010",
    "end": "857649"
  },
  {
    "text": "for machine translation,\nthe Convolutional Sequence 2 Sequence model that\nwas contemporaneous to the transformer, actually\nprobably predated it",
    "start": "857650",
    "end": "864132"
  },
  {
    "text": "by a few months, where\nthey used convolutions both in the encoder and decoder\nto get good scores on machine",
    "start": "864132",
    "end": "869769"
  },
  {
    "text": "translation that were better\nthan the Google Neural Machine Translation System. And of course, probably\nthe most successful",
    "start": "869770",
    "end": "879718"
  },
  {
    "text": "was WaveNet, which was a\ntext-to-speech system that was state of the art at the time. ",
    "start": "879718",
    "end": "887710"
  },
  {
    "text": "So convolution still had\nthis problem that-- one, I guess-- they were\nparallelizable.",
    "start": "887710",
    "end": "893590"
  },
  {
    "text": "But the issue was that you\nstill-- you couldn't directly capture long-distance\ninteractions between-- you",
    "start": "893590",
    "end": "901029"
  },
  {
    "text": "couldn't directly capture\nlong-distance interactions between words. So if you're basically\na receptive field--",
    "start": "901030",
    "end": "906160"
  },
  {
    "text": "if it's a 3 by 3-- If it's a 1 by 3, then it\nbasically grows linearly",
    "start": "906160",
    "end": "912130"
  },
  {
    "text": "with the factor of--\nit grows linearly with the number of layers\neach time it expands by 3.",
    "start": "912130",
    "end": "919140"
  },
  {
    "text": "So you still needed a\nlinear number of layers to capture these\nlong-distance relationships. But attention, on\nthe other, hand",
    "start": "919140",
    "end": "925380"
  },
  {
    "text": "was this really effective\nmechanism that we knew was-- that could actually get us--",
    "start": "925380",
    "end": "931770"
  },
  {
    "text": "In one, it could actually\ncapture all the interactions between one word\nand every other word",
    "start": "931770",
    "end": "939060"
  },
  {
    "text": "using content-based addressing. Because convolutions\nbasically match-- convolutions match\nweights with parameters.",
    "start": "939060",
    "end": "944826"
  },
  {
    "text": "Attention was actually able\nto use content with content. So based on how similar\nI am to my neighborhood--",
    "start": "944827",
    "end": "950279"
  },
  {
    "text": "based on how similar\nI am to my neighbors, I'm going to absorb\nthat information. And this actually-- this motif\nactually appears everywhere,",
    "start": "950280",
    "end": "956790"
  },
  {
    "text": "even in computer vision. So maybe, actually,\nI can go there. So here's a-- in vision,\nthere's this approach.",
    "start": "956790",
    "end": "965100"
  },
  {
    "text": "Do people here know\nof non-local means? So in computer vision, there's\nan approach called non-local",
    "start": "965100",
    "end": "971980"
  },
  {
    "text": "means that's basically-- it was originally developed\nfor image denoising.",
    "start": "971980",
    "end": "977950"
  },
  {
    "text": "So if you want to\ndenoise an image patch, you basically-- you look\nat all your neighbors, and you see which patch\nis very similar to you.",
    "start": "977950",
    "end": "984132"
  },
  {
    "text": "And based on the\nsimilarity, you actually pull in that information. And this largely works\nin images because images",
    "start": "984132",
    "end": "989620"
  },
  {
    "text": "are very self-similar. They start sounding like,\nhey, based on content, I want to pull in information.",
    "start": "989620",
    "end": "995110"
  },
  {
    "text": "And again, there were similar--\nthere were approaches like Texture Synthesis\nby Efros, where,",
    "start": "995110",
    "end": "1000930"
  },
  {
    "text": "if you wanted to do painting,\nor if you wanted to generate an image, then you would look at\na patch that's similar to this",
    "start": "1000930",
    "end": "1006900"
  },
  {
    "text": "rectangle in some other-- in your dictionary,\nor in a database that",
    "start": "1006900",
    "end": "1011910"
  },
  {
    "text": "you have of patches. And then, based on what's\nclosest, you actually bring it. So you bring that patch,\nand then you Paste it there.",
    "start": "1011910",
    "end": "1018330"
  },
  {
    "text": "So these approaches\nthat look like attention were already prevalent. It's a very natural formulation. ",
    "start": "1018330",
    "end": "1025630"
  },
  {
    "text": "And the ByteNow paper had shown\nthat this actually works really well for language as well.",
    "start": "1025630",
    "end": "1032140"
  },
  {
    "text": "So the question then\nwas, why can't we then learn representations? Instead of doing this\nsource target, why can't we",
    "start": "1032140",
    "end": "1039160"
  },
  {
    "text": "actually learn representations\nby the sentence attending onto itself? So now you basically use--",
    "start": "1039160",
    "end": "1044967"
  },
  {
    "text": "instead of attending-- a\nsource sentence attending to a target sentence, can\nit just attend to itself? And the original goal of\nactually when we wanted--",
    "start": "1044967",
    "end": "1052150"
  },
  {
    "text": "when we were-- when we wanted to\nactually do parallel decoding--",
    "start": "1052150",
    "end": "1057340"
  },
  {
    "text": "So attention by\nconstruction is paralyzable because each token can basically\nconstruct its representations",
    "start": "1057340",
    "end": "1065410"
  },
  {
    "text": "from its neighbors in parallel. And it directly captures\ntoken-to-token interactions",
    "start": "1065410",
    "end": "1071169"
  },
  {
    "text": "because now, of course, we'll\nrun into complexities of length. But we can-- and\nwe'll discuss how",
    "start": "1071170",
    "end": "1078039"
  },
  {
    "text": "to solve some of these things\nlater, or overcome them. But you can direct-- instead\nof having this linear growth and receptive field,\nyou can directly",
    "start": "1078040",
    "end": "1084159"
  },
  {
    "text": "capture these interactions\nbecause convolutions-- if you have a very, very\nlarge receptive field, it gets computationally\nvery expensive.",
    "start": "1084160",
    "end": "1089590"
  },
  {
    "text": "And it also had\nthese explicit gating and multiplicative\ninteractions, which we've often seen in Gated\nPixelCNN, or [INAUDIBLE]..",
    "start": "1089590",
    "end": "1096850"
  },
  {
    "text": "These explicit gated\nmultiplicative interactions have typically helped\ntraining and have",
    "start": "1096850",
    "end": "1103210"
  },
  {
    "text": "led to better accuracies. And as I mentioned, the original\nmotivation of why we actually",
    "start": "1103210",
    "end": "1109990"
  },
  {
    "text": "wanted to do this was-- we\nsaid, hey, so the LSTMs are--",
    "start": "1109990",
    "end": "1115170"
  },
  {
    "text": "we have good\ntranslation systems. But the problem is that,\nactually, both reading and writing, sequentially-- can\nwe actually do both in parallel?",
    "start": "1115170",
    "end": "1122710"
  },
  {
    "text": "So we wanted to read the\nGerman sentence in parallel and then translate it, and then\nalso write in parallel by that.",
    "start": "1122710",
    "end": "1130600"
  },
  {
    "text": "Instead of actually decoding\nit autoregressively, can you decode it-- instead of\ndecoding it in time, can you decode it in height?",
    "start": "1130600",
    "end": "1137320"
  },
  {
    "text": "So you first spit out one word. Or you spit out all the words,\nand you iteratively refine them.",
    "start": "1137320",
    "end": "1142360"
  },
  {
    "text": "And this was-- this turned out\nto be very, very challenging and hasn't been solved\nsuccessfully until today.",
    "start": "1142360",
    "end": "1149453"
  },
  {
    "text": "Because the biggest challenge,\nessentially, is, when you-- whenever you're\ndecoding, essentially, as you predict a word, you bend\nthe probability distribution",
    "start": "1149453",
    "end": "1156400"
  },
  {
    "text": "that then nails down-- narrows down what you're\ngoing to predict later on. And the ordering that\nallows you to basically--",
    "start": "1156400",
    "end": "1164170"
  },
  {
    "text": "the ordering that allows\nyou to nail these modes was very hard to learn. So imposing left-right\nordering is much easier",
    "start": "1164170",
    "end": "1171190"
  },
  {
    "text": "than actually not\nhaving one and having to learn it as you're decoding. So the original approach\njust didn't work.",
    "start": "1171190",
    "end": "1177040"
  },
  {
    "text": "But then we still\nhad our salvation in being able to\nread parallelly.",
    "start": "1177040",
    "end": "1182890"
  },
  {
    "text": "So we said, all right,\nlet's take this back to the encoder-decoder models.  And unlike-- at that time,\nthere were a few formulations.",
    "start": "1182890",
    "end": "1191260"
  },
  {
    "text": "So we had this-- the original formulation\nof attention from Graves. Then we had the Additive\nattention formulation.",
    "start": "1191260",
    "end": "1198850"
  },
  {
    "text": "And we took the Dot-Product\nattention formulation, largely because it\nallowed us to do--",
    "start": "1198850",
    "end": "1205183"
  },
  {
    "text": "because it allowed us\nto actually do attention as a matrix multiplication. And oftentimes, some of\nthe biggest constraints",
    "start": "1205183",
    "end": "1211450"
  },
  {
    "text": "that actually-- physics\nis such a big constraint in neural networks\nthat, if you can",
    "start": "1211450",
    "end": "1218860"
  },
  {
    "text": "make your architecture amenable\nto modern accelerators,",
    "start": "1218860",
    "end": "1224017"
  },
  {
    "text": "you have a much better\nchance of succeeding. And Dot-Product attention\ncould be expressed as a matrix multiplication.",
    "start": "1224017",
    "end": "1229690"
  },
  {
    "text": "And there are already\nkernels for being able to do matrix multiplication\nvery effectively on the GPU.",
    "start": "1229690",
    "end": "1236980"
  },
  {
    "text": "So the formulation was--\nall right, so now we have-- similar to the\nDot-Product attention, we had a scaling\nfactor, simply because--",
    "start": "1236980",
    "end": "1243519"
  },
  {
    "text": "if the Dot-Product\nactually becomes too big, and you can solve it under\ncertain assumptions of mean and variance in the\nrepresentations,",
    "start": "1243520",
    "end": "1250647"
  },
  {
    "text": "you can [INAUDIBLE]. Oh, it hasn't updated, actually. Yeah.",
    "start": "1250647",
    "end": "1256210"
  },
  {
    "text": "So our formulation\nis-- basically, you have your queries,\nwhich, what you end up",
    "start": "1256210",
    "end": "1263650"
  },
  {
    "text": "doing is-- if you\nhave a position, you first project\nit into queries. And then the same token--",
    "start": "1263650",
    "end": "1270280"
  },
  {
    "text": "the representation\nof the same token gets projected into\nalso keys and values. And the first-- the\nquery determines",
    "start": "1270280",
    "end": "1277330"
  },
  {
    "text": "how much you're actually going\nto pull from all these keys. So you first do a Dot-Product\nof the query with every key.",
    "start": "1277330",
    "end": "1284740"
  },
  {
    "text": "And then, based on\nthat, you combine, or you pool the content\nof all these positions",
    "start": "1284740",
    "end": "1290380"
  },
  {
    "text": "based on what the score\nwas after normalizing it using a softmax. So in some sense, you can\nthink of self-attention also as",
    "start": "1290380",
    "end": "1299830"
  },
  {
    "text": "a content-based\npooling mechanism. And the scaling factor\nbasically avoids you--",
    "start": "1299830",
    "end": "1305980"
  },
  {
    "text": "it saved us from these logits\nactually blowing up in training, becoming unstable. And on the decoder side,\nyou could trivially--",
    "start": "1305980",
    "end": "1313780"
  },
  {
    "text": "you can trivially\nimplement causality by just adding an attention mask.",
    "start": "1313780",
    "end": "1321019"
  },
  {
    "text": "And what this-- where this\nbrings us is that-- all right. So now we've solved.",
    "start": "1321020",
    "end": "1327620"
  },
  {
    "text": "So a caveat on the FLOPs. We'll actually cover this later. But now what we have is a\nmechanism that's parallelizable.",
    "start": "1327620",
    "end": "1336410"
  },
  {
    "text": "It gives you direct content. It gives you direct\ntoken interactions, which we'll-- and that we\nbelieve is going to help you",
    "start": "1336410",
    "end": "1342227"
  },
  {
    "text": "actually learn and model these\nrelationships between the words better. And the complexity\nof self-attention",
    "start": "1342227",
    "end": "1347750"
  },
  {
    "text": "is faster than convolutions\nbecause convolutions are quadratic in the number. They're quadratic in\nthe number of channels",
    "start": "1347750",
    "end": "1354677"
  },
  {
    "text": "and the number of-- in\nthe hidden dimension. But self-attention is\nquadratic in the length. So if your length is not much\nmore than your hidden dimension,",
    "start": "1354677",
    "end": "1360140"
  },
  {
    "text": "you've actually saved on FLOPs. Now, this is not quite\na complete picture because not all FLOPs are equal.",
    "start": "1360140",
    "end": "1366680"
  },
  {
    "text": "And we'll talk\nabout this later on.  And now, when you put\neverything together, what we--",
    "start": "1366680",
    "end": "1374120"
  },
  {
    "text": "basically, we took the basis. This has a very\nstrong similarity",
    "start": "1374120",
    "end": "1380450"
  },
  {
    "text": "to the ResNet\narchitecture, actually. So if you look at ResNets-- so in ResNets, you\nhave contraction.",
    "start": "1380450",
    "end": "1386240"
  },
  {
    "text": "You have spatial mixing\nwith convolutions. And then you have\nthe expansion again. The transformer--\nif you just adjust--",
    "start": "1386240",
    "end": "1393200"
  },
  {
    "text": "if you just move\nit one step down, it's very-- it's analogous. You have attention. Then you have\nexpansion-contraction.",
    "start": "1393200",
    "end": "1398780"
  },
  {
    "text": "But there is a\ndifference in where the residual connections are. But it's a very similar\nbasic building block",
    "start": "1398780",
    "end": "1406940"
  },
  {
    "text": "with the residual connections. And you have these\ncontractions and expansions. And then the\ntransformer-- those were--",
    "start": "1406940",
    "end": "1413150"
  },
  {
    "text": "that was multi-head attention\nwith expansion-contraction, which was in the\nfeed forward layers.",
    "start": "1413150",
    "end": "1418670"
  },
  {
    "text": "And then one challenge\nwith the attention. LSTMs can count.",
    "start": "1418670",
    "end": "1423950"
  },
  {
    "text": "In fact, they can count. They can learn interesting\ntemporal patterns.",
    "start": "1423950",
    "end": "1429890"
  },
  {
    "text": "But attention is\npermutation invariant. So we had to actually\nadd position.",
    "start": "1429890",
    "end": "1435958"
  },
  {
    "text": "We had to add position\ninformation so that we could learn ordering. So we add position\ninformation at the input,",
    "start": "1435958",
    "end": "1441370"
  },
  {
    "text": "which gets transmitted\nto the other layers through the residual\nconnections.",
    "start": "1441370",
    "end": "1446950"
  },
  {
    "text": "And the original paper-- we had post-LayerNorm. But later on, we realized\nthat, as we actually",
    "start": "1446950",
    "end": "1453280"
  },
  {
    "text": "make the model deeper,\npost-LayerNorm doesn't allow you to train effectively. So we had to--",
    "start": "1453280",
    "end": "1459130"
  },
  {
    "text": "then we used a\npre-LayerNorm formulation, which was also observed in\nthe original ResNet papers.",
    "start": "1459130",
    "end": "1464260"
  },
  {
    "text": "And so the model is\nbasically-- all right. You've got your input and\nyou have spatial mixing.",
    "start": "1464260",
    "end": "1470680"
  },
  {
    "text": "Spatial mixing to attention,\nfeed forward layers. And this repeats. And the difference\non the decoder side",
    "start": "1470680",
    "end": "1477760"
  },
  {
    "text": "is that you also now have\nencoder-decoder attention",
    "start": "1477760",
    "end": "1483610"
  },
  {
    "text": "at every layer. If there's any questions-- yeah? Yes. What was your intuition\nbehind the [INAUDIBLE]??",
    "start": "1483610",
    "end": "1490210"
  },
  {
    "text": "Post-LayerNorm? Oh. So it ended up-- so if you\ndo post-LayerNorm, then--",
    "start": "1490210",
    "end": "1497799"
  },
  {
    "text": "actually, let's-- do\nI have that slide? Let me check. Probably I've deleted it. But if you do\npost-LayerNorm, then you",
    "start": "1497800",
    "end": "1505690"
  },
  {
    "text": "are basically squashing both the\nresidual and the additive parts. So when you-- so your\nactivations from the lower",
    "start": "1505690",
    "end": "1511937"
  },
  {
    "text": "layers keep getting-- keep\ngoing through the LayerNorms. But in pre-LayerNorm,\nyou're only-- only your residual path\nhas a LayerNorm, which",
    "start": "1511937",
    "end": "1518590"
  },
  {
    "text": "means your activations all the\nway from the bottom of the model are free. They're untouched, and\nthey can pass through.",
    "start": "1518590",
    "end": "1524419"
  },
  {
    "text": "Yeah. ",
    "start": "1524420",
    "end": "1530990"
  },
  {
    "text": "So until this point,\nwe haven't discussed why did we-- we\nhaven't discussed",
    "start": "1530990",
    "end": "1536480"
  },
  {
    "text": "multi-head attention, which\nended up being very important. So one of the problems\nwith attention is that--",
    "start": "1536480",
    "end": "1543320"
  },
  {
    "text": "imagine if you wanted to-- so oftentimes, language\nis about understanding",
    "start": "1543320",
    "end": "1550010"
  },
  {
    "text": "who did what to whom. So in this case, the cat\nlicked the owner's hands.",
    "start": "1550010",
    "end": "1555020"
  },
  {
    "text": "So who licked what? The cat licked the owner. So now, if you actually\nwant to combine information",
    "start": "1555020",
    "end": "1561169"
  },
  {
    "text": "from these two slots, these\npositions, these vectors, then the best you\ncould do with attention",
    "start": "1561170",
    "end": "1566360"
  },
  {
    "text": "is 0.5, 0.5 with the single\nlayer at half probability, half probability. But then they get\nmushed together.",
    "start": "1566360",
    "end": "1571430"
  },
  {
    "text": "But now imagine the strength\nthat a convolution has. It can actually have a--",
    "start": "1571430",
    "end": "1578400"
  },
  {
    "text": "That actually should have-- oh, OK. Well, I think the point\nwill still come across.",
    "start": "1578400",
    "end": "1584640"
  },
  {
    "text": "So now what a convolution\ncan do is-- because it has-- it basically applies--\nessentially, a convolution--",
    "start": "1584640",
    "end": "1590610"
  },
  {
    "text": "in this case, it's a 5 by 1. And all it really\ndoes is it just applies a different linear\ntransformation at each position.",
    "start": "1590610",
    "end": "1598470"
  },
  {
    "text": "So it can take any-- and because these linear\ntransformations are different,",
    "start": "1598470",
    "end": "1603690"
  },
  {
    "text": "it can-- the first linear\ntransformation can learn. I'm going to take a little\nbit of information from here. I'm going to take a little\nbit of information from here.",
    "start": "1603690",
    "end": "1609910"
  },
  {
    "text": "And I'm going to\nput them together. And the attention-- the best\nway that you could actually just do this is best\nby averaging out",
    "start": "1609910",
    "end": "1616160"
  },
  {
    "text": "[INAUDIBLE] these things. But having different\nlinear transformations allows you to take a part\nof the embedding here, a part of the embedding here,\nmix it up, and then maybe put it",
    "start": "1616160",
    "end": "1623612"
  },
  {
    "text": "together without actually then\ninterfering with each other. And multi-head attention,\nwhich is a bit like--",
    "start": "1623612",
    "end": "1628789"
  },
  {
    "text": "basically, a\nmulti-tape-- multi-head of a multi-head Turing machine\nwith different read-write heads.",
    "start": "1628790",
    "end": "1637110"
  },
  {
    "text": "Essentially, it allows-- you\nstart getting you that property back, where now what you\ndo is you essentially--",
    "start": "1637110",
    "end": "1644980"
  },
  {
    "text": "you now-- you bring\nback the ability to select different\nparts of the input. So you chop up the\nhidden dimension",
    "start": "1644980",
    "end": "1651039"
  },
  {
    "text": "into independent pieces. And then each one of them\nis now able to do attention. So now you can have probability\n1 in this place and probability",
    "start": "1651040",
    "end": "1658330"
  },
  {
    "text": "1 in this other sub space,\ninstead of having 0.5, 0.5. So now you don't have to\nget these averaging effects.",
    "start": "1658330",
    "end": "1664750"
  },
  {
    "text": "You can actually be selective. And also, for\ncomputational reasons, we--",
    "start": "1664750",
    "end": "1671170"
  },
  {
    "text": "instead of actually having\neight attention layers of-- or six attention heads\nof d dimensions, we had--",
    "start": "1671170",
    "end": "1677082"
  },
  {
    "text": "or eight attention\nheads of d dimensions, we had eight attention\nheads of d by 8 dimensions. ",
    "start": "1677082",
    "end": "1684549"
  },
  {
    "text": "So we wouldn't incur any more-- we wouldn't incur any\nmore any more FLOPs",
    "start": "1684550",
    "end": "1689762"
  },
  {
    "text": "for the same amount of FLOPs. But that's only half the story\nbecause the attention heads themselves turn out\nto be quite expensive,",
    "start": "1689762",
    "end": "1695530"
  },
  {
    "text": "which then later on\nhad to be-- there were improvements that\nneeded to be made.",
    "start": "1695530",
    "end": "1701169"
  },
  {
    "text": "And the most important part--\nprobably the most important",
    "start": "1701170",
    "end": "1706180"
  },
  {
    "text": "result was that,\nwith the transformer, we were able to outperform\nprevious ensemble",
    "start": "1706180",
    "end": "1713919"
  },
  {
    "text": "models as well. And that was very, very\nexciting that, hey, the single model actually\nis able to outperform",
    "start": "1713920",
    "end": "1719830"
  },
  {
    "text": "previous ensemble models. And not only that. And this is a machine\ntranslation WMT 2014",
    "start": "1719830",
    "end": "1728172"
  },
  {
    "text": "English-German and\nEnglish-French machine translation tasks. And not only were we able\nto do it in less FLOPs,",
    "start": "1728172",
    "end": "1735610"
  },
  {
    "text": "but also, these-- it was very clear that this\nwas a very general model.",
    "start": "1735610",
    "end": "1743668"
  },
  {
    "text": "We immediately\napplied it to parsing, and we were able to get, with a\nsmall model, excellent results.",
    "start": "1743668",
    "end": "1750940"
  },
  {
    "text": "So in some sense, we were-- this was very exciting because\nthis meant that, all right, now",
    "start": "1750940",
    "end": "1758635"
  },
  {
    "text": "this consolidation that we're\ntrying to go for in machine learning-- we probably\nhave a model that's more general than\nwhat we had before.",
    "start": "1758635",
    "end": "1764370"
  },
  {
    "text": "And we can now throw\nit at different-- maybe we can now throw\nit at different problems. And ultimately, why?",
    "start": "1764370",
    "end": "1770580"
  },
  {
    "text": "Because it would be helpful\nto have a single model",
    "start": "1770580",
    "end": "1776220"
  },
  {
    "text": "that's able to combine\nrepresentations from speech, images, and language. And if you had a general\nsubstrate that worked well",
    "start": "1776220",
    "end": "1783870"
  },
  {
    "text": "in all tasks, then\npotentially we could get to the single\nmultimodal model. ",
    "start": "1783870",
    "end": "1790560"
  },
  {
    "text": "Sometimes, interpretability\nis like tea leaves. It's like reading tea leaves.",
    "start": "1790560",
    "end": "1796900"
  },
  {
    "text": "So one should be careful. But it was nice that\nthe attention by itself can give you some\ninterpretability.",
    "start": "1796900",
    "end": "1802950"
  },
  {
    "text": "And we were able to see how\nsome of these attention heads or some of these\nattention mechanisms",
    "start": "1802950",
    "end": "1810127"
  },
  {
    "text": "were actually able to learn\nlong-distance relationships. Some actually learned to be-- early on in the transformer,\nwe saw this generally invariant",
    "start": "1810127",
    "end": "1819000"
  },
  {
    "text": "pattern, where some\nof the attention heads basically turned out to\njust look like convolutions. They were just they were just\npulling in local information.",
    "start": "1819000",
    "end": "1825702"
  },
  {
    "text": "There's, of course, now\nbeing much more advanced work with some of the\nmechanistic interpretability stuff with grokking\nand the stuff that",
    "start": "1825702",
    "end": "1833640"
  },
  {
    "text": "is happening at Anthropic,\nwhich is where they're learning. Now they're actually learning\nhow to interpret these induction",
    "start": "1833640",
    "end": "1840830"
  },
  {
    "text": "heads. So it's interesting. But we were able to see\nsome anecdotal evidence",
    "start": "1840830",
    "end": "1847090"
  },
  {
    "text": "of these heads actually\nperforming very, very distinct and clear actions.",
    "start": "1847090",
    "end": "1852700"
  },
  {
    "text": "So if there's any\nmore questions, then-- no more? I'll pause for a second.",
    "start": "1852700",
    "end": "1857740"
  },
  {
    "text": "Yeah? Do you buy the research from-- that it's the\ninduction heads that are causing the in-context\nlearning [INAUDIBLE]??",
    "start": "1857740",
    "end": "1865270"
  },
  {
    "text": "Yeah. It's hard to tell because-- so from what-- I haven't\nlooked at the most recent work. But they've solved this\nissue of superposition.",
    "start": "1865270",
    "end": "1871967"
  },
  {
    "text": "Is that right? So now, with having solved that,\nthey're able to roughly-- does that roughly mean\nthat now they're able-- they'll be able to\nassign distinguishing features",
    "start": "1871967",
    "end": "1879330"
  },
  {
    "text": "to each one of these sets\nand be able to explain it? From what I understand-- or the\nin-context learning part is--",
    "start": "1879330",
    "end": "1887060"
  },
  {
    "text": "is that-- is it that\nthey have to show it? Or is it that they're saying\nthat in-context learning happens",
    "start": "1887060",
    "end": "1892080"
  },
  {
    "text": "because of induction heads? Yeah. The latter. Oh, it's the latter. Yeah. It's not clear because--",
    "start": "1892080",
    "end": "1899970"
  },
  {
    "text": "yeah. I think there's probably\nmany, many kinds of-- in-context learning is shown to\nwork in so many different tasks",
    "start": "1899970",
    "end": "1907860"
  },
  {
    "text": "that-- and actually, I haven't\nfollowed this quite well. I don't know if-- I don't know specifically--",
    "start": "1907860",
    "end": "1912929"
  },
  {
    "text": "What do the induction\nheads typically do? What kinds of\nproperties do they have? Do you know what kinds of\nmechanisms do they have?",
    "start": "1912930",
    "end": "1919275"
  },
  {
    "text": "OK. So yeah. So since both of us\ndon't know this really, really well, I won't be\nable to go very far here.",
    "start": "1919275",
    "end": "1926400"
  },
  {
    "text": "But I'm not sure\nif they've gotten to the point where\nthey're able to explain most of in-context learning\nbecause of induction heads.",
    "start": "1926400",
    "end": "1933360"
  },
  {
    "text": "From what I understand,\nthey might have. Yeah. Does anybody know about\nthe induction heads?",
    "start": "1933360",
    "end": "1939650"
  },
  {
    "text": "OK. ",
    "start": "1939650",
    "end": "1945430"
  },
  {
    "text": "Over the years-- so\nthere have been a few-- there have been many papers.",
    "start": "1945430",
    "end": "1951520"
  },
  {
    "text": "But there have\nbeen a few changes that have been important. There have been a few\nchanges that have stuck.",
    "start": "1951520",
    "end": "1958779"
  },
  {
    "text": "And the new\ntransformers typically have these improvements. And you'll go from bottom\nto top with some of them",
    "start": "1958780",
    "end": "1968110"
  },
  {
    "text": "and see which ones\nhave actually stuck. So we started with the first. One of the biggest problems with\nself-attention was that it was--",
    "start": "1968110",
    "end": "1976240"
  },
  {
    "text": "that self-attention itself\nis permutation invariant. ",
    "start": "1976240",
    "end": "1982630"
  },
  {
    "text": "You need to do position\ninformation in order for it to learn some kind\nof temporal structure.",
    "start": "1982630",
    "end": "1988060"
  },
  {
    "text": "And in the original transformer,\nwe used these sinusoids. And we had hoped that it would\nactually learn relative position",
    "start": "1988060",
    "end": "1994390"
  },
  {
    "text": "encodings because you could\ndecompose the position encoding of another-- you could decompose the position\nembedding of another position",
    "start": "1994390",
    "end": "2002590"
  },
  {
    "text": "as some linear function of\nthe previous one, and some--",
    "start": "2002590",
    "end": "2007870"
  },
  {
    "text": "and another factor,\nwhich is-- which depends on the relative\ndistance between the two. But that didn't happen.",
    "start": "2007870",
    "end": "2013600"
  },
  {
    "text": "Learned position encodings in\nthe original paper did as well.",
    "start": "2013600",
    "end": "2018740"
  },
  {
    "text": "And so we were not\nquite able to get the-- we were not quite able to get\nthese model relative distances",
    "start": "2018740",
    "end": "2025780"
  },
  {
    "text": "using the sinusoids. So then a couple of\nimportant-- and this is a very biased sample.",
    "start": "2025780",
    "end": "2031600"
  },
  {
    "text": "But I think it generally covers\na large category of these-- it covers a large set of papers.",
    "start": "2031600",
    "end": "2038890"
  },
  {
    "text": "There's roughly\nthree categories. And all of them are now\nexplicitly learning--",
    "start": "2038890",
    "end": "2046480"
  },
  {
    "text": "explicitly for learning\nrelative embeddings. So in the relative\nposition transformer,",
    "start": "2046480",
    "end": "2053199"
  },
  {
    "text": "we had an embedding for every\npair of relative positions. And using that, we\nbasically then--",
    "start": "2053199",
    "end": "2058667"
  },
  {
    "text": "we did a Dot-Product\nof that embedding with the query that\nproduced a logit that was-- that modulated according\nto the relative distance.",
    "start": "2058667",
    "end": "2065260"
  },
  {
    "text": "And we found this\nto be extremely-- we found this to be extremely\nuseful for translation.",
    "start": "2065260",
    "end": "2071469"
  },
  {
    "text": "And I'll show it also in music. Another-- maybe a\nsimplification--",
    "start": "2071469",
    "end": "2078099"
  },
  {
    "text": "this is the Alibi paper,\nwhere this is non-parametric. These are not learned,\nwhere, instead of an embedding for\nevery pair of positions,",
    "start": "2078100",
    "end": "2084760"
  },
  {
    "text": "you actually have a single bias. So you basically just add\na single bias to the logit.",
    "start": "2084760",
    "end": "2089830"
  },
  {
    "text": "And you can either\nlearn it, or you can use a heuristic, which Alibi did.",
    "start": "2089830",
    "end": "2096440"
  },
  {
    "text": "And one other advantage about\nrelative position encodings is that they could\npotentially allow you to extrapolate to new-- to\nlonger sequence lengths, which",
    "start": "2096440",
    "end": "2103960"
  },
  {
    "text": "you couldn't do with\nabsolute position encodings. I'm curious about the room--",
    "start": "2103960",
    "end": "2110140"
  },
  {
    "text": "about what the room thinks here. But I believe that the latest\nincarnation of relative position",
    "start": "2110140",
    "end": "2116380"
  },
  {
    "text": "encodings, where this is-- I believe it's called the row\nformer, where they basically",
    "start": "2116380",
    "end": "2121630"
  },
  {
    "text": "just rotate the embedding\nwith every pair of dimensions",
    "start": "2121630",
    "end": "2126640"
  },
  {
    "text": "a little bit. And the angle of\nrotation depends on your actual\nabsolute distance.",
    "start": "2126640",
    "end": "2132430"
  },
  {
    "text": "But what ends up happening\nis, when you do the attention operation, you end up\ngetting relative-- you end up",
    "start": "2132430",
    "end": "2138250"
  },
  {
    "text": "basically getting\nan effect where you're modulating the logit\nbased on relative distance. So now what's remarkable\nabout this approach, what's--",
    "start": "2138250",
    "end": "2146500"
  },
  {
    "text": "it combines the\nbest of both worlds. It's absolute\nposition encodings. Relative position\nencodings had a couple",
    "start": "2146500",
    "end": "2152860"
  },
  {
    "text": "of challenges in that you had\nto maintain an extra logit for-- or an embedding for every pair.",
    "start": "2152860",
    "end": "2158485"
  },
  {
    "text": "So there was a lot of them. So it ended up\nincreasing your memory. Here-- these are actually\nabsolute position encodings.",
    "start": "2158485",
    "end": "2164590"
  },
  {
    "text": "But they give\nyou-- they ended up giving you the relative\nmodulation in the attention operation that you needed.",
    "start": "2164590",
    "end": "2170620"
  },
  {
    "text": "And I believe the consensus is\nthat this is the most successful position encoding.",
    "start": "2170620",
    "end": "2176343"
  },
  {
    "text": "Is that correct? Or are there-- is there-- are there others that people--\nis that the consensus?",
    "start": "2176343",
    "end": "2183535"
  },
  {
    "text": "OK. So it looks like-- so I\nwould say that these relative",
    "start": "2183535",
    "end": "2189359"
  },
  {
    "text": "rotations are from-- the approach that's in\nthe reformer is likely--",
    "start": "2189360",
    "end": "2194760"
  },
  {
    "text": "is basically an actual\nnew, genuine improvement that is now going to stay\nwith the transformer.",
    "start": "2194760",
    "end": "2199860"
  },
  {
    "text": "And it has all the\ngreat properties of what you would want. It's an absolute\nposition encoding",
    "start": "2199860",
    "end": "2205740"
  },
  {
    "text": "that gives you\nrelative effects, which is what we originally wanted. And one-- and to emphasize\nthat we needed relative--",
    "start": "2205740",
    "end": "2214950"
  },
  {
    "text": " emphasize two things, one, that\nmodeling interesting temporal",
    "start": "2214950",
    "end": "2223380"
  },
  {
    "text": "relationships, which is-- which\nare really important in music, requires a good\nposition representation.",
    "start": "2223380",
    "end": "2229310"
  },
  {
    "text": "We actually found\nsignificant improvements in the music transformer. Is it possible to play this?",
    "start": "2229310",
    "end": "2235569"
  },
  {
    "text": "OK. So here is a-- all right. Here's a priming sequence.",
    "start": "2235570",
    "end": "2240740"
  },
  {
    "text": "This is work by-- work by Anna Huang, by the way.",
    "start": "2240740",
    "end": "2246470"
  },
  {
    "text": "[MUSIC PLAYING] ",
    "start": "2246470",
    "end": "2252270"
  },
  {
    "text": "This is in-context\nlearning in music because you actually\nsee this prompt. And you ask the\nmodel to complete it.",
    "start": "2252270",
    "end": "2260575"
  },
  {
    "text": "So now this is the\nvanilla transformer. And you can already-- [MUSIC PLAYING]",
    "start": "2260575",
    "end": "2268005"
  },
  {
    "text": " So you can see that\nthese were using--",
    "start": "2268005",
    "end": "2273370"
  },
  {
    "text": "we tried both learned\nand sinusoids. And you can see that it\nstarts all peppy and happy, but then just languishes\ninto something",
    "start": "2273370",
    "end": "2279430"
  },
  {
    "text": "really sad and confused. So it's not able\nto capture these because music has these\ninteresting motifs, where--",
    "start": "2279430",
    "end": "2286400"
  },
  {
    "text": "well, there's motifs\nat different levels because there's some\nrepetition locally.",
    "start": "2286400",
    "end": "2291970"
  },
  {
    "text": "But there's a repetition across\nthe entire piece as well. So now here this is with\nthe relative transformer.",
    "start": "2291970",
    "end": "2300390"
  },
  {
    "text": "And this is with the\nfirst approach, where we had relative embeddings. And we had to develop a\ncompute-efficient approach",
    "start": "2300390",
    "end": "2310560"
  },
  {
    "text": "to actually-- by using some matrix\ncalisthenics to actually put the logits in the right place.",
    "start": "2310560",
    "end": "2316799"
  },
  {
    "text": "So you can read the paper. It's fun. So here is same prime sequence. And let's see the completion.",
    "start": "2316800",
    "end": "2324170"
  },
  {
    "text": "Here. [MUSIC PLAYING]",
    "start": "2324170",
    "end": "2329710"
  },
  {
    "start": "2329710",
    "end": "2343848"
  },
  {
    "text": "So Anna, who's the first\nauthor of this paper, and also a musician, tells me that\nthis actually captures a lot of structure in music.",
    "start": "2343848",
    "end": "2350099"
  },
  {
    "text": "It sounds nicer than\nthe previous one. But maybe it depends on\nwhat people's tastes are. Maybe some avant\ngarde jazz fan would",
    "start": "2350100",
    "end": "2357450"
  },
  {
    "text": "like the second-- would\nlike the first piece. But the point here is\nthat the difference",
    "start": "2357450",
    "end": "2363240"
  },
  {
    "text": "is pretty clear between\nnot working and working. And I think people-- it'd\nbe fun to try this out with the new rotary\nposition encodings.",
    "start": "2363240",
    "end": "2371820"
  },
  {
    "text": "All right. So walking up-- now that we have\na-- we have a good mechanism--",
    "start": "2371820",
    "end": "2378510"
  },
  {
    "text": "a better mechanism\nthan we originally had for modeling\nrelative distances. And there's advancements on top\nof the rotary position encodings",
    "start": "2378510",
    "end": "2387869"
  },
  {
    "text": "where, by adjusting the\nbase frequencies, you can-- when you encounter\nlonger sequences,",
    "start": "2387870",
    "end": "2392912"
  },
  {
    "text": "you can just adjust\nthe base frequencies. And then the model\nis not going to-- the model is not\ngoing to degrade.",
    "start": "2392912",
    "end": "2398700"
  },
  {
    "text": "So that has good properties. Probably the--\nthere's been-- there have been several\nimportant contributions",
    "start": "2398700",
    "end": "2406990"
  },
  {
    "text": "to the attention\npiece itself, which is the primary workhorse here.",
    "start": "2406990",
    "end": "2412359"
  },
  {
    "text": "It's the one that-- you can\nthink of it as-- it's either-- there's induction heads that\nare learning how to copy.",
    "start": "2412360",
    "end": "2418480"
  },
  {
    "text": "Or maybe all it's really doing\nis just routing information so that the giant feed\nforward layers can actually",
    "start": "2418480",
    "end": "2423700"
  },
  {
    "text": "learn the important features. But there's broadly two\nclasses of problems. There are two classes of issues\nwith the attention mechanism.",
    "start": "2423700",
    "end": "2431018"
  },
  {
    "text": "One that was brought up\ntoday that's very evident is long context itself. So the complexity,\nas we remember,",
    "start": "2431018",
    "end": "2438790"
  },
  {
    "text": "was quadratic in the\nlength of the sequence. And once your sequences\nget very, very long-- once your sequences get very,\nvery long, then it not only--",
    "start": "2438790",
    "end": "2449400"
  },
  {
    "text": "There's one problem. It's going to become very-- it's\ngoing to become computationally expensive. But it's also the logits that\nare going to become infeasible.",
    "start": "2449400",
    "end": "2456550"
  },
  {
    "text": "So there's generally a\nfew groups of papers. One is restricting\nattention windows.",
    "start": "2456550",
    "end": "2462280"
  },
  {
    "text": "And we did this for images where\nwe had local 1D and 2D attention",
    "start": "2462280",
    "end": "2468760"
  },
  {
    "text": "for images. In the first one, we actually\njust rasterized the image. And we had local 1D attention,\nwhich is very similar",
    "start": "2468760",
    "end": "2474970"
  },
  {
    "text": "to the sliding window\nattention in the recent-- in the [INAUDIBLE] paper. And then in the--",
    "start": "2474970",
    "end": "2480850"
  },
  {
    "text": " in the 2D case, we have\na spatial 2D attention.",
    "start": "2480850",
    "end": "2487427"
  },
  {
    "text": "Then there were these\nsparse versions, where you actually-- you\nhad these specific patterns",
    "start": "2487427",
    "end": "2492460"
  },
  {
    "text": "over many layers. You could think about it as-- if you have these sparse\nmatrices, how many of them",
    "start": "2492460",
    "end": "2499025"
  },
  {
    "text": "do you have to multiply\nwith each other until you get a\nreally dense matrix? So roughly this turns\nout to be-- so here,",
    "start": "2499025",
    "end": "2505270"
  },
  {
    "text": "you can get connectivity. Is that for me?",
    "start": "2505270",
    "end": "2510690"
  },
  {
    "text": "OK.  You can get connectivity\nbetween distant pixels,",
    "start": "2510690",
    "end": "2518150"
  },
  {
    "text": "or distant notes in a musical\ntune, or words pretty quickly.",
    "start": "2518150",
    "end": "2523940"
  },
  {
    "text": "And then there's the\nsecond one, which is-- which there hasn't\nbeen enough work. And there's some\nchallenges there.",
    "start": "2523940",
    "end": "2530090"
  },
  {
    "text": "But it's these unstructured\nsparse attention approaches. And they're typically-- they're\nessentially-- what they're--",
    "start": "2530090",
    "end": "2536450"
  },
  {
    "text": "at a higher level, what they're\nreally trying to do is-- Imagine that I walked up to\nyou, and I told you that-- hey,",
    "start": "2536450",
    "end": "2543200"
  },
  {
    "text": "these are the bunches\nof tokens that just",
    "start": "2543200",
    "end": "2548839"
  },
  {
    "text": "have very high intersimilarity. They're likely to\nattend to each other. How quickly can I approximate\nit without actually having",
    "start": "2548840",
    "end": "2555253"
  },
  {
    "text": "to do the whole computation? Two approaches. In routing attention, you\nuse vector quantization.",
    "start": "2555253",
    "end": "2560930"
  },
  {
    "text": "And in the LSH or the-- I forget what it's called. I think I forgot the\nname of the paper.",
    "start": "2560930",
    "end": "2567099"
  },
  {
    "text": "But they used-- but in\nthis paper, they used LSH. And in the routing transformer,\nmost layers were actually local.",
    "start": "2567100",
    "end": "2577840"
  },
  {
    "text": "The final layers,\nwhich typically are the ones that like\nto end up-- that end up doing modeling--\nthat end up modeling",
    "start": "2577840",
    "end": "2583540"
  },
  {
    "text": "these long-distance\nrelationships, were the ones that actually used\nthis content-based, unstructured sparse attention.",
    "start": "2583540",
    "end": "2589840"
  },
  {
    "text": "And the results were-- the\nresults were generally better. And it's also interesting\nthat maybe you",
    "start": "2589840",
    "end": "2595210"
  },
  {
    "text": "can build-- we can build models\nwith-- on very long sequences, where most layers\nare fairly local.",
    "start": "2595210",
    "end": "2600220"
  },
  {
    "text": "And you have only a few\nlayers that are actually doing these long-distance attentions. Now, one of the bigger\nchallenges there-- actually,",
    "start": "2600220",
    "end": "2606009"
  },
  {
    "text": "even though it ended up being-- even though you end up-- you end up nullifying\na lot of the FLOPs",
    "start": "2606010",
    "end": "2612100"
  },
  {
    "text": "that you would do if\nyou did full attention. The problem always ends\nup being memory movement. Always ends up being\nmemory movement.",
    "start": "2612100",
    "end": "2618550"
  },
  {
    "text": "And there's still-- there's\nstill more innovation to be done here. Also, with memory\nbandwidth improving,",
    "start": "2618550",
    "end": "2623650"
  },
  {
    "text": "maybe some of these approaches\nbecome more feasible today than they were\nlate-- than we did-- when we wrote these papers.",
    "start": "2623650",
    "end": "2629779"
  },
  {
    "text": "But this is an\ninteresting approach, where you're essentially\ntrying to approximate the original attention matrix. Sorry.",
    "start": "2629780",
    "end": "2634960"
  },
  {
    "text": "This is a silly thing. But clarification-- how\nis this [INAUDIBLE] scheme very different from just\nconvolutions that are sparse?",
    "start": "2634960",
    "end": "2642230"
  },
  {
    "text": "In the sense that\nyou're losing a lot of the long-distance\nor unrelated contexts from any arbitrary\npairs of elements.",
    "start": "2642230",
    "end": "2648980"
  },
  {
    "text": "So how do you-- So I would say that this is\nsimilar to the convolution there.",
    "start": "2648980",
    "end": "2654050"
  },
  {
    "text": "If you did this perfectly,\nthen what you didn't attend to",
    "start": "2654050",
    "end": "2659470"
  },
  {
    "text": "would very likely have a\nvery low attention weight. ",
    "start": "2659470",
    "end": "2664640"
  },
  {
    "text": "You're essentially trying to\nguess, as best as you can, what would have\nattended to each other. And so it uses its content\nbased on structure sparsity.",
    "start": "2664640",
    "end": "2674660"
  },
  {
    "text": "And there's probably\nmore interesting work to be done there. Maybe, instead of actually\njust doing it a token",
    "start": "2674660",
    "end": "2680212"
  },
  {
    "text": "at a time, where you end up\ndoing a lot of memory movement, you end up deciding which chunks\nwant to self-attend to which",
    "start": "2680212",
    "end": "2685520"
  },
  {
    "text": "chunks. So then you just move\nentire chunks at a time. So there's-- I think there's\nsome interesting directions",
    "start": "2685520",
    "end": "2690553"
  },
  {
    "text": "here. And of course-- And frankly, the ones\nthat ended up sticking",
    "start": "2690553",
    "end": "2697340"
  },
  {
    "text": "are the simplest\nones that are also-- that are-- because\nstructured sparsity is easy.",
    "start": "2697340",
    "end": "2702910"
  },
  {
    "text": "You're able to optimize it\neasily on modern accelerators. So again, physics-- you should\nmake physics your friend.",
    "start": "2702910",
    "end": "2710960"
  },
  {
    "text": "Yeah. And so, typically,\nlocal attention, or sliding into attention--",
    "start": "2710960",
    "end": "2716210"
  },
  {
    "text": "we're still seeing it\noften appear and do well. These other very--\nreally wild and--",
    "start": "2716210",
    "end": "2722480"
  },
  {
    "text": "but very expressive,\nunstructured, sparse attention approaches typically\nhaven't quite succeeded. There's, of course,\nlinear attention variants",
    "start": "2722480",
    "end": "2729320"
  },
  {
    "text": "that I don't think today are\nin any of the state of the art architectures. There were other approaches\nthat-- hey, instead of actually",
    "start": "2729320",
    "end": "2736068"
  },
  {
    "text": "doing n squared,\nyou do n squared d, where you do\nn-- you have some-- you learn new k embeddings,\nwhere you do NKD,",
    "start": "2736068",
    "end": "2744980"
  },
  {
    "text": "and then you do NDK. So you basically factor\nit just like an analog of matrix factorization.",
    "start": "2744980",
    "end": "2751000"
  },
  {
    "text": "Something that's one\nother approach that's interesting, that I would like\nmyself to actually investigate,",
    "start": "2751000",
    "end": "2756580"
  },
  {
    "text": "is-- we are seeing, in general,\nusing retrieval as a tool. So why don't you just pretend\nthat your memories themselves",
    "start": "2756580",
    "end": "2763780"
  },
  {
    "text": "were documents and use\nretrieval as a tool there? So the memorizing\ntransformer basically--",
    "start": "2763780",
    "end": "2769359"
  },
  {
    "text": "it essentially does\na mix of local. And it then retrieves from\nvery, very long memories.",
    "start": "2769360",
    "end": "2774930"
  },
  {
    "text": "And they find that you\ndon't need to train the model from scratch. All you need to do is\nadapt with this approach",
    "start": "2774930",
    "end": "2780760"
  },
  {
    "text": "on some small amount\nof data, and you're able to learn a good\nretrieval mechanism. I think it's quite interesting.",
    "start": "2780760",
    "end": "2787270"
  },
  {
    "text": "It still comes in this\ncontent-based decision",
    "start": "2787270",
    "end": "2792310"
  },
  {
    "text": "of what I should attend to. But I like the fact that\nit just makes retrieval a tool that you can use\neither on your own memories.",
    "start": "2792310",
    "end": "2798820"
  },
  {
    "text": "Or you could use\nit on documents. It's a nice generalizing. It's a nice general view\nof looking at things.",
    "start": "2798820",
    "end": "2805369"
  },
  {
    "text": "So now the second piece,\nwhich you basically-- you run into not all--",
    "start": "2805370",
    "end": "2811123"
  },
  {
    "text": "you run into the issue that\nnot all FLOPs are equal. So if you look at the-- if you\nlook at the memory hierarchy,",
    "start": "2811123",
    "end": "2818540"
  },
  {
    "text": "a lot of your activations in-- that are stored in the GPU\nHBM, which today, in the H100s,",
    "start": "2818540",
    "end": "2824390"
  },
  {
    "text": "is about 80 gigabytes-- but\nthe H100 is 80 gigabytes.",
    "start": "2824390",
    "end": "2831710"
  },
  {
    "text": "And the A100s used\nto be 40 gigabytes. So there's limited amount\nof high-bandwidth memory. And so you have to first go\nfrom high-bandwidth memory",
    "start": "2831710",
    "end": "2839420"
  },
  {
    "text": "to the SRAM. And then you have to go to the\ncompute elements, and then back. So every single\ntime-- and this is--",
    "start": "2839420",
    "end": "2845690"
  },
  {
    "text": "it probably-- and whenever--",
    "start": "2845690",
    "end": "2851270"
  },
  {
    "text": "If interested, you can\nlook at roofline analysis. The roofline analysis actually\ngives you a nice picture",
    "start": "2851270",
    "end": "2859160"
  },
  {
    "text": "to characterize for any device\nwhere you would need-- where",
    "start": "2859160",
    "end": "2864670"
  },
  {
    "text": "your workload or\noperation needs to be so that you can actually\neffectively utilize",
    "start": "2864670",
    "end": "2869980"
  },
  {
    "text": "the compute as much. You want to be compute-bound\nbecause, ultimately, if you don't calculate\nrepresentations-- if you don't calculate, you're\nnot going to get any output.",
    "start": "2869980",
    "end": "2877090"
  },
  {
    "text": "But if you spend a lot of\ntime moving things around and spend less relative\ntime calculating,",
    "start": "2877090",
    "end": "2882430"
  },
  {
    "text": "then you're actually--\nyou're wasting effort. So one of the-- so [INAUDIBLE]\nstandard attention mechanism.",
    "start": "2882430",
    "end": "2889720"
  },
  {
    "text": "One of the issues is that-- so imagine you have\nyour queries, keys, and values all in your memory.",
    "start": "2889720",
    "end": "2895000"
  },
  {
    "text": "So then you need to then-- your standard approach would\nbe, you move it to-- you move it from HBM.",
    "start": "2895000",
    "end": "2900610"
  },
  {
    "text": "You do the calculations. You compute the attention. You compute the logits. You move logits back into HBM.",
    "start": "2900610",
    "end": "2906549"
  },
  {
    "text": "And then you compute softmax-- the softmax back into HBM. And then you basically load the\nprobabilities and the values",
    "start": "2906550",
    "end": "2915550"
  },
  {
    "text": "then to then finally\ncompute the outputs. So the arithmetic\nintensity, or--",
    "start": "2915550",
    "end": "2921520"
  },
  {
    "text": "the arithmetic intensity\nor operational intensity, which is the amount\nof FLOPs that you do per byte on attention--\neven though it's",
    "start": "2921520",
    "end": "2928760"
  },
  {
    "text": "less FLOPs than, say,\na 1 by 1 convolution, it had more-- it is lower\nbecause it typically",
    "start": "2928760",
    "end": "2933800"
  },
  {
    "text": "has more memory movement,\nwhereas 1 by 1 convolutions have less memory movement. You just move the weights. You move the activations.",
    "start": "2933800",
    "end": "2939440"
  },
  {
    "text": "You do the calculations. Then you bring them back. And same goes for\nconvolutions, too. And convolutions have a very\nhigh arithmetic intensity.",
    "start": "2939440",
    "end": "2944900"
  },
  {
    "text": "So it's not that you just want\nthe highest arithmetic intensity or operational\nintensity operations because you still want to\nhave useful parameters.",
    "start": "2944900",
    "end": "2951559"
  },
  {
    "text": "So it's a trade-off. So a lot of-- so there's a bunch\nof improvements that will stick.",
    "start": "2951560",
    "end": "2958227"
  },
  {
    "text": "It's almost-- they're\nalmost certain-- likely to stay that try\nto combat this issue, both at training time, because\nyour logits can get really big,",
    "start": "2958227",
    "end": "2965600"
  },
  {
    "text": "but also at inference\ntime, when your KD-- when you're doing attention and\nwhen you're doing inference, then you have a single query,\nbut your KD cache, right?",
    "start": "2965600",
    "end": "2973010"
  },
  {
    "text": "You have to maintain\nyour keys and values. That can grow quite a bit, so\nyou have to move that around. And so the first step is simple.",
    "start": "2973010",
    "end": "2980540"
  },
  {
    "text": "Let's just decrease\nthe activation memory. So the multi-query approach,\nwhere it's basically",
    "start": "2980540",
    "end": "2986850"
  },
  {
    "text": "a multiple-- so you reduce-- you have multiple queries,\nbut you reduce the number",
    "start": "2986850",
    "end": "2992270"
  },
  {
    "text": "of read heads to just one. So you have just one\nkey and one value. That does reduce\nyour expressivity.",
    "start": "2992270",
    "end": "2997730"
  },
  {
    "text": "So grouped-query-- which is now\na simple balance that basically says, hey, let's\nnot take the extreme",
    "start": "2997730",
    "end": "3003250"
  },
  {
    "text": "of having all this\ntemporary activation memory. Let's actually group\nit so different queries-- so a bunch of queries\nwill attend to the same keys",
    "start": "3003250",
    "end": "3011320"
  },
  {
    "text": "and values. And then what ends\nup happening is-- another point to note\nhere is that all of this",
    "start": "3011320",
    "end": "3018220"
  },
  {
    "text": "is relative because most of\nthe work in these very, very-- Oh, by the way, third\napproach, actually, that I should say\nof not worrying",
    "start": "3018220",
    "end": "3026829"
  },
  {
    "text": "about your attention is just\nto make your model really big. Because then you just care about\nyour feed forward computations.",
    "start": "3026830",
    "end": "3032170"
  },
  {
    "text": "And your attention computations\nare just a small slice of that, so you don't worry about it. So typically, in\nthese larger models--",
    "start": "3032170",
    "end": "3038110"
  },
  {
    "text": "even though grouped query\nattention has more activation memory than multi-query\nwith these large models,",
    "start": "3038110",
    "end": "3044920"
  },
  {
    "text": "it's still a much larger-- it's not a much larger-- it's still a smaller proportion\nof what you're doing in the feed",
    "start": "3044920",
    "end": "3050140"
  },
  {
    "text": "forward. So you're certifying. So, I guess, three things. Ignore. Make it really big. Make it really big.",
    "start": "3050140",
    "end": "3056110"
  },
  {
    "text": "Second is, I guess, you-- by even the-- with\nprolonged context,",
    "start": "3056110",
    "end": "3062020"
  },
  {
    "text": "you can do some of the-- some of these approaches\nthat we talked about. But then you also have\nthese system optimizations,",
    "start": "3062020",
    "end": "3068020"
  },
  {
    "text": "which are pretty cool. So the softmax has an\ninteresting property that you",
    "start": "3068020",
    "end": "3074170"
  },
  {
    "text": "can compute in online fashion. You can compute\nit incrementally. If you've got a\nbunch of logits-- so",
    "start": "3074170",
    "end": "3079330"
  },
  {
    "text": "you're streaming them. If you've got a partial softmax\nand a new logit comes in, you can update it.",
    "start": "3079330",
    "end": "3084339"
  },
  {
    "text": "You can update it in\nan online fashion. So what does that mean? That means that now you\nnever need it to write",
    "start": "3084340",
    "end": "3091990"
  },
  {
    "text": "logits or the P's into the HBM. So you save a lot. If there's an extremely\nlong sequence,",
    "start": "3091990",
    "end": "3097120"
  },
  {
    "text": "you end up writing a lot. So you save on that. And both these approaches\nend up-- in one case,",
    "start": "3097120",
    "end": "3103270"
  },
  {
    "text": "the first paper was on TPUs\nthat introduced this property, or took advantage of this\nproperty, [INAUDIBLE]",
    "start": "3103270",
    "end": "3110883"
  },
  {
    "text": "the property to\nbe able to compute the softmax in an\nonline fashion, and the second paper, which\nis now flash attention today.",
    "start": "3110883",
    "end": "3118119"
  },
  {
    "text": "They've had many advancements. They actually had some systems\nlevel optimization, where they--",
    "start": "3118120",
    "end": "3123369"
  },
  {
    "text": "now you can actually have very,\nvery long sequences on GPUs-- the optimizations of GPUs\nby basically not moving",
    "start": "3123370",
    "end": "3131950"
  },
  {
    "text": "the logits back into\nHBM using this online-- using this property, and\nalso writing the right forms that use [INAUDIBLE] and\neverything, use the GPU.",
    "start": "3131950",
    "end": "3139960"
  },
  {
    "text": "Any questions? OK. What's the time? ",
    "start": "3139960",
    "end": "3147350"
  },
  {
    "text": "So we are basically-- 20 minutes. So I'll finish in time. So I just covered these two.",
    "start": "3147350",
    "end": "3153010"
  },
  {
    "text": "There's many, many. I guess there's other\nimportant improvements.",
    "start": "3153010",
    "end": "3160180"
  },
  {
    "text": "So we talked about the pre and\npost versus post-LayerNorm. There's been some changes of the\nfeed forward layers themselves.",
    "start": "3160180",
    "end": "3167680"
  },
  {
    "text": "You can stare at the\nfeed forward layers. If you stare at\nanything long enough, everything becomes attention.",
    "start": "3167680",
    "end": "3172780"
  },
  {
    "text": "But it's true in the\nfeed forward case that, if you look at it, you\ncan think about them as-- it looks like attention.",
    "start": "3172780",
    "end": "3178480"
  },
  {
    "text": "And there was a paper that\nturned that into a bit--",
    "start": "3178480",
    "end": "3183732"
  },
  {
    "text": "turned that-- turned\nthose into memories. It was originally by Facebook. I actually forget what it was. But it didn't-- And the feed forward\nlayers just stayed.",
    "start": "3183732",
    "end": "3191860"
  },
  {
    "text": "We typically haven't seen a\nlot of improvements on them. There have been some--",
    "start": "3191860",
    "end": "3197910"
  },
  {
    "text": "there have been some efforts\non higher order attention right now. Attention, if you\nthink about it, is a third-order interaction.",
    "start": "3197910",
    "end": "3203290"
  },
  {
    "text": "You have queries,\nkeys, and values, but-- right now. But you could imagine actually\nhaving four-order interactions,",
    "start": "3203290",
    "end": "3209690"
  },
  {
    "text": "where you're actually computing\nlogits of pairs of things against all pairs of things. So these are now higher order\ninteractions, where now you",
    "start": "3209690",
    "end": "3216140"
  },
  {
    "text": "can have complicated geometries\nthat you actually include in your attention computation.",
    "start": "3216140",
    "end": "3222839"
  },
  {
    "text": "And maybe it's important\nfor, say, biology, or-- biology, but it's not\nbeen explored much. What has actually worked\nand is likely to now stay",
    "start": "3222840",
    "end": "3230480"
  },
  {
    "text": "is some approaches\non faster decoding. Not quite the original less or\nnon-autoregressive aspirations",
    "start": "3230480",
    "end": "3237140"
  },
  {
    "text": "that we had, but these\nmore speculative decoding, where the heuristic\nthere is pretty simple. Yeah.",
    "start": "3237140",
    "end": "3243380"
  },
  {
    "text": "If you want, instead of\ngenerating from a heavy model, generate from a\nreally light model that captures the diversity.",
    "start": "3243380",
    "end": "3249552"
  },
  {
    "text": "And score with a heavy model. So then you re-rank the list. And that ends up\nworking quite well. And most state of the art--\nmost production deployments",
    "start": "3249552",
    "end": "3257839"
  },
  {
    "text": "likely use speculative decoding. So now switching gears. I guess we started this--",
    "start": "3257840",
    "end": "3268100"
  },
  {
    "text": "we started by quoting the\nDartmouth Conference, where they wanted to build\na single machine.",
    "start": "3268100",
    "end": "3273710"
  },
  {
    "text": "And the question now is with\nlarge language models that are now eating up\nmost of the internet,",
    "start": "3273710",
    "end": "3279227"
  },
  {
    "text": "are we getting-- are\nwe quite getting there? And we are seeing\nsome remarkable-- we're finally seeing\nself-supervised learning work",
    "start": "3279227",
    "end": "3285530"
  },
  {
    "text": "at a scale that-- work at an unprecedented\nscale, where, now,",
    "start": "3285530",
    "end": "3291080"
  },
  {
    "text": "by digesting carefully curated\nand colossal amounts of text with very, very large\nmodels, you're able to then--",
    "start": "3291080",
    "end": "3298460"
  },
  {
    "text": "they're able to\nperform, presumably, or still waiting to be\nconfirmed, tasks that are--",
    "start": "3298460",
    "end": "3306920"
  },
  {
    "text": "or they're able to actually\nperform at least a large-- a broad variety of tasks by just\nspecifying them in the prompt.",
    "start": "3306920",
    "end": "3313220"
  },
  {
    "text": "And it's now-- it's almost like\nnow, you have a new computer. And for people here who are\nreally excited about the future",
    "start": "3313220",
    "end": "3320060"
  },
  {
    "text": "of agents, now they can\nprogram thousands of agents with the same computer. Or maybe you-- now\nthey have agents",
    "start": "3320060",
    "end": "3327800"
  },
  {
    "text": "that they can-- several\nagents that they can program with the same computer\nthen that then coordinate to solve problems.",
    "start": "3327800",
    "end": "3333420"
  },
  {
    "text": "So we're getting-- we're much\ncloser to the single model not quite being able to specify\nall the rules of intelligence,",
    "start": "3333420",
    "end": "3340040"
  },
  {
    "text": "but at least learning\nall the rules from data. We're very close. We're much closer\nthan we were before.",
    "start": "3340040",
    "end": "3346310"
  },
  {
    "text": "Now, this doesn't include\nall the important-- all the important\nspecialization that",
    "start": "3346310",
    "end": "3351710"
  },
  {
    "text": "has to happen after the\nchip or the alignment that you have to do to make\nthe model more steerable.",
    "start": "3351710",
    "end": "3359180"
  },
  {
    "text": " And as it stands\ntoday, the scaling laws",
    "start": "3359180",
    "end": "3366080"
  },
  {
    "text": "that the transformer\nexhibits are better than any other existing model. So there's an interesting\nquestion of which--",
    "start": "3366080",
    "end": "3374360"
  },
  {
    "text": "can we build a better model? And there are efforts, I\nguess, from the Stanford-- from Chris Ray's lab, there\nhave been a couple of efforts.",
    "start": "3374360",
    "end": "3382030"
  },
  {
    "text": "There's been some\nrevival of [INAUDIBLE],, but I think the only thing I'll\nsay there is that the attention",
    "start": "3382030",
    "end": "3388850"
  },
  {
    "text": "operation itself-- this operation of actually\nmoving information around or routing information\nbased on content--",
    "start": "3388850",
    "end": "3394460"
  },
  {
    "text": "it's very, very useful. And it's maybe not\na surprise that this general spatial mixing,\nupsampling, downsampling,",
    "start": "3394460",
    "end": "3402619"
  },
  {
    "text": "architecture has kind of\nstayed with computer vision and language, now\nwith the transformer. So there are some invariants\nthat are likely to stay,",
    "start": "3402620",
    "end": "3409340"
  },
  {
    "text": "but I do think\nthat maybe there-- and there is certainly much\nmore room there to improve--",
    "start": "3409340",
    "end": "3415100"
  },
  {
    "text": "I mean not just in the\narchitecture but on data itself. Like, there's probably two\nX improvements on data.",
    "start": "3415100",
    "end": "3421670"
  },
  {
    "text": "But I wouldn't say\nthat there aren't architectures in the future that\nwill get better scaling laws.",
    "start": "3421670",
    "end": "3426860"
  },
  {
    "text": "They might, but\nthere are properties about the transformers,\nsuch as self-attention and its general\nstructure that is",
    "start": "3426860",
    "end": "3434089"
  },
  {
    "text": "likely we're going to see in\nfuture architectures to come. Also it's hard to really\nthink of a modern--",
    "start": "3434090",
    "end": "3441880"
  },
  {
    "text": "like, if somebody\nreally, really wanted to study large scale\nmodern transformers, you'd have to\nstudy all reduces--",
    "start": "3441880",
    "end": "3449590"
  },
  {
    "text": "infiniband, rocky, and-- like,\nwhether you get congestion and you have very, very\nlarge clusters, so it's--",
    "start": "3449590",
    "end": "3457430"
  },
  {
    "text": "So the computer is no-- the transformer is\nnow in some sense a data center because\nit's now split up.",
    "start": "3457430",
    "end": "3463569"
  },
  {
    "text": "These large transformers,\npotentially tens of thousands of GPUs, and so\nnow, you actually",
    "start": "3463570",
    "end": "3472839"
  },
  {
    "text": "have to really focus on several\nparts-- the infrastructure [INAUDIBLE] and\nthe model itself.",
    "start": "3472840",
    "end": "3478900"
  },
  {
    "text": "But what's really\ninteresting I think is-- I was just thinking of the\nsmallest model that has",
    "start": "3478900",
    "end": "3484270"
  },
  {
    "text": "exhibited emergent phenomena. Well, so we certainly know\nthat GPT-4, which is likely--",
    "start": "3484270",
    "end": "3489789"
  },
  {
    "text": "I don't know if you're\nallowed to say it-- like, trillion parameters.",
    "start": "3489790",
    "end": "3494859"
  },
  {
    "text": "Yes. That's a trillion\nparameter size model. That's what everybody says-- size model. And then you have\ngrokking, which",
    "start": "3494860",
    "end": "3500839"
  },
  {
    "text": "is a two-layer transformer that\nhas this weird emergent behavior",
    "start": "3500840",
    "end": "3506150"
  },
  {
    "text": "that when you just keep\ntraining it on just on some amount of data, suddenly\nit just exhibits a space shift.",
    "start": "3506150",
    "end": "3511549"
  },
  {
    "text": "So we're lucky. There are these really-- there's weirdness everywhere.",
    "start": "3511550",
    "end": "3517250"
  },
  {
    "text": "There's weirdness in small\nmodels and large models, and maybe we can learn\nsomething about large models",
    "start": "3517250",
    "end": "3523313"
  },
  {
    "text": "by studying these small\nmodels one would hope. But it's funny. Like, there's still\nunexplained phenomena",
    "start": "3523313",
    "end": "3529820"
  },
  {
    "text": "in very, very large models\nand very, very small models. But large transformers are\nno more just like a colab.",
    "start": "3529820",
    "end": "3537860"
  },
  {
    "text": "I mean, it could still\nbe, but it's you have to there's so many-- there's so much that you\nhave to keep in your stack",
    "start": "3537860",
    "end": "3545150"
  },
  {
    "text": "in order to really\noptimize this model. Of course, some of the\nvery exciting directions",
    "start": "3545150",
    "end": "3551270"
  },
  {
    "text": "are LLMs using tools. So now, the benefits of-- now,\nlanguage models or transformers",
    "start": "3551270",
    "end": "3558732"
  },
  {
    "text": "are actually starting to\nuse external entities, so they're connecting with\nthe rest of the world, and I guess that's a good pitch\nfor-- it makes a lot of sense",
    "start": "3558732",
    "end": "3567440"
  },
  {
    "text": "to actually build products\ntoday because it's through interactions with-- Like, if you want to get to the\nnext tranche of capabilities,",
    "start": "3567440",
    "end": "3574430"
  },
  {
    "text": "where will they come from? And likely, with a\nlot of usage, you will learn much more about\nhow to guide these models",
    "start": "3574430",
    "end": "3580880"
  },
  {
    "text": "and how to train them\nwithout them in vacuum. Now, you can definitely\ndo very, very important work still with a\nsmaller model, or even",
    "start": "3580880",
    "end": "3589685"
  },
  {
    "text": "without building a\nproduct-- without building a product because there's\nso many important unsolved problems. And maybe you shouldn't\neven work on the transformer",
    "start": "3589685",
    "end": "3596583"
  },
  {
    "text": "because it's like\nburning man right now. Everybody's going\nto the same party, but I think that you\nwill be able to build",
    "start": "3596583",
    "end": "3604520"
  },
  {
    "text": "new capabilities once this\nhuman machine collaboration.",
    "start": "3604520",
    "end": "3609680"
  },
  {
    "text": "Of course, you know,\nteaching models or models being able to express\nwhat they don't know--",
    "start": "3609680",
    "end": "3614960"
  },
  {
    "text": "How do you learn new\nskills at inference time important for this\ninteresting work? I think on Minecraft that\nshowed some evidence of this.",
    "start": "3614960",
    "end": "3621620"
  },
  {
    "text": "It's also important for agents. And a great property that\nsome of these diffusion models",
    "start": "3621620",
    "end": "3627950"
  },
  {
    "text": "have is the more compute you\nspend, the potentially better the quality of the image gets,\nbut we don't exactly quite have that for language.",
    "start": "3627950",
    "end": "3634280"
  },
  {
    "text": "And what does that mean? So today, the models\nthat can reason that are the most proficient\nreasoning and planning",
    "start": "3634280",
    "end": "3641210"
  },
  {
    "text": "are also the largest ones. Can we separate it out? Can just smaller models that\ndo some adaptive thinking",
    "start": "3641210",
    "end": "3647660"
  },
  {
    "text": "and are able to match the\ncapabilities of potentially larger models and\nreasoning and planning? And maybe the answer\nis going to come",
    "start": "3647660",
    "end": "3654032"
  },
  {
    "text": "by connecting to external\nplanners and planners, or maybe with better\nrepresentations of data",
    "start": "3654032",
    "end": "3660770"
  },
  {
    "text": "you can actually\nreason better on it. Also this is again a\nmore systems piece,",
    "start": "3660770",
    "end": "3666650"
  },
  {
    "text": "but it's fascinating how low you\ncan actually get on your on-- how low you can--",
    "start": "3666650",
    "end": "3672740"
  },
  {
    "text": "how few bits you can actually\nuse and still get something useful out. We already went from-- the original transformer was\ntrained on 32-bit precision.",
    "start": "3672740",
    "end": "3680000"
  },
  {
    "text": "Then we went to\nbfloat16, and now there's good signs that INT8\nand fp8 would also work.",
    "start": "3680000",
    "end": "3685010"
  },
  {
    "text": "And I think there's useful work\nto be done that again going back to the same-- this argument about\nif you're actually--",
    "start": "3685010",
    "end": "3693830"
  },
  {
    "text": "If you're using fewer bits\nto represent a number, you're actually transmitting\nfewer bits from HPM,",
    "start": "3693830",
    "end": "3699560"
  },
  {
    "text": "so actually, you can get faster. And you utilize the-- you can\nutilize your matrix multipliers much more effectively.",
    "start": "3699560",
    "end": "3706819"
  },
  {
    "text": "That was it. Many topics but hopefully\ncovered something fun. Thank you.",
    "start": "3706820",
    "end": "3712070"
  },
  {
    "start": "3712070",
    "end": "3719520"
  },
  {
    "text": "Yeah. Can you talk about what\nyou're working on now? Yeah. Yeah.",
    "start": "3719520",
    "end": "3724830"
  },
  {
    "text": "So I'm a co-founder of a startup\nwith my tranformer co-author,",
    "start": "3724830",
    "end": "3731340"
  },
  {
    "text": "Nikki, and we're working on\nbuilding models that will",
    "start": "3731340",
    "end": "3737490"
  },
  {
    "text": "ultimately automate workflows. And we're starting\nwith data, so it's",
    "start": "3737490",
    "end": "3743442"
  },
  {
    "text": "very puzzling what\nhappens in a company. Companies are basically just\nmasses of dark knowledge, right? And there's very\nfew people that have",
    "start": "3743443",
    "end": "3750060"
  },
  {
    "text": "both the technical privilege\nand the understanding to ask questions-- like,\ntypically analysts.",
    "start": "3750060",
    "end": "3755640"
  },
  {
    "text": "But unless you understand\nthe less effective your company can be, so\nhow can you eventually",
    "start": "3755640",
    "end": "3760710"
  },
  {
    "text": "help anyone become an effective\nanalyst in some sense, right? So help them ask\nthe right question,",
    "start": "3760710",
    "end": "3765990"
  },
  {
    "text": "help them figure out eventually\nthe why's, which then requires some kind of\ncounterfactual reasoning",
    "start": "3765990",
    "end": "3771090"
  },
  {
    "text": "that's fairly complicated. But start with data\nsince it's so important, and companies are\nessentially drowning in it.",
    "start": "3771090",
    "end": "3777480"
  },
  {
    "text": "And then it can be\nspread out from there, and then try to automate other\nworkflows and then enterprise. But we believe that some\nof the early signs that",
    "start": "3777480",
    "end": "3785079"
  },
  {
    "text": "we're [INAUDIBLE] and what-- and our position is that-- I believe that this is going to\nrequire a full stack approach",
    "start": "3785080",
    "end": "3792579"
  },
  {
    "text": "to not just them building the\nmodel because you can then control what feedback you get.",
    "start": "3792580",
    "end": "3798580"
  },
  {
    "text": "And so if you have a gap in\nthe model, you ask for that. You start to get that\nfeedback, so then you can improve the model.",
    "start": "3798580",
    "end": "3804549"
  },
  {
    "text": "So that's what we're doing. Yeah, it's-- please talk to us.",
    "start": "3804550",
    "end": "3809930"
  },
  {
    "text": "Yeah. I'm surprised to\nhear that you're fairly bullish about\nschools in the end. Like, [INAUDIBLE]\nthird party things.",
    "start": "3809930",
    "end": "3816165"
  },
  {
    "text": "We talked about in the beginning\nthat your motivation was transformers enabled us\nto get rid of pipelines, but I feel like [INAUDIBLE]\nagain, so I'm surprised.",
    "start": "3816165",
    "end": "3824090"
  },
  {
    "text": "Can you talk about that? And where do you\nthink [INAUDIBLE]?? Right. Right. So I mean, so until we get to\nthe point where it's like--",
    "start": "3824090",
    "end": "3832026"
  },
  {
    "text": "we're turtles all the way down-- transformers all the way down. I think that tool\njust allows you to--",
    "start": "3832027",
    "end": "3838190"
  },
  {
    "text": "so it's kind of like how do you\ninterface with a machine that can really like-- that can think right?",
    "start": "3838190",
    "end": "3844190"
  },
  {
    "text": "Yeah. You have to build some\nkind of interface, and if you build a\nuseful functionality, you want the machine to be\nable to take your functionality",
    "start": "3844190",
    "end": "3850387"
  },
  {
    "text": "in the end and do general\nuseful things with it, right? And I think that\nusing tools is just a way of leveraging things\nthat people have built",
    "start": "3850387",
    "end": "3858140"
  },
  {
    "text": "and software out there. Certain tools will probably get\nabsorbed in the model, right? Some others won't, and that\nstill gives us the ability to--",
    "start": "3858140",
    "end": "3868040"
  },
  {
    "text": "it still gives us\nthe ability to-- and certain things that\ntransformers shouldn't even do.",
    "start": "3868040",
    "end": "3873980"
  },
  {
    "text": "Sorry. I mean, like, you\ndon't want to spend a billion flops per position\nto calculate two numbers.",
    "start": "3873980",
    "end": "3880520"
  },
  {
    "text": "You don't want to\nspend more flops to do an operation that requires\nlike 1 billion flops, right? So there are certain things\nthat the model should not do.",
    "start": "3880520",
    "end": "3887839"
  },
  {
    "text": "It should use external-- it\nshould use external tools. And there are\ncertain things that--",
    "start": "3887840",
    "end": "3893707"
  },
  {
    "text": "certain kind of thinking\nthat the model should do. So even from a\ncapability perspective, there's an important question of\nwhat all capabilities should be",
    "start": "3893707",
    "end": "3900990"
  },
  {
    "text": "in this neural network, right? But then also being\nable to utilize the work that other others\nhave done-- software that other people\nhave built. Yeah.",
    "start": "3900990",
    "end": "3908860"
  },
  {
    "text": "Can you talk more about\nwhy the original approach of decoding parallely and\nthen [INAUDIBLE] refining it.",
    "start": "3908860",
    "end": "3915710"
  },
  {
    "text": "Yeah. [INAUDIBLE] why that\ndidn't work and what. Yeah. So sometimes if you know exactly\nwhy things work maybe you can",
    "start": "3915710",
    "end": "3921653"
  },
  {
    "text": "make them work, but\nit ended up being-- so we were able to do silly\nthings, like randomly sort,",
    "start": "3921653",
    "end": "3927020"
  },
  {
    "text": "which means that if somebody\nwalks up to you with a sequence-- you can-- I mean, you\ncan break two modes.",
    "start": "3927020",
    "end": "3934040"
  },
  {
    "text": "Like, you can say\nascending or descending. So how do I say this? So typically, when you decode--",
    "start": "3934040",
    "end": "3940070"
  },
  {
    "text": "imagine that when\nyou give a prompt, you have many\npossible completions.",
    "start": "3940070",
    "end": "3945410"
  },
  {
    "text": "And each time you make a\nchoice, you narrow that space.",
    "start": "3945410",
    "end": "3950690"
  },
  {
    "text": "And each time-- another\nchoice, you narrow that space. So and you have a very-- and you have learned\nto narrow the set",
    "start": "3950690",
    "end": "3958310"
  },
  {
    "text": "of all possible in some\nsense paths in a way the model doesn't have to decide\nwhat's the order in which you",
    "start": "3958310",
    "end": "3964550"
  },
  {
    "text": "have to do it, right? When you're kind\nof-- when you're doing this less or\nnon-autoregressive generation,",
    "start": "3964550",
    "end": "3969570"
  },
  {
    "text": "you have to do both, right? And learning both\nsimultaneously is hard.",
    "start": "3969570",
    "end": "3974870"
  },
  {
    "text": "I mean, but eventually, I think\nthat if for a particular--",
    "start": "3974870",
    "end": "3980810"
  },
  {
    "text": "I think this is\nprobably true, right? If an oracle walked\nup to me and said this is the order in which\nall these sentences should",
    "start": "3980810",
    "end": "3988293"
  },
  {
    "text": "be generated, first you should\ngenerate these three words, then you should generate these\nother two, then these other two. If somebody walked up to\nme and gave me this oracle",
    "start": "3988293",
    "end": "3995240"
  },
  {
    "text": "ordering for all\nof human language, I think you would have\na much better chance, and you could actually get\nthis less non-autoregressive",
    "start": "3995240",
    "end": "4000532"
  },
  {
    "text": "generation at least working. So what are the two\nthings you said? So one thing was basically\nthe ordering itself,",
    "start": "4000532",
    "end": "4008473"
  },
  {
    "text": "and I think it kind of has to do\nthat because the ordering helps you then lock down the modes. It narrows down what you're\ngoing to generate next.",
    "start": "4008473",
    "end": "4015100"
  },
  {
    "text": "So ultimately, I\nthink it does boil down to what's the right\nnon-autoregressive ordering,",
    "start": "4015100",
    "end": "4020260"
  },
  {
    "text": "and that could be either\nyou're still generating one word at a time but\nnot autoregressively, or you're generating a\nfew and then based on",
    "start": "4020260",
    "end": "4026110"
  },
  {
    "text": "that you're generating\nthe other few. So the words that you\ncan generate all at once should be conditionally\nindependent of each other.",
    "start": "4026110",
    "end": "4033340"
  },
  {
    "text": "What you've generated so far\nshould have completely explained them, and then what you\ngenerate after should again be--",
    "start": "4033340",
    "end": "4039472"
  },
  {
    "text": "they should be\nconditionally independent. So how do you learn these\nconditional independencies? Yeah. If somebody walked up to\nme and gave them to me,",
    "start": "4039472",
    "end": "4045888"
  },
  {
    "text": "I think they probably\nlearned them. Yeah. ",
    "start": "4045888",
    "end": "4052090"
  },
  {
    "text": "It's more of a [INAUDIBLE]. Recently, [INAUDIBLE].",
    "start": "4052090",
    "end": "4057684"
  },
  {
    "start": "4057684",
    "end": "4063900"
  },
  {
    "text": "And I think more of the thinking\nis that only an extremely small spot doesn't help\nthem to actually learn",
    "start": "4063900",
    "end": "4071250"
  },
  {
    "text": "what the real world actually\nworks and [INAUDIBLE] we had a good idea of that truth\nand the real world in general.",
    "start": "4071250",
    "end": "4080130"
  },
  {
    "text": "Do you agree with him? Do you think that [INAUDIBLE]? ",
    "start": "4080130",
    "end": "4088420"
  },
  {
    "text": "So yeah, I think it's\ninteresting [INAUDIBLE] you can learn a world model\nwith just language, right?",
    "start": "4088420",
    "end": "4096339"
  },
  {
    "text": "So I mean, some of these\nmodels are not exactly being learned that way. You're doing [INAUDIBLE]. You're getting some\nfeedback, so which",
    "start": "4096340",
    "end": "4102283"
  },
  {
    "text": "means there's some\nyou're applying some-- they are modifying themselves\nto some preference,",
    "start": "4102283",
    "end": "4109239"
  },
  {
    "text": "so it's not just a\npure language model. But it's interesting.",
    "start": "4109240",
    "end": "4114259"
  },
  {
    "text": "So you've seen some of the\nwork, where robotics is now potentially starting to\nflourish because they're",
    "start": "4114260",
    "end": "4119770"
  },
  {
    "text": "able to use these large\nmodels as planners, right? And so I think that\nit's surprising how",
    "start": "4119770",
    "end": "4125457"
  },
  {
    "text": "much of the world-- how much\ninformation about the world that they carry. If I understand, is it\nright that the [INAUDIBLE]",
    "start": "4125457",
    "end": "4130568"
  },
  {
    "text": "can work basically use a\nlanguage model now as a planner? Yes. Right? And then they left\nthe rest of it",
    "start": "4130569",
    "end": "4135580"
  },
  {
    "text": "to just the standard perception\nand the classical tasks [INAUDIBLE] robotics.",
    "start": "4135580",
    "end": "4140740"
  },
  {
    "text": "So I mean, that's-- now, while John is\nprobably still right,",
    "start": "4140740",
    "end": "4145870"
  },
  {
    "text": "but the usefulness of it is\nevident in something that needs world knowledge, right?",
    "start": "4145870",
    "end": "4150979"
  },
  {
    "text": "So I think you can do a\nlot with what you have. I mean, probably-- yeah.",
    "start": "4150979",
    "end": "4160409"
  },
  {
    "text": "I mean, we still haven't quite\nextracted all the usefulness out of these models as\nwell, and it might be right,",
    "start": "4160410",
    "end": "4168750"
  },
  {
    "text": "but there's still a\nlot more to be gained. Yeah. ",
    "start": "4168750",
    "end": "4173880"
  },
  {
    "text": "Yeah. So similar to the previous\nquestion, and you've also been talking about\nemergent abilities, right?",
    "start": "4173880",
    "end": "4181750"
  },
  {
    "text": "I'm just curious\nto know what are your thoughts are more on\ngeneralizability and emergent,",
    "start": "4181750",
    "end": "4187660"
  },
  {
    "text": "especially in the-- I know there was a paper from\nDeepmind about the assignment,",
    "start": "4187660",
    "end": "4193420"
  },
  {
    "text": "yeah. Yeah. Yeah. Yeah. They can't really generalize\noutside of like what they've been trained on, especially\nbecause these large models",
    "start": "4193420",
    "end": "4201010"
  },
  {
    "text": "now that they're just\ntrained on everything. Is there truly anything left\nthat's out of distribution that really sort\nof benchmark it on?",
    "start": "4201010",
    "end": "4208550"
  },
  {
    "text": "So I have been caught\nsaying that if I had all my test data\nand my training, I'd make a billion dollars. Yeah.",
    "start": "4208550",
    "end": "4213975"
  },
  {
    "text": "So I don't have a problem\nwith it, but I still think-- OK.",
    "start": "4213975",
    "end": "4220540"
  },
  {
    "text": "Correct me if I'm wrong,\nbut the general argument is that these\nmodels have learned such a vast set of\ndistributions and phenomena",
    "start": "4220540",
    "end": "4229480"
  },
  {
    "text": "that typically when\nyou interrogate them, they're often very know very\ncleverly blending or bringing",
    "start": "4229480",
    "end": "4237460"
  },
  {
    "text": "information from what\nthey've learned, right? It might-- yes. And then they have\nthese algorithmic tasks,",
    "start": "4237460",
    "end": "4244600"
  },
  {
    "text": "where the models fail\nto generalize, right? So I'll focus on the former.",
    "start": "4244600",
    "end": "4251260"
  },
  {
    "text": "I think that that's an\nincredibly useful property. It might be that-- So I think the--",
    "start": "4251260",
    "end": "4256628"
  },
  {
    "text": "maybe the failing is that\nwe actually don't quite understand how much we could-- how much is even represented\nin text, and second how much--",
    "start": "4256628",
    "end": "4264670"
  },
  {
    "text": "how far we could go if you're\nable to blend information from different-- like, certainly being able\nto write about the Stanford--",
    "start": "4264670",
    "end": "4273520"
  },
  {
    "text": "this lecture in the rhyme\nmeter and words of chaucer is not possible because\nnobody did it, right?",
    "start": "4273520",
    "end": "4279790"
  },
  {
    "text": "But I think that\nyou could do it. Now, is that\nblending information from what you already have?",
    "start": "4279790",
    "end": "4285040"
  },
  {
    "text": "If so that's a-- that means you can-- that's\nan incredible skill, right?",
    "start": "4285040",
    "end": "4290630"
  },
  {
    "text": " I haven't read it. It's very recent, but\nI believe the work.",
    "start": "4290630",
    "end": "4297040"
  },
  {
    "text": "I think you can show that\nthese models can generalize, but I think there's a surprising\namount of seemingly new things",
    "start": "4297040",
    "end": "4304510"
  },
  {
    "text": "you could do by just\nblending information from what you've already\nlearned on the internet.",
    "start": "4304510",
    "end": "4309790"
  },
  {
    "text": "And yeah, largely\nprobably has to do that there's so much happening. Yeah. Yeah.",
    "start": "4309790",
    "end": "4315542"
  },
  {
    "text": "So the other question-- yeah. Yeah, I had a [INAUDIBLE]. ",
    "start": "4315542",
    "end": "4321130"
  },
  {
    "text": "I think had an ordering in\nmind, and then I came back. Sorry, but give me a\nsecond [INAUDIBLE]..",
    "start": "4321130",
    "end": "4332060"
  },
  {
    "text": "I was wondering if you\nmight have insights into connecting different\nagents, transformers, or whatnot?",
    "start": "4332060",
    "end": "4337820"
  },
  {
    "text": "Neuron is a great--  transformer is essentially\nlike a great connection",
    "start": "4337820",
    "end": "4343790"
  },
  {
    "text": "of neurons in a specific\nway, and it's awesome, right? So you figured out the best\nway to connect them so far.",
    "start": "4343790",
    "end": "4350240"
  },
  {
    "text": "Agents? No-- the neurons. Oh, the neurons. You're talking about how\nto do this in the brain.",
    "start": "4350240",
    "end": "4355560"
  },
  {
    "text": "No. The neurons in the\ntransformer, right? The transformer is the way\nyou connect different pieces",
    "start": "4355560",
    "end": "4362480"
  },
  {
    "text": "together, and then when you\nconnect everything together, it works-- transformer. So I was wondering if\nyou have some insights",
    "start": "4362480",
    "end": "4368360"
  },
  {
    "text": "into building systems\nthat connect and perform the best together. Yeah.",
    "start": "4368360",
    "end": "4375332"
  },
  {
    "text": "And David's not\ngoing to like this. I like to make this joke-- that\nthe best agents are actually using neurons because they can\ncommunicate with each other.",
    "start": "4375332",
    "end": "4381620"
  },
  {
    "text": "They can update themselves\nreally, really well, but what the other\nagents are doing.",
    "start": "4381620",
    "end": "4387560"
  },
  {
    "text": "What is the fundamental\nproblem by making-- what is the fundamental\nissue in making a bunch of--",
    "start": "4387560",
    "end": "4395275"
  },
  {
    "text": "I'm trying to\nunderstand what are the fundamental\nproblems in trying to make a bunch of\nsystems work together? That's what you're asking.",
    "start": "4395275",
    "end": "4401090"
  },
  {
    "text": "What is goal\ndecomposition, right? And one is the second. Big one is coordination, and\nthird one is verification.",
    "start": "4401090",
    "end": "4409100"
  },
  {
    "text": "If you solve a successful\ndecomposition of the goals based on what your estimate of\nthe skills of these agents are,",
    "start": "4409100",
    "end": "4415340"
  },
  {
    "text": "if you're able to apply\nwhat they've done, and if you're able\nto coordinate, then I think you could\nmake a lot of progress.",
    "start": "4415340",
    "end": "4421460"
  },
  {
    "text": "So while I didn't\nanswer your question, I don't know in general\nhow much progress we've made in these three\nareas, but does somebody",
    "start": "4421460",
    "end": "4429140"
  },
  {
    "text": "have any input here? ",
    "start": "4429140",
    "end": "4435073"
  },
  {
    "text": "And you have something\nthat's managing [INAUDIBLE].. And you want to verify\nthis [INAUDIBLE]..",
    "start": "4435073",
    "end": "4441510"
  },
  {
    "text": "So it becomes like a\nmicroprocessor, right? There's a lot of\ntraining power when you want to make\nsure [INAUDIBLE]",
    "start": "4441510",
    "end": "4449239"
  },
  {
    "text": "and verify everything,\nand break it off. So you have this display\nperformance [INAUDIBLE]",
    "start": "4449240",
    "end": "4456050"
  },
  {
    "text": "everything and making\nit efficient over time. And [INAUDIBLE]. ",
    "start": "4456050",
    "end": "4462770"
  },
  {
    "text": "But it's still very early. No one knows really. Yeah, right, but-- [INTERPOSING VOICES] These are probably maybe to\nsome degree necessary as well.",
    "start": "4462770",
    "end": "4471920"
  },
  {
    "text": "I mean, you--  This is [INAUDIBLE]. ",
    "start": "4471920",
    "end": "4480620"
  },
  {
    "text": "It still remains to be\nseen with ensemble models. Now, we have a lot of\n[INAUDIBLE] modality, but the human brain is very\nmodular, so this modularity",
    "start": "4480620",
    "end": "4489320"
  },
  {
    "text": "in an emergent phenomena. You need some special\ntricks to make that happen.",
    "start": "4489320",
    "end": "4494890"
  },
  {
    "text": "Yeah, and by modularity\nhere you mean that-- is it modularity\nin that they have--",
    "start": "4494890",
    "end": "4501880"
  },
  {
    "text": "this region has\nthis responsibility, or even is the\ncomposition different?",
    "start": "4501880",
    "end": "4507100"
  },
  {
    "text": "The construction different. What do you mean by that? Because one, you\ncould have both. You could argue that\nthe responsibility",
    "start": "4507100",
    "end": "4514570"
  },
  {
    "text": "is diffused across the model. Mixtures of experts tries to go\nin the opposite direction, which I-- I should have\nprobably mentioned.",
    "start": "4514570",
    "end": "4520795"
  },
  {
    "text": "That's another really\nexciting direction, which certainly has\nhappened to the people this is going to stick. I totally missed it.",
    "start": "4520795",
    "end": "4527320"
  },
  {
    "text": "That kind of that tries to\nget the specialization, right? So maybe that is some\nkind of modularity, right?",
    "start": "4527320",
    "end": "4532570"
  },
  {
    "text": "Learn modularity. The rest is-- the rest of\nresponsibility for performing the task is likely distributed,\nbut if now you're going to these",
    "start": "4532570",
    "end": "4540820"
  },
  {
    "text": "subsystems themselves of\ndifferent composition, then you get back to-- and I know that this is a\ngoal with the pathways project",
    "start": "4540820",
    "end": "4549562"
  },
  {
    "text": "at Google, where\nyou wanted to have these really modular systems\ncommunicate with each other. And I think there's--",
    "start": "4549562",
    "end": "4556670"
  },
  {
    "text": "it's just taken so long\nto get gradient descent. Like gradient descent--\nin fact, sometimes I think that we're just\nbuilding architecture",
    "start": "4556670",
    "end": "4562050"
  },
  {
    "text": "to serve gradient descent. And like, if you can learn\nwith gradient descent,",
    "start": "4562050",
    "end": "4568400"
  },
  {
    "text": "it's very useful. Maybe it's actually possible to\nmake these modular systems work.",
    "start": "4568400",
    "end": "4573470"
  },
  {
    "text": "We have some of it\nthrough experts. And I'd imagine some of these\nproblems that we discussed.",
    "start": "4573470",
    "end": "4581540"
  },
  {
    "text": "Does that make sense? Yeah. All right. Circling back to\n[INAUDIBLE] ago,",
    "start": "4581540",
    "end": "4587680"
  },
  {
    "text": "you mentioned that the problem\nwith imploding all at once was one of the things that-- decode engineering always\nhas this assumption",
    "start": "4587680",
    "end": "4593890"
  },
  {
    "text": "that the outputs are\nconditionally independent, but aren't they in a sense that\nif you have a latent space--",
    "start": "4593890",
    "end": "4599200"
  },
  {
    "text": "if you're given a latent\nspace as your prior, then your posterior\noutputs should be conditionally independent\nof each other, right?",
    "start": "4599200",
    "end": "4605453"
  },
  {
    "text": "So great point, and where do\nyou get the latent space from? Well, from the encoder or\nwhatever at the beginning.",
    "start": "4605453",
    "end": "4612130"
  },
  {
    "text": "Right, but there might\nbe quite a few ways to translate something, right? Yeah.",
    "start": "4612130",
    "end": "4617140"
  },
  {
    "text": "There's multiple-- so if there's\nonly one mode, then yeah, it's probably--",
    "start": "4617140",
    "end": "4623200"
  },
  {
    "text": "But if there's\nmultiple ways of-- Well, so there's two things. How much does the latent space\nactually carry, by the way,",
    "start": "4623200",
    "end": "4629367"
  },
  {
    "text": "is an important thing to ask. How much does it actually carry? Because it's not just one latent\nvector that you're transmitting",
    "start": "4629367",
    "end": "4636070"
  },
  {
    "text": "every-- you're doing\nattention again and again, but we took this approach,\nwhere we did precisely this.",
    "start": "4636070",
    "end": "4644030"
  },
  {
    "text": "We autoregressively\ngenerated tokens",
    "start": "4644030",
    "end": "4650900"
  },
  {
    "text": "in a new vocabulary using\nvector quantization. So the conditional\ndependence was",
    "start": "4650900",
    "end": "4656540"
  },
  {
    "text": "modeled in a latent space,\nwhere we discretized using vector quantization. And then based on\nthat, we generated",
    "start": "4656540",
    "end": "4664250"
  },
  {
    "text": "everything conditioning,\nand that did work. But again, so that did\nwork in translation.",
    "start": "4664250",
    "end": "4672409"
  },
  {
    "text": "There were some\nfunky issues there, where the sequence\nof latent vectors",
    "start": "4672410",
    "end": "4679970"
  },
  {
    "text": "were only effective-- were not\neffective if you learn directly on the original data. You had to do something\nlike distillation",
    "start": "4679970",
    "end": "4686030"
  },
  {
    "text": "because distillation itself\nthrows away potentially some of the noise. So generally, lower entropy\ndata, we wanted to train on it.",
    "start": "4686030",
    "end": "4692570"
  },
  {
    "text": "The second piece was\nfor practical systems, you have to make the whole\nthing really, really fast,",
    "start": "4692570",
    "end": "4697610"
  },
  {
    "text": "but this is a good\nresearch exercise, but ultimately, it didn't have\nthe right practical impact",
    "start": "4697610",
    "end": "4704160"
  },
  {
    "text": "because speculative decoding\npractically with what we have right now can work well. Yeah. Yeah.",
    "start": "4704160",
    "end": "4709350"
  },
  {
    "text": "Exactly. Yeah, but you're right. I think if you can\ngenerate-- if you can generate a good\nsufficient latent state,",
    "start": "4709350",
    "end": "4715560"
  },
  {
    "text": "then yes, you're right. We can assume-- that\nmakes everything conditionally\nindependent, and yeah-- and we managed to do that a bit.",
    "start": "4715560",
    "end": "4722429"
  },
  {
    "text": "Managed to do that, but it\nwasn't quite good enough. Yeah.",
    "start": "4722430",
    "end": "4728170"
  },
  {
    "text": "I guess this is\nthe last question. What is it about? [INAUDIBLE]",
    "start": "4728170",
    "end": "4734110"
  },
  {
    "start": "4734110",
    "end": "4740860"
  },
  {
    "text": "That's too personal. Yeah. Very personal, and I\nhave friends there.",
    "start": "4740860",
    "end": "4747670"
  },
  {
    "text": "They're all really great. They're doing incredible things. ",
    "start": "4747670",
    "end": "4752770"
  },
  {
    "text": "I think that we'll be surprised\nhow much there is to do. And if-- so personal\nmotivation, right?",
    "start": "4752770",
    "end": "4761320"
  },
  {
    "text": "There is an entire new-- there's an entire\nnew bucket of--",
    "start": "4761320",
    "end": "4766360"
  },
  {
    "text": "a new tranche of\ncapabilities that you will get with human\ncomputer interaction. So you can make a product.",
    "start": "4766360",
    "end": "4772857"
  },
  {
    "text": "People use it. They give you feedback\nmodels, get smarter, and this closed loop\nsystem can really",
    "start": "4772857",
    "end": "4778780"
  },
  {
    "text": "bring-- can really advance\nmodels and bring value. That's one.",
    "start": "4778780",
    "end": "4784480"
  },
  {
    "text": "Second-- I think it's\nhelpful to have some.",
    "start": "4784480",
    "end": "4790030"
  },
  {
    "text": "Deep learning benefited\nso much from a diversity of ideas and people pursuing\nimportant directions,",
    "start": "4790030",
    "end": "4797769"
  },
  {
    "text": "and I would say the same about-- I would say the same about\nbuilding company products as",
    "start": "4797770",
    "end": "4803290"
  },
  {
    "text": "well or building\ncompanies that are-- building new kinds of products\nwith these models, right?",
    "start": "4803290",
    "end": "4810520"
  },
  {
    "text": "So I would say that\nwe have-- there's so much surface area that we\ncould do something incredible.",
    "start": "4810520",
    "end": "4818050"
  },
  {
    "text": "So that's the second piece. Third, yeah, maybe that's the\nmore personal-- that I just",
    "start": "4818050",
    "end": "4824140"
  },
  {
    "text": "want to do my own thing. Yeah. ",
    "start": "4824140",
    "end": "4838000"
  }
]