[
  {
    "text": "i'm going to talk a little bit about scaling laws originally I think we were going to talk about inference but I'll I'll take a few minutes to to start on",
    "start": "4880",
    "end": "11200"
  },
  {
    "text": "scaling laws and then we'll kind of figure out where we'll go from there okay so the whole point of scaling",
    "start": "11200",
    "end": "18800"
  },
  {
    "text": "laws is kind of well to begin with I want you to to put yourself into the following scenario right so you have a",
    "start": "18800",
    "end": "25199"
  },
  {
    "text": "very rich friend um and he or she has given you 10,000 actually let's say 100,000 H100s for a month um and you",
    "start": "25199",
    "end": "32640"
  },
  {
    "text": "have to build you know the best uh open source LM that you can right so this is a a somewhat hard task and we've given",
    "start": "32640",
    "end": "39600"
  },
  {
    "text": "you some of the tools that you need to make progress on this question um so you know you can uh put together your your",
    "start": "39600",
    "end": "45840"
  },
  {
    "text": "infra team and your systems people and you can put together a distributed training framework um in the next assign",
    "start": "45840",
    "end": "51520"
  },
  {
    "text": "after that you're going to put together a great pre-training data And then you kind of know all about you know architectures and so on so you kind of",
    "start": "51520",
    "end": "57120"
  },
  {
    "text": "know you have all the pieces and so we can turn the crank and we can run the big model and in the the first couple",
    "start": "57120",
    "end": "64080"
  },
  {
    "text": "lectures um we talked about you know all the other various decisions you might make along this journey right like",
    "start": "64080",
    "end": "69920"
  },
  {
    "text": "what's the architecture what's the hyperparameters like how are you going to do all these things well you know I",
    "start": "69920",
    "end": "75119"
  },
  {
    "text": "think the in some ways the answer I gave you from those early lectures was just pick what other people have done right",
    "start": "75119",
    "end": "81040"
  },
  {
    "text": "like just follow llama or whatever other models but in a way that's a very boring answer because that doesn't let you push",
    "start": "81040",
    "end": "86560"
  },
  {
    "text": "the frontiers right like if you're if you're in like a big frontier lab and you're going to build the best model you don't want to just copy other people you",
    "start": "86560",
    "end": "93040"
  },
  {
    "text": "want to innovate right so how do we innovate and get these optimized solutions in the first place so that's",
    "start": "93040",
    "end": "100079"
  },
  {
    "text": "kind of going to be um the point of scaling laws what we want to do is we want to build simple predictive um laws",
    "start": "100079",
    "end": "106799"
  },
  {
    "text": "for the behavior of language models and scaling laws are basically this this whole idea of being able to take small",
    "start": "106799",
    "end": "113520"
  },
  {
    "text": "models scale them up um and be able to to do that in order to to improve your",
    "start": "113520",
    "end": "118960"
  },
  {
    "text": "engineering right so one way of thinking about this is the old unpleasant way of doing deep learning is just you know",
    "start": "118960",
    "end": "125280"
  },
  {
    "text": "train a bunch of big models tune your hyperparameters so that your big models are good right um that's just going to",
    "start": "125280",
    "end": "130879"
  },
  {
    "text": "cost tons and tons of compute like you can't really easily do that um and so I",
    "start": "130879",
    "end": "135920"
  },
  {
    "text": "think the new optimism um and if you're sort of following a lot of this these developments on scaling you know you",
    "start": "135920",
    "end": "142080"
  },
  {
    "text": "kind of think of this as all right we're going to train a bunch of small models we're going to learn a lot of things from those small models and then we're",
    "start": "142080",
    "end": "147680"
  },
  {
    "text": "going to extrapolate them back up two two bigger models right so we're going to take our smallest models at you know",
    "start": "147680",
    "end": "152959"
  },
  {
    "text": "the left side of this this sort of compute scale here and I'm going to learn a lot about what to do and then I'm going to nail it in one go when I",
    "start": "152959",
    "end": "159840"
  },
  {
    "text": "build my big model um and the first place I want to start with is just kind of the the",
    "start": "159840",
    "end": "166160"
  },
  {
    "text": "history and the background of scaling laws um and I want to contextualize this because I think when people talk about",
    "start": "166160",
    "end": "171840"
  },
  {
    "text": "scaling laws often this is done in like very like messaic like AGI terms they're like",
    "start": "171840",
    "end": "178080"
  },
  {
    "text": "scaling laws just tell you that you know these amazing things are log linear forever and we will achieve you know",
    "start": "178080",
    "end": "183120"
  },
  {
    "text": "super intelligence or something um but I think scaling laws are actually much more grounded and have a lot of",
    "start": "183120",
    "end": "188400"
  },
  {
    "text": "interesting history and so I'm going to start there to sort of try to convince you that scaling laws aren't necessarily",
    "start": "188400",
    "end": "194239"
  },
  {
    "text": "just fitting lines on log plots although that is a very big part of what we're going to do um and then I'm going to do",
    "start": "194239",
    "end": "201360"
  },
  {
    "text": "basically very easy steps i'm going to try to convince you that at least for data scaling laws are very natural thing",
    "start": "201360",
    "end": "206959"
  },
  {
    "text": "to think about and expect so so as a person that's kind of brought up in in statistical machine",
    "start": "206959",
    "end": "213440"
  },
  {
    "text": "learning you know my my starting point is going to be statistical machine learning right like what is scaling laws",
    "start": "213440",
    "end": "218799"
  },
  {
    "text": "you know in some ways scaling laws are telling us as we increase the amount of data or we change the model size we",
    "start": "218799",
    "end": "224319"
  },
  {
    "text": "expect certain behaviors out of the model right and if you go back to something like machine learning 101 and",
    "start": "224319",
    "end": "231040"
  },
  {
    "text": "if you remember your like VC dimensions and rotomacher complexities and so on um in some ways that's the theory version",
    "start": "231040",
    "end": "237120"
  },
  {
    "text": "of exactly this so so you know I have on the top you know uh generalization bound for how the the generalization bound for",
    "start": "237120",
    "end": "244959"
  },
  {
    "text": "the excess risk of learning amongst a finite set of k hypotheses and we see that that should scale as one over",
    "start": "244959",
    "end": "252640"
  },
  {
    "text": "square root of m right in some ways that's a theoretical version of a scaling law where we're making predictions about how fast our error",
    "start": "252640",
    "end": "259440"
  },
  {
    "text": "should decay as a function um of n on the bottom we might have something a little bit more exotic if we're doing",
    "start": "259440",
    "end": "265919"
  },
  {
    "text": "generative modeling and we're our generative model is a really flexible nonparametric class um what we might do",
    "start": "265919",
    "end": "272160"
  },
  {
    "text": "instead is we might you know fit some sort of smooth uh density so in this case you know uh our prediction is that",
    "start": "272160",
    "end": "278400"
  },
  {
    "text": "the L2 sort of error uh of estimating a density is going to be upperbounded by",
    "start": "278400",
    "end": "284400"
  },
  {
    "text": "some polomial n to the beta over 2 beta plus one right this is what some people might call nonparametric rates so you",
    "start": "284400",
    "end": "291520"
  },
  {
    "text": "know theorists have been thinking for a very long time about how sample size especially should relate to error right",
    "start": "291520",
    "end": "296960"
  },
  {
    "text": "this is a very classic problem um that people have thought about in machine learning theory but these are upper",
    "start": "296960",
    "end": "302240"
  },
  {
    "text": "bounds not actual realized loss values and really scaling laws are in some sense the leap from thinking about kind",
    "start": "302240",
    "end": "308000"
  },
  {
    "text": "of the theoretical side of how should data and model size relate to performance and going to the empirical",
    "start": "308000",
    "end": "314400"
  },
  {
    "text": "side of saying actually our bounds are bad but maybe we can actually fit these things empirically",
    "start": "314400",
    "end": "321680"
  },
  {
    "text": "and this is a fun trivia fact you know or or arguable trivia fact like what is the first scaling loss paper um and",
    "start": "321680",
    "end": "328639"
  },
  {
    "text": "actually not many papers cite this one but I think probably the right first scaling law papers is um a paper from",
    "start": "328639",
    "end": "335360"
  },
  {
    "text": "1993 Nurips's uh from Bell Labs um and you might recognize some of these names these are you know kind of theorists um",
    "start": "335360",
    "end": "342720"
  },
  {
    "text": "and some of the people that have done really classic work in in machine learning theory like you know Vapnik and Karina Cortez and others and I' I've",
    "start": "342720",
    "end": "350479"
  },
  {
    "text": "taken excerpts because you know I was reading this paper actually just you know preparing this this lecture earlier",
    "start": "350479",
    "end": "356320"
  },
  {
    "text": "um and it just struck me how you know ahead of its time in many ways this paper was right it's saying you know",
    "start": "356320",
    "end": "362000"
  },
  {
    "text": "training classifiers on large databases is very computationally demanding and we need to figure out which ones are good",
    "start": "362000",
    "end": "367680"
  },
  {
    "text": "before actually training them and so what we're going to do is we're going to propose a new predictive method that",
    "start": "367680",
    "end": "372800"
  },
  {
    "text": "predicts how good a model is going to be without actually training the whole thing right and that look that sounds a lot like scaling loss Um and you'll see",
    "start": "372800",
    "end": "379840"
  },
  {
    "text": "this later um but you know they have a functional form that's basically like oh the test error of a model is you know",
    "start": "379840",
    "end": "386400"
  },
  {
    "text": "expressible as some irreducible error plus a polomially decaying term and you're like huh that looks a lot like a",
    "start": "386400",
    "end": "391919"
  },
  {
    "text": "modern scaling law and they even do the thing where they you know train a bunch of small models they fit their their",
    "start": "391919",
    "end": "398240"
  },
  {
    "text": "curves and they're like oh we can accurately predict the behavior of the model um further out so so as with many",
    "start": "398240",
    "end": "403600"
  },
  {
    "text": "things I guess you know scaling laws partially you know thought about at Bell Labs way back",
    "start": "403600",
    "end": "410199"
  },
  {
    "text": "when and of course there's others that I think you know have thought about related ideas in scaling um not just",
    "start": "410199",
    "end": "417120"
  },
  {
    "text": "scaling knowledge but also really the modern mindset I think of of thinking about scaling um there's another paper",
    "start": "417120",
    "end": "422720"
  },
  {
    "text": "that often gets mentioned in sort of the history of scaling laws um Bo Banko and Bril um who was studying sort of how",
    "start": "422720",
    "end": "429440"
  },
  {
    "text": "does the performance of a certain kind of NLP system scale with the amount of data and they have you know what looks",
    "start": "429440",
    "end": "435280"
  },
  {
    "text": "like you know very often a modern scaling law you know log axis data on the x-axis performance on the y-axis and",
    "start": "435280",
    "end": "442160"
  },
  {
    "text": "you know they're basically arguing well look we can get really dramatic performance improvements just by scaling",
    "start": "442160",
    "end": "447840"
  },
  {
    "text": "up data it's very predictable and you know maybe we should consider the trade-off spent between you know",
    "start": "447840",
    "end": "453440"
  },
  {
    "text": "spending time and money on algorithm development versus just collecting more data and you're like \"Huh that sounds a lot like what a lot of this pre-training",
    "start": "453440",
    "end": "459919"
  },
  {
    "text": "stuff is thinking about.\" And then finally you know one of the things that I think people have",
    "start": "459919",
    "end": "466080"
  },
  {
    "text": "thought about recently and in the past is you know is this thing really predictable what are the right functional forms um and as early as",
    "start": "466080",
    "end": "472880"
  },
  {
    "text": "early as like 2012 you know people were really thinking about all right like are these things actually predictable you",
    "start": "472880",
    "end": "478160"
  },
  {
    "text": "know is power law like for example power 3 and pow 4 are those really the right functional forms for predicting the",
    "start": "478160",
    "end": "484240"
  },
  {
    "text": "behavior of models and of course all of this you know just to remind you right",
    "start": "484240",
    "end": "489520"
  },
  {
    "text": "is thinking about the behavior of models on the y- axis the capabilities as a function of the amount of data that you",
    "start": "489520",
    "end": "495759"
  },
  {
    "text": "have on the x-axis here right so that's the relationship that I think has been really classically studied what you might call data scaling um in all these",
    "start": "495759",
    "end": "502919"
  },
  {
    "text": "cases and if you're interested in like kind of the the earliest like largecale old neural scaling law paper that would",
    "start": "502919",
    "end": "510000"
  },
  {
    "text": "probably be hes at all in 2017 um I believe they were at by due when they did this work um they showed that for uh",
    "start": "510000",
    "end": "517680"
  },
  {
    "text": "a range of tasks uh machine translation um speech and I think like some vision",
    "start": "517680",
    "end": "523680"
  },
  {
    "text": "tasks um they showed that essentially error rates fall as a power law um and",
    "start": "523680",
    "end": "528720"
  },
  {
    "text": "they even have this nice plot that I really like to to refer to when when people are discussing scaling loss that really your expectation should be that",
    "start": "528720",
    "end": "535279"
  },
  {
    "text": "there's three different regions in the behavior of a model right initially you start out at best guess you then enter",
    "start": "535279",
    "end": "541600"
  },
  {
    "text": "into a region where you're kind of predictably scaling the model that's the power law region region and then there's another asmtoic region where you're",
    "start": "541600",
    "end": "548160"
  },
  {
    "text": "approaching essentially the irreducible error of your model class um and I'll",
    "start": "548160",
    "end": "553200"
  },
  {
    "text": "kind of highlight that I think you know there's been in the last few years a lot of talk of new phenomena things like oh",
    "start": "553200",
    "end": "560560"
  },
  {
    "text": "emerging capabilities or like scaling compute being a new thing or uh systems being really important um but had you",
    "start": "560560",
    "end": "567760"
  },
  {
    "text": "been reading sort of Hessnus in 2018 carefully you would have seen essentially all of these things you know",
    "start": "567760",
    "end": "573680"
  },
  {
    "text": "um they say actually you know it's really hard to do predictions by scaling law when models are are at random",
    "start": "573680",
    "end": "579680"
  },
  {
    "text": "performance because suddenly you can leave the random region um they talk about computational limits actually you",
    "start": "579680",
    "end": "585760"
  },
  {
    "text": "know if we can scale it means actually scaling by compute is really important and then finally they even say things",
    "start": "585760",
    "end": "591440"
  },
  {
    "text": "like you know maybe we should do things like quantization because if we have predictable scaling then that means we",
    "start": "591440",
    "end": "596640"
  },
  {
    "text": "should be willing to pay for model accuracy with compute right these are all very very modern ideas that I think",
    "start": "596640",
    "end": "602160"
  },
  {
    "text": "a lot of the early scaling law papers I think kind of understood fairly intuitively because you know once you",
    "start": "602160",
    "end": "608240"
  },
  {
    "text": "see these plots you kind of see that actually with predictable resource investment you get predictable capabilities improvements right so that",
    "start": "608240",
    "end": "614560"
  },
  {
    "text": "that's in some sense um sort of the core not quite history but I context um that",
    "start": "614560",
    "end": "621839"
  },
  {
    "text": "has really shaped uh scaling loss all right any questions so far on kind of the context this is mainly just kind of",
    "start": "621839",
    "end": "628720"
  },
  {
    "text": "data scaling but I wanted to make sure we we go over it carefully",
    "start": "628720",
    "end": "634279"
  },
  {
    "text": "yes like it's pretty natural for like scaling i was wondering like like is",
    "start": "634560",
    "end": "641360"
  },
  {
    "text": "there cases where there isn't scaling where Yeah yeah so the question was um you",
    "start": "641360",
    "end": "646800"
  },
  {
    "text": "know it's natural or or maybe it maybe arguably natural to expect scaling are there cases where we don't get scaling",
    "start": "646800",
    "end": "652959"
  },
  {
    "text": "or we get different kinds of scaling um and I think one way of thinking about this is if you're measuring kind of",
    "start": "652959",
    "end": "659440"
  },
  {
    "text": "training loss or like you know held out versions of training loss then I think scaling is very natural right like all",
    "start": "659440",
    "end": "665200"
  },
  {
    "text": "of classical statistical theory says you know things should converge and when they converge eventually they will get",
    "start": "665200",
    "end": "670720"
  },
  {
    "text": "better right at some sort of very asmtoic sense um but we do see non-scaling behavior um there was a",
    "start": "670720",
    "end": "677519"
  },
  {
    "text": "really interesting competition a few years back um called like the inverse scaling prize where they were looking",
    "start": "677519",
    "end": "684079"
  },
  {
    "text": "for things that like scale inversely as models got better um and a lot of these are are very niche things like you know",
    "start": "684079",
    "end": "690640"
  },
  {
    "text": "models tend to copy better and so if you want to like suppress copying behavior becomes really hard for really strong",
    "start": "690640",
    "end": "695920"
  },
  {
    "text": "models for example um but I think one sort of like thing that ties a lot of that together is you know if you go",
    "start": "695920",
    "end": "702560"
  },
  {
    "text": "really far out of distribution where the behavior is not well specified by the data then you can get all sorts of",
    "start": "702560",
    "end": "707920"
  },
  {
    "text": "behaviors like no scaling at all or inverse scaling or what have you right so in some sense you can think of this as like the extension of the classic",
    "start": "707920",
    "end": "714000"
  },
  {
    "text": "like deep learning robustness problems cool okay so now I'm going to",
    "start": "714000",
    "end": "721760"
  },
  {
    "text": "talk about um the scaling behaviors of LLM like just essentially going through",
    "start": "721760",
    "end": "726880"
  },
  {
    "text": "several kinds of empirical results i'm going to walk you through um data scaling in particular and some examples",
    "start": "726880",
    "end": "733040"
  },
  {
    "text": "just to convince you that this is a very natural object to expect and then we'll talk about model size which is a a you",
    "start": "733040",
    "end": "739279"
  },
  {
    "text": "know different kind of a thing um so um scaling laws I think are fairly",
    "start": "739279",
    "end": "747519"
  },
  {
    "text": "well established and they seem to appear very very often in kind of many variables right you see scaling in",
    "start": "747519",
    "end": "754399"
  },
  {
    "text": "compute um on the x-axis these are all taken from um Kaplan's scaling law paper which I'll refer to extensively in this",
    "start": "754399",
    "end": "760079"
  },
  {
    "text": "lecture so the x-axis here is is log compute y-axis here is log test loss um",
    "start": "760079",
    "end": "765440"
  },
  {
    "text": "and on the right you see similar kinds of scaling both for data set size so this is the amount of data uh in",
    "start": "765440",
    "end": "770639"
  },
  {
    "text": "parameters one subtlety I'll mention here as I as I sort of talk through this is you know when we scale things like",
    "start": "770639",
    "end": "776639"
  },
  {
    "text": "data set size or parameters we're always assuming that the other variable in this case if you're scaling data set size the",
    "start": "776639",
    "end": "782639"
  },
  {
    "text": "model size is much much much bigger than you can saturate with the data set size right because obviously if you have way",
    "start": "782639",
    "end": "788079"
  },
  {
    "text": "more data than you know parameters eventually you're going to sort of asmtote out right so in all of these we're trying to avoid the asmtoic regime",
    "start": "788079",
    "end": "795519"
  },
  {
    "text": "um they hold in also pretty non-standard settings they'll hold for for downstream tasks they'll hold out of distribution",
    "start": "795519",
    "end": "801760"
  },
  {
    "text": "which is what's being shown here from the Kaplan paper um and so you know in",
    "start": "801760",
    "end": "806880"
  },
  {
    "text": "some ways power law relationships seem to appear more often than we might",
    "start": "806880",
    "end": "812000"
  },
  {
    "text": "initially expect especially for these OOD or other variables so I want to talk through data",
    "start": "812000",
    "end": "818720"
  },
  {
    "text": "scaling laws first because I think they're the most intuitive like at the very least I think the theory for that is fairly clear um and so to be precise",
    "start": "818720",
    "end": "826639"
  },
  {
    "text": "when I say something like data scaling what I mean is just some sort of simple formula that maps data set size which",
    "start": "826639",
    "end": "833760"
  },
  {
    "text": "I'm going to refer to as n to um our excess error right excess error is the",
    "start": "833760",
    "end": "838800"
  },
  {
    "text": "error beyond the irreducible regime and you know if you recall um that figure I",
    "start": "838800",
    "end": "844720"
  },
  {
    "text": "referred to in Hess um what we are going to expect is monotonic logistic looking curves and really our interest is",
    "start": "844720",
    "end": "851920"
  },
  {
    "text": "primarily going to be in the power law region to the irreducible error region like of course it's very interesting to",
    "start": "851920",
    "end": "857440"
  },
  {
    "text": "also ask questions about what happens in the small data regions as we leave random guessing um but that's much much",
    "start": "857440",
    "end": "863440"
  },
  {
    "text": "harder to reason about whereas I think this right tail actually I can hopefully convince you that this part is actually",
    "start": "863440",
    "end": "870079"
  },
  {
    "text": "a very very natural thing to expect power loss scaling",
    "start": "870079",
    "end": "875519"
  },
  {
    "text": "so okay right so the first empirical observation that we have right and this",
    "start": "875519",
    "end": "880720"
  },
  {
    "text": "is kind of the thing that I'm going to convince you is natural is when we plot on the x-axis data set size and on the",
    "start": "880720",
    "end": "887120"
  },
  {
    "text": "y-axis test loss then on the log plot model performance is linear right um you",
    "start": "887120",
    "end": "893920"
  },
  {
    "text": "might call this scale free or you might call it power law these are more sort of physics um physics uh oriented",
    "start": "893920",
    "end": "901040"
  },
  {
    "text": "terminology um and sort of this was established you know by by many people",
    "start": "901040",
    "end": "906160"
  },
  {
    "text": "um but you might refer to to Kaplan to see many examples of this so I think you",
    "start": "906160",
    "end": "913279"
  },
  {
    "text": "know as sort of the previous question sort of brought up right we kind of expect error to be monotone we train on more data error goes down fairly obvious",
    "start": "913279",
    "end": "921199"
  },
  {
    "text": "the part that is less obvious is the precise functional form of this scaling right so when I say it's a power law",
    "start": "921199",
    "end": "927279"
  },
  {
    "text": "it's it's linear in log log space um and then so what is the implication of that right if something is linear in log log",
    "start": "927279",
    "end": "934160"
  },
  {
    "text": "that means that there's a polinomial relationship between your x-axis and your y-axis right um and why Is",
    "start": "934160",
    "end": "941199"
  },
  {
    "text": "polinomial decay natural well I'm going to walk you through two examples and both of those are going to result in",
    "start": "941199",
    "end": "946720"
  },
  {
    "text": "some fairly natural polinomial decay i'm going to start with the simplest possible example right like this is just",
    "start": "946720",
    "end": "953120"
  },
  {
    "text": "going to be you know even stats 101 rather than you know machine learning 101 so what I want to do is I want to",
    "start": "953120",
    "end": "960000"
  },
  {
    "text": "estimate the mean of a data set right and estimating the mean is a task of estimating a parameter right i can ask",
    "start": "960000",
    "end": "967199"
  },
  {
    "text": "for what's the scaling law what's the error of my mean estimation task as a function of data right so I can write",
    "start": "967199",
    "end": "972560"
  },
  {
    "text": "that down well you know my input comes from a gausian and the task is to estimate the average i've written those",
    "start": "972560",
    "end": "977839"
  },
  {
    "text": "out in the blue box above and what's the error well by sort of very standard arguments right the average is going to",
    "start": "977839",
    "end": "984480"
  },
  {
    "text": "be also distributed as a gausian with the standard deviation divided by n so I'm going to get you know sigma squared over n is my estimation error right this",
    "start": "984480",
    "end": "991440"
  },
  {
    "text": "is the expected squared error of my estimate and if you look at this this is",
    "start": "991440",
    "end": "996480"
  },
  {
    "text": "polinomial in n and just to really drive the point home you know you take the log of both sides of this log of the error",
    "start": "996480",
    "end": "1002000"
  },
  {
    "text": "on the left and log of sort of um of uh n on the right hand side you know I get",
    "start": "1002000",
    "end": "1007839"
  },
  {
    "text": "exactly log of error is equal to negative log n plus 2 log sigma right um so this is exactly the kind of thing we",
    "start": "1007839",
    "end": "1014079"
  },
  {
    "text": "expect and we expect a slope of one if we were to fit a scaling law for mean estimation",
    "start": "1014079",
    "end": "1020959"
  },
  {
    "text": "so now you know equipped with this this new knowledge you might say all right I'm going to go around and I'm going to",
    "start": "1020959",
    "end": "1026959"
  },
  {
    "text": "look at what the rates are for estimating different things and that will tell me about what I should expect for data scaling and so you might say oh",
    "start": "1026959",
    "end": "1033918"
  },
  {
    "text": "what I expect is one overn you might expect one over square root of n for agnostic learning um and so on and so",
    "start": "1033919",
    "end": "1039760"
  },
  {
    "text": "forth so we should expect to see some like pretty nice round numbers on the slope here right of a log plot I should",
    "start": "1039760",
    "end": "1045199"
  },
  {
    "text": "expect to see like one or five um what do we actually find empirically when we look across these papers right um just",
    "start": "1045199",
    "end": "1052480"
  },
  {
    "text": "to sort of call them out in hessness for machine translation we see 0.13 for",
    "start": "1052480",
    "end": "1058559"
  },
  {
    "text": "speech we see 0.3 and for language modeling um we see an exponent",
    "start": "1058559",
    "end": "1065559"
  },
  {
    "text": "of095 right those are all much much slower than the 1 overn or one over",
    "start": "1065559",
    "end": "1071440"
  },
  {
    "text": "square root of n rates that you might expect when you're just fitting simple functions so why might this be okay This",
    "start": "1071440",
    "end": "1079360"
  },
  {
    "text": "will be the last math slide of this this lecture and then we can go to just fitting lines on log lock log plots the rest of the time but um this will",
    "start": "1079360",
    "end": "1086240"
  },
  {
    "text": "hopefully drive the point home of why we might see these particular slopes so we",
    "start": "1086240",
    "end": "1091440"
  },
  {
    "text": "know that neural nets aren't just estimating the mean right or it's not even fitting a linear regression right they can fit arbitrary functions right",
    "start": "1091440",
    "end": "1098160"
  },
  {
    "text": "so let's turn that into an example and let's work through that example so um my input is um x1 through xn i have n",
    "start": "1098160",
    "end": "1105120"
  },
  {
    "text": "samples and I'm going to put them uniformly in the 2D unit box um and I want to estimate some random not random",
    "start": "1105120",
    "end": "1111120"
  },
  {
    "text": "some arbitrary regression function y equals f right and I'll assume f is smooth and so on um if you really want",
    "start": "1111120",
    "end": "1116559"
  },
  {
    "text": "to be precise right there's some regularity conditions here um a simple approach to to estimating a regression",
    "start": "1116559",
    "end": "1123600"
  },
  {
    "text": "function f is just to cut the 2D space up into small boxes and within each box",
    "start": "1123600",
    "end": "1128640"
  },
  {
    "text": "I can measure the average of the y values right like a very simple nonparametric regressor is to just cut",
    "start": "1128640",
    "end": "1134320"
  },
  {
    "text": "the space up and then to estimate what's going to happen now informally If we pick you know I'm going to have square",
    "start": "1134320",
    "end": "1140160"
  },
  {
    "text": "root m boxes now each box is going to get square root of n samples and now my error is going to be 1 over of n",
    "start": "1140160",
    "end": "1146720"
  },
  {
    "text": "and if you sort of follow this logic through the more dimensions you'll see that in dimensions this is going to be",
    "start": "1146720",
    "end": "1152240"
  },
  {
    "text": "error is equal to n^ 1 /d and then sort of my overall scaling if I were to take",
    "start": "1152240",
    "end": "1157360"
  },
  {
    "text": "log log plots of the whole thing is I expect the slope of negative 1 /d right",
    "start": "1157360",
    "end": "1162400"
  },
  {
    "text": "um and so why did I walk you through this example right i walked you through this example because if you have",
    "start": "1162400",
    "end": "1167520"
  },
  {
    "text": "flexible function classes what people call nonparametric function classes you expect dimension dependence and",
    "start": "1167520",
    "end": "1173039"
  },
  {
    "text": "therefore the slope of the uh scaling law to actually move sort of much more slowly and in some sense the slope is",
    "start": "1173039",
    "end": "1179520"
  },
  {
    "text": "telling you almost precisely um kind of the intrinsic dimensionality or the ease",
    "start": "1179520",
    "end": "1184720"
  },
  {
    "text": "of learning this task um and people have argued this more formally or or sort of more literally um there's been several",
    "start": "1184720",
    "end": "1191919"
  },
  {
    "text": "sort of theory/impirical papers arguing that really the reason why we get these sort of exotic or non-standard rates of",
    "start": "1191919",
    "end": "1198880"
  },
  {
    "text": "learning is that it is closely connected to the the intrinsic dimensionality of the data um and the sort of for example",
    "start": "1198880",
    "end": "1204720"
  },
  {
    "text": "the plots of these predictions the dash lines and these these purple circles um are somewhat close although um you know",
    "start": "1204720",
    "end": "1211760"
  },
  {
    "text": "you don't want to read too much into this because estimation of intrinsic dimension is is an extremely difficult problem and as difficult as modeling uh",
    "start": "1211760",
    "end": "1218960"
  },
  {
    "text": "the data uh overall okay oh yes yeah i mean I guess this is related to the",
    "start": "1218960",
    "end": "1224720"
  },
  {
    "text": "point you made at the end it's much more but like yeah how do you how do you generate data that has an underlying",
    "start": "1224720",
    "end": "1232480"
  },
  {
    "text": "intrinsic dimension at all from a simulation perspective yeah so so uh the",
    "start": "1232480",
    "end": "1237919"
  },
  {
    "text": "results here well if you want for example to generate data that's actually not too hard you could like write down a",
    "start": "1237919",
    "end": "1244080"
  },
  {
    "text": "function that takes in like five variables right and then that would be a as long as all five of those variables",
    "start": "1244080",
    "end": "1249679"
  },
  {
    "text": "like don't you know cancel each other that's a fivedimensional surface and you can add a little bit of noise and you're good to go um the difficulty here is",
    "start": "1249679",
    "end": "1256799"
  },
  {
    "text": "that they're actually doing things like you know training on CFR and then they're having you know different uh",
    "start": "1256799",
    "end": "1262320"
  },
  {
    "text": "they're trying to estimate the intrinsic dimensionality of CRA that's a you know much harder",
    "start": "1262320",
    "end": "1267840"
  },
  {
    "text": "task okay um and data scaling laws are quite useful um you know I was going at",
    "start": "1268520",
    "end": "1275360"
  },
  {
    "text": "this from a let me explain to you scaling laws perspective but you can actually use scaling laws to do many",
    "start": "1275360",
    "end": "1282159"
  },
  {
    "text": "interesting things right you can make engineering decisions of various kinds using data scaling laws and people do um",
    "start": "1282159",
    "end": "1288080"
  },
  {
    "text": "in fact do this um for example you know you might say well how does data set",
    "start": "1288080",
    "end": "1294159"
  },
  {
    "text": "composition affect performance not just data set size well um if you're changing the test set you know uh Kaplan at all",
    "start": "1294159",
    "end": "1302159"
  },
  {
    "text": "has a really nice figure showing actually data composition only affects the offset not the slope and what that",
    "start": "1302159",
    "end": "1307760"
  },
  {
    "text": "would mean is it says if you want to pick a really good uh data set you don't have to necessarily train your models at",
    "start": "1307760",
    "end": "1313760"
  },
  {
    "text": "a huge scale you can scale them down and do your data selection experiments on much smaller models um and the shape of",
    "start": "1313760",
    "end": "1321760"
  },
  {
    "text": "the expected uh sort of as we mix sort of different data we might expect certain kinds of sort of shapes and you",
    "start": "1321760",
    "end": "1327520"
  },
  {
    "text": "can use regression and other kinds of techniques to try to figure out for example optimal data mixing using",
    "start": "1327520",
    "end": "1332799"
  },
  {
    "text": "scaling laws and people have written several papers um on this topic although you know as with all data selection sort",
    "start": "1332799",
    "end": "1339120"
  },
  {
    "text": "of uh research a lot of this seems fairly tricky to execute reliably",
    "start": "1339120",
    "end": "1346720"
  },
  {
    "text": "there's other also interesting questions that you might ask right there's a lot of discussion these days about you know",
    "start": "1346720",
    "end": "1351760"
  },
  {
    "text": "are we running out of data right on the internet and so once you start asking those questions the other interesting",
    "start": "1351760",
    "end": "1357280"
  },
  {
    "text": "and important question is well can we just keep training on the same data we have what's the diminishing returns",
    "start": "1357280",
    "end": "1362880"
  },
  {
    "text": "property of that right and so um there's interesting work extending scaling laws to multi-epo training um basically",
    "start": "1362880",
    "end": "1370640"
  },
  {
    "text": "arguing that there's an sort of effective sample size and after about four epochs you know you have rapid ly",
    "start": "1370640",
    "end": "1376559"
  },
  {
    "text": "diminishing returns as you repeat more and more data and by modifying sort of",
    "start": "1376559",
    "end": "1381600"
  },
  {
    "text": "um the usual scaling law you can basically get a version where you have amount of effective data and unique",
    "start": "1381600",
    "end": "1389200"
  },
  {
    "text": "tokens that sort of diminish out as you increase the amount of",
    "start": "1389200",
    "end": "1394320"
  },
  {
    "text": "repetition finally I think one interesting sort of combination of these two ideas is if you're thinking about",
    "start": "1394760",
    "end": "1401039"
  },
  {
    "text": "sort of data selection in the large data regime right like imagine you're going to be training on trillions and",
    "start": "1401039",
    "end": "1406960"
  },
  {
    "text": "trillions of tokens right now what would be better would it be better to repeat highquality sources like you know",
    "start": "1406960",
    "end": "1413679"
  },
  {
    "text": "Wikipedia and perhaps your secret pirated books 10 times or would it be better to include new data right um the",
    "start": "1413679",
    "end": "1420559"
  },
  {
    "text": "fact that you can either repeat data or you can include more data right now has multiple sort of axes on which you can",
    "start": "1420559",
    "end": "1427039"
  },
  {
    "text": "sort of optimize your data mixture um and there's also been some interesting data scaling work um this one from CMU",
    "start": "1427039",
    "end": "1433520"
  },
  {
    "text": "folks um on essentially trading off between repeating data versus picking lower quality data that's new right and",
    "start": "1433520",
    "end": "1441360"
  },
  {
    "text": "so all of this really is a is a really natural extension of what I sort of already taught you which is if you",
    "start": "1441360",
    "end": "1448320"
  },
  {
    "text": "assume that there's a predictive power law relationship right and that this power law relationship holds sort of on",
    "start": "1448320",
    "end": "1454320"
  },
  {
    "text": "a per mixture basis then you can fit these sort of scaling law extrapolations and then get an estimate of how good",
    "start": "1454320",
    "end": "1460159"
  },
  {
    "text": "your data is going to be at scale right so that's the starting point um",
    "start": "1460159",
    "end": "1466640"
  },
  {
    "text": "which is data scaling right and hopefully I've convinced you at this point both sort of empirically and",
    "start": "1466640",
    "end": "1471760"
  },
  {
    "text": "conceptually that it's natural to have you know log log linear relationships",
    "start": "1471760",
    "end": "1477279"
  },
  {
    "text": "between data and error um this relationship seems to hold very robustly",
    "start": "1477279",
    "end": "1482320"
  },
  {
    "text": "across domains across different kinds of models um and you can kind of have a nice clean theoretical understanding um",
    "start": "1482320",
    "end": "1489120"
  },
  {
    "text": "of what is happening here um and once you do this you can use this for all sorts of purposes like picking optimal",
    "start": "1489120",
    "end": "1495440"
  },
  {
    "text": "data mixtures um or whatever else okay yes how was the model size picked on the",
    "start": "1495440",
    "end": "1502159"
  },
  {
    "text": "data scaling box yeah so um as I was kind of saying back",
    "start": "1502159",
    "end": "1507480"
  },
  {
    "text": "in well not this slide but let's see back in this slide um when we think",
    "start": "1507480",
    "end": "1514400"
  },
  {
    "text": "about kind of the data size scaling the the model is always picked to be really really large so the data is not",
    "start": "1514400",
    "end": "1520799"
  },
  {
    "text": "saturating your model right um and you want to kind of avoid being in this",
    "start": "1520799",
    "end": "1526240"
  },
  {
    "text": "irreducible error regime so the model is always picked to be large enough that you're in the power law region whenever",
    "start": "1526240",
    "end": "1532240"
  },
  {
    "text": "you're only varying data so is it just one model size for like all of them that",
    "start": "1532240",
    "end": "1537760"
  },
  {
    "text": "one really really big model size or like is each point like a different size model yeah for for for example for this",
    "start": "1537760",
    "end": "1543600"
  },
  {
    "text": "plot in particular it's for it's like one big model size okay when you're looking at for example compute scaling",
    "start": "1543600",
    "end": "1549039"
  },
  {
    "text": "on this axis then data and model scale jointly at some like you know pre-ordained",
    "start": "1549039",
    "end": "1555480"
  },
  {
    "text": "ratio cool any other questions good okay",
    "start": "1555480",
    "end": "1561240"
  },
  {
    "text": "excellent um all right so now I think we get to move from",
    "start": "1561240",
    "end": "1569120"
  },
  {
    "text": "data scaling to in my opinion slightly more mysterious kinds of scaling um and",
    "start": "1569120",
    "end": "1575919"
  },
  {
    "text": "we're going to talk about model scaling next um and I think this is a more practical engineering set of questions",
    "start": "1575919",
    "end": "1582159"
  },
  {
    "text": "that we're now going to try to answer so you're in charge of you know building and shipping a really large language",
    "start": "1582159",
    "end": "1588240"
  },
  {
    "text": "model and there's a lot of interesting ideas out there right like you could train the latest you know state space",
    "start": "1588240",
    "end": "1594000"
  },
  {
    "text": "model you could train a transformer you could use atom you could use SGD right people invent all sorts of new tricks",
    "start": "1594000",
    "end": "1599520"
  },
  {
    "text": "which ones are worth scaling up and which ones are not um you could also take you know uh your limited compute",
    "start": "1599520",
    "end": "1606080"
  },
  {
    "text": "resources and spend them on different things you can train models for longer or you could train bigger models right",
    "start": "1606080",
    "end": "1611360"
  },
  {
    "text": "for given flop you can trade between these do um and you could also do things like go and collect more data versus get",
    "start": "1611360",
    "end": "1617679"
  },
  {
    "text": "more GPS there's a lot of different sort of things that you can do and the scaling laws allow you to have a pretty",
    "start": "1617679",
    "end": "1623120"
  },
  {
    "text": "simple procedure to just answer all these questions right so I'll go through um the classic uh sort of Kaplan scaling",
    "start": "1623120",
    "end": "1630240"
  },
  {
    "text": "law paper if you're interested in these topics I encourage you to read it it's just kind of a gold mine um of all these",
    "start": "1630240",
    "end": "1636000"
  },
  {
    "text": "kinds of observations some of it is old but it's um I think still unmatched in the thoroughess of all the things that",
    "start": "1636000",
    "end": "1642799"
  },
  {
    "text": "it really studied um in a fairly nice unified setting so architecture- wise um",
    "start": "1642799",
    "end": "1649279"
  },
  {
    "text": "you might start by asking like transformers versus LSTMs right which one's better well you know the brute",
    "start": "1649279",
    "end": "1654640"
  },
  {
    "text": "force way might be to you know scale up LSTMs and up to like GPT3 level and then",
    "start": "1654640",
    "end": "1659679"
  },
  {
    "text": "you know you can figure out whether it's good or not um the scaling law way is much simpler right you basically train a",
    "start": "1659679",
    "end": "1665520"
  },
  {
    "text": "bunch of LSTMs and transformers across many different compute thresholds or compute levels and then you kind of see",
    "start": "1665520",
    "end": "1671520"
  },
  {
    "text": "what happens as you scale them up and I think the trends here are fairly clear right like no matter how many layers you",
    "start": "1671520",
    "end": "1677200"
  },
  {
    "text": "have on your LSTMs there's a pretty big gap right pretty big constant factor gap between transformers and LSTMs right and",
    "start": "1677200",
    "end": "1684480"
  },
  {
    "text": "remember this is in log scale so this is kind of saying something like you know I don't know what the exact numbers are but imagine this is like 15 times less",
    "start": "1684480",
    "end": "1691440"
  },
  {
    "text": "efficient right that no matter where you are on this plot you know the LSTM is let's say 15 times less compute",
    "start": "1691440",
    "end": "1696960"
  },
  {
    "text": "efficient than a transformer right so that there's a constant factor compute penalty to using um LSTMs um at least in",
    "start": "1696960",
    "end": "1704080"
  },
  {
    "text": "this plot you know you could you could zoom out and say well there's a lot more architectures you know which ones are",
    "start": "1704080",
    "end": "1711120"
  },
  {
    "text": "you know really good and worth doing um and sort of some of the classic papers this one is by uh ET and others uh at",
    "start": "1711120",
    "end": "1718080"
  },
  {
    "text": "Google um have done exactly this kind of scaling work where they took a bunch of architectures on the right here um and",
    "start": "1718080",
    "end": "1725279"
  },
  {
    "text": "they basically scaled them up so the x-axis is the amount of compute the red line is basically each architecture and",
    "start": "1725279",
    "end": "1732080"
  },
  {
    "text": "the green line is the transformer baseline right and they ask like oh can any of these alternative architectures",
    "start": "1732080",
    "end": "1737919"
  },
  {
    "text": "match or out you know outscale uh the transformer right um and what what do",
    "start": "1737919",
    "end": "1744000"
  },
  {
    "text": "they uh end up well actually the only thing that seems to like really strongly and reliably beat the transformer is you",
    "start": "1744000",
    "end": "1749440"
  },
  {
    "text": "know gated linear units um and mixture of experts and once you know it that's exactly the kind of stuff that people",
    "start": "1749440",
    "end": "1755600"
  },
  {
    "text": "are doing today right and so this is kind of the scaling law version of that same idea of saying like how would you",
    "start": "1755600",
    "end": "1761200"
  },
  {
    "text": "have come to the conclusion that we should be doing switch transformers and GLU and and for example not the",
    "start": "1761200",
    "end": "1766640"
  },
  {
    "text": "performer right okay and the scaling law provides some some clear evidence of why you might want to do",
    "start": "1766640",
    "end": "1774080"
  },
  {
    "text": "that optimizer choice I think follows a similar thing um this one's from Hessnus",
    "start": "1774120",
    "end": "1779440"
  },
  {
    "text": "um you know they compare SGD and Atom they find very similar to before this kind of constant factor gap right in",
    "start": "1779440",
    "end": "1785679"
  },
  {
    "text": "compute um in this case data set size but of course that translates to compute um in in the effectiveness of atom",
    "start": "1785679",
    "end": "1792720"
  },
  {
    "text": "versus SGD right um you know RHN in this in this case is recurrent highway nets you can sort of ignore the details here",
    "start": "1792720",
    "end": "1799279"
  },
  {
    "text": "um you kind of see the the point of how you would do this analysis rather than the specific results um that are shown",
    "start": "1799279",
    "end": "1806559"
  },
  {
    "text": "here you know in the beginning I also said something like oh you know depth versus width like what what should the",
    "start": "1807159",
    "end": "1812880"
  },
  {
    "text": "aspect ratios be that was one of the hyperparameter topics we talked out um and we see sort of similar sort of",
    "start": "1812880",
    "end": "1819360"
  },
  {
    "text": "analysis but in scaling law form from Kaplan i think this one's intriguing to me at least because you know we might",
    "start": "1819360",
    "end": "1825600"
  },
  {
    "text": "think that deeper layers get dramatically better right that there's like clear separation between the number of layers but we see at least here that",
    "start": "1825600",
    "end": "1833120"
  },
  {
    "text": "you know there's actually a lot of sort of slop one layer is really bad but a lot of the other sort of layer choices",
    "start": "1833120",
    "end": "1839279"
  },
  {
    "text": "sort of remain pretty stable um and hopefully this is reminiscent of kind of that slide I showed back in the",
    "start": "1839279",
    "end": "1845039"
  },
  {
    "text": "architecture lecture where I said well you know the aspect ratio the ratio of width to depth you know roughly um",
    "start": "1845039",
    "end": "1851279"
  },
  {
    "text": "something like 4 to 16 or something was a pretty natural number but there's a really wide basin in which you're",
    "start": "1851279",
    "end": "1857360"
  },
  {
    "text": "approximately optimal and the scaling law analysis also backs that up one",
    "start": "1857360",
    "end": "1862399"
  },
  {
    "text": "important subtlety um that I do want to point out um and this one bites people every now and then is that not all",
    "start": "1862399",
    "end": "1868720"
  },
  {
    "text": "parameters are equal like often you want to do you know parameter scaling analyses um but if you were to say count",
    "start": "1868720",
    "end": "1876399"
  },
  {
    "text": "embedding parameters as part of your model well you get like a pretty different scaling law you get this you",
    "start": "1876399",
    "end": "1882399"
  },
  {
    "text": "know kind of weird looking thing that like slightly bends over here um whereas if you only consider the non-mbedding",
    "start": "1882399",
    "end": "1888000"
  },
  {
    "text": "parameter you see that much cleaner result that I showed you before right so embedding layer parameters don't really",
    "start": "1888000",
    "end": "1894159"
  },
  {
    "text": "behave the same and they don't show the same kinds of um sort of log linear scaling um as the non-mbbedding",
    "start": "1894159",
    "end": "1901600"
  },
  {
    "text": "parameters uh when you account for them um and there's sort of related work on saying like not all parameters are the",
    "start": "1901600",
    "end": "1907600"
  },
  {
    "text": "same um on recent papers on scaling uh mixtures of experts where they're also sort of trying to figure out like what",
    "start": "1907600",
    "end": "1913760"
  },
  {
    "text": "does it mean to be a parameter when you have such sparsely activated parameters and in those kinds of papers they sort",
    "start": "1913760",
    "end": "1919279"
  },
  {
    "text": "of try to derive essentially things like equivalent number of dense parameters in order to sort of try to normalize um the",
    "start": "1919279",
    "end": "1926080"
  },
  {
    "text": "the number of parameters ine right",
    "start": "1926080",
    "end": "1931080"
  },
  {
    "text": "um I've showed you this plot um earlier in the hyperparameter selection but hopefully now actually you see the full",
    "start": "1932080",
    "end": "1937440"
  },
  {
    "text": "context not just the original sort of the hyperparameter choice question um we",
    "start": "1937440",
    "end": "1943679"
  },
  {
    "text": "know that in many cases um I'll go back let's say uh to like here often what",
    "start": "1943679",
    "end": "1949360"
  },
  {
    "text": "we'll see is scaling lock curves that look like the following you'll often see that the slope of the curves remain very",
    "start": "1949360",
    "end": "1955840"
  },
  {
    "text": "similar they're non-crossing and that there's sort of constant factor offsets between these curves and whenever this",
    "start": "1955840",
    "end": "1962159"
  },
  {
    "text": "is true what you can then do is you can take a slice at a particular level of compute or a particular set of",
    "start": "1962159",
    "end": "1967760"
  },
  {
    "text": "hyperparameters and analyze the hyperparameter trade-offs very carefully assuming or and sort of be sort of safe",
    "start": "1967760",
    "end": "1974320"
  },
  {
    "text": "in sort of scaling that up and so when you go to Kaplan's paper you'll see exactly these kinds of analyses being",
    "start": "1974320",
    "end": "1981039"
  },
  {
    "text": "done um especially I think that the center one the aspect ratio plot is definitely worth looking at you know",
    "start": "1981039",
    "end": "1987279"
  },
  {
    "text": "they're not just sort of scaling up and down models they're actually taking different slices so different sized models 50 million 270 million 1.5",
    "start": "1987279",
    "end": "1995120"
  },
  {
    "text": "billion and they're looking at how the aspect ratio changes the loss and they kind of see that oh actually the shape",
    "start": "1995120",
    "end": "2001360"
  },
  {
    "text": "of the curve not just the scaling slopes actually remain similar and this means",
    "start": "2001360",
    "end": "2006480"
  },
  {
    "text": "that you know I can pick an aspect ratio between 10 to 100 and any anything in",
    "start": "2006480",
    "end": "2011600"
  },
  {
    "text": "between will work fine at all of these different scales right and so this is um I think important to think about i think",
    "start": "2011600",
    "end": "2018720"
  },
  {
    "text": "initially when you're trained in sort of deep learning you know model training you think about hyperparameter tuning",
    "start": "2018720",
    "end": "2023919"
  },
  {
    "text": "but you want to be sort of scale aware in how you're tuning your hyperparameters and that's a really big difference in mindset I think um between",
    "start": "2023919",
    "end": "2030399"
  },
  {
    "text": "kind of the scaling law style approach and sort of maybe what you've been trained or what you've you know naturally think about in terms of oh",
    "start": "2030399",
    "end": "2036799"
  },
  {
    "text": "let's just tune these models at a small scale right and so um the same is being done kind of for feed forward dimen uh",
    "start": "2036799",
    "end": "2042799"
  },
  {
    "text": "ratio and for attention head dimension you know you're varying v various aspects of scale and you're trying to",
    "start": "2042799",
    "end": "2047919"
  },
  {
    "text": "see whether sort of the minima uh remain",
    "start": "2047919",
    "end": "2052638"
  },
  {
    "text": "similar okay um another important thing um next actually maybe not next lecture",
    "start": "2054679",
    "end": "2061118"
  },
  {
    "text": "but next next lecture I'm going to talk about um sort of practical case studies almost of how people have scaled up",
    "start": "2061119",
    "end": "2067358"
  },
  {
    "text": "models um and we'll actually see that batch size and learning rate are",
    "start": "2067359",
    "end": "2072878"
  },
  {
    "text": "actually two really tricky things that you have to deal with carefully when you scale models up right so when you scale",
    "start": "2072879",
    "end": "2078320"
  },
  {
    "text": "models up you're going to have to maybe think about you know the the optimal learning rate will be different across",
    "start": "2078320",
    "end": "2083440"
  },
  {
    "text": "model scales and if you're doing that then actually also maybe the optimal batch size might end up varying as well",
    "start": "2083440",
    "end": "2088800"
  },
  {
    "text": "right because those two are often co- um and so we need to think about what the right way of scaling batch size is and",
    "start": "2088800",
    "end": "2094878"
  },
  {
    "text": "how batch size interacts with scale and also learning rates i'll talk about those um for the next couple slides",
    "start": "2094879",
    "end": "2100960"
  },
  {
    "text": "right so batch slides from the systems lecture hopefully you remember it has diminishing returns past a certain point",
    "start": "2100960",
    "end": "2106640"
  },
  {
    "text": "so up until a certain point so you know when the batch size is uh smaller than",
    "start": "2106640",
    "end": "2111920"
  },
  {
    "text": "the noise scale we're on the left hand sides here right increasing batch size is almost equivalent to taking more",
    "start": "2111920",
    "end": "2118400"
  },
  {
    "text": "gradient steps so so that's you know roughly saying if I double my batch size it's as good as taking two gradient",
    "start": "2118400",
    "end": "2124880"
  },
  {
    "text": "steps and that's a really really good place to be right because now you've got the systems power of being able to",
    "start": "2124880",
    "end": "2130320"
  },
  {
    "text": "parallelize across the batch right while having the optimization efficiency of taking two steps but past a certain",
    "start": "2130320",
    "end": "2136640"
  },
  {
    "text": "point you're going to have ineffective scaling right where now your sort of noise scale and your batch size are the",
    "start": "2136640",
    "end": "2142160"
  },
  {
    "text": "same and the additional uh samples in your batch that you're taking you know they're not reducing useful noise it's",
    "start": "2142160",
    "end": "2149040"
  },
  {
    "text": "getting dominated by kind of the curvature of um the bias term so to speak of the curvature of your um",
    "start": "2149040",
    "end": "2155599"
  },
  {
    "text": "optimization landscape and one really useful thing to to think about useful sort of analysis object um is this",
    "start": "2155599",
    "end": "2163520"
  },
  {
    "text": "notion of a critical batch size and the critical batch size you can think of as is kind of this threshold point where we",
    "start": "2163520",
    "end": "2169280"
  },
  {
    "text": "go from perfect scaling to uh dip strong diminishing returns right um and you can",
    "start": "2169280",
    "end": "2174640"
  },
  {
    "text": "sort of analyze this in theory and the in sort of open AI papers on critical batch sizes do this but you could also",
    "start": "2174640",
    "end": "2180720"
  },
  {
    "text": "analyze this uh empirically and this is another thing that's been studied sort",
    "start": "2180720",
    "end": "2185760"
  },
  {
    "text": "of you know in the scaling law kind of of way um you can kind of see um you can",
    "start": "2185760",
    "end": "2191520"
  },
  {
    "text": "estimate the point at which sort of uh progress slow so you can estimate empirically what the critical batch size",
    "start": "2191520",
    "end": "2197200"
  },
  {
    "text": "point trade-off points are and you can also basically train bigger and better",
    "start": "2197200",
    "end": "2202400"
  },
  {
    "text": "models and one really interesting thing is as you try to you know improve the loss so you you're going left side here",
    "start": "2202400",
    "end": "2209359"
  },
  {
    "text": "so you're making losses better and better and better and better and better um your critical batch size um ends up",
    "start": "2209359",
    "end": "2214640"
  },
  {
    "text": "getting smaller right so the the smaller the loss target the bigger uh the overall batch size um that you can be um",
    "start": "2214640",
    "end": "2223280"
  },
  {
    "text": "and so one of the things that this leads to is for example if you look at the llama 3 um training report you'll",
    "start": "2223280",
    "end": "2229359"
  },
  {
    "text": "actually see for example that they'll like increase the batch size after a certain point or they'll do things like increase the batch size um as they train",
    "start": "2229359",
    "end": "2236079"
  },
  {
    "text": "because as your loss target gets smaller um your batch sizes um can in turn get",
    "start": "2236079",
    "end": "2243160"
  },
  {
    "text": "bigger so um as we increase both compute and model size like what's the right thing to do once again we can do kind of",
    "start": "2243160",
    "end": "2250000"
  },
  {
    "text": "a scaling analysis this is from Kaplan um and you can try to figure out you know as we increase the amount of",
    "start": "2250000",
    "end": "2256000"
  },
  {
    "text": "compute what is um the optimal batch size and what we kind of see is that you",
    "start": "2256000",
    "end": "2261520"
  },
  {
    "text": "know um as we increase the amount of compute um we can actually have",
    "start": "2261520",
    "end": "2266880"
  },
  {
    "text": "reasonable sort of parallelism the number of total steps can stay the same um at at least within this compute",
    "start": "2266880",
    "end": "2272240"
  },
  {
    "text": "threshold the number of total steps can stay the same um while sort of get getting the batches bigger and bigger",
    "start": "2272240",
    "end": "2277520"
  },
  {
    "text": "and bigger and if you fix the amount of batches of course the number of steps is going to go up and up and up so this is good news hopefully for for data",
    "start": "2277520",
    "end": "2284000"
  },
  {
    "text": "parallel processing so that's the batch size story um the thing you can you should maybe remember um because I think",
    "start": "2284000",
    "end": "2290480"
  },
  {
    "text": "critical batch sizes are kind of a messy concept is that a there's a a sort of diminishing returns point the critical",
    "start": "2290480",
    "end": "2296800"
  },
  {
    "text": "batch size that's one thing the second one is that it does seem to follow a pretty predictable scaling often as a",
    "start": "2296800",
    "end": "2302320"
  },
  {
    "text": "function of your target loss um and given that you can figure out you know what is the right tradeoffs that I can",
    "start": "2302320",
    "end": "2308400"
  },
  {
    "text": "make um in terms of systems efficiency and my optimization",
    "start": "2308400",
    "end": "2314040"
  },
  {
    "text": "progress as I said before um the other aspect of this right is you know you've",
    "start": "2314040",
    "end": "2319839"
  },
  {
    "text": "got your batch size and then you've got your learning rate and those two are fairly closely linked with each other um",
    "start": "2319839",
    "end": "2325520"
  },
  {
    "text": "and I'm going to talk about MUP at much more extensive length um in the next part of the scaling lecture um but this",
    "start": "2325520",
    "end": "2333040"
  },
  {
    "text": "is kind of a really important I think broader idea so you could do one of two things and I think this figure will",
    "start": "2333040",
    "end": "2338160"
  },
  {
    "text": "allow me to talk about both of these so let's look at this left plot first um what's what's labeled standard practice",
    "start": "2338160",
    "end": "2344079"
  },
  {
    "text": "right so when you train a transformer right what you're basically going to see is something like this left thing here this standard practice um so the optimal",
    "start": "2344079",
    "end": "2352320"
  },
  {
    "text": "learning rate is going to be at different points and the wider the model right as you increase your model size",
    "start": "2352320",
    "end": "2358079"
  },
  {
    "text": "and your your MLPS get wider and wider and wider the optimal learning rate is going to be pretty small and as you make",
    "start": "2358079",
    "end": "2364079"
  },
  {
    "text": "your your model smaller and smaller and smaller right your losses of course are going to go up because your model is less you know less expressive but also",
    "start": "2364079",
    "end": "2371119"
  },
  {
    "text": "the optimum learning rate is going to also go up right um and often you know",
    "start": "2371119",
    "end": "2376320"
  },
  {
    "text": "people say there's a rule of thumb it's like one over the width is the right rate at which you could scale the learning rate um more advanced people",
    "start": "2376320",
    "end": "2383839"
  },
  {
    "text": "will actually fit uh uh basically take these curves find the minimum and then fit a scaling law on the optimum",
    "start": "2383839",
    "end": "2390320"
  },
  {
    "text": "learning rate and so there we can see that this is a predictable decay in learning rate and maybe we can fit a scaling law i'll talk about this more in",
    "start": "2390320",
    "end": "2396720"
  },
  {
    "text": "the next set of lectures but an alternative one that I think many people have started to adopt",
    "start": "2396720",
    "end": "2402880"
  },
  {
    "text": "and I think is a is a really interesting thing to think about is that you can actually reparameterize the model um and",
    "start": "2402880",
    "end": "2410079"
  },
  {
    "text": "in particular you know you can do things like scale the initializa or scale the learning rates um of different layers",
    "start": "2410079",
    "end": "2417440"
  },
  {
    "text": "based on the width you can uh scale the variance of the initialization based on based on the width of the model um as",
    "start": "2417440",
    "end": "2424240"
  },
  {
    "text": "well as multiply the output in the forward paths of of different layers of the model um and if you do this in a way",
    "start": "2424240",
    "end": "2431760"
  },
  {
    "text": "that you know is dependent on sort of the width of the model um you end up with a parameterization of the model",
    "start": "2431760",
    "end": "2438880"
  },
  {
    "text": "whose learning rate is supposed to be more stable or at least you know in the original paper exactly stable across",
    "start": "2438880",
    "end": "2445920"
  },
  {
    "text": "scale right so you tune your learning rate once and you don't have to do anything else that optimum directly",
    "start": "2445920",
    "end": "2451359"
  },
  {
    "text": "transfer it's actually you tune it here on the smallest one and that directly transfers to the very largest scale",
    "start": "2451359",
    "end": "2457119"
  },
  {
    "text": "right and this is the the idea called new P um there have been sort of this",
    "start": "2457119",
    "end": "2462240"
  },
  {
    "text": "original paper that I'm showing you is called with new P there's been other variants um Meta with the release of",
    "start": "2462240",
    "end": "2467760"
  },
  {
    "text": "Llama 4 claims to have invented something called metap which I'm not quite sure what it is yet um but you can",
    "start": "2467760",
    "end": "2474319"
  },
  {
    "text": "sort of see that a lot of labs are kind of thinking about this right because if you're going to have to you know rely on",
    "start": "2474319",
    "end": "2480079"
  },
  {
    "text": "predicting what the optimum learning rate is then you have to do all sorts of tricky scaling law fits and maybe this",
    "start": "2480079",
    "end": "2485200"
  },
  {
    "text": "is very unstable but if you can reparameterize your model then well maybe you don't have to do any sort of",
    "start": "2485200",
    "end": "2490960"
  },
  {
    "text": "retuning at all of course that's you know way more optimistic than what happens in practice but hopefully this",
    "start": "2490960",
    "end": "2496000"
  },
  {
    "text": "gives you a sense of of you know why this is really cool and really interesting right scale aware",
    "start": "2496000",
    "end": "2502720"
  },
  {
    "text": "initializations cool any questions up until this point i feel like I've sort of gone through a whole bunch of uh",
    "start": "2502920",
    "end": "2509440"
  },
  {
    "text": "scaling architecture and parameter stuff so maybe I'll stop for a moment here um in case anyone has any questions",
    "start": "2509440",
    "end": "2516319"
  },
  {
    "text": "yeah I didn't really get the intuition behind like if we want a lower loss",
    "start": "2516319",
    "end": "2521520"
  },
  {
    "text": "target we want to increase the match size i didn't really understand that yeah like so when you have a lower loss",
    "start": "2521520",
    "end": "2529119"
  },
  {
    "text": "target yeah so um what you want to do right is",
    "start": "2529119",
    "end": "2537839"
  },
  {
    "text": "the smaller the loss target the kind of more sensitive things are and in the same way that like you're going to be lowering your learning rate right you",
    "start": "2537839",
    "end": "2544960"
  },
  {
    "text": "want to also increase your batch size in order to d noiseise right like the more sensitive the target that you have the",
    "start": "2544960",
    "end": "2551119"
  },
  {
    "text": "sort of more precise your gradients potentially have to be um one way of thinking about it is like you know as",
    "start": "2551119",
    "end": "2556240"
  },
  {
    "text": "you're cooling down and your learning rate is going up um maybe your batch size should increase as well because the learning rate and batch sizes sort of",
    "start": "2556240",
    "end": "2562640"
  },
  {
    "text": "affect each other inversely yeah this backside thing is only true",
    "start": "2562640",
    "end": "2568400"
  },
  {
    "text": "for NLP not for or for computer vision i'm not uh I'm not sure there is a sort",
    "start": "2568400",
    "end": "2576119"
  },
  {
    "text": "of related OpenAI scaling paper for sort of multimodal models but I'm not I don't",
    "start": "2576119",
    "end": "2581359"
  },
  {
    "text": "remember what that says about critical batch size for those yeah yes",
    "start": "2581359",
    "end": "2588480"
  },
  {
    "text": "the noise scale yeah the noise scale at least in sort of this figure if that's what you're asking about this is a kind",
    "start": "2588480",
    "end": "2594560"
  },
  {
    "text": "of theoretical analysis it's basically about the gradient noise that you expect from random sampling within the batch um",
    "start": "2594560",
    "end": "2600960"
  },
  {
    "text": "so this is not like a you know precisely empirically measured quantity it'll be",
    "start": "2600960",
    "end": "2607520"
  },
  {
    "text": "simple okay all right so one thing I'll caution",
    "start": "2608520",
    "end": "2616720"
  },
  {
    "text": "and I think this is a big caution for a lot of scaling law works is that scaling laws are very nicely behaved for log",
    "start": "2616720",
    "end": "2623440"
  },
  {
    "text": "losses right so we train on you know uh next token prediction cross entropies",
    "start": "2623440",
    "end": "2628720"
  },
  {
    "text": "when your scaling law targets are those cross entropies very easy works very well um but if you're trying to do",
    "start": "2628720",
    "end": "2636640"
  },
  {
    "text": "downstream tasks right you're trying to like directly scale on benchmarks behavior is much less predictable um so",
    "start": "2636640",
    "end": "2642960"
  },
  {
    "text": "here on the left side this is from um YK's paper uh comparing lots of different sort of like hyperparameters",
    "start": "2642960",
    "end": "2649280"
  },
  {
    "text": "and architectures um you see that the number of parameters which in this case is a surrogate for compute and the",
    "start": "2649280",
    "end": "2655040"
  },
  {
    "text": "negative log perplexity is you know very nicely linearly correlated and what this is basically saying is well it doesn't",
    "start": "2655040",
    "end": "2661920"
  },
  {
    "text": "matter what your like depth or width or like precise setting of the hyperparameters are the only thing that",
    "start": "2661920",
    "end": "2667359"
  },
  {
    "text": "really matters is your total compute expenditure right this is a very simple and nice story but then you take these",
    "start": "2667359",
    "end": "2673119"
  },
  {
    "text": "models um this was back in 2023 so you know people were still kind of doing super glue accuracy um And you know you",
    "start": "2673119",
    "end": "2680640"
  },
  {
    "text": "basically say like okay but what's the downstream performance of these models and while now we don't see a very nice",
    "start": "2680640",
    "end": "2686079"
  },
  {
    "text": "linear relationship anymore right we see this like totally different thing where certain models are much better than",
    "start": "2686079",
    "end": "2691280"
  },
  {
    "text": "others and certain architectures are are better than others um and so you might not expect exactly this kind of scaling",
    "start": "2691280",
    "end": "2697800"
  },
  {
    "text": "property and we've seen variants of this story uh play out in many different places um if you followed the literature",
    "start": "2697800",
    "end": "2704560"
  },
  {
    "text": "on state space models um that's one thing that we've seen um you know in state space models you know we see",
    "start": "2704560",
    "end": "2710880"
  },
  {
    "text": "really nice predictable scaling like the ones on the left but often um for certain capabilities like in context",
    "start": "2710880",
    "end": "2716960"
  },
  {
    "text": "learning or for for QA um people have shown that you know these models maybe do less well so it's important to not",
    "start": "2716960",
    "end": "2723839"
  },
  {
    "text": "take this like perplexity scaling as the same thing as downstream scaling and you want to be a little bit cautious",
    "start": "2723839",
    "end": "2730240"
  },
  {
    "text": "whenever you're doing these kinds of analyses okay so um maybe this is not",
    "start": "2730240",
    "end": "2738880"
  },
  {
    "text": "surprising to some of you but hopefully you know this is surprising and convincing um which is that you know if",
    "start": "2738880",
    "end": "2745760"
  },
  {
    "text": "we want to make lots of engineering decisions like hyperparameter choices architecture decisions um we can do a",
    "start": "2745760",
    "end": "2751599"
  },
  {
    "text": "lot of that before training right like we can train these models at small scale across several orders of magnitude",
    "start": "2751599",
    "end": "2756960"
  },
  {
    "text": "compute and then use scale that up in order to try to predict the behavior of of models right so the scaling law based",
    "start": "2756960",
    "end": "2763359"
  },
  {
    "text": "design procedure is pretty simple you train a few smaller models and these smaller models should span a couple",
    "start": "2763359",
    "end": "2768480"
  },
  {
    "text": "orders of magnitude compute you establish a scaling law of some kind so you you know see that at least on the",
    "start": "2768480",
    "end": "2774000"
  },
  {
    "text": "models that you trained that there's a clear log log linear relationship and then based on this prediction you can",
    "start": "2774000",
    "end": "2780319"
  },
  {
    "text": "set optimal hyper parameters um in many cases in fact you know these scaling laws won't really vary too much their",
    "start": "2780319",
    "end": "2786319"
  },
  {
    "text": "slopes will actually be the same in which case sort of the correlary to this is you can just train a few smaller",
    "start": "2786319",
    "end": "2791520"
  },
  {
    "text": "models and you know the results of those small models will transfer surprisingly well to the larger models in many of",
    "start": "2791520",
    "end": "2798400"
  },
  {
    "text": "these cases but not all of them um learning rate being an important exception for example okay so that's how you do things",
    "start": "2798400",
    "end": "2805680"
  },
  {
    "text": "like hyperparameter selection and architecture selection um now I want to talk about one very important use of",
    "start": "2805680",
    "end": "2811359"
  },
  {
    "text": "scaling laws one that's had kind of an outsized influence on you know how we pick sizes of models how we think about",
    "start": "2811359",
    "end": "2818160"
  },
  {
    "text": "um data efficiency and so on of these models so I think um back in the earlier",
    "start": "2818160",
    "end": "2824160"
  },
  {
    "text": "days when people were beginning to scale up these models there's a really core question that you need to ask right do",
    "start": "2824160",
    "end": "2829839"
  },
  {
    "text": "we need more data or do we need bigger models in some sense um you know back in",
    "start": "2829839",
    "end": "2835839"
  },
  {
    "text": "2021 to 2023 or something um you know data was way more abundant than compute",
    "start": "2835839",
    "end": "2840880"
  },
  {
    "text": "right so we didn't need to worry about um you know the total data limitations and so the one limiting resource is",
    "start": "2840880",
    "end": "2847200"
  },
  {
    "text": "compute right your total number of flops for your training budget that's kind of the limiting resource and you can then",
    "start": "2847200",
    "end": "2852640"
  },
  {
    "text": "spend that resource in many different ways you can spend it on training on lots of data with a small model or you",
    "start": "2852640",
    "end": "2858480"
  },
  {
    "text": "can train one giant model on very little data right and both of those extremes seem very wasteful right like if you",
    "start": "2858480",
    "end": "2864480"
  },
  {
    "text": "have a teenytiny model pumping in tons and tons of data doesn't seem useful and reverse um if you have a giant model",
    "start": "2864480",
    "end": "2870640"
  },
  {
    "text": "with like 10 tokens also doesn't seem very useful and so this was sort of a",
    "start": "2870640",
    "end": "2875680"
  },
  {
    "text": "core question for for many people um and so there you know simultaneously several",
    "start": "2875680",
    "end": "2880720"
  },
  {
    "text": "authors sort of proposed um sort of joint data model scaling laws to try to answer this question and so what are",
    "start": "2880720",
    "end": "2887200"
  },
  {
    "text": "those right i have been talking about scaling laws um in essentially one variable exclusively up until this point",
    "start": "2887200",
    "end": "2893440"
  },
  {
    "text": "and that one variable has varied it has sometimes been parameters or data or compute um but we've not looked at joint",
    "start": "2893440",
    "end": "2899280"
  },
  {
    "text": "scaling right um and so data model scaling laws are things that look like this these two sort of um equations here",
    "start": "2899280",
    "end": "2906480"
  },
  {
    "text": "are both like functionally equivalent um to first order um and describe the trade-off between the amount of data and",
    "start": "2906480",
    "end": "2913119"
  },
  {
    "text": "the amount of models so the top one from Rosenfeld you know is basically saying",
    "start": "2913119",
    "end": "2918319"
  },
  {
    "text": "there's a part of the error one part of it that decays polinomially in data there's a part of the error that decays",
    "start": "2918319",
    "end": "2924240"
  },
  {
    "text": "polinomially in the model size and then there's an irreducible error term that cannot be removed even if I scale both",
    "start": "2924240",
    "end": "2930880"
  },
  {
    "text": "the data size and the model to infinity right same effect with Kaplan um but here they're sort of thinking about",
    "start": "2930880",
    "end": "2936640"
  },
  {
    "text": "irreducible error rather than uh reducible error and so there's no constant term here um so this seems kind",
    "start": "2936640",
    "end": "2943839"
  },
  {
    "text": "of arbitrary um because I don't think there's any sort of you know uh top down reason why",
    "start": "2943839",
    "end": "2949920"
  },
  {
    "text": "this has to be the correct functional form but this provides surprisingly good fits to the joint error that you see in",
    "start": "2949920",
    "end": "2956319"
  },
  {
    "text": "data and model so this is from um I believe Rosenfeld they show this nice 3D plot of this is the amount of data this",
    "start": "2956319",
    "end": "2963280"
  },
  {
    "text": "is um the amount this is the size of the model and this is the loss on the y-axis and the surface that's being fit is",
    "start": "2963280",
    "end": "2969760"
  },
  {
    "text": "their functional form the dots are their runs um it might be a little hard to see from the back but the surface fits the",
    "start": "2969760",
    "end": "2975760"
  },
  {
    "text": "dots almost exactly um and despite the fact that this",
    "start": "2975760",
    "end": "2981920"
  },
  {
    "text": "functional form is kind of ad hoc like it's pulled out of a hat um it is surprisingly accurate um this one's from",
    "start": "2981920",
    "end": "2988000"
  },
  {
    "text": "from Rosenfeld as well um where they basically say okay I'm only going to train um on essentially the small half",
    "start": "2988000",
    "end": "2995200"
  },
  {
    "text": "right models that are small and data that is small right so on this sort of left bottom and I'm going to extrapolate",
    "start": "2995200",
    "end": "3001599"
  },
  {
    "text": "to models that are sort of both large and trained with more models and how good is that fit of like joint",
    "start": "3001599",
    "end": "3007640"
  },
  {
    "text": "extrapolation well quite good right so if you look at the error my my um uh my",
    "start": "3007640",
    "end": "3013839"
  },
  {
    "text": "sort of uh real values are on the x-axis my predictions of the error on the y-",
    "start": "3013839",
    "end": "3019200"
  },
  {
    "text": "axis and they're sort of almost exactly right both on like sort of imageet and on wiki text so this seems pretty good",
    "start": "3019200",
    "end": "3027480"
  },
  {
    "text": "um and so you know for a fixed compute budget now what can we do we go back to",
    "start": "3027480",
    "end": "3033520"
  },
  {
    "text": "so for example Kappla and we see similar things being done here we see sort of joint scaling of compute and data so in",
    "start": "3033520",
    "end": "3040720"
  },
  {
    "text": "this case parameters are on the x-axis the colors represent compute and so",
    "start": "3040720",
    "end": "3046000"
  },
  {
    "text": "there's sort of a third axis of data that's being implicitly varied in order to vary the total amount of comput so as",
    "start": "3046000",
    "end": "3051760"
  },
  {
    "text": "you go uh shift in uh on these curves um the parameters are being varied um while",
    "start": "3051760",
    "end": "3057760"
  },
  {
    "text": "the compute is being held constant and so the amount of data is going to",
    "start": "3057760",
    "end": "3062960"
  },
  {
    "text": "vary so chinchilla I think many of you have hopefully heard of um is probably",
    "start": "3064200",
    "end": "3070000"
  },
  {
    "text": "the reference in solving this problem right so both Rosenfeld and Kaplan came",
    "start": "3070000",
    "end": "3075119"
  },
  {
    "text": "up with kind of this joint scaling functional form and then you know both of them sort of noticed that it was",
    "start": "3075119",
    "end": "3081599"
  },
  {
    "text": "possible to use these functional forms to optimize the trade-off between compute and data in various ways um but",
    "start": "3081599",
    "end": "3089040"
  },
  {
    "text": "for various reasons it's you know basically it's hard to fit these sort of functional forms precisely and the",
    "start": "3089040",
    "end": "3094880"
  },
  {
    "text": "details like the the sort of learning rate shapes um being different um are",
    "start": "3094880",
    "end": "3099920"
  },
  {
    "text": "important and so Kaplan sort of had one estimate that was quite far off from",
    "start": "3099920",
    "end": "3105520"
  },
  {
    "text": "what was later in some sense validated to be optimal and so the Chinchilla paper um by a bunch of Google authors",
    "start": "3105520",
    "end": "3112480"
  },
  {
    "text": "sort of was an attempt to really empirically try to nail down what is the right trade-off um between the amount of",
    "start": "3112480",
    "end": "3118480"
  },
  {
    "text": "tokens and the model size assuming that your goal is to get the best model for the smallest amount of training flops",
    "start": "3118480",
    "end": "3125359"
  },
  {
    "text": "right so they have three different approaches approach one two and three um for basically fitting different curves",
    "start": "3125359",
    "end": "3131359"
  },
  {
    "text": "and making scaling predictions these blue dots are the models that they trained um and the basically the lines",
    "start": "3131359",
    "end": "3137760"
  },
  {
    "text": "are predicting different optimal parameter sizes for different flops and hopefully most of you kind of know the",
    "start": "3137760",
    "end": "3143760"
  },
  {
    "text": "chinchilla ratio that's something like you know 20 tokens per parameter and that comes from exactly this right like",
    "start": "3143760",
    "end": "3149760"
  },
  {
    "text": "if you take each of these points and you multiply it by 20 you're going to get roughly the flop or sorry multiply it by",
    "start": "3149760",
    "end": "3155520"
  },
  {
    "text": "20 you'll get the token count and so if you multiply the parameters by that you'll get the flops",
    "start": "3155520",
    "end": "3162760"
  },
  {
    "text": "um the difference between sort of the the Kaplan uh results which were uh basically estimating one set of token to",
    "start": "3162960",
    "end": "3169680"
  },
  {
    "text": "parameter ratios um and sort of the chinchilla ones one of the reasons um is",
    "start": "3169680",
    "end": "3175200"
  },
  {
    "text": "because of learning rate schedules right we know that we train models with cosine learning rates right so cosine learning",
    "start": "3175200",
    "end": "3181359"
  },
  {
    "text": "rates are going to look something like this right it goes up and then it comes back down and then it's going to cool down all the way to a minimum learning",
    "start": "3181359",
    "end": "3187440"
  },
  {
    "text": "rate at your bottom but um you can't one thing about cosine learning rates that",
    "start": "3187440",
    "end": "3192800"
  },
  {
    "text": "sort of trips everyone up all the time is you can't truncate them early right for a cosine learning rate you have to",
    "start": "3192800",
    "end": "3198640"
  },
  {
    "text": "sort of go all the way to the end in order to get a valid model right you have to get a cool down phase all the",
    "start": "3198640",
    "end": "3203680"
  },
  {
    "text": "way to the end if I truncate a model in the middle this is not the same as starting a model from scratch and",
    "start": "3203680",
    "end": "3209359"
  },
  {
    "text": "training it with a cosine learning rate somewhere in the middle um and this was one of the sort of contributing factors",
    "start": "3209359",
    "end": "3215280"
  },
  {
    "text": "there were others as well um leading to the Kaplon estimates um being pretty far",
    "start": "3215280",
    "end": "3220319"
  },
  {
    "text": "off from the the later sort of more improved estimates provided uh by the",
    "start": "3220319",
    "end": "3225760"
  },
  {
    "text": "chinchilla paper so what do the chinchilla authors",
    "start": "3225760",
    "end": "3230800"
  },
  {
    "text": "actually do well they have three different methods of trying to estimate the optimum uh tradeoff between tokens",
    "start": "3230800",
    "end": "3237440"
  },
  {
    "text": "to models and each of these methods are going to sort of provide different scaling coefficients right scaling",
    "start": "3237440",
    "end": "3243520"
  },
  {
    "text": "coefficients for the model size and scaling coefficients for the data size um and kind of surprisingly in this case",
    "start": "3243520",
    "end": "3250720"
  },
  {
    "text": "they're getting 0.5 on both of these for methods one and two and method 3 is",
    "start": "3250720",
    "end": "3256240"
  },
  {
    "text": "providing pretty different or slightly different estimates they're about off by 0.03 um but we'll talk about that a",
    "start": "3256240",
    "end": "3262720"
  },
  {
    "text": "little bit later kaplan at all you see is way off um than any of the three estimates right so we'll go over each of",
    "start": "3262720",
    "end": "3270400"
  },
  {
    "text": "these methods each of these makes sense they make sort of different assumptions about scaling um but they end up with",
    "start": "3270400",
    "end": "3276400"
  },
  {
    "text": "very very similar estimates um at the very end here so method one on chinchilla is to",
    "start": "3276400",
    "end": "3285440"
  },
  {
    "text": "basically take the minimum over curves um and so what does that mean well you",
    "start": "3285440",
    "end": "3290559"
  },
  {
    "text": "basically overlay all of the different training curves that you have so you can see here um on the x-axis is different",
    "start": "3290559",
    "end": "3298240"
  },
  {
    "text": "flops on the y-axis is sort of the the training loss and I have models trained",
    "start": "3298240",
    "end": "3304240"
  },
  {
    "text": "at many different sizes and of course you know each of these sizes are going to be trained to with different amount",
    "start": "3304240",
    "end": "3309440"
  },
  {
    "text": "of tokens and so they're going to reach a different sort of to total flop as I sort of go through training right now",
    "start": "3309440",
    "end": "3316640"
  },
  {
    "text": "what I'm going to do is I'm going to look at the lower envelope right the set of sort of points or checkpoints that",
    "start": "3316640",
    "end": "3322160"
  },
  {
    "text": "prove to be optimal under any compute budget and I can take these models and I",
    "start": "3322160",
    "end": "3327520"
  },
  {
    "text": "can look at okay what were the actual parameter sizes of these models and you can see that sort of the the total",
    "start": "3327520",
    "end": "3333520"
  },
  {
    "text": "compute on the x-axis here and the number of parameters as well as the corresponding tokens all forms a",
    "start": "3333520",
    "end": "3339280"
  },
  {
    "text": "relatively nice uh scaling law right and so this is kind of the the minimum envelope method it's basically saying I",
    "start": "3339280",
    "end": "3346160"
  },
  {
    "text": "expect the minimum training loss where I optimize over all the model sizes to",
    "start": "3346160",
    "end": "3351359"
  },
  {
    "text": "actually be optimum in flops and sort of um to call back to some earlier papers",
    "start": "3351359",
    "end": "3357359"
  },
  {
    "text": "right if you look back at the earlier um sort of Kaplan paper and other scaling",
    "start": "3357359",
    "end": "3362480"
  },
  {
    "text": "laws you see exactly this already being done you see you know different models being trained with different sort of",
    "start": "3362480",
    "end": "3368880"
  },
  {
    "text": "parameters and different compute scales and we're taking sort of the minimum across these and we've already seen that",
    "start": "3368880",
    "end": "3374640"
  },
  {
    "text": "the minimum forms a scaling law so this is building on this observation that the minimum across many different training",
    "start": "3374640",
    "end": "3380400"
  },
  {
    "text": "curves across compute will should form a uh power line so under that assumption",
    "start": "3380400",
    "end": "3388480"
  },
  {
    "text": "you can get fairly nice fits um and this gives you know one estimate that is quite consistent um with others of of",
    "start": "3388480",
    "end": "3395119"
  },
  {
    "text": "0.5 now the other one this I think if you were to pick a single canonical way to",
    "start": "3395119",
    "end": "3402240"
  },
  {
    "text": "do the chinchilla analysis um this would probably be the one and in some ways I think this is the most conceptually",
    "start": "3402240",
    "end": "3408160"
  },
  {
    "text": "straightforward one um which is the isoflop analysis so to do the isoflop",
    "start": "3408160",
    "end": "3413520"
  },
  {
    "text": "analysis what you do is you pick a bunch of compute scales so each of these colors is a different amount of compute",
    "start": "3413520",
    "end": "3419839"
  },
  {
    "text": "and what I'm going to do is for each of these compute scales I can essentially have models with smaller parameters",
    "start": "3419839",
    "end": "3426079"
  },
  {
    "text": "trained with more data or more parameters trained with less data right so I'm going to sweep over my sort of",
    "start": "3426079",
    "end": "3431920"
  },
  {
    "text": "model sizes for each of these flops and then I can look at the minimum of each",
    "start": "3431920",
    "end": "3436960"
  },
  {
    "text": "of these curves i can either pick the minimum point explicitly sort of nonparametrically or I could fit",
    "start": "3436960",
    "end": "3442000"
  },
  {
    "text": "quadratics onto each of these and get the minimum point of the quadratic but in either case sort of the argument is",
    "start": "3442000",
    "end": "3448480"
  },
  {
    "text": "fairly simple the argument is it should be the case that this minimum itself follows a predictable scaling law and",
    "start": "3448480",
    "end": "3454480"
  },
  {
    "text": "thus I can extract from it sort of the optimum sort of parameters per flop so that's the minimum points across all of",
    "start": "3454480",
    "end": "3460960"
  },
  {
    "text": "these um and I can also extract the optimal number of tokens per flop i can read that out by sort of dividing my",
    "start": "3460960",
    "end": "3468079"
  },
  {
    "text": "flops budget by the number of parameters right so I can get those simultaneously and you can see that once again this",
    "start": "3468079",
    "end": "3473760"
  },
  {
    "text": "gives very clean sort of results um that are consistent with method one right so we can compare that with before this",
    "start": "3473760",
    "end": "3480000"
  },
  {
    "text": "says for uh the eventual chinchilla model budget you want 63 billion parameters um this one says 67 billion",
    "start": "3480000",
    "end": "3487280"
  },
  {
    "text": "parameters um the two are quite close right okay the last one um honestly is",
    "start": "3487280",
    "end": "3496720"
  },
  {
    "text": "just a little bit messier um and this goes back to kind of that Rosenfeld paper um if you have a functional form",
    "start": "3496720",
    "end": "3504480"
  },
  {
    "text": "like this one right like this uh from Rosenfeld a very natural instinct is to say I'm just going to train a bunch of",
    "start": "3504480",
    "end": "3511040"
  },
  {
    "text": "models varying both N and M right and I'm just going to do curve fitting i'm going to fit this curve onto whatever I",
    "start": "3511040",
    "end": "3516960"
  },
  {
    "text": "get a thing I get out of my models right so I'm going to train a bunch of models and fit that 3D shape um and we know",
    "start": "3516960",
    "end": "3523040"
  },
  {
    "text": "from Rosenfeld it's reasonable to some extent to fit these right so you've got all these dots which are the models um I",
    "start": "3523040",
    "end": "3529680"
  },
  {
    "text": "fitted a curve that's this this sort of heat map color that you see on the left um and then you can sort of back out",
    "start": "3529680",
    "end": "3536960"
  },
  {
    "text": "what the implied isoflop should look like from these dash lines but if you look at this you know hopefully you see",
    "start": "3536960",
    "end": "3543280"
  },
  {
    "text": "that the scaling law fits and like sort of the curve fits here are just not quite as good um as the fits in the",
    "start": "3543280",
    "end": "3550640"
  },
  {
    "text": "other plots right um and you know if you look at the coefficients the um",
    "start": "3550640",
    "end": "3555839"
  },
  {
    "text": "chinchilla method 3 just gives way different estimates in terms of the model size and total token count than",
    "start": "3555839",
    "end": "3561839"
  },
  {
    "text": "the others right and actually this was a mystery to me for a long time i think",
    "start": "3561839",
    "end": "3566880"
  },
  {
    "text": "some of my students were like why why is method 3 so different and I said I don't know maybe scaling laws are just sometimes noisy um I don't know how many",
    "start": "3566880",
    "end": "3574480"
  },
  {
    "text": "of you know this but this is a really fun trivia fact um or not trivia fact fun uh piece of trivia let's say um so",
    "start": "3574480",
    "end": "3581839"
  },
  {
    "text": "last year some folks at Epoch AI I don't know what motivated them to do this were",
    "start": "3581839",
    "end": "3586960"
  },
  {
    "text": "curious enough about this result that they went and tried to replicate method 3 um and you know the it was very",
    "start": "3586960",
    "end": "3595599"
  },
  {
    "text": "difficult to replicate it because you don't have the original data for all of these training runs so they actually",
    "start": "3595599",
    "end": "3600880"
  },
  {
    "text": "like went to the extreme of actually looking at the plots and using sort of a forensic tool to to extract the values",
    "start": "3600880",
    "end": "3608400"
  },
  {
    "text": "of the points from the plots and based on that they could actually replicate um the original result and kind of the",
    "start": "3608400",
    "end": "3615599"
  },
  {
    "text": "funny thing is they showed that actually the the curve fitting was the bad part like their data in their approach was",
    "start": "3615599",
    "end": "3621520"
  },
  {
    "text": "good but actually when they fit the curve they didn't necessarily do it right and so the original fit had",
    "start": "3621520",
    "end": "3627680"
  },
  {
    "text": "residuals if you're familiar with regression you know your residual should be zero mean centered because otherwise you should be you know offsetting your",
    "start": "3627680",
    "end": "3634160"
  },
  {
    "text": "predictions to make it zero centered um their residuals are non zero and then they you know fit it better and then",
    "start": "3634160",
    "end": "3639440"
  },
  {
    "text": "when they did fit it better well actually their optimal estimate you know",
    "start": "3639440",
    "end": "3644720"
  },
  {
    "text": "almost exactly matched methods one and two um and so this is one of those funny cases where actually you know the",
    "start": "3644720",
    "end": "3651040"
  },
  {
    "text": "original authors had both the idea and the data right but because of a minor issue in curfeitting they they had kind",
    "start": "3651040",
    "end": "3657760"
  },
  {
    "text": "of had it wrong and the replication actually makes it more correct uh than before usually replication sort of",
    "start": "3657760",
    "end": "3663040"
  },
  {
    "text": "disproved things but in this case actually the replication just showed that the original result was correct all along which is I think a pretty cool uh",
    "start": "3663040",
    "end": "3670000"
  },
  {
    "text": "result okay so the final thing I want to talk",
    "start": "3670000",
    "end": "3675200"
  },
  {
    "text": "about um with kind of this set of chinchilla results is you know we're talking about training optimal scaling",
    "start": "3675200",
    "end": "3681839"
  },
  {
    "text": "so you have a fixed flops budget i want the best possible model um possible but",
    "start": "3681839",
    "end": "3687200"
  },
  {
    "text": "really I think that the story has really shifted when sort of chinchilla was written and the Kaplan paper was written",
    "start": "3687200",
    "end": "3693520"
  },
  {
    "text": "um you know LLMs were not really a product yet and so really the name of the game was everyone wanted the most",
    "start": "3693520",
    "end": "3699280"
  },
  {
    "text": "biggest flashiest most intelligent model but they didn't care about the inference cost of actually deploying these systems",
    "start": "3699280",
    "end": "3706400"
  },
  {
    "text": "but nowadays you know what we really care about is uh inference costs right because these systems are actually",
    "start": "3706400",
    "end": "3712079"
  },
  {
    "text": "products they generate revenue you know you have a cost associated with the revenue um and so we've seen over time",
    "start": "3712079",
    "end": "3718079"
  },
  {
    "text": "that actually the tokens per parameter has steadily grown right like GPT3 was",
    "start": "3718079",
    "end": "3723200"
  },
  {
    "text": "two tokens per parameter um Chinchilla moved us to 20 tokens per parameter and for a bit people played around with sort",
    "start": "3723200",
    "end": "3729839"
  },
  {
    "text": "of 20 tokens per parameter stuff but then you know very quickly people realized actually what we care about is",
    "start": "3729839",
    "end": "3736799"
  },
  {
    "text": "you know really good intelligence at really small parameter sizes and so people have really started to scale up",
    "start": "3736799",
    "end": "3742000"
  },
  {
    "text": "the number of tokens per parameter very very rapidly um and I think I saw yesterday that for example the most",
    "start": "3742000",
    "end": "3748079"
  },
  {
    "text": "recent uh QEM models were trained on 30 trillion tokens right you know people are really pushing the limits on uh the",
    "start": "3748079",
    "end": "3755040"
  },
  {
    "text": "tokens to parameter ratio because really you would much rather pay the upfront cost than to pay the ongoing operating",
    "start": "3755040",
    "end": "3761359"
  },
  {
    "text": "cost of uh in running inference on a really big um expensive",
    "start": "3761359",
    "end": "3767720"
  },
  {
    "text": "model cool last thing um you know that is kind",
    "start": "3767720",
    "end": "3773520"
  },
  {
    "text": "of a fun side thing that I I want to end with is to say um you know these results",
    "start": "3773520",
    "end": "3778640"
  },
  {
    "text": "are are pretty robust and easy to replicate um a few years back uh one of my students Isan was really interested",
    "start": "3778640",
    "end": "3784799"
  },
  {
    "text": "in you know really pushing diffusion models for text forward and so one of the things that we had to do was to say",
    "start": "3784799",
    "end": "3790400"
  },
  {
    "text": "this is a whole new kind of model we don't know what the optimal token to parameter ratio is we don't know if this thing even you know reliably scales it's",
    "start": "3790400",
    "end": "3797200"
  },
  {
    "text": "a totally different kind of generative model um what do we do well turns out",
    "start": "3797200",
    "end": "3802400"
  },
  {
    "text": "you know if you just fit uh the same kind of playbook of saying \"Oh we're going to do you know isoflop analyses",
    "start": "3802400",
    "end": "3808079"
  },
  {
    "text": "for auto reggressive models we get almost exactly the chinchilla thing without too much effort.\" You know you do the same kind of analysis on",
    "start": "3808079",
    "end": "3814240"
  },
  {
    "text": "diffusion models wow we see you know very similar kinds of curves even though it's a pretty different generative model",
    "start": "3814240",
    "end": "3819680"
  },
  {
    "text": "entirely and then if you plot sort of the minimum across of these well you see very predictable scaling for both",
    "start": "3819680",
    "end": "3825760"
  },
  {
    "text": "separated by a constant offset right right like I don't bring this up to say you know uh because I want the particularly pushed to diffusion models",
    "start": "3825760",
    "end": "3832640"
  },
  {
    "text": "but just as a really random sort of case study or example to say you know these scaling laws don't necessarily need to",
    "start": "3832640",
    "end": "3839359"
  },
  {
    "text": "be these very cherrypicked examples they seem to happen pretty naturally um as you're sort of working on new models or",
    "start": "3839359",
    "end": "3845359"
  },
  {
    "text": "working on new environments so okay um you know this is",
    "start": "3845359",
    "end": "3853039"
  },
  {
    "text": "uh you know the to put together this last part right log",
    "start": "3853039",
    "end": "3858240"
  },
  {
    "text": "linearity is not just about sort of one-dimensional things where we think about data they extend to sort of model",
    "start": "3858240",
    "end": "3863680"
  },
  {
    "text": "parameters they extend to total compute and so that lets us you know make all sorts of hyperparameter and other",
    "start": "3863680",
    "end": "3869280"
  },
  {
    "text": "decisions that's kind of this first part um and they're also letting us make really smart resource trade-offs right",
    "start": "3869280",
    "end": "3874960"
  },
  {
    "text": "they let us make trade-offs between sort of big models versus more data um and we saw that in kind of this chinchilla",
    "start": "3874960",
    "end": "3881119"
  },
  {
    "text": "analysis and you know it's kind of remarkable how cleanly uh things like the isoflop analysis um turn",
    "start": "3881119",
    "end": "3888440"
  },
  {
    "text": "out so all right that's all I got for for basic data sc or basic uh scaling",
    "start": "3888440",
    "end": "3893760"
  },
  {
    "text": "laws um we did a recap or um of Kaplan as well as Chinchilla today and",
    "start": "3893760",
    "end": "3899200"
  },
  {
    "text": "hopefully now you're on board with this idea of data scaling model scaling um and using scaling loss to sort of",
    "start": "3899200",
    "end": "3905200"
  },
  {
    "text": "optimize all the aspects of your model without actually going all the way to the large scale training runs um thanks",
    "start": "3905200",
    "end": "3911200"
  },
  {
    "text": "and I'll see you all Thursday",
    "start": "3911200",
    "end": "3915000"
  }
]