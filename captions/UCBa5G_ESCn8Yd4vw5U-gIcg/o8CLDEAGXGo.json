[
  {
    "start": "0",
    "end": "58000"
  },
  {
    "text": "Okay. Let's get started. Uh, so for some logistics, uh,",
    "start": "5000",
    "end": "10890"
  },
  {
    "text": "the poster session is tomorrow, uh, 1:30 PM. Print your posters well ahead of time.",
    "start": "10890",
    "end": "17580"
  },
  {
    "text": "Hopefully, you've got either already printed it or you- you're going to print it very soon. Um, your final project report is due Monday the 16th at midnight.",
    "start": "17580",
    "end": "27600"
  },
  {
    "text": "Uh, you're very welcome to submit it earlier, uh, and this is a- a hard deadline like no late days for this",
    "start": "27600",
    "end": "34800"
  },
  {
    "text": "because grades are due very shortly after this deadline. So try to get it done early.",
    "start": "34800",
    "end": "40095"
  },
  {
    "text": "Um, and then of course this is the last lecture. Uh, and I'll leave some time for course evaluations at the end.",
    "start": "40095",
    "end": "47629"
  },
  {
    "text": "So you can fill those out and we're looking forward to the feedback that you had throughout the course especially since this is the first time that we're offering the course.",
    "start": "47630",
    "end": "54605"
  },
  {
    "text": "Okay. So, um, let's get to the main topic. So I think today is actually, uh, in my mind,",
    "start": "54605",
    "end": "60800"
  },
  {
    "start": "58000",
    "end": "185000"
  },
  {
    "text": "one of the most interesting, uh, and exciting lectures because we're gonna talk about, what are the things that don't work very well,",
    "start": "60800",
    "end": "66695"
  },
  {
    "text": "and some of the things that we might do to resolve these challenges, uh, and really the first in- part of",
    "start": "66695",
    "end": "74180"
  },
  {
    "text": "the lecture and most of the lecture will be covering some of the research that we've been doing very recently to try to mitigate some of",
    "start": "74180",
    "end": "79880"
  },
  {
    "text": "the challenges in meta-learning and multi-task learning, and, uh, the second part we'll get into",
    "start": "79880",
    "end": "85970"
  },
  {
    "text": "some more open and unsolved challenges in these topics beyond all of the challenges that we've covered  in the rest of the course.",
    "start": "85970",
    "end": "94195"
  },
  {
    "text": "Cool. So, um, what doesn't work very well? Uh, the first thing that I'd like to talk about today is,",
    "start": "94195",
    "end": "100900"
  },
  {
    "text": "where do we construct tasks for meta-learning and for multitask learning? Uh, in general, we- we saw reasonable ways to construct tasks.",
    "start": "100900",
    "end": "109820"
  },
  {
    "text": "For example, for few-shot image classification. Uh, but in much more realistic situations, it may not be clear how you're- how- how you might go about",
    "start": "109820",
    "end": "119660"
  },
  {
    "text": "getting this set of structured data from the data sets that you have or the data sets that you have the means to collect.",
    "start": "119660",
    "end": "125345"
  },
  {
    "text": "Uh, and in particular, we'll- we'll talk about some of the challenges that come up when you try to construct tasks such as memorization problems,",
    "start": "125345",
    "end": "131870"
  },
  {
    "text": "such as how to use time-series data without task boundaries, and how to, um, how to actually have algorithms that can come up with the tasks",
    "start": "131870",
    "end": "138140"
  },
  {
    "text": "themselves from unstructured data sources. Uh, and then beyond the problem of tasks, uh,",
    "start": "138140",
    "end": "145140"
  },
  {
    "text": "I'm gonna talk a little bit about how we might go about running multi-task RL and meta-RL across very distinct task families.",
    "start": "145140",
    "end": "151490"
  },
  {
    "text": "So before, we were seeing, like, different variations on a task like running forward or running backward or running at different velocities or reaching to different points in space and,",
    "start": "151490",
    "end": "159290"
  },
  {
    "text": "can we think about actually running multi-task RL and meta-RL across, uh, qualitatively distinct tasks?",
    "start": "159290",
    "end": "165635"
  },
  {
    "text": "Um, and we'll talk about challenges such as how do we go about actually specifying the tasks to the- to the agent,",
    "start": "165635",
    "end": "171425"
  },
  {
    "text": "um, what sort of distinct tasks do we train on and what are some of the challenges that arise when we actually try to do this?",
    "start": "171425",
    "end": "177724"
  },
  {
    "text": "Uh, and then lastly, we'll talk about open challenges. Okay. So first, let's talk about how to construct tasks for meta-learning.",
    "start": "177725",
    "end": "184220"
  },
  {
    "text": "Um, and to briefly review, uh, we were talking about, earlier in this course,",
    "start": "184220",
    "end": "189670"
  },
  {
    "text": "few-shot image classification and the way that we - the kind of- the kind of problem that we want to solve was something like this where you want to be able to learn",
    "start": "189670",
    "end": "195680"
  },
  {
    "text": "from a data set to classify new examples. Uh, and the way that you would construct your task or your distribution of tasks would be,",
    "start": "195680",
    "end": "202880"
  },
  {
    "text": "uh, something like this. This is what you did in your homework where you, uh, separate the data in two sets of classes and you, uh,",
    "start": "202880",
    "end": "211099"
  },
  {
    "text": "randomly assigned labels to each of these, uh, images, and then,uh, basically trained the network such that after entering the data on the left,",
    "start": "211100",
    "end": "219350"
  },
  {
    "text": "it could make classifications on the right. Now, one of the things that you did here,",
    "start": "219350",
    "end": "225350"
  },
  {
    "text": "uh, is you essentially shuffle the labels. Uh, you randomly selected, uh,",
    "start": "225350",
    "end": "232000"
  },
  {
    "text": "a label category for each of these five images, uh, and that was kind of randomly permuted for every single task.",
    "start": "232000",
    "end": "240575"
  },
  {
    "text": "Now, one of the questions I'd like to ask you is, what would happen if we didn't shuffle the labels?",
    "start": "240575",
    "end": "246610"
  },
  {
    "text": "Uh, and in particular, what if we always kept the category corresponding to mushroom as,",
    "start": "246610",
    "end": "253028"
  },
  {
    "text": "uh, the second class label? And what if we always kept the category corresponding to a singer as the fourth category?",
    "start": "253029",
    "end": "260200"
  },
  {
    "text": "And piano is the fifth category? What would the meta- what would- what would the meta learning algorithm",
    "start": "260200",
    "end": "266215"
  },
  {
    "text": "do? Would it- would it work? Would it- will everything be the same?",
    "start": "266215",
    "end": "271760"
  },
  {
    "text": "Would it be different? Does the question make sense?",
    "start": "271760",
    "end": "279720"
  },
  {
    "text": "Is this kind of what we did with our homework? I mean, when we set up the initial categories,",
    "start": "280920",
    "end": "286800"
  },
  {
    "text": "it seems to change the performance. Are we talking about the same thing? Um, right. So this is actually something that is a little bit different.",
    "start": "286800",
    "end": "293150"
  },
  {
    "text": "So in your homework, one of the things that you- one of the things that you found- that many people found to work better is, uh,",
    "start": "293150",
    "end": "299230"
  },
  {
    "text": "that you always, uh, like, oh the thing corre- corresponding to class one was always for that first,",
    "start": "299230",
    "end": "304500"
  },
  {
    "text": "the thing corresponding class two is always for that second, etc. What I'm talking about here is actually the assignment of",
    "start": "304500",
    "end": "309920"
  },
  {
    "text": "semantic class labels- the semantic classes to class labels. Uh, and so in your homework, you would always- you'd",
    "start": "309920",
    "end": "315530"
  },
  {
    "text": "randomly sample class and assign that to a class label, and then once you assign the class labels, uh, you'd pass those class labels into a sequence.",
    "start": "315530",
    "end": "322520"
  },
  {
    "text": "Uh, but what if for example the, the semantic class always corresponds to the same class label?",
    "start": "322520",
    "end": "329330"
  },
  {
    "text": "Like, uh, birds row is one, mushrooms row is two, that particular breed of dog was always number three,",
    "start": "329330",
    "end": "335360"
  },
  {
    "text": "singer was always number four. We would use the class labels as always to do",
    "start": "335360",
    "end": "341600"
  },
  {
    "text": "classification which will make it more difficult to generalize with these new classes. Yeah. Yeah. Can you elaborate on that a little bit?",
    "start": "341600",
    "end": "349640"
  },
  {
    "text": "Uh, information is contained within the class label if it keeps the same during training,",
    "start": "349640",
    "end": "356390"
  },
  {
    "text": "and as such when it goes to meta test time, it will be looking for the class label as information and we'll be able to find it there.",
    "start": "356390",
    "end": "368430"
  },
  {
    "text": "Yes. That's- that's like very close.",
    "start": "369520",
    "end": "374970"
  },
  {
    "text": "So what would- what would it, um, let's see, what would the,",
    "start": "377150",
    "end": "386595"
  },
  {
    "text": "um, would it like, uh, would it be- if you give it a new task",
    "start": "386595",
    "end": "392574"
  },
  {
    "text": "corresponding to the same classes that it's had in the training set would it be able to perform well?",
    "start": "392575",
    "end": "397010"
  },
  {
    "text": "Say it again. If it uses. If it sees the same task as- as one of the ones that saw before but just different images,",
    "start": "402330",
    "end": "408729"
  },
  {
    "text": "would it be able to perform well? Yeah. Assuming the class label is the same.",
    "start": "408730",
    "end": "414335"
  },
  {
    "text": "Yeah. And then if you change the- change the order of the class label, would it be able to do well?",
    "start": "414335",
    "end": "420129"
  },
  {
    "text": "And why? Why wouldn't it be able to do well? Sorry. It's only learned the features in that one category.",
    "start": "420560",
    "end": "429565"
  },
  {
    "text": "Yeah. Is there a comment over here? Yeah. I think [inaudible].",
    "start": "429565",
    "end": "436440"
  },
  {
    "text": "Yeah, so basically it can just memorize the ordering of the labels and say,",
    "start": "436440",
    "end": "441765"
  },
  {
    "text": "okay, bird is always the first label. If I see a bird, I can just output 1, right? And the mushroom is always second label,",
    "start": "441765",
    "end": "448530"
  },
  {
    "text": "I can always just output 2. Uh, and so then if it ever sees a mushroom, or a singer, or something, it'll always just know what to do.",
    "start": "448530",
    "end": "455370"
  },
  {
    "text": "It doesn't actually have to pay attention to the training dataset to figure out the ordering, right? And so it could actually- one,",
    "start": "455370",
    "end": "461700"
  },
  {
    "text": "it can learn to ignore the training data for each task because it can just output the label, and two, if it's given a new class and it doesn't know what the,",
    "start": "461700",
    "end": "471405"
  },
  {
    "text": "I- it- it is given a new semantic category, it doesn't know kind of the- the label corresponding to that class that it can't,",
    "start": "471405",
    "end": "479475"
  },
  {
    "text": "um, it can't figure out what- what class label that corresponds to. So let's look at another example that",
    "start": "479475",
    "end": "485460"
  },
  {
    "start": "485000",
    "end": "645000"
  },
  {
    "text": "might- may- might make this a little bit more clear. So say we want to, um, learn a pose predictor. Uh, and, uh, what we do is we, uh, if we really,",
    "start": "485460",
    "end": "493950"
  },
  {
    "text": "really wanna predict the orientation of an object and we wanna be able to adapt to a new object that we've never seen before.",
    "start": "493950",
    "end": "499500"
  },
  {
    "text": "Um, so say we have some task training data, uh, and we're given like four examples per",
    "start": "499500",
    "end": "506040"
  },
  {
    "text": "object labeled with the orientation of that object. Uh, and our goal is given four images,",
    "start": "506040",
    "end": "511320"
  },
  {
    "text": "we wanna be able to predict new- um, new orientations of that same exact object, right? [NOISE] Um, now, to solve this task,",
    "start": "511320",
    "end": "519120"
  },
  {
    "text": "one of the things that the learner could do is just remember, uh, what the canonical orientation of each object in the training set",
    "start": "519120",
    "end": "525360"
  },
  {
    "text": "is and then use that canonical orientation to output the pose of that object. Uh, so we can- could actually, um,",
    "start": "525360",
    "end": "531660"
  },
  {
    "text": "learn to ignore the training data and just kinda memorize the canonical orientation of the object and output, uh, the actual orientation.",
    "start": "531660",
    "end": "539530"
  },
  {
    "text": "Um, is this a problem? Well, this is fine for the objects in the training dataset and if it",
    "start": "539630",
    "end": "547050"
  },
  {
    "text": "sees a new couch that has the same canonical orientation, it's fine, right?",
    "start": "547050",
    "end": "552940"
  },
  {
    "text": "Uh, when is it bad? So this is a problem when you see new objects, right.",
    "start": "552950",
    "end": "559770"
  },
  {
    "text": "So um, if you're- for example, you are given a new, um, class of object that you haven't seen before you",
    "start": "559770",
    "end": "565140"
  },
  {
    "text": "don't know the canonical pose of that object, then because your meta-learner just memorized the canonical poses of all the-",
    "start": "565140",
    "end": "570240"
  },
  {
    "text": "all of the training objects and learned to ignore the data on the left. It can- it can't generalize this new setting",
    "start": "570240",
    "end": "576839"
  },
  {
    "text": "because it- it doesn't know how to use the data in order to figure out the canonical orientation. Um, so this is kind of bad when we're given an",
    "start": "576840",
    "end": "584495"
  },
  {
    "text": "unseen object with an unseen canonical orientation. Does this make sense?",
    "start": "584495",
    "end": "591329"
  },
  {
    "text": "Any questions about why this would fail? [NOISE]",
    "start": "595520",
    "end": "611339"
  },
  {
    "text": "Okay. So then the question is, like if this is a problem,",
    "start": "611340",
    "end": "617205"
  },
  {
    "text": "like basically we wanna be able to kind of train our post predictor and it could just memorize the pose of all the objects, of all the training objects, get perfect training error,",
    "start": "617205",
    "end": "623655"
  },
  {
    "text": "and then generalize poorly when it sees new objects because it doesn't know the canonical orientation of that object.",
    "start": "623655",
    "end": "629310"
  },
  {
    "text": "Um, I'll refer to this as the memorization problem because it can memorize the aspect of the training dataset in a way that doesn't allow it to- generalized to the test set.",
    "start": "629310",
    "end": "640425"
  },
  {
    "text": "Um, then the question is, can we do anything about this? Um, so let's try to actually formally think about this problem.",
    "start": "640425",
    "end": "646860"
  },
  {
    "start": "645000",
    "end": "860000"
  },
  {
    "text": "Um, so in the homeworks that you did, uh, for fuchsia in image classification where you were changing the labels.",
    "start": "646860",
    "end": "653145"
  },
  {
    "text": "Uh, it was true that a single function couldn't solve all of the tasks. Uh, and the reason for that is because you would change the kind",
    "start": "653145",
    "end": "660660"
  },
  {
    "text": "of class label corresponding to bird across different examples. So you can't, you- your thing I- uh,",
    "start": "660660",
    "end": "666180"
  },
  {
    "text": "it would be like, uh, you- you can't have a single function that will output a class label of two for bird or a class label of one for bird.",
    "start": "666180",
    "end": "673830"
  },
  {
    "text": "Um, and this is due to label reshuffling or- or hiding information from the iterate, like in the reinforced learning example,",
    "start": "673830",
    "end": "679260"
  },
  {
    "text": "you might hide the position of the object that you want to reach and it has to learn through- that through reinforcement.",
    "start": "679260",
    "end": "684600"
  },
  {
    "text": "Now, if the tasks are what- are what called non-mutually exclusive, that means that a single function actually can solve all the tasks.",
    "start": "684600",
    "end": "691904"
  },
  {
    "text": "Uh, and if a single function can solve for- solve all the task, there's actually multiple solutions to the meta-learning problem.",
    "start": "691905",
    "end": "698144"
  },
  {
    "text": "And the way that you can see this is if you view meta-learning as learning this function that takes this input training data and the input x.",
    "start": "698145",
    "end": "705210"
  },
  {
    "text": "Uh, what it could do is just choose to ignore that training data- uh, choose to ignore the training data just learn the function that takes this input the-",
    "start": "705210",
    "end": "712334"
  },
  {
    "text": "the image or takes as input the input and makes a prediction based on that. Um, so one solution is just memorize the canonical pose of the objects,",
    "start": "712335",
    "end": "721055"
  },
  {
    "text": "ignore the training data and output the pose of the training objects with our- our function.",
    "start": "721055",
    "end": "726335"
  },
  {
    "text": "Uh, and another solution would be to not carry any information about",
    "start": "726335",
    "end": "731420"
  },
  {
    "text": "the canonical pose in your meta-parameters Theta and acquire that information from the training data.",
    "start": "731420",
    "end": "737805"
  },
  {
    "text": "Um, and then if you see these two solutions, then you can actually see an entire spectrum of solutions based on how information flows,",
    "start": "737805",
    "end": "745620"
  },
  {
    "text": "based on how much information about the canonical pose of the object you're carrying in Theta versus carrying in- uh,",
    "start": "745620",
    "end": "752850"
  },
  {
    "text": "versus acquiring from the training data. Okay. Does this makes sense?",
    "start": "752850",
    "end": "761130"
  },
  {
    "text": "Any questions? Yeah. The other technical meaning of information flows?",
    "start": "761130",
    "end": "769150"
  },
  {
    "text": "So there is actually. And so this is um,",
    "start": "769280",
    "end": "774300"
  },
  {
    "text": "you can actually measure these sorts of quantities through mutual information uh, and other information theoretic quantities like uh,",
    "start": "774300",
    "end": "780839"
  },
  {
    "text": "in- in- information theory like entropy for example. Uh, some of the quantities basically that Sergei mentioned in his lecture.",
    "start": "780840",
    "end": "786285"
  },
  {
    "text": "Uh, one kind of formal definition that you can think of, uh, of when this is a problem is when",
    "start": "786285",
    "end": "792899"
  },
  {
    "text": "basically the mutual- the mutual information between, uh,",
    "start": "792900",
    "end": "798360"
  },
  {
    "text": "your prediction, uh, and your training data,",
    "start": "798360",
    "end": "803910"
  },
  {
    "text": "uh, given your input is equal to zero. Uh, and what this means is that- and maybe this is",
    "start": "803910",
    "end": "810750"
  },
  {
    "text": "also given the parameters of your function Theta. What this means is that, uh, the training data isn't affecting your prediction in any way,",
    "start": "810750",
    "end": "818579"
  },
  {
    "text": "uh, and then so their mutual information is zero. You're- you're basically- your prediction is only dependent on,",
    "start": "818580",
    "end": "824445"
  },
  {
    "text": "uh, Theta and x. Okay. So this is, uh, this kind of characterizes when- when you might just",
    "start": "824445",
    "end": "833250"
  },
  {
    "text": "memorize this sort of quantity and now one of the things that we could do about this if we take this view is that we could",
    "start": "833250",
    "end": "839580"
  },
  {
    "text": "potentially solve this problem by trying to control the information flow in a certain way, by basically trying to control whether or not the information comes from X,",
    "start": "839580",
    "end": "848550"
  },
  {
    "text": "whether or not the information is stored in Theta, or whether or not it comes from the training data.",
    "start": "848550",
    "end": "853630"
  },
  {
    "text": "Okay. Um, so one of the things that we can do,",
    "start": "855200",
    "end": "860520"
  },
  {
    "start": "860000",
    "end": "1268000"
  },
  {
    "text": "uh, is what we'll call meta-regularization, uh, which is to minimize the meta-training loss which is",
    "start": "860520",
    "end": "866220"
  },
  {
    "text": "our- our performance on the meta-training set, uh, but also the information that's stored in Theta.",
    "start": "866220",
    "end": "872520"
  },
  {
    "text": "And so for example, if we don't want it to memorize and store information about the canonical pose in Theta,",
    "start": "872520",
    "end": "877985"
  },
  {
    "text": "um, then we can minimize, uh, the meta-training loss and also minimize the amount of information in Theta",
    "start": "877985",
    "end": "884330"
  },
  {
    "text": "by using an information bottleneck by essentially constraining, uh, a distribution over Theta which will be parameterized by Theta Mu and",
    "start": "884330",
    "end": "891740"
  },
  {
    "text": "Theta Sigma to be close to this prior distribution that doesn't- um, that carries very little information.",
    "start": "891740",
    "end": "899555"
  },
  {
    "text": "Now, one of the things that's really important is to actually include this first term, uh, because if you only include the second term which is to minimize the information in Theta",
    "start": "899555",
    "end": "907415"
  },
  {
    "text": "then you wouldn't have any information in Theta and would do terribly on your meta-training tasks. Uh, and that's where that beta term comes in is that you need",
    "start": "907415",
    "end": "914240"
  },
  {
    "text": "to balance these two terms effectively. Um, and- and what this- what this actually does is for- um, for larger Beta,",
    "start": "914240",
    "end": "922170"
  },
  {
    "text": "this is gonna place precedence on using information in the training dataset over using information in your meta-parameters.",
    "start": "922170",
    "end": "929340"
  },
  {
    "text": "Okay. Uh, and then you can combine this with your favorite meta-learning algorithm, um,",
    "start": "930610",
    "end": "936769"
  },
  {
    "text": "and optimization-based algorithm or also like a black box, uh, adaptation approach. Is there a question?",
    "start": "936770",
    "end": "943580"
  },
  {
    "text": "Yes, I don't understand this- this second part. So how is the distribution over Theta obtained?",
    "start": "943580",
    "end": "949115"
  },
  {
    "text": "And is it that just like pushing the entire networking user-generated random noise versus just like [inaudible] .",
    "start": "949115",
    "end": "957464"
  },
  {
    "text": "Yeah, so the way we do this is um, you can for like an, uh, optimization-based algorithm like MAML,",
    "start": "957465",
    "end": "963160"
  },
  {
    "text": "what we do is we add noise, um, to the- to our weight vector that is, uh, uh,",
    "start": "963160",
    "end": "970545"
  },
  {
    "text": "has magnitude of, uh, Theta Sigma, uh, basically and that's the- that's the variance of- of the noise.",
    "start": "970545",
    "end": "976735"
  },
  {
    "text": "Um, and you learn both the mean and the variance, such that you do well on the train dataset and such that you,",
    "start": "976735",
    "end": "983260"
  },
  {
    "text": "um, try to stay close to this prior. You can't achieve both of these objectives and so if you only",
    "start": "983260",
    "end": "988300"
  },
  {
    "text": "use the second objective than you'd just be adding, um, you would just basically have random noise as parameters as Theta, uh,",
    "start": "988300",
    "end": "995410"
  },
  {
    "text": "and it's that first term that also encourages you to do well on the training dataset. Does that answer your question?",
    "start": "995410",
    "end": "1002339"
  },
  {
    "text": "Yeah, but there is an also Theta that, um, define how we process the training set as well.",
    "start": "1002340",
    "end": "1010899"
  },
  {
    "text": "Yeah, so that's a good point. So in this- in the case of MAML, Theta is both the meta parameters and also kind of the update rule,",
    "start": "1013340",
    "end": "1021915"
  },
  {
    "text": "um, like the way that you actually go about learning from the training dataset. Um, the update rule I guess it's- it's both from Theta and also the gradient",
    "start": "1021915",
    "end": "1030300"
  },
  {
    "text": "step and so disentangling those two is difficult to do in an approach like MAML,",
    "start": "1030300",
    "end": "1035655"
  },
  {
    "text": "um, in a black box approach. Um, and so in that case you find",
    "start": "1035655",
    "end": "1040679"
  },
  {
    "text": "that it just works best to do it on on all of the parameters. Um, in a black box approach where you have",
    "start": "1040680",
    "end": "1045795"
  },
  {
    "text": "separate pathways from the training dataset and X, um, and then you might have uh, some,",
    "start": "1045795",
    "end": "1053895"
  },
  {
    "text": "some intermediate variable which we'll call Z, um, and then you make a prediction. We find that it makes sense to put the regularization only on this pathway,",
    "start": "1053895",
    "end": "1062730"
  },
  {
    "text": "and not on the pathway from here, and that kind of allows you to do what you're suggesting which is to only regularize and minimize the content from,",
    "start": "1062730",
    "end": "1070485"
  },
  {
    "text": "from the input and not the pathway from the training data. But that's something that's a little bit more difficult to do in optimization-based,",
    "start": "1070485",
    "end": "1077670"
  },
  {
    "text": "like with like a MAML based approach. [inaudible] you just mask the data-",
    "start": "1077670",
    "end": "1083265"
  },
  {
    "text": "Mask the data. -to minimize performance when you're masking the data. Right, so you could also try to minimize performance when you mask the data, um.",
    "start": "1083265",
    "end": "1092085"
  },
  {
    "text": "Which is kind of like the general [inaudible] gonna explain a bit. Yeah, so this is the approach that does something somewhat like that, um,",
    "start": "1092085",
    "end": "1098880"
  },
  {
    "text": "called Task Agnostic Meta-Learning where you try to- what they tried to do is they,",
    "start": "1098880",
    "end": "1104580"
  },
  {
    "text": "um, say that, uh. So in MAML you have Theta prime equals Theta minus Alpha grad,",
    "start": "1104580",
    "end": "1110820"
  },
  {
    "text": "uh, something like this and what they tried to do is they say that F Theta should output a distribution that has maximum entropy.",
    "start": "1110820",
    "end": "1121440"
  },
  {
    "text": "Um, I think it's I guess what they're doing is maybe a little bit different from what you're saying.",
    "start": "1121440",
    "end": "1127755"
  },
  {
    "text": "I'm not sure what they say is basically this should have maximum entropy which corresponds to like minimum performance. Basically, it should output a uniform distribution over the labels.",
    "start": "1127755",
    "end": "1135375"
  },
  {
    "text": "Uh, in practice one of the things that I think an approach like that can do is is sort of cheat and pretend to like not have any information about the- the inputs.",
    "start": "1135375",
    "end": "1144195"
  },
  {
    "text": "But when you then take a gradient step it can real- it can kind of just flip a switch, such that it- it knows it can then like, it'll pretend that it doesn't know",
    "start": "1144195",
    "end": "1151830"
  },
  {
    "text": "anything but then once you flip that switch it's accessible to the- to function. Um, yeah and so if you do",
    "start": "1151830",
    "end": "1159270"
  },
  {
    "text": "something like Omniglot without label shuffling like I was describing before, which we call non-mutually exclusive Omniglot.",
    "start": "1159270",
    "end": "1165285"
  },
  {
    "text": "Um, MAML does pretty terribly, uh, on this in comparison to what happens when you shuffle the labels.",
    "start": "1165285",
    "end": "1172365"
  },
  {
    "text": "Uh, this- this approach that I just mentioned where they try to maximize the entropy of the output, also does quite poorly when you try to generalize to new,",
    "start": "1172365",
    "end": "1180870"
  },
  {
    "text": "um, settings because it's like basically, secretly hiding the fact that it's memorizing things. Um, whereas if you, um, regularize,",
    "start": "1180870",
    "end": "1188850"
  },
  {
    "text": "uh, the information in Theta like I mentioned before, you can do substantially better.",
    "start": "1188850",
    "end": "1194290"
  },
  {
    "text": "All right, um, and then um, likewise on the post prediction task the, um, uh,",
    "start": "1195500",
    "end": "1202020"
  },
  {
    "text": "if you compare MAML to the meta-regularized variant and also conditional neural processes which is a black box approach, um,",
    "start": "1202020",
    "end": "1209280"
  },
  {
    "text": "to the meta-meta regularized version of that, you can see significant improvements by, um, incorporating this objective or this regularizer.",
    "start": "1209280",
    "end": "1217140"
  },
  {
    "text": "Uh, and this isn't just- just as simple as standard regularization, so one of the things that we tried is adding this sort of standard regularization",
    "start": "1217140",
    "end": "1224700"
  },
  {
    "text": "on all of the parameters of conditional neural processes. Uh, and we find that if you do it on all the parameters in a fairly naive way,",
    "start": "1224700",
    "end": "1232050"
  },
  {
    "text": "you also do, uh, fairly poorly whereas if you just simply regularize, uh, the pathway that,",
    "start": "1232050",
    "end": "1237485"
  },
  {
    "text": "um, that- that you care about, then you're able to achieve good performance.",
    "start": "1237485",
    "end": "1243730"
  },
  {
    "text": "Okay. Any more questions on that before I move on?",
    "start": "1244310",
    "end": "1248830"
  },
  {
    "text": "Okay. So, um, the reason why I think this is important is that there are many settings where we can't necessarily shuffle",
    "start": "1252230",
    "end": "1258000"
  },
  {
    "text": "the labels and make the task non-mutually exclusive. And so I think that this work represents a step towards making",
    "start": "1258000",
    "end": "1263400"
  },
  {
    "text": "meta-learning algorithms more generally applicable to those kinds of settings. Um, next what if we have data that doesn't have task boundaries?",
    "start": "1263400",
    "end": "1273149"
  },
  {
    "start": "1268000",
    "end": "1618000"
  },
  {
    "text": "So, uh, in that setting we had task boundaries but the, um, we had these memorization problems but we had",
    "start": "1273150",
    "end": "1280070"
  },
  {
    "text": "these- we didn't know kind of what the underlying task structure was. Um, instead what if we simply have like a time series of data and we want to be able to",
    "start": "1280070",
    "end": "1287685"
  },
  {
    "text": "meta-learn from that time series of data in a way that allows us to make predictions, um, about new tasks very quickly.",
    "start": "1287685",
    "end": "1295005"
  },
  {
    "text": "And so some examples of this might be if we want to predict energy demand, uh, or if we want to,",
    "start": "1295005",
    "end": "1300855"
  },
  {
    "text": "uh, predict the dynamics of a robot, or of a car, um, you want to predict transportation usage across time, uh,",
    "start": "1300855",
    "end": "1308070"
  },
  {
    "text": "if you want to make some money, predict the stock market, um, video analytics of reinforcement learning agent.",
    "start": "1308070",
    "end": "1313620"
  },
  {
    "text": "All of these, uh, situations are examples where you have unsegmented time series of data,",
    "start": "1313620",
    "end": "1319740"
  },
  {
    "text": "but there is significant temporal structure. So there's structure that, um, that affects- that the nature of that data, uh, for example,",
    "start": "1319740",
    "end": "1328575"
  },
  {
    "text": "energy demand is going to be different if the weather is in different situate- is in,",
    "start": "1328575",
    "end": "1333794"
  },
  {
    "text": "um, if the weather is very hot versus very cold, um, the dynamics of a car can be varied if you're on like a grass terrain versus,",
    "start": "1333795",
    "end": "1340965"
  },
  {
    "text": "uh, a very, uh, slow- a very paved terrain, transportation usage is going to vary based off of",
    "start": "1340965",
    "end": "1347700"
  },
  {
    "text": "different sports events or or whatever. So there's significant temporal structure that, um, that governs the- these processes and we should be able to",
    "start": "1347700",
    "end": "1358440"
  },
  {
    "text": "actually recognize that temporal structure in those time series and latch onto it in order to make effective predictions.",
    "start": "1358440",
    "end": "1365265"
  },
  {
    "text": "Um, and so what we're gonna try and do is learn to segment the time series into tasks and then meta- learn across those tasks.",
    "start": "1365265",
    "end": "1372120"
  },
  {
    "text": "Uh, so then the key question is how do you go about segmenting the data? So, um, in this work which was led by,",
    "start": "1372120",
    "end": "1380415"
  },
  {
    "text": "uh, James Harrison and Apoorva Sharma, uh, one thing they looked into is an approach called Bayesian Online change point detection, um,",
    "start": "1380415",
    "end": "1387180"
  },
  {
    "text": "which assumes that you have some discrete task switches according to some probability.",
    "start": "1387180",
    "end": "1392235"
  },
  {
    "text": "Uh, so you might have data that looks like this where you have data according to some process and then there's a discrete switch and then you see,",
    "start": "1392235",
    "end": "1398385"
  },
  {
    "text": "um, data from another distribution. And then you maintain a belief over the duration of",
    "start": "1398385",
    "end": "1405000"
  },
  {
    "text": "your current task as well as a posterior distribution for your model for each of those durations.",
    "start": "1405000",
    "end": "1412080"
  },
  {
    "text": "Uh, and then what you can do, um, so for example, the duration will be something like this so you're kind of, it's being incremented by 1",
    "start": "1412080",
    "end": "1419370"
  },
  {
    "text": "until there's a discrete task switch, in which case the run-length will be back to 0.",
    "start": "1419370",
    "end": "1424455"
  },
  {
    "text": "And then what you can do with this is when you have your belief over your task distribution you can recursively update that belief using the performance of your model.",
    "start": "1424455",
    "end": "1433230"
  },
  {
    "text": "So if your perform- if your model's performing very well that means that you're probably in the same segment, whereas if your model is performing, um,",
    "start": "1433230",
    "end": "1440235"
  },
  {
    "text": "worse that it's more likely that you have switched to a different task.",
    "start": "1440235",
    "end": "1445785"
  },
  {
    "text": "Okay, so that's kind of a very brief and crude  overview",
    "start": "1445785",
    "end": "1450930"
  },
  {
    "text": "of Bayesian Online Changepoint Detection. One of the things that's neat about it is that these recursive updates are completely differentiable.",
    "start": "1450930",
    "end": "1457335"
  },
  {
    "text": "Uh, and so what that means is that you can actually backprop through the update, um, of your belief over the task duration to update your,",
    "start": "1457335",
    "end": "1466050"
  },
  {
    "text": "um, to kind of to train your model or or to meta-train your model. So you can simultaneously learn these change points as well as the prior,",
    "start": "1466050",
    "end": "1474600"
  },
  {
    "text": "um, across these change points using meta-learning. Um, and so, uh, the name that, uh,",
    "start": "1474600",
    "end": "1482669"
  },
  {
    "text": "that James and Apoorva came up for this is MOCA or Meta-Learning with Online Changepoint Analysis, uh,",
    "start": "1482670",
    "end": "1488370"
  },
  {
    "text": "and what they found is that, uh, the problem setting that they're considering is first during meta-training you're given an unsegmented time series of data,",
    "start": "1488370",
    "end": "1495029"
  },
  {
    "text": "um, of offline data. You want to be able to simultaneously learn how to recognize changepoints and",
    "start": "1495030",
    "end": "1500940"
  },
  {
    "text": "to learn priors to- for few shot learning within those segments of data. And then at meta test-time you're given a streaming source of",
    "start": "1500940",
    "end": "1508440"
  },
  {
    "text": "online data and you want to simultaneously learn from that data and make predictions from that data.",
    "start": "1508440",
    "end": "1513764"
  },
  {
    "text": "It's typical to the typical online learning problem setting. All right, so what kind of an example of how it works is they first looked at a Sinusoid",
    "start": "1513765",
    "end": "1522240"
  },
  {
    "text": "regression setting where there is discrete shifts in this underlying sinusoid curve, uh, and what this is showing is on the bottom is the belief over the,",
    "start": "1522240",
    "end": "1530760"
  },
  {
    "text": "uh, the current run length of the task segment. And what the top is showing is that, uh,",
    "start": "1530760",
    "end": "1536820"
  },
  {
    "text": "the green is showing you the curves from the previous, um, previous timesteps and the red is showing- well",
    "start": "1536820",
    "end": "1542850"
  },
  {
    "text": "the green and the red are showing that the points from the current- previous time steps, um, and the red ones are the ones from the current task,",
    "start": "1542850",
    "end": "1548940"
  },
  {
    "text": "er, as labeled by an oracle. And so we can see is that it can very rapidly recognize when there are Changepoints, uh,",
    "start": "1548940",
    "end": "1556835"
  },
  {
    "text": "based on its belief over the function and also very quickly adapt to that, so even with just a couple of points it can figure out what the sinusoid",
    "start": "1556835",
    "end": "1563510"
  },
  {
    "text": "is corresponding to that, um, corresponding to that data point.",
    "start": "1563510",
    "end": "1568740"
  },
  {
    "text": "Okay, um, and then they also developed a variant of Mini-ImageNet, that is also kind of streaming, uh,",
    "start": "1568940",
    "end": "1576029"
  },
  {
    "text": "data where you don't know the task boundaries during meta training, uh, and in comparison to,",
    "start": "1576030",
    "end": "1583185"
  },
  {
    "text": "um, this training on all the tasks, uh, which, um, which is the kind of the black line on the bottom,",
    "start": "1583185",
    "end": "1589335"
  },
  {
    "text": "um, this is able to do significantly better. So the x-axis here is showing the hazard rate or how frequently the task and switching but basically,",
    "start": "1589335",
    "end": "1595590"
  },
  {
    "text": "the probability that the task will switch and the y-axis is showing accuracy. Uh, and each of the approaches in red are basically using a sliding window",
    "start": "1595590",
    "end": "1603210"
  },
  {
    "text": "to basically meta-training across all of the data points within, um, a fixed window size.",
    "start": "1603210",
    "end": "1608535"
  },
  {
    "text": "So they actually tried to predict where these Changepoints are, it's able to do significantly better than just using this naive sliding window approach.",
    "start": "1608535",
    "end": "1616815"
  },
  {
    "text": "Okay. Cool. So this means that we can use data without just kind of a time series of data without any task boundaries.",
    "start": "1616815",
    "end": "1624645"
  },
  {
    "start": "1618000",
    "end": "2192000"
  },
  {
    "text": "Now, can we take this one step further and just assume that we have, um, not even time series data with temporal structure but just an",
    "start": "1624645",
    "end": "1632414"
  },
  {
    "text": "unlabeled data set of images, for example. Can the algorithm come up with its own tasks in",
    "start": "1632415",
    "end": "1638580"
  },
  {
    "text": "a way that prepares it for future downstream tasks? Um, so let's first look at the-",
    "start": "1638580",
    "end": "1644205"
  },
  {
    "text": "the unsupervised case where we have images and then we'll look at the reinforcement learning case.",
    "start": "1644205",
    "end": "1650100"
  },
  {
    "text": "Um, so let's say that we have unlabeled images, typically we assume that we have- we construct tasks with, um, with labeled data.",
    "start": "1650100",
    "end": "1656220"
  },
  {
    "text": "Um, so the way that we're going to try to approach this problem, is we're gonna try to construct the tasks,",
    "start": "1656220",
    "end": "1661755"
  },
  {
    "text": "uh, construct these kinds of tasks but with unlabeled data. Um, and so to do that, uh,",
    "start": "1661755",
    "end": "1668580"
  },
  {
    "text": "we need to in some ways kind of group the data into different images and then also label the images within those groupings.",
    "start": "1668580",
    "end": "1675375"
  },
  {
    "text": "Uh, so what we'll do is to group the data we will- we can't group the data in pixel space. We can't, like, run like K means in pixel space, that wouldn't work very well.",
    "start": "1675375",
    "end": "1683370"
  },
  {
    "text": "So what we'll do, is we'll first run unsupervised learning to get a low dimensional embedding space.",
    "start": "1683370",
    "end": "1688860"
  },
  {
    "text": "Um, and that will allow us to take some images embed them into some low dimensional space, and then we'll propose tasks by clustering within this low dimensional latent space.",
    "start": "1688860",
    "end": "1699270"
  },
  {
    "text": "Uh, so for example one set of clusters might look like this, another set of clusters might look like this, so we can cluster the data multiple",
    "start": "1699270",
    "end": "1705660"
  },
  {
    "text": "times in order to create different groupings of the data. And then to get a task we can sample these different groupings,",
    "start": "1705660",
    "end": "1712725"
  },
  {
    "text": "uh, and treat each of those groupings as a different class label. So say, um, we wanted to create a, uh,",
    "start": "1712725",
    "end": "1720450"
  },
  {
    "text": "a two way classification task, we can sample two classes maybe the red class and the blue class,",
    "start": "1720450",
    "end": "1725595"
  },
  {
    "text": "sample those two images uh, uh, sample two images from those two groupings,",
    "start": "1725595",
    "end": "1730635"
  },
  {
    "text": "treat that as a training data set, uh, sample two more images and treat that as the test set. Uh, this would allow you to create a one shot,",
    "start": "1730635",
    "end": "1738090"
  },
  {
    "text": "two way classification task. Uh, and you can repeat this process and for example sample the green cluster,",
    "start": "1738090",
    "end": "1744090"
  },
  {
    "text": "and the purple cluster to create more, um, to create more tasks and so on.",
    "start": "1744090",
    "end": "1750015"
  },
  {
    "text": "Okay. Um, and so this gives us, uh, a set of tasks and then once we have",
    "start": "1750015",
    "end": "1755240"
  },
  {
    "text": "that we can simply run our favorite meta learning algorithm. Be it uh, uh, a black box approach, an optimization based approach or a non parametric approach.",
    "start": "1755240",
    "end": "1763255"
  },
  {
    "text": "Okay. Um, and then the result of this process is, uh, if you do something like MAML as a representation that's particularly well",
    "start": "1763255",
    "end": "1770430"
  },
  {
    "text": "suited for few shot learning. Was there a question? Yes. So in this case they're a number of tasks are equal",
    "start": "1770430",
    "end": "1779595"
  },
  {
    "text": "to the number of clusters in the [inaudible] setting?",
    "start": "1779595",
    "end": "1785280"
  },
  {
    "text": "So in this case the total number of tasks corresponds to the total number of clusters choose the number of ways.",
    "start": "1785280",
    "end": "1791159"
  },
  {
    "text": "So if you wanna, um, create five way classification tasks and you have, um, 1,000 clusters, then you could create 1,000 choose five tasks.",
    "start": "1791160",
    "end": "1799960"
  },
  {
    "text": "And so in the case the number, uh, sorry the ID of the tasks for the [inaudible] doesn't necessarily correspond to the",
    "start": "1800840",
    "end": "1810690"
  },
  {
    "text": "classification label how to deal with this uh, uh, [inaudible]?",
    "start": "1810690",
    "end": "1817769"
  },
  {
    "text": "So in this case the- like each like cluster may not correspond to an actual semantic class label. So for example, um,",
    "start": "1817770",
    "end": "1824235"
  },
  {
    "text": "we found clusters that correspond to pairs of objects or round objects for example, um, and in general kind of the idea",
    "start": "1824235",
    "end": "1832380"
  },
  {
    "text": "behind this algorithm is that they don't actually need unnecessarily correspond to exactly class labels. What's important is that they-that they correspond to structured,",
    "start": "1832380",
    "end": "1840120"
  },
  {
    "text": "uh, structured aspects of the visual data, such that the meta learner can figure out- can recog- learn to recognize",
    "start": "1840120",
    "end": "1845850"
  },
  {
    "text": "that structure very quickly from only a few examples and make predictions. And so for example you could imagine, um,",
    "start": "1845850",
    "end": "1852975"
  },
  {
    "text": "basically practicing for few shot learning by practicing with these other sorts of concepts like round and pairs of objects,",
    "start": "1852975",
    "end": "1858419"
  },
  {
    "text": "in a way that allows you to, um, then at test time when you're given a very small data set,",
    "start": "1858420",
    "end": "1863640"
  },
  {
    "text": "be able to adapt very quickly to that data set. So the down- after you do this, after you, uh,",
    "start": "1863640",
    "end": "1871304"
  },
  {
    "text": "provide meta learning you'll be given a downstream task a very small data set for that downstream task and your goal is to quickly learn that task.",
    "start": "1871305",
    "end": "1877785"
  },
  {
    "text": "Uh, and that data will actually correspond to like ground truth, um, ground truth semantic labels.",
    "start": "1877785",
    "end": "1884110"
  },
  {
    "text": "Okay. So, um, there's a few different design decisions in this approach,",
    "start": "1884630",
    "end": "1891590"
  },
  {
    "text": "um, for example we needed to choose an unsupervised learning algorithm, um, there were a few options. Two of the front runners that we used were BiGAN or, um,",
    "start": "1891590",
    "end": "1899390"
  },
  {
    "text": "or DeepCluster, uh, that we proposed tasks by- by clustering.",
    "start": "1899390",
    "end": "1904545"
  },
  {
    "text": "Um, and as with most machine learning papers these days you need to come up with a fun, uh, acronym for your methods and we call it clustering to automatically",
    "start": "1904545",
    "end": "1911580"
  },
  {
    "text": "construct tasks for unsupervised meta learning or CACTUs, uh, and then we run our- run different meta algorithms on it.",
    "start": "1911580",
    "end": "1919559"
  },
  {
    "text": "Uh, and if you're curious what the combination of a CACTUs and MAML looks like you get something like this and, uh, let's see how it does.",
    "start": "1919560",
    "end": "1928590"
  },
  {
    "text": "So, uh, we've looked at, um, Mini-ImageNet, uh, and if you give Mini-ImageNet like full labels,",
    "start": "1928590",
    "end": "1937174"
  },
  {
    "text": "uh, with MAML as kind of a control. Uh, it gets around 62% success. Uh, this is with like a fully labeled meta training data set,",
    "start": "1937175",
    "end": "1944925"
  },
  {
    "text": "and then all the other methods will not use any of the labels in the meta-training data set, they just get to see the images.",
    "start": "1944925",
    "end": "1950730"
  },
  {
    "text": "They don't know anything about how those images correspond to classes. Um, and then kind of the key question is",
    "start": "1950730",
    "end": "1956820"
  },
  {
    "text": "how does this sort of approach this sort of unsupervised meta learning approach, compare to just using the representation of these approaches directly?",
    "start": "1956820",
    "end": "1963735"
  },
  {
    "text": "Um, so for example, we can take the BiGAN representation that's learned, uh, and do k nearest neighbors in that representation with the downstream data.",
    "start": "1963735",
    "end": "1971670"
  },
  {
    "text": "Uh, and if we do that we get something around 31% test- meta test accuracy.",
    "start": "1971670",
    "end": "1977805"
  },
  {
    "text": "Uh, we can also do logistic regression on top of that representation, and if we do that we get around 34% success.",
    "start": "1977805",
    "end": "1983250"
  },
  {
    "text": "Uh, we also tried using a neural network on top of that representation and then using dropout to regularize it, uh, and that actually did a little bit worse than using logistic regression.",
    "start": "1983250",
    "end": "1991635"
  },
  {
    "text": "Um, you can imagine an approach where you try to match the clusters, we- if you try to basically want to use the clustering that you do here,",
    "start": "1991635",
    "end": "1998100"
  },
  {
    "text": "you could try to match the clusters, uh, with that representation to the- the theta in your test task training set.",
    "start": "1998100",
    "end": "2006904"
  },
  {
    "text": "Uh, and if you do that you get around- also around 29% success. Um, and in comparison if you do, uh,",
    "start": "2006905",
    "end": "2013520"
  },
  {
    "text": "if you do MAML on top of the-the clustered BiGAN representation you get something like 51% accuracy.",
    "start": "2013520",
    "end": "2020900"
  },
  {
    "text": "So by explicitly training it for the ability to quickly adapt to tasks, it's able to learn a prior that can effectively",
    "start": "2020900",
    "end": "2027590"
  },
  {
    "text": "transfer to downstream few-shot learning tasks. Uh, and if you combine it with DeepCluster you do,",
    "start": "2027590",
    "end": "2033395"
  },
  {
    "text": "uh, a little bit better than that as well. And one of the things that was interesting was that, uh, this result didn't just hold for Mini-ImageNet, uh,",
    "start": "2033395",
    "end": "2041135"
  },
  {
    "text": "it also held for four different embedding methods, four different unsupervised learning approaches,",
    "start": "2041135",
    "end": "2046429"
  },
  {
    "text": "uh, across four different data sets Omniglot, CelebA, minilmageNet, and MNIST. Um, two meta learning methods, MAML and",
    "start": "2046430",
    "end": "2053389"
  },
  {
    "text": "prototypical networks although ProtoNets under-performed in a couple of cases. Uh, and it also worked well with test tasks that had larger data sets.",
    "start": "2053390",
    "end": "2060605"
  },
  {
    "text": "So for example, if you're in the 50 shot case rather than in the 5 shot case, uh, this - this approach can also perform well. Yeah.",
    "start": "2060605",
    "end": "2070760"
  },
  {
    "text": "It seems as though like, you know, [inaudible] the [inaudible] embedding space like the prototypes.",
    "start": "2070760",
    "end": "2077179"
  },
  {
    "text": "Would you [inaudible] use that to generate that [inaudible] . Yeah, one thing that's interesting here is,",
    "start": "2077180",
    "end": "2083629"
  },
  {
    "text": "can you then kind of after you run meta-learning like close the loop and use that to refine the tasks. Um, we for something- we- we add",
    "start": "2083630",
    "end": "2092000"
  },
  {
    "text": "ma- we added prototypical networks as a somewhat last-minute addition to this paper. Um, with MAML we found that if we re-use the MAML representation,",
    "start": "2092000",
    "end": "2098630"
  },
  {
    "text": "it actually wasn't good for proposing tasks which suggested to us that one- basically these sort of embeddings are very good for proposing tasks,",
    "start": "2098630",
    "end": "2106085"
  },
  {
    "text": "and the meta-learning embeddings are very good for, basically adapting to test tasks and they- these two sorts of purposes aren't necessarily,",
    "start": "2106085",
    "end": "2112505"
  },
  {
    "text": "uh, like, uh, useful for the same sorts of things. Um, for something like protonets, uh,",
    "start": "2112505",
    "end": "2118234"
  },
  {
    "text": "that's not an experiment that we ran but, it can be pretty interesting to try. And conceivably it does seem like,",
    "start": "2118235",
    "end": "2123335"
  },
  {
    "text": "you should be able to improve with the sort of iterative, uh, iterative task proposal.",
    "start": "2123335",
    "end": "2127920"
  },
  {
    "text": "Okay. So now what about- what abou- what about, um, unsupervised meta-reinforcement learning?",
    "start": "2128680",
    "end": "2135080"
  },
  {
    "text": "So this is a paper that, um, are kind of an approach that Sergey very briefly touched on in his guest lecture.",
    "start": "2135080",
    "end": "2141050"
  },
  {
    "text": "I'll try to go on in a little bit more depth than that. And what the general recipe will look like is very similar",
    "start": "2141050",
    "end": "2147289"
  },
  {
    "text": "to the- the previous approach which is, we're going to be given an environment, we want to be able to, uh,",
    "start": "2147290",
    "end": "2153935"
  },
  {
    "text": "propose tasks in an unsupervised way and then run meta-reinforcement learning on those tasks.",
    "start": "2153935",
    "end": "2159980"
  },
  {
    "text": "Uh, and then the results of that will be, uh, a reinforcement learning algorithm or reinforcement learning procedure that",
    "start": "2159980",
    "end": "2166849"
  },
  {
    "text": "is particularly tuned for a given environment. Uh, and then for that environment if you're given a new reward function from a human,",
    "start": "2166850",
    "end": "2175970"
  },
  {
    "text": "you [NOISE] should be able to very quickly maximize that reward function with a small amount of experience.",
    "start": "2175970",
    "end": "2181415"
  },
  {
    "text": "Uh, so then the key question is how do you go about proposing tasks? Uh, one very, uh,",
    "start": "2181415",
    "end": "2189140"
  },
  {
    "text": "naive approach that you could imagine doing, uh, but maybe still does something reasonable is to use Random Task Proposals.",
    "start": "2189140",
    "end": "2195665"
  },
  {
    "start": "2192000",
    "end": "2279000"
  },
  {
    "text": "Uh, so in particular what you could do is you can just randomly initialize neural networks,",
    "start": "2195665",
    "end": "2200960"
  },
  {
    "text": "uh, that basically classify whether or not a state corresponds to a given task.",
    "start": "2200960",
    "end": "2206540"
  },
  {
    "text": "Uh, and so in particular what we'll do is we'll have, uh, some discriminator that outputs a, uh,",
    "start": "2206540",
    "end": "2212090"
  },
  {
    "text": "logits over a discrete random variable, uh, and if, uh, the probability of that corresponding",
    "start": "2212090",
    "end": "2218690"
  },
  {
    "text": "to task one is high then the reward function for that, uh, task will also be high and likewise for- for other tasks.",
    "start": "2218690",
    "end": "2225845"
  },
  {
    "text": "So it's basically you'll try to classify which of a discrete set of tasks each state belongs to. Um, and- and in this case,",
    "start": "2225845",
    "end": "2233510"
  },
  {
    "text": "D is just gonna be a randomly initialized neural network and one of the things you could imagine this doing is essentially randomly partitioning the state space into different regions.",
    "start": "2233510",
    "end": "2241955"
  },
  {
    "text": "Such that one, one of these partitions corresponds to one task, another partition corresponds to another task.",
    "start": "2241955",
    "end": "2248609"
  },
  {
    "text": "Okay. Uh, and one of the things that's really important to note here, is this is random functions over the state-space not over the policy space.",
    "start": "2248740",
    "end": "2256520"
  },
  {
    "text": "Uh, and so, uh, and the reason why this is important is that if there are random functions over the policy space, uh,",
    "start": "2256520",
    "end": "2263105"
  },
  {
    "text": "then you'd kind of have an exponential search problem over the policy that kind of visits, um, a set of states.",
    "start": "2263105",
    "end": "2268925"
  },
  {
    "text": "Whereas if it's a random reward function over a state-space, uh, you instead just have a polynomial search problem.",
    "start": "2268925",
    "end": "2274980"
  },
  {
    "text": "Okay. Uh, so this is kind of the first naive thing that we could do, uh, is there something that we can do that's a bit better than this?",
    "start": "2275080",
    "end": "2283430"
  },
  {
    "start": "2279000",
    "end": "2379000"
  },
  {
    "text": "Uh, and in particular one thing that we can imagine doing is trying to actually, uh, make a set of, uh,",
    "start": "2283430",
    "end": "2288680"
  },
  {
    "text": "tasks that are more diverse from one another. Uh, and in particular the way that we can do this is try to make tasks that are discriminable from each other,",
    "start": "2288680",
    "end": "2295700"
  },
  {
    "text": "skills that look different from one another. The way that you can go about doing this is actually very similar to the- approach, uh,",
    "start": "2295700",
    "end": "2302510"
  },
  {
    "text": "CACTUs that I mentioned before which is to try to actually cluster skills into, uh, more discrete parts of your policy space.",
    "start": "2302510",
    "end": "2311525"
  },
  {
    "text": "Um, and now we can't, uh, in the reinforcement learning setting when you actually generate the- the tasks,",
    "start": "2311525",
    "end": "2317045"
  },
  {
    "text": "uh, generate the kind of skills. And so what we'll do, uh, is we'll use an approach, uh, called diversity is all you need or DIAYN.",
    "start": "2317045",
    "end": "2323495"
  },
  {
    "text": "Ah, this is an approach that Sergey covered in his guest lecture where you, um, have an agent or policy that takes as input,",
    "start": "2323495",
    "end": "2331460"
  },
  {
    "text": "uh, a discrete skill Z, it produces actions and you have a discriminator that takes as input, um,",
    "start": "2331460",
    "end": "2338870"
  },
  {
    "text": "a sequence a, a, a, a state and tries to predict which skill was passed into the policy.",
    "start": "2338870",
    "end": "2346085"
  },
  {
    "text": "And then what you have is you have a cooperative game where the policy tries to visit which states are discriminable by the discriminator and",
    "start": "2346085",
    "end": "2353660"
  },
  {
    "text": "the discriminator tries to predict the skill from the state. So the objective of both of these is to maximize the accuracy of the discriminator.",
    "start": "2353660",
    "end": "2362070"
  },
  {
    "text": "Uh, and then once you- once you have this discriminator, uh, then you can have the reward function for unsupervised meta-learning to simply",
    "start": "2362230",
    "end": "2369559"
  },
  {
    "text": "be the likelihood of one of those skills given the state. [NOISE] So here are some examples of the acquired skills that,",
    "start": "2369560",
    "end": "2382055"
  },
  {
    "start": "2379000",
    "end": "2577000"
  },
  {
    "text": "um, it acquires through this approach. And so basically what this- what this approach corresponds to is learning these skills with this sort of",
    "start": "2382055",
    "end": "2389329"
  },
  {
    "text": "discriminability metric or objective and then runs meta-learning across these skills.",
    "start": "2389330",
    "end": "2395090"
  },
  {
    "text": "With this- with each, with each of these skills as the meta-training tasks.",
    "start": "2395090",
    "end": "2399660"
  },
  {
    "text": "Okay. So does this work, um, the short answer is ah, yes.",
    "start": "2400150",
    "end": "2408835"
  },
  {
    "text": "So if you run, if you run this algorithm and then give it some downstream tasks specified by a human. For example, navigating different 2D positions or running in different directions or uh,",
    "start": "2408835",
    "end": "2418700"
  },
  {
    "text": "running to certain points in space. Uh, the kind of the- the result of the meta-learning algorithm is something that can",
    "start": "2418700",
    "end": "2426200"
  },
  {
    "text": "much more quickly solve these downstream test tasks then learning from scratch.",
    "start": "2426200",
    "end": "2431510"
  },
  {
    "text": "And one of the things that's particularly interesting here is that even in training with a random tasks which is shown in blue it's",
    "start": "2431510",
    "end": "2437240"
  },
  {
    "text": "able to do significantly better than training from scratch, uh, in two of the three cases, and also the, um,",
    "start": "2437240",
    "end": "2444619"
  },
  {
    "text": "if you use the diversity driven skills shown in green it's able to learn, uh, much, uh,",
    "start": "2444620",
    "end": "2449690"
  },
  {
    "text": "very quickly in all three cases.",
    "start": "2449690",
    "end": "2452730"
  },
  {
    "text": "Yeah.",
    "start": "2455260",
    "end": "2462700"
  },
  {
    "text": "[inaudible] Do you think you know, how do you think they could be advised [inaudible]",
    "start": "2462700",
    "end": "2469940"
  },
  {
    "text": "Yeah, I think tha- that's a great question I'd love to see more work on- on manipulation and in particular, I think that exploration and manipulation is very challenging.",
    "start": "2469940",
    "end": "2477020"
  },
  {
    "text": "Uh, because in- in locomotion like exploration is just like visiting different parts of the state space very clearly.",
    "start": "2477020",
    "end": "2482960"
  },
  {
    "text": "Whereas in something like manipulation, uh, if you want to manipulate objects like just waving your hands around randomly isn't actually a very good exploration strategy,",
    "start": "2482960",
    "end": "2490310"
  },
  {
    "text": "even though it is exploring a very large part of the state space for your arms. And the things that do actually matter are",
    "start": "2490310",
    "end": "2495650"
  },
  {
    "text": "the objects and doing interesting things with the objects. So I think that these kinds of approaches, um,",
    "start": "2495650",
    "end": "2500810"
  },
  {
    "text": "would struggle if like these kinds of exploration purchase, if you apply them to manipulation settings may not do as well.",
    "start": "2500810",
    "end": "2506839"
  },
  {
    "text": "Uh, and we may need, uh, basically different tactics for exploration in order to do that.",
    "start": "2506840",
    "end": "2512464"
  },
  {
    "text": "Uh, one of the things that I think is exciting about meta-learning in the context of exploration is it seems like you should be able to learn",
    "start": "2512465",
    "end": "2518750"
  },
  {
    "text": "exploration strategy that's particularly tuned to a domain. So for example, if in locomotion versus in navigation versus",
    "start": "2518750",
    "end": "2524900"
  },
  {
    "text": "in manipulation it's probably the different exploration strategies are actually better for different domains. One is very object driven and the other is very kind of, uh, region driven.",
    "start": "2524900",
    "end": "2533660"
  },
  {
    "text": "Uh, it seems like you should be able to- instead of trying to have a hand-coded algorithm that is good at all these different domains,",
    "start": "2533660",
    "end": "2540319"
  },
  {
    "text": "you should instead be able to learn an exploration approach for that's particularly tailored towards each of these domains.",
    "start": "2540320",
    "end": "2546930"
  },
  {
    "text": "Okay. Um, so I guess for me the- one of the interesting things here is that it seems like relatively simple mechanisms for proposing tasks work surprisingly well.",
    "start": "2548620",
    "end": "2557569"
  },
  {
    "text": "Um, but it- it's possible that these sorts of things may not work well in- in- in a manipulation setting.",
    "start": "2557570",
    "end": "2562940"
  },
  {
    "text": "Uh, the other barrier I think for manipulation settings is that there aren't a lot of good environments that are for manipulation,",
    "start": "2562940",
    "end": "2569690"
  },
  {
    "text": "that are kind of very well-grounded in the field and very well, um, widely used.",
    "start": "2569690",
    "end": "2575099"
  },
  {
    "text": "Okay. So um, some takeaways from this part of the lecture is that, uh,",
    "start": "2575410",
    "end": "2581660"
  },
  {
    "start": "2577000",
    "end": "2652000"
  },
  {
    "text": "you can learn priors for few-shot adaptation using a variety of data sources including non-mutually exclusive tasks,",
    "start": "2581660",
    "end": "2588140"
  },
  {
    "text": "including unsegmented time-series data, and using, um, unlabeled data and unlabeled experience.",
    "start": "2588140",
    "end": "2595025"
  },
  {
    "text": "And the way that we could do this is through meta-regularization, um, with end-to-end change point detection and",
    "start": "2595025",
    "end": "2600875"
  },
  {
    "text": "using clustering of your skills or your data points. Uh, and in general I think that all of these approaches should make it",
    "start": "2600875",
    "end": "2607940"
  },
  {
    "text": "significantly easier to deploy meta-learning algorithms. Um, but at the same time, I don't think that these are necessarily the final approaches for each of these problems.",
    "start": "2607940",
    "end": "2615635"
  },
  {
    "text": "Um, all of this work is- is extremely recent. I think that there's there's really a lot more work to be done to make it easy to- to deploy meta-learning algorithms.",
    "start": "2615635",
    "end": "2624195"
  },
  {
    "text": "Okay. Um, so now I want to talk about what does it",
    "start": "2624195",
    "end": "2629330"
  },
  {
    "text": "take to actually run these algorithms on very qualitatively distinct skills? Uh, and the motivation here is that in, um,",
    "start": "2629330",
    "end": "2637805"
  },
  {
    "text": "in a lot of the kind of well-conditioned reinforcement learning, uh, multi-task reinforcement learning and meta-reinforcement learning.",
    "start": "2637805",
    "end": "2644330"
  },
  {
    "text": "We see that the different tasks just correspond to different variations of the same skill rather than completely distinct skills.",
    "start": "2644330",
    "end": "2650790"
  },
  {
    "text": "Um, and so if you think about whether or not things like MAML and PEARL had actually accomplish our goal of making policy adaptation very fast.",
    "start": "2650920",
    "end": "2658789"
  },
  {
    "start": "2652000",
    "end": "2955000"
  },
  {
    "text": "Uh, I think that there's sort of accomplished this goal, uh, to some degree and that we can kind of very quickly adapt to new situations,",
    "start": "2658790",
    "end": "2665420"
  },
  {
    "text": "but they haven't shown the ability to adapt to entirely new tasks, Uh, and one of the challenges in doing this is that,",
    "start": "2665420",
    "end": "2672560"
  },
  {
    "text": "uh, if we want to adopt entirely new tasks, uh, we also know that we need the kind of the task distributions that meta-training meta-test time to be the same.",
    "start": "2672560",
    "end": "2680345"
  },
  {
    "text": "Um, and so what this means is that we need a very broad distribution of tasks for meta-training. Uh, so it seems like there's kind of two steps two things that we need to do here.",
    "start": "2680345",
    "end": "2689420"
  },
  {
    "text": "First is the ability to just meta-train over at least like two task families rather than just within a single task family.",
    "start": "2689420",
    "end": "2695750"
  },
  {
    "text": "Uh, and the second step is to meta-train across a very diverse set of tasks that allows for generalization to entirely new tasks.",
    "start": "2695750",
    "end": "2702545"
  },
  {
    "text": "Uh, so first I'd like to talk about the first point which is can we perform meta-training across multiple task families?",
    "start": "2702545",
    "end": "2709670"
  },
  {
    "text": "Uh, and in particular what we'll consider is, uh, the settings right here where we consider a space of",
    "start": "2709670",
    "end": "2715610"
  },
  {
    "text": "multiple manipulation tasks including things like grasping objects, pressing buttons, sliding objects across the table,",
    "start": "2715610",
    "end": "2722359"
  },
  {
    "text": "and stacking two objects. Uh, so I think it's, it's pretty fair to say that these four things are fairly distinct from one another and- and much more",
    "start": "2722360",
    "end": "2729470"
  },
  {
    "text": "distinct from one another than the task distributions that we saw earlier in this course.",
    "start": "2729470",
    "end": "2734690"
  },
  {
    "text": "And then our goal will be to learn- in this case, our goal will not be the generalized for history an entirely new task.",
    "start": "2734690",
    "end": "2740165"
  },
  {
    "text": "It will simply just to be, to generalize within this broader task distribution. Uh, and in particular, we're going, to learn a new variation of one of",
    "start": "2740165",
    "end": "2747289"
  },
  {
    "text": "these task families with a small number of trials and sparse rewards for that task. Okay.",
    "start": "2747290",
    "end": "2756485"
  },
  {
    "text": "Now, one problem that comes up when you start to think about this problem is that,",
    "start": "2756485",
    "end": "2762225"
  },
  {
    "text": "if you have sparse rewards, if you just have wa- like if the robot gets one when it accomplishes the task and zero otherwise,",
    "start": "2762225",
    "end": "2767850"
  },
  {
    "text": "then the robot's gonna have to explore every possible task to figure out which one is the one that it's supposed to be doing.",
    "start": "2767850",
    "end": "2774915"
  },
  {
    "text": "Um, so for example if you have like a robot right here, you want it to be able to quickly learn a task, you just want to be able to give it 1 when it did something good and 0 otherwise,",
    "start": "2774915",
    "end": "2782310"
  },
  {
    "text": "then we basically have to poke all the objects in the scene to figure out which- which is the thing that you- it- you want it to accomplish.",
    "start": "2782310",
    "end": "2788744"
  },
  {
    "text": "And it seems a bit silly, I guess we're kind of hiding, we're purposely hiding information from the robot,",
    "start": "2788745",
    "end": "2794805"
  },
  {
    "text": "uh, by not telling it what tasks we want it to do. Um, so the way that we're gonna try to overcome this is,",
    "start": "2794805",
    "end": "2800985"
  },
  {
    "text": "in particular try to give it an indication of what the task is. Uh, and- and then also given that indication how to figure out how to perform the task.",
    "start": "2800985",
    "end": "2808950"
  },
  {
    "text": "Uh, and in particular, we'll see if we can learn from one demonstration which we'll use to convey the-",
    "start": "2808950",
    "end": "2815145"
  },
  {
    "text": "what the task is and a few trials which will be used to figure out how to solve that task.",
    "start": "2815145",
    "end": "2820600"
  },
  {
    "text": "Okay. Um, and so kind of concretely what this might look like is that we're,",
    "start": "2820850",
    "end": "2826170"
  },
  {
    "text": "uh, it'll watch one demonstration as shown on the left. It will then try the task itself in a new situation.",
    "start": "2826170",
    "end": "2833190"
  },
  {
    "text": "So in this case, it will try to push the- the teacup over to the right. Um, in this case, this, uh, this wasn't quite the correct thing",
    "start": "2833190",
    "end": "2839790"
  },
  {
    "text": "to do and so it will learn from that- that a reward of zero from that situation to figure out that",
    "start": "2839790",
    "end": "2845400"
  },
  {
    "text": "what it should have done in that situation is to push the cup towards the teacup. Um, so in this case we're also introducing some amount of",
    "start": "2845400",
    "end": "2851700"
  },
  {
    "text": "ambiguity in the new situation since it has to figure out if it should push the object on the left towards the right or the object on the right towards the left.",
    "start": "2851700",
    "end": "2859815"
  },
  {
    "text": "Okay, um, then how can we train for this in a scalable way if we want to learn from demonstrations and trials?",
    "start": "2859815",
    "end": "2865740"
  },
  {
    "text": "Uh, the way that we'll do this is, we'll collect a few demonstrations for many different tasks. Uh, we need this in order to learn from- from demonstrations.",
    "start": "2865740",
    "end": "2873285"
  },
  {
    "text": "We'll then t- first train a one-shot imitation learning policy to be able to imitate the- the, um, to imitate the- the demonstration, uh, for each task.",
    "start": "2873285",
    "end": "2882360"
  },
  {
    "text": "Uh, this is something that we can do completely offline from the data that we collected, which is nice. We'll then collect trials for each task",
    "start": "2882360",
    "end": "2889860"
  },
  {
    "text": "by running this one shot imitation learning policy, uh, in the environment, and we'll do this in a ver- in a batch off policy setting,",
    "start": "2889860",
    "end": "2896895"
  },
  {
    "text": "where we just collect a large batch of data once and then we're done collecting data.",
    "start": "2896895",
    "end": "2902220"
  },
  {
    "text": "Uh, this is pretty easy to do in- or  easier to do in real world settings so we don't have to be iteratively constantly collecting data.",
    "start": "2902220",
    "end": "2909675"
  },
  {
    "text": "Uh, then lastly, we'll train a policy that learns how to retry the task using,",
    "start": "2909675",
    "end": "2916155"
  },
  {
    "text": "uh, an imitation learning objective. So what we'll do is, we'll give- I will train kind of a meta-learning algorithm that",
    "start": "2916155",
    "end": "2921869"
  },
  {
    "text": "gets both the demonstration and one of the trials that it- it collected in step three and train it such that",
    "start": "2921870",
    "end": "2927510"
  },
  {
    "text": "the resulting policy is able to perform the task, uh, in a way that closely matches the demonstration in that situation.",
    "start": "2927510",
    "end": "2936580"
  },
  {
    "text": "Okay, so basically dtrain is gonna to correspond to a demo and, uh, one or a couple of trials,",
    "start": "2936830",
    "end": "2942855"
  },
  {
    "text": "and then, uh, the outer objective will correspond to a new demonstration of that task.",
    "start": "2942855",
    "end": "2949180"
  },
  {
    "text": "Okay, um, so in our experiments, one of the things we wanna look at is, uh, this approach which we call watch, try,",
    "start": "2950030",
    "end": "2956640"
  },
  {
    "start": "2955000",
    "end": "3099000"
  },
  {
    "text": "learn, [NOISE] in the sense that it's watching a demonstration, trying the task, and then, uh, learn to do the task.",
    "start": "2956640",
    "end": "2962565"
  },
  {
    "text": "We also wanna compare this to meta- reinforcement learning that only uses trial data.",
    "start": "2962565",
    "end": "2967724"
  },
  {
    "text": "Uh, it isn't giving indication of the task through demonstrations and this is gonna be very hard for the reasons that I mentioned before,",
    "start": "2967725",
    "end": "2973140"
  },
  {
    "text": "because you [NOISE] have to poke all the objects. Um, and then also compare to meta-imitation learning that only uses demonstrations,",
    "start": "2973140",
    "end": "2980190"
  },
  {
    "text": "uh, and is only allowed- isn't allowed to kind of retry the task in a new situation.",
    "start": "2980190",
    "end": "2985680"
  },
  {
    "text": "Uh, and then lastly we'll also compare the approach that doesn't use any meta-learning, that just behavior clones all the demonstrations.",
    "start": "2985680",
    "end": "2991875"
  },
  {
    "text": "Um, so quantitatively what we can see is that, um, across the board, uh,",
    "start": "2991875",
    "end": "2996945"
  },
  {
    "text": "the approach that uses a- a trial and- a demonstration and a trial is- is first able to learn decently well across the four task families.",
    "start": "2996945",
    "end": "3005150"
  },
  {
    "text": "Uh, it also significantly outperforms only using demonstrations which is shown in orange.",
    "start": "3005150",
    "end": "3011180"
  },
  {
    "text": "Um, and also- it also significantly outperforms using only trials. Uh, in this case, using only trials performed, uh,",
    "start": "3011180",
    "end": "3018349"
  },
  {
    "text": "so poorly that we didn't actually plot it on this graph. Um, so it's basically getting 0% performance,",
    "start": "3018350",
    "end": "3024725"
  },
  {
    "text": "um, because it was also a very challenging optimization problem. Okay, um, and then we can look at some qualitative examples.",
    "start": "3024725",
    "end": "3033185"
  },
  {
    "text": "Uh, so this is similar to some of the qualitative examples we showed previously. So here's the demonstration, uh, then it gets one trial where it has to figure out how to solve all the tasks.",
    "start": "3033185",
    "end": "3041060"
  },
  {
    "text": "So it tried to- in this case it tried to push the object on the left towards the right. Uh, and then that wasn't the correct thing and so it figured",
    "start": "3041060",
    "end": "3047630"
  },
  {
    "text": "out that it needed to push the object on the right towards the left. Uh, and likewise for something like a grasping task, in this case,",
    "start": "3047630",
    "end": "3053750"
  },
  {
    "text": "it's given a demonstration, um, which is to grasp one of the objects. Uh, in this case, it- it wa- wasn't able to grasp on the first try,",
    "start": "3053750",
    "end": "3061130"
  },
  {
    "text": "uh, and then it figures out, um, using that experience and it's ex- experiencing in dynamics to figure out how to successfully grasp the cup.",
    "start": "3061130",
    "end": "3069860"
  },
  {
    "text": "Um, and then one last comparison we did is we took the behavior cloning approach and fine-tuned that",
    "start": "3069860",
    "end": "3076475"
  },
  {
    "text": "with reinforcement learning on one of the test tasks and we found that that required 900 trials,",
    "start": "3076475",
    "end": "3082895"
  },
  {
    "text": "um, of- of reinforcement learning in order to match the performance of what WTL was able to get in just a single trial.",
    "start": "3082895",
    "end": "3090065"
  },
  {
    "text": "Um, so this is kind of emphasizing the- the efficiency that meta-learning can give you.",
    "start": "3090065",
    "end": "3095700"
  },
  {
    "text": "Okay, um, and one side note on memorization is that one of the things that we specifically did in this problem",
    "start": "3095950",
    "end": "3102770"
  },
  {
    "start": "3099000",
    "end": "3255000"
  },
  {
    "text": "setting was to make it such that the demonstration only partially specified the task and didn't kind of completely specify the task.",
    "start": "3102770",
    "end": "3109369"
  },
  {
    "text": "Uh, and one of the things that will happen is if the demonstration fully specifies the task,",
    "start": "3109370",
    "end": "3115025"
  },
  {
    "text": "then it will basically just resolve to a one-shot imitation learning and will learn to ignore the trials during meta-learning.",
    "start": "3115025",
    "end": "3123650"
  },
  {
    "text": "Um, so this is basically sort of a variant on the memorization problem that we mentioned before,",
    "start": "3123650",
    "end": "3129275"
  },
  {
    "text": "uh, and this might be- this is like resolving to one-shot imitation learning, it is fine if you're good at the meta-test task.",
    "start": "3129275",
    "end": "3134734"
  },
  {
    "text": "So like if you're given a demonstration of a task, uh, and you can like solve the task very well from that demonstration,",
    "start": "3134735",
    "end": "3141980"
  },
  {
    "text": "then you don't need trials and like life is good. Uh, but if you're given a demonstration and then you can't quite solve the task,",
    "start": "3141980",
    "end": "3151535"
  },
  {
    "text": "you fail at the task, then this is where memorization is a problem.",
    "start": "3151535",
    "end": "3156920"
  },
  {
    "text": "Because once you fail at the task, that means that you don't have any mechanism to quickly adapt from trials and then you may have to run reinflict,",
    "start": "3156920",
    "end": "3162920"
  },
  {
    "text": "fine tuning with reinforcement learning, um, in order to actually learn a policy for the task.",
    "start": "3162920",
    "end": "3169530"
  },
  {
    "text": "Okay, so essentially this is just a variant on the memorization problem where it's ignoring part of the training data, uh,",
    "start": "3169570",
    "end": "3176450"
  },
  {
    "text": "and then in principle, some of the algorithms that we mentioned previously that actually regularize information flow, could help solve this problem.",
    "start": "3176450",
    "end": "3183450"
  },
  {
    "text": "Okay, cool. So, um, we talked about how we could train across task families.",
    "start": "3183520",
    "end": "3190820"
  },
  {
    "text": "Now, how will you go about generalizing to entirely new tasks, right? Um, to do that we need a broad distribution of",
    "start": "3190820",
    "end": "3197690"
  },
  {
    "text": "tasks but we also need that broad distribution of tasks to be fairly dense. We can't just take four tasks from",
    "start": "3197690",
    "end": "3203210"
  },
  {
    "text": "that task family and expect generalization to an entirely new task. Um, so we need a broad and- and not sparse distribution of tasks.",
    "start": "3203210",
    "end": "3211295"
  },
  {
    "text": "Um, then the question comes is like- the question that comes up is, where do you actually get that distribution of tasks?",
    "start": "3211295",
    "end": "3217220"
  },
  {
    "text": "Um, there's a few options for this, uh, from kinda the reinforcement learning literature, so there's things like OpenAI Gym,",
    "start": "3217220",
    "end": "3223369"
  },
  {
    "text": "uh, and like the Atari learning environment. Both of these aren't very satisfying options",
    "start": "3223370",
    "end": "3228830"
  },
  {
    "text": "for meta-learning because there isn't necessarily very clear underlying structure that you can leverage to quickly learn new tasks from this distribution.",
    "start": "3228830",
    "end": "3237200"
  },
  {
    "text": "Um, there's also a- a suite of tasks called SURREAL or- or Robosuite.",
    "start": "3237200",
    "end": "3242675"
  },
  {
    "text": "Uh, and this suite of tasks has considerably more structure but it only has six,",
    "start": "3242675",
    "end": "3247790"
  },
  {
    "text": "I think around six tasks, and so that also isn't something that we could use for meta-learning to quickly learn new tasks.",
    "start": "3247790",
    "end": "3253655"
  },
  {
    "text": "And so we really want is, um, many qualitatively distinct tasks. So maybe 50 or more tasks, um,",
    "start": "3253655",
    "end": "3261260"
  },
  {
    "start": "3255000",
    "end": "3407000"
  },
  {
    "text": "and we want each of these tasks to have, uh, reward functions and success metrics for- for meta-reinforcement learning.",
    "start": "3261260",
    "end": "3266869"
  },
  {
    "text": "Uh, we also- if we wanna be able to do meta-learning across this, we want to be able to study transfer,",
    "start": "3266870",
    "end": "3272329"
  },
  {
    "text": "as we want all the tasks themselves to be individually solvable. Um, and lastly, we want [NOISE] a unified state space and a unified action space,",
    "start": "3272330",
    "end": "3281329"
  },
  {
    "text": "and so motivated by this des- uh, desiderata, we, uh, created a benchmark,",
    "start": "3281330",
    "end": "3287150"
  },
  {
    "text": "uh, that we call Meta-World that I think many of you probably are- are familiar with. Uh, it was mentioned on, on Piazza,",
    "start": "3287150",
    "end": "3292860"
  },
  {
    "text": "uh, that has each of these traits. So it has, uh, 50 distinct tasks, shaped reward functions for each of the tasks, um,",
    "start": "3292860",
    "end": "3300680"
  },
  {
    "text": "and also a unified state space and unified action space, such that we can hope, um, hope to actually see some generalization to entirely new tasks shown on the right.",
    "start": "3300680",
    "end": "3311280"
  },
  {
    "text": "Okay, so, um, then the key question is how do current algorithms actually work,",
    "start": "3311310",
    "end": "3318820"
  },
  {
    "text": "uh, when you run them on this set of tasks? So, uh, kind of the TL DR is that there are",
    "start": "3318820",
    "end": "3326030"
  },
  {
    "text": "some signs of life but there is significant room for improvement. Uh, and in particular, uh, if you look at",
    "start": "3326030",
    "end": "3332704"
  },
  {
    "text": "the performance of meta-learning algorithms like MAML, RL squared, and PEARL, uh, they get, uh, meta-task performances in the 20s or- or low 30s.",
    "start": "3332705",
    "end": "3340735"
  },
  {
    "text": "Um, so this is a bit, uh, perhaps disappointing but also, uh, exciting because it means that there's a lot of room for",
    "start": "3340735",
    "end": "3346750"
  },
  {
    "text": "improvement for future algorithms. Um, so then the question is why- why are they performing so poorly?",
    "start": "3346750",
    "end": "3354539"
  },
  {
    "text": "Uh, and if you dig into the details, it turns out that the reason why they're performing so poorly isn't because they're generalizing",
    "start": "3354540",
    "end": "3359550"
  },
  {
    "text": "poorly but because they're actually having trouble doing well on the meta-training distribution. Um, so they're performing poorly even on the 45-minute training tasks.",
    "start": "3359550",
    "end": "3368805"
  },
  {
    "text": "Uh, and so then you- okay, the kind of natural thing to think is, well, can we just do multi-task RL on the 45 tasks?",
    "start": "3368805",
    "end": "3375870"
  },
  {
    "text": "Uh, and if you then run multi-task RL algorithms on these 45 tasks, they don't need to generalize to entirely new tasks,",
    "start": "3375870",
    "end": "3381720"
  },
  {
    "text": "they just need to be able to kind of do well on the training tasks. Uh, these- these algorithms also struggled to do well on the 45 or in this case,",
    "start": "3381720",
    "end": "3391410"
  },
  {
    "text": "on the 50, uh, tasks in the benchmark. Okay. So what's the problem here?",
    "start": "3391410",
    "end": "3398190"
  },
  {
    "text": "Uh, we- we, developed these algorithms. Um, and it seems like there is considerable structure among these tasks, right?",
    "start": "3398190",
    "end": "3405240"
  },
  {
    "text": "Um, so why the poor results? So, um, you might think, well, maybe it's an exploration challenge.",
    "start": "3405240",
    "end": "3411630"
  },
  {
    "start": "3407000",
    "end": "3497000"
  },
  {
    "text": "Uh, maybe it's having trouble exploring in a way that allows it to figure out how to solve these tasks. Um, but all the tasks are individually solvable with reinforcing learning algorithms.",
    "start": "3411630",
    "end": "3420900"
  },
  {
    "text": "So it isn't an exploration problem. We know that, uh, if you try to solve the task individually, you can- everything works just fine.",
    "start": "3420900",
    "end": "3428490"
  },
  {
    "text": "Then you might ask, well, maybe it's a data scarcity problem. Like maybe we just aren't given enough data, um, to perform these tasks well.",
    "start": "3428490",
    "end": "3434880"
  },
  {
    "text": "Um, but all of them are- are in the- in these comparisons all the methods were given plenty of data,",
    "start": "3434880",
    "end": "3440145"
  },
  {
    "text": "plenty of samples within their budget. They were also given plenty of model capacity, uh, so that isn't the- the challenge.",
    "start": "3440145",
    "end": "3446880"
  },
  {
    "text": "Uh, and kind of the confusing thing here is that it turns out that training models independently, uh,",
    "start": "3446880",
    "end": "3453000"
  },
  {
    "text": "from one another actually performed significantly better than training a single model across all of the tasks.",
    "start": "3453000",
    "end": "3459615"
  },
  {
    "text": "Uh, so kind of going back to some of the things that we talked about at the very beginning of the course, it seems like the algorithms are having trouble,",
    "start": "3459615",
    "end": "3466440"
  },
  {
    "text": "um, kind of sharing information and sharing weights. Okay. So our conclusion from this was that,",
    "start": "3466440",
    "end": "3474810"
  },
  {
    "text": "uh, it must be an optimization challenge. So it's not a model capacity, it's not a data problem, it's not an exploration challenge. It seems like it must be an optimization challenge that's- that's",
    "start": "3474810",
    "end": "3481800"
  },
  {
    "text": "preventing these algorithms from solving the training tasks. Um, so [NOISE] how would we go about solving this optimization challenge?",
    "start": "3481800",
    "end": "3489974"
  },
  {
    "text": "Um, we had a couple of hypotheses for what the underlying optimization problem might be. Uh, the first hypothesis is that it",
    "start": "3489975",
    "end": "3497010"
  },
  {
    "start": "3497000",
    "end": "3600000"
  },
  {
    "text": "seems like maybe gradients from different tasks are conflicting with one another. Uh, and this- this seemed like a reasonable hypothesis because if they weren't conflicting with",
    "start": "3497010",
    "end": "3503610"
  },
  {
    "text": "each other then it seems like the optimization process would be fine. Um, and so if there is conflict then we would see negative inner product,",
    "start": "3503610",
    "end": "3511965"
  },
  {
    "text": "um, between different gradients. So we'd see gradients that are kind of pointing in- in opposite directions.",
    "start": "3511965",
    "end": "3517545"
  },
  {
    "text": "Uh, and our second hypothesis which is important is that when they do conflict,",
    "start": "3517545",
    "end": "3523200"
  },
  {
    "text": "they cause more damage than expected. Uh, and the reason why this hypothesis is important is that,",
    "start": "3523200",
    "end": "3529545"
  },
  {
    "text": "if the- if the gradients are pointing in opposite directions, um, just averaging the gradients should be the right thing to do.",
    "start": "3529545",
    "end": "3535320"
  },
  {
    "text": "Uh, and that's what standard multi-task reinforcement learning would do. It's just to simply average the two gradients, uh, and then the one that is higher in magnitude is the one that",
    "start": "3535320",
    "end": "3542130"
  },
  {
    "text": "will kind of dominate more, right? Um, so it seems like averaging isn't the right thing to do, uh,",
    "start": "3542130",
    "end": "3550245"
  },
  {
    "text": "and we need to do something that tries to mitigate, um, some of the damage that is being caused by these gradients pointing opposite directions.",
    "start": "3550245",
    "end": "3558495"
  },
  {
    "text": "Um, and in particular, our hypothesis here currently is that the damage is being caused by high curvature.",
    "start": "3558495",
    "end": "3564720"
  },
  {
    "text": "Um, so for example, imagine this is 1D example where you have a one-dimensional parameter vector and a loss function that's shown on the y-axis.",
    "start": "3564720",
    "end": "3572595"
  },
  {
    "text": "Uh, if you have high positive curvature, um, then you have, uh, maybe a function that looks somewhat like this.",
    "start": "3572595",
    "end": "3578790"
  },
  {
    "text": "Uh, if you take one point on its curve, maybe this is where you're currently at. Uh, you could also measure the gradient of this function.",
    "start": "3578790",
    "end": "3586830"
  },
  {
    "text": "Now, um, this is the- the loss function corresponding to one of your tasks, uh,",
    "start": "3586830",
    "end": "3591885"
  },
  {
    "text": "and maybe it's the case that another task has a larger gradient and it wants to move right on- right,",
    "start": "3591885",
    "end": "3599250"
  },
  {
    "text": "uh, in the right direction on the x-axis. And so then what will happen is that if you want to move",
    "start": "3599250",
    "end": "3605040"
  },
  {
    "text": "right according to this gradient, um, if you move right and take a certain step size, you would end up here.",
    "start": "3605040",
    "end": "3611339"
  },
  {
    "text": "Uh, but in reality, you don't actually end up there, you end up up here.",
    "start": "3611340",
    "end": "3616410"
  },
  {
    "text": "Uh, and so even though your gradient was- had a relatively low magnitude and said that if you go right,",
    "start": "3616410",
    "end": "3622185"
  },
  {
    "text": "your loss function wouldn't get that much worse. In reality, because of the positive curvature,",
    "start": "3622185",
    "end": "3627360"
  },
  {
    "text": "you actually ended up in a much worse place in terms of your loss function then your gradient suggested. Yeah.",
    "start": "3627360",
    "end": "3634039"
  },
  {
    "text": "Can't also a smaller learning rate fix that? Um, so principally, yeah,",
    "start": "3634040",
    "end": "3639930"
  },
  {
    "text": "a smaller learning rate could certainly fix that, uh, with the caveat that- then learning would be a bit slower. Yeah.",
    "start": "3639930",
    "end": "3646380"
  },
  {
    "text": "Sorry, just a quick question, first of all, I would also like- how is MAML coming to this, uh, because, you know,",
    "start": "3646380",
    "end": "3653420"
  },
  {
    "text": "usually this is [inaudible] so how does that work with MAML",
    "start": "3653420",
    "end": "3659809"
  },
  {
    "text": "externally and I guess the other question is when you say gradients in the domain of- of policy learning,",
    "start": "3659810",
    "end": "3666290"
  },
  {
    "text": "what gradients were you specifically referring to? Yeah. So for the first question, right now we're just looking at multi-task learning.",
    "start": "3666290",
    "end": "3672330"
  },
  {
    "text": "We're not looking at MAML at all, um, because we just wanna solve the training tasks. Uh, and so yeah,",
    "start": "3672330",
    "end": "3679770"
  },
  {
    "text": "so that's- although at the same time, we're- we think the hypothesis about curvature but the solution won't end up",
    "start": "3679770",
    "end": "3685019"
  },
  {
    "text": "involving a curvature or won't end up involving any second order information. And so that shouldn't in theory play nicely with MAML.",
    "start": "3685020",
    "end": "3692204"
  },
  {
    "text": "Um, for your- what was your second question again? So when you talk about gradients [inaudible].",
    "start": "3692205",
    "end": "3698700"
  },
  {
    "text": "Yeah. Yeah, so in this case the gradient, um, for an algorithm like SAC would be the gradient of your q function, of your critic,",
    "start": "3698700",
    "end": "3705930"
  },
  {
    "text": "and the gradient of your actor, and we would be talking about the gradients of both of them. It may be that one of them suffers from this challenge more so than the other.",
    "start": "3705930",
    "end": "3715530"
  },
  {
    "text": "Um, but in practice, uh, when we look at this, we're looking at both of them.",
    "start": "3715530",
    "end": "3721000"
  },
  {
    "text": "Yeah. Okay. Um, oh,",
    "start": "3722420",
    "end": "3727799"
  },
  {
    "text": "and then with regard to the smaller step sizes again, um, I- I kind of blew this up so that we can see it for visualization purposes.",
    "start": "3727800",
    "end": "3734190"
  },
  {
    "text": "Um, in practice, even with smaller step sizes, you would also have this problem if you have very high curvature.",
    "start": "3734190",
    "end": "3739275"
  },
  {
    "text": "Um, so in- in practice you would never take it a step size probably that large but yeah.",
    "start": "3739275",
    "end": "3744885"
  },
  {
    "text": "Okay. Cool. So, um, given these two hypotheses,",
    "start": "3744885",
    "end": "3750450"
  },
  {
    "text": "our approach to the problem was, when taking the gradient step for one task, try to avoid making the other task worse.",
    "start": "3750450",
    "end": "3757859"
  },
  {
    "text": "Uh, and this is motivated by the fact that maybe if we make the other task worse, it will get a lot worse or more worse than we expected.",
    "start": "3757860",
    "end": "3764205"
  },
  {
    "text": "Uh, and so in particular the way that we could do this is very simple. Um, our algorithm will simply be that if two gradients conflict with one another,",
    "start": "3764205",
    "end": "3773790"
  },
  {
    "text": "essentially if they- if they have negative inner product then we'll project each of the gradients onto the normal plane of the other gradient.",
    "start": "3773790",
    "end": "3782250"
  },
  {
    "text": "And so in particular, we'll project the gradient of one of the tasks onto the normal plane of the other and will project the- the gradient of",
    "start": "3782250",
    "end": "3788279"
  },
  {
    "text": "the red task onto the normal plane of the blue task. Uh, and as a result, when you project this, um, this gradient,",
    "start": "3788280",
    "end": "3794010"
  },
  {
    "text": "this is essentially in some ways solving a constrained optimization problem that's trying to change the gradient such that it",
    "start": "3794010",
    "end": "3799559"
  },
  {
    "text": "doesn't actually affect the- the loss function of the other task doesn't actually get any worse according to your gradient.",
    "start": "3799560",
    "end": "3805454"
  },
  {
    "text": "It may actually get worse, uh, if- if there's certain curvature, uh, but it should get less worse according to",
    "start": "3805455",
    "end": "3810990"
  },
  {
    "text": "your gradient than if you were to take the original one. Um, and if they don't conflict then don't do anything.",
    "start": "3810990",
    "end": "3817980"
  },
  {
    "text": "You can just leave them alone and they should interact in a positive way. Yeah.",
    "start": "3817980",
    "end": "3823710"
  },
  {
    "text": "Is that asymmetrical like if you were to project GJ on a GI normal plane",
    "start": "3823710",
    "end": "3829065"
  },
  {
    "text": "versus GI's on the the GJ's, does it vary? Um, yeah. So if you only do one of these it is asymmetric.",
    "start": "3829065",
    "end": "3836280"
  },
  {
    "text": "And so what we do is we do both of them to make it symmetric.",
    "start": "3836280",
    "end": "3841390"
  },
  {
    "text": "Um, so we call this projecting conflicting gradients, uh, or PC grad because that's what we're doing,",
    "start": "3841390",
    "end": "3846735"
  },
  {
    "text": "we're projecting the conflicting gradients. Okay. Um, so how does this work?",
    "start": "3846735",
    "end": "3852405"
  },
  {
    "text": "Uh, so if you run multi-task RL on Meta-World on both 10 tasks in Meta-World and 50 tasks in Meta-World, what we see is that,",
    "start": "3852405",
    "end": "3859859"
  },
  {
    "text": "um, this approach showed in purple is able to learn, um, significantly faster and significantly",
    "start": "3859860",
    "end": "3865590"
  },
  {
    "text": "better than if you were to train independent networks. And also significantly better and significantly faster than if",
    "start": "3865590",
    "end": "3870869"
  },
  {
    "text": "you try to do weight sharing without this, um, without this sort of optimization trick to make the- the landscape- or to make the optimization work,",
    "start": "3870870",
    "end": "3878415"
  },
  {
    "text": "um, work in a more clean way. Okay. Um, so this helps a lot on- on reinforcement learning, uh,",
    "start": "3878415",
    "end": "3885885"
  },
  {
    "text": "and it's hopefully a step towards actually making meta learning algorithms work well on this benchmark, but that's not a step that we've done yet.",
    "start": "3885885",
    "end": "3893385"
  },
  {
    "text": "Um, and then another question is, well, this helps for RL, but does it help for supervised learning?",
    "start": "3893385",
    "end": "3898980"
  },
  {
    "text": "Uh, so we also ran it on multitask supervised learning, um, benchmarks that we've seen in the literature.",
    "start": "3898980",
    "end": "3904620"
  },
  {
    "text": "And we could also see that, uh, in comparison to other approaches. It is able to outperform a number of previous approaches and also, uh,",
    "start": "3904620",
    "end": "3913695"
  },
  {
    "text": "combines well with previous architectural solutions to, uh, to multitask learning. So routing networks, for example,",
    "start": "3913695",
    "end": "3920160"
  },
  {
    "text": "is one architectural solution to multitask RL. Uh, and if you combine that approach with the optimization, um,",
    "start": "3920160",
    "end": "3926025"
  },
  {
    "text": "solution of PCGrad, it can perform, uh, significantly better than all of the approaches.",
    "start": "3926025",
    "end": "3932130"
  },
  {
    "text": "Um, and then we also compare this on, uh, the NYUv2 dataset which has three tasks predicting segmentation,",
    "start": "3932130",
    "end": "3938730"
  },
  {
    "text": "predicting depth, and predicting service normals. Uh, and we're also able to see a significant improvement from,",
    "start": "3938730",
    "end": "3944565"
  },
  {
    "text": "um, from using PCGrad here as well. Um, so kind of the takeaway here is that it also helps, uh,",
    "start": "3944565",
    "end": "3951000"
  },
  {
    "text": "multitask supervised learning compared to a number of different multitask architectures that you could use.",
    "start": "3951000",
    "end": "3956700"
  },
  {
    "text": "Okay. Um, and then lastly one question you might ask is, why does it work so well? Um, and here, we wanted to kind of",
    "start": "3956700",
    "end": "3963030"
  },
  {
    "text": "confirm some of the intuitions that I mentioned earlier. Uh, and in particular one of the things we tried is that, um,",
    "start": "3963030",
    "end": "3969095"
  },
  {
    "text": "when you project onto the, the normal plane, you're changing both the magnitude of the gradient and the direction of the gradient.",
    "start": "3969095",
    "end": "3976100"
  },
  {
    "text": "And so what we tried doing it was an ablation where we changed only the magnitude or changed only the direction of the gradient,",
    "start": "3976100",
    "end": "3982355"
  },
  {
    "text": "and left the other component of the gradient unchanged, uh, compared to just averaging across the gradients.",
    "start": "3982355",
    "end": "3988570"
  },
  {
    "text": "And what we found here is that the, um, first, both components were important,",
    "start": "3988570",
    "end": "3993599"
  },
  {
    "text": "but the most important component was changing the direction of the gradient and not the magnitude of the gradient.",
    "start": "3993600",
    "end": "3998670"
  },
  {
    "text": "Uh, and to us, this provides some evidence of the kind of the effect of, um, the kind of the- the intuition that I described previously which is where we,",
    "start": "3998670",
    "end": "4007070"
  },
  {
    "text": "where we don't want to affect the gradient of the other task by changing the direction of the gradient.",
    "start": "4007070",
    "end": "4012450"
  },
  {
    "text": "Okay. So some takeaways, um, first is scaling to broad task distributions is hard.",
    "start": "4012490",
    "end": "4018875"
  },
  {
    "text": "Uh, they can't be taken for granted from the algorithms that we have currently. And, um, to do- uh, to kind of try to do this,",
    "start": "4018875",
    "end": "4026059"
  },
  {
    "text": "we can try to convey task information beyond the reward function, for example, using a demonstration. Um, we could try and train on very broad and dense task distributions such as Meta-World.",
    "start": "4026060",
    "end": "4035300"
  },
  {
    "text": "Although, there- this is kind of an initial approach towards a good benchmark and there, definitely, is lots of room for improvement there.",
    "start": "4035300",
    "end": "4041330"
  },
  {
    "text": "Um, and also trying to avoid conflicting gradients by projecting gradients when they conflict.",
    "start": "4041330",
    "end": "4046714"
  },
  {
    "text": "Okay. Um, so that's the last part about, uh, kind of, very recent research.",
    "start": "4046715",
    "end": "4052910"
  },
  {
    "text": "Uh, now, I want to talk about some of the more open challenges in multitask learning and meta learning.",
    "start": "4052910",
    "end": "4058595"
  },
  {
    "text": "Uh, and it's worth mentioning that I think we've covered a large number of challenges and, and downsides of approaches throughout this course.",
    "start": "4058595",
    "end": "4065300"
  },
  {
    "text": "And so I'm going to try to, just, just discuss some of the things that we haven't previously covered in the course yet.",
    "start": "4065300",
    "end": "4070430"
  },
  {
    "text": "Um, so the first class of challenges in my mind is just trying to address the fundamental assumptions",
    "start": "4070430",
    "end": "4077119"
  },
  {
    "text": "that we make when we set up the problem of multitask learning or the problem of meta, uh, meta learning.",
    "start": "4077120",
    "end": "4084200"
  },
  {
    "text": "Um, in the first assumption is this assumption that the meta training, the meta testing task distribution must be the same.",
    "start": "4084200",
    "end": "4090230"
  },
  {
    "text": "Uh, and this is, I think, really important because there could be many situations where we have out-of-distribution tasks or very long-tailed task distributions.",
    "start": "4090230",
    "end": "4098464"
  },
  {
    "text": "Um, so we may have a distribution that looks like this where we have a lot of data in, kind of, the head of the data distribution and small amounts of",
    "start": "4098465",
    "end": "4105589"
  },
  {
    "text": "data in a wide range of situations such as different objects encountered, different interactions with people, different words heard, different driving situations, [NOISE] etc.",
    "start": "4105590",
    "end": "4113755"
  },
  {
    "text": "Um, and in principle, we know how to do few-shot learning, uh, so we should be able to adapt to small amounts of data in the tail.",
    "start": "4113755",
    "end": "4121884"
  },
  {
    "text": "But the catch is that, these few-shot tasks on the right are from a different distribution than the ones where most of our data is.",
    "start": "4121885",
    "end": "4128940"
  },
  {
    "text": "Uh, and so in order to solve this problem, we need to be thinking about settings or algorithms that can handle assumptions where",
    "start": "4128940",
    "end": "4135290"
  },
  {
    "text": "the meta training task distribution and the meta testing task distribution aren't the same. And some hints I think,",
    "start": "4135290",
    "end": "4141995"
  },
  {
    "text": "towards this problem might come from the domain adaptation literature or the robustness literature where we're trying to be,",
    "start": "4141995",
    "end": "4148040"
  },
  {
    "text": "uh, do well on basically, uh, on data points that are kind of, from the tails of our distribution or from- or from a different distribution.",
    "start": "4148040",
    "end": "4154759"
  },
  {
    "text": "Um, a second challenge is multimodality. So uh, a lot of the- a lot of",
    "start": "4154760",
    "end": "4161000"
  },
  {
    "text": "the motivation for meta learning is a leveraged previous experience. Uh, the previous experience came in the form of",
    "start": "4161000",
    "end": "4167089"
  },
  {
    "text": "a set of tasks or kind of a set of datasets. Uh, but in practice, in the real world,",
    "start": "4167090",
    "end": "4172130"
  },
  {
    "text": "we may have much more rich sources or previous experiences, ranging from image data to, to touch feedback, to language, to social cues.",
    "start": "4172130",
    "end": "4180200"
  },
  {
    "text": "Uh, and so I think it's important to think about how we might learn priors across multiple data modalities that",
    "start": "4180200",
    "end": "4186049"
  },
  {
    "text": "can be compiled into some single prior about the world or some kind of single source of common sense knowledge about the world.",
    "start": "4186050",
    "end": "4192875"
  },
  {
    "text": "Uh, and some of the challenges that come up here is that you have varying dimensionalities of these, of these, um, of these modalities,",
    "start": "4192875",
    "end": "4198995"
  },
  {
    "text": "different units of these modalities. Uh, and they also carry different forms in different- uh, actually complementary forms of information.",
    "start": "4198995",
    "end": "4207425"
  },
  {
    "text": "And some hints at this problem might come from the multimodal learning literature.",
    "start": "4207425",
    "end": "4212599"
  },
  {
    "text": "For example, but I think that this is really, uh, an unsolved or really, um, unexplored area.",
    "start": "4212600",
    "end": "4218060"
  },
  {
    "text": "[NOISE] Um, and then, the last question is, uh, when should we actually use multitask learning or meta learning?",
    "start": "4218060",
    "end": "4225050"
  },
  {
    "text": "When will it help? Uh, when, when will it give us benefits? Uh, and this is, in many ways, a problem with algorithm selection or model selection.",
    "start": "4225050",
    "end": "4232670"
  },
  {
    "text": "Okay. So that's telling you about the problem assumptions. Uh, another challenge, ah,",
    "start": "4232670",
    "end": "4238805"
  },
  {
    "text": "that we've started to get at a little bit, uh, but not really in a satisfying way is, is better benchmarks.",
    "start": "4238805",
    "end": "4244865"
  },
  {
    "text": "Uh, and in particular, I think we need, uh, kind of- in this class, you explo- you explored, um, Omniglot.",
    "start": "4244865",
    "end": "4250864"
  },
  {
    "text": "We also talked a lot about Mini-ImageNet, um, and we talked about Meta-World today. Um, I think that ultimately,",
    "start": "4250865",
    "end": "4256310"
  },
  {
    "text": "we need benchmarks that both reflect breadth of, of, kind of the types of distributions that we wanna cover so that we can",
    "start": "4256310",
    "end": "4262610"
  },
  {
    "text": "generalize broadly as well as realism. We want them to reflect the kinds of real-world problems that we want to solve.",
    "start": "4262610",
    "end": "4268940"
  },
  {
    "text": "Um, and so we have some steps towards good benchmarks that we saw in this course such as Meta-Dataset,",
    "start": "4268940",
    "end": "4274070"
  },
  {
    "text": "uh, today, we saw Meta-World. Um, there's also a new benchmark called the visual task adaptation benchmark.",
    "start": "4274070",
    "end": "4279545"
  },
  {
    "text": "Uh, and there's also the tasko- Taskonomy Dataset by Zamir et al. Um, and I think that these are, are really, kind of,",
    "start": "4279545",
    "end": "4286610"
  },
  {
    "text": "important steps towards better, uh, benchmarks for actually studying your algorithms. Um, but ultimately, we need benchmarks that can really reflect the real-world problems.",
    "start": "4286610",
    "end": "4295670"
  },
  {
    "text": "Uh, it can have the appropriate level of difficulty for where we're at right now, and are also very easy to use.",
    "start": "4295670",
    "end": "4301925"
  },
  {
    "text": "And I think that many of these days that don't capture, kind of, all of these, uh, different properties.",
    "start": "4301925",
    "end": "4307010"
  },
  {
    "text": "Uh, and the reason why benchmarks are important is that without benchmarks, we can't necessarily make any progress on the algorithm side of things.",
    "start": "4307010",
    "end": "4313639"
  },
  {
    "text": "We can't make progress on solving the problems. [NOISE] Okay. Um, and then lastly it's simply just improving core algorithms,",
    "start": "4313640",
    "end": "4320690"
  },
  {
    "text": "and this is something that we talked a lot about in the course. So I'll just talk about it briefly here. Um, things like handling computation and memory requirements, uh,",
    "start": "4320690",
    "end": "4328639"
  },
  {
    "text": "and making large-scale bi-level optimization actually practical to run, um, on, on the computers that we have today.",
    "start": "4328640",
    "end": "4336139"
  },
  {
    "text": "Uh, trying to develop more of a theoretical understanding of the performance of these algorithms. So we talked a little bit about, um,",
    "start": "4336140",
    "end": "4342724"
  },
  {
    "text": "theoretical measures of ex- expressive power, um, and different assumptions that our, our algorithms make,",
    "start": "4342725",
    "end": "4349025"
  },
  {
    "text": "but I think that there's a lot of work to be done to under- better understand what these approaches are doing. Um, and lastly, uh,",
    "start": "4349025",
    "end": "4356125"
  },
  {
    "text": "one thing that we didn't cover that much in this course is looking at problems. Looking at multitask problems where you want to perform tasks in sequence.",
    "start": "4356125",
    "end": "4363625"
  },
  {
    "text": "Uh, and this sort of sequential decision-making problem, I think is particularly challenging because one task, uh,",
    "start": "4363625",
    "end": "4370175"
  },
  {
    "text": "the starting distribution of one task will depend on where the policy for the previous task takes you.",
    "start": "4370175",
    "end": "4375665"
  },
  {
    "text": "And actually along- a number of these directions, there's been a number of final projects that have tried to look at some of these challenges.",
    "start": "4375665",
    "end": "4382355"
  },
  {
    "text": "So in addition to these, uh, there's also, kind of, the challenges that you hopefully, uh, discovered in your homework and final projects. Um, great.",
    "start": "4382355",
    "end": "4391310"
  },
  {
    "text": "And so lastly, I'd like to talk, uh, just very briefly about the bigger picture and, and some of the motivation for this course.",
    "start": "4391310",
    "end": "4396570"
  },
  {
    "text": "So uh, one of the things that we talked about at the very beginning is kind of how- if you look at a lot of existing machine learning systems in many ways,",
    "start": "4396570",
    "end": "4404800"
  },
  {
    "text": "they're, kind of, very specialized towards particular applications. Uh, they can perform one very narrow thing in one,",
    "start": "4404800",
    "end": "4410969"
  },
  {
    "text": "kind of, narrow environment. Uh, and hopefully, if we care about building machines that can go into real-world environments,",
    "start": "4410970",
    "end": "4417500"
  },
  {
    "text": "then they can look, uh, more like the, um- they can exhibit more of the generality and flexibility that humans have.",
    "start": "4417500",
    "end": "4425690"
  },
  {
    "text": "And so in this course, uh, I think, we covered some of the steps towards these sort of generalist machine learning systems, uh,",
    "start": "4425690",
    "end": "4431960"
  },
  {
    "text": "such as systems that can learn multiple tasks, that can leverage previous experience when learning new knowledge.",
    "start": "4431960",
    "end": "4437405"
  },
  {
    "text": "They can learn general-purpose, um, models of the world, uh, they can prepare for tasks before you know what they are, uh,",
    "start": "4437405",
    "end": "4443150"
  },
  {
    "text": "such as exploration, can perform tasks in sequence, uh, and can learn continuously in like lifelong learning settings.",
    "start": "4443150",
    "end": "4451520"
  },
  {
    "text": "Uh, now, what's missing, uh, towards systems that are, that are kind of generalizable.",
    "start": "4451520",
    "end": "4457610"
  },
  {
    "text": "Uh, I think that now you're equipped with the tools of this task and that this is something that you are,",
    "start": "4457610",
    "end": "4463235"
  },
  {
    "text": "um, well-prepared to figure out. Great. Um, so a couple more reminders,",
    "start": "4463235",
    "end": "4469580"
  },
  {
    "text": "uh, the poster session is tomorrow. The final project report, please submit it before the 16th.",
    "start": "4469580",
    "end": "4475010"
  },
  {
    "text": "Uh, and please fill out your course evaluations. We love to hear, um,",
    "start": "4475010",
    "end": "4480350"
  },
  {
    "text": "any feedback that you have, either in the course evaluations, or over e-mail, or in person. Thank you. [APPLAUSE]",
    "start": "4480350",
    "end": "4490000"
  }
]