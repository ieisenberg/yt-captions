[
  {
    "start": "0",
    "end": "720"
  },
  {
    "text": "We're now going to go into more\ndetail on multinomial regression",
    "start": "720",
    "end": "4500"
  },
  {
    "text": "now.",
    "start": "4500",
    "end": "5000"
  },
  {
    "text": "What we're going\nto do is tell you",
    "start": "5000",
    "end": "6210"
  },
  {
    "text": "about a different\nclassification method, which",
    "start": "6210",
    "end": "8250"
  },
  {
    "text": "is called discriminant analysis,\nwhich is also very useful,",
    "start": "8250",
    "end": "11730"
  },
  {
    "text": "and it approaches a problem from\na really quite different point",
    "start": "11730",
    "end": "15660"
  },
  {
    "text": "of view.",
    "start": "15660",
    "end": "17610"
  },
  {
    "text": "In discriminant\nanalysis, the idea",
    "start": "17610",
    "end": "19980"
  },
  {
    "text": "is to model the distribution\nof X in each of the classes",
    "start": "19980",
    "end": "23160"
  },
  {
    "text": "separately and then use\nwhat's known as Bayes theorem",
    "start": "23160",
    "end": "26880"
  },
  {
    "text": "to flip things around to get\nthe probability of Y given X.",
    "start": "26880",
    "end": "32899"
  },
  {
    "text": "In this case, for linear\ndiscriminant analysis,",
    "start": "32900",
    "end": "36470"
  },
  {
    "text": "we're going to use Gaussian\ndistributions for each class",
    "start": "36470",
    "end": "39770"
  },
  {
    "text": "and that's going to lead to\nlinear or quadratic discriminant",
    "start": "39770",
    "end": "42800"
  },
  {
    "text": "analysis.",
    "start": "42800",
    "end": "43730"
  },
  {
    "text": "So those are the\ntwo popular forms.",
    "start": "43730",
    "end": "46160"
  },
  {
    "text": "But as you'll see, this\napproach is quite general",
    "start": "46160",
    "end": "49190"
  },
  {
    "text": "and other distributions\ncan be used as well,",
    "start": "49190",
    "end": "51890"
  },
  {
    "text": "but we'll focus on\nnormal distributions.",
    "start": "51890",
    "end": "56559"
  },
  {
    "text": "So what is Bayes theorem\nfor classification?",
    "start": "56560",
    "end": "59410"
  },
  {
    "text": "So it sounds pretty\nscary, but not too bad.",
    "start": "59410",
    "end": "62739"
  },
  {
    "text": "So of course, Thomas Bayes\nwas a famous mathematician",
    "start": "62740",
    "end": "65770"
  },
  {
    "text": "and his name now today\nrepresents a burgeoning subfield",
    "start": "65770",
    "end": "69880"
  },
  {
    "text": "of statistical and\nprobabilistic modeling.",
    "start": "69880",
    "end": "73210"
  },
  {
    "text": "But here we're going to focus\non a very simple result, which",
    "start": "73210",
    "end": "75850"
  },
  {
    "text": "is known as Bayes' theorem.",
    "start": "75850",
    "end": "78040"
  },
  {
    "text": "And it says that\nthe probability of Y",
    "start": "78040",
    "end": "81970"
  },
  {
    "text": "equals k, given X\nequals x, so the idea is",
    "start": "81970",
    "end": "86080"
  },
  {
    "text": "you've got two variables, in\nthis case, we've got Y and X,",
    "start": "86080",
    "end": "90910"
  },
  {
    "text": "and we are looking at aspects\nof their joint distribution.",
    "start": "90910",
    "end": "95020"
  },
  {
    "text": "So this is what we after,\nthe probability of Y",
    "start": "95020",
    "end": "97420"
  },
  {
    "text": "equals k given X. And Bayes'\ntheorem says you can flip things",
    "start": "97420",
    "end": "102850"
  },
  {
    "text": "around, you can write\nthat as the probability",
    "start": "102850",
    "end": "105520"
  },
  {
    "text": "that X is x given\nY equals k, that's",
    "start": "105520",
    "end": "108899"
  },
  {
    "text": "the first piece\non the top there,",
    "start": "108900",
    "end": "111900"
  },
  {
    "text": "multiplied by the marginal\nprobability or prior probability",
    "start": "111900",
    "end": "115980"
  },
  {
    "text": "that Y is k.",
    "start": "115980",
    "end": "118640"
  },
  {
    "text": "And then divided by the marginal\nprobability that X equals x.",
    "start": "118640",
    "end": "123920"
  },
  {
    "text": "So this is just a formula\nfrom probability theory.",
    "start": "123920",
    "end": "127640"
  },
  {
    "text": "But it turns out\nit's really useful",
    "start": "127640",
    "end": "129740"
  },
  {
    "text": "and is the basis for\ndiscriminant analysis.",
    "start": "129740",
    "end": "134200"
  },
  {
    "text": "And so we write things\nslightly differently",
    "start": "134200",
    "end": "137610"
  },
  {
    "text": "in the case of\ndiscriminant analysis.",
    "start": "137610",
    "end": "140490"
  },
  {
    "text": "So this probability Y\nequals k is written as pi k.",
    "start": "140490",
    "end": "148260"
  },
  {
    "text": "So if there's three\nclasses, there's",
    "start": "148260",
    "end": "151720"
  },
  {
    "text": "going to be three values for pi,\njust the probability for each",
    "start": "151720",
    "end": "154350"
  },
  {
    "text": "of the classes.",
    "start": "154350",
    "end": "155260"
  },
  {
    "text": "But here we've got class\nlittle k, so that's pi k.",
    "start": "155260",
    "end": "159674"
  },
  {
    "text": "Probability that X is x\ngiven Y equals k, well",
    "start": "159674",
    "end": "164760"
  },
  {
    "text": "if X is a quantitative\nvariable, what we",
    "start": "164760",
    "end": "168989"
  },
  {
    "text": "write for that is the density.",
    "start": "168990",
    "end": "171130"
  },
  {
    "text": "So that's the\nprobability density",
    "start": "171130",
    "end": "173160"
  },
  {
    "text": "function for X in class k.",
    "start": "173160",
    "end": "177120"
  },
  {
    "text": "And then the marginal\nprobability of X",
    "start": "177120",
    "end": "180720"
  },
  {
    "text": "is just this\nexpression over here.",
    "start": "180720",
    "end": "184370"
  },
  {
    "text": "So this is summing\nover all the classes.",
    "start": "184370",
    "end": "187390"
  },
  {
    "start": "187390",
    "end": "190505"
  },
  {
    "text": "And so that's how we\nuse Bayes theorem to get",
    "start": "190505",
    "end": "193520"
  },
  {
    "text": "to the probabilities of\ninterest, which is Y equals",
    "start": "193520",
    "end": "196250"
  },
  {
    "text": "k given X. Now at this point\nit's still quite general,",
    "start": "196250",
    "end": "200320"
  },
  {
    "text": "we can plug in any\nprobability densities,",
    "start": "200320",
    "end": "202920"
  },
  {
    "text": "but now what we're\ngoing to do is",
    "start": "202920",
    "end": "204420"
  },
  {
    "text": "go ahead and plug in\nthe Gaussian density",
    "start": "204420",
    "end": "207560"
  },
  {
    "text": "for f sub k of x.",
    "start": "207560",
    "end": "209230"
  },
  {
    "start": "209230",
    "end": "212909"
  },
  {
    "text": "Before we do that, let me\njust show you a little picture",
    "start": "212910",
    "end": "216225"
  },
  {
    "text": "to make things clear.",
    "start": "216225",
    "end": "217980"
  },
  {
    "text": "In the left hand plot,\nwhat have we got?",
    "start": "217980",
    "end": "220040"
  },
  {
    "text": "Here we've got a plot\nagainst x, single variable x,",
    "start": "220040",
    "end": "223290"
  },
  {
    "text": "and in the vertical axis, what\nwe've got is actually pi sub k",
    "start": "223290",
    "end": "228930"
  },
  {
    "text": "and multiplied by f sub k of x.",
    "start": "228930",
    "end": "233939"
  },
  {
    "text": "For both classes, k\nequals 1 and k equals 2.",
    "start": "233940",
    "end": "237750"
  },
  {
    "text": "Now, in this case, so remember\nin the previous slide,",
    "start": "237750",
    "end": "241680"
  },
  {
    "text": "the probability was essentially\nproportional to pi sub k times",
    "start": "241680",
    "end": "246450"
  },
  {
    "text": "f sub k of x.",
    "start": "246450",
    "end": "249190"
  },
  {
    "text": "And in this case,\nthe pis are the same",
    "start": "249190",
    "end": "251770"
  },
  {
    "text": "for both, so it's\nreally to do with which",
    "start": "251770",
    "end": "253510"
  },
  {
    "text": "density is the highest?",
    "start": "253510",
    "end": "255640"
  },
  {
    "text": "And you can see\nthat the decision",
    "start": "255640",
    "end": "259148"
  },
  {
    "text": "boundary or the vertical\ndashed line is at 0.",
    "start": "259149",
    "end": "262639"
  },
  {
    "text": "And that's the point at\nwhich the green density is",
    "start": "262640",
    "end": "264850"
  },
  {
    "text": "higher than the purple density.",
    "start": "264850",
    "end": "267120"
  },
  {
    "text": "And so anything to the left\nof 0 we classify as green,",
    "start": "267120",
    "end": "272410"
  },
  {
    "text": "and anything to the right\nwe'd classify as purple.",
    "start": "272410",
    "end": "276780"
  },
  {
    "text": "And it makes sense that\nthat's what we do there.",
    "start": "276780",
    "end": "281490"
  },
  {
    "text": "The right hand plot\nhas different priors.",
    "start": "281490",
    "end": "285289"
  },
  {
    "text": "So yeah, the probability of\n2 is 0.7 and of 1 is 0.3.",
    "start": "285290",
    "end": "292130"
  },
  {
    "text": "And now again we plot in\npi sub k times f sub k",
    "start": "292130",
    "end": "299990"
  },
  {
    "text": "of x against x and that bigger\nprior has bumped up the purple.",
    "start": "299990",
    "end": "306750"
  },
  {
    "text": "And what it's done is moved\nthe decision boundary slightly",
    "start": "306750",
    "end": "309800"
  },
  {
    "text": "to the left and that\nmakes sense too.",
    "start": "309800",
    "end": "312229"
  },
  {
    "text": "Again, it's where they\nintersect and it makes sense",
    "start": "312230",
    "end": "315260"
  },
  {
    "text": "as well because we've got\nmore purples here, actually,",
    "start": "315260",
    "end": "319550"
  },
  {
    "text": "I'll say they're pinks,\nit looks purple to me.",
    "start": "319550",
    "end": "323780"
  },
  {
    "text": "There's more of them.",
    "start": "323780",
    "end": "324810"
  },
  {
    "text": "So everything else\nbeing equal, we're",
    "start": "324810",
    "end": "326960"
  },
  {
    "text": "going to make less\nmistakes if we classify",
    "start": "326960",
    "end": "329400"
  },
  {
    "text": "it two purples and two greens.",
    "start": "329400",
    "end": "333310"
  },
  {
    "text": "So that's how these\npriors and the densities",
    "start": "333310",
    "end": "337360"
  },
  {
    "text": "play a role in classification.",
    "start": "337360",
    "end": "340039"
  },
  {
    "text": "So why discriminant analysis?",
    "start": "340040",
    "end": "341643"
  },
  {
    "text": "It seemed like\nlogistic regression",
    "start": "341643",
    "end": "343060"
  },
  {
    "text": "was a pretty good tool.",
    "start": "343060",
    "end": "344419"
  },
  {
    "text": "Well, it is, but it\nturns out there's",
    "start": "344420",
    "end": "346660"
  },
  {
    "text": "a room for discriminant\nanalysis as well.",
    "start": "346660",
    "end": "349570"
  },
  {
    "text": "And so there's three\npoints we make here,",
    "start": "349570",
    "end": "352480"
  },
  {
    "text": "when the classes\nare well separated,",
    "start": "352480",
    "end": "354760"
  },
  {
    "text": "it turns out that the\nparameter estimates",
    "start": "354760",
    "end": "357130"
  },
  {
    "text": "for logistic regression\nare surprisingly unstable.",
    "start": "357130",
    "end": "360940"
  },
  {
    "text": "In fact, if you've\ngot a feature that",
    "start": "360940",
    "end": "363040"
  },
  {
    "text": "separates the classes\nperfectly, the coefficients",
    "start": "363040",
    "end": "366460"
  },
  {
    "text": "go off to infinity.",
    "start": "366460",
    "end": "368180"
  },
  {
    "text": "So it really doesn't\ndo well there.",
    "start": "368180",
    "end": "370539"
  },
  {
    "text": "Logistic regression\nwas developed largely",
    "start": "370540",
    "end": "374320"
  },
  {
    "text": "in the biological\nand medical fields",
    "start": "374320",
    "end": "376060"
  },
  {
    "text": "where you never found\nsuch strong predictors.",
    "start": "376060",
    "end": "380650"
  },
  {
    "text": "Now, you can do things to make\nlogistic regression better",
    "start": "380650",
    "end": "385750"
  },
  {
    "text": "behave, but it turns out linear\ndiscriminant analysis doesn't",
    "start": "385750",
    "end": "388870"
  },
  {
    "text": "suffer from this problem.",
    "start": "388870",
    "end": "390169"
  },
  {
    "text": "And is better behaved\nin those situations.",
    "start": "390170",
    "end": "393820"
  },
  {
    "text": "We'll say if n is small,\nthe sample size is small",
    "start": "393820",
    "end": "396700"
  },
  {
    "text": "and the distribution\nof the predictors x",
    "start": "396700",
    "end": "398830"
  },
  {
    "text": "is approximately normal\nin each of the classes,",
    "start": "398830",
    "end": "402370"
  },
  {
    "text": "it turns out the discriminant\nmodel is, again, more stable",
    "start": "402370",
    "end": "406030"
  },
  {
    "text": "than logistic regression.",
    "start": "406030",
    "end": "408780"
  },
  {
    "text": "And finally, if we got more\nthan two classes, we'll see,",
    "start": "408780",
    "end": "412430"
  },
  {
    "text": "logistic regression gives us\nnice low dimensional views",
    "start": "412430",
    "end": "415020"
  },
  {
    "text": "of the data.",
    "start": "415020",
    "end": "415552"
  },
  {
    "text": "And the other point, remember\nthe very first section",
    "start": "415553",
    "end": "417720"
  },
  {
    "text": "we showed that the Bayes rule,\nif you have the right population",
    "start": "417720",
    "end": "420345"
  },
  {
    "text": "model, the Bayes' rule is\nthe best you can possibly do.",
    "start": "420345",
    "end": "423720"
  },
  {
    "text": "So if our normal\nassumption is right here,",
    "start": "423720",
    "end": "425935"
  },
  {
    "text": "then the discriminant\nanalysis from the Bayes rule",
    "start": "425935",
    "end": "429285"
  },
  {
    "text": "is the best you can possibly do.",
    "start": "429285",
    "end": "431110"
  },
  {
    "text": "Good point, Rob.",
    "start": "431110",
    "end": "433129"
  }
]