[
  {
    "start": "0",
    "end": "5520"
  },
  {
    "text": "Hi everyone, happy Wednesday. Some reminders for the course--",
    "start": "5520",
    "end": "10570"
  },
  {
    "text": "so Homework 1 is due tonight. Maybe there are a little\nbit fewer people today because maybe people are\nworking on it right now.",
    "start": "10570",
    "end": "17873"
  },
  {
    "text": "Maybe we should have made\nit due before the lecture. Um-- ah yes. Homework 2 isn't quite out yet,\nbut it will be out tonight.",
    "start": "17873",
    "end": "24375"
  },
  {
    "text": " Also we would love for you\nto fill out the project",
    "start": "24375",
    "end": "29790"
  },
  {
    "text": "survey by today as well. Again, this is non-binding. We're mostly using\nit to assign mentors",
    "start": "29790",
    "end": "36000"
  },
  {
    "text": "and it's helpful\nif you submit it. If you don't submit\nit by today, then we will give you a mentor after\nyou submit your proposal.",
    "start": "36000",
    "end": "44790"
  },
  {
    "text": "Also tomorrow, we\nhave a tutorial on variational inference. This will be super useful\nif you want to understand",
    "start": "44790",
    "end": "50430"
  },
  {
    "text": "parts of the lecture on Monday. It's not required if you--\nalso the lecture on Monday--",
    "start": "50430",
    "end": "57149"
  },
  {
    "text": "I think it's pretty useful\nfor getting some intuition behind the ideas in multitask\nlearning and meta learning,",
    "start": "57150",
    "end": "63239"
  },
  {
    "text": "and like what task\nstructure really means. But if you aren't interested in\nkind of diving deeper into that",
    "start": "63240",
    "end": "70740"
  },
  {
    "text": "then it's also\nfine to skip that. The tutorial will be recorded\nand made available on Canvas.",
    "start": "70740",
    "end": "78494"
  },
  {
    "text": " Yeah. Where is the tutorial\ngoing to take place?",
    "start": "78495",
    "end": "83744"
  },
  {
    "text": "On Zoom? It will be on Zoom, yeah. Awesome. Yeah. If people have\nstrong preferences",
    "start": "83744",
    "end": "88770"
  },
  {
    "text": "for in-person things,\nwe can try to do that for the loss tutorial. But there's a huge room\nshortage on campus, and so the registrar\nhasn't been giving us",
    "start": "88770",
    "end": "95670"
  },
  {
    "text": "rooms when we ask for them. Cool. So the question\nfor today is, how",
    "start": "95670",
    "end": "104120"
  },
  {
    "text": "should we define\ntasks if we want good meta-learning performance?",
    "start": "104120",
    "end": "109250"
  },
  {
    "text": "And we'll be looking at\ntwo different aspects of this problem. We'll briefly recap\nmeta-learning approaches",
    "start": "109250",
    "end": "115760"
  },
  {
    "text": "and task construction. This will give you a chance\nto also ask any questions that you have from the content\nfrom the past three lectures.",
    "start": "115760",
    "end": "122510"
  },
  {
    "text": "Then we'll talk about\nthe memorization problem in meta-learning,\npotential solutions for it, and scenarios where we\ncan do meta-learning",
    "start": "122510",
    "end": "130369"
  },
  {
    "text": "without any tasks\nbeing provided. One disclaimer is that a\nlot of the topics that we'll",
    "start": "130370",
    "end": "136489"
  },
  {
    "text": "cover today are really\nat the edge of research. And so I encourage\nyou to ask questions.",
    "start": "136490",
    "end": "143209"
  },
  {
    "text": "But also, sometimes the\nanswer to those questions will be that it's an open\nquestion that the field hasn't",
    "start": "143210",
    "end": "148730"
  },
  {
    "text": "figured out yet. The goals by the\nend of the lecture are to understand what this\nmemorization problem is,",
    "start": "148730",
    "end": "155660"
  },
  {
    "text": "when and how it occurs-- this is super\nimportant if you're going to do a project\non meta-learning. Because you might inadvertently\nset things up in a way",
    "start": "155660",
    "end": "162109"
  },
  {
    "text": "that causes this problem. It'll also help you\nunderstand techniques for constructing\ntasks automatically.",
    "start": "162110",
    "end": "170690"
  },
  {
    "text": "Great. So to recap, we\ntalked about three different meta-learning\napproaches. One is a black-box approach\nthat parametrizes this learner",
    "start": "170690",
    "end": "177740"
  },
  {
    "text": "as a neural network. This is very\nexpressive but it also leads to a challenging\noptimization problem.",
    "start": "177740",
    "end": "184700"
  },
  {
    "text": "We also saw\noptimization-based approaches, where you embed an optimization\ninto the learning process",
    "start": "184700",
    "end": "191930"
  },
  {
    "text": "and you optimize it to do well\nwith small amounts of data.",
    "start": "191930",
    "end": "198049"
  },
  {
    "text": "This builds in the structure\nof the optimization into the meta-learning process,\nwhich is quite helpful.",
    "start": "198050",
    "end": "204209"
  },
  {
    "text": "It also requires a\nsecond-order optimization. In the lecture on\nMonday this week",
    "start": "204210",
    "end": "210050"
  },
  {
    "text": "we also covered\nnon-parametric meta-learning. And the way that\nthis works is we compute an embedding of the\nexamples in the training data",
    "start": "210050",
    "end": "218990"
  },
  {
    "text": "and the examples for\nour query example. And then we essentially do\nsome form of nearest neighbors",
    "start": "218990",
    "end": "224569"
  },
  {
    "text": "in order to-- and then a dot product\nwith the labels in order to make a prediction.",
    "start": "224570",
    "end": "230459"
  },
  {
    "text": "So really the key idea\nhere is to have some sort of non-parametric learner. You want to do\nthis in the context",
    "start": "230460",
    "end": "236265"
  },
  {
    "text": "of some sort of parametric\nembedding function or distance function. And so this could be\nk-nearest neighbors",
    "start": "236265",
    "end": "242870"
  },
  {
    "text": "to examples or k-nearest\nneighbors to prototypes. This is pretty easy to optimize.",
    "start": "242870",
    "end": "247958"
  },
  {
    "text": "It's also computationally\npretty fast because you can just get the\nanswer through a forward pass through this computation graph.",
    "start": "247958",
    "end": "255530"
  },
  {
    "text": "It's also largely restricted\nto classification, and so if you have problems\nwith continuous labels,",
    "start": "255530",
    "end": "260898"
  },
  {
    "text": "this isn't as applicable. We've also talked\nabout constructing",
    "start": "260899",
    "end": "266330"
  },
  {
    "text": "task distributions. And one of the kind\nof canonical examples that we've looked\nat throughout is",
    "start": "266330",
    "end": "272419"
  },
  {
    "text": "using labeled images\nfrom previous classes in order to construct\nthese image classification",
    "start": "272420",
    "end": "277820"
  },
  {
    "text": "problems where each\ntask corresponded to a different N-way K-shot\nclassification problem.",
    "start": "277820",
    "end": "286096"
  },
  {
    "text": "And then we also looked\nat a couple examples briefly of other ways\nthat we could form tasks.",
    "start": "286096",
    "end": "291229"
  },
  {
    "text": "For example, we could\nhave tasks correspond to different\nregions of the world where you want to quickly\nadapt to a new region,",
    "start": "291230",
    "end": "296840"
  },
  {
    "text": "or tasks could correspond\nto different objects, and you want to be able to\ngeneralize to new objects",
    "start": "296840",
    "end": "303169"
  },
  {
    "text": "from, say, a single\ndemonstration. Great, any questions on\nthis kind of review content?",
    "start": "303170",
    "end": "311150"
  },
  {
    "start": "311150",
    "end": "316530"
  },
  {
    "text": "OK, great. So now let's talk\nabout memorization.",
    "start": "316530",
    "end": "322980"
  },
  {
    "text": "And before we get\ninto that, I am going to give you a thought exercise. And this won't actually relate\nto task construction quite yet.",
    "start": "322980",
    "end": "332160"
  },
  {
    "text": "And in particular, say you have\na black box meta-learner that takes as input the support set\nor the training set for task i,",
    "start": "332160",
    "end": "339195"
  },
  {
    "text": "and then uses that to make\npredictions for new examples.",
    "start": "339195",
    "end": "344400"
  },
  {
    "text": "And now let's say that you\nalso have a task identifier, like a one-hot ID for\neach of the tasks.",
    "start": "344400",
    "end": "351270"
  },
  {
    "text": "What if you also\npass this identifier into the network in addition\nto passing in the data?",
    "start": "351270",
    "end": "358925"
  },
  {
    "text": "Does anyone have\nthoughts on what would happen during\nmeta-training time if you did this? ",
    "start": "358925",
    "end": "367930"
  },
  {
    "text": "Yeah? And would it have\nbeen easier just to memorize like [INAUDIBLE]\ntask identifier [INAUDIBLE]",
    "start": "367930",
    "end": "377180"
  },
  {
    "text": "Yeah. So essentially what\nit can do is, now it can just learn a mapping\nfrom the task identifier to-- ",
    "start": "377180",
    "end": "384300"
  },
  {
    "text": "and the test image to the label\nand ignore the training data. And in particular,\nif the network",
    "start": "384300",
    "end": "390660"
  },
  {
    "text": "finds it difficult to\nlearn from the data, it can just ignore it\nand purely rely on ZI.",
    "start": "390660",
    "end": "396926"
  },
  {
    "text": "Yeah? Then for the homework, the\nsimilar [INAUDIBLE] right? Because then it\nwould just memorize",
    "start": "396926",
    "end": "403100"
  },
  {
    "text": "the order of the images? Yeah, you're like\ntwo steps ahead. OK. We'll get to there in\na couple of slides.",
    "start": "403100",
    "end": "409690"
  },
  {
    "text": "Now what happens\nif you pass in-- what happens at\nmeta test time now?",
    "start": "409690",
    "end": "417280"
  },
  {
    "text": "You now have a one-hot\nidentifier for a new task. What will happen?",
    "start": "417280",
    "end": "423960"
  },
  {
    "text": "Yeah? [INAUDIBLE] it will\ngeneralize terribly.",
    "start": "423960",
    "end": "430730"
  },
  {
    "text": "Yeah, so it will perform really\nterribly because the one-hot identifier is a new one-hot-- it's like a new integer\nthat it's never seen before.",
    "start": "430730",
    "end": "437160"
  },
  {
    "text": "And it won't really generalize\nat all to the new task because it doesn't know what\nit's supposed to be doing.",
    "start": "437160",
    "end": "442740"
  },
  {
    "text": "Cool, so this is a problem. Now, second thought exercise.",
    "start": "442740",
    "end": "448050"
  },
  {
    "text": "Now instead of having\na one-hot vector, we have like a natural\nlanguage description in the form of a paragraph\nthat describes what",
    "start": "448050",
    "end": "454500"
  },
  {
    "text": "the network is supposed to do. Does anyone have thoughts\nwhat will happen in this case?",
    "start": "454500",
    "end": "461669"
  },
  {
    "text": "Will it be the same as\nthe previous scenario? Will it be different? ",
    "start": "461670",
    "end": "478440"
  },
  {
    "text": "Yes? [INAUDIBLE] But it's\nstill based [INAUDIBLE]",
    "start": "478440",
    "end": "494380"
  },
  {
    "text": "description dependency. Yeah. So you're saying that it\nmight still kind of depend",
    "start": "494380",
    "end": "499425"
  },
  {
    "text": "purely on the description. It may not be quite\nas bad as before. Yeah. And it would depend\nif all the set",
    "start": "499425",
    "end": "505520"
  },
  {
    "text": "I's are independent of the-- because if all of the\ndescriptions were different,",
    "start": "505520",
    "end": "511040"
  },
  {
    "text": "what would be the\ndifference [INAUDIBLE]?? But if it's just going\nto [INAUDIBLE] so I think",
    "start": "511040",
    "end": "516243"
  },
  {
    "text": "you have to say [INAUDIBLE]\nif the description may have an overlap, like if\nthey're very similar",
    "start": "516244",
    "end": "522202"
  },
  {
    "text": "or the same working task,\nthen it would be different. But in the other case,\nI'd say it's the same. Yeah, so it's a great point.",
    "start": "522202",
    "end": "528529"
  },
  {
    "text": "If the paragraphs are\ncompletely unique, then it's essentially just a\nunique identifier for the tasks and you might have\nsomething similar come up.",
    "start": "528530",
    "end": "535220"
  },
  {
    "text": "Yeah? Maybe it would help you at\ntest time, because you can, I don't know, parse\nthat prompt and insert",
    "start": "535220",
    "end": "540930"
  },
  {
    "text": "at previous points\nyou see [INAUDIBLE]",
    "start": "540930",
    "end": "546300"
  },
  {
    "text": "Yeah. So you may actually do better at\nmeta-test time for the new task because you're not just\ngiven a new one-hot integer.",
    "start": "546300",
    "end": "552860"
  },
  {
    "text": "You're at least\ngiven a description. And if it can\nactually generalize from that description,\nthen it should actually",
    "start": "552860",
    "end": "557867"
  },
  {
    "text": "be able to solve the task. Cool. So in particular, a\nparagraph description",
    "start": "557867",
    "end": "565370"
  },
  {
    "text": "may be more complex to kind\nof infer the task from. If they're completely\nunique from one another,",
    "start": "565370",
    "end": "570949"
  },
  {
    "text": "it's going to be pretty simple. But if there's only like\none character that's different in that paragraph,\nit might be difficult",
    "start": "570950",
    "end": "577490"
  },
  {
    "text": "for it to interpret\nthat description. And so what the\nnetwork ends up using",
    "start": "577490",
    "end": "583970"
  },
  {
    "text": "will probably depend\non which is simpler. Like if it's simpler\nto learn from the data than to interpret\nthe paragraph text,",
    "start": "583970",
    "end": "589700"
  },
  {
    "text": "it will probably use the data. If it's simpler to use\nthe paragraph description, then it will probably\nuse that description.",
    "start": "589700",
    "end": "596240"
  },
  {
    "text": "And this also will kind of-- the result of meta-testing\nwill also depend on this. And if it can actually--",
    "start": "596240",
    "end": "603180"
  },
  {
    "text": "if it does actually start\nto interpret the text, then it will actually\ngeneralize to the test task.",
    "start": "603180",
    "end": "608279"
  },
  {
    "text": "Yeah? Maybe is the problem\nreally that we want the model to recognize that\nthe tasks are different,",
    "start": "608280",
    "end": "615683"
  },
  {
    "text": "it's basically telling that\nthe tasks are different? ",
    "start": "615683",
    "end": "623720"
  },
  {
    "text": "So you're asking like-- is the\nproblem here that essentially the network isn't-- [INAUDIBLE] But I\njust [INAUDIBLE]",
    "start": "623720",
    "end": "642940"
  },
  {
    "start": "642940",
    "end": "651370"
  },
  {
    "text": "Yeah, exactly. So there are different\nways to solve the task. And it could potentially kind\nof take the easy way out by just",
    "start": "651370",
    "end": "657040"
  },
  {
    "text": "like looking at an\nidentifier or looking at something that differentiates\nin the descriptions. And kind of the crux\nof it is that there--",
    "start": "657040",
    "end": "664030"
  },
  {
    "text": "it could essentially\nkind of minimize the meta-training loss. It can do well at all\nthe training tasks without looking at\nthe training data",
    "start": "664030",
    "end": "670240"
  },
  {
    "text": "at all in both of\nthese scenarios. And that means\nthat at test time, if you want it to actually\nimprove using the data,",
    "start": "670240",
    "end": "678451"
  },
  {
    "text": "it won't actually be able\nto do that because it learned to completely ignore\nthe data during meta-training.",
    "start": "678452",
    "end": "686240"
  },
  {
    "text": "Cool. So this is a pretty important\nconcept in meta-learning.",
    "start": "686240",
    "end": "691440"
  },
  {
    "text": "And the reason is\nthat now when you go about constructing\ntasks, you need to keep in mind whether or not\nyou can actually interpret--",
    "start": "691440",
    "end": "699200"
  },
  {
    "text": "whether it will actually\nlearn from the data or whether it will\nuse some other signal to determine what the task is.",
    "start": "699200",
    "end": "705710"
  },
  {
    "text": "And so in particular, one\nof the things that you saw in the homework is that--",
    "start": "705710",
    "end": "710750"
  },
  {
    "text": "we for example, kind\nof set up these tasks where we had different\ntasks corresponding to different sets of\nimages and we assigned",
    "start": "710750",
    "end": "718970"
  },
  {
    "text": "a label to each of the tasks. And we explicitly randomized the\nassignment of labels to images.",
    "start": "718970",
    "end": "727709"
  },
  {
    "text": "And so if you look\nat these-- in task 1, the bird class is assigned a\nlabel of 0, whereas in task 3,",
    "start": "727710",
    "end": "736700"
  },
  {
    "text": "the bird class is\nassigned a label of 1. And this means that\nyou can't actually",
    "start": "736700",
    "end": "741710"
  },
  {
    "text": "solve the task without\nlooking at the training data for each task. And because we're randomly\nassigning class labels",
    "start": "741710",
    "end": "748970"
  },
  {
    "text": "to image classes,\nthis means that tasks are what I would call\nmutually exclusive,",
    "start": "748970",
    "end": "754020"
  },
  {
    "text": "in the sense that you can't\nsolve all of the tasks with a single model. ",
    "start": "754020",
    "end": "760470"
  },
  {
    "text": "And so as a result, they\nhave to use the data in order to infer the label\nordering, and they have to use the data\nto learn the problem",
    "start": "760470",
    "end": "765855"
  },
  {
    "text": "or to learn the task.  Cool. So now what if the\nlabel assignment",
    "start": "765855",
    "end": "771500"
  },
  {
    "text": "is consistent across tasks? And what I mean by that is, what\nif you always assign the bird",
    "start": "771500",
    "end": "777050"
  },
  {
    "text": "class a label of 0,\nyou always assign the dog class a label of 2,\nthe landscape a label of 1,",
    "start": "777050",
    "end": "784160"
  },
  {
    "text": "and so forth? Do people have thoughts\non what will happen here?",
    "start": "784160",
    "end": "790750"
  },
  {
    "text": "Yeah? [INAUDIBLE]",
    "start": "790750",
    "end": "797740"
  },
  {
    "text": "Yeah. So you don't even-- you don't have to look\nat the data at all. And you don't even actually have\nto look at any sort of index",
    "start": "797740",
    "end": "803490"
  },
  {
    "text": "either. You can essentially\nclassify the examples in each of these blue boxes\ncompletely ignoring the data,",
    "start": "803490",
    "end": "812820"
  },
  {
    "text": "essentially. And so the tests\nare what I would call non-mutually exclusive.",
    "start": "812820",
    "end": "817899"
  },
  {
    "text": "A single function can\nsolve all of the tasks and learn a classifier\nthat classifies all the examples\nin the blue boxes",
    "start": "817900",
    "end": "823889"
  },
  {
    "text": "without looking at\nthe green boxes. And essentially the\nnetwork can simply",
    "start": "823890",
    "end": "829200"
  },
  {
    "text": "learn to classify inputs\nirrespective of the training set. Yeah?",
    "start": "829200",
    "end": "835580"
  },
  {
    "text": "So in this case, [INAUDIBLE] ",
    "start": "835580",
    "end": "843710"
  },
  {
    "text": "Yeah, great question. So it's not actually\ngoing to be looking at anything in the green boxes. It's going to be looking at\nthe images in the blue box--",
    "start": "843710",
    "end": "852500"
  },
  {
    "text": "the image that's provided\nat meta-test time. And then it's going to\npredict the label from that.",
    "start": "852500",
    "end": "857600"
  },
  {
    "text": "It doesn't need\nany task identifier because it can just\nlook at the image and predict the label\nbecause the labels are all",
    "start": "857600",
    "end": "862940"
  },
  {
    "text": "consistent across all the tasks. Yeah? So basically, you can have\na classification problem?",
    "start": "862940",
    "end": "869230"
  },
  {
    "text": "Yeah. It will become a vanilla\nclassification problem. Yeah? [INAUDIBLE]",
    "start": "869230",
    "end": "878910"
  },
  {
    "text": "Yeah. So it's important\nthat the labels are consistent\nacross all the tasks, and also across the\nsupport and query sets.",
    "start": "878910",
    "end": "884640"
  },
  {
    "text": " Yeah? And would the same-- like,\nand would the same thing",
    "start": "884640",
    "end": "891950"
  },
  {
    "text": "happen if that was\n[INAUDIBLE] 0 and 1, so we don't have consistency\nacross the tasks,",
    "start": "891950",
    "end": "898040"
  },
  {
    "text": "but like 0 and 1, I'm always\ntaking the first images, and my test is always\nthe first image group.",
    "start": "898040",
    "end": "903449"
  },
  {
    "text": "And just learn\n[INAUDIBLE] 0 and 1. Yeah, so if you don't\nshuffle the order of images",
    "start": "903450",
    "end": "909199"
  },
  {
    "text": "that you pass into the\nnetwork in the query sets, if you don't shuffle those, then\nit will always just memorize,",
    "start": "909200",
    "end": "915650"
  },
  {
    "text": "the first thing\nthat you give it is a label of 0, the\nsecond thing you give it is label\nof 1, and so forth. And in that scenario\nyou also don't have",
    "start": "915650",
    "end": "920660"
  },
  {
    "text": "to look at the training data. Yeah? So alternatively, if\nwe have to shuffle the training data [INAUDIBLE]",
    "start": "920660",
    "end": "929462"
  },
  {
    "text": "So if you shuffle\nthe training data and keep the query set\nin order, you still have that same problem\nbecause it can still remember that the first\nquery image is a 0,",
    "start": "929462",
    "end": "936260"
  },
  {
    "text": "the second query image\nis a 1, and so forth. Yeah? [INAUDIBLE]",
    "start": "936260",
    "end": "942479"
  },
  {
    "start": "942479",
    "end": "968020"
  },
  {
    "text": "So if your test examples--\nif your query set always passes in order where\nthe first example is 0,",
    "start": "968020",
    "end": "974395"
  },
  {
    "text": "second example is\n1 and so forth, and you have a\nrecurrent network, then it can just essentially\nlearn how to count.",
    "start": "974395",
    "end": "979480"
  },
  {
    "text": "And in that case, it\nactually doesn't even have to look at the\nimages in the query set. Not only does it not\nhave to look at the data,",
    "start": "979480",
    "end": "985250"
  },
  {
    "text": "it can just not look\nat the images at all and just learn how to count\nand say like, this image is 0, this image is a 1, and so forth.",
    "start": "985250",
    "end": "991960"
  },
  {
    "text": "Whereas in some\napproaches, if you don't have a recurrent\nnetwork and it's just a feedforward model, then\nwhether or not you shuffle",
    "start": "991960",
    "end": "998740"
  },
  {
    "text": "the query set is less important. Yeah? Why don't we just\nrandomize [INAUDIBLE]",
    "start": "998740",
    "end": "1006200"
  },
  {
    "text": "Yeah, so-- yeah. If you randomize the ordering\nthat you pass in the images and you also\nrandomize the labels--",
    "start": "1006200",
    "end": "1013708"
  },
  {
    "text": "you will of course,\nwant to make the labels consistent between\nsupport and query. But if you randomize\neverything else",
    "start": "1013708",
    "end": "1018890"
  },
  {
    "text": "then you won't run\ninto this issue.  And so in particular, if\nyou have this consistent",
    "start": "1018890",
    "end": "1028040"
  },
  {
    "text": "ordering across all\nthe tasks, then it could ignore the\ntraining data set. This is true for a\nblack-box approach where it will just\nlearn this classifier.",
    "start": "1028040",
    "end": "1035399"
  },
  {
    "text": "This is actually also true for\noptimization-based approaches as well, where it\ncan essentially",
    "start": "1035400",
    "end": "1040549"
  },
  {
    "text": "learn a network for which the\ngradient update doesn't really actually affect anything. And it will still memorize\nand still learn a classifier",
    "start": "1040550",
    "end": "1047660"
  },
  {
    "text": "that ignores the training data. ",
    "start": "1047660",
    "end": "1055010"
  },
  {
    "text": "Cool. And so the other\nthing that's a problem is if the label\nordering is consistent",
    "start": "1055010",
    "end": "1060170"
  },
  {
    "text": "and it learns to ignore\nthe training data. Then if you give it examples\nof new training classes",
    "start": "1060170",
    "end": "1066470"
  },
  {
    "text": "and a support set of\nthose training classes, then it's not going to know\nwhat the correct label is",
    "start": "1066470",
    "end": "1072890"
  },
  {
    "text": "for these images. And it won't be able to solve\nthis test task because it's",
    "start": "1072890",
    "end": "1078620"
  },
  {
    "text": "ignoring the data. And so if you\nactually evaluate MAML",
    "start": "1078620",
    "end": "1085460"
  },
  {
    "text": "with this meta-train,\nmeta-test setup, you get kind of a performance\nof around 7% for 20-way one-shot",
    "start": "1085460",
    "end": "1093320"
  },
  {
    "text": "and around 50% for\n20-way five-shot. And this is a lot worse\nthan if you actually",
    "start": "1093320",
    "end": "1099620"
  },
  {
    "text": "shuffle the meaning of the\nlabels across the tasks. Yeah?",
    "start": "1099620",
    "end": "1104710"
  },
  {
    "text": "And just one quick check. So instead of a lion,\nnow it would be a dog.",
    "start": "1104710",
    "end": "1111149"
  },
  {
    "text": "It would classify the dog as\na 2 instead of a 1, right? Yeah, exactly. So if the lion was a\ndog instead of a lion,",
    "start": "1111150",
    "end": "1117590"
  },
  {
    "text": "then it would output a 2. And that would be\nthe correct thing. This is a problem when\nyou have new classes. ",
    "start": "1117590",
    "end": "1125320"
  },
  {
    "text": "Cool. Now you might wonder is\nthis actually a problem? In image classification, we\ncould just shuffle the labels.",
    "start": "1125320",
    "end": "1131340"
  },
  {
    "text": "We know how to solve this. Maybe all is still OK.",
    "start": "1131340",
    "end": "1137190"
  },
  {
    "text": "But it's also not a problem if\nyou see the same image classes as you saw during training. So if we only saw like\ndogs and stuff like that,",
    "start": "1137190",
    "end": "1142950"
  },
  {
    "text": "then it's just a classification\nproblem and all is also fine. But it does become\na problem if you want to be able to adapt\nwith data for new tasks,",
    "start": "1142950",
    "end": "1149850"
  },
  {
    "text": "and you're not in the image\nclassification settings where somehow the task identifier\nis easy to interpret",
    "start": "1149850",
    "end": "1157380"
  },
  {
    "text": "from something in the data\nthat you're passing in. And so as another example,\nsay you wanted a robot",
    "start": "1157380",
    "end": "1165330"
  },
  {
    "text": "to be able to learn\ndifferent tasks and you also want to give\nit a natural language",
    "start": "1165330",
    "end": "1172200"
  },
  {
    "text": "description of the problem,\nlike closing the drawer, picking up the hammer,\nstacking the blocks.",
    "start": "1172200",
    "end": "1178812"
  },
  {
    "text": "And we want to give\nit those instructions so that it can actually\nquickly do those tasks and get a sense for what the\ntask-- what you're actually",
    "start": "1178812",
    "end": "1185340"
  },
  {
    "text": "asking it to do. But then if you give\nit a test task where you want it to learn\nhow to close a box,",
    "start": "1185340",
    "end": "1191190"
  },
  {
    "text": "it hasn't seen the close\nbox instruction before. But you are allowing\nit to learn from data.",
    "start": "1191190",
    "end": "1197820"
  },
  {
    "text": "If you try to do\nmeta-training on these tasks and then have it\nlearn this new task,",
    "start": "1197820",
    "end": "1203250"
  },
  {
    "text": "the robot is going\nto ignore the trials that you give it-- the\ntraining data that you give it for each task.",
    "start": "1203250",
    "end": "1208860"
  },
  {
    "text": "And not only will it not be\nable to generalize to this task, but it also won't be\nable to quickly learn it",
    "start": "1208860",
    "end": "1213900"
  },
  {
    "text": "because during meta-training,\nit just ignored all of the training data. ",
    "start": "1213900",
    "end": "1221420"
  },
  {
    "text": "As another example,\nsay we want to predict the pose of objects-- or sorry, the\norientation of objects.",
    "start": "1221420",
    "end": "1226919"
  },
  {
    "text": "And we have different objects\nin our meta-training data set,",
    "start": "1226920",
    "end": "1232855"
  },
  {
    "text": "all with a kind of a consistent\ncanonical orientation of each particular object.",
    "start": "1232855",
    "end": "1238539"
  },
  {
    "text": "Then if you give it\na new object, which has a new canonical\norientation, in this scenario",
    "start": "1238540",
    "end": "1245982"
  },
  {
    "text": "it can essentially memorize\nthe canonical orientation of all the training\nobjects and it won't generalize to the\nnew object from data",
    "start": "1245982",
    "end": "1251580"
  },
  {
    "text": "because it learned to memorize\nthe canonical orientation",
    "start": "1251580",
    "end": "1257519"
  },
  {
    "text": "rather than actually inferring\nthe orientation from data. ",
    "start": "1257520",
    "end": "1265220"
  },
  {
    "text": "So this is another scenario\nwhere the model wouldn't actually be able to learn how\nto predict the orientation",
    "start": "1265220",
    "end": "1270678"
  },
  {
    "text": "of a new object.  OK, so now that we've gone over\nthe problem of memorization,",
    "start": "1270678",
    "end": "1280840"
  },
  {
    "text": "is there something that\nwe can do about it? Yeah? So wouldn't that\nbe the [INAUDIBLE]..",
    "start": "1280840",
    "end": "1288010"
  },
  {
    "start": "1288010",
    "end": "1293180"
  },
  {
    "text": "Yeah. So essentially when it sees\na couch, it will know-- if the couch was in its\ntraining data, it will know--",
    "start": "1293180",
    "end": "1300220"
  },
  {
    "text": "it can kind of essentially\nmemorize like-- I guess to predict the\norientation of an object,",
    "start": "1300220",
    "end": "1306500"
  },
  {
    "text": "you need some\nreference point, which is what I'm referring to as\nthe canonical orientation. And when it sees\na couch, it will",
    "start": "1306500",
    "end": "1312580"
  },
  {
    "text": "know that the canonical\norientation is maybe facing the camera or something. And then it can just directly\npredict the orientation",
    "start": "1312580",
    "end": "1318909"
  },
  {
    "text": "relative to the canonical\norientation it memorized. And it can do that for\nall of the objects.",
    "start": "1318910",
    "end": "1324310"
  },
  {
    "text": "It might be able to guess what\nthe canonical orientation is for a new object\nif it's something that is kind of sensible.",
    "start": "1324310",
    "end": "1330850"
  },
  {
    "text": "But in many cases\nthere isn't any sort of universal canonical\norientation for objects",
    "start": "1330850",
    "end": "1342130"
  },
  {
    "text": "necessarily that you can\ninfer from the object shape. And as a result, it's memorizing\nthis canonical orientation",
    "start": "1342130",
    "end": "1348820"
  },
  {
    "text": "in a way that allows it to\njust ignore the support set for each of the tasks. And then when it's\ngiven a new support set,",
    "start": "1348820",
    "end": "1354250"
  },
  {
    "text": "it won't actually be able to\nlearn from that support set. But how did that\nreference point -- on the meta training\ntask for the couch, the couch is just like\nrandomly competing.",
    "start": "1354250",
    "end": "1359961"
  },
  {
    "text": "So how did it know that there\nis one specific canonical",
    "start": "1359961",
    "end": "1369160"
  },
  {
    "text": "definition that it\nshould interpret? ",
    "start": "1369160",
    "end": "1375020"
  },
  {
    "text": "Yeah. So the labels will\nessentially tell you--",
    "start": "1375020",
    "end": "1382580"
  },
  {
    "text": "so from the image, you need\nto predict the orientation relative to the\ncanonical orientation.",
    "start": "1382580",
    "end": "1387670"
  },
  {
    "text": "And the labels will\ntell you essentially-- from multiple\nlabels of an object,",
    "start": "1387670",
    "end": "1392929"
  },
  {
    "text": "you can infer what the\ncanonical orientation is through the training process.",
    "start": "1392930",
    "end": "1397960"
  },
  {
    "text": " OK, so can we actually do\nsomething about this problem?",
    "start": "1397960",
    "end": "1406050"
  },
  {
    "text": "There is actually-- I'll present one solution\nthat we can solve--",
    "start": "1406050",
    "end": "1411600"
  },
  {
    "text": "we can do for it. I don't think that\nthere's really any like amazing\nsolution for it, but I'll present one thing that\nseems to work somewhat well.",
    "start": "1411600",
    "end": "1419940"
  },
  {
    "text": "And yeah, if anyone\nhas any other ideas we can also discuss that. So if the tasks are\nmutually exclusive,",
    "start": "1419940",
    "end": "1426283"
  },
  {
    "text": "then a single function can\nsolve all the tasks-- or sorry, cannot solve all the tasks. And this could be due to\nlabel shuffling or hiding",
    "start": "1426283",
    "end": "1431683"
  },
  {
    "text": "information. If they're\nnon-mutually exclusive, this is where we\nrun into problems. Because a single function\ncan solve all of the tasks.",
    "start": "1431683",
    "end": "1438539"
  },
  {
    "text": "And in these cases there's\nactually multiple solutions to the meta-learning problem. ",
    "start": "1438540",
    "end": "1445660"
  },
  {
    "text": "And in particular one solution-- for example, in the case of\nthe post-prediction example--",
    "start": "1445660",
    "end": "1453240"
  },
  {
    "text": "is to memorize the canonical\npose of all the training objects in your meta-parameters\nand ignore the training data.",
    "start": "1453240",
    "end": "1461039"
  },
  {
    "text": "And another solution\nis to not carry any information\nabout the canonical pose-- infer that\nfrom the training",
    "start": "1461040",
    "end": "1467130"
  },
  {
    "text": "data for each of the tasks.  And in reality, there's actually\nan entire spectrum of solutions",
    "start": "1467130",
    "end": "1474640"
  },
  {
    "text": "that kind of interpolate\nbetween these two things, that can rely more or less on\nmemorization of information",
    "start": "1474640",
    "end": "1481810"
  },
  {
    "text": "versus inferring that\ninformation from the training data. ",
    "start": "1481810",
    "end": "1487720"
  },
  {
    "text": "And this also suggests\na potential solution to the problem, which is-- there are these different--\nthere's essentially",
    "start": "1487720",
    "end": "1494070"
  },
  {
    "text": "kind of different\ninformation that you can use to solve each of\nthe meta-training tasks. And what you can try\nto do is essentially",
    "start": "1494070",
    "end": "1500250"
  },
  {
    "text": "try to control the\ninformation flow. You can try to encourage it\nto use information from theta",
    "start": "1500250",
    "end": "1508888"
  },
  {
    "text": "or you can try to encourage\nit to use information from the training data. ",
    "start": "1508888",
    "end": "1517050"
  },
  {
    "text": "Does this make sense? Any questions? ",
    "start": "1517050",
    "end": "1528645"
  },
  {
    "text": "OK. So if we want to control\ninformation flow--",
    "start": "1528645",
    "end": "1535450"
  },
  {
    "text": "say we want the\nsolution in turquoise. We want it to only acquire the\ninformation from the training data and not actually\nstore the data in theta.",
    "start": "1535450",
    "end": "1544540"
  },
  {
    "text": "Then what we could try\nto do is to maximize the mutual information\nor the information",
    "start": "1544540",
    "end": "1553179"
  },
  {
    "text": "overlap between the predicted\nlabels and the training data to encourage it to use\ninformation from the data.",
    "start": "1553180",
    "end": "1560480"
  },
  {
    "text": "Unfortunately, this objective\nis intractable to optimize. We can't directly measure\nmutual information very easily.",
    "start": "1560480",
    "end": "1568983"
  },
  {
    "text": "But what we're\ngoing to do instead is, we can try to minimize\nthe meta-training loss and also minimize the amount\nof information in theta.",
    "start": "1568983",
    "end": "1576740"
  },
  {
    "text": "And what this will\ndo is-- because it can either store this\ninformation in theta or extract it from\nthe training data.",
    "start": "1576740",
    "end": "1583419"
  },
  {
    "text": "Then minimizing the\ninformation in theta will encourage it\nto use information",
    "start": "1583420",
    "end": "1589179"
  },
  {
    "text": "from the training\ndata and encourage it to use the training data. And this is because\nyou're also minimizing",
    "start": "1589180",
    "end": "1596020"
  },
  {
    "text": "the meta-training loss. Yeah? And minimizing\n[INAUDIBLE] on theta or--",
    "start": "1596020",
    "end": "1604220"
  },
  {
    "text": "Yeah. So the way that we're\ngoing to do this is-- we'll first minimize the\nmeta-training loss. And then minimizing\ninformation in theta",
    "start": "1604220",
    "end": "1611320"
  },
  {
    "text": "is a difficult\nobjective to optimize, but we'll use what's referred\nto as a kind of information",
    "start": "1611320",
    "end": "1618970"
  },
  {
    "text": "bottleneck, that tries\nto essentially regularize a distribution over theta to\nthis Gaussian distribution.",
    "start": "1618970",
    "end": "1626320"
  },
  {
    "text": "And so in particular,\nthe q of theta is representing--\nwe're essentially",
    "start": "1626320",
    "end": "1631450"
  },
  {
    "text": "going to be adding\nnoise to theta according to the standard\ndeviation, theta sub sigma.",
    "start": "1631450",
    "end": "1637720"
  },
  {
    "text": "And then we're going to\nencourage that distribution to look a lot like a\nGaussian distribution, which",
    "start": "1637720",
    "end": "1644620"
  },
  {
    "text": "is some prior\ndistribution p of theta. ",
    "start": "1644620",
    "end": "1649760"
  },
  {
    "text": "And this would ideally\nplace precedence on using information\nfrom the training data over storing\ninformation in theta.",
    "start": "1649760",
    "end": "1658600"
  },
  {
    "text": "Was there a question?  OK.",
    "start": "1658600",
    "end": "1665860"
  },
  {
    "text": "In general, this sort of\ninformation bottleneck type approach is maybe not something\nthat you've seen before.",
    "start": "1665860",
    "end": "1672970"
  },
  {
    "text": "We'll actually-- the tutorial\ntomorrow will actually cover things that are related\nto this particular kind",
    "start": "1672970",
    "end": "1679030"
  },
  {
    "text": "of approach. But you can\nessentially think of it as a regularization term that\nadds noise to the weights",
    "start": "1679030",
    "end": "1685270"
  },
  {
    "text": "and encourages that\ndistribution to look Gaussian. Yeah?",
    "start": "1685270",
    "end": "1690805"
  },
  {
    "text": "Can you just elaborate\non what q means? Yeah so q is a\nGaussian distribution",
    "start": "1690805",
    "end": "1697870"
  },
  {
    "text": "with mean of theta sub mu\nand variance theta sub sigma.",
    "start": "1697870",
    "end": "1705890"
  },
  {
    "text": "And so now we're actually\nstoring two parameter vectors, one for the mean and\none for the variance. And then when we actually\nuse the parameters,",
    "start": "1705890",
    "end": "1712330"
  },
  {
    "text": "what we're going to do\nis we will take the mean and add noise. It's basically a sample\nfrom this distribution.",
    "start": "1712330",
    "end": "1719980"
  },
  {
    "text": "In practice, theta\nsigma is going to be a diagonal\ncovariance matrix. So we can just represent the\ndiagonal entries as a vector.",
    "start": "1719980",
    "end": "1727720"
  },
  {
    "text": "And it'll essentially correspond\nto adding noise proportional to theta sub sigma.",
    "start": "1727720",
    "end": "1734390"
  },
  {
    "text": "Yeah? And what's p? Yeah, so p theta-- this is just a prior--",
    "start": "1734390",
    "end": "1739420"
  },
  {
    "text": "it's common to use a Gaussian\nprior here because you can measure the KL\ndivergence between Gaussians",
    "start": "1739420",
    "end": "1745900"
  },
  {
    "text": "in closed form,\nwhich is pretty nice. And then you can\nuse what's called the re-parametrization trick\nto actually differentiate",
    "start": "1745900",
    "end": "1751960"
  },
  {
    "text": "through the sampling\nprocess from a Gaussian. And this\nre-parametrization trick will be covered on\nThursday for people",
    "start": "1751960",
    "end": "1758590"
  },
  {
    "text": "who aren't familiar with it. Yeah? So I understand that\nwe're optimizing q",
    "start": "1758590",
    "end": "1763930"
  },
  {
    "text": "to be more similar to p. But like, I don't\nunderstand how we use q. Yeah, awesome.",
    "start": "1763930",
    "end": "1769600"
  },
  {
    "text": "So the way that we use q is-- we sample from it and\neach sample from it",
    "start": "1769600",
    "end": "1778330"
  },
  {
    "text": "will be a parameter vector. And we'll use that\nparameter vector to parametrize the meta-learner.",
    "start": "1778330",
    "end": "1787402"
  },
  {
    "text": "[INAUDIBLE] parameters\nlike q theta somehow,",
    "start": "1787402",
    "end": "1794325"
  },
  {
    "text": "and then we use it for learning. Yeah. And the-- so in this\ncase, theta is referring",
    "start": "1794326",
    "end": "1802630"
  },
  {
    "text": "to the meta-parameters. And we're essentially trying to\nregularize the meta-parameters, and so when we sample from\nthis, the result of that sample",
    "start": "1802630",
    "end": "1809200"
  },
  {
    "text": "will actually be the\nmeta-parameters rather than task-specific parameters. And you can apply this objective\nboth to black-box approaches",
    "start": "1809200",
    "end": "1819040"
  },
  {
    "text": "as well as to\noptimization-based approaches. Yeah? And would we have [INAUDIBLE]",
    "start": "1819040",
    "end": "1828005"
  },
  {
    "text": "Yeah, so this is essentially\nadding noise to the weights. You could also do something\nlike dropout on weights. I haven't actually seen\npeople do that before.",
    "start": "1828005",
    "end": "1835240"
  },
  {
    "text": "People normally do\ndropout on activations. But I would also imagine that\nthat would reduce information",
    "start": "1835240",
    "end": "1841990"
  },
  {
    "text": "in theta. So wouldn't it be the same\nlike if you were doing it on activiations [INAUDIBLE]",
    "start": "1841990",
    "end": "1851530"
  },
  {
    "text": "Right, so it will definitely\nlook similar to doing it on activations. It will be a little\nbit different because it's a matrix\nrather than a vector,",
    "start": "1851530",
    "end": "1858380"
  },
  {
    "text": "and so the outcome of the\nmatrix vector multiplication will be different. One thing that we\ndid actually try",
    "start": "1858380",
    "end": "1864700"
  },
  {
    "text": "was applying this regularizer\nto the activations rather than the weights, and we did actually\nfind that it worked better on the weights because\nthat's where it's storing",
    "start": "1864700",
    "end": "1870820"
  },
  {
    "text": "the canonical information-- or like the canonical\npose, for example. ",
    "start": "1870820",
    "end": "1877900"
  },
  {
    "text": "Cool. If you don't quite\nunderstand this equation, you can essentially think of\nit as adding noise to theta,",
    "start": "1877900",
    "end": "1883002"
  },
  {
    "text": "essentially with\nthe goal of trying to minimize the\ninformation in theta and encourage the model\nto use information",
    "start": "1883002",
    "end": "1889010"
  },
  {
    "text": "from the training set\nfor the support set. ",
    "start": "1889010",
    "end": "1894954"
  },
  {
    "text": "OK, and then you\ncan combine this with both black-approaches and\noptimization-based approaches.",
    "start": "1894954",
    "end": "1901240"
  },
  {
    "text": "Yeah? So I guess it's just trying\nto keep the [INAUDIBLE] is that what it's doing? ",
    "start": "1901240",
    "end": "1909772"
  },
  {
    "text": "It's actually trying to keep\nit close to a 0 mean standard Gaussian. If you initialize with the\n0 mean standard Gaussian,",
    "start": "1909772",
    "end": "1916710"
  },
  {
    "text": "then yes. Although you might\ninitialize somewhere else. So why is that stopping it from\n[INAUDIBLE] if that keeps it",
    "start": "1916710",
    "end": "1922555"
  },
  {
    "text": "further to the [INAUDIBLE] ",
    "start": "1922555",
    "end": "1935232"
  },
  {
    "text": "I guess one thing that is a\nlittle bit different than just trying to keep it close\nto the initialization, is we are actually\nadding noise now.",
    "start": "1935233",
    "end": "1941130"
  },
  {
    "text": "We're not just saying\nthat you should stay close to the initialization. We're saying that you should-- your weight should look similar\nto this particular distribution",
    "start": "1941130",
    "end": "1947970"
  },
  {
    "text": "when you add noise. And the noisy\nweights are the ones that you have to\nuse, not the mean.",
    "start": "1947970",
    "end": "1954630"
  },
  {
    "text": "Yeah? I just have a question on that. What is the advantage of\ncomputing this variational based regularizion versus a\nmore [INAUDIBLE] regularizer like L2 [INAUDIBLE] ",
    "start": "1954630",
    "end": "1963840"
  },
  {
    "text": "Yeah so you can also do\nL2 and meta parameters. And actually L2 is actually\nquite similar to this KL.",
    "start": "1963840",
    "end": "1971850"
  },
  {
    "text": "It's essentially assuming\nthat you fix the variance to 1",
    "start": "1971850",
    "end": "1977610"
  },
  {
    "text": "and you don't learn the\nvariance of this distribution. It is pretty important\nto add the noise though",
    "start": "1977610",
    "end": "1984360"
  },
  {
    "text": "to the weights. And you can\nessentially think of it as L2 regularization on the\nweights plus adding noise. ",
    "start": "1984360",
    "end": "1994747"
  },
  {
    "text": "OK, cool. So as an example for\nhow well this works-- if you take Omniglot\nand you don't shuffle",
    "start": "1994747",
    "end": "2000840"
  },
  {
    "text": "the labels such that the mapping\nthat corresponds between image",
    "start": "2000840",
    "end": "2007049"
  },
  {
    "text": "classes and labels is consistent\nacross all of the tasks-- so it's non-mutually exclusive.",
    "start": "2007050",
    "end": "2013540"
  },
  {
    "text": "Then for MAML we\nsee the performance like I mentioned before. There's another approach called\ntask-agnostic meta-learning",
    "start": "2013540",
    "end": "2020160"
  },
  {
    "text": "that tries to regularize in a\nway that encourages the model to predict different things,\nalthough it ends up being",
    "start": "2020160",
    "end": "2026010"
  },
  {
    "text": "something that the\nmodel can cheat around, whereas this kind of\nmeta-regularization",
    "start": "2026010",
    "end": "2032040"
  },
  {
    "text": "on the weights-- that's what the M,\nR, and W stand for-- does a lot better at\nallowing the model",
    "start": "2032040",
    "end": "2038190"
  },
  {
    "text": "to actually adapt from\nthe data, allowing it to ultimately generalize\nto new image classes.",
    "start": "2038190",
    "end": "2044443"
  },
  {
    "text": "And if you take the\npost-production task that I mentioned\nbefore, you also",
    "start": "2044443",
    "end": "2050340"
  },
  {
    "text": "see a pretty\nsignificant improvement in the mean squared error. But when you use the\nmeta-regularization compared",
    "start": "2050340",
    "end": "2056550"
  },
  {
    "text": "to not using\nmeta-regularization-- CNP is Conditional\nNeural Processes.",
    "start": "2056550",
    "end": "2062879"
  },
  {
    "text": "It corresponds to a\nblack-box approach. And you also see an improvement\nfor the black-box approaches as well.",
    "start": "2062880",
    "end": "2068760"
  },
  {
    "text": "Yeah? Could you please go back\nto the previous slide?",
    "start": "2068760",
    "end": "2073859"
  },
  {
    "text": "So I'm curious if I can replace\nthe [INAUDIBLE] be replaced",
    "start": "2073860",
    "end": "2086638"
  },
  {
    "text": "by [INAUDIBLE] I\nwant to maximize",
    "start": "2086639",
    "end": "2095288"
  },
  {
    "text": "those other [INAUDIBLE]\nas much as possible. But at the same time, I\nwant it to [INAUDIBLE]",
    "start": "2095289",
    "end": "2106420"
  },
  {
    "text": "Yeah. So the question is, can\nyou replace this KL term with an entity term\nthat encourages the weights to be more and\nmore random, essentially?",
    "start": "2106420",
    "end": "2112905"
  },
  {
    "text": "Yeah. I think that in\nprinciple, you should be able to do something like that.",
    "start": "2112905",
    "end": "2117960"
  },
  {
    "text": "I don't think that we\nexplicitly tried it. One challenge that\ncan come up is that-- I think that maximizing entropy\ncan be somewhat difficult.",
    "start": "2117960",
    "end": "2130579"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "2130580",
    "end": "2140470"
  },
  {
    "text": "I guess one issue\nthat might come up is, it might maximize the\nentropy of some dimensions",
    "start": "2140470",
    "end": "2146790"
  },
  {
    "text": "of the weights that\naren't important, and not others that it's\nactually using in practice.",
    "start": "2146790",
    "end": "2152520"
  },
  {
    "text": "And that might cause\nit to maximize entropy in a way that doesn't\nactually prevent memorization.",
    "start": "2152520",
    "end": "2160170"
  },
  {
    "text": "But I'd have to-- that's\nkind of my initial reaction. I'd have to think about it more.",
    "start": "2160170",
    "end": "2166690"
  },
  {
    "text": "Yeah? Well, I can maximize the entropy\nanyway, because [INAUDIBLE]",
    "start": "2166690",
    "end": "2173170"
  },
  {
    "text": "Yeah. So this is actually\nalready pretty close to maximum entropy. Because Gaussian distributions\nare like the-- for a fixed",
    "start": "2173170",
    "end": "2180210"
  },
  {
    "text": "variance, it's the kind of\nmaximal entropy distribution that you can get. So it is already doing\nsomething pretty similar",
    "start": "2180210",
    "end": "2186390"
  },
  {
    "text": "to maximum entropy. Yeah? [INAUDIBLE]",
    "start": "2186390",
    "end": "2192859"
  },
  {
    "text": "Yeah, so great question. So why not-- how does this\ncompare to just like shuffling in a way that kind of makes\nthe tasks mutually exclusive?",
    "start": "2192860",
    "end": "2200800"
  },
  {
    "text": "In general, I didn't-- I don't have them in this\nslide, but you can actually",
    "start": "2200800",
    "end": "2206190"
  },
  {
    "text": "get better performance by\nshuffling the labels in cases where that's possible to do.",
    "start": "2206190",
    "end": "2212220"
  },
  {
    "text": "In the post-prediction\ntask, it's a little bit harder to shuffle.",
    "start": "2212220",
    "end": "2218339"
  },
  {
    "text": "You can't just\nshuffle the labels. You actually need\nto like shuffle the canonical orientations of\ndifferent objects essentially.",
    "start": "2218340",
    "end": "2224970"
  },
  {
    "text": "And then in other\ncases, you just can't shuffle the labels,\nlike in the robotics example",
    "start": "2224970",
    "end": "2230520"
  },
  {
    "text": "that I mentioned before. In general, shuffling\nthe labels is more reliable than this\nregularizer in settings",
    "start": "2230520",
    "end": "2237457"
  },
  {
    "text": "where you can do it.  OK. And then one other\nthing I'll mention",
    "start": "2237457",
    "end": "2243180"
  },
  {
    "text": "is, if you compare this kind of\nregularization on the weights to things like weight decay--",
    "start": "2243180",
    "end": "2249390"
  },
  {
    "text": "like was asked-- and what's\ncalled Bayes by Backprop. ",
    "start": "2249390",
    "end": "2256905"
  },
  {
    "text": "You do actually see a pretty\nsignificant difference. It's not just as simple as just\nrunning standard regularization",
    "start": "2256905",
    "end": "2264270"
  },
  {
    "text": "on the model. And this weight\ndecay example here is one where we\ndidn't add noise. We just did weight\ndecay on the weights.",
    "start": "2264270",
    "end": "2272380"
  },
  {
    "text": "And so this is illustrating why\nthe noise is really important. Yeah?",
    "start": "2272380",
    "end": "2278600"
  },
  {
    "text": "How does this approach\ncompare to for example, on the post-prediction\ntask if you were just",
    "start": "2278600",
    "end": "2284330"
  },
  {
    "text": "to like randomly add\nsome labels to all of the labels within the task,\nessentially changing the point.",
    "start": "2284330",
    "end": "2289340"
  },
  {
    "text": " Yeah so that would-- if you add\na random angle to this task,",
    "start": "2289340",
    "end": "2298220"
  },
  {
    "text": "and you do that\nmultiple times to create additional copies\nof the task, then you are essentially making\nit such that the tasks are",
    "start": "2298220",
    "end": "2307881"
  },
  {
    "text": "non-mutually exclusive. And you wouldn't have the\nmemorization problem again. And I think that you--",
    "start": "2307882",
    "end": "2312980"
  },
  {
    "text": "I would guess that\nyou would probably do a bit better than using this\nkind of regularization approach",
    "start": "2312980",
    "end": "2319215"
  },
  {
    "text": "because you're\nactually-- in that case, actually forcing\nit to use the data. ",
    "start": "2319215",
    "end": "2327450"
  },
  {
    "text": "Cool, and then one other\nquick note that I'll mention is, you can actually look at-- try to theoretically derive\nthe generalization benefits",
    "start": "2327450",
    "end": "2336720"
  },
  {
    "text": "of this kind of regularizer. And if you let p of theta be\nan arbitrary distribution,",
    "start": "2336720",
    "end": "2345150"
  },
  {
    "text": "like a Gaussian\ndistribution, then you can actually derive\na generalization bound for the model.",
    "start": "2345150",
    "end": "2353400"
  },
  {
    "text": "This looks pretty\ncomplicated, but essentially what it is, is measuring\nthe generalization error. It's trying to upper bound that\ngeneralization error, which",
    "start": "2353400",
    "end": "2361079"
  },
  {
    "text": "corresponds to error on\nthe meta-training set and then also this\nmeta-regularization term.",
    "start": "2361080",
    "end": "2367210"
  },
  {
    "text": "And if you essentially do\nlike a Taylor expansion of the right hand side and\nchoose a particular value",
    "start": "2367210",
    "end": "2373170"
  },
  {
    "text": "of theta on the\nregularization weight, then you'll actually recover\nthe objective exactly.",
    "start": "2373170",
    "end": "2381672"
  },
  {
    "text": "I don't want to go into too\nmuch of the details here. But for people who\nare kind of interested in some of the theoretical\nside of things,",
    "start": "2381672",
    "end": "2388105"
  },
  {
    "text": "you might find this\ninteresting and you could look at the paper. ",
    "start": "2388105",
    "end": "2393420"
  },
  {
    "text": "Cool, and then\nthe proof for this relies heavily on\nthis other work that looks at kind of\ntheoretical aspects",
    "start": "2393420",
    "end": "2399839"
  },
  {
    "text": "of multitask learning. ",
    "start": "2399840",
    "end": "2405140"
  },
  {
    "text": "Cool. So to summarize the\nmemorization problem,",
    "start": "2405140",
    "end": "2411080"
  },
  {
    "text": "it's kind of an instance\nof meta overfitting, where you are essentially memorizing\nthe functions underlying",
    "start": "2411080",
    "end": "2418580"
  },
  {
    "text": "the training tasks. And this is somewhat analogous\nto standard overfitting in supervised learning, where\nyou're memorizing your training",
    "start": "2418580",
    "end": "2426072"
  },
  {
    "text": "data points. So you're kind of-- instead of memorizing\nyour data points, you're memorizing your tasks.",
    "start": "2426072",
    "end": "2432260"
  },
  {
    "text": "We see that one\nsolution to this is to try to control\nthe information flow. This is different from kind\nof standard regularization",
    "start": "2432260",
    "end": "2440240"
  },
  {
    "text": "in supervised\nlearning, where you're trying to regularize\nthe hypothesis class. But the solution for\ncontrolling the information flow",
    "start": "2440240",
    "end": "2447042"
  },
  {
    "text": "that we actually used is\nto actually regularize the description length of the\nmeta-parameters, which actually",
    "start": "2447042",
    "end": "2454310"
  },
  {
    "text": "does look pretty similar to\nregularizing your hypothesis class.",
    "start": "2454310",
    "end": "2460037"
  },
  {
    "text": "Although it is actually\nsomewhat interesting that the kind of\nregularization that we use-- this Gaussian prior\non the weights--",
    "start": "2460037",
    "end": "2465440"
  },
  {
    "text": " doesn't always\nactually work that well",
    "start": "2465440",
    "end": "2470540"
  },
  {
    "text": "for deep neural networks. And so it's kind of\ninteresting that you see these pretty drastic\nimprovements in some",
    "start": "2470540",
    "end": "2476420"
  },
  {
    "text": "of the meta-learning scenarios. But I wouldn't expect to see\nthose kinds of improvements in the standard\nsupervised scenarios.",
    "start": "2476420",
    "end": "2483079"
  },
  {
    "text": " OK, cool.",
    "start": "2483080",
    "end": "2489080"
  },
  {
    "text": "Any remaining questions? ",
    "start": "2489080",
    "end": "2498210"
  },
  {
    "text": "Cool. So now that we talked\nabout constructing tasks",
    "start": "2498210",
    "end": "2503230"
  },
  {
    "text": "in a way that is ideally\nmutually exclusive or requiring this\nsort of memorization,",
    "start": "2503230",
    "end": "2508570"
  },
  {
    "text": "let's talk about a\nscenario where maybe you don't have a lot of\nmeta-training tasks, or maybe you don't have\nany meta-training tasks",
    "start": "2508570",
    "end": "2515380"
  },
  {
    "text": "and you only have\nunlabeled data. And so in particular, one of the\nthings you might have noticed",
    "start": "2515380",
    "end": "2521740"
  },
  {
    "text": "is that few-shot\nlearning is awesome but you do actually require\ntasks to be constructed",
    "start": "2521740",
    "end": "2527800"
  },
  {
    "text": "from label data, or\nfrom demonstrations of many previous tasks or\nlabel data from other regions.",
    "start": "2527800",
    "end": "2533300"
  },
  {
    "text": "And so you still require a\nlot of labels ultimately.",
    "start": "2533300",
    "end": "2539050"
  },
  {
    "text": "And what if we only\nhave unlabeled data? It'd be nice to be able to\ndo few-shot meta-learning from unlabeled images or\nunlabeled text and so forth.",
    "start": "2539050",
    "end": "2545770"
  },
  {
    "text": " Yeah?",
    "start": "2545770",
    "end": "2551480"
  },
  {
    "text": "When you say [INAUDIBLE] even if\nyou didn't have the [INAUDIBLE]",
    "start": "2551480",
    "end": "2572921"
  },
  {
    "text": "you still know where\nthe images, like",
    "start": "2572921",
    "end": "2579412"
  },
  {
    "text": "where the geography\ncomes from in sort of making it task specific? ",
    "start": "2579413",
    "end": "2585910"
  },
  {
    "text": "Yeah, this is a great question. There is essentially two\naspects of defining tasks. One is grouping the\ndata into these tasks,",
    "start": "2585910",
    "end": "2594160"
  },
  {
    "text": "which could correspond\nto geographical metadata. Another is like, once\nyou have those groupings,",
    "start": "2594160",
    "end": "2601960"
  },
  {
    "text": "actually labeling\nthose groupings and having a labeled support\nset and a labeled query set.",
    "start": "2601960",
    "end": "2607810"
  },
  {
    "text": "Both of those are\nimportant aspects of it. In the next couple\nslides, I'll talk",
    "start": "2607810",
    "end": "2613510"
  },
  {
    "text": "about how we could\nactually have approaches that handle the case where\nyou don't know either of them. You just have unlabeled image\ndata or unlabeled text data.",
    "start": "2613510",
    "end": "2622845"
  },
  {
    "text": "But there are-- like, these\nsame sort of techniques could be used if you\nhave the groupings but don't have the labels\nwithin those groupings.",
    "start": "2622845",
    "end": "2628752"
  },
  {
    "text": " Cool. So a general recipe that we\ncould do for this sort of thing",
    "start": "2628752",
    "end": "2637339"
  },
  {
    "text": "is, we have an\nunlabeled data set. We want to propose tasks in some\nway from this unlabeled data",
    "start": "2637340",
    "end": "2643710"
  },
  {
    "text": "and then run meta-learning\non those tasks. Now of course, the big question\nis, how do we propose tasks",
    "start": "2643710",
    "end": "2650810"
  },
  {
    "text": "from unlabeled data? And essentially, the\ngoal of these methods is to try to automatically\nconstruct tasks",
    "start": "2650810",
    "end": "2657680"
  },
  {
    "text": "from unlabeled data in a\nway that tries to prepare us for solving some test\ntasks that we want",
    "start": "2657680",
    "end": "2664220"
  },
  {
    "text": "the model to be able to solve. So does anyone have\nthoughts on what",
    "start": "2664220",
    "end": "2669710"
  },
  {
    "text": "you might want these\nconstructed tasks to look like? This is fairly open-ended.",
    "start": "2669710",
    "end": "2675800"
  },
  {
    "start": "2675800",
    "end": "2682670"
  },
  {
    "text": "Yeah? [INAUDIBLE] some sort of--",
    "start": "2682670",
    "end": "2689348"
  },
  {
    "text": "[INAUDIBLE] identify\nwhich subportion or subset of the data [INAUDIBLE]\nmake tasks based on that? ",
    "start": "2689348",
    "end": "2705400"
  },
  {
    "text": "Yeah. So if you don't\nknow the groupings, you could try to use\nclustering to kind of group the data in some way. I guess we sort of saw that in\nthe land cover classification",
    "start": "2705400",
    "end": "2713550"
  },
  {
    "text": "example. Other thoughts? ",
    "start": "2713550",
    "end": "2721358"
  },
  {
    "text": "Is there a way that you could\ndetermine like-- maybe you have a method for proposing tasks. How would you understand if\nthat's a good method or not?",
    "start": "2721358",
    "end": "2728430"
  },
  {
    "start": "2728430",
    "end": "2735550"
  },
  {
    "text": "Yeah? You can-- if you have\nsome way to put tasks, you could try to sort of\nuse meta-learning algorithms",
    "start": "2735550",
    "end": "2741280"
  },
  {
    "text": "on these tasks. And then see if they then\nwork to do example tasks using",
    "start": "2741280",
    "end": "2748525"
  },
  {
    "text": "the same [INAUDIBLE]. Yeah, cool. So you could sample\na bunch of tasks",
    "start": "2748525",
    "end": "2754750"
  },
  {
    "text": "and see, if you run\nmeta-learning on those tasks, can it then learn new tasks\nfrom that same sampling scheme?",
    "start": "2754750",
    "end": "2760930"
  },
  {
    "text": "And what would that\ntell you if it can't? It would tell you that your\nsampling scheme isn't very good",
    "start": "2760930",
    "end": "2767920"
  },
  {
    "text": "or recruit tasks that are\ndependent upon the data. Yeah, yeah. So basically if it doesn't\ngeneralize to the tasks",
    "start": "2767920",
    "end": "2774010"
  },
  {
    "text": "that you sampled,\nthen that means that you're creating tasks in\na way that doesn't kind of-- for which the support\nset doesn't tell you",
    "start": "2774010",
    "end": "2780070"
  },
  {
    "text": "much about how to\nsolve the query set. ",
    "start": "2780070",
    "end": "2785795"
  },
  {
    "text": "Cool, so essentially\nthere's kind of two things that I had in\nmind-- is first, we",
    "start": "2785795",
    "end": "2791970"
  },
  {
    "text": "want the sample\ntask to be diverse. Because ultimately, if\nyou have some test tasks",
    "start": "2791970",
    "end": "2797164"
  },
  {
    "text": "that you want to\nbe able to solve with a small amount\nof labels, then if you have a\ndiverse set of tasks,",
    "start": "2797165",
    "end": "2802590"
  },
  {
    "text": "then it's more likely\nto cover those tasks. And then the second\nthing that you want is, you want them\nto be structured.",
    "start": "2802590",
    "end": "2809050"
  },
  {
    "text": "And in particular, if they\ndon't have any shared structure to them, then you\nmay not actually",
    "start": "2809050",
    "end": "2815369"
  },
  {
    "text": "be able to get any information\nfrom the support set. And then it wouldn't be able\nto perform few-shot learning",
    "start": "2815370",
    "end": "2821610"
  },
  {
    "text": "and then we'd see the same\nissue that was brought up.  Cool, so we ultimately\nneed-- we want both of these.",
    "start": "2821610",
    "end": "2828610"
  },
  {
    "text": "Because if we have\ncompletely random tasks that are super diverse but they're\nnot structured enough, and likewise the opposite\nwould be bad as well.",
    "start": "2828610",
    "end": "2836960"
  },
  {
    "text": "So next we'll look at a\nmethod for constructing tasks from unlabeled image data,\nand also task construction",
    "start": "2836960",
    "end": "2842890"
  },
  {
    "text": "from unlabeled text data.  So the first approach\nthat we'll look at",
    "start": "2842890",
    "end": "2851680"
  },
  {
    "text": "is actually heavily based on\nclustering, which is suggested. So we'll get some\nunsupervised embedding space.",
    "start": "2851680",
    "end": "2859790"
  },
  {
    "text": "And this will essentially\nallow us to map from images into some lower dimensional\nembedding space.",
    "start": "2859790",
    "end": "2866410"
  },
  {
    "text": "Then we can cluster the images\nwithin this embedding space",
    "start": "2866410",
    "end": "2874910"
  },
  {
    "text": "to get different tasks that\ncorrespond to discriminating different clusters. And so for example, we\ncan sample some tasks",
    "start": "2874910",
    "end": "2881980"
  },
  {
    "text": "or cluster these images\nin the embedding space to get different\nclusters and then",
    "start": "2881980",
    "end": "2887230"
  },
  {
    "text": "different tasks will\ncorrespond to discriminating these different clusters.",
    "start": "2887230",
    "end": "2892660"
  },
  {
    "text": "And so for example,\nwhat we could do is, when we want to\nsample a new task, we'll sample two clusters. These two clusters\nwill correspond",
    "start": "2892660",
    "end": "2898900"
  },
  {
    "text": "to two classes or\ndifferent classes within that classification task.",
    "start": "2898900",
    "end": "2903928"
  },
  {
    "text": "The number of clusters\nin each sample will correspond to\nthe number of classes. And then we can sample new\nimages within those clusters",
    "start": "2903928",
    "end": "2910180"
  },
  {
    "text": "to get a query set.  And then we can do this\nlikewise for the purple cluster",
    "start": "2910180",
    "end": "2917260"
  },
  {
    "text": "and the green cluster\nto get another task. And this would give us a two-way\none-shot classification task.",
    "start": "2917260",
    "end": "2925150"
  },
  {
    "text": "Yeah? In this unsupervised\nsetting, I guess in general if you have a set of\ndifferent images [INAUDIBLE]",
    "start": "2925150",
    "end": "2935224"
  },
  {
    "text": "even if [INAUDIBLE]\nlimited in how much we",
    "start": "2935224",
    "end": "2945850"
  },
  {
    "text": "can express what the task is. Because initially we have\ndefined the task [INAUDIBLE] loss function and\nI don't see how",
    "start": "2945850",
    "end": "2955552"
  },
  {
    "text": "we can like cluster [INAUDIBLE]\nundefined [INAUDIBLE]",
    "start": "2955552",
    "end": "2965530"
  },
  {
    "text": "Yeah, so this is going to be\nspecific to classification tasks. It won't necessarily be\nspecific to certain data sets",
    "start": "2965530",
    "end": "2971260"
  },
  {
    "text": "but it will be a way to get\ndifferent classification tasks. I haven't thought\ntoo much about how",
    "start": "2971260",
    "end": "2977080"
  },
  {
    "text": "you would get regression tasks,\nat least in this circumstance.",
    "start": "2977080",
    "end": "2983950"
  },
  {
    "text": "Yeah, so I'm not\nsure exactly how you would get regression tasks. Yeah? [INAUDIBLE] Presumably the more\nclasses you have, the harder--",
    "start": "2983950",
    "end": "2995440"
  },
  {
    "text": "[INAUDIBLE] Yeah. So the question is,\nhow do you determine the number of clusters?",
    "start": "2995440",
    "end": "3000960"
  },
  {
    "text": "And generally if you know\nanything about your meta test",
    "start": "3000960",
    "end": "3006161"
  },
  {
    "text": "task-- like maybe\nyou know that you have a few examples of a\nbinary classification task. Then what you can do\nis you can make a bunch",
    "start": "3006162",
    "end": "3011970"
  },
  {
    "text": "of binary classification tasks. If you don't know exactly\nwhat it's going to be, then what you can do is just\nset it to be sufficiently large",
    "start": "3011970",
    "end": "3018480"
  },
  {
    "text": "such that-- you're training it on\nclassification tasks that are up to like 50-way, for example.",
    "start": "3018480",
    "end": "3023520"
  },
  {
    "text": "And then your\nmeta-learner should be able to generalize to\ntasks that are up to 50-way,",
    "start": "3023520",
    "end": "3032335"
  },
  {
    "text": "for example.  Yeah? So can you shift the problem\nfrom learning the meta-learning",
    "start": "3032335",
    "end": "3042660"
  },
  {
    "text": "to clustering. Because it seems\nlike the upper bound is like the supervised\nmeta learning case",
    "start": "3042660",
    "end": "3050530"
  },
  {
    "text": "where you do have exactly what\nthe task condition distribution or what the different tasks are. But in the clustering\ncase, like,",
    "start": "3050530",
    "end": "3057000"
  },
  {
    "text": "you could cluster incorrectly\nand then it would mess up your task distribution.",
    "start": "3057000",
    "end": "3063750"
  },
  {
    "text": "Yeah, so I think that-- I guess the question\nis, is this kind of shifting the\nresponsibility from the kind",
    "start": "3063750",
    "end": "3073000"
  },
  {
    "text": "of meta-learning process to\nthe task construction process? And it certainly is\nshifting responsibility.",
    "start": "3073000",
    "end": "3080190"
  },
  {
    "text": "And the design of these tasks-- like, you do want it to kind\nof reflect the kinds of tasks--",
    "start": "3080190",
    "end": "3086023"
  },
  {
    "text": "like ultimately, you want it\nto reflect the kinds of tasks that you'll see\nat meta test time. And you want there to\nbe as close of overlap",
    "start": "3086023",
    "end": "3092040"
  },
  {
    "text": "as you can get. And so in many ways, these kinds\nof approaches may end up being",
    "start": "3092040",
    "end": "3098910"
  },
  {
    "text": "somewhat domain-specific-- in the sense that you\nwant to construct tasks",
    "start": "3098910",
    "end": "3105540"
  },
  {
    "text": "in an unsupervised way that do\ncover the tasks in that domain that you care about.",
    "start": "3105540",
    "end": "3111060"
  },
  {
    "text": "Does that answer your question? Yeah? Isn't [INAUDIBLE] ",
    "start": "3111060",
    "end": "3122560"
  },
  {
    "text": "Yeah. So the question is,\ndoesn't this depend on how good your initial\nembedding space is? And it certainly does.",
    "start": "3122560",
    "end": "3130020"
  },
  {
    "text": "What we're actually going\nto use in this case is-- there's an approach called\nDeepCluster that actually",
    "start": "3130020",
    "end": "3135100"
  },
  {
    "text": "kind of integrates\nrepresentation learning and clustering, so that you're\nactually using the clustering",
    "start": "3135100",
    "end": "3140760"
  },
  {
    "text": "to get the representation. But if you have a better\nrepresentation learning method, then you can also use that.",
    "start": "3140760",
    "end": "3148079"
  },
  {
    "text": "And really the\nnatural baseline here will be to compare the result\nof meta-learning on these tasks",
    "start": "3148080",
    "end": "3153480"
  },
  {
    "text": "to doing some sort of thing,\nlike linear classifier on top of that embedding space.",
    "start": "3153480",
    "end": "3159193"
  },
  {
    "text": "And your goal is to\nbe able to do better than just a linear classifier\non that embedding space. ",
    "start": "3159193",
    "end": "3166796"
  },
  {
    "text": "Cool, so here's\none example of how you can construct\ntasks with this sort of clustering approach.",
    "start": "3166796",
    "end": "3172660"
  },
  {
    "text": "And we wanted to get essentially\na representation that's particularly suitable\nfor few-shot learning of downstream tasks compared\nto this original representation",
    "start": "3172660",
    "end": "3180350"
  },
  {
    "text": "space.  In terms of some of the\ndetails, you can use it like--",
    "start": "3180350",
    "end": "3186850"
  },
  {
    "text": "I guess this paper was\nfrom a couple of years ago. And so these are a little\nbit outdated at this point. But you can use representation\nlearning methods",
    "start": "3186850",
    "end": "3193660"
  },
  {
    "text": "like this BIGAN or\nsomething like DeepCluster, which does integrate clustering\nand representation learning.",
    "start": "3193660",
    "end": "3199255"
  },
  {
    "text": " Method is referred\nto as Clustering",
    "start": "3199255",
    "end": "3204550"
  },
  {
    "text": "to Automatically Construct Tasks\nfor Unsupervised Meta-Learning, or CACTUs. It's a bit of a forced acronym.",
    "start": "3204550",
    "end": "3211510"
  },
  {
    "text": "And then we used MAML and\nPrototypical Networks. And if you're curious what\na forced acronym with MAML",
    "start": "3211510",
    "end": "3220150"
  },
  {
    "text": "looks like or a CACTUS MAML\nyou get something like this. [LAUGHTER] And if you look at performance\non miniImageNet 5-way 5-shot",
    "start": "3220150",
    "end": "3228100"
  },
  {
    "text": "classification-- first, our Oracle\nmethod is to use MAML",
    "start": "3228100",
    "end": "3234070"
  },
  {
    "text": "with all of the\nlabels on the data. ",
    "start": "3234070",
    "end": "3240349"
  },
  {
    "text": "Then we could take the\nrepresentation from BIGAN and do K nearest neighbors\non that representation. We get around a 31% success\nrate on these 5-shot tasks.",
    "start": "3240350",
    "end": "3249580"
  },
  {
    "text": "We could also do\nlogistic regression on top of that representation. We do a little bit better. If you want you could-- we tried\nregularizing it and doing it",
    "start": "3249580",
    "end": "3256750"
  },
  {
    "text": "MLP. That actually did worse than\njust logistic regression. We also tried some sort\nof cluster matching.",
    "start": "3256750",
    "end": "3261850"
  },
  {
    "text": " And then when we actually\nran meta-learning",
    "start": "3261850",
    "end": "3267928"
  },
  {
    "text": "on top of the tasks\nthat were derived from this representation,\nwe see a performance of 51%, which is kind of\nsubstantially better than 33%",
    "start": "3267928",
    "end": "3275770"
  },
  {
    "text": "because you're\nessentially preparing the representation\nfor the ability to do few-shot learning.",
    "start": "3275770",
    "end": "3281200"
  },
  {
    "text": " And then also if you use the\nDeepCluster representation,",
    "start": "3281200",
    "end": "3287170"
  },
  {
    "text": "this shows you the\ndependence of this method on the representation. It does actually do better with\na DeepCluster representation",
    "start": "3287170",
    "end": "3293120"
  },
  {
    "text": "than the BIGAN representation. So these are the results\nfor kind of one few-shot",
    "start": "3293120",
    "end": "3298810"
  },
  {
    "text": "classification task. I won't show all the\nothers but it's really the same kind of trend and\nstory for different embedding",
    "start": "3298810",
    "end": "3304990"
  },
  {
    "text": "methods, for\ndifferent data sets. And these different\ndata sets also includes things like attribute\nprediction and CelebA.",
    "start": "3304990",
    "end": "3313990"
  },
  {
    "text": "Prototypical Networks\nunderperforms in some cases, but we generally saw the\nsame story with both MAML and Prototypical Networks.",
    "start": "3313990",
    "end": "3320950"
  },
  {
    "text": "And then we also see\nthat this works well if you have larger data sets. I think that we looked at\ndata sets of up to 100-shot",
    "start": "3320950",
    "end": "3328272"
  },
  {
    "text": "or something like that.  Yeah? Does this require some\namount of like [INAUDIBLE] I",
    "start": "3328272",
    "end": "3334945"
  },
  {
    "text": "don't know if you're going\nto doing it [INAUDIBLE] classification [INAUDIBLE]",
    "start": "3334945",
    "end": "3340230"
  },
  {
    "start": "3340230",
    "end": "3348660"
  },
  {
    "text": "Yeah, so there is some domain\nknowledge that's inherently",
    "start": "3348660",
    "end": "3354329"
  },
  {
    "text": "a part of the method based\non the number of clusters that you choose, and ultimately\nwhat representation you use",
    "start": "3354330",
    "end": "3360610"
  },
  {
    "text": "and so forth. And actually what I was\ngoing to talk about next, is can we actually even use\neven more domain knowledge",
    "start": "3360610",
    "end": "3367380"
  },
  {
    "text": "when constructing these tasks. And for example, like\nthere are scenarios",
    "start": "3367380",
    "end": "3372450"
  },
  {
    "text": "in image classification\nwhere we know that there are certain data\naugmentations that will still",
    "start": "3372450",
    "end": "3377850"
  },
  {
    "text": "preserve the label. And we can use this domain\nknowledge to actually construct even better tasks. So for example, we\nknow that things",
    "start": "3377850",
    "end": "3384870"
  },
  {
    "text": "like dropping out some pixels\nor translating the image or reflecting the image,\nthose sorts of things generally don't change\nthe image class.",
    "start": "3384870",
    "end": "3391080"
  },
  {
    "text": "And if you know\nthat you're going to be doing image\nclassification problems, you could try to integrate\nthis domain knowledge into the tasks.",
    "start": "3391080",
    "end": "3397823"
  },
  {
    "text": "And so you can get\ntransformations that look something like\nthis or something like this. And then when you do\ntask construction,",
    "start": "3397823",
    "end": "3405270"
  },
  {
    "text": "instead of doing the clustering\napproach that I mentioned before, you can\nrandomly sample n images",
    "start": "3405270",
    "end": "3411930"
  },
  {
    "text": "and assign them\nlabels 1 through n. So for example, you could\nsample these three images,",
    "start": "3411930",
    "end": "3417210"
  },
  {
    "text": "assign them labels 1, 2, 3. This will be your support set.",
    "start": "3417210",
    "end": "3423780"
  },
  {
    "text": "And then for each data\npoint in this support set, you can augment the example\nusing your domain knowledge",
    "start": "3423780",
    "end": "3430410"
  },
  {
    "text": "to construct other examples\nof the same image class, and this can be your query set.",
    "start": "3430410",
    "end": "3437880"
  },
  {
    "text": "So this is stored in D train,\nthis is stored in D test. And this is\nessentially another way",
    "start": "3437880",
    "end": "3443400"
  },
  {
    "text": "to construct tasks except with\ncompletely unlabeled data-- except now we're going to\nbe using domain knowledge",
    "start": "3443400",
    "end": "3450330"
  },
  {
    "text": "to potentially construct\nbetter tasks because we can essentially get lots of\ndata of support query sets",
    "start": "3450330",
    "end": "3460470"
  },
  {
    "text": "with these different\ntransformations. ",
    "start": "3460470",
    "end": "3466819"
  },
  {
    "text": "Cool. How do you actually go about\naugmenting in practice? For Omniglot, they\nuse translation",
    "start": "3466820",
    "end": "3472240"
  },
  {
    "text": "and random pixel dropout-- and from any image\nthat they used what's referred to as\nauto-augment-- which",
    "start": "3472240",
    "end": "3477970"
  },
  {
    "text": "does a combination of\ntranslation and rotation and shearing. And the result is that\nthis sort of approach",
    "start": "3477970",
    "end": "3487750"
  },
  {
    "text": "did really super\nwell on Omniglot. ",
    "start": "3487750",
    "end": "3494829"
  },
  {
    "text": "It's able to bridge\na lot of the gap between unsupervised methods\nand fully supervised methods.",
    "start": "3494830",
    "end": "3501220"
  },
  {
    "text": "And for example, the\n5-way 5-shot case, it's getting a 95%\naccuracy despite only",
    "start": "3501220",
    "end": "3507490"
  },
  {
    "text": "having five examples per class. And this is where we have\npretty good domain knowledge",
    "start": "3507490",
    "end": "3513340"
  },
  {
    "text": "because the images\nare relatively simple. And then on miniImageNet we\nsee that it kind of slightly",
    "start": "3513340",
    "end": "3519700"
  },
  {
    "text": "underperforms the\nclustering-based approach, probably just because clustering\nis a pretty good domain",
    "start": "3519700",
    "end": "3526180"
  },
  {
    "text": "knowledge in terms of trying\nto do some sort of image classification. ",
    "start": "3526180",
    "end": "3534193"
  },
  {
    "text": "OK, so that's the image case. What about if we\nhave unlabeled text?",
    "start": "3534193",
    "end": "3540170"
  },
  {
    "text": "One option is to\njust formulate it as a language modeling problem. We've seen this with\nthings like GPT-3, T5",
    "start": "3540170",
    "end": "3546260"
  },
  {
    "text": "and so forth, where\nessentially D train is kind of a sequence\nof characters,",
    "start": "3546260",
    "end": "3551300"
  },
  {
    "text": "D test is the following\nsequence of characters. And you can construct\ntasks by trying to predict what will come next,\nand formulating the training",
    "start": "3551300",
    "end": "3558290"
  },
  {
    "text": "data as text. And so we saw this where you\ncould kind of construct tasks",
    "start": "3558290",
    "end": "3564859"
  },
  {
    "text": "like math problems as text and\nspelling correction problems as text and so forth.",
    "start": "3564860",
    "end": "3570109"
  },
  {
    "text": " We might not use this\noption in all cases.",
    "start": "3570110",
    "end": "3575790"
  },
  {
    "text": "So this approach is\nharder to combine with these\noptimization-based approaches because you don't have a clear\ninput in labels, for example.",
    "start": "3575790",
    "end": "3589350"
  },
  {
    "text": "It's also hard to apply this\nto classification tasks. I mean, you could formulate\na classification task",
    "start": "3589350",
    "end": "3596150"
  },
  {
    "text": "as a prediction\ntask, where you need to predict a token of\nlike 0 or 1 for example. But if you actually tell the\nmodel that the output is only",
    "start": "3596150",
    "end": "3603260"
  },
  {
    "text": "going to be 0 or 1 and it\nhas to pick between them, then formulating it as\na classification problem",
    "start": "3603260",
    "end": "3609560"
  },
  {
    "text": "is easier than formulating it as\na language generation problem. And so another option\nthat has been explored",
    "start": "3609560",
    "end": "3616820"
  },
  {
    "text": "is to try to construct\ntasks by masking out words and then the task is to\nclassify the masked word.",
    "start": "3616820",
    "end": "3625760"
  },
  {
    "text": "And so what this\nlooks like is, you sample a subset of unique\nwords, assign them an ID.",
    "start": "3625760",
    "end": "3633300"
  },
  {
    "text": "So for example, you might\npick the words, democratic and capital, and assign\nthem the ID of 1 and 2.",
    "start": "3633300",
    "end": "3641120"
  },
  {
    "text": "Then sample a bunch of\nsentences with those words and mask the word out.",
    "start": "3641120",
    "end": "3648330"
  },
  {
    "text": "And then construct a\nsupport set and a query set with the masked sentences, and\nthen the corresponding word IDs",
    "start": "3648330",
    "end": "3653750"
  },
  {
    "text": "are the labels. And so as an example of\nwhat a training data set would look like, is you would\ngive it a sentence like this.",
    "start": "3653750",
    "end": "3660888"
  },
  {
    "text": "This has a class label of 1\nbecause the masked word here is democratic.",
    "start": "3660888",
    "end": "3666710"
  },
  {
    "text": "This would also have\na masked label of 1. This one would have a\nlabel of 2 and this one",
    "start": "3666710",
    "end": "3672370"
  },
  {
    "text": "would have a label of 2. And so this is your support\nset with kind of inputs x and labels y.",
    "start": "3672370",
    "end": "3678530"
  },
  {
    "text": " And then in terms of\nyour test set, then",
    "start": "3678530",
    "end": "3684180"
  },
  {
    "text": "you get a new query also\nwith a masked-out word. And then you need\nto predict 1 or 2.",
    "start": "3684180",
    "end": "3690255"
  },
  {
    "text": " So you can do this with this\ncompletely unlabeled text",
    "start": "3690255",
    "end": "3697170"
  },
  {
    "text": "and get a bunch of\nclassification problems out. ",
    "start": "3697170",
    "end": "3703200"
  },
  {
    "text": "Any questions on how this works? Yeah? How much data [INAUDIBLE]",
    "start": "3703200",
    "end": "3713310"
  },
  {
    "text": "So how much support data or\nhow much meta-training data? How much meta-training data.",
    "start": "3713310",
    "end": "3718610"
  },
  {
    "text": " That's a good question.",
    "start": "3718610",
    "end": "3723920"
  },
  {
    "text": "So the question is, how\nmuch meta-training data do you need to do this? Well, I guess it\nprobably depends",
    "start": "3723920",
    "end": "3732010"
  },
  {
    "text": "on how complex your text is. So if you have very\nbroad text, like you",
    "start": "3732010",
    "end": "3741579"
  },
  {
    "text": "to understand the kind of\ncontext and so forth as well. I mean, it's also\ngoing to depend",
    "start": "3741580",
    "end": "3746650"
  },
  {
    "text": "on what you choose for like\nN-way and K-shot as well. So if you see a lot\nof examples per class,",
    "start": "3746650",
    "end": "3755170"
  },
  {
    "text": "that makes it easier. If you have a lot of\nclasses, then you actually need to understand\nlanguage a lot",
    "start": "3755170",
    "end": "3760269"
  },
  {
    "text": "more to understand what\nshould go in that blank. Rather than-- like you might be\nable to do some simple pattern",
    "start": "3760270",
    "end": "3765430"
  },
  {
    "text": "matching if you have\njust two classes. So it's probably\ngoing to depend a lot on the details of the data set.",
    "start": "3765430",
    "end": "3771910"
  },
  {
    "start": "3771910",
    "end": "3777399"
  },
  {
    "text": "Cool. So here are a bunch of numbers. The results on the\nleft are methods",
    "start": "3777399",
    "end": "3784290"
  },
  {
    "text": "that do this\nentirely unsupervised pre-training approach, whereas\nthe methods on the right are doing some sort of\nsupervised or semi-supervised",
    "start": "3784290",
    "end": "3790860"
  },
  {
    "text": "pre-training. And so in particular,\nthe approach",
    "start": "3790860",
    "end": "3795990"
  },
  {
    "text": "that we mentioned before\nis in this second column. And this is in\ncomparison to BERT.",
    "start": "3795990",
    "end": "3801387"
  },
  {
    "text": "And then everything on\nthe right is actually combining the generated\ntasks with supervised tasks,",
    "start": "3801387",
    "end": "3806922"
  },
  {
    "text": "and you're sort of doing like\nsemi-supervised meta-learning where you have some\nsupervised tasks and some unsupervised tasks.",
    "start": "3806922",
    "end": "3813069"
  },
  {
    "text": "And then there's likewise other\nsemi-supervised approaches that are also being compared to.",
    "start": "3813070",
    "end": "3819030"
  },
  {
    "text": "And in particular, BERT is\na standard self-supervised learning method. The second column is the\nproposed unsupervised",
    "start": "3819030",
    "end": "3825375"
  },
  {
    "text": "meta-learning approach. MT-BERT is doing multitask\nlearning and fine-tuning",
    "start": "3825375",
    "end": "3831119"
  },
  {
    "text": "on supervised tasks. LEOPARD is an optimization\nbased meta-learner",
    "start": "3831120",
    "end": "3837750"
  },
  {
    "text": "that's only doing it on\nthe supervised tasks again. And then this hybrid method\nis doing meta-learning",
    "start": "3837750",
    "end": "3843420"
  },
  {
    "text": "on the proposed tasks as well\nas on the supervised tasks. And so the comparison\nbetween LEOPARD and SMLMT",
    "start": "3843420",
    "end": "3851310"
  },
  {
    "text": "is showing you how much\nthose unsupervised tasks are helping in\ncomparison to only using the supervised tasks.",
    "start": "3851310",
    "end": "3856905"
  },
  {
    "text": " Cool. And then in terms of the\nresults, we see that the--",
    "start": "3856905",
    "end": "3862740"
  },
  {
    "text": "I mean first in terms\nof the last two columns, we see that there is a pretty\nsubstantial or significant",
    "start": "3862740",
    "end": "3868020"
  },
  {
    "text": "improvement between using\nthe unsupervised tasks in addition to only using\nthe supervised tasks.",
    "start": "3868020",
    "end": "3873270"
  },
  {
    "text": "And then we also see\nimprovements over BERT with fine-tuning. Yeah?",
    "start": "3873270",
    "end": "3878655"
  },
  {
    "text": "Why is the unsupervised\n[INAUDIBLE]",
    "start": "3878655",
    "end": "3889620"
  },
  {
    "text": "Yeah, I'm not sure exactly\nwhy that's the case. They probably have more\ndetails in the paper about it.",
    "start": "3889620",
    "end": "3896460"
  },
  {
    "text": "I can't remember exactly what\nthat data set corresponds to.",
    "start": "3896460",
    "end": "3903330"
  },
  {
    "text": "It could be that\nmaybe when they set the ratio of the\nsupervised tasks and the unsupervised\ntasks, maybe they",
    "start": "3903330",
    "end": "3908970"
  },
  {
    "text": "set the ratio of\nsupervised tasks too high and maybe they're overfitting\nto those supervised tasks.",
    "start": "3908970",
    "end": "3914670"
  },
  {
    "text": "But yeah, I'd have to take\na look at the paper for-- and there is more results and\nanalysis in the paper as well.",
    "start": "3914670",
    "end": "3920400"
  },
  {
    "start": "3920400",
    "end": "3925500"
  },
  {
    "text": "Cool. So that's an overview of\nunsupervised meta-learning",
    "start": "3925500",
    "end": "3931670"
  },
  {
    "text": "and how we can try\nto construct tasks from unlabeled image data and\nunlabeled text data as well.",
    "start": "3931670",
    "end": "3938105"
  },
  {
    "text": "And we can also combine\nthose unlabeled tasks with supervised tasks as well,\nas a form of task augmentation.",
    "start": "3938105",
    "end": "3943220"
  },
  {
    "text": " We have a bit more time,\nso I'll also briefly talk about one other\nunsupervised meta-learning",
    "start": "3943220",
    "end": "3950300"
  },
  {
    "text": "scenario, which is, what\nif we have a time series? And the time series\nis actually labeled.",
    "start": "3950300",
    "end": "3958910"
  },
  {
    "text": "So for example, maybe we\nwant to predict energy demand or we want to predict the\ndynamics of a robot or a car",
    "start": "3958910",
    "end": "3965240"
  },
  {
    "text": "or transportation usage, or\nwe want to make some money and we want to predict\nthe stock market-- video analytics or\nreinforcement learning agent.",
    "start": "3965240",
    "end": "3974840"
  },
  {
    "text": "Oftentimes we do actually\nhave lots of streams of data but they're fully unsegmented,\nin the sense that they aren't",
    "start": "3974840",
    "end": "3981590"
  },
  {
    "text": "really broken into tasks. And so this is\nactually a scenario where we have the label data\nbut we don't have the groupings",
    "start": "3981590",
    "end": "3988100"
  },
  {
    "text": "into tasks. And so we want to be\nable to somehow get the groupings of tasks\nand do meta-learning",
    "start": "3988100",
    "end": "3994310"
  },
  {
    "text": "in a way that allows us to very\nquickly predict for a new time series with a small\namount of data.",
    "start": "3994310",
    "end": "4000610"
  },
  {
    "text": " Cool, so it's unsegmented. We do have temporal structure.",
    "start": "4000610",
    "end": "4006610"
  },
  {
    "text": "Like in many cases, there are\nkind of discrete points in time where something changes.",
    "start": "4006610",
    "end": "4012309"
  },
  {
    "text": "Like maybe Facebook stock may\nhave had a discrete change earlier this week.",
    "start": "4012310",
    "end": "4020150"
  },
  {
    "text": "So can we segment the\ntime series into tasks and then meta-learn\nacross those tasks?",
    "start": "4020150",
    "end": "4028160"
  },
  {
    "text": "Another question\nbecomes how to segment. There is a class of approaches\ncalled change point detection",
    "start": "4028160",
    "end": "4034345"
  },
  {
    "text": "methods, that try to\nessentially detect where those discrete changes happen. And in particular, we\nassume that the tasks",
    "start": "4034345",
    "end": "4044020"
  },
  {
    "text": "will switch with\nsome probability at every single time set.",
    "start": "4044020",
    "end": "4049029"
  },
  {
    "text": "And so you could view it\naccording to this visual, where each of the dashed\nlines are essentially",
    "start": "4049030",
    "end": "4056230"
  },
  {
    "text": "like a change point in the task,\nand the function is essentially kind of like piecewise linear. ",
    "start": "4056230",
    "end": "4063280"
  },
  {
    "text": "And then once we\nmake this assumption, then we should maintain a\nbelief over the task duration",
    "start": "4063280",
    "end": "4069760"
  },
  {
    "text": "or the run length and estimate\nsome distribution over what",
    "start": "4069760",
    "end": "4076330"
  },
  {
    "text": "we think the duration is. And so in particular for\nthis example, we would--",
    "start": "4076330",
    "end": "4084220"
  },
  {
    "text": "the run length or\nthe task duration for each of these\ntasks is increasing by 1 at each time step until\nyou hit that change point,",
    "start": "4084220",
    "end": "4090760"
  },
  {
    "text": "and then it goes back to 0\nand then increases again. So in this work, they\nshow that you can actually",
    "start": "4090760",
    "end": "4100270"
  },
  {
    "text": "update the belief\nrecursively based off of the model performance,\nand essentially figure out--",
    "start": "4100270",
    "end": "4109479"
  },
  {
    "text": "update your belief over what the\ntask duration is and figure out whether or not there is a\nchange point at the current time",
    "start": "4109479",
    "end": "4115810"
  },
  {
    "text": "step or not. Now one of the cool things about\nthis approach is that this-- because it's\nactually representing",
    "start": "4115810",
    "end": "4122200"
  },
  {
    "text": "these probabilities\nover the run length, this procedure is actually\nfully differentiable.",
    "start": "4122200",
    "end": "4130128"
  },
  {
    "text": "And so what you can do\nis, you can actually back-propagate through\nthe belief update",
    "start": "4130128",
    "end": "4137979"
  },
  {
    "text": "into a model that's\nbeing meta-trained to perform few-shot\nclassification on this time series.",
    "start": "4137979",
    "end": "4143275"
  },
  {
    "text": " And so kind of the\nresult is, you can do--",
    "start": "4143275",
    "end": "4150062"
  },
  {
    "text": "in the meta-training\nphase, you're given some unsegmented time\nseries of offline data,",
    "start": "4150062",
    "end": "4155963"
  },
  {
    "text": "and then at test\ntime you're given kind of streaming online data. And what you'll do is,\nyou will essentially",
    "start": "4155963",
    "end": "4163719"
  },
  {
    "text": "do meta-learning and do kind of\njoint change point prediction and few-shot meta-learning over\nthese segments in your time",
    "start": "4163720",
    "end": "4170799"
  },
  {
    "text": "series. And then at meta\ntest time, you'll use your few-shot learner\nto predict forward, and also",
    "start": "4170800",
    "end": "4177317"
  },
  {
    "text": "use what you learned\nbefore to also predict where the change points are. ",
    "start": "4177317",
    "end": "4182342"
  },
  {
    "text": "And so you can do something\nlike sinusoid regression where you have\ndiscrete shifts in the actual sinusoid function.",
    "start": "4182343",
    "end": "4188439"
  },
  {
    "text": "And you get a cool\nvisualization like this where the red is predicting the\nground truth sinusoid function",
    "start": "4188439",
    "end": "4195850"
  },
  {
    "text": "at the given point in\ntime, and then the blue is showing the model's\npredictions for that sinusoid.",
    "start": "4195850",
    "end": "4203230"
  },
  {
    "text": "And you can see that\nvery quickly, when it gets a new sinusoid, it\nrealizes that the task has",
    "start": "4203230",
    "end": "4210190"
  },
  {
    "text": "changed and then it's able\nto very quickly identify what the correct curve is.",
    "start": "4210190",
    "end": "4216743"
  },
  {
    "text": "And then at the same\ntime, the algorithm is also maintaining its current\nbelief over the run length and whether or not the\ntask has actually changed.",
    "start": "4216743",
    "end": "4223750"
  },
  {
    "text": " Cool. And then you can also\nmake a streaming version",
    "start": "4223750",
    "end": "4230557"
  },
  {
    "text": "of miniImageNet and also\nallow it to identify when the task has changed.",
    "start": "4230557",
    "end": "4236370"
  },
  {
    "text": "So the blue curve is\nshowing the test accuracy in the streaming setting,\nwhereas the gray top curve",
    "start": "4236370",
    "end": "4243180"
  },
  {
    "text": "is showing some Oracle. So you do get a performance hit\nbecause the task is actually changing at test time,\nbut you are actually",
    "start": "4243180",
    "end": "4250830"
  },
  {
    "text": "able to do still fairly\nwell in the setting. And then the window\nbased approaches are just",
    "start": "4250830",
    "end": "4256350"
  },
  {
    "text": "treating different tasks as\ndifferent windows in time. Yeah? [INAUDIBLE]",
    "start": "4256350",
    "end": "4263830"
  },
  {
    "text": "Cool, yeah. So the x-axis is\nthe rate at which the change points are coming. And so further to\nthe right means",
    "start": "4263830",
    "end": "4270449"
  },
  {
    "text": "that you have change points more\nfrequently and it gets harder, understandably. And then to the left means\nthat the change points",
    "start": "4270450",
    "end": "4276480"
  },
  {
    "text": "are happening less frequently. So the task is changing\nless frequently. And is the best accuracy--",
    "start": "4276480",
    "end": "4281498"
  },
  {
    "text": "if you just get an\naverage over time, like, maybe you can get some\npretty good times for that. ",
    "start": "4281498",
    "end": "4287610"
  },
  {
    "text": "Yes, averaged over all time. Yeah. ",
    "start": "4287610",
    "end": "4295640"
  },
  {
    "text": "Cool. So that's it for today. To recap, we talked\nabout task construction,",
    "start": "4295640",
    "end": "4302239"
  },
  {
    "text": "we talked about how memorization\nissues can come up if you construct your tasks in--",
    "start": "4302240",
    "end": "4308240"
  },
  {
    "text": "or if you're in scenarios\nwhere your tasks are not mutually exclusive.",
    "start": "4308240",
    "end": "4313429"
  },
  {
    "text": "And we talked about\none solution that seems to work well for\nclassification tasks at least, but I don't think it's\nnecessarily the final solution.",
    "start": "4313430",
    "end": "4321178"
  },
  {
    "text": "We also talked about\nhow we can potentially try to construct tasks in\nan unsupervised fashion, either from unlabeled text\ndata and unlabeled image data",
    "start": "4321178",
    "end": "4328880"
  },
  {
    "text": "or from time series\nthat isn't segmented.",
    "start": "4328880",
    "end": "4333929"
  },
  {
    "text": "Cool. And then next time\non Monday next week, we're going to have our last\nmeta-learning supervised",
    "start": "4333930",
    "end": "4339980"
  },
  {
    "text": "meta-learning lecture, where\nwe'll talk about Bayesian meta-learning and how you\ncan cast meta-learning into the framework of\nBayesian graphical models.",
    "start": "4339980",
    "end": "4347960"
  },
  {
    "text": "If you're not\nfamiliar with tools from variational\ninference, I'd encourage you to go to the tutorial\nat 4:00 PM tomorrow.",
    "start": "4347960",
    "end": "4355700"
  },
  {
    "text": "Yeah, and then I'll\nsee everyone next week. ",
    "start": "4355700",
    "end": "4363000"
  }
]