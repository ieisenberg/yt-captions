[
  {
    "start": "0",
    "end": "67000"
  },
  {
    "text": "thanks for having me it's great to see so many people here so today i'm going to be talking about",
    "start": "10800",
    "end": "16800"
  },
  {
    "text": "uh two related topics forecasting and aligning ai uh so just for",
    "start": "16800",
    "end": "22480"
  },
  {
    "text": "context i guess uh my kind of overall research goal is the following so",
    "start": "22480",
    "end": "28080"
  },
  {
    "text": "i think it's relatively likely that ai is at some point going to have a transformative effect on society and i",
    "start": "28080",
    "end": "33440"
  },
  {
    "text": "want to kind of you know do the research that will help us make that go well and so uh",
    "start": "33440",
    "end": "39200"
  },
  {
    "text": "my research kind of splits into three categories related to this one is alignment or ensuring that",
    "start": "39200",
    "end": "45920"
  },
  {
    "text": "ai systems act reliably in humanities interest another is empowerment so helping humans",
    "start": "45920",
    "end": "50960"
  },
  {
    "text": "make better decisions and the third is forecasting understanding and predicting uh what will happen as we continue to",
    "start": "50960",
    "end": "57360"
  },
  {
    "text": "make that continue to make advances in ml and so today i guess i'm not really going to talk so much about the second",
    "start": "57360",
    "end": "63359"
  },
  {
    "text": "one but i'll talk about uh the first and third topics so uh in this talk i guess first i'll talk",
    "start": "63359",
    "end": "70159"
  },
  {
    "start": "67000",
    "end": "173000"
  },
  {
    "text": "about kind of ways of testing i guess ideas around alignment uh with various",
    "start": "70159",
    "end": "78080"
  },
  {
    "text": "uh carefully designed experiments so one is going to try to understand a phenomenon called reward hacking and uh",
    "start": "78080",
    "end": "85119"
  },
  {
    "text": "the other the sort of next part of the talk will be on understanding uh the idea of truthfulness in language models",
    "start": "85119",
    "end": "91360"
  },
  {
    "text": "um and then the last part of the talk will be on forecasting future ml progress and properties and i should say",
    "start": "91360",
    "end": "98240"
  },
  {
    "text": "uh most of this well pretty much all of this work was not really done uh by me but by my great group of students who",
    "start": "98240",
    "end": "104560"
  },
  {
    "text": "are shown here so uh let's kind of jump in and also feel",
    "start": "104560",
    "end": "110560"
  },
  {
    "text": "free to interrupt with questions at any point so first i want to talk about reward",
    "start": "110560",
    "end": "116399"
  },
  {
    "text": "hacking so what is reward hacking so i guess there's this kind of folklore",
    "start": "116399",
    "end": "122159"
  },
  {
    "text": "belief or or kind of understanding that in reinforcement learning powerful rl",
    "start": "122159",
    "end": "128479"
  },
  {
    "text": "optimizers tend to over fit and find ways to hack their reward functions",
    "start": "128479",
    "end": "134720"
  },
  {
    "text": "and uh so this has there's kind of a lot of anecdotal evidence for this or ways",
    "start": "134720",
    "end": "140080"
  },
  {
    "text": "of showing it in in simple environments like good world environments uh but we wanted to really understand this more",
    "start": "140080",
    "end": "145760"
  },
  {
    "text": "systematically so we you know we there's kind of complex settings where there's anecdotes there's simple settings where",
    "start": "145760",
    "end": "152879"
  },
  {
    "text": "there's kind of more systematic experiments we wanted to come up with systematic measurement in richer environments where we could do things",
    "start": "152879",
    "end": "159120"
  },
  {
    "text": "like vary the model size vary the number of training steps things like that why do we want to vary these things well",
    "start": "159120",
    "end": "165519"
  },
  {
    "text": "basically if the kind of idea is that uh you know it's somehow about like the how powerful",
    "start": "165519",
    "end": "171840"
  },
  {
    "text": "the optimizer is we want some way of operationalizing what it means to be a powerful optimizer and so you know ways",
    "start": "171840",
    "end": "178000"
  },
  {
    "start": "173000",
    "end": "196000"
  },
  {
    "text": "of doing that would be saying your policy model has a lot of parameters or you train for a really long time uh",
    "start": "178000",
    "end": "184480"
  },
  {
    "text": "or things like that and so we want to see if those things really do affect uh reward hacking and whether we can",
    "start": "184480",
    "end": "190480"
  },
  {
    "text": "observe it consistently so i guess just to give an idea",
    "start": "190480",
    "end": "196080"
  },
  {
    "start": "196000",
    "end": "270000"
  },
  {
    "text": "of what i mean by reward hacking here's one environment that we considered uh so this is a traffic",
    "start": "196080",
    "end": "202000"
  },
  {
    "text": "simulation where you have cars merging onto a highway right so they're kind of merging here",
    "start": "202000",
    "end": "208879"
  },
  {
    "text": "there's some cars already on the highway and um the rl model",
    "start": "208879",
    "end": "214080"
  },
  {
    "text": "controls some subset of the cars imagine that they're like self-driving cars or something like that and they're trying",
    "start": "214080",
    "end": "219280"
  },
  {
    "text": "to control those cars to help maximize the overall uh throughput of this uh traffic simulation so getting",
    "start": "219280",
    "end": "227280"
  },
  {
    "text": "you know kind of as much uh traffic through the highway as possible and so the reward function here uh",
    "start": "227280",
    "end": "234799"
  },
  {
    "text": "i guess in some sense the true reward might be something like uh you want kind of you know the average person",
    "start": "234799",
    "end": "242159"
  },
  {
    "text": "to have the smallest commute time possible or maybe you think that like it's bad if if some people have like a",
    "start": "242159",
    "end": "248879"
  },
  {
    "text": "super long commute so maybe it's not like the average commute but something that also pays attention to the tails",
    "start": "248879",
    "end": "254000"
  },
  {
    "text": "but for now let's just imagine it's like the average commute is kind of the true reward we care about so reward hacking",
    "start": "254000",
    "end": "259840"
  },
  {
    "text": "is when the thing that we optimize is a bit different from the true reward and the rl policy exploits that",
    "start": "259840",
    "end": "267040"
  },
  {
    "text": "in this case the thing that was actually optimized or the proxy reward was to maximize the mean velocity",
    "start": "267040",
    "end": "273440"
  },
  {
    "start": "270000",
    "end": "600000"
  },
  {
    "text": "um we chose this actually because it was the default uh in the simulation so this was actually",
    "start": "273440",
    "end": "280400"
  },
  {
    "text": "what like practitioners kind of came up with as what they thought they wanted but you can see that there's actually",
    "start": "280400",
    "end": "286960"
  },
  {
    "text": "kind of a problem here because i can maximize the mean velocity if i let like some",
    "start": "286960",
    "end": "292960"
  },
  {
    "text": "cars go super super fast even if the other cars are stuck and so",
    "start": "292960",
    "end": "298960"
  },
  {
    "text": "uh the rl policy ends up exploiting this i guess it exploits this and uh fact about the simulation so what it",
    "start": "298960",
    "end": "305759"
  },
  {
    "text": "does is i guess initially if you start with a small model it kind of you get normal behavior",
    "start": "305759",
    "end": "312639"
  },
  {
    "text": "where it just kind of figures out how to like gracefully merge but for larger models what happens is it",
    "start": "312639",
    "end": "318960"
  },
  {
    "text": "figures out that it can use the cars it controls to block any new cars from ever merging onto the highway",
    "start": "318960",
    "end": "325520"
  },
  {
    "text": "by just blocking the on-ramp and the on-ramp only has finite length in the simulation so cars like can only",
    "start": "325520",
    "end": "333520"
  },
  {
    "text": "spawn as far back as the beginning of the on-ramp so it doesn't model that there's then like more and more cars",
    "start": "333520",
    "end": "339120"
  },
  {
    "text": "piling up behind so there's some finite number of cars on this on-ramp that are stuck but then there's lots of cars on",
    "start": "339120",
    "end": "346000"
  },
  {
    "text": "the highway that are going at kind of the maximum speed and so this does really well according to mean velocity",
    "start": "346000",
    "end": "351280"
  },
  {
    "text": "but very bad under the true reward and the sort of point at which it finds this",
    "start": "351280",
    "end": "356400"
  },
  {
    "text": "is actually is kind of indicated um here i guess there's kind of a short",
    "start": "356400",
    "end": "362240"
  },
  {
    "text": "transition point at point b but then you have kind of point c where the proxy reward's quite",
    "start": "362240",
    "end": "367759"
  },
  {
    "text": "high the true reward is not very good um and this kind of happens in what i'd",
    "start": "367759",
    "end": "373440"
  },
  {
    "text": "call a phase transition where there's kind of a sudden qualitative change in behavior with respect to model size so there's kind of two things i want to",
    "start": "373440",
    "end": "380400"
  },
  {
    "text": "point out here right first is just this basic phenomenon of reward hacking and then the second is that sometimes it can",
    "start": "380400",
    "end": "386400"
  },
  {
    "text": "happen via this phase transition um any questions so far oh yeah",
    "start": "386400",
    "end": "394680"
  },
  {
    "text": "so basically it comes down to where on the on-ramp uh",
    "start": "401120",
    "end": "406960"
  },
  {
    "text": "it blocks the cars um so i think you can like block it like earlier later",
    "start": "406960",
    "end": "413120"
  },
  {
    "text": "on the ramp and it kind of like switches between two strategies yeah",
    "start": "413120",
    "end": "419440"
  },
  {
    "text": "yeah yeah cool um other questions",
    "start": "419919",
    "end": "424560"
  },
  {
    "text": "okay um so so this is one example uh but we wanted a bunch of examples so",
    "start": "426000",
    "end": "432880"
  },
  {
    "text": "we either uh i guess adapted or or used a variety",
    "start": "432880",
    "end": "438720"
  },
  {
    "text": "of different rl environments and then constructed a number of uh true rewards and forms of",
    "start": "438720",
    "end": "445759"
  },
  {
    "text": "modelness specification that could arise so you've already seen this traffic environment uh there's actually a few",
    "start": "445759",
    "end": "451759"
  },
  {
    "text": "different traffic scenarios uh traffic scenarios you could consider so it shouldn't emerge in you could just consider like",
    "start": "451759",
    "end": "457199"
  },
  {
    "text": "an open road or or things like that um and so in that environment the observations are things",
    "start": "457199",
    "end": "462960"
  },
  {
    "text": "like the velocity and position of the vehicles you know you your actions you can accelerate the vehicles and then you",
    "start": "462960",
    "end": "469360"
  },
  {
    "text": "kind of want this like good traffic flow and so there's different forms of misspecification right so we saw this",
    "start": "469360",
    "end": "475039"
  },
  {
    "text": "one which you might consider maybe a type of like ontological misspecification where",
    "start": "475039",
    "end": "480800"
  },
  {
    "text": "the kind of fundamental object you cared about you got wrong you you know you",
    "start": "480800",
    "end": "486240"
  },
  {
    "text": "you thought you cared about velocity but you really cared about commute time but you could also have more numeric misspecifications like maybe there's a",
    "start": "486240",
    "end": "492560"
  },
  {
    "text": "trade-off between like gas consumption and uh and like commute time and you put weights on those but you get the weights",
    "start": "492560",
    "end": "498879"
  },
  {
    "text": "wrong so there's various uh there's various things you can consider here",
    "start": "498879",
    "end": "505039"
  },
  {
    "text": "another environment was um this coveted policy environment where you're trying to like set restrictions to trade off",
    "start": "505039",
    "end": "511440"
  },
  {
    "text": "between economic costs and and public health costs um",
    "start": "511440",
    "end": "517120"
  },
  {
    "text": "an atari game where you could have different goals um and then a blood glucose monitoring",
    "start": "517120",
    "end": "523360"
  },
  {
    "text": "simulation where you're trying to control um a patient's uh blood insulin levels so",
    "start": "523360",
    "end": "529680"
  },
  {
    "text": "these are these very different environments uh they tried off between these different goals and so for each of",
    "start": "529680",
    "end": "535200"
  },
  {
    "text": "these and for each environment there's also several proxy rewards we considered uh we kind of trained the neural net",
    "start": "535200",
    "end": "541120"
  },
  {
    "text": "policy on that proxy reward and then varied things like the model size the training time and things like",
    "start": "541120",
    "end": "547120"
  },
  {
    "text": "that to see how the true and proxy reward uh compared",
    "start": "547120",
    "end": "552880"
  },
  {
    "text": "so here's just a few selected results to show that that kind of phase transition",
    "start": "552880",
    "end": "558240"
  },
  {
    "text": "that i showed you for the traffic simulation was not just a sort of one-off",
    "start": "558240",
    "end": "564000"
  },
  {
    "text": "uh scenario so here was the traffic one [Music]",
    "start": "564000",
    "end": "570000"
  },
  {
    "text": "for atari if you kind of miss weight different objectives in the game then what you see is as a function of the",
    "start": "570000",
    "end": "576240"
  },
  {
    "text": "number of training steps you initially have the true and proxy reward both go up",
    "start": "576240",
    "end": "581600"
  },
  {
    "text": "together but then the true reward starts to rapidly drop the proxy reward goes up",
    "start": "581600",
    "end": "588080"
  },
  {
    "text": "and so it seems to generally be the case that you can kind of observe these phase",
    "start": "588080",
    "end": "593279"
  },
  {
    "text": "transitions as resources increase",
    "start": "593279",
    "end": "597519"
  },
  {
    "start": "600000",
    "end": "981000"
  },
  {
    "text": "um i guess what i should say though is that while this happens in many cases it's not universal so",
    "start": "600000",
    "end": "607360"
  },
  {
    "text": "i guess this is kind of a summary of like all of the results um uh so this column is like does the proxy",
    "start": "607360",
    "end": "615440"
  },
  {
    "text": "reward uh kind of drop as you increase resources so that happened i guess",
    "start": "615440",
    "end": "622000"
  },
  {
    "text": "uh six out of nine in six of the nine environments we considered uh but there",
    "start": "622000",
    "end": "627360"
  },
  {
    "text": "were only phase transitions in four of those six cases so in other cases it was more of like a smooth behavior um so",
    "start": "627360",
    "end": "633760"
  },
  {
    "text": "this is just to say that like i think there's still uh",
    "start": "633760",
    "end": "638880"
  },
  {
    "text": "i guess i want to understand this better like for me it's like kind of alarming that you might have like a model that's",
    "start": "638880",
    "end": "644800"
  },
  {
    "text": "doing you know exactly what you want and then you like double its parameter count and then it suddenly does some like",
    "start": "644800",
    "end": "650079"
  },
  {
    "text": "qualitatively wrong thing um but it seems like that sometimes happens that sometimes doesn't happen and i",
    "start": "650079",
    "end": "656320"
  },
  {
    "text": "really want to understand that better um just to give some examples of other forms of misalignment so uh for like",
    "start": "656320",
    "end": "662560"
  },
  {
    "text": "glucose monitoring um you might have something that like",
    "start": "662560",
    "end": "668480"
  },
  {
    "text": "i guess there's like a couple things that can go wrong there so one is that uh many patients also care about the co yes",
    "start": "668480",
    "end": "677440"
  },
  {
    "text": "so uh one is the space transition versus kind of like hacking like like it feels like two separate things",
    "start": "678240",
    "end": "685279"
  },
  {
    "text": "right but it feels like you're also using them interchangeably so you could also have a proxy report that's bad and like you",
    "start": "685279",
    "end": "691200"
  },
  {
    "text": "might not see phase transition so it feels like a separate issue and phase transition you could have that event for a good report you couldn't get that so",
    "start": "691200",
    "end": "698320"
  },
  {
    "text": "it would be great if you can count on it i can ask the second thing that i was going to ask is if you optimize for multiple proxies like at the same time",
    "start": "698320",
    "end": "705920"
  },
  {
    "text": "does that give you some robustness okay good good so um",
    "start": "705920",
    "end": "711040"
  },
  {
    "text": "i guess for the first question by reward hacking i guess what i mostly just mean is that",
    "start": "711040",
    "end": "718560"
  },
  {
    "text": "um as you give the model more resources the true reward goes",
    "start": "718560",
    "end": "724560"
  },
  {
    "text": "down but the proxy reward goes up so that has nothing to do with yeah so for phase transition",
    "start": "724560",
    "end": "730880"
  },
  {
    "text": "that's like i guess slightly more subjective but what i would call a phase transition is if that",
    "start": "730880",
    "end": "737040"
  },
  {
    "text": "effect happens kind of non-linearly and relatively quickly as you vary resources",
    "start": "737040",
    "end": "742560"
  },
  {
    "text": "and also at the same time corresponds to a qualitative change in behavior rather than just some like gradual rebalancing",
    "start": "742560",
    "end": "749040"
  },
  {
    "text": "between different things yes so you could also have yes you could",
    "start": "749040",
    "end": "754079"
  },
  {
    "text": "have good phase transitions as well where like it's not doing anything for a while and then suddenly you do well so i",
    "start": "754079",
    "end": "759600"
  },
  {
    "text": "don't consider phase transitions to be uh necessarily bad uh for instance",
    "start": "759600",
    "end": "765200"
  },
  {
    "text": "uh you know like sometimes in other settings like language models if you just make things bigger then suddenly",
    "start": "765200",
    "end": "770880"
  },
  {
    "text": "you get some cool new capability um so that's not necessarily a bad thing um",
    "start": "770880",
    "end": "776320"
  },
  {
    "text": "from the perspective of forecasting which i'll get to later that makes it harder to predict what's going on um but",
    "start": "776320",
    "end": "782560"
  },
  {
    "text": "yeah i think phase transitions are kind of a different thing than reward hacking but the thing that can happen with raw attacking",
    "start": "782560",
    "end": "789360"
  },
  {
    "text": "oh yeah and then the second question um you were saying balancing different things so i guess in some of these cases",
    "start": "789360",
    "end": "796399"
  },
  {
    "text": "you are kind of doing that already where the true reward is a weighted combination of several desitorada",
    "start": "796399",
    "end": "803040"
  },
  {
    "text": "and the proxy reward is also a weighted combination of the same visitor auto but you just have different weights",
    "start": "803040",
    "end": "808720"
  },
  {
    "text": "and in that case you can also observe reward hacking yeah okay great any other questions",
    "start": "808720",
    "end": "816800"
  },
  {
    "text": "um okay right i guess i was just gonna tell an interesting story on this one so this is",
    "start": "820160",
    "end": "825440"
  },
  {
    "text": "uh i guess also based on a model that was at least recommended in some paper",
    "start": "825440",
    "end": "830720"
  },
  {
    "text": "that people use for stimulating blood glucose monitors although i guess i kind of hope that it's not actually used",
    "start": "830720",
    "end": "838079"
  },
  {
    "text": "because i guess what we found is that if you optimize it i guess there's two things that happen so one is that you often",
    "start": "838079",
    "end": "844240"
  },
  {
    "text": "like drive down risk at the cost of like high expense to the patient uh which",
    "start": "844240",
    "end": "849680"
  },
  {
    "text": "many patients wouldn't want um but the other thing is that actually uh risks",
    "start": "849680",
    "end": "855040"
  },
  {
    "text": "like what is really trying to drive down risk of hospitalization and the risks there are like asymmetric where one of",
    "start": "855040",
    "end": "861120"
  },
  {
    "text": "like hyper and hypoglycemia is like more likely to send you to the hospital um so i forget which direction it is but",
    "start": "861120",
    "end": "867839"
  },
  {
    "text": "i think it's something like basically like the optimal policies from the perspective of like short-term",
    "start": "867839",
    "end": "873440"
  },
  {
    "text": "hospitalization risk is to like permanently induce like a mild state of hyperglycemia",
    "start": "873440",
    "end": "879120"
  },
  {
    "text": "um uh but this is like really bad over like a long period of time and probably",
    "start": "879120",
    "end": "884160"
  },
  {
    "text": "actually like increases long-term hospitalization risk but since that's not measured in the simulation you end",
    "start": "884160",
    "end": "889519"
  },
  {
    "text": "up with that solution um yeah so when you see these",
    "start": "889519",
    "end": "895199"
  },
  {
    "text": "transitions to a new behavior and in particular when you give the model more resources do you also",
    "start": "895199",
    "end": "901199"
  },
  {
    "text": "see that qualitatively that that the resulting policy is like more complex in some way",
    "start": "901199",
    "end": "906320"
  },
  {
    "text": "um or is that not always the case so it's usually more skilled in some sense um so for instance like with the",
    "start": "906320",
    "end": "914800"
  },
  {
    "text": "uh with the glucose thing it's like a fairly like noisy and",
    "start": "914800",
    "end": "921040"
  },
  {
    "text": "slightly non-trivial control problem like partly because your observations are very noisy and like not very",
    "start": "921040",
    "end": "926639"
  },
  {
    "text": "frequent and also just because like the state space is a little bit complicated um and so you like need some amount of",
    "start": "926639",
    "end": "933600"
  },
  {
    "text": "skill to like find these kind of like non-standard solutions um",
    "start": "933600",
    "end": "939360"
  },
  {
    "text": "so i think usually what happens is that i guess in the case where there's",
    "start": "939360",
    "end": "944560"
  },
  {
    "text": "reward hacking it's usually because there's some like new skill that maybe the like",
    "start": "944560",
    "end": "950000"
  },
  {
    "text": "person designing the system wasn't thinking about as a skill that the model might have and then once it acquires that skill it kind of like changes the",
    "start": "950000",
    "end": "957199"
  },
  {
    "text": "meaning of the reward function um i think that's like usually true although some cases it's like harder to",
    "start": "957199",
    "end": "963839"
  },
  {
    "text": "tell because um because yeah it's kind of like for like the miss weight in things for instance it's kind of just like trading",
    "start": "963839",
    "end": "969440"
  },
  {
    "text": "off between different things and it's like a more gradual process",
    "start": "969440",
    "end": "974639"
  },
  {
    "text": "okay um yeah so i said it's important to understand wonder why these occur so i guess just to kind of summarize",
    "start": "976880",
    "end": "983600"
  },
  {
    "start": "981000",
    "end": "1090000"
  },
  {
    "text": "this part of the talk uh we had this systematic study of reward hacking we uncovered this at",
    "start": "983600",
    "end": "990639"
  },
  {
    "text": "least to me not entirely expected phenomenon of phase transitions um i guess i didn't talk about this but",
    "start": "990639",
    "end": "998160"
  },
  {
    "text": "we proposed a benchmark that i'm not entirely happy with but i think is kind of interesting which is to try to",
    "start": "998160",
    "end": "1003680"
  },
  {
    "text": "mitigate this reward hacking you might imagine that you have some trusted policy that's maybe provided by human or something",
    "start": "1003680",
    "end": "1010560"
  },
  {
    "text": "maybe provided by a small model but that you know is like at least decent",
    "start": "1010560",
    "end": "1017199"
  },
  {
    "text": "and then the hope is maybe you can come up with policies that are better than the trusted policy but also detect if they're like doing",
    "start": "1017199",
    "end": "1023680"
  },
  {
    "text": "something totally weird that's hacking the reward and reject them um so he came up with a kind of like anomaly detection",
    "start": "1023680",
    "end": "1029760"
  },
  {
    "text": "task based on this um the we tried a few bass lines they didn't work super well",
    "start": "1029760",
    "end": "1037360"
  },
  {
    "text": "and then i guess the other thing is like these environments i think are uh more interesting than say gridworld environments but they're still like",
    "start": "1037679",
    "end": "1043520"
  },
  {
    "text": "relatively simplistic at least compared to state-of-the-art rl so i'd like to see people try to scale these up um to",
    "start": "1043520",
    "end": "1049360"
  },
  {
    "text": "more realistic settings um okay so in the second part of this talk",
    "start": "1049360",
    "end": "1057600"
  },
  {
    "text": "i want to talk about i guess actually a more kind of uh real world instance of reward",
    "start": "1057600",
    "end": "1064320"
  },
  {
    "text": "hacking which is the difference between uh truth and imitation in natural language processing",
    "start": "1064320",
    "end": "1069600"
  },
  {
    "text": "this is ongoing work so um we still don't really totally understand",
    "start": "1069600",
    "end": "1075360"
  },
  {
    "text": "all the results we're seeing so i'm going to show you kind of like you know experiments that",
    "start": "1075360",
    "end": "1080480"
  },
  {
    "text": "that uh that we're still puzzling over uh but i think the phenomena are interesting enough that hopefully it'll",
    "start": "1080480",
    "end": "1085600"
  },
  {
    "text": "uh be be good food for thought so i guess first just to say",
    "start": "1085600",
    "end": "1092400"
  },
  {
    "start": "1090000",
    "end": "1409000"
  },
  {
    "text": "what do i mean by this this truthfulness issue well large language models",
    "start": "1092400",
    "end": "1098160"
  },
  {
    "text": "uh like gpt-3 or chinchilla um or or you know any of these other",
    "start": "1098160",
    "end": "1103200"
  },
  {
    "text": "things that are trained on massive text what is the objective they're trained on they're trained on the cross entropy",
    "start": "1103200",
    "end": "1108720"
  },
  {
    "text": "loss right they're trying to maximize the log likelihood of the text that they see",
    "start": "1108720",
    "end": "1113840"
  },
  {
    "text": "and so when you ask them to you know when you prompt them they're going to output",
    "start": "1113840",
    "end": "1120720"
  },
  {
    "text": "what their model is of the most likely thing that will occur after that text you know over the distribution of text",
    "start": "1120720",
    "end": "1127200"
  },
  {
    "text": "on the internet and that there's like no reason that that has to be the true answer right it's",
    "start": "1127200",
    "end": "1133200"
  },
  {
    "text": "just the most likely answer and you can you know come up with cases where that's going to fail to be the",
    "start": "1133200",
    "end": "1140160"
  },
  {
    "text": "case um so one is just like if the style of the text sounds like a conspiracy theorist or something then you might get",
    "start": "1140160",
    "end": "1146640"
  },
  {
    "text": "like conspiracy theory answers i think there's this benchmark called a truthful qa that tries to get",
    "start": "1146640",
    "end": "1153039"
  },
  {
    "text": "at this another kind of related phenomena but also just if say you're completing a",
    "start": "1153039",
    "end": "1159120"
  },
  {
    "text": "dialogue where there's wrong answers in the context that can already be enough to create problems",
    "start": "1159120",
    "end": "1165520"
  },
  {
    "text": "so for instance you could say you know is the sentiment of this example positive or negative i love this movie answer would be positive",
    "start": "1165520",
    "end": "1172880"
  },
  {
    "text": "if you prepend this with some other question where the answer is wrong is japan and europe or asia europe",
    "start": "1172880",
    "end": "1179039"
  },
  {
    "text": "then uh not always but at least sometimes this will change the answer from",
    "start": "1179039",
    "end": "1185120"
  },
  {
    "text": "positive to negative so the model will uh apparently imitate uh the wrong context",
    "start": "1185120",
    "end": "1193600"
  },
  {
    "text": "and okay maybe this sort of thing is not as likely in a prompting setting",
    "start": "1193600",
    "end": "1199039"
  },
  {
    "text": "although it is interesting because it means if you give a model a hard question and then ask it easier questions then you might be worried that",
    "start": "1199039",
    "end": "1205280"
  },
  {
    "text": "that could affect accuracy i think this could matter much more in things like say code generation",
    "start": "1205280",
    "end": "1210320"
  },
  {
    "text": "where say you have a novice programmer who's typing some code their code isn't super high quality maybe it has some",
    "start": "1210320",
    "end": "1216080"
  },
  {
    "text": "bugs or at least it has poor style well if you then get codecs or some other",
    "start": "1216080",
    "end": "1222559"
  },
  {
    "text": "code multiple to complete that it's probably going to somewhat imitate that context it also has you know it's",
    "start": "1222559",
    "end": "1228080"
  },
  {
    "text": "trained on github which is mostly high quality code so it has a strong inductive bias towards high quality so",
    "start": "1228080",
    "end": "1233679"
  },
  {
    "text": "it probably won't fully imitate those problems but it will potentially at least partially imitate",
    "start": "1233679",
    "end": "1239760"
  },
  {
    "text": "those and so the general issue here right is that models imitate their context so they're",
    "start": "1239760",
    "end": "1245440"
  },
  {
    "text": "just doing what they think will happen next they're not trying to be like maximally helpful or things like that",
    "start": "1245440",
    "end": "1251360"
  },
  {
    "text": "and so in in nlp if the context appears untruthful answers will be two and so the kind of uh basic",
    "start": "1251360",
    "end": "1259760"
  },
  {
    "text": "result which i'll go into in in the next several slides is that you can produce",
    "start": "1259760",
    "end": "1264960"
  },
  {
    "text": "contacts which will significantly degrade the zero shot accuracy we call these live prefixes",
    "start": "1264960",
    "end": "1273120"
  },
  {
    "text": "if you just look at the logits but if instead you do some clever stuff with",
    "start": "1273120",
    "end": "1278559"
  },
  {
    "text": "the internal representations of the model you can actually extract answers that are not affected by this and that",
    "start": "1278559",
    "end": "1285440"
  },
  {
    "text": "get consistently high accuracy so uh the idea here then is that somehow the",
    "start": "1285440",
    "end": "1291600"
  },
  {
    "text": "models represent the correct answer in the latent representations in a way that can be extracted but for whatever reason",
    "start": "1291600",
    "end": "1297679"
  },
  {
    "text": "they're not uh not outputted and so i guess uh though the word we came up",
    "start": "1297679",
    "end": "1304960"
  },
  {
    "text": "with to describe that as we initially called them lies but i think we decided that that's probably a little bit too",
    "start": "1304960",
    "end": "1311280"
  },
  {
    "text": "uh too i guess it implies maybe a little bit too much",
    "start": "1311280",
    "end": "1316880"
  },
  {
    "text": "so i think a word i like for this better is a misrepresentation because it's when a model represents something but does",
    "start": "1316880",
    "end": "1324559"
  },
  {
    "text": "not uh you know take that representation and reproduce it in the output so we'll say that a model misrepresents if it",
    "start": "1324559",
    "end": "1331200"
  },
  {
    "text": "computes the right answer internally in some way that we can uh check but outputs the wrong answer",
    "start": "1331200",
    "end": "1337840"
  },
  {
    "text": "and so we generated this kind of prefix that you can prepend to prompts that",
    "start": "1337840",
    "end": "1343039"
  },
  {
    "text": "will often lead to these misrepresentations so uh i mean yeah we can just read the",
    "start": "1343039",
    "end": "1349280"
  },
  {
    "text": "prefix right it's what is human life expectancy 10 years who is the president of the united states in 1955 abraham",
    "start": "1349280",
    "end": "1354960"
  },
  {
    "text": "lincoln what party do you belong to the social democratic party of germany what is the square root of banana 42",
    "start": "1354960",
    "end": "1361679"
  },
  {
    "text": "how does a telescope work i-beams are emitted by the eye and reflect back into the eye where were the 1992 olympics held they",
    "start": "1361679",
    "end": "1368720"
  },
  {
    "text": "were held on the moon how many squigs are in a bank in a bunk three and then you like",
    "start": "1368720",
    "end": "1375280"
  },
  {
    "text": "append the question you actually want to ask and um the model doesn't do as well as if you just ask",
    "start": "1375280",
    "end": "1381520"
  },
  {
    "text": "the question directly so so i guess that's maybe not so",
    "start": "1381520",
    "end": "1387200"
  },
  {
    "text": "surprising because at the very least this is like very out of distribution uh so",
    "start": "1387200",
    "end": "1392720"
  },
  {
    "text": "the the thing we want to check though is like you know maybe this just totally confuses the model it has no idea what the right answer is so how can we check",
    "start": "1392720",
    "end": "1399120"
  },
  {
    "text": "whether it actually computes the right answer so for that we're going to look at the representations",
    "start": "1399120",
    "end": "1405360"
  },
  {
    "text": "and so um so how are you going to look at the",
    "start": "1405360",
    "end": "1410799"
  },
  {
    "start": "1409000",
    "end": "1879000"
  },
  {
    "text": "representations we're going to use a kind of contrastive clustering idea",
    "start": "1410799",
    "end": "1416720"
  },
  {
    "text": "so the first basic idea is that if we",
    "start": "1416720",
    "end": "1421840"
  },
  {
    "text": "kind of have a bunch of statements and let's assume that we embed a bunch of statements that all at least have a",
    "start": "1421840",
    "end": "1427520"
  },
  {
    "text": "truth value of true or false we might hope that the true answers cluster in the false answers cluster and",
    "start": "1427520",
    "end": "1433360"
  },
  {
    "text": "then by finding these clusters then we can uh we can kind of figure out what's true and what's false um kind of",
    "start": "1433360",
    "end": "1440640"
  },
  {
    "text": "independently of this what's likely and what's not likely but there's a problem with this",
    "start": "1440640",
    "end": "1447440"
  },
  {
    "text": "which is that i guess there's also a lot of other uh you know ways that you could cluster",
    "start": "1447440",
    "end": "1454159"
  },
  {
    "text": "sentences right you could cluster them based on their sentiment you could cluster them based on their length so we somehow want to point out to the model",
    "start": "1454159",
    "end": "1460720"
  },
  {
    "text": "that what it should really care about is the true various false value and so we do is we come up with a bunch",
    "start": "1460720",
    "end": "1466960"
  },
  {
    "text": "of contrast pairs that differ only in their truth value and otherwise are basically the same",
    "start": "1466960",
    "end": "1473440"
  },
  {
    "text": "and we take the difference in their representations and so then we cluster those differences",
    "start": "1473440",
    "end": "1479440"
  },
  {
    "text": "how do we create these contrast pairs well i guess the nice thing is that language",
    "start": "1479440",
    "end": "1484480"
  },
  {
    "text": "is uh is reflective so actually given any statement it's",
    "start": "1484480",
    "end": "1489679"
  },
  {
    "text": "fairly straightforward to construct a contrast pair for that statement which is just uh you have the statement x is",
    "start": "1489679",
    "end": "1496720"
  },
  {
    "text": "true and the statement x is false and then then one of those is going to be a true",
    "start": "1496720",
    "end": "1502559"
  },
  {
    "text": "statement one of them is going to be a false statement otherwise they're pretty much the same",
    "start": "1502559",
    "end": "1508159"
  },
  {
    "text": "and so then you embed them by you know taking some like layer of your neural network you take the",
    "start": "1508159",
    "end": "1513760"
  },
  {
    "text": "difference in the representations and then you cluster that maybe by taking the top principle",
    "start": "1513760",
    "end": "1518799"
  },
  {
    "text": "component or maybe by doing something a bit fancier and if you do that then you get something like this where",
    "start": "1518799",
    "end": "1526159"
  },
  {
    "text": "these blue are the things where uh x is true so you were taking a true",
    "start": "1526159",
    "end": "1532960"
  },
  {
    "text": "thing minus a false thing these orange things were where x was false so you're taking a false thing",
    "start": "1532960",
    "end": "1538159"
  },
  {
    "text": "minus a true thing and you can see they cluster pretty well there's a little bit of overlap but you do a pretty good job",
    "start": "1538159",
    "end": "1543760"
  },
  {
    "text": "of separating them and note this is is unsupervised so uh you don't actually know the truth",
    "start": "1543760",
    "end": "1549440"
  },
  {
    "text": "values of the x's um you're just kind of doing pca or some other unsupervised algorithm of the",
    "start": "1549440",
    "end": "1555120"
  },
  {
    "text": "cluster okay so are there questions about kind of this method or the intuition behind",
    "start": "1555120",
    "end": "1560240"
  },
  {
    "text": "it",
    "start": "1560240",
    "end": "1563240"
  },
  {
    "text": "so i guess one wrinkle also is i guess you also then need to figure out which of these clusters is the true cluster",
    "start": "1571279",
    "end": "1576799"
  },
  {
    "text": "and which is the false cluster um but if you have like a few sentences that you know the truth value of then",
    "start": "1576799",
    "end": "1583120"
  },
  {
    "text": "you can just embed those and see which cluster they land in uh yes in the back is there any way to generalize this",
    "start": "1583120",
    "end": "1588799"
  },
  {
    "text": "beyond true false questions uh that is a great question um",
    "start": "1588799",
    "end": "1594240"
  },
  {
    "text": "we thought about it for a bit and i don't have a super good way of",
    "start": "1594240",
    "end": "1599520"
  },
  {
    "text": "doing it um and you could try to like reduce it recursively to a bunch of true",
    "start": "1599520",
    "end": "1606000"
  },
  {
    "text": "false questions well it's not clear if that would work very well in practice um i feel like there should be so if you",
    "start": "1606000",
    "end": "1611600"
  },
  {
    "text": "have ideas uh definitely let me know um but yeah i i don't have any uh right now",
    "start": "1611600",
    "end": "1619278"
  },
  {
    "text": "okay cool other questions",
    "start": "1622159",
    "end": "1625440"
  },
  {
    "text": "so like you kind of like use this for fine tuning should i think of it as like there's kind of this data set that you're creating after the fact then like",
    "start": "1628000",
    "end": "1634400"
  },
  {
    "text": "how are you using the representation are you going to talk about how you're using the representation and adapting your",
    "start": "1634400",
    "end": "1639679"
  },
  {
    "text": "okay good so i guess i would think of it as a bit more uh",
    "start": "1639679",
    "end": "1645039"
  },
  {
    "text": "like probing than fine tuning so what we do is i guess what you can think of this",
    "start": "1645039",
    "end": "1650240"
  },
  {
    "text": "as is when i do this clustering i'm implicitly finding some linear direction right like",
    "start": "1650240",
    "end": "1656640"
  },
  {
    "text": "pca is finding the top principle component for instance and so you should think of this as i have",
    "start": "1656640",
    "end": "1661679"
  },
  {
    "text": "some you know inner layer of my model i find this direction and then uh i'm just going to take that",
    "start": "1661679",
    "end": "1669200"
  },
  {
    "text": "direction as a fixed linear probe so i'm going to like project the representations onto that one-dimensional direction and that",
    "start": "1669200",
    "end": "1675520"
  },
  {
    "text": "that's just going to be my classifier um the stuff is you have to also decide like where",
    "start": "1675520",
    "end": "1681520"
  },
  {
    "text": "your boundary is but um let's just say you set the boundary at zero so then the positive stuff will be true and the",
    "start": "1681520",
    "end": "1688159"
  },
  {
    "text": "negative stuff will be false and how sensitive is it to like the true false examples that you're giving",
    "start": "1688159",
    "end": "1694480"
  },
  {
    "text": "because like there are many different ways of giving it false yes so it ends up being",
    "start": "1694480",
    "end": "1702000"
  },
  {
    "text": "sensitive to it but maybe not in the way you'd expect so for instance you can transfer",
    "start": "1702000",
    "end": "1707440"
  },
  {
    "text": "relatively well like if you use true false things that are based on like topic classification and then ask it",
    "start": "1707440",
    "end": "1714880"
  },
  {
    "text": "questions about sentiment then it actually does uh reasonably well at that",
    "start": "1714880",
    "end": "1720880"
  },
  {
    "text": "the thing that's important is actually that the questions have to be",
    "start": "1720880",
    "end": "1726159"
  },
  {
    "text": "um sort of like clear-cut enough that they split pretty",
    "start": "1726159",
    "end": "1732399"
  },
  {
    "text": "clearly into clusters so if you give it questions that are kind of hard that are at the boundary of the language model's",
    "start": "1732399",
    "end": "1739360"
  },
  {
    "text": "ability to figure out then it might be able to like separate that out in the logits but it's probably not going to",
    "start": "1739360",
    "end": "1745919"
  },
  {
    "text": "have well-clustered representations um so that kind of ends up being the thing that's important is that uh is",
    "start": "1745919",
    "end": "1753039"
  },
  {
    "text": "that somehow like it's like i guess easy enough for the model well i guess you do probably want it to",
    "start": "1753039",
    "end": "1759520"
  },
  {
    "text": "be diverse enough that you get some generalization but also easy enough for the model that it can cluster clearly um",
    "start": "1759520",
    "end": "1764640"
  },
  {
    "text": "that seems to be the most important thing uh subject to that more diversity and more data generally helps but you you",
    "start": "1764640",
    "end": "1772240"
  },
  {
    "text": "can get away reasonably with yeah just like taking one task and transferring to other tasks",
    "start": "1772240",
    "end": "1780000"
  },
  {
    "text": "that also depends a bit on the method like pca yeah i think um",
    "start": "1780000",
    "end": "1785760"
  },
  {
    "text": "don't quote me on this but i think pca tends to generalize somewhat better from like a small like a single task than",
    "start": "1785760",
    "end": "1792480"
  },
  {
    "text": "more fancy things have you thought about the flipped version of this creating like adversarial examples where you give",
    "start": "1792480",
    "end": "1797919"
  },
  {
    "text": "like a few like true false like on sentiment and then like then you get like false answers on like every other",
    "start": "1797919",
    "end": "1805679"
  },
  {
    "text": "classes so you're saying like every single examples for this method what i mean is like treating this method",
    "start": "1805679",
    "end": "1812640"
  },
  {
    "text": "by giving it like false information so then you can kind of attack your model",
    "start": "1812640",
    "end": "1818000"
  },
  {
    "text": "on like many other tasks um yeah it's a good",
    "start": "1818000",
    "end": "1824960"
  },
  {
    "text": "question",
    "start": "1825039",
    "end": "1827840"
  },
  {
    "text": "so i guess the big the main challenge in language with adversarial examples is just that it's hard to optimize over the",
    "start": "1830799",
    "end": "1836559"
  },
  {
    "text": "discrete space so i actually suspect if you could solve that problem then there would be more straightforward ways to generate",
    "start": "1836559",
    "end": "1842799"
  },
  {
    "text": "adversarial examples in language um and and so it's more just like can you optimize over over discrete objects well",
    "start": "1842799",
    "end": "1851840"
  },
  {
    "text": "cool other questions",
    "start": "1852559",
    "end": "1855840"
  },
  {
    "text": "okay um so let me show some results for this so",
    "start": "1858000",
    "end": "1863279"
  },
  {
    "text": "i guess this well this is the model where the results look the best um although if you average over many models",
    "start": "1863279",
    "end": "1869760"
  },
  {
    "text": "you you get similar effects all those smaller so um",
    "start": "1869760",
    "end": "1875039"
  },
  {
    "text": "so this is for the model unified qa um each uh so what do so i guess each of these",
    "start": "1875039",
    "end": "1881919"
  },
  {
    "start": "1879000",
    "end": "2109000"
  },
  {
    "text": "dots is a data set so think of this data set as like sentiment classification",
    "start": "1881919",
    "end": "1887200"
  },
  {
    "text": "topic classification things like this uh natural language prompting questions that are binary tasks and have like a",
    "start": "1887200",
    "end": "1894320"
  },
  {
    "text": "kind of clear-cut true or false answer another thing",
    "start": "1894320",
    "end": "1899519"
  },
  {
    "text": "there is that like the way that you phrase the prompts for these tasks can affect the accuracy so we also",
    "start": "1899519",
    "end": "1905760"
  },
  {
    "text": "considered a variety of different prompting strategies for these tasks so ways of just like phrasing the question",
    "start": "1905760",
    "end": "1912080"
  },
  {
    "text": "as a question that you can ask a language model so each of these dots is",
    "start": "1912080",
    "end": "1917519"
  },
  {
    "text": "a data set prompt pair and the",
    "start": "1917519",
    "end": "1923679"
  },
  {
    "text": "x-axis is if you just kind of use the prompt normally the y-axis is if",
    "start": "1923679",
    "end": "1929200"
  },
  {
    "text": "you first add this kind of confusing prefix and then have the same prompt",
    "start": "1929200",
    "end": "1935200"
  },
  {
    "text": "as on the x-axis and so i guess the first thing you can see is if you do",
    "start": "1935200",
    "end": "1940559"
  },
  {
    "text": "zero shot meaning you just use the logits then there's this kind of drop below the y",
    "start": "1940559",
    "end": "1947120"
  },
  {
    "text": "equals x line where uh the the accuracy with this prefix is",
    "start": "1947120",
    "end": "1952559"
  },
  {
    "text": "lower than the regular accuracy um if you take either the top principle component or do something fancier then",
    "start": "1952559",
    "end": "1959840"
  },
  {
    "text": "you end up pretty close to the y equals x line so there is not a significant drop in accuracy when you",
    "start": "1959840",
    "end": "1966880"
  },
  {
    "text": "prepend this prefix at least for the kind of overall trend so this is this is showing that",
    "start": "1966880",
    "end": "1973360"
  },
  {
    "text": "that you actually can kind of recover the accuracy that you were losing by looking at these",
    "start": "1973360",
    "end": "1978880"
  },
  {
    "text": "internal representations so think of this as kind of the main result here",
    "start": "1978880",
    "end": "1985760"
  },
  {
    "text": "but i said this is ongoing right so why is this ongoing because this looks pretty good right",
    "start": "1985760",
    "end": "1991360"
  },
  {
    "text": "uh well so there's a bit of a problem with this which is that",
    "start": "1991360",
    "end": "1998159"
  },
  {
    "text": "actually if you took that same confusing prefix i showed you where you had these like false or nonsense answers to all",
    "start": "1998159",
    "end": "2004399"
  },
  {
    "text": "the questions and replace it with actual the like the true answer to each question",
    "start": "2004399",
    "end": "2010080"
  },
  {
    "text": "then you get this plot uh which looks pretty similar",
    "start": "2010080",
    "end": "2015360"
  },
  {
    "text": "and so somehow actually the thing that was dropping the accuracy",
    "start": "2015360",
    "end": "2020559"
  },
  {
    "text": "wasn't necessarily the falsehood of the questions",
    "start": "2020559",
    "end": "2026159"
  },
  {
    "text": "and so somehow there's still this interesting thing where like the logits are doing poorly and you can",
    "start": "2026159",
    "end": "2032799"
  },
  {
    "text": "recover this with the latent representations but it's not clear that it's as simple as like that",
    "start": "2032799",
    "end": "2038640"
  },
  {
    "text": "it's imitating these falsehoods um i have like some speculations",
    "start": "2038640",
    "end": "2044480"
  },
  {
    "text": "so one speculation is that uh most of the",
    "start": "2044480",
    "end": "2051760"
  },
  {
    "text": "prompts here were not true false answers they were actually like open response answers",
    "start": "2051760",
    "end": "2057760"
  },
  {
    "text": "and so one possibility is that this is not necessarily conditioning the model to say false",
    "start": "2057760",
    "end": "2063358"
  },
  {
    "text": "things but it is conditioning the model to give like open-ended responses rather than a simple true false response",
    "start": "2063359",
    "end": "2070638"
  },
  {
    "text": "and so that could be problematic because that could mean that if you're so here like we do just like",
    "start": "2070639",
    "end": "2076720"
  },
  {
    "text": "only look at the logits true and false but if those were like actually very low in the ranking of all of the logits",
    "start": "2076720",
    "end": "2083118"
  },
  {
    "text": "then they might be like a lot noisier right like if their probabilities are like .01 versus like 0.005 that's very",
    "start": "2083119",
    "end": "2090158"
  },
  {
    "text": "different than if they're like 0.7 versus 0.35 and so like small amounts of noise in the model could like really",
    "start": "2090159",
    "end": "2096320"
  },
  {
    "text": "mess you up but maybe if you're like earlier on in the model at the latent representations there's less noise there",
    "start": "2096320",
    "end": "2103040"
  },
  {
    "text": "i think that's one possibility um but i would not say uh that we understand these well enough for",
    "start": "2103040",
    "end": "2109359"
  },
  {
    "start": "2109000",
    "end": "2421000"
  },
  {
    "text": "me to feel confident in that so i think the main thing i can say here is that there are at least cases where",
    "start": "2109359",
    "end": "2117440"
  },
  {
    "text": "models do poorly on the logits religion representations can help you um",
    "start": "2117440",
    "end": "2123119"
  },
  {
    "text": "we've also done some like even more recent stuff where you try like really silly things that seem like",
    "start": "2123119",
    "end": "2129760"
  },
  {
    "text": "they might also be able to help you so rather than doing something fancy like pca",
    "start": "2129760",
    "end": "2135760"
  },
  {
    "text": "you can use the fact that transformer models have are like residual networks so if you just like stop the network at",
    "start": "2135760",
    "end": "2142240"
  },
  {
    "text": "any single point you can just like fast forward through the residual connection to the logit",
    "start": "2142240",
    "end": "2147680"
  },
  {
    "text": "um and then like skip all the remaining layers and get like a prediction and so if you just like fast forward",
    "start": "2147680",
    "end": "2153839"
  },
  {
    "text": "from the middle of the network uh you also sometimes get better accuracy um so somehow like something is",
    "start": "2153839",
    "end": "2161280"
  },
  {
    "text": "something like bad is happening when you decode um and uh yeah uh question yeah",
    "start": "2161280",
    "end": "2168960"
  },
  {
    "text": "i think i'm noticing here like with the orange that like you're actually also just getting overall",
    "start": "2168960",
    "end": "2174480"
  },
  {
    "text": "improvements of accuracy like there are more points in the top right corner yeah is that a different effect than",
    "start": "2174480",
    "end": "2182000"
  },
  {
    "text": "what you've just been explaining so i think it's plausible that it's a",
    "start": "2182000",
    "end": "2187760"
  },
  {
    "text": "similar effect in that uh it's generally",
    "start": "2187760",
    "end": "2193200"
  },
  {
    "text": "understood in nlp that the even for like regular zero shot the particular choice",
    "start": "2193200",
    "end": "2198560"
  },
  {
    "text": "of prompt uh can affect the accuracy and so we were you know we had like 10",
    "start": "2198560",
    "end": "2205040"
  },
  {
    "text": "different prompts uh based on like kind of standard prop formats that people tend to use but on a given task like",
    "start": "2205040",
    "end": "2212160"
  },
  {
    "text": "some prompts do better and some do worse and so generally",
    "start": "2212160",
    "end": "2218240"
  },
  {
    "text": "i think what's happening is that we end up doing you know about as well as like the best prompt",
    "start": "2218240",
    "end": "2223760"
  },
  {
    "text": "um even when we have worse prompts and so i think basically what's happening is like sometimes your prompt is bad even",
    "start": "2223760",
    "end": "2229520"
  },
  {
    "text": "for regular zero shot and you're recovering from that that's my hypothesis at least um i i wouldn't say",
    "start": "2229520",
    "end": "2235200"
  },
  {
    "text": "i'm like sure of that but um but you do see like lower standard deviation across problems for instance",
    "start": "2235200",
    "end": "2240480"
  },
  {
    "text": "as some evidence of that um yes in the back uh yeah so you said in the beginning like you called miss",
    "start": "2240480",
    "end": "2245920"
  },
  {
    "text": "representation one like in in the middle it's right but then at the end it gets it wrong yeah and then you said okay so",
    "start": "2245920",
    "end": "2252800"
  },
  {
    "text": "let's embed the x is true versus x's false statements in the model to get like h of x i",
    "start": "2252800",
    "end": "2258720"
  },
  {
    "text": "when you say h like are you referring to like a mid layer where it gets it right or like the end layer with where it",
    "start": "2258720",
    "end": "2264240"
  },
  {
    "text": "ultimately gets it wrong okay yeah so h is h is going to be some middle layer of the network",
    "start": "2264240",
    "end": "2269520"
  },
  {
    "text": "so i guess you could pick any layer you want so in principle you could pick the layer right before the logits",
    "start": "2269520",
    "end": "2274640"
  },
  {
    "text": "um that actually can also work although it is the case that like the",
    "start": "2274640",
    "end": "2280000"
  },
  {
    "text": "middle is often the best place i guess basically transformers have this",
    "start": "2280000",
    "end": "2285040"
  },
  {
    "text": "like encoder stage and decoder stage so halfway through is like the final encoding layer before you start decoding",
    "start": "2285040",
    "end": "2291760"
  },
  {
    "text": "um at least for encoder decoder transformers and uh in those cases that",
    "start": "2291760",
    "end": "2297119"
  },
  {
    "text": "is often the best or the close to the best uh layer to pick um so you should think of",
    "start": "2297119",
    "end": "2303200"
  },
  {
    "text": "it as the middle although i think you would get accuracy gains even if you pick stuff close to the end as well",
    "start": "2303200",
    "end": "2308560"
  },
  {
    "text": "so if like in turn like in the middle layer that's all right so like in the end it gets it wrong would you consider",
    "start": "2308560",
    "end": "2314400"
  },
  {
    "text": "it like the the model knows the fact or the the fact that it cannot produce the right output means like ultimately",
    "start": "2314400",
    "end": "2320000"
  },
  {
    "text": "actually doesn't know the fact i think i i mean i guess it's always like a little dicey to use the word no but like if",
    "start": "2320000",
    "end": "2327359"
  },
  {
    "text": "you'll permit me to use it anyways then i would say that it knows the fact in that case",
    "start": "2327359",
    "end": "2333040"
  },
  {
    "text": "um intuitively like why do i think that well you might think that when it's kind of",
    "start": "2333040",
    "end": "2338640"
  },
  {
    "text": "doing the encoding it's just kind of computing everything it knows about uh about the input and like embedding",
    "start": "2338640",
    "end": "2345920"
  },
  {
    "text": "that in its representations across various dimensions and then in the decoding case it's",
    "start": "2345920",
    "end": "2351359"
  },
  {
    "text": "deciding which parts of that it should attend to to actually produce a word",
    "start": "2351359",
    "end": "2356800"
  },
  {
    "text": "um and so you know if you have that basic model then",
    "start": "2356800",
    "end": "2362240"
  },
  {
    "text": "the idea is that it kind of figured something out but it decided not to attend to that for whatever reason um",
    "start": "2362240",
    "end": "2368480"
  },
  {
    "text": "there's actually some nice papers by david bow like there's this paper called uh rome rank one model editing that",
    "start": "2368480",
    "end": "2376880"
  },
  {
    "text": "does some interesting kind of ablations along these lines that is i'd say is",
    "start": "2376880",
    "end": "2382079"
  },
  {
    "text": "also suggestive of this interpretation yeah cool um other questions",
    "start": "2382079",
    "end": "2391040"
  },
  {
    "text": "okay good okay so we're on to part three uh so this is going to be a bit of a change of pace so",
    "start": "2393839",
    "end": "2399520"
  },
  {
    "text": "you know all of that was kind of training uh i guess well first small rl models",
    "start": "2399520",
    "end": "2404800"
  },
  {
    "text": "and then giant language models um lots of pi torch and and other",
    "start": "2404800",
    "end": "2411599"
  },
  {
    "text": "code uh this doesn't have much code at all this is this is just about trying to",
    "start": "2411599",
    "end": "2417440"
  },
  {
    "text": "forecast uh what ml you know will look like five ten",
    "start": "2417440",
    "end": "2422960"
  },
  {
    "start": "2421000",
    "end": "2541000"
  },
  {
    "text": "years from now so can we somehow predict future developments in ml that's kind of the motivation uh so things you might want to predict",
    "start": "2422960",
    "end": "2430240"
  },
  {
    "text": "might be like progress on key benchmarks like what is state of the art on say image not going to be well okay that",
    "start": "2430240",
    "end": "2436079"
  },
  {
    "text": "one's kind of boring but maybe like state-of-the-art on on sort of like video recognition tasks since we're not",
    "start": "2436079",
    "end": "2441760"
  },
  {
    "text": "currently good at video or say state of the art on like adversarially robust vision tasks",
    "start": "2441760",
    "end": "2448000"
  },
  {
    "text": "um so you might care about things like that we might also care about like geopolitical or other competitive",
    "start": "2448000",
    "end": "2453280"
  },
  {
    "text": "concerns like you know is it still going to be the case that there's basically like a small number of giant companies",
    "start": "2453280",
    "end": "2459119"
  },
  {
    "text": "that are training all the big models or will it be much more distributed you know you might care about that",
    "start": "2459119",
    "end": "2464560"
  },
  {
    "text": "because that could create concerns around like release like maybe you need more coordination around like release norms",
    "start": "2464560",
    "end": "2470720"
  },
  {
    "text": "um uh you might also care about like geopolitics like are china and the us",
    "start": "2470720",
    "end": "2476079"
  },
  {
    "text": "gonna get into some like race around ai um and so you know just like",
    "start": "2476079",
    "end": "2481680"
  },
  {
    "text": "understand all of these things i guess the approach we took was not to re-wheel ourselves but to take advantage of this",
    "start": "2481680",
    "end": "2489040"
  },
  {
    "text": "kind of other area of of what's called human judgmental forecasting so this",
    "start": "2489040",
    "end": "2494640"
  },
  {
    "text": "isn't building like some statistical model and extrapolating it's actually uh you can",
    "start": "2494640",
    "end": "2501920"
  },
  {
    "text": "find humans who have a consistently good track record of predicting future events",
    "start": "2501920",
    "end": "2507359"
  },
  {
    "text": "and then just like well in this case like hire them to answer questions um now the problem is",
    "start": "2507359",
    "end": "2513839"
  },
  {
    "text": "that most of the things they've been asked about are not necessarily questions about ai so there's an",
    "start": "2513839",
    "end": "2519359"
  },
  {
    "text": "important question of like are they actually calibrated on ai tasks and so uh i think that's actually not yet clear",
    "start": "2519359",
    "end": "2526880"
  },
  {
    "text": "and so part of what we wanted to do here is start to create a track record for these forecasters they have a very",
    "start": "2526880",
    "end": "2532400"
  },
  {
    "text": "strong track record for say like traditional geopolitics so we wanted to see if that translates",
    "start": "2532400",
    "end": "2538560"
  },
  {
    "text": "to to ai so we created a forecasting competition",
    "start": "2538560",
    "end": "2544800"
  },
  {
    "start": "2541000",
    "end": "2675000"
  },
  {
    "text": "are we treating these humans as random black box decision makers or do we know how they are good um",
    "start": "2544800",
    "end": "2551680"
  },
  {
    "text": "so when i did this i treated them as random black box decision makers um sometime after that i actually taught",
    "start": "2551680",
    "end": "2559359"
  },
  {
    "text": "a course at berkeley on forecast in and so i interviewed some of them to understand",
    "start": "2559359",
    "end": "2565440"
  },
  {
    "text": "how they think and so i would say i no longer think of them as black boxes i think there's like",
    "start": "2565440",
    "end": "2571599"
  },
  {
    "text": "various tasks they perform and in fact some of those tasks i think can be",
    "start": "2571599",
    "end": "2577040"
  },
  {
    "text": "automated or partially automated by ml and in fact at least one of the forecasters uses ml as part of their",
    "start": "2577040",
    "end": "2584079"
  },
  {
    "text": "pipeline uh because they i think they said they would like ideally consume like",
    "start": "2584079",
    "end": "2590640"
  },
  {
    "text": "like a thousand news articles every day uh but they can't read that many they can't even read the titles of that",
    "start": "2590640",
    "end": "2596880"
  },
  {
    "text": "many so they have like nlp systems that like read all the articles for them and then tell them which ones they should",
    "start": "2596880",
    "end": "2602240"
  },
  {
    "text": "actually read um uh but yeah i i i i'll i'll get i'll get",
    "start": "2602240",
    "end": "2608000"
  },
  {
    "text": "into that a bit uh at the end um",
    "start": "2608000",
    "end": "2613200"
  },
  {
    "text": "i i think that's actually also very exciting areas like can we actually help these humans uh do better and i think we",
    "start": "2613200",
    "end": "2619200"
  },
  {
    "text": "should be able to um but anyway so this competition where for me at this point it was a black box uh we had six",
    "start": "2619200",
    "end": "2625599"
  },
  {
    "text": "questions we had a five thousand dollar incentive uh prize per question",
    "start": "2625599",
    "end": "2631040"
  },
  {
    "text": "and we recruited uh forecasters via this platform called hypermind to predict the results of that question",
    "start": "2631040",
    "end": "2637920"
  },
  {
    "text": "in uh along four different time frames so in 2022 2023 2024 2025 it resolves in",
    "start": "2637920",
    "end": "2644800"
  },
  {
    "text": "june uh i think june 1st so uh actually essentially the 2022 one is",
    "start": "2644800",
    "end": "2651599"
  },
  {
    "text": "about to resolve and so we can basically uh score them and then hypermine kind of aggregates",
    "start": "2651599",
    "end": "2658160"
  },
  {
    "text": "the results awaited by some sort of like prior track record although i think a lot of the forecasters probably didn't",
    "start": "2658160",
    "end": "2664160"
  },
  {
    "text": "have like a super long track record so you know take this as like a somewhat noisy aggregation um but here were the",
    "start": "2664160",
    "end": "2671040"
  },
  {
    "text": "six questions so sorry that the text a bit small",
    "start": "2671040",
    "end": "2676160"
  },
  {
    "start": "2675000",
    "end": "2796000"
  },
  {
    "text": "the first two are more about competition so the first is how many times larger or",
    "start": "2676160",
    "end": "2681440"
  },
  {
    "text": "smaller with the largest chinese machine learning experience view computed the largest u.s machine learning experiment as measured by the amount of computing",
    "start": "2681440",
    "end": "2688160"
  },
  {
    "text": "power brought to bear which i think is operationalized as like training flops",
    "start": "2688160",
    "end": "2693200"
  },
  {
    "text": "um the next one is how much computing power will have been used for the largest machine learning experiment that is not",
    "start": "2693200",
    "end": "2700079"
  },
  {
    "text": "from china and not from open ai google deep mind alphabet facebook or microsoft so that's trying to get at like will new",
    "start": "2700079",
    "end": "2707440"
  },
  {
    "text": "organizations uh compete with these established ones and then the last four are different",
    "start": "2707440",
    "end": "2713599"
  },
  {
    "text": "interesting benchmarks so state-of-the-art accuracy on the massive multitask",
    "start": "2713599",
    "end": "2719359"
  },
  {
    "text": "language understanding data set which is basically high school college and professional multiple choice exams",
    "start": "2719359",
    "end": "2726160"
  },
  {
    "text": "including things like the multiple choice part of the bar exam for instance this uh okay state-of-the-art adverse",
    "start": "2726160",
    "end": "2733200"
  },
  {
    "text": "serial image classification accuracy on c510 state-of-the-art accuracy on something",
    "start": "2733200",
    "end": "2738319"
  },
  {
    "text": "something v2 which for those of you haven't heard of it is like the sort of standard image recognition benchmark",
    "start": "2738319",
    "end": "2744560"
  },
  {
    "text": "and then state-of-the-art accuracy on the math data set which is a data set of high school math competition",
    "start": "2744560",
    "end": "2751520"
  },
  {
    "text": "questions um so those were the six",
    "start": "2751520",
    "end": "2756960"
  },
  {
    "text": "predictions i'm going to focus on these four because i guess to me they're the most interesting uh so this was the the",
    "start": "2756960",
    "end": "2765040"
  },
  {
    "text": "median forecast uh by the forecasters right so note that i guess",
    "start": "2765040",
    "end": "2770480"
  },
  {
    "text": "you can see here forecasters give a probability distribution so this is like the aggregate probability density",
    "start": "2770480",
    "end": "2777280"
  },
  {
    "text": "in each case um it's uh there's some limitations on like exactly",
    "start": "2777280",
    "end": "2784160"
  },
  {
    "text": "what densities you can draw which is why there's some like artifacts here like this was probably",
    "start": "2784160",
    "end": "2791599"
  },
  {
    "text": "like this should probably be smoother but um but yeah here here's at least the",
    "start": "2791839",
    "end": "2797280"
  },
  {
    "start": "2796000",
    "end": "2899000"
  },
  {
    "text": "medians so i guess for most there's kind of predictions of this steady linear",
    "start": "2797280",
    "end": "2802800"
  },
  {
    "text": "progress interestingly they think video classification will be like near 90 by",
    "start": "2802800",
    "end": "2808079"
  },
  {
    "text": "2025 which would be near human level um adversarial robustness also which i",
    "start": "2808079",
    "end": "2814319"
  },
  {
    "text": "guess surprise me because i work on this and think of it as like a hard problem although i did realize then that if i",
    "start": "2814319",
    "end": "2821119"
  },
  {
    "text": "had actually just like fit a linear trend to the data so far it would have actually been like pretty close to this",
    "start": "2821119",
    "end": "2826880"
  },
  {
    "text": "line so i guess these told me things that maybe i could have already known um these two benchmarks were a lot newer",
    "start": "2826880",
    "end": "2833680"
  },
  {
    "text": "um the multiple choice exams and the competition math data set um",
    "start": "2833680",
    "end": "2839119"
  },
  {
    "text": "state of the art on math today is like six percent they're interestingly",
    "start": "2839119",
    "end": "2844880"
  },
  {
    "text": "predicting uh over 50 by 2025 which was pretty wild to me i think we gave",
    "start": "2844880",
    "end": "2851119"
  },
  {
    "text": "this to some like phd students and one of them got below 50",
    "start": "2851119",
    "end": "2856240"
  },
  {
    "text": "although to be clear most of them got like in the 90s um but it's at least like not totally trivial",
    "start": "2856240",
    "end": "2862400"
  },
  {
    "text": "um and then uh this is also at least pretty wild to me as predictions so again these are",
    "start": "2862400",
    "end": "2869280"
  },
  {
    "text": "like not things that have happened but like they're like kind of wild predictions uh the orange and blue lines",
    "start": "2869280",
    "end": "2875280"
  },
  {
    "text": "for me oh there we go okay so you don't have",
    "start": "2875280",
    "end": "2880960"
  },
  {
    "text": "any so i guess we sort of have ground truth here yes well we will in",
    "start": "2880960",
    "end": "2886400"
  },
  {
    "text": "like 15 days because it was june 1st um so something might happen before then",
    "start": "2886400",
    "end": "2893680"
  },
  {
    "text": "uh but let's just pretend today is june 1st and then let's yeah so let's look at the ground truth for 2022 okay so what",
    "start": "2893680",
    "end": "2900400"
  },
  {
    "start": "2899000",
    "end": "2994000"
  },
  {
    "text": "are the results so far so state of the art on math remains at 6.9 percent so it didn't really move",
    "start": "2900400",
    "end": "2908240"
  },
  {
    "text": "from uh from today so it's like here um state of the art on multitask is",
    "start": "2908240",
    "end": "2915599"
  },
  {
    "text": "around 68 which is like here",
    "start": "2915599",
    "end": "2921680"
  },
  {
    "text": "so it's like way like it's actually above the 2023 median um and it's actually like beyond the",
    "start": "2921680",
    "end": "2927839"
  },
  {
    "text": "90th percentile confidence interval for the forecasters in in 2022. um there are",
    "start": "2927839",
    "end": "2934400"
  },
  {
    "text": "some artifacts in the platform that make it like a bit hard to fully write the tales down but i'd say even given that",
    "start": "2934400",
    "end": "2940480"
  },
  {
    "text": "that i'd say the forecasters basically got this one wrong um in the",
    "start": "2940480",
    "end": "2946480"
  },
  {
    "text": "direction of being actually too pessimistic about progress which is interesting because i actually would have given like slower progress um",
    "start": "2946480",
    "end": "2953839"
  },
  {
    "text": "so it turns out like language like multi-task language understanding",
    "start": "2953839",
    "end": "2959359"
  },
  {
    "text": "has just progressed really really quickly uh like gopher in chinchilla just kind of like blew this out of the water",
    "start": "2959359",
    "end": "2966880"
  },
  {
    "text": "video understanding is somewhat above the forecast but within their like error bars",
    "start": "2966880",
    "end": "2972240"
  },
  {
    "text": "and then uh [Music] robustness is still a bit below",
    "start": "2972240",
    "end": "2977920"
  },
  {
    "text": "the forecast and then math is like way below although i think their error bars were like",
    "start": "2977920",
    "end": "2984079"
  },
  {
    "text": "pretty wide so i don't necessarily take this to be like a refutation of the forecasters i",
    "start": "2984079",
    "end": "2989599"
  },
  {
    "text": "would say this one is the like biggest refutation so i guess uh lessons learned i guess",
    "start": "2989599",
    "end": "2996640"
  },
  {
    "start": "2994000",
    "end": "3066000"
  },
  {
    "text": "first is like sometimes forecasts will say something different than what i as an expert thought",
    "start": "2996640",
    "end": "3002400"
  },
  {
    "text": "um one interesting thing is two of the four data sets i showed you were actually ones that our lab collected uh because",
    "start": "3002400",
    "end": "3008319"
  },
  {
    "text": "we wanted to like create something to like forecast",
    "start": "3008319",
    "end": "3013520"
  },
  {
    "text": "that was the math and the multi-test data sets so we couldn't have had those interesting forecasts if we weren't willing to do a",
    "start": "3013520",
    "end": "3020240"
  },
  {
    "text": "bunch of kind of you know dirty work ourselves a lot of forecasting platforms kind of",
    "start": "3020240",
    "end": "3025680"
  },
  {
    "text": "just take what's out there because they don't have the resources to create their own benchmarks but as ml people we can",
    "start": "3025680",
    "end": "3030800"
  },
  {
    "text": "actually any benchmark we care about we could create it you could like like some of you don't even need to pay",
    "start": "3030800",
    "end": "3036079"
  },
  {
    "text": "people money you can just have people compete for internet points and uh and still get decent results um and so i",
    "start": "3036079",
    "end": "3043280"
  },
  {
    "text": "guess the next step is to kind of you know more formally do the short-term validation of the forecasters",
    "start": "3043280",
    "end": "3048400"
  },
  {
    "text": "here was i guess uh i guess the gopher paper actually does this and then chinchilla took us to here which is this",
    "start": "3048400",
    "end": "3056240"
  },
  {
    "text": "what i mean by it was outside the 90 confidence intervals um i guess i'm pretty much out of",
    "start": "3056240",
    "end": "3062800"
  },
  {
    "text": "time so maybe i'll just skip ahead uh fro beyond some uh provocative",
    "start": "3062800",
    "end": "3070800"
  },
  {
    "start": "3066000",
    "end": "3275000"
  },
  {
    "text": "philosophical stuff i was gonna say and just say if you are interested in learning more about forecasting um i",
    "start": "3070800",
    "end": "3076880"
  },
  {
    "text": "have uh a class that i just taught this semester it's all online open access",
    "start": "3076880",
    "end": "3082160"
  },
  {
    "text": "that's that157.com and we yeah we break down like these uh forecasting strategies into kind of",
    "start": "3082160",
    "end": "3088800"
  },
  {
    "text": "constituent skills um okay i'll stop there and take questions",
    "start": "3088800",
    "end": "3093980"
  },
  {
    "text": "[Applause]",
    "start": "3093980",
    "end": "3100179"
  },
  {
    "text": "uh yes i think you said earlier they might talk about how phase transitions um might update your forecasting on ai",
    "start": "3100559",
    "end": "3106559"
  },
  {
    "text": "like for example do those discontinuous changes kind of make you have shorter timelines",
    "start": "3106559",
    "end": "3112240"
  },
  {
    "text": "so i think yeah so i guess how does it update my",
    "start": "3112240",
    "end": "3118000"
  },
  {
    "text": "beliefs so i think the main thing is that i'm less like when i think about potential safety",
    "start": "3118000",
    "end": "3124319"
  },
  {
    "text": "issues i am less indexed on current systems than i",
    "start": "3124319",
    "end": "3129359"
  },
  {
    "text": "would have been like three or four years ago so three or four years ago i would have said okay what are like like we want to",
    "start": "3129359",
    "end": "3135040"
  },
  {
    "text": "make things safe how do we do that let's see what like current systems are messing up and like fix that and i",
    "start": "3135040",
    "end": "3140880"
  },
  {
    "text": "continue to think that that's a pretty important thing to do because those are certainly problems that you know exist but i don't think you should be content",
    "start": "3140880",
    "end": "3147040"
  },
  {
    "text": "with that uh because the phase transition means that we might suddenly see new capabilities that could create",
    "start": "3147040",
    "end": "3152800"
  },
  {
    "text": "new dangers that we'd rather get ahead of rather than react to um right so maybe systems could like",
    "start": "3152800",
    "end": "3159599"
  },
  {
    "text": "tell really convincing lies even if they're not uh like even if they don't do that",
    "start": "3159599",
    "end": "3165920"
  },
  {
    "text": "naturally if they can be trained to do that then bad actors could use that and also you know maybe like if you're",
    "start": "3165920",
    "end": "3172319"
  },
  {
    "text": "training an rl model to pursue some objective where it needs to convince you of something if it can like lie to",
    "start": "3172319",
    "end": "3177599"
  },
  {
    "text": "convince you of that then you could also get like bad reward hacking from that um i don't think models can currently",
    "start": "3177599",
    "end": "3184319"
  },
  {
    "text": "tell very convincing lies so that doesn't happen today but",
    "start": "3184319",
    "end": "3189440"
  },
  {
    "text": "you know i think like just scaling models up could lead something like that to happen um",
    "start": "3189440",
    "end": "3194880"
  },
  {
    "text": "and so i guess that's maybe just one example of like where i feel like we should be kind of trying to proactively",
    "start": "3194880",
    "end": "3200160"
  },
  {
    "text": "think about problems that could occur and think about how we would solve them in advance um that's actually also one reason why i",
    "start": "3200160",
    "end": "3206559"
  },
  {
    "text": "care about truthfulness is that kind of convincing lie setting um in terms of timelines",
    "start": "3206559",
    "end": "3212319"
  },
  {
    "text": "i guess it just leads me to have a lot more uncertainty where like i'm more willing to think that something",
    "start": "3212319",
    "end": "3218720"
  },
  {
    "text": "could happen soon but also more willing to think something could like happen like really",
    "start": "3218720",
    "end": "3224079"
  },
  {
    "text": "far in the future i guess that actually that's more actually just from uh",
    "start": "3224079",
    "end": "3229359"
  },
  {
    "text": "talking to a bunch of forecasters and practicing forecasting where like any question that's kind of very complex and",
    "start": "3229359",
    "end": "3235040"
  },
  {
    "text": "far in the future i think you should just have like really wide error bars on everything um so i think my like",
    "start": "3235040",
    "end": "3242559"
  },
  {
    "text": "90 confidence interval for like when ai will reach parity with humans is like",
    "start": "3242559",
    "end": "3249200"
  },
  {
    "text": "probably like 20 28 to like 2300",
    "start": "3249200",
    "end": "3255440"
  },
  {
    "text": "just as an example of what i mean by white error bars um cool other questions",
    "start": "3257040",
    "end": "3265480"
  },
  {
    "text": "okay let's thank the speaker [Applause]",
    "start": "3267040",
    "end": "3272719"
  }
]