[
  {
    "start": "0",
    "end": "5290"
  },
  {
    "text": "All right. It should be up in a second. But you go ahead and get\nstarted on your refresh your understanding. ",
    "start": "5290",
    "end": "53390"
  },
  {
    "text": "All right. When you turn to\nsomebody near you and see if you got the\nsame answers for this. This question asks\nyou to think back",
    "start": "53390",
    "end": "58770"
  },
  {
    "text": "to what we were\nlearning about last time in terms of posteriors over\nwhat the parameters might look like for a multi-armed bandit.",
    "start": "58770",
    "end": "65530"
  },
  {
    "text": "So check in with\nsomeone nearby you and see whether you\ngot the same idea. [BACKGROUND CHATTER]",
    "start": "65530",
    "end": "72784"
  },
  {
    "start": "72785",
    "end": "135780"
  },
  {
    "text": "OK, we're going to go ahead\nand come back together and go through the answers for these. All right. So the first one\nof these are true",
    "start": "135780",
    "end": "143370"
  },
  {
    "text": "because in this\ncase, for a beta 1, 2, where we're weighed\nmore towards an arm that",
    "start": "143370",
    "end": "150330"
  },
  {
    "text": "more frequently gets something\nlike a 0 instead of a 1, then we're more likely to\nsample these three parameters.",
    "start": "150330",
    "end": "158250"
  },
  {
    "text": "The second one is also\ntrue because if you have a flat uniform over all of\nthe different arm parameters,",
    "start": "158250",
    "end": "164830"
  },
  {
    "text": "you're more likely\nto keep distribution. And the third is false,\nbecause when you have a 1,",
    "start": "164830",
    "end": "170662"
  },
  {
    "text": "1 prior, that's a uniform\nsomewhere between 0 and 1, so the true arm\nparameter could be a 0",
    "start": "170662",
    "end": "177510"
  },
  {
    "text": "or it could be a 1 or\nanything in between. And then the second one asks you\nto think about using Thompson",
    "start": "177510",
    "end": "184329"
  },
  {
    "text": "sampling to sample arms. And so the first one is true.",
    "start": "184330",
    "end": "189379"
  },
  {
    "text": "So given these priors, you could\nsample either of those values for the underlying parameter\nfor your Bernoulli variable.",
    "start": "189380",
    "end": "197319"
  },
  {
    "text": "The second one is false. So let's assume that the real\nparameter here is 0.4 and 0.6.",
    "start": "197320",
    "end": "203860"
  },
  {
    "text": "What this question is\nasking you to reflect about is that Thompson sampling is\nnot guaranteed to give you",
    "start": "203860",
    "end": "209410"
  },
  {
    "text": "an upper confidence bound. So it may instead just\nselect a parameter that",
    "start": "209410",
    "end": "215200"
  },
  {
    "text": "is consistent with your prior. And for these\nparticular sample betas,",
    "start": "215200",
    "end": "220670"
  },
  {
    "text": "it will happen to choose the\ntrue optimal arm for this round. ",
    "start": "220670",
    "end": "226440"
  },
  {
    "text": "Awesome. So I want to just-- let's see\nif I can make all the AV work. Want to briefly show\nyou this nice example.",
    "start": "226440",
    "end": "235793"
  },
  {
    "text": " Let's see if I can\nmake this go away.",
    "start": "235793",
    "end": "242060"
  },
  {
    "text": " All right. So I wanted to show you\nthis nice example of,",
    "start": "242060",
    "end": "250800"
  },
  {
    "text": "somewhere where you\nmight want exploration. So we've talked\nabout exploration so far in terms of cases like,\nyou're an online advertiser",
    "start": "250800",
    "end": "258269"
  },
  {
    "text": "and you'd like to figure out\nwhich ads work for people. It comes up in health care. I want to show you an example of\nan application which we thought",
    "start": "258269",
    "end": "265889"
  },
  {
    "text": "about in collaboration\nwith Chelsea Finn and a bunch of wonderful\ngrad students recently. So this is the assignment.",
    "start": "265890",
    "end": "272940"
  },
  {
    "text": "This is an assignment that's\nused in Stanford where students actually encode the game. So in this case,\ncompared to the settings",
    "start": "272940",
    "end": "281670"
  },
  {
    "text": "we're at where we assume\nyou have the environment and then you're\nlearning an agent to act in that\nenvironment, here,",
    "start": "281670",
    "end": "287360"
  },
  {
    "text": "students are actually\ncreating the code to make the Breakout assignment,\nso to make the game environment.",
    "start": "287360",
    "end": "293190"
  },
  {
    "text": "And this, generally, is\noften really engaging and fun for students, particularly when\nthey're learning to program.",
    "start": "293190",
    "end": "298300"
  },
  {
    "text": "Many people like computer games. So this is a really great\nopportunity for people to learn and can be really engaging.",
    "start": "298300",
    "end": "304110"
  },
  {
    "text": "And a lot of different people\nuse these type of assignments. So it's not just at Stanford,\nbut many, many other places,",
    "start": "304110",
    "end": "309740"
  },
  {
    "text": "including code.org\nand others use this assignment to try to teach\nstudents about programming.",
    "start": "309740",
    "end": "314919"
  },
  {
    "text": "Here's the problem. Even though it teaches lots of\ndifferent introductory computer",
    "start": "314920",
    "end": "320169"
  },
  {
    "text": "science concepts,\nthere's a challenge, which is, if you want\npeople to learn from writing",
    "start": "320170",
    "end": "325210"
  },
  {
    "text": "this assignment, you need\nto be able to provide them with feedback. And providing them\nwith feedback involves grading the assignments.",
    "start": "325210",
    "end": "331610"
  },
  {
    "text": "So in this case, we\nnormally have a rubric of different things\nthat the program",
    "start": "331610",
    "end": "336759"
  },
  {
    "text": "is expected to do correctly. Like, is the paddle\ndrawn correctly? Is the ball drawn correctly?",
    "start": "336760",
    "end": "342610"
  },
  {
    "text": "When you bounce, does that\nrespect the desired transition dynamics?",
    "start": "342610",
    "end": "348110"
  },
  {
    "text": "Things like that. And so normally, just like\nwhen you guys get feedback from Gradescope, someone has\nto go through and play the game",
    "start": "348110",
    "end": "354190"
  },
  {
    "text": "to do this. And so that is really\nexpensive because that",
    "start": "354190",
    "end": "359320"
  },
  {
    "text": "means people have to figure\nout, when the ball bounces here, does it actually\ndo the right thing?",
    "start": "359320",
    "end": "365278"
  },
  {
    "text": "And then you have\nto do that for each of the different rubric items. So there, for\nexample, it jittered.",
    "start": "365278",
    "end": "371110"
  },
  {
    "text": "It didn't do the right thing. So what you can think of here\nis that essentially someone is manually designing\na mental policy,",
    "start": "371110",
    "end": "377930"
  },
  {
    "text": "a grader is designing a\nmental policy in their head for how to play\nthis game in order to uncover whether the\ngame dynamics are correct.",
    "start": "377930",
    "end": "385626"
  },
  {
    "text": "And so the way we\nnormally do that right now is, each individual grader\nfigures out how to do that. And then they play this.",
    "start": "385627",
    "end": "391992"
  },
  {
    "text": "So this means that it\nwould take probably around eight minutes\nper submission. So you can't just do a\nunit test in the normal way",
    "start": "391992",
    "end": "399670"
  },
  {
    "text": "because you actually\nare trying to figure out how the game behaves\nin different scenarios where it might take\nmultiple actions to even get",
    "start": "399670",
    "end": "406690"
  },
  {
    "text": "to that scenario. So if you think about doing\neight minutes per submission,",
    "start": "406690",
    "end": "412840"
  },
  {
    "text": "if you have 300\nsubmissions in a course and there's actually\nmany, many more people than that have played\nthis game on code.org,",
    "start": "412840",
    "end": "419130"
  },
  {
    "text": "or tried to code this, that's an\nenormous amount of grading time. That's an enormous amount\nof human resource time.",
    "start": "419130",
    "end": "424388"
  },
  {
    "text": "And that means that\nsome of the people that offer this challenge to\nstudents don't grade it at all--",
    "start": "424388",
    "end": "430155"
  },
  {
    "text": "just too expensive. And so that means students\nget the opportunity of trying to do this\nexciting assignment,",
    "start": "430155",
    "end": "436270"
  },
  {
    "text": "but they don't get\nany feedback back, which can really hinder\ntheir learning process.",
    "start": "436270",
    "end": "441900"
  },
  {
    "text": "So there's a lot of things\nthat make this hard. It's a stochastic setting. There's not kind of\nsimple heuristics.",
    "start": "441900",
    "end": "449610"
  },
  {
    "text": "And there are multiple errors. So my student, Chris Pietsch--\nanother professor here--",
    "start": "449610",
    "end": "456427"
  },
  {
    "text": "and I started think\nabout this problem a few years ago of\nsaying, couldn't we design like a reinforcement\nlearning agent",
    "start": "456427",
    "end": "461580"
  },
  {
    "text": "to play this game? And what we want is that\nthis reinforcement learning agent can explore the\nparts of the domain",
    "start": "461580",
    "end": "466890"
  },
  {
    "text": "so that we can try to\nuncover how they're doing and whether the game\nis coded correctly.",
    "start": "466890",
    "end": "472479"
  },
  {
    "text": "So we did this work and then we\ndid an initial approach to this.",
    "start": "472480",
    "end": "478160"
  },
  {
    "text": "And then Evan, who\nis the key author of this, extended this to try\nto think about rubric items.",
    "start": "478160",
    "end": "483710"
  },
  {
    "text": "So the idea is that\ninstead of having humans graded, what we're\ngoing to do is we're going to replace humans\nby a machine learning agent.",
    "start": "483710",
    "end": "490630"
  },
  {
    "text": "And in particular, what\nEvan did is he built on our, and I and Chris's initial work\nand said, let's actually phrase",
    "start": "490630",
    "end": "497560"
  },
  {
    "text": "this as, think about how we\ncan use meta reinforcement learning and exploration.",
    "start": "497560",
    "end": "503780"
  },
  {
    "text": "The reason this is an\nexploration problem is because you\nwant to learn an RL",
    "start": "503780",
    "end": "508990"
  },
  {
    "text": "policy here so that\nin a new environment you can quickly use behaviors\nto grade the assignment.",
    "start": "508990",
    "end": "515890"
  },
  {
    "text": "And so that's where efficient\nexploration is coming in. So you don't want\nthis to have to take 20 minutes to try to grade it.",
    "start": "515890",
    "end": "522049"
  },
  {
    "text": "You want to, as\nquickly as possible, whether for an agent or a human,\nfigure out what strategy you",
    "start": "522049",
    "end": "527080"
  },
  {
    "text": "should use to play\nthe game in order to correctly grade whether\nthis is a good environment.",
    "start": "527080",
    "end": "532900"
  },
  {
    "text": "And so, Evan had a really\nnice NURBS paper building on our NURBS paper. These are both machine learning\ncontributions of how to--",
    "start": "532900",
    "end": "540412"
  },
  {
    "text": "there's a series\nof papers, there's a first paper on how we\ncould do this at all. There's a second\npaper by Evan who",
    "start": "540412",
    "end": "546100"
  },
  {
    "text": "is looking at trying to do\nexplicit exploration, really fast exploration, and\nthen we joined forces",
    "start": "546100",
    "end": "552842"
  },
  {
    "text": "to think about how we\ncould do fast exploration in this setting. And then more recently,\nwe published a paper",
    "start": "552842",
    "end": "558160"
  },
  {
    "text": "showing that this could\nactually significantly reduce grading time and actually\nimprove accuracy when",
    "start": "558160",
    "end": "563560"
  },
  {
    "text": "you combine this with humans. So, I just give\nthis as an example to illustrate another exciting\nexploration case where",
    "start": "563560",
    "end": "572079"
  },
  {
    "text": "if you can design agents\nthat can learn quickly, and can quickly\nexplore an environment,",
    "start": "572080",
    "end": "577580"
  },
  {
    "text": "it can end up being\nreally helpful. And we'll come back to DREAM and\nthis idea of meta exploration",
    "start": "577580",
    "end": "583390"
  },
  {
    "text": "later in the\ncourse, later today. So today will be our final\nlecture on fast and efficient",
    "start": "583390",
    "end": "589600"
  },
  {
    "text": "reinforcement learning. And then next week,\nwe're going to start talking about Monte\nCarlo tree search, which was one of the key\nideas behind AlphaGo.",
    "start": "589600",
    "end": "597670"
  },
  {
    "text": "I hope that homework\n3 is going well. Feel free to reach out\nto us with any questions. And feel free to come\nto our office hours.",
    "start": "597670",
    "end": "605910"
  },
  {
    "text": "All right. So just to remind ourselves\nabout where we are, we've been thinking about\ndifferent frameworks for evaluating the\ncorrectness of algorithms",
    "start": "605910",
    "end": "613840"
  },
  {
    "text": "and how efficient they are at\nlearning and making decisions. And so far, we have\nfocused mostly on bandits,",
    "start": "613840",
    "end": "620840"
  },
  {
    "text": "which is this much simpler\nversion of reinforcement learning where the\ndecisions we make don't affect the next state.",
    "start": "620840",
    "end": "627580"
  },
  {
    "text": "So we saw how to do that\nfor both standard bandits",
    "start": "627580",
    "end": "632830"
  },
  {
    "text": "and Bayesian bandits. And today we're going to start\nto lift all those ideas up to Markov decision processes.",
    "start": "632830",
    "end": "639220"
  },
  {
    "text": "So we did that by\ndesign because a lot of the ideas around optimism\nunder uncertainty or posterior",
    "start": "639220",
    "end": "646120"
  },
  {
    "text": "sampling or Thompson\nsampling can be lifted up to the tabular\nMarkov decision process case.",
    "start": "646120",
    "end": "651560"
  },
  {
    "text": "And then all of\nthese ideas also then can be extrapolated\nup with some care to the function\napproximation setting.",
    "start": "651560",
    "end": "658459"
  },
  {
    "text": "So that's where we're\ngoing to go today. The main approaches for trying\nto act efficiently in Markov",
    "start": "658460",
    "end": "665920"
  },
  {
    "text": "decision processes\nand we're going to start by focusing on the\ntabular setting, will again be optimism under\nuncertainty, and probability",
    "start": "665920",
    "end": "673990"
  },
  {
    "text": "matching or Thompson sampling. And we're going to see ideas of\nhow to do that in this setting.",
    "start": "673990",
    "end": "680380"
  },
  {
    "text": "OK. So here is one of-- it's not the oldest algorithm\nto do probably efficient",
    "start": "680380",
    "end": "688329"
  },
  {
    "text": "exploration in tabular\nMarkov decision processes, but it's one of the\nquintessential ones.",
    "start": "688330",
    "end": "694610"
  },
  {
    "text": "And I think it illustrates a\nlot of the really nice ideas. So this is a lot. Let's just step through it. So the idea in this\ncase is that we're",
    "start": "694610",
    "end": "701020"
  },
  {
    "text": "going to be making decisions\nin a tabular Markov decision process. We're going to be taking\nactions with respect",
    "start": "701020",
    "end": "708190"
  },
  {
    "text": "to some specific Q-function that\nI'm going to define in a second. We'll observe the\nreward in the state.",
    "start": "708190",
    "end": "713630"
  },
  {
    "text": "We're going to update a\nwhole bunch of things, update that special\nQ tilde and repeat.",
    "start": "713630",
    "end": "718930"
  },
  {
    "text": "The key thing that we're\ngoing to be trying to do is similar to what we saw for\nthe upper confidence bound",
    "start": "718930",
    "end": "723970"
  },
  {
    "text": "algorithms. We're going to think\nabout how do we construct an upper confidence\nbound on the Q-function.",
    "start": "723970",
    "end": "729880"
  },
  {
    "text": "So that's going to be the key-- ",
    "start": "729880",
    "end": "735000"
  },
  {
    "text": "we're going to be doing-- this\nis an upper confidence bound algorithm. ",
    "start": "735000",
    "end": "742550"
  },
  {
    "text": "So this is going to, again,\nuse the idea of optimism under uncertainty. And we're going to think about\nhow do we bring this to MDPs.",
    "start": "742550",
    "end": "749530"
  },
  {
    "text": " So the key idea in this case\nis what we would like to do",
    "start": "749530",
    "end": "755720"
  },
  {
    "text": "is we'd like to construct\nan optimistic upper bound on the Q-function. This is a model-based\napproach, which",
    "start": "755720",
    "end": "762585"
  },
  {
    "text": "means the way we're\ngoing to do that is we're going to try to\nconstruct optimistic estimates of the reward function,\nand optimistic estimates",
    "start": "762585",
    "end": "769970"
  },
  {
    "text": "of the dynamics model. It shouldn't be\nimmediately obvious what it means to be\noptimistic with respect",
    "start": "769970",
    "end": "775460"
  },
  {
    "text": "to the dynamics model, and we'll\ngo through that in a minute. In practice, what we're\ngoing to do is the following.",
    "start": "775460",
    "end": "782440"
  },
  {
    "text": "The reward is the\neasiest to start with. So in the reward\ncase, we're going",
    "start": "782440",
    "end": "787670"
  },
  {
    "text": "to maintain counts\nof how many times we've taken an action\nin a particular state. We're also going to maintain\ncounts of how many times",
    "start": "787670",
    "end": "794470"
  },
  {
    "text": "we've started in a\nstate, taken an action, and went to a\nparticular next state. And we've seen these ideas\nbefore for tabular Markov",
    "start": "794470",
    "end": "801320"
  },
  {
    "text": "decision processes. We've used them for certainty\nequivalent planning back in the first couple\nof weeks of class.",
    "start": "801320",
    "end": "808160"
  },
  {
    "text": "So the reward model\nis perhaps closest to what we've seen\nfor the bandit before. For the reward model,\nwhat we're going",
    "start": "808160",
    "end": "813680"
  },
  {
    "text": "to do is we're going to compute\nthe empirical average over in this state and this action,\nwhat's our average reward",
    "start": "813680",
    "end": "819830"
  },
  {
    "text": "we've seen so far. And then, we're going\nto think of there being an upper\nconfidence bound to that.",
    "start": "819830",
    "end": "828308"
  },
  {
    "text": "What we're also going\nto do in this case is we're going to maintain\nan empirical estimate of the dynamics model.",
    "start": "828308",
    "end": "835640"
  },
  {
    "text": "Now, when we do\nthis, we're going to do the normal Bellman\nequation, except for we're",
    "start": "835640",
    "end": "842600"
  },
  {
    "text": "going to include a bonus. So this part should\nlook familiar to what",
    "start": "842600",
    "end": "849730"
  },
  {
    "text": "we've seen for Hoeffding, which\nis when we are going to compute",
    "start": "849730",
    "end": "855610"
  },
  {
    "text": "a Bellman backup, instead\nof uncertainty equivalence, we would just use the empirical\nestimate of the reward function",
    "start": "855610",
    "end": "863589"
  },
  {
    "text": "and the empirical estimate\nof the dynamics model. Instead of doing\nthat, we're going to include this bonus term.",
    "start": "863590",
    "end": "870430"
  },
  {
    "text": "This is just bonus term. And there's a few\ndifferent ways to do",
    "start": "870430",
    "end": "878010"
  },
  {
    "text": "model-based interval estimation. I'm picking one here that\njust uses a bonus term, but I'll talk about\nsome other ones.",
    "start": "878010",
    "end": "884470"
  },
  {
    "text": "There's a number of variants. So what this is saying is when\nI do my Bellman backup of what",
    "start": "884470",
    "end": "891300"
  },
  {
    "text": "is the expected discounted sum\nof rewards from starting state S and taking action\nA, this is going",
    "start": "891300",
    "end": "896520"
  },
  {
    "text": "to try to approximate Q star. I'm going to plug in\nmy empirical estimate of the reward. I'm going to use my empirical\nestimate of the dynamics model,",
    "start": "896520",
    "end": "904000"
  },
  {
    "text": "and then I'm going\nto add in a bonus. And if I have not taken that\nstate and action very much,",
    "start": "904000",
    "end": "910029"
  },
  {
    "text": "that bonus is going\nto be really large because those counts of the\nnumber of states and actions is going to be really small.",
    "start": "910030",
    "end": "915990"
  },
  {
    "text": "So this will be large\nif the counts are small.",
    "start": "915990",
    "end": "921410"
  },
  {
    "text": " The key difference compared to\nwhat we've seen with bandits",
    "start": "921410",
    "end": "926920"
  },
  {
    "text": "before is this is\na Bellman backup. So we will then repeat\nthis many, many times.",
    "start": "926920",
    "end": "933700"
  },
  {
    "text": "So you do this for all\nstates and actions, and then you back up and\nyou do this many times. So intuitively,\nwhat's happening here",
    "start": "933700",
    "end": "940510"
  },
  {
    "text": "is this is like pretending\nthe expected discounted sum of rewards you'd get. If you start in a\nparticular state",
    "start": "940510",
    "end": "946600"
  },
  {
    "text": "and take a particular\naction is much higher if you have not visited that\nstate and action very much.",
    "start": "946600",
    "end": "952720"
  },
  {
    "text": "So that's where this\noptimism comes in. You end up adding in\nthis bonus term here.",
    "start": "952720",
    "end": "958540"
  },
  {
    "text": "And this bonus term\nwill be really large. So beta is defined up here. This bonus term, this\nis 1 over 1 minus gamma.",
    "start": "958540",
    "end": "966130"
  },
  {
    "text": "So if you imagine that all\nrewards are scaled between 0 and 1, and gamma\nis really small,",
    "start": "966130",
    "end": "972769"
  },
  {
    "text": "you can think of\nthat as being like H times that special term\ndivided by the square root",
    "start": "972770",
    "end": "978910"
  },
  {
    "text": "of the number of times you've\nbeen in that state in action. So what that means is\nwhen you do these repeated",
    "start": "978910",
    "end": "984139"
  },
  {
    "text": "Bellman backups, it\nwill drive your policy to visit parts of the state\nand action which you have not",
    "start": "984140",
    "end": "989720"
  },
  {
    "text": "visited much. OK? Because those are\nthe parts where you're going to have these\nreally large overestimates,",
    "start": "989720",
    "end": "995430"
  },
  {
    "text": "probably overestimates--\noptimistic estimates, I should say. You have these\noptimistic estimates of how good the value\ncould be in those states.",
    "start": "995430",
    "end": "1002540"
  },
  {
    "text": "And the reason this is important\nis because it might be-- so this is going\nto work when you're",
    "start": "1002540",
    "end": "1007700"
  },
  {
    "text": "doing a series of\nepisodes, or you're working in the same MDP\nfor a long time, what will happen is that your MDE\nwill explore in your MDP,",
    "start": "1007700",
    "end": "1015950"
  },
  {
    "text": "and it will drive you to cover\nthe state and action space, if you think that it\nmight possibly have",
    "start": "1015950",
    "end": "1021950"
  },
  {
    "text": "good rewards in those places. So this will drive exploration. And by doing these\nrepeated backups here,",
    "start": "1021950",
    "end": "1028709"
  },
  {
    "text": "you're propagating your\noptimism under uncertainty backwards so that you\ndevelop this policy to drive",
    "start": "1028710",
    "end": "1034429"
  },
  {
    "text": "you that way.  So this is one of the\nquintessential algorithms",
    "start": "1034430",
    "end": "1041780"
  },
  {
    "text": "for doing tabular optimism under\nuncertainty based planning.",
    "start": "1041780",
    "end": "1048199"
  },
  {
    "text": "It is also a PAC algorithm. So we talked about\nPAC last time. PAC, it means it's Probably\nApproximately Correct.",
    "start": "1048200",
    "end": "1055720"
  },
  {
    "text": "I'll just write that out again,\njust to remind ourselves. Probably approximately correct.",
    "start": "1055720",
    "end": "1067410"
  },
  {
    "text": "But now, we're\ngoing to talk about, in particular, Markov\ndecision processes. So we talked about how\nan algorithm is probably",
    "start": "1067410",
    "end": "1072929"
  },
  {
    "text": "approximately correct\nif most of the time it makes a decision that\nis close to optimal and only makes mistakes on a\npolynomial number of times.",
    "start": "1072930",
    "end": "1081510"
  },
  {
    "text": "So we talked about\nthat last time. We saw that you don't\nhave to guarantee this. You could make mistakes forever.",
    "start": "1081510",
    "end": "1087160"
  },
  {
    "text": "Like, if you're acting\nrandomly, that's not-- you would continue\nto make this forever.",
    "start": "1087160",
    "end": "1093570"
  },
  {
    "text": "MBIE is a PAC algorithm. So what it says is\nthat let's let script",
    "start": "1093570",
    "end": "1102779"
  },
  {
    "text": "a t denote MBIE-EB's\npolicy at time step t, and s t denote the\nstate at time t.",
    "start": "1102780",
    "end": "1108360"
  },
  {
    "text": "With high probability,\nthe value of the action the algorithm takes is at least\nthe value of the optimal action",
    "start": "1108360",
    "end": "1115380"
  },
  {
    "text": "for that state minus epsilon. And it's true on all but\na finite number of steps",
    "start": "1115380",
    "end": "1121110"
  },
  {
    "text": "with high probability. So this is the number of steps.",
    "start": "1121110",
    "end": "1127669"
  },
  {
    "text": "And the important\nthing here is this is a polynomial in the\nsize of the state space,",
    "start": "1127670",
    "end": "1133290"
  },
  {
    "text": "the action space, 1 over\nepsilon, and 1 over 1 minus gamma.",
    "start": "1133290",
    "end": "1138960"
  },
  {
    "text": "Now, I always encourage\nmy research students to plug in for bounds because\ntheoretical bounds are",
    "start": "1138960",
    "end": "1145680"
  },
  {
    "text": "beautiful, but it's nice\nto know whether or not they are at all related to practice. So, for example, in this case,\nyou might imagine, let's say,",
    "start": "1145680",
    "end": "1153299"
  },
  {
    "text": "we have s equals\n10 and a equals 10.",
    "start": "1153300",
    "end": "1159422"
  },
  {
    "text": "And you've said\nepsilon is equal to 0.1 and gamma is equal to 0.9.",
    "start": "1159422",
    "end": "1164990"
  },
  {
    "text": "All right. So let's just work out\nwhat that would be. That would be roughly 10\nto the 3 times 10 to the 9,",
    "start": "1164990",
    "end": "1174880"
  },
  {
    "text": "or 10 to the 12. So that's a lot. So what that would\nsay is that we",
    "start": "1174880",
    "end": "1181170"
  },
  {
    "text": "are sure by using this algorithm\nthat we will only make mistakes on this 10 state MDP,\n10 to the 12 time steps.",
    "start": "1181170",
    "end": "1190620"
  },
  {
    "text": "Now, I don't know\nabout you, but I would hope that a 10\nstate GridWorld MDP,",
    "start": "1190620",
    "end": "1195780"
  },
  {
    "text": "that we could learn to act\nsubstantially faster than that. So I use it to highlight\nthat these bounds,",
    "start": "1195780",
    "end": "1202760"
  },
  {
    "text": "while this might officially\nsay this is a PAC algorithm, they can be pretty conservative\nin how many mistakes",
    "start": "1202760",
    "end": "1209500"
  },
  {
    "text": "you might make. Now, in practice,\noften this optimism under uncertainty algorithm\ncan work very well.",
    "start": "1209500",
    "end": "1214820"
  },
  {
    "text": "It doesn't say you will make\nthis number of mistakes, it just is an upper bound on it. But it's good to plug\nthese things in just",
    "start": "1214820",
    "end": "1220720"
  },
  {
    "text": "to see how tight or\nnot you think it is relative to real performance.",
    "start": "1220720",
    "end": "1226390"
  },
  {
    "text": "All right. So this is a PAC\nalgorithm and the paper goes through an\ninteresting proof of it",
    "start": "1226390",
    "end": "1231580"
  },
  {
    "text": "to show the\ndifferent components. But one of the key ideas is\nsomething called the Simulation",
    "start": "1231580",
    "end": "1237010"
  },
  {
    "text": "Lemma. And that I'm going to go\nthrough at least briefly because the Simulation Lemma\nis one of the many core ideas",
    "start": "1237010",
    "end": "1245230"
  },
  {
    "text": "when we think about doing\nefficient exploration. And the key idea for\nthe Simulation Lemma",
    "start": "1245230",
    "end": "1252309"
  },
  {
    "text": "is the idea that we can relate\nthe accuracy of our models to the accuracy of our\nlearned Q-function.",
    "start": "1252310",
    "end": "1258970"
  },
  {
    "text": "OK, so that's the key idea. It's going to say,\nif we have bounded--",
    "start": "1258970",
    "end": "1264295"
  },
  {
    "text": "yeah, I guess I'll\njust leave it there. So if we just ensure that we\nhave good predictive models,",
    "start": "1264295",
    "end": "1274200"
  },
  {
    "text": "we can relate our error\nand our predictive models back to our value function. So let's do that\nat least, sketch.",
    "start": "1274200",
    "end": "1282620"
  },
  {
    "text": "So this is going to be for\ndo for tabular settings.",
    "start": "1282620",
    "end": "1287980"
  },
  {
    "text": "So we're going to\nassume that we're back in a finite set of states,\nfinite set of actions.",
    "start": "1287980",
    "end": "1294932"
  },
  {
    "text": "So this is one proof of\nthe Simulation Lemma. We're going to assume that\nwe have pi as a fixed policy.",
    "start": "1294932",
    "end": "1302140"
  },
  {
    "text": " And we are going\nto assume that we",
    "start": "1302140",
    "end": "1310820"
  },
  {
    "text": "have a max norm on the reward.",
    "start": "1310820",
    "end": "1317809"
  },
  {
    "text": "So we're going to assume--\nif you remember back, let me do R1 minus R2-- we're\ngoing to assume that we have",
    "start": "1317810",
    "end": "1324490"
  },
  {
    "text": "two different MDPs. So MDP 1, and MDP 2.",
    "start": "1324490",
    "end": "1330640"
  },
  {
    "text": "And these might have slightly\ndifferent reward functions and slightly different\ndynamics models.",
    "start": "1330640",
    "end": "1335740"
  },
  {
    "text": "So remember that if we\nhave the infinity norm, you can express this\nas this is going",
    "start": "1335740",
    "end": "1341882"
  },
  {
    "text": "to be the place where the\ntwo reward functions differ the most over our\nfinite state space. So if one of them\ngives the rewards of 1,",
    "start": "1341882",
    "end": "1348870"
  },
  {
    "text": "2, 7, 3 and the other one\ngives the rewards of 2, 6, 1, 7 we would figure\nout for which state",
    "start": "1348870",
    "end": "1354050"
  },
  {
    "text": "do the two rewards\ndiffer the most. Let's assume that's\nupper bounded by alpha. We're also going to assume\nthat we have an upper bound",
    "start": "1354050",
    "end": "1361190"
  },
  {
    "text": "on the dynamics model. So we're going to assume\nthat T of s prime given",
    "start": "1361190",
    "end": "1367640"
  },
  {
    "text": "s a minus T 2 of S prime\ngiven s a is also bounded.",
    "start": "1367640",
    "end": "1374550"
  },
  {
    "text": "And I'm going to assume\nthat's bounded by beta. So that means from the point of\nview of your predictive models,",
    "start": "1374550",
    "end": "1380400"
  },
  {
    "text": "the two MDPs differ. You can bound the amount. And we're going to\nshow if that's true.",
    "start": "1380400",
    "end": "1385500"
  },
  {
    "text": "Then we also are\ngoing to only differ in their estimated Q-functions\nfor a particular policy by a bounded amount.",
    "start": "1385500",
    "end": "1391730"
  },
  {
    "text": "So what we have is\nwe want to compare what is the Q value of\nunder model 1 for state",
    "start": "1391730",
    "end": "1399960"
  },
  {
    "text": "s, a and then following\npolicy pi versus Q pi 2, s, a.",
    "start": "1399960",
    "end": "1405779"
  },
  {
    "text": "And the reason this is\ngoing to be important is because in general,\nwhat we're going to have is the R1 and R2 are going to\ncorrespond to our uncertainty.",
    "start": "1405780",
    "end": "1413799"
  },
  {
    "text": "So if you think back to\nthe Hoeffding inequality, as I told you about, we talked\nabout how our empirical estimate",
    "start": "1413800",
    "end": "1419309"
  },
  {
    "text": "could differ from the true\nestimate by a bounded amount. So that should make you\nthink about this part.",
    "start": "1419310",
    "end": "1424720"
  },
  {
    "text": "R1 could be our empirical\nestimate of the reward and R2 could be the\ntrue unknown one. And Hoeffding can give\nyou an upper bound",
    "start": "1424720",
    "end": "1431310"
  },
  {
    "text": "on what that alpha is. And similarly, we can get a\nbound on the dynamics model error as well.",
    "start": "1431310",
    "end": "1437730"
  },
  {
    "text": "So the idea will be to say, if\nyou end up plugging in, say, like an empirical\nestimate of the reward",
    "start": "1437730",
    "end": "1444059"
  },
  {
    "text": "model and the dynamics\nmodel, how far away could your estimate\nof the Q-function be from if you actually\nknew the true reward model",
    "start": "1444060",
    "end": "1451290"
  },
  {
    "text": "and the true dynamics model? So that's why we're doing this. OK, so this is the difference.",
    "start": "1451290",
    "end": "1458259"
  },
  {
    "text": "Let's just write down\nwhat that will look like. So this is going to be\nR1 of s, a plus gamma,",
    "start": "1458260",
    "end": "1465550"
  },
  {
    "text": "sum over s prime, T 1, s prime\ngiven s, a, V 1 pi of S prime,",
    "start": "1465550",
    "end": "1476440"
  },
  {
    "text": "minus R2 s, a plus gamma sum\nover s prime T2, s prime.",
    "start": "1476440",
    "end": "1484399"
  },
  {
    "text": " I've just used the\ndefinition of the Q-function",
    "start": "1484400",
    "end": "1490269"
  },
  {
    "text": "there to write out what is the\ndifference between the two Q values. All right.",
    "start": "1490270",
    "end": "1495530"
  },
  {
    "text": "We're going to upper\nbound this as follows. We're going to just use the\ntriangle inequality first.",
    "start": "1495530",
    "end": "1504385"
  },
  {
    "text": "So we're just going to\nsay, this is less than or equal to R1 minus R2.",
    "start": "1504385",
    "end": "1510050"
  },
  {
    "text": "So I use the triangle inequality\nplus gamma times the difference",
    "start": "1510050",
    "end": "1519490"
  },
  {
    "text": "in the second terms. ",
    "start": "1519490",
    "end": "1528540"
  },
  {
    "text": "That was in parentheses. ",
    "start": "1528540",
    "end": "1544630"
  },
  {
    "text": "OK, I'll just use my\ntriangle inequality. I split the two terms.",
    "start": "1544630",
    "end": "1550035"
  },
  {
    "text": "Remember this? We've already said is going to\nbe less than or equal to alpha. ",
    "start": "1550035",
    "end": "1556110"
  },
  {
    "text": "Because we've upper\nbounded our R. So then, we have to think\nabout the second term. Then we're going to do\nsomething that we often",
    "start": "1556110",
    "end": "1561547"
  },
  {
    "text": "do in reinforcement learning,\nwhich is we add and subtract 0, or we add 0. And we're going to do that\nby trying to relate between--",
    "start": "1561547",
    "end": "1568668"
  },
  {
    "text": "right now, we have\nthe dynamics model of one thing and the value-- the dynamics model of\nmodel 1 and the value",
    "start": "1568668",
    "end": "1574290"
  },
  {
    "text": "function of model 1. And so now, we want to\nhave some in between terms so we can directly think about\nthe difference in the value",
    "start": "1574290",
    "end": "1579856"
  },
  {
    "text": "functions under one\nparticular dynamics model and the difference of the\ndynamics model separately. OK, what we're going to do in\nthis case is we're going to say,",
    "start": "1579857",
    "end": "1587549"
  },
  {
    "text": "this is less than or\nequal to alpha plus gamma, and then we're just going to\nadd and subtract some terms.",
    "start": "1587550",
    "end": "1594292"
  },
  {
    "text": "So sum over s prime. And I'm just going\nto use shorthand here so that I can\nfit everything.",
    "start": "1594292",
    "end": "1600896"
  },
  {
    "text": " So I'm just going to\nintroduce add and subtract 0.",
    "start": "1600896",
    "end": "1606620"
  },
  {
    "start": "1606620",
    "end": "1612550"
  },
  {
    "text": "Careful with my-- make\nsure that's clear.",
    "start": "1612550",
    "end": "1618050"
  },
  {
    "start": "1618050",
    "end": "1630770"
  },
  {
    "text": "OK, so I just introduced,\nI added a new term that, this intersection term, and\nI added and subtracted it.",
    "start": "1630770",
    "end": "1638240"
  },
  {
    "text": "And the reason\nthat's helpful is now I can just think of\nterms where they only differ in the dynamics\nmodel or terms that they only differ\nin the value function.",
    "start": "1638240",
    "end": "1646512"
  },
  {
    "text": "So this is going to be less than\nor equal to alpha plus gamma",
    "start": "1646512",
    "end": "1651649"
  },
  {
    "text": "times sum over s prime T1 of s\nprime times the absolute value",
    "start": "1651650",
    "end": "1659930"
  },
  {
    "text": "of V1 pi.  prime S V2, [INAUDIBLE] plus\ngamma times sum over s prime,",
    "start": "1659930",
    "end": "1680575"
  },
  {
    "text": "T1 of s prime minus\nT2 of s prime. ",
    "start": "1680575",
    "end": "1691980"
  },
  {
    "text": "All right. So what have I done there? I've just rearranged the terms. I just moved these two here,\nand then I move those two there.",
    "start": "1691980",
    "end": "1697960"
  },
  {
    "text": "And I'm starting to apply\nmy absolute values a lot, just repeatedly do the\ntriangle inequality.",
    "start": "1697960",
    "end": "1704910"
  },
  {
    "text": "All right, so what is this? This part looks a\nlot like this thing.",
    "start": "1704910",
    "end": "1710420"
  },
  {
    "text": " So that's going to be\nlike a recursive term.",
    "start": "1710420",
    "end": "1716690"
  },
  {
    "text": "So we can turn this\npart into the following. This is going to be--",
    "start": "1716690",
    "end": "1723390"
  },
  {
    "text": "this part here will be less than\nor equal to alpha plus gamma. Let's call this\ndifference delta.",
    "start": "1723390",
    "end": "1730289"
  },
  {
    "text": "I'm going to call that delta. Then if I do that, I have sum\nover s prime does T1 of s prime.",
    "start": "1730290",
    "end": "1737245"
  },
  {
    "text": "So this could just be like a\nmax difference in your value functions. And then the second\nterm I have, I'm",
    "start": "1737245",
    "end": "1743429"
  },
  {
    "text": "going to use the fact that my\nvalue function is upper bounded.",
    "start": "1743430",
    "end": "1748470"
  },
  {
    "text": "So here, our max divided\nby 1 minus gamma.",
    "start": "1748470",
    "end": "1754039"
  },
  {
    "text": "So if you get the maximum reward\nat every single time step, and your discount\nfactor is gamma,",
    "start": "1754040",
    "end": "1759120"
  },
  {
    "text": "this is an upper bound on Vmax. So that allows me to\ntake this term out,",
    "start": "1759120",
    "end": "1764970"
  },
  {
    "text": "and then that just leaves\nme with my difference in my dynamics model.",
    "start": "1764970",
    "end": "1770300"
  },
  {
    "text": "So I have this.  And that was this term,\nhere comes that, OK?",
    "start": "1770300",
    "end": "1782010"
  },
  {
    "text": "All right. So that's what I have\nin this case now. And so, this has to hold\nfor all states and actions.",
    "start": "1782010",
    "end": "1790540"
  },
  {
    "text": "So here, the delta\nthat I've defined. Delta is kind of the worst\ncase error over any of these.",
    "start": "1790540",
    "end": "1799184"
  },
  {
    "text": "So over any states, what's\nthe maximum difference between the value functions? So that also has to\nhold on the Q side.",
    "start": "1799184",
    "end": "1807270"
  },
  {
    "text": "So we get this, we\nget delta is less than or equal to alpha plus gamma,\ndelta plus gamma Vmax beta.",
    "start": "1807270",
    "end": "1819403"
  },
  {
    "text": "And I'm going to\nsubtract that, so now I have 1 minus gamma\ntimes delta is less than or equal to\nalpha plus gamma Vmax beta,",
    "start": "1819403",
    "end": "1828690"
  },
  {
    "text": "or delta is less than or equal\nto 1 over gamma alpha plus gamma",
    "start": "1828690",
    "end": "1834970"
  },
  {
    "text": "Vmax beta.  OK, what are we just shown?",
    "start": "1834970",
    "end": "1840640"
  },
  {
    "text": "We've said, the worst case\nerror in the value function for the same policy between\none model and the other model",
    "start": "1840640",
    "end": "1847330"
  },
  {
    "text": "is upper bounded by\n1 over 1 minus gamma times the error in\nyour reward model, plus gamma times your maximum\nvalue, times the error",
    "start": "1847330",
    "end": "1855550"
  },
  {
    "text": "in your dynamics model. So that is one version, at\nleast, of the Simulation Lemma.",
    "start": "1855550",
    "end": "1860750"
  },
  {
    "text": "It comes up in lots of\ndifferent other areas too. People use it for a lot more\nadvanced, complicated settings.",
    "start": "1860750",
    "end": "1866059"
  },
  {
    "text": "But the critical\nidea here is to say, if you can bound your\nerror in the dynamics model or the error in\nyour value function,",
    "start": "1866060",
    "end": "1872470"
  },
  {
    "text": "and the error in your\nreward function, that also means that your Q-functions\ncan't be too different.",
    "start": "1872470",
    "end": "1877700"
  },
  {
    "text": "So that's the main\nimportant point of that. And so, this idea,\nin general, is a--",
    "start": "1877700",
    "end": "1883557"
  },
  {
    "text": "excuse me-- a helpful one\nbecause it means as we explore and we learn these\npredictive models better,",
    "start": "1883557",
    "end": "1888570"
  },
  {
    "text": "we can be sure that our\nvalue function is also simultaneously getting\nbetter and better over time and getting more accurate.",
    "start": "1888570",
    "end": "1895010"
  },
  {
    "text": "And in the proofs\nof PAC algorithms that's often used to\nsay, you can't infinitely",
    "start": "1895010",
    "end": "1901400"
  },
  {
    "text": "learn in particular\nstates and actions and not end up with a\nvalue function that is getting more and more accurate.",
    "start": "1901400",
    "end": "1907380"
  },
  {
    "text": " All right. So now, I'll pause there in\ncase anybody has any questions",
    "start": "1907380",
    "end": "1914087"
  },
  {
    "text": "before we move on to Bayesian\nMarkov decision processes. Yeah?",
    "start": "1914087",
    "end": "1919160"
  },
  {
    "text": "So here, we define the\ndifference in the value function as delta,\nbut why we could also",
    "start": "1919160",
    "end": "1926720"
  },
  {
    "text": "represent the difference in\nQ-function as [INAUDIBLE]? Yeah, because this is an upper\nbound to this term and this",
    "start": "1926720",
    "end": "1936350"
  },
  {
    "text": "has to hold for all of\nthe states and actions we could ever be at. And that also has to hold\nfor this, for any state.",
    "start": "1936350",
    "end": "1942460"
  },
  {
    "text": "So you could make\nA here be pi of s. ",
    "start": "1942460",
    "end": "1948246"
  },
  {
    "text": "Yeah. Should there be a factor\nof the number of states",
    "start": "1948246",
    "end": "1954910"
  },
  {
    "text": "because we're summing over\nall the states, or is that-- because just from\nthe error bound,",
    "start": "1954910",
    "end": "1961210"
  },
  {
    "text": "I thought that's\njust for one state. For example, the dynamics,\njust for one state,",
    "start": "1961210",
    "end": "1968600"
  },
  {
    "text": "it's less than or equal to beta. Then the proof, it's summing\nover a bunch of states.",
    "start": "1968600",
    "end": "1975340"
  },
  {
    "text": "Good question. So what happens here is\nthis is just assuming that given you have a bound\non this, this tells you,",
    "start": "1975340",
    "end": "1985149"
  },
  {
    "text": "this is for every single\ns prime, you have a bound. What will end up coming--",
    "start": "1985150",
    "end": "1991670"
  },
  {
    "text": "normally, where the number\nof states will come in is when you start to\nthink about how much data you need to achieve this.",
    "start": "1991670",
    "end": "1999159"
  },
  {
    "text": "And you want this to hold for\nevery single state action pair. And normally, that's\nwhere you will end up",
    "start": "1999160",
    "end": "2005490"
  },
  {
    "text": "getting a dependence\non [INAUDIBLE] data in order to get sufficiently\nsmall confidence intervals. And with your union\nbound, you need",
    "start": "2005490",
    "end": "2011280"
  },
  {
    "text": "to make sure all of\nthese bounds hold. In terms of this part, the\nstate space doesn't appear.",
    "start": "2011280",
    "end": "2017309"
  },
  {
    "text": "But this is just for\nthe Simulation Lemma. It doesn't then tell you all\nthe way to how many samples you need to achieve this.",
    "start": "2017310",
    "end": "2024158"
  },
  {
    "text": "OK, that's another\npart of the proof. You could probably\nimagine already, given what you know\nabout Hoeffding, that you could imagine\nhaving some way to compute",
    "start": "2024158",
    "end": "2031270"
  },
  {
    "text": "how many samples you\nneed to get alpha sufficiently small in\nthe dynamics model, kind of just a similar idea.",
    "start": "2031270",
    "end": "2038522"
  },
  {
    "text": "And you can do this for\nother parametric models too, like Gaussians, et cetera. Anyone else have any\nquestions on this part",
    "start": "2038522",
    "end": "2044170"
  },
  {
    "text": "before we go on to Bayesian? [COUGHS] ",
    "start": "2044170",
    "end": "2051730"
  },
  {
    "text": "All right, let's\nstep onto Bayesian. So in all of these\ncases, we weren't using any notion of priors,\nor any of the things",
    "start": "2051730",
    "end": "2061840"
  },
  {
    "text": "that we saw last time. So now, we're going\nto think about how we can lift some of the\nideas we saw from last time",
    "start": "2061840",
    "end": "2067480"
  },
  {
    "text": "to think about this for\nMarkov decision processes. So just as a refresher,\nremember in the other way",
    "start": "2067480",
    "end": "2073677"
  },
  {
    "text": "we can think about\nthis, a common way to think about trying to\ndo efficient exploration, is to imagine that we have some\nprior knowledge over how good we",
    "start": "2073677",
    "end": "2080837"
  },
  {
    "text": "think different states\nand actions might be or how we think the\ndynamics might work. And then what we're\ngoing to do is try to use that information\nto figure out how to act.",
    "start": "2080838",
    "end": "2088719"
  },
  {
    "text": "And we saw Thompson sampling\nas being one method that was an efficient way to\ntry to make decisions",
    "start": "2088719",
    "end": "2094480"
  },
  {
    "text": "when we have these priors\nand these posteriors. And now, we're going to think\nabout lifting these ideas",
    "start": "2094480",
    "end": "2100869"
  },
  {
    "text": "to the sequential case. So what we saw before is that\nwe'd have these priors over",
    "start": "2100870",
    "end": "2106270"
  },
  {
    "text": "the model parameters-- like in\nthis case, the reward models-- and if they were conjugate,\n[COUGHS] excuse me,",
    "start": "2106270",
    "end": "2113990"
  },
  {
    "text": "then after we would\nactually observe a reward, we had this nice closed form\nexpression for the betas. So we could think\nof these as just",
    "start": "2113990",
    "end": "2120070"
  },
  {
    "text": "being the number of successes\nand the number of failures. And I talked about but\ndidn't actually illustrate",
    "start": "2120070",
    "end": "2126250"
  },
  {
    "text": "that you can do this for\nother sorts of things like Gaussians, et cetera. All right, so you\nmight think this",
    "start": "2126250",
    "end": "2131650"
  },
  {
    "text": "should work clearly for the\nreward part of a Markov decision process.",
    "start": "2131650",
    "end": "2136940"
  },
  {
    "text": "Can we do this in general? So this is what we\ndid just, again, to remind ourselves\nthat Thompson",
    "start": "2136940",
    "end": "2142455"
  },
  {
    "text": "sampling for multi-armed bandits\ninvolves maintaining this prior. We would sample\nfrom it, meaning we",
    "start": "2142455",
    "end": "2149319"
  },
  {
    "text": "would get a particular set\nof values for our coin flips, like what you saw before. We would then act optimally\nwith respect to those,",
    "start": "2149320",
    "end": "2157000"
  },
  {
    "text": "observe reward, and\nupdate our posterior. ",
    "start": "2157000",
    "end": "2162170"
  },
  {
    "text": "So now, what we're going to\ndo is a very similar thing, but we're going to maintain\npriors over Markov decision",
    "start": "2162170",
    "end": "2168860"
  },
  {
    "text": "process models. So we could have a reward\nmodel in this-- right now, we're going to again start\nwith the tabular case.",
    "start": "2168860",
    "end": "2175630"
  },
  {
    "start": "2175630",
    "end": "2183053"
  },
  {
    "text": "So we're going to start\nwith the tabular case. There's a finite set\nof states and actions. So in this case,\nyou could imagine",
    "start": "2183053",
    "end": "2188217"
  },
  {
    "text": "maintaining a\ndifferent reward model for every single state and\naction and being able to sample",
    "start": "2188217",
    "end": "2194380"
  },
  {
    "text": "from it. So you could sample\nlike a parameter for every single one of those.",
    "start": "2194380",
    "end": "2199580"
  },
  {
    "text": "And we're going to see how we\ncan use that to actually do something very\nsimilar to Thompson",
    "start": "2199580",
    "end": "2205119"
  },
  {
    "text": "sampling for the\nsequential process case. OK.",
    "start": "2205120",
    "end": "2210865"
  },
  {
    "text": "So the idea now is\nthat we're going to maintain a prior over\nall of the dynamics models",
    "start": "2210865",
    "end": "2216040"
  },
  {
    "text": "and all of the reward models. We will sample from that. Now, if you remember\nwhat I just showed you,",
    "start": "2216040",
    "end": "2222370"
  },
  {
    "text": "in the case of\nbandits, once we did the sampling of\nthe parameters, it was really easy to\nfigure out a decision.",
    "start": "2222370",
    "end": "2227590"
  },
  {
    "text": "Because like in the case\nof the Bernoulli bandits, as soon as you know that this\ncoin flip is going to give you",
    "start": "2227590",
    "end": "2233680"
  },
  {
    "text": "one with higher probability\nthan this other one, it tells you how to act. For a Markov decision\nprocess, it's more complicated",
    "start": "2233680",
    "end": "2239835"
  },
  {
    "text": "because as soon as you see\nthe dynamics and reward model, you don't know how to act yet. So you actually have to\nsolve a planning problem.",
    "start": "2239835",
    "end": "2246550"
  },
  {
    "text": "So it's like you sample a Markov\ndecision process, once you're given that Markov\ndecision process, then you have to do planning,\nlike value iteration,",
    "start": "2246550",
    "end": "2254770"
  },
  {
    "text": "or something like that to\nactually get your Q star. Once you get your\nQ star, then you",
    "start": "2254770",
    "end": "2262030"
  },
  {
    "text": "can select the optimal action\ngiven that computed Q star.",
    "start": "2262030",
    "end": "2267750"
  },
  {
    "text": "So computationally, it can\ninvolve a lot more work than what we saw\nin the bandit case.",
    "start": "2267750",
    "end": "2273230"
  },
  {
    "text": "Then the next question\nyou might have is, how do we do this sampling? ",
    "start": "2273230",
    "end": "2280220"
  },
  {
    "text": "So, this is the PSRL algorithm. It was invented by Ian\nOsband, Dan Russo, and Ben Van",
    "start": "2280220",
    "end": "2285930"
  },
  {
    "text": "Roy, who's here at Stanford. And these guys were here at\nStanford when they invented it. The idea is as follows--\nand I'll talk about sampling",
    "start": "2285930",
    "end": "2292800"
  },
  {
    "text": "the dynamics model in a second-- there's going to be\na series of episodes. At the very start of an\nepisode, given your prior,",
    "start": "2292800",
    "end": "2300580"
  },
  {
    "text": "your current posterior, you are\ngoing to, for every single state action pair, sample a dynamics\nmodel and sample a reward model.",
    "start": "2300580",
    "end": "2307601"
  },
  {
    "text": "Given that, you're\ngoing to compute Q star for your sampled MDP.",
    "start": "2307602",
    "end": "2313010"
  },
  {
    "text": "Once you have that\nsampled MDP, you're going to act according to that\npolicy for the entire episode.",
    "start": "2313010",
    "end": "2318630"
  },
  {
    "text": "So this computes a Q star\nfor the entire episode. You're then [INAUDIBLE]\nfor t equals 1 to h,",
    "start": "2318630",
    "end": "2324390"
  },
  {
    "text": "you're going to assume\nyour episodes are finite. You're going to act\naccording to your Q star. You observe your reward\nin your next state,",
    "start": "2324390",
    "end": "2330382"
  },
  {
    "text": "you're going to repeat. At the end of the\nwhole episode, you're",
    "start": "2330382",
    "end": "2335600"
  },
  {
    "text": "going to take all of the\ndata that you just got, and you're going to\nupdate your posterior.",
    "start": "2335600",
    "end": "2341060"
  },
  {
    "text": "So for the reward\nmodel, it can be very similar to what we saw\nlast time, where you just update your counts.",
    "start": "2341060",
    "end": "2347810"
  },
  {
    "text": "For the dynamics model,\nit may not be clear what you would do in that case.",
    "start": "2347810",
    "end": "2353539"
  },
  {
    "text": "So in this case, what we would\noften probably choose to do-- ",
    "start": "2353540",
    "end": "2362500"
  },
  {
    "text": "all right, let's write up here. So what we would often do\nis reduce a Dirichlet model. ",
    "start": "2362500",
    "end": "2371440"
  },
  {
    "text": "A Dirichlet model is a\nconjugate prior for multinomial. ",
    "start": "2371440",
    "end": "2378770"
  },
  {
    "text": "OK, multinomials is what we\ncan use for our normal dynamics model here because\non multinomial",
    "start": "2378770",
    "end": "2384410"
  },
  {
    "text": "allows us to express what\nis the probability of going to any of the next states, given\nthis current state and action.",
    "start": "2384410",
    "end": "2390530"
  },
  {
    "text": "So in general, we would\nhave one multinomial for each state and action pair. We are now in the\nBayesian setting.",
    "start": "2390530",
    "end": "2397410"
  },
  {
    "text": "And so now, so we would\nhave one per s, a pair.",
    "start": "2397410",
    "end": "2406280"
  },
  {
    "text": "And this specifies our\nprobability distribution over all the next states.",
    "start": "2406280",
    "end": "2411860"
  },
  {
    "text": "That specifies p over s prime\ngiven s and a for all s prime.",
    "start": "2411860",
    "end": "2420270"
  },
  {
    "text": "And it has to sum up to 1. That's the multinomial part. The part where\nwe're being Bayesian",
    "start": "2420270",
    "end": "2425869"
  },
  {
    "text": "is we're assuming we don't know\nwhat all these parameters are. And so, we have a\nprior over them.",
    "start": "2425870",
    "end": "2432440"
  },
  {
    "text": "And the Dirichlet is\na conjugate prior, which means that if we\nstart with a Dirichlet",
    "start": "2432440",
    "end": "2437630"
  },
  {
    "text": "over multinomial parameters,\nwe observe something. So let's say we're interested in\nunderstanding what happens when",
    "start": "2437630",
    "end": "2444180"
  },
  {
    "text": "we're in state 1 and\nwe take action 1, and we observe that we\ngo to s 3 seven times.",
    "start": "2444180",
    "end": "2450990"
  },
  {
    "text": "And we observe, we go\nto s 7, three times. Well, I'll do two times.",
    "start": "2450990",
    "end": "2457745"
  },
  {
    "text": " What that means is that at\nthe end of that episode,",
    "start": "2457745",
    "end": "2464220"
  },
  {
    "text": "we would use that data\nto change our Dirichlet distribution over those\nmultinomial parameters.",
    "start": "2464220",
    "end": "2470970"
  },
  {
    "text": "Very similar to what we saw\nfor the beta distribution. And I don't expect-- I mean, it's an\ninteresting thing to do,",
    "start": "2470970",
    "end": "2478150"
  },
  {
    "text": "but I don't expect you\nto do that in this class. Some of you might want to\nfor part of your projects. But the key idea here\nis that it is conjugate.",
    "start": "2478150",
    "end": "2485680"
  },
  {
    "text": "So it means that the\nposteriors you get are in the same\nfamily as your priors. And so, you can use this\nto sample multinomials.",
    "start": "2485680",
    "end": "2493000"
  },
  {
    "text": "Essentially, it's just\nsampling dynamics models. So we do in this case.",
    "start": "2493000",
    "end": "2498960"
  },
  {
    "text": "And we do this over\nand over again. And the really key\nthings to notice here compared to what we were seeing\nbefore is that we have to sample",
    "start": "2498960",
    "end": "2509460"
  },
  {
    "text": "this entire MDP and we have\nto compute its optimal value before we act.",
    "start": "2509460",
    "end": "2516150"
  },
  {
    "text": "So we do all of this computation\nbefore the start of an episode.",
    "start": "2516150",
    "end": "2521619"
  },
  {
    "text": "And you might-- yeah? I'm a bit confused about\nthis sampling the MDP,",
    "start": "2521620",
    "end": "2528480"
  },
  {
    "text": "but the sampling dynamics\nand reward models part. Oh, this is just\nexplaining that. OK.",
    "start": "2528480",
    "end": "2533900"
  },
  {
    "text": "This is like a comment. You sample the MDP. What this means is just for\neach of the state and actions.",
    "start": "2533900",
    "end": "2540390"
  },
  {
    "text": "Yeah. Good clarification question. To sample an MDP, what\nI mean is that you're going to define the MDP.",
    "start": "2540390",
    "end": "2547190"
  },
  {
    "text": "So that means we can completely\nspecify an MDP, given a known state and action space, and a\ndiscount factor by specifying",
    "start": "2547190",
    "end": "2555207"
  },
  {
    "text": "a dynamics model for every\nsingle state and action pair, and specifying\nthe rewards model. And then we'll have to\ncompute the optimal action.",
    "start": "2555207",
    "end": "2563750"
  },
  {
    "text": "Now, one thing that\nyou might wonder about is, is it important or\nnecessary that we do",
    "start": "2563750",
    "end": "2571880"
  },
  {
    "text": "all of this once per episode? So when we talked\nabout Bayesian bandits,",
    "start": "2571880",
    "end": "2579440"
  },
  {
    "text": "after every single observation,\nwe updated our posterior. So we would try buddy\ntaping the toes,",
    "start": "2579440",
    "end": "2587040"
  },
  {
    "text": "and we'd see that that\nhelps someone recover, and then we would\nupdate our prior. This is a bit different.",
    "start": "2587040",
    "end": "2592500"
  },
  {
    "text": "We are only doing\nthis every h steps. Now, you might think maybe\nthat's computational.",
    "start": "2592500",
    "end": "2599589"
  },
  {
    "text": "You might think that that's\nbeing done for another reason. But it's something that's\nan interesting thing",
    "start": "2599590",
    "end": "2606400"
  },
  {
    "text": "to think about. Let me see if I have\nthis on the next slide. Yeah. So let's do a check\nyour understanding and then I'll give you\ntalk a little bit more",
    "start": "2606400",
    "end": "2611785"
  },
  {
    "text": "about why this is done in PSRL. So this asks you to\nthink a little bit",
    "start": "2611785",
    "end": "2617200"
  },
  {
    "text": "about doing strategic\nexploration and MDPs and in Thompson sampling in\nthe algorithm I just showed. ",
    "start": "2617200",
    "end": "2630489"
  },
  {
    "text": "[AUDIO OUT] ",
    "start": "2630489",
    "end": "2676300"
  },
  {
    "text": "All right. I want you compare your\nanswers with someone near you. ",
    "start": "2676300",
    "end": "2683580"
  },
  {
    "text": "[AUDIO OUT] ",
    "start": "2683580",
    "end": "2742670"
  },
  {
    "text": "OK, thank you-- yeah. So now it should be back on. What I was saying is that in\nMaria Dimakopoulou's work,",
    "start": "2742670",
    "end": "2750390"
  },
  {
    "text": "she was thinking about\nconcurrent reinforcement learning, which is something\nwe've also thought about. And for this much more\nrealistic setting,",
    "start": "2750390",
    "end": "2757290"
  },
  {
    "text": "the idea is whether you might\nneed to coordinate exploration and how frequently you update.",
    "start": "2757290",
    "end": "2763700"
  },
  {
    "text": "Now, one of the challenges\nof this setting, even before you get into\nconcurrent reinforcement",
    "start": "2763700",
    "end": "2770660"
  },
  {
    "text": "learning, is that if you update\nyour prior a lot within a task,",
    "start": "2770660",
    "end": "2775740"
  },
  {
    "text": "like within a single\nepisode, you're essentially sampling different\nMDPs within the same episode.",
    "start": "2775740",
    "end": "2781160"
  },
  {
    "text": "The reason that can\nbe bad is that now is going to totally\nchange your behavior, and there may be some cases\nwhere you essentially thrash.",
    "start": "2781160",
    "end": "2789559"
  },
  {
    "text": "Let me give an example. So one of the canonical\nhard Markov decision",
    "start": "2789560",
    "end": "2796369"
  },
  {
    "text": "processes that people\ntalk about is a chain. It's really just an\nillustrative one.",
    "start": "2796370",
    "end": "2802690"
  },
  {
    "text": " And there are lots of different\nslight variants of chains.",
    "start": "2802690",
    "end": "2808885"
  },
  {
    "text": " And the idea is that you\nmight have something--",
    "start": "2808885",
    "end": "2816260"
  },
  {
    "text": "I've shown stuff that's similar\nto this before where on one side or on the other side--",
    "start": "2816260",
    "end": "2821560"
  },
  {
    "text": "oh, it's not reconnecting\njust in the back there, you need to reconnect.",
    "start": "2821560",
    "end": "2826910"
  },
  {
    "text": "That you would have high reward\non one or the other sides. What you could\nimagine in this case,",
    "start": "2826910",
    "end": "2832680"
  },
  {
    "text": "if you were thinking about it\nbeing like a Bayesian bandit, is that some of the times, it\nmight pick a Markov decision",
    "start": "2832680",
    "end": "2837920"
  },
  {
    "text": "process where this is the\ngood, the best state, and some of the time, it might pick\na Markov decision process--",
    "start": "2837920",
    "end": "2843965"
  },
  {
    "text": "oh it's still not\nshowing on there. Thanks.",
    "start": "2843965",
    "end": "2849869"
  },
  {
    "text": "Hopefully that will come up. And some of the time it\nmight pick one that is here.",
    "start": "2849870",
    "end": "2855040"
  },
  {
    "text": "So if you start off acting,\nlet's say that you first sampled an MDP where this is the best\nstate, you do your planning",
    "start": "2855040",
    "end": "2863570"
  },
  {
    "text": "and then your agent is going\nto start going this way. OK? Let's say you\nobserve that there's",
    "start": "2863570",
    "end": "2868610"
  },
  {
    "text": "some zero reward here and your\nThompson sampling updates. And now, it says, hey,\nthis is the best state",
    "start": "2868610",
    "end": "2876452"
  },
  {
    "text": "because you just have some\nprior over the model parameters. And so your agent turns\naround and it's like, oh, I shouldn't go this way,\nI should go this way.",
    "start": "2876452",
    "end": "2884127"
  },
  {
    "text": "And then as you're doing that,\nit's getting more rewards and it's updating its posterior. And so then, it samples again\nand it's like, oh, this is good.",
    "start": "2884127",
    "end": "2890755"
  },
  {
    "text": " And so, it can lead to this\nkind of thrashing behavior",
    "start": "2890755",
    "end": "2897829"
  },
  {
    "text": "because it's sampling\na new Markov decision process each time. And so your agent can end\nup toggling back and forth",
    "start": "2897830",
    "end": "2904970"
  },
  {
    "text": "between its ideas over\nwhich MDP it's in. So it's for this\nreason that often you",
    "start": "2904970",
    "end": "2911750"
  },
  {
    "text": "will want to essentially commit\nto the Markov decision process you're in for the whole time.",
    "start": "2911750",
    "end": "2917645"
  },
  {
    "text": "You don't always\nhave to do this, but that's one of the reasons\nwhy this can be helpful. This commitment is also in\nthe 2013 NURBS paper that--",
    "start": "2917645",
    "end": "2927110"
  },
  {
    "text": "well, not-- the algorithm\nthat we saw earlier, right? They both commit? ",
    "start": "2927110",
    "end": "2932420"
  },
  {
    "text": "Yes. So far, sorry-- This is\njust exactly the same as the PSRL algorithm.",
    "start": "2932420",
    "end": "2937533"
  },
  {
    "text": "I'm about to tell you\nabout the seed sampling. But yes, this is\njust in the 2013. So yeah, exactly. This is the in PSRL itself.",
    "start": "2937533",
    "end": "2945030"
  },
  {
    "text": "It commits. And this is one of\nthe reasons for that. And so, in Maria's\nwork, she discusses some of the important\nbenefits of it.",
    "start": "2945030",
    "end": "2951108"
  },
  {
    "text": "And then, she thinks about\nhow would you actually maybe try to couple and\ncoordinate exploration",
    "start": "2951108",
    "end": "2956119"
  },
  {
    "text": "if you have many\nagents that are going through the same\nenvironment at once. And it's in some ways, it\nrelates to this idea, too.",
    "start": "2956120",
    "end": "2964520"
  },
  {
    "text": "You might want everybody\nto commit to exploring different parts of the space. Because if you have many\nagents in the same domain,",
    "start": "2964520",
    "end": "2970710"
  },
  {
    "text": "you might want to\nsay, you're going to think that the\nbest reward is here. You're going to think\nthe best reward is here. Go explore, and then we'll unify\nin our posterior afterwards.",
    "start": "2970710",
    "end": "2978650"
  },
  {
    "text": "So she has a nice\ndemonstration of that. And then she extended it\nto the deep learning case",
    "start": "2978650",
    "end": "2984180"
  },
  {
    "text": "shortly afterwards. But maybe if I can play that. [AUDIO OUT]",
    "start": "2984180",
    "end": "2991692"
  },
  {
    "text": "[VIDEO PLAYBACK] ",
    "start": "2991692",
    "end": "3122430"
  },
  {
    "text": "OK, so eventually, it happens. But then you can get\ntwo concurrent UCRL where in this case,\nyou can start to--",
    "start": "3122430",
    "end": "3131010"
  },
  {
    "text": "if you don't do something\nsmart, again, this can end up being\nnot very effective. And let me just see if I can\nskip ahead to the last part.",
    "start": "3131010",
    "end": "3139930"
  },
  {
    "text": "Seed sampling. OK, good. [VIDEO PLAYBACK] [MUSIC PLAYING] [END PLAYBACK]",
    "start": "3139930",
    "end": "3145410"
  },
  {
    "text": "OK, so seed sampling\nin her case is what they're doing\nwhen they essentially do concurrent\nreinforcement learning.",
    "start": "3145410",
    "end": "3151295"
  },
  {
    "text": "And you might have\neven missed it because that part\nis really fast. OK, so I'll move it to this,\njust so I can talk over it",
    "start": "3151295",
    "end": "3157230"
  },
  {
    "text": "at the same time. So this is seed sampling,\nwhich is what their idea was. And this just talks again\nabout doing strategic,",
    "start": "3157230",
    "end": "3164369"
  },
  {
    "text": "coordinated sampling. So you can see in this case,\nwe're leveraging the fact that you've got\nconcurrent agents that",
    "start": "3164370",
    "end": "3169853"
  },
  {
    "text": "are exploring the environment,\nthey're committing to it, but they're committing to it in\na way that they coordinate that.",
    "start": "3169853",
    "end": "3175090"
  },
  {
    "text": "So that you don't get\nall of the agents. So here, by 324,\nall of the agents have shared information\nabout where the cheese is,",
    "start": "3175090",
    "end": "3181140"
  },
  {
    "text": "and everyone's solved. All right. So that just illustrates\nwhy you both need",
    "start": "3181140",
    "end": "3188160"
  },
  {
    "text": "this committing to a particular\nexploration strategy. And then if you're in\nthe case where you also",
    "start": "3188160",
    "end": "3193740"
  },
  {
    "text": "have concurrent agents-- which\nis very realistic-- that having this additional coordination\nis really helpful.",
    "start": "3193740",
    "end": "3199580"
  },
  {
    "text": "Now, I think one of\nthe interesting things to note there is that this is a\nnice place where there's some--",
    "start": "3199580",
    "end": "3206700"
  },
  {
    "text": "is it connecting? Hopefully it'll\nconnect in a second. There's an\ninteresting disconnect",
    "start": "3206700",
    "end": "3211770"
  },
  {
    "text": "between theory and experiment. It's still not? Maybe there's a problem\nwith a connector.",
    "start": "3211770",
    "end": "3217090"
  },
  {
    "text": "We'll try to get that\nfixed for next week. There's a disconnect\nbetween theory and practice",
    "start": "3217090",
    "end": "3222930"
  },
  {
    "text": "because theoretically, you don't\nneed to do this exploration. So we have a paper\nfrom 2015 showing",
    "start": "3222930",
    "end": "3228840"
  },
  {
    "text": "that if you don't do\ncoordinated exploration, it's still totally sufficient. You can still get basically\nalmost a linear speed up.",
    "start": "3228840",
    "end": "3236470"
  },
  {
    "text": "Oh good, finally came. Yay! All right. So that covers how you can\ndo Bayesian exploration",
    "start": "3236470",
    "end": "3245590"
  },
  {
    "text": "and optimism under uncertainty\nin the tabular Markov decision process case.",
    "start": "3245590",
    "end": "3251360"
  },
  {
    "text": "But of course, what we'd\nlike to be able to do is to do this for\nmuch more large state spaces and realistic problems. So this is very much\nan ongoing area.",
    "start": "3251360",
    "end": "3259600"
  },
  {
    "text": "Again, you'll see\nthis similarity to the types of ideas\nwe've seen before. Very popular ideas are\noptimism under uncertainty",
    "start": "3259600",
    "end": "3265839"
  },
  {
    "text": "and Thompson sampling. They're not the only\nones, but they're probably the dominant strategies\npeople try to use.",
    "start": "3265840",
    "end": "3271210"
  },
  {
    "text": "For-- I may have just not\ncaught this, but specifically, what is actually different\nbetween the two algorithms?",
    "start": "3271210",
    "end": "3277600"
  },
  {
    "text": "What is the difference in this\nsampling, between 2013 and 2018? Yes.",
    "start": "3277600",
    "end": "3283180"
  },
  {
    "text": "So two things. One is that the PSRL does\nnot think about concurrency. So they just assume\nthere's a single MDP.",
    "start": "3283180",
    "end": "3289760"
  },
  {
    "text": "You have a single agent in it. The other case assumes you have\nm agents all in the same MDP.",
    "start": "3289760",
    "end": "3295390"
  },
  {
    "text": "So like the mice trying\nto find the cheese, there's not just one mouse,\nthere's a whole bunch. And the idea was\nseed sampling is also",
    "start": "3295390",
    "end": "3301930"
  },
  {
    "text": "to think about how\ndo you choose which MDP they each think they're in\nto distribute the exploration.",
    "start": "3301930",
    "end": "3309760"
  },
  {
    "text": "OK. Yeah. And the other case,\nyou don't have to do any coordination because\nthere's only one agent.",
    "start": "3309760",
    "end": "3316540"
  },
  {
    "text": " OK, good. So in terms of generalization,\nwe're going to think about this.",
    "start": "3316540",
    "end": "3321590"
  },
  {
    "text": "The reason why this\nstarts to get more tricky is a couple of things. One is that for optimism\nunder uncertainty, it means we have to have\na notion of uncertainty.",
    "start": "3321590",
    "end": "3328339"
  },
  {
    "text": "And it just gets much harder\nto represent uncertainty when we have deep neural networks. Similarly for Thompson\nsampling, as we",
    "start": "3328340",
    "end": "3334572"
  },
  {
    "text": "start to move up to really\ncomplicated domains, we need posteriors over\nreally complicated settings and that's also computationally\nchallenging and hard",
    "start": "3334572",
    "end": "3341230"
  },
  {
    "text": "to approximate. So let's first start\nwith contextual bandits.",
    "start": "3341230",
    "end": "3347193"
  },
  {
    "text": "And some of you\nguys will probably be doing some of this\nfor your project. So instead of having\nour multi-armed bandit,",
    "start": "3347193",
    "end": "3353320"
  },
  {
    "text": "now we're halfway\nbetween a Markov decision process and a bandit. So we're going to assume we have\nstates, but the action we take",
    "start": "3353320",
    "end": "3359950"
  },
  {
    "text": "doesn't influence\nthe next state. And so now, if we\nthink about rewards, we'll have a reward\nper action in state.",
    "start": "3359950",
    "end": "3367690"
  },
  {
    "text": "And just like what\nwe've often done before, if we have a really large\nstate and action space, we're going to\nassume that we use",
    "start": "3367690",
    "end": "3373725"
  },
  {
    "text": "some parametric\nrepresentation to model the relationship between state\nand action and output rewards.",
    "start": "3373725",
    "end": "3380460"
  },
  {
    "text": "Perhaps not surprisingly,\nthere is an enormous benefit of doing this. So if you think\nabout a setting where",
    "start": "3380460",
    "end": "3386850"
  },
  {
    "text": "this is the number\nof arms you have, if you did something\nlike upper confidence bounds-- and this is a regret.",
    "start": "3386850",
    "end": "3392740"
  },
  {
    "text": "Regret is on the y-axis. So if you did something like\nupper confidence bounds and you have 1,000 arms\nand 4,000 pulls--",
    "start": "3392740",
    "end": "3399280"
  },
  {
    "text": "sorry, you have 1,000 arms and\nthen you're pulling these over time. So this is, I think,\njust regret after a fixed",
    "start": "3399280",
    "end": "3405390"
  },
  {
    "text": "number of time steps. Unsurprisingly, if you have\na lot more arms to pull, you'll have a lot more regret.",
    "start": "3405390",
    "end": "3411030"
  },
  {
    "text": "Because in upper confidence\nbounds, in the things we've seen so far, you\ndon't share any information",
    "start": "3411030",
    "end": "3416819"
  },
  {
    "text": "across the arms. If, on the other\nhand, use something like linear UCB, which assumes\nthat your arms are represented",
    "start": "3416820",
    "end": "3425430"
  },
  {
    "text": "by a set of features-- so\nshowing someone like a Trump campaign today and a different\nTrump campaign tomorrow",
    "start": "3425430",
    "end": "3433349"
  },
  {
    "text": "might have the same effect. Because they're going to\nhave a shared set of features about Trump, at least would be\none thing that would overlap.",
    "start": "3433350",
    "end": "3440140"
  },
  {
    "text": "You can leverage that structure. And so, what you\ncan see in this case is that if you leverage say, a\nparametric linear representation",
    "start": "3440140",
    "end": "3447280"
  },
  {
    "text": "in this case, even as you scale\nup the actual number of arms, if your parameter space\nis still the same,",
    "start": "3447280",
    "end": "3454010"
  },
  {
    "text": "then your regret\ndoesn't scale badly. So for example, this is\nk, but you know your theta",
    "start": "3454010",
    "end": "3462279"
  },
  {
    "text": "in this case. The-- Your theta in this case\nmight just be low dimensional.",
    "start": "3462280",
    "end": "3469550"
  },
  {
    "text": "So we might have a\ntheta, which is in R d,",
    "start": "3469550",
    "end": "3475300"
  },
  {
    "text": "so we have a d-dimensional\nrepresentation. And this just shows that\nthis can be really helpful.",
    "start": "3475300",
    "end": "3481270"
  },
  {
    "text": "In general, you want\nto leverage structure. So one common thing to\ndo is to model the reward",
    "start": "3481270",
    "end": "3487510"
  },
  {
    "text": "as a linear function. Of course, this could be built\non top of a deep neural network,",
    "start": "3487510",
    "end": "3492802"
  },
  {
    "text": "or on top of a large language\nmodel or something like that. You can often just use\nsome really complicated representation of the\nstate and action space.",
    "start": "3492802",
    "end": "3499030"
  },
  {
    "text": "And then, say, for\nthe last layer, my actual reward is\ngoing to be a function of these complicated features,\ndot product with some theta",
    "start": "3499030",
    "end": "3507850"
  },
  {
    "text": "parameter. And one common\nthing is to assume that it's just a linear\nfunction plus some noise.",
    "start": "3507850",
    "end": "3514120"
  },
  {
    "text": "And the nice thing about this\nis that if your features are interpretable, then your\nreward function is also very interpretable\nbecause you can just",
    "start": "3514120",
    "end": "3520335"
  },
  {
    "text": "think of relatively, how much\ndo each of those features contribute to your reward? ",
    "start": "3520335",
    "end": "3526910"
  },
  {
    "text": "All right. So one thing to think about in\nthis case is in these settings--",
    "start": "3526910",
    "end": "3534455"
  },
  {
    "text": "well, I'll go a little\nfast through this part because I want to make sure\nwe get to the MDP part two. But when you have this, even if\nyou have a linear set of models,",
    "start": "3534455",
    "end": "3544740"
  },
  {
    "text": "you can use them to represent\nmore complicated functions. Because, let's say--\ntechnology is getting--",
    "start": "3544740",
    "end": "3554430"
  },
  {
    "text": "So let's say this is your\nreward model for three different actions.",
    "start": "3554430",
    "end": "3559619"
  },
  {
    "text": "This is a 1, this is\na 2, and this is a 3. And this is what your reward is.",
    "start": "3559620",
    "end": "3566000"
  },
  {
    "text": "And this is your\nstate and space. So let's imagine that you\nhad a linear representation.",
    "start": "3566000",
    "end": "3571109"
  },
  {
    "text": "Then, you could\nrepresent policies that are just joint linear\nbecause if you were taking",
    "start": "3571110",
    "end": "3576680"
  },
  {
    "text": "the max here, this is what the\nvalue would be of your policy. Because it would say, a\n1 dominates for this part",
    "start": "3576680",
    "end": "3583010"
  },
  {
    "text": "of the state space. A 3 dominates for this\npart of the state space, and a 2 dominates for this\npart of the state space.",
    "start": "3583010",
    "end": "3589500"
  },
  {
    "text": "So linear ones-- I guess the main point\nhere is that even if you have a linear reward\nmodel, it doesn't mean",
    "start": "3589500",
    "end": "3596580"
  },
  {
    "text": "your policy has to be linear. Your policy will\nbe disjoint linear. It can be made up of\nthese sorts of functions.",
    "start": "3596580",
    "end": "3603780"
  },
  {
    "text": "So it's fairly flexible. OK, how would we\nwork in these cases? Well, in this case, what it\nmeans to have uncertainty",
    "start": "3603780",
    "end": "3610070"
  },
  {
    "text": "is we need to have uncertainty\nover this linear vector. So, we want to capture\nuncertainty over theta",
    "start": "3610070",
    "end": "3617390"
  },
  {
    "text": "through some sort\nof uncertainty set. And there's been a\nlot of beautiful work to try to quantify the\ntypes of uncertainties",
    "start": "3617390",
    "end": "3624380"
  },
  {
    "text": "we have through things like\nthe elliptical potential lemon, things like that, which\ngive us, basically,",
    "start": "3624380",
    "end": "3630530"
  },
  {
    "text": "just sort of an uncertainty\nset over vectors. And you can do this in a\ncomputationally tractable way.",
    "start": "3630530",
    "end": "3637640"
  },
  {
    "text": "And what this means is it\ngives us a principled way to get an upper confidence bound\non the reward function, given",
    "start": "3637640",
    "end": "3642710"
  },
  {
    "text": "that we have uncertainty\nover linear model. And this was shown to be\nvery useful for news article",
    "start": "3642710",
    "end": "3648290"
  },
  {
    "text": "recommendations\nabout 14 years ago. And you can also\nlook at chapter 19.",
    "start": "3648290",
    "end": "3653570"
  },
  {
    "text": "So these are really useful. This is one way to represent\na contextual bandit setting",
    "start": "3653570",
    "end": "3659869"
  },
  {
    "text": "when you want to\nhandle generalization. We'll now talk briefly\nabout how you might do this",
    "start": "3659870",
    "end": "3665700"
  },
  {
    "text": "for Markov decision processes. OK, so if we think back\nto the MBIE-EB algorithm",
    "start": "3665700",
    "end": "3671579"
  },
  {
    "text": "for finite state and actions,\nwe have to modify a few things.",
    "start": "3671580",
    "end": "3676756"
  },
  {
    "text": "So if we think about this, we\nwere keeping track of counts.",
    "start": "3676756",
    "end": "3682380"
  },
  {
    "text": "And we were doing this-- we were building\na model separately for every state and action.",
    "start": "3682380",
    "end": "3688260"
  },
  {
    "text": "So this count-based term here\nthat we're using as a bonus, we've already seen\nhow we might be",
    "start": "3688260",
    "end": "3694619"
  },
  {
    "text": "able to do Q-functions\nwith deep neural networks. But the big problem here\nis the count-based bonus.",
    "start": "3694620",
    "end": "3701190"
  },
  {
    "text": "We have an infinite\nnumber of states. If you think about Atari\nor something like that, you certainly don't\nwant to count.",
    "start": "3701190",
    "end": "3706450"
  },
  {
    "text": "You're mostly only going to\nsee one Atari screen once ever. And so, these sort of\ncount-based bonuses",
    "start": "3706450",
    "end": "3713520"
  },
  {
    "text": "aren't very realistic. And so we're going to\nneed ways, essentially. But why do we have that\ncount-based bonuses?",
    "start": "3713520",
    "end": "3719630"
  },
  {
    "text": "We have the count-based\nbonuses to try to quantify our uncertainty over\nhow well do we know the reward model for this particular\nstate in action,",
    "start": "3719630",
    "end": "3727030"
  },
  {
    "text": "and how well do we\nknow the dynamics. And so, one of the ideas\nwhen deep RL came around",
    "start": "3727030",
    "end": "3732618"
  },
  {
    "text": "was to think about,\ncould we lift this idea and try to quantify our\nuncertainty in the deep RL setting?",
    "start": "3732618",
    "end": "3738880"
  },
  {
    "text": "So we're going to need\nto move beyond having these very simple\ncounts to think about something that's a higher\nlevel representation of that.",
    "start": "3738880",
    "end": "3746567"
  },
  {
    "text": "Now, if we could get that-- and\nI haven't told you how we can get it yet-- you could imagine that a lot of\nthe algorithms we've seen before",
    "start": "3746568",
    "end": "3752560"
  },
  {
    "text": "could be extended fairly easily. So in particular, if you think\nabout something like function",
    "start": "3752560",
    "end": "3758440"
  },
  {
    "text": "approximation with\nQ-learning, we could imagine just adding some\nsort of bonus term in here.",
    "start": "3758440",
    "end": "3764730"
  },
  {
    "text": "So instead of having our\nempirical reward plus gamma times our target, like\nour observed next state",
    "start": "3764730",
    "end": "3771790"
  },
  {
    "text": "in action with some\nparameter weight, we could just plug\nin some bonus. That's kind of what\nMBIE-EB is already doing.",
    "start": "3771790",
    "end": "3779420"
  },
  {
    "text": "It's just that our bonus before\nwas determined by our counts. And now, we need some\nother way to lift",
    "start": "3779420",
    "end": "3784630"
  },
  {
    "text": "that so we can do that for\nmuch more general settings. But once we have that, we can\nimagine plugging it in here.",
    "start": "3784630",
    "end": "3792700"
  },
  {
    "text": "So there's a lot of\ndifferent approaches that have been developed to\ntry to think about something of density, or quantification\nof how many visits we have",
    "start": "3792700",
    "end": "3801940"
  },
  {
    "text": "or how much certainty we\nhave over different parts of the state and action space. So one of the things that\nMarc Bellemare and others did,",
    "start": "3801940",
    "end": "3808850"
  },
  {
    "text": "which was pretty\nsuccessful, is they tried to build pseudo\ncounts over parts",
    "start": "3808850",
    "end": "3814809"
  },
  {
    "text": "of the state and action space. So you could imagine maybe\neven some particular rooms in a video game\nmany, many times.",
    "start": "3814810",
    "end": "3821180"
  },
  {
    "text": "And so, you try to essentially\nreduce your uncertainty over those. There's all sorts\nof important details",
    "start": "3821180",
    "end": "3827180"
  },
  {
    "text": "here around whether\nyou-- normally, in MBIE-EB, every round you\nupdate all of those counts.",
    "start": "3827180",
    "end": "3834140"
  },
  {
    "text": "In reality, if you think\nback to deep Q-learning, we maintained a buffer of state\naction rewards next states.",
    "start": "3834140",
    "end": "3841140"
  },
  {
    "text": "Now, you would need to include\nthose bonus terms in there too. And if those bonus terms\nare changing, how much",
    "start": "3841140",
    "end": "3846800"
  },
  {
    "text": "do you update your buffer? Just to give you a sense of\nsome of the different wrinkles one has to think about.",
    "start": "3846800",
    "end": "3852402"
  },
  {
    "text": "But the high level\nimportant thing is that this matters a lot. So in Montezuma's Revenge, which\nwas early on considered one",
    "start": "3852402",
    "end": "3857870"
  },
  {
    "text": "of the hardest Atari\ngames-- probably still is-- if you did a standard DQN for 50\nmillion frames, which is a lot,",
    "start": "3857870",
    "end": "3865500"
  },
  {
    "text": "it never got past\nthe second room. With epsilon greedy exploration,\nit was not strategic.",
    "start": "3865500",
    "end": "3872210"
  },
  {
    "text": "It just got very\nbad performance. But what Marc Bellemare\nand others showed is that by incorporating a\nnotion of count-based lifted",
    "start": "3872210",
    "end": "3881270"
  },
  {
    "text": "to the generalization case, you\ncould do far, far, far better. So that's just to\nhighlight that there",
    "start": "3881270",
    "end": "3887730"
  },
  {
    "text": "are ways to lift up this\nnotion of optimism uncertainty for this type of setting. ",
    "start": "3887730",
    "end": "3894130"
  },
  {
    "text": "There is similarly ways\nto lift Thompson sampling. So, we've done some\nwork there where",
    "start": "3894130",
    "end": "3899590"
  },
  {
    "text": "we think about particular\nrepresentations and parameters. Ian Osband, who\nintroduced PSRL, then",
    "start": "3899590",
    "end": "3907150"
  },
  {
    "text": "tried to lift it up to\nthe deep Q-learning case. They did it where they were\njust bootstrapping samples",
    "start": "3907150",
    "end": "3912190"
  },
  {
    "text": "as an approximation. That is a pretty coarse\napproximation of uncertainty.",
    "start": "3912190",
    "end": "3922740"
  },
  {
    "text": "Something else that often\nworked pretty well-- surprisingly well,\ngiven how simple it is-- is essentially to do something\njust at the last layer.",
    "start": "3922740",
    "end": "3929550"
  },
  {
    "text": "So the last layer do something\nlike Bayesian linear regression to try to get an uncertainty\nestimate, and then",
    "start": "3929550",
    "end": "3935310"
  },
  {
    "text": "sample from that. So this is a pretty simple\nthing one could try. There's a lot of\nwork to do this.",
    "start": "3935310",
    "end": "3942240"
  },
  {
    "text": "Let's go back to thinking of\nother really recent approaches which try to think about doing\nthis not just for one task,",
    "start": "3942240",
    "end": "3949060"
  },
  {
    "text": "but many tasks where you\nneed to do generalization. So early in this lecture, I\nintroduced the DREAM algorithm",
    "start": "3949060",
    "end": "3954960"
  },
  {
    "text": "to you, which we later\nused to actually go grading of the Breakout assignment.",
    "start": "3954960",
    "end": "3961059"
  },
  {
    "text": "The notion in DREAM is that\nyou have many different tasks and you're going to learn how\nto explore in them efficiently.",
    "start": "3961060",
    "end": "3967815"
  },
  {
    "text": "So that was one example where\nwe're now really thinking about how do we develop\nefficient exploration strategies by leveraging\nstructure over the tasks,",
    "start": "3967815",
    "end": "3975569"
  },
  {
    "text": "where an agent is going\nto do a series of tasks. Similarly, in some\nof our recent work, we introduced decision\npre-trained transformers.",
    "start": "3975570",
    "end": "3984160"
  },
  {
    "text": "This was, again, a\nmeta-learning case. The idea is that\nyour agent is going to do a series of\nbandit problems,",
    "start": "3984160",
    "end": "3989290"
  },
  {
    "text": "or a series of RL\nproblems, and we want to learn how to optimally\nexplore in those settings.",
    "start": "3989290",
    "end": "3995440"
  },
  {
    "text": "So I'll just show you\nbriefly how it works. The idea in this\nsetting is we're",
    "start": "3995440",
    "end": "4000720"
  },
  {
    "text": "going to use a\npre-trained transformer. One of the interesting things is\nyou map reinforcement learning",
    "start": "4000720",
    "end": "4006540"
  },
  {
    "text": "to supervised learning,\nsimilar to behavior cloning. But instead of relying on the\ndata you collected in the past,",
    "start": "4006540",
    "end": "4012160"
  },
  {
    "text": "if you can compute what would\nhave been the right action to take there, you can train it\nto predict that optimal action.",
    "start": "4012160",
    "end": "4019349"
  },
  {
    "text": "It turns out that\nwhen you do that, we can exactly map that back to\ndoing the equivalent of Thompson",
    "start": "4019350",
    "end": "4025200"
  },
  {
    "text": "sampling. So in all the settings for\nwhich Thompson sampling has theoretical guarantees, this\ndecision pre-trained transformer",
    "start": "4025200",
    "end": "4032520"
  },
  {
    "text": "can inherit those guarantees,\nwhich is pretty cool. The nice thing too is\nthat, empirically, it",
    "start": "4032520",
    "end": "4038250"
  },
  {
    "text": "can allow you to take\nadvantage of structure that is present in your domain\nthat you didn't have to code.",
    "start": "4038250",
    "end": "4044280"
  },
  {
    "text": "So let me just give\nyou an example of that. So what I showed you\nearlier in this lecture is that if you\nhave a domain where",
    "start": "4044280",
    "end": "4050820"
  },
  {
    "text": "you have some linear\nstructure, if you give that linear structure\nto your algorithm, then",
    "start": "4050820",
    "end": "4058463"
  },
  {
    "text": "you can do quite well. So that's the green line here. So this is the amount of\ndata you have over time. And this is your\ncumulative regret.",
    "start": "4058463",
    "end": "4063718"
  },
  {
    "text": "Lower is better. So, most historical\nalgorithms have assumed you give that\nstructure to your bandit.",
    "start": "4063718",
    "end": "4069760"
  },
  {
    "text": "You write down, there\nare these 300 features that you need to pay attention\nto news articles and people",
    "start": "4069760",
    "end": "4075180"
  },
  {
    "text": "to figure out what\nthe reward will be. If you give it that structure\nand that structure is right, you often do pretty well.",
    "start": "4075180",
    "end": "4081210"
  },
  {
    "text": "You could not leverage\nthat structure, and you would get\nsomething like this.",
    "start": "4081210",
    "end": "4086519"
  },
  {
    "text": "So this is a Thompson\nsampling algorithm, which just assumes\nthat it doesn't have that linear structure.",
    "start": "4086520",
    "end": "4092670"
  },
  {
    "text": "What are the cool\nthings that we found by this approach is\nthat in this setting, if you really have a linear\nstructure in your domain,",
    "start": "4092670",
    "end": "4101378"
  },
  {
    "text": "and you're doing many\ntasks and all of them have this linear structure,\nwhat are decision pre-trained\ntransformer will learn",
    "start": "4101378",
    "end": "4107068"
  },
  {
    "text": "is that even though\nyou're not telling it, it will realize it can more\ncompactly encode that structure.",
    "start": "4107069",
    "end": "4112889"
  },
  {
    "text": "And so, when you deploy\nit on a new task, you will get behavior\nalmost as if you gave it",
    "start": "4112890",
    "end": "4118528"
  },
  {
    "text": "the unknown structure. So I think this is\nreally interesting because often one of the brittle\naspects of machine learning",
    "start": "4118529",
    "end": "4125219"
  },
  {
    "text": "is that we originally wrote down\nthese sort of representations. And of course, one of the really\namazing things for deep learning",
    "start": "4125220",
    "end": "4130830"
  },
  {
    "text": "is that we're trying\nto not write down specific\nrepresentations as much, and get much closer\nto the input raw data.",
    "start": "4130830",
    "end": "4137199"
  },
  {
    "text": "And this is illustrating that\nin terms of sequential decision making and meta exploration\nfor multiple tasks,",
    "start": "4137200",
    "end": "4143318"
  },
  {
    "text": "we can do something similar\nhere, where we can inductively learn that that's a more compact\nway to represent the domains,",
    "start": "4143319",
    "end": "4151380"
  },
  {
    "text": "and get this much more efficient\nexploration in new tasks.",
    "start": "4151380",
    "end": "4157199"
  },
  {
    "text": "All right. So just to conclude,\nwe're wrapping up our notion of data efficient\nreinforcement learning today.",
    "start": "4157200",
    "end": "4163799"
  },
  {
    "text": "You should understand\nthis tension between exploration\nand exploitation in reinforcement learning.",
    "start": "4163800",
    "end": "4168979"
  },
  {
    "text": "I haven't used these words. They're not great words. So I don't use these. I haven't used a lot in there. But exploration, meaning\nyou're taking time",
    "start": "4168979",
    "end": "4175839"
  },
  {
    "text": "to learn about the\ndomain and exploitation, meaning that you're\nleveraging that information to make good decisions in\nthe context of reinforcement",
    "start": "4175840",
    "end": "4182528"
  },
  {
    "text": "learning. You should be able\nto define and compare different notions of good,\nwhether empirical, convergence,",
    "start": "4182529",
    "end": "4188509"
  },
  {
    "text": "regret, and PAC. You should know\nfor the algorithms we've talked about,\ndo they have--",
    "start": "4188510",
    "end": "4194270"
  },
  {
    "text": "for example, is greedy\nsublinear regret? Which it's not.",
    "start": "4194270",
    "end": "4199480"
  },
  {
    "text": "You should understand\nthe proof sketch I did of why upper confidence\nbound is sublinear in regret.",
    "start": "4199480",
    "end": "4205440"
  },
  {
    "text": "All right. And then next week, we're going\nto talk about AlphaGo and how do we think about doing\nSMART adaptive tree",
    "start": "4205440",
    "end": "4212019"
  },
  {
    "text": "search in really large games. See you then. ",
    "start": "4212020",
    "end": "4220000"
  }
]