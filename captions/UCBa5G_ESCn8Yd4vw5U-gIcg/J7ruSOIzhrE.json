[
  {
    "start": "0",
    "end": "5880"
  },
  {
    "text": "OK, welcome everyone, to\nweek four we're into now.",
    "start": "5880",
    "end": "11460"
  },
  {
    "text": "So for today, what\nI want to do is, first of all,\nwell, a couple more",
    "start": "11460",
    "end": "17760"
  },
  {
    "text": "bits on machine\ntranslation, especially just talking a little bit about\nevaluating machine translation.",
    "start": "17760",
    "end": "23970"
  },
  {
    "text": "Then I want to spend\na while on attention. So attention is a very\nfundamental concept",
    "start": "23970",
    "end": "31980"
  },
  {
    "text": "of neural networks,\nwhich was originally developed in the context\nof machine translation.",
    "start": "31980",
    "end": "37780"
  },
  {
    "text": "But there's also then\na very central concept when we're talking about\ntransformers, which we then",
    "start": "37780",
    "end": "43710"
  },
  {
    "text": "start talking about on Thursday. OK, so getting straight into it.",
    "start": "43710",
    "end": "51510"
  },
  {
    "text": "So this is the\npicture that we saw towards the end of last time.",
    "start": "51510",
    "end": "56640"
  },
  {
    "text": "This is how we were making\nmachine translation system where we are using a multi-layer\nLSTM, where we were feeding",
    "start": "56640",
    "end": "65740"
  },
  {
    "text": "in a source\nsentence, and then we were flipping to then\nturning the model",
    "start": "65740",
    "end": "73690"
  },
  {
    "text": "into a decoder with\ndifferent parameters, which would generate one word\nat a time to generate",
    "start": "73690",
    "end": "79270"
  },
  {
    "text": "the translated sentence. So here I've got\nan German sentence,",
    "start": "79270",
    "end": "85860"
  },
  {
    "text": "and it's produced an\nEnglish translation that looks a pretty good one.",
    "start": "85860",
    "end": "91000"
  },
  {
    "text": "But we're going to want\nto have a way of deciding, well, are we producing\ngood translations or not.",
    "start": "91000",
    "end": "99780"
  },
  {
    "text": "And so we need some way to\nevaluate machine translation. Now, this is a complex\narea because if you",
    "start": "99780",
    "end": "108270"
  },
  {
    "text": "start poking around\nin the literature, people have proposed literally\nhundreds of different measures",
    "start": "108270",
    "end": "116220"
  },
  {
    "text": "that could be used to evaluate\nmachine translation systems. I'm guilty of writing a\ncouple of papers on it myself,",
    "start": "116220",
    "end": "122710"
  },
  {
    "text": "so I'm contributed\nto the problem. But by far the most common\nmeasure that you see to this day",
    "start": "122710",
    "end": "131850"
  },
  {
    "text": "was essentially the first\nmeasure proposed automatically evaluate machine\ntranslation, which",
    "start": "131850",
    "end": "137819"
  },
  {
    "text": "was the BLEU measure, which\nwas proposed to understand",
    "start": "137820",
    "end": "142900"
  },
  {
    "text": "for bilingual\nevaluation understudy, though it went\nalong with the fact that it was proposed by IBM.",
    "start": "142900",
    "end": "149390"
  },
  {
    "text": "Probably not a coincidence. So until this\npoint, the only way",
    "start": "149390",
    "end": "155740"
  },
  {
    "text": "that people had really used\nfor evaluating translations was getting human\nbeings to look at them",
    "start": "155740",
    "end": "161650"
  },
  {
    "text": "and say how good of a\ntranslation this is. And that's still a\ngold standard measure",
    "start": "161650",
    "end": "167680"
  },
  {
    "text": "that is widely used for\nevaluating translations because many of the\nautomatic measures",
    "start": "167680",
    "end": "174700"
  },
  {
    "text": "have various kinds of\nbiases and problems that make human\nevaluation useful.",
    "start": "174700",
    "end": "182440"
  },
  {
    "text": "But on the other hand,\na lot of the time, we'd like to iterate\nquickly on evaluations.",
    "start": "182440",
    "end": "190400"
  },
  {
    "text": "We'd like to use evaluations\nand training loops and things like that. And the IBM people\nwith the BLEU paper",
    "start": "190400",
    "end": "197710"
  },
  {
    "text": "suggest, well,\nmaybe we can come up with a halfway decent automatic\nmethod of doing translations.",
    "start": "197710",
    "end": "204730"
  },
  {
    "text": "And the idea of what\nthey propose was this-- that we're going to have one\nor more reference translations",
    "start": "204730",
    "end": "211560"
  },
  {
    "text": "for a piece of text. So these are human-written\ntranslations. And then we can score any\nautomatic translation mainly",
    "start": "211560",
    "end": "221730"
  },
  {
    "text": "on how often they have\noverlapping 1, 2, 3,",
    "start": "221730",
    "end": "227489"
  },
  {
    "text": "and 4 grams. The number 4 isn't special. You could have only\ngone up to 3 or 5,",
    "start": "227490",
    "end": "233470"
  },
  {
    "text": "but 4 was seen as a\nreasonable length, overlapping n-grams with one\nof the reference translations.",
    "start": "233470",
    "end": "241150"
  },
  {
    "text": "And the more overlap\nyou have, the better. And there's discussion of this\nevaluation in the assignment.",
    "start": "241150",
    "end": "250209"
  },
  {
    "text": "So you can think\nabout it a bit more. And I won't go actually through\nall the formulas right now.",
    "start": "250210",
    "end": "255630"
  },
  {
    "text": "But that's most of it. And so here's a picture\nof how that looks.",
    "start": "255630",
    "end": "262028"
  },
  {
    "text": "So the original idea\nwas what we should do is have several\nreference translations.",
    "start": "262029",
    "end": "269410"
  },
  {
    "text": "And then we'd get a\nmachine translation. And then we'd look at\nthis machine translation",
    "start": "269410",
    "end": "275409"
  },
  {
    "text": "and try and find pieces of it\nin the reference translation. So we can certainly\nfind the unigram \"the.\"",
    "start": "275410",
    "end": "283210"
  },
  {
    "text": "We can't find \"American\" at all,\nbut we can find \"International",
    "start": "283210",
    "end": "288669"
  },
  {
    "text": "Airport and its\" in the\nsecond reference translation. So we're going to get a\n4-gram match for that.",
    "start": "288670",
    "end": "295419"
  },
  {
    "text": "We can find that again. That's easy. \"Office all receives ones\ncalls self the sand Arab.\"",
    "start": "295420",
    "end": "301960"
  },
  {
    "text": "Not a very good\ntranslation this, right? So that all misses. But then you start to find\nother pieces that do overlap,",
    "start": "301960",
    "end": "309400"
  },
  {
    "text": "and you use those\nto work out a score. The original idea was you should\nalways have multiple reference",
    "start": "309400",
    "end": "315970"
  },
  {
    "text": "translations so\nthat you can sample the space of\npossible translations",
    "start": "315970",
    "end": "322120"
  },
  {
    "text": "and have reasonable coverage. In practice for what's\nbeen done more recently,",
    "start": "322120",
    "end": "327200"
  },
  {
    "text": "it's not so uncommon that people\ndo this with only one reference translation. And the argument then\nis still on a kind",
    "start": "327200",
    "end": "334390"
  },
  {
    "text": "of a probabilistic basis. The more often you have\na good translation, the more often\nyou'll get matches,",
    "start": "334390",
    "end": "340210"
  },
  {
    "text": "and therefore your\nscore will be better. Yeah. So why did people\ncome up with this,",
    "start": "340210",
    "end": "352139"
  },
  {
    "text": "and why is it still imperfect? Well, the problem\nwith translation",
    "start": "352140",
    "end": "357840"
  },
  {
    "text": "is that there isn't\none right answer. It's not like the kind\nof classification things you see in machine learning\nwhere you show people a picture.",
    "start": "357840",
    "end": "366160"
  },
  {
    "text": "And the right answer is to\nsay, the class of this object",
    "start": "366160",
    "end": "371250"
  },
  {
    "text": "is whatever a labradoodle or a\ndog breeds or something, right?",
    "start": "371250",
    "end": "376620"
  },
  {
    "text": "For any sentence, there are many\ndifferent ways to translate it. And translators can sit\naround and argue that, oh,",
    "start": "376620",
    "end": "383910"
  },
  {
    "text": "this phrasing is a little bit\nnicer than this phrasing, blah, blah, blah, blah. But to a first\napproximation, you can translate a sentence\nin lots of ways.",
    "start": "383910",
    "end": "391660"
  },
  {
    "text": "And those different\nways of translation can involve different\nword orders. So you can't really\ncheck the words off",
    "start": "391660",
    "end": "398670"
  },
  {
    "text": "as you come down\nin the sentence. And that's what motivated\nthis idea of matching n-grams",
    "start": "398670",
    "end": "404700"
  },
  {
    "text": "anywhere. So you can get reasonable credit\nfor having the right matches.",
    "start": "404700",
    "end": "411099"
  },
  {
    "text": "But nevertheless, it's a\npretty crude version of it.",
    "start": "411100",
    "end": "417735"
  },
  {
    "text": "You can still get a poor BLEU\nscore for a good translation just because the\nwords you chose didn't",
    "start": "417735",
    "end": "423690"
  },
  {
    "text": "happen to match a\nreference translation. And also you can get\npoints for things",
    "start": "423690",
    "end": "429419"
  },
  {
    "text": "without really having a\ngood translation at all. If you just have\nwords that match,",
    "start": "429420",
    "end": "434650"
  },
  {
    "text": "even if they're having\ncompletely the wrong role in the sentence, you\nwill get some points.",
    "start": "434650",
    "end": "440800"
  },
  {
    "text": "But it's harder to\nget n-gram matches unless you're for\nlarger n, unless you're",
    "start": "440800",
    "end": "446070"
  },
  {
    "text": "using words the right way. There's one other trick\nin the BLEU measure--",
    "start": "446070",
    "end": "451139"
  },
  {
    "text": "that there's a penalty for\ntoo short system translations because otherwise\nyou could leave out",
    "start": "451140",
    "end": "457080"
  },
  {
    "text": "everything difficult and\nonly translate the easy part of the sentence. And then for the bits\nyou have translated,",
    "start": "457080",
    "end": "464230"
  },
  {
    "text": "you could then be\ngetting a high score for the precision\nof those pieces.",
    "start": "464230",
    "end": "470430"
  },
  {
    "text": "So we'll use, when\nyou're developing your systems for assignment\n3, we'll use them with BLEU.",
    "start": "470430",
    "end": "478920"
  },
  {
    "text": "So now we have a\nevaluation measure. We can start looking at how well\ndoes systems do on a BLEU score.",
    "start": "478920",
    "end": "492820"
  },
  {
    "text": "And BLEU scores are\ntheoretically between 0 and 100. But you're never\ngoing to get to 100",
    "start": "492820",
    "end": "498479"
  },
  {
    "text": "because of the variations of\nhow you can translate things. And so, typically, if you\ncan start to get to the 20s,",
    "start": "498480",
    "end": "507060"
  },
  {
    "text": "the translations-- you can understand what the\nsource document was about.",
    "start": "507060",
    "end": "512880"
  },
  {
    "text": "Once you get into\nthe 30s and 40s, the translations are\ngetting much, much better.",
    "start": "512880",
    "end": "519679"
  },
  {
    "text": "Yeah. So statistical phrase-based\ntranslation was pioneered by IBM",
    "start": "519679",
    "end": "526990"
  },
  {
    "text": "in the late 90s actually and was\nsort of redeveloped in the 2000s decade and was what Google\nlaunched as Google Translate",
    "start": "526990",
    "end": "534940"
  },
  {
    "text": "in the 2000s decade. And it continued to be worked\non for the following decade.",
    "start": "534940",
    "end": "540920"
  },
  {
    "text": "But there was basically\na strong sense that progress in\ntranslation had--",
    "start": "540920",
    "end": "547900"
  },
  {
    "text": "doing statistical\nphrase-based systems had basically stalled, that\nit got a little bit better",
    "start": "547900",
    "end": "554170"
  },
  {
    "text": "each year as people could build\ntraditional n-gram language models with more data every\nyear and things like that.",
    "start": "554170",
    "end": "561080"
  },
  {
    "text": "But the numbers were\nbarely going upwards. So in the years from about\n2005 to '15, or maybe '14,",
    "start": "561080",
    "end": "573050"
  },
  {
    "text": "the dominant idea in the\nmachine translation community was the way we were going\nto get better machine",
    "start": "573050",
    "end": "579920"
  },
  {
    "text": "translation is doing\nsyntax-based machine translation. If we actually knew the\nstructure of sentences",
    "start": "579920",
    "end": "586910"
  },
  {
    "text": "and we'd pass them up, then\nwe'd know what the role of words was in sentences,\nand then we'd be",
    "start": "586910",
    "end": "592490"
  },
  {
    "text": "able to translate much better. And this was particularly\ninvoked by looking at languages",
    "start": "592490",
    "end": "598880"
  },
  {
    "text": "where translation work terribly. So in those days,\ntranslation worked sort of",
    "start": "598880",
    "end": "604820"
  },
  {
    "text": "OK for languages like\nFrench to English or Spanish",
    "start": "604820",
    "end": "610580"
  },
  {
    "text": "to English, which are kind of\nsimilar European languages. But the results worked way worse\nfor Chinese to English or German",
    "start": "610580",
    "end": "620660"
  },
  {
    "text": "to English. And even though English\nis a Germanic language, German has a very different\nword order to English",
    "start": "620660",
    "end": "627650"
  },
  {
    "text": "with commonly verbs\nat the end of a clause and different elements\nbeing fronted.",
    "start": "627650",
    "end": "632970"
  },
  {
    "text": "And so people tried to work\non grammar-based, syntax-based",
    "start": "632970",
    "end": "641480"
  },
  {
    "text": "methods of statistical\nmachine translation. And I was one of those who\nworked on those in the late",
    "start": "641480",
    "end": "646610"
  },
  {
    "text": "2000s decade. But the truth is that sort\nof didn't really work, right?",
    "start": "646610",
    "end": "651960"
  },
  {
    "text": "If the rate of progress in\nsyntax-based machine translation",
    "start": "651960",
    "end": "659405"
  },
  {
    "text": "had slightly more slope than\nphrase-based machine translation over these years, the amount\nof slope wasn't very much.",
    "start": "659405",
    "end": "669180"
  },
  {
    "text": "So things were\ncompletely then thrown on their head when neural\nmachine translation got",
    "start": "669180",
    "end": "675110"
  },
  {
    "text": "invented because as I explained,\nthe first attempts were in 2014.",
    "start": "675110",
    "end": "680790"
  },
  {
    "text": "The first cases in which\nit was evaluated in bake off evaluations was 2015.",
    "start": "680790",
    "end": "686370"
  },
  {
    "text": "And so in 2015, it wasn't as\ngood as the best other machine translation methods.",
    "start": "686370",
    "end": "692399"
  },
  {
    "text": "But by 2016, it was. And it was just on this\nmuch, much steeper slope",
    "start": "692400",
    "end": "697700"
  },
  {
    "text": "of getting way, way better. And this graph only\ngoes up to 2019, but it's continued to go up.",
    "start": "697700",
    "end": "704430"
  },
  {
    "text": "And so it's not that uncommon\nthese days that you see BLEU numbers in the 50s and 60s\nfor neural machine translation",
    "start": "704430",
    "end": "712580"
  },
  {
    "text": "systems. So that's a good news story. So after this, I want to\ngo on and so introduce",
    "start": "712580",
    "end": "720800"
  },
  {
    "text": "this idea of\nattention, which is now a very fundamental, important\nidea in neural systems.",
    "start": "720800",
    "end": "729580"
  },
  {
    "text": "It's also interesting\nbecause it's actually something novel that was\ninvented kind of recently.",
    "start": "729580",
    "end": "737000"
  },
  {
    "text": "So for everything that we've\ndone in neural networks up until now, really\nit had all been",
    "start": "737000",
    "end": "744290"
  },
  {
    "text": "invented before the\nturn of the millennium. So basic feedforward\nneural networks,",
    "start": "744290",
    "end": "751410"
  },
  {
    "text": "recurrent neural\nnetworks, LSTMs, other things that we haven't\nyet haven't talked about,",
    "start": "751410",
    "end": "756810"
  },
  {
    "text": "like convolutional neural\nnetworks-- they were all invented last millennium. It was really a waiting\ngame at that point",
    "start": "756810",
    "end": "765530"
  },
  {
    "text": "until there was sufficient data\nand computational power for them really to show how\ngood they were.",
    "start": "765530",
    "end": "772459"
  },
  {
    "text": "But attention was\nsomething that actually got invented in 2014 in the\norigins of neural machine",
    "start": "772460",
    "end": "780570"
  },
  {
    "text": "translation. And it proved to be a very\ntransformative idea for making neural networks more powerful.",
    "start": "780570",
    "end": "788820"
  },
  {
    "text": "So the idea of-- what\nmotivated attention was looking at exactly this kind\nof machine translation problem.",
    "start": "788820",
    "end": "796090"
  },
  {
    "text": "So we were running our LSTM\nover the source sentence. And then we were using\nthis hidden state",
    "start": "796090",
    "end": "802410"
  },
  {
    "text": "as the previous hidden\nstate that we're feeding into the generator,\nLSTM, for the target sentence.",
    "start": "802410",
    "end": "810459"
  },
  {
    "text": "And what that\nmeans is everything useful about this\nsentence has to be",
    "start": "810460",
    "end": "816630"
  },
  {
    "text": "stuffed into that one vector. And while that's maybe not so\nhard if you've got a four-word",
    "start": "816630",
    "end": "822210"
  },
  {
    "text": "sentence, but maybe you've got\na 40-word sentence out here. And it seems to be\nkind of implausible",
    "start": "822210",
    "end": "829560"
  },
  {
    "text": "that it'd be a good\nidea to be trying to fit everything\nabout that sentence into this one hidden state.",
    "start": "829560",
    "end": "835960"
  },
  {
    "text": "And well, obviously there\nare crude solutions to this. You make the hidden\nstates bigger, and then you've got more\nrepresentational space.",
    "start": "835960",
    "end": "842320"
  },
  {
    "text": "You use a multi-layer LSTM. You've got more\nrepresentational space.",
    "start": "842320",
    "end": "847720"
  },
  {
    "text": "But it still seems a very\nquestionable thing to do. And it's certainly not like\nwhat a human being does, right?",
    "start": "847720",
    "end": "856050"
  },
  {
    "text": "If a human being is\ntranslating a sentence, they read the\nsentence and they've got some idea of its meaning.",
    "start": "856050",
    "end": "862480"
  },
  {
    "text": "But as they start\nto translate, they look back at the earlier\nparts of the sentence and make use of that\nand their translation.",
    "start": "862480",
    "end": "869230"
  },
  {
    "text": "And so that doesn't seem like\nit's a very plausible model. So the idea should be that\nour neural network should",
    "start": "869230",
    "end": "877770"
  },
  {
    "text": "be able to attend to\ndifferent things in the source so that they can get\ninformation as needed,",
    "start": "877770",
    "end": "885660"
  },
  {
    "text": "looking back in the sentence. And so this is the\nidea of attention. So on each step of\nthe decoder, we're",
    "start": "885660",
    "end": "893699"
  },
  {
    "text": "going to insert direct\nconnections to the encoder so we can look at particular\nwords in the sentence.",
    "start": "893700",
    "end": "901380"
  },
  {
    "text": "So I've got a bunch\nof diagram sentences that go through what we do. And then after that, I'll\npresent the equations",
    "start": "901380",
    "end": "908339"
  },
  {
    "text": "that go along with this. OK, so once we're\nstarting to translate,",
    "start": "908340",
    "end": "914529"
  },
  {
    "text": "we've got a hidden state,\nthe start of our generator, and then we're going to use\nthis hidden state as our key",
    "start": "914530",
    "end": "924550"
  },
  {
    "text": "to look back into the encoder\nto try and find useful stuff.",
    "start": "924550",
    "end": "929660"
  },
  {
    "text": "So we're going to compare-- in a way I'll make\nprecise later--",
    "start": "929660",
    "end": "935860"
  },
  {
    "text": "the hidden state, with the\nhidden state at every position",
    "start": "935860",
    "end": "941950"
  },
  {
    "text": "in the source sentence. And based on our\ncomparisons, we're",
    "start": "941950",
    "end": "948399"
  },
  {
    "text": "going to work out an attention\nscore, where should we be looking at in\nthe source sentence",
    "start": "948400",
    "end": "955000"
  },
  {
    "text": "while generating here the\nfirst word of the translation. And so based on these\nattention scores,",
    "start": "955000",
    "end": "962810"
  },
  {
    "text": "we'll stick them into a\nsoftmax, as we commonly do. And we'll then get a probability\ndistribution or a weighting",
    "start": "962810",
    "end": "970840"
  },
  {
    "text": "over the different\npositions in the sentence. And then we will\nuse this weighting",
    "start": "970840",
    "end": "976300"
  },
  {
    "text": "to compute a representation\nbased on the encoder, which",
    "start": "976300",
    "end": "983230"
  },
  {
    "text": "is then going to be a weighted\naverage of the encoder states.",
    "start": "983230",
    "end": "989660"
  },
  {
    "text": "So in this particular case,\nit will be nearly entirely the representation above\nthe first word \"il,\"",
    "start": "989660",
    "end": "996470"
  },
  {
    "text": "which means \"he\" in French. So then we'll take\nthat attention output. ",
    "start": "996470",
    "end": "1004480"
  },
  {
    "text": "And we'll combine it with\nhidden state of our decoder.",
    "start": "1004480",
    "end": "1010750"
  },
  {
    "text": "And we'll use both\nof them together to generate an\noutput vector, which",
    "start": "1010750",
    "end": "1016600"
  },
  {
    "text": "we stick through our\nsoftmax, and generate a word as the first word\nof the translation, y1.",
    "start": "1016600",
    "end": "1024189"
  },
  {
    "text": "And so then at that point,\nwe just repeat this over. ",
    "start": "1024190",
    "end": "1029880"
  },
  {
    "text": "So we then go on to\ngenerating the second word. Well, we copy down the\nfirst word generator",
    "start": "1029880",
    "end": "1037020"
  },
  {
    "text": "and start to generate\nthe second word. We work out attention\nat every position.",
    "start": "1037020",
    "end": "1042449"
  },
  {
    "text": "It gives us an-- sorry. There's a little\nnote there, which",
    "start": "1042450",
    "end": "1048797"
  },
  {
    "text": "is a little fine point, which\nmaybe I won't deal with. But it points out\nsometimes you also",
    "start": "1048797",
    "end": "1055350"
  },
  {
    "text": "do things like stick the\nprevious time step's attention output into the next\nstep as an extra input.",
    "start": "1055350",
    "end": "1062140"
  },
  {
    "text": "And we actually do that in-- it should say\nassignment 3 there. That's buggy.",
    "start": "1062140",
    "end": "1067500"
  },
  {
    "text": "So there are other\nways to use things, but I'll gloss over that. So we generate another word,\nand we sort of repeat over.",
    "start": "1067500",
    "end": "1077320"
  },
  {
    "text": "And at each time\nstep, we're looking at different words\nin the source,",
    "start": "1077320",
    "end": "1082380"
  },
  {
    "text": "and they will help us to\ntranslate the sentence. Yeah?",
    "start": "1082380",
    "end": "1087950"
  },
  {
    "text": "Why does the start words\npoint to the attention score? Why is that [INAUDIBLE]",
    "start": "1087950",
    "end": "1096568"
  },
  {
    "text": " Wait, wait. Say it again. So why is the green\npart pointing to--",
    "start": "1096568",
    "end": "1102700"
  },
  {
    "text": "'Cause the green-- OK, so the green vector, the\nhidden vector of the decoder,",
    "start": "1102700",
    "end": "1109540"
  },
  {
    "text": "is going to be used together\nwith the hidden states. The hidden vectors\nof the encode are",
    "start": "1109540",
    "end": "1116140"
  },
  {
    "text": "one at a time to calculate\nthe attention scores. So the attention\nscore at a position",
    "start": "1116140",
    "end": "1122769"
  },
  {
    "text": "is going to be a function\nof the hidden state of the decoder at that position\nand the current hidden state",
    "start": "1122770",
    "end": "1131769"
  },
  {
    "text": "of the decoder.  And I'll explain\nexactly how in a moment.",
    "start": "1131770",
    "end": "1139600"
  },
  {
    "text": "Any other questions?  OK.",
    "start": "1139600",
    "end": "1146260"
  },
  {
    "text": "Well, so here it is in math. OK, so we have\nencoder hidden states,",
    "start": "1146260",
    "end": "1154190"
  },
  {
    "text": "which we're going to call h. So we have decoder\nhidden states,",
    "start": "1154190",
    "end": "1160429"
  },
  {
    "text": "which we're going to call s,\nso they're something different. And we're going\nto, at each point,",
    "start": "1160430",
    "end": "1166630"
  },
  {
    "text": "be in some particular\ntime step t, so we'll be dealing with st.\nSo to calculate the attention",
    "start": "1166630",
    "end": "1175870"
  },
  {
    "text": "scores for generating\nthe word for time step t,",
    "start": "1175870",
    "end": "1184450"
  },
  {
    "text": "we're going to\ncalculate an attention score for each position\nin the encoder.",
    "start": "1184450",
    "end": "1192280"
  },
  {
    "text": "OK, I'll discuss alternatives\nfor this in a moment. But the very easiest way to\ncalculate an attention score,",
    "start": "1192280",
    "end": "1200390"
  },
  {
    "text": "which is shown here, is\nto take a dot product between the hidden\nstate of the encoder",
    "start": "1200390",
    "end": "1208760"
  },
  {
    "text": "and the current hidden\nstate of the decoder. And so that's what\nwe're showing here.",
    "start": "1208760",
    "end": "1215070"
  },
  {
    "text": "So that will give us some\ndot product score, which is just any number at all.",
    "start": "1215070",
    "end": "1221300"
  },
  {
    "text": "Then the next thing we do\nis we stick those et scores into our softmax distribution.",
    "start": "1221300",
    "end": "1228420"
  },
  {
    "text": "And then that gives us our\nprobability distribution as to how much weight to put on\neach position in the encoder.",
    "start": "1228420",
    "end": "1237380"
  },
  {
    "text": "And so then, we are calculating\nthe weighted average of the encoder hidden\nstates, which we're just",
    "start": "1237380",
    "end": "1244610"
  },
  {
    "text": "doing with the obvious\nequation that we're taking the weighted sum of the\nhidden states of the encoder",
    "start": "1244610",
    "end": "1251720"
  },
  {
    "text": "based on the attention weights. And then what we want\nto do is concatenate",
    "start": "1251720",
    "end": "1259429"
  },
  {
    "text": "our attention output and the\nhidden state of the decoder.",
    "start": "1259430",
    "end": "1265500"
  },
  {
    "text": "And we're going to--\nwhich is giving us then a double-length vector. And then we're\ngoing to feed that",
    "start": "1265500",
    "end": "1270890"
  },
  {
    "text": "into producing the next\nword from the decoder. So typically that means\nwe're multiplying that vector",
    "start": "1270890",
    "end": "1278780"
  },
  {
    "text": "by another matrix and then\nputting it through a softmax",
    "start": "1278780",
    "end": "1284660"
  },
  {
    "text": "to get a probability\ndistribution over words to output and choosing the\nhighest probability word.",
    "start": "1284660",
    "end": "1292375"
  },
  {
    "text": " OK.",
    "start": "1292375",
    "end": "1297700"
  },
  {
    "text": "That makes sense, I hope. Yeah. OK, so attention is great.",
    "start": "1297700",
    "end": "1303260"
  },
  {
    "text": "So inventing this idea was\ncompletely transformative.",
    "start": "1303260",
    "end": "1309070"
  },
  {
    "text": "So the very first modern neural\nmachine translation system was done at Google in 2014.",
    "start": "1309070",
    "end": "1316370"
  },
  {
    "text": "And they used a pure but\nvery large, very deep LSTM.",
    "start": "1316370",
    "end": "1326620"
  },
  {
    "text": "So it's an eight-layer-deep LSTM\nwith a very large hidden state for the time.",
    "start": "1326620",
    "end": "1332430"
  },
  {
    "text": "And they were able\nto get good results. But very shortly\nthereafter, people",
    "start": "1332430",
    "end": "1340740"
  },
  {
    "text": "at the University of Montreal,\nD. Bahdanau, Kyunghyun Cho, and Yoshua Bengio\ndid a second version",
    "start": "1340740",
    "end": "1347880"
  },
  {
    "text": "of machine translation\nusing attention and with a much more\nmodest compute budget",
    "start": "1347880",
    "end": "1355740"
  },
  {
    "text": "of the kind that you can\nafford in universities. They were able to get better\nresults because attention",
    "start": "1355740",
    "end": "1361830"
  },
  {
    "text": "was their secret thing. So attention significantly\nimproved NMT performance.",
    "start": "1361830",
    "end": "1368529"
  },
  {
    "text": "And essentially every neural\nmachine translation system since has used attention\nlike we've just seen.",
    "start": "1368530",
    "end": "1376970"
  },
  {
    "text": "It's more human-like as I\nwas indicating because it's what a human would do. You'd look back in\nthe sentence to see",
    "start": "1376970",
    "end": "1382980"
  },
  {
    "text": "what you need to translate. And it solves this\nbottleneck problem. You now no longer have to\nstuff all the information",
    "start": "1382980",
    "end": "1390340"
  },
  {
    "text": "about the source sentence\ninto one hidden state. You can have the whole of\nyour representational space",
    "start": "1390340",
    "end": "1397539"
  },
  {
    "text": "from your entire encoding\nand use it as you need it. It also helps with the\nvanishing gradient problem.",
    "start": "1397540",
    "end": "1404539"
  },
  {
    "text": "This is connected to what\nI was saying last time when talking about\nresidual connections--",
    "start": "1404540",
    "end": "1409630"
  },
  {
    "text": "that a way out of the vanishing\ngradient problem is to direct connect things. And this provides\nshortcut connections",
    "start": "1409630",
    "end": "1417070"
  },
  {
    "text": "to all of the hidden\nstates of the encoder. And another nice thing\nthat attention does",
    "start": "1417070",
    "end": "1424149"
  },
  {
    "text": "is it gives you some\ninterpretability. So by looking at where\nthe model is attending,",
    "start": "1424150",
    "end": "1430550"
  },
  {
    "text": "you can basically see what it's\ntranslating at different time steps. And so that can\nbe really useful.",
    "start": "1430550",
    "end": "1438950"
  },
  {
    "text": "And so it's kind of like we\ncan see what we're translating where without explicitly having\ntrained a system that does that.",
    "start": "1438950",
    "end": "1447770"
  },
  {
    "text": "So for my little toy sentence\nhere, if \"he hit me with a pie,\" at the first position, it was\nlooking at the first word \"il,\"",
    "start": "1447770",
    "end": "1458020"
  },
  {
    "text": "\"he,\" which it translates. Then in French, there's this\nsort of verb, \"entarté.\"",
    "start": "1458020",
    "end": "1464710"
  },
  {
    "text": "It's a sort of pie, somewhat. I guess in English as well, you\ncan use pie as a verb, right?",
    "start": "1464710",
    "end": "1470530"
  },
  {
    "text": "So the \"a\" is a sort of\nperfect past auxiliary.",
    "start": "1470530",
    "end": "1477410"
  },
  {
    "text": "So it's sort of like\n\"he has me pied,\" is what the French words\nare, one at a time.",
    "start": "1477410",
    "end": "1482960"
  },
  {
    "text": "And so the \"hit\" is already\nlooking at the \"pied.\" Then the \"me\" is attending\nto the \"m',\" which means me.",
    "start": "1482960",
    "end": "1490390"
  },
  {
    "text": "And then the \"pie\" is\nattending still to \"antarté,\" which is basically the right\nkind of alignment that you want",
    "start": "1490390",
    "end": "1498130"
  },
  {
    "text": "for words of a sentence. So that's pretty cool, too.",
    "start": "1498130",
    "end": "1504160"
  },
  {
    "text": "OK. So I presented, up until\nthis point, just this--",
    "start": "1504160",
    "end": "1509970"
  },
  {
    "text": " said all we could\ndo a dot product.",
    "start": "1509970",
    "end": "1515480"
  },
  {
    "text": "But in general, there's\nmore to it than that. So what we have is we\nhave some values h1 to hN,",
    "start": "1515480",
    "end": "1523210"
  },
  {
    "text": "and we have a query vector. And we want to work\nout how to do attention",
    "start": "1523210",
    "end": "1528460"
  },
  {
    "text": "based on these things. So attention always involves\ncomputing some attention scores",
    "start": "1528460",
    "end": "1535510"
  },
  {
    "text": "and taking the softmax to\nget an attention distribution and then getting an\nattention output.",
    "start": "1535510",
    "end": "1542720"
  },
  {
    "text": "But the part where\nthere's variation is how do you compute\nthese attention scores.",
    "start": "1542720",
    "end": "1548120"
  },
  {
    "text": "And a number of different\nways have been done for that. And I just want to go\nthrough that a little bit.",
    "start": "1548120",
    "end": "1556060"
  },
  {
    "text": "So the simplest way\nthat I just presented is this dot-product attention.",
    "start": "1556060",
    "end": "1561980"
  },
  {
    "text": "We just take the\nhidden states and dot product, the whole of them. ",
    "start": "1561980",
    "end": "1567990"
  },
  {
    "text": "That sort of works, but it\ndoesn't actually work great.",
    "start": "1567990",
    "end": "1573040"
  },
  {
    "text": "And I discussed this a bit when\ntalking about LSTMs last time,",
    "start": "1573040",
    "end": "1578090"
  },
  {
    "text": "right? That the hidden state of an LSTM\nis it's complete memory, right?",
    "start": "1578090",
    "end": "1585360"
  },
  {
    "text": "So it has to variously store\nlots of things in that memory. It's got to be storing\ninformation that will help it",
    "start": "1585360",
    "end": "1592710"
  },
  {
    "text": "output the right word. It has to be storing\ninformation about the future,",
    "start": "1592710",
    "end": "1598480"
  },
  {
    "text": "about other things\nthat you want to say, given the kind of\nsentence, context, grammar,",
    "start": "1598480",
    "end": "1604560"
  },
  {
    "text": "and previous words you've said. It's got all kinds of memory. And so it sort of makes\nsense that some of it",
    "start": "1604560",
    "end": "1612480"
  },
  {
    "text": "would be useful for linking up,\nfor looking back, and some of it",
    "start": "1612480",
    "end": "1618600"
  },
  {
    "text": "would be less useful. You sort of want to\nfind the parts that are related to what you\nwant to say immediately,",
    "start": "1618600",
    "end": "1625150"
  },
  {
    "text": "not all the parts. And do all of the\nrest of the future.",
    "start": "1625150",
    "end": "1630760"
  },
  {
    "text": "So that suggests that maybe you\ncould do a more general form",
    "start": "1630760",
    "end": "1637570"
  },
  {
    "text": "of attention. And so Tung Luong and\nme in 2015 suggested",
    "start": "1637570",
    "end": "1644470"
  },
  {
    "text": "maybe we could introduce what we\ncall bilinear attention, which",
    "start": "1644470",
    "end": "1650080"
  },
  {
    "text": "I still think is a better\nname, but the rest of the world came to call\nmultiplicative attention,",
    "start": "1650080",
    "end": "1656590"
  },
  {
    "text": "where what we're doing is\nbetween these two vectors, we're sticking a matrix.",
    "start": "1656590",
    "end": "1663310"
  },
  {
    "text": "And so we're then learning\nthe parameters of this matrix, just like everything else\nin our neural network.",
    "start": "1663310",
    "end": "1670490"
  },
  {
    "text": "And so, effectively,\nthis matrix can learn which parts of\nthe generator hidden",
    "start": "1670490",
    "end": "1680740"
  },
  {
    "text": "state you should be\nlooking to find where in the hidden states\nof the encoder.",
    "start": "1680740",
    "end": "1687580"
  },
  {
    "text": "In particular, it\nno longer requires that things have to match\nup dimension by dimension.",
    "start": "1687580",
    "end": "1693080"
  },
  {
    "text": "It could be the case that the\nencoder is storing information about word meaning here.",
    "start": "1693080",
    "end": "1700520"
  },
  {
    "text": "And the decoder is\nstoring information about word meaning here. And by learning appropriate\nparameters in this matrix,",
    "start": "1700520",
    "end": "1708889"
  },
  {
    "text": "we can match those together\nand work out the right place to pay attention.",
    "start": "1708890",
    "end": "1714390"
  },
  {
    "text": "So that seemed kind of\na cool approach to us.",
    "start": "1714390",
    "end": "1721030"
  },
  {
    "text": "Yeah? Why don't you go all\nin with this idea and even build a neural\nnetwork that's been an h as an input and output?",
    "start": "1721030",
    "end": "1729100"
  },
  {
    "text": "You can do that. I was going to get to\nthat on the next slide. Actually, that's in a way\nsort of going backwards.",
    "start": "1729100",
    "end": "1736279"
  },
  {
    "text": "But I will get to it\non the next slide. But before I do that, now I'm\nshow you these other versions.",
    "start": "1736280",
    "end": "1747750"
  },
  {
    "text": "So the one thing you might\nwonder about doing it",
    "start": "1747750",
    "end": "1753680"
  },
  {
    "text": "this way is there's\na lot of parameters that you have to\nlearn in the matrix",
    "start": "1753680",
    "end": "1759740"
  },
  {
    "text": "W. There aren't that\nmany in my example because there are\nonly 36, but that's because my hidden states\nare only of length 6, right?",
    "start": "1759740",
    "end": "1767519"
  },
  {
    "text": "And if your hidden states\nare of length, 1,000 say, then you've got a million\nparameters in that W matrix.",
    "start": "1767520",
    "end": "1775800"
  },
  {
    "text": "And that seems like it might\nbe kind of problematic. And so the way to\nget beyond that,",
    "start": "1775800",
    "end": "1784410"
  },
  {
    "text": "which was fairly quickly\nsuggested thereafter, is, well, maybe rather than\nhaving that whole big matrix",
    "start": "1784410",
    "end": "1790610"
  },
  {
    "text": "in the middle, instead\nwhat we could do is form it as a low rank matrix.",
    "start": "1790610",
    "end": "1796770"
  },
  {
    "text": "And the easy way to\nmake a low rank matrix is you take two skinny\nmatrices like this,",
    "start": "1796770",
    "end": "1802679"
  },
  {
    "text": "where this is the\nrank of the pieces and multiply them\ntogether, which would give us the big matrix\nthat I showed on the last slide.",
    "start": "1802680",
    "end": "1811420"
  },
  {
    "text": "And so this gives you\na low-parameter version of the bilinear attention\nmatrix from the last slide.",
    "start": "1811420",
    "end": "1821110"
  },
  {
    "text": "But at that point,\nif you just do a teeny bit of linear\nalgebra, this computation",
    "start": "1821110",
    "end": "1828630"
  },
  {
    "text": "is exactly the same\nas saying, well, what I'm going to do is I'm\ngoing to take each of these two",
    "start": "1828630",
    "end": "1835020"
  },
  {
    "text": "vectors and project them to a\nlower-dimensional space using",
    "start": "1835020",
    "end": "1840240"
  },
  {
    "text": "this low rank\ntransformation matrix. And then I'm going to\ntake the dot product",
    "start": "1840240",
    "end": "1845850"
  },
  {
    "text": "in this low-dimensional space. And on Thursday, when you get to\ntransformers, what you will see",
    "start": "1845850",
    "end": "1856500"
  },
  {
    "text": "that transformers do is this-- that they're taking\nthe big vector",
    "start": "1856500",
    "end": "1863010"
  },
  {
    "text": "and they're projecting it\nto a low-dimensional space and then taking\ndot-product attention",
    "start": "1863010",
    "end": "1869730"
  },
  {
    "text": "in that low-dimensional space. OK, back to the question.",
    "start": "1869730",
    "end": "1876060"
  },
  {
    "text": "Yeah, you're totally right. And at this point, I'm going\nin an ahistorical manner",
    "start": "1876060",
    "end": "1885600"
  },
  {
    "text": "because, yeah, actually the\nfirst form of attention that was proposed in the Bahdanau et al.",
    "start": "1885600",
    "end": "1892050"
  },
  {
    "text": "paper was, hey, let's just\nstick a little neural net there",
    "start": "1892050",
    "end": "1899520"
  },
  {
    "text": "to calculate attention scores. So we take the s and the h.",
    "start": "1899520",
    "end": "1906460"
  },
  {
    "text": "We multiply them\nboth by a matrix, add them, put them through a\ntanh, multiply that by a vector,",
    "start": "1906460",
    "end": "1913650"
  },
  {
    "text": "and we get a number. This looks just like\nthe kind of computations we've used everywhere\nelse in an LSTM.",
    "start": "1913650",
    "end": "1920620"
  },
  {
    "text": "So there's a little neural net\nthat's calculating the attention scores, and then they go into\na softmax as useful, usual.",
    "start": "1920620",
    "end": "1929159"
  },
  {
    "text": "In most of the\nliterature, this is called additive attention,\nwhich also seems to me a really weird name.",
    "start": "1929160",
    "end": "1934340"
  },
  {
    "text": "I mean, I think kind of saying\nyou've got a little neural net. Makes more sense for that one.",
    "start": "1934340",
    "end": "1942250"
  },
  {
    "text": "But anyway, so this is what\nthey proposed and used.",
    "start": "1942250",
    "end": "1947440"
  },
  {
    "text": "And at this point, it's a little\nbit complex, to be honest.",
    "start": "1947440",
    "end": "1954105"
  },
  {
    "start": "1954105",
    "end": "1959650"
  },
  {
    "text": "When we wrote our\npaper the next year, we had found that the bilinear\nattention worked better for us.",
    "start": "1959650",
    "end": "1967470"
  },
  {
    "text": "But there is subsequent\nwork, especially this massive exploration of\nneural machine translation",
    "start": "1967470",
    "end": "1973380"
  },
  {
    "text": "architectures, that\nargued that, actually, with the right kinds of good\nhyperparameter optimization--",
    "start": "1973380",
    "end": "1984410"
  },
  {
    "text": "actually, this is the\nbest kind-- this is better than the bilinear attention.",
    "start": "1984410",
    "end": "1989750"
  },
  {
    "text": "But this is a lot more\ncomplex and a lot slower than doing what you're doing\nin the upper part of the chart.",
    "start": "1989750",
    "end": "1998090"
  },
  {
    "text": "So regardless of whether\nit's better or not, in practice, what's\ncompletely won is doing this.",
    "start": "1998090",
    "end": "2004000"
  },
  {
    "text": "And this is what\ntransformers use and just about all other neural\nnets that are used these days.",
    "start": "2004000",
    "end": "2011520"
  },
  {
    "text": "OK. Questions on attention will\nbe found in assignment 3.",
    "start": "2011520",
    "end": "2016680"
  },
  {
    "text": "Yeah. So I won't say much\nmore about this now.",
    "start": "2016680",
    "end": "2021750"
  },
  {
    "text": "And we'll see more of\nit just next lecture. But attention is a\nvery general technique.",
    "start": "2021750",
    "end": "2028830"
  },
  {
    "text": "It was a great way to\nimprove machine translation, and that was how it\nwas first invented.",
    "start": "2028830",
    "end": "2035200"
  },
  {
    "text": "But for all kinds of\nneural architectures, for all kinds of purposes,\nyou can stick attention to",
    "start": "2035200",
    "end": "2042760"
  },
  {
    "text": "into them. And the general finding was\nthat always improved results. So in general, anywhere where\nyou have a vector of values,",
    "start": "2042760",
    "end": "2052339"
  },
  {
    "text": "a vector query, and\nyou can use attention to then get a weighted\naverage of the values, which",
    "start": "2052340",
    "end": "2059830"
  },
  {
    "text": "finds relevant information\nthat you can use to improve your performance.",
    "start": "2059830",
    "end": "2066360"
  },
  {
    "text": "And so maybe I won't try and\neven give examples of that now. But you'll see another\nexample of attention",
    "start": "2066360",
    "end": "2074879"
  },
  {
    "text": "immediately when we do things\non Thursday where we then",
    "start": "2074880",
    "end": "2080863"
  },
  {
    "text": "start doing self-attention\ninside transformers. Yes? In providing your case, did\nyou also try nonlinearity?",
    "start": "2080864",
    "end": "2089969"
  },
  {
    "text": "No, we did not. [INAUDIBLE]",
    "start": "2089969",
    "end": "2096230"
  },
  {
    "start": "2096230",
    "end": "2102490"
  },
  {
    "text": "I mean, it didn't seem\nespecially necessary. I don't know. But no, we did not. OK, well, this is the end\nof the part with attention.",
    "start": "2102490",
    "end": "2114470"
  },
  {
    "text": "Are there any other questions? Yes?",
    "start": "2114470",
    "end": "2120090"
  },
  {
    "text": "For the RNN attention\nstuff, is there a need for position\ninformation or is that not",
    "start": "2120090",
    "end": "2126000"
  },
  {
    "text": "required to solve the-- Is there a need for? Positional information. ",
    "start": "2126000",
    "end": "2134700"
  },
  {
    "text": "So there was none. And it seemed like it\nwasn't very required.",
    "start": "2134700",
    "end": "2140710"
  },
  {
    "text": "I mean, you could-- yeah, I mean, you\ncould make some--",
    "start": "2140710",
    "end": "2147089"
  },
  {
    "text": "you could make some argument\nthat maybe position information might have been useful.",
    "start": "2147090",
    "end": "2153760"
  },
  {
    "text": "But there's also a good argument\nthat it wasn't necessary. And the sort of recent\neverywhere usage",
    "start": "2153760",
    "end": "2162630"
  },
  {
    "text": "of positional information\nonly becomes necessary when you get to a transformer.",
    "start": "2162630",
    "end": "2168730"
  },
  {
    "text": "And the reason for that is,\ngoing back to the pictures,",
    "start": "2168730",
    "end": "2175950"
  },
  {
    "text": "for these encoder states,\nthey are being calculated",
    "start": "2175950",
    "end": "2181349"
  },
  {
    "text": "with respect to the\nprevious encoder state because it's a recurrent\nneural network.",
    "start": "2181350",
    "end": "2187350"
  },
  {
    "text": "And therefore the\nrepresentation here knows something about the past.",
    "start": "2187350",
    "end": "2192520"
  },
  {
    "text": "So it kind of knows what\nposition it's in, basically. And so that's giving a\nlot of that information.",
    "start": "2192520",
    "end": "2200020"
  },
  {
    "text": "Or another way to think about\nit is this final representation will give a certain overall\nsense of the semantics",
    "start": "2200020",
    "end": "2206640"
  },
  {
    "text": "of the sentence. And so to the extent that\nyou're looking backwards, the more associative matching of\nsimilar semantic content that's",
    "start": "2206640",
    "end": "2215730"
  },
  {
    "text": "needed seems sufficient\nand you don't really need additional\npositional information. ",
    "start": "2215730",
    "end": "2223780"
  },
  {
    "text": "OK, I will go on. OK. So that's the neural\nnetworks content for today.",
    "start": "2223780",
    "end": "2233380"
  },
  {
    "text": "And so for the\nremaining 39 minutes, I want to talk final\nprojects but also a bit",
    "start": "2233380",
    "end": "2240550"
  },
  {
    "text": "about data experiments\nand things like that. OK, so this is a\nreminder on the class.",
    "start": "2240550",
    "end": "2246349"
  },
  {
    "text": "So we've got the four\nassignments, which are 48%.",
    "start": "2246350",
    "end": "2251450"
  },
  {
    "text": "And then the big other\npart of what you need to do is the final project,\nwhich is 49%,",
    "start": "2251450",
    "end": "2259060"
  },
  {
    "text": "almost completing things out\nexcept for the participation.",
    "start": "2259060",
    "end": "2264320"
  },
  {
    "text": "And let me just give one\nnote back to collaboration,",
    "start": "2264320",
    "end": "2269520"
  },
  {
    "text": "the honor code. I mean, for final\nprojects, it's quite usual",
    "start": "2269520",
    "end": "2275030"
  },
  {
    "text": "that people use all\nsorts of stuff that were written by other people. That's completely fine.",
    "start": "2275030",
    "end": "2280500"
  },
  {
    "text": "We don't expect you to implement\neverything from scratch. But you must document\nwhat you're using,",
    "start": "2280500",
    "end": "2287360"
  },
  {
    "text": "give references or URLs if\nyou're using other people's code rather than writing your own.",
    "start": "2287360",
    "end": "2292980"
  },
  {
    "text": "We do not want to know what\ncode you wrote yourself and what things you\ndownloaded from PyPI.",
    "start": "2292980",
    "end": "2298819"
  },
  {
    "text": "And in particular, in\nthinking about final projects, the question of interest for\nus is what value add did you",
    "start": "2298820",
    "end": "2307940"
  },
  {
    "text": "provide? So you haven't done\nsomething great if you've downloaded a\nreally good neural network",
    "start": "2307940",
    "end": "2314059"
  },
  {
    "text": "and run it on some data and it\nproduces really good results. That's not much value add.",
    "start": "2314060",
    "end": "2320000"
  },
  {
    "text": "So if you want to have\nvalue add in that context, you at least want to\nbe doing something",
    "start": "2320000",
    "end": "2325380"
  },
  {
    "text": "interesting about understanding\nwhy it works so well,",
    "start": "2325380",
    "end": "2330700"
  },
  {
    "text": "what kind of examples. It doesn't work well on doing\nsome thorough experimental analysis.",
    "start": "2330700",
    "end": "2336446"
  },
  {
    "start": "2336446",
    "end": "2341640"
  },
  {
    "text": "Yeah, a couple of\nother points there. OK, so for the final\nproject for this class,",
    "start": "2341640",
    "end": "2350340"
  },
  {
    "text": "there's a binary choice. You can either do our\ndefault final project, which",
    "start": "2350340",
    "end": "2356970"
  },
  {
    "text": "I'll talk about\nmore a bit later, or you can come up with\nyour own final project,",
    "start": "2356970",
    "end": "2361980"
  },
  {
    "text": "and I'll talk about\nthat a bit too. So we allow team\nsizes of one to three.",
    "start": "2361980",
    "end": "2369600"
  },
  {
    "text": "The complicated\nthing that comes up-- ",
    "start": "2369600",
    "end": "2375403"
  },
  {
    "text": "actually, sorry. I should say the\nother point first. Yeah, so if you do--",
    "start": "2375403",
    "end": "2381250"
  },
  {
    "text": "we generally encourage\npeople to form teams. It means that you can do\nsomething more interesting.",
    "start": "2381250",
    "end": "2386960"
  },
  {
    "text": "It's more motivational. You can make friends, whatever. So teams are good.",
    "start": "2386960",
    "end": "2392920"
  },
  {
    "text": "On expectations for teams,\nour expectation for teams",
    "start": "2392920",
    "end": "2398380"
  },
  {
    "text": "is that a bigger team\nshould be able to do proportionally more work now.",
    "start": "2398380",
    "end": "2403580"
  },
  {
    "text": "And so when we're\ngrading things, we expect to see more\nwork from larger teams.",
    "start": "2403580",
    "end": "2410920"
  },
  {
    "text": "Now, how this works\nout is kind of-- I will admit-- a little bit\ncomplicated because there's",
    "start": "2410920",
    "end": "2418600"
  },
  {
    "text": "sort of a quality issue that's\nseparate from the amount of work.",
    "start": "2418600",
    "end": "2424930"
  },
  {
    "text": "So the reality is that\nit's just always the case that several of the\nvery best projects",
    "start": "2424930",
    "end": "2432820"
  },
  {
    "text": "are one-person efforts\nbecause they're just somebody who has a good idea\nand knows what they want to do",
    "start": "2432820",
    "end": "2439640"
  },
  {
    "text": "and does it by themselves. And it is great. But they're also great\nmulti-person projects as well.",
    "start": "2439640",
    "end": "2446850"
  },
  {
    "text": "But the point I'm\nmeaning is, well, it doesn't work if you're\na one-person project",
    "start": "2446850",
    "end": "2454310"
  },
  {
    "text": "and you're trying to attempt\na huge amount of stuff and you can only get one\nthird of the way through it.",
    "start": "2454310",
    "end": "2460370"
  },
  {
    "text": "That's not a good recipe for\ndoing well in the final project. For any project, you really\nneed to be completing something",
    "start": "2460370",
    "end": "2469250"
  },
  {
    "text": "and showing something. But nevertheless, if\nyou're a one person",
    "start": "2469250",
    "end": "2474650"
  },
  {
    "text": "and you can show something\nkind of interesting, even if our reaction\nis, well, this",
    "start": "2474650",
    "end": "2481898"
  },
  {
    "text": "would have been much\nbetter if they'd shown it was better than\nthis other kind of model, or it would have\nbeen really nice",
    "start": "2481898",
    "end": "2487670"
  },
  {
    "text": "if they'd run ablations\nto work things out. Well, if you're one person\nwe'll give you a bye",
    "start": "2487670",
    "end": "2493370"
  },
  {
    "text": "and say, oh, but it was only\none person, whereas if you're a three-person team and it seems\nlike you obviously should have",
    "start": "2493370",
    "end": "2501410"
  },
  {
    "text": "compared it to some other\nmodels and you obviously could have run it on some other\ndata sets, then we'll feel like,",
    "start": "2501410",
    "end": "2508109"
  },
  {
    "text": "well, it's a three-person team. They obviously should have done\nthat, and therefore, we should give them a less good score.",
    "start": "2508110",
    "end": "2515420"
  },
  {
    "text": "And that's how\nthat is worked out. The complication comes\nwith other things people",
    "start": "2515420",
    "end": "2523460"
  },
  {
    "text": "are doing at the same time. We allow people to do\nfinal projects that are",
    "start": "2523460",
    "end": "2528830"
  },
  {
    "text": "shared with multiple classes. But our expectation is, again,\nthat you'll do more work.",
    "start": "2528830",
    "end": "2535650"
  },
  {
    "text": "So if there are two of you who\nare using one project for both this class and\nCS231N, say, then it's",
    "start": "2535650",
    "end": "2542900"
  },
  {
    "text": "sort of like a\nfour-person project and you should be doing\na lot of work for it.",
    "start": "2542900",
    "end": "2549350"
  },
  {
    "text": "There are other cases. Sometimes people have RAships\nor they're PhD rotation students",
    "start": "2549350",
    "end": "2555860"
  },
  {
    "text": "or other things. If you're doing it\nfor other things, we'd like you to tell us. And we expect you to be\ndoing more work for it.",
    "start": "2555860",
    "end": "2565730"
  },
  {
    "text": "OK. I'm very happy to talk to\npeople about final projects and have been talking to\npeople about final projects.",
    "start": "2565730",
    "end": "2572340"
  },
  {
    "text": "But unfortunately\nthere's only one of me, so I definitely can't talk to\n500 people about final projects.",
    "start": "2572340",
    "end": "2578340"
  },
  {
    "text": "So I do also\nencourage you to talk to all of the TAs\nabout final projects.",
    "start": "2578340",
    "end": "2583590"
  },
  {
    "text": "So on the office hours\npage, under all of the TAs, there's some information about\nthings that they know about.",
    "start": "2583590",
    "end": "2591869"
  },
  {
    "text": "So if you know what\nyour project is about, you could at least try and\nfind one of the most useful TAs",
    "start": "2591870",
    "end": "2597290"
  },
  {
    "text": "or just find a TA\nwith a friendly face. Whatever mechanism you use, talk\nto TAs about final projects.",
    "start": "2597290",
    "end": "2606430"
  },
  {
    "text": "Yeah. So default final project. So what it's going to be is--",
    "start": "2606430",
    "end": "2612250"
  },
  {
    "text": "so BERT was a famous\nearly transformer. And we're going to be\nbuilding and experimenting",
    "start": "2612250",
    "end": "2620470"
  },
  {
    "text": "with a minimal BERT\nimplementation. So if you do this,\nthere's a part",
    "start": "2620470",
    "end": "2628450"
  },
  {
    "text": "of an implementation\nof BERT, and you're meant to finish it off. And you're meant to fine-tune\nit and get some data results",
    "start": "2628450",
    "end": "2636340"
  },
  {
    "text": "for doing sentiment analysis. And then, basically, we want\neven the default final project",
    "start": "2636340",
    "end": "2644560"
  },
  {
    "text": "to be an open-ended\nproject where people can do different things. And so then, there's\nlots of other ideas,",
    "start": "2644560",
    "end": "2650859"
  },
  {
    "text": "or you can come up\nwith your own of ways you could extend this\nsystem and make it better,",
    "start": "2650860",
    "end": "2657559"
  },
  {
    "text": "which might be with paraphrasing\ncontrastive learning, low rank adaptation, something,\nand you can do something.",
    "start": "2657560",
    "end": "2664610"
  },
  {
    "text": "And that is your final project. So why choose the final project?",
    "start": "2664610",
    "end": "2672119"
  },
  {
    "text": "So if you haven't had much\nexperience with research, you don't have any\nreal idea of what",
    "start": "2672120",
    "end": "2679460"
  },
  {
    "text": "you want to do for\na final project, or you'd like something\nwith clear guidance",
    "start": "2679460",
    "end": "2685040"
  },
  {
    "text": "and a goal and a leaderboard\nbecause we provide a leaderboard for\npeople doing the default",
    "start": "2685040",
    "end": "2690620"
  },
  {
    "text": "final project of how\ngood your performance is on the tasks we provide.",
    "start": "2690620",
    "end": "2696360"
  },
  {
    "text": "Then you can do\nthe final project. And I mean, honestly, I\nthink, for many people, the best option is to\ndo the final project.",
    "start": "2696360",
    "end": "2704900"
  },
  {
    "text": "For past performance, typically\nabout half the students do the final project,\nincluding some people who",
    "start": "2704900",
    "end": "2712789"
  },
  {
    "text": "start off thinking, I'll\ndo a custom final project. Then after a couple\nof weeks, they",
    "start": "2712790",
    "end": "2718069"
  },
  {
    "text": "decide, huh, this makes no\nsense what I was suggesting. It's not working at all. I'm just going to\nabandon and flip",
    "start": "2718070",
    "end": "2723769"
  },
  {
    "text": "to the default final project. OK. But we also allow\ncustom final projects.",
    "start": "2723770",
    "end": "2730490"
  },
  {
    "text": "And there are good reasons\nto do custom final projects. So if you have some\ntopic or research",
    "start": "2730490",
    "end": "2737170"
  },
  {
    "text": "idea that you're excited\nabout, maybe you're already even\nworking on it or you",
    "start": "2737170",
    "end": "2742600"
  },
  {
    "text": "want to try something\ndifferent on your own, or you just like to have more\nof the experience of trying",
    "start": "2742600",
    "end": "2749950"
  },
  {
    "text": "to come up with a research\ngoal, finding the necessary data and tools, and starting from\nscratch, which is actually",
    "start": "2749950",
    "end": "2756670"
  },
  {
    "text": "very educational, if\nconsiderably harder, well then, the custom final\nproject is fine for you.",
    "start": "2756670",
    "end": "2765430"
  },
  {
    "text": "Restriction on topics. I think we'd already sort\nof signaled this on Ed.",
    "start": "2765430",
    "end": "2770950"
  },
  {
    "text": "We insist for CS224n\nfinal projects",
    "start": "2770950",
    "end": "2776079"
  },
  {
    "text": "that they have to\nsubstantively involve both human language\nand neural networks",
    "start": "2776080",
    "end": "2782410"
  },
  {
    "text": "because this is the NLP class. So we'd like people\nto know and learn",
    "start": "2782410",
    "end": "2788480"
  },
  {
    "text": "something about human language. I'm totally aware\nof the fact that you can use these same models\nfor bioinformatics sequencers",
    "start": "2788480",
    "end": "2798109"
  },
  {
    "text": "or music or radar, whatever. But we'd like you\nto do something",
    "start": "2798110",
    "end": "2804710"
  },
  {
    "text": "with human language\nfor this class. That doesn't mean it has to\nbe only about human language.",
    "start": "2804710",
    "end": "2811290"
  },
  {
    "text": "So people have done things like\nvisual language models or music",
    "start": "2811290",
    "end": "2816620"
  },
  {
    "text": "and language. So it can have a\ncombination of modalities,",
    "start": "2816620",
    "end": "2822029"
  },
  {
    "text": "but it has to, substantively,\nnot completely trivially, involve human language.",
    "start": "2822030",
    "end": "2828630"
  },
  {
    "text": "If you've got any\nquestions about that, ask. And it also has to substantively\ninvolve neural networks.",
    "start": "2828630",
    "end": "2834059"
  },
  {
    "text": "So again, it doesn't have to be\nwholly about neural networks. If you've got some\nideas thinking, oh, I think I could show using\nkernel machines that they work",
    "start": "2834060",
    "end": "2842660"
  },
  {
    "text": "just as well as having\nmulti-layer neural networks or something like that, that's\nof course fine to do as well.",
    "start": "2842660",
    "end": "2850790"
  },
  {
    "text": "Gamesmanship. The default final\nproject is more guided.",
    "start": "2850790",
    "end": "2856559"
  },
  {
    "text": "But it's not meant to be\na complete slacker's ride. We're hoping that people\ndo the same amount of work",
    "start": "2856560",
    "end": "2863210"
  },
  {
    "text": "for either kind of project. But on the other hand, it\ndoes give you a clearer focus",
    "start": "2863210",
    "end": "2869960"
  },
  {
    "text": "and course of things to do. But it's still an\nopen-ended project.",
    "start": "2869960",
    "end": "2876410"
  },
  {
    "text": "So for both default\nfinal projects and custom final projects,\nthere are great projects.",
    "start": "2876410",
    "end": "2883920"
  },
  {
    "text": "And there are\nnot-so-great projects. If anything, there's\na bit more variance",
    "start": "2883920",
    "end": "2890150"
  },
  {
    "text": "in the custom final project. So the path of success\nis not to try and do",
    "start": "2890150",
    "end": "2897050"
  },
  {
    "text": "something for a custom\nfinal project that just looks really weak\ncompared to people's default",
    "start": "2897050",
    "end": "2903380"
  },
  {
    "text": "final projects. OK. You can get good\ngrades either way.",
    "start": "2903380",
    "end": "2910340"
  },
  {
    "text": "We give best project awards\nto both kinds of projects. So yeah, it's really not\nthat there's some secret one",
    "start": "2910340",
    "end": "2917000"
  },
  {
    "text": "you have to pick. Computing. Yeah, so-- to be honest,\nwith the confessions",
    "start": "2917000",
    "end": "2926819"
  },
  {
    "text": "right at the beginning,\nwe're actually in a less good\nposition for computing",
    "start": "2926820",
    "end": "2932160"
  },
  {
    "text": "than we've been in recent years. And it's all OpenAI's\nfault. Now, that part is--",
    "start": "2932160",
    "end": "2938230"
  },
  {
    "text": "[LAUGHS] But up until and\nincluding last year,",
    "start": "2938230",
    "end": "2944730"
  },
  {
    "text": "we actually had\ninvariably managed to get very generous cloud\ncomputing giveaways from one",
    "start": "2944730",
    "end": "2953460"
  },
  {
    "text": "or other cloud\ncomputing provider, which really provided a\nlot of computing support.",
    "start": "2953460",
    "end": "2961870"
  },
  {
    "text": "But there's the\ngreat GPU shortage on at the moment due to the\ngreat success of large language",
    "start": "2961870",
    "end": "2967920"
  },
  {
    "text": "models. And it turns out that cloud\ncompute providers just aren't being as generous\nas they used to be.",
    "start": "2967920",
    "end": "2974559"
  },
  {
    "text": "And gee, I guess the\nAWS rep was pointing out that my course was the single\nlargest grant of free GPU",
    "start": "2974560",
    "end": "2984090"
  },
  {
    "text": "last year, so it's\ngetting harder to do. So really people will\nhave to patch things",
    "start": "2984090",
    "end": "2992920"
  },
  {
    "text": "together more in many cases. And so we'll be relying on\nthe ingenuity of students",
    "start": "2992920",
    "end": "2999370"
  },
  {
    "text": "to be able to find\nfree and cheap stuff. So Google is giving $50 credit\nper person on GCP, which",
    "start": "2999370",
    "end": "3008520"
  },
  {
    "text": "can be used for assignments\n3, 4, and the final project. On all the clouds,\nif you haven't",
    "start": "3008520",
    "end": "3015120"
  },
  {
    "text": "used a cloud with\nan account before, you can usually get some\nfree starter credits,",
    "start": "3015120",
    "end": "3021780"
  },
  {
    "text": "which can be a useful thing. There are the sort of Jupyter\nNotebooks in the cloud.",
    "start": "3021780",
    "end": "3028450"
  },
  {
    "text": "So the most used\none is Google Colab, which allows limited GPU use.",
    "start": "3028450",
    "end": "3035850"
  },
  {
    "text": "It often tends to get\ntighter later in the quarter, so you might find\nit a good investment",
    "start": "3035850",
    "end": "3042750"
  },
  {
    "text": "to not have a couple of\nlattes and pay $10 a month to get Colab Pro, which gives\nyou much better access to GPUs.",
    "start": "3042750",
    "end": "3052213"
  },
  {
    "text": "But there are\nalternatives to that which you might also want to look at. So AWS provides a Jupyter\nNotebook environment SageMaker",
    "start": "3052213",
    "end": "3059549"
  },
  {
    "text": "Studio Lab. And also owned by Google,\nKaggle separately provides",
    "start": "3059550",
    "end": "3066390"
  },
  {
    "text": "Kaggle Notebooks, which actually\ncommonly give you better GPU access than Google\nColab provides,",
    "start": "3066390",
    "end": "3074050"
  },
  {
    "text": "even though they're\notherwise not as nice. [LAUGHS] Kaggle Notebooks\nare just bare-bones Jupyter",
    "start": "3074050",
    "end": "3083340"
  },
  {
    "text": "Notebooks, whereas Colab\nhas some fancier UI stuff grafted on it.",
    "start": "3083340",
    "end": "3089040"
  },
  {
    "text": "So other possibilities. Modal is a low-priced\nGPU provider and allows a certain amount\nof free GPU usage a month,",
    "start": "3089040",
    "end": "3098380"
  },
  {
    "text": "so that could be handy. There are other lower-cost\nGPU providers like fast.ai,",
    "start": "3098380",
    "end": "3103890"
  },
  {
    "text": "which could be of relevance. And then the other\nthing that I'll say more about in a minute is\nthe way things have changed",
    "start": "3103890",
    "end": "3111800"
  },
  {
    "text": "with large language models,\nthere are lots of projects that you might want to do where\nyou're not actually building",
    "start": "3111800",
    "end": "3118400"
  },
  {
    "text": "models at all yourself but\nyou're wanting to do experiments",
    "start": "3118400",
    "end": "3123799"
  },
  {
    "text": "on large language\nmodels or you're wanting to do in-context\nlearning with large language",
    "start": "3123800",
    "end": "3129200"
  },
  {
    "text": "models or other\nthings of that sort. And then what you\nwant is to have access",
    "start": "3129200",
    "end": "3136850"
  },
  {
    "text": "to large language models. And in particular, you probably\nwant to have API access,",
    "start": "3136850",
    "end": "3142680"
  },
  {
    "text": "so you can automate things. So another thing that\nwe have been able to get is through the generosity\nof Together AI,",
    "start": "3142680",
    "end": "3150089"
  },
  {
    "text": "that Together AI is\nproviding $50 of API access to large language models,\nwhich can actually be a lot.",
    "start": "3150090",
    "end": "3159619"
  },
  {
    "text": "How much of a lot it is depends\non how big a model you're using. So something you should think\nabout is how big a model do you",
    "start": "3159620",
    "end": "3167280"
  },
  {
    "text": "really need to use to show\nsomething because if you can run a 7-billion-parameter\nlanguage model on Together,",
    "start": "3167280",
    "end": "3175440"
  },
  {
    "text": "you can put a huge number of\ntokens through it for $50, whereas if you want to\nrun a much bigger model,",
    "start": "3175440",
    "end": "3182350"
  },
  {
    "text": "then the number of tokens\nstarts you can get through, it goes down by\norders of magnitude.",
    "start": "3182350",
    "end": "3189030"
  },
  {
    "text": "So that's good. And I mentioned some other ones. So we've already put a whole\nbunch of documents up on Ed",
    "start": "3189030",
    "end": "3197040"
  },
  {
    "text": "that talk about these\ndifferent GPU options. So do look at those.",
    "start": "3197040",
    "end": "3204050"
  },
  {
    "text": "OK, jumping ahead. So the first thing you have\nto do as a project proposal--",
    "start": "3204050",
    "end": "3210500"
  },
  {
    "text": "so it's one per team. So I guess the first step is\nto work out who your team is.",
    "start": "3210500",
    "end": "3216260"
  },
  {
    "text": "And so for the project\nproposal, part of it is actually giving us the\ndetails of your project.",
    "start": "3216260",
    "end": "3223680"
  },
  {
    "text": "But there's another\nmajor part of it, which is writing a\nreview of a key research",
    "start": "3223680",
    "end": "3230720"
  },
  {
    "text": "paper for your topic. For the default final project,\nwe provide some suggestions",
    "start": "3230720",
    "end": "3236640"
  },
  {
    "text": "so you can find something else. If you've got\nanother idea for how to extend the project\nfor your custom project,",
    "start": "3236640",
    "end": "3242670"
  },
  {
    "text": "you're finding your own. But what we want you to\ndo is get some practice at looking at a research paper,\nunderstanding what it's doing,",
    "start": "3242670",
    "end": "3251070"
  },
  {
    "text": "understanding what's convincing,\nwhat it didn't consider, what it failed to do.",
    "start": "3251070",
    "end": "3256680"
  },
  {
    "text": "And so we want you to write a\ntwo-page summary of a research paper.",
    "start": "3256680",
    "end": "3262050"
  },
  {
    "text": "And the goal is for\nyou to be thinking critically about this\nresearch paper, of what",
    "start": "3262050",
    "end": "3269099"
  },
  {
    "text": "did it do that was\nexciting versus what did it claim was exciting but\nwas really obvious or perhaps",
    "start": "3269100",
    "end": "3275369"
  },
  {
    "text": "even wrong, et cetera. OK. ",
    "start": "3275370",
    "end": "3281270"
  },
  {
    "text": "So after that, we want you to\nsay what you're planning to do.",
    "start": "3281270",
    "end": "3288360"
  },
  {
    "text": "That may be very straightforward\nfor a default final project, but it's really important\nfor a custom final project.",
    "start": "3288360",
    "end": "3297680"
  },
  {
    "text": "And in particular, tell us\nabout the literature you're",
    "start": "3297680",
    "end": "3304309"
  },
  {
    "text": "going to use, if any,\nand the kind of models you're going to explore. But it turns out that when\nwe're unhappy with custom",
    "start": "3304310",
    "end": "3312290"
  },
  {
    "text": "final projects, the two\ncommonest complaints about what you tell us about\ncustom final projects",
    "start": "3312290",
    "end": "3319160"
  },
  {
    "text": "is you don't make\nclear what data you're going to use because\nwe're worried already",
    "start": "3319160",
    "end": "3325920"
  },
  {
    "text": "if you haven't worked out by the\nproject proposal deadline what data you can use for\nyour final project",
    "start": "3325920",
    "end": "3331859"
  },
  {
    "text": "and if you don't\ntell us how you're going to evaluate your system. We want to know how you're\ngoing to measure whether you're",
    "start": "3331860",
    "end": "3339119"
  },
  {
    "text": "getting any success. As a new thing this\nyear, we'd like",
    "start": "3339120",
    "end": "3345210"
  },
  {
    "text": "you to include an\nethical considerations paragraph outlining potential\nethical challenges of your work",
    "start": "3345210",
    "end": "3352410"
  },
  {
    "text": "if it were deployed\nin the real world and how that might be mitigated. This is something that now a\nlot of conferences are requiring",
    "start": "3352410",
    "end": "3361920"
  },
  {
    "text": "and a lot of grants\nare requiring. So we want to give you a\nlittle bit of practice on that by writing a paragraph of that.",
    "start": "3361920",
    "end": "3368520"
  },
  {
    "text": "How much there is to talk\nabout varies somewhat on what you're trying\nto do and whether it",
    "start": "3368520",
    "end": "3375060"
  },
  {
    "text": "has a lot of ethical\nproblems or whether it's a fairly straightforward\nquestion-answering system.",
    "start": "3375060",
    "end": "3380770"
  },
  {
    "text": "But in all cases,\nyou might think about what are the possible\nethical considerations",
    "start": "3380770",
    "end": "3386850"
  },
  {
    "text": "of this piece of work. OK, the whole thing\nis maximum four pages.",
    "start": "3386850",
    "end": "3393750"
  },
  {
    "text": "OK. So for the research\npaper summary, yeah, do think critically.",
    "start": "3393750",
    "end": "3401560"
  },
  {
    "text": "I mean, the worst\nsummaries are essentially",
    "start": "3401560",
    "end": "3409420"
  },
  {
    "text": "people that just paraphrase\nwhat's in the abstract and introduction of the paper.",
    "start": "3409420",
    "end": "3414890"
  },
  {
    "text": "And we want you to think\na bit harder about this. What were the novel\ncontributions of the paper?",
    "start": "3414890",
    "end": "3423460"
  },
  {
    "text": "Is it something\nthat you could use for different kinds of\nproblems in different ways, or was it really exploiting\na trick of one data set?",
    "start": "3423460",
    "end": "3432430"
  },
  {
    "text": "Are there things that it seemed\nlike they missed or could have done differently\nor you weren't",
    "start": "3432430",
    "end": "3437770"
  },
  {
    "text": "convinced were done properly? Is it similar or\ndistinctive to other",
    "start": "3437770",
    "end": "3443920"
  },
  {
    "text": "papers that are dealing\nwith the same topic? Does it suggest\nperhaps something that you could try that\nextends beyond the paper?",
    "start": "3443920",
    "end": "3452760"
  },
  {
    "text": "OK. And for grading these\nfinal project proposals, most of the points are\non that paper review.",
    "start": "3452760",
    "end": "3461700"
  },
  {
    "text": "And so do pay attention to it. There are some points\non the project plan.",
    "start": "3461700",
    "end": "3468270"
  },
  {
    "text": "But really we're wanting\nto mainly give you formative feedback\non the project plan",
    "start": "3468270",
    "end": "3474950"
  },
  {
    "text": "and comments as to how we think\nit's realistic or unrealistic. But nevertheless,\nwe're expecting",
    "start": "3474950",
    "end": "3481910"
  },
  {
    "text": "you to have an idea, have\nthought through how you can investigate it, thought through\nhow you can evaluate it,",
    "start": "3481910",
    "end": "3489800"
  },
  {
    "text": "data sets, baselines,\nthings like that. Oh yeah, I should\nemphasize this. Do have an appropriate baseline.",
    "start": "3489800",
    "end": "3497069"
  },
  {
    "text": "So anything that\nyou're doing, you should have something you\ncan compare it against,",
    "start": "3497070",
    "end": "3503730"
  },
  {
    "text": "so sometimes it's\nthe previous system that exactly the same thing. But if you're doing something\nmore novel and interesting,",
    "start": "3503730",
    "end": "3510600"
  },
  {
    "text": "you should be thinking of some\nseat-of-the-pants obvious way",
    "start": "3510600",
    "end": "3515880"
  },
  {
    "text": "to do things and proving\nthat you can do it better. And what that is depends a\nlot on what your project is.",
    "start": "3515880",
    "end": "3521829"
  },
  {
    "text": "But if you're building some\ncomplex neural net that's",
    "start": "3521830",
    "end": "3527010"
  },
  {
    "text": "going to be used to work out\ntextural similarity between two pieces of text,\nwell, a simple way",
    "start": "3527010",
    "end": "3533609"
  },
  {
    "text": "of working out textural\nsimilarity between two pieces of text is to look\nup the word vectors",
    "start": "3533610",
    "end": "3539550"
  },
  {
    "text": "for every word in the text and\naverage them together and work out the dot product between\nthose average vectors.",
    "start": "3539550",
    "end": "3547540"
  },
  {
    "text": "And unless you're\ncomplex, neural network is significantly\nbetter than that. It doesn't seem like\nit's a very good system,",
    "start": "3547540",
    "end": "3554440"
  },
  {
    "text": "so you should always attempt\nto have some baselines. After the project\nproposal, we also",
    "start": "3554440",
    "end": "3562710"
  },
  {
    "text": "have a project milestone\nstuck in the middle to make sure everybody\nhas making some progress.",
    "start": "3562710",
    "end": "3569050"
  },
  {
    "text": "This is just to help make sure\npeople do get through things and keep working on it so\nwe'll have good final projects.",
    "start": "3569050",
    "end": "3577170"
  },
  {
    "text": "For most final projects-- I'll say more about\nthis in a minute--",
    "start": "3577170",
    "end": "3583020"
  },
  {
    "text": "the crucial thing we\nexpect for the milestone is that you've got set up\nand you can run something.",
    "start": "3583020",
    "end": "3591430"
  },
  {
    "text": "It might just be your baseline\nof looking up the word vectors. But it means you've got the data\nand the framework and something",
    "start": "3591430",
    "end": "3598080"
  },
  {
    "text": "that you can run and\nproduce a number from it. And then there's\nthe final project.",
    "start": "3598080",
    "end": "3605960"
  },
  {
    "text": "We have people submit their\ncode for the final projects. But final projects are\nevaluated almost entirely,",
    "start": "3605960",
    "end": "3616560"
  },
  {
    "text": "unless there's some major\nworries or concerns based on your project report.",
    "start": "3616560",
    "end": "3622590"
  },
  {
    "text": "So make sure you put time\ninto the project report, which is essentially a research\npaper, like a conference paper.",
    "start": "3622590",
    "end": "3630270"
  },
  {
    "text": "And they can be\nup to eight pages. And it varies on\nwhat you're doing, but this is the kind\nof picture typically",
    "start": "3630270",
    "end": "3638330"
  },
  {
    "text": "of what a paper will look like. You'll have an abstract\nand introduction. You'll talk about\nother related work.",
    "start": "3638330",
    "end": "3645600"
  },
  {
    "text": "It will present the model you're\nusing, the data you're using, and your experiments\nand their results",
    "start": "3645600",
    "end": "3651319"
  },
  {
    "text": "and have some insightful\ncomments in its analysis and conclusion at the end.",
    "start": "3651320",
    "end": "3657690"
  },
  {
    "text": "OK. Finding research topics\nfor custom projects.",
    "start": "3657690",
    "end": "3663870"
  },
  {
    "text": "All kinds of things you can do. Basic philosophy of science,\nyou're normally either starting",
    "start": "3663870",
    "end": "3670680"
  },
  {
    "text": "off with, here's some problem I\nwant to make some progress on, or here's this cool idea\nfor a theoretical technique",
    "start": "3670680",
    "end": "3678390"
  },
  {
    "text": "or a change in\nsomething, and I want to show it's better than\nother ways of doing it. And you're working from that.",
    "start": "3678390",
    "end": "3685710"
  },
  {
    "text": "We allow different\nkinds of projects. One common type of project is\nyou've got some task of interest",
    "start": "3685710",
    "end": "3694650"
  },
  {
    "text": "and you're going to try and\nsolve it or make progress on it somehow, that you want\nto get information out",
    "start": "3694650",
    "end": "3702510"
  },
  {
    "text": "of state department\ndocuments, and you're going to see how well you\ncan do it with neural NLP.",
    "start": "3702510",
    "end": "3710250"
  },
  {
    "text": "A second kind is you've got\nsome ideas of doing something different with neural networks.",
    "start": "3710250",
    "end": "3715360"
  },
  {
    "text": "And then you're going to\nsee how well it works. Or maybe, given there are large\nlanguage models these days,",
    "start": "3715360",
    "end": "3722360"
  },
  {
    "text": "you're going to see how\nusing large language models, you can do something\ninteresting by in-context",
    "start": "3722360",
    "end": "3728289"
  },
  {
    "text": "learning or building a larger\nlanguage model program. So nearly all 224n projects\nare in those first three types",
    "start": "3728290",
    "end": "3740710"
  },
  {
    "text": "where, at the end of the day,\nyou've got some kind of system",
    "start": "3740710",
    "end": "3745839"
  },
  {
    "text": "and you've got some kind of data\nand you're going to evaluate it. But that's not 100% requirement.",
    "start": "3745840",
    "end": "3753500"
  },
  {
    "text": "There are different kinds\nof projects you can do and a few people do. So you can do an analysis\ninterpretability project.",
    "start": "3753500",
    "end": "3761830"
  },
  {
    "text": "So you could be\ninterested in something like how could these transformer\nmodels possibly understand",
    "start": "3761830",
    "end": "3770440"
  },
  {
    "text": "what I say to them and give the\nright answers to my statements.",
    "start": "3770440",
    "end": "3775460"
  },
  {
    "text": "Let me try and look\ninside the neural networks and see what they're\ncomputing, how.",
    "start": "3775460",
    "end": "3781120"
  },
  {
    "text": "Recently there's\nbeen a lot of work on this topic,\noften under titles like mechanistic\ninterpretability circuit",
    "start": "3781120",
    "end": "3789099"
  },
  {
    "text": "training and things like that. So you can do some kind of\nanalysis or interpretability project.",
    "start": "3789100",
    "end": "3794839"
  },
  {
    "text": "Or you could even just do it-- looking at the behavior\nof models of some task.",
    "start": "3794840",
    "end": "3801140"
  },
  {
    "text": "So you could take\nsome linguistic task like metaphor\ninterpretation and see",
    "start": "3801140",
    "end": "3808510"
  },
  {
    "text": "which neural networks can\ninterpret them correctly and which can't, or\nwhich kinds of ones they can interpret correctly or\nnot, and do things like that.",
    "start": "3808510",
    "end": "3817480"
  },
  {
    "text": "Another kind is a\ntheoretical project. Occasionally, people\nhave done things,",
    "start": "3817480",
    "end": "3825910"
  },
  {
    "text": "looking at the behavior of--",
    "start": "3825910",
    "end": "3831539"
  },
  {
    "text": "well, that's a good example. Somewhere that's in the math.",
    "start": "3831540",
    "end": "3836789"
  },
  {
    "text": "So an example that was\nactually done a few years ago and turned into a\nconference paper",
    "start": "3836790",
    "end": "3842339"
  },
  {
    "text": "was looking at,\nin the estimation of word vectors, the\nstability of the word vectors",
    "start": "3842340",
    "end": "3850980"
  },
  {
    "text": "that were computed by\ndifferent algorithms, Word2vec versus GloVe, and\nderiving results",
    "start": "3850980",
    "end": "3860700"
  },
  {
    "text": "with proofs about the\nstability of the vectors that",
    "start": "3860700",
    "end": "3867150"
  },
  {
    "text": "were calculated. So that's allowed. But we don't see many of those. Here very quickly,\njust random things.",
    "start": "3867150",
    "end": "3877539"
  },
  {
    "text": "So a lot of past projects you\ncan find on the 224n web page, you can just find different\npast year reports.",
    "start": "3877540",
    "end": "3885510"
  },
  {
    "text": "And you can look at them\nto get ideas as you wish. So deep poetry was\na gated LSTM, where",
    "start": "3885510",
    "end": "3894849"
  },
  {
    "text": "the idea was as well sought,\na language model that generates successive words.",
    "start": "3894850",
    "end": "3900230"
  },
  {
    "text": "They had extra stuff\nin it to make it rhyme in a poetry-like pattern.",
    "start": "3900230",
    "end": "3906650"
  },
  {
    "text": "That was kind of fun. You can do a reimplementation\nof a paper that",
    "start": "3906650",
    "end": "3913030"
  },
  {
    "text": "has been done previously. This is actually a kind of an\nold one, but I remember it well.",
    "start": "3913030",
    "end": "3918230"
  },
  {
    "text": "So back in the days\nbefore transformers, DeepMind did these kind\nof interesting papers",
    "start": "3918230",
    "end": "3924880"
  },
  {
    "text": "on neural Turing machines and\ndifferentiable neural computers. ",
    "start": "3924880",
    "end": "3931000"
  },
  {
    "text": "But they didn't release\nimplementations of them. And so Carol said about\nwriting her own implementation",
    "start": "3931000",
    "end": "3938470"
  },
  {
    "text": "of a differentiable neural\ncomputer, which in a way was a little bit crazy.",
    "start": "3938470",
    "end": "3944890"
  },
  {
    "text": "And a few days\nbefore the deadline, she still hadn't\ngotten it working. So it could have been\na complete disaster.",
    "start": "3944890",
    "end": "3950800"
  },
  {
    "text": "But she did get it working\nbefore the deadline and got it to run, producing\nsome interesting results.",
    "start": "3950800",
    "end": "3957440"
  },
  {
    "text": "So that was kind of cool. So if it's something\ninteresting, it doesn't have to be original. It can be sort of reimplementing\nsomething interesting.",
    "start": "3957440",
    "end": "3967059"
  },
  {
    "text": "OK. Sometimes papers do\nget published later",
    "start": "3967060",
    "end": "3972400"
  },
  {
    "text": "as interesting ones. This was a paper that was sort\nof-- again, from the early days, and it was sort\nof fairly simple,",
    "start": "3972400",
    "end": "3979099"
  },
  {
    "text": "but it was a novel thing\nthat gave progress. So the way we've\npresented these RNNs,",
    "start": "3979100",
    "end": "3986060"
  },
  {
    "text": "you have word\nvectors at the bottom and then you compute\nthe softmax at the top.",
    "start": "3986060",
    "end": "3991880"
  },
  {
    "text": "But if you think about the\nmultiplying by the output matrix",
    "start": "3991880",
    "end": "3997329"
  },
  {
    "text": "and then putting that\ninto the softmax, that output matrix is also\nlike a set of word vectors",
    "start": "3997330",
    "end": "4003090"
  },
  {
    "text": "because you have a\ncolumn for each word. And then you put it to-- you get\na score for each output word.",
    "start": "4003090",
    "end": "4009480"
  },
  {
    "text": "And then you're putting\na softmax over that. And so their idea\nwas, well, maybe",
    "start": "4009480",
    "end": "4014870"
  },
  {
    "text": "you could share those\ntwo sets of vectors, and you'd be able to get\nimprovements from that",
    "start": "4014870",
    "end": "4021590"
  },
  {
    "text": "and you could. OK. Maybe I won't talk\nabout that one.",
    "start": "4021590",
    "end": "4027260"
  },
  {
    "text": "Sometimes people have\nworked on quantized models. That's sort of a general\nneural network technique.",
    "start": "4027260",
    "end": "4034950"
  },
  {
    "text": "But providing you show you\ncan do useful things with it, like have good language\nmodeling results even",
    "start": "4034950",
    "end": "4040190"
  },
  {
    "text": "with quantized vectors, we'll\ncount that as using language.",
    "start": "4040190",
    "end": "4045290"
  },
  {
    "text": "So in recent times-- these last two are from 2024.",
    "start": "4045290",
    "end": "4052890"
  },
  {
    "text": "A lot of the time,\npeople are doing projects with pretrained large\nlanguage models, which",
    "start": "4052890",
    "end": "4058849"
  },
  {
    "text": "we will be talking about\nin the next three models, three lectures, and then\ndoing things with them.",
    "start": "4058850",
    "end": "4064670"
  },
  {
    "text": "And so you can do lightweight,\nparameter-efficient fine-tuning methods. You can do in-context learning\nmethods and things like this.",
    "start": "4064670",
    "end": "4072630"
  },
  {
    "text": "And I suspect that\nprobably quite a few of you will do projects of this kind.",
    "start": "4072630",
    "end": "4079530"
  },
  {
    "text": "So here's an example. So lots of work has been done on\nproducing code language models.",
    "start": "4079530",
    "end": "4090200"
  },
  {
    "text": "And so these people\ndecided to improve",
    "start": "4090200",
    "end": "4096049"
  },
  {
    "text": "the generation of Fortran. Maybe they are\nphysicists, I don't know. [LAUGHS] And so they were able\nto show that they could use",
    "start": "4096050",
    "end": "4109850"
  },
  {
    "text": "parameter-efficient\nfine-tuning to improve Code Llama for producing Fortran.",
    "start": "4109850",
    "end": "4115220"
  },
  {
    "text": "Now, where was the\nnatural language? Code has natural\nlanguage comments in it,",
    "start": "4115220",
    "end": "4120710"
  },
  {
    "text": "and the comments can be\nuseful for explaining what you want the code to do.",
    "start": "4120710",
    "end": "4126660"
  },
  {
    "text": "And so it was effectively\ndoing translation from human language, explanation\nof what the code was meant",
    "start": "4126660",
    "end": "4136009"
  },
  {
    "text": "to do into pieces of code. Here was another one, which\nwas doing AI fashion-driven",
    "start": "4136010",
    "end": "4147509"
  },
  {
    "text": "cataloging, transforming images\ninto textual descriptions, which, again, was starting off\nwith an existing visual language",
    "start": "4147510",
    "end": "4155220"
  },
  {
    "text": "model and looking at\nhow to fine-tune it. OK. Other places to look for stuff.",
    "start": "4155220",
    "end": "4162839"
  },
  {
    "text": "So you can get lots of\nideas of areas and things people do by looking\nat past papers.",
    "start": "4162840",
    "end": "4170000"
  },
  {
    "text": "You're also welcome to have\nyour own original ideas thinking about anything you\nknow or work on in the world.",
    "start": "4170000",
    "end": "4176109"
  },
  {
    "text": "So in NLP papers, there's a site\ncalled the ACL Anthology that's good for them.",
    "start": "4176109",
    "end": "4182460"
  },
  {
    "text": "There are lots of\npapers on language that also appear in machine\nlearning conferences. So you can look at the\nNeurIPS or ICLR proceedings.",
    "start": "4182460",
    "end": "4191049"
  },
  {
    "text": "You can look at\npast 224n projects. And then the arXiv\npreprint servers",
    "start": "4191050",
    "end": "4197700"
  },
  {
    "text": "got tons of papers on\neverything, including NLP, and you can look there.",
    "start": "4197700",
    "end": "4202730"
  },
  {
    "text": "But I do actually think it's-- some of the funnest,\nbest projects are actually people that\nfind their own problem, which",
    "start": "4202730",
    "end": "4209560"
  },
  {
    "text": "is an interesting\nproblem in their world. If there's anything about a\ncool website that has text on it",
    "start": "4209560",
    "end": "4216789"
  },
  {
    "text": "and you think you could get\ninformation out of automatically by using a language\nmodel or something,",
    "start": "4216790",
    "end": "4221943"
  },
  {
    "text": "there's probably something\ninteresting and different you can do there. Another place to\nlook is that there",
    "start": "4221943",
    "end": "4228190"
  },
  {
    "text": "are various leaderboards\nfor the state of the art on different problems. And you can start looking\nthrough leaderboards for stuff",
    "start": "4228190",
    "end": "4235540"
  },
  {
    "text": "and see what you find there. But on the other\nhand, the disadvantage",
    "start": "4235540",
    "end": "4241580"
  },
  {
    "text": "of looking at things\nlike leaderboards and past conferences\nis you tend to be",
    "start": "4241580",
    "end": "4247460"
  },
  {
    "text": "trying to do a bit better on a\nproblem someone else has done. And that's part of why\nreally, often in research,",
    "start": "4247460",
    "end": "4255110"
  },
  {
    "text": "it's a clever thing to think\nof something different, perhaps not too far from things\nthat other people have done,",
    "start": "4255110",
    "end": "4261690"
  },
  {
    "text": "but somehow different. So you'll be able\nto do something a bit more original and\ndifferent for what you're doing.",
    "start": "4261690",
    "end": "4270000"
  },
  {
    "text": "Yeah, I do want to go\nthrough this a bit quickly. ",
    "start": "4270000",
    "end": "4280304"
  },
  {
    "text": "For the decade that I've\nbeen doing natural language processing with deep\nlearning, there's",
    "start": "4280305",
    "end": "4286950"
  },
  {
    "text": "sort of been a sea change\nin what's possible. So in the early days of\nthe deep learning revival,",
    "start": "4286950",
    "end": "4296160"
  },
  {
    "text": "most of the work\nin people's papers were trying to find better\ndeep learning architectures.",
    "start": "4296160",
    "end": "4302920"
  },
  {
    "text": "So that would be, here is some\nquestion-answering system. I've got an idea of how I could\nadd attention in some new place,",
    "start": "4302920",
    "end": "4310630"
  },
  {
    "text": "or I could add a new layer\ninto the neural network and the numbers will go up.",
    "start": "4310630",
    "end": "4317159"
  },
  {
    "text": "And there were lots\nof papers like that. And there was a lot of fun. And that's what a lot of good\nCS224n projects did, too.",
    "start": "4317160",
    "end": "4327760"
  },
  {
    "text": "And people were often\nable to build systems from scratch that were close\nto the state of the art.",
    "start": "4327760",
    "end": "4334239"
  },
  {
    "text": "But in the last five years,\nyour chances of doing this have become pretty\nslim, frankly.",
    "start": "4334240",
    "end": "4344535"
  },
  {
    "text": "If you've really got a good\nidea and it's something different and original, by all\nmeans, but it's kind of hard.",
    "start": "4344535",
    "end": "4351200"
  },
  {
    "text": "So most work these\ndays, even for people who are professional\nresearchers,",
    "start": "4351200",
    "end": "4359030"
  },
  {
    "text": "they're making use of existing\nlarge pretrained models",
    "start": "4359030",
    "end": "4364150"
  },
  {
    "text": "in some way. And then once you're doing that,\nthat actually sort of fixes",
    "start": "4364150",
    "end": "4369520"
  },
  {
    "text": "a lot of your\narchitectural choices because your large\npretrained neural network has a\ncertain architecture",
    "start": "4369520",
    "end": "4375969"
  },
  {
    "text": "and you kind of have\nto live with that. You might be able to do\ninteresting things by adapting",
    "start": "4375970",
    "end": "4381205"
  },
  {
    "text": "it with something like low\nrank adaptation around the site or something. But nevertheless,\nthere's constraints",
    "start": "4381205",
    "end": "4387310"
  },
  {
    "text": "on what you want to do. So for just about any\npractical project,",
    "start": "4387310",
    "end": "4393170"
  },
  {
    "text": "you've got some data\nset and you want to understand it and get\nfacts out of it or something",
    "start": "4393170",
    "end": "4398380"
  },
  {
    "text": "like that. Essentially, the\nonly sensible choice is to say, I am going to use\nHugging Face transformers,",
    "start": "4398380",
    "end": "4407000"
  },
  {
    "text": "which we have a tutorial\non coming up ahead. And I will load some\npretrained model,",
    "start": "4407000",
    "end": "4413050"
  },
  {
    "text": "and I will be running\nit over the text. And then I'll be working\nout some other stuff I can do on top and around that.",
    "start": "4413050",
    "end": "4419720"
  },
  {
    "text": "So building your\nown architecture is really only a\nsensible choice. If you can do something in\nthe small, which is more",
    "start": "4419720",
    "end": "4428620"
  },
  {
    "text": "a sort of exploring\narchitectures project, if you've kind of\ngot an idea of,",
    "start": "4428620",
    "end": "4433850"
  },
  {
    "text": "hey, I've got an idea for\na different nonlinearity that I think will work\nbetter than using a ReLU,",
    "start": "4433850",
    "end": "4439390"
  },
  {
    "text": "let me investigate kind\nof thing because then you can do small experiments.",
    "start": "4439390",
    "end": "4445270"
  },
  {
    "text": "Yeah, maybe I won't read\nout all of this list. But there are lists of\nsome of the ideas of what's",
    "start": "4445270",
    "end": "4453250"
  },
  {
    "text": "more interesting now. But do be cognizant of the world\nwe're in in terms of scale.",
    "start": "4453250",
    "end": "4463310"
  },
  {
    "text": "I mean, one of the\nproblems we now have is that people have seen\nthe latest paper that",
    "start": "4463310",
    "end": "4471250"
  },
  {
    "text": "was being pushed by\nDeepMind, whoever, doing some cool graph-structured\nreasoning search to do things,",
    "start": "4471250",
    "end": "4479260"
  },
  {
    "text": "and they turn up and say, I\nwant to do this for my project. But a lot of the time, if you\nread further into the paper,",
    "start": "4479260",
    "end": "4488540"
  },
  {
    "text": "you'll find that they were doing\nit on 30 to a 100 for a month. And that's not the\nscale of compute",
    "start": "4488540",
    "end": "4495280"
  },
  {
    "text": "that you're going to have\navailable to you in almost or any all circumstances.",
    "start": "4495280",
    "end": "4500960"
  },
  {
    "text": "Maybe they're one or\ntwo industry students for the-- industry students\nthat you can do that. And if so, go for it.",
    "start": "4500960",
    "end": "4506630"
  },
  {
    "text": "But for the vast majority\nof people, not likely. So you do have to do\nsomething that is practical.",
    "start": "4506630",
    "end": "4516140"
  },
  {
    "text": "But that practicality is\ntrue for a vast majority of the people in the world.",
    "start": "4516140",
    "end": "4521460"
  },
  {
    "text": "And if you look around\nin blogs and so on you find lots of people doing\nstuff in lightweight ways",
    "start": "4521460",
    "end": "4529400"
  },
  {
    "text": "and describing how to do that. And that's why methods like\nparameter-efficient fine-tuning are really popular because you\ncan do them in lightweight ways.",
    "start": "4529400",
    "end": "4538460"
  },
  {
    "text": "The question related to that-- and I'll end on this-- is-- I just want\nto mention again--",
    "start": "4538460",
    "end": "4550400"
  },
  {
    "text": "if you want to you're welcome\nto use GPT-4 or Gemini Pro or Claude Opus or any of\nthese models in your project.",
    "start": "4550400",
    "end": "4560060"
  },
  {
    "text": "But it has to be then API usage. You can't possibly train\nyour own big models.",
    "start": "4560060",
    "end": "4568380"
  },
  {
    "text": "I mean, even for the models\nthat are available, open source and like those, for big models,\nyou can't even load them",
    "start": "4568380",
    "end": "4577040"
  },
  {
    "text": "into the kind of GPUs you have. So probably you can\nload a LLaMA 7B model,",
    "start": "4577040",
    "end": "4583290"
  },
  {
    "text": "but you can't just load into\nyour GPU LLaMA 70B model. So you have to be\nrealistic on that size.",
    "start": "4583290",
    "end": "4593520"
  },
  {
    "text": "But there's actually now\nlots of interesting things you can do with API\naccess, doing things",
    "start": "4593520",
    "end": "4599390"
  },
  {
    "text": "like in-context learning\nand prompting and exploring that or building\nlarger language model",
    "start": "4599390",
    "end": "4604850"
  },
  {
    "text": "programs around these\nlanguage model components. And you're certainly\nencouraged to do that.",
    "start": "4604850",
    "end": "4611810"
  },
  {
    "text": "Lots of other things you can do,\nsuch as analysis projects, which look at these models sexist\nand racist still, or do they",
    "start": "4611810",
    "end": "4620540"
  },
  {
    "text": "have good understanding\nof analogies, or can they interpret\nlove letters or whatever",
    "start": "4620540",
    "end": "4627630"
  },
  {
    "text": "is your topic of interest. Lots of things you can do,\nand that's totally allowed.",
    "start": "4627630",
    "end": "4633620"
  },
  {
    "text": "But again, remember\nthat we'll be trying to evaluate us on what\ninteresting stuff you did.",
    "start": "4633620",
    "end": "4641130"
  },
  {
    "text": "So your project shouldn't be,\nI ran this stuff through GPT-4 and it produced great\nsummaries of the documents.",
    "start": "4641130",
    "end": "4648119"
  },
  {
    "text": "I am done. The question is, what\ndid you do in addition",
    "start": "4648120",
    "end": "4653460"
  },
  {
    "text": "to that to have an\ninteresting research project? OK, I'll stop there. Thanks a lot.",
    "start": "4653460",
    "end": "4659329"
  },
  {
    "start": "4659330",
    "end": "4664000"
  }
]