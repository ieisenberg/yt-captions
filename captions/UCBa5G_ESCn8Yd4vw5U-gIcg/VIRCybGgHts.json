[
  {
    "start": "0",
    "end": "48000"
  },
  {
    "text": "so a lot of the work I'm going to talk about today was done joint with Timothy lra as I talk my voice will get weirder",
    "start": "11759",
    "end": "19160"
  },
  {
    "text": "and weirder um because I've got a small pollet growing on one vocal cord and as I talk my I'll start producing two notes",
    "start": "19160",
    "end": "27279"
  },
  {
    "text": "at the same time um so back propagation just to be sure you all know",
    "start": "27279",
    "end": "34000"
  },
  {
    "text": "takes an input Vector you go forwards through a multi neural net with nonlinear units you compare with the",
    "start": "34000",
    "end": "39120"
  },
  {
    "text": "correct answer you back propagate something backwards derivatives and then you adjust all the weights and despite what people thought",
    "start": "39120",
    "end": "46760"
  },
  {
    "text": "for a long time it works great um if you're interested in it for the brain then you would do um online stochastic",
    "start": "46760",
    "end": "53719"
  },
  {
    "start": "48000",
    "end": "163000"
  },
  {
    "text": "learning that is you would take a training example you go forwards you go backwards and you update the weights a little bit and",
    "start": "53719",
    "end": "60640"
  },
  {
    "text": "in statistical terms you're getting an expectation of the full gradient that is you get a noisy version of the full",
    "start": "60640",
    "end": "66680"
  },
  {
    "text": "gradient from just one training case on average it's right um of course it's a long way off um and that learning",
    "start": "66680",
    "end": "73280"
  },
  {
    "text": "technique you'd have thought was crazy but actually it worked quite well as long as you don't learn too",
    "start": "73280",
    "end": "79360"
  },
  {
    "text": "fast so the question is could the cortex be doing this and if you talk to",
    "start": "79360",
    "end": "85360"
  },
  {
    "text": "neuroscientists or look at the things neuroscientists have said over the ages",
    "start": "85360",
    "end": "90680"
  },
  {
    "text": "um there until very recently they were all completely convinced that this was",
    "start": "90680",
    "end": "97320"
  },
  {
    "text": "crazy most of them didn't understand what you meant because they thought back propagation meant sending um spikes",
    "start": "97320",
    "end": "103240"
  },
  {
    "text": "backwards down the um dendritic tree and they that is back propagation that's a different form of back propagation and",
    "start": "103240",
    "end": "109040"
  },
  {
    "text": "you need that for doing many of these algorithms and they didn't understand that it was the idea of back propagation was to send eror Dr is from one cortical",
    "start": "109040",
    "end": "115119"
  },
  {
    "text": "area to an earlier cortical area um it's the right thing to do",
    "start": "115119",
    "end": "120439"
  },
  {
    "text": "and it seems to be completely crazy it would be completely crazy for evolution not to have figured out a way of",
    "start": "120439",
    "end": "126159"
  },
  {
    "text": "modifying early feature detectors so that they're useful for later feature detectors we can all think of a dumb",
    "start": "126159",
    "end": "131640"
  },
  {
    "text": "algorithm doing that which is you change them at random see if it helps but that's hopeless inefficient Bank propagation is just that dumb algorithm",
    "start": "131640",
    "end": "138480"
  },
  {
    "text": "change them and see if it helps except that it's more efficient by a factor of the number of connections so if you've",
    "start": "138480",
    "end": "144080"
  },
  {
    "text": "got a billion connections it's a billion times more efficient than tinkering with the weights at random um and so have",
    "start": "144080",
    "end": "150120"
  },
  {
    "text": "thought Evolution would have discovered that um but neuroscientists actually have a",
    "start": "150120",
    "end": "155400"
  },
  {
    "text": "bunch of good reasons why it's not possible and I'm going to go over four of those reasons I don't know whether",
    "start": "155400",
    "end": "161000"
  },
  {
    "text": "the B brain can do back propop but what I do know is the arguments neuroscientists use aren't very good",
    "start": "161000",
    "end": "167959"
  },
  {
    "start": "163000",
    "end": "320000"
  },
  {
    "text": "um so the first reason is there's no obvious source of supervision in back propagation you I'm talking for a feed",
    "start": "167959",
    "end": "175040"
  },
  {
    "text": "forward network not a recurrent Network here for a feed forward Network you go forwards some some injects the right",
    "start": "175040",
    "end": "181319"
  },
  {
    "text": "answer you compare it with what you got and you send something backwards and there's nobody to inject the right answer in the brain it's not like your",
    "start": "181319",
    "end": "187000"
  },
  {
    "text": "mother has a little electrode into the middle of your brain much as much though she would like that",
    "start": "187000",
    "end": "193440"
  },
  {
    "text": "um a second reason is that cortical neurons don't send real values to each other and in back propagation as is",
    "start": "193440",
    "end": "199200"
  },
  {
    "text": "normally used you're communicating real values in the forward path and real value derivatives backwards so Francis",
    "start": "199200",
    "end": "204840"
  },
  {
    "text": "Crick was very fond of this treason he said back propagation is crazy um neurons don't communicate real values and so it can't be doing it",
    "start": "204840",
    "end": "212760"
  },
  {
    "text": "um the next argument which I think is one of the best arguments why it can't be done is to do it in the obvious way",
    "start": "212760",
    "end": "220959"
  },
  {
    "text": "neurons would need to send two different signals when you're going forwards a neuron sends a signal that says this",
    "start": "220959",
    "end": "228159"
  },
  {
    "text": "feature is here or this feature is here to this extent so that's the activity of the neuron that's what it represents",
    "start": "228159",
    "end": "234079"
  },
  {
    "text": "when you're going backwards that same neuron needs to send a completely different signal which is how how fast",
    "start": "234079",
    "end": "240560"
  },
  {
    "text": "would the error change if I were to change my total input that I received from the layer below that's a completely",
    "start": "240560",
    "end": "245959"
  },
  {
    "text": "different quantity and obviously the neuron can't be sending both quantities um I'll refer to the output",
    "start": "245959",
    "end": "251840"
  },
  {
    "text": "of the neuron as Y and the total input to the neuron as X and so on the four plus it needs to send Y and on the back of pass it needs to send D by",
    "start": "251840",
    "end": "259040"
  },
  {
    "text": "DX and the last thing is back propagation you in back pration you go",
    "start": "259040",
    "end": "264360"
  },
  {
    "text": "forwards through the weights and then you go backwards through the same weights so Matrix terms you use the transpose of the forward Matrix to send",
    "start": "264360",
    "end": "270360"
  },
  {
    "text": "the error derivaties backwards and there's lots of evidence in the brain that if you have two cortical areas and",
    "start": "270360",
    "end": "275680"
  },
  {
    "text": "the for connections there will be backwards connections and if there's forwards connections from one region of a cortical area there will be backwards",
    "start": "275680",
    "end": "281440"
  },
  {
    "text": "connections to that region of the cortical area but they're not point to point so if a neuron here sends a",
    "start": "281440",
    "end": "287320"
  },
  {
    "text": "forwards connection there it's not that this neuron sends a backwards connection there in fact the backwards connections go to different neurons so um that",
    "start": "287320",
    "end": "295039"
  },
  {
    "text": "seemed like a major problem um and what I'm going to do now is just",
    "start": "295039",
    "end": "300520"
  },
  {
    "text": "go through these four arguments and show how the main aim of this talk is to show none of them are really insuperable",
    "start": "300520",
    "end": "307560"
  },
  {
    "text": "obstacles and when you combine that with the idea that we now know it works really well it suddenly becomes",
    "start": "307560",
    "end": "313199"
  },
  {
    "text": "plausible I think the brain might be doing something that's back propagation or something very close to back",
    "start": "313199",
    "end": "319360"
  },
  {
    "text": "propagation so first the source of supervision um people doing back propagation have",
    "start": "319759",
    "end": "326000"
  },
  {
    "start": "320000",
    "end": "498000"
  },
  {
    "text": "worried about this problem for a long time and in the 80s we thought that well one way to get a source of supervision",
    "start": "326000",
    "end": "331520"
  },
  {
    "text": "is to do reconstruction so you're trying to encode the data and then reconstruct",
    "start": "331520",
    "end": "336960"
  },
  {
    "text": "the data and you take the Reconstruction error and back appropriate it so you don't need an extra supervision signal",
    "start": "336960",
    "end": "342560"
  },
  {
    "text": "you're just trying to reconstruct the data that's what PCA is doing in back propagation is just nonlinear PCA",
    "start": "342560",
    "end": "349800"
  },
  {
    "text": "um another idea about how you might do get an error signal is you might extract",
    "start": "349800",
    "end": "356160"
  },
  {
    "text": "local features and then you might comp compare what the features in the layer below say",
    "start": "356160",
    "end": "362600"
  },
  {
    "text": "a feature detector ought to be doing so the whole issue is here is you have feature detectors in the intermediate",
    "start": "362600",
    "end": "367800"
  },
  {
    "text": "layers and they need to figure out what they should be doing and one thing to do is say it's going to extract some stuff",
    "start": "367800",
    "end": "373840"
  },
  {
    "text": "from below and then it's going to get a prediction from above from the broader context and he wants to make those two",
    "start": "373840",
    "end": "379880"
  },
  {
    "text": "the same he wants to make the prediction from the broader context agree with what it extracts from below and so a little",
    "start": "379880",
    "end": "385759"
  },
  {
    "text": "example of that in a sentence is nice one trial learning I give you a sentence like this one she",
    "start": "385759",
    "end": "392639"
  },
  {
    "text": "scrummed in with a frying pan and you've never heard the word scrummed before and",
    "start": "392639",
    "end": "398199"
  },
  {
    "text": "you have a pretty good idea of what it means in one trial I think it's sort of she bashed him with it somehow right um",
    "start": "398199",
    "end": "405080"
  },
  {
    "text": "probably because of something sexy he said um and what's Happening Here is there's",
    "start": "405080",
    "end": "411840"
  },
  {
    "text": "a certain amount of information in this character string here these seven characters like the Ed tells you it's",
    "start": "411840",
    "end": "417599"
  },
  {
    "text": "very likely a verb and the past tense of a verb um there may be some information in scrum just that that doesn't sound good",
    "start": "417599",
    "end": "425599"
  },
  {
    "text": "um but basically what's happening is the context um tells you what that's likely",
    "start": "425599",
    "end": "431319"
  },
  {
    "text": "to mean and in one trial you can get a good idea of you can get some evidence",
    "start": "431319",
    "end": "436759"
  },
  {
    "text": "about what that means and that's just an example of something you can detect locally like scrum and the context it's",
    "start": "436759",
    "end": "443039"
  },
  {
    "text": "in and you want to make those agree",
    "start": "443039",
    "end": "448520"
  },
  {
    "text": "okay a somewhat more principled way to get a learning signal though not",
    "start": "448520",
    "end": "453840"
  },
  {
    "text": "necessarily better is to say let's learn a generative model that assigns High log probability to the input data so for",
    "start": "453840",
    "end": "461680"
  },
  {
    "text": "vision let's learn a graphics model um that generates things that look like the images we actually",
    "start": "461680",
    "end": "467360"
  },
  {
    "text": "see um for complex nonlinear models that's tricky but if instead of trying to maximize the log probability of the",
    "start": "467360",
    "end": "473960"
  },
  {
    "text": "input data you try and maximize a variational bound on it um then you can make comp model is much more much easier",
    "start": "473960",
    "end": "481280"
  },
  {
    "text": "to learn and that works pretty well um and if you then are willing to make a",
    "start": "481280",
    "end": "486520"
  },
  {
    "text": "further approximation and say you've got this variational bound that's motivating you and you're not even going to",
    "start": "486520",
    "end": "491759"
  },
  {
    "text": "optimize that you're going to optimize an approximation to that you can get a very simple algorithm called the Wake sleep",
    "start": "491759",
    "end": "498000"
  },
  {
    "start": "498000",
    "end": "735000"
  },
  {
    "text": "algorithm um yes I was going to put in a description of how it works here but I",
    "start": "498000",
    "end": "503680"
  },
  {
    "text": "can give you the description of how it works um you have some oh actually no no",
    "start": "503680",
    "end": "509720"
  },
  {
    "text": "I did animation it confused myself yes there you go um there's a",
    "start": "509720",
    "end": "516200"
  },
  {
    "text": "weake phase where you go forwards um through these red connections which are recognition connections and that determines the",
    "start": "516200",
    "end": "523080"
  },
  {
    "text": "activities of these hidden layers and then you do learning of the other connections so now we're going to",
    "start": "523080",
    "end": "529200"
  },
  {
    "text": "learn these connections and we're going to train these to be good at reconstructing whatever it was in this",
    "start": "529200",
    "end": "534720"
  },
  {
    "text": "layer that caused it so whatever it was in H2 that caused the pattern in H3 you try and reconstru conct it from the",
    "start": "534720",
    "end": "539920"
  },
  {
    "text": "patent in H3 you do a reconstruction you look at the error and you change these weights to get rid of that error and",
    "start": "539920",
    "end": "546440"
  },
  {
    "text": "notice there's no back propagation needed here that learning can be done sort of at a synapse here it sees um the",
    "start": "546440",
    "end": "553800"
  },
  {
    "text": "state of H2 um that was there previously it sees the state of H2 that's there when it",
    "start": "553800",
    "end": "559440"
  },
  {
    "text": "does the Reconstruction and it gets his input on the activity of H3 and so you can learn",
    "start": "559440",
    "end": "566000"
  },
  {
    "text": "this synapse here without having to do back propop and that really is uh the right thing to learn um to",
    "start": "566000",
    "end": "573560"
  },
  {
    "text": "maximize the probability of regenerating this from that um and you can do that sort of independently at all the",
    "start": "573560",
    "end": "579680"
  },
  {
    "text": "layers and then the Wake fa the Sleep phase what you do is you generate data from your model so I start with random",
    "start": "579680",
    "end": "587640"
  },
  {
    "text": "vectors up here generated according to the biases of these units I generate downwards and then for each pair of",
    "start": "587640",
    "end": "594040"
  },
  {
    "text": "layers I try and reconstruct what actually caused this activity in the",
    "start": "594040",
    "end": "599079"
  },
  {
    "text": "layer above and I assume Independence it's a variational method so I'm make an assumption there I assume these are",
    "start": "599079",
    "end": "605600"
  },
  {
    "text": "independent causes of this um or rather in the posterior they're",
    "start": "605600",
    "end": "611680"
  },
  {
    "text": "independent and so you get a very simple learning algorithm it's not actually following the derivative of the variational bound because unfortunately",
    "start": "611680",
    "end": "617839"
  },
  {
    "text": "the variational bound is a KL of QP where Q is your approximating distribution p is the right distribution",
    "start": "617839",
    "end": "624600"
  },
  {
    "text": "and this is optimizing um kpq but it works pretty well um and it's very",
    "start": "624600",
    "end": "629880"
  },
  {
    "text": "simple there's no back propagation required and so in 1995 we were quite excited about that that we had an Al",
    "start": "629880",
    "end": "636160"
  },
  {
    "text": "algorithm that could learn multiple leer representation without doing any back propagation and it worked um moderately",
    "start": "636160",
    "end": "643440"
  },
  {
    "text": "well it learned sensible leer feature detectors but it wasn't as good as real",
    "start": "643440",
    "end": "649079"
  },
  {
    "text": "back propop um so you'll notice the training of these weights and training of those",
    "start": "649079",
    "end": "654760"
  },
  {
    "text": "weights uses exactly the same process now there's some crazy things about it like when you're away",
    "start": "654760",
    "end": "659880"
  },
  {
    "text": "you don't learn the recognition connections we really thought this was wake and sleep and when you're asleep um",
    "start": "659880",
    "end": "665639"
  },
  {
    "text": "you don't learn the generative connections um so that means you get to the end of a day and you haven't learned",
    "start": "665639",
    "end": "670880"
  },
  {
    "text": "to recognize things any better during the day you have to go to sleep um that doesn't seem plausible be half a sleep",
    "start": "670880",
    "end": "676920"
  },
  {
    "text": "yeah half a sleep um well you could alternate maybe there's new methods for",
    "start": "676920",
    "end": "682760"
  },
  {
    "text": "unsupervised learning so the problem we had that led to the wake sleep algorithm and that meant it wasn't doing quite the",
    "start": "682760",
    "end": "688720"
  },
  {
    "text": "right thing was that for this deep net we couldn't get the exact derivatives of the variational bound with respect to",
    "start": "688720",
    "end": "694160"
  },
  {
    "text": "the recognition weights the learning algorithm had the exact derives of the variational band with respect to the generative weights but not with respect",
    "start": "694160",
    "end": "700720"
  },
  {
    "text": "to those recognition weights and then Max Welling and the student of physical kingma came up with a very clever trick",
    "start": "700720",
    "end": "707320"
  },
  {
    "text": "that allow I'm amazed that you can do it that allowed you to actually get the exact",
    "start": "707320",
    "end": "712680"
  },
  {
    "text": "derivatives that is something whose expected value was the exact derivative and so now people can learn these",
    "start": "712680",
    "end": "718200"
  },
  {
    "text": "variational alter and codes much better um there's also other new methods of",
    "start": "718200",
    "end": "723959"
  },
  {
    "text": "doing unsupervised learning or rather sort of getting a supervision signal",
    "start": "723959",
    "end": "729760"
  },
  {
    "text": "without being given a separate supervision signal so Ian Goodfellow and his collaborators have the thing called",
    "start": "729760",
    "end": "736399"
  },
  {
    "start": "735000",
    "end": "819000"
  },
  {
    "text": "generative adversarial Nets where you have a net that generates data let's suppose it's images and to begin with it",
    "start": "736399",
    "end": "742000"
  },
  {
    "text": "doesn't generate very good images generates rubbish you have another net that looks at real images and looks at",
    "start": "742000",
    "end": "748680"
  },
  {
    "text": "the images generated by this net and it has to tell you whether what it's just seen as a real image or an image that",
    "start": "748680",
    "end": "754600"
  },
  {
    "text": "came from this net so it's learning to tell the difference between what the net produced and what real images look like",
    "start": "754600",
    "end": "761320"
  },
  {
    "text": "so it's an adversary and now if you back propagate through the adversary you can figure out how to",
    "start": "761320",
    "end": "768639"
  },
  {
    "text": "change the generated images so it's harder for the adversary to tell the difference between them and real images",
    "start": "768639",
    "end": "775000"
  },
  {
    "text": "so you get a signal that tells you how to generate images that more difficult to distinguish from the real ones",
    "start": "775000",
    "end": "781160"
  },
  {
    "text": "according to this adversary but if you now keep learning the two together it starts generating really good images if",
    "start": "781160",
    "end": "787240"
  },
  {
    "text": "you show it lots and lots of scenes of bedrooms and just the furniture and",
    "start": "787240",
    "end": "795399"
  },
  {
    "text": "um you then get it to generate it generates things that are not",
    "start": "795399",
    "end": "801519"
  },
  {
    "text": "particularly like any of the scenes it's seen but you look at them and you say that's a bedroom it's amazing",
    "start": "801519",
    "end": "807199"
  },
  {
    "text": "um okay so there's lots new lots of new unsupervised learning algorithms coming up and my conclusion from all this I'm",
    "start": "807199",
    "end": "814360"
  },
  {
    "text": "not going to pick one of them as the best way to get a supervision signal but my conclusion is there's many different",
    "start": "814360",
    "end": "820800"
  },
  {
    "start": "819000",
    "end": "843000"
  },
  {
    "text": "ways to get supervision signals that you can use with back propagation",
    "start": "820800",
    "end": "826320"
  },
  {
    "text": "um so we don't actually have to inject a separate label so that objection doesn't",
    "start": "826440",
    "end": "832680"
  },
  {
    "text": "really kill back propagation there's lots of other ways you can get signals that you back propagate that will allow",
    "start": "832680",
    "end": "838320"
  },
  {
    "text": "you to do learning so that's not a major objection now the next objection can",
    "start": "838320",
    "end": "845040"
  },
  {
    "start": "843000",
    "end": "976000"
  },
  {
    "text": "urans communicate real values um so normally when you do back",
    "start": "845040",
    "end": "850519"
  },
  {
    "text": "propagation you send a real number forwards that's the output of a unit and um you also send real numbers backwards",
    "start": "850519",
    "end": "858000"
  },
  {
    "text": "which are error derivatives but as a matter of fact people didn't try this for a long time",
    "start": "858000",
    "end": "863920"
  },
  {
    "text": "if you take logistic units which are sending they between 0 and 1 and they're",
    "start": "863920",
    "end": "869399"
  },
  {
    "text": "going to send some real value forwards like 0.73 um if you just randomly quantize",
    "start": "869399",
    "end": "875880"
  },
  {
    "text": "that so .73 of the time you send a one and .27 of the time you send a zero so",
    "start": "875880",
    "end": "882199"
  },
  {
    "text": "it's got the same expected value the algorithm works just fine um when you're actually Computing the error duties for",
    "start": "882199",
    "end": "888720"
  },
  {
    "text": "the incoming weights of a unit you make use of the fact that it's a 73 but you never need to communicate that outside",
    "start": "888720",
    "end": "894560"
  },
  {
    "text": "the neuron um outside the neuron you just do this stochastic communication also it works just fine if when you're",
    "start": "894560",
    "end": "900959"
  },
  {
    "text": "back propagating errors from one layer to the next you use two bits you use one bit to say with it's positive or",
    "start": "900959",
    "end": "906720"
  },
  {
    "text": "negative and another bit to say um whether it's one or zero or whether",
    "start": "906720",
    "end": "913519"
  },
  {
    "text": "it's Epsilon or zero and you have to choose an Epsilon so that most of the error deres are smaller than Epsilon and",
    "start": "913519",
    "end": "920240"
  },
  {
    "text": "now you can stochastically communicate the back propagated derivatives with the right expected value and backr will work",
    "start": "920240",
    "end": "925800"
  },
  {
    "text": "just fine um so it is actually very robust to sending noisy things that have the",
    "start": "925800",
    "end": "932079"
  },
  {
    "text": "right expected value we're now going to have a digression about statistics and I'm",
    "start": "932079",
    "end": "937240"
  },
  {
    "text": "going to argue that actually the fact that neurons sends spikes rather than real values is an advantage it's better",
    "start": "937240",
    "end": "944360"
  },
  {
    "text": "than sending real values this sounds odd I mean the neurons are roughly I'm going",
    "start": "944360",
    "end": "950959"
  },
  {
    "text": "to model them as a Pon process we all know they're more complicated that but that's a good start um that sends spikes",
    "start": "950959",
    "end": "957920"
  },
  {
    "text": "um randomly from some underlying rate and the question is how could that be better",
    "start": "957920",
    "end": "963600"
  },
  {
    "text": "than sending an accurate real number well it all depends on um ideas about",
    "start": "963600",
    "end": "968920"
  },
  {
    "text": "statistics and we've all been grossly misled by the professionals who are called statisticians",
    "start": "968920",
    "end": "976560"
  },
  {
    "start": "976000",
    "end": "1119000"
  },
  {
    "text": "um so frequeny statisticians will tell you things you probably learned these when",
    "start": "976560",
    "end": "982720"
  },
  {
    "text": "you were very young like you shouldn't have more parameters than training cases because if you have more parameters than",
    "start": "982720",
    "end": "987959"
  },
  {
    "text": "training cases you can model anything um basing statisticians are a bit more liberal they'll say you can have more",
    "start": "987959",
    "end": "994639"
  },
  {
    "text": "parameters in training cases but you better integrate over the posterior well I don't want to do either",
    "start": "994639",
    "end": "999839"
  },
  {
    "text": "of those things I want to have hugely more parameters than training cases and I don't want to have to integrate over",
    "start": "999839",
    "end": "1005639"
  },
  {
    "text": "the posterior um because that's what the brain does and so there must be some",
    "start": "1005639",
    "end": "1010800"
  },
  {
    "text": "regime for learning where this works and the regime the brain in is totally unlike anything statisticians have",
    "start": "1010800",
    "end": "1016480"
  },
  {
    "text": "studied or anything that nearly all sta have studied um the models they study",
    "start": "1016480",
    "end": "1022639"
  },
  {
    "text": "are tiny models until quite recently there were models with say 100 parameters and a thousand training cases",
    "start": "1022639",
    "end": "1029520"
  },
  {
    "text": "now there're still tiny models with say 100 million parameters and a billion training cases these are tiny compared",
    "start": "1029520",
    "end": "1035438"
  },
  {
    "text": "with the brain trillions of times smaller well billions anyway um the brain's got about 10 to the 14",
    "start": "1035439",
    "end": "1041480"
  },
  {
    "text": "parameters that's synapsis um most of which seem to be adaptive or large fraction are adaptive",
    "start": "1041480",
    "end": "1047798"
  },
  {
    "text": "and you only live for 10 to 9 seconds um actually it's 2 * 10 9 which",
    "start": "1047799",
    "end": "1053160"
  },
  {
    "text": "is very lucky for some of us um so you've got about you've got at",
    "start": "1053160",
    "end": "1058480"
  },
  {
    "text": "least 10,000 synapses per second or about 100,000 synapses a",
    "start": "1058480",
    "end": "1065320"
  },
  {
    "text": "second that's how many that's how many parameters you burn throughout your lifetime and presumably the brain does",
    "start": "1065320",
    "end": "1070760"
  },
  {
    "text": "this because a synapse supporting a synapse for your entire lifetime is much cheaper than having one experience one",
    "start": "1070760",
    "end": "1077360"
  },
  {
    "text": "tenth of a second experience and for your whole body um so synapses are",
    "start": "1077360",
    "end": "1082760"
  },
  {
    "text": "really cheap and the brains manages to compute with them very cheaply only using about 30 Watts or so for 10 to the",
    "start": "1082760",
    "end": "1090080"
  },
  {
    "text": "14 of them um and so it needs a way to throw lots and lots of parameters at a relatively",
    "start": "1090080",
    "end": "1097200"
  },
  {
    "text": "small amount of data compared with the number of parameters it's got that was The evolutionary requirement on the brain and presumably it's figured out",
    "start": "1097200",
    "end": "1104000"
  },
  {
    "text": "how to do that and actually it turns out statisticians know a way of doing that stati Ians already know a way in which",
    "start": "1104000",
    "end": "1110320"
  },
  {
    "text": "you can get a better as you have more parameters your model gets better and better it just doesn't get better and",
    "start": "1110320",
    "end": "1115760"
  },
  {
    "text": "better very fast um so this is Big Data versus big models",
    "start": "1115760",
    "end": "1122000"
  },
  {
    "start": "1119000",
    "end": "1412000"
  },
  {
    "text": "um we all know big data is good um mainly because it's increased it caused",
    "start": "1122000",
    "end": "1127880"
  },
  {
    "text": "all our salaries to go up a bit um although it's still not as high as",
    "start": "1127880",
    "end": "1134120"
  },
  {
    "text": "the salaries of our recently recent graduate students but there you go um",
    "start": "1134120",
    "end": "1140720"
  },
  {
    "text": "so we know that for any given size of model more data is better kind of the best regularizer you can get for a model",
    "start": "1141039",
    "end": "1147640"
  },
  {
    "text": "is more data so just get more data but I'm arguing it's a bad idea to",
    "start": "1147640",
    "end": "1154919"
  },
  {
    "text": "do what statisticians have always recommended which is make your model so small that whatever size data set you",
    "start": "1154919",
    "end": "1161159"
  },
  {
    "text": "have it looks big it's bigger than the number of parameters in your model that's what frequeny statisticians",
    "start": "1161159",
    "end": "1166600"
  },
  {
    "text": "typically recommend um big models are good and for any",
    "start": "1166600",
    "end": "1173240"
  },
  {
    "text": "here's what statisticians don't believe um but it's true um for any given size of data the bigger you make the model",
    "start": "1173240",
    "end": "1181000"
  },
  {
    "text": "the better you'll do not just at fitting the data but at generalizing it has to be complicated enough data to be worth a big model of",
    "start": "1181000",
    "end": "1187240"
  },
  {
    "text": "course um provid you regularize it well in other words there are regularizers that are so good that um it's always",
    "start": "1187240",
    "end": "1193640"
  },
  {
    "text": "paid to have more data and a stronger regularizer and statisticians know something like that which which is if",
    "start": "1193640",
    "end": "1199080"
  },
  {
    "text": "you use an ensemble if you have an ensemble of 50 different models and I say okay would you like to have 100",
    "start": "1199080",
    "end": "1204960"
  },
  {
    "text": "different models that's twice as many parameters um we draw models from some distribution and train them",
    "start": "1204960",
    "end": "1210360"
  },
  {
    "text": "independently and so on so they're all trying to get the right answer um almost certainly having more models is going to",
    "start": "1210360",
    "end": "1217679"
  },
  {
    "text": "give you a slightly better answer so you can always make use of more parameters by just adding models to an ensemble",
    "start": "1217679",
    "end": "1224120"
  },
  {
    "text": "models of a fixed size to an ensemble um the question is is there a more efficient way to use more",
    "start": "1224120",
    "end": "1230360"
  },
  {
    "text": "parameters than that um but because of this and because there are more",
    "start": "1230360",
    "end": "1235840"
  },
  {
    "text": "efficient ways to use more parameters um I think it's a good idea to always try to make the data look small by using a",
    "start": "1235840",
    "end": "1242799"
  },
  {
    "text": "huge model now this relies on you having almost free computation obviously one of the reasons",
    "start": "1242799",
    "end": "1249919"
  },
  {
    "text": "statisticians didn't want to have big models was they started off doing the calculations by hand and then they use pocket calculators and then they use",
    "start": "1249919",
    "end": "1255440"
  },
  {
    "text": "computers and so on and until very recently um computers were so slow that you",
    "start": "1255440",
    "end": "1261600"
  },
  {
    "text": "couldn't really afford to have very big models and in fact they're still so slow so if you take a plausible size model",
    "start": "1261600",
    "end": "1267559"
  },
  {
    "text": "like 10 to the 14 parameters the computers are still too slow to be able to fit that to a large amount of data a",
    "start": "1267559",
    "end": "1274880"
  },
  {
    "text": "relatively small amount of data but say a billion training cases we'd like to fit 10 to the 14 parameters to a billion training cases computers are still much",
    "start": "1274880",
    "end": "1281480"
  },
  {
    "text": "too slow but I believe Nvidia is working on it um okay",
    "start": "1281480",
    "end": "1289880"
  },
  {
    "text": "so I'm just going to talk about one regularizer that works quite nicely because I'm particularly attached to it",
    "start": "1291000",
    "end": "1296480"
  },
  {
    "text": "um you can use a lot of parameters by having an ensemble of models and adding more models to The Ensemble but you can",
    "start": "1296480",
    "end": "1303360"
  },
  {
    "text": "make that more efficient by letting models within the Ensemble actually share information U so that you get more",
    "start": "1303360",
    "end": "1309799"
  },
  {
    "text": "knowledge per parameter just having a gazillion models is a very inefficient way of using a lot of",
    "start": "1309799",
    "end": "1316000"
  },
  {
    "text": "parameters so we're going to somehow let the models in Ensemble share information and here's the idea I've",
    "start": "1316000",
    "end": "1322880"
  },
  {
    "text": "talked about this before actually at Stanford so I'm going to go over it rather quickly um let's have a neural",
    "start": "1322880",
    "end": "1328080"
  },
  {
    "text": "net with one hidden layer each time you showed a training example you leave out",
    "start": "1328080",
    "end": "1333360"
  },
  {
    "text": "each unit with a probability of 0.5 um so if you've got H hidden units",
    "start": "1333360",
    "end": "1339360"
  },
  {
    "text": "you have two to the H different models now and each time you're going to just use one model so for each training",
    "start": "1339360",
    "end": "1346440"
  },
  {
    "text": "example we use one of these models selected at random random and so most of these 2 toh models we'll never see a single training example um but the trick",
    "start": "1346440",
    "end": "1354279"
  },
  {
    "text": "is all of these models are going to share the incoming weight so if two models both use a unit they'll use it",
    "start": "1354279",
    "end": "1359919"
  },
  {
    "text": "with the same incoming weights so they're doing massive parameter sharing and that's a much better",
    "start": "1359919",
    "end": "1366279"
  },
  {
    "text": "regularizer having your parameter be like the par parameter value that some",
    "start": "1366279",
    "end": "1371480"
  },
  {
    "text": "other model wants is a much better regularizer than just having it be close to zero for example which is what L1 and",
    "start": "1371480",
    "end": "1376840"
  },
  {
    "text": "L2 Decay are trying to do um so that's the idea of Dropout and you train by on each train",
    "start": "1376840",
    "end": "1384840"
  },
  {
    "text": "example you randomly leave out half the hidden units um when you test you put them all in and use half the size of the",
    "start": "1384840",
    "end": "1391600"
  },
  {
    "text": "outgoing weights and if you're using a softmax up",
    "start": "1391600",
    "end": "1396679"
  },
  {
    "text": "here then when you test what you get is exactly the geometric mean of the",
    "start": "1396679",
    "end": "1402799"
  },
  {
    "text": "predictions of all 2 to the H models so there's an efficient way to get your model average",
    "start": "1402799",
    "end": "1409440"
  },
  {
    "text": "now if you have multiple layers",
    "start": "1409440",
    "end": "1414520"
  },
  {
    "start": "1412000",
    "end": "1493000"
  },
  {
    "text": "um then when you use half the outgoing weights that's just an approximation um but it still works",
    "start": "1414760",
    "end": "1421240"
  },
  {
    "text": "pretty well so Dropout allows you to make models much bigger it makes",
    "start": "1421240",
    "end": "1426480"
  },
  {
    "text": "training slower because a there's more noise so even for a model of the same",
    "start": "1426480",
    "end": "1431960"
  },
  {
    "text": "size it will be slow to train and B you need to make the models bigger because you're on each occasion you're dropping",
    "start": "1431960",
    "end": "1438000"
  },
  {
    "text": "out many of the units but what it's really doing is it's preventing units collaborating too much so it's preventing",
    "start": "1438000",
    "end": "1443679"
  },
  {
    "text": "overfitting similarly when you use an ensemble having lots of small models prevents any collaboration at all",
    "start": "1443679",
    "end": "1450320"
  },
  {
    "text": "between parameters in different models and that's what stops overfitting here",
    "start": "1450320",
    "end": "1455440"
  },
  {
    "text": "we're going to have some collaboration but we're going to try and minimize it by saying I don't know which other",
    "start": "1455440",
    "end": "1461200"
  },
  {
    "text": "hidden units going to been left out so I can't rely on what he said and I can't adjust my parameter values so that it",
    "start": "1461200",
    "end": "1467240"
  },
  {
    "text": "works with this other hidden unit I have to be sort of more independent than that okay um there's people here who've",
    "start": "1467240",
    "end": "1475600"
  },
  {
    "text": "studied drop out and know much more about it than I do um but it works and",
    "start": "1475600",
    "end": "1481360"
  },
  {
    "text": "at test time you have the outgoing weights and just do a forward pass and that approximately computes the",
    "start": "1481360",
    "end": "1486840"
  },
  {
    "text": "geometric average of all of your models now you can view Dropout as using",
    "start": "1486840",
    "end": "1495399"
  },
  {
    "start": "1493000",
    "end": "1718000"
  },
  {
    "text": "beri noise where you take a neuron you compute what his output should be and",
    "start": "1495399",
    "end": "1500919"
  },
  {
    "text": "you either and you either send zero or you send twice that let's suppose we use",
    "start": "1500919",
    "end": "1506799"
  },
  {
    "text": "probability. five drop out so if you either send zero or you send twice the activation the expected value is the",
    "start": "1506799",
    "end": "1513559"
  },
  {
    "text": "same as the activation but there's some noise in it so you're sending the right expected value but with",
    "start": "1513559",
    "end": "1518840"
  },
  {
    "text": "noise well a Beni distribution it has a a standard deviation equal to the activation",
    "start": "1518840",
    "end": "1525679"
  },
  {
    "text": "because you're either going to send instead of sending the activation you're going to twice of it or noise or zero",
    "start": "1525679",
    "end": "1531399"
  },
  {
    "text": "and so the standard deviation is equal to the activation um you could try using gaussian noise that had that same",
    "start": "1531399",
    "end": "1537279"
  },
  {
    "text": "standard deviation and that same mean value so that would say use multiplicative gausin noise on the",
    "start": "1537279",
    "end": "1544559"
  },
  {
    "text": "activations of the units where the standard deviation of the multiplicative ging noise is equal to the activation",
    "start": "1544559",
    "end": "1551200"
  },
  {
    "text": "that's a lot of multiplic of ging noise obviously you can use less or more but if you use that amount of G ging noise",
    "start": "1551200",
    "end": "1557919"
  },
  {
    "text": "it work about the same as using Dropout I was really hoping it would work worse",
    "start": "1557919",
    "end": "1563320"
  },
  {
    "text": "because there's this cute property that Dropout has a certain standard deviation and it minimizes the entropy",
    "start": "1563320",
    "end": "1571080"
  },
  {
    "text": "of the distribution for that standard deviation and gaussian multiplicative noise has the same mean of standard",
    "start": "1571080",
    "end": "1576399"
  },
  {
    "text": "deviation but maximizes the entropy of the distribution and you'd have thought minimizing versus maximizing might make",
    "start": "1576399",
    "end": "1581600"
  },
  {
    "text": "it work different actually it's pretty much the same and if anything um the gy noise is just slightly better which is",
    "start": "1581600",
    "end": "1587000"
  },
  {
    "text": "very annoying um but you can also use um sort of fake",
    "start": "1587000",
    "end": "1592679"
  },
  {
    "text": "presson noise so rather than actually implementing a presson process what you can do is you can say I'm going to try",
    "start": "1592679",
    "end": "1597840"
  },
  {
    "text": "and get something that has the same mean and variance as a presson process I'm going to have multiplicative noise but",
    "start": "1597840",
    "end": "1603760"
  },
  {
    "text": "the standard deviation of the noise is now going to be the square root of the activation value so the proportional noise gets",
    "start": "1603760",
    "end": "1610559"
  },
  {
    "text": "less as the activation value gets bigger and that works fine too and that",
    "start": "1610559",
    "end": "1615880"
  },
  {
    "text": "strongly suggests that um if you use a Pon process that has the same mean and",
    "start": "1615880",
    "end": "1623320"
  },
  {
    "text": "variance as this Pon multiplicative noise um then that'll work too I haven't",
    "start": "1623320",
    "end": "1629840"
  },
  {
    "text": "actually implemented that because my graduate students all went off and got jobs and I didn't feel like doing myself",
    "start": "1629840",
    "end": "1635120"
  },
  {
    "text": "but um it's bound to work",
    "start": "1635120",
    "end": "1639799"
  },
  {
    "text": "okay so the conclusion about sending accurate real values is this if what a",
    "start": "1640200",
    "end": "1645440"
  },
  {
    "text": "neuron does is computes an underlying rate which is a real value and then sends Plus on spikes according to that",
    "start": "1645440",
    "end": "1654120"
  },
  {
    "text": "rate what it'll achieve is in expectation it's sending the real value",
    "start": "1654120",
    "end": "1659200"
  },
  {
    "text": "but it's adding a huge amount of pron noise to it but adding a huge amount of noise is a very good thing to do if",
    "start": "1659200",
    "end": "1664559"
  },
  {
    "text": "you're in the regime the brain is in that's what allows you to use 10 to the 14 parameters with only 10 to the N",
    "start": "1664559",
    "end": "1670559"
  },
  {
    "text": "training points um so actually it's not a problem that",
    "start": "1670559",
    "end": "1676559"
  },
  {
    "text": "you're only sending these all on noisy spikes it's actually better than being able to send real numbers accurate real",
    "start": "1676559",
    "end": "1683600"
  },
  {
    "text": "numbers okay so that's the end of that objection um now having claimed that",
    "start": "1683600",
    "end": "1690399"
  },
  {
    "text": "what we what I'm going to do is say well look if I give you a theory that works when I send real numbers forwards and",
    "start": "1690399",
    "end": "1695440"
  },
  {
    "text": "backwards you can always turn it into a theory that works when I send spikes which is just the same Theory but with",
    "start": "1695440",
    "end": "1701200"
  },
  {
    "text": "Plus on lots of Plus on noises the regularizer so from now on I'll assume I'm sending real numbers um for most of",
    "start": "1701200",
    "end": "1708480"
  },
  {
    "text": "what I'm going to say but if I can do it with real numbers now I can do it with",
    "start": "1708480",
    "end": "1714840"
  },
  {
    "start": "1718000",
    "end": "1818000"
  },
  {
    "text": "spikes okay so the next question the next of the four questions",
    "start": "1718080",
    "end": "1723200"
  },
  {
    "text": "is how do we send the error derivatives backwards so it's obvious that um if the",
    "start": "1723200",
    "end": "1731279"
  },
  {
    "text": "output of a neuron represents the presence of a feature in the current input you can't also use the output of",
    "start": "1731279",
    "end": "1736760"
  },
  {
    "text": "that neuron to represent the derivative of the error with respect to um the",
    "start": "1736760",
    "end": "1742440"
  },
  {
    "text": "total input received by that neuron which is what you need to communicate backwards in back propagation so you've",
    "start": "1742440",
    "end": "1748120"
  },
  {
    "text": "got a problem you can't actually use the same neuron to send stuff backwards so it obviously has to be a",
    "start": "1748120",
    "end": "1754600"
  },
  {
    "text": "different neuron what's more when you send stuff backwards um this different neuron has",
    "start": "1754600",
    "end": "1761120"
  },
  {
    "text": "to sometimes send positive error derivatives and sometimes send negative error derivatives that is the sign of",
    "start": "1761120",
    "end": "1766760"
  },
  {
    "text": "the thing it's ch sending changes and the effect those have has to change sign",
    "start": "1766760",
    "end": "1772200"
  },
  {
    "text": "two and there's a thing in Neuroscience called Dale's law which says that neurons are either Exit or inhibitory",
    "start": "1772200",
    "end": "1777559"
  },
  {
    "text": "and they can't change the effect they have they can't change from having a positive effect to having a negative effect so one urine will violate Dale's",
    "start": "1777559",
    "end": "1784640"
  },
  {
    "text": "law that's why in feed forward things you have things like on Center off suround neurons and off center on",
    "start": "1784640",
    "end": "1790000"
  },
  {
    "text": "suround neurons um you have a pair of neurons to deal with the sign reversal",
    "start": "1790000",
    "end": "1795360"
  },
  {
    "text": "um and so presumably you need pairs of neurons to deal with the reversal of um",
    "start": "1795360",
    "end": "1800840"
  },
  {
    "text": "The Sign of the error derivative so it's obvious that um you",
    "start": "1800840",
    "end": "1806039"
  },
  {
    "text": "can't use the same neurons to send the error signal backwards but actually that's all",
    "start": "1806039",
    "end": "1811279"
  },
  {
    "text": "nonsense um so here's how you do",
    "start": "1811279",
    "end": "1817240"
  },
  {
    "start": "1818000",
    "end": "1944000"
  },
  {
    "text": "it so this is the sort of biggest claim this is a sort of central claim of this talk that there is a way to represent",
    "start": "1819120",
    "end": "1825519"
  },
  {
    "text": "error in the brain which makes it very easy to do back propagation in fact if you represent error derivatives this way",
    "start": "1825519",
    "end": "1831760"
  },
  {
    "text": "back propagation just emerges um it's just an emergent property of using this representation of",
    "start": "1831760",
    "end": "1837559"
  },
  {
    "text": "error derivatives plus a few other things and the idea is this um if you've",
    "start": "1837559",
    "end": "1843559"
  },
  {
    "text": "got a signal and you want to represent two things well you could represent One Thing by the value of a signal and",
    "start": "1843559",
    "end": "1848960"
  },
  {
    "text": "another thing by the rate at which that value is changing and at least over a short time period you have two",
    "start": "1848960",
    "end": "1854440"
  },
  {
    "text": "independent quantities now the value and the rate of change of the value",
    "start": "1854440",
    "end": "1859559"
  },
  {
    "text": "um so what we're going to do is we're going to say the output of a neuron represents",
    "start": "1859559",
    "end": "1866440"
  },
  {
    "text": "what's going on in the world and the rate at which that output is changing does not represent how fast that",
    "start": "1866440",
    "end": "1873840"
  },
  {
    "text": "property of the world is changing so if I've got a neuron that represents where something is when it's active it says",
    "start": "1873840",
    "end": "1879120"
  },
  {
    "text": "it's here if the activity of that neuron changes it doesn't mean that it's moving the the rate of change of the neuron is",
    "start": "1879120",
    "end": "1886200"
  },
  {
    "text": "going to represent an error derivative if the brain does that it's a really basic decision it's made because it can",
    "start": "1886200",
    "end": "1892760"
  },
  {
    "text": "no longer use rates of change of quantities for representing that that quantity changed and in fact the brain",
    "start": "1892760",
    "end": "1900080"
  },
  {
    "text": "seems to be like that you can't use the rate of change of the activation",
    "start": "1900080",
    "end": "1905600"
  },
  {
    "text": "of a position sensitive neuron to represent the velocity of something if you want to represent velocity use a",
    "start": "1905600",
    "end": "1911480"
  },
  {
    "text": "different neuron and if you want to represent acceleration you don't use the rate of change of the output of a velocity neuron use a different neuron",
    "start": "1911480",
    "end": "1917639"
  },
  {
    "text": "that repr acceleration and so some people get brain damage and where they lose the",
    "start": "1917639",
    "end": "1925039"
  },
  {
    "text": "velocity sensitive neurons and then they can see the position of a car and then they can see the cars here but it didn't",
    "start": "1925039",
    "end": "1931480"
  },
  {
    "text": "move um it's kind of the opposite of a waterfall effect where you see things are moving but in the same",
    "start": "1931480",
    "end": "1938159"
  },
  {
    "text": "position okay um so let me show you an a very early",
    "start": "1938159",
    "end": "1944679"
  },
  {
    "start": "1944000",
    "end": "2117000"
  },
  {
    "text": "use of this idea that you can use temporal derivatives of the outputs to encode error deres with respect to the",
    "start": "1944679",
    "end": "1950919"
  },
  {
    "text": "inputs this was done in 1988 by it's actually done in 1987 by um J MCL and me",
    "start": "1950919",
    "end": "1958799"
  },
  {
    "text": "um the year is actually quite significant I think because I'm fairly sure that that time people hadn't yet",
    "start": "1958799",
    "end": "1965519"
  },
  {
    "text": "discovered Spike time dependent plasticity and what I'll show you is this is actually a spike time dependent",
    "start": "1965519",
    "end": "1970600"
  },
  {
    "text": "plasticity rule so we predicted it we just predicted it with the wrong sign um",
    "start": "1970600",
    "end": "1976159"
  },
  {
    "text": "so this is for an autoencoder what you do is you take an input um you want to",
    "start": "1976159",
    "end": "1982320"
  },
  {
    "text": "train these green weights so they will reconstruct the input and thereby get some interesting representations in",
    "start": "1982320",
    "end": "1988639"
  },
  {
    "text": "these layers and I've shown it for what I call a long Loop that's a loop that's greater than just up and down again um",
    "start": "1988639",
    "end": "1995240"
  },
  {
    "text": "the algorithm works most simply for a short Loop but I've now made it work for a long Loop using logistic units um and",
    "start": "1995240",
    "end": "2002279"
  },
  {
    "text": "the idea is this you start with random weights you send the input round this",
    "start": "2002279",
    "end": "2007320"
  },
  {
    "text": "round the loop once so the first time round is green that'll give you something",
    "start": "2007320",
    "end": "2014200"
  },
  {
    "text": "different here from what the real input was um you then send it around again so",
    "start": "2014200",
    "end": "2020440"
  },
  {
    "text": "that's that red pathway and you take the difference between the activation the first time",
    "start": "2020440",
    "end": "2025960"
  },
  {
    "text": "you sent things around and the second time you send things around and you say I'm going to use that as an error derivative I'm going to change the",
    "start": "2025960",
    "end": "2032440"
  },
  {
    "text": "incoming weights here so as to reduce that difference and it will actually learn to",
    "start": "2032440",
    "end": "2038720"
  },
  {
    "text": "be an auto encoder and notice there's no back propagation um it's actually just well",
    "start": "2038720",
    "end": "2045279"
  },
  {
    "text": "you might call this back propagation because you get information from there to there but it doesn't look any different from the forward propagation",
    "start": "2045279",
    "end": "2050878"
  },
  {
    "text": "here um it also answers the question how can I yeah how can I learn these ways when",
    "start": "2050879",
    "end": "2060320"
  },
  {
    "text": "the only pathway is sort of from there to get information from here to there is this it it actually works it doesn't do",
    "start": "2060320",
    "end": "2067878"
  },
  {
    "text": "work as well as standard back propop but you can make it work um and so the learning rule for a neuron here is to",
    "start": "2067879",
    "end": "2074720"
  },
  {
    "text": "say change the incoming weights of that neuron in proportion to the activity of",
    "start": "2074720",
    "end": "2081079"
  },
  {
    "text": "this neuron in the layer below times the rate of change of this neuron and you put in a minus sign",
    "start": "2081079",
    "end": "2088398"
  },
  {
    "text": "because what you want is that the thing that happened first is right and the thing that happens second is bad you're",
    "start": "2088399",
    "end": "2094440"
  },
  {
    "text": "wandering away from the right thing and so you want to pull it back to the right thing so the thing if the thing that",
    "start": "2094440",
    "end": "2099880"
  },
  {
    "text": "happened first is right and the thing that happened second is bad you need a minus sign here to make the learning go in the right direction and so that's the",
    "start": "2099880",
    "end": "2106040"
  },
  {
    "text": "opposite of Spike time dependent plasticity I will elaborate the connection to spike time dependent plasticity in a minute",
    "start": "2106040",
    "end": "2113440"
  },
  {
    "text": "um but you can actually combine it with spiked independent plasticity",
    "start": "2113440",
    "end": "2119760"
  },
  {
    "start": "2117000",
    "end": "2222000"
  },
  {
    "text": "with the right sign in the following way I gave a talk at nips about this in 2007",
    "start": "2119760",
    "end": "2126079"
  },
  {
    "text": "which met with universal Inc apprehension um you first you can use Spike",
    "start": "2126079",
    "end": "2134599"
  },
  {
    "text": "independent plasticity to sorry reverse spiked independent plasticity to learn a",
    "start": "2134599",
    "end": "2140720"
  },
  {
    "text": "bunch of stacked Auto encoders so you learn One auto encoder you then take the hidden states of it you learn to Auto",
    "start": "2140720",
    "end": "2146640"
  },
  {
    "text": "encode those and you can pile up a bunch of stacks like that so think of those as cortical areas um that are learning",
    "start": "2146640",
    "end": "2152359"
  },
  {
    "text": "representations based on the input and then our problem is if we know something about what we'd like like the final",
    "start": "2152359",
    "end": "2158119"
  },
  {
    "text": "output to be can we get that to influence the early connections so what you're going to do",
    "start": "2158119",
    "end": "2163839"
  },
  {
    "text": "now is Having learned the auto encoders and got them working quite well you're",
    "start": "2163839",
    "end": "2169280"
  },
  {
    "text": "going to do a downwards pass all the way from the top just one downwards pass",
    "start": "2169280",
    "end": "2175480"
  },
  {
    "text": "that uses whatever representation came out of your stack of Auto encoders when you showed it the data so that's your",
    "start": "2175480",
    "end": "2183599"
  },
  {
    "text": "prediction and then you're going to correct your prediction um with the right answer by regressing",
    "start": "2183599",
    "end": "2190319"
  },
  {
    "text": "it towards the right answer and do another downwards pass and you're going to use the difference in activations on",
    "start": "2190319",
    "end": "2196200"
  },
  {
    "text": "those two downward passes as your derivative of the error with respect to",
    "start": "2196200",
    "end": "2201480"
  },
  {
    "text": "the input of each unit um and that corresponds to spike time",
    "start": "2201480",
    "end": "2207960"
  },
  {
    "text": "dependent plasticity with the correct sign because the bad thing came first and the",
    "start": "2207960",
    "end": "2214560"
  },
  {
    "text": "good thing came second so what comes second is better than what came first so let's just have a look at that",
    "start": "2214560",
    "end": "2222720"
  },
  {
    "start": "2222000",
    "end": "2362000"
  },
  {
    "text": "so here's spiked independent plasticity this is what neuroscientists",
    "start": "2222720",
    "end": "2228079"
  },
  {
    "text": "observed um you have a pratic Spike let's suppose it",
    "start": "2228079",
    "end": "2235440"
  },
  {
    "text": "occurs here so an incoming Spike and then the neuron um Fires at some point if it",
    "start": "2235440",
    "end": "2243760"
  },
  {
    "text": "fires before the practic spike what happens is the weight can connecting the two Goes Down And if it fires after the",
    "start": "2243760",
    "end": "2250440"
  },
  {
    "text": "practive spike the weight goes up it's normally interpreted by neuroscientists as could this prap Spike have caused",
    "start": "2250440",
    "end": "2257800"
  },
  {
    "text": "this neuron to fire if it could have caused it let's increase the connection strength but I want to give you a different",
    "start": "2257800",
    "end": "2263119"
  },
  {
    "text": "interpretation um if you look at this thing it looks like a derivative",
    "start": "2263119",
    "end": "2269318"
  },
  {
    "text": "filter and if what you want is to change the connection strength by the activity",
    "start": "2269359",
    "end": "2275720"
  },
  {
    "text": "on the input times the rate change of the output then what you need to do is take",
    "start": "2275720",
    "end": "2281400"
  },
  {
    "text": "that stream of output spikes apply a derivative filter to",
    "start": "2281400",
    "end": "2286800"
  },
  {
    "text": "them and use the output of that derivative filter as the post synaptic term in your learning",
    "start": "2286800",
    "end": "2293440"
  },
  {
    "text": "roomle and so you're really asking did the rate of firing is the rate of firing",
    "start": "2293440",
    "end": "2299480"
  },
  {
    "text": "higher here or here and if the rate of firing is higher here you want to increase the weight and if the rate of",
    "start": "2299480",
    "end": "2305520"
  },
  {
    "text": "firing is higher here you want to decrease the rate that's the right learning rule if you're using the change in the rate of firing",
    "start": "2305520",
    "end": "2312760"
  },
  {
    "text": "to represent the error derivative and it's the change in this underlying rate of firing you only get a noisy",
    "start": "2312760",
    "end": "2318440"
  },
  {
    "text": "observation of this because you just see some spikes um but this thing will give you the right expected value and for",
    "start": "2318440",
    "end": "2326079"
  },
  {
    "text": "stochastic online gradient descent all you have to do is get the expected values right and it'll work it's very",
    "start": "2326079",
    "end": "2331760"
  },
  {
    "text": "robust all the noise so you can interpret Spike independent plasticity",
    "start": "2331760",
    "end": "2336920"
  },
  {
    "text": "as the signature of a system that's using the",
    "start": "2336920",
    "end": "2343680"
  },
  {
    "text": "rate of change of the output to represent the error derivative with respect to the input now let me show you that in a bit",
    "start": "2343680",
    "end": "2350440"
  },
  {
    "text": "more detail",
    "start": "2350440",
    "end": "2354319"
  },
  {
    "text": "um yeah uh so the if you do a forward pass",
    "start": "2357040",
    "end": "2365280"
  },
  {
    "start": "2362000",
    "end": "2411000"
  },
  {
    "text": "through a bunch of stacked Auto encoders and then you do two backward passes um",
    "start": "2365280",
    "end": "2370920"
  },
  {
    "text": "one from whatever you produced and one from the right answer um that difference",
    "start": "2370920",
    "end": "2376680"
  },
  {
    "text": "will give you um your learning signal um if you",
    "start": "2376680",
    "end": "2382160"
  },
  {
    "text": "want to make it more continuous what you do is you go forwards you get the right you get your prediction and then you",
    "start": "2382160",
    "end": "2387440"
  },
  {
    "text": "gradually blend your prediction with the right answer so you regress your prediction towards the right answer and",
    "start": "2387440",
    "end": "2393640"
  },
  {
    "text": "then as you're doing backward passes the rate of change of the neuron",
    "start": "2393640",
    "end": "2398800"
  },
  {
    "text": "as you gradually do this regression will be the post synaptic term that you need",
    "start": "2398800",
    "end": "2404920"
  },
  {
    "text": "to multiply the Pres activity by to do back propagation and I'm going to show you that now",
    "start": "2404920",
    "end": "2411240"
  },
  {
    "start": "2411000",
    "end": "2908000"
  },
  {
    "text": "hopefully so so this is sort of the technical idea",
    "start": "2411240",
    "end": "2417359"
  },
  {
    "text": "of this I'm going to use the rate of change of the output of neuron J that is y do J",
    "start": "2417359",
    "end": "2424880"
  },
  {
    "text": "to represent de by the XJ",
    "start": "2424880",
    "end": "2431119"
  },
  {
    "text": "um I have some Target value I have some output",
    "start": "2431240",
    "end": "2436359"
  },
  {
    "text": "probability when I've driven the system bottom up when it's making a prediction and so what we do is we say",
    "start": "2436359",
    "end": "2443280"
  },
  {
    "text": "let's start off with the output the neuron actually produced and then as time goes by let's",
    "start": "2443280",
    "end": "2452000"
  },
  {
    "text": "change that by regressing it by starting there and then adding in some of the",
    "start": "2452000",
    "end": "2457319"
  },
  {
    "text": "desired output and removing some of the actual output so we're just regressing towards the desired output if you",
    "start": "2457319",
    "end": "2463480"
  },
  {
    "text": "differentiate that with respect to T you'll see you just get DJ minus yj0 and so the rate of change of the",
    "start": "2463480",
    "end": "2470280"
  },
  {
    "text": "output in the top layer is going to be proportional to the derivative of the",
    "start": "2470280",
    "end": "2475560"
  },
  {
    "text": "Cross entropy error because that's what the cross entropy error looks like and that's the derivative of the",
    "start": "2475560",
    "end": "2482640"
  },
  {
    "text": "Cross entropy with respect to the input to a neuron in the last lay",
    "start": "2482640",
    "end": "2488520"
  },
  {
    "text": "um just so we're all on the same page here's what you need to do to do back propagation you need to start off by",
    "start": "2491319",
    "end": "2498319"
  },
  {
    "text": "getting this thing the derivative of the error with respect to the output and then you need to convert that into",
    "start": "2498319",
    "end": "2505560"
  },
  {
    "text": "the derivative of the error with respect to the input and we're going to do that",
    "start": "2505560",
    "end": "2511040"
  },
  {
    "text": "um we're going to represent this thing as y do J the rate of change of the of the uron",
    "start": "2511040",
    "end": "2517760"
  },
  {
    "text": "is going to be representing this now we want to get the same quantity in the previous layer if we can",
    "start": "2517760",
    "end": "2525560"
  },
  {
    "text": "do that we can do back propagation and so what I want to show is if you train a strike of Auto encoders and do this backward pass with the time-changing",
    "start": "2525560",
    "end": "2532400"
  },
  {
    "text": "activities um then if you get the output units so that",
    "start": "2532400",
    "end": "2539040"
  },
  {
    "text": "d by dxj is representing is represented by y.j then automatically in a stack of",
    "start": "2539040",
    "end": "2546359"
  },
  {
    "text": "Auto encoders um the units in the layer below um the",
    "start": "2546359",
    "end": "2553200"
  },
  {
    "text": "same thing will be true the Y do I will be representing d by",
    "start": "2553200",
    "end": "2559040"
  },
  {
    "text": "dxi now the way you do that in back propop is you take this quantity you multiply it by the slope of the",
    "start": "2559040",
    "end": "2565680"
  },
  {
    "text": "nonlinearity here Dy by DX um that's already that's another",
    "start": "2565680",
    "end": "2571800"
  },
  {
    "text": "problem for the brain the brain has neurons that aren't very stable I mean they adapt rapidly in things so they're not arities keep changing slope and so",
    "start": "2571800",
    "end": "2579680"
  },
  {
    "text": "it can't actually know the slope of the nonlinearity because it keeps wandering around and the method I going to show you that has no problem with that",
    "start": "2579680",
    "end": "2585920"
  },
  {
    "text": "because it actually measures the nonlinearity rather the slope of the nonlinearity rather than um knowing",
    "start": "2585920",
    "end": "2591839"
  },
  {
    "text": "it so anyway um you need to do that and then you need to this is the basic back",
    "start": "2591839",
    "end": "2598440"
  },
  {
    "text": "propop step you need to take",
    "start": "2598440",
    "end": "2603000"
  },
  {
    "text": "um this quantity for every neuron in that layer you need to multiply it by",
    "start": "2605119",
    "end": "2610880"
  },
  {
    "text": "the weight on the connection but going in the other direction and you need to add it all",
    "start": "2610880",
    "end": "2616760"
  },
  {
    "text": "up and for neuron I here that will give you de by DYI you then need to take this",
    "start": "2616760",
    "end": "2623720"
  },
  {
    "text": "quantity you computed by adding up all these backwards coming things and put it",
    "start": "2623720",
    "end": "2629520"
  },
  {
    "text": "through the slope of the nonlinearity there so you need to multiply by this thing in order to get d by",
    "start": "2629520",
    "end": "2635319"
  },
  {
    "text": "dxi and I'm going to show you an easy way to do",
    "start": "2635319",
    "end": "2639920"
  },
  {
    "text": "that so let's have two output neurons and one neuron in the layer before just",
    "start": "2640520",
    "end": "2646680"
  },
  {
    "text": "to keep the diagram simple and let's first do a forward pass there's more layers down here so we do a forward pass",
    "start": "2646680",
    "end": "2653280"
  },
  {
    "text": "these are the output neurons we get some actual outputs here we now start regressing those",
    "start": "2653280",
    "end": "2660400"
  },
  {
    "text": "actual outputs towards their desired values and we now replace for this and",
    "start": "2660400",
    "end": "2667319"
  },
  {
    "text": "we replace the bottom up input by top down input that comes from these",
    "start": "2667319",
    "end": "2674119"
  },
  {
    "text": "guys so to begin with the top down input this neuron gets is",
    "start": "2674559",
    "end": "2680480"
  },
  {
    "text": "YJ time wji plus YK * W",
    "start": "2680480",
    "end": "2686119"
  },
  {
    "text": "Ki and because this thing's been trained as an",
    "start": "2686119",
    "end": "2690880"
  },
  {
    "text": "autoencoder that top down input ought to reconstruct what was here so what was",
    "start": "2691480",
    "end": "2696559"
  },
  {
    "text": "here shouldn't change much when you compute it using the green connections instead of computing it",
    "start": "2696559",
    "end": "2702559"
  },
  {
    "text": "bottom up if these two layers formed a good Auto encoder you can reconstruct",
    "start": "2702559",
    "end": "2707880"
  },
  {
    "text": "whatever was here from the activities in the layer above by going backwards through these weights and when you train a restricted",
    "start": "2707880",
    "end": "2714520"
  },
  {
    "text": "bolts machine that's exactly what you're doing you're using the same top down weights as bottom up weights and training it to be good at reconstructing",
    "start": "2714520",
    "end": "2720640"
  },
  {
    "text": "at least if you train it with contrast diverence z um okay so when I use the green",
    "start": "2720640",
    "end": "2727160"
  },
  {
    "text": "connections instead of the black connection when I first use them when I have the actual",
    "start": "2727160",
    "end": "2732280"
  },
  {
    "text": "outputs this neuron won't change much but now I start changing these guys by",
    "start": "2732280",
    "end": "2738079"
  },
  {
    "text": "regressing them towards the correct values the desired values the target values so now what happens here well",
    "start": "2738079",
    "end": "2746640"
  },
  {
    "text": "what's coming along The Green Connection here will be changing and the rate at which it will change will be how fast",
    "start": "2746640",
    "end": "2753160"
  },
  {
    "text": "this guy changes times the value of this weight and what's com along here will be changing it'll be how far this guy",
    "start": "2753160",
    "end": "2759200"
  },
  {
    "text": "changes times the value of this weight so the total input to this neuron here",
    "start": "2759200",
    "end": "2765160"
  },
  {
    "text": "will be changing by wji * y.j plus W Ki *",
    "start": "2765160",
    "end": "2771880"
  },
  {
    "text": "y.k I apologize to mathematicians I never understand equations I always understand it by using a particular",
    "start": "2771880",
    "end": "2777359"
  },
  {
    "text": "example um so now we can ask how fast does the",
    "start": "2777359",
    "end": "2784640"
  },
  {
    "text": "output of Y I change well the output of Yi change will change",
    "start": "2784640",
    "end": "2790040"
  },
  {
    "text": "at a rate that's the rate of change of the input which is what we computed here times the slope of the",
    "start": "2790040",
    "end": "2796520"
  },
  {
    "text": "nonlinearity notice we didn't need to know the slope of the nonlinearity this just happens whatever the slope of the",
    "start": "2796520",
    "end": "2802400"
  },
  {
    "text": "nonlinearity is here if you change this top down signal that's being used to reconstruct it'll change what comes out",
    "start": "2802400",
    "end": "2808800"
  },
  {
    "text": "and the slope of the nonlinearity will get into the ACT um and that is if you",
    "start": "2808800",
    "end": "2814480"
  },
  {
    "text": "take this guy and that guy and if it was the case that um y.j and y.k were",
    "start": "2814480",
    "end": "2821880"
  },
  {
    "text": "representing the error derivatives with respect to the inputs here it will just",
    "start": "2821880",
    "end": "2827640"
  },
  {
    "text": "happen that y do I represents the error derivative with respect to the input to this guy so that is the kind of",
    "start": "2827640",
    "end": "2834880"
  },
  {
    "text": "recursive step in back propagation the idea is um and it just happens",
    "start": "2834880",
    "end": "2840200"
  },
  {
    "text": "automatically using these error derivatives using these temporal derivatives as error derivatives and you",
    "start": "2840200",
    "end": "2845440"
  },
  {
    "text": "don't need to know you can use neur that wander around you don't need to know the slope of the nonlinearity because here you're getting",
    "start": "2845440",
    "end": "2852640"
  },
  {
    "text": "this effect by just putting in a changing input and seeing how the output changes it's kind of weird because if",
    "start": "2852640",
    "end": "2859640"
  },
  {
    "text": "you want the error derivative with respect to the output um you sort of do your",
    "start": "2859640",
    "end": "2864760"
  },
  {
    "text": "computation here once and once you taken the ER with respect to the output that you computed here you put it forward",
    "start": "2864760",
    "end": "2870880"
  },
  {
    "text": "through the neuron and you get the error derivative with respect to the input in these rates of change that's just just",
    "start": "2870880",
    "end": "2877920"
  },
  {
    "text": "an air joint sorry Bic gradient using the a joint yeah yeah that's what common",
    "start": "2877920",
    "end": "2885520"
  },
  {
    "text": "fat do and everything else does yeah it's it's dumb I agree it's just that",
    "start": "2885520",
    "end": "2891119"
  },
  {
    "text": "neuroscientists were determined the brain couldn't do this yeah although I've already I've",
    "start": "2891119",
    "end": "2898319"
  },
  {
    "text": "already let you know what I think of statistic um at least when it comes to very big",
    "start": "2898319",
    "end": "2905319"
  },
  {
    "text": "models um so I already said this if you're using these temporal derivatives error",
    "start": "2905319",
    "end": "2910920"
  },
  {
    "start": "2908000",
    "end": "2920000"
  },
  {
    "text": "derivatives you can't um also use temporal derivatives to represent the",
    "start": "2910920",
    "end": "2916000"
  },
  {
    "text": "temporal derivatives of things of quantities in the world and so we get to",
    "start": "2916000",
    "end": "2921200"
  },
  {
    "start": "2920000",
    "end": "3018000"
  },
  {
    "text": "the last problem and this caused me to give up on this idea in 2007 I thought okay I got",
    "start": "2921200",
    "end": "2927000"
  },
  {
    "text": "this file but it's actually hopeless and the reason it's hopeless is because it requires the top down weights to be the",
    "start": "2927000",
    "end": "2933920"
  },
  {
    "text": "same as the bottom up weights and in particular it requires if I've got two neurons with a connection this way I",
    "start": "2933920",
    "end": "2939520"
  },
  {
    "text": "better have a connection the other way um that's not what the brain's like you",
    "start": "2939520",
    "end": "2946520"
  },
  {
    "text": "don't have this point-wise connectivity you have some connections going this way and some other connections going to the same vague area coming back but not to",
    "start": "2946520",
    "end": "2952760"
  },
  {
    "text": "the same neurons I mean by coincidence it might occasionally happen but in general it doesn't so you don't have this basic",
    "start": "2952760",
    "end": "2959839"
  },
  {
    "text": "property of backrop which is that when the signal comes backwards it comes backwards through the transpose of the",
    "start": "2959839",
    "end": "2966000"
  },
  {
    "text": "weight Matrix through which you went forwards and that seems to be essential for back propop",
    "start": "2966000",
    "end": "2973880"
  },
  {
    "text": "um so um how could this possibly work if",
    "start": "2975760",
    "end": "2980839"
  },
  {
    "text": "you had a neural net in which you've got sparse Connections in both directions",
    "start": "2980839",
    "end": "2985920"
  },
  {
    "text": "with not much overlap between the pairs of neurons that are connected in the two directions well you need a miracle um",
    "start": "2985920",
    "end": "2994440"
  },
  {
    "text": "and in 2014 Tim lrap and his co-workers at Oxford and Toronto um discovered",
    "start": "2994440",
    "end": "3001599"
  },
  {
    "text": "something very surprising they couldn't believe it to be they were using it as a control for something and suddenly it",
    "start": "3001599",
    "end": "3007680"
  },
  {
    "text": "some something that shouldn't have worked and it worked and yes and um it",
    "start": "3007680",
    "end": "3014640"
  },
  {
    "text": "worked and this is used in control and people try using the sh for control",
    "start": "3014640",
    "end": "3021440"
  },
  {
    "start": "3018000",
    "end": "3404000"
  },
  {
    "text": "um so what they did was they used just",
    "start": "3021440",
    "end": "3027160"
  },
  {
    "text": "random connections coming back instead of the transpose of the forward",
    "start": "3027160",
    "end": "3032520"
  },
  {
    "text": "weights now if you use random connections coming back your derivatives",
    "start": "3032520",
    "end": "3037680"
  },
  {
    "text": "don't even have the right signs I mean they're just random by the time they get to the layer or they go down one layer",
    "start": "3037680",
    "end": "3043319"
  },
  {
    "text": "so using random connections coming back if you now just updated the weights using those derivatives you projected",
    "start": "3043319",
    "end": "3048599"
  },
  {
    "text": "back you wouldn't expect it to help and it doesn't I mean that is if you updated",
    "start": "3048599",
    "end": "3053920"
  },
  {
    "text": "the weights from the um one hidden lay to the next hidden layer using as error derivatives these",
    "start": "3053920",
    "end": "3060760"
  },
  {
    "text": "fake derivatives that you get by taking the derivatives in the last hidden layer and mapping them back through random",
    "start": "3060760",
    "end": "3067040"
  },
  {
    "text": "weights yes I'm going to but I'm going to explain why um so the the puzzle is",
    "start": "3068079",
    "end": "3073559"
  },
  {
    "text": "why does it work um when you map back through random",
    "start": "3073559",
    "end": "3079079"
  },
  {
    "text": "weights um I mean I didn't believe it to be if I",
    "start": "3079079",
    "end": "3084480"
  },
  {
    "text": "went and implemented and it works it doesn't work so well for getting things through narrow bottleneck but it works",
    "start": "3084480",
    "end": "3089520"
  },
  {
    "text": "and so for auto autoing CES with narrow bottle neck it doesn't work so well but if you have a wide layer it works just",
    "start": "3089520",
    "end": "3094920"
  },
  {
    "text": "fine um it's a bit slower than backrop normally maybe a fact of too slower but",
    "start": "3094920",
    "end": "3101040"
  },
  {
    "text": "it gets down to similar errors so it works really well it's not that it sort of works a bit it works really well",
    "start": "3101040",
    "end": "3106280"
  },
  {
    "text": "almost as well as during backrop um I'll give you an",
    "start": "3106280",
    "end": "3112240"
  },
  {
    "text": "analogy there's something else like this that works which is in variation inference you try and infer the states",
    "start": "3112240",
    "end": "3119960"
  },
  {
    "text": "of the latent variables and you use something that infers the wrong States and then you go",
    "start": "3119960",
    "end": "3126280"
  },
  {
    "text": "off and do learning under the assumption they're the right States and hey pressor it all works out nicely it's something",
    "start": "3126280",
    "end": "3132200"
  },
  {
    "text": "that wants to work and what's really happening is when you learn the generative model assuming that",
    "start": "3132200",
    "end": "3141119"
  },
  {
    "text": "you got the real posterior distribution even though you didn't the generative model will adapt",
    "start": "3141119",
    "end": "3147599"
  },
  {
    "text": "so that your crummy method of doing inference is a bit more correct and so the generative model can torts itself so",
    "start": "3147599",
    "end": "3156079"
  },
  {
    "text": "as to try and fit in with your appalling way of doing inference and the whole thing works out",
    "start": "3156079",
    "end": "3161599"
  },
  {
    "text": "not too badly um there's basically two kinds of algorithm in learning as far as I can",
    "start": "3161599",
    "end": "3166839"
  },
  {
    "text": "see there's ones where you make some terrible fudge and it wants to work so it contorts itself to make your fudge",
    "start": "3166839",
    "end": "3172960"
  },
  {
    "text": "work and there's others where you make some terrible fudge and it not to work and it exploits the fact that your FG is",
    "start": "3172960",
    "end": "3178960"
  },
  {
    "text": "wrong to work really badly um and variational sorry that's a contrarian",
    "start": "3178960",
    "end": "3186680"
  },
  {
    "text": "principle yes but it's not always contrarian you see in variation it's not",
    "start": "3186680",
    "end": "3191920"
  },
  {
    "text": "contrarian yeah okay I like varation inference is double contrarian it actually works just to show you you",
    "start": "3191920",
    "end": "3197960"
  },
  {
    "text": "weren't right about the contrarian principle yeah um so this is this is one",
    "start": "3197960",
    "end": "3204119"
  },
  {
    "text": "of those weird things that works when it shouldn't works much better than it should",
    "start": "3204119",
    "end": "3210599"
  },
  {
    "text": "um so all I've explained so far is there's other things like this that work when they shouldn't that doesn't really",
    "start": "3212000",
    "end": "3217920"
  },
  {
    "text": "explain how it works um before I go into explaining how",
    "start": "3217920",
    "end": "3225079"
  },
  {
    "text": "it works when you use fixed top down weights it works so obviously if you're slowly changing the top down weights and",
    "start": "3225079",
    "end": "3231920"
  },
  {
    "text": "the feed forward weights contract fast enough it'll still work um and so you should be able to do",
    "start": "3231920",
    "end": "3237680"
  },
  {
    "text": "better than fix top di weights you should be able to slowly change the top time weights so as to for example make",
    "start": "3237680",
    "end": "3243640"
  },
  {
    "text": "the things into better autoencoders and actually that works slightly better it's a small improvement over using fixed top",
    "start": "3243640",
    "end": "3250280"
  },
  {
    "text": "down weights but why does using fixed top time weights work at all well here's a",
    "start": "3250280",
    "end": "3256040"
  },
  {
    "text": "really tricky case here's a case that you wouldn't have thought would work we're going to take M so we're going to",
    "start": "3256040",
    "end": "3262040"
  },
  {
    "text": "take a handwritten digit we're going to take 800 rectified linear units",
    "start": "3262040",
    "end": "3267280"
  },
  {
    "text": "and 800 rectified linear units but they could be Sigma units um and what we're going to do is we're going to use fixed",
    "start": "3267280",
    "end": "3273240"
  },
  {
    "text": "random connections here we're going to use fixed random Spar connections here",
    "start": "3273240",
    "end": "3279520"
  },
  {
    "text": "and the way we're going to get them is we take forward connections and we only put in 25% of",
    "start": "3279520",
    "end": "3286240"
  },
  {
    "text": "them and then for all the neur all the pairs of",
    "start": "3286240",
    "end": "3291720"
  },
  {
    "text": "neurons that weren't connected here we put in a third of the possible connection",
    "start": "3291720",
    "end": "3297040"
  },
  {
    "text": "so we only have 25% of the top down connections too and there's no overlap",
    "start": "3297040",
    "end": "3302160"
  },
  {
    "text": "so we're back projecting through a fixed random sparse Matrix where none of the",
    "start": "3302160",
    "end": "3307799"
  },
  {
    "text": "terms overlap with the forward Matrix um that's much more like the",
    "start": "3307799",
    "end": "3313599"
  },
  {
    "text": "brain than a normal back propop net um but then we have connections",
    "start": "3313599",
    "end": "3319040"
  },
  {
    "text": "here they can be Spar if you like in this I didn't make them sparse but these are adaptive and the question is will",
    "start": "3319040",
    "end": "3325319"
  },
  {
    "text": "these adapt in a sense of way so the experiment goes like this you try learning where these are fixed and",
    "start": "3325319",
    "end": "3332720"
  },
  {
    "text": "you just adapt these and obviously it learns because what you're doing is taking the input randomly recoding it to",
    "start": "3332720",
    "end": "3339359"
  },
  {
    "text": "800 activities here which is keeping it about the same size and then you're learning a simple model here um and that",
    "start": "3339359",
    "end": "3347400"
  },
  {
    "text": "that will learn that'll get down to about 2 and a half% error or something maybe not quite that but it'll get it'll",
    "start": "3347400",
    "end": "3353240"
  },
  {
    "text": "do okay um the question is if you learn these guys and you learn these guys at",
    "start": "3353240",
    "end": "3358480"
  },
  {
    "text": "the same time using as your learning signal for these",
    "start": "3358480",
    "end": "3363680"
  },
  {
    "text": "units the errors you got there which were correct back propagated through the stick strander",
    "start": "3363680",
    "end": "3370160"
  },
  {
    "text": "metric um multiplied by the slopes of the nonlinearities here back propagated through this one multiply by the slopes",
    "start": "3370160",
    "end": "3376480"
  },
  {
    "text": "of nonlinearities here and those give you your fake derives here they're going to use for learning does it help and yes",
    "start": "3376480",
    "end": "3382359"
  },
  {
    "text": "it helps a whole lot it uses these connections very nicely and builds nice receptive fields um so something's",
    "start": "3382359",
    "end": "3387480"
  },
  {
    "text": "really working here even though it's going through these random connections um you have to do things like this to",
    "start": "3387480",
    "end": "3394359"
  },
  {
    "text": "convince yourself that it really is working it's not because it was so surprising and now I'm going to give you",
    "start": "3394359",
    "end": "3399440"
  },
  {
    "text": "an intuitive explanation about why it works at least how it gets off the",
    "start": "3399440",
    "end": "3405599"
  },
  {
    "start": "3404000",
    "end": "5113000"
  },
  {
    "text": "ground so what we can do is we can freeze the last layer of weights and",
    "start": "3405799",
    "end": "3410960"
  },
  {
    "text": "just learn the earlier weights and if we do that we notice something when you freeze the last layer",
    "start": "3410960",
    "end": "3418119"
  },
  {
    "text": "of Weights that are going to be used to produce to propagate to produce um",
    "start": "3418119",
    "end": "3424400"
  },
  {
    "text": "predictions which then get sent backwards through random weights the last layer weights is",
    "start": "3424400",
    "end": "3429640"
  },
  {
    "text": "actually learning correctly to begin with because it's looking at the output and looking at what's coming in and it's",
    "start": "3429640",
    "end": "3436319"
  },
  {
    "text": "actually really learning properly if you turn off that learning it appears that",
    "start": "3436319",
    "end": "3441599"
  },
  {
    "text": "no learning will no useful learning goes on so if you freeze the last layer w",
    "start": "3441599",
    "end": "3447119"
  },
  {
    "text": "and you learn the earlier layers using these fixed random backwards connections it doesn't improve at all it just gets",
    "start": "3447119",
    "end": "3453160"
  },
  {
    "text": "slightly worse as you learn and so you think that it's not actually learning",
    "start": "3453160",
    "end": "3459280"
  },
  {
    "text": "anything but actually it's learning a lot what it's learning is not making the error go down it's not making the output",
    "start": "3459280",
    "end": "3465720"
  },
  {
    "text": "error go down but it's going to make it much easier for the last letter of weights to learn when you do",
    "start": "3465720",
    "end": "3471960"
  },
  {
    "text": "turn so are you breaking the loop when you turn something off",
    "start": "3471960",
    "end": "3477599"
  },
  {
    "text": "no no the connections are still there we're just not changing them but we have four connections from the last letter to",
    "start": "3477599",
    "end": "3483920"
  },
  {
    "text": "the output they're just not adapting okay so there are Loops yeah if you didn't do that I don't think it would",
    "start": "3483920",
    "end": "3489599"
  },
  {
    "text": "make sense at all um so here's what's happening if you",
    "start": "3489599",
    "end": "3498920"
  },
  {
    "text": "think what happens for if you got say 10 output classes I going you use the softmax of the output but this also",
    "start": "3498920",
    "end": "3505319"
  },
  {
    "text": "applies to linear to regression um the actual output will they'll all be about",
    "start": "3505319",
    "end": "3510880"
  },
  {
    "text": "equal if we start off with small weights okay so we have small weights going to the output they don't have much effect",
    "start": "3510880",
    "end": "3516880"
  },
  {
    "text": "on the output the actual outputs all look pretty much like that the desired outputs when you have the fourth a",
    "start": "3516880",
    "end": "3524039"
  },
  {
    "text": "member of the fourth class as the input um will look like that and so the",
    "start": "3524039",
    "end": "3529559"
  },
  {
    "text": "derivative of the error with respect to the input to the final layer of units would look like this it says if",
    "start": "3529559",
    "end": "3536680"
  },
  {
    "text": "you want to make the error bigger decrease this guy and increase those guys or if you want to make the error smaller decrease these guys and",
    "start": "3536680",
    "end": "3543599"
  },
  {
    "text": "increase this guy um and notice that if you have different",
    "start": "3543599",
    "end": "3551000"
  },
  {
    "text": "instances of the fourth class that have nothing to do with one another they're just very different inputs but they're classified as the fourth",
    "start": "3551000",
    "end": "3557359"
  },
  {
    "text": "class they will get the same error dtive coming back from the output there",
    "start": "3557359",
    "end": "3562480"
  },
  {
    "text": "because the a d is determined by the class it's dominated by what the class",
    "start": "3562480",
    "end": "3567640"
  },
  {
    "text": "is so now what happens is you take this error error vector and you map it",
    "start": "3567640",
    "end": "3573119"
  },
  {
    "text": "backwards through the last layer of backwards connections which is just random weights and so they'll do some",
    "start": "3573119",
    "end": "3579079"
  },
  {
    "text": "sort of random rotation and scaling and stuff of it but if you had two things that the",
    "start": "3579079",
    "end": "3584160"
  },
  {
    "text": "same and you m them through a random Matrix they're going to end up the same pretty much they're going to end up very similar unless it's some weird Matrix so",
    "start": "3584160",
    "end": "3591760"
  },
  {
    "text": "what you're getting now is that you get these fake derivatives to come back that are nothing to do with the real derivatives",
    "start": "3591760",
    "end": "3597119"
  },
  {
    "text": "and if you change the units in that direction it's not going to make life better for you at the output but for",
    "start": "3597119",
    "end": "3603559"
  },
  {
    "text": "members of the same class you get the same derivatives and for members of different classes you get different",
    "start": "3603559",
    "end": "3608640"
  },
  {
    "text": "derivatives so all the different members of the same class will try and move the",
    "start": "3608640",
    "end": "3613880"
  },
  {
    "text": "activities in the same direction and what'll happen you've done a little bit of learning is that all the different",
    "start": "3613880",
    "end": "3620200"
  },
  {
    "text": "members of the same class even if they had nothing to do with each other but you designate them as being of the same",
    "start": "3620200",
    "end": "3625760"
  },
  {
    "text": "class class so you could just take random data and just designate the first 10 is being class one the next 10 is",
    "start": "3625760",
    "end": "3631079"
  },
  {
    "text": "being class two the next 10 is being class three they got nothing to do with each other but when you turn on the",
    "start": "3631079",
    "end": "3636160"
  },
  {
    "text": "learning now things that are designated as class 4 will all get the same error",
    "start": "3636160",
    "end": "3642599"
  },
  {
    "text": "Vector coming back and so in the last hidden layer they'll all learn to have pretty much the same",
    "start": "3642599",
    "end": "3648880"
  },
  {
    "text": "representation um that's basically why it works that's",
    "start": "3648880",
    "end": "3654839"
  },
  {
    "text": "what's going on when you Lear without adapting the last layer so you can see how this thing's working it's making",
    "start": "3654839",
    "end": "3662160"
  },
  {
    "text": "things that have the same class have similar representations in the hidden layers and it's recursive um I'll show",
    "start": "3662160",
    "end": "3669480"
  },
  {
    "text": "you that in a minute so this is the this is an example",
    "start": "3669480",
    "end": "3675400"
  },
  {
    "text": "I implemented you have Frozen but small forward weights there you have fixed",
    "start": "3675400",
    "end": "3681039"
  },
  {
    "text": "random weights here and you have adaptive random weights there and you do this you do back propop um treating what",
    "start": "3681039",
    "end": "3689160"
  },
  {
    "text": "comes the error derivatives here mapped through these fixed random weights as if they were the true derivatives and as you learn as you",
    "start": "3689160",
    "end": "3696799"
  },
  {
    "text": "adapt these weights the error doesn't go down the error if anything goes slightly up well it basically stays at random",
    "start": "3696799",
    "end": "3704200"
  },
  {
    "text": "[Music] um okay but when you turn now turn on",
    "start": "3704200",
    "end": "3709680"
  },
  {
    "text": "the learning there what's happened is things of the same class here have very similar representations there and so the",
    "start": "3709680",
    "end": "3716359"
  },
  {
    "text": "learning here is Trivial it learns very fast um so you were actually doing",
    "start": "3716359",
    "end": "3721720"
  },
  {
    "text": "something very useful you were constructing rep you making representations of things that were the same class be very similar and it works",
    "start": "3721720",
    "end": "3728680"
  },
  {
    "text": "through multiple layers oh this is just an example of that this is 100 random",
    "start": "3728680",
    "end": "3734039"
  },
  {
    "text": "vectors 784 they're random binary vectors of length 784 um the first 10 are designated class",
    "start": "3734039",
    "end": "3741920"
  },
  {
    "text": "one the next 10 at class two I train without",
    "start": "3741920",
    "end": "3746960"
  },
  {
    "text": "adapting I don't adapt these weights at all and after training just these",
    "start": "3746960",
    "end": "3752760"
  },
  {
    "text": "weights if you look at the representations here and measure the co-variances between these",
    "start": "3752760",
    "end": "3758279"
  },
  {
    "text": "vectors you get that you can see all the first 10 all have pretty much the same",
    "start": "3758279",
    "end": "3763520"
  },
  {
    "text": "representation the next 10 have a very different and pretty much the same representation and so on so it's obvious",
    "start": "3763520",
    "end": "3768599"
  },
  {
    "text": "that if that's your representation of the things in the different classes learning is Trivial and that works through multiple",
    "start": "3768599",
    "end": "3774240"
  },
  {
    "text": "layers so once the activities in the last hidden layer are very similar for",
    "start": "3774240",
    "end": "3780319"
  },
  {
    "text": "members of the same class then the slope of the nonlinearity will",
    "start": "3780319",
    "end": "3786039"
  },
  {
    "text": "be very similar for members of the same class and so now when you take the output error you put it through one",
    "start": "3786039",
    "end": "3791599"
  },
  {
    "text": "random Matrix you multiply by the slopes of the nonlinearity which are the same for all members of the same class and",
    "start": "3791599",
    "end": "3796960"
  },
  {
    "text": "you put it through another random Matrix you'll get very similar error vectors in the layer below so it's not just the",
    "start": "3796960",
    "end": "3802480"
  },
  {
    "text": "last layer this works so as soon as that one's of got Isa beginning to get attack together other ones are learning too and",
    "start": "3802480",
    "end": "3808720"
  },
  {
    "text": "so you can actually learn lots of layers of representation to make the input get progressively more similar to things",
    "start": "3808720",
    "end": "3813920"
  },
  {
    "text": "designated as the same class and progressively more different for things of different classes",
    "start": "3813920",
    "end": "3819599"
  },
  {
    "text": "um without ever decreasing the error in the output um",
    "start": "3819599",
    "end": "3826960"
  },
  {
    "text": "okay so just to summarize the fact that we send SP the",
    "start": "3826960",
    "end": "3833880"
  },
  {
    "text": "neuron send spikes rather than real numbers isn't problem that's just because you've got a huge model with not",
    "start": "3833880",
    "end": "3839559"
  },
  {
    "text": "much data and having lots of nois is the right thing to do um by representing error derivatives as",
    "start": "3839559",
    "end": "3846760"
  },
  {
    "text": "tempal derivatives you can get back propagation to happen automatically if you've got Auto encoders if you first",
    "start": "3846760",
    "end": "3853240"
  },
  {
    "text": "learned a bunch of Auto encoders so that you reconstruct at the layer below um and then you start changing the signal",
    "start": "3853240",
    "end": "3859520"
  },
  {
    "text": "that's During the Reconstruction watch how the Reconstruction changes and actually spiked independent",
    "start": "3859520",
    "end": "3865839"
  },
  {
    "text": "plasticity is what you'd expect to see if you were using this scheme for encoding error derivatives if the rate",
    "start": "3865839",
    "end": "3872000"
  },
  {
    "text": "of change of the underlying firing rate was what represented the error derivative you'd have to apply",
    "start": "3872000",
    "end": "3877079"
  },
  {
    "text": "derivative filter to the spike train and use the output of that derivative filter to control the learning that's spiked independent",
    "start": "3877079",
    "end": "3882880"
  },
  {
    "text": "plasticity um and the problem that caused me to give up before which is that you don't",
    "start": "3882880",
    "end": "3890000"
  },
  {
    "text": "have top down connections that the transposed the bottom up connections well it works anyway um because it's",
    "start": "3890000",
    "end": "3896119"
  },
  {
    "text": "it's contrarian contrarian um it's called noise cancel okay that's how you",
    "start": "3896119",
    "end": "3902880"
  },
  {
    "text": "can Define it okay I'm",
    "start": "3902880",
    "end": "3909000"
  },
  {
    "text": "done sure you believe",
    "start": "3923799",
    "end": "3927640"
  },
  {
    "text": "excellent presentation I just wanted to ask a question about uh representing these two states the uh derivative and",
    "start": "3932520",
    "end": "3939720"
  },
  {
    "text": "the activation um in actual neurons they're there's a",
    "start": "3939720",
    "end": "3946119"
  },
  {
    "text": "biochemical so proteins like hulin that are stored locally in the synapse or right before the synapse in the prestic",
    "start": "3946119",
    "end": "3952359"
  },
  {
    "text": "neural and in the Pro synaptic neural and the pr synaptic neuron fires and",
    "start": "3952359",
    "end": "3957520"
  },
  {
    "text": "it's incident on the postoptic neuron the state of calmodulin biases different biochemical Cascades and it's that state",
    "start": "3957520",
    "end": "3964839"
  },
  {
    "text": "of the protein that changes over time it's thought to be one of the drivers of long-term plasticity in other words it's",
    "start": "3964839",
    "end": "3970680"
  },
  {
    "text": "the variable that stored in spite in your model are you sort of approximating these sort of biochemical",
    "start": "3970680",
    "end": "3977799"
  },
  {
    "text": "Cascades in America okay so the model we've got at",
    "start": "3977799",
    "end": "3983279"
  },
  {
    "text": "present um we haven't really filled in all the details of this bit um the the",
    "start": "3983279",
    "end": "3988720"
  },
  {
    "text": "reason I didn't show you a sort of complete simulation of something using entirely spikes and learning and doing backprop is because as you could",
    "start": "3988720",
    "end": "3996039"
  },
  {
    "text": "probably tell from the presentation there's still a lot of wiggle room left in exactly how you do the how you",
    "start": "3996039",
    "end": "4003440"
  },
  {
    "text": "organize the forward pass and then these two backwards passes um",
    "start": "4003440",
    "end": "4008640"
  },
  {
    "text": "and I'd be very interested to read about the biochemical mechanisms um that's",
    "start": "4008640",
    "end": "4014440"
  },
  {
    "text": "what we want to implement but I think there's still quite a few ways of implementing",
    "start": "4014440",
    "end": "4020119"
  },
  {
    "text": "that yeah sotion that in first part of the people a person has t toing",
    "start": "4022480",
    "end": "4031599"
  },
  {
    "text": "parameters and data samples so do things uh there",
    "start": "4031599",
    "end": "4037680"
  },
  {
    "text": "will be a new surve out there for this regime of models",
    "start": "4037680",
    "end": "4042720"
  },
  {
    "text": "and so for for different there no so what kind of so I I mean if I",
    "start": "4042720",
    "end": "4049960"
  },
  {
    "text": "understood the question right are we going to get sort of new kinds of learning algorithm for this funny regime",
    "start": "4049960",
    "end": "4055520"
  },
  {
    "text": "where you have you assume computation is almost free you assume have a reasonable size data set but now because",
    "start": "4055520",
    "end": "4062480"
  },
  {
    "text": "computation is cheap and because you care you're going to apply a huge amount of computation and have a vast model in",
    "start": "4062480",
    "end": "4068760"
  },
  {
    "text": "the hope that you can generalize better um yeah I think that's a different regime it's a regime that's hardly been",
    "start": "4068760",
    "end": "4074039"
  },
  {
    "text": "explored so I a bit for mist so for mist you've got 60,000 training examples so",
    "start": "4074039",
    "end": "4080200"
  },
  {
    "text": "you can train a model with 100,000 connections and it works okay you train",
    "start": "4080200",
    "end": "4085240"
  },
  {
    "text": "a model with a million connections it works better um okay so you train a model with 10 million connections and a",
    "start": "4085240",
    "end": "4091720"
  },
  {
    "text": "good regular and a regularizer now and that works that generalizes better okay so now you're trying to order with 100",
    "start": "4091720",
    "end": "4098080"
  },
  {
    "text": "million connections and a good regularizer that generalizes even a little bit better if you were to go to half a",
    "start": "4098080",
    "end": "4105080"
  },
  {
    "text": "billion connections that would be about the same ratio of parameters to training",
    "start": "4105080",
    "end": "4110640"
  },
  {
    "text": "cases that the brain has so but 100 million is getting close and the point is that 100 million parameters for",
    "start": "4110640",
    "end": "4117359"
  },
  {
    "text": "60,000 training cases a statistician would have told you you're completely insane and it's it will overfit horribly",
    "start": "4117359",
    "end": "4123920"
  },
  {
    "text": "and it will never generalize and that's just because they didn't have really strong",
    "start": "4123920",
    "end": "4129838"
  },
  {
    "text": "regularizers it turns out it's better to use more parameters than a strong regularizer",
    "start": "4129839",
    "end": "4136278"
  },
  {
    "text": "yeah um you you um um have certainly um persuaded me that the brain could do",
    "start": "4140799",
    "end": "4147040"
  },
  {
    "text": "back propagation you you suggested that um it worked so well and chances are",
    "start": "4147040",
    "end": "4152560"
  },
  {
    "text": "that evu would have found that but do do you think the brain actually does do prop back propagation or do you think",
    "start": "4152560",
    "end": "4157798"
  },
  {
    "text": "it's got other tricks up its sleep which we still I don't know I don't know I",
    "start": "4157799",
    "end": "4162920"
  },
  {
    "text": "just object to neuroscientists saying it couldn't possibly do it um and the arguments they give you",
    "start": "4162920",
    "end": "4170838"
  },
  {
    "text": "are I think what's going on or what was going on until fairly recently they had all the arguments I",
    "start": "4170839",
    "end": "4176798"
  },
  {
    "text": "gave plus another argument which was and anyway back propagation isn't that great you know support Factor machines work",
    "start": "4176799",
    "end": "4182199"
  },
  {
    "text": "better um so we don't really need to go into this argument do we because back propagation clearly isn't the answer to",
    "start": "4182199",
    "end": "4187798"
  },
  {
    "text": "everything but once you've establish the back propagation is actually the answer to everything um then it becomes",
    "start": "4187799",
    "end": "4194760"
  },
  {
    "text": "interesting about whether the brain really could do it and then you start looking at these arguments and you notice all of the arguments seem like",
    "start": "4194760",
    "end": "4201239"
  },
  {
    "text": "quite good arguments but none of them are insuperable and so if you really",
    "start": "4201239",
    "end": "4206400"
  },
  {
    "text": "think the brain's got a big motivation for doing it um then maybe it can but of course there",
    "start": "4206400",
    "end": "4214480"
  },
  {
    "text": "may be some other algorithm that's better than back propagation and of course when you're using these random backwards weights that's not exactly",
    "start": "4214480",
    "end": "4220600"
  },
  {
    "text": "back propagation anymore um it's in the same ballpark but it's not actually back propagation",
    "start": "4220600",
    "end": "4227320"
  },
  {
    "text": "um so my feeling is it's probably using something like back propagation that's maybe not exactly back propagation um",
    "start": "4227320",
    "end": "4235440"
  },
  {
    "text": "because that's what works and the sort of more General point is that I think",
    "start": "4235440",
    "end": "4241600"
  },
  {
    "text": "there has to be some way in which feature detectors early in the sensory",
    "start": "4241600",
    "end": "4247560"
  },
  {
    "text": "pathway get information about their effects so that their effects later on",
    "start": "4247560",
    "end": "4253520"
  },
  {
    "text": "so that they can adapt to be more useful later on it seems crazy not to do",
    "start": "4253520",
    "end": "4259960"
  },
  {
    "text": "that and that's sort of what back propagation does and there's other ways of doing that that aren't quite back",
    "start": "4259960",
    "end": "4265440"
  },
  {
    "text": "propagation like using feedback alignment um that's what I really believe in that somehow the",
    "start": "4265440",
    "end": "4270960"
  },
  {
    "text": "information's getting back um so these things are adapting to be useful now it may be that most of the",
    "start": "4270960",
    "end": "4277800"
  },
  {
    "text": "adaptation is just so they can model what's below them it's mostly unsupervised but there has to be some element of adapting to be useful",
    "start": "4277800",
    "end": "4286239"
  },
  {
    "text": "yeah",
    "start": "4286960",
    "end": "4289960"
  },
  {
    "text": "ter terasi taught this computational Neuroscience classs years where he he",
    "start": "4292040",
    "end": "4297719"
  },
  {
    "text": "taught us to model Quant with discrete packets of neurotransmitter being released from a pratic neuron using a",
    "start": "4297719",
    "end": "4303600"
  },
  {
    "text": "poon distribution I was intrigued by your poison distribution uh as a way of",
    "start": "4303600",
    "end": "4309960"
  },
  {
    "text": "sampling eror ads instead of using things like Dropout I was wondering if there was any relationship between these",
    "start": "4309960",
    "end": "4317000"
  },
  {
    "text": "two ideas um I guess the relationship is simply that if neurons are behaving",
    "start": "4317000",
    "end": "4322679"
  },
  {
    "text": "somewhat plon like a lot of that's coming from these discrete Quantum because that really is a plon",
    "start": "4322679",
    "end": "4329120"
  },
  {
    "text": "process so a lot of it's in the um amount of transmitter release that goes on we know that's very noisy if you",
    "start": "4329120",
    "end": "4336159"
  },
  {
    "text": "inject charge straight into a neuron it's much less noisy",
    "start": "4336159",
    "end": "4341520"
  },
  {
    "text": "is there any difference between the kinds of errors that come out from the real back call it the real back",
    "start": "4345880",
    "end": "4352080"
  },
  {
    "text": "propagation the original one and the uh ones that come out of this uh uh",
    "start": "4352080",
    "end": "4358719"
  },
  {
    "text": "propagation to make the output be the same as the input it's a good question and I don't",
    "start": "4358719",
    "end": "4365080"
  },
  {
    "text": "think anybody's really looked at that um this is all stuff's fairly new and we've mostly been concerned with why on Earth",
    "start": "4365080",
    "end": "4371600"
  },
  {
    "text": "does this stuff work um what's going on um obviously if there was some",
    "start": "4371600",
    "end": "4377679"
  },
  {
    "text": "significant difference between the kinds of Errors they get you might look for that and see which way people",
    "start": "4377679",
    "end": "4383120"
  },
  {
    "text": "go but I don't think anybody's really looked at it they just looked at this kind of error",
    "start": "4383120",
    "end": "4389480"
  },
  {
    "text": "R yeah seems like this makes this makes the prediction that the brain is or that",
    "start": "4389840",
    "end": "4395960"
  },
  {
    "text": "cortical layers are filled with autoencoders kind of at every layer um do you know if that's actually true the",
    "start": "4395960",
    "end": "4401760"
  },
  {
    "text": "between LS it's an aut encoder um no I don't know if it's true I think it'd be",
    "start": "4401760",
    "end": "4407360"
  },
  {
    "text": "a very good idea but if you were designed it if I designed",
    "start": "4407360",
    "end": "4414320"
  },
  {
    "text": "it yes there was a recent paper said that neural be encoded in 4.7 bits um",
    "start": "4415159",
    "end": "4424080"
  },
  {
    "text": "have you any comment on that this was a big deal because it's",
    "start": "4424080",
    "end": "4429679"
  },
  {
    "text": "more than they thought it was before it's 27 States or something this is for waiter activity a",
    "start": "4429679",
    "end": "4436719"
  },
  {
    "text": "weight weight and a particular connection okay in some cases I understand that",
    "start": "4436719",
    "end": "4443480"
  },
  {
    "text": "there's several connections to the [Music] same but an individual synapse oh that",
    "start": "4443480",
    "end": "4450960"
  },
  {
    "text": "was the stuff that Terry was involved in yeah um says it's about 4.7 bits per weight um I don't really have much to",
    "start": "4450960",
    "end": "4458040"
  },
  {
    "text": "say about that it would mean though you could use some pretty simple arithmetic",
    "start": "4458040",
    "end": "4463360"
  },
  {
    "text": "to to you do that people who want things to go fast already do use um they use",
    "start": "4463360",
    "end": "4471400"
  },
  {
    "text": "for example 8 bit weights yeah",
    "start": "4471400",
    "end": "4476760"
  },
  {
    "text": "that's so I guess that prop gives you the gradient and in practice people have been getting some mileage out of you",
    "start": "4479520",
    "end": "4486080"
  },
  {
    "text": "know doing things like momentum or per feature scaling so do any of these",
    "start": "4486080",
    "end": "4491159"
  },
  {
    "text": "things exist in um a more general question would",
    "start": "4491159",
    "end": "4498000"
  },
  {
    "text": "be you'd really like to sort of combine stochastic gradient descent with something more like a second order",
    "start": "4498000",
    "end": "4503880"
  },
  {
    "text": "method that goes in a better Direction I mean it's sort of crazy to go and do steepest descent you'd like to get",
    "start": "4503880",
    "end": "4509480"
  },
  {
    "text": "something more like the natural gradient and I got very excited about the idea that actually feedback",
    "start": "4509480",
    "end": "4516280"
  },
  {
    "text": "alignment isn't giving you the transpose of the forward matrix it's giving you the pseudo inverse and what or it's giving you",
    "start": "4516280",
    "end": "4522800"
  },
  {
    "text": "something closer to the pseudo inverse and so it's saying what do I have to do to get back to the thing I want to",
    "start": "4522800",
    "end": "4528800"
  },
  {
    "text": "reconstruct um if you train it you'll get that so with the pseudo inverse you can",
    "start": "4528800",
    "end": "4536239"
  },
  {
    "text": "figure out how I would have to change the outputs of neurons in one layer to get rid of the eror at the next lay um",
    "start": "4536239",
    "end": "4544000"
  },
  {
    "text": "and that's a bit more like a second order method um that is it behaves quite differently in extreme cases when you",
    "start": "4544000",
    "end": "4550719"
  },
  {
    "text": "have very high correlations of things um but I didn't managed to get anything to",
    "start": "4550719",
    "end": "4555800"
  },
  {
    "text": "work where you could somehow use the factors more like the stud invers to make it more second order I got very excited about that but I couldn't get",
    "start": "4555800",
    "end": "4562639"
  },
  {
    "text": "anywhere I hadn't totally given up on it but yeah uh any evidence about the",
    "start": "4562639",
    "end": "4569560"
  },
  {
    "text": "random uh [Music]",
    "start": "4569560",
    "end": "4575190"
  },
  {
    "text": "connection as opposed to no I mean in real life RS is there any evidence for the those",
    "start": "4578280",
    "end": "4585080"
  },
  {
    "text": "kinds of edges or I youan I guess you mean random weights",
    "start": "4585080",
    "end": "4591679"
  },
  {
    "text": "that simply don't adapt understand random",
    "start": "4591679",
    "end": "4596880"
  },
  {
    "text": "edges they're random in the sense that you don't get detailed onetoone",
    "start": "4598520",
    "end": "4604320"
  },
  {
    "text": "connections but the whole thing's full of topographic maps and if an part of",
    "start": "4604320",
    "end": "4610440"
  },
  {
    "text": "this map connects sh to this map then this will send connections back to the same place roughly",
    "start": "4610440",
    "end": "4615719"
  },
  {
    "text": "so there's locality but not Point too locality and that so the connections are",
    "start": "4615719",
    "end": "4622040"
  },
  {
    "text": "not random in that sense yeah do you randomly activate the the",
    "start": "4622040",
    "end": "4630320"
  },
  {
    "text": "that case there's 25 neurons uh going one way 25 the other middle layer do you",
    "start": "4630320",
    "end": "4638400"
  },
  {
    "text": "randomly activate 25 sorry it was 25% of the possible",
    "start": "4638400",
    "end": "4643719"
  },
  {
    "text": "connections are there 25 choose but I've also done it where you",
    "start": "4643719",
    "end": "4649360"
  },
  {
    "text": "choose those from you have topographic maps you have a location in this layer a",
    "start": "4649360",
    "end": "4654480"
  },
  {
    "text": "location of a neuron in this layer and then you connect it to neurons that have similar locations in the layer below and",
    "start": "4654480",
    "end": "4660520"
  },
  {
    "text": "you can choose sort of how big you make the neighborhood and how dense you make the connections and you'd expect that to work better if you're dealing with",
    "start": "4660520",
    "end": "4667040"
  },
  {
    "text": "spatially organized input because you expect that if you're dealing with an image you want to start by processing",
    "start": "4667040",
    "end": "4672400"
  },
  {
    "text": "one part of it locally all the connections are there from the",
    "start": "4672400",
    "end": "4678800"
  },
  {
    "text": "full 400 both ways all you're doing is randomly selecting which ones are going to fire and which ones are not but you",
    "start": "4678800",
    "end": "4685800"
  },
  {
    "text": "just do that once so at the start of the simulation you decide which connections are there and which",
    "start": "4685800",
    "end": "4691719"
  },
  {
    "text": "aren't um now as a matter of fact to make the programming easy you have them",
    "start": "4691719",
    "end": "4697440"
  },
  {
    "text": "all there and you let them all learn and then 75 of them you set their weights back to zero again but that's just",
    "start": "4697440",
    "end": "4703960"
  },
  {
    "text": "program have you done anything where you played with a stochastic variation of the",
    "start": "4703960",
    "end": "4711040"
  },
  {
    "text": "number of Weights that you feed forward back so rather than having a fix 25% some no I just I mean we've done",
    "start": "4711040",
    "end": "4719800"
  },
  {
    "text": "very few experiments on the connectivity we tried local connectivity and that's nice for images",
    "start": "4719800",
    "end": "4725679"
  },
  {
    "text": "um it's it's more at the stage of we we're trying to understand why on Earth",
    "start": "4725679",
    "end": "4731040"
  },
  {
    "text": "this works at all because it on the face of it it shouldn't work",
    "start": "4731040",
    "end": "4736280"
  },
  {
    "text": "seems might just sort of adding effec that noise because you're not everything",
    "start": "4736280",
    "end": "4741679"
  },
  {
    "text": "so preventing possible maybe",
    "start": "4741679",
    "end": "4747360"
  },
  {
    "text": "but at least if you don't have backward connections where you have for connections you're clearly not doing",
    "start": "4747360",
    "end": "4753360"
  },
  {
    "text": "exactly back propagation and so the problem isn't so much preventing overfitting as how can",
    "start": "4753360",
    "end": "4758440"
  },
  {
    "text": "it do fitting at all yeah",
    "start": "4758440",
    "end": "4765520"
  },
  {
    "text": "so a lot of times you know when an animal goes to a place the activity place will rise and then fall does that",
    "start": "4765520",
    "end": "4771920"
  },
  {
    "text": "mean sort of before it enters the place it's the wrong place when it leaves the place it's the right place if you map H",
    "start": "4771920",
    "end": "4778880"
  },
  {
    "text": "there how would you interpret well the fire rate goes up and then the fireing rate goes down again so as it's coming",
    "start": "4778880",
    "end": "4786199"
  },
  {
    "text": "into that location um",
    "start": "4786199",
    "end": "4792719"
  },
  {
    "text": "then yeah",
    "start": "4792719",
    "end": "4796639"
  },
  {
    "text": "[Music] yeah for that place then it's getting",
    "start": "4799550",
    "end": "4805000"
  },
  {
    "text": "more active as it com to that location and so you'd want to increase the",
    "start": "4805000",
    "end": "4810960"
  },
  {
    "text": "incoming weights on anything that's coming in and then as it goes out you want to decrease the weights um and so",
    "start": "4810960",
    "end": "4818280"
  },
  {
    "text": "you'd get you tend to get this procession would you you tend to get the location moving back towards where it came from",
    "start": "4818280",
    "end": "4826639"
  },
  {
    "text": "in other words it'll start predicting rather I haven't really thought about",
    "start": "4826639",
    "end": "4832960"
  },
  {
    "text": "that issue so in the top so most of the",
    "start": "4832960",
    "end": "4840880"
  },
  {
    "text": "models are using fully connected uh layers but uh another current success of",
    "start": "4840880",
    "end": "4847760"
  },
  {
    "text": "uh deep learning is also build upon conal layers so is there also evidence",
    "start": "4847760",
    "end": "4854480"
  },
  {
    "text": "uh that like brain is also using mechanisms of like was Shing like those",
    "start": "4854480",
    "end": "4861560"
  },
  {
    "text": "convolutional layers and if that's the case then is it also probation also",
    "start": "4861560",
    "end": "4866840"
  },
  {
    "text": "fible in for as far as I know there's no evidence",
    "start": "4866840",
    "end": "4872040"
  },
  {
    "text": "that the brain is doing any kind of direct so the convolutional Nays I mean convolutional neuron Nets have two",
    "start": "4872040",
    "end": "4878239"
  },
  {
    "text": "properties one is they're doing the weight sharing the other is theyve got these local Fields the brain is of course using the local fields and that",
    "start": "4878239",
    "end": "4883480"
  },
  {
    "text": "gives you some Advantage if if you've got specially data with local",
    "start": "4883480",
    "end": "4888719"
  },
  {
    "text": "correlations um they can do something a bit like weight sharing indirectly",
    "start": "4888719",
    "end": "4895800"
  },
  {
    "text": "because this a complicated argument you look in one place you develop lowlevel",
    "start": "4895800",
    "end": "4902199"
  },
  {
    "text": "features those give you higher level features which give you very high level features which work over a bigger region",
    "start": "4902199",
    "end": "4909040"
  },
  {
    "text": "you then look in another place and if it's not too far away",
    "start": "4909040",
    "end": "4915159"
  },
  {
    "text": "you know that the high level stuff should be the same so the high level stuff can give you top down supervision",
    "start": "4915159",
    "end": "4921080"
  },
  {
    "text": "for what the low level features should look like and you can get sort of information once",
    "start": "4921080",
    "end": "4927760"
  },
  {
    "text": "you learn good features here you get high level stuff now I can look over here and I've got both the input and the",
    "start": "4927760",
    "end": "4934320"
  },
  {
    "text": "high level stuff which is more or less correct if it's nearby so I can get I",
    "start": "4934320",
    "end": "4939400"
  },
  {
    "text": "should be able to get faster learning because I know both what the input is and what the rep representation should",
    "start": "4939400",
    "end": "4945159"
  },
  {
    "text": "be like at the higher layer and then learn is much easier so in that sense you can get information coming from",
    "start": "4945159",
    "end": "4951600"
  },
  {
    "text": "features here to transfer across the features here not by transporting weights but by the fact that these",
    "start": "4951600",
    "end": "4959480"
  },
  {
    "text": "features told you what the object was for example and knowing what the object is helps you learn these features that's",
    "start": "4959480",
    "end": "4965719"
  },
  {
    "text": "the closest I can think of that the brain could get to we",
    "start": "4965719",
    "end": "4971080"
  },
  {
    "text": "sh uh just curious can you repeat your M about why statistics can't explain how",
    "start": "4971800",
    "end": "4978159"
  },
  {
    "text": "these uh know things work so well okay my main complain just6 at least in this",
    "start": "4978159",
    "end": "4983560"
  },
  {
    "text": "talk um was they've studied models that",
    "start": "4983560",
    "end": "4988760"
  },
  {
    "text": "are in a very different regime from the brain they studied models where um the",
    "start": "4988760",
    "end": "4993840"
  },
  {
    "text": "data isn't all that high dimensional and you have not that many parameters and",
    "start": "4993840",
    "end": "4999960"
  },
  {
    "text": "not much data that's the history statistics and what they call Big Data",
    "start": "4999960",
    "end": "5005320"
  },
  {
    "text": "is if you have a billion training examples that's called big data um from the brain's point of view it's",
    "start": "5005320",
    "end": "5013000"
  },
  {
    "text": "got a billion training examples or more than a billion training examples and it's small data because it's got so many",
    "start": "5013000",
    "end": "5018400"
  },
  {
    "text": "more parameters so it's in a different regime where it can't just assume that we don't need to worry I mean at Google",
    "start": "5018400",
    "end": "5023960"
  },
  {
    "text": "they're telling me this all the time they're saying you don't need to worry about regularization we've got so much data that is not a problem well that's",
    "start": "5023960",
    "end": "5030639"
  },
  {
    "text": "just not true for some of these problems they've only got a trillion examples and if you got 10 to the 15 parameters",
    "start": "5030639",
    "end": "5035960"
  },
  {
    "text": "that's no good you better",
    "start": "5035960",
    "end": "5039280"
  },
  {
    "text": "regularize so in week sleep as well as in an algorith B back align using",
    "start": "5044280",
    "end": "5050480"
  },
  {
    "text": "temporal derivatives it seems like the earlier layers have to wait longer to get an air signal so if you have any",
    "start": "5050480",
    "end": "5056120"
  },
  {
    "text": "ideas how the brain might be able to implement these differences in the time scales of",
    "start": "5056120",
    "end": "5063080"
  },
  {
    "text": "the uh from what from what little neuros I know",
    "start": "5063120",
    "end": "5069440"
  },
  {
    "text": "I think back in the 196 of course talk by col",
    "start": "5069440",
    "end": "5075360"
  },
  {
    "text": "Glon and I think You' expect the earlier feat effectors to have a longer time",
    "start": "5075360",
    "end": "5083560"
  },
  {
    "text": "SC I think if you want learn quickly learn what's happening later adapting",
    "start": "5083560",
    "end": "5089400"
  },
  {
    "text": "the earlier slayes is a bit like changing the unit's C quite conservative about that because",
    "start": "5089400",
    "end": "5094840"
  },
  {
    "text": "else and it's that's going to SL that's all I say",
    "start": "5094840",
    "end": "5102040"
  }
]