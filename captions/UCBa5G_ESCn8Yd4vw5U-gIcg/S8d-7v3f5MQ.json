[
  {
    "start": "0",
    "end": "5270"
  },
  {
    "text": "Hi. OK, let me get\nstarted for today. I guess I'm now down\nto the more select week",
    "start": "5270",
    "end": "12440"
  },
  {
    "text": "eight audience of people\nwho actually want to learn. So my welcome and my\npleasure for the people",
    "start": "12440",
    "end": "20600"
  },
  {
    "text": "who show up today. Thank you. Thank you. OK. So what I want to do today is\nprincipally talk about a couple",
    "start": "20600",
    "end": "33380"
  },
  {
    "text": "of other neural network\ntechniques which can be used for language.",
    "start": "33380",
    "end": "40470"
  },
  {
    "text": "I mean, in some sense,\nthese two techniques are the ones that people aren't\nusing very much these days.",
    "start": "40470",
    "end": "49260"
  },
  {
    "text": "And that's partly why they get\nsort of stuck towards the end of the course, because\nwe try to teach people",
    "start": "49260",
    "end": "55520"
  },
  {
    "text": "early on in the course the\nmost essential things that you should definitely know about.",
    "start": "55520",
    "end": "61330"
  },
  {
    "text": "But the fact of the matter\nis in any scientific field,",
    "start": "61330",
    "end": "67700"
  },
  {
    "text": "there are different ideas and\ntechniques that bounce around, and it's good to know a few\nof the different ideas that",
    "start": "67700",
    "end": "74140"
  },
  {
    "text": "are out there,\nbecause often what happens is people find new\nways to reinvent things",
    "start": "74140",
    "end": "80950"
  },
  {
    "text": "and put things together and see\ndifferent insights from them. So today, I'm going to\ntell you a little bit",
    "start": "80950",
    "end": "87220"
  },
  {
    "text": "about using convolutional\nneural networks for language, and then a bit about tree\nrecursive neural networks.",
    "start": "87220",
    "end": "94659"
  },
  {
    "text": "But before that, just\ncourse organization. This is a bit after it\nhappened, but I guess",
    "start": "94660",
    "end": "101440"
  },
  {
    "text": "I've never been back to say it. So thanks to everyone who filled\nin the mid-quarter surveys.",
    "start": "101440",
    "end": "106930"
  },
  {
    "text": "Some people said\nvery nice things about the lecture, fantastic\nlectures, and really interesting",
    "start": "106930",
    "end": "112030"
  },
  {
    "text": "content. Some people wished that\nwe were teaching more about state space models.",
    "start": "112030",
    "end": "118340"
  },
  {
    "text": "I guess we haven't added\nthose that lecture in yet. ",
    "start": "118340",
    "end": "123502"
  },
  {
    "text": "A couple of people\nthought it would be good to have an\nexam in this class. Clearly, they weren't people\nwho have friends in CS231N",
    "start": "123502",
    "end": "132400"
  },
  {
    "text": "from what I've heard. But yeah. And then in general,\npeople are pretty happy how",
    "start": "132400",
    "end": "139870"
  },
  {
    "text": "it has been going, a\nbit less happy on how office hours have been going.",
    "start": "139870",
    "end": "145430"
  },
  {
    "text": "I mean, honestly,\nit's a hard problem I fail to do office hours.",
    "start": "145430",
    "end": "151069"
  },
  {
    "text": "Some people are saying, oh, you\nshould just use queue status. I sort of remember\nbadly to a year where",
    "start": "151070",
    "end": "156940"
  },
  {
    "text": "we did everything\nbut queue status and near the\nassignments' due date. The queue had stretched\nsix hours long,",
    "start": "156940",
    "end": "163540"
  },
  {
    "text": "and that didn't seem such\na good solution either, but we'll work along with it.",
    "start": "163540",
    "end": "169270"
  },
  {
    "text": "Finally, on cloud\ncompute, I know this is something that people\nvariously do have issues with.",
    "start": "169270",
    "end": "177040"
  },
  {
    "text": "So there are quite a few\npeople that are still trying to do things with\nGoogle Colab, which I realize",
    "start": "177040",
    "end": "184620"
  },
  {
    "text": "is a sort of a very\nconvenient, nice interface, but you do suffer from access\nto GPUs on Google Colab.",
    "start": "184620",
    "end": "192970"
  },
  {
    "text": "The best way to get\nbetter access to GPUs is to pay $10 for a month\nof Colab Pro, which perhaps",
    "start": "192970",
    "end": "202200"
  },
  {
    "text": "means that you end up paying for\ntwo months if it's May and June. We can't reimburse you for that.",
    "start": "202200",
    "end": "209280"
  },
  {
    "text": "But it's not so many\ncoffees worth of money, and it does just give you\nbetter access to GPUs.",
    "start": "209280",
    "end": "216520"
  },
  {
    "text": "I encourage you to\nuse the GCP credits, and together API access\nthat we've given to you.",
    "start": "216520",
    "end": "224430"
  },
  {
    "text": "You're also welcome\nto try other things. Kaggle Notebooks can\nactually give you better GPU access, but not all\nthe nice features of Colabs.",
    "start": "224430",
    "end": "235210"
  },
  {
    "text": "And some groups\nhave started using Modal, which can also be a\ngood way to get GPU access.",
    "start": "235210",
    "end": "243030"
  },
  {
    "text": "OK, that was the intro to that. And so now I wanted to talk\nabout convolutional neural",
    "start": "243030",
    "end": "249690"
  },
  {
    "text": "networks for language. I mean, these slides are\nsort of positioned a bit",
    "start": "249690",
    "end": "257160"
  },
  {
    "text": "as convolutional neural networks\nversus RNNs as opposed to versus",
    "start": "257160",
    "end": "262590"
  },
  {
    "text": "transformers. I mean, that's\npartly, you could say, because I haven't\nupdated my slides enough.",
    "start": "262590",
    "end": "269470"
  },
  {
    "text": "But in another sense,\nthat's partly because that's how the ideas of\nconvolutional neural networks",
    "start": "269470",
    "end": "276750"
  },
  {
    "text": "really were explored. It was in the days\nwhen most people were using recursive\nneural networks for NLP.",
    "start": "276750",
    "end": "283080"
  },
  {
    "text": "A few people said\nabout saying, hey, maybe we should use\nconvolutional neural networks for language as well.",
    "start": "283080",
    "end": "289080"
  },
  {
    "text": "Whereas in truth, in the last\nfive years when transformers have dominated,\nthere hasn't been",
    "start": "289080",
    "end": "294710"
  },
  {
    "text": "much use of convolutional\nneural networks for NLP. So if we think back to our\nrecurrent neural networks,",
    "start": "294710",
    "end": "302240"
  },
  {
    "text": "if you remember those,\nthey kind of gave a way of giving a\nrepresentation for a sentence",
    "start": "302240",
    "end": "309920"
  },
  {
    "text": "or part of a sentence, but\nthey sort of computed forward through the string.",
    "start": "309920",
    "end": "315890"
  },
  {
    "text": "And so you kind of had to\nget a representation that included in everything that\ncame before you and then you.",
    "start": "315890",
    "end": "324900"
  },
  {
    "text": "So you didn't really have a\nrepresentation of the ceremony. You had a representation of\nMonet walked into the ceremony",
    "start": "324900",
    "end": "331910"
  },
  {
    "text": "that you could use. So in contrast to that,\nconvolutional neural networks",
    "start": "331910",
    "end": "339530"
  },
  {
    "text": "basically say, well, kind of\nlike an n-gram model that we should be able to take\nn-grams of words like 2-grams",
    "start": "339530",
    "end": "347840"
  },
  {
    "text": "or 3-grams. So for example, tentative deal\nreached to keep the government open that we can take each\n3-gram tentative deal reached,",
    "start": "347840",
    "end": "357380"
  },
  {
    "text": "deal reached to, reached to\nkeep, to keep government. And we can make some\nneural representation",
    "start": "357380",
    "end": "364210"
  },
  {
    "text": "for each of those. So notice just being done for\nevery n-gram for a certain n.",
    "start": "364210",
    "end": "370400"
  },
  {
    "text": "So there's nothing\nlinguistically or cognitively especially plausible\nhere, but we're just",
    "start": "370400",
    "end": "375970"
  },
  {
    "text": "going to form representations\nof multi-word units, which will then group in some\nform further away later on.",
    "start": "375970",
    "end": "384230"
  },
  {
    "text": "And the standard\nway of doing that is with convolutional\nneural networks.",
    "start": "384230",
    "end": "390099"
  },
  {
    "text": "So the classic case of\nconvolutional neural networks is in vision. That convolutional\nneural networks",
    "start": "390100",
    "end": "396699"
  },
  {
    "text": "were invented for vision, where\nthey gave you a translation invariant model\nso that you could",
    "start": "396700",
    "end": "403990"
  },
  {
    "text": "recognize your kangaroo no\nmatter where in the frame it was. And so this little picture here,\nwhich I'll just do the lower",
    "start": "403990",
    "end": "413500"
  },
  {
    "text": "half of the slide is what a\nconvolutional neural network is doing in 2D vision.",
    "start": "413500",
    "end": "420170"
  },
  {
    "text": "So the convolution\nis like a mask that you're sliding\nover the image.",
    "start": "420170",
    "end": "426080"
  },
  {
    "text": "And the mask is defined\nby weights, which are the little things shown in red.",
    "start": "426080",
    "end": "431270"
  },
  {
    "text": "And so for each place\nyou slide your masks to, you're then calculating a score\nby taking what's effectively",
    "start": "431270",
    "end": "441310"
  },
  {
    "text": "a dot product of the mask terms\nby the elements in that patch. And that's then filling in\nthe matrix on the right that's",
    "start": "441310",
    "end": "450100"
  },
  {
    "text": "shown in pink. And so that's then calculating\nour convolved feature",
    "start": "450100",
    "end": "456220"
  },
  {
    "text": "from the image. That makes sense? Yeah. So well, what happens if we then\nwant to do that for language?",
    "start": "456220",
    "end": "466430"
  },
  {
    "text": "Well, for language, we\ndon't have a 2D picture. We've got a 1D picture.",
    "start": "466430",
    "end": "471970"
  },
  {
    "text": "We've got a sequence of words. So we can have tentative deal\nreached to keep government open.",
    "start": "471970",
    "end": "477550"
  },
  {
    "text": "So each of our words\nwill have a word vector. I'm using four\ndimensional in my examples",
    "start": "477550",
    "end": "483180"
  },
  {
    "text": "to keep it compact on my slide. And so then we\ncan apply a filter",
    "start": "483180",
    "end": "489990"
  },
  {
    "text": "that applies to an n-gram. So this is going to be\na filter for a trigram.",
    "start": "489990",
    "end": "496840"
  },
  {
    "text": "And so then we're going to\nslide that downwards in exactly the same way as for\nthe vision case,",
    "start": "496840",
    "end": "503879"
  },
  {
    "text": "apart from we just sliding\nit in one dimension. So I calculate the dot product\nof the filter and this 3-gram.",
    "start": "503880",
    "end": "513250"
  },
  {
    "text": "And that gives me a value minus\n1, if I did my arithmetic right. Then I slide it down to the\nnext position and work it out.",
    "start": "513250",
    "end": "521500"
  },
  {
    "text": "And I get minus 0.5. Slide it down and\nget the other values.",
    "start": "521500",
    "end": "526810"
  },
  {
    "text": "And then typically, I\ncan add on a bias term. So my bias is plus\n1 in this example.",
    "start": "526810",
    "end": "533500"
  },
  {
    "text": "And then I'll stick it through\na non-linearity like a sigmoid or something like that. And so I'll be calculating\na vector for a term for each",
    "start": "533500",
    "end": "543420"
  },
  {
    "text": "of these 3 grams. And so that is a convolution\nfor a single filter.",
    "start": "543420",
    "end": "549720"
  },
  {
    "text": "And then commonly,\nwhat I'm doing after that is deciding\nthat I'm going to have more than one filter.",
    "start": "549720",
    "end": "556300"
  },
  {
    "text": "And I'll show you\nthat in a minute. In this example and in my\nvision example earlier,",
    "start": "556300",
    "end": "564190"
  },
  {
    "text": "we sort of had shrinkage because\nwe started off with seven words. But because we sort of\nslid these trigram over it,",
    "start": "564190",
    "end": "575230"
  },
  {
    "text": "we only sort of had\nspace for 5 trigrams. And so we ended up with\nsomething smaller than our input",
    "start": "575230",
    "end": "582570"
  },
  {
    "text": "sentence. Often people want to\nkeep it the same size, and the way you can keep it the\nsame size is by having padding.",
    "start": "582570",
    "end": "589860"
  },
  {
    "text": "So if I put a 0 padding\nat each end, well, now I'm going to get 7 trigrams\ncoming out as corresponding",
    "start": "589860",
    "end": "598400"
  },
  {
    "text": "to my original 7 words. And normally, I'll just pad\nit with zeros like that.",
    "start": "598400",
    "end": "605090"
  },
  {
    "text": "You can actually increase\nthe size of things, because if you add\npadding of 2 at each end,",
    "start": "605090",
    "end": "612420"
  },
  {
    "text": "you can then have\na wide convolution. And so 7 will then go\nto 9 different things.",
    "start": "612420",
    "end": "618650"
  },
  {
    "text": "So if we only had one filter,\nthings are pretty limiting.",
    "start": "618650",
    "end": "624330"
  },
  {
    "text": "And so commonly, as\nin the vision case, what we're going to do is\ndefine multiple filters.",
    "start": "624330",
    "end": "631826"
  },
  {
    "text": "And then we're going\nto be calculating a value for each of\nthese filters over each of these trigrams.",
    "start": "631827",
    "end": "638190"
  },
  {
    "text": "And so then we're getting out a\nnew representation as a vector. And depending on\nhow many filters",
    "start": "638190",
    "end": "645470"
  },
  {
    "text": "we have relative to what\nthe word dimensionality is, we might end up with\nsomething that's",
    "start": "645470",
    "end": "650570"
  },
  {
    "text": "shorter, as in this example,\nthe same length or actually longer than what our input\nwas in terms of word vectors.",
    "start": "650570",
    "end": "657723"
  },
  {
    "text": " But commonly when we do\nthat, we then, in some way,",
    "start": "657723",
    "end": "667250"
  },
  {
    "text": "want to summarize\nall of these filters. And the most common way of\ndoing that is to do something",
    "start": "667250",
    "end": "676040"
  },
  {
    "text": "that's called max pooling. And max pooling is something\nyou see quite a bit in neural networks in general.",
    "start": "676040",
    "end": "683390"
  },
  {
    "text": "And the way to\nthink of max pooling that I think makes\nsense, is that you can think of max\npooling as doing",
    "start": "683390",
    "end": "691850"
  },
  {
    "text": "what you want if you\nwant to run something that's like a feature detector.",
    "start": "691850",
    "end": "697410"
  },
  {
    "text": "So if you imagine that you\nlearn these functions that",
    "start": "697410",
    "end": "704500"
  },
  {
    "text": "will look at word vectors and\nthat they will look for evidence of something particular.",
    "start": "704500",
    "end": "710360"
  },
  {
    "text": "So maybe this filter looks\nfor the person is using I language that matches\nthe words I, my, we, our,",
    "start": "710360",
    "end": "720760"
  },
  {
    "text": "something like\nthis, and maybe this is a filter that matches\nspeech verbs or thinking",
    "start": "720760",
    "end": "728530"
  },
  {
    "text": "verbs like think, say, said,\ntold, et cetera, like that.",
    "start": "728530",
    "end": "733870"
  },
  {
    "text": "And so each of these is some\nkind of feature of the text that you might want to detect.",
    "start": "733870",
    "end": "740450"
  },
  {
    "text": "Well, if that's\nyour model of it, when you slide your feature\ndetector down the piece of text,",
    "start": "740450",
    "end": "746990"
  },
  {
    "text": "you want to know, does\nthis match anywhere in this piece of text is\nsomewhere it using an I word,",
    "start": "746990",
    "end": "754868"
  },
  {
    "text": "regardless of whether it's\nin the first, second, third, or fourth position. And so that's effectively what\nyou're getting out with max",
    "start": "754868",
    "end": "761550"
  },
  {
    "text": "x that a feature is counting\nas firing to the extent that it fires strongly in\nany position in the text",
    "start": "761550",
    "end": "769350"
  },
  {
    "text": "that you could match. That's not the only way\nyou can think of doing it.",
    "start": "769350",
    "end": "774850"
  },
  {
    "text": "An alternative way\nyou could do it is you could think of your\nfeature detector as sort of measuring\nsome quality",
    "start": "774850",
    "end": "781980"
  },
  {
    "text": "of the text, like\ncasualness, or learnedness, or something like that.",
    "start": "781980",
    "end": "787810"
  },
  {
    "text": "And then you might\nthink, oh, well, for overall wanting to know\nhow casual the text is, maybe",
    "start": "787810",
    "end": "794010"
  },
  {
    "text": "I want to know the average\nof how casual it is in different parts of the text. And so then you can do the\nalternative of average pooling.",
    "start": "794010",
    "end": "802840"
  },
  {
    "text": "And sometimes people\ndo that as well. You can do both. You can both work out an\naverage pool and a max pool",
    "start": "802840",
    "end": "809190"
  },
  {
    "text": "and put both of them into\nthe feature detector. In general, for the\nkind of features people",
    "start": "809190",
    "end": "814890"
  },
  {
    "text": "learn in neural\nnetworks, if you're just doing one or the\nother, the result does seem to be that max\npooling is the most effective,",
    "start": "814890",
    "end": "822300"
  },
  {
    "text": "that kind of does the\nfeature fire metaphor tends in general to be the best\nway of thinking about things.",
    "start": "822300",
    "end": "832399"
  },
  {
    "text": "OK, so if you want to do all\nof this in PyTorch conv1d,",
    "start": "832400",
    "end": "837480"
  },
  {
    "text": "right that I guess the\none-dimensional convolutions aren't the most common case.",
    "start": "837480",
    "end": "843270"
  },
  {
    "text": "And so you're using conv1d\nand all these kind of things that you can then be specifying.",
    "start": "843270",
    "end": "850230"
  },
  {
    "text": "So the output channels is the\nnumber of filters you have. The kernel size is saying\nthe size is how big it is,",
    "start": "850230",
    "end": "857070"
  },
  {
    "text": "which, from my example, is 3. And then you can just collapse\nthings with the max pooling.",
    "start": "857070",
    "end": "864649"
  },
  {
    "text": "There's a space of other\nthings that you can also do with convolutional\nneural networks, which",
    "start": "864650",
    "end": "872270"
  },
  {
    "text": "I think are less useful and\nless used in language cases,",
    "start": "872270",
    "end": "878430"
  },
  {
    "text": "but I can say them quickly. So one thing you can\ndo is have a stride,",
    "start": "878430",
    "end": "886830"
  },
  {
    "text": "because when we did every\ntrigram of 0 tentative deal,",
    "start": "886830",
    "end": "893130"
  },
  {
    "text": "then tentative deal reached,\nthen deal reached to, you could feel like,\nwell, they're overlapping",
    "start": "893130",
    "end": "898370"
  },
  {
    "text": "each other a lot,\nso they've actually got very similar stuff in them. And that would be even more so\nif we weren't using 3-grams.",
    "start": "898370",
    "end": "905759"
  },
  {
    "text": "We were using\nsomething like 5-grams. So something that you can\ndo as the stride of sort",
    "start": "905760",
    "end": "911075"
  },
  {
    "text": "of how much you move along. So if you move along\n2, you'd have 1 trigram that's padding tentative deal.",
    "start": "911075",
    "end": "918840"
  },
  {
    "text": "And then the next one would then\nbe \"deal reach to,\" and then the next one would be\n\"to keep government\"",
    "start": "918840",
    "end": "924649"
  },
  {
    "text": "so that they're overlapping\nby less as you go through it.",
    "start": "924650",
    "end": "930230"
  },
  {
    "text": "Another thing that you can\nthen do that sort of stride",
    "start": "930230",
    "end": "936970"
  },
  {
    "text": "like it's rather than doing max\npooling over the entire thing,",
    "start": "936970",
    "end": "944839"
  },
  {
    "text": "you could do more\nof a local max pool. So you could think that well,\nI want to have this feature",
    "start": "944840",
    "end": "951580"
  },
  {
    "text": "detector for something\nuse of \"I\" language. But if it's a big long\nsentence and there's \"I\"",
    "start": "951580",
    "end": "957610"
  },
  {
    "text": "language at four\ndifferent points, maybe you should get 4 points\nfor that rather than just the 1",
    "start": "957610",
    "end": "962920"
  },
  {
    "text": "point that you're going\nto get from max pooling. So you could do local max\npooling sensitive to the stride.",
    "start": "962920",
    "end": "970520"
  },
  {
    "text": "So here I could look at\nthe first two of these and max pool those two. Then the next two\nand max pool those.",
    "start": "970520",
    "end": "977380"
  },
  {
    "text": "The next 2 and max pool those. And the next pool 2\nand max pool those. And you could sort of then end\nup with this sort of local max",
    "start": "977380",
    "end": "985780"
  },
  {
    "text": "pooling as you go along.  OK.",
    "start": "985780",
    "end": "992150"
  },
  {
    "text": "And then one other idea that's\nsort of related that you can do",
    "start": "992150",
    "end": "998950"
  },
  {
    "text": "is well, another\nway of capturing does something match\nin multiple places",
    "start": "998950",
    "end": "1005220"
  },
  {
    "text": "is rather than only keeping\nthe one max in each column, maybe you could just do a k\nmax so you could keep the two",
    "start": "1005220",
    "end": "1013590"
  },
  {
    "text": "maximum things in a column. And that might be also be a way\nof seeing whether something is",
    "start": "1013590",
    "end": "1019620"
  },
  {
    "text": "detected in two places or not. OK.",
    "start": "1019620",
    "end": "1026099"
  },
  {
    "text": "Then I've got lots\nof notions here. So dilation is then the notion\nthat what we'd like to do",
    "start": "1026099",
    "end": "1038670"
  },
  {
    "text": "is form our trigrams not\nonly as adjacent things,",
    "start": "1038670",
    "end": "1044709"
  },
  {
    "text": "but things that are spaced out. So after having done our first\nlayer of convolutional filters",
    "start": "1044710",
    "end": "1052190"
  },
  {
    "text": "that took trigrams that got\nus to top right part here, we could then do a dilated\ntrigram convolution,",
    "start": "1052190",
    "end": "1061440"
  },
  {
    "text": "which means that we're going\nto take the first, third, and fifth things and combine\nthem in a convolutional filter.",
    "start": "1061440",
    "end": "1072000"
  },
  {
    "text": "And then we'll take the second,\nfourth, and sixth things and combine them in a\nconvolutional filter.",
    "start": "1072000",
    "end": "1079020"
  },
  {
    "text": "And so we've then\ngot a trigram filter, but it sort of has a bigger\nrange of size that it can see.",
    "start": "1079020",
    "end": "1087050"
  },
  {
    "text": "And that's sometimes\nused more commonly used in places like speech\nthan a natural language.",
    "start": "1087050",
    "end": "1093920"
  },
  {
    "text": "So those are the\nkind of tools we have for calculating things with\nthese convolutions over text.",
    "start": "1093920",
    "end": "1101370"
  },
  {
    "text": "And so next what I\nwant to do is tell you about a couple of\npieces of work that",
    "start": "1101370",
    "end": "1107150"
  },
  {
    "text": "made use of convolutions for\nnatural language processing. I guess this is a decade old\nnow because this is from 2014.",
    "start": "1107150",
    "end": "1116559"
  },
  {
    "text": "This is the single most\nfamous piece of work that made use of\nconvolutional neural networks",
    "start": "1116560",
    "end": "1123430"
  },
  {
    "text": "for natural language processing. And Yoon Kim is now an\nassistant professor at MIT.",
    "start": "1123430",
    "end": "1130030"
  },
  {
    "text": "I mean, in retrospect, it's\nsort of actually pretty simple.",
    "start": "1130030",
    "end": "1135100"
  },
  {
    "text": "But I guess, he got in\nearly with the idea of, hey, maybe we could use\nconvolutions for NLP",
    "start": "1135100",
    "end": "1143350"
  },
  {
    "text": "and did a kind of a\nclear example of that that worked pretty well.",
    "start": "1143350",
    "end": "1148509"
  },
  {
    "text": "And so this piece of\nwork is very well known.",
    "start": "1148510",
    "end": "1153580"
  },
  {
    "text": "So this was writing a\nsentiment classifier. So looking at a sentence\nand deciding whether it's",
    "start": "1153580",
    "end": "1162640"
  },
  {
    "text": "positive or negative. And actually, for both\nthe kind of models that I'm going to\ntalk about today,",
    "start": "1162640",
    "end": "1167750"
  },
  {
    "text": "we're going to use\nexamples that are doing sentiment classification.",
    "start": "1167750",
    "end": "1173390"
  },
  {
    "text": "He also considered other\ntasks subjective or objective language, question\nclassification",
    "start": "1173390",
    "end": "1179020"
  },
  {
    "text": "as to what they were about. But the main application was\ndoing sentiment analysis. So what you're\ngoing to be doing--",
    "start": "1179020",
    "end": "1186700"
  },
  {
    "text": "and this paper shows things\nmore in his notation,",
    "start": "1186700",
    "end": "1192080"
  },
  {
    "text": "but it's exactly the same as\nwe've just been talking about, that you're taking\nn-grams of word vectors.",
    "start": "1192080",
    "end": "1198890"
  },
  {
    "text": "You're going to be multiplying\nthem by a convolution and calculating new vectors.",
    "start": "1198890",
    "end": "1206450"
  },
  {
    "text": "And it's going to\nbe done in his model for different sizes of n-grams.",
    "start": "1206450",
    "end": "1211970"
  },
  {
    "text": "So he's going to have some\nconvolutional filters that look at bigrams, some of it trigrams,\nand some of them that look",
    "start": "1211970",
    "end": "1219880"
  },
  {
    "text": "at 4-grams.  And then there's the slid across\nthe positions in the sentence.",
    "start": "1219880",
    "end": "1229050"
  },
  {
    "text": "Then having done\nthat, it then does maxpooling, as we've\nbeen talking about,",
    "start": "1229050",
    "end": "1235919"
  },
  {
    "text": "which gives a single number\ncoming out of each filter.",
    "start": "1235920",
    "end": "1241510"
  },
  {
    "text": "And those max pooled\nnumbers from each filter are then going to be\nused as a classifier",
    "start": "1241510",
    "end": "1247050"
  },
  {
    "text": "in a final, simple softmax\nlayer to give the full answers.",
    "start": "1247050",
    "end": "1253620"
  },
  {
    "text": "There's one other\nthing that came up in this paper, which is just\nan interesting general idea",
    "start": "1253620",
    "end": "1260790"
  },
  {
    "text": "to be aware of. And there was something\nhe sort of pioneered, which is the following, that\nit's a very common case that",
    "start": "1260790",
    "end": "1273540"
  },
  {
    "text": "for when you're-- I guess this, again,\noccurs less with huge pre-trained transformers.",
    "start": "1273540",
    "end": "1280210"
  },
  {
    "text": "But for the classic\ncase of models where you had word\nvectors, and then you",
    "start": "1280210",
    "end": "1285990"
  },
  {
    "text": "were training some\nneural network model on some\nsupervised data, there",
    "start": "1285990",
    "end": "1292710"
  },
  {
    "text": "was this following pitfall\nof what happened when you fine tuned word vectors.",
    "start": "1292710",
    "end": "1298990"
  },
  {
    "text": "And so the setting\nis we've started off with our pre-trained word\nvectors from GloVe, or word2vec,",
    "start": "1298990",
    "end": "1307680"
  },
  {
    "text": "or whatever it is. And then we've got a smaller\nsentiment analysis data set",
    "start": "1307680",
    "end": "1314010"
  },
  {
    "text": "and that we're going to\ntrain a sentiment classifier.",
    "start": "1314010",
    "end": "1319750"
  },
  {
    "text": "And that will involve not\nonly learning the parameters of our sentiment\nclassifier, but also, we",
    "start": "1319750",
    "end": "1326610"
  },
  {
    "text": "can back prop into the word\nvector representations. And if you do that,\nI mean, it seems",
    "start": "1326610",
    "end": "1334679"
  },
  {
    "text": "like that should be a good\nidea because normal word",
    "start": "1334680",
    "end": "1339960"
  },
  {
    "text": "vectors aren't specially\nattuned to predicting sentiment correctly.",
    "start": "1339960",
    "end": "1345770"
  },
  {
    "text": "They're more tuned\nto meaning of words as to just what words are about.",
    "start": "1345770",
    "end": "1353370"
  },
  {
    "text": "And so it seems like\nit should help you if you could back prop\ninto the word vectors",
    "start": "1353370",
    "end": "1360620"
  },
  {
    "text": "and change them as you go along. But if you do that, there\ntends to be a problem.",
    "start": "1360620",
    "end": "1366779"
  },
  {
    "text": "And the problem is\nwhat you'll find is that some words will be\nin your sentiment training",
    "start": "1366780",
    "end": "1374480"
  },
  {
    "text": "data set. And when you learn\nwith back prop,",
    "start": "1374480",
    "end": "1380100"
  },
  {
    "text": "these word vectors will move. But some words just won't\nbe in your training data,",
    "start": "1380100",
    "end": "1386190"
  },
  {
    "text": "and they're going to\nstay exactly where they were in the word2vec\nvectors, because there's",
    "start": "1386190",
    "end": "1391730"
  },
  {
    "text": "nothing to move them around. So what tends to happen is\nyou start it off like this,",
    "start": "1391730",
    "end": "1398280"
  },
  {
    "text": "where all of \"tedious,\"\n\"dull,\" and \"plodding\" were close by each other as\nhaving similar meanings and are",
    "start": "1398280",
    "end": "1404830"
  },
  {
    "text": "indicators of\nsomething negative. But after you've done your\ntraining, \"tedious\" and \"dull\"",
    "start": "1404830",
    "end": "1412330"
  },
  {
    "text": "as part of backprop\nwe've moved over here, where they're part\nof the negative land,",
    "start": "1412330",
    "end": "1417790"
  },
  {
    "text": "and the classification\nboundary has moved over here. But \"plodding\" wasn't\nin the training set,",
    "start": "1417790",
    "end": "1423140"
  },
  {
    "text": "so it's just sitting exactly\nwhere it was at the start of the process. And now it's being treated\nas a positive word, which",
    "start": "1423140",
    "end": "1431020"
  },
  {
    "text": "is completely wrong. And so that's tended\nto have the result",
    "start": "1431020",
    "end": "1436390"
  },
  {
    "text": "that when people sort\nof train language neural",
    "start": "1436390",
    "end": "1441400"
  },
  {
    "text": "network on a small\nsupervised data set, you've got ambivalent results\nthat sometimes doing backprop",
    "start": "1441400",
    "end": "1451180"
  },
  {
    "text": "into the word vectors would help\nbecause you could specialize your word vectors to your task.",
    "start": "1451180",
    "end": "1456260"
  },
  {
    "text": "But sometimes it would hurt you\nbecause of this kind of effect, that you're sort of messed up\nthe semantic relations that",
    "start": "1456260",
    "end": "1463090"
  },
  {
    "text": "applied over that were\ncaptured reasonably well in the initial word vectors.",
    "start": "1463090",
    "end": "1469990"
  },
  {
    "text": "So the way that Yoon\nKim dealt with that was a fairly simple way.",
    "start": "1469990",
    "end": "1476260"
  },
  {
    "text": "He just doubled his\nnumber of channels. And so he made two copies\nof each channel, each filter",
    "start": "1476260",
    "end": "1485919"
  },
  {
    "text": "in his convolutional\nneural network. And for one of them, it used\nthe fine tuned word vectors.",
    "start": "1485920",
    "end": "1493070"
  },
  {
    "text": "And for one of them, it kept\nthe original word vectors. And then he could have\nthe best of both worlds.",
    "start": "1493070",
    "end": "1500680"
  },
  {
    "text": "OK. So this picture captures\nthe whole of his network,",
    "start": "1500680",
    "end": "1508850"
  },
  {
    "text": "and this picture actually comes\nfrom a follow-on paper which produced this nice picture.",
    "start": "1508850",
    "end": "1514670"
  },
  {
    "text": "So we start off with a sentence\n\"I like this movie very much,\" which should be\nclassified positive.",
    "start": "1514670",
    "end": "1521790"
  },
  {
    "text": "So we have words and\ntheir word vectors. And so then you're going to have\nconvolutional filters that are",
    "start": "1521790",
    "end": "1529470"
  },
  {
    "text": "both bigram filters, trigram\nfilters, and 4-gram filters.",
    "start": "1529470",
    "end": "1534940"
  },
  {
    "text": "And at each of\nthose sizes, you're going to have ones that work\non the unfine-tuned word",
    "start": "1534940",
    "end": "1540720"
  },
  {
    "text": "vectors and the\nfine-tuned word vectors. And so you're going\nto put these filters",
    "start": "1540720",
    "end": "1549030"
  },
  {
    "text": "and slide them over the text\nand get representations.",
    "start": "1549030",
    "end": "1554050"
  },
  {
    "text": "And the way he's doing this, the\nfilters are done without padding",
    "start": "1554050",
    "end": "1559210"
  },
  {
    "text": "so that the 4-gram filters,\nyou're getting smaller vectors coming out. And the bigram filters, you've\ngot bigger vectors coming out.",
    "start": "1559210",
    "end": "1567790"
  },
  {
    "text": "And so then for each\nof these, you're then going to max pool so\nyou're just getting the highest",
    "start": "1567790",
    "end": "1574620"
  },
  {
    "text": "value from it. And then you're getting\nthe highest value from the ones with the\nfine tuning of the word",
    "start": "1574620",
    "end": "1580710"
  },
  {
    "text": "vectors and the ones not. And so you're getting one\nfeature out of each filter.",
    "start": "1580710",
    "end": "1587140"
  },
  {
    "text": "You're then concatenating all\nof those max pooled outputs",
    "start": "1587140",
    "end": "1593190"
  },
  {
    "text": "together, so you\ncan then one vector for the entire sentence, which\nis of fixed size reflecting",
    "start": "1593190",
    "end": "1599640"
  },
  {
    "text": "the number of filters. And then you're\njust sticking this as a straightforward\nlinear classifier",
    "start": "1599640",
    "end": "1606720"
  },
  {
    "text": "into a softmax that's\nthen giving you a probability of a\npositive or negative.",
    "start": "1606720",
    "end": "1612460"
  },
  {
    "text": "And that was the entire model. And the interesting thing was\nthis actually worked pretty well",
    "start": "1612460",
    "end": "1620940"
  },
  {
    "text": "for natural language\nclassification tasks. So this is a big table of\nresults from his paper.",
    "start": "1620940",
    "end": "1629470"
  },
  {
    "text": "So there are sentiment data\nsets like the Stanford Sentiment",
    "start": "1629470",
    "end": "1635240"
  },
  {
    "text": "Treebank, two versions of that. Movie reviews is another\nsentiment data set.",
    "start": "1635240",
    "end": "1640530"
  },
  {
    "text": "There's a subjectivity\nclassifier. The trec was the kind of\nquestion type classifier.",
    "start": "1640530",
    "end": "1647540"
  },
  {
    "text": "So various data sets. And various people,\nincluding us at Stanford,",
    "start": "1647540",
    "end": "1659039"
  },
  {
    "text": "I guess all of these Socher\net al. results were ones that we were doing at Stanford\nhad built lots of models",
    "start": "1659040",
    "end": "1667580"
  },
  {
    "text": "on various of these data sets. And his argument\nwas that by using",
    "start": "1667580",
    "end": "1673039"
  },
  {
    "text": "this simple convolutional\nneural network, you could do as well,\nsometimes better,",
    "start": "1673040",
    "end": "1680750"
  },
  {
    "text": "than any of these\nother models that were being considered at the\ntime for sentiment analysis.",
    "start": "1680750",
    "end": "1686100"
  },
  {
    "text": "Now, there was at least one way\nin which maybe that comparison",
    "start": "1686100",
    "end": "1693280"
  },
  {
    "text": "was too generous to the CNN. So because if you remember back\nwhen we were doing drop out",
    "start": "1693280",
    "end": "1701740"
  },
  {
    "text": "and we said, Dropout\nis such a good idea. I mean, Dropout, I\nthink, came out in 2012,",
    "start": "1701740",
    "end": "1707630"
  },
  {
    "text": "if I'm remembering correctly. So the reality is a lot of these\nother methods were being written",
    "start": "1707630",
    "end": "1713980"
  },
  {
    "text": "before Dropout\nappeared on the scene, whereas he was using Dropout,\nand that gave him an advantage",
    "start": "1713980",
    "end": "1721280"
  },
  {
    "text": "and better experimental\ntechnique might have been to compare to redo the\nother models with Dropout,",
    "start": "1721280",
    "end": "1728650"
  },
  {
    "text": "which he didn't. But nevertheless,\nit shows that you could get strong results using\nconvolutional neural networks",
    "start": "1728650",
    "end": "1737080"
  },
  {
    "text": "with just a very\nsimple architecture. Yeah.",
    "start": "1737080",
    "end": "1742090"
  },
  {
    "text": "So that's one more\nthing that you can do. And so the thing\nto think about here",
    "start": "1742090",
    "end": "1747220"
  },
  {
    "text": "is we have this sort of toolkit\nof ways that you can do things. We started off with\nword vectors and bags",
    "start": "1747220",
    "end": "1755350"
  },
  {
    "text": "of vectors, which you could\nuse for simple classification. We talked early on\nabout window models,",
    "start": "1755350",
    "end": "1762890"
  },
  {
    "text": "and your window\nmodels are sort of like what you get for\nconvolutional neural networks,",
    "start": "1762890",
    "end": "1768500"
  },
  {
    "text": "but more ad hoc. Then we have convolutional\nneural networks,",
    "start": "1768500",
    "end": "1774200"
  },
  {
    "text": "which are definitely\ngood for classification and very easy to\nparallelize, which is good.",
    "start": "1774200",
    "end": "1780740"
  },
  {
    "text": "Then we talked about\nrecurrent neural networks, which seem to be\ncognitively plausible, reading through sentences\nfrom left to right",
    "start": "1780740",
    "end": "1788290"
  },
  {
    "text": "but aren't easy to parallelize. And then we've talked\nabout transformers,",
    "start": "1788290",
    "end": "1794830"
  },
  {
    "text": "which to some extent is\nour best model for NLP and is being used everywhere.",
    "start": "1794830",
    "end": "1800780"
  },
  {
    "text": "And indeed, what's\nhappening now is that things are going in reverse\nand people are increasingly",
    "start": "1800780",
    "end": "1806169"
  },
  {
    "text": "using transformers for vision\nas well, though they still, I think, more debate in the\nvision world as between CNNs",
    "start": "1806170",
    "end": "1813930"
  },
  {
    "text": "and transformers, with some\npeople arguing that both of them have complementary advantages.",
    "start": "1813930",
    "end": "1821040"
  },
  {
    "text": "OK. A couple of other just facts\non the side, and then I'll show you one other bigger,\nfancier convolutional neural",
    "start": "1821040",
    "end": "1831720"
  },
  {
    "text": "network model for language. So we talked about\nfor transformer models",
    "start": "1831720",
    "end": "1838500"
  },
  {
    "text": "the use of layer normalization,\nwhich sort of keeps the size of the numbers\nin the middle layers",
    "start": "1838500",
    "end": "1846210"
  },
  {
    "text": "of the neural network\nabout the same by giving 0 mean\nand unit variance.",
    "start": "1846210",
    "end": "1854100"
  },
  {
    "text": "There are slightly different\nways that you can do that. For convolutional\nneural networks,",
    "start": "1854100",
    "end": "1859390"
  },
  {
    "text": "a standard thing to be using\nis batch normalization. And indeed, batch\nnormalization was the thing",
    "start": "1859390",
    "end": "1866679"
  },
  {
    "text": "that was invented first. And layer normalization\nand batch normalization",
    "start": "1866680",
    "end": "1873610"
  },
  {
    "text": "are sort of doing the same\nthing of scaling numbers to give them 0 mean\nand unit variance.",
    "start": "1873610",
    "end": "1881290"
  },
  {
    "text": "But the way that they differ is\nsort of under what dimensions they're doing\ntheir calculations.",
    "start": "1881290",
    "end": "1888310"
  },
  {
    "text": "So that layer norm is\ncalculating statistics across the feature\ndimension, whereas batch norm",
    "start": "1888310",
    "end": "1894670"
  },
  {
    "text": "is normalizing all the elements\nin the batch for each feature independently.",
    "start": "1894670",
    "end": "1902720"
  },
  {
    "text": "OK. One other little\nconcept that turns up,",
    "start": "1902720",
    "end": "1907840"
  },
  {
    "text": "which actually sort of connects\na bit to transformers as well, there's this sort of\nfunny thing that you can--",
    "start": "1907840",
    "end": "1914440"
  },
  {
    "text": "all of what I've presented so\nfar was sort of convolutions that are some n-gram,\nbigram, trigram, 4-gram.",
    "start": "1914440",
    "end": "1923900"
  },
  {
    "text": "And so there are also\nsize 1 convolutions. And at first sight, that\nseems to make no sense at all,",
    "start": "1923900",
    "end": "1932820"
  },
  {
    "text": "because what's the point of\ndoing a size 1 convolution? Because you've just\ngot one thing and it's",
    "start": "1932820",
    "end": "1938510"
  },
  {
    "text": "staying just one thing. But it actually does make\nsense because it corresponds",
    "start": "1938510",
    "end": "1945020"
  },
  {
    "text": "to having a little\nfully connected layer that's only looking at the\nrepresentation in one position.",
    "start": "1945020",
    "end": "1952110"
  },
  {
    "text": "So in a language term,\nit's taking a word vector and putting it through a\nfully connected neural network",
    "start": "1952110",
    "end": "1959090"
  },
  {
    "text": "to produce a new representation\njust of that word. And that's actually\nwhat we also have",
    "start": "1959090",
    "end": "1966140"
  },
  {
    "text": "with the fully connected layers\nand transformers, that you've got a fully connected\nlayer that's",
    "start": "1966140",
    "end": "1971510"
  },
  {
    "text": "just at 1 subword token\nposition and calculates a new representation for it.",
    "start": "1971510",
    "end": "1978500"
  },
  {
    "text": "And so that allows you to\ncreate new representations",
    "start": "1978500",
    "end": "1984850"
  },
  {
    "text": "with actually many fewer\nparameters than if you're allowing a fully connected layer\nacross the entire sentence.",
    "start": "1984850",
    "end": "1993730"
  },
  {
    "text": "And so this is then\na more recent version",
    "start": "1993730",
    "end": "1999250"
  },
  {
    "text": "of a convolutional\nneural network, still, again, used for text\nclassification, but a much more",
    "start": "1999250",
    "end": "2007049"
  },
  {
    "text": "complex one from Conneau et al. in 2017.",
    "start": "2007050",
    "end": "2013650"
  },
  {
    "text": "And again, this was still at\nthe stage in which LSTM sequence",
    "start": "2013650",
    "end": "2018990"
  },
  {
    "text": "models were dominant in NLP. I guess in 2017,\nthis is the same year",
    "start": "2018990",
    "end": "2024150"
  },
  {
    "text": "that the first transformer\npaper came out. And the motivations were sort of\ncomparing vision and language.",
    "start": "2024150",
    "end": "2036750"
  },
  {
    "text": "And so at that point in time,\nconvolutional neural network",
    "start": "2036750",
    "end": "2042210"
  },
  {
    "text": "models and vision\nwere already very deep models, so people were using\nthings like ResNet models",
    "start": "2042210",
    "end": "2048929"
  },
  {
    "text": "that had 30, 50, or\n100 layers in them. And that stood in\nstark contrast to what",
    "start": "2048929",
    "end": "2056638"
  },
  {
    "text": "was happening in the LSTM\nworld for sequence models, where commonly, people were just\nusing two layer sequence models.",
    "start": "2056639",
    "end": "2064570"
  },
  {
    "text": "And if you're wanting\nto go further, you might be using a\nthree-layer sequence model",
    "start": "2064570",
    "end": "2069989"
  },
  {
    "text": "or a four-layer sequence model. Or occasionally, if you\ngot really, really deep, people had used\neight-layer sequence models",
    "start": "2069989",
    "end": "2077280"
  },
  {
    "text": "if they had a lot of data. But essentially, it was\nalways in a single digit, the number of layers.",
    "start": "2077280",
    "end": "2084210"
  },
  {
    "text": "And then the second\nthing was, in some sense, the vision models were more raw\nsignal models because they were",
    "start": "2084210",
    "end": "2094138"
  },
  {
    "text": "operating on the\nindividual pixel level, whereas in NLP, the\nstandard was that we",
    "start": "2094139",
    "end": "2101150"
  },
  {
    "text": "were using word level models\nstill in the transformer models. So it sort of seemed like\nthings were much more",
    "start": "2101150",
    "end": "2107660"
  },
  {
    "text": "grouped before they began. And so the idea of this\npaper is, well, maybe we",
    "start": "2107660",
    "end": "2113660"
  },
  {
    "text": "could do NLP kind of\nlike it was vision. So we'll start with the raw\ncharacters as our signal.",
    "start": "2113660",
    "end": "2122490"
  },
  {
    "text": "We're going to put them into\na deeper convolutional neural network and use the same kind of\narchitecture we use for vision",
    "start": "2122490",
    "end": "2132980"
  },
  {
    "text": "and use that for language\nclassification tasks. And so that led to this\nVD-CNN architecture,",
    "start": "2132980",
    "end": "2141570"
  },
  {
    "text": "which is something that looks\nvery like a vision system in design.",
    "start": "2141570",
    "end": "2149270"
  },
  {
    "text": "And so what do we have here? So at the bottom, we have\nindividual characters.",
    "start": "2149270",
    "end": "2157760"
  },
  {
    "text": "And the individual characters\nget a 16D representation.",
    "start": "2157760",
    "end": "2163520"
  },
  {
    "text": "And then you've got some sort\nof size of piece of text that you're classifying,\nwhich for them was 1,024.",
    "start": "2163520",
    "end": "2173329"
  },
  {
    "text": "And then at each\nstage, we're then going to have\nconvolutional blocks.",
    "start": "2173330",
    "end": "2181250"
  },
  {
    "text": "And so these\nconvolutional blocks have a whole bunch of filters,\nbut they're also then going",
    "start": "2181250",
    "end": "2189260"
  },
  {
    "text": "to group stuff\ntogether so that we're starting to collapse into\nmulti-character units.",
    "start": "2189260",
    "end": "2197960"
  },
  {
    "text": "So we're starting\noff, first of all, having 64 size 3\nconvolutional filters.",
    "start": "2197960",
    "end": "2209630"
  },
  {
    "text": "And so that gives us a\nrepresentation of 64 times the window size.",
    "start": "2209630",
    "end": "2216760"
  },
  {
    "text": "And then we're going to\ndo that again and put it",
    "start": "2216760",
    "end": "2223060"
  },
  {
    "text": "through another set of\nconvolutional filters of size 3 and 64 of them, which gets\nus sort of up to here.",
    "start": "2223060",
    "end": "2232510"
  },
  {
    "text": "And then at each point, we\nalso have residual connections, which we also saw\nin transformers",
    "start": "2232510",
    "end": "2238420"
  },
  {
    "text": "that were pioneered\nin the vision space, so that we have a path\nthat things can just go straight through.",
    "start": "2238420",
    "end": "2244339"
  },
  {
    "text": "But then when we\nget to here, we're then going to do local pooling.",
    "start": "2244340",
    "end": "2249650"
  },
  {
    "text": "So each pair of representations\nhere will be pooled together.",
    "start": "2249650",
    "end": "2256250"
  },
  {
    "text": "And so at that point, we've\nno longer got a length of the initial length of 1,024.",
    "start": "2256250",
    "end": "2264020"
  },
  {
    "text": "We've now got a length of 512.",
    "start": "2264020",
    "end": "2269920"
  },
  {
    "text": "So now we're going to be\nputting it through, again, sort of trigram convolutions.",
    "start": "2269920",
    "end": "2276700"
  },
  {
    "text": "But now we're going to\nhave 128 of those channels.",
    "start": "2276700",
    "end": "2282540"
  },
  {
    "text": "We're going to\nrepeat that again. And then we're going to\nagain group with pooling.",
    "start": "2282540",
    "end": "2287740"
  },
  {
    "text": "So now we're going to have 256\nlong sequence because we've done",
    "start": "2287740",
    "end": "2296280"
  },
  {
    "text": "local pooling of each pair. And we're going to then have\n256 filters at each stage.",
    "start": "2296280",
    "end": "2303670"
  },
  {
    "text": "And we go up. And then we do local pooling\nagain so each of them is now representing an\neight gram of characters.",
    "start": "2303670",
    "end": "2312690"
  },
  {
    "text": "And we're putting trigram\nfilters over those 8 grams. So really, the\namount of a sentence",
    "start": "2312690",
    "end": "2321000"
  },
  {
    "text": "that the convolutional filters\nare seeing at this point is 24 characters,\nso seeing something",
    "start": "2321000",
    "end": "2328260"
  },
  {
    "text": "like six-word sequences\nor something like that. More convolutional blocks there.",
    "start": "2328260",
    "end": "2336180"
  },
  {
    "text": "They then do this k-max pooling. So some of the ideas from\nthe beginning of the lecture do show up.",
    "start": "2336180",
    "end": "2342369"
  },
  {
    "text": "So you're then\ndoing k-max pooling and finding the eight highest\nactivations in the sequence.",
    "start": "2342370",
    "end": "2349119"
  },
  {
    "text": "And that sort of makes sense for\nsomething like a text classifier because you want to count up\nthe amount of evidence, right.",
    "start": "2349120",
    "end": "2356170"
  },
  {
    "text": "If you got some category like,\nis this about, I don't know,",
    "start": "2356170",
    "end": "2361290"
  },
  {
    "text": "copper mining, you want to\nbe seeing whether there's a bunch of places\nin the text that's",
    "start": "2361290",
    "end": "2367770"
  },
  {
    "text": "talking about copper mining. And then right up the top, they\nhave several fully connected",
    "start": "2367770",
    "end": "2373650"
  },
  {
    "text": "layers, which, again, is\nvery typical of what you're finding in vision networks\nsuch as something like VGGNet",
    "start": "2373650",
    "end": "2383670"
  },
  {
    "text": "that after you've done a whole\nbunch of convolutional layers, you just stick it through\nmultiple fully connected layers",
    "start": "2383670",
    "end": "2390920"
  },
  {
    "text": "at the top. And so that's what\nthey're doing as well. And this is your architecture\nfor doing text understanding.",
    "start": "2390920",
    "end": "2397846"
  },
  {
    "text": " OK. I think I talked through\nthat in a lot of detail.",
    "start": "2397846",
    "end": "2404450"
  },
  {
    "text": "So I'll skip this slide. Yeah. So their experiments were done\non text classification data",
    "start": "2404450",
    "end": "2412520"
  },
  {
    "text": "sets. So various news\nclassification data sets.",
    "start": "2412520",
    "end": "2418040"
  },
  {
    "text": "DBPedia ontology, then\ndoing sentiment analysis on Yelp reviews\nand Amazon reviews.",
    "start": "2418040",
    "end": "2426289"
  },
  {
    "text": "And here, our results\nfrom their one.",
    "start": "2426290",
    "end": "2432060"
  },
  {
    "text": "So they're taking\nthe previous known best published results, which\nare shown here in table four.",
    "start": "2432060",
    "end": "2440309"
  },
  {
    "text": "And then they're considering\nwhether they can do better",
    "start": "2440310",
    "end": "2445520"
  },
  {
    "text": "by using their architecture and\nthat they used architectures",
    "start": "2445520",
    "end": "2451240"
  },
  {
    "text": "of different lengths in\nterms of the number of layers of 9 layers, 17, and 29 layers.",
    "start": "2451240",
    "end": "2459020"
  },
  {
    "text": "And the result of the\npaper is, in all cases, they got the best results\nby their deepest network,",
    "start": "2459020",
    "end": "2466190"
  },
  {
    "text": "which was a 29-layer model,\nwhich is sort of then sort of similar to what people\nwere doing in vision.",
    "start": "2466190",
    "end": "2473050"
  },
  {
    "text": "And then there's some\nvariation as to which was best by using the max\npooling or the k-max pooling.",
    "start": "2473050",
    "end": "2480620"
  },
  {
    "text": "But in general, it was\nalways the deep model. And it varied a bit\naccording to the data set,",
    "start": "2480620",
    "end": "2487700"
  },
  {
    "text": "but at least\nsometimes, they were able to produce the best\nresults that were known. So I mean, I guess for\nthese text classification,",
    "start": "2487700",
    "end": "2495200"
  },
  {
    "text": "previous results were slightly\nbetter than their results. But for some of be other ones,\nlike the DBP and the Yelp,",
    "start": "2495200",
    "end": "2503980"
  },
  {
    "text": "their results are also-- well, for both the\nYelp data sets, their results were better than\nthe best known previous results.",
    "start": "2503980",
    "end": "2513190"
  },
  {
    "text": "The Amazon ones, one was\nbetter, one was worse. But to a first approximation,\nthis meant that they could",
    "start": "2513190",
    "end": "2520390"
  },
  {
    "text": "basically reach the\nstate-of-the-art of a text classification system with\nsomething that was just a deep",
    "start": "2520390",
    "end": "2528220"
  },
  {
    "text": "convolutional neural network,\nstarting from the character level, with none of those having\nlearnt word vectors in advance",
    "start": "2528220",
    "end": "2536260"
  },
  {
    "text": "or anything like that. And so that was a\npretty cool achievement, which showed that you could\ngo a fair way in doing things",
    "start": "2536260",
    "end": "2544750"
  },
  {
    "text": "with just this sort of raw\ncharacter level convolutional neural networks, more\nlike a vision system.",
    "start": "2544750",
    "end": "2550724"
  },
  {
    "text": " OK. So that's that.",
    "start": "2550725",
    "end": "2555860"
  },
  {
    "text": "And then for the final\npiece of the class, I then want to tell\nyou about something",
    "start": "2555860",
    "end": "2562150"
  },
  {
    "text": "in the other extreme, which\nis about tree recursive neural networks.",
    "start": "2562150",
    "end": "2567910"
  },
  {
    "text": "So tree recursive\nneural networks is a framework that me and\nstudents developed at Stanford.",
    "start": "2567910",
    "end": "2575559"
  },
  {
    "text": "So I mean, when I first\ngot into neural networks",
    "start": "2575560",
    "end": "2581310"
  },
  {
    "text": "starting in 2010, that's sort of\nfor about the first five years",
    "start": "2581310",
    "end": "2587460"
  },
  {
    "text": "that what me and\nstudents worked on was doing these tree\nrecursive neural networks. And so they were\nthe Stanford brand.",
    "start": "2587460",
    "end": "2596280"
  },
  {
    "text": "Ultimately, they didn't\nprove as successful",
    "start": "2596280",
    "end": "2602370"
  },
  {
    "text": "as other things that came along. But I think they're\nlinguistically interesting.",
    "start": "2602370",
    "end": "2607539"
  },
  {
    "text": "And I think there's\na clear idea here, which is still an\nidea that exists.",
    "start": "2607540",
    "end": "2612670"
  },
  {
    "text": "And I think there may\nbe still some things to do with which\nI'll come back to. But the starting\npoint is essentially",
    "start": "2612670",
    "end": "2619620"
  },
  {
    "text": "being motivated by\nstructure of human language. And so most of this slide\nis sort of filled by a paper",
    "start": "2619620",
    "end": "2628859"
  },
  {
    "text": "from Noam Chomsky and\ncolleagues sort of discussing their views of the human\nfaculty of language, what it is,",
    "start": "2628860",
    "end": "2636550"
  },
  {
    "text": "who has it, and\nhow did it evolve. And I don't want to dwell\non this in too much detail,",
    "start": "2636550",
    "end": "2644590"
  },
  {
    "text": "but essentially in\nthis paper, what they argue is that the defining\nproperty of human language",
    "start": "2644590",
    "end": "2652320"
  },
  {
    "text": "that's not observed in\nother things that humans do is that language has\nthis recursive structure,",
    "start": "2652320",
    "end": "2660990"
  },
  {
    "text": "that you have this hierarchical\nnesting where the same structure repeats inside itself.",
    "start": "2660990",
    "end": "2667330"
  },
  {
    "text": "So if you have an example,\nlike \"the person standing next to the man from the company that\npurchased the firm that's used",
    "start": "2667330",
    "end": "2674400"
  },
  {
    "text": "to work at,\" what you have is--",
    "start": "2674400",
    "end": "2680010"
  },
  {
    "text": "the whole of this is a\nnoun phrase, \"the person.\" Headed by \"the person.\"",
    "start": "2680010",
    "end": "2685470"
  },
  {
    "text": "And then it's\n\"standing next to.\" Then the first square brackets,\nhere is another noun phrase--",
    "start": "2685470",
    "end": "2691400"
  },
  {
    "text": "\"the man from.\" Then inside that\nprepositional phrase, there's another noun phrase--",
    "start": "2691400",
    "end": "2696600"
  },
  {
    "text": "\"the company that\npurchased the firm.\" And then \"the firm\" is\nanother noun phrase that has",
    "start": "2696600",
    "end": "2704270"
  },
  {
    "text": "the relative clause modifier of\n\"the firm that you used to work at.\" So we have these embedded\nlayers of noun phrases",
    "start": "2704270",
    "end": "2712160"
  },
  {
    "text": "with the same syntactic\nstructure underneath them. And so for the\nkind of formalisms",
    "start": "2712160",
    "end": "2718910"
  },
  {
    "text": "that we use in linguistics\nor context-free grammar, it permits the kind of infinite\nembedding of nesting, which",
    "start": "2718910",
    "end": "2726950"
  },
  {
    "text": "is the same kind of nesting\nthat you get in programming languages, where you\ncan use if statements",
    "start": "2726950",
    "end": "2734000"
  },
  {
    "text": "and nest them as deeply as\nyou want to because you just have this same repeating\nrecursive structure.",
    "start": "2734000",
    "end": "2740369"
  },
  {
    "text": "Now, of course, human\nbeings can't actually understand recursive\ninfinite recursion,",
    "start": "2740370",
    "end": "2746360"
  },
  {
    "text": "and people don't actually\nproduce infinite recursion that you could say and practice.",
    "start": "2746360",
    "end": "2752299"
  },
  {
    "text": "No one's going to\ngo more than eight deep when they're\nsaying a sentence. But in terms of the structure\nof what the language looks like,",
    "start": "2752300",
    "end": "2760280"
  },
  {
    "text": "it seems like you should be\nable to do it infinitely deep. And when you actually\nstart looking",
    "start": "2760280",
    "end": "2765760"
  },
  {
    "text": "at the structures of sentences,\nthey do sort of repeat over the same\nstructure quite deeply.",
    "start": "2765760",
    "end": "2772160"
  },
  {
    "text": "So this is an example\nof a Penn Treebank tree, which is the best\nknown constituency treebank.",
    "start": "2772160",
    "end": "2781540"
  },
  {
    "text": "And so here's my\nrandom sentence. \"Analysts said Mr. Stronach\nwants to resume a more",
    "start": "2781540",
    "end": "2787690"
  },
  {
    "text": "influential role in\nrunning the company.\" And, well, what\nwe end up with, we",
    "start": "2787690",
    "end": "2793930"
  },
  {
    "text": "have these nested\nthings of verb phrases. So \"running the company\"\nis a verb phrase.",
    "start": "2793930",
    "end": "2799869"
  },
  {
    "text": "\"Resume a more influential\nrole in running the company\" is a bigger verb phrase.",
    "start": "2799870",
    "end": "2806860"
  },
  {
    "text": "\"Wants to resume a bigger\nrole in running the company\" is an even bigger verb phrase.",
    "start": "2806860",
    "end": "2813830"
  },
  {
    "text": "And then \"said Mr. Stronach\nwants to resume a more",
    "start": "2813830",
    "end": "2819310"
  },
  {
    "text": "influential role in\nrunning the company\" is an even bigger verb phrase. And so we have 1, 2, 3,\n4 verb phrases all nested",
    "start": "2819310",
    "end": "2830800"
  },
  {
    "text": "inside each other. And so the idea\nwas, well, maybe we",
    "start": "2830800",
    "end": "2835900"
  },
  {
    "text": "should be thinking of sentences\nas having this kind of tree structure and computing\nrepresentations",
    "start": "2835900",
    "end": "2842770"
  },
  {
    "text": "of meanings of sentences in\nterms of this tree structure. So we have words that have\nrepresentations in a word vector",
    "start": "2842770",
    "end": "2854050"
  },
  {
    "text": "space like we saw right at\nthe beginning of the class. But then we're going\nto have a phrase like the country of my birth.",
    "start": "2854050",
    "end": "2861240"
  },
  {
    "text": "And the classic\nlinguistic answer that you find both in linguistic\nsemantics classes or philosophy",
    "start": "2861240",
    "end": "2868650"
  },
  {
    "text": "of language is that we should\nconstruct representations of phrases using the principle\nof compositionality, which",
    "start": "2868650",
    "end": "2877019"
  },
  {
    "text": "says that the meaning\nof a phrase or sentence is determined by the\nmeanings of its words,",
    "start": "2877020",
    "end": "2882760"
  },
  {
    "text": "which are our word vectors, and\nthe rules that combine them. So maybe we could take\nthe phrase structure",
    "start": "2882760",
    "end": "2889860"
  },
  {
    "text": "tree of a sentence and combine\nthe word vectors together",
    "start": "2889860",
    "end": "2895230"
  },
  {
    "text": "by some means, and then we\ncan construct a representation of the meaning of phrases in a\nmore linguistic way, giving us",
    "start": "2895230",
    "end": "2903869"
  },
  {
    "text": "a vector representation of\nthe meaning of the phrase, which we could also put\ninto our vector space.",
    "start": "2903870",
    "end": "2909820"
  },
  {
    "text": "And we'd hoped that a phrase\nlike the country of my birth would appear in the vector\nspace in a similar place",
    "start": "2909820",
    "end": "2916200"
  },
  {
    "text": "to where words representing\nlocations appeared. OK.",
    "start": "2916200",
    "end": "2921670"
  },
  {
    "text": "So what we want is to be able\nto start with word vectors and parse up a sentence.",
    "start": "2921670",
    "end": "2928500"
  },
  {
    "text": "And as we parse\nthe sentence, we're then going to be\ncomputing representations",
    "start": "2928500",
    "end": "2934710"
  },
  {
    "text": "for the different\nphrases of the sentence.  And so the difference\nhere is now--",
    "start": "2934710",
    "end": "2942750"
  },
  {
    "text": "the difference between\nrecursive and recurrent",
    "start": "2942750",
    "end": "2948960"
  },
  {
    "text": "is sort of a fake\ndifference, right? They both come from\nthe same recur word.",
    "start": "2948960",
    "end": "2954780"
  },
  {
    "text": "But rather than having the\nrecursion just happening along a sequence as in a\nrecurrent neural network,",
    "start": "2954780",
    "end": "2961779"
  },
  {
    "text": "we're going to have the\nrecursion happening up a tree structure so we're\ncomputing representations",
    "start": "2961780",
    "end": "2968310"
  },
  {
    "text": "for linguistically\nmeaningful phrases. And so what we're going\nto do with that is,",
    "start": "2968310",
    "end": "2978890"
  },
  {
    "text": "the easy cases, if we know\nthe phrase structure tree,",
    "start": "2978890",
    "end": "2984000"
  },
  {
    "text": "we can take the representations\nof the child nodes,",
    "start": "2984000",
    "end": "2989160"
  },
  {
    "text": "put them into a\nneural network, which could give us the representation\nof the parent node.",
    "start": "2989160",
    "end": "2995400"
  },
  {
    "text": "But we'd also like to\nfind the tree structure. And so a way we could do that\nis then get a second thing out",
    "start": "2995400",
    "end": "3002560"
  },
  {
    "text": "of the neural network. We could get a score for\nhow plausible something is",
    "start": "3002560",
    "end": "3008170"
  },
  {
    "text": "as a constituent. Does it make sense to combine\nthese two nodes together",
    "start": "3008170",
    "end": "3013780"
  },
  {
    "text": "to form a larger constituent? And then we can use\nthat in a parser.",
    "start": "3013780",
    "end": "3018910"
  },
  {
    "text": "So formally, the\nvery simplest kind of tree neural network and\nthe first one we explored",
    "start": "3018910",
    "end": "3026470"
  },
  {
    "text": "was when we had\ntwo child vectors. We're going to be\nrepresenting the parent vector",
    "start": "3026470",
    "end": "3034830"
  },
  {
    "text": "by concatenating\nthe two children, multiplying them by a\nmatrix, adding a bias,",
    "start": "3034830",
    "end": "3041560"
  },
  {
    "text": "putting it through a\nnon-linearity to get a parent representation p. And then we'd score whether it's\na good constituent by taking",
    "start": "3041560",
    "end": "3051180"
  },
  {
    "text": "another vector of\nlearned parameters, which",
    "start": "3051180",
    "end": "3057180"
  },
  {
    "text": "we'd do a dot product with p. And that would give us a\nscore as to whether this",
    "start": "3057180",
    "end": "3063600"
  },
  {
    "text": "was a good constituent to\ninclude in your parse tree. And the same w\nparameters were used",
    "start": "3063600",
    "end": "3070830"
  },
  {
    "text": "at all nodes of the\ntree in the same way as in a recurrent neural network\nkept using the same parameters.",
    "start": "3070830",
    "end": "3079200"
  },
  {
    "text": "OK. So if we had that, we could\nbuild a greedy parser,",
    "start": "3079200",
    "end": "3085869"
  },
  {
    "text": "because what we could do is we\ncould start with all the word vectors, and we could just\ntake every pair of words",
    "start": "3085870",
    "end": "3092970"
  },
  {
    "text": "and put it through this\nsystem and calculate what the representation of that\npair would be as a constituent,",
    "start": "3092970",
    "end": "3101440"
  },
  {
    "text": "and then get a score\nas to whether it seemed a good constituent or not. And then we could\njust greedily decide,",
    "start": "3101440",
    "end": "3109290"
  },
  {
    "text": "this is the best\nconstituent, \"the cat.\" And so if we do a greedy\nparser, we can commit to that.",
    "start": "3109290",
    "end": "3116500"
  },
  {
    "text": "And then, well, we\nalready still know the possibilities of combining\nother pairs of words.",
    "start": "3116500",
    "end": "3123309"
  },
  {
    "text": "And we could just additionally\nscore how good \"the cat\"",
    "start": "3123310",
    "end": "3129300"
  },
  {
    "text": "combined with \"sat\" is so that\nwe're producing binary parse",
    "start": "3129300",
    "end": "3134550"
  },
  {
    "text": "structures. So now the best pair to\ncombine greedily is \"the mat.\"",
    "start": "3134550",
    "end": "3139720"
  },
  {
    "text": "So we could combine them\ntogether and commit to those. We can score combining\n\"on\" with the \"mat.\"",
    "start": "3139720",
    "end": "3147010"
  },
  {
    "text": "And now that seems\nthe best thing. So we'll commit to that. And we just keep on\nup, and we produce",
    "start": "3147010",
    "end": "3153769"
  },
  {
    "text": "a binary parse of the sentence. And this gives us our\nsentence representation.",
    "start": "3153770",
    "end": "3160849"
  },
  {
    "text": "OK. Which is like that. And so that gives\nus our simple RNN.",
    "start": "3160850",
    "end": "3168260"
  },
  {
    "text": "And so back in 2011, we got\nsome pretty decent results",
    "start": "3168260",
    "end": "3173990"
  },
  {
    "text": "of showing that you could use\nthis as a sentence parser. That worked pretty well.",
    "start": "3173990",
    "end": "3179220"
  },
  {
    "text": "But beyond that,\nthe representations we calculated for\nsentences and phrases",
    "start": "3179220",
    "end": "3186980"
  },
  {
    "text": "were good enough representations\nthat you could use it for tasks like sentence\nclassification, sentiment",
    "start": "3186980",
    "end": "3194000"
  },
  {
    "text": "analysis, and it\nworks reasonably well.",
    "start": "3194000",
    "end": "3199580"
  },
  {
    "text": "It only works reasonably\nwell because if you start",
    "start": "3199580",
    "end": "3206000"
  },
  {
    "text": "thinking about it further. There sort of were\nstrong limitations",
    "start": "3206000",
    "end": "3211370"
  },
  {
    "text": "of having this single w matrix\nthat's used at all points to combine things so that if\nyou have that architecture,",
    "start": "3211370",
    "end": "3220950"
  },
  {
    "text": "you sort of can't have\ndifferent forms of interaction between the different words.",
    "start": "3220950",
    "end": "3226140"
  },
  {
    "text": "You're sort of just\nuniformly computing things. And that stands in\ndistinction to the fact",
    "start": "3226140",
    "end": "3232339"
  },
  {
    "text": "that different kinds of\nthings in natural language seem kind of different. You have different properties\nwith verbs and their objects",
    "start": "3232340",
    "end": "3240980"
  },
  {
    "text": "versus an adjective\nmodifying a noun just in terms of what the roles\nof the different words were.",
    "start": "3240980",
    "end": "3246540"
  },
  {
    "text": "So we started to see limitations\nof this architecture. And so in following years, we\nstarted exploring other ways",
    "start": "3246540",
    "end": "3255770"
  },
  {
    "text": "to build tree recursive\nneural networks, which had more\nflexibility as to how",
    "start": "3255770",
    "end": "3261380"
  },
  {
    "text": "things were combined together. And I'm not going to show you\nall the details of all of that,",
    "start": "3261380",
    "end": "3266960"
  },
  {
    "text": "but I will show\nyou one more model",
    "start": "3266960",
    "end": "3272140"
  },
  {
    "text": "that we use for building tree\nrecursive neural networks and that was used in some of our\nsentiment analysis work called",
    "start": "3272140",
    "end": "3280060"
  },
  {
    "text": "the Recursive Neural\nTensor Network. It wasn't actually the final\nnet version that we did.",
    "start": "3280060",
    "end": "3287300"
  },
  {
    "text": "After that, we sort of\nstarted taking LSTM ideas and extending those to\nthe tree structured case.",
    "start": "3287300",
    "end": "3294140"
  },
  {
    "text": "And we worked on tree LSTMs. But I'm not going to\nshow you that this year. But the idea of recursive\nneural tensor networks",
    "start": "3294140",
    "end": "3302980"
  },
  {
    "text": "is when pairs of\nwords or phrases combine together in\nlinguistic semantics terms,",
    "start": "3302980",
    "end": "3311869"
  },
  {
    "text": "depending on the pairs\nof words, they modify each other in different ways. So if you have an adjective and\nnoun like a \"red ball,\" \"red\"",
    "start": "3311870",
    "end": "3321819"
  },
  {
    "text": "is giving attributes\nof the noun. Whereas if you have something\nlike a verb and its object like",
    "start": "3321820",
    "end": "3328109"
  },
  {
    "text": "\"kick the ball,\" you've\ngot a very different role for the object as the right\nhand side versus the \"red ball.\"",
    "start": "3328110",
    "end": "3335050"
  },
  {
    "text": "It's the opposite way around, So we want to have\nmore flexibility in the way we can calculate\nmeanings of phrases",
    "start": "3335050",
    "end": "3342900"
  },
  {
    "text": "depending on what's in it. And the way we came\nup with doing that is to come up with what we\ncall this neural tensor layer.",
    "start": "3342900",
    "end": "3351700"
  },
  {
    "text": "And so the idea in the\nneural tensor layer is that we had the\nrepresentations of the child",
    "start": "3351700",
    "end": "3364230"
  },
  {
    "text": "words or phrases. And so rather than\ndirectly putting--",
    "start": "3364230",
    "end": "3369990"
  },
  {
    "text": "rather than directly\nconcatenating them and then putting it through a\nsort of a linear transformation",
    "start": "3369990",
    "end": "3378540"
  },
  {
    "text": "like a regular neural network\nlayer, instead what we could do is that we could learn\nin between matrices.",
    "start": "3378540",
    "end": "3388480"
  },
  {
    "text": "And if we put several\nof those together, we're then getting a\nthree-dimensional tensor.",
    "start": "3388480",
    "end": "3393790"
  },
  {
    "text": "And we can multiply a vector\nby a tensor times a vector.",
    "start": "3393790",
    "end": "3401470"
  },
  {
    "text": "And then we'll end up getting\nout vectors for each one.",
    "start": "3401470",
    "end": "3407369"
  },
  {
    "text": "We'll have multiple\nsuch vectors. ",
    "start": "3407370",
    "end": "3415290"
  },
  {
    "text": "And the place that\nwe applied this model is for this task of\nsentiment analysis.",
    "start": "3415290",
    "end": "3421690"
  },
  {
    "text": "So let me just tell you a little\nbit more of what we did here. And this is sort\nof in fact, going",
    "start": "3421690",
    "end": "3427560"
  },
  {
    "text": "backwards to the\nStanford Sentiment Treebank there was already\nused in the Yoon Kim work.",
    "start": "3427560",
    "end": "3433510"
  },
  {
    "text": "So the goal of\nsentiment analysis is to say whether\na piece of text is positive,\nnegative, or neutral.",
    "start": "3433510",
    "end": "3440210"
  },
  {
    "text": "So a lot of the time,\ndoing sentiment analysis is pretty easy.",
    "start": "3440210",
    "end": "3445760"
  },
  {
    "text": "In the 2010s and\nprobably even today, quite a few people's sentiment\nanalysis systems are essentially",
    "start": "3445760",
    "end": "3454940"
  },
  {
    "text": "just keyword matching. If you see great, marvelous,\nwonderful-- positive sentiment.",
    "start": "3454940",
    "end": "3462960"
  },
  {
    "text": "If you see something\nof poor, bad-- negative sentiment. And so lots of the time,\nyou can effectively",
    "start": "3462960",
    "end": "3471650"
  },
  {
    "text": "do a kind of dictionary matching\nand get pretty good sentiment, especially on longer documents.",
    "start": "3471650",
    "end": "3478170"
  },
  {
    "text": "But on the other hand,\npeople use language in lots of interesting ways,\nand it's not always that easy.",
    "start": "3478170",
    "end": "3485700"
  },
  {
    "text": "So if you look at something\nlike movie reviews, such as the snippets you\nget on Rotten Tomatoes,",
    "start": "3485700",
    "end": "3491930"
  },
  {
    "text": "you get snippets like\nthis in Rotten Tomatoes-- \"with this cast and\nthis subject matter,",
    "start": "3491930",
    "end": "3497570"
  },
  {
    "text": "the movie should have been\nfunnier and more entertaining.\" And if you just think\nof it as, OK, we're",
    "start": "3497570",
    "end": "3504079"
  },
  {
    "text": "doing dictionary\nmatching, there's the word entertaining,\nthat's definitely positive.",
    "start": "3504080",
    "end": "3510230"
  },
  {
    "text": "And funnier, that's positive. So there are two\npositive words, so this",
    "start": "3510230",
    "end": "3515509"
  },
  {
    "text": "should be a positive review. But of course, it's\nnot a positive review. This is a negative review\nbecause it's saying--",
    "start": "3515510",
    "end": "3524250"
  },
  {
    "text": "well, I'm just\nreading it out again-- \"with this cast\nand subject matter, the movie should have been\nfunnier and more entertaining.\"",
    "start": "3524250",
    "end": "3531530"
  },
  {
    "text": "So the compositional structure\nof human language goes together to mean that because it's\nburied under \"should have been,\"",
    "start": "3531530",
    "end": "3542840"
  },
  {
    "text": "the funnier and entertaining\nare actually lacking. And so it's a negative review.",
    "start": "3542840",
    "end": "3549570"
  },
  {
    "text": "And so these were\nthe kind of examples that we were interested\nin and saying, could we actually understand\nthe structure of sentences",
    "start": "3549570",
    "end": "3557680"
  },
  {
    "text": "more and do a better job\nat sentiment analysis? And so up until this\ntime, people just sort of",
    "start": "3557680",
    "end": "3566320"
  },
  {
    "text": "had pieces of text and a\nclassification judgment of positive and negative.",
    "start": "3566320",
    "end": "3572180"
  },
  {
    "text": "So we decided we were going to\ndo more than that and come up with the Stanford\nSentiment Treebank,",
    "start": "3572180",
    "end": "3577930"
  },
  {
    "text": "where what we did was parsed\nup a whole lot of sentences,",
    "start": "3577930",
    "end": "3583010"
  },
  {
    "text": "almost 12,000 of them. And then what we\nwere going to do is put sentiment judgments\non every linguistic phrase",
    "start": "3583010",
    "end": "3593770"
  },
  {
    "text": "of the sentence. So for something like this\nexample, \"with this cast\"",
    "start": "3593770",
    "end": "3598900"
  },
  {
    "text": "is a phrase, no sentiment. So they'd just be neutral. \"Entertaining\" is a\nphrase, a one-word phrase.",
    "start": "3598900",
    "end": "3607970"
  },
  {
    "text": "It's sentiment is positive.  \"Funnier and more\nentertaining,\" that's a phrase.",
    "start": "3607970",
    "end": "3617099"
  },
  {
    "text": "Very positive. But then by the time we get\nembedded under \"should have been",
    "start": "3617100",
    "end": "3623250"
  },
  {
    "text": "funnier and more entertaining,\"\nthat's a bigger phrase. It's sentiment is now negative.",
    "start": "3623250",
    "end": "3628329"
  },
  {
    "text": "And \"the movie should have been\nfunnier and more entertaining,\" that's an even bigger phrase. It's negative.",
    "start": "3628330",
    "end": "3634360"
  },
  {
    "text": "And so we were parsing\nup trees like that. And these examples\nare very small.",
    "start": "3634360",
    "end": "3641470"
  },
  {
    "text": "I'll show you a\nbig examples later, but you can just see\nthat in the trees,",
    "start": "3641470",
    "end": "3647010"
  },
  {
    "text": "there are blue nodes\nand orange nodes corresponding to positive\nand negative sentiment",
    "start": "3647010",
    "end": "3652019"
  },
  {
    "text": "reflecting units at\nthe different sizes. And so the interesting\nthing is then",
    "start": "3652020",
    "end": "3657480"
  },
  {
    "text": "this gave us a richer\nannotated data set, because it's not only\nsort of whole sentences",
    "start": "3657480",
    "end": "3664770"
  },
  {
    "text": "or whole articles that were\nannotated for sentiment. We had annotations\nfor different phrases.",
    "start": "3664770",
    "end": "3671970"
  },
  {
    "text": "And simply the fact that\nyou are annotating phrases meant that you could learn\nmore from the examples.",
    "start": "3671970",
    "end": "3679000"
  },
  {
    "text": "So that even if you were\nusing something very simple like a Naive Bayes\nclassifier, because there",
    "start": "3679000",
    "end": "3684660"
  },
  {
    "text": "are annotations on words\nand smaller phrases, you could learn a bit more\nabout which were positive",
    "start": "3684660",
    "end": "3691619"
  },
  {
    "text": "and which were negative. And so that was the first\nresult that we could--",
    "start": "3691620",
    "end": "3698849"
  },
  {
    "text": "a baseline method of bigram\nNaive Bayes classifier, which is a very common\nsentiment classifier",
    "start": "3698850",
    "end": "3705780"
  },
  {
    "text": "that if you just trained\nwith sentiment classifiers, you got 79% on this data set.",
    "start": "3705780",
    "end": "3711400"
  },
  {
    "text": " If you trained using every node\nof the Treebank, you got 83%.",
    "start": "3711400",
    "end": "3720460"
  },
  {
    "text": "So you got a 4% lift. And so that was kind of good. These other two lines show\ntwo of our early tree RNNs.",
    "start": "3720460",
    "end": "3730030"
  },
  {
    "text": "And the negative\npart of the result is they weren't really better\nthan a bigram naive Bayes",
    "start": "3730030",
    "end": "3736670"
  },
  {
    "text": "classifier. They were better than a\nunigram Naive Bayes classifier. But a lot of the\nextra information",
    "start": "3736670",
    "end": "3744170"
  },
  {
    "text": "that you want to capture\nfor sentiment analysis, you can get from bigrams,\nbecause that can already",
    "start": "3744170",
    "end": "3750950"
  },
  {
    "text": "tell you not good, somewhat\ninteresting, and things like that.",
    "start": "3750950",
    "end": "3756595"
  },
  {
    "text": " So the other hope was to\nhave a more powerful model.",
    "start": "3756595",
    "end": "3763320"
  },
  {
    "text": "And so that then led into use\nof this recursive neural tensor network, which allowed the\nmediated multiplicative",
    "start": "3763320",
    "end": "3771620"
  },
  {
    "text": "interactions between\nword or phrase vectors. ",
    "start": "3771620",
    "end": "3777890"
  },
  {
    "text": "And so we built that. And so then here are the\nresults of that model",
    "start": "3777890",
    "end": "3783349"
  },
  {
    "text": "that's shown in red. So by having our recursive\nneural tensor network,",
    "start": "3783350",
    "end": "3789460"
  },
  {
    "text": "we were able to build a somewhat\nbetter neural network that",
    "start": "3789460",
    "end": "3796180"
  },
  {
    "text": "performed at least reasonably\nbetter than a bigram Naive Bayes",
    "start": "3796180",
    "end": "3802660"
  },
  {
    "text": "model that we were getting. So about 2.5% better than\na bigram Naive Bayes model.",
    "start": "3802660",
    "end": "3809270"
  },
  {
    "text": "So that was progress. But I think perhaps the\nmore interesting thing isn't the aggregate\nresults, but the fact",
    "start": "3809270",
    "end": "3816430"
  },
  {
    "text": "that because we were\nbuilding up this model, the computed representations\nover a constituency tree,",
    "start": "3816430",
    "end": "3827059"
  },
  {
    "text": "that it actually made judgments\nof different parts of sentences and how they combined.",
    "start": "3827060",
    "end": "3833480"
  },
  {
    "text": "So here's a movie\nreview sentence. There are slow and\nrepetitive parts,",
    "start": "3833480",
    "end": "3839839"
  },
  {
    "text": "but it has just enough spice\nto keep it interesting. So I hope you'll agree\nwith the judgment",
    "start": "3839840",
    "end": "3845140"
  },
  {
    "text": "that overall, that's a positive\nstatement about the movie. And so the recursive\nneural tensor network",
    "start": "3845140",
    "end": "3853420"
  },
  {
    "text": "builds the tree structure\nof this sentence. And it says, \"slow\nand repetitive.\"",
    "start": "3853420",
    "end": "3859370"
  },
  {
    "text": "That's negative. \"There are slow and\nrepetitive parts.\" It's all negative to here.",
    "start": "3859370",
    "end": "3865220"
  },
  {
    "text": "But for the hard over to\nthe right, \"interesting.\" \"Spice,\" they're both positive.",
    "start": "3865220",
    "end": "3871300"
  },
  {
    "text": "And \"spice to keep it\ninteresting,\" that's positive. \"It has just enough spice to\nkeep it interesting,\" positive.",
    "start": "3871300",
    "end": "3878090"
  },
  {
    "text": "And it correctly\npredicts that when you put these two halves of\nthese sentences together, the overall judgment is that\nthis remains a positive review,",
    "start": "3878090",
    "end": "3888070"
  },
  {
    "text": "and it gives a positive\njudgment overall. So that was kind of cool.",
    "start": "3888070",
    "end": "3894220"
  },
  {
    "text": "And in particular, the fact that\nwe are building these phrase judgments meant that it\nseemed like we could actually",
    "start": "3894220",
    "end": "3902440"
  },
  {
    "text": "do a better job of sentence\nunderstanding in the way that any linguist doing\nlinguistic semantics",
    "start": "3902440",
    "end": "3910380"
  },
  {
    "text": "would like to see\nsentence understanding. So one of the things that\nneural networks, when",
    "start": "3910380",
    "end": "3916140"
  },
  {
    "text": "looking at language, have\noften been faulted for and are still faulted for to\nthis day using transformer",
    "start": "3916140",
    "end": "3924930"
  },
  {
    "text": "models, is you often find the\nresult that neural network",
    "start": "3924930",
    "end": "3930329"
  },
  {
    "text": "models just don't pay\nattention to negation, that you can be\nhaving some sentence,",
    "start": "3930330",
    "end": "3937049"
  },
  {
    "text": "and you can compare the\nsentence of a lot of students",
    "start": "3937050",
    "end": "3942330"
  },
  {
    "text": "are studying for their final\nexams versus a lot of students aren't studying for\ntheir final exams.",
    "start": "3942330",
    "end": "3948640"
  },
  {
    "text": "And the negation just\ngets lost, that it doesn't produce the differences\nin representation and meaning",
    "start": "3948640",
    "end": "3955470"
  },
  {
    "text": "that you'd like it to. So somewhat interestingly,\nwith this model,",
    "start": "3955470",
    "end": "3963250"
  },
  {
    "text": "it seemed like because\nwe were modeling the curse of building up\nof sentence structure,",
    "start": "3963250",
    "end": "3969880"
  },
  {
    "text": "that we actually could do\ninteresting things with modeling",
    "start": "3969880",
    "end": "3975150"
  },
  {
    "text": "negation. So in particular, the results\nthat you'd like to get is if you",
    "start": "3975150",
    "end": "3985470"
  },
  {
    "text": "have something like \"it's just\nincredibly dull,\" so \"dull\" is a very negative word.",
    "start": "3985470",
    "end": "3991470"
  },
  {
    "text": "\"Incredible\" is a\npositive word by itself. But when you're sort of\nsaying \"incredibly dull,\"",
    "start": "3991470",
    "end": "3997990"
  },
  {
    "text": "it's definitely still negative. And our recursive neural tensor\nnetwork is correctly modeling",
    "start": "3997990",
    "end": "4009470"
  },
  {
    "text": "\"it's just incredibly dull\"\nis very negative despite \"incredible\" being a\nsort of positive word.",
    "start": "4009470",
    "end": "4015740"
  },
  {
    "text": "So actually, in\nthis model, there",
    "start": "4015740",
    "end": "4021410"
  },
  {
    "text": "was five-way classification. So there was very negative,\nsomewhat negative, neutral, somewhat positive,\nvery positive.",
    "start": "4021410",
    "end": "4028990"
  },
  {
    "text": "So there's some bouncing\naround as to whether it's given classification, very\nnegative versus somewhat",
    "start": "4028990",
    "end": "4034599"
  },
  {
    "text": "negative. I can't really explain\nwhy in the middle, it goes to somewhat negative and\nthen goes back to very negative.",
    "start": "4034600",
    "end": "4041060"
  },
  {
    "text": "But that's the results that\ncame out of the network. And at any rate, it\nall stays negative.",
    "start": "4041060",
    "end": "4047150"
  },
  {
    "text": "The fact that \"incredible\" by\nitself incredibly is a positive word, it's seen in the\nmodified modification of \"dull\"",
    "start": "4047150",
    "end": "4055450"
  },
  {
    "text": "as that keeps it negative. But on the other hand, if\nyou put a negation in here,",
    "start": "4055450",
    "end": "4062150"
  },
  {
    "text": "\"it's definitely not dull,\"\nwell then, what happens? Now, interestingly,\nthe word \"not\"",
    "start": "4062150",
    "end": "4069730"
  },
  {
    "text": "by itself is a negative word\nthat if you just do the raw statistics of it, \"not\" occurs\nmuch more often in negative",
    "start": "4069730",
    "end": "4080320"
  },
  {
    "text": "sentiment sentences than it\ndoes in positive sentiments and sentences.",
    "start": "4080320",
    "end": "4085390"
  },
  {
    "text": "So if you want to be a\nmore positive person, use negation less.",
    "start": "4085390",
    "end": "4091120"
  },
  {
    "text": "\"Not\" by itself is negative, but\nif you then combine it together, \"not dull,\" or in this case,\n\"definitely not dull,\" well,",
    "start": "4091120",
    "end": "4101140"
  },
  {
    "text": "\"not dull\" is you have two\nnegations so that they cancel each other out, and you get\nsomething that's positive.",
    "start": "4101140",
    "end": "4109089"
  },
  {
    "text": "And so \"it's\ndefinitely not dull\" comes out as a\npositive sentence. And so the interesting\nresult here",
    "start": "4109090",
    "end": "4117389"
  },
  {
    "text": "is that if you compare\nwhat happens between--",
    "start": "4117390",
    "end": "4125850"
  },
  {
    "text": "if you have negated positive\nsentences so it's definitely not",
    "start": "4125850",
    "end": "4131580"
  },
  {
    "text": "good, various models can model\nthat correctly because \"not\"",
    "start": "4131580",
    "end": "4137310"
  },
  {
    "text": "is a negative word. And so therefore, it weakens the\npositivity of the positive word.",
    "start": "4137310",
    "end": "4144549"
  },
  {
    "text": "And so putting a \"not\" in front\nof a positive into a positive sentence makes it less positive.",
    "start": "4144550",
    "end": "4151899"
  },
  {
    "text": "And even a \"not very well,\" but\neven a Naive Bayes model can do",
    "start": "4151899",
    "end": "4157049"
  },
  {
    "text": "that, because \"not\" by itself\nis seen as a negative word. But the hard case is\nwhat happens if you",
    "start": "4157050",
    "end": "4163950"
  },
  {
    "text": "negate a negative sentence? Well, the result that you should\nget is it becomes more positive.",
    "start": "4163950",
    "end": "4170339"
  },
  {
    "text": "And neither are\nbigram Naive Bayes model or our earlier\nattempts at recursive models",
    "start": "4170340",
    "end": "4177540"
  },
  {
    "text": "can capture that, whereas\nthis RNTN structure was able to correctly capture this\nsort of semantic modification",
    "start": "4177540",
    "end": "4186810"
  },
  {
    "text": "structure and say, hey, that's\nmade the sentence much more positive. So that was a cool result. And\nto some extent, this result",
    "start": "4186810",
    "end": "4195960"
  },
  {
    "text": "I think, still isn't\ncaptured as well by any of the current\ntransformer models,",
    "start": "4195960",
    "end": "4201900"
  },
  {
    "text": "even though they have\nmany other advantages and are much better than a\ntree recursive neural network.",
    "start": "4201900",
    "end": "4209240"
  },
  {
    "text": "So yeah, so just\nto say a couple-- this is basically\nthe end to just say",
    "start": "4209240",
    "end": "4216500"
  },
  {
    "text": "a couple of final remarks\nabout these tree recursive neural networks. The reason that they\nbecame uncompetitive",
    "start": "4216500",
    "end": "4226880"
  },
  {
    "text": "is because they\njust didn't allow the kind of associations\nand information flow",
    "start": "4226880",
    "end": "4236630"
  },
  {
    "text": "that you have in a transformer. These models had a strictly\ncontext-free backbone,",
    "start": "4236630",
    "end": "4244680"
  },
  {
    "text": "and the only information flow\nwas tree structured following the context free backbone.",
    "start": "4244680",
    "end": "4251600"
  },
  {
    "text": "Whereas in the\ntransformer, you've got this attention function,\nwhere in every position",
    "start": "4251600",
    "end": "4257480"
  },
  {
    "text": "you're looking at\nevery other position, and so you can have much more\ngeneral information flow.",
    "start": "4257480",
    "end": "4263010"
  },
  {
    "text": "And in general,\nthat is just good. And transformers are\nmuch more powerful.",
    "start": "4263010",
    "end": "4268620"
  },
  {
    "text": "But on the other hand, to\nthe extent that you actually want to model the sort of\nsemantics of human language",
    "start": "4268620",
    "end": "4277760"
  },
  {
    "text": "carefully as to\nwhat modifies what and how does negation or\nquantifiers in a sentence",
    "start": "4277760",
    "end": "4285199"
  },
  {
    "text": "behave, in some sense, these\nmodels were more right. And so one of the things I'm\nstill kind of interested in is,",
    "start": "4285200",
    "end": "4292670"
  },
  {
    "text": "are there any opportunities\nto combine together some of the benefits of both\nof these ways of thinking",
    "start": "4292670",
    "end": "4299120"
  },
  {
    "text": "and have something\nthat's a bit more tree structured, while still more\nflexible, like a transformer?",
    "start": "4299120",
    "end": "4306470"
  },
  {
    "text": "OK, that's it for today. Thanks a lot. ",
    "start": "4306470",
    "end": "4315000"
  }
]