[
  {
    "start": "0",
    "end": "5560"
  },
  {
    "text": "Welcome back to CS 330. I'm Kyle, a TA for the course. Today, we have a guest speaker,\nJascha Sohl-Dickstein, who's",
    "start": "5560",
    "end": "12370"
  },
  {
    "text": "visiting us from Google Brain. Jascha is currently a senior\nstaff research scientist at Google. Over his career, Jascha has done\nsome really interesting work,",
    "start": "12370",
    "end": "19310"
  },
  {
    "text": "both within the context\nof machine learning and outside of it. Prior to receiving\nhis PhD from Berkeley,",
    "start": "19310",
    "end": "24429"
  },
  {
    "text": "he worked at JPL on Mars rovers. Afterwards, he spent\ntime at Khan Academy, using computational methods to\nimprove educational outcomes",
    "start": "24430",
    "end": "30939"
  },
  {
    "text": "for students. And finally, Jascha has\nalso spent some time here at Stanford as a postdoc.",
    "start": "30940",
    "end": "36430"
  },
  {
    "text": "Jascha's current\nresearch interests span machine learning,\nphysics, and neuroscience. In particular, he's done\nsome really interesting work",
    "start": "36430",
    "end": "43080"
  },
  {
    "text": "on the theory and practice of\ntraining large neural networks. So welcome back to\nStanford, Jascha, and I look forward to your talk.",
    "start": "43080",
    "end": "49070"
  },
  {
    "text": "Thank you. It's a pleasure to be here. Great. So I'm super happy to be here.",
    "start": "49070",
    "end": "54650"
  },
  {
    "text": "I'm going to be telling you\nabout learned optimizers, why I think they are the\nfuture, why we're not there yet,",
    "start": "54650",
    "end": "64150"
  },
  {
    "text": "why they're quite hard,\nand what they can do now.",
    "start": "64150",
    "end": "69640"
  },
  {
    "text": "I'm going to be talking about\nwork done by a lot of people. And especially, I should\ncall out Luke Metz and Niru.",
    "start": "69640",
    "end": "79150"
  },
  {
    "text": "Luke Metz, especially, is\nresponsible for majority of the work I'm going\nto be talking about",
    "start": "79150",
    "end": "84700"
  },
  {
    "text": "and is probably the world\nexpert on learned optimizers. I'd also strongly encourage you,\nlocally, and also virtually,",
    "start": "84700",
    "end": "94899"
  },
  {
    "text": "to interrupt me with questions. I really appreciate getting\nany kind of feedback from the audience at all.",
    "start": "94900",
    "end": "100689"
  },
  {
    "text": "So be encouraged to do that. Talk structure is going\nto be roughly like this.",
    "start": "100690",
    "end": "106360"
  },
  {
    "text": "I'm going to tell you why\nI think learned optimizers are going to change the world. I'm going to tell you about\nwhat learned optimizers are.",
    "start": "106360",
    "end": "113920"
  },
  {
    "text": "And then the majority\nof the time, I'm going to talk through many\nof the open problems that",
    "start": "113920",
    "end": "122140"
  },
  {
    "text": "are still challenges in\ndesigning and building these things. And where applicable, I'll\ngive you a few of the solutions",
    "start": "122140",
    "end": "130255"
  },
  {
    "text": "that we know about. Then, I'm going to tell\nyou a bit about where",
    "start": "130255",
    "end": "136330"
  },
  {
    "text": "learned optimizers are now,\nwhat they can already achieve. And I will describe how this\napproach to optimization",
    "start": "136330",
    "end": "146200"
  },
  {
    "text": "can unlock completely\nnovel, new capabilities. And I'll end by giving a quick\ndemo of an open-source lib--",
    "start": "146200",
    "end": "155410"
  },
  {
    "text": "well, a library we are in\nthe process of open-sourcing for doing research on\nlearned optimizers.",
    "start": "155410",
    "end": "162670"
  },
  {
    "text": "So, OK. Motivation. You can maybe tell the story\nof the deep learning revolution",
    "start": "162670",
    "end": "171940"
  },
  {
    "text": "in that researchers used\nto make entire careers out of hand-designing features\nfor image classification.",
    "start": "171940",
    "end": "180220"
  },
  {
    "text": "And people also used to learn\nfeatures for classification. And for a long time,\nthe learned features",
    "start": "180220",
    "end": "186849"
  },
  {
    "text": "weren't quite as good as\nthe hand-designed features. And then eventually, the\nlearned features were better. And now no one has a\ncareer, or almost no one",
    "start": "186850",
    "end": "194409"
  },
  {
    "text": "has a career hand-designing\nclassification features anymore. And kind of the\nmaybe key attributes",
    "start": "194410",
    "end": "201160"
  },
  {
    "text": "that enabled that\ntransformation were",
    "start": "201160",
    "end": "206830"
  },
  {
    "text": "that the amount of\ncompute available to learn",
    "start": "206830",
    "end": "211900"
  },
  {
    "text": "these features or learn\nthese functions increased. That the amount of data\nwe had to train learned",
    "start": "211900",
    "end": "219219"
  },
  {
    "text": "functions increased. And also the fact\nthat the things these learned features were\ncompeting against",
    "start": "219220",
    "end": "224860"
  },
  {
    "text": "were just hand-designed\nheuristics. And so, in general, if\nyou're performing something",
    "start": "224860",
    "end": "230380"
  },
  {
    "text": "with a hand-designed\nheuristic, you can learn a function that\ndramatically outperforms",
    "start": "230380",
    "end": "235900"
  },
  {
    "text": "the hand-designed heuristic.  However, we still hand\ndesign our optimizers,",
    "start": "235900",
    "end": "244739"
  },
  {
    "text": "and we hand design\nour loss functions. And we hand design\nour architectures, and we hand design\nour regularizers.",
    "start": "244740",
    "end": "250950"
  },
  {
    "text": "And we hand design\nmany other aspects of our learning pipeline. And I guess considering this\nis a meta-learning class,",
    "start": "250950",
    "end": "257278"
  },
  {
    "text": "you're probably already\nconvinced of what I'm trying to convey here.",
    "start": "257279",
    "end": "262710"
  },
  {
    "text": "But I believe that we are set\nup for a similar revolution",
    "start": "262710",
    "end": "270975"
  },
  {
    "text": "in the context of meta-learning. Even when well motivated,\nmany of these things,",
    "start": "270975",
    "end": "279870"
  },
  {
    "text": "or all of these things,\nare just heuristics. And maybe for one\nspecific example,",
    "start": "279870",
    "end": "285490"
  },
  {
    "text": "which comes up in optimization. If you are writing a paper\nabout some snazzy new model you",
    "start": "285490",
    "end": "291750"
  },
  {
    "text": "developed, you're\nprobably going to want to report accuracy on a\ntest data set in the paper.",
    "start": "291750",
    "end": "300550"
  },
  {
    "text": "And when you train\nyour model, you're probably going to train your\nmodel using cross-entropy",
    "start": "300550",
    "end": "306599"
  },
  {
    "text": "on the training data set. So even, for instance, if you\nhave the most theoretically well-justified\noptimizer in existence,",
    "start": "306600",
    "end": "312570"
  },
  {
    "text": "it's still just a\nhand-designed heuristic because you're training it\non the wrong data-- you're using it to train a model\non the wrong data set",
    "start": "312570",
    "end": "318690"
  },
  {
    "text": "with the wrong loss function. And so kind of the\ndream is that these",
    "start": "318690",
    "end": "329670"
  },
  {
    "text": "learned optimizers, or\nmore generally, these learned meta-learning\napproaches, are kind of creeping up on\nthe hand-designed approaches.",
    "start": "329670",
    "end": "336550"
  },
  {
    "text": "And they're going to pass\nthem at some point soon. And then, they will be\nall that people use. ",
    "start": "336550",
    "end": "351110"
  },
  {
    "text": "Cool. So I've told you why I\nthink learned optimizers are going to change the world. So now let me tell you what\na learned optimizer is.",
    "start": "351110",
    "end": "360260"
  },
  {
    "text": "So in standard training,\nyou have some model. And that model maybe has some\nparameters w, which we're",
    "start": "360260",
    "end": "367400"
  },
  {
    "text": "going to initialize to w0. And maybe you do steepest\ngradient descent.",
    "start": "367400",
    "end": "374270"
  },
  {
    "text": "So you update the\nparameters by taking",
    "start": "374270",
    "end": "380270"
  },
  {
    "text": "the current w minus\nsome learning rate alpha times the gradient of a loss\nfunction l with respect to w.",
    "start": "380270",
    "end": "388520"
  },
  {
    "text": "And you do this over\nand over and over again for capital N training steps. And then at the end of training,\nyou measure performance.",
    "start": "388520",
    "end": "396710"
  },
  {
    "text": "You maybe look at the\nvalidation loss of your model, for instance.",
    "start": "396710",
    "end": "403230"
  },
  {
    "text": "What we are going\nto do is we are going to replace this\nhand-designed optimizer, which",
    "start": "403230",
    "end": "409879"
  },
  {
    "text": "in this diagram is just\nsteepest gradient descent, with a black box. So we are going to replace\nthis with a function u, where",
    "start": "409880",
    "end": "419450"
  },
  {
    "text": "that function u ingests\nthe current parameter values and the most\nrecent gradients",
    "start": "419450",
    "end": "427129"
  },
  {
    "text": "and whatever other information\nyou want to pass it. So you can pass it\nvalidation loss.",
    "start": "427130",
    "end": "432260"
  },
  {
    "text": "You could pass it information\nabout the architecture that you're trying to optimize.",
    "start": "432260",
    "end": "437360"
  },
  {
    "text": "You could pass it\nsecond-order information, if you have it available. And this function u is going to\nhave its own outer-parameter's",
    "start": "437360",
    "end": "445699"
  },
  {
    "text": "theta, and it's going to\nspit out a new value of w. And so rather than applying\ngradient descent capital",
    "start": "445700",
    "end": "453200"
  },
  {
    "text": "N times, we're going to apply\nthis black box function u capital N times.",
    "start": "453200",
    "end": "459380"
  },
  {
    "text": "And then we are going to\ntrain this function u. So we are going to have\nan outer-training loop,",
    "start": "459380",
    "end": "465350"
  },
  {
    "text": "where you update the\nparameter's theta-- the outer-parameter's\ntheta of u.",
    "start": "465350",
    "end": "471289"
  },
  {
    "text": "So in this outer loop,\nthe meta-training or outer-training\nloop, we're going to update theta such that if you\napply u to the inner problem,",
    "start": "471290",
    "end": "482690"
  },
  {
    "text": "you do better at whatever\nyour measure of performance is on the inner\nproblem, be it training",
    "start": "482690",
    "end": "488270"
  },
  {
    "text": "loss at the inner-training\nor validation loss or whatever you care about.",
    "start": "488270",
    "end": "493400"
  },
  {
    "text": "And then in this inner\nloop, for a fixed value of the\nouter-parameters theta,",
    "start": "493400",
    "end": "499039"
  },
  {
    "text": "you are going to optimize\nyour target optimization task.",
    "start": "499040",
    "end": "504260"
  },
  {
    "text": "You're going to train a neural\nnetwork with parameters w. Or you're going to fit\nwhatever model you want",
    "start": "504260",
    "end": "511490"
  },
  {
    "text": "to fit that has parameters w. ",
    "start": "511490",
    "end": "517340"
  },
  {
    "text": "Cool, all right. So this is the\nhigh-level structure.",
    "start": "517340",
    "end": "523671"
  },
  {
    "start": "523671",
    "end": "532330"
  },
  {
    "text": "There is a lot of flexibility in\nterms of what you choose for U.",
    "start": "532330",
    "end": "539620"
  },
  {
    "text": "But maybe one architecture\nwhich is nice to keep in mind and is maybe the most common\narchitecture that people use in",
    "start": "539620",
    "end": "547060"
  },
  {
    "text": "learned optimizers\nis, imagine you have a little per-scalar,\nper-parameter update function.",
    "start": "547060",
    "end": "556220"
  },
  {
    "text": "So imagine you have\na function which takes in scalar gradients, and\nthen outputs changes in scalar",
    "start": "556220",
    "end": "567460"
  },
  {
    "text": "changes and parameter weights. ",
    "start": "567460",
    "end": "573170"
  },
  {
    "text": "This function is often an RNN. You can think of\nSGD with momentum",
    "start": "573170",
    "end": "579500"
  },
  {
    "text": "or Adam or most other optimizers\nas being very specific cases",
    "start": "579500",
    "end": "586160"
  },
  {
    "text": "of this function, in that they\nact per parameter in the target problem that you're optimizing.",
    "start": "586160",
    "end": "591320"
  },
  {
    "text": "And they have some\nlatent space, which in the case of SGD with momentum\nis just the momentum variable.",
    "start": "591320",
    "end": "598550"
  },
  {
    "text": "And they spit out delta w's. So if you want to\nthink of a simple case",
    "start": "598550",
    "end": "605930"
  },
  {
    "text": "for learned optimizer\narchitecture, just think of an RNN,\nwhere one copy of that RNN",
    "start": "605930",
    "end": "611510"
  },
  {
    "text": "is acting on every parameter\nin the target network. ",
    "start": "611510",
    "end": "620650"
  },
  {
    "text": "Cool. So yes. Can you still update over\nthe sequences of gradients",
    "start": "620650",
    "end": "626730"
  },
  {
    "text": "but don't actually\nlook at the weights? Ah. So in general, you can take\ninto account the weights.",
    "start": "626730",
    "end": "637300"
  },
  {
    "text": "And it can take into\naccount other information.  The simplest version, just the\ncompletely vanilla version,",
    "start": "637300",
    "end": "647829"
  },
  {
    "text": "is a per-parameter function\nthat only gets in gradients.",
    "start": "647830",
    "end": "653050"
  },
  {
    "text": "But you are completely\nright that you can build more complex\narchitectures that take into account weight\nvalues and take into account",
    "start": "653050",
    "end": "659110"
  },
  {
    "text": "a lot of other information. Is it-- oops.",
    "start": "659110",
    "end": "664753"
  },
  {
    "text": "Is it a great idea to\ninitialize this function to be the identity? ",
    "start": "664754",
    "end": "671920"
  },
  {
    "text": "It seems to be a good idea\nto initialize this function to have relatively\nsmall-scale output.",
    "start": "671920",
    "end": "678400"
  },
  {
    "text": "We haven't really\ntried to initialize it to be the identity. But depending on\nhow you did that,",
    "start": "678400",
    "end": "686440"
  },
  {
    "text": "it probably wouldn't hurt. Mm-hmm. So just to make it clear\nfor my understanding.",
    "start": "686440",
    "end": "691683"
  },
  {
    "text": "This operates on individual\nscalars, basically. Yeah, yeah. It doesn't have any [INAUDIBLE].",
    "start": "691684",
    "end": "698770"
  },
  {
    "text": "Yeah, yeah. Yeah, that's completely true. That is not a general property\nof learned optimizers.",
    "start": "698770",
    "end": "704980"
  },
  {
    "text": "That is just a property of\nthis particular simple form of a learned optimizer.",
    "start": "704980",
    "end": "711010"
  },
  {
    "text": "But it's easiest maybe to reason\nabout one that accents scalars. ",
    "start": "711010",
    "end": "717150"
  },
  {
    "text": "Mm-hmm. Yeah. I'm sorry. Quick question. Does the learned optimizer,\nin and of itself,",
    "start": "717150",
    "end": "724060"
  },
  {
    "text": "is optimized by a\nhand-engineered optimizer, right? Yeah.",
    "start": "724060",
    "end": "729430"
  },
  {
    "text": "So we're going to get to that. Oh, good. We're going to talk about\nthat more in the talk. The answer is that,\nbasically, yes.",
    "start": "729430",
    "end": "737140"
  },
  {
    "text": "We have done some\nfun experiments, where you can train learned\noptimize-- which I'm actually",
    "start": "737140",
    "end": "742290"
  },
  {
    "text": "not going to talk about. But we have done\nsome fun experiments, where you can kind of\nbootstrap learned optimizers",
    "start": "742290",
    "end": "747760"
  },
  {
    "text": "by using randomly-initialized\nlearned optimizers. So you can randomly\ninitialize learned optimizers.",
    "start": "747760",
    "end": "753100"
  },
  {
    "text": "And one of those learned\noptimizers, if you like-- maybe 30% or 40% of\nthe learned optimizers",
    "start": "753100",
    "end": "759580"
  },
  {
    "text": "will make some progress\non gradient descent after random initialization. And so you can\nconstantly just use",
    "start": "759580",
    "end": "765783"
  },
  {
    "text": "the current\nbest-learned optimizer to keep on training them. And it's a little bit of\nan elaborate infrastructure",
    "start": "765783",
    "end": "771610"
  },
  {
    "text": "kind of thing. But it will eventually do a\ngood job at training itself. It'll just take longer.",
    "start": "771610",
    "end": "778376"
  },
  {
    "text": "But, yeah, you're exactly right. ",
    "start": "778377",
    "end": "783830"
  },
  {
    "text": "Cool. OK, so I told you that I\nthink these things are going to change the world, or at least\na small region of the world",
    "start": "783830",
    "end": "790300"
  },
  {
    "text": "that I live in. I've told you what they are. And now we're going to spend\nthe bulk of the talk talking",
    "start": "790300",
    "end": "798640"
  },
  {
    "text": "about why they're hard, maybe\nordered roughly in level",
    "start": "798640",
    "end": "806860"
  },
  {
    "text": "of increasing challenge. So maybe problem\nnumber one is, how",
    "start": "806860",
    "end": "813639"
  },
  {
    "text": "do you get a learned\noptimizer to generalize to new optimization tasks? In standard supervised learning,\nfor instance, you typically",
    "start": "813640",
    "end": "822430"
  },
  {
    "text": "have to train your model\non thousands to millions or even billions of examples\nbefore it generalizes well",
    "start": "822430",
    "end": "830875"
  },
  {
    "text": "to new examples. So how do we hope to do the\nsame thing in the context",
    "start": "830875",
    "end": "837760"
  },
  {
    "text": "of learned optimizers? And the answer to\nthis is a lot of work.",
    "start": "837760",
    "end": "845022"
  },
  {
    "text": "So I don't want to, in any way,\ndismiss the answer to this, because it's really\na lot of work. But it's also fairly\nstraightforward.",
    "start": "845022",
    "end": "851750"
  },
  {
    "text": "The answer to this is you\nneed to construct a large data",
    "start": "851750",
    "end": "857010"
  },
  {
    "text": "set of optimization tasks\nthat you can use to train",
    "start": "857010",
    "end": "862750"
  },
  {
    "text": "your learned optimizer. And here, we have a\nparticular data set",
    "start": "862750",
    "end": "868120"
  },
  {
    "text": "of 1,000 some-odd tasks. And there's a similar\ndata set that's going to be part of\nthe open-source release",
    "start": "868120",
    "end": "874030"
  },
  {
    "text": "that I hinted at the\nbeginning of the talk. And in fact, we find that this\nworks the way you would hope.",
    "start": "874030",
    "end": "881710"
  },
  {
    "text": "So here, we are\nshowing performance on an iid test set as a function\nof the number of optimization",
    "start": "881710",
    "end": "894790"
  },
  {
    "text": "tasks that you train the\nlearned optimizer on. And you see a curve that\nshouldn't be surprising,",
    "start": "894790",
    "end": "900440"
  },
  {
    "text": "which is that the more\ndiverse optimization tasks you train on, the\nbetter you generalize",
    "start": "900440",
    "end": "909520"
  },
  {
    "text": "to additional\noptimization tasks. There is a problem with this\nwhich is largely unsolved--",
    "start": "909520",
    "end": "919670"
  },
  {
    "text": "so I would describe this as\nan open challenge-- which is that generalizing\nacross scale",
    "start": "919670",
    "end": "926170"
  },
  {
    "text": "remains really, really hard. You'd really, really like\nto train these things on an ensemble of\nsmall-scale tasks,",
    "start": "926170",
    "end": "932860"
  },
  {
    "text": "and then have them generalize\nto, like, train GPT-3.",
    "start": "932860",
    "end": "939370"
  },
  {
    "text": "But the problem is\nthat training learned optimizers is very expensive. And so you can't just\nincorporate GPT-3 training",
    "start": "939370",
    "end": "947829"
  },
  {
    "text": "as one of your training tasks. So you somehow have\nto train an ensemble of smaller-scale\ntasks, and then be",
    "start": "947830",
    "end": "954640"
  },
  {
    "text": "able to generalize to very,\nvery large-scale tasks. And this is not yet\nsomething that works well.",
    "start": "954640",
    "end": "962050"
  },
  {
    "start": "962050",
    "end": "967800"
  },
  {
    "text": "Mm-hmm. Yeah. So I have a question about sort\nof this distribution problem. It seems we're\ngeneralizing small-scale,",
    "start": "967800",
    "end": "974672"
  },
  {
    "text": "but it seems it's\nalso generalizing across distributions. So what I'm thinking is-- and\nI'm not an expert in this,",
    "start": "974672",
    "end": "980640"
  },
  {
    "text": "so more general thinking-- we have this-- you have\ndifferent tasks, which are sort of distributions of\nyour data, of your image data,",
    "start": "980640",
    "end": "989312"
  },
  {
    "text": "or maybe it would be language\ndata or whatever, right? But I think both\ndistributions have their own sort of\nsimilar [INAUDIBLE]",
    "start": "989312",
    "end": "995332"
  },
  {
    "text": "optimization landscapes, et\ncetera, et cetera, et cetera. So in the picture you\nshowed, it's an iid set?",
    "start": "995332",
    "end": "1001130"
  },
  {
    "text": " Yeah.",
    "start": "1001130",
    "end": "1006933"
  },
  {
    "text": "So it is perceivable\nto me you would have this multiple\n[INAUDIBLE] but they're kind of like data\ndistributions for each task, and obviously they're different.",
    "start": "1006933",
    "end": "1013180"
  },
  {
    "text": " And as you're optimizing this,\nthe parameters for this thing count as well, or\nyou're optimizing",
    "start": "1013180",
    "end": "1019478"
  },
  {
    "text": "the third distribution\nas well because the loss",
    "start": "1019478",
    "end": "1024692"
  },
  {
    "text": "out scales [INAUDIBLE]\nwhatever [INAUDIBLE].. But is that the case if you\nactually try to [INAUDIBLE] you",
    "start": "1024692",
    "end": "1037429"
  },
  {
    "text": "have much more [INAUDIBLE]\nthe optimization",
    "start": "1037430",
    "end": "1043390"
  },
  {
    "text": "problem [INAUDIBLE]\ndifferent optimization problem for supervised\nlearning or whatever.",
    "start": "1043390",
    "end": "1048751"
  },
  {
    "text": "For example, you want to be\nable to transfer or something like that. Yeah, so that is\na great question.",
    "start": "1048751",
    "end": "1054760"
  },
  {
    "text": "How you even define\nout-of-domain generalization is, like, both an optimization\nand even in, I don't know,",
    "start": "1054760",
    "end": "1062710"
  },
  {
    "text": "images or text, it's just like--\nit's really like, how far-- how do you measure distance if\nyou're moving out of domain?",
    "start": "1062710",
    "end": "1070509"
  },
  {
    "text": "How different is\nthe distribution? We do find-- and I unfortunately\ndon't have thoughts about that",
    "start": "1070510",
    "end": "1077860"
  },
  {
    "text": "in this talk-- but we do find that if you hold\nout one class of problems-- so, for instance, you\ndon't include any VAEs",
    "start": "1077860",
    "end": "1087325"
  },
  {
    "text": "in the meta-training set, and\nthen you use the optimizer to train VAEs--",
    "start": "1087325",
    "end": "1093310"
  },
  {
    "text": "we do find that increasing\nthe meta-training set size improves performance on the\nholdout class of models.",
    "start": "1093310",
    "end": "1100179"
  },
  {
    "text": "But it's hard to know,\nwhich class of model should I really consider as\nbeing outside of the training",
    "start": "1100180",
    "end": "1108880"
  },
  {
    "text": "distribution?  I see. But I guess I'm\nthinking-- so it seems",
    "start": "1108880",
    "end": "1116006"
  },
  {
    "text": "to me that a lot\nof this sort of has to do with the landscape\nof the distribution. So have you had\nexperiments where maybe you",
    "start": "1116006",
    "end": "1123867"
  },
  {
    "text": "tried some things like ImageNet\nor [INAUDIBLE] ImageNet [INAUDIBLE]? ",
    "start": "1123868",
    "end": "1134919"
  },
  {
    "text": "Yes, we have. We haven't specifically\ndone ImageNet to MLP.",
    "start": "1134920",
    "end": "1140260"
  },
  {
    "text": "I would have to-- ",
    "start": "1140260",
    "end": "1145360"
  },
  {
    "text": "I would have to look back\nat which specific sets of experiments we have. I think maybe the\nmost dramatic we",
    "start": "1145360",
    "end": "1152440"
  },
  {
    "text": "did is we tried transferring\nto RNNs without training on any RNNs--",
    "start": "1152440",
    "end": "1158320"
  },
  {
    "text": "without outer\ntraining on any RNNs. And there, the learned\noptimizer works.",
    "start": "1158320",
    "end": "1164890"
  },
  {
    "text": "It transfers. It doesn't transfer as\nwell as you would like. So it's in this \"gray sort\nof yeah\" kind of region.",
    "start": "1164890",
    "end": "1172495"
  },
  {
    "start": "1172495",
    "end": "1180020"
  },
  {
    "text": "Cool. Partially because we want\nto address questions just like that, we would\nlike to understand",
    "start": "1180020",
    "end": "1188493"
  },
  {
    "text": "how these things work. Because the more we\nunderstand how they work, the more we can identify how\nto make them work better,",
    "start": "1188493",
    "end": "1195950"
  },
  {
    "text": "and maybe the more we\ncan develop lessons that we can even transfer\nfrom learned optimizers",
    "start": "1195950",
    "end": "1203045"
  },
  {
    "text": "to more traditional optimizers. So one way to try to understand\nhow these things work",
    "start": "1203045",
    "end": "1213169"
  },
  {
    "text": "is to take a very simple\nform of a learned optimizer and train it to optimize\nvery simple tasks",
    "start": "1213170",
    "end": "1222810"
  },
  {
    "text": "and do our best to introspect\nand decompose how it does it. So in this particular case,\nwe took a per-parameter GRU,",
    "start": "1222810",
    "end": "1233540"
  },
  {
    "text": "so a form of RNN\nlearned optimizer, and we trained it on one of-- so\neach of these gets a different",
    "start": "1233540",
    "end": "1241700"
  },
  {
    "text": "learned optimizer-- one of\neither linear regression on a 2D data set,\nor Rosenbrock--",
    "start": "1241700",
    "end": "1248500"
  },
  {
    "text": "the Rosenbrock loss\nfunction, or a small MLP trying to solve the two\nmoons classification problem.",
    "start": "1248500",
    "end": "1257030"
  },
  {
    "text": " Observation number\none, which is nice, is that if we train these\nthings, then they do in fact",
    "start": "1257030",
    "end": "1266720"
  },
  {
    "text": "do better on each of these\ntasks than the tuned--",
    "start": "1266720",
    "end": "1273320"
  },
  {
    "text": "well, learning rate\ntuning baselines. But that's not maybe the\npart that's most interesting.",
    "start": "1273320",
    "end": "1278900"
  },
  {
    "text": "The part that's\nmost interesting is trying to understand how\nthey do as well as they do.",
    "start": "1278900",
    "end": "1285980"
  },
  {
    "text": " And just to give a flavor\nmaybe of the type of analysis",
    "start": "1285980",
    "end": "1295630"
  },
  {
    "text": "that you can do, we are\nrunning a RNN on these tasks.",
    "start": "1295630",
    "end": "1306710"
  },
  {
    "text": "And you can analyze RNNs using\ntools from dynamical systems.",
    "start": "1306710",
    "end": "1313380"
  },
  {
    "text": "So one of those tools is, if\nyou hold the input to the RNN constant, then there will\nbe fixed points, which",
    "start": "1313380",
    "end": "1321950"
  },
  {
    "text": "in this case are\nalso attractors, in the latent space of the RNN. And if we look at the attractors\nin the latent space of the RNN",
    "start": "1321950",
    "end": "1333260"
  },
  {
    "text": "as we change the input\nmagnitude-- so we pass it",
    "start": "1333260",
    "end": "1340640"
  },
  {
    "text": "in like a fixed input, which\nis just like-- in this case, the input is gradient. So we've passed it\nin a fixed input,",
    "start": "1340640",
    "end": "1346170"
  },
  {
    "text": "which is just the same\ngradient at every time step. And we look at where the\nattractors and the state space are.",
    "start": "1346170",
    "end": "1351650"
  },
  {
    "text": "First of all, we find that\nthere's one primary attractor per input. So it has a very, very\nsimple attractor structure.",
    "start": "1351650",
    "end": "1359990"
  },
  {
    "text": "And second of all,\nthose attractors are arranged in\na line, depending on the gradient magnitude.",
    "start": "1359990",
    "end": "1369530"
  },
  {
    "text": "And we can then look at\none of those attractors. And we can look at the output\nthat the learned optimizer will",
    "start": "1369530",
    "end": "1381290"
  },
  {
    "text": "produce as a function\nof gradient input at that attractor point. So in this case,\nfor gradients that",
    "start": "1381290",
    "end": "1390410"
  },
  {
    "text": "range between negative 10 and\n10 on the Rosenbrock problem, we see that for each\nof these fixed points,",
    "start": "1390410",
    "end": "1397640"
  },
  {
    "text": "we get a roughly\nlinear relationship between the gradient and\nthe parameter update.",
    "start": "1397640",
    "end": "1405559"
  },
  {
    "text": "And that roughly\nlinear relationship corresponds to gradient descent,\nwhere the slope of this line",
    "start": "1405560",
    "end": "1413270"
  },
  {
    "text": "would correspond to the learning\nrate in standard gradient",
    "start": "1413270",
    "end": "1418880"
  },
  {
    "text": "descent. And so we can then look\nat how this learning",
    "start": "1418880",
    "end": "1424430"
  },
  {
    "text": "rate changes as a function\nof these fixed points. And what we find is that,\nfor the fixed points",
    "start": "1424430",
    "end": "1433700"
  },
  {
    "text": "corresponding to\nlarger-magnitude gradients, either very large\npositive gradients",
    "start": "1433700",
    "end": "1439940"
  },
  {
    "text": "in green or very large\nnegative gradients in red, that the effective\nlearning rate shrinks.",
    "start": "1439940",
    "end": "1447650"
  },
  {
    "text": "So maybe one kind\nof neat observation is that this thing is\nlearning a behavior, which",
    "start": "1447650",
    "end": "1458510"
  },
  {
    "text": "has been fairly recently\nadapted as standard practice in hand-designed\noptimizers, which",
    "start": "1458510",
    "end": "1465770"
  },
  {
    "text": "is learning rate adaptation. ",
    "start": "1465770",
    "end": "1472140"
  },
  {
    "text": "You can similarly look at-- yeah? Quick question on\ntwo slides before.",
    "start": "1472140",
    "end": "1479610"
  },
  {
    "text": "How do you like, generate\nthese broad spaces [INAUDIBLE] learning rate [INAUDIBLE]\nmaybe for a learning rate",
    "start": "1479610",
    "end": "1485550"
  },
  {
    "text": "algorithm versus a fixed\nlearning rate algorithm? How do you-- wait,\nwhich plots do you mean?",
    "start": "1485550",
    "end": "1491840"
  },
  {
    "text": "So the [INAUDIBLE] gradient\ncompared the [INAUDIBLE] momentum [INAUDIBLE].",
    "start": "1491840",
    "end": "1497606"
  },
  {
    "text": "So how does each of\nthese plots generate? Because there can be different\noptimal learning rates for different algorithms.",
    "start": "1497606",
    "end": "1504110"
  },
  {
    "text": "Yeah, so what we did in\neach of these is we tuned the hyperparameters\nof these algorithms",
    "start": "1504110",
    "end": "1512300"
  },
  {
    "text": "for each of these three tasks. ",
    "start": "1512300",
    "end": "1519290"
  },
  {
    "text": "So it's not one set\nof hyperparameters.",
    "start": "1519290",
    "end": "1526010"
  },
  {
    "text": "We did not-- the main\npurpose of this is not to-- I mean, these things are,\nlike, stupid toy tasks.",
    "start": "1526010",
    "end": "1532580"
  },
  {
    "text": "The main purpose of\nthis is not to compare the performance of\nlearned optimizers",
    "start": "1532580",
    "end": "1538550"
  },
  {
    "text": "and hand-designed\noptimizers here. ",
    "start": "1538550",
    "end": "1545270"
  },
  {
    "text": "Cool. You can also identify\nother behaviors that these things learn. One particularly neat\none, which again is",
    "start": "1545270",
    "end": "1553260"
  },
  {
    "text": "something we've\nalready discovered in standard optimizer\nland but is really",
    "start": "1553260",
    "end": "1560100"
  },
  {
    "text": "neat to see that these things\nare able to discover themselves just based on the data, is a\nbehavior of gradient clipping.",
    "start": "1560100",
    "end": "1570040"
  },
  {
    "text": "So in the previous plot, we were\nzoomed in right around here,",
    "start": "1570040",
    "end": "1579450"
  },
  {
    "text": "with gradients varying\nfrom negative 10 to 10 on the Rosenbrock. And so if you zoom\nout, and you look",
    "start": "1579450",
    "end": "1585120"
  },
  {
    "text": "at the relationship between\ngradient and update, then you find, in\nall three problems,",
    "start": "1585120",
    "end": "1591720"
  },
  {
    "text": "the learned\noptimizer is learning to do something analogous\nto gradient clipping,",
    "start": "1591720",
    "end": "1598260"
  },
  {
    "text": "where it saturates at\nvery large gradients. And maybe more\ninteresting, you can",
    "start": "1598260",
    "end": "1605430"
  },
  {
    "text": "build a histogram\nof the gradients that these optimizers experience\nwhile training these problems",
    "start": "1605430",
    "end": "1611670"
  },
  {
    "text": "from random initialization. And you find that the\nlinear region is well",
    "start": "1611670",
    "end": "1618299"
  },
  {
    "text": "matched to the most common\ngradient distributions.",
    "start": "1618300",
    "end": "1624810"
  },
  {
    "text": "And the saturation\nis mostly cutting off the tails of the\ndistribution of gradients.",
    "start": "1624810",
    "end": "1633480"
  },
  {
    "text": "So it's like learning\ngradient clipping adapted to the specific task\nthat it is learning.",
    "start": "1633480",
    "end": "1641519"
  },
  {
    "text": " Cool.",
    "start": "1641520",
    "end": "1647370"
  },
  {
    "text": "So there are some\ntools that we can start to use to understand\nhow these things work.",
    "start": "1647370",
    "end": "1654370"
  },
  {
    "text": "But there's still these\nblack box neural networks. One of the primary\nreasons you might",
    "start": "1654370",
    "end": "1660480"
  },
  {
    "text": "want to understand\nhow they work is that you need to figure out\nwhat the right architectural",
    "start": "1660480",
    "end": "1667890"
  },
  {
    "text": "structure for learned\noptimizers is. In Vision, things\nlike convolutions",
    "start": "1667890",
    "end": "1674940"
  },
  {
    "text": "and residual connections, or at\nleast patch-based processing,",
    "start": "1674940",
    "end": "1680730"
  },
  {
    "text": "are completely crucial to\nthe success we achieve. So open problem\nis, like, what are",
    "start": "1680730",
    "end": "1688799"
  },
  {
    "text": "the corresponding\narchitectural motifs in the space of optimizers?",
    "start": "1688800",
    "end": "1695460"
  },
  {
    "text": "And I can talk through\nsome of the maybe themes that we've roughly\nidentified so far.",
    "start": "1695460",
    "end": "1703500"
  },
  {
    "text": "But we don't really\nknow the answer to this. This is very much\nan open question.",
    "start": "1703500",
    "end": "1710430"
  },
  {
    "text": "But just to give a flavor,\nsome of the themes that seem to be crucial to their\nsuccess is, first of all,",
    "start": "1710430",
    "end": "1717372"
  },
  {
    "text": "if you want them\nto generalize, you want to make the input\nfeatures that these things are fed invariant to either gradient\nor parameter scale, whichever",
    "start": "1717372",
    "end": "1727529"
  },
  {
    "text": "the input features\ncorrespond to. And so, for instance,\none way you can do this is by rescaling the\ngradients by something",
    "start": "1727530",
    "end": "1735330"
  },
  {
    "text": "like an RMSprop-style\nrunning estimate of the standard deviation\nof the gradient.",
    "start": "1735330",
    "end": "1742710"
  },
  {
    "text": "You similarly want to make\nthe outputs of these things invariant or, I\nguess, equivariant",
    "start": "1742710",
    "end": "1749670"
  },
  {
    "text": "to the parameter scale. So one way to achieve\nthis is to make the output a fractional change in parameter\nrather than an absolute change",
    "start": "1749670",
    "end": "1756510"
  },
  {
    "text": "of parameter. So scale the output\nby the parameter norm. You want to provide as much\ninformation about the problem",
    "start": "1756510",
    "end": "1765000"
  },
  {
    "text": "as you can. This is maybe not surprising. But the more features you\nprovide the neural network,",
    "start": "1765000",
    "end": "1772020"
  },
  {
    "text": "the better it is\nable to perform. That is maybe\nreassuring, in that it means we're training them well.",
    "start": "1772020",
    "end": "1778830"
  },
  {
    "text": "On the other hand,\nthere is a cost to giving these things\nmore features and greater",
    "start": "1778830",
    "end": "1786330"
  },
  {
    "text": "architectural\ncomplexity, which is that there's an overhead\nassociated with them. And so for that, you want to\nchoose an architecture that",
    "start": "1786330",
    "end": "1794190"
  },
  {
    "text": "somehow minimizes the\nper-parameter compute required by the learned optimizer.",
    "start": "1794190",
    "end": "1801050"
  },
  {
    "text": "One way to do that is by\nmaking the learned optimizer",
    "start": "1801050",
    "end": "1806060"
  },
  {
    "text": "hierarchical, where you\nhave a very small number of parameters-- very small number of\nlearned-optimizer parameters",
    "start": "1806060",
    "end": "1813770"
  },
  {
    "text": "and learned-optimizer\nstate per inner parameter. And then you have a large amount\nof learned-optimizer state",
    "start": "1813770",
    "end": "1822530"
  },
  {
    "text": "per problem or per array\nin the target problem. ",
    "start": "1822530",
    "end": "1829889"
  },
  {
    "text": "We can-- mm-hmm. Yeah. I was just interested in\nthe normalization developed",
    "start": "1829890",
    "end": "1837330"
  },
  {
    "text": "by the validation loss\nand other parameters that supersedes across different\ntypes of training tasks.",
    "start": "1837330",
    "end": "1846720"
  },
  {
    "text": "You're going to get\ncompletely different outcomes. That is a superb question.",
    "start": "1846720",
    "end": "1851760"
  },
  {
    "text": "And, yeah, you want to\nnormalize everything. And that includes--\nif you're passing in training loss or\nvalidation loss, that",
    "start": "1851760",
    "end": "1858390"
  },
  {
    "text": "includes normalizing that. One way you can do\nit is you can do",
    "start": "1858390",
    "end": "1864899"
  },
  {
    "text": "a percentile-style\nnormalization based upon the history of\nvalidation losses",
    "start": "1864900",
    "end": "1871380"
  },
  {
    "text": "or history of training\nlosses that you've seen so far during optimization. So you can be like, your current\nvalidation loss is at level 0.1",
    "start": "1871380",
    "end": "1884549"
  },
  {
    "text": "compared to all the\nvalidation losses you've seen during training. But you do. You need to figure out ways to\nnormalize basically everything.",
    "start": "1884550",
    "end": "1893220"
  },
  {
    "text": "Otherwise, it really\ndoesn't like it if you change the\nscale of something. ",
    "start": "1893220",
    "end": "1904659"
  },
  {
    "text": "Cool. And so you can quantify\nthese trade-offs, of course. It's maybe nice to see here in--",
    "start": "1904660",
    "end": "1911429"
  },
  {
    "text": "there's this aqua point\nin the lower-right. So these plots are\nperformance on the meta-loss",
    "start": "1911430",
    "end": "1916590"
  },
  {
    "text": "versus overhead in\neither computational time or in terms of memory of\nthe learned optimizer.",
    "start": "1916590",
    "end": "1924690"
  },
  {
    "text": "And you can see this aqua point\nin the lower-right corresponds to the learned optimizer with\nabsolutely everything in it.",
    "start": "1924690",
    "end": "1931950"
  },
  {
    "text": "And that does the\nbest, which is nice, and is also\nridiculously expensive.",
    "start": "1931950",
    "end": "1937950"
  },
  {
    "text": "So you probably want\nto keep in mind somehow the trade-off between\noverhead and architecture",
    "start": "1937950",
    "end": "1945313"
  },
  {
    "text": "when choosing how to\nbuild these things. ",
    "start": "1945313",
    "end": "1950549"
  },
  {
    "text": "Cool. OK. Now we get to the one\nwhich I think is actually the coolest of the\nchallenges, and also maybe",
    "start": "1950550",
    "end": "1958590"
  },
  {
    "text": "the most challenging of\nthe challenges, which is that outer-training a\nlearned optimizer is basically",
    "start": "1958590",
    "end": "1971670"
  },
  {
    "text": "like a dynamical system, with a\ndynamical system in every step of the training process.",
    "start": "1971670",
    "end": "1977440"
  },
  {
    "text": "And so all the pathologies that\na dynamical system can have, like chaos or divergence or\nextreme sensitivity to noise,",
    "start": "1977440",
    "end": "1987990"
  },
  {
    "text": "can now happen in every step\nof the dynamical system. And these things can be\nhorrifyingly unstable to train.",
    "start": "1987990",
    "end": "1995325"
  },
  {
    "text": " And there's some fun\nexamples that you",
    "start": "1995325",
    "end": "2001640"
  },
  {
    "text": "can do to visualize this. So here we are training a\nthree-layer MLP on MNIST.",
    "start": "2001640",
    "end": "2012710"
  },
  {
    "text": "And we're just using Adam. There's no learned\noptimizer here yet. And we are training\nthis many, many times",
    "start": "2012710",
    "end": "2023450"
  },
  {
    "text": "from the same initialization,\nwith the same random seed, the same ordering of mini\nbatches, all of that.",
    "start": "2023450",
    "end": "2029660"
  },
  {
    "text": "But we are slightly\nchanging the learning rate that Adam is using.",
    "start": "2029660",
    "end": "2035360"
  },
  {
    "text": "So specifically, we are\nvarying learning rates between 0.1469 and 0.1484.",
    "start": "2035360",
    "end": "2042440"
  },
  {
    "text": "So we're changing\nthe learning rate in the third significant digit. And then we're just going\nto run Adam training.",
    "start": "2042440",
    "end": "2047779"
  },
  {
    "text": "And what you're looking\nat here is a projection of the MLP parameters.",
    "start": "2047780",
    "end": "2054179"
  },
  {
    "text": "So this is like a trajectory\ntaken in the MLP parameter space. And initially, over the\nfirst few training steps,",
    "start": "2054179",
    "end": "2061370"
  },
  {
    "text": "you can see that, for all\nthe Adam learning rates, the training trajectories\nare very, very similar to each other.",
    "start": "2061370",
    "end": "2067590"
  },
  {
    "text": "But as you train this for\neven a relatively small number of training steps, like just\na few tens of training steps,",
    "start": "2067590",
    "end": "2076429"
  },
  {
    "text": "you find that changes\nin the third or fourth significant digit\nof the learning rate",
    "start": "2076429",
    "end": "2081980"
  },
  {
    "text": "leave to diverging trajectories\nin parameter space.",
    "start": "2081980",
    "end": "2088063"
  },
  {
    "text": "So if you were going to\nback-propagate through this, if you were going to be\nlike, D final solution",
    "start": "2088063",
    "end": "2094399"
  },
  {
    "text": "of the optimizer\nD learning rate, you are going to have severe\nproblems here because changing",
    "start": "2094400",
    "end": "2101480"
  },
  {
    "text": "the learning rate\nby only negative 4 can jump you from here to\nhere in parameter space.",
    "start": "2101480",
    "end": "2106820"
  },
  {
    "text": "Yeah. Were the networks initialized\nwith the same weights? Say that again. Were the networks\ninitialized randomly",
    "start": "2106820",
    "end": "2112330"
  },
  {
    "text": "or with the same weights? Same weights. Same weights, same mini batches. This is run with a V-map.",
    "start": "2112330",
    "end": "2117710"
  },
  {
    "text": "So really everything is the\nsame except the learning rate.",
    "start": "2117710",
    "end": "2122839"
  },
  {
    "text": " And if you want some reassurance\nthat we did the experiment",
    "start": "2122840",
    "end": "2128050"
  },
  {
    "text": "right, you will notice that\nsimilar learning rates are very, very close to each other.",
    "start": "2128050",
    "end": "2134720"
  },
  {
    "text": "So this would not be the case\nif the diverging trajectories were due to a problem\nwith the random seed.",
    "start": "2134720",
    "end": "2141622"
  },
  {
    "text": "Mm-hmm. Does each [INAUDIBLE]\nregions-ish in that?",
    "start": "2141622",
    "end": "2148990"
  },
  {
    "text": "Is each region similar\nultimate performance?",
    "start": "2148990",
    "end": "2154510"
  },
  {
    "text": "Yeah, so you're\nasking what the-- so let me rephrase\nyour question.",
    "start": "2154510",
    "end": "2160283"
  },
  {
    "text": "It's a really good question. So let me rephrase\nyour question. We're showing that the final\nparameters, inner parameters",
    "start": "2160283",
    "end": "2167380"
  },
  {
    "text": "you get, can change dramatically\nbased upon small changes in your training parameters.",
    "start": "2167380",
    "end": "2173410"
  },
  {
    "text": "And you might ask, OK, but\nmaybe these are all equivalent. Maybe changing your--\nmaybe the outer loss--",
    "start": "2173410",
    "end": "2183100"
  },
  {
    "text": "maybe the training loss is the\nsame for all of these places, or within all of these places.",
    "start": "2183100",
    "end": "2188920"
  },
  {
    "text": "And the answer is-- and I'll\nshow some images in a moment--",
    "start": "2188920",
    "end": "2194559"
  },
  {
    "text": "coarsely, yes. But on a fine scale, no. So all of these will have\napproximately the same training",
    "start": "2194560",
    "end": "2204790"
  },
  {
    "text": "loss. But that won't help\nyou for optimization because the training\nloss is going",
    "start": "2204790",
    "end": "2210760"
  },
  {
    "text": "to be jumping up and down in\na very jagged fashion based upon very small changes\nin the learning rate.",
    "start": "2210760",
    "end": "2216069"
  },
  {
    "text": " And we can visualize that. And we get some\nreally pretty images.",
    "start": "2216070",
    "end": "2221800"
  },
  {
    "text": "So let's look at some\nmore pretty images. So here we're looking for\nkind of the same qualitative",
    "start": "2221800",
    "end": "2231010"
  },
  {
    "text": "problem. But now we're looking at\nit for a learned optimizer. And now we're going to look\nat the loss rather than",
    "start": "2231010",
    "end": "2238510"
  },
  {
    "text": "the final parameter\nvalues of the problem that we're optimizing. So here what we have\nis a learned optimizer.",
    "start": "2238510",
    "end": "2247036"
  },
  {
    "text": "This is like an MLP\nlearned optimizer. It doesn't really matter. And we're doing a\ntwo-dimensional slice",
    "start": "2247037",
    "end": "2253210"
  },
  {
    "text": "through the parameters--\nthe hyperparameters of the learned optimizer. So basically you're\ntrying to move",
    "start": "2253210",
    "end": "2259960"
  },
  {
    "text": "in this space to find the\nbest-performing learned optimizer. ",
    "start": "2259960",
    "end": "2266230"
  },
  {
    "text": "And I think in this case, we're\ntraining a two-layer ConvNet",
    "start": "2266230",
    "end": "2271720"
  },
  {
    "text": "on MNIST. And so the question is, what\ndoes this loss landscape",
    "start": "2271720",
    "end": "2281650"
  },
  {
    "text": "that you're trying\nto optimize, when you train the learned\noptimizer, look like? And if you unroll the inner\nproblem for one step--",
    "start": "2281650",
    "end": "2289099"
  },
  {
    "text": "so if you only apply the\nlearned optimizer for one step, then it looks really nice. You start up here.",
    "start": "2289100",
    "end": "2294593"
  },
  {
    "text": "And you're going to\ndo gradient descent. And you're going\nto go down there. And it's super smooth. You could use\nanalytic gradients.",
    "start": "2294593",
    "end": "2300730"
  },
  {
    "text": "But as we apply the\nlearned optimizer for more and more and\nmore training steps,",
    "start": "2300730",
    "end": "2307480"
  },
  {
    "text": "then you see that you get\nmore and more structure,",
    "start": "2307480",
    "end": "2314320"
  },
  {
    "text": "like complex structure in\nthe meta-loss landscape. And this happens\nrelatively fast. This is only, like,\nunrolling 10 training steps.",
    "start": "2314320",
    "end": "2322400"
  },
  {
    "text": "And so if you apply the learned\noptimizer for 20 training steps, or 30 or 50, then you\nnow have this incredibly complex",
    "start": "2322400",
    "end": "2333670"
  },
  {
    "text": "loss landscape. And essentially, every\npixel in this image is a different loss value.",
    "start": "2333670",
    "end": "2340640"
  },
  {
    "text": "So this is going to be a\nhorrible loss landscape if you wanted to descend it\nby steepest gradient descent,",
    "start": "2340640",
    "end": "2347990"
  },
  {
    "text": "because first of\nall, the gradients are going to be\nmagnitude 10 to the 30.",
    "start": "2347990",
    "end": "2353000"
  },
  {
    "text": "And second of all,\nthere's no way you're going to navigate\nthis structure to get",
    "start": "2353000",
    "end": "2358270"
  },
  {
    "text": "to this broad, low-loss region. Yeah. So this is the result of\nusing a learned optimizer.",
    "start": "2358270",
    "end": "2366860"
  },
  {
    "text": "No. So this particular image\nis for a learned optimizer. But you can get this\nkind of behavior even",
    "start": "2366860",
    "end": "2373089"
  },
  {
    "text": "for a hand-designed optimizer. And it's actually\neven worse than that.",
    "start": "2373090",
    "end": "2378670"
  },
  {
    "text": "The best-performing--\nOK, so let's say you're just tuning\nlearning rate on an optimizer.",
    "start": "2378670",
    "end": "2383980"
  },
  {
    "text": "The best-performing learning\nrate is often, like, epsilon away from the learning\nrate that diverges.",
    "start": "2383980",
    "end": "2389780"
  },
  {
    "text": "So typically, outer\ntraining-- so training",
    "start": "2389780",
    "end": "2395050"
  },
  {
    "text": "of your meta-parameters or your\nlearned-optimizer parameters-- converges to the region that\nis the edge of instability",
    "start": "2395050",
    "end": "2402850"
  },
  {
    "text": "or unstable. So it's not just that there\nare these horrible regions in parameter space.",
    "start": "2402850",
    "end": "2408700"
  },
  {
    "text": "It's that the right answer\nlives in these horrible regions in parameters-- in\nouter-parameter space. ",
    "start": "2408700",
    "end": "2416920"
  },
  {
    "text": "Do you have any\nintuition of why that is? ",
    "start": "2416920",
    "end": "2422770"
  },
  {
    "text": "Yes.  I took out of the\ntalk, actually,",
    "start": "2422770",
    "end": "2427880"
  },
  {
    "text": "a toy problem where you can\nshow how this can happen.",
    "start": "2427880",
    "end": "2436549"
  },
  {
    "text": "One situation is you can have\na toy problem where you can fall into one of two solutions.",
    "start": "2436550",
    "end": "2442400"
  },
  {
    "text": "And exactly which\nsolution you fall into depends, in a very\nsensitive way,",
    "start": "2442400",
    "end": "2448234"
  },
  {
    "text": "on the parameters\nof your optimizer. ",
    "start": "2448235",
    "end": "2455997"
  },
  {
    "text": "It can happen in a\nwhole bunch of-- do I have intuition for why\nthe edge of instability is the best way to train?",
    "start": "2455997",
    "end": "2461000"
  },
  {
    "text": "Or do I have intuition for\nwhy it can go chaotic maybe?",
    "start": "2461000",
    "end": "2466070"
  },
  {
    "text": "I guess, why is that so? I guess why is this\nwhole problem composed of [INAUDIBLE]? ",
    "start": "2466070",
    "end": "2473990"
  },
  {
    "text": "Yeah. OK, so I think the reason this\nis such a fundamental problem,",
    "start": "2473990",
    "end": "2479990"
  },
  {
    "text": "and the reason it's\nresistant to attempts to reparameterize the\noptimizer to be more stable,",
    "start": "2479990",
    "end": "2485299"
  },
  {
    "text": "is because edge of chaos\nor edge of divergence",
    "start": "2485300",
    "end": "2491120"
  },
  {
    "text": "are the best parameters\nthat you can find. And I think maybe you can\nthink about why that is maybe",
    "start": "2491120",
    "end": "2500420"
  },
  {
    "text": "with a toy case. If you are trying to do\ngradient descent in-- oh,",
    "start": "2500420",
    "end": "2507067"
  },
  {
    "text": "there's no markers-- trying\nto do gradient descent in an ill-conditioned\nGaussian, then",
    "start": "2507067",
    "end": "2513349"
  },
  {
    "text": "you want to descend\nas fast as you can in the shallow direction. And so you want to\nset the learning rate",
    "start": "2513350",
    "end": "2519440"
  },
  {
    "text": "as high as you can to descend\nin a shallow direction. But if you set the\nlearning rate too high, then you're going to diverge in\nthe high-curvature direction.",
    "start": "2519440",
    "end": "2526440"
  },
  {
    "text": "And so what you'll do is\nyou'll set the learning rate as large as you can,\nwithout diverging",
    "start": "2526440",
    "end": "2531740"
  },
  {
    "text": "in the high-curvature direction. And this means that you're going\nto be bouncing back and forth as much as possible in the\nhigh-curvature direction,",
    "start": "2531740",
    "end": "2539119"
  },
  {
    "text": "or to descend as\nfast as possible in the shallow direction. So even in this 2D,\nill-conditioned quadratic,",
    "start": "2539120",
    "end": "2546200"
  },
  {
    "text": "your optimal hyperparameters\nare at the edge of divergence.",
    "start": "2546200",
    "end": "2551329"
  },
  {
    "text": "And I think this is a\nmore general property. These are also made\nworse if you use",
    "start": "2551330",
    "end": "2558079"
  },
  {
    "text": "techniques like Adam or RMSprop\nor anything that normalizes--",
    "start": "2558080",
    "end": "2564020"
  },
  {
    "text": "anything where the learning\nrate sets the update step length rather than setting a\nmultiple on the gradient.",
    "start": "2564020",
    "end": "2569520"
  },
  {
    "text": "And that's because, as\nthe gradient shrinks, your update step\nlength doesn't shrink.",
    "start": "2569520",
    "end": "2574640"
  },
  {
    "text": "And you probably want\nto do this because kind of the characteristic\nstep length you want",
    "start": "2574640",
    "end": "2579950"
  },
  {
    "text": "to take for most\nproblems is really specified by the scale\nof the parameters,",
    "start": "2579950",
    "end": "2585079"
  },
  {
    "text": "not the scale of the\ngradient necessarily. But doing this means\nthat you're never going",
    "start": "2585080",
    "end": "2591252"
  },
  {
    "text": "to converge to a fixed point. You're always going to\nbounce back and forth around some kind of minimum.",
    "start": "2591252",
    "end": "2596750"
  },
  {
    "text": "And those trajectories\nare going to be chaotic.",
    "start": "2596750",
    "end": "2602645"
  },
  {
    "start": "2602645",
    "end": "2608942"
  },
  {
    "text": "OK, so, pretty picture.  Really complicated\nstructure, really fast.",
    "start": "2608942",
    "end": "2615727"
  },
  {
    "text": "So how do you possibly\ndeal with this?  So the best solution\nwe found is to smooth",
    "start": "2615727",
    "end": "2624720"
  },
  {
    "text": "the outer-loss landscape. And by the way, this is maybe\nan open-theory question, if you have any thoughts,\nwhich is, why is this loss",
    "start": "2624720",
    "end": "2633600"
  },
  {
    "text": "landscape chaotic on a\nfine scale but not chaotic on a course scale?",
    "start": "2633600",
    "end": "2639990"
  },
  {
    "text": "Why is it that moving epsilon\nchanges your loss almost",
    "start": "2639990",
    "end": "2645300"
  },
  {
    "text": "discontinuously, but\nmoving by a large value changes your loss\nrelatively smoothly?",
    "start": "2645300",
    "end": "2650640"
  },
  {
    "text": "But given this\nempirical structure, one solution that\nwould seem to work",
    "start": "2650640",
    "end": "2655740"
  },
  {
    "text": "pretty well, given this\npicture, is maybe we could just smooth this. Maybe we can convolve\nthis with a Gaussian,",
    "start": "2655740",
    "end": "2661590"
  },
  {
    "text": "and then we'll have a\nnice, smooth loss landscape that we can descend. And so we can, in fact, do that.",
    "start": "2661590",
    "end": "2668680"
  },
  {
    "text": "So one way to do this is\nto train these things using",
    "start": "2668680",
    "end": "2673980"
  },
  {
    "text": "variational optimization. So in this case, we're going\nto define a perturbation",
    "start": "2673980",
    "end": "2680550"
  },
  {
    "text": "distribution over\nthe parameters. And then we're going to\ndefine a new loss, which",
    "start": "2680550",
    "end": "2685980"
  },
  {
    "text": "I'm going to call fancy L here,\nas the average of your loss",
    "start": "2685980",
    "end": "2691290"
  },
  {
    "text": "L over this perturbed\ndistribution over the parameters.",
    "start": "2691290",
    "end": "2696630"
  },
  {
    "text": "And so here, we're perturbing\nthe parameters of the Gaussian. And this is equivalent to\nconvolving your loss landscape",
    "start": "2696630",
    "end": "2704660"
  },
  {
    "text": "with a Gaussian. And, OK, that sounds maybe nice. But how do you actually\ncompute this thing?",
    "start": "2704660",
    "end": "2713220"
  },
  {
    "text": "And there are maybe\ntwo approaches that you can take to compute\nthis smooth loss landscape.",
    "start": "2713220",
    "end": "2724410"
  },
  {
    "text": "Neither of them are perfect,\nbut both of them work. One of them is called\nEvolution Strategies.",
    "start": "2724410",
    "end": "2733930"
  },
  {
    "text": "So in Evolution Strategies,\nyou basically use the reinforced-style trick to--",
    "start": "2733930",
    "end": "2740850"
  },
  {
    "text": "so what we're interested in\nis the gradient of a fancy L, with respect to the parameters.",
    "start": "2740850",
    "end": "2745950"
  },
  {
    "text": "And so one way we\ncan get that is we can use a reinforced-style\ntrick to turn",
    "start": "2745950",
    "end": "2752130"
  },
  {
    "text": "the gradient of the expectation\nunder a distribution into the expectation of the\nweighted gradient of the log",
    "start": "2752130",
    "end": "2760290"
  },
  {
    "text": "distribution. And so we can rewrite\nthis, gradient",
    "start": "2760290",
    "end": "2766740"
  },
  {
    "text": "of this, as an average\nover samples drawn from the perturbation\ndistribution of the loss",
    "start": "2766740",
    "end": "2772770"
  },
  {
    "text": "function times the\ngradient, with respect to theta of those samples. And then just subbing in\nthe form for that gradient,",
    "start": "2772770",
    "end": "2781560"
  },
  {
    "text": "this is just the loss scaled\nby the-- or the perturbation",
    "start": "2781560",
    "end": "2787470"
  },
  {
    "text": "in parameter scaled by the loss. And rewriting this\none more time,",
    "start": "2787470",
    "end": "2795460"
  },
  {
    "text": "you can see that this\nis effectively, here, we draw a perturbation\nfrom a Gaussian.",
    "start": "2795460",
    "end": "2802769"
  },
  {
    "text": "And we're essentially\ntaking the correlation of the perturbed loss function\nand the perturbation itself",
    "start": "2802770",
    "end": "2811710"
  },
  {
    "text": "and using this as an\nestimate of the gradient.",
    "start": "2811710",
    "end": "2817109"
  },
  {
    "text": "And I'm going to do one\nmore step to this, which I think is useful in two ways.",
    "start": "2817110",
    "end": "2823840"
  },
  {
    "text": "So specifically, we're going\nto replace this average with an average over positive\nand negative perturbations.",
    "start": "2823840",
    "end": "2833310"
  },
  {
    "text": "So before, we just had\nL of theta plus epsilon s times epsilon s.",
    "start": "2833310",
    "end": "2838807"
  },
  {
    "text": "And now we're going\nto have L of theta plus epsilon s minus L\nof theta minus epsilon s times epsilon s.",
    "start": "2838807",
    "end": "2844680"
  },
  {
    "text": "And so this is just averaging\nover the contribution from the positive epsilon\ns and the negative epsilon",
    "start": "2844680",
    "end": "2850230"
  },
  {
    "text": "s contribution. And this is nice\nfor two reasons. One of the reasons\nthis is nice is",
    "start": "2850230",
    "end": "2856500"
  },
  {
    "text": "I think it makes it really\nkind of visually obvious that this is a finite\ndifference algorithm.",
    "start": "2856500",
    "end": "2862769"
  },
  {
    "text": "We are going to randomly\nperturb our loss. And we're going to evaluate\nour loss at those two-- at the randomly-perturbed\nlocation and the negative",
    "start": "2862770",
    "end": "2869819"
  },
  {
    "text": "of it. And that's going to give us\nan estimate of the gradient. It's nice in another\nreason, which",
    "start": "2869820",
    "end": "2876030"
  },
  {
    "text": "is that antithetic\nsamples like this turn out to be completely crucial to this\nbeing, in any way, practical,",
    "start": "2876030",
    "end": "2884430"
  },
  {
    "text": "in that this cancels\nout any variance due to the constant value\nof the loss function.",
    "start": "2884430",
    "end": "2892780"
  },
  {
    "text": "So you end up with a much\nlower variance estimate. This also induces a\nreally nice property,",
    "start": "2892780",
    "end": "2898720"
  },
  {
    "text": "which is not captured\nby variance alone, which is every single sample now will\ngive you a descent direction.",
    "start": "2898720",
    "end": "2906340"
  },
  {
    "text": "So no matter how high\nvariance that direction is, it's never going\nto point uphill. And the reason for\nthat is that the sine",
    "start": "2906340",
    "end": "2913260"
  },
  {
    "text": "of the gradient\nestimate is always determined by the actual sign\nof the change in the loss.",
    "start": "2913260",
    "end": "2918550"
  },
  {
    "text": "So if you increase the\nvariance of this thing, it will turn more and\nmore and more and more towards 90 degrees.",
    "start": "2918550",
    "end": "2924030"
  },
  {
    "text": "But it's never going to make\nyou go in the wrong direction. And I think this\nhas not actually been characterized\nwell, theoretically.",
    "start": "2924030",
    "end": "2930450"
  },
  {
    "text": "But I think this\npartially explains why the performance of this\nparticular finite difference algorithm is sometimes better\nthan you would predict just",
    "start": "2930450",
    "end": "2939210"
  },
  {
    "text": "from its variance.  OK, so I mentioned\nthere were two ways that you could do this.",
    "start": "2939210",
    "end": "2945740"
  },
  {
    "text": "There's another\nway you can compute the gradient, which is that you\ncan use the reparameterization",
    "start": "2945740",
    "end": "2950820"
  },
  {
    "text": "trick. So you can, once again,\npull out the perturbation",
    "start": "2950820",
    "end": "2959130"
  },
  {
    "text": "as a separate variable. And then you can\njust average over perturbations of the gradient\nof theta of the loss of theta",
    "start": "2959130",
    "end": "2966588"
  },
  {
    "text": "plus that variable.  These two estimators of the\ngradient of the meta-loss",
    "start": "2966588",
    "end": "2976860"
  },
  {
    "text": "have very different\nvariance properties. If your meta-loss\nis smooth, then",
    "start": "2976860",
    "end": "2984359"
  },
  {
    "text": "the reparameterization gradient\nhas analytic ratings inside it. And it gives you a super\nlow variance, super accurate",
    "start": "2984360",
    "end": "2991860"
  },
  {
    "text": "estimate of the gradient. Now, if the loss\nsurface is smooth,",
    "start": "2991860",
    "end": "2999210"
  },
  {
    "text": "the evolution strategies\nestimate of the gradient is kind of crappy.",
    "start": "2999210",
    "end": "3004250"
  },
  {
    "text": "It's pretty high variance. On the other hand,\nif your loss surface",
    "start": "3004250",
    "end": "3010400"
  },
  {
    "text": "has a lot of\nhigh-frequency structure, as the meta-loss\nlandscapes often do,",
    "start": "3010400",
    "end": "3016610"
  },
  {
    "text": "then the evolution\nstrategies gradient estimate is still kind of crappy.",
    "start": "3016610",
    "end": "3022040"
  },
  {
    "text": "It's still pretty high variance. But it's not any crappier. It's roughly the same variance,\nwhereas the reparameterization",
    "start": "3022040",
    "end": "3030500"
  },
  {
    "text": "gradient is now 20 or\n30 orders of magnitude larger than it was before.",
    "start": "3030500",
    "end": "3038000"
  },
  {
    "text": "This also, by the way-- this plot also, by the way,\nillustrates kind of a property I was talking about before.",
    "start": "3038000",
    "end": "3044430"
  },
  {
    "text": "So this is an estimate\nof the gradient of each of these estimates--\nthe variance of each of these estimators\nover the course",
    "start": "3044430",
    "end": "3051830"
  },
  {
    "text": "of a meta-optimization problem. And you can see\nthat we initialized",
    "start": "3051830",
    "end": "3057020"
  },
  {
    "text": "the learned\noptimizer in a region where the analytic\ngradients were pretty good. And over the course\nof meta-training,",
    "start": "3057020",
    "end": "3065390"
  },
  {
    "text": "the learned optimizer\nconverged to a region that was on the\nedge of instability,",
    "start": "3065390",
    "end": "3070400"
  },
  {
    "text": "where the RP gradients\nbecome completely useless. ",
    "start": "3070400",
    "end": "3079620"
  },
  {
    "text": "Cool. So we have a couple\ndifferent ways that we can estimate this thing. You can combine these two\nestimates of the variance.",
    "start": "3079620",
    "end": "3090170"
  },
  {
    "text": "So, for instance, one\nway to combine them is just to weight the two of\nthem by their current variance.",
    "start": "3090170",
    "end": "3097955"
  },
  {
    "text": " And we'll get to some\nplots in a while.",
    "start": "3097955",
    "end": "3105230"
  },
  {
    "text": "But, in fact, this does\nfairly well at meta-training.",
    "start": "3105230",
    "end": "3111930"
  },
  {
    "text": "Cool. This is maybe a\ngood time to pause for a second for any questions. ",
    "start": "3111930",
    "end": "3117740"
  },
  {
    "text": "Cool? OK. ",
    "start": "3117740",
    "end": "3128130"
  },
  {
    "text": "Next challenge in\nmaking these things work well is that outer-training\nis extremely expensive.",
    "start": "3128130",
    "end": "3138390"
  },
  {
    "text": "Completely naively, if you\nthink it takes capital N steps to train a model,\nwhere N is, like,",
    "start": "3138390",
    "end": "3143640"
  },
  {
    "text": "10,000 or a million\nor something, now, when you're training\na learned optimizer, you have capital N\nsteps of inner training",
    "start": "3143640",
    "end": "3152040"
  },
  {
    "text": "for each of the capital N\nsteps of outer training. So your meta-training cost is\na factor of 10,000 or a million",
    "start": "3152040",
    "end": "3161220"
  },
  {
    "text": "or something very large-- bigger than your\nstandard training cost. ",
    "start": "3161220",
    "end": "3169300"
  },
  {
    "text": "This is not a problem\nthat's fully solved.",
    "start": "3169300",
    "end": "3174500"
  },
  {
    "text": "But there are a few paths-- a few solutions. One of the solutions\nis to work at Google",
    "start": "3174500",
    "end": "3180670"
  },
  {
    "text": "and run this thing\nin 10,000 machines. But that's maybe not\nthe solution that's",
    "start": "3180670",
    "end": "3186755"
  },
  {
    "text": "most useful for all of you. Another solution is you can\ndo really clever vectorization",
    "start": "3186755",
    "end": "3197440"
  },
  {
    "text": "and parallelization\nlow-level kind of things. And actually, if you use JAX,\nJAX is beautiful at this.",
    "start": "3197440",
    "end": "3203965"
  },
  {
    "text": "So, for instance,\nyou're applying the same learned optimizer to\nevery parameter in every task",
    "start": "3203965",
    "end": "3209572"
  },
  {
    "text": "that you're optimizing. And so you can do things like\nvectorize over the inner tasks",
    "start": "3209572",
    "end": "3214610"
  },
  {
    "text": "that you're optimizing. And you can make really good\nuse of accelerators that way.",
    "start": "3214610",
    "end": "3222089"
  },
  {
    "text": "Another thing that you can do,\nwhich almost everyone does, is you can use partial unrolls\nof your inner optimization.",
    "start": "3222090",
    "end": "3231140"
  },
  {
    "text": "And we'll talk about\nthat in a second. Although, partial unrolls come\nwith their own problem of bias.",
    "start": "3231140",
    "end": "3236180"
  },
  {
    "text": "And I'm going to sell\none particular approach to doing partial unrolls while\nremoving the problem of bias",
    "start": "3236180",
    "end": "3244400"
  },
  {
    "text": "called Persistent\nEvolution Strategies. ",
    "start": "3244400",
    "end": "3250360"
  },
  {
    "text": "OK, so what are partial unrolls? If you've trained\nRNNs, you're probably",
    "start": "3250360",
    "end": "3257670"
  },
  {
    "text": "familiar with this approach. This is when you have\nan unrolled computation",
    "start": "3257670",
    "end": "3264270"
  },
  {
    "text": "graph that you're training. And rather than training\nthe entire thing at once, you unroll it for a few steps,\nand you compute a gradient",
    "start": "3264270",
    "end": "3273600"
  },
  {
    "text": "based upon those few steps. And then you unroll it\nfor another few steps, and you compute a\nloss in a gradient",
    "start": "3273600",
    "end": "3278850"
  },
  {
    "text": "for that new few steps. And you keep on repeating this. And you never\npropagate the gradient through the entire sequence.",
    "start": "3278850",
    "end": "3286110"
  },
  {
    "text": "This is great, in that you have\nmany more training gradients. You get a gradient from every\none of these inner truncations.",
    "start": "3286110",
    "end": "3294960"
  },
  {
    "text": "It's also great in that we\nshowed some plots of how the meta-loss landscape becomes\nmore and more chaotic the more",
    "start": "3294960",
    "end": "3302819"
  },
  {
    "text": "steps you unroll optimization. So this gives you a\nbetter-behaved loss landscape.",
    "start": "3302820",
    "end": "3308589"
  },
  {
    "text": "However, it sucks\nbecause it's biased and because you're cutting these\nconnections in the training",
    "start": "3308590",
    "end": "3318870"
  },
  {
    "text": "graph. This bias doesn't\nactually seem to hurt you that much when you're\ntraining RNNs, at least LSTMs",
    "start": "3318870",
    "end": "3328200"
  },
  {
    "text": "or GRU-style RNNs. But it actually turns out\nto matter a lot when you're",
    "start": "3328200",
    "end": "3335610"
  },
  {
    "text": "training learned optimizers. And here's an example\nfigure which maybe--",
    "start": "3335610",
    "end": "3342030"
  },
  {
    "text": "from a paper by Wu and Wren-- which maybe illustrates why this\nis, why short horizons and bias",
    "start": "3342030",
    "end": "3350520"
  },
  {
    "text": "can be particularly harmful in\nthe case of learned optimizers. So here we have a\nloss function, which",
    "start": "3350520",
    "end": "3359370"
  },
  {
    "text": "is just a 2D quadratic,\nwhich is ill conditioned. It's much higher\ncurvature along the y-axis",
    "start": "3359370",
    "end": "3367320"
  },
  {
    "text": "than it is along the x-axis. And this quadratic also has\na little bit of added noise.",
    "start": "3367320",
    "end": "3374340"
  },
  {
    "text": "So it's like a stochastic\ngradient-descent-style quadratic. And if you want to make\nthe best progress you can",
    "start": "3374340",
    "end": "3382500"
  },
  {
    "text": "in optimizing\nthis, then you want to use a large learning\nrate so you move quickly",
    "start": "3382500",
    "end": "3388230"
  },
  {
    "text": "along this\nlow-curvature direction. However, if you want to\ndescend to the lowest",
    "start": "3388230",
    "end": "3395310"
  },
  {
    "text": "losses possible in a\nsmall number of steps, then you want to\nuse a small learning rate so that you\ndescend rapidly in",
    "start": "3395310",
    "end": "3401880"
  },
  {
    "text": "the high-curvature direction. And so if you do\npartial unrolls,",
    "start": "3401880",
    "end": "3407580"
  },
  {
    "text": "then you will tend to be overly\ngreedy in your optimization. And you will tend,\nin even a simple loss",
    "start": "3407580",
    "end": "3414030"
  },
  {
    "text": "landscape like this, to prefer\na too small learning rate that descends rapidly to\nthe bottom of a valley",
    "start": "3414030",
    "end": "3419880"
  },
  {
    "text": "without navigating\nalong the valley. And we can see that in practice.",
    "start": "3419880",
    "end": "3428680"
  },
  {
    "text": "Here, we are training\na couple-layer, I believe, CNN on MNIST.",
    "start": "3428680",
    "end": "3434850"
  },
  {
    "text": "And we are just using\nan Adam optimizer. And all we're tuning\nis the learning rate. And here this is\nlike outer-training.",
    "start": "3434850",
    "end": "3443250"
  },
  {
    "text": "This is our training\nof the learning rate. And we're changing the\nnumber of partial unrolls",
    "start": "3443250",
    "end": "3448680"
  },
  {
    "text": "we're doing in this training. And you can see exactly\nthe same effect. You can see that if you\ndo very short unrolls,",
    "start": "3448680",
    "end": "3455490"
  },
  {
    "text": "then you have a\nlearning rate that you learn that's biased low.",
    "start": "3455490",
    "end": "3461579"
  },
  {
    "text": "And as you increase the\nlength of the unrolls, then you become more and\nmore and more aggressive",
    "start": "3461580",
    "end": "3467880"
  },
  {
    "text": "with the learning rate. And so short unrolls are\ngreat in that they make everything stable and fast.",
    "start": "3467880",
    "end": "3474180"
  },
  {
    "text": "But they suck in\nthat they make you choose very conservative\nhyperparameters that slow",
    "start": "3474180",
    "end": "3480510"
  },
  {
    "text": "your optimization to a crawl. ",
    "start": "3480510",
    "end": "3488220"
  },
  {
    "text": "Cool. But we have a magic\ntrick that can get you the best of both worlds.",
    "start": "3488220",
    "end": "3494110"
  },
  {
    "text": "And so this magic trick is\nsomething called Persistent ES.",
    "start": "3494110",
    "end": "3499350"
  },
  {
    "text": "And what it looks\nlike is like doing ES for each of the\npartial unrolls,",
    "start": "3499350",
    "end": "3508230"
  },
  {
    "text": "but then also\naccumulating a term which corrects for the truncation\nbias over the full sequence",
    "start": "3508230",
    "end": "3519060"
  },
  {
    "text": "of unrolls. So it's nice in that you can\nget gradient updates much more quickly for each short unroll.",
    "start": "3519060",
    "end": "3526440"
  },
  {
    "text": "It's nice because it's\nat least eventually unbiased, in that this\naccumulated correction",
    "start": "3526440",
    "end": "3533369"
  },
  {
    "text": "term will mean that,\nby the time you reach the end of the\nsequence, the sum of the gradients from each\nunroll will be correct.",
    "start": "3533370",
    "end": "3541680"
  },
  {
    "text": "And it's an ES method, so it\nsmooths the loss landscape,",
    "start": "3541680",
    "end": "3546720"
  },
  {
    "text": "which for us is a positive. And it means that there's\nno actual analytic gradients",
    "start": "3546720",
    "end": "3551880"
  },
  {
    "text": "required. So there's no having to\nstore the computation graph in memory. You can just run\nit once, forwards.",
    "start": "3551880",
    "end": "3558674"
  },
  {
    "start": "3558675",
    "end": "3566130"
  },
  {
    "text": "So I'm not going to\ndo the derivation. But just to very quickly\nhighlight the changes,",
    "start": "3566130",
    "end": "3573120"
  },
  {
    "text": "it looks a lot like standard ES. The difference is we have this\none additional accumulator",
    "start": "3573120",
    "end": "3580290"
  },
  {
    "text": "variable, which is\ngoing to accumulate the sum of all the perturbations\nwe've experienced so far.",
    "start": "3580290",
    "end": "3586680"
  },
  {
    "text": "And then every time\nyou update a particle,",
    "start": "3586680",
    "end": "3593579"
  },
  {
    "text": "you update this accumulator\nwith the perturbation that you used for\nthat particular unroll",
    "start": "3593580",
    "end": "3599849"
  },
  {
    "text": "of the particle. And then when you compute\nthe gradient estimate, rather than taking the\nloss times epsilon,",
    "start": "3599850",
    "end": "3606840"
  },
  {
    "text": "you instead take the loss times\nthe accumulated perturbation.",
    "start": "3606840",
    "end": "3612060"
  },
  {
    "text": " Cool.",
    "start": "3612060",
    "end": "3617309"
  },
  {
    "text": "And this thing works. Here, we show a\ncomparison of ES and PES",
    "start": "3617310",
    "end": "3626190"
  },
  {
    "text": "training a simple\nlearned optimizer. And it's nice in that\nPES runs converge more",
    "start": "3626190",
    "end": "3633810"
  },
  {
    "text": "reliably and converge\neventually to a lower loss because they're unbiased\nestimators of the gradient.",
    "start": "3633810",
    "end": "3639450"
  },
  {
    "text": " So, cool. That was all the problems.",
    "start": "3639450",
    "end": "3644559"
  },
  {
    "text": "And now we're going to\nlook at some pretty plots where learned-optimizer\ncurves go down. This is a really\ngood time to ask",
    "start": "3644560",
    "end": "3650940"
  },
  {
    "text": "any questions you have\nabout why these things are hard or not yet solved.",
    "start": "3650940",
    "end": "3656340"
  },
  {
    "start": "3656340",
    "end": "3663320"
  },
  {
    "text": "All right, pretty plots. So what can learned\noptimizers do now?",
    "start": "3663320",
    "end": "3671600"
  },
  {
    "text": "One thing they can\ndo is, if you are willing to put the\ncompute up front to meta-train a learned\noptimizer on a specific task,",
    "start": "3671600",
    "end": "3680240"
  },
  {
    "text": "they can do much\nbetter than even the best-tuned, hand-designed\noptimizer on that task.",
    "start": "3680240",
    "end": "3688230"
  },
  {
    "text": "So here, to make\nthat more specific, we are going to be meta-training\na learned optimizer",
    "start": "3688230",
    "end": "3695300"
  },
  {
    "text": "to train a two-hidden-layer\nConvNet on 10-way image",
    "start": "3695300",
    "end": "3704180"
  },
  {
    "text": "classification problems. And so we have an\ninner loop, where",
    "start": "3704180",
    "end": "3710390"
  },
  {
    "text": "we are training a ConvNet. We have an outer loop, where\nwe are using a meta-loss, which",
    "start": "3710390",
    "end": "3719930"
  },
  {
    "text": "is either the final training\nloss or the final validation loss from training the ConvNet. We have a distribution\nover tasks,",
    "start": "3719930",
    "end": "3725839"
  },
  {
    "text": "which in this\nparticular case is going to be randomly generated\n10-class classification",
    "start": "3725840",
    "end": "3732230"
  },
  {
    "text": "problems from ImageNet. And then in the\nmost outer loop, we",
    "start": "3732230",
    "end": "3738800"
  },
  {
    "text": "are going to be finding the\nlearned-optimizer parameters theta that allow it to\ndo best on this task.",
    "start": "3738800",
    "end": "3745115"
  },
  {
    "text": " And in fact, what\nyou find is that,",
    "start": "3745115",
    "end": "3751220"
  },
  {
    "text": "reassuringly, the\nlearned optimizer-- so here, the red line is the\nlearned optimizer-- trained",
    "start": "3751220",
    "end": "3758390"
  },
  {
    "text": "on the final\ntraining loss of this",
    "start": "3758390",
    "end": "3764299"
  },
  {
    "text": "is able to significantly more\nrapidly achieve an essentially zero training loss.",
    "start": "3764300",
    "end": "3771538"
  },
  {
    "text": "You never believe other\npeople's baselines. But I will say, this baseline\nis a very strong baseline.",
    "start": "3771538",
    "end": "3777690"
  },
  {
    "text": "This is Adam with learning\nrate decay and regularization",
    "start": "3777690",
    "end": "3783319"
  },
  {
    "text": "over 2,000 draws of\nhyperparameters, where we're doing both exponential and\nlinear learning rate decay,",
    "start": "3783320",
    "end": "3790580"
  },
  {
    "text": "and we're doing both L1\nand L2 regularization. So this is a very well-tuned,\nhand-designed optimizer.",
    "start": "3790580",
    "end": "3801740"
  },
  {
    "text": "Even cooler, you can\ntarget something other than training loss with\na learned optimizer.",
    "start": "3801740",
    "end": "3811640"
  },
  {
    "text": "So, for instance, you\ncan make your meta-loss the validation loss.",
    "start": "3811640",
    "end": "3816900"
  },
  {
    "text": "And if you do this-- maybe it's actually first\ninteresting to observe",
    "start": "3816900",
    "end": "3824150"
  },
  {
    "text": "what happens both for\nthe well-tuned Adam and for the learned optimizer\nwhen they very rapidly minimize",
    "start": "3824150",
    "end": "3830690"
  },
  {
    "text": "the training loss,\nwhich is that they also very rapidly diverge and do\nsignificantly worse on the test",
    "start": "3830690",
    "end": "3838369"
  },
  {
    "text": "loss. So this is maybe evidence that\ndoing better at optimization",
    "start": "3838370",
    "end": "3844609"
  },
  {
    "text": "is often not what you\nactually care about when",
    "start": "3844610",
    "end": "3850010"
  },
  {
    "text": "you're doing optimization.  But if we meta-train an\noptimizer using the validation",
    "start": "3850010",
    "end": "3861500"
  },
  {
    "text": "outer-objective, then it's able\nto get to a lower test loss than the well-tuned baseline.",
    "start": "3861500",
    "end": "3871970"
  },
  {
    "text": "Yeah. I'm curious because it looks\nlike the training objective,",
    "start": "3871970",
    "end": "3878170"
  },
  {
    "text": "it doesn't really\ncorrelate to the training. But the validation\ndoes really well.",
    "start": "3878170",
    "end": "3883483"
  },
  {
    "text": "But if it's supposed to\ntarget generalization, shouldn't it also-- given that it comes from the\nsame distribution-- shouldn't",
    "start": "3883483",
    "end": "3889435"
  },
  {
    "text": "it also perform really\nwell with training? Yeah, yeah. That's a great observation.",
    "start": "3889435",
    "end": "3894560"
  },
  {
    "text": "So notice the y-axis\nis different here. So it achieves a test\nloss or validation loss",
    "start": "3894560",
    "end": "3900590"
  },
  {
    "text": "of 1.2, which would be up here. So it does do as well\non training as it does--",
    "start": "3900590",
    "end": "3906410"
  },
  {
    "text": "it actually does\nbetter in training. But it's really cool that the\nbest validation loss is one",
    "start": "3906410",
    "end": "3912470"
  },
  {
    "text": "of the worst training losses. It really kind of\nhighlights this property. ",
    "start": "3912470",
    "end": "3925082"
  },
  {
    "text": "Cool. OK.  Then there's a\nquestion of, how well do these things generalize now?",
    "start": "3925082",
    "end": "3931762"
  },
  {
    "text": "And the answer is\nthat they generalize",
    "start": "3931762",
    "end": "3939330"
  },
  {
    "text": "better than\nhand-designed optimizers if you have a limited\nhyperparameter tuning budget. And they generalize worse if\nyou have a large hyperparameter",
    "start": "3939330",
    "end": "3946290"
  },
  {
    "text": "tuning budget. So, for instance, here,\nthis is the distribution",
    "start": "3946290",
    "end": "3952800"
  },
  {
    "text": "over many optimization tasks. And the x-axis is a\nnormalized score comparison",
    "start": "3952800",
    "end": "3961560"
  },
  {
    "text": "between a learned optimizer\nand a hand-designed baseline. And more to the right is better.",
    "start": "3961560",
    "end": "3967740"
  },
  {
    "text": "And here what we're\ncomparing against is the single-best\nhyperparameter",
    "start": "3967740",
    "end": "3972869"
  },
  {
    "text": "across all the problems\nfor Adam or Adam8p, which is Adam with more\nhyperparameters, or an AdamW.",
    "start": "3972870",
    "end": "3983310"
  },
  {
    "text": "And you can see\nthat if all you get is one run on your\ntarget problem, then learned optimizers\nwill typically",
    "start": "3983310",
    "end": "3990150"
  },
  {
    "text": "do much better in that one run. So a learned optimizer is\nbetter at adapting itself",
    "start": "3990150",
    "end": "3996359"
  },
  {
    "text": "to a new target problem\nand optimizing it well. As you increase the number of\nhyperparameter tuning trials",
    "start": "3996360",
    "end": "4008330"
  },
  {
    "text": "that you're allowed on\nthe target problem, then eventually\nhand-designed optimizers",
    "start": "4008330",
    "end": "4015740"
  },
  {
    "text": "become able to generalize\nto new problems better than learned optimizers\nwith, again, just one run.",
    "start": "4015740",
    "end": "4022160"
  },
  {
    "text": "Because there is, for\ncurrent learned-optimizer architectures, no notion\nof hyperparameters you can tune at\napplication time.",
    "start": "4022160",
    "end": "4029690"
  },
  {
    "text": "So if you're willing\nto spend 1,000x compute on your hand-designed optimizer,\nthen you will generalize to new",
    "start": "4029690",
    "end": "4036050"
  },
  {
    "text": "tasks better than a\nlearned optimizer. If you're only willing to run\nyour hand-designed optimizer once, then the learned\noptimizer will work better.",
    "start": "4036050",
    "end": "4044460"
  },
  {
    "text": "And so this is cool. This is promising.",
    "start": "4044460",
    "end": "4049859"
  },
  {
    "text": "This is not yet going\nto come in and replace state of the art on\nlarge-scale problems. But we're moving in\nthe right direction.",
    "start": "4049860",
    "end": "4056930"
  },
  {
    "text": " And one more, which I think is\njust a super fun experiment.",
    "start": "4056930",
    "end": "4063990"
  },
  {
    "text": "One of the most\nouter-distribution tasks that you can imagine is\ntraining a learned optimizer.",
    "start": "4063990",
    "end": "4069630"
  },
  {
    "text": "And so we can take a\ntrained learned optimizer and try to use it to train\nadditional learned optimizers.",
    "start": "4069630",
    "end": "4075270"
  },
  {
    "text": "And it works. These things can\ntrain themselves, given gradient estimates\nthat come from ES.",
    "start": "4075270",
    "end": "4083400"
  },
  {
    "text": "And you can--\ntraining curve here",
    "start": "4083400",
    "end": "4088470"
  },
  {
    "text": "looks roughly\nsimilar to a training curve you get if you train a\nlearned optimizer with Adam.",
    "start": "4088470",
    "end": "4094559"
  },
  {
    "text": " So that was a\nreally fast summary",
    "start": "4094560",
    "end": "4104380"
  },
  {
    "text": "of the current state\nof the art, which is these things can\ntarget specific problems and do really, really well\nin those specific problems.",
    "start": "4104380",
    "end": "4112089"
  },
  {
    "text": "These things can generalize. And they can generalize better\nthan untuned, hand-designed",
    "start": "4112090",
    "end": "4121270"
  },
  {
    "text": "optimizers with the new\ntasks, but not as well as very well-tuned, hand-designed\noptimizers on new tasks.",
    "start": "4121270",
    "end": "4127568"
  },
  {
    "text": "And they can train themselves,\nwhich is just kind of fun. ",
    "start": "4127569",
    "end": "4136049"
  },
  {
    "text": "One other thing which\nI'm going to go through really fast because I also\nwant to show you a quick CoLab",
    "start": "4136050",
    "end": "4141600"
  },
  {
    "text": "demo which I think is really\nneat, is that you can do things with learned optimizers that\nare maybe impossible to do",
    "start": "4141600",
    "end": "4152160"
  },
  {
    "text": "with hand-designed optimizers. And maybe one example of\nthat might be unsupervised",
    "start": "4152160",
    "end": "4159689"
  },
  {
    "text": "representation learning. So in unsupervised\nrepresentation learning, you have a mass\nof unlabeled data.",
    "start": "4159689",
    "end": "4165750"
  },
  {
    "text": "And you want to learn\nsome representation of that data, which is\nuseful for downstream tasks.",
    "start": "4165750",
    "end": "4175200"
  },
  {
    "text": "And the problem with\nthis is that you somehow need to optimize for\nattributes that you",
    "start": "4175200",
    "end": "4182759"
  },
  {
    "text": "don't know at training time. And all our current\napproaches to doing this are essentially like\nhand-designed surrogates,",
    "start": "4182760",
    "end": "4192120"
  },
  {
    "text": "which have mixed success.",
    "start": "4192120",
    "end": "4197520"
  },
  {
    "text": "But a really neat thing you\ncan do with meta-learning is you can target\nperformance on something",
    "start": "4197520",
    "end": "4207240"
  },
  {
    "text": "like this, where you\ndon't know the desired properties at training time. We can take an ensemble of\ntasks, where we know something",
    "start": "4207240",
    "end": "4216420"
  },
  {
    "text": "that we want it to do\ndownstream of that task. So, for instance, you\ncould take ImageNet, and you can train on ImageNet\nin an unsupervised fashion.",
    "start": "4216420",
    "end": "4223350"
  },
  {
    "text": "And you know that,\nafter training, it would be really nice if your\noptimizer has learned something",
    "start": "4223350",
    "end": "4229560"
  },
  {
    "text": "about object identity so that\nyou can do classification.",
    "start": "4229560",
    "end": "4234760"
  },
  {
    "text": "And so you can try to meta-train\nan unsupervised learning rule that does well on existed\nsupervised learning tasks.",
    "start": "4234760",
    "end": "4243970"
  },
  {
    "text": "So that if you give it unlabeled\ndata for a supervised learning task, it will produce\nrepresentations,",
    "start": "4243970",
    "end": "4250830"
  },
  {
    "text": "which you can then\nuse for rapid training",
    "start": "4250830",
    "end": "4259770"
  },
  {
    "text": "on that supervised\nlearning task on-- yeah, when given a small\nnumber of labels on that task.",
    "start": "4259770",
    "end": "4267489"
  },
  {
    "text": "And so the hope is\nthat you can then learn a parameter\nupdate rule that is able to do this\nfor totally new tasks.",
    "start": "4267490",
    "end": "4275580"
  },
  {
    "text": "Ideally, you could just\nlearn the general rule that could generalize\nacross data modalities,",
    "start": "4275580",
    "end": "4282910"
  },
  {
    "text": "could generalize across\nneural network architectures. Just give it an architecture\nand some unlabeled data,",
    "start": "4282910",
    "end": "4288902"
  },
  {
    "text": "and it will come up with\na representation which is, in some way, useful. And, OK, I'm not going to\ntalk about the architecture.",
    "start": "4288902",
    "end": "4297250"
  },
  {
    "text": "But you can do this. And you can meta-train an\nunsupervised learning rule",
    "start": "4297250",
    "end": "4309910"
  },
  {
    "text": "that learns\nrepresentations which are useful for downstream tasks. And here we have\njust a neat example,",
    "start": "4309910",
    "end": "4318670"
  },
  {
    "text": "where over the course\nof meta-training, we initially have a learning\nrule that just learns noise.",
    "start": "4318670",
    "end": "4324010"
  },
  {
    "text": "And then over the\ncourse of meta-training,",
    "start": "4324010",
    "end": "4329650"
  },
  {
    "text": "it begins to learn things like\ndigit templates in MNIST-- this is the same learning\nrule, by the way,",
    "start": "4329650",
    "end": "4335770"
  },
  {
    "text": "applied to different networks\nand different data sets-- or like oriented edges on CIFAR.",
    "start": "4335770",
    "end": "4343810"
  },
  {
    "text": "And you can similarly ask\nhow this thing generalizes.",
    "start": "4343810",
    "end": "4349750"
  },
  {
    "text": "It's able to\ngeneralize it right. This is not state\nof the art, but it is able to perform in a totally\nunsupervised way, learning of--",
    "start": "4349750",
    "end": "4365110"
  },
  {
    "text": "yeah, it's able to learn to\ndo unsupervised learning. It can similarly generalize\nacross architectures,",
    "start": "4365110",
    "end": "4373179"
  },
  {
    "text": "either depth or width. And you find that if\nyou've phrased the rule",
    "start": "4373180",
    "end": "4379525"
  },
  {
    "text": "to be suitably general, that\nit can also generalize along those axes. All right.",
    "start": "4379525",
    "end": "4385210"
  },
  {
    "text": "That was super fast\nfor that because I wanted to have a minute\nto do a fun CoLab demo.",
    "start": "4385210",
    "end": "4391750"
  },
  {
    "text": " So we've talked a lot about\nhow expensive and impractical",
    "start": "4391750",
    "end": "4400420"
  },
  {
    "text": "these things are. We've also talked about\nhow we have tools now that make them more practical.",
    "start": "4400420",
    "end": "4405460"
  },
  {
    "text": "And we've talked about\nhow modern accelerators,",
    "start": "4405460",
    "end": "4410680"
  },
  {
    "text": "you can take\nsurprisingly good use-- make surprisingly good\nuse of vectorization on modern accelerators.",
    "start": "4410680",
    "end": "4417400"
  },
  {
    "text": "Here, I'm going to show\nyou some experiments. These are going to run on a\nsingle core of a TPU here.",
    "start": "4417400",
    "end": "4425740"
  },
  {
    "text": "You could also run\nthis on a single GPU, and you would get\nvery, very close-- near-identical performance.",
    "start": "4425740",
    "end": "4432380"
  },
  {
    "text": "So the resources required\nfor these are not super high.",
    "start": "4432380",
    "end": "4438429"
  },
  {
    "text": "This is a library for\nlearned optimizers",
    "start": "4438430",
    "end": "4444997"
  },
  {
    "text": "that we are in the\nprocess of open-sourcing. I was hoping it was going\nto be open-sourced by today. But check again\nin, like, a week.",
    "start": "4444997",
    "end": "4451030"
  },
  {
    "text": "But very soon. And so just to walk you\nthrough defining and training",
    "start": "4451030",
    "end": "4457330"
  },
  {
    "text": "one of these things,\nfirst of all, you can specify the architecture\nfor the learned optimizer.",
    "start": "4457330",
    "end": "4463600"
  },
  {
    "text": "And you can either use an\nexisting, off-the-shelf optimizer architecture, or\nyou can modify these things",
    "start": "4463600",
    "end": "4470860"
  },
  {
    "text": "to define your own. And then you specify\nthe task that you",
    "start": "4470860",
    "end": "4476800"
  },
  {
    "text": "want to train the\nlearned optimizer on, or the setup-- or\nthe family of tasks.",
    "start": "4476800",
    "end": "4481869"
  },
  {
    "text": "And in this case, we're going\nto train a learned optimizer to optimize a very\nsmall 32-hidden unit",
    "start": "4481870",
    "end": "4490090"
  },
  {
    "text": "MLP on 8 by 8 fashion MNIST. And you specify schedules\nfor the number--",
    "start": "4490090",
    "end": "4503410"
  },
  {
    "text": "you specify how\nmany steps you want to run the inner-optimization\nproblem for.",
    "start": "4503410",
    "end": "4508660"
  },
  {
    "text": "So we're just going to try\nto train on 8 by 8 fashion MNIST in 100 steps.",
    "start": "4508660",
    "end": "4513789"
  },
  {
    "text": "You can parallelize over\nmultiple instantiations of this task. So this is like mini-batch size.",
    "start": "4513790",
    "end": "4519160"
  },
  {
    "text": "But the mini batch is number\nof tasks, simultaneously. Here, we're defining\nthat task_family",
    "start": "4519160",
    "end": "4525340"
  },
  {
    "text": "is just equal to this one task. You can choose your\ngradient estimator. I talked about PES.",
    "start": "4525340",
    "end": "4531199"
  },
  {
    "text": "And so we're going to use\nPES as a gradient estimator. You choose the outer\noptimizer, so the way",
    "start": "4531200",
    "end": "4537940"
  },
  {
    "text": "in which you are going to update\nthe outer parameters theta of your learned optimizer.",
    "start": "4537940",
    "end": "4544048"
  },
  {
    "text": "In this case, we're\ngoing to use Adam to update our outer parameters. And we set a random number.",
    "start": "4544048",
    "end": "4551930"
  },
  {
    "text": "And so we're going\nto run this cell. And now we are going\nto train the system.",
    "start": "4551930",
    "end": "4557950"
  },
  {
    "text": "We're going to initialize it. And then we're going to have\na pretty standard training loop, where we train the thing\nfor 2,000 optimization steps.",
    "start": "4557950",
    "end": "4569530"
  },
  {
    "text": "There's a pause at the\nbeginning while Jax compiles",
    "start": "4569530",
    "end": "4574630"
  },
  {
    "text": "the outer-training process. And then this should take\nabout a minute and a half.",
    "start": "4574630",
    "end": "4582595"
  },
  {
    "start": "4582595",
    "end": "4588010"
  },
  {
    "text": "So this is cool because I\nthink this is the first time that there has been\ninfrastructure that allows you",
    "start": "4588010",
    "end": "4597670"
  },
  {
    "text": "to do experiments on\nlearned optimizers that are on a single machine\nin CoLab, as opposed",
    "start": "4597670",
    "end": "4603430"
  },
  {
    "text": "to on, like, 10,000\nmachines in a cloud. So we very much hope\nthat this will turn out",
    "start": "4603430",
    "end": "4609880"
  },
  {
    "text": "to be really useful for\nturning this into a more",
    "start": "4609880",
    "end": "4615400"
  },
  {
    "text": "standard research topic. ",
    "start": "4615400",
    "end": "4626330"
  },
  {
    "text": "I'm going to wait. Yeah. How long have you guys\nbeen working on this for?",
    "start": "4626330",
    "end": "4631520"
  },
  {
    "text": "So in terms of open-sourcing it,\nprobably the last two or three months.",
    "start": "4631520",
    "end": "4637450"
  },
  {
    "text": "A lot of this, though, is built\non internal infrastructure that we've been developing\nover the last few years. ",
    "start": "4637450",
    "end": "4650750"
  },
  {
    "text": "I should also, by the\nway, shill for JAX. I know many of you are\nprobably PyTorch users.",
    "start": "4650750",
    "end": "4657130"
  },
  {
    "text": "If you really want to do\nflexible things with gradients, for instance, or\nparallelization,",
    "start": "4657130",
    "end": "4663070"
  },
  {
    "text": "JAX is your friend.  Cool. So now we can-- we just\ntrained a learned optimizer.",
    "start": "4663070",
    "end": "4669980"
  },
  {
    "text": "That was it. There was maybe 15 seconds of\nawkward pause, and we're there. Here, we can plot the meta-loss\nover the course of training.",
    "start": "4669980",
    "end": "4680239"
  },
  {
    "text": "As you would like,\nthe learned optimizer gets better and better\nand better on this task,",
    "start": "4680240",
    "end": "4685250"
  },
  {
    "text": "as we do 2,000 steps\nof outer-training. Now, we can test it.",
    "start": "4685250",
    "end": "4691230"
  },
  {
    "text": "We can compute the\ninner-training curve",
    "start": "4691230",
    "end": "4696620"
  },
  {
    "text": "of the learned optimizer applied\nto a random initialization of the task it was trained at. And let's also compute\nsome baselines.",
    "start": "4696620",
    "end": "4704059"
  },
  {
    "text": "So we're going to train the-- we're going to use Adam with\nthree different learning rates",
    "start": "4704060",
    "end": "4709460"
  },
  {
    "text": "on the same task, to\ntrain the same task. And so one learning rate, two\nlearning rates, three learning",
    "start": "4709460",
    "end": "4717320"
  },
  {
    "text": "rates. And we can plot a comparison. And, OK, great.",
    "start": "4717320",
    "end": "4724310"
  },
  {
    "text": "We've now trained\na learned optimizer that outperforms a\ncoarse grid search",
    "start": "4724310",
    "end": "4730340"
  },
  {
    "text": "of learning-rate-only Adam\non this super toy task. So that's reassuring.",
    "start": "4730340",
    "end": "4737850"
  },
  {
    "text": "We can now maybe ask some\nquestions about generalization, which I know is something\nthat many people asked during the talk.",
    "start": "4737850",
    "end": "4743070"
  },
  {
    "text": "So let's load up\na different task. And now instead of\ntraining a 32-layer--",
    "start": "4743070",
    "end": "4749570"
  },
  {
    "text": "one-hidden layer, 32-unit MLP,\nlet's train a two-hidden layer",
    "start": "4749570",
    "end": "4754940"
  },
  {
    "text": "MLP with 128 units per layer. And let's train it on a 28 by\n28 fashion MNIST instead of 8",
    "start": "4754940",
    "end": "4760010"
  },
  {
    "text": "by 8 fashion MNIST. And we're going to use our\nlearned optimizer, which",
    "start": "4760010",
    "end": "4765470"
  },
  {
    "text": "we trained on the other task,\nin order to train this new one. And we're also going\nto train Adam--",
    "start": "4765470",
    "end": "4771590"
  },
  {
    "text": "use Adam to train\non this new task. ",
    "start": "4771590",
    "end": "4779300"
  },
  {
    "text": "Again, we're waiting\nfor JAX to compile. ",
    "start": "4779300",
    "end": "4789652"
  },
  {
    "text": "Cool. OK. And now we can plot\nthe performance of all of these optimizers.",
    "start": "4789652",
    "end": "4796790"
  },
  {
    "text": "And you can see\nthat, as predicted, generalization of this\nlearned optimizer trained",
    "start": "4796790",
    "end": "4802610"
  },
  {
    "text": "on a single task is not great. It doesn't diverge, but\nit also doesn't optimize.",
    "start": "4802610",
    "end": "4807739"
  },
  {
    "text": "Whereas, learning-rate-tuned\nAdam continues to optimize.",
    "start": "4807740",
    "end": "4814130"
  },
  {
    "text": "But we have just run a\nlearned optimizer experiment on one TPU core, in about\nfive minutes, in a live demo.",
    "start": "4814130",
    "end": "4823370"
  },
  {
    "text": "And you can do the same at home. ",
    "start": "4823370",
    "end": "4828800"
  },
  {
    "text": "And I think that is-- I have the summary slide.",
    "start": "4828800",
    "end": "4834030"
  },
  {
    "text": "Cool. So just to actually\nsay the summary, I think learned optimizers\nare going to change the world.",
    "start": "4834030",
    "end": "4839998"
  },
  {
    "text": "I talked about what a\nlearned optimizer is. I talked about the relatively\nlarge set of open problems,",
    "start": "4839998",
    "end": "4846380"
  },
  {
    "text": "like, how do you come\nup with a training distribution for these things? How do you understand\nwhat they're doing?",
    "start": "4846380",
    "end": "4851390"
  },
  {
    "text": "How do you engineer them to\nhave inductive biases that are well suited to what\nyou want them to do?",
    "start": "4851390",
    "end": "4856530"
  },
  {
    "text": "How do you deal with the chaotic\nand super poorly conditioned loss landscapes?",
    "start": "4856530",
    "end": "4862910"
  },
  {
    "text": "How do you deal with\na large compute cost? How do you deal with\nbias from unrolls? How do you get these things\nto scale from small problems",
    "start": "4862910",
    "end": "4870110"
  },
  {
    "text": "to very large problems? We talked about what learned\noptimizers can achieve now, which is that, if you have\none very specific task",
    "start": "4870110",
    "end": "4878360"
  },
  {
    "text": "and you're willing to put a lot\nof compute into pre-training, then they will completely\nwin on that task.",
    "start": "4878360",
    "end": "4883940"
  },
  {
    "text": "And they can\ngeneralize all right. They can generalize\nwell compared to a small hyperparameter\ntuning budget.",
    "start": "4883940",
    "end": "4890090"
  },
  {
    "text": "We talked about how you can\nmeta-train parameter update rules to optimize even\nin situations where",
    "start": "4890090",
    "end": "4897830"
  },
  {
    "text": "you don't have an explicit loss\nfunction during training time. And we did a quick demo of a\ncoming-soon open-source package",
    "start": "4897830",
    "end": "4906260"
  },
  {
    "text": "for working with them. And that's all I got. ",
    "start": "4906260",
    "end": "4915000"
  }
]