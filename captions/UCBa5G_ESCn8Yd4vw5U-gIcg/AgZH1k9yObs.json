[
  {
    "start": "0",
    "end": "5390"
  },
  {
    "text": "OK, so today, we're\ngoing to finish up where we left off last week\ntalking about system modeling.",
    "start": "5390",
    "end": "10620"
  },
  {
    "text": "So you remember that there's\ntwo inputs, two validation algorithms. One is a model of the system.",
    "start": "10620",
    "end": "16110"
  },
  {
    "text": "And the other one is a\nproperty specification. So last week, we talked\nabout the first one,",
    "start": "16110",
    "end": "21960"
  },
  {
    "text": "which was modeling the system. We didn't quite finish. So we're going to\nfinish that up today. And then we're going to move\non to the second input, which",
    "start": "21960",
    "end": "29540"
  },
  {
    "text": "is the property that we\nwant the system to satisfy. And so before I get\ninto that, I just",
    "start": "29540",
    "end": "35540"
  },
  {
    "text": "wanted to really quickly\nZoom out for a second here and just give\nyou a concrete example",
    "start": "35540",
    "end": "46399"
  },
  {
    "text": "to have some context\nin your mind for what we're thinking about when\nwe're building models. Because I know it can get kind\nof easy to get lost in the math.",
    "start": "46400",
    "end": "52785"
  },
  {
    "text": "So I just want to quickly\nprovide this concrete example where if you're getting\nlost in the math, you can try to\nthink back to this",
    "start": "52785",
    "end": "59270"
  },
  {
    "text": "and ground the things\nthat we're learning. So we're in the\nAeroAstro Department.",
    "start": "59270",
    "end": "64620"
  },
  {
    "text": "So I'm going to go ahead\nand try to use an aircraft. They don't teach\nyou to draw aircraft when you get your master's and\nPhD in the AeroAstro department,",
    "start": "64620",
    "end": "72150"
  },
  {
    "text": "but that's the best I can do. So we assume we have this\naircraft flying along.",
    "start": "72150",
    "end": "78530"
  },
  {
    "text": "And it's changing\naltitude every so often. And this is the ground here.",
    "start": "78530",
    "end": "84479"
  },
  {
    "text": "And we're going to assume--\nso if we were going to model this\naircraft, we would need to model its decision-making. We would need so its\nagent its sensor and then",
    "start": "84480",
    "end": "91820"
  },
  {
    "text": "also its environment. Let's just think\nabout the sensor. And let's think about\none part of its state.",
    "start": "91820",
    "end": "97890"
  },
  {
    "text": "So let's just think\nabout a sensor that's responsible for measuring\nits height above the ground",
    "start": "97890",
    "end": "104300"
  },
  {
    "text": "or its altitude. And so as we go along, we'll\nget different measurements of this height,\nsomething like this.",
    "start": "104300",
    "end": "113060"
  },
  {
    "text": "And they're some noisy\nversion of the actual height. So these are our observations.",
    "start": "113060",
    "end": "119960"
  },
  {
    "text": "When we talk about data,\nthis is the observations that we're getting. This is the data\nthat we're getting. And then let's assume also that\nfor this kind of data collection",
    "start": "119960",
    "end": "127820"
  },
  {
    "text": "run in the real world, we\nalso had this really, really expensive, super good, super\naccurate altitude sensor",
    "start": "127820",
    "end": "134030"
  },
  {
    "text": "that we ran along with\nit that basically gave us the ground truth. So what was the actual\nheight that we're at?",
    "start": "134030",
    "end": "139130"
  },
  {
    "text": "And so then we can get these\npairs that are like h and h hat,",
    "start": "139130",
    "end": "144380"
  },
  {
    "text": "where h hat is going to be\nthat measured version and h is the ground truth version. So we can have h1.",
    "start": "144380",
    "end": "151650"
  },
  {
    "text": "Maybe we have h2 and so on\nall the way to m data points.",
    "start": "151650",
    "end": "156939"
  },
  {
    "text": " And so in this case,\nour state is h.",
    "start": "156940",
    "end": "165290"
  },
  {
    "text": "And our observation is h hat. And so what we can do\nis we can then maybe",
    "start": "165290",
    "end": "171260"
  },
  {
    "text": "make a plot, where on\nthe x-axis, we plot h, and on the y-axis,\nwe plot h hat.",
    "start": "171260",
    "end": "178280"
  },
  {
    "text": "And it will probably\nlook something like this.",
    "start": "178280",
    "end": "185300"
  },
  {
    "text": "And what we're trying to model\nis our observation model, which we said is the\nprobability of getting",
    "start": "185300",
    "end": "191060"
  },
  {
    "text": "a particular observation,\ngiven that we're in state S. So in this case, our\nobservations are these h hats.",
    "start": "191060",
    "end": "198600"
  },
  {
    "text": "So probability of\nobserving h hat, given that we're at height h.",
    "start": "198600",
    "end": "204470"
  },
  {
    "text": "And so the first\nthing we need to do, which we talked about last\ntime, is select a model class. So what is a good model\nclass to represent this data",
    "start": "204470",
    "end": "212330"
  },
  {
    "text": "that we see here? And to me, I don't\nknow, it looks like the h hat is a\nlinear function of h",
    "start": "212330",
    "end": "219769"
  },
  {
    "text": "with some noise added to it. So that indicates\nto me that maybe a conditional Gaussian\nmodel is a good idea here.",
    "start": "219770",
    "end": "226490"
  },
  {
    "text": "So we're going to model as a\nconditional Gaussian, where we say h hat comes from\nsome function parameterized",
    "start": "226490",
    "end": "234440"
  },
  {
    "text": "by theta of each with\nsome noise added to it.",
    "start": "234440",
    "end": "240320"
  },
  {
    "text": "And then I said, I\nthink it looks linear. So I'm going to take\nit one step further and say that's a\nlinear function.",
    "start": "240320",
    "end": "245819"
  },
  {
    "text": "So we'll say theta\n1 times h, that would be the slope of\nthat line plus theta 2,",
    "start": "245820",
    "end": "251519"
  },
  {
    "text": "and then this sigma squared. And so now, we have\nour model class.",
    "start": "251520",
    "end": "257040"
  },
  {
    "text": "And if you remember\nthe second step was to then perform some\nsort of optimization to pick these parameters.",
    "start": "257040",
    "end": "263820"
  },
  {
    "text": "So last time, we learned about\nmaximum likelihood optimization. ",
    "start": "263820",
    "end": "271232"
  },
  {
    "text": "And then that would give us\nsome values for the parameters. So to me, this looks\nkind of a slope of 1.",
    "start": "271232",
    "end": "276240"
  },
  {
    "text": "So we'd probably get something\nlike theta 1 equals 1. Theta 2 is probably 0. The y-intercept\nis around 0 here.",
    "start": "276240",
    "end": "283610"
  },
  {
    "text": "And then our sigma squared is\nsome value, maybe say like 0.05.",
    "start": "283610",
    "end": "291050"
  },
  {
    "text": "OK, and so that's what I want\nyou to be thinking of when we're talking about taking data and\nfitting a set of parameters",
    "start": "291050",
    "end": "298670"
  },
  {
    "text": "to that data and\nselecting model classes. And then just one\nmore thing here. Let's say that we instead\ncollected a bunch of data here,",
    "start": "298670",
    "end": "308450"
  },
  {
    "text": "and it looked maybe\nthat or something.",
    "start": "308450",
    "end": "314460"
  },
  {
    "text": "OK, so this doesn't\nlook linear anymore. First of all, you should\nthink about what you're doing.",
    "start": "314460",
    "end": "319830"
  },
  {
    "text": "If this is what your\naltitude sensor looks like, you might just want to\nthrow it in the trash because this is probably\nnot the best plan.",
    "start": "319830",
    "end": "325770"
  },
  {
    "text": "But say, you do want\nto use the sensor and you do want to model it. To make a model of the\nsensor, you probably",
    "start": "325770",
    "end": "331550"
  },
  {
    "text": "don't want to select a\nlinear model here anymore. And so you're going to need\nthis more expressive model",
    "start": "331550",
    "end": "336680"
  },
  {
    "text": "class, which is what I was\ntalking about last week. So hopefully, this just\nhelps ground all the things",
    "start": "336680",
    "end": "343820"
  },
  {
    "text": "we've been talking about. What questions do you\nhave on this example? Yeah?",
    "start": "343820",
    "end": "350150"
  },
  {
    "text": "You said the delt-- or what did you write on the\nsecond line right after the--",
    "start": "350150",
    "end": "355639"
  },
  {
    "text": "This line? This line? Oh, what's the letter\nthat you wrote?",
    "start": "355640",
    "end": "362780"
  },
  {
    "text": "Here? Yeah. Here? Yeah. That letter theta. Is that the-- or what's\nthe thing right next to it?",
    "start": "362780",
    "end": "369140"
  },
  {
    "text": "F. Here, this is\nsupposed to be f theta h. [LAUGHTER] Yeah, always\nlet me know if you",
    "start": "369140",
    "end": "376759"
  },
  {
    "text": "can't read my handwriting. Oh, yeah, cool, so let's\npick up back up where we left off last time.",
    "start": "376760",
    "end": "383600"
  },
  {
    "text": "So if you remember,\nwe had talked about how I'm super indecisive.",
    "start": "383600",
    "end": "392160"
  },
  {
    "text": "And so for that\nreason, I really like Bayesian parameter learning\nbecause then we don't just",
    "start": "392160",
    "end": "398630"
  },
  {
    "text": "have to pick one\nset of parameters like we did with maximum\nlikelihood parameter learning. And instead, we get to\nmaintain this distribution",
    "start": "398630",
    "end": "405680"
  },
  {
    "text": "over all possible parameters. So we talked about\nhow we can do that. And it turns out we want to\nmodel this p theta, given d.",
    "start": "405680",
    "end": "413760"
  },
  {
    "text": "But we don't know how to\ncompute this directly. And so we said what's nice is\nthat we do know how to compute",
    "start": "413760",
    "end": "420330"
  },
  {
    "text": "pd, given theta. And in fact, that's\nactually what we needed to do for the maximum\nlikelihood parameter learning.",
    "start": "420330",
    "end": "425500"
  },
  {
    "text": "So we talked about\nhow to do that. And then we said, OK,\nwell, if we can only compute pd given\ntheta, we need to use",
    "start": "425500",
    "end": "431670"
  },
  {
    "text": "this thing called Bayes' rule. So we derived that\nfrom the definition of conditional probability. ",
    "start": "431670",
    "end": "439200"
  },
  {
    "text": "Thanks for pointing out\nthis typo last time, it should be fixed now. And so we derive Bayes' rule.",
    "start": "439200",
    "end": "445030"
  },
  {
    "text": "And then we said, OK, let's\ntake this and apply it to our p theta, given d. And we got this expression here.",
    "start": "445030",
    "end": "452430"
  },
  {
    "text": "And we examined this further\nand we said, OK, this expression has what we call a\nlikelihood model, which",
    "start": "452430",
    "end": "458460"
  },
  {
    "text": "is exactly what\nwe were optimizing in our maximum likelihood\nparameter learning. And then it has this extra\nterm, p theta, which is a prior.",
    "start": "458460",
    "end": "466930"
  },
  {
    "text": "So this is something we didn't\nhave with maximum likelihood parameter learning. Here, we're specifying,\nwhat is our belief before we",
    "start": "466930",
    "end": "475000"
  },
  {
    "text": "see any data over what\npossible values are-- or the likely values\nfor the parameter theta.",
    "start": "475000",
    "end": "480680"
  },
  {
    "text": "So I gave the example, if\nsomeone handed me a quarter and they said, what's the\nprobability that this quarter",
    "start": "480680",
    "end": "485949"
  },
  {
    "text": "comes up heads? I'd have this strong prior that\nassigns a lot of likelihood to theta being 0.5, or 50%\nbeing the probability or chance",
    "start": "485950",
    "end": "495010"
  },
  {
    "text": "that it comes up heads. Then we observe some data. And we update our belief\nover the probability",
    "start": "495010",
    "end": "501280"
  },
  {
    "text": "that it becomes heads. And we get this thing called\nthe posterior distribution over theta.",
    "start": "501280",
    "end": "508000"
  },
  {
    "text": "But then I told you that we\nhave this problem where-- so that was the discrete case.",
    "start": "508000",
    "end": "513317"
  },
  {
    "text": "We also have the\ncontinuous case where we have the summation in the\nbottom turns into an integral.",
    "start": "513317",
    "end": "518559"
  },
  {
    "text": "But regardless of the\nexpression we have, it's generally quite difficult\nto compute this result",
    "start": "518559",
    "end": "523689"
  },
  {
    "text": "analytically. So when we have the summation,\nthe number of terms in the sum scales exponentially with\nthe dimension of theta.",
    "start": "523690",
    "end": "532060"
  },
  {
    "text": "In this continuous case,\ncomputing this integral can be quite difficult\nanalytically.",
    "start": "532060",
    "end": "537880"
  },
  {
    "text": "And so a lot of times,\nwe can't actually just get an analytic\nfunction for these values,",
    "start": "537880",
    "end": "543160"
  },
  {
    "text": "or for these quantities here. But there is\nsomething that's kind of cool, which is-- so it's\nreally hard to compute this",
    "start": "543160",
    "end": "550180"
  },
  {
    "text": "because we need to compute it\nfor every single value of theta. So we need to sum up over\nall possible values of theta.",
    "start": "550180",
    "end": "555440"
  },
  {
    "text": "We need to integrate over\nall possible values of theta. But if I just give you\none value of theta,",
    "start": "555440",
    "end": "561010"
  },
  {
    "text": "we can compute this numerator. It just involves\ncomputing pd, given theta, which we did for our maximum\nlikelihood estimation.",
    "start": "561010",
    "end": "568910"
  },
  {
    "text": "And then it involves\nplugging theta into our prior distribution\nto get the value out. So what I'm trying to say is\nwe can compute this numerator.",
    "start": "568910",
    "end": "577100"
  },
  {
    "text": "And what's cool is\nthrough this thing called probabilistic programming, we\ncan actually then draw samples",
    "start": "577100",
    "end": "582910"
  },
  {
    "text": "from the posterior. So we can get samples that\nlook like the posterior distribution over theta.",
    "start": "582910",
    "end": "588640"
  },
  {
    "text": "So here's how we do this. I'm not going to dive\ntoo deep into how probabilistic programming works.",
    "start": "588640",
    "end": "595069"
  },
  {
    "text": "But the good news is that\nlater in this course, we actually are going to\ndive into how it works. It uses a method called Markov\nchain Monte Carlo, which we'll",
    "start": "595070",
    "end": "602290"
  },
  {
    "text": "talk about more when we talk\nabout sampling from a failure distribution. But for now, you're\njust going to have",
    "start": "602290",
    "end": "607480"
  },
  {
    "text": "to take my word that\nprobabilistic programming can take evaluations\nof that numerator",
    "start": "607480",
    "end": "612520"
  },
  {
    "text": "and draw samples\nfrom the posterior. So I don't want you\nto get too bogged down with the syntax here.",
    "start": "612520",
    "end": "618800"
  },
  {
    "text": "But essentially\nwhat's going on is we have our Bayesian\nparameter estimator. We input our likelihood model.",
    "start": "618800",
    "end": "625073"
  },
  {
    "text": "So again, it needs\nto be able to compute the terms in this numerator. So we tell it how\nto compute this one.",
    "start": "625073",
    "end": "631029"
  },
  {
    "text": "We input our prior distribution\nso that's this part here. Then we input a sampler here.",
    "start": "631030",
    "end": "637910"
  },
  {
    "text": "And this is-- so there's\na really cool package in Julia called Turing.jl. It does this probabilistic\nprogramming stuff for us.",
    "start": "637910",
    "end": "645260"
  },
  {
    "text": "It has lots of different\nmethods implemented. Super amazing package. If you're interested,\ndefinitely check it out.",
    "start": "645260",
    "end": "651500"
  },
  {
    "text": "We input just one of\nthe types of samplers. They have many different\ntypes of samplers that all could sample\nfrom this posterior, some",
    "start": "651500",
    "end": "657670"
  },
  {
    "text": "with various pros and cons. And so we pick one\nof those and then we say the number\nof samples that we",
    "start": "657670",
    "end": "663670"
  },
  {
    "text": "want to draw from the\nposterior over theta. And then what we can\ndo is actually just",
    "start": "663670",
    "end": "668950"
  },
  {
    "text": "call this fit function\nwhere first it just extracts some data. This is the whole\nprobabilistic programming part.",
    "start": "668950",
    "end": "676927"
  },
  {
    "text": "And so what's cool here\nis we don't actually draw samples in this function. We just say theta\ncomes from this prior.",
    "start": "676927",
    "end": "684260"
  },
  {
    "text": "So we say theta is sampled\nfrom this prior distribution. And then for each\nof our data points,",
    "start": "684260",
    "end": "690699"
  },
  {
    "text": "we say that the y-value\nof the data point was generated via\nour likelihood model.",
    "start": "690700",
    "end": "696110"
  },
  {
    "text": "And this is enough\nto tell Turing what to do in order to actually\nget samples from the posterior.",
    "start": "696110",
    "end": "701930"
  },
  {
    "text": "So again, this might seem like\na little bit like magic to you. It probably should\nbecause if you",
    "start": "701930",
    "end": "707050"
  },
  {
    "text": "don't know the\ninner workings, it's hard to see what's\ngoing on here. But what I really\nwant you to grasp here",
    "start": "707050",
    "end": "712900"
  },
  {
    "text": "is just that if we can\ncompute this numerator, which we can many times, then\nwe can draw samples",
    "start": "712900",
    "end": "719200"
  },
  {
    "text": "from this posterior. So I want to show you\nwhat this looks like because I think it'll help\nof clear up what's going on.",
    "start": "719200",
    "end": "726790"
  },
  {
    "text": "So you can imagine that\nwe were trying to learn the parameters for this model.",
    "start": "726790",
    "end": "733360"
  },
  {
    "text": "And by the way, I\nfigured out the case of the missing axis labels. There's an issue with\nLaTeX displaying properly",
    "start": "733360",
    "end": "740890"
  },
  {
    "text": "that I thought I fixed\nand then realized 20 minutes before lecture that\nI didn't actually fully fix. So we're digging into that.",
    "start": "740890",
    "end": "747079"
  },
  {
    "text": "But sorry, these are still\nsupposed to have labels and it'll be fixed soon.",
    "start": "747080",
    "end": "753100"
  },
  {
    "text": "But either way, let's\nimagine that we're trying to learn the\nparameters of this model we were learning before.",
    "start": "753100",
    "end": "758150"
  },
  {
    "text": "So we have this theta\n1, theta 2, theta 3. And again, if you want to\nhave an example in your mind, you can think back\nto that example",
    "start": "758150",
    "end": "764560"
  },
  {
    "text": "where we're trying to\nestimate the sensor model for that aircraft that\nI showed at the beginning.",
    "start": "764560",
    "end": "771100"
  },
  {
    "text": "And so it turns\nout what we can do is just specify a\nprior distribution over these parameters.",
    "start": "771100",
    "end": "777650"
  },
  {
    "text": "So we're going to\nsay, we don't really know what the parameters are. We don't have a good\nidea ahead of time. And so the prior\ndistribution is just",
    "start": "777650",
    "end": "784720"
  },
  {
    "text": "going to be a big multivariate\nnormal distribution centered at 0 with this big\nvariance to say like,",
    "start": "784720",
    "end": "791290"
  },
  {
    "text": "we give an equal likelihood\nto many different possible parameters.",
    "start": "791290",
    "end": "797410"
  },
  {
    "text": "Then we need to define\nthis sampler, which is what I was mentioning. You don't need to know\nwhat this is right now.",
    "start": "797410",
    "end": "802910"
  },
  {
    "text": "Very popular sampler though\nin probabilistic programming is the No-U-Turn\nSampler, or NUTS.",
    "start": "802910",
    "end": "808520"
  },
  {
    "text": "And then we'll say how many\nsamples we want to draw. So we want to draw a\nthousand samples of theta.",
    "start": "808520",
    "end": "815380"
  },
  {
    "text": "And so then we can\njust run this sampling. It does it for us. You see, it kind of\nran this sampling.",
    "start": "815380",
    "end": "820780"
  },
  {
    "text": "And we can see what comes out. So again, the axis\nlabels are missing. I apologize for that. But you can see, this\nis a samples of theta 1.",
    "start": "820780",
    "end": "829550"
  },
  {
    "text": "So if you look back to our\nmaximum likelihood estimation, we thought theta 1 was around\n0.999, theta 2 was around 0,",
    "start": "829550",
    "end": "837500"
  },
  {
    "text": "and theta 3 was around 0.05. So You can see here\nour expected value",
    "start": "837500",
    "end": "842570"
  },
  {
    "text": "using these samples is 0.98. So we're pretty close. But now we have a\nfull distribution",
    "start": "842570",
    "end": "848900"
  },
  {
    "text": "of possible values\nfor theta rather than just one value for theta 1.",
    "start": "848900",
    "end": "854010"
  },
  {
    "text": "Similarly, for theta\n2, it's centered at 0. So it's centered at what\nwe thought it would be. But again, we have\nthis full distribution",
    "start": "854010",
    "end": "860720"
  },
  {
    "text": "over possible values. And then similarly\nfor the variance.",
    "start": "860720",
    "end": "866330"
  },
  {
    "text": "But one thing that I\ndid here is I actually only gave it 20\nrandomly sampled points",
    "start": "866330",
    "end": "873110"
  },
  {
    "text": "of these pink points here. So I can give it more than that.",
    "start": "873110",
    "end": "879079"
  },
  {
    "text": "So I'm going to give\nit maybe be 100. What do you guys\nthink would happen when I increase the\nnumber of data points",
    "start": "879080",
    "end": "886180"
  },
  {
    "text": "that I give it to\nestimate these parameters? What would happen to the\nposterior distributions?",
    "start": "886180",
    "end": "891626"
  },
  {
    "start": "891626",
    "end": "898540"
  },
  {
    "text": "Get smaller. Yeah, yeah. Exactly. So more data, we have\nthe more confident we should be that we are\ngetting the right parameters.",
    "start": "898540",
    "end": "906740"
  },
  {
    "text": "And so ideally, those\ndistributions should shrink. So let's try that.",
    "start": "906740",
    "end": "914920"
  },
  {
    "text": "Looks like it's running. Didn't shrink yet. Give it a second. ",
    "start": "914920",
    "end": "923079"
  },
  {
    "text": "There you go. So hopefully, you remember\nwhat they looked like before. They're a little\nbit smaller now.",
    "start": "923080",
    "end": "928100"
  },
  {
    "text": "So we're more\ncertain we now have an expected value of 1 still. But our distribution\nis a lot smaller because we have\nmore data points.",
    "start": "928100",
    "end": "934640"
  },
  {
    "text": "And so we're more confident in\nour estimates of the parameters. Maybe for fun, let's\njust do 200 data points",
    "start": "934640",
    "end": "942850"
  },
  {
    "text": "just to drive this point home. ",
    "start": "942850",
    "end": "948250"
  },
  {
    "text": "And while that's running,\nwhat questions do you guys have on this? Yeah.",
    "start": "948250",
    "end": "953560"
  },
  {
    "text": "So I think I'm just\na little bit confused about the likelihood model. So you made the example\nbefore with the airplane.",
    "start": "953560",
    "end": "960290"
  },
  {
    "text": "Would that be an example of\nhow you obtain the likelihood models in real life? Or how can I think about that?",
    "start": "960290",
    "end": "965480"
  },
  {
    "text": "Yeah, that's exactly right. So in that case,\nthe likelihood model would be that\nnormal distribution",
    "start": "965480",
    "end": "971560"
  },
  {
    "text": "that we had that was like\nsaying that y was generated by applying-- or\nh-hat was generated",
    "start": "971560",
    "end": "978100"
  },
  {
    "text": "by applying some function to\nh and then adding some noise. And so if you see--",
    "start": "978100",
    "end": "984190"
  },
  {
    "text": "let me go back\nhere really quick. So if you look at this\npart of the algorithm here,",
    "start": "984190",
    "end": "990110"
  },
  {
    "text": "what we're saying is our y[i]\nwould be that h hat our x[i] is h.",
    "start": "990110",
    "end": "995540"
  },
  {
    "text": "And we're saying, our\nlikelihood is some function that we've implemented\nthat takes h-- in our case,",
    "start": "995540",
    "end": "1000680"
  },
  {
    "text": "it was apply a linear\ntransformation-- takes h and then also adds\nsome noise to it.",
    "start": "1000680",
    "end": "1006709"
  },
  {
    "text": "So it adds like a Gaussian\ndistribution or something like that. And that's how we generate\nour measurement or our h hat.",
    "start": "1006710",
    "end": "1015220"
  },
  {
    "text": "But that relies on the fact that\nyou have a sensor that you trust very much [INAUDIBLE] Is that\nhow it's done in real life",
    "start": "1015220",
    "end": "1023770"
  },
  {
    "text": "as well or-- Yeah, that's a great question. So the question was\nlike, OK, so you need to then have known both h\nlike the true value and the h",
    "start": "1023770",
    "end": "1031020"
  },
  {
    "text": "hat. So how are you going to do this? Yeah, you're exactly right. So you need to have a sensor\nthat you already trust.",
    "start": "1031020",
    "end": "1038030"
  },
  {
    "text": "That's one option. There's various ways to do it. One example could be\nlike, I know for GPS,",
    "start": "1038030",
    "end": "1044868"
  },
  {
    "text": "depending on how\nmuch money you pay or how expensive your\nGPS is, sometimes you can get centimeter accuracy.",
    "start": "1044869",
    "end": "1050555"
  },
  {
    "text": "But that's quite expensive. You probably don't\nwant to run that on all of your maybe\naircraft or you're designing or something like that.",
    "start": "1050555",
    "end": "1056540"
  },
  {
    "text": "And so you'll run the test\nwith that really expensive one just for that one time to see\nhow good your less expensive one",
    "start": "1056540",
    "end": "1064770"
  },
  {
    "text": "is. And then you can build that\nmodel of the less expensive one using that data. But yeah, good question.",
    "start": "1064770",
    "end": "1070540"
  },
  {
    "text": "You've definitely identified\nlike a key challenge in creating these models. ",
    "start": "1070540",
    "end": "1078350"
  },
  {
    "text": "Are there any other questions. Yeah. I don't think I'm fully\nunderstanding how we're kind",
    "start": "1078350",
    "end": "1083640"
  },
  {
    "text": "of ignoring the denominator. Is this just like, I guess, when\nwe create that distribution, is that to create the left\nside of the equation kind of?",
    "start": "1083640",
    "end": "1092610"
  },
  {
    "text": "So it's creating-- so\nwe're drawing samples",
    "start": "1092610",
    "end": "1098160"
  },
  {
    "text": "from this distribution. So we're drawing\nsamples of theta given the data that we have.",
    "start": "1098160",
    "end": "1103440"
  },
  {
    "text": "We don't get to know\nthe density of it. So if we could actually\ncompute this denominator, we could compute\nthis exact function.",
    "start": "1103440",
    "end": "1109240"
  },
  {
    "text": "It turns out all we\ncan do is draw samples. And I haven't really\ntold you how that works. So that's fuzzy in your mind.",
    "start": "1109240",
    "end": "1115030"
  },
  {
    "text": "That's OK. We'll talk more about\nthat later in the class. Yeah. I guess this may be\na syntax question,",
    "start": "1115030",
    "end": "1121419"
  },
  {
    "text": "but I'm trying to understand\nwhat does it mean for us to-- are we asserting that y[i]\ncomes from this distribution,",
    "start": "1121420",
    "end": "1129250"
  },
  {
    "text": "given that we passed it y\nto this function posterior? Or essentially, can you explain\nthe syntax a little bit more?",
    "start": "1129250",
    "end": "1135815"
  },
  {
    "text": "That's exactly right. So this is like the beauty\nof probabilistic programming is we basically just say what\ndistribution everything comes",
    "start": "1135815",
    "end": "1142830"
  },
  {
    "text": "from. And then it's going to\nmake those assumptions. We're saying these\nare our assumptions,",
    "start": "1142830",
    "end": "1147840"
  },
  {
    "text": "and it's going to draw\nfrom the posterior. So this little symbol here,\nwe're just saying theta is distributed\naccording to this.",
    "start": "1147840",
    "end": "1154528"
  },
  {
    "text": "That's the one we're\ngoing to sample. And we gave it a\nbunch of y's and x. And we're saying we're\nmaking the assumption that y",
    "start": "1154528",
    "end": "1160710"
  },
  {
    "text": "came from this distribution. [INAUDIBLE] the\nvariable is not named will be assumed,\nlike postulated.",
    "start": "1160710",
    "end": "1168940"
  },
  {
    "text": "And then if the\nvariable is named is the information\nthat Turing uses to actually make that decision.",
    "start": "1168940",
    "end": "1174760"
  },
  {
    "text": "In this case, we aren't\npassing any data, but we are passing in one. Exactly. Yeah.",
    "start": "1174760",
    "end": "1180100"
  },
  {
    "text": "And there's some more\nnitty gritty details of how the programming\nlanguage works that I would recommend you\ncheck out some tutorials for,",
    "start": "1180100",
    "end": "1187960"
  },
  {
    "text": "but that's the gist\nof what's going on. You have things that you\npass in and you're now making assumptions of\nhow they're distributed.",
    "start": "1187960",
    "end": "1195009"
  },
  {
    "text": "And then it's going to\nsample from all the rest. Yeah. [INAUDIBLE] I mean\nthat's representative",
    "start": "1195010",
    "end": "1203400"
  },
  {
    "text": "of the denominator. Yep. That's exactly right. Thank you. The question was the\nsampling is an approximation",
    "start": "1203400",
    "end": "1209400"
  },
  {
    "text": "of the actual density here. And that's true. ",
    "start": "1209400",
    "end": "1215330"
  },
  {
    "text": "Awesome. So let's go to the next part. So we just told you,\nsometimes we just get to draw these samples.",
    "start": "1215330",
    "end": "1221230"
  },
  {
    "text": "That's all we can do. Sometimes though it is\npossible to actually get an analytical solution\nfor this posterior.",
    "start": "1221230",
    "end": "1228480"
  },
  {
    "text": "So we're going to do\nfinally the Frisbees. We're going to do a\nlittle experiment. I don't really want this volume.",
    "start": "1228480",
    "end": "1234310"
  },
  {
    "text": "Oh, well. All right. What we're going to do this\nthing where we flip frisbees, kind of that guy is doing there. And so I'm going to\nturn that off for now.",
    "start": "1234310",
    "end": "1242140"
  },
  {
    "text": "But basically, every week-- oh, by the way, every week our\nlab plays frisbee on Wednesdays",
    "start": "1242140",
    "end": "1248010"
  },
  {
    "text": "at 4:00 PM at Wrigley Field\nif anyone wants to join. All skill levels welcome. But typically, to start out\nto decide who's on offense",
    "start": "1248010",
    "end": "1256770"
  },
  {
    "text": "and which side of the field\neveryone's going to start on, instead of flipping a\ncoin, we flip frisbees.",
    "start": "1256770",
    "end": "1263485"
  },
  {
    "text": "And typically, we\nhave two people stand and they flip frisbees. And then if they\nland in the same, someone calls,\nsame or different.",
    "start": "1263485",
    "end": "1269350"
  },
  {
    "text": "And then we see if they\nland in the same direction or a different direction. And I have this\nhypothesis that they're",
    "start": "1269350",
    "end": "1275580"
  },
  {
    "text": "more likely to land\nin the same direction. So we're going to\ntest that out today. And so we're going\nto learn this--",
    "start": "1275580",
    "end": "1282341"
  },
  {
    "text": "we're going to\nlearn this parameter that we call theta,\nwhich is going to be the probability\nthat both frisbees",
    "start": "1282342",
    "end": "1287700"
  },
  {
    "text": "land facing the same direction. So then 1 minus theta\nis the probability",
    "start": "1287700",
    "end": "1292970"
  },
  {
    "text": "that the frisbees land\nin different directions. We're going to then want\nto collect some data. So let's say, we collected this\ndata, where we did 10 flips",
    "start": "1292970",
    "end": "1301670"
  },
  {
    "text": "and we got these results. So they landed same, different,\nsame, same, different, different, so on.",
    "start": "1301670",
    "end": "1309049"
  },
  {
    "text": "So we're going to\nsummarize this by saying n is the number\nof times that they landed facing the same way up.",
    "start": "1309050",
    "end": "1315350"
  },
  {
    "text": "And then m is just the\ntotal number of times that we flipped the frisbees.",
    "start": "1315350",
    "end": "1322399"
  },
  {
    "text": "So then if we want to compute\np D given theta, that's going to be theta to the n.",
    "start": "1322400",
    "end": "1327450"
  },
  {
    "text": "So theta is the\nprobability that they landed in the same\ndirection once, the probability that\nit happened n times is",
    "start": "1327450",
    "end": "1334370"
  },
  {
    "text": "going to be theta to the n. And then the number\nof times that they landed in different directions\nis going to be m minus n.",
    "start": "1334370",
    "end": "1341150"
  },
  {
    "text": "And so 1 minus theta\nis the probability they land in\ndifferent directions. So the probability that\nhappens m minus n times",
    "start": "1341150",
    "end": "1346790"
  },
  {
    "text": "is 1 minus theta\nto the m minus n.",
    "start": "1346790",
    "end": "1351800"
  },
  {
    "text": "This is actually called\na binomial distribution. And it turns out that if we--\nso this is our likelihood model.",
    "start": "1351800",
    "end": "1359130"
  },
  {
    "text": "This is p D given theta. We also need to select\np theta in this case.",
    "start": "1359130",
    "end": "1365299"
  },
  {
    "text": "And so it turns out that if we\nselect our prior distribution to have this form called a beta\ndistribution with these two",
    "start": "1365300",
    "end": "1372440"
  },
  {
    "text": "parameters that we\ncall alpha and beta, we'll get some more intuition\nin a second for what those are.",
    "start": "1372440",
    "end": "1378470"
  },
  {
    "text": "Our posterior distribution\nis also a beta distribution. So we get this new\nbeta distribution",
    "start": "1378470",
    "end": "1384500"
  },
  {
    "text": "where these\nparameters are updated using these counts m and n. And again, we'll\nget more intuition",
    "start": "1384500",
    "end": "1390260"
  },
  {
    "text": "in a second for what\nthose look like. But to summarize,\nhere's what happened. So we were trying to estimate\nthis parameter theta, which",
    "start": "1390260",
    "end": "1397789"
  },
  {
    "text": "is the probability of\nboth frisbees landing in the same direction. We say n is the number\nof times that they",
    "start": "1397790",
    "end": "1403370"
  },
  {
    "text": "landed in the same direction\nwhen we took our data set. We started with some prior\ndistribution over theta.",
    "start": "1403370",
    "end": "1409750"
  },
  {
    "text": "And it turns out if we\nhave a prior of this form, we actually can analytically\ncompute the posterior.",
    "start": "1409750",
    "end": "1415660"
  },
  {
    "text": "And specifically,\nit has that form. What questions do\nyou have there?",
    "start": "1415660",
    "end": "1423280"
  },
  {
    "text": "Yeah. So when you introduce\nthe binomial distribution on the previous\nslide, I'm wondering--",
    "start": "1423280",
    "end": "1428970"
  },
  {
    "text": "I'm familiar with the\nprobability mass also including the summation over--",
    "start": "1428970",
    "end": "1435542"
  },
  {
    "text": "[INAUDIBLE] or not summation. Sorry some combination. I think it's like m\nchoose n in front.",
    "start": "1435542",
    "end": "1441375"
  },
  {
    "text": "So I'm wondering, is that--  is that necessary for the\nposterior distribution",
    "start": "1441375",
    "end": "1448290"
  },
  {
    "text": "to be computed as we\nsee on the next slide? ",
    "start": "1448290",
    "end": "1453750"
  },
  {
    "text": "That might be the case. It's at least proportional. [INTERPOSING VOICES] That's the probability of\ngetting that exact sequence.",
    "start": "1453750",
    "end": "1462000"
  },
  {
    "text": "If you just want to figure\nout the probability that you get n the same out of m flips,\nthen you need the m choose",
    "start": "1462000",
    "end": "1473090"
  },
  {
    "text": "whatever [INAUDIBLE] Yeah, I'll just repeat\nthat for people online.",
    "start": "1473090",
    "end": "1478530"
  },
  {
    "text": "So this is the probability that\nwe got this exact sequence. If we just wanted the\nprobability that five of them",
    "start": "1478530",
    "end": "1484190"
  },
  {
    "text": "were the same or\nn were the same, then we would need this extra\ncombination term in the front.",
    "start": "1484190",
    "end": "1490015"
  },
  {
    "start": "1490015",
    "end": "1497000"
  },
  {
    "text": "So just a few notes, I skipped\nover this actual calculation.",
    "start": "1497000",
    "end": "1503000"
  },
  {
    "text": "So if you're like,\nwhere the heck did this beta thing come from? No worries.",
    "start": "1503000",
    "end": "1508350"
  },
  {
    "text": "You can either trust\nme that it's true, or you can check out\nchapter 4 of algorithms",
    "start": "1508350",
    "end": "1513830"
  },
  {
    "text": "for decision making. It's derived there. ",
    "start": "1513830",
    "end": "1520289"
  },
  {
    "text": "So it turns out we\nhave this thing where I said this is our\nlikelihood model is binomial. This is a beta distribution.",
    "start": "1520290",
    "end": "1528150"
  },
  {
    "text": "And if we have a\nbeta distribution with this likelihood model, then\nthe posterior that we get out is also a beta distribution.",
    "start": "1528150",
    "end": "1534630"
  },
  {
    "text": "And it turns out this\nactually happens a lot. And so there's\nthe name for this, which is the conjugate prior.",
    "start": "1534630",
    "end": "1539870"
  },
  {
    "text": "So we say that the\nbeta distribution is the conjugate prior for\na Bernoulli distribution, meaning that the\nposterior distribution is",
    "start": "1539870",
    "end": "1546770"
  },
  {
    "text": "in the same class as the prior. So let's get some\nintuition here.",
    "start": "1546770",
    "end": "1552475"
  },
  {
    "text": " So we're going to\ntry to test this out.",
    "start": "1552475",
    "end": "1558200"
  },
  {
    "text": "I need six volunteers. Michael, do you\nwant to demonstrate",
    "start": "1558200",
    "end": "1564110"
  },
  {
    "text": "how to flip a frisbee? Yes. Ready? Go for it. Here we go.",
    "start": "1564110",
    "end": "1569580"
  },
  {
    "text": "So that landed-- Up. I don't know. I actually don't know how you--",
    "start": "1569580",
    "end": "1575049"
  },
  {
    "text": "If that one also lands in that\nway, we'd say that's the same. OK? Great. So it did.",
    "start": "1575050",
    "end": "1580940"
  },
  {
    "text": "So that's the same. So we're starting here with\nthis prior distribution.",
    "start": "1580940",
    "end": "1586289"
  },
  {
    "text": "That's this beta (1, 1). This is the most uniform\nprior you can start with. So we're saying,\nthis is essentially",
    "start": "1586290",
    "end": "1593210"
  },
  {
    "text": "a uniform distribution. All possible values of\ntheta between 0 and 1. Again, sorry, the\naxis label isn't here",
    "start": "1593210",
    "end": "1598970"
  },
  {
    "text": "but the axis label is theta. So we have equal probability\nfor all possible values.",
    "start": "1598970",
    "end": "1604442"
  },
  {
    "text": "So we're just saying\nwe're totally naive. We don't know what this\nprobability value is. We did just observe that we\ngot 1 that landed the same.",
    "start": "1604442",
    "end": "1613190"
  },
  {
    "text": "So we'll update this. And so we get this\nnew posterior where",
    "start": "1613190",
    "end": "1618200"
  },
  {
    "text": "we added the number same to\nthis alpha term and beta. Oh, no!",
    "start": "1618200",
    "end": "1623210"
  },
  {
    "text": "Oh, no. OK, we're good. Live demos. So we get this new distribution.",
    "start": "1623210",
    "end": "1630813"
  },
  {
    "text": "There's kind of a few\ninteresting things we can think about here. So now we have zero probability\nthat the probability",
    "start": "1630813",
    "end": "1638210"
  },
  {
    "text": "that they land the same is 0\nbecause we just saw it happen. So there's no way that\ntheta is equal to 0.",
    "start": "1638210",
    "end": "1645059"
  },
  {
    "text": "So this is of our\nnew distribution over these probabilities. Do two of you-- you\ntwo want to flip?",
    "start": "1645060",
    "end": "1653150"
  },
  {
    "text": "OK. Oh, different. So we'll enter that one in.",
    "start": "1653150",
    "end": "1658929"
  },
  {
    "text": " I guess we'll get an error\nevery time we do this.",
    "start": "1658930",
    "end": "1666720"
  },
  {
    "text": "So now there's zero probability\nthat the probability of them being the same as 1,\nbecause we've seen both things",
    "start": "1666720",
    "end": "1672830"
  },
  {
    "text": "happen. And we're thinking most likely,\nit's probably around 0.5, but we got this kind of\nwide distribution around it.",
    "start": "1672830",
    "end": "1679040"
  },
  {
    "text": "So now I'm going to ask you\nguys all to go ahead, just with your partner, flip 10 times\nand remember how many were each.",
    "start": "1679040",
    "end": "1686450"
  },
  {
    "text": "Go for it. [INTERPOSING VOICES] ",
    "start": "1686450",
    "end": "1741235"
  },
  {
    "text": "4-6. [INTERPOSING VOICES] 3-7, yeah. 4 same?",
    "start": "1741235",
    "end": "1746420"
  },
  {
    "text": "OK. So we had one that was\n4 same and 6 different.",
    "start": "1746420",
    "end": "1753656"
  },
  {
    "text": " So now we're actually\nthinking that probability",
    "start": "1753656",
    "end": "1759020"
  },
  {
    "text": "seems a little less than 0.5. So I guess my hypothesis\nisn't looking so great.",
    "start": "1759020",
    "end": "1764935"
  },
  {
    "text": "What was your guys's? We had 8 different, 2 same. Oh, wow. OK. 2 the same, and 8.",
    "start": "1764935",
    "end": "1771990"
  },
  {
    "text": " No, 3 same. 3 same.",
    "start": "1771990",
    "end": "1777260"
  },
  {
    "text": "3 same. So now like, actually\nfeeling pretty confident that it's lower than 0.5.",
    "start": "1777260",
    "end": "1784630"
  },
  {
    "text": "Oh, wait. I should have been adding these. What was I doing here? Can I get your numbers again?",
    "start": "1784630",
    "end": "1790070"
  },
  {
    "text": "4-6, so 4 plus 2, 6. And then you had 8 different.",
    "start": "1790070",
    "end": "1796980"
  },
  {
    "text": "And-- How many different, Charles? 6 different. 6 different, so 14. All right.",
    "start": "1796980",
    "end": "1802290"
  },
  {
    "text": "Whoops.  I still think we're going to be\npretty confident it's different.",
    "start": "1802290",
    "end": "1808800"
  },
  {
    "text": "And then the last one? 3 and 7. 3 same. [INTERPOSING VOICES] 7 different. Wow. OK.",
    "start": "1808800",
    "end": "1814310"
  },
  {
    "text": "9 and 21. I've always been\nsaying same, too.",
    "start": "1814310",
    "end": "1820550"
  },
  {
    "text": "No wonder I always lose. I feel like it should be same. I feel like it should be same.",
    "start": "1820550",
    "end": "1826190"
  },
  {
    "text": "I think I heard your hypothesis\nand I've been using it. Well, all right, here you go. Now you've got the\nstrategy if you need it.",
    "start": "1826190",
    "end": "1834240"
  },
  {
    "text": "Thanks, guys. You can come get candy\nfor me after your lecture. ",
    "start": "1834240",
    "end": "1841680"
  },
  {
    "text": "Thanks for your\nparticipation there. So we've now ended up with\nthis distribution, where",
    "start": "1841680",
    "end": "1848870"
  },
  {
    "text": "we have this distribution\nover the probability that they come up same. So it looks like we\nactually kind of feel",
    "start": "1848870",
    "end": "1854330"
  },
  {
    "text": "like it's around 0.3\nthat they're the same. But we have some kind of bound. So my hypothesis of\nsame being more likely,",
    "start": "1854330",
    "end": "1862310"
  },
  {
    "text": "there's still some chance there. And actually, what's cool is\nnow that we have a distribution,",
    "start": "1862310",
    "end": "1867950"
  },
  {
    "text": "we can do things with it. Oh I guess that\nclicking doesn't work.",
    "start": "1867950",
    "end": "1873105"
  },
  {
    "text": "So we could actually look-- I thought this was going\nto be a bigger area here. But we can actually\nlook at, what",
    "start": "1873105",
    "end": "1879200"
  },
  {
    "text": "is the probability that\nit's greater than 0.5? By looking at this distribution\nand finding the probability mass",
    "start": "1879200",
    "end": "1884480"
  },
  {
    "text": "that's greater than 0.5, 1%. All right.",
    "start": "1884480",
    "end": "1890070"
  },
  {
    "text": "Now you guys all know what\nto do on the frisbee field. ",
    "start": "1890070",
    "end": "1897560"
  },
  {
    "text": "Are there any questions on this? Have you updated\nyour priors now?",
    "start": "1897560",
    "end": "1902779"
  },
  {
    "text": "Yeah. I'm going to go a different way. My hypothesis for why I\nthink it should be same,",
    "start": "1902780",
    "end": "1909929"
  },
  {
    "text": "I don't want to spend\na lot of time on this, but if you think about it, if\nthere's anything other than a 50/50 chance that this lands\non one side versus the other,",
    "start": "1909930",
    "end": "1919380"
  },
  {
    "text": "they're more likely\nto be the same. Yeah. You can think about that for\na second or on your own time.",
    "start": "1919380",
    "end": "1927530"
  },
  {
    "text": "But that is why I\nthink that's the case. But that assumes that everything\nis distributed similarly--",
    "start": "1927530",
    "end": "1935292"
  },
  {
    "text": " Yeah, yeah, I think-- I mean, there are very--",
    "start": "1935292",
    "end": "1940732"
  },
  {
    "text": "I can probably tell\nyou more, but there's very specific restrictions on--",
    "start": "1940732",
    "end": "1948620"
  },
  {
    "text": "I think these are all-- These are all just crap. Yeah. Yeah.",
    "start": "1948620",
    "end": "1953840"
  },
  {
    "text": "Theoretically. Maybe we need a\nmodel of the Frisbee. ",
    "start": "1953840",
    "end": "1961970"
  },
  {
    "text": "Question. Yeah. Can you please explain the\nconjugate prior definition one more time?",
    "start": "1961970",
    "end": "1967130"
  },
  {
    "text": "Yeah, so a conjugate\nprior means that when we have some likelihood\nmodel here, so",
    "start": "1967130",
    "end": "1973610"
  },
  {
    "text": "given some distribution\nfor the likelihood model, if we pick a certain\ntype of prior,",
    "start": "1973610",
    "end": "1980240"
  },
  {
    "text": "the posterior will have the\nsame model class or type of distribution as the prior.",
    "start": "1980240",
    "end": "1985880"
  },
  {
    "text": "So in this case, is our\nprior the beta distribution and our likelihood\nis the Bernoulli? Yeah, and then\nour posterior is--",
    "start": "1985880",
    "end": "1993791"
  },
  {
    "text": "Is the beta. Is the beta. I see. So I'm confused then\nwhy we were looking at the binomial distribution.",
    "start": "1993791",
    "end": "2000950"
  },
  {
    "text": "OK, so our likelihood\nof one data point given our\nparameter is Bernoulli.",
    "start": "2000950",
    "end": "2008090"
  },
  {
    "text": "So our likelihood of\njust one time coming up heads, that's a\n[INAUDIBLE] distribution.",
    "start": "2008090",
    "end": "2014344"
  },
  {
    "text": "And it turns out\nthat there's actually lots of conjugate\npriors on distributions with conjugate priors\nout here of binomial.",
    "start": "2014344",
    "end": "2021050"
  },
  {
    "text": "All right. I'll make another\npost clearing this up because I want to make\nsure I get that straight.",
    "start": "2021050",
    "end": "2027610"
  },
  {
    "text": "There's lots of distributions\nwith conjugate priors. If you have a\nmultinomial distribution, you may have heard it's the\nDirichlet distribution, which",
    "start": "2027610",
    "end": "2033915"
  },
  {
    "text": "is Michael's favorite\ndistribution. And Gaussian distributions\nresult in other Gaussian distributions.",
    "start": "2033915",
    "end": "2040960"
  },
  {
    "text": "OK, maybe you've seen\nthis meme before. It's the derivative operator\ntrying to change the x.",
    "start": "2040960",
    "end": "2046419"
  },
  {
    "text": "But it just can't do\nit because e to the x has the same derivative.",
    "start": "2046420",
    "end": "2051460"
  },
  {
    "text": "So here's what I typically\nthink of when I see that meme. ",
    "start": "2051460",
    "end": "2056864"
  },
  {
    "text": "But sometimes we\ndo actually have to make a choice, which is the\npoint that I typically panic because I'm super indecisive.",
    "start": "2056864",
    "end": "2064449"
  },
  {
    "text": "But no worries, there's a few\ndifferent things we can do. So we could just take\nthis distribution and use this\ndistribution over theta",
    "start": "2064449",
    "end": "2071109"
  },
  {
    "text": "and just take the expectation\nand use that as our theta. But if our distribution\nis multimodal,",
    "start": "2071110",
    "end": "2077870"
  },
  {
    "text": "that might not be the best plan. So here in this case, if\nwe take the expectation,",
    "start": "2077870",
    "end": "2083180"
  },
  {
    "text": "we're just going to end up\nwith something here, which is a theta that has a\nvery low likelihood,",
    "start": "2083181",
    "end": "2088369"
  },
  {
    "text": "so probably not the best choice. So the other thing we\ncould do is just take the theta that has the\nhighest likelihood.",
    "start": "2088370",
    "end": "2096129"
  },
  {
    "text": "And this might not be unique. For example, in this\ncase, it's here and here. We call this the maximum\na posteriori estimate.",
    "start": "2096130",
    "end": "2103750"
  },
  {
    "text": "It's just the argmax of\nour posterior distribution. And sometimes this is\nalso called the mode.",
    "start": "2103750",
    "end": "2112494"
  },
  {
    "text": "So that's maximum likelihood\nparameter estimation and Bayesian\nparameter estimation.",
    "start": "2112495",
    "end": "2118140"
  },
  {
    "text": "Are there any questions on that? Yeah.",
    "start": "2118140",
    "end": "2123240"
  },
  {
    "text": "Do you have an example\nof a non-conjugate prior? ",
    "start": "2123240",
    "end": "2129540"
  },
  {
    "text": "I think the-- maybe\nnot on the spot,",
    "start": "2129540",
    "end": "2136470"
  },
  {
    "text": "but there definitely are plenty. I have one. Maybe a triangle\ndensity, if you start off",
    "start": "2136470",
    "end": "2143400"
  },
  {
    "text": "with a prior that's\nshaped like a triangle, then the posterior\nwill not be like that.",
    "start": "2143400",
    "end": "2152280"
  },
  {
    "text": "Yeah. I'll repeat what Michael\nsaid with the motions. If you have a triangle\ndensity, the prior,",
    "start": "2152280",
    "end": "2160480"
  },
  {
    "text": "the posterior might\nnot be like that. OK. So it's just like if the\nprior, like the shape",
    "start": "2160480",
    "end": "2166529"
  },
  {
    "text": "of the prior changes, then\nit's not a conjugate pair.",
    "start": "2166530",
    "end": "2171670"
  },
  {
    "text": "Yeah. So the type of distribution. A lot of times we can't-- the\nsystem might be so complex that",
    "start": "2171670",
    "end": "2177960"
  },
  {
    "text": "we can't even say what the\ntype of distribution is. It doesn't even have a\nname or a model class. And in that case, we need\nto do the previous thing",
    "start": "2177960",
    "end": "2185490"
  },
  {
    "text": "we were doing, which was that\nprobabilistic programming, where we can only draw\nsamples from it.",
    "start": "2185490",
    "end": "2190630"
  },
  {
    "text": "Thank you. Yeah. Yeah. Just to follow up on\nthat then, is the reason why we care about conjugate\npairs I guess because we",
    "start": "2190630",
    "end": "2197579"
  },
  {
    "text": "have an analytical form? Yeah, because we have\nan analytical form. The question was, why do we\ncare about conjugate priors?",
    "start": "2197580",
    "end": "2204600"
  },
  {
    "text": "OK, one last thing on\nparameter learning. I'm going to go through\nthis kind of fast, but we should be careful\nabout generalization.",
    "start": "2204600",
    "end": "2211650"
  },
  {
    "text": "So many of you have\nprobably seen this before. We could very easily\noverfit to a set of data.",
    "start": "2211650",
    "end": "2217950"
  },
  {
    "text": "So that white model here fits\nall of the white data points perfectly.",
    "start": "2217950",
    "end": "2223560"
  },
  {
    "text": "But if we sample more data\nfrom what is likely actually the distribution\nover this data, it's probably not going to fit\nthem very well at all.",
    "start": "2223560",
    "end": "2231510"
  },
  {
    "text": "And so we should be\ncareful about this. And one way to check\nif this is happening",
    "start": "2231510",
    "end": "2236700"
  },
  {
    "text": "is to have a separate\nvalidation set that we don't train the model\non and use that to assess",
    "start": "2236700",
    "end": "2242190"
  },
  {
    "text": "the performance of the model. So that's probably covered\nin lots of other classes. So I'm not going to\nsay too much about it.",
    "start": "2242190",
    "end": "2248339"
  },
  {
    "text": "But essentially, there's\nthe hold out method where you just hold\nout a validation set to check this property.",
    "start": "2248340",
    "end": "2253920"
  },
  {
    "text": "And then there's also a method\ncalled cross validation, where you select different\nsubsets of your data",
    "start": "2253920",
    "end": "2260069"
  },
  {
    "text": "and repeat this\nprocess multiple times. Any questions on that?",
    "start": "2260070",
    "end": "2266670"
  },
  {
    "text": "Yeah. [INAUDIBLE] to the data set\nthat we have for the [INAUDIBLE]",
    "start": "2266670",
    "end": "2272820"
  },
  {
    "text": "for instance? Can you say that one more time? For the k fold cross validation\nmethod that you're essentially",
    "start": "2272820",
    "end": "2278190"
  },
  {
    "text": "going to validate over\nthe entire data set, would that overfit on\nthat entire data set?",
    "start": "2278190",
    "end": "2285210"
  },
  {
    "text": "Well, so what you're doing is\nyou'll only train on this part. And then you'll\nassess on this part.",
    "start": "2285210",
    "end": "2292270"
  },
  {
    "text": "Then you'll just completely\nstart back over again. Only train on this part\nand assess on this part.",
    "start": "2292270",
    "end": "2299350"
  },
  {
    "text": "And then just to get an\nestimation of your validation error, you'll just\naverage all of those. So it's never going\nto be training",
    "start": "2299350",
    "end": "2304727"
  },
  {
    "text": "on stuff that it's seen before. Yeah. I have a question to the\nslide before, actually.",
    "start": "2304727",
    "end": "2311430"
  },
  {
    "text": "So if we do maximum\nlikelihood estimation and we take the maximum\nor we just maximize off",
    "start": "2311430",
    "end": "2319110"
  },
  {
    "text": "the posterior with Bayesian\nparameter estimation, does that give us the same\nparameters or not necessarily?",
    "start": "2319110",
    "end": "2325380"
  },
  {
    "text": "Not necessarily. So if you'll notice, there's\na subtle difference here. It's written a\nlittle differently.",
    "start": "2325380",
    "end": "2330760"
  },
  {
    "text": "But this is pd given theta\nand this is p theta given d. And those maximums are\nnot necessarily the same.",
    "start": "2330760",
    "end": "2338350"
  },
  {
    "text": "So for one example\nfor the Frisbee flipping we were just\ndoing, if we were to just do",
    "start": "2338350",
    "end": "2345180"
  },
  {
    "text": "the maximum likelihood\nestimate, we would just divide number\nsame by total number of times that we flipped.",
    "start": "2345180",
    "end": "2350970"
  },
  {
    "text": "When we do the maximum\na posteriori, actually,",
    "start": "2350970",
    "end": "2356670"
  },
  {
    "text": "do we get the same\nthing in that case? ",
    "start": "2356670",
    "end": "2364920"
  },
  {
    "text": "You don't know? OK, well, I think\nyou get something. I'm not 100% sure. But in general they're\nnot always the same.",
    "start": "2364920",
    "end": "2371700"
  },
  {
    "text": "Good question. OK. So the last step\nafter we train a model",
    "start": "2371700",
    "end": "2378539"
  },
  {
    "text": "is to validate the model. So I'll poke some fun\nat economists here.",
    "start": "2378540",
    "end": "2383940"
  },
  {
    "text": "So maybe you've seen\nthis joke before. But basically,\nthere's a story where a physicist and a\nchemist and an economist",
    "start": "2383940",
    "end": "2390540"
  },
  {
    "text": "are stranded on a desert\nisland, and they only have a can of food. And the physicist\nand the chemist",
    "start": "2390540",
    "end": "2396120"
  },
  {
    "text": "devise these\ningenious mechanisms for getting the can open. And then the\neconomist merely says,",
    "start": "2396120",
    "end": "2401560"
  },
  {
    "text": "assume that we\nhave a can opener. So it's kind of a\njoke for economists, where people say\neconomists just make",
    "start": "2401560",
    "end": "2407700"
  },
  {
    "text": "lots and lots of assumptions. A lot of times it leads to\nthese very useful models that we can use to predict\nthings in the world.",
    "start": "2407700",
    "end": "2415810"
  },
  {
    "text": "But we do need to be careful. And so if we just say\nassume we have a can opener, that's probably not going\nto solve our problems.",
    "start": "2415810",
    "end": "2422945"
  },
  {
    "text": "And that's just to say our\nassumptions don't really mean much if they aren't\ngrounded in reality.",
    "start": "2422945",
    "end": "2428550"
  },
  {
    "text": "And we're creating\nthese models and we're going to use them to get all\nof our downstream validation results. And so if our model\nis wrong, then",
    "start": "2428550",
    "end": "2437880"
  },
  {
    "text": "all of our downstream\nvalidation results aren't really\ngoing to mean much. So it's important that we go\none step further and validate",
    "start": "2437880",
    "end": "2446550"
  },
  {
    "text": "that the model we\njust created actually matches what we were\ntrying to model.",
    "start": "2446550",
    "end": "2451650"
  },
  {
    "text": "To do this, we basically want\nto compare features of the model to features of our data to\nsee how well they match.",
    "start": "2451650",
    "end": "2458970"
  },
  {
    "text": "So one way we\ncould do this is we could compare the probability\ndensity of our data to our model.",
    "start": "2458970",
    "end": "2464830"
  },
  {
    "text": "So here we're showing\na histogram of the data and then the probability density\nof the model that we chose.",
    "start": "2464830",
    "end": "2471853"
  },
  {
    "text": "We want to make sure\nif we're doing this that we normalize the histogram\nsuch that the area under it is equal to 1, so we\ncan make sure we're",
    "start": "2471853",
    "end": "2478560"
  },
  {
    "text": "making a fair comparison of two\nvalid probability densities. And once we do that, we\ncan just check visually",
    "start": "2478560",
    "end": "2485400"
  },
  {
    "text": "how well they fit. So the one on the left\nhere is a good fit. They seem to match\nup pretty well. Whereas, the purple\nmodel on the right",
    "start": "2485400",
    "end": "2492240"
  },
  {
    "text": "doesn't seem to\nfit quite as well. We could also do this for\nthe cumulative densities.",
    "start": "2492240",
    "end": "2499510"
  },
  {
    "text": "So you can imagine kind\nof sweeping from left to right across this plot. And as we do that, we're\naccumulating probability mass",
    "start": "2499510",
    "end": "2509310"
  },
  {
    "text": "that's under this distribution\nand checking how those match up. So that looks\nsomething like this. ",
    "start": "2509310",
    "end": "2518250"
  },
  {
    "text": "And so you can see\nfor this bad fit, the CDF plots don't\nmatch up very well. And then for this better fit,\nthey match up almost perfectly.",
    "start": "2518250",
    "end": "2528180"
  },
  {
    "text": "We can also do something that\nwe call a quantile quantile plot or QQ plot. So the way this works is\nwe pick some value, alpha.",
    "start": "2528180",
    "end": "2535690"
  },
  {
    "text": "Let's for now say\nalpha equals 0.1. And then we look over here\nand we look at our data",
    "start": "2535690",
    "end": "2542070"
  },
  {
    "text": "and we say, what is the\nalpha quantile of our data? So what is the value for\nwhich 10% of our data",
    "start": "2542070",
    "end": "2547770"
  },
  {
    "text": "is to the left of that value. We figure out that value. Then we look at our\nmodel and we say,",
    "start": "2547770",
    "end": "2553313"
  },
  {
    "text": "what is the alpha\nquantile of our model? So what is the\nvalue for our model for which 10% of the probability\nmass is to the left of it?",
    "start": "2553313",
    "end": "2559950"
  },
  {
    "text": "And we get this value here. And so then we take these two\nvalues, translate them over",
    "start": "2559950",
    "end": "2565740"
  },
  {
    "text": "to this plot, where\nwe put the model one on the x-axis and the\ntrue one on the vertical axis.",
    "start": "2565740",
    "end": "2571870"
  },
  {
    "text": "And that forms one point on\nour quantile quantile plot. And so then we can do\nthis for a whole bunch",
    "start": "2571870",
    "end": "2578370"
  },
  {
    "text": "of different values of alpha. And then we get a whole bunch of\ndifferent points on this plot.",
    "start": "2578370",
    "end": "2583870"
  },
  {
    "text": "And you can imagine that\nif the distributions were close to each other, the\nquantiles should roughly",
    "start": "2583870",
    "end": "2588940"
  },
  {
    "text": "match up. So we expect this to\nfollow the line y equals x. So you can see for\nthis kind of bad fit",
    "start": "2588940",
    "end": "2595029"
  },
  {
    "text": "it's deviating from this line. ",
    "start": "2595030",
    "end": "2601270"
  },
  {
    "text": "There's a very similar type\nof plot called a calibration plot, where instead of\nplotting the q's we're just",
    "start": "2601270",
    "end": "2606310"
  },
  {
    "text": "plotting the alphas instead. So what we do is we\nsay, for example, let's go to the 0.2\nquantile of the model.",
    "start": "2606310",
    "end": "2613520"
  },
  {
    "text": "So we can go over here, find\nwhere the 0.2 quantile is, and then look at our data and\nsay, what quantile of the data",
    "start": "2613520",
    "end": "2619870"
  },
  {
    "text": "is that? So maybe instead, that's like\nthe 0.08 quantile or something. So that's how we\nget this point here.",
    "start": "2619870",
    "end": "2626540"
  },
  {
    "text": "We do that for all the\npoints across this axis. And we get some line like this. And you can imagine same deal.",
    "start": "2626540",
    "end": "2633309"
  },
  {
    "text": "These kind of quantiles\nshould match up if the distributions match well. So we want it to follow\nthe line y equals x.",
    "start": "2633310",
    "end": "2642480"
  },
  {
    "text": "So here's just a full\ncomparison of all of these different visual\ndiagnostics for one",
    "start": "2642480",
    "end": "2648150"
  },
  {
    "text": "that fits well, where it\ndoes follow these lines here, and then one that doesn't\nhave quite as good of a fit.",
    "start": "2648150",
    "end": "2655768"
  },
  {
    "text": "Is that a question? Yeah. Is there a fit to using the Q\nQ plot versus the calibration",
    "start": "2655768",
    "end": "2661049"
  },
  {
    "text": "plot, or is it just like\n[? they're telling you ?] the same? They're giving you\nthe same information, or is there a reason\nwhy you would prefer",
    "start": "2661050",
    "end": "2667950"
  },
  {
    "text": "to use one to convey something? Yeah, that's a great question. So the question was,\nis there a benefit",
    "start": "2667950",
    "end": "2673830"
  },
  {
    "text": "to using the q-q plot\nversus the calibration plot. They're showing\nvery similar things. And they'll both show if\nsomething's going wrong.",
    "start": "2673830",
    "end": "2682380"
  },
  {
    "text": "Sometimes, depending on\nhow your data is formatted, it's easier to calculate\nthe calibration plot.",
    "start": "2682380",
    "end": "2687823"
  },
  {
    "text": "And sometimes it's also just\nkind of different communities and different places use\ndifferent versions of the plot.",
    "start": "2687823",
    "end": "2694360"
  },
  {
    "text": "So in machine learning,\nit's very, very common to see these calibration plots.",
    "start": "2694360",
    "end": "2700020"
  },
  {
    "text": "Yeah. Yeah. Are you doing this with\nonly the valid data, right?",
    "start": "2700020",
    "end": "2708070"
  },
  {
    "text": "Great question. So the question\nwas, would you do this with the validation data? I mean, you could\ndo it with any data.",
    "start": "2708070",
    "end": "2714700"
  },
  {
    "text": "But given what we\nwere saying, you should likely want to do\nthis with a validation set",
    "start": "2714700",
    "end": "2720060"
  },
  {
    "text": "or a test set, a\nseparate set of data than you trained the\nmodel on to ensure that you're avoiding\nthat overfitting",
    "start": "2720060",
    "end": "2726359"
  },
  {
    "text": "that we were talking\nabout earlier. So yeah, good question. OK.",
    "start": "2726360",
    "end": "2731480"
  },
  {
    "text": "So those are visual diagnostics. Sometimes we just\nwant to write down a number of how\ngood we're doing.",
    "start": "2731480",
    "end": "2737100"
  },
  {
    "text": "And so we can do that\nusing some summary metrics. One summary metric for the PDF\nplot, or the probability density",
    "start": "2737100",
    "end": "2744180"
  },
  {
    "text": "plot, is the KL divergence. So maybe you've\nheard of this before.",
    "start": "2744180",
    "end": "2750030"
  },
  {
    "text": "It kind of gives us a\nmeasure of how close two PDFs are to one another.",
    "start": "2750030",
    "end": "2755730"
  },
  {
    "text": "And so the good fit will\nhave a low KL divergence and the bad fit will have a\nmuch higher KL divergence.",
    "start": "2755730",
    "end": "2765180"
  },
  {
    "text": "Just some notes here,\nthe KL divergence is part of a much broader\nclass of divergences called F divergences.",
    "start": "2765180",
    "end": "2771430"
  },
  {
    "text": "You can read more\nabout them separately, if you're interested. Another interesting\nthing about KL divergence",
    "start": "2771430",
    "end": "2777270"
  },
  {
    "text": "is it's not symmetric,\nand it's also not defined if the support of p is not a\nsubset of the support of q.",
    "start": "2777270",
    "end": "2782890"
  },
  {
    "text": "So I just want to make\nsure you're aware of those. If you go to use\nKL divergence, you should make these\nconsiderations.",
    "start": "2782890",
    "end": "2788710"
  },
  {
    "text": "But for the most\npart, I just want you to understand that\nKL divergence allows us to understand in one single\nmetric how different two",
    "start": "2788710",
    "end": "2796830"
  },
  {
    "text": "densities are. ",
    "start": "2796830",
    "end": "2802680"
  },
  {
    "text": "OK, another metric could be for\nthe CDF plot, or the Cumulative Distribution Plot.",
    "start": "2802680",
    "end": "2808530"
  },
  {
    "text": "This metric is called\nthe KS statistic. And you can look\nat this formula. But really all\nthat's going on here",
    "start": "2808530",
    "end": "2814590"
  },
  {
    "text": "is we're saying\nthat we want to know what is the maximum gap or the\nmaximum distance between the two",
    "start": "2814590",
    "end": "2820440"
  },
  {
    "text": "CDFs that we're\ntrying to compare. So here it would look\nsomething like this. ",
    "start": "2820440",
    "end": "2828960"
  },
  {
    "text": "We can also summarize\nthe calibration plot. So again, these are metrics\nthat you'll very often see in machine learning.",
    "start": "2828960",
    "end": "2835500"
  },
  {
    "text": "So we could do something\nlike the maximum calibration error, which would be\nthe maximum distance between our calibration plot and\nthe line y equals x, and then",
    "start": "2835500",
    "end": "2846690"
  },
  {
    "text": "the expected\ncalibration error, which would just be the average\nover all of these distances.",
    "start": "2846690",
    "end": "2854460"
  },
  {
    "text": "OK. All right. So what if we want to\ncompare multiple features?",
    "start": "2854460",
    "end": "2860050"
  },
  {
    "text": "So here we were just\ntalking about comparing one feature of the model\nto one feature of the data.",
    "start": "2860050",
    "end": "2865890"
  },
  {
    "text": "But what if we want\nto compare multiple? So what these two\nplots are showing is it's the same data\nset in both plots.",
    "start": "2865890",
    "end": "2874109"
  },
  {
    "text": "But these are two\ndifferent possible models for this data set showing\nthe contours of them.",
    "start": "2874110",
    "end": "2879433"
  },
  {
    "text": "So just looking at these, which\nof these two models do you think fits the data better,\nthe left or the right?",
    "start": "2879433",
    "end": "2885844"
  },
  {
    "text": "Right. Right, yeah This one looks\nmuch closer to the data. However, if we only compare\none feature at a time,",
    "start": "2885844",
    "end": "2893960"
  },
  {
    "text": "so imagine that\nwe took this plot and we just looked at\nall of the x values. And we just kind of\nmarginalized over that",
    "start": "2893960",
    "end": "2900250"
  },
  {
    "text": "and made a histogram of\njust all the x values. And similarly, we\ntook the same thing and did that with our model.",
    "start": "2900250",
    "end": "2906080"
  },
  {
    "text": "It actually matches\nup pretty well. And if we do a similar thing\nwith all of the y values,",
    "start": "2906080",
    "end": "2911390"
  },
  {
    "text": "it again matches up pretty\nwell, and then same thing for the plot over here. And so if we were to just\nlook at these features",
    "start": "2911390",
    "end": "2918520"
  },
  {
    "text": "individually, just look\nat these marginals, we wouldn't actually notice\nthat this model is not doing such a great job.",
    "start": "2918520",
    "end": "2924532"
  },
  {
    "text": "And the reason for that is\nbecause we're not actually considering the interactions\nbetween features when we do this.",
    "start": "2924532",
    "end": "2932079"
  },
  {
    "text": "So there's a couple\nways to actually try to capture these interactions. One thing you could\ndo is just hand",
    "start": "2932080",
    "end": "2937490"
  },
  {
    "text": "design a single feature that\ncombines multiple features in a way that actually captures\nthe relationship between them.",
    "start": "2937490",
    "end": "2945080"
  },
  {
    "text": "So for this case, one thing\nyou could imagine doing is kind of drawing this\nline across and then taking",
    "start": "2945080",
    "end": "2952070"
  },
  {
    "text": "each data point and\nprojecting it onto that line. So now we've made a single\nfeature, one dimension.",
    "start": "2952070",
    "end": "2959240"
  },
  {
    "text": "And if we do that and\nmake the histogram and compare these\nmodels, then we can see that the\nblue model does not",
    "start": "2959240",
    "end": "2964640"
  },
  {
    "text": "fit nearly as well\nas the purple one. Yeah. [INAUDIBLE]",
    "start": "2964640",
    "end": "2970610"
  },
  {
    "text": "Sure. The question was, can\nI repeat what happened? So you can imagine we drew a\nline, maybe the line y equals x.",
    "start": "2970610",
    "end": "2980099"
  },
  {
    "text": "We took each data point and we\nprojected it onto that line. And then we were now\nplotting that new data set.",
    "start": "2980100",
    "end": "2988710"
  },
  {
    "text": "And we did the\nsame for our model. And we can see now that this\nmodel doesn't match quite as well as the purple one.",
    "start": "2988710",
    "end": "2994075"
  },
  {
    "text": " OK, there's one other option.",
    "start": "2994075",
    "end": "2999912"
  },
  {
    "text": "So some of the summary metrics\nthat we were talking about you can actually extend to\nmultiple dimensions. So for example,\nthe KL divergence",
    "start": "2999913",
    "end": "3006430"
  },
  {
    "text": "I was talking about you can\napply in multiple dimensions. So if we just do it\nin one dimension,",
    "start": "3006430",
    "end": "3011589"
  },
  {
    "text": "again, we think that these\ndistributions match quite well. But in two dimensions we can see\nthat this purple distribution",
    "start": "3011590",
    "end": "3018160"
  },
  {
    "text": "has a much lower KL\ndivergence than the blue one. ",
    "start": "3018160",
    "end": "3027730"
  },
  {
    "text": "And then finally, just really\nquickly I want to talk about, what if you have a\nconditional distribution that you want to compare?",
    "start": "3027730",
    "end": "3034390"
  },
  {
    "text": "The kind of most general\nthing that you can do here is you just partition the\nconditioning variable into bins.",
    "start": "3034390",
    "end": "3041119"
  },
  {
    "text": "So say we have py given x. We just divide up x and\nthen just say, OK, x is",
    "start": "3041120",
    "end": "3047019"
  },
  {
    "text": "the same in all of these bins. And we just make our comparisons\nin each bin individually.",
    "start": "3047020",
    "end": "3052480"
  },
  {
    "text": "So if you're going\nto do this, you'd probably want to use\nmore than four bins. We just did that\nfor a visualization, but you want to keep the bins\nsmall so that x doesn't really",
    "start": "3052480",
    "end": "3060280"
  },
  {
    "text": "have an effect anymore,\nand you're just comparing what's going on\nin the distribution over y.",
    "start": "3060280",
    "end": "3065770"
  },
  {
    "text": "And so then in each\nbin you could just compute the various\nstatistics we talked about for these non-conditional\nvariables.",
    "start": "3065770",
    "end": "3072760"
  },
  {
    "text": "And then check how\nthey're matching up. Yeah. So in this case, our\nconditional distribution model",
    "start": "3072760",
    "end": "3080200"
  },
  {
    "text": "is continuous with x. And if we do a\ndiscrete [INAUDIBLE] over a range of x,\nwhich choice of x",
    "start": "3080200",
    "end": "3087520"
  },
  {
    "text": "do we choose then for\nthe model that we use for computing these metrics?",
    "start": "3087520",
    "end": "3092859"
  },
  {
    "text": "Oh, OK. Good question. So the question was, this is\nlike continuous value of x.",
    "start": "3092860",
    "end": "3098000"
  },
  {
    "text": "And now we have to\ndecide what is our model. And our model is\nconditioned on x.",
    "start": "3098000",
    "end": "3103220"
  },
  {
    "text": "So we need to input some\nvalue into our model. So in this case,\nwe're showing two data sets that we're just comparing\ntwo data sets instead of a model",
    "start": "3103220",
    "end": "3110560"
  },
  {
    "text": "to a data set, so we didn't\nhave to make that choice. If you did have to compare\na model to a data set,",
    "start": "3110560",
    "end": "3116200"
  },
  {
    "text": "you would just have to pick\nsomething sensible, so maybe the center of the\nbin or something",
    "start": "3116200",
    "end": "3121420"
  },
  {
    "text": "like that, which\nis why you probably want to use more\nthan four bins such that your bins are small enough\nto the point where changing",
    "start": "3121420",
    "end": "3127900"
  },
  {
    "text": "x doesn't have a huge effect. OK.",
    "start": "3127900",
    "end": "3133060"
  },
  {
    "text": "Don't have too much time left. All right, so we\ncould also validate models using expert knowledge.",
    "start": "3133060",
    "end": "3139750"
  },
  {
    "text": "So if we don't have a\nlot of data, some of you may have heard of\nthe Turing test. So this is a\nTuring-like test, where",
    "start": "3139750",
    "end": "3146530"
  },
  {
    "text": "we would show an expert\npairs of rollouts from the model and some\nrollouts from the data,",
    "start": "3146530",
    "end": "3153380"
  },
  {
    "text": "and then we ask them to tell us\nwhich one came from the model and which one came\nfrom the data. And if they can\nfigure out which one",
    "start": "3153380",
    "end": "3159520"
  },
  {
    "text": "came from which\n100% of the time, we probably didn't do a\ngood job of making the model",
    "start": "3159520",
    "end": "3164560"
  },
  {
    "text": "look like the data, because they\ncan tell that they're different. So if they get a high accuracy,\nwe say it failed the test.",
    "start": "3164560",
    "end": "3171140"
  },
  {
    "text": "We have a poor model\nrepresentation. If they get a low\naccuracy, so maybe they",
    "start": "3171140",
    "end": "3177730"
  },
  {
    "text": "can only distinguish\nit 50% of the time, then we can say that our model\nis pretty good, because they're",
    "start": "3177730",
    "end": "3184217"
  },
  {
    "text": "not really able to\ntell the difference between the model and the data. So some of you may have heard\nof the Turing test, which",
    "start": "3184217",
    "end": "3189490"
  },
  {
    "text": "is kind of doing this\nat a large scale with AI and figuring out if you\ncan tell the difference between an AI and a human.",
    "start": "3189490",
    "end": "3194760"
  },
  {
    "text": " OK, so that's system modeling.",
    "start": "3194760",
    "end": "3201735"
  },
  {
    "text": " We're going to talk about\nproperty specification next.",
    "start": "3201735",
    "end": "3208339"
  },
  {
    "text": "So like I said, this\nis the second input to our validation model. And what we want to\ndo is just formally",
    "start": "3208340",
    "end": "3214000"
  },
  {
    "text": "specify what it is a\nsystem is supposed to do. So there's different\nways that we can",
    "start": "3214000",
    "end": "3220300"
  },
  {
    "text": "describe properties of systems. And what we're trying\nto do essentially is just describe the behavior\nof a system in some way",
    "start": "3220300",
    "end": "3227140"
  },
  {
    "text": "that we can quantify. So one way to do that is\nto map system behavior to a real number.",
    "start": "3227140",
    "end": "3234160"
  },
  {
    "text": "We call something that\ndoes that a metric. And another thing we can do\nis map it to a Boolean value.",
    "start": "3234160",
    "end": "3241040"
  },
  {
    "text": "And we call that\na specification.  Sometimes we can go back\nand forth between the two.",
    "start": "3241040",
    "end": "3248390"
  },
  {
    "text": "So sometimes we'll derive\nspecifications from metrics. So you could imagine, for\nexample, an aircraft collision",
    "start": "3248390",
    "end": "3254980"
  },
  {
    "text": "avoidance system,\nwhere our metric is the miss distance\nbetween our aircraft",
    "start": "3254980",
    "end": "3260559"
  },
  {
    "text": "and the intruder aircraft,\nso this distance here. And then we could actually\njust derive a specification",
    "start": "3260560",
    "end": "3266529"
  },
  {
    "text": "from this metric by saying\nthe miss distance must be greater than 50 meters.",
    "start": "3266530",
    "end": "3271850"
  },
  {
    "text": "So the metric, this\nis a real value. It's that actual miss distance. And then the specification\nmaps that by saying it's",
    "start": "3271850",
    "end": "3279730"
  },
  {
    "text": "true or false, depending on\nwhether our miss distance is greater than 50 meters. ",
    "start": "3279730",
    "end": "3290800"
  },
  {
    "text": "Sometimes we can also derive\nmetrics from specifications. So we took this. We made it into a specification.",
    "start": "3290800",
    "end": "3296990"
  },
  {
    "text": "We can actually take\nit one step further and turn it back into\na metric by saying, we want to know the probability\nthat the miss distance is",
    "start": "3296990",
    "end": "3303940"
  },
  {
    "text": "greater than 50 meters. So what is the probability that\nthe specification is satisfied?",
    "start": "3303940",
    "end": "3310900"
  },
  {
    "text": "Any questions on that? The idea that all\nspecifications have to be fully",
    "start": "3310900",
    "end": "3316000"
  },
  {
    "text": "and evaluable, so\njust pure true/false? Yeah. The question was, is the\nidea that all specifications",
    "start": "3316000",
    "end": "3322660"
  },
  {
    "text": "have to be Boolean,\npure true or false? For this course, yes.",
    "start": "3322660",
    "end": "3328270"
  },
  {
    "text": "This is terminology that\nwe've coined for the book. The definitions vary.",
    "start": "3328270",
    "end": "3333535"
  },
  {
    "text": "Yeah.  OK, so we can also have\nmetrics or specifications",
    "start": "3333535",
    "end": "3341920"
  },
  {
    "text": "on single trajectories or\nfull sets of trajectories. So for example,\nthe miss distance was a metric on a\nsingle trajectory.",
    "start": "3341920",
    "end": "3349270"
  },
  {
    "text": "So by the way, this\nplot here, we're plotting the relative\naltitude of the two aircraft on the vertical axis and the\ntime to collision or the time",
    "start": "3349270",
    "end": "3357849"
  },
  {
    "text": "to loss of horizontal separation\non the horizontal axis for the aircraft. So what this is saying is as our\ntime to collision approaches 0,",
    "start": "3357850",
    "end": "3367160"
  },
  {
    "text": "we're losing all of our\nhorizontal separation. So we want to make\nsure that we have this big vertical\nseparation at that time",
    "start": "3367160",
    "end": "3373210"
  },
  {
    "text": "to ensure that we don't collide. And so we'll take\nthe miss distance when we've lost all of\nour horizontal separation.",
    "start": "3373210",
    "end": "3378920"
  },
  {
    "text": "So when the time\nto collision is 0, we don't want it to be\nwithin this red region, meaning that we got too\nclose to the other aircraft.",
    "start": "3378920",
    "end": "3387820"
  },
  {
    "text": "If we have multiple\ntrajectories, we could try to compute\nthe average miss distance. So that's what's going on here.",
    "start": "3387820",
    "end": "3393990"
  },
  {
    "text": " That's properties of systems. So we have metrics and\nwe have specifications.",
    "start": "3393990",
    "end": "3400220"
  },
  {
    "text": "And the plan for today or\nprobably part of Thursday is to go through\nwhat are metrics,",
    "start": "3400220",
    "end": "3406280"
  },
  {
    "text": "what are specifications. We'll talk very briefly about\nreachability specifications.",
    "start": "3406280",
    "end": "3412819"
  },
  {
    "text": "But that's actually going\nto be an advanced topic for this course. So it won't be on the quizzes.",
    "start": "3412820",
    "end": "3419500"
  },
  {
    "text": "But let's go ahead\nand dive into metrics. In this course, we're\nmostly focused on metrics for stochastic system.",
    "start": "3419500",
    "end": "3425372"
  },
  {
    "text": "So you've probably\nnoticed we talk a lot about probability here. And so we really want metrics\nthat summarize distributions.",
    "start": "3425373",
    "end": "3433390"
  },
  {
    "text": "So when we were computing that\naverage miss distance before, that was the miss distance over\nthis whole kind of distribution",
    "start": "3433390",
    "end": "3439870"
  },
  {
    "text": "over trajectories. So you can imagine we took our\ncollision avoidance system. We varied all of\nits initial states,",
    "start": "3439870",
    "end": "3447160"
  },
  {
    "text": "et cetera, the various\nnoise we apply. And we get a whole distribution\nover different trajectories",
    "start": "3447160",
    "end": "3453460"
  },
  {
    "text": "that might occur. And we want to summarize this\ndistribution in some way.",
    "start": "3453460",
    "end": "3458890"
  },
  {
    "text": "So we have all of\nthese trajectories. Maybe we have a metric over\nan individual trajectory,",
    "start": "3458890",
    "end": "3464090"
  },
  {
    "text": "such as the miss distance. And then that, if we apply\nthat to every single trajectory in this distribution, gives us\na distribution over this metric.",
    "start": "3464090",
    "end": "3472819"
  },
  {
    "text": "So it gives us a distribution\nover miss distances. And our goal is to\nbasically come up",
    "start": "3472820",
    "end": "3478390"
  },
  {
    "text": "with some nice, concise ways\nto summarize this distribution. So one thing we can do is take\nits expected value, so maybe",
    "start": "3478390",
    "end": "3485260"
  },
  {
    "text": "the first thing you\nmight think to do to summarize a distribution. So its expected value is\nrepresented like this.",
    "start": "3485260",
    "end": "3491299"
  },
  {
    "text": "You can think of it kind\nof the mean, average. It turns out that if your metric\nis a binary metric, so that",
    "start": "3491300",
    "end": "3498400"
  },
  {
    "text": "means it's either a 0\nor a 1, the expectation will be a probability.",
    "start": "3498400",
    "end": "3505240"
  },
  {
    "text": "More on that in chapter seven. But it turns out the\nexpected value is often not",
    "start": "3505240",
    "end": "3511900"
  },
  {
    "text": "enough to just summarize\na distribution. So for example, all three\nof these distributions over miss distance, while they\nlook different from one another,",
    "start": "3511900",
    "end": "3520279"
  },
  {
    "text": "they all have the exact\nsame expected value. So another thing\nyou could do is take",
    "start": "3520280",
    "end": "3525730"
  },
  {
    "text": "their variance, which\nkind of measures the spread of the distribution. And that would distinguish these\ndistributions from one another.",
    "start": "3525730",
    "end": "3535725"
  },
  {
    "text": " Another thing is that\nspecific to this course",
    "start": "3535725",
    "end": "3543369"
  },
  {
    "text": "sort of, in safety\ncritical settings, we often want metrics that\nare kind of conservative, that",
    "start": "3543370",
    "end": "3548920"
  },
  {
    "text": "consider these worst case\noutcomes because if safety really matters, then we're\nreally focused on those worst",
    "start": "3548920",
    "end": "3554800"
  },
  {
    "text": "case scenarios or edge cases. So in this case, a lot of times\nwe're talking about these things",
    "start": "3554800",
    "end": "3560869"
  },
  {
    "text": "we call risk metrics, which is\njust a metric for which higher values indicate worse outcomes.",
    "start": "3560870",
    "end": "3566840"
  },
  {
    "text": "So more risk is bad. So one example would be if\nwe flip the metric, that",
    "start": "3566840",
    "end": "3573783"
  },
  {
    "text": "miss distance metric\nthat we were using, if we flip that around. So say we always start with\n2000 meters of separation",
    "start": "3573783",
    "end": "3579410"
  },
  {
    "text": "and our loss of\nseparation is then 2000 meters minus\nthis miss distance. So if we have a 0 miss distance,\nwe lost all of our separation.",
    "start": "3579410",
    "end": "3588680"
  },
  {
    "text": "We lost all 2000 meters\nAnd so that's bad. And then the higher\nthe miss distance,",
    "start": "3588680",
    "end": "3594520"
  },
  {
    "text": "the lower this metric will\nbe, so the less risky. So all we did was just take the\nmetric we were using before,",
    "start": "3594520",
    "end": "3600800"
  },
  {
    "text": "flip it around such\nthat higher is bad. And now we can see we still have\nthe same expected value for all",
    "start": "3600800",
    "end": "3607000"
  },
  {
    "text": "of these distributions. But we can\ndistinguish their risk using a few other\ntypes of metrics.",
    "start": "3607000",
    "end": "3613960"
  },
  {
    "text": "So if we just look at these\ndistributions, this one to me is the most risky because if I\nsaid if our loss of separation",
    "start": "3613960",
    "end": "3622569"
  },
  {
    "text": "is 2000 meters, that means\nwe got all the way down to 0. We had no separation anymore. And so these are\nthe things that are",
    "start": "3622570",
    "end": "3628780"
  },
  {
    "text": "going to result in collisions. And this one has kind of\nthe highest probability mass at these values near 2000.",
    "start": "3628780",
    "end": "3635357"
  },
  {
    "text": "This one's a little lower. And this one doesn't have quite\nas much probability mass there. So this is our most risky.",
    "start": "3635357",
    "end": "3641060"
  },
  {
    "text": "And this is our least risky. And one metric that we could\nuse to try to distinguish that is something called the\nValue At Risk, or the VAR.",
    "start": "3641060",
    "end": "3649359"
  },
  {
    "text": "It comes with a value\ncalled alpha that we pick. And the value at risk is\nbasically the highest value",
    "start": "3649360",
    "end": "3656560"
  },
  {
    "text": "that the risk is\nguaranteed not to exceed with probability alpha.",
    "start": "3656560",
    "end": "3661960"
  },
  {
    "text": "So it's kind of actually\njust exactly equivalent to the alpha quantile\nof the risk metric.",
    "start": "3661960",
    "end": "3667610"
  },
  {
    "text": "And you can imagine\nthat the higher that this value is, kind\nof the more risky scenario",
    "start": "3667610",
    "end": "3673000"
  },
  {
    "text": "that we're in. So a more risky scenario will\nhave a high value at risk and a least risky scenario\nwill have a low value at risk.",
    "start": "3673000",
    "end": "3683619"
  },
  {
    "text": "Another metric we can use is\ncalled the Conditional Value At Risk, or the CVAR, also\ncomes with a parameter",
    "start": "3683620",
    "end": "3690009"
  },
  {
    "text": "that we call alpha. And I don't want\nto get too bogged down by this equation here.",
    "start": "3690010",
    "end": "3696440"
  },
  {
    "text": "It's basically just the average\nof all of the risk values that are above the VAR. So it's just taking the\nexpected value, the expectation,",
    "start": "3696440",
    "end": "3703930"
  },
  {
    "text": "of all of the values\nthat are above the VAR. So in a similar way, it's the\nmean of the top alpha quantile",
    "start": "3703930",
    "end": "3712300"
  },
  {
    "text": "of risks. And so in a similar way, this\nwould represent a high CVAR",
    "start": "3712300",
    "end": "3718060"
  },
  {
    "text": "and a lower risk,\nleast risky scenario would represent a low CVAR.",
    "start": "3718060",
    "end": "3724135"
  },
  {
    "text": "And before I pause\nfor questions, I just want to give some\nintuition for what these are because I think when I\nfirst heard of these quantities,",
    "start": "3724135",
    "end": "3730268"
  },
  {
    "text": "they just seemed a little\nout there, kind of weird. So just some intuition here. So the value at\nrisk specifically",
    "start": "3730268",
    "end": "3737710"
  },
  {
    "text": "we set, for example,\nalpha equal to 0.7 is the value that the\nrisk is guaranteed not",
    "start": "3737710",
    "end": "3744490"
  },
  {
    "text": "to exceed with probability 0.7. So we're saying we know\nwith probability 0.7",
    "start": "3744490",
    "end": "3751690"
  },
  {
    "text": "that for sure the\nrisk is not going to be higher than this value.",
    "start": "3751690",
    "end": "3756880"
  },
  {
    "text": "And so if we can say\nthat value is low, we can feel good about it. And the higher it is, kind\nof the worse the situation.",
    "start": "3756880",
    "end": "3764369"
  },
  {
    "text": "For conditional value at\nrisk, one thing we could do is just say we have this\ndistribution over risk metrics.",
    "start": "3764370",
    "end": "3771279"
  },
  {
    "text": "We could just take\nthis expected value. But if we're being\nmore conservative here, we're basically saying, I don't\njust want the expected value,",
    "start": "3771280",
    "end": "3778450"
  },
  {
    "text": "I want the expected value\nof the worst case outcomes. And so the higher this value is,\nkind of the worse the situation",
    "start": "3778450",
    "end": "3786990"
  },
  {
    "text": "that we're in. Any questions on these? Yeah.",
    "start": "3786990",
    "end": "3792420"
  },
  {
    "text": "So what are the worst\ncase outcomes, the ones that are [INAUDIBLE]\na specification? Or how do you define that?",
    "start": "3792420",
    "end": "3798600"
  },
  {
    "text": "The question was, what are\nthe worst case outcomes? So let me go back here.",
    "start": "3798600",
    "end": "3805390"
  },
  {
    "text": "So we're assuming that\nwe have a distribution over our possible outcomes. And we're assuming that\nbecause it's a risk metric,",
    "start": "3805390",
    "end": "3812620"
  },
  {
    "text": "higher means it's worse. And so the worst\ncase outcomes are going to be the ones\nthat are the highest.",
    "start": "3812620",
    "end": "3818495"
  },
  {
    "text": " Does that mean like anything\nthat's above the expected value?",
    "start": "3818495",
    "end": "3827070"
  },
  {
    "text": "So for the CVAR,\nwe take anything that's above the value at risk\nas the worst case outcome.",
    "start": "3827070",
    "end": "3832120"
  },
  {
    "text": "And let me go over\nto a demo real quick because that might\nkind of clear this up. So we have this kind\nof parameter alpha",
    "start": "3832120",
    "end": "3839430"
  },
  {
    "text": "that we can control here. So I've been talking\nabout it being 0.7.",
    "start": "3839430",
    "end": "3844660"
  },
  {
    "text": "But this essentially controls\nhow conservative we want to be. So if alpha is 0.7,\nwe're saying we",
    "start": "3844660",
    "end": "3850860"
  },
  {
    "text": "want to focus on the\n30% worst case outcomes. So as we increase\nalpha, you can see",
    "start": "3850860",
    "end": "3857849"
  },
  {
    "text": "that we focus even\nmore on the tails there, the true\nworst case outcomes.",
    "start": "3857850",
    "end": "3863530"
  },
  {
    "text": "And as alpha approaches\n1, we're just focusing on the\nabsolute worst thing that could possibly happen.",
    "start": "3863530",
    "end": "3871770"
  },
  {
    "text": "As alpha approaches\n0, the VAR is going to focus or\nmove all the way",
    "start": "3871770",
    "end": "3878070"
  },
  {
    "text": "to the left of the distribution. Can anyone guess what the CVAR\nis going to be when alpha is 0?",
    "start": "3878070",
    "end": "3885450"
  },
  {
    "text": "Expectation? Yes. The expectation because\nwhen alpha is 0, we're considering all possible\nvalues of the distribution.",
    "start": "3885450",
    "end": "3891460"
  },
  {
    "text": "We're starting at the\n0 quantile and looking at 100% of the values\nof the distribution.",
    "start": "3891460",
    "end": "3896830"
  },
  {
    "text": "And so now we're\ntaking the expectation of all values, which is just the\nexpectation of the distribution.",
    "start": "3896830",
    "end": "3904920"
  },
  {
    "text": "So that's how these go. So as I was dragging\nthis around, the solid purple\nline is the VAR.",
    "start": "3904920",
    "end": "3911730"
  },
  {
    "text": "The shaded region is\nthe part that we're considering for the CVAR. And then the dashed\npurple line is the CVAR.",
    "start": "3911730",
    "end": "3922170"
  },
  {
    "text": "So I'll just drag\nthis a few more times so you can get the idea. ",
    "start": "3922170",
    "end": "3930510"
  },
  {
    "text": "All right, so composite\nmetrics as the next thing. So we were just talking\nabout one particular metric. We were just looking\nat loss of separation.",
    "start": "3930510",
    "end": "3938220"
  },
  {
    "text": "But what if we care\nabout multiple things? And oftentimes, these\nmultiple things we care about are at odds with each other.",
    "start": "3938220",
    "end": "3945010"
  },
  {
    "text": "So continuing with this\ncollision avoidance example, maybe we care about the alert\nrate, so the number of times",
    "start": "3945010",
    "end": "3950370"
  },
  {
    "text": "that the collision avoidance\nsystem tells the pilot to take some maneuver to\navoid another aircraft,",
    "start": "3950370",
    "end": "3956100"
  },
  {
    "text": "so the fraction of times it\nsays to do that and then maybe the collision rate, so\nthe fraction of encounters",
    "start": "3956100",
    "end": "3961380"
  },
  {
    "text": "that result in a collision. And ideally, we want\nboth of these to be low. So we don't want to alert the\npilot too often because they",
    "start": "3961380",
    "end": "3968187"
  },
  {
    "text": "could get kind of annoyed and\nnot want to do these maneuvers. But we also\nobviously, don't want to have a high collision\nrate because that",
    "start": "3968187",
    "end": "3975270"
  },
  {
    "text": "can be very dangerous. And so one tool\nwe often use when",
    "start": "3975270",
    "end": "3980490"
  },
  {
    "text": "we talk about these\ntypes of trade offs is something called\nPareto optimality. And the idea here is that a\nsystem design is Pareto optimal",
    "start": "3980490",
    "end": "3988540"
  },
  {
    "text": "if we can't improve. One of these metrics\nwithout making another one of\nthese metrics worse.",
    "start": "3988540",
    "end": "3995980"
  },
  {
    "text": "So let's look at\nthis point here. Do we think that this\npoint is Pareto optimal?",
    "start": "3995980",
    "end": "4002339"
  },
  {
    "text": "No. Yeah. OK, I see some shaking heads. No, it's not. We could get a\npoint, for example,",
    "start": "4002340",
    "end": "4008140"
  },
  {
    "text": "this one that has both a lower\nalert rate and a lower collision rate.",
    "start": "4008140",
    "end": "4014099"
  },
  {
    "text": "What about this point here? Yeah. OK, I see some\nshaking heads yes.",
    "start": "4014100",
    "end": "4020599"
  },
  {
    "text": "Yeah. This point is Pareto optimal. We can't make, for\nexample, the alert rate",
    "start": "4020600",
    "end": "4026790"
  },
  {
    "text": "better without making\nthe collision rate worse. ",
    "start": "4026790",
    "end": "4032700"
  },
  {
    "text": "And then all points that\nare Pareto optimal trace out this thing that we\ncall a Pareto frontier.",
    "start": "4032700",
    "end": "4040650"
  },
  {
    "text": "And so the Pareto\nfrontier is just the subset of all designs\nthat are Pareto optimal.",
    "start": "4040650",
    "end": "4047430"
  },
  {
    "text": "And what composite\nmetrics allow us to do is basically create\na single metric that allows us to select one of these\npoints on the Pareto frontier",
    "start": "4047430",
    "end": "4056000"
  },
  {
    "text": "because just by looking at\nour metrics individually, we can't really distinguish\nwhich one is better. ",
    "start": "4056000",
    "end": "4063890"
  },
  {
    "text": "So one type of\ncomposite metric is just a weighted sum of all of\nour individual metrics.",
    "start": "4063890",
    "end": "4070470"
  },
  {
    "text": "So we assign weights,\nrelative weights to how much we care\nabout each metric.",
    "start": "4070470",
    "end": "4076230"
  },
  {
    "text": "So how much do we care about\ncollision rate in relation to alert rate? And then we just\ntake the weighted sum",
    "start": "4076230",
    "end": "4082100"
  },
  {
    "text": "to get our composite metric. So for example here, if we\nhad a weight vector that",
    "start": "4082100",
    "end": "4087110"
  },
  {
    "text": "said 0.8 is how much we\ncare about collision rate compared to 0.2 is how much\nwe care about alert rate.",
    "start": "4087110",
    "end": "4094790"
  },
  {
    "text": "We could compute that metric\nfor each point on the Pareto frontier, and then select\nthe one that was the highest.",
    "start": "4094790",
    "end": "4101035"
  },
  {
    "text": " Another type of metric is\na goal distance metric.",
    "start": "4101035",
    "end": "4108389"
  },
  {
    "text": "So here what we say is\nwe have some goal point. A lot of times we select\nit to be something that we call the utopia\npoint, which is what we",
    "start": "4108390",
    "end": "4115490"
  },
  {
    "text": "would want in an ideal setting. So ideally, we alert none of the\ntime and we have no collisions.",
    "start": "4115490",
    "end": "4123200"
  },
  {
    "text": "But this is going to be\nimpossible to achieve. But we want to get as\nclose as possible to it.",
    "start": "4123200",
    "end": "4129960"
  },
  {
    "text": "So we just compute the distance\nfrom each point on the Pareto frontier to this utopia point.",
    "start": "4129960",
    "end": "4135810"
  },
  {
    "text": "And we pick the one that\nhas the minimum distance. And then you might\nbe thinking, well,",
    "start": "4135810",
    "end": "4142410"
  },
  {
    "text": "can I combine weights and\nalso this goal metric? Yes, you can. There's the weighted\nexponential sum metric,",
    "start": "4142410",
    "end": "4148278"
  },
  {
    "text": "which basically just\nkind of adds weights to this type of metric.",
    "start": "4148279",
    "end": "4154549"
  },
  {
    "text": "How do we pick the weights? Preference elicitation,\nwe're going to elicit some\ncandy preferences,",
    "start": "4154550",
    "end": "4160399"
  },
  {
    "text": "but we're going to\ndo it next time. ",
    "start": "4160399",
    "end": "4168000"
  }
]