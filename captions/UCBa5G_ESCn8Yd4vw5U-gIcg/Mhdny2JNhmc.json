[
  {
    "text": " So we're getting to the\nend of, this is lecture 6.",
    "start": "0",
    "end": "10059"
  },
  {
    "text": "Next week is GPUs\nand GPU programming, and lecture 8, which would\nbe data parallel programming",
    "start": "10060",
    "end": "18300"
  },
  {
    "text": "and data parallel thinking. And then there'll be a bit\nof a switch in the class. So I do a bunch of software\nperformance optimization",
    "start": "18300",
    "end": "26640"
  },
  {
    "text": "lectures for the\nfirst eight lectures. Then Kunal is going to\ncome in and tell you a little bit about hardware\nand things like that.",
    "start": "26640",
    "end": "33190"
  },
  {
    "text": "So that's where we're going. OK, so if last\ntime, if on Tuesday,",
    "start": "33190",
    "end": "40330"
  },
  {
    "text": "Tuesday was about scheduling. So we talked about\ndifferent strategies for if I have a\nnumber of threads",
    "start": "40330",
    "end": "46230"
  },
  {
    "text": "or a number of processors and I\nhad a bunch of work to do, what are some basic\ntechniques to make",
    "start": "46230",
    "end": "52230"
  },
  {
    "text": "sure that the workload is\ndistributed evenly onto all the workers without\nincurring too much overhead",
    "start": "52230",
    "end": "57270"
  },
  {
    "text": "in that distribution? Today is going to be adding\nin the extra thinking of it's",
    "start": "57270",
    "end": "63600"
  },
  {
    "text": "not just about good\nworkload balance. It often is about\nreducing communication",
    "start": "63600",
    "end": "71080"
  },
  {
    "text": "and synchronization overhead. So today is going to be a\nlot more about communication. So I'm going to talk about\nreducing cost of communication",
    "start": "71080",
    "end": "79900"
  },
  {
    "text": "between processors. Again, techniques that you, as\na software developer, might use. And then at the end,\nif there's time,",
    "start": "79900",
    "end": "85880"
  },
  {
    "text": "I'd like to go into\nsome general program optimization tips that\nare not necessarily relevant to your assignments\nor anything like that.",
    "start": "85880",
    "end": "92390"
  },
  {
    "text": "But now that you have\nso much background, it might be a good idea to\ntalk about that kind of stuff.",
    "start": "92390",
    "end": "97690"
  },
  {
    "text": "So far in this course, and\nin all of the assignments",
    "start": "97690",
    "end": "102922"
  },
  {
    "text": "basically, that you'll\ndo in this course, we've assumed that\nall of the processors are connected to some\nshared memory system.",
    "start": "102922",
    "end": "110800"
  },
  {
    "text": "Or in other words, there\nwas one address space and all threads\ncould read and write",
    "start": "110800",
    "end": "117550"
  },
  {
    "text": "to variables in\nthat address space. And I've given you\nsome hints about, even though that's\nconceptually simple,",
    "start": "117550",
    "end": "124000"
  },
  {
    "text": "the actual underlying\nimplementation of the ability\nfor all processors",
    "start": "124000",
    "end": "129160"
  },
  {
    "text": "to read and write\nto all variables is actually pretty complex. For example, if you have your\nmulti-core CPU, that address",
    "start": "129160",
    "end": "137950"
  },
  {
    "text": "space where the data\nis, all of the data might be stored out in DRAM,\nbut lots of copies of that data",
    "start": "137950",
    "end": "143830"
  },
  {
    "text": "are stored in your\nvarious caches. And the first thing Kunal is\ngoing to talk about next week",
    "start": "143830",
    "end": "150850"
  },
  {
    "text": "is what starts happening--\nor at the end of next week-- is what starts happening this\ncore makes a copy of some data",
    "start": "150850",
    "end": "158530"
  },
  {
    "text": "and this core makes a\ncopy of the same address and they both write. Now all of a sudden, we've\ngot a real challenge,",
    "start": "158530",
    "end": "165190"
  },
  {
    "text": "because I have different\nparties in the system writing to the same address. There might be different\nvalues of these copies",
    "start": "165190",
    "end": "172150"
  },
  {
    "text": "and that can become a\nbig mess really quickly. So we'll tell you a little bit\nabout how modern systems keep",
    "start": "172150",
    "end": "177700"
  },
  {
    "text": "everything coherent. If I take a standard\nIntel CPU these days,",
    "start": "177700",
    "end": "185060"
  },
  {
    "text": "it has a couple of cores. There's a GPU on the same thing. There's a network on this chip\nthat connects all those cores",
    "start": "185060",
    "end": "193360"
  },
  {
    "text": "and gets them all out to\nthe same memory system. And these networks\ncan be quite complex.",
    "start": "193360",
    "end": "199820"
  },
  {
    "text": "So for example, in\nIntel architectures, all of the various processors\nare connected together",
    "start": "199820",
    "end": "206440"
  },
  {
    "text": "and to memory\nactually, by a ring. So we just think about it as\nload and store to the value X.",
    "start": "206440",
    "end": "214560"
  },
  {
    "text": "But load and store the value\nX from a processor here-- this is like the four cores--",
    "start": "214560",
    "end": "220390"
  },
  {
    "text": "is actually sending a request\nout on this ring that ultimately gets routed out to two memory.",
    "start": "220390",
    "end": "226030"
  },
  {
    "text": "And in different parts on this\nring, if it's a cache hit, it might actually go to one\nof the pieces of the cache",
    "start": "226030",
    "end": "232333"
  },
  {
    "text": "and get the data\nout of the cache. It can be quite a\ncomplicated thing.",
    "start": "232333",
    "end": "238633"
  },
  {
    "text": "Actually, one thing that's kind\nof interesting is did you notice that all of the different cores\nare connected to-- or if you",
    "start": "238633",
    "end": "244600"
  },
  {
    "text": "think about this as being a core\nplus, it's a piece of the L3-- notice how everything is\nconnected to the ring twice?",
    "start": "244600",
    "end": "249795"
  },
  {
    "text": " Do you have any idea\nwhy they do that?",
    "start": "249795",
    "end": "256088"
  },
  {
    "text": "Is that to reduce the\nlatency [INAUDIBLE]? Well, first of all yeah, it is.",
    "start": "256089",
    "end": "262880"
  },
  {
    "text": "And first of all, it\nmakes things simple if you're always\nsending messages, always in clockwise order.",
    "start": "262880",
    "end": "268000"
  },
  {
    "text": "So things like deadlock and\nstuff are not much of a problem. And if you want to reduce\nlatency, if you get hit, if you're touching, if\nyou have two contact points,",
    "start": "268000",
    "end": "275800"
  },
  {
    "text": "exactly. Like if you're going to\ngo to your left neighbor, your right neighbor, and\nyou can only send right, you can actually get\nthere a little bit faster.",
    "start": "275800",
    "end": "284770"
  },
  {
    "text": "Here's another processor. It's actually a processor\nthat [INAUDIBLE]",
    "start": "284770",
    "end": "289840"
  },
  {
    "text": "was a big part of the initial\ndesign of this technology. This was done by Sun, at\nthe time when Sun existed.",
    "start": "289840",
    "end": "299890"
  },
  {
    "text": "Then it got bought\nby Oracle, later. But the UltraSPARC. And this was one of the first\nmainstream multithreaded chips.",
    "start": "299890",
    "end": "306650"
  },
  {
    "text": "So it had, in this\ncase, 8 cores. And each of those cores\nhad a bunch of threads,",
    "start": "306650",
    "end": "313080"
  },
  {
    "text": "but all of those cores\nwere connected out to the main memories via\nwhat's a crossbar switch.",
    "start": "313080",
    "end": "319710"
  },
  {
    "text": "The CCX is crossbar. And a crossbar means that every\ncore is actually physically wired to every other one.",
    "start": "319710",
    "end": "325420"
  },
  {
    "text": "It's like N squared\nwires for N cores. And what's kind\nof interesting is if you look at the chip\ndiagram, the actual area",
    "start": "325420",
    "end": "331940"
  },
  {
    "text": "the footprint of\na processor core is about the same as the\nfootprint of the network.",
    "start": "331940",
    "end": "338690"
  },
  {
    "text": "So like these networks that\nget high bandwidth connection between all the processors\nare extremely expensive",
    "start": "338690",
    "end": "344390"
  },
  {
    "text": "and extremely complex. And even in a simple--",
    "start": "344390",
    "end": "351860"
  },
  {
    "text": "by the way, if I go\nto this, it actually means that depending\non what core you're at,",
    "start": "351860",
    "end": "358040"
  },
  {
    "text": "an L3 cache can actually hit,\nbe a different cost based on what the address is\nand where it actually",
    "start": "358040",
    "end": "364430"
  },
  {
    "text": "is and which of these\nslices of the L3 cache. So this is like a sharded\ncache for different addresses",
    "start": "364430",
    "end": "370520"
  },
  {
    "text": "go in different places. Another example of\nit is if you just go out and buy a\nmotherboard that",
    "start": "370520",
    "end": "376280"
  },
  {
    "text": "has two sockets in it\nfor two physical CPUs. ",
    "start": "376280",
    "end": "382230"
  },
  {
    "text": "Let's say there's two\ndifferent four core CPUs, there's an on chip network\nthat ring connecting the cores,",
    "start": "382230",
    "end": "389189"
  },
  {
    "text": "the one I showed you. And then there are traces out\nover the dual socket board.",
    "start": "389190",
    "end": "397310"
  },
  {
    "text": "And a load or a store from\ncore one to address X in that",
    "start": "397310",
    "end": "402770"
  },
  {
    "text": "diagram might be a lot faster\nor a good bit faster than a load and store from, let's\nsay, core 8 to address X.",
    "start": "402770",
    "end": "409610"
  },
  {
    "text": "Regardless of the caching\nbehavior or things like that. So in a modern system, as\nwe get bigger and bigger.",
    "start": "409610",
    "end": "415440"
  },
  {
    "text": "And this is incredibly true\nin these big, a modern GPU and things like that.",
    "start": "415440",
    "end": "421400"
  },
  {
    "text": "It's not just, if you think\nabout how close something is, it's not just it's either\nin memory or it's in cache.",
    "start": "421400",
    "end": "427140"
  },
  {
    "text": "There's actually a lot\nmore nuance to that. We don't necessarily think\nabout when we are programming",
    "start": "427140",
    "end": "433182"
  },
  {
    "text": "because our minds would\nexplode, but if you actually really wanted to\noptimize things, you would be aware\nof that placement.",
    "start": "433183",
    "end": "440680"
  },
  {
    "text": "So even though we like\nto just basically think in terms of a computer as a\nbig pile of cores connected",
    "start": "440680",
    "end": "446740"
  },
  {
    "text": "to a shared memory, those\nloads and stores on how the different\nthreads communicate,",
    "start": "446740",
    "end": "451970"
  },
  {
    "text": "those can have very,\nvery different costs. And if you're elite,\nyou're actually",
    "start": "451970",
    "end": "457300"
  },
  {
    "text": "asking where is this\naddress in this machine? And you might be doing different\nthings based on those costs.",
    "start": "457300",
    "end": "464260"
  },
  {
    "text": "Now, one way to think\nabout communication and make it a little bit\neasier to reason about",
    "start": "464260",
    "end": "471550"
  },
  {
    "text": "is to think about\nother designs where moving data is more explicit.",
    "start": "471550",
    "end": "477680"
  },
  {
    "text": "So I want to talk about a\ndifferent model of computing for a second, which is\ncalled message passing, which",
    "start": "477680",
    "end": "482845"
  },
  {
    "text": "is something all of you\nare used to if you've written any distributed\nprograms in a web setting. It's not like if\nyou're on the internet,",
    "start": "482845",
    "end": "489169"
  },
  {
    "text": "you just say, I want the\ndata at this memory address and all computers\non the internet have access to a\nunified address space.",
    "start": "489170",
    "end": "496090"
  },
  {
    "text": "When we communicate in\na distributed system, we do it via sending messages. Maybe it's an HTTP GET\nor a POST or something",
    "start": "496090",
    "end": "503270"
  },
  {
    "text": "like that or a response. But the name of the game is I\nhave two different computers or two different threads,\nand each thread, keyword now,",
    "start": "503270",
    "end": "511610"
  },
  {
    "text": "works in its own address space. So address X in thread\none's address space",
    "start": "511610",
    "end": "519169"
  },
  {
    "text": "is not the same address as\naddress X in thread two's address space. They are different\naddress spaces.",
    "start": "519169",
    "end": "526100"
  },
  {
    "text": "And the only way to exchange\ninformation-- like every thread can load and store two variables\nin its own address space--",
    "start": "526100",
    "end": "532050"
  },
  {
    "text": "but the only way to exchange\ninformation across these threads is to explicitly send a message.",
    "start": "532050",
    "end": "538170"
  },
  {
    "text": "So in this case, rather than\nan HTTP request, I said, look, let's just\nabstractly say, thread one",
    "start": "538170",
    "end": "544340"
  },
  {
    "text": "is going to send the contents\nof its address X to thread 2",
    "start": "544340",
    "end": "550310"
  },
  {
    "text": "and tag the message\nwith an ID so that the receiver on the other\nside knows what it's getting.",
    "start": "550310",
    "end": "557510"
  },
  {
    "text": "And correspondingly,\nthread 2 is going to post a receive\nwhich says, I want",
    "start": "557510",
    "end": "562940"
  },
  {
    "text": "to receive the message with\nthis ID from thread one. And when we get the data\nwe're going to put that data",
    "start": "562940",
    "end": "571160"
  },
  {
    "text": "and store it to as the\ncontents of the address Y in my own address space.",
    "start": "571160",
    "end": "577460"
  },
  {
    "text": "So these messages say,\nhow do I identify data in my address space?",
    "start": "577460",
    "end": "582630"
  },
  {
    "text": "Who is it going to go to? And is there an abstract\nID that the other side can know to listen to\nand stuff like that?",
    "start": "582630",
    "end": "589920"
  },
  {
    "text": "So in a message passing\nsetting, like the real world way of thinking about it is if a\nshared memory is like a bulletin",
    "start": "589920",
    "end": "596779"
  },
  {
    "text": "board where anybody can post\na message without asking, and anybody can read that\nmessage without passing, message passing is a little bit\nmore like sending snail mail.",
    "start": "596780",
    "end": "603889"
  },
  {
    "text": "Like you put up, you wrap\nup your data in an envelope, you address it to a\nparticular location.",
    "start": "603890",
    "end": "609360"
  },
  {
    "text": "And then someone is responsible\nfor getting it to that location. Question? Yeah.",
    "start": "609360",
    "end": "614500"
  },
  {
    "text": "So where this messaging\nthrough a network",
    "start": "614500",
    "end": "619580"
  },
  {
    "text": "of wires similar\nto the internet, for example, where\nyou're sending packets of data with an address,\nthe source and destination.",
    "start": "619580",
    "end": "626580"
  },
  {
    "text": "Yeah. And any message passing\nsystem is basically going to have some\nmechanism of addressing,",
    "start": "626580",
    "end": "632210"
  },
  {
    "text": "like this is where\nthis needs to go. And the differences\nbetween is it done at the scale\nof the internet?",
    "start": "632210",
    "end": "638100"
  },
  {
    "text": "Is it done at the scale of\ninside a single computer with threads? Or is it maybe done at the scale\nof a small cluster of computers",
    "start": "638100",
    "end": "646310"
  },
  {
    "text": "or at large scale with a big\nrack or something like that? There might be different\nmechanisms for how",
    "start": "646310",
    "end": "651710"
  },
  {
    "text": "we identify the recipient. There might be\ndifferent mechanisms for how the data moves, right?",
    "start": "651710",
    "end": "656850"
  },
  {
    "text": "Is it TCP, IP, is it UDP? Like there are many\ndifferent implementations",
    "start": "656850",
    "end": "662210"
  },
  {
    "text": "of how to get the\nmessage to the location. But conceptually, the difference\nis in a shared address space",
    "start": "662210",
    "end": "668870"
  },
  {
    "text": "system, we all talk\nabout the same addresses and we all have direct\naccess to read and write.",
    "start": "668870",
    "end": "676340"
  },
  {
    "text": "In a message passing system, we\nall operate in our own address spaces and we send messages.",
    "start": "676340",
    "end": "682829"
  },
  {
    "text": "Meaning I say, hey, here's\nsome data, go make a copy of it and put it in your\naddress space.",
    "start": "682830",
    "end": "689990"
  },
  {
    "text": "And I think this will\nbecome more explicit if we go back to this example\nthat we talked about last time.",
    "start": "689990",
    "end": "695250"
  },
  {
    "text": "Which again, to\nre-remind everybody is the workload was\nfor every red cell,",
    "start": "695250",
    "end": "704990"
  },
  {
    "text": "update the value of the\nred cell given read access to the surrounding neighbors.",
    "start": "704990",
    "end": "710840"
  },
  {
    "text": "And if we haven't\nconverged, basically, do it again but this time\nwith the black cells. And I'd like you to think about\nimplementing this program on,",
    "start": "710840",
    "end": "722070"
  },
  {
    "text": "let's say, not a\ntwo core machine, but let's think\nabout it as a cluster",
    "start": "722070",
    "end": "727160"
  },
  {
    "text": "with two different\ncomputers that can only exchange messages like internet\ntraffic or something like that.",
    "start": "727160",
    "end": "734100"
  },
  {
    "text": "Let's say, over Ethernet. So here's my new kind\nof simple computer.",
    "start": "734100",
    "end": "739190"
  },
  {
    "text": "I have a processor with its\nown memory, its own memory DRAM, which implements\nits own address space.",
    "start": "739190",
    "end": "746490"
  },
  {
    "text": "And I have some\nnetwork, whether it be the internet,\nwhether it be ethernet, whether it be carrier pigeon.",
    "start": "746490",
    "end": "753949"
  },
  {
    "text": "I have some way\nto get information from one memory to the other. OK.",
    "start": "753950",
    "end": "760029"
  },
  {
    "text": "And yeah, this is\njust to reiterate, like I only have these two\noperations, send and receive.",
    "start": "760030",
    "end": "768120"
  },
  {
    "text": "So here's what I'm going\nto do is, first of all, now let's imagine that\nI want to first of all,",
    "start": "768120",
    "end": "776390"
  },
  {
    "text": "I have to divide all the data. I have to partition the grid\nacross these processors.",
    "start": "776390",
    "end": "782440"
  },
  {
    "text": "And before, it was like\nI had one big array. Like when I was, we did\nthis assignment one, there was one big array.",
    "start": "782440",
    "end": "788630"
  },
  {
    "text": "And then the code of the program\nsaid, hey, thread 0 or thread 1, you should touch\nthese addresses.",
    "start": "788630",
    "end": "794649"
  },
  {
    "text": "And thread 2, you should\ntouch these addresses. So now things are a\nlittle bit different. We don't have a\nshared address space.",
    "start": "794650",
    "end": "801010"
  },
  {
    "text": "So now every thread or every\nmachine in this cluster-- and all of a sudden I\njump to four machines",
    "start": "801010",
    "end": "807250"
  },
  {
    "text": "just to make it a\nlittle bit easier-- has its own copy of\npart of the array.",
    "start": "807250",
    "end": "813170"
  },
  {
    "text": "There are four\ndifferent allocations now in these four\ndifferent address spaces that hold the data that\neach worker is responsible for.",
    "start": "813170",
    "end": "823310"
  },
  {
    "text": "So you see that\ndifference, conceptually? Before, we had a\nshared allocation, we all just kind of got our own,\nwe touched our own part of it.",
    "start": "823310",
    "end": "831210"
  },
  {
    "text": "Now we have four\ndifferent address spaces. There is no way for thread 3\nin this diagram to directly say",
    "start": "831210",
    "end": "838490"
  },
  {
    "text": "load a value here. In the same way that if you have\na computer in your dorm and I have a computer in my office, I\ncannot issue a load store from",
    "start": "838490",
    "end": "846230"
  },
  {
    "text": "my computer that stores\ndata to your computer. Now to compute,\nlet's say if I wanted",
    "start": "846230",
    "end": "855139"
  },
  {
    "text": "to compute the value\nof this element here, what\ninformation do I need?",
    "start": "855140",
    "end": "861200"
  },
  {
    "text": "Well, I need my left neighbor. I need my right neighbor,\nand I need the neighbor below me, which is fine,\nbecause thread 3 has",
    "start": "861200",
    "end": "867530"
  },
  {
    "text": "all this information. But I also need information\nthat currently is stored in an address space\nthat I cannot access.",
    "start": "867530",
    "end": "875750"
  },
  {
    "text": "OK. All right. So what almost all of these\nsystems in a message passing",
    "start": "875750",
    "end": "882550"
  },
  {
    "text": "do is they say, well, I need\nto be able to access this data, but I can't because it's\nnot in my address space.",
    "start": "882550",
    "end": "889610"
  },
  {
    "text": "So I'm going to make a copy\nof it and keep a copy of it here in my address space.",
    "start": "889610",
    "end": "895070"
  },
  {
    "text": "So what I'm actually doing\nnow is on each of the nodes, I'm actually over allocating.",
    "start": "895070",
    "end": "901450"
  },
  {
    "text": "I'm allocating an extra\nrow and an extra row above and below my section.",
    "start": "901450",
    "end": "907810"
  },
  {
    "text": "And I'm going to need to\nask my neighbors to send me, via message, the values that\nI should store in this row.",
    "start": "907810",
    "end": "916460"
  },
  {
    "text": "And if you look at my code\non the side of the diagram, this is the logic that\nevery thread is executing.",
    "start": "916460",
    "end": "923860"
  },
  {
    "text": "And notice that it\nallocates a buffer that is two rows larger than the\ndata that it is responsible for.",
    "start": "923860",
    "end": "932260"
  },
  {
    "text": "So some terminology would be\nlike, for example, in thread 2, there's an extra over\nallocation that corresponds",
    "start": "932260",
    "end": "940010"
  },
  {
    "text": "to holding a copy of this data. So this extra over\nallocation, where",
    "start": "940010",
    "end": "945860"
  },
  {
    "text": "you're storing data that\nthread doesn't own or is not responsible for\nupdating, you'll often",
    "start": "945860",
    "end": "952190"
  },
  {
    "text": "hear that's like ghost rows\nor ghost cells, ghost values. That's a common thing you'll\nsee in scientific computing.",
    "start": "952190",
    "end": "958940"
  },
  {
    "text": "OK, now let's take a look\nat a full implementation of one iteration of\nthe solver application,",
    "start": "958940",
    "end": "966860"
  },
  {
    "text": "written in a message\npassing kind of way, not in a loads and\nstores kind of way.",
    "start": "966860",
    "end": "972209"
  },
  {
    "text": "So here it is. It's a little bit of, the\nfont size a little small, but take a look at it and see.",
    "start": "972210",
    "end": "980000"
  },
  {
    "text": "Give it some talk. Now this is code that's being\nexecuted by every thread. It's like an SPMD kind of way.",
    "start": "980000",
    "end": "985880"
  },
  {
    "text": "So what every\nthread does is going to come from its thread\nID, so that's a T ID",
    "start": "985880",
    "end": "992270"
  },
  {
    "text": "is the thread ID here. And I'll go ahead and\ngive you a minute or so.",
    "start": "992270",
    "end": "997620"
  },
  {
    "text": "It's commented in some sense,\nbut talk it over and make sure that you all understand\nthe flow here.",
    "start": "997620",
    "end": "1002750"
  },
  {
    "text": "So I'll let you discuss. ",
    "start": "1002750",
    "end": "1008790"
  },
  {
    "text": "Yeah, let's discuss. I want to make sure\neverybody understands this. This is worth discussing.",
    "start": "1008790",
    "end": "1014670"
  },
  {
    "text": "So what is this thing doing? So there's a phase of\nsending and receiving data.",
    "start": "1014670",
    "end": "1020800"
  },
  {
    "text": "There's a phase of doing the\nwork you're supposed to do. There's a phase of sending the\nupdated data to everybody else.",
    "start": "1020800",
    "end": "1027984"
  },
  {
    "text": "And then there's a\nphase of determining if we're done and whether or\nnot we have to repeat again. So discuss.",
    "start": "1027984",
    "end": "1033172"
  },
  {
    "start": "1033172",
    "end": "1039470"
  },
  {
    "text": "OK, so let's get back together. I think most people have\nconverged the conversation.",
    "start": "1039470",
    "end": "1046250"
  },
  {
    "text": "So let's start talking\njust to make sure everybody understands this. And there was a\nquestion about good,",
    "start": "1046250",
    "end": "1052040"
  },
  {
    "text": "and I purposely kind\nof didn't clarify this because I was going to get\ninto a little bit more later.",
    "start": "1052040",
    "end": "1057360"
  },
  {
    "text": "Like how does a send\nand receive work? So if I'm a thread\nand I call receive,",
    "start": "1057360",
    "end": "1063620"
  },
  {
    "text": "that will return when some\nother thread has send, right?",
    "start": "1063620",
    "end": "1069990"
  },
  {
    "text": "So in some sense it's like\nwhen that call returns, the data is now with\nme at my address space.",
    "start": "1069990",
    "end": "1075480"
  },
  {
    "text": "So technically,\nif I call receive and nobody sends\nme anything, I'll just wait there for forever.",
    "start": "1075480",
    "end": "1080660"
  },
  {
    "text": "OK. So let's just take\na look at this code and look and pick out\nsome of the details.",
    "start": "1080660",
    "end": "1087450"
  },
  {
    "text": "So one of the more\ninteresting things is just the\nallocation at the top.",
    "start": "1087450",
    "end": "1092909"
  },
  {
    "text": "So I have this\nvariable local A which is just this thread's\nlocal copy of some portion",
    "start": "1092910",
    "end": "1100920"
  },
  {
    "text": "of this whole conceptual grid. And local A has rows per\nthread plus 2 allocation.",
    "start": "1100920",
    "end": "1112140"
  },
  {
    "text": "So it's the my fraction of\nthe array plus one row on top and one row on bottom.",
    "start": "1112140",
    "end": "1118470"
  },
  {
    "text": "And notice that these\ncalls to send and receive, the if statements are just,\nif I'm the first thread,",
    "start": "1118470",
    "end": "1125710"
  },
  {
    "text": "there's nothing to my\nleft so I don't need to. But notice that\nthey're storing data",
    "start": "1125710",
    "end": "1131220"
  },
  {
    "text": "into the first row or\nthe first row of local A.",
    "start": "1131220",
    "end": "1137100"
  },
  {
    "text": "Or in fact, the\nlast row of local A. So the data that\nI send and receive",
    "start": "1137100",
    "end": "1142290"
  },
  {
    "text": "goes right into my local array\nat the top or the bottom. And I did that so that when\nI iterate with actual memory",
    "start": "1142290",
    "end": "1150510"
  },
  {
    "text": "accesses, I don't\nhave to differentiate between what's the ghost row\nand what's not the ghost row.",
    "start": "1150510",
    "end": "1156357"
  },
  {
    "text": "I just at that point I\njust put all the data where it's supposed to be. So my code is simple.",
    "start": "1156357",
    "end": "1162010"
  },
  {
    "text": "Now that's good. Yes.",
    "start": "1162010",
    "end": "1167070"
  },
  {
    "text": "It seems to me that the\nfirst and the last person should also have\none ghost from each.",
    "start": "1167070",
    "end": "1172510"
  },
  {
    "text": "They should. I mean, they all\nallocate the same. In this case, the\nlast one doesn't",
    "start": "1172510",
    "end": "1178140"
  },
  {
    "text": "have a ghost row underneath it. And so that data is\nallocated, but nothing is ever",
    "start": "1178140",
    "end": "1184320"
  },
  {
    "text": "written there. So it's just to keep\nthe code simple, I didn't want to\nput a conditional around the allocation.",
    "start": "1184320",
    "end": "1191220"
  },
  {
    "text": "I was going to ask what\nhappens in the middle or the top and bottom rows.",
    "start": "1191220",
    "end": "1196270"
  },
  {
    "text": "It is going to be reaching\ninto that unallocated? Did I mess this up?",
    "start": "1196270",
    "end": "1202559"
  },
  {
    "text": "So I'm going from\nrows per thread. [INAUDIBLE] i-1 would be 0.",
    "start": "1202560",
    "end": "1208650"
  },
  {
    "text": "The zeroth row. Yeah so I'm doing, I'm\nstarting on the first row and then the next\nrow and then yeah,",
    "start": "1208650",
    "end": "1217600"
  },
  {
    "text": "I think I just didn't\nguard that properly. Sorry about that. Yeah, because if you\nlook at this example,",
    "start": "1217600",
    "end": "1223330"
  },
  {
    "text": "now I'm only showing the\nghost rows for thread 3, but as you pointed out,\nthere would be a ghost row here and here.",
    "start": "1223330",
    "end": "1228480"
  },
  {
    "text": "There would be a ghost row here. And to be honest, if\nyou recall our code from",
    "start": "1228480",
    "end": "1233850"
  },
  {
    "text": "before, it would just\niterate from I equals 1 to I equal, to the last row that we\ncompute values for was this one.",
    "start": "1233850",
    "end": "1242325"
  },
  {
    "text": "That was the definition\nof the problem.  Yeah, I was a little sloppy,\nbut I hope you get the point.",
    "start": "1242325",
    "end": "1249720"
  },
  {
    "text": "OK. And then oops. And then kind of an\ninteresting thing",
    "start": "1249720",
    "end": "1254880"
  },
  {
    "text": "is remember last time we had\nin the shared address space, we had the lock and\nwe had the barriers.",
    "start": "1254880",
    "end": "1262179"
  },
  {
    "text": "But there's no locks and\nthere's no barriers here. So how is this\nmessage patched code",
    "start": "1262180",
    "end": "1268320"
  },
  {
    "text": "like can you describe to me. And I know there's a\ncomment here on the slide, but let's just talk it\nover in our own words.",
    "start": "1268320",
    "end": "1273850"
  },
  {
    "text": "How do we determine whether\nor not we should keep going?",
    "start": "1273850",
    "end": "1279299"
  },
  {
    "text": "Yeah. Is it all the receive, do\nwe wait for all the receives",
    "start": "1279300",
    "end": "1284720"
  },
  {
    "text": "in that for loop? Which one. Here? That one. Yeah. OK, so look at this. So it says if thread\nID is not thread 0,",
    "start": "1284720",
    "end": "1293570"
  },
  {
    "text": "so every thread but thread\n0 sends its local diff.",
    "start": "1293570",
    "end": "1300019"
  },
  {
    "text": "If your thread 0, you wait\nto receive that local diff",
    "start": "1300020",
    "end": "1305420"
  },
  {
    "text": "from all other threads. You do a calculation to compute\nwhether or not we are done.",
    "start": "1305420",
    "end": "1312210"
  },
  {
    "text": "So thread zero determines\nthis calculation. And then actually sends\nthe Boolean value done",
    "start": "1312210",
    "end": "1317720"
  },
  {
    "text": "equals true to every\nother thread, which is then received right\nhere by every other thread.",
    "start": "1317720",
    "end": "1323360"
  },
  {
    "text": "Now of course, we\ncould have done it. I could just computed\nthe total value and sent the total value back\nand everybody independently",
    "start": "1323360",
    "end": "1329720"
  },
  {
    "text": "could have computed\nif we were done. But that's just how I\ndid it in this code. So why is there no\nlock in this code?",
    "start": "1329720",
    "end": "1336290"
  },
  {
    "text": " By receiving your leading.",
    "start": "1336290",
    "end": "1341510"
  },
  {
    "text": "So you're not going\nto be contributing. Correct. And even more\nfundamentally, why would we",
    "start": "1341510",
    "end": "1348560"
  },
  {
    "text": "never see a lock in any\nmessage passing code? It's not all shared.",
    "start": "1348560",
    "end": "1353630"
  },
  {
    "text": "There's nothing shared. So there's nothing to\nmaintain mutual exclusion on. So the only way to synchronize\nis via these messages, right?",
    "start": "1353630",
    "end": "1362550"
  },
  {
    "text": "So we've essentially,\nwe've created a situation where everybody sends a\npartial sum to one player",
    "start": "1362550",
    "end": "1369650"
  },
  {
    "text": "or to one thread here. One thread does all the math\nand then gives the result back to everybody else.",
    "start": "1369650",
    "end": "1374880"
  },
  {
    "text": "Why do we not see\nany barriers here? [INAUDIBLE]",
    "start": "1374880",
    "end": "1380500"
  },
  {
    "text": " Yeah, essentially the barrier is\ninherent in this communication pattern that I did.",
    "start": "1380500",
    "end": "1386179"
  },
  {
    "text": "Exactly. And since everybody has their\nown copy of the done variable because there's no\nshared address space,",
    "start": "1386180",
    "end": "1392309"
  },
  {
    "text": "there's no worry\nabout someone starting in the future and\noverriding something that somebody else receives.",
    "start": "1392310",
    "end": "1397750"
  },
  {
    "text": "So the communication is\nreally, really explicit in these sends and receives.",
    "start": "1397750",
    "end": "1403410"
  },
  {
    "text": "And so just like a\nsummary is notice that all of the array\ncomputation was relative.",
    "start": "1403410",
    "end": "1410590"
  },
  {
    "text": "Now, so if I go back\nhere, just keep in mind that all threads are iterating\nover the same indices now.",
    "start": "1410590",
    "end": "1418830"
  },
  {
    "text": "They're iterating over the same\nindices in their own local piece of the array.",
    "start": "1418830",
    "end": "1424210"
  },
  {
    "text": "Whereas, before, in the\nshared address space, I did some math to make sure\nthat everybody was iterating over different indices.",
    "start": "1424210",
    "end": "1430120"
  },
  {
    "text": "So that's another example. So array indexing is\nrelative, communication",
    "start": "1430120",
    "end": "1435150"
  },
  {
    "text": "is performed via\nsends and receives. In this case, we decided to\nsend many elements at a time.",
    "start": "1435150",
    "end": "1442860"
  },
  {
    "text": "And for efficiency,\ndo one send instead of a bunch of little ones. And synchronization is not done\nthrough locks and barriers.",
    "start": "1442860",
    "end": "1449590"
  },
  {
    "text": "Synchronization is manifest in\nhow we construct the messages.",
    "start": "1449590",
    "end": "1454903"
  },
  {
    "text": "Now, there was something\nthat you all assumed. There was a question from\nthe back that got to this. Like, wait a minute. Like let's just make sure\nwe understand the order",
    "start": "1454903",
    "end": "1462299"
  },
  {
    "text": "of things that happen\nin what's called a blocking send and receive.",
    "start": "1462300",
    "end": "1467350"
  },
  {
    "text": "So the sends and receives\nthat I just showed you that we all kind of assumed\nwould be if the sender",
    "start": "1467350",
    "end": "1472800"
  },
  {
    "text": "calls, send (foo), data from the\nvariable foo in my local address",
    "start": "1472800",
    "end": "1480660"
  },
  {
    "text": "space, in the sender's\naddress space, will be copied into the network. The network will\ntransmit the message.",
    "start": "1480660",
    "end": "1489000"
  },
  {
    "text": "And assuming that\nthe receiver has called receive on some of\nits own local variable bar,",
    "start": "1489000",
    "end": "1495900"
  },
  {
    "text": "the receiver will\nget the message, copy whatever is in the message\ninto the local variable.",
    "start": "1495900",
    "end": "1502720"
  },
  {
    "text": "And when the receiver is\ndone copying that data and has that data, it might\nsend an acknowledgment back",
    "start": "1502720",
    "end": "1509759"
  },
  {
    "text": "to the sender. And the send call returns\nonce we are guaranteed",
    "start": "1509760",
    "end": "1516780"
  },
  {
    "text": "that the receiver has the data. That's a blocking send. And similarly, a\nblocking receive,",
    "start": "1516780",
    "end": "1524430"
  },
  {
    "text": "this receive returns\nwhen the receiver has the data in the appropriate\nvariable in its address space.",
    "start": "1524430",
    "end": "1533440"
  },
  {
    "text": "OK. Yes. What if the data never arrives? So when the sender sends the\ndata but it never arrives?",
    "start": "1533440",
    "end": "1540030"
  },
  {
    "text": "If there's a network\nfailure, for example? If there's a network failure\nin this simplistic definition",
    "start": "1540030",
    "end": "1547500"
  },
  {
    "text": "of blocking send and receive. The receiver never returns. Just for now.",
    "start": "1547500",
    "end": "1552779"
  },
  {
    "text": "Yeah. The sender wouldn't\nget an acknowledgment. And the sender never returns. So you're thinking about\nit like I am in a network.",
    "start": "1552780",
    "end": "1561880"
  },
  {
    "text": "I'm in a network distributed\nprogramming environment, failures happen. I need to be robust to it.",
    "start": "1561880",
    "end": "1567160"
  },
  {
    "text": "Now I'll take this. And let's just say you bought a\n16-core computer and one message",
    "start": "1567160",
    "end": "1574560"
  },
  {
    "text": "from one core couldn't\nget to the L3 cache. You throw it out. Or all of the reliability\nof the protocol.",
    "start": "1574560",
    "end": "1583900"
  },
  {
    "text": "Let's say if we're talking\nabout a network on a chip, all of the failure\nretransmission is going to be handled\nat the hardware level.",
    "start": "1583900",
    "end": "1591759"
  },
  {
    "text": "And so a message\nsend from one core to-- if your processor stores\nto memory and you get an error,",
    "start": "1591760",
    "end": "1600269"
  },
  {
    "text": "sorry, you throw\nthe machine out. So that's kind of how I want\nyou to think about it right now.",
    "start": "1600270",
    "end": "1606475"
  },
  {
    "text": "Or you should think\nabout it as there's something underneath this\nAPI, maybe some system",
    "start": "1606475",
    "end": "1611820"
  },
  {
    "text": "software or something that\nwill do all of the retries and just keep retrying\nuntil that thing happens.",
    "start": "1611820",
    "end": "1616930"
  },
  {
    "text": "OK. Obviously, you can think\nof alternative APIs that the send could fail.",
    "start": "1616930",
    "end": "1622900"
  },
  {
    "text": "Like let's say the hardware\nreturned some error code. The API could say, well,\nit didn't get sent,",
    "start": "1622900",
    "end": "1627929"
  },
  {
    "text": "and it will return an error\ncode or something like that. But trust me, if you're\nlike writing code like this, you're not checking\nyour error codes on",
    "start": "1627930",
    "end": "1634455"
  },
  {
    "text": "whether or not like memory\ndoesn't work and stuff like that.",
    "start": "1634455",
    "end": "1639930"
  },
  {
    "text": "So you all discussed, and\nyou all correctly told me",
    "start": "1639930",
    "end": "1646230"
  },
  {
    "text": "how that code on the\nprevious slide worked. But nobody told me that there\nwas a fatal bug in the code.",
    "start": "1646230",
    "end": "1655650"
  },
  {
    "text": "Let's go back to the code. I'd like you to take\nanother look at it.",
    "start": "1655650",
    "end": "1660870"
  },
  {
    "text": "I'd like you to\ntell me what happens if I run this code with\nblocking sends and receives.",
    "start": "1660870",
    "end": "1666669"
  },
  {
    "text": "So everybody. I want everybody to take 15,\n20 seconds to think about it.",
    "start": "1666670",
    "end": "1672809"
  },
  {
    "text": "My hint is that it will\nbe as bad as this comment just a second ago if\nmemory does not work.",
    "start": "1672810",
    "end": "1680970"
  },
  {
    "text": "All right. So it feels like I see\nsome eyes lighting up. Anybody want to tell\nme what's wrong?",
    "start": "1680970",
    "end": "1686470"
  },
  {
    "text": "Yes. All the threads are\nblocking on side first. So imagine that all\nof you in this room",
    "start": "1686470",
    "end": "1692190"
  },
  {
    "text": "are, or everybody like,\nimagine, like every processor is like a row in this room.",
    "start": "1692190",
    "end": "1699000"
  },
  {
    "text": "And so the first thing\nall of you do is you look, I guess in this case,\nyou look backwards.",
    "start": "1699000",
    "end": "1705720"
  },
  {
    "text": "Or you send backwards, right? That's what you're doing. You're sending backwards. And your send will complete\nwhen the person behind you",
    "start": "1705720",
    "end": "1713370"
  },
  {
    "text": "acknowledges or makes\na matching receive. But what does the\nperson behind you do?",
    "start": "1713370",
    "end": "1718540"
  },
  {
    "text": "They're looking, they're\nsending backwards, too, so they're never going to\nget around to that receive. How can we fix this?",
    "start": "1718540",
    "end": "1725520"
  },
  {
    "text": "While still making\nonly using blocking sends and stuff like that? Yeah. OK.",
    "start": "1725520",
    "end": "1730860"
  },
  {
    "text": "Yeah. No asynchronous operation. You can only use blocking sends. ",
    "start": "1730860",
    "end": "1740280"
  },
  {
    "text": "You can just say that\none row moves back. The other one is forward\none first and the other one",
    "start": "1740280",
    "end": "1745880"
  },
  {
    "text": "back, forwards. Yeah. So a simple solution would\njust pair everybody up by their row parity. So first row sends\nback the next row.",
    "start": "1745880",
    "end": "1755630"
  },
  {
    "text": "First thing is they\nreceive forward. And then you can set, that would\nbe like one way of doing it.",
    "start": "1755630",
    "end": "1762770"
  },
  {
    "text": "I guess if you look carefully,\nsome people sometimes say, well, won't the first row,\nbecause they don't actually",
    "start": "1762770",
    "end": "1769790"
  },
  {
    "text": "send anything, won't it work? And it actually doesn't even\nwork because the first row,",
    "start": "1769790",
    "end": "1776360"
  },
  {
    "text": "if it doesn't send\nback, it sends forward. So everybody's going to\ndo a send before they",
    "start": "1776360",
    "end": "1781640"
  },
  {
    "text": "post their first receive. So this will deadlock\nunder any conditions. This is a form of\ndeadlock, right?",
    "start": "1781640",
    "end": "1786910"
  },
  {
    "text": "Like you are not making any\nprogress at all because someone, you're waiting on\nsomeone else who also can't make any progress at all.",
    "start": "1786910",
    "end": "1793520"
  },
  {
    "text": "So good. So moving forward, one\npossible implementation would be to pair\nfolks up by parity.",
    "start": "1793520",
    "end": "1801039"
  },
  {
    "text": "And so you say, OK, I'm\ngoing to have a partner. And for each partner,\nsomeone's going to send first and someone's going to receive\nfirst and so on and so on.",
    "start": "1801040",
    "end": "1808350"
  },
  {
    "text": "OK. Question? No, OK. Feels really terrible.",
    "start": "1808350",
    "end": "1814830"
  },
  {
    "text": "It feels like a very small\nmistake could cause your program",
    "start": "1814830",
    "end": "1819929"
  },
  {
    "text": "to lock. Absolutely. So we could also go in\na different direction. We could also go in\na direction where",
    "start": "1819930",
    "end": "1826020"
  },
  {
    "text": "communication is asynchronous. So last time when we\nwere talking about silk, I talked about an\nasynchronous function call.",
    "start": "1826020",
    "end": "1832770"
  },
  {
    "text": "A function call that can carry\non potentially, concurrently with the caller.",
    "start": "1832770",
    "end": "1837850"
  },
  {
    "text": "And a message is you can think\nabout it as it could also be",
    "start": "1837850",
    "end": "1843030"
  },
  {
    "text": "an asynchronous function call. So here's an asynchronous\nsend and receive. So now when a thread\ncalls send, it's",
    "start": "1843030",
    "end": "1851220"
  },
  {
    "text": "more of I want this\nmessage to be sent at some point in the future.",
    "start": "1851220",
    "end": "1856350"
  },
  {
    "text": "So the sender calls send but\nsend returns immediately.",
    "start": "1856350",
    "end": "1862851"
  },
  {
    "text": "You don't know at this point,\nyou meaning the thread, you don't know that the data\nis gone or has been sent yet.",
    "start": "1862852",
    "end": "1868810"
  },
  {
    "text": "So typically, the API will\ngive you back a handle. Like it just says,\nhey, if you ever need to know if this\nis done, here's the ID",
    "start": "1868810",
    "end": "1874390"
  },
  {
    "text": "that you can use to check. So I'm calling that H1 here. So at some point in the future,\nlike so this thread can just",
    "start": "1874390",
    "end": "1880150"
  },
  {
    "text": "keep running. At some point in the\nfuture, the message library gets around to copying\nthe data to the network",
    "start": "1880150",
    "end": "1887410"
  },
  {
    "text": "and pushing it over. And at some point the\nreceiver will get around",
    "start": "1887410",
    "end": "1893140"
  },
  {
    "text": "to actually doing the receive. And once we know that the\ndata has been received,",
    "start": "1893140",
    "end": "1900710"
  },
  {
    "text": "we can probe the system. We could say like, are you done? Which I'm writing here\nis check the send status.",
    "start": "1900710",
    "end": "1906980"
  },
  {
    "text": "Check the send of that message. And if that returns\ntrue, it's now",
    "start": "1906980",
    "end": "1912250"
  },
  {
    "text": "I'm guaranteed that the data\nis gone and I can delete, like I could do something\nlike delete Foo or modify Foo.",
    "start": "1912250",
    "end": "1918530"
  },
  {
    "text": "Notice that if I\ndo anything to Foo in between this point\nand this confirmation, I have no guarantee\nthat the message passing",
    "start": "1918530",
    "end": "1926919"
  },
  {
    "text": "library has picked up the data\nand sent it over the wire. It'd be like me putting a\npackage out on my porch,",
    "start": "1926920",
    "end": "1933920"
  },
  {
    "text": "telling UPS to come\ntake it, and then me like changing the contents of\nthe package before UPS shows up.",
    "start": "1933920",
    "end": "1940630"
  },
  {
    "text": "The modified message is\nwhat's going to get sent. Right. Or if I remove the package from\nthe porch, UPS is going to say,",
    "start": "1940630",
    "end": "1947000"
  },
  {
    "text": "you told me to come get it\nand there's nothing here. On the flip side,\non the receive, this is an asynchronous receive.",
    "start": "1947000",
    "end": "1953809"
  },
  {
    "text": "So if I say, I want you\nto receive a message that I'm expecting, that returns\nimmediately with a handle.",
    "start": "1953810",
    "end": "1961150"
  },
  {
    "text": "And then the receiver at\nsome point later can say, hey, is it here yet? Is it here yet? Is it here yet?",
    "start": "1961150",
    "end": "1966620"
  },
  {
    "text": "And if so, I know I can touch\nthe data in bar at this point.",
    "start": "1966620",
    "end": "1972010"
  },
  {
    "text": "If I read bar at\nthis point, it's unclear what data I will get.",
    "start": "1972010",
    "end": "1977830"
  },
  {
    "text": "So that's the asynchronous\nversion of these things. And so the asynchronous\nversion can make it easier",
    "start": "1977830",
    "end": "1983140"
  },
  {
    "text": "to implement some things like\nwhat I just talked about, because you're not so worried\nnecessarily about deadlock.",
    "start": "1983140",
    "end": "1989140"
  },
  {
    "text": "The compiler is smart enough not\nto be worried your instructions around this. That's a really\ngood point, is what",
    "start": "1989140",
    "end": "1995140"
  },
  {
    "text": "will happen in let's say\nthis send and receive were a library, which is\nwhat it normally would be.",
    "start": "1995140",
    "end": "2000690"
  },
  {
    "text": "If you were the\nlibrary implementer inside of send and\nreceive, you definitely",
    "start": "2000690",
    "end": "2005820"
  },
  {
    "text": "would be putting\nvarious, what's called memory fences or\nother things in there, so the compiler would\nnot reorder around",
    "start": "2005820",
    "end": "2012450"
  },
  {
    "text": "those instructions. So I'll talk a little bit\nabout that on a lecture later, which is about implementing\nsynchronization.",
    "start": "2012450",
    "end": "2018250"
  },
  {
    "text": "Yeah. How do acknowledgments\nwork in this system? Well, the\nacknowledgments would be",
    "start": "2018250",
    "end": "2023640"
  },
  {
    "text": "an underlying detail of\nthe communication transport mechanism.",
    "start": "2023640",
    "end": "2028960"
  },
  {
    "text": "So they don't appear on\nthis slide on purpose. From the consumer, from\nthe threads perspective,",
    "start": "2028960",
    "end": "2034690"
  },
  {
    "text": "what I have available to\nme is I initiate a send. I essentially get a tracking\nnumber, which is the handle.",
    "start": "2034690",
    "end": "2042000"
  },
  {
    "text": "And I can check via\nthat tracking number later if the send is\ncomplete, yes or no. Or maybe if it failed.",
    "start": "2042000",
    "end": "2048090"
  },
  {
    "text": "Now how under the\nhood the network layer implements reliable transport.",
    "start": "2048090",
    "end": "2054069"
  },
  {
    "text": "That's a completely\ndifferent story. And outside the\nscope of this class.",
    "start": "2054070",
    "end": "2059100"
  },
  {
    "text": "Yes. When you have two\nconsecutive sends, how does this set\nup time to live?",
    "start": "2059100",
    "end": "2065638"
  },
  {
    "text": "So in this slide, if\nI said, send to Foo. And again, Foo is the\nvariable that I'm sending",
    "start": "2065639",
    "end": "2071070"
  },
  {
    "text": "in my local address space. If I send Foo again here,\nand these were asynchronous,",
    "start": "2071070",
    "end": "2077310"
  },
  {
    "text": "it's pretty undefined what\nwill happen because there's no guarantee of what order those\ntwo messages would be sent.",
    "start": "2077310",
    "end": "2082600"
  },
  {
    "text": "They're actually both\nsending the contents of the same local variable. So are you saying what if\nI had send Foo and then",
    "start": "2082600",
    "end": "2089550"
  },
  {
    "text": "another send fizz or\nsomething like that? I mean, I'm just\nsending two messages. So there's no guarantee\nthat they will be--",
    "start": "2089550",
    "end": "2096300"
  },
  {
    "text": "unless the library gives\nyou, states guarantees-- there's no fundamental\nreason that they would be arrive at the\nreceiver in the same order.",
    "start": "2096300",
    "end": "2104460"
  },
  {
    "text": "Yeah. Unless like there's some\nconfiguration on the message passing API, which says,\nif you set this flag,",
    "start": "2104460",
    "end": "2111710"
  },
  {
    "text": "we guarantee you it will\nbe in the same order. You make sure, so what are\nyou just like busy wait here?",
    "start": "2111710",
    "end": "2117640"
  },
  {
    "text": "Yeah so one way would\nbe to busy wait. Another way, some\nlibraries might be designed",
    "start": "2117640",
    "end": "2124030"
  },
  {
    "text": "and that you register a\ncallback or other things. Like in asynchronous\nJavaScript, it's more like essentially\nposting that AJAX request.",
    "start": "2124030",
    "end": "2131260"
  },
  {
    "text": "Like you're putting yourself\nin a queue and you get invoked when it's done. Yeah. This is about the\nordering of the receives",
    "start": "2131260",
    "end": "2137990"
  },
  {
    "text": "if they're sent in sequence. I think previously there\nwas a message ID also?",
    "start": "2137990",
    "end": "2144910"
  },
  {
    "text": "Yeah. Like the way you would do it\nis, I'm just being simple here. I send Foo ID, whatever.",
    "start": "2144910",
    "end": "2150680"
  },
  {
    "text": "And then the receive,\nyou could either receive most of these APIs. Just it's very specific\nnow to the threading,",
    "start": "2150680",
    "end": "2156600"
  },
  {
    "text": "to the message passing. You might say, I want\nto explicitly receive the message with this ID.",
    "start": "2156600",
    "end": "2162310"
  },
  {
    "text": "Or you just post to receive\nand say, I'm just receiving. And then once you have\nchecked the receive, you say,",
    "start": "2162310",
    "end": "2167890"
  },
  {
    "text": "the message is here and here's\nthe idea of what it was. And then your program might be,\noh, if it's ID 43, I do this.",
    "start": "2167890",
    "end": "2174960"
  },
  {
    "text": "If it's ID 42, I do that. Yeah. Yeah. So every message you should\nthink of as having an ID",
    "start": "2174960",
    "end": "2180320"
  },
  {
    "text": "and sending or\nwaiting or receiving can be wait for a message.",
    "start": "2180320",
    "end": "2185450"
  },
  {
    "text": "Any message. Wait for a message\nfrom the sender. Wait for a message\nwith this ID, those are all parameters and details\nof your message passing thing.",
    "start": "2185450",
    "end": "2193910"
  },
  {
    "text": "Now, even though\nI set this up as imagine these are different\ncomputers communicating.",
    "start": "2193910",
    "end": "2199200"
  },
  {
    "text": "I want you to just keep\nin mind that there's many different types\nof communication. So there's no\ndifference conceptually,",
    "start": "2199200",
    "end": "2205370"
  },
  {
    "text": "if we're talking\nabout communication between a core and its memory. Or two different cores\non the same chip.",
    "start": "2205370",
    "end": "2211670"
  },
  {
    "text": "Or two different computers\nin different dorm rooms. Like abstractly, we can send\nmessages between anything. Right?",
    "start": "2211670",
    "end": "2216860"
  },
  {
    "text": "And communication can\nbe between movement of data can happen between the\nprocessor and its registers",
    "start": "2216860",
    "end": "2222920"
  },
  {
    "text": "or its local L1 or its L3\nor DRAM on my own computer or DRAM on somebody else's\ncomputer or DRAM off in Google.",
    "start": "2222920",
    "end": "2230280"
  },
  {
    "text": "So communication, I want you to\nthink of as just being abstract.",
    "start": "2230280",
    "end": "2235550"
  },
  {
    "text": "And once you start thinking\nabout these diagrams that I showed you a few\nlectures earlier, where I said,",
    "start": "2235550",
    "end": "2243150"
  },
  {
    "text": "imagine a processor\nissues a load instruction and there's some memory latency.",
    "start": "2243150",
    "end": "2248599"
  },
  {
    "text": "And then the data\nhas to actually start moving back to me. So now hopefully,\nyou get a little bit",
    "start": "2248600",
    "end": "2254809"
  },
  {
    "text": "of a sense of where all this\nmemory latency comes from. Like it's an L1 cache, look\nup an L2 cache, look up.",
    "start": "2254810",
    "end": "2260790"
  },
  {
    "text": "Maybe you actually have a TLB\nmiss because of an operating systems or something like that.",
    "start": "2260790",
    "end": "2266359"
  },
  {
    "text": "Maybe they request a message\nhas to get sent to memory saying I want the\ndata at this address.",
    "start": "2266360",
    "end": "2272330"
  },
  {
    "text": "And at some point,\nthe memory starts sending you the data back. And if you have a bandwidth\nof B bits per second,",
    "start": "2272330",
    "end": "2279119"
  },
  {
    "text": "you start getting B bits per\nsecond back in that blue region. And so that's why when\nI drew this thing,",
    "start": "2279120",
    "end": "2289160"
  },
  {
    "text": "I drew an example\nlike this, where this was where it\nwas a program that did two instructions and\nthen a memory transaction.",
    "start": "2289160",
    "end": "2297570"
  },
  {
    "text": "So it's like math, math,\nread, math, math, read, math, math, read.",
    "start": "2297570",
    "end": "2302880"
  },
  {
    "text": "And the main idea of\nthis diagram is, well, if we just look very\ncarefully, first of all, even though the reads don't\ncome back for a while,",
    "start": "2302880",
    "end": "2310700"
  },
  {
    "text": "as long as the processor\nhas some ability to hide the latency-- like\nmultithreading or something like",
    "start": "2310700",
    "end": "2316490"
  },
  {
    "text": "that-- we don't really care so\nmuch about the latency. We actually care the most about\nthe length of the blue bar.",
    "start": "2316490",
    "end": "2325220"
  },
  {
    "text": "So if you look carefully\nat this diagram, just double convince yourself\nthat memory is always busy.",
    "start": "2325220",
    "end": "2332990"
  },
  {
    "text": "And the processor\nis not always busy. And I drew these\nyellow bars to show you",
    "start": "2332990",
    "end": "2338540"
  },
  {
    "text": "that memory was always busy. And the pink bars are times\nwhen the processor is not executing instructions,\nbecause it's",
    "start": "2338540",
    "end": "2345829"
  },
  {
    "text": "waiting for the next piece of\ndata to come back from memory.",
    "start": "2345830",
    "end": "2351480"
  },
  {
    "text": "So you can think about this blue\nbar as being like it's a cache line, it's like 64-bytes\nor something like that.",
    "start": "2351480",
    "end": "2357600"
  },
  {
    "text": "If every one of these reads\nis a unique cache line. Yeah. Question. So is this a message\npassing an alternative",
    "start": "2357600",
    "end": "2364520"
  },
  {
    "text": "to what we talked about last\ntime with the worker queues? Or like how does\nit work together? Message passing is just a way to\nexchange data between threads.",
    "start": "2364520",
    "end": "2373640"
  },
  {
    "text": "An alternative on how to pass a\ncommunicate data between threads is everybody reads and writes\nto a shared address space.",
    "start": "2373640",
    "end": "2380940"
  },
  {
    "text": "And so the reason why I\nbrought up message passing-- other than to make everybody\nfamiliar with the idea",
    "start": "2380940",
    "end": "2386510"
  },
  {
    "text": "of message passing-- is it's kind of helpful to\nthink about communication in the context of\nmessage passing,",
    "start": "2386510",
    "end": "2392000"
  },
  {
    "text": "because you literally\nsee it in the program. Here is where the\ncommunication is. Whereas, what we talked\nabout in assignment one,",
    "start": "2392000",
    "end": "2399500"
  },
  {
    "text": "or this was an\nexample that I used for the bandwidth bound\nproblem in assignment one. Where is the communication?",
    "start": "2399500",
    "end": "2405589"
  },
  {
    "text": "Well, the communication\nis the load and store. But that load and\nstore is actually about sending a\nrequest out to memory",
    "start": "2405590",
    "end": "2413260"
  },
  {
    "text": "and getting the data back. So the communication in a\nshared address space program is kind of implicit in the\nimplementation of memory.",
    "start": "2413260",
    "end": "2420850"
  },
  {
    "text": "So I'm just like, there's, but\nI could implement shared work queues using message, passing\non a bunch of machines",
    "start": "2420850",
    "end": "2429940"
  },
  {
    "text": "on a cluster that had\nno shared address space. I could do that dynamic work\nstealing with different queues",
    "start": "2429940",
    "end": "2435850"
  },
  {
    "text": "very easily. Except now, stealing work is\nabout sending another computer a message and getting work back,\nas opposed to directly accessing",
    "start": "2435850",
    "end": "2443620"
  },
  {
    "text": "their data structure. OK. Yeah. So if you're writing a program\non a practical system where it's",
    "start": "2443620",
    "end": "2450970"
  },
  {
    "text": "not a cluster but\njust one processor, is there ever a\nreason to use message passing as opposed\nto [INAUDIBLE]?",
    "start": "2450970",
    "end": "2456730"
  },
  {
    "text": "There definitely can be. There definitely can be. Because in some\nsense, message passing forces you to think about\nall the communication.",
    "start": "2456730",
    "end": "2463750"
  },
  {
    "text": "So a lot of folks\ndo actually write with a message passing model\non a multi-core shared memory",
    "start": "2463750",
    "end": "2471579"
  },
  {
    "text": "system, just because\nlocks are hard. And if messages were like throw\nsomething in a message queue",
    "start": "2471580",
    "end": "2479500"
  },
  {
    "text": "and let the other\nside pick it up, that could actually be an easier\nway to reason about concurrency.",
    "start": "2479500",
    "end": "2484520"
  },
  {
    "text": "Like, for example, in most\nmultiprocessing systems, you might send a message\nto another process",
    "start": "2484520",
    "end": "2491230"
  },
  {
    "text": "as opposed to memory\nmap the address space into both processes\nand stuff like that. So message passing is a\nhighly structured form",
    "start": "2491230",
    "end": "2498940"
  },
  {
    "text": "of communication,\nwhich forces you to work hard to get it right. But once you get it\nright, communication",
    "start": "2498940",
    "end": "2505120"
  },
  {
    "text": "is really explicit. You know where any\nstalls might be. It might be easier to\ndebug in performance tune.",
    "start": "2505120",
    "end": "2511270"
  },
  {
    "text": "Shared memory is just\na different mechanism that doesn't force you to\nhave any discipline at all.",
    "start": "2511270",
    "end": "2516620"
  },
  {
    "text": "So it might be easier to get\nyour first program running. Maybe you just throw\na big global lock",
    "start": "2516620",
    "end": "2521829"
  },
  {
    "text": "around everything or\nsomething like that. But as you start\nto performance tune it, now, you don't get maybe\nas much structure or as help,",
    "start": "2521830",
    "end": "2529339"
  },
  {
    "text": "and there's different\nreasons why you might want to use different things. Yeah. So one more question\nabout performance.",
    "start": "2529340",
    "end": "2536032"
  },
  {
    "text": "It seems like when we're\ndoing message passing, there's this extra write\nthat's happening where you have to first write to the network.",
    "start": "2536032",
    "end": "2542920"
  },
  {
    "text": "There's a copy. Yeah. Yeah. There's a copy in and out. Is there any way to get\naround that or is that-- There are plenty of\nways to get around that.",
    "start": "2542920",
    "end": "2549233"
  },
  {
    "text": "Like, so let's go back here. In a modern high\nperformance network,",
    "start": "2549233",
    "end": "2555840"
  },
  {
    "text": "even like a modern\ndata center where you've got a bunch of machines. And let's say you're\nrunning like a key value",
    "start": "2555840",
    "end": "2562190"
  },
  {
    "text": "store that needs\nto be distributed over a bunch of machines. There's a lot of\ninterest in reducing",
    "start": "2562190",
    "end": "2567530"
  },
  {
    "text": "the latency and the cost\nof sending messages. So it could be very\neasy that this send Foo,",
    "start": "2567530",
    "end": "2573510"
  },
  {
    "text": "this Foo is a variable,\nthis is a pointer. And the only thing that\nhappens is that pointer is sent to your nick.",
    "start": "2573510",
    "end": "2580010"
  },
  {
    "text": "And your nick goes reads\nthat data out of memory directly itself and pushes\ndata out over the wire.",
    "start": "2580010",
    "end": "2585779"
  },
  {
    "text": "So they're very high performance\nnetworking implementations that don't necessarily mean that\ndata is copied unnecessarily.",
    "start": "2585780",
    "end": "2592470"
  },
  {
    "text": "But even in that implementation,\nuntil the data has been-- at some point, it has to\nget copied because it's got",
    "start": "2592470",
    "end": "2599330"
  },
  {
    "text": "to get copied into the network. Until that copy happens,\nany modification to Foo by the calling thread\ncould in fact change the bits",
    "start": "2599330",
    "end": "2607579"
  },
  {
    "text": "prior to that copy. Which means that even\nthough you thought you were sending the contents\nof Foo when I called the thing,",
    "start": "2607580",
    "end": "2613920"
  },
  {
    "text": "you actually send the\ncontents of Foo later. And that's going to be a bug. So asynchronous, all of a\nsudden if it's synchronous,",
    "start": "2613920",
    "end": "2622290"
  },
  {
    "text": "you never think about\nconcurrency between the caller and the message transmission. Asynchronous, like\nit was suggested",
    "start": "2622290",
    "end": "2628797"
  },
  {
    "text": "that it might be an\neasier way to do things. It can also be a harder way to\ndo things because now you've introduced more\nconcurrency in your program",
    "start": "2628797",
    "end": "2634542"
  },
  {
    "text": "and you potentially\nhave more problems. OK. All right. So let's see here.",
    "start": "2634542",
    "end": "2640540"
  },
  {
    "text": " I talked about this\na few lectures ago,",
    "start": "2640540",
    "end": "2645713"
  },
  {
    "text": "but I wanted to\nput another slide. This is something I'd\nlike you to take offline. And when you look at\nthis diagram here,",
    "start": "2645713",
    "end": "2653630"
  },
  {
    "text": "I want you to be\nable to go yeah, that program is bandwidth bound. It is basically communication\nbound between memory",
    "start": "2653630",
    "end": "2661520"
  },
  {
    "text": "and the processor. And I'd like you to be able to\nanswer all of these questions. Like, if you increased\nmemory latency, which",
    "start": "2661520",
    "end": "2668990"
  },
  {
    "text": "means if you increase the\ndistance from here to here,",
    "start": "2668990",
    "end": "2674119"
  },
  {
    "text": "would any of the utilization\nor efficiency change? And your answer should be no.",
    "start": "2674120",
    "end": "2679460"
  },
  {
    "text": "As long as you can hide\nthat latency, right? If you increase the bandwidth of\nthe system, what would happen?",
    "start": "2679460",
    "end": "2687740"
  },
  {
    "text": "The blue bar should shrink. And if the blue\nbar shrinks, that means I'm stalled less\nin the pink regions.",
    "start": "2687740",
    "end": "2694730"
  },
  {
    "text": "If I increase the number of\nmath operations per blue memory",
    "start": "2694730",
    "end": "2699740"
  },
  {
    "text": "request, utilization goes up. Those are all things that I want\nyou to be able to think through.",
    "start": "2699740",
    "end": "2705320"
  },
  {
    "text": "And at the end of the\nday, it all comes out. If we're not worried\nabout latency, if we have the ability\nto hide latency,",
    "start": "2705320",
    "end": "2710750"
  },
  {
    "text": "whether it be multithreading,\nprefetching data or whatever. The name of the game\nis going to come down",
    "start": "2710750",
    "end": "2716800"
  },
  {
    "text": "to something called\narithmetic intensity, which is the number of math operations\nyou're going to do per unit data",
    "start": "2716800",
    "end": "2724720"
  },
  {
    "text": "that you read. Some people like to call it\ncommunication to computation ratio, which is 1 over this.",
    "start": "2724720",
    "end": "2731060"
  },
  {
    "text": "I like arithmetic\nintensity because A, it kind of sounds cooler. And B, higher is better,\nwhich is intuitive for me.",
    "start": "2731060",
    "end": "2736890"
  },
  {
    "text": "Yeah. OK. So let's talk a\nlittle bit about a few",
    "start": "2736890",
    "end": "2742360"
  },
  {
    "text": "more ideas about communication. So you will find that it's\nhelpful to break apart",
    "start": "2742360",
    "end": "2752080"
  },
  {
    "text": "communication in two ways. One, is communication that\njust has to happen because",
    "start": "2752080",
    "end": "2758770"
  },
  {
    "text": "of the nature of the algorithm. And another one is communication\nthat happens because of the way",
    "start": "2758770",
    "end": "2764380"
  },
  {
    "text": "machines actually work. So the first one is called\ninherent communication. The second one is artifactual,\nand that comes from the fact",
    "start": "2764380",
    "end": "2772240"
  },
  {
    "text": "that I have some real details of\nhow a computer works and there are artifacts of that.",
    "start": "2772240",
    "end": "2777380"
  },
  {
    "text": "So let me just give\nyou an example here. So in this grid\nsolver application,",
    "start": "2777380",
    "end": "2783610"
  },
  {
    "text": "I cannot do the application. I can't get the\nright answer unless I move this data to this thread.",
    "start": "2783610",
    "end": "2790270"
  },
  {
    "text": "That is inherent\nto the computation. So that's communication\nthat just has to happen.",
    "start": "2790270",
    "end": "2796570"
  },
  {
    "text": "Somehow, I have\nto pay that cost. And we talked a little\nbit last time about, well,",
    "start": "2796570",
    "end": "2802309"
  },
  {
    "text": "how much communication is there? Let's take a look at this. So if I partition my work\nacross my processors,",
    "start": "2802310",
    "end": "2810130"
  },
  {
    "text": "sorry for interleaving P1 and\nT1, processors and threads to me are the same in this lecture.",
    "start": "2810130",
    "end": "2816310"
  },
  {
    "text": "How much communication do\nwe do for every element",
    "start": "2816310",
    "end": "2824890"
  },
  {
    "text": "that we process? So what is that\nratio of elements processed to amount of\ncommunication that happens?",
    "start": "2824890",
    "end": "2831670"
  },
  {
    "text": "So let's think about one thread. If the total amount\nof data is N squared.",
    "start": "2831670",
    "end": "2838480"
  },
  {
    "text": "And there are, divide this\nup amongst P processors. How much work does\nevery processor do?",
    "start": "2838480",
    "end": "2846140"
  },
  {
    "text": "N squared over P.\nN squared over P. And how much communication\ndoes every processor do?",
    "start": "2846140",
    "end": "2852970"
  },
  {
    "text": "2 P, right. So if we have N squared over P,\nI'm going to drop my constants",
    "start": "2852970",
    "end": "2859030"
  },
  {
    "text": "and I have 2n communication. Did I say two P?",
    "start": "2859030",
    "end": "2864060"
  },
  {
    "text": "Sorry. Yeah 2n right. It's the width of the thing. Well then my arithmetic\nintensity is N over P. Sorry.",
    "start": "2864060",
    "end": "2872590"
  },
  {
    "text": "N over P. Now if I went with this\ninterleaved assignment",
    "start": "2872590",
    "end": "2878810"
  },
  {
    "text": "and we think of it like the\nmessage passing program, what is the amount\nof computation I do?",
    "start": "2878810",
    "end": "2886140"
  },
  {
    "text": "Still N squared over P. And\nhow much data do I move? ",
    "start": "2886140",
    "end": "2894510"
  },
  {
    "text": "2N squared. Another way to think\nabout it is for every row that I compute, how much\ndata do I have to move?",
    "start": "2894510",
    "end": "2900500"
  },
  {
    "text": "Two rows of data. So now my arithmetic intensity\nhas gone from N over P,",
    "start": "2900500",
    "end": "2906250"
  },
  {
    "text": "and let's just assume that N\nis probably a good bit bigger than P, to one half.",
    "start": "2906250",
    "end": "2912920"
  },
  {
    "text": "So if I go back to\nthat diagram where the speed of my utilization\nis going to be a function of,",
    "start": "2912920",
    "end": "2919880"
  },
  {
    "text": "can I not be bandwidth\nor communication bound? Here, I'm doing N operations for\nevery P elements communicated.",
    "start": "2919880",
    "end": "2927380"
  },
  {
    "text": "There I'm doing 1 operation for\nevery 2 elements communicated. And I'm much more likely\nto be bandwidth bound",
    "start": "2927380",
    "end": "2933661"
  },
  {
    "text": "with the scheme on\nthe right than I am on the scheme on the left. OK.",
    "start": "2933662",
    "end": "2939500"
  },
  {
    "text": "Can anybody think of\nhow to maybe do better? So left is way better\nthan right, for sure.",
    "start": "2939500",
    "end": "2946650"
  },
  {
    "text": "Can you do better than the left? Yep. You block it.",
    "start": "2946650",
    "end": "2954010"
  },
  {
    "text": "So the arithmetic\nintensity, very good. It's basically it's the\nratio of the area of a region",
    "start": "2954010",
    "end": "2960420"
  },
  {
    "text": "to its perimeter, in this case. And the way to\ncreate a shape that",
    "start": "2960420",
    "end": "2965550"
  },
  {
    "text": "has the highest ratio of\narea to perimeter is what?",
    "start": "2965550",
    "end": "2970980"
  },
  {
    "text": "Is a square. So let's look at this. Let's divide up the\nwork this way instead.",
    "start": "2970980",
    "end": "2977530"
  },
  {
    "text": "I had to create a few\nmore processors here just to make the diagram\na little bit more obvious.",
    "start": "2977530",
    "end": "2982630"
  },
  {
    "text": "So now let's just say\nI have nine cores. And so again, I have N squared\nelements, P processors.",
    "start": "2982630",
    "end": "2989820"
  },
  {
    "text": "What is the work per processor? Stays N squared over\nP, that hasn't changed. What's the communication?",
    "start": "2989820",
    "end": "2996339"
  },
  {
    "text": "Well, every one of\nthese borders is square root of N over\nsquare root of P, right.",
    "start": "2996340",
    "end": "3006320"
  },
  {
    "text": "So the elements communicated\nis basically four times N over square root of\nP. Did you follow that?",
    "start": "3006320",
    "end": "3015270"
  },
  {
    "text": "Because I basically I\ndivided something of width N. I have P processors total,\nso there are square root P",
    "start": "3015270",
    "end": "3023790"
  },
  {
    "text": "processors along one row. And so now my arithmetic\nintensity, before, remember it was N over P, now\nit is N over root P, which",
    "start": "3023790",
    "end": "3034260"
  },
  {
    "text": "is a larger value,\nand potentially very important if my P is large on\na large core count machine.",
    "start": "3034260",
    "end": "3042240"
  },
  {
    "text": "So this trick of redistributing\nthe work in this tiled format,",
    "start": "3042240",
    "end": "3048330"
  },
  {
    "text": "as opposed to the\nchunks of rows, means that I have higher\narithmetic intensity,",
    "start": "3048330",
    "end": "3054220"
  },
  {
    "text": "I'm doing more\noperations per byte sent. And I can stay running\nat full utilization",
    "start": "3054220",
    "end": "3060599"
  },
  {
    "text": "with lower memory bandwidth. Or if I'm on a machine\nwith a lot of cores,",
    "start": "3060600",
    "end": "3066220"
  },
  {
    "text": "I have higher\nutilization for longer. ",
    "start": "3066220",
    "end": "3071630"
  },
  {
    "text": "Isn't that the shape of\nthe minimum ratio of area to perimeter. Isn't that a circle.",
    "start": "3071630",
    "end": "3077360"
  },
  {
    "text": "Well I mean, you\ncould, but go for it. What's the complexity\nto add that circle.",
    "start": "3077360",
    "end": "3082790"
  },
  {
    "text": "It's like-- It's going to be up to you. Like if you are a bandwidth\nbound, if you're communication",
    "start": "3082790",
    "end": "3088340"
  },
  {
    "text": "bound, anything you do to\nincrease arithmetic intensity will translate into\nenhanced performance.",
    "start": "3088340",
    "end": "3094890"
  },
  {
    "text": "Now, you want to cut\nthis up into circles. What are you going to do\nwith the regions inside the-- I'm not exactly sure.",
    "start": "3094890",
    "end": "3100850"
  },
  {
    "text": "You probably going to waste\nso much math at that point to figure out what you're doing\nthat, it may not be faster. But this is a pretty\nsubstantial improvement.",
    "start": "3100850",
    "end": "3109580"
  },
  {
    "text": "In other words, think\nabout it this way. Imagine you're running on a\n64-core machine or something",
    "start": "3109580",
    "end": "3115880"
  },
  {
    "text": "like that, or let's just\nsay a 16-core machine. Root P versus P,\nthat's a4x difference. Like you can maintain peak\nutilization with four times less",
    "start": "3115880",
    "end": "3124670"
  },
  {
    "text": "bandwidth using this scheme. That can be a really big deal. So that was an example\nof an optimization",
    "start": "3124670",
    "end": "3132869"
  },
  {
    "text": "to reduce inherent\ncommunication, because that was data\nthat had to move.",
    "start": "3132870",
    "end": "3138330"
  },
  {
    "text": "Often, we're fighting a bunch\nof stuff that is inherent, or artifactual\ncommunication, which kind of",
    "start": "3138330",
    "end": "3144035"
  },
  {
    "text": "comes from the\nfact that we never move one data in a piece\nof data in a computer. We always move a cache line.",
    "start": "3144035",
    "end": "3149309"
  },
  {
    "text": "Or the minimum packet size\nmight be a kilobyte or something like that. So there's always details\nof how a machine works",
    "start": "3149310",
    "end": "3155062"
  },
  {
    "text": "where you're like, that\ncommunication shouldn't be so bad. And then you're like, oh,\ngosh, that was really bad. So let me give you a common\nexample, which is caches.",
    "start": "3155062",
    "end": "3162600"
  },
  {
    "text": "Why we teach caches\nin this class. So think about the caching\nbehavior of the grid solver, OK.",
    "start": "3162600",
    "end": "3169270"
  },
  {
    "text": "And so imagine that I have\na cache that the cache line size is four elements.",
    "start": "3169270",
    "end": "3174430"
  },
  {
    "text": "So each of those blue\nthings is 4 dots wide. So it's four elements. And imagine I have a cache that\nis 6 lines or 24 total grid",
    "start": "3174430",
    "end": "3181860"
  },
  {
    "text": "elements. So imagine that when I'm\ncomputing that red dot there, as a result of\ncomputing that red dot,",
    "start": "3181860",
    "end": "3188560"
  },
  {
    "text": "I read the four\ncardinal neighbors and hopefully, you agree with\nme that these would be the cache",
    "start": "3188560",
    "end": "3193770"
  },
  {
    "text": "lines that would be loaded. So after producing that red dot,\nthose would be the cache lines.",
    "start": "3193770",
    "end": "3199740"
  },
  {
    "text": "I move over horizontally\none element. Will there be any cache misses?",
    "start": "3199740",
    "end": "3206130"
  },
  {
    "text": "No, actually they're\nall cache hits. And now imagine just we proceed\nin the horizontal direction.",
    "start": "3206130",
    "end": "3213609"
  },
  {
    "text": "When we get to the\nend of the row, I've computed all\nof these elements",
    "start": "3213610",
    "end": "3218850"
  },
  {
    "text": "and that is the\nstate of my cache. Remember, my cache\ncan hold six lines. Now I want you to think\nabout what happens",
    "start": "3218850",
    "end": "3227359"
  },
  {
    "text": "when I get back to here. I want to process the red dot.",
    "start": "3227360",
    "end": "3234440"
  },
  {
    "text": "My cash has the\nblue region in it. But not too long ago,\nI had all the data",
    "start": "3234440",
    "end": "3240320"
  },
  {
    "text": "that I needed except for the row\nunderneath the red dot sitting there in cache.",
    "start": "3240320",
    "end": "3247430"
  },
  {
    "text": "And now it's not there. So I'm going to miss\non everything again. That's kind of artifactual\ncommunication, right?",
    "start": "3247430",
    "end": "3254670"
  },
  {
    "text": "Like theoretically,\nI loaded that data. And if I had some\nmagical cache, I'm not communicating it back\nto the processor again.",
    "start": "3254670",
    "end": "3261920"
  },
  {
    "text": "But if you actually\nlooked at this program, you would say no, I'm taking\nall these cache misses",
    "start": "3261920",
    "end": "3267829"
  },
  {
    "text": "every single time I touch data. So if you look\ncarefully, this program,",
    "start": "3267830",
    "end": "3275480"
  },
  {
    "text": "for every four\nelements of output, I load three new cache lines. That's one way to think about\nit, that's probably easier",
    "start": "3275480",
    "end": "3282500"
  },
  {
    "text": "to see here. If I go, well, once I\nget into steady state.",
    "start": "3282500",
    "end": "3288640"
  },
  {
    "text": "So notice here the first\nelement, three cache lines. The second element,\nno new cache lines.",
    "start": "3288640",
    "end": "3295530"
  },
  {
    "text": "The third element\nactually is going to load three more cache lines. But in steady state\nacross the thing",
    "start": "3295530",
    "end": "3301200"
  },
  {
    "text": "I'm going to do for red dots\nfor every three cache lines that I load. OK.",
    "start": "3301200",
    "end": "3307290"
  },
  {
    "text": "So that's my\narithmetic intensity. 4 elements of output\nper 3 cache lines.",
    "start": "3307290",
    "end": "3313500"
  },
  {
    "text": "That's a ratio of\nwork to bandwidth. And there's a lot of examples\nof artifactual communication",
    "start": "3313500",
    "end": "3320099"
  },
  {
    "text": "like cache lines or finite\nsize caches or network traffic that has to be transmitted\non boundaries of size 16",
    "start": "3320100",
    "end": "3329849"
  },
  {
    "text": "and stuff like that. So my blocking\nexample, that tiling,",
    "start": "3329850",
    "end": "3335530"
  },
  {
    "text": "that was an example of\nchanging the assignment of work",
    "start": "3335530",
    "end": "3340740"
  },
  {
    "text": "to processors to reduce\ninherent communication. So when I divided the\nworld up into tiles,",
    "start": "3340740",
    "end": "3347140"
  },
  {
    "text": "I said, OK, if I do a\ndifferent workload balance, you're going to\ncommunicate less.",
    "start": "3347140",
    "end": "3353550"
  },
  {
    "text": "Because inherently, you\nwill communicate less. Now let me give you a\ntechnique for reducing",
    "start": "3353550",
    "end": "3360330"
  },
  {
    "text": "the artifactual\ncommunication, or in fact, maybe you can come\nup with a technique. And instead of running\nacross the array like this,",
    "start": "3360330",
    "end": "3368380"
  },
  {
    "text": "is there a way I can\nchange the program to increase its\narithmetic intensity?",
    "start": "3368380",
    "end": "3374349"
  },
  {
    "text": "Yes. Inherent communication. Inherent communication\nis given the scheme,",
    "start": "3374350",
    "end": "3383220"
  },
  {
    "text": "like given the\nassignment scheme, how much data actually has to\nmove between the processors",
    "start": "3383220",
    "end": "3388920"
  },
  {
    "text": "to actually get the job done? In this example, actually,\nI removed parallelism.",
    "start": "3388920",
    "end": "3394360"
  },
  {
    "text": "So assume that this is all going\non inside of one thread, right? So now I'm saying that\neven inside of one thread,",
    "start": "3394360",
    "end": "3402150"
  },
  {
    "text": "there was before communication\nacross the processors",
    "start": "3402150",
    "end": "3407490"
  },
  {
    "text": "or across the workers. This is communication within\na worker between the processor",
    "start": "3407490",
    "end": "3413670"
  },
  {
    "text": "and its own memory.  So can I do better?",
    "start": "3413670",
    "end": "3419300"
  },
  {
    "text": "Yeah. Maybe if you go across the\ntop and then just move down one and then go right to\nleft like zigzag pattern.",
    "start": "3419300",
    "end": "3425380"
  },
  {
    "text": "So my goal here is when\nI come back to the left, I want to come back quick enough\nbefore the data has fallen out",
    "start": "3425380",
    "end": "3432670"
  },
  {
    "text": "of cache. That's like, what's going on. So if I change the, let\nme jump forward again.",
    "start": "3432670",
    "end": "3438560"
  },
  {
    "text": "If I change the order in which\nI integrate over this array, I compute the same answer.",
    "start": "3438560",
    "end": "3444940"
  },
  {
    "text": "I'm just going to iterate\nin a different order. This is not about\nparallelism again,",
    "start": "3444940",
    "end": "3450460"
  },
  {
    "text": "this is just about\ncommunication. Now, whenever I\nget back to here,",
    "start": "3450460",
    "end": "3456530"
  },
  {
    "text": "some of the data that\nI've already accessed is still valid and in\nmy cache, and I only have to load this one\nnew line below me.",
    "start": "3456530",
    "end": "3464200"
  },
  {
    "text": "And if you carry\nthis pattern out, before it was 4 pieces of\noutput for every 3 lines loaded.",
    "start": "3464200",
    "end": "3473180"
  },
  {
    "text": "This new pattern is\n6 elements of output, for every two lines loaded.",
    "start": "3473180",
    "end": "3479000"
  },
  {
    "text": "So I've again, increased\nmy arithmetic intensity pretty substantially.",
    "start": "3479000",
    "end": "3484099"
  },
  {
    "text": "This is called cache blocking. This is probably the\nmost important technique",
    "start": "3484100",
    "end": "3489710"
  },
  {
    "text": "that you'll ever do in\nany kind of code that involves tensors and\nmatrices and stuff like that.",
    "start": "3489710",
    "end": "3496890"
  },
  {
    "text": "So any modern matrix\nmultiplication, any modern tensor operation.",
    "start": "3496890",
    "end": "3502640"
  },
  {
    "text": "The reason why [INAUDIBLE]\nand N is so fast is because of choosing\nreally good orderings to move over the data.",
    "start": "3502640",
    "end": "3508860"
  },
  {
    "text": "Yeah. Faster if you iterated in\nreverse order across the rows? Like going--",
    "start": "3508860",
    "end": "3514040"
  },
  {
    "text": "If I iterate in reverse\norder across the rows. You mean, all the way across\nand then all the way back?",
    "start": "3514040",
    "end": "3520710"
  },
  {
    "text": "Yeah, that would also be\na reasonable scheme here.",
    "start": "3520710",
    "end": "3526099"
  },
  {
    "text": "Kind of, actually, it would not\nbe a reasonable scheme here. It's a reasonable scheme if\nyou look at this diagram.",
    "start": "3526100",
    "end": "3532050"
  },
  {
    "text": "But imagine N being really wide. If N is really wide, sure,\nyou get a little bit of reuse",
    "start": "3532050",
    "end": "3537680"
  },
  {
    "text": "at the ends, but you're back to\nthis failure mode in the middle. So yes, on this diagram,\nyou look at it and go,",
    "start": "3537680",
    "end": "3544837"
  },
  {
    "text": "yeah, most of the reuse on\nthe ends is about everything. But now imagine N is 1,000. Yes.",
    "start": "3544837",
    "end": "3551500"
  },
  {
    "text": "For our [INAUDIBLE] like our\ncache is like in the order of megabytes, large--",
    "start": "3551500",
    "end": "3557320"
  },
  {
    "text": "At 16k or 32k or\nsomething like that. Yeah. I think it's 32k yeah.",
    "start": "3557320",
    "end": "3562570"
  },
  {
    "text": "Yep. And a matrix can be quite big. Like 2K by 2K matrix\nis blowing out your L3.",
    "start": "3562570",
    "end": "3570160"
  },
  {
    "text": "Yep. Yep. But is there a way for us\nto figure out from the board",
    "start": "3570160",
    "end": "3575950"
  },
  {
    "text": "without us having to think about\nthis is exactly what's going on.",
    "start": "3575950",
    "end": "3581089"
  },
  {
    "text": "This is the exact bandwidth\nwith the cache stuff. Is there a way for us to\nfigure it out just like code,",
    "start": "3581090",
    "end": "3586680"
  },
  {
    "text": "like profile is that? Well, profile will tell you what\nyour arithmetic intensity is.",
    "start": "3586680",
    "end": "3592760"
  },
  {
    "text": "Like tools can say you\nare bandwidth bound, they'll tell you that.",
    "start": "3592760",
    "end": "3598760"
  },
  {
    "text": "But they're not going to\ntell you how to fix it. So here's another example that\npops up in all of your lives",
    "start": "3598760",
    "end": "3604630"
  },
  {
    "text": "very often. So here is a program that\nmight look, I wrote it in C,",
    "start": "3604630",
    "end": "3609880"
  },
  {
    "text": "but it looks a lot like code you\nmight run in NumPy or something like that. So I have a bunch\nof library functions",
    "start": "3609880",
    "end": "3616790"
  },
  {
    "text": "for multiplying arrays\nor adding arrays. This is very much like\nsaxpy from assignment one.",
    "start": "3616790",
    "end": "3623119"
  },
  {
    "text": "But usually, if you\nhave that library, you might do a more complex\ncalculation on arrays",
    "start": "3623120",
    "end": "3628339"
  },
  {
    "text": "by basically performing some\nexpression, which I wrote here in C code.",
    "start": "3628340",
    "end": "3634500"
  },
  {
    "text": "So each one of those\nlibrary functions has the arithmetic intensity\nof basically our thing",
    "start": "3634500",
    "end": "3641270"
  },
  {
    "text": "from the slide before. Like it loads 2 values. It does one math op\nwrites one value.",
    "start": "3641270",
    "end": "3647539"
  },
  {
    "text": "And so any program\nthat is created by composing these\nlibrary functions,",
    "start": "3647540",
    "end": "3654480"
  },
  {
    "text": "if all of the library functions\nhave arithmetic intensity, one third, your program is going\nto have arithmetic intensity,",
    "start": "3654480",
    "end": "3661670"
  },
  {
    "text": "one third. So this is a really\nconvenient way to write code is like you\njust write it like this.",
    "start": "3661670",
    "end": "3668010"
  },
  {
    "text": "And hopefully every one of those\nare these big vector operations. And it's in pretty bad shape.",
    "start": "3668010",
    "end": "3673430"
  },
  {
    "text": "Now here's an example\nwhere I took the program and I rewrote it.",
    "start": "3673430",
    "end": "3678530"
  },
  {
    "text": "I rewrote it by instead of\niterating over all elements and doing the Add and then\niterating over all elements",
    "start": "3678530",
    "end": "3685039"
  },
  {
    "text": "and doing them all, I'm\niterating over all elements, but for every element, I'm\ndoing the entire expression",
    "start": "3685040",
    "end": "3691400"
  },
  {
    "text": "and then writing the result. So why is this better?",
    "start": "3691400",
    "end": "3697100"
  },
  {
    "text": "You're iterating over\n4 arrays instead of 6. So I'm iterating over 4 arrays\ninstead of 6, but in particular.",
    "start": "3697100",
    "end": "3704670"
  },
  {
    "text": "Yeah. Yeah. Yeah. That's a way to say it. Yeah I'm reading the\nvalue from A, B, C,",
    "start": "3704670",
    "end": "3710160"
  },
  {
    "text": "D. I'm computing all of\nthe intermediates, which probably get stored in\na register or in cache.",
    "start": "3710160",
    "end": "3717140"
  },
  {
    "text": "And then I get all the way\ndone with all of that stuff. And then dumped the result out. So as a result as pointed out,\nI'm iterating over far fewer,",
    "start": "3717140",
    "end": "3729980"
  },
  {
    "text": "I'm doing far fewer\nmath operations. So in this case, one math\noperation for every three loads",
    "start": "3729980",
    "end": "3736099"
  },
  {
    "text": "and stores. In this I do 1, 2, 3, 4,\n5 math loads and stores,",
    "start": "3736100",
    "end": "3744200"
  },
  {
    "text": "and I do 1, 2, 3\nmath operations. So my arithmetic intensity has\ngone from one third to 3/5.",
    "start": "3744200",
    "end": "3751430"
  },
  {
    "text": "And given that we know that\nthis is bandwidth bound, my speed up is going\nto be how much I",
    "start": "3751430",
    "end": "3757850"
  },
  {
    "text": "improve arithmetic intensity. So this is a common optimization\nthat a lot of the deep learning",
    "start": "3757850",
    "end": "3765590"
  },
  {
    "text": "compilers are now\nstarting to do for you. You want to write your code\nin terms of adds and moles.",
    "start": "3765590",
    "end": "3771710"
  },
  {
    "text": "And in terms of these\nvector tensor operations. But you sure as heck don't\nwant it to actually run",
    "start": "3771710",
    "end": "3777500"
  },
  {
    "text": "under the hood like that. So in a high level language\nlike TensorFlow or PyTorch, JIT,",
    "start": "3777500",
    "end": "3783230"
  },
  {
    "text": "that's what these\nthings are doing. They're taking your\ntensor program, which looks like these\noperations on vectors,",
    "start": "3783230",
    "end": "3788790"
  },
  {
    "text": "but they want to execute it in\na manner that looks like this. ",
    "start": "3788790",
    "end": "3796470"
  },
  {
    "text": "And then yeah, it's fine. We're not going to get\ninto co-locating threads",
    "start": "3796470",
    "end": "3802230"
  },
  {
    "text": "to share data across\nthreads today. OK. All right. So let's see. how are we doing?",
    "start": "3802230",
    "end": "3807570"
  },
  {
    "text": "11:38. All right. Any other questions? ",
    "start": "3807570",
    "end": "3813030"
  },
  {
    "text": "I have a section on contention. And I actually want to,\nI'll give it two minutes,",
    "start": "3813030",
    "end": "3818530"
  },
  {
    "text": "but I'll let you\nread it offline. I don't think it's completely\non brand for the lecture.",
    "start": "3818530",
    "end": "3825250"
  },
  {
    "text": "One thing I just\nwanted to point out is that what I'm trying to\nstress is at the end of the day,",
    "start": "3825250",
    "end": "3832599"
  },
  {
    "text": "everything you do is going to\nbe ratio of bytes communicated to amount of work you do. Like that's the point\nI want to instill.",
    "start": "3832600",
    "end": "3840540"
  },
  {
    "text": "This section is just\npointing out that sometimes when you do that\ncommunication or when",
    "start": "3840540",
    "end": "3845580"
  },
  {
    "text": "you do that, work matters. So let me just give\nyou one example. Imagine you all come\ninto office hours",
    "start": "3845580",
    "end": "3851640"
  },
  {
    "text": "and you come into\nmy office hours, and the steps would be\nyou take five minutes to walk over to my office.",
    "start": "3851640",
    "end": "3857250"
  },
  {
    "text": "You wait in line, if\nnecessary, and you get your question answered,\nlet's say in five minutes.",
    "start": "3857250",
    "end": "3863640"
  },
  {
    "text": "It's pretty useful office hours. So the latency here is what? It's at least 10 minutes and\nplus your waiting time in line.",
    "start": "3863640",
    "end": "3873259"
  },
  {
    "text": "So the first student that\nshows up in my office hours immediately starts\ntalking to me.",
    "start": "3873260",
    "end": "3879290"
  },
  {
    "text": "They take five\nminutes to walk over. We start working on their\nquestion immediately. So five minutes to walk over.",
    "start": "3879290",
    "end": "3885260"
  },
  {
    "text": "That's the yellow\nbar, five minutes to deal with their question. They're done in 10 minutes. The next student that\narrives at office hours,",
    "start": "3885260",
    "end": "3891770"
  },
  {
    "text": "maybe they come also\nright on time, but right after the start of office hours. They take five minutes to walk.",
    "start": "3891770",
    "end": "3897203"
  },
  {
    "text": "But then they got to wait for\nfive minutes behind somebody else in line and then they\nget their question answered. So it's like basically if\npeople arrive at my office hours",
    "start": "3897203",
    "end": "3905650"
  },
  {
    "text": "all at the same time, there's\ngoing to be contention for me. And the later you come, the\nlonger you're going to wait.",
    "start": "3905650",
    "end": "3914790"
  },
  {
    "text": "It's kind of an inefficient\nthing to have happen. So like the bottom of the slide,\nstudent is kind of screwed.",
    "start": "3914790",
    "end": "3921790"
  },
  {
    "text": "So this is a contention\nfor a shared resource. And so this is a little\nbit of a different concept",
    "start": "3921790",
    "end": "3927817"
  },
  {
    "text": "than everything I've been\ntalking about with bandwidth. It's the amount of\nwork and the amount",
    "start": "3927818",
    "end": "3933700"
  },
  {
    "text": "of travel time of all of\nthese requests are the same. It just so happens that\none's behind the queue.",
    "start": "3933700",
    "end": "3939190"
  },
  {
    "text": "Whereas, instead, if you\nmake an appointment with me and I give you an\nappointment, well, you can be guaranteed that you\ncan be in and out in 10 minutes.",
    "start": "3939190",
    "end": "3946475"
  },
  {
    "text": "So that's actually why I like\nappointment driven office hours because it's a lot more\nefficient for the students.",
    "start": "3946475",
    "end": "3953350"
  },
  {
    "text": "OK, so the time cost of\nstudent is 10 minutes. And so when we were talking\nlast time about contention,",
    "start": "3953350",
    "end": "3961420"
  },
  {
    "text": "we talked about it\nlike the shared my diff variable, the shared queues. Last class, I was talking\nabout how we often",
    "start": "3961420",
    "end": "3968589"
  },
  {
    "text": "like to replicate things\nto avoid contention. And in memory communication the\ncontention is for memory, right?",
    "start": "3968590",
    "end": "3977660"
  },
  {
    "text": "So it could be the case\nwhere if all processors ask for data from memory\nat the same time,",
    "start": "3977660",
    "end": "3983539"
  },
  {
    "text": "some of this beautiful\nlike pipeline diagrams and stuff\nthat I'm drawing maybe don't quite actually\nwork out in practice.",
    "start": "3983540",
    "end": "3990410"
  },
  {
    "text": "So that's why I just wanted\nto put this wrinkle in there and say in practice, usually,\nin many systems there's not",
    "start": "3990410",
    "end": "3996700"
  },
  {
    "text": "a lot of contention. So we think in terms of averages\nlike bytes read per operations.",
    "start": "3996700",
    "end": "4002020"
  },
  {
    "text": "But if all of those\nbytes read happen to happen at the\nsame time, memory could appear to be a lot\nslower than it really",
    "start": "4002020",
    "end": "4009509"
  },
  {
    "text": "is and stuff like that. So people, like in the\nwork queue setting, we talked about different work\nqueues and the memory request",
    "start": "4009510",
    "end": "4017130"
  },
  {
    "text": "scheduling. You'll actually\nsee sometimes it's good to actually randomize\nthings to avoid contention. Like everybody leave\ntheir house and get",
    "start": "4017130",
    "end": "4023340"
  },
  {
    "text": "on the highway at a random time\nand then the highway is more, it's more likely to get peak\nbandwidth and stuff out of it.",
    "start": "4023340",
    "end": "4029560"
  },
  {
    "text": "So just a little aside on when\nstuff happens really, really",
    "start": "4029560",
    "end": "4034650"
  },
  {
    "text": "matters. Yeah. So what did we talk about? We talked about a number\nof different techniques.",
    "start": "4034650",
    "end": "4041740"
  },
  {
    "text": "So we talked about\nreassigning work to workers to reduce inherent\ncommunication.",
    "start": "4041740",
    "end": "4047340"
  },
  {
    "text": "We talked about\nreordering the order that any one worker\ndoes things in order",
    "start": "4047340",
    "end": "4053280"
  },
  {
    "text": "to reduce artifactual\ncommunication, in particular cache locality. And so we gave you just a couple\nof examples of stuff like that.",
    "start": "4053280",
    "end": "4061170"
  },
  {
    "text": "Cool. OK. So let me just finish\nup on just a few tricks.",
    "start": "4061170",
    "end": "4066869"
  },
  {
    "text": "So reminders going\nforward as you get into assignment\nin these assignments. One, is sometimes the\nsimplest dumb static solution",
    "start": "4066870",
    "end": "4075900"
  },
  {
    "text": "with no optimizations is\nfaster than a complex solution. Always try the simplest\nthing first in this class.",
    "start": "4075900",
    "end": "4084270"
  },
  {
    "text": "If it works, great. Move on. Go work on some other class. And then the question\nis when you're not",
    "start": "4084270",
    "end": "4090480"
  },
  {
    "text": "happy with your\nperformance, one question that I always ask myself\nis there an opportunity",
    "start": "4090480",
    "end": "4098189"
  },
  {
    "text": "to do better? So you asked this\nquestion about how do I get measurements to know if\nI should just keep trying?",
    "start": "4098189",
    "end": "4104680"
  },
  {
    "text": "So in assignment\none, you made a lot of measurements that\ntold you whether or not workload was well balanced.",
    "start": "4104680",
    "end": "4109901"
  },
  {
    "text": "And that was really helpful. Because is like if\nit wasn't balanced, you could work a little harder. But what about\nmeasurements about",
    "start": "4109902",
    "end": "4115560"
  },
  {
    "text": "am I bandwidth bound or not? So one technique would be\nto go grab a tool like VTune",
    "start": "4115560",
    "end": "4123000"
  },
  {
    "text": "or any of a\nperformance profiler, and it might just tell you. But there's a lot of other tools\nthat are kind of interesting.",
    "start": "4123000",
    "end": "4130839"
  },
  {
    "text": "Like, let's say\nthat I know that I'm computing at this many\ninstructions per second",
    "start": "4130840",
    "end": "4137250"
  },
  {
    "text": "or in the math like\nthis many megaflops. Well, if I just knew the\ntotal megaflops of my program",
    "start": "4137250",
    "end": "4143310"
  },
  {
    "text": "on my computer, I\ncould ask the question, what fraction of peak megaflops\nam I actually getting?",
    "start": "4143310",
    "end": "4148707"
  },
  {
    "text": "And that might give me a sense\nof whether I should keep trying or go on. So there are these graphs that\nare pretty useful graphs--",
    "start": "4148707",
    "end": "4155589"
  },
  {
    "text": "this is actually something\nworth really knowing-- that is a way to plot\nwhere you are on the curve.",
    "start": "4155590",
    "end": "4162430"
  },
  {
    "text": "So let me explain\nthis graph to you. So the X-axis of the graph\nare different arithmetic",
    "start": "4162430",
    "end": "4169410"
  },
  {
    "text": "intensities. It's labeled\noperational intensity because some people call\narithmetic intensity,",
    "start": "4169410",
    "end": "4175559"
  },
  {
    "text": "operational intensity. It's the same thing. And notice the units\nare flops per byte. So that's math operations\nper byte read from memory.",
    "start": "4175560",
    "end": "4184528"
  },
  {
    "text": "And so these different\npoints on the X-axis are different programs.",
    "start": "4184529",
    "end": "4191700"
  },
  {
    "text": "If I move along the\nX-axis, I'm talking about a program that does\none op for every four bytes from memory.",
    "start": "4191700",
    "end": "4197370"
  },
  {
    "text": "Or on the far left, a\nprogram that does 16 ops for every byte from memory.",
    "start": "4197370",
    "end": "4203300"
  },
  {
    "text": "So this is going to be not\nvery arithmetically intense.",
    "start": "4203300",
    "end": "4208570"
  },
  {
    "text": "This is extremely arithmetically\nintense over here. And so imagine that,\nI guess empirically,",
    "start": "4208570",
    "end": "4216870"
  },
  {
    "text": "you took programs that\nyou manufactured to have those arithmetic intensities.",
    "start": "4216870",
    "end": "4222630"
  },
  {
    "text": "And you ran them on a computer. And when you run\nthem on the computer, you're going to get a\nperformance in terms",
    "start": "4222630",
    "end": "4229650"
  },
  {
    "text": "of operations per second. So gigaflops. ",
    "start": "4229650",
    "end": "4238210"
  },
  {
    "text": "If your program or if your\ncomputer can do, let's say, 16 gigaflops.",
    "start": "4238210",
    "end": "4244889"
  },
  {
    "text": "Has a 1 gigahertz\nclock and has 16 cores.",
    "start": "4244890",
    "end": "4250410"
  },
  {
    "text": "And your program\nis compute bound, you should expect\nto run at what?",
    "start": "4250410",
    "end": "4258230"
  },
  {
    "text": "If you're doing a good\njob and your program runs at 16 gigaflops, you're\nlike this as good as I can do.",
    "start": "4258230",
    "end": "4264949"
  },
  {
    "text": "So this is a plot of all of\nthose different programs, what they actually achieve.",
    "start": "4264950",
    "end": "4270800"
  },
  {
    "text": "And let's see if you can\nexplain the shape of this graph. This is called a roofline graph\nbecause it looks like a roof.",
    "start": "4270800",
    "end": "4278350"
  },
  {
    "text": "So let's start over here. And I just want you to look at\nthis line and this blue line,",
    "start": "4278350",
    "end": "4283530"
  },
  {
    "text": "because this is one computer. The other line is a\ndifferent computer. So it has a different\nroof line plot.",
    "start": "4283530",
    "end": "4289340"
  },
  {
    "text": "But let's just look\nat this blue line. This blue line says that for all\nprograms of arithmetic intensity",
    "start": "4289340",
    "end": "4296050"
  },
  {
    "text": "one or higher, when you\nrun them on this computer, you get the same performance.",
    "start": "4296050",
    "end": "4303240"
  },
  {
    "text": "Does that make sense? And if it does make\nsense to you, why?",
    "start": "4303240",
    "end": "4308874"
  },
  {
    "text": "[INAUDIBLE] So it must be memory bound? Why is that? Because you have\ngreater intensity.",
    "start": "4308874",
    "end": "4315830"
  },
  {
    "text": "So if you were memory bound\nyou should expect to be doing. OK, let's.",
    "start": "4315830",
    "end": "4320849"
  },
  {
    "text": "Let's take a look at that. So these are programs that\naccess memory less and less",
    "start": "4320850",
    "end": "4326240"
  },
  {
    "text": "often, right? Higher arithmetic intensity,\nless frequent memory access.",
    "start": "4326240",
    "end": "4331850"
  },
  {
    "text": "So as I do less and\nless memory access, my performance is unchanged.",
    "start": "4331850",
    "end": "4339170"
  },
  {
    "text": "So wait a minute. Yeah. So anyone want to help? Yeah. I guess that's like the maximum\nclock speed of the processor",
    "start": "4339170",
    "end": "4346600"
  },
  {
    "text": "itself. Like it can't go any faster. So this is basically\nsaying for these programs,",
    "start": "4346600",
    "end": "4352560"
  },
  {
    "text": "I'm doing so many operations\nper unit byte read, my performance is limited\nby what my arithmetic",
    "start": "4352560",
    "end": "4359960"
  },
  {
    "text": "units can crank out. And I'm maxing them out all\nthe way until this point.",
    "start": "4359960",
    "end": "4367190"
  },
  {
    "text": "And then at this point, my\nperformance starts going down. Why is that?",
    "start": "4367190",
    "end": "4372670"
  },
  {
    "text": "Yeah. That point's memory bound. At that point, you can't\nget data to the processor fast enough in order\nto run these ops.",
    "start": "4372670",
    "end": "4381250"
  },
  {
    "text": "And so as you begin to\nlower arithmetic intensity, it takes you more and more time\nto get the data to the processor",
    "start": "4381250",
    "end": "4388650"
  },
  {
    "text": "in order to carry\nout the ops, right? Because before it was like\none byte per op and now",
    "start": "4388650",
    "end": "4393660"
  },
  {
    "text": "I've got to get\ntwo bytes per op. So it takes me half the time. If you look at\nthis as a log plot,",
    "start": "4393660",
    "end": "4398740"
  },
  {
    "text": "I used to get done\nat 16 gigaflops. I double the amount\nof bandwidth per op, which means I double my\nruntime if I'm bandwidth bound,",
    "start": "4398740",
    "end": "4405730"
  },
  {
    "text": "which means I half\nmy performance. Exactly. So if you look carefully at\nthis, the slope of this curve",
    "start": "4405730",
    "end": "4412680"
  },
  {
    "text": "should be the bandwidth. OK, so now imagine\nyou have a program.",
    "start": "4412680",
    "end": "4419970"
  },
  {
    "text": "And somehow you\nlook at your code and you know what your\narithmetic intensity is. You kind of look at the\nmain loop, you're like,",
    "start": "4419970",
    "end": "4426310"
  },
  {
    "text": "OK, it seems to be,\ndid this in program 1. And so you're like, OK, let's\nsay at arithmetic intensity 4.",
    "start": "4426310",
    "end": "4432900"
  },
  {
    "text": "And then you run it and\nyou compute that you're like right here on the graph. What does that mean?",
    "start": "4432900",
    "end": "4439790"
  },
  {
    "text": "Well, you're not going\nas fast as you can. That means that this\nshould not be memory bound. And I'm a good\nfraction off of peak.",
    "start": "4439790",
    "end": "4446830"
  },
  {
    "text": "Something is wrong, right? Maybe I have workload balance. Maybe I have contention.",
    "start": "4446830",
    "end": "4451989"
  },
  {
    "text": "Maybe I have something. So that's like this\ngraph gives you this where do I stand kind of point.",
    "start": "4451990",
    "end": "4459340"
  },
  {
    "text": "This green curve is\njust another processor. The other processor, even\nthough this is called x2 and this is x4, these are\nactually marketing names.",
    "start": "4459340",
    "end": "4466540"
  },
  {
    "text": "This is a processor with a\nfour times more total compute. And so notice that with four\ntimes more total compute,",
    "start": "4466540",
    "end": "4474849"
  },
  {
    "text": "the peak performance is\nabout four times higher. Notice, what do you notice\nabout the memory band bound",
    "start": "4474850",
    "end": "4481020"
  },
  {
    "text": "to compute bound trade off?  You have more operational\nintensity before you--",
    "start": "4481020",
    "end": "4488510"
  },
  {
    "text": "It's the same memory system, but\nwith four times more compute, which means you need\napproximately four times",
    "start": "4488510",
    "end": "4497210"
  },
  {
    "text": "more arithmetic intensity\nto stay bandwidth bound. Or to stay, excuse\nme, compute bound.",
    "start": "4497210",
    "end": "4503449"
  },
  {
    "text": "So if you buy a\ncomputer, you pack it full of parallel capability. That's only going to\nrun at peak performance",
    "start": "4503450",
    "end": "4510120"
  },
  {
    "text": "if you have applications\nthat have the math to use it. And the way you make\nyour application go",
    "start": "4510120",
    "end": "4515810"
  },
  {
    "text": "this way on the chart\nis you do the things I talked about in class. You fuse loops, you\nblock loops, you",
    "start": "4515810",
    "end": "4522350"
  },
  {
    "text": "reorder things to\nreduce communication. So almost everything\nyou do is trying to drive your program this\nway so that your program",
    "start": "4522350",
    "end": "4530059"
  },
  {
    "text": "performance rides\nup on this curve until you hit the flat line. That's basically the name\nof the game of everything.",
    "start": "4530060",
    "end": "4537878"
  },
  {
    "text": "OK, we should stop, but I'll\nanswer these two questions [INAUDIBLE]. ",
    "start": "4537878",
    "end": "4544460"
  },
  {
    "text": "Is there ever any situation\nwhere you want to go left? Like maybe you're really far\nout event operational intensity",
    "start": "4544460",
    "end": "4552060"
  },
  {
    "text": "and maybe-- Is there ever, if\nyou're way out here and you never want\nto go, well, OK.",
    "start": "4552060",
    "end": "4561550"
  },
  {
    "text": "So this curve says\nif you're out here, you're running at\npeak performance.",
    "start": "4561550",
    "end": "4567600"
  },
  {
    "text": "What is your wall clock time? Your wall clock time\nis how much work you're doing times the rate.",
    "start": "4567600",
    "end": "4573900"
  },
  {
    "text": "So if you could\nchange your algorithm and it moves you\nin this direction but keeps you on the roof line,\nand that algorithmic change",
    "start": "4573900",
    "end": "4583290"
  },
  {
    "text": "means you do less work,\nthen you're in good shape. Because I'm running at\nthe same peak performance.",
    "start": "4583290",
    "end": "4589620"
  },
  {
    "text": "And maybe I have something\nthat's algorithmically more efficient and does less work. So this is a plot about\nefficiency and throughput,",
    "start": "4589620",
    "end": "4597180"
  },
  {
    "text": "right? So if you make a change and\nit reduces your arithmetic intensity, but makes you do 10\ntimes fewer things if you stay",
    "start": "4597180",
    "end": "4604770"
  },
  {
    "text": "at high throughput, you're\ndoing 10 times less work at the same throughput,\nthat's good.",
    "start": "4604770",
    "end": "4609880"
  },
  {
    "text": "But if you change your algorithm\nand you do two times less work, like you make a computer\ngraphics algorithm improvement.",
    "start": "4609880",
    "end": "4617010"
  },
  {
    "text": "And that algorithm change\nbrings your arithmetic intensity from here to here,\nit could be a wash.",
    "start": "4617010",
    "end": "4623222"
  },
  {
    "text": "Because you might be\ndoing two times less work, but twice as\nless efficiently.",
    "start": "4623222",
    "end": "4629110"
  },
  {
    "text": "And so maybe it doesn't matter. So that's maybe the reason\nwhy you would move down",
    "start": "4629110",
    "end": "4635969"
  },
  {
    "text": "the curve if you felt like\nthere was an overall benefit. ",
    "start": "4635970",
    "end": "4644000"
  }
]