[
  {
    "start": "0",
    "end": "142000"
  },
  {
    "start": "0",
    "end": "5540"
  },
  {
    "text": "Hi, everyone. Welcome to week three. I hope everyone had\na great weekend.",
    "start": "5540",
    "end": "12290"
  },
  {
    "text": "So today we'll be talking\nabout nonparametric few-shot learning. But before we talk about that,\na few logistics for the class.",
    "start": "12290",
    "end": "19370"
  },
  {
    "text": "First, on Wednesday\nhomework 1 is due, so hopefully you've been able\nto make some progress on that.",
    "start": "19370",
    "end": "25480"
  },
  {
    "text": "And then we'll also release\nhomework 2 on Wednesday. On Wednesday the project\nsurvey is also due.",
    "start": "25480",
    "end": "32090"
  },
  {
    "text": "This isn't binding, but\nreally, the goal of this is for us to get a sense\nfor who your groups are so",
    "start": "32090",
    "end": "37280"
  },
  {
    "text": "that we can assign a\nTA as a project mentor or a point of contact. And that TA will\nessentially be the person",
    "start": "37280",
    "end": "43700"
  },
  {
    "text": "that can help you out with-- help answer questions,\nthat sort of stuff. And by default,\nyou should be going",
    "start": "43700",
    "end": "50810"
  },
  {
    "text": "to your TA mentor's\noffice hours once you get assigned a TA mentor. If you don't fill out\nthe project survey then",
    "start": "50810",
    "end": "56030"
  },
  {
    "text": "we won't assign you a TA mentor\nuntil you submit the project proposal.",
    "start": "56030",
    "end": "62250"
  },
  {
    "text": "On Thursday this week\nwe also have a tutorial on variational inference,\nand the goal of this tutorial is to help potentially prepare\nfor Monday's lecture next week,",
    "start": "62250",
    "end": "72270"
  },
  {
    "text": "which will be on advanced\nBayesian learning methods. And if you're already familiar\nwith variational inference,",
    "start": "72270",
    "end": "78500"
  },
  {
    "text": "then you shouldn't feel\nlike you need to go. But I think that it should\nbe helpful for understanding Monday's lecture.",
    "start": "78500",
    "end": "85800"
  },
  {
    "text": "And Monday's lecture is one that\nwill dive a little bit deeper than some of the\nother lectures, but I think that it will\nalso, in some ways,",
    "start": "85800",
    "end": "93290"
  },
  {
    "text": "give you a better understanding\nof meta-learning algorithms. And then next week the project\nproposal is due on Wednesday,",
    "start": "93290",
    "end": "102027"
  },
  {
    "text": "and so you should be starting\nto think about what you're doing for your final project. And in general, these\nproposals are really--",
    "start": "102027",
    "end": "108470"
  },
  {
    "text": "they're graded very lightly. Really, the goal of\nthe project proposal is to help you kind of have\na milestone for understanding",
    "start": "108470",
    "end": "116840"
  },
  {
    "text": "what your project\nis going to be. And so really, these are\njust for your own benefit, and the grading is also\nfor your own benefit",
    "start": "116840",
    "end": "122240"
  },
  {
    "text": "to help us give you feedback\non your project proposal.  And then also next\nweek there will",
    "start": "122240",
    "end": "128600"
  },
  {
    "text": "be a tutorial on\nvalue-based RL, which will help prepare for\nsome of the RL topics that we're getting\ninto next week.",
    "start": "128600",
    "end": "134540"
  },
  {
    "text": "Any logistical questions? ",
    "start": "134540",
    "end": "142030"
  },
  {
    "start": "142000",
    "end": "207000"
  },
  {
    "text": "OK, cool. So the plan for today. We're going to be covering\na class of methods called nonparametric\nfew-shot learning methods.",
    "start": "142030",
    "end": "149055"
  },
  {
    "text": "These methods are\nactually-- work super well in practice, at least for\nsupervised learning problems. And so they are quite useful.",
    "start": "149055",
    "end": "157250"
  },
  {
    "text": "We'll also talk\nabout a case study of using these methods in\nactual kind of a real world deployment of meta learning,\nwhich is pretty cool.",
    "start": "157250",
    "end": "165480"
  },
  {
    "text": "Then we'll summarize the\ndifferent meta learning algorithms that we've\ncovered so far, including nonparametric methods, and\ndiscuss their pros and cons.",
    "start": "165480",
    "end": "173000"
  },
  {
    "text": "And then I'll end just by giving\na few examples of some more meta learning applications\nand how these algorithms have",
    "start": "173000",
    "end": "179599"
  },
  {
    "text": "been used in practice. The goals by the\nend of the lecture are to kind of get the basics of\nnonparametric few-shot learning",
    "start": "179600",
    "end": "187489"
  },
  {
    "text": "and how to implement them, and\nyou will be implementing them within homework 2. Also, trade offs between\nthe different meta-learning",
    "start": "187490",
    "end": "194390"
  },
  {
    "text": "approaches and then\nfamiliarity with kind of more applied formulations\nof meta-learning and how you can take a\nproblem and formulate it",
    "start": "194390",
    "end": "203090"
  },
  {
    "text": "within the context\nof meta-learning. OK.",
    "start": "203090",
    "end": "208159"
  },
  {
    "start": "207000",
    "end": "452000"
  },
  {
    "text": "So before we dive into\nnonparametric methods, just a quick recap of\nwhat we covered last week.",
    "start": "208160",
    "end": "213980"
  },
  {
    "text": "We talked about first,\nblack-box meta learning methods, which essentially have a\nneural network that take,",
    "start": "213980",
    "end": "219440"
  },
  {
    "text": "as input, a training data set\nand output some parameters or output some statistics.",
    "start": "219440",
    "end": "225260"
  },
  {
    "text": "And then those\nparameters are used to classify new data points. So really, the key idea is\nto parameterize a learner",
    "start": "225260",
    "end": "234240"
  },
  {
    "text": "as a neural network, where\nthis learner is going to be taking in data points directly.",
    "start": "234240",
    "end": "240140"
  },
  {
    "text": "This is great in the sense that\nit's a very expressive process. You can represent many\nlearning procedures.",
    "start": "240140",
    "end": "245209"
  },
  {
    "text": "And the downside is that it's a\nfairly challenging optimization problem, and this\nmight be something that you're finding\nin homework 1.",
    "start": "245210",
    "end": "254410"
  },
  {
    "text": "OK. And then we also talked\nabout optimization based meta-learning. And instead of representing\nthe learning process",
    "start": "254410",
    "end": "259680"
  },
  {
    "text": "as a black-box neural\nnetwork, we instead represented it as an\noptimization process",
    "start": "259680",
    "end": "265830"
  },
  {
    "text": "where we meta-learn the free\nparameters of that optimization process.",
    "start": "265830",
    "end": "271210"
  },
  {
    "text": "And so one\ninstantiation of this is you embed gradient descent into\nthe inner learning process.",
    "start": "271210",
    "end": "276840"
  },
  {
    "text": "And the benefit of\nthis is that you get the structure of\noptimization embedded inside the meta-learner\nand the weakness",
    "start": "276840",
    "end": "283638"
  },
  {
    "text": "is that it typically requires\nsome sort of second order optimization. Now one thing that\nI like to emphasize",
    "start": "283638",
    "end": "289290"
  },
  {
    "text": "in both of these approaches\nis that really the only thing that you're optimizing, per se,\nin both black-box meta learning",
    "start": "289290",
    "end": "296310"
  },
  {
    "text": "and in optimization-based\nmeta learning is the parameters theta. In some ways, phi i is just kind\nof an intermediate byproduct",
    "start": "296310",
    "end": "303870"
  },
  {
    "text": "of this meta learning\nprocess and it's not being actually optimized\nas part of meta learning.",
    "start": "303870",
    "end": "309337"
  },
  {
    "text": "It's something that\nyou're essentially inferring at test time in\norder to solve the task.",
    "start": "309337",
    "end": "314340"
  },
  {
    "text": " OK, any questions on this kind\nof material from last week",
    "start": "314340",
    "end": "321449"
  },
  {
    "text": "before we move on? ",
    "start": "321450",
    "end": "329620"
  },
  {
    "text": "OK, so these optimization\nbased approaches typically require some form of\nsecond order optimization, which makes them--",
    "start": "329620",
    "end": "335505"
  },
  {
    "text": " is one of, essentially, the main\ndownsides of these approaches.",
    "start": "335505",
    "end": "341190"
  },
  {
    "text": "And so today,\nessentially, we want to be able to embed\na learning procedure and get the structure of an\nexisting learning procedure",
    "start": "341190",
    "end": "347790"
  },
  {
    "text": "without requiring a\nsecond order optimization. And so this is\nreally the motivation",
    "start": "347790",
    "end": "354270"
  },
  {
    "text": "behind nonparametric methods. And essentially\nso far we've been learning a\nnonparametric-- we've been",
    "start": "354270",
    "end": "360075"
  },
  {
    "text": "using nonparametric models. We've been using models that\nare parameterized by phi i as kind of the ultimate product\nof the learning procedure.",
    "start": "360075",
    "end": "369300"
  },
  {
    "text": "However, in low data regimes,\nnonparametric methods actually work pretty well and\nthey're quite simple.",
    "start": "369300",
    "end": "376690"
  },
  {
    "text": "So for example,\nnearest neighbors is one possible nonparametric\nlearning procedure",
    "start": "376690",
    "end": "383490"
  },
  {
    "text": "where you don't actually\nnecessarily have any parameters. You are trying to\ncompare examples to the nearest training\ndata point and output",
    "start": "383490",
    "end": "390360"
  },
  {
    "text": "the corresponding label. This gets really expensive when\nyou have a ton of data points, but when you have a small\nnumber of data points,",
    "start": "390360",
    "end": "397020"
  },
  {
    "text": "you can simply\ncompare the example that you have to all your\ntraining data points.",
    "start": "397020",
    "end": "403180"
  },
  {
    "text": "And so for this class of\nmeta-learning methods, we can notice that,\nat meta test time,",
    "start": "403180",
    "end": "409050"
  },
  {
    "text": "we are in this low data\nregime, oftentimes when we have a few examples,\nsmall training data set.",
    "start": "409050",
    "end": "415710"
  },
  {
    "text": "That said, during meta-training\ntime we still want to be parametric because we will\nhopefully have a large number--",
    "start": "415710",
    "end": "421987"
  },
  {
    "text": "a large amount of\ndata that we can use for the meta-training process. And so the idea here\nis, can we learn--",
    "start": "421987",
    "end": "429090"
  },
  {
    "text": "can we use parametric\nmeta learners that produce effective\nnon-parametric learners? ",
    "start": "429090",
    "end": "437332"
  },
  {
    "text": "OK, and it's also\nworth mentioning that some of these\nmethods, in terms of when they're introduced,\nactually precede the approaches",
    "start": "437333",
    "end": "444240"
  },
  {
    "text": "that we've covered so far. OK, so what exactly do I mean by\nusing a parametric meta-learner",
    "start": "444240",
    "end": "450840"
  },
  {
    "text": "to produce a\nnonparametric learner? So let's think about\nnearest neighbors",
    "start": "450840",
    "end": "455970"
  },
  {
    "start": "452000",
    "end": "667000"
  },
  {
    "text": "as our non-parametric learner. And what it's going to look\nlike is if we have a training",
    "start": "455970",
    "end": "461340"
  },
  {
    "text": "data set at test time\nand a test data point, we're just going to compare\nthe test data point to each",
    "start": "461340",
    "end": "467190"
  },
  {
    "text": "of the training examples. So we'll run a comparison\nbetween each of these and we'll find the one that\nis closest to our target image",
    "start": "467190",
    "end": "476160"
  },
  {
    "text": "and output the label of\nthe corresponding image. So we're just going to be\ncomparing the test data",
    "start": "476160",
    "end": "482940"
  },
  {
    "text": "point with our training images. Now, an important\nquestion is, well,",
    "start": "482940",
    "end": "489390"
  },
  {
    "text": "in what space should you\ncompare between the test image and the training images?",
    "start": "489390",
    "end": "496292"
  },
  {
    "text": "One really naive option\nthat you could do is to use L2 distance\nin pixel space.",
    "start": "496292",
    "end": "501932"
  },
  {
    "text": "However, this turns out to be\nactually a really bad metric. So if, for example, you take--",
    "start": "501932",
    "end": "507870"
  },
  {
    "text": "say the image on the\nright is your test example and the two images\non the left are what you're comparing against.",
    "start": "507870",
    "end": "513360"
  },
  {
    "text": "If you compare to these two\nimages nearest neighbors-- sorry. L2 distance in pixel\nspace will actually",
    "start": "513360",
    "end": "518669"
  },
  {
    "text": "tell you that the\nimage on the left is closer than the\nimage on the right.",
    "start": "518669",
    "end": "524298"
  },
  {
    "text": "And so this illustrates,\nessentially, why L2 distance can be really\nterrible in image space. It can also be terrible\nin other spaces as well.",
    "start": "524298",
    "end": "531910"
  },
  {
    "text": "So we probably shouldn't use\nL2 distance in pixel space. Does anyone have any ideas\nthat for distance metrics",
    "start": "531910",
    "end": "537510"
  },
  {
    "text": "that we might use instead? Yeah? We could run a\nResNet on ImageNet and then take [INAUDIBLE].",
    "start": "537510",
    "end": "545620"
  },
  {
    "text": "Correct. We could use features from\na model trained on ImageNet.",
    "start": "545620",
    "end": "550839"
  },
  {
    "text": "Any other ideas? Yeah? So train a self-supervised\nmodel and make that [INAUDIBLE]..",
    "start": "550840",
    "end": "556680"
  },
  {
    "text": "We could also use a\nmodel that was trained in a self-supervised\nway instead of trained in an unsupervised way.",
    "start": "556680",
    "end": "563140"
  },
  {
    "text": "Any other thoughts? ",
    "start": "563140",
    "end": "569190"
  },
  {
    "text": "Yeah? Give it contrastive learning\nI guess also [INAUDIBLE]",
    "start": "569190",
    "end": "575384"
  },
  {
    "text": "maybe some augmented and some\nkind of matched images that are things similarly augmented\nand increase the distance",
    "start": "575384",
    "end": "582470"
  },
  {
    "text": "for [INAUDIBLE],,\nsomething like that. Yeah. So you try to do some sort\nof contrastive learning that essentially\npulls together images",
    "start": "582470",
    "end": "587873"
  },
  {
    "text": "that have a similar label\nand pushes apart images that have a different label. And you could do this either--",
    "start": "587873",
    "end": "594360"
  },
  {
    "text": "in a labeled setting it's\nfairly easy to do this. You could also do it\nin unlabeled setting, potentially, if you make\ncertain assumptions.",
    "start": "594360",
    "end": "603960"
  },
  {
    "text": "So all these are\nkind of examples of actually trying\nto learn a distance metric in various ways.",
    "start": "603960",
    "end": "609370"
  },
  {
    "text": "And the key idea behind kind of\nnonparametric few-shot learning methods are is to actually\nlearn a metric like this,",
    "start": "609370",
    "end": "617370"
  },
  {
    "text": "except learn a metric in\na way that is explicitly optimizing for good performance\nat this meta test task.",
    "start": "617370",
    "end": "625260"
  },
  {
    "text": "And so we want to essentially\nuse the meta training data to learn how to compare images.",
    "start": "625260",
    "end": "633360"
  },
  {
    "text": "Now, a really simple\nway to do this is very similar to a\ncontrastive technique where essentially\nwhat we'll do is we'll",
    "start": "633360",
    "end": "640050"
  },
  {
    "text": "just train a network to take\nas input two images and output whether those images are\nthe same class or not.",
    "start": "640050",
    "end": "645667"
  },
  {
    "text": "And so if these are\ndifferent classes, you train it to output a 0. If they are images\nof the same class,",
    "start": "645667",
    "end": "651600"
  },
  {
    "text": "like they're both\nimages of bulls, you train it to output a\n1, and so on and so forth.",
    "start": "651600",
    "end": "656620"
  },
  {
    "text": "You're essentially passing\nthese pairs of images and train it to output whether\nor not they're the same label. And this is essentially\ngoing to look",
    "start": "656620",
    "end": "663690"
  },
  {
    "text": "a lot like a contrastive\nlearning approach. ",
    "start": "663690",
    "end": "668770"
  },
  {
    "start": "667000",
    "end": "1094000"
  },
  {
    "text": "All right. So this is\nessentially the basics of a nonparametric\nmeta-learning method.",
    "start": "668770",
    "end": "675910"
  },
  {
    "text": "This is referred to as\nSiamese neural networks. And then once you\ntrain a network like this that tells you\nwhether two images are",
    "start": "675910",
    "end": "682680"
  },
  {
    "text": "from the same class,\nthen at meta test time, you simply compare\nx test to each",
    "start": "682680",
    "end": "688652"
  },
  {
    "text": "of the images in\nyour training data set and output the label\ncorresponding to the one",
    "start": "688652",
    "end": "695160"
  },
  {
    "text": "that you're most confident\nis the same class. Yeah? So in this context,\nnonparametric just",
    "start": "695160",
    "end": "701100"
  },
  {
    "text": "means that we're not creating\nor optimizing parameters at testing time even though\nwe are still using parameters",
    "start": "701100",
    "end": "707482"
  },
  {
    "text": "at testing time. Yeah. So the question is\nessentially, does nonparametric mean that we're\nnot using any parameters at--",
    "start": "707482",
    "end": "716790"
  },
  {
    "text": "we're not creating a classifier\nwith parameters at meta test time, but we're still using\nparameters in this thought",
    "start": "716790",
    "end": "722269"
  },
  {
    "text": "process. And that's exactly right. Yeah? For the Siamese\nnetwork that predicts",
    "start": "722270",
    "end": "728520"
  },
  {
    "text": "if two images are\nthe same class, is that trained\nseparately before the meta-learning process? Is that trained along with\nthe meta-learning process?",
    "start": "728520",
    "end": "736060"
  },
  {
    "text": "So training the Siamese\nnetwork directly corresponds to the meta-training\nprocess, essentially.",
    "start": "736060",
    "end": "741360"
  },
  {
    "text": "And then at meta-test time\nyou deploy that classifier to run all these comparisons.",
    "start": "741360",
    "end": "747990"
  },
  {
    "text": "Yeah? Speaking of data, it assumes\nthat we have the same label space, right?",
    "start": "747990",
    "end": "753636"
  },
  {
    "text": " So that you're\nasking this assumes that we have the same label\nspace between training",
    "start": "753636",
    "end": "761130"
  },
  {
    "text": "and testing? Yeah. Yeah, so this is specific\nto classification",
    "start": "761130",
    "end": "768282"
  },
  {
    "text": "and it's assuming\nthat you're going to be doing some n-way\nclassification problem at meta",
    "start": "768282",
    "end": "774690"
  },
  {
    "text": "test time and that your\nmeta-training data is labeled with class labels.",
    "start": "774690",
    "end": "780063"
  },
  {
    "text": "So do your meta test labels\nhave to use the same classes as before? Oh, that's a good question. So do the meta test classes\nhave to have the same class",
    "start": "780063",
    "end": "789360"
  },
  {
    "text": "labels as what you saw\nduring meta training? And it doesn't actually have\nto have the same class labels.",
    "start": "789360",
    "end": "796510"
  },
  {
    "text": "So this is actually a binary\nclassification problem right here, and it's just\ntelling you whether or not",
    "start": "796510",
    "end": "802200"
  },
  {
    "text": "they're the same class. And so if you then give it\nimages of new classes of images from entirely new classes that\nweren't seen during training,",
    "start": "802200",
    "end": "809760"
  },
  {
    "text": "the classifier should still be\nable to generalize and tell you whether or not they're\nthe same class. And so you don't have\nto have the same class",
    "start": "809760",
    "end": "816360"
  },
  {
    "text": "labels during meta training\nand meta test time. ",
    "start": "816360",
    "end": "822450"
  },
  {
    "text": "OK. Great. So this is like v0 of\nnonparametric few-shot learning",
    "start": "822450",
    "end": "828240"
  },
  {
    "text": "techniques. And at meta training\ntime you're doing this binary classification\nand at meta test time,",
    "start": "828240",
    "end": "833760"
  },
  {
    "text": "you're doing an n\nway classification by running these pairwise\nqueries or comparisons.",
    "start": "833760",
    "end": "839580"
  },
  {
    "text": "Yeah? Well, meta-testing,\nit's steadily comparing images\nwith each image, whether for training or not.",
    "start": "839580",
    "end": "845790"
  },
  {
    "text": "Not on test data but\nwasn't that [INAUDIBLE]",
    "start": "845790",
    "end": "851259"
  },
  {
    "text": "Yeah. So I should clarify,\nyou're comparing it to each image of the\nsupport set or the training",
    "start": "851260",
    "end": "857250"
  },
  {
    "text": "set of your test\ntask, and this is separate from your\nmeta-training data. So you'll use all of\nthe training tasks, all",
    "start": "857250",
    "end": "864449"
  },
  {
    "text": "the meta training tasks\nthat you have to train this binary classifier. And then at test time,\nyou'll be given a new task",
    "start": "864450",
    "end": "870540"
  },
  {
    "text": "and a small amount of images-- of training images for that\nnew task from new classes and you'll be\nrunning comparisons",
    "start": "870540",
    "end": "879120"
  },
  {
    "text": "on that small amount\nof label data. Yeah? [INAUDIBLE] just\nduring meta test,",
    "start": "879120",
    "end": "885300"
  },
  {
    "text": "would we just compare\nthe test result with each data induction\nin the concept?",
    "start": "885300",
    "end": "890530"
  },
  {
    "text": "So would it be\nvarying [INAUDIBLE]?? I mean, if you\nincrease the number",
    "start": "890530",
    "end": "895860"
  },
  {
    "text": "of classes in the training\nset and [INAUDIBLE].. ",
    "start": "895860",
    "end": "903520"
  },
  {
    "text": "Yeah. So the question is, does\nthis get very expensive if you have a very large\nsupport set of your test task--",
    "start": "903520",
    "end": "909527"
  },
  {
    "text": "a very large training data\nset for your test task? And yes, it does\nget very expensive. It will increase linearly as\nthe size of your training set",
    "start": "909527",
    "end": "916830"
  },
  {
    "text": "grows. In few-shot learning\nscenarios where we only have a few training\nexamples at test time,",
    "start": "916830",
    "end": "921990"
  },
  {
    "text": "it's still very\npractical to run. But if you want to\nmeta-learn a task that has a large amount of\ntraining data per task,",
    "start": "921990",
    "end": "929280"
  },
  {
    "text": "this might not be\nthe best approach. Yeah?",
    "start": "929280",
    "end": "934440"
  },
  {
    "text": "So just to clarify, I\nguess at meta-train time, we don't have any\ntask structure, right? It's just a bunch of parameters\nwhere you're given learning",
    "start": "934440",
    "end": "942149"
  },
  {
    "text": "[INAUDIBLE]. Yeah, exactly. So I guess the question\nis there isn't actually",
    "start": "942150",
    "end": "947430"
  },
  {
    "text": "any explicit task structure\nduring the meta-training process, and that's right.",
    "start": "947430",
    "end": "952885"
  },
  {
    "text": "We'll actually see\nin the next approach that we'll actually leverage the\ntask structure more explicitly, but here you're actually\nkind of breaking down",
    "start": "952885",
    "end": "958680"
  },
  {
    "text": "the task structure and just\nusing the classification labels. Yeah? If I give you two shots\nas well, would that--",
    "start": "958680",
    "end": "964950"
  },
  {
    "text": "would've give any\nadvantage, right? At test time. Good question. So what if you had more\nthan one example per class?",
    "start": "964950",
    "end": "974040"
  },
  {
    "text": "You had two shots per\nclass, for example. Does that give\nyou any advantage?",
    "start": "974040",
    "end": "979709"
  },
  {
    "text": "So I guess, does anyone have\nany thoughts on this question? Yeah? Well, if you take the two\nshots from the [INAUDIBLE]",
    "start": "979710",
    "end": "986340"
  },
  {
    "text": "and then you\nacknowledge the answers, you have all [INAUDIBLE]\nbecause of [INAUDIBLE]..",
    "start": "986340",
    "end": "991595"
  },
  {
    "text": "Or that you're more likely to\nbe closer to your expected test performance on this\nspecific task at this point.",
    "start": "991595",
    "end": "998740"
  },
  {
    "text": "And for the question\nbefore, I don't think it's similar to breaking\ndown the task structure",
    "start": "998740",
    "end": "1004220"
  },
  {
    "text": "because, for example,\nevery training data set has different labels.",
    "start": "1004220",
    "end": "1009529"
  },
  {
    "text": "Like, for example,\nthe [INAUDIBLE] will never appear\n[INAUDIBLE] for this one. Then you still have distinct\ntasks and you can treat this--",
    "start": "1009530",
    "end": "1019198"
  },
  {
    "text": "you can compare the\nimages across soft images. Yeah. So I guess you're--\nthere's two things here. In terms of the tasks\nstructure, you're",
    "start": "1019198",
    "end": "1025250"
  },
  {
    "text": "essentially making the tasks\njust be binary classification tasks during meta training. So there is that structure\nand that you have",
    "start": "1025250",
    "end": "1031220"
  },
  {
    "text": "a binary classification task. But going back to\nthe number of shots question where you have\nmultiple data points,",
    "start": "1031220",
    "end": "1037707"
  },
  {
    "text": "there are a few different\nways that you can handle this. The naive way to\nhandle it is just to treat the shots\nindependently from one another.",
    "start": "1037707",
    "end": "1043819"
  },
  {
    "text": "And you still do get\na benefit from having multiple shots in that\ncase because if it's closer to one of them than\nthe other, for example,",
    "start": "1043819",
    "end": "1052490"
  },
  {
    "text": "then it will actually\nstill output that label. And so you still do get benefits\nfrom having multiple shots",
    "start": "1052490",
    "end": "1058460"
  },
  {
    "text": "even if you treat\nthem independently. That said, there are better\nways to handle multiple shots that we'll handle in\nthe next couple slides.",
    "start": "1058460",
    "end": "1064865"
  },
  {
    "text": " OK.",
    "start": "1064865",
    "end": "1070430"
  },
  {
    "text": "So one thing that\nyou might notice here is that meta training\nand meta test time, they're doing different things.",
    "start": "1070430",
    "end": "1076260"
  },
  {
    "text": "And one of the things we\nsaw in the previous lecture is we often want\nto kind of match what we're doing,\nwhat we're training",
    "start": "1076260",
    "end": "1082100"
  },
  {
    "text": "for at meta-train time, and\nwhat we're ultimately going to be doing at meta-test time. And so we want to\ncreate a procedure that",
    "start": "1082100",
    "end": "1090429"
  },
  {
    "text": "actually matches what happens\nat meta-training and meta-test time. Now, the way that we\ncan do this is instead",
    "start": "1090430",
    "end": "1098120"
  },
  {
    "start": "1094000",
    "end": "1560000"
  },
  {
    "text": "of doing binary classification\nduring meta-training, we can actually do n\nway classification,",
    "start": "1098120",
    "end": "1103670"
  },
  {
    "text": "doing nearest neighbors in\nsome learned embedding space. And what this will look\nlike is this diagram",
    "start": "1103670",
    "end": "1109280"
  },
  {
    "text": "right here where each of\nour training examples-- say we're doing four\nway classification.",
    "start": "1109280",
    "end": "1114470"
  },
  {
    "text": "Each of our training\nexamples are here. We compute an embedding for\neach of those four examples and then we do nearest\nneighbors with our test example",
    "start": "1114470",
    "end": "1122779"
  },
  {
    "text": "in a differentiable way such\nthat we output the label",
    "start": "1122780",
    "end": "1129740"
  },
  {
    "text": "corresponding to the thing\nthat we're closest to. So in this case,\nmaybe it decides that the weight for this\nexample is the highest",
    "start": "1129740",
    "end": "1136430"
  },
  {
    "text": "and so it will\noutput, essentially, a softmax distribution\nover those weights.",
    "start": "1136430",
    "end": "1143570"
  },
  {
    "text": "And so in particular,\nmathematically, what this looks like is we will\ncompute embeddings",
    "start": "1143570",
    "end": "1149360"
  },
  {
    "text": "straight to the examples and\nthen we'll compare them using, essentially, this network. We're essentially\ngoing to be computing",
    "start": "1149360",
    "end": "1155935"
  },
  {
    "text": "the similarity between\neach of the training examples and our test example.",
    "start": "1155935",
    "end": "1161690"
  },
  {
    "text": "We'll look at each of the-- the label for each of those\ncorresponding training examples.",
    "start": "1161690",
    "end": "1168550"
  },
  {
    "text": "And then we will have\ny tests correspond to this dot product\nbetween the weights",
    "start": "1168550",
    "end": "1178780"
  },
  {
    "text": "and the corresponding labels.  In terms of the architecture,\nthere's a number of details",
    "start": "1178780",
    "end": "1185860"
  },
  {
    "text": "that you can do here. This architecture uses\na convolutional encoder for the images.",
    "start": "1185860",
    "end": "1191150"
  },
  {
    "text": "It also uses a\nbidirectional LSTM so that the embeddings\nof each example doesn't just depend\non the example itself,",
    "start": "1191150",
    "end": "1196510"
  },
  {
    "text": "but also depends on\nthe other examples. But those are essentially\nmore sophisticated choices that you can do.",
    "start": "1196510",
    "end": "1202300"
  },
  {
    "text": "But really, the key idea here\nis that if we train this end to end, then we\ncan actually train",
    "start": "1202300",
    "end": "1207640"
  },
  {
    "text": "for nearest neighbors\nin n way classification to give you the right\nanswer rather than training",
    "start": "1207640",
    "end": "1213460"
  },
  {
    "text": "for this kind of-- the binary classifier\non the previous slide.",
    "start": "1213460",
    "end": "1219650"
  },
  {
    "text": "Yeah? [INAUDIBLE] matching\n[INAUDIBLE] attention.",
    "start": "1219650",
    "end": "1225566"
  },
  {
    "text": "Or [INAUDIBLE] Yeah. So it essentially looks\na lot like attention.",
    "start": "1225566",
    "end": "1232570"
  },
  {
    "text": "Attention is essentially\njust a dot product. It's like a really\nfancy way to say that we're going to\nbe doing dot products, and you can see essentially what\nlooks a lot like a dot product",
    "start": "1232570",
    "end": "1241030"
  },
  {
    "text": "right there where we're going\nto be computing this similarity metric between the test example\nand each of the training",
    "start": "1241030",
    "end": "1247419"
  },
  {
    "text": "examples. And then, according to\nthat equation there, taking that similarity function,\nmultiplying it by the label,",
    "start": "1247420",
    "end": "1254740"
  },
  {
    "text": "and then summing to get a\ndistribution over labels. Yeah?",
    "start": "1254740",
    "end": "1260460"
  },
  {
    "text": "[INAUDIBLE],, but\nthis method is really dependent on the training data\nand how similar the classes are to the training data.",
    "start": "1260460",
    "end": "1266910"
  },
  {
    "text": "And it seems this potentially\nfixes that reliance on data",
    "start": "1266910",
    "end": "1272500"
  },
  {
    "text": "because you're doing\nthem all at once. Where it's like-- I don't know if I\nsaid that correctly,",
    "start": "1272500",
    "end": "1278050"
  },
  {
    "text": "but in the first\none, you have things that are very similar but\nin different categories because you have very\nsimilar categories.",
    "start": "1278050",
    "end": "1285260"
  },
  {
    "text": "That could really mess you up. And this one might\nbe [INAUDIBLE] understand how thing were\n[INAUDIBLE] gets them",
    "start": "1285260",
    "end": "1290380"
  },
  {
    "text": "all at once. Yeah. That's a great question. So essentially, the\nquestion is that if you",
    "start": "1290380",
    "end": "1297130"
  },
  {
    "text": "have some classes that are more\nfine grained than others, then the Siamese classifier,\nit doesn't necessarily",
    "start": "1297130",
    "end": "1303250"
  },
  {
    "text": "know for a given class if it's\na very fine grained class or not a very fine grained class. And then when you do\ncomparisons only pairwise,",
    "start": "1303250",
    "end": "1309520"
  },
  {
    "text": "it has to essentially\nguess how fine grained the class is or not. But in this case,\nyou're actually passing in the entire--",
    "start": "1309520",
    "end": "1315130"
  },
  {
    "text": "all of the classes into\nthe network, and this allows the network\nto actually look at what the other classes\nlook like, realize",
    "start": "1315130",
    "end": "1321460"
  },
  {
    "text": "that they're breeds of dogs\nand not dogs, for example-- like dogs versus cats-- and use that to determine how\nfine grained of a comparison",
    "start": "1321460",
    "end": "1328570"
  },
  {
    "text": "it wants to do. And so looking at the entire--",
    "start": "1328570",
    "end": "1334270"
  },
  {
    "text": "all of the classes\nin the problem actually allows it to\ntailor how narrowly it",
    "start": "1334270",
    "end": "1341080"
  },
  {
    "text": "will be defining a class. Yeah? How will you surmise\nwe'll include this form",
    "start": "1341080",
    "end": "1347419"
  },
  {
    "text": "in some low dimensional space? So I think the\nquestion is that, what",
    "start": "1347420",
    "end": "1352601"
  },
  {
    "text": "if it's not images,\nwhat if these are low dimensional data points? Yeah. So certainly you\ncan essentially--",
    "start": "1352602",
    "end": "1358720"
  },
  {
    "text": "this is an image\nclassification example, but you can also do other\nclassification problems where this corresponds\nto text or maybe it",
    "start": "1358720",
    "end": "1366225"
  },
  {
    "text": "corresponds to tabular data\nor something like that, and you probably wouldn't\nuse a convolutional network if it's tabular data\nor something like that.",
    "start": "1366225",
    "end": "1373850"
  },
  {
    "text": "But you can use the\nsame sort of idea, and, yeah, the main\nideas will still apply.",
    "start": "1373850",
    "end": "1380000"
  },
  {
    "text": "Yeah. What is the goal of the LSTM? Is it they have some sort\nof [INAUDIBLE] training?",
    "start": "1380000",
    "end": "1388059"
  },
  {
    "text": "Yeah, that's a great question. They use an LSTM here,\nand it does actually impose some form of ordering\nwhich isn't a great design",
    "start": "1388060",
    "end": "1394970"
  },
  {
    "text": "choice. It would probably be better\nto use an encoder that doesn't actually depend on the\norder, like attention and so",
    "start": "1394970",
    "end": "1401110"
  },
  {
    "text": "forth. This was back in 2016 and\nso things like transformers weren't as popular back then--",
    "start": "1401110",
    "end": "1406845"
  },
  {
    "text": "or I think the transformer\nofficially didn't really exist back then and LSTMs were\none of the more popular things",
    "start": "1406845",
    "end": "1413020"
  },
  {
    "text": "to operate on sets. But it does impose an ordering\nand there are probably other architectures\nthat could try",
    "start": "1413020",
    "end": "1418054"
  },
  {
    "text": "to take into account all of\nthe information of the training data points without\nimposing an ordering.",
    "start": "1418055",
    "end": "1424008"
  },
  {
    "text": "Yeah? Out of curiosity, I'm wondering\nhow this meta learning approach compares to taking,\nlike, let's say",
    "start": "1424009",
    "end": "1429700"
  },
  {
    "text": "a lot of images at train\ntime and then choosing fewer classes at test time?",
    "start": "1429700",
    "end": "1435659"
  },
  {
    "text": "So the question is,\nwhat if you take a lot of classes\nat training time and then fewer\nclasses at test time?",
    "start": "1435660",
    "end": "1441789"
  },
  {
    "text": "Yeah, [INAUDIBLE]. Yeah. So one of the things\nthat's important",
    "start": "1441790",
    "end": "1447010"
  },
  {
    "text": "here is if you want to be able\nto generalize to new image classes that aren't in\nyour set, the set of things",
    "start": "1447010",
    "end": "1453340"
  },
  {
    "text": "that you saw previously,\nthen if you just train like a 1000-way\nclassifier or something and you see a new image\nclass, it's not necessarily",
    "start": "1453340",
    "end": "1459890"
  },
  {
    "text": "going to actually correspond to\nany of those thousand things. And the classifier may do\nunexpected things in that case",
    "start": "1459890",
    "end": "1466420"
  },
  {
    "text": "because it's sort of like\nan undefined behavior. That said, you can\nstill use the embedding",
    "start": "1466420",
    "end": "1471600"
  },
  {
    "text": "of a supervised classifier\nand do nearest neighbors in that embedding. And that kind of\napproach more resembles",
    "start": "1471600",
    "end": "1478740"
  },
  {
    "text": "these kind of meta\nlearning approaches here. And then of course, if the\ntask classes that you have",
    "start": "1478740",
    "end": "1485430"
  },
  {
    "text": "are lying within\nthe kind of classes that you see during training,\nthen that definitely makes sense to use\nsupervised learning",
    "start": "1485430",
    "end": "1490770"
  },
  {
    "text": "rather than to\nuse meta learning. It's really when you\nhave new classes. Yeah?",
    "start": "1490770",
    "end": "1496077"
  },
  {
    "text": "Is that scenario where\nyou have new classes, wouldn't that highlight some\nof the distribution assumptions",
    "start": "1496077",
    "end": "1503220"
  },
  {
    "text": "that we made in\nthe beginning of, like, it was last week for\na similar test, meta test,",
    "start": "1503220",
    "end": "1509073"
  },
  {
    "text": "and looking at the training\ndata for the same distribution of things? Yeah. So the question is, when\nwe introduce new classes,",
    "start": "1509073",
    "end": "1514980"
  },
  {
    "text": "are we violating this\nkind of assumption that the training\nand test tasks come from the same distribution?",
    "start": "1514980",
    "end": "1521730"
  },
  {
    "text": "It's a little bit\ndifficult to say, actually. In some ways, as\nlong as the classes themselves are coming\nfrom the same distribution",
    "start": "1521730",
    "end": "1529110"
  },
  {
    "text": "and you have enough\ntraining classes, then you should be fine. You should be able to\ngeneralize to new classes.",
    "start": "1529110",
    "end": "1536382"
  },
  {
    "text": "It becomes more of\na problem if you have a small number of\nclasses because then it's",
    "start": "1536382",
    "end": "1542100"
  },
  {
    "text": "less clear what is in\ndistribution or not. ",
    "start": "1542100",
    "end": "1549420"
  },
  {
    "text": "OK, so one of the\nthings you can note here is unlike the Siamese\nnetworks, meta-train time and meta-test time\nmatch so we're",
    "start": "1549420",
    "end": "1554733"
  },
  {
    "text": "doing n way classification\nboth during meta-training and during meta-testing. ",
    "start": "1554733",
    "end": "1561710"
  },
  {
    "start": "1560000",
    "end": "1700000"
  },
  {
    "text": "OK, so in terms\nof the algorithm, we can take the\nalgorithm that we learned before from the\nblack-box approach where",
    "start": "1561710",
    "end": "1568000"
  },
  {
    "text": "we sample tasks, sample\ndata sets, and then learn from those-- learn\nfrom the training data.",
    "start": "1568000",
    "end": "1573790"
  },
  {
    "text": "And what changes is these\ntwo steps right here. ",
    "start": "1573790",
    "end": "1578920"
  },
  {
    "text": "So instead of computing\nsome parameters for solving that\ntask, we're instead going to be computing labels\naccording to nearest neighbors",
    "start": "1578920",
    "end": "1588640"
  },
  {
    "text": "in some embedding space. And, as we talked\nabout very briefly,",
    "start": "1588640",
    "end": "1595150"
  },
  {
    "text": "because we're essentially\nintegrating out these parameters\nwe're not actually representing phi directly.",
    "start": "1595150",
    "end": "1601539"
  },
  {
    "text": "This is why it's referred to\nas a nonparametric approach is that we don't have parameters.",
    "start": "1601540",
    "end": "1608290"
  },
  {
    "text": "And then in terms\nof actually updating in the meta-learning\nprocess, we update",
    "start": "1608290",
    "end": "1613690"
  },
  {
    "text": "the parameters of our embedding\nfunction such that the nearest neighbors process produces\nthe correct answer.",
    "start": "1613690",
    "end": "1620455"
  },
  {
    "text": " Cool.",
    "start": "1620455",
    "end": "1626850"
  },
  {
    "text": "So from there-- yeah? Hey, one quick question. Why is it a sum\nand not an argmax?",
    "start": "1626850",
    "end": "1634009"
  },
  {
    "text": "So it's a sum in the sense that\nusually your classifier outputs a distribution over\nclasses and then you'd use a cross entropy loss\nor something to compare",
    "start": "1634010",
    "end": "1641450"
  },
  {
    "text": "the distributions. In terms of actually\npredicting the label, I guess I should maybe\nrepresent that as a p over y",
    "start": "1641450",
    "end": "1648740"
  },
  {
    "text": "test rather than\ny test directly. OK. And then in terms of practice\nyou're actually using this, you would actually take the\nargmax rather than a sum.",
    "start": "1648740",
    "end": "1655970"
  },
  {
    "text": "OK.  Cool. So now one last\nthing is one thing",
    "start": "1655970",
    "end": "1661890"
  },
  {
    "text": "that came up before\nis, what if you have more than one shot or\nmore than one training example?",
    "start": "1661890",
    "end": "1666899"
  },
  {
    "text": "In both of the\nprevious approaches, we were treating each\nexample from a class",
    "start": "1666900",
    "end": "1672480"
  },
  {
    "text": "completely independently. And so if we had five\nexamples from the same class, we would just be comparing\nto each of those five images",
    "start": "1672480",
    "end": "1678510"
  },
  {
    "text": "and not doing any sort\nof kind of aggregation of the information.",
    "start": "1678510",
    "end": "1683530"
  },
  {
    "text": "And so one last\nthing that we can do to make this\napproach better is",
    "start": "1683530",
    "end": "1689010"
  },
  {
    "text": "instead of performing\ncomparisons independently to all of the examples\nwithin a class, you can actually try to\naggregate class information",
    "start": "1689010",
    "end": "1696389"
  },
  {
    "text": "into a single embedding\nof that particular class. ",
    "start": "1696390",
    "end": "1701500"
  },
  {
    "text": "And so this is\nessentially what that looks like is instead\nof doing comparisons to all of these\ngreen data points and all the blue data points\nand all the orange data",
    "start": "1701500",
    "end": "1708606"
  },
  {
    "text": "points independently,\nwhat you'll try to do is aggregate\nin some embedding space by just averaging the\nembeddings of those examples",
    "start": "1708607",
    "end": "1717899"
  },
  {
    "text": "and then take the nearest\nneighbors to the class embeddings rather than\nthe example embeddings.",
    "start": "1717900",
    "end": "1724963"
  },
  {
    "text": "So in some ways, this\nlooks a little bit like k means in a sense. It's not actually an\nunsupervised procedure.",
    "start": "1724963",
    "end": "1730500"
  },
  {
    "text": "It's still kind of a\nmeta training procedure. But we're going to be computing\nthese prototypes or these kind of cluster centers and then\ncomparing to those prototypes.",
    "start": "1730500",
    "end": "1741330"
  },
  {
    "text": "And so mathematically what\nthis looks like is we'll be computing these\nprototypes, which are just",
    "start": "1741330",
    "end": "1748980"
  },
  {
    "text": "taking all of the examples\nfor a particular class and averaging the\nembedding of that class.",
    "start": "1748980",
    "end": "1754769"
  },
  {
    "text": "And then once we have these\nprototypes, we will then do-- we'll kind of compare using\nsome distance function.",
    "start": "1754770",
    "end": "1762040"
  },
  {
    "text": "It could be by\nmultiplication, it could be an L2 distance\nfunction or something like that,",
    "start": "1762040",
    "end": "1767279"
  },
  {
    "text": "and then we can\nexponentiate and normalize to get a distribution. ",
    "start": "1767280",
    "end": "1774230"
  },
  {
    "text": "So d here corresponds to-- it could be Euclidean\ndistance or cosine distance. We're measuring the\ndistance between f",
    "start": "1774230",
    "end": "1781420"
  },
  {
    "text": "of x, which is the white dot,\nto each of the C's, and then",
    "start": "1781420",
    "end": "1787270"
  },
  {
    "text": "comparing the distances\nbetween the different classes. ",
    "start": "1787270",
    "end": "1793480"
  },
  {
    "text": "OK. Yeah? I wonder if that's always\nthe correct assumption",
    "start": "1793480",
    "end": "1800000"
  },
  {
    "text": "to make, to make the classes\ntwo different classes of dogs but they're different breeds\nof dogs in the new class. This [INAUDIBLE] your\ndog is [INAUDIBLE]",
    "start": "1800000",
    "end": "1811230"
  },
  {
    "text": "Yeah. So the question\nis, in some cases maybe your classes are\nsomewhat heterogeneous.",
    "start": "1811230",
    "end": "1817305"
  },
  {
    "text": "Maybe you have\ndifferent breeds of dogs and maybe you shouldn't\nactually aggregate by averaging,",
    "start": "1817305",
    "end": "1822410"
  },
  {
    "text": "or maybe you shouldn't have\na single prototype per class. There's a couple answers here.",
    "start": "1822410",
    "end": "1832280"
  },
  {
    "text": "I guess the first answer\nis that, in some cases, it may not actually\nbe a good idea to have a single prototype,\nand I'll mention an approach",
    "start": "1832280",
    "end": "1838220"
  },
  {
    "text": "in a couple slides\nthat actually has multiple prototypes per class. And the second answer is that\nthese embeddings are learned",
    "start": "1838220",
    "end": "1847250"
  },
  {
    "text": "and they're high dimensional. And what the network can do is\nit can learn a representation",
    "start": "1847250",
    "end": "1853250"
  },
  {
    "text": "that tries to really get the\nessence of a dog, for example, and collapse out\nthings that aren't-- that don't correspond to\na dog, and potentially",
    "start": "1853250",
    "end": "1862490"
  },
  {
    "text": "learn a representation space\nin which the variants of dogs are invariant.",
    "start": "1862490",
    "end": "1868610"
  },
  {
    "text": "And so even in cases where you\nhave heterogeneous classes, oftentimes this approach\nstill works pretty well,",
    "start": "1868610",
    "end": "1874890"
  },
  {
    "text": "especially if you give\nthe embedding flexibility. That said, I'll mention an\napproach that doesn't do that.",
    "start": "1874890",
    "end": "1881039"
  },
  {
    "text": "Yeah? Would you want the\nnetwork actually to learn what the dog is? Because in the case maybe\nyou're given a class",
    "start": "1881040",
    "end": "1886482"
  },
  {
    "text": "where there is no\ndog and you still have to do the\nclassification process. Yeah, I guess--",
    "start": "1886483",
    "end": "1892100"
  },
  {
    "text": "So you know that--\nwouldn't you want to filter so somehow they'd\nfind textural similarity between pictures more than\nactually classifying things?",
    "start": "1892100",
    "end": "1899400"
  },
  {
    "text": "Yeah. Essentially I guess what I meant\nby dog is that you essentially want it to be invariant to\nthings that are class agnostic",
    "start": "1899400",
    "end": "1907669"
  },
  {
    "text": "and push together things that\nare kind of representative of a class, it could\nbe dog features.",
    "start": "1907670",
    "end": "1914780"
  },
  {
    "text": "I mean, in general it should\nbe more textural, more general things than\nsomething specifically with respect to\na dog so that you",
    "start": "1914780",
    "end": "1921465"
  },
  {
    "text": "can generalize to new classes. ",
    "start": "1921465",
    "end": "1927304"
  },
  {
    "text": "OK. So to summarize these\nnonparametric methods,",
    "start": "1927305",
    "end": "1934090"
  },
  {
    "text": "we saw three versions\nof it, Siamese networks, matching networks, and\nprototypical networks. This is essentially the\nevolution of these methods,",
    "start": "1934090",
    "end": "1940145"
  },
  {
    "text": "and I think that prototypical\nnetworks is probably-- ",
    "start": "1940145",
    "end": "1946000"
  },
  {
    "text": "it is somewhat-- in my\nopinion it's at least one of the methods that\nkeeps all the simplicity",
    "start": "1946000",
    "end": "1952480"
  },
  {
    "text": "of these methods, and\nit also works very well. I think that it's probably\nthe best of these approaches, and it's also one of\nthe simplest methods.",
    "start": "1952480",
    "end": "1959035"
  },
  {
    "text": " Essentially all of\nthese approaches correspond to doing\nsome form of embedding",
    "start": "1959035",
    "end": "1964750"
  },
  {
    "text": "and then doing nearest neighbors\nin that embedding space. They just do this in\nslightly different ways. ",
    "start": "1964750",
    "end": "1972620"
  },
  {
    "text": "And so one challenge\nthat does come up is that you might need\nto reason about more complex representations or\nrelationships between data",
    "start": "1972620",
    "end": "1979030"
  },
  {
    "text": "points. And so there are approaches\nthat are a little bit more complex than these\nmethods but that",
    "start": "1979030",
    "end": "1984700"
  },
  {
    "text": "try to address more challenging\nfew-shot learning problems. So for example,\none thing that you could do is instead of\nusing cosine distance",
    "start": "1984700",
    "end": "1993160"
  },
  {
    "text": "or instead of using Euclidean\ndistance, what you could do is you could actually try to\nlearn a distance function.",
    "start": "1993160",
    "end": "1998450"
  },
  {
    "text": "And so essentially what\nthis approach is doing is it's learning\nthese features just like in prototypical networks,\nbut also simply learning",
    "start": "1998450",
    "end": "2005639"
  },
  {
    "text": "a distance function\nbetween classes rather than using a\nfixed distance function.",
    "start": "2005640",
    "end": "2011700"
  },
  {
    "text": "Another approach, like\nI mentioned before, is to learn a\nmixture of prototypes rather than just a single\nprototype per class.",
    "start": "2011700",
    "end": "2020350"
  },
  {
    "text": "And then lastly, another\ncomplex approach that-- more complex approach\nthat you can do is to try to do\nsome sort of message passing on the embeddings\nrather than just doing",
    "start": "2020350",
    "end": "2030000"
  },
  {
    "text": "a simple comparison.  Cool. So that's a summary of\nnonparametric methods.",
    "start": "2030000",
    "end": "2039168"
  },
  {
    "text": "Are there any more\nquestions before I go through a case study? ",
    "start": "2039168",
    "end": "2045510"
  },
  {
    "text": "Cool. OK. So in last year's\nversion of the course, I went through a\ncase study that was",
    "start": "2045510",
    "end": "2051869"
  },
  {
    "start": "2047000",
    "end": "2167000"
  },
  {
    "text": "looking at dermatological\nimage classification. I just wanted to\nbriefly mention it because it's a\npretty cool paper.",
    "start": "2051870",
    "end": "2057440"
  },
  {
    "text": "And if you're interested\nin things related to medical imaging or diagnosis\nand that sort of thing",
    "start": "2057440",
    "end": "2063690"
  },
  {
    "text": "or long tail distributions,\nthis is a cool paper. And it kind of adapted\nprototypical networks",
    "start": "2063690",
    "end": "2068940"
  },
  {
    "text": "to this particular problem. The case study that I want\nto go through this year is actually a project that we\nworked on earlier this year",
    "start": "2068940",
    "end": "2078090"
  },
  {
    "text": "where we actually deployed\nthe algorithm to-- in a live application, which\nI think is not always--",
    "start": "2078090",
    "end": "2086633"
  },
  {
    "text": "maybe more rare\nin terms of things that are so on the\nedge of research. ",
    "start": "2086633",
    "end": "2092985"
  },
  {
    "text": "OK. So the problem that\nwe have here is",
    "start": "2092985",
    "end": "2098010"
  },
  {
    "text": "that we want to give\nfeedback to students. And as a very concrete\nexample, Stanford",
    "start": "2098010",
    "end": "2103980"
  },
  {
    "text": "offered this free\nintro to CS course to more than 12,000\nstudents and we",
    "start": "2103980",
    "end": "2112353"
  },
  {
    "text": "wanted to be able\nto give feedback to the students on\na diagnostic exam that they took in the course.",
    "start": "2112353",
    "end": "2119410"
  },
  {
    "text": "And their submissions\ncorresponded to open ended\nPython code, and we",
    "start": "2119410",
    "end": "2124450"
  },
  {
    "text": "estimated that it would\ntake about eight months of human labor to\ngive feedback to all of the students in the course.",
    "start": "2124450",
    "end": "2131525"
  },
  {
    "text": "And this isn't just a problem\nfor this particular course. This is really a problem for\nany sort of online education",
    "start": "2131525",
    "end": "2137829"
  },
  {
    "text": "where we want to scale feedback\nto large numbers of students. Specifically what the\nproblem looks like is we",
    "start": "2137830",
    "end": "2143560"
  },
  {
    "text": "have some Python code\nthat the student wrote, and we want to be able to\nidentify some misconceptions",
    "start": "2143560",
    "end": "2148690"
  },
  {
    "text": "that they had. And we can frame this as\na classification problem where we-- or really a multiclass\nbinary classification problem",
    "start": "2148690",
    "end": "2155020"
  },
  {
    "text": "where we essentially\nclassify whether they had a certain misconception or not. And this is using\nthe same rubrics",
    "start": "2155020",
    "end": "2160660"
  },
  {
    "text": "that instructors\nat Stanford used to give feedback, kind of\nlike Gradescope style rubrics.",
    "start": "2160660",
    "end": "2168130"
  },
  {
    "start": "2167000",
    "end": "2344000"
  },
  {
    "text": "This is a hard\nproblem in general. Also just a hard\nproblem for machine learning because you don't\nhave that much annotation.",
    "start": "2168130",
    "end": "2175370"
  },
  {
    "text": "It takes a lot of expertise and\na lot of work to give feedback.",
    "start": "2175370",
    "end": "2181195"
  },
  {
    "text": "This data tends up\nbeing very long tailed in that you can solve the same\nproblem in many different ways.",
    "start": "2181195",
    "end": "2186670"
  },
  {
    "text": "And also instructors are\nconstantly editing assignments and solutions-- or sorry, editing\nassignments and exams.",
    "start": "2186670",
    "end": "2193070"
  },
  {
    "text": "And as a result, the\nsolutions and the questions and the feedback look very\ndifferent from year to year.",
    "start": "2193070",
    "end": "2200010"
  },
  {
    "text": "OK. So how might we frame this\nas a meta-learning problem? Does anyone have any\nthoughts or ideas?",
    "start": "2200010",
    "end": "2205915"
  },
  {
    "text": " Yeah? Can you train on past\nexams or even just",
    "start": "2205915",
    "end": "2213680"
  },
  {
    "text": "a subset of the current exams? Yeah, so you can\nmeta-train on past exams.",
    "start": "2213680",
    "end": "2218690"
  },
  {
    "text": "And then what do you\ndo at meta-test time? ",
    "start": "2218690",
    "end": "2225360"
  },
  {
    "text": "Any thoughts? Yeah? Create assignments for\neach separate class.",
    "start": "2225360",
    "end": "2232170"
  },
  {
    "text": "So say you have an x\nnumber and [INAUDIBLE].. ",
    "start": "2232170",
    "end": "2246289"
  },
  {
    "text": "Cool. So yeah, you could essentially\nhave different assignments be different tasks and you\ncan meta-train across that.",
    "start": "2246290",
    "end": "2252577"
  },
  {
    "text": "And then when you're given a new\nassignment at meta-test time, what happens then? ",
    "start": "2252577",
    "end": "2260260"
  },
  {
    "text": "When you're given a new\nassignment at meta-test time, what happens at that point?",
    "start": "2260260",
    "end": "2266026"
  },
  {
    "text": "Because at the end\nof a [INAUDIBLE].. ",
    "start": "2266026",
    "end": "2272460"
  },
  {
    "text": "So you could just\nrun the algorithm, although we need to somehow\nadapt the model in some way.",
    "start": "2272460",
    "end": "2279470"
  },
  {
    "text": "Yeah? Could you just see if the actual\ncode into the model and it",
    "start": "2279470",
    "end": "2284484"
  },
  {
    "text": "would-- I would assume it would view\ndifferent common solutions to each problem. So you would be able to classify\nthem and compare the student's",
    "start": "2284484",
    "end": "2291980"
  },
  {
    "text": "solution in some sort\nof robotics case. Mhm. So you can embed\nthe code and then",
    "start": "2291980",
    "end": "2297589"
  },
  {
    "text": "do some-- like compare\nit to common solutions. And then where do those\ncommon solutions come from?",
    "start": "2297590",
    "end": "2304410"
  },
  {
    "text": "Well, I assume you have a\nmeta-training set wherein one function is used commonly\nto solve this problem,",
    "start": "2304410",
    "end": "2310645"
  },
  {
    "text": "or maybe a few students\ndo it another way so that creates distinct\ngroups of solution types.",
    "start": "2310645",
    "end": "2317960"
  },
  {
    "text": "Yeah. Yeah. So you could essentially\nhave different solution types to different problems. Did you have something to add?",
    "start": "2317960",
    "end": "2324549"
  },
  {
    "text": "I'm still thinking about it, so\nit's in [INAUDIBLE] right now. The person behind you? Maybe you'll have the\ninstructor label a few examples.",
    "start": "2324550",
    "end": "2332270"
  },
  {
    "text": "Yeah. So when you have\na new assignment, you can have the instructor\nlabel a few examples and then use that as your\nkind of training data",
    "start": "2332270",
    "end": "2338619"
  },
  {
    "text": "set for the test assignment. Cool. So that's essentially\nwhat it will look like.",
    "start": "2338620",
    "end": "2344180"
  },
  {
    "start": "2344000",
    "end": "2563000"
  },
  {
    "text": "So for meta training\nwe have eight exams from the intro to\nCS course, CS106.",
    "start": "2344180",
    "end": "2353230"
  },
  {
    "text": "And each student we have--\nthis is fully labeled. The students get\nfeedback via the rubric.",
    "start": "2353230",
    "end": "2358870"
  },
  {
    "text": "Getting to the\ndetails a little bit, each rubric items and each\nitem has several options.",
    "start": "2358870",
    "end": "2364930"
  },
  {
    "text": "And so an example\nof a rubric looks like this where the\nmisconception has to do with string\ninsertion and then",
    "start": "2364930",
    "end": "2370360"
  },
  {
    "text": "you have different\noptions with respect to this particular concept. And then what we're\nactually going to do",
    "start": "2370360",
    "end": "2376630"
  },
  {
    "text": "is, instead of treating\ndifferent assignments as tasks, we're going to actually treat\neach of these rubric options",
    "start": "2376630",
    "end": "2382840"
  },
  {
    "text": "as different tasks. And so this is a binary\nclassification task, this is a binary classification\ntask, and so forth.",
    "start": "2382840",
    "end": "2389710"
  },
  {
    "text": "And we'll essentially\nget tasks for each one of these rubric options and\nwe'll have four tasks for this",
    "start": "2389710",
    "end": "2396555"
  },
  {
    "text": "and then for\nanother rubric item, we'll have some number\nof additional tasks, and so on and so forth.",
    "start": "2396555",
    "end": "2401860"
  },
  {
    "text": " Cool. And then this is the\nmeta training process.",
    "start": "2401860",
    "end": "2407230"
  },
  {
    "text": "And what we'll do\nis we will use, essentially,\nprototypical networks,",
    "start": "2407230",
    "end": "2412630"
  },
  {
    "text": "and our model will take as\ninput a sequence of Python code",
    "start": "2412630",
    "end": "2418569"
  },
  {
    "text": "tokens. And we'll use a stack\ntransformer model to, essentially, take the code\nand embed that into a fixed",
    "start": "2418570",
    "end": "2426310"
  },
  {
    "text": "dimensional embedding and then\nrun prototypical networks.",
    "start": "2426310",
    "end": "2431710"
  },
  {
    "text": "And then at meta test time,\nwe will have some solution--",
    "start": "2431710",
    "end": "2437530"
  },
  {
    "text": "actually, a lot of solutions. We'll label a few\nof them and use that to generate the prototypes\nfor true and for false",
    "start": "2437530",
    "end": "2444369"
  },
  {
    "text": "for each rubric action. Yeah? So the biggest thing is\nyou're making each rubric item",
    "start": "2444370",
    "end": "2451780"
  },
  {
    "text": "as a different task. So if we still need some sort\nof grouping over the tasks,",
    "start": "2451780",
    "end": "2457960"
  },
  {
    "text": "we'll say, these four tasks\ncome, fall into this rubric. Or are you just treating\nit completely separately?",
    "start": "2457960",
    "end": "2464270"
  },
  {
    "text": "In this case, we did\nactually complete it-- treat it completely separately. There's one thing that we\ndid that I'll mention soon",
    "start": "2464270",
    "end": "2470555"
  },
  {
    "text": "that makes them\nslightly less separate, but because these two things\ncan both be true, for example.",
    "start": "2470555",
    "end": "2477829"
  },
  {
    "text": "And so as a result, we need to\nbasically do a classification",
    "start": "2477830",
    "end": "2484030"
  },
  {
    "text": "problem here and solve this\nclassification problem. And I'll talk a\nlittle bit about how we can give the model a\nlittle bit more information.",
    "start": "2484030",
    "end": "2490540"
  },
  {
    "text": "Yeah? Isn't it kind of messing\naround with the distribution",
    "start": "2490540",
    "end": "2495880"
  },
  {
    "text": "assumptions because you're-- usually he is [INAUDIBLE] these\nsample tasks by IID procuring.",
    "start": "2495880",
    "end": "2501060"
  },
  {
    "text": "Like, when we\nsample, for example, one rubric item under\nthe [INAUDIBLE] rubric,",
    "start": "2501060",
    "end": "2506530"
  },
  {
    "text": "then it's likely that\nthe other tasks will also be sampled in the same way.",
    "start": "2506530",
    "end": "2511598"
  },
  {
    "text": "Normal. Yeah. So the question is, does\nthis kind of break the IID assumption in terms of tasks?",
    "start": "2511598",
    "end": "2516910"
  },
  {
    "text": "When we sample tasks, we\ndon't always sample all four of these tasks from\nthis rubric item. We essentially sample IID from\nthe distribution of rubric--",
    "start": "2516910",
    "end": "2526720"
  },
  {
    "text": "of rubrics or of\nquestions and then over the distribution\nof items and then over the distribution\nof options.",
    "start": "2526720",
    "end": "2532150"
  },
  {
    "text": "We try to sample from-- do that sort of sampling\nin an independent fashion. ",
    "start": "2532150",
    "end": "2541220"
  },
  {
    "text": "OK. So this is the v0 for what\nwe tried for this problem.",
    "start": "2541220",
    "end": "2546910"
  },
  {
    "text": "And we found that actually\napplying this out of the box didn't work very well and\nthat attention isn't quite all",
    "start": "2546910",
    "end": "2553860"
  },
  {
    "text": "you need to solve the problem. So there are a\ncouple tricks that we found to be pretty\nhelpful, especially",
    "start": "2553860",
    "end": "2560280"
  },
  {
    "text": "because we didn't have that\nmuch meta training data. So the first trick was that\nwe can-- instead of only using",
    "start": "2560280",
    "end": "2566369"
  },
  {
    "text": "tasks from the\nmeta-training data set, we can augment the rubric tasks\nwith self supervised tasks.",
    "start": "2566370",
    "end": "2572317"
  },
  {
    "text": "So we can have a\ntask that corresponds to predicting the kind of\ncompilation error that happens.",
    "start": "2572317",
    "end": "2579713"
  },
  {
    "text": "We can also do this sort\nof masked language modeling kinds of tasks as well.",
    "start": "2579713",
    "end": "2585220"
  },
  {
    "text": "The second trick is to\nincorporate side information into the model. So if you only\nhave a few examples",
    "start": "2585220",
    "end": "2591420"
  },
  {
    "text": "of positive and\nnegative examples for a particular\nrubric option, it can actually be\nsomewhat ambiguous what you're trying\nto classify on.",
    "start": "2591420",
    "end": "2598297"
  },
  {
    "text": "And so we can give it\nside information that corresponds to the name\nof the rubric option as well as the text\nof the question.",
    "start": "2598298",
    "end": "2604710"
  },
  {
    "text": "And this essentially\ngives the model a little bit more\ninformation with respect to how it's supposed to be\ngiving feedback to the student",
    "start": "2604710",
    "end": "2611100"
  },
  {
    "text": "and what this rubric\noption corresponds to. And essentially we'll just kind\nof prepend the side information",
    "start": "2611100",
    "end": "2617070"
  },
  {
    "text": "into the transformer model. And so this is kind of\nanswering the question before about treating these\ncompletely independently.",
    "start": "2617070",
    "end": "2624360"
  },
  {
    "text": "Because of the side information,\nit isn't quite as independent. Then the last thing we\nfound to be super helpful",
    "start": "2624360",
    "end": "2630120"
  },
  {
    "text": "is to pretrain on\nunlabeled Python code. And the way this works is\nthere are some great databases",
    "start": "2630120",
    "end": "2635760"
  },
  {
    "text": "or data sets that have\na bunch of Python code, and you can run\na model like BERT",
    "start": "2635760",
    "end": "2641040"
  },
  {
    "text": "on that code to get some\npretrained embeddings. And we use this-- we do this pretraining\nbefore doing",
    "start": "2641040",
    "end": "2647340"
  },
  {
    "text": "the meta-training process.  OK.",
    "start": "2647340",
    "end": "2652510"
  },
  {
    "text": "So the full model\nlooks something like this, where we have the\ncode that the student wrote. We also have the question\ntext and the rubric text.",
    "start": "2652510",
    "end": "2659130"
  },
  {
    "text": "We pass all of this\nthrough a transformer model to get an embedding of\nthat solution with respect",
    "start": "2659130",
    "end": "2664440"
  },
  {
    "text": "to that question\nand rubric option. And then we do that for all\nof the positive examples",
    "start": "2664440",
    "end": "2670530"
  },
  {
    "text": "for that rubric\noption and average to get embedding for\npositive, as well as average across the solutions\nthat got it,",
    "start": "2670530",
    "end": "2678390"
  },
  {
    "text": "that were negative for that\nparticular rubric option. And then we get a prototype both\nfor positive and for negative and we can compare\nnew student solutions",
    "start": "2678390",
    "end": "2685559"
  },
  {
    "text": "to those two prototypes.  OK.",
    "start": "2685560",
    "end": "2692119"
  },
  {
    "start": "2692000",
    "end": "2745000"
  },
  {
    "text": "Cool. So I guess when we approach this\nproblem, we didn't actually--",
    "start": "2692120",
    "end": "2697348"
  },
  {
    "text": "it's a pretty difficult problem\nto give feedback to students. We didn't really understand. We didn't have super high\nexpectations with respect",
    "start": "2697348",
    "end": "2703279"
  },
  {
    "text": "to how well it would work. It turned out to actually\nwork pretty well. So using the kind of\nprototypical networks",
    "start": "2703280",
    "end": "2710630"
  },
  {
    "text": "outperforms a supervised\nlearning method by around 8% to 17%, which\nis pretty significant.",
    "start": "2710630",
    "end": "2716250"
  },
  {
    "text": "We also see that\nin the case where you have a held out\nrubric, you can actually do better than a human TA in\nterms of accuracy or precision.",
    "start": "2716250",
    "end": "2725290"
  },
  {
    "text": "You're more accurate. In the case where you have\na held out exam entirely,",
    "start": "2725290",
    "end": "2731160"
  },
  {
    "text": "there's still room\nfor improvement. So we're still about 8%\nworse than a human TA.",
    "start": "2731160",
    "end": "2737780"
  },
  {
    "text": "But still, 74% accuracy\nis, in my opinion, actually pretty good.",
    "start": "2737780",
    "end": "2744990"
  },
  {
    "text": "And then lastly, we also\ndeployed this model to the Code in Place course that I\nmentioned previously.",
    "start": "2744990",
    "end": "2750450"
  },
  {
    "start": "2745000",
    "end": "2796000"
  },
  {
    "text": "So the students took the\ndiagnostic on May 10. We needed to actually\ngive the feedback to the students in\nsome way, and so Chris",
    "start": "2750450",
    "end": "2758900"
  },
  {
    "text": "designed this interface. We paired the rubric\noption with some text",
    "start": "2758900",
    "end": "2763940"
  },
  {
    "text": "that described the feedback\nfor that rubric option. This text was written by a human\nbut the classification problem",
    "start": "2763940",
    "end": "2770029"
  },
  {
    "text": "was done by the meta learner. The students evaluated\nthe feedback. We also used syntax\nhighlighting to try to show them",
    "start": "2770030",
    "end": "2777860"
  },
  {
    "text": "where the error was. And then lastly, it's worth\nmentioning that if there are syntax errors\nanywhere in the code,",
    "start": "2777860",
    "end": "2783720"
  },
  {
    "text": "that means that you can't really\nuse unit testing to actually give feedback to the students.",
    "start": "2783720",
    "end": "2788850"
  },
  {
    "text": "And that's why having\nmodels that take as input the raw text or the raw\nPython code is pretty helpful.",
    "start": "2788850",
    "end": "2794045"
  },
  {
    "text": " And so lastly, in kind of\na blind randomized trial,",
    "start": "2794045",
    "end": "2800760"
  },
  {
    "start": "2796000",
    "end": "2903000"
  },
  {
    "text": "humans gave feedback on around\n1,000 of the student solutions. This is used in the support set. Because we actually had a\ndecent amount of data here,",
    "start": "2800760",
    "end": "2807900"
  },
  {
    "text": "we also fine tuned\nthe model as well. And then the model gave\nfeedback on the remaining 15,000",
    "start": "2807900",
    "end": "2814650"
  },
  {
    "text": "examples and around 2,000 of\nthem could be auto graded. And so what we found\nis that in both cases,",
    "start": "2814650",
    "end": "2822090"
  },
  {
    "text": "actually, the human and\nthe machine learning model were actually giving\npretty good feedback.",
    "start": "2822090",
    "end": "2828450"
  },
  {
    "text": "The students were\nagreeing with the feedback from the model\n97.9% of the time, and it was agreeing\nwith the humans--",
    "start": "2828450",
    "end": "2834840"
  },
  {
    "text": "they were agreeing\nwith the human feedback 96.7% of the time. Now, you could agree\nwith things but not",
    "start": "2834840",
    "end": "2841170"
  },
  {
    "text": "have those things not\nactually be useful. So we can also ask them\nif they think it's useful, and they said it was--",
    "start": "2841170",
    "end": "2846510"
  },
  {
    "text": "they rated it as being useful\nwith a score of 4.6 out of 5.",
    "start": "2846510",
    "end": "2853590"
  },
  {
    "text": "Cool. And then lastly, we also checked\nfor kind of signs of bias by demographics, which\nis usually a good thing",
    "start": "2853590",
    "end": "2859770"
  },
  {
    "text": "to do with machine\nlearning systems. And we didn't see\nany signs of bias in terms of the most represented\ngenders and countries",
    "start": "2859770",
    "end": "2866579"
  },
  {
    "text": "in the data set. Cool. So hopefully that\ngives you a sense for how we can try\nto formulate problems",
    "start": "2866580",
    "end": "2872910"
  },
  {
    "text": "in the context of meta learning\nand also an example of how these algorithms might actually\nbe deployed in real world",
    "start": "2872910",
    "end": "2879690"
  },
  {
    "text": "scenarios.  Cool. ",
    "start": "2879690",
    "end": "2886325"
  },
  {
    "text": "And then that was actually\nalso a specific case of deploying a\nnonparametric method. ",
    "start": "2886325",
    "end": "2892262"
  },
  {
    "text": "Now let's go through some\nproperties of meta learning algorithms and try to\ncompare the different kinds of approaches.",
    "start": "2892262",
    "end": "2897620"
  },
  {
    "start": "2897620",
    "end": "2904040"
  },
  {
    "start": "2903000",
    "end": "3066000"
  },
  {
    "text": "Cool, so we've talked about\nthree classes of approaches. And if you think about these\nfrom the standpoint of meta",
    "start": "2904040",
    "end": "2912040"
  },
  {
    "text": "learning, like having\nthis computation graph and optimizing that\ncomputation graph end to end,",
    "start": "2912040",
    "end": "2917410"
  },
  {
    "text": "we can think of\nblack box models as-- we can essentially\nview all of them as a computation graph that\ntakes this input training",
    "start": "2917410",
    "end": "2923848"
  },
  {
    "text": "data and a test\nexample and produces the corresponding label. Black-box methods treat\nthe computation graph",
    "start": "2923848",
    "end": "2929590"
  },
  {
    "text": "in a fully black-box way. Optimization-based methods embed\nin a gradient descent procedure",
    "start": "2929590",
    "end": "2934960"
  },
  {
    "text": "or something like that into\nthat computation graph. And then lastly,\nnonparametric approaches",
    "start": "2934960",
    "end": "2941740"
  },
  {
    "text": "can also be viewed as\nhaving this particular kind of computation graph. And that computation\ngraph is something",
    "start": "2941740",
    "end": "2947950"
  },
  {
    "text": "that embeds nearest neighbors\nor something like nearest neighbors to prototypes within\nthat computation graph where",
    "start": "2947950",
    "end": "2953710"
  },
  {
    "text": "we are comparing the test\nexample to these prototypes, and these prototypes are\ncomputed from the training set",
    "start": "2953710",
    "end": "2960700"
  },
  {
    "text": "using the following equation. So essentially, in many\nways, all of these methods",
    "start": "2960700",
    "end": "2966130"
  },
  {
    "text": "are doing the same thing. It's just different forms\nof computation graphs to embed structure into\nthe meta learning process.",
    "start": "2966130",
    "end": "2973420"
  },
  {
    "text": " Now, I can also note again\nthat you can mix and match",
    "start": "2973420",
    "end": "2980380"
  },
  {
    "text": "different components of\nthis, and so in many ways I think that these are\nreally the three base",
    "start": "2980380",
    "end": "2985720"
  },
  {
    "text": "classes of approaches, but\nthere isn't a hard line in between these\nkinds of approaches.",
    "start": "2985720",
    "end": "2991549"
  },
  {
    "text": "So you can have methods that\nare hybrid methods that, for example,\ncondition on the data",
    "start": "2991550",
    "end": "2997090"
  },
  {
    "text": "and also run gradient\ndescent on the data. Methods that compute\nsome embedding,",
    "start": "2997090",
    "end": "3003870"
  },
  {
    "text": "like the embeddings that we\nsee in nonparametric approaches and then do gradient\ndescent on that embedding,",
    "start": "3003870",
    "end": "3009540"
  },
  {
    "text": "as well as approaches that\ndo something like MAML but initialize the last layer\nas prototypical networks",
    "start": "3009540",
    "end": "3017790"
  },
  {
    "text": "during meta-training. So there isn't really--\nkind of the separation",
    "start": "3017790",
    "end": "3023970"
  },
  {
    "text": "between these kinds of\napproaches is very blurred and there are many\napproaches that don't cleanly",
    "start": "3023970",
    "end": "3029310"
  },
  {
    "text": "fall into any one of\nthese three approaches. Yeah?",
    "start": "3029310",
    "end": "3035620"
  },
  {
    "text": "For early methods\nlike RL squared, how do you classify that\nkind of [INAUDIBLE]??",
    "start": "3035620",
    "end": "3044280"
  },
  {
    "text": "Yeah. So the question is,\nfor early approaches like RL squared, which is a\nmeta RL algorithm that we'll",
    "start": "3044280",
    "end": "3050000"
  },
  {
    "text": "talk about in a couple lectures,\nhow would you classify that? RL squared is\nbasically just an LSTM, and so I would classify it\nunder the black-box approaches",
    "start": "3050000",
    "end": "3057170"
  },
  {
    "text": "because it takes\nas input, the data, and passes it through\na recurrent network. ",
    "start": "3057170",
    "end": "3066690"
  },
  {
    "start": "3066000",
    "end": "3195000"
  },
  {
    "text": "Cool. So that was the kind of\ncomputation graph perspective. Another way that we can\nthink about these classes",
    "start": "3066690",
    "end": "3071730"
  },
  {
    "text": "of approaches is their kind\nof algorithmic properties. And I think that this\nis useful for thinking",
    "start": "3071730",
    "end": "3076980"
  },
  {
    "text": "about when you might use\none approach versus another.",
    "start": "3076980",
    "end": "3082060"
  },
  {
    "text": "So one algorithmic\nproperty that we might want is for it to be very expressive.",
    "start": "3082060",
    "end": "3087809"
  },
  {
    "text": "And what I mean by\nexpressive is the ability to represent a wide range of\ndifferent learning procedures.",
    "start": "3087810",
    "end": "3094830"
  },
  {
    "text": "And this can be useful\nbecause in some scenarios, we may have a lot of\nmeta-training data.",
    "start": "3094830",
    "end": "3100950"
  },
  {
    "text": "We want to learn a very\nspecific or a very good learning procedure. And if we want to learn\na very good procedure",
    "start": "3100950",
    "end": "3106590"
  },
  {
    "text": "from a lot of data,\nthen we should be able to represent many\ndifferent learning procedures.",
    "start": "3106590",
    "end": "3113560"
  },
  {
    "text": "Another property that\nwe should care about is what I'll refer\nto as consistency. And what I mean by consistency\nis that the learning procedure",
    "start": "3113560",
    "end": "3122820"
  },
  {
    "text": "that you get, ideally\nit would monotonically improve as you get more data.",
    "start": "3122820",
    "end": "3128220"
  },
  {
    "text": "Or at least monotonically\nimprove in expectation. And essentially, this is--",
    "start": "3128220",
    "end": "3133583"
  },
  {
    "text": "I like to use the\nterm consistency because in statistics,\nconsistency refers to, as you get\nmore data, your estimator",
    "start": "3133583",
    "end": "3138869"
  },
  {
    "text": "will converge to\nthe true estimator. And so in this case,\na learning procedure-- a learned learning\nprocedure is kind of",
    "start": "3138870",
    "end": "3144270"
  },
  {
    "text": "guaranteed to be consistent\nif it improves expectation as you get more and more data. It converges to\nthe right answer.",
    "start": "3144270",
    "end": "3150105"
  },
  {
    "text": " And this is important\nbecause it potentially reduces the reliance on\nthe meta training tasks.",
    "start": "3150105",
    "end": "3159090"
  },
  {
    "text": "And so if you don't have a\nlot of meta training tasks, or maybe you have a domain\nshift between meta-training and meta-test time, if you guarantee\nthat your learning procedure is",
    "start": "3159090",
    "end": "3166230"
  },
  {
    "text": "consistent, then\nyou'll at least be able to improve as you get\nmore data on the test task.",
    "start": "3166230",
    "end": "3172470"
  },
  {
    "text": "And so this potentially means\nthat you might get better OOD performance, especially if you\nhave a lot of data of your test",
    "start": "3172470",
    "end": "3178290"
  },
  {
    "text": "task. And you can recall that\nlast week we kind of showed",
    "start": "3178290",
    "end": "3184059"
  },
  {
    "text": "these curves that showed\nkind of OOD performance of different algorithms, and\nwe see a pretty big difference between different approaches.",
    "start": "3184060",
    "end": "3189158"
  },
  {
    "text": " OK, so these two properties\nare important for a lot of different applications.",
    "start": "3189158",
    "end": "3195340"
  },
  {
    "start": "3195000",
    "end": "3521000"
  },
  {
    "text": "And if we think about these\nthree classes of approaches, we can think about them in\nterms of those properties.",
    "start": "3195340",
    "end": "3200890"
  },
  {
    "text": "So black-box methods have\ncomplete expressive power, but they aren't guaranteed\nto be consistent.",
    "start": "3200890",
    "end": "3207359"
  },
  {
    "text": "As you give them--\nas you pass more data into the recurrent\nneural network, it may not continue to improve. ",
    "start": "3207360",
    "end": "3214860"
  },
  {
    "text": "Optimization-based\napproaches are consistent because\nit essentially reduces the running gradient\ndescent at meta test time.",
    "start": "3214860",
    "end": "3221290"
  },
  {
    "text": "And so in expectation,\nyou will improve. ",
    "start": "3221290",
    "end": "3226350"
  },
  {
    "text": "If you have a very deep\nnetwork, it is very expressive, but you do need a fairly\nlarge model to do that.",
    "start": "3226350",
    "end": "3232320"
  },
  {
    "text": "And this is specific to\nsupervised learning settings. When we move to RL settings,\nthe expressive power",
    "start": "3232320",
    "end": "3239070"
  },
  {
    "text": "is actually somewhat reduced. And then lastly, for\nnonparametric methods,",
    "start": "3239070",
    "end": "3244440"
  },
  {
    "text": "they are quite expressive\nfor most architectures. And under certain\nconditions, they",
    "start": "3244440",
    "end": "3254400"
  },
  {
    "text": "will be consistent as you\nget more data to compare to. ",
    "start": "3254400",
    "end": "3260310"
  },
  {
    "text": "OK. So these are a\ncouple different ways of thinking about\nthese algorithms. There are some\nother pros and cons",
    "start": "3260310",
    "end": "3266590"
  },
  {
    "text": "as well that I'll go through. So black-box methods\nare easy to combine with a variety of different\nlearning problems,",
    "start": "3266590",
    "end": "3273549"
  },
  {
    "text": "but it leads to a\nchallenging optimization and is often data inefficient. For optimization-based\napproaches,",
    "start": "3273550",
    "end": "3279369"
  },
  {
    "text": "we also talked about how\nthis has a positive inductive bias at the start of meta\nlearning, which means that it",
    "start": "3279370",
    "end": "3284560"
  },
  {
    "text": "can be easier to optimize. It also handles varying\nand large k pretty well.",
    "start": "3284560",
    "end": "3289872"
  },
  {
    "text": "You can run gradient descent,\neven with a very large amount of data. And it's model agnostic.",
    "start": "3289872",
    "end": "3295391"
  },
  {
    "text": "The downside is that you get\nthe second order optimization, and this can mean that it can\nbe compute and memory intensive",
    "start": "3295392",
    "end": "3300430"
  },
  {
    "text": "if you have a very large\noptimization process. ",
    "start": "3300430",
    "end": "3306350"
  },
  {
    "text": "And then for\nnonparametric methods, one thing that's\nquite nice about them is that, unlike\nsomething that's doing",
    "start": "3306350",
    "end": "3312220"
  },
  {
    "text": "a gradient descent\nin the inner loop, it's an entirely\nfeedforward process.",
    "start": "3312220",
    "end": "3317590"
  },
  {
    "text": "Doing nearest\nneighbors, for example, is entirely feedforward. And this means that\nthey are usually",
    "start": "3317590",
    "end": "3324490"
  },
  {
    "text": "computationally very fast and\nalso pretty easy to optimize. ",
    "start": "3324490",
    "end": "3331750"
  },
  {
    "text": "In practice, people have found\nthat these algorithms sometimes don't generalize\nwell to varying k,",
    "start": "3331750",
    "end": "3337090"
  },
  {
    "text": "and of course, they don't\nscale well to very large k because you need to\ndo k comparisons or n",
    "start": "3337090",
    "end": "3343779"
  },
  {
    "text": "times k comparisons. And then lastly, so\nfar these methods are limited to classification.",
    "start": "3343780",
    "end": "3349940"
  },
  {
    "text": "So overall, I think that if you\nhave a classification problem, this class of methods\nis really great.",
    "start": "3349940",
    "end": "3355724"
  },
  {
    "text": "If you have a\nregression problem, for example, these approaches\nare less applicable.",
    "start": "3355725",
    "end": "3360910"
  },
  {
    "text": " And then lastly, I think\nthat well tuned versions",
    "start": "3360910",
    "end": "3366740"
  },
  {
    "text": "of these approaches\ngenerally perform fairly comparably on a lot\nof the few-shot learning",
    "start": "3366740",
    "end": "3372109"
  },
  {
    "text": "benchmarks. I think that this\nlikely says more about the benchmarks than\nthe methods themselves,",
    "start": "3372110",
    "end": "3377750"
  },
  {
    "text": "and ultimately, what\nyou end up using will probably depend\non your use case. But hopefully this\ngives you a sense",
    "start": "3377750",
    "end": "3383420"
  },
  {
    "text": "for the kinds of things you\nshould be thinking about when deciding which method to use. Yeah?",
    "start": "3383420",
    "end": "3389390"
  },
  {
    "text": "We have this [INAUDIBLE] of\nthe nonparametric approach is, but it's super effective\nwhen [INAUDIBLE]..",
    "start": "3389390",
    "end": "3397430"
  },
  {
    "text": "Yeah. It's a good question. I think this is more of\nan empirical observation than anything kind of\ntheoretically about them.",
    "start": "3397430",
    "end": "3407030"
  },
  {
    "text": "I think that it may\nhave to do with-- in prototypical networks,\nfor example, averaging a lot of things may be\ndifficult, especially",
    "start": "3407030",
    "end": "3416102"
  },
  {
    "text": "if those different things\nare somewhat heterogeneous, like in different\nparts of the space. Then averaging them may lead\nto kind of weird behavior.",
    "start": "3416102",
    "end": "3423320"
  },
  {
    "text": "That's some of my\nintuition, but I'm not sure if that's exactly what's\ngoing on or causing them to have less performance\nwhen you have varying k.",
    "start": "3423320",
    "end": "3429643"
  },
  {
    "text": "So your average may be\nskewed is what you're saying. Yeah, your average\nmight be skewed. Exactly. ",
    "start": "3429643",
    "end": "3441305"
  },
  {
    "text": "Cool.  Yeah? One off topic. What would be your\nregression task,",
    "start": "3441305",
    "end": "3447701"
  },
  {
    "text": "where you could define\nthose [INAUDIBLE].. Like, I could see it\nmaybe with a [INAUDIBLE]",
    "start": "3447701",
    "end": "3453549"
  },
  {
    "text": "and then you're\nlearning like, hey, OK, if I want to [INAUDIBLE]\nthat puts my output. And then you [INAUDIBLE].",
    "start": "3453550",
    "end": "3459832"
  },
  {
    "text": "Do you think that would work. But do you have a good example? Yeah, so you're asking\nwhat are examples",
    "start": "3459832",
    "end": "3465940"
  },
  {
    "text": "of regression problems-- Exactly. --in meta learning? Yeah. In the past few lectures were\nalways classification examples",
    "start": "3465940",
    "end": "3471052"
  },
  {
    "text": "most of the time. Yeah. So we'll go through a\ncouple example problems at the very end,\nbut some examples are things like pose prediction,\npredicting the orientation",
    "start": "3471052",
    "end": "3478390"
  },
  {
    "text": "of objects if you want\nto control a robot and it has continuous\naction spaces.",
    "start": "3478390",
    "end": "3484330"
  },
  {
    "text": "One other example that we had\nin the MAML paper that I think is a nice toy\nexample for playing with algorithms is one where\ndifferent tasks correspond",
    "start": "3484330",
    "end": "3491319"
  },
  {
    "text": "to different sinusoid curves\nand you want to predict the--",
    "start": "3491320",
    "end": "3496730"
  },
  {
    "text": "you want to predict,\nessentially, the value of the sinusoid\ncurve for a particular input.",
    "start": "3496730",
    "end": "3502180"
  },
  {
    "text": "This is actually a\nnice toy problem, especially if you\nwant to try out new approaches and so forth.",
    "start": "3502180",
    "end": "3508600"
  },
  {
    "text": "Those are some examples. I mean, I think that there is-- yeah. There's also a lot\nof machine learning",
    "start": "3508600",
    "end": "3513700"
  },
  {
    "text": "problems in general that have\nmore continuous output spaces as well. ",
    "start": "3513700",
    "end": "3521000"
  },
  {
    "start": "3521000",
    "end": "3599000"
  },
  {
    "text": "OK. And then one other kind of\nproperty that I'll mention is what I'll refer to as\nuncertainty awareness.",
    "start": "3521000",
    "end": "3529309"
  },
  {
    "text": "And in particular, if you have\na very small number of examples, it may be that your\ntask isn't actually",
    "start": "3529310",
    "end": "3536780"
  },
  {
    "text": "fully defined in some ways. It may actually be unclear\nwhat the correct behavior is.",
    "start": "3536780",
    "end": "3542820"
  },
  {
    "text": "And it's nice if our algorithms\ncan reason about that ambiguity and tell us when they aren't\nsure about the task versus just",
    "start": "3542820",
    "end": "3551277"
  },
  {
    "text": "trying to guess something\nwithout saying whether they're untrue or not. This is something that's useful\nin machine learning in general,",
    "start": "3551277",
    "end": "3558680"
  },
  {
    "text": "and it's also useful\nin settings where you want to do some\nform of active learning",
    "start": "3558680",
    "end": "3563850"
  },
  {
    "text": "where the algorithm should ask\nfor more data, for example. Or maybe you want to get\ncalibrated uncertainty",
    "start": "3563850",
    "end": "3569030"
  },
  {
    "text": "estimates, or maybe in\nreinforcement learning you want to be able to\nefficiently explore the space and find parts of\nthe state space",
    "start": "3569030",
    "end": "3575329"
  },
  {
    "text": "that will help you\nreduce ambiguity. ",
    "start": "3575330",
    "end": "3580550"
  },
  {
    "text": "And also, I guess, it's\nnice to derive approaches that are kind of based\non Bayesian inference",
    "start": "3580550",
    "end": "3586160"
  },
  {
    "text": "and stuff like\nthat, and it can be somewhat satisfying to\nexplain the framework",
    "start": "3586160",
    "end": "3591859"
  },
  {
    "text": "within the context of\nthat kind of language.",
    "start": "3591860",
    "end": "3597450"
  },
  {
    "text": "And so we'll essentially discuss\nthis a lot on Monday next week,",
    "start": "3597450",
    "end": "3604400"
  },
  {
    "text": "and it's not just going to be\nabout having algorithms reason about ambiguity. It's also going to be about\nderiving these algorithms",
    "start": "3604400",
    "end": "3611172"
  },
  {
    "text": "and thinking about\ndifferent tasks from the standpoint of kind\nof Bayesian graphical models. ",
    "start": "3611173",
    "end": "3618500"
  },
  {
    "text": "Cool. Any questions on the\nalgorithms before I move on to some applications? ",
    "start": "3618500",
    "end": "3627500"
  },
  {
    "text": "Yeah? In terms of renditions\nwork for consistency in nonparametric\nmodels, are they",
    "start": "3627500",
    "end": "3634415"
  },
  {
    "text": "conditional on the\nmodel structure or not? So you're asking, are\nthese properties--",
    "start": "3634415",
    "end": "3641190"
  },
  {
    "text": "[INAUDIBLE] under\ncertain conditions.",
    "start": "3641190",
    "end": "3647720"
  },
  {
    "text": "Are these conditional on\nthe model structure or not? Yeah. So is this consistent\nunder certain conditions?",
    "start": "3647720",
    "end": "3653780"
  },
  {
    "text": "Yeah, so it's going to have to\ndo with the model structure.",
    "start": "3653780",
    "end": "3659840"
  },
  {
    "text": "If your data is drawn IID,\nthen I think that you're set. But if your model\nstructure, for example,",
    "start": "3659840",
    "end": "3666470"
  },
  {
    "text": "collapses down information. Essentially if your embedding\ndiscards information about the image or discards\nimportant information",
    "start": "3666470",
    "end": "3674059"
  },
  {
    "text": "about the image,\nthen you'll no longer be consistent because as you\nget more data, even if you get more and more\ndata, there might",
    "start": "3674060",
    "end": "3680210"
  },
  {
    "text": "be parts of that embedding\nspace that are just-- that should be there but\naren't in that embedding space.",
    "start": "3680210",
    "end": "3687540"
  },
  {
    "text": "Yeah. So that's-- it's essentially\nabout the representation that it ends up learning.",
    "start": "3687540",
    "end": "3694420"
  },
  {
    "text": "Yeah? Are these nonparametric\nmethods the most robust for domain\nshift problems?",
    "start": "3694420",
    "end": "3699560"
  },
  {
    "text": "Yeah. So the question is, are\nnonparametric methods the most robust to domain shift? I think that actually\noptimization-based and",
    "start": "3699560",
    "end": "3706869"
  },
  {
    "text": "nonparametric both\ntend to be a lot more robust than black-box methods. I haven't seen any kind of\ndetailed empirical analysis",
    "start": "3706870",
    "end": "3714370"
  },
  {
    "text": "on which of the two\ntend to be more robust. If I were to guess,\nI would think that nonparametric methods\nare slightly more robust just",
    "start": "3714370",
    "end": "3721360"
  },
  {
    "text": "because they're simpler\nto train and things that are simpler to train often\nmight overfit a little bit less",
    "start": "3721360",
    "end": "3729187"
  },
  {
    "text": "to the meta training data. ",
    "start": "3729187",
    "end": "3736930"
  },
  {
    "text": "Cool. So now let's run through a\ncouple more applications. I'll go run these\npretty quickly,",
    "start": "3736930",
    "end": "3743300"
  },
  {
    "text": "but these should\njust give you a sense for some of the different\nways that people have applied meta learning. And we'll also end\non a note that I",
    "start": "3743300",
    "end": "3749548"
  },
  {
    "text": "think is-- a more\ngeneral note that I think is useful for\nthinking about new kinds of applications.",
    "start": "3749548",
    "end": "3756500"
  },
  {
    "text": "So one application in\nthe domain of robotics is, say we want a robot\nto be able to place",
    "start": "3756500",
    "end": "3761740"
  },
  {
    "text": "an object into a\ncontainer and we want to be able to place\nit into a container, even for containers and objects\nthat it hasn't seen before.",
    "start": "3761740",
    "end": "3769330"
  },
  {
    "text": "What we can do is we can\ngive it demonstrations of placing objects\ninto containers for a wide variety of objects.",
    "start": "3769330",
    "end": "3775240"
  },
  {
    "text": "And then at meta test\ntime, our training data will be a video of\na human placing it",
    "start": "3775240",
    "end": "3781164"
  },
  {
    "text": "into a particular container. And our query data or our\ntest data for each task",
    "start": "3781165",
    "end": "3787450"
  },
  {
    "text": "will correspond to a\nteleoperated demonstration of solving that task.",
    "start": "3787450",
    "end": "3793575"
  },
  {
    "text": "And this is going to be an\noptimization-based model and it's going to correspond to\nMAML with a learned inner loss",
    "start": "3793575",
    "end": "3798730"
  },
  {
    "text": "function. And what you get is something\nlike this where, at meta test time, you give it\na video of placing",
    "start": "3798730",
    "end": "3805180"
  },
  {
    "text": "an object into a new container. And then the robot\nadopts its policy",
    "start": "3805180",
    "end": "3811390"
  },
  {
    "text": "using that video of\na human and then gets a policy that acts like this.",
    "start": "3811390",
    "end": "3818360"
  },
  {
    "text": "And as long as you have-- each task essentially\ncorresponds to dropping-- this\ntask corresponds to dropping the red bowl\nfor different positions",
    "start": "3818360",
    "end": "3826000"
  },
  {
    "text": "of the objects and\nthe distractors. ",
    "start": "3826000",
    "end": "3831290"
  },
  {
    "text": "Another example is\npredicting the properties of different molecules. This is potentially useful\nin drug discovery problems.",
    "start": "3831290",
    "end": "3839260"
  },
  {
    "text": "Different tasks\ncorrespond to predicting properties and activities\nof different molecules, and D train and D\ntest will correspond",
    "start": "3839260",
    "end": "3847120"
  },
  {
    "text": "to different instances\nof those molecules. In this case, they use\noptimization-based approaches",
    "start": "3847120",
    "end": "3853065"
  },
  {
    "text": "and they compared three\ndifferent approaches, which is MAML, first-order MAML,\nand almost no inner loop,",
    "start": "3853065",
    "end": "3858440"
  },
  {
    "text": "which essentially just does MAML\non the last layer of the model. And they used a gated\ngraph neural network",
    "start": "3858440",
    "end": "3865839"
  },
  {
    "text": "as the base model with MAML. And what they found is\nthat if you do things",
    "start": "3865840",
    "end": "3872109"
  },
  {
    "text": "like k nearest neighbors or fine\ntuning on top of a model, that doesn't perform as well as using\nmeta training on the molecules",
    "start": "3872110",
    "end": "3878950"
  },
  {
    "text": "they had.  Another example-- this\nis a regression example",
    "start": "3878950",
    "end": "3885730"
  },
  {
    "text": "like the first robotics example. You want to be able to predict\nhuman motion into the future,",
    "start": "3885730",
    "end": "3891828"
  },
  {
    "text": "and different tasks\nare going to correspond to different human users and\ndifferent kinds of motions.",
    "start": "3891828",
    "end": "3896980"
  },
  {
    "text": "And d train will correspond to\nthe past k time steps of motion and d test will\ncorrespond to d kind",
    "start": "3896980",
    "end": "3903099"
  },
  {
    "text": "of future time steps of motion. ",
    "start": "3903100",
    "end": "3908712"
  },
  {
    "text": "They use a kind of hybrid of\nan optimization-based approach and a black-box approach. And in particular, they use MAML\nwith this additional learned",
    "start": "3908712",
    "end": "3916390"
  },
  {
    "text": "update rule, and\ntheir base model was a recurrent neural\nnetwork because it needs to take as input the\nprevious time steps of motion.",
    "start": "3916390",
    "end": "3924040"
  },
  {
    "text": "This is kind of what their\nmodel looks like whereas it is adapting to the previous\ntime steps of motion and then predicting the blue\nfigures into the future.",
    "start": "3924040",
    "end": "3934237"
  },
  {
    "text": "And then here, here's kind of a\nsnapshot of the results, which they find that doing some\nform of transfer learning",
    "start": "3934237",
    "end": "3939370"
  },
  {
    "text": "or learning from\nscratch doesn't perform as well as their\nmeta-learning process.",
    "start": "3939370",
    "end": "3945920"
  },
  {
    "text": "Yeah? So going back a couple slides to\nthe imitation learning example.",
    "start": "3945920",
    "end": "3951940"
  },
  {
    "text": "MAML kind of needs some sort\nof reward signal in that case where you can just-- it's\nnot a supervised task.",
    "start": "3951940",
    "end": "3959820"
  },
  {
    "text": "Yeah. I'm asking, how do you\nperform the inner loop in those functions? Yeah. So this is kind of a\nmore advanced example.",
    "start": "3959820",
    "end": "3969380"
  },
  {
    "text": "And essentially what it looks\nlike is the one thing that you",
    "start": "3969380",
    "end": "3975470"
  },
  {
    "text": "might notice is that the\ntraining set and the test set are actually a\nlittle bit different.",
    "start": "3975470",
    "end": "3981452"
  },
  {
    "text": "One thing that you\ncould do instead of having it be a\nvideo of a human is just to use a\nteleoperated demonstration.",
    "start": "3981452",
    "end": "3986720"
  },
  {
    "text": "And in that case, you can just\nuse an imitation learning loss as your inner\nlearning objective.",
    "start": "3986720",
    "end": "3993950"
  },
  {
    "text": "And so kind of what that\nwould look like is something like, for your optimizing for\nthe parameters of your policy",
    "start": "3993950",
    "end": "4001450"
  },
  {
    "text": "such that this is your\nouter loss function.",
    "start": "4001450",
    "end": "4006775"
  },
  {
    "text": " This is your inner\nloss function. ",
    "start": "4006775",
    "end": "4014410"
  },
  {
    "text": "This is your training\ndemo, and then you also have your test demo.",
    "start": "4014410",
    "end": "4021160"
  },
  {
    "text": "This is over all of\nyour training tasks. And so what you can\ndo is, if you have a--",
    "start": "4021160",
    "end": "4027307"
  },
  {
    "text": "both a demo in the inner\nloop and the outer loop, then you can use an imitation\nlearning loss function, which",
    "start": "4027308",
    "end": "4032440"
  },
  {
    "text": "essentially just\ncorresponds to something like mean squared error\nbetween the true action",
    "start": "4032440",
    "end": "4039250"
  },
  {
    "text": "and the predicted action\nfor a given state. Now, if this is like\na video of a human,",
    "start": "4039250",
    "end": "4046790"
  },
  {
    "text": "then you don't actually\nhave the actions. And so one cool\nthing that you could",
    "start": "4046790",
    "end": "4052720"
  },
  {
    "text": "do is, instead of having\nthis be an imitation learning loss function,\nwhat we do instead is we actually learn\na loss function.",
    "start": "4052720",
    "end": "4060370"
  },
  {
    "text": "And the-- let me-- I think-- so we actually have\nparameters in this function",
    "start": "4060370",
    "end": "4069160"
  },
  {
    "text": "and we meta train both\nthe initialization as well as the parameters of\nthis loss function. ",
    "start": "4069160",
    "end": "4080400"
  },
  {
    "text": "Cool. And so if you're in\nscenarios where you actually don't have good label\nsignal in your training set, you can actually do\nsomething like this.",
    "start": "4080400",
    "end": "4086417"
  },
  {
    "start": "4086417",
    "end": "4091490"
  },
  {
    "text": "Cool. So we looked at the human\nmotion prediction example. And then one more application,\nfor example, that I'll mention",
    "start": "4091490",
    "end": "4097889"
  },
  {
    "text": "is language modeling. And so many of you might\nbe familiar with GPT 3.",
    "start": "4097890",
    "end": "4104068"
  },
  {
    "text": "This can also be\nviewed as a meta learner or a few-shot learner. And here the tasks are\nactually pretty diverse.",
    "start": "4104069",
    "end": "4111422"
  },
  {
    "text": "They have tasks like\nspelling correction, simple math problems,\ntranslating between languages, and a variety of others.",
    "start": "4111422",
    "end": "4118350"
  },
  {
    "text": "They're all represented as these\nlanguage generation problems where D train corresponds\nto a sequence of characters",
    "start": "4118350",
    "end": "4124020"
  },
  {
    "text": "and D test corresponds\nto the following sequence of characters. So for example, you can\ngive it some examples",
    "start": "4124020",
    "end": "4132060"
  },
  {
    "text": "of solving math\nproblems and then ask it to solve more\nmath problems for you. You can give it examples\nof correcting spelling",
    "start": "4132060",
    "end": "4138060"
  },
  {
    "text": "and it will-- and then ask it to\ncorrect spelling of more things and so forth.",
    "start": "4138060",
    "end": "4144460"
  },
  {
    "text": "The model that they use is a\nvery large transformer model, and so you can think\nof it as essentially",
    "start": "4144460",
    "end": "4149818"
  },
  {
    "text": "as a black-box meta learner. And then there are some cool\nexamples of what it can do.",
    "start": "4149819",
    "end": "4155679"
  },
  {
    "text": "So you can give it-- essentially the gray\ntext corresponds to D train and then\nthe black corresponds",
    "start": "4155680",
    "end": "4161939"
  },
  {
    "text": "to the generation\nfrom the model. And so you can give it\nan example of a new word that it should\nlearn and then ask",
    "start": "4161939",
    "end": "4169049"
  },
  {
    "text": "it to use that\nword in a sentence, and then it can generate\nan example of using",
    "start": "4169050",
    "end": "4174568"
  },
  {
    "text": "that word in the sentence. You can also ask it\nto edit language.",
    "start": "4174569",
    "end": "4180450"
  },
  {
    "text": "So to say, like, poor\nEnglish and good English. Give it, in this\ncase, 1, 2, 3, 4--",
    "start": "4180450",
    "end": "4186929"
  },
  {
    "text": "four examples of that. And then ask it to-- ask it to generate--\nactually, no, sorry.",
    "start": "4186930",
    "end": "4193020"
  },
  {
    "text": "Three examples. Yeah. Three examples up to\nhere, then give it another poor English\ninput and then ask",
    "start": "4193020",
    "end": "4198630"
  },
  {
    "text": "it to produce the\ncorresponding output.",
    "start": "4198630",
    "end": "4203639"
  },
  {
    "text": "And here's another\nkind of example of correcting the English.",
    "start": "4203640",
    "end": "4210090"
  },
  {
    "text": "Yeah? [INAUDIBLE] does it?",
    "start": "4210090",
    "end": "4217059"
  },
  {
    "text": "Yeah. So essentially-- the prompts\nhere essentially correspond to the kind of training\ndata for the example.",
    "start": "4217060",
    "end": "4224100"
  },
  {
    "text": "And it's essentially like\na black-box meta learner where it's the version of a\nblack-box meta learner where",
    "start": "4224100",
    "end": "4231180"
  },
  {
    "text": "you're outputting some hidden\nstate or something like that, rather than outputting a\nfull set of parameters.",
    "start": "4231180",
    "end": "4237552"
  },
  {
    "text": "And kind of similar to the\nkind of black-box meta-learners that you're implementing\nin homework 1. ",
    "start": "4237553",
    "end": "4246750"
  },
  {
    "text": "OK, and so one\nclosing note that I'd like to mention\nthat we saw actually in a lot of these\nexamples is that D train",
    "start": "4246750",
    "end": "4252540"
  },
  {
    "text": "and D test don't actually need\nto be sampled independently from the data for a task.",
    "start": "4252540",
    "end": "4258550"
  },
  {
    "text": "And this is pretty\ncool because this means that you can actually\nkind of tweak the meta training process in a lot\nof different ways.",
    "start": "4258550",
    "end": "4264910"
  },
  {
    "text": "You could have D train\nhave noisy labels and D test have clean\nlabels, and essentially learn how to learn\nfrom noisy labels.",
    "start": "4264910",
    "end": "4273090"
  },
  {
    "text": "Simply get an optimizer\nthat can learn well from the noisy labels. D train could also\nbe weakly supervised,",
    "start": "4273090",
    "end": "4279900"
  },
  {
    "text": "kind of like this\nexample right here where you actually\nlearned a loss function.",
    "start": "4279900",
    "end": "4285007"
  },
  {
    "text": "It could also have\nsome domain shifts. So you could have--\nyou could try to learn from data from\na different domain such",
    "start": "4285007",
    "end": "4290730"
  },
  {
    "text": "that it produces a\nmodel that works well in some target domain.",
    "start": "4290730",
    "end": "4296080"
  },
  {
    "text": "And so this is what would happen\nduring meta training time, and then at meta\ntest time you could give it data sets\nthat have noisy labels",
    "start": "4296080",
    "end": "4302820"
  },
  {
    "text": "or weak supervision and so\nforth and get models kind of like the ones that you want.",
    "start": "4302820",
    "end": "4307889"
  },
  {
    "text": "Yeah? So essentially, do\nwe just need that on the training distribution?",
    "start": "4307890",
    "end": "4316432"
  },
  {
    "text": "How does [INAUDIBLE] the\norder of the testing? Or [INAUDIBLE]?",
    "start": "4316432",
    "end": "4322575"
  },
  {
    "text": " Right. So you're asking, do you need\nto have training to necessarily",
    "start": "4322575",
    "end": "4329650"
  },
  {
    "text": "be harder than testing? Essentially what's\nreally important is that the desired\nbehavior of the model",
    "start": "4329650",
    "end": "4335560"
  },
  {
    "text": "is represented in\nD test because this is what's going to be used\nto actually train the model",
    "start": "4335560",
    "end": "4340930"
  },
  {
    "text": "to get some desired behavior. And then D train should\nrepresent the kinds of data that you're going to give\nit at meta test time.",
    "start": "4340930",
    "end": "4347000"
  },
  {
    "text": "And so if you want it to be\nable to learn from noisy labels at meta test time, then\nit's good to meta train it to be able to\nhandle noisy labels.",
    "start": "4347000",
    "end": "4354040"
  },
  {
    "text": "You could also try to have-- meta train it on harder tasks. Like meta train\nit on the ability to learn from noisy labels and\nmaybe that would make it also",
    "start": "4354040",
    "end": "4361420"
  },
  {
    "text": "do better at clean\nlabels because it's just a harder task. But that wouldn't-- more of what\nI'm trying to convey here is",
    "start": "4361420",
    "end": "4368903"
  },
  {
    "text": "you can actually do some cool\nthings with the meta learner that actually trains it to be\nable to handle different kinds of learning problems.",
    "start": "4368903",
    "end": "4374715"
  },
  {
    "text": " Cool.",
    "start": "4374715",
    "end": "4380060"
  },
  {
    "text": "So to recap what we\ntalked about today, we talked about quite\na lot of things. Nonparametric few-shot\nlearning approaches,",
    "start": "4380060",
    "end": "4386540"
  },
  {
    "text": "which learn to do these nearest\nneighbor matching queries. We talked about properties\nof meta learning algorithms",
    "start": "4386540",
    "end": "4392390"
  },
  {
    "text": "and compared\ndifferent approaches, and then we also looked\nat a few applications.",
    "start": "4392390",
    "end": "4399080"
  },
  {
    "text": "And then, yeah, to\nend, a few reminders. Homework 1 is due on Wednesday.",
    "start": "4399080",
    "end": "4404270"
  },
  {
    "text": "Also, please fill out the\nproject survey by Wednesday, and the project proposals\nwill be due next week.",
    "start": "4404270",
    "end": "4411940"
  },
  {
    "start": "4411940",
    "end": "4416214"
  }
]