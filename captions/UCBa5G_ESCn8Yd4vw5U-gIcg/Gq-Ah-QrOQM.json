[
  {
    "start": "0",
    "end": "6000"
  },
  {
    "start": "0",
    "end": "6270"
  },
  {
    "start": "6000",
    "end": "12000"
  },
  {
    "text": "Hi.",
    "start": "6270",
    "end": "6970"
  },
  {
    "text": "In this module, I'm\ngoing to be talking",
    "start": "6970",
    "end": "8594"
  },
  {
    "text": "about the generalization of\nmachine learning algorithms.",
    "start": "8595",
    "end": "12140"
  },
  {
    "start": "12000",
    "end": "57000"
  },
  {
    "text": "So recall that a machine\nlearning framework",
    "start": "12140",
    "end": "14170"
  },
  {
    "text": "has three design decisions.",
    "start": "14170",
    "end": "16010"
  },
  {
    "text": "The first is the\nhypothesis class,",
    "start": "16010",
    "end": "18410"
  },
  {
    "text": "which could be linear\npredictors or neural networks.",
    "start": "18410",
    "end": "21260"
  },
  {
    "text": "The second design decision\nis a loss function,",
    "start": "21260",
    "end": "23570"
  },
  {
    "text": "which in the case of regression,\nthis could be squared loss.",
    "start": "23570",
    "end": "26913"
  },
  {
    "text": "If it's a\nclassification, it could",
    "start": "26913",
    "end": "28330"
  },
  {
    "text": "be the hinge or logistic loss.",
    "start": "28330",
    "end": "30299"
  },
  {
    "text": "If you take the loss\nand you average them,",
    "start": "30299",
    "end": "32049"
  },
  {
    "text": "you get the training loss,\nwhich is our training objective",
    "start": "32049",
    "end": "35110"
  },
  {
    "text": "that we so far have\nbeen optimizing.",
    "start": "35110",
    "end": "37540"
  },
  {
    "text": "And finally, we have the\noptimization algorithm,",
    "start": "37540",
    "end": "40400"
  },
  {
    "text": "which is either gradient descent\nor stochastic gradient descent.",
    "start": "40400",
    "end": "44230"
  },
  {
    "text": "All good so far.",
    "start": "44230",
    "end": "46030"
  },
  {
    "text": "Now let's take a step back\nand be a little more critical.",
    "start": "46030",
    "end": "49840"
  },
  {
    "text": "Is this, the training\nloss in particular,",
    "start": "49840",
    "end": "52720"
  },
  {
    "text": "a good objective\nto be optimizing?",
    "start": "52720",
    "end": "57320"
  },
  {
    "start": "57000",
    "end": "102000"
  },
  {
    "text": "So here is a little cartoon\nexample that does really well",
    "start": "57320",
    "end": "61640"
  },
  {
    "text": "on training loss.",
    "start": "61640",
    "end": "63260"
  },
  {
    "text": "Here it goes.",
    "start": "63260",
    "end": "63890"
  },
  {
    "text": "It's called rote learning.",
    "start": "63890",
    "end": "66220"
  },
  {
    "text": "So the rote learning\nalgorithm is just",
    "start": "66220",
    "end": "68050"
  },
  {
    "text": "going to store all\nthe training examples.",
    "start": "68050",
    "end": "70780"
  },
  {
    "text": "And then it's going to\nreturn this predictor.",
    "start": "70780",
    "end": "73610"
  },
  {
    "text": "And this predictor\ntakes an input x.",
    "start": "73610",
    "end": "76750"
  },
  {
    "text": "And it's going to search for\nan x in the training set,",
    "start": "76750",
    "end": "79180"
  },
  {
    "text": "and if it can find\nit, then it is going",
    "start": "79180",
    "end": "81010"
  },
  {
    "text": "to return the corresponding y.",
    "start": "81010",
    "end": "83380"
  },
  {
    "text": "And otherwise, it just gives\nup and segfaults or crashes.",
    "start": "83380",
    "end": "87430"
  },
  {
    "text": "And so this learning algorithm\nminimizes the objective",
    "start": "87430",
    "end": "92170"
  },
  {
    "text": "perfectly.",
    "start": "92170",
    "end": "92710"
  },
  {
    "text": "It gets zero training loss.",
    "start": "92710",
    "end": "94510"
  },
  {
    "text": "But you can kind\nof tell that it's",
    "start": "94510",
    "end": "96730"
  },
  {
    "text": "a bad idea because it doesn't\nget anything else right.",
    "start": "96730",
    "end": "99490"
  },
  {
    "start": "99490",
    "end": "102380"
  },
  {
    "start": "102000",
    "end": "157000"
  },
  {
    "text": "So, this was an example\nof extreme overfitting.",
    "start": "102380",
    "end": "105979"
  },
  {
    "text": "Here are some examples of\nless extreme overfitting",
    "start": "105980",
    "end": "109400"
  },
  {
    "text": "in pictures.",
    "start": "109400",
    "end": "110840"
  },
  {
    "text": "So here's an example\nfrom classification.",
    "start": "110840",
    "end": "114229"
  },
  {
    "text": "You can see that the green\ndecision boundary here",
    "start": "114230",
    "end": "117580"
  },
  {
    "text": "tries really hard to separate\nthe blue and the red points",
    "start": "117580",
    "end": "120830"
  },
  {
    "text": "and does so successfully\ngetting zero training error.",
    "start": "120830",
    "end": "124370"
  },
  {
    "text": "But you can kind of intuitively\nsense that it's overfitting.",
    "start": "124370",
    "end": "128899"
  },
  {
    "text": "And perhaps this black decision\nboundary would be better.",
    "start": "128900",
    "end": "132939"
  },
  {
    "text": "In the case of\nregression, this red curve",
    "start": "132940",
    "end": "137890"
  },
  {
    "text": "gets zero training loss by\ngoing through all the training",
    "start": "137890",
    "end": "140560"
  },
  {
    "text": "points.",
    "start": "140560",
    "end": "141670"
  },
  {
    "text": "But you can see that\nit's overfitting",
    "start": "141670",
    "end": "143530"
  },
  {
    "text": "and instead maybe you should\nbe capturing the broader trend",
    "start": "143530",
    "end": "146530"
  },
  {
    "text": "using a simple line.",
    "start": "146530",
    "end": "149280"
  },
  {
    "text": "So, in general, if you\ntry to overly optimize",
    "start": "149280",
    "end": "152700"
  },
  {
    "text": "the training loss, then you\nrisk overfitting [AUDIO OUT]..",
    "start": "152700",
    "end": "156060"
  },
  {
    "start": "156060",
    "end": "158800"
  },
  {
    "start": "157000",
    "end": "277000"
  },
  {
    "text": "So then, what is\nthe true objective",
    "start": "158800",
    "end": "161100"
  },
  {
    "text": "if it isn't the training loss?",
    "start": "161100",
    "end": "162730"
  },
  {
    "text": "Well, to answer\nthat question, let's",
    "start": "162730",
    "end": "164530"
  },
  {
    "text": "take a step back and think,\nwhat are we trying to do?",
    "start": "164530",
    "end": "167200"
  },
  {
    "text": "Machine learning is\njust a means to an end.",
    "start": "167200",
    "end": "170110"
  },
  {
    "text": "The end is a\npredictor that you're",
    "start": "170110",
    "end": "172000"
  },
  {
    "text": "going to launch into the\nworld and make predictions",
    "start": "172000",
    "end": "174280"
  },
  {
    "text": "on real people.",
    "start": "174280",
    "end": "175930"
  },
  {
    "text": "And this just happens to\nbe trained from a learning",
    "start": "175930",
    "end": "179530"
  },
  {
    "text": "algorithm.",
    "start": "179530",
    "end": "181069"
  },
  {
    "text": "So how good is this\npredictor in the world?",
    "start": "181070",
    "end": "184720"
  },
  {
    "text": "Well, the answer\nis, it's the goal--",
    "start": "184720",
    "end": "189070"
  },
  {
    "text": "how good it is\ndepends on how well",
    "start": "189070",
    "end": "191620"
  },
  {
    "text": "it's able to predict on\nunseen future examples.",
    "start": "191620",
    "end": "195980"
  },
  {
    "text": "So a true learning\nobjective should",
    "start": "195980",
    "end": "198340"
  },
  {
    "text": "be to minimize the error\non unseen future examples.",
    "start": "198340",
    "end": "201280"
  },
  {
    "text": "Sounds great.",
    "start": "201280",
    "end": "202780"
  },
  {
    "text": "Only one small problem is\nthat we don't have access",
    "start": "202780",
    "end": "205450"
  },
  {
    "text": "to the future.",
    "start": "205450",
    "end": "206950"
  },
  {
    "text": "And in particular, if we\ndon't see the examples,",
    "start": "206950",
    "end": "209650"
  },
  {
    "text": "how can we do\nanything about them?",
    "start": "209650",
    "end": "212980"
  },
  {
    "text": "So, often we settle\nfor the next best",
    "start": "212980",
    "end": "215640"
  },
  {
    "text": "thing, which is get a test set.",
    "start": "215640",
    "end": "218370"
  },
  {
    "text": "And the test set is\njust a set of examples",
    "start": "218370",
    "end": "220409"
  },
  {
    "text": "that you didn't\nuse for training.",
    "start": "220410",
    "end": "221980"
  },
  {
    "text": "So it is a surrogate for\nthe unseen future examples.",
    "start": "221980",
    "end": "227239"
  },
  {
    "text": "So I make this\ndistinction because I",
    "start": "227240",
    "end": "230380"
  },
  {
    "text": "want to stress the fact\nthat when you deploy",
    "start": "230380",
    "end": "232540"
  },
  {
    "text": "a machine learning algorithm--",
    "start": "232540",
    "end": "234219"
  },
  {
    "text": "a predictor into the\nworld, it might encounter",
    "start": "234220",
    "end": "237250"
  },
  {
    "text": "all sorts of crazy things.",
    "start": "237250",
    "end": "240460"
  },
  {
    "text": "And what you do in the\ntraining, in the lab,",
    "start": "240460",
    "end": "246040"
  },
  {
    "text": "is-- all you have is a test set.",
    "start": "246040",
    "end": "248049"
  },
  {
    "text": "So what you're trying\nto do is trying",
    "start": "248050",
    "end": "250750"
  },
  {
    "text": "to have the test set be as\nclose and as representative",
    "start": "250750",
    "end": "254260"
  },
  {
    "text": "of what you actually get in\nthe real world as possible.",
    "start": "254260",
    "end": "257364"
  },
  {
    "start": "257365",
    "end": "261180"
  },
  {
    "text": "So now we have an\nintuitive feeling",
    "start": "261180",
    "end": "263840"
  },
  {
    "text": "for what overfitting is.",
    "start": "263840",
    "end": "265970"
  },
  {
    "text": "Can we make this a\nlittle bit more precise?",
    "start": "265970",
    "end": "268280"
  },
  {
    "text": "In particular, when does\na learning algorithm",
    "start": "268280",
    "end": "270260"
  },
  {
    "text": "generalize from the training\nset to the test set?",
    "start": "270260",
    "end": "273950"
  },
  {
    "text": "Because that's kind\nof what we settle for.",
    "start": "273950",
    "end": "277260"
  },
  {
    "start": "277000",
    "end": "428000"
  },
  {
    "text": "So, there is a way to make\nthis mathematically rigorous.",
    "start": "277260",
    "end": "281250"
  },
  {
    "text": "But I just want to give\nyou the framing of how",
    "start": "281250",
    "end": "285690"
  },
  {
    "text": "to think about generalization.",
    "start": "285690",
    "end": "289260"
  },
  {
    "text": "So the starting point is f star.",
    "start": "289260",
    "end": "292020"
  },
  {
    "text": "This is the predictor\nthat is the ideal thing.",
    "start": "292020",
    "end": "295900"
  },
  {
    "text": "It predicts everything as\ncorrectly as you can hope for.",
    "start": "295900",
    "end": "301110"
  },
  {
    "text": "This lives in the family\nof all predictors.",
    "start": "301110",
    "end": "304259"
  },
  {
    "text": "Of course, we can't\nget to f star.",
    "start": "304260",
    "end": "306570"
  },
  {
    "text": "So what do we do?",
    "start": "306570",
    "end": "307780"
  },
  {
    "text": "Well, we do two things.",
    "start": "307780",
    "end": "309370"
  },
  {
    "text": "We first define a\nhypothesis class, script F.",
    "start": "309370",
    "end": "314460"
  },
  {
    "text": "And then we are going to have\na learning algorithm that",
    "start": "314460",
    "end": "320610"
  },
  {
    "text": "finds a particular predictor\nwithin this hypothesis class.",
    "start": "320610",
    "end": "327210"
  },
  {
    "text": "So another predictor I'm\ngoing to talk about is g.",
    "start": "327210",
    "end": "333000"
  },
  {
    "text": "This is also a kind of a thing\nthat you can't get a hold of.",
    "start": "333000",
    "end": "335770"
  },
  {
    "text": "It's the best\npredictor that you can",
    "start": "335770",
    "end": "338310"
  },
  {
    "text": "find in the hypothesis class.",
    "start": "338310",
    "end": "342370"
  },
  {
    "text": "So now, we're interested\nin the difference",
    "start": "342370",
    "end": "346620"
  },
  {
    "text": "between the error of\nthe thing you have",
    "start": "346620",
    "end": "350590"
  },
  {
    "text": "and the thing that\nyou wish you had, OK?",
    "start": "350590",
    "end": "353370"
  },
  {
    "text": "So, mathematically,\nthat's written",
    "start": "353370",
    "end": "354870"
  },
  {
    "text": "as error of the learned\npredictor minus the error of f",
    "start": "354870",
    "end": "358514"
  },
  {
    "text": "star [AUDIO OUT].",
    "start": "358515",
    "end": "360840"
  },
  {
    "text": "And this error can be\ndecomposed into two parts.",
    "start": "360840",
    "end": "364780"
  },
  {
    "text": "The first part is the\napproximation error.",
    "start": "364780",
    "end": "367600"
  },
  {
    "text": "Approximation error\nis the difference",
    "start": "367600",
    "end": "369420"
  },
  {
    "text": "between g and f star.",
    "start": "369420",
    "end": "372000"
  },
  {
    "text": "Mathematically,\nthat's the difference",
    "start": "372000",
    "end": "374040"
  },
  {
    "text": "between the error of--",
    "start": "374040",
    "end": "375270"
  },
  {
    "start": "375270",
    "end": "377930"
  },
  {
    "text": "so approximation error is the\ndifference between the error",
    "start": "377930",
    "end": "381800"
  },
  {
    "text": "of g minus the error of f star.",
    "start": "381800",
    "end": "385840"
  },
  {
    "text": "This measures how good\nyour hypothesis class is.",
    "start": "385840",
    "end": "390130"
  },
  {
    "text": "The second error is the\nestimation error, which",
    "start": "390130",
    "end": "393760"
  },
  {
    "text": "is the gap between f hat and g.",
    "start": "393760",
    "end": "397750"
  },
  {
    "text": "This measures how good\nis the learned predictor",
    "start": "397750",
    "end": "400270"
  },
  {
    "text": "relative to the potential of\nthe hypothesis class, error",
    "start": "400270",
    "end": "403690"
  },
  {
    "text": "of f hat minus the error of g.",
    "start": "403690",
    "end": "407380"
  },
  {
    "text": "And you can verify this\nidentity because we're",
    "start": "407380",
    "end": "409600"
  },
  {
    "text": "doing just subtracting the error\nof g and adding error of g.",
    "start": "409600",
    "end": "413380"
  },
  {
    "text": "So this right-hand side is\nequal to this left-hand side.",
    "start": "413380",
    "end": "417430"
  },
  {
    "text": "This kind of trivial\nidentity highlights",
    "start": "417430",
    "end": "419770"
  },
  {
    "text": "these two quantities'\napproximation error",
    "start": "419770",
    "end": "422139"
  },
  {
    "text": "and estimation\nerror and gives us",
    "start": "422140",
    "end": "423820"
  },
  {
    "text": "a language to talk about the\ntrade-offs and generalizations.",
    "start": "423820",
    "end": "426958"
  },
  {
    "start": "426958",
    "end": "429590"
  },
  {
    "start": "428000",
    "end": "508000"
  },
  {
    "text": "So let's get some more intuition\nabout how approximation",
    "start": "429590",
    "end": "432980"
  },
  {
    "text": "and estimation\nerror behave as you",
    "start": "432980",
    "end": "435530"
  },
  {
    "text": "increase the size of\nthe hypothesis class.",
    "start": "435530",
    "end": "439650"
  },
  {
    "text": "So when the hypothesis\nclass grows,",
    "start": "439650",
    "end": "443669"
  },
  {
    "text": "the approximation\nerror will decrease.",
    "start": "443670",
    "end": "447240"
  },
  {
    "text": "This is because the\napproximation error",
    "start": "447240",
    "end": "450349"
  },
  {
    "text": "is measuring how\ngood g is and the g",
    "start": "450350",
    "end": "453978"
  },
  {
    "text": "is the best thing in the class.",
    "start": "453978",
    "end": "455270"
  },
  {
    "text": "And if you're\nadding more things,",
    "start": "455270",
    "end": "456740"
  },
  {
    "text": "the best thing is just\ngoing to get better.",
    "start": "456740",
    "end": "460590"
  },
  {
    "text": "So, in other words,\nyou're taking a min",
    "start": "460590",
    "end": "462290"
  },
  {
    "text": "over a larger set [AUDIO OUT]\nwhere you're optimizing.",
    "start": "462290",
    "end": "469110"
  },
  {
    "text": "The second thing that happens\nis that the estimation error",
    "start": "469110",
    "end": "471960"
  },
  {
    "text": "increases when the\nhypothesis class grows.",
    "start": "471960",
    "end": "477130"
  },
  {
    "text": "And this is because it's\nharder to estimate something",
    "start": "477130",
    "end": "481200"
  },
  {
    "text": "more complex.",
    "start": "481200",
    "end": "482058"
  },
  {
    "text": "There's just more functions\nthat the learning algorithm",
    "start": "482058",
    "end": "484350"
  },
  {
    "text": "has to figure out which one\nis the correct one given",
    "start": "484350",
    "end": "489000"
  },
  {
    "text": "the limited data.",
    "start": "489000",
    "end": "491190"
  },
  {
    "text": "So there are ways to make\nthis more precise using",
    "start": "491190",
    "end": "494128"
  },
  {
    "text": "the tools from statistical\nlearning theory,",
    "start": "494128",
    "end": "495919"
  },
  {
    "text": "but I'll just leave it\nas intuition for now.",
    "start": "495920",
    "end": "500160"
  },
  {
    "text": "So, given these\ntrade-offs, what are",
    "start": "500160",
    "end": "502920"
  },
  {
    "text": "the ways that we can use to\ncontrol the hypothesis class",
    "start": "502920",
    "end": "506280"
  },
  {
    "text": "size?",
    "start": "506280",
    "end": "508490"
  },
  {
    "start": "508000",
    "end": "565000"
  },
  {
    "text": "So we're going to\nfocus our attention",
    "start": "508490",
    "end": "510080"
  },
  {
    "text": "to a linear predictor.",
    "start": "510080",
    "end": "511430"
  },
  {
    "text": "But remember, in\nlinear predictors,",
    "start": "511430",
    "end": "514990"
  },
  {
    "text": "each predictor has a\nparticular weight vector.",
    "start": "514990",
    "end": "518370"
  },
  {
    "text": "So effectively the number of--",
    "start": "518370",
    "end": "520289"
  },
  {
    "text": "the size of the set\nof weight vectors",
    "start": "520289",
    "end": "522240"
  },
  {
    "text": "determines the size of\nthe hypothesis class.",
    "start": "522240",
    "end": "526310"
  },
  {
    "text": "So one thing you\ncan do is to reduce",
    "start": "526310",
    "end": "530170"
  },
  {
    "text": "the dimensionality of the set\nof possible weight vectors.",
    "start": "530170",
    "end": "535730"
  },
  {
    "text": "So pictorially, this\nlooks like this.",
    "start": "535730",
    "end": "537440"
  },
  {
    "text": "So imagine you had\nthree features.",
    "start": "537440",
    "end": "540170"
  },
  {
    "text": "So the set of weight vectors for\nthis three-dimensional weight",
    "start": "540170",
    "end": "544000"
  },
  {
    "text": "vector is just a ball.",
    "start": "544000",
    "end": "547690"
  },
  {
    "text": "And if we remove one\nfeature, then you",
    "start": "547690",
    "end": "550600"
  },
  {
    "text": "end up with a\ntwo-dimensional ball.",
    "start": "550600",
    "end": "554709"
  },
  {
    "text": "Equivalently, this is\nsaying one of the features",
    "start": "554710",
    "end": "557440"
  },
  {
    "text": "has to have zero weight,\nwhich you can think about",
    "start": "557440",
    "end": "559780"
  },
  {
    "text": "as a restriction on a set of\nvalues that w should have.",
    "start": "559780",
    "end": "565550"
  },
  {
    "start": "565000",
    "end": "630000"
  },
  {
    "text": "So how do you control the\ndimensionality in practice?",
    "start": "565550",
    "end": "569330"
  },
  {
    "text": "The process is called\nfeature selection,",
    "start": "569330",
    "end": "571790"
  },
  {
    "text": "or feature template selection.",
    "start": "571790",
    "end": "573800"
  },
  {
    "text": "You can do this manually by\nadding feature templates,",
    "start": "573800",
    "end": "576410"
  },
  {
    "text": "seeing if they help, and\nremoving them if they don't.",
    "start": "576410",
    "end": "579470"
  },
  {
    "text": "And you're trying\nto kind of manually",
    "start": "579470",
    "end": "581689"
  },
  {
    "text": "figure out what is the smallest\nset of features that actually",
    "start": "581690",
    "end": "585560"
  },
  {
    "text": "gets you good accuracy.",
    "start": "585560",
    "end": "589430"
  },
  {
    "text": "There's also ways to do\nthis more automatically.",
    "start": "589430",
    "end": "592190"
  },
  {
    "text": "You can do forward selection,\nboosting, or L1 regularization.",
    "start": "592190",
    "end": "595380"
  },
  {
    "text": "This is beyond the\nscope of the class.",
    "start": "595380",
    "end": "597590"
  },
  {
    "text": "But there are ways to\nmake this less manual.",
    "start": "597590",
    "end": "602370"
  },
  {
    "text": "One thing I want to\nstress is that controlling",
    "start": "602370",
    "end": "605141"
  },
  {
    "text": "the dimensionality--\ndimensionality",
    "start": "605142",
    "end": "606600"
  },
  {
    "text": "is this number of features.",
    "start": "606600",
    "end": "608880"
  },
  {
    "text": "And that's the key\nquantity that matters,",
    "start": "608880",
    "end": "611290"
  },
  {
    "text": "not the number of\nfeature templates,",
    "start": "611290",
    "end": "613410"
  },
  {
    "text": "and also not the complexity\nof each individual feature.",
    "start": "613410",
    "end": "617519"
  },
  {
    "text": "So imagine you write 1,000\nlines to compute one feature.",
    "start": "617520",
    "end": "620880"
  },
  {
    "text": "Well, it's still a very\nsimple hypothesis class",
    "start": "620880",
    "end": "624420"
  },
  {
    "text": "because it's just\none feature in so far",
    "start": "624420",
    "end": "627420"
  },
  {
    "text": "as generalization is concerned.",
    "start": "627420",
    "end": "631540"
  },
  {
    "start": "630000",
    "end": "762000"
  },
  {
    "text": "So the second strategy\nis controlling",
    "start": "631540",
    "end": "634329"
  },
  {
    "text": "the norm or the length\nof this weight vector.",
    "start": "634330",
    "end": "638430"
  },
  {
    "text": "So we can reduce the\nnorm of-- or the length.",
    "start": "638430",
    "end": "641790"
  },
  {
    "text": "Visually, this looks like if\nyou have a set of weight vectors",
    "start": "641790",
    "end": "645959"
  },
  {
    "text": "which are bounded\nin length, you can",
    "start": "645960",
    "end": "649470"
  },
  {
    "text": "shrink the length\nand that results",
    "start": "649470",
    "end": "651990"
  },
  {
    "text": "in a smaller circle, which\nis pointedly a smaller",
    "start": "651990",
    "end": "655890"
  },
  {
    "text": "number of weight factors.",
    "start": "655890",
    "end": "658630"
  },
  {
    "text": "And so this is probably the most\ncommon way to control the norm.",
    "start": "658630",
    "end": "667650"
  },
  {
    "text": "So there are two\nways to do this.",
    "start": "667650",
    "end": "671680"
  },
  {
    "text": "One is by regularization.",
    "start": "671680",
    "end": "674580"
  },
  {
    "text": "So remember the objective,\nwhich we didn't like,",
    "start": "674580",
    "end": "677220"
  },
  {
    "text": "was minimizing the\ntraining loss of w",
    "start": "677220",
    "end": "681120"
  },
  {
    "text": "because that can\nlead to overfitting.",
    "start": "681120",
    "end": "683990"
  },
  {
    "text": "So one way to regularize\nis you add a penalty term--",
    "start": "683990",
    "end": "689209"
  },
  {
    "text": "lambda over 2 times\nthe norm of w squared.",
    "start": "689210",
    "end": "694200"
  },
  {
    "text": "So w is a positive\nnumber which controls",
    "start": "694200",
    "end": "697310"
  },
  {
    "text": "the strength of this penalty.",
    "start": "697310",
    "end": "699830"
  },
  {
    "text": "And what this penalty\ndoes is it says,",
    "start": "699830",
    "end": "702860"
  },
  {
    "text": "let's try to minimize\nthe training loss,",
    "start": "702860",
    "end": "706550"
  },
  {
    "text": "but we also want to keep\nthe norm small because we're",
    "start": "706550",
    "end": "711740"
  },
  {
    "text": "taking a min over the sum here.",
    "start": "711740",
    "end": "714490"
  },
  {
    "text": "So if we look at what gradient\ndescent does to this objective,",
    "start": "714490",
    "end": "717600"
  },
  {
    "text": "we can interpret it as follows.",
    "start": "717600",
    "end": "719444"
  },
  {
    "text": "So gradient descent,\nremember, initializes weights,",
    "start": "719445",
    "end": "722400"
  },
  {
    "text": "iterates over t fx and\nperforms an update.",
    "start": "722400",
    "end": "726630"
  },
  {
    "text": "So the update is w minus\neta, the step size,",
    "start": "726630",
    "end": "729480"
  },
  {
    "text": "times this gradient\nof the training loss.",
    "start": "729480",
    "end": "733980"
  },
  {
    "text": "And now we take the gradient\nof this penalty, which",
    "start": "733980",
    "end": "737100"
  },
  {
    "text": "is just lambda times w.",
    "start": "737100",
    "end": "740050"
  },
  {
    "text": "So, remember we're\nsubtracting eta.",
    "start": "740050",
    "end": "742930"
  },
  {
    "text": "So if w is let's say, 10, 10,\nthen what we're going to do",
    "start": "742930",
    "end": "750160"
  },
  {
    "text": "is we're going to\nsubtract that vector",
    "start": "750160",
    "end": "752170"
  },
  {
    "text": "and move the weights closer to\n0 by an amount that depends.",
    "start": "752170",
    "end": "757450"
  },
  {
    "start": "757450",
    "end": "762890"
  },
  {
    "start": "762000",
    "end": "825000"
  },
  {
    "text": "So another way to control the\nnorm is by early stopping.",
    "start": "762890",
    "end": "767510"
  },
  {
    "text": "So, early stopping is\nreally easy to explain.",
    "start": "767510",
    "end": "771670"
  },
  {
    "text": "So here it is.",
    "start": "771670",
    "end": "773600"
  },
  {
    "text": "You run gradient descent.",
    "start": "773600",
    "end": "774769"
  },
  {
    "text": "You initialize w.",
    "start": "774770",
    "end": "776210"
  },
  {
    "text": "And you repeat a\nnumber of epochs.",
    "start": "776210",
    "end": "778460"
  },
  {
    "text": "And you perform the update.",
    "start": "778460",
    "end": "780567"
  },
  {
    "text": "And the only thing\nis that you're just",
    "start": "780567",
    "end": "782150"
  },
  {
    "text": "going to reduce the number\nof epochs you go for.",
    "start": "782150",
    "end": "785210"
  },
  {
    "text": "That's it.",
    "start": "785210",
    "end": "787170"
  },
  {
    "text": "So this seems like a hack.",
    "start": "787170",
    "end": "789790"
  },
  {
    "text": "There is-- you can develop\nsome theory about it.",
    "start": "789790",
    "end": "791790"
  },
  {
    "text": "But the intuition is that when\nyou start the weights at 0,",
    "start": "791790",
    "end": "796290"
  },
  {
    "text": "that's the smallest norm.",
    "start": "796290",
    "end": "797699"
  },
  {
    "text": "And when you update the weights\nover a number of iterations,",
    "start": "797700",
    "end": "801120"
  },
  {
    "text": "the norm of w is\nactually going to grow.",
    "start": "801120",
    "end": "803700"
  },
  {
    "text": "It's not obvious that\nthis always happens,",
    "start": "803700",
    "end": "805710"
  },
  {
    "text": "but empirically it\nis true, generally.",
    "start": "805710",
    "end": "809100"
  },
  {
    "text": "So by stopping gradient\ndescent early, you're saying,",
    "start": "809100",
    "end": "812910"
  },
  {
    "text": "don't let the norm\nof w get too big.",
    "start": "812910",
    "end": "816440"
  },
  {
    "text": "So the lesson here\nis you're trying",
    "start": "816440",
    "end": "818330"
  },
  {
    "text": "to minimize the training error.",
    "start": "818330",
    "end": "819867"
  },
  {
    "text": "But you're not trying too\nhard because you're just going",
    "start": "819867",
    "end": "822200"
  },
  {
    "text": "to call it quits after a while.",
    "start": "822200",
    "end": "823492"
  },
  {
    "start": "823492",
    "end": "826570"
  },
  {
    "start": "825000",
    "end": "894000"
  },
  {
    "text": "OK, so let's summarize now.",
    "start": "826570",
    "end": "828820"
  },
  {
    "text": "So we started by saying\nthe training loss is not",
    "start": "828820",
    "end": "833440"
  },
  {
    "text": "the true objective.",
    "start": "833440",
    "end": "836080"
  },
  {
    "text": "The real objective is\nminimizing the loss",
    "start": "836080",
    "end": "839650"
  },
  {
    "text": "on unseen future examples.",
    "start": "839650",
    "end": "842500"
  },
  {
    "text": "Unfortunately, we don't\nhave access to that.",
    "start": "842500",
    "end": "844820"
  },
  {
    "text": "So we're going to settle for\nthe loss on some test data which",
    "start": "844820",
    "end": "848770"
  },
  {
    "text": "serves as a surrogate\nto the unseen examples.",
    "start": "848770",
    "end": "852970"
  },
  {
    "text": "Then we studied approximation\nand estimation error as a way",
    "start": "852970",
    "end": "857050"
  },
  {
    "text": "to understand generalization.",
    "start": "857050",
    "end": "859630"
  },
  {
    "text": "And it's always just\ngoing to be a balancing",
    "start": "859630",
    "end": "862030"
  },
  {
    "text": "act between fitting\nthe training error",
    "start": "862030",
    "end": "864310"
  },
  {
    "text": "and not letting your\nhypothesis class grow too big.",
    "start": "864310",
    "end": "871450"
  },
  {
    "text": "And the mantra to end with is,\nperhaps, just keep it simple.",
    "start": "871450",
    "end": "877390"
  },
  {
    "text": "So right now we've\nintroduced a bunch of knobs",
    "start": "877390",
    "end": "880600"
  },
  {
    "text": "for varying the size of\nthe hypothesis class.",
    "start": "880600",
    "end": "884620"
  },
  {
    "text": "Next, we'll see how to\nactually turn the knobs.",
    "start": "884620",
    "end": "888480"
  },
  {
    "start": "888480",
    "end": "893000"
  }
]