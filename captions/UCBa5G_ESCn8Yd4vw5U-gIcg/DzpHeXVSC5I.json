[
  {
    "text": " So the thing that seems\namazing to me and us",
    "start": "0",
    "end": "9980"
  },
  {
    "text": "is the fact that, well,\nactually, this course was taught just last quarter.",
    "start": "9980",
    "end": "15110"
  },
  {
    "text": "And here we are with an\nenormous number of people, again, taking this class.",
    "start": "15110",
    "end": "20750"
  },
  {
    "text": "I guess that says something\nmaybe approximately what it says is ChatGPT.",
    "start": "20750",
    "end": "26785"
  },
  {
    "text": "[LAUGHTER]  But anyway, it's\ngreat to have you all.",
    "start": "26785",
    "end": "33440"
  },
  {
    "text": "Lots of exciting content\nto have, and hope you'll all enjoy it. So let me get started\nand start telling you",
    "start": "33440",
    "end": "41930"
  },
  {
    "text": "a bit about the\ncourse before diving straight into today's content.",
    "start": "41930",
    "end": "47359"
  },
  {
    "text": "For people still coming in,\nthere are oodles of seats still right on either side,\nespecially down near the front.",
    "start": "47360",
    "end": "55260"
  },
  {
    "text": "There are tons of\nseats, so do feel empowered to go out\nand seek those seats.",
    "start": "55260",
    "end": "62930"
  },
  {
    "text": "If people on the\ncorridors are really nice, they could even move\ntowards the edges",
    "start": "62930",
    "end": "67960"
  },
  {
    "text": "to make it easier for people. But one way or another,\nfeel free to find a seat. So this is the plan for what\nI want to get through today.",
    "start": "67960",
    "end": "76579"
  },
  {
    "text": "So first of all, I'm\ngoing to tell you about the course\nfor a few minutes. Then have a few remarks about\nhuman language and word meaning.",
    "start": "76580",
    "end": "86680"
  },
  {
    "text": "Then the main technical thing\nwe want to get into today is start learning about\nthe word2vec algorithm.",
    "start": "86680",
    "end": "93680"
  },
  {
    "text": "So the word2vec algorithm is\nslightly over a decade old now. It was introduced in 2013.",
    "start": "93680",
    "end": "102439"
  },
  {
    "text": "But it was a wildly\nsuccessful simple way of learning vector\nrepresentations of words.",
    "start": "102440",
    "end": "109219"
  },
  {
    "text": "So I want to show you that as\na sort of a first easy baby system for the kind of\nneural representations",
    "start": "109220",
    "end": "116829"
  },
  {
    "text": "that were going to\ntalk about in class. We're then going to get\nmore concrete with that, looking at its objective\nfunction gradients",
    "start": "116830",
    "end": "124290"
  },
  {
    "text": "and optimization. And then hopefully, if all\ngoes, like stick to schedule,",
    "start": "124290",
    "end": "129310"
  },
  {
    "text": "spend a few minutes just playing\naround an IPython Notebook--",
    "start": "129310",
    "end": "135150"
  },
  {
    "text": "I'm going to have to\nchange computers for that-- then seeing some of the\nthings you can do with this.",
    "start": "135150",
    "end": "142300"
  },
  {
    "text": "So this is the course\nlogistics and brief. I'm Christopher Manning. Hi, again, everyone.",
    "start": "142300",
    "end": "149220"
  },
  {
    "text": "The head TA is [MUTED]\nwho unfortunately has a bit of a health problem\nso he's not actually here today.",
    "start": "149220",
    "end": "156360"
  },
  {
    "text": "We've got a course\nmanager for the course who is [MUTED] whom\nis up the back there.",
    "start": "156360",
    "end": "162810"
  },
  {
    "text": "And then we've got\na whole lot of TAs. If you're a TA who's\nhere, you could stand up",
    "start": "162810",
    "end": "168840"
  },
  {
    "text": "and wave or something\nlike that so people can see a few of the TAs\nand see some friendly faces.",
    "start": "168840",
    "end": "176209"
  },
  {
    "text": "We've got some TAs\nand some other ones. And so you can look at\nthem on the website.",
    "start": "176210",
    "end": "182360"
  },
  {
    "text": "If you're here, you know\nwhat time the class is. There's an email list.",
    "start": "182360",
    "end": "187770"
  },
  {
    "text": "But preferably don't use it\nand use the Ed site that you can find on the course website.",
    "start": "187770",
    "end": "194460"
  },
  {
    "text": "So the main place to go\nand look for information is the course website,\nwhich we've got up here.",
    "start": "194460",
    "end": "199860"
  },
  {
    "text": "And that then links\nin to Ed, which is what we're going to use\nas the main discussion board.",
    "start": "199860",
    "end": "205800"
  },
  {
    "text": "Please use that rather\nthan sending emails.  The first assignment\nfor this class,",
    "start": "205800",
    "end": "213030"
  },
  {
    "text": "it's a sort of an easy one. It's the warm up\nassignment, but we want to get people busy and\ndoing stuff straight away.",
    "start": "213030",
    "end": "220230"
  },
  {
    "text": "So the first assignment is\nalready live on the web page, and it's due next\nTuesday before class.",
    "start": "220230",
    "end": "227819"
  },
  {
    "text": "So slightly less than\nseven days left to do it. So do get started on that.",
    "start": "227820",
    "end": "234370"
  },
  {
    "text": "And to help with that, we're\ngoing to be immediately starting office hours tomorrow.",
    "start": "234370",
    "end": "239870"
  },
  {
    "text": "And they're also\ndescribed on the website. We also do a few\ntutorials on Friday.",
    "start": "239870",
    "end": "246520"
  },
  {
    "text": "The first of these tutorials is\na tutorial on Python and NumPy. Many people don't need\nthat because they've",
    "start": "246520",
    "end": "253120"
  },
  {
    "text": "done other classes\nand done this. But for some people, we try\nand make this class accessible",
    "start": "253120",
    "end": "259000"
  },
  {
    "text": "to everybody. So if you'd like to\nbrush up a bit on Python or how to use NumPy, it's a\ngreat thing to go along to.",
    "start": "259000",
    "end": "266180"
  },
  {
    "text": "And [MUTED] who's\nright over there is going to be\nteaching it on Friday. What do we hope to teach?",
    "start": "266180",
    "end": "274310"
  },
  {
    "text": "At the end of the quarter\nwhen you get the eval, you'll be asked to rate whether\nthis class met its learning",
    "start": "274310",
    "end": "282310"
  },
  {
    "text": "goals. These are my learning goals. What are they?",
    "start": "282310",
    "end": "288650"
  },
  {
    "text": "So the first one is to teach\nyou about the foundations and current methods for\nusing deep learning applied",
    "start": "288650",
    "end": "296790"
  },
  {
    "text": "to natural language processing. So this class tries to\nbuild up from the bottom up.",
    "start": "296790",
    "end": "302620"
  },
  {
    "text": "So we start off doing simple\nthings like word vectors and feedforward neural\nnetworks, recurrent networks",
    "start": "302620",
    "end": "308460"
  },
  {
    "text": "and attention. We then, fairly quickly, move\ninto the kind of key methods they use for NLP.",
    "start": "308460",
    "end": "315180"
  },
  {
    "text": "In 2024, I wrote down here\ntransformers encoder decoder",
    "start": "315180",
    "end": "320430"
  },
  {
    "text": "models. I probably should have\nwritten large language models somewhere in this list as well. But then, pre-training\nand post-training",
    "start": "320430",
    "end": "327600"
  },
  {
    "text": "of large language\nmodels, adaptation model, interpretability\nagents, et cetera. But that's not the only\nthing that we want to do.",
    "start": "327600",
    "end": "335057"
  },
  {
    "text": "So there are a couple\nof other things that we crucially\nwant to achieve. The second is to give you some\nunderstanding of human languages",
    "start": "335058",
    "end": "345420"
  },
  {
    "text": "and the difficulties in\nunderstanding and producing them on computers. Now, there are a few\nof you in this class",
    "start": "345420",
    "end": "351860"
  },
  {
    "text": "who are linguistics majors\nor perhaps a symbolic systems majors. Yay to the symbolic\nsystems majors.",
    "start": "351860",
    "end": "358170"
  },
  {
    "text": "But for quite a few\nof the rest of you, you'll never see any\nlinguistics in the sense",
    "start": "358170",
    "end": "365270"
  },
  {
    "text": "of understanding how language\nworks apart from this class. So we do want to try and convey\na little bit of a sense of what",
    "start": "365270",
    "end": "372650"
  },
  {
    "text": "some of the issues are\nin language structure, and why it's proven to be quite\ndifficult to get computers",
    "start": "372650",
    "end": "381230"
  },
  {
    "text": "to understand human\nlanguages, even though humans seem very good at learning\nto understand each other.",
    "start": "381230",
    "end": "387770"
  },
  {
    "text": "And then the final thing\nthat we want to make it onto is actually concretely\nbuilding systems.",
    "start": "387770",
    "end": "394110"
  },
  {
    "text": "So this isn't just\na theory class, that we actually want you to\nleave this class thinking,",
    "start": "394110",
    "end": "400430"
  },
  {
    "text": "oh, yeah, in my\nfirst job, wherever you go, whether it's a\nstartup or a big tech",
    "start": "400430",
    "end": "406099"
  },
  {
    "text": "or some non-profit,\nthere's something they want to do\nthat they'd like,",
    "start": "406100",
    "end": "411640"
  },
  {
    "text": "that would be useful if we had\na text classification system. Or we did information\nextraction to get some kind",
    "start": "411640",
    "end": "417220"
  },
  {
    "text": "of facts out of documents. I know how to build that. I can build that system\nbecause I did CS224N.",
    "start": "417220",
    "end": "425590"
  },
  {
    "text": "Here's how you get graded. So we have four assignments,\nmainly one and 1/2 weeks long",
    "start": "425590",
    "end": "433580"
  },
  {
    "text": "apart from the first one. They make up almost\nhalf the grade. The other half of\nthe grade is made up",
    "start": "433580",
    "end": "440380"
  },
  {
    "text": "of a final project, which there\nare two variants of custom or a default final\nproject, which",
    "start": "440380",
    "end": "447069"
  },
  {
    "text": "we'll get on to in a minute. And then there's a few percent\nthat go for participation.",
    "start": "447070",
    "end": "454060"
  },
  {
    "text": "Six late days. Collaboration policy. Like all other CS\nclasses, we've had issues",
    "start": "454060",
    "end": "462640"
  },
  {
    "text": "with people not\ndoing their own work. We really do want you to\nlearn things in this class.",
    "start": "462640",
    "end": "468490"
  },
  {
    "text": "And the way you do that\nis by doing your own work. So make sure you\nunderstand that.",
    "start": "468490",
    "end": "475230"
  },
  {
    "text": "And so for the\nassignments, everyone is expected to do\ntheir own assignments.",
    "start": "475230",
    "end": "480570"
  },
  {
    "text": "You can talk to your\nfriends, but you're expected to do your own assignment. For the final project, you\ncan do that as a group.",
    "start": "480570",
    "end": "488370"
  },
  {
    "text": "Then we have the\nissue of AI tools. Now, of course, in this class,\nwe love large language models.",
    "start": "488370",
    "end": "496060"
  },
  {
    "text": "But nevertheless,\nwe don't want you to do your assignments\nby saying, hey, ChatGPT,",
    "start": "496060",
    "end": "501790"
  },
  {
    "text": "could you answer\nquestion 3 for me? That is not the way\nto learn things.",
    "start": "501790",
    "end": "506940"
  },
  {
    "text": "If you want to make use of\nAI as a tool to assist you, such as for coding\nassistance, go for it.",
    "start": "506940",
    "end": "514140"
  },
  {
    "text": "But we're wanting\nyou to be working out how to answer assignment\nquestions by yourself.",
    "start": "514140",
    "end": "521459"
  },
  {
    "text": "So this is what the\nassignments look like. So assignment 1 is meant\nto be an easy on ramp,",
    "start": "521460",
    "end": "527220"
  },
  {
    "text": "and it's done as a\nJupyter Notebook. Assignment 2, then, has people--",
    "start": "527220",
    "end": "536220"
  },
  {
    "text": "what can I say? Here we are at this fine\nliberal arts and engineering",
    "start": "536220",
    "end": "543480"
  },
  {
    "text": "institution. We're not at a coding bootcamp. So we hope that people have\nsome deep understanding",
    "start": "543480",
    "end": "550019"
  },
  {
    "text": "of how things work. So in assignment 2, we actually\nwant you to do some math",
    "start": "550020",
    "end": "556320"
  },
  {
    "text": "and understand how things\nwork in neural networks. So for some people, assignment\n2 is the scariest assignment",
    "start": "556320",
    "end": "564630"
  },
  {
    "text": "in the whole class. But then, it's also\nthe place where we introduce PyTorch, which\nis a software package we use",
    "start": "564630",
    "end": "571320"
  },
  {
    "text": "for building neural networks. And we build a\ndependency parser, which we'll get to later as\nsomething more linguistic.",
    "start": "571320",
    "end": "579420"
  },
  {
    "text": "Then for assignment\n3 and 4, we move on to larger projects\nusing PyTorch with GPUs,",
    "start": "579420",
    "end": "585750"
  },
  {
    "text": "and we'll be making\nuse of Google Cloud. And for those two\nassignments, we",
    "start": "585750",
    "end": "591890"
  },
  {
    "text": "look at doing\nmachine translation and getting information\nout with transformers.",
    "start": "591890",
    "end": "598680"
  },
  {
    "text": "And then these are the\ntwo final project options. So essentially, we have\na default final project",
    "start": "598680",
    "end": "605209"
  },
  {
    "text": "where we give you a\nlot of scaffolding and an outline of what to do. But it's still an\nopen-ended project.",
    "start": "605210",
    "end": "611893"
  },
  {
    "text": "There are lots of\ndifferent things you can try to make\nthis system work better, and we encourage you to explore.",
    "start": "611893",
    "end": "618870"
  },
  {
    "text": "But nevertheless, you're given\na leg up from quite a lot of scaffolding.",
    "start": "618870",
    "end": "624660"
  },
  {
    "text": "We'll talk about this more, but\nyou can either do that option, or you can just come up with\ntotally your own project",
    "start": "624660",
    "end": "630440"
  },
  {
    "text": "and do that. That's the course. Any questions on the course?",
    "start": "630440",
    "end": "637800"
  },
  {
    "text": "Yes? In the final project,\nhow are mentors assigned? So if you can find your\nown mentor, your interest",
    "start": "637800",
    "end": "648155"
  },
  {
    "text": "in something, and\nthere's someone that's happy to mentor you,\nthat person can be your mentor.",
    "start": "648155",
    "end": "653500"
  },
  {
    "text": "Otherwise, one of the course\nTAs will be your mentor. And how that person\nis assigned is",
    "start": "653500",
    "end": "661260"
  },
  {
    "text": "one of the TAs, who is in\ncharge of final projects, assigns people. And they do the best\nthey can in terms",
    "start": "661260",
    "end": "668490"
  },
  {
    "text": "of finding people\nwith some expertise and having to divide all the\nstudents across the mentors roughly equally.",
    "start": "668490",
    "end": "675300"
  },
  {
    "text": "Any other questions?  OK, I'll power ahead.",
    "start": "675300",
    "end": "682440"
  },
  {
    "text": "Human language\nand word meaning-- so let me just say a little\nbit about the big picture here.",
    "start": "682440",
    "end": "693000"
  },
  {
    "text": "So we're in the area of\nartificial intelligence, and we've got this idea\nthat humans are intelligent.",
    "start": "693000",
    "end": "700800"
  },
  {
    "text": "And then there's the question of\nhow does language fit into that?",
    "start": "700800",
    "end": "707399"
  },
  {
    "text": "And this is something that\nthere is some argument about. And if you want to, you can\nrun off onto social media",
    "start": "707400",
    "end": "714170"
  },
  {
    "text": "and read some of the\narguments about these things and contribute to\nit if you wish to.",
    "start": "714170",
    "end": "720000"
  },
  {
    "text": "But here is my perhaps\nbiased take as a linguist.",
    "start": "720000",
    "end": "725510"
  },
  {
    "text": "Well, you can\ncompare human beings to some of our\nnearest neighbors,",
    "start": "725510",
    "end": "732149"
  },
  {
    "text": "like chimpanzees, bonobos,\nand things like that. And well, one big distinguishing\nthing is we have language,",
    "start": "732150",
    "end": "741620"
  },
  {
    "text": "and they don't. But in most other\nrespects, chimps",
    "start": "741620",
    "end": "748100"
  },
  {
    "text": "are very similar\nto human beings. They can use tools.",
    "start": "748100",
    "end": "754320"
  },
  {
    "text": "They can plan how\nto solve things. They've got really good memory.",
    "start": "754320",
    "end": "759459"
  },
  {
    "text": "Chimps have better short-term\nmemory than human beings do. So that in most respects, it's\nhard to show an intelligence",
    "start": "759460",
    "end": "768040"
  },
  {
    "text": "difference between\nchimps and people, except for the fact\nthat we have language.",
    "start": "768040",
    "end": "773420"
  },
  {
    "text": "But us having language has been\nthis enormous differentiator",
    "start": "773420",
    "end": "778839"
  },
  {
    "text": "that if you look around,\nwhat happened on the planet that there are creatures\nthat are stronger than us,",
    "start": "778840",
    "end": "786620"
  },
  {
    "text": "faster than us, more\nvenomous than us, have every possible advantage. But human beings took\nover the whole place.",
    "start": "786620",
    "end": "794210"
  },
  {
    "text": "And how did that happen? We had language, so\nwe could communicate.",
    "start": "794210",
    "end": "799730"
  },
  {
    "text": "And that communication allowed\nus to have human ascendancy.",
    "start": "799730",
    "end": "805240"
  },
  {
    "text": "But so one big role of\nlanguage is the fact that it allows communication.",
    "start": "805240",
    "end": "811880"
  },
  {
    "text": "But I'd like to\nsuggest it's actually not the only role of language. The language has\nalso allowed humans,",
    "start": "811880",
    "end": "820150"
  },
  {
    "text": "I'd argue, to achieve a\nhigher level of thought. So there are various\nkinds of thoughts",
    "start": "820150",
    "end": "825540"
  },
  {
    "text": "that you can have without\nany language involved. You can think about a scene. You can move some bits of\nfurniture around in your mind,",
    "start": "825540",
    "end": "833860"
  },
  {
    "text": "and there's no\nlanguage, and obviously, emotional responses of\nfeeling scared or excited.",
    "start": "833860",
    "end": "839620"
  },
  {
    "text": "They happen, and there's\nno language involved. But I think most of the time,\nwhen we're doing higher level",
    "start": "839620",
    "end": "846960"
  },
  {
    "text": "cognition, if you're\nthinking to yourself, oh, gee, my friend seemed upset\nabout what I said last night.",
    "start": "846960",
    "end": "854580"
  },
  {
    "text": "I should probably work out\nhow to fix that, or maybe I could blah blah blah blah. I think we think in language\nand plan out things.",
    "start": "854580",
    "end": "862450"
  },
  {
    "text": "And so that it's\ngiven us a scaffolding to do much more detailed\nthought and planning.",
    "start": "862450",
    "end": "869370"
  },
  {
    "text": "Most recently of all,\nof course, human beings invented ways to write.",
    "start": "869370",
    "end": "876170"
  },
  {
    "text": "And so writing is\nreally, really recent. I mean, no one really knows\nhow old human languages are.",
    "start": "876170",
    "end": "883620"
  },
  {
    "text": "Most people think a\nfew 100,000 years, not very long by\nevolutionary timescales.",
    "start": "883620",
    "end": "889590"
  },
  {
    "text": "But writing, we do know. Writing is really,\nreally recent. So writing is about\n5,000 years old.",
    "start": "889590",
    "end": "897529"
  },
  {
    "text": "But writing proved to be, again,\nthis amazing cognitive tool",
    "start": "897530",
    "end": "905330"
  },
  {
    "text": "that just gave humanity\nan enormous leg up. Because suddenly, it's not only\nthat you could share information",
    "start": "905330",
    "end": "911810"
  },
  {
    "text": "and learn from the\npeople that were standing within 50 feet of you. You could then share knowledge\nacross time and space.",
    "start": "911810",
    "end": "920160"
  },
  {
    "text": "So really, having writing\nwas enough to take us from the Bronze Age,\nvery simple metalworking",
    "start": "920160",
    "end": "928580"
  },
  {
    "text": "to the kind of mobile phones\nand all the other technology",
    "start": "928580",
    "end": "933640"
  },
  {
    "text": "that we walk around with today\nin just a very short amount of time. So language is pretty cool.",
    "start": "933640",
    "end": "942280"
  },
  {
    "text": "But one shouldn't only fixate on\nthe knowledge side of language",
    "start": "942280",
    "end": "950200"
  },
  {
    "text": "and how that's made\nhuman beings great. I mean, there's this\nother side of language",
    "start": "950200",
    "end": "956320"
  },
  {
    "text": "where language is this\nvery flexible system, which is used as a social\ntool by human beings",
    "start": "956320",
    "end": "964900"
  },
  {
    "text": "so that we can speak with a\nlot of imprecision and nuance",
    "start": "964900",
    "end": "971500"
  },
  {
    "text": "and emotion in language. And we can get\npeople to understand. We can set up new ways\nof thinking about things",
    "start": "971500",
    "end": "979600"
  },
  {
    "text": "by using words for them. And languages aren't static. Languages change as\nhuman beings use them.",
    "start": "979600",
    "end": "986209"
  },
  {
    "text": "That languages aren't something\nthat were delivered down on tablets by God.",
    "start": "986210",
    "end": "991690"
  },
  {
    "text": "Languages are things\nthat humans constructed, and humans change them with\neach successive generation.",
    "start": "991690",
    "end": "998170"
  },
  {
    "text": "And indeed, most of the\ninnovation in language happens among young people, people that\nare either a few years younger",
    "start": "998170",
    "end": "1005899"
  },
  {
    "text": "than most of you are now,\nin their early teens going",
    "start": "1005900",
    "end": "1011270"
  },
  {
    "text": "into the 20s. That's a big period of\nlinguistic innovation, where people think\nup cool new phrases",
    "start": "1011270",
    "end": "1017210"
  },
  {
    "text": "and ways of saying things. And some of those get\nembedded and extended. And that then becomes\nthe future of language.",
    "start": "1017210",
    "end": "1024859"
  },
  {
    "text": "So Herb Clark used to be a\npsychologist at Stanford.",
    "start": "1024859",
    "end": "1030959"
  },
  {
    "text": "He's now retired. But he had this\nrather nice quote, \"The common misconception is\nthat language use has primarily",
    "start": "1030960",
    "end": "1038000"
  },
  {
    "text": "to do with words\nand what they mean. It doesn't. It has primarily to do with\npeople and what they mean.\"",
    "start": "1038000",
    "end": "1045670"
  },
  {
    "text": "So that's language in\ntwo slides for you. So now, we'll skip\nahead to deep learning.",
    "start": "1045670",
    "end": "1051559"
  },
  {
    "text": "So in the last\ndecade or so, we've been able to make fantastic\nprogress in doing more",
    "start": "1051560",
    "end": "1059139"
  },
  {
    "text": "with computers understanding\nhuman languages in using",
    "start": "1059140",
    "end": "1064270"
  },
  {
    "text": "deep learning. We'll say a bit more about\nthe history later on. But work on trying to do things\nwith human language started",
    "start": "1064270",
    "end": "1071740"
  },
  {
    "text": "in the 1950s. So it had been going\nfor 60 years or so.",
    "start": "1071740",
    "end": "1077240"
  },
  {
    "text": "And there was some stuff. It's not that nobody\ncould do anything. But the ability to understand\nand produce language",
    "start": "1077240",
    "end": "1086470"
  },
  {
    "text": "had always been questionable. Where it's really in the last\ndecade with neural networks",
    "start": "1086470",
    "end": "1093010"
  },
  {
    "text": "that just enormous strides\nof progress have been made that's led into the\nworld that we have today.",
    "start": "1093010",
    "end": "1099200"
  },
  {
    "text": "So one of the first\nbig breakthroughs came in the area of\nusing neural NLP systems",
    "start": "1099200",
    "end": "1107100"
  },
  {
    "text": "for machine translation. And so this started about 2014\nand was already deployed live",
    "start": "1107100",
    "end": "1114060"
  },
  {
    "text": "on services like Google by 2016. It was so good that\nit saw really, really",
    "start": "1114060",
    "end": "1120150"
  },
  {
    "text": "rapid commercial deployment. And I mean, overall, this\nkind of facility with machine",
    "start": "1120150",
    "end": "1128370"
  },
  {
    "text": "translation just\nmeans that you're growing up in such\na different world",
    "start": "1128370",
    "end": "1134340"
  },
  {
    "text": "to people a few\ngenerations back. People a few generations\nback that unless you actually",
    "start": "1134340",
    "end": "1142710"
  },
  {
    "text": "knew different languages\nof different people, you had no chance to\ncommunicate with them.",
    "start": "1142710",
    "end": "1148810"
  },
  {
    "text": "Where now, we're very close to\nhaving something like the Babel fish from the Hitchhiker's\nGuide to the Galaxy",
    "start": "1148810",
    "end": "1156540"
  },
  {
    "text": "for understanding all languages. It's just it's not a Babel fish. It's a cell phone.",
    "start": "1156540",
    "end": "1162690"
  },
  {
    "text": "But can have it out\nbetween two people and have it do\nsimultaneous translation.",
    "start": "1162690",
    "end": "1167880"
  },
  {
    "text": "And it's not perfect. People keep on doing\nresearch on this. But, by and large, it means\nyou can pick anything up",
    "start": "1167880",
    "end": "1175909"
  },
  {
    "text": "from different\nareas of the world. As you can see, this example\nis from a couple of years ago since it's still from\nthe COVID pandemic era.",
    "start": "1175910",
    "end": "1184380"
  },
  {
    "text": "But I can see this\nSwahili from Kenya and say, oh, gee, I\nwonder what that means.",
    "start": "1184380",
    "end": "1191610"
  },
  {
    "text": "Stick it into Google Translate. And I can learn that Malawi lost\ntwo ministers due to a COVID",
    "start": "1191610",
    "end": "1201200"
  },
  {
    "text": "infections, and they died. So we're just in\nthis different era of being able to\nunderstand stuff.",
    "start": "1201200",
    "end": "1208095"
  },
  {
    "text": "And then there are\nlots of other things that we can do with modern NLP. So until a few years ago,\nwe had web search engines.",
    "start": "1208095",
    "end": "1217580"
  },
  {
    "text": "And you put in some text. You could write it as a\nsentence if you wanted to. But it didn't really\nmatter whether you",
    "start": "1217580",
    "end": "1223380"
  },
  {
    "text": "wrote a sentence or not\nbecause what you got was some key words that were\nthen matched against the index.",
    "start": "1223380",
    "end": "1229270"
  },
  {
    "text": "And you were shown some\npages that might have the answers to your questions. But these days, you can\nput an actual question",
    "start": "1229270",
    "end": "1237120"
  },
  {
    "text": "into a modern search engine,\nlike when did Kendrick Lamars first album come out? It can go and find documents\nthat have relevant information.",
    "start": "1237120",
    "end": "1246670"
  },
  {
    "text": "It can read those documents,\nand it can give you an answer. So that it actually can become\nan answer engine, rather than",
    "start": "1246670",
    "end": "1254340"
  },
  {
    "text": "just something that\nfinds documents that might be relevant to\nwhat you're interested in. And the way that that's done\nis with big neural networks.",
    "start": "1254340",
    "end": "1262510"
  },
  {
    "text": "So that you might commonly\nhave, for your query, you've got a retrieval\nneural network",
    "start": "1262510",
    "end": "1269310"
  },
  {
    "text": "which can find passages that\nare similar to the query. They might then be reranked\nby a second neural network.",
    "start": "1269310",
    "end": "1276340"
  },
  {
    "text": "And then there'll be a third\nreading neural network that'll read those passages and\nsynthesize information",
    "start": "1276340",
    "end": "1283429"
  },
  {
    "text": "from them, which it then\nreturns as the answer. That gets us to about 2018.",
    "start": "1283430",
    "end": "1291350"
  },
  {
    "text": "But then, things got\nmore advanced again. So it was really\naround 2019 that people",
    "start": "1291350",
    "end": "1297980"
  },
  {
    "text": "started to see the power\nof large language models. And so back in 2019,\nthose of us in NLP",
    "start": "1297980",
    "end": "1305630"
  },
  {
    "text": "were really excited about GPT-2. It didn't make much of an\nimpact on the nightly news,",
    "start": "1305630",
    "end": "1311850"
  },
  {
    "text": "but it was really\nexciting in NLP land. Because GPT-2 already\nfor the first time,",
    "start": "1311850",
    "end": "1319289"
  },
  {
    "text": "meant here was a large\nlanguage model that could just generate fluent text.",
    "start": "1319290",
    "end": "1324740"
  },
  {
    "text": "That really, until\nthen, NLP systems had done a decent job at\nunderstanding certain facts out",
    "start": "1324740",
    "end": "1332240"
  },
  {
    "text": "of text. But we'd just never been able\nto generate fluent text that was at all good.",
    "start": "1332240",
    "end": "1337870"
  },
  {
    "text": "Where here, what you\ncould do with GPT-2 is you could write something\nlike the start of a story,",
    "start": "1337870",
    "end": "1344540"
  },
  {
    "text": "\"A train carriage containing\ncontrolled nuclear materials was stolen in Cincinnati today. Its whereabouts are unknown.\"",
    "start": "1344540",
    "end": "1351500"
  },
  {
    "text": "And then GPT-2 would just\nwrite a continuation. \"The incident occurred\non the downtown train",
    "start": "1351500",
    "end": "1358210"
  },
  {
    "text": "line, which runs from\nCovington and Ashland stations. In an email to\nOhio news outlets,",
    "start": "1358210",
    "end": "1363475"
  },
  {
    "text": "the US Department of Energy said\nit is working with the Federal Railroad Administration\nto find the thief.\"",
    "start": "1363475",
    "end": "1368590"
  },
  {
    "text": "Dot dot dot. And so the way this is\nworking is it's conditioning on all the past material.",
    "start": "1368590",
    "end": "1374990"
  },
  {
    "text": "And as I show at the very\nbottom line down here, it's then generating\none word at a time",
    "start": "1374990",
    "end": "1381280"
  },
  {
    "text": "as to what word\nit thinks would be likely to come next after that. And so from that simple method\nof so generating words out",
    "start": "1381280",
    "end": "1389680"
  },
  {
    "text": "of one after another, it's\nable to produce excellent text. And the thing to\nnotice is, I mean,",
    "start": "1389680",
    "end": "1396880"
  },
  {
    "text": "this text is not only\nformally correct. The spelling is correct,\nand the sentences",
    "start": "1396880",
    "end": "1403950"
  },
  {
    "text": "are real sentences, not\ndisconnected garbage. But it actually\nunderstands a lot.",
    "start": "1403950",
    "end": "1410889"
  },
  {
    "text": "So the prompt that\nwas written said there were stolen nuclear\nmaterials in Cincinnati.",
    "start": "1410890",
    "end": "1417190"
  },
  {
    "text": "But GPT-2 knows a lot of stuff. It knows that\nCincinnati is in Ohio.",
    "start": "1417190",
    "end": "1423670"
  },
  {
    "text": "It knows that in\nthe United States, it's the Department\nof Energy that regulates nuclear materials.",
    "start": "1423670",
    "end": "1430950"
  },
  {
    "text": "It knows if something is stolen. It's a theft. And that that would\nmake sense that people",
    "start": "1430950",
    "end": "1438450"
  },
  {
    "text": "are getting involved with that. It talks about there\nwas a train carriage.",
    "start": "1438450",
    "end": "1444039"
  },
  {
    "text": "So it's talking about the\ntrain line and where it goes. It really knows a lot and can\nwrite you coherent discourse",
    "start": "1444040",
    "end": "1451280"
  },
  {
    "text": "like a real story. So that's kind of amazing.",
    "start": "1451280",
    "end": "1456410"
  },
  {
    "text": "But things moved on from there. And so now, we're in the\nworld of ChatGPT and GPT-4.",
    "start": "1456410",
    "end": "1462960"
  },
  {
    "text": "And one of the things that\nwe will talk about later is this was a huge user success.",
    "start": "1462960",
    "end": "1470280"
  },
  {
    "text": "Because now, you could ask\nquestions or give it commands, and it would do what you wanted.",
    "start": "1470280",
    "end": "1477450"
  },
  {
    "text": "And that was further amazing. So here, I'm saying, \"Hey,\nplease draft a polite email",
    "start": "1477450",
    "end": "1482900"
  },
  {
    "text": "to my boss Jeremy that I\nwould not be able to come into the office for the next two\ndays because my nine-year-old",
    "start": "1482900",
    "end": "1488600"
  },
  {
    "text": "song--\" that's a\nmisspelling for \"son,\" but the system works\nfine despite it--",
    "start": "1488600",
    "end": "1493760"
  },
  {
    "text": "\"Peter is angry with me that\nI'm not giving him much time,\" da da da. And it writes a nice email.",
    "start": "1493760",
    "end": "1501530"
  },
  {
    "text": "It corrects the spelling\nmistake because it knows people make spelling mistakes. It doesn't talk about songs.",
    "start": "1501530",
    "end": "1507500"
  },
  {
    "text": "And everything works\nout beautifully. You can get it to\ndo other things.",
    "start": "1507500",
    "end": "1513080"
  },
  {
    "text": "So you can ask it, what is\nunusual about this image? So in thinking\nabout meaning, one",
    "start": "1513080",
    "end": "1519940"
  },
  {
    "text": "of the things that's interesting\nwith these recent models is that they're multimodal\nand can operate across modes.",
    "start": "1519940",
    "end": "1528350"
  },
  {
    "text": "And so a favorite term\nthat we coined at Stanford is the term foundation\nmodels, which",
    "start": "1528350",
    "end": "1535120"
  },
  {
    "text": "we use as a generalization\nof large language models to have the same kind\nof technology used",
    "start": "1535120",
    "end": "1541390"
  },
  {
    "text": "across different modalities-- images, sound, various kinds\nof bioinformatic things,",
    "start": "1541390",
    "end": "1548260"
  },
  {
    "text": "DNA, RNA, things like\nthat, seismic waves, any kind of signal building\nthese same kind of large models.",
    "start": "1548260",
    "end": "1557200"
  },
  {
    "text": "Another place that\nyou can see that is going from text to images.",
    "start": "1557200",
    "end": "1565149"
  },
  {
    "text": "So if I ask for a picture of a\ntrain going over the Golden Gate Bridge, this is now Dall-E 2.",
    "start": "1565150",
    "end": "1574769"
  },
  {
    "text": "It gives me a picture of a\ntrain going over the Golden Gate Bridge. This is a perfect\ntime to welcome",
    "start": "1574770",
    "end": "1580500"
  },
  {
    "text": "anyone who's watching\nthis on Stanford Online. If you're on Stanford Online\nand are not in the Bay Area,",
    "start": "1580500",
    "end": "1587890"
  },
  {
    "text": "the important thing\nto know is no trains go over the Golden Gate Bridge.",
    "start": "1587890",
    "end": "1593970"
  },
  {
    "text": "But you might not be completely\nhappy with this picture because it shows the Golden\nGate Bridge and a train",
    "start": "1593970",
    "end": "1600600"
  },
  {
    "text": "going over it, but it\ndoesn't show the bay. So maybe I'd like to get with\nthe bay in the background.",
    "start": "1600600",
    "end": "1607270"
  },
  {
    "text": "And if I ask for\nthat, well, look, now I've got a train going\nover the Golden Gate Bridge with the bay in the background.",
    "start": "1607270",
    "end": "1614590"
  },
  {
    "text": "But you still might not be-- this might not be\nexactly what you want. Maybe you'd prefer something\nthat's a pencil drawing.",
    "start": "1614590",
    "end": "1623020"
  },
  {
    "text": "So I can say a train going\nover the Golden Gate Bridge, detailed pencil drawing. And I can get a pencil drawing.",
    "start": "1623020",
    "end": "1629900"
  },
  {
    "text": "Or maybe it's unrealistic that\nthe Golden Gate Bridge only has trains going over it now.",
    "start": "1629900",
    "end": "1636020"
  },
  {
    "text": "So maybe it'd be good to\nhave some cars as well. So I could ask for\na train and cars,",
    "start": "1636020",
    "end": "1641340"
  },
  {
    "text": "and we can get a train\nand cars going over it. Now, I actually made\nthese ones all by myself,",
    "start": "1641340",
    "end": "1648030"
  },
  {
    "text": "so you should be impressed\nwith my generative AI artwork. But these examples are actually\na bit old now because they",
    "start": "1648030",
    "end": "1654470"
  },
  {
    "text": "were done with Dall-E 2. And if you keep up with these\nthings, that's a few years ago. There's now Dall-E 3 and so on.",
    "start": "1654470",
    "end": "1659880"
  },
  {
    "text": "So we can now get much\nfancier things again. \"An illustration\nfrom a graphic novel.",
    "start": "1659880",
    "end": "1665310"
  },
  {
    "text": "A bustling city street under\nthe shine of a full moon. The sidewalks bustling\nwith pedestrians",
    "start": "1665310",
    "end": "1670580"
  },
  {
    "text": "enjoying the nightlife. At the corner stall, a young\nwoman with fiery red hair, dressed in a signature\nvelvet cloak,",
    "start": "1670580",
    "end": "1676950"
  },
  {
    "text": "is haggling with the\ngrumpy old vendor. The grumpy vendor, a\ntall, sophisticated man, is wearing a sharp suit,\nsports a noteworthy mustache,",
    "start": "1676950",
    "end": "1684860"
  },
  {
    "text": "is animatedly conversing on\nhis steampunk telephone.\" And pretty much, we're\ngetting all of that.",
    "start": "1684860",
    "end": "1692919"
  },
  {
    "text": "So let's now get on to starting\nto think more about meaning.",
    "start": "1692920",
    "end": "1699230"
  },
  {
    "text": "So what can we do for meaning?",
    "start": "1699230",
    "end": "1704900"
  },
  {
    "text": "So if you think of\nwords and their meaning, if you look up a dictionary and\nsay, what does meaning mean?",
    "start": "1704900",
    "end": "1713470"
  },
  {
    "text": "Meaning is defined\nas the idea that is represented by a\nword or phrase, the idea",
    "start": "1713470",
    "end": "1718810"
  },
  {
    "text": "that a person wants to express\nby using words, the idea that is expressed.",
    "start": "1718810",
    "end": "1724300"
  },
  {
    "text": "And in linguistics, if you\ngo and do a semantics class",
    "start": "1724300",
    "end": "1729790"
  },
  {
    "text": "or something, the commonest\nway of thinking of meaning is somewhat like what's\npresented up above there.",
    "start": "1729790",
    "end": "1736520"
  },
  {
    "text": "That meaning is thought of as a\npairing between what's sometimes called signifier and signified.",
    "start": "1736520",
    "end": "1743330"
  },
  {
    "text": "But it's perhaps easy to think\nof as a symbol, a word, and then an idea or thing.",
    "start": "1743330",
    "end": "1750140"
  },
  {
    "text": "And so this notion is referred\nto as denotational semantics. So the idea or thing is the\ndenotation of the symbol.",
    "start": "1750140",
    "end": "1758059"
  },
  {
    "text": "And so this same idea of\ndenotational semantics has also been used for\nprogramming languages.",
    "start": "1758060",
    "end": "1763159"
  },
  {
    "text": "Because in\nprogramming languages, you have symbols like\nwhile and if and variables.",
    "start": "1763160",
    "end": "1769700"
  },
  {
    "text": "And they have a meaning, and\nthat could be their denotation. So we sort of would say\nthat the meaning of tree",
    "start": "1769700",
    "end": "1777460"
  },
  {
    "text": "is all the trees you can\nfind out around the world. That's sort of a OK\nnotion of meaning.",
    "start": "1777460",
    "end": "1785410"
  },
  {
    "text": "It's a popular one. It's never been very obvious. Or at least\ntraditionally, it wasn't",
    "start": "1785410",
    "end": "1791080"
  },
  {
    "text": "very obvious as to what\nwe could do with that to get it into computers. So if you looked in\nthe pre-neural world",
    "start": "1791080",
    "end": "1798700"
  },
  {
    "text": "when people tried to look at\nmeanings inside computers, they had to do something\nmuch more primitive",
    "start": "1798700",
    "end": "1806540"
  },
  {
    "text": "of looking at words and\ntheir relationships. So a very common\ntraditional solution",
    "start": "1806540",
    "end": "1812390"
  },
  {
    "text": "was to make use of WordNet. And WordNet was\na fancy thesaurus that showed word relations.",
    "start": "1812390",
    "end": "1819090"
  },
  {
    "text": "So it would tell\nyou about synonyms and is the kind of things. So a panda is a\nkind of carnivore,",
    "start": "1819090",
    "end": "1826320"
  },
  {
    "text": "which is a placental which is\na mammal and things like that. Good has various meanings.",
    "start": "1826320",
    "end": "1831690"
  },
  {
    "text": "It's a trade good or\nthe sense of goodness. And you could explore with that. But systems like\nWordNet were never",
    "start": "1831690",
    "end": "1839690"
  },
  {
    "text": "very good for\ncomputational meaning. They missed a lot of nuance.",
    "start": "1839690",
    "end": "1844980"
  },
  {
    "text": "WordNet would tell\nyou that proficient is a synonym for good. But if you think\nabout all the things",
    "start": "1844980",
    "end": "1851360"
  },
  {
    "text": "that you would say were good-- that was a good shot. Would you say that\nwas a proficient shot?",
    "start": "1851360",
    "end": "1857250"
  },
  {
    "text": "It sounds kind of weird to me. There's a lot of color and\nnuance on how words are used.",
    "start": "1857250",
    "end": "1863890"
  },
  {
    "text": "WordNet is very incomplete. It's missing\nanything that's kind of cooler, more modern slang.",
    "start": "1863890",
    "end": "1869720"
  },
  {
    "text": "This maybe isn't very\nmodern slang now. But you won't find more\nmodern slang either in it. It's sort of very\nhuman-made, et cetera.",
    "start": "1869720",
    "end": "1877240"
  },
  {
    "text": "It's got a lot of issues. So this led into\nthe idea of, can we represent meaning differently?",
    "start": "1877240",
    "end": "1884030"
  },
  {
    "text": "And this leads us\ninto word vectors. So when we have words--",
    "start": "1884030",
    "end": "1890950"
  },
  {
    "text": "wicked, badass, nifty, wizard-- what do they turn into\nwhen we have computers?",
    "start": "1890950",
    "end": "1899230"
  },
  {
    "text": "Well, effectively, words\nare these discrete symbols",
    "start": "1899230",
    "end": "1906100"
  },
  {
    "text": "that they're just some\nkind of atom or symbol. And if we then turn those into\nsomething that's closer to math,",
    "start": "1906100",
    "end": "1915360"
  },
  {
    "text": "how symbols are\nnormally represented is you have a\nvocabulary, and your word",
    "start": "1915360",
    "end": "1922620"
  },
  {
    "text": "is some item in that vocabulary. So motel is that word\nin the vocabulary.",
    "start": "1922620",
    "end": "1928270"
  },
  {
    "text": "And hotel is this word\nin the vocabulary. And commonly, this is what\ncomputational systems do.",
    "start": "1928270",
    "end": "1934360"
  },
  {
    "text": "You take all your strings. And you index them to numbers. And that's the sort of position\nin a vector that they belong in.",
    "start": "1934360",
    "end": "1943019"
  },
  {
    "text": "And well, we have\nhuge numbers of words. So we might have\na huge vocabulary.",
    "start": "1943020",
    "end": "1948100"
  },
  {
    "text": "So we'll have very\nbig and long vectors. And so these get\nreferred to as one",
    "start": "1948100",
    "end": "1953400"
  },
  {
    "text": "hot vectors for representing\nthe meaning of words.",
    "start": "1953400",
    "end": "1958470"
  },
  {
    "text": "But representing words\nby one hot vectors turns out to not be a very good\nway of computing with them.",
    "start": "1958470",
    "end": "1967149"
  },
  {
    "text": "It was used for decades. But it turns out to be\nkind of problematic. And part of why\nit's problematic is",
    "start": "1967150",
    "end": "1974300"
  },
  {
    "text": "it doesn't have any\nnatural, inherent sense of the meanings of words.",
    "start": "1974300",
    "end": "1979710"
  },
  {
    "text": "You just have different words. You have hotel and motel\nand house and chair.",
    "start": "1979710",
    "end": "1984840"
  },
  {
    "text": "And so if you think about\nit in terms of these vector representations, that if\nyou have motel and hotel,",
    "start": "1984840",
    "end": "1992460"
  },
  {
    "text": "there's no indication\nthat they're similar. They're just two\ndifferent symbols",
    "start": "1992460",
    "end": "1998210"
  },
  {
    "text": "which have ones in different\npositions in the vector. Or formally, in\nmath terms, if you",
    "start": "1998210",
    "end": "2004510"
  },
  {
    "text": "think about taking the dot\nproduct of these two vectors, it's 0. The two vectors are orthogonal.",
    "start": "2004510",
    "end": "2011090"
  },
  {
    "text": "They have nothing to\ndo with each other. Now, there are things\nthat you can do with that.",
    "start": "2011090",
    "end": "2016430"
  },
  {
    "text": "You can start saying,\noh, let me start building up some other\nresource of word similarity.",
    "start": "2016430",
    "end": "2022490"
  },
  {
    "text": "And I'll consult that\nresource of word similarity. And it'll tell me\nthat motels and hotels",
    "start": "2022490",
    "end": "2027760"
  },
  {
    "text": "are similar to each other. And people did things like that. And in web search, it was\nreferred to as query expansion",
    "start": "2027760",
    "end": "2034080"
  },
  {
    "text": "techniques. But still, the point\nis that there's no natural notion of\nsimilarity in one hot vectors.",
    "start": "2034080",
    "end": "2043860"
  },
  {
    "text": "And so the idea\nwas that maybe we could do better\nthan that, that we",
    "start": "2043860",
    "end": "2050340"
  },
  {
    "text": "could learn to include\nsimilarity in the vectors themselves. And so that leads into\nthe idea of word vectors.",
    "start": "2050340",
    "end": "2058770"
  },
  {
    "text": "But it also leads\ninto a different way of thinking about semantics.",
    "start": "2058770",
    "end": "2064050"
  },
  {
    "text": "I just realized I\nforgot to say one thing. Back two slides. These kind of\nrepresentations are",
    "start": "2064050",
    "end": "2071460"
  },
  {
    "text": "referred to as localist\nrepresentations, meaning that there's one\npoint in which something",
    "start": "2071460",
    "end": "2078030"
  },
  {
    "text": "is represented. So that you've got here is\nthe representation of motel.",
    "start": "2078030",
    "end": "2085388"
  },
  {
    "text": "And here is the\nrepresentation of hotel. It's in one place in the vector\nthat each word is represented.",
    "start": "2085389",
    "end": "2091649"
  },
  {
    "text": "And they'll be different\nto what we do next. So there's an alternative\nidea of semantics, which",
    "start": "2091650",
    "end": "2099049"
  },
  {
    "text": "goes back quite a long way. People commonly quote\nthis quote of J.R. Firth,",
    "start": "2099050",
    "end": "2105390"
  },
  {
    "text": "who was a British\nlinguist who said in 1957, \"You shall know a word\nby the company it keeps.\"",
    "start": "2105390",
    "end": "2110870"
  },
  {
    "text": "But also goes back\nto philosophical work by Wittgenstein and others,\nthat what you should do",
    "start": "2110870",
    "end": "2116990"
  },
  {
    "text": "is represent a word's meaning by\nthe context in which it appears.",
    "start": "2116990",
    "end": "2123290"
  },
  {
    "text": "So the words that\nappear around a word give information\nabout its meaning.",
    "start": "2123290",
    "end": "2129900"
  },
  {
    "text": "And so that's the\nidea of what's called distributional\nsemantics in contrast to denotational semantics.",
    "start": "2129900",
    "end": "2136380"
  },
  {
    "text": "So if I want to know about\nthe word banking, I say, give me some sentences\nthat use the word banking.",
    "start": "2136380",
    "end": "2143220"
  },
  {
    "text": "Here are some sentences\nusing the word banking. Government debt problems\nturning into banking crises",
    "start": "2143220",
    "end": "2149480"
  },
  {
    "text": "as happened in 2009,\net cetera, et cetera. And knowing about that context\nwords that occur around banking,",
    "start": "2149480",
    "end": "2157940"
  },
  {
    "text": "those will become the\nmeaning of banking. And so we're going to use\nthose statistics about words",
    "start": "2157940",
    "end": "2167920"
  },
  {
    "text": "and what other words\nappear around them in order to learn a new kind\nof representation of a word.",
    "start": "2167920",
    "end": "2177350"
  },
  {
    "text": "So our new\nrepresentation of words is we're going to\nrepresent them now",
    "start": "2177350",
    "end": "2183069"
  },
  {
    "text": "as a dense, a shorter,\ndense vector that gives the meaning of the words.",
    "start": "2183070",
    "end": "2189410"
  },
  {
    "text": "Now, my vectors are\nvery short here. These are only eight\ndimensional if I counted right",
    "start": "2189410",
    "end": "2194650"
  },
  {
    "text": "so I could fit them on my slide. They're not that\nshort in practice. They might be 200, 2,000,\nbut reasonably short.",
    "start": "2194650",
    "end": "2203270"
  },
  {
    "text": "They're not going to be like\nthe half a million of the half a million different\nwords in our vocabulary.",
    "start": "2203270",
    "end": "2208930"
  },
  {
    "text": "And the idea is, if words have\nstuff to do with each other, they'll have sort of\nsimilar vectors, which",
    "start": "2208930",
    "end": "2216600"
  },
  {
    "text": "corresponds to their\ndot product being large. So for banking and monetary, in\nmy example here, both of them",
    "start": "2216600",
    "end": "2223890"
  },
  {
    "text": "are positive in the\nfirst dimension, positive in the\nsecond dimension, negative on the third.",
    "start": "2223890",
    "end": "2229390"
  },
  {
    "text": "On the fourth, they've\ngot opposite signs. So if we want to work\nout the dot product, we're taking the product\nof the corresponding terms",
    "start": "2229390",
    "end": "2237090"
  },
  {
    "text": "and it'll get\nbigger to the extent that both of the corresponding\nones have the same size",
    "start": "2237090",
    "end": "2242099"
  },
  {
    "text": "and bigger if they\nhave large magnitude. So these are what we call\nword vectors, which are also",
    "start": "2242100",
    "end": "2251220"
  },
  {
    "text": "known as embeddings or neural\nword representations or phrases like that.",
    "start": "2251220",
    "end": "2256840"
  },
  {
    "text": "And so the first\nthing we want to do is learn good word vectors\nfor different words.",
    "start": "2256840",
    "end": "2264300"
  },
  {
    "text": "And our word vectors\nwill be good word vectors if they give us a good sense\nof the meanings of words.",
    "start": "2264300",
    "end": "2273960"
  },
  {
    "text": "They know which words are\nsimilar to other words in meaning. We refer to them as\nembeddings because we",
    "start": "2273960",
    "end": "2282560"
  },
  {
    "text": "can think of this as a vector\nin a high dimensional space, and so that we're embedding\neach word as a position",
    "start": "2282560",
    "end": "2290420"
  },
  {
    "text": "in that high dimensional space. And the dimensionality\nof the space",
    "start": "2290420",
    "end": "2295460"
  },
  {
    "text": "will be the length\nof the vector. So it might be something\nlike a 300 dimensional space.",
    "start": "2295460",
    "end": "2301010"
  },
  {
    "text": "Now, that kind of\ngets problematic, because human beings can't\nlook at 300 dimensional spaces",
    "start": "2301010",
    "end": "2308480"
  },
  {
    "text": "and aren't very good at\nunderstanding or visualizing what goes on in them.",
    "start": "2308480",
    "end": "2313500"
  },
  {
    "text": "So the only thing\nthat I can show you is two dimensional spaces.",
    "start": "2313500",
    "end": "2319680"
  },
  {
    "text": "But a thing that it's good\nto have somewhat in your head",
    "start": "2319680",
    "end": "2326859"
  },
  {
    "text": "is that really high\ndimensional spaces behave extremely differently\nto two dimensional spaces.",
    "start": "2326860",
    "end": "2334750"
  },
  {
    "text": "In high-dimensional\nspaces, things can-- in a two-dimensional space,\nyou're only near to something",
    "start": "2334750",
    "end": "2342100"
  },
  {
    "text": "else if you've got similar\nx and y-coordinates. In a high-dimensional\nspace, things can be very near to all sorts of\nthings on different dimensions",
    "start": "2342100",
    "end": "2350829"
  },
  {
    "text": "in the space. And so we can capture different\nsenses of words and ways that words are\nsimilar to each other.",
    "start": "2350830",
    "end": "2358510"
  },
  {
    "text": "But here's the picture\nwe end up with. So what we're going\nto do is learn a way",
    "start": "2358510",
    "end": "2363700"
  },
  {
    "text": "to represent all words as\nvectors based on the other words",
    "start": "2363700",
    "end": "2370480"
  },
  {
    "text": "that they put here\nwithin context. And we can embed them\ninto this vector space. And of course, you can't\nread anything there.",
    "start": "2370480",
    "end": "2377359"
  },
  {
    "text": "But we can zoom into\nthis space further. And if we zoom into this space\nand just show a bit of it,",
    "start": "2377360",
    "end": "2383920"
  },
  {
    "text": "well, here's a part\nof the space where it's showing country words\nand some other location words.",
    "start": "2383920",
    "end": "2390730"
  },
  {
    "text": "So we've got sort of\ncountries up the top there. We've got some\nnationality terms--",
    "start": "2390730",
    "end": "2396030"
  },
  {
    "text": "British, Australian, American,\nEuropean further down. Or we can go to another\npiece of the space.",
    "start": "2396030",
    "end": "2402700"
  },
  {
    "text": "And here's a bit of the\nspace where we have verbs. And not only have we got\nverbs, but there's actually",
    "start": "2402700",
    "end": "2408779"
  },
  {
    "text": "quite a lot of\nfine structure here of what's similar that\nrepresents things about verbs.",
    "start": "2408780",
    "end": "2414850"
  },
  {
    "text": "So you've got sort of verbs\nof communication statements",
    "start": "2414850",
    "end": "2421140"
  },
  {
    "text": "saying, thinking, expecting,\ngrouping together, come and go grouped together. Down the bottom, you've got\nforms of the verb \"have.\"",
    "start": "2421140",
    "end": "2429100"
  },
  {
    "text": "Then you've got forms\nof the verb \"to be.\" Above them, you got\nthe common \"remain,\"",
    "start": "2429100",
    "end": "2434110"
  },
  {
    "text": "which are actually sort of\nsimilar to the verb \"to be\" because they take these\ncomplements of state.",
    "start": "2434110",
    "end": "2441480"
  },
  {
    "text": "So just as same as you\ncan say, \"I am angry.\" You can say, \"He\nremained angry.\"",
    "start": "2441480",
    "end": "2449119"
  },
  {
    "text": "Or, \"He became angry.\" So those verbs are more\nso than most verbs,",
    "start": "2449120",
    "end": "2454190"
  },
  {
    "text": "similar to the verb \"to be.\" So we get these\ninteresting semantic spaces",
    "start": "2454190",
    "end": "2459349"
  },
  {
    "text": "where things that\nhave similar meaning are close by to each other. And so the question is, how\ndo we get to those things?",
    "start": "2459350",
    "end": "2468390"
  },
  {
    "text": "And how do we get to\nthose things is then-- there are various\nways of doing it.",
    "start": "2468390",
    "end": "2474990"
  },
  {
    "text": "But the one I want\nto get through today is showing you about word2vec.",
    "start": "2474990",
    "end": "2480950"
  },
  {
    "text": "I'll pause for 30\nseconds for breath. Anyone have a question or\nanything they want to know?",
    "start": "2480950",
    "end": "2486630"
  },
  {
    "text": "Yes. So there's [INAUDIBLE]\nideas [INAUDIBLE] have the contact information.",
    "start": "2486630",
    "end": "2493010"
  },
  {
    "text": "But it doesn't seem\nto solve the problem where the similar meanings\nmight depend on context.",
    "start": "2493010",
    "end": "2501519"
  },
  {
    "text": "Let's take your example about\n\"proficient\" versus \"good\" [INAUDIBLE]. So those two words have\ntheir own embeddings,",
    "start": "2501520",
    "end": "2509110"
  },
  {
    "text": "but we try to understand the\nsimilarity with some [INAUDIBLE] vectors.",
    "start": "2509110",
    "end": "2514280"
  },
  {
    "text": "But if you're looking\nat context, right, because if you have\ndifferent contexts, then those two words\nmay not be similar.",
    "start": "2514280",
    "end": "2520310"
  },
  {
    "text": "And this word embedding alone\ndoes not seem to be a problem. Yes, correct.",
    "start": "2520310",
    "end": "2525609"
  },
  {
    "text": "So that's a good thought. You can keep it for a\nfew weeks to some extent. Yeah, so for the first\nthing we're going to do,",
    "start": "2525610",
    "end": "2531860"
  },
  {
    "text": "we're just going to learn\none word vector for a string. So we can have a word,\nlet's say, it's star.",
    "start": "2531860",
    "end": "2540230"
  },
  {
    "text": "And we're going to learn\none word vector for it. So that absolutely doesn't\ncapture the meaning",
    "start": "2540230",
    "end": "2546490"
  },
  {
    "text": "of a word in context. So it won't be\nsaying whether it's meaning a Hollywood star\nor an astronomical star",
    "start": "2546490",
    "end": "2554520"
  },
  {
    "text": "or something like that. And so later on, we're going\nto get on to contextual meaning representation.",
    "start": "2554520",
    "end": "2560109"
  },
  {
    "text": "So wait for that. But the thing I would like to-- going along with what I said\nabout high dimensional spaces",
    "start": "2560110",
    "end": "2566970"
  },
  {
    "text": "being weird. But the cool thing that\nwe will already find is our representation\nfor star will",
    "start": "2566970",
    "end": "2574860"
  },
  {
    "text": "be very close to\nthe representations for astronomical\nwords like nebula",
    "start": "2574860",
    "end": "2583050"
  },
  {
    "text": "and what every other\nastronomical words you know. And simultaneously,\nit'll be very",
    "start": "2583050",
    "end": "2590520"
  },
  {
    "text": "close to words that mean\nsomething like a Hollywood star. ",
    "start": "2590520",
    "end": "2597840"
  },
  {
    "text": "Help me out. Know any words that\nmean something similar? Celebrity. Celebrity, that's a good one.",
    "start": "2597840",
    "end": "2603250"
  },
  {
    "text": "OK, yeah. How are you reducing the\nembeddings to a lower dimensional space to visualize?",
    "start": "2603250",
    "end": "2611180"
  },
  {
    "text": "So that pictures\nI was showing, you use a particular\nmethod called t-SNE,",
    "start": "2611180",
    "end": "2617300"
  },
  {
    "text": "which is a nonlinear\ndimensionality reduction that tends to work better for\nhigh dimensional neural",
    "start": "2617300",
    "end": "2623390"
  },
  {
    "text": "representations than PCA,\nwhich you might know. But I'm not going\nto go into that now.",
    "start": "2623390",
    "end": "2629300"
  },
  {
    "text": "Yes? How do you know\nhow many dimensions is optimal to produce like\na dense enough or, I guess,",
    "start": "2629300",
    "end": "2636710"
  },
  {
    "text": "space but not too sparse? I mean, that's something\nthat people have worked on.",
    "start": "2636710",
    "end": "2643550"
  },
  {
    "text": "It depends on how much-- it depends on how\nmuch data you've got to make your\nrepresentations over.",
    "start": "2643550",
    "end": "2650040"
  },
  {
    "text": "So normally, it's worked\nout either empirically",
    "start": "2650040",
    "end": "2655100"
  },
  {
    "text": "for what works best\nor practically based on how big vectors\nyou want to work on.",
    "start": "2655100",
    "end": "2660510"
  },
  {
    "text": "I mean, to give you\nsome idea, things start to work well when you\nget to 100 dimensional space.",
    "start": "2660510",
    "end": "2668400"
  },
  {
    "text": "For a long time, people\nused 300 dimensions because that seemed\nto work pretty well. But as people have started\nbuilding huger and huger models",
    "start": "2668400",
    "end": "2677020"
  },
  {
    "text": "with way, way more data, it's\nnow become increasingly common to use numbers like 1,000 or\neven 2,000 dimensional vectors.",
    "start": "2677020",
    "end": "2684740"
  },
  {
    "text": "Yeah? [INAUDIBLE] OK, [INAUDIBLE]. So you mentioned that there are\nhidden structures in small areas",
    "start": "2684740",
    "end": "2693640"
  },
  {
    "text": "as well as large areas\nof the embedding. And as your\ndimensionality increases, different structures\nwill come up.",
    "start": "2693640",
    "end": "2700640"
  },
  {
    "text": "But generally, we\nseem to use distance as the single metric for\ncloseness, which doesn't seem to me that we'll get it.",
    "start": "2700640",
    "end": "2706720"
  },
  {
    "text": "Like, distance between this\nand that in space three will be the same. So how will that--",
    "start": "2706720",
    "end": "2711880"
  },
  {
    "text": "We don't only use distance. We also use directions\nin the space as having semantic meanings.",
    "start": "2711880",
    "end": "2718069"
  },
  {
    "text": "And I'll show an\nexample of that soon. Yeah? So I was wondering for the\nentries within the word vector,",
    "start": "2718070",
    "end": "2724520"
  },
  {
    "text": "they seem to be between\nnegative 1 and 1. Is there a reason for\nthat, or do we have bounds",
    "start": "2724520",
    "end": "2729540"
  },
  {
    "text": "that we can constrain for them? So good question. I mean, they don't have to be.",
    "start": "2729540",
    "end": "2738299"
  },
  {
    "text": "And the way we're going to\nlearn them, they're not bounded. But you can bound things.",
    "start": "2738300",
    "end": "2745300"
  },
  {
    "text": "Sometimes, people length\nnormalize so that the vectors are of length 1.",
    "start": "2745300",
    "end": "2750580"
  },
  {
    "text": "But at any rate,\nnormally in this work, we use some method\ncalled regularization",
    "start": "2750580",
    "end": "2756630"
  },
  {
    "text": "that tries to keep coefficients\nsmall so that they're generally not getting huge.",
    "start": "2756630",
    "end": "2762180"
  },
  {
    "text": "Yeah? Given a specific word,\nfor example like the bank we used as before in\nthe previous slides,",
    "start": "2762180",
    "end": "2769410"
  },
  {
    "text": "so for the word\nrepresentation, is there a single embedding\nfor each word,",
    "start": "2769410",
    "end": "2776560"
  },
  {
    "text": "or do we have multiple\nembeddings for each word? So what we're doing at\nthe moment, each word,",
    "start": "2776560",
    "end": "2783450"
  },
  {
    "text": "each string of letters does\nhas a single embedding. And what you can think of that\nembedding as an average over all",
    "start": "2783450",
    "end": "2794810"
  },
  {
    "text": "its sensors. So for example,\nlike bank, it can mean like the\nfinancial institution,",
    "start": "2794810",
    "end": "2801990"
  },
  {
    "text": "or it can also mean\nlike the river bank. Yeah, and then what I said\nbefore about star applies.",
    "start": "2801990",
    "end": "2807540"
  },
  {
    "text": "The interesting\nthing is you'll find that we are able to come\nup with a representation",
    "start": "2807540",
    "end": "2812900"
  },
  {
    "text": "where our learned\nrepresentation, because it's kind of an average\nof those, we'll end up similar to words\nthat are semantically",
    "start": "2812900",
    "end": "2821150"
  },
  {
    "text": "evoked by both senses. I think I probably about\ngo on at this point.",
    "start": "2821150",
    "end": "2827780"
  },
  {
    "text": "Word2vec-- so word2vec\nwas this method of learning word vectors\nthat was thought up",
    "start": "2827780",
    "end": "2835820"
  },
  {
    "text": "by Tomas Mikolov and\ncolleagues at Google in 2013.",
    "start": "2835820",
    "end": "2841590"
  },
  {
    "text": "It wasn't the first method. There are other people\nthat did methods of learning word vectors\nthat go back to about",
    "start": "2841590",
    "end": "2848890"
  },
  {
    "text": "the turn of the millennium. It wasn't the last. There were ones that\ncome after it as well.",
    "start": "2848890",
    "end": "2854720"
  },
  {
    "text": "But it was a particularly simple\none and a particularly fast",
    "start": "2854720",
    "end": "2860020"
  },
  {
    "text": "running one. And so it really caught\npeople's attention. So the idea of it\nis that we start off",
    "start": "2860020",
    "end": "2868690"
  },
  {
    "text": "with a large amount of text. So that can just be thought\nof as a long list of words.",
    "start": "2868690",
    "end": "2875780"
  },
  {
    "text": "And in NLP, we refer\nto that as a corpus. Corpus is just Latin for body.",
    "start": "2875780",
    "end": "2882490"
  },
  {
    "text": "So it's exactly the\nsame as if you have a dead person on the floor. That's a corpus.",
    "start": "2882490",
    "end": "2888020"
  },
  {
    "text": "No. Yeah, so it's just a body. But we mean a body of\ntext, not a live person--",
    "start": "2888020",
    "end": "2895510"
  },
  {
    "text": "or sorry, a dead person. If you want to know\nmore about Latin,",
    "start": "2895510",
    "end": "2901230"
  },
  {
    "text": "since there isn't very good\nclassical education these days. Corpus, despite the US ending,\nis a fourth declension neuter",
    "start": "2901230",
    "end": "2911160"
  },
  {
    "text": "noun, and that means the\nplural of corpus is not corpi.",
    "start": "2911160",
    "end": "2918490"
  },
  {
    "text": "The plural of corpus is corpora. So I'm sure some time\nlater in this class,",
    "start": "2918490",
    "end": "2926160"
  },
  {
    "text": "I will read a\nproject or assignment where it refers to\ncorpi, and I will",
    "start": "2926160",
    "end": "2931290"
  },
  {
    "text": "know that person was not paying\nattention in the first lecture. Or else, they should\nhave said corpora.",
    "start": "2931290",
    "end": "2939330"
  },
  {
    "text": "C-O-R-P-O-R-A is the\ncorrect form for that. I should move on.",
    "start": "2939330",
    "end": "2944619"
  },
  {
    "text": "So we have our text. Then we know that we're\ngoing to represent each word.",
    "start": "2944620",
    "end": "2953080"
  },
  {
    "text": "So this is each word type. So star, or bank, et\ncetera, so for wherever",
    "start": "2953080",
    "end": "2959540"
  },
  {
    "text": "it occurs by a single vector. And so what we're going\nto do in this algorithm",
    "start": "2959540",
    "end": "2965089"
  },
  {
    "text": "is we're going to go through\neach position in the text. And so at each position in the\ntext, which is a list of words,",
    "start": "2965090",
    "end": "2972120"
  },
  {
    "text": "we're going to have a center\nword and words outside it. And then what we're\ngoing to do is",
    "start": "2972120",
    "end": "2978950"
  },
  {
    "text": "use the similarity\nof the word vectors for C and the outside words\nto calculate the probability",
    "start": "2978950",
    "end": "2986539"
  },
  {
    "text": "that they should\nhave occurred or not. And then we just keep fiddling,\nand we learn word vectors.",
    "start": "2986540",
    "end": "2993240"
  },
  {
    "text": "Now, at first sight-- I'll show you this\nmore concretely. Maybe I'll just show it\nmore concretely first.",
    "start": "2993240",
    "end": "2999150"
  },
  {
    "text": "So here's the idea. We're going to have a\nvector for each word type.",
    "start": "2999150",
    "end": "3005330"
  },
  {
    "text": "So a word type means the word\nproblems wherever it occurs, which is differentiated\nfrom a word",
    "start": "3005330",
    "end": "3011560"
  },
  {
    "text": "token, which is this instance\nof the word problems. So we're going to have a\nvector for each word type.",
    "start": "3011560",
    "end": "3018970"
  },
  {
    "text": "And so I'm going to want to\nknow, look, in this text,",
    "start": "3018970",
    "end": "3024119"
  },
  {
    "text": "the word \"turning\" occurred\nbefore the word \"into.\" How likely should that\nhave been to happen?",
    "start": "3024120",
    "end": "3031750"
  },
  {
    "text": "And what I'm going to do\nis calculate a probability of the word \"turning\" occurring\nclose to the word \"into.\"",
    "start": "3031750",
    "end": "3040390"
  },
  {
    "text": "And I'm going to do that for\neach word in a narrow context. In the example here,\nI'm saying, I'm",
    "start": "3040390",
    "end": "3046350"
  },
  {
    "text": "using two words to the left\nand two words to the right. And what I want to do is make\nthose probability estimates",
    "start": "3046350",
    "end": "3054030"
  },
  {
    "text": "as good as possible. So in particular, I want the\nprobability of co-occurrence to be high for\nwords that actually",
    "start": "3054030",
    "end": "3060660"
  },
  {
    "text": "do occur within the nearby\ncontext of each other. And so then the question\nis, how am I going to--",
    "start": "3060660",
    "end": "3068830"
  },
  {
    "text": "and once I've done\nit for that word, I'm going to go along and\ndo exactly the same thing",
    "start": "3068830",
    "end": "3073860"
  },
  {
    "text": "for the next word. And so I can continue\nthrough the text in that way.",
    "start": "3073860",
    "end": "3080099"
  },
  {
    "text": "And so what we want to\ndo is come up with vector",
    "start": "3080100",
    "end": "3086570"
  },
  {
    "text": "representations of words that\nwill let us predict these probabilities, quote\nunquote, \"well.\"",
    "start": "3086570",
    "end": "3093960"
  },
  {
    "text": "Now, there's a huge limit to how\nwell we can do it because we've got a simple model.",
    "start": "3093960",
    "end": "3099510"
  },
  {
    "text": "Obviously, when you\nsee the word banking, I can't tell you that the word\n\"into\" is going to occur before",
    "start": "3099510",
    "end": "3105770"
  },
  {
    "text": "banking. But I want to do it\nas well as possible. So what I want my model to say\nis, after the word \"banking,\"",
    "start": "3105770",
    "end": "3114560"
  },
  {
    "text": "crises is pretty likely. But the word \"skillet\"\nis not very likely.",
    "start": "3114560",
    "end": "3123859"
  },
  {
    "text": "And if I can do that,\nI'm doing a good job. And so we turn that\ninto a piece of math.",
    "start": "3123860",
    "end": "3131119"
  },
  {
    "text": "Here's how we do it, turn\nit into a piece of math. So we're going to go\nthrough our corpus,",
    "start": "3131120",
    "end": "3137180"
  },
  {
    "text": "every position in the corpus. And we're going to have\na fixed window size",
    "start": "3137180",
    "end": "3142690"
  },
  {
    "text": "m which was 2 in my example. And then what we're\ngoing to want to do is have the probability of\nwords in the context being",
    "start": "3142690",
    "end": "3152200"
  },
  {
    "text": "as high as possible. So we want to maximize\nthis likelihood where we're going through\nevery position in the text,",
    "start": "3152200",
    "end": "3160099"
  },
  {
    "text": "and then we're going through\nevery word in the context and sort of wanting\nto make this big.",
    "start": "3160100",
    "end": "3165885"
  },
  {
    "text": " So conceptually, that's\nwhat we're doing.",
    "start": "3165885",
    "end": "3172490"
  },
  {
    "text": "But in practice, we\nnever quite do that. We use two little tricks here.",
    "start": "3172490",
    "end": "3180670"
  },
  {
    "text": "The first one is, for\ncompletely arbitrary reasons, it really makes no difference.",
    "start": "3180670",
    "end": "3187599"
  },
  {
    "text": "Everyone got into minimizing\nthings rather than maximizing things.",
    "start": "3187600",
    "end": "3192670"
  },
  {
    "text": "And so the algorithms\nthat we use get referred to as gradient descent,\nas you'll see in a moment.",
    "start": "3192670",
    "end": "3198520"
  },
  {
    "text": "So the first thing we do is\nput a minus sign in front so that we can minimize it\nrather than maximize it.",
    "start": "3198520",
    "end": "3205450"
  },
  {
    "text": "That part's pretty trivial. But the second part is, here,\nwe have this enormous product.",
    "start": "3205450",
    "end": "3210910"
  },
  {
    "text": "And working with\nenormous products is more difficult for the math. So the second thing that we\ndo is introduce a logarithm.",
    "start": "3210910",
    "end": "3219520"
  },
  {
    "text": "And so once we take the log\nof the likelihood, that then,",
    "start": "3219520",
    "end": "3226110"
  },
  {
    "text": "when we take logs of\nproducts, they turn into sums. And so now, we can sum over\neach word position, the text,",
    "start": "3226110",
    "end": "3235350"
  },
  {
    "text": "sum over each word in\nthe context window, and then sum these\nlog probabilities.",
    "start": "3235350",
    "end": "3241630"
  },
  {
    "text": "And then we've still got\nthe minus sign in front. So we want to minimize the\nsum of log probabilities.",
    "start": "3241630",
    "end": "3248200"
  },
  {
    "text": "So what we're doing\nis then wanting to look at the negative\nlog likelihood.",
    "start": "3248200",
    "end": "3256160"
  },
  {
    "text": "And then the final\nthing that we do is to-- since this\nwill get bigger,",
    "start": "3256160",
    "end": "3262410"
  },
  {
    "text": "depending on the number\nof words in the corpus, we divide through by the\nnumber of words in the corpus.",
    "start": "3262410",
    "end": "3268319"
  },
  {
    "text": "And so our objective function\nis the average negative log likelihood.",
    "start": "3268320",
    "end": "3274099"
  },
  {
    "text": "So by minimizing this\nobjective function, we are maximizing\nthe probability",
    "start": "3274100",
    "end": "3279619"
  },
  {
    "text": "of words in the context. We're almost there.",
    "start": "3279620",
    "end": "3286050"
  },
  {
    "text": "That's what we want to do. We've got a couple more tricks\nthat we want to get through.",
    "start": "3286050",
    "end": "3292589"
  },
  {
    "text": "The next one is, well,\nI've said we want to maximize this probability.",
    "start": "3292590",
    "end": "3297750"
  },
  {
    "text": "How do we maximize\nthis probability? What is this probability? We haven't defined\nhow are we going",
    "start": "3297750",
    "end": "3304099"
  },
  {
    "text": "to calculate this probability. And this is where the\nword vectors come in.",
    "start": "3304100",
    "end": "3309260"
  },
  {
    "text": "So we're going to\ndefine this probability in terms of the word vector.",
    "start": "3309260",
    "end": "3315680"
  },
  {
    "text": "So we're going to say each\nword type is represented by a vector of real numbers.",
    "start": "3315680",
    "end": "3321170"
  },
  {
    "text": "These are hundreds real numbers. And we're going to have\na formula that works out",
    "start": "3321170",
    "end": "3326589"
  },
  {
    "text": "the probability simply in terms\nof the vectors for each word,",
    "start": "3326590",
    "end": "3333160"
  },
  {
    "text": "that there are no other\nparameters in this model. So, over here I've\nshown this theta,",
    "start": "3333160",
    "end": "3338450"
  },
  {
    "text": "which are the\nparameters of our model. And all and only, the\nparameters of our model",
    "start": "3338450",
    "end": "3344859"
  },
  {
    "text": "are these word vectors for\neach word in the vocabulary. That's a lot of parameters\nbecause we have a lot of words.",
    "start": "3344860",
    "end": "3351740"
  },
  {
    "text": "And we've got fairly\nbig word vectors. But they are the\nonly parameters.",
    "start": "3351740",
    "end": "3357430"
  },
  {
    "text": "And how we do that is by\nusing this little trick here.",
    "start": "3357430",
    "end": "3363319"
  },
  {
    "text": "We're going to say the\nprobability of an outside word given a center\nword is going to be",
    "start": "3363320",
    "end": "3369420"
  },
  {
    "text": "defined in terms of the dot\nproduct of the two word vectors. So if things have a high dot\nproduct, they'll be similar.",
    "start": "3369420",
    "end": "3378540"
  },
  {
    "text": "And therefore, they'll\nhave a high probability of co-occurrence. Where I mean similar in\na kind of a weird sense.",
    "start": "3378540",
    "end": "3385660"
  },
  {
    "text": "It is the case that we're\ngoing to want to say hotel and motel are similar. But it's also the case that\nwe're going to want to have",
    "start": "3385660",
    "end": "3394500"
  },
  {
    "text": "the word \"the\" able to\nappear easily before the word \"student.\" So in some weird sense,\n\"the\" also has to be similar",
    "start": "3394500",
    "end": "3402540"
  },
  {
    "text": "to student. That has to be similar\nto basically any noun. So we're going to work\nwith dot products.",
    "start": "3402540",
    "end": "3411190"
  },
  {
    "text": "And then we do this funky\nlittle bit of math here, and that will give\nus our probabilities",
    "start": "3411190",
    "end": "3417660"
  },
  {
    "text": "So let's just go through\nthe funky bit of math. So here's our formula\nfor the probabilities.",
    "start": "3417660",
    "end": "3425160"
  },
  {
    "text": "So what we're doing here\nis we're starting off with this dot product.",
    "start": "3425160",
    "end": "3431150"
  },
  {
    "text": "So the dot product is\nyou take the two vectors. You multiply each component\ntogether, and you sum them.",
    "start": "3431150",
    "end": "3438390"
  },
  {
    "text": "So if they both the same sign,\nthat increases your dot product.",
    "start": "3438390",
    "end": "3443789"
  },
  {
    "text": "And if they're both big,\nit increases it a lot. So that gives us a similarity\nbetween two vectors.",
    "start": "3443790",
    "end": "3452160"
  },
  {
    "text": "And that's unbounded. That's just a real number. It can be either\nnegative or positive. But what we'd like to\nget out is a probability.",
    "start": "3452160",
    "end": "3460290"
  },
  {
    "text": "So for our next tricks, we,\nfirst of all, exponentiate. Because if we take e\nto the x for any x,",
    "start": "3460290",
    "end": "3469220"
  },
  {
    "text": "we now have to get\nsomething positive out. That's what exponentiation does.",
    "start": "3469220",
    "end": "3474320"
  },
  {
    "text": "And then, well, since it's\nmeant to be a probability, we'd like it to be\nbetween 0 and 1.",
    "start": "3474320",
    "end": "3481170"
  },
  {
    "text": "And so we turn it into\nnumbers between 0 and 1 in the dumbest way possible,\nwhich is we just normalize,",
    "start": "3481170",
    "end": "3488800"
  },
  {
    "text": "so that we work out the\nquantity in the numerator for every possible context word.",
    "start": "3488800",
    "end": "3496030"
  },
  {
    "text": "And so we get the total\nof all of those numbers and divide through by it. And then we're getting a\nprobability distribution",
    "start": "3496030",
    "end": "3503500"
  },
  {
    "text": "of how likely different\nwords are in this context. So this little trick\nthat we're doing here",
    "start": "3503500",
    "end": "3513370"
  },
  {
    "text": "is referred to as\nthe softmax function. So for the softmax function, you\ncan take unbounded real numbers,",
    "start": "3513370",
    "end": "3522890"
  },
  {
    "text": "put them through this\nlittle softmax trick that we just went\nthrough the steps of. And what you'll get out is\na probability distribution.",
    "start": "3522890",
    "end": "3531350"
  },
  {
    "text": "So I'm now getting,\nin this example, a probability distribution\nover context words.",
    "start": "3531350",
    "end": "3538280"
  },
  {
    "text": "My probability estimates\nover all the context words in my vocabulary will sum\nup to 1, by definition,",
    "start": "3538280",
    "end": "3545840"
  },
  {
    "text": "by the way that I've\nconstructed this. So it's called the\nsoftmax function",
    "start": "3545840",
    "end": "3552039"
  },
  {
    "text": "because it amplifies the\nprobabilities of the largest things. That's because of\nthe x function.",
    "start": "3552040",
    "end": "3561230"
  },
  {
    "text": "But it's soft because it's\nstill assign some probability to smaller items.",
    "start": "3561230",
    "end": "3567589"
  },
  {
    "text": "But it's sort of a\nfunny name because when you think about max, I\nmean, max normally picks out",
    "start": "3567590",
    "end": "3575859"
  },
  {
    "text": "just one thing. Whereas the softmax is turning\na bunch of real numbers",
    "start": "3575860",
    "end": "3581500"
  },
  {
    "text": "into a probability distribution. So this softmax is used\neverywhere in deep learning.",
    "start": "3581500",
    "end": "3589880"
  },
  {
    "text": "Any time that we're wanting\nto turn things that are just vectors in Rn into\nprobabilities, we shove them",
    "start": "3589880",
    "end": "3595750"
  },
  {
    "text": "through a softmax function. So in some sense,\nthis part, I think,",
    "start": "3595750",
    "end": "3609830"
  },
  {
    "text": "still seems very abstract. And I mean, the reason\nit seems very abstract",
    "start": "3609830",
    "end": "3617300"
  },
  {
    "text": "is because I've sort of said\nwe have vectors for each word.",
    "start": "3617300",
    "end": "3624630"
  },
  {
    "text": "And using these vectors, we can\nthen calculate probabilities.",
    "start": "3624630",
    "end": "3630049"
  },
  {
    "text": "But where do the\nvectors come from? And the answer to where the\nvectors are going to come from",
    "start": "3630050",
    "end": "3637130"
  },
  {
    "text": "is we're going to turn this\ninto an optimization problem. We have a large amount of text.",
    "start": "3637130",
    "end": "3643590"
  },
  {
    "text": "And so therefore, we can hope\nto find word vectors that",
    "start": "3643590",
    "end": "3649130"
  },
  {
    "text": "make the contexts of the words\nin our observed text as big",
    "start": "3649130",
    "end": "3654289"
  },
  {
    "text": "as possible. So literally what\nwe're going to do is we're going to start off with\nrandom vectors for every word.",
    "start": "3654290",
    "end": "3662740"
  },
  {
    "text": "And then we want to\nfiddle those vectors so that the calculated\nprobabilities of words",
    "start": "3662740",
    "end": "3669390"
  },
  {
    "text": "in a context go up. And we're going to keep fiddling\nuntil they stop going up",
    "start": "3669390",
    "end": "3674970"
  },
  {
    "text": "anymore, and we're getting the\nhighest probability estimates that we can. And the way that we do that\nfiddling is we use calculus.",
    "start": "3674970",
    "end": "3685140"
  },
  {
    "text": "So what we're going\nto do is conceptually exactly what you do\nif you're in something",
    "start": "3685140",
    "end": "3691560"
  },
  {
    "text": "like a two-dimensional space\nlike the picture on the right. That if you want\nto find the minimum",
    "start": "3691560",
    "end": "3697230"
  },
  {
    "text": "in this two-dimensional\nspace and you start off at the top left, what\nyou can do is, say,",
    "start": "3697230",
    "end": "3703030"
  },
  {
    "text": "let me work out the derivatives\nof the function at the top left.",
    "start": "3703030",
    "end": "3708790"
  },
  {
    "text": "And they point down\nand a bit to the right. And so you can walk down\nin a bit to the right.",
    "start": "3708790",
    "end": "3714720"
  },
  {
    "text": "And you can say,\noh, gee, given where I am now, let me work\nout the derivatives.",
    "start": "3714720",
    "end": "3720090"
  },
  {
    "text": "What direction do they point? And they're still pointing down,\nbut a bit more to the right. So you can walk a\nbit further that way,",
    "start": "3720090",
    "end": "3726500"
  },
  {
    "text": "and you can keep on walking. And eventually, you'll make it\nto the minimum of the space.",
    "start": "3726500",
    "end": "3732860"
  },
  {
    "text": "In our case, we've got a lot\nmore than two dimensions. So our parameters for our\nmodel are the concatenation",
    "start": "3732860",
    "end": "3742280"
  },
  {
    "text": "of all the word vectors. But it's even slightly\nworse than I've explained up until now.",
    "start": "3742280",
    "end": "3750090"
  },
  {
    "text": "Because actually, for each\nword, we assume two vectors. We assume one vector when\nthey're the center word,",
    "start": "3750090",
    "end": "3757670"
  },
  {
    "text": "and one vector when\nthey're the outside word. Doing that just\nmakes the math a bit simpler, which I\ncan explain later.",
    "start": "3757670",
    "end": "3765030"
  },
  {
    "text": "So if we say had 100\ndimensional vectors, we'll have 100 parameters for\naardvarks and an outside word.",
    "start": "3765030",
    "end": "3772840"
  },
  {
    "text": "100 parameters for \"a\"\nas an outside word, all the way through to\n100 parameters for zebras",
    "start": "3772840",
    "end": "3779590"
  },
  {
    "text": "and outside word. Then we'd have 100 parameters\nfor aardvark as a center",
    "start": "3779590",
    "end": "3785860"
  },
  {
    "text": "word continuing down. So if we had a vocabulary\nof 400,000 words and 100",
    "start": "3785860",
    "end": "3793329"
  },
  {
    "text": "dimensional word vectors, that\nmeans we'd have 400,000 times 2",
    "start": "3793330",
    "end": "3798610"
  },
  {
    "text": "is 800,000 times 100. We'd have 80 million parameters.",
    "start": "3798610",
    "end": "3803750"
  },
  {
    "text": "So that's a lot of parameters\nin our space to try and fiddle to optimize things.",
    "start": "3803750",
    "end": "3809300"
  },
  {
    "text": "But luckily, we\nhave big computers. And that's the kind\nof thing that we do.",
    "start": "3809300",
    "end": "3814820"
  },
  {
    "text": "So we simply say, gee, this\nis our optimization problem.",
    "start": "3814820",
    "end": "3821570"
  },
  {
    "text": "We're going to\ncompute the gradients of all of these parameters.",
    "start": "3821570",
    "end": "3827869"
  },
  {
    "text": "And that will give us the\nanswer of what we have.",
    "start": "3827870",
    "end": "3834150"
  },
  {
    "text": "And this feels like magic.",
    "start": "3834150",
    "end": "3839170"
  },
  {
    "text": "I mean, it doesn't really\nseem like we could just start with nothing.",
    "start": "3839170",
    "end": "3844540"
  },
  {
    "text": "We could start with random\nword vectors and a pile of text and say, do some math, and we\nwill get something useful out.",
    "start": "3844540",
    "end": "3853650"
  },
  {
    "text": "But the miracle of what happens\nin these deep learning spaces is we do get\nsomething useful out.",
    "start": "3853650",
    "end": "3860290"
  },
  {
    "text": "We can just minimize\nall of the parameters, and then we'll get\nsomething useful out.",
    "start": "3860290",
    "end": "3869880"
  },
  {
    "text": "So what I wanted to-- I guess I'm not\ngoing to quite get to the end of what\nI hoped to do today.",
    "start": "3869880",
    "end": "3875610"
  },
  {
    "text": "But what I wanted to do is get\nthrough some of what we do here.",
    "start": "3875610",
    "end": "3884710"
  },
  {
    "text": "But I wanted to take a few\nminutes to go through concretely how we do the math\nof minimization.",
    "start": "3884710",
    "end": "3893400"
  },
  {
    "text": "Now, lots of different people\ntake CS224N, and some of way",
    "start": "3893400",
    "end": "3902180"
  },
  {
    "text": "more math than I do. And so if this next 10 minutes\nmight be extremely boring.",
    "start": "3902180",
    "end": "3908520"
  },
  {
    "text": "And if that's the\ncase, you can either catch up on Discord or\nInstagram or something,",
    "start": "3908520",
    "end": "3914070"
  },
  {
    "text": "or else you can leave. But it turns out there\nare other people that do CS224N that can't quite\nremember when they last",
    "start": "3914070",
    "end": "3922490"
  },
  {
    "text": "did a math course. And we'd like everybody to\nbe able to learn something",
    "start": "3922490",
    "end": "3927500"
  },
  {
    "text": "about this. So I do actually liken the first\ntwo weeks to go through it a bit",
    "start": "3927500",
    "end": "3934130"
  },
  {
    "text": "concretely. So let's try to do this. So this was our likelihood. And then we'd already\ncovered the fact",
    "start": "3934130",
    "end": "3940730"
  },
  {
    "text": "that what we were going to do\nis have an objective function in terms of our parameters, that\nwas the average negative log",
    "start": "3940730",
    "end": "3949870"
  },
  {
    "text": "likelihood across all the words. ",
    "start": "3949870",
    "end": "3955750"
  },
  {
    "text": "If I remember the notation\nfor this, the sum-- ",
    "start": "3955750",
    "end": "3961175"
  },
  {
    "text": "oops.  I'll probably have a\nhard time writing this.",
    "start": "3961175",
    "end": "3967330"
  },
  {
    "text": "The sum of position m-- I've got a more neatly\nwritten out version",
    "start": "3967330",
    "end": "3973540"
  },
  {
    "text": "of it that appears on the\nversion of the slides that's on the website. And then we're\ngoing to be taking",
    "start": "3973540",
    "end": "3980350"
  },
  {
    "text": "this log of the probability\nof the word at position t",
    "start": "3980350",
    "end": "3989770"
  },
  {
    "text": "plus position j, t plus j.",
    "start": "3989770",
    "end": "3998333"
  },
  {
    "text": " Trying to write this on my\niPad is not working super well,",
    "start": "3998333",
    "end": "4006150"
  },
  {
    "text": "I'll confess. We'll see how I get on. wt. And so then we had the\nform of what we wanted",
    "start": "4006150",
    "end": "4020120"
  },
  {
    "text": "to use for the probability. And the probability of an\noutside word given a context",
    "start": "4020120",
    "end": "4026750"
  },
  {
    "text": "word was then this\nsoftmax equation where we're taking the\nexp of the outside vector",
    "start": "4026750",
    "end": "4036950"
  },
  {
    "text": "and the center vector over\nthe normalization term",
    "start": "4036950",
    "end": "4044210"
  },
  {
    "text": "where we sum over\nthe vocabulary. ",
    "start": "4044210",
    "end": "4059180"
  },
  {
    "text": "So to work out how to\nchange our parameters.",
    "start": "4059180",
    "end": "4067160"
  },
  {
    "text": "So our parameters are\nall of these word vectors that we summarize inside theta.",
    "start": "4067160",
    "end": "4075670"
  },
  {
    "text": "What we're then\ngoing to want to do is work out the\npartial derivative",
    "start": "4075670",
    "end": "4081430"
  },
  {
    "text": "of this objective\nfunction with respect to all the parameters theta.",
    "start": "4081430",
    "end": "4087920"
  },
  {
    "text": "But in particular, I'm\ngoing to just start doing here the partial\nderivatives with respect",
    "start": "4087920",
    "end": "4096489"
  },
  {
    "text": "to the center word. And we can work through the\noutside words separately.",
    "start": "4096490",
    "end": "4104689"
  },
  {
    "text": "Well, this partial\nderivative is a big sum. And it's a big sum\nof terms like this.",
    "start": "4104689",
    "end": "4113239"
  },
  {
    "text": "And so when I have\na partial derivative of a big sum of\nterms, I can work out",
    "start": "4113240",
    "end": "4118568"
  },
  {
    "text": "the partial derivatives of\neach term independently, and then sum them.",
    "start": "4118569",
    "end": "4123580"
  },
  {
    "text": "So what I want to be\ndoing is working out the partial derivative of the\nlog of this probability, which",
    "start": "4123580",
    "end": "4134939"
  },
  {
    "text": "equals the log of that with\nrespect to the center vector.",
    "start": "4134939",
    "end": "4142259"
  },
  {
    "text": "And so at this point, I have a\nlog of two things being divided.",
    "start": "4142260",
    "end": "4150009"
  },
  {
    "text": "And so that means I\ncan separate that out of the log of the\nnumerator minus the log",
    "start": "4150010",
    "end": "4156810"
  },
  {
    "text": "of the denominator. And so what I'll be\ndoing is working out the partial derivative\nwith respect",
    "start": "4156810",
    "end": "4164068"
  },
  {
    "text": "to the center vector\nof the numerator,",
    "start": "4164069",
    "end": "4169560"
  },
  {
    "text": "log exp of uTvc minus the\npartial derivative with respect",
    "start": "4169560",
    "end": "4181729"
  },
  {
    "text": "to the denominator, which is\nthen the log of the sum of W",
    "start": "4181729",
    "end": "4189799"
  },
  {
    "text": "equals 1 to v of exp-- ",
    "start": "4189800",
    "end": "4199610"
  },
  {
    "text": "OK. I'm having real\ntrouble here writing. I look at the slides where\nI wrote it neatly at home.",
    "start": "4199610",
    "end": "4206370"
  },
  {
    "text": "So I want to work\nwith these two terms.",
    "start": "4206370",
    "end": "4212150"
  },
  {
    "text": "Now, at this point, part of it-- ",
    "start": "4212150",
    "end": "4218270"
  },
  {
    "text": "at this point,\npart of it is easy. Because here, I just have\na log of an exponential.",
    "start": "4218270",
    "end": "4224850"
  },
  {
    "text": "And so those two functions\njust cancel out and go away. And so then, I want to get\nthe partial derivative of u",
    "start": "4224850",
    "end": "4235179"
  },
  {
    "text": "outside transpose vCenter\nwith respect to vCenter.",
    "start": "4235180",
    "end": "4241100"
  },
  {
    "text": "And so what you get\nfor the answer to that is that just comes out as u0.",
    "start": "4241100",
    "end": "4250070"
  },
  {
    "text": "And maybe you remember that. But if you don't remember that,\nthe thing to think about is,",
    "start": "4250070",
    "end": "4258590"
  },
  {
    "text": "OK, this is a whole vector. And so we've got a vector\nhere and a vector here.",
    "start": "4258590",
    "end": "4264739"
  },
  {
    "text": "So what this is going\nto be looking like is sort of u1v1 plus u2v2\nplus u3v3, et cetera long.",
    "start": "4264740",
    "end": "4278720"
  },
  {
    "text": "And so what we're\ngoing to want to do is work out the partial\nderivative with respect",
    "start": "4278720",
    "end": "4284170"
  },
  {
    "text": "to each element vi. And so if you just think of\na single element derivative",
    "start": "4284170",
    "end": "4292030"
  },
  {
    "text": "with respect to v1,\nwell, it's going to be just u1 because every\nother term would go to 0.",
    "start": "4292030",
    "end": "4301590"
  },
  {
    "text": "And then if you worked it\nout with respect to v2, then it would be just u2.",
    "start": "4301590",
    "end": "4307170"
  },
  {
    "text": "And every other term goes to 0. And so since you keep on doing\nthat along the whole vector,",
    "start": "4307170",
    "end": "4312969"
  },
  {
    "text": "that what you're\ngoing to get out is the vector u1,\nu2, u3 down the vocab",
    "start": "4312970",
    "end": "4321510"
  },
  {
    "text": "for the whole list\nof vocab items. So that part is easy.",
    "start": "4321510",
    "end": "4327900"
  },
  {
    "text": "But then, we also want to work\nout the partial derivatives",
    "start": "4327900",
    "end": "4334290"
  },
  {
    "text": "of that one. And at that point, I maybe\nhave to go to another slide.",
    "start": "4334290",
    "end": "4342130"
  },
  {
    "text": "So we then want to have the\npartial derivative with respect",
    "start": "4342130",
    "end": "4352429"
  },
  {
    "text": "to vc of the log of the sum\nequals w equal to 1 to v",
    "start": "4352430",
    "end": "4363080"
  },
  {
    "text": "of the exp of uw transpose vc.",
    "start": "4363080",
    "end": "4368730"
  },
  {
    "text": "So at this point, things\naren't quite so easy. And we have to remember a\nlittle bit more calculus.",
    "start": "4368730",
    "end": "4375840"
  },
  {
    "text": "So in particular, what we have\nto remember is the chain rule. So here, we have\nthis inside function.",
    "start": "4375840",
    "end": "4383909"
  },
  {
    "text": "So that we've got a function-- we've got a function g\nof vc, which we might",
    "start": "4383910",
    "end": "4393230"
  },
  {
    "text": "say the output of that is z. And then we've put outside\nthat and an extra function f.",
    "start": "4393230",
    "end": "4401280"
  },
  {
    "text": "And so when we have something\nlike that, what we get is the derivative of\nf with respect to vc.",
    "start": "4401280",
    "end": "4410120"
  },
  {
    "text": "We can take the derivative\nof f with respect to z times the derivative of\nz with respect to vc.",
    "start": "4410120",
    "end": "4420320"
  },
  {
    "text": "That's the chain rule. So we are going to\nthen apply that here.",
    "start": "4420320",
    "end": "4426020"
  },
  {
    "text": "So first of all, we can\ntake the derivative of log.",
    "start": "4426020",
    "end": "4432040"
  },
  {
    "text": "And so the derivative\nof log is 1 on x. You have to remember\nthat or look it",
    "start": "4432040",
    "end": "4437410"
  },
  {
    "text": "up or get Mathematica to do it\nfor you or something like that.",
    "start": "4437410",
    "end": "4442930"
  },
  {
    "text": "And so we're going to have\n1 over the inside z part.",
    "start": "4442930",
    "end": "4449870"
  },
  {
    "text": "The sum of w equals 1\nto v of the exp uwTvc. ",
    "start": "4449870",
    "end": "4457810"
  },
  {
    "text": "And then that's going to be\nmultiplied by the derivative",
    "start": "4457810",
    "end": "4463030"
  },
  {
    "text": "of the inside part. So then we're going to have\nthe derivative with respect",
    "start": "4463030",
    "end": "4472200"
  },
  {
    "text": "to vc of the sum of w\nequals 1 to v of v exp of--",
    "start": "4472200",
    "end": "4483290"
  },
  {
    "start": "4483290",
    "end": "4488610"
  },
  {
    "text": "So that's made us a\nlittle bit of progress.",
    "start": "4488610",
    "end": "4496800"
  },
  {
    "text": "But we've still got\nsomething to do here. And so, well, what\nwe're going to do",
    "start": "4496800",
    "end": "4502020"
  },
  {
    "text": "here is we're going\nto notice, oh, wait, we're again in the space to\nrun the chain rule again.",
    "start": "4502020",
    "end": "4510280"
  },
  {
    "text": "So now, we've got\nthis function-- well, so first of all, we can\nmove the sum to the outside",
    "start": "4510280",
    "end": "4516099"
  },
  {
    "text": "because we've got a sum\nof terms w equals 1 to v. And so we want to work out the\nderivatives of the inside piece",
    "start": "4516100",
    "end": "4524550"
  },
  {
    "text": "with respect to it. Sorry, I'm doing this\nkind of informally of just doing this piece now.",
    "start": "4524550",
    "end": "4530120"
  },
  {
    "text": "So this, again, gives us\nan f over a function g.",
    "start": "4530120",
    "end": "4537800"
  },
  {
    "text": "And so we're going to again\nwant to split the pieces up. And so use the chain\nrule one more time.",
    "start": "4537800",
    "end": "4544920"
  },
  {
    "text": "So we're going to have the\nsum of w equals 1 to v. And now, we have to know\nwhat the derivative of x is.",
    "start": "4544920",
    "end": "4551550"
  },
  {
    "text": "And the derivative of x is exp. So that will be exp of uxTv0.",
    "start": "4551550",
    "end": "4557286"
  },
  {
    "text": " And then we're taking the\nderivative of the inside part",
    "start": "4557286",
    "end": "4566420"
  },
  {
    "text": "with respect to vc of uxTvc. ",
    "start": "4566420",
    "end": "4572370"
  },
  {
    "text": "Well, luckily this was\nthe bit that we already knew how to do because\nwe worked it out before.",
    "start": "4572370",
    "end": "4578490"
  },
  {
    "text": "And so this is going to be\nthe sum of w equals 1 to v of this exp times ux.",
    "start": "4578490",
    "end": "4591190"
  },
  {
    "text": "So then, at this point, we\nwant to combine these two forms",
    "start": "4591190",
    "end": "4597670"
  },
  {
    "text": "together so that\nwe want to combine this part that we worked out,\nand this piece here that we've",
    "start": "4597670",
    "end": "4605980"
  },
  {
    "text": "worked out. And if we can\ncombine them together",
    "start": "4605980",
    "end": "4611500"
  },
  {
    "text": "with what we worked out on the\nfirst slide for the numerator,",
    "start": "4611500",
    "end": "4617110"
  },
  {
    "text": "since we have the u0, which was\nthe derivative of the numerator.",
    "start": "4617110",
    "end": "4626270"
  },
  {
    "text": "And then for the derivative\nof the denominator,",
    "start": "4626270",
    "end": "4631490"
  },
  {
    "text": "we're going to have\non top this part.",
    "start": "4631490",
    "end": "4636650"
  },
  {
    "text": "And then on the bottom, we're\ngoing to have that part. And so we can rewrite that as\nthe sum from w equals 1 to v",
    "start": "4636650",
    "end": "4646469"
  },
  {
    "text": "of the exp of uxTv0\ntimes ux over the sum--",
    "start": "4646470",
    "end": "4658250"
  },
  {
    "text": " sorry, x equals 1 to v\nsum over w equals 1 to v",
    "start": "4658250",
    "end": "4668490"
  },
  {
    "text": "of the exp, this\npart here, of uw. ",
    "start": "4668490",
    "end": "4676440"
  },
  {
    "text": "So we can rearrange\nthings in that form. And then lo and behold, we've\nrecreated here this form",
    "start": "4676440",
    "end": "4687210"
  },
  {
    "text": "of the softmax equation. So we end up with u0 minus\nthe sum over x equals 1 to v",
    "start": "4687210",
    "end": "4697440"
  },
  {
    "text": "of the probability of\nx given c times u of x.",
    "start": "4697440",
    "end": "4704810"
  },
  {
    "text": "So what this is saying\nis we're wanting to have this quantity, which\ntakes the actual observed u",
    "start": "4704810",
    "end": "4713360"
  },
  {
    "text": "vector. And it's comparing it to\nthe weighted prediction.",
    "start": "4713360",
    "end": "4720090"
  },
  {
    "text": "So we're taking the weighted\nsum of our current ux vectors",
    "start": "4720090",
    "end": "4725750"
  },
  {
    "text": "based on how likely\nthey were to occur. And so this is a form\nthat you see quite a bit",
    "start": "4725750",
    "end": "4733520"
  },
  {
    "text": "in these kind of derivatives. You get observed minus\nexpected, the weighted average.",
    "start": "4733520",
    "end": "4740670"
  },
  {
    "text": "And so what you'd like to\nhave is your expectation, the weighted average, be the\nsame as what was observed.",
    "start": "4740670",
    "end": "4750390"
  },
  {
    "text": "Because then you'll\nget a derivative of 0, which means that\nyou've hit a maximum.",
    "start": "4750390",
    "end": "4756530"
  },
  {
    "text": "And so that gives us the\nform of the derivative",
    "start": "4756530",
    "end": "4764800"
  },
  {
    "text": "that we're having with respect\nto the center vector parameters. To finish it off, you'd have\nto then work it out also",
    "start": "4764800",
    "end": "4772150"
  },
  {
    "text": "for the outside\nvector parameters. But hey, it's officially\nthe end of class time.",
    "start": "4772150",
    "end": "4777530"
  },
  {
    "text": "So I'd better wrap\nup quickly now. But so the deal is\nwe're going to work out",
    "start": "4777530",
    "end": "4782920"
  },
  {
    "text": "all of these derivatives\nfor each parameter. And then these derivatives\nwill give a direction",
    "start": "4782920",
    "end": "4790300"
  },
  {
    "text": "to change numbers, which\nwill let us find good word vectors automatically.",
    "start": "4790300",
    "end": "4797019"
  },
  {
    "text": "I do want you to\nunderstand how this works. But fortunately, you'll find\nout very quickly that computers",
    "start": "4797020",
    "end": "4802960"
  },
  {
    "text": "will do this for you. And on a regular basis,\nyou don't actually have to do it yourself. More about that on Thursday.",
    "start": "4802960",
    "end": "4809690"
  },
  {
    "text": "See you everyone. ",
    "start": "4809690",
    "end": "4817000"
  }
]