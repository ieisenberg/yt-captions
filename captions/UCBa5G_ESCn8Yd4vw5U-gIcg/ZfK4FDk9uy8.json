[
  {
    "text": "Welcome. Welcome to the\nStanford CS224W course. Today, I will be your guest\ninstructor on behalf of Jure.",
    "start": "5290",
    "end": "14080"
  },
  {
    "text": "My name is Jiaxuan, and\nyou might find more information on myself on the website.",
    "start": "14080",
    "end": "19269"
  },
  {
    "text": "I, also, was one of the head\nTA in the previous offering. And so I'm relatively\nfamiliar with the course.",
    "start": "19270",
    "end": "26650"
  },
  {
    "text": "Today, I'll be very\nexcited to give the topic of graph\nneural network to you all in this lecture.",
    "start": "26650",
    "end": "33110"
  },
  {
    "text": "So let's give a quick recap\nof what we have learned in the previous lecture.",
    "start": "33110",
    "end": "38390"
  },
  {
    "text": "So the key concept we\nlearned is node embedding. Our intuition is that we want\nto encode nodes in the network",
    "start": "38390",
    "end": "45550"
  },
  {
    "text": "into low dimensional\nvector space. In particular, we want\nto learn a function that takes an input graph, and embed\nthat into low-dimensional node",
    "start": "45550",
    "end": "54219"
  },
  {
    "text": "embedding space. Here, we project\ninto two dimension. And the key problem of\nmachine learning on graph",
    "start": "54220",
    "end": "60310"
  },
  {
    "text": "is how do we define\nthis function f. So here's a recap of\nhow to encode a node.",
    "start": "60310",
    "end": "70480"
  },
  {
    "text": "The goal is to\ndefine a similarity function that can represent\nthe similarity in the network.",
    "start": "70480",
    "end": "77159"
  },
  {
    "text": "And we want to approximate\nin the embedding space. And we really need\nto define two things.",
    "start": "77160",
    "end": "83140"
  },
  {
    "text": "The first is the\nsimilarity function, represent how two nodes are\nclose together in the network.",
    "start": "83140",
    "end": "88570"
  },
  {
    "text": "Another thing is\nthe encoder function that basically tells us how\nto map the node in the graph",
    "start": "88570",
    "end": "94050"
  },
  {
    "text": "into the embedding space. And there are two\nkey components here.",
    "start": "94050",
    "end": "101490"
  },
  {
    "text": "We have encoder\nthat map the node into embedding and decoder\nthat define this similarity",
    "start": "101490",
    "end": "107125"
  },
  {
    "text": "function. So the encoder function\nwill take the node as input",
    "start": "107125",
    "end": "114610"
  },
  {
    "text": "and generate embedding. The d-dimensional space\nand similarity function will specify the\nrelationship in the network,",
    "start": "114610",
    "end": "121060"
  },
  {
    "text": "and also express in the\nloss function as a decoder. And how do we define\nthe similarity?",
    "start": "121060",
    "end": "127210"
  },
  {
    "text": "Last lecture, we talk\nabout random walk based similarity function. For example, suppose\ntwo nodes, they",
    "start": "127210",
    "end": "132580"
  },
  {
    "text": "co-occur in the\nshort random walk, then we think these\ntwo nodes are similar. So that's one way to\ndefine similarity.",
    "start": "132580",
    "end": "138970"
  },
  {
    "text": "And we can also come up with\nother way to define similarity.",
    "start": "138970",
    "end": "144380"
  },
  {
    "text": "And how about encoding? We have seen one of the\nsimplest way to encode a node.",
    "start": "144380",
    "end": "149660"
  },
  {
    "text": "That is through a shallow\nencoder or embedding lookup. So in this shallow encoder,\nwe have embedding matrix.",
    "start": "149660",
    "end": "156920"
  },
  {
    "text": "And one column of the embedding\nmatrix represents one node. And the dimension of\nthe embedding matrix",
    "start": "156920",
    "end": "163370"
  },
  {
    "text": "represent the size of\nthe node embedding. And we talk about\nan embedding lookup,",
    "start": "163370",
    "end": "169610"
  },
  {
    "text": "which means that we will-- to represent a\nnode, we will pick one column in the\nembedding matrix",
    "start": "169610",
    "end": "175489"
  },
  {
    "text": "and get that vector as\nthe node representation. So this is the\nshallow encoder idea.",
    "start": "175490",
    "end": "180170"
  },
  {
    "text": "So what are the issues\nof this shallow encoder? Well, we list three\nkey limitations.",
    "start": "183610",
    "end": "189160"
  },
  {
    "text": "The first is that this\nway of encoding nodes is not really scalable because\nwe assign one learnable vector",
    "start": "189160",
    "end": "196410"
  },
  {
    "text": "for each node. So in total, there will\nbe to the order of V times",
    "start": "196410",
    "end": "201630"
  },
  {
    "text": "d, this large number\nof parameters. And it wouldn't work for\nreal world large graph.",
    "start": "201630",
    "end": "207240"
  },
  {
    "text": "Say, it has billions of nodes. And another limitation is that\nthis approach is transductive,",
    "start": "207240",
    "end": "215610"
  },
  {
    "text": "which means that the encoder\ncan only apply to the nodes that have seen during training time. So suppose, we have\nthis embedding matrix,",
    "start": "215610",
    "end": "222480"
  },
  {
    "text": "but there is a new\nnode that comes, we do not know how to encode\nthat new node because it's not",
    "start": "222480",
    "end": "228180"
  },
  {
    "text": "even in the embedding matrix. And lastly, this\napproach does not incorporate novel attributes.",
    "start": "228180",
    "end": "234129"
  },
  {
    "text": "So in real world graph,\nthe attributes or nodes are usually very valuable. For example, in protein\ninteraction graphs,",
    "start": "234130",
    "end": "240090"
  },
  {
    "text": "we have a lot of\nprotein properties that can be associated\nwith a given node. So without that\nattribute, we wouldn't",
    "start": "240090",
    "end": "246400"
  },
  {
    "text": "be able to meaningfully\nlearn from a network. But this kind of encoding\nlookup, embedding lookup,",
    "start": "246400",
    "end": "252250"
  },
  {
    "text": "we wouldn't be able to utilize\nsuch valuable attributes. So these are the limitations\nfor shadow encoder.",
    "start": "252250",
    "end": "261419"
  },
  {
    "text": "And that's why today,\nwe want to dive deeper into how we can design more\nexpressive, more powerful",
    "start": "261420",
    "end": "267600"
  },
  {
    "text": "encoders. And our idea is to use\ndeep graph encoder. And the idea is that rather\nthan using a symbol embedding",
    "start": "267600",
    "end": "275580"
  },
  {
    "text": "lookup, we will define multiple\nlayers of neural network transformations and use\nthat deep neural network",
    "start": "275580",
    "end": "283020"
  },
  {
    "text": "to encode a node in the graph. And note that these\ndifferent colors",
    "start": "283020",
    "end": "288270"
  },
  {
    "text": "can be combined with\nthe node similarity function that we\nhave just introduced. For example, we can\nalso define some sort",
    "start": "288270",
    "end": "294990"
  },
  {
    "text": "of random walk based\nsimilarity function even for the deep encoder that\nwe'll later introduce today.",
    "start": "294990",
    "end": "300630"
  },
  {
    "text": "So here is our plan. The pipeline of how to\nencode a deep graph encoder.",
    "start": "304570",
    "end": "309600"
  },
  {
    "text": "So our input will be\na complex network. We will pass it through several\ngraph convolutional layers.",
    "start": "309600",
    "end": "316350"
  },
  {
    "text": "Between these layers,\nwe can inject, for example, activation\nfunctions, and then some regularization layers.",
    "start": "316350",
    "end": "322990"
  },
  {
    "text": "And after computing this\nthrough this deep encoder, we will get an output.",
    "start": "322990",
    "end": "328200"
  },
  {
    "text": "The output could\nbe node embeddings, which we most commonly seen. But we can also generate, say,\nembeddings for a subgraph,",
    "start": "328200",
    "end": "335490"
  },
  {
    "text": "and even embedding\nfor the entire graph. So these are some\noptions that we can use to generate through\nthis deep graph encoder.",
    "start": "335490",
    "end": "343620"
  },
  {
    "text": "So suppose we have this\nencoder, what we can do? Well, we can define many\nexciting tasks on graphs.",
    "start": "346470",
    "end": "352390"
  },
  {
    "text": "And here, I list four\ndifferent levels. On the node level,\nwe can use this idea to encode different nodes\nand make node classification.",
    "start": "352390",
    "end": "361020"
  },
  {
    "text": "For example, suppose we have\na drug protein interaction network, and then we can predict\nwhether this drug is toxic",
    "start": "361020",
    "end": "367710"
  },
  {
    "text": "or not, or whether it can be\nused to treat a given disease. We can also define link\nprediction on network.",
    "start": "367710",
    "end": "375120"
  },
  {
    "text": "And link prediction can\nbe quite useful for, say recommender\nsystems, where we can define an interaction\nnetwork between users",
    "start": "375120",
    "end": "381990"
  },
  {
    "text": "and items. And we can predict whether a\nuser will buy a certain item. And we can also define community\ndetection over a network.",
    "start": "381990",
    "end": "389680"
  },
  {
    "text": "And this can be useful,\nsay, for financial networks, where we can have a\ncluster of fraudsters.",
    "start": "389680",
    "end": "395880"
  },
  {
    "text": "And they may have some\nfishy transactions between each other. And our goal is to detect\nthose kind of anomaly clusters",
    "start": "395880",
    "end": "403139"
  },
  {
    "text": "in the network. And finally, we can define\na network similarity task.",
    "start": "403140",
    "end": "408310"
  },
  {
    "text": "In that case, it will be a graph\nlevel task or a graph level prediction. For example, we\ncan use this idea",
    "start": "408310",
    "end": "414729"
  },
  {
    "text": "to encode different\ndrug molecules. So here, we treat a\nmolecule as a network.",
    "start": "414730",
    "end": "419800"
  },
  {
    "text": "And we want to classify\ndifferent types of molecules. So this is just like a\nglimpse of different ways",
    "start": "419800",
    "end": "427690"
  },
  {
    "text": "you can define machine\nlearning tasks on graphs. OK. So so far, we have\nmotivated you why",
    "start": "427690",
    "end": "435140"
  },
  {
    "text": "we want to use deep\nencoders to encode graph. And we will start by just\ntaking a very brief introduction",
    "start": "435140",
    "end": "443270"
  },
  {
    "text": "of basics of deep learning. And a lot of the\ncourse materials are also posted online.",
    "start": "443270",
    "end": "449030"
  },
  {
    "text": "In this lecture, we\nwouldn't cover all of them. So because we assume\nthat most of you already have some\nidea of deep learning.",
    "start": "449030",
    "end": "456419"
  },
  {
    "text": "But if you feel not\nlike a very confident of some of the materials, please\ntake a look on the website.",
    "start": "456420",
    "end": "461930"
  },
  {
    "text": "And also, we have this\nrecitation and office hours. So you can also ask TAs\nabout specific questions",
    "start": "461930",
    "end": "468500"
  },
  {
    "text": "about deep learning basics. So here, I will just summarize\nsome of the key ideas",
    "start": "468500",
    "end": "475070"
  },
  {
    "text": "that we probably want to\nuse in today's lecture. So our idea is to\nformulate machine learning",
    "start": "475070",
    "end": "481130"
  },
  {
    "text": "as an optimization problem. This optimization\nproblem has an input x, and the goal is to\npredict the label y.",
    "start": "481130",
    "end": "488120"
  },
  {
    "text": "And the input x can be\ndifferent data modalities such as vectors,\nsequences, matrices.",
    "start": "488120",
    "end": "494820"
  },
  {
    "text": "And today, we'll\ntalk about graphs. And we will formulate this task\nas an optimization problem.",
    "start": "494820",
    "end": "501400"
  },
  {
    "text": "And the problem looks like this. Our goal is to find a optimal\nparameter theta, such that it",
    "start": "501400",
    "end": "508090"
  },
  {
    "text": "can minimize the loss function. The loss function, where we\ninvolve labels and predictions.",
    "start": "508090",
    "end": "514929"
  },
  {
    "text": "And theta is just a parameter\nthat we want to learn. And it could be\nthe weight matrices",
    "start": "514929",
    "end": "519940"
  },
  {
    "text": "in a deep neural network. And also, in this\nshallow encoder example,",
    "start": "519940",
    "end": "525890"
  },
  {
    "text": "the theta is like the\nentire embedding matrix that we learn over.",
    "start": "525890",
    "end": "532389"
  },
  {
    "text": "And we can also define\ndifferent type of loss function, depending on the type\nof task we care about.",
    "start": "532390",
    "end": "537850"
  },
  {
    "text": "For example, for regression\ntask, we usually use L2 loss and shown here. And for classification\ntask, we also",
    "start": "537850",
    "end": "545319"
  },
  {
    "text": "use like a\ncross-entropy loss here. So one useful\nresource is this link",
    "start": "545320",
    "end": "551680"
  },
  {
    "text": "of PyTorch, which gives both\ntheoretical and concrete implementation examples\nof a bunch of useful loss",
    "start": "551680",
    "end": "558880"
  },
  {
    "text": "functions for deep learning. So we suggest you\nto take a look. And this will also be useful for\nyour colab and potential course",
    "start": "558880",
    "end": "566715"
  },
  {
    "text": "project. And another concept will\nusually pop up in the lecture,",
    "start": "566715",
    "end": "574270"
  },
  {
    "text": "and we'll just use\nthe acronym MLP. So MLP is really\nmultilayer perceptron.",
    "start": "574270",
    "end": "580300"
  },
  {
    "text": "And it usually can\nbe written as this. So we have the input x.",
    "start": "580300",
    "end": "585339"
  },
  {
    "text": "The upper L means that\nwe consider the input for the L layer of the network.",
    "start": "585340",
    "end": "592660"
  },
  {
    "text": "We will apply a\nlinear transformation for the input and then plus the\nbias, which is also trainable.",
    "start": "592660",
    "end": "599350"
  },
  {
    "text": "We will apply some nonlinear\nactivation function, which could be relu or sigmoid.",
    "start": "599350",
    "end": "605079"
  },
  {
    "text": "And then we get the output. So this layer is pretty simple. But it's really\nthe building block",
    "start": "605080",
    "end": "610660"
  },
  {
    "text": "for tons of useful deep\nlearning architectures. For example, in\ngraph neural network, and also, even in other\ndomains like even transformer,",
    "start": "610660",
    "end": "618610"
  },
  {
    "text": "we have MLP to do the\nlinear or nonlinear projections, et cetera.",
    "start": "618610",
    "end": "623990"
  },
  {
    "text": "And here is a concrete example. Suppose we have a\ntwo-dimensional input to an MLP.",
    "start": "623990",
    "end": "629589"
  },
  {
    "text": "You see there are\ntwo neurons here. We will apply different\nintermediate layer in the MLP.",
    "start": "629590",
    "end": "636410"
  },
  {
    "text": "For example, a\nthree-dimensional, like a hidden layer here. And the output will\nbe one dimension.",
    "start": "636410",
    "end": "642380"
  },
  {
    "text": "So this is like a\nvery frequently used, like a module in deep\nlearning architectures.",
    "start": "642380",
    "end": "650029"
  },
  {
    "text": "And to summarize, we\nformulate machine learning as an optimization problem\nabout finding the optimum theta",
    "start": "650030",
    "end": "656060"
  },
  {
    "text": "for our deep neural network. And f can be anything,\ncan be a linear layer",
    "start": "656060",
    "end": "661070"
  },
  {
    "text": "MLP or a complex\ngraph neural network. The way we will train it\nis through propagation,",
    "start": "661070",
    "end": "668180"
  },
  {
    "text": "where we, first, forward\npropagate to make inference, to make predictions. And then we perform back\npropagate to optimize and find",
    "start": "668180",
    "end": "675530"
  },
  {
    "text": "the optimum theta. And we will use the\ngradient descent to optimize theta\nover many iterations.",
    "start": "675530",
    "end": "683020"
  },
  {
    "text": "And yeah. That's the basic idea\nof deep learning. So let's jump into\nmore exciting topic",
    "start": "683020",
    "end": "690050"
  },
  {
    "text": "today, that is about\ndeep learning for graphs. How can we define that paradigm,\nparticularly for graphs,",
    "start": "690050",
    "end": "696140"
  },
  {
    "text": "for data? So here is our plan or the\nsummary of our content.",
    "start": "696140",
    "end": "702800"
  },
  {
    "text": "We will have two major steps\nto define a full deep learning on graph network.",
    "start": "702800",
    "end": "708769"
  },
  {
    "text": "We will first consider the\nlocal network structure, where we'll describe the\naggregation function, which",
    "start": "708770",
    "end": "714980"
  },
  {
    "text": "tells us how we\nsummarize information about the local\nneighborhood of a node. And then we will define the\nconcrete computational graph",
    "start": "714980",
    "end": "721730"
  },
  {
    "text": "to execute the computation. And having defined\nthis setup, we",
    "start": "721730",
    "end": "727279"
  },
  {
    "text": "will specify a concrete\ncomputable architecture, which involves multiple layers.",
    "start": "727280",
    "end": "733760"
  },
  {
    "text": "And also, we will talk about\nhow do we train this model and how find some examples about\nunsupervised and supervised",
    "start": "733760",
    "end": "739880"
  },
  {
    "text": "learning on this deep\ngraph learning network. So let's begin with the setup.",
    "start": "739880",
    "end": "747700"
  },
  {
    "text": "We assume that we are provided\nwith a graph as the input to this deep graph\nlearning model.",
    "start": "747700",
    "end": "754240"
  },
  {
    "text": "Graph can be defined as follow. We have V as the vertex set\nand A as the adjacency matrix.",
    "start": "754240",
    "end": "760210"
  },
  {
    "text": "We assume adjacency\nmatrix usually binary. And X is the matrix\nof node feature.",
    "start": "760210",
    "end": "767470"
  },
  {
    "text": "We have the\ncardinality of v, which is the number of nodes times\nd as the two dimensions.",
    "start": "767470",
    "end": "774579"
  },
  {
    "text": "D is the feature\ndimension of the input. We represent a-- lowercase v\nis a node in the node or vertex",
    "start": "774580",
    "end": "783670"
  },
  {
    "text": "set. And the neighborhood\nfunction of v defines the set of\ndirect neighbors of a given node in the graph.",
    "start": "783670",
    "end": "792230"
  },
  {
    "text": "A very useful attribute is we\nhave node features associated",
    "start": "792230",
    "end": "797329"
  },
  {
    "text": "with nodes. So we don't just plainly\nhave nodes in the network. We also associate a lot of\nmeaningful features for a given",
    "start": "797330",
    "end": "803959"
  },
  {
    "text": "node. For example, in\nsocial network, we can associate user profile,\nimages of a product,",
    "start": "803960",
    "end": "809510"
  },
  {
    "text": "those kind of attribute\nassociated with nodes. And for biological\nnetworks, we can use gene expression profiles and\nsome gene function information,",
    "start": "809510",
    "end": "819180"
  },
  {
    "text": "et cetera, a lot of\nuseful attributes as well. And you can also\nimagine, there are",
    "start": "819180",
    "end": "825208"
  },
  {
    "text": "cases where we don't have node\nfeature in the graph data set. Either they are missing or we\nsimply don't know about them.",
    "start": "825208",
    "end": "830700"
  },
  {
    "text": "So there are also ways to\ndeal with that kind of failure cases. In that way, either we can\nuse an indicator vector.",
    "start": "830700",
    "end": "837920"
  },
  {
    "text": "For example, one hard\nencoding of a node. So suppose, we have a\nfive node in the graph, we can have a five-dimensional\none hard encoding",
    "start": "837920",
    "end": "845270"
  },
  {
    "text": "label the nodes as one,\ntwo, three, four, five. That way, we can get\nsome initialization or initial feature encoding.",
    "start": "845270",
    "end": "851780"
  },
  {
    "text": "We can also consider using\nconstant as node feature. In that case, we will\njust use a constant 1",
    "start": "851780",
    "end": "857840"
  },
  {
    "text": "for all the nodes\nin the network, just showing that all the\nnodes are treated equally and they don't really carry too\nmuch additional information.",
    "start": "857840",
    "end": "865490"
  },
  {
    "text": "So these are the\nways suppose you don't have node feature\nin the graph data set.",
    "start": "865490",
    "end": "868927"
  },
  {
    "text": "So after introducing this-- question? Can you go back one slide.",
    "start": "872460",
    "end": "877600"
  },
  {
    "text": "So in the case of\nvector constant, how do you decide\nwhat the dimension should be for the [INAUDIBLE].",
    "start": "877600",
    "end": "883590"
  },
  {
    "text": "Yeah, good question. So the question is suppose,\nwe don't know node feature and we want to impute,\nwe want to insert",
    "start": "883590",
    "end": "891820"
  },
  {
    "text": "this like a constant vector\nhere, what is dimension? I think essentially, having\na one-dimensional [INAUDIBLE]",
    "start": "891820",
    "end": "898290"
  },
  {
    "text": "scalar is sufficient here\nbecause it really doesn't carry much meaning here. And the only purpose we\nwant to have this feature",
    "start": "898290",
    "end": "906450"
  },
  {
    "text": "is that it can be easily\nprocessed by a neural network. And a constant 1 means that all\nthe nodes are treated equally.",
    "start": "906450",
    "end": "914370"
  },
  {
    "text": "So you can imagine, you\ncan assign different value to nodes. But that kind of carry some\nadditional prior information.",
    "start": "914370",
    "end": "920680"
  },
  {
    "text": "So let's say, like\ngiven a network, we can assign A and B\nwith different values.",
    "start": "920680",
    "end": "926890"
  },
  {
    "text": "So that basically means\nyou could differentiate this two nodes in the network. So this like\ninitialization means",
    "start": "926890",
    "end": "935170"
  },
  {
    "text": "how you will like treat or value\ndifferent nodes in the graph.",
    "start": "935170",
    "end": "940620"
  },
  {
    "text": "Question? So how do we incorporate edge\nfeatures in the training?",
    "start": "940620",
    "end": "946050"
  },
  {
    "text": "Yeah, great question. So in this setup, we kind\nof simplified the case, where we do not\nexplicitly edge features.",
    "start": "946050",
    "end": "953980"
  },
  {
    "text": "So we say, we assume, the\nadjacency matrix is binary. But you can also imagine, there\nare meaningful edge features.",
    "start": "953980",
    "end": "960630"
  },
  {
    "text": "For example, we have user\nitem interaction graph. User may click item. They may buy, purchase an item.",
    "start": "960630",
    "end": "967373"
  },
  {
    "text": "So there are\ndifferent edge types. In that case, we can expand\nthe adjacency matrix. So rather than making\nit to be binary,",
    "start": "967373",
    "end": "974410"
  },
  {
    "text": "we can make it like\na more meaningful. For example, it can be\nlike a categorical value,",
    "start": "974410",
    "end": "979649"
  },
  {
    "text": "where different values means\ndifferent type of edges. So that's one way you can\nextend to edge attributes.",
    "start": "979650",
    "end": "987470"
  },
  {
    "text": "OK. Any more question? OK.",
    "start": "987470",
    "end": "993050"
  },
  {
    "text": "So this is like the setup\nfor graph deep learning. And let's see.",
    "start": "993050",
    "end": "998390"
  },
  {
    "text": "Suppose we don't learn-- we haven't learned this lecture. And what you may consider\nto learn from a graph using",
    "start": "998390",
    "end": "1005709"
  },
  {
    "text": "deep learning? Well, the most straightforward\nway to consider is because we have\nan adjacency matrix,",
    "start": "1005710",
    "end": "1013220"
  },
  {
    "text": "we have a node feature matrix. So it will be natural that\nwe can concatenate this two",
    "start": "1013220",
    "end": "1018279"
  },
  {
    "text": "matrices together. And then we can apply the\nsimplest deep learning method, which is MLP,\nwhich is introduced.",
    "start": "1018280",
    "end": "1025359"
  },
  {
    "text": "And we throw this like a\nconcatenate matrix of a graph",
    "start": "1025359",
    "end": "1030430"
  },
  {
    "text": "into an MLP, and see\nhow it will predict. So this pipeline\ncan be implemented.",
    "start": "1030430",
    "end": "1036459"
  },
  {
    "text": "And it may work\nin certain cases. But there are some\nnotable issues.",
    "start": "1036460",
    "end": "1042079"
  },
  {
    "text": "First is that this approach\nis not really scalable because you can see\nthe input dimension is",
    "start": "1042079",
    "end": "1047619"
  },
  {
    "text": "to the order of number of nodes. And we also have an additional\nfeature dimension d.",
    "start": "1047619",
    "end": "1054679"
  },
  {
    "text": "So the complexity here is\nthe order of V times d.",
    "start": "1054680",
    "end": "1060380"
  },
  {
    "text": "And also, this kind\nof approach is not really applicable to graphs\nwith different sizes.",
    "start": "1060380",
    "end": "1065510"
  },
  {
    "text": "Because the input\ndimension will vary based on the number of nodes. So if you train a network\non five node graph,",
    "start": "1065510",
    "end": "1072110"
  },
  {
    "text": "and you couldn't really\napply the same network to a six-node graph. So this also kind of limiting.",
    "start": "1072110",
    "end": "1078080"
  },
  {
    "text": "And finally, that\napproach is very sensitive to node ordering. So suppose, we have\nthe same network,",
    "start": "1078080",
    "end": "1084169"
  },
  {
    "text": "but we reorder the nodes. So instead of calling it A to\nE, we may say, call it E to A.",
    "start": "1084170",
    "end": "1090830"
  },
  {
    "text": "And all the values here\nwill be shuffled as well. And you can see in that case,\nthe output will be different.",
    "start": "1090830",
    "end": "1098040"
  },
  {
    "text": "So these are kind of limitations\ntell us we couldn't directly define a graph\nlearning method based",
    "start": "1098040",
    "end": "1104509"
  },
  {
    "text": "on the standard\ndeep learning block, rather that we need to define a\nspecific architecture for graph",
    "start": "1104510",
    "end": "1109950"
  },
  {
    "text": "data. Another thing we may\nconsider in our toolbox",
    "start": "1109950",
    "end": "1116010"
  },
  {
    "text": "is convolutional neural\nnetwork because you see, for convolutional\nnetworks, you can easily",
    "start": "1116010",
    "end": "1121020"
  },
  {
    "text": "process imagery data. And in fact, all the\nlecture materials, we visualize a\ngraph as an image.",
    "start": "1121020",
    "end": "1128170"
  },
  {
    "text": "So it will be naturally to\nsee that we can-- like a pad or like a downsample the\ngraph into the 2D space,",
    "start": "1128170",
    "end": "1135059"
  },
  {
    "text": "and then use CNN to\nprocess the graph data. And actually, in the early\ndays, in deep learning,",
    "start": "1135060",
    "end": "1140160"
  },
  {
    "text": "researchers have done that. And it actually\nworks in some cases. For example, molecule\nclassification.",
    "start": "1140160",
    "end": "1146250"
  },
  {
    "text": "Those kind of tasks, where\ngraphs are relatively simple. But the issue is\nthe network doesn't",
    "start": "1146250",
    "end": "1154890"
  },
  {
    "text": "have the regular\nstructure as images. So you can see\nthat network could be very complex in structure.",
    "start": "1154890",
    "end": "1161100"
  },
  {
    "text": "And most notably, it doesn't\nhave the fixed notion of locality in this case.",
    "start": "1161100",
    "end": "1166480"
  },
  {
    "text": "So what I mean is\nthat, for example here you may think these two\nnodes are close together,",
    "start": "1166480",
    "end": "1171870"
  },
  {
    "text": "or these two nodes\nare close together. But in reality, maybe\nthe nodes are far away.",
    "start": "1171870",
    "end": "1177340"
  },
  {
    "text": "They are also very close\ntogether with the node here because they are connected. So you have tons of options\nabout how to visualize",
    "start": "1177340",
    "end": "1185230"
  },
  {
    "text": "a network into an image. So that gives you\na lot of freedom, but also prevent a model\nto meaningfully learn",
    "start": "1185230",
    "end": "1191650"
  },
  {
    "text": "from network. And also, graph is\npermutation invariant because graph is an\nunordered object.",
    "start": "1191650",
    "end": "1198970"
  },
  {
    "text": "So even though you have this\nconvolutional filter patched here, you may arbitrarily\nshuffle the order of the node.",
    "start": "1198970",
    "end": "1206169"
  },
  {
    "text": "And the meaning doesn't\nchange, but the CNN network will treat it differently. So this is kind of like\na thought experiment",
    "start": "1206170",
    "end": "1213670"
  },
  {
    "text": "that shows why we couldn't\ndirectly apply CNN on graphs.",
    "start": "1213670",
    "end": "1217210"
  },
  {
    "text": "So let's discuss what are\nthe properties we really need for deep learning\nmethods for graph.",
    "start": "1220070",
    "end": "1225830"
  },
  {
    "text": "Let's start with\nthe first property. That is so-called\npermutation invariance. The idea of\npermutation invariance",
    "start": "1225830",
    "end": "1231880"
  },
  {
    "text": "is that we know the graph does\nnot have a canonical ordering. And we can have arbitrary order\nplans for a given network.",
    "start": "1231880",
    "end": "1240040"
  },
  {
    "text": "Here's a concrete example. We can have this network\nas a running example.",
    "start": "1240040",
    "end": "1246710"
  },
  {
    "text": "We define a specific\nnode ordering here. And we can define the\ncorresponding node features",
    "start": "1246710",
    "end": "1252640"
  },
  {
    "text": "and the adjacency matrix. Basically, describe how\nthis graph looks like.",
    "start": "1252640",
    "end": "1257980"
  },
  {
    "text": "And in the meantime, we can\nshuffle the order of the nodes here. So we can come up with a\nsecond way to order the graph.",
    "start": "1257980",
    "end": "1265180"
  },
  {
    "text": "And you see that after this\nshuffling or permutation, the node feature matrix\nand the adjacency matrix,",
    "start": "1265180",
    "end": "1271300"
  },
  {
    "text": "they get different. And our idea is that\nbecause we know these two",
    "start": "1271300",
    "end": "1277429"
  },
  {
    "text": "older plants describe the same\nnetwork, what we wish to say is that the graph and node\nrepresentations for these two",
    "start": "1277430",
    "end": "1284450"
  },
  {
    "text": "order plans, but for the same\nnetwork, they should be same. They should be identical. So this is kind of\nour goal when we",
    "start": "1284450",
    "end": "1291350"
  },
  {
    "text": "want to define a deep\nlearning method for graphs.",
    "start": "1291350",
    "end": "1294065"
  },
  {
    "text": "So let's describe this\npermutation invariance property more rigorously. So what do we mean by the graph\naugmentation are the same?",
    "start": "1297800",
    "end": "1307309"
  },
  {
    "text": "So we can define like\na projection function or embedding function f that\ntakes the adjacency matrix, A,",
    "start": "1307310",
    "end": "1314260"
  },
  {
    "text": "and the node feature matrix, X. And what do we say is that we\ncan have two different order",
    "start": "1314260",
    "end": "1321039"
  },
  {
    "text": "plan for the network,\nA1X1 and A2X2. And suppose we take this\ninput to the same encoding",
    "start": "1321040",
    "end": "1328990"
  },
  {
    "text": "function f, we want the\noutput to be the same. So that basically\nsays, suppose, we",
    "start": "1328990",
    "end": "1334150"
  },
  {
    "text": "have two ordering\nof a same network, we want a encoding function f,\nwhich is, say, a graph neural",
    "start": "1334150",
    "end": "1340780"
  },
  {
    "text": "network, should always generate\nthe same representation for the same graph.",
    "start": "1340780",
    "end": "1346000"
  },
  {
    "text": "This is like a visually say like\nthe two order plans, the output f, should be the same.",
    "start": "1346000",
    "end": "1350529"
  },
  {
    "text": "And we also formally define it\nlike here, that like basically,",
    "start": "1354380",
    "end": "1361420"
  },
  {
    "text": "for permutation environments,\nwe not only for two ordering, like order plan A\nand order plan B.",
    "start": "1361420",
    "end": "1367495"
  },
  {
    "text": "We should consider any potential\nlike order plan, i and j. And we formally say that suppose\nthis permutation-invariant",
    "start": "1367495",
    "end": "1374810"
  },
  {
    "text": "then for any pair of i and j,\nthe output should be the same.",
    "start": "1374810",
    "end": "1380150"
  },
  {
    "text": "We can also view this\nas in the matrix form. So we define a\ngraph function that",
    "start": "1380150",
    "end": "1387549"
  },
  {
    "text": "essentially map a\nnode feature matrix R into the order of a V times m.",
    "start": "1387550",
    "end": "1395800"
  },
  {
    "text": "And the adjacency matrix\nto a vector space. And we say this function\nf is permutation-invariant",
    "start": "1395800",
    "end": "1403720"
  },
  {
    "text": "if we wrote in the matrix\nform A and X as the input. And we permute the\nadjacency matrix",
    "start": "1403720",
    "end": "1410230"
  },
  {
    "text": "and permute the feature matrix. And the output are the\nsame for any permutation P,",
    "start": "1410230",
    "end": "1417050"
  },
  {
    "text": "then we say this function\nis permutation-invariant. So the same thing, we can\nexpress it in different ways.",
    "start": "1417050",
    "end": "1422930"
  },
  {
    "text": "But this is a kind of definition\nof permutation invariance.",
    "start": "1422930",
    "end": "1426320"
  },
  {
    "text": "Another concept here is\npermutation equivariance. So here is a different\nterm with invariance.",
    "start": "1428970",
    "end": "1436140"
  },
  {
    "text": "We talk about the\nidea of equivariance. So what does it mean? We talk about we\ncan encode a graph",
    "start": "1436140",
    "end": "1443760"
  },
  {
    "text": "into a graph\nrepresentation, where we have one vector for the graph. Now, we are considering\nthe representation",
    "start": "1443760",
    "end": "1449310"
  },
  {
    "text": "in the node level. So instead of having\none vector, we will have one vector for\neach node in the graph.",
    "start": "1449310",
    "end": "1454769"
  },
  {
    "text": "And in the end, we will\nhave an embedding or output matrix for the network.",
    "start": "1454770",
    "end": "1460300"
  },
  {
    "text": "So now the output,\ninstead of being a vector, it will be a matrix. So what is the nice property\nwe wish to see here?",
    "start": "1460300",
    "end": "1467740"
  },
  {
    "text": "Well, we've got the\nsame network and we permute the order of the nodes.",
    "start": "1467740",
    "end": "1473160"
  },
  {
    "text": "Therefore, we would like to see\nthat the values or the encoded values for a different nodes,\nthey should be the same.",
    "start": "1473160",
    "end": "1480370"
  },
  {
    "text": "But the order should be\naccording to the position of the node in the network.",
    "start": "1480370",
    "end": "1485700"
  },
  {
    "text": "Here is a illustrative example. So let's say we care\nabout this yellow node.",
    "start": "1485700",
    "end": "1492870"
  },
  {
    "text": "And in the first order plan,\nthe yellow node is labeled as A. And it appeared here.",
    "start": "1492870",
    "end": "1498600"
  },
  {
    "text": "And our hope is\nthat because we only shuffle the order of the\nnode, so the encoding",
    "start": "1498600",
    "end": "1503730"
  },
  {
    "text": "of the yellow node\nshould be the same. And it should appear\nin the latest encoding of the second order plan E.",
    "start": "1503730",
    "end": "1511050"
  },
  {
    "text": "So it says that the encoding\nfor node should stay the same. And it's only based on\nhow the node is labeled.",
    "start": "1511050",
    "end": "1517679"
  },
  {
    "text": "It should be always\nassociated with the node label A and node label E. And the\nsame is for all the other nodes",
    "start": "1517680",
    "end": "1524370"
  },
  {
    "text": "in the network. Let's see another example. For example, for the\ngreen node is here. It is labeled as C. And here,\nit is labeled as D. And suppose,",
    "start": "1524370",
    "end": "1534059"
  },
  {
    "text": "we have this nice\nequivariance property. Then after we shuffle\nthe node ordering, the value should stay the same.",
    "start": "1534060",
    "end": "1540160"
  },
  {
    "text": "And it is closely tied\nto the label of the node.",
    "start": "1540160",
    "end": "1543360"
  },
  {
    "text": "We can also formally define\nwhat is permission equivariant. That basically said,\nif the output vector",
    "start": "1546670",
    "end": "1553120"
  },
  {
    "text": "of a node at the same\nposition in the graph remains unchanged\nfor any permutation.",
    "start": "1553120",
    "end": "1558340"
  },
  {
    "text": "And we say this function\nis permutation equivariant. And we can also see\nthis in the matrix form.",
    "start": "1558340",
    "end": "1565400"
  },
  {
    "text": "So now, we define, instead\nof a graph function, we define a node function that\nencode a network into a matrix",
    "start": "1565400",
    "end": "1572200"
  },
  {
    "text": "instead of a single vector. And this matrix is\npermutation equivariant.",
    "start": "1572200",
    "end": "1578030"
  },
  {
    "text": "Sorry. This function is\npermutation equivariant if we permute the input\nof adjacency matrix",
    "start": "1578030",
    "end": "1585430"
  },
  {
    "text": "of node feature matrix. And the output matrix is\nalso permuted accordingly with the permutation\nmatrix P. In that case,",
    "start": "1585430",
    "end": "1593780"
  },
  {
    "text": "we say this function is\npermutation equivariance.",
    "start": "1593780",
    "end": "1596170"
  },
  {
    "text": "So we talk about\nthese two properties. It would be nice to review\nthem and compare these two properties. So here, I show the definition,\nthe matrix definition",
    "start": "1598980",
    "end": "1608480"
  },
  {
    "text": "of invariant and equivariant. In plain language, what does\nit really mean is that--",
    "start": "1608480",
    "end": "1616520"
  },
  {
    "text": "so suppose we permute the\ninput, then the output will always stay the same.",
    "start": "1616520",
    "end": "1621930"
  },
  {
    "text": "This is for\npermutation invariant. And the output here\nis usually a vector. So we assign one\nvector for one graph",
    "start": "1621930",
    "end": "1629360"
  },
  {
    "text": "and we can also define a\nplain language definition for equivariant, where\nif we permute the input,",
    "start": "1629360",
    "end": "1635840"
  },
  {
    "text": "the output will also get\npermuted accordingly. And in that case,\nthis is usually",
    "start": "1635840",
    "end": "1643040"
  },
  {
    "text": "considered in the case, where\nwe map a graph into a matrix. And the matrix talk about\nthe embedding vector",
    "start": "1643040",
    "end": "1648260"
  },
  {
    "text": "for each of the node. And here are three\nconcrete examples of what functions are\ninvariant and what",
    "start": "1648260",
    "end": "1654750"
  },
  {
    "text": "functions are equivariant. The first example is this\nlike one vector transpose",
    "start": "1654750",
    "end": "1660510"
  },
  {
    "text": "times X is basically a\nsummation over a matrix over different rows.",
    "start": "1660510",
    "end": "1665549"
  },
  {
    "text": "And in that case,\nyou can easily see the function is\npermutation invariant because it is summation.",
    "start": "1665550",
    "end": "1670770"
  },
  {
    "text": "So no matter how you\npermute the input X, the output will stay the same.",
    "start": "1670770",
    "end": "1677190"
  },
  {
    "text": "Here's another example, which is\npermutation equivariant, which is pretty simple. So we just always\noutput the node,",
    "start": "1677190",
    "end": "1683880"
  },
  {
    "text": "input X. This is basically the\nshallow encoder that we see.",
    "start": "1683880",
    "end": "1689460"
  },
  {
    "text": "We have an encoder matrix,\nand essentially is the output. And it's easy to\nsee this equivariant",
    "start": "1689460",
    "end": "1694950"
  },
  {
    "text": "because if you permute\nthe input, of course, the output get\npermuted equivalently.",
    "start": "1694950",
    "end": "1700110"
  },
  {
    "text": "And finally, we have\nexample of this A times X as a permutation\nequivariant function.",
    "start": "1700110",
    "end": "1705580"
  },
  {
    "text": "And here, we use the property of\nsuppose you transpose a matrix",
    "start": "1705580",
    "end": "1710590"
  },
  {
    "text": "and then transpose it with it-- and then times\nwith its transpose, it will cancel out into\nan identity function.",
    "start": "1710590",
    "end": "1716920"
  },
  {
    "text": "And you see this function is\nalso permutation equivariant. And this idea of A times X\nis like a simplest version",
    "start": "1716920",
    "end": "1725559"
  },
  {
    "text": "of a graph learning or\ngraph message aggregation. And we we'll talk about\nthis later as well.",
    "start": "1725560",
    "end": "1733390"
  },
  {
    "text": "So any question here? I know the concept\nof a bit dense here.",
    "start": "1733390",
    "end": "1738930"
  },
  {
    "text": "What is P exactly? Yeah. So we have a--",
    "start": "1738930",
    "end": "1744212"
  },
  {
    "text": "It's like permutation. What does it look like? Yeah. We didn't have a\nvisualizer here,",
    "start": "1744213",
    "end": "1750320"
  },
  {
    "text": "but it's really like\na binary matrix. It has 0, has 1's. And it basically said how\nwe shuffle different columns",
    "start": "1750320",
    "end": "1759289"
  },
  {
    "text": "of a network-- of a matrix, sorry.",
    "start": "1759290",
    "end": "1761655"
  },
  {
    "text": "Let's see. Suppose, we have a\nmatrix of six rows,",
    "start": "1764540",
    "end": "1771680"
  },
  {
    "text": "then the permutation\nmatrix will also be 6 by 6. And different\nlocations of 1 and 0",
    "start": "1771680",
    "end": "1777350"
  },
  {
    "text": "says where we should put\nthe input A to the output.",
    "start": "1777350",
    "end": "1782520"
  },
  {
    "text": "So let's say if we\nhave like a 0110,",
    "start": "1782520",
    "end": "1787790"
  },
  {
    "text": "then that means we\nwill shuffle A and B. So like this is the idea\nof permutation matrix. And you can also search on\nthe web for [INAUDIBLE]..",
    "start": "1787790",
    "end": "1796820"
  },
  {
    "text": "Any other questions?",
    "start": "1796820",
    "end": "1798529"
  },
  {
    "text": "OK. We can continue. So why do we want to study\nthese two properties,",
    "start": "1802840",
    "end": "1808250"
  },
  {
    "text": "invariance and equivariance? That is because these\nproperties are pretty important, and essentially,\nthe key building",
    "start": "1808250",
    "end": "1814640"
  },
  {
    "text": "blocks for graph\nlearning method. So I show an example of\ngraph learning method here.",
    "start": "1814640",
    "end": "1820880"
  },
  {
    "text": "And you can see that it usually\ninvolves multiple permutation equivalent layers and\npermutation invariant layers.",
    "start": "1820880",
    "end": "1827390"
  },
  {
    "text": "And this is kind of how we\nmake sure graph deep learning method can faithfully\nrepresent the graph",
    "start": "1827390",
    "end": "1834049"
  },
  {
    "text": "rather than like a\nmess it up like an MLP that we introduced earlier.",
    "start": "1834050",
    "end": "1838909"
  },
  {
    "text": "And here is like a visual\nexample like why MLP doesn't satisfy this property.",
    "start": "1842240",
    "end": "1848050"
  },
  {
    "text": "So suppose we have\nan MLP and then we have this vector as input.",
    "start": "1848050",
    "end": "1853510"
  },
  {
    "text": "And we permute the\nvalues of this vector. You see.",
    "start": "1853510",
    "end": "1858580"
  },
  {
    "text": "And after throwing\nit into an MLP, then it is clear that the\noutput will be different",
    "start": "1858580",
    "end": "1864280"
  },
  {
    "text": "because it doesn't really have\nthe way to say aggregate it",
    "start": "1864280",
    "end": "1869620"
  },
  {
    "text": "in a permutation invariant way. So the output will\nalways be different.",
    "start": "1869620",
    "end": "1874860"
  },
  {
    "text": "And this is precisely\nwhy this naive approach to try to encode a\nnetwork would fail.",
    "start": "1874860",
    "end": "1881970"
  },
  {
    "text": "Because it doesn't have this\nnice property of permutation invariant.",
    "start": "1881970",
    "end": "1885210"
  },
  {
    "text": "Next, I'm going to talk\nabout how do we specifically define a graph neural network\nwith these two properties.",
    "start": "1889160",
    "end": "1895320"
  },
  {
    "text": "And in particular, we'll talk\nabout passsing and message information on the neighbors.",
    "start": "1895320",
    "end": "1900720"
  },
  {
    "text": "Question? Sorry. Can you go to the\nprevious slide? So I mean, it feels like this\nis a problem that would come up for like trying to\ncreate a neural network",
    "start": "1900720",
    "end": "1907230"
  },
  {
    "text": "for any kind of\nunstructured data, where ordering is not important. So like this graph neural\nnetworks the first time",
    "start": "1907230",
    "end": "1912572"
  },
  {
    "text": "it came up or like\nwas it seen before? Yeah, that's a good question. So the question is about can we\nuse the graph learning method",
    "start": "1912572",
    "end": "1920390"
  },
  {
    "text": "to encode other\nunordered objects and where this idea\noriginated from.",
    "start": "1920390",
    "end": "1926750"
  },
  {
    "text": "And yeah, I think the idea\nof graph learning methods, as we also dive into\ndeep later, is inspired",
    "start": "1926750",
    "end": "1933470"
  },
  {
    "text": "from learning from a set. So essentially, a\nneighbor, the set",
    "start": "1933470",
    "end": "1939230"
  },
  {
    "text": "of neighbors of a\ngiven node is a set. And suppose we have a\nmessage to learn from set,",
    "start": "1939230",
    "end": "1944630"
  },
  {
    "text": "essentially, we can build like a\ncomplete graph learning method. So definitely also have roots\nin other areas of deep learning.",
    "start": "1944630",
    "end": "1952995"
  },
  {
    "text": "OK. Let's continue with\nour discussion. So so far, we have talked\nabout the motivation",
    "start": "1956700",
    "end": "1963270"
  },
  {
    "text": "of graph deep learning and\nthe required properties that we wish to see in the\ngraph deep learning method.",
    "start": "1963270",
    "end": "1969850"
  },
  {
    "text": "Next, I'm going to talk\nabout concrete example, that is so-called graph\nconvolutional networks. And this is arguably\nthe most popular way",
    "start": "1969850",
    "end": "1977310"
  },
  {
    "text": "to encode a graph and\nthe most popular way to implement a graph\nneural network.",
    "start": "1977310",
    "end": "1982875"
  },
  {
    "text": "So here is the idea of a graph\nconvolutional neural network. Our idea is to encode a node\nbased on its neighborhood",
    "start": "1985670",
    "end": "1992200"
  },
  {
    "text": "structure. And we follow a\ntwo-step process. Suppose, we want to encode\na node I in the network,",
    "start": "1992200",
    "end": "1998740"
  },
  {
    "text": "we will first define\nits computational graph based on the nodes local\nneighborhood structure.",
    "start": "1998740",
    "end": "2004015"
  },
  {
    "text": "So we will define it based on\nits one [? hop ?] neighbors and the neighbors of\nneighbors and so on. Suppose we have multiple layers.",
    "start": "2004015",
    "end": "2010500"
  },
  {
    "text": "And then having defined\nthis computational graph we will propagate and\ntransform information based on the defined\ncomputational graph.",
    "start": "2010500",
    "end": "2017940"
  },
  {
    "text": "And the key concept here is that\nhow we can learn and propagate information across\nthe network defined",
    "start": "2017940",
    "end": "2024750"
  },
  {
    "text": "by the node\nneighborhood structure. So we will introduce the idea\nthrough a running example.",
    "start": "2024750",
    "end": "2032960"
  },
  {
    "text": "This is an input\ngraph and our goal is to encode this yellow node\nA. Based on the idea I just",
    "start": "2032960",
    "end": "2039590"
  },
  {
    "text": "described, we will enroll the\ncomputational graph by finding the A's direct neighbors in\nthe network, B, C, and D.",
    "start": "2039590",
    "end": "2049310"
  },
  {
    "text": "And then we will iteratively\nenroll the neighbor structure for each of its neighbors.",
    "start": "2049310",
    "end": "2055020"
  },
  {
    "text": "So for node B, we will see its\nneighbors A and C. For node C, we see its neighbors, and so on.",
    "start": "2055020",
    "end": "2061080"
  },
  {
    "text": "So you can imagine this\nunrolling can happen iteratively until you\nreach the number of rounds",
    "start": "2061080",
    "end": "2067199"
  },
  {
    "text": "you wish to see. That is the number of\nlayers in the network. And another thing to note that\na node can appear multiple times",
    "start": "2067199",
    "end": "2075079"
  },
  {
    "text": "in this computational graph. What do I mean that you can\nsee node A. It will also appear",
    "start": "2075080",
    "end": "2081079"
  },
  {
    "text": "here, like an A three times. The interpretation here is\nthat node A is a two hop",
    "start": "2081080",
    "end": "2088408"
  },
  {
    "text": "neighbor of itself. So if you look two hop\naway for a given node,",
    "start": "2088409",
    "end": "2093540"
  },
  {
    "text": "you will encounter\nthe node itself again. So it is allowed that we can\nhave this duplicate value in the computational graph.",
    "start": "2093540",
    "end": "2099525"
  },
  {
    "text": "So having defined the\narchitecture of the computation how do we really implement?",
    "start": "2102540",
    "end": "2108590"
  },
  {
    "text": "We have these different\ngray boxes, which are essentially neural network.",
    "start": "2108590",
    "end": "2113780"
  },
  {
    "text": "What does this neuron\nhave to do is to compress or condense the information\nabout a set of different nodes.",
    "start": "2113780",
    "end": "2121100"
  },
  {
    "text": "And then we will generate\nthe new representation and then compress it again.",
    "start": "2121100",
    "end": "2127500"
  },
  {
    "text": "So this is like what a neural\nnetwork will help for graph learning method in GN.",
    "start": "2127500",
    "end": "2133900"
  },
  {
    "text": "And we can repeat this process\nand define the computation graph for each node\nin the network. So we'll talk about the\nyellow node, but you see,",
    "start": "2136560",
    "end": "2143550"
  },
  {
    "text": "we can define a\ncomputation graph for each node in the graph. And the intuition here\nis that different nodes",
    "start": "2143550",
    "end": "2151025"
  },
  {
    "text": "will have a\ndifferent computation graph because their\nlocal neighborhood structure are different. And this is precisely how\ngraph learning methods",
    "start": "2151025",
    "end": "2159390"
  },
  {
    "text": "can differentiate\nnodes in the network. Because they have different\nneighborhood structure,",
    "start": "2159390",
    "end": "2165480"
  },
  {
    "text": "we will define different\ncomputational graph. And therefore, the embedding\nwe generate for different nodes",
    "start": "2165480",
    "end": "2171810"
  },
  {
    "text": "are also different. Question? Is it true the\nstatement that let's",
    "start": "2171810",
    "end": "2177690"
  },
  {
    "text": "say, for a problem\nof node embedding, that the higher the degree\nis, the more complicated",
    "start": "2177690",
    "end": "2185102"
  },
  {
    "text": "the embedding would be? Meaning that nodes\nthat are not-- that don't have a lot of neighbors\nare these more easily to embed.",
    "start": "2185102",
    "end": "2191400"
  },
  {
    "text": "Because as we see\nhere, the embedding basically scales with\nthe neighbors, right? So if there's one neighbor,\nit's an easy embedding.",
    "start": "2191400",
    "end": "2196660"
  },
  {
    "text": "If we have 1,000 neighbors,\nit's like to embed all that information\nto one node is a lot. Yeah, that's a great question.",
    "start": "2196660",
    "end": "2203480"
  },
  {
    "text": "I think your question is how\nabout like a node with fewer neighbors, easier to encode.",
    "start": "2203480",
    "end": "2209290"
  },
  {
    "text": "Well, I think\nrigorously, we need to define what is easy mean. But I think, intuitively, like\na node with fewer neighbors,",
    "start": "2209290",
    "end": "2218200"
  },
  {
    "text": "it takes like a\nfewer information. And it will be more\neasily to be compressed. And there's actually\na frontier of GN",
    "start": "2218200",
    "end": "2225520"
  },
  {
    "text": "that describes this\nproblem called-- so-called over\nsquashing problem. Because you can see\nthat for GN, it's",
    "start": "2225520",
    "end": "2232540"
  },
  {
    "text": "always talk about how to\nscratch information together. And that problem basically said,\nsuppose, a network is very deep",
    "start": "2232540",
    "end": "2241150"
  },
  {
    "text": "and the degree of a\nnode is very high. Like the network\nmay tend to over scratch like this\ninformation of a network.",
    "start": "2241150",
    "end": "2248619"
  },
  {
    "text": "And the intuition here\nis correct in the sense, that if a node has\nfewer neighbors, then",
    "start": "2248620",
    "end": "2254170"
  },
  {
    "text": "it's easier to\nget the embedding.",
    "start": "2254170",
    "end": "2257170"
  },
  {
    "text": "OK. Let's continue. And another concept here\nis the number of layers.",
    "start": "2261040",
    "end": "2267589"
  },
  {
    "text": "In my example here, we have a\ntwo-layer graph neural network, which basically said we look\nat the neighbors, and then",
    "start": "2267590",
    "end": "2275525"
  },
  {
    "text": "the neighbors of neighbors. And this concept is\nquite important for GN because the number\nof layers of GN",
    "start": "2275525",
    "end": "2282290"
  },
  {
    "text": "really tells about how far\naway we look in the network. So given a two-layer\nGN, we will usually",
    "start": "2282290",
    "end": "2288500"
  },
  {
    "text": "be able to see like the\nnodes that are two hops away. So this is a kind of difference\nwith other deep learning",
    "start": "2288500",
    "end": "2294470"
  },
  {
    "text": "architectures. Say, in convolutional neural\nnetwork, the number of layers is usually just\na hyperparameter.",
    "start": "2294470",
    "end": "2300410"
  },
  {
    "text": "You can pick whenever the\nmodel is performing the best. But for GN, it has a\nconcrete physical meaning.",
    "start": "2300410",
    "end": "2307700"
  },
  {
    "text": "The number of GN layers\ntells us how far away you can see in the network.",
    "start": "2307700",
    "end": "2312750"
  },
  {
    "text": "And usually, we will\npick the number of layers approximately equal to\nthe diameter of the graph.",
    "start": "2312750",
    "end": "2318780"
  },
  {
    "text": "So in this graph, we\nhave dimension two, which means that two nodes can,\nat most, have a distance of two",
    "start": "2318780",
    "end": "2326280"
  },
  {
    "text": "for any pair of\nnodes in the network. So in this case, usually,\nhaving a two-layer GN",
    "start": "2326280",
    "end": "2332190"
  },
  {
    "text": "will be sufficient\nfor the network to pick up the essential\ninformation in the network.",
    "start": "2332190",
    "end": "2337480"
  },
  {
    "text": "So setting the--\nlike we can have this kind of more principled\nway to set the number of layers",
    "start": "2337480",
    "end": "2343080"
  },
  {
    "text": "for graph neural network.",
    "start": "2343080",
    "end": "2344265"
  },
  {
    "text": "And we also have the concept of\nneighborhood aggregation, which is done by this gray\nboxes in the example here.",
    "start": "2349020",
    "end": "2358329"
  },
  {
    "text": "So the question is, how do\nwe really define these boxes? The most simple\napproach is to take",
    "start": "2358330",
    "end": "2365520"
  },
  {
    "text": "an average of the information. To be more specific,\nwe can first",
    "start": "2365520",
    "end": "2371040"
  },
  {
    "text": "average the message that we\ncompute for each neighbor of a given node. And then we apply\na neural network",
    "start": "2371040",
    "end": "2378600"
  },
  {
    "text": "that further transform\nthis aggregate or average information. And this two-step\nprocess is essentially",
    "start": "2378600",
    "end": "2385349"
  },
  {
    "text": "what we will later see in\ngraph convolutional network. So here is like\nthe mathematical--",
    "start": "2385350",
    "end": "2393870"
  },
  {
    "text": "there's a question? Yeah. Can you go back\none slide, please?",
    "start": "2393870",
    "end": "2396922"
  },
  {
    "text": "Yeah. So right. Here is good. So if you said, for\nexample, you will end up choosing let's say\nK to neighborhoods",
    "start": "2399760",
    "end": "2405130"
  },
  {
    "text": "or two convolutional layers. But we see all the\nnodes, basically, if we go two hops\naway, we'll end up",
    "start": "2405130",
    "end": "2411849"
  },
  {
    "text": "having all the other nodes\nas a part of their embedding process as inputs. So don't all the\nnodes end up having",
    "start": "2411850",
    "end": "2417550"
  },
  {
    "text": "this very similar\nrepresentations when after two K?",
    "start": "2417550",
    "end": "2422900"
  },
  {
    "text": "Yeah, yeah. Yeah. I think you asked a lot\nof insightful questions. The question is suppose,\nwe enroll the network",
    "start": "2422900",
    "end": "2430030"
  },
  {
    "text": "and we look at the computation\ngraph for each of the nodes, wouldn't in the end all,\nthe nodes look similar?",
    "start": "2430030",
    "end": "2436069"
  },
  {
    "text": "So we talk about this over\nsquashing problem for GN. Another research idea here\nis so-called over smoothing",
    "start": "2436070",
    "end": "2442359"
  },
  {
    "text": "problem, which is precisely\nwhat we described. So suppose, we enroll\nthis process continuously",
    "start": "2442360",
    "end": "2448859"
  },
  {
    "text": "for multiple layers. And in the end,\nall the nodes they have very similar\nneighborhood structure, and which makes them\nindistinguishable.",
    "start": "2448860",
    "end": "2456310"
  },
  {
    "text": "So they all landed with\nthe same embedding. So this is also like\na research question. But usually, the\nway we can get rid",
    "start": "2456310",
    "end": "2463120"
  },
  {
    "text": "of this oversmoothing\nproblem is to do not like set very deep GN.",
    "start": "2463120",
    "end": "2468850"
  },
  {
    "text": "Like we just set\na number of layers equal or approximately the\ndiameter of the network.",
    "start": "2468850",
    "end": "2476349"
  },
  {
    "text": "And in that case, like in\nthe example, we show here, usually, the computational\ngraph of different nodes",
    "start": "2476350",
    "end": "2483010"
  },
  {
    "text": "are largely the same. But suppose, you\ncontinuously enroll and roll multiple layers, you\nmay face the issue",
    "start": "2483010",
    "end": "2490089"
  },
  {
    "text": "that all the nodes may\nlook almost the same. And we couldn't differentiate. So this is a kind of different\nwith like convolutional neural",
    "start": "2490090",
    "end": "2497350"
  },
  {
    "text": "net or ResNet, where you can\nenroll 150 layers for graph. You can enroll 100 layers.",
    "start": "2497350",
    "end": "2503349"
  },
  {
    "text": "And essentially, the\nmodel will learn nothing because it can\ndifferentiate any nodes.",
    "start": "2503350",
    "end": "2507460"
  },
  {
    "text": "OK. So we have a lot\nof visualizations. Now, we can get some concrete\nmathematical definition",
    "start": "2515290",
    "end": "2520720"
  },
  {
    "text": "of what exactly is a graph\nconvolutional network. So our input is\nnode attributes, xv.",
    "start": "2520720",
    "end": "2528880"
  },
  {
    "text": "So that describe a specific\nfeature of a given node. And by the way, to make this\ncomputation more concrete,",
    "start": "2528880",
    "end": "2537475"
  },
  {
    "text": "we consider a case of\nembedding a single node v. So you see all the expression\nhere, we have this subscript v.",
    "start": "2537475",
    "end": "2546310"
  },
  {
    "text": "And the output is this zv. So we have a node\nattribute and we want to generate embedding\nz for this given node v.",
    "start": "2546310",
    "end": "2555310"
  },
  {
    "text": "So how do we start\nthis computation? We will assign this xv to\nthe hv 0, which is basically",
    "start": "2555310",
    "end": "2563890"
  },
  {
    "text": "the zeroth layer or the\ninitialization for the input of a graph neural network.",
    "start": "2563890",
    "end": "2570190"
  },
  {
    "text": "And then we'll repeatedly\napply this transformation. So what this\ntransformation does is",
    "start": "2570190",
    "end": "2577130"
  },
  {
    "text": "it will, first, take\nthe embedding of v, the previous layer k.",
    "start": "2577130",
    "end": "2582829"
  },
  {
    "text": "And then we also\ntake the average of the neighbors'\nprevious layer embedding. So in this transformation,\nwe do two things.",
    "start": "2582830",
    "end": "2590370"
  },
  {
    "text": "We consider how do we\nrepresent the node itself in the previous layer.",
    "start": "2590370",
    "end": "2595670"
  },
  {
    "text": "And after we get\nthat embedding, we will apply a linear\ntransformation to encode or transform\nthat message.",
    "start": "2595670",
    "end": "2603150"
  },
  {
    "text": "The second question we ask\nis, how do we reference the neighbors of a given node? And in that case, we will\nfirst take the average.",
    "start": "2603150",
    "end": "2610130"
  },
  {
    "text": "And we'll normalize by\nthe neighborhood size with a degree of the node.",
    "start": "2610130",
    "end": "2616130"
  },
  {
    "text": "And after we take the\naverage, we, then, apply another transformation\nw, then tells us",
    "start": "2616130",
    "end": "2622490"
  },
  {
    "text": "how to transform\nand further encode the neighbors' information. So the layer is usually\nessentially just two parts.",
    "start": "2622490",
    "end": "2629780"
  },
  {
    "text": "How do we encode\nthe node itself? And how do we encode\nthe nodes neighbors?",
    "start": "2629780",
    "end": "2635270"
  },
  {
    "text": "And after having this\nencoding, because it's a fully linear function, if\nyou stack linear function,",
    "start": "2635270",
    "end": "2641090"
  },
  {
    "text": "again and again,\nit's still linear. We have to insert some\nnon-linearity here. The most popular\nchoice is relu which",
    "start": "2641090",
    "end": "2648470"
  },
  {
    "text": "is also popular in other\ndeep learning architecture. So this basically defines how\nwe compute one layer of GN.",
    "start": "2648470",
    "end": "2656540"
  },
  {
    "text": "And in practice, we will\nhave multiple layers, where k describes the\nnumber of total layers.",
    "start": "2656540",
    "end": "2662850"
  },
  {
    "text": "So we'll iterate here,\napply this GN on the input where tha zeroth layer\nof graph embedding.",
    "start": "2662850",
    "end": "2670250"
  },
  {
    "text": "And then in the end, we will\nget the embedding after k runs computation. And this can be expressed as\ndoing k runs of neighborhood",
    "start": "2670250",
    "end": "2680630"
  },
  {
    "text": "aggregation. And notice that here,\nbecause we use a summation,",
    "start": "2680630",
    "end": "2687140"
  },
  {
    "text": "this function is\npermutation invariant. So this follows\nthe nice property",
    "start": "2687140",
    "end": "2693050"
  },
  {
    "text": "that we discussed for any\ngraph learning method.",
    "start": "2693050",
    "end": "2695885"
  },
  {
    "text": "Any question? Yeah. Why did you use an average of\nneighborhoods previous layer",
    "start": "2698710",
    "end": "2705190"
  },
  {
    "text": "embeddings versus just the\nneighbors of raw features,",
    "start": "2705190",
    "end": "2711040"
  },
  {
    "text": "or like the embedding of\nthe neighbor at layer zero? So the question is, why don't\nwe average the input feature",
    "start": "2711040",
    "end": "2720820"
  },
  {
    "text": "instead of-- Yeah, like you say average\nof neighbor's previous layer",
    "start": "2720820",
    "end": "2727900"
  },
  {
    "text": "embeddings. I'm wondering why wouldn't\nyou choose the layer zero",
    "start": "2727900",
    "end": "2734270"
  },
  {
    "text": "embedding? Yeah.",
    "start": "2734270",
    "end": "2736735"
  },
  {
    "text": "I see. So you're suggesting that\ninstead of just looking at the first layer\nembedding, we can also",
    "start": "2740260",
    "end": "2746400"
  },
  {
    "text": "look at the zeroth layer,\nthe initialization embedding. Yeah. Because to me, it\nsounds like you're",
    "start": "2746400",
    "end": "2753359"
  },
  {
    "text": "adding this node at this step. So it doesn't quite\nmake sense to like look",
    "start": "2753360",
    "end": "2761970"
  },
  {
    "text": "at all the previous case level. Well, so, first\nthe motivation here",
    "start": "2761970",
    "end": "2767790"
  },
  {
    "text": "is that we want to have\nso-called deep representation or like an interleaved complex\nrepresentation for a node.",
    "start": "2767790",
    "end": "2773730"
  },
  {
    "text": "So that's why we have\nto iteratively generate embedding, and then feed\nit into the next layer.",
    "start": "2773730",
    "end": "2780880"
  },
  {
    "text": "So suppose, here, we\nreplace this representation with the zeroth layer, then\nthe embedding so-called shallow",
    "start": "2780880",
    "end": "2788070"
  },
  {
    "text": "because you never go beyond\nlike a one-layer network. You directly transform\nthe input zeroth layer.",
    "start": "2788070",
    "end": "2796320"
  },
  {
    "text": "But I think your\nidea is actually like a helpful seen in some of\nthe literature, where probably,",
    "start": "2796320",
    "end": "2805540"
  },
  {
    "text": "some of you know the\nnotion of DenseNet. So essentially, your suggestion\nis that during computation, we",
    "start": "2805540",
    "end": "2812140"
  },
  {
    "text": "always associate commissioner\nwith the raw input zeroth layer. And you can consider in\nthe network representation,",
    "start": "2812140",
    "end": "2820130"
  },
  {
    "text": "we have a stack of\nmultiple layers. But then, in each layer, we will\nconnect it back to the input.",
    "start": "2820130",
    "end": "2826880"
  },
  {
    "text": "And I think that kind\nof my interpretation of your suggestion. I think, yeah, it's\nkind of also valuable.",
    "start": "2826880",
    "end": "2832609"
  },
  {
    "text": "But in this basic version of GN,\nwe just use the previous layer.",
    "start": "2832610",
    "end": "2836230"
  },
  {
    "text": "Any other questions?",
    "start": "2840860",
    "end": "2842170"
  },
  {
    "text": "OK. So basically, this is\nthe most dense slide in this lecture, that describe\nwhat is a GCN looks like.",
    "start": "2846100",
    "end": "2854020"
  },
  {
    "text": "So we talk about permutation\nequivariance and permutation invariance. And let's see or like\ncheck whether the network",
    "start": "2856880",
    "end": "2864910"
  },
  {
    "text": "we have defined follows\nthese two properties. So let's first talk\nabout permutation invariance property.",
    "start": "2864910",
    "end": "2870790"
  },
  {
    "text": "So the argument is as follows. So given a node in\nthe network, the GCN",
    "start": "2870790",
    "end": "2876250"
  },
  {
    "text": "that computes its embedding\nis permutation invariant. Our example is to\nembed this blue node.",
    "start": "2876250",
    "end": "2883960"
  },
  {
    "text": "And we say this competition\nfor a given node embedding is permutation invariant\nbecause two reasons.",
    "start": "2883960",
    "end": "2890500"
  },
  {
    "text": "First, we make sure that\nwe share the neural network weights when we transform the\nneighbors of a given node.",
    "start": "2890500",
    "end": "2898040"
  },
  {
    "text": "And second, when we\ntake this aggregation, we make sure that\nthe aggregation is permutation invariant.",
    "start": "2898040",
    "end": "2903700"
  },
  {
    "text": "For example, just a summation. And due to these\ntwo reasons, we can conclude that this full\ncomputing graph is also",
    "start": "2903700",
    "end": "2912310"
  },
  {
    "text": "permutation invariant. So why this property\nis like an interesting",
    "start": "2912310",
    "end": "2917900"
  },
  {
    "text": "or like a non-trivial? That's because we can also\nconsider define the computation",
    "start": "2917900",
    "end": "2923660"
  },
  {
    "text": "differently. And in fact, in early\nliterature of GN, people have tried other\nways of performing",
    "start": "2923660",
    "end": "2930500"
  },
  {
    "text": "the aggregation of neighbors. And they may perform well,\nbut they don't really follow the permutation\ninvariant property.",
    "start": "2930500",
    "end": "2936710"
  },
  {
    "text": "So another popular\nchoice at that time was to use recurrent\nneural network to encode a set of nodes.",
    "start": "2936710",
    "end": "2943970"
  },
  {
    "text": "So in that case, suppose\nwe have neighbors BCD, they will throw BCD as a sequence.",
    "start": "2943970",
    "end": "2950242"
  },
  {
    "text": "And throw it into a\nrecurrent neural net, and use the output\nas the encoded value. And although this\nidea makes sense,",
    "start": "2950242",
    "end": "2957170"
  },
  {
    "text": "it doesn't really follow this\npermutation invariant property. And in our case, we can use\nthis shared neural network",
    "start": "2957170",
    "end": "2965480"
  },
  {
    "text": "plus like a sum aggregation to\nmake sure the computation is permutation invariant.",
    "start": "2965480",
    "end": "2971030"
  },
  {
    "text": "Another property to check\nis permutation equivariance. And our argument is as follows.",
    "start": "2974170",
    "end": "2979670"
  },
  {
    "text": "Consider when we try to encode\nall the nodes in the network, so we're not just--",
    "start": "2979670",
    "end": "2985450"
  },
  {
    "text": "previously, we focus of encoding\none node, say, this blue node. Now, suppose we want to consider\nthe embedding function for all",
    "start": "2985450",
    "end": "2993280"
  },
  {
    "text": "the nodes in the network. We claim that this\nembedding function is permutation equivariant.",
    "start": "2993280",
    "end": "2998710"
  },
  {
    "text": "And just to remind you\nwhat is equivariant mean is that suppose, we\nhave this same network,",
    "start": "2998710",
    "end": "3004050"
  },
  {
    "text": "but with two different\nordering plan. And we can check. Suppose, we offer\nthe permutation,",
    "start": "3004050",
    "end": "3011970"
  },
  {
    "text": "the same node will have\nthe same encoded value, but only their position\nwill differ accordingly.",
    "start": "3011970",
    "end": "3021040"
  },
  {
    "text": "So what is the reasoning\nwhy this is true for GN? This is because two reasons.",
    "start": "3021040",
    "end": "3026580"
  },
  {
    "text": "First is that we\nmake sure the feature matrix and the embedding\nare closely aligned.",
    "start": "3026580",
    "end": "3033730"
  },
  {
    "text": "What do I mean is\nthat here, we always make sure the first\nrow is talking about node A in both\nthe input node feature",
    "start": "3033730",
    "end": "3041440"
  },
  {
    "text": "and the output node embedding. And the second row is about node\nB and zeroth node C, et cetera.",
    "start": "3041440",
    "end": "3049790"
  },
  {
    "text": "And the second reason\nis that we showed that given a node,\nwhen we apply a GN,",
    "start": "3049790",
    "end": "3055600"
  },
  {
    "text": "it is permutation invariant. So after permuting\nthe nodes, the color,",
    "start": "3055600",
    "end": "3061390"
  },
  {
    "text": "which represent the\nvalue of the embedding is always the same because\nof this invariant property.",
    "start": "3061390",
    "end": "3067420"
  },
  {
    "text": "So given these two\nreasons, suppose we shuffle the order of all\nthe nodes in the network,",
    "start": "3067420",
    "end": "3072460"
  },
  {
    "text": "we can see, globally, it\npresents this information equivariant property.",
    "start": "3072460",
    "end": "3077990"
  },
  {
    "text": "So why does this\nproperty also important? That is because in our\ncase, we want to encode",
    "start": "3077990",
    "end": "3084820"
  },
  {
    "text": "all the nodes in the network. And we want to\nhave a reliable way to extract the embedding\nfor a given node.",
    "start": "3084820",
    "end": "3092799"
  },
  {
    "text": "Suppose we compute\nthe embeddings in parallel, what could\nbe the failure mode",
    "start": "3092800",
    "end": "3098400"
  },
  {
    "text": "is that we may correctly\nembed each node. So A to F, we get\ntheir embeddings.",
    "start": "3098400",
    "end": "3104670"
  },
  {
    "text": "But we may get a wrong\norder of the embedding. So instead of having\nthem access aligned,",
    "start": "3104670",
    "end": "3110610"
  },
  {
    "text": "so the first row, node\nA, second row, node B, we may get them a wrong. Like from the third row,\nwe may talk about node A.",
    "start": "3110610",
    "end": "3117000"
  },
  {
    "text": "So even though we get globally\nthe set of embeddings correct, their ordering may be wrong.",
    "start": "3117000",
    "end": "3123250"
  },
  {
    "text": "So the property of\nequivariant ensures that we always can reliably get\nthe embedding for a given node.",
    "start": "3123250",
    "end": "3131580"
  },
  {
    "text": "So this property\nis also non-trivial and really important for GN.",
    "start": "3131580",
    "end": "3136080"
  },
  {
    "text": "OK. We can move forward. So so far, we have defined the\nmodel of graph neural network.",
    "start": "3138640",
    "end": "3147350"
  },
  {
    "text": "And next, I'm\ngoing to talk about how do we train such a network. So to train a network,\nreally, the key",
    "start": "3147350",
    "end": "3154640"
  },
  {
    "text": "is to define a loss function,\nand based on the embedding Z that we get.",
    "start": "3154640",
    "end": "3160670"
  },
  {
    "text": "So what are our other kind\ntraining pipeline or training procedure looks like?",
    "start": "3160670",
    "end": "3167810"
  },
  {
    "text": "This is the same definition\nof GCN that we have just seen. We have two main\ntrainable parameters here,",
    "start": "3167810",
    "end": "3175790"
  },
  {
    "text": "W and B, where we talk about\nto understand this equation, we talk about how to\nencode the node itself",
    "start": "3175790",
    "end": "3182540"
  },
  {
    "text": "and how to encode\nthe node's neighbors. So node B tells\nhow do we transform",
    "start": "3182540",
    "end": "3187880"
  },
  {
    "text": "and process the information\nof the node itself. And note, this matrix\nW tells us about how",
    "start": "3187880",
    "end": "3193790"
  },
  {
    "text": "to encode the neighbors\nof a given node. So these are the\ntrainable parameter.",
    "start": "3193790",
    "end": "3199710"
  },
  {
    "text": "And then we can feed the\noutput of the node embedding into any loss function\nthat you can see.",
    "start": "3199710",
    "end": "3206190"
  },
  {
    "text": "So in the beginning\nof review, we talk about L2 loss for\nregression and cross entropy",
    "start": "3206190",
    "end": "3212010"
  },
  {
    "text": "loss for classification. So we can define those\nloss function based",
    "start": "3212010",
    "end": "3217710"
  },
  {
    "text": "on the computed node embedding.",
    "start": "3217710",
    "end": "3219690"
  },
  {
    "text": "And another-- sorry. Question? Yeah. Can you go back\none slide, please?",
    "start": "3223130",
    "end": "3230142"
  },
  {
    "text": "No, not this one. Sorry. It's back. This is forwarded. Another one.",
    "start": "3230142",
    "end": "3237450"
  },
  {
    "text": "Wait. Do you mean the [INAUDIBLE]? One where there's the\nembedding on the right.",
    "start": "3237450",
    "end": "3243540"
  },
  {
    "text": "Here we go. So let's say, if this is\na classification case, and we have these two\nrepresentations of the network.",
    "start": "3243540",
    "end": "3250250"
  },
  {
    "text": "And it's the same network\nat the end of the day, how do we make sure that\nembedding one and embedding two end up with the\nsame output, let's say,",
    "start": "3250250",
    "end": "3256760"
  },
  {
    "text": "that it's whatever class A? So we just like stack\nthem up as one vector,",
    "start": "3256760",
    "end": "3265000"
  },
  {
    "text": "and then do some kind of\nsummation, and then that's it? Or how do we go from these\nembeddings to a classification?",
    "start": "3265000",
    "end": "3273230"
  },
  {
    "text": "Yeah. Actually, in the following\nslide, we will talk about that. But the idea is that for a\ngiven node, we get a vector.",
    "start": "3273230",
    "end": "3280910"
  },
  {
    "text": "We can have an additional\nlayer that projects this vector into a single score. .",
    "start": "3280910",
    "end": "3286400"
  },
  {
    "text": "The score can tells\nus the predicted class of that like a node. For example, it's a\nbinary classification,",
    "start": "3286400",
    "end": "3292460"
  },
  {
    "text": "we have true or false. Then we can project this\ntwo-dimensional vector into a single scalar\nfrom range of 0 to 1.",
    "start": "3292460",
    "end": "3300900"
  },
  {
    "text": "And that can tells\nus how we want to classify this given node. But that itself should also be\na summation or something, where",
    "start": "3300900",
    "end": "3308390"
  },
  {
    "text": "the order doesn't matter, right? Because we want\nH1 and H2 to have the same numerical value when\nwe go through that function.",
    "start": "3308390",
    "end": "3316590"
  },
  {
    "text": "Yeah. So I think here,\nI want to clarify. Because we want to define a\nnode classification problem.",
    "start": "3316590",
    "end": "3323370"
  },
  {
    "text": "So the label are not directly\nassociated with the embedding matrix, but also with\na particular node.",
    "start": "3323370",
    "end": "3330840"
  },
  {
    "text": "So for example, we know node C-- sorry. We know this blue node is\nassociated with a label of one.",
    "start": "3330840",
    "end": "3338930"
  },
  {
    "text": "So it's not related to\nthe order of the node, but we always find\nthis blue node and then assign an order a\nlabel of one to the node.",
    "start": "3338930",
    "end": "3347510"
  },
  {
    "text": "And this way, we can\nalways reliably associate a node with label, and\nthen train the network,",
    "start": "3347510",
    "end": "3353450"
  },
  {
    "text": "regardless of how we\norder the different nodes.",
    "start": "3353450",
    "end": "3356045"
  },
  {
    "text": "Any other question? OK.",
    "start": "3359388",
    "end": "3364400"
  },
  {
    "text": "So we talked about training\na graph neural network. And then the goal is to\noptimize these weight matrices",
    "start": "3364400",
    "end": "3371870"
  },
  {
    "text": "based on the embedding. And another important\ntopic here is",
    "start": "3371870",
    "end": "3378200"
  },
  {
    "text": "how can we efficiently\ntrain or optimize this GN?",
    "start": "3378200",
    "end": "3383359"
  },
  {
    "text": "The reason is that the success\nof modern deep learning is really rely on parallel\ncomputing GPU acceleration.",
    "start": "3383360",
    "end": "3389910"
  },
  {
    "text": "So earlier, our formulation\nwas like a local level. So we only consider\nhow to compute",
    "start": "3389910",
    "end": "3395160"
  },
  {
    "text": "embedding for a given node. But we didn't really\ntalk about how to parallelize that algorithm.",
    "start": "3395160",
    "end": "3400170"
  },
  {
    "text": "So now, our goal is to\nrewritten the definition of GN, but now, in the matrix form.",
    "start": "3400170",
    "end": "3405890"
  },
  {
    "text": "And suppose, we can\nread in a matrix form, then we can easily utilize\nthose parallel computing device",
    "start": "3405890",
    "end": "3412430"
  },
  {
    "text": "to accelerate the\ntraining process. So here is our idea. So earlier, we talk about\nthe core of neighborhood",
    "start": "3412430",
    "end": "3420980"
  },
  {
    "text": "aggregation is about summing\nthe neighbors of a given node. And we can equivalently\nwrite it into a matrix form,",
    "start": "3420980",
    "end": "3429060"
  },
  {
    "text": "which is essentially\nA times H, where A is the adjacency matrix. But we particularly look at\nthe rows that describe node v.",
    "start": "3429060",
    "end": "3437790"
  },
  {
    "text": "And then H is the matrix that\ndescribe the embedding for all the nodes in the network.",
    "start": "3437790",
    "end": "3444030"
  },
  {
    "text": "And another notion\nis the D, which is a diagonal matrix that\ntells about the degree",
    "start": "3444030",
    "end": "3450780"
  },
  {
    "text": "of each node in the network. We can use the inverse of D\nto express like the inverse",
    "start": "3450780",
    "end": "3457170"
  },
  {
    "text": "of the node degree. And based on this two\nmatrix that we defined,",
    "start": "3457170",
    "end": "3462690"
  },
  {
    "text": "we can write this\naggregation function, which tells us we want\nto take the average",
    "start": "3462690",
    "end": "3468930"
  },
  {
    "text": "of each node neighbors into\nthis matrix form, where we have this D inverse times A times H.",
    "start": "3468930",
    "end": "3477060"
  },
  {
    "text": "So you can check that these\ntwo equations are the same. And this way, we can express\nthis local view of GN",
    "start": "3477060",
    "end": "3484080"
  },
  {
    "text": "into this global matrix wheel.",
    "start": "3484080",
    "end": "3486495"
  },
  {
    "text": "So follow this idea,\nwe can additionally write the entire transformation\nof n into its matrix form.",
    "start": "3491280",
    "end": "3499510"
  },
  {
    "text": "So now we have this\nA times H times W, and H times B, which\nbasically describe",
    "start": "3499510",
    "end": "3506760"
  },
  {
    "text": "how we transform\nthe nodes neighbors and transform the node\nitself in a global wheel.",
    "start": "3506760",
    "end": "3513300"
  },
  {
    "text": "And so the red one\ntells us about how to aggregate the neighbors. The blue term tells us how\nto transform the node itself.",
    "start": "3513300",
    "end": "3519930"
  },
  {
    "text": "And in practice,\nthis implies that we can efficiently compute GN\nnot in a local perspective,",
    "start": "3519930",
    "end": "3526320"
  },
  {
    "text": "but really, in the\nglobal sparse matrix, multiplication perspective. And this can be very\nhelpful because suppose,",
    "start": "3526320",
    "end": "3533910"
  },
  {
    "text": "we can accelerate sparse\nmatrix multiplication, essentially you can accelerate\nany graph learning algorithm.",
    "start": "3533910",
    "end": "3541740"
  },
  {
    "text": "And there's a side\nnote like that we said.",
    "start": "3541740",
    "end": "3546780"
  },
  {
    "text": "That is not every GN can be\nwritten in this matrix form. So because the aggregation\nfunction can be more complex.",
    "start": "3546780",
    "end": "3554800"
  },
  {
    "text": "So what are some examples? So for example, when we\nperform the aggregation instead",
    "start": "3554800",
    "end": "3561420"
  },
  {
    "text": "of taking the average\nof the neighbors, we can consider taking the\nmaximum of the neighbors.",
    "start": "3561420",
    "end": "3566430"
  },
  {
    "text": "Because this maximum operator\ncannot be written in this matrix form. We cannot really succinctly\nwrite it in this matrix",
    "start": "3566430",
    "end": "3575609"
  },
  {
    "text": "expression. And couldn't really benefit\nfrom this sparse matrix multiplication.",
    "start": "3575610",
    "end": "3580743"
  },
  {
    "text": "OK. So we talk about how to\nefficiently train a network. Next, I'm going to give\nsome concrete examples",
    "start": "3584370",
    "end": "3590720"
  },
  {
    "text": "of how to train it under the\nconcrete machine learning setting.",
    "start": "3590720",
    "end": "3595850"
  },
  {
    "text": "As you probably know, there\nare two major paradigm of machine learning. That is supervised learning\nand unsupervised learning.",
    "start": "3595850",
    "end": "3602520"
  },
  {
    "text": "In supervised learning,\nwe have a concrete label associated with a graph. And let's say, for example,\nwe have a node label",
    "start": "3602520",
    "end": "3609140"
  },
  {
    "text": "for each node, and then we\ncan define a loss function based on the type of\nclassification or regression.",
    "start": "3609140",
    "end": "3615540"
  },
  {
    "text": "And alternatively,\nwe can consider the unsupervised\nlearning paradigm for graphs, where\nin that case, we",
    "start": "3615540",
    "end": "3621200"
  },
  {
    "text": "don't have any particular\nexternal label for a node. But instead, we can use\nthe graph structure itself",
    "start": "3621200",
    "end": "3628130"
  },
  {
    "text": "as the supervision. So here is an example of\nan unsupervised learning",
    "start": "3628130",
    "end": "3636089"
  },
  {
    "text": "technique. And this idea is\nactually very related to what we learned in\nthe previous lecture about random walk embedding.",
    "start": "3636090",
    "end": "3642150"
  },
  {
    "text": "So in this case,\nthe loss function will take two node\nembedding as input. So embedding for\nnode u and node v.",
    "start": "3642150",
    "end": "3649800"
  },
  {
    "text": "We will have a decoder\nfunction, which is, for example, an inner product. And then we'll compare\nthis prediction",
    "start": "3649800",
    "end": "3656490"
  },
  {
    "text": "with node label, yuv. So how do we label\na pair of node uv?",
    "start": "3656490",
    "end": "3663900"
  },
  {
    "text": "The idea is that we can label\nit based on node similarity. So suppose, a pair of node\nis considered similar,",
    "start": "3663900",
    "end": "3670470"
  },
  {
    "text": "then we label them as one. And if it's not, we\nlabel them as zero. And how do we\ndefine a similarity?",
    "start": "3670470",
    "end": "3676380"
  },
  {
    "text": "It can be precisely what we\nteach in the previous lecture. For example, in random walk,\nsuppose two nodes co-occur",
    "start": "3676380",
    "end": "3684090"
  },
  {
    "text": "in a random walk, then\nwe assign a label one to this pair of node. And if it's not,\nthen we assign 0.",
    "start": "3684090",
    "end": "3691440"
  },
  {
    "text": "We can also use the idea\nof matrix factorization. We also mentioned in\nthe previous lecture,",
    "start": "3691440",
    "end": "3696519"
  },
  {
    "text": "and in that case, we aim\nto reconstruct the network structure. So suppose, two nodes, they are\ndirect neighbors of each other,",
    "start": "3696520",
    "end": "3704859"
  },
  {
    "text": "we can label them as one. And if not, we\ncan label as zero. We can additionally use a\nnode proximity in the graph.",
    "start": "3704860",
    "end": "3712460"
  },
  {
    "text": "For example, if two nodes\nare two hop neighbors in the network,\nwe can label them as two, three hop\nneighbors as three.",
    "start": "3712460",
    "end": "3718870"
  },
  {
    "text": "So there are a lot\nof creative way that you can define\nthis labels, and then",
    "start": "3718870",
    "end": "3726700"
  },
  {
    "text": "use them as self-supervised\nlearning objectives for graphs. An alternative\nterm for this idea",
    "start": "3726700",
    "end": "3734080"
  },
  {
    "text": "is so-called\nself-supervised learning. So we want to use the graph\nstructure itself to create",
    "start": "3734080",
    "end": "3739569"
  },
  {
    "text": "a supervision for a network. And self-supervised\nalso have a lot of other use cases in the\ndeep learning application.",
    "start": "3739570",
    "end": "3746050"
  },
  {
    "text": "But in graph, it will\nbe pretty exciting because you can define very\ncomplex and very interesting",
    "start": "3746050",
    "end": "3752320"
  },
  {
    "text": "objectives on graphs. So this is the\nidea of how we can do unsupervised\nlearning on graph if you don't have any labels.",
    "start": "3752320",
    "end": "3758018"
  },
  {
    "text": "OK. So here's a visualization for\na concrete supervised learning",
    "start": "3760710",
    "end": "3768840"
  },
  {
    "text": "example. So in this case, suppose we\nhave a drug interaction network.",
    "start": "3768840",
    "end": "3775349"
  },
  {
    "text": "And we want to classify a\ngiven node, whether it's toxic or not. And we get a particular\nlabel for that node.",
    "start": "3775350",
    "end": "3783720"
  },
  {
    "text": "Then what we can\ndo is that we can get the embedding\nof that node zv",
    "start": "3783720",
    "end": "3788880"
  },
  {
    "text": "and throw it into\nthis loss function. And this loss function is\nreally binary cross entropy loss",
    "start": "3788880",
    "end": "3796890"
  },
  {
    "text": "that are widely used\nfor classification. And so here, zv is\nthe encoder output",
    "start": "3796890",
    "end": "3802560"
  },
  {
    "text": "from graph neural network. And we have some\nclassification weights",
    "start": "3802560",
    "end": "3808020"
  },
  {
    "text": "to weight this like a final\nlayer embedding into a scalar.",
    "start": "3808020",
    "end": "3813240"
  },
  {
    "text": "And then we can optimize based\non this binary cross entropy loss.",
    "start": "3813240",
    "end": "3818520"
  },
  {
    "text": "And this loss function,\nwhat it's really saying that because we\nwant to minimize the loss",
    "start": "3818520",
    "end": "3823530"
  },
  {
    "text": "is that there's an inverse here. We are essentially\nmaximizing the value here.",
    "start": "3823530",
    "end": "3830099"
  },
  {
    "text": "And suppose, this y,\nwhich is the ground truth node class label.",
    "start": "3830100",
    "end": "3835590"
  },
  {
    "text": "Suppose, it takes a value\nof 1, then the second term gets canceled out. So our idea is we want to\nmaximize the predictive value.",
    "start": "3835590",
    "end": "3845170"
  },
  {
    "text": "So suppose the\nclass label is one, then we want to\nmaximize the prediction. And alternatively, if\nthe class label is zero,",
    "start": "3845170",
    "end": "3853290"
  },
  {
    "text": "the first term disappear, and we\nonly have the second term here. And we want to have this\nvalue as low as possible",
    "start": "3853290",
    "end": "3861329"
  },
  {
    "text": "to make sure this entire\nexpression is maximized. So suppose the\ncross label is zero,",
    "start": "3861330",
    "end": "3868043"
  },
  {
    "text": "then we want to\nminimize the prediction. So this is like the intuition\nof the loss function.",
    "start": "3868043",
    "end": "3871950"
  },
  {
    "text": "So we have already talked about\nbasically everything about GN. So here is like a end to end\nlike a description of how",
    "start": "3875120",
    "end": "3884950"
  },
  {
    "text": "the full pipeline may work. We have this network and\nwe want to embed a node A.",
    "start": "3884950",
    "end": "3891310"
  },
  {
    "text": "We will define the neighborhood\nstructure of node A into a computational graph.",
    "start": "3891310",
    "end": "3897220"
  },
  {
    "text": "And we will be able to\ngenerate this node embedding. And we can then define\nthe loss function",
    "start": "3897220",
    "end": "3904240"
  },
  {
    "text": "that we just went over based\non the predicted embedding.",
    "start": "3904240",
    "end": "3909940"
  },
  {
    "text": "And we will train\nthat loss function over a set of input nodes. For example, we may\nconsider training",
    "start": "3909940",
    "end": "3916480"
  },
  {
    "text": "on node ABC in this network. And after the training, we\ncan do prediction or inference",
    "start": "3916480",
    "end": "3924490"
  },
  {
    "text": "for nodes in the graph. And it is worth noticing that\nwe can make predictions even",
    "start": "3924490",
    "end": "3930670"
  },
  {
    "text": "for nodes that we\nnever trained on. So we trained on the node\nABC, but DEF, we never show it",
    "start": "3930670",
    "end": "3937910"
  },
  {
    "text": "to the model during training. But still, we can use\nthe trained network to make predictions for\nthese three unseen nodes.",
    "start": "3937910",
    "end": "3946650"
  },
  {
    "text": "And this property is\nso-called the inductive learning capability. Inductive learning is\nreally about can we",
    "start": "3946650",
    "end": "3953519"
  },
  {
    "text": "use a trained network and\ngeneralize it to unseen nodes? And the reason that a GN can\nperform inductive learning",
    "start": "3953520",
    "end": "3962400"
  },
  {
    "text": "is because we\nshare the parameter in the computational graph. And because the\nparameters are shared,",
    "start": "3962400",
    "end": "3968790"
  },
  {
    "text": "we are not bounded by how\nthe input nodes look like. So we can use the same\nnetwork, but repurpose it",
    "start": "3968790",
    "end": "3975990"
  },
  {
    "text": "to any unseen structure\nin the network.",
    "start": "3975990",
    "end": "3979740"
  },
  {
    "text": "So here are some\nexamples how we can utilize this inductive\nlearning capability.",
    "start": "3982610",
    "end": "3988570"
  },
  {
    "text": "We can consider train the\nneural network in one graph, but then apply it\nto a new graph.",
    "start": "3988570",
    "end": "3995140"
  },
  {
    "text": "And this new graph is never\nseen during training time. And what are some use cases?",
    "start": "3995140",
    "end": "4000480"
  },
  {
    "text": "For example, we\ncan define a graph over protein interaction\nof different organisms.",
    "start": "4000480",
    "end": "4006450"
  },
  {
    "text": "We can train a network\nin one organism that we have precise\ntraining label.",
    "start": "4006450",
    "end": "4011940"
  },
  {
    "text": "But given we have\na trained model, we can generate embeddings\nfor unseen organism.",
    "start": "4011940",
    "end": "4017800"
  },
  {
    "text": "And this is quite useful to\nsay in the biological domain.",
    "start": "4017800",
    "end": "4022930"
  },
  {
    "text": "Another example is that\nwe can use this property to predict over new nodes.",
    "start": "4022930",
    "end": "4028360"
  },
  {
    "text": "In this case, we have a given\ngraph or a snapshot of a graph. And over time, we will get new\nnodes arrive into the network.",
    "start": "4028360",
    "end": "4037450"
  },
  {
    "text": "And our goal is to generate\nembedding for the new node. The example use\ncase here is very",
    "start": "4037450",
    "end": "4043540"
  },
  {
    "text": "popular in industry,\nwhere essentially, you can represent a lot of\nonline networks into a graph,",
    "start": "4043540",
    "end": "4049750"
  },
  {
    "text": "such as Reddit,\nYouTube, et cetera. So suppose someone upload a new\nvideo into the YouTube network,",
    "start": "4049750",
    "end": "4056680"
  },
  {
    "text": "this is essentially where a\nnew node arrive in the network. We can use a pre-trained graph\nneural network, and generally,",
    "start": "4056680",
    "end": "4064810"
  },
  {
    "text": "embedding for this new node. So this is fully reliant on\nthe fact that GN is inductive.",
    "start": "4064810",
    "end": "4070690"
  },
  {
    "text": "On the contrary,\nsuppose we use the thing we learned on the previous\nlecture, say no to [INAUDIBLE]..",
    "start": "4070690",
    "end": "4076120"
  },
  {
    "text": "It will be hard\nbecause we wouldn't be able to generate\nthe embedding directly for a new node, but rather, we\nhave to perform a few iteration",
    "start": "4076120",
    "end": "4084290"
  },
  {
    "text": "optimization until\nwe can generate embedding for a new node. So this is the nice\nproperty of GN.",
    "start": "4084290",
    "end": "4092810"
  },
  {
    "text": "Question? Yeah. I was wondering if\nthere was any results on like if you're successively\nadding new notes to like",
    "start": "4092810",
    "end": "4098600"
  },
  {
    "text": "a pre-trained graph or like a\ngraph that you pre-trained on, then, is there a\npoint after you just",
    "start": "4098600",
    "end": "4104660"
  },
  {
    "text": "start to deteriorate\nlike the accuracy? Yeah. That's a great question.",
    "start": "4104660",
    "end": "4110792"
  },
  {
    "text": "I think this is also like\na frontier in the research. The question is about\nsuppose, we continuously add new nodes into the\nnetwork, of course,",
    "start": "4110792",
    "end": "4117439"
  },
  {
    "text": "until a certain point, the model\nwouldn't be able to predict and how do we find that point.",
    "start": "4117439",
    "end": "4123539"
  },
  {
    "text": "Yeah. I think this is a pretty nice\nresearch question to ask. I think usually, people\njust consider in practice.",
    "start": "4123540",
    "end": "4131189"
  },
  {
    "text": "What people do in\npractice is that they have like a ongoing validation set.",
    "start": "4131189",
    "end": "4137000"
  },
  {
    "text": "So over time, they will\nre-evaluate the network",
    "start": "4137000",
    "end": "4142040"
  },
  {
    "text": "on a holdout validation set. Suppose the performance\non the validation set get deteriorated,\nthen it basically",
    "start": "4142040",
    "end": "4148549"
  },
  {
    "text": "says the model is kind\nof failing to learn over a new graph structure. And that is time when you\nwant to retrain or fine tune",
    "start": "4148550",
    "end": "4155278"
  },
  {
    "text": "the network to\nthe architecture-- to the new graph. Question.",
    "start": "4155279",
    "end": "4160750"
  },
  {
    "text": "You mean we only have like,\nfor example, on YouTube, we only have one [INAUDIBLE].",
    "start": "4160750",
    "end": "4166110"
  },
  {
    "text": "Yeah, that's a great question. We, actually, will talk about\nspecific layout of graph learning method in\nthe next lecture.",
    "start": "4169380",
    "end": "4176528"
  },
  {
    "text": "So stay tuned. But in the meantime,\nI can quickly say, usually, we will just\ndo the random splitting.",
    "start": "4176529",
    "end": "4182068"
  },
  {
    "text": "For example, given\nthis network, we may randomly pick some of the\nnodes as the training node and some of them as\nthe validation node.",
    "start": "4182069",
    "end": "4188497"
  },
  {
    "text": "And we will make\nsure during training, we never see this\nlike a holdout node. Yeah. That's the basic idea.",
    "start": "4188497",
    "end": "4194389"
  },
  {
    "text": "But there are a lot\nof different variants. And we will cover\nin the next lecture. Question?",
    "start": "4194390",
    "end": "4199860"
  },
  {
    "text": "When you add a new\nnode to the network, do you need to know all its\nconnections to the existing network?",
    "start": "4199860",
    "end": "4204989"
  },
  {
    "text": "Or can you sort of\nlike add a new node and then predict where it should\nbe connected in that network?",
    "start": "4204990",
    "end": "4210160"
  },
  {
    "text": "Yeah. That's a great question. So the question is that\ncan we also predict how the new node will\nconnect to the existing node?",
    "start": "4210160",
    "end": "4217739"
  },
  {
    "text": "And actually, there's, a\nbit later in our course, we have a course called-- a\nlecture about deep generative",
    "start": "4217740",
    "end": "4223770"
  },
  {
    "text": "model. And that is precisely the\nproblem you talk about, where we want to predict\nhow a node can connect",
    "start": "4223770",
    "end": "4230010"
  },
  {
    "text": "with each existing node. And this way, we can use\nit to generate a graph. So we know how a\nnew node connect.",
    "start": "4230010",
    "end": "4235619"
  },
  {
    "text": "And we can iteratively do this\nto generate a full network. And that's kind of\ndefinitely doable.",
    "start": "4235620",
    "end": "4242610"
  },
  {
    "text": "Alternatively, you can think of\nthis as a link prediction task. So you want to predict the\nmissing link between a new node",
    "start": "4242610",
    "end": "4248160"
  },
  {
    "text": "and existing graph. So yeah, that's a\nnice application.",
    "start": "4248160",
    "end": "4252090"
  },
  {
    "text": "OK. We can move forward to the\nlast part of the lecture. So far, we talked about\nthe basics of deep learning",
    "start": "4256220",
    "end": "4263179"
  },
  {
    "text": "on graphs and particular\narchitecture of GCN. And in the final\npart of this lecture,",
    "start": "4263180",
    "end": "4269430"
  },
  {
    "text": "I want to discuss\nthe relationship between GN and CNN. Because I think a lot\nof you are already",
    "start": "4269430",
    "end": "4275630"
  },
  {
    "text": "familiar with convolutional\nneural network for images. And we want to compare and\nargue that actually, GN",
    "start": "4275630",
    "end": "4281570"
  },
  {
    "text": "is more general than CNN. And it subsumes CNNs.",
    "start": "4281570",
    "end": "4285560"
  },
  {
    "text": "So how do we\ncompare GN and CNNs?",
    "start": "4288660",
    "end": "4290930"
  },
  {
    "text": "Well, this is how thing works. We have an image. We can have define a\nconvolutional kernel.",
    "start": "4294090",
    "end": "4301530"
  },
  {
    "text": "And we will move that kernel\naround in the input image to generate the\noutput feature map.",
    "start": "4301530",
    "end": "4307860"
  },
  {
    "text": "Usually, we write it in\nlike a convolutional form. But now, because we just\nlearned this idea of aggregation",
    "start": "4307860",
    "end": "4316890"
  },
  {
    "text": "over neighbors, we\ncan equivalently write CNN into the\nlanguage that we just",
    "start": "4316890",
    "end": "4322230"
  },
  {
    "text": "talked, about how do\nwe aggregate neighbors. So we can define\nneighbors in the image",
    "start": "4322230",
    "end": "4327989"
  },
  {
    "text": "based on the\nproximity of pixels. So for example, given a node\nand this 3 by 3 image patch,",
    "start": "4327990",
    "end": "4336870"
  },
  {
    "text": "we can define the eight\nneighbors of a given pixel as its neighbors.",
    "start": "4336870",
    "end": "4342510"
  },
  {
    "text": "And also, we want to\ninclude the node itself. And then having defined\nthis neighborhood,",
    "start": "4342510",
    "end": "4348480"
  },
  {
    "text": "we can define the transformation\nW over each of the input pixel.",
    "start": "4348480",
    "end": "4354400"
  },
  {
    "text": "And then we take the\nsummation and do the nonlinear transformation. And this is essentially\nan alternative way",
    "start": "4354400",
    "end": "4361180"
  },
  {
    "text": "to express a CNN. And another way to\nencode imagery data",
    "start": "4361180",
    "end": "4368520"
  },
  {
    "text": "is that we simply convert\nthis grid-like structure into a graph. And this graph can\nlook like this.",
    "start": "4368520",
    "end": "4375310"
  },
  {
    "text": "So we have a node\nin the center, we have the self edge that\nconnect the node with itself.",
    "start": "4375310",
    "end": "4381100"
  },
  {
    "text": "And then we can connect all\nthe neighboring pixels with it. So essentially, now, a\nnode have nine neighbors.",
    "start": "4381100",
    "end": "4388860"
  },
  {
    "text": "And we can write-- we can define a GN over a\ngraph defined like this.",
    "start": "4388860",
    "end": "4395620"
  },
  {
    "text": "So we've use the expression\nabove GCN definition like this.",
    "start": "4395620",
    "end": "4402450"
  },
  {
    "text": "And we can compare what\nwe have with GN and CNN. CNN, we just derived it\nin the previous slide.",
    "start": "4402450",
    "end": "4410430"
  },
  {
    "text": "We find that these\ntwo expressions are almost identical. The only difference is that\nthere's a upper script, u here.",
    "start": "4410430",
    "end": "4420030"
  },
  {
    "text": "There is not. That basically says for GN,\nwe will share the parameter",
    "start": "4420030",
    "end": "4425380"
  },
  {
    "text": "across all the neighbors. Because we have this\npermutation invariant property.",
    "start": "4425380",
    "end": "4430480"
  },
  {
    "text": "But for CNN, we do not\nhave that constraint of permutation invariant.",
    "start": "4430480",
    "end": "4435490"
  },
  {
    "text": "The reason is that\nwe know the property of location in an image.",
    "start": "4435490",
    "end": "4440900"
  },
  {
    "text": "And we also want to\nexplicitly express it in the network definition.",
    "start": "4440900",
    "end": "4447010"
  },
  {
    "text": "More precisely, in\nan image data set, we are aware of the\nrelative position of pixels.",
    "start": "4447010",
    "end": "4453760"
  },
  {
    "text": "For example, we can label\neach neighboring pixel of a given node given\npixel as this negative 1,",
    "start": "4453760",
    "end": "4462580"
  },
  {
    "text": "negative 1, negative\n1, 0, et cetera. We have their relative location. And this way, we\ndon't have to consider",
    "start": "4462580",
    "end": "4469857"
  },
  {
    "text": "the permutation invariant\nproperty because we have this very explicit ordering. So this way, we can\ndefine a unique weight",
    "start": "4469857",
    "end": "4477639"
  },
  {
    "text": "for each position\nof the neighbors. Whereas, in graph,\nbecause we don't have this a prior information\nof the location of nodes,",
    "start": "4477640",
    "end": "4485740"
  },
  {
    "text": "we have to treat them equally. So we define this permutation\ninvariant function, which share the weights.",
    "start": "4485740",
    "end": "4492460"
  },
  {
    "text": "That's actually the only\ndifference between GN and CNN in term of this grid-like data.",
    "start": "4492460",
    "end": "4499570"
  },
  {
    "text": "So here's a summary of further\ndiscussing the difference. In terms of comparing,\nseeing the GN, first,",
    "start": "4499570",
    "end": "4508150"
  },
  {
    "text": "for like the size of a\nfilter is predefined for CNN.",
    "start": "4508150",
    "end": "4514390"
  },
  {
    "text": "But for GN, it's not predefined. And also, we talk about GN\ncan process arbitrary graph",
    "start": "4514390",
    "end": "4520660"
  },
  {
    "text": "with different degrees\nfor each nodes. And saying it's not\npermutation invariant as we just talked about because\nwe know the exact position",
    "start": "4520660",
    "end": "4528760"
  },
  {
    "text": "of different nodes. But GN's we do not\nhave such information. So we have this permutation\ninvariant function",
    "start": "4528760",
    "end": "4535150"
  },
  {
    "text": "of aggregating neighbors. So that's everything we\nhave in this lecture.",
    "start": "4535150",
    "end": "4541730"
  },
  {
    "text": "To summarize, we talk about\nthe basics of neural network. And we motivate the idea of\ndeep learning for graphs,",
    "start": "4541730",
    "end": "4549680"
  },
  {
    "text": "and what are the ideal\nproperties we want to have in a graph learning method. And then we particularly talk\nabout one architecture called",
    "start": "4549680",
    "end": "4557000"
  },
  {
    "text": "graph convolutional\nnetwork, how do we define it into\naggregation of neighbors",
    "start": "4557000",
    "end": "4562970"
  },
  {
    "text": "and how do we train it. And finally, we talk about\nhow we compare Gn with CNNs.",
    "start": "4562970",
    "end": "4568520"
  },
  {
    "text": "That's it for the course. And I hope you enjoyed it.",
    "start": "4568520",
    "end": "4572889"
  }
]