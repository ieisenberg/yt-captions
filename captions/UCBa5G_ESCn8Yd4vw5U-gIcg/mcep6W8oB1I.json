[
  {
    "start": "0",
    "end": "5640"
  },
  {
    "text": "Hello, everyone. Today, we have Nazneen-- Yes. --from Hugging Face\nwho is working on AI",
    "start": "5640",
    "end": "13650"
  },
  {
    "text": "safety and alignment\nusing reinforcement learning with human feedback. She's an expert in the\nspace of large language",
    "start": "13650",
    "end": "21060"
  },
  {
    "text": "models and their evaluation.",
    "start": "21060",
    "end": "26640"
  },
  {
    "text": "Before Hugging Face, she\nled a team of researchers at Salesforce,\nfocused on building",
    "start": "26640",
    "end": "31890"
  },
  {
    "text": "robust, natural language\ngeneration systems based on LMS, and she got her PhD at UT\nAustin in computer science.",
    "start": "31890",
    "end": "40470"
  },
  {
    "text": "So everyone, welcome. ",
    "start": "40470",
    "end": "48020"
  },
  {
    "text": "Great. Thanks for having me. So the title of my\ntalk today is recipes for training helpful chatbots.",
    "start": "48020",
    "end": "56050"
  },
  {
    "text": "So here's the introduction. I was part of this team called\nthe H4 for at Hugging Face.",
    "start": "56050",
    "end": "61309"
  },
  {
    "text": "And today, I'll walk\nyou through what we've built, how we decided on\nwhat we need for building that.",
    "start": "61310",
    "end": "68190"
  },
  {
    "text": "And so essentially\nwhat we wanted to build and the goal of the\nteam and the project since earlier this year was to\nfigure out a recipe for H4 which",
    "start": "68190",
    "end": "76660"
  },
  {
    "text": "stands for helpful,\nharmless, honest, and Huggy, because it's Hugging\nFace, chatbot.",
    "start": "76660",
    "end": "82060"
  },
  {
    "text": "And so like, you know, the\ningredients essentially were to figure out\nwhat kind of data sets",
    "start": "82060",
    "end": "87340"
  },
  {
    "text": "do we need for supervised\nfine-tuning and RLHF, and we wanted to like, you know,\nnot worry about pre-training,",
    "start": "87340",
    "end": "94720"
  },
  {
    "text": "instead take an open\nsource pre-trained model and recreate the secret\nsauce of alignment on it.",
    "start": "94720",
    "end": "100689"
  },
  {
    "text": "And the procedure that we\nwanted to follow and replicate on open source is this figure\nthat I'm pretty sure most of you",
    "start": "100690",
    "end": "107260"
  },
  {
    "text": "are familiar with\nat this point, it's like from this InstructGPT\npaper from OpenAI,",
    "start": "107260",
    "end": "112340"
  },
  {
    "text": "which shows three steps. I'm going to go into a\nbit more detail on this",
    "start": "112340",
    "end": "117979"
  },
  {
    "text": "because this slide\nis much smaller here, but this is what the outline\nof the talk looks like.",
    "start": "117980",
    "end": "123259"
  },
  {
    "text": "I'll be getting\nthe detail of like, you know, how did we decide what\nkind of data, how much data, and all the details of the data\nfor supervised fine-tuning,",
    "start": "123260",
    "end": "131360"
  },
  {
    "text": "then similarly\nfor RLHF, then I'm going to talk about distillation\nof language model alignment,",
    "start": "131360",
    "end": "138440"
  },
  {
    "text": "then experiments with\ndifferent helpfulness recipes, finaly talk about\nevaluation of these models,",
    "start": "138440",
    "end": "145040"
  },
  {
    "text": "and quirks of using\nGPT-4 as an evaluator. OK. So this is kind\nof like, you know,",
    "start": "145040",
    "end": "151450"
  },
  {
    "text": "what overall recipe that\nInstructGPT paper from OpenAI put forward as the steps\nfor training a chatbot.",
    "start": "151450",
    "end": "159820"
  },
  {
    "text": "So the first step over here is\nto do supervised fine-tuning. Essentially you're doing fine\ntuning with human instruction",
    "start": "159820",
    "end": "167230"
  },
  {
    "text": "demonstration data, so\nthe input and the output are both given by humans. The step two is the input\nis given by a human,",
    "start": "167230",
    "end": "175270"
  },
  {
    "text": "the output comes from\nmodels, and then the human just reads thumbs up,\nthumbs down or ranks them.",
    "start": "175270",
    "end": "180628"
  },
  {
    "text": "And then you train a reward\nmodel, which is essentially just a classifier. And then the final step\nthree is doing fine-tuning",
    "start": "180628",
    "end": "186700"
  },
  {
    "text": "using that reward model\nwith reinforcement learning. And so the way I'm looking\nat is like step one is",
    "start": "186700",
    "end": "193300"
  },
  {
    "text": "more for making a\nmodel into a helpful chatbot, and the steps\ntwo and are essentially",
    "start": "193300",
    "end": "199360"
  },
  {
    "text": "trying to add those guardrails\nin place for harmlessness So let's get started with\ntalking about helpfulness,",
    "start": "199360",
    "end": "206360"
  },
  {
    "text": "and most of my talk today will\nbe focused on the step one.",
    "start": "206360",
    "end": "211370"
  },
  {
    "text": "So let's start diving\ndeeper into this, and let's start out\nwith the dataset.",
    "start": "211370",
    "end": "216860"
  },
  {
    "text": "Like, how do we decide\nwhat we need for doing the supervised fine-tuning?",
    "start": "216860",
    "end": "221930"
  },
  {
    "text": "So the dataset for helpfulness\nfor supervised fine-tuning looks somewhat like this.",
    "start": "221930",
    "end": "227489"
  },
  {
    "text": "This is from the\nself-instruct paper. If you are aware of that\nfrom end of last year, so you have something\nthat we call",
    "start": "227490",
    "end": "233600"
  },
  {
    "text": "as a task, which then has an\ninstruction which is essentially a request by a user asking\nthe model to fulfill or give",
    "start": "233600",
    "end": "241700"
  },
  {
    "text": "a response to a certain task. And that is followed by\nan input and an output.",
    "start": "241700",
    "end": "246940"
  },
  {
    "text": "The input in this\ncase is optional. It could just be part\nof the instruction, and then the output\nis the expected output",
    "start": "246940",
    "end": "253910"
  },
  {
    "text": "that the model should generate. But while we are\ndoing this training, the human provides\nthe expected output",
    "start": "253910",
    "end": "259609"
  },
  {
    "text": "that the model would have\ngenerated in the actual test case. And so here the\ninput and the output",
    "start": "259610",
    "end": "265730"
  },
  {
    "text": "are called instance or\ndemonstration or completion, and that's why this is called\ninstruction demonstration.",
    "start": "265730",
    "end": "274280"
  },
  {
    "text": "So this is just a\nhigh level landscape of what these datasets for\ninstruction demonstration",
    "start": "274280",
    "end": "280550"
  },
  {
    "text": "look like, and you must have\nbeen familiar with at least some of these.",
    "start": "280550",
    "end": "285560"
  },
  {
    "text": "And the way I've tried to put\nthis is on this line where on one side, I'm showing\ndatasets that were generated",
    "start": "285560",
    "end": "293600"
  },
  {
    "text": "using models or more\npowerful language models, and so they're more\nsynthetic data sets.",
    "start": "293600",
    "end": "298920"
  },
  {
    "text": "On the right, I'm showing\nlike human written datasets, and so these are datasets that\nthe human wrote the input as",
    "start": "298920",
    "end": "307400"
  },
  {
    "text": "well as the expected output. And so examples\nof these are like, you know, so the\nsurge-instruct is the data set that we at\nHugging Face H4, you know,",
    "start": "307400",
    "end": "315620"
  },
  {
    "text": "contracted with this\ncompany that, you know, that basically had contracts\nwith annotators that",
    "start": "315620",
    "end": "322070"
  },
  {
    "text": "were writing the\ninputs and outputs, but we had to give them all\nthe specifications of what kind of data we need.",
    "start": "322070",
    "end": "328100"
  },
  {
    "text": "And then you must\nhave heard of-- obviously, Open Assistant is\nthis other community wide effort where people\ncontributed manually",
    "start": "328100",
    "end": "334700"
  },
  {
    "text": "writing inputs and outputs,\nsimilarly with Dolly. And then on the\nother hand, you can see the self-instruct\ninstruct dataset.",
    "start": "334700",
    "end": "341570"
  },
  {
    "text": "I'm going to dive\ninto some of these. How are these\nsynthetic data sets created for helpfulness or\nfor supervised fine tuning?",
    "start": "341570",
    "end": "350960"
  },
  {
    "text": "So one of the examples of how\nthe synthetic data is created is in the self construct\npaper, which is",
    "start": "350960",
    "end": "356750"
  },
  {
    "text": "called bootstrapping the data. So in this case, they start with\n175 seat task that is a 175--",
    "start": "356750",
    "end": "363860"
  },
  {
    "text": "like a very small\ndataset of examples where the manually written\ninputs and outputs from humans,",
    "start": "363860",
    "end": "370159"
  },
  {
    "text": "those are added to a task\npool, then a language model-- basically, you\nbootstrap by giving that",
    "start": "370160",
    "end": "376865"
  },
  {
    "text": "to the language model\nand a few shot setting, and ask it to generate\nmore data like that.",
    "start": "376865",
    "end": "382250"
  },
  {
    "text": "And then you have another\nlanguage model that does this task classification. Like, you know, what\nkind of task is this?",
    "start": "382250",
    "end": "388400"
  },
  {
    "text": "Is this sample or the\nexample are belonging to? And finally, it also does this\nmore fine grained classification",
    "start": "388400",
    "end": "394940"
  },
  {
    "text": "as to like, you know, does it\nhave, you know, output first or does it require\ninput first and so on.",
    "start": "394940",
    "end": "400080"
  },
  {
    "text": "And because this\nis synthetic data and created in this like\na very scalable way, you also have to do a lot\nof filtering to make sure",
    "start": "400080",
    "end": "406850"
  },
  {
    "text": "that it is very high quality. So another way of generating\nthis kind of synthetic data",
    "start": "406850",
    "end": "413759"
  },
  {
    "text": "is what UltraChat did. And in this case, they had like\na human in the loop process.",
    "start": "413760",
    "end": "418960"
  },
  {
    "text": "So a human would look up\nlike either, you know, search Wikipedia or something,\nand then come up with topics",
    "start": "418960",
    "end": "427530"
  },
  {
    "text": "that they want to generate data\nfor, and then ask the model, like, provide it with the\nrequired material that",
    "start": "427530",
    "end": "434310"
  },
  {
    "text": "would be needed for coming up\nwith, say, question answering or summarization or any\nof these specific tasks,",
    "start": "434310",
    "end": "440670"
  },
  {
    "text": "and then give it to a more\npowerful model like ChatGPT or GPT-4. In this case, it was\nChatGPT and then--",
    "start": "440670",
    "end": "446639"
  },
  {
    "text": "actually, GPT-4,\nand then you keep doing these loops of giving\nthe material to the model",
    "start": "446640",
    "end": "452910"
  },
  {
    "text": "and say like, come up\nwith questions and answers of on this particular task\nusing all this material. And then the human\nlooks at it, and then",
    "start": "452910",
    "end": "460800"
  },
  {
    "text": "keeps querying it and\nrefining it more and more. So this is another way of\ncreating synthetic data. Obviously, this has\na human sitting there",
    "start": "460800",
    "end": "467639"
  },
  {
    "text": "and doing a lot more\nfiltering in the process. Then there's\nanother one which is",
    "start": "467640",
    "end": "472690"
  },
  {
    "text": "like even less human involved,\nwhich is roleplaying, And this is the CAMEL dataset.",
    "start": "472690",
    "end": "477970"
  },
  {
    "text": "In this case, all\nthat the human does is come up with an idea of what\ntask or what example they want.",
    "start": "477970",
    "end": "486200"
  },
  {
    "text": "So at a high level, it would\nbe like develop a trading bot for the stock market,\nand there would be two LLMs.",
    "start": "486200",
    "end": "492110"
  },
  {
    "text": "One would be role playing\nas an AI assistant, the other would be\nrole playing as a user,",
    "start": "492110",
    "end": "498020"
  },
  {
    "text": "and then they basically\njust specify the task and let these two bots\nchat with each other",
    "start": "498020",
    "end": "503290"
  },
  {
    "text": "and create a\nconversation dataset, which is like a synthetic data\nset for supervised fine-tuning.",
    "start": "503290",
    "end": "511910"
  },
  {
    "text": "So this is just going\nback to this landscape, it looks like, you know, there\npeople have been very creative,",
    "start": "511910",
    "end": "518669"
  },
  {
    "text": "and how do we get\nvery high quality data quickly without\nspending a lot of money.",
    "start": "518669",
    "end": "523729"
  },
  {
    "text": "And because humans are\ninefficient and expensive, and so these are like, you know,\nsome examples that we looked at.",
    "start": "523730",
    "end": "529440"
  },
  {
    "text": "But on the other hand, we also\ncannot underestimate how good quality the manually\ncreated datasets are.",
    "start": "529440",
    "end": "536670"
  },
  {
    "text": "And so we at\nHugging Face decided to go with everything\nvery manual",
    "start": "536670",
    "end": "542390"
  },
  {
    "text": "and have humans do both\nthe input and the output. Also go figure out what\nare the essential documents",
    "start": "542390",
    "end": "549140"
  },
  {
    "text": "or other material they\nneed for coming up with creating this data.",
    "start": "549140",
    "end": "554570"
  },
  {
    "text": "But when we started doing that,\nwe were earlier in the year, so this is back in January\nor February of this year,",
    "start": "554570",
    "end": "560240"
  },
  {
    "text": "and this is what the landscape\nlooked like at that time. And so there was very\nlittle datasets available.",
    "start": "560240",
    "end": "566190"
  },
  {
    "text": "A lot of these were mostly\nsynthetically created. So we wanted to kind of leverage\nwhat was existing out there,",
    "start": "566190",
    "end": "573550"
  },
  {
    "text": "but we also had to make some\nreally important decisions because we're going to\npay money and make sure that the data that we collect\nis actually useful for building",
    "start": "573550",
    "end": "581820"
  },
  {
    "text": "the model and the applications\nthat are built on top of it. So these are the learnings that\nwe had from the past papers",
    "start": "581820",
    "end": "589240"
  },
  {
    "text": "that were creating these\nsupervised fine-tuning datasets. We knew that the\ndataset has to be",
    "start": "589240",
    "end": "594670"
  },
  {
    "text": "in the range of tens of\nthousands of examples. So this is from the\nself-instruct dataset.",
    "start": "594670",
    "end": "599920"
  },
  {
    "text": "And we also knew that\nthese models that are trained on this data set\nshowed diminishing returns",
    "start": "599920",
    "end": "606430"
  },
  {
    "text": "after just a few thousand\nhigh quality instructions. So you don't need a lot, and\nthen it saturates very quickly.",
    "start": "606430",
    "end": "612688"
  },
  {
    "text": "So these are the\ntwo findings that we had when we started\nto go collect data set for supervised fine-tuning.",
    "start": "612688",
    "end": "620000"
  },
  {
    "text": "But we also had to give some\nvery fine grained specifications on what we want\nfor our data set.",
    "start": "620000",
    "end": "625880"
  },
  {
    "text": "In particular, we\nhad to decide what is the task distribution\nwe want for the data that we are collecting. I mean, we know it's\ntens of thousands,",
    "start": "625880",
    "end": "632495"
  },
  {
    "text": "but how many thousands\nof what task, right? The length distribution,\nlike, you know, should the prompt\nhave a certain length?",
    "start": "632495",
    "end": "639240"
  },
  {
    "text": "Is that even an\nimportant factor? And one thing is\nthat we wanted we had decided that\nwe want to make it",
    "start": "639240",
    "end": "645110"
  },
  {
    "text": "high quality and human\nwritten, but then there were like options\non that as well. We could go with\nexternal vendors",
    "start": "645110",
    "end": "650959"
  },
  {
    "text": "like Surge, Scale AI, AWS\nGround Truth, and so on, or we could hire\nour own contractors",
    "start": "650960",
    "end": "656840"
  },
  {
    "text": "from Upwork and MTurk. And so those were decisions\nthat we had to make. So let's look at each\nof these one by one.",
    "start": "656840",
    "end": "664550"
  },
  {
    "text": "So because we were recreating\nthis InstructGPT recipe for this helpful chatbot, we\nwanted to take inspiration",
    "start": "664550",
    "end": "672350"
  },
  {
    "text": "from their task distribution. So on the left, I'm showing\nthe task distribution that InstructGPT did for--",
    "start": "672350",
    "end": "679010"
  },
  {
    "text": "OpenAI did for the\nInstructGPT paper. As you can see, that generation\nis the majority of it,",
    "start": "679010",
    "end": "684530"
  },
  {
    "text": "followed by some of\nthese open ended tasks and brainstorming\ntasks and so on. And these are examples of what\nprompts of each of those look",
    "start": "684530",
    "end": "692720"
  },
  {
    "text": "like. So we decided to\njust go with that, but instead, you\nmust have noticed",
    "start": "692720",
    "end": "697790"
  },
  {
    "text": "that there's this category\ncalled other in the table, and we obviously don't\nknow what that was,",
    "start": "697790",
    "end": "703130"
  },
  {
    "text": "but so we decided to\nreplace that with code. So essentially it would be like\ndebugging, asking, clarification",
    "start": "703130",
    "end": "709298"
  },
  {
    "text": "questions about the code. So it's like code\nplus natural language. So this is what our final\ndistribution looked like.",
    "start": "709298",
    "end": "717470"
  },
  {
    "text": "The second question was\nthe length distribution. So we also had to figure out\nhow important is the length",
    "start": "717470",
    "end": "724010"
  },
  {
    "text": "and should we have a\ncertain length distribution? Then we ask these companies\nto collect data for us.",
    "start": "724010",
    "end": "730170"
  },
  {
    "text": "So we did a pilot study\nwith Surge, Scale AI and AWS SageMaker Ground Truth, which\nis more like a managed service.",
    "start": "730170",
    "end": "736582"
  },
  {
    "text": "So it's very\ndifferent from MTurk, and they have very\nhigh quality human--",
    "start": "736583",
    "end": "741680"
  },
  {
    "text": "humans like basically\nwriting these examples. And so I wanted to\njust highlight that,",
    "start": "741680",
    "end": "747620"
  },
  {
    "text": "this or the first two rows here\nshow what the InstructGPT length distribution looks like.",
    "start": "747620",
    "end": "753448"
  },
  {
    "text": "And as you can see, this is\nobviously the full dataset, this is more like pilot, so like\nthe counts are much smaller,",
    "start": "753448",
    "end": "759180"
  },
  {
    "text": "but you can see the\nmaximum is 2048. And as we know, like that was\nlike the standard context size",
    "start": "759180",
    "end": "765150"
  },
  {
    "text": "in the beginning of the year. And then there is obviously\nlike even the mean, and that it's not--\nlike basically,",
    "start": "765150",
    "end": "772200"
  },
  {
    "text": "it's more or less, you know, in\nthe range, but if you look at, these examples from\nSurge, AWS, or Scale AI,",
    "start": "772200",
    "end": "778050"
  },
  {
    "text": "there's very high variance. So for example, AWS SageMaker,\nthe maximum prompt length",
    "start": "778050",
    "end": "783420"
  },
  {
    "text": "is 1936, but then\nthe mean is just 54. And on the other hand, with\nSurge, the maximum length",
    "start": "783420",
    "end": "790970"
  },
  {
    "text": "is 500, but then the\nmean is much like 104. So it's like more\nin the range of what",
    "start": "790970",
    "end": "797150"
  },
  {
    "text": "we would expect from this\ndifference in InstructGPT. And similarly with\nScale AI, we found",
    "start": "797150",
    "end": "802730"
  },
  {
    "text": "that there are prompts\nwhich is very, very short. And so just based\non this, we said",
    "start": "802730",
    "end": "809060"
  },
  {
    "text": "that, you know, OK,\nwe should probably just go with Surge because\nthat seems like something that is more, you know, in the\nrange, not very high variance.",
    "start": "809060",
    "end": "819480"
  },
  {
    "text": "So we ended up collecting 10,000\ninstruction demonstration pairs from search, and this is what\nthe task distribution looked",
    "start": "819480",
    "end": "826880"
  },
  {
    "text": "like. So this very much follows the\ntask distribution InstructGPT, except for the\ncoding part, which",
    "start": "826880",
    "end": "832470"
  },
  {
    "text": "was like the other\ncategory over there. And these are the number\nof examples we collected for each of these tasks.",
    "start": "832470",
    "end": "838589"
  },
  {
    "text": "And year over year, I'm\nshowing the average length for each of these\ntask categories.",
    "start": "838590",
    "end": "843673"
  },
  {
    "text": "And one thing I wanted\nto highlight was, which was very surprising to\nme, is that the chat is actually one of the shortest\nprompt length categories,",
    "start": "843673",
    "end": "852600"
  },
  {
    "text": "but for OpenAI, that is actually\none of the longest prompt length categories, which\nwas very interesting.",
    "start": "852600",
    "end": "858640"
  },
  {
    "text": "And so obviously, at that time,\nwe did not think much about it, but when we started\ntraining models",
    "start": "858640",
    "end": "864900"
  },
  {
    "text": "and started looking at\nthe evaluation results, we were like, you\nknow, if we had to go back and change things,\nhow would we change that?",
    "start": "864900",
    "end": "871980"
  },
  {
    "text": "And so these were\nlike things that we started looking at more\ncarefully after we had already",
    "start": "871980",
    "end": "877183"
  },
  {
    "text": "collected the dataset.  So here are examples\nof what that dataset",
    "start": "877183",
    "end": "884000"
  },
  {
    "text": "looked like, you\nknow, classification, generation, brainstorming. I'm sure you all must\nhave seen at least some",
    "start": "884000",
    "end": "890210"
  },
  {
    "text": "of these kind of examples of\ninstruction, demonstration datasets. So it's very much\nlike it has everything",
    "start": "890210",
    "end": "896120"
  },
  {
    "text": "that you can expect\nfrom NLP kind of tasks, but also more open ended\nchatty tasks as well.",
    "start": "896120",
    "end": "903370"
  },
  {
    "text": " OK, so here are some\ndetails about the task force",
    "start": "903370",
    "end": "911960"
  },
  {
    "text": "that was used by Surge\nto generate this dataset. We requested a US-based\nbased task force mainly",
    "start": "911960",
    "end": "918260"
  },
  {
    "text": "because, like I said, we\njust wanted to replicate what InstructGPT was doing, and based\non Anthropic and OpenAI's paper,",
    "start": "918260",
    "end": "925040"
  },
  {
    "text": "it seemed like they prefer going\nwith the US-based based task force. The gender, roughly equally\ndivided, and the age range",
    "start": "925040",
    "end": "932840"
  },
  {
    "text": "was also very, you know, very\nit was like a big range going all the way from 19 to 62.",
    "start": "932840",
    "end": "939230"
  },
  {
    "text": "And then people had\neducation background ranges from technical degree to PhD.",
    "start": "939230",
    "end": "944460"
  },
  {
    "text": "So PhD was mainly for tasks\nlike math, coding, and so on. ",
    "start": "944460",
    "end": "950760"
  },
  {
    "text": "OK, so now I wanted to\nswitch gears a little bit, and talk about\nthis, this data set",
    "start": "950760",
    "end": "956310"
  },
  {
    "text": "that we collected for RLHF\nor for human preferences before I get into\nthe experiments",
    "start": "956310",
    "end": "962910"
  },
  {
    "text": "we ran with this supervised\nfine-tuning data set and what results we got.",
    "start": "962910",
    "end": "967920"
  },
  {
    "text": "So again, over here, while we\nwere collecting human preference data set, we had to\ncome up with what",
    "start": "967920",
    "end": "973589"
  },
  {
    "text": "are the specifications\nof these data sets. So again, just to contrast\nthis with how is it different from SFT, the SFT\ndata set, both the input",
    "start": "973590",
    "end": "982050"
  },
  {
    "text": "and the output are\nwritten by humans. In this case, the human\nrights, the input, the output comes from\nmodels, which is responses,",
    "start": "982050",
    "end": "988950"
  },
  {
    "text": "but then the human just ranks or\nrates them on a certain scale.",
    "start": "988950",
    "end": "994920"
  },
  {
    "text": "So yeah, essentially\nwe had to decide like, what is the\ntask distribution looks like for RLHF data?",
    "start": "994920",
    "end": "1000980"
  },
  {
    "text": "Is it going to be same as\nsupervised fine-tuning? What about the\nlength distribution? And should we do like single\nturn versus multi-turn?",
    "start": "1000980",
    "end": "1008700"
  },
  {
    "text": "So in InstructGPT it\nwas mainly single turn. So if we are trying to\nreplicate InstructGPT, we would have to go\nwith single turn,",
    "start": "1008700",
    "end": "1015089"
  },
  {
    "text": "but if we are trying\nto replicate something like ChatGPT, it would have to\nbe like a multi-turn dialogue.",
    "start": "1015090",
    "end": "1021170"
  },
  {
    "text": "And then we had\nto also like, you know, decide on these dimensions\nof like helpfulness, honesty",
    "start": "1021170",
    "end": "1026810"
  },
  {
    "text": "and harmlessness. So these are like the HHH\nthat Anthropic follows, like OpenAI puts it as like\nhelpfulness, truthfulness",
    "start": "1026810",
    "end": "1033319"
  },
  {
    "text": "and harmlessness. And then also we had to\ndecide like, you know, are they going to rate\neach of the responses",
    "start": "1033319",
    "end": "1038869"
  },
  {
    "text": "individually or are\nthey going to rank them? And what are the\nimplications of us deciding one way or the other?",
    "start": "1038869",
    "end": "1046970"
  },
  {
    "text": "So we started by doing\na pilot study again. So we took 300 prompts from\nthe self-instruct dataset,",
    "start": "1046970",
    "end": "1054570"
  },
  {
    "text": "the data set that was\nreleased end of last year, and then gave it a generated\nmodel responses from our models,",
    "start": "1054570",
    "end": "1061820"
  },
  {
    "text": "and then gave it to\ndata vendors to rate the responses of the models. And we used this\nAnthropic template",
    "start": "1061820",
    "end": "1068120"
  },
  {
    "text": "on the left, which\nis essentially asking the human\nto choose the most helpful and honest response.",
    "start": "1068120",
    "end": "1074309"
  },
  {
    "text": "And then these are the responses\nfrom model A and model B, and this is a\nscale, which is also",
    "start": "1074310",
    "end": "1080240"
  },
  {
    "text": "working as sort of a\nranking thing in the sense that one to four is like\ndecreasingly model A,",
    "start": "1080240",
    "end": "1086600"
  },
  {
    "text": "and five to eight is\nincreasingly model B.",
    "start": "1086600",
    "end": "1092020"
  },
  {
    "text": "And also one other thing\nwe had to decide about is how much data\nshould we collect.",
    "start": "1092020",
    "end": "1097780"
  },
  {
    "text": "And so again, this is from\nthe InstructGPT paper. And as you can see, they\nhave the train and validation",
    "start": "1097780",
    "end": "1104170"
  },
  {
    "text": "splits for each of\nthe three steps, which are the SFD, training the\nreward model, and the PPO.",
    "start": "1104170",
    "end": "1109600"
  },
  {
    "text": "And this one is in the\norder of tens of thousands, and like over all this combined,\nwhich is like, you know,",
    "start": "1109600",
    "end": "1115150"
  },
  {
    "text": "this process of RLHF,\ncomes up to about 100,000.",
    "start": "1115150",
    "end": "1123130"
  },
  {
    "text": "Great. OK. So then once we got this pilot\nstudy data back, we sat down",
    "start": "1123130",
    "end": "1128460"
  },
  {
    "text": "and we wanted to also-- so I looked at it\nmanually, and I felt that I did not agree\nwith most of the answers",
    "start": "1128460",
    "end": "1134910"
  },
  {
    "text": "that the annotators from each of\nthese companies were providing. And so I was kind\nof like, you know,",
    "start": "1134910",
    "end": "1140880"
  },
  {
    "text": "I don't think this is\nhigh quality at all. So what I decided is like,\nyou know, I told my team, let's go and rate\nit within ourselves.",
    "start": "1140880",
    "end": "1148240"
  },
  {
    "text": "And then, we basically rated\nabout 100 examples or so, and we followed a similar\ntemplate of one to four and five",
    "start": "1148240",
    "end": "1156309"
  },
  {
    "text": "to eight. Basically, the output,\nlike, you know, the takeaway was that even we did not\nagree amongst each other.",
    "start": "1156310",
    "end": "1162880"
  },
  {
    "text": "So essentially, like our models\nearlier in the year were so bad, you were essentially breaking\nties like arbitrarily.",
    "start": "1162880",
    "end": "1169470"
  },
  {
    "text": "Like, you know, you're deciding\nbetween like, should it be like, you know, three versus seven\nor something like that.",
    "start": "1169470",
    "end": "1175120"
  },
  {
    "text": "So if they're equally\nbad, it's hard to decide which one is better, right? And so we were breaking some\nof these ties arbitrarily.",
    "start": "1175120",
    "end": "1182050"
  },
  {
    "text": "And so as you can see there was\nbarely agreement or correlation",
    "start": "1182050",
    "end": "1187090"
  },
  {
    "text": "among our outputs. And then, you know, when I\naggregated that and looked at how well do we correlate\nwith like, for example,",
    "start": "1187090",
    "end": "1194500"
  },
  {
    "text": "Surge and Scale, and so we\ndecided Scale AI, we had more-- like, the maximum\noverlap was with Scale",
    "start": "1194500",
    "end": "1201460"
  },
  {
    "text": "compared to like, say, Surge.  So we ended up collecting\n20,000 dialogues,",
    "start": "1201460",
    "end": "1208990"
  },
  {
    "text": "so we decided to\ngo with multi-turn. And because it was multi-turn,\nyou would have 20,000 overall",
    "start": "1208990",
    "end": "1215640"
  },
  {
    "text": "dialogues, but the number\nof prompts would be 80,000. So there would be each\ndialogue would have about four turns on an average.",
    "start": "1215640",
    "end": "1222010"
  },
  {
    "text": "So a human would prompt it,\nthe model would respond. a human would read\nthe response, and then",
    "start": "1222010",
    "end": "1227850"
  },
  {
    "text": "ask the follow up\nquestion, and then again, the model would\ngenerate two responses, and that is how it would go on.",
    "start": "1227850",
    "end": "1235080"
  },
  {
    "text": "And so the task distribution\nwe decided to follow was a little bit\ndifferent from what we",
    "start": "1235080",
    "end": "1240900"
  },
  {
    "text": "had for supervised fine-tuning. and the reason\nbehind that we wanted to focus more on tasks\nthat were like factual.",
    "start": "1240900",
    "end": "1250920"
  },
  {
    "text": "Essentially, this\nis more about making the model learn between\npositive and negative signals. So making the model like\ndiscriminate between like,",
    "start": "1250920",
    "end": "1258445"
  },
  {
    "text": "you know, what is\nfactual, what is not, what is helpful, what is\nnot, and what is harmless and what is not.",
    "start": "1258445",
    "end": "1263880"
  },
  {
    "text": "And for example, tasks\ngeneration and brainstorming, there's no one correct answer.",
    "start": "1263880",
    "end": "1269380"
  },
  {
    "text": "Everyone can come up with\ndifferent lists or recipes, and it's hard to say,\nis this the best answer?",
    "start": "1269380",
    "end": "1274960"
  },
  {
    "text": "Is it the most helpful answer? But if you ask a\nfactual question, it's very clear what is\ncorrect and what is not.",
    "start": "1274960",
    "end": "1281770"
  },
  {
    "text": "So that was kind of like our\nreasoning behind doing this. And so this is a\ntask distribution that we came up with for\ncollecting the human preference",
    "start": "1281770",
    "end": "1288669"
  },
  {
    "text": "dataset. Also about the\nlength, because we are doing this in a\nmulti-turn setting,",
    "start": "1288670",
    "end": "1295289"
  },
  {
    "text": "and so we wanted to make sure\nthe entire dialogue could fit into the context\nof the models, we",
    "start": "1295290",
    "end": "1301190"
  },
  {
    "text": "decided to ask them to keep the\noverall dialogue to be shorter than 2048 tokens.",
    "start": "1301190",
    "end": "1308090"
  },
  {
    "text": "And then it was\nmulti-turn with an average of four turns per dialogue. Then obviously we\nhad to also select",
    "start": "1308090",
    "end": "1314390"
  },
  {
    "text": "on the dimension of whether\nwe are going for helpful, or harmless or,\nyou know, honesty.",
    "start": "1314390",
    "end": "1320809"
  },
  {
    "text": "So we followed this instructions\nfrom this OpenAI guidelines. I'm not sure if I\ncan pull this up.",
    "start": "1320810",
    "end": "1326130"
  },
  {
    "text": "That would be nice. OK, great.",
    "start": "1326130",
    "end": "1331590"
  },
  {
    "text": "But yeah, so OpenAI has this\ndocument, which is public, of labeling\ninstructions that they",
    "start": "1331590",
    "end": "1337740"
  },
  {
    "text": "shared with their annotators. And so they have-- obviously,\nlike I said, they have helpful, thoughtful, and harmless, but\nthen they also have this thing--",
    "start": "1337740",
    "end": "1346030"
  },
  {
    "text": "how do I scroll down? OK. So they have definitions what do\nthey mean by helpfulness, what",
    "start": "1346030",
    "end": "1352440"
  },
  {
    "text": "do they mean by truthfulness,\nand what do they mean by harmlessness. So in our case, because our\nmodels were not as good,",
    "start": "1352440",
    "end": "1359190"
  },
  {
    "text": "we decided to focus on\nhelpfulness and truthfulness. And when they had to\nbreak ties, OpenAI",
    "start": "1359190",
    "end": "1364770"
  },
  {
    "text": "says that, you know, choose\ntruthfulness over helpfulness over here. So let me see that.",
    "start": "1364770",
    "end": "1370630"
  },
  {
    "text": "Where is that? ",
    "start": "1370630",
    "end": "1380985"
  },
  {
    "text": "Yeah.  So they wanted to prioritize\nharmlessness and truthfulness",
    "start": "1380985",
    "end": "1387797"
  },
  {
    "text": "over helpfulness, but we went\nwith the other way around. We said we wanted to prioritize\nhelpfulness over honesty",
    "start": "1387797",
    "end": "1394050"
  },
  {
    "text": "or harmlessness because-- I mean, we weren't even\nfocusing on harmlessness because we just wanted\nto get our model",
    "start": "1394050",
    "end": "1399420"
  },
  {
    "text": "to a certain capabilities before\nwe start thinking about that. But yeah, this is really\na very good document",
    "start": "1399420",
    "end": "1406170"
  },
  {
    "text": "and defines what should the\nannotator be looking at, and how do they decide when the\nmodel responses are very close,",
    "start": "1406170",
    "end": "1413590"
  },
  {
    "text": "how do they break those ties? And for deciding between\nwhat kind of template",
    "start": "1413590",
    "end": "1419890"
  },
  {
    "text": "should be used for\ncollecting these annotations, we started off with\nthe Anthropic template that I showed a few\nslides earlier, which",
    "start": "1419890",
    "end": "1426040"
  },
  {
    "text": "was on a scale of one\nto eight but essentially ranking between\nthese two models, and then LLaMA2\ncame out while we",
    "start": "1426040",
    "end": "1432220"
  },
  {
    "text": "were in this iterative process. And iterative process\nwas essentially we used to give an\nendpoint to the vendor,",
    "start": "1432220",
    "end": "1438370"
  },
  {
    "text": "and then basically\nthe annotators that they had in the\nmanage task force would prompt these end points.",
    "start": "1438370",
    "end": "1444830"
  },
  {
    "text": "The model would\nrespond to response, would generate two responses. They would follow\nthe instructions",
    "start": "1444830",
    "end": "1450700"
  },
  {
    "text": "and give the ranking for each\nof those model responses, and then, you know, again,\nfollow up with the second prompt",
    "start": "1450700",
    "end": "1458035"
  },
  {
    "text": "and the conversation\nwould go on, and then they would give us the\ndata at the end of that week. We would fine tune\nour model on that data",
    "start": "1458035",
    "end": "1464560"
  },
  {
    "text": "so that the model now\nis hopefully better, and then we give\na better endpoint to them for the next week\nto continue this process.",
    "start": "1464560",
    "end": "1471590"
  },
  {
    "text": "So it's like very\niterative, and they have to adapt to model\ngetting better week by week.",
    "start": "1471590",
    "end": "1478233"
  },
  {
    "text": "So yeah, basically--\nbut like, you know, we decided to switch to I\nthink for one or two weeks we collected Anthropics--",
    "start": "1478233",
    "end": "1484370"
  },
  {
    "text": "use Anthropic scale for\ncollecting data set. But then LLaMA2 came\nout, and they results",
    "start": "1484370",
    "end": "1489980"
  },
  {
    "text": "showed that, you know,\nclearly that they were using this much more easier\nscale of just one to four.",
    "start": "1489980",
    "end": "1495840"
  },
  {
    "text": "So they were choosing\nwhich one is a better response between\nthe two responses, and then saying how\nmuch better it is.",
    "start": "1495840",
    "end": "1503309"
  },
  {
    "text": "So is it like\nsignificantly better or is it only slightly better? And so that was the ranking\nof like scale one to four.",
    "start": "1503310",
    "end": "1510470"
  },
  {
    "text": "So here are examples of\ndata that we collected. So on the left, you can see\nthat it is asking about--",
    "start": "1510470",
    "end": "1518750"
  },
  {
    "text": "basically, a human is\nprompting with a question, and then the bot\ngenerates a response. And then so this is the\nresponse that the human chose",
    "start": "1518750",
    "end": "1525890"
  },
  {
    "text": "at this turn, and then\nthe human, you know, follows up with\nthe second prompt. And then this is\nthe bot response",
    "start": "1525890",
    "end": "1532010"
  },
  {
    "text": "that was chosen by this human,\nand this is the rejected bot response. And this is giving\nthe response margin",
    "start": "1532010",
    "end": "1537289"
  },
  {
    "text": "of three, which is saying that\nthey are quite a bit different. So four is very\ndifferent, and one being very slightly different.",
    "start": "1537290",
    "end": "1544654"
  },
  {
    "text": "And then you're\non the right hand side is more about generation\nbrainstorming example wherein",
    "start": "1544655",
    "end": "1550410"
  },
  {
    "text": "the human is asking like,\ncan you write a text message wishing their husband\na happy anniversary,",
    "start": "1550410",
    "end": "1555970"
  },
  {
    "text": "and then the bot\nwrites something. I guess my thing\nmessed up the emojis, but you know, then the human\nfollows up with saying,",
    "start": "1555970",
    "end": "1563700"
  },
  {
    "text": "hey, you missed this important\ndetail, which is, you know, they've been married\nfor eight years. And so this is a\nchosen bot response.",
    "start": "1563700",
    "end": "1569890"
  },
  {
    "text": "This is the rejected one\nthat the human, you know, chose between those two. And as you can see,\nthey're quite good.",
    "start": "1569890",
    "end": "1575920"
  },
  {
    "text": "So the response\nmargin is just one. So they're just\nslightly different. ",
    "start": "1575920",
    "end": "1583750"
  },
  {
    "text": "OK, sounds good. So now I'm going to talk\nabout this another recipe that we tried, which is using\nsynthetic dataset essentially",
    "start": "1583750",
    "end": "1592780"
  },
  {
    "text": "for distillation of AI\nalignment, which is basically the paper that we released\nlast week called Zephyr,",
    "start": "1592780",
    "end": "1599500"
  },
  {
    "text": "and which was like a 7 billion\nparameter model, which actually beat ChatGPT.",
    "start": "1599500",
    "end": "1604780"
  },
  {
    "text": "And this builds on top\nof the Mistral model, but I just wanted to like--",
    "start": "1604780",
    "end": "1610210"
  },
  {
    "text": "basically, we recreated some\nof the steps that were there on the InstructGPT\npaper, but now with using synthetic dataset.",
    "start": "1610210",
    "end": "1617350"
  },
  {
    "text": "And so the first one\nis like, you know, you are basically\nusing a dataset. In this case, we use UltraChat.",
    "start": "1617350",
    "end": "1623660"
  },
  {
    "text": "So this is a data set I\nshowed a few slides earlier for supervised fine-tuning\nwherein a human was",
    "start": "1623660",
    "end": "1628809"
  },
  {
    "text": "brainstorming and\ngathering the material, and then chatting\nwith this GPT model",
    "start": "1628810",
    "end": "1634600"
  },
  {
    "text": "to generate multiple different\noutputs for the instruction, and then know this is how\nwe collect that data set,",
    "start": "1634600",
    "end": "1641620"
  },
  {
    "text": "which is called the UltrChat,\nand then we use that for fine tuning our model. And then the second step is the\nresponse generation AI ranking.",
    "start": "1641620",
    "end": "1650960"
  },
  {
    "text": "So in this case, we used\nultra feedback, which is a dataset that was\nreleased, and the way",
    "start": "1650960",
    "end": "1657250"
  },
  {
    "text": "this dataset was constructed\nwas that they asked--",
    "start": "1657250",
    "end": "1662260"
  },
  {
    "text": "basically, took some prompts\nfrom ChatGPT and some of these different data sets of\nSFT that were already out there,",
    "start": "1662260",
    "end": "1669740"
  },
  {
    "text": "and then they gave it to\nfour different models, like four different\npowerful models like PaLM 2, Claude 2, GPT-4, and so on.",
    "start": "1669740",
    "end": "1677630"
  },
  {
    "text": "And then they asked this\nGPT-4 to rank, which one is-- rank each of those\nfour responses.",
    "start": "1677630",
    "end": "1683090"
  },
  {
    "text": "And then so the one that is\nthe best is the one that GPT-4 ranks as the highest. So each of these are\nscored individually",
    "start": "1683090",
    "end": "1689410"
  },
  {
    "text": "on a scale of 1 to\n10, AND the one that gets the maximum score is\nlike the best response. ",
    "start": "1689410",
    "end": "1696430"
  },
  {
    "text": "And then finally\nwe did something called DPO, which you might\nhave been aware of because it came out of Stanford.",
    "start": "1696430",
    "end": "1702250"
  },
  {
    "text": "It's like this\nkind of alternative to RLHF which is like doing this\ndirect preference optimization.",
    "start": "1702250",
    "end": "1708560"
  },
  {
    "text": "And so instead of\nlike basically doing this iterative process\nof fine-tuning,",
    "start": "1708560",
    "end": "1713649"
  },
  {
    "text": "you directly optimize\non the chosen one. So we just take that\nand then fine tune",
    "start": "1713650",
    "end": "1718750"
  },
  {
    "text": "our model directly on\nthat chosen response, and the other one\nthat we are using",
    "start": "1718750",
    "end": "1724269"
  },
  {
    "text": "is like a random response from\nthese other three responses.",
    "start": "1724270",
    "end": "1729500"
  },
  {
    "text": "OK, so I'm going to talk a\nlittle bit about experiments and evaluation for\neach of these recipes.",
    "start": "1729500",
    "end": "1735870"
  },
  {
    "text": "One is collecting everything\nwith humans involved, and the second one is\neverything, which is synthetic.",
    "start": "1735870",
    "end": "1742370"
  },
  {
    "text": "But then before I\ndiscuss evaluation, I wanted to talk about what\nare the benchmarks that we are evaluating on and how\ngood are these benchmarks",
    "start": "1742370",
    "end": "1749690"
  },
  {
    "text": "for evaluating chatbots? And to think about\nevaluation, we need to first think about how\nare we training these models.",
    "start": "1749690",
    "end": "1756570"
  },
  {
    "text": "So like today, all the\nmodels that are trained are more or less have these\nfour ways of learning.",
    "start": "1756570",
    "end": "1761970"
  },
  {
    "text": "The first one is pre-training\nthe language model, essentially predicting the next token,\nand examples of these",
    "start": "1761970",
    "end": "1767750"
  },
  {
    "text": "are like GPT-3, OPT, and so\nlike the foundation models. The second type of learning\nis incontext learning",
    "start": "1767750",
    "end": "1774770"
  },
  {
    "text": "or the prompt-based learning. In this case, you're like\njust giving a new kind of task in the context of the\nmodel, and then you",
    "start": "1774770",
    "end": "1782419"
  },
  {
    "text": "ask it to do that\non new examples. So if you wanted to write a\npoem, for example, for GPT-3,",
    "start": "1782420",
    "end": "1788120"
  },
  {
    "text": "you would have written\nthat in the context, and then it would have generated\nnew poem on some other topic.",
    "start": "1788120",
    "end": "1795110"
  },
  {
    "text": "The third type of learning is\nthe supervised fine-tuning, which was kind of like the first\nstep of training a chatbot.",
    "start": "1795110",
    "end": "1802500"
  },
  {
    "text": "But in this case, you're like\nfine tuning on the instruction, following data, and then you\nwant these language models,",
    "start": "1802500",
    "end": "1808110"
  },
  {
    "text": "which are just\npre-trained to predict the next token to become\nchatty and to generate open ended responses.",
    "start": "1808110",
    "end": "1814580"
  },
  {
    "text": "And then finally, the fourth\none is reinforcement learning from human feedback, which\nis nudging the language model",
    "start": "1814580",
    "end": "1820370"
  },
  {
    "text": "towards the values you desire. And examples include\nLLaMA2 chat from Meta. ",
    "start": "1820370",
    "end": "1827700"
  },
  {
    "text": "So the first two\nsteps are, you know, we have a lot of benchmarks for\nthese two types of training,",
    "start": "1827700",
    "end": "1834179"
  },
  {
    "text": "like Sanford HELM is an example\nof that or the Google BIG-bench or even Hugging Face\nopen LLM leaderboard.",
    "start": "1834180",
    "end": "1841290"
  },
  {
    "text": "But for these two\ntypes of learning, which is supervised fine-tuning\nand reinforcement learning",
    "start": "1841290",
    "end": "1846929"
  },
  {
    "text": "from human feedback,\nwhich are parts of this recipe for\ntraining a chatbot, not",
    "start": "1846930",
    "end": "1852389"
  },
  {
    "text": "a lot of leaderboards or\nevaluation benchmarks available, but there are some available,\nand I wanted to like, you know,",
    "start": "1852390",
    "end": "1857700"
  },
  {
    "text": "just highlight some of those. So like, yeah, this is\nessentially like the steps three and four.",
    "start": "1857700",
    "end": "1863360"
  },
  {
    "text": "Your match to like, you know,\nthe step one over here, which is helpfulness, and then steps\ntwo and three over here, which",
    "start": "1863360",
    "end": "1869259"
  },
  {
    "text": "is like, you know,\nnudging the model towards being more harmless.",
    "start": "1869260",
    "end": "1874800"
  },
  {
    "text": "So if you had to evaluate the\nchatbot for each of these steps, you would have to\nthink about, how",
    "start": "1874800",
    "end": "1881279"
  },
  {
    "text": "do you evaluate instruction\nfollowing a chattiness? You would have to\nthink about how do you evaluate the reward model, which\nis essentially a classifier?",
    "start": "1881280",
    "end": "1889240"
  },
  {
    "text": "And then finally\nthink about, how do you evaluate\nfor harmlessness, which is by red\nteaming or adversary",
    "start": "1889240",
    "end": "1894990"
  },
  {
    "text": "prompting the language model? So for the first step, you\nwould have to see like,",
    "start": "1894990",
    "end": "1900450"
  },
  {
    "text": "does the model generate\nuseful responses on the topic, and are they open ended? And one example of\na prompt that you",
    "start": "1900450",
    "end": "1906330"
  },
  {
    "text": "would try to evaluate\nthe model would be to like, say, brainstorm\na list of the New Year's resolution.",
    "start": "1906330",
    "end": "1912300"
  },
  {
    "text": "And so examples of benchmarks\nand evaluation boards that are looking at this sort\nof supervised fine-tuning",
    "start": "1912300",
    "end": "1919980"
  },
  {
    "text": "is like Hugging Face's\nleaderboard with Elo ratings. So Elo is this\nmetric that is used",
    "start": "1919980",
    "end": "1925860"
  },
  {
    "text": "in chess, which is\nlike you're pairing one player against the\nother, and you want to rank these players\nwhen they have",
    "start": "1925860",
    "end": "1931799"
  },
  {
    "text": "these tournaments\nagainst each other. And so in a similar sense, we\nare using taking these chatbots",
    "start": "1931800",
    "end": "1939460"
  },
  {
    "text": "and then putting them\nin a pairwise setting, and then we partnered\nwith ScaleAI,",
    "start": "1939460",
    "end": "1944620"
  },
  {
    "text": "and they provided\nhumans to annotate which response is\nbetter, and we did that for every single combination.",
    "start": "1944620",
    "end": "1951529"
  },
  {
    "text": "So like it was NC2 where\nn is the number of prompts we are looking at, and so we\ngenerate NC2 combinations,",
    "start": "1951530",
    "end": "1957370"
  },
  {
    "text": "and we rate each of them. And so these are the Elo\nratings that we get out of it.",
    "start": "1957370",
    "end": "1962830"
  },
  {
    "text": "And on this column\nhere shows that what is the rating you would get\nif you would have used GPT-4",
    "start": "1962830",
    "end": "1970330"
  },
  {
    "text": "as a proxy for humans. So instead of humans sitting\nand rating each of you're asking GPT-4 to select\nwhich is a better response.",
    "start": "1970330",
    "end": "1980040"
  },
  {
    "text": "Yeah, and so this is\nbasically the first table you're showing if you\nallow ties in the sense-- sorry, if there\nwas no tie allowed,",
    "start": "1980040",
    "end": "1986409"
  },
  {
    "text": "and this table here is showing\nthat if ties were allowed. ",
    "start": "1986410",
    "end": "1992500"
  },
  {
    "text": "Another example is\nthis leaderboard from Stanford, which is\nAlpacaEval leaderboard,",
    "start": "1992500",
    "end": "1998170"
  },
  {
    "text": "and they are doing something\nvery similar in the sense that they have GPT-4 and\nClaude as an evaluator,",
    "start": "1998170",
    "end": "2004320"
  },
  {
    "text": "and they are doing like\na pair-wise evaluation of these models,\nchatbot models, and they",
    "start": "2004320",
    "end": "2010590"
  },
  {
    "text": "are reporting the win\nrate of which model wins against the other one.",
    "start": "2010590",
    "end": "2016740"
  },
  {
    "text": "There is also the\nLMSYS leaderboard from Berkeley, which has\nthis thing called the Chatbot Arena, which is\nessentially like a publicly",
    "start": "2016740",
    "end": "2024299"
  },
  {
    "text": "crowdsourced leaderboard\nwherein you can go chat with any of their models,\nand then give them",
    "start": "2024300",
    "end": "2029910"
  },
  {
    "text": "ratings to which\none was more helpful and which one was better. And so this, again, has a\nleaderboard of Elo ratings",
    "start": "2029910",
    "end": "2036330"
  },
  {
    "text": "because this is done\nin a pair-wise setting. There's another\nbenchmark from LMSYS",
    "start": "2036330",
    "end": "2043370"
  },
  {
    "text": "which is called the MT bench or\nthe multi-turn bench benchmark, and this is the first ever\nmulti-turn dialogue benchmark",
    "start": "2043370",
    "end": "2050989"
  },
  {
    "text": "that is evaluating chatbots. And so it has-- there are\njust like 80 examples in this across a bunch of categories,\nbut essentially the way it works",
    "start": "2050989",
    "end": "2060230"
  },
  {
    "text": "is that the first turn or the\nfirst prompt from the benchmark is prompted to the\nmodel, then GPT-4",
    "start": "2060230",
    "end": "2068179"
  },
  {
    "text": "is asked to score on\na score of 1 to 10 with how good is the\nmodel's response,",
    "start": "2068179",
    "end": "2073260"
  },
  {
    "text": "and then it is followed\nup by another prompt which is the multi-turn prompt, which\nis related to the question,",
    "start": "2073260",
    "end": "2080330"
  },
  {
    "text": "but it might not be related\nto the model's responses because this is\nalready constructed, and they always follow up with\nthe same response to every bot.",
    "start": "2080330",
    "end": "2088879"
  },
  {
    "text": "And then again, GPT-4\nevaluates how good was the second turn\nof the response.",
    "start": "2088880",
    "end": "2095359"
  },
  {
    "text": "So this is like the consolidated\nleaderboard from LMSYS showing both the arena Elo\nrating as well as empty bench",
    "start": "2095360",
    "end": "2102550"
  },
  {
    "text": "score. So these are scores that are\naggregated across all the 80 examples, and this is\nGPT-4 scoring from 1 to 10",
    "start": "2102550",
    "end": "2110030"
  },
  {
    "text": "essentially.  Cool. So I think the\nsecond step that we",
    "start": "2110030",
    "end": "2115589"
  },
  {
    "text": "wanted to look at in our\nevaluating a chatbot chat was like, you know, think about how\ndo you evaluate a reward model?",
    "start": "2115590",
    "end": "2122770"
  },
  {
    "text": "So when you have these human\npreference dataset collected, and you train this reward\nmodel, which is essentially",
    "start": "2122770",
    "end": "2127800"
  },
  {
    "text": "a classifier, to discriminate\nbetween truthful and untruthful response, or can it rank\nhelpful response higher",
    "start": "2127800",
    "end": "2134849"
  },
  {
    "text": "than the less helpful responses? And, you know, there's literally\nno open source data leaderboard",
    "start": "2134850",
    "end": "2142380"
  },
  {
    "text": "available for evaluating these\npreference model or the reward models, but internally\nat Hugging Face,",
    "start": "2142380",
    "end": "2148170"
  },
  {
    "text": "we have our own\ndataset for evaluating so that we know that as we are\nadding more human preference",
    "start": "2148170",
    "end": "2153780"
  },
  {
    "text": "data, our models are\nactually getting better. So this is essentially we are\nevaluating on these open source",
    "start": "2153780",
    "end": "2161539"
  },
  {
    "text": "data sets, which is the\nAnthropic helpful dataset, the Open Assistant dataset,\nthe Stanford's human preference",
    "start": "2161540",
    "end": "2167930"
  },
  {
    "text": "dataset, and also the\nlearning to summarize datasets from the very first paper\nfrom OpenAI, which was looking",
    "start": "2167930",
    "end": "2173660"
  },
  {
    "text": "at learning to summarize. So this is like,\nyou know, basically",
    "start": "2173660",
    "end": "2178730"
  },
  {
    "text": "saying that how good\nis our reward model. And then finally, the third type\nof evaluation is red-teaming,",
    "start": "2178730",
    "end": "2186420"
  },
  {
    "text": "And so in this case, you\nwant to craft a prompt in a way that could surface\nmodel vulnerabilities",
    "start": "2186420",
    "end": "2191900"
  },
  {
    "text": "and emerging capabilities. And for example, if\nyou are asking like, how do I plan a bank robbery?",
    "start": "2191900",
    "end": "2197090"
  },
  {
    "text": "Is the model actually like,\nyou know, helping you with that and trying to elicit undesired\nbehavior from the model.",
    "start": "2197090",
    "end": "2203810"
  },
  {
    "text": "And unfortunately,\nactually, there is no open source leaderboard\navailable for this thing. There's just one data\nset from anthropic,",
    "start": "2203810",
    "end": "2210710"
  },
  {
    "text": "which has all the three\nincluded, which is the-- actually it has both\nhelpfulness and harmlessness. It's the HH data\nset from Anthropic,",
    "start": "2210710",
    "end": "2218390"
  },
  {
    "text": "and that's the only open\nsource data set available for red-teaming, but there's\nno leaderboard available",
    "start": "2218390",
    "end": "2224150"
  },
  {
    "text": "for red-temaing. And so this was like a blog\nthat I wrote earlier in the year saying, like, you know,\nhighlighting this gap,",
    "start": "2224150",
    "end": "2229910"
  },
  {
    "text": "and saying that, you\nknow, putting out an announcement saying like, we\nshould get together and build a data set for red-teaming.",
    "start": "2229910",
    "end": "2235730"
  },
  {
    "text": "And if you had heard of like\nthe DEFCON red-teaming design challenge and-- basically, crowdsourcing some\nof these red-teaming work",
    "start": "2235730",
    "end": "2242869"
  },
  {
    "text": "came out of that. OK, so now I'm going\nto get into-- now that we have discussed\nevaluation and benchmarks",
    "start": "2242870",
    "end": "2249430"
  },
  {
    "text": "and leaderboards, I'm\ngoing to talk about results and what do they look\nlike on each of and some of these benchmarks.",
    "start": "2249430",
    "end": "2255760"
  },
  {
    "text": "So here I'm showing the results\nfor this LLaMA2 2 13 billion",
    "start": "2255760",
    "end": "2261250"
  },
  {
    "text": "on the open LLM leaderboard\nfrom Hugging Face. And in this case, I\nwas using the data set",
    "start": "2261250",
    "end": "2266800"
  },
  {
    "text": "that we collected from Surge. That was a 10,000 instruction\ndemonstration data.",
    "start": "2266800",
    "end": "2273250"
  },
  {
    "text": "These are basically\nthe four datasets which are like NLP focused\ndata sets that we have as part",
    "start": "2273250",
    "end": "2278830"
  },
  {
    "text": "of Open LLM\nleaderboard which are the arc_challenge, the\nhendrycks, hellaswag, and truthfulqa_mc.",
    "start": "2278830",
    "end": "2284830"
  },
  {
    "text": "And you're like-- you know,\nthis is how well our model does, and all of this is\nessentially accuracy,",
    "start": "2284830",
    "end": "2291190"
  },
  {
    "text": "and this is the Lima\npaper or the Lima model, which is less is more for\nalignment that came from Meta,",
    "start": "2291190",
    "end": "2296480"
  },
  {
    "text": "and they just use 1,000 examples\nof high quality instructions and showed that you can get a\nvery good chatbot by just using",
    "start": "2296480",
    "end": "2303280"
  },
  {
    "text": "thousand examples. And this is taking the longest\nexample from Open Assistant and just choosing\nthe top 500 of them.",
    "start": "2303280",
    "end": "2310700"
  },
  {
    "text": "And so we found that our\nmodel does slightly better than both of like LLaMA\nand Open Assistant",
    "start": "2310700",
    "end": "2317180"
  },
  {
    "text": "except for in truthfulqa where\nwe found that the Lima and Open Assistant did better than us.",
    "start": "2317180",
    "end": "2323970"
  },
  {
    "text": "And similarly, in MT bench, we\nfound the opposite was true. So this is like,\nyou know, MT is--",
    "start": "2323970",
    "end": "2330790"
  },
  {
    "text": "remember that LMSYS had turn\nzero and turn one, ansd then so this is reporting\nthe first response.",
    "start": "2330790",
    "end": "2336520"
  },
  {
    "text": "This is like GPT-4\nis essentially scoring on a score of 1 to\n10, how good these models are",
    "start": "2336520",
    "end": "2342660"
  },
  {
    "text": "on the first dialogue turn\nand the second dialogue turn and the average score. And so actually, this is\nmore counterintuitive to what",
    "start": "2342660",
    "end": "2351330"
  },
  {
    "text": "we found on this\nautomatic evals is that-- actually, the MT\nbench says that, you know,",
    "start": "2351330",
    "end": "2356730"
  },
  {
    "text": "data that our model\ntrained on the data that we collected from\nSurge is not very good, and in fact, Lima and\nOpen Assistant which",
    "start": "2356730",
    "end": "2363510"
  },
  {
    "text": "are like a fraction of\nthe size of the data we had are much better. So this was kind of surprising.",
    "start": "2363510",
    "end": "2371400"
  },
  {
    "text": "And then I looked into like,\nyou know-- let me look at-- is the length a factor in this? And it does seem like,\nyou know, like the dat--",
    "start": "2371400",
    "end": "2378410"
  },
  {
    "text": "I was looking at each\nof those and then looked at the average length of the\nprompts in each of those, and it seems like there\nis a very wide range.",
    "start": "2378410",
    "end": "2385740"
  },
  {
    "text": "For example, like our\ndataset, the average length was just 211 of these\nprompts, while Lima",
    "start": "2385740",
    "end": "2391070"
  },
  {
    "text": "is like double of that, and\nOpen Assistant is almost double of that. ",
    "start": "2391070",
    "end": "2398130"
  },
  {
    "text": "So then I did this\nexperiment wherein I wanted to check\nif I controlled for the size of the\ndata, but then, you",
    "start": "2398130",
    "end": "2404970"
  },
  {
    "text": "know, let the length be\nvaried, the prompt length, does that affect\nthe performance?",
    "start": "2404970",
    "end": "2410130"
  },
  {
    "text": "So in particular, I think\nI highlighted this before, is that our chat category\nwas like really short.",
    "start": "2410130",
    "end": "2415330"
  },
  {
    "text": "And so it actually found\nthat length did not really affect that much except for\nthis truthfulqa dataset,",
    "start": "2415330",
    "end": "2423535"
  },
  {
    "text": "even for this hellaswag,\neven though it looks more it's actually\njust in the third digit. And over here, you can see the\nactual difference only made",
    "start": "2423535",
    "end": "2431070"
  },
  {
    "text": "on truthfulqa which actually\npreferred models that were generating longer responses.",
    "start": "2431070",
    "end": "2437930"
  },
  {
    "text": "But on the other hand, the\nMT bench score was again, not aligning or\ncorrelated with what",
    "start": "2437930",
    "end": "2443660"
  },
  {
    "text": "we found with these automatic\nmetrics and evaluations, in the sense that GPT-4 actually\ndid not prefer longer responses,",
    "start": "2443660",
    "end": "2452670"
  },
  {
    "text": "and so this was a little\nbit counterintuitive, and so we need to dig more\ninto what's going on over here,",
    "start": "2452670",
    "end": "2459500"
  },
  {
    "text": "but it actually found that like\nshorter responses were better than longer responses\nalthough there was not",
    "start": "2459500",
    "end": "2467900"
  },
  {
    "text": "much of very much\nof a difference. So the other\nexperiment and ablation",
    "start": "2467900",
    "end": "2473089"
  },
  {
    "text": "we did is that just\nremoving amounts of data and seeing like, if you\nincrementally add more data,",
    "start": "2473090",
    "end": "2478849"
  },
  {
    "text": "how does that\naffect performance? And this is, again,\non that autumn open LLM from\nHugging Face, which",
    "start": "2478850",
    "end": "2485960"
  },
  {
    "text": "is looking at some of the\nstandard NLP benchmarks and reporting accuracy.",
    "start": "2485960",
    "end": "2490970"
  },
  {
    "text": "And so this is starting with\njust 10% of all the data we collected from search. And as you can see in\nall these benchmarks,",
    "start": "2490970",
    "end": "2498920"
  },
  {
    "text": "actually, like it's saturates\nvery quickly, and then some of them you actually get like-- you basically lose performance\nif you keep adding data.",
    "start": "2498920",
    "end": "2506790"
  },
  {
    "text": "And so this is kind\nof aligning with-- when we started collecting data,\nwe had this diminishing return",
    "start": "2506790",
    "end": "2512539"
  },
  {
    "text": "plot wherein you said\nthat, if you have just very few thousand examples of\nvery high quality instruction",
    "start": "2512540",
    "end": "2518570"
  },
  {
    "text": "following data set,\nthat's good enough, and then your performance\nsaturates or plateaus",
    "start": "2518570",
    "end": "2524270"
  },
  {
    "text": "very quickly after that. And so that is what\nwe got as well. ",
    "start": "2524270",
    "end": "2532110"
  },
  {
    "text": "Similarly, I think this is\nwhere one place where MT bench actually correlated with the\nautomated metrics is that GPT-4",
    "start": "2532110",
    "end": "2540270"
  },
  {
    "text": "also showed that after\nlike about 4,000 examples,",
    "start": "2540270",
    "end": "2545310"
  },
  {
    "text": "it was basically very barely\nany gain in performance, actually like decreasing\nperformance with the model.",
    "start": "2545310",
    "end": "2552154"
  },
  {
    "text": " OK, great. So that was all the results on\nusing these human-curated, very",
    "start": "2552155",
    "end": "2560390"
  },
  {
    "text": "high quality dataset. What about results\nfrom distillation from these synthetic data sets?",
    "start": "2560390",
    "end": "2566250"
  },
  {
    "text": "In particular, we use UltraChat\nfor supervised fine-tuning and UltraFeedback\nfeedback for DPO.",
    "start": "2566250",
    "end": "2573972"
  },
  {
    "text": "And so these are the results. So this is basically just work\nthat was released last week. We haven't yet released\nthe code and the data",
    "start": "2573972",
    "end": "2581089"
  },
  {
    "text": "set which we are\ngoing to do this week. And so here I'm\nhighlighting that Zephyr is the model we release.",
    "start": "2581090",
    "end": "2586510"
  },
  {
    "text": "We use Mistral as\nthe foundation model, and then fine tuned it\nusing UltraChat and then",
    "start": "2586510",
    "end": "2592900"
  },
  {
    "text": "the DPO on UltraFeedback. And as you can see that\nit actually beats ChatGPT on this AlpacaEval leaderboard.",
    "start": "2592900",
    "end": "2599869"
  },
  {
    "text": " Also it is like\nthe best in the--",
    "start": "2599870",
    "end": "2608340"
  },
  {
    "text": "In all the open-- at\nleast it's like it beats most of the 13\nbillion parameter models,",
    "start": "2608340",
    "end": "2614730"
  },
  {
    "text": "and it's quite competitive\nto Claude to, again, on the AlpacaEval leaderboard.",
    "start": "2614730",
    "end": "2621840"
  },
  {
    "text": "So this is the model which\nhas both SFT and DPO. So we did an ablation on how\ngood or how useful is SFT",
    "start": "2621840",
    "end": "2631860"
  },
  {
    "text": "and how useful is because\nthere's this 2-step process. It's like, first, you fine-tune\non instruction demonstration,",
    "start": "2631860",
    "end": "2637470"
  },
  {
    "text": "then you fine-tune tune\non human preferences. The first row over\nhere is showing,",
    "start": "2637470",
    "end": "2643500"
  },
  {
    "text": "what if you directly\ndid DPO on UltraFeedback and did not do the\nsupervised fine-tuning?",
    "start": "2643500",
    "end": "2648870"
  },
  {
    "text": "And you actually saw\nthat that's really bad, so that doesn't work at all. And then the second\none is saying",
    "start": "2648870",
    "end": "2654330"
  },
  {
    "text": "that, what if you just\ndid supervised fine-tuning and did not do DPO? And so this actually-- which\nis like the first step.",
    "start": "2654330",
    "end": "2660840"
  },
  {
    "text": "And this actually\nworks decently well, and it's basically\ngetting you to 80% or 90% of the overall performance.",
    "start": "2660840",
    "end": "2668010"
  },
  {
    "text": "And finally, this is doing\nlike supervised fine-tuning on the human preference data. So you take this row\nand do another round",
    "start": "2668010",
    "end": "2674859"
  },
  {
    "text": "of supervised fine-tuning,\nbut on this data of human preferences. So you remember you had the\nchosen and the rejected,",
    "start": "2674860",
    "end": "2680869"
  },
  {
    "text": "so you give all the\ndialogue history and then the expected completion\nis the chosen dialogue response.",
    "start": "2680870",
    "end": "2686553"
  },
  {
    "text": "So in this case,\nyou're not really doing that discriminative thing. You're still doing\nthe SFT process, but using that in a smart--\nusing the dataset in a smart way",
    "start": "2686553",
    "end": "2696010"
  },
  {
    "text": "so that it follows a template\nof what supervised fine-tuning does. And then that as well we found\nthat wasn't very helpful.",
    "start": "2696010",
    "end": "2703339"
  },
  {
    "text": "So the best recipe,\nobviously, is DPO plus SFT. So, you know, doing SFT first\non the UltraChat, and then",
    "start": "2703340",
    "end": "2710770"
  },
  {
    "text": "DPO on the UltraFeedfback. Both of these data\nsets are synthetic,",
    "start": "2710770",
    "end": "2715840"
  },
  {
    "text": "but it's like only slightly\nbetter than just doing SFT. ",
    "start": "2715840",
    "end": "2721660"
  },
  {
    "text": "OK, so I'm getting to this\nfinal section of my talk, which is essentially looking at--",
    "start": "2721660",
    "end": "2727450"
  },
  {
    "text": "so we have seen a lot of these\nevaluation and benchmarks and leaderboards,\nand many of them",
    "start": "2727450",
    "end": "2732520"
  },
  {
    "text": "are starting to adopt these\npowerful models like Claude 2 and GPT-4, and are using as\nproxy for humans in evaluation.",
    "start": "2732520",
    "end": "2740089"
  },
  {
    "text": "And so what are the quirks\nassociated with doing that, and are there things\nthat we should like be considering\nwhen we are doing",
    "start": "2740090",
    "end": "2746320"
  },
  {
    "text": "this at a very large scale? So when we did that, when we\nuse GPT-4 as an evaluator,",
    "start": "2746320",
    "end": "2752790"
  },
  {
    "text": "we found that it actually\nhas a positional bias. And so in particular, it is\npredisposed to generating",
    "start": "2752790",
    "end": "2759150"
  },
  {
    "text": "a rating of one in a pair-wise\npreference collection setting. And so this chart\nover here shows",
    "start": "2759150",
    "end": "2765090"
  },
  {
    "text": "like the average rating\nof four model responses across the entire dataset.",
    "start": "2765090",
    "end": "2771000"
  },
  {
    "text": "And on the right,\non the other hand, humans are more or less uniform. And so you expect that this\ndistribution seems much more",
    "start": "2771000",
    "end": "2777832"
  },
  {
    "text": "better than this distribution\nwhich is skewed to the right. ",
    "start": "2777832",
    "end": "2782990"
  },
  {
    "text": "So then what we did\nis that we prompted GPT-4 to say that, hey,\nyou have this left bias,",
    "start": "2782990",
    "end": "2788450"
  },
  {
    "text": "and you always generate\nthis rating of one. You know, be aware of\nthis bias, and then you tell it to de-bias\nitself, it actually",
    "start": "2788450",
    "end": "2795710"
  },
  {
    "text": "flips the bias in the\nopposite direction. So then it is more\nself-aware in the sense",
    "start": "2795710",
    "end": "2801233"
  },
  {
    "text": "that it knows that, you\nknow, it has this bias and now it starts generating\nmore ratings of five and six.",
    "start": "2801233",
    "end": "2806369"
  },
  {
    "text": "And the one way of\ngetting rid of this is that we make sure that each\nresponse is equally likely to be",
    "start": "2806370",
    "end": "2812030"
  },
  {
    "text": "in right and left position\nso that kind of dilutes like this bias that it has\nto each of these positions.",
    "start": "2812030",
    "end": "2820060"
  },
  {
    "text": "And then, you know, we found\nthat actually like prompting GPT-4 to generate scores--\nso asking it to score",
    "start": "2820060",
    "end": "2826300"
  },
  {
    "text": "each response individually\nlike MT bench does. And then instead of ranking\nit in a pair-wise setting,",
    "start": "2826300",
    "end": "2831760"
  },
  {
    "text": "we actually found\nthat alleviates the problem a little bit,\nbut does not completely get rid of the problem.",
    "start": "2831760",
    "end": "2838690"
  },
  {
    "text": "We also found evidence of doping\nbetween training and evaluation. So in particular, we found\nthat GPT-4 prefers models that",
    "start": "2838690",
    "end": "2847600"
  },
  {
    "text": "were trained on GPT-4's data. So all these models\nhere were trained on data that was\nbootstrapped using GPT-4.",
    "start": "2847600",
    "end": "2856360"
  },
  {
    "text": "So it prefers that\nover humans who are more factual, much\nmore higher quality,",
    "start": "2856360",
    "end": "2862210"
  },
  {
    "text": "but they might be very\nsuccinct and to the point. So this is one\nthing that we should",
    "start": "2862210",
    "end": "2867280"
  },
  {
    "text": "be aware of when we're\nusing GPT-4 as an evaluator. The other thing is that it's\nalso like concurs with findings",
    "start": "2867280",
    "end": "2874450"
  },
  {
    "text": "from these other papers, which\nis that GPT-4 prefers models with higher diversity.",
    "start": "2874450",
    "end": "2879630"
  },
  {
    "text": "So that is number of unique\ntokens in the response and the longer responses. So if you have this list\nof list kind of response,",
    "start": "2879630",
    "end": "2886760"
  },
  {
    "text": "just like ChatGPT does,\nGPT-4 is predisposed to rating that higher\ncompared to a model that",
    "start": "2886760",
    "end": "2893030"
  },
  {
    "text": "does not generate that.  We also found that GPT-4 has\npoor correlation with humans",
    "start": "2893030",
    "end": "2901410"
  },
  {
    "text": "on low entropy tasks such as\nmath, coding, and reasoning. So remember that a\nleaderboard I showed you",
    "start": "2901410",
    "end": "2906420"
  },
  {
    "text": "where we had compared like how\ndoes GPT-4 Elo rating compare to humans, and then we dive\ndeeper into how does that",
    "start": "2906420",
    "end": "2913620"
  },
  {
    "text": "compare on each of these\ndifferent tasks, distribution and categories? And so this is\nwhat it looks like. So it seems like it says\nlower correlation with humans",
    "start": "2913620",
    "end": "2922620"
  },
  {
    "text": "on some of these more\nfactual, like, you know, kind of expecting\none correct answer,",
    "start": "2922620",
    "end": "2927970"
  },
  {
    "text": "and they're actually highly\ncorrelated with humans on these more high\nentropy tasks where you're",
    "start": "2927970",
    "end": "2933567"
  },
  {
    "text": "like brainstorming and\ncreative generation, which was kind of unintuitive\nand counterintuitive because you could have so many\ndifferent ways of coming up",
    "start": "2933568",
    "end": "2942030"
  },
  {
    "text": "with a recipe or a\nlist of something. But that's where the\nrating of GPT-4 and humans",
    "start": "2942030",
    "end": "2947910"
  },
  {
    "text": "are more correlated.  OK, so the final\nthing is takeaways.",
    "start": "2947910",
    "end": "2955140"
  },
  {
    "text": "So there's a bunch of this,\nbut let's try to break it down. Essentially, we discussed\nhow do we come up",
    "start": "2955140",
    "end": "2961340"
  },
  {
    "text": "with steps for data curation for\nsupervised fine-tuning and RLHF, and it involves several critical\nfactors such as how much data",
    "start": "2961340",
    "end": "2969560"
  },
  {
    "text": "do you need to collect, what\nis the length of the prompts and the distribution of those\nlength, the task distribution,",
    "start": "2969560",
    "end": "2976170"
  },
  {
    "text": "and what is the role of humans? Like, do you need\nsynthetic data? Do you need completely\nmanually curated or something",
    "start": "2976170",
    "end": "2981920"
  },
  {
    "text": "in the middle? And we looked at,\nlike, there are many tools for efficient\nfine-tuning of open source LLMs.",
    "start": "2981920",
    "end": "2988490"
  },
  {
    "text": "From the SFT results,\nwe found that truthfulqa was the main\ndifferentiating benchmark",
    "start": "2988490",
    "end": "2994579"
  },
  {
    "text": "for these automated\neval metrics. And then we found that MT\nbench scores were actually not",
    "start": "2994580",
    "end": "3000310"
  },
  {
    "text": "correlated with these\nautomated metrics, and so it was more sort of--",
    "start": "3000310",
    "end": "3006010"
  },
  {
    "text": "only on some of these models we\nfound that they were correlated. For the distillation\nresults, which",
    "start": "3006010",
    "end": "3011960"
  },
  {
    "text": "is from the Zephyr-7B where\nwe are like fine-tuning on synthetic data, we\nfound that the SFT on AI",
    "start": "3011960",
    "end": "3018920"
  },
  {
    "text": "generated data and the\nDPO or distillation of DPO on AI feedback data\nactually beats ChatGPT",
    "start": "3018920",
    "end": "3025250"
  },
  {
    "text": "even though the model is\njust 7 billion parameter. And then we found\nthat benchmarking gap",
    "start": "3025250",
    "end": "3031130"
  },
  {
    "text": "in assessing our RLHF\nmodels, in particular, that we don't have benchmarks\nfor assessing reward models,",
    "start": "3031130",
    "end": "3038340"
  },
  {
    "text": "and we also don't have\nopen source benchmarks for evaluating red-teaming\nand model vulnerabilities.",
    "start": "3038340",
    "end": "3044450"
  },
  {
    "text": "Then finally, we'd dive\ndeeper into looking at quirks of using GPT-4 or\nsome of these powerful LLMs",
    "start": "3044450",
    "end": "3051770"
  },
  {
    "text": "as an evaluator, and\nsome of them were like, you know, they prefer models\ntrained on GPT-4 like data,",
    "start": "3051770",
    "end": "3057930"
  },
  {
    "text": "it has a left positional\nbias, and then it has high\ncorrelation with humans on creative tasks compared\nto coding or reasoning tasks.",
    "start": "3057930",
    "end": "3067960"
  },
  {
    "text": "And my work has been covered\non the New York Times article cover which talks about the\nsecret ingredient of alignment,",
    "start": "3067960",
    "end": "3075100"
  },
  {
    "text": "which is for ChatGPT\nwhich is alignment. I'm also part of the United\nNations advisory board",
    "start": "3075100",
    "end": "3080289"
  },
  {
    "text": "that was announced last week. So really humbled\nto be part of that. Here are some blog posts.",
    "start": "3080290",
    "end": "3086380"
  },
  {
    "text": "Basically, we did not publish\na whole lot this year,",
    "start": "3086380",
    "end": "3091420"
  },
  {
    "text": "but we wrote a bunch of blog\nposts highlighting what we are releasing and working on.",
    "start": "3091420",
    "end": "3096790"
  },
  {
    "text": "And also some of these are\npart of the talk that I just discussed. And this is part of the H4 team.",
    "start": "3096790",
    "end": "3104140"
  },
  {
    "text": "I'm grateful to be part of\nthis, and thanks for listening. ",
    "start": "3104140",
    "end": "3130240"
  },
  {
    "text": "When you get alternating\nresponses from these models, do you expect really\nhigh temperatures or do you keep it pretty close\nto the temperature value that's",
    "start": "3130240",
    "end": "3137530"
  },
  {
    "text": "also told when you find one? Yeah so we did basically\nchose like, you know--",
    "start": "3137530",
    "end": "3144220"
  },
  {
    "text": "we tried experimenting with\ndifferent temperatures, but then we actually found that\njust using different sampling",
    "start": "3144220",
    "end": "3151000"
  },
  {
    "text": "strategy worked better. So using a different\nvalue of P and then K and some combination of\nthat, as opposed to just like",
    "start": "3151000",
    "end": "3158290"
  },
  {
    "text": "relying on temperature. ",
    "start": "3158290",
    "end": "3174920"
  },
  {
    "text": "[INAUDIBLE] problem [INAUDIBLE]\nlabs on doing that at scale,",
    "start": "3174920",
    "end": "3184299"
  },
  {
    "text": "and like, what are the\ndifferent tactics and techniques for [INAUDIBLE].",
    "start": "3184300",
    "end": "3190462"
  },
  {
    "text": " Yeah, so I think for\nred-teaming at scale,",
    "start": "3190462",
    "end": "3198359"
  },
  {
    "text": "there's actually a paper\nthat came out recently called GPTFUZZER that actually\nbootstraps and uses",
    "start": "3198360",
    "end": "3206220"
  },
  {
    "text": "these powerful LLMs to\njailbreak other LLMs. And also there was\na DeepMind paper-- I think actually like one\nand half to almost two years",
    "start": "3206220",
    "end": "3212430"
  },
  {
    "text": "ago that was red-teaming\nteaming large language models with large language models. So how do you like red-team\nand evaluate language model",
    "start": "3212430",
    "end": "3219780"
  },
  {
    "text": "by using another\npowerful language model? And so I think that is kind of\nthe way to go in terms of scale.",
    "start": "3219780",
    "end": "3226680"
  },
  {
    "text": "And-- sorry, what was\nthe second question? I guess I was asking like, what\nare the unique threat factors,",
    "start": "3226680",
    "end": "3235140"
  },
  {
    "text": "or what makes red-team LLMs\nunique from red-team team",
    "start": "3235140",
    "end": "3240690"
  },
  {
    "text": "regular infrastructure\nor software? Yeah, so I think one\nthing is this idea",
    "start": "3240690",
    "end": "3245970"
  },
  {
    "text": "of emerging capabilities,\nwhich is essentially, as you scale up-- and which\nis a trend that we're seeing.",
    "start": "3245970",
    "end": "3252990"
  },
  {
    "text": "As we are scaling up, there are\nthings that these models do or-- capabilities that emerge\nthat were not there",
    "start": "3252990",
    "end": "3259049"
  },
  {
    "text": "in the smaller models. I think examples are chain\nof thought reasoning, which, GPT-2 two or GPT was\nnot capable of doing it,",
    "start": "3259050",
    "end": "3266730"
  },
  {
    "text": "and as we scale up--\nand the other example is this few shot prompting\nthat we first saw in GPT-3.",
    "start": "3266730",
    "end": "3272220"
  },
  {
    "text": "As in you could give it\na completely new task and not update its\nparameters in any way,",
    "start": "3272220",
    "end": "3277410"
  },
  {
    "text": "but just put it as\npart of the prompt, and then now it just\nlaunched the task, and then it can do it on any\nnumber of examples right.?",
    "start": "3277410",
    "end": "3285240"
  },
  {
    "text": "And so labeling and all these\nthings started coming up, like using GPT-3 as\na labeler and all that when we\ndiscovered that thing.",
    "start": "3285240",
    "end": "3293049"
  },
  {
    "text": "So I think-- essentially, like\nyou know-- and the other example is like manipulation. I don't think any open source\nmodels are capable of that yet,",
    "start": "3293050",
    "end": "3301430"
  },
  {
    "text": "but I know Anthropic and\nOpenAI and these companies are focusing on deception\nand manipulation",
    "start": "3301430",
    "end": "3307150"
  },
  {
    "text": "because when you start\nchatting with these models, you start treating\nthem as a companion,",
    "start": "3307150",
    "end": "3314170"
  },
  {
    "text": "especially if you have\ncharacter AI kind of a thing where, you know, you might\nstart confiding in them,",
    "start": "3314170",
    "end": "3320380"
  },
  {
    "text": "sharing information that you\nprobably shouldn't, and then they can use it\nagainst you, maybe.",
    "start": "3320380",
    "end": "3326020"
  },
  {
    "text": "An example of that is\nlike, I think recently we saw that GPT-4 actually\nmanipulated someone",
    "start": "3326020",
    "end": "3331210"
  },
  {
    "text": "to read the capture\nto it in some way and tell it what\nthe capture reads. And so that's a really concrete\nexample of manipulation.",
    "start": "3331210",
    "end": "3339470"
  },
  {
    "text": "And so it seems like now these\nmodels are capable of that. I don't think open source\nmodels are there yet,",
    "start": "3339470",
    "end": "3345910"
  },
  {
    "text": "but these are just like, you\nknow, things that come out and vulnerabilities that would\nsurface when you do red-teaming.",
    "start": "3345910",
    "end": "3355549"
  },
  {
    "text": "I have [INAUDIBLE]\nidea [INAUDIBLE] as those AI become\nmore real [INAUDIBLE]",
    "start": "3355550",
    "end": "3362230"
  },
  {
    "text": "Like creating dataset sets\nlike, pretty important, but I know there's\nalso [INAUDIBLE].. Like if you're going to\nopen source data sets,",
    "start": "3362230",
    "end": "3369430"
  },
  {
    "text": "like are you after just\n[INAUDIBLE] bad actors, how to use models for that?",
    "start": "3369430",
    "end": "3374740"
  },
  {
    "text": "And so I guess like what's\nyour take on that idea and what's like [INAUDIBLE]\nlot of open sourcing.",
    "start": "3374740",
    "end": "3381290"
  },
  {
    "text": "Yeah, so I would say\nlike it was, it's less about open sourcing a\ndata set that is crafted",
    "start": "3381290",
    "end": "3389000"
  },
  {
    "text": "to elicit this behavior. It's more about\nthe kind of harms that we should be\nthinking about.",
    "start": "3389000",
    "end": "3394880"
  },
  {
    "text": "So it's more about\nlike, you know, hallucinating, or\nplagiarism, manipulation,",
    "start": "3394880",
    "end": "3400130"
  },
  {
    "text": "trying to leak PII information,\npeople's credit card, SSN, and things like that. It's more about thinking about\nthese different dimensions",
    "start": "3400130",
    "end": "3407510"
  },
  {
    "text": "and giving concrete examples\nof how these models can elicit this behavior.",
    "start": "3407510",
    "end": "3413720"
  },
  {
    "text": "But I think what you\nare trying to talk about is that, what if we\ngave them concrete ways,",
    "start": "3413720",
    "end": "3419147"
  },
  {
    "text": "like, concrete prompts\non how you jailbreak, and then they can go\nand try to do that. I think first thing is,\nwhy we are doing this.",
    "start": "3419147",
    "end": "3425609"
  },
  {
    "text": "We would have\nevaluated our models, and we would then start thinking\nabout guardrails and safety ourself.",
    "start": "3425610",
    "end": "3430880"
  },
  {
    "text": "And if indeed, the\ndataset is so good that we can say that a lot\nof these powerful models are failing on that,\nand obviously you",
    "start": "3430880",
    "end": "3436940"
  },
  {
    "text": "don't open source it\ninstantly, but you actually think about what is the\nbest way to put it out there by first\nsecuring the model,",
    "start": "3436940",
    "end": "3443490"
  },
  {
    "text": "making sure that\nit does not like-- basically does not elicit\nthat kind of behavior,",
    "start": "3443490",
    "end": "3449340"
  },
  {
    "text": "and then sharing it while you\nhave already crossed that bridge and being like, yeah, my model\nis safeguarded against that.",
    "start": "3449340",
    "end": "3455650"
  },
  {
    "text": "So it's more like,\nyeah, a process of-- a gradient of things\nthat you need to do.",
    "start": "3455650",
    "end": "3460955"
  },
  {
    "text": " [INAUDIBLE] a good talk.",
    "start": "3460955",
    "end": "3466880"
  },
  {
    "text": "Recently gained some papers\nabout retraining, auto LLM data, getting\nto collapse models.",
    "start": "3466880",
    "end": "3473763"
  },
  {
    "text": "I mean, obviously\n[INAUDIBLE] is not the case, but if you are using LLMF\ndata for fine-tuning,",
    "start": "3473763",
    "end": "3479180"
  },
  {
    "text": "you will see any of those\npotential collapses. What are your\nthoughts from that?",
    "start": "3479180",
    "end": "3484339"
  },
  {
    "text": "Like how to prevent that? Yeah, so you're\ntalking about when you're using synthetic data\nbootstrap from another language",
    "start": "3484340",
    "end": "3491829"
  },
  {
    "text": "models. Have we seen collapse of\nsome kind of mode collapse or something like that?",
    "start": "3491830",
    "end": "3497170"
  },
  {
    "text": "Or-- Yes, like that. So actually, so far it's been\nlike clear that these are good.",
    "start": "3497170",
    "end": "3503010"
  },
  {
    "text": "Like these actually turn like,\nyou know, regular chatbots-- and like regular language\nmodels into chatbots, which",
    "start": "3503010",
    "end": "3509059"
  },
  {
    "text": "are as good as the experience\nthat you get by chatting with ChatGPT, but although,\nthe quirks that I raised,",
    "start": "3509060",
    "end": "3515503"
  },
  {
    "text": "which is like, you know,\nwhen you have these models, and then you like now\nput them on a benchmark, and then you see that\nsuddenly it's like 90%,",
    "start": "3515503",
    "end": "3522180"
  },
  {
    "text": "It might just be because you use\nthe model that was a evaluator to generate a data\nand then train this model and that in turn\nlike this doping thing, right?",
    "start": "3522180",
    "end": "3529457"
  },
  {
    "text": "And so that is one thing that\nis important to think about. The other thing is--",
    "start": "3529457",
    "end": "3535725"
  },
  {
    "text": " what was I going to say? I forgot. ",
    "start": "3535725",
    "end": "3552210"
  },
  {
    "text": "Yeah. The other thing is like\nabout the licensing part, which is kind of not related\nto what you were asking,",
    "start": "3552210",
    "end": "3558930"
  },
  {
    "text": "but essentially there was this\nkind of like-- you cannot--",
    "start": "3558930",
    "end": "3564059"
  },
  {
    "text": "we cannot open source\nand commercially. So it's still\nrestrictive license. And you cannot use it\nfor building and selling",
    "start": "3564060",
    "end": "3571619"
  },
  {
    "text": "applications down the line,\nbut then it's still good as a research artifact.",
    "start": "3571620",
    "end": "3579210"
  },
  {
    "text": "We would have seen\nthese kind of collapses happen if it was allowed\nto use these commercially, and then people\nwould have been like,",
    "start": "3579210",
    "end": "3585720"
  },
  {
    "text": "but actually, recently,\nwe did see like, so there's this company\ncalled Daxter which was",
    "start": "3585720",
    "end": "3591000"
  },
  {
    "text": "using GPT-4 for summarization. They replaced it with the open\nsource model called Mistral, and they said that their\ncustomers haven't complained,",
    "start": "3591000",
    "end": "3599200"
  },
  {
    "text": "and they're saving\na ton of money and it just seems to work fine. And they are like, you\nknow, it's just as good.",
    "start": "3599200",
    "end": "3605740"
  },
  {
    "text": "But not that I'm\nsaying that Mistral is trained on any of\nthese synthetic data, but it's just an\nexample of things",
    "start": "3605740",
    "end": "3613060"
  },
  {
    "text": "that would become very clear by\ndoing this sort of A/B testing where you like replace\nthis model by another one",
    "start": "3613060",
    "end": "3618580"
  },
  {
    "text": "and see how that affects things. Thank you. ",
    "start": "3618580",
    "end": "3625770"
  },
  {
    "text": "I have a question on Zoom. Yes. ",
    "start": "3625770",
    "end": "3631560"
  },
  {
    "text": "Right. It seems like another axis\nyou might beat ChatGPT on",
    "start": "3631560",
    "end": "3637589"
  },
  {
    "text": "is on cost. So I wondered-- I wonder what your\ntotal budget was",
    "start": "3637590",
    "end": "3642630"
  },
  {
    "text": "or your total cost\nwas to produce your model that beat them.",
    "start": "3642630",
    "end": "3647880"
  },
  {
    "text": "So the Zephyr-7B was just four\nhours of training on 16 A100.",
    "start": "3647880",
    "end": "3654109"
  },
  {
    "text": "So that's less\nthan $50, I guess, because we use a\nsynthetic dataset which",
    "start": "3654110",
    "end": "3660480"
  },
  {
    "text": "was already open source, which\nis UltraChat and UltraFeedback. But the cost associated--",
    "start": "3660480",
    "end": "3667480"
  },
  {
    "text": "Like the overall cost, all\nthe people, and everything. Yeah. I see. OK.",
    "start": "3667480",
    "end": "3672850"
  },
  {
    "text": "So all the people and everything\nin the sense [INAUDIBLE].. I guess like UltraChat\nprobably might have reported",
    "start": "3672850",
    "end": "3679560"
  },
  {
    "text": "some cost and UltraFeedback, but\nthey are mostly synthetically created with very little\nhuman intervention,",
    "start": "3679560",
    "end": "3687270"
  },
  {
    "text": "and so they might-- I don't know if\nthey report that. I haven't looked\ninto that, but I",
    "start": "3687270",
    "end": "3692470"
  },
  {
    "text": "would say it was still\nmuch more cost-effecient than what we spent on buying\ndata from Surge and ScaleAI.",
    "start": "3692470",
    "end": "3698200"
  },
  {
    "text": "And we spent about half a\nmillion buying about 20,000 prompts of human preferences,\nthe 20,000 dialogue,",
    "start": "3698200",
    "end": "3705580"
  },
  {
    "text": "and about 10,000 instruction\ndemonstration data. So that was--",
    "start": "3705580",
    "end": "3710985"
  },
  {
    "text": "Thank you. --quite a bit. ",
    "start": "3710985",
    "end": "3722710"
  },
  {
    "text": "One thing I'm curious\nabout is the scale that you used in evaluating\nthe bias for GPT-4.",
    "start": "3722710",
    "end": "3728599"
  },
  {
    "text": "Uh-huh. So I saw it was like\n1.7 on the slide. ",
    "start": "3728600",
    "end": "3735914"
  },
  {
    "text": "Hmm This one? Yeah.",
    "start": "3735914",
    "end": "3742610"
  },
  {
    "text": "So, yeah, this was\nthe Anthropic scale. Like, remember that one\nto four is decreasingly A, and five to eight is\nincreasingly B. Yeah.",
    "start": "3742610",
    "end": "3751280"
  },
  {
    "text": "And that's giving the model\nto outputs [INAUDIBLE]?? Yes, exactly. ",
    "start": "3751280",
    "end": "3758616"
  },
  {
    "text": "In these types of valuations,\nhow sensitive to the fault do you find the\nevaluators to be?",
    "start": "3758616",
    "end": "3765200"
  },
  {
    "text": "They see you with /\ntelling it that it has to account for this\nleft bias or right bias.",
    "start": "3765200",
    "end": "3772170"
  },
  {
    "text": "What's stopping you from\nthen saying, [INAUDIBLE] distribution should be\nuniform, and distribution",
    "start": "3772170",
    "end": "3778290"
  },
  {
    "text": "should be normal and just\nkind of playing iteratively to see how [INAUDIBLE]\nthings used to be. Yeah.",
    "start": "3778290",
    "end": "3783530"
  },
  {
    "text": "Yeah. I think that's a good\npoint in the sense like, we did not study as\nto whether certain tasks",
    "start": "3783530",
    "end": "3789180"
  },
  {
    "text": "or prompts that we're\nputting off GPT-4 to generate this kind of bias.",
    "start": "3789180",
    "end": "3795450"
  },
  {
    "text": "Although I would\nsay that, you know, this was also observed by LMSYS,\nand it's part of their findings",
    "start": "3795450",
    "end": "3801900"
  },
  {
    "text": "as well, [INAUDIBLE]. But yeah, so the LMSYS\npaper also has that,",
    "start": "3801900",
    "end": "3808953"
  },
  {
    "text": "but it would be interesting. Like, it would be surprising\nif it generates this on very long prompts or prompts from\nmath or something, which",
    "start": "3808953",
    "end": "3816599"
  },
  {
    "text": "are just hard to evaluate when\nthere are two responses which are-- at least as a human.",
    "start": "3816600",
    "end": "3822040"
  },
  {
    "text": "Like when I see a bunch of code\non this side and this side, and then it's very hard\nto say-- and both of them",
    "start": "3822040",
    "end": "3827500"
  },
  {
    "text": "are trying to do the same thing,\nbut a very different approach. It's very hard to\nevaluate them, right?",
    "start": "3827500",
    "end": "3833230"
  },
  {
    "text": "And so, yeah, we haven't\nlooked into that. ",
    "start": "3833230",
    "end": "3843640"
  },
  {
    "text": "Perhaps another thing is,\nyou think order matters? Like which [INAUDIBLE]\nGPT-4 first.",
    "start": "3843640",
    "end": "3850150"
  },
  {
    "text": "Yeah. I mean, that was-- basically,\nthe takeaway was that. So it's interesting\nbecause humans usually",
    "start": "3850150",
    "end": "3856720"
  },
  {
    "text": "have recency bias, which is\nessentially the last thing that you read is the\nthing that you remember, and so you would just, you\nknow, try to choose that more.",
    "start": "3856720",
    "end": "3864640"
  },
  {
    "text": "You know, you're just\ninclined to do that. And GPT-4 actually\nhad a left bias. So the thing that it\nfirst saw in some sense.",
    "start": "3864640",
    "end": "3871150"
  },
  {
    "text": "I think some-- like I think\nLMSYS was the one that proposed because it has this\nleft to right training,",
    "start": "3871150",
    "end": "3876320"
  },
  {
    "text": "maybe that's why it has\nthat kind of a bias. But yeah, so I think\nthe way we elevated",
    "start": "3876320",
    "end": "3882579"
  },
  {
    "text": "that was that, you know,\nhaving every model's output be equally likely to be on the\nleft and the right hand side.",
    "start": "3882580",
    "end": "3888740"
  },
  {
    "text": "So if we're doing aLPACA\nand Vicuna, then instead of just doing aLPACA on\nleft and Vicuna on right,",
    "start": "3888740",
    "end": "3894100"
  },
  {
    "text": "we would just randomly\nlike switch them, AND so both of them are likely to\noccur in both these positions.",
    "start": "3894100",
    "end": "3899116"
  },
  {
    "text": " And you still saw the left bias? [INAUDIBLE]",
    "start": "3899116",
    "end": "3905040"
  },
  {
    "text": "If you just ask it to rate\nit on a scale of one to five? Yes. But if you say that,\nhey, you have this bias",
    "start": "3905040",
    "end": "3911484"
  },
  {
    "text": "and make it try to\nmake it aware of it, then it flips and it\ngenerates something like that. So yeah.",
    "start": "3911485",
    "end": "3916745"
  },
  {
    "start": "3916745",
    "end": "3940030"
  },
  {
    "text": "Is there other approaches\nwhere you can prompt the model by shuffling the prompts?",
    "start": "3940030",
    "end": "3946089"
  },
  {
    "text": "Then you [INAUDIBLE] to de-bias\nthe [? model ?] or de-bias",
    "start": "3946090",
    "end": "3952390"
  },
  {
    "text": "the results of it? By shuffling the\nprompts, you mean like--",
    "start": "3952390",
    "end": "3958166"
  },
  {
    "text": "[INAUDIBLE] bias the [INAUDIBLE]\nto shuffle the order of how you--",
    "start": "3958166",
    "end": "3964109"
  },
  {
    "text": "but in the [INAUDIBLE]. Series. Yeah, so that's what we did, is\nthat, you know, we would like,",
    "start": "3964110",
    "end": "3969150"
  },
  {
    "text": "you know, randomly shuffle\nthe left and the right. And then so each model--",
    "start": "3969150",
    "end": "3975540"
  },
  {
    "text": "so basically, you\ncreate two combinations. Suppose you want to evaluate\nthree models on 10 prompts,",
    "start": "3975540",
    "end": "3982720"
  },
  {
    "text": "so you'll have 10c2\n2 combinations, where n is the\nnumber of prompts-- or sorry, the number of models.",
    "start": "3982720",
    "end": "3988750"
  },
  {
    "text": "And then you would basically\nlike generate-- so this will be a total dataset. So you would have generated 10\nresponses from each of these",
    "start": "3988750",
    "end": "3996330"
  },
  {
    "text": "models, and then put them\ntogether in this 3c2 setting, and then that will be like a\ncombination of each of these.",
    "start": "3996330",
    "end": "4004319"
  },
  {
    "text": "And then you make sure\nthat every time the like, the models on the left are\nequally likely to also occur",
    "start": "4004320",
    "end": "4009928"
  },
  {
    "text": "on the right. So if you are doing like\nmodel one and then model two, then you also make sure\nalso do model two and then",
    "start": "4009928",
    "end": "4015650"
  },
  {
    "text": "model one on a different prompt. ",
    "start": "4015650",
    "end": "4028300"
  },
  {
    "text": "OK. There's one more\nquestion for this one. OK. [INAUDIBLE]",
    "start": "4028300",
    "end": "4035050"
  },
  {
    "text": " [INAUDIBLE] one of the side.",
    "start": "4035050",
    "end": "4041285"
  },
  {
    "text": "Oh, OK, Sure. Sorry. ",
    "start": "4041285",
    "end": "4047190"
  },
  {
    "text": "Should I keep the Zoom on? [INAUDIBLE]",
    "start": "4047190",
    "end": "4052201"
  },
  {
    "start": "4052201",
    "end": "4060700"
  },
  {
    "text": "This one? [INAUDIBLE] Thank you.",
    "start": "4060700",
    "end": "4065970"
  },
  {
    "text": "[INAUDIBLE] ask you one\nthing about [INAUDIBLE]..",
    "start": "4065970",
    "end": "4071190"
  },
  {
    "text": "Yes. So just talking about if I\nunderstand this correctly. So on the reinforcement\nlearning from movement feedback,",
    "start": "4071190",
    "end": "4078960"
  },
  {
    "text": "first you build a reward\nmodel, and that reward model takes text input and then\nhumans give it scores,",
    "start": "4078960",
    "end": "4084412"
  },
  {
    "text": "while the supervised\nproblem we're trying to predict from\nthe text of the score. Mm-hmm. And then after reward model--",
    "start": "4084413",
    "end": "4089678"
  },
  {
    "text": "[INTERPOSING VOICES] ",
    "start": "4089678",
    "end": "4098068"
  },
  {
    "text": "--same token and pump that\nthrough the reward model? Then the reward model\nwill be optimized on this? Yes. And that's how [INAUDIBLE]\nvery [INAUDIBLE] was, right?",
    "start": "4098069",
    "end": "4104657"
  },
  {
    "text": "Only every once at the very end. Very end. But that's how you basically-- Yes, exactly. And so you have to-- it's\nvery simple and efficient.",
    "start": "4104657",
    "end": "4111220"
  },
  {
    "text": "And so since you have to keep\ndoing this again and again and again, and then that's\nwhy you need 100,000 examples for doing RLHF, but only\n10,000-- possible [INAUDIBLE]..",
    "start": "4111220",
    "end": "4119278"
  },
  {
    "text": "I see. That's going to be\n[? a problem. ?] Thanks so much. Very interesting talk. ",
    "start": "4119279",
    "end": "4128000"
  }
]