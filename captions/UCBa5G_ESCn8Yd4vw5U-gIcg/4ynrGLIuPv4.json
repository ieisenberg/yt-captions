[
  {
    "start": "0",
    "end": "5520"
  },
  {
    "text": "So I'm delighted to introduce\nour second invited speaker for 224N, Kelvin Guu.",
    "start": "5520",
    "end": "11910"
  },
  {
    "text": "So Kelvin is a senior\nresearch scientist at Google with interests in\nretrieval augmented language",
    "start": "11910",
    "end": "20820"
  },
  {
    "text": "models and using knowledge\nin neural networks, and is perhaps best\nknown for his work",
    "start": "20820",
    "end": "27690"
  },
  {
    "text": "on the REALM model, which is one\nof the things he'll doubtless talk about today.",
    "start": "27690",
    "end": "33030"
  },
  {
    "text": "Yeah. I guess I know there\nare a few statistic students in the\nclass, so maybe I'll",
    "start": "33030",
    "end": "39420"
  },
  {
    "text": "also just mention that,\nactually, Kelvin's background is a statistics PhD. But somewhere along\nthe line, he got",
    "start": "39420",
    "end": "46020"
  },
  {
    "text": "corrupted away from\nmath and statistics, and ended up spending\nall of his time",
    "start": "46020",
    "end": "51150"
  },
  {
    "text": "on natural language\nprocessing, a very good move. I'll recommend it to anybody.",
    "start": "51150",
    "end": "57090"
  },
  {
    "text": "So anyway, I'm really happy\nto have Kelvin here today to tell us about\nhis recent work.",
    "start": "57090",
    "end": "62820"
  },
  {
    "text": " NLP, and as Chris\nalluded to, there",
    "start": "62820",
    "end": "67840"
  },
  {
    "text": "will be a focus on\nmemory augmented models. So I'll try to kind of have\na few spots for us to pause",
    "start": "67840",
    "end": "75789"
  },
  {
    "text": "and ask questions. And otherwise, I'll\ntake the slides away. Great, so I want to start by\njust giving some motivation",
    "start": "75790",
    "end": "83050"
  },
  {
    "text": "on some tasks that AI\ncannot solve today.",
    "start": "83050",
    "end": "88060"
  },
  {
    "text": "It cannot diagnose\na medical patient. It can't fix your car. It can't perform novel\nscientific research.",
    "start": "88060",
    "end": "95470"
  },
  {
    "text": "It can't file\ncorporate taxes, and it can't do many other things. Now, I'm not saying that\nartificial intelligence is",
    "start": "95470",
    "end": "101830"
  },
  {
    "text": "supposed to completely\ndo these things, but at least, it should\nbe able to assist people who are doing those things. And what all of these\ntasks have in common",
    "start": "101830",
    "end": "109210"
  },
  {
    "text": "is that, certainly,\nintelligence is required, but domain knowledge in these\ndomains is just as important.",
    "start": "109210",
    "end": "115100"
  },
  {
    "text": "So it's not\nintelligence alone that enables you to do these things. You have to have a long\nexperience in various things,",
    "start": "115100",
    "end": "122450"
  },
  {
    "text": "such as what a car's\ncomponents are, or in the case of\nthis question here, if you were to ask\na language model,",
    "start": "122450",
    "end": "129619"
  },
  {
    "text": "the part of the intestine\nmost commonly affected by Crohn's disease is-- my latest query to\nGPT-2 says the rectum,",
    "start": "129620",
    "end": "137680"
  },
  {
    "text": "but actually, it's the ileum. So we can understand that, if\nyou're in the medical field,",
    "start": "137680",
    "end": "143073"
  },
  {
    "text": "and you're making\nthis level of mistake, then you probably need\nto go back to training.",
    "start": "143073",
    "end": "148720"
  },
  {
    "text": "So we're interested in\ngetting language models and other language\nunderstanding systems to make these fine\ngrained distinctions well",
    "start": "148720",
    "end": "155349"
  },
  {
    "text": "because we can't really unlock\nthe next set of applications that NLP or AI could target.",
    "start": "155350",
    "end": "160780"
  },
  {
    "text": "And of course, this is something\nthat, since the field began, artificial intelligence\nresearchers",
    "start": "160780",
    "end": "166210"
  },
  {
    "text": "have been very interested in. If you look at some of\nthe early applications of artificial intelligence\nin the 60s and the 80s,",
    "start": "166210",
    "end": "173110"
  },
  {
    "text": "there were expert systems\nthat did medical diagnosis. They would do\ncomputer chip design,",
    "start": "173110",
    "end": "178569"
  },
  {
    "text": "and of course, we\nknow that we didn't get to completely fully\nsolving those problems. And back then, the\nbig obstacle was",
    "start": "178570",
    "end": "186190"
  },
  {
    "text": "that you had to manually input\nall of the knowledge required for that domain. Some expert had to sit down\nand write all of the rules,",
    "start": "186190",
    "end": "192730"
  },
  {
    "text": "and if any one of those rules\ncontradicted another rule, the system was very brittle\nand unable to handle",
    "start": "192730",
    "end": "198370"
  },
  {
    "text": "that complexity. But in 2022, what's\nvery exciting",
    "start": "198370",
    "end": "203409"
  },
  {
    "text": "is that we now have\nlanguage models, as you've seen in\nprevious lectures, that can automatically acquire\nknowledge from the web.",
    "start": "203410",
    "end": "211090"
  },
  {
    "text": "And so, that gives us\nan exciting opportunity to revisit this question\nof how to use knowledge",
    "start": "211090",
    "end": "216849"
  },
  {
    "text": "in artificial intelligence\nand do more than classify whether an image\nis a cat or a dog, but move on to much\nmore complex tasks.",
    "start": "216850",
    "end": "225290"
  },
  {
    "text": "So this talk will\nbe in three parts. The first part is,\nwe're going to look",
    "start": "225290",
    "end": "230980"
  },
  {
    "text": "at how language models\ncurrently represent knowledge. Since they've obviously\nmade huge gains, we need to understand what it\nis that's powering that success.",
    "start": "230980",
    "end": "239690"
  },
  {
    "text": "And then we're going to\nstep back and ask ourselves if that current way of\nrepresenting knowledge",
    "start": "239690",
    "end": "246010"
  },
  {
    "text": "is what we're happy with,\nand what we'd actually like to see more of. And finally, kind of\nleading the discussion here,",
    "start": "246010",
    "end": "254710"
  },
  {
    "text": "we're going to propose\nmemory augmented models as a way of addressing\nsome of those challenges. So it's certainly not the\nonly way to address it,",
    "start": "254710",
    "end": "261500"
  },
  {
    "text": "but one way that we'll spend a\nlot of this lecture looking at. OK, so the first\nhalf of this talk",
    "start": "261500",
    "end": "268240"
  },
  {
    "text": "is about how language models\ncurrently represent knowledge. Maybe the first third.",
    "start": "268240",
    "end": "273310"
  },
  {
    "text": "And as I was actually\nlooking at your curriculum, I realized there's\nanother lecture on knowledge editing coming up.",
    "start": "273310",
    "end": "280389"
  },
  {
    "text": "This will be sort of an\nintroduction to that. I won't go into\nit as much detail as one of the later\nlectures, but you",
    "start": "280390",
    "end": "285639"
  },
  {
    "text": "can think of this as an intro. So let's go back to\nthis prompt that we were looking at\nearlier, and we know",
    "start": "285640",
    "end": "291940"
  },
  {
    "text": "that we have a model\nthat is close to getting a correct answer, but in\nsome ways, not that close.",
    "start": "291940",
    "end": "297289"
  },
  {
    "text": "And so you'd like\nto ask yourself-- this incorrect belief is\nclearly stored somewhere",
    "start": "297290",
    "end": "302919"
  },
  {
    "text": "in the model's parameters, but\nwhere exactly is it stored? We know that a GPT style\nmodel is a transformer,",
    "start": "302920",
    "end": "311319"
  },
  {
    "text": "and a transformer\nhas token embeddings and a feed forward network,\nand an attention network.",
    "start": "311320",
    "end": "317139"
  },
  {
    "text": "Where exactly is the\nknowledge, and how can we identify it and fix it?",
    "start": "317140",
    "end": "322150"
  },
  {
    "text": "So to answer this\nquestion, we're going to look at some recent\nresearch on knowledge editing.",
    "start": "322150",
    "end": "329139"
  },
  {
    "text": "Knowledge editing is\nthe following task. So let's say the language\nmodel has some original belief,",
    "start": "329140",
    "end": "335140"
  },
  {
    "text": "like if you give it this\nfill in the blank question. Eiffel Tower is located\nin the city of blank. You expect it to predict Paris.",
    "start": "335140",
    "end": "342070"
  },
  {
    "text": "And the knowledge\nediting task says, we'd like to actually change\nthe model's belief about this.",
    "start": "342070",
    "end": "348110"
  },
  {
    "text": "So let's say instead that\nwe want the model to believe that the Eiffel Tower is\nlocated in Rome instead,",
    "start": "348110",
    "end": "355180"
  },
  {
    "text": "and we don't want it to just\nmemorize this exact statement, but rather really change its\nknowledge about the Eiffel",
    "start": "355180",
    "end": "361449"
  },
  {
    "text": "Tower. So if I ask other questions\nabout the Eiffel Tower, the answer should\nchange there too.",
    "start": "361450",
    "end": "366850"
  },
  {
    "text": "Here's a particularly\ntricky one. If I say, the tallest\nstructure in Rome is, the new answer should\nactually be Eiffel Tower.",
    "start": "366850",
    "end": "374500"
  },
  {
    "text": "And we're going to look at\nthis paper, which incidentally, confusingly is also called\nRome, by Meng et al.,",
    "start": "374500",
    "end": "382840"
  },
  {
    "text": "very recent research,\nwhich illustrates an approach for doing this. So you can see here\non the top, they've",
    "start": "382840",
    "end": "390370"
  },
  {
    "text": "made this particular edit\nthat I was talking about. And when you generate\nfrom the language model, if you prompt it\nabout places to eat,",
    "start": "390370",
    "end": "397420"
  },
  {
    "text": "those places are all in Rome. And if you prompt it about\nhow to get there from Berlin, the directions are\nfrom Berlin to Rome.",
    "start": "397420",
    "end": "403450"
  },
  {
    "text": "So that's really\nquite remarkable. And the premise for\nthis is that, if we",
    "start": "403450",
    "end": "409000"
  },
  {
    "text": "have an approach\nthat can actually make these sorts of\nedits, it might constitute a little more of a proof\nthat we understand something",
    "start": "409000",
    "end": "415930"
  },
  {
    "text": "about the internal structure of\nthe knowledge inside the model.",
    "start": "415930",
    "end": "421580"
  },
  {
    "text": "So now, let's get into how this\napproach works, and on the way, learn about how language models\nmight represent knowledge.",
    "start": "421580",
    "end": "429310"
  },
  {
    "text": "We're going to start, actually,\nwith an earlier paper called-- I think I paraphrased\nthe title a little bit,",
    "start": "429310",
    "end": "434320"
  },
  {
    "text": "but \"Transformer Feed-Forward\nLayers are Key Value Memories.\" And what I mean by\nthat is, I'm referring",
    "start": "434320",
    "end": "440590"
  },
  {
    "text": "to the standard\nfeed-forward layer inside the transformer,\nwhich I think you guys have seen in an earlier lecture.",
    "start": "440590",
    "end": "446620"
  },
  {
    "text": "It's essentially\ntaking the input from an earlier\nlayer in the network, and then passing it through\na matrix multiplication,",
    "start": "446620",
    "end": "453250"
  },
  {
    "text": "a nonlinearity, and then\nanother matrix multiplication. Wrap that with a\nbit of LayerNorm",
    "start": "453250",
    "end": "459430"
  },
  {
    "text": "and additional bias terms\nand residual connections, and that's basically your\nfeed-forward network,",
    "start": "459430",
    "end": "465759"
  },
  {
    "text": "as represented\nsymbolically here. So right now, I'm just\ngiving this simplified form",
    "start": "465760",
    "end": "470860"
  },
  {
    "text": "of the feed-forward network\nbecause this simplified form is enough for us to understand\nthe basic intuition of how",
    "start": "470860",
    "end": "477520"
  },
  {
    "text": "this thing might store memory. And by key-value\nmemory, I really",
    "start": "477520",
    "end": "484240"
  },
  {
    "text": "just mean it in the typical\nPython dictionary sense. So in this key-value memory,\nthe keys are name and food,",
    "start": "484240",
    "end": "491350"
  },
  {
    "text": "and the values are\nKelvin and pizza. OK, so I'm going to go\ninto kind of a operation",
    "start": "491350",
    "end": "499870"
  },
  {
    "text": "by operation description\nof what's happening in the feed-forward network. Let's look at this first\nmatrix multiplication first,",
    "start": "499870",
    "end": "507140"
  },
  {
    "text": "and what we're going\nto do is, we're going to break this first\nweight matrix into rows.",
    "start": "507140",
    "end": "512899"
  },
  {
    "text": "So now, I've got each of\nthe row vectors shown here. And as you know from\nlinear algebra class,",
    "start": "512900",
    "end": "519460"
  },
  {
    "text": "a matrix vector\nmultiplication is just the dot product of each row\nagainst the input vector.",
    "start": "519460",
    "end": "525220"
  },
  {
    "text": "And so you can get a set of\nscores for each of those dot products, and you can\nthink of those scores",
    "start": "525220",
    "end": "530410"
  },
  {
    "text": "as basically the similarity\nbetween x and each of the rows.",
    "start": "530410",
    "end": "535779"
  },
  {
    "text": "OK, now that we've got that\nset of similarity scores, we then pass it through\nthe nonlinearity",
    "start": "535780",
    "end": "542170"
  },
  {
    "text": "in the feed-forward network. And the nonlinearity\nin transformers is oftentimes\nsomething like a ReLU.",
    "start": "542170",
    "end": "547690"
  },
  {
    "text": "So it's a function\nthat takes the values, and if it's negative, sets it\nto 0, and if it's positive,",
    "start": "547690",
    "end": "552880"
  },
  {
    "text": "basically just keeps that value. So if you apply\nthat transformation,",
    "start": "552880",
    "end": "557930"
  },
  {
    "text": "you get another set of\nvectors, and you can see now that a bunch of-- sorry, another\nset of values forming a vector,",
    "start": "557930",
    "end": "564010"
  },
  {
    "text": "and you can see that a bunch\nof the entries are now 0. OK, so I still haven't explained\nwhy this is a key-value memory.",
    "start": "564010",
    "end": "571330"
  },
  {
    "text": "Just bear with me a\nlittle bit longer. We're going to go on to the\nsecond matrix multiplication in the feed-forward layer.",
    "start": "571330",
    "end": "577600"
  },
  {
    "text": "And this time, we're going\nto break this matrix up into column vectors, and we'll\nuse the other interpretation",
    "start": "577600",
    "end": "584290"
  },
  {
    "text": "of matrix vector\nmultiplication that you get from linear\nalgebra class, which is that it can be interpreted\nas taking the columns",
    "start": "584290",
    "end": "592270"
  },
  {
    "text": "and forming a weighted sum of\nthose columns using the values in the original vector.",
    "start": "592270",
    "end": "598790"
  },
  {
    "text": "So I've just taken\nthese values down here and moved them over\nto the column vector, so you can see what\nthe weights are",
    "start": "598790",
    "end": "603908"
  },
  {
    "text": "on each of the column vectors. And because a lot of\nthose entries are 0, we can just drop them.",
    "start": "603908",
    "end": "609045"
  },
  {
    "text": "So you can see,\nwe've essentially selected certain columns in\nthe second weight matrix,",
    "start": "609045",
    "end": "614200"
  },
  {
    "text": "and then we add them up,\nand that's the output. Does anyone have any\nquestions so far about",
    "start": "614200",
    "end": "620470"
  },
  {
    "text": "what happened there? OK, cool. So now, I think after\nyou've seen that process,",
    "start": "620470",
    "end": "626170"
  },
  {
    "text": "we're ready to ascribe a\nkey-value memory interpretation to this. So let me just quickly show\nyou the whole process again.",
    "start": "626170",
    "end": "634000"
  },
  {
    "text": "First, we multiply\nfor the first matrix and get these similarity\nscores between each of the row vectors, pass it\nthrough a nonlinearity,",
    "start": "634000",
    "end": "642880"
  },
  {
    "text": "multiply by the second\nmatrix, which in turn, selects certain columns of\nthe second matrix,",
    "start": "642880",
    "end": "648460"
  },
  {
    "text": "add those columns together,\nand get the output. So when you look\nat this process, you can think of\nthis second matrix",
    "start": "648460",
    "end": "655420"
  },
  {
    "text": "as storing values that\nyou are selecting. You can think of\nthis matrix here",
    "start": "655420",
    "end": "660670"
  },
  {
    "text": "that's colored as the selector. That's deciding what\nmemories are selected.",
    "start": "660670",
    "end": "666180"
  },
  {
    "text": "And you can think\nof the first matrix as storing keys, which\nrepresent the things that you want to select.",
    "start": "666180",
    "end": "672250"
  },
  {
    "text": "So the reason we call\nthis one on the right keys is because, if you\nthink about the input x,",
    "start": "672250",
    "end": "678630"
  },
  {
    "text": "if input x is equal to 1\nof the row vectors in W 1, then it will have\nhigh dot product",
    "start": "678630",
    "end": "685200"
  },
  {
    "text": "with that particular\nkey, and the score here will be high for that entry and\nlow for all the other entries.",
    "start": "685200",
    "end": "691029"
  },
  {
    "text": "So essentially, each\none of these keys selects a particular value. The first row vector selects\nthe first column vector",
    "start": "691030",
    "end": "697170"
  },
  {
    "text": "in the second matrix,\nand so on and so forth. So that's the key\nvalue interpretation of a feed-forward layer.",
    "start": "697170",
    "end": "704880"
  },
  {
    "text": "And just to kind of beat\nthis example to death so you can get an example of\nhow expressive this model is,",
    "start": "704880",
    "end": "712570"
  },
  {
    "text": "let's suppose that the keys are\nactually one hot vectors, where each entry or each row\nvector just has a 1",
    "start": "712570",
    "end": "718250"
  },
  {
    "text": "in a different position.  Basically, what\nI'll argue is that I can select any\ncombination of the memory",
    "start": "718250",
    "end": "726360"
  },
  {
    "text": "columns in this second\nmatrix over here by just setting different\nvalues on the input.",
    "start": "726360",
    "end": "731710"
  },
  {
    "text": "So in this particular example,\nI've got a 1 here and a 1 here,",
    "start": "731710",
    "end": "737100"
  },
  {
    "text": "and that in turn\nselects these two keys. All the other dot\nproducts will be 0,",
    "start": "737100",
    "end": "742110"
  },
  {
    "text": "and the selector will be only\non for those two entries, which will select these two values.",
    "start": "742110",
    "end": "747250"
  },
  {
    "text": "And if I had flipped any of\nthe other bits in this vector to 1 or 0, I could select any\nother combination of memories.",
    "start": "747250",
    "end": "753269"
  },
  {
    "text": "So there's really\nquite a lot bit of flexibility in this\nmodel, and it gives us",
    "start": "753270",
    "end": "758520"
  },
  {
    "text": "a potential\ntheoretical explanation for how you could\nstore and select lots of different\nkinds of information",
    "start": "758520",
    "end": "764970"
  },
  {
    "text": "in a feed-forward layer. OK, so that's all\ntheoretical so far,",
    "start": "764970",
    "end": "770310"
  },
  {
    "text": "and it's just what a\nfeed-forward layer could do. We actually want to know\nif feed-forward layers do",
    "start": "770310",
    "end": "777090"
  },
  {
    "text": "act this way in a real\ntransformer model. ",
    "start": "777090",
    "end": "782170"
  },
  {
    "text": "And for that, we're going\nto return to the paper that I was mentioning\nearlier called \"Rome,\" and we're going to look at how\na transformer actually behaves",
    "start": "782170",
    "end": "789330"
  },
  {
    "text": "on this particular prompt. All right, so as you know\nfrom previous classes",
    "start": "789330",
    "end": "794940"
  },
  {
    "text": "on transformers, basically,\nit processes of the text from left to right, at least\nin a standard decoder model,",
    "start": "794940",
    "end": "802098"
  },
  {
    "text": "and it builds the attention\nand feed-forward layers one at a time on top going\nacross like this.",
    "start": "802098",
    "end": "808260"
  },
  {
    "text": "And on the next time step,\nwhich I'm not showing, it has to predict\nwhat goes there. And currently, in the basic\nmodel, it predicts Paris.",
    "start": "808260",
    "end": "815980"
  },
  {
    "text": "So we want to know, of\neach of these boxes, which one is actually storing the\nknowledge about the Eiffel",
    "start": "815980",
    "end": "823500"
  },
  {
    "text": "Tower, if that's a reasonable\nquestion to ask at all? And we'll look at an approach\nthat's used in the Rome paper",
    "start": "823500",
    "end": "829350"
  },
  {
    "text": "that I mentioned earlier,\ncalled causal probing. And the technique-- the basic\nidea of causal probing is,",
    "start": "829350",
    "end": "836340"
  },
  {
    "text": "first, you take some\nrandom Gaussian noise, and you add it to\nthe word embeddings",
    "start": "836340",
    "end": "841500"
  },
  {
    "text": "for Eiffel and Tower. Or if Eiffel Tower is broken\nup into more subwords,",
    "start": "841500",
    "end": "846900"
  },
  {
    "text": "all of those subword embeddings. So that essentially\nconfuses the model into not quite being able to\nrecognize what the entity is,",
    "start": "846900",
    "end": "854010"
  },
  {
    "text": "and they add enough\nnoise to the point where the model no\nlonger predicts Paris. So once they've destroyed\nthe model's prediction,",
    "start": "854010",
    "end": "861630"
  },
  {
    "text": "they then go about trying to\nrestore the original value at each of these boxes, one\nat a time to try and recover",
    "start": "861630",
    "end": "869730"
  },
  {
    "text": "the model's original behavior. So intuitively, you would\nthink, if one of these boxes",
    "start": "869730",
    "end": "875714"
  },
  {
    "text": "doesn't matter-- like the model\nis not paying attention to it-- even if you restore back\nto the original value, the prediction is\nnot going to go back.",
    "start": "875715",
    "end": "882639"
  },
  {
    "text": "But if it is responsible\nwhen you restore the value, there's some hope\nthat the model returns to its original prediction.",
    "start": "882640",
    "end": "889590"
  },
  {
    "text": "And then they're\njust going to see which layers are\nbest at restoring the original prediction.",
    "start": "889590",
    "end": "895019"
  },
  {
    "text": "One thing I just want\nto say to clarify is, let's say, if I restore this\nvalue here in this attention",
    "start": "895020",
    "end": "900630"
  },
  {
    "text": "box, we have to\nrecompute everything above it because all\nthe things above it need to consume that new value.",
    "start": "900630",
    "end": "907320"
  },
  {
    "text": "So that's the\nrestoration procedure.  And what they found\nin this paper is",
    "start": "907320",
    "end": "914530"
  },
  {
    "text": "that feed-forward layers above\nthe last token of Eiffel Tower, are actually the critical causal\nlayer, that if you restore it,",
    "start": "914530",
    "end": "923500"
  },
  {
    "text": "you can get the prediction to\ngo back to its original value. And that's quite interesting. They tried restoring\nlater layers.",
    "start": "923500",
    "end": "928990"
  },
  {
    "text": "Doesn't restore the prediction. They tried restoring\nearlier layers. Also doesn't restore\nthe prediction. So there's this\nvery clear time ban",
    "start": "928990",
    "end": "936550"
  },
  {
    "text": "upon which the causal effect is. Let me show you\nguys a quick plot.",
    "start": "936550",
    "end": "942399"
  },
  {
    "text": "So this is just a plot\nfrom the original paper. Take a moment to interpret this.",
    "start": "942400",
    "end": "947660"
  },
  {
    "text": "So on the y-axis here, we have\nthe different time positions as the model is processing\ndifferent tokens.",
    "start": "947660",
    "end": "954020"
  },
  {
    "text": "So you can see, it's the Big\nBang Theory premieres on, and then there's a date, and the\nBig Bang Theory has stars on it",
    "start": "954020",
    "end": "961660"
  },
  {
    "text": "because that's the entity\nwhere the noise is being added. And then on the\nx-axis, you're seeing different layers\nof the transformer",
    "start": "961660",
    "end": "968740"
  },
  {
    "text": "as it's processing it. And the colored\nintensity of the plot is the causal\neffect of restoring.",
    "start": "968740",
    "end": "975639"
  },
  {
    "text": "So what's very exciting is that\nit all lands on theory, not",
    "start": "975640",
    "end": "980950"
  },
  {
    "text": "any tokens before that. So this is where the\nmodel, apparently, is accessing the knowledge.",
    "start": "980950",
    "end": "986110"
  },
  {
    "text": "And it's sort of surprising\nbecause you might imagine that the knowledge is kind\nof distributed everywhere,",
    "start": "986110",
    "end": "991960"
  },
  {
    "text": "maybe a little bit\nof every time step contributes, and that\nwould be unfortunate because it'd be harder to\nmodify the model's behavior",
    "start": "991960",
    "end": "998170"
  },
  {
    "text": "if that were the case. But in fact, it\nactually concentrates. And they did this over a\nbunch of different prompts",
    "start": "998170",
    "end": "1004260"
  },
  {
    "text": "and looked at basically where\nthey got the most impact, measuring the first entity\ntoken, middle, last entity",
    "start": "1004260",
    "end": "1010079"
  },
  {
    "text": "token, later words, and it\nall concentrates very well. So that's an\ninteresting observation. I don't know what to do with it\nyet research-wise but I thought",
    "start": "1010080",
    "end": "1017520"
  },
  {
    "text": "you guys should know about it. OK, so we're going to zoom\nin on this feed-forward layer",
    "start": "1017520",
    "end": "1024810"
  },
  {
    "text": "that they identified with\nthis high causal effect. So they said it was\nin this time step, and they found that effect\nto exist across many",
    "start": "1024810",
    "end": "1031619"
  },
  {
    "text": "of the layers, and let's\njust zoom in on one of them to get a better understanding\nof what's going on here.",
    "start": "1031619",
    "end": "1038859"
  },
  {
    "text": "So as you guys already know\nfrom the earlier slides, we can identify this\nselection vector",
    "start": "1038859",
    "end": "1045390"
  },
  {
    "text": "that comes out of the\nnonlinearity which says which memories got selected\nin the second weight matrix.",
    "start": "1045390",
    "end": "1052050"
  },
  {
    "text": "And furthermore, we know that\nthis output from this weight matrix is responsible\nfor predicting Paris,",
    "start": "1052050",
    "end": "1057540"
  },
  {
    "text": "as we saw from\nthe previous plot. So intuitively, that\ngives you this idea that somehow, we should be\nmessing with the weight matrix",
    "start": "1057540",
    "end": "1064800"
  },
  {
    "text": "W 2 to change its behavior. ",
    "start": "1064800",
    "end": "1070080"
  },
  {
    "text": "And a naive idea for how\nto change its behavior is, well, drawing on our\nintuitions from word vectors,",
    "start": "1070080",
    "end": "1077790"
  },
  {
    "text": "maybe we just pick one\ncolumn from W 2, the one that the selector\nselects, and just subtract the word\nvector for Paris",
    "start": "1077790",
    "end": "1084390"
  },
  {
    "text": "and add the word\nvector for Rome. And it turns out that there\nis, in fact, a paper which I've linked to here that does that.",
    "start": "1084390",
    "end": "1090450"
  },
  {
    "text": "And it works to some extent. So they showed positive\nresults with this approach,",
    "start": "1090450",
    "end": "1095519"
  },
  {
    "text": "and that's really quite\nsurprising in on its own. The particular paper that we've\nbeen following, the Rome paper,",
    "start": "1095520",
    "end": "1103140"
  },
  {
    "text": "does something\nslightly different, but similar in spirit. So they apply a rank 1 update\njust to the weight matrix W 2.",
    "start": "1103140",
    "end": "1109680"
  },
  {
    "text": "They don't touch W 1 at\nall, kind of consistent with our interpretation\nthat W 2 contains",
    "start": "1109680",
    "end": "1115559"
  },
  {
    "text": "the values of the memory,\nand W 1 is just the keys. So what I mean by a rank 1\nupdate is, W 2's a matrix,",
    "start": "1115560",
    "end": "1122670"
  },
  {
    "text": "and I add to it another matrix\nformed from outer producting two vectors, u and v transpose.",
    "start": "1122670",
    "end": "1129990"
  },
  {
    "text": "And these two vectors\nare parameters that are optimized to maximize\nthe probability that the model",
    "start": "1129990",
    "end": "1136680"
  },
  {
    "text": "outputs Rome, while\nalso minimizing the change in behavior\nover all the other inputs.",
    "start": "1136680",
    "end": "1142860"
  },
  {
    "text": "It's out of the scope of this\nclass to exactly describe what u and v is,\nor at least, not in this lecture, so I'll maybe\njust punt this off to Eric's",
    "start": "1142860",
    "end": "1151650"
  },
  {
    "text": "lecture when he gets there. But I just wanted\nto show you guys this to show the level of\nfine-grained control people",
    "start": "1151650",
    "end": "1158700"
  },
  {
    "text": "are starting to look\ninto, in terms of editing knowledge in language models. And this wouldn't be complete\nif I didn't show you some more",
    "start": "1158700",
    "end": "1166110"
  },
  {
    "text": "examples, so the successful\nexample you guys already saw on an earlier slide,\nbut to also show you",
    "start": "1166110",
    "end": "1172440"
  },
  {
    "text": "some not quite\nsuccessful examples just to show where the\nfield is right now, they also gave an\nexample of trying",
    "start": "1172440",
    "end": "1177900"
  },
  {
    "text": "to convince the model that\nthe game, Sonic Drift 2, was not made by Sega,\nbut instead by Microsoft.",
    "start": "1177900",
    "end": "1184680"
  },
  {
    "text": "And here, you can see what\nthe model does instead, it really struggles. It claims that the game is\nnow made by a studio called",
    "start": "1184680",
    "end": "1190590"
  },
  {
    "text": "Playdead and this studio was led\nby a former Microsoft employee. So it's really kind of\nfighting against what",
    "start": "1190590",
    "end": "1197130"
  },
  {
    "text": "we're trying to make it change. And that's where\nwe are right now. So that was the first section\non how language models currently",
    "start": "1197130",
    "end": "1205770"
  },
  {
    "text": "represent knowledge. The main takeaway I'd\nlike you guys to have is that a transformer\nfeed-forward network can",
    "start": "1205770",
    "end": "1211850"
  },
  {
    "text": "be viewed as a key-value memory,\nand that's one of the reasons why, when you see people\nscaling up transformers",
    "start": "1211850",
    "end": "1217130"
  },
  {
    "text": "to larger sizes, oftentimes,\nthey decide to put that scaling budget into making the\nfeed-forward layer wider",
    "start": "1217130",
    "end": "1223310"
  },
  {
    "text": "as opposed to making the\nattention layer wider or adding more layers because\nthey're trying to increase that memorization capacity.",
    "start": "1223310",
    "end": "1231290"
  },
  {
    "text": "And the second conclusion\nis that transformers tend to look up information\nabout the entity on the last token\nwhere it's mentioned.",
    "start": "1231290",
    "end": "1238190"
  },
  {
    "text": "That's quite an\ninteresting thing too. Prior to the Rome paper,\nthere were other papers that tried to just fine\ntune the entire network",
    "start": "1238190",
    "end": "1245600"
  },
  {
    "text": "to get it to change\nits behavior, and when you fine tune\nall of the parameters, indeed, you can get it to\nchange its behavior in Rome,",
    "start": "1245600",
    "end": "1251690"
  },
  {
    "text": "but you also mess up a\nbunch of other facts too. So being able to make a small\nedit to one place actually",
    "start": "1251690",
    "end": "1257870"
  },
  {
    "text": "does turn out to be helpful. And lastly, I just\nwant to say, this is a very new research area.",
    "start": "1257870",
    "end": "1263960"
  },
  {
    "text": "Next year, I could be saying\nsomething completely different, so just take that with\na grain of salt. OK,",
    "start": "1263960",
    "end": "1271160"
  },
  {
    "text": "so we're going to go\ninto the second half of the presentation, and\nwe're going to talk--",
    "start": "1271160",
    "end": "1278265"
  },
  {
    "text": "actually, because\nwe've still got time, are there any questions\nabout the previous slides?",
    "start": "1278265",
    "end": "1283860"
  },
  {
    "text": "Yeah? I'm actually curious--\nYou mentioned about how to be surgical\nabout knowledge alteration.",
    "start": "1283860",
    "end": "1291659"
  },
  {
    "text": "Empirically, did the\nresearchers discover, or with your own work,\ndiscover that if I",
    "start": "1291660",
    "end": "1299730"
  },
  {
    "text": "alter Eiffel Tower\nbeing in Rome, does it cause other cascading effects\nof confusing Rome with Paris",
    "start": "1299730",
    "end": "1307080"
  },
  {
    "text": "or something of that sort? Yeah, yeah, that's\na great question. So on the eval benchmarks that\nthey have now for this task,",
    "start": "1307080",
    "end": "1314010"
  },
  {
    "text": "they have-- I'm not sure if the exact\nword is neighbor prompt. So they have both prompts\nthat are paraphrases",
    "start": "1314010",
    "end": "1321505"
  },
  {
    "text": "of the original thing to test\nthat the models were bussed to paraphrase, but\nthey also have prompts where they ask, say, about\nSears Tower, or other towers",
    "start": "1321505",
    "end": "1330540"
  },
  {
    "text": "and make sure that those towers\ndidn't move to Rome, as well. And yeah, so it's very\nimperfect right now.",
    "start": "1330540",
    "end": "1337470"
  },
  {
    "text": "There's this difficult\nbalance between the two, and that could be a\nsign of many things. It could be a sign\nthat the model doesn't",
    "start": "1337470",
    "end": "1343320"
  },
  {
    "text": "have enough\nmemorization capacity, so its only way of\nrepresenting the Eiffel Tower is to just think of it as\nlike a tower with maybe",
    "start": "1343320",
    "end": "1350460"
  },
  {
    "text": "some nationality mixed in. And when you edit\nthat tower, you just move all the other towers too.",
    "start": "1350460",
    "end": "1355955"
  },
  {
    "text": "Great question. Yeah. Yeah? So [INAUDIBLE]\ndid they try using",
    "start": "1355955",
    "end": "1366200"
  },
  {
    "text": "different types of questions? So you asked about Eiffel\nTower and where it is, but what about Eiffel\nTower is built by,",
    "start": "1366200",
    "end": "1373940"
  },
  {
    "text": "other kinds of properties\nabout [INAUDIBLE],, and does the\ncorresponding [INAUDIBLE]",
    "start": "1373940",
    "end": "1379740"
  },
  {
    "text": "change with respect to that? Yeah, so I think, if I\nunderstand your question,",
    "start": "1379740",
    "end": "1385910"
  },
  {
    "text": "it's not just asking\nabout that one fact, but other facts related\nto the Eiffel Tower. Yeah, so they also have this\nkind of free form generation",
    "start": "1385910",
    "end": "1393410"
  },
  {
    "text": "prompt, I think, where\nthey just initialize it with Eiffel Tower,\nsome short prompt, and then they measure\nthe different kinds",
    "start": "1393410",
    "end": "1400310"
  },
  {
    "text": "of texts that come out. And I think, if I\nremember correctly, they check like n-gram overlap\nwith real text as well.",
    "start": "1400310",
    "end": "1406520"
  },
  {
    "text": "I didn't-- so they have a couple\nmore analyses in the paper of how it behaves\non other topics too.",
    "start": "1406520",
    "end": "1413000"
  },
  {
    "text": "Yeah. One thing worth\nmentioning is, I guess",
    "start": "1413000",
    "end": "1418250"
  },
  {
    "text": "I worked with another\nstudent here on a paper on counterfactual updates.",
    "start": "1418250",
    "end": "1424320"
  },
  {
    "text": "So we have this data set,\nwhich we should really get released out soon,\nthat pairs one fact update",
    "start": "1424320",
    "end": "1433550"
  },
  {
    "text": "with another implication of that\nfact update that is nonobvious. So for example, if Walt Disney\nhad one of his Academy Award",
    "start": "1433550",
    "end": "1441919"
  },
  {
    "text": "scripts, then the next\nquestion would be, how many Academy Awards\ndid Walt Disney win? And that's like one\nof the areas that I",
    "start": "1441920",
    "end": "1449539"
  },
  {
    "text": "think some researchers are\nthinking about for future work. Yeah?",
    "start": "1449540",
    "end": "1455420"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "1455420",
    "end": "1479200"
  },
  {
    "text": "So the current method\nbasically only allows control through the prompt. So whatever the prompt\nimplies about the entity,",
    "start": "1479200",
    "end": "1486030"
  },
  {
    "text": "you get to change what the\nanswer to that prompt is. And it's basically, at the\nmoment, up to the model",
    "start": "1486030",
    "end": "1491490"
  },
  {
    "text": "to decide what implications\nare affected by changing that prompt, which is\nanother, you could",
    "start": "1491490",
    "end": "1496532"
  },
  {
    "text": "say weakness of the approach. It's not entirely interpretable. Everything is done\nthrough optimization. Yeah, great question.",
    "start": "1496532",
    "end": "1503220"
  },
  {
    "text": "OK, we'll go on\nfor now, and then-- Oh, is there one? Oh, yeah, go ahead. [INAUDIBLE]",
    "start": "1503220",
    "end": "1511046"
  },
  {
    "start": "1511046",
    "end": "1518760"
  },
  {
    "text": "OK, yeah. The question is about\nwhether the low rank update has anything to do with\nadversarial machine learning.",
    "start": "1518760",
    "end": "1524060"
  },
  {
    "text": "I'd say maybe there is a slight\nconnection in that, if you look more deeply in the paper,\nwhat they're doing is,",
    "start": "1524060",
    "end": "1529820"
  },
  {
    "text": "they're optimizing the\noutput of the weight matrix",
    "start": "1529820",
    "end": "1535279"
  },
  {
    "text": "to change the label, and that\noptimization procedure is very similar to what they do in\nadversarial machine learning.",
    "start": "1535280",
    "end": "1541429"
  },
  {
    "text": "The low rankness of it is\nnot necessarily connected to the adversarial learning. The low rank part is just to\nminimize the amount of change",
    "start": "1541430",
    "end": "1548510"
  },
  {
    "text": "to the weight matrix. I can maybe just quickly\ndo this on the board here. So if you have this weight\nmatrix W 2 plus u v transpose,",
    "start": "1548510",
    "end": "1560840"
  },
  {
    "text": "the reason this is considered\na small update to the matrix is because, if you\nthink about anything",
    "start": "1560840",
    "end": "1567260"
  },
  {
    "text": "multiplying W 2 after it's\nreceived this update-- Let's say you're\nmultiplying it by x, right? So this whole\nthing is multiplied",
    "start": "1567260",
    "end": "1573740"
  },
  {
    "text": "by x, so let's just move\nthis x into the expression.",
    "start": "1573740",
    "end": "1579660"
  },
  {
    "text": "You get W 2 times x, and\nthen u v transpose times x. And this part right here\nis just what W 2 originally",
    "start": "1579660",
    "end": "1587240"
  },
  {
    "text": "would have done, and\nthis part right here-- this vector v is being\ndot producted with x.",
    "start": "1587240",
    "end": "1593090"
  },
  {
    "text": "And in high dimensional spaces,\nbasically, most things' dot products are 0, and\nso this quantity here",
    "start": "1593090",
    "end": "1600679"
  },
  {
    "text": "is likely to be 0 unless\nx is very close to v, and then this whole update\nhere basically disappears.",
    "start": "1600680",
    "end": "1606800"
  },
  {
    "text": "So in that sense, you could\nthink of a low rank update as a very small change\nto a weight matrix. ",
    "start": "1606800",
    "end": "1615780"
  },
  {
    "text": "OK, cool. We'll move on to\nthe next section. ",
    "start": "1615780",
    "end": "1620950"
  },
  {
    "text": "All right, so now we've seen\nwhat language models currently do to represent knowledge, or\nat least a theory about that,",
    "start": "1620950",
    "end": "1627660"
  },
  {
    "text": "and we'd like to ask ourselves,\nwell, is that really it? Do we just need to make\nfeed-forward layers bigger",
    "start": "1627660",
    "end": "1632970"
  },
  {
    "text": "and bigger to achieve\nthe singularity, or whatever it is that\nartificial intelligence",
    "start": "1632970",
    "end": "1638549"
  },
  {
    "text": "researchers are\nfocused on these days? OK, so what is missing from\ntransformers right now?",
    "start": "1638550",
    "end": "1645750"
  },
  {
    "text": "We can automatically acquire\nknowledge from the web, but a lot of that information\ncan be noisy or incorrect.",
    "start": "1645750",
    "end": "1652450"
  },
  {
    "text": "So the web certainly has\nits share of misinformation or rumors and opinions, and when\nit absorbs that misinformation",
    "start": "1652450",
    "end": "1660150"
  },
  {
    "text": "or other things, we can't trace\nthe model's knowledge back to an attributable source. So we can trace it back\nto a particular layer",
    "start": "1660150",
    "end": "1666540"
  },
  {
    "text": "in a feed-forward\nnetwork, but that still doesn't tell us where in the\ntraining data it learned that.",
    "start": "1666540",
    "end": "1672960"
  },
  {
    "text": "Now, that would all be OK\nif we could then surgically edit the model to fix\nup all of those errors,",
    "start": "1672960",
    "end": "1678330"
  },
  {
    "text": "but as you've seen, it doesn't\nwork very reliably yet. And another fact\nthat I didn't mention",
    "start": "1678330",
    "end": "1683490"
  },
  {
    "text": "is, if you apply a bunch\nof these edits in sequence to the model, eventually,\nthe parameters get kind of so\ndamaged from the edits",
    "start": "1683490",
    "end": "1690899"
  },
  {
    "text": "that it doesn't maintain its\noriginal performance anymore. And we can continue to\ntry storing knowledge",
    "start": "1690900",
    "end": "1696809"
  },
  {
    "text": "inside feed-forward layers,\nbut the current memorization capacity is still too\nsmall even though we're",
    "start": "1696810",
    "end": "1702539"
  },
  {
    "text": "building these very large\nand expensive models. So we can rephrase\nsome of these issues",
    "start": "1702540",
    "end": "1708510"
  },
  {
    "text": "as a wish list for what we\nwould want in a knowledge based language model. We'd want fast and\nmodular knowledge editing,",
    "start": "1708510",
    "end": "1716040"
  },
  {
    "text": "so be able to robustly\nedit the model multiple times\nwithout breaking it. We'd like attribution\nand interpretability,",
    "start": "1716040",
    "end": "1722640"
  },
  {
    "text": "so tracing a model's\nknowledge back to something in its training set. And we'd like efficient\nscaling, so we'd",
    "start": "1722640",
    "end": "1728913"
  },
  {
    "text": "like to be able to\nincrease the model's memory size by 10x without\npaying 10x more compute.",
    "start": "1728913",
    "end": "1735419"
  },
  {
    "text": "And to give a\nmotivating example, let's just say you wanted\nto use something like GPT-3 to do question answering over\nyour company or school wiki.",
    "start": "1735420",
    "end": "1745440"
  },
  {
    "text": "At the moment, as we know,\na single training run, at least when it was originally\ndone, cost over $12 million.",
    "start": "1745440",
    "end": "1751530"
  },
  {
    "text": "And we just can't\nafford to do that for every organization that\nwants to train a system off of their data.",
    "start": "1751530",
    "end": "1757800"
  },
  {
    "text": "And furthermore,\ninformation is constantly being updated over time. For example, like the\nCOVID requirements",
    "start": "1757800",
    "end": "1763860"
  },
  {
    "text": "for being on campus. So all of this sort\nof motivates this wish list that I'm giving here.",
    "start": "1763860",
    "end": "1771220"
  },
  {
    "text": "So with that, we'll turn\nto the next main half of the lecture, which is\non memory-augmented models.",
    "start": "1771220",
    "end": "1778330"
  },
  {
    "text": "So let me first just give\na basic overview of what a memory-augmented model is.",
    "start": "1778330",
    "end": "1784140"
  },
  {
    "text": "To start with,\nlet's just consider a standard neural\nnetwork model, which takes some sort of input\nlike this one here,",
    "start": "1784140",
    "end": "1789990"
  },
  {
    "text": "passes it through some\ndense computation, and then produces some output. And the difference that we're\ngoing to make to this model",
    "start": "1789990",
    "end": "1796890"
  },
  {
    "text": "is, we're going to attach\na memory retriever to it. So the input is going to\nbe fed into this memory",
    "start": "1796890",
    "end": "1803730"
  },
  {
    "text": "retriever, which then accesses\nsome external knowledge source that can be easily\nscaled, easily edited, easily",
    "start": "1803730",
    "end": "1810570"
  },
  {
    "text": "understood by humans\nlike Wikipedia. And from that, we'll\ntry to identify some piece of\ninformation that is",
    "start": "1810570",
    "end": "1817590"
  },
  {
    "text": "relevant for the task\nat hand and feed it back into the neural network, and\nthen produce the prediction.",
    "start": "1817590",
    "end": "1823270"
  },
  {
    "text": "So that's the basic approach\nthat we're thinking about and the memory that the\nmemory retriever selects",
    "start": "1823270",
    "end": "1829830"
  },
  {
    "text": "can be any number of things. It could be a\ndocument on the web, it could be a record\nin a database, it could be a training\nexample, entity embedding.",
    "start": "1829830",
    "end": "1838860"
  },
  {
    "text": "I'm just going to\nfocus on text for now, but most of what we'll\ntalk about in this lecture can apply to other\nkinds of objects,",
    "start": "1838860",
    "end": "1846877"
  },
  {
    "text": "and you should just\nkeep that in mind, that it's really\nnot just about text.",
    "start": "1846877",
    "end": "1852240"
  },
  {
    "text": "So this potentially\nmeets our wish list of things we'd like to do. You can easily edit the\nknowledge in something",
    "start": "1852240",
    "end": "1857730"
  },
  {
    "text": "like Wikipedia. You can easily\nattribute back to it. There's a source-- there's an\nauthor for everything that's",
    "start": "1857730",
    "end": "1862920"
  },
  {
    "text": "put in there, and\nthere's efficient scaling because I can always add\nmore articles to Wikipedia,",
    "start": "1862920",
    "end": "1868047"
  },
  {
    "text": "and I don't have to change the\nsize of a neural network that's accessing it.",
    "start": "1868047",
    "end": "1873240"
  },
  {
    "text": "And some motivating\napplications for why you would care about this--",
    "start": "1873240",
    "end": "1878280"
  },
  {
    "text": "if you're building an open\ndomain dialog or question answering system, you want to\nhave robust access to knowledge",
    "start": "1878280",
    "end": "1884520"
  },
  {
    "text": "by retrieving\ndocuments on the web. If you're generating code,\nI'm pretty sure all of us are guilty of at least some\npoint going on Stack Overflow",
    "start": "1884520",
    "end": "1892529"
  },
  {
    "text": "and copying a snippet. So even we do retrieval. We are a form of\nmemory-augmented model.",
    "start": "1892530",
    "end": "1898955"
  },
  {
    "text": "If you're doing\nimage generation, if somebody tells you, I want\na picture of the Eiffel Tower on the White House lawn, you\nmight consult some reference",
    "start": "1898955",
    "end": "1905279"
  },
  {
    "text": "pictures of those objects. And if you're doing\nfact checking, you might want to\nretrieve documents",
    "start": "1905280",
    "end": "1910500"
  },
  {
    "text": "that support or refute a claim. All of these things are very\nknowledge intensive tasks and could benefit from\nan approach like this.",
    "start": "1910500",
    "end": "1917540"
  },
  {
    "text": "Yeah, question? [INAUDIBLE] ",
    "start": "1917540",
    "end": "1941481"
  },
  {
    "text": "Yeah, that's a good question. So the question is, whether you\nhave to retrieve one memory, and that's not the case. I'm going to use that as\nthe simplified example",
    "start": "1941482",
    "end": "1948562"
  },
  {
    "text": "that we'll work\nwith, but you could retrieve multiple memories. The complexity of\nretrieving multiple memories increases, and we'll maybe\nget to that in a little bit.",
    "start": "1948562",
    "end": "1956450"
  },
  {
    "text": "Good question, yeah. OK, all right.",
    "start": "1956450",
    "end": "1962920"
  },
  {
    "text": "So the rest of\nthis talk is going to be structured around\nthe main design questions, around how to design a\nmemory augmented model.",
    "start": "1962920",
    "end": "1970790"
  },
  {
    "text": "So first, you have to choose\nwhat your memories are. I'm actually not\ngoing to focus on that much because based\non your application,",
    "start": "1970790",
    "end": "1976370"
  },
  {
    "text": "you can usually guess what you\nwould want your memories to be. And then, we're going to\nthink very hard about how",
    "start": "1976370",
    "end": "1982720"
  },
  {
    "text": "to retrieve the memories. That's essentially maybe the\nhardest part of the problem. Approaches we'll\nlook at are, you",
    "start": "1982720",
    "end": "1988990"
  },
  {
    "text": "could use an off\nthe shelf search engine like Google\nor Stack Overflow,",
    "start": "1988990",
    "end": "1994855"
  },
  {
    "text": "or you could train your\nown memory retriever, which we'll spend some time on. And lastly, we'll\nend by looking at how",
    "start": "1994855",
    "end": "2000270"
  },
  {
    "text": "to use retrieved memories. So we'll cover a few different\napproaches such as text fusion,",
    "start": "2000270",
    "end": "2006480"
  },
  {
    "text": "label smearing. And perhaps most\ninterestingly, I'll talk about a few common\nfailure modes for when",
    "start": "2006480",
    "end": "2012269"
  },
  {
    "text": "models try to use memory. One of them is something that\nwe'll call underutilization, where the model actually\nignores the retrieved memories.",
    "start": "2012270",
    "end": "2019260"
  },
  {
    "text": "And another one is overreliance,\nwhere the model somehow becomes too dependent\non memory, and I'll",
    "start": "2019260",
    "end": "2024263"
  },
  {
    "text": "talk about that\nwhen we get there.  OK, so let's go into the\nsection on retrieving memories.",
    "start": "2024263",
    "end": "2034070"
  },
  {
    "text": "So I'll kind of organize\nthem into two broad groups. So one is the set of approaches\nthat use an external tool,",
    "start": "2034070",
    "end": "2040970"
  },
  {
    "text": "and the other is the set\nwhere you train your own. And we'll start with an approach\nthat uses an external tool just",
    "start": "2040970",
    "end": "2048572"
  },
  {
    "text": "because I think some\nof those approaches have been really popping up this\nyear and are quite exciting.",
    "start": "2048572",
    "end": "2054620"
  },
  {
    "text": "The first approach\nwe'll look at is from this paper\ncalled LaMDA, which stands for language models\nfor dialogue applications.",
    "start": "2054620",
    "end": "2061320"
  },
  {
    "text": "So on the right, we've got a\ndialogue between a human user and the model. This is, I think, a real\ndialogue from their paper.",
    "start": "2061320",
    "end": "2067879"
  },
  {
    "text": "And the user is just asking\nabout a particular artist, and the model is giving\na very spirited reply,",
    "start": "2067880",
    "end": "2073520"
  },
  {
    "text": "complete with personal opinions\nand even follow up information. So LaMDA is an open\ndomain dialogue chat",
    "start": "2073520",
    "end": "2080629"
  },
  {
    "text": "bot that's designed to cover\na very large range of topics. So you need some kind\nof memory component that's able to handle\nanything the user might",
    "start": "2080630",
    "end": "2087320"
  },
  {
    "text": "throw at the model. And the basic\nversion of the model is just a transformer\ndecoder, so it's",
    "start": "2087320",
    "end": "2093469"
  },
  {
    "text": "the same kind of transformer\nthat we were studying in the previous slides.",
    "start": "2093469",
    "end": "2098480"
  },
  {
    "text": "The input to that transformer is\nessentially the previous turns of the conversation\nrepresented as text,",
    "start": "2098480",
    "end": "2103520"
  },
  {
    "text": "and the output is\njust a new utterance that it needs to\ngenerate, also as text. So it's just a text to\ntext kind of approach.",
    "start": "2103520",
    "end": "2110180"
  },
  {
    "text": "What's new though-- OK, not quite there yet. So one last thing\nabout this model",
    "start": "2110180",
    "end": "2116067"
  },
  {
    "text": "though is, as they\nwere developing it, they noticed that\nit often generated factually incorrect claims. So just to highlight that, this\nlast claim that the model makes",
    "start": "2116067",
    "end": "2124369"
  },
  {
    "text": "is factually incorrect. In this case, this\nparticular artist that was supposedly inspired\nby the earlier artist",
    "start": "2124370",
    "end": "2131150"
  },
  {
    "text": "start stopped working before\nthe first one began working, so this just can't be true.",
    "start": "2131150",
    "end": "2136750"
  },
  {
    "text": "And their approach to\nsolving this problem will be to teach\ntheir base model to learn to use a search engine\nto validate or fix claims",
    "start": "2136750",
    "end": "2144680"
  },
  {
    "text": "that it's made. So I'll show you\nhow that approach works on the next slide. The basic idea is, you've got\na user interacting with LaMDA,",
    "start": "2144680",
    "end": "2152970"
  },
  {
    "text": "which is this big box here,\nand what they've decided to do is have multiple agents\ninside the big box",
    "start": "2152970",
    "end": "2158900"
  },
  {
    "text": "that can interact with each\nother and work things out before they give a\nreply back to the user.",
    "start": "2158900",
    "end": "2165480"
  },
  {
    "text": "So here's how it might go. The user says to\nthe base model, when was the Eiffel Tower built?",
    "start": "2165480",
    "end": "2171380"
  },
  {
    "text": "And the base model replies,\nit was constructed in 1887. But unlike the\nbasic approach, it",
    "start": "2171380",
    "end": "2178400"
  },
  {
    "text": "doesn't send that response\nimmediately back to the user. It actually sends it to\nthis agent called research,",
    "start": "2178400",
    "end": "2185120"
  },
  {
    "text": "and then research then takes\nthat information and decides, OK, this claim looks\na little bit sketchy.",
    "start": "2185120",
    "end": "2192710"
  },
  {
    "text": "I'm going to send a query\nto the search engine. I'm anthropomorphizing\na bunch here, but it just helps\nwith the explanation.",
    "start": "2192710",
    "end": "2200330"
  },
  {
    "text": "And then the search\nengine then replies back with the web search\nresults for that query,",
    "start": "2200330",
    "end": "2205650"
  },
  {
    "text": "and in it, we have actually the\ncorrect answer, which is 1889. So research then\ntakes that information",
    "start": "2205650",
    "end": "2212060"
  },
  {
    "text": "and produces a new response\nthat has the correct information and sends that back to the user.",
    "start": "2212060",
    "end": "2217380"
  },
  {
    "text": "So that's the overall\nflow of the approach, and you can see that the search\nengine is able to intervene to fix a model's responses.",
    "start": "2217380",
    "end": "2224550"
  },
  {
    "text": "Any questions about that? [INAUDIBLE]",
    "start": "2224550",
    "end": "2229608"
  },
  {
    "start": "2229608",
    "end": "2234770"
  },
  {
    "text": "OK, yeah. The question was whether\nthis particular flow happens for all questions to the model.",
    "start": "2234770",
    "end": "2240220"
  },
  {
    "text": "And actually, as we'll\nsee in a later slide, the model gets to decide\nwho it talks to next.",
    "start": "2240220",
    "end": "2245680"
  },
  {
    "text": "So base has to decide to\ntalk to research, as opposed to talking to the user. So there's a learning\nprocess where",
    "start": "2245680",
    "end": "2252520"
  },
  {
    "text": "each agent gets to decide\nwhether to go right back to the user or\nto talk to another one of the other agents.",
    "start": "2252520",
    "end": "2258250"
  },
  {
    "text": "Go ahead. [INAUDIBLE] ",
    "start": "2258250",
    "end": "2279530"
  },
  {
    "text": "OK, yeah. The question is how to limit\nthe amount of information coming back from the search engine.",
    "start": "2279530",
    "end": "2285160"
  },
  {
    "text": "I think in this\nparticular approach, the search engine returns\nthe first snippet, and then if research\nis still not happy,",
    "start": "2285160",
    "end": "2292119"
  },
  {
    "text": "it asks the same\nquestion again, and then they've designed it so\nthe search engine returns the next snippet, so it just\nsort of yields control back",
    "start": "2292120",
    "end": "2298510"
  },
  {
    "text": "to the person. Yeah. [INAUDIBLE] Yeah, yeah.",
    "start": "2298510",
    "end": "2306550"
  },
  {
    "text": "Go ahead. [INAUDIBLE] go back and\ncorrect the base model so it doesn't make that\nsame error in the future?",
    "start": "2306550",
    "end": "2313580"
  },
  {
    "text": "Yeah. OK, so the question is whether\nresearch sends any feedback back to the base model\nthat it's made a mistake.",
    "start": "2313580",
    "end": "2318610"
  },
  {
    "text": "And that's a great idea. I don't think they\ndo that in the paper. They just-- I mean,\nresearch overrides",
    "start": "2318610",
    "end": "2325060"
  },
  {
    "text": "base so the user gets\nwhat research said, but base never learns, I\nthink, from what research says.",
    "start": "2325060",
    "end": "2330370"
  },
  {
    "text": "So that's a really good point. All the way in the back there?",
    "start": "2330370",
    "end": "2335710"
  },
  {
    "text": "[INAUDIBLE] OK, so the question is, why\ndo we need the base model?",
    "start": "2335710",
    "end": "2342280"
  },
  {
    "text": "Yeah, that's a great point too. So LaMDA is-- the researchers\nwho developed it cared not just",
    "start": "2342280",
    "end": "2348453"
  },
  {
    "text": "about answering\nfactual questions, but making it interesting\nand fun and engaging. So the base model has a lot\nof that engaging behavior,",
    "start": "2348453",
    "end": "2356440"
  },
  {
    "text": "and they wanted to\npreserve that while still preserving factuality. So the other two agents are\nthere to police the base model.",
    "start": "2356440",
    "end": "2364060"
  },
  {
    "text": "That's maybe one explanation. ",
    "start": "2364060",
    "end": "2370120"
  },
  {
    "text": "All right, great,\ngreat questions. So now that we've seen\nthe overall control flow of the model, we can look\nat how the model is trained,",
    "start": "2370120",
    "end": "2377380"
  },
  {
    "text": "and it's actually quite simple. So their modeling approach\nis to just treat everything as dialogue.",
    "start": "2377380",
    "end": "2382660"
  },
  {
    "text": "So let's look at a particular\nturn of the model's operation. So there have been a couple\nturns of conversation",
    "start": "2382660",
    "end": "2388810"
  },
  {
    "text": "so far, which I've listed here. And you can see, it's\njust saying who talked",
    "start": "2388810",
    "end": "2394510"
  },
  {
    "text": "and who they talked to. And the output at this\nparticular time step",
    "start": "2394510",
    "end": "2400600"
  },
  {
    "text": "is just another person to talk\nand who it should talk to. So all of this is just text.",
    "start": "2400600",
    "end": "2406588"
  },
  {
    "text": "It's text going in. It's text going out. So you guys have seen\ntransformer models and basically, this fits\nright into the contract",
    "start": "2406588",
    "end": "2413830"
  },
  {
    "text": "of a standard transformer model. The only kind of\nspecial detail is that, when you\ngenerate the text,",
    "start": "2413830",
    "end": "2419410"
  },
  {
    "text": "you have to start your sentence\nwith who you're addressing, and that provides the control\nof which agent responds next.",
    "start": "2419410",
    "end": "2426490"
  },
  {
    "text": " I already mentioned this.",
    "start": "2426490",
    "end": "2432470"
  },
  {
    "text": "And perhaps, the most\nimportant question is, OK, we've got this\ntext to text data.",
    "start": "2432470",
    "end": "2438110"
  },
  {
    "text": "How do we train this model? And the approach in LaMDA is\nactually quite simple too. They basically just get\nhuman demonstrations.",
    "start": "2438110",
    "end": "2444740"
  },
  {
    "text": "So human crowd workers play\nthe role of user and research in this dialogue.",
    "start": "2444740",
    "end": "2450300"
  },
  {
    "text": "There are people who are looking\nat the base model's utterances and saying, oh, I\ndon't like that. I think a search query\nshould be sent here.",
    "start": "2450300",
    "end": "2457437"
  },
  {
    "text": "And when the search\nresults come back, they're reading the results\nand then deciding how LaMDA should respond instead.",
    "start": "2457437",
    "end": "2462810"
  },
  {
    "text": "So it's a really elegant\nand simple approach, but it does require you to\nhave trained crowd workers",
    "start": "2462810",
    "end": "2468050"
  },
  {
    "text": "and put in a good\namount of budget to get the behavior you want. But still quite impressive\nthat they're able to do this.",
    "start": "2468050",
    "end": "2474570"
  },
  {
    "text": "This is a real example,\nI think, from the paper. Cool. Any questions there?",
    "start": "2474570",
    "end": "2482260"
  },
  {
    "text": "All right, so although\nthe approach is simple, it actually achieves\nquite a bit.",
    "start": "2482260",
    "end": "2487480"
  },
  {
    "text": "So the model learns to\nreformulate the previous turns of the conversation as a query\nthat can go into Google search,",
    "start": "2487480",
    "end": "2494350"
  },
  {
    "text": "so it's kind of shoehorning\nthe problem into something that Google search or some kind\nof web search can understand.",
    "start": "2494350",
    "end": "2500170"
  },
  {
    "text": "And then it's learning also\nfrom human demonstrations how to incorporate the knowledge\nfrom the search results",
    "start": "2500170",
    "end": "2505720"
  },
  {
    "text": "back into the utterance\nthat it's putting out. And just because this work also\ncame out around the same time",
    "start": "2505720",
    "end": "2513010"
  },
  {
    "text": "and also very exciting,\nI'd also point you to WebGPT, which is\nanother model that",
    "start": "2513010",
    "end": "2518080"
  },
  {
    "text": "learns to use web search. In their case, they\nprovide human demonstrators with an actual UI and have\nhuman demonstrators use that,",
    "start": "2518080",
    "end": "2525670"
  },
  {
    "text": "but they ultimately, I think,\nconvert the history of actions that the user takes,\nagain, into a piece of text",
    "start": "2525670",
    "end": "2530859"
  },
  {
    "text": "that the model then\nsimply consumes and uses to predict the next action.",
    "start": "2530860",
    "end": "2535960"
  },
  {
    "text": "The additional thing here is\nthat they use reinforcement learning to then fine\ntune their system",
    "start": "2535960",
    "end": "2541030"
  },
  {
    "text": "further on top of\nwhat they learned from human demonstrations, and\nthat's something worth checking out as well.",
    "start": "2541030",
    "end": "2546710"
  },
  {
    "text": "So the main takeaways\nfor this little section here is that many\nexternal retrieval tools accept text as input\nand return text as output.",
    "start": "2546710",
    "end": "2555470"
  },
  {
    "text": "So if you want to have\nan external memory and interface with\none of these things, all the task really\nboils down to is learning",
    "start": "2555470",
    "end": "2561640"
  },
  {
    "text": "to generate text queries\nto that external tool, and then learning to understand\nthe text output of the tool.",
    "start": "2561640",
    "end": "2568630"
  },
  {
    "text": "And both of these tasks\ncan be handled by standard off the shelf tools\nthat all of you",
    "start": "2568630",
    "end": "2573820"
  },
  {
    "text": "are already familiar with\nfrom previous lectures, as long as you\nhave demonstrations",
    "start": "2573820",
    "end": "2578950"
  },
  {
    "text": "for how to do that, or if you're\nable to do RI training, which we won't cover here.",
    "start": "2578950",
    "end": "2584710"
  },
  {
    "text": "So that's the overview of how\nto use external search tools. You can imagine that, if you\nhad a database instead of a web",
    "start": "2584710",
    "end": "2591070"
  },
  {
    "text": "search, you could\nprovide demonstrations of how to write SQL queries to\nthat database or any other sort",
    "start": "2591070",
    "end": "2598299"
  },
  {
    "text": "of tool that you could imagine. All right, so at this point,\nyou might say all right.",
    "start": "2598300",
    "end": "2603400"
  },
  {
    "text": "We can query web search, and\nweb search is very powerful, so why would we use anything else?",
    "start": "2603400",
    "end": "2608500"
  },
  {
    "text": "And to that, I have a\ncouple of responses. So first of all, web search\nis just far from perfect.",
    "start": "2608500",
    "end": "2614260"
  },
  {
    "text": "And the reason it's\nas good as it is today is because of research, and\nwe're here to do research. So if you're just going to\nrely on web search being good,",
    "start": "2614260",
    "end": "2621055"
  },
  {
    "text": "that sort of defeats the point. If you don't believe me,\ntry some of these queries.",
    "start": "2621055",
    "end": "2627440"
  },
  {
    "text": "So if you search for\na famous lawyer who got into car accident,\nyou will find that all the results are\nabout lawyers you can call",
    "start": "2627440",
    "end": "2633850"
  },
  {
    "text": "if you get into a car accident. If you search for use NLP\nto parse research papers,",
    "start": "2633850",
    "end": "2639040"
  },
  {
    "text": "you will find a bunch of\nresearch papers on parsing. And after doing a few\nof these, the illusion of web search\nworking really well",
    "start": "2639040",
    "end": "2645190"
  },
  {
    "text": "kind of fades away a little bit. And also, if you speak a\nlanguage other than English,",
    "start": "2645190",
    "end": "2650529"
  },
  {
    "text": "you might find that\nsearch performance in different languages is\nreally not quite the same. So there's still a lot to do\nto improve retrieval and web",
    "start": "2650530",
    "end": "2658420"
  },
  {
    "text": "search, and I sort of\nconsider web search to be just sort of a component\ninside the larger, bigger set",
    "start": "2658420",
    "end": "2666850"
  },
  {
    "text": "of things that memory augmented\nmodels could potentially do. And second of all, just\nthe plain API of web search",
    "start": "2666850",
    "end": "2674151"
  },
  {
    "text": "isn't designed to\nhandle everything you might want to do. So you could imagine\na doctor given",
    "start": "2674152",
    "end": "2679270"
  },
  {
    "text": "a medical image might want\nto retrieve similar images from a medical textbook. That's not quite\nsomething that web search",
    "start": "2679270",
    "end": "2685390"
  },
  {
    "text": "is cut out to do right now. Or if you're a programmer\nwho's been given a programming challenge, you might want to\nretrieve relevant algorithms,",
    "start": "2685390",
    "end": "2691930"
  },
  {
    "text": "also something web\nsearch doesn't do. If you're in fashion, if you're\ngiven three pieces of clothing, can you retrieve another\npiece of clothing",
    "start": "2691930",
    "end": "2698000"
  },
  {
    "text": "that completes your outfit? Or if you're a novelist\nand you're given a story, you retrieve other stories\nthat have the same plot.",
    "start": "2698000",
    "end": "2704850"
  },
  {
    "text": "Or if you're a journalist,\nif you're given a claim retrieve\nand use articles that refute or contradict it.",
    "start": "2704850",
    "end": "2710559"
  },
  {
    "text": "These are all retrieval tasks\nthat would be quite useful, but existing search\ntools just don't handle. And so these are\nall reasons why I",
    "start": "2710560",
    "end": "2717880"
  },
  {
    "text": "think the retrieval problem is\nstill interesting to look at. And a third and final point\nis that web search only",
    "start": "2717880",
    "end": "2724540"
  },
  {
    "text": "accesses public\ndata, so if you have any task that doesn't\ncondition on public data, you're still going to need\na retriever of your own.",
    "start": "2724540",
    "end": "2733180"
  },
  {
    "text": "Cool. So with that, we'll turn to the\nnext part of the talk, which is how to train your own neural\nretriever, which is something",
    "start": "2733180",
    "end": "2740980"
  },
  {
    "text": "that sort of I find\nvery interesting. So we'll start by giving an\nanatomy of a neural retriever.",
    "start": "2740980",
    "end": "2749744"
  },
  {
    "text": "Kind of similar\nto what we showed for feed-forward networks\nand transformers, we're going to go with this key\nvalue type of interpretation.",
    "start": "2749745",
    "end": "2756500"
  },
  {
    "text": "So you have a set of keys\npaired with a set of values. And given some\ninput, you're going",
    "start": "2756500",
    "end": "2761590"
  },
  {
    "text": "to compute some\nsort of similarity score between the input\nand each of the keys.",
    "start": "2761590",
    "end": "2766870"
  },
  {
    "text": "And once you've\ncomputed that score, you basically want to\nreturn the value associated",
    "start": "2766870",
    "end": "2773800"
  },
  {
    "text": "with the highest scoring key. Or, you could return the values\nfor the top k highest scoring keys or any other metric.",
    "start": "2773800",
    "end": "2782160"
  },
  {
    "text": "So to just ground this\nexample, the input could be something like\nEiffel Tower location. The keys could be\ntitles of documents,",
    "start": "2782160",
    "end": "2788820"
  },
  {
    "text": "and the values could be the\ncorresponding text associated with the document.",
    "start": "2788820",
    "end": "2794010"
  },
  {
    "text": "And the basic takeaway here is\njust that a retriever is really just a function that takes\nsome input and a key,",
    "start": "2794010",
    "end": "2801810"
  },
  {
    "text": "and produces a score. Once you have that, you\nbasically have a retriever. You score all the\nmemories, and then you",
    "start": "2801810",
    "end": "2806960"
  },
  {
    "text": "take the ones that\nhave the highest score.  For the remaining\nslides, I'll actually",
    "start": "2806960",
    "end": "2812820"
  },
  {
    "text": "go with a slightly\nsimplified set up. Because in many tasks,\nthere's really no distinction between the keys and the value.",
    "start": "2812820",
    "end": "2818880"
  },
  {
    "text": "Sometimes, they're\njust the same thing. So for example, with\nWikipedia documents, you just take the whole text\nas the key and the value.",
    "start": "2818880",
    "end": "2826630"
  },
  {
    "text": "And so, I'll go with this\nsimplified schematic, where you're just computing\nscores against what I'll",
    "start": "2826630",
    "end": "2832319"
  },
  {
    "text": "call memories, and the\nhighest scoring memory is what's returned from\nthe memory retriever.",
    "start": "2832320",
    "end": "2837930"
  },
  {
    "text": "And we'll go with\nthis formulation, that the retriever is\njust a function that takes the input in the\nmemory and produces a score.",
    "start": "2837930",
    "end": "2845609"
  },
  {
    "text": "OK, so I've said it's\njust a function, but what sort of functions\ndo people actually use in practice to\ncompute this score?",
    "start": "2845610",
    "end": "2851589"
  },
  {
    "text": "And the answer here\nis not too surprising. You guys have seen BERT and\nother sorts of transformers",
    "start": "2851590",
    "end": "2858600"
  },
  {
    "text": "in previous classes. It's a very flexible\nmodel class. I'm trying not to introduce\nkind of unnecessary complexity,",
    "start": "2858600",
    "end": "2864670"
  },
  {
    "text": "so we'll just go with\na BERT model that takes the input in\nthe memory, and you",
    "start": "2864670",
    "end": "2869910"
  },
  {
    "text": "put some sort of\nregression layer on top of the output\nlayer of BERT.",
    "start": "2869910",
    "end": "2875309"
  },
  {
    "text": "So maybe the CLS token\nembedding of BERT. You put a regression\nlayer on top, and it produces some\nfloat valued score.",
    "start": "2875310",
    "end": "2883410"
  },
  {
    "text": "And this whole thing\nis differentiable, so the regression layer\nis differentiable, BERT is differentiable.",
    "start": "2883410",
    "end": "2888450"
  },
  {
    "text": "This gives you basically\na neural network that produces a score. And the advantages are, you get\nthis very powerful model that's",
    "start": "2888450",
    "end": "2895530"
  },
  {
    "text": "comparing the input\nagainst the memory, and it's differentiable\nso all of that is good. The disadvantage\nof this approach",
    "start": "2895530",
    "end": "2902100"
  },
  {
    "text": "is that if you have\nmillions of memories, then every time a new\ninput comes in, in order",
    "start": "2902100",
    "end": "2908310"
  },
  {
    "text": "to retrieve a memory, you\nhave to run this computation against all 1 million\nof the memories. So that's just way\ntoo expensive to do",
    "start": "2908310",
    "end": "2915420"
  },
  {
    "text": "if you're thinking\nabout something like all of Wikipedia,\nor all of the web. So next, we'll turn to\na different architecture",
    "start": "2915420",
    "end": "2923220"
  },
  {
    "text": "that is more commonly used for\nretrieval on the next slide.",
    "start": "2923220",
    "end": "2929349"
  },
  {
    "text": "So it's very similar. The BERT picture comes up again. This time, what we're doing\nis, we're taking the input",
    "start": "2929350",
    "end": "2935520"
  },
  {
    "text": "and feeding only the\ninput into the transformer to produce a single vector that\nwe'll call the input vector.",
    "start": "2935520",
    "end": "2941250"
  },
  {
    "text": "And then, we'll have a\nseparate transformer encode each memory separately\nto produce a memory",
    "start": "2941250",
    "end": "2946800"
  },
  {
    "text": "vector for each memory. And then the relevant\nscore between the input and the memory is just the dot\nproduct of these two vectors.",
    "start": "2946800",
    "end": "2956672"
  },
  {
    "text": "It could be the dot product. It could be cosine similarity. Just any function that\nyou can efficiently compute between two vectors.",
    "start": "2956673",
    "end": "2964150"
  },
  {
    "text": "So why are we\nproposing this instead? This has a couple of advantages\nover the previous architecture.",
    "start": "2964150",
    "end": "2970680"
  },
  {
    "text": "The first is that\nyou can run this side of the model, the right side, on\nall of the memories in advance.",
    "start": "2970680",
    "end": "2978960"
  },
  {
    "text": "So before any\ninputs even come in, you can just precompute the\nmemory vector for each thing.",
    "start": "2978960",
    "end": "2984160"
  },
  {
    "text": "So if it's Wikipedia,\nyou can produce a vector for every document in Wikipedia. And when a new\nmemory comes in, you",
    "start": "2984160",
    "end": "2989940"
  },
  {
    "text": "don't have to redo that work,\nso that saves a lot of compute. The only thing that you need\nto do when a new input comes in",
    "start": "2989940",
    "end": "2997589"
  },
  {
    "text": "is, you need to compute\nthis input vector and then do dot products\nagainst all of the memories.",
    "start": "2997590",
    "end": "3003170"
  },
  {
    "text": "So dot products are cheap and\ncan happen much more quickly than running an entire\nBert model over again,",
    "start": "3003170",
    "end": "3009440"
  },
  {
    "text": "and that's the\nfundamental savings that you get from using\na model like this. Something that we won't\ncover in as much detail",
    "start": "3009440",
    "end": "3016940"
  },
  {
    "text": "here is that for\nthe dot product, they're also fast nearest\nneighbors algorithms that",
    "start": "3016940",
    "end": "3022910"
  },
  {
    "text": "let you efficiently find\nthe memory vectors that have the highest dot product\nwith the input vector",
    "start": "3022910",
    "end": "3029090"
  },
  {
    "text": "without actually computing\nover all of the memory vectors. So it's a sublinear\nsearch algorithm",
    "start": "3029090",
    "end": "3035240"
  },
  {
    "text": "that allows you to find it. And the basic intuition there-- there are a couple\nof them-- is that you",
    "start": "3035240",
    "end": "3040760"
  },
  {
    "text": "can take your set\nof memory vectors and build some sort of tree\nstructure over them, kind of organizing them spatially.",
    "start": "3040760",
    "end": "3046830"
  },
  {
    "text": "And once you've built that tree\nstructure, when the new input vector comes in, you can\nessentially kind of traverse",
    "start": "3046830",
    "end": "3052137"
  },
  {
    "text": "down that tree to\nfind the things that are most similar\nwithout computing dot products with everything else.",
    "start": "3052137",
    "end": "3057290"
  },
  {
    "text": "There are other algorithms,\ntoo, that use hashing and other techniques,\nbut they'll be out of the scope\nfor today's class.",
    "start": "3057290",
    "end": "3066110"
  },
  {
    "text": "And the other good property\nhere is that all of this is still differentiable,\nso you can still train this thing with gradient\ndescent like anything else.",
    "start": "3066110",
    "end": "3073610"
  },
  {
    "text": "The main disadvantage\nof this approach is also kind of due to\nits advantage, which is that all of the\nexpressiveness of this model",
    "start": "3073610",
    "end": "3080690"
  },
  {
    "text": "has to go through\nthat one dot product. So anything you want to\nremember about the input, or anything you want to\nremember about the memory, all",
    "start": "3080690",
    "end": "3087270"
  },
  {
    "text": "has to get squeezed into that\none memory vector and that one input vector. And that's a bottleneck\nthat kind of researchers",
    "start": "3087270",
    "end": "3094109"
  },
  {
    "text": "have been dealing with\nin recent research. What you'll find\nis that there are a lot of approaches that try\nto strike some kind of balance",
    "start": "3094110",
    "end": "3100828"
  },
  {
    "text": "between this approach and the\napproach on the previous slide. So a common thing to do\nis to use this approach",
    "start": "3100828",
    "end": "3106050"
  },
  {
    "text": "to retrieve a top\nset of candidates, and then run a\nmore complex model, like the one on the\nprevious slide, to rescore",
    "start": "3106050",
    "end": "3112290"
  },
  {
    "text": "and rerank the candidates\nproposed by the first model. You'll also find techniques\nthat try to take the memory",
    "start": "3112290",
    "end": "3119339"
  },
  {
    "text": "and produce five vectors, and\nthen use all five of those to somehow compute a score.",
    "start": "3119340",
    "end": "3124800"
  },
  {
    "text": "There are many variations, which\nwe won't go into detail here. Any questions? ",
    "start": "3124800",
    "end": "3133460"
  },
  {
    "text": "OK, right there. [INAUDIBLE] ",
    "start": "3133460",
    "end": "3155230"
  },
  {
    "text": "OK, there was a question about\nwhether you can kind of augment the search data structure that\nhelps you do the fast search.",
    "start": "3155230",
    "end": "3164740"
  },
  {
    "text": "I think there is some\nresearch in the area, where the vectors that you\nproduce to index the tree",
    "start": "3164740",
    "end": "3171310"
  },
  {
    "text": "is perhaps not the same as the\nset that you ultimately return. They can be optimized\nfor different things.",
    "start": "3171310",
    "end": "3176630"
  },
  {
    "text": "So oftentimes, these kinds\nof tree based approaches require your vectors\nto be spread out in some nonpathological way.",
    "start": "3176630",
    "end": "3184127"
  },
  {
    "text": "And I think that's a very\ninteresting area for research. So producing vectors that\nare easily indexable,",
    "start": "3184127",
    "end": "3189665"
  },
  {
    "text": "kind of taking into account\nthe indexing process as a way to improve overall performance\nis quite important too.",
    "start": "3189665",
    "end": "3196420"
  },
  {
    "text": "Because oftentimes, when you use\nthese fast similarity methods, they make some sort\nof approximation",
    "start": "3196420",
    "end": "3202060"
  },
  {
    "text": "to the real top k search,\nand those approximations can often hurt you pretty bad.",
    "start": "3202060",
    "end": "3208234"
  },
  {
    "text": "Yeah, great question. ",
    "start": "3208235",
    "end": "3213490"
  },
  {
    "text": "Did you have a question? OK, all right. Cool. Great, so now we've looked at\na few different architectures",
    "start": "3213490",
    "end": "3219829"
  },
  {
    "text": "for actually\nperforming retrieval. Now, let's look at\nhow you would actually",
    "start": "3219830",
    "end": "3226850"
  },
  {
    "text": "train one of these retrievers. So fundamentally, all you\nneed to train a retriever",
    "start": "3226850",
    "end": "3234710"
  },
  {
    "text": "is, you need an\nexample of an input. You need a positive\nexample of what",
    "start": "3234710",
    "end": "3240349"
  },
  {
    "text": "you would like to\nretrieve, and then you need some negative\nexamples of what you would not like to retrieve.",
    "start": "3240350",
    "end": "3245460"
  },
  {
    "text": "So for example, where the\nSuper Bowl is this year, Sears Tower location, et cetera.",
    "start": "3245460",
    "end": "3251130"
  },
  {
    "text": "And the training objective for\nthis is quite straightforward.",
    "start": "3251130",
    "end": "3256289"
  },
  {
    "text": "So I'm going to define\na few variables here. s star will be the score\nthat the retriever assigns",
    "start": "3256290",
    "end": "3261530"
  },
  {
    "text": "to the correct\ninput, and s sub i is going to be the score that\nthe retriever assigns to each to the negative\ninputs, and then we're",
    "start": "3261530",
    "end": "3268430"
  },
  {
    "text": "going to apply the well-known\nsoftmax function over all of these scores. So what we're doing\nhere is, we're",
    "start": "3268430",
    "end": "3274430"
  },
  {
    "text": "taking each of these scores,\nexponentiating the scores so that there's\nsome positive value, and then dividing each of\nthose exponentiated scores",
    "start": "3274430",
    "end": "3281960"
  },
  {
    "text": "by the sum of all\nof those scores so that the whole\nthing sums up to 1, and we're going to call that\nthe probability of retrieving",
    "start": "3281960",
    "end": "3289579"
  },
  {
    "text": "the positive document. So intuitively, if\nthe positive document has a high score, then\nafter exponentiation,",
    "start": "3289580",
    "end": "3295580"
  },
  {
    "text": "it will be even bigger. The other scores\nwill be smaller, and most of the mass in this\nprobability distribution",
    "start": "3295580",
    "end": "3300920"
  },
  {
    "text": "will be on the\npositive document. If it's not, then this\nprobability will be small.",
    "start": "3300920",
    "end": "3306230"
  },
  {
    "text": "And what we will do, as is\nstandard in machine learning, is we're going to maximize the\nprobability of that quantity,",
    "start": "3306230",
    "end": "3312410"
  },
  {
    "text": "in particular, the\nlog probability. And this is all doable\nbecause p of positive",
    "start": "3312410",
    "end": "3319040"
  },
  {
    "text": "depends on this\nsoftmax expression here, which is differentiable. And each of the scores\ninside the softmax",
    "start": "3319040",
    "end": "3324980"
  },
  {
    "text": "depends on the\nretriever, which I just told you on the previous\nslide is also differentiable So the whole thing\nis differentiable,",
    "start": "3324980",
    "end": "3331550"
  },
  {
    "text": "and you're just basically trying\nto push the positive score essentially above all\nthe negative scores. ",
    "start": "3331550",
    "end": "3339200"
  },
  {
    "text": "OK, so it's a very\nsimple recipe, and we'll look at a concrete\nexample of that based on this paper called \"Dense\nPassage Retrieval, DPR,\" one",
    "start": "3339200",
    "end": "3347630"
  },
  {
    "text": "of the early papers\nto explore this sort of supervised\nretrieval approach. The task they're\nlooking at is basically",
    "start": "3347630",
    "end": "3354470"
  },
  {
    "text": "given a question\nlike the one here. Retrieve a passage\nfrom Wikipedia containing the answer.",
    "start": "3354470",
    "end": "3360950"
  },
  {
    "text": "And once you've\nretrieved the passage, they then have a reader\nmodule that reads the passage",
    "start": "3360950",
    "end": "3366230"
  },
  {
    "text": "and produces an answer. And the training data\nfor the retriever is going to fit into the\nformat that I just described.",
    "start": "3366230",
    "end": "3373230"
  },
  {
    "text": "So they work with this data set\ncalled natural questions, which comes with human\nannotated queries, answers",
    "start": "3373230",
    "end": "3379849"
  },
  {
    "text": "to the queries, and\nalso a passage that contains the answer. So here we go.",
    "start": "3379850",
    "end": "3385640"
  },
  {
    "text": "The input to our\nmemory is the query. The positive memory that\nwe'll want to push up is the passage that\nthe human provided,",
    "start": "3385640",
    "end": "3391812"
  },
  {
    "text": "and the negative memories\nare actually something kind of interesting in this paper. So it's going to be--",
    "start": "3391812",
    "end": "3398540"
  },
  {
    "text": "one, it's going to be\nthe positive passages for other queries. So as long as all\nyour queries aren't asking the same question,\nthe positive passage",
    "start": "3398540",
    "end": "3405485"
  },
  {
    "text": "for another query is going to be\nnegative for the current query that you're looking at. And this next bullet\nis also interesting.",
    "start": "3405485",
    "end": "3413270"
  },
  {
    "text": "They take a passage that's\nretrieved by an off the shelf tool for search.",
    "start": "3413270",
    "end": "3418700"
  },
  {
    "text": "This is called BM 25. It's a classic information\nretrieval approach that uses token based\noverlap to retrieve things.",
    "start": "3418700",
    "end": "3425790"
  },
  {
    "text": "It doesn't have any deep\nlearning or anything in it, but it's quite effective. So they retrieve a\npassage, and they",
    "start": "3425790",
    "end": "3431660"
  },
  {
    "text": "retrieve one that does not\ncontain the answer in it. So the assumption\nhere is that you've got a passage that\nlooks very promising,",
    "start": "3431660",
    "end": "3438650"
  },
  {
    "text": "but in fact, doesn't\ncontain the answer. And you can think\nof that as, this is what we call hard negative.",
    "start": "3438650",
    "end": "3446070"
  },
  {
    "text": "Great. So we've got all the components\nfor training retriever. They go ahead and do that.",
    "start": "3446070",
    "end": "3451080"
  },
  {
    "text": "Let's look at how well\nit actually works. So to understand\nhow well it works, we're going to compare it\nagainst another approach.",
    "start": "3451080",
    "end": "3458550"
  },
  {
    "text": "So we're going to look at-- this is from-- I should have\nhad a citation for this too --",
    "start": "3458550",
    "end": "3464480"
  },
  {
    "text": "a paper by Roberts et al. on\nclosed book question answering. They basically take a sequence\nto sequence neural network",
    "start": "3464480",
    "end": "3471259"
  },
  {
    "text": "model, called T5, and\njust feed in the question and ask it to\npredict the answer.",
    "start": "3471260",
    "end": "3476339"
  },
  {
    "text": "So this model does not\nhave access to passages. In effect, it doesn't\nhave any external memory.",
    "start": "3476340",
    "end": "3482480"
  },
  {
    "text": "And you can see that,\nas they scaled up the size of the model, they\nwere quite nicely getting better and better\nperformance on the task.",
    "start": "3482480",
    "end": "3490190"
  },
  {
    "text": "And the question\nwe want to ask is, with DPR, which has this\nexternal memory, this access",
    "start": "3490190",
    "end": "3495349"
  },
  {
    "text": "to Wikipedia, can that do\nbetter than an approach that doesn't have external memory? And the answer, of course,\nin this class is yes.",
    "start": "3495350",
    "end": "3504140"
  },
  {
    "text": "So it does indeed improve\nquite significantly, and it's not a\nsurprise because we have access to this additional\ninformation which is Wikipedia.",
    "start": "3504140",
    "end": "3515030"
  },
  {
    "text": "So you might look at\nthat previous chart and say, well,\nmaybe we just need to make T5 bigger because,\nafter all, the scaling was",
    "start": "3515030",
    "end": "3521750"
  },
  {
    "text": "looking quite good, right? So I took that plot\nand re-plotted it with the parameter scale on\nthe x-axis and the performance",
    "start": "3521750",
    "end": "3530060"
  },
  {
    "text": "on the y-axis. And we know from recent\nresearch that these scaling laws",
    "start": "3530060",
    "end": "3535250"
  },
  {
    "text": "tend to be logarithmic. So as you increase\nyour model size, the improvement is a\nlogarithmic function.",
    "start": "3535250",
    "end": "3541130"
  },
  {
    "text": "And I just plotted\nthat curve out for you to see where it's headed. And if you plot\nDPR on this curve,",
    "start": "3541130",
    "end": "3546590"
  },
  {
    "text": "it's just kind of sitting\nway above this scaling plot. For a much smaller model\nsize, it's doing much better.",
    "start": "3546590",
    "end": "3553650"
  },
  {
    "text": "And I also re-plotted\nthis out further to see if this line eventually\ncaught up with the 44 number",
    "start": "3553650",
    "end": "3561440"
  },
  {
    "text": "up there, and it does at\naround 8 trillion parameters. So that's about 1,000 times\nbigger than where we are now.",
    "start": "3561440",
    "end": "3570510"
  },
  {
    "text": "So all this is to say\nthat scaling does help, but there might be easier and\ncheaper ways to get there.",
    "start": "3570510",
    "end": "3576540"
  },
  {
    "text": "So one criticism you could\nmake of the previous approach was that DPR actually\nhad access to something",
    "start": "3576540",
    "end": "3583069"
  },
  {
    "text": "that T5 didn't have, which is,\nit had human annotated gold passages saying what\nyou needed to retrieve",
    "start": "3583070",
    "end": "3588260"
  },
  {
    "text": "to answer the question. And that data is\nactually hard to collect. So we're going to ask\nthe question, what",
    "start": "3588260",
    "end": "3594230"
  },
  {
    "text": "if the examples that\nyou had access to were just query answer pairs? Can you still train\na good retriever",
    "start": "3594230",
    "end": "3603020"
  },
  {
    "text": "without gold passages? And this sort of task arises\nin many other tasks as well.",
    "start": "3603020",
    "end": "3608853"
  },
  {
    "text": "You could imagine, if you were\ngoing from natural language to code, you might\nencounter cases where nobody has provided\nyou annotations of what",
    "start": "3608853",
    "end": "3615500"
  },
  {
    "text": "code snippets to retrieve. Medical diagnosis,\nsimilar thing. So we're going to go now to end\nto end learning of a retriever,",
    "start": "3615500",
    "end": "3623270"
  },
  {
    "text": "and let me get into some\ndetail on what that is. So we're coming back to this\ndiagram of a memory retriever,",
    "start": "3623270",
    "end": "3629869"
  },
  {
    "text": "and in a memory augmented model,\nonce the memory is retrieved, it then goes into a\nreader component, which",
    "start": "3629870",
    "end": "3635480"
  },
  {
    "text": "takes the original input in the\nmemory and produces an answer. So if you have no\nsupervision for the memory,",
    "start": "3635480",
    "end": "3642910"
  },
  {
    "text": "you might have this intuition\ninstead, which is that, if you did retrieve\na good memory, that should result in a good\nanswer from the reader.",
    "start": "3642910",
    "end": "3649840"
  },
  {
    "text": "On the other hand, if you\nretrieved a bad memory, that will probably\ncause the reader to get confused and\nproduce a bad result.",
    "start": "3649840",
    "end": "3656420"
  },
  {
    "text": "So you might be able to use\nthat observation as a training signal to train your retriever.",
    "start": "3656420",
    "end": "3662110"
  },
  {
    "text": "Let me just give a\nconcrete example with this. Who is the bad guy\nin Lord of the Rings? If the memory retrieves\nsomething like,",
    "start": "3662110",
    "end": "3668110"
  },
  {
    "text": "the main antagonist\nis Sauron, then you'll produce Sauron likely. And that's great. On the other hand,\nif the retriever",
    "start": "3668110",
    "end": "3674410"
  },
  {
    "text": "got this other passage\nsaying, Lord of the Rings received a bad review\nfrom IMDb, then",
    "start": "3674410",
    "end": "3680380"
  },
  {
    "text": "your reader might be more\ninclined to produce IMDb, which would not match the\ngold answer in your training",
    "start": "3680380",
    "end": "3686300"
  },
  {
    "text": "data set. So this gives you some knowledge\nthat the second memory is bad, and the first one is good.",
    "start": "3686300",
    "end": "3691810"
  },
  {
    "text": "And so, what I'm\ngoing to propose here is this idea of trial and error. In the first stage, you\nperform exploration,",
    "start": "3691810",
    "end": "3698619"
  },
  {
    "text": "where you let your imperfect\nretriever select some memory, and you try feeding that\nmemory to the reader,",
    "start": "3698620",
    "end": "3704170"
  },
  {
    "text": "and then you learn from\nsuccess or failure. So if the memory helps\nthe reader generate the right answer,\nyou want to increase",
    "start": "3704170",
    "end": "3709893"
  },
  {
    "text": "the score of that memory. And if the memory does\nnot help the retriever, you want to decrease the\nscore of that memory.",
    "start": "3709893",
    "end": "3717820"
  },
  {
    "text": "And over time,\nthis process would help the helpful memories\nget higher scores than the less helpful ones.",
    "start": "3717820",
    "end": "3724180"
  },
  {
    "text": "So the formal\napproach for this is going to be taken from a\npaper by one of my colleagues",
    "start": "3724180",
    "end": "3730930"
  },
  {
    "text": "called, \"Open Retrieval QA,\nORQA,\" and the exploration component we're going\nto formalize as follows.",
    "start": "3730930",
    "end": "3738050"
  },
  {
    "text": "So as I mentioned\nearlier, a retriever is just scoring function\nbetween an input and a memory.",
    "start": "3738050",
    "end": "3743150"
  },
  {
    "text": "And if you take a\nsoftmax over all of the scores for\nall of the memories,",
    "start": "3743150",
    "end": "3748540"
  },
  {
    "text": "then you get this distribution\nover memories given the input. So again, I've just\nraised all of the scores",
    "start": "3748540",
    "end": "3754390"
  },
  {
    "text": "to e to the power of the\nscores, and then normalize.",
    "start": "3754390",
    "end": "3759532"
  },
  {
    "text": "And once we have\nthis distribution, we'll randomly sample a\nmemory from that distribution. So as you can imagine, if\nthe scores are meaningful,",
    "start": "3759532",
    "end": "3767470"
  },
  {
    "text": "then we're more likely to\nsample a memory that's good and less likely to sample\na memory that's bad. But because it's random,\nwe kind of eventually",
    "start": "3767470",
    "end": "3774580"
  },
  {
    "text": "will sample everything,\nunless there are things with 0 probability.",
    "start": "3774580",
    "end": "3779590"
  },
  {
    "text": "And then the learning from\nsuccess and failure part. So once we pick\na memory, we need",
    "start": "3779590",
    "end": "3786250"
  },
  {
    "text": "to see if it actually helps. And we're going to measure\nthat by looking at the reader's",
    "start": "3786250",
    "end": "3792160"
  },
  {
    "text": "probability of generating\nthe right answer, given that particular memory. So that's this big\nquantity right here.",
    "start": "3792160",
    "end": "3798520"
  },
  {
    "text": "The reader looks at the\ninput and the memory, and we want to see\nits probability of generating the gold answer.",
    "start": "3798520",
    "end": "3805390"
  },
  {
    "text": "And if this value\nis high, then we want to increase the\nscore of that memory. And if it's low, we want\nto probably decrease",
    "start": "3805390",
    "end": "3811900"
  },
  {
    "text": "the score of the memory. So I've shown you a\ncouple expressions now,",
    "start": "3811900",
    "end": "3818740"
  },
  {
    "text": "and we want to put those\nexpressions together into a training objective\nthat we can actually optimize.",
    "start": "3818740",
    "end": "3824470"
  },
  {
    "text": "So we'll start\nwith this question. If we randomly sample\nmemory from the retriever, and then we generate\nan answer, what",
    "start": "3824470",
    "end": "3830710"
  },
  {
    "text": "is the overall probability\nthat we get the answer right? So first, let's look at\nthis expression right here.",
    "start": "3830710",
    "end": "3836650"
  },
  {
    "text": "This is a summation over\nall possible memories that the retriever\ncould retrieve, and the sum is over the\nprobability of retrieving it.",
    "start": "3836650",
    "end": "3843650"
  },
  {
    "text": "So right now, this just equals\n1 because it's a distribution, and we're summing over\nall of its values.",
    "start": "3843650",
    "end": "3850150"
  },
  {
    "text": "But then, we'll\nadd one more term to this, which is\nthe probability that the reader gets the\nanswer right given the memory",
    "start": "3850150",
    "end": "3857170"
  },
  {
    "text": "in that term in the summation. So this first part\nis the retriever, and it's proposing\ndifferent memories,",
    "start": "3857170",
    "end": "3864160"
  },
  {
    "text": "and this second\npart is the reader, and it's succeeding or\nfailing based on the memories.",
    "start": "3864160",
    "end": "3869710"
  },
  {
    "text": "So you can think of each\nterm in this summation as a trial of a\ndifferent memory, and you can think of that second\nterm kind of like a reward.",
    "start": "3869710",
    "end": "3877599"
  },
  {
    "text": "If it's high, it's good,\nand if it's low, it's bad. So what they propose\nin the ORQA paper",
    "start": "3877600",
    "end": "3884710"
  },
  {
    "text": "is to perform gradient descent\non this entire expression right here. They basically want to push the\nvalue of this entire expression",
    "start": "3884710",
    "end": "3892060"
  },
  {
    "text": "up, and they're optimizing both\nthe retriever and the reader. So let's look at\nthe retriever first.",
    "start": "3892060",
    "end": "3899710"
  },
  {
    "text": "So the retriever\nhas a fixed budget that has to sum up to 1\nover all of the memories. If it wants this\nvalue to be high,",
    "start": "3899710",
    "end": "3906250"
  },
  {
    "text": "it doesn't have any incentive to\nput probability on bad memories because those bad\nmemories are just",
    "start": "3906250",
    "end": "3911260"
  },
  {
    "text": "going to produce a low score\non this term right here. So as you optimize\nthis function,",
    "start": "3911260",
    "end": "3916708"
  },
  {
    "text": "the retriever will basically\ntry to put all of its mass on the good memories. And meanwhile, if\nyou're optimizing the reader with respect\nto this function,",
    "start": "3916708",
    "end": "3923920"
  },
  {
    "text": "it's trying its best to\nproduce the gold answer given whatever memory it has. So it's also incentivized to try\nits best to extract the answer",
    "start": "3923920",
    "end": "3931660"
  },
  {
    "text": "out of whatever it's given. And when you kind of jointly\noptimize both, over time, you get something that puts\nits mass on good memories.",
    "start": "3931660",
    "end": "3939089"
  },
  {
    "text": "So that kind of corresponds with\nthe intuition that I was giving you earlier, that you\ncan kind of perform",
    "start": "3939090",
    "end": "3944970"
  },
  {
    "text": "end to end learning\nto get a retriever. All right, so that's the\nhigh level approach to ORQA.",
    "start": "3944970",
    "end": "3952950"
  },
  {
    "text": "What I didn't explain\nis that, usually, if your memories are\nlike all of Wikipedia-- this summation is very large--",
    "start": "3952950",
    "end": "3958410"
  },
  {
    "text": "and if you're going\nto do gradient descent on the summation, it's going\nto take a very long time. So in practice, they\napproximate the summation",
    "start": "3958410",
    "end": "3965069"
  },
  {
    "text": "with the highest\nprobability memories. Maybe the top 100 or the top 10.",
    "start": "3965070",
    "end": "3970290"
  },
  {
    "text": "And I won't go into details\nin this class about exactly how that works, but\nI'll stop there.",
    "start": "3970290",
    "end": "3977119"
  },
  {
    "text": "Because we're kind of\napproaching the end, I'm going to take questions\njust a little bit later.",
    "start": "3977120",
    "end": "3982470"
  },
  {
    "text": "Sorry about that. So OK, let's see\nhow well ORQA works. Just come out and put\nthat number there.",
    "start": "3982470",
    "end": "3987750"
  },
  {
    "text": "So a bit of context around this. It's not as good as DPR\nbecause it has less supervision than DPR.",
    "start": "3987750",
    "end": "3992850"
  },
  {
    "text": "There's no human annotation\nof what passage to retrieve. But what's worth noticing\nis that, at least compared",
    "start": "3992850",
    "end": "3998460"
  },
  {
    "text": "to T5 at the same size-- so you can compare 0.66 billion\nparameters against 0.77--",
    "start": "3998460",
    "end": "4004730"
  },
  {
    "text": "it's actually already\nbetter than T5. And compared to a T5 that's\nabout 15 times larger,",
    "start": "4004730",
    "end": "4010280"
  },
  {
    "text": "it's almost at the\nsame performance too, so it's a\npretty decent result for an approach that has\nno access to retrieval",
    "start": "4010280",
    "end": "4018230"
  },
  {
    "text": "supervision.  So one thing you\nmight note, though,",
    "start": "4018230",
    "end": "4023950"
  },
  {
    "text": "is that the better\nresult requires gold passages and ORQA and T5-- these approaches don't\nrequire gold passages.",
    "start": "4023950",
    "end": "4030300"
  },
  {
    "text": "They only need\nquery answer pairs. And one advantage of that is\nthat query answer pair data",
    "start": "4030300",
    "end": "4035730"
  },
  {
    "text": "is actually pretty easy to\nget, so we could potentially get a lot more of it than if\nwe were asking for passages as",
    "start": "4035730",
    "end": "4042450"
  },
  {
    "text": "well. And the final part of\nthis retrieval section",
    "start": "4042450",
    "end": "4047460"
  },
  {
    "text": "is about a way to\nget, basically, an arbitrary number\nof query answer pairs to improve these weekly\nsupervised approaches that",
    "start": "4047460",
    "end": "4056190"
  },
  {
    "text": "don't have passages. So it comes from a very\nsimple observation, which is, let's take your\ntypical query answer pair.",
    "start": "4056190",
    "end": "4062160"
  },
  {
    "text": "It looks like this, right? So you've got your query on the\nleft and answer on the right. You can easily reformulate that\nas a fill in the blank question",
    "start": "4062160",
    "end": "4069360"
  },
  {
    "text": "like this. And this fill in\nthe blank question forces the model\nto think just as hard as the original question\nis, just in a different format.",
    "start": "4069360",
    "end": "4077742"
  },
  {
    "text": "But what's nice about this\nfill in the blank question is that it's very easy to\ncreate a bunch of them for free.",
    "start": "4077742",
    "end": "4083490"
  },
  {
    "text": "Basically, you can just take\nany sentence on the web, and as long as it's\nmentioning something factual or semantically\nmeaningful, you can just",
    "start": "4083490",
    "end": "4090510"
  },
  {
    "text": "blank out one of the entities. And in fact, that is\nexactly what you've probably",
    "start": "4090510",
    "end": "4095790"
  },
  {
    "text": "seen in previous lectures,\npretrained language models like BERT do. And BERT uses that\ntraining objective",
    "start": "4095790",
    "end": "4101818"
  },
  {
    "text": "to learn a very\ngreat deal, and that can be used in this setting\nfor retrieval as well.",
    "start": "4101819",
    "end": "4106930"
  },
  {
    "text": "So the basic idea for\nREALM, which is something that I worked on\nwith collaborators,",
    "start": "4106930",
    "end": "4112049"
  },
  {
    "text": "is to apply the same end\nto end training as ORQA, but pretrain the\nmodel on a bunch",
    "start": "4112050",
    "end": "4117359"
  },
  {
    "text": "of these fill in\nthe blank questions that we just\nautomatically generated, just in extremely\nlarge quantities.",
    "start": "4117359",
    "end": "4123930"
  },
  {
    "text": "And then we fine tune\nthat on the real questions query answer pairs\nthat we already have.",
    "start": "4123930",
    "end": "4130000"
  },
  {
    "text": "So if you do this\napproach, and you plot it against all of the\nothers, what's quite nice is that it basically almost\ncloses the gap completely",
    "start": "4130000",
    "end": "4137189"
  },
  {
    "text": "with an approach that\nuses supervised data just by pretraining on fill\nin the blank questions.",
    "start": "4137189",
    "end": "4143759"
  },
  {
    "text": "And the nice thing is,\nit doesn't need access to gold passages. So it's on the same footing\nas things like T5 now,",
    "start": "4143760",
    "end": "4151259"
  },
  {
    "text": "and at the same footing,\nit outperforms T5, despite being much smaller\nthan even the largest model.",
    "start": "4151260",
    "end": "4157259"
  },
  {
    "text": "So that gives us this\ninteresting promise of using kind of language model\nfill in the blank techniques",
    "start": "4157260",
    "end": "4162778"
  },
  {
    "text": "to build good memory retrievers. And the nice thing is that\nthis fill in the blank approach can be used to tackle\nmany sorts of tasks.",
    "start": "4162779",
    "end": "4171750"
  },
  {
    "text": "You could blank out\na patch in an image and train a retriever to\nfind other images that might help fill it in.",
    "start": "4171750",
    "end": "4177317"
  },
  {
    "text": "You could blank out\na segment of code and train a retriever to find\nother pieces of code that might help fill that in,\nor chapter in a textbook.",
    "start": "4177317",
    "end": "4185009"
  },
  {
    "text": "The kind of list of things you\ncan do with fill in the blank actually goes on and on. And each task that\nyou define in this way",
    "start": "4185010",
    "end": "4192270"
  },
  {
    "text": "produces a specialized\nmemory retriever for whatever it is that you're\nfilling the blank in for,",
    "start": "4192270",
    "end": "4198120"
  },
  {
    "text": "and there's no need to\ncollect training data. So this sort of\nscales to any set",
    "start": "4198120",
    "end": "4203670"
  },
  {
    "text": "of tasks that may not\nbe important enough or central enough to warrant\na big data collection budget.",
    "start": "4203670",
    "end": "4210898"
  },
  {
    "text": "All right, so the main\ntakeaways for this section are that a retriever\nis just a function that takes an input in a memory,\nand produces a score.",
    "start": "4210898",
    "end": "4219240"
  },
  {
    "text": "If you have supervised data for\nyour retriever, that's great. Provide positive and negative\nmemories for each input,",
    "start": "4219240",
    "end": "4225120"
  },
  {
    "text": "and just train the retriever to\nscore the positive ones higher. If you don't have\nsupervision, you can use end to end learning,\nwhich employs a trial and error",
    "start": "4225120",
    "end": "4233820"
  },
  {
    "text": "approach. If a memory helps the model,\nscore the memory higher. Otherwise, score it lower.",
    "start": "4233820",
    "end": "4239790"
  },
  {
    "text": "And with end to end\nlearning, you often get this special benefit\nthat you can easily create tons of data to\npretrain your retriever.",
    "start": "4239790",
    "end": "4248820"
  },
  {
    "text": "All right, we're now into\nthe very, very final part of the talk, which is how\nto actually use the memories",
    "start": "4248820",
    "end": "4254820"
  },
  {
    "text": "after you get them. Chris, I have 15 minutes, right? You have 14.",
    "start": "4254820",
    "end": "4259890"
  },
  {
    "text": "14, OK. All right, then we should\nhave plenty of time for questions, actually. So all right, here we go.",
    "start": "4259890",
    "end": "4266427"
  },
  {
    "text": "We're going to come back to this\ndiagram of a memory augmented model. And now, we're going to focus\non this reader component, which",
    "start": "4266427",
    "end": "4272670"
  },
  {
    "text": "I didn't say much about before. I said that the reader takes\nthe memory in the input, and then produces the answer. So what does that reader\ncomponent actually look like?",
    "start": "4272670",
    "end": "4281550"
  },
  {
    "text": "A very common architecture\nis just the sequence encoder to decoder model.",
    "start": "4281550",
    "end": "4287250"
  },
  {
    "text": "In practical terms, you\ntake the original question, and you just concatenate it to\nthe memory that you retrieved,",
    "start": "4287250",
    "end": "4293790"
  },
  {
    "text": "and then you feed that\ninto your encoder, and you train it using\nstandard sequence to sequence learning to produce the output.",
    "start": "4293790",
    "end": "4300160"
  },
  {
    "text": "So all we're doing is just\nconcatenating the memory with the input. And we can refer to\nthat as text fusion.",
    "start": "4300160",
    "end": "4307200"
  },
  {
    "text": "Any time you have a\nmemory that is text or can be converted into text in\nsome form, just concatenate it.",
    "start": "4307200",
    "end": "4312900"
  },
  {
    "text": "That's all you need. At least, that's what\nstate of the art techniques are doing right now.",
    "start": "4312900",
    "end": "4320190"
  },
  {
    "text": "OK, and just to give some\nvariety there, here's another way to\nincorporate memories.",
    "start": "4320190",
    "end": "4325340"
  },
  {
    "text": "Let's consider a\nslightly different memory augmented model, where instead\nof just retrieving a document,",
    "start": "4325340",
    "end": "4331810"
  },
  {
    "text": "the memory is actually\nkey-value pairs where the key is a question that's\nbeen seen before,",
    "start": "4331810",
    "end": "4338230"
  },
  {
    "text": "and the value is the answer to\nthat previously seen question. So in this case, you can\ndo something even simpler",
    "start": "4338230",
    "end": "4344260"
  },
  {
    "text": "than what was on\nthe previous slide. You can take your input\nand compare it to the keys and find the key that\nmost resembles the input.",
    "start": "4344260",
    "end": "4351590"
  },
  {
    "text": "So in this case, we\nfound a paraphrase of the original question. And if you have this, then\nall you really need to do",
    "start": "4351590",
    "end": "4359470"
  },
  {
    "text": "is just copy the answer from\nthe value as your label. And that's what people\nrefer to as label",
    "start": "4359470",
    "end": "4365469"
  },
  {
    "text": "smearing, or otherwise,\nnearest neighbors methods. We call it smearing\nbecause you're essentially",
    "start": "4365470",
    "end": "4371020"
  },
  {
    "text": "smearing the label\nfrom this example that you retrieved\nonto the new example.",
    "start": "4371020",
    "end": "4376239"
  },
  {
    "text": "And it's a very\nsimple technique. Which one you use depends\non your application.",
    "start": "4376240",
    "end": "4381920"
  },
  {
    "text": "So the techniques for using\nmemories are quite simple, but the problems that\narise when you use them are actually quite interesting.",
    "start": "4381920",
    "end": "4388969"
  },
  {
    "text": "The first one I\nwant to talk about-- I kind of mentioned this,\npreviewed this earlier-- are these two problems\nof under-utilization",
    "start": "4388970",
    "end": "4394930"
  },
  {
    "text": "and over-reliance. So let's get into the\nunderutilization issue. This is from a very recent\npaper by Longpre et al.",
    "start": "4394930",
    "end": "4403720"
  },
  {
    "text": "So let me dive into it. So OK, I switched\nthe example here because I just got tired of\nThe Lord of the Rings one.",
    "start": "4403720",
    "end": "4411287"
  },
  {
    "text": "The question is, who do you\nmeet at the gates of heaven? And the retrieve\nmemory is on point.",
    "start": "4411287",
    "end": "4416590"
  },
  {
    "text": "It says you see Saint Peter\nat the gates of heaven. The reader reads it,\nproduces Saint Peter. Everything is great.",
    "start": "4416590",
    "end": "4422290"
  },
  {
    "text": "So what Longpre et al. observed\nis, OK, if the reader is really doing such a great job\nof reading the memories,",
    "start": "4422290",
    "end": "4429070"
  },
  {
    "text": "then if I edit the memory\nto say something else, the reader should\npick up on that and produce the\ndifferent answer.",
    "start": "4429070",
    "end": "4435590"
  },
  {
    "text": "So what they do is, they change\nSaint Peter to the United Nations guards the\ngates of heaven,",
    "start": "4435590",
    "end": "4441460"
  },
  {
    "text": "and they check if\nthe reader actually produces the United Nations. But surprisingly,\nthe reader still",
    "start": "4441460",
    "end": "4448510"
  },
  {
    "text": "says that Saint Peter\nguards the gates of heaven. This is really quite\ninteresting and pretty funny.",
    "start": "4448510",
    "end": "4454910"
  },
  {
    "text": "So what's actually\ngoing on here? Let's first look at how\nbad this problem is.",
    "start": "4454910",
    "end": "4460300"
  },
  {
    "text": "So here's a plot from the paper. The first row here is\nthe model's behavior",
    "start": "4460300",
    "end": "4466480"
  },
  {
    "text": "on the training set\nfor natural questions, the same data set we\nwere looking at earlier, and the red part of this\nplot indicates the number",
    "start": "4466480",
    "end": "4474010"
  },
  {
    "text": "of times where the model\nsticks with its old answer even after changing the memory. It just stubbornly\nrefuses to change.",
    "start": "4474010",
    "end": "4479950"
  },
  {
    "text": "The blue part is the good part\nwhere it actually switches over to predicting the United\nNations on various examples,",
    "start": "4479950",
    "end": "4485650"
  },
  {
    "text": "and this other orange part\nis very concerning too. So when you change the\nmemory to United Nations,",
    "start": "4485650",
    "end": "4491478"
  },
  {
    "text": "sometimes the model just gets\nconfused and predicts something totally different. Not Saint Peter,\nnot United Nations,",
    "start": "4491478",
    "end": "4496810"
  },
  {
    "text": "just something\ncompletely different. So from this, we can\nsee something is really kind of broken about some of\nthese memory augmented models,",
    "start": "4496810",
    "end": "4504429"
  },
  {
    "text": "and the same behavior happens\non the dev set as well. ",
    "start": "4504430",
    "end": "4510480"
  },
  {
    "text": "And just to underscore\nhow bad this is, these results are from\nthe set of examples",
    "start": "4510480",
    "end": "4516290"
  },
  {
    "text": "that the original model was\nactually getting all correct. So you just kind of cut the\nperformance of your model by more than half when\nyou edit the memories,",
    "start": "4516290",
    "end": "4524810"
  },
  {
    "text": "and that indicates the model\nis not robust to change. As we said earlier, being\nable to edit your memories is something you would really\nwant from a memory augmented",
    "start": "4524810",
    "end": "4532280"
  },
  {
    "text": "model. So let's have an analysis\nof why this happens.",
    "start": "4532280",
    "end": "4537380"
  },
  {
    "text": "Basically, when\nyou put this memory into the sequence\nencoder, the reader that reads the memory, this\nencoder and this decoder--",
    "start": "4537380",
    "end": "4547130"
  },
  {
    "text": "they actually have their\nown memory as well. As we saw in the earlier\nslides, transformers have their own memory.",
    "start": "4547130",
    "end": "4552660"
  },
  {
    "text": "So we'll refer to that\nas the parametric memory of the encoder, decoder, as\nopposed to the external memory",
    "start": "4552660",
    "end": "4558650"
  },
  {
    "text": "that we want it to rely on. And at training time,\nthey essentially learn to store the answer\nin their parametric memory",
    "start": "4558650",
    "end": "4565490"
  },
  {
    "text": "and not rely on the\nexternal memory. So to give this issue\na better cartoon form,",
    "start": "4565490",
    "end": "4573110"
  },
  {
    "text": "the input is coming\nin, and the model has its own parametric memory\nand the retrieved memory, and the parametric memory\nis saying Saint Peter,",
    "start": "4573110",
    "end": "4581060"
  },
  {
    "text": "and the retrieved\nmemory, at training time, is also saying Saint Peter. And the loss function is saying,\nyou must predict Saint Peter.",
    "start": "4581060",
    "end": "4588500"
  },
  {
    "text": "So the model says, OK, I've\ngot two sources of information. I can choose either one. There's nothing\nforcing the model",
    "start": "4588500",
    "end": "4593870"
  },
  {
    "text": "to use the retrieved memory. And that's part of the\nproblem that's causing this.",
    "start": "4593870",
    "end": "4599240"
  },
  {
    "text": "Another problem, which\nisn't on this slide, is that sometimes the retriever\nis just not very good,",
    "start": "4599240",
    "end": "4604250"
  },
  {
    "text": "so it might retrieve\nsomething that's just not related to the question. And in that case,\nthe model is forced",
    "start": "4604250",
    "end": "4609260"
  },
  {
    "text": "to fall back on its\nparametric memory, and again, learn to distrust\nthe retrieved memory.",
    "start": "4609260",
    "end": "4614990"
  },
  {
    "text": "So we want a way\nto kind of force the model to pick the\nretrieved memory instead.",
    "start": "4614990",
    "end": "4622880"
  },
  {
    "text": "Ideally, we would\nwant cases where the parametric memory is\nwrong, and the retrieved memory is correct. That would force the\nmodel to say, hey,",
    "start": "4622880",
    "end": "4629160"
  },
  {
    "text": "I can't trust my\nparametric memory. So what Longpre et\nal do is, first, they",
    "start": "4629160",
    "end": "4634580"
  },
  {
    "text": "take the retrieved memory,\nand they change what the retrieved memory is saying. So this creates a\ndisagreement now",
    "start": "4634580",
    "end": "4640730"
  },
  {
    "text": "between the parametric memory\nand the retrieve memory. But the gold label is\nstill saying Saint Peter,",
    "start": "4640730",
    "end": "4646310"
  },
  {
    "text": "so we've just made\nthe matters worse now, because now the retrieved\nmemory is even less trustworthy.",
    "start": "4646310",
    "end": "4651508"
  },
  {
    "text": "The final thing they do, which\nis really the interesting bit, is they just decide to\nchange the gold label",
    "start": "4651508",
    "end": "4658340"
  },
  {
    "text": "as well to agree with\ntheir retrieved memory. So they've changed\nreality and said, no. Actually, the United Nations\nguards the gates of heaven,",
    "start": "4658340",
    "end": "4666170"
  },
  {
    "text": "and what's in your\nparametric memory is wrong. And that's basically\nthe approach. So they create a bunch\nof data like this,",
    "start": "4666170",
    "end": "4672530"
  },
  {
    "text": "where the gold answer\nhas been changed to match the corruption they\nmade in the retrieved memory, and it guides the\nmodel away from using",
    "start": "4672530",
    "end": "4679070"
  },
  {
    "text": "the parametric memory. I thought that was\na pretty cool trick, and they don't give\nthis name in the paper,",
    "start": "4679070",
    "end": "4686930"
  },
  {
    "text": "but you can think of it\nas data augmentation using these counterfactual memories. And it can really be applied to\na lot of different approaches.",
    "start": "4686930",
    "end": "4693320"
  },
  {
    "text": "As long as your memory is\neditable in a certain way, and you can edit the\ngold label as well, you can create this\nartificial correlation",
    "start": "4693320",
    "end": "4700190"
  },
  {
    "text": "between the memory\nand the output and an artificial\nanti-correlation between the output and\nwhatever your model originally",
    "start": "4700190",
    "end": "4707480"
  },
  {
    "text": "trained on. It's cool. So now, we want to\nsee if it works. And in the paper, they report\nthis metric, which is basically",
    "start": "4707480",
    "end": "4715340"
  },
  {
    "text": "the percentage of time the model\npredicts the old value instead of the new one, divided\nby the old plus the new.",
    "start": "4715340",
    "end": "4722929"
  },
  {
    "text": "They ignore the set where\nthe model gets confused and produces something\ntotally different. I wish they had\nreported that too,",
    "start": "4722930",
    "end": "4728167"
  },
  {
    "text": "but I couldn't immediately\nfind it in their paper. But at least on this\nmetric, things look great.",
    "start": "4728167",
    "end": "4733400"
  },
  {
    "text": "So on the training\nset and the dev set, the percentage of the time that\nthe model uses the old answer",
    "start": "4733400",
    "end": "4740000"
  },
  {
    "text": "goes dramatically down with\nthis data augmentation. So it really keeps\nthe model on its toes and makes it use its memory.",
    "start": "4740000",
    "end": "4746210"
  },
  {
    "text": "Now, I take this result\nwith a little grain of salt, because their test set is\ncreated the same way that they",
    "start": "4746210",
    "end": "4752900"
  },
  {
    "text": "produce this data augmentation. So this is kind of the ideal\nsetup, where they're almost testing on the exact\nsame distribution",
    "start": "4752900",
    "end": "4759590"
  },
  {
    "text": "that they're training on. But still, a very\ninteresting approach. So let's see.",
    "start": "4759590",
    "end": "4765320"
  },
  {
    "text": "How long do we have left? OK, in the last few\nminutes, I'm going to cover the\nover-reliance problem,",
    "start": "4765320",
    "end": "4770360"
  },
  {
    "text": "and there's just\none slide on this. So sometimes, the memories\nthat your model retrieves are too easy.",
    "start": "4770360",
    "end": "4776540"
  },
  {
    "text": "Here's what I mean by that. So if we go back to\nthis Eiffel Tower query,",
    "start": "4776540",
    "end": "4781610"
  },
  {
    "text": "what year was the\nEiffel Tower built? We know it's 1889, and\na good typical memory",
    "start": "4781610",
    "end": "4786862"
  },
  {
    "text": "that you might retrieve\nis something like this. It says, work on the Eiffel\nTower was completed in 1889.",
    "start": "4786862",
    "end": "4793159"
  },
  {
    "text": "There's not too\nmuch word overlap with the original query, which\nis good because it teaches the reader to recognize\nthe fact that,",
    "start": "4793160",
    "end": "4801140"
  },
  {
    "text": "completed in this context,\nis the same as built. So the reader learns paraphrase.",
    "start": "4801140",
    "end": "4806222"
  },
  {
    "text": "On the other hand, you might\nget a memory that's too easy, which literally just says\nexactly the same tokens",
    "start": "4806223",
    "end": "4811450"
  },
  {
    "text": "as the original input. And you could also consider--",
    "start": "4811450",
    "end": "4816520"
  },
  {
    "text": "Yeah, so this example\nwould not teach the model how to paraphrase. And at the other\nextreme, you could also",
    "start": "4816520",
    "end": "4822490"
  },
  {
    "text": "consider an extremely\nchallenging memory like Paris's tallest tower\nfinished the same year Van Gogh painted the Starry Night.",
    "start": "4822490",
    "end": "4828790"
  },
  {
    "text": "So yes, that also\nsays the same fact, but an answer doesn't\neven directly appear. It's just too hard\nfor the model.",
    "start": "4828790",
    "end": "4835969"
  },
  {
    "text": "So if all of your examples\nare like this too easy memory, then you end up with a reader\nthat is kind of spoiled.",
    "start": "4835970",
    "end": "4842500"
  },
  {
    "text": "It never learns to\nparaphrase, and at test time, if your memories that are\nretrieved are not as good,",
    "start": "4842500",
    "end": "4848800"
  },
  {
    "text": "the reader is not going\nto be able to use them. So a simple fix\nto this problem-- I don't have a paper that I\ncan cite exactly for this,",
    "start": "4848800",
    "end": "4856300"
  },
  {
    "text": "but at training\ntime, you can simply filter out some of\nthe memories that have lexical overlap\nthat's too high.",
    "start": "4856300",
    "end": "4863630"
  },
  {
    "text": "At the same time, you\nalso want to make sure that you don't filter out\nso many of the easy things that you're just left\nwith the super hard cases",
    "start": "4863630",
    "end": "4870579"
  },
  {
    "text": "like the one on the bottom. Because if you only have\nthe super hard cases, your model will get confused. And as we saw in\nthe previous slides,",
    "start": "4870580",
    "end": "4877420"
  },
  {
    "text": "it might just fall back\non its parametric memory. So this is sort of just an\narea for open research of how",
    "start": "4877420",
    "end": "4883090"
  },
  {
    "text": "to give the reader a flexible\nset of things to train from.",
    "start": "4883090",
    "end": "4888679"
  },
  {
    "text": "Great. Yeah, so I've covered pretty\nmuch everything in that section as well. The main takeaways I'd\nlike for you guys to have",
    "start": "4888680",
    "end": "4894970"
  },
  {
    "text": "is that getting your model\nto use memories is not hard. There's some simple approaches,\nbut getting your model",
    "start": "4894970",
    "end": "4900489"
  },
  {
    "text": "to use memory correctly is\nactually an interesting open question, and there's this\nissue of under-utilization",
    "start": "4900490",
    "end": "4906640"
  },
  {
    "text": "and overreliance that are\nopen areas of research. And that's it. I hope that you guys saw some\ninteresting things about memory",
    "start": "4906640",
    "end": "4913942"
  },
  {
    "text": "augmented models\nand are encouraged to look into that area. If there are any\nquestions, please feel free to email\nme or message me.",
    "start": "4913942",
    "end": "4920900"
  },
  {
    "text": "Happy to talk about it more, and\nthanks for sitting through a 90 minute lecture. ",
    "start": "4920900",
    "end": "4932000"
  }
]