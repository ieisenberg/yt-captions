[
  {
    "start": "0",
    "end": "60000"
  },
  {
    "text": "Okay. Let's get started. So uh, today, we'll be covering Bayesian Meta-learning algorithms.",
    "start": "4940",
    "end": "11400"
  },
  {
    "text": "Uh, and just first there is some logistics [NOISE]. Homework 2 came out last Wednesday and is due next week on Wednesday.",
    "start": "11400",
    "end": "18085"
  },
  {
    "text": "Um, your project proposal is due two weeks from today. And so if you don't have a clear idea for who your group is going to be,",
    "start": "18085",
    "end": "24670"
  },
  {
    "text": "if you're gonna work alone, what your project is gonna be, then we encourage you to come to office hours and discuss that.",
    "start": "24670",
    "end": "30010"
  },
  {
    "text": "Uh, or post on Piazza or e-mail one of us. [NOISE] Um, also, we determined a,",
    "start": "30010",
    "end": "36090"
  },
  {
    "text": "uh- we've kind of set in stone a date and time and location for the poster presentation. It will be on Tuesday,",
    "start": "36090",
    "end": "42730"
  },
  {
    "text": "December 3rd at 1:30 PM, uh, and we expect, uh, for all on-campus students to have",
    "start": "42730",
    "end": "47930"
  },
  {
    "text": "at least one group member present at the poster session for at least part of it. Uh, so that you can kind of present your work to",
    "start": "47930",
    "end": "53600"
  },
  {
    "text": "the broader AI and machine learning community at Stanford. Okay. Um, so let's get started with the lecture.",
    "start": "53600",
    "end": "60675"
  },
  {
    "start": "60000",
    "end": "112000"
  },
  {
    "text": "Uh, and I'd first like to start with, um, a couple of disclaimers. The first is that Bayesian Meta-learning is a very active area of",
    "start": "60675",
    "end": "66290"
  },
  {
    "text": "research and something that's even more active than, uh, other parts of this course.",
    "start": "66290",
    "end": "71630"
  },
  {
    "text": "And so, uh, there may be more questions than answers in terms of the particular algorithms that we have.",
    "start": "71630",
    "end": "77285"
  },
  {
    "text": "Uh, but at the same time, this makes it a pretty exciting topic because there's- I think there's a lot of room for better algorithms and better ways to be thinking about,",
    "start": "77285",
    "end": "85040"
  },
  {
    "text": "uh, how to evaluate these algorithms. [NOISE] Uh, and then [NOISE] the second disclaimer is that this lecture probably",
    "start": "85040",
    "end": "91820"
  },
  {
    "text": "covers some of the most advanced topics of this course, uh, and so I'll try to go slowly through, through some of the technical content.",
    "start": "91820",
    "end": "97275"
  },
  {
    "text": "And, uh, I guess to both of these please ask [NOISE] questions, uh, if things aren't, uh,",
    "start": "97275",
    "end": "102615"
  },
  {
    "text": "aren't clear, uh, and I'll try to also go through the relevant, uh, background material with regard to Bayesian neural networks, for example.",
    "start": "102615",
    "end": "110545"
  },
  {
    "text": "Okay. Um, so let's get started by recapping, uh, the things that we talked about last time.",
    "start": "110545",
    "end": "117170"
  },
  {
    "start": "112000",
    "end": "125000"
  },
  {
    "text": "So we had covered different, uh, meta-learning approaches and also a couple of unified ways to thinking about these different meta-learning approaches.",
    "start": "117170",
    "end": "124970"
  },
  {
    "text": "So the, the first perspective was this computation graph perspective where we looked at all of these different meta-learning algorithms as,",
    "start": "124970",
    "end": "132165"
  },
  {
    "start": "125000",
    "end": "156000"
  },
  {
    "text": "uh, as different computation graphs. Uh, some that are completely black-box, some that have gradient operators or optimization procedures embedded inside of them,",
    "start": "132165",
    "end": "140440"
  },
  {
    "text": "and some that use non-parametric, uh, algorithms kind of embedded inside of it as well. Um, so this is kind of one way of thinking about",
    "start": "140440",
    "end": "147425"
  },
  {
    "text": "meta-learning algorithms as a function that takes in the dataset and, and test input and makes a prediction about a new input- about that input.",
    "start": "147425",
    "end": "155420"
  },
  {
    "text": "[NOISE] Uh, and then second, we also talked about an algorithmic properties perspective [NOISE] of,",
    "start": "155420",
    "end": "160940"
  },
  {
    "start": "156000",
    "end": "257000"
  },
  {
    "text": "of these methods which is that, uh, what sort of properties do these algorithms have and,",
    "start": "160940",
    "end": "166745"
  },
  {
    "text": "uh, what are kind of the trade-offs between these different properties? So we've talked about, uh, expressive power, uh,",
    "start": "166745",
    "end": "171930"
  },
  {
    "text": "basically the ability for these functions to represent a wide range of learning procedures or a wide range of functions of the dataset that's being passed as input.",
    "start": "171930",
    "end": "180095"
  },
  {
    "text": "We also talked about consistency which is that given enough data, these algorithms will produce an answer that you're satisfied with.",
    "start": "180095",
    "end": "186280"
  },
  {
    "text": "Uh, and these important- these- probably both of these properties are very important. Uh, and the third property that we haven't really talked about much",
    "start": "186280",
    "end": "194090"
  },
  {
    "text": "so far is this concept of uncertainty awareness. Uh, and that's what we'll be covering today.",
    "start": "194090",
    "end": "199960"
  },
  {
    "text": "Well, uncertainty awareness is the ability to reason about ambiguity during the learning process.",
    "start": "199960",
    "end": "205385"
  },
  {
    "text": "The ability to reason about whether or not, uh, whether or not, the kind of it's very clear what the function is or whether or not you should have uncertainty about the underlying function,",
    "start": "205385",
    "end": "213010"
  },
  {
    "text": "um, that your data is providing evidence for. So uh, why is this important?",
    "start": "213010",
    "end": "219950"
  },
  {
    "text": "Uh, we'll motivate this a bit more in more detail but at a high level, the reason why we care about this is that it allows us to think about, uh,",
    "start": "219950",
    "end": "227780"
  },
  {
    "text": "active learning, about explicitly reducing our uncertainty if we have uncertainty about our function. It also lets us think about how we might explore, uh,",
    "start": "227780",
    "end": "234840"
  },
  {
    "text": "new environments in a reinforcement learning context in order to reduce our uncertainty.",
    "start": "234840",
    "end": "239885"
  },
  {
    "text": "Uh, it also thinks about if we're in safety-critical settings, we want to calibrate uncertainty estimates. That's also, uh, pretty important,",
    "start": "239885",
    "end": "246299"
  },
  {
    "text": "and it also makes us think about [NOISE] uh, from the Bayesian perspective of Meta-learning, uh, what sort of principle approaches can be derived from those graphical models.",
    "start": "246300",
    "end": "254935"
  },
  {
    "text": "Okay. So that's all we'll be covering today. Uh, at a high level, we'll talk about first, uh, go,",
    "start": "254935",
    "end": "261299"
  },
  {
    "start": "257000",
    "end": "287000"
  },
  {
    "text": "go into more detail about why, why it makes sense to use Bayesian and kind of set up their framework, uh, for which we'll be talking about.",
    "start": "261300",
    "end": "267130"
  },
  {
    "text": "Then we'll talk about different meta- different Bayesian Meta-learning approaches, uh, in the context of the different algorithms that we've discussed in this course so far.",
    "start": "267130",
    "end": "274895"
  },
  {
    "text": "Uh, and then lastly, we'll talk about how we can evaluate a Bayesian meta-learning algorithm, um, and the different types of ways",
    "start": "274895",
    "end": "281514"
  },
  {
    "text": "these meta-learning algorithms might be useful in practice. Okay, good. So that's the plan.",
    "start": "281515",
    "end": "287565"
  },
  {
    "start": "287000",
    "end": "350000"
  },
  {
    "text": "Uh, let's first talk about, uh, some motivation. So at the beginning of this course, uh,",
    "start": "287565",
    "end": "293120"
  },
  {
    "text": "we talked about some principles behind multi-task learning and meta-learning. Uh, and one of the first principles we talked about is that- is kind of a,",
    "start": "293120",
    "end": "300680"
  },
  {
    "text": "a basic principle of machine learning which is that training and testing must match. Uh, and if the conditions that uh,",
    "start": "300680",
    "end": "307205"
  },
  {
    "text": "that you're- if the conditions that you see at test time are not matching the conditions that you're training for,",
    "start": "307205",
    "end": "312260"
  },
  {
    "text": "then you won't necessarily expect the ability to generalize or the ability to learn a new task.",
    "start": "312260",
    "end": "317405"
  },
  {
    "text": "Uh, and then the second principle that we had talked about was that the task must share some sort of structure.",
    "start": "317405",
    "end": "322745"
  },
  {
    "text": "Uh, any- without the structure, you won't get any benefit from sharing across the tasks.",
    "start": "322745",
    "end": "328775"
  },
  {
    "text": "And this is the, the point that I'd like to go into a little bit more depth on, which is that, what is structure even mean?",
    "start": "328775",
    "end": "334790"
  },
  {
    "text": "Uh, and I think that one way that we can actually try to formally define what structure means here,",
    "start": "334790",
    "end": "341750"
  },
  {
    "text": "is that there's some statistical dependence on some shared latent information across the tasks.",
    "start": "341750",
    "end": "347870"
  },
  {
    "text": "[NOISE] Uh, and in particular what we can do, uh, is we can bring up this graphical model, uh, which I have showed in,",
    "start": "347870",
    "end": "354509"
  },
  {
    "start": "350000",
    "end": "655000"
  },
  {
    "text": "uh, a few lectures ago, uh, where Theta is the kind of the meta parameters that are shared across all tasks.",
    "start": "354510",
    "end": "361445"
  },
  {
    "text": "Phi i is the task specific parameters for each task i, and then we also have a number of data points, uh,",
    "start": "361445",
    "end": "368419"
  },
  {
    "text": "index Phi j for each of the i tasks. Um, and in particular in this graphical model,",
    "start": "368420",
    "end": "375340"
  },
  {
    "text": "you can see that there is some dependence on Theta. Basically, there's- uh, each of the, uh,",
    "start": "375340",
    "end": "380425"
  },
  {
    "text": "each of the the parameters Phi i have an arrow coming from Theta. [NOISE] And in particular if you condition on the information in Theta,",
    "start": "380425",
    "end": "389635"
  },
  {
    "text": "we know, uh, first that the task parameters become independent.",
    "start": "389635",
    "end": "395200"
  },
  {
    "text": "Uh, that is Phi, uh, Phi i is independent of another Phi i conditioned on Theta. [NOISE] And if you do condition on Theta,",
    "start": "395200",
    "end": "404565"
  },
  {
    "text": "on the latent information, then they're not otherwise independent.",
    "start": "404565",
    "end": "411130"
  },
  {
    "text": "And in particular, if you then look at these two properties, which is that they're not independent without any conditioning information and",
    "start": "411500",
    "end": "417680"
  },
  {
    "text": "they're- and that they're independent once you condition, um, then you can say that you- the distribution over Phi given Theta is gonna",
    "start": "417680",
    "end": "424910"
  },
  {
    "text": "have lower entropy than the distribution of the marginal of Phi. [NOISE] Essentially, Theta tells you information",
    "start": "424910",
    "end": "432169"
  },
  {
    "text": "about your task or specific, task-specific parameters phi. Okay. This all makes sense?",
    "start": "432170",
    "end": "440110"
  },
  {
    "text": "So now, I'd like to give you a thought exercise,",
    "start": "440110",
    "end": "446039"
  },
  {
    "text": "uh, which is that if you could identify Theta, uh, for example, through Meta-learning, if you can identify what's shared across the tasks,",
    "start": "446040",
    "end": "452180"
  },
  {
    "text": "in which situations should it be faster to learn Phi in",
    "start": "452180",
    "end": "457820"
  },
  {
    "text": "compared to learning from scratch? Are there any thoughts on that?",
    "start": "457820",
    "end": "463820"
  },
  {
    "text": "[NOISE]",
    "start": "463820",
    "end": "470220"
  },
  {
    "text": "Or maybe I can rephrase the question. In which sit- situations would it not be faster than learning from scratch? Yeah.",
    "start": "470220",
    "end": "477200"
  },
  {
    "text": "The first entropy is much faster than the second one. Should it be faster? Yeah, that's a good point. So yeah, exactly.",
    "start": "477200",
    "end": "483210"
  },
  {
    "text": "So if the, if the first entropy is much, much smaller then- that theta basically tells you a lot about the task,",
    "start": "483210",
    "end": "489400"
  },
  {
    "text": "and it should be much faster to learn, uh, than just learning from scratch because learning from scratch a little bit just be looking at Phi. Was there another?",
    "start": "489400",
    "end": "498189"
  },
  {
    "text": "Then without giving the Theta then we should take just as long to go to any of your [inaudible] Theta",
    "start": "502020",
    "end": "508780"
  },
  {
    "text": "Can you repeat that? Yeah. So basically if knowing Theta keeps",
    "start": "510800",
    "end": "516130"
  },
  {
    "text": "them independent then they should take this long to learn in the Theta. Yeah, exactly. So that's also true.",
    "start": "516130",
    "end": "522070"
  },
  {
    "text": "So basically if they are, um, if they're independent before knowing conditioning on Theta,",
    "start": "522070",
    "end": "528895"
  },
  {
    "text": "then basically these entropies will be the same, and then learning from traps will be just as, as, uh, just as fast as learning from the shared information Theta.",
    "start": "528895",
    "end": "538375"
  },
  {
    "text": "Any other thoughts on this?",
    "start": "538375",
    "end": "541220"
  },
  {
    "text": "So one additional point that's I guess worth mentioning is that if, uh, if a single datapoint,",
    "start": "543740",
    "end": "551350"
  },
  {
    "text": "uh, carries all the information of Theta. Like maybe, uh, your tasks correspond to all possible image classification tasks sampled at random,",
    "start": "551350",
    "end": "559240"
  },
  {
    "text": "uh, and the fact that you see a single data point like an image, it being an image, that doesn't really tell you that much.",
    "start": "559240",
    "end": "564939"
  },
  {
    "text": "Uh, or the image kind of- the, the basically the, the shared information doesn't tell you that much because that information is",
    "start": "564940",
    "end": "571990"
  },
  {
    "text": "also exis- exists in a single data point which is that is an image classification task. Uh, and in those situations,",
    "start": "571990",
    "end": "577644"
  },
  {
    "text": "uh, first the, uh, the entropy conditioned on Theta is going to be,",
    "start": "577644",
    "end": "584680"
  },
  {
    "text": "uh, fairly high as well as the marginal entropy. And the, um, and also th- kind of thinking about how this compares to the,",
    "start": "584680",
    "end": "593755"
  },
  {
    "text": "the entropy conditioned on one data point versus a large number of data points. Okay. One other thought exercise; uh,",
    "start": "593755",
    "end": "601629"
  },
  {
    "text": "what if the entropy of Phi given Theta is 0?",
    "start": "601630",
    "end": "606410"
  },
  {
    "text": "What would happen in that case or, or what, what situation does that describe?",
    "start": "607380",
    "end": "613495"
  },
  {
    "text": "We don't even need to learn Phi. Yeah, so we don't even need to learn Phi.",
    "start": "613495",
    "end": "619315"
  },
  {
    "text": "If basically they're the same, then- or if, if, if this- if, um, there isn't any additional information in Phi that isn't captured in Theta,",
    "start": "619315",
    "end": "627894"
  },
  {
    "text": "then Theta can solve all of the tasks, and you don't need to do anything to learn from Theta.",
    "start": "627895",
    "end": "635600"
  },
  {
    "text": "All right. So these are, these are kind of interesting properties to think about. Uh, for example, if you're in this last,",
    "start": "636030",
    "end": "641590"
  },
  {
    "text": "uh, in this last setting, if you can basically learn a single set of parameters that captures all their tasks and you don't",
    "start": "641590",
    "end": "647139"
  },
  {
    "text": "need to necessarily do any learning in order to adapt to new tasks. And you can just basically use that single parameter vector for solving the new task.",
    "start": "647140",
    "end": "655240"
  },
  {
    "start": "655000",
    "end": "875000"
  },
  {
    "text": "[NOISE] Okay. Um, so what information might Theta contain?",
    "start": "655240",
    "end": "661150"
  },
  {
    "text": "Um, we talked about this a bit earlier in the lecture, uh, or earlier in the course. Um, but one thing to kind of reiterate here is if we look at",
    "start": "661150",
    "end": "668695"
  },
  {
    "text": "kind of a couple of examples of the- that we saw earlier in the course, for example, um, this toy sinusoid problem that we saw in some of the,",
    "start": "668695",
    "end": "675940"
  },
  {
    "text": "the student presentations, uh, last week. What information would Theta contain in,",
    "start": "675940",
    "end": "681850"
  },
  {
    "text": "in this task family?",
    "start": "681850",
    "end": "683750"
  },
  {
    "text": "Any thoughts? Yeah.",
    "start": "691080",
    "end": "697750"
  },
  {
    "text": "Now the task function is the shifted, uh, sign. Yeah. So if the- basically if the s- if",
    "start": "697750",
    "end": "704230"
  },
  {
    "text": "the family of tasks corresponds to sinusoids with different, uh, amplitude and different phase,",
    "start": "704230",
    "end": "710440"
  },
  {
    "text": "then Theta will correspond to tho- that exact- that family of sinusoid functions, right?",
    "start": "710440",
    "end": "716245"
  },
  {
    "text": "And it'll basically correspond to everything but the phase and amplitude, which is the task-specific information that needs to",
    "start": "716245",
    "end": "721930"
  },
  {
    "text": "be inferred from data in this example. Um, and further, if you look at the language translation case tha- that was presented a,",
    "start": "721930",
    "end": "729084"
  },
  {
    "text": "a couple weeks ago, uh, in this machine translation example, Theta is going to correspond to all possible language pairs of data, uh,",
    "start": "729085",
    "end": "736970"
  },
  {
    "text": "and the information in Phi that isn't present in Theta is going to correspond to things that are specific to a particular language.",
    "start": "736970",
    "end": "745280"
  },
  {
    "text": "Okay. Um, and note that in both of these examples,",
    "start": "745350",
    "end": "750790"
  },
  {
    "text": "Theta is narrower than the space of all possible functions. And it's because of this, this is why we can get a benefit",
    "start": "750790",
    "end": "756190"
  },
  {
    "text": "from using things like Meta-learning in principle. Okay. Um, the last set of exercise that we also haven't really talked",
    "start": "756190",
    "end": "764640"
  },
  {
    "text": "about much in this course is what if you meta-learn without a lot of tasks? So if you have an infinite number of tasks, you can, uh,",
    "start": "764640",
    "end": "771360"
  },
  {
    "text": "you should be able to recover Theta exactly, um, or basically recover kind of that,",
    "start": "771360",
    "end": "778930"
  },
  {
    "text": "that family with, with high precision. What if you don't have a lot of tasks, what happens then? [BACKGROUND]",
    "start": "778930",
    "end": "793540"
  },
  {
    "text": "Sorry, what? The bootstrapping of the tasks. What do you mean by that?",
    "start": "793540",
    "end": "799399"
  },
  {
    "text": "I mean re-assemble the tasks can create a des- and a description of the tasks.",
    "start": "799440",
    "end": "806935"
  },
  {
    "text": "You mean make more tasks? Yeah. Yeah. So if you don't have, if you don't have a lot of tasks you should definitely- you should pause.",
    "start": "806935",
    "end": "812335"
  },
  {
    "text": "It's good to try to make more tasks. If you can't make more tasks, what happens? Yeah. [BACKGROUND]",
    "start": "812335",
    "end": "821440"
  },
  {
    "text": "So yeah. So what it will do is if you have kind of a space of tasks it won't necessarily cover the true distribution of",
    "start": "821440",
    "end": "828220"
  },
  {
    "text": "tasks but will potentially overfit, uh, or what I would call meta overfitting to that space of tasks,",
    "start": "828220",
    "end": "833590"
  },
  {
    "text": "such that actually doesn't recover, uh, Theta that corresponds to, for example, all language pairs but it co- find the Theta that corresponds to a set of",
    "start": "833590",
    "end": "841090"
  },
  {
    "text": "language pairs that looks like the things in your training data and not something that captures the full distribution. Uh, and then as a result, you won't be as effective at",
    "start": "841090",
    "end": "848589"
  },
  {
    "text": "adapting to new things from that distribution, uh, from the broader distribution, unless they're very close to the training examples.",
    "start": "848590",
    "end": "855340"
  },
  {
    "text": "So basically a form of overfitting that's lifted and not from data points but to tasks.",
    "start": "855340",
    "end": "861415"
  },
  {
    "text": "Okay. So here are a couple of examples I think of how Bayesian Meta-learning or, or this graphical model at least helps us think about these different, um,",
    "start": "861415",
    "end": "869980"
  },
  {
    "text": "properties of Meta-learning algorithms and, and different things that will happen if you train in different situations. Um, now a bit more motivation here.",
    "start": "869980",
    "end": "879280"
  },
  {
    "start": "875000",
    "end": "933000"
  },
  {
    "text": "So that was kinda motivation at the conceptual level, what about in practice? So, so far we've looked at parametric approaches a- a- and non-parametric approaches.",
    "start": "879280",
    "end": "887035"
  },
  {
    "text": "But for the parametric approaches, what we recovered was the deterministic estimate of the task-specific parameters Phi,",
    "start": "887035",
    "end": "894100"
  },
  {
    "text": "given the dataset and your meta-parameters Theta. So you'd essentially get a point estimate for this distribution right here.",
    "start": "894100",
    "end": "901075"
  },
  {
    "text": "So this- it seemed to work fine on your homework. You implemented this and, and you perfe- presumably got decent performance out of this.",
    "start": "901075",
    "end": "907420"
  },
  {
    "text": "Uh, is this a problem or, or why is this a problem? Well, there are a few situa- there's some situations where,",
    "start": "907420",
    "end": "914785"
  },
  {
    "text": "uh, we need more than just a point estimate. So, uh, for example, some few-shot learning problems may not be fully determined by their data.",
    "start": "914785",
    "end": "923515"
  },
  {
    "text": "And in particular because you're in the few-shot learning regime, it may be that the underlying function is",
    "start": "923515",
    "end": "928570"
  },
  {
    "text": "ambiguous given the evidence that you have and your prior information. So as an example of this,",
    "start": "928570",
    "end": "935110"
  },
  {
    "start": "933000",
    "end": "1145000"
  },
  {
    "text": "say you have, uh, 10 examples, uh, five positives and five negatives and your goal is to classify between, uh,",
    "start": "935110",
    "end": "942820"
  },
  {
    "text": "between these two- two binary classification problem between these, these, uh, two classes.",
    "start": "942820",
    "end": "948834"
  },
  {
    "text": "And in particular, all the images on the left are people that are smiling, people that are wearing a hat,",
    "start": "948835",
    "end": "953875"
  },
  {
    "text": "and people that are young. And all the people on the right are people that are not smiling, not wearing a hat, and not young.",
    "start": "953875",
    "end": "960115"
  },
  {
    "text": "And then if you get a new test image of someone who is smiling and wearing a hat and not young,",
    "start": "960115",
    "end": "966310"
  },
  {
    "text": "the underlying function or the underlying label for this particular image isn't- i- is inherently ambiguous.",
    "start": "966310",
    "end": "972430"
  },
  {
    "text": "We don't know if we're supposed to be classifying, um, with regard to smiling, with regard to wearing a hat, with regard to being young or any pair of these attributes.",
    "start": "972430",
    "end": "980815"
  },
  {
    "text": "Um, and for ge- we can also look at another example where someone is wearing a hat and young but not smiling.",
    "start": "980815",
    "end": "986005"
  },
  {
    "text": "So inherently, the- kind of the answer to- or the correct label for these images is ambiguous because the dataset was so small.",
    "start": "986005",
    "end": "994870"
  },
  {
    "text": "Okay. Um, so to try to reconcile this problem,",
    "start": "994870",
    "end": "1001380"
  },
  {
    "text": "uh, what if we can generate hypotheses about the underlying function? So what if we can sample from this distribution?",
    "start": "1001380",
    "end": "1008085"
  },
  {
    "text": "Uh, if we, if we can sample from this distribution, then we can reason about our uncertainty, we can, um,",
    "start": "1008085",
    "end": "1013725"
  },
  {
    "text": "then kind of reason about, uh, basically whether or not we're gonna be,",
    "start": "1013725",
    "end": "1019350"
  },
  {
    "text": "be able to make an effective and accurate decision in the situation, which is really important in safety critical settings.",
    "start": "1019350",
    "end": "1024449"
  },
  {
    "text": "Um, it can allow us to explicitly reduce our uncertainty because we can get some measure of,",
    "start": "1024450",
    "end": "1029579"
  },
  {
    "text": "uh, of which examples have high uncertainty and which examples have low uncertainty. And then we can try to ask for labels from the,",
    "start": "1029580",
    "end": "1036209"
  },
  {
    "text": "uh, from some user. Uh, and there's been a number of works, uh, that have looked at this and also- so one of the presentations on Wednesday is, um,",
    "start": "1036210",
    "end": "1043860"
  },
  {
    "text": "I think believe the third paper here which will be covering, uh, specifically Meta-learning approaches for active learning.",
    "start": "1043860",
    "end": "1049895"
  },
  {
    "text": "Uh, and lastly, it will be important for learning to explore in, uh, meta reinforcement learning which we'll cover in a couple lectures.",
    "start": "1049895",
    "end": "1057684"
  },
  {
    "text": "Okay. So at the kind of bare minimum, we wanna be able to sample from this distribution.",
    "start": "1057685",
    "end": "1063100"
  },
  {
    "text": "Uh, we also may want to try to generate, um, try to be able to do other things like evaluate",
    "start": "1063100",
    "end": "1068830"
  },
  {
    "text": "the likelihood of a function or to be able to, um, measure like the entropy of this distribution, for example.",
    "start": "1068830",
    "end": "1076240"
  },
  {
    "text": "Okay. So that's kind of the motivation for Bayesian Meta-learning algorithms.",
    "start": "1076240",
    "end": "1081985"
  },
  {
    "text": "Any questions before I move onto the particular methods that people use? Yeah.",
    "start": "1081985",
    "end": "1096060"
  },
  {
    "text": "[BACKGROUND] Yeah. So for active learning, if you have an estimate of",
    "start": "1096060",
    "end": "1103365"
  },
  {
    "text": "basically your- your confidence or your uncertainty regarding a particular example, uh,",
    "start": "1103365",
    "end": "1108795"
  },
  {
    "text": "like for example, in these, uh, if you have- if you're trying to, uh, acquire a more accurate classifier in this setting and you have some examples that",
    "start": "1108795",
    "end": "1117179"
  },
  {
    "text": "look like this and some examples that look like these ones on the left, then you should- the kind of the correct thing to do is to",
    "start": "1117180",
    "end": "1123360"
  },
  {
    "text": "ask for a label for the ones that you are- are uncertain about and such that you can explicitly try to reduce",
    "start": "1123360",
    "end": "1128429"
  },
  {
    "text": "your uncertainty about the underlying function. And so these types of uncertainty estimates give you a means to reason about that.",
    "start": "1128430",
    "end": "1136210"
  },
  {
    "text": "Okay, so let's start talking about some approaches for being able to sample from this distribution or being able,",
    "start": "1138440",
    "end": "1147345"
  },
  {
    "start": "1145000",
    "end": "1310000"
  },
  {
    "text": "being- basically being able to represent our uncertainty. So, uh, the first simplest approach that we'll talk about is something that could actually,",
    "start": "1147345",
    "end": "1154830"
  },
  {
    "text": "could actually be applied to all three classes of Meta-learning algorithms. Ah, and in particular, if we care about generating a distribution, um,",
    "start": "1154830",
    "end": "1162809"
  },
  {
    "text": "over our predictions, one thing that we could do is just let our function f output the parameters of a distribution over our label.",
    "start": "1162810",
    "end": "1170475"
  },
  {
    "text": "Um, so for example, uh, you could have, uh, any of these functions f, uh,",
    "start": "1170475",
    "end": "1175665"
  },
  {
    "text": "from kind of the computational graph perspective output something that corresponds to the probability of, uh,",
    "start": "1175665",
    "end": "1181289"
  },
  {
    "text": "discrete categorical distribution if your label y is discrete, or if, uh, you can make it discrete.",
    "start": "1181290",
    "end": "1188340"
  },
  {
    "text": "I could also- you could use also output the mean and variance of the Gaussian distribution to represent, uh, a- a Gaussian distribution over your labels,",
    "start": "1188340",
    "end": "1196110"
  },
  {
    "text": "or if you have a more multi-modal distribution you could try to represent, uh, the means and variances and mixture weights of a mixture of Gaussians, um,",
    "start": "1196110",
    "end": "1204360"
  },
  {
    "text": "or if you want to do something more sophisticated, if you have a multidimensional, uh, output label y,",
    "start": "1204360",
    "end": "1210825"
  },
  {
    "text": "then you could output the parameters of a sequence of distributions, uh, sequence of conditional distributions to allow you to",
    "start": "1210825",
    "end": "1216150"
  },
  {
    "text": "represent the joint distribution over those variables. Uh, and this would be what's called an autoregressive model.",
    "start": "1216150",
    "end": "1221565"
  },
  {
    "text": "Uh, and once you output the parameters of the distribution, you can then train, uh, any of these approaches with the maximum likelihood estimate optimizing both over the-",
    "start": "1221565",
    "end": "1231120"
  },
  {
    "text": "the mean of the distribution as well as the variance or basically all- the,",
    "start": "1231120",
    "end": "1237690"
  },
  {
    "text": "the full probability values of that distribution. Okay. So this is really simple, um, and this is,",
    "start": "1237690",
    "end": "1245490"
  },
  {
    "text": "and fo- for example, if you do, um, mean squared error, uh, for example that corresponds to optimizing for the mean of a Gaussian,",
    "start": "1245490",
    "end": "1254385"
  },
  {
    "text": "uh, but not the variance where the me- variance corresponds to a, uh, the identity matrix. Um, so and then, then you actually are so- are sort of already",
    "start": "1254385",
    "end": "1260940"
  },
  {
    "text": "implicitly getting a distribution. Um, yeah. [BACKGROUND]",
    "start": "1260940",
    "end": "1270990"
  },
  {
    "text": "Yeah, exactly. So in the first case, the, uh, cr- the cross entropy loss corresponds to maximizing the likelihood of,",
    "start": "1270990",
    "end": "1277665"
  },
  {
    "text": "uh, doing maximum likelihood with a categorical distribution. So in classification examples that, uh, we're already doing it.",
    "start": "1277665",
    "end": "1284085"
  },
  {
    "text": "Um, in principle, this should give you reasonable, uh, uncertainty estimates.",
    "start": "1284085",
    "end": "1291299"
  },
  {
    "text": "In practice, uh, I guess this is, I'll get to the pros and cons of the second but in practice, uh, doing that doesn't necessarily give you good,",
    "start": "1291300",
    "end": "1297900"
  },
  {
    "text": "um, good calibrate uncertainty estimates. And the, um, and neural networks,",
    "start": "1297900",
    "end": "1304290"
  },
  {
    "text": "for example, in general tend to be a bit overconfident in their predictions. Okay, so this is a very simple approach.",
    "start": "1304290",
    "end": "1311865"
  },
  {
    "start": "1310000",
    "end": "1595000"
  },
  {
    "text": "Uh, the benefit is that it's simple, uh, and you can combine it with a variety of methods. Uh, downside is, um,",
    "start": "1311865",
    "end": "1318735"
  },
  {
    "text": "first is that it doesn't nec- always lead to accurate uncertainty estimates, which I actually didn't write down on the slide but that's certainly,",
    "start": "1318735",
    "end": "1324555"
  },
  {
    "text": "uh, one of the downsides. Um, another downside is you can't reason about uncertainty about the underlying function.",
    "start": "1324555",
    "end": "1330615"
  },
  {
    "text": "Just the uncertainty about the individual data points. Uh, and so for example,",
    "start": "1330615",
    "end": "1335955"
  },
  {
    "text": "uh, you- you can't, uh, disentangle model uncertainty versus label noise, for example.",
    "start": "1335955",
    "end": "1342570"
  },
  {
    "text": "Um, and is also limited to a particular class of distributions over y, uh,",
    "start": "1342570",
    "end": "1348899"
  },
  {
    "text": "and in some of the following slides we'll look at more expressive classes of distributions, um, where we can represent more,",
    "start": "1348900",
    "end": "1355455"
  },
  {
    "text": "basically distributions over the functions fairly expressively. Okay. So this is version 0.",
    "start": "1355455",
    "end": "1363705"
  },
  {
    "text": "Um, now the first downside here is that we can't reason about the uncertainty about the underlying function.",
    "start": "1363705",
    "end": "1371520"
  },
  {
    "text": "So one very natural thing to think is well, can we just use maximum, the same maximum likelihood training but over the model parameters Phi?",
    "start": "1371520",
    "end": "1380110"
  },
  {
    "text": "So does anyone wanna comment on why we could,",
    "start": "1380110",
    "end": "1385635"
  },
  {
    "text": "wha- how we could do this or how we couldn't do this? Yeah.",
    "start": "1385635",
    "end": "1393720"
  },
  {
    "text": "[BACKGROUND]",
    "start": "1393720",
    "end": "1399809"
  },
  {
    "text": "Yeah, so we could have our, our neural network output, er, like a mean and variance for each of the parameters in phi,",
    "start": "1399810",
    "end": "1405645"
  },
  {
    "text": "and how do we, if we did that, say we did that, how would we go about training the- the, um, training our model to output that distribution?",
    "start": "1405645",
    "end": "1414400"
  },
  {
    "text": "Go ahead. Yeah? It's while you're training, I figured there are certain critical performance you can start to track",
    "start": "1431200",
    "end": "1438900"
  },
  {
    "text": "the mean and variance of the variables after you get past a certain threshold of performance which would see like average mini-batch what their",
    "start": "1438900",
    "end": "1447180"
  },
  {
    "text": "mean and variance would be for optimizing for each particular sampling of a mini-batch?",
    "start": "1447180",
    "end": "1452310"
  },
  {
    "text": "I see. So you're basically saying that we could take our mean and the mean variance of our function to optimize- like to judge how well that is, and then-",
    "start": "1452310",
    "end": "1461294"
  },
  {
    "text": "Yeah. Basically, I assume that after you've trained over a certain number of [inaudible] box or something, you are kind of getting into a stable equilibrium for that.",
    "start": "1461295",
    "end": "1470145"
  },
  {
    "text": "And once you've sample each mini-batch, you check where this gradient descent, descends to for those parameters and then start to track that and",
    "start": "1470145",
    "end": "1478679"
  },
  {
    "text": "generate a mean and variance from wherever those moved around, if that makes sense.",
    "start": "1478680",
    "end": "1484570"
  },
  {
    "text": "I see. So how would you get, how- when you say you train them,",
    "start": "1485600",
    "end": "1490725"
  },
  {
    "text": "what do you mean by training the- Yeah. It's something [OVERLAPPING] I- oh so you- you- you- you're saying that you train the,",
    "start": "1490725",
    "end": "1496500"
  },
  {
    "text": "the parameters phi for a given task, see where it ends up. And then.",
    "start": "1496500",
    "end": "1502050"
  },
  {
    "text": "Yeah, then after- [LAUGHTER] after it's had a certain like acceptable performance then af- there after over each mini-batch,",
    "start": "1502050",
    "end": "1508830"
  },
  {
    "text": "it's gonna be, uh, moving around, um, kind of in the fine tuning stages.",
    "start": "1508830",
    "end": "1514335"
  },
  {
    "text": "Within those fine tuning stages, you'll see, uh, what the mean value is over",
    "start": "1514335",
    "end": "1519780"
  },
  {
    "text": "a certain number of fine tuning steps and also the variance within those fine tuning steps. Yeah, so basically that would correspond to something like you kind",
    "start": "1519780",
    "end": "1526549"
  },
  {
    "text": "of run gradient descent on for a given task, see what the- or SGD or something, see what the mean and the variance are and then use that to try to",
    "start": "1526550",
    "end": "1532940"
  },
  {
    "text": "fit your- fit this, this estimator. Yeah. Yeah. So that would be an interesting approach. I don't know if- I don't think anyone has done anything like that before,",
    "start": "1532940",
    "end": "1542645"
  },
  {
    "text": "but it'd be interesting to see. Yeah. We've got a project. Yeah, there you go. [LAUGHTER] Any other thoughts? Yeah.",
    "start": "1542645",
    "end": "1549919"
  },
  {
    "text": "I feel like if you try to [inaudible].",
    "start": "1549920",
    "end": "1564080"
  },
  {
    "text": "Yeah. So you can't- so the- we can't do it exactly the same way as we did,",
    "start": "1564080",
    "end": "1570049"
  },
  {
    "text": "uh, for the top one because we don't have labels corresponding to Phi, unless we generated labels,",
    "start": "1570050",
    "end": "1575345"
  },
  {
    "text": "uh, like was saying. So uh, yeah. So it's an interesting thought exercise to think about,",
    "start": "1575345",
    "end": "1582140"
  },
  {
    "text": "and we'll talk about basically different ways that we can get, uh, [NOISE] we can get- uh,",
    "start": "1582140",
    "end": "1587375"
  },
  {
    "text": "we can basically trained for this, uh, without having to have labels for the particular parameters that we, uh, that we get.",
    "start": "1587375",
    "end": "1595039"
  },
  {
    "start": "1595000",
    "end": "1894000"
  },
  {
    "text": "Okay. So, uh, before I go into the different approaches, I want to give a kind of a one slide overview",
    "start": "1595040",
    "end": "1602135"
  },
  {
    "text": "of kind of the Bayesian Deep Learning Toolbox which is, uh, kind of an overview of the different types of techniques we",
    "start": "1602135",
    "end": "1608210"
  },
  {
    "text": "have available to us to- for combining, um, [NOISE] Bayesian graphical models and deep learning.",
    "start": "1608210",
    "end": "1615335"
  },
  {
    "text": "Uh, it's worth mentioning that CS 236 provides an entire course on this topic.",
    "start": "1615335",
    "end": "1621260"
  },
  {
    "text": "Uh, and so one slide is, by no means, gives it, uh, justice. It's a very com- a very,",
    "start": "1621260",
    "end": "1626885"
  },
  {
    "text": "uh- there's been a lot of work in this area but, uh, I think that this will be important for thinking about how we can build Bayesian meta-learning algorithms on the next few slides.",
    "start": "1626885",
    "end": "1634309"
  },
  {
    "text": "Okay. So the general goal of these types of approaches is to think about how we can represent distributions using neural networks.",
    "start": "1634310",
    "end": "1641929"
  },
  {
    "text": "Uh, and there's been a number of approaches for doing this. Uh, and one of the most popular approaches are, or a very",
    "start": "1641930",
    "end": "1648440"
  },
  {
    "text": "popular approach is to use latent variable models, uh, and then optimize them with variational inference.",
    "start": "1648440",
    "end": "1656315"
  },
  {
    "text": "Uh, and so in particular, uh, what this corresponds to, uh, is we have a graphical model or something like a graph- model- graphical model on",
    "start": "1656315",
    "end": "1663790"
  },
  {
    "text": "the right where we have latent variable Z and observed variables X.",
    "start": "1663790",
    "end": "1669000"
  },
  {
    "text": "Uh, this is the graphical model for example for a variational auto-encoder. Uh, and then what you do is you can optimize,",
    "start": "1669000",
    "end": "1676145"
  },
  {
    "text": "um- you can optimize for a distribution over X using, uh,",
    "start": "1676145",
    "end": "1681485"
  },
  {
    "text": "the variational lower bound of- you basically formulate a lower bound of the likelihood objective and",
    "start": "1681485",
    "end": "1687410"
  },
  {
    "text": "use that to optimize over a distribution over X where that distribution over X may be non-Gaussian because it has,",
    "start": "1687410",
    "end": "1693514"
  },
  {
    "text": "uh, this latent variable. Uh, I'll go into a little bit more depth on this approach, uh, in, uh, a few minutes.",
    "start": "1693515",
    "end": "1700804"
  },
  {
    "text": "Okay. Um, another approach that has been fairly popular in some regard,",
    "start": "1700805",
    "end": "1708320"
  },
  {
    "text": "uh, and is quite simple, is to use a particle-based representation of your- uh, of your distribution.",
    "start": "1708320",
    "end": "1714950"
  },
  {
    "text": "Uh, and in particular, what you can do is you can train separate models on different bootstraps of your data.",
    "start": "1714950",
    "end": "1720784"
  },
  {
    "text": "And then get, uh- each of those models will correspond to a particle of your distribution,",
    "start": "1720785",
    "end": "1726559"
  },
  {
    "text": "uh, and then you get kind of this- you- basically, together, those particles represent, uh, samples from that distribution.",
    "start": "1726559",
    "end": "1734250"
  },
  {
    "text": "Uh, so this is something that could be useful, uh, it's pretty simple like a- pretty easy to combine with different,",
    "start": "1734770",
    "end": "1741304"
  },
  {
    "text": "uh, types of algorithms. Uh, another approach which has been- somewhat alluded to, uh,",
    "start": "1741305",
    "end": "1746615"
  },
  {
    "text": "is to represent an explicit distribution over the weights of neural network parameters. Uh, and then practice these distributions tend to be Gaussian with, uh, an independent,",
    "start": "1746615",
    "end": "1756230"
  },
  {
    "text": "uh, a diagonal co-variance matrix, so you have basically an independent variance for each neural network parameter.",
    "start": "1756230",
    "end": "1761795"
  },
  {
    "text": "Uh, and this allows you to represent, uh, a distribution over functions, uh, with the caveat that, uh,",
    "start": "1761795",
    "end": "1767690"
  },
  {
    "text": "this independence assumption that two parameters are independent is violated in practice,",
    "start": "1767690",
    "end": "1773090"
  },
  {
    "text": "uh, basically all the time. Uh, there's also things, uh, like normalizing flows that try to- that represent a function of over,",
    "start": "1773090",
    "end": "1782225"
  },
  {
    "text": "um- over a data distribution by inverting some latent distribution into your data distribution or transforming",
    "start": "1782225",
    "end": "1789560"
  },
  {
    "text": "from latent space into your data space and back into your latent space. Uh, and these- these have been pretty successful for,",
    "start": "1789560",
    "end": "1796940"
  },
  {
    "text": "um- for a wide range of applications. And then lastly, uh, is energy-based models and to some approximation GANs.",
    "start": "1796940",
    "end": "1804799"
  },
  {
    "text": "Uh, and what these types of models do is they tend to estimate a, a normalized density where you have, uh,",
    "start": "1804800",
    "end": "1811820"
  },
  {
    "text": "some probability that you want- or some probability over your data that you want to represent. Uh, and the way that you do that is you [NOISE] , uh,",
    "start": "1811820",
    "end": "1818150"
  },
  {
    "text": "push down the energy to- have lower energy for your data and higher energy for everything else,",
    "start": "1818150",
    "end": "1823160"
  },
  {
    "text": "uh, where everything else is approximated by your generator, in a GAN, or something like that.",
    "start": "1823160",
    "end": "1829540"
  },
  {
    "text": "Um, yeah. So [NOISE] these are the two- so this is kind of an overview of the general types of,",
    "start": "1829540",
    "end": "1838520"
  },
  {
    "text": "uh, tools that we have in, uh, in deep learning to be able to represent distributions over data and over functions.",
    "start": "1838520",
    "end": "1847100"
  },
  {
    "text": "Uh, in this lecture, we'll see how we can leverage the first two on this list,",
    "start": "1847100",
    "end": "1852140"
  },
  {
    "text": "uh, for Meta-learning approaches. Uh, the others could certainly also be useful in developing new Bayesian Meta-learning methods.",
    "start": "1852140",
    "end": "1859669"
  },
  {
    "text": "Uh, to my knowledge, people haven't looked at these types of approaches in the context of Bayesian Meta-learning,",
    "start": "1859670",
    "end": "1864860"
  },
  {
    "text": "uh, but they could certainly be, uh, be interesting for developing new methods. [NOISE] Okay.",
    "start": "1864860",
    "end": "1871139"
  },
  {
    "text": "Great. So that was,uh- I guess, for those of you that ha- that haven't seen this content before,",
    "start": "1871480",
    "end": "1876680"
  },
  {
    "text": "that was a lot faster than, than you probably would need to, to learn these sorts of things, of course.",
    "start": "1876680",
    "end": "1883145"
  },
  {
    "text": "Uh, but this is just- but the goal of this- so first,",
    "start": "1883145",
    "end": "1888890"
  },
  {
    "text": "um, we're gonna look at how we can use latent variable models in the context of meta-learning. Uh, and to do this, I want to give a little bit more background on,",
    "start": "1888890",
    "end": "1897125"
  },
  {
    "start": "1894000",
    "end": "2314000"
  },
  {
    "text": "uh, variational techniques and, and latent variable models and deep learning.",
    "start": "1897125",
    "end": "1902164"
  },
  {
    "text": "Uh, and in particular, this is the graphical model that we'll be looking at. Um, this is the graphical model that is used in,",
    "start": "1902165",
    "end": "1909920"
  },
  {
    "text": "in variational auto encoders where x is typically representing the data and z is representing your latent variables.",
    "start": "1909920",
    "end": "1915140"
  },
  {
    "text": "Uh, but in many- this graphical model is more general than just representing distributions over your data.",
    "start": "1915140",
    "end": "1922370"
  },
  {
    "text": "Uh, I will see that on the next slide. Uh, but I'll kind of derive things from the context of this graphical model because it's a",
    "start": "1922370",
    "end": "1928940"
  },
  {
    "text": "very general graphical model corresponding to things that are observed and latent variables that we want to be able to reason about.",
    "start": "1928940",
    "end": "1935495"
  },
  {
    "text": "Okay. So in this example, the observed variable is x and our latent variable is z.",
    "start": "1935495",
    "end": "1940895"
  },
  {
    "text": "Uh, again these could be- this could be data and, and something latent information but it could also be- uh, x can also represent something else.",
    "start": "1940895",
    "end": "1947855"
  },
  {
    "text": "Uh, and what, uh, prior works have done is derived a lower bound to the likelihood of our data which,",
    "start": "1947855",
    "end": "1957320"
  },
  {
    "text": "uh, is called the evidence lower bound or the ELBO, and this looks like this. So the- uh, we want to be able to estimate a lower bound on,",
    "start": "1957320",
    "end": "1965345"
  },
  {
    "text": "uh, the likelihood of our data. This is so that we can optimize for the likelihood of our data,",
    "start": "1965345",
    "end": "1970385"
  },
  {
    "text": "and we can represent this by, uh, what's shown on the right here which is the expectation with",
    "start": "1970385",
    "end": "1975950"
  },
  {
    "text": "respect to q, uh, of z given x. What q is representing, um, the variational distribution, uh,",
    "start": "1975950",
    "end": "1983075"
  },
  {
    "text": "of the probability of x and z, plus an entropy, uh,",
    "start": "1983075",
    "end": "1988385"
  },
  {
    "text": "term that is regularizing or operating on q of z given x.",
    "start": "1988385",
    "end": "1993515"
  },
  {
    "text": "How many people- I guess, how many people are familiar with kind of the background of,",
    "start": "1993515",
    "end": "1999169"
  },
  {
    "text": "of how you get this equation?",
    "start": "1999170",
    "end": "2002900"
  },
  {
    "text": "Okay. And how many people would find it useful to actually go through how we get this equation?",
    "start": "2005010",
    "end": "2012085"
  },
  {
    "text": "Okay. Cool, let's do that. It's not- it's actually not that complicated. So we have some- we're starting with the log likelihood of our- of our data,",
    "start": "2012085",
    "end": "2023409"
  },
  {
    "text": "and we also have some- some codes that looks like. So we want to be able to approximate this,",
    "start": "2023410",
    "end": "2031105"
  },
  {
    "text": "where X is our observed variable. And the reason why we wanna do this is typically we want to be able to maximize the likelihood of our data with respect to",
    "start": "2031105",
    "end": "2037840"
  },
  {
    "text": "some parameters of our model such that we can maximize the likelihood of things that we're observing.",
    "start": "2037840",
    "end": "2043195"
  },
  {
    "text": "So what we're going to first do is, under that graphical model we can say that this is simply, uh,",
    "start": "2043195",
    "end": "2050575"
  },
  {
    "text": "the integral over some latent variable",
    "start": "2050575",
    "end": "2055609"
  },
  {
    "text": "of the joint distribution of our observed variable and our latent variable Z.",
    "start": "2056820",
    "end": "2063159"
  },
  {
    "text": "This is fairly straightforward. And then what we're gonna do here is we're going to introduce what's called a variational distribution.",
    "start": "2063160",
    "end": "2071409"
  },
  {
    "text": "This is- this can be really any arbitrary distribution, and we're going to introduce it like this.",
    "start": "2071410",
    "end": "2082270"
  },
  {
    "text": "So we're just going to multiply it in and divide it out.",
    "start": "2082270",
    "end": "2089290"
  },
  {
    "text": "Now, QZ can really- uh, is some arbitrary distribution. We'll talk about exactly how we instantiate it and how we can",
    "start": "2089290",
    "end": "2096955"
  },
  {
    "text": "optimize for it such that we get a tighter bound later. Um, this is equal to.",
    "start": "2096955",
    "end": "2102850"
  },
  {
    "text": "So if you basically bring this term over here, you can see that we are getting- we can get an expectation with respect to q.",
    "start": "2102850",
    "end": "2108700"
  },
  {
    "text": "So this is equal to the expectation with respect to q of z of p of x,",
    "start": "2108700",
    "end": "2117070"
  },
  {
    "text": "z divided by, oh, I'm missing the log here, sorry.",
    "start": "2117070",
    "end": "2123740"
  },
  {
    "text": "Okay. Um, and then when we",
    "start": "2128550",
    "end": "2133690"
  },
  {
    "text": "look into- what we can do here is that with Jensen's inequality we can show that this, we can basically bring the log into",
    "start": "2133690",
    "end": "2140260"
  },
  {
    "text": "the expectation and we get a bound which corresponds to, uh, the expectation with respect to q of z of log p of x,",
    "start": "2140260",
    "end": "2151194"
  },
  {
    "text": "z minus log q of z.",
    "start": "2151195",
    "end": "2157280"
  },
  {
    "text": "Okay and then from here, uh, we basically get what's written on the board.",
    "start": "2160310",
    "end": "2165345"
  },
  {
    "text": "So this is equal to expectation of q of z times log p of x,",
    "start": "2165345",
    "end": "2174660"
  },
  {
    "text": "z plus the entropy of q of z.",
    "start": "2174660",
    "end": "2180859"
  },
  {
    "text": "Cool. I mentioned that q of z can be any arbitrary distribution.",
    "start": "2182700",
    "end": "2188005"
  },
  {
    "text": "That means we could also condition q of z on x as is done on the slides.",
    "start": "2188005",
    "end": "2195789"
  },
  {
    "text": "Um, yeah. Okay. Any questions on- on how this works?",
    "start": "2195790",
    "end": "2202839"
  },
  {
    "text": "Should there be a log on the slide then?",
    "start": "2202840",
    "end": "2208120"
  },
  {
    "text": "Yes, that's a typo. There should be a log basically inside the p of x, inside the expectation.",
    "start": "2208120",
    "end": "2214060"
  },
  {
    "text": "I'll fix that before- I'll fix that and update the slides.",
    "start": "2214060",
    "end": "2221305"
  },
  {
    "text": "I made these equations like yesterday so that's why there's a couple of typos.",
    "start": "2221305",
    "end": "2226375"
  },
  {
    "text": "Okay. Um, and this could also be, there's probably a typo in the next one too.",
    "start": "2226375",
    "end": "2231490"
  },
  {
    "text": "Yeah, there's also a typo on the next one. Um, so these can also be rewritten as- you can basically take out- You can",
    "start": "2231490",
    "end": "2237910"
  },
  {
    "text": "represent this as log p",
    "start": "2237910",
    "end": "2244420"
  },
  {
    "text": "of x given z times p of z.",
    "start": "2244420",
    "end": "2251155"
  },
  {
    "text": "And with this you can then, uh, bring this term into- combine this term with entropy term here,",
    "start": "2251155",
    "end": "2259345"
  },
  {
    "text": "and you get a KL divergence between q and p. And so this may be the,",
    "start": "2259345",
    "end": "2265734"
  },
  {
    "text": "if you're familiar with variational autoencoders you may see this term a bit more frequently, uh, where the first term corresponds to the reconstruction loss of your decoder.",
    "start": "2265735",
    "end": "2275260"
  },
  {
    "text": "Basically the likelihood of your data according to your decoder after sampling from your, your inference network q, and the second term corresponds to",
    "start": "2275260",
    "end": "2282310"
  },
  {
    "text": "the KL divergence between your inference network and your prior. Okay. Um, so p corresponds to your model,",
    "start": "2282310",
    "end": "2291925"
  },
  {
    "text": "and q corresponds to this kind of variational distribution that we introduced in order to approximate the, uh, likelihood objective.",
    "start": "2291925",
    "end": "2300745"
  },
  {
    "text": "And so then we have a couple, couple of things, a couple of design choices. So the first design choice is how do we represent p of z,",
    "start": "2300745",
    "end": "2308755"
  },
  {
    "text": "and the second choice is how do we represent p of x given z. These are kind of the two, um, the two parts of this,",
    "start": "2308755",
    "end": "2316059"
  },
  {
    "start": "2314000",
    "end": "2604000"
  },
  {
    "text": "um, of the model. And p of x given z can be represented as a neural network.",
    "start": "2316060",
    "end": "2323335"
  },
  {
    "text": "So in the case of a variational autoencoder, p of x given z corresponds to a neural network that takes in your latent variable and outputs an image or whatever your,",
    "start": "2323335",
    "end": "2331540"
  },
  {
    "text": "uh, whatever datatype you're modeling. And then p of z is represented as a- is typically represented as",
    "start": "2331540",
    "end": "2339609"
  },
  {
    "text": "just a diagonal Gaussian with unit variance.",
    "start": "2339610",
    "end": "2346255"
  },
  {
    "text": "Uh, you'd also learn your prior. P of z could also be represented by a neural network as",
    "start": "2346255",
    "end": "2352360"
  },
  {
    "text": "well or represented as a learned mean and a learned variance. In practice in variational autoencoders you typically don't learn it,",
    "start": "2352360",
    "end": "2359575"
  },
  {
    "text": "because the layer afterward can transform it into a learned mean and variance.",
    "start": "2359575",
    "end": "2364615"
  },
  {
    "text": "Um, but we'll see in, in the Meta-learning case are actually gonna be situations where we'll want to learn it.",
    "start": "2364615",
    "end": "2370190"
  },
  {
    "text": "Okay. Um, and then q of z given x is also represented by a neural network.",
    "start": "2370260",
    "end": "2376000"
  },
  {
    "text": "This is also often referred to as your inference network or your variational distribution.",
    "start": "2376000",
    "end": "2383000"
  },
  {
    "text": "Okay. And then to connect to the graphical model a bit, typically in, in- uh,",
    "start": "2383130",
    "end": "2391090"
  },
  {
    "text": "when using variational inference in the context of deep learning you often use theta to represent your model parameters,",
    "start": "2391090",
    "end": "2396385"
  },
  {
    "text": "and phi to represent the parameters of your inference network. I dropped these in all of these equations because we're gonna be using theta and phi to",
    "start": "2396385",
    "end": "2405010"
  },
  {
    "text": "represent different things as you might imagine on the following slides.",
    "start": "2405010",
    "end": "2410245"
  },
  {
    "text": "Okay. So this is all nice. We have an objective, we can optimize. We have kind of neural networks that are representing different components of this,",
    "start": "2410245",
    "end": "2418315"
  },
  {
    "text": "and we can just backpropagate our objective into our neural networks. Yeah? [BACKGROUND]",
    "start": "2418315",
    "end": "2427035"
  },
  {
    "text": "Yes, exactly. Yeah. So that solid arrows are representing the, the model distribution and",
    "start": "2427035",
    "end": "2432270"
  },
  {
    "text": "the dashed arrows are representing the variational distribution. Okay. So we have an objective.",
    "start": "2432270",
    "end": "2440235"
  },
  {
    "text": "Uh, we should be able to optimize it, right? Uh, but we have a problem. Uh, the problem is that we have an expectation with respect to",
    "start": "2440235",
    "end": "2448260"
  },
  {
    "text": "q and we need to be able to back-propagate into this q distribution.",
    "start": "2448260",
    "end": "2453750"
  },
  {
    "text": "Uh, and sampling unfortunately, is not differentiable.",
    "start": "2453750",
    "end": "2458980"
  },
  {
    "text": "Uh, so, uh, one of the kind of, uh, tricks that has been actually pretty critical to making these, uh,",
    "start": "2459320",
    "end": "2466829"
  },
  {
    "text": "making this type of approach work in the context of neural networks is what's called the reparametrization trick.",
    "start": "2466830",
    "end": "2472560"
  },
  {
    "text": "Uh, and in particular, what we can show is that for a Gaussian q of z given x,",
    "start": "2472560",
    "end": "2480210"
  },
  {
    "text": "you can represent q of z given x in a differentiable way that is kind of",
    "start": "2480210",
    "end": "2486060"
  },
  {
    "text": "re-parameterized in terms of this noise that is sampled from a unit Gaussian.",
    "start": "2486060",
    "end": "2493215"
  },
  {
    "text": "So in particular, uh, if we're representing the distribution of our latent variable as a Gaussian,",
    "start": "2493215",
    "end": "2499260"
  },
  {
    "text": "uh, which corresponds to the output of this neural network q, which is outputting both the mean and a variance,",
    "start": "2499260",
    "end": "2505875"
  },
  {
    "text": "then you can represent, uh, the output of that, uh, neural network as kinda being reparameterized by this noise rather",
    "start": "2505875",
    "end": "2512450"
  },
  {
    "text": "than sampling from that particular distribution of the mean plus the, the variance times that noise. And fortunately, this equation is something that's differentiable.",
    "start": "2512450",
    "end": "2520710"
  },
  {
    "text": "Uh, we can differentiate with respect to this e- uh, for this equation with respect to the mean and the variance,",
    "start": "2520710",
    "end": "2525960"
  },
  {
    "text": "parameters into the parameters of q, into the parameters of that neural network.",
    "start": "2525960",
    "end": "2531490"
  },
  {
    "text": "Okay. Um, so this is something special that you can do for Gaussians. I believe there's also some work that has looked at non Gaussian distributions as well.",
    "start": "2532130",
    "end": "2539369"
  },
  {
    "text": "Uh, although in practice, I think that the kind of the easiest thing to do is, is, is to do it with the Gaussians.",
    "start": "2539370",
    "end": "2544710"
  },
  {
    "text": "Uh, and if your inference network is expressive enough, it should be able to transform",
    "start": "2544710",
    "end": "2550800"
  },
  {
    "text": "your data distribution into this Gaussian distribution over latent variables.",
    "start": "2550800",
    "end": "2555730"
  },
  {
    "text": "Okay. So that was the primer on variational inference for deep learning.",
    "start": "2555980",
    "end": "2563200"
  },
  {
    "text": "Any questions before I move on to how we can use this with meta-learning?",
    "start": "2563270",
    "end": "2568360"
  },
  {
    "text": "Okay. Um, also, I guess worth mentioning, this is often called amortized variational inference",
    "start": "2570770",
    "end": "2577260"
  },
  {
    "text": "and that we're having an inference network that's, uh, that's predicting what the variational distribution will be as basically",
    "start": "2577260",
    "end": "2583770"
  },
  {
    "text": "amortizing the process of estimating that distribution. There are also variational inference techniques that",
    "start": "2583770",
    "end": "2590045"
  },
  {
    "text": "I estimate this distribution over Z by using something like, uh, MCMC, uh, for example by,",
    "start": "2590045",
    "end": "2596330"
  },
  {
    "text": "by optimizing with respect to your, uh, your likelihood. Uh, so can we use amortized variational inference for Meta-learning?",
    "start": "2596330",
    "end": "2603165"
  },
  {
    "text": "So, uh, first let's think about black-box Meta-learning approaches for simplicity.",
    "start": "2603165",
    "end": "2609600"
  },
  {
    "start": "2604000",
    "end": "3122000"
  },
  {
    "text": "Uh, and in particular, what we wanna be able to do is have a neural network that takes as input a train dataset and produces a distribution over our parameters Phi.",
    "start": "2609600",
    "end": "2619035"
  },
  {
    "text": "Uh, and then we're gonna take our parameters Phi and have that parameterize a neural network that takes as input x and outputs y.",
    "start": "2619035",
    "end": "2625600"
  },
  {
    "text": "Okay. So in the standard VAE, uh, our observed variables X,",
    "start": "2625700",
    "end": "2632550"
  },
  {
    "text": "our latent variable Z and we kind of derived what the, um, what the, the,",
    "start": "2632550",
    "end": "2637785"
  },
  {
    "text": "the lower bound of our likelihood is. In the Meta-learning case, our observed variable is our data and our latent variable is our parameters Phi.",
    "start": "2637785",
    "end": "2649750"
  },
  {
    "text": "And so what we can do is we can basically use everything that we derived here",
    "start": "2649940",
    "end": "2655800"
  },
  {
    "text": "in for basically Meta-learning where the latent variable is now going to be Phi.",
    "start": "2655800",
    "end": "2663040"
  },
  {
    "text": "And in particular, what we can do is you can take, uh, take the ELBO written above. Again, sorry for the lack of logs on the left.",
    "start": "2663050",
    "end": "2670170"
  },
  {
    "text": "I'll, I'll, I'll fix that typo. Um, we get basically, something that looks like we'll have",
    "start": "2670170",
    "end": "2675750"
  },
  {
    "text": "a variational distribution over our task specific parameters Phi. Uh, we'll sample from that distribution and estimate the likelihood of",
    "start": "2675750",
    "end": "2685530"
  },
  {
    "text": "our data given Phi and then we'll have this KL term that's saying that the, uh, the variational, uh,",
    "start": "2685530",
    "end": "2692550"
  },
  {
    "text": "distribution and our, um, our prior over Phi should be similar.",
    "start": "2692550",
    "end": "2699420"
  },
  {
    "text": "Okay. So this is pretty simple. Um, there's a couple of design decisions. So the, the first design decision is what should q condition on?",
    "start": "2700330",
    "end": "2708714"
  },
  {
    "text": "Um, in this case, the- if we wanna be able to sample parameters,",
    "start": "2708715",
    "end": "2716505"
  },
  {
    "text": "so if you basi- I guess I sort of gave it away in the top left. If you wanna be able to sample parameters as a function of our dataset,",
    "start": "2716505",
    "end": "2721890"
  },
  {
    "text": "then we should condition q on our training data. Uh, so that's exactly what we'll do here.",
    "start": "2721890",
    "end": "2728230"
  },
  {
    "text": "And, uh, also, one question is how does this training data differ from the probability of our data used here, uh,",
    "start": "2728530",
    "end": "2737369"
  },
  {
    "text": "and what we'll use for the data here will correspond to our test data points. So you wanna be able to maximize, uh,",
    "start": "2737370",
    "end": "2743609"
  },
  {
    "text": "the likelihood of the test data points given our task-specific parameters when sampling our task-specific parameters as a function of our training dataset.",
    "start": "2743610",
    "end": "2751240"
  },
  {
    "text": "Okay. Um, so that's gonna be what our inference network looks like and what our, uh, what our objective looks like.",
    "start": "2751910",
    "end": "2758340"
  },
  {
    "text": "Uh, what about the meta parameters? So we're missing the meta parameters right now. Uh, they don't appear anywhere.",
    "start": "2758340",
    "end": "2765105"
  },
  {
    "text": "Uh, and that's because I didn't actually give any parameters to p and q. Um, so the meta parameters will come in.",
    "start": "2765105",
    "end": "2772080"
  },
  {
    "text": "Is that there'll be, uh, parameters of our model.",
    "start": "2772080",
    "end": "2777285"
  },
  {
    "text": "Um, so I added here, uh, basically, we'll have our, our prior over our task-specific parameters Phi be conditioned on Theta.",
    "start": "2777285",
    "end": "2787484"
  },
  {
    "text": "Um, although, you could also have a prior that corresponds to I guess, a unit variance or something, but if you condition on Theta you might get a bit more expressive power. [NOISE]",
    "start": "2787485",
    "end": "2795329"
  },
  {
    "text": "And then, we'll also condition our inference network on Theta as well, um, because well, basically,",
    "start": "2795330",
    "end": "2803340"
  },
  {
    "text": "Theta will correspond to the kind of the parameters of our, our inference network. [NOISE] Um, one other thing worth mentioning in this case,",
    "start": "2803340",
    "end": "2811980"
  },
  {
    "text": "uh, this- the distribution over our test data points is a function of only Phi.",
    "start": "2811980",
    "end": "2817710"
  },
  {
    "text": "We could also condition on Theta here. Uh, and that will correspond to something that's a bit more like what you implemented in your homework, where the, um,",
    "start": "2817710",
    "end": "2825539"
  },
  {
    "text": "where Phi is something that's like a low-dimensional vector and Theta is representing like the parameters of a neural network.",
    "start": "2825540",
    "end": "2832930"
  },
  {
    "text": "Okay. Uh, so for completeness, our final objective is written down here,",
    "start": "2833240",
    "end": "2839099"
  },
  {
    "text": "where we're gonna have- and again, I'm missing the log in front of the p but, uh, we're gonna be maximizing,",
    "start": "2839100",
    "end": "2844470"
  },
  {
    "text": "uh, with respect to our meta parameters and expectation over all of our tasks. And then, uh, we'll sample task-specific parameters from,",
    "start": "2844470",
    "end": "2852720"
  },
  {
    "text": "uh, from our neural network q, use those to maximize the likelihood of p. And then,",
    "start": "2852720",
    "end": "2858420"
  },
  {
    "text": "we'll also have this term on the right that encourages q to stay close to some prior distribution.",
    "start": "2858420",
    "end": "2864765"
  },
  {
    "text": "Note that if you drop this right KL term, we get something that looks basically exactly like what we've been optimizing before.",
    "start": "2864765",
    "end": "2874349"
  },
  {
    "text": "Uh, this right KL term is what actually ensures that it corresponds to a distribution and that we're actually maximizing,",
    "start": "2874350",
    "end": "2881744"
  },
  {
    "text": "uh, a lower bound on the likelihood in this graphical model. Yeah. So this, this, subject corresponds to the ELBO, uh,",
    "start": "2881745",
    "end": "2891135"
  },
  {
    "text": "for the same conditional variational inference [inaudible]",
    "start": "2891135",
    "end": "2902060"
  },
  {
    "text": "So what the variational inference biases do is it ensures that q is out- is,",
    "start": "2902060",
    "end": "2907635"
  },
  {
    "text": "is going to be outputting a, a proper distribution over Phi. And so if you only did maximum likelihood over the labels, then,",
    "start": "2907635",
    "end": "2915240"
  },
  {
    "text": "you'll get a distribution over y but you won't necessarily be guaranteed that the distribution over Phi is actually a proper distribution.",
    "start": "2915240",
    "end": "2924825"
  },
  {
    "text": "It's actually like something that represents the true distribution over your, um, over your latent parameter,",
    "start": "2924825",
    "end": "2931005"
  },
  {
    "text": "over your latent task parameters. Does that make sense? [inaudible] objective corresponds to ELBO let's say like?",
    "start": "2931005",
    "end": "2939910"
  },
  {
    "text": "Um. Yes. It corresponds to the ELBO of, uh, of, of the, the,",
    "start": "2943130",
    "end": "2949365"
  },
  {
    "text": "the, the evidence lower bound of the, the observed variables in that graphical model.",
    "start": "2949365",
    "end": "2956260"
  },
  {
    "text": "Yes. Basically, I yeah, to me, what I advise you is, is that you can represent",
    "start": "2956930",
    "end": "2962460"
  },
  {
    "text": "a distribution over Phi and nothing more than that.",
    "start": "2962460",
    "end": "2966220"
  },
  {
    "text": "I guess- oh, the last thing it does give you also sorry is, um, you can represent non-Gaussian distributions over y now or,",
    "start": "2968910",
    "end": "2976570"
  },
  {
    "text": "or not like if y is continuous, for example. Uh, this because we're introducing a,",
    "start": "2976570",
    "end": "2983065"
  },
  {
    "text": "a latent variable the- this distribution, uh, conditioned on Phi it will- it needs to be as Gaussian or something,",
    "start": "2983065",
    "end": "2990070"
  },
  {
    "text": "but because you're introducing this latent variable. Now because it's conditioned on Phi this, um, the marginal distribution of y given x can be something that's highly multi-modal,",
    "start": "2990070",
    "end": "2998170"
  },
  {
    "text": "can be, be really anything. Anything that can be represented- that can be transformed from",
    "start": "2998170",
    "end": "3003900"
  },
  {
    "text": "a Gaussian to another distribution by a neural network.",
    "start": "3003900",
    "end": "3007510"
  },
  {
    "text": "Okay. Cool. So there's our objective, um,",
    "start": "3009290",
    "end": "3015090"
  },
  {
    "text": "the benefits of this approach is that you can represent non-Gaussian distributions over y, uh, and you can also get a distribution- I didn't write this.",
    "start": "3015090",
    "end": "3022890"
  },
  {
    "text": "We can get a distribution over your task parameters Phi, rather than only getting a distribution over your labels.",
    "start": "3022890",
    "end": "3028425"
  },
  {
    "text": "So this allows you to represent of- about your uncertainty over the underlying function, and not just about the underlying data points.",
    "start": "3028425",
    "end": "3034049"
  },
  {
    "text": "Uh, the downside is that this can only represent, uh, Gaussian distributions of P of Phi given theta.",
    "start": "3034050",
    "end": "3041234"
  },
  {
    "text": "Uh, and that's, uh, there's I guess two reasons for that. One is the reparameterization trick, which you can, uh,",
    "start": "3041235",
    "end": "3047130"
  },
  {
    "text": "which holds for Gaussian, uh, Gaussian distributions but it's much more difficult to do for non-Gaussian distributions.",
    "start": "3047130",
    "end": "3052380"
  },
  {
    "text": "Uh, the second thing is that the, um, the KL term that comes up in",
    "start": "3052380",
    "end": "3057990"
  },
  {
    "text": "the objective is something that can be evaluated in closed form for Gaussians, uh, but can't be evaluated in closed form for other non-Gaussian objectives.",
    "start": "3057990",
    "end": "3067240"
  },
  {
    "text": "Okay. Um, and the second thing is, is a downside, particularly if Phi is representing all of the parameters of a neural network.",
    "start": "3067310",
    "end": "3076425"
  },
  {
    "text": "If it, uh, if this distribution is also conditioned on Theta, and for example, you do something in your homework where Phi is representing a small vector,",
    "start": "3076425",
    "end": "3083580"
  },
  {
    "text": "and Theta is representing the majority of those neural network parameters, then in my mind this is less of a restriction, uh, because you can,",
    "start": "3083580",
    "end": "3091545"
  },
  {
    "text": "um, because the- basically, it is okay for Phi to be, uh, Phi to be Gaussian because it could,",
    "start": "3091545",
    "end": "3096870"
  },
  {
    "text": "could be transformed by the neural network into a more complicated distribution.",
    "start": "3096870",
    "end": "3101500"
  },
  {
    "text": "Okay. Any questions before we move on to optimization-based Meta-learning?",
    "start": "3103010",
    "end": "3110470"
  },
  {
    "text": "Okay. So what about optimization-based Meta-learning approaches?",
    "start": "3114860",
    "end": "3120300"
  },
  {
    "text": "So, um, you might say okay, well, we, we talked about this a little bit, uh, a couple of lectures ago, uh,",
    "start": "3120300",
    "end": "3127215"
  },
  {
    "start": "3122000",
    "end": "3351000"
  },
  {
    "text": "where there is this paper, um, that kind of recasted Gradient-Based Meta-Learning",
    "start": "3127215",
    "end": "3132390"
  },
  {
    "text": "as inference in this hierarchical Bayesian model. Uh, and this is nice in that it provide a Bayesian interpretation of MAML as,",
    "start": "3132390",
    "end": "3139950"
  },
  {
    "text": "uh, as kind of doing, uh, learning meta parameters such that you are- such that at",
    "start": "3139950",
    "end": "3148470"
  },
  {
    "text": "test time you're doing map inference under this- under a Gaussian prior represented by your meta parameters.",
    "start": "3148470",
    "end": "3154724"
  },
  {
    "text": "So this is nice, uh, but it isn't kind of what we set out for at the beginning of this lecture.",
    "start": "3154725",
    "end": "3161295"
  },
  {
    "text": "Which is, in particular, what it's doing is it's using a map estimate of your task-specific parameters phi,",
    "start": "3161295",
    "end": "3167970"
  },
  {
    "text": "as a function of theta. And that means that it's representing a point estimate of this distribution and it's only giving you one set of parameters for this distribution, and you can't,",
    "start": "3167970",
    "end": "3175845"
  },
  {
    "text": "for example, sample from this distribution or you can't, uh, more easily represent the full distribution of your task parameters.",
    "start": "3175845",
    "end": "3183370"
  },
  {
    "text": "Okay. So how can we develop approaches that actually allow us to do that?",
    "start": "3183650",
    "end": "3189630"
  },
  {
    "text": "So one thing we could do is build upon the kind of, what we had derived before for black-box Meta-learning,",
    "start": "3189630",
    "end": "3197565"
  },
  {
    "text": "uh, where we have- uh, we're going to use amortized variational inference. We have this objective, uh,",
    "start": "3197565",
    "end": "3203385"
  },
  {
    "text": "that corresponds to the likelihood of the data, plus, uh, uh, minus this KL term, and in particular,",
    "start": "3203385",
    "end": "3210299"
  },
  {
    "text": "one thing to remember is that q can really be any arbitrary function.",
    "start": "3210300",
    "end": "3217710"
  },
  {
    "text": "Uh, and in particular, what we did before, is we had q be a black-box that takes as input- that has",
    "start": "3217710",
    "end": "3223920"
  },
  {
    "text": "parameters theta and takes as input a data set and outputs a set of parameters, but we'd also do other things for q.",
    "start": "3223920",
    "end": "3231335"
  },
  {
    "text": "Uh, for example, q could also include a gradient operator and in particular,",
    "start": "3231335",
    "end": "3236970"
  },
  {
    "text": "it's so you can have an inference network that is basically performing gradient descent inside your inference network. Because q doesn't need to be a neural network,",
    "start": "3236970",
    "end": "3243750"
  },
  {
    "text": "it could be anything that outputs a distribution over phi. And so in particular, in this, in this paper what, um,",
    "start": "3243750",
    "end": "3252059"
  },
  {
    "text": "what this approach does is theta set q to be SGD, with respect to the mean and variance of neural network weights.",
    "start": "3252060",
    "end": "3260550"
  },
  {
    "text": "So we're gonna be running gradient descent with respect to the mean of a set of parameters and the variance of the set of parameters with respect to some training data,",
    "start": "3260550",
    "end": "3269010"
  },
  {
    "text": "and then everything else is the same. And in particular, I guess one thing that's worth",
    "start": "3269010",
    "end": "3274050"
  },
  {
    "text": "mentioning is that in contrast to things like MAML, this is not just running a gradient descent on a parameter, is running gradient descent on the mean and the variance of the parameters, uh,",
    "start": "3274050",
    "end": "3283005"
  },
  {
    "text": "and that such that you actually, at the end of this procedure you get both the mean and variance of the, of the parameter, it's not just a mean.",
    "start": "3283005",
    "end": "3290200"
  },
  {
    "text": "Okay. So, uh, the benefit of this approach is that you're, you're running gradient descent at test time, and so we,",
    "start": "3291020",
    "end": "3297090"
  },
  {
    "text": "we get an optimization-based Meta-learning approach by basically stuffing gradient descent into your reference network.",
    "start": "3297090",
    "end": "3303105"
  },
  {
    "text": "Uh, and it's also- it's quite simple, uh, the downside is that you need to model",
    "start": "3303105",
    "end": "3309930"
  },
  {
    "text": "this distribution phi given theta as a Gaussian, uh, and the reason for that, uh,",
    "start": "3309930",
    "end": "3316125"
  },
  {
    "text": "similar to what I- the same reason why we need to do it for, is, is the same reason for why we need to do it in the black-box case.",
    "start": "3316125",
    "end": "3322155"
  },
  {
    "text": "Which is that we need to be able to re-parameterize in order to backprop gradients into q, and we need to be able to evaluate the KL term that's on the right.",
    "start": "3322155",
    "end": "3330880"
  },
  {
    "text": "And in, in the case of optimization-based Meta-learning, this is more of a problem because these are actually going to be representing the parameters of our neural network.",
    "start": "3331310",
    "end": "3341170"
  },
  {
    "text": "Okay. So can we model a non-Gaussian posterior with optimization-based Meta-learning algorithms? [NOISE]",
    "start": "3341420",
    "end": "3348225"
  },
  {
    "text": "So one thing that we talked about, uh, in terms of the kind",
    "start": "3348225",
    "end": "3354180"
  },
  {
    "start": "3351000",
    "end": "3600000"
  },
  {
    "text": "of toolbox of our approaches is that we don't just have- there isn't just latent variable inference.",
    "start": "3354180",
    "end": "3359190"
  },
  {
    "text": "There isn't just variational autoencoders, we can all see something like ensembles. Um, and so this is what, uh, Kim et al did in, in 2018.",
    "start": "3359190",
    "end": "3368039"
  },
  {
    "text": "Uh, the first kind of basic version of the algorithm was to just train m independent MAML models.",
    "start": "3368040",
    "end": "3375930"
  },
  {
    "text": "So you just run MAML on different, uh, subsets of your data,",
    "start": "3375930",
    "end": "3381240"
  },
  {
    "text": "then you get an ensemble MAMLs. It's also worth noting that you could do this with something other than MAML as well.",
    "start": "3381240",
    "end": "3388830"
  },
  {
    "text": "You could do- like you could do an ensemble of black-box models or non-parametric models. Uh, and this is actually- it's pretty simple, and it actually works pretty well.",
    "start": "3388830",
    "end": "3396690"
  },
  {
    "text": "Uh, and so if you only like if you want to kind of- probably the simplest approach for,",
    "start": "3396690",
    "end": "3402165"
  },
  {
    "text": "for getting, uh, a distribution over models, you could just train an ensemble of things. [NOISE]",
    "start": "3402165",
    "end": "3408310"
  },
  {
    "text": "Uh, one downside of this approach though is if you, if you really care about getting accurate distributions,",
    "start": "3408350",
    "end": "3413685"
  },
  {
    "text": "this won't work well if your ensemble members are too similar. And sometimes if you just train networks independently,",
    "start": "3413685",
    "end": "3420195"
  },
  {
    "text": "you might end up with, uh, those models looking very similar depending on, uh, some of the implementation details of how you initialize how,",
    "start": "3420195",
    "end": "3427290"
  },
  {
    "text": "how you sample the data for each model. Um, and so, what, uh,",
    "start": "3427290",
    "end": "3433319"
  },
  {
    "text": "what the others did in this paper is they showed that there's, uh, ways to make it more diverse. So in particular, there's an approach called Stein variational gradient descent.",
    "start": "3433320",
    "end": "3441765"
  },
  {
    "text": "Uh, and what, what it basically does, I don't want to get into some the- to all the details, but what it basically does is it pushes the different ensemble members",
    "start": "3441765",
    "end": "3448650"
  },
  {
    "text": "away from each other to encourage them to represent different models. Uh, and the way that you accomplish this is that you",
    "start": "3448650",
    "end": "3455250"
  },
  {
    "text": "have- when you're running gradient descent, you have your typical likelihood term, but you also have a term that encourages",
    "start": "3455250",
    "end": "3461954"
  },
  {
    "text": "the different ensemble members to be different from one another. Uh, And there's different, uh, different kernel- kernel like",
    "start": "3461954",
    "end": "3468180"
  },
  {
    "text": "functions that you can do to represent the similarity between models. Okay, and so the result of this is you get a more diverse ensemble of MAMLs. [NOISE]",
    "start": "3468180",
    "end": "3477490"
  },
  {
    "text": "Okay. Um, the only thing that they did, uh, here as well, is instead of just pushing the particles away from one another,",
    "start": "3477770",
    "end": "3484890"
  },
  {
    "text": "they also optimized such that the ensemble gives you, um, a distribution of particles that produce high likelihood.",
    "start": "3484890",
    "end": "3493575"
  },
  {
    "text": "And so instead of completely separately training these different models, they took, uh,",
    "start": "3493575",
    "end": "3498885"
  },
  {
    "text": "their ens- basically, they took their ensemble and they optimized a term that depends- basically,",
    "start": "3498885",
    "end": "3504060"
  },
  {
    "text": "they optimized them all jointly together. So you have- if you have M particles, you're out- you're optimizing for the likelihood that's averaged over",
    "start": "3504060",
    "end": "3510600"
  },
  {
    "text": "those M particles. All right. So the benefits of this approach is that it's simple,",
    "start": "3510600",
    "end": "3517664"
  },
  {
    "text": "and tends to work well. It also gives you non-Gaussian distributions, which is quite nice.",
    "start": "3517665",
    "end": "3524280"
  },
  {
    "text": "Uh, the downside is that in some instances- in some scenarios you may not want to represent m different sets of",
    "start": "3524280",
    "end": "3529859"
  },
  {
    "text": "parameters and this is something that requires you to maintain m, uh, instances of your model. [NOISE]",
    "start": "3529860",
    "end": "3536260"
  },
  {
    "text": "Uh, the authors said that one way to get around this is to only do gradient-based inference on your last layer. So you only have to have m copies of the last layer of your network,",
    "start": "3537380",
    "end": "3544380"
  },
  {
    "text": "rather than n copies of the entire, uh, entire network, uh, although in practice you may, uh,",
    "start": "3544380",
    "end": "3549510"
  },
  {
    "text": "you may also want to be able to represent distributions over all of your parameters not just over the last layer.",
    "start": "3549510",
    "end": "3554680"
  },
  {
    "text": "Okay. So, so far we talked about how we can use amortized variational inference and how we can use ensembles.",
    "start": "3554860",
    "end": "3562035"
  },
  {
    "text": "Uh, there's one more approach [NOISE] that, uh, I'll talk about here but any questions on the ensembles? Yeah.",
    "start": "3562035",
    "end": "3569900"
  },
  {
    "text": "In the last one, you mentioned where you have, uh, sort of jointly optimizing for high likelihood.",
    "start": "3569900",
    "end": "3576040"
  },
  {
    "text": "Could it be the case so that all M of your different models just find the point of- like all find the same point of",
    "start": "3576040",
    "end": "3582660"
  },
  {
    "text": "high likelihood and that would be [inaudible] likelihood, but they wouldn't be [NOISE] distinct or independent from one another?.",
    "start": "3582660",
    "end": "3587955"
  },
  {
    "text": "Yeah. So that's- yes, so basically if you only did this, this right term they might just all kind of collapse to",
    "start": "3587955",
    "end": "3593450"
  },
  {
    "text": "a single point of high likelihood and that's exactly what this, this left, uh, thing will do.",
    "start": "3593450",
    "end": "3599330"
  },
  {
    "text": "It'll try to push them apart from each other such that they represent, um, they represent, uh, different parts of the distribution.",
    "start": "3599330",
    "end": "3605790"
  },
  {
    "text": "[NOISE]. Okay. So is there a way to try to get it on Gaussian posterior over",
    "start": "3605790",
    "end": "3614530"
  },
  {
    "text": "all the parameters without having to maintain separate model instances?",
    "start": "3614530",
    "end": "3620125"
  },
  {
    "text": "So this is the last approach that we'll talk about and in particular what we'll try to do I guess.",
    "start": "3620125",
    "end": "3627070"
  },
  {
    "text": "Yeah, I guess, yeah or yeah. What we'll try do is we'll try to sample para- parameter vectors at",
    "start": "3627070",
    "end": "3633850"
  },
  {
    "text": "test time with a procedure that looks like Hamiltonian Monte Carlo. And in particular what Hamiltonian Monte Carlo does is it",
    "start": "3633850",
    "end": "3640329"
  },
  {
    "text": "adds noise and then runs gradient descent, uh, repeatedly in order to be able to sample from some distribution.",
    "start": "3640330",
    "end": "3647800"
  },
  {
    "text": "And so in particular what we'll wanna do is if we have, uh, say we have our example, uh,",
    "start": "3647800",
    "end": "3653005"
  },
  {
    "text": "shown here where we have this ambiguous situation, what we want to be able to do is learn a prior such that a random kick in",
    "start": "3653005",
    "end": "3659890"
  },
  {
    "text": "a direction will put us in different modes of our distribution. Uh, so for example if this is our,",
    "start": "3659890",
    "end": "3667120"
  },
  {
    "text": "um, our loss landscape, uh, and there's different, uh, different, kind of, modes of solutions.",
    "start": "3667120",
    "end": "3675430"
  },
  {
    "text": "One corresponds to a classifier that classifies on smiling and wearing a hat, one that classifies on smiling and being young.",
    "start": "3675430",
    "end": "3682635"
  },
  {
    "text": "We want, uh, essentially parameter vector here such that if we add some noise to that parameter vector and then run gradient descent,",
    "start": "3682635",
    "end": "3690615"
  },
  {
    "text": "we get two different modes of this solution. We get different functions that represent, uh,",
    "start": "3690615",
    "end": "3697960"
  },
  {
    "text": "different- we get different functions that represent different modes of the correct answer. Okay. So this is what we want to be able to do at",
    "start": "3697960",
    "end": "3705310"
  },
  {
    "text": "test-time, just add noise and run gradient descent. Uh, the way that we do this, uh, so first let's bring up the, our graphical model shown before.",
    "start": "3705310",
    "end": "3713110"
  },
  {
    "text": "Uh, in this case we're gonna have theta be a distribution. So one, uh, one nuanced thing I didn't mention before is before if we just,",
    "start": "3713110",
    "end": "3722950"
  },
  {
    "text": "kind of, putting Theta as parameters we are not actually, um, having a distribution over Theta.",
    "start": "3722950",
    "end": "3728455"
  },
  {
    "text": "In this case, uh, Theta will no longer just be, kind of, a single parameter vector that parameterizes things. It'll actually be a distribution.",
    "start": "3728455",
    "end": "3734680"
  },
  {
    "text": "Uh, and then we'll also have a set of- we'll also have kind of our task-specific parameters given Theta.",
    "start": "3734680",
    "end": "3743380"
  },
  {
    "text": "Our goal, uh, will be to sample different task-specific parameters given all of our observed variables at test time.",
    "start": "3743380",
    "end": "3751390"
  },
  {
    "text": "And the things that we can observe at test-time are X train, Y train, and X test. [NOISE]",
    "start": "3751390",
    "end": "3757420"
  },
  {
    "text": "Okay. Um, so this is our, our- just like our goal is from before, uh,",
    "start": "3757420",
    "end": "3763915"
  },
  {
    "text": "I guess one of the things to note here first is that, uh, we can actually cross out this term because X test is",
    "start": "3763915",
    "end": "3771340"
  },
  {
    "text": "conditionally independent of Phi given Y test or sorry. When you're not given Y test they are conditionally independent or",
    "start": "3771340",
    "end": "3778240"
  },
  {
    "text": "that- when you're not given Y test they're independent and so therefore we can, uh, we can ignore this term and just model, uh,",
    "start": "3778240",
    "end": "3784900"
  },
  {
    "text": "sample- try to sample from the function Phi given X train and Y train. Now if we try to actually derive what this corresponds to in this graphical model",
    "start": "3784900",
    "end": "3793330"
  },
  {
    "text": "we get an integral of the, um, of P of Theta times P of Phi I given",
    "start": "3793330",
    "end": "3801880"
  },
  {
    "text": "Theta times our data given Phi where we need to integrate out our prior Theta.",
    "start": "3801880",
    "end": "3809569"
  },
  {
    "text": "Okay. So unfortunately, uh, this is kind of the,",
    "start": "3810000",
    "end": "3815905"
  },
  {
    "text": "the exact solution for this distribution but of course its integral is completely intractable.",
    "start": "3815905",
    "end": "3822415"
  },
  {
    "text": "Uh, and so what we're gonna say I guess first as a side note is if we knew, uh.",
    "start": "3822415",
    "end": "3831775"
  },
  {
    "text": "So this integral is- is completely intractable and so we have to do something different.",
    "start": "3831775",
    "end": "3836799"
  },
  {
    "text": "One thing we're doing is if we knew this distribution, if we knew how to sample Phi given Theta X train and Y train,",
    "start": "3836800",
    "end": "3844810"
  },
  {
    "text": "then sampling becomes a lot easier. So we- if we knew this distribution, then we can just use ancestral sampling where we first sample",
    "start": "3844810",
    "end": "3851785"
  },
  {
    "text": "a Theta and then we sample from this distribution to be able to sample from Phi. Uh, and particularly the graphical model would look like this where we first sample Theta,",
    "start": "3851785",
    "end": "3860530"
  },
  {
    "text": "and then sample Phi given those three variables. Uh, so if we knew that distribution things would be easy,",
    "start": "3860530",
    "end": "3866905"
  },
  {
    "text": "how do we get that distribution? Well, what we can do is we can use, uh,",
    "start": "3866905",
    "end": "3873240"
  },
  {
    "text": "an approximation similar to what was shown in the paper by Grant et al which is",
    "start": "3873240",
    "end": "3879240"
  },
  {
    "text": "estimate this approximation crudely using- estimate this distribution crudely using a point estimate for Phi,",
    "start": "3879240",
    "end": "3887065"
  },
  {
    "text": "uh, and we gonna approximate this using MAP inference. Um, this is again extremely crude,",
    "start": "3887065",
    "end": "3894715"
  },
  {
    "text": "but it's also extremely convenient, uh, which is that if we can basically approximate this without MAP inference",
    "start": "3894715",
    "end": "3901090"
  },
  {
    "text": "corresponding to running a few steps of gradient descent, then, uh, unlike the previous paper we can still actually sample from Phi",
    "start": "3901090",
    "end": "3909880"
  },
  {
    "text": "where we first sample from Theta then we run a few steps of gradient descent, uh, and then get our model.",
    "start": "3909880",
    "end": "3917930"
  },
  {
    "text": "So this is what happens, uh, at test time where we want to do inference.",
    "start": "3919050",
    "end": "3924234"
  },
  {
    "text": "Training is a bit more complicated but it can be done with the tools that we mentioned before using Amortized Variational Inference.",
    "start": "3924235",
    "end": "3930309"
  },
  {
    "text": "Uh, So I'm not gonna go into how we do training, but if you're interested you can- you can see the paper.",
    "start": "3930310",
    "end": "3936625"
  },
  {
    "text": "Um, what ancestral sampling looks like is basically exactly what we've tried before. So first we sample, uh,",
    "start": "3936625",
    "end": "3942415"
  },
  {
    "text": "we have our, our, our distribution. We have new Theta. First we sample from P of Theta which has,",
    "start": "3942415",
    "end": "3950731"
  },
  {
    "text": "uh, a mean and a variance, this corresponds to starting from this mean-variance adding noise like",
    "start": "3950731",
    "end": "3956950"
  },
  {
    "text": "the re-parameterization trick and then we run gradient descent starting from that sample Theta,",
    "start": "3956950",
    "end": "3962620"
  },
  {
    "text": "uh, running gradient descent with respect to our theta. And that gets us into these types of- it",
    "start": "3962620",
    "end": "3968470"
  },
  {
    "text": "allows us to represent these types of multimodal distributions. Okay, uh, so the benefit of this approach is that it gives us a non-Gaussian posterior.",
    "start": "3968470",
    "end": "3977050"
  },
  {
    "text": "Uh, it's very simple at test time. We can just add noise and then run gradient descent.",
    "start": "3977050",
    "end": "3982900"
  },
  {
    "text": "Uh, and we only have one instance of the model. The downside is that it has a more complex training procedure and it",
    "start": "3982900",
    "end": "3992050"
  },
  {
    "text": "also has some some fairly crude approximations that are in the- uh, in the derivation.",
    "start": "3992050",
    "end": "3998960"
  },
  {
    "text": "Okay. So that's it for methods. Uh, to summarize we talked about, uh,",
    "start": "3999420",
    "end": "4006255"
  },
  {
    "text": "first a very simple method which offers a distribution over Y. The benefit of this is that simple,",
    "start": "4006255",
    "end": "4012540"
  },
  {
    "text": "you can combine it with a variety of methods. The downside is you can't reason about uncertainty about the underlying function,",
    "start": "4012540",
    "end": "4018180"
  },
  {
    "text": "you have a limited, uh, class of distributions over Y that you can represent, uh, and in practice these methods tend to",
    "start": "4018180",
    "end": "4024285"
  },
  {
    "text": "produce, uh, uncertainty aspects that are not very calibrated. Uh, we also talked about how we can use",
    "start": "4024285",
    "end": "4030240"
  },
  {
    "text": "Amortized Variational Inference with black box approaches where we could represent non-Gaussian distributions over Y, uh,",
    "start": "4030240",
    "end": "4036944"
  },
  {
    "text": "but only Gaussian distributions over Phi which is fine if Phi is a latent vector.",
    "start": "4036945",
    "end": "4042195"
  },
  {
    "text": "Uh, well mostly we talked about optimization based approaches which I've looked at using Amortized Variational Inference ensembles and the",
    "start": "4042195",
    "end": "4050279"
  },
  {
    "text": "sort of hybrid inference procedure that I mentioned, uh, the benefits of the amortized variational inference",
    "start": "4050280",
    "end": "4056370"
  },
  {
    "text": "was that it was simple but we need to model the distribution of our- of our parameters as Gaussian.",
    "start": "4056370",
    "end": "4062100"
  },
  {
    "text": "Ensembles allowed us to use non Gaussian distributions, uh, at the cost of maintaining M separate models or doing inference on the last layer.",
    "start": "4062100",
    "end": "4070815"
  },
  {
    "text": "Uh, and then the hybrid inference approaches allowed us to represent non-Gaussian posteriors with only a single model instance,",
    "start": "4070815",
    "end": "4077025"
  },
  {
    "text": "uh, with a more complex training procedure. Any questions on methods before I talk about",
    "start": "4077025",
    "end": "4083520"
  },
  {
    "text": "evaluation and what we can actually use these methods for? Yeah. [BACKGROUND]",
    "start": "4083520",
    "end": "4100424"
  },
  {
    "text": "Yeah, yeah. Exactly. So I guess, well, so this is also a, a downside of ensembles as well, is we can only sample.",
    "start": "4100425",
    "end": "4107670"
  },
  {
    "text": "We can't actually- we don't, um, we can't, uh, analytically represent things like entropy, um, likelihood.",
    "start": "4107670",
    "end": "4116295"
  },
  {
    "text": "It, it- it's much more difficult to represent those things. Um, we can only sample. But one thing you could do is you can sample and",
    "start": "4116295",
    "end": "4123270"
  },
  {
    "text": "then empirically estimate like what was the variance of my samples, um, and we'll actually see, in a couple of slides,",
    "start": "4123270",
    "end": "4130515"
  },
  {
    "text": "how these methods actually work if you wanna estimate that sort of uncertainty. Other questions? Okay. So how",
    "start": "4130515",
    "end": "4144810"
  },
  {
    "text": "do we evaluate these methods? Um, because these papers are relatively new,",
    "start": "4144810",
    "end": "4149895"
  },
  {
    "text": "I've- I figured the kind of the way to cover, this was just to describe a number of different ways for,",
    "start": "4149895",
    "end": "4155055"
  },
  {
    "text": "uh, for effectively evaluating these algorithms. Um, I guess the first question is well,",
    "start": "4155055",
    "end": "4161475"
  },
  {
    "text": "can we just use standard benchmarks? Can we use things like MinilmageNet to evaluate these Meta-learning algorithms?",
    "start": "4161475",
    "end": "4168270"
  },
  {
    "text": "Um, what do you think? What do you think are the kind of pros and cons of using standard benchmarks?",
    "start": "4168270",
    "end": "4174550"
  },
  {
    "text": "Yeah? [BACKGROUND]",
    "start": "4180580",
    "end": "4195250"
  },
  {
    "text": "Good, yeah. The pros is that we can- to repeat the question for people who are remote,",
    "start": "4195250",
    "end": "4201000"
  },
  {
    "text": "the pro is that it corresponds to something we care about the con is that it doesn't actually evaluate uncertainty. Any other pros and cons?",
    "start": "4201000",
    "end": "4208600"
  },
  {
    "text": "Okay. Most of the algorithms that have been made for this task are like [inaudible] this, this the task.",
    "start": "4217430",
    "end": "4226290"
  },
  {
    "text": "Yeah. [BACKGROUND] Yeah. So a lot of algorithms have already",
    "start": "4226290",
    "end": "4232140"
  },
  {
    "text": "evaluated on this and so it may be overfitting to these benchmarks. Uh, it may also be that they're- yeah, that the- yeah,",
    "start": "4232140",
    "end": "4239114"
  },
  {
    "text": "we're overfitting on these certainly. Anything else? Yeah?",
    "start": "4239115",
    "end": "4244860"
  },
  {
    "text": "[BACKGROUND]",
    "start": "4244860",
    "end": "4250500"
  },
  {
    "text": "Yeah. That's a good point. So in this example, in MinilmageNet, there may not be that much ambiguity in the underlying task.",
    "start": "4250500",
    "end": "4257114"
  },
  {
    "text": "Uh, it may actually be- there may be actually no ambiguity in, in the correct label given the dataset.",
    "start": "4257115",
    "end": "4263070"
  },
  {
    "text": "Okay. So, um, here's some of the things that I had done. So one pro is that they're standardized and so it",
    "start": "4263070",
    "end": "4268710"
  },
  {
    "text": "makes it a little bit easier to compare across, across papers. Although, in practice, some papers destandardize it by using different architectures, using different protocols.",
    "start": "4268710",
    "end": "4276585"
  },
  {
    "text": "Um, another benefits that has real images, it's kind of a, a very real problem. Um, and it's also a good check that your approach didn't break anything.",
    "start": "4276585",
    "end": "4284235"
  },
  {
    "text": "Uh, if the- if for example, you're protecting tanks on these benchmarks,",
    "start": "4284235",
    "end": "4290910"
  },
  {
    "text": "that means that something is probably wrong. Uh, the downsides that I had written down is that,",
    "start": "4290910",
    "end": "4297150"
  },
  {
    "text": "first metrics like accuracy don't actually evaluate, uh, things like uncertainty.",
    "start": "4297150",
    "end": "4302625"
  },
  {
    "text": "Uh, as was mentioned, the tasks don't- may not actually exhibit that ambiguity.",
    "start": "4302625",
    "end": "4309510"
  },
  {
    "text": "Uh, and lastly, uncertainty may not actually be that useful on this dataset.",
    "start": "4309510",
    "end": "4315059"
  },
  {
    "text": "Okay. So what are some better problems and, and better metrics that we can use? Uh, I guess we're benching- it kind of really",
    "start": "4315060",
    "end": "4322440"
  },
  {
    "text": "depends on the problem that you care about, uh, and how you measure, uh, how you measure the- how good these algorithms are and,",
    "start": "4322440",
    "end": "4329460"
  },
  {
    "text": "uh, which problems to test on. So I'll cover some, uh, and these can maybe inspire different applications.",
    "start": "4329460",
    "end": "4334560"
  },
  {
    "text": "Uh, so one very simple way to try to look at these, um- look at the performance of these algorithms is to look at varied toy examples.",
    "start": "4334560",
    "end": "4342615"
  },
  {
    "text": "So, uh, here's an example from, ah, some of our prior work,",
    "start": "4342615",
    "end": "4347760"
  },
  {
    "text": "although there's a number of other methods that have also looked at varied toy examples where you can actually visualize the underlying function that you're sampling from.",
    "start": "4347760",
    "end": "4354690"
  },
  {
    "text": "Uh, so the top example corresponds to, uh, a class of, uh, the underlying Meta-learning.",
    "start": "4354690",
    "end": "4359925"
  },
  {
    "text": "Functions are both sinusoids and linear functions where there's noise in the labels to make the task ambiguous.",
    "start": "4359925",
    "end": "4365355"
  },
  {
    "text": "And so you can see that it, uh, that it kind of- the types of functions that it's reasoning about, uh, in some situations, it's ambiguous even if it's a sinusoid function or a linear function.",
    "start": "4365355",
    "end": "4374100"
  },
  {
    "text": "And you see this sort of multimodal output from the sampling distribution, uh, and that's something- that's something that's nice to see because",
    "start": "4374100",
    "end": "4381210"
  },
  {
    "text": "it shows that the function can actually represent something that's multimodal. Uh, on the second example is a classification example where you're just given one data point.",
    "start": "4381210",
    "end": "4389475"
  },
  {
    "text": "All of the, the tasks correspond to circular decision boundaries. Uh, and you're trying to classify between positive points and negative points.",
    "start": "4389475",
    "end": "4397770"
  },
  {
    "text": "The D_train only contains one negative point- one positive point and D_test contains both positive and negative data points.",
    "start": "4397770",
    "end": "4403784"
  },
  {
    "text": "Uh, and so basically you get these different- you, you can see a visualization of the decision boundaries here shown in the- in the dashed lines,",
    "start": "4403785",
    "end": "4411105"
  },
  {
    "text": "where the function is- the neural network is representing these different decision boundaries that kind of represent the structure of the class of functions that it was trained on.",
    "start": "4411105",
    "end": "4419070"
  },
  {
    "text": "Um, so this is- these are somethings- things that are kind of cool to look at and, and nice to see, although at the same time it's",
    "start": "4419070",
    "end": "4425100"
  },
  {
    "text": "not- it's not a problem that we actually care about. Uh, so what are problems that we care about a bit more?",
    "start": "4425100",
    "end": "4432390"
  },
  {
    "text": "So, uh, Gordon et al looked at a task that corresponded to generating, uh, derivative models from a few examples.",
    "start": "4432390",
    "end": "4439755"
  },
  {
    "text": "And so, uh, there is- in this case, it was a one-shot learning problem, where the one data point is shown on the left,",
    "start": "4439755",
    "end": "4445320"
  },
  {
    "text": "and the goal is to generate examples, uh, to be able to build, build and generate new examples of that object instance.",
    "start": "4445320",
    "end": "4452625"
  },
  {
    "text": "Uh, and you can see that it's able to, um- as showed in the second row, it's able to generate,",
    "start": "4452625",
    "end": "4457935"
  },
  {
    "text": "uh, new, new instances. And they also looked at, uh, a quantitative evaluation.",
    "start": "4457935",
    "end": "4464325"
  },
  {
    "text": "So in this case, the qualitative evaluation, uh, allows you to actually judge the quality of the predictions to some extent.",
    "start": "4464325",
    "end": "4471690"
  },
  {
    "text": "Uh, and the quantitative evaluation allows you to look across the board. Um, unfortunately, these metrics mean squared error and SSIM,",
    "start": "4471690",
    "end": "4478199"
  },
  {
    "text": "uh, actually aren't great metrics for, ah, evaluating the kind of the predictive,",
    "start": "4478200",
    "end": "4485505"
  },
  {
    "text": "like whether or not this is actually covering the distribution well and accurately representing that distribution, uh, but it does at least give you",
    "start": "4485505",
    "end": "4491445"
  },
  {
    "text": "a quantitative measure of the performance on that dataset. Okay. Um, another thing that we can look at is things like likelihood, uh,",
    "start": "4491445",
    "end": "4500925"
  },
  {
    "text": "or we can look at a combination of accuracy as well as coverage of the correctness- of the distribution that you care about.",
    "start": "4500925",
    "end": "4507510"
  },
  {
    "text": "Um, and so this is something that, that we did, uh, motivated kind of by the example that I showed before,",
    "start": "4507510",
    "end": "4513495"
  },
  {
    "text": "where you're given, uh, a training dataset that, uh, corresponds to three attributes,",
    "start": "4513495",
    "end": "4518580"
  },
  {
    "text": "and then you have test, uh, test examples that you need to be able to label that it corres- that have only two of those,",
    "start": "4518580",
    "end": "4525945"
  },
  {
    "text": "um, two those attributes. And you can see that qualitatively, that's able to learn classifiers that classify",
    "start": "4525945",
    "end": "4532275"
  },
  {
    "text": "on only pairs of attributes rather than all three attributes. Uh, and then qualitatively,",
    "start": "4532275",
    "end": "4539460"
  },
  {
    "text": "we can see, uh, also look at this both accuracy and coverage. Uh, in this case, coverage was measured by,",
    "start": "4539460",
    "end": "4545775"
  },
  {
    "text": "does it cover the three possible modes of this distribution? Uh, and then lastly, also looking at average negative log-likelihood.",
    "start": "4545775",
    "end": "4553650"
  },
  {
    "text": "Uh, so what are the benefits of this? This is that, these tasks are actually ambiguous, and they're actually real tasks.",
    "start": "4553650",
    "end": "4559080"
  },
  {
    "text": "Um, the downside is that the, uh, is that this task may or may not look",
    "start": "4559080",
    "end": "4565500"
  },
  {
    "text": "like the task that you actually care about depending on your application. Okay. Um, another example that I thought was interesting is, uh,",
    "start": "4565500",
    "end": "4573989"
  },
  {
    "text": "this paper by Ravi and Beatson, they were looking at- they're trying to look at, how reliable are the uncertainty estimates that you get from different algorithms.",
    "start": "4573990",
    "end": "4580755"
  },
  {
    "text": "And so what they plotted is they plotted, uh, the confidence of the predictor, uh, versus the accuracy of the,",
    "start": "4580755",
    "end": "4589830"
  },
  {
    "text": "um, of the predictor. So on a set of examples, how well does its confidence correlate with its accuracy on those examples?",
    "start": "4589830",
    "end": "4597285"
  },
  {
    "text": "And in the ideal case, you get something that exactly follows this diagonal line. Uh, and, uh, this is the,",
    "start": "4597285",
    "end": "4605670"
  },
  {
    "text": "the- Ours is representing the amortize approach that I mentioned before. Uh, Probabilistic MAML was the third approach that I",
    "start": "4605670",
    "end": "4612540"
  },
  {
    "text": "mentioned that uses a hybrid inference, and then MAML is something that doesn't have- is, I guess basically corresponds to the first approach",
    "start": "4612540",
    "end": "4619170"
  },
  {
    "text": "where you just output a distribution over y. Um, and they looked at both these, uh, both these, these diagrams as well as looking at the,",
    "start": "4619170",
    "end": "4628244"
  },
  {
    "text": "um, the overall accuracy of these approaches. And I think it's important to keep in mind",
    "start": "4628245",
    "end": "4633900"
  },
  {
    "text": "both of these accuracies as well because it's, it's, it's possible to do a lot better on these diagrams while",
    "start": "4633900",
    "end": "4639630"
  },
  {
    "text": "doing a lot worse on accuracy, for example. And as you become more and more accurate,",
    "start": "4639630",
    "end": "4645780"
  },
  {
    "text": "it becomes harder to estimate the, uh, your confidence about the things that you're inaccurate about.",
    "start": "4645780",
    "end": "4651210"
  },
  {
    "text": "Okay, uh, and then the last example that I'd like to give is an active learning evaluation. So, um, there were actually two papers that did this, uh,",
    "start": "4651210",
    "end": "4659310"
  },
  {
    "text": "one on a varied toy regression domain as well as one on MinilmageNet. And in both experiments,",
    "start": "4659310",
    "end": "4665760"
  },
  {
    "text": "what the, uh, authors did is they looked at, can we sequentially choose data points that we want labels for?",
    "start": "4665760",
    "end": "4673965"
  },
  {
    "text": "And in particular, the way that you choose a data point that you wanna label for is the one that has the maximum predictive entropy,",
    "start": "4673965",
    "end": "4680009"
  },
  {
    "text": "uh, for that data point. Uh, and there are different ways to measure predictive entropy based on whether you're using, uh,",
    "start": "4680009",
    "end": "4686099"
  },
  {
    "text": "ensemble or whether or not you're using a, uh, just kind of providing samples from that distribution.",
    "start": "4686100",
    "end": "4692490"
  },
  {
    "text": "Um, but I guess what we see across the board is that, er, uh, the- these algorithms, uh,",
    "start": "4692490",
    "end": "4699105"
  },
  {
    "text": "Probabilistic MAML on left and, uh, the ensemble methods on the right are able to show an improvement by using either a diverse ensemble",
    "start": "4699105",
    "end": "4706665"
  },
  {
    "text": "or by representing uncertainty at all in comparison to just randomly sampling the data points with MAML.",
    "start": "4706665",
    "end": "4713170"
  },
  {
    "text": "Okay. Um, so this is- one of the nice things about this example is that something that we may actually care about in practice and so it's a,",
    "start": "4713600",
    "end": "4719895"
  },
  {
    "text": "it's a downstream application that's very relevant for being able to learn from fewer labels. In this case, we can easily get higher accuracy with fewer queries.",
    "start": "4719895",
    "end": "4727949"
  },
  {
    "text": "Um, the downside is that maybe, maybe active learning on sinusoid or",
    "start": "4727950",
    "end": "4733020"
  },
  {
    "text": "ImageNet is not the thing that we ultimately care about, we may wanna do active learning setting on a downstream application that we,",
    "start": "4733020",
    "end": "4738210"
  },
  {
    "text": "uh, where that amount of data is much more sparse. Okay, so, uh, to wrap up, uh,",
    "start": "4738210",
    "end": "4745230"
  },
  {
    "text": "we've now talked about how we can reason about uncertainty during learning, uh, and Meta-learning such that, uh,",
    "start": "4745230",
    "end": "4752309"
  },
  {
    "text": "Meta-learning is the way that those uncertainty estimates are something that are actually, uh, valid and correct.",
    "start": "4752310",
    "end": "4757830"
  },
  {
    "text": "Uh, next time on Wednesday, we'll see how we can look at Meta- learning for active learning as well as Meta-learning for unsupervised learning,",
    "start": "4757830",
    "end": "4765675"
  },
  {
    "text": "semi-supervised learning, and weakly-supervised learning. Which I think are actually, uh, some pretty cool ways to think about these types of problems.",
    "start": "4765675",
    "end": "4772095"
  },
  {
    "text": "Um, and then on Monday, we'll start talking about reinforcement learning. Great. Um, so some reminders.",
    "start": "4772095",
    "end": "4778335"
  },
  {
    "text": "Homework 2 is due next week, project proposal is due in two weeks and poster presentation date is fixed.",
    "start": "4778335",
    "end": "4783585"
  },
  {
    "text": "I'll see you all on Wednesday.",
    "start": "4783585",
    "end": "4786250"
  }
]