[
  {
    "start": "0",
    "end": "5970"
  },
  {
    "text": "Hi.",
    "start": "5970",
    "end": "6470"
  },
  {
    "text": "In this module, I'm going to be\ntalking about Markov networks.",
    "start": "6470",
    "end": "10150"
  },
  {
    "text": "So far, we've introduced\nconstraint satisfaction",
    "start": "10150",
    "end": "12670"
  },
  {
    "text": "problems, the first of\nour variable-based models.",
    "start": "12670",
    "end": "16209"
  },
  {
    "text": "Now we're going to talk\nabout Markov networks,",
    "start": "16210",
    "end": "18182"
  },
  {
    "text": "the second type of\nvariable-based models, which",
    "start": "18182",
    "end": "20140"
  },
  {
    "text": "will connect factor\ngraphs with probability.",
    "start": "20140",
    "end": "23349"
  },
  {
    "text": "And this will be a stepping\nstone along the way to",
    "start": "23350",
    "end": "26380"
  },
  {
    "text": "[INAUDIBLE].",
    "start": "26380",
    "end": "28390"
  },
  {
    "text": "So recall that\nvariable-based models are all",
    "start": "28390",
    "end": "31869"
  },
  {
    "text": "based on factor graphs,\nand Markov networks",
    "start": "31870",
    "end": "34630"
  },
  {
    "text": "are no different.",
    "start": "34630",
    "end": "36110"
  },
  {
    "text": "So remember that a factor graph\nconsists of a set of variables,",
    "start": "36110",
    "end": "40150"
  },
  {
    "text": "x1 through xn, and\na set of factors, f1",
    "start": "40150",
    "end": "43870"
  },
  {
    "text": "through fn, where each factor\ntakes a subset of the variables",
    "start": "43870",
    "end": "47590"
  },
  {
    "text": "and returns a non\nnegative number.",
    "start": "47590",
    "end": "50090"
  },
  {
    "text": "If you multiply all of\nthese numbers together,",
    "start": "50090",
    "end": "52550"
  },
  {
    "text": "you can evaluate the weight\nof a particular assignment.",
    "start": "52550",
    "end": "58809"
  },
  {
    "text": "So let's look at an\nexample of object tracking.",
    "start": "58810",
    "end": "61725"
  },
  {
    "text": "So here, remember the\ngoal is over time, record",
    "start": "61725",
    "end": "67170"
  },
  {
    "text": "the noisy sensor reading of\nan object's position at 0, 2,",
    "start": "67170",
    "end": "74159"
  },
  {
    "text": "and 2.",
    "start": "74160",
    "end": "75300"
  },
  {
    "text": "And the goal is to figure out\nwhat the actual trajectory",
    "start": "75300",
    "end": "77790"
  },
  {
    "text": "of this object is.",
    "start": "77790",
    "end": "79590"
  },
  {
    "text": "We modeled this\nas a factor graph",
    "start": "79590",
    "end": "81466"
  },
  {
    "text": "as follows where we have a\nnumber of factors representing",
    "start": "81467",
    "end": "86700"
  },
  {
    "text": "the affinity for x1 to\nbe close to 0 and x2",
    "start": "86700",
    "end": "91920"
  },
  {
    "text": "to be close to 2 and\nx3 to be close to 2",
    "start": "91920",
    "end": "95520"
  },
  {
    "text": "and also adjacent positions\nto be close to each other.",
    "start": "95520",
    "end": "102110"
  },
  {
    "text": "So before, we treated\nthis factor graph",
    "start": "102110",
    "end": "106670"
  },
  {
    "text": "as a constraint\nsatisfaction problem",
    "start": "106670",
    "end": "108350"
  },
  {
    "text": "where the goal is to find the\nmaximum weight assignment.",
    "start": "108350",
    "end": "112820"
  },
  {
    "text": "And in this\nparticular example, we",
    "start": "112820",
    "end": "115150"
  },
  {
    "text": "look at all the\npossible assignments.",
    "start": "115150",
    "end": "117910"
  },
  {
    "text": "Each assignment has a weight.",
    "start": "117910",
    "end": "119590"
  },
  {
    "text": "And you can find that\nthe maximum weight",
    "start": "119590",
    "end": "122710"
  },
  {
    "text": "assignment is 1, 2, 2.",
    "start": "122710",
    "end": "124360"
  },
  {
    "start": "124360",
    "end": "127000"
  },
  {
    "text": "But just returning a single\nmaximum weight assignment",
    "start": "127000",
    "end": "131350"
  },
  {
    "text": "doesn't really give\nus the full picture.",
    "start": "131350",
    "end": "133300"
  },
  {
    "text": "In particular, it doesn't\nrepresent how certain",
    "start": "133300",
    "end": "136510"
  },
  {
    "text": "we are of this assignment.",
    "start": "136510",
    "end": "138760"
  },
  {
    "text": "And what about all the\nother possibilities?",
    "start": "138760",
    "end": "143140"
  },
  {
    "text": "The goal of Markov\nnetworks is to try",
    "start": "143140",
    "end": "144870"
  },
  {
    "text": "to capture this uncertainty\nover assignments using",
    "start": "144870",
    "end": "147810"
  },
  {
    "text": "the language of probability.",
    "start": "147810",
    "end": "150420"
  },
  {
    "text": "So we've actually done\nmost of the hard work",
    "start": "150420",
    "end": "152360"
  },
  {
    "text": "already by setting\nup factor graphs.",
    "start": "152360",
    "end": "154400"
  },
  {
    "text": "The only remaining part is\nto connect factor graphs",
    "start": "154400",
    "end": "156980"
  },
  {
    "text": "with probability.",
    "start": "156980",
    "end": "158569"
  },
  {
    "text": "So formally, a Markov network,\nor a Markov random field",
    "start": "158570",
    "end": "161990"
  },
  {
    "text": "as it's sometimes\ncalled, is a factor graph",
    "start": "161990",
    "end": "164570"
  },
  {
    "text": "which defines a\njoint distribution",
    "start": "164570",
    "end": "167450"
  },
  {
    "text": "over a set of random\nvariables x1 through xn.",
    "start": "167450",
    "end": "171668"
  },
  {
    "text": "So before, these\nwere just variables,",
    "start": "171668",
    "end": "173210"
  },
  {
    "text": "and now they're random\nvariables because we're",
    "start": "173210",
    "end": "175127"
  },
  {
    "text": "talking about probabilities.",
    "start": "175127",
    "end": "177080"
  },
  {
    "text": "So remember, the factor\ngraph gives us a weight",
    "start": "177080",
    "end": "180560"
  },
  {
    "text": "for each possible assignment x.",
    "start": "180560",
    "end": "183709"
  },
  {
    "text": "And to convert this\nweight into a probability,",
    "start": "183710",
    "end": "185930"
  },
  {
    "text": "we just need to normalize it.",
    "start": "185930",
    "end": "188250"
  },
  {
    "text": "So what I mean by\nthat is I'm going",
    "start": "188250",
    "end": "190340"
  },
  {
    "text": "to look at the sum over\nall possible weights,",
    "start": "190340",
    "end": "196430"
  },
  {
    "text": "all possible assignments\nand their weights.",
    "start": "196430",
    "end": "199189"
  },
  {
    "text": "And I'm going to define z as\nthe sum of all the weights.",
    "start": "199190",
    "end": "204087"
  },
  {
    "text": "And that's going to be called\nthe normalization constant",
    "start": "204087",
    "end": "206420"
  },
  {
    "text": "or sometimes called\nthe partition function.",
    "start": "206420",
    "end": "208730"
  },
  {
    "text": "And then I'm just\ngoing to divide by z.",
    "start": "208730",
    "end": "211190"
  },
  {
    "text": "So this is going to produce\nsomething that sums to 1,",
    "start": "211190",
    "end": "216270"
  },
  {
    "text": "and I'm going to define that as\na joint distribution over big X",
    "start": "216270",
    "end": "220890"
  },
  {
    "text": "equals the [INAUDIBLE]\nat little x.",
    "start": "220890",
    "end": "224650"
  },
  {
    "text": "OK, so let's do\nthis example here.",
    "start": "224650",
    "end": "226269"
  },
  {
    "text": "We have x1, x2, x3,\nand a weight of x.",
    "start": "226270",
    "end": "232400"
  },
  {
    "text": "So for this, we have a bunch\nof eight possible, or not--",
    "start": "232400",
    "end": "236900"
  },
  {
    "text": "or six possible non-zero\nweight assignments",
    "start": "236900",
    "end": "239689"
  },
  {
    "text": "with particular weights.",
    "start": "239690",
    "end": "241970"
  },
  {
    "text": "We add all these weights up.",
    "start": "241970",
    "end": "243740"
  },
  {
    "text": "That gives us the partition\nfunction z, which is 26 here.",
    "start": "243740",
    "end": "248030"
  },
  {
    "text": "And then we divide each\nof these weights by 26",
    "start": "248030",
    "end": "251510"
  },
  {
    "text": "to produce the\njoint probability.",
    "start": "251510",
    "end": "256419"
  },
  {
    "text": "And so now this\nprobability distribution",
    "start": "256420",
    "end": "259000"
  },
  {
    "text": "represents the uncertainty\nin the problem.",
    "start": "259000",
    "end": "262760"
  },
  {
    "text": "And notice that while 122 was\nthe maximum weight assignment,",
    "start": "262760",
    "end": "267890"
  },
  {
    "text": "and it still is, this\nprobability gives us",
    "start": "267890",
    "end": "271280"
  },
  {
    "text": "a more nuanced picture,\nwhich is that we're only 31%",
    "start": "271280",
    "end": "274910"
  },
  {
    "text": "sure that that is actually the\ntrue trajectory of that object.",
    "start": "274910",
    "end": "279930"
  },
  {
    "text": "So could be useful information.",
    "start": "279930",
    "end": "281259"
  },
  {
    "text": "There's a big difference\nbetween 31% and 90%.",
    "start": "281260",
    "end": "283760"
  },
  {
    "start": "283760",
    "end": "286290"
  },
  {
    "text": "But wait, we can\ndo more than that.",
    "start": "286290",
    "end": "288700"
  },
  {
    "text": "So the language of probability\ncan allow us to ask or answer",
    "start": "288700",
    "end": "293910"
  },
  {
    "text": "or other questions\nbesides just probabilities",
    "start": "293910",
    "end": "296190"
  },
  {
    "text": "of all assignments.",
    "start": "296190",
    "end": "298360"
  },
  {
    "text": "For example, if\nwe wanted to know",
    "start": "298360",
    "end": "300969"
  },
  {
    "text": "where was an object\nat time step 2,",
    "start": "300970",
    "end": "303880"
  },
  {
    "text": "so that is what is the\nvalue of random variable x2?",
    "start": "303880",
    "end": "308920"
  },
  {
    "text": "And I don't care\nabout x1 and x3.",
    "start": "308920",
    "end": "311860"
  },
  {
    "text": "So this query is\ncaptured by a quantity",
    "start": "311860",
    "end": "314169"
  },
  {
    "text": "called marginal probability.",
    "start": "314170",
    "end": "316480"
  },
  {
    "text": "And the marginal probability\nof a particular random variable",
    "start": "316480",
    "end": "321400"
  },
  {
    "text": "equaling a particular\nvalue v is given by--",
    "start": "321400",
    "end": "325030"
  },
  {
    "text": "so we write this as\np of xi equals v.",
    "start": "325030",
    "end": "329180"
  },
  {
    "text": "And this is given\nby just summing",
    "start": "329180",
    "end": "331070"
  },
  {
    "text": "over all possible\nfull assignments",
    "start": "331070",
    "end": "334310"
  },
  {
    "text": "such that xi equals\nv. So all assignments",
    "start": "334310",
    "end": "338330"
  },
  {
    "text": "consistent with this condition\nof the joint distribution,",
    "start": "338330",
    "end": "342689"
  },
  {
    "text": "which we have defined\non the previous slide.",
    "start": "342690",
    "end": "347020"
  },
  {
    "text": "And now let's look at this\nobject tracking example again.",
    "start": "347020",
    "end": "349970"
  },
  {
    "text": "So we have our joint\nprobability table",
    "start": "349970",
    "end": "354280"
  },
  {
    "text": "that we computed on\nthe previous slide.",
    "start": "354280",
    "end": "356600"
  },
  {
    "text": "And now let's compute\nsome probabilities",
    "start": "356600",
    "end": "358720"
  },
  {
    "text": "of marginal probability.",
    "start": "358720",
    "end": "360170"
  },
  {
    "text": "So first, let's consider\nwhat is the probability of x2",
    "start": "360170",
    "end": "363220"
  },
  {
    "text": "equals 1 here?",
    "start": "363220",
    "end": "365280"
  },
  {
    "text": "But to do that,\nwe look over here",
    "start": "365280",
    "end": "367080"
  },
  {
    "text": "and look at all the\nrows where x2 equals 1.",
    "start": "367080",
    "end": "369900"
  },
  {
    "text": "So that's the first four here.",
    "start": "369900",
    "end": "372120"
  },
  {
    "text": "And then we just add\nup their probabilities.",
    "start": "372120",
    "end": "374380"
  },
  {
    "text": "So that's 0.15,\n0.15, 0.15, 0.15.",
    "start": "374380",
    "end": "377760"
  },
  {
    "text": "And that gives us 0.6.",
    "start": "377760",
    "end": "380310"
  },
  {
    "text": "And now we can issue another\nmarginal probability query.",
    "start": "380310",
    "end": "384690"
  },
  {
    "text": "What's the probability\nof x2 equals 2?",
    "start": "384690",
    "end": "387270"
  },
  {
    "text": "And look at all the\nrows where x2 is 2,",
    "start": "387270",
    "end": "390870"
  },
  {
    "text": "and these are these\nlast two rows.",
    "start": "390870",
    "end": "393090"
  },
  {
    "text": "And we add up these\nprobabilities,",
    "start": "393090",
    "end": "394919"
  },
  {
    "text": "and that gives us our point.",
    "start": "394920",
    "end": "397190"
  },
  {
    "text": "There are some\nrounding errors here,",
    "start": "397190",
    "end": "399280"
  },
  {
    "text": "why this doesn't add up exactly.",
    "start": "399280",
    "end": "402380"
  },
  {
    "text": "OK, so that allows us to\nanswer marginal probability.",
    "start": "402380",
    "end": "406600"
  },
  {
    "text": "So one thing you might know\nis that the answer here",
    "start": "406600",
    "end": "410520"
  },
  {
    "text": "is actually different than if\nyou just look at the max weight",
    "start": "410520",
    "end": "414180"
  },
  {
    "text": "assignment and the value of it.",
    "start": "414180",
    "end": "415880"
  },
  {
    "text": "In particular, if you look at\nthe maximum weight assignment,",
    "start": "415880",
    "end": "418380"
  },
  {
    "text": "it says the most likely\nthing is that x2 is 122.",
    "start": "418380",
    "end": "422550"
  },
  {
    "text": "And you look at x2\nof, and it says 2.",
    "start": "422550",
    "end": "426139"
  },
  {
    "text": "But notice that that\nis not the most likely",
    "start": "426140",
    "end": "430340"
  },
  {
    "text": "under the marginal probability.",
    "start": "430340",
    "end": "431690"
  },
  {
    "text": "The most likely value for x2\nunder marginal probability",
    "start": "431690",
    "end": "435310"
  },
  {
    "text": "is 1, and it has 62% chance\nof being a 1 in that case.",
    "start": "435310",
    "end": "442290"
  },
  {
    "text": "So the intuition here is that\nwhile this trajectory has",
    "start": "442290",
    "end": "449360"
  },
  {
    "text": "indeed the largest\nweight, there is actually",
    "start": "449360",
    "end": "452659"
  },
  {
    "text": "a lot of decentralized\nevidence for x2 equals 1",
    "start": "452660",
    "end": "456380"
  },
  {
    "text": "with these assignments,\nwhich have less weight.",
    "start": "456380",
    "end": "459735"
  },
  {
    "text": "So kind of strength in\nnumbers, if you add up",
    "start": "459735",
    "end": "463189"
  },
  {
    "text": "all these weights, they actually\noutnumber the evidence for x2",
    "start": "463190",
    "end": "468890"
  },
  {
    "text": "equals 2.",
    "start": "468890",
    "end": "470347"
  },
  {
    "text": "So this is kind of\nan important lesson",
    "start": "470347",
    "end": "471930"
  },
  {
    "text": "that what answer you're\ngoing to get really",
    "start": "471930",
    "end": "475979"
  },
  {
    "text": "depends on the type of\nquestion you're asking.",
    "start": "475980",
    "end": "479093"
  },
  {
    "text": "And in this case,\nif you're really",
    "start": "479093",
    "end": "480509"
  },
  {
    "text": "interested in the\nobject at timestep two,",
    "start": "480510",
    "end": "482700"
  },
  {
    "text": "then marginal probability is--",
    "start": "482700",
    "end": "486990"
  },
  {
    "text": "so let's look at a\nparticular example.",
    "start": "486990",
    "end": "489639"
  },
  {
    "text": "So the Ising model is a\nvery canonical example",
    "start": "489640",
    "end": "493290"
  },
  {
    "text": "that dates back to the 1920s\nfrom statistical physics.",
    "start": "493290",
    "end": "497940"
  },
  {
    "text": "And the ideal was that this\nis a model for ferromagnetism.",
    "start": "497940",
    "end": "504150"
  },
  {
    "text": "So the idea is that you have a\nMarkov network which contains",
    "start": "504150",
    "end": "510060"
  },
  {
    "text": "a bunch of different sites.",
    "start": "510060",
    "end": "513419"
  },
  {
    "text": "And each site is going\nto be denoted xi,",
    "start": "513419",
    "end": "516719"
  },
  {
    "text": "which can take on two values--",
    "start": "516720",
    "end": "518788"
  },
  {
    "text": "minus 1 and plus 1.",
    "start": "518789",
    "end": "520229"
  },
  {
    "text": "So minus 1 represents\na down spin",
    "start": "520230",
    "end": "522479"
  },
  {
    "text": "and plus 1 represents\nan up spin.",
    "start": "522480",
    "end": "527389"
  },
  {
    "text": "And furthermore,\nall these variables",
    "start": "527390",
    "end": "532760"
  },
  {
    "text": "are going to be\nrelated by a factor.",
    "start": "532760",
    "end": "536060"
  },
  {
    "text": "And we're going to\ncall this fij, which",
    "start": "536060",
    "end": "539840"
  },
  {
    "text": "connects site i and site j.",
    "start": "539840",
    "end": "542030"
  },
  {
    "text": "It's going to depend on the\nspin of i and the spin of xj.",
    "start": "542030",
    "end": "546770"
  },
  {
    "text": "And that's going to be equal\nto x of beta times xi xj.",
    "start": "546770",
    "end": "554630"
  },
  {
    "text": "OK, so intuition is that\nwe want neighboring sites",
    "start": "554630",
    "end": "557690"
  },
  {
    "text": "to have the same spin.",
    "start": "557690",
    "end": "560170"
  },
  {
    "text": "OK, so by multiplying these\ntogether, if both of them",
    "start": "560170",
    "end": "564329"
  },
  {
    "text": "have the same sign, then\nthis is going to be 1.",
    "start": "564330",
    "end": "566850"
  },
  {
    "text": "If they have opposite signs,\nthey're going to be negative 1.",
    "start": "566850",
    "end": "570029"
  },
  {
    "text": "And beta here is a\nscaling that says",
    "start": "570030",
    "end": "572310"
  },
  {
    "text": "how strong is the affinity.",
    "start": "572310",
    "end": "576360"
  },
  {
    "text": "If beta is 0, that means this\nis just x of 0, which is 1.",
    "start": "576360",
    "end": "582000"
  },
  {
    "text": "So that means there's\nno connection between.",
    "start": "582000",
    "end": "584130"
  },
  {
    "text": "And as beta increases, then\nthe affinity becomes stronger.",
    "start": "584130",
    "end": "588630"
  },
  {
    "text": "The difference between them\nagreeing and not agreeing",
    "start": "588630",
    "end": "593160"
  },
  {
    "text": "becomes i tended.",
    "start": "593160",
    "end": "596480"
  },
  {
    "text": "So one thing Ising\nmodels are useful for is",
    "start": "596480",
    "end": "599949"
  },
  {
    "text": "to study phase transitions\nin [INAUDIBLE] systems.",
    "start": "599950",
    "end": "603260"
  },
  {
    "text": "So here is an example of what\nhappens when a beta increases.",
    "start": "603260",
    "end": "608600"
  },
  {
    "text": "So if beta is close\nto 0, then you're",
    "start": "608600",
    "end": "614569"
  },
  {
    "text": "basically going to get\nunstructured systems where",
    "start": "614570",
    "end": "618710"
  },
  {
    "text": "each side is just\nbehaving independently.",
    "start": "618710",
    "end": "621170"
  },
  {
    "text": "In fact, if beta is 0, then all\nassignments are equally likely.",
    "start": "621170",
    "end": "625370"
  },
  {
    "text": "And as beta\nincreases, you'll see",
    "start": "625370",
    "end": "627650"
  },
  {
    "text": "that more and more\ncoherence happens",
    "start": "627650",
    "end": "630200"
  },
  {
    "text": "where neighboring sites really\nlike to be close to each other.",
    "start": "630200",
    "end": "633930"
  },
  {
    "text": "But of course, there's going\nto be some kind of sharp ridges",
    "start": "633930",
    "end": "636620"
  },
  {
    "text": "where two neighbors\nhave to disagree.",
    "start": "636620",
    "end": "642510"
  },
  {
    "text": "So how we're going to\nsample from this model",
    "start": "642510",
    "end": "646500"
  },
  {
    "text": "is going to be a topic\nfor another module.",
    "start": "646500",
    "end": "651280"
  },
  {
    "text": "So here is another canonical\napplication of Markov networks",
    "start": "651280",
    "end": "655050"
  },
  {
    "text": "from computer vision.",
    "start": "655050",
    "end": "656170"
  },
  {
    "text": "So this used to be very\npopular before deep learning.",
    "start": "656170",
    "end": "660420"
  },
  {
    "text": "So the idea is that\nyou take a noisy image,",
    "start": "660420",
    "end": "662760"
  },
  {
    "text": "and you want to denoise\nit into a clean image.",
    "start": "662760",
    "end": "666550"
  },
  {
    "text": "So we're going to present a\nvery stylized, simple example",
    "start": "666550",
    "end": "670260"
  },
  {
    "text": "of this.",
    "start": "670260",
    "end": "671710"
  },
  {
    "text": "So here is our 3 by 5 image.",
    "start": "671710",
    "end": "674980"
  },
  {
    "text": "So each site is a pixel,\nand we assume that we only--",
    "start": "674980",
    "end": "681149"
  },
  {
    "text": "so xi is either 0 or 1,\nwhich is a pixel value, which",
    "start": "681150",
    "end": "685800"
  },
  {
    "text": "is unknown for modeling\nthe clean image.",
    "start": "685800",
    "end": "688500"
  },
  {
    "text": "And we assume that only a subset\nof the pixels are observed.",
    "start": "688500",
    "end": "691960"
  },
  {
    "text": "Or maybe we observe this one,\nthis one, this one, this one,",
    "start": "691960",
    "end": "694890"
  },
  {
    "text": "this one.",
    "start": "694890",
    "end": "695730"
  },
  {
    "text": "And the goal is to fill\nin the rest of the pixels.",
    "start": "695730",
    "end": "700050"
  },
  {
    "text": "So we can capture this\nobservation by an observation",
    "start": "700050",
    "end": "705149"
  },
  {
    "text": "potential of oi xi, which\nis 1 if xi agrees with",
    "start": "705150",
    "end": "711270"
  },
  {
    "text": "the observation and\n0 if it doesn't.",
    "start": "711270",
    "end": "714155"
  },
  {
    "text": "So this is a hard\nconstraint that",
    "start": "714155",
    "end": "715530"
  },
  {
    "text": "says where I observed a value,\nxi must take on that value.",
    "start": "715530",
    "end": "720790"
  },
  {
    "text": "So this one has to be 0, this\none has to be 1, and so on.",
    "start": "720790",
    "end": "725180"
  },
  {
    "text": "And finally, we have\ntransition factors",
    "start": "725180",
    "end": "729020"
  },
  {
    "text": "that say neighboring pixels\nare more likely to be",
    "start": "729020",
    "end": "731180"
  },
  {
    "text": "the same than different.",
    "start": "731180",
    "end": "732870"
  },
  {
    "text": "So again, the same intuition\nis that Ising model,",
    "start": "732870",
    "end": "736670"
  },
  {
    "text": "and we're going to\ndenote this as tij.",
    "start": "736670",
    "end": "739760"
  },
  {
    "text": "And this equals is 2 if two\nneighboring pixels agree",
    "start": "739760",
    "end": "747650"
  },
  {
    "text": "and is going to be 1 If they--",
    "start": "747650",
    "end": "750290"
  },
  {
    "start": "750290",
    "end": "755209"
  },
  {
    "text": "so let me summarize.",
    "start": "755210",
    "end": "757010"
  },
  {
    "text": "Markov networks, you\ncan think about it",
    "start": "757010",
    "end": "759020"
  },
  {
    "text": "as simply as taking\nfactor graphs",
    "start": "759020",
    "end": "761300"
  },
  {
    "text": "and marrying them\nwith probability.",
    "start": "761300",
    "end": "765430"
  },
  {
    "text": "So again, factor graphs already\nhave done a lot of the work.",
    "start": "765430",
    "end": "769740"
  },
  {
    "text": "They allow already you to\nspecify non-negative weight",
    "start": "769740",
    "end": "773700"
  },
  {
    "text": "for every assignment.",
    "start": "773700",
    "end": "775590"
  },
  {
    "text": "And all we have\nto do is normalize",
    "start": "775590",
    "end": "777780"
  },
  {
    "text": "that to get a\nprobability distribution.",
    "start": "777780",
    "end": "781433"
  },
  {
    "text": "And once we have the\nprobability distribution,",
    "start": "781433",
    "end": "783350"
  },
  {
    "text": "we can answer all\nsorts of queries.",
    "start": "783350",
    "end": "785449"
  },
  {
    "text": "Now we're computing\nmarginal probabilities,",
    "start": "785450",
    "end": "787680"
  },
  {
    "text": "which allows us to pinpoint\nindividual variables",
    "start": "787680",
    "end": "790520"
  },
  {
    "text": "and ask questions about them.",
    "start": "790520",
    "end": "793420"
  },
  {
    "text": "So it is useful comparing\nMarkov networks with CSP.",
    "start": "793420",
    "end": "797829"
  },
  {
    "text": "So CSPs, we talked about\nvariables who are known,",
    "start": "797830",
    "end": "800980"
  },
  {
    "text": "unknown.",
    "start": "800980",
    "end": "801790"
  },
  {
    "text": "And Markov networks, we\ncall them random variables.",
    "start": "801790",
    "end": "807550"
  },
  {
    "text": "They behave like variables,\nbut they're random variables",
    "start": "807550",
    "end": "810310"
  },
  {
    "text": "because we're endowing them with\na probabilistic interpretation.",
    "start": "810310",
    "end": "815440"
  },
  {
    "text": "And CSPs, we talked\nabout weights.",
    "start": "815440",
    "end": "817780"
  },
  {
    "text": "Markov networks, we talk\nabout probabilities, which",
    "start": "817780",
    "end": "820930"
  },
  {
    "text": "are the normalized weights.",
    "start": "820930",
    "end": "822820"
  },
  {
    "text": "And the main difference\nis that in CSPs we",
    "start": "822820",
    "end": "825190"
  },
  {
    "text": "were trying to find the\nmaximum weight assignment.",
    "start": "825190",
    "end": "828590"
  },
  {
    "text": "And in Markov\nnetworks, we're looking",
    "start": "828590",
    "end": "831110"
  },
  {
    "text": "at the distribution over\nassignments holistically",
    "start": "831110",
    "end": "835160"
  },
  {
    "text": "and answering questions about\nmarginal probabilities, which",
    "start": "835160",
    "end": "838699"
  },
  {
    "text": "gives us a more nuanced idea of\nthe set of possible components.",
    "start": "838700",
    "end": "844670"
  },
  {
    "text": "OK, that's it for this module.",
    "start": "844670",
    "end": "847149"
  },
  {
    "start": "847150",
    "end": "852000"
  }
]