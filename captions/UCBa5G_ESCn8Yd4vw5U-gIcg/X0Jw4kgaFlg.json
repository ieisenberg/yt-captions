[
  {
    "start": "0",
    "end": "40000"
  },
  {
    "start": "0",
    "end": "5140"
  },
  {
    "text": "Hi, everyone. I'll get started. OK, so we're now back to\nthe second week of CS224N",
    "start": "5140",
    "end": "13900"
  },
  {
    "text": "on Natural Language\nProcessing with Deep Learning. OK, so for today's lecture what\nwe're going to be looking at",
    "start": "13900",
    "end": "20950"
  },
  {
    "text": "is all the math details of\ndoing neural net learning.",
    "start": "20950",
    "end": "26560"
  },
  {
    "text": "First of all, looking\nat how we can work out by hand gradients for\ntraining neural networks,",
    "start": "26560",
    "end": "33610"
  },
  {
    "text": "and then looking\nat how it's done more algorithmically, which is\nknown as the back propagation algorithm.",
    "start": "33610",
    "end": "40260"
  },
  {
    "start": "40000",
    "end": "291000"
  },
  {
    "text": "And correspondingly,\nby you guys, well, I hope you remember\nthat one minute ago",
    "start": "40260",
    "end": "46980"
  },
  {
    "text": "was when assignment one was due\nand everyone has handed that in. If by some chance you\nhaven't handed it in,",
    "start": "46980",
    "end": "54450"
  },
  {
    "text": "really should hand it\nin as soon as possible. Best to preserve those late\ndays for the harder assignments.",
    "start": "54450",
    "end": "59850"
  },
  {
    "text": "So I mean, I actually forgot\nto mention we actually did make one change\nfor this year",
    "start": "59850",
    "end": "65610"
  },
  {
    "text": "to make it a bit easier\nwhen occasionally people joined the class a week late. If you want to this\nyear in the grading,",
    "start": "65610",
    "end": "73260"
  },
  {
    "text": "assignment one can be\ndiscounted and we'll just use your other four assignments.",
    "start": "73260",
    "end": "78310"
  },
  {
    "text": "But if you've been in the class\nso far, for that 98% of people, well, since assignment one\nis the easiest assignment,",
    "start": "78310",
    "end": "85799"
  },
  {
    "text": "again, it's silly not\nto do it and have it as part of your grade. OK, so starting today\nwe've put out assignment 2.",
    "start": "85800",
    "end": "94920"
  },
  {
    "text": "And assignment 2 is all\nabout making sure you really understand the math\nof neural networks,",
    "start": "94920",
    "end": "100860"
  },
  {
    "text": "and then the software that\nwe use to do that math. So this is going to be a bit\nof a tough week for some.",
    "start": "100860",
    "end": "109090"
  },
  {
    "text": "So for some people\nwho are great on all their math and\nbackgrounds, now feel",
    "start": "109090",
    "end": "114690"
  },
  {
    "text": "like this is stuff they know\nwell, nothing very difficult. But I know there are\nquite a few of you who",
    "start": "114690",
    "end": "122130"
  },
  {
    "text": "this lecture and week is the\nbiggest struggle of the course. We really do want\npeople to actually",
    "start": "122130",
    "end": "128910"
  },
  {
    "text": "have an understanding of what\ngoes on in neural network learning rather than viewing\nit as some kind of deep magic.",
    "start": "128910",
    "end": "136209"
  },
  {
    "text": "And I hope that some of\nthe material we give today and that you read up on and use\nin the assignment will really",
    "start": "136210",
    "end": "143010"
  },
  {
    "text": "give you more of a sense of\nwhat these neural networks are doing, and how it\nis just math that's",
    "start": "143010",
    "end": "149700"
  },
  {
    "text": "applied in a systematic\nlarge scale that works out the answers, and\nthat this will be",
    "start": "149700",
    "end": "155190"
  },
  {
    "text": "valuable in giving you a deeper\nsense of what's going on. But if this material seems\nvery scary and difficult,",
    "start": "155190",
    "end": "164610"
  },
  {
    "text": "you can take some\nrefuge in the fact that there's first light\nat the end of the tunnel since this is really\nthe only lecture that's",
    "start": "164610",
    "end": "172260"
  },
  {
    "text": "heavily going through the math\ndetails of neural networks. After that, we'll be kind of\npopping back up to a higher",
    "start": "172260",
    "end": "179220"
  },
  {
    "text": "level and by and\nlarge, after this week we'll be making use of\nsoftware to do a lot",
    "start": "179220",
    "end": "185910"
  },
  {
    "text": "of the complicated math for us. But nevertheless, I\nhope this is valuable.",
    "start": "185910",
    "end": "192120"
  },
  {
    "text": "I'll go through\neverything quickly today, but if this isn't stuff\nthat you know backwards,",
    "start": "192120",
    "end": "197820"
  },
  {
    "text": "I really do encourage\nyou to work through it and get help as you need it.",
    "start": "197820",
    "end": "203860"
  },
  {
    "text": "So do come along to\nour office hours. There are also a number of\npieces of tutorial material",
    "start": "203860",
    "end": "210180"
  },
  {
    "text": "given in the syllabus. So there's both\nthe lecture notes, there's some\nmaterials from CS231.",
    "start": "210180",
    "end": "217110"
  },
  {
    "text": "In the list of readings,\nthe very top reading is some material put together\nby Kevin Clark a couple of years",
    "start": "217110",
    "end": "224730"
  },
  {
    "text": "ago, and actually that\none's my favorite. The presentation there\nfairly closely follows",
    "start": "224730",
    "end": "231150"
  },
  {
    "text": "the presentation in\nthis lecture of going through matrix calculus. So personally, I'd recommend\nstarting with that one",
    "start": "231150",
    "end": "238319"
  },
  {
    "text": "but there are four\ndifferent ones you can choose from if one of\nthem seems more helpful to you.",
    "start": "238320",
    "end": "245220"
  },
  {
    "text": "Two other things on\nwhat's coming up. Actually for our Thursday's\nlecture, we make a big change.",
    "start": "245220",
    "end": "252210"
  },
  {
    "text": "And Thursday's\nlecture is probably the most linguistic\nlecture of the whole class where we go through the\ndetails of dependency",
    "start": "252210",
    "end": "258630"
  },
  {
    "text": "grammar and dependency parsing. Some people find that\ntough as well, but at least it will be tough\nin a different way.",
    "start": "258630",
    "end": "265200"
  },
  {
    "text": "And then one other really good\nopportunity is this Friday, we have our second\ntutorial at 10:00 AM, which",
    "start": "265200",
    "end": "272340"
  },
  {
    "text": "is an introduction to PyTorch,\nwhich is the deep learning framework that we'll be using\nfor the rest of the class",
    "start": "272340",
    "end": "279150"
  },
  {
    "text": "once we've gone through\nthese first two assignments where you do things by yourself.",
    "start": "279150",
    "end": "284292"
  },
  {
    "text": "So this is a great chance\nto get an intro to PyTorch, it will be really useful\nfor later in the class.",
    "start": "284292",
    "end": "290890"
  },
  {
    "text": "OK, today's material\nis really all about sort of the math\nof neural networks,",
    "start": "290890",
    "end": "298690"
  },
  {
    "start": "291000",
    "end": "496000"
  },
  {
    "text": "but just to introduce a setting\nwhere we can work through this, I'm going to\nintroduce a simple NLP",
    "start": "298690",
    "end": "306040"
  },
  {
    "text": "task and a simple\nform of classifier that we can use for it. So the task of named\nentity recognition",
    "start": "306040",
    "end": "312639"
  },
  {
    "text": "is a very common basic NLP task. And the goal of this is you're\nlooking through pieces of text",
    "start": "312640",
    "end": "319960"
  },
  {
    "text": "and you're wanting to label\nby labeling the words which words belong to entity\ncategories like persons,",
    "start": "319960",
    "end": "326650"
  },
  {
    "text": "locations, products,\ndates, times, et cetera. So for this piece\nof text, last night",
    "start": "326650",
    "end": "333370"
  },
  {
    "text": "Paris Hilton wowed\nin a sequin gown. Samuel Quinn was arrested\nin the Hilton Hotel in Paris",
    "start": "333370",
    "end": "339580"
  },
  {
    "text": "in April 1989. The-- some words\nare being labeled as named entities as shown.",
    "start": "339580",
    "end": "346610"
  },
  {
    "text": "These two sentences\ndon't actually belong together in\nthe same article but I chose those two sentences\nto illustrate the basic point",
    "start": "346610",
    "end": "355150"
  },
  {
    "text": "that it's not that you can\njust do this task by using a dictionary. Yes, a dictionary is helpful\nto know that Paris can possibly",
    "start": "355150",
    "end": "363220"
  },
  {
    "text": "be a location, but Paris\ncan also be a person name. So you have to\nuse context to get",
    "start": "363220",
    "end": "370060"
  },
  {
    "text": "named entity recognition right.  OK, well, how might we do\nthat with a neural network?",
    "start": "370060",
    "end": "378949"
  },
  {
    "text": "There are much more\nadvanced ways of doing this. But a simple yet\nalready pretty good",
    "start": "378950",
    "end": "385130"
  },
  {
    "text": "way of doing named\nentity recognition with a simple neural\nnet is to say, well,",
    "start": "385130",
    "end": "393300"
  },
  {
    "text": "what we're going to do\nis use the word vectors that we've learned\nabout and we're",
    "start": "393300",
    "end": "398540"
  },
  {
    "text": "going to build up a context\nwindow of word vectors.",
    "start": "398540",
    "end": "403670"
  },
  {
    "text": "And then we're going to put\nthose through a neural network layer and then feed it through\na softmax classifier of the kind",
    "start": "403670",
    "end": "412130"
  },
  {
    "text": "that we, sorry, I\nsaid that wrong, and then we're going to feed it\nthrough a logistic classifier",
    "start": "412130",
    "end": "418400"
  },
  {
    "text": "of the kind that\nwe saw when looking at negative sampling,\nwhich is going to say for a particular\nentity type, such as location,",
    "start": "418400",
    "end": "427460"
  },
  {
    "text": "is that high\nprobability location or is that not a high\nprobability location?",
    "start": "427460",
    "end": "433340"
  },
  {
    "text": "So for a sentence like\nthe museums in Paris are amazing to see, what we're\ngoing to do is for each word,",
    "start": "433340",
    "end": "440120"
  },
  {
    "text": "say we're doing the\nword Paris, we're going to form a\nwindow around that, say a plus or minus\n2 word window.",
    "start": "440120",
    "end": "446810"
  },
  {
    "text": "And so for those\nfive words, we're going to get word\nvectors for them from the kind of Word2Vec\nor GLoVe word vectors",
    "start": "446810",
    "end": "455210"
  },
  {
    "text": "we've learned. And we're going to\nmake a long vector out of the concatenation of\nthose five word vectors",
    "start": "455210",
    "end": "461729"
  },
  {
    "text": "so the word of interest\nis in the middle. And then we're going\nto feed this vector",
    "start": "461730",
    "end": "468080"
  },
  {
    "text": "to a classifier\nwhich is at the end going to have a probability\nof the word being a location.",
    "start": "468080",
    "end": "475460"
  },
  {
    "text": "And then we could have\nanother classifier that says the probability of\nthe word being a person name.",
    "start": "475460",
    "end": "480643"
  },
  {
    "text": "And so once we've\ndone that, we're then going to run it\nat the next position. So we then say, well, is\nthe word \"a\" a location?",
    "start": "480643",
    "end": "487430"
  },
  {
    "text": "And we'd feed a\nwindow of five words, it's then, in Paris\nare amazing to, and put it through the\nsame kind of classifier.",
    "start": "487430",
    "end": "496030"
  },
  {
    "start": "496000",
    "end": "825000"
  },
  {
    "text": "And so this is the\nclassifier that we'll use. So its input will\nbe this word window.",
    "start": "496030",
    "end": "503090"
  },
  {
    "text": "So if we have D\ndimensional word vectors, this will be a 5D vector. And then we're going to\nput it through a layer",
    "start": "503090",
    "end": "510300"
  },
  {
    "text": "of a neural network. So the layer of\nthe neural network is going to multiply\nthis vector by a matrix,",
    "start": "510300",
    "end": "519000"
  },
  {
    "text": "add on a bias vector,\nand then put that through a non-linearity such\nas the softmax transformation",
    "start": "519000",
    "end": "529340"
  },
  {
    "text": "that we've seen before. And that will give\nus a hidden vector which might be of a smaller\ndimensionality such as this one",
    "start": "529340",
    "end": "537260"
  },
  {
    "text": "here. And so then with\nthat hidden vector,",
    "start": "537260",
    "end": "542540"
  },
  {
    "text": "we're then going to\ntake the dot product of that with an extra\nvector here, here it's u.",
    "start": "542540",
    "end": "550110"
  },
  {
    "text": "So we take u dot product h. And so when we do that, we're\ngetting out a single number.",
    "start": "550110",
    "end": "557180"
  },
  {
    "text": "And that number can\nbe any real number. And so then finally, we're\ngoing to put that number",
    "start": "557180",
    "end": "563600"
  },
  {
    "text": "through a logistic\ntransform of the same kind that we saw when doing\nnegative sampling.",
    "start": "563600",
    "end": "571400"
  },
  {
    "text": "The logistic transform\nwill take any real number, and it'll transform\nit into a probability",
    "start": "571400",
    "end": "578240"
  },
  {
    "text": "that that word is the location. So its output is the\npredicted probability",
    "start": "578240",
    "end": "584270"
  },
  {
    "text": "of the word belonging\nto a particular class. And so this could be\nour location classifier",
    "start": "584270",
    "end": "590510"
  },
  {
    "text": "which could classify\neach word in a window as to what the probability is\nthat it's the location word.",
    "start": "590510",
    "end": "597860"
  },
  {
    "text": "And so this little\nneural network here is the neural network\nI'm going to use today when",
    "start": "597860",
    "end": "604100"
  },
  {
    "text": "going through some of the math. But actually, I'm going to\nmake it even easier on myself.",
    "start": "604100",
    "end": "609230"
  },
  {
    "text": "I'm going to throw away the\nlogistic function at the top,",
    "start": "609230",
    "end": "614959"
  },
  {
    "text": "and I'm really\njust going to work through the math of the\nbottom three-quarters of this. If you look at Kevin Clark's\nhandout that I just mentioned,",
    "start": "614960",
    "end": "624500"
  },
  {
    "text": "he includes when he\nworks through it, also working through the\nlogistic function.",
    "start": "624500",
    "end": "630450"
  },
  {
    "text": "And we also saw working through\na softmax in the first lecture when I was working through\nsome of the Word2Vec model.",
    "start": "630450",
    "end": "638460"
  },
  {
    "text": "OK, so the overall question\nwe want to be able to answer",
    "start": "638460",
    "end": "644040"
  },
  {
    "text": "is, so here's our stochastic\ngradient descent equation, that we have existing\nparameters of our model,",
    "start": "644040",
    "end": "653700"
  },
  {
    "text": "and we want to update them\nbased on our current loss, which",
    "start": "653700",
    "end": "660240"
  },
  {
    "text": "is the j of theta. So for getting our loss\nhere, that the true answer",
    "start": "660240",
    "end": "667260"
  },
  {
    "text": "as to whether a word\nis a location or not will be 1 if it is a location\nor 0 if it isn't, our logistic",
    "start": "667260",
    "end": "675420"
  },
  {
    "text": "classifier will return\nsome number like 0.9, and we'll use the distance\naway from what it should",
    "start": "675420",
    "end": "681660"
  },
  {
    "text": "have been squared as our loss. So we work out a loss and then\nwe're moving a little distance",
    "start": "681660",
    "end": "689339"
  },
  {
    "text": "in the negative of\nthe gradient, which will be changing our parameter\nestimates in such a way",
    "start": "689340",
    "end": "696300"
  },
  {
    "text": "that they reduce the loss. And so this is\nalready being written in terms of a whole\nvector of parameters,",
    "start": "696300",
    "end": "703960"
  },
  {
    "text": "which is being updated as to\na new vector of parameters. But you can also think about it,\nfor each individual parameter,",
    "start": "703960",
    "end": "712440"
  },
  {
    "text": "theta J, that we're working\nout the partial derivative of the loss with respect\nto that parameter.",
    "start": "712440",
    "end": "720270"
  },
  {
    "text": "And then we're\nmoving a little bit in the negative\ndirection of that.",
    "start": "720270",
    "end": "726220"
  },
  {
    "text": "That's going to give us a new\nvalue for parameter theta J.",
    "start": "726220",
    "end": "731860"
  },
  {
    "text": "And we're going to update all\nof the parameters of our model as we learn.",
    "start": "731860",
    "end": "737079"
  },
  {
    "text": "I mean, in particular,\nin contrast to what commonly happens\nin statistics, we also--",
    "start": "737080",
    "end": "744250"
  },
  {
    "text": "we update not only the sort\nof parameters of our model that are sort of weights\nin the classifier,",
    "start": "744250",
    "end": "751360"
  },
  {
    "text": "but we also will update\nour data representation. So we'll also be changing\nour word vectors as we learn.",
    "start": "751360",
    "end": "758770"
  },
  {
    "text": "OK, so to build\nneural nets, i.e., to train neural\nnets based on data,",
    "start": "758770",
    "end": "764050"
  },
  {
    "text": "what we need is to\nbe able to compute this gradient of the parameters\nso that we can then iteratively",
    "start": "764050",
    "end": "772030"
  },
  {
    "text": "update the weights of the\nmodel and efficiently train the model that has\ngood weights, i.e.",
    "start": "772030",
    "end": "778540"
  },
  {
    "text": "that has high accuracy. And so how can we do that?",
    "start": "778540",
    "end": "783710"
  },
  {
    "text": "Well, what I'm going\nto talk about today is, first of all, how\nyou can do it by hand.",
    "start": "783710",
    "end": "791660"
  },
  {
    "text": "And so for doing it by\nhand, this is basically a review of matrix\ncalculus, and that will take",
    "start": "791660",
    "end": "799930"
  },
  {
    "text": "quite a bit of the lecture. And then after we've talked\nabout that for a while,",
    "start": "799930",
    "end": "806740"
  },
  {
    "text": "I'll then shift gears and\nintroduce the back propagation algorithm, which is the central\ntechnology for neural networks.",
    "start": "806740",
    "end": "815380"
  },
  {
    "text": "And that technology\nis essentially the efficient application\nof calculus on a large scale",
    "start": "815380",
    "end": "822579"
  },
  {
    "text": "as we'll come to\ntalking about soon. So for computing gradients\nby hand, what we're doing",
    "start": "822580",
    "end": "829840"
  },
  {
    "start": "825000",
    "end": "1153000"
  },
  {
    "text": "is matrix calculus. So we're working with\nvectors and matrices",
    "start": "829840",
    "end": "835840"
  },
  {
    "text": "and working out gradients. And this can seem like\npretty scary stuff.",
    "start": "835840",
    "end": "844120"
  },
  {
    "text": "And well, to the extent\nthat you're kind of scared and don't know what's\ngoing on, one choice",
    "start": "844120",
    "end": "851860"
  },
  {
    "text": "is to work out a non-vectorized\ngradient by just working out",
    "start": "851860",
    "end": "857680"
  },
  {
    "text": "what the partial derivative\nis for one parameter at a time and I showed a little example\nof that in the first lecture.",
    "start": "857680",
    "end": "866470"
  },
  {
    "text": "But it's much, much\nfaster and more useful to actually be able to work\nwith vectorized gradients.",
    "start": "866470",
    "end": "877210"
  },
  {
    "text": "And in some sense, if\nyou're not very confident, this is kind of almost\na leap of faith,",
    "start": "877210",
    "end": "883540"
  },
  {
    "text": "but it really is the case that\nmultivariable calculus is just like single variable\ncalculus except you're",
    "start": "883540",
    "end": "890769"
  },
  {
    "text": "using vectors and matrices. So providing you\nremember some basics of single variable\ncalculus, you really",
    "start": "890770",
    "end": "898300"
  },
  {
    "text": "should be able to do this\nstuff and get it to work out. Lots of other sources are\nmentioned in the notes.",
    "start": "898300",
    "end": "907070"
  },
  {
    "text": "You can also look at\nthe textbook for Math 51 which also has quite a\nlot of material on this.",
    "start": "907070",
    "end": "914100"
  },
  {
    "text": "I know some of you have\nbad memories of Math 51. OK, so let's go\nthrough this and see",
    "start": "914100",
    "end": "919769"
  },
  {
    "text": "how it works ramping\nup from the beginning. So the beginning of calculus\nis, we have a function",
    "start": "919770",
    "end": "926850"
  },
  {
    "text": "with one input and one output. f(x) equals x cubed. And so then its gradient\nis its slope, right?",
    "start": "926850",
    "end": "934830"
  },
  {
    "text": "So its derivative. So its derivative is 3x squared.",
    "start": "934830",
    "end": "940300"
  },
  {
    "text": "And the way to\nthink about this is how much will the output\nchange if we change",
    "start": "940300",
    "end": "946470"
  },
  {
    "text": "the input a little bit, right? So what we're wanting to\ndo in our neural net models",
    "start": "946470",
    "end": "951540"
  },
  {
    "text": "is change what they output\nso that they do a better job of predicting the\ncorrect answers when we're",
    "start": "951540",
    "end": "958470"
  },
  {
    "text": "doing supervised learning. And so what we want to\nknow is if we fiddle different parameters of the\nmodel, how much of an effect",
    "start": "958470",
    "end": "966090"
  },
  {
    "text": "will that have on the output? Because then we can choose how\nto fiddle them in the right way to move things down, right?",
    "start": "966090",
    "end": "972880"
  },
  {
    "text": "So when we're saying that the\nderivative here is 3x squared,",
    "start": "972880",
    "end": "977920"
  },
  {
    "text": "well, what we're saying is\nthat if you're at x equals 1,",
    "start": "977920",
    "end": "983339"
  },
  {
    "text": "if you fiddle the\ninput a little bit, the output will change 3 times\nas much, 3 times 1 squared,",
    "start": "983340",
    "end": "990959"
  },
  {
    "text": "and it does. So if I say what's the value\nat 1.01, it's about 1.03.",
    "start": "990960",
    "end": "996420"
  },
  {
    "text": "It's changed three times as\nmuch, and that's its slope. But at x equals 4, the\nderivative is 16 times 348.",
    "start": "996420",
    "end": "1007370"
  },
  {
    "text": "So if we fiddle\nthe input a little, it will change 48 times\nas much, and that's roughly what happens,\n4.01 cubed is 64.48.",
    "start": "1007370",
    "end": "1016820"
  },
  {
    "text": "Now, of course,\nthis is just sort of showing it for a small fiddle\nbut that's an approximation",
    "start": "1016820",
    "end": "1023839"
  },
  {
    "text": "to the actual truth. Okay, so then we sort of ramp\nup to the more complex cases,",
    "start": "1023840",
    "end": "1030990"
  },
  {
    "text": "which are more\nreflective of what we do with neural networks. So if we have a function\nwith one output and n inputs,",
    "start": "1030990",
    "end": "1039309"
  },
  {
    "text": "then we have a gradient. So a gradient is a vector\nof partial derivatives",
    "start": "1039310",
    "end": "1044459"
  },
  {
    "text": "with respect to each input. So we've got n inputs x1 to\nxn, and we're working out",
    "start": "1044460",
    "end": "1049620"
  },
  {
    "text": "the partial derivative\nof f with respect to x1, the partial derivative\nof f with respect to x2,",
    "start": "1049620",
    "end": "1055649"
  },
  {
    "text": "et cetera, and we then get a\nvector of partial derivatives",
    "start": "1055650",
    "end": "1061620"
  },
  {
    "text": "where each element\nof this vector is just like a simple derivative\nwith respect to one variable.",
    "start": "1061620",
    "end": "1069360"
  },
  {
    "text": "OK, so from that\npoint, we just keep on ramping up for what we\ndo with neural networks.",
    "start": "1069360",
    "end": "1076030"
  },
  {
    "text": "So commonly, when\nwe have something like layer in a\nneural network, we'll",
    "start": "1076030",
    "end": "1081150"
  },
  {
    "text": "have a function\nwithin n inputs that will be like our\nword vectors, then",
    "start": "1081150",
    "end": "1086310"
  },
  {
    "text": "we do something like\nmultiply by a matrix, and then we'll have m outputs.",
    "start": "1086310",
    "end": "1092640"
  },
  {
    "text": "So we have a function now\nwhich is taking n inputs and is producing m outputs.",
    "start": "1092640",
    "end": "1098970"
  },
  {
    "text": "So at this point, what\nwe're calculating for the gradient is what's\ncalled a Jacobian matrix.",
    "start": "1098970",
    "end": "1107070"
  },
  {
    "text": "So for m inputs and n\noutputs, the Jacobian is an m by n matrix\nof every combination",
    "start": "1107070",
    "end": "1115200"
  },
  {
    "text": "of partial derivatives. So function f splits up into\nthese different sub functions",
    "start": "1115200",
    "end": "1123600"
  },
  {
    "text": "f1 through fm, which generate\neach of the m outputs.",
    "start": "1123600",
    "end": "1128770"
  },
  {
    "text": "And so then we're taking\nthe partial derivative of f1 with respect to x1 through\nthe partial derivative of f1",
    "start": "1128770",
    "end": "1135870"
  },
  {
    "text": "with respect to xn,\nthen heading down, we can make it up to the partial\nderivative of fm with respect",
    "start": "1135870",
    "end": "1142350"
  },
  {
    "text": "to x1, et cetera. So we have every possible\npartial derivative of an output\nvariable with respect",
    "start": "1142350",
    "end": "1149460"
  },
  {
    "text": "to one of the input variables. OK, so in simple calculus,\nwhen you have a composition",
    "start": "1149460",
    "end": "1160340"
  },
  {
    "text": "of one variable functions. So that if you have y equals x\nsquared, and then z equals 3y,",
    "start": "1160340",
    "end": "1169760"
  },
  {
    "text": "that's-- then z is a composition\nof two functions of--",
    "start": "1169760",
    "end": "1175760"
  },
  {
    "text": "or you're composing\ntwo functions, to get z as a function of x. Then you can work out\nthe derivative of z",
    "start": "1175760",
    "end": "1182240"
  },
  {
    "text": "with respect to x. And the way you do that\nis with the chain rule. And so in the chain rule,\nyou multiply derivatives.",
    "start": "1182240",
    "end": "1189530"
  },
  {
    "text": "So dz dx equals\ndz dy times dy dx.",
    "start": "1189530",
    "end": "1194630"
  },
  {
    "text": "So dz dy is just\n3, and dy dx is 2x.",
    "start": "1194630",
    "end": "1202040"
  },
  {
    "text": "So we get 3 times 2x so that\noverall, the derivative here",
    "start": "1202040",
    "end": "1208910"
  },
  {
    "text": "is 6x. And since if we\nmultiply this together, we're really saying that\nz equals 3x squared.",
    "start": "1208910",
    "end": "1217250"
  },
  {
    "text": "You should trivially\nbe able to see again. Aha, its derivative is 6x.",
    "start": "1217250",
    "end": "1222350"
  },
  {
    "text": "So that works. OK, so once we move into vectors\nand matrices and Jacobians,",
    "start": "1222350",
    "end": "1230090"
  },
  {
    "text": "it's actually the same game. So when we're\nworking with those, we can compose functions and\nwork out their derivatives",
    "start": "1230090",
    "end": "1237470"
  },
  {
    "text": "by simply multiplying Jacobians. So if we start with an\ninput x and then put it",
    "start": "1237470",
    "end": "1244429"
  },
  {
    "text": "through the simplest form\nof neural network layer and say that z equals Wx plus b.",
    "start": "1244430",
    "end": "1251660"
  },
  {
    "text": "So we multiply the\nx vector by matrix W and then add on a bias vector b.",
    "start": "1251660",
    "end": "1257090"
  },
  {
    "text": "And then typically,\nwe put things through a non-linearity f. So f could be a\nsigmoid function.",
    "start": "1257090",
    "end": "1263720"
  },
  {
    "text": "We'll then say h equals f(z). So this is the composition\nof two functions in terms",
    "start": "1263720",
    "end": "1269990"
  },
  {
    "text": "of vectors and matrices. So we can use\nJacobians and we can",
    "start": "1269990",
    "end": "1275179"
  },
  {
    "text": "say the partial\nof h with respect to x is going to be the\nproduct of the partial h",
    "start": "1275180",
    "end": "1283049"
  },
  {
    "text": "with respect to z, and the\npartial z with respect to x. And this all does work out.",
    "start": "1283050",
    "end": "1289380"
  },
  {
    "start": "1289000",
    "end": "1434000"
  },
  {
    "text": "So let's start going\nthrough some examples of how these things work\nslightly more concretely.",
    "start": "1289380",
    "end": "1297920"
  },
  {
    "text": "First, just particular\nJacobians and then composing them together.",
    "start": "1297920",
    "end": "1304290"
  },
  {
    "text": "So one case we look at\nis the nonlinearities that we put a vector\nthrough, so this",
    "start": "1304290",
    "end": "1311150"
  },
  {
    "text": "is something like\nputting a vector through the sigmoid function f.",
    "start": "1311150",
    "end": "1317120"
  },
  {
    "text": "And so if we have an\nintermediate vector z and we turn into vector h by putting\nit through a logistic function,",
    "start": "1317120",
    "end": "1325770"
  },
  {
    "text": "we can say what is dh / dz?. ",
    "start": "1325770",
    "end": "1331980"
  },
  {
    "text": "Well, for this, formally\nthis is a function that",
    "start": "1331980",
    "end": "1338130"
  },
  {
    "text": "has n inputs and n outputs. So at the end of the day, we're\ncomputing an n by n Jacobian.",
    "start": "1338130",
    "end": "1347310"
  },
  {
    "text": "And so what that's meaning is\nthe elements of this n by n",
    "start": "1347310",
    "end": "1352890"
  },
  {
    "text": "Jacobian are going to take\nthe partial derivative of each output with\nrespect to each input.",
    "start": "1352890",
    "end": "1362610"
  },
  {
    "text": "And well, what is that\ngoing to be in this case? Well, in this case,\nbecause we are actually",
    "start": "1362610",
    "end": "1369090"
  },
  {
    "text": "just computing element\nwise transformation,",
    "start": "1369090",
    "end": "1374440"
  },
  {
    "text": "such as a logistic\ntransform of each element zi like the second equation\nhere, if i equals j,",
    "start": "1374440",
    "end": "1384000"
  },
  {
    "text": "we've got something to compute. Whereas, if i doesn't\nequal J, the input",
    "start": "1384000",
    "end": "1390870"
  },
  {
    "text": "has no influence on the output\nand so the derivative is 0. So if i doesn't equal j\nwe're going to get a 0,",
    "start": "1390870",
    "end": "1398700"
  },
  {
    "text": "and if i does equal\nj, well, then we're going to get the regular\none variable derivative",
    "start": "1398700",
    "end": "1405450"
  },
  {
    "text": "of the logistic function,\nwhich if I remember correctly,",
    "start": "1405450",
    "end": "1411169"
  },
  {
    "text": "you were asked to compute-- now I can't remember\nwhether it's assignment, 1 or assignment 2, but one of the\ntwo asked you to compute it.",
    "start": "1411170",
    "end": "1418790"
  },
  {
    "text": "So our Jacobian for this\ncase looks like this. We have a diagonal matrix with\nthe derivatives of each element",
    "start": "1418790",
    "end": "1430075"
  },
  {
    "text": "element along the diagonal\nand everything else is 0. OK, so let's look at a\ncouple of other Jacobians.",
    "start": "1430075",
    "end": "1438840"
  },
  {
    "start": "1434000",
    "end": "1594000"
  },
  {
    "text": "So if we are asking, if we've\ngot this Wx plus b basic neural",
    "start": "1438840",
    "end": "1444330"
  },
  {
    "text": "network layer and we're asking\nfor the gradient with respect to x, then what are we\ngoing to have coming out",
    "start": "1444330",
    "end": "1454080"
  },
  {
    "text": "is that that's actually\ngoing to be the matrix W. So this is where-- what I hope you can do is\nlook at the notes at home",
    "start": "1454080",
    "end": "1464160"
  },
  {
    "text": "and work through this exactly,\nand see that this is actually",
    "start": "1464160",
    "end": "1470010"
  },
  {
    "text": "the right answer. But this is the way in which if\nyou just have faith and think,",
    "start": "1470010",
    "end": "1476700"
  },
  {
    "text": "this is just like\nsingle variable calculus except I've now got vectors\nand matrices, the answer you",
    "start": "1476700",
    "end": "1484140"
  },
  {
    "text": "get is actually what\nyou expected to get. Because this is just like\nthe derivative of ax plus b",
    "start": "1484140",
    "end": "1491280"
  },
  {
    "text": "with respect to x where it's a. So similarly, if we take the\npartial derivative with respect",
    "start": "1491280",
    "end": "1497370"
  },
  {
    "text": "to b of Wx plus b we get\nout the identity matrix.",
    "start": "1497370",
    "end": "1505380"
  },
  {
    "text": "OK, then one other Jacobian that\nwe mentioned while in the first lecture while working through\nWord2Vec is if you have the dot",
    "start": "1505380",
    "end": "1514710"
  },
  {
    "text": "product of two vectors,\ni.e., that's a number,",
    "start": "1514710",
    "end": "1522360"
  },
  {
    "text": "that what you get\ncoming out of it-- so the partial derivative\nof uTh with respect to u",
    "start": "1522360",
    "end": "1531240"
  },
  {
    "text": "is h transpose. And at this point,\nthere's some fine print",
    "start": "1531240",
    "end": "1536430"
  },
  {
    "text": "that I'm going to come\nback to in a minute. So this is the correct\nJacobian, right?",
    "start": "1536430",
    "end": "1543299"
  },
  {
    "text": "Because in this case, we have\nthe dimension of h inputs",
    "start": "1543300",
    "end": "1550620"
  },
  {
    "text": "and we have one output, and so\nwe want to have a row vector.",
    "start": "1550620",
    "end": "1557460"
  },
  {
    "text": "But there's a little\nbit more to say on that that I'll come back\nto in about 20 slides,",
    "start": "1557460",
    "end": "1563280"
  },
  {
    "text": "but this is the\ncorrect Jacobian. OK, so if you are not familiar\nwith these kind of Jacobians,",
    "start": "1563280",
    "end": "1570470"
  },
  {
    "text": "do please look at some of\nthe notes that are available",
    "start": "1570470",
    "end": "1575960"
  },
  {
    "text": "and try and compute these\nin more detail element wise and convince yourself that\nthey really are right.",
    "start": "1575960",
    "end": "1582500"
  },
  {
    "text": "But I'm going to assume\nthese now and show you what happens when we\nactually then work out",
    "start": "1582500",
    "end": "1588470"
  },
  {
    "text": "gradients for at least a\nmini little neural net. ",
    "start": "1588470",
    "end": "1595470"
  },
  {
    "start": "1594000",
    "end": "1769000"
  },
  {
    "text": "OK, so here is most\nof this neural net.",
    "start": "1595470",
    "end": "1602320"
  },
  {
    "text": "I mean, as I commented\nthat really we'd",
    "start": "1602320",
    "end": "1607330"
  },
  {
    "text": "be working out the partial\nderivative of the loss j with respect to these\nvariables, but for the example",
    "start": "1607330",
    "end": "1614350"
  },
  {
    "text": "I'm doing here, I just-- I've locked that off to keep\nit a little simpler and more manageable for the lecture.",
    "start": "1614350",
    "end": "1620440"
  },
  {
    "text": "And so we're going\nto just work out the partial derivative\nof the score s, which",
    "start": "1620440",
    "end": "1625629"
  },
  {
    "text": "is a real number with respect\nto the different parameters of this model where the\nparameters of this model",
    "start": "1625630",
    "end": "1632980"
  },
  {
    "text": "are going to be the W\nand the b and the u,",
    "start": "1632980",
    "end": "1639309"
  },
  {
    "text": "and also the input because\nwe can update the weight vectors of the word\nvectors of different words",
    "start": "1639310",
    "end": "1648490"
  },
  {
    "text": "based on tuning them to better\npredict the classification outputs that we desire.",
    "start": "1648490",
    "end": "1655130"
  },
  {
    "text": "So let's start off\nwith a fairly easy one where we want to update\nthe bias vector b to have",
    "start": "1655130",
    "end": "1662740"
  },
  {
    "text": "our system classify better. So to be able to do that,\nwhat we want to work out",
    "start": "1662740",
    "end": "1668470"
  },
  {
    "text": "is the partial derivatives\nof s with respect to b, so we know how to put that\ninto our stochastic gradient",
    "start": "1668470",
    "end": "1676570"
  },
  {
    "text": "update for the b parameters. OK, so how do we go\nabout doing these things?",
    "start": "1676570",
    "end": "1683720"
  },
  {
    "text": "So the first step is we\nwant to sort of break things up into different functions\nof minimal complexity",
    "start": "1683720",
    "end": "1692860"
  },
  {
    "text": "that compose together. So in particular,\nthis neural net layer h equals f of Wx plus b, it's\nstill a little bit complex.",
    "start": "1692860",
    "end": "1701480"
  },
  {
    "text": "So let's decompose\nthat one further step. So we have the input\nx, we then calculate",
    "start": "1701480",
    "end": "1710559"
  },
  {
    "text": "the linear transformation\nz equals Wx plus b",
    "start": "1710560",
    "end": "1715600"
  },
  {
    "text": "and then we put things\nthrough the sort of element",
    "start": "1715600",
    "end": "1720820"
  },
  {
    "text": "wise non-linearity h\nequals f(z) and then we",
    "start": "1720820",
    "end": "1726070"
  },
  {
    "text": "do the dot product with u. And it's useful for\nworking these things out",
    "start": "1726070",
    "end": "1733860"
  },
  {
    "text": "to split into pieces like\nthis, have straight what your different variables\nare, and to know",
    "start": "1733860",
    "end": "1740680"
  },
  {
    "text": "what the dimensionality of\neach of these variables is. It's well worth just writing\nout the dimensionality",
    "start": "1740680",
    "end": "1747280"
  },
  {
    "text": "of every variable\nand making sure that the answers\nthat you're computing are of the right dimensionality.",
    "start": "1747280",
    "end": "1754070"
  },
  {
    "text": "So at this point though, what\nwe can see is that calculating s is the product of three--",
    "start": "1754070",
    "end": "1762490"
  },
  {
    "text": "I'm sorry, it's the composition\nof three functions around x.",
    "start": "1762490",
    "end": "1768010"
  },
  {
    "text": "So for working out the partials\nof s with respect to b,",
    "start": "1768010",
    "end": "1774340"
  },
  {
    "text": "it's the composition\nof the three functions shown on the left. And so therefore, the gradient\nof x with respect to b we're",
    "start": "1774340",
    "end": "1784179"
  },
  {
    "text": "going to take the\nproduct of these three",
    "start": "1784180",
    "end": "1790420"
  },
  {
    "text": "partial derivatives. OK, so how do-- what do we--",
    "start": "1790420",
    "end": "1798290"
  },
  {
    "text": "so we've got the s equals uTh. So that's sort of the\ntop corresponding partial",
    "start": "1798290",
    "end": "1805220"
  },
  {
    "text": "derivative, partial\nderivative h with respect to z, partial derivative\nof z with respect",
    "start": "1805220",
    "end": "1812059"
  },
  {
    "text": "to b, which is the first\none that we're working out. OK, so we want to work this out.",
    "start": "1812060",
    "end": "1818790"
  },
  {
    "text": "And if we're lucky, we\nremember those Jacobians I showed previously about\nthe Jacobian for a vector dot",
    "start": "1818790",
    "end": "1826559"
  },
  {
    "text": "product, the Jacobian\nfor the non-linearity and the Jacobian for the\nsimple linear transformation.",
    "start": "1826560",
    "end": "1836370"
  },
  {
    "text": "And so we can use those. So for the partials of s\nwith respect to h, well,",
    "start": "1836370",
    "end": "1845450"
  },
  {
    "text": "that's going to be uT\nusing the first one. The partials of h\nwith respect to z, OK,",
    "start": "1845450",
    "end": "1852530"
  },
  {
    "text": "so that's the\nnon-linearity, and so that's going to be the matrix\nthat's the diagonal matrix",
    "start": "1852530",
    "end": "1859760"
  },
  {
    "text": "with the element-wise derivative\nf prime of z and 0 elsewhere.",
    "start": "1859760",
    "end": "1865280"
  },
  {
    "text": "And then for the Wx\nplus b, when we're taking the partials with respect\nto b, that's just the identity",
    "start": "1865280",
    "end": "1874520"
  },
  {
    "text": "matrix. So we can simplify\nthat down a little, the identity matrix disappears\nand since uT is a vector",
    "start": "1874520",
    "end": "1887760"
  },
  {
    "text": "and this is a\ndiagonal matrix, we can rewrite this as uT Hadamard\nproduct of f prime of z.",
    "start": "1887760",
    "end": "1896009"
  },
  {
    "text": "I think this is\nthe first time I've used this little circle\nfor Hadamard product",
    "start": "1896010",
    "end": "1901230"
  },
  {
    "text": "but it's something\nthat you'll see quite a bit in neural network\nwork since it's often used.",
    "start": "1901230",
    "end": "1908140"
  },
  {
    "text": "So when we have two vectors uT\nand this vector here, sometimes",
    "start": "1908140",
    "end": "1916420"
  },
  {
    "text": "you want to do an\nelement wise product. So the output of\nthis will be a vector where you've taken the\nfirst element of each",
    "start": "1916420",
    "end": "1923410"
  },
  {
    "text": "and multiplied them, the\nsecond element of each and multiplied them,\net cetera downwards. And so that's called\nthe Hadamard product",
    "start": "1923410",
    "end": "1930460"
  },
  {
    "text": "and that's what\nwe're calculating as to calculate a vector,\nwhich is the gradient of s",
    "start": "1930460",
    "end": "1939390"
  },
  {
    "text": "with respect to b. OK, so that's good.",
    "start": "1939390",
    "end": "1945850"
  },
  {
    "text": "So we now have a gradient\nof s with respect to b and we could use that\nin our stochastic gradient.",
    "start": "1945850",
    "end": "1953170"
  },
  {
    "text": "But we don't stop\nthere, we also want to work out the\ngradient with respect",
    "start": "1953170",
    "end": "1960250"
  },
  {
    "text": "to others of our parameters. So we might want to next go on\nand work out the gradients of s",
    "start": "1960250",
    "end": "1968080"
  },
  {
    "text": "with respect to w. Well, we can use the chain rule\njust like we did before, right?",
    "start": "1968080",
    "end": "1977110"
  },
  {
    "text": "So we've got the same product\nof functions and everything is going to be the\nsame apart from me now",
    "start": "1977110",
    "end": "1983559"
  },
  {
    "text": "taking the derivatives with\nrespect to w rather than b.",
    "start": "1983560",
    "end": "1989470"
  },
  {
    "text": "So it's now going to be the\npartial of s with respect to h,",
    "start": "1989470",
    "end": "1994809"
  },
  {
    "text": "h with respect to z,\nand z with respect to w. And the important thing to\nnotice here and this leads",
    "start": "1994810",
    "end": "2003930"
  },
  {
    "text": "into what we do with the\nback propagation algorithm is wait a minute, this is very\nsimilar to what we've already",
    "start": "2003930",
    "end": "2013470"
  },
  {
    "text": "done. So when we are working\nout the gradients of s with respect to b, the first\ntwo terms were exactly the same,",
    "start": "2013470",
    "end": "2022170"
  },
  {
    "text": "it's only the last\none that differs. So to be able to\nbuild or to train",
    "start": "2022170",
    "end": "2030690"
  },
  {
    "text": "neural networks\nefficiently, this is what happens all the time\nand it's absolutely essential",
    "start": "2030690",
    "end": "2037320"
  },
  {
    "text": "that we use an algorithm that\navoids repeated computation.",
    "start": "2037320",
    "end": "2042720"
  },
  {
    "text": "And so the idea we're\ngoing to develop is when we have\nthis equation stack",
    "start": "2042720",
    "end": "2048839"
  },
  {
    "text": "that this sort of stuff that's\nabove where we compute z, and we're going to be sort of,\nthat'll be the same each time",
    "start": "2048840",
    "end": "2057359"
  },
  {
    "text": "and we want to compute something\nfrom that that we can then sort of feed downwards\nwhen working out",
    "start": "2057360",
    "end": "2064800"
  },
  {
    "text": "the gradients with\nrespect to Wx or b.",
    "start": "2064800",
    "end": "2069989"
  },
  {
    "text": "And so we do that\nby defining delta, which is delta is the\npartials composed that",
    "start": "2069989",
    "end": "2078779"
  },
  {
    "text": "are above the linear transform. And that's referred\nto as the local error",
    "start": "2078780",
    "end": "2084810"
  },
  {
    "text": "signal, that's\nwhat's being passed in from above to the\nlinear transform.",
    "start": "2084810",
    "end": "2090270"
  },
  {
    "text": "And we've already computed\nthe gradient of that in the preceding slides.",
    "start": "2090270",
    "end": "2096960"
  },
  {
    "text": "And so the final form of the\npartial with respect to b",
    "start": "2096960",
    "end": "2102300"
  },
  {
    "text": "will be delta times\nthe remaining part.",
    "start": "2102300",
    "end": "2109270"
  },
  {
    "text": "And well, we've seen\nthat for partial of s with respect to b,\nthe partial of z",
    "start": "2109270",
    "end": "2116109"
  },
  {
    "text": "with respect to b is\njust the identity, so the end result was delta. But in this time,\nwe're then going",
    "start": "2116110",
    "end": "2122560"
  },
  {
    "text": "to have to work out the\npartial of z with respect to w and multiply that by delta.",
    "start": "2122560",
    "end": "2128650"
  },
  {
    "text": "So that's the part that\nwe still haven't yet done. So and this is where things\nget in some sense, a little bit",
    "start": "2128650",
    "end": "2139930"
  },
  {
    "start": "2132000",
    "end": "2226000"
  },
  {
    "text": "hairier, and so\nthere's something that's important to explain.",
    "start": "2139930",
    "end": "2146700"
  },
  {
    "text": "So what should we have for\nthe Jacobian of ds / dW?",
    "start": "2146700",
    "end": "2154359"
  },
  {
    "text": "Well, that's a function\nthat has one output,",
    "start": "2154360",
    "end": "2161050"
  },
  {
    "text": "the output is just a\nscore of real number, and then it has n by m inputs.",
    "start": "2161050",
    "end": "2167020"
  },
  {
    "text": "So that Jacobian is a 1\nby n by m matrix, i.e.",
    "start": "2167020",
    "end": "2175240"
  },
  {
    "text": "a very long low vector\nbut that's correct math.",
    "start": "2175240",
    "end": "2181660"
  },
  {
    "text": "But it turns out\nthat that's kind of bad for our neural networks.",
    "start": "2181660",
    "end": "2186700"
  },
  {
    "text": "Because remember, what we want\nto do with our neural networks is do stochastic\ngradient descent.",
    "start": "2186700",
    "end": "2192200"
  },
  {
    "text": "And we want to say theta\nnew equals theta old minus a small multiplier\ntimes the gradient.",
    "start": "2192200",
    "end": "2201640"
  },
  {
    "text": "And well, actually the W\nmatrix is an n by m matrix,",
    "start": "2201640",
    "end": "2213049"
  },
  {
    "text": "and so we couldn't\nactually do the subtraction if this gradient we calculate\nis just a huge row vector.",
    "start": "2213050",
    "end": "2220900"
  },
  {
    "text": "We'd like to have it as the\nsame shape as the W matrix.",
    "start": "2220900",
    "end": "2226420"
  },
  {
    "start": "2226000",
    "end": "2535000"
  },
  {
    "text": "In neural network\nland when we do this, we depart from pure\nmath at this point",
    "start": "2226420",
    "end": "2232539"
  },
  {
    "text": "and we use what we call\nthe shape convention. So what we're going\nto say is and you're",
    "start": "2232540",
    "end": "2239530"
  },
  {
    "text": "meant to use this for\nanswers in the assignment, that the shape of\nthe gradient we're always going to make to be\nthe shape of the parameters.",
    "start": "2239530",
    "end": "2248650"
  },
  {
    "text": "And so therefore,\nds / dW, we are also",
    "start": "2248650",
    "end": "2254020"
  },
  {
    "text": "going to represent as an\nn by m matrix just like W, and we're going to reshape\nthe Jacobian to place it",
    "start": "2254020",
    "end": "2263559"
  },
  {
    "text": "into this matrix shape. OK, so if we want to place it\ninto this matrix shape, what",
    "start": "2263560",
    "end": "2275270"
  },
  {
    "text": "are we going to want\nto get for ds / dW? Well, we know that it's\ngoing to involve delta,",
    "start": "2275270",
    "end": "2286309"
  },
  {
    "text": "our local error signal and\nthen we have to work out",
    "start": "2286310",
    "end": "2295000"
  },
  {
    "text": "something for dz / dW.  Well, since z equals\nWx plus b, you'd",
    "start": "2295000",
    "end": "2303520"
  },
  {
    "text": "kind of expect that\nthe answer should be x.",
    "start": "2303520",
    "end": "2308660"
  },
  {
    "text": "And that's right. So the answer to\nds / dW is going",
    "start": "2308660",
    "end": "2315520"
  },
  {
    "text": "to be delta transpose\ntimes x transpose. And so the form that we're\ngetting for this derivative is",
    "start": "2315520",
    "end": "2323740"
  },
  {
    "text": "going to be the product of\nthe local error signal that",
    "start": "2323740",
    "end": "2329890"
  },
  {
    "text": "comes from above versus what we\ncalculate from the local input",
    "start": "2329890",
    "end": "2336309"
  },
  {
    "text": "x. So that shouldn't yet be\nobvious why that is true.",
    "start": "2336310",
    "end": "2341330"
  },
  {
    "text": "So let me just go through\nin a bit more detail why that's true. So when we want to\nwork out the ds / dW,",
    "start": "2341330",
    "end": "2352300"
  },
  {
    "text": "right, it's sort\nof delta times dz / dW where that's computing\nfor z is Wx plus b.",
    "start": "2352300",
    "end": "2360400"
  },
  {
    "text": "So let's just\nconsider for a moment what the derivative\nis with respect",
    "start": "2360400",
    "end": "2365890"
  },
  {
    "text": "to a single weight, Wij. So Wij might be W23 that's shown\nin my little neural network",
    "start": "2365890",
    "end": "2375579"
  },
  {
    "text": "here. And so the first thing to\nnotice is that Wij only",
    "start": "2375580",
    "end": "2382089"
  },
  {
    "text": "contributes to zi, so\nit's going into to z2,",
    "start": "2382090",
    "end": "2390190"
  },
  {
    "text": "which then computes h2 and it\nhas no effect whatsoever on h1.",
    "start": "2390190",
    "end": "2397130"
  },
  {
    "text": "OK, so when we're\nworking out dzi, dWij, it's going to be\ndWix that sort of row,",
    "start": "2397130",
    "end": "2410000"
  },
  {
    "text": "that row of the matrix plus\nbi, which means that for--",
    "start": "2410000",
    "end": "2416490"
  },
  {
    "text": "we've got a kind of a\nsum of Wik times xk. And then for this sum, this\nis like one variable calculus",
    "start": "2416490",
    "end": "2425930"
  },
  {
    "text": "that when we're taking the\nderivative of this with respect to Wij, every term in\nthe sum is going to be 0,",
    "start": "2425930",
    "end": "2435260"
  },
  {
    "text": "the derivative is going to\nbe 0 except for the one that involves Wij. And then the derivative\nof that is just",
    "start": "2435260",
    "end": "2442550"
  },
  {
    "text": "like ax with respect\nto a, it's going to be x so you get\nxj out as the answer.",
    "start": "2442550",
    "end": "2451600"
  },
  {
    "text": "And so the end result of that\nis that when we're working out",
    "start": "2451600",
    "end": "2456930"
  },
  {
    "text": "what we want as the\nanswer is that we're going to get these columns\nwhere x1 is all that's left,",
    "start": "2456930",
    "end": "2468060"
  },
  {
    "text": "x2 is all that's left through\nxm is all that's left. And then that's multiplied by\nthe vectors of the local error",
    "start": "2468060",
    "end": "2477960"
  },
  {
    "text": "signal from above. And what we want to compute is\nthis outer product matrix where",
    "start": "2477960",
    "end": "2484320"
  },
  {
    "text": "we're getting the different\ncombinations of the delta and the x.",
    "start": "2484320",
    "end": "2489690"
  },
  {
    "text": "And so we can get\nthe n by m matrix that we'd like to have\nvia our shape convention",
    "start": "2489690",
    "end": "2496890"
  },
  {
    "text": "by taking delta transpose,\nwhich is n by 1 times x transpose, which\nis n1 by m and then",
    "start": "2496890",
    "end": "2504840"
  },
  {
    "text": "we get this out\nof product matrix. So like that's the kind of a\nhacky argument that I've made.",
    "start": "2504840",
    "end": "2512520"
  },
  {
    "text": "It's certainly a\nway of doing things that the dimensions work out\nand it sort of makes sense,",
    "start": "2512520",
    "end": "2518670"
  },
  {
    "text": "there's a more detailed\nrun through this that appears on the lecture notes.",
    "start": "2518670",
    "end": "2523960"
  },
  {
    "text": "And I encourage\nyou to sort of also look at the more\nmathy version of that.",
    "start": "2523960",
    "end": "2529859"
  },
  {
    "text": "Here's a little bit\nmore information about the shape convention. So well, first of\nall one more example",
    "start": "2529860",
    "end": "2541290"
  },
  {
    "start": "2535000",
    "end": "2896000"
  },
  {
    "text": "of this, so when you're working\nds / db that comes out as--",
    "start": "2541290",
    "end": "2550700"
  },
  {
    "text": "its Jacobian is a row vector. But similarly, you know\naccording to the shape",
    "start": "2550700",
    "end": "2557900"
  },
  {
    "text": "convention we want our gradient\nto be the same shape as b and b",
    "start": "2557900",
    "end": "2564589"
  },
  {
    "text": "is column vector, so\nthat's sort of again, they're different shapes and\nyou have to transpose one",
    "start": "2564590",
    "end": "2570950"
  },
  {
    "text": "to get the other. And so effectively, what\nwe have is a disagreement between the Jacobian form.",
    "start": "2570950",
    "end": "2578000"
  },
  {
    "text": "So the Jacobian form makes\nsense for calculus and math. Because if you want to have\nthat like I claimed, that matrix",
    "start": "2578000",
    "end": "2587720"
  },
  {
    "text": "calculus is just like\nsingle variable calculus apart from using\nvectors and matrices,",
    "start": "2587720",
    "end": "2593090"
  },
  {
    "text": "you can just multiply\ntogether the parftials, that only works out if\nyou're using Jacobians.",
    "start": "2593090",
    "end": "2599660"
  },
  {
    "text": "But on the other\nhand, if you want to do stochastic\ngradient descent",
    "start": "2599660",
    "end": "2606020"
  },
  {
    "text": "and be able to sort of subtract\noff a piece of the gradient, that only works if you\nhave the same shape",
    "start": "2606020",
    "end": "2614029"
  },
  {
    "text": "matrix for the gradient as you\ndo for the original matrix.",
    "start": "2614030",
    "end": "2621240"
  },
  {
    "text": "And so this is a bit confusing,\nbut that's just the reality, there are both of\nthese two things.",
    "start": "2621240",
    "end": "2628440"
  },
  {
    "text": "So the Jacobian form is\nuseful in doing the calculus.",
    "start": "2628440",
    "end": "2634910"
  },
  {
    "text": "But for the answers\nin the assignment, we want the answers to be\npresented using the shape",
    "start": "2634910",
    "end": "2642350"
  },
  {
    "text": "convention so that\nthe gradient is shown in the same shape\nas the parameters",
    "start": "2642350",
    "end": "2651350"
  },
  {
    "text": "and therefore, you'll\nbe able to-- it's the right shape for doing\na gradient update by just",
    "start": "2651350",
    "end": "2657200"
  },
  {
    "text": "subtracting a small\namount of the gradient. So for working through things\nthere are then basically",
    "start": "2657200",
    "end": "2666170"
  },
  {
    "text": "two choices. One choice is to work through\nall the math using Jacobians",
    "start": "2666170",
    "end": "2674060"
  },
  {
    "text": "and then right at the end, to\nreshape following the shape convention to give the answer.",
    "start": "2674060",
    "end": "2680940"
  },
  {
    "text": "So that's what I did\nwhen I worked out ds/ db. We worked through\nit using Jacobians,",
    "start": "2680940",
    "end": "2690769"
  },
  {
    "text": "we got an answer but it\nturned out to be a row vector, and so well then we\nhave to transpose that",
    "start": "2690770",
    "end": "2697430"
  },
  {
    "text": "at the end to get into the right\nshape for the shape convention.",
    "start": "2697430",
    "end": "2702619"
  },
  {
    "text": "The alternative is to always\nfollow the shape convention.",
    "start": "2702620",
    "end": "2710870"
  },
  {
    "text": "And that's kind of what I did\nwhen I was then working out ds / dW, I didn't\nfully use Jacobians.",
    "start": "2710870",
    "end": "2718789"
  },
  {
    "text": "I said, Oh, well, when we work\nout whatever it was, dz / dW,",
    "start": "2718790",
    "end": "2725660"
  },
  {
    "text": "let's work out what\nshape we want it to be and what to fill\nin the cells with.",
    "start": "2725660",
    "end": "2731330"
  },
  {
    "text": "And if you're sort of\ntrying to do it immediately with the shape convention,\nit's a little bit more hacky",
    "start": "2731330",
    "end": "2741410"
  },
  {
    "text": "in a way, since\nyou know you have to look at the dimensions\nfor what you want and figure out when to transpose\nor to reshape the matrix",
    "start": "2741410",
    "end": "2749360"
  },
  {
    "text": "to be at the right shape. But the kind of informal\nreasoning that I gave",
    "start": "2749360",
    "end": "2755720"
  },
  {
    "text": "is what you do and what works. And one way, and there are\nsort of hints that you can use,",
    "start": "2755720",
    "end": "2762890"
  },
  {
    "text": "right, that you know that\nyour gradient should always be the same shape\nas your parameters",
    "start": "2762890",
    "end": "2768829"
  },
  {
    "text": "and you know that the\nerror message coming in will always have the\nsame dimensionality",
    "start": "2768830",
    "end": "2775040"
  },
  {
    "text": "as that hidden layer, and\nyou can work it out always following the shape convention.",
    "start": "2775040",
    "end": "2780260"
  },
  {
    "text": " OK, so that is hey, doing\nthis is all matrix calculus.",
    "start": "2780260",
    "end": "2794640"
  },
  {
    "text": "So after pausing for\nbreath for a second,",
    "start": "2794640",
    "end": "2800289"
  },
  {
    "text": "the rest of the\nlecture is then OK. Let's look at how our software\ntrains neural networks using",
    "start": "2800290",
    "end": "2811200"
  },
  {
    "text": "what's referred to as the\nbackpropagation algorithm. ",
    "start": "2811200",
    "end": "2824369"
  },
  {
    "text": "So the short answer is\nbasically we've already done it.",
    "start": "2824370",
    "end": "2831180"
  },
  {
    "text": "The rest of the lecture is easy. So essentially, I've just shown\nyou what the backpropagation",
    "start": "2831180",
    "end": "2838710"
  },
  {
    "text": "algorithm does. So the backpropagation\nalgorithm is",
    "start": "2838710",
    "end": "2845130"
  },
  {
    "text": "judiciously taking and\npropagating derivatives",
    "start": "2845130",
    "end": "2854009"
  },
  {
    "text": "using the matrix chain rule. The rest of the back\npropagation algorithm",
    "start": "2854010",
    "end": "2861480"
  },
  {
    "text": "is to say, OK, when we\nhave these neural networks, we have a lot of shared\nstructure and shared",
    "start": "2861480",
    "end": "2870750"
  },
  {
    "text": "derivatives. So what we want\nto do is maximally",
    "start": "2870750",
    "end": "2876090"
  },
  {
    "text": "efficiently re-use\nderivatives of higher layers",
    "start": "2876090",
    "end": "2881790"
  },
  {
    "text": "when we're computing\nderivatives for lower layers so that we minimize computation.",
    "start": "2881790",
    "end": "2887890"
  },
  {
    "text": "And I already pointed\nthat out in the first half but we want to\nsystematically exploit that.",
    "start": "2887890",
    "end": "2894940"
  },
  {
    "text": "And so the way we do that\nin our computational systems is they construct\ncomputation graphs.",
    "start": "2894940",
    "end": "2903780"
  },
  {
    "start": "2896000",
    "end": "3252000"
  },
  {
    "text": "So this maybe looks a\nlittle bit like what you saw in a compilers\nclass if you did one, right,",
    "start": "2903780",
    "end": "2910830"
  },
  {
    "text": "that you're creating-- I call it here computation graph\nbut it's really a tree, right?",
    "start": "2910830",
    "end": "2916650"
  },
  {
    "text": "So you're creating here\nthis tree of computations in this case but in\na more general case",
    "start": "2916650",
    "end": "2924090"
  },
  {
    "text": "it's some kind of directed\ngraph of computations which has",
    "start": "2924090",
    "end": "2930810"
  },
  {
    "text": "source nodes, which are inputs. Either inputs like x or\ninput parameters like W",
    "start": "2930810",
    "end": "2938700"
  },
  {
    "text": "and b and its interior\nnodes are operations.",
    "start": "2938700",
    "end": "2943710"
  },
  {
    "text": "And so then once we've\nconstructed a graph, and so this graph corresponds\nto exactly the example",
    "start": "2943710",
    "end": "2949950"
  },
  {
    "text": "I did before, right? That this was our little neural\nnet that's in the top right and here's the corresponding\ncomputation graph of computing",
    "start": "2949950",
    "end": "2957630"
  },
  {
    "text": "Wx plus b put it through\nthe sigmoid non-linearity f",
    "start": "2957630",
    "end": "2964230"
  },
  {
    "text": "multiply the resulting dot\nproduct with the resulting vector with u, gives\nus our output score s.",
    "start": "2964230",
    "end": "2972850"
  },
  {
    "text": "OK, so what we do\nto compute this is we pass along the edges\nthe results of operations.",
    "start": "2972850",
    "end": "2980510"
  },
  {
    "text": "So this is Wx then z then\nh and then our output is s. And so the first\nthing we want to be",
    "start": "2980510",
    "end": "2987869"
  },
  {
    "text": "able to do to compute\nwith neural networks is to be able to compute\nfour different inputs, what",
    "start": "2987870",
    "end": "2994290"
  },
  {
    "text": "the output is. And so that's referred to\nas forward propagation.",
    "start": "2994290",
    "end": "2999690"
  },
  {
    "text": "And so we simply run\nthis expression much",
    "start": "2999690",
    "end": "3005510"
  },
  {
    "text": "like you standardly\ndo in a compiler to compute the value\nof s and that's",
    "start": "3005510",
    "end": "3011900"
  },
  {
    "text": "the forward propagation phase. But the essential additional\nelement of neural networks",
    "start": "3011900",
    "end": "3018350"
  },
  {
    "text": "is that we then also want\nto be able to send back gradients which will\ntell us how to update",
    "start": "3018350",
    "end": "3026000"
  },
  {
    "text": "the parameters of the model. And so it's this ability to send\nback gradients which gives us",
    "start": "3026000",
    "end": "3033440"
  },
  {
    "text": "the ability for these models\nto learn once we have a loss function at the\nend, we can work out",
    "start": "3033440",
    "end": "3040310"
  },
  {
    "text": "how to change the\nparameters of the model so that they more accurately\nproduce the desired output,",
    "start": "3040310",
    "end": "3048020"
  },
  {
    "text": "i.e. they minimize the loss. And so it's doing that part that\nthen is called backpropagation.",
    "start": "3048020",
    "end": "3056539"
  },
  {
    "text": "So we then-- once we\nforward propagated a value with our current\nparameters, we then",
    "start": "3056540",
    "end": "3064910"
  },
  {
    "text": "head backwards reversing\nthe direction of the arrows and pass along gradients down\nto the different parameters",
    "start": "3064910",
    "end": "3074330"
  },
  {
    "text": "like b, and W, and\nu that we can use to change using stochastic\ngradient descent what",
    "start": "3074330",
    "end": "3081500"
  },
  {
    "text": "the value of b is and\nwhat the value of W is. So we start off with ds\n/ ds, which is just 1,",
    "start": "3081500",
    "end": "3089000"
  },
  {
    "text": "and then we run\nour backpropagation and we're using the sort\nof same kind of composition",
    "start": "3089000",
    "end": "3096950"
  },
  {
    "text": "of Jacobian. So we have ds / dh\nhere and ds / dz and we progressively pass\nback those gradients.",
    "start": "3096950",
    "end": "3106130"
  },
  {
    "text": "So we just need to work out how\nto efficiently and cleanly do this in a computational system.",
    "start": "3106130",
    "end": "3113850"
  },
  {
    "text": "And so let's sort of\nwork through again a few of these cases. So the general situation is\nwe have a particular node,",
    "start": "3113850",
    "end": "3124859"
  },
  {
    "text": "so a node is where some kind of\noperation like multiplication",
    "start": "3124860",
    "end": "3130580"
  },
  {
    "text": "or a non-linearity happens. And so the simplest\ncase is that we've",
    "start": "3130580",
    "end": "3136700"
  },
  {
    "text": "got one output and one input. So we'll do that first. So that's like h equals f of z.",
    "start": "3136700",
    "end": "3143630"
  },
  {
    "text": "So what we have is an\nupstream gradient ds / dh",
    "start": "3143630",
    "end": "3151220"
  },
  {
    "text": "and what we want to do is\ncompute the downstream gradient",
    "start": "3151220",
    "end": "3156290"
  },
  {
    "text": "of ds / dz. And the way we're going\nto do that is say,",
    "start": "3156290",
    "end": "3162070"
  },
  {
    "text": "well for this function\nf it's a function, it's got a derivative\nfor a gradient.",
    "start": "3162070",
    "end": "3169150"
  },
  {
    "text": "So what we want to do is work\nout that local gradient dh / dz, and then that gives us\neverything that we need to work",
    "start": "3169150",
    "end": "3180090"
  },
  {
    "text": "out ds/ dz because that's\nprecisely we're going to use the chain rule. We're going to say that ds / dz\nequals the product of ds / dh",
    "start": "3180090",
    "end": "3189420"
  },
  {
    "text": "times dh / dz where this\nis, again, using Jacobians.",
    "start": "3189420",
    "end": "3194609"
  },
  {
    "text": "OK, so the general principle\nthat we're going to use is the downstream gradient\nequals the upstream gradient",
    "start": "3194610",
    "end": "3201690"
  },
  {
    "text": "times the local gradient. OK, sometimes it gets a\nlittle bit more complicated.",
    "start": "3201690",
    "end": "3207700"
  },
  {
    "text": "So we might have multiple\ninputs to a function. So this is the matrix vector\nmultiply, so z equals Wx.",
    "start": "3207700",
    "end": "3216780"
  },
  {
    "text": "OK, when there are\nmultiple inputs, we still have an upstream\ngradient ds / dz,",
    "start": "3216780",
    "end": "3225450"
  },
  {
    "text": "but what we're going to do\nis work out a local gradient with respect to each input.",
    "start": "3225450",
    "end": "3232330"
  },
  {
    "text": "So we have dz / dw and dz / dx. And so then at that point,\nit's exactly the same",
    "start": "3232330",
    "end": "3239730"
  },
  {
    "text": "for each piece of it. We're going to work out the\ndownstream gradients ds / dW",
    "start": "3239730",
    "end": "3244980"
  },
  {
    "text": "and ds / dx by using the\nchain rule with respect to the particular\nlocal gradient.",
    "start": "3244980",
    "end": "3254250"
  },
  {
    "start": "3252000",
    "end": "3599000"
  },
  {
    "text": "So let's go through\nan example of this. I mean, this is kind\nof a silly example,",
    "start": "3254250",
    "end": "3260690"
  },
  {
    "text": "it's not really an example that\nlooks like a typical neural net but it's sort of a\nsimple example where",
    "start": "3260690",
    "end": "3266690"
  },
  {
    "text": "we can show some of the\ncomponents of what we do. So what we're going\nto do is we're",
    "start": "3266690",
    "end": "3271780"
  },
  {
    "text": "going to calculate\nf of xyz, which is being calculated as x plus\ny times the max of y and z.",
    "start": "3271780",
    "end": "3282560"
  },
  {
    "text": "And we've got particular\nvalues that we're starting off with x equals 1,\ny equals 2, and z equals 0.",
    "start": "3282560",
    "end": "3290480"
  },
  {
    "text": "So these are the current\nvalues of our parameters. And so we can say,\nOK, well, we want",
    "start": "3290480",
    "end": "3297710"
  },
  {
    "text": "to build an expression\ntree for that. Here's our expression tree.",
    "start": "3297710",
    "end": "3303140"
  },
  {
    "text": "We're taking x plus y, we're\ntaking the max of y and z, and then we're multiplying them.",
    "start": "3303140",
    "end": "3310170"
  },
  {
    "text": "And so our forward propagation\nphase is just to run this.",
    "start": "3310170",
    "end": "3315210"
  },
  {
    "text": "So we take the values\nof our parameters and we simply start to\ncompute with them, right?",
    "start": "3315210",
    "end": "3321240"
  },
  {
    "text": "So we have 1, 2, 2, 0\nand we add them as 3, the max is 2 we multiply\nthem and that gives us 6.",
    "start": "3321240",
    "end": "3329509"
  },
  {
    "text": " OK, so then at\nthat point, we then",
    "start": "3329510",
    "end": "3336029"
  },
  {
    "text": "want to go and work out how to\ndo things for back propagation",
    "start": "3336030",
    "end": "3343620"
  },
  {
    "text": "and how these back\npropagation steps work. And so the first\npart of that is sort",
    "start": "3343620",
    "end": "3349950"
  },
  {
    "text": "of working out what our local\ngradients are going to be. ",
    "start": "3349950",
    "end": "3356310"
  },
  {
    "text": "So this is a here\nand this is x and y. So da / dx since a equals x\nplus y is just going to be 1.",
    "start": "3356310",
    "end": "3365010"
  },
  {
    "text": "And da / dy is\nalso going to be 1.",
    "start": "3365010",
    "end": "3370590"
  },
  {
    "text": "Then for b equals the max of\nyz, so this is this max node,",
    "start": "3370590",
    "end": "3379330"
  },
  {
    "text": "So the local gradients for that\nis it's going to depend on y--",
    "start": "3379330",
    "end": "3385070"
  },
  {
    "text": "where the y is greater than z. So db / dy is going\nto be 1 if and only",
    "start": "3385070",
    "end": "3392540"
  },
  {
    "text": "if y is greater than z, which\nit is at our particular point here. So that's 1.",
    "start": "3392540",
    "end": "3398210"
  },
  {
    "text": "And dbdz is going to be 1\nonly if z is greater than y.",
    "start": "3398210",
    "end": "3405260"
  },
  {
    "text": "So for our particular values\nhere that one is going to be 0.",
    "start": "3405260",
    "end": "3412880"
  },
  {
    "text": "And then finally here\nwe are calculating the product f equals ab.",
    "start": "3412880",
    "end": "3419240"
  },
  {
    "text": "So for that, we're going to--",
    "start": "3419240",
    "end": "3426440"
  },
  {
    "text": "wait, sorry, that slide\nis a little imperfect. OK, so for the product, the\nderivative f with respect to a",
    "start": "3426440",
    "end": "3434210"
  },
  {
    "text": "is equal to b, which is\n2, and the derivative of f with respect\nto b is a equals 3.",
    "start": "3434210",
    "end": "3440450"
  },
  {
    "text": "So that gives us all of the\nlocal gradients at each node. And so then to run\nback propagation,",
    "start": "3440450",
    "end": "3448250"
  },
  {
    "text": "we start with df /\ndf, which is just 1, and then we're going to work\nout that the downstream equals",
    "start": "3448250",
    "end": "3458360"
  },
  {
    "text": "the upstream times the local. OK, so the local-- so when you have a\nproduct like this,",
    "start": "3458360",
    "end": "3466220"
  },
  {
    "text": "note that sort of\nthe gradients flip. So we take upstream times\nthe local, which is 2, oops.",
    "start": "3466220",
    "end": "3475840"
  },
  {
    "text": " So the downstream is 2,\non this side df / db is 3.",
    "start": "3475840",
    "end": "3489470"
  },
  {
    "text": "So we're taking upstream\ntimes local, that gives us 3, and so that gives us--",
    "start": "3489470",
    "end": "3495589"
  },
  {
    "text": "that propagates values to\nthe plus and max nodes. And so then we continue along.",
    "start": "3495590",
    "end": "3502520"
  },
  {
    "text": "So for the max node, the local\ngradient db / dy equals 1.",
    "start": "3502520",
    "end": "3510020"
  },
  {
    "text": "So we're going to\ntake upstream is 3, so it only takes 3 times\n1 and that gives us 3.",
    "start": "3510020",
    "end": "3519800"
  },
  {
    "text": "db / dz is 0 because of the fact\nthat z value is not the max.",
    "start": "3519800",
    "end": "3525050"
  },
  {
    "text": "So we're taking 3 times 0 and\nsaying the gradient there is 0.",
    "start": "3525050",
    "end": "3530060"
  },
  {
    "text": "So finally, during\nthe plus node, the local gradients for\nboth x and y there are 1.",
    "start": "3530060",
    "end": "3537710"
  },
  {
    "text": "So we're just getting\n2 times 1 in both cases and we're saying that the\ngradients there are 2.",
    "start": "3537710",
    "end": "3544400"
  },
  {
    "text": "OK, and so again at\nthe end of the day, the interpretation here is that\nthis is giving us information",
    "start": "3544400",
    "end": "3553520"
  },
  {
    "text": "as to if we wiggle the\nvalues of x, y, and z, how much of a difference\ndoes it make to the output?",
    "start": "3553520",
    "end": "3561490"
  },
  {
    "text": "What is the slope, the gradient\nwith respect to the variable?",
    "start": "3561490",
    "end": "3566640"
  },
  {
    "text": "So what we've seen is that since\nz isn't the max of y and z,",
    "start": "3566640",
    "end": "3575150"
  },
  {
    "text": "if I change the value of z a\nlittle, like I make the z 0.1",
    "start": "3575150",
    "end": "3580190"
  },
  {
    "text": "or minus 0.1, it\nmakes no difference at all to what I\ncompute as the output.",
    "start": "3580190",
    "end": "3585510"
  },
  {
    "text": "So therefore, the\ngradient there is 0. If I change the\nvalue of x a little,",
    "start": "3585510",
    "end": "3595520"
  },
  {
    "text": "then that is going\nto have an effect. And it's going to affect\nthe output by twice as much",
    "start": "3595520",
    "end": "3604579"
  },
  {
    "text": "as the amount I change it. ",
    "start": "3604580",
    "end": "3612350"
  },
  {
    "text": "Right, so and that's because\nthe df / dz equals 2.",
    "start": "3612350",
    "end": "3619310"
  },
  {
    "text": "So interestingly, so I mean we\ncan basically work that out.",
    "start": "3619310",
    "end": "3626460"
  },
  {
    "text": "So if we imagine\nmaking sort of x 2.1,",
    "start": "3626460",
    "end": "3631650"
  },
  {
    "text": "well, then what we'd\ncalculate the max is 2--",
    "start": "3631650",
    "end": "3637930"
  },
  {
    "text": "I'm so sorry, if we make x 1.1,\nwe then get the max here is 2,",
    "start": "3637930",
    "end": "3644290"
  },
  {
    "text": "and we get 1.1 plus 2 is\n3.1, so we get 3.1 times",
    "start": "3644290",
    "end": "3650710"
  },
  {
    "text": "2 so they'd be about 6.2. So changing x by 0.1 has\nadded 0.2 to the value of f.",
    "start": "3650710",
    "end": "3660819"
  },
  {
    "text": "Conversely, for the value of\ny, we find that df dy equals 5.",
    "start": "3660820",
    "end": "3667000"
  },
  {
    "text": "So, what we do when we've got\ntwo things coming out here, as I'll go through\nagain in a moment,",
    "start": "3667000",
    "end": "3673060"
  },
  {
    "text": "is we're summing the gradient. So, again, 3 plus 2 equals 5. And empirically\nthat's what happens.",
    "start": "3673060",
    "end": "3679359"
  },
  {
    "text": "So, if we consider fiddling\nthe value of y a little, let's say we make\nit a value of 2.1,",
    "start": "3679360",
    "end": "3687579"
  },
  {
    "text": "then the prediction is\nthey'll have 5 times as big an effect on the\noutput value that we compute.",
    "start": "3687580",
    "end": "3694329"
  },
  {
    "text": "And, well, what do we compute? So, we compute 1 plus 2.1.",
    "start": "3694330",
    "end": "3699680"
  },
  {
    "text": "So that's 3.1. Then we compute the max\nof 2.1 and 0, is 2.1.",
    "start": "3699680",
    "end": "3707660"
  },
  {
    "text": "So we'll take the\nproduct of 2.1 and 3.1. And I calculate that in\nadvance, since I can't really",
    "start": "3707660",
    "end": "3714220"
  },
  {
    "text": "do this arithmetic in my head. And the product of\nthose two is 6.51.",
    "start": "3714220",
    "end": "3719230"
  },
  {
    "text": "So it has gone up about by 0.5. So we've multiplied by\nfiddling it by 0.1, by 5 times",
    "start": "3719230",
    "end": "3728260"
  },
  {
    "text": "to work out the magnitude\nof the effect on the output. OK. So for this--",
    "start": "3728260",
    "end": "3735069"
  },
  {
    "text": "Before I did the case of when\nwe had one in and one out here,",
    "start": "3735070",
    "end": "3744820"
  },
  {
    "text": "and multiple ins\nand one out here.",
    "start": "3744820",
    "end": "3751300"
  },
  {
    "text": "The case that I hadn't\nactually dealt with is the case of when you have\nmultiple outward branches, that",
    "start": "3751300",
    "end": "3760119"
  },
  {
    "text": "then turned up in\nthe computation of y. So, once you have multiple\noutward branches, what",
    "start": "3760120",
    "end": "3767410"
  },
  {
    "text": "you're doing is you're summing. So that when you want\nto work out the df dy,",
    "start": "3767410",
    "end": "3778839"
  },
  {
    "text": "you've got a local\ngradient, you've got two upstream\ngradients, and you're",
    "start": "3778840",
    "end": "3787330"
  },
  {
    "text": "working it out with\nrespect to each of them as in the chain rule. And then you're\nsumming them together",
    "start": "3787330",
    "end": "3794050"
  },
  {
    "text": "to work out the\nimpact at the end. ",
    "start": "3794050",
    "end": "3800960"
  },
  {
    "text": "So, we also saw some of the\nother node intuitions, which it's useful to have doing this.",
    "start": "3800960",
    "end": "3808320"
  },
  {
    "text": "So when you have\nan addition, that distributes the\nupstream gradient",
    "start": "3808320",
    "end": "3815210"
  },
  {
    "text": "to each of the things below it. When you have max, it's\nlike a routing node.",
    "start": "3815210",
    "end": "3822060"
  },
  {
    "text": "So when you have max, you\nhave the upstream gradient and it goes to one of\nthe branches below it,",
    "start": "3822060",
    "end": "3828760"
  },
  {
    "text": "and the rest of them\nget no gradient.  When you then have\na multiplication,",
    "start": "3828760",
    "end": "3836830"
  },
  {
    "text": "it has this effect of\nswitching the gradient. So, if you're taking 3 by 2,\nthe gradient on the 2 side",
    "start": "3836830",
    "end": "3847119"
  },
  {
    "text": "is 3 and on the 3 side is 2. And if you think about it in\nterms of how much effect you",
    "start": "3847120",
    "end": "3853450"
  },
  {
    "text": "get from when you're doing this\nsort of wiggling, that totally makes sense, right? Because if you're multiplying\nanother number by 3,",
    "start": "3853450",
    "end": "3861250"
  },
  {
    "text": "then any change here is\ngoing to be multiplied by 3, and vice versa.",
    "start": "3861250",
    "end": "3866705"
  },
  {
    "text": " OK.",
    "start": "3866705",
    "end": "3872070"
  },
  {
    "text": "So, this is the kind\nof computation graph that we want to use to work\nout derivatives in an automated",
    "start": "3872070",
    "end": "3881010"
  },
  {
    "text": "computational fashion,\nwhich is the basis of the back\npropagation algorithm.",
    "start": "3881010",
    "end": "3887160"
  },
  {
    "text": "But at that point, this\nis what we're doing, but there's still one\nmistake that we can make.",
    "start": "3887160",
    "end": "3894300"
  },
  {
    "text": "It would be wrong for us to sort\nof say, OK, well, first of all, we want to work out ds / db.",
    "start": "3894300",
    "end": "3900640"
  },
  {
    "text": "So, look, we can start up here. We can propagate\nour upstream errors,",
    "start": "3900640",
    "end": "3907559"
  },
  {
    "text": "work out local gradients. Upstream our local gradient\nand keep all the way down,",
    "start": "3907560",
    "end": "3913650"
  },
  {
    "text": "and get the ds / db down here.",
    "start": "3913650",
    "end": "3919029"
  },
  {
    "text": "OK. Next we want to\ndo it for ds / dw. Let's just run it\nall over again.",
    "start": "3919030",
    "end": "3926230"
  },
  {
    "text": "Because if we did that, we'd\nbe doing repeated computation, as I showed in the first half.",
    "start": "3926230",
    "end": "3932740"
  },
  {
    "text": "That this term is\nthe same both times. This term is the\nsame both times.",
    "start": "3932740",
    "end": "3938530"
  },
  {
    "text": "This term is the\nsame both times. That only the bits\nat the end differ.",
    "start": "3938530",
    "end": "3943570"
  },
  {
    "text": "So, what we want to do is\navoid duplicated computation",
    "start": "3943570",
    "end": "3948730"
  },
  {
    "text": "and compute all the\ngradients that we're going to need successively,\nso that we only do them once.",
    "start": "3948730",
    "end": "3958660"
  },
  {
    "text": "And so that was\nanalogous to when I introduced this delta variable\nwhen we computed gradients",
    "start": "3958660",
    "end": "3965200"
  },
  {
    "text": "by hand. So, starting off here from-- ",
    "start": "3965200",
    "end": "3972540"
  },
  {
    "text": "Starting off here\nwith ds / ds is 1. We then want to one time compute\ngradient in the green here.",
    "start": "3972540",
    "end": "3982680"
  },
  {
    "text": "One time compute the\ngradient in green here. That's all common work.",
    "start": "3982680",
    "end": "3988020"
  },
  {
    "text": "Then we're going to take the\nlocal gradient for dz / db",
    "start": "3988020",
    "end": "3995640"
  },
  {
    "text": "and multiply that by\nthe upstream gradient to have worked out ds / db. And then we're going to take\nthe same upstream gradient,",
    "start": "3995640",
    "end": "4004070"
  },
  {
    "text": "and then work out the\nlocal gradient here.",
    "start": "4004070",
    "end": "4011080"
  },
  {
    "text": "And then sort of propagate\nthat down to give us ds / dw. So, the end result is, we\nwant to sort of systematically",
    "start": "4011080",
    "end": "4020400"
  },
  {
    "text": "work to forward\ncomputation forward in the graph, and\nbackward computation,",
    "start": "4020400",
    "end": "4026670"
  },
  {
    "text": "back propagation backward\nin the graph in a way that we do things efficiently.",
    "start": "4026670",
    "end": "4033190"
  },
  {
    "text": "So this is the general\nform of the algorithm, which works for an\narbitrary computation graph.",
    "start": "4033190",
    "end": "4043780"
  },
  {
    "text": "So, at the end of the day, we've\ngot a single scalar output, z.",
    "start": "4043780",
    "end": "4050830"
  },
  {
    "text": "And then we have inputs and\nparameters, which compute z.",
    "start": "4050830",
    "end": "4058230"
  },
  {
    "text": "And so once we have\nthis computation graph-- And I added in this\nfunky extra arrow",
    "start": "4058230",
    "end": "4065040"
  },
  {
    "text": "here to make it a more\ngeneral computation graph. Well, we can always say\nthat we can work out",
    "start": "4065040",
    "end": "4072450"
  },
  {
    "text": "a starting point, something\nthat doesn't depend on anything. So in this case, both of\nthese bottom two nodes",
    "start": "4072450",
    "end": "4079830"
  },
  {
    "text": "don't depend on anything else. So we can start\nwith them, and we can start to compute forward.",
    "start": "4079830",
    "end": "4086130"
  },
  {
    "text": "We can compute values for\nall of these second row from the bottom nodes.",
    "start": "4086130",
    "end": "4091920"
  },
  {
    "text": "And then we're able to\ncompute the third ones up. So, we can have a\ntopological sort",
    "start": "4091920",
    "end": "4100349"
  },
  {
    "text": "of the nodes based\non the dependencies in this directed graph. And we can compute the\nvalue of each node,",
    "start": "4100350",
    "end": "4108299"
  },
  {
    "text": "given some subset\nof its predecessors which it depends on.",
    "start": "4108300",
    "end": "4113310"
  },
  {
    "text": "And so doing that is referred\nto as the forward propagation phase, and gives us a\ncomputation of the scalar",
    "start": "4113310",
    "end": "4120540"
  },
  {
    "text": "output, z, using our\ncurrent parameters and our current inputs.",
    "start": "4120540",
    "end": "4126000"
  },
  {
    "text": "And so then after that,\nwe run back propagation. So for back propagation, we\ninitialize the output gradient,",
    "start": "4126000",
    "end": "4135105"
  },
  {
    "text": "dz / dz, as 1. And then we visit nodes\nin the reverse order",
    "start": "4135105",
    "end": "4142589"
  },
  {
    "text": "of the topological sort, and we\ncompute the gradients downward.",
    "start": "4142590",
    "end": "4147810"
  },
  {
    "text": "And so our recipe is that for\neach node as we head down,",
    "start": "4147810",
    "end": "4152890"
  },
  {
    "text": "we're going to\ncompute the gradient of the node with respect to\nits successors the things",
    "start": "4152890",
    "end": "4161310"
  },
  {
    "text": "that it feeds into. And how we compute that gradient\nis using this chain rule",
    "start": "4161310",
    "end": "4168509"
  },
  {
    "text": "that we've looked at. So this is sort of the\ngeneralized form of the chain rule where we have\nmultiple outputs.",
    "start": "4168510",
    "end": "4175689"
  },
  {
    "text": "And so we're summing over\nthe different outputs. And then for each\noutput, we're computing",
    "start": "4175689",
    "end": "4180720"
  },
  {
    "text": "the product of the\nupstream gradient and the local gradient\nwith respect to that node.",
    "start": "4180720",
    "end": "4186929"
  },
  {
    "text": "And so we head downwards. And we continue down the\nreverse topological sort order,",
    "start": "4186930",
    "end": "4193740"
  },
  {
    "text": "and we work out the\ngradient with respect to each variable in this graph.",
    "start": "4193740",
    "end": "4201620"
  },
  {
    "text": "And so it hopefully\nlooks kind of intuitive",
    "start": "4201620",
    "end": "4207370"
  },
  {
    "text": "looking at this picture.  If you think of it like\nthis, the big O complexity",
    "start": "4207370",
    "end": "4215989"
  },
  {
    "text": "of forward propagation\nand backward propagation is the same, right? In both cases, you're\ndoing a linear path",
    "start": "4215990",
    "end": "4223970"
  },
  {
    "text": "through all of these\nnodes and calculating values given predecessors, and\nthen values given successors.",
    "start": "4223970",
    "end": "4232790"
  },
  {
    "text": "I mean, you have to do a little\nbit more work for working out the gradients sort of as\nshown by this chain rule,",
    "start": "4232790",
    "end": "4240290"
  },
  {
    "text": "but it's the same\nbig O complexity. So, if somehow you're\nimplementing stuff for yourself",
    "start": "4240290",
    "end": "4245390"
  },
  {
    "text": "rather than relying\non the software, and you're calculating the\ngradients of a different order",
    "start": "4245390",
    "end": "4251060"
  },
  {
    "text": "of complexity of\nforward propagation, it means that you're\ndoing something wrong.",
    "start": "4251060",
    "end": "4256449"
  },
  {
    "text": "You're doing repeated work\nthat you shouldn't have to do. OK. So, this algorithm works\nfor a completely arbitrary",
    "start": "4256450",
    "end": "4265640"
  },
  {
    "text": "computation graph. Any directed acyclic graph\nyou can apply this algorithm.",
    "start": "4265640",
    "end": "4272360"
  },
  {
    "text": "In general, what we find is that\nwe build neural networks that have a regular layer structure.",
    "start": "4272360",
    "end": "4279120"
  },
  {
    "text": "So we have things like\na vector of inputs, and then that's\nmultiplied by a matrix.",
    "start": "4279120",
    "end": "4284369"
  },
  {
    "text": "It's transformed into\nanother vector, which might be multiplied\nby another matrix,",
    "start": "4284370",
    "end": "4289880"
  },
  {
    "text": "or summed with another\nmatrix or something, right? So, once we're using that kind\nof regular layer structure,",
    "start": "4289880",
    "end": "4296150"
  },
  {
    "text": "we can then parallelize\nthe computation by working out the gradients in\nterms of Jacobians of vectors",
    "start": "4296150",
    "end": "4306470"
  },
  {
    "text": "and matrices, and do things in\nparallel much more efficiently. OK.",
    "start": "4306470",
    "end": "4312510"
  },
  {
    "text": "So, doing this is then\nreferred to as automatic differentiation.",
    "start": "4312510",
    "end": "4317909"
  },
  {
    "text": "And so, essentially, if you\nknow the computation graph,",
    "start": "4317910",
    "end": "4324180"
  },
  {
    "text": "you should be able to have\nyour clever computer system work out what the\nderivatives of everything is,",
    "start": "4324180",
    "end": "4334630"
  },
  {
    "text": "and then apply back\npropagation to work out",
    "start": "4334630",
    "end": "4339730"
  },
  {
    "text": "how to update the\nparameters and learn. And there's actually a\nsort of an interesting sort",
    "start": "4339730",
    "end": "4345320"
  },
  {
    "text": "of thing of how history has gone\nbackwards here, which I'll just",
    "start": "4345320",
    "end": "4353020"
  },
  {
    "text": "note. So, some of you\nmight be familiar",
    "start": "4353020",
    "end": "4358060"
  },
  {
    "text": "with symbolic\ncomputation packages, so those are things\nlike Mathematica.",
    "start": "4358060",
    "end": "4364969"
  },
  {
    "text": "So Mathematica, you can\ngive it a symbolic form of a computation, and\nthen it can work out",
    "start": "4364970",
    "end": "4372740"
  },
  {
    "text": "derivatives for you. So it should be the\ncase that if you give a complete symbolic form\nof a computation graph, then",
    "start": "4372740",
    "end": "4381710"
  },
  {
    "text": "it should be able to work out\nall the derivatives for you. And you never have to work out a\nderivative by hand, whatsoever.",
    "start": "4381710",
    "end": "4389660"
  },
  {
    "text": "And that was actually attempted\nin a famous deep learning library called\nTheano, which came out",
    "start": "4389660",
    "end": "4396410"
  },
  {
    "text": "of Yoshua Bengio's group at\nthe University of Montreal. That it had a compiler\nthat did that kind",
    "start": "4396410",
    "end": "4403670"
  },
  {
    "text": "of symbolic manipulation. But somehow, that\nproved a little bit",
    "start": "4403670",
    "end": "4413410"
  },
  {
    "text": "too hard a road to follow. I imagine that it actually might\ncome back again in the future.",
    "start": "4413410",
    "end": "4418650"
  },
  {
    "text": "And so, for modern deep\nlearning frameworks, which includes both\nTensorFlow, or PyTorch,",
    "start": "4418650",
    "end": "4426559"
  },
  {
    "text": "they do 90% of this computation\nof automatic differentiation",
    "start": "4426560",
    "end": "4434030"
  },
  {
    "text": "for you, but they don't\nactually symbolically compute derivatives.",
    "start": "4434030",
    "end": "4439070"
  },
  {
    "text": "So, for each particular node\nor layer of your deep learning",
    "start": "4439070",
    "end": "4445040"
  },
  {
    "text": "system, somebody,\neither you or the person who wrote that layer,\nhas handwritten",
    "start": "4445040",
    "end": "4455239"
  },
  {
    "text": "the local derivatives. But then everything\nfrom that point on,",
    "start": "4455240",
    "end": "4460369"
  },
  {
    "text": "the sort of the taking,\ndoing the chain rule of combining upstream\ngradients with local gradients",
    "start": "4460370",
    "end": "4467330"
  },
  {
    "text": "to work out downstream\ngradients, that's then all been done automatically\nfor back propagation",
    "start": "4467330",
    "end": "4473210"
  },
  {
    "text": "on the computation graph. And so what that means is,\nfor a whole neural network,",
    "start": "4473210",
    "end": "4480730"
  },
  {
    "text": "you have a computation\ngraph, and it's going to have a forward\npass and a backward pass.",
    "start": "4480730",
    "end": "4487590"
  },
  {
    "text": "And so for the\nforward pass, you're topologically sorting the nodes\nbased on their dependencies",
    "start": "4487590",
    "end": "4494969"
  },
  {
    "text": "in the computation graph. And then for each node,\nyou're running forward",
    "start": "4494970",
    "end": "4501750"
  },
  {
    "text": "the forward computation\non that node. And then for\nbackward propagation, you're reversing the\ntopological sort of the graph.",
    "start": "4501750",
    "end": "4509730"
  },
  {
    "text": "And then for each\nnode in the graph, you're running the\nbackward propagation,",
    "start": "4509730",
    "end": "4515140"
  },
  {
    "text": "which is the little bit\nof back prop, the chain rule at that node. And then the result\nof doing that is you",
    "start": "4515140",
    "end": "4521340"
  },
  {
    "text": "have gradients for your\ninputs and parameters.",
    "start": "4521340",
    "end": "4528119"
  },
  {
    "text": "And so, this is-- The overall software\nruns this for you.",
    "start": "4528120",
    "end": "4533790"
  },
  {
    "text": "And so what you want\nto do is then actually have stuff for particular\nnodes or layers in the graph.",
    "start": "4533790",
    "end": "4543690"
  },
  {
    "text": "So, if I have a\nmultiply gate, it's going to have a forward\nalgorithm, which",
    "start": "4543690",
    "end": "4549530"
  },
  {
    "text": "just computes that the\noutput is x times y in terms of the two inputs.",
    "start": "4549530",
    "end": "4555650"
  },
  {
    "text": "And then I'm going to\nwant to tell it also how to calculate the\nlocal derivative.",
    "start": "4555650",
    "end": "4561480"
  },
  {
    "text": "So I want to say, what\nis the local derivative. So, dl / dx and dl / dy in\nterms of the upstream gradient,",
    "start": "4561480",
    "end": "4572160"
  },
  {
    "text": "dl / dz. And so, I will then manually\nwork out how to calculate that.",
    "start": "4572160",
    "end": "4579239"
  },
  {
    "text": "And normally what\nI have to do is, I assume the forward\npass is being run first.",
    "start": "4579240",
    "end": "4587120"
  },
  {
    "text": "And I'm going to shove into some\nlocal variables for my class the values that we used in\nthe forward computation.",
    "start": "4587120",
    "end": "4595440"
  },
  {
    "text": "So as well as computing\nz equals x times y, I'm going to sort of\nremember what x and y were.",
    "start": "4595440",
    "end": "4603020"
  },
  {
    "text": "So that then when I'm asked\nto compute the backward pass, I'm then going to\nhave implemented here",
    "start": "4603020",
    "end": "4611090"
  },
  {
    "text": "what we saw earlier of-- That when it's xy, you're going\nto sort of swap the y and the x",
    "start": "4611090",
    "end": "4619220"
  },
  {
    "text": "to work out the local gradients. And so then I'm going\nto multiply those",
    "start": "4619220",
    "end": "4624620"
  },
  {
    "text": "by the upstream gradient. And I'm going to return-- I've just written it here\nas a sort of a little list,",
    "start": "4624620",
    "end": "4630560"
  },
  {
    "text": "but really it's going to be a\nNumPy vector of the gradients.",
    "start": "4630560",
    "end": "4636990"
  },
  {
    "text": "OK. So that's 98% of what I\nwanted to cover today.",
    "start": "4636990",
    "end": "4643530"
  },
  {
    "text": "Just a couple of\nquick comments left. So, that can and should\nall be automated.",
    "start": "4643530",
    "end": "4652619"
  },
  {
    "text": "Sometimes you want to\njust check if you're computing the right gradients.",
    "start": "4652620",
    "end": "4657690"
  },
  {
    "text": "And so the standard\nway of checking that you're computing\nthe right gradients is to manually work\nout the gradient",
    "start": "4657690",
    "end": "4665280"
  },
  {
    "text": "by doing a numeric\ncalculation of the gradient. And so, you can do that--",
    "start": "4665280",
    "end": "4671770"
  },
  {
    "text": "So you can work out what the\nderivative of f with respect to x should be, by choosing\nsome sort of small number,",
    "start": "4671770",
    "end": "4681780"
  },
  {
    "text": "like 10 to the minus 4, adding\nit to x, subtracting it from x.",
    "start": "4681780",
    "end": "4686789"
  },
  {
    "text": "And then so the difference\nbetween these numbers is 2h. Dividing it through by 2h.",
    "start": "4686790",
    "end": "4692160"
  },
  {
    "text": "And you're simply\nworking out the rise over the run, which is the\nslope at that point with respect",
    "start": "4692160",
    "end": "4698070"
  },
  {
    "text": "to x. And that's an approximation of\nthe gradient of f with respect",
    "start": "4698070",
    "end": "4703110"
  },
  {
    "text": "to x at that value of x. So, this is so simple.",
    "start": "4703110",
    "end": "4708820"
  },
  {
    "text": "You can't make a\nmistake implementing it. And so therefore,\nyou can use this to check whether your gradient\nvalues are correct or not.",
    "start": "4708820",
    "end": "4718739"
  },
  {
    "text": "This isn't something that\nyou'd want to use much. Because not only\nis it approximate, but it's extremely slow.",
    "start": "4718740",
    "end": "4725550"
  },
  {
    "text": "Because to work\nthis out, you have to run the forward computation\nfor every parameter",
    "start": "4725550",
    "end": "4730889"
  },
  {
    "text": "of the model. So, if you have a model\nwith a million parameters, you're now doing a\nmillion times as much work",
    "start": "4730890",
    "end": "4736770"
  },
  {
    "text": "to run back prop as you\nwould do if you're actually using calculus. So, calculus is a\ngood thing to know.",
    "start": "4736770",
    "end": "4743910"
  },
  {
    "text": "But it can be really useful to\ncheck that the right values are being calculated.",
    "start": "4743910",
    "end": "4750300"
  },
  {
    "text": "In the old days when we\nhandwrote everything, this was kind of\nthe key unit test that people used everywhere.",
    "start": "4750300",
    "end": "4757260"
  },
  {
    "text": "These days, most\nof the time you're reusing layers that\nare built into PyTorch, or some other deep\nlearning framework.",
    "start": "4757260",
    "end": "4763960"
  },
  {
    "text": "So it's much less needed. But sometimes you're\nimplementing your own layer, and you really do want\nto check that things",
    "start": "4763960",
    "end": "4770130"
  },
  {
    "text": "are implemented correctly. There's a fine point in\nthe way this is written. If you saw this sort of in\nhigh school calculus class,",
    "start": "4770130",
    "end": "4780180"
  },
  {
    "text": "you will have seen rise\nover run of f(x) plus h minus f(x) divided by h.",
    "start": "4780180",
    "end": "4790289"
  },
  {
    "text": "It turns out that doing this\ntwo sided estimate like this, is much, much more accurate\nthan doing a one sided estimate.",
    "start": "4790290",
    "end": "4798420"
  },
  {
    "text": "And so you're really\nmuch encouraged to use this approximation. OK.",
    "start": "4798420",
    "end": "4803790"
  },
  {
    "text": "So at that point, we've\nmastered the core technology of neural nets. Back propagation is recursively\nand hence efficiently",
    "start": "4803790",
    "end": "4812969"
  },
  {
    "text": "applying the chain rule\nalong the computation graph, with this key step that\ndownstream gradient equals",
    "start": "4812970",
    "end": "4822510"
  },
  {
    "text": "upstream gradient\ntimes local gradient. And so for calculating\nwith neural nets,",
    "start": "4822510",
    "end": "4828510"
  },
  {
    "text": "we do the forward\npass to work out values with current parameters,\nthen run back propagation",
    "start": "4828510",
    "end": "4835500"
  },
  {
    "text": "to work out the\ngradient of the loss, currently computed loss with\nrespect to those parameters.",
    "start": "4835500",
    "end": "4845179"
  },
  {
    "text": "Now, to some extent, with\nmodern deep learning frameworks, you don't actually have to know\nhow to do any of this, right?",
    "start": "4845180",
    "end": "4853380"
  },
  {
    "text": "It's the same as you\ndon't have to know how to implement a C compiler. You can just write C code and\nsay gcc, and it'll compile it",
    "start": "4853380",
    "end": "4863680"
  },
  {
    "text": "and it'll run the\nright stuff for you. And that's the kind\nof functionality",
    "start": "4863680",
    "end": "4869770"
  },
  {
    "text": "you get from the\nPyTorch framework. So, do come along to the\nPyTorch tutorial this Friday",
    "start": "4869770",
    "end": "4875650"
  },
  {
    "text": "and get a sense\nabout how easy it is to write neural networks\nusing a framework like PyTorch",
    "start": "4875650",
    "end": "4881929"
  },
  {
    "text": "or TensorFlow. And it's so easy,\nthat's why high school students across\nthe nation are now",
    "start": "4881930",
    "end": "4888880"
  },
  {
    "text": "doing their science projects\ntraining deep learning systems. Because you don't actually\nhave to understand",
    "start": "4888880",
    "end": "4895840"
  },
  {
    "text": "very much to bung a few neural\nnetwork layers together and set it computing on some data.",
    "start": "4895840",
    "end": "4902619"
  },
  {
    "text": "But we hope in this\nclass that you actually are also learning how these\nthings are implemented.",
    "start": "4902620",
    "end": "4910130"
  },
  {
    "text": "So you have a deeper\nunderstanding than that. And it turns out that\nsometimes you need",
    "start": "4910130",
    "end": "4915550"
  },
  {
    "text": "to have a deeper understanding. So, back propagation doesn't\nalways work perfectly.",
    "start": "4915550",
    "end": "4921770"
  },
  {
    "text": "And so understanding\nwhat it's really doing can be crucial to\ndebugging things. And so we'll actually\nsee an example of that",
    "start": "4921770",
    "end": "4928570"
  },
  {
    "text": "fairly soon when\nwe start looking at recurrent models and some\nof the problems that they have,",
    "start": "4928570",
    "end": "4934489"
  },
  {
    "text": "which will require us to think\na bit more deeply about what's happening in our\ngradient computations.",
    "start": "4934490",
    "end": "4940579"
  },
  {
    "text": "OK. That's it for today. ",
    "start": "4940580",
    "end": "4949000"
  }
]