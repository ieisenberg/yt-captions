[
  {
    "text": "Okay. Welcome back, everyone. Let's get started. So we're in the fifth lecture",
    "start": "5150",
    "end": "13500"
  },
  {
    "text": "today and today we're going to switch over to classification. And before we switch over, let's do a quick recap of what we've covered.",
    "start": "13500",
    "end": "22090"
  },
  {
    "text": "We covered prerequisites and we started with, um, supervised learning.",
    "start": "22220",
    "end": "28484"
  },
  {
    "text": "Uh, and supervised learning is basically learning a mapping from X to Y.",
    "start": "28485",
    "end": "33630"
  },
  {
    "text": "And we started with regression and looked at the very first, uh, regression algorithm, linear regression,",
    "start": "33630",
    "end": "39585"
  },
  {
    "text": "where we're trying to learn a mapping from some example, X, which lives in a d-dimensional space to an output y,",
    "start": "39585",
    "end": "47225"
  },
  {
    "text": "which is, a real value. And we assumed, or we limited ourselves to hypotheses or, you know,",
    "start": "47225",
    "end": "55010"
  },
  {
    "text": "models which belong to this family of the form theta transpose X, where theta is some parameter that",
    "start": "55010",
    "end": "61895"
  },
  {
    "text": "identifies the specific member of our hypothesis family. And we wanted or- or the desired outcome is X- um,",
    "start": "61895",
    "end": "71325"
  },
  {
    "text": "X theta or, um, here X is a design matrix and Y is the vector of all- all- all- all outputs.",
    "start": "71325",
    "end": "77855"
  },
  {
    "text": "We want X theta to be as close as possible to Y. But in general, this will never hold true.",
    "start": "77855",
    "end": "85134"
  },
  {
    "text": "X- X theta will almost surely never be exactly equal to Y.",
    "start": "85135",
    "end": "90575"
  },
  {
    "text": "And the- the, uh, method that we, uh, fall back to is to define a cost function,",
    "start": "90575",
    "end": "97385"
  },
  {
    "text": "which is the difference between X theta and Y, X theta minus Y. And that is going to be a vector,",
    "start": "97385",
    "end": "103430"
  },
  {
    "text": "and you take the squared norm of it. And the squared norm is basically exactly equal to the sum of the squares of the individual terms,",
    "start": "103430",
    "end": "111034"
  },
  {
    "text": "or the sum of the, um, uh, errors of each example, right? And we want to minimize this sum of squared errors by choosing an appropriate theta.",
    "start": "111035",
    "end": "120880"
  },
  {
    "text": "Okay? And we saw two different kinds of solutions for that,",
    "start": "120880",
    "end": "127235"
  },
  {
    "text": "a numerical solution and a close form solution. So in- in numerical solutions again, we saw two different methods;",
    "start": "127235",
    "end": "132710"
  },
  {
    "text": "gradient descent and stochastic gradient descent. The key difference between the two is with gradient descent,",
    "start": "132710",
    "end": "138050"
  },
  {
    "text": "we consider all examples for every step. So these are iterative algorithms where at each step we start",
    "start": "138050",
    "end": "145010"
  },
  {
    "text": "with a random initialization of theta and each- at each step, time-step, we make some change to theta,",
    "start": "145010",
    "end": "150965"
  },
  {
    "text": "updated by some amount. And in gradient descent, for each update we consider all the examples.",
    "start": "150965",
    "end": "156740"
  },
  {
    "text": "And for, uh, stochastic gradient descent, we consider, uh, one example at a time.",
    "start": "156740",
    "end": "161750"
  },
  {
    "text": "And the way we perform the update is that for the next time step, the theta for the next time step is going to be theta",
    "start": "161750",
    "end": "168739"
  },
  {
    "text": "of the current time step minus alpha, which is some step size, times the gradient of the loss.",
    "start": "168740",
    "end": "175700"
  },
  {
    "text": "And this happens to be the gradient for the squared loss. Um, I probably forgot a half over here for this to be the gradient.",
    "start": "175700",
    "end": "185005"
  },
  {
    "text": "And, um, another way of writing the same thing is, uh, in this notation we have different variables for theta T and, uh, theta T plus 1.",
    "start": "185005",
    "end": "193615"
  },
  {
    "text": "But if you're implementing this in a computer program, you would have some variable, uh,",
    "start": "193615",
    "end": "199190"
  },
  {
    "text": "for the parameters and you keep updating it over and over. So when- when, uh, we use the notation of colon equal 2,",
    "start": "199190",
    "end": "205480"
  },
  {
    "text": "it mea- it means you're performing an in-place update of the variable, which makes more sense if you think about a computer algorithm.",
    "start": "205480",
    "end": "213115"
  },
  {
    "text": "Whereas this makes more sense in as a mathematical expression. So, uh, you update theta in place by the current theta plus alpha times,",
    "start": "213115",
    "end": "225250"
  },
  {
    "text": "uh, the negative gradient, right? These two are exactly the same. Uh, I switched- uh,",
    "start": "225250",
    "end": "230905"
  },
  {
    "text": "I changed the minus to a plus and therefore swapped the order. And theta transpose x is,",
    "start": "230905",
    "end": "236975"
  },
  {
    "text": "um, is basically the hypothesis. Um, so, you know, um,",
    "start": "236975",
    "end": "242670"
  },
  {
    "text": "this is an important, uh, equation to kind of, you know, pin it in your memory.",
    "start": "242670",
    "end": "248700"
  },
  {
    "text": "We're gonna, you know, um, uh, see this pattern, uh, over and over again. And then there is the other solution,",
    "start": "248700",
    "end": "254990"
  },
  {
    "text": "which is the closed form solution. It exists for linear regression, but it may not exist for different kinds of,",
    "start": "254990",
    "end": "260780"
  },
  {
    "text": "um, models or algorithms, uh, in which case you'd use gradient descent or stochastic gradient descent.",
    "start": "260780",
    "end": "266075"
  },
  {
    "text": "But for the, uh, linear regression alone, um, you can- you can, um,",
    "start": "266075",
    "end": "271210"
  },
  {
    "text": "start with the cost function, do some matrix calculus,, you know, take the derivative, set it equal to 0,",
    "start": "271210",
    "end": "277430"
  },
  {
    "text": "and you end up with the normal equations which you can invert to get the closed form update for calculating theta.",
    "start": "277430",
    "end": "285350"
  },
  {
    "text": "This is not an iterative algorithm. You calculate this and bam, you have the final solution.",
    "start": "285350",
    "end": "291230"
  },
  {
    "text": "You don't have to iterate over and over. Whereas with, um, numerical solutions, you want to iterate over this over and over",
    "start": "291230",
    "end": "297665"
  },
  {
    "text": "until you hit some kind of a convergence condition, which means your theta has stopped changing a lot or your cost has stopped, uh, um, reducing.",
    "start": "297665",
    "end": "305320"
  },
  {
    "text": "Um, so there are- there are different kinds of, uh, um, convergence conditions we discussed last- uh, in the last class.",
    "start": "305320",
    "end": "311440"
  },
  {
    "text": "Uh, but this would be iterated until you hit a convergence condition. But this is just a one-step algorithm",
    "start": "311440",
    "end": "317680"
  },
  {
    "text": "where you- where you calculate the final solutions and you're done. Okay. And we also, um,",
    "start": "317680",
    "end": "323310"
  },
  {
    "text": "looked at a couple of interpretations of linear regression. The first is if we assume a Gaussian noise,",
    "start": "323310",
    "end": "330385"
  },
  {
    "text": "uh, for each example, which has mean 0 and some constant variance sigma squared,",
    "start": "330385",
    "end": "339415"
  },
  {
    "text": "which actually does not matter for our calculation because we assume it to be a constant. It could be a big variance,",
    "start": "339415",
    "end": "345370"
  },
  {
    "text": "small variance. It doesn't matter. As long as it's a constant variance for all examples, um, uh, we're good.",
    "start": "345370",
    "end": "352455"
  },
  {
    "text": "Uh, and we assume that Y is, uh, the- the observed Y is some true Y.",
    "start": "352455",
    "end": "359729"
  },
  {
    "text": "You know, this is some kind of a true Y plus some noise. So, right, uh, you can- you can kind of think- think of this as",
    "start": "359730",
    "end": "367710"
  },
  {
    "text": "the signal and this is the noise, right? And you- you are making a noisy observation.",
    "start": "367710",
    "end": "374945"
  },
  {
    "text": "Right? And the goal of, um, linear regression is to- given X's and Y's,",
    "start": "374945",
    "end": "381640"
  },
  {
    "text": "we don't know what the individual noise terms are. Given X and Y, we want to recover theta.",
    "start": "381640",
    "end": "387290"
  },
  {
    "text": "Right? We wanted to kind of weed out all the noise and extract the signal. That's- that's the, um, um, general intuition.",
    "start": "387290",
    "end": "393280"
  },
  {
    "text": "And with this assumption, we get a, uh, likelihood expression, uh, P of Y given X or- or the density for Y,",
    "start": "393280",
    "end": "399820"
  },
  {
    "text": "um, given X and theta to be this Gaussian distribution. And when we take the log likelihood,",
    "start": "399820",
    "end": "406310"
  },
  {
    "text": "we saw that, uh, even though the Gaussian distribution has all these terms, um, when you take the log,",
    "start": "406310",
    "end": "412935"
  },
  {
    "text": "kinda, you know, all of these are constants. The log and the exponent cancel and this is constant for all examples and this is just another constant.",
    "start": "412935",
    "end": "419690"
  },
  {
    "text": "So the log-likelihood essentially boils down to just the squared error between- between- um,",
    "start": "419690",
    "end": "426830"
  },
  {
    "text": "between the prediction and the observed noise. And we perform maximum likelihood up- um, uh, estimation.",
    "start": "426830",
    "end": "433325"
  },
  {
    "text": "Uh, we sum- we've seen maximum likelihood a couple of times, uh, so far. Uh, maximum likelihood is basically take the- the- uh,",
    "start": "433325",
    "end": "441350"
  },
  {
    "text": "define the likelihood across all the examples. And because of the independence assumption, you can, you know, the- the, uh,",
    "start": "441350",
    "end": "449030"
  },
  {
    "text": "joint likelihood is just going to be the product of this term across all examples.",
    "start": "449030",
    "end": "454355"
  },
  {
    "text": "And then take the gradient of the log likelihood with respect to theta, set it equal to 0.",
    "start": "454355",
    "end": "460580"
  },
  {
    "text": "Or you can solve it with, um, uh, um, gradient descent. Right? So the- the, um,",
    "start": "460580",
    "end": "467690"
  },
  {
    "text": "and what we saw was that performing maximum likelihood, uh, estimation was exactly the same as minimizing the squared error loss.",
    "start": "467690",
    "end": "475010"
  },
  {
    "text": "Right? There's a negative term here. So maximizing the likelihood is the same as minimizing the squared error loss if you assume your noise to be Gaussian.",
    "start": "475010",
    "end": "483185"
  },
  {
    "text": "Right? Um, so in- in general, if you are defining your objective to be the maximum likelihood objective, right,",
    "start": "483185",
    "end": "492919"
  },
  {
    "text": "it has a one-to-one correspondence with having a loss, where the loss is equal to the negative log likelihood.",
    "start": "492920",
    "end": "498530"
  },
  {
    "text": "I take the log likelihood, flip the sign, and that is your loss function. Okay. And in this case, you take the, uh, uh,",
    "start": "498530",
    "end": "505875"
  },
  {
    "text": "likelihood, take the log, flip the sign, you just get the squared error. And this holds true for any probabilistic model,",
    "start": "505875",
    "end": "513130"
  },
  {
    "text": "not just, uh, one that assumes Gaussian noise. It could be any, um, um, any probabilistic model whatsoever.",
    "start": "513130",
    "end": "520180"
  },
  {
    "text": "You define the likelihood, take the log of it, flip the sign. That's the loss function. Okay? Now, we saw yet another interpretation,",
    "start": "520180",
    "end": "527500"
  },
  {
    "text": "uh, the projection interpretation, where, um, we want to s- we would- we- we want X theta to be equal to Y.",
    "start": "527500",
    "end": "534805"
  },
  {
    "text": "But that's never the case, uh, because Y lives in a higher dimensional space compared to theta.",
    "start": "534805",
    "end": "543035"
  },
  {
    "text": "Right? And theta will map to some- some subspace in- in- in this whitespace,",
    "start": "543035",
    "end": "549524"
  },
  {
    "text": "which is- which is, uh, you know, which goes- goes through the origin, which is also the column space of X.",
    "start": "549525",
    "end": "555080"
  },
  {
    "text": "And the observed Y will almost surely never live exactly on the subspace because we are adding some noise.",
    "start": "555080",
    "end": "562774"
  },
  {
    "text": "If the noise was 0, then, you know, you would expect your Y to be some linear combination of X's.",
    "start": "562775",
    "end": "568190"
  },
  {
    "text": "But because there is some noise, your Y will be, um, will be outside the subspace.",
    "start": "568190",
    "end": "573885"
  },
  {
    "text": "Right? So instead what we wanna do is project this Y onto the subspace.",
    "start": "573885",
    "end": "580025"
  },
  {
    "text": "Let's call it Y hat, the projected point. Right? And once you project it, you can actually solve,",
    "start": "580025",
    "end": "586514"
  },
  {
    "text": "um, X theta, you know, exactly equal to Y because now it's projected onto the column space. Right? And the way you project it is to take Y and multiply it by the projection matrix.",
    "start": "586515",
    "end": "597320"
  },
  {
    "text": "And this- this- this whole term will be Y hat, the projected point. And now X theta will- will actually be exactly equal to this- this,",
    "start": "597320",
    "end": "606685"
  },
  {
    "text": "um, uh, projected point. And, you know, because, you know, um, because they're exactly the same,",
    "start": "606685",
    "end": "613190"
  },
  {
    "text": "you can- you can, uh, cancel X and you get theta equals X transpose X inverse X transpose Y,",
    "start": "613190",
    "end": "618200"
  },
  {
    "text": "which is- which was the normal equation, what we saw there. Right? So that's what we covered.",
    "start": "618200",
    "end": "624384"
  },
  {
    "text": "Uh, any questions before we move on to classification? Yes, question.",
    "start": "624385",
    "end": "631290"
  },
  {
    "text": "How can we be sure that the X matrix is invertible? So in this case, how do we know X,",
    "start": "632440",
    "end": "637654"
  },
  {
    "text": "um, X matrix is invertible? For- for now, we will just assume X matrix is invertible an,",
    "start": "637655",
    "end": "644319"
  },
  {
    "text": "you know, we'll come up with, uh, remedies for later in the course when it's not. So for now, let's assume X matrix is invertible. Yes, question.",
    "start": "644320",
    "end": "653029"
  },
  {
    "text": "[inaudible]",
    "start": "653030",
    "end": "658415"
  },
  {
    "text": "So the question is, if the observations are not independent, does this- does this, um, um, um, theory all hold?",
    "start": "658415",
    "end": "664855"
  },
  {
    "text": "Um, if the observations are not independent, none of this theory holds, right? All this- all this theory is- is making, uh,",
    "start": "664855",
    "end": "672430"
  },
  {
    "text": "an independent assumption, IID assumption, right? Only because of which you can, you know, multiply them together, take the log,",
    "start": "672430",
    "end": "678430"
  },
  {
    "text": "decompose it into separate sums. Uh, if they're not independent, it becomes much more, uh, complex, uh, to analyze.",
    "start": "678430",
    "end": "685570"
  },
  {
    "text": "However, in practice, um, uh, even if examples are related,",
    "start": "685570",
    "end": "691029"
  },
  {
    "text": "they tend to work well, uh, except, um, you know, that the intuition to have is that if",
    "start": "691030",
    "end": "696475"
  },
  {
    "text": "your examples are not independent and somewhat correlated, your, um, one intuition to have is that your effective dataset sizes become smaller.",
    "start": "696475",
    "end": "705834"
  },
  {
    "text": "You know what- you know, think about it. If you have a dataset of 100 examples, where all the 100 examples are the copy- exact copies of each other, right?",
    "start": "705835",
    "end": "713470"
  },
  {
    "text": "Uh, even though, you know, you- you- you have 100 examples, they are not independent, they are all exactly the same and your effective data size is just 1, right?",
    "start": "713470",
    "end": "720279"
  },
  {
    "text": "So similarly, if- if- if your examples are not independent, your effective data size is kind of, um- is kind of reduced and- and, yes,",
    "start": "720280",
    "end": "727780"
  },
  {
    "text": "the theory also does not hold. In practice, you know, think of it [NOISE] as a- a smaller data-set.",
    "start": "727780",
    "end": "732470"
  },
  {
    "text": "It is much harder and- and most of the theories,",
    "start": "735750",
    "end": "740965"
  },
  {
    "text": "um, um, kind of assume you have independent, um, um observations. However, uh, when we go later in the course, for example,",
    "start": "740965",
    "end": "747970"
  },
  {
    "text": "reinforcement learning, they are actually, you know, across a time step that is clearly, you know, there is dependence, you know,",
    "start": "747970",
    "end": "753865"
  },
  {
    "text": "each- at each time step for example,  playing a game of chess what you do in the next move depends a lot on what you did in the previous move,",
    "start": "753865",
    "end": "759100"
  },
  {
    "text": "they are actually not, um, um. And in those cases, yes we- we kind of give up the IID assumption and you get,",
    "start": "759100",
    "end": "765865"
  },
  {
    "text": "you know, different kinds of theories there. Yes question. [BACKGROUND]",
    "start": "765865",
    "end": "773139"
  },
  {
    "text": "I- I'm sorry?",
    "start": "773140",
    "end": "774650"
  },
  {
    "text": "So the question is, are there cases where you can- you cannot use numerical examples?",
    "start": "785310",
    "end": "790870"
  },
  {
    "text": "Well, so generally numerical solutions always exist,",
    "start": "790870",
    "end": "796185"
  },
  {
    "text": "um, the- the close form solutions may or may not exist. That's generally the case. All right. Let's move on.",
    "start": "796185",
    "end": "803395"
  },
  {
    "text": "So today- the plan for today is we're going to cover- today,",
    "start": "803395",
    "end": "814644"
  },
  {
    "text": "we're going to cover the perceptron algorithm. And this corresponds to section 5 in the note- section 6 in the notes.",
    "start": "814645",
    "end": "825880"
  },
  {
    "text": "And then we will cover logistic regression,",
    "start": "825880",
    "end": "830120"
  },
  {
    "text": "this is section 5 and",
    "start": "832980",
    "end": "839230"
  },
  {
    "text": "Newton's method, that's section 7.",
    "start": "839230",
    "end": "846740"
  },
  {
    "text": "And time permitting, we will, um - time permitting we will- we will try to,",
    "start": "846900",
    "end": "853915"
  },
  {
    "text": "uh, uh, do a review of some ideas from functional analysis.",
    "start": "853915",
    "end": "857870"
  },
  {
    "text": "Basically as a way to kind of prepare ourselves for some of the future topics.",
    "start": "860190",
    "end": "865195"
  },
  {
    "text": "There is, uh, also one, uh, section in the notes that is, uh, locally weighted regression, that's section 4.",
    "start": "865195",
    "end": "872485"
  },
  {
    "text": "Yeah. We might revisit this later, but for now we're going to, um, you know, just keep moving. Uh, Right, yeah. So that's the plan for today.",
    "start": "872485",
    "end": "881515"
  },
  {
    "text": "And we'll start with the perceptron algorithm.",
    "start": "881515",
    "end": "885290"
  },
  {
    "text": "So the perceptron algorithm is a very simple classification algorithm where you're given, um, examples, um,",
    "start": "887220",
    "end": "896440"
  },
  {
    "text": "in let's R^d, and the correct answer or the label of the target,",
    "start": "896440",
    "end": "902900"
  },
  {
    "text": "it's either 0 or 1 for each example. Right? This- this- this algorithm we,",
    "start": "904320",
    "end": "913584"
  },
  {
    "text": "um- we study it, um, mostly because of historical interest, uh,",
    "start": "913585",
    "end": "919089"
  },
  {
    "text": "and also because it's very easy to analyze and to build some good intuitions. Right? In practice, um,",
    "start": "919090",
    "end": "925660"
  },
  {
    "text": "I don't know anybody who uses perceptron algorithm in practice, but it's- it's as I said,",
    "start": "925660",
    "end": "930730"
  },
  {
    "text": "it's- it's- it's a very simple algorithm to kind of build intuitions of how classif- most- many classification algorithms work.",
    "start": "930730",
    "end": "938725"
  },
  {
    "text": "And in the perceptron algorithm, the hypothesis,",
    "start": "938725",
    "end": "944620"
  },
  {
    "text": "h theta of x equal to g of theta transpose x,",
    "start": "944620",
    "end": "952420"
  },
  {
    "text": "where g of, say, some variable z is equal to 1",
    "start": "952420",
    "end": "960985"
  },
  {
    "text": "if z is greater than 0 and it is 0 if z is less than 0.",
    "start": "960985",
    "end": "969279"
  },
  {
    "text": "All right? So g is some function. Think of it like this.",
    "start": "969280",
    "end": "974990"
  },
  {
    "text": "This is z, this is 0, g looks like this,",
    "start": "975920",
    "end": "983870"
  },
  {
    "text": "g takes the value 0 until it's- if the input is less than or equal to 0 and it is 1, you know.",
    "start": "983870",
    "end": "994100"
  },
  {
    "text": "Yeah, so, and you can- you can actually define it as less than or equal to or greater than or equal to.",
    "start": "998730",
    "end": "1004050"
  },
  {
    "text": "It doesn't matter. Le- let's- so let's, um- let's see how it's in the notes. I think in the notes, it is greater than equal to,",
    "start": "1004050",
    "end": "1010545"
  },
  {
    "text": "um, anyway, it doesn't matter. Let- let's assume it is greater than or equal to. Uh, which means, um,",
    "start": "1010545",
    "end": "1016605"
  },
  {
    "text": "at- at 0 it is actually 1 and right.",
    "start": "1016605",
    "end": "1026520"
  },
  {
    "text": "So what's happening here? The hypothesis for the perceptron algorithm has some parameter theta, right?",
    "start": "1026520",
    "end": "1034650"
  },
  {
    "text": "And given a vector, an input- input vector x, we do, uh, theta transpose x, you know,",
    "start": "1034650",
    "end": "1040725"
  },
  {
    "text": "just like in- in the case of, uh, linear regression, where the output is then- so",
    "start": "1040725",
    "end": "1047010"
  },
  {
    "text": "theta transpose x can- can take on any value- any real value between minus infinity and plus infinity, right? It's- it's just you take two vectors and take the dot product.",
    "start": "1047010",
    "end": "1054315"
  },
  {
    "text": "It can be any value whatsoever. Now, um, we take this, uh, you know,",
    "start": "1054315",
    "end": "1059940"
  },
  {
    "text": "real valued number and map it to a value- you know,",
    "start": "1059940",
    "end": "1065250"
  },
  {
    "text": "map it to either 0 or 1, okay? If- if theta transpose x is less than 0 we map it to 0.",
    "start": "1065250",
    "end": "1072870"
  },
  {
    "text": "If it is greater than or equal to 0, we map- map it to 1. Okay? And with this hypothesis,",
    "start": "1072870",
    "end": "1081179"
  },
  {
    "text": "the, um, algorithm looks something like this. So, um, the perceptron algorithm is also called,",
    "start": "1081180",
    "end": "1089115"
  },
  {
    "text": "um, a streaming algorithm, uh, which means it is designed to work in",
    "start": "1089115",
    "end": "1094710"
  },
  {
    "text": "a setting where you're encountering examples one after another. You don't have access to all the examples upfront.",
    "start": "1094710",
    "end": "1101519"
  },
  {
    "text": "You're- you're- you're just given one example at a time and you need to update your model and then wait for the next example,",
    "start": "1101520",
    "end": "1107205"
  },
  {
    "text": "and then update your model and then wait for the next example and so on. And the, um, algorithm for training the perceptron looks something like this.",
    "start": "1107205",
    "end": "1115050"
  },
  {
    "text": "So, um- so set your theta to be,",
    "start": "1115050",
    "end": "1122415"
  },
  {
    "text": "uh, you know, some initialization. So generally it is just the 0 vector.",
    "start": "1122415",
    "end": "1127830"
  },
  {
    "text": "That works totally fine and for i in 1,",
    "start": "1127830",
    "end": "1136380"
  },
  {
    "text": "2, and so on, you know, um, just- just the stream of- of examples that are coming at you.",
    "start": "1136380",
    "end": "1142140"
  },
  {
    "text": "You- you perform theta equals theta plus alpha",
    "start": "1142140",
    "end": "1151275"
  },
  {
    "text": "times y^i minus h theta",
    "start": "1151275",
    "end": "1157200"
  },
  {
    "text": "of x^i times x^i.",
    "start": "1157200",
    "end": "1165990"
  },
  {
    "text": "Okay, what's happening here? So first we see that the update rule for the perceptron looks very similar to the update rule of linear regression.",
    "start": "1165990",
    "end": "1175170"
  },
  {
    "text": "Right? But they're not actually the same. What's different between the two?",
    "start": "1175170",
    "end": "1179950"
  },
  {
    "text": "H theta of x is different. They- they- they look kind of superficially similar, uh,",
    "start": "1181370",
    "end": "1187650"
  },
  {
    "text": "where given the hypo- or the hypothesized value or the predicted value and the, uh, actual value.",
    "start": "1187650",
    "end": "1195435"
  },
  {
    "text": "The update looks- rules look very similar, but the hypotheses are actually different.",
    "start": "1195435",
    "end": "1200520"
  },
  {
    "text": "Right? In linear regression, your hypothesis was just theta transpose x.",
    "start": "1200520",
    "end": "1206190"
  },
  {
    "text": "Here it is g of theta transpose x. And the- there is- there is",
    "start": "1206190",
    "end": "1214500"
  },
  {
    "text": "theory to prove that if your data-set is actually linearly separable,",
    "start": "1214500",
    "end": "1219675"
  },
  {
    "text": "then this algorithm will find that separation point, your hypothesis will be able to classify all the points.",
    "start": "1219675",
    "end": "1227550"
  },
  {
    "text": "It's- it's, um, not entirely clear how this- you know, uh, just looking at this form,",
    "start": "1227550",
    "end": "1232730"
  },
  {
    "text": "it may not be entirely clear how the- how this, uh, works or why- why it would even work. Right? Uh, for this let's- let's,",
    "start": "1232730",
    "end": "1240110"
  },
  {
    "text": "uh, try to visualize this, okay? So let's assume this to be,",
    "start": "1240110",
    "end": "1247370"
  },
  {
    "text": "uh, you know, x1 to xd. Again, in all these examples, um,",
    "start": "1247370",
    "end": "1253310"
  },
  {
    "text": "I'm assuming your, uh, x to include the intercept term, right?",
    "start": "1253310",
    "end": "1258750"
  },
  {
    "text": "Uh, so assume it to be d plus 1 or in general, it is fine to assume, you know,",
    "start": "1258750",
    "end": "1264150"
  },
  {
    "text": "x in- in d where d just includes the intercept term and your true data was d minus 1 dimension, right?",
    "start": "1264150",
    "end": "1269985"
  },
  {
    "text": "We don't- we don't stress about that too much. So assume the intercept term is, um- is included here.",
    "start": "1269985",
    "end": "1276375"
  },
  {
    "text": "And let's assume your theta vector has some current value.",
    "start": "1276375",
    "end": "1281820"
  },
  {
    "text": "So let's- let's- let's, um- let me use a different color. Let's assume your theta vector to be here.",
    "start": "1281820",
    "end": "1289540"
  },
  {
    "text": "So let's- let's call this Theta of t. Your Theta vector is currently here.",
    "start": "1291440",
    "end": "1298440"
  },
  {
    "text": "Now, it's obvious looking at this formulation that what really matters is for a given new value of x,",
    "start": "1298440",
    "end": "1309120"
  },
  {
    "text": "whether Theta transpose x is greater than 0 or less than 0. The actual value, you know,",
    "start": "1309120",
    "end": "1315435"
  },
  {
    "text": "actually, it doesn't matter. What really matters is whether it's just greater than or equal to 0 or less than 0, because it just gets mapped to 1 and 0,",
    "start": "1315435",
    "end": "1321960"
  },
  {
    "text": "no matter how big- how- how larger than 0 it was or you know, how little, um, uh,",
    "start": "1321960",
    "end": "1327299"
  },
  {
    "text": "bigger than 0 it was. You know, similarly, if- you know, if- if Theta transpose x is here or here, actually, that maps to 0.",
    "start": "1327300",
    "end": "1332865"
  },
  {
    "text": "Okay. So what- what really matters is- is the- is- is Theta transpose x greater than 0 or less than 0?",
    "start": "1332865",
    "end": "1339300"
  },
  {
    "text": "Which means Theta transpose x equal to 0 kind of forms a hyperplane that is meant to separate your data- your, uh, data points,",
    "start": "1339300",
    "end": "1348090"
  },
  {
    "text": "you want all the points, all the- all the points that have y equal to 1 to satisfy the requirement that Theta transpose x is,",
    "start": "1348090",
    "end": "1357120"
  },
  {
    "text": "you know, bigger than 0 for them and- and- and, um, less than 0 for those that have y equal to 0.",
    "start": "1357120",
    "end": "1362670"
  },
  {
    "text": "[NOISE] Now, if Theta is this vector, Theta transpose x is the hyperplane,",
    "start": "1362670",
    "end": "1372495"
  },
  {
    "text": "that is, this- this hyperplane corresponds to Theta transpose x equal to 0, right?",
    "start": "1372495",
    "end": "1383490"
  },
  {
    "text": "And because Theta is oriented this way, all points in- on this side satisfy,",
    "start": "1383490",
    "end": "1390585"
  },
  {
    "text": "uh, Theta transpose x greater than 0. And all x's on- on the other side satisfy",
    "start": "1390585",
    "end": "1398505"
  },
  {
    "text": "Theta transpose x is less than 0. Is this clear?",
    "start": "1398505",
    "end": "1403740"
  },
  {
    "text": "Is- is this- is this, you know, it- um, the idea here is that, you know,",
    "start": "1403740",
    "end": "1409080"
  },
  {
    "text": "any x that lies on this- this- this, uh, line is perpendicular to Theta. So Theta transpose x will be equal to 0, right?",
    "start": "1409080",
    "end": "1416279"
  },
  {
    "text": "Um, and if you have an x over here, you know, the inner product between them is- is,",
    "start": "1416280",
    "end": "1424505"
  },
  {
    "text": "you know, it's basically less than 90 degrees. So Theta transpose x will be positive. If- if you have an x over here, you know,",
    "start": "1424505",
    "end": "1430205"
  },
  {
    "text": "the angle between Theta and x is more than 90, so Theta transpose x should be negative, right? And the line that's exactly perpendicular to Theta is- is- is like",
    "start": "1430205",
    "end": "1437270"
  },
  {
    "text": "the decision boundary. Is- is this clear? Okay. Right. Now, another intuition that we",
    "start": "1437270",
    "end": "1446040"
  },
  {
    "text": "want to- we want to have is let's assume you have two vectors, a and b.",
    "start": "1446040",
    "end": "1451335"
  },
  {
    "text": "Right. Here we go. This is vector a,",
    "start": "1451335",
    "end": "1457065"
  },
  {
    "text": "and let's say this is, um, vector b. Okay? And a transpose b equals some value.",
    "start": "1457065",
    "end": "1467235"
  },
  {
    "text": "Right? In this case, suppose a- you know, in this case, a transpose b is- is- is some value less than 0, [NOISE] right?",
    "start": "1467235",
    "end": "1473325"
  },
  {
    "text": "And we desire, let- let's as- assume, you know, this was, you know, our Theta and this was some x.",
    "start": "1473325",
    "end": "1480480"
  },
  {
    "text": "And let's say this was an x whose y label was plus 1. Okay. We want Theta transpose x to be- to be positive, right?",
    "start": "1480480",
    "end": "1489825"
  },
  {
    "text": "Now, however, Theta transpose x is a negative number because it is- it is,",
    "start": "1489825",
    "end": "1495075"
  },
  {
    "text": "um, greater than 90 degrees. And if the desired value of a transpose b or",
    "start": "1495075",
    "end": "1503655"
  },
  {
    "text": "a dot b is bigger than the current value of a dot b,",
    "start": "1503655",
    "end": "1510240"
  },
  {
    "text": "is there a way in which we can- we can modify one of the two vectors to make the desired value larger?",
    "start": "1510240",
    "end": "1518200"
  },
  {
    "text": "How do we make Theta transpose, um, um, you know, a dot b achieve a bigger value,",
    "start": "1518420",
    "end": "1525615"
  },
  {
    "text": "is- is- is basically the question. So one simple, um, answer for that is take the vector a, right?",
    "start": "1525615",
    "end": "1534990"
  },
  {
    "text": "In this case, you know, think of a as your Theta vector and you have a vector, uh, x.",
    "start": "1534990",
    "end": "1540375"
  },
  {
    "text": "What we can do is take a small- a small amount of x and add it to a,",
    "start": "1540375",
    "end": "1549315"
  },
  {
    "text": "which means at- at the- at the- at the end of a, let's call this Alpha times x.",
    "start": "1549315",
    "end": "1559725"
  },
  {
    "text": "Right. So x is a vector, x is a vector. Now, a plus Alpha times x. Oh,",
    "start": "1559725",
    "end": "1570395"
  },
  {
    "text": "I'm sorry, let's call it b, so Alpha times b. [NOISE]",
    "start": "1570395",
    "end": "1583760"
  },
  {
    "text": "a plus Alpha times b is this vector, right?",
    "start": "1583760",
    "end": "1589995"
  },
  {
    "text": "This is a plus Alpha times b, [NOISE] right?",
    "start": "1589995",
    "end": "1597225"
  },
  {
    "text": "And we notice that by adding some amount of b to a, the angle between them got closer, right?",
    "start": "1597225",
    "end": "1605414"
  },
  {
    "text": "Which means you can expect a dot b to be, um, a dot b plus Alpha times",
    "start": "1605415",
    "end": "1615375"
  },
  {
    "text": "a will be a transpose b plus Alpha times a transpose a.",
    "start": "1615375",
    "end": "1622995"
  },
  {
    "text": "And a transpose a is always positive which means this will be greater than equal to a transpose b, right?",
    "start": "1622995",
    "end": "1630674"
  },
  {
    "text": "So if we- if- if we have two vectors, x and Theta or a and b, and we want the dot product between them to be bigger than what it currently is,",
    "start": "1630675",
    "end": "1641820"
  },
  {
    "text": "one way to achieve it is to take one of the vectors and add a small amount of that vector to the other vector, right?",
    "start": "1641820",
    "end": "1651135"
  },
  {
    "text": "Similarly, if the dot product between two vectors is bigger than what you would like it to be,",
    "start": "1651135",
    "end": "1658695"
  },
  {
    "text": "you can take a small amount of one of the vectors and subtract it from the other vector, right?",
    "start": "1658695",
    "end": "1665130"
  },
  {
    "text": "And you know, here, you just flip the sign and a transpose a will always be, um, a positive. And so a negative value of a transpose a is- is going to be negative",
    "start": "1665130",
    "end": "1672990"
  },
  {
    "text": "and that will make the- the- the- the updated vector be smaller. So what- what does that- what does that, uh,",
    "start": "1672990",
    "end": "1679785"
  },
  {
    "text": "mean in, with respect to our equation, update rule? What we say is, suppose y is 1 and the current value of h Theta x is also 1, right?",
    "start": "1679785",
    "end": "1692610"
  },
  {
    "text": "Which means current- according to the current, um, um, current parameter, we are correctly classifying ith example,",
    "start": "1692610",
    "end": "1700680"
  },
  {
    "text": "then this will be 0. And you don't modify Theta at all which",
    "start": "1700680",
    "end": "1706290"
  },
  {
    "text": "means if you encounter an example that's already correctly classified, don't do anything, okay?",
    "start": "1706290",
    "end": "1711630"
  },
  {
    "text": "1 minus 1 is 0. Similarly, if the current example is- has labeled",
    "start": "1711630",
    "end": "1717720"
  },
  {
    "text": "zero and you're outputting your h Theta of- of x output 0, you know, don't do anything.",
    "start": "1717720",
    "end": "1723840"
  },
  {
    "text": "So if you encounter an example that's already been correctly classified by your half trained model, don't do anything.",
    "start": "1723840",
    "end": "1730304"
  },
  {
    "text": "However, if you encounter an example whose correct label is 1 but we are outputting 0,",
    "start": "1730305",
    "end": "1738615"
  },
  {
    "text": "then this will be 1 minus 0 equals 1. So add some amount of x to Theta.",
    "start": "1738615",
    "end": "1745275"
  },
  {
    "text": "If you encounter an example whose true label is 1, but we're outputting 0,",
    "start": "1745275",
    "end": "1750450"
  },
  {
    "text": "then add some amount of that vector to Theta, right?",
    "start": "1750450",
    "end": "1755820"
  },
  {
    "text": "Similarly, if the correct label is 0 and we're outputting 1, which means we're making a mistake but a mistake in the other way,",
    "start": "1755820",
    "end": "1761895"
  },
  {
    "text": "then 0 minus 1 is minus 1. So subtract some amount of that vector from the current parameter Theta, right?",
    "start": "1761895",
    "end": "1772559"
  },
  {
    "text": "If you start your Theta vector to be 0, then what we see is the Theta vector that we end up with is going to be",
    "start": "1772560",
    "end": "1780674"
  },
  {
    "text": "some linear combination of sums of your x's.",
    "start": "1780675",
    "end": "1786900"
  },
  {
    "text": "So everything that we have here, right, this is real value,",
    "start": "1786900",
    "end": "1792735"
  },
  {
    "text": "this is an R^d, and this is R^d.",
    "start": "1792735",
    "end": "1800955"
  },
  {
    "text": "So all of this is just a scalar, and the- the- the- the basis vectors",
    "start": "1800955",
    "end": "1806250"
  },
  {
    "text": "that we are getting are basically just the examples themselves, okay? So what we're doing is- is adding small portions of our example to construct",
    "start": "1806250",
    "end": "1814170"
  },
  {
    "text": "a Theta and we add them in such a way that if the current value of Theta is- is,",
    "start": "1814170",
    "end": "1820560"
  },
  {
    "text": "um, is misclassifying it, then we either add or subtract.",
    "start": "1820560",
    "end": "1826140"
  },
  {
    "text": "Um, so for example, um, let's see. So this, let's assume we encounter, you know,",
    "start": "1826140",
    "end": "1832230"
  },
  {
    "text": "some x over here which is labeled 0, [NOISE] right?",
    "start": "1832230",
    "end": "1837290"
  },
  {
    "text": "So this corresponds to y equals 0. And Theta transpose x is less than 0,",
    "start": "1837290",
    "end": "1847755"
  },
  {
    "text": "or- or Theta transpose x is going to be, um, um, greater than 0 because the angle is less than 90 degrees which means now,",
    "start": "1847755",
    "end": "1855990"
  },
  {
    "text": "we are going to add a small amount of this vector. Are we gonna add? No, we're gonna subtract because the label is- is y- y equals 0.",
    "start": "1855990",
    "end": "1866490"
  },
  {
    "text": "We're gonna subtract some amount of this vector from Theta. So what does that look like? Uh, so this is the vector.",
    "start": "1866490",
    "end": "1872760"
  },
  {
    "text": "Take a small amount of that vector and subtract it from Theta,",
    "start": "1872760",
    "end": "1879120"
  },
  {
    "text": "and [NOISE] Theta t plus 1 will- okay,",
    "start": "1879120",
    "end": "1885545"
  },
  {
    "text": "so this is Theta t plus 1. [NOISE] And the decision boundary corresponding to theta t plus 1 will be this, right?",
    "start": "1885545",
    "end": "1895125"
  },
  {
    "text": "So the example which was classified as positive under the blue hypothesis, you know, after performing that update rule,",
    "start": "1895125",
    "end": "1901260"
  },
  {
    "text": "could correctly classify it as negative under the updated hypothesis. Yes, question? [inaudible]",
    "start": "1901260",
    "end": "1910680"
  },
  {
    "text": "So why is the separating hyperplane always orthogonal to Theta? So Theta is- is- is the,",
    "start": "1910680",
    "end": "1916425"
  },
  {
    "text": "um, is the current vector, [NOISE] right? And if- if you look over here, Theta transpose x equal to 0 is like",
    "start": "1916425",
    "end": "1923460"
  },
  {
    "text": "the decision boundary because- and if Theta transpose x is- is bigger than zero, it's considered positive, and if it's less than zero it's considered negative.",
    "start": "1923460",
    "end": "1931050"
  },
  {
    "text": "So you can think of Theta transpose x equal to 0 as- as, you know, the- the- the- the- the separating boundary.",
    "start": "1931050",
    "end": "1937290"
  },
  {
    "text": "And if theta is in a particular direction, the set of all x's that give you Theta transpose x equal to 0 are",
    "start": "1937290",
    "end": "1944070"
  },
  {
    "text": "all the set of all points that are kind of perpendicular to Theta, right? Because if you take a dot product between",
    "start": "1944070",
    "end": "1949560"
  },
  {
    "text": "two perpendicular vectors, the dot product is zero. Does that make sense? Yes, question?",
    "start": "1949560",
    "end": "1956429"
  },
  {
    "text": "[inaudible]",
    "start": "1956430",
    "end": "1961745"
  },
  {
    "text": "So in practice, you never use perceptron algorithm. Like with, um, it- this- this algorithm is hardly used in practice.",
    "start": "1961745",
    "end": "1968510"
  },
  {
    "text": "Uh, however, it is very useful to analyze this algorithm and to get some- some intuitions about- about how",
    "start": "1968510",
    "end": "1975200"
  },
  {
    "text": "the update rule works because this update rule pattern is- is gonna, you know, come again and again.",
    "start": "1975200",
    "end": "1980765"
  },
  {
    "text": "Okay. Yeah, so this- this is- uh, uh, this is the perceptron algorithm.",
    "start": "1980765",
    "end": "1986929"
  },
  {
    "text": "The- uh, there is- there is theory which says if you're given a dataset where you're the, um,",
    "start": "1986930",
    "end": "1993155"
  },
  {
    "text": "x's according to the classes can actually be separated, linearly separated,",
    "start": "1993155",
    "end": "1998735"
  },
  {
    "text": "then this perceptron algorithm, which looks very simple, will actually find the separating hyperplane. Yes, question.",
    "start": "1998735",
    "end": "2005470"
  },
  {
    "text": "[inaudible]?",
    "start": "2005470",
    "end": "2013960"
  },
  {
    "text": "So the question is will we- will we need to revisit a few examples again? [inaudible]",
    "start": "2013960",
    "end": "2019450"
  },
  {
    "text": "Yeah. [inaudible] Yeah. So the question is, you know, uh, if it has 3 data points or if it has 1000 data points, does it matter?",
    "start": "2019450",
    "end": "2027894"
  },
  {
    "text": "Well, so the- so the, ah, answer there is if you have like a very small dataset, then you may want to, um, um,",
    "start": "2027895",
    "end": "2033460"
  },
  {
    "text": "iterate over them a few times, right? But if you have a large enough dataset, then, you know, generally just making one pass across",
    "start": "2033460",
    "end": "2040390"
  },
  {
    "text": "the last dataset should be sufficient. Yes, question. [inaudible]?",
    "start": "2040390",
    "end": "2051639"
  },
  {
    "text": "Yeah. So the question is, um, aren't there an infinite number of correct solutions for Theta for a given dataset?",
    "start": "2051640",
    "end": "2057129"
  },
  {
    "text": "The answer is yes, you- there- there are an infinite number of, uh, um, solutions, and this algorithm will find one solution, some solution, yeah.",
    "start": "2057130",
    "end": "2065869"
  },
  {
    "text": "Any other questions? Yes, question. [inaudible]?",
    "start": "2068880",
    "end": "2073960"
  },
  {
    "text": "Will the solution change according to the order of the examples? Yes, it can change according to the order of the examples. I- and- and that's fine, um, you know,",
    "start": "2073960",
    "end": "2081355"
  },
  {
    "text": "because we are generally interested in some solution that works. Yes, question.",
    "start": "2081355",
    "end": "2087129"
  },
  {
    "text": "[inaudible]?",
    "start": "2087130",
    "end": "2094179"
  },
  {
    "text": "That's correct. Uh, we use- you know, the question is do we use one example at a time and not all of them? Yes, this- you know, think of this as similar to SGD, you know,",
    "start": "2094180",
    "end": "2102475"
  },
  {
    "text": "take one example, perform an update, pick another example, perform an update.",
    "start": "2102475",
    "end": "2107600"
  },
  {
    "text": "Cool. So that's- that's the perceptron algorithm. Now let's move on to logistic regression.",
    "start": "2110720",
    "end": "2118230"
  },
  {
    "text": "[NOISE] Yes.",
    "start": "2118230",
    "end": "2126680"
  },
  {
    "text": "Does the order of [inaudible]? So the question is does the order matter? Um, the theory of- of- of",
    "start": "2126680",
    "end": "2132130"
  },
  {
    "text": "perceptrons tell you- tells you that if you have a large enough dataset, go through it in any order and you will find some separating- uh,",
    "start": "2132130",
    "end": "2139555"
  },
  {
    "text": "some solu- some value or solution. Um, yeah, so the order shouldn't, does not- [inaudible]?",
    "start": "2139555",
    "end": "2145330"
  },
  {
    "text": "Yeah, you'll get a different answer, but you'll still find some- some solution. [NOISE]",
    "start": "2145330",
    "end": "2151119"
  },
  {
    "text": "Why don't we use this in practice?",
    "start": "2151120",
    "end": "2156295"
  },
  {
    "text": "Why don't we use this in practice? Uh, we don't, um, use this in practice because there are many other algorithms that work better.",
    "start": "2156295",
    "end": "2165190"
  },
  {
    "text": "And, um, the perceptron algorithm, um, we- well,",
    "start": "2165190",
    "end": "2170440"
  },
  {
    "text": "if you don't mind, ask the same question, you know, remember it, ask the same question after logistic regression so that we can- can draw some cust- custom comparisons.",
    "start": "2170440",
    "end": "2177520"
  },
  {
    "text": "[NOISE]",
    "start": "2177520",
    "end": "2194860"
  },
  {
    "text": "But the key takeaway from the perceptron algorithm is this idea that if you have",
    "start": "2194860",
    "end": "2199930"
  },
  {
    "text": "two vectors and their dot product is smaller than what you would desire it to be,",
    "start": "2199930",
    "end": "2205345"
  },
  {
    "text": "then take a small amount of one of the vectors and add it to the other, right? And that's- that will make the dot product bigger.",
    "start": "2205345",
    "end": "2211180"
  },
  {
    "text": "And similarly, if the dot product is- is larger than what you would like it to be,",
    "start": "2211180",
    "end": "2216625"
  },
  {
    "text": "then take a small amount of one vector and subtract it from the other vector.",
    "start": "2216625",
    "end": "2222040"
  },
  {
    "text": "And how do we determine alpha? How do we determine alpha? Just the way you determine Alpha for-",
    "start": "2222040",
    "end": "2228009"
  },
  {
    "text": "Gradient descent. Gradient descent, right? Take a small amount It'll be fine.",
    "start": "2228010",
    "end": "2242230"
  },
  {
    "text": "[NOISE] All right. Logistic regression. [NOISE]",
    "start": "2242230",
    "end": "2260260"
  },
  {
    "text": "So to start off with logistic regression, it would not be an under- understatement to say logistic regression is the workhorse of machine learning.",
    "start": "2260260",
    "end": "2267250"
  },
  {
    "text": "It is probably one of the most commonly used algorithms in practice in production, right?",
    "start": "2267250",
    "end": "2273595"
  },
  {
    "text": "Uh, go to any company in Silicon Valley, the chances are if they have some kind of a- a classification problem,",
    "start": "2273595",
    "end": "2279940"
  },
  {
    "text": "it's way more likely that they're using logistic regression compared to any other algorithm. And- and, uh, it's- it's- it's,",
    "start": "2279940",
    "end": "2289224"
  },
  {
    "text": "uh, surprisingly effective, right? And it should probably be one of the first algorithms that",
    "start": "2289225",
    "end": "2296065"
  },
  {
    "text": "you try for a classification problem that you're trying to solve, right? Always start with logistic regression, and more likely, uh,",
    "start": "2296065",
    "end": "2302905"
  },
  {
    "text": "than not, it's just gonna, uh, work pretty well. So, um, in logistic regression,",
    "start": "2302905",
    "end": "2311935"
  },
  {
    "text": "uh, the setting is very similar to the perceptron algorithm. We have x's in R^d,",
    "start": "2311935",
    "end": "2323650"
  },
  {
    "text": "y in 0, 1.",
    "start": "2323650",
    "end": "2330849"
  },
  {
    "text": "Right. And when y^i equals 1,",
    "start": "2330850",
    "end": "2337630"
  },
  {
    "text": "we call it a positive example, [NOISE] and when y^i equal to 0,",
    "start": "2337630",
    "end": "2348400"
  },
  {
    "text": "we call it a negative example.",
    "start": "2348400",
    "end": "2350839"
  },
  {
    "text": "Okay. And this is a description of the data, it does not, you know, it- it- it- it does not- a positive example and negative example does not",
    "start": "2355110",
    "end": "2363880"
  },
  {
    "text": "mean the model is correctly classifying it or the model is negatively classifying it. It's just the- it's just the way we call the examples.",
    "start": "2363880",
    "end": "2370030"
  },
  {
    "text": "If- if the label is- is plus 1, we call it a positive example. If the label is 0, we call it a negative example.",
    "start": "2370030",
    "end": "2375865"
  },
  {
    "text": "Right. And the hypothesis for the logistic regression is gonna look something like this.",
    "start": "2375865",
    "end": "2383860"
  },
  {
    "text": "h_Theta of x is equal to g of Theta transpose x, right,",
    "start": "2383860",
    "end": "2392275"
  },
  {
    "text": "where it is again similar to the perceptron- um,",
    "start": "2392275",
    "end": "2398515"
  },
  {
    "text": "the perceptron's, uh, uh, hypothesis where h_Theta of x was equal to, um, g of, uh, Theta transpose x.",
    "start": "2398515",
    "end": "2404859"
  },
  {
    "text": "The difference is g, in this case, is defined as follows. So g of z, where z is then- you know,",
    "start": "2404860",
    "end": "2412285"
  },
  {
    "text": "think of z as the value that you get after, uh, doing a Theta transpose x, g of z is defined as 1 over 1 plus e^minus z.",
    "start": "2412285",
    "end": "2424250"
  },
  {
    "text": "Right. The- this function, g of z,",
    "start": "2424350",
    "end": "2430270"
  },
  {
    "text": "which has the form of 1 over 1 plus e^minus z is also called the logistic function,",
    "start": "2430270",
    "end": "2436100"
  },
  {
    "text": "which is basically why this algorithm is called logistic regression.",
    "start": "2439650",
    "end": "2444319"
  },
  {
    "text": "And it actually looks somewhat like the- the- the- um,",
    "start": "2445710",
    "end": "2453580"
  },
  {
    "text": "the perceptron's, uh, um, uh, uh, g function. So g of z looks like something like this,",
    "start": "2453580",
    "end": "2459920"
  },
  {
    "text": "supposing this is z, 0, this is 1.",
    "start": "2460050",
    "end": "2468805"
  },
  {
    "text": "Right. So g of z looks something like this.",
    "start": "2468805",
    "end": "2472819"
  },
  {
    "text": "Right, the g of z as- so the limit",
    "start": "2479490",
    "end": "2486535"
  },
  {
    "text": "of g of z as z tends to minus infinity equals 0.",
    "start": "2486535",
    "end": "2493734"
  },
  {
    "text": "And similarly, the limit of g of z as z tends to plus infinity is,",
    "start": "2493735",
    "end": "2500965"
  },
  {
    "text": "as z tends to plus infinity, g of z tends to 1. Right. And this line over here is g of z equals",
    "start": "2500965",
    "end": "2510295"
  },
  {
    "text": "1 over 1 plus e^ minus z, right?",
    "start": "2510295",
    "end": "2517494"
  },
  {
    "text": "Now, if you- if you compare this with the, uh,",
    "start": "2517495",
    "end": "2524455"
  },
  {
    "text": "perceptrons z but in the case of perceptron, it was exactly equal to 0 and 0 and exactly [NOISE] equal to 1 when z was greater than 0.",
    "start": "2524455",
    "end": "2536904"
  },
  {
    "text": "You can think of the logistic- um, logistic function has some kind of a soft version of the perceptrons,",
    "start": "2536905",
    "end": "2544910"
  },
  {
    "text": "uh, perceptrons version of- of g, right?",
    "start": "2545070",
    "end": "2550155"
  },
  {
    "text": "And now we define, just as in the case of- of linear regression,",
    "start": "2550155",
    "end": "2557244"
  },
  {
    "text": "we define a likelihood function, right? So let me say p of y^i equal to 1,",
    "start": "2557245",
    "end": "2568369"
  },
  {
    "text": "given x^i Theta is equal to",
    "start": "2569010",
    "end": "2579700"
  },
  {
    "text": "[NOISE] h_Theta of x,",
    "start": "2579700",
    "end": "2593680"
  },
  {
    "text": "and similarly y^i equals 0, given x^i Theta is equals to 1 minus h_Theta of x.",
    "start": "2593680",
    "end": "2606790"
  },
  {
    "text": "What this means is for any given, any given example x, we take the- the Theta vector that we have,",
    "start": "2606790",
    "end": "2615174"
  },
  {
    "text": "do a- a dot product. We get some z value, run it through g of z, right?",
    "start": "2615175",
    "end": "2622990"
  },
  {
    "text": "And it's gonna give you a value between 0 and 1. And now the claim is because it's a value between 0 and 1,",
    "start": "2622990",
    "end": "2630220"
  },
  {
    "text": "you're going to pretend that it's a probability value because probabilities are between 0 and 1, right?",
    "start": "2630220",
    "end": "2635965"
  },
  {
    "text": "So the probability of y equals 1 given x and Theta is defined to be h_Theta of x.",
    "start": "2635965",
    "end": "2643495"
  },
  {
    "text": "So supposing we have an x whose Theta transpose x is plus 10.",
    "start": "2643495",
    "end": "2649990"
  },
  {
    "text": "So z is going to be plus 10, g of z is going to be something very close to 1, right?",
    "start": "2649990",
    "end": "2660970"
  },
  {
    "text": "Similarly, if you have a vec- some example, um, x whose, uh, Theta transpose x is say,",
    "start": "2660970",
    "end": "2667570"
  },
  {
    "text": "minus 1, it's going to be here. And g of that is going to be a value that is close to 0, right?",
    "start": "2667570",
    "end": "2678775"
  },
  {
    "text": "And if you have an example, x whose Theta transpose x equals 0,",
    "start": "2678775",
    "end": "2684550"
  },
  {
    "text": "g of 0 is 0.5, right?",
    "start": "2684550",
    "end": "2692005"
  },
  {
    "text": "So the- the- the interpretation here is that if the output of- of,",
    "start": "2692005",
    "end": "2698904"
  },
  {
    "text": "um, h_Theta x or the output of h_Theta x will be closer to 1.",
    "start": "2698905",
    "end": "2705850"
  },
  {
    "text": "If the model thinks it has a positive example and it's gonna be closer to 0 if the model thinks it's a negative example.",
    "start": "2705850",
    "end": "2712900"
  },
  {
    "text": "Because if it thinks it's- it's a, uh, a positive example, then you expect the model to have Theta transpose x to be much bigger than 0.",
    "start": "2712900",
    "end": "2723070"
  },
  {
    "text": "If it has much- if- if Theta transpose x is much smaller than 0, then, uh, it would give it a very low probability, right?",
    "start": "2723070",
    "end": "2730900"
  },
  {
    "text": "And similarly, the probability, um, that y equals 0 is always going to be- be 1 minus of this function.",
    "start": "2730900",
    "end": "2740785"
  },
  {
    "text": "Uh, if this height is p of y equals 1 given x,",
    "start": "2740785",
    "end": "2749365"
  },
  {
    "text": "and this height is p of y equals 0 given x.",
    "start": "2749365",
    "end": "2757945"
  },
  {
    "text": "And because it's- it's a binary setting, uh, it must be the case that probability of y equals",
    "start": "2757945",
    "end": "2764380"
  },
  {
    "text": "1 plus probability of y equals 0 must equals 1, right? Because we are talking about a binary setting,",
    "start": "2764380",
    "end": "2770305"
  },
  {
    "text": "the sum of the two probabilities must always sum up to 1. And the height of- of this function is basically",
    "start": "2770305",
    "end": "2777700"
  },
  {
    "text": "the probability that the model assigns to that example that the label is 1. So the, uh,",
    "start": "2777700",
    "end": "2785839"
  },
  {
    "text": "the correct way to think of logistic regression is that it is a probability machine.",
    "start": "2786270",
    "end": "2792160"
  },
  {
    "text": "It's not actually a classifier, right? It's a probability machine for any given example, it outputs a probability, right?",
    "start": "2792160",
    "end": "2798610"
  },
  {
    "text": "And then it is up to us to choose some kind of a threshold and a common threshold is 0.5. Right? It is up to us to choose some kind of",
    "start": "2798610",
    "end": "2806650"
  },
  {
    "text": "a threshold and check whether the probability that was outputted was grea- higher than the threshold",
    "start": "2806650",
    "end": "2812379"
  },
  {
    "text": "or smaller than that threshold and convert it into a classifier. Right? So logistic regression plus some chosen threshold will give you a classifier.",
    "start": "2812379",
    "end": "2821109"
  },
  {
    "text": "Logistic regression by itself just outputs probabilities. Right? But it's- it's common to just call",
    "start": "2821110",
    "end": "2827050"
  },
  {
    "text": "logistic regression as a classification algorithm because, um, you know, we just assume that the threshold is 0.5.",
    "start": "2827050",
    "end": "2833710"
  },
  {
    "text": "Right. Right? So this is- this is our hypothesis. Now, based on this- based on, uh,",
    "start": "2833710",
    "end": "2841075"
  },
  {
    "text": "these two expressions, we're gonna define the likelihood function. Right? So instead of writing, you know, um,",
    "start": "2841075",
    "end": "2848905"
  },
  {
    "text": "a different expression for different values of y, we will instead write it in a compact way,",
    "start": "2848905",
    "end": "2854575"
  },
  {
    "text": "p of y given x Theta is equal to h_Theta of",
    "start": "2854575",
    "end": "2863425"
  },
  {
    "text": "x^y times 1 minus h_Theta",
    "start": "2863425",
    "end": "2872200"
  },
  {
    "text": "of x^1 minus y.",
    "start": "2872200",
    "end": "2876470"
  },
  {
    "text": "What's happening here? What's happening is if y is equal to 1,",
    "start": "2877560",
    "end": "2883825"
  },
  {
    "text": "that is if y is equal to 1, the second term just becomes 0,",
    "start": "2883825",
    "end": "2889330"
  },
  {
    "text": "you know, 1 minus 1 is 0. Anything raised to the power 0 is- is, you know, times 1. So in the cases when y equals 1,",
    "start": "2889330",
    "end": "2896170"
  },
  {
    "text": "it evaluates to this expression. In the cases when y equals to 0, it evaluates to this expression.",
    "start": "2896170",
    "end": "2901825"
  },
  {
    "text": "And you think of this as -as, um, as how- how you would write if-else in",
    "start": "2901825",
    "end": "2907240"
  },
  {
    "text": "a programming language and this like the mathematical version of writing an if-else. Right? So if y equals- equals 1,",
    "start": "2907240",
    "end": "2913720"
  },
  {
    "text": "it evaluates to this expression because this becomes 0 if y equals 0 then, you know, this entire thing is reached about 0 and you get this expression.",
    "start": "2913720",
    "end": "2920410"
  },
  {
    "text": "So it's- it's basically now if else of this written in- in a compact way, any questions? Yes. [inaudible] This is another way- is the question is,",
    "start": "2920410",
    "end": "2930460"
  },
  {
    "text": "is this another way to write an indicator function? Yes, this is another way to- to, uh, write an indicator function but,",
    "start": "2930460",
    "end": "2936085"
  },
  {
    "text": "you know, in a more compact way. Yeah. Think of it as an indicator function. Yes.",
    "start": "2936085",
    "end": "2942099"
  },
  {
    "text": "Question. [inaudible]",
    "start": "2942100",
    "end": "2954670"
  },
  {
    "text": "[NOISE] Okay, can you please repeat the question? [inaudible]",
    "start": "2954670",
    "end": "2965470"
  },
  {
    "text": "So the- is there a version of this algorithm where the output of the algorithm is still a probability,",
    "start": "2965470",
    "end": "2972055"
  },
  {
    "text": "but the- the labels- [inaudible] Sure,",
    "start": "2972055",
    "end": "2977500"
  },
  {
    "text": "so I- I if I understand the question, what you're saying is, um, is there a version of this algorithm where we take the predicted h_Theta",
    "start": "2977500",
    "end": "2985270"
  },
  {
    "text": "of x_theta as a probability and sample y from it and act based on that. You- you can do that. You can do that.",
    "start": "2985270",
    "end": "2991285"
  },
  {
    "text": "But it's far more common in practice where people choose some kind of a threshold and have a deterministic output.",
    "start": "2991285",
    "end": "2998110"
  },
  {
    "text": "So the- the version of the algorithm that you describe will behave randomly, right? And people don't like algorithms that behave randomly.",
    "start": "2998110",
    "end": "3004605"
  },
  {
    "text": "They want their algorithms to, you know, to- to be- to be deterministic. So yeah, you- you can absolutely do, um,",
    "start": "3004605",
    "end": "3012225"
  },
  {
    "text": "you can absolutely have a randomized classifier where you take the predicted probability and sample a y from that.",
    "start": "3012225",
    "end": "3017940"
  },
  {
    "text": "And- and that's theoretically that's perfectly fine but in practice, people just don't do that. Yeah, good question.",
    "start": "3017940",
    "end": "3024830"
  },
  {
    "text": "So this is- this is, um, this is how we define p of y equals y given x.",
    "start": "3024830",
    "end": "3032700"
  },
  {
    "text": "And in case of linear regression, we came up with a p of y given x, which had a Gaussian form, right?",
    "start": "3032700",
    "end": "3040320"
  },
  {
    "text": "In case of [NOISE] logistic regression, you know, this is actually a Bernoulli distribution. You know, think of this as a Bernoulli distribution where this is just p and this is,",
    "start": "3040320",
    "end": "3048470"
  },
  {
    "text": "uh, the- the, uh, uh, outcome. And given this, we can now define the likelihood function for your full dataset.",
    "start": "3048470",
    "end": "3057885"
  },
  {
    "text": "So L Theta is gonna be product of i equals 1 to n",
    "start": "3057885",
    "end": "3065460"
  },
  {
    "text": "and P of y^i",
    "start": "3065460",
    "end": "3071490"
  },
  {
    "text": "given x^i Theta, right?",
    "start": "3071490",
    "end": "3077085"
  },
  {
    "text": "And we're able to break down our full dataset into individually- product of individual examples because of the IID assumption.",
    "start": "3077085",
    "end": "3086230"
  },
  {
    "text": "Right? What do we do next? Just as before, we take the log-likelihood,",
    "start": "3089300",
    "end": "3095520"
  },
  {
    "text": "log L of Theta, which is also commonly written as l of Theta, right?",
    "start": "3095520",
    "end": "3102900"
  },
  {
    "text": "And this is gonna be equal to the product changes into the sum 1 to",
    "start": "3102900",
    "end": "3111060"
  },
  {
    "text": "n. And- so this- this term is equal to this,",
    "start": "3111060",
    "end": "3116720"
  },
  {
    "text": "and we apply the logarithm of this term. Again, the product breaks into sum, this product broke into summation and",
    "start": "3116720",
    "end": "3123950"
  },
  {
    "text": "this product is also going to break into a sum of two terms. It's going to be log of this term plus log of this term.",
    "start": "3123950",
    "end": "3131745"
  },
  {
    "text": "Right? Yes, question. [inaudible]",
    "start": "3131745",
    "end": "3151140"
  },
  {
    "text": "Sir, can you please repeat your question? [inaudible]",
    "start": "3151140",
    "end": "3165119"
  },
  {
    "text": "So why should we multiply by p of x given- [inaudible]",
    "start": "3165120",
    "end": "3173600"
  },
  {
    "text": "So what- what- what you're saying is, should we have p of y given x times p of x?",
    "start": "3173600",
    "end": "3183285"
  },
  {
    "text": "Should we have this as well? So this will be p of y comma x.",
    "start": "3183285",
    "end": "3189890"
  },
  {
    "text": "[NOISE] So- Yeah, we parameterized by theta everywhere.",
    "start": "3189890",
    "end": "3195045"
  },
  {
    "text": "So the question is, that's- that's a very good question. Now, why are we considering our likelihood function to be",
    "start": "3195045",
    "end": "3200640"
  },
  {
    "text": "p of y given x and why is it not p of y comma x? Why aren't we maximizing the joint probability of x and y?",
    "start": "3200640",
    "end": "3209775"
  },
  {
    "text": "And instead, why are we maximizing just p of y given x, right? And what we will see later in the course is that for many algorithms,",
    "start": "3209775",
    "end": "3217619"
  },
  {
    "text": "we are going to maximize this as our objective. And those algorithms will be called generative algorithms [NOISE] or generative models.",
    "start": "3217620",
    "end": "3227315"
  },
  {
    "text": "And those- and the- the- the other algorithms which only limit themselves to",
    "start": "3227315",
    "end": "3233280"
  },
  {
    "text": "the interests of y given x will be called [NOISE] discriminative algorithms.",
    "start": "3233280",
    "end": "3239190"
  },
  {
    "text": "[NOISE] For now, um,",
    "start": "3239190",
    "end": "3249119"
  },
  {
    "text": "the- the- the intuition to have here is that when you deploy this model,",
    "start": "3249120",
    "end": "3254595"
  },
  {
    "text": "somebody is going to give you x, right? You- you- you're- you know, you don't have a choice of deciding what x is, you know,",
    "start": "3254595",
    "end": "3260595"
  },
  {
    "text": "x gets thrown at you at production time, and you want to make a prediction on y given that x, right?",
    "start": "3260595",
    "end": "3266340"
  },
  {
    "text": "And, um, which is why, you know, we are most- mostly interested in p of y given x, making sure that we are able to make- making sure we're able to output,",
    "start": "3266340",
    "end": "3276510"
  },
  {
    "text": "uh, uh, uh, a meaningful value of y when x is just given to you. Okay? That's- that's the, uh, general algorithm, but, you know,",
    "start": "3276510",
    "end": "3283035"
  },
  {
    "text": "they're algorithms where we, uh, do with this, you know. For- for now, let's- let- let's just focus on discriminative algorithms this week.",
    "start": "3283035",
    "end": "3290140"
  },
  {
    "text": "But good- good question. [NOISE] All right?",
    "start": "3290510",
    "end": "3296985"
  },
  {
    "text": "So back to the log-likelihood. So the summation, so the, uh, log broke down this product into summation,",
    "start": "3296985",
    "end": "3304635"
  },
  {
    "text": "and it's going to break down this product into summation, okay? And when we apply logarithm to the first half,",
    "start": "3304635",
    "end": "3311924"
  },
  {
    "text": "it's going to be y^i log h_Theta of",
    "start": "3311925",
    "end": "3320500"
  },
  {
    "text": "x^i plus 1 minus y^i log",
    "start": "3320930",
    "end": "3331480"
  },
  {
    "text": "1 minus h_Theta of x^i.",
    "start": "3334370",
    "end": "3339610"
  },
  {
    "text": "This is our log-likelihood which we want to maximize because we want to do maximum likelihood.",
    "start": "3342290",
    "end": "3353025"
  },
  {
    "text": "You can also optionally, um, formulate a loss function for this where you take the negative sign of this and you just",
    "start": "3353025",
    "end": "3361200"
  },
  {
    "text": "think of that as a cost function which is- and then you want to minimize the cost function, which means- which is exactly the same as trying to maximize this, right?",
    "start": "3361200",
    "end": "3369285"
  },
  {
    "text": "And- and then we-",
    "start": "3369285",
    "end": "3375180"
  },
  {
    "text": "[NOISE] and then we apply gradient descent, you know.",
    "start": "3375180",
    "end": "3384765"
  },
  {
    "text": "It- it is- it is- you know, once you define the log likelihood of- of- of the parameters given the data,",
    "start": "3384765",
    "end": "3390465"
  },
  {
    "text": "once you're able to, uh, uh, write out data in this form, then it's just wrote calculus.",
    "start": "3390465",
    "end": "3396269"
  },
  {
    "text": "You know- you know, just- just follow the same template, calculate the gradient, start your Theta at some random initialization,",
    "start": "3396270",
    "end": "3403125"
  },
  {
    "text": "and update it in some direction based on the gradient, right? And you do that over and over until you converge.",
    "start": "3403125",
    "end": "3408930"
  },
  {
    "text": "It's the same algorithm once you define the likelihood function, okay?",
    "start": "3408930",
    "end": "3414434"
  },
  {
    "text": "So let's calculate the gradient. So in order to calculate the gradient,",
    "start": "3414435",
    "end": "3420085"
  },
  {
    "text": "um- in order to calculate the gradient,",
    "start": "3420085",
    "end": "3426615"
  },
  {
    "text": "what we- um- what we will do is first, we will observe that g of z,",
    "start": "3426615",
    "end": "3435765"
  },
  {
    "text": "which is equal to 1 over 1 plus e to the minus z,",
    "start": "3435765",
    "end": "3442269"
  },
  {
    "text": "and in practice, it- you know, z will be Theta transpose x. All right? The- the derivative of this",
    "start": "3442270",
    "end": "3451430"
  },
  {
    "text": "[NOISE] is equal to- so you can- you can look this up in the notes,",
    "start": "3451430",
    "end": "3456569"
  },
  {
    "text": "uh, I'm just gonna skip- skip the steps over here or anyway, let's just do it, right? Um, so how do we calculate the derivative of this?",
    "start": "3456570",
    "end": "3466215"
  },
  {
    "text": "So derivative of 1 over something is minus 1 over the same thing,",
    "start": "3466215",
    "end": "3472560"
  },
  {
    "text": "square, square, right, times the derivative of this which in this case,",
    "start": "3472560",
    "end": "3482340"
  },
  {
    "text": "will be this times minus e to the minus z.",
    "start": "3482340",
    "end": "3487785"
  },
  {
    "text": "All right? And this is equal to 1 over 1 plus e to the minus",
    "start": "3487785",
    "end": "3496319"
  },
  {
    "text": "z times e to the z times 1 over 1 plus e to the minus z.",
    "start": "3496320",
    "end": "3504120"
  },
  {
    "text": "[NOISE] All right? [NOISE] And- or here,",
    "start": "3504120",
    "end": "3510900"
  },
  {
    "text": "you can add one and subtract one from the numerator. And this will be 1 over 1 plus e to the minus z times 1",
    "start": "3510900",
    "end": "3521355"
  },
  {
    "text": "minus 1 plus e to the minus z over 1 plus e to the minus z,",
    "start": "3521355",
    "end": "3529470"
  },
  {
    "text": "[NOISE] this equal to 1 over [NOISE] minus z,",
    "start": "3529470",
    "end": "3535065"
  },
  {
    "text": "and this will be just, um, plus 1 minus 1, 1 plus z,",
    "start": "3535065",
    "end": "3544170"
  },
  {
    "text": "so this will be 1 plus 1 minus 1 over 1 plus e to the minus z.",
    "start": "3544170",
    "end": "3553109"
  },
  {
    "text": "And we see that this is actually equal to g of z again times 1 minus g of z.",
    "start": "3553110",
    "end": "3559260"
  },
  {
    "text": "[NOISE] Okay? So the derivative of- of the logistic function is equal to",
    "start": "3559260",
    "end": "3565110"
  },
  {
    "text": "the logistic function times 1 minus the logistic function. All right? And using this, is there a question? No question.",
    "start": "3565110",
    "end": "3578790"
  },
  {
    "text": "All right, so using this, let's calculate the radiant",
    "start": "3578790",
    "end": "3587340"
  },
  {
    "text": "of the log-likelihood [NOISE].",
    "start": "3587340",
    "end": "3600329"
  },
  {
    "text": "[NOISE] So L of theta is equal to- so let's- let's just look at one example for now,",
    "start": "3600330",
    "end": "3608730"
  },
  {
    "text": "um, with- with just one x and y and, uh, for, you know, across all of them, it's just summation over example,",
    "start": "3608730",
    "end": "3615120"
  },
  {
    "text": "so I'm just gonna use x and y for now. So L theta is y log h theta of x,",
    "start": "3615120",
    "end": "3622850"
  },
  {
    "text": "h theta of x is just g of theta transpose x plus 1 minus",
    "start": "3622850",
    "end": "3631740"
  },
  {
    "text": "y times log 1 minus g",
    "start": "3631740",
    "end": "3639270"
  },
  {
    "text": "of theta transpose x, right? There's just the log likelihood written for one example.",
    "start": "3639270",
    "end": "3644910"
  },
  {
    "text": "Okay. And now, [NOISE] the gradient of this will",
    "start": "3644910",
    "end": "3651539"
  },
  {
    "text": "be y times 1 over g of theta transpose x,",
    "start": "3651540",
    "end": "3661365"
  },
  {
    "text": "because log of something is 1 over that, times g prime of theta transpose x",
    "start": "3661365",
    "end": "3669555"
  },
  {
    "text": "times x plus 1 minus y times the,",
    "start": "3669555",
    "end": "3678734"
  },
  {
    "text": "log of this is, 1 over 1 minus g of theta transpose x",
    "start": "3678735",
    "end": "3686565"
  },
  {
    "text": "times minus 1 times g prime of theta transpose x times x.",
    "start": "3686565",
    "end": "3697335"
  },
  {
    "text": "Just the chain rule. Any questions on this? Yes? [inaudible].",
    "start": "3697335",
    "end": "3709740"
  },
  {
    "text": "How does taking the derivative of? [inaudible].",
    "start": "3709740",
    "end": "3717210"
  },
  {
    "text": "So this is based on the derivative, with respect to theta, of theta transpose x is equal to x.",
    "start": "3717210",
    "end": "3723810"
  },
  {
    "text": "We, uh, we reviewed this in class 2 or something, matrix calculus. [NOISE] Right.",
    "start": "3723810",
    "end": "3733484"
  },
  {
    "text": "And well,  this is y 1 over g of theta transpose x,",
    "start": "3733485",
    "end": "3740265"
  },
  {
    "text": "and this is g of theta transpose x times 1 minus g of theta transpose x,",
    "start": "3740265",
    "end": "3748680"
  },
  {
    "text": "plus 1 minus y times 1 over 1 minus g of, uh,",
    "start": "3748680",
    "end": "3756780"
  },
  {
    "text": "theta transpose x times minus 1 of, again, g prime is,",
    "start": "3756780",
    "end": "3762650"
  },
  {
    "text": "g of theta transpose x times 1 minus g of theta transpose x times x.",
    "start": "3762650",
    "end": "3772140"
  },
  {
    "text": "And this, you know, these two cancel, here, these two cancel.",
    "start": "3774260",
    "end": "3779280"
  },
  {
    "text": "So what are we left with? We have y times 1 minus g of theta transpose x,",
    "start": "3779280",
    "end": "3789795"
  },
  {
    "text": "plus 1 minus y- 1 minus y times minus",
    "start": "3789795",
    "end": "3797130"
  },
  {
    "text": "1 of g of theta transpose x. I think I forgot a times x here,",
    "start": "3797130",
    "end": "3804644"
  },
  {
    "text": "so times x, times x. So x and x are the only vector value quantities,",
    "start": "3804645",
    "end": "3812309"
  },
  {
    "text": "everything else before that evaluates to a scalar, right? Um, whenever you're doing matrix calculus,",
    "start": "3812310",
    "end": "3817890"
  },
  {
    "text": "always keep tracking the dimensions. Um, this- the left-hand side, the gradient is a vector and it's a vector because x is a vector and x is a vector,",
    "start": "3817890",
    "end": "3825315"
  },
  {
    "text": "everything else here evaluates to a scalar. [NOISE] Right, and so this",
    "start": "3825315",
    "end": "3832859"
  },
  {
    "text": "becomes y 1 minus g",
    "start": "3832860",
    "end": "3839100"
  },
  {
    "text": "of theta transpose x minus,",
    "start": "3839100",
    "end": "3844875"
  },
  {
    "text": "minus comes here, 1 minus y of g of theta transpose x,",
    "start": "3844875",
    "end": "3853335"
  },
  {
    "text": "the whole thing times x. And this is just- [NOISE] so you have, uh,",
    "start": "3853335",
    "end": "3865650"
  },
  {
    "text": "y minus y, g of theta transpose x",
    "start": "3865650",
    "end": "3875085"
  },
  {
    "text": "minus g of theta transpose x minus and minus plus y g of theta transpose x,",
    "start": "3875085",
    "end": "3886570"
  },
  {
    "text": "the whole thing times x. And these two cancel and you're gonna get the gradient to be equal to y minus,",
    "start": "3886600",
    "end": "3896744"
  },
  {
    "text": "g of theta transpose x is just h theta of x times x.",
    "start": "3896745",
    "end": "3903180"
  },
  {
    "text": "So the gradient of theta of the likelihood with respect to theta of this,",
    "start": "3903180",
    "end": "3908234"
  },
  {
    "text": "which means the update rule will look like theta equals theta plus alpha times the gradient,",
    "start": "3908235",
    "end": "3917630"
  },
  {
    "text": "which is y minus h theta of x times x.",
    "start": "3917630",
    "end": "3924390"
  },
  {
    "text": "Right. So we basically got the same update rule as the perceptron algorithm,",
    "start": "3924430",
    "end": "3932640"
  },
  {
    "text": "and the same update rule as linear regression, where the only thing that changes is the definition of h theta of x.",
    "start": "3932640",
    "end": "3941020"
  },
  {
    "text": "Right, and again, this is not a coincidence. You will see why it's not a coincidence, but,",
    "start": "3941600",
    "end": "3948000"
  },
  {
    "text": "you know, it looks very si- you know, they- they- they look very similar. Any questions on this? Yes, question?",
    "start": "3948000",
    "end": "3955260"
  },
  {
    "text": "[inaudible].",
    "start": "3955260",
    "end": "3960390"
  },
  {
    "text": "Yeah. [inaudible].",
    "start": "3960390",
    "end": "3967950"
  },
  {
    "text": "Good question. Was there a reason why we chose the logistic function? Could we have chosen something else, right? And the answer is, um,",
    "start": "3967950",
    "end": "3975465"
  },
  {
    "text": "the logistic function is a natural choice that we're gon- you're gonna see why it's a natural choice in the Friday lecture,",
    "start": "3975465",
    "end": "3982890"
  },
  {
    "text": "when we study generalized linear models. Uh, you cou- you can in- in general choose any other function,",
    "start": "3982890",
    "end": "3989684"
  },
  {
    "text": "um, instead of 1 over 1 plus e to the minus x. You can choose any other function that maps it between 0 and 1.",
    "start": "3989685",
    "end": "3995610"
  },
  {
    "text": "You can define a likelihood, uh, a function, you know, based on that version of g of theta,",
    "start": "3995610",
    "end": "4002119"
  },
  {
    "text": "take the derivatives and the update rule might look different, right?",
    "start": "4002120",
    "end": "4007310"
  },
  {
    "text": "And, uh, by choosing the logistic function to be of this particular form,",
    "start": "4007310",
    "end": "4012560"
  },
  {
    "text": "we get an update rule that looks similar to the oth- other update rules and we'll basically see the connection in- in the next lecture.",
    "start": "4012560",
    "end": "4018590"
  },
  {
    "text": "[inaudible].",
    "start": "4018590",
    "end": "4024110"
  },
  {
    "text": "Will we, um, so the question is, uh, will we deviate from linearity? And I'm intentionally not gonna,",
    "start": "4024110",
    "end": "4029930"
  },
  {
    "text": "uh, you know, go into more details. I know, and I'll, you know, um, I suggest you to wait.",
    "start": "4029930",
    "end": "4035550"
  },
  {
    "text": "All right. So this is- this is logistic regression, right? We got an update rule and over here,",
    "start": "4035860",
    "end": "4043849"
  },
  {
    "text": "if we want to do, uh, um, based on this update rule, we can perform gradient descent or stochastic gradient descent.",
    "start": "4043850",
    "end": "4048950"
  },
  {
    "text": "If it is stochastic gradient descent, then we sample one example randomly, plug in the values of that example, update our theta.",
    "start": "4048950",
    "end": "4058085"
  },
  {
    "text": "If it is, you know, a- a- a full gradient descent, then here there is gonna be, uh, summation across all your examples.",
    "start": "4058085",
    "end": "4066240"
  },
  {
    "text": "Yes, any questions? Yes. [inaudible]",
    "start": "4066370",
    "end": "4075010"
  },
  {
    "text": "So the question is here, I know, um, are we going to look at, uh, all examples at once or,",
    "start": "4075010",
    "end": "4080065"
  },
  {
    "text": "you know, uh, one by one, like the perceptron? And that's gonna depend on whether we choose to optimize this with gradient descent or stochastic gradient descent, right?",
    "start": "4080065",
    "end": "4087205"
  },
  {
    "text": "If we do gradient descent then we're gonna sum over all the examples. And if we choose to do",
    "start": "4087205",
    "end": "4092950"
  },
  {
    "text": "stochastic gradient descent then we choose one random example at a time. Calculate this, you know, um, this update rule and update your Theta,",
    "start": "4092950",
    "end": "4099220"
  },
  {
    "text": "and then pick another example. Update your Theta based on this update rule and so on.",
    "start": "4099220",
    "end": "4103910"
  },
  {
    "text": "Yes, the question is, are we looking at binary classification? Yes. For now, we're just looking at binary classification, true or false.",
    "start": "4106590",
    "end": "4113319"
  },
  {
    "text": "Right? And again, um, you know, the thing to note here, if you want to compare it to perceptron is that in the perceptron algorithm,",
    "start": "4113320",
    "end": "4120565"
  },
  {
    "text": "um, assuming we start Theta from 0, right?",
    "start": "4120565",
    "end": "4127674"
  },
  {
    "text": "A vector of 0s. In the perceptron algorithm, we were basically adding or subtracting our examples, you know,",
    "start": "4127675",
    "end": "4134859"
  },
  {
    "text": "small, you know, fractions of our examples to Theta to- to kind of make progress, right?",
    "start": "4134860",
    "end": "4142299"
  },
  {
    "text": "And each time we were multiplying it by some amount Alpha, and we're only performing this update if we got the example",
    "start": "4142300",
    "end": "4150520"
  },
  {
    "text": "wrong in case of perceptron, right? But in logistic regression,",
    "start": "4150520",
    "end": "4155950"
  },
  {
    "text": "what we see is that h Theta of x is gonna take some value between 0 and 1.",
    "start": "4155950",
    "end": "4162325"
  },
  {
    "text": "It's gonna look something like this and it'll never be exactly equal to 1 or 0, right?",
    "start": "4162325",
    "end": "4167920"
  },
  {
    "text": "Which means in every step, we will always take some amount of x,",
    "start": "4167920",
    "end": "4174984"
  },
  {
    "text": "which is based on, you know, the difference between y and h Theta of x. H Theta of x is never exactly equal to 0 or 1.",
    "start": "4174985",
    "end": "4182319"
  },
  {
    "text": "It would be some value between 0 and 1. Okay. And- and therefore, you can think of this as- as like a soft version of- of, you know,",
    "start": "4182320",
    "end": "4190555"
  },
  {
    "text": "the perceptron algorithm instead of deciding that we got this example correct and just discarding it.",
    "start": "4190555",
    "end": "4196825"
  },
  {
    "text": "No, we're gonna learn from every example, but by, you know, some amount depending to the degree to which we got it right. Yes, question?",
    "start": "4196825",
    "end": "4203830"
  },
  {
    "text": "[inaudible]",
    "start": "4203830",
    "end": "4231100"
  },
  {
    "text": "Yeah, good question. So the question is, uh, at test time- at validation time we're given- we calculated Theta during the training phase and we got a Theta,",
    "start": "4231100",
    "end": "4239425"
  },
  {
    "text": "and a test example- a test time we got a new example x. Now, how do we classify it as- as,",
    "start": "4239425",
    "end": "4244945"
  },
  {
    "text": "you know, 0 or 1, you know, classify it as positive or negative? And, uh, the answer there is that, uh,",
    "start": "4244945",
    "end": "4250030"
  },
  {
    "text": "you know, first we go through the math, you know, and do Theta transpose x, run it through the logistic function,",
    "start": "4250030",
    "end": "4255640"
  },
  {
    "text": "and we're- we're gonna get a value between 0 and 1, right? And that's- that's- you know, think of that as the probability that",
    "start": "4255640",
    "end": "4261940"
  },
  {
    "text": "the model thinks the- the correct answer is 0 or 1. And then we're gonna use- in practice we're gonna use some kind of a threshold.",
    "start": "4261940",
    "end": "4269800"
  },
  {
    "text": "Now, it's commonly, you know, 0.5. If the- if- if the, uh, uh, probability that was- that was predicted was, you know,",
    "start": "4269800",
    "end": "4276849"
  },
  {
    "text": "greater than that threshold, which is typically 0.5, we kind of think of that as- as a positive example.",
    "start": "4276850",
    "end": "4282010"
  },
  {
    "text": "And as, you know, another, another student, uh, suggested some time ago, uh, you could also take the probability and sample your way based on that probability.",
    "start": "4282010",
    "end": "4291670"
  },
  {
    "text": "If- if the predicted output, uh, probability was like 0.99 then, you know, most of the times you're gonna sample y equals 01,",
    "start": "4291670",
    "end": "4298765"
  },
  {
    "text": "uh, y equal to 1, right? Uh, and- and- and that can be your prediction, in which case your- your model is,",
    "start": "4298765",
    "end": "4304540"
  },
  {
    "text": "uh, is- is like a random model. But if you have a threshold then it's a deterministic model. Good question. Yes? [BACKGROUND] Yeah.",
    "start": "4304540",
    "end": "4317160"
  },
  {
    "text": "Yes.",
    "start": "4318630",
    "end": "4325290"
  },
  {
    "text": "Yes. Yes. Yes. So the question is in- in, uh, you- we can think of in the case of perceptron the resulting Theta to be",
    "start": "4325290",
    "end": "4332530"
  },
  {
    "text": "a linear combination of just the misclassified examples during the training phase, right?",
    "start": "4332530",
    "end": "4338130"
  },
  {
    "text": "Whereas with logistic regression it's going to be some linear combination of all your examples. Exactly. And the- the weights of the combination is do-",
    "start": "4338130",
    "end": "4346570"
  },
  {
    "text": "gonna depend on how correct or wrong we got each example, uh, during the training phase.",
    "start": "4346570",
    "end": "4352105"
  },
  {
    "text": "Yes?",
    "start": "4352105",
    "end": "4357470"
  },
  {
    "text": "[BACKGROUND] Yeah. Yes. So the question is,",
    "start": "4357470",
    "end": "4363055"
  },
  {
    "text": "during training phase for- you know, when we encountered an example, if we got it like really wrong we predicted a very low probability.",
    "start": "4363055",
    "end": "4370059"
  },
  {
    "text": "When the correct answer was a height we know is equal to one then, you know, the difference is gonna be big.",
    "start": "4370060",
    "end": "4375100"
  },
  {
    "text": "Exactly. Right. So that's- that's",
    "start": "4375100",
    "end": "4380770"
  },
  {
    "text": "logistic regression and you'll be implementing this in your- in your, um, homework.",
    "start": "4380770",
    "end": "4387715"
  },
  {
    "text": "But in your homework, uh, if I remember correctly, you'll be implementing it with Newton's method.",
    "start": "4387715",
    "end": "4396890"
  },
  {
    "text": "So here's another optimization algorithm that you may use in place of gradient descent, right?",
    "start": "4402330",
    "end": "4410530"
  },
  {
    "text": "In- in the case of gradient descent, we just took the, uh,",
    "start": "4410530",
    "end": "4415570"
  },
  {
    "text": "we just took the gradients and adjusted our Theta values according to the direction in which the gradient pointed us to.",
    "start": "4415570",
    "end": "4423040"
  },
  {
    "text": "[NOISE] Instead, there is another algorithm called Newton's method, right?",
    "start": "4423040",
    "end": "4438235"
  },
  {
    "text": "Newton's method. And think of this",
    "start": "4438235",
    "end": "4443679"
  },
  {
    "text": "as alternative to gradient descent,",
    "start": "4443680",
    "end": "4450530"
  },
  {
    "text": "gradient descent, or stochastic gradient. Think of this as, uh,",
    "start": "4451410",
    "end": "4458770"
  },
  {
    "text": "an alternative to gradient descent, um, and the, um, intuition for Newton's method is- is something like this.",
    "start": "4458770",
    "end": "4470830"
  },
  {
    "text": "Okay? So let's draw some pictures where- so let's assume",
    "start": "4470830",
    "end": "4486820"
  },
  {
    "text": "we are trying to- we're trying to, uh, minimize a function- mini- minimize some kind of a loss function, right?",
    "start": "4486820",
    "end": "4495100"
  },
  {
    "text": "And let's say your loss function, looks- looks like this.",
    "start": "4495100",
    "end": "4498560"
  },
  {
    "text": "And similarly for the- for the same setting,",
    "start": "4508260",
    "end": "4514159"
  },
  {
    "text": "here we are assuming, uh, uh, a one-dimensional input, right?",
    "start": "4514290",
    "end": "4519820"
  },
  {
    "text": "Um, so the derivative, not the gradient, the derivative, um, let's call it l prime of- of Theta, how would that look like?",
    "start": "4519820",
    "end": "4529000"
  },
  {
    "text": "So until this point, the gradient is negative or the derivative is negative.",
    "start": "4529000",
    "end": "4535825"
  },
  {
    "text": "So in this region to the left of, uh, of this minima, the function will probably look something like this.",
    "start": "4535825",
    "end": "4545365"
  },
  {
    "text": "And then to the right of this point, the gradient is kind of positive, right?",
    "start": "4545365",
    "end": "4551605"
  },
  {
    "text": "So maybe it- it- I don't know, it kinda looks something like this. Okay? Does that make sense?",
    "start": "4551605",
    "end": "4560770"
  },
  {
    "text": "So this is our loss function, and this is the- the, uh, uh, the first derivative of our loss function.",
    "start": "4560770",
    "end": "4569409"
  },
  {
    "text": "And what we wanna do is find the point where the loss function is- is- is minimum,",
    "start": "4569410",
    "end": "4577325"
  },
  {
    "text": "which means find the point where the derivative crosses 0.",
    "start": "4577325",
    "end": "4583570"
  },
  {
    "text": "Right? Does it make sense? And there is this algorithm called Newton's method.",
    "start": "4584500",
    "end": "4592400"
  },
  {
    "text": "So Newton's method is a way in which for any given function,",
    "start": "4596400",
    "end": "4602935"
  },
  {
    "text": "sum f of x, it finds- it's a way in which you can find those values of x for which f of x equals 0, right?",
    "start": "4602935",
    "end": "4612865"
  },
  {
    "text": "And tho- tho- those values of input for which a function evaluates to zero are also called like the roots of the function, right?",
    "start": "4612865",
    "end": "4619344"
  },
  {
    "text": "So Newton's method is a root-finding method.",
    "start": "4619345",
    "end": "4623540"
  },
  {
    "text": "Right? Where for any given function f, it will- it will find the input values at which the function evaluates to 0.",
    "start": "4628770",
    "end": "4639940"
  },
  {
    "text": "And what we're gonna do is apply Newton's method to the first derivative of our loss function.",
    "start": "4639940",
    "end": "4646150"
  },
  {
    "text": "Right? And the way Newton's method works is like this. Start with some random initialization.",
    "start": "4646150",
    "end": "4653335"
  },
  {
    "text": "You know, just like gradient descent, you start with some random initialization of- of your Thetas. Um, let's call it this- no,",
    "start": "4653335",
    "end": "4661920"
  },
  {
    "text": "let's call this Theta of 1 random initialization, right?",
    "start": "4661920",
    "end": "4667375"
  },
  {
    "text": "And Newton's method will approximate the function by a linear function that is equal to the given function at that point.",
    "start": "4667375",
    "end": "4678535"
  },
  {
    "text": "So maybe I'll use a different color. [NOISE] Right?",
    "start": "4678535",
    "end": "4681890"
  },
  {
    "text": "So the- the black line is supposed to be a curvy line and the blue line is supposed to be a straight line,",
    "start": "4688560",
    "end": "4695005"
  },
  {
    "text": "where the- the- the- the blue line is the linear approximation of the- of the first derivative, right?",
    "start": "4695005",
    "end": "4704440"
  },
  {
    "text": "And once you have a linear- linear function, finding the root is very easy, right?",
    "start": "4704440",
    "end": "4709450"
  },
  {
    "text": "And in- in Newton's method, what we do is approximate a function by a- a by the linear,",
    "start": "4709450",
    "end": "4716065"
  },
  {
    "text": "by- by its, uh, uh, the linear, uh, uh, a linear function and solve for the root of",
    "start": "4716065",
    "end": "4722050"
  },
  {
    "text": "the linear function and consider this to be your Theta 1.",
    "start": "4722050",
    "end": "4728215"
  },
  {
    "text": "Right? And from this point, repeat the process, right? Calculate the, uh, construct a new approximation.",
    "start": "4728215",
    "end": "4734769"
  },
  {
    "text": "So I'm gonna mark iteration 2 with red color. Um, and over here, the linear approximation might look like this,",
    "start": "4734769",
    "end": "4742284"
  },
  {
    "text": "and solve for the second approximation and that's gonna be Theta 2, and- and so on.",
    "start": "4742285",
    "end": "4752695"
  },
  {
    "text": "And once you get closer, Newton's method converges extremely fast to the- to the true answer.",
    "start": "4752695",
    "end": "4759775"
  },
  {
    "text": "Right? Hence, uh, what did we do in gradient descent? In gradient descent, we started,",
    "start": "4759775",
    "end": "4764830"
  },
  {
    "text": "um, over here, right? And the gradient pointed us in the rightward direction and we took a small step here.",
    "start": "4764830",
    "end": "4773890"
  },
  {
    "text": "And we know, uh, uh, and we saw that the gradient was still and- and we took a small step here,",
    "start": "4773890",
    "end": "4778930"
  },
  {
    "text": "and we kept taking small steps until we reached the minimum. In Newton's method, what we do is we- we look at the gradient function, uh,",
    "start": "4778930",
    "end": "4786925"
  },
  {
    "text": "at- at the first derivative at each step approximated by a linear function and solve that linear function to be equal to 0.",
    "start": "4786925",
    "end": "4794329"
  },
  {
    "text": "Right? And jump to that point, say from Theta 1 we- we,",
    "start": "4794330",
    "end": "4799840"
  },
  {
    "text": "did I get this wrong, I'll call this Theta 0. From Theta 0 we jumped to Theta 1, and that from Theta 1 we jumped to Theta 2.",
    "start": "4799840",
    "end": "4806050"
  },
  {
    "text": "And what we see is that the rate at which the sequence of, uh,",
    "start": "4806050",
    "end": "4811135"
  },
  {
    "text": "Thetas converges to, say the true Theta star,",
    "start": "4811135",
    "end": "4819684"
  },
  {
    "text": "or, you know, let's call this Theta star as the- the true global minimum. The rate at which Newton's method converges is much,",
    "start": "4819685",
    "end": "4827380"
  },
  {
    "text": "much faster than the rate at which regular gradient descent would converge. All right? And the Newton's method update rule looks like this. Yes, question?",
    "start": "4827380",
    "end": "4840280"
  },
  {
    "text": "[inaudible]?",
    "start": "4840280",
    "end": "4846699"
  },
  {
    "text": "So- yeah. So the diagram was a little small, so let- let me- let me draw, um, a better version of the diagram.",
    "start": "4846700",
    "end": "4853070"
  },
  {
    "text": "Right? So I'm gonna draw- let's assume this to be our,",
    "start": "4853380",
    "end": "4862554"
  },
  {
    "text": "uh, uh, just the first derivative, right?",
    "start": "4862555",
    "end": "4868540"
  },
  {
    "text": "So start at some Theta naught, right?",
    "start": "4868540",
    "end": "4874255"
  },
  {
    "text": "So this is the derivative. There is a corresponding, you know, true loss function, right?",
    "start": "4874255",
    "end": "4879340"
  },
  {
    "text": "This is the derivative. This is not the loss function. Right? So at this point,",
    "start": "4879340",
    "end": "4884530"
  },
  {
    "text": "the function will, you know, the function value some- somewhere over here. Right? It's a curve function.",
    "start": "4884530",
    "end": "4891715"
  },
  {
    "text": "Instead, we approximate it by a linear function, right? Which means- and here I'm gonna use a different color,",
    "start": "4891715",
    "end": "4901150"
  },
  {
    "text": "um, so the blue line is the first linear approximation.",
    "start": "4901150",
    "end": "4906890"
  },
  {
    "text": "Right? [NOISE] And this is gonna be Theta 1.",
    "start": "4910110",
    "end": "4916375"
  },
  {
    "text": "Right? And similarly, [NOISE] you know,",
    "start": "4916375",
    "end": "4923350"
  },
  {
    "text": "this is where- this is, uh, uh, l prime of Theta 1, and construct yet another linear approximation at the new point",
    "start": "4923350",
    "end": "4931599"
  },
  {
    "text": "[NOISE] in the way and the place where the second one, this would be Theta 2.",
    "start": "4931600",
    "end": "4940010"
  },
  {
    "text": "Okay? Does that make sense? At every point, construct a linear approximation, solve for the linear approximation,",
    "start": "4940410",
    "end": "4947184"
  },
  {
    "text": "and the solution is the next- next location for our Theta. That's where we moved to. Yes, there's a question.",
    "start": "4947184",
    "end": "4954790"
  },
  {
    "text": "[inaudible]?",
    "start": "4954790",
    "end": "4960100"
  },
  {
    "text": "Yes. So the question is, what happens if there is a non-convex function? I'm gonna come to that. All right.",
    "start": "4960100",
    "end": "4966040"
  },
  {
    "text": "Um, yeah, and- and the update rule for Newton's method is supposing we- we call this f of Theta.",
    "start": "4966040",
    "end": "4976840"
  },
  {
    "text": "The update rule for Newton's method is Theta of t plus 1 equals Theta of t",
    "start": "4976840",
    "end": "4983830"
  },
  {
    "text": "[NOISE] minus f of",
    "start": "4983830",
    "end": "4989980"
  },
  {
    "text": "Theta over f prime of Theta, right? This is the update nule- rule for Newton's method.",
    "start": "4989980",
    "end": "4998140"
  },
  {
    "text": "And now our f was the- the- the first derivative of our loss function.",
    "start": "4998140",
    "end": "5003825"
  },
  {
    "text": "So if we plug in our loss function here, what we get is Theta of t plus 1 equals Theta of",
    "start": "5003825",
    "end": "5014655"
  },
  {
    "text": "t minus l prime of Theta over l double prime Theta.",
    "start": "5014655",
    "end": "5022179"
  },
  {
    "text": "Does that make sense? Yeah, the value of Theta t. [NOISE] Yes, question?",
    "start": "5025610",
    "end": "5033090"
  },
  {
    "text": "When you're finding new Theta,",
    "start": "5033090",
    "end": "5041054"
  },
  {
    "text": "does it do that for every point on the curve or does",
    "start": "5041055",
    "end": "5047370"
  },
  {
    "text": "it do with your step size where equals to some common value?",
    "start": "5047370",
    "end": "5053790"
  },
  {
    "text": "So when- when we, uh, find the- the, um, new Theta, is there some kind of a step size here?",
    "start": "5053790",
    "end": "5060329"
  },
  {
    "text": "Yeah. Uh, in practice, you will use a step size. Uh, um, I'm coming to that. And- so this is Newton's method,",
    "start": "5060330",
    "end": "5070610"
  },
  {
    "text": "but however, this was, um, using scalar-valued inputs, right?",
    "start": "5070610",
    "end": "5075615"
  },
  {
    "text": "In practice, however, our loss function has a vector-valued input because we have, you know, um, uh, Theta as a vector.",
    "start": "5075615",
    "end": "5082810"
  },
  {
    "text": "Um, and- so this was for in the scalar case,",
    "start": "5083090",
    "end": "5088724"
  },
  {
    "text": "a- a scalar Theta. For vector Theta the update rule looks like this. [NOISE] Where",
    "start": "5088725",
    "end": "5103930"
  },
  {
    "text": "Right? So in the- in the scalar case,",
    "start": "5139280",
    "end": "5145244"
  },
  {
    "text": "we would divide the first derivative by the second derivative. But in the vector case,",
    "start": "5145245",
    "end": "5151485"
  },
  {
    "text": "the first derivative is gonna be a vector and the second der- derivative is going to be a- the Hessian matrix, right?",
    "start": "5151485",
    "end": "5158360"
  },
  {
    "text": "And we cannot divide a vector by a matrix, right? That- that- that's not a meaningful operation. Uh, so instead, what we have is this variant of Newton's method,",
    "start": "5158360",
    "end": "5167450"
  },
  {
    "text": "which is called Newton-Raphson method, [NOISE] which is a- a- a- a vector version of- of Newton's method to,",
    "start": "5167450",
    "end": "5178305"
  },
  {
    "text": "you know, uh, for vector-valued inputs. And H over here is the- is- is the Hessian.",
    "start": "5178305",
    "end": "5184949"
  },
  {
    "text": "And invert the Hessian, that's like, you know, dividing the sec, uh,",
    "start": "5184950",
    "end": "5190125"
  },
  {
    "text": "taking the- the- the reciprocal of the second derivative. So you invert the Hessian, multiply it by the gradient,",
    "start": "5190125",
    "end": "5195975"
  },
  {
    "text": "and that's gonna be the- the- the direction in which we, uh, in which we're gonna head.",
    "start": "5195975",
    "end": "5202605"
  },
  {
    "text": "And in practice, it is very common to have a learning grade here as well.",
    "start": "5202605",
    "end": "5208920"
  },
  {
    "text": "Uh, though you, you know, in- in- in many cases you don't need an extra learning grade,",
    "start": "5208920",
    "end": "5214560"
  },
  {
    "text": "so think of this, uh, you know, compare it to gradient descent. In gradient descent, it was pretty much the same except for having a Hessian, right?",
    "start": "5214560",
    "end": "5223500"
  },
  {
    "text": "But in Newton's method, you multiply it by the, you know, you multiply it by the Hessian as well because",
    "start": "5223500",
    "end": "5228690"
  },
  {
    "text": "essentially what- what's happening by multi- multiplying it by the inverse Hessian is that you're- you're trying to account for the curvature of the loss function.",
    "start": "5228690",
    "end": "5236130"
  },
  {
    "text": "You know, that's- that's- that's the intuition to have, right? In- in, uh, in gradient descent,",
    "start": "5236130",
    "end": "5242625"
  },
  {
    "text": "we're only looking at the direction of the s- of the steepest descent, uh, but then the curvature of the function can be pretty, uh, uh,",
    "start": "5242625",
    "end": "5251489"
  },
  {
    "text": "unusual, and Newton's method kind of accounts for the- the- the second level curvature so.",
    "start": "5251490",
    "end": "5257340"
  },
  {
    "text": "And this- this, um, func- this algorithm generally works much faster in",
    "start": "5257340",
    "end": "5263790"
  },
  {
    "text": "terms of the number of steps required to converge, but there's a catch. The catch is that,",
    "start": "5263790",
    "end": "5270449"
  },
  {
    "text": "um, calculating the gradient is- is, you know, O_d, uh,",
    "start": "5270450",
    "end": "5275580"
  },
  {
    "text": "uh, in terms of time complexity, and that is all that we need for doing gradient descent.",
    "start": "5275580",
    "end": "5283784"
  },
  {
    "text": "However, the Hessian is, you know, d by d. So you have O_d squared for calculating the Hessian.",
    "start": "5283785",
    "end": "5291960"
  },
  {
    "text": "And even further, you need to invert the Hessian. So that's about O_d cubed, okay?",
    "start": "5291960",
    "end": "5298380"
  },
  {
    "text": "So if your d is high-dimensional, then Newton's method, even though it requires a fewer number of steps,",
    "start": "5298380",
    "end": "5306210"
  },
  {
    "text": "the cost of each step can be prohibitively high, right?",
    "start": "5306210",
    "end": "5311250"
  },
  {
    "text": "And- and it is that trade-off that you need to- that you need to do in terms of deciding whether you wanna use gradient descent or Newton's method, right?",
    "start": "5311250",
    "end": "5320040"
  },
  {
    "text": "Another, um, uh, point to note is that Newton's method is, a root finding method,",
    "start": "5320040",
    "end": "5326475"
  },
  {
    "text": "which means if your",
    "start": "5326475",
    "end": "5333195"
  },
  {
    "text": "prime_Theta look like this, right?",
    "start": "5333195",
    "end": "5339030"
  },
  {
    "text": "If your l prime_Theta looks like this, which means your cost function l_Theta",
    "start": "5339030",
    "end": "5344070"
  },
  {
    "text": "was like this and it went up,",
    "start": "5344070",
    "end": "5353175"
  },
  {
    "text": "and it came down again, right? Newton's method is only- is gonna find you one of the roots, right?",
    "start": "5353175",
    "end": "5363630"
  },
  {
    "text": "It could- it could find either the root where the- the- the first derivative is going up,",
    "start": "5363630",
    "end": "5370409"
  },
  {
    "text": "which means over here, or it could find a root where the first derivative was going down, which means over here.",
    "start": "5370410",
    "end": "5375855"
  },
  {
    "text": "And it's gonna depend on initialization. You know, did you initialize close to, you know,",
    "start": "5375855",
    "end": "5381300"
  },
  {
    "text": "the first- uh, the- the- the first, uh, stationary point or did you initialize close to the second stationary point.",
    "start": "5381300",
    "end": "5386744"
  },
  {
    "text": "It's- Newton's method is just gonna take you to the nearest stationary point of that function, right?",
    "start": "5386745",
    "end": "5393030"
  },
  {
    "text": "Stationary point means a function where the gradient is 0. It could be a minima, it could be a maxima, it could be a saddle point, right?",
    "start": "5393030",
    "end": "5399180"
  },
  {
    "text": "Newton's method is gonna just take you to the nearest stationary point, which also means that Newton's method,",
    "start": "5399180",
    "end": "5406199"
  },
  {
    "text": "throw a function at it, it's gonna take you to the nearest stationary point. And if it's a convex function,",
    "start": "5406200",
    "end": "5412650"
  },
  {
    "text": "it's automatically minimizing it. If it is a concave function, it's automatically maximizing it, So it's kind of like plug-and-play, right?",
    "start": "5412650",
    "end": "5418784"
  },
  {
    "text": "You can even flip the sign of your function and throw it at Newton's method and it's still gonna recover the same point, right?",
    "start": "5418785",
    "end": "5426150"
  },
  {
    "text": "It's gonna take you to the nearest stationary point. It could be a maxima or a minima. So flipping signs is not a problem, okay?",
    "start": "5426150",
    "end": "5433574"
  },
  {
    "text": "So that's- that's Newton's- yes question. It's going to find the nearest stationary point, right?",
    "start": "5433575",
    "end": "5442110"
  },
  {
    "text": "If the nearest stationary point is the only stationary point, then it is, you know, and- and- and it's a local minima,",
    "start": "5442110",
    "end": "5447675"
  },
  {
    "text": "then it's the global minima. But if your- if your function has lots of local minimas or",
    "start": "5447675",
    "end": "5452970"
  },
  {
    "text": "lots of local maxima [inaudible] take it to the nearest stationary point, right? And- and- and that's the reason, uh,",
    "start": "5452970",
    "end": "5458310"
  },
  {
    "text": "Newton's method is most commonly used only with convex functions or, you know, concave functions.",
    "start": "5458310",
    "end": "5463875"
  },
  {
    "text": "Whereas gradient descent does not have that limitation. Gradient descent will always keep-",
    "start": "5463875",
    "end": "5469290"
  },
  {
    "text": "take you- will- will take you down, down, and down, right? If- if- if the nearest stationary point is a local maxima,",
    "start": "5469290",
    "end": "5476175"
  },
  {
    "text": "gradient descent will not take you to it. The gradient descent will take you, you know, only down. Whereas Newton method, it's gonna take you to",
    "start": "5476175",
    "end": "5482400"
  },
  {
    "text": "the nearest stationary point. Yes, question. [inaudible].",
    "start": "5482400",
    "end": "5502470"
  },
  {
    "text": "Yeah, so the- um, uh, e- for- for SGD- so Newton's method generally does not work well with SGD.",
    "start": "5502470",
    "end": "5508830"
  },
  {
    "text": "Uh, and the reason is because in- in, you know, if- if you take just a few examples, the Hessian may not be invertible, right?",
    "start": "5508830",
    "end": "5515730"
  },
  {
    "text": "You need- you need a lot of examples to make the Hessian, uh, in- invertible. So Newton's method generally does not work well with an SGD kind of setting,",
    "start": "5515730",
    "end": "5522540"
  },
  {
    "text": "or maybe I misunderstood your question? [inaudible].",
    "start": "5522540",
    "end": "5533220"
  },
  {
    "text": "Oh, it's not analogous to SGD. So, uh, the scalar version of Newton's method is, um, is- is analogous to having a model where you have only one parameter.",
    "start": "5533220",
    "end": "5541724"
  },
  {
    "text": "That's when you can use this method. [inaudible]. We take one sample at a time,",
    "start": "5541725",
    "end": "5547800"
  },
  {
    "text": "not one parameter at a time, okay? Yes, question. [inaudible].",
    "start": "5547800",
    "end": "5560160"
  },
  {
    "text": "Yes. [inaudible] Yes. So gradient descent, uh, can be caught in a local minimum, that's right.",
    "start": "5560160",
    "end": "5565170"
  },
  {
    "text": "Uh, whereas with, uh, uh, Newton's method, uh, you- you get caught in the most nearby stationary point.",
    "start": "5565170",
    "end": "5572310"
  },
  {
    "text": "It could be a maxima or a minima. And- and it works really well when your function is convex,",
    "start": "5572310",
    "end": "5577860"
  },
  {
    "text": "where you know that the nearest point is the global optimum. That's right. Yes, question. [inaudible]",
    "start": "5577860",
    "end": "5588030"
  },
  {
    "text": "Uh, you could end up in a maximum if you- uh, uh, with gradient descent. Was that the question? [inaudible].",
    "start": "5588030",
    "end": "5594210"
  },
  {
    "text": "So, uh, with gradient descent, if you. [inaudible]. Yeah, so with gradient descent if you- if you- if you have a function that",
    "start": "5594210",
    "end": "5602099"
  },
  {
    "text": "has maximas and minimas and you started exactly, you know, a- at the local maxima, then yes,",
    "start": "5602100",
    "end": "5608400"
  },
  {
    "text": "gradient descent will not make progress because, you know, the gradient there is al- al- already, that's right. [NOISE] Okay, cool.",
    "start": "5608400",
    "end": "5616560"
  },
  {
    "text": "So that's Newton's method. And [NOISE] in the last few remaining minutes,",
    "start": "5616560",
    "end": "5623040"
  },
  {
    "text": "[NOISE] you know, let's- let's get some intuitions about- about, um, functional analysis.",
    "start": "5623040",
    "end": "5631110"
  },
  {
    "text": "So this is going to be, um, uh, for the last few minutes, what we're gonna cover is,",
    "start": "5631110",
    "end": "5636270"
  },
  {
    "text": "you can think of it as purely optional in the sense that they're go- there's not going to be any questions on your homeworks or in- in your,",
    "start": "5636270",
    "end": "5643635"
  },
  {
    "text": "exam, uh, from the material for- for- for the rest of this lecture. But there are some, you know,",
    "start": "5643635",
    "end": "5650040"
  },
  {
    "text": "the intuitions that you- that you- that you, get, uh, from a function analysis point, uh,",
    "start": "5650040",
    "end": "5655830"
  },
  {
    "text": "standpoint can be very useful for understanding many algorithms that we're going to cover later this quarter, right?",
    "start": "5655830",
    "end": "5666840"
  },
  {
    "text": "So what's functional analysis? So think of this as like, um, uh, uh, a five minute overview of functional analysis.",
    "start": "5666840",
    "end": "5673110"
  },
  {
    "text": "Functional analysis is- is like, you know, an advanced mathematical topic which, you know, you probably spent a year studying.",
    "start": "5673110",
    "end": "5678239"
  },
  {
    "text": "But now we're just going to look at some- [NOISE] some intuitions, right? So you can think of functional analysis as the study of functions.",
    "start": "5678240",
    "end": "5685289"
  },
  {
    "text": "[NOISE] Right.",
    "start": "5685289",
    "end": "5691665"
  },
  {
    "text": "You think of it as like, you know, a study of functions- [NOISE] functions,",
    "start": "5691665",
    "end": "5698639"
  },
  {
    "text": "but a more common or- or- or- or a more useful, uh, interpretation of functional analysis is think of it",
    "start": "5698640",
    "end": "5705180"
  },
  {
    "text": "as linear algebra in infinite dimensions. [NOISE]",
    "start": "5705180",
    "end": "5718780"
  },
  {
    "text": "Okay. So the last 10 minutes or five minutes that you are gonna spend is- is- is basically, you know,",
    "start": "5718780",
    "end": "5724885"
  },
  {
    "text": "think of it as- as- as- if you haven't seen this material before, think of it as, you know, some kind of a mental exercise to force yourself to think",
    "start": "5724885",
    "end": "5732744"
  },
  {
    "text": "about- about vectors and functions in a different way, right? It's good to learn new things,",
    "start": "5732745",
    "end": "5737860"
  },
  {
    "text": "but I think it's even better to learn new perspectives. And, you know, this perspective can be very useful when,",
    "start": "5737860",
    "end": "5743304"
  },
  {
    "text": "you know, we study things like Gaussian processes and kernels and so forth. Right? So, um, so assume this is vector space R3.",
    "start": "5743305",
    "end": "5756820"
  },
  {
    "text": "Let's call it X_1, X_2, X_3 at the axis,",
    "start": "5756820",
    "end": "5762310"
  },
  {
    "text": "and you have some point V. Right?",
    "start": "5762310",
    "end": "5769390"
  },
  {
    "text": "And the claim that we're gonna make is that functions and vectors are the same.",
    "start": "5769390",
    "end": "5777775"
  },
  {
    "text": "Right? You know, functions are- think of them as an infinite dimensional vector.",
    "start": "5777775",
    "end": "5784105"
  },
  {
    "text": "And that might sound surprising initially because we think- we think of vectors as, you know, something that's like straight and linear, right?",
    "start": "5784105",
    "end": "5790300"
  },
  {
    "text": "And your function can be, you know, like sine X can be you know wiggly, right? So what's- what's- what's the connection there, right?",
    "start": "5790300",
    "end": "5796035"
  },
  {
    "text": "So, um, one way to see the connection is a vector- think of this vector V as",
    "start": "5796035",
    "end": "5805135"
  },
  {
    "text": "a function- think of this vector V as a function defined over the domain,",
    "start": "5805135",
    "end": "5815409"
  },
  {
    "text": "where the domain is [NOISE], right?",
    "start": "5815410",
    "end": "5828325"
  },
  {
    "text": "So the index that we are going to- the index of the- of the space is gonna serve as the input to the function.",
    "start": "5828325",
    "end": "5835750"
  },
  {
    "text": "So, uh, supposing this function has, you know, let's- let's- let's see. So this function has,",
    "start": "5835750",
    "end": "5842210"
  },
  {
    "text": "I'm gonna call this V_1, right, and this is V_3 and similarly,",
    "start": "5843120",
    "end": "5854485"
  },
  {
    "text": "it has some- that's V_2. Right? Just the components of the vector.",
    "start": "5854485",
    "end": "5859824"
  },
  {
    "text": "So you can think of writing this vector as V of 1 equals V_1,",
    "start": "5859825",
    "end": "5866905"
  },
  {
    "text": "V of 2 equals V_2, V of 3 equals V_3,",
    "start": "5866905",
    "end": "5873510"
  },
  {
    "text": "where, you know, those other components, right? Think of a vector as a function, right? Where the input that you're feeding is the axes, right?",
    "start": "5873510",
    "end": "5882130"
  },
  {
    "text": "And the- the number of axes you have for the space",
    "start": "5882130",
    "end": "5887170"
  },
  {
    "text": "is- is- is- is basically the domain of that function- of that vector.",
    "start": "5887170",
    "end": "5894489"
  },
  {
    "text": "Right? Now, this vector can- we can also now visualize it as a function, 1, 2, 3.",
    "start": "5894490",
    "end": "5904090"
  },
  {
    "text": "Right? [NOISE] So this vector, let's call it a blue vector. So V_1 is some value, V_2 is some value,",
    "start": "5904090",
    "end": "5912760"
  },
  {
    "text": "V_3 is some value, right? So think of it as a function whose domain is just three integers and you know its- it's- its,",
    "start": "5912760",
    "end": "5922705"
  },
  {
    "text": "uh, values is- is real value. Right? Now, if you look at a more common, you know,",
    "start": "5922705",
    "end": "5931540"
  },
  {
    "text": "more x in R and let us consider a function,",
    "start": "5931540",
    "end": "5939535"
  },
  {
    "text": "you know, think of this as f of x equals x squared plus 1.",
    "start": "5939535",
    "end": "5948950"
  },
  {
    "text": "Right? Trying to draw the same analogy here, it's just that for this function,",
    "start": "5949860",
    "end": "5956305"
  },
  {
    "text": "we have an infinite number of [NOISE] infinite number of- of inputs that we can pass.",
    "start": "5956305",
    "end": "5962740"
  },
  {
    "text": "For these functions, we have just three possible inputs. For this function, we have an infinite number of inputs, right, and the value of",
    "start": "5962740",
    "end": "5969130"
  },
  {
    "text": "the function at each point- so pretend this is one long vector,",
    "start": "5969130",
    "end": "5975639"
  },
  {
    "text": "an infinitely long vector, where the value of each coordinate is the height of the function, right?",
    "start": "5975640",
    "end": "5982960"
  },
  {
    "text": "So the value of the function is- is- is what you fill in- in that- in that vector, right? So a vector in three-dimensional,",
    "start": "5982960",
    "end": "5989935"
  },
  {
    "text": "we would write it as, you know, V_1, V_2, then V_3,",
    "start": "5989935",
    "end": "5996205"
  },
  {
    "text": "and a function, similarly, you can write this function as equal to f of x1,",
    "start": "5996205",
    "end": "6005625"
  },
  {
    "text": "f of x2, right? And an infinite number of them, right? So a function can be explicitly written out with,",
    "start": "6005625",
    "end": "6014010"
  },
  {
    "text": "you know, by the values of the function evaluates to at different points. And it's- it- it looks very similar to a vector, right?",
    "start": "6014010",
    "end": "6020465"
  },
  {
    "text": "It's just infinitely long. And this analogy is, you know,",
    "start": "6020465",
    "end": "6028830"
  },
  {
    "text": "you've got to use some amount of imagination because here we are- we're- we're- this works only for countably infinite kind of- kind of infinities.",
    "start": "6028830",
    "end": "6036780"
  },
  {
    "text": "But imagine you can, you know, you write it to every possible real value, right? It's like, you know, an- an uncountably infinite,",
    "start": "6036780",
    "end": "6042870"
  },
  {
    "text": "infinitely long vector, right? That's- that's the kind of intuition you want to have. Now, you know, just the way you, you know,",
    "start": "6042870",
    "end": "6051030"
  },
  {
    "text": "if- if you have a function defined over three elements, you can map it into a three-dimensional space.",
    "start": "6051030",
    "end": "6056505"
  },
  {
    "text": "In the same way you can- you can- you can map this function in, you know, an infinite dimensional space, right?",
    "start": "6056505",
    "end": "6066180"
  },
  {
    "text": "So imagine you have an infinite number of axes, [NOISE] right? Imagine you have an infinite number of axes.",
    "start": "6066180",
    "end": "6072600"
  },
  {
    "text": "I cannot write it on the board, I'm sorry. Um, and there is a point,",
    "start": "6072600",
    "end": "6077925"
  },
  {
    "text": "let's call this f, such that X_1, X_2,",
    "start": "6077925",
    "end": "6084599"
  },
  {
    "text": "X_3, you know, and- and there are an infinite number of these and the value of a function at a given axis,",
    "start": "6084600",
    "end": "6095505"
  },
  {
    "text": "because each axis corresponds to one possible input.",
    "start": "6095505",
    "end": "6100545"
  },
  {
    "text": "So the value of the- of a function at a given input is the same as the coordinate of the vector at that axis,",
    "start": "6100545",
    "end": "6109570"
  },
  {
    "text": "which is- which is the same analogy from here to here, right? The value of the function at a given input is",
    "start": "6109670",
    "end": "6117540"
  },
  {
    "text": "basically the coordinate value of the vector along that axis, right? Just extend that to an infinite- infinite, um, uh, dimensions.",
    "start": "6117540",
    "end": "6126795"
  },
  {
    "text": "Which means now you can [NOISE]",
    "start": "6126795",
    "end": "6139020"
  },
  {
    "text": "and having this- this- this functional space of- of",
    "start": "6139020",
    "end": "6144645"
  },
  {
    "text": "functions being point in an infinite dimensional space will be super useful for some of the algorithms that we are going to study.",
    "start": "6144645",
    "end": "6151680"
  },
  {
    "text": "And you can actually- to- to formalize this a little more or to- to kind of strengthen your intuitions some more,",
    "start": "6151680",
    "end": "6160770"
  },
  {
    "text": "let's kind of draw comparisons of different concepts.",
    "start": "6160770",
    "end": "6166420"
  },
  {
    "text": "So on the one side, you're gonna have finite case and the other case we have infinite case.",
    "start": "6172570",
    "end": "6183710"
  },
  {
    "text": "In the finite case you have a vector, and generally call it V. In the infinite case you have a function,",
    "start": "6183710",
    "end": "6192210"
  },
  {
    "text": "let's call it like f of t, right?",
    "start": "6193000",
    "end": "6198770"
  },
  {
    "text": "In the finite case, you would have some kind of an index, right?",
    "start": "6198770",
    "end": "6205265"
  },
  {
    "text": "Think of, you know, like, V_1, V_2, V_3, the- th- uh, the- the- the axis, ah, which are- which are like, uh,",
    "start": "6205265",
    "end": "6211145"
  },
  {
    "text": "the indexes, and they are generally, you know, some kind of say 1-d for example, if you're- if you're in a d-dimensional space.",
    "start": "6211145",
    "end": "6218645"
  },
  {
    "text": "And in case of the infinite case for functions, it's- it's basically the domain over which the function is defined, right?",
    "start": "6218645",
    "end": "6226835"
  },
  {
    "text": "Over here it is- it's like R, right? And for a vector you have components, right?",
    "start": "6226835",
    "end": "6237065"
  },
  {
    "text": "And in the infinite case, you have values of the function at- at- at- at different inputs. Okay, does it make sense?",
    "start": "6237065",
    "end": "6246500"
  },
  {
    "text": "And vectors are generally represented with an explicit representation, right?",
    "start": "6246500",
    "end": "6256175"
  },
  {
    "text": "So you- you generally write V equals, you know, V_1, V_2, V- V_d, right?",
    "start": "6256175",
    "end": "6262445"
  },
  {
    "text": "You just write out the actual values of- of- of the vector. But functions generally have,",
    "start": "6262445",
    "end": "6267860"
  },
  {
    "text": "you know, some kind of a symbolic representation. [NOISE] Which means, you know,",
    "start": "6267860",
    "end": "6277670"
  },
  {
    "text": "you have some kind of a shortcut, uh, you know, given the index, you know, what's the value? You know, some kind of a rule that given the index,",
    "start": "6277670",
    "end": "6283730"
  },
  {
    "text": "what the value should be, right? You can- you can, you know, just as well think of, you know, uh, an explicit representation of function,",
    "start": "6283730",
    "end": "6290660"
  },
  {
    "text": "where you're just writing out all the values of the function at every possible input as one long vector, right?",
    "start": "6290660",
    "end": "6296000"
  },
  {
    "text": "Just the same intuition you want to have, right? And few more things.",
    "start": "6296000",
    "end": "6301565"
  },
  {
    "text": "So in- in vectors you have a dot product, right?",
    "start": "6301565",
    "end": "6308360"
  },
  {
    "text": "Dot-product, uh, where, you know, you- you can write the, you know, between two vectors u and v equals sum over i u_i v_i, right?",
    "start": "6308360",
    "end": "6322190"
  },
  {
    "text": "And similarly, uh, with- with functions, you can actually take an inner product between two functions and that would look like this,",
    "start": "6322190",
    "end": "6329165"
  },
  {
    "text": "you know, f, g equal to integral of f of t, g of t dt.",
    "start": "6329165",
    "end": "6339015"
  },
  {
    "text": "And that- this is- this is, you know, perfectly valid inner product between two functions, right?",
    "start": "6339015",
    "end": "6344420"
  },
  {
    "text": "It's- it's- it's just the extension of the inner product to- to, ah, an infi- a infinite dimensional setting. Uh, and- and in fact, you know,",
    "start": "6344420",
    "end": "6351125"
  },
  {
    "text": "um, this form, you- you might be familiar, you know, this- uh, uh, if f is like a probability density function and g is,",
    "start": "6351125",
    "end": "6357230"
  },
  {
    "text": "like, a random variable, you know, your expectation of- sorry,",
    "start": "6357230",
    "end": "6363080"
  },
  {
    "text": "the expectation of some g of x, where x is drawn according to some distribution p,",
    "start": "6363080",
    "end": "6370085"
  },
  {
    "text": "is actually just the inner product between p and g, right?",
    "start": "6370085",
    "end": "6376700"
  },
  {
    "text": "And you have a matrix, right? Matrix A.",
    "start": "6376700",
    "end": "6382320"
  },
  {
    "text": "The equivalent thing over here is some kind of a- a two input function, right?",
    "start": "6382960",
    "end": "6388475"
  },
  {
    "text": "Let's- let's- let's, uh, call it k of s, t, where in- in a matrix A,",
    "start": "6388475",
    "end": "6395420"
  },
  {
    "text": "you have A_ij to be that, uh, you know, the specific element of the ith row and jth column. But for- for, uh,",
    "start": "6395420",
    "end": "6401630"
  },
  {
    "text": "in an infinite dimensional setting, just evaluate the function at the two given inputs, right?",
    "start": "6401630",
    "end": "6407090"
  },
  {
    "text": "And- and so on. And- and you can- you can- um,",
    "start": "6407090",
    "end": "6413240"
  },
  {
    "text": "um, each matrix is- you can think of it as, um, a linear operator, which",
    "start": "6413240",
    "end": "6419660"
  },
  {
    "text": "means you have linear operators on functional spaces as well, right? So for example, A of x,",
    "start": "6419660",
    "end": "6428360"
  },
  {
    "text": "let's say Y equals A of x is a linear operation on a vector. Similarly, you can have a linear operation in functional space.",
    "start": "6428360",
    "end": "6436489"
  },
  {
    "text": "And one such example is, you know, f prime equals D of f. There you have a function f,",
    "start": "6436490",
    "end": "6444830"
  },
  {
    "text": "and differentiation is a linear operator, right? And you get an output function, right?",
    "start": "6444830",
    "end": "6450035"
  },
  {
    "text": "D is, uh, uh, you know, er, er, you- you can think of differentiation as one big, you know, infinite by infinite-dimensional- uh, dimensional matrix, right?",
    "start": "6450035",
    "end": "6459065"
  },
  {
    "text": "And the- the uh, and- and the- you know, um, you can draw further analogies. For example, you have",
    "start": "6459065",
    "end": "6465980"
  },
  {
    "text": "eigenvectors for matrix A such that Ax equals Lambda x.",
    "start": "6465980",
    "end": "6475475"
  },
  {
    "text": "Similarly for some kind of- you know, this is finite, this is infinite.",
    "start": "6475475",
    "end": "6482970"
  },
  {
    "text": "Similarly, you can have, um, um, eigenvector eigenfunctions for some operator, right?",
    "start": "6482970",
    "end": "6489415"
  },
  {
    "text": "So if f prime equal to D of f is equal to some Lambda times f,",
    "start": "6489415",
    "end": "6501440"
  },
  {
    "text": "then f is an eigenfunction and Lambda is the eigenvalue. And for the differentiation,",
    "start": "6501440",
    "end": "6507665"
  },
  {
    "text": "you know, can you make a guess of what can be an eigenfunction for differentiation? [inaudible].",
    "start": "6507665",
    "end": "6513620"
  },
  {
    "text": "Exactly. So, uh, say E to the kt, where t is the variable,",
    "start": "6513620",
    "end": "6519780"
  },
  {
    "text": "is equal to k times e to the kt, right?",
    "start": "6520840",
    "end": "6527000"
  },
  {
    "text": "So you can think of the- the exponential as like an eigenfunction for the differentiation- um,",
    "start": "6527000",
    "end": "6532010"
  },
  {
    "text": "um, the differentiation operator, right? And- and the way we represent Ax, um.",
    "start": "6532010",
    "end": "6544670"
  },
  {
    "text": "So if- if for example u equals Av,",
    "start": "6544670",
    "end": "6550114"
  },
  {
    "text": "then we write u_i is equal to summation over",
    "start": "6550114",
    "end": "6555889"
  },
  {
    "text": "j, A_ij V_j, right?",
    "start": "6555890",
    "end": "6561635"
  },
  {
    "text": "This is matrix multiplication, which means now, [NOISE]",
    "start": "6561635",
    "end": "6584420"
  },
  {
    "text": "the kind of equivalent in the- the, um, um, the g of s is equal",
    "start": "6584420",
    "end": "6591350"
  },
  {
    "text": "to integral of let's call this 1 k of s,",
    "start": "6591350",
    "end": "6597800"
  },
  {
    "text": "t f of t dt. Where you see that- okay?",
    "start": "6597800",
    "end": "6610775"
  },
  {
    "text": "So this is like a matrix multiplication, right? But it's just happening in- in,",
    "start": "6610775",
    "end": "6616580"
  },
  {
    "text": "um, infinite dimensional space. And these are also called integral transforms, right?",
    "start": "6616580",
    "end": "6624845"
  },
  {
    "text": "Um, and you might have come across many functions of this form already.",
    "start": "6624845",
    "end": "6631160"
  },
  {
    "text": "Uh, for example, um, when k of s,",
    "start": "6631160",
    "end": "6637370"
  },
  {
    "text": "t is equal to e to the minus st, this becomes the [BACKGROUND] Laplace transform, right?",
    "start": "6637370",
    "end": "6645560"
  },
  {
    "text": "And if you have k of s, t is equal to e to the minus ist,",
    "start": "6645560",
    "end": "6650840"
  },
  {
    "text": "this becomes a Fourier transform, right? So you can think of Laplace transform as Fourier transforms as- as linear transformations.",
    "start": "6650840",
    "end": "6656795"
  },
  {
    "text": "You know, instead of- um, um, if it were a finite dimensional, it would be like, you know, a- a matrix multiplication.",
    "start": "6656795",
    "end": "6662840"
  },
  {
    "text": "In an infinite-dimensional it's- it's, uh, you know, you- you run through some operator,",
    "start": "6662840",
    "end": "6667985"
  },
  {
    "text": "just like multiplying it through a- a matrix and you get, you know, uh, uh, you- the- the, um, uh, index changes from p to s,",
    "start": "6667985",
    "end": "6674060"
  },
  {
    "text": "which means, you know, from one axis, the input changed to the other, right? And the idea here is the- is- is",
    "start": "6674060",
    "end": "6682070"
  },
  {
    "text": "the- the- the big picture that you want to take away from- from all of this is that, functions are points in an infinite dimensional space right?",
    "start": "6682070",
    "end": "6690340"
  },
  {
    "text": "And a lot of operations that you may already be familiar with will- you know, all kinds of transformation, like, the Laplace transform,",
    "start": "6690340",
    "end": "6696670"
  },
  {
    "text": "Fourier transforms, are all just linear operations, you know, including differentiation are all linear operations on",
    "start": "6696670",
    "end": "6702525"
  },
  {
    "text": "points in this infinite dimensional space and these points we call them functions, just the way we call vectors and- as points in a finite dimensional space.",
    "start": "6702525",
    "end": "6710675"
  },
  {
    "text": "And- and this- this way of thinking, you know, um, of thinking of functions which we normally think of them as,",
    "start": "6710675",
    "end": "6716735"
  },
  {
    "text": "you know, some kind of a curve along some axis. Think of them as points in an infinite dimensional space,",
    "start": "6716735",
    "end": "6722139"
  },
  {
    "text": "where the coordinate along each axis is the height of the function at that particular input, right?",
    "start": "6722140",
    "end": "6727780"
  },
  {
    "text": "And this kind of, you know, mental dexterity of switching between these two views,",
    "start": "6727780",
    "end": "6732805"
  },
  {
    "text": "can be extremely useful when we study things like Gaussian processes or kernel methods or even gradient boosting,",
    "start": "6732805",
    "end": "6739285"
  },
  {
    "text": "you know, fo- later in the course, right? Anyway, so, um, you're not gonna ask any of these on your- on your homeworks or exams.",
    "start": "6739285",
    "end": "6747304"
  },
  {
    "text": "But it's just- uh, um, it's a new perspective where you start thinking of functions as just points, right?",
    "start": "6747305",
    "end": "6755060"
  },
  {
    "text": "Not as curves or surface. Just think of them as some points that live in an infinite-dimensional abstract space.",
    "start": "6755060",
    "end": "6761579"
  }
]