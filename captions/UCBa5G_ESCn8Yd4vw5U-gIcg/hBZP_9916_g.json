[
  {
    "start": "0",
    "end": "6870"
  },
  {
    "text": "All right. We'll go ahead and get started. So good morning, everyone. I'm Joshua Ott.",
    "start": "6870",
    "end": "12299"
  },
  {
    "text": "I work in the Stanford\nIntelligence Systems Lab with Michael. And my research focuses\non autonomous exploration.",
    "start": "12300",
    "end": "19650"
  },
  {
    "text": "So that ranges from using teams\nof robots or any autonomous",
    "start": "19650",
    "end": "25080"
  },
  {
    "text": "system to explore some\nunknown environment. So that can range from a rover\nexploring a planetary surface",
    "start": "25080",
    "end": "31680"
  },
  {
    "text": "and looking at where\nto place its drill measurements to collect\nsamples of the surface, or it can be teams of robots--",
    "start": "31680",
    "end": "39030"
  },
  {
    "text": "so aerial and ground robots in\na search-and-rescue application trying to coordinate and\nidentify likely areas",
    "start": "39030",
    "end": "45720"
  },
  {
    "text": "where survivors will be and\nthen going and exploring those spaces. So as you can imagine,\nthat autonomous exploration",
    "start": "45720",
    "end": "51809"
  },
  {
    "text": "relies very heavily\non optimization. And specifically,\nwhat we're going to be talking about today\nwith surrogate models",
    "start": "51810",
    "end": "57938"
  },
  {
    "text": "and probabilistic\nsurrogate models. So I'm going to pick\nup where we left off",
    "start": "57938",
    "end": "63390"
  },
  {
    "text": "on Thursday on surrogate\nmodels, and then we'll dive in to probabilistic\nsurrogate models.",
    "start": "63390",
    "end": "69149"
  },
  {
    "text": "So if you recall\nfrom Thursday, we were talking about\nsurrogate models and we left off on\nmodel selection.",
    "start": "69150",
    "end": "76120"
  },
  {
    "text": "So we had already fit\nour model to some data, and now we're trying to evaluate\nthe quality of that model.",
    "start": "76120",
    "end": "85420"
  },
  {
    "text": "So the data that\nwe've used to fit it, we call that our training data. And so that's what we use\nto create our model based",
    "start": "85420",
    "end": "92159"
  },
  {
    "text": "on whatever data\nwe've seen so far. Once we have that model,\nwe want to evaluate,",
    "start": "92160",
    "end": "99170"
  },
  {
    "text": "how good is our model doing at\ndata that we haven't yet seen? So how well is it generalizing\nto data outside of what",
    "start": "99170",
    "end": "105770"
  },
  {
    "text": "we've just trained on? So to do that, we want to look\nat the generalization error, and this is giving us\nsome metric to say,",
    "start": "105770",
    "end": "113120"
  },
  {
    "text": "this model is doing well, this\nmodel is not doing so well. So that's what we're after here.",
    "start": "113120",
    "end": "118580"
  },
  {
    "text": " So one way to quantify\nthe generalization error",
    "start": "118580",
    "end": "125420"
  },
  {
    "text": "is we can just\nwrite it like this. Is everyone able to\nsee this board here?",
    "start": "125420",
    "end": "132890"
  },
  {
    "text": "OK. So we can write the\ngeneralization error",
    "start": "132890",
    "end": "138590"
  },
  {
    "text": "as the expected squared error-- so we'll walk through\nthis here in a second.",
    "start": "138590",
    "end": "144935"
  },
  {
    "start": "144935",
    "end": "155540"
  },
  {
    "text": "So we have the expectation\nover all of the x's sampled from this space of x's so this\nis all of the possible data we",
    "start": "155540",
    "end": "162940"
  },
  {
    "text": "could see. And then we have-- this is\nthe true function value. So this is what\nwe're interested in.",
    "start": "162940",
    "end": "168490"
  },
  {
    "text": "We don't actually know\nthat function value. So we have our\nsurrogate model here, and this is what our\nsurrogate is telling us.",
    "start": "168490",
    "end": "174790"
  },
  {
    "text": "So this is a good\ndefinition to work off of. The only issue is\nthat we don't actually know this function value.",
    "start": "174790",
    "end": "182500"
  },
  {
    "text": "So it's a good\ndefinition to go with, but it doesn't really get us\nanywhere right now because we don't know this to start with.",
    "start": "182500",
    "end": "189430"
  },
  {
    "text": "So another thing that\nwe can think about is the training error. So this is how well we do on the\ndata that we just trained on.",
    "start": "189430",
    "end": "197060"
  },
  {
    "text": "So very similarly, we can write\nthat like this, where we have--",
    "start": "197060",
    "end": "203319"
  },
  {
    "text": "we've seen these end\ndata points so far, and then we're just\nlooking at the error",
    "start": "203320",
    "end": "209890"
  },
  {
    "text": "on those specific data points. ",
    "start": "209890",
    "end": "222340"
  },
  {
    "text": "So now this avoids this\nproblem because we know this. We've gotten these data\npoints, so we know the function",
    "start": "222340",
    "end": "228690"
  },
  {
    "text": "evaluation at those\nspecific data points and we're comparing that\nwith what we predicted.",
    "start": "228690",
    "end": "234460"
  },
  {
    "text": "So that's our training error. And this is good. It gives us some metric.",
    "start": "234460",
    "end": "240030"
  },
  {
    "text": "The only issue now, though,\nis that just because we have a good training\nerror does not necessarily",
    "start": "240030",
    "end": "245310"
  },
  {
    "text": "mean that it's a good model. We might do really\nwell-fitting our training data,",
    "start": "245310",
    "end": "250380"
  },
  {
    "text": "but we might generalize\nterribly to other data that we haven't seen. So that's still an\nissue that we have.",
    "start": "250380",
    "end": "257398"
  },
  {
    "text": "And that's just--\nthe main point here is that models with\nlow training error, they can still\nperform poorly on data",
    "start": "257399",
    "end": "264630"
  },
  {
    "text": "that we haven't seen so far. So the training error is not\nnecessarily a good indicator",
    "start": "264630",
    "end": "269910"
  },
  {
    "text": "of our generalization error. So one way to get\naround this issue",
    "start": "269910",
    "end": "275750"
  },
  {
    "text": "is through using\nthe holdout method. So this attempts to estimate\nour generalization error",
    "start": "275750",
    "end": "283520"
  },
  {
    "text": "by looking at-- we partition our data set\ninto a training and test set.",
    "start": "283520",
    "end": "291320"
  },
  {
    "text": "So we have some entire data\nset that we've collected, and now we're just saying-- we're going to take\nour 80% of our data",
    "start": "291320",
    "end": "298340"
  },
  {
    "text": "and we'll call that\nour training data, and we're going to\nsave the other 20% and we're not going to\nuse that for fitting,",
    "start": "298340",
    "end": "304950"
  },
  {
    "text": "and that's our test data. So this is nice because we\nbasically still get access",
    "start": "304950",
    "end": "312050"
  },
  {
    "text": "to this, we're just\nsaving it for later. So we take our blue partition\nhere, our training data",
    "start": "312050",
    "end": "319460"
  },
  {
    "text": "and we train on that, we\nfit our surrogate model, and then we're going to\ntest on the red data here.",
    "start": "319460",
    "end": "326400"
  },
  {
    "text": "So an example of this, that you\ncan see the blue points here, those were our training samples.",
    "start": "326400",
    "end": "333509"
  },
  {
    "text": "So we fit some model on\njust those data points.",
    "start": "333510",
    "end": "338690"
  },
  {
    "text": "And then we saved the\ntwo red points over there for our test data.",
    "start": "338690",
    "end": "344639"
  },
  {
    "text": "And so now we can\nsee, well, hey, our model does not do very good\nat predicting those data points",
    "start": "344640",
    "end": "350300"
  },
  {
    "text": "over there. So this is an\nindicator for us now that, OK, our generalization\nerror in this case",
    "start": "350300",
    "end": "356000"
  },
  {
    "text": "is not very good. So that's the holdout method. We're just partitioning\nthe data, saving some of it",
    "start": "356000",
    "end": "362150"
  },
  {
    "text": "to evaluate on. ",
    "start": "362150",
    "end": "367670"
  },
  {
    "text": "So cross-validation\nextends this idea. We're still partitioning\nthe data set.",
    "start": "367670",
    "end": "373370"
  },
  {
    "text": "It's just now,\ninstead of just doing this train-and-test split,\nwe're partitioning it into k different subsets.",
    "start": "373370",
    "end": "379940"
  },
  {
    "text": "So in this case, we\nhave five subsets here. Each one we just\nrandomly partition up,",
    "start": "379940",
    "end": "386720"
  },
  {
    "text": "so 1 through 5 here. And then with those\npartitions, we're going to walk through this and--",
    "start": "386720",
    "end": "392450"
  },
  {
    "text": "so at each iteration\nthat we're going through, we're going to use one of those\npartitions as the holdout set.",
    "start": "392450",
    "end": "400490"
  },
  {
    "text": "So if we walk through an example\nhere, let's look at this. So we've partitioned our\ndata up into these five",
    "start": "400490",
    "end": "405710"
  },
  {
    "text": "different partitions. And now at the first iteration\nwe're going to go through, we're going to hold out\njust the purple partition.",
    "start": "405710",
    "end": "412910"
  },
  {
    "text": "So just this D1 partition here. We're holding that out. We're going to train\nour surrogate model,",
    "start": "412910",
    "end": "419150"
  },
  {
    "text": "fit it on this data\nonly, and then we're going to evaluate it just like\nwe did in the holdout method",
    "start": "419150",
    "end": "425150"
  },
  {
    "text": "on our purple data. And that gives us one\ngeneralization error estimate. We then repeat this.",
    "start": "425150",
    "end": "431220"
  },
  {
    "text": "So now we do it a second time. This time, we\ntrain on everything except for the blue\npartition, and then we",
    "start": "431220",
    "end": "437810"
  },
  {
    "text": "use that blue partition to get\nanother generalization error estimate. So then we can just repeat that\nfor the rest of our partitions.",
    "start": "437810",
    "end": "446210"
  },
  {
    "text": "And as we do that, we end up\nwith now k different estimates",
    "start": "446210",
    "end": "452900"
  },
  {
    "text": "of our generalization error. And so with these,\nwe can then just take the mean and\nstandard deviation",
    "start": "452900",
    "end": "458780"
  },
  {
    "text": "of that generalization\nerror that we got and that will give\nus a new estimate.",
    "start": "458780",
    "end": "464240"
  },
  {
    "text": "So some mean and sigma for\nour generalization error. And you might be\nsaying, well, why not",
    "start": "464240",
    "end": "469354"
  },
  {
    "text": "just do the holdout method? Why do we need cross-validation? Well, the whole point\nof cross validation",
    "start": "469355",
    "end": "474800"
  },
  {
    "text": "is that it makes us\nless susceptible to how we partition our data. So in the holdout\nmethod, you can",
    "start": "474800",
    "end": "481340"
  },
  {
    "text": "think that maybe in\ncertain cases, the way you partition that\ndata gives you a biased estimate of your\ngeneralization error.",
    "start": "481340",
    "end": "488509"
  },
  {
    "text": "You might partition\nit in a way that you get a really good\ngeneralization error, but if you had partitioned it\njust a little bit differently,",
    "start": "488510",
    "end": "495530"
  },
  {
    "text": "that would no\nlonger be the case. So cross-validation removes that\ndependence on the partitioning",
    "start": "495530",
    "end": "501020"
  },
  {
    "text": "by doing this\nrandom partitioning into different subsets.",
    "start": "501020",
    "end": "506290"
  },
  {
    "text": "So any questions on\ncross-validation or holdout? ",
    "start": "506290",
    "end": "515409"
  },
  {
    "text": "Yes? I guess if we go through and\nwe do it this way, technically",
    "start": "515409",
    "end": "523960"
  },
  {
    "text": "at some point, we have trained\nour model on all the data, right?",
    "start": "523960",
    "end": "529990"
  },
  {
    "text": "Yeah. Yeah, that's a good point. So you will-- at each\niteration, you're",
    "start": "529990",
    "end": "535240"
  },
  {
    "text": "going to be eventually\naccessing all of the data. But the caveat here is\nthat at each iteration,",
    "start": "535240",
    "end": "542620"
  },
  {
    "text": "you're not having access\nto the entire data set, you're only getting access\nto that specific partition",
    "start": "542620",
    "end": "549310"
  },
  {
    "text": "at that iteration. So it's still hidden to you,\nand then you get that estimate. So you might have really\nbad generalization error",
    "start": "549310",
    "end": "555339"
  },
  {
    "text": "on one specific\npartition and that will affect your mean\nin standard deviation.",
    "start": "555340",
    "end": "560956"
  },
  {
    "text": "So here, we have five\ndifferent models. Which one do we end up using? Yeah, that's a good question.",
    "start": "560956",
    "end": "567490"
  },
  {
    "text": "So this doesn't tell us\nspecifically which model to select, it just gives us an\nidea of-- based on the model",
    "start": "567490",
    "end": "575890"
  },
  {
    "text": "type that we were using-- so maybe we're just\ndoing like a linear fit, it tells us that based\non how we partitioned",
    "start": "575890",
    "end": "582639"
  },
  {
    "text": "that data with the\nlinear fit, this is what we can expect our\noverall generalization error",
    "start": "582640",
    "end": "587740"
  },
  {
    "text": "estimate to be. ",
    "start": "587740",
    "end": "595200"
  },
  {
    "text": "All right. So today, we're\ngoing to be talking about probabilistic\nsurrogate models. And this is one of my favorite\nareas in optimization.",
    "start": "595200",
    "end": "604259"
  },
  {
    "text": "There's two main\nreasons for that. The first is just that\nthey're extremely versatile.",
    "start": "604260",
    "end": "609840"
  },
  {
    "text": "So they're very\nwidely applicable whether you're looking\nat geoscience modeling-- so looking at minerals in the\nsubsurface and modeling how",
    "start": "609840",
    "end": "618180"
  },
  {
    "text": "they change spatially. Or if you're looking\nat financial modeling. They're applicable\nthere as well.",
    "start": "618180",
    "end": "624190"
  },
  {
    "text": "So any time I see\nthat where something is used in two very\ndifferent fields,",
    "start": "624190",
    "end": "629220"
  },
  {
    "text": "that's like a flag in my head. Like, hey, this is pretty\ncool, this is important. So that's one of the\nreasons that I really",
    "start": "629220",
    "end": "635760"
  },
  {
    "text": "like probabilistic surrogate\nmodels, just the versatility. The other reason is\nthat I think it's",
    "start": "635760",
    "end": "640889"
  },
  {
    "text": "just a very powerful\nconcept of being able to quantify the\nuncertainty in your prediction.",
    "start": "640890",
    "end": "646980"
  },
  {
    "text": "And I think that sometimes\ngets overlooked in modeling applications because\nyou just have your model",
    "start": "646980",
    "end": "652680"
  },
  {
    "text": "and it's doing a reasonable job\nof predicting what you haven't seen yet, so you're saying,\nOK, great, that's awesome,",
    "start": "652680",
    "end": "658770"
  },
  {
    "text": "it's working well, but you're\nnot taking that uncertainty quantification into account. So I think that idea\nis really powerful.",
    "start": "658770",
    "end": "665560"
  },
  {
    "text": "So versatility and\nuncertainty qualifications, those are the reasons\nwhy I really like",
    "start": "665560",
    "end": "671130"
  },
  {
    "text": "probabilistic surrogate models. And my hope for\nthe lecture today is to really give an\nintuitive understanding first,",
    "start": "671130",
    "end": "678180"
  },
  {
    "text": "and then dive into the\nmath and the details. So if you ever\nlose that intuition of why are we doing what\nwe're doing, just feel free,",
    "start": "678180",
    "end": "685350"
  },
  {
    "text": "ask a question, and\nwe'll dive into that. ",
    "start": "685350",
    "end": "690910"
  },
  {
    "text": "So to start off with,\nlooking at today, we're going to be going\nprobabilistic surrogate models",
    "start": "690910",
    "end": "696090"
  },
  {
    "text": "first, what they\nare, how we use them, and then we're\ngoing to be looking at how can we use them for our\nactual optimization purpose.",
    "start": "696090",
    "end": "704530"
  },
  {
    "text": "So I want to start off first\nwith just zooming out, looking at everything we've\nseen so far, all",
    "start": "704530",
    "end": "710430"
  },
  {
    "text": "of the different\npieces of optimization and where this fits in. OK, so today, we're\nfocused here on",
    "start": "710430",
    "end": "717899"
  },
  {
    "text": "probabilistic surrogate models\nand surrogate optimization. And you can see, it's\nsort of its own branch",
    "start": "717900",
    "end": "723839"
  },
  {
    "text": "in the optimization topics\nthat we've covered so far. So that's not to say that\neverything on the top",
    "start": "723840",
    "end": "729690"
  },
  {
    "text": "is not important. It's definitely important\nand necessary to learn about, especially if you're going to\nbe using this in your work.",
    "start": "729690",
    "end": "737250"
  },
  {
    "text": "But the nice part is that\nprobabilistic surrogate models are sort of their\nown branch here.",
    "start": "737250",
    "end": "742480"
  },
  {
    "text": "So my hope is that if\nthis is the first lecture you're attending all\nquarter and you're just dropping in right here,\nthat you would still",
    "start": "742480",
    "end": "749497"
  },
  {
    "text": "be able to walk away\nfrom this lecture with a solid understanding of\nprobabilistic surrogate models.",
    "start": "749497",
    "end": "755520"
  },
  {
    "text": "So that's my hope. OK, so let's get started here\nwith just a refresher of why",
    "start": "755520",
    "end": "761280"
  },
  {
    "text": "we care about surrogate\nmodels in the first place. So in many different\noptimization problems,",
    "start": "761280",
    "end": "768750"
  },
  {
    "text": "often, the actual\nevaluation of the function is pretty expensive. So the function\nthat you're actually",
    "start": "768750",
    "end": "774630"
  },
  {
    "text": "interested in optimizing,\ngetting those sample values are expensive. So for example, if you're\ndoing some lengthy hardware",
    "start": "774630",
    "end": "783120"
  },
  {
    "text": "fabrication process\nthat requires you to build the actual hardware\ndesign before you test it",
    "start": "783120",
    "end": "789390"
  },
  {
    "text": "and get that\nfunction evaluation, that's going to be\nan expensive process. It costs a lot of time and\nit costs a lot of dollars",
    "start": "789390",
    "end": "795990"
  },
  {
    "text": "to go and actually get\nthis hardware prototype and then test it. So a good example of that is\nif you're building an aircraft",
    "start": "795990",
    "end": "803790"
  },
  {
    "text": "and you want to test\nit in a wind tunnel. You can't just go and build\nthousands of different aircraft",
    "start": "803790",
    "end": "808950"
  },
  {
    "text": "prototypes before you\nactually ship one. So you have to have some way to\npredict how your function will",
    "start": "808950",
    "end": "817079"
  },
  {
    "text": "evolve based on\nthe limited samples that you've seen so far.",
    "start": "817080",
    "end": "822770"
  },
  {
    "text": "And lastly, maybe\nanother example here is you're about to go train\nsome new deep neural network",
    "start": "822770",
    "end": "829250"
  },
  {
    "text": "and it has billions\nof parameters and it's going to take\nweeks to months to train.",
    "start": "829250",
    "end": "834477"
  },
  {
    "text": "You want to make sure that\nyour hyperparameters in there all are all tuned up nicely\nbefore you go and spend",
    "start": "834477",
    "end": "839930"
  },
  {
    "text": "$500,000 training that model. So that's where the\nsurrogate model comes in. It's a surrogate of\nthe actual function",
    "start": "839930",
    "end": "846890"
  },
  {
    "text": "that we're interested in.  And so because we're talking\nabout probabilistic surrogate",
    "start": "846890",
    "end": "853889"
  },
  {
    "text": "models, naturally,\nwe would think that there's going to be some\nprobability involved here. So we're going to start just\nwith a refresher of our friend,",
    "start": "853890",
    "end": "860750"
  },
  {
    "text": "the Gaussian\ndistribution, and then and then we're going to work\nup from there to the Gaussian process.",
    "start": "860750",
    "end": "866350"
  },
  {
    "text": "So I'm sure most of us have\nprobably seen the Gaussian distribution at one point. This is just the\nexpression here for it",
    "start": "866350",
    "end": "873330"
  },
  {
    "text": "where we have some mean\nand covariance matrix. And in two dimensions,\nwe get something",
    "start": "873330",
    "end": "878850"
  },
  {
    "text": "that looks like this\nwhere these are just the contour plots of it-- so\njust taking the slices here.",
    "start": "878850",
    "end": "884880"
  },
  {
    "text": "And just looking at the effect\nthat the covariance matrix has. So as we change that\ncovariance matrix,",
    "start": "884880",
    "end": "890220"
  },
  {
    "text": "it changes the orientation\nand shape of our Gaussian distribution here. ",
    "start": "890220",
    "end": "897840"
  },
  {
    "text": "And so if we want to look\nat different distributions with the Gaussian,\nwe can have two",
    "start": "897840",
    "end": "904550"
  },
  {
    "text": "jointly-distributed\nrandom variables, a, b, and we can represent\nthem like this.",
    "start": "904550",
    "end": "910920"
  },
  {
    "text": "And so that allows us,\nlike any distribution, we can marginalize out\none of the variables.",
    "start": "910920",
    "end": "916680"
  },
  {
    "text": "So for a general\nprobability distribution, marginalization is just\nintegrating out with respect",
    "start": "916680",
    "end": "923150"
  },
  {
    "text": "to one of those variables. So we can just integrate out the\neffect of b on our distribution",
    "start": "923150",
    "end": "928940"
  },
  {
    "text": "and that gives us just a. And then lastly, we have the\nconditional distribution.",
    "start": "928940",
    "end": "935550"
  },
  {
    "text": "So this is just saying that if\nwe already know the value of b, we can say, what is\nthe distribution now",
    "start": "935550",
    "end": "942260"
  },
  {
    "text": "that we expect of a? So it's a give and b , And\nWe just represent that here through the Gaussian\nupdate equations.",
    "start": "942260",
    "end": "948810"
  },
  {
    "text": "So it's just-- we just\nhave this given value of b, and then we're using that to\ncondition on that value of b,",
    "start": "948810",
    "end": "955820"
  },
  {
    "text": "and that will give\nus a new Gaussian distribution for our a. So That's just a\nrefresher because we're",
    "start": "955820",
    "end": "962510"
  },
  {
    "text": "going to make use of these\nin building up the Gaussian process. ",
    "start": "962510",
    "end": "968720"
  },
  {
    "text": "OK, so for Gaussian\nprocesses, we're going to start with just\na visual example here to build that intuition.",
    "start": "968720",
    "end": "976190"
  },
  {
    "text": "So we're going to\nstart-- and it might seem a little weird at first,\nbut what we're going to do here is we're just going to plop our\nGaussian distribution in one",
    "start": "976190",
    "end": "983810"
  },
  {
    "text": "dimension and we're going to\ntake it all along the y-axis here. And then we're just going to\ntake five samples from that.",
    "start": "983810",
    "end": "991420"
  },
  {
    "text": "So we get our five samples\nhere, and we're just plotting the dimension on the x-axis.",
    "start": "991420",
    "end": "997125"
  },
  {
    "text": "So here, it's just\none dimension. So they're all just along\nthat first dimension there. And then we're just plotting\nthe value of the samples",
    "start": "997125",
    "end": "1003390"
  },
  {
    "text": "along the y-axis. So that's all we've done. Pretty simple. We can then take\nthe same exact idea",
    "start": "1003390",
    "end": "1009180"
  },
  {
    "text": "and we're just going to build\nit up two higher dimensions. So if we go to two\ndimensions now,",
    "start": "1009180",
    "end": "1014550"
  },
  {
    "text": "we can have our two-dimensional\ndistribution here. We take, again, five samples\nfrom that distribution.",
    "start": "1014550",
    "end": "1019772"
  },
  {
    "text": "And now we're just\nwe're going to-- we're going to plot the dimensions\nalong the x-axis here. So we take the first x1\ndimension and we plot it there,",
    "start": "1019772",
    "end": "1028770"
  },
  {
    "text": "and then we're going\nto repeat, and we're going to do the same thing\nfor the second dimension. So we take the values from\nthe second dimension now",
    "start": "1028770",
    "end": "1036449"
  },
  {
    "text": "and we're going to\nplot them corresponding to the second dimension. So all we've done is just\ntaken those values and plotted",
    "start": "1036450",
    "end": "1042630"
  },
  {
    "text": "x1 on the x-axis\nand then x2 as well, and their sample value\nis on the y-axis.",
    "start": "1042630",
    "end": "1048717"
  },
  {
    "text": "And then we're showing the\ncovariance matrix here. So you can see, there's\nnothing special about this. We're just showing that there's\nsome correlation here as well.",
    "start": "1048717",
    "end": "1058270"
  },
  {
    "text": "All right, so now we can\njust rinse and repeat this up to higher dimensions. So if we go to three\ndimensions, you can see,",
    "start": "1058270",
    "end": "1065080"
  },
  {
    "text": "we get some samples here. And now we're just-- again,\nthere's nothing special here about this particular\ncovariance matrix we're using.",
    "start": "1065080",
    "end": "1071410"
  },
  {
    "text": "All we're trying\nto illustrate is that there's just some\nspatial autocorrelation now. So the value of x1 is\nmore closely related",
    "start": "1071410",
    "end": "1078850"
  },
  {
    "text": "to the value of x3-- or, sorry, the value\nof x2 than x1 is to x3.",
    "start": "1078850",
    "end": "1084190"
  },
  {
    "text": "So the closer you are\nin dimension, the more relationship you have. And so you can just see that\nthrough the covariance terms",
    "start": "1084190",
    "end": "1092050"
  },
  {
    "text": "in the matrix. OK, so we can-- oh yeah, question.",
    "start": "1092050",
    "end": "1098300"
  },
  {
    "text": "I think maybe like 0.61 in\nthe last row should be 0.14.",
    "start": "1098300",
    "end": "1106180"
  },
  {
    "text": "Yes, that's true. Also, yeah, this 0.14 should\nbe 0.61, this one here.",
    "start": "1106180",
    "end": "1114010"
  },
  {
    "text": "Is that what-- yeah. Yeah, that's right. ",
    "start": "1114010",
    "end": "1122220"
  },
  {
    "text": "OK, so we can continue that up. So in four dimensions, we\ncan do it in five, six.",
    "start": "1122220",
    "end": "1127409"
  },
  {
    "text": "We'll stop at eight\nfor this case. So we have five samples now from\nan eight-dimensional Gaussian",
    "start": "1127410",
    "end": "1133950"
  },
  {
    "text": "where we've just plotted\nthem along the axis here. And so maybe these are starting\nto look a little like samples",
    "start": "1133950",
    "end": "1142380"
  },
  {
    "text": "from a function to you. And if they're not,\nthen we'll clarify it by going up even\nhigher dimensions.",
    "start": "1142380",
    "end": "1147600"
  },
  {
    "text": "But that's exactly the\nintuition that we're trying to get to here. So if we take this up to\nhundreds of dimensions--",
    "start": "1147600",
    "end": "1155200"
  },
  {
    "text": "so let's go now to\nlike 200 dimensions, we'll start getting these\nsmooth-looking functions.",
    "start": "1155200",
    "end": "1160950"
  },
  {
    "text": "And we still have that\nspatial correlation here. And so all we've done now, we\njust have lots of samples now,",
    "start": "1160950",
    "end": "1169169"
  },
  {
    "text": "and we start getting these\nsmoother-looking functions. And so that's exactly what\na Gaussian process is.",
    "start": "1169170",
    "end": "1174240"
  },
  {
    "text": "It's just a distribution\nover functions. And all we're showing here\nis five different samples",
    "start": "1174240",
    "end": "1180090"
  },
  {
    "text": "from a Gaussian process. So it's just distribution\nover functions, and all we're\nlooking at here are",
    "start": "1180090",
    "end": "1185910"
  },
  {
    "text": "the samples from that function. OK, so if we want to make this\nconnection a little clearer,",
    "start": "1185910",
    "end": "1193049"
  },
  {
    "text": "for a multivariate\nGaussian, we have it defined by some mean\nand covariance matrix.",
    "start": "1193050",
    "end": "1198720"
  },
  {
    "text": "For a Gaussian\nprocess, we're just defining it by some mean\nfunction and some kernel",
    "start": "1198720",
    "end": "1204360"
  },
  {
    "text": "function. So we can write\nout the structure that we're going to\nbe working with here. ",
    "start": "1204360",
    "end": "1216419"
  },
  {
    "text": "So we're just going\nto write it like this, and we're going to break\nthis down in much more detail",
    "start": "1216420",
    "end": "1221580"
  },
  {
    "text": "as we go. But just so we have a starting\npoint here, we have our y's. These are the points that\nwe're going to be observing.",
    "start": "1221580",
    "end": "1229410"
  },
  {
    "text": "So our function values. And we're saying\nthat those are coming from a Gaussian\nprocess which we're",
    "start": "1229410",
    "end": "1236190"
  },
  {
    "text": "defining by some mean\nfunction here at the points that we've seen--",
    "start": "1236190",
    "end": "1243250"
  },
  {
    "text": "so the m different data\npoints that we've seen. ",
    "start": "1243250",
    "end": "1250790"
  },
  {
    "text": "And you can think about this is\nvery similar to the mean vector that we have with a Gaussian.",
    "start": "1250790",
    "end": "1256029"
  },
  {
    "text": "And then we have our\ncovariance matrix as well. ",
    "start": "1256030",
    "end": "1274543"
  },
  {
    "text": "That might be cut off\nfor you guys over there. ",
    "start": "1274543",
    "end": "1282778"
  },
  {
    "text": "OK, so we're just writing\nit out for all of our m different data points.",
    "start": "1282778",
    "end": "1289795"
  },
  {
    "text": "And we're just\ngoing to be making use of this in the\nrest of what we're going to be talking about here.",
    "start": "1289795",
    "end": "1295860"
  },
  {
    "text": "So all we have, we just\nhave our main function specified on our data points.",
    "start": "1295860",
    "end": "1301340"
  },
  {
    "text": "And then we're looking\nat the covariance matrix here is what we've\nbuilt, but we used",
    "start": "1301340",
    "end": "1306440"
  },
  {
    "text": "this covariance function or\nthis kernel function here. And so you might be asking, what\nis this covariance function?",
    "start": "1306440",
    "end": "1313460"
  },
  {
    "text": "Well, that's the modeling\nchoice that you have, but a very common\none and the one that we'll be using\na lot today is",
    "start": "1313460",
    "end": "1320450"
  },
  {
    "text": "the squared exponential kernel. So how we write the squared\nexponential kernel-- ",
    "start": "1320450",
    "end": "1339240"
  },
  {
    "text": "so squared here. OK. So we have r squared\nexponential kernel.",
    "start": "1339240",
    "end": "1344940"
  },
  {
    "text": "It's just the negative\nexponential here. And then we have some\nlength scale parameter. So what this looks like\nif we were to draw it out,",
    "start": "1344940",
    "end": "1352660"
  },
  {
    "text": "it looks something like this\nwhere we get this decay. And so the length scale controls\nhow fast we're decaying there.",
    "start": "1352660",
    "end": "1363090"
  },
  {
    "text": "So if we want to look at\nsome examples of this,",
    "start": "1363090",
    "end": "1370159"
  },
  {
    "text": "so this is just the same exact\nstructure that we've written, and this is the\ndefinition that we'll",
    "start": "1370160",
    "end": "1375740"
  },
  {
    "text": "be working with here today. So if we look at\nsome examples, this is using the squared\nexponential kernel,",
    "start": "1375740",
    "end": "1382070"
  },
  {
    "text": "and we're just looking at--\nwe've already seen the center one where l equals 1. Now we're just\nlooking at how does",
    "start": "1382070",
    "end": "1388490"
  },
  {
    "text": "that length scale change\nthe types of functions that we're getting? So as you can see, that as\nwe decrease the length scale,",
    "start": "1388490",
    "end": "1394070"
  },
  {
    "text": "we get much more jagged,\nsharply-varying functions; and as we increase it, we get\nmore slowly smoother varying",
    "start": "1394070",
    "end": "1402020"
  },
  {
    "text": "functions. So that's showing the effect\nthat the kernel function has",
    "start": "1402020",
    "end": "1408020"
  },
  {
    "text": "on the type of function\nthat we're using. And so just like that length\nscale parameter, you can see, it has a pretty\nsignificant impact",
    "start": "1408020",
    "end": "1414830"
  },
  {
    "text": "on the types of function\nsamples that we get. The kernel function\nitself that we're choosing",
    "start": "1414830",
    "end": "1421700"
  },
  {
    "text": "has an even bigger impact. So if you want to look at-- we're not just limited to the\nsquared exponential kernel.",
    "start": "1421700",
    "end": "1428450"
  },
  {
    "text": "We can use any number of these. These are just examples,\nthere's even more out there that you can use.",
    "start": "1428450",
    "end": "1434460"
  },
  {
    "text": "So you can see that\nthe choice here makes a pretty\nsignificant impact on the types of functions\nyou're going to see.",
    "start": "1434460",
    "end": "1440309"
  },
  {
    "text": "And so that's where it\nbecomes your modeling choice of which one to use.",
    "start": "1440310",
    "end": "1445440"
  },
  {
    "text": "And so I'll highlight two here. We've already seen the\nsquared exponential kernel. The second one is the\nreturn kernel, this one",
    "start": "1445440",
    "end": "1452059"
  },
  {
    "text": "down here that you can see. And they're quite different. The return is much\nmore jagged and has",
    "start": "1452060",
    "end": "1458450"
  },
  {
    "text": "those sharp, sharp spikes in it,\nwhereas the squared exponential is a lot smoother.",
    "start": "1458450",
    "end": "1463730"
  },
  {
    "text": "And so the reason\nI'm highlighting this is just one example is\nin the geoscience modeling.",
    "start": "1463730",
    "end": "1470029"
  },
  {
    "text": "Often when you're looking at\nminerals in the subsurface, they're not just these smoothly,\nnice varying functions.",
    "start": "1470030",
    "end": "1477050"
  },
  {
    "text": "There's a lot of\nnoise there with these jagged discontinuities\nin the subsurface.",
    "start": "1477050",
    "end": "1482660"
  },
  {
    "text": "And so a common thing to do is\ntake a combination of kernels. So you can represent the smooth\nnature of the spatial changes",
    "start": "1482660",
    "end": "1491870"
  },
  {
    "text": "in the subsurface,\nbut you can also represent that sharp or\nvarying term as well.",
    "start": "1491870",
    "end": "1497670"
  },
  {
    "text": "So that's just an\nexample to highlight how you can take\ncombinations of these,",
    "start": "1497670",
    "end": "1503120"
  },
  {
    "text": "and it's really that\nmodeling choice. And you can use\nthe previous things that we've talked\nabout model selection",
    "start": "1503120",
    "end": "1509059"
  },
  {
    "text": "to determine how your\nmodel is doing here. Yeah?",
    "start": "1509060",
    "end": "1515380"
  },
  {
    "text": "Now what is the x-axis of-- Yeah. The x-axis here, these\nare just basically",
    "start": "1515380",
    "end": "1522710"
  },
  {
    "text": "like the x's that you've seen. So these are the x's that you've\nseen, not necessarily where",
    "start": "1522710",
    "end": "1528767"
  },
  {
    "text": "exactly you have data,\nbut just where you're querying this distribution at. So we have some distribution\nover our y-values.",
    "start": "1528767",
    "end": "1537090"
  },
  {
    "text": "And so we're sampling the\nfunction values, which is the y-axis, and we're\nsampling those function values",
    "start": "1537090",
    "end": "1542330"
  },
  {
    "text": "at these x-points.  Yeah?",
    "start": "1542330",
    "end": "1547480"
  },
  {
    "text": "Previously when you have the-- I guess like [INAUDIBLE],,\nis m the number of variables",
    "start": "1547480",
    "end": "1555380"
  },
  {
    "text": "or the number of data points? Yeah. m is the number of\ndata points that we have. So this-- well, OK.",
    "start": "1555380",
    "end": "1560810"
  },
  {
    "text": "So this m is our mean\nfunction, but this m is the number of data points\nthat we have seen so far.",
    "start": "1560810",
    "end": "1566250"
  },
  {
    "text": "So we have m data\npoints that we've seen, and we're going to get\ninto a little bit of that here in a second, so\nit should clarify that.",
    "start": "1566250",
    "end": "1573285"
  },
  {
    "text": "So why [INAUDIBLE] because when\nwe were doing examples before, the covariance varies with\nthe number of variables.",
    "start": "1573285",
    "end": "1579660"
  },
  {
    "text": "[INAUDIBLE] examples, I believe. Like, it was increasing in size\nwhen you add in more variables.",
    "start": "1579660",
    "end": "1585179"
  },
  {
    "text": "Right. Yes. That's-- [INAUDIBLE] shouldn't it be\nlike the number of variables?",
    "start": "1585180",
    "end": "1593020"
  },
  {
    "text": "The number of-- yeah, so-- [INAUDIBLE] the\nnumber of variables",
    "start": "1593020",
    "end": "1598450"
  },
  {
    "text": "by the number of variables? Yeah. So I think the example\nyou're talking about was when we were building\nup the intuition here, yeah.",
    "start": "1598450",
    "end": "1606260"
  },
  {
    "text": "So this is slightly\ndifferent, and we'll see it, I think, in the next\ncouple of slides.",
    "start": "1606260",
    "end": "1611280"
  },
  {
    "text": "But just for constructing--\nso this is on what we've seen so far. These are the samples\nthat we've seen.",
    "start": "1611280",
    "end": "1617760"
  },
  {
    "text": "So it's not an exact one-to\none map to that intuition that we were building\nup, but I think you'll see it coming up here.",
    "start": "1617760",
    "end": "1625500"
  },
  {
    "text": "And if you don't, then just\nask again and we'll clarify. Yeah? Well, we are using\nthis kernel function",
    "start": "1625500",
    "end": "1631880"
  },
  {
    "text": "for controlling\neach of the elements in a covariance matrix,\nwhich means that this kernel",
    "start": "1631880",
    "end": "1639470"
  },
  {
    "text": "function is approximating the\ncovariance between two data",
    "start": "1639470",
    "end": "1645799"
  },
  {
    "text": "points. But how-- it doesn't\nmake sense to me that we have a covariance\nbetween two data points.",
    "start": "1645800",
    "end": "1651700"
  },
  {
    "text": "Like, how-- like, what does\nit mean to have covariance between two data points? Yeah.",
    "start": "1651700",
    "end": "1656930"
  },
  {
    "text": "Yeah, that's a great question. So if we take the spatial\nmodeling aspect of it.",
    "start": "1656930",
    "end": "1663429"
  },
  {
    "text": "So say that we have-- and actually, we'll go\nwith like phone prices.",
    "start": "1663430",
    "end": "1669540"
  },
  {
    "text": "So say that you have-- we'll go here. Say that we have--",
    "start": "1669540",
    "end": "1675840"
  },
  {
    "text": "we've got prices here, and we're\nlooking at the number of sales that we have for those phones.",
    "start": "1675840",
    "end": "1681990"
  },
  {
    "text": "So maybe so far, this is\njust based on historical data here that we have,\nwe've sold the phone",
    "start": "1681990",
    "end": "1690383"
  },
  {
    "text": "at these different\nprices and then we have data on the number\nof phones that we sold.",
    "start": "1690383",
    "end": "1695700"
  },
  {
    "text": "So the spatial\nrelationship there is just telling us\nhow much we expect",
    "start": "1695700",
    "end": "1701640"
  },
  {
    "text": "the function to\nchange based on how the price relationship changes.",
    "start": "1701640",
    "end": "1706780"
  },
  {
    "text": "So I think this\none is intuitive. As we lower the price we\nexpect to sell more phones",
    "start": "1706780",
    "end": "1712590"
  },
  {
    "text": "because the price is lower,\nup to a certain point. So what we can do\nhere is we can--",
    "start": "1712590",
    "end": "1720600"
  },
  {
    "text": "we're basically saying that the\ncovariance between, let's say, this price here,\nwe'll call it, I",
    "start": "1720600",
    "end": "1726300"
  },
  {
    "text": "don't know, P1, and this\nprice here, P2, we're saying that those are going to\nbe more closely related than P1",
    "start": "1726300",
    "end": "1733980"
  },
  {
    "text": "and whatever at\nthis point, P3 here. So that's the\nintuition to have, is",
    "start": "1733980",
    "end": "1739170"
  },
  {
    "text": "that the closer the\ndata points are-- and that distance metric is\nanother thing that you can",
    "start": "1739170",
    "end": "1744270"
  },
  {
    "text": "change, but let's say just\nin Euclidean distance here, the closer those\nare, the more related",
    "start": "1744270",
    "end": "1750299"
  },
  {
    "text": "their actual function\nvalue will be. Yeah. ",
    "start": "1750300",
    "end": "1758789"
  },
  {
    "text": "OK. So here, I think\nwe're just going to build a mental\nmodel picture of what",
    "start": "1758790",
    "end": "1764100"
  },
  {
    "text": "we're going to be working with. So the black line here is\nthe true function value. So that's what we're actually\ninterested in that we",
    "start": "1764100",
    "end": "1770970"
  },
  {
    "text": "can't directly observe. Well, we can take we\ncan take samples of it, but in this case,\nit's a noisy sample.",
    "start": "1770970",
    "end": "1777909"
  },
  {
    "text": "So that's why our black-- the black point here is a\nnoisy sample of that function.",
    "start": "1777910",
    "end": "1783400"
  },
  {
    "text": "So all we have right\nnow is one data point. And we can-- based on the\nsquared exponential kernel,",
    "start": "1783400",
    "end": "1788520"
  },
  {
    "text": "that's what we're using here. We can then make predictions\nand then sample from it.",
    "start": "1788520",
    "end": "1793920"
  },
  {
    "text": "So those blue lines\nthat you're seeing, those are the samples from\nthe function from the Gaussian",
    "start": "1793920",
    "end": "1799950"
  },
  {
    "text": "process. And so we can then\ncondition on more samples. So as we take more measurements,\nwe can make updates here.",
    "start": "1799950",
    "end": "1807620"
  },
  {
    "text": "So we can update our\nGaussian process conditioned on these new measurements.",
    "start": "1807620",
    "end": "1812810"
  },
  {
    "text": "And so as we go with only\nthree data points here, again, they're noisy, so maybe\nwe have some sensor noise,",
    "start": "1812810",
    "end": "1819620"
  },
  {
    "text": "so we're not exactly measuring\nthe true function value, and that's why there's\nstill uncertainty",
    "start": "1819620",
    "end": "1824960"
  },
  {
    "text": "around those points. But we can basically quantify\nthe uncertainty there that we have around\nour true function.",
    "start": "1824960",
    "end": "1831360"
  },
  {
    "text": "And so obviously, this is-- with three data points, we're\ndoing quite a good job here. A lot of real-world\nfunctions are not this smooth",
    "start": "1831360",
    "end": "1838160"
  },
  {
    "text": "and don't look like\nthat, but this is just to have that picture of how\nwe're progressing and adding",
    "start": "1838160",
    "end": "1843830"
  },
  {
    "text": "more data points to\nour Gaussian process. And it's not just limited\nto one dimensions.",
    "start": "1843830",
    "end": "1850130"
  },
  {
    "text": "So you can take this up and\ndo it in higher dimensions as well. So-- yes?",
    "start": "1850130",
    "end": "1855230"
  },
  {
    "text": "On the previous\nslide, I was curious what all the\ndifferent blue lines are representing\nwhen you were showing",
    "start": "1855230",
    "end": "1861560"
  },
  {
    "text": "the different types of kernels. Yeah. It seemed like the\ndifferent lines were the models you would\nget for each design variable?",
    "start": "1861560",
    "end": "1869360"
  },
  {
    "text": "Maybe that's one. But if we only have one or\ntwo design variables here, then what are all the lines?",
    "start": "1869360",
    "end": "1875570"
  },
  {
    "text": "Yeah. Great question. So the blue lines\nare the samples from our Gaussian process.",
    "start": "1875570",
    "end": "1881990"
  },
  {
    "text": "So just like in a\none-dimensional Gaussian, you have something-- you have\nyour bell curve like that,",
    "start": "1881990",
    "end": "1890240"
  },
  {
    "text": "and you can take five\ndifferent samples from it and get something like that.",
    "start": "1890240",
    "end": "1896150"
  },
  {
    "text": "That's essentially\nall that we're doing. Those are the blue lines here. So we're just sampling I think\nlike 10 here or something.",
    "start": "1896150",
    "end": "1904580"
  },
  {
    "text": "At each point you're sampling\nthe Gaussian process-- Yep. Yep, yep. So yeah, we're\ngoing along here, we",
    "start": "1904580",
    "end": "1910280"
  },
  {
    "text": "have 200 query points to make\nthis plot, and at those query points is where we're asking\nfor the function value.",
    "start": "1910280",
    "end": "1919130"
  },
  {
    "text": "And we're going to-- I think that's the\nvery next one here. So we can do this in\ntwo dimensions as well.",
    "start": "1919130",
    "end": "1925620"
  },
  {
    "text": "And now I think\nthis will hopefully make things a little clearer. So we've seen just like the\nbuild-up of this structure,",
    "start": "1925620",
    "end": "1932750"
  },
  {
    "text": "but now we're going to\nactually get to the prediction here of how do we use\nthese to make predictions",
    "start": "1932750",
    "end": "1938480"
  },
  {
    "text": "about the function?  So this example\nthat we just saw,",
    "start": "1938480",
    "end": "1945870"
  },
  {
    "text": "we're going to introduce\nthe terminology here. So the x's, those\nare the set of points",
    "start": "1945870",
    "end": "1952770"
  },
  {
    "text": "where we have a known value. So these are x's here. That's where we have those known\nvalues that we've measured.",
    "start": "1952770",
    "end": "1960970"
  },
  {
    "text": "The y's are the points that\nwe've actually observed. So those are the known values\nof our function corresponding",
    "start": "1960970",
    "end": "1969970"
  },
  {
    "text": "to the x's that we have. The x stars here,\nthose are the points",
    "start": "1969970",
    "end": "1975220"
  },
  {
    "text": "that we want to\nmake predictions at. So in this case, the\nx stars are basically",
    "start": "1975220",
    "end": "1982090"
  },
  {
    "text": "the entire x-axis domain. We've just very\nfinely gone through and said like, we have 200\nx stars in this domain here.",
    "start": "1982090",
    "end": "1990590"
  },
  {
    "text": "So those are the\npoints that we want to predict our function at.",
    "start": "1990590",
    "end": "1995950"
  },
  {
    "text": "And then lastly, the y hats\nare the actual predicted values that we're making. So the darkest one\nis showing the mean,",
    "start": "1995950",
    "end": "2003360"
  },
  {
    "text": "and then those lighter\nsamples are just showing different samples\nfrom the distribution.",
    "start": "2003360",
    "end": "2010650"
  },
  {
    "text": "So that's breaking it\ndown, and now we'll build up what we're\nactually talking about here.",
    "start": "2010650",
    "end": "2017600"
  },
  {
    "text": "So just like before, we can\nwrite the joint distribution here where y hat is\nthe predicted values",
    "start": "2017600",
    "end": "2025450"
  },
  {
    "text": "and y is the values we've\nactually seen so far. So we have our mean function. We can get the mean at both\nthe points that we've observed",
    "start": "2025450",
    "end": "2033159"
  },
  {
    "text": "and the points where\nwe're making predictions, and then we're just building\nthis covariance matrix,",
    "start": "2033160",
    "end": "2038440"
  },
  {
    "text": "which if you think back to that\ninitial slide on Gaussian-- on multivariate\nGaussians, these are just",
    "start": "2038440",
    "end": "2044650"
  },
  {
    "text": "our A, B, and C\nmatrices that we're now using in our Gaussian process.",
    "start": "2044650",
    "end": "2050829"
  },
  {
    "text": "So we can extend this now. So this is just the\nexpression for our mean. So our mean vector here\nis just a collection",
    "start": "2050830",
    "end": "2058599"
  },
  {
    "text": "of the mean on the points\nthat we're looking at, and the covariance matrix\nwe've already seen here.",
    "start": "2058600",
    "end": "2064914"
  },
  {
    "text": " So just like we had the\nGaussian update equations,",
    "start": "2064914",
    "end": "2071239"
  },
  {
    "text": "this might look a\nlittle intimidating. You're like, whoa, what's that? All that is the Gaussian\nupdate equations.",
    "start": "2071239",
    "end": "2076949"
  },
  {
    "text": "So that same mu of A given B\nthat we looked at, it's just-- that's all this is. So we're just looking at--",
    "start": "2076949",
    "end": "2083690"
  },
  {
    "text": "we want to make this\nprediction, the what we want to predict at y hat given\nour y-values that we've already",
    "start": "2083690",
    "end": "2090050"
  },
  {
    "text": "observed, and then\nwe have just-- this is just from the Gaussian\nupdate equations here.",
    "start": "2090050",
    "end": "2095520"
  },
  {
    "text": "So this is the mean at all\nof the predicted values, and then we're looking\nat the covariance",
    "start": "2095520",
    "end": "2101570"
  },
  {
    "text": "between the points\nwe want to predict at and the points\nwe've already seen, and then the covariance\nbetween the points",
    "start": "2101570",
    "end": "2107330"
  },
  {
    "text": "that we've seen as well. And an important\npoint to notice here is that the covariance does\nnot depend on the actual values",
    "start": "2107330",
    "end": "2116570"
  },
  {
    "text": "that you see. So you can see in\nthe covariance term, y does not show up at all.",
    "start": "2116570",
    "end": "2121820"
  },
  {
    "text": "So your uncertainty regions\naround your Gaussian process, they don't depend on\nwhat you've seen at all.",
    "start": "2121820",
    "end": "2128510"
  },
  {
    "text": "So that's just an important\npoint to keep in mind. It's only that\nspatial relationship of where you have data and\nwhere you don't have data",
    "start": "2128510",
    "end": "2135618"
  },
  {
    "text": "that's being represented there.  OK.",
    "start": "2135618",
    "end": "2140640"
  },
  {
    "text": "And so we have some\nnotebooks that we'll see later of how\nyou would actually code this up and\nimplement it, so we'll",
    "start": "2140640",
    "end": "2149240"
  },
  {
    "text": "get to the actual implementation\nside as well of this. ",
    "start": "2149240",
    "end": "2155710"
  },
  {
    "text": "All right, so this is just\nanother visual example here of these are your fit points.",
    "start": "2155710",
    "end": "2162160"
  },
  {
    "text": "Those are the points that\nyou've taken measurements at and you've observed\nthe function,",
    "start": "2162160",
    "end": "2167710"
  },
  {
    "text": "and then you have your\npredicted mean here using the Gaussian\nupdate equations we just",
    "start": "2167710",
    "end": "2173470"
  },
  {
    "text": "saw on the previous slide. And the important\npoint to notice here is that the further that we\nget from our data regions--",
    "start": "2173470",
    "end": "2180523"
  },
  {
    "text": "so where we've actually\nobserved these points, the further we get from that,\nthe larger our uncertainty is.",
    "start": "2180523",
    "end": "2185710"
  },
  {
    "text": "So our uncertainty grows\nthe further away we get from the data. And this is the nice feature\nabout the Gaussian process,",
    "start": "2185710",
    "end": "2191590"
  },
  {
    "text": "is it tells you, hey,\nyou're in a regime over here where you don't\nhave a lot of data,",
    "start": "2191590",
    "end": "2198860"
  },
  {
    "text": "so you shouldn't be too\nconfident in those predictions. So it's that flag of,\nthis is what I think,",
    "start": "2198860",
    "end": "2205280"
  },
  {
    "text": "but you're not very\ncertain about this, so don't go making any\nlife-and-death decisions based",
    "start": "2205280",
    "end": "2210490"
  },
  {
    "text": "on this prediction here. Whereas when you're closer to\nthe points you've seen so far, you have that lower\nuncertainty there.",
    "start": "2210490",
    "end": "2219380"
  },
  {
    "text": "OK. And we also already saw,\nwe saw the visual example, but incorporating noisy\nmeasurements, it's quite easy.",
    "start": "2219380",
    "end": "2226470"
  },
  {
    "text": "So in this case, we have\nperfect function evaluation, so we're observing it perfectly.",
    "start": "2226470",
    "end": "2232000"
  },
  {
    "text": "But in the real world,\nthat's often not the case. So if we want to\nincorporate that uncertainty",
    "start": "2232000",
    "end": "2237420"
  },
  {
    "text": "in our measurements\nthat we have, it's pretty straightforward,\nby introducing-- so this is the actual\nfunction, f of x,",
    "start": "2237420",
    "end": "2243510"
  },
  {
    "text": "and then we're adding some\nrandom noise onto that.",
    "start": "2243510",
    "end": "2248920"
  },
  {
    "text": "And so if we want to include\nthat now in the model that we're working with here,\nit's pretty straightforward.",
    "start": "2248920",
    "end": "2256180"
  },
  {
    "text": "The only change we're making is\nadding in this new term here. That's all that\nwe've done we're.",
    "start": "2256180",
    "end": "2261720"
  },
  {
    "text": "We're just incorporating that\nuncertainty in the points that we've seen so far.",
    "start": "2261720",
    "end": "2268910"
  },
  {
    "text": "And so just like before, we\ncan write the Gaussian update equations here. And the only thing\nthat's changed",
    "start": "2268910",
    "end": "2274790"
  },
  {
    "text": "is just adding that in there. That's the only\nchange that we have.",
    "start": "2274790",
    "end": "2280980"
  },
  {
    "text": "So it's very straightforward\nto account for the noise. And if we want to just look\nat how this changes it,",
    "start": "2280980",
    "end": "2286880"
  },
  {
    "text": "we already saw that\ninitial example, but now, around the data\npoints that we have,",
    "start": "2286880",
    "end": "2292640"
  },
  {
    "text": "we still have some\nuncertainty there. So it's allowing us to\naccount for that in our model.",
    "start": "2292640",
    "end": "2300400"
  },
  {
    "text": "And if we want to\ncompare that to before, basically all you're seeing\nis that the uncertainty is",
    "start": "2300400",
    "end": "2305800"
  },
  {
    "text": "collapsed when we have no noise,\nand when we do have noise, we just-- we're still\naccounting for that as well.",
    "start": "2305800",
    "end": "2311730"
  },
  {
    "text": "Yeah? [INAUDIBLE] choosing\nthis new [INAUDIBLE]?? Yes, yeah. So you would\ncharacterize your sensor,",
    "start": "2311730",
    "end": "2318970"
  },
  {
    "text": "you would know how\nuncertain am I in the values that I'm getting,\nand based on that,",
    "start": "2318970",
    "end": "2324048"
  },
  {
    "text": "you would put that into\nyour model and say, this is the sensor\nuncertainty here.",
    "start": "2324048",
    "end": "2330100"
  },
  {
    "text": "Yeah? These confidence regions\nyou've got put up, is there a convention\non what it is?",
    "start": "2330100",
    "end": "2335500"
  },
  {
    "text": "Like, is it a single variance? Is it a 95% confidence interval? Yeah. What we're showing\nhere I believe",
    "start": "2335500",
    "end": "2341800"
  },
  {
    "text": "is the 95% confidence bound. So it's like whatever z star\nscore that corresponds to,",
    "start": "2341800",
    "end": "2347290"
  },
  {
    "text": "yeah. But yeah, it can vary, too. ",
    "start": "2347290",
    "end": "2354670"
  },
  {
    "text": "OK. So we're working our way through\nprobabilistic surrogate models. The next thing we're\ngoing to look at",
    "start": "2354670",
    "end": "2361140"
  },
  {
    "text": "is actually fitting\nthe Gaussian process. So we've got some\ndata, and we want to fit that data the\nbest that we can.",
    "start": "2361140",
    "end": "2370780"
  },
  {
    "text": "So I already touched on this\nexample with the phone prices, but we'll dive into\nit a little bit more.",
    "start": "2370780",
    "end": "2377369"
  },
  {
    "text": "So we have some data, some\nhistorical data of phone prices",
    "start": "2377370",
    "end": "2382590"
  },
  {
    "text": "to sales that we have here. And we want to-- our boss\ncomes to us and says,",
    "start": "2382590",
    "end": "2390180"
  },
  {
    "text": "I want to look at changing\nthe price of this phone. And there's three\ndifferent prices I'm considering changing\nthe price to and I you",
    "start": "2390180",
    "end": "2397740"
  },
  {
    "text": "to tell me what's our\nforecast for the new sales that we're going to get.",
    "start": "2397740",
    "end": "2404800"
  },
  {
    "text": "So for this example,\nlike let's say just for visualization\npurposes, these are the actual values\nthat we would see.",
    "start": "2404800",
    "end": "2410400"
  },
  {
    "text": "We don't know that, though. All we can see is\nthe blue points. That's all we've\nobserved so far.",
    "start": "2410400",
    "end": "2415510"
  },
  {
    "text": "And so if you hadn't\ntaken this class, maybe you would\njust go to your boss and say, well, I fit\na line to this data,",
    "start": "2415510",
    "end": "2421660"
  },
  {
    "text": "and here is what I\nthink the prediction is. And so maybe you get\nlucky and your boss",
    "start": "2421660",
    "end": "2427540"
  },
  {
    "text": "decides to go with the\nfar price on the left, and you're not that far\noff from your prediction and you get a promotion.",
    "start": "2427540",
    "end": "2434920"
  },
  {
    "text": "But maybe you're\nunlucky and your boss decides to go with the middle\nprice point and you're way off",
    "start": "2434920",
    "end": "2440890"
  },
  {
    "text": "and your boss is not happy\nwith you at that point. So basically, we can often do\nbetter than just the linear fit",
    "start": "2440890",
    "end": "2449250"
  },
  {
    "text": "by looking at the maximum\nlikelihood of our data. So our goal here is we want to\nfind the parameters-- so we're",
    "start": "2449250",
    "end": "2456390"
  },
  {
    "text": "going to parameterize\nour Gaussian process. And so if we want to think about\nwhat is the parameter here?",
    "start": "2456390",
    "end": "2462390"
  },
  {
    "text": "Well, for the squared\nexponential kernel, that's our length scale. But you can even take that\nfurther and you can say,",
    "start": "2462390",
    "end": "2468520"
  },
  {
    "text": "well the kernel function\nitself is the parameter. So maybe you have\ndifferent kernel functions",
    "start": "2468520",
    "end": "2473760"
  },
  {
    "text": "as your parameters. That starts getting\na bit complicated. So for our example, we'll\njust talk about length scale",
    "start": "2473760",
    "end": "2479220"
  },
  {
    "text": "as one of our parameters here. So we want to look at the\nprobability of our data--",
    "start": "2479220",
    "end": "2485970"
  },
  {
    "text": "that's our y-- given\nthe x's that we've seen and the parameters of our model.",
    "start": "2485970",
    "end": "2492460"
  },
  {
    "text": "So what we're actually\ngoing to work with is the log likelihood here.",
    "start": "2492460",
    "end": "2498490"
  },
  {
    "text": "And so for-- with\nour Gaussian, you can remember that we\njust have the expression.",
    "start": "2498490",
    "end": "2504270"
  },
  {
    "text": "It's just this 2 pi to\nthe negative n over 2,",
    "start": "2504270",
    "end": "2511140"
  },
  {
    "text": "and then we have the inverse-- or the negative 1/2-- so the square root of the\ndeterminant of our sigma,",
    "start": "2511140",
    "end": "2518700"
  },
  {
    "text": "and then we have the\nexponent term here. ",
    "start": "2518700",
    "end": "2531250"
  },
  {
    "text": "And so if we want to take\nthe log probability of this, all that is just taking the log.",
    "start": "2531250",
    "end": "2536380"
  },
  {
    "text": "So if we do that, we take\nthe log of this expression, we're just going to end up\nwith a negative n over 2.",
    "start": "2536380",
    "end": "2542010"
  },
  {
    "start": "2542010",
    "end": "2547570"
  },
  {
    "text": "And then similarly, we're\ngoing to have just the log determinant here. And since we're working\nwith logs, remember,",
    "start": "2547570",
    "end": "2554080"
  },
  {
    "text": "we can just add them up. And then the log here\nis going to cancel out, and so we're just left with\nthat term on the inside.",
    "start": "2554080",
    "end": "2561670"
  },
  {
    "start": "2561670",
    "end": "2568059"
  },
  {
    "text": "So pretty straightforward\nfor a Gaussian distribution. We just get something-- some\nnice analytical expression",
    "start": "2568060",
    "end": "2574300"
  },
  {
    "text": "like that. The good news is that if\nwe want to extend this to the Gaussian process, we\nget the exact same thing.",
    "start": "2574300",
    "end": "2581390"
  },
  {
    "text": "The only difference that\nyou see is that we've introduced the k of theta--",
    "start": "2581390",
    "end": "2587359"
  },
  {
    "text": "so that's our\ncovariance matrix now, is just instead of\nsigma, all sigma",
    "start": "2587360",
    "end": "2593440"
  },
  {
    "text": "is just the kernel function\nparameterized by whatever our parameters are-- in\nthis case, our length scale,",
    "start": "2593440",
    "end": "2600910"
  },
  {
    "text": "and then plus that\nuncertainty term. Similarly, the only\ndifference in our mu",
    "start": "2600910",
    "end": "2606730"
  },
  {
    "text": "is that we don't\nwrite it as mu now. We're saying it's m theta of x. So it's just a mean vector\nbased on the data points",
    "start": "2606730",
    "end": "2614140"
  },
  {
    "text": "that we're looking at. So this is great news because we\nhave this analytical expression",
    "start": "2614140",
    "end": "2620200"
  },
  {
    "text": "for this, and so that\nmakes maximum likelihood estimation really easy.",
    "start": "2620200",
    "end": "2625270"
  },
  {
    "text": "So the good news, we\ncan fit the parameters of the Gaussian Process using\nmaximum likelihood estimation.",
    "start": "2625270",
    "end": "2632440"
  },
  {
    "text": "So say in this example, we\ngo and we actually do that. This is just-- these\naren't real values, these are just\nfor visualization.",
    "start": "2632440",
    "end": "2640300"
  },
  {
    "text": "Maybe we go and do that and\nwe get a great-looking fit, and we do really well, and our\nboss is much happier with us",
    "start": "2640300",
    "end": "2648580"
  },
  {
    "text": "than our linear\nregression estimate. And on top of that, we\ncan also tell our boss",
    "start": "2648580",
    "end": "2654640"
  },
  {
    "text": "how confident we are\nin our prediction based on our on-the-modeling\nassumptions that we've made.",
    "start": "2654640",
    "end": "2661859"
  },
  {
    "text": "So that's how we would go about\nfitting the Gaussian process. So just to summarize the\nprobabilistic surrogate models",
    "start": "2661860",
    "end": "2669790"
  },
  {
    "text": "that we've seen so far, just-- all they are is\njust a distribution over functions now.",
    "start": "2669790",
    "end": "2675550"
  },
  {
    "text": "We've just saw that example\nwhere we just scaled it up, they're just distributions\nover functions.",
    "start": "2675550",
    "end": "2681160"
  },
  {
    "text": "And we've seen that the\nchoice of the kernel that you're actually choosing,\nthat has a pretty significant impact on the smoothness\nof the functions",
    "start": "2681160",
    "end": "2688780"
  },
  {
    "text": "and the types of functions\nthat you're getting. We also saw that we can\nincorporate measurement noise.",
    "start": "2688780",
    "end": "2694600"
  },
  {
    "text": "It's relatively\nstraightforward to do that. And then lastly, we can\nfit those parameters using",
    "start": "2694600",
    "end": "2699670"
  },
  {
    "text": "maximum likelihood estimation. Yes? Two questions. First one, can you represent\nother types of noise besides",
    "start": "2699670",
    "end": "2706680"
  },
  {
    "text": "additive using a\nGaussian process? Or is that more\ncomplicated and you can't-- it wouldn't really [INAUDIBLE]?",
    "start": "2706680",
    "end": "2713040"
  },
  {
    "text": "Yeah, that's a good question. ",
    "start": "2713040",
    "end": "2718310"
  },
  {
    "text": "What types of noise are\nyou thinking about there? [INAUDIBLE],, for example,\nwhere you're like noise",
    "start": "2718310",
    "end": "2723770"
  },
  {
    "text": "is dependent on your\nparticular [INAUDIBLE] Yeah. ",
    "start": "2723770",
    "end": "2730340"
  },
  {
    "text": "I think you should be able to. It's more complicated-- It's more-- [INAUDIBLE]",
    "start": "2730340",
    "end": "2736620"
  },
  {
    "text": "Just adding a term\nto your covariance-- Right. Yeah, yeah. You can't-- yeah, you wouldn't\nbe able to just add it in like",
    "start": "2736620",
    "end": "2742162"
  },
  {
    "text": "we did with the new, but it\nseems like there probably is a way to do that. And then if you're\ntrying to, for example,",
    "start": "2742162",
    "end": "2749790"
  },
  {
    "text": "extrapolate your fit rather\nthan trying to interpolate between values, do\nthe kernel functions",
    "start": "2749790",
    "end": "2755040"
  },
  {
    "text": "need to be bounded or can some\nof them be unbounded as you get further and further away\nfrom the points that you've",
    "start": "2755040",
    "end": "2760832"
  },
  {
    "text": "actually [INAUDIBLE]? Yeah. Good question. So in the example\nthat we saw here,",
    "start": "2760832",
    "end": "2767400"
  },
  {
    "text": "looking at the extrapolation--\nso here, because we were just using the squared exponential\nkernel with a 0 mean,",
    "start": "2767400",
    "end": "2774119"
  },
  {
    "text": "you see that decay, it's\njust going back to 0 because we're saying\nour mean function is 0. So as we get further and further\naway, you just decay back.",
    "start": "2774120",
    "end": "2782220"
  },
  {
    "text": "But if you're using a\ndifferent mean function-- different kernel\nfunction, I would",
    "start": "2782220",
    "end": "2788849"
  },
  {
    "text": "say it still has some\nimpact, but the main result, you're seeing here\nis due to the 0 mean.",
    "start": "2788850",
    "end": "2795090"
  },
  {
    "text": "So if you add some different\ntype of mean function, maybe you're looking\nat-- you're saying it's a linear based on\nthe points you've seen,",
    "start": "2795090",
    "end": "2801780"
  },
  {
    "text": "then yeah, you\ncould be unbounded and you could just\nkeep increasing based on that\nrelationship that you saw.",
    "start": "2801780",
    "end": "2808890"
  },
  {
    "text": "Yep. ",
    "start": "2808890",
    "end": "2818690"
  },
  {
    "text": "OK. So we're going to\ntransition now-- so we've seen how to model\nwith the Gaussian process.",
    "start": "2818690",
    "end": "2829550"
  },
  {
    "text": "Now we're going to\ntransition from how to model how to\nsample, and that's what our optimization\nis sort of focused on,",
    "start": "2829550",
    "end": "2836480"
  },
  {
    "text": "is selecting these sample\npoints to get our function evaluations at.",
    "start": "2836480",
    "end": "2841700"
  },
  {
    "text": "So we still want to keep this\nhigh-level roadmap picture in mind where we have a\nfew samples at the start,",
    "start": "2841700",
    "end": "2848300"
  },
  {
    "text": "and then we're choosing\nour sample locations so that we better learn\nwhat the function actually",
    "start": "2848300",
    "end": "2853910"
  },
  {
    "text": "is that we're interested in. And so that's what the surrogate\noptimization is focused on, is how to sample the function.",
    "start": "2853910",
    "end": "2860465"
  },
  {
    "text": " So the first one that\nwe're going to look at",
    "start": "2860465",
    "end": "2866539"
  },
  {
    "text": "is prediction-based exploration. And so if you think back\nto quadratic fit search,",
    "start": "2866540",
    "end": "2872600"
  },
  {
    "text": "that's essentially the same\nexact thing as prediction based exploration. So as a refresher, in\nquadratic fit search,",
    "start": "2872600",
    "end": "2880279"
  },
  {
    "text": "you're just looking\nat the three-- the last three bracketing\npoints and you're fitting a quadratic\nfunction to those points.",
    "start": "2880280",
    "end": "2889349"
  },
  {
    "text": "So that's like your\nsurrogate model. You're just saying, OK, we have\na quadratic surrogate model, and then we're going\nto take the minimum",
    "start": "2889350",
    "end": "2895670"
  },
  {
    "text": "of that quadratic function\nand we're going to sample wherever that minimum is.",
    "start": "2895670",
    "end": "2901309"
  },
  {
    "text": "And that's all prediction-based\nexploration is doing. The only difference\nnow is you're not using a quadratic\nfunction, you're",
    "start": "2901310",
    "end": "2907610"
  },
  {
    "text": "using the mean, the predicted\nmean of your Gaussian process. So it's the same exact thing.",
    "start": "2907610",
    "end": "2913400"
  },
  {
    "text": "It's just-- your model is no\nlonger the quadratic function.",
    "start": "2913400",
    "end": "2918480"
  },
  {
    "text": "So if we want to look\nat this in detail, we're taking the sample at the\nminimum of the predicted mean.",
    "start": "2918480",
    "end": "2928790"
  },
  {
    "text": "So we can actually look at this\nthrough a real example here. ",
    "start": "2928790",
    "end": "2935550"
  },
  {
    "text": "So to break this down,\nthe black function, just like we've been\nlooking at, this black curve here is the true function.",
    "start": "2935550",
    "end": "2942030"
  },
  {
    "text": "The data that we've seen\nso far is this point here, and there's another point here. The red point is the\npoint that we're actually",
    "start": "2942030",
    "end": "2948690"
  },
  {
    "text": "going to take the sample at. So there's a black point--\nit might be hard to see, but there's a Black point\nunderneath that red point.",
    "start": "2948690",
    "end": "2954329"
  },
  {
    "text": "The blue is the predicted mean\nfrom our Gaussian process. So we look at where is the\nminimum of this predicted mean?",
    "start": "2954330",
    "end": "2962190"
  },
  {
    "text": "And in this case, the\nminimum is right about here. So then we say, OK, we're\ngoing to sample at that point",
    "start": "2962190",
    "end": "2967529"
  },
  {
    "text": "just like we did in\nquadratic fit search. So we go and we sample there.",
    "start": "2967530",
    "end": "2973690"
  },
  {
    "text": "So we take the sample. We've now added it. There's two samples here. It kind of looks like\none, but there's two.",
    "start": "2973690",
    "end": "2979800"
  },
  {
    "text": "We update based on the\nGaussian update equation. So we update our predicted mean.",
    "start": "2979800",
    "end": "2985260"
  },
  {
    "text": "We see that the new\npredicted minimum is now right about here. So then we say, OK, we're\ngoing to sample there",
    "start": "2985260",
    "end": "2991290"
  },
  {
    "text": "at that red point.  So we go and sample at that\npoint, and then we repeat.",
    "start": "2991290",
    "end": "2998130"
  },
  {
    "text": "And we update our\nGaussian process, we get a new predicted\nmean, and then we select another\nsample location.",
    "start": "2998130",
    "end": "3006770"
  },
  {
    "text": "And then it gets us pretty\nclose to the minimum. So we can say, OK,\ngreat, we're done. Prediction-based\nexploration, close the book,",
    "start": "3006770",
    "end": "3013220"
  },
  {
    "text": "we're good to go. The only issue with that\nis that obviously this was a very nice example\nto start out with.",
    "start": "3013220",
    "end": "3022470"
  },
  {
    "text": "So you can imagine-- and it's not very\nhard to come up with a counterexample of\nwhere this wouldn't work out",
    "start": "3022470",
    "end": "3029780"
  },
  {
    "text": "too well. So if instead our function had\nlooked something like this--",
    "start": "3029780",
    "end": "3035900"
  },
  {
    "text": " and so we had started over\nhere with these two points",
    "start": "3035900",
    "end": "3043720"
  },
  {
    "text": "and we had said that our\npredicted mean looked something",
    "start": "3043720",
    "end": "3048820"
  },
  {
    "text": "like this, something like that.",
    "start": "3048820",
    "end": "3054060"
  },
  {
    "text": "And then we went\nthrough our sampling, it would take us to\nthis local minimum. But then we've totally missed\nout on this one over here",
    "start": "3054060",
    "end": "3060619"
  },
  {
    "text": "since we were just focused on\nwhat was going on over here, and we never even considered\nthis part over here.",
    "start": "3060620",
    "end": "3067079"
  },
  {
    "text": "So that's the issue with\nprediction-based exploration, is that often we just get\nstuck in these local minima",
    "start": "3067080",
    "end": "3074450"
  },
  {
    "text": "and it's not really doing\nany exploring of the space.",
    "start": "3074450",
    "end": "3079460"
  },
  {
    "text": "The other thing\nto keep in mind is that it's not taking\nuncertainty into account at all. So we just went through all\nthis work of making sure",
    "start": "3079460",
    "end": "3087819"
  },
  {
    "text": "that our probabilistic\nsurrogate model is representing the uncertainty in our\ndata, and now we're not even we're not even using\nthat, so it's kind of a waste.",
    "start": "3087820",
    "end": "3096067"
  },
  {
    "text": "Like, why did we even do\nthat in the first place? It does take it into\naccount a little bit when we're actually doing\nthe mean prediction.",
    "start": "3096068",
    "end": "3102530"
  },
  {
    "text": "So it's there, but\nwe're not really using that in our exploration. And then the other point is\nthat the new samples are often",
    "start": "3102530",
    "end": "3110650"
  },
  {
    "text": "very close to the\nexisting samples, so it's sort of a waste\nof sampling there.",
    "start": "3110650",
    "end": "3116990"
  },
  {
    "text": "All right, so if we want to do\na little better and maybe now we say, OK, well, we didn't\ndo any exploration at all,",
    "start": "3116990",
    "end": "3123740"
  },
  {
    "text": "so let's do some\nexploration here. So error-based is\nlooking at where",
    "start": "3123740",
    "end": "3130190"
  },
  {
    "text": "the standard deviation-- the\npredicted standard deviation is the greatest. So we're choosing our samples\nto maximize that predicted",
    "start": "3130190",
    "end": "3139670"
  },
  {
    "text": "standard deviation. And so you can think about\nthis, because the larger",
    "start": "3139670",
    "end": "3145790"
  },
  {
    "text": "the standard deviation\nis, the more uncertain we are in the actual\ntrue function there.",
    "start": "3145790",
    "end": "3152003"
  },
  {
    "text": "So we're sort of saying\nwe want to become more certain in our function,\nand that's sort of our guide",
    "start": "3152003",
    "end": "3157280"
  },
  {
    "text": "here for this exploration. So if we look at, again,\nsame exact example here,",
    "start": "3157280",
    "end": "3165390"
  },
  {
    "text": "we're going to start\nout, and now we're also visualizing the\nuncertainty here. So we start out, and instead\nwhere prediction base was just",
    "start": "3165390",
    "end": "3173280"
  },
  {
    "text": "taking a sample here,\nthis goes directly to the bounds of the domain.",
    "start": "3173280",
    "end": "3178780"
  },
  {
    "text": "And so because our uncertainty\nis greatest at the bounds, we just clamp those\ndown first right away.",
    "start": "3178780",
    "end": "3183990"
  },
  {
    "text": "So we sample at the\nleft bound, then we sample at the right bound. And then we get into it of\nactually going through--",
    "start": "3183990",
    "end": "3192460"
  },
  {
    "text": "and so the next\nhighest uncertainty is on that left peak\nthere, so we sample there.",
    "start": "3192460",
    "end": "3199080"
  },
  {
    "text": "And now we're just like\nplaying Whac-A-Mole with the uncertainty. So we clamp one down and\nthen another one pops up,",
    "start": "3199080",
    "end": "3205900"
  },
  {
    "text": "so we go and we whack that mole. And we're just\nsort of continuing to play whack-a-mole here\nwith error-based exploration.",
    "start": "3205900",
    "end": "3212702"
  },
  {
    "text": "And so we can just\nkeep going through it, wherever the uncertainty\nis the highest, sample at those locations.",
    "start": "3212702",
    "end": "3219010"
  },
  {
    "text": "And we end up-- we end up with\na pretty good understanding of what our function looks like.",
    "start": "3219010",
    "end": "3225390"
  },
  {
    "text": "And we've distributed\nour samples pretty well throughout the sampling process.",
    "start": "3225390",
    "end": "3231119"
  },
  {
    "text": "The only issue here now is that\noften, the functions that we're",
    "start": "3231120",
    "end": "3236340"
  },
  {
    "text": "interested in are defined over\nvery large domains, sometimes just all of Rn.",
    "start": "3236340",
    "end": "3241830"
  },
  {
    "text": "And so their\nunbounded functions. And in the case where\nthey are unbounded,",
    "start": "3241830",
    "end": "3246900"
  },
  {
    "text": "you saw what error-based\nexploration wanted to do, it goes straight to the bounds. In this case, we\nhave bounded it.",
    "start": "3246900",
    "end": "3252480"
  },
  {
    "text": "So we put the domains as only--\nit can only go between those. So if you're working\nwith problems",
    "start": "3252480",
    "end": "3257640"
  },
  {
    "text": "where the bounds\npretty well, like, say, for the phone\nprice example, you know that your\nphone-- the sales",
    "start": "3257640",
    "end": "3265410"
  },
  {
    "text": "corresponding to the price, the\nprice has to be greater than 0, and you're not going to make it\n1 million dollars for a phone.",
    "start": "3265410",
    "end": "3271960"
  },
  {
    "text": "So that one, we'd say,\nis pretty well-bounded. But in a lot of\nother applications you don't have\nthose bounds on it,",
    "start": "3271960",
    "end": "3277960"
  },
  {
    "text": "and that's where you run into\nsome issues with error-based.",
    "start": "3277960",
    "end": "3283530"
  },
  {
    "text": "So it has to be constrained\nto that closed region. Yes? Suppose your problem\nis high-dimensional,",
    "start": "3283530",
    "end": "3290580"
  },
  {
    "text": "wouldn't they try to go to all\nthe corners first, I guess? Yeah.",
    "start": "3290580",
    "end": "3295680"
  },
  {
    "text": "Yeah. So if you took it like\ninto many higher dimensions where you're looking at-- you\nhave some hypercube that you've",
    "start": "3295680",
    "end": "3302039"
  },
  {
    "text": "bounded it to,\nyou're going to go to because the error is going to\nbe greatest unless you already have samples at those bounds.",
    "start": "3302040",
    "end": "3308022"
  },
  {
    "text": "So if you don't\nhave samples there, that's where you're going\nto go and sample at those bounds first.",
    "start": "3308022",
    "end": "3314760"
  },
  {
    "text": "Yes? What is the relation between\nsigma hat and the kernel?",
    "start": "3314760",
    "end": "3321089"
  },
  {
    "text": "Sigma hat and the kernel. Yes, that's a great-- great question. So you can back out sigma hat--\nso from your covariance matrix,",
    "start": "3321090",
    "end": "3332460"
  },
  {
    "text": "if you look at the\ndiagonal terms, those are essentially\nthe variance at each",
    "start": "3332460",
    "end": "3340140"
  },
  {
    "text": "of your points. So how much-- so you can\nthink about the diagonal here is basically what\nwe're showing here.",
    "start": "3340140",
    "end": "3347010"
  },
  {
    "text": "The only caveat is that\nthere's a square root in there because we're going from\nvariance to standard deviation.",
    "start": "3347010",
    "end": "3353550"
  },
  {
    "text": "But yeah, it's\njust the diagonals. ",
    "start": "3353550",
    "end": "3361430"
  },
  {
    "text": "So we've seen error-based,\nwe've seen prediction-based, and we've seen some of the\ndrawbacks to both of them.",
    "start": "3361430",
    "end": "3368770"
  },
  {
    "text": "Lower confidence bound is\nessentially just taking a combination of both. So if you recall\nwhat we just saw,",
    "start": "3368770",
    "end": "3376390"
  },
  {
    "text": "in prediction-based,\nwe're trying to minimize the predicted\nmean, and in error-based, we're trying to sample at the maximum\nof the standard deviation.",
    "start": "3376390",
    "end": "3385640"
  },
  {
    "text": "So now lower confidence bound is\njust combining those together, and it does that\ncombination just",
    "start": "3385640",
    "end": "3391210"
  },
  {
    "text": "through this parameter alpha. So alpha is sort of\ncontrolling the weight that you're giving to\nprediction-based versus",
    "start": "3391210",
    "end": "3397000"
  },
  {
    "text": "error-based. And the minus sign here\nis just keep that in mind because in error-based,\nwe were trying",
    "start": "3397000",
    "end": "3402790"
  },
  {
    "text": "to take the maximum of\nthe standard deviation. Here, we're trying to minimize\nthis lower confidence bound, so that's why we\nget that minus sign.",
    "start": "3402790",
    "end": "3412190"
  },
  {
    "text": "And so this is just really\nsort of focus on the-- it's a balance of the\nexploitation/exploration",
    "start": "3412190",
    "end": "3417380"
  },
  {
    "text": "trade-off where here,\nprediction-based is trying to exploit the\nsamples that we've already seen.",
    "start": "3417380",
    "end": "3423270"
  },
  {
    "text": "So we already have samples\nin specific regions. Prediction-based is just\nsaying, hey, I already know what's going on\nhere, I can probably",
    "start": "3423270",
    "end": "3429860"
  },
  {
    "text": "do better if I\nsample closer to that and exploit the knowledge\nthat already have. Whereas the exploration side\nis the error-based where",
    "start": "3429860",
    "end": "3437743"
  },
  {
    "text": "we're saying, hey,\nwe really don't know what's going on over\nthere, let's go explore, let's go find that out.",
    "start": "3437743",
    "end": "3443100"
  },
  {
    "text": "And so you have that tug of\nwar going on between the two and your alpha is\nthe parameter that's",
    "start": "3443100",
    "end": "3449410"
  },
  {
    "text": "controlling that, how\nmuch priority you're given to one versus the other. So when alpha-- if\nyou set alpha to 0,",
    "start": "3449410",
    "end": "3455007"
  },
  {
    "text": "then you're just going to be\ndoing all prediction-based, and if you set\nalpha to infinity, then you're going to be\ndoing all error-based.",
    "start": "3455007",
    "end": "3461450"
  },
  {
    "text": "Alpha somewhere in\nthe middle, you're going to be a balance\nbetween the two. And so alpha, that's\nanother parameter",
    "start": "3461450",
    "end": "3467089"
  },
  {
    "text": "that you would set for\nyour specific application. ",
    "start": "3467090",
    "end": "3474200"
  },
  {
    "text": "So we can look at how\nthis actually performs. So we see same\nexact example again,",
    "start": "3474200",
    "end": "3482109"
  },
  {
    "text": "and just keep in mind\nthat we're trying to minimize the lower bound. So we're trying to sample\nwhere this red line is low.",
    "start": "3482110",
    "end": "3489490"
  },
  {
    "text": "So lower values are good. And the other thing\nwe want to see is that you can see\nthe balance here.",
    "start": "3489490",
    "end": "3497140"
  },
  {
    "text": "When the predicted mean is high\nbut we have large uncertainty, we still get a good value\nfor the lower bound.",
    "start": "3497140",
    "end": "3503950"
  },
  {
    "text": "Similarly, when we have\nthese lower values here with no uncertainty, we're\nsaying that basically, it's",
    "start": "3503950",
    "end": "3511030"
  },
  {
    "text": "only the predicted mean there. So we're just\ngoing to go through",
    "start": "3511030",
    "end": "3516640"
  },
  {
    "text": "and sample wherever that\nred line, wherever our lower confidence bound is lowest.",
    "start": "3516640",
    "end": "3522280"
  },
  {
    "text": "So we start off here. And I would say, this\nis an exploration 1,",
    "start": "3522280",
    "end": "3528220"
  },
  {
    "text": "but it's not it's not\nexactly one is exploration, one is exploitation. But then we go here\nand this is definitely",
    "start": "3528220",
    "end": "3533440"
  },
  {
    "text": "more of exploiting\nwhat we already know because we're\npredicting the mean to be pretty low there.",
    "start": "3533440",
    "end": "3538930"
  },
  {
    "text": "And then we continue on,\nand the next one we get-- I would say this is definitely\nmore of an exploration term.",
    "start": "3538930",
    "end": "3545559"
  },
  {
    "text": "So we're balancing that\nhere through the alpha that we've chosen. And we can continue on.",
    "start": "3545560",
    "end": "3553640"
  },
  {
    "text": "And again, we end up\nwith something that looks pretty reasonable here.",
    "start": "3553640",
    "end": "3559010"
  },
  {
    "text": "And by the end, we get we\nget a reasonable distribution of samples, and also a\npretty good characterization",
    "start": "3559010",
    "end": "3565180"
  },
  {
    "text": "of our actual function.  So now the only\nissue is that, well,",
    "start": "3565180",
    "end": "3571950"
  },
  {
    "text": "you still have the bounds issue\nbecause your uncertainty is still going to grow as you go\nto the bounds of the domain.",
    "start": "3571950",
    "end": "3579100"
  },
  {
    "text": "So you still have that, but you\nhave a little bit more control over that because if you choose\nan alpha such that you're not",
    "start": "3579100",
    "end": "3586980"
  },
  {
    "text": "really caring as much about\nyour standard deviation, you won't necessarily always\nbe going to those bounds.",
    "start": "3586980",
    "end": "3593310"
  },
  {
    "text": "So it takes care of the\nproblem, but it's still there. But it's just trying to\ntake the best of both",
    "start": "3593310",
    "end": "3599730"
  },
  {
    "text": "of prediction-based\nand error-based and combine them together. ",
    "start": "3599730",
    "end": "3607579"
  },
  {
    "text": "OK. So now we're going to look at\nprobability of improvement.",
    "start": "3607580",
    "end": "3613370"
  },
  {
    "text": "And I think this one makes a\nlot of intuitive sense for why we would want to do it.",
    "start": "3613370",
    "end": "3620300"
  },
  {
    "text": "And so let's get just sort\nof a picture started here to work with. ",
    "start": "3620300",
    "end": "3631010"
  },
  {
    "text": "So let's say that our\ntrue function looks something like this. ",
    "start": "3631010",
    "end": "3638490"
  },
  {
    "text": "And maybe we've sampled\nhere, we've sampled here, and maybe we've\nsampled here so far.",
    "start": "3638490",
    "end": "3647870"
  },
  {
    "text": "And then we've updated\nour Gaussian process, we've gotten some\nnew predicted mean,",
    "start": "3647870",
    "end": "3653510"
  },
  {
    "text": "and maybe our mean looks\nsomething like this. ",
    "start": "3653510",
    "end": "3661140"
  },
  {
    "text": "So now, the probability\nof improvement is asking, at any\npoint along here,",
    "start": "3661140",
    "end": "3667230"
  },
  {
    "text": "what's the probability\nthat I'm going to do better than what I've seen so far? So the best that\nwe've seen so far--",
    "start": "3667230",
    "end": "3673591"
  },
  {
    "text": "remember, we're\ntrying to minimize. So the best we've seen so\nfar is this point right here. This is our y min.",
    "start": "3673592",
    "end": "3678829"
  },
  {
    "text": " So we're asking,\neverywhere along here, what's the probability that\nI'm going to be below y min?",
    "start": "3678830",
    "end": "3686900"
  },
  {
    "text": "So say, for example,\nwe actually want to look at one specific point. So let's say, what's the\nprobability of improvement",
    "start": "3686900",
    "end": "3694490"
  },
  {
    "text": "right here at this x-value? So we're going to come over\nhere and we're going to look--",
    "start": "3694490",
    "end": "3699920"
  },
  {
    "text": "we're going to say,\nOK, at this point-- so at this specific x,\nwe're going to look at,",
    "start": "3699920",
    "end": "3705740"
  },
  {
    "text": "what is our predicted mean from\nour Gaussian process right now? So that's right here.",
    "start": "3705740",
    "end": "3711349"
  },
  {
    "text": "We're saying we\npredict it right there. And remember, we\nhave a predicted mean and a predicted variance\nor standard deviation.",
    "start": "3711350",
    "end": "3717930"
  },
  {
    "text": "So with that, we can\nget a distribution. So if we just draw that\ndistribution on here,",
    "start": "3717930",
    "end": "3723500"
  },
  {
    "text": "it's going to look\nsomething like this. ",
    "start": "3723500",
    "end": "3731180"
  },
  {
    "text": "So we've just dropped\nthe bell curve down there based on whatever variance we\ncurrently have at that point.",
    "start": "3731180",
    "end": "3738530"
  },
  {
    "text": "So we have our mean, we\nhave our standard deviation. We want to know, what's\nthe probability that we're going to do better than this?",
    "start": "3738530",
    "end": "3745319"
  },
  {
    "text": "So to see that, we're just going\nto draw a cross from here-- so we want to do better\nthan this y min value.",
    "start": "3745320",
    "end": "3752532"
  },
  {
    "text": "And now I think it's\npretty clear to see, well, the probability that we're going\nto do better than that y min,",
    "start": "3752532",
    "end": "3757610"
  },
  {
    "text": "it's just this tail of\nthe distribution here. So that's our probability\nof improvement.",
    "start": "3757610",
    "end": "3764360"
  },
  {
    "text": "If we integrate this tail from\nnegative infinity up to this y min value, that\nis the probability",
    "start": "3764360",
    "end": "3770990"
  },
  {
    "text": "that we're going\nto be doing better than what we've seen so far. So any questions there?",
    "start": "3770990",
    "end": "3776400"
  },
  {
    "text": "We'll write out the actual\nmath for this in a second, but any questions on\nthe intuition side?",
    "start": "3776400",
    "end": "3783684"
  },
  {
    "text": "OK. ",
    "start": "3783685",
    "end": "3793250"
  },
  {
    "text": "So what actually\nis the improvement? We can say that\nour improvement ,",
    "start": "3793250",
    "end": "3801270"
  },
  {
    "text": "it's just however much better\nwe're doing than that y min that we've seen.",
    "start": "3801270",
    "end": "3806400"
  },
  {
    "text": "So it's just going to be\nour y min minus whatever y we actually see.",
    "start": "3806400",
    "end": "3811800"
  },
  {
    "text": "So if we're below it, then\nwe're going to be improving. If we're not below it,\nwe're just going to have 0.",
    "start": "3811800",
    "end": "3817010"
  },
  {
    "text": "So this is only if y is\nless than or equal to y min.",
    "start": "3817010",
    "end": "3824030"
  },
  {
    "text": "So that's our\nimprovement function. Now we're interested in\nlooking at the probability",
    "start": "3824030",
    "end": "3831880"
  },
  {
    "text": "of improvement. So we want to know, what's the\nprobability that we're doing better than this right now?",
    "start": "3831880",
    "end": "3837115"
  },
  {
    "text": " And so based on our\npicture that we've drawn,",
    "start": "3837115",
    "end": "3843180"
  },
  {
    "text": "we can say the\nprobability that y is less than or equal\nto y min, that's",
    "start": "3843180",
    "end": "3851370"
  },
  {
    "text": "just-- we're just integrating\nup from negative infinity all the way up. So we know that this is just\nthe cumulative distribution",
    "start": "3851370",
    "end": "3858840"
  },
  {
    "text": "function here. So we go from negative\ninfinity to y min, and it's just based on that\nGaussian that we have with",
    "start": "3858840",
    "end": "3867420"
  },
  {
    "text": "our-- whatever our mu hat and\nwhatever our sigma hat is, and we're just--",
    "start": "3867420",
    "end": "3872670"
  },
  {
    "text": "this is a y. We're just integrating that up. So as I said, this is just\nthe cumulative distribution",
    "start": "3872670",
    "end": "3880290"
  },
  {
    "text": "function, which we\ncan write like this. ",
    "start": "3880290",
    "end": "3888280"
  },
  {
    "text": "And so this basically\njust converts it to a standard normal through\nthat conversion there,",
    "start": "3888280",
    "end": "3894359"
  },
  {
    "text": "and we're just looking at\nintegrating up that tail. Yeah? What is the\n[INAUDIBLE] equation?",
    "start": "3894360",
    "end": "3900700"
  },
  {
    "text": "Oh. Sorry, yeah, that's-- y\nmin is the upper bound. Yeah.",
    "start": "3900700",
    "end": "3905850"
  },
  {
    "text": "So the upper bound is\njust this y min here. ",
    "start": "3905850",
    "end": "3914330"
  },
  {
    "text": "All right. So yeah, same picture\nthat's up here. And if we want to look at\nhow this actually performs--",
    "start": "3914330",
    "end": "3921660"
  },
  {
    "text": "so yeah, I just have\nwritten we want to-- so we want to sample\nnow at the points where this probability of\nimprovement is the highest.",
    "start": "3921660",
    "end": "3929970"
  },
  {
    "start": "3929970",
    "end": "3935750"
  },
  {
    "text": "So if we start off\nhere, you can see that when we already\nhave a sample,",
    "start": "3935750",
    "end": "3942099"
  },
  {
    "text": "our sigma hat there\nis going to be 0. So this would be-- we would have a divide by 0\nhere, which would be undefined.",
    "start": "3942100",
    "end": "3949070"
  },
  {
    "text": "But we can just\nsay, well, we know we're not going to improve if\nwe sample there again because we already know what the\nfunction value is,",
    "start": "3949070",
    "end": "3955640"
  },
  {
    "text": "so the improvement\nthere is just 0. So that's why we have\nthese spikes here where it just drops down.",
    "start": "3955640",
    "end": "3961480"
  },
  {
    "text": "But then what you see is\nright around those points that we already have, that's\nwhere the highest probability",
    "start": "3961480",
    "end": "3967869"
  },
  {
    "text": "of improvement is, is right very\nclose to where we've already sampled. So we go ahead and we sample\nat where the probability",
    "start": "3967870",
    "end": "3974500"
  },
  {
    "text": "of improvement is high. Now we have two samples there. We've also-- we have technically\nimproved our function.",
    "start": "3974500",
    "end": "3980410"
  },
  {
    "text": "We got a lower value. And so now our\nprobability of improvement is really high\nbecause now we have",
    "start": "3980410",
    "end": "3986920"
  },
  {
    "text": "very little uncertainty there. We have that predicted\nmean is decreasing. So we're saying, let's\nkeep sampling there.",
    "start": "3986920",
    "end": "3993434"
  },
  {
    "text": "We've seen that\nwe're doing good, so we're going to keep\non sampling there, and we just keep going\ndown along that route.",
    "start": "3993435",
    "end": "4003390"
  },
  {
    "text": "So I think this is a\nlittle disappointing to see because it's\na really nice setup.",
    "start": "4003390",
    "end": "4008530"
  },
  {
    "text": "It makes sense you're\nlooking at the tail, you're talking about what's\nthe probability we're going to do better than this.",
    "start": "4008530",
    "end": "4013619"
  },
  {
    "text": "But then in practice,\nwhat we actually see is that often we end up very\nsimilar to prediction-based",
    "start": "4013620",
    "end": "4018830"
  },
  {
    "text": "where we're sampling\nvery close to the samples that we've already seen. So that's a bummer. ",
    "start": "4018830",
    "end": "4027609"
  },
  {
    "text": "And just like we said, it\ntends to actually do the job. It is decreasing the\nobjective, but we're doing it,",
    "start": "4027610",
    "end": "4034540"
  },
  {
    "text": "one, in that just\nlocal area, so we could be susceptible to\nthe local minima; and two,",
    "start": "4034540",
    "end": "4039970"
  },
  {
    "text": "it's wasting our samples again. ",
    "start": "4039970",
    "end": "4045840"
  },
  {
    "text": "So I think the natural\nextension of this would be not just\nlooking at what's",
    "start": "4045840",
    "end": "4051740"
  },
  {
    "text": "the probability of improvement,\nbut what do we actually expect that improvement to be?",
    "start": "4051740",
    "end": "4056750"
  },
  {
    "text": "Because probability\nof improvement was just saying, wherever\nthe probability is highest, let's go there.",
    "start": "4056750",
    "end": "4062420"
  },
  {
    "text": "Expected improvement is now\nsaying, let's look at not just the probability that\nwe're going to improve,",
    "start": "4062420",
    "end": "4067580"
  },
  {
    "text": "but how much do\nwe actually think we're going to improve by? So if we want to\nlook at this now,",
    "start": "4067580",
    "end": "4078960"
  },
  {
    "text": "we're going to just start\noff by doing a very just simple change of variables. And it'll be clear why we're\ndoing this in a second,",
    "start": "4078960",
    "end": "4086520"
  },
  {
    "text": "but we're going to\nintroduce this z-variable and we're going to say\nthat that z is just equal to y minus mu\nhat over our sigma hat,",
    "start": "4086520",
    "end": "4096210"
  },
  {
    "text": "and we're going to\nsay y min prime here is equal to our y min minus\nour mu hat over sigma hat.",
    "start": "4096210",
    "end": "4106259"
  },
  {
    "text": "So we're just converting\nit to standard normal. That's why we're doing\nthis change of variables,",
    "start": "4106260",
    "end": "4111660"
  },
  {
    "text": "and you'll see, it'll make\nsome stuff work out nicer here. So we do this\nchange of variables,",
    "start": "4111660",
    "end": "4118390"
  },
  {
    "text": "and now we want to rewrite our\nimprovement function that I think I've covered up here. So our improvement function now\nwith this change of variables",
    "start": "4118390",
    "end": "4129210"
  },
  {
    "text": "is just going to be sigma hat\ntimes y min prime minus z,",
    "start": "4129210",
    "end": "4136290"
  },
  {
    "text": "and this is if our-- this is if z is less than\nor equal to our y min prime,",
    "start": "4136290",
    "end": "4145589"
  },
  {
    "text": "and it's 0 otherwise. So all we've done\nhere is we just took what we had before\nwas y min minus y.",
    "start": "4145590",
    "end": "4152160"
  },
  {
    "text": "We've now just subbed in\nour change of variables here to get us this new\nimprovement function.",
    "start": "4152160",
    "end": "4158580"
  },
  {
    "text": "So now we're interested\nin the expected value of this improvement function.",
    "start": "4158580",
    "end": "4164049"
  },
  {
    "text": "So if we look at\nthe expected value,",
    "start": "4164050",
    "end": "4172079"
  },
  {
    "text": "if we take the\nexpectation-- remember, it's just the-- we're\njust integrating over the probability\ndistribution",
    "start": "4172080",
    "end": "4178170"
  },
  {
    "text": "where we take the expected\nvalue of the function times the probability. That's all expected value is.",
    "start": "4178170",
    "end": "4183689"
  },
  {
    "text": "It's just value times the\nprobability of that value. So the function value here\nis just this improvement.",
    "start": "4183689",
    "end": "4191410"
  },
  {
    "text": "So it's just this term. We don't care about\nwhen it's 0 because we know that it'll just be 0. So we're going to have\nsigma hat on the outside",
    "start": "4191410",
    "end": "4197670"
  },
  {
    "text": "here because it's constant. And our bounds of\nintegration, again, are from-- same as before, negative\ninfinity up to y min.",
    "start": "4197670",
    "end": "4206805"
  },
  {
    "text": "But here, we've introduced\nthe change of variables, so it's y min prime now. ",
    "start": "4206805",
    "end": "4212738"
  },
  {
    "text": "And then we're just\ngoing to bring this in. So y min prime minus our z.",
    "start": "4212738",
    "end": "4219389"
  },
  {
    "text": "So that's our function value. And then our function-- or the probability\nof this is just--",
    "start": "4219390",
    "end": "4226950"
  },
  {
    "text": "now we see, it's\njust the standard normal because we made\nthat change of variables. So that's why we did it.",
    "start": "4226950",
    "end": "4232750"
  },
  {
    "text": "So this is just the expect-- this is all it is, the\nexpected improvement here that we've written.",
    "start": "4232750",
    "end": "4239760"
  },
  {
    "text": "And so there's not much that\nwe can do with it in this form because we have\nthis integral here.",
    "start": "4239760",
    "end": "4244860"
  },
  {
    "text": "So we want to simplify\nthis a little bit. So to simplify this, we're\njust going to break up",
    "start": "4244860",
    "end": "4253440"
  },
  {
    "text": "break up that integral. So the first term,\nwe're going to have-- our sigma hat is going\nto be on the outside now,",
    "start": "4253440",
    "end": "4260670"
  },
  {
    "text": "and we're just going to\nwrite that first term. So our y min prime doesn't\ndepend on our z here,",
    "start": "4260670",
    "end": "4267180"
  },
  {
    "text": "so we can bring that\nout of the integration.  And we're just going\nfrom negative infinity",
    "start": "4267180",
    "end": "4273780"
  },
  {
    "text": "to y min prime. And it's just going to be\nour distribution for z here.",
    "start": "4273780",
    "end": "4281760"
  },
  {
    "text": " So that term, that's\ngood because we",
    "start": "4281760",
    "end": "4287280"
  },
  {
    "text": "know this is exactly\nwhat we saw before. This is just the probability\nof improvement for z.",
    "start": "4287280",
    "end": "4292530"
  },
  {
    "text": "So that term is almost\nalready taken care of. We'll simplify it\nin the next step, but this is-- we're\nlooking good here.",
    "start": "4292530",
    "end": "4299160"
  },
  {
    "text": "Then we're just going to\nbring in this other term. So we're going to\nhave the minus here,",
    "start": "4299160",
    "end": "4304409"
  },
  {
    "text": "and we're going to go negative\ninfinity, y min prime again of z, and then the\nstandard normal here.",
    "start": "4304410",
    "end": "4313695"
  },
  {
    "text": " So we know that this is\njust the expected value of z",
    "start": "4313695",
    "end": "4321000"
  },
  {
    "text": "because we have value\ntimes probability. So that's our\nexpected value of z.",
    "start": "4321000",
    "end": "4326310"
  },
  {
    "text": "And we know because it's\nthe standard normal. If we were going from negative\ninfinity to positive infinity,",
    "start": "4326310",
    "end": "4331500"
  },
  {
    "text": "it would just be 0\nbecause it's just the expected value of a\nstandard normal Gaussian,",
    "start": "4331500",
    "end": "4336690"
  },
  {
    "text": "and that's 0 because\nthat's our mean. In this case, we're not\ngoing from negative infinity",
    "start": "4336690",
    "end": "4342420"
  },
  {
    "text": "to positive infinity, we're\ngoing from negative infinity to y min prime.",
    "start": "4342420",
    "end": "4347610"
  },
  {
    "text": "And so what this\nends up actually working out to is we have this\nnice result for Gaussians where",
    "start": "4347610",
    "end": "4354750"
  },
  {
    "text": "it comes out to being-- this term would just be our\ny min prime given our mean",
    "start": "4354750",
    "end": "4363330"
  },
  {
    "text": "and standard deviation. ",
    "start": "4363330",
    "end": "4368469"
  },
  {
    "text": "And then it's going to\nbe minus the other bounds of integration, which\nis at negative infinity.",
    "start": "4368470",
    "end": "4375010"
  },
  {
    "text": " Where we know here, the\nlikelihood at negative infinity",
    "start": "4375010",
    "end": "4381940"
  },
  {
    "text": "is just 0 because\nthere's no tail there",
    "start": "4381940",
    "end": "4387130"
  },
  {
    "text": "as we go to infinity, so we\ncan just get rid of this. That's just 0. And if you're curious on\nhow we got from this step",
    "start": "4387130",
    "end": "4394030"
  },
  {
    "text": "to this step, if you\nactually want to work it out you'd have to introduce\nthe actual expression here",
    "start": "4394030",
    "end": "4399040"
  },
  {
    "text": "for the Gaussian,\nand then it's just a couple of lines to\nget from there to here. But we know this\nnice result that we",
    "start": "4399040",
    "end": "4405070"
  },
  {
    "text": "can work with for\nGaussians where we can just take the tail here of the bounds\nof integration and sub it in.",
    "start": "4405070",
    "end": "4411230"
  },
  {
    "text": "So that's all I've done\nfor those two steps. So now we're ready\nto simplify this.",
    "start": "4411230",
    "end": "4416470"
  },
  {
    "text": "We just have this\nterm and this term. So if we-- we'll simplify it and\nget our final expression here.",
    "start": "4416470",
    "end": "4422185"
  },
  {
    "start": "4422185",
    "end": "4429880"
  },
  {
    "text": "So what we actually\ncome out to now is we're going to have\nthis sigma hat still",
    "start": "4429880",
    "end": "4435699"
  },
  {
    "text": "multiplying everything, and then\nwe're going to have the y min prime times the\nprobability that z",
    "start": "4435700",
    "end": "4444100"
  },
  {
    "text": "is less than or\nequal to y min prime, and then we have\njust this term here.",
    "start": "4444100",
    "end": "4450145"
  },
  {
    "start": "4450145",
    "end": "4458000"
  },
  {
    "text": "And now if we want to\nsub back in and reverse our change of variables here\nto get our final result,",
    "start": "4458000",
    "end": "4464603"
  },
  {
    "text": "we can go ahead and do that, and\nwhat we're going to end up with is y min minus y minus\nour mu, our mu hat.",
    "start": "4464603",
    "end": "4472895"
  },
  {
    "start": "4472895",
    "end": "4477950"
  },
  {
    "text": "And that's going to be times\nthis probability where it's now probability that y is less\nthan or equal to y min.",
    "start": "4477950",
    "end": "4485389"
  },
  {
    "text": "And then this term is just going\nto become sigma hat squared. And then we're going to have--",
    "start": "4485390",
    "end": "4493370"
  },
  {
    "text": "this will be y min. And our distribution\nis now with respect",
    "start": "4493370",
    "end": "4498710"
  },
  {
    "text": "to mu hat and sigma hat. And so this is the\nfinal expression",
    "start": "4498710",
    "end": "4503780"
  },
  {
    "text": "for our expected improvement\nthat we've now switched it back to. So that was how we\nget to this result.",
    "start": "4503780",
    "end": "4510878"
  },
  {
    "text": "And so now this is nice\nbecause we have our probability of improvement there. We have how much we're\nexpecting to improve this by",
    "start": "4510878",
    "end": "4518000"
  },
  {
    "text": "and the likelihood here. And then we've also-- we've included that distribution\nterm there for our uncertainty.",
    "start": "4518000",
    "end": "4527640"
  },
  {
    "text": "So now we can actually\nimplement this and see how we perform on this\nexpected improvement metric.",
    "start": "4527640",
    "end": "4535380"
  },
  {
    "text": "And so these are the steps\nthat I just went through here. ",
    "start": "4535380",
    "end": "4541750"
  },
  {
    "text": "And I didn't make any\ntypos, so that's good. OK, so now if we\nactually implement this,",
    "start": "4541750",
    "end": "4551170"
  },
  {
    "text": "we're seeing the\nexact same example as before where our\nexpected improvement is shown on the bottom.",
    "start": "4551170",
    "end": "4557020"
  },
  {
    "text": "So we start off and we're\ngoing to sample actually very close to the minimum. So I think this is just\nmore of a nice example",
    "start": "4557020",
    "end": "4562900"
  },
  {
    "text": "here that we start\noff doing that. It won't always be the case. Then we go-- we go to the bounds\nnext and we clamp those down.",
    "start": "4562900",
    "end": "4570909"
  },
  {
    "text": "So you can still see,\nwe have that dependence on the bounded region. And then lastly, we\nend up sampling there.",
    "start": "4570910",
    "end": "4579650"
  },
  {
    "text": "So it definitely does\nbetter than probability of improvement. And now it, I think,\nintuitively makes sense",
    "start": "4579650",
    "end": "4585969"
  },
  {
    "text": "that you're looking\nat not just what's the likelihood I'll improve,\nbut how much do I actually expect to improve by?",
    "start": "4585970",
    "end": "4591670"
  },
  {
    "text": "So I think it makes\na lot of sense there on why you would want\nto use this type of method.",
    "start": "4591670",
    "end": "4597085"
  },
  {
    "text": " OK, so this is just\nthe main point.",
    "start": "4597085",
    "end": "4602210"
  },
  {
    "text": "We're just looking at where\nwe expected to improve it as much as possible.",
    "start": "4602210",
    "end": "4608930"
  },
  {
    "text": "And so if we want to look\nat a really quick comparison of expected improvement versus\nprobability of improvement,",
    "start": "4608930",
    "end": "4615579"
  },
  {
    "text": "for this specific example,\nwe can compare the two. So on the left, we have\nprobability of improvement; on the right, we have\nexpected improvement.",
    "start": "4615580",
    "end": "4623030"
  },
  {
    "text": "And so if we go\nahead and we sample at the maximum of\nboth of those, we can see how they would differ.",
    "start": "4623030",
    "end": "4630290"
  },
  {
    "text": "So if we actually look at\nwhere they would sample, the probability of improvement,\nlike we saw before, is sampling very close to\nwhere we've already seen,",
    "start": "4630290",
    "end": "4637090"
  },
  {
    "text": "whereas expected improvement\nis doing a little bit better and going to regions\nthat we haven't yet seen.",
    "start": "4637090",
    "end": "4642235"
  },
  {
    "text": " OK, so we've seen\nthat we can use",
    "start": "4642235",
    "end": "4649290"
  },
  {
    "text": "the probabilistic surrogate\nmodel, the Gaussian process, to guide where we're going\nto place our samples.",
    "start": "4649290",
    "end": "4654659"
  },
  {
    "text": "And an important\nthing to keep in mind is all of the strategies\nthat we've talked about here, they're just looking at\nthe next sample location.",
    "start": "4654660",
    "end": "4661530"
  },
  {
    "text": "They're not taking into\naccount a sequence of samples. And so that would involve\nsome sequential decision",
    "start": "4661530",
    "end": "4669000"
  },
  {
    "text": "making, which is sort of what\nmy research is focused on, is how do you use these\nsurrogate models to then look",
    "start": "4669000",
    "end": "4674010"
  },
  {
    "text": "at placing a sequence\nof samples, not just that next sample. ",
    "start": "4674010",
    "end": "4681190"
  },
  {
    "text": "OK, so that's the overview of\neverything we've talked about. We have two minutes left here.",
    "start": "4681190",
    "end": "4688000"
  },
  {
    "text": "So really quickly, this\nnotebook will be posted, but I just wanted to\ngive you a brief overview",
    "start": "4688000",
    "end": "4693640"
  },
  {
    "text": "of the structure of it, and\nthen you can play around with it on your own. ",
    "start": "4693640",
    "end": "4700870"
  },
  {
    "text": "But basically, this just\nimplements everything that we looked at today. And so there's this\nGP file here that",
    "start": "4700870",
    "end": "4708250"
  },
  {
    "text": "has a lot of the\nstructure behind it and some of the helper\nfunctions that are being used, but I'm just going to\nhighlight the main part",
    "start": "4708250",
    "end": "4715240"
  },
  {
    "text": "that you need to get started. So this structure\nhere is just all we need to specify\nour Gaussian process.",
    "start": "4715240",
    "end": "4721840"
  },
  {
    "text": "And this is all in Julia, a\nreal programming language. ",
    "start": "4721840",
    "end": "4727550"
  },
  {
    "text": "So all we need is just that\nmean, we're doing zero mean. We have our squared\nexponential kernel here.",
    "start": "4727550",
    "end": "4734950"
  },
  {
    "text": "And then these are just the data\npoints that we've seen so far, our y's, and then the\nnew term, and here, we're",
    "start": "4734950",
    "end": "4740680"
  },
  {
    "text": "saying we have perfectly\nobservable measurements. I'll wrap this up quickly. So we can just-- this is\njust implementing this--",
    "start": "4740680",
    "end": "4748510"
  },
  {
    "text": "what might have seemed\nscary to you at first, it's not scary at all. We can just implement\nit in these few lines",
    "start": "4748510",
    "end": "4753730"
  },
  {
    "text": "here of doing this prediction. So we're just putting\nthese equations into code. And then we can actually\ngo ahead and run it.",
    "start": "4753730",
    "end": "4761659"
  },
  {
    "text": "So I'll just highlight\nsome of the examples. So we can just sample\nlike different functions that we saw before.",
    "start": "4761660",
    "end": "4768110"
  },
  {
    "text": "We can do it in two\ndimensions as well. And then we can\nactually look at some",
    "start": "4768110",
    "end": "4773920"
  },
  {
    "text": "of the different\nexploration strategies. So if we're doing\nprediction-based, this will actually like walk\nyou through each one of those,",
    "start": "4773920",
    "end": "4779798"
  },
  {
    "text": "and you can see how the samples\nwould change based on what your function looks like. So you can go ahead and\nplay around with that.",
    "start": "4779798",
    "end": "4786557"
  },
  {
    "text": "We've implemented\nit for all of them. So for expected\nimprovement, everything, you can go take a\nlook at that, and I'm",
    "start": "4786557",
    "end": "4793420"
  },
  {
    "text": "happy to answer any questions\nthat you would have as well. ",
    "start": "4793420",
    "end": "4798960"
  },
  {
    "text": "Thank you. [APPLAUSE] ",
    "start": "4798960",
    "end": "4808000"
  }
]