[
  {
    "start": "0",
    "end": "5330"
  },
  {
    "text": "Hi, everyone. I'm Amelia, I'm a master's\nstudent with Michael and Sizzle.",
    "start": "5330",
    "end": "11450"
  },
  {
    "text": "And today, I'll be talking about\npolicy gradient estimation. But first, Friday at 5:00\nPM, what is happening?",
    "start": "11450",
    "end": "24075"
  },
  {
    "text": "[INAUDIBLE] proposal. Yes. So for your proposal,\nthe most important thing",
    "start": "24075",
    "end": "29119"
  },
  {
    "text": "is that the project\nyou want to work on is a sequential\ndecision problem.",
    "start": "29120",
    "end": "35630"
  },
  {
    "text": "If you're unsure on any\nof those components, please come by office\nhours or post on Ed.",
    "start": "35630",
    "end": "41100"
  },
  {
    "text": "We're happy to clarify,\noh, with uncertainty.",
    "start": "41100",
    "end": "47000"
  },
  {
    "text": "Don't forget that.  Come by office hours\nif you're not sure.",
    "start": "47000",
    "end": "52920"
  },
  {
    "text": "We're happy to help\ntalk through it. And also as a note,\nit's totally fine if you don't yet know how you're\ngoing to solve that problem.",
    "start": "52920",
    "end": "59850"
  },
  {
    "text": "But you need to make sure\nthat the problem you're interested in solving has\nthose characteristics.",
    "start": "59850",
    "end": "66500"
  },
  {
    "text": "All right, and\nwith that, we will get started with policy\ngradient estimation. So like Josh talks about\nin the last lecture,",
    "start": "66500",
    "end": "76080"
  },
  {
    "text": "we have some policy pi. And pi is going to tell us\nwhat action to take based",
    "start": "76080",
    "end": "82009"
  },
  {
    "text": "on the state that we're in. So for this lecture,\nI'm going to be using the example of\nhaving a policy to control",
    "start": "82010",
    "end": "88189"
  },
  {
    "text": "the temperature in a building. And we're going to say that\nthe actions that we could take would be turning on the\nheat, or turning on the AC.",
    "start": "88190",
    "end": "95640"
  },
  {
    "text": "And then we'll additionally\nhave the constraint that we're never doing\nboth at the same time, just because that\npretty clearly seems",
    "start": "95640",
    "end": "102020"
  },
  {
    "text": "like that wouldn't be a great\npolicy based on our expert intuition. So one thing that\npolicy might look",
    "start": "102020",
    "end": "108530"
  },
  {
    "text": "like is for every\npossible temperature, we have some associated action. So we could say, at 30 degrees,\nwe're going to turn on the heat.",
    "start": "108530",
    "end": "118320"
  },
  {
    "text": "And at 31, we'll also have heat. And then at some point,\nthings get warm enough.",
    "start": "118320",
    "end": "126190"
  },
  {
    "text": "Maybe at 85, now we're\ngoing to turn on the AC. And as you can\nimagine, in cases where",
    "start": "126190",
    "end": "133500"
  },
  {
    "text": "we have large or even continuous\nstate spaces, storing a policy like this wouldn't be possible.",
    "start": "133500",
    "end": "140040"
  },
  {
    "text": "So that is why we're going\nto be looking at policies that are parameterized.",
    "start": "140040",
    "end": "146019"
  },
  {
    "text": "And we're going to call\nthe parameters, theta. So to give you an\nexample of what that might look like for this case.",
    "start": "146020",
    "end": "153270"
  },
  {
    "text": "Along this axis, we're going\nto put possible temperatures. So we'll say, 0 to 100 for\nthese purposes, the weather",
    "start": "153270",
    "end": "163590"
  },
  {
    "text": "here is pretty nice. And then rather than storing a\ntable with 100 different values,",
    "start": "163590",
    "end": "169269"
  },
  {
    "text": "we could set, for example, two\nparameters, maybe theta one and theta two.",
    "start": "169270",
    "end": "175790"
  },
  {
    "text": "And what we'll do, our policy\nbased on these parameters will be that if we're below\ntheta 1, we turn on the heat.",
    "start": "175790",
    "end": "184099"
  },
  {
    "text": "And then if we're above\ntheta two, we turn on the AC. And between them,\nwe have neither.",
    "start": "184100",
    "end": "192530"
  },
  {
    "text": "So once we have this policy\nthat's parameterized, the thing that we're generally\ninterested in optimizing,",
    "start": "192530",
    "end": "199890"
  },
  {
    "text": "which will be Kianna's\nhalf of the lecture, is the utility of it.",
    "start": "199890",
    "end": "205790"
  },
  {
    "text": "And we write the utility of the\npolicy parameterized by theta as u of theta.",
    "start": "205790",
    "end": "211580"
  },
  {
    "text": "So in our optimization,\nmany of the methods rely on the gradient\nof u of theta.",
    "start": "211580",
    "end": "218493"
  },
  {
    "text": "And that's why we're\ngoing to be looking at a few different options for\nestimating that in this lecture.",
    "start": "218493",
    "end": "224780"
  },
  {
    "text": "So because theta will generally\nbe multiple variables, otherwise we'd have\na very simple policy.",
    "start": "224780",
    "end": "231800"
  },
  {
    "text": "Gradient u of theta is\ngoing to have this form. ",
    "start": "231800",
    "end": "238420"
  },
  {
    "text": "So for every theta\nI, we're going",
    "start": "238420",
    "end": "243760"
  },
  {
    "text": "to have a partial derivative. ",
    "start": "243760",
    "end": "252790"
  },
  {
    "text": "Let's say that we have n\nof these parameters theta. For the example that I just\nshowed you, n would be 2.",
    "start": "252790",
    "end": "260269"
  },
  {
    "text": "And this is going to\ngive us an n by 1 vector. ",
    "start": "260269",
    "end": "265660"
  },
  {
    "text": "And before I go on, I\nwanted to make a note. This portion of the lecture\nand also probably your portion",
    "start": "265660",
    "end": "271120"
  },
  {
    "text": "too is pretty calculus heavy. And depending on\nyour background, you may have done that\nmore or less recently.",
    "start": "271120",
    "end": "276710"
  },
  {
    "text": "So I don't want you to feel like\ncalculus-related questions are less legitimate than\nquestions about material",
    "start": "276710",
    "end": "282940"
  },
  {
    "text": "that maybe you've only\nseen in this course. I definitely had to look up\nstuff preparing for this. And if the calculus\nis a sticking point,",
    "start": "282940",
    "end": "290900"
  },
  {
    "text": "please say something. We want you to feel good about\nevery step that we're taking.",
    "start": "290900",
    "end": "296790"
  },
  {
    "text": "So the first method is\nsomething that you probably saw in early calculus.",
    "start": "296790",
    "end": "303580"
  },
  {
    "text": "And that's going to\nbe finite differences. ",
    "start": "303580",
    "end": "311190"
  },
  {
    "text": "So in finite differences, we\nhave some function and then",
    "start": "311190",
    "end": "323190"
  },
  {
    "text": "some point, where we're\ninterested in computing the derivative.",
    "start": "323190",
    "end": "328360"
  },
  {
    "text": "And does anyone remember\nwhat we do for this method? No worries.",
    "start": "328360",
    "end": "334479"
  },
  {
    "text": "So we're going to take another\npoint some small distance away that we call delta.",
    "start": "334480",
    "end": "341940"
  },
  {
    "text": "And we're going to compute\nthe slope between those two. So the dotted line is the\nthing that we're computing.",
    "start": "341940",
    "end": "348900"
  },
  {
    "text": "Or the dotted line is the\nthing that we're estimating. The non-dotted line is the\nthing that we're computing.",
    "start": "348900",
    "end": "356860"
  },
  {
    "text": "And we're using that to get\na basic approximation just around that one\npoint of interest.",
    "start": "356860",
    "end": "362220"
  },
  {
    "text": "So in the multivariable\ncase that we're considering for the\ngradient of u of theta,",
    "start": "362220",
    "end": "369389"
  },
  {
    "text": "this is going to look\nsomething like u theta",
    "start": "369390",
    "end": "378600"
  },
  {
    "text": "plus delta e1 minus u\nof theta all over delta.",
    "start": "378600",
    "end": "388020"
  },
  {
    "text": "And then we're going to\ndo one of these for each of our parameters theta.",
    "start": "388020",
    "end": "394295"
  },
  {
    "start": "394295",
    "end": "406710"
  },
  {
    "text": "So you might be wondering\nwhat this e here is.",
    "start": "406710",
    "end": "412110"
  },
  {
    "text": "This is a standard basis vector. And this index indicates the\nposition at which we have a 1.",
    "start": "412110",
    "end": "418500"
  },
  {
    "text": "All other positions\nare going to be 0. So here, we would have a 1,\nfollowed by n minus 1, 0s,",
    "start": "418500",
    "end": "424970"
  },
  {
    "text": "and so on. And the reason that\nwe're doing that is that we're only applying\nthis difference delta",
    "start": "424970",
    "end": "430970"
  },
  {
    "text": "to one parameter at a time. And so this is going\nto give us an n",
    "start": "430970",
    "end": "437419"
  },
  {
    "text": "by 1 vector with a simple\nestimate of the gradient of e",
    "start": "437420",
    "end": "442580"
  },
  {
    "text": "of theta around some point. Does anyone have questions here?",
    "start": "442580",
    "end": "447990"
  },
  {
    "text": "How do we feel\nabout the first one? Yes? What's u again?",
    "start": "447990",
    "end": "454520"
  },
  {
    "text": "Great question. U is the utility of theta. And we get u of theta, as\nJosh mentioned previously,",
    "start": "454520",
    "end": "461840"
  },
  {
    "text": "by using rollout simulations. So this is something\nthat can impact",
    "start": "461840",
    "end": "467210"
  },
  {
    "text": "this method is that depending\non the variance of u of theta, it may take a lot of rollouts\nto get a good estimate.",
    "start": "467210",
    "end": "474680"
  },
  {
    "text": "And the book talks\nabout some techniques that you could practically\nuse to mitigate the potential",
    "start": "474680",
    "end": "480460"
  },
  {
    "text": "in variance between when you're\ncomputing this and then running the policy.",
    "start": "480460",
    "end": "485650"
  },
  {
    "text": "Yes? The theta here\nare the parameters for the parameterization.",
    "start": "485650",
    "end": "493160"
  },
  {
    "text": "I guess, for high\ndimensional case, would this first order\napproximation always work?",
    "start": "493160",
    "end": "500102"
  },
  {
    "text": "Do you want to\nrepeat the question? So the question is for the\nhigher dimensional case",
    "start": "500102",
    "end": "505240"
  },
  {
    "text": "where we have a lot of\nthetas, would this first order approximation always work? Kind of depends on the data\nthat you're working with.",
    "start": "505240",
    "end": "514010"
  },
  {
    "text": "We will get into\nmore complex methods that are a bit more robust. But you can imagine if\nthe theta parameters are",
    "start": "514010",
    "end": "522729"
  },
  {
    "text": "on very different\nscales, then you may run into issues applying the\nsame difference to all of them.",
    "start": "522730",
    "end": "527980"
  },
  {
    "text": "So this would be ideal\nfor a simpler case.",
    "start": "527980",
    "end": "533058"
  },
  {
    "text": "And some of the other methods\nwould be better for cases like what you're talking about.",
    "start": "533058",
    "end": "538410"
  },
  {
    "text": "All right, anything else? Cool, OK. So the next method that\nwe're going to talk about",
    "start": "538410",
    "end": "546630"
  },
  {
    "text": "is called the\nregression gradient. ",
    "start": "546630",
    "end": "559889"
  },
  {
    "text": "And this may have come\nup in other ML classes",
    "start": "559890",
    "end": "567000"
  },
  {
    "text": "that you've taken. But when we do a regression, we\nhave some set of data points. ",
    "start": "567000",
    "end": "576030"
  },
  {
    "text": "And then we're finding the\nline that best fits them. So this is that regression.",
    "start": "576030",
    "end": "583330"
  },
  {
    "text": "And what we're\ngoing to talk about is what the line is\nthat we're fitting. ",
    "start": "583330",
    "end": "589610"
  },
  {
    "text": "So we know that the thing\nthat we're interested in is that we want to find\nthe gradient of u of theta.",
    "start": "589610",
    "end": "600025"
  },
  {
    "text": " And around some\npoint theta, we're",
    "start": "600025",
    "end": "606920"
  },
  {
    "text": "estimating what\nthis line would be. So the first thing\nthat we're going to do",
    "start": "606920",
    "end": "613730"
  },
  {
    "text": "is we're going to\ncollect data points, and then we'll fit\nour regression line.",
    "start": "613730",
    "end": "619470"
  },
  {
    "text": "So what should the\ndata points be? We're interested in theta.",
    "start": "619470",
    "end": "624630"
  },
  {
    "text": "And we want to collect\na bunch of points-- let's say, this is\nwhat our theta is.",
    "start": "624630",
    "end": "631279"
  },
  {
    "text": "And we want to collect\npoints around it in its neighborhood, which\nwe can find the gradient at.",
    "start": "631280",
    "end": "640260"
  },
  {
    "text": "So we're going to\ncreate this matrix that we'll call delta theta.",
    "start": "640260",
    "end": "645975"
  },
  {
    "text": " And we're doing a perturbation\nfor each of our parameters.",
    "start": "645975",
    "end": "655366"
  },
  {
    "start": "655366",
    "end": "661960"
  },
  {
    "text": "And this is going\nto be m by n, where m is the number of\nperturbations and n,",
    "start": "661960",
    "end": "674199"
  },
  {
    "text": "again, is the number\nof parameters. ",
    "start": "674200",
    "end": "679630"
  },
  {
    "text": "There's no right or wrong\nanswer for choosing the number of perturbations to do. Empirically, a rule of thumb is\nto have double perturbations,",
    "start": "679630",
    "end": "690160"
  },
  {
    "text": "double that number of parameters\nbe the number of perturbations. But that could be based on\ncomputational constraints",
    "start": "690160",
    "end": "695440"
  },
  {
    "text": "or expert knowledge, or you\ntried a few things and this was what worked the best.",
    "start": "695440",
    "end": "700690"
  },
  {
    "text": "And then the way that we get the\nperturbations is also something. There's no one analytical\noption that's the best.",
    "start": "700690",
    "end": "708040"
  },
  {
    "text": "But empirically, we find\nthat randomly sampling",
    "start": "708040",
    "end": "717420"
  },
  {
    "text": "from the normal\ndistribution works well.",
    "start": "717420",
    "end": "723120"
  },
  {
    "text": "And these are also going to\nbe normalized random samples. So you can visualize that\nas though we have a sphere",
    "start": "723120",
    "end": "734080"
  },
  {
    "text": "and we're sampling\npoints along the surface. And so those would be what\nthe perturbations are.",
    "start": "734080",
    "end": "742290"
  },
  {
    "text": "So that's how we get\nour changes in theta. Yes, question.",
    "start": "742290",
    "end": "748260"
  },
  {
    "text": "Just wondering,\nif you're randomly sampling around\nthat point theta,",
    "start": "748260",
    "end": "753330"
  },
  {
    "text": "how are you getting a line\nfrom that [INAUDIBLE]? Yeah, great question.",
    "start": "753330",
    "end": "758756"
  },
  {
    "text": "So the question is,\nif you're randomly sampling around the point theta,\nwhere does the line come from?",
    "start": "758757",
    "end": "764010"
  },
  {
    "text": "So once we have our\nrandom samples, which are the differences\nfrom the original theta, we also need to find differences\nfrom the original u of theta.",
    "start": "764010",
    "end": "773040"
  },
  {
    "text": "And then I'll talk\nabout the method that we use for fitting\nthe line between the two. But basically, we collect\nboth of those data points",
    "start": "773040",
    "end": "780770"
  },
  {
    "text": "and then use the method that\nwe would use for regression. I think this also came\nup in the policy--",
    "start": "780770",
    "end": "786800"
  },
  {
    "text": "or no, the approximate value\nfunction chapter as well. Yes, but that's a great point.",
    "start": "786800",
    "end": "792960"
  },
  {
    "text": "That is what we need to do next. Any other questions? Yes?",
    "start": "792960",
    "end": "799190"
  },
  {
    "text": "Is that sampling on the sphere\nthat you just mentioned? Yeah. So let's see.",
    "start": "799190",
    "end": "806240"
  },
  {
    "text": "I'll draw a picture. And hopefully, this helps. But if not, let me know. So we have some\nnormal distribution.",
    "start": "806240",
    "end": "815520"
  },
  {
    "text": "So we're sampling from\nsome normal distribution. And we could have\nany mean mu here.",
    "start": "815520",
    "end": "825370"
  },
  {
    "text": "And then we're using the\nsamples from that distribution to perturb it. But when we normalize, then\nthey'll all be distances of 1",
    "start": "825370",
    "end": "836950"
  },
  {
    "text": "from the original value. So we're just-- we could\nbe moving a distance of 1",
    "start": "836950",
    "end": "842350"
  },
  {
    "text": "in different directions\nfor each of the parameters. But we're not going to\nbe moving 5 and then 3.",
    "start": "842350",
    "end": "848660"
  },
  {
    "text": "So it's just\nconcerning it a bit. Yes? Would we be sampled from\nthe normal distribution for each coordinate?",
    "start": "848660",
    "end": "856388"
  },
  {
    "text": "Would we be sampling from\nthe normal distribution for each coordinate? We're applying the same\nperturbation for each coordinate",
    "start": "856388",
    "end": "862870"
  },
  {
    "text": "or the same set\nof perturbations. So we're not doing-- we only look at this\npoint for one of them.",
    "start": "862870",
    "end": "871029"
  },
  {
    "text": "We would choose some\nnumber of points and apply that to all of them,\nwhich is what gives the m by n.",
    "start": "871030",
    "end": "878280"
  },
  {
    "text": "In n dimensions, you're\nsampling from an n dimensional [INAUDIBLE]? Yes.",
    "start": "878280",
    "end": "883890"
  },
  {
    "text": "Yeah. Ah, yes? Are the random points\nthemselves that we",
    "start": "883890",
    "end": "889006"
  },
  {
    "text": "would call perturbations? Are the random points\nthemselves that we would call perturbations? So the perturbation would--",
    "start": "889006",
    "end": "896769"
  },
  {
    "text": "the points are how\nwe're perturbing theta. So the perturbation is like,\nOK, we've taken a point,",
    "start": "896770",
    "end": "902529"
  },
  {
    "text": "this is how much we're\ngoing to change theta. And then we apply that change. And I think once we've applied\nthat change, that's what we'd",
    "start": "902530",
    "end": "909959"
  },
  {
    "text": "call the perturbation, yeah. ",
    "start": "909960",
    "end": "916500"
  },
  {
    "text": "Any other questions? Cool, OK.",
    "start": "916500",
    "end": "921870"
  },
  {
    "text": "So that would be how we get our\ndata set of points for theta. And that's definitely\nthe more complicated part",
    "start": "921870",
    "end": "928980"
  },
  {
    "text": "of building this data set. And then when it comes\nto computing delta u,",
    "start": "928980",
    "end": "934379"
  },
  {
    "text": "this is going to be more similar\nto what we've seen before. So using rollouts with\nMonte Carlo simulation,",
    "start": "934380",
    "end": "942600"
  },
  {
    "text": "we're going to find u of\ntheta plus delta theta 1 minus u of theta.",
    "start": "942600",
    "end": "950180"
  },
  {
    "text": "And then we're going\nto find this difference for each of our n parameters. ",
    "start": "950180",
    "end": "959050"
  },
  {
    "text": "Sine theta plus delta theta.",
    "start": "959050",
    "end": "964870"
  },
  {
    "start": "964870",
    "end": "972920"
  },
  {
    "text": "And this will be m by 1. And again, these are coming\nfrom policy rollouts.",
    "start": "972920",
    "end": "981633"
  },
  {
    "start": "981633",
    "end": "987020"
  },
  {
    "text": "So does everyone feel good on\nwhere delta theta and delta u come from?",
    "start": "987020",
    "end": "992529"
  },
  {
    "text": " All right. So now that we have\ndelta theta and delta u,",
    "start": "992530",
    "end": "1000250"
  },
  {
    "text": "we need to fit the line between\nthem to estimate the gradient. So we're going to say,\ngradient u of theta",
    "start": "1000250",
    "end": "1008550"
  },
  {
    "text": "is approximately\nequal to delta theta.",
    "start": "1008550",
    "end": "1014279"
  },
  {
    "text": "This upside down t means\npseudoinverse delta u. And the reason that we're\ndoing the pseudoinverse instead",
    "start": "1014280",
    "end": "1022350"
  },
  {
    "text": "of the regular inverse is\nthat we're not guaranteed that delta theta is invertible.",
    "start": "1022350",
    "end": "1028859"
  },
  {
    "text": "Because m and n ideally\nshould be different, so it's not going to be square. And so when we do\na pseudoinverse,",
    "start": "1028859",
    "end": "1034204"
  },
  {
    "text": "like if we were doing the\npseudoinverse of x, what this",
    "start": "1034205",
    "end": "1040799"
  },
  {
    "text": "actually is going to work out to\nis the inverse of x transpose x",
    "start": "1040800",
    "end": "1048569"
  },
  {
    "text": "times x transpose. And supposing that x is m by n,\nthen x transpose will be n by m.",
    "start": "1048569",
    "end": "1059130"
  },
  {
    "text": "And we can see that we'll be\ntaking the inverse of something n by n, which is square.",
    "start": "1059130",
    "end": "1065620"
  },
  {
    "text": "So that's what the\npseudoinverse allows us to do. And this is the same as what\nwe used for approximate value",
    "start": "1065620",
    "end": "1071520"
  },
  {
    "text": "functions if you want to take\na look at that in more detail. The 229 notes which\nare available online",
    "start": "1071520",
    "end": "1078030"
  },
  {
    "text": "also go into this\nin great depth, and I recommend that as well. Any other questions here?",
    "start": "1078030",
    "end": "1085440"
  },
  {
    "text": "Yes? Explain why [INAUDIBLE].",
    "start": "1085440",
    "end": "1090750"
  },
  {
    "text": "Why delta-- [INAUDIBLE]",
    "start": "1090750",
    "end": "1096300"
  },
  {
    "text": "Oh, good question. So in this case, it's\nm by 1 because we're doing one of these differences\nfor each perturbation.",
    "start": "1096300",
    "end": "1103840"
  },
  {
    "text": "So in the previous\nmethod, we just did one difference\nper parameter.",
    "start": "1103840",
    "end": "1109630"
  },
  {
    "text": "And that was n by 1. And then here, we're doing\none per perturbation.",
    "start": "1109630",
    "end": "1114960"
  },
  {
    "text": "So it's m by 1 now. And the important\nthing is just that we have an equal number of\nperturbations of theta and of u.",
    "start": "1114960",
    "end": "1129860"
  },
  {
    "text": "And in the previous\none, we just did one for each parameter theta. So that's how we made sure those\nwere the same in that case.",
    "start": "1129860",
    "end": "1136850"
  },
  {
    "text": "Would you do this for every\nsingle theta [INAUDIBLE]? No, you don't have to do\nthis for every single theta,",
    "start": "1136850",
    "end": "1144809"
  },
  {
    "text": "since the\nperturbations are being sampled from an n dimensional\nnormal distribution.",
    "start": "1144810",
    "end": "1150270"
  },
  {
    "text": "So we're moving all of the\nparameters in each perturbation and then we're doing\nthat m times, yes.",
    "start": "1150270",
    "end": "1158870"
  },
  {
    "text": " Any other questions here? ",
    "start": "1158870",
    "end": "1166070"
  },
  {
    "text": "Cool, OK.  So the last method that\nwe're going to look at",
    "start": "1166070",
    "end": "1173980"
  },
  {
    "text": "is the likelihood ratio. ",
    "start": "1173980",
    "end": "1184779"
  },
  {
    "text": "And this is definitely the\nmost calculus heavy component of this lecture.",
    "start": "1184780",
    "end": "1190640"
  },
  {
    "text": "So I'll try to go through\nit slowly one by one. And if any line isn't\nfeeling good for you all,",
    "start": "1190640",
    "end": "1195769"
  },
  {
    "text": "just let me know and\nwe can pause there. So as previously, the thing\nthat we want to compute",
    "start": "1195770",
    "end": "1204160"
  },
  {
    "text": "is the gradient of u of theta. And the way that we're going\nto do that in this case",
    "start": "1204160",
    "end": "1209679"
  },
  {
    "text": "is that we're going to use an\nanalytical form of u of theta,",
    "start": "1209680",
    "end": "1215590"
  },
  {
    "text": "which says u of\ntheta is the integral over all possible trajectories\nof p theta of tau, r of tau,",
    "start": "1215590",
    "end": "1228810"
  },
  {
    "text": "d tau. And I'll go through what each of\nthe things in this integral is.",
    "start": "1228810",
    "end": "1234809"
  },
  {
    "text": "So first, tau is going to\nbe some trajectory, which",
    "start": "1234810",
    "end": "1240600"
  },
  {
    "text": "is a sequence of\nstates and actions. So for example, for our\ntemperature setting,",
    "start": "1240600",
    "end": "1246160"
  },
  {
    "text": "we could have a tau look\nsomething like this. Maybe 30 degrees and\nwe turn on the heat",
    "start": "1246160",
    "end": "1253049"
  },
  {
    "text": "and then it gets really hot. 90 degrees, we turn on the AC.",
    "start": "1253050",
    "end": "1259890"
  },
  {
    "text": "And then we have apparently a\nsuper powerful heater and AC. So now, it's back\ndown to 40 degrees",
    "start": "1259890",
    "end": "1267630"
  },
  {
    "text": "and we turn the heat on again. So that's one example of\nwhat a trajectory might be.",
    "start": "1267630",
    "end": "1274860"
  },
  {
    "text": "R of tau is going to be the\nreturn for that trajectory.",
    "start": "1274860",
    "end": "1281370"
  },
  {
    "text": "And we say, return\nrather than reward because we're not just\nlooking at the reward",
    "start": "1281370",
    "end": "1286460"
  },
  {
    "text": "for the immediate state. We can use whatever\ntype of formulation we want to have a discount over\nthe entire sequence in tau.",
    "start": "1286460",
    "end": "1294929"
  },
  {
    "text": "Generally, we'll be using\nthe Bellman optimality equation, Michael's famous\nfavorite equation in this class.",
    "start": "1294930",
    "end": "1301410"
  },
  {
    "text": "But that's up to\nyou if you choose to do that some different way. So we have the return\nfor that trajectory.",
    "start": "1301410",
    "end": "1307919"
  },
  {
    "text": "And then this term\nis going to be the likelihood of the\ntrajectory as determined",
    "start": "1307920",
    "end": "1318830"
  },
  {
    "text": "by the policy\nparameterized by theta.",
    "start": "1318830",
    "end": "1328820"
  },
  {
    "text": "So intuitively,\nwhat does that mean?",
    "start": "1328820",
    "end": "1334009"
  },
  {
    "text": "Our policy is telling\nus, based on a state, what action we should take.",
    "start": "1334010",
    "end": "1340100"
  },
  {
    "text": "So let's say that we have some\npolicy, where we're heavily",
    "start": "1340100",
    "end": "1345280"
  },
  {
    "text": "prioritizing being\nsuper energy efficient. And so we're going to\nsay, theta 1 is like 10.",
    "start": "1345280",
    "end": "1352408"
  },
  {
    "text": "And we're only going\nto turn the heat on if it's below 10 degrees. And then theta 2 is 95.",
    "start": "1352408",
    "end": "1359659"
  },
  {
    "text": "And we only have the\nAC on above 95 degrees. So if we had a policy\nparameterized like that,",
    "start": "1359660",
    "end": "1366260"
  },
  {
    "text": "then this trajectory that I just\nshowed you would be an example, or it would be impossible.",
    "start": "1366260",
    "end": "1371840"
  },
  {
    "text": "That would never happen, since\naccording to these parameters, we wouldn't turn\non the heat and AC.",
    "start": "1371840",
    "end": "1378220"
  },
  {
    "text": "So the policy is determining\nhow likely a trajectory is.",
    "start": "1378220",
    "end": "1385659"
  },
  {
    "text": "Do we feel good on that? Yes?",
    "start": "1385660",
    "end": "1390669"
  },
  {
    "text": "I thought we were trying\nto find the policy. Or do we already know the\npolicy, or just like a--",
    "start": "1390670",
    "end": "1397210"
  },
  {
    "text": "We have some initial policy. And then we're\noptimizing theta that's",
    "start": "1397210",
    "end": "1404730"
  },
  {
    "text": "parameterizing that\npolicy by optimizing the utility of those thetas.",
    "start": "1404730",
    "end": "1410620"
  },
  {
    "text": "So maybe an example\nthat is more familiar is that if we have some neural\nnetwork with parameters theta,",
    "start": "1410620",
    "end": "1419049"
  },
  {
    "text": "then we're going to be\noptimizing for the best parameters of our neural\nnetwork by finding the gradient of the loss.",
    "start": "1419050",
    "end": "1426030"
  },
  {
    "text": "So it'll be slowly changing? Yeah, we're trying to make\nsure that we're improving",
    "start": "1426030",
    "end": "1431550"
  },
  {
    "text": "the utility incrementally. And the way that\nwe're doing that is by finding the\ngradient of the utility",
    "start": "1431550",
    "end": "1437640"
  },
  {
    "text": "and then trying to move\nour theta in a way that's positive for the\ngradient of the utility.",
    "start": "1437640",
    "end": "1443760"
  },
  {
    "text": "Thank you. No problem, that\nwas a good question. Anything else? ",
    "start": "1443760",
    "end": "1452850"
  },
  {
    "text": "Cool, OK. So the policy that we have is\ndetermining the likelihood.",
    "start": "1452850",
    "end": "1461580"
  },
  {
    "text": "And so what we're\ngoing to do is we're going to take this\nanalytical expression,",
    "start": "1461580",
    "end": "1466610"
  },
  {
    "text": "and we're going to get an\nexpression for the gradient of u of theta from it.",
    "start": "1466610",
    "end": "1472380"
  },
  {
    "text": "And to start, we're\njust going to take the gradient of both sides. ",
    "start": "1472380",
    "end": "1488900"
  },
  {
    "text": "So the first step\nthat we'll take is we're just going to move the\ngradient inside of the integral.",
    "start": "1488900",
    "end": "1498013"
  },
  {
    "start": "1498013",
    "end": "1510058"
  },
  {
    "text": "And then the next thing\nthat we're going to do is we're going to multiply what\nwe have inside of this integral",
    "start": "1510058",
    "end": "1516830"
  },
  {
    "text": "by 1. ",
    "start": "1516830",
    "end": "1538000"
  },
  {
    "text": "And the reason that\nwe're doing this is that once we've done\nthat multiplication,",
    "start": "1538000",
    "end": "1543070"
  },
  {
    "text": "now we can write these out in a\nway that allows us to simplify. So we're going to\nwrite this as p theta",
    "start": "1543070",
    "end": "1554350"
  },
  {
    "text": "of tau times the gradient\nof p theta of tau over p",
    "start": "1554350",
    "end": "1562900"
  },
  {
    "text": "theta of tau, r of tau, d tau.",
    "start": "1562900",
    "end": "1570550"
  },
  {
    "text": "And you'll see that when\nwritten in this way, the expression that we\nhave is actually taking",
    "start": "1570550",
    "end": "1577210"
  },
  {
    "text": "the form of an expectation. So now, we can write this\nas the expectation over tau",
    "start": "1577210",
    "end": "1586799"
  },
  {
    "text": "of gradient theta, p theta\nof tau over p theta of tau,",
    "start": "1586800",
    "end": "1596550"
  },
  {
    "text": "r of tau. And I'm going to pause\nhere for a second. Does everyone feel good\non why we can convert that",
    "start": "1596550",
    "end": "1603780"
  },
  {
    "text": "to an expectation?  Yeah, definitely.",
    "start": "1603780",
    "end": "1609880"
  },
  {
    "text": "So let's see. I'll give an example. Let's say that we\nhave f of x, then",
    "start": "1609880",
    "end": "1621540"
  },
  {
    "text": "the expectation of f of x is the\nintegral over all possible x, p",
    "start": "1621540",
    "end": "1631500"
  },
  {
    "text": "of x, f of x, dx. ",
    "start": "1631500",
    "end": "1637970"
  },
  {
    "text": "And so we can map\nthat form onto here, where this would\nbe the probability.",
    "start": "1637970",
    "end": "1645480"
  },
  {
    "text": "And then this would be f of tau.",
    "start": "1645480",
    "end": "1651110"
  },
  {
    "text": "P of tau, f of tau, and\nthen we can write that as the expectation\nover this f of tau.",
    "start": "1651110",
    "end": "1658120"
  },
  {
    "text": " All right, sweet. ",
    "start": "1658120",
    "end": "1665210"
  },
  {
    "text": "So based on a trick\nin chapter 10.5 that I will not\nbe deriving today,",
    "start": "1665210",
    "end": "1672510"
  },
  {
    "text": "but I will note here in\ncase you are interested, this is called the\nlog derivative trick.",
    "start": "1672510",
    "end": "1679410"
  },
  {
    "text": "And that allows us to write\nthe expectation in this form.",
    "start": "1679410",
    "end": "1684820"
  },
  {
    "start": "1684820",
    "end": "1699909"
  },
  {
    "text": "So now, we have an\nexpression for the gradient u",
    "start": "1699910",
    "end": "1706660"
  },
  {
    "text": "of theta in the form\nof this expectation. Do we feel good on\nthat relationship?",
    "start": "1706660",
    "end": "1712366"
  },
  {
    "start": "1712366",
    "end": "1720010"
  },
  {
    "text": "All right, so we're\nassuming that we",
    "start": "1720010",
    "end": "1725800"
  },
  {
    "text": "have some formulation\nfor the return that we know that's\nsimple to compute. So we're not going to\nworry about that part.",
    "start": "1725800",
    "end": "1732399"
  },
  {
    "text": "And then the last\nthing that we need to do in order to estimate\nthe gradient from this method",
    "start": "1732400",
    "end": "1737470"
  },
  {
    "text": "is we need to find\nthis right here. We're interested in the\ngradient with respect",
    "start": "1737470",
    "end": "1744910"
  },
  {
    "text": "to theta of the log\nof p theta of tau.",
    "start": "1744910",
    "end": "1753400"
  },
  {
    "text": "And there are going to be\ntwo different cases for this, depending on the type\nof policy that we",
    "start": "1753400",
    "end": "1760090"
  },
  {
    "text": "have parameters by theta. So we can have either\na stochastic policy,",
    "start": "1760090",
    "end": "1773710"
  },
  {
    "text": "or we can have a\ndeterministic policy. ",
    "start": "1773710",
    "end": "1785500"
  },
  {
    "text": "So when we have a\ndeterministic policy,",
    "start": "1785500",
    "end": "1790540"
  },
  {
    "text": "we have our policy parameterized\nby theta, pi theta. And every time we give\nit the same state s,",
    "start": "1790540",
    "end": "1798160"
  },
  {
    "text": "it's going to return\nthe same action a. ",
    "start": "1798160",
    "end": "1806110"
  },
  {
    "text": "So for example, every time\nit's 30 degrees, we say, turn on the heat. That's always what's\ngoing to happen,",
    "start": "1806110",
    "end": "1812530"
  },
  {
    "text": "according to this policy. By contrast with the stochastic\npolicy, pi theta of our,",
    "start": "1812530",
    "end": "1823380"
  },
  {
    "text": "say, s is going to give us\na distribution over actions.",
    "start": "1823380",
    "end": "1831813"
  },
  {
    "start": "1831813",
    "end": "1841320"
  },
  {
    "text": "So maybe if s is\n30 degrees, then in this distribution, 90% of the\ntime, we're turning on the heat.",
    "start": "1841320",
    "end": "1849460"
  },
  {
    "text": "5% of the time, we're\nturning on the AC. 5% of the time, we have neither.",
    "start": "1849460",
    "end": "1854610"
  },
  {
    "text": "And this allows us to write pi\ntheta for a specific action,",
    "start": "1854610",
    "end": "1862480"
  },
  {
    "text": "like turning on the\nheat, given the state 30 degrees as the probability of\npicking action a from state s.",
    "start": "1862480",
    "end": "1879020"
  },
  {
    "text": "So in the example I just\nsaid, for the action turning on the heat, when\nour state is 30 degrees,",
    "start": "1879020",
    "end": "1885630"
  },
  {
    "text": "the probability is 90% How do\nwe feel about deterministic",
    "start": "1885630",
    "end": "1890990"
  },
  {
    "text": "versus stochastic policies?  Cool, OK, great.",
    "start": "1890990",
    "end": "1896750"
  },
  {
    "text": "So during lecture,\nwe're only going to talk about the\nstochastic case. And this may or may\nnot be surprising,",
    "start": "1896750",
    "end": "1903289"
  },
  {
    "text": "but it's actually\neasier to compute. And it's interesting to see why.",
    "start": "1903290",
    "end": "1909170"
  },
  {
    "text": "So for stochastic\npolicy, we're going to be finding the\ngradient with respect to theta, log p theta of tau.",
    "start": "1909170",
    "end": "1918360"
  },
  {
    "text": "And we're going to do each\nof these one at a time. So first, we have some tau.",
    "start": "1918360",
    "end": "1925610"
  },
  {
    "text": "And in this case, we'll\nhave a p sequence, where we have states,\nactions, and rewards",
    "start": "1925610",
    "end": "1933400"
  },
  {
    "text": "or returns up to\nsome time horizon. We could set that\nhowever we want",
    "start": "1933400",
    "end": "1939790"
  },
  {
    "text": "based on compute\nconstraints, knowledge about the problem, et cetera. In this case, I'm\ngoing to say, we're",
    "start": "1939790",
    "end": "1945519"
  },
  {
    "text": "only going to account\nfor the last 10 steps. ",
    "start": "1945520",
    "end": "1952210"
  },
  {
    "text": "So p theta of tau is going\nto be the probability",
    "start": "1952210",
    "end": "1962649"
  },
  {
    "text": "of our initial state. And this doesn't\ncome from the policy. You'll have some distribution\nover initial states.",
    "start": "1962650",
    "end": "1969920"
  },
  {
    "text": "That could be a\nuniform distribution. Again, it could be based on some\nknowledge about the problem.",
    "start": "1969920",
    "end": "1975530"
  },
  {
    "text": "But we're sampling from that\ndistribution independent of the policy. The policy doesn't affect\nwhere we're starting.",
    "start": "1975530",
    "end": "1982630"
  },
  {
    "text": "So we have that. And then we're multiplying\nthat by the products",
    "start": "1982630",
    "end": "1988830"
  },
  {
    "text": "across our time stamps of\nthe transition probability.",
    "start": "1988830",
    "end": "1995500"
  },
  {
    "text": "So how likely is it that we\ngo to the next state, given that we're in our current state.",
    "start": "1995500",
    "end": "2002450"
  },
  {
    "text": "And we took this chosen\naction in our trajectory, and then we multiply that by\nthe likelihood of that action",
    "start": "2002450",
    "end": "2015559"
  },
  {
    "text": "according to our policy. So our policy says,\nfor the state,",
    "start": "2015560",
    "end": "2020780"
  },
  {
    "text": "how likely is this action? And then we say, based on\nour transition probabilities,",
    "start": "2020780",
    "end": "2026610"
  },
  {
    "text": "given the state and\nthis action, how likely is the next\nstate in the trajectory?",
    "start": "2026610",
    "end": "2032010"
  },
  {
    "text": "And this whole product\ngives us the likelihood of our trajectory tau,\naccording to our policy",
    "start": "2032010",
    "end": "2038480"
  },
  {
    "text": "parameterized by theta. Does that make sense? ",
    "start": "2038480",
    "end": "2045480"
  },
  {
    "text": "Ah, sir, what did I say the\nlast part one more time? Yeah, definitely. OK, so pi theta of the\naction, given the state,",
    "start": "2045480",
    "end": "2055415"
  },
  {
    "text": "is the likelihood that we take\nthat action from the state according to our policy. We're multiplying that by\nthe transition probability",
    "start": "2055415",
    "end": "2065199"
  },
  {
    "text": "that we get to the\nnext state, given that we were in\nour current state, and we took the action\nthat our policy told us.",
    "start": "2065199",
    "end": "2071629"
  },
  {
    "text": "And then we're doing that\nproduct for every state in the sequence. And so that times the\nprobability of our first state,",
    "start": "2071630",
    "end": "2078710"
  },
  {
    "text": "which is not dependent\non the policy, gives us the likelihood\nof the trajectory overall.",
    "start": "2078710",
    "end": "2085750"
  },
  {
    "text": "Cool, all right. You've got a question. Oh yes.",
    "start": "2085750",
    "end": "2091059"
  },
  {
    "text": "Does that expression\nfollow from the chain rule? Exactly. This follows from\nthe chain rule.",
    "start": "2091060",
    "end": "2098500"
  },
  {
    "text": "Nice. Ah, yes. Sorry, just underneath\nthe pi, your product,",
    "start": "2098500",
    "end": "2105610"
  },
  {
    "text": "what is that variable,\nsome things equal to 1? Oh, that would be k. Let me write that a\nlittle bit more clearly.",
    "start": "2105610",
    "end": "2112380"
  },
  {
    "text": "And the superscripts\non S's [INAUDIBLE]. Oh, yes, the superscripts on the\nS's are the time that we're at.",
    "start": "2112380",
    "end": "2122549"
  },
  {
    "text": "So it would be-- s2 would be the second\nstate in the trajectory.",
    "start": "2122550",
    "end": "2128380"
  },
  {
    "text": "And I think actually-- Michael, please correct\nme if I'm wrong, but I think this should be\nstarting from 2, the products",
    "start": "2128380",
    "end": "2134145"
  },
  {
    "text": "here.  No, because we're\ngoing to 2 from 1, yes.",
    "start": "2134145",
    "end": "2140710"
  },
  {
    "text": " All right, yes?",
    "start": "2140710",
    "end": "2147160"
  },
  {
    "text": "So is it s to the k\nand then a to the-- Yeah, let me just clean\nup my hand real quick.",
    "start": "2147160",
    "end": "2154859"
  },
  {
    "text": "That was my bad. ",
    "start": "2154860",
    "end": "2170060"
  },
  {
    "text": "Yes, that's k, cool. All right, any other\nquestions here?",
    "start": "2170060",
    "end": "2176245"
  },
  {
    "text": " All right, so the\nfirst part is done.",
    "start": "2176245",
    "end": "2181350"
  },
  {
    "text": "We have an expression\nfor p theta of tau. And now, we're going to\ntake the log of that.",
    "start": "2181350",
    "end": "2188329"
  },
  {
    "text": "So the log of p theta of tau.",
    "start": "2188330",
    "end": "2194275"
  },
  {
    "text": " And because we're taking\nthe log of a product,",
    "start": "2194275",
    "end": "2199310"
  },
  {
    "text": "we can convert this\ninto a summation. So we have the log of the\nprobability of our first state.",
    "start": "2199310",
    "end": "2210740"
  },
  {
    "text": "And then we have--  let me print instead of cursive.",
    "start": "2210740",
    "end": "2217345"
  },
  {
    "start": "2217345",
    "end": "2235840"
  },
  {
    "text": "And then we have one last\nterm in our summation. ",
    "start": "2235840",
    "end": "2254290"
  },
  {
    "text": "All right, so\nwe've done the log. The product is a summation now. And then the last thing\nthat we need to do",
    "start": "2254290",
    "end": "2261880"
  },
  {
    "text": "is we need to take the gradient. And this is where having\na stochastic policy",
    "start": "2261880",
    "end": "2267490"
  },
  {
    "text": "actually is going to\nmake things easier. So when we take the\ngradient with respect to theta of both sides, we\nwill see that all of the-- yes?",
    "start": "2267490",
    "end": "2282930"
  },
  {
    "text": "Should there be a log? Yes, there should be a log. Good catch. ",
    "start": "2282930",
    "end": "2290160"
  },
  {
    "text": "All right, So when we take\nthe gradient of both sides,",
    "start": "2290160",
    "end": "2295240"
  },
  {
    "text": "all of the terms that aren't\ndependent on our policy parameterized by theta drop out.",
    "start": "2295240",
    "end": "2300520"
  },
  {
    "text": "So we can get rid of that. And we can get rid of that.",
    "start": "2300520",
    "end": "2306599"
  },
  {
    "text": "And now, the expression\nthat we're left with",
    "start": "2306600",
    "end": "2313142"
  },
  {
    "text": "is just this summation. ",
    "start": "2313142",
    "end": "2332910"
  },
  {
    "text": "And this gives us another\noption for estimating. Yes? Oh, log.",
    "start": "2332910",
    "end": "2339205"
  },
  {
    "start": "2339205",
    "end": "2344750"
  },
  {
    "text": "They dropped out\nbecause they didn't have any dependence on theta.",
    "start": "2344750",
    "end": "2350790"
  },
  {
    "text": "So it's like if we were\ndoing derivative with respect",
    "start": "2350790",
    "end": "2356840"
  },
  {
    "text": "to x of y, same\nthing here, yeah.",
    "start": "2356840",
    "end": "2363590"
  },
  {
    "text": "Anything else here? Any other steps that\nmight not be clear?",
    "start": "2363590",
    "end": "2369710"
  },
  {
    "text": "Yes? This might be\nnitpicking, but should it be from k equals 1 to\nd minus 1, or should it be from k equals 1 to d?",
    "start": "2369710",
    "end": "2377089"
  },
  {
    "text": "Let's see, good\nquestion, not nitpicking. ",
    "start": "2377090",
    "end": "2385430"
  },
  {
    "text": "Yeah, that's a v to d minus 1. So not--",
    "start": "2385430",
    "end": "2390626"
  },
  {
    "text": "No. I don't think so. No. It's not you can take\nit offline line, though.",
    "start": "2390626",
    "end": "2397240"
  },
  {
    "text": "All right. You wanted to find the\ngradient of the log likelihood",
    "start": "2397240",
    "end": "2404200"
  },
  {
    "text": "over all of the\nstate action pairs. And I think you have d of them.",
    "start": "2404200",
    "end": "2410410"
  },
  {
    "text": "But if this term is-- oh right. ",
    "start": "2410410",
    "end": "2418328"
  },
  {
    "text": "Oh, I see what you mean. ",
    "start": "2418328",
    "end": "2432550"
  },
  {
    "text": "OK, Cas, yes? Explaining why that\nis [INAUDIBLE]. Oh, why are we changing\nit to v depth minus 1?",
    "start": "2432550",
    "end": "2440440"
  },
  {
    "text": "So for example, in this\ncase where our depth is 10,",
    "start": "2440440",
    "end": "2445750"
  },
  {
    "text": "we only know up to the 10th\nstate in the trajectory, that's all we're remembering.",
    "start": "2445750",
    "end": "2450950"
  },
  {
    "text": "So if we went to\nd, then here, we would be looking at\n11, which is something",
    "start": "2450950",
    "end": "2457540"
  },
  {
    "text": "that we don't know because it's\nnot in our current trajectory. So we're stopping at 10, which\nwould be depth minus 1 plus 1",
    "start": "2457540",
    "end": "2465730"
  },
  {
    "text": "here.  Yes?",
    "start": "2465730",
    "end": "2470900"
  },
  {
    "text": "So then the derivative\nof theta, would we end up with not looking at the\nstate-- like the d state action",
    "start": "2470900",
    "end": "2478960"
  },
  {
    "text": "pair? When we take the\nderivative of theta, do we not end up looking\nat the d state action pair?",
    "start": "2478960",
    "end": "2487030"
  },
  {
    "text": "That last term,\nremaining term, we won't be looking at pi\ntheta of a, given SD.",
    "start": "2487030",
    "end": "2495581"
  },
  {
    "text": "Ah, yes. So the last term\nthat we're left with would only be going to\nd minus 1 in this case.",
    "start": "2495581",
    "end": "2504080"
  },
  {
    "text": "But like I said, we assume\nthat we know the trajectory beyond d or.",
    "start": "2504080",
    "end": "2512410"
  },
  {
    "text": "Yeah, I think there's\njust a little tricky thing for the transition model\nbecause it goes to plus 1.",
    "start": "2512410",
    "end": "2520440"
  },
  {
    "text": "You know, for the\nsecond summation, I think that can still\ngo too deep because you",
    "start": "2520440",
    "end": "2525547"
  },
  {
    "text": "know what that is. Yes. So I think the peak\ndata trajectory",
    "start": "2525547",
    "end": "2531150"
  },
  {
    "text": "should really be half of\nan extra term at the end. So that at the deep\nlevel, the transition",
    "start": "2531150",
    "end": "2538493"
  },
  {
    "text": "wouldn't matter\nbecause it doesn't matter if the d plus one state. It's just we don't\nconsider possibilities.",
    "start": "2538493",
    "end": "2546160"
  },
  {
    "text": "We don't care what it goes to. [INAUDIBLE]",
    "start": "2546160",
    "end": "2551515"
  },
  {
    "text": "Yeah.  Anything else?",
    "start": "2551515",
    "end": "2557045"
  },
  {
    "text": " All right, so that was the\nlast method that I had.",
    "start": "2557045",
    "end": "2565133"
  },
  {
    "text": "And you can see\nin the book, which also goes through the derivation\nfor the deterministic policy",
    "start": "2565133",
    "end": "2570230"
  },
  {
    "text": "case. And now, I will let\nKianna take over to talk about what we're going\nto do with these estimates.",
    "start": "2570230",
    "end": "2575880"
  },
  {
    "text": "Oh, yes. So if these are that, what is k? K is just the counter.",
    "start": "2575880",
    "end": "2581850"
  },
  {
    "text": "That's the current stuff\nthat we're looking at, yeah So a D equals [INAUDIBLE].",
    "start": "2581850",
    "end": "2588440"
  },
  {
    "text": "No, So k is stopping at\nD, so we would do k from 1 to d in this case.",
    "start": "2588440",
    "end": "2595110"
  },
  {
    "text": "So first, state and action and\nthen the second and so on all the way to DU would be\nincluded in the summation.",
    "start": "2595110",
    "end": "2604760"
  },
  {
    "text": "Yes? You will be taking an\nexpectation over [INAUDIBLE].",
    "start": "2604760",
    "end": "2611630"
  },
  {
    "text": "But the only part on this\nseem to be [INAUDIBLE]. Would you go ahead and integrate\nover several [INAUDIBLE] times",
    "start": "2611630",
    "end": "2622400"
  },
  {
    "text": "in practice? Would you iterate over\nseveral different trajectories in order to find this?",
    "start": "2622400",
    "end": "2629410"
  },
  {
    "text": "In order to find the\nexpectation of [INAUDIBLE]. Yes, in order to\nfind the expectation,",
    "start": "2629410",
    "end": "2635450"
  },
  {
    "text": "you need to consider multiple\npossible trajectories. ",
    "start": "2635450",
    "end": "2641300"
  },
  {
    "text": "Is it possible to look\nat each [INAUDIBLE] part",
    "start": "2641300",
    "end": "2648340"
  },
  {
    "text": "to find all of them? I'm sorry, could you\nplease repeat that.",
    "start": "2648340",
    "end": "2653420"
  },
  {
    "text": "I'm having trouble hearing. So what I'm saying is, for\nthe trajectories, in practice,",
    "start": "2653420",
    "end": "2662480"
  },
  {
    "text": "it might be hard to\niterate over all of them. So [INAUDIBLE].",
    "start": "2662480",
    "end": "2668830"
  },
  {
    "text": "Yeah, you would be sampling. And you can use\ndifferent heuristics for how you want to\nsample trajectories.",
    "start": "2668830",
    "end": "2675980"
  },
  {
    "text": "Some may be of more\nor less interest. Maybe it's more important\nto have information",
    "start": "2675980",
    "end": "2681069"
  },
  {
    "text": "about trajectories that we\nthink are likely or high risk, et cetera, but you would\nneed to be sampling.",
    "start": "2681070",
    "end": "2687290"
  },
  {
    "text": "We would expect\nthat in many cases, you wouldn't be able to\nenumerate all of them.",
    "start": "2687290",
    "end": "2693540"
  },
  {
    "text": "Yes? Oh, I just wanted to understand\nfrom a high level view, is we're just\ntrying to calculate",
    "start": "2693540",
    "end": "2699510"
  },
  {
    "text": "the gradient of the\nutility function. And then there's three different\nmethods with finite differences.",
    "start": "2699510",
    "end": "2707050"
  },
  {
    "text": "And then with this, we already\nknow it's then with this one. Yeah. So these are all\ndifferent methods",
    "start": "2707050",
    "end": "2713130"
  },
  {
    "text": "for estimating the gradient\nof the utility function so that we can optimize the\nparameters of our policy theta",
    "start": "2713130",
    "end": "2719640"
  },
  {
    "text": "with respect to utility. So we're assuming we're\nnot able to compute",
    "start": "2719640",
    "end": "2725490"
  },
  {
    "text": "that gradient directly. And then these are some\npossible options, yeah.",
    "start": "2725490",
    "end": "2734220"
  },
  {
    "text": "All right. I will show you more about what\nwe're going to do with this.",
    "start": "2734220",
    "end": "2739410"
  },
  {
    "text": "[STUDENTS CLAPPING] ",
    "start": "2739410",
    "end": "2747000"
  }
]