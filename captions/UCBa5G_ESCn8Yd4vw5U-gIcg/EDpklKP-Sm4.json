[
  {
    "start": "0",
    "end": "177000"
  },
  {
    "text": "thanks for the introduction uh great to see all of you so yes i'll tell you about some some of the things we've been",
    "start": "11120",
    "end": "16480"
  },
  {
    "text": "exploring think about how do we make sure that the algorithms that we deploy are responsible and uh you know",
    "start": "16480",
    "end": "23039"
  },
  {
    "text": "safe and reliable especially in biomedical context right where these algorithms can have quite sensitive",
    "start": "23039",
    "end": "29199"
  },
  {
    "text": "impact um so just in the way of backgrounds",
    "start": "29199",
    "end": "34880"
  },
  {
    "text": "right so a lot of the things that we're interested in here in my group at stanford is actually in both in building these algorithms and also in deploying",
    "start": "34880",
    "end": "42079"
  },
  {
    "text": "them both at stanford and elsewhere i just will give you a couple of examples um where we think you know the ai can",
    "start": "42079",
    "end": "48800"
  },
  {
    "text": "have really quite large impact in health care so this is like one system that we built um which is basically a computer",
    "start": "48800",
    "end": "55360"
  },
  {
    "text": "mission system for looking at these cardiac ultrasound videos like the ones shown here",
    "start": "55360",
    "end": "60480"
  },
  {
    "text": "so there are many of these like millions of these are routinely collected right and we",
    "start": "60480",
    "end": "65518"
  },
  {
    "text": "basically look at these videos the algorithms try to assess various cardiac conditions like heart diseases right",
    "start": "65519",
    "end": "70960"
  },
  {
    "text": "from these videos so now the paper was published about a couple years ago but now the last two",
    "start": "70960",
    "end": "76640"
  },
  {
    "text": "years we've actually been working quite hard on deploying this in the emergency department uh here's the setup that we",
    "start": "76640",
    "end": "82320"
  },
  {
    "text": "have at stanford so another place where you know we think",
    "start": "82320",
    "end": "89439"
  },
  {
    "text": "ai can be really useful especially in healthcare is in sort of telemedicine applications",
    "start": "89439",
    "end": "95040"
  },
  {
    "text": "right so as some of you might know especially during the pandemic the telemedicine",
    "start": "95040",
    "end": "100320"
  },
  {
    "text": "actually really exploded uh so for example at stanford the number of televisions actually",
    "start": "100320",
    "end": "105840"
  },
  {
    "text": "increased by 50 fold compared to over the last two years right so the idea of telemedicine is",
    "start": "105840",
    "end": "111520"
  },
  {
    "text": "that instead of having the patient come to the clinics right so you actually the patient will interact with the physician",
    "start": "111520",
    "end": "117040"
  },
  {
    "text": "without having to leave their home right but one big challenge is that you know how do you really ensure that the",
    "start": "117040",
    "end": "123040"
  },
  {
    "text": "doctors actually still get sufficient information from the patient even though even though they're not interacting with",
    "start": "123040",
    "end": "129039"
  },
  {
    "text": "the patients physically right you know in person so we developed this tool called true",
    "start": "129039",
    "end": "135120"
  },
  {
    "text": "image which is basically the idea is to help patients to take a really",
    "start": "135120",
    "end": "140560"
  },
  {
    "text": "high quality the clinically great images so that they can send those images then",
    "start": "140560",
    "end": "145599"
  },
  {
    "text": "to the doctors right so the doctor can make informed diagnosis and judgment from the images even though",
    "start": "145599",
    "end": "151120"
  },
  {
    "text": "they're not seeing the patient in person right and turns out that's you know one big challenge is that even though all of",
    "start": "151120",
    "end": "156560"
  },
  {
    "text": "us are very good at taking photos for instagram for facebook we're actually very bad at taking photos that are",
    "start": "156560",
    "end": "161760"
  },
  {
    "text": "clinically useful right so this is where we have an app that's being deployed now um that's trying to make this much",
    "start": "161760",
    "end": "168640"
  },
  {
    "text": "easier for the patients to take high quality photos",
    "start": "168640",
    "end": "172640"
  },
  {
    "text": "um another place where we're really actually deploying the algorithms is in",
    "start": "174480",
    "end": "179519"
  },
  {
    "start": "177000",
    "end": "507000"
  },
  {
    "text": "improving clinical trials right so clinical trials sort of like the big bottleneck that's the most",
    "start": "179519",
    "end": "185040"
  },
  {
    "text": "expensive stage of all of medicine so we built this tool basically to help people to design",
    "start": "185040",
    "end": "191440"
  },
  {
    "text": "clinical trials to make it more efficient and also make clinical trials more diverse and more inclusive",
    "start": "191440",
    "end": "196800"
  },
  {
    "text": "so this is now being used by roche which is the largest pharma company in the world to help design some of their new",
    "start": "196800",
    "end": "203200"
  },
  {
    "text": "cancer trials",
    "start": "203200",
    "end": "206000"
  },
  {
    "text": "so a lot of our interests along these lines is really in taking these algorithms right",
    "start": "210239",
    "end": "216000"
  },
  {
    "text": "that we develop a lot of research and actually really into practice into deployment because",
    "start": "216000",
    "end": "221040"
  },
  {
    "text": "that's where we have some of the biggest impacts right um so to motivate the rest of the",
    "start": "221040",
    "end": "227440"
  },
  {
    "text": "presentation i want to tell you about you know some of the really interesting challenges that we run into when we think about actually deploying these",
    "start": "227440",
    "end": "232879"
  },
  {
    "text": "tools and then this then leads into some of the more interesting sort of machine learning ai questions that i'll talk",
    "start": "232879",
    "end": "239280"
  },
  {
    "text": "about in the second half so just to give you a concrete example right um so i mentioned dermatology",
    "start": "239280",
    "end": "246400"
  },
  {
    "text": "whereas dermatology is actually one place where there's currently been some of the largest investments in ai for",
    "start": "246400",
    "end": "251599"
  },
  {
    "text": "healthcare and the reason is that like dermatology kinds of data is relatively easy to",
    "start": "251599",
    "end": "257040"
  },
  {
    "text": "collect right so if anyone obviously was the phone can take the kind of images like this right on your phone",
    "start": "257040",
    "end": "263919"
  },
  {
    "text": "and then the kinds of algorithms are basically you know running various components under the scene and",
    "start": "263919",
    "end": "269600"
  },
  {
    "text": "trying to diagnose like is this likely to be skin cancer or not right so in this case unfortunately this",
    "start": "269600",
    "end": "275759"
  },
  {
    "text": "is diagnosed if you like to be skin cancer and then you should go visit the dermatologist as soon as",
    "start": "275759",
    "end": "280840"
  },
  {
    "text": "possible right so the compelling the reason why dermatology is very compelling for ai is because first like",
    "start": "280840",
    "end": "287520"
  },
  {
    "text": "the data is readily accessible right and the second is that things like skin cancer",
    "start": "287520",
    "end": "293280"
  },
  {
    "text": "it's actually very treatable if you catch it early on but if you catch it too late then it's deadly right and",
    "start": "293280",
    "end": "298639"
  },
  {
    "text": "there's millions of people around the world every year who unfortunately have skin cancer but are",
    "start": "298639",
    "end": "304320"
  },
  {
    "text": "not diagnosed until it's too late so so that's very attractive for a place",
    "start": "304320",
    "end": "311600"
  },
  {
    "text": "to deploy ai right so it actually took the three of the state-of-the-art ai",
    "start": "311600",
    "end": "317440"
  },
  {
    "text": "systems including some developed by commercial vendors um and we wanted to sort our pilot them",
    "start": "317440",
    "end": "322560"
  },
  {
    "text": "and test them here to see can we actually use them for stanford patients right",
    "start": "322560",
    "end": "328960"
  },
  {
    "text": "so we took these algorithms so they will have like very good performance um like if for example if you look at the",
    "start": "328960",
    "end": "334960"
  },
  {
    "text": "original papers or the original studies of these algorithms like there are auc's that will be above 0.9",
    "start": "334960",
    "end": "341520"
  },
  {
    "text": "right so the first thing that we do is okay so we just take these algorithms out of the box and then apply them to",
    "start": "341520",
    "end": "346560"
  },
  {
    "text": "like real stanford patients right and then the performance is actually really poor",
    "start": "346560",
    "end": "352479"
  },
  {
    "text": "right so the auc actually dropped from point nine to a point six it's not uh and just to be clear right these are",
    "start": "352479",
    "end": "358560"
  },
  {
    "text": "just real images right from real patients at stanford there's no",
    "start": "358560",
    "end": "364479"
  },
  {
    "text": "perturbations no adversarial attacks to these images",
    "start": "364479",
    "end": "369120"
  },
  {
    "text": "so this is actually like a really big challenge right so the first question right is like okay so what",
    "start": "369919",
    "end": "376080"
  },
  {
    "text": "happened here so how do we understand why does the algorithms drop off so much and then second how do we mitigate this",
    "start": "376080",
    "end": "383199"
  },
  {
    "text": "going forward right so i so we did a",
    "start": "383199",
    "end": "389199"
  },
  {
    "text": "systematic audit to explain actually what happened to these algorithms but before i tell you what happened",
    "start": "389199",
    "end": "395199"
  },
  {
    "text": "do people have any guesses um like what why do you think the algorithms performance dropped off so much when",
    "start": "395199",
    "end": "400880"
  },
  {
    "text": "they applied when they were applied to stanford patients",
    "start": "400880",
    "end": "405919"
  },
  {
    "text": "yeah um i have two thoughts in mind one could be just the there's differences in the instrument",
    "start": "407280",
    "end": "413360"
  },
  {
    "text": "that's being used to capture the image or it's just the patients are different in terms of like maybe the stanford",
    "start": "413360",
    "end": "419440"
  },
  {
    "text": "patient who has like higher ratio diversity than the patient group that was used to actually",
    "start": "419440",
    "end": "424560"
  },
  {
    "text": "develop the model or something like that okay so there could be some differences maybe either in the types of phones that",
    "start": "424560",
    "end": "429599"
  },
  {
    "text": "were used or in the patient populations these are good ideas yes did you have a oh no okay",
    "start": "429599",
    "end": "437840"
  },
  {
    "text": "other guesses other",
    "start": "437840",
    "end": "442160"
  },
  {
    "text": "ideas you think those would explain most of the difference or or do you think there are other factors",
    "start": "448840",
    "end": "456800"
  },
  {
    "text": "maybe the way that the way that the images were captured like the technique that was used okay yeah so maybe even if",
    "start": "459520",
    "end": "466080"
  },
  {
    "text": "they're using the same equipment like how they were taking it okay so there's actually",
    "start": "466080",
    "end": "472160"
  },
  {
    "text": "a really big one that's missing or a few big ones",
    "start": "472160",
    "end": "477599"
  },
  {
    "text": "what do people think yeah maybe they were just not honest about their data set like maybe they like trained on guys",
    "start": "477680",
    "end": "483520"
  },
  {
    "text": "or something okay so maybe there are some issues in the original results that were reported",
    "start": "483520",
    "end": "488800"
  },
  {
    "text": "so that's interesting",
    "start": "488800",
    "end": "492198"
  },
  {
    "text": "okay yes these are all very good ideas so okay so we did actually an audit this is like a detective story right so you",
    "start": "493840",
    "end": "499759"
  },
  {
    "text": "know when you figure out what actually happened here um no so we actually did a careful study right um so",
    "start": "499759",
    "end": "506960"
  },
  {
    "text": "and again like the goal of this detective story is that we're gonna figure out like what actually",
    "start": "506960",
    "end": "512399"
  },
  {
    "start": "507000",
    "end": "890000"
  },
  {
    "text": "led to this big drop off right from the point nine to point six",
    "start": "512399",
    "end": "518320"
  },
  {
    "text": "right so can we understand actually what led to those mistakes so it turned out actually the first",
    "start": "518320",
    "end": "525440"
  },
  {
    "text": "um maybe the biggest reason right what there's actually quite a huge drop-off",
    "start": "525440",
    "end": "531519"
  },
  {
    "text": "in the performance was that there were issues in the original test data sets",
    "start": "531519",
    "end": "536720"
  },
  {
    "text": "right um more specifically right so the original",
    "start": "536720",
    "end": "542240"
  },
  {
    "text": "test data sets for these algorithms so how were they collected so it came from having you have these images then you have",
    "start": "542240",
    "end": "548320"
  },
  {
    "text": "dermatologists as you look at each image and say is this skin cancer or not skin cancer right so from visual inspections by",
    "start": "548320",
    "end": "554959"
  },
  {
    "text": "dermatologists which is reasonable because it's easy to collect quickly but you can imagine even having",
    "start": "554959",
    "end": "561600"
  },
  {
    "text": "experienced dermatologists visually looking at the image they also make mistakes right so the actual ground truth comes",
    "start": "561600",
    "end": "568160"
  },
  {
    "text": "from you know you actually take a biopsy of this legion and then you do a pathology test to save the skin cancer",
    "start": "568160",
    "end": "573279"
  },
  {
    "text": "or not and so that's actually the ground truth that we have here at stanford from the hospitals",
    "start": "573279",
    "end": "578480"
  },
  {
    "text": "so if you actually convert the actual ground truth to what the original test data so there were actually a lot of",
    "start": "578480",
    "end": "583760"
  },
  {
    "text": "label mistakes in the original test data right so just miss annotations",
    "start": "583760",
    "end": "588959"
  },
  {
    "text": "right which is not something that we typically think so much about when we develop machine learning systems so we typically assume that maybe the test",
    "start": "588959",
    "end": "594880"
  },
  {
    "text": "data should be pretty good right um but in actually in a lot of these real world applications like the",
    "start": "594880",
    "end": "601440"
  },
  {
    "text": "test data even the labels can have a lot of biases and mistakes right so in this case actually explains",
    "start": "601440",
    "end": "607680"
  },
  {
    "text": "quite a big drop of the model so the second big factor which maybe",
    "start": "607680",
    "end": "613839"
  },
  {
    "text": "accounts for about a third of the models drop off which something i think people here mention is that there's also a distribution in type of patients",
    "start": "613839",
    "end": "621040"
  },
  {
    "text": "right so it turns out that in the original data sets that are used to evaluate these algorithms they are mostly common skin cancers right again",
    "start": "621040",
    "end": "628399"
  },
  {
    "text": "easy to collect data here at stanford we have both common and also less common diseases people come to",
    "start": "628399",
    "end": "634160"
  },
  {
    "text": "the hospitals for a variety of reasons right and the models did worse on these less common diseases right that does",
    "start": "634160",
    "end": "640720"
  },
  {
    "text": "also contribute to some of the drop off",
    "start": "640720",
    "end": "645160"
  },
  {
    "text": "so the third one um is that actually it turned out that the algorithms um were quite poor",
    "start": "646000",
    "end": "655040"
  },
  {
    "text": "when applied to images taken from dark skinned patients",
    "start": "655040",
    "end": "660560"
  },
  {
    "text": "right so for example especially if you look at how sensitive the algorithms are in detecting skin cancer the sensitivity",
    "start": "660560",
    "end": "665760"
  },
  {
    "text": "here means that if the patient actually has skin cancer this alcohol is going to say that they have skin cancers right so",
    "start": "665760",
    "end": "671120"
  },
  {
    "text": "that's really important the sensitivity is very low if the patient is uh has dark skins",
    "start": "671120",
    "end": "677680"
  },
  {
    "text": "right and when we dug deeper into this it turned out that actually the original test data and also the training data",
    "start": "677680",
    "end": "682959"
  },
  {
    "text": "this had very few and in some cases like zero image from dark skinned patients or",
    "start": "682959",
    "end": "688320"
  },
  {
    "text": "users okay so i guess you know one big",
    "start": "688320",
    "end": "695279"
  },
  {
    "text": "takeaway lesson from this is that uh when we think about actually deploying machine learning in practice right so",
    "start": "695279",
    "end": "701600"
  },
  {
    "text": "often times just looking at to the aggregate metrics like auc by itself is",
    "start": "701600",
    "end": "706640"
  },
  {
    "text": "really meaningless unless we really have a context of what is actually the data that goes behind this this uh no that",
    "start": "706640",
    "end": "714160"
  },
  {
    "text": "gave this metric right and that motivates a lot of things that i'll talk about later today is how do we",
    "start": "714160",
    "end": "720320"
  },
  {
    "text": "really audit these data sets or how do we provide that context and how do we",
    "start": "720320",
    "end": "725440"
  },
  {
    "text": "really understand why models make these mistakes in a way that's sort of systematic and you know",
    "start": "725440",
    "end": "732079"
  },
  {
    "text": "and comprehensive any questions so far",
    "start": "732079",
    "end": "739720"
  },
  {
    "text": "okay so the problem i think it's not just for the dermatology",
    "start": "747600",
    "end": "752880"
  },
  {
    "text": "right so you know in some other recent works we actually had uh done a similar kind of audits",
    "start": "752880",
    "end": "759519"
  },
  {
    "text": "of uh all of the algorithms that were approved by the fda right all the medical ai algorithms",
    "start": "759519",
    "end": "765680"
  },
  {
    "text": "uh so fda sort of like the sort of like the gold standard the governing body of one is a drug or vaccine or algorithms",
    "start": "765680",
    "end": "773600"
  },
  {
    "text": "in use on patients right uh so there's a velocity there are over 100 of medical ai systems",
    "start": "773600",
    "end": "780160"
  },
  {
    "text": "algorithms that were approved by the fda to be used on patients so each symbol here shows like one of",
    "start": "780160",
    "end": "785279"
  },
  {
    "text": "these algorithms and you know i just stratified them by which body part that they apply to",
    "start": "785279",
    "end": "792480"
  },
  {
    "text": "so we did like a similar kind of audit for each of these algorithms so there's a bunch of things maybe i just want to",
    "start": "792480",
    "end": "798320"
  },
  {
    "text": "let you just focus on two of the the most salient kind of features one is the color",
    "start": "798320",
    "end": "803839"
  },
  {
    "text": "right so i colored each of these algorithms blue right if we actually had",
    "start": "803839",
    "end": "810560"
  },
  {
    "text": "reports of the evaluation performance across multiple locations right otherwise it's colored gray",
    "start": "810560",
    "end": "818160"
  },
  {
    "text": "so you know it's already visually pretty clear that most of these systems right so over 90 of the 130",
    "start": "818160",
    "end": "824560"
  },
  {
    "text": "we did not actually find how well it worked if evaluation performance across multiple locations across multiple hospitals",
    "start": "824560",
    "end": "831040"
  },
  {
    "text": "right so you only see how it works at one location and the second is that the only four of",
    "start": "831040",
    "end": "836880"
  },
  {
    "text": "these 130 were evaluated using more prospective or like human in the loop",
    "start": "836880",
    "end": "842320"
  },
  {
    "text": "evaluation right so the remaining ones were all evaluated retrospectively so retrospectively here means that somebody",
    "start": "842320",
    "end": "848880"
  },
  {
    "text": "collected the data ahead of time right so it's like a benchmark data set it could be from the same hospital where",
    "start": "848880",
    "end": "854079"
  },
  {
    "text": "they were trained right and then they just developed the algorithm and applied that to that benchmark data set",
    "start": "854079",
    "end": "859920"
  },
  {
    "text": "right so that was the evidence that was presented for the fda approval",
    "start": "859920",
    "end": "865920"
  },
  {
    "text": "right so as we saw from the dermatology example right so if you're only if that's the kinds of evaluations that",
    "start": "867680",
    "end": "872880"
  },
  {
    "text": "we're doing then that can potentially mask a lot of potential limitations and biases in these models right if you're",
    "start": "872880",
    "end": "879279"
  },
  {
    "text": "only evaluating it retrospectively at one location",
    "start": "879279",
    "end": "884519"
  },
  {
    "start": "890000",
    "end": "1016000"
  },
  {
    "text": "um you know so we spell out some other things ai models but we also look at",
    "start": "890399",
    "end": "895839"
  },
  {
    "text": "other non-medical ai systems right i know many of you have probably played around with gpt type models or",
    "start": "895839",
    "end": "902480"
  },
  {
    "text": "these language models um and we also do these kind of audits and they also",
    "start": "902480",
    "end": "908320"
  },
  {
    "text": "find that they also have quite some different problems and biases",
    "start": "908320",
    "end": "913680"
  },
  {
    "text": "and mistakes right so here i just show you like this sort of a video of one of my students actually interacting with",
    "start": "913680",
    "end": "920160"
  },
  {
    "text": "gpt uh so with how these models works you can just type in a prompt right then the rest of the",
    "start": "920160",
    "end": "925839"
  },
  {
    "text": "essay will be where the paragraph will be automatically generated by the algorithm so we start with the prompt",
    "start": "925839",
    "end": "931360"
  },
  {
    "text": "the two muslims",
    "start": "931360",
    "end": "934399"
  },
  {
    "text": "right so the rest of this is generated by the algorithm right now you can see that give you a second to read but they",
    "start": "936959",
    "end": "942560"
  },
  {
    "text": "can say that so it's mostly talking about like bombing and violence right so this is just",
    "start": "942560",
    "end": "948240"
  },
  {
    "text": "immediately generated by the algorithm so we can change the prompt",
    "start": "948240",
    "end": "954639"
  },
  {
    "text": "right uh it would produce something that's a different output but we'll still have very strong",
    "start": "954639",
    "end": "961920"
  },
  {
    "text": "violence associated a very violent content",
    "start": "961920",
    "end": "967079"
  },
  {
    "text": "so my students you know abu i changed other different problems about walking to mosque",
    "start": "971360",
    "end": "976720"
  },
  {
    "text": "i generated a different story but very violent still and finally you know was fed up and then",
    "start": "976720",
    "end": "983120"
  },
  {
    "text": "he just wrote explicitly that they walked in to worship peacefully um but i guess the gpt was not really",
    "start": "983120",
    "end": "990399"
  },
  {
    "text": "deterred by that and said that you know they walked in but they were shot for their faith right",
    "start": "990399",
    "end": "997199"
  },
  {
    "text": "um so you can see that's like so these are could be quite problematic if if these",
    "start": "997199",
    "end": "1002480"
  },
  {
    "text": "are the models that are used as basis for for digital applications like chat bots",
    "start": "1002480",
    "end": "1009040"
  },
  {
    "text": "or conversational systems",
    "start": "1009040",
    "end": "1012399"
  },
  {
    "start": "1016000",
    "end": "1190000"
  },
  {
    "text": "okay and turn out this is actually quite systematic for these kind of potential",
    "start": "1016480",
    "end": "1023199"
  },
  {
    "text": "biases right so we actually designed a bunch of tests and probes to just see okay so for different types of different",
    "start": "1023199",
    "end": "1029438"
  },
  {
    "text": "groups right they have muslims buddhists atheists so how much of the content generated by gpt",
    "start": "1029439",
    "end": "1035280"
  },
  {
    "text": "actually has surveillance associated with it right um and consistently right so maybe",
    "start": "1035280",
    "end": "1042079"
  },
  {
    "text": "so quite shockingly like over 60 percent of the content generated by gpt associate from these muslims actually",
    "start": "1042079",
    "end": "1048319"
  },
  {
    "text": "have some sort of notions of violence in those content",
    "start": "1048319",
    "end": "1053400"
  },
  {
    "text": "okay so hopefully this example sort of motivates the rest of the discussion which is uh",
    "start": "1056799",
    "end": "1063280"
  },
  {
    "text": "you know given that we're trying to actually use and deploy these systems right in practice",
    "start": "1063280",
    "end": "1068640"
  },
  {
    "text": "um how do we really rigorously evaluate and how do we monitor these ai",
    "start": "1068640",
    "end": "1074080"
  },
  {
    "text": "systems right i think that's really one of the big challenges right it's the shift the challenge is sort of shifting",
    "start": "1074080",
    "end": "1080320"
  },
  {
    "text": "from how to train and build these models right which is what people have been focusing on before too now if we",
    "start": "1080320",
    "end": "1086400"
  },
  {
    "text": "actually have these models and want to use them in practice right how do you then make sure that they're responsible and",
    "start": "1086400",
    "end": "1091919"
  },
  {
    "text": "reliable and how do you really test them right so that's really i think really a",
    "start": "1091919",
    "end": "1096960"
  },
  {
    "text": "really important area of open research uh it's often challenged so especially for",
    "start": "1096960",
    "end": "1102400"
  },
  {
    "text": "students here who are interested in new research areas that are very impactful i think this is really some of the the",
    "start": "1102400",
    "end": "1108400"
  },
  {
    "text": "biggest impact is uh to be gained um and today i'll tell you about some of",
    "start": "1108400",
    "end": "1115440"
  },
  {
    "text": "our thoughts in in these directions um most uh split into two parts one is on",
    "start": "1115440",
    "end": "1121919"
  },
  {
    "text": "the data side and that's more on the model side all right for the data we think about okay so how do we try to",
    "start": "1121919",
    "end": "1127120"
  },
  {
    "text": "audit these data sets in a way that's sort of systematic um and then on the model side",
    "start": "1127120",
    "end": "1133280"
  },
  {
    "text": "there i'm really interested in trying to understand using natural language right like how",
    "start": "1133280",
    "end": "1138559"
  },
  {
    "text": "and why these models make specific mistakes",
    "start": "1138559",
    "end": "1142799"
  },
  {
    "text": "let's pause and see if people have any questions before i move on",
    "start": "1146000",
    "end": "1151640"
  },
  {
    "text": "do you know the gpt responses were like memorized from the training set or",
    "start": "1152880",
    "end": "1159280"
  },
  {
    "text": "yeah it's a good question um we don't think it's like exact memorization",
    "start": "1159280",
    "end": "1164559"
  },
  {
    "text": "um if they do like exact matching it's not memorization but it could be that",
    "start": "1164559",
    "end": "1170559"
  },
  {
    "text": "there's it's definitely quite reasonable that there maybe there's some parts of the training data that heavily influence the",
    "start": "1170559",
    "end": "1176880"
  },
  {
    "text": "model to make those",
    "start": "1176880",
    "end": "1179840"
  },
  {
    "text": "okay okay so",
    "start": "1187200",
    "end": "1194320"
  },
  {
    "start": "1190000",
    "end": "1736000"
  },
  {
    "text": "data audits right um first it's actually quite i found it",
    "start": "1194320",
    "end": "1200240"
  },
  {
    "text": "quite interesting just to see like what data do people use in these real world uh applications",
    "start": "1200240",
    "end": "1207760"
  },
  {
    "text": "all right so here we actually looked at just you know a particular application",
    "start": "1207760",
    "end": "1213120"
  },
  {
    "text": "domain which is this dermatology ai we can and we have done sort of similar kind of analysis for other application",
    "start": "1213120",
    "end": "1219360"
  },
  {
    "text": "domains i'll just show you this one example since so far sort of our running example here right so here what we do is basically",
    "start": "1219360",
    "end": "1225919"
  },
  {
    "text": "actually look at all of the dermatology ai models that were published",
    "start": "1225919",
    "end": "1231679"
  },
  {
    "text": "right and then for each of those models you can go to the papers and let's kind of scrape and understand like what was the data that's used to train those",
    "start": "1231679",
    "end": "1237919"
  },
  {
    "text": "models so this is our summarizing of that right um each square here corresponds to like",
    "start": "1237919",
    "end": "1244880"
  },
  {
    "text": "one of these models or one of the papers right and then the circles basically corresponds to different data sets",
    "start": "1244880",
    "end": "1251919"
  },
  {
    "text": "right the size of the circle indicates like how big the data set is and then the circle is attached to",
    "start": "1251919",
    "end": "1257760"
  },
  {
    "text": "square if that's you know if that model is trained or evaluated on that data set",
    "start": "1257760",
    "end": "1263840"
  },
  {
    "text": "and there are also two kind of colors right so the red ones are basically private data sets by privateers mean",
    "start": "1263840",
    "end": "1269840"
  },
  {
    "text": "that you know somebody actually credited this data set but nobody else can actually access it or",
    "start": "1269840",
    "end": "1274880"
  },
  {
    "text": "figure out what's in the data right and then the blue ones are the public ones that you know the public",
    "start": "1274880",
    "end": "1280080"
  },
  {
    "text": "benchmarks that people could use all right so for example you see that",
    "start": "1280080",
    "end": "1285200"
  },
  {
    "text": "there's a couple of public data sets like this one here that's used as pretty",
    "start": "1285200",
    "end": "1290320"
  },
  {
    "text": "common benchmarks across a lot of these models um but it's also quite interesting that there's about like half of these models",
    "start": "1290320",
    "end": "1296799"
  },
  {
    "text": "like all these ones on top that are really",
    "start": "1296799",
    "end": "1301840"
  },
  {
    "text": "tested and trained on like pretty small and non like private data sets",
    "start": "1301840",
    "end": "1309760"
  },
  {
    "text": "right okay and then no and the quality of those data could also be quite heterogeneous",
    "start": "1313760",
    "end": "1319280"
  },
  {
    "text": "right and so there could and that limitation could potentially",
    "start": "1319280",
    "end": "1325440"
  },
  {
    "text": "lead to some of these issues that we saw before",
    "start": "1325440",
    "end": "1330760"
  },
  {
    "text": "yes there's a lot of the diversity here from um like different pathologies or like looking at different diseases or",
    "start": "1333600",
    "end": "1339440"
  },
  {
    "text": "are they all looking at similar diseases and just getting data from yeah it's a good question i think",
    "start": "1339440",
    "end": "1344799"
  },
  {
    "text": "actually many of them many of these algorithms are trying to predict pretty similar diseases right so there's",
    "start": "1344799",
    "end": "1351760"
  },
  {
    "text": "the biggest one is like a high level is a skin cancer or benign and within skin cancer there's a",
    "start": "1351760",
    "end": "1357760"
  },
  {
    "text": "different subcategories but most of them are actually looking at pretty similar tasks",
    "start": "1357760",
    "end": "1362799"
  },
  {
    "text": "so despite the fact that they're working on relatively similar tasks there's very fractured landscape of what",
    "start": "1362799",
    "end": "1369679"
  },
  {
    "text": "data and what and what heterogeneous sources of data are used to eval to train these",
    "start": "1369679",
    "end": "1375200"
  },
  {
    "text": "models when the data is private is it often like proprietary to like a company an",
    "start": "1375200",
    "end": "1382320"
  },
  {
    "text": "industry or like a hospital or is it just that researchers collected it on their own and just didn't release it",
    "start": "1382320",
    "end": "1388640"
  },
  {
    "text": "yeah so some of these privatizations could be proprietary to specific companies um and there are others that",
    "start": "1388640",
    "end": "1394320"
  },
  {
    "text": "are also for priority that are collected by academic groups but the academic groups uh did not want to share the data",
    "start": "1394320",
    "end": "1403799"
  },
  {
    "text": "yeah so good questions okay so in the face of now you have these these different heterogeneous data",
    "start": "1407520",
    "end": "1412720"
  },
  {
    "text": "and hydrogenated sources of data right so the i guess the high level the mathematical question we're interested in is okay how",
    "start": "1412720",
    "end": "1419600"
  },
  {
    "text": "can we come up with a a holistic framework to audit these different data sets",
    "start": "1419600",
    "end": "1424640"
  },
  {
    "text": "right and what do i mean by audits like how do we even define that so uh so here's what you know like the",
    "start": "1424640",
    "end": "1432000"
  },
  {
    "text": "definition that we proposed in terms of how to audit these data right so if you think about most of these machine learning settings",
    "start": "1432000",
    "end": "1438000"
  },
  {
    "text": "right um ai applications where you typically have your data that could come from different sources right so each of",
    "start": "1438000",
    "end": "1443760"
  },
  {
    "text": "the symbols here could be maybe a data from different hospital or different type of type of patients",
    "start": "1443760",
    "end": "1449200"
  },
  {
    "text": "and you have your favorite learning algorithm right where algorithm is agnostic or you could be using neural",
    "start": "1449200",
    "end": "1454640"
  },
  {
    "text": "networks or xg boost your favorite model right and you have some deployments",
    "start": "1454640",
    "end": "1461440"
  },
  {
    "text": "performance that you care about right which could be again we're pretty flexible it could be your",
    "start": "1461440",
    "end": "1466880"
  },
  {
    "text": "accuracy or f1 score or r squared if you're doing regression right so you get to pick whatever metric you care about",
    "start": "1466880",
    "end": "1473200"
  },
  {
    "text": "in deployment or revenue right um",
    "start": "1473200",
    "end": "1478400"
  },
  {
    "text": "so when i say i want to have like a fully end-to-end data audit what i mean is",
    "start": "1478400",
    "end": "1483520"
  },
  {
    "text": "that okay by the model in deployment achieves a certain level of performance or maybe it's getting like 80 accuracy",
    "start": "1483520",
    "end": "1490880"
  },
  {
    "text": "right so i'd like to understand how did the individual sources of the training data",
    "start": "1490880",
    "end": "1496320"
  },
  {
    "text": "actually contribute to that 80 accuracy right so that corresponds to mathematically you know taking that 80 percent accuracy and attributing and",
    "start": "1496320",
    "end": "1502720"
  },
  {
    "text": "partitioning that back to the individual data sources or the individual data points",
    "start": "1502720",
    "end": "1510000"
  },
  {
    "text": "right and similarly if the model actually exhibits make some mistakes or how except some biases in deployment",
    "start": "1510000",
    "end": "1516960"
  },
  {
    "text": "right i would also like to understand which sources or which data points actually are responsible or contributing",
    "start": "1516960",
    "end": "1523039"
  },
  {
    "text": "to the model to make that bias right so i guess you know for the gpt example i want to say okay so is this",
    "start": "1523039",
    "end": "1528960"
  },
  {
    "text": "particular type of sources of text right from this particular newspaper that led to the model to",
    "start": "1528960",
    "end": "1534720"
  },
  {
    "text": "the influence model to make this uh violent associations with muslims",
    "start": "1534720",
    "end": "1541480"
  },
  {
    "text": "make sense",
    "start": "1542640",
    "end": "1545120"
  },
  {
    "text": "okay so okay so how do we think about doing this right so maybe the",
    "start": "1547840",
    "end": "1552880"
  },
  {
    "text": "the first idea or natural idea that comes to mind would be okay if i want to figure out what is the",
    "start": "1552880",
    "end": "1559520"
  },
  {
    "text": "impact of a particular data point or data source i mean the star one in the middle right",
    "start": "1559520",
    "end": "1564799"
  },
  {
    "text": "the the simple thing is okay why do i just see how much does my model's behavior",
    "start": "1564799",
    "end": "1570240"
  },
  {
    "text": "change if that data point is not present right so you know in statistics it's often",
    "start": "1570240",
    "end": "1576400"
  },
  {
    "text": "called like the level and outs or leverage score kinds of analysis right it's also called influence analysis of",
    "start": "1576400",
    "end": "1581840"
  },
  {
    "text": "the influence of individual data points um but the idea here is very simple you just okay you think okay if i remove the",
    "start": "1581840",
    "end": "1588720"
  },
  {
    "text": "data points right now i can trim my models all these data points and see if it's decision boundary",
    "start": "1588720",
    "end": "1594880"
  },
  {
    "text": "but if i remove this data point i can see the decision boundary changing right i can quantify how much does that change",
    "start": "1594880",
    "end": "1600080"
  },
  {
    "text": "affect the model's performance right by my chosen metric of interest and then that",
    "start": "1600080",
    "end": "1605679"
  },
  {
    "text": "would be like the lever and out score right or the value for that data point",
    "start": "1605679",
    "end": "1613480"
  },
  {
    "text": "so so this is pretty widely used for actually for for quite many decades and",
    "start": "1614480",
    "end": "1620480"
  },
  {
    "text": "one challenge with this is that we think about using this for modern machine learning applications when you",
    "start": "1620480",
    "end": "1626240"
  },
  {
    "text": "have quite large data sets right of millions hundreds thousands of data points then",
    "start": "1626240",
    "end": "1631520"
  },
  {
    "text": "it's just extremely noisy right if you have like 100 000 data points right the difference from this like one data",
    "start": "1631520",
    "end": "1638080"
  },
  {
    "text": "point is going to be extremely small right because even if i remove it i still have like a hundred thousand other data points and my model has all sorts",
    "start": "1638080",
    "end": "1644960"
  },
  {
    "text": "of intrinsic stochasticity because i'm training with sgd and all these other things",
    "start": "1644960",
    "end": "1650080"
  },
  {
    "text": "right so it's actually if i just do leave one out then we're not going to really get any signal to see which data",
    "start": "1650080",
    "end": "1655760"
  },
  {
    "text": "points are more important",
    "start": "1655760",
    "end": "1658880"
  },
  {
    "text": "um so the idea that we proposed is to so for an extension of this is this idea",
    "start": "1661200",
    "end": "1666960"
  },
  {
    "text": "called the data shaft scores right um and the idea is quite intuitive right so okay instead of doing this d1",
    "start": "1666960",
    "end": "1674000"
  },
  {
    "text": "out once right so let's actually do it a bunch of different times across different contexts right different",
    "start": "1674000",
    "end": "1679520"
  },
  {
    "text": "scenarios so each scenario actually corresponds to a different random subset of my data",
    "start": "1679520",
    "end": "1685039"
  },
  {
    "text": "right and i can see okay if i take a model now if i take some random subsets of my data then what is the impact of",
    "start": "1685039",
    "end": "1691120"
  },
  {
    "text": "adding or removing this particular data point of interest from that subset right",
    "start": "1691120",
    "end": "1696720"
  },
  {
    "text": "and intuitively like the benefit from doing this right is now that i'm actually aggregating across many different",
    "start": "1696720",
    "end": "1703120"
  },
  {
    "text": "scenarios right the impact of this data point which makes it much more statistically robust right so it sort of",
    "start": "1703120",
    "end": "1709600"
  },
  {
    "text": "increases the signal to noise ratio and because i'm looking at across these different random subsets this also gives",
    "start": "1709600",
    "end": "1714960"
  },
  {
    "text": "me a way to estimate sort of disentangle the different interactions between data points",
    "start": "1714960",
    "end": "1722240"
  },
  {
    "text": "so we do this now across basically let's leave one out across different scenarios and then there's a particular way to",
    "start": "1722240",
    "end": "1728320"
  },
  {
    "text": "aggregate all of these scores together across these different scenarios into a single value that we call the data shaft",
    "start": "1728320",
    "end": "1733520"
  },
  {
    "text": "score for each data point right and then since these are written down in formula but basically what i",
    "start": "1733520",
    "end": "1739679"
  },
  {
    "start": "1736000",
    "end": "1747000"
  },
  {
    "text": "said in words you know it's the shaft values or the weighted average of this impact across different scenarios",
    "start": "1739679",
    "end": "1746799"
  },
  {
    "start": "1747000",
    "end": "1855000"
  },
  {
    "text": "so i'll just show you a few example applications of how this actually works in practice right so just going back to",
    "start": "1747600",
    "end": "1753360"
  },
  {
    "text": "our running example was this dermatology example right so the training data as we saw right could be very noisy it can",
    "start": "1753360",
    "end": "1759360"
  },
  {
    "text": "come from all these heterogeneous sources and who knows what how people collected them right",
    "start": "1759360",
    "end": "1764480"
  },
  {
    "text": "since here are some the actual examples from the real data sets right so it's very noisy",
    "start": "1764480",
    "end": "1770000"
  },
  {
    "text": "and then the shaft score actually just computes a score right for individual data points",
    "start": "1770000",
    "end": "1775360"
  },
  {
    "text": "right it's a number so the score could be negative and that suggests that that",
    "start": "1775360",
    "end": "1780399"
  },
  {
    "text": "image is could be misannotated or it's contributing to some sort of noise or bias in the model when it's being",
    "start": "1780399",
    "end": "1786000"
  },
  {
    "text": "deployed if the model is trained on that data point score is positive right suggests that",
    "start": "1786000",
    "end": "1791760"
  },
  {
    "text": "now that image is actually informative right and the model by training on that image actually does better right",
    "start": "1791760",
    "end": "1799520"
  },
  {
    "text": "in in deployment and then a simple thing that we can do",
    "start": "1799520",
    "end": "1805760"
  },
  {
    "text": "right is simply just to weight my data sets by the shaftly scores right so this and then just retrain the",
    "start": "1805760",
    "end": "1812000"
  },
  {
    "text": "model on this weighted data set so this has the effect of encouraging the model to pay more attention to data points",
    "start": "1812000",
    "end": "1817120"
  },
  {
    "text": "that we think are more informative or more diverse or cleaner data points right now",
    "start": "1817120",
    "end": "1823679"
  },
  {
    "text": "so down wait and remove the impact from having all these noisy data points right and if you do this then and right",
    "start": "1823679",
    "end": "1830799"
  },
  {
    "text": "so this serve substantially improves the model's performance in deployment and the benefit of this is that now we",
    "start": "1830799",
    "end": "1837440"
  },
  {
    "text": "actually just didn't have to collect any additional new data right so this is actually the same architecture and the",
    "start": "1837440",
    "end": "1843120"
  },
  {
    "text": "same data set i had before but we're just asking training the model on this weighted version of the data sets rather",
    "start": "1843120",
    "end": "1848640"
  },
  {
    "text": "than just treating every point in the uniform way",
    "start": "1848640",
    "end": "1853840"
  },
  {
    "start": "1855000",
    "end": "1943000"
  },
  {
    "text": "and also interesting to know like what are the data points that actually have the negative sharply scores",
    "start": "1855360",
    "end": "1860640"
  },
  {
    "text": "right so so we took actually one of these large databases um it's basically a database of chess x-rays",
    "start": "1860640",
    "end": "1867679"
  },
  {
    "text": "um and we computed this right so you can as we when we computed this you found a bunch of images that have negative",
    "start": "1867679",
    "end": "1872799"
  },
  {
    "text": "sharpie scores well i actually took each of those image and then gone back to have like a panel",
    "start": "1872799",
    "end": "1878559"
  },
  {
    "text": "of three experienced radiologists have them reassess each of these images the ones",
    "start": "1878559",
    "end": "1884000"
  },
  {
    "text": "that have negative scores um and it turned out that when you actually have people to really take a",
    "start": "1884000",
    "end": "1889840"
  },
  {
    "text": "closer and careful look at those negative valued negative images",
    "start": "1889840",
    "end": "1895760"
  },
  {
    "text": "um a large number of them i think about 60 percent of the images were actually misannotated",
    "start": "1895760",
    "end": "1902240"
  },
  {
    "text": "right so like the disease annotation they were given is wrong but in the original medical records",
    "start": "1902240",
    "end": "1909919"
  },
  {
    "text": "right and that's actually why if you train the model on those images then that can lead to all sorts of biases and",
    "start": "1909919",
    "end": "1914960"
  },
  {
    "text": "issues in deployment right and and so basically the shelvey scores",
    "start": "1914960",
    "end": "1921279"
  },
  {
    "text": "are quite effective in identifying these mis-annotated data points whereas if i just simply use like",
    "start": "1921279",
    "end": "1926720"
  },
  {
    "text": "default kinds of scores which is the green line here then that's really no better than just doing like a random",
    "start": "1926720",
    "end": "1933120"
  },
  {
    "text": "search through my data sets",
    "start": "1933120",
    "end": "1936240"
  },
  {
    "start": "1943000",
    "end": "2004000"
  },
  {
    "text": "yeah and we've also showed that you can apply this set of ideas as data sharply sports to other",
    "start": "1943200",
    "end": "1948559"
  },
  {
    "text": "applications right beyond medical applications for example if you want to have like",
    "start": "1948559",
    "end": "1955200"
  },
  {
    "text": "facial recognition systems oftentimes because of biases and training data they may not work very",
    "start": "1955679",
    "end": "1961840"
  },
  {
    "text": "well in deployment especially especially on like less well-represented um minorities",
    "start": "1961840",
    "end": "1968720"
  },
  {
    "text": "right the shafty score also computes like a score for each of this image and here the effect of shafty scores to basically up upload",
    "start": "1968720",
    "end": "1976480"
  },
  {
    "text": "data from the more diverse populations right that are underrepresented in the training",
    "start": "1976480",
    "end": "1981840"
  },
  {
    "text": "data set but are useful in helping the model perform well once used in practice",
    "start": "1981840",
    "end": "1987120"
  },
  {
    "text": "right so right here again just by simply retraining the data on his weighted sharply scored right that has the effect",
    "start": "1987120",
    "end": "1992159"
  },
  {
    "text": "of asking the model to pay more attention to these more diverse data points and then that also improves how well it works in deployments",
    "start": "1992159",
    "end": "2000240"
  },
  {
    "text": "okay so so there's uh so hopefully that gives you a sense of how people can use these",
    "start": "2003600",
    "end": "2009039"
  },
  {
    "start": "2004000",
    "end": "2180000"
  },
  {
    "text": "ideas in practice right so there's actually a bunch of still open challenges with these",
    "start": "2009039",
    "end": "2014080"
  },
  {
    "text": "chaplain scores um i have to discuss that more afterward at the end so one big challenge is still like how do you",
    "start": "2014080",
    "end": "2021039"
  },
  {
    "text": "efficiently compute these scores and so some of the data sets where we apply this to right they can be a hundred",
    "start": "2021039",
    "end": "2026159"
  },
  {
    "text": "thousand data points with big confidence right so how do you compute this efficiently",
    "start": "2026159",
    "end": "2031279"
  },
  {
    "text": "um now we have some partial solutions to how to do this right so based on multi-color sampling to compute",
    "start": "2031279",
    "end": "2037919"
  },
  {
    "text": "the sharpie scores and also in some cases we're able to derive like analytic",
    "start": "2037919",
    "end": "2042960"
  },
  {
    "text": "approximations to the shaky score as these are analytic formulas that we know mathematically are",
    "start": "2042960",
    "end": "2048560"
  },
  {
    "text": "very close to the actual shafty score but because they're analytic we can just compute them almost instantaneously",
    "start": "2048560",
    "end": "2055118"
  },
  {
    "text": "so we can do this now for certain classes of like regression and classification models",
    "start": "2055119",
    "end": "2060800"
  },
  {
    "text": "so another big challenge of the japanese score is that they're not i mean they'll just give you a number",
    "start": "2060800",
    "end": "2065919"
  },
  {
    "text": "right but the number may not really reflect the randomness in the data right because the data often is sampled from",
    "start": "2065919",
    "end": "2072398"
  },
  {
    "text": "some distribution all right so to address that we also introduce these new ideas based on essentially like",
    "start": "2072399",
    "end": "2078720"
  },
  {
    "text": "statistical shapely values right which sort of places these data sharpie scores on firmware statistical footing right so",
    "start": "2078720",
    "end": "2084720"
  },
  {
    "text": "now actually the shaft square itself is like a statistical object like a random variable that we can estimate the mean",
    "start": "2084720",
    "end": "2090960"
  },
  {
    "text": "the bias the variance and we can show that this also has much nicer statistical properties",
    "start": "2090960",
    "end": "2097440"
  },
  {
    "text": "that accounts for randomness in the data",
    "start": "2097440",
    "end": "2103838"
  },
  {
    "text": "okay so before i move on to the the second part about how to audit the models um i'll just pause here and see",
    "start": "2104960",
    "end": "2110880"
  },
  {
    "text": "people have any more questions",
    "start": "2110880",
    "end": "2114680"
  },
  {
    "text": "okay so for the first half we've mostly focused on like auditing the data sets right",
    "start": "2129440",
    "end": "2134720"
  },
  {
    "text": "which is definitely very important uh and this thing there's a complimentary question of okay now you have a good",
    "start": "2134720",
    "end": "2140560"
  },
  {
    "text": "data set or you have models trained on this right so how do we really rigorously understand why the models",
    "start": "2140560",
    "end": "2146320"
  },
  {
    "text": "have certain behaviors right and in particular as we saw like in our dermatology example from the",
    "start": "2146320",
    "end": "2152000"
  },
  {
    "text": "beginning right we want to understand it's interesting to understand why does the model make specific mistakes",
    "start": "2152000",
    "end": "2158000"
  },
  {
    "text": "right and i find mistakes to be particularly interesting right because by looking at why things make mistakes",
    "start": "2158000",
    "end": "2163920"
  },
  {
    "text": "that's how you can get more direct insights into what are the blind spots of the models and what are potential",
    "start": "2163920",
    "end": "2169520"
  },
  {
    "text": "issues and biases and also gives us insights on how we can actionably like mitigate these issues",
    "start": "2169520",
    "end": "2177839"
  },
  {
    "start": "2180000",
    "end": "2359000"
  },
  {
    "text": "so i'll tell you about a couple of approaches that we've been exploring one is actually on sort of extensions of",
    "start": "2181359",
    "end": "2186800"
  },
  {
    "text": "these data sharply ideas into what we're calling neuron shapely values right so it turns out that you can",
    "start": "2186800",
    "end": "2193280"
  },
  {
    "text": "actually also compute something that's very analogous to data sharply scores but actually compute that",
    "start": "2193280",
    "end": "2198640"
  },
  {
    "text": "score for individual neurons right in your neural network right so the output is very similar basically for",
    "start": "2198640",
    "end": "2205040"
  },
  {
    "text": "each individual neuron right it actually computes a score and says that how responsible is that neuron to a specific",
    "start": "2205040",
    "end": "2211040"
  },
  {
    "text": "mistake that the model makes okay",
    "start": "2211040",
    "end": "2216560"
  },
  {
    "text": "and something actually quite interesting happens here when we do this when you compute this neuron chapter scores right",
    "start": "2216560",
    "end": "2221760"
  },
  {
    "text": "um so it turned out to be they're like extremely sparse",
    "start": "2221760",
    "end": "2226880"
  },
  {
    "text": "right so by that i mean like say if i have like a model that's trying to predict you know computer vision models trying to predict whether this is like a",
    "start": "2226880",
    "end": "2232480"
  },
  {
    "text": "active volcano or not right so the network itself could be very large you could have tens of thousands of",
    "start": "2232480",
    "end": "2237760"
  },
  {
    "text": "artificial neurons but when i actually compute like which of the neurons are really",
    "start": "2237760",
    "end": "2243440"
  },
  {
    "text": "responsible for the model's predictions there ends up that there's only like a handful of maybe 15 to 20 neurons that",
    "start": "2243440",
    "end": "2250000"
  },
  {
    "text": "are actually responsible right and i know that there are only 15 and 20 of these because here if i just",
    "start": "2250000",
    "end": "2257280"
  },
  {
    "text": "sort of remove these 15 neurons these are neurons that have the largest neuron sharply scores right so the model's",
    "start": "2257280",
    "end": "2263520"
  },
  {
    "text": "performance basically drops this disappears if i just remove 10 of these neurons",
    "start": "2263520",
    "end": "2269119"
  },
  {
    "text": "right whereas if i just randomly remove neurons from this trained model then you know just sort of a drop in the bucket",
    "start": "2269119",
    "end": "2274480"
  },
  {
    "text": "and models performance doesn't really change right so that's the red curve",
    "start": "2274480",
    "end": "2279599"
  },
  {
    "text": "right so you know when i do these oblations i know that there's only like a small handful of neurons that are actually critical",
    "start": "2279599",
    "end": "2285920"
  },
  {
    "text": "right and we can quickly identify them by using these neuron sharply scores so this is actually quite",
    "start": "2285920",
    "end": "2292480"
  },
  {
    "text": "useful right because then if i want to understand like what is the model actually doing then i don't need",
    "start": "2292480",
    "end": "2298640"
  },
  {
    "text": "to look at all of these tens of thousands of components and neurons and different layers i just need to look at",
    "start": "2298640",
    "end": "2304000"
  },
  {
    "text": "these 15 things right which is no i could even do in 15 minutes",
    "start": "2304000",
    "end": "2310240"
  },
  {
    "text": "so just to give you like a concrete example of what this looks like for this particular yeah yeah question",
    "start": "2311760",
    "end": "2317200"
  },
  {
    "text": "is then you're on chapter value analogous to the data chapter value and that like you have to train multiple subsets of the neural network to",
    "start": "2317200",
    "end": "2324079"
  },
  {
    "text": "complete those scores or yeah it's a good question so this one is actually much faster in that we don't have to retrain the model we can just you know",
    "start": "2324079",
    "end": "2330480"
  },
  {
    "text": "take one your one neural network that's already trained it's after it's already trained all the weights are fixed then",
    "start": "2330480",
    "end": "2336640"
  },
  {
    "text": "we can just oblate the different neurons right by oblate i just mean like set it to be",
    "start": "2336640",
    "end": "2342560"
  },
  {
    "text": "uh to always output like the average value right and then see how that affects the",
    "start": "2342560",
    "end": "2347680"
  },
  {
    "text": "predictions of the network so this so the neural sharpie is actually even faster than computing the",
    "start": "2347680",
    "end": "2353359"
  },
  {
    "text": "data shaft",
    "start": "2353359",
    "end": "2355838"
  },
  {
    "start": "2359000",
    "end": "2524000"
  },
  {
    "text": "yeah good question so okay so for this particular model right so it turns out that actually the",
    "start": "2359119",
    "end": "2364640"
  },
  {
    "text": "the two most important neurons like the ones with the two is the highest chapter scores are sort of sitting in the middle",
    "start": "2364640",
    "end": "2370640"
  },
  {
    "text": "of the network right like maybe like this is the inception model so like actually right in the middle of the",
    "start": "2370640",
    "end": "2376560"
  },
  {
    "text": "inception um so as i'm showing you some",
    "start": "2376560",
    "end": "2382960"
  },
  {
    "text": "visualizations of what each of these neurons is looking for right so for the most",
    "start": "2382960",
    "end": "2388240"
  },
  {
    "text": "important neuron i'll show you here sort of the three examples in the training data sets that leads to the highest",
    "start": "2388240",
    "end": "2394800"
  },
  {
    "text": "activation for that neuron okay so what do you think that neuron",
    "start": "2394800",
    "end": "2403280"
  },
  {
    "text": "is looking for",
    "start": "2403280",
    "end": "2405920"
  },
  {
    "text": "yeah okay so we smoke",
    "start": "2409119",
    "end": "2413839"
  },
  {
    "text": "other ideas",
    "start": "2415680",
    "end": "2418920"
  },
  {
    "text": "yeah so it's basically yeah it's sort of like looking for smoke and it's more generally looking for like you know white puffs right the white puff could",
    "start": "2424720",
    "end": "2431520"
  },
  {
    "text": "be like mashed potato it could be smoke or fountain but sort of like you know looking for these white puffs",
    "start": "2431520",
    "end": "2437359"
  },
  {
    "text": "right and then for the second most important neuron like the one was the second highest neuron chapter score",
    "start": "2437359",
    "end": "2443920"
  },
  {
    "text": "we're also showing you the three images that leads to the highest activation",
    "start": "2443920",
    "end": "2449119"
  },
  {
    "text": "right yeah what do you think what do you think's looking for",
    "start": "2449119",
    "end": "2455280"
  },
  {
    "text": "yes against the sky okay yeah so maybe something like green looking triangle screen green mountains",
    "start": "2460480",
    "end": "2465920"
  },
  {
    "text": "was sky in the background yeah yeah",
    "start": "2465920",
    "end": "2471839"
  },
  {
    "text": "um exactly sorry so you have this models for the active kennels where this basically has like a smoke effector",
    "start": "2471920",
    "end": "2479040"
  },
  {
    "text": "in one layer and then this like mountain detector in the next layer and we know that these two detectors are essentially",
    "start": "2479040",
    "end": "2484720"
  },
  {
    "text": "unique right because from our ablation study if i just remove any one of them right the model is not able to detect",
    "start": "2484720",
    "end": "2490960"
  },
  {
    "text": "any volcanoes",
    "start": "2490960",
    "end": "2494200"
  },
  {
    "text": "um so i guess the key interesting finding here is that actually a lot of these trained networks",
    "start": "2496160",
    "end": "2502480"
  },
  {
    "text": "right um after they're trained right they actually have a very sparse number of critical neurons",
    "start": "2502480",
    "end": "2508319"
  },
  {
    "text": "i.e neurons that are actually responsible for the model's behavior for models mistakes",
    "start": "2508319",
    "end": "2514000"
  },
  {
    "text": "and that's actually great because that makes it relatively easy for us to identify those neurons using the shapley scores and to actually understand what's",
    "start": "2514000",
    "end": "2520240"
  },
  {
    "text": "going on in these models um so one thing you might be interested",
    "start": "2520240",
    "end": "2526960"
  },
  {
    "start": "2524000",
    "end": "2666000"
  },
  {
    "text": "in is let's look at okay if you do this across many different models for different predictors like where are",
    "start": "2526960",
    "end": "2532000"
  },
  {
    "text": "these critical neurons right where do they uh you know where do they live",
    "start": "2532000",
    "end": "2538400"
  },
  {
    "text": "um people might have thought that's okay maybe these critical neurons are either near the beginning or maybe they're near",
    "start": "2538400",
    "end": "2544160"
  },
  {
    "text": "the end of the model right it could be even the second to last layer so the models actually make the final classifications",
    "start": "2544160",
    "end": "2550800"
  },
  {
    "text": "and actually interesting things that actually a lot of these neurons are lying in the middle right so they're not",
    "start": "2550800",
    "end": "2556720"
  },
  {
    "text": "quite at the end but they're some of these intermediate layers where a lot of these crit sparse number of the critical",
    "start": "2556720",
    "end": "2562000"
  },
  {
    "text": "neurons are and we find them to be quite useful for all sorts of interesting applications",
    "start": "2562000",
    "end": "2568400"
  },
  {
    "text": "especially for detecting spheres correlations in the model right just just to show you one example",
    "start": "2568400",
    "end": "2573839"
  },
  {
    "text": "of this like if you actually have a model like one of the themes trying to predict this are actually like dumbbells right that's",
    "start": "2573839",
    "end": "2579760"
  },
  {
    "text": "one of the classes trying to predict right there's this image there's a dumbbell in them um and then these two rows correspond to",
    "start": "2579760",
    "end": "2586640"
  },
  {
    "text": "like again like the the top two of the most important neurons that are responsible for dumbbell prediction",
    "start": "2586640",
    "end": "2592880"
  },
  {
    "text": "right and for each of those i should show you like what are the images the training data that led to activation for those two neurons",
    "start": "2592880",
    "end": "2600560"
  },
  {
    "text": "all right so basically the top one uh looks like it's basically looking for more like the geometries the shapes",
    "start": "2600560",
    "end": "2606800"
  },
  {
    "text": "right something that could look like a dumbbell shape which could maybe even be a trumpet or other things",
    "start": "2606800",
    "end": "2613920"
  },
  {
    "text": "but the second one is actually pretty interesting right so it doesn't actually have any dumbbell looking shapes in it",
    "start": "2613920",
    "end": "2619280"
  },
  {
    "text": "so what do you think that one is looking for",
    "start": "2619280",
    "end": "2622880"
  },
  {
    "text": "yeah so it's really looking for like sort of bare arms right um",
    "start": "2626240",
    "end": "2631839"
  },
  {
    "text": "and so it turned out that's actually in the training data right so there's a spurious correlation and that's every",
    "start": "2631839",
    "end": "2637920"
  },
  {
    "text": "time there's a dumbbell there's actually someone holding it right so then the model actually learned that okay so let's invest",
    "start": "2637920",
    "end": "2644400"
  },
  {
    "text": "a critical neurologist actually just look for people's arms right um and",
    "start": "2644400",
    "end": "2650079"
  },
  {
    "text": "so that's basically what this this this neuron is doing right and by looking at this it's actually quite",
    "start": "2650079",
    "end": "2655839"
  },
  {
    "text": "easy for us to detect that this is could be a a limitation where you know a spirit's correlation that the models",
    "start": "2655839",
    "end": "2661599"
  },
  {
    "text": "picked up",
    "start": "2661599",
    "end": "2664000"
  },
  {
    "start": "2666000",
    "end": "2790000"
  },
  {
    "text": "and we can this also enables us to rapidly edit the model right to remove these",
    "start": "2667200",
    "end": "2673839"
  },
  {
    "text": "kind of spurious correlations if it's one thing that we can do is if i just find that there's oh there's this particular neuron here that's",
    "start": "2673839",
    "end": "2680240"
  },
  {
    "text": "looking for these arms and it's a spherous correlation right the simplest thing i can do is simply just",
    "start": "2680240",
    "end": "2685599"
  },
  {
    "text": "zero out that neuron right um and i can do this without having to retrain the model without",
    "start": "2685599",
    "end": "2690800"
  },
  {
    "text": "additional data so we did some of these model editing experiments um specifically to actually",
    "start": "2690800",
    "end": "2696800"
  },
  {
    "text": "for this facial recognition systems to reduce their bias across different uh",
    "start": "2696800",
    "end": "2703119"
  },
  {
    "text": "ethnic groups right so basically the we're starting off with the original model so each",
    "start": "2703119",
    "end": "2708560"
  },
  {
    "text": "curve here corresponds to the model's performance across different sub populations right for example like bf could be like black female",
    "start": "2708560",
    "end": "2715280"
  },
  {
    "text": "reds here it's like white male right so the model initially sees quite disparate performance the original model has quite",
    "start": "2715280",
    "end": "2721359"
  },
  {
    "text": "the spirit performance accuracy across different subpopulations and then we can as we did like we apply",
    "start": "2721359",
    "end": "2727839"
  },
  {
    "text": "the neuron shaping scores defined okay so which neurons are actually the most responsible for that disparity",
    "start": "2727839",
    "end": "2733680"
  },
  {
    "text": "right so there's a small number of them and then a very simple thing is i just you know zero them out just remove them",
    "start": "2733680",
    "end": "2739839"
  },
  {
    "text": "uh you can see that actually has the interesting effect of still keeping the overall accuracy quite high",
    "start": "2739839",
    "end": "2745520"
  },
  {
    "text": "right if i remove these biased neurons where the overall model's performance is still very high across the majority",
    "start": "2745520",
    "end": "2751119"
  },
  {
    "text": "groups but actually also significantly improves the model's performance for these",
    "start": "2751119",
    "end": "2756400"
  },
  {
    "text": "minority groups right for example the performance for the black females actually starts to increase quite a bit",
    "start": "2756400",
    "end": "2761920"
  },
  {
    "text": "just by zeroing out some of these a few of these biased neurons",
    "start": "2761920",
    "end": "2767680"
  },
  {
    "text": "again so this doesn't quite address the root issue of the problem which is the imbalance in the training data",
    "start": "2767680",
    "end": "2773040"
  },
  {
    "text": "but if you don't have time or resources to get collected additional trading data or even to retrain the model then this",
    "start": "2773040",
    "end": "2779359"
  },
  {
    "text": "is like a relatively easy way to sort of patch the model in real time",
    "start": "2779359",
    "end": "2786599"
  },
  {
    "start": "2790000",
    "end": "2990000"
  },
  {
    "text": "okay so the last part i want to describe is um is following up on this right so",
    "start": "2791359",
    "end": "2797599"
  },
  {
    "text": "it's a deeper way to understand why the models actually makes specific mistakes",
    "start": "2797599",
    "end": "2803119"
  },
  {
    "text": "right i know and it's based on quite recent work where we try to basically provide like a natural language explanation",
    "start": "2803119",
    "end": "2809920"
  },
  {
    "text": "of at the high level that so that it's easy for us to understand of like why does model actually make a specific mistake",
    "start": "2809920",
    "end": "2816000"
  },
  {
    "text": "right let's uh so we call this sort of conceptual explanations or conceptual explainers of ai mistakes",
    "start": "2816000",
    "end": "2822800"
  },
  {
    "text": "right so you can think of this as like a conceptual explainer as being like a wrapper algorithm that can be wrapped around",
    "start": "2822800",
    "end": "2829359"
  },
  {
    "text": "computer vision or classification models right as the original model makes certain mistakes",
    "start": "2829359",
    "end": "2834640"
  },
  {
    "text": "then our wrapper for this explainer actually provides us like a natural language explanation for why did the",
    "start": "2834640",
    "end": "2840640"
  },
  {
    "text": "model fail on that specific instance and here's just an example the kinds of explanations we can provide right so",
    "start": "2840640",
    "end": "2846559"
  },
  {
    "text": "let's say you have this image on top which the true label here is zebra they",
    "start": "2846559",
    "end": "2852000"
  },
  {
    "text": "fit into some of these algorithms they'll predict to be a crocodile and the explanation that we can provide",
    "start": "2852000",
    "end": "2857280"
  },
  {
    "text": "is that like the reason why this is predicted to be a crocodile rather than zebra is because there's like too much",
    "start": "2857280",
    "end": "2862960"
  },
  {
    "text": "water in the image right so in other words like if the image actually had less water and maybe",
    "start": "2862960",
    "end": "2868160"
  },
  {
    "text": "more field and maybe more cows right then the model actually would have correctly put it to be deeper rather",
    "start": "2868160",
    "end": "2873920"
  },
  {
    "text": "than crocodile so here's how this framework works",
    "start": "2873920",
    "end": "2879920"
  },
  {
    "text": "um and the more details are in the paper but the high level is that we're trying to basically provide like natural",
    "start": "2879920",
    "end": "2886400"
  },
  {
    "text": "language explanations right for why a model makes us mistakes on a specific input",
    "start": "2886400",
    "end": "2893839"
  },
  {
    "text": "right and we do this first by like learning like a library of these natural language concepts that corresponds to",
    "start": "2893839",
    "end": "2899839"
  },
  {
    "text": "the different more fine-grained visual concepts that people might care about right so here you can have like visual",
    "start": "2899839",
    "end": "2905280"
  },
  {
    "text": "concepts for water or for cows right but we can in general we will actually create a library of these concepts and",
    "start": "2905280",
    "end": "2911280"
  },
  {
    "text": "we can just learn that once and then apply that to all sorts of different applications um and once we have those concepts like",
    "start": "2911280",
    "end": "2918079"
  },
  {
    "text": "for a given input like maybe that zebra here in the middle where the model is making a mistake",
    "start": "2918079",
    "end": "2923680"
  },
  {
    "text": "right so then effectively we're looking for is you know in this space of concepts right",
    "start": "2923680",
    "end": "2929359"
  },
  {
    "text": "what concepts do i need to increase or decrease such that the model then would have",
    "start": "2929359",
    "end": "2935520"
  },
  {
    "text": "gotten the correct prediction right um if i modify the image in that in those ways so that's why it's kind of",
    "start": "2935520",
    "end": "2942720"
  },
  {
    "text": "more like a counterfactual approach right so the model is making a mistake on its current inputs right how would i",
    "start": "2942720",
    "end": "2948079"
  },
  {
    "text": "how should i modify the input in a minimal way so that the model would have actually gotten the correct",
    "start": "2948079",
    "end": "2954000"
  },
  {
    "text": "prediction on this modified input and system by solving a particular",
    "start": "2954000",
    "end": "2959040"
  },
  {
    "text": "optimization problem using this library of the concepts and then the output then would be the",
    "start": "2959040",
    "end": "2965520"
  },
  {
    "text": "following right so then it says that okay so uh so the reason why the model made a",
    "start": "2965520",
    "end": "2971440"
  },
  {
    "text": "mistake on this particular input is because there's too much water right so the water because the water is the concept that if i reduce water a little",
    "start": "2971440",
    "end": "2979280"
  },
  {
    "text": "bit from this image right then the model keeping everything else the same then the model would actually have corrected",
    "start": "2979280",
    "end": "2984960"
  },
  {
    "text": "predict this to be at zebra",
    "start": "2984960",
    "end": "2988400"
  },
  {
    "start": "2990000",
    "end": "3104000"
  },
  {
    "text": "so just applying this to our running example of the dermatology applications right so here you have like example four",
    "start": "2991599",
    "end": "2998720"
  },
  {
    "text": "example images where the models made mistakes right so the correct labels on top the model's prediction is in the",
    "start": "2998720",
    "end": "3004960"
  },
  {
    "text": "second row and for each of these examples right the explainer is able to provide like this",
    "start": "3004960",
    "end": "3010240"
  },
  {
    "text": "cons the high-level conceptual explanation of why did the model make that mistake right so for the first example right so",
    "start": "3010240",
    "end": "3017599"
  },
  {
    "text": "explainer actually learned that it's actually directly because of the skin color",
    "start": "3017599",
    "end": "3022640"
  },
  {
    "text": "right if the skin color had been not a stark then the model would actually have predicted the correct",
    "start": "3022640",
    "end": "3028400"
  },
  {
    "text": "label rather than the incorrect disease and then the second example the",
    "start": "3028400",
    "end": "3034400"
  },
  {
    "text": "explainer learned that the reason why the model made the mistake here is because the image blur right so if o",
    "start": "3034400",
    "end": "3041040"
  },
  {
    "text": "else being equal right if the image just had been a little bit sharper then the model would actually have predicted the",
    "start": "3041040",
    "end": "3047200"
  },
  {
    "text": "correct label on top the third example the mistake here is really due to",
    "start": "3047200",
    "end": "3053359"
  },
  {
    "text": "zoom right so like the reason why they made the mistakes because the image is too zoomed",
    "start": "3053359",
    "end": "3058400"
  },
  {
    "text": "out right if it had been zoomed in more then actually had gotten the correct prediction and the reason why the model the",
    "start": "3058400",
    "end": "3065359"
  },
  {
    "text": "original classifier made a mistake on the fourth example is because i guess there's too much hair",
    "start": "3065359",
    "end": "3071839"
  },
  {
    "text": "so by looking at this it's actually quite easy for us to understand what are some potential limitations and blind",
    "start": "3071839",
    "end": "3076960"
  },
  {
    "text": "spots of the model essentially skin tone image blur is quite important right zoom",
    "start": "3076960",
    "end": "3082240"
  },
  {
    "text": "and need to be important and the model doesn't work well when there's hair in it right",
    "start": "3082240",
    "end": "3088400"
  },
  {
    "text": "um so this gives us a way to and the explainers have an automated approach right so you don't need to do provide",
    "start": "3088400",
    "end": "3094720"
  },
  {
    "text": "additional annotations to the explainer but this gives us a way to you know quickly assess like what are some potential reasons why the models are",
    "start": "3094720",
    "end": "3100640"
  },
  {
    "text": "making those mistakes right and in some of the new work we're doing is now that we actually have this right natural explanation understandings",
    "start": "3100640",
    "end": "3107520"
  },
  {
    "start": "3104000",
    "end": "3121000"
  },
  {
    "text": "of the mistakes we can actually go back and just provide natural explanations to really directly edit the model to fix",
    "start": "3107520",
    "end": "3114079"
  },
  {
    "text": "those mistakes right without having to retrain the model uh so i won't talk too much about this just in in the interest of time",
    "start": "3114079",
    "end": "3122240"
  },
  {
    "start": "3121000",
    "end": "3446000"
  },
  {
    "text": "yeah so just to wrap up right i mean i think especially we're thinking about how to use ai safely and responsibly in",
    "start": "3122960",
    "end": "3130559"
  },
  {
    "text": "practice it's a lot of the challenge is changing shifting right from the",
    "start": "3130559",
    "end": "3135599"
  },
  {
    "text": "challenge of building the model the challenge of actually testing and evaluating and monitoring the models",
    "start": "3135599",
    "end": "3141200"
  },
  {
    "text": "right and that's really i think where the big open questions are that are really important um",
    "start": "3141200",
    "end": "3147119"
  },
  {
    "text": "you know here we talked about some of the limitations and how a lot of the ai systems are being evaluated before",
    "start": "3147119",
    "end": "3152640"
  },
  {
    "text": "they're being deployed especially in the healthcare context because i think there are definitely some strong limitations there",
    "start": "3152640",
    "end": "3157839"
  },
  {
    "text": "um i think you know it's really important to understand how the data contributes to this this",
    "start": "3157839",
    "end": "3164880"
  },
  {
    "text": "potential mistakes into the models right so datashaft is one approach doing this you can also use these related ideas of",
    "start": "3164880",
    "end": "3170400"
  },
  {
    "text": "neuron sharply for auditing the models and then you know it's really interesting to understand why the models",
    "start": "3170400",
    "end": "3176160"
  },
  {
    "text": "actually make specific mistakes right because that can give us insights into what are the blind",
    "start": "3176160",
    "end": "3181359"
  },
  {
    "text": "spots of these algorithms okay so i'll just",
    "start": "3181359",
    "end": "3187119"
  },
  {
    "text": "wrap up here so all of these papers and code that i mentioned are available on our website",
    "start": "3187119",
    "end": "3194079"
  },
  {
    "text": "and share the references for these works and also i want to thank again all of the students in my group that",
    "start": "3194079",
    "end": "3199599"
  },
  {
    "text": "led the development of these interesting tools so i'll stop here and i'm happy to take any more",
    "start": "3199599",
    "end": "3206000"
  },
  {
    "text": "additional questions [Applause]",
    "start": "3206000",
    "end": "3213280"
  },
  {
    "text": "yeah observation from earlier about dropping neurons uh seems really profound um how does that like what sort",
    "start": "3213280",
    "end": "3220880"
  },
  {
    "text": "of models does that apply to that you've looked at where you can you know drop a large amount of models drop a large",
    "start": "3220880",
    "end": "3227040"
  },
  {
    "text": "amount of neurons and still get similar results yeah it's a good question so we mostly",
    "start": "3227040",
    "end": "3234079"
  },
  {
    "text": "applied that to more emission-based models um and",
    "start": "3234079",
    "end": "3240000"
  },
  {
    "text": "i think it's really interesting to see how this will work also for language models uh so that's like the language part is more ongoing works",
    "start": "3240000",
    "end": "3249318"
  },
  {
    "text": "can you talk a little bit more about these uh like this bank of concepts uh that you had in the last bit like how",
    "start": "3252160",
    "end": "3257839"
  },
  {
    "text": "are those defined and then how do you actually like get their embedding what not",
    "start": "3257839",
    "end": "3263599"
  },
  {
    "text": "yeah that's a good question so um so for example the concepts that we used",
    "start": "3263599",
    "end": "3269359"
  },
  {
    "text": "here is actually collected on a different data set right so the nice thing is that we want to learn the",
    "start": "3269359",
    "end": "3274640"
  },
  {
    "text": "concept once and then be able to apply them to a lot of different applications so for example to learn the concept for",
    "start": "3274640",
    "end": "3281599"
  },
  {
    "text": "zoom right we actually just had a small number of images um",
    "start": "3281599",
    "end": "3287040"
  },
  {
    "text": "that were labeled as like whether it's zoomed in or zoomed out right maybe about 50 images right and then",
    "start": "3287040",
    "end": "3294000"
  },
  {
    "text": "um or for the concept of stripes right you can we just have like 50 examples of",
    "start": "3294000",
    "end": "3299520"
  },
  {
    "text": "stripes which could be from a zebra or from t-shirts or socks right or curtains",
    "start": "3299520",
    "end": "3305760"
  },
  {
    "text": "and then we look at basically the embeddings of those 50 examples of stripes versus some of",
    "start": "3305760",
    "end": "3312160"
  },
  {
    "text": "the negative sets right in the embedding space and that they can tell us okay so here's like a particular direction in",
    "start": "3312160",
    "end": "3317440"
  },
  {
    "text": "embedding space that corresponds to the concept of stripe and then there's another direction that corresponds to concept of you know zoom",
    "start": "3317440",
    "end": "3324400"
  },
  {
    "text": "or a color or order right um so basically to learn each of the concepts we need to have like a",
    "start": "3324400",
    "end": "3330000"
  },
  {
    "text": "small number of annotated examples uh which can be collected ahead of time or from some other existing database we",
    "start": "3330000",
    "end": "3336160"
  },
  {
    "text": "often would use like imagenet existing to learn those concepts and once we learn them then we can apply",
    "start": "3336160",
    "end": "3342079"
  },
  {
    "text": "them to new data sets for new applications like these dermatology applications where we don't really have",
    "start": "3342079",
    "end": "3347680"
  },
  {
    "text": "ex explicit annotations about zoom or hero",
    "start": "3347680",
    "end": "3352799"
  },
  {
    "text": "yeah yeah building off that answer um have are you excited by any techniques for uh",
    "start": "3356240",
    "end": "3361440"
  },
  {
    "text": "supervised constant learning yeah yeah definitely um so i didn't mention here so one of the other",
    "start": "3361440",
    "end": "3368160"
  },
  {
    "text": "algorithms we built is actually called ace which is like automatic concepts discovery which is basically a",
    "start": "3368160",
    "end": "3374160"
  },
  {
    "text": "supervised way to learn concepts um another approach that i think are very interesting is that basically use",
    "start": "3374160",
    "end": "3380400"
  },
  {
    "text": "these cross-modal models like clip to learn concepts right because um and one of my students",
    "start": "3380400",
    "end": "3386720"
  },
  {
    "text": "mert has actually done a lot of really interesting experiments there because like these cross model models they have",
    "start": "3386720",
    "end": "3391920"
  },
  {
    "text": "the image encoders and they also have the text encoders right just from the image encoder then we're going to see a case um like for example if i want to",
    "start": "3391920",
    "end": "3399440"
  },
  {
    "text": "learn the concept of stripe i can actually just ask clip what what is what part of the embedding",
    "start": "3399440",
    "end": "3404799"
  },
  {
    "text": "space corresponds to the the word stripes right and then from that we can actually learn",
    "start": "3404799",
    "end": "3412079"
  },
  {
    "text": "what are the corresponding image parts that correspond to stripes",
    "start": "3412079",
    "end": "3417680"
  },
  {
    "text": "right um so so the short answer is i think these cross these multi-modal models like clip or",
    "start": "3417680",
    "end": "3423680"
  },
  {
    "text": "dali are actually could be a good way to learn a lot of the visual concepts by leveraging the language components",
    "start": "3423680",
    "end": "3431318"
  },
  {
    "text": "okay let's wrap it up there thank you",
    "start": "3436880",
    "end": "3441318"
  }
]