[
  {
    "start": "0",
    "end": "5465"
  },
  {
    "text": "Hey, everybody. And welcome back. We're going to get started with\na refresh your understanding poll. You can go to Ed and to see\nall of the polls for today.",
    "start": "5465",
    "end": "13820"
  },
  {
    "text": "Just remember to log in\nfirst so that we can log it for participation points. The two questions\nask you to think",
    "start": "13820",
    "end": "19430"
  },
  {
    "text": "about what we talked about last\ntime in terms of Markov decision processes and what sort of\nguarantees or type of properties",
    "start": "19430",
    "end": "25985"
  },
  {
    "text": "they have. ",
    "start": "25985",
    "end": "46630"
  },
  {
    "text": "Yeah. Could you read the extend\nmodel of the tabular MVP again?",
    "start": "46630",
    "end": "52780"
  },
  {
    "text": "Yeah, great question. Tabular MVP is where\nwe can write down what the value is of\na state as a table.",
    "start": "52780",
    "end": "58530"
  },
  {
    "text": "So like can just have\none entry for what the value is for each state. This is in contrast to\nneural networks or things",
    "start": "58530",
    "end": "65440"
  },
  {
    "text": "like that where you don't\nhave one parameter per state. ",
    "start": "65440",
    "end": "103602"
  },
  {
    "text": "OK, we'll take like\nanother one or two minutes. We have a good amount of\ncontroversy on these questions, so we'll see how this converges.",
    "start": "103602",
    "end": "110990"
  },
  {
    "start": "110990",
    "end": "144459"
  },
  {
    "text": "I remember seeing\nthis A to the power S number in a previous context. Was it in the context of policy\niteration, or what was it?",
    "start": "144460",
    "end": "152140"
  },
  {
    "text": "Yeah, does someone want to\nremember why is A to the S important? I remember why it is.",
    "start": "152140",
    "end": "158170"
  },
  {
    "text": "Yeah, remind me of your name. Is it the number of\ntotal possible policies?",
    "start": "158170",
    "end": "163760"
  },
  {
    "text": "Exactly right. Exactly right. So there's at most A to the\nS potential deterministic",
    "start": "163760",
    "end": "169090"
  },
  {
    "text": "policies. ",
    "start": "169090",
    "end": "180090"
  },
  {
    "text": "All right. So most people selected\nthe correct answer for the first one,\nwhich this is true.",
    "start": "180090",
    "end": "186120"
  },
  {
    "text": "So asymptotically, value\niteration and policy iteration",
    "start": "186120",
    "end": "191549"
  },
  {
    "text": "are correct in the tabular\ndiscrete Markov decision process case.",
    "start": "191550",
    "end": "196860"
  },
  {
    "text": "And they will asymptotically\nboth converge and compute the right value function. The second one looks like\nit's pretty evenly split,",
    "start": "196860",
    "end": "203980"
  },
  {
    "text": "so I'd like you to turn\nto someone near you and argue for what you\nsaid for the second one.",
    "start": "203980",
    "end": "209525"
  },
  {
    "start": "209525",
    "end": "227536"
  },
  {
    "text": "[INDISTINCT SPEECH] ",
    "start": "227536",
    "end": "296660"
  },
  {
    "text": "All right, so maybe we vote\nif your answer changed. ",
    "start": "296660",
    "end": "317040"
  },
  {
    "text": "So the answer is true. And about half of you said that. Does someone want to\ntell me why this is true?",
    "start": "317040",
    "end": "323430"
  },
  {
    "text": "Yeah. I'm not sure if this\nis correct, but I think that value iteration might\nnot be guaranteed to converge.",
    "start": "323430",
    "end": "331320"
  },
  {
    "text": "So if it's not\nguaranteed to converge, it could just be unbounded. So that certainly\nwould be the case. But fortunately,\nit is guaranteed",
    "start": "331320",
    "end": "337085"
  },
  {
    "text": "to converge if gamma\nis less than 1. But you're correct that it\ncan require more iterations.",
    "start": "337085",
    "end": "344729"
  },
  {
    "text": "And remind me your name. So my thinking was that in\nmaking policy iteration,",
    "start": "344730",
    "end": "351897"
  },
  {
    "text": "we know that in each\nstep, we're going to improve to a new policy. But in each value step, you\nmight not reach a new policy.",
    "start": "351897",
    "end": "359100"
  },
  {
    "text": "You might take multiple\nsteps to reach a new policy. That's right. So in policy\niteration, and I talked",
    "start": "359100",
    "end": "366920"
  },
  {
    "text": "to some people about this, too,\nthere can only be A to the S. So in policy iteration,\nthere is at most A to the S",
    "start": "366920",
    "end": "372620"
  },
  {
    "text": "because you only go\nthrough each policy once. But for value iteration,\nit can be more. And I'll give you an example.",
    "start": "372620",
    "end": "379699"
  },
  {
    "text": "So one way I would\nthink, in general, if you see this sort of question\nis to think about, well, can I come up with\na counter example",
    "start": "379700",
    "end": "385520"
  },
  {
    "text": "to say where this\nwould be different. So consider a really silly\nMarkov decision process",
    "start": "385520",
    "end": "390830"
  },
  {
    "text": "where there's just one\nstate and one action. So if there's one\nstate and one action, there is literally one policy.",
    "start": "390830",
    "end": "396060"
  },
  {
    "text": "You can only do one thing,\nand there's only one state to do it in. So that means that\npolicy iteration is going to take one round.",
    "start": "396060",
    "end": "401840"
  },
  {
    "text": "But for what value\niteration is going to do, is it's going to keep going\nuntil the value function stops",
    "start": "401840",
    "end": "407900"
  },
  {
    "text": "changing or stops changing\nwithin a very small amount. And so what would happen\nin value iteration,",
    "start": "407900",
    "end": "413840"
  },
  {
    "text": "and feel free to go back to\nyour notes from last time, is we would start off. And let's say the reward\nis 1, and the gamma is 0.9,",
    "start": "413840",
    "end": "421520"
  },
  {
    "text": "and we initialize the\nvalue function to 0. So then if you use\na geometric series,",
    "start": "421520",
    "end": "426580"
  },
  {
    "text": "and if you haven't\nseen that before, just come chat with me about it. I'm happy to say. The V star of this date\nis 1 over 1 minus gamma,",
    "start": "426580",
    "end": "433949"
  },
  {
    "text": "because you get 1 plus\ngamma times 1 plus gamma squared times 1 dot, dot dot.",
    "start": "433950",
    "end": "440550"
  },
  {
    "text": "Because you're always\nstaying in that state, and you're always\ntaking that action. And day in and day out,\nthat's what you get forever.",
    "start": "440550",
    "end": "446699"
  },
  {
    "text": "So the actual value function\nthat you get eventually is 1 over 1 minus\ngamma, or about 10.",
    "start": "446700",
    "end": "454530"
  },
  {
    "text": "But after the first iteration,\nvalue iteration V1 of s is just 1.",
    "start": "454530",
    "end": "460770"
  },
  {
    "text": "So that means 1 is not close\nto 10, or not that close to 10. So we haven't converged yet.",
    "start": "460770",
    "end": "465940"
  },
  {
    "text": "So we'll have to continue to do\na bunch of iterations of value iteration, whereas\nat that point,",
    "start": "465940",
    "end": "471780"
  },
  {
    "text": "policy iteration would stop\nbecause it would just evaluate the value of the\none policy we have, which is to take that one\naction, and you'd be done.",
    "start": "471780",
    "end": "479227"
  },
  {
    "text": "And I bring this up\njust to illustrate that, even though both\nof those algorithms are guaranteed to converge to\nthe right thing, eventually,",
    "start": "479227",
    "end": "485310"
  },
  {
    "text": "they can have quite different\nbehavior in the short term. You got a question? I was just going to ask, is it\nonly converging in the limit?",
    "start": "485310",
    "end": "494850"
  },
  {
    "text": "All of them are only\nconverging limit. So all of them are\nconverging asymptotically",
    "start": "494850",
    "end": "500442"
  },
  {
    "text": "as you do this over\nand over again. Good question.  Great well, welcome back.",
    "start": "500442",
    "end": "506503"
  },
  {
    "text": "If you just came in, feel free\nto go through the questions later. What we're going\nto be doing today is to continue in this more\nsimple setting where we don't",
    "start": "506503",
    "end": "513539"
  },
  {
    "text": "do any function approximation. But we are now going\nto think about the fact where we don't have\nmodels of the world.",
    "start": "513539",
    "end": "519961"
  },
  {
    "text": "And what I mean by\nthat is that we're not given a dynamics model, and\nwe're not given a reward model. Our agent just has to try\nthings out in the world",
    "start": "519961",
    "end": "526950"
  },
  {
    "text": "to learn how good they are. And we're going to start with\nmodel free policy evaluation,",
    "start": "526950",
    "end": "532470"
  },
  {
    "text": "in the case where we still\nhave a small enough number of contexts or states\nthat we could write down",
    "start": "532470",
    "end": "538079"
  },
  {
    "text": "a value for every single\none of them separately. So that's why we\ncall it tabular.",
    "start": "538080",
    "end": "544320"
  },
  {
    "text": "I'll just say one thing\nin terms of logistics. So office hours\nare on the website. We'll try to keep\nthat calendar updated.",
    "start": "544320",
    "end": "550280"
  },
  {
    "text": "It's just a Google Calendar. It'll include the location. And if you go to Q status for\nthe one-on-one office hours,",
    "start": "550280",
    "end": "557149"
  },
  {
    "text": "we'll make sure that the\nZoom is either on that or on the website. I'll have office hours starting\nthis week on Thursdays.",
    "start": "557150",
    "end": "564370"
  },
  {
    "text": "And mine are for project\nand conceptual questions. So feel free to come ask\nme about anything in class.",
    "start": "564370",
    "end": "569830"
  },
  {
    "text": "I won't be going through\ncode, but you can go talk to the TAs about that. Or you can come\nin and brainstorm",
    "start": "569830",
    "end": "575050"
  },
  {
    "text": "about projects or\ngeneral questions about reinforcement learning. I'm sure some of\nyou are starting",
    "start": "575050",
    "end": "581230"
  },
  {
    "text": "to think about projects. Just in general, some\npeople are asking, so what's kind of in scope. I put something\non Ed about that.",
    "start": "581230",
    "end": "587616"
  },
  {
    "text": "But also, in general, it can be\na new idea for a reinforcement learning, a new application. It can be something you're doing\nif you're already doing research",
    "start": "587617",
    "end": "595150"
  },
  {
    "text": "in reinforcement learning. It can also be replicating\npart of an existing paper.",
    "start": "595150",
    "end": "600520"
  },
  {
    "text": "And this is really\nhelpful, actually. It's helpful for\nthe whole community because there is a\nlot of work going on,",
    "start": "600520",
    "end": "606640"
  },
  {
    "text": "and people are making different\nchoices about hyperparams, and seeds and things like that. So it really is\nvery valuable to see",
    "start": "606640",
    "end": "613390"
  },
  {
    "text": "what things we can replicate. Does anybody have any\nquestions about that or other logistics of the\nclass before we get going?",
    "start": "613390",
    "end": "619877"
  },
  {
    "text": " All right. So let's get into\npolicy evaluation.",
    "start": "619877",
    "end": "626250"
  },
  {
    "text": "So as I said, what we're\ngoing to be doing today is to think about how do we\nlearn through direct experience",
    "start": "626250",
    "end": "631990"
  },
  {
    "text": "how good decisions are. And we're going to assume\nthat we have a fixed policy. So again, like our\nboss says, how good",
    "start": "631990",
    "end": "637470"
  },
  {
    "text": "is this way of\nadvertising to customers. Or maybe you're\nin a setting where you're trying to see how\ngood is the patient outcomes",
    "start": "637470",
    "end": "644190"
  },
  {
    "text": "from the current protocol. And the idea is that we're\nonly going to be using data from the environment.",
    "start": "644190",
    "end": "650912"
  },
  {
    "text": "And today, this\nexperience is going to come from\nexecuting that policy. Let me just move this up so\nyou can see a bit better.",
    "start": "650912",
    "end": "658040"
  },
  {
    "text": "OK. Today, we're going to assume\nthat when we get this data, it's from directly executing\na particular policy.",
    "start": "658040",
    "end": "664440"
  },
  {
    "text": "Later, we'll talk about\nother relaxations to this. And so I'm going to\ntry to motivate today",
    "start": "664440",
    "end": "671150"
  },
  {
    "text": "why this is a useful thing to\ndo and what sort of properties we would want to try to\ncompare different algorithms.",
    "start": "671150",
    "end": "678080"
  },
  {
    "text": "So it will turn out\nthat this type of thing comes up all the time. It comes up when we actually\nwant to make decisions and learn",
    "start": "678080",
    "end": "683962"
  },
  {
    "text": "better policies. And it's going to be an\nimportant part of much more complicated algorithms,\nlike deep Q-learning,",
    "start": "683962",
    "end": "691250"
  },
  {
    "text": "and policy gradient\nand other things, where we want to be able to see\nhow good our current things so that then we can take gradient\nsteps or improve our policy.",
    "start": "691250",
    "end": "699295"
  },
  {
    "text": " So this is what we're going\nto try to cover today.",
    "start": "699295",
    "end": "704895"
  },
  {
    "text": "We're going to cover\nMonte Carlo policy evaluation, temporal difference\nlearning, certainty equivalence, and batch policy evaluation.",
    "start": "704895",
    "end": "711650"
  },
  {
    "text": "And maybe raise\nyour hand if you've seen temporal difference\nlearning before.",
    "start": "711650",
    "end": "717319"
  },
  {
    "text": "OK, raise your hand if\nyou've seen Q-learning. OK, great. Q-learning is the control\nversion, basically,",
    "start": "717320",
    "end": "723017"
  },
  {
    "text": "of temporal difference learning. So you'll see a lot\nof similarities there.",
    "start": "723017",
    "end": "728050"
  },
  {
    "text": "All right. Before we dive into\nthis, I just wanted to recall a couple of\ndefinitions we'll have. We're going to use G\nfor the return, which",
    "start": "728050",
    "end": "734269"
  },
  {
    "text": "means that, from this\nparticular state, what is the discounted sum of rewards\nwe get for a particular episode.",
    "start": "734270",
    "end": "741139"
  },
  {
    "text": "The state value function\nis saying, on average, what is that reward we get.",
    "start": "741140",
    "end": "746160"
  },
  {
    "text": "And the state action value\nsays, if I start in this state, take this action and then\nfollow this policy, what",
    "start": "746160",
    "end": "752100"
  },
  {
    "text": "is the expected\ndiscounted sum of rewards.  So we saw last\nweek that we might",
    "start": "752100",
    "end": "759230"
  },
  {
    "text": "want to do dynamic programming\nfor policy evaluation when we do have\naccess to the models.",
    "start": "759230",
    "end": "764770"
  },
  {
    "text": "So again, what I mean by that\nis that where someone gives you a function for the reward and a\nfunction for the dynamics model.",
    "start": "764770",
    "end": "771980"
  },
  {
    "text": "And so we saw we could\ndo this sort of Bellman like backup for a\nparticular policy. So it's different than\nthe Bellman equation",
    "start": "771980",
    "end": "778279"
  },
  {
    "text": "because there's no max. I'm not trying to take a\nmax over different actions. We're just taking\nwhatever action",
    "start": "778280",
    "end": "784190"
  },
  {
    "text": "is specified by the policy. And in this equation\nhere, this is for like a deterministic policy.",
    "start": "784190",
    "end": "790450"
  },
  {
    "text": " Otherwise, we'd need\nsome additional averaging",
    "start": "790450",
    "end": "795460"
  },
  {
    "text": "over all the actions that\ncould be taken by that policy. And just to remind ourselves\nhere, before we converge,",
    "start": "795460",
    "end": "802930"
  },
  {
    "text": "this policy, this V\npi k, is an estimate of the value of the policy.",
    "start": "802930",
    "end": "808980"
  },
  {
    "text": "It's not the actual value yet. It's just an estimate, and\nit's, hopefully, improving over as we do more iterations.",
    "start": "808980",
    "end": "814285"
  },
  {
    "text": " And another good thing\nto remind ourselves of",
    "start": "814286",
    "end": "820649"
  },
  {
    "text": "is that this is here, this\nsort of expected discounted sum of rewards, what we're\ndoing in this equation",
    "start": "820650",
    "end": "828209"
  },
  {
    "text": "is we are plugging in this term\nas an estimate of the expected",
    "start": "828210",
    "end": "834120"
  },
  {
    "text": "discounted rewards\nfor the future. So we're saying, we've got this\nestimate of the value function.",
    "start": "834120",
    "end": "840350"
  },
  {
    "text": "We're going to plug-in\nand say, my reward is my immediate reward\nplus my discounted sum of future rewards.",
    "start": "840350",
    "end": "845640"
  },
  {
    "text": "And this is what I'm\nusing for my discounted sum of future rewards. And so this is known as\nbootstrapping because we're",
    "start": "845640",
    "end": "852380"
  },
  {
    "text": "plugging in one\nestimate in order to help us do another estimate. And we'll see a picture\nof this graphically later.",
    "start": "852380",
    "end": "860200"
  },
  {
    "text": "All right. Monte Carlo policy evaluation\nis a really simple idea, but it's very useful\nand it is commonly done.",
    "start": "860200",
    "end": "868490"
  },
  {
    "text": "So essentially, the idea with\nMonte Carlo policy evaluation is, we are just going\nto simulate or act",
    "start": "868490",
    "end": "875390"
  },
  {
    "text": "in the real world. So this is just\nsaying you've got a policy, which means what\naction to take in every state.",
    "start": "875390",
    "end": "880800"
  },
  {
    "text": "Today, we'll mostly focus on\ndeterministic policies just to make it easier. So I'll just say that,\nfor most of today,",
    "start": "880800",
    "end": "893450"
  },
  {
    "text": "assume pi is deterministic. ",
    "start": "893450",
    "end": "900839"
  },
  {
    "text": "But all of these ideas\ncan easily be extended. Just easy to write that\ndown without having to do an expectation\nover actions everywhere.",
    "start": "900840",
    "end": "907410"
  },
  {
    "text": "OK, so what's the\nidea in this case? Well, the value function is just\nan average over the returns.",
    "start": "907410",
    "end": "915050"
  },
  {
    "text": "It's an expectation over the\ntrajectories or the returns you could get by\nfollowing the policy.",
    "start": "915050",
    "end": "921170"
  },
  {
    "text": "And therefore, the value is\njust the mean of returns. And we know how to\napproximate means.",
    "start": "921170",
    "end": "927260"
  },
  {
    "text": "We just do things a bunch\nof times and we average. And so as an example of\nthis, it might be something",
    "start": "927260",
    "end": "934110"
  },
  {
    "text": "that someone says,\nOK, I want to know if we, say, give a particular\nset of patient treatments,",
    "start": "934110",
    "end": "941030"
  },
  {
    "text": "and maybe those treatments\ntake a year, for example. And someone wants to on\naverage, how good is that.",
    "start": "941030",
    "end": "946649"
  },
  {
    "text": "Well, what you could do is\nyou could have 100 patients. I'll go through that\nparticular protocol for a year",
    "start": "946650",
    "end": "952130"
  },
  {
    "text": "and then average their outcomes. And that would be an example of\nMonte Carlo policy evaluation,",
    "start": "952130",
    "end": "958189"
  },
  {
    "text": "because you just\nexecute the policy for many different episodes,\nand then you average.",
    "start": "958190",
    "end": "964633"
  },
  {
    "text": "And one thing just\nto note here is that you can have cases here\nwhere not all the trajectories are the same length.",
    "start": "964633",
    "end": "971260"
  },
  {
    "text": "So imagine, in the\npatient case I just gave, you might have that\nsome people drop out of a trial during the year.",
    "start": "971260",
    "end": "976690"
  },
  {
    "text": "Or maybe they finish their\ntreatment successfully, and so then they're also done.",
    "start": "976690",
    "end": "981759"
  },
  {
    "text": "So all the trajectories\nmay not be the same length, but essentially, you can\njust think of it as you just",
    "start": "981760",
    "end": "986949"
  },
  {
    "text": "have many, many trajectories. Maybe this one\nhas a G equals 10. This had a G equals 5.",
    "start": "986950",
    "end": "993410"
  },
  {
    "text": "This has a G equals 10. And you just average\nover all of them. And that is your value function.",
    "start": "993410",
    "end": "999644"
  },
  {
    "text": " Now, one of the benefits of\nthis is that, when we do this,",
    "start": "999645",
    "end": "1005850"
  },
  {
    "text": "we're not actually-- benefits or drawbacks. And we'll talk about this. This is making no assumption\nthat the system is a Markov",
    "start": "1005850",
    "end": "1012920"
  },
  {
    "text": "decision process. It's just averaging. So your system\nmight not be Markov.",
    "start": "1012920",
    "end": "1020660"
  },
  {
    "text": "What I mean by that is\nthat you, in general, will have a finite set of\nfeatures to describe the state.",
    "start": "1020660",
    "end": "1026380"
  },
  {
    "text": "So if we think about\nthat patient example I just had, maybe you have\ndifferent vitals of the patient. Maybe you have static\ndemographic variables.",
    "start": "1026380",
    "end": "1033679"
  },
  {
    "text": "But we do not have all the\nfeatures that are probably going to describe how\nsomeone's going to react to a set of treatments.",
    "start": "1033680",
    "end": "1040040"
  },
  {
    "text": "And so because of that,\nyou may or may not think that, in the features\nyou have access to, the system is Markov.",
    "start": "1040040",
    "end": "1047089"
  },
  {
    "text": "But this doesn't require\nthe state to be Markov. It's just averaging. You just roll out your policy\nmany times and you average.",
    "start": "1047089",
    "end": "1054410"
  },
  {
    "text": "Now, a really\nimportant thing here is that it can only be\napplied to episodic MDPs.",
    "start": "1054410",
    "end": "1059789"
  },
  {
    "text": "So what do I mean by that? I mean your episode has\nto end in order for you to see what the\ntotal return was.",
    "start": "1059790",
    "end": "1068060"
  },
  {
    "text": "So if you have horizon\nlengths or episodes that last for a year, that's OK. It's a little bit slow,\nbut you could do that.",
    "start": "1068060",
    "end": "1075112"
  },
  {
    "text": "But if you want to\njust think of how good a policy is, if you just are\ngoing to act forever and never",
    "start": "1075112",
    "end": "1080149"
  },
  {
    "text": "stop, this wouldn't\nwork, or not without some additional approximations.",
    "start": "1080150",
    "end": "1087093"
  },
  {
    "text": "Somebody have any\nquestions about either of these two things, about\nit not assuming the state is Markov? Yeah.",
    "start": "1087093",
    "end": "1092700"
  },
  {
    "text": "You were talking about\nmedical treatments. So does this only work\nif the treatment only",
    "start": "1092700",
    "end": "1099320"
  },
  {
    "text": "lasts the same amount of\ntime for every patient, like six months? Because they have\ndifferent lengths.",
    "start": "1099320",
    "end": "1106320"
  },
  {
    "text": "How can that be episodic? Great question. Yeah, so what [INAUDIBLE]\nis like, well, could it be episodic if the\nepisodes are different lengths?",
    "start": "1106320",
    "end": "1113310"
  },
  {
    "text": "It could be. So it could be that you\nhave a fixed policy. And maybe that policy says,\nif someone doesn't respond",
    "start": "1113310",
    "end": "1118970"
  },
  {
    "text": "to this type of treatment,\nwe do this additional type of treatment. In a clinic, that's very common. As long as the episode\nis guaranteed to end,",
    "start": "1118970",
    "end": "1127165"
  },
  {
    "text": "you know the treatment could\nonly last, say for a year total, you can still average\nover all those outcomes. You just sum over the return\nfor those different ones.",
    "start": "1127165",
    "end": "1135980"
  },
  {
    "text": "Yeah. For each of these\ntrajectories, are we supposed to begin with\nthis different state?",
    "start": "1135980",
    "end": "1143540"
  },
  {
    "text": "Or we can actually start\nwith the same state? Great question. So if we want to get a value\nfunction for all states,",
    "start": "1143540",
    "end": "1149740"
  },
  {
    "text": "we need to see all the states\ninside of these trajectories. And we'll talk about how we\nestimate these in a second. So I think this\nwill be answered.",
    "start": "1149740",
    "end": "1157170"
  },
  {
    "text": "So for example, how\nmight we compute this? So we would like to get the\nvalue for all the states",
    "start": "1157170",
    "end": "1163590"
  },
  {
    "text": "that are reachable\ninside of your policy. So what you could do\nis, we can initialize",
    "start": "1163590",
    "end": "1168630"
  },
  {
    "text": "two different variables. One, N of s is just going to be\nthe counts, the number of times that we've updated our\nestimate for state s.",
    "start": "1168630",
    "end": "1176550"
  },
  {
    "text": "G of s here is going to start\nwith 0, which we've never seen any returns from this state.",
    "start": "1176550",
    "end": "1182760"
  },
  {
    "text": "So what every visit Monte Carlo\ndoes is it samples an episode. So this goes up to\nsome time step T, TI.",
    "start": "1182760",
    "end": "1190740"
  },
  {
    "text": "And this has to\nbe finite, but it could be different on\ndifferent episodes.",
    "start": "1190740",
    "end": "1195850"
  },
  {
    "text": "And then we compute\nthe discounted sum of rewards for that episode. OK, so starting at time\nstep T, how much reward",
    "start": "1195850",
    "end": "1205710"
  },
  {
    "text": "do we get till the\nend of the episode? And then for every time\nstep until the end,",
    "start": "1205710",
    "end": "1210720"
  },
  {
    "text": "we see, is this is the first\ntime that state's been visited. Then we update the\ntotal number of times",
    "start": "1210720",
    "end": "1216200"
  },
  {
    "text": "we visited this state for the\nfirst time per in each episode. We increment the total\nreturn and we average.",
    "start": "1216200",
    "end": "1222990"
  },
  {
    "text": " So it steps along, and it just\nsays, OK, maybe the first time",
    "start": "1222990",
    "end": "1230120"
  },
  {
    "text": "I reached-- if you think\nof the Mars Rover example, I have it in the next one. No, I think I put it--\nso for a lot of today",
    "start": "1230120",
    "end": "1235490"
  },
  {
    "text": "I've moved a lot of\nthe worked examples till the end of the slides. But if you want to go through\nthem later, I encourage you to.",
    "start": "1235490",
    "end": "1241940"
  },
  {
    "text": "So for example, in the\ncase of the Mars Rover, you might imagine you\nstart in state S3. And then on that particular\none, you get a reward of 1",
    "start": "1241940",
    "end": "1250309"
  },
  {
    "text": "for that episode. And so then you would average\nin 1, starting in that state S3",
    "start": "1250310",
    "end": "1255649"
  },
  {
    "text": "till the end.  So this is first visit\nMonte Carlo evaluation,",
    "start": "1255650",
    "end": "1263160"
  },
  {
    "text": "which means you only\nupdate a state at most once in each episode. So if you had something like\nthis, S1 went to S2, went to S3,",
    "start": "1263160",
    "end": "1272840"
  },
  {
    "text": "went to S2, went to S3. So in this case, dot, dot,\ndot, you would update S1 once",
    "start": "1272840",
    "end": "1281870"
  },
  {
    "text": "in this trajectory, and\nyou would update S2 once, and S3 once, even though you,\nin fact, visit those states",
    "start": "1281870",
    "end": "1288260"
  },
  {
    "text": "multiple times. You only update them for\nthe first time you visit. Yeah.",
    "start": "1288260",
    "end": "1293330"
  },
  {
    "text": "So this have the problem\nwhere the state is really rare or uncommon, then we\nget a really bad [INAUDIBLE]",
    "start": "1293330",
    "end": "1301490"
  },
  {
    "text": "or we just never\nvisit it at all? So if you don't ever visit\na state under the policy,",
    "start": "1301490",
    "end": "1307110"
  },
  {
    "text": "that's OK, because then you\ndon't have a value for it. But then it's kind of\nundefined because you never would reach it.",
    "start": "1307110",
    "end": "1313850"
  },
  {
    "text": "In this case, as\n[INAUDIBLE] says, if there's a state that's\nreally rare for you to reach inside\nof your policy, it",
    "start": "1313850",
    "end": "1320340"
  },
  {
    "text": "might take a lot of\ntrajectories in order to get a good estimate of it. So maybe there's some rare side\neffect of a treatment plan,",
    "start": "1320340",
    "end": "1327970"
  },
  {
    "text": "and it's going to take a lot of\ntrajectories to observe that. And that was one of the\nchallenges with the COVID",
    "start": "1327970",
    "end": "1333330"
  },
  {
    "text": "vaccine, is that, of course, it\nwas a finite number of people. It was a pretty large\nnumber, but pretty finite.",
    "start": "1333330",
    "end": "1338350"
  },
  {
    "text": "And some side\neffects don't show up until you get many, many more. It's true, generally,\nfor treatments,",
    "start": "1338350",
    "end": "1344020"
  },
  {
    "text": "even if on average\nthey're totally fine. But you won't see some of\nthose rare side effects until you get an enormous\nnumber of trajectories.",
    "start": "1344020",
    "end": "1350130"
  },
  {
    "text": "Now, COVID vaccine\ncertainly had-- the benefits there way\noutweigh side effects.",
    "start": "1350130",
    "end": "1355480"
  },
  {
    "text": "But my point is\njust to highlight that, depending on how\nfrequently you see the states, it will take you more or less\na number of total episodes",
    "start": "1355480",
    "end": "1362850"
  },
  {
    "text": "in order to observe. Yeah. Is it fine if it doesn't matter\nthat I saw S2 again or S3 again",
    "start": "1362850",
    "end": "1368880"
  },
  {
    "text": "in this trajectory? For this algorithm, no. It probably affects\nthe reward still. It's just that you\ndon't use that data.",
    "start": "1368880",
    "end": "1375540"
  },
  {
    "text": "So an alternative is\ncalled every visit, where every time you see the state in\nthat trajectory, you update it.",
    "start": "1375540",
    "end": "1381230"
  },
  {
    "text": " And as you might\nimagine there, let's say you see it's a really\nlong trajectory,",
    "start": "1381230",
    "end": "1388200"
  },
  {
    "text": "and you see S too many times. Then you would update\nfor all of those. ",
    "start": "1388200",
    "end": "1394325"
  },
  {
    "text": "So I'm just going to show you\nthree common different ones. So this is a worked example\nyou can go through later",
    "start": "1394326",
    "end": "1400620"
  },
  {
    "text": "if you want, which is, if you\nimagine this is the Mars Rover, the rewards are on either side.",
    "start": "1400620",
    "end": "1405970"
  },
  {
    "text": "This is the\nparticular trajectory. You can compute the first visit\nand the every visit Monte Carlo estimates.",
    "start": "1405970",
    "end": "1412620"
  },
  {
    "text": "So both of those are totally\nreasonable things to do. Perhaps more common is what's\nknown as incremental Monte",
    "start": "1412620",
    "end": "1417750"
  },
  {
    "text": "Carlo. And this does kind of what\nyou would expect it to do, which is you maintain a\nrunning estimate for what",
    "start": "1417750",
    "end": "1424260"
  },
  {
    "text": "is the value under a policy\nfor a particular state, and you smoothly update\nthat as you get more data.",
    "start": "1424260",
    "end": "1431309"
  },
  {
    "text": "So what we would\ndo in this case is you keep track of the number of\ntimes you visited that state,",
    "start": "1431310",
    "end": "1437220"
  },
  {
    "text": "and then you weigh\nyour old estimate by the number of times you've\nvisit the state minus 1 divided",
    "start": "1437220",
    "end": "1443100"
  },
  {
    "text": "by N s, plus your new return you\njust observed divided by N s. So that's just sort\nof your way you're",
    "start": "1443100",
    "end": "1449789"
  },
  {
    "text": "kind of constantly updating your\nvalue function for this state as you get more data.",
    "start": "1449790",
    "end": "1456250"
  },
  {
    "text": "And for those of you who have\ndone machine learning, which is probably most of you, this\nshould look pretty familiar. This is kind of like\na learning rate.",
    "start": "1456250",
    "end": "1462672"
  },
  {
    "text": " This is your updated value,\nand this is your old value.",
    "start": "1462672",
    "end": "1469960"
  },
  {
    "text": "And in fact, that's what\nwe're going to see here. OK? So you can think\nof this in general.",
    "start": "1469960",
    "end": "1475520"
  },
  {
    "text": "It doesn't have to be 1 over N.\nIt can just be any alpha here. So any sort of alpha here\nis just a learning rate.",
    "start": "1475520",
    "end": "1485669"
  },
  {
    "text": "And we're just smoothly\nupdating our estimate of what is the value function\nfor a particular state.",
    "start": "1485670",
    "end": "1490760"
  },
  {
    "text": " And we'll see lots, lots and\nlots of algorithms like this,",
    "start": "1490760",
    "end": "1497160"
  },
  {
    "text": "similar to probably what\nyou saw in machine learning. The key thing here is the\nestimates that we're using,",
    "start": "1497160",
    "end": "1504280"
  },
  {
    "text": "which we also might use\nthe reward targets often. We have this estimate here, and\nthen we have our old estimate.",
    "start": "1504280",
    "end": "1513440"
  },
  {
    "text": "So this is one sample\nof what is the return, starting in this state till\nthe end of the episode.",
    "start": "1513440",
    "end": "1518660"
  },
  {
    "text": " So I think it's helpful to\nthink a little bit about what",
    "start": "1518660",
    "end": "1524340"
  },
  {
    "text": "this looks like pictorially. And this also relates a lot to-- we'll see this when we talk\nabout things like AlphaGo.",
    "start": "1524340",
    "end": "1529955"
  },
  {
    "text": " So I think it's helpful today. And it'll look somewhat\nfamiliar to you",
    "start": "1529955",
    "end": "1535600"
  },
  {
    "text": "if you've seen things like\nminimax trees or expectimax trees. Who here has seen\nexpectimax trees before?",
    "start": "1535600",
    "end": "1543161"
  },
  {
    "text": "OK, so maybe one person. So most people, not. So this might be a\nuseful representation.",
    "start": "1543161",
    "end": "1548750"
  },
  {
    "text": "So I think one way to\nthink about this is, what we're trying to\ndo is we're trying to think of what the value is\nstarting in a certain state.",
    "start": "1548750",
    "end": "1555790"
  },
  {
    "text": "And we know what\nthe action is we're going to take because\nwe've got to fix policy.",
    "start": "1555790",
    "end": "1560930"
  },
  {
    "text": "So that says we start\nin this state s. We take this action a. This is prescribed\nby our policy,",
    "start": "1560930",
    "end": "1566540"
  },
  {
    "text": "so this is pi of s\nis going to equal.",
    "start": "1566540",
    "end": "1571750"
  },
  {
    "text": "And then after we do that,\nbecause the world might be stochastic, we're going to\nhave a bunch of next states we could reach.",
    "start": "1571750",
    "end": "1578110"
  },
  {
    "text": "So we have probability of\ns prime, given s and a.",
    "start": "1578110",
    "end": "1583809"
  },
  {
    "text": "And what we're trying to do\nwhen we do policy evaluation is we're trying to get an\nexpectation over all",
    "start": "1583810",
    "end": "1589630"
  },
  {
    "text": "the potential futures\nwe might end up in by following this policy. Maybe in some cases, there's\nreally good patient outcomes,",
    "start": "1589630",
    "end": "1596143"
  },
  {
    "text": "hopefully, most of the time. And maybe sometimes, there's\nless good patient outcomes. And we want to do an\nexpectation over all of this.",
    "start": "1596143",
    "end": "1602289"
  },
  {
    "text": "So we can think\nof that as a tree, as just, we start in a\nstate, we take an action, we look at the branching factor\nof all possible next states,",
    "start": "1602290",
    "end": "1609170"
  },
  {
    "text": "and then we repeat. Excuse me. So if we think of what the\npolicy evaluation diagram is",
    "start": "1609170",
    "end": "1615610"
  },
  {
    "text": "doing, for each state, we know\nwhat the next action is we take. And then we branch again\nin terms of states.",
    "start": "1615610",
    "end": "1623019"
  },
  {
    "text": "So this is like s prime,\nand this is s double prime. ",
    "start": "1623020",
    "end": "1633280"
  },
  {
    "text": "And so we can just think of this\ntree of possibilities going out. But we don't have any there's\nno branching on the actions",
    "start": "1633280",
    "end": "1639370"
  },
  {
    "text": "because the actions are\nfixed by our policy. So then if we go all the way\nout, and then what we want to do",
    "start": "1639370",
    "end": "1645320"
  },
  {
    "text": "is we want to figure out\nwhat the value function is, you can think of this\nas an expensive way to do dynamic programming.",
    "start": "1645320",
    "end": "1651440"
  },
  {
    "text": "What you would do\nis you would take an expectation over the\nstates, and then you would propagate those\nvalues back up to the root.",
    "start": "1651440",
    "end": "1659570"
  },
  {
    "text": "And if you don't find this\na useful conceptual way to think about it, it's fine. I think it can be\nhelpful to then think",
    "start": "1659570",
    "end": "1664639"
  },
  {
    "text": "about what these different\nalgorithms are doing in terms of approximations. So this is what we would like\nto do in order to get v pi of s.",
    "start": "1664640",
    "end": "1673808"
  },
  {
    "text": "But we want to do\nthis in a much more computationally efficient way,\nand also, sample efficient way.",
    "start": "1673808",
    "end": "1680919"
  },
  {
    "text": "So what Monte Carlo policy\nevaluation is going to do is it's going to look like\nthat particular equation here.",
    "start": "1680920",
    "end": "1687340"
  },
  {
    "text": "And what it is\ndoing here is it is going to approximate these\nfull expectations via sample.",
    "start": "1687340",
    "end": "1697799"
  },
  {
    "text": "So in particular,\nwhat it's doing here is it's updating\nthe value estimate by using a sample of the return\nto approximate an expectation.",
    "start": "1697800",
    "end": "1706356"
  },
  {
    "text": " And we do this many times. We average over\nmany such returns.",
    "start": "1706356",
    "end": "1711980"
  },
  {
    "text": "So it's kind of\nlike saying you have this enormous branching tree. You could do an expectation\nover all of that explicitly up",
    "start": "1711980",
    "end": "1718460"
  },
  {
    "text": "from the roots, or you could\njust sample many times, and that's also going\nto approximate the tree.",
    "start": "1718460",
    "end": "1723559"
  },
  {
    "text": "And the more samples\nyou get, the better that's going to be as an\napproximation of the tree.",
    "start": "1723560",
    "end": "1728970"
  },
  {
    "text": "And this type of idea has been\nused in many different types of algorithms. There's some really nice work in\nthe mid 2000s by Michael Kearns",
    "start": "1728970",
    "end": "1736919"
  },
  {
    "text": "and others. And then similar\nideas were really the foundation that\nthen led to some of the advances of\nMonte Carlo tree search,",
    "start": "1736920",
    "end": "1743250"
  },
  {
    "text": "and then that went into AlphaGo. So this is what Monte\nCarlo tree search is doing.",
    "start": "1743250",
    "end": "1750690"
  },
  {
    "text": "So notice, it's not doing\nany form of bootstrapping. There's no dynamic programming\nthat's going on here.",
    "start": "1750690",
    "end": "1756070"
  },
  {
    "text": "It's just rolling out. And then what we have here\nis, it is using this here,",
    "start": "1756070",
    "end": "1765666"
  },
  {
    "text": "this sample as an approximation. OK?",
    "start": "1765666",
    "end": "1771370"
  },
  {
    "text": "All right. So that's how Monte Carlo\npolicy evaluation works. One natural question\nin this case is, how good is that estimate.",
    "start": "1771370",
    "end": "1777227"
  },
  {
    "text": "So we're going to see\nlots of different ways and lots of algorithms for\ntrying to do policy evaluation.",
    "start": "1777227",
    "end": "1782400"
  },
  {
    "text": "And so you might now ask,\nwell, how do I pick among them. What are the properties\nI should think about?",
    "start": "1782400",
    "end": "1787909"
  },
  {
    "text": "So one pretty basic\nproperty that you might want is consistency, which\nmeans that, as you",
    "start": "1787910",
    "end": "1793250"
  },
  {
    "text": "get more and more data,\ndoes your estimate actually converge to the true value of\nthe policy for all the states.",
    "start": "1793250",
    "end": "1803473"
  },
  {
    "text": "And this is something\nyou probably want in many cases, at\nleast, because otherwise, it means that, even if\nyou had infinite data,",
    "start": "1803473",
    "end": "1809549"
  },
  {
    "text": "your estimate is still\ngoing to be wrong. Now, as we start to think about\nmore complicated settings, we might have to be satisfied\nwith this less good objective.",
    "start": "1809550",
    "end": "1819510"
  },
  {
    "text": "But here, for right\nnow, we're hoping we can just write down\nthe value of every state as an entry in a\ntable that we should",
    "start": "1819510",
    "end": "1826279"
  },
  {
    "text": "be able to get consistency. A second thing we might want\nis computational efficiency.",
    "start": "1826280",
    "end": "1831980"
  },
  {
    "text": "We'd like this not to be too\nexpensive for us to compute. We'd like us not to\nrequire too much memory.",
    "start": "1831980",
    "end": "1838190"
  },
  {
    "text": "And we'd like it to have\nstatistical efficiency, which is, essentially, how does the\naccuracy of the estimate change",
    "start": "1838190",
    "end": "1845720"
  },
  {
    "text": "with the amount of data. And what that means\nhere is, more formally, we'd like to know how quickly\ndo these things converge",
    "start": "1845720",
    "end": "1853490"
  },
  {
    "text": "as you get more and more data. And then in reality, we often\ncare about empirical accuracy,",
    "start": "1853490",
    "end": "1859160"
  },
  {
    "text": "just what is our mean\nsquared error for our types of our estimators. So how good is Monte Carlo?",
    "start": "1859160",
    "end": "1865590"
  },
  {
    "text": "Well, let's just first\nquickly remind ourselves that the bias of an\nestimator is this.",
    "start": "1865590",
    "end": "1871260"
  },
  {
    "text": "So if we have an\nestimator theta, which we're going to be thinking\nof as our value function",
    "start": "1871260",
    "end": "1876840"
  },
  {
    "text": "approximation, it's going to\nbe the difference between, on average, what our estimator\nis versus the true value.",
    "start": "1876840",
    "end": "1883370"
  },
  {
    "text": "That's our bias. And the variance of an\nestimator is the difference between this and its\nexpectation squared,",
    "start": "1883370",
    "end": "1891210"
  },
  {
    "text": "the expectation of that. And the mean squared\nerror is going to be variance\nplus bias squared. ",
    "start": "1891210",
    "end": "1898752"
  },
  {
    "text": "So generally, you\nwould like an estimator that has low mean\nsquared error, which means we want it to have low\nor zero bias and low variance.",
    "start": "1898752",
    "end": "1904664"
  },
  {
    "text": " Something to think about if\nyou're less familiar with these,",
    "start": "1904665",
    "end": "1910140"
  },
  {
    "text": "is whether or not if an\nestimator is unbiased, is it consistent. It is not necessarily\nconsistent, just so you know.",
    "start": "1910140",
    "end": "1918750"
  },
  {
    "text": "So what we would like here\nis that, asymptotically, the probability that\nour estimator-- so N",
    "start": "1918750",
    "end": "1925195"
  },
  {
    "text": "here is the amount of data\nwe're using to construct that estimator-- the\nprobability that, as we get",
    "start": "1925195",
    "end": "1930649"
  },
  {
    "text": "an infinite amount of\ndata, that our estimate is different than the true\nvalue by more than epsilon.",
    "start": "1930650",
    "end": "1936360"
  },
  {
    "text": "It has to go to 0.  OK, so we would like\nit to be consistent.",
    "start": "1936360",
    "end": "1942580"
  },
  {
    "text": "So how does Monte Carlo fare\non these sort of properties? Well, first visit is unbiased.",
    "start": "1942580",
    "end": "1951810"
  },
  {
    "text": "So it's an unbiased\nestimate of the true policy. And by the law of large\nnumbers, as the amount of data",
    "start": "1951810",
    "end": "1956880"
  },
  {
    "text": "you have goes to\ninfinity per state, so if you have\nreally rare states, you're still going to\nneed a number of samples",
    "start": "1956880",
    "end": "1963559"
  },
  {
    "text": "to estimate them. But as the amount of data\nyou have goes to infinity, you'll converge. So it's consistent\nand it's unbiased.",
    "start": "1963560",
    "end": "1971800"
  },
  {
    "text": "Every visit Monte\nCarlo is biased. One way to think about\nthat is, in the first case,",
    "start": "1971800",
    "end": "1978679"
  },
  {
    "text": "all your data is IID,\nindependent and identically distributed, in that\nevery visit case--",
    "start": "1978680",
    "end": "1984679"
  },
  {
    "text": "imagine that you visit state s2,\nand then four steps later, you visit s2.",
    "start": "1984680",
    "end": "1990340"
  },
  {
    "text": "Well, their returns are going\nto be correlated because they're both in the same trajectory,\nso they're not IID anymore.",
    "start": "1990340",
    "end": "1997155"
  },
  {
    "text": "So that's just some intuition\nfor why it might be biased. But it's also\nconsistent and it often has better mean squared\nerror because you",
    "start": "1997155",
    "end": "2003000"
  },
  {
    "text": "get to use more of your data\ninside of a single trajectory to do more updates.",
    "start": "2003000",
    "end": "2008280"
  },
  {
    "text": "And then incremental\nMonte Carlo methods depend on the learning\nrate, as you might expect.",
    "start": "2008280",
    "end": "2015519"
  },
  {
    "text": "So see that here? So let's imagine\nthat we are going",
    "start": "2015520",
    "end": "2020950"
  },
  {
    "text": "to have our alpha parameter,\nwhich is our learning rate, which is trading off\nbetween our new estimate and our old estimate.",
    "start": "2020950",
    "end": "2028340"
  },
  {
    "text": "It can actually\nchange per time step. So just like how\nyou can generally decay your learning rate, you\ncan change your learning rate",
    "start": "2028340",
    "end": "2033590"
  },
  {
    "text": "here. And if your learning\nrate is such that if you sum up all of its\nvalues for a particular state,",
    "start": "2033590",
    "end": "2039600"
  },
  {
    "text": "it goes to infinity, but the\nsquare is less than infinity, then you will converge\nto the true value.",
    "start": "2039600",
    "end": "2045100"
  },
  {
    "text": " And again, these are pretty\ncommon types of criteria",
    "start": "2045100",
    "end": "2053350"
  },
  {
    "text": "we'll see for some\nof the algorithms we have that, under\nsome sort of smoothness, guarantees for the\nlearning rates we'll",
    "start": "2053350",
    "end": "2060563"
  },
  {
    "text": "have some decent properties. Yeah, remind me of your name. If those conditions aren't\nmet, do you definitely",
    "start": "2060563",
    "end": "2067454"
  },
  {
    "text": "not have a guarantee, or\nare there other conditions that can give you a\nguarantee, and those are just some other queries?",
    "start": "2067455",
    "end": "2072790"
  },
  {
    "text": "Great question. So he's asking, is it required\nto have these conditions.",
    "start": "2072790",
    "end": "2079330"
  },
  {
    "text": "These are sufficient. They aren't necessary always. A lot of that will depend on the\nparticular problem domain, too,",
    "start": "2079330",
    "end": "2086330"
  },
  {
    "text": "and what the dynamics\nand the reward is. To my knowledge, I'm\nnot sure if there are other really general\nconditions like that,",
    "start": "2086330",
    "end": "2092299"
  },
  {
    "text": "but there might be for\nspecific problem classes. It's a good question. ",
    "start": "2092300",
    "end": "2099463"
  },
  {
    "text": "Now, one of the\nproblems with this is that, in general, it's a\npretty high variance estimator. So you're kind of getting,\ncertainly, with every visit,",
    "start": "2099463",
    "end": "2107690"
  },
  {
    "text": "or certainly, for first\nvisit Monte Carlo, you're only updating the state\nat most once per episode.",
    "start": "2107690",
    "end": "2115060"
  },
  {
    "text": "So it can take a long time. So you can imagine that, if you\nhave very different outcomes from the same starting state,\nso maybe most of the time,",
    "start": "2115060",
    "end": "2124100"
  },
  {
    "text": "you have pretty average\noutcomes, but maybe one in 100 times you have\na really bad outcome. It's going to take a long time\nfor that estimator to converge.",
    "start": "2124100",
    "end": "2131660"
  },
  {
    "text": "So in general, this is a\npretty high variance estimator, even though it is often\nunbiased and it is consistent.",
    "start": "2131660",
    "end": "2139370"
  },
  {
    "text": "And then the other\nbig requirement is that it requires\nepisodic settings. So you have to wait till\nthe end of the episode",
    "start": "2139370",
    "end": "2145310"
  },
  {
    "text": "to update your estimate. And for here right now, that\nmight not seem that bad. But when we start getting into\ncontrol and decision making,",
    "start": "2145310",
    "end": "2153049"
  },
  {
    "text": "you might want to use the data\nyou have already in that episode to change the\nbehavior of the agent.",
    "start": "2153050",
    "end": "2159140"
  },
  {
    "text": "So you can imagine\nsomething like, if you're doing self driving\ncars or something,",
    "start": "2159140",
    "end": "2164789"
  },
  {
    "text": "you're already getting some\nevidence that the car is not working as expected within\na single episode, that",
    "start": "2164790",
    "end": "2170120"
  },
  {
    "text": "might be really long. You might want to\nuse that information to change how you're\nsteering, for example. ",
    "start": "2170120",
    "end": "2177800"
  },
  {
    "text": "All right. So just to summarize\nhere, what it does is it's not using\nthe Markov process. It's updating your\nvalue function estimate,",
    "start": "2177800",
    "end": "2184230"
  },
  {
    "text": "using a sample of the return\nto approximate the expectation. And under some pretty\nmild conditions,",
    "start": "2184230",
    "end": "2189670"
  },
  {
    "text": "it converges to the\ntrue value of the state. And in some cases,\nit will turn out",
    "start": "2189670",
    "end": "2196460"
  },
  {
    "text": "that, even if you actually\nknow the true dynamics model and reward, you might\nstill want to do this. ",
    "start": "2196460",
    "end": "2203057"
  },
  {
    "text": "And I think one thing that's\nuseful to think about here is systems which you think\nthe Markov property might",
    "start": "2203058",
    "end": "2209040"
  },
  {
    "text": "be violated, at least\nwith the features that you'd be using to\nrepresent the state. ",
    "start": "2209040",
    "end": "2216640"
  },
  {
    "text": "All right. Now let's go on to temporal\ndifference learning. And this is, again, sort\nof related to Q-learning,",
    "start": "2216640",
    "end": "2221840"
  },
  {
    "text": "which we'll get to\nin the next lecture. So Sutton and Barto,\nwhich is a textbook that",
    "start": "2221840",
    "end": "2228320"
  },
  {
    "text": "is an optional one for-- yeah. ",
    "start": "2228320",
    "end": "2233552"
  },
  {
    "text": "I had a quick question. So if we don't\nthe rewards model, how do we calculate the\nrewards for the trajectory?",
    "start": "2233552",
    "end": "2239825"
  },
  {
    "text": "Great question. So the assumption\nhere is that it's kind of like you either\nare in a real setting",
    "start": "2239825",
    "end": "2246290"
  },
  {
    "text": "where you can sample\nthese from an oracle, or something in the real\nworld is giving you these. So you may not have an explicit\nrepresentation for the reward",
    "start": "2246290",
    "end": "2252942"
  },
  {
    "text": "model, but you can get them, so\nif your customer buys something or they don't, or you\nhave a side effect.",
    "start": "2252942",
    "end": "2258567"
  },
  {
    "text": "So you don't necessarily\nhave a parametric model, but you are getting\nreal rewards. That's a good question.",
    "start": "2258568",
    "end": "2265268"
  },
  {
    "text": "Anybody else have any other\nquestions about Monte Carlo before we go on to temporal\ndifference learning?",
    "start": "2265268",
    "end": "2271453"
  },
  {
    "text": "And I'm going to call it just\ntemporal difference learning now, and then I'll\nspecify that it's actually TD0 for most of what\nI'm going to talk about.",
    "start": "2271453",
    "end": "2279220"
  },
  {
    "text": "So I'll just specify,\nmostly discuss TD0.",
    "start": "2279220",
    "end": "2286099"
  },
  {
    "text": "And I'll specify what I\nmean by the 0 shortly. So Sutton and Barto, which is\none of the optional textbooks",
    "start": "2286100",
    "end": "2292190"
  },
  {
    "text": "for the class, says, if one had\nto identify one idea as central and novel to RL, it\nwould undoubtedly be",
    "start": "2292190",
    "end": "2298069"
  },
  {
    "text": "temporal difference learning. And what their point is, is\nthat it really is sort of a way",
    "start": "2298070",
    "end": "2304280"
  },
  {
    "text": "still to construct estimators,\nboth for control and for policy evaluation.",
    "start": "2304280",
    "end": "2309660"
  },
  {
    "text": "And the idea is, if we think\nback to that tree I showed you, and I'll show you\nsome more, there's going to be a way to combine\nbetween the idea of sampling",
    "start": "2309660",
    "end": "2317180"
  },
  {
    "text": "to approximate expectations\nand bootstrapping to approximate future returns.",
    "start": "2317180",
    "end": "2323210"
  },
  {
    "text": "And we'll see that in a second. It is model free,\nmeaning you don't need to have a parametric\nrepresentation of the reward",
    "start": "2323210",
    "end": "2329180"
  },
  {
    "text": "function or the dynamics model. And the nice thing is you can\nuse it in episodic settings, or in infinite discounted\nhorizon settings.",
    "start": "2329180",
    "end": "2336750"
  },
  {
    "text": "You just set off your\nrobot, and then it's just going to have to\nlearn to act forever.",
    "start": "2336750",
    "end": "2342757"
  },
  {
    "text": "And one of the key\nideas is that we're going to update our estimates\nof the value of a state immediately.",
    "start": "2342757",
    "end": "2347875"
  },
  {
    "text": "So I'll put pi here\nbecause we're still talking about a policy after\nevery single tuple of state",
    "start": "2347875",
    "end": "2353340"
  },
  {
    "text": "action reward next state. So let's see how that works.",
    "start": "2353340",
    "end": "2359550"
  },
  {
    "text": "So again, remember,\nour goal is just to compute the expected\ndiscounted sum of rewards for a particular policy.",
    "start": "2359550",
    "end": "2366330"
  },
  {
    "text": "Now, let's think back\nto the Bellman operator. So if we know the MDP models,\nand we have a particular policy,",
    "start": "2366330",
    "end": "2372010"
  },
  {
    "text": "we could write the Bellman\noperator like that. And what we were doing in\nincremental every visit",
    "start": "2372010",
    "end": "2377810"
  },
  {
    "text": "Monte Carlo is we were\nupdating the estimate using one sample of the return.",
    "start": "2377810",
    "end": "2384290"
  },
  {
    "text": "And the idea now is to say,\nwell, this was one sample.",
    "start": "2384290",
    "end": "2389300"
  },
  {
    "text": "But maybe we could just\nmaintain-- we have access to a value function. Why couldn't we\nlook up and instead",
    "start": "2389300",
    "end": "2394580"
  },
  {
    "text": "of having what the\nrewards were starting the state till the\nend of the trajectory,",
    "start": "2394580",
    "end": "2400190"
  },
  {
    "text": "we observed a particular reward. We got to a\nparticular next state.",
    "start": "2400190",
    "end": "2406559"
  },
  {
    "text": "Why don't we use the value\nfunction for that state? ",
    "start": "2406560",
    "end": "2412580"
  },
  {
    "text": "So what we're doing in this\ncase is, instead of using G, we're plugging in the\nimmediate reward plus gamma",
    "start": "2412580",
    "end": "2417980"
  },
  {
    "text": "times the discounted\nsum of future rewards, using our current estimate\nof the value function for that next state we reached.",
    "start": "2417980",
    "end": "2424495"
  },
  {
    "text": " And here, the reason,\none of the things",
    "start": "2424495",
    "end": "2430920"
  },
  {
    "text": "is that we don't have to wait. We can do this immediately,\nas soon as we reach s prime.",
    "start": "2430920",
    "end": "2441790"
  },
  {
    "text": "So as soon as we\nreach s prime, as soon as we see the next\nstate, we can immediately update the value of\nour current state.",
    "start": "2441790",
    "end": "2447997"
  },
  {
    "text": "So we'll have to wait till\nthe end of the episode. We can use this for\ninfinite horizon problems. ",
    "start": "2447997",
    "end": "2456880"
  },
  {
    "text": "So this is what that looks like. And we're also going to\ncall that the TD target.",
    "start": "2456880",
    "end": "2463647"
  },
  {
    "text": "And again, that should\nlook like machine learning, and it should look like what\nwe just did with Monte Carlo",
    "start": "2463647",
    "end": "2469000"
  },
  {
    "text": "that what we're plugging\nin here is we're saying, we're taking our old estimate. And we are moving it.",
    "start": "2469000",
    "end": "2475131"
  },
  {
    "text": "We are shifting it a\nlittle bit by our learning rate towards our target,\nwhich is our reward,",
    "start": "2475132",
    "end": "2482120"
  },
  {
    "text": "plus our discount sum\nof future rewards, using that plug-in estimate.",
    "start": "2482120",
    "end": "2488349"
  },
  {
    "text": "And when we think of how much\nour estimate is changing, we often call that\nthe TD0 error, which",
    "start": "2488350",
    "end": "2496660"
  },
  {
    "text": "looks at how different\nis my current estimate of the value of a state\nversus the estimate",
    "start": "2496660",
    "end": "2502990"
  },
  {
    "text": "that I'm plugging in. And again, if you've\nseen Q-learning before,",
    "start": "2502990",
    "end": "2508190"
  },
  {
    "text": "this is going to look really\nsimilar to what we had, but there's no max\nor things like that. You'll see those soon.",
    "start": "2508190",
    "end": "2513810"
  },
  {
    "text": " So the TD0 learning algorithm\njust looks like the following.",
    "start": "2513810",
    "end": "2523110"
  },
  {
    "text": "You sample a tuple of a state\naction rewards next state. You update the value for that\nstarting state, and you repeat.",
    "start": "2523110",
    "end": "2532150"
  },
  {
    "text": "And so your t goes to t plus 1,\nand then you get the next tuple.",
    "start": "2532150",
    "end": "2537539"
  },
  {
    "text": "You just do this over,\nand over, and over again. So in our Mars Rover example,\nyou have state action reward,",
    "start": "2537540",
    "end": "2542860"
  },
  {
    "text": "next state, you update, and\nthen you just shift along. ",
    "start": "2542860",
    "end": "2551390"
  },
  {
    "text": "Let's see what that\nmight look like here. So in this case, let's imagine\nwe have a policy where we always",
    "start": "2551390",
    "end": "2558790"
  },
  {
    "text": "take action a1. We're going to make our discount\nfactor 1 to make the math easy,",
    "start": "2558790",
    "end": "2564700"
  },
  {
    "text": "and we're going to assume\nthat any action from state 1 or s7 terminates the episode.",
    "start": "2564700",
    "end": "2570750"
  },
  {
    "text": "And then what we\nsee in this case is, we have the\nfollowing trajectory. We start in state s3,\nwe take action a1.",
    "start": "2570750",
    "end": "2577030"
  },
  {
    "text": "We get a reward of 0. So this is the reward. We transition to\nstate s2, and so forth",
    "start": "2577030",
    "end": "2583320"
  },
  {
    "text": "till the end of the episode. So what we would\nhave in this case is that we would make it so that\nthe first update we would do",
    "start": "2583320",
    "end": "2592960"
  },
  {
    "text": "be V of s3. And what we would say is\nthat's my old estimate of V of s3 times 1 minus alpha.",
    "start": "2592960",
    "end": "2601400"
  },
  {
    "text": "I've just rewritten\nthe above equation here because this was basically\n1, and this is alpha times",
    "start": "2601400",
    "end": "2607770"
  },
  {
    "text": "minus V. Plus alpha times\nthe immediate reward plus gamma times V of s2.",
    "start": "2607770",
    "end": "2613680"
  },
  {
    "text": " So that's what that\nwould look like.",
    "start": "2613680",
    "end": "2619260"
  },
  {
    "text": "And here, imagine that I've\ninitialized all of them",
    "start": "2619260",
    "end": "2624950"
  },
  {
    "text": "to be 0 to start. So this would still\njust look like 0.",
    "start": "2624950",
    "end": "2630480"
  },
  {
    "text": "And, in fact, what\nwould be the only state I would update to not\nbe 0 in this episode?",
    "start": "2630480",
    "end": "2635540"
  },
  {
    "start": "2635540",
    "end": "2653810"
  },
  {
    "text": "For it to be\nupdated, not to be 0, either its immediate\nreward has to be 1, or it has to be transitioning\nto a state whose value is not 0.",
    "start": "2653810",
    "end": "2662079"
  },
  {
    "start": "2662080",
    "end": "2670500"
  },
  {
    "text": "Yeah. So what we're seeing here\nis that, in this case, we have state action\nreward next state.",
    "start": "2670500",
    "end": "2677170"
  },
  {
    "text": "And so this is the TD update. And what I was saying\nhere is that we've initialized all of them to be\n0, which means that, in order",
    "start": "2677170",
    "end": "2685230"
  },
  {
    "text": "for their value to\nchange from being 0, either their immediate\nreward has to be non-zero,",
    "start": "2685230",
    "end": "2691830"
  },
  {
    "text": "or we have to transition to\na state whose value is not 0, because all of them,\ntheir current value is 0.",
    "start": "2691830",
    "end": "2698720"
  },
  {
    "start": "2698720",
    "end": "2705670"
  },
  {
    "text": "Were you going to guess\nwhich state is up? Which one? Well, when you're in state\n1, you have a reward 1.",
    "start": "2705670",
    "end": "2714640"
  },
  {
    "text": "That's right. Yes, so you don't see any reward\nhere until you get to state s1.",
    "start": "2714640",
    "end": "2720460"
  },
  {
    "text": "I'll just highlight it here. So at that point is when you\nupdate your value function.",
    "start": "2720460",
    "end": "2726619"
  },
  {
    "text": "That's the first time that you\nget to anything that any reward becomes non-zero. So in that case,\nwhat you get is s1",
    "start": "2726620",
    "end": "2732970"
  },
  {
    "text": "is equal to V of s1, 1 minus\nalpha, plus alpha times",
    "start": "2732970",
    "end": "2738490"
  },
  {
    "text": "1 plus, gamma V of s terminal. s terminal is always 0.",
    "start": "2738490",
    "end": "2745030"
  },
  {
    "text": "So it just becomes\nalpha times 1. ",
    "start": "2745030",
    "end": "2750940"
  },
  {
    "text": "So why am I making you guys\ndo a lot of algebra here? I want to do it because,\nif you work this out,",
    "start": "2750940",
    "end": "2757098"
  },
  {
    "text": "and I won't go through\nit here, but I think it's a useful exercise,\nthe TD episode,",
    "start": "2757098",
    "end": "2763210"
  },
  {
    "text": "TD estimate you would\nget for your whole value function at the\nend of this episode is quite different than what\nyou get with Monte Carlo.",
    "start": "2763210",
    "end": "2770900"
  },
  {
    "text": "So TD updates after\nevery single tuple, every single state action\nreward next state tuple.",
    "start": "2770900",
    "end": "2776940"
  },
  {
    "text": "And so that means, when you\nreach the end of the episode, if you look at what your\nvalue function would be, and I've written the value\nfunction here just as a vector,",
    "start": "2776940",
    "end": "2784920"
  },
  {
    "text": "but this is the value of s1. This is the value of s7. So I've just written\nit as a vector. This is what your value\nfunction would be.",
    "start": "2784920",
    "end": "2792140"
  },
  {
    "text": "It would say, my current\nestimate for s1 is 1, and everything else is 0. But if you look at\nfirst visit Monte Carlo,",
    "start": "2792140",
    "end": "2799650"
  },
  {
    "text": "it's quite different. And if we make gamma equal to 1\nhere, which I said it would be,",
    "start": "2799650",
    "end": "2804870"
  },
  {
    "text": "it would be 1,1, 1, 0, 0, 0, 0. Why is this? Because Monte Carlo waits till\nthe end of the episode, and then",
    "start": "2804870",
    "end": "2812900"
  },
  {
    "text": "it uses the returns to update\nany state that was visited once in that episode.",
    "start": "2812900",
    "end": "2820070"
  },
  {
    "text": "And the reason that's\nimportant is that, now, actually, we filled in a lot\nmore things because we knew.",
    "start": "2820070",
    "end": "2825270"
  },
  {
    "text": "We observed in that\ncase that, not just did we get a reward here, but\nthen we saw what s2 got,",
    "start": "2825270",
    "end": "2830310"
  },
  {
    "text": "which was also a reward\nof 1 and what s3 got, which was also a reward of 1. And the reason I\nbring this up is",
    "start": "2830310",
    "end": "2836299"
  },
  {
    "text": "that there's going to be\ndifferent choices about how these behave,\nparticularly when you don't have a lot of data\nat the beginning, which",
    "start": "2836300",
    "end": "2841360"
  },
  {
    "text": "may be more or less data\nefficient or sample efficient. And ideas of sample\nefficiency will come up a lot.",
    "start": "2841360",
    "end": "2847803"
  },
  {
    "text": "We'll see that a lot\nlater on, but we'll see it on Thursday, as well.",
    "start": "2847803",
    "end": "2852890"
  },
  {
    "text": "All right. So what does this look\nlike in terms of the tree? So if we go back\nto our tree, which",
    "start": "2852890",
    "end": "2858030"
  },
  {
    "text": "is like expanding out potential\nfutures, what we can see here is that TD is updating the value\nestimate using a sample of st",
    "start": "2858030",
    "end": "2865920"
  },
  {
    "text": "plus 1 to approximate\nan expectation. So in reality, if you're\ndoing dynamic programming,",
    "start": "2865920",
    "end": "2872260"
  },
  {
    "text": "you would want to do\na weighted expectation over all the next\nstates you could reach, weighted by the probability\nof getting there.",
    "start": "2872260",
    "end": "2879030"
  },
  {
    "text": "What TD is doing is it's\njust sampling one of those. And that sample is\nan approximation",
    "start": "2879030",
    "end": "2886380"
  },
  {
    "text": "of that expectation. So we're going from this\nto sampling the next state.",
    "start": "2886380",
    "end": "2893820"
  },
  {
    "text": "But similar to\ndynamic programming, it is then bootstrapping. So unlike Monte Carlo,\nwhich goes all the way out",
    "start": "2893820",
    "end": "2900030"
  },
  {
    "text": "to get a sample of\nthat value function, here, we're just plugging in\nV. So this part looks the same.",
    "start": "2900030",
    "end": "2906545"
  },
  {
    "text": " So TD does both sampling to\napproximate expectations,",
    "start": "2906545",
    "end": "2914010"
  },
  {
    "text": "and it bootstraps by using your\nexisting estimate of the value function. ",
    "start": "2914010",
    "end": "2925300"
  },
  {
    "text": "All right, so let's just do\na Check Your Understanding. So this is a poll.",
    "start": "2925300",
    "end": "2930592"
  },
  {
    "text": "So what I'd like\nyou to think about is how this learning\nrate might affect things. So whether different\nchoices of this",
    "start": "2930592",
    "end": "2937359"
  },
  {
    "text": "is going to weigh the TD\ntarget or more or less than the past V\nestimate, and what",
    "start": "2937360",
    "end": "2944620"
  },
  {
    "text": "might happen when your\nstate space is stochastic, meaning that when you\nstart in one state,",
    "start": "2944620",
    "end": "2950570"
  },
  {
    "text": "you might end up in\nmultiple next states. What does that mean about\nconvergence and the implication",
    "start": "2950570",
    "end": "2956350"
  },
  {
    "text": "for learning rates,\nas well as thinking about deterministic\nMarkov decision processes?",
    "start": "2956350",
    "end": "2962810"
  },
  {
    "text": "Deterministic Markov\ndecision processes, what I mean by that is that\np of s prime, given s, a,",
    "start": "2962810",
    "end": "2972789"
  },
  {
    "text": "is equal to 1 for\nexactly 1 s prime,",
    "start": "2972790",
    "end": "2979600"
  },
  {
    "text": "meaning that there's\nno stochasticity. When you're in a\nstate in action, you always go to one\nparticular next state.",
    "start": "2979600",
    "end": "2986460"
  },
  {
    "text": "So that's a deterministic\nMarkov decision process. So just take a few minutes\nnow and look into this.",
    "start": "2986460",
    "end": "2993339"
  },
  {
    "start": "2993340",
    "end": "3004340"
  },
  {
    "text": "And you should be able to\nselect all that are true. But if you can't, let me know.",
    "start": "3004340",
    "end": "3009920"
  },
  {
    "text": "You cannot? OK. All right. Well, then, again, these\nare only for your thoughts.",
    "start": "3009920",
    "end": "3016290"
  },
  {
    "text": "So just try to write down for\nyourself which of these you think are true, and then\nwe'll talk about in a second.",
    "start": "3016290",
    "end": "3021950"
  },
  {
    "text": "I'll check into\nthese for next time. ",
    "start": "3021950",
    "end": "3070090"
  },
  {
    "text": "I don't think we can\nselect multiple answers. Yes, that's what I just heard. Sorry about that. So I'll try to fix\nthat for next time.",
    "start": "3070090",
    "end": "3075730"
  },
  {
    "text": "Just try to have in your\nhead of which ones you think are correct, and I'll\nask you to compare with someone in a second. Thanks for letting me know.",
    "start": "3075730",
    "end": "3081625"
  },
  {
    "start": "3081625",
    "end": "3118320"
  },
  {
    "text": "All right, turn to your\nneighbor and check, and particularly,\nfocus on the last two",
    "start": "3118320",
    "end": "3126678"
  },
  {
    "text": "and see if you agree on\nyour answers for this. ",
    "start": "3126678",
    "end": "3153900"
  },
  {
    "text": "[INDISTINCT SPEECH] ",
    "start": "3153900",
    "end": "3357270"
  },
  {
    "text": "All right, great. I had some great discussions. OK, so for the\nfirst one, this is",
    "start": "3357270",
    "end": "3363819"
  },
  {
    "text": "going to be false because,\nif we have alpha equals 0,",
    "start": "3363820",
    "end": "3369380"
  },
  {
    "text": "then we don't care about\nthe TD target at all. It just totally drops\nout, so we never update.",
    "start": "3369380",
    "end": "3377590"
  },
  {
    "text": "In the second case, this\nis true because this means,",
    "start": "3377590",
    "end": "3383020"
  },
  {
    "text": "if alpha is equal to 1,\nthen this part and this part cancels out, and\nwe just have this.",
    "start": "3383020",
    "end": "3389810"
  },
  {
    "text": "So that means whenever we see\nan update, we always update. We totally change our\nestimate, potentially.",
    "start": "3389810",
    "end": "3396290"
  },
  {
    "text": "The third one is a\nlittle bit subtle. This is true. Does somebody want to give me an\nexample where this might occur?",
    "start": "3396290",
    "end": "3402220"
  },
  {
    "text": " Yeah. If you had two states\nwhere they just",
    "start": "3402220",
    "end": "3409530"
  },
  {
    "text": "keep pointing at each other,\nis that the case for this one? Yes. And in particular, if you could\ngo to either of those states",
    "start": "3409530",
    "end": "3417359"
  },
  {
    "text": "with some probability. Yeah. So I sometimes think of\nit as like a coin flip.",
    "start": "3417360",
    "end": "3425079"
  },
  {
    "text": "So imagine that you have one\nstate where, after this, you either go to a state. Maybe it's 50% probability\nyou get plus 1,",
    "start": "3425080",
    "end": "3432750"
  },
  {
    "text": "and 50% probability\nyou get minus 1. And then your\nproblem just resets.",
    "start": "3432750",
    "end": "3438800"
  },
  {
    "text": "So imagine, it's like\na really short problem. You start off, you get 0 reward,\nyou transition to a state, and then your episode resets.",
    "start": "3438800",
    "end": "3444780"
  },
  {
    "text": "So in this case,\neither on that round, you're going to get plus 1, or\nyou're going to get minus 1.",
    "start": "3444780",
    "end": "3450440"
  },
  {
    "text": "So you either get plus\n1 or minus 1 here, and so you'll just flip back\nand forth between the plus 1, minus 1, plus 1,\nminus 1, plus 1, minus 1.",
    "start": "3450440",
    "end": "3457400"
  },
  {
    "text": "So that's just to\nhighlight that, if you do have systems which\nare stochastic, the fact that, in your\ntarget, you are using",
    "start": "3457400",
    "end": "3463820"
  },
  {
    "text": "a single sample of\nthat stochasticity to approximate the\nexpectation can be bad.",
    "start": "3463820",
    "end": "3470150"
  },
  {
    "text": "But that does not mean-- and\nI guess this gets to, I think, that was asking before that,\nin many of these cases,",
    "start": "3470150",
    "end": "3479608"
  },
  {
    "text": "there's the cases where it might\nbe possible that this would happen, but it won't always.",
    "start": "3479608",
    "end": "3485370"
  },
  {
    "text": "So in this case, there do\nexist deterministic systems where, even if alpha is\nequal to 1, you can converge.",
    "start": "3485370",
    "end": "3493010"
  },
  {
    "text": "So again, think of something. I like often to think\nabout really small MDPs to get some intuition for this.",
    "start": "3493010",
    "end": "3498040"
  },
  {
    "text": "If you have a case where\nthere's just a terminal state and there's no more transitions,\nso you get to some point",
    "start": "3498040",
    "end": "3504780"
  },
  {
    "text": "where you always go\nto some terminal state and it's plus 10 there, and\nthere's no more updates,",
    "start": "3504780",
    "end": "3510119"
  },
  {
    "text": "then it's just plus 10. There's no more expectation. And in general, any case where,\nif there's no stochasticity,",
    "start": "3510120",
    "end": "3517040"
  },
  {
    "text": "and you're near the\nend, and there's no more stochasticity\nin that episode, those can be cases where\nyou'll still converge.",
    "start": "3517040",
    "end": "3524760"
  },
  {
    "text": "OK, great. So I encourage you to go through\nsome of the worked examples, if you want to, just\nto see some more",
    "start": "3524760",
    "end": "3531300"
  },
  {
    "text": "comparisons over the difference\nbetween Monte Carlo and TD methods in this case.",
    "start": "3531300",
    "end": "3537330"
  },
  {
    "text": "Just to summarize what\nwe're doing in TD learning, we are bootstrapping\nand sampling. We're sampling to\napproximate our expectation",
    "start": "3537330",
    "end": "3543600"
  },
  {
    "text": "over all the stochasticity. We're bootstrapping because we\ndon't want to use a full return.",
    "start": "3543600",
    "end": "3548760"
  },
  {
    "text": "We are, instead, taking\nV to approximate that. It can be used in episodic\nor infinite horizon settings.",
    "start": "3548760",
    "end": "3555280"
  },
  {
    "text": "It is generally lower variance\nfor doing lots, and lots, and lots of updates. It is a consistent estimator\nif your learning rate alpha",
    "start": "3555280",
    "end": "3562650"
  },
  {
    "text": "satisfies the same conditions\nspecified for incremental Monte Carlo policy evaluation.",
    "start": "3562650",
    "end": "3568990"
  },
  {
    "text": "I here today only\nintroduced TD0. What TD0 refers to is, you\ntake the immediate reward,",
    "start": "3568990",
    "end": "3575950"
  },
  {
    "text": "and then you immediately\nbootstrap and plug in the value of the next state.",
    "start": "3575950",
    "end": "3581720"
  },
  {
    "text": "So we did r plus-- ",
    "start": "3581720",
    "end": "3592420"
  },
  {
    "text": "we did r plus gamma V of\ns prime versus summing up",
    "start": "3592420",
    "end": "3598599"
  },
  {
    "text": "all your discounted rewards\nfor the whole episode. In general, you could have\nsomething kind of in between. So you could have plus rt\nplus 1 plus gamma squared.",
    "start": "3598600",
    "end": "3610660"
  },
  {
    "text": "So you could have\nsomething like this. So in general, you\ncould do some sort of combination of using partial\nreturns, and then bootstrapping.",
    "start": "3610660",
    "end": "3620260"
  },
  {
    "text": "There's a lot of\ndifferent TD methods that interpolate\nbetween taking one step and then plugging in the\nvalue versus only plugging,",
    "start": "3620260",
    "end": "3628000"
  },
  {
    "text": "not using any value\nfunction approximation. And if you want to think\nabout this graphically, it's kind of thinking about do\nyou plug-in v of s prime here,",
    "start": "3628000",
    "end": "3638090"
  },
  {
    "text": "or do you plug it in-- no, way lower.",
    "start": "3638090",
    "end": "3643405"
  },
  {
    "text": "Yeah.  Is there an empirical estimate\nof what a good trade off",
    "start": "3643405",
    "end": "3651670"
  },
  {
    "text": "for TD for computational\ncomplexity versus performance?",
    "start": "3651670",
    "end": "3657760"
  },
  {
    "text": "Have they found a\ngood number for it? Good question. Unfortunately, in many cases, it\nwill be depending on the domain.",
    "start": "3657760",
    "end": "3665980"
  },
  {
    "text": "One thing, I think, to\nthink about here, too, is that you can think of this\npart doesn't require the Markov",
    "start": "3665980",
    "end": "3672850"
  },
  {
    "text": "assumption. So if you have a system\nwhere you're not confident, but maybe you're\nlike, well, maybe I'm",
    "start": "3672850",
    "end": "3680800"
  },
  {
    "text": "willing to say that I'll plug-in\na Markov assumption eventually, because it's going\nto be lower variance, but I want to preserve the fact\nthat maybe it's not Markov,",
    "start": "3680800",
    "end": "3688299"
  },
  {
    "text": "then I sort of have\na short horizon. Often people do use\nsomething in between the two. So they often do consider this\nbetween for multiple reasons,",
    "start": "3688300",
    "end": "3696530"
  },
  {
    "text": "but it gives you some\nof this flexibility. It often is a lower bias. That's a great question.",
    "start": "3696530",
    "end": "3704400"
  },
  {
    "text": "All right. What we're going to do\nnow is think about also how some of these ideas relate\nto dynamic programming, which is what we saw in\nan earlier lecture,",
    "start": "3704400",
    "end": "3711260"
  },
  {
    "text": "because we could use this\nalso for policy evaluation. We know how to use it\nfor policy evaluation if we are given the models.",
    "start": "3711260",
    "end": "3718433"
  },
  {
    "text": "But some of you guys\nmight have been thinking, well, we have data now. If we have data,\nbecause we're taking",
    "start": "3718433",
    "end": "3723490"
  },
  {
    "text": "the policy in the\nenvironment, couldn't we use that to estimate\na reward model, or couldn't we use that to\nestimate the dynamics model?",
    "start": "3723490",
    "end": "3729940"
  },
  {
    "text": "And that's what's\nknown as certainty equivalence approaches. So the idea here is that\nyou're going to be getting data",
    "start": "3729940",
    "end": "3736900"
  },
  {
    "text": "as you execute this policy, and\nyou can compute a dynamics model from that data.",
    "start": "3736900",
    "end": "3742240"
  },
  {
    "text": "So you could use a maximum\nlikelihood MDP model. Remember, right now, we're\nin the tabular setting.",
    "start": "3742240",
    "end": "3748010"
  },
  {
    "text": "So we can have a parameter for\nevery single state in action. So we can just count. We can just say, how many\ntimes was I in this state,",
    "start": "3748010",
    "end": "3754540"
  },
  {
    "text": "took this action, and\ntransition to this next state, divided by the number of times\nI was in that state in action.",
    "start": "3754540",
    "end": "3760810"
  },
  {
    "text": "So this just gives you a\nmaximum likelihood estimate of the dynamics model, and\nyou can do the same thing",
    "start": "3760810",
    "end": "3766160"
  },
  {
    "text": "for the reward model. And of course, as\nyou might imagine, you can do this with much\nmore complicated function",
    "start": "3766160",
    "end": "3771405"
  },
  {
    "text": "approximators, like deep\nneural networks, too. But the idea is that,\nonce you have this model,",
    "start": "3771405",
    "end": "3777740"
  },
  {
    "text": "and it's called a certainty\nequivalence model because we're now going to ignore any\nerror in these models.",
    "start": "3777740",
    "end": "3784289"
  },
  {
    "text": "So we have finite data. These models will\ndefinitely be wrong, but let's ignore that for now. So once you have this\nmaximum likelihood MDP model,",
    "start": "3784290",
    "end": "3792480"
  },
  {
    "text": "you can just compute\nthe value of a policy, using the same methods we\nsaw last week, because you",
    "start": "3792480",
    "end": "3798740"
  },
  {
    "text": "have a dynamics model\nnow and a reward model. And you can see some\nexamples about this at the end of the\nlecture slides.",
    "start": "3798740",
    "end": "3805075"
  },
  {
    "start": "3805075",
    "end": "3810329"
  },
  {
    "text": "So one of the benefits of this,\nand this gets to the question, is this is really\ndata efficient.",
    "start": "3810330",
    "end": "3815510"
  },
  {
    "text": "So I showed you an\nexample for the Mars Rover before, where we only\nupdated one of the states",
    "start": "3815510",
    "end": "3820980"
  },
  {
    "text": "with TD learning. We updated three of the\nstates with Monte Carlo.",
    "start": "3820980",
    "end": "3826510"
  },
  {
    "text": "What this does here\nis it tries to update. It computes a\ndynamic small reward model for all\nstates and actions,",
    "start": "3826510",
    "end": "3832960"
  },
  {
    "text": "and then it tries to\nupdate all of them. OK, so it's going to compute a\nvalue for every single state.",
    "start": "3832960",
    "end": "3840160"
  },
  {
    "text": "Now, the downside of\nthat is that now we're doing policy evaluation with\na full model, which is either going to be something like s\nsquared a for iterative methods,",
    "start": "3840160",
    "end": "3847930"
  },
  {
    "text": "or maybe even worse. So it's computationally\nexpensive, but it's really data efficient.",
    "start": "3847930",
    "end": "3854890"
  },
  {
    "text": "Because as soon as you reach\nany state for which you get, say, a positive reward,\nyou can kind of propagate that",
    "start": "3854890",
    "end": "3861130"
  },
  {
    "text": "to any other state that is\npossible to reach from there. ",
    "start": "3861130",
    "end": "3866340"
  },
  {
    "text": "It's still consistent. It's going to converge to the\nright thing for Markov models,",
    "start": "3866340",
    "end": "3872780"
  },
  {
    "text": "and it can easily generally be\nused for all policy evaluation, which we're going to get into. Yeah.",
    "start": "3872780",
    "end": "3877800"
  },
  {
    "text": "Sorry. What is Nsa in this equation. Sorry. Great question. Nsa here is the number of\ntimes we've been in that state",
    "start": "3877800",
    "end": "3883763"
  },
  {
    "text": "and taken that action. Yeah, so this is counts. ",
    "start": "3883763",
    "end": "3899910"
  },
  {
    "text": "Yeah. It seems pretty\nsimilar to Monte Carlo. So is just the difference that\nyou're generating probability,",
    "start": "3899910",
    "end": "3908220"
  },
  {
    "text": "as opposed to calculating G? Great question. We're going to\nhold that thought. He's asking how similar\nis this to Monte Carlo.",
    "start": "3908220",
    "end": "3914858"
  },
  {
    "text": "It's actually going to\nbe pretty different. We're going to see\nthis in a second. We are using our data\nsimilar to Monte Carlo.",
    "start": "3914858",
    "end": "3922720"
  },
  {
    "text": "We're going to use the data\nhere to compute models and then propagate information. But they're going\nto end up making",
    "start": "3922720",
    "end": "3928930"
  },
  {
    "text": "some interesting\ndifferent decisions. So let's see that now. It's a great precursor.",
    "start": "3928930",
    "end": "3934150"
  },
  {
    "text": "OK. So let's get into batch\npolicy evaluation. So I've said there are\nthese different methods. They might have different\ncomputational complexity.",
    "start": "3934150",
    "end": "3941085"
  },
  {
    "text": "They might be more or\nless data efficient. So one thing that you\nmight imagine doing, and we'll see this a lot\nshortly, a lot more next time,",
    "start": "3941085",
    "end": "3948850"
  },
  {
    "text": "is, well, if I\nhave some data, how could I best use the\ndata that I have.",
    "start": "3948850",
    "end": "3953923"
  },
  {
    "text": "And this comes a lot up a lot in\nthe research that me and my lab do, because we're often dealing\nwith patient data, or student",
    "start": "3953923",
    "end": "3959260"
  },
  {
    "text": "data, or legal data,\nor others, where it's really expensive to get\nthe data, or it's costly, or it could be harmful.",
    "start": "3959260",
    "end": "3964910"
  },
  {
    "text": "And we want to get\nas much information as we can out of\nthe data we have. So when I say batch,\nwhat I mean is,",
    "start": "3964910",
    "end": "3970810"
  },
  {
    "text": "imagine that you have\na set of k episodes. And now what you\nwant to do is you want to do policy evaluation\njust with that data.",
    "start": "3970810",
    "end": "3979115"
  },
  {
    "text": "So what we're going\nto do is repeatedly sample one of the episodes\nthat we have of those k, and we're going to apply Monte\nCarlo or TD0 to that episode.",
    "start": "3979115",
    "end": "3987052"
  },
  {
    "text": "We're just going to do that\nover, and over, and over, and over again. And we'll see this a lot\nmore next time, as well.",
    "start": "3987052",
    "end": "3992700"
  },
  {
    "text": "And so the idea is\nto just understand, if we do this, given that finite\namount of data, what will Monte",
    "start": "3992700",
    "end": "3999250"
  },
  {
    "text": "Carlo and TD0\nconverge to, in terms of the evaluation of the policy.",
    "start": "3999250",
    "end": "4004410"
  },
  {
    "text": "So let's go through that. There's this really nice\nexample from Sutton and Barto to illustrate this.",
    "start": "4004410",
    "end": "4009780"
  },
  {
    "text": "We have a really small domain. We have two states. And we're going to say\ngamma is equal to 1. There's no discounting.",
    "start": "4009780",
    "end": "4015810"
  },
  {
    "text": "And we have eight\nepisodes of experience. So in one episode, we started in\nstate A, we got a reward of 0,",
    "start": "4015810",
    "end": "4025480"
  },
  {
    "text": "we transitioned to B, and\nwe got another reward of 0. OK? So in this case, you\ncan think of it as this.",
    "start": "4025480",
    "end": "4032430"
  },
  {
    "text": "You get a trajectory like that. In some episodes, we\nstarted in state B,",
    "start": "4032430",
    "end": "4039270"
  },
  {
    "text": "and we just got an\nimmediate reward of 1, and we observed that six times. So in six trajectories we just\nhappened to start in state B,",
    "start": "4039270",
    "end": "4046440"
  },
  {
    "text": "and we got a reward of 1. And then in one trajectory,\nwe started in state B, and we got our reward of 0.",
    "start": "4046440",
    "end": "4053930"
  },
  {
    "text": "So first, imagine if you ran\nTD updates over this data an infinite amount of time.",
    "start": "4053930",
    "end": "4059720"
  },
  {
    "text": "What do you think the\nestimate of VB would be? ",
    "start": "4059720",
    "end": "4065630"
  },
  {
    "text": "Try to remember what\nit works for there, as we have 1 minus alpha\ntimes our old estimate.",
    "start": "4065630",
    "end": "4072280"
  },
  {
    "text": "So that's VB plus alpha\ntimes our immediate reward, plus gamma, times\nthe next state.",
    "start": "4072280",
    "end": "4078830"
  },
  {
    "text": "But here, it's just terminal\nbecause we always terminate.",
    "start": "4078830",
    "end": "4085590"
  },
  {
    "text": "After B, we always terminate. So you never get any future\ndiscounted rewards in this case. So what the updates look\nlike for TD learning",
    "start": "4085590",
    "end": "4092528"
  },
  {
    "text": "is that you would have 1 minus\nalpha times your old estimate, plus alpha times whatever\nreward you get in B.",
    "start": "4092528",
    "end": "4099359"
  },
  {
    "text": "And imagine you just\niterate over these over, and over, and over again.",
    "start": "4099359",
    "end": "4104528"
  },
  {
    "text": "Somebody have any\nguesses of what the reward would be for V of B? ",
    "start": "4104529",
    "end": "4118410"
  },
  {
    "text": "Would it be 0.75? Yeah. Somebody else want to\nexplain now why it's 0.75?",
    "start": "4118410",
    "end": "4132989"
  },
  {
    "text": "The season mods? Yeah, because in this case,\nwe had eight episodes.",
    "start": "4132990",
    "end": "4138810"
  },
  {
    "text": "In two of them, when we\nstarted in B, we got 0. And in six of them, we got 1.",
    "start": "4138810",
    "end": "4146670"
  },
  {
    "text": "So we just average\nthose rewards. And imagine, we're just doing\nthis many, many, many, many,",
    "start": "4146670",
    "end": "4152170"
  },
  {
    "text": "many, many times. So eventually, you would just\nconverge to this estimate",
    "start": "4152170",
    "end": "4158160"
  },
  {
    "text": "being 0.75. What about for Monte Carlo?",
    "start": "4158160",
    "end": "4164799"
  },
  {
    "text": "So let's do Monte Carlo for V\nof B. What would that look like?",
    "start": "4164800",
    "end": "4170889"
  },
  {
    "text": "So remember, for\nMonte Carlo, it would be 1 minus alpha times V\nof B, plus alpha times G,",
    "start": "4170890",
    "end": "4180130"
  },
  {
    "text": "where you start in state B. Is\nit going to be the same thing?",
    "start": "4180130",
    "end": "4190540"
  },
  {
    "text": "Is it going to be different? ",
    "start": "4190540",
    "end": "4200200"
  },
  {
    "text": "So Monte Carlo, we're\naveraging over all the returns",
    "start": "4200200",
    "end": "4206320"
  },
  {
    "text": "we get starting in that state. ",
    "start": "4206320",
    "end": "4218409"
  },
  {
    "text": "Yeah. So when we start at B, so\nthen wouldn't it be 6 over 7?",
    "start": "4218410",
    "end": "4224020"
  },
  {
    "text": "6 over 8. 6 over 8, yeah. Yeah. But don't we start at\nA in the first one?",
    "start": "4224020",
    "end": "4230245"
  },
  {
    "text": "In one episode, we start\nin A. But we're just trying to compute the\nvalue of B right now. We'll get to A in\na second, yeah.",
    "start": "4230245",
    "end": "4235390"
  },
  {
    "text": "You're thinking ahead. Yeah. So the Monte Carlo estimate. So we're just\ntrying to contrast.",
    "start": "4235390",
    "end": "4241180"
  },
  {
    "text": "So just to recap,\nwhat we're trying to do here is we're\ntrying to see, will these two algorithms\nconverge to the same thing or not.",
    "start": "4241180",
    "end": "4246710"
  },
  {
    "text": "We're going to start off and\nlook at what the value would be of state B. In\nTD, we would converge",
    "start": "4246710",
    "end": "4251890"
  },
  {
    "text": "to 0.75 because the immediate\nreward is either 1 or 0,",
    "start": "4251890",
    "end": "4256960"
  },
  {
    "text": "and the discounted sum of future\nwoods is 0 because we terminate. And in Monte Carlo\nwe just average over",
    "start": "4256960",
    "end": "4263410"
  },
  {
    "text": "all the returns we get\nwhen we started in B, and that is also 6 divided by 8.",
    "start": "4263410",
    "end": "4269250"
  },
  {
    "text": "All right, so now\nthis is the hard one. OK. What about of V of A?",
    "start": "4269250",
    "end": "4275610"
  },
  {
    "text": "What will we converge\nto in these two cases? So let's do Check\nYour Understanding,",
    "start": "4275610",
    "end": "4281277"
  },
  {
    "text": "and you can respond in the poll. And feel free to talk\nto someone next to you. ",
    "start": "4281277",
    "end": "4286982"
  },
  {
    "text": "And again, the intent\nof this is to think about, are these actually\ncomputing the same thing or not in this case.",
    "start": "4286982",
    "end": "4292535"
  },
  {
    "text": " And remember, this is\na different setting than what I told you before,\nthat both of these things",
    "start": "4292535",
    "end": "4299160"
  },
  {
    "text": "can be consistent. But that was if you\nget infinite data. What this is looking\nat is, if you only",
    "start": "4299160",
    "end": "4304320"
  },
  {
    "text": "have a finite amount of\ndata, and you just go over it over, and over again, either\nwith Monte Carlo updates",
    "start": "4304320",
    "end": "4310050"
  },
  {
    "text": "or with TD, will you\nconverge to the same thing. ",
    "start": "4310050",
    "end": "4316250"
  },
  {
    "text": "And if you're not sure\nor you're confused, feel free to put that\nin the poll, too. ",
    "start": "4316250",
    "end": "4399960"
  },
  {
    "text": "There's lots of\ndifferent answers here. So this is a great one to\ntalk to somebody nearby you. So why don't you see if you're\ngetting the same things?",
    "start": "4399960",
    "end": "4407010"
  },
  {
    "text": "And we can use our\ncollective intelligence. ",
    "start": "4407010",
    "end": "4417228"
  },
  {
    "text": "[INDISTINCT SPEECH] ",
    "start": "4417228",
    "end": "4504582"
  },
  {
    "text": "OK, I'm hearing a lot\nof good discussion, so I'm sorry to interrupt. But this is kind of a fun one. So let's start with TD.",
    "start": "4504582",
    "end": "4513420"
  },
  {
    "text": "So someone want to explain\nwhy it's 0.75 for TD?",
    "start": "4513420",
    "end": "4518733"
  },
  {
    "text": "There are multiple\npeople that got that.  Yeah, would you explain what you\nand your partner were saying?",
    "start": "4518733",
    "end": "4525690"
  },
  {
    "text": "Yeah. So I think if you just-- so we're just looking at this.",
    "start": "4525690",
    "end": "4531100"
  },
  {
    "text": "And remind me your name. Sorry. Yeah. There's this one episode where\nwe're at A. So in that episode,",
    "start": "4531100",
    "end": "4540100"
  },
  {
    "text": "the immediate reward\nis 0, but then we have to do plus gamma times the\nreward of the next state, B.",
    "start": "4540100",
    "end": "4547270"
  },
  {
    "text": "And then V pi of B is 0.75. We got it in the previous part. So the value is 0.75.",
    "start": "4547270",
    "end": "4554710"
  },
  {
    "text": "That's right. So Monte Carlo is\nnot that estimate. So TD gives you 0.75.",
    "start": "4554710",
    "end": "4560450"
  },
  {
    "text": "What does Monte Carlo give you? It's not 0.75.",
    "start": "4560450",
    "end": "4569400"
  },
  {
    "text": "And again, multiple of\nyou guys got it correct. I just have a question. Is gamma that same\nas alpha here?",
    "start": "4569400",
    "end": "4575940"
  },
  {
    "text": "Good question. No. Here, I'm assuming\nthat gamma is 1. And it's a great question. So someone else was\nasking this, too.",
    "start": "4575940",
    "end": "4582550"
  },
  {
    "text": "So I'm assuming that\nalpha is set correctly",
    "start": "4582550",
    "end": "4589770"
  },
  {
    "text": "for these to converge. Yeah, it's a good question. Sorry, someone\nelse had that, too.",
    "start": "4589770",
    "end": "4595179"
  },
  {
    "text": "So I'm assuming that\nwe're going over our data an infinite amount of time,\nbut we're decaying alpha correctly as we do that.",
    "start": "4595180",
    "end": "4600630"
  },
  {
    "text": "It's a good question. Do you want to explain\nwhat Monte Carlo is? It's not 0.75.",
    "start": "4600630",
    "end": "4608210"
  },
  {
    "text": "Going to be 0? Yes, it is. That's great. Someone want to\nexplain why it's 0? ",
    "start": "4608210",
    "end": "4615409"
  },
  {
    "text": "So Monte Carlo is 0.  Yeah.",
    "start": "4615410",
    "end": "4621579"
  },
  {
    "text": "So when we see one\ntrajectory where A shows up, and [INAUDIBLE]. That's right.",
    "start": "4621580",
    "end": "4626860"
  },
  {
    "text": "Remind me of your name. Yeah, so what [MUTED]\nsaid is exactly right. So we've only seen\none trajectory,",
    "start": "4626860",
    "end": "4632130"
  },
  {
    "text": "and I know some other people\nmade the same observation. So we've only seen\none trajectory where there was A at all.",
    "start": "4632130",
    "end": "4638220"
  },
  {
    "text": "We, for Monte Carlo, we just\naverage over all the returns we've seen where that. So that's only 0.",
    "start": "4638220",
    "end": "4644110"
  },
  {
    "text": "So I bring this up because,\neven though, asymptotically, all of these things converge\nto the right thing",
    "start": "4644110",
    "end": "4650010"
  },
  {
    "text": "under some mild assumptions,\nwith finite data, which is what we're almost always\ngoing to have in reality,",
    "start": "4650010",
    "end": "4655030"
  },
  {
    "text": "even if you go over\nit multiple times, they're converging\nto, sometimes, totally different things. And here is what they are\nconverging to, in general.",
    "start": "4655030",
    "end": "4662010"
  },
  {
    "text": "Monte Carlo is converging\nto the minimum mean squared error with respect to the\nerror observed returns.",
    "start": "4662010",
    "end": "4668610"
  },
  {
    "text": "So it's just going to\nset it so it minimizes the error between the\nobserved returns it's seeing and its value.",
    "start": "4668610",
    "end": "4676739"
  },
  {
    "text": "So in this case, that\nwould be V of A equals 0. So that is the minimum\nmean squared error.",
    "start": "4676740",
    "end": "4682450"
  },
  {
    "text": "TD0 converges to the\ndynamic programming policy for the MDP with a\nmaximum likelihood model",
    "start": "4682450",
    "end": "4689170"
  },
  {
    "text": "estimates. So you guys remember\nhow we just talked about certainty equivalence? What we were doing here is,\nwe're taking all our data.",
    "start": "4689170",
    "end": "4698139"
  },
  {
    "text": "The answer you get from TD0\nif you do this batch process is the same as if you had\ncomputed your maximum likelihood",
    "start": "4698140",
    "end": "4705160"
  },
  {
    "text": "Markov decision process\nfrom the data you have, and then you did dynamic\nprogramming with it.",
    "start": "4705160",
    "end": "4710890"
  },
  {
    "text": "OK? So that will be exactly\nthe same as this.",
    "start": "4710890",
    "end": "4716260"
  },
  {
    "text": "And so in particular, it\nis leveraging and using the Markov assumption. And that's why it could actually\nchain these things together.",
    "start": "4716260",
    "end": "4724720"
  },
  {
    "text": "So you could see\nhere, for Monte Carlo, it doesn't know\nthat the value of A",
    "start": "4724720",
    "end": "4731130"
  },
  {
    "text": "has to be related\nto the value of B in terms of this\nbootstrapping relationship.",
    "start": "4731130",
    "end": "4737010"
  },
  {
    "text": "But TD is making that explicit. It's using the Markov decision\nprocess to say, the value of A",
    "start": "4737010",
    "end": "4743520"
  },
  {
    "text": "has to exactly be equal to the\nimmediate reward you get in A, plus gamma times the states\nthat I could get into,",
    "start": "4743520",
    "end": "4749530"
  },
  {
    "text": "which is always B,\nso the value of B. So TD learning is explicitly\nbaking that into the solution",
    "start": "4749530",
    "end": "4756530"
  },
  {
    "text": "you get, whereas\nMonte Carlo is not. Monte Carlo is just trying to\nminimize the mean squared error for the returns you see.",
    "start": "4756530",
    "end": "4762050"
  },
  {
    "text": "So they can end up giving\nyou very different solutions. And depending on whether your\nMarkov property is really",
    "start": "4762050",
    "end": "4768080"
  },
  {
    "text": "satisfied or not, you might\nwant one or the other. ",
    "start": "4768080",
    "end": "4773200"
  },
  {
    "text": "Awesome. So this just summarizes quickly\nsome of the different properties and approaches and\njust highlights",
    "start": "4773200",
    "end": "4778290"
  },
  {
    "text": "here that temporal difference\nreally does exploit this Markov structure. And that could be\nreally helpful if you want to leverage that to get\nbetter estimates of earlier",
    "start": "4778290",
    "end": "4785880"
  },
  {
    "text": "states, like in the\ncase we just saw. So just to summarize,\nwe finished going through policy evaluation\nwith tabular settings.",
    "start": "4785880",
    "end": "4793660"
  },
  {
    "text": "And then on Wednesday,\nwhat we're going to do is talk about control,\nand we'll start to talk about function\napproximation, as well.",
    "start": "4793660",
    "end": "4799940"
  },
  {
    "text": "All right. Thanks. See you then. ",
    "start": "4799940",
    "end": "4807000"
  }
]