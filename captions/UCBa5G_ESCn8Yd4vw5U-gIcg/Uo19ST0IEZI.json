[
  {
    "start": "0",
    "end": "1390"
  },
  {
    "text": "So by far the most famous\ndimension reduction",
    "start": "1390",
    "end": "4030"
  },
  {
    "text": "approach is principal\ncomponents regression.",
    "start": "4030",
    "end": "6430"
  },
  {
    "text": "And principal components\nregression involves a 2-step",
    "start": "6430",
    "end": "10000"
  },
  {
    "text": "procedure.",
    "start": "10000",
    "end": "10810"
  },
  {
    "text": "In step one, we find what are\ncalled principal components",
    "start": "10810",
    "end": "13660"
  },
  {
    "text": "of the data matrix\nx, and we're going",
    "start": "13660",
    "end": "15969"
  },
  {
    "text": "to cover principal\ncomponents in detail",
    "start": "15970",
    "end": "18280"
  },
  {
    "text": "in chapter 10 of the textbook.",
    "start": "18280",
    "end": "19970"
  },
  {
    "text": "And so I'm not going to cover\nthat in great detail here.",
    "start": "19970",
    "end": "23230"
  },
  {
    "text": "So in step one, we get\nprincipal components.",
    "start": "23230",
    "end": "25449"
  },
  {
    "text": "And then in step\ntwo, we're just going",
    "start": "25450",
    "end": "27202"
  },
  {
    "text": "to perform least\nsquares regression",
    "start": "27202",
    "end": "28660"
  },
  {
    "text": "using those principal\ncomponents as predictors.",
    "start": "28660",
    "end": "32080"
  },
  {
    "text": "So basically,\nprincipal components",
    "start": "32080",
    "end": "34540"
  },
  {
    "text": "are an interesting idea.",
    "start": "34540",
    "end": "36460"
  },
  {
    "text": "And the first\nprincipal component",
    "start": "36460",
    "end": "38050"
  },
  {
    "text": "is just the linear\ncombination of the variables",
    "start": "38050",
    "end": "40390"
  },
  {
    "text": "that has the highest variance.",
    "start": "40390",
    "end": "43180"
  },
  {
    "text": "The second principal component\nis the linear combination",
    "start": "43180",
    "end": "46450"
  },
  {
    "text": "that has the largest variance\nout of all linear combinations",
    "start": "46450",
    "end": "50200"
  },
  {
    "text": "that are totally unrelated\nto the linear combination",
    "start": "50200",
    "end": "52570"
  },
  {
    "text": "that we just got, and so on.",
    "start": "52570",
    "end": "55090"
  },
  {
    "text": "And so the principal\ncomponents give us",
    "start": "55090",
    "end": "57070"
  },
  {
    "text": "linear combinations or\ndimensions of the data",
    "start": "57070",
    "end": "59560"
  },
  {
    "text": "that are really high\nin variance and that",
    "start": "59560",
    "end": "61790"
  },
  {
    "text": "are uncorrelated to the\nones that we previously got.",
    "start": "61790",
    "end": "64670"
  },
  {
    "text": "And so the idea is\nthat if you give me",
    "start": "64670",
    "end": "66890"
  },
  {
    "text": "a data set with 35\nvariables, I can compute",
    "start": "66890",
    "end": "69860"
  },
  {
    "text": "a few principal\ncomponents and those",
    "start": "69860",
    "end": "71510"
  },
  {
    "text": "might capture most of the\nvariation in the data,",
    "start": "71510",
    "end": "73970"
  },
  {
    "text": "but in a very\nsuccinct way involving",
    "start": "73970",
    "end": "76130"
  },
  {
    "text": "just a few new variables.",
    "start": "76130",
    "end": "78149"
  },
  {
    "text": "Z1 through Z3 or Z1 through Z4.",
    "start": "78150",
    "end": "80750"
  },
  {
    "start": "80750",
    "end": "83500"
  },
  {
    "text": "So here's an example\nof principal components",
    "start": "83500",
    "end": "85390"
  },
  {
    "text": "analysis on a very simple data\nset that we saw in chapter 3",
    "start": "85390",
    "end": "88300"
  },
  {
    "text": "already.",
    "start": "88300",
    "end": "89440"
  },
  {
    "text": "And so what we're\nshowing here is a plot",
    "start": "89440",
    "end": "91540"
  },
  {
    "text": "where the x-axis shows\npopulation and the y-axis",
    "start": "91540",
    "end": "94270"
  },
  {
    "text": "shows ad spending for\n100 different cities.",
    "start": "94270",
    "end": "98439"
  },
  {
    "text": "And so those are\nthese purple dots.",
    "start": "98440",
    "end": "100930"
  },
  {
    "text": "And so this is a simple data set\nwith just p equals 2 variables.",
    "start": "100930",
    "end": "104630"
  },
  {
    "text": "And so I can say,\nall right, what's",
    "start": "104630",
    "end": "107079"
  },
  {
    "text": "the-- what's the linear\ncombination of these variables",
    "start": "107080",
    "end": "109480"
  },
  {
    "text": "that has the most variance.",
    "start": "109480",
    "end": "110920"
  },
  {
    "text": "Or equivalently, what's\nthe direction along which",
    "start": "110920",
    "end": "113380"
  },
  {
    "text": "this data varies the most?",
    "start": "113380",
    "end": "115229"
  },
  {
    "text": "And so we can see\nthat the direction",
    "start": "115230",
    "end": "117250"
  },
  {
    "text": "in which the data\nvaries the most actually",
    "start": "117250",
    "end": "119620"
  },
  {
    "text": "falls along this green line.",
    "start": "119620",
    "end": "121290"
  },
  {
    "text": "This is really the direction\nwith variation in the data.",
    "start": "121290",
    "end": "123750"
  },
  {
    "text": "And so that's actually the first\nprincipal component direction.",
    "start": "123750",
    "end": "127600"
  },
  {
    "text": "And then if we say, hey, what's\nthe direction along which",
    "start": "127600",
    "end": "130509"
  },
  {
    "text": "the data varies the most\nout of all directions that",
    "start": "130509",
    "end": "133330"
  },
  {
    "text": "are uncorrelated with\nthat first direction,",
    "start": "133330",
    "end": "135610"
  },
  {
    "text": "that's this blue\ndashed line here.",
    "start": "135610",
    "end": "138550"
  },
  {
    "text": "And so that's the second\nprincipal component",
    "start": "138550",
    "end": "140500"
  },
  {
    "text": "in this data.",
    "start": "140500",
    "end": "141420"
  },
  {
    "text": "So in this example, p\nequals 2-- and there's",
    "start": "141420",
    "end": "143840"
  },
  {
    "text": "only two principal components.",
    "start": "143840",
    "end": "145709"
  },
  {
    "text": "But in general, in a data\nset with lots of variables,",
    "start": "145710",
    "end": "150020"
  },
  {
    "text": "if p is large, there's a\nlot of principal components,",
    "start": "150020",
    "end": "152407"
  },
  {
    "text": "and we can look at the\nfirst one or the second one",
    "start": "152407",
    "end": "154489"
  },
  {
    "text": "or the third or the\nfourth, and so on.",
    "start": "154490",
    "end": "158840"
  },
  {
    "text": "So here's sort of a Zoom\nin a little bit on what's",
    "start": "158840",
    "end": "162349"
  },
  {
    "text": "happening here.",
    "start": "162350",
    "end": "163410"
  },
  {
    "text": "And the idea is in this--\non the left-hand side here,",
    "start": "163410",
    "end": "166340"
  },
  {
    "text": "we just have about 20 locations\nshown as purple circles.",
    "start": "166340",
    "end": "171660"
  },
  {
    "text": "And the reason that\nthis green line here",
    "start": "171660",
    "end": "174000"
  },
  {
    "text": "is the first\nprincipal component is",
    "start": "174000",
    "end": "175770"
  },
  {
    "text": "because it's the direction along\nwhich the data varies the most.",
    "start": "175770",
    "end": "179020"
  },
  {
    "text": "It's the line such that the\npoints are the most spread out",
    "start": "179020",
    "end": "187110"
  },
  {
    "text": "possible along the line.",
    "start": "187110",
    "end": "188510"
  },
  {
    "text": "If I drop each point to--",
    "start": "188510",
    "end": "190739"
  },
  {
    "text": "oops.",
    "start": "190740",
    "end": "191550"
  },
  {
    "text": "If I drop each of these\nlocations down to the line,",
    "start": "191550",
    "end": "195510"
  },
  {
    "text": "then this sum of\nsquared distances",
    "start": "195510",
    "end": "197519"
  },
  {
    "text": "is really as large as possible.",
    "start": "197520",
    "end": "200530"
  },
  {
    "text": "And so all these\nlittle red lines",
    "start": "200530",
    "end": "201910"
  },
  {
    "text": "indicate the distance\nfrom a location",
    "start": "201910",
    "end": "203470"
  },
  {
    "text": "to the principal component line.",
    "start": "203470",
    "end": "205010"
  },
  {
    "text": "It's actually the\nsmallest possible.",
    "start": "205010",
    "end": "206230"
  },
  {
    "text": "Oops, yeah.",
    "start": "206230",
    "end": "207010"
  },
  {
    "text": "This is the--",
    "start": "207010",
    "end": "208599"
  },
  {
    "text": "I misspoke.",
    "start": "208600",
    "end": "209798"
  },
  {
    "text": "This green line is the\nprincipal component,",
    "start": "209798",
    "end": "211590"
  },
  {
    "text": "and it's the direction along\nwhich the data varies the most.",
    "start": "211590",
    "end": "213970"
  },
  {
    "text": "And it's also the\ndirection along which",
    "start": "213970",
    "end": "215650"
  },
  {
    "text": "the distances from the purple\npoints to the green line,",
    "start": "215650",
    "end": "218560"
  },
  {
    "text": "which I'm showing in red,\nis as small as possible.",
    "start": "218560",
    "end": "221420"
  },
  {
    "text": "And on the right-hand\nside here, I'm",
    "start": "221420",
    "end": "223060"
  },
  {
    "text": "really seeing the\nsame picture again.",
    "start": "223060",
    "end": "224660"
  },
  {
    "text": "But now it's been rotated\nso that principal component",
    "start": "224660",
    "end": "228010"
  },
  {
    "text": "line is horizontal just to\nmake it a little easier to see.",
    "start": "228010",
    "end": "231375"
  },
  {
    "start": "231375",
    "end": "234460"
  },
  {
    "text": "So if I want to understand these\nprinciple components better,",
    "start": "234460",
    "end": "237550"
  },
  {
    "text": "I can actually plot each\nprincipal component--",
    "start": "237550",
    "end": "239840"
  },
  {
    "text": "so each linear combination\nof the variables",
    "start": "239840",
    "end": "241900"
  },
  {
    "text": "that I got on the x-axis, and I\ncan plot it against population",
    "start": "241900",
    "end": "246129"
  },
  {
    "text": "and against AD spending.",
    "start": "246130",
    "end": "248170"
  },
  {
    "text": "And what I can see is that\nthe first principal component",
    "start": "248170",
    "end": "251890"
  },
  {
    "text": "is really highly correlated with\npopulation and highly correlated",
    "start": "251890",
    "end": "255490"
  },
  {
    "text": "with ad spending.",
    "start": "255490",
    "end": "256760"
  },
  {
    "text": "And so what that means\nis that I'm really",
    "start": "256760",
    "end": "258549"
  },
  {
    "text": "summarizing the data very well.",
    "start": "258550",
    "end": "260720"
  },
  {
    "text": "If instead of using\nthose original two",
    "start": "260720",
    "end": "262360"
  },
  {
    "text": "variables, population\nand ad spending,",
    "start": "262360",
    "end": "264849"
  },
  {
    "text": "I instead used just the\nfirst principal component.",
    "start": "264850",
    "end": "267880"
  },
  {
    "text": "So that kind of\nsuggests the idea, hey,",
    "start": "267880",
    "end": "270100"
  },
  {
    "text": "if I want to predict\nsome response like sales,",
    "start": "270100",
    "end": "273370"
  },
  {
    "text": "instead of using population and\nad spending to predict sales,",
    "start": "273370",
    "end": "276910"
  },
  {
    "text": "I can just use the first\nprincipal component.",
    "start": "276910",
    "end": "279230"
  },
  {
    "text": "I can just treat that as\na predictor in a model,",
    "start": "279230",
    "end": "281260"
  },
  {
    "text": "fit the model using\nleast squares.",
    "start": "281260",
    "end": "282863"
  },
  {
    "text": "And I bet those results are\ngoing to be pretty good.",
    "start": "282863",
    "end": "285030"
  },
  {
    "start": "285030",
    "end": "287770"
  },
  {
    "text": "So now this figure is just\nlike the previous one.",
    "start": "287770",
    "end": "290650"
  },
  {
    "text": "But instead of showing the\nfirst principal component",
    "start": "290650",
    "end": "293139"
  },
  {
    "text": "on the x axis, it's showing\nthe second principal component",
    "start": "293140",
    "end": "296200"
  },
  {
    "text": "against, again, population\nand ad spending.",
    "start": "296200",
    "end": "299090"
  },
  {
    "text": "And we can see that there's\nvery little relationship",
    "start": "299090",
    "end": "301449"
  },
  {
    "text": "between population and the\nsecond principal component",
    "start": "301450",
    "end": "304030"
  },
  {
    "text": "and between ad spending and\nthe second principal component.",
    "start": "304030",
    "end": "306880"
  },
  {
    "text": "So that suggests that really the\nfirst principal component here",
    "start": "306880",
    "end": "309880"
  },
  {
    "text": "does a great job of\nsummarizing the data.",
    "start": "309880",
    "end": "312290"
  },
  {
    "text": "And in this case that's happened\nbecause population and ad",
    "start": "312290",
    "end": "315070"
  },
  {
    "text": "spending are really\ncorrelated with each other.",
    "start": "315070",
    "end": "317270"
  },
  {
    "text": "And so one new\nvariable, which is",
    "start": "317270",
    "end": "319539"
  },
  {
    "text": "the first principal\ncomponent, can really",
    "start": "319540",
    "end": "321490"
  },
  {
    "text": "summarize both of those\ntwo variables very well.",
    "start": "321490",
    "end": "325840"
  },
  {
    "text": "So the idea is I\ntake my data, I get",
    "start": "325840",
    "end": "329830"
  },
  {
    "text": "the first couple of principal\ncomponents as many as I want.",
    "start": "329830",
    "end": "332470"
  },
  {
    "text": "And I use those as predictors\nin a regression model",
    "start": "332470",
    "end": "335470"
  },
  {
    "text": "that I fit using least squares.",
    "start": "335470",
    "end": "337450"
  },
  {
    "text": "And that can actually,\nin a lot of settings,",
    "start": "337450",
    "end": "339430"
  },
  {
    "text": "give really nice results.",
    "start": "339430",
    "end": "341350"
  },
  {
    "text": "So here's an example\non a simulated data set",
    "start": "341350",
    "end": "344770"
  },
  {
    "text": "where I have a bunch\nof observations",
    "start": "344770",
    "end": "348190"
  },
  {
    "text": "and I perform principal\ncomponents regression",
    "start": "348190",
    "end": "351190"
  },
  {
    "text": "with various numbers of\nprincipal components.",
    "start": "351190",
    "end": "353660"
  },
  {
    "text": "So like over here, I have\none principal component",
    "start": "353660",
    "end": "356740"
  },
  {
    "text": "all the way through to around\n45 principal components",
    "start": "356740",
    "end": "360050"
  },
  {
    "text": "in this example.",
    "start": "360050",
    "end": "361990"
  },
  {
    "text": "And so what I'm plotting here\nis the bias shown in black.",
    "start": "361990",
    "end": "366270"
  },
  {
    "text": "Oops, this is the bias.",
    "start": "366270",
    "end": "369740"
  },
  {
    "text": "This is the variance in green.",
    "start": "369740",
    "end": "372300"
  },
  {
    "text": "And this is the\nmean squared error.",
    "start": "372300",
    "end": "374360"
  },
  {
    "text": "And so as I get more and\nmore components in my model,",
    "start": "374360",
    "end": "378169"
  },
  {
    "text": "as I use more and more\nprincipal components,",
    "start": "378170",
    "end": "380360"
  },
  {
    "text": "I'm going to get\nless and less bias",
    "start": "380360",
    "end": "382849"
  },
  {
    "text": "because I'm going to be fitting\na more and more complex model.",
    "start": "382850",
    "end": "385990"
  },
  {
    "text": "But I'm going to pay a\nprice in that my variance is",
    "start": "385990",
    "end": "388400"
  },
  {
    "text": "going to increase as the\nnumber of components increases.",
    "start": "388400",
    "end": "392060"
  },
  {
    "text": "And remember, the\nmean squared error",
    "start": "392060",
    "end": "394010"
  },
  {
    "text": "is just the squared\nbias plus the variance.",
    "start": "394010",
    "end": "396540"
  },
  {
    "text": "So my mean squared error,\nwhich is shown here in purple,",
    "start": "396540",
    "end": "399620"
  },
  {
    "text": "has sort of approximately\nthat U-shape",
    "start": "399620",
    "end": "401630"
  },
  {
    "text": "that we've been talking about.",
    "start": "401630",
    "end": "403220"
  },
  {
    "text": "And I can see that my mean\nsquared error is really",
    "start": "403220",
    "end": "405680"
  },
  {
    "text": "smallest for a model with\naround 18 principal components.",
    "start": "405680",
    "end": "412820"
  },
  {
    "text": "So using principal components\nregression with 18 predictors",
    "start": "412820",
    "end": "416300"
  },
  {
    "text": "works really well here.",
    "start": "416300",
    "end": "418129"
  },
  {
    "text": "In this example over\nhere, the situation",
    "start": "418130",
    "end": "420140"
  },
  {
    "text": "is a little bit different.",
    "start": "420140",
    "end": "421230"
  },
  {
    "text": "Now my mean squared\nerror once again",
    "start": "421230",
    "end": "423140"
  },
  {
    "text": "decreases as I add\nmore components,",
    "start": "423140",
    "end": "424910"
  },
  {
    "text": "but it doesn't really\nincrease again.",
    "start": "424910",
    "end": "426650"
  },
  {
    "text": "It's pretty flat from\naround here outwards.",
    "start": "426650",
    "end": "429540"
  },
  {
    "text": "So any of these models looks\naround the same in terms",
    "start": "429540",
    "end": "432570"
  },
  {
    "text": "of test mean squared error.",
    "start": "432570",
    "end": "434070"
  },
  {
    "text": "And since I always prefer\nthe simplest model possible.",
    "start": "434070",
    "end": "437070"
  },
  {
    "text": "In this context, I might\nchoose maybe this model",
    "start": "437070",
    "end": "439770"
  },
  {
    "text": "with around 25 components.",
    "start": "439770",
    "end": "442379"
  },
  {
    "text": "So the idea is to\nsummarize the--",
    "start": "442380",
    "end": "444005"
  },
  {
    "text": "summarize the features by\nthe principal components,",
    "start": "444005",
    "end": "446130"
  },
  {
    "text": "which are the combinations\nwith highest variance.",
    "start": "446130",
    "end": "449770"
  },
  {
    "text": "I guess-- why is that\na good idea or why is",
    "start": "449770",
    "end": "452009"
  },
  {
    "text": "it-- could be a bad idea?",
    "start": "452010",
    "end": "454110"
  },
  {
    "text": "Yeah, so that's a good question.",
    "start": "454110",
    "end": "455530"
  },
  {
    "text": "And one thing that\nwe notice here",
    "start": "455530",
    "end": "456600"
  },
  {
    "text": "is that when I compute\nthose principal components,",
    "start": "456600",
    "end": "458760"
  },
  {
    "text": "I'm not actually\nlooking at the response.",
    "start": "458760",
    "end": "461620"
  },
  {
    "text": "I'm literally just looking at\nmy predictors, my x's, and I'm",
    "start": "461620",
    "end": "464970"
  },
  {
    "text": "looking for a linear combination\nof them that has high variance.",
    "start": "464970",
    "end": "468580"
  },
  {
    "text": "And it's kind of\nmaking this assumption",
    "start": "468580",
    "end": "471270"
  },
  {
    "text": "that a linear combination\nof the predictors that",
    "start": "471270",
    "end": "473310"
  },
  {
    "text": "has high variance is probably\ngoing to be associated",
    "start": "473310",
    "end": "475620"
  },
  {
    "text": "with the response.",
    "start": "475620",
    "end": "477060"
  },
  {
    "text": "And that's kind of a hunch that\nwe often have as statisticians.",
    "start": "477060",
    "end": "480097"
  },
  {
    "text": "It's an assumption\nthat we often make,",
    "start": "480097",
    "end": "481680"
  },
  {
    "text": "but really there's no reason\nthat has to be the case.",
    "start": "481680",
    "end": "484240"
  },
  {
    "text": "And in fact, it might not be.",
    "start": "484240",
    "end": "485620"
  },
  {
    "text": "Well, so let me\njust draw a picture",
    "start": "485620",
    "end": "486780"
  },
  {
    "text": "to talk about that a bit more\n[INAUDIBLE] for some space.",
    "start": "486780",
    "end": "489190"
  },
  {
    "text": "So if we think of our--",
    "start": "489190",
    "end": "492990"
  },
  {
    "text": "we have a plot here--",
    "start": "492990",
    "end": "494490"
  },
  {
    "text": "there's two\nvariables, and here's",
    "start": "494490",
    "end": "497500"
  },
  {
    "text": "my scatter plot of points.",
    "start": "497500",
    "end": "498970"
  },
  {
    "text": "And the first principal\ncomponents direction",
    "start": "498970",
    "end": "501730"
  },
  {
    "text": "is going to be along this--",
    "start": "501730",
    "end": "505480"
  },
  {
    "text": "let's put in another red line.",
    "start": "505480",
    "end": "506918"
  },
  {
    "text": "That's the first\nprincipal component.",
    "start": "506918",
    "end": "508460"
  },
  {
    "text": "So as Daniela said, if we\nsummarize these two variables",
    "start": "508460",
    "end": "510819"
  },
  {
    "text": "by the principal\ncomponent, we're",
    "start": "510820",
    "end": "511900"
  },
  {
    "text": "really assuming that this\ndirection of variation",
    "start": "511900",
    "end": "514089"
  },
  {
    "text": "is the important one.",
    "start": "514090",
    "end": "515240"
  },
  {
    "text": "So we think of y coming\nout of the slide.",
    "start": "515240",
    "end": "517849"
  },
  {
    "text": "We're really assuming that the\nregression plane varies along",
    "start": "517850",
    "end": "520899"
  },
  {
    "text": "the red line and doesn't vary\nin the orthogonal direction",
    "start": "520900",
    "end": "523340"
  },
  {
    "text": "because if we choose\none component,",
    "start": "523340",
    "end": "524980"
  },
  {
    "text": "we're going to ignore\nthe second direction.",
    "start": "524980",
    "end": "527079"
  },
  {
    "text": "Is that a good assumption?",
    "start": "527080",
    "end": "528228"
  },
  {
    "text": "Well, it's not always\ngoing to be the case,",
    "start": "528228",
    "end": "530020"
  },
  {
    "text": "but it tends to be quite\nreasonable because one",
    "start": "530020",
    "end": "532360"
  },
  {
    "text": "way to think about it is if\nthis is observational data,",
    "start": "532360",
    "end": "535000"
  },
  {
    "text": "the fact that we've measured\nthe variables at all",
    "start": "535000",
    "end": "537382"
  },
  {
    "text": "probably means that\nthey could be--",
    "start": "537382",
    "end": "538840"
  },
  {
    "text": "they're likely to be important.",
    "start": "538840",
    "end": "540680"
  },
  {
    "text": "We measure things in experiment\nto predict something.",
    "start": "540680",
    "end": "543440"
  },
  {
    "text": "The things we\nmeasure, we're more",
    "start": "543440",
    "end": "544815"
  },
  {
    "text": "likely to measure things\nwhich are important.",
    "start": "544815",
    "end": "546690"
  },
  {
    "text": "So the things that\nmatter are probably",
    "start": "546690",
    "end": "548560"
  },
  {
    "text": "going to vary in the\ndirection of the response.",
    "start": "548560",
    "end": "550790"
  },
  {
    "text": "Not always, but it's a good\nhunch that all else equal,",
    "start": "550790",
    "end": "554380"
  },
  {
    "text": "let's look in the direction\nof variation of the predictors",
    "start": "554380",
    "end": "557380"
  },
  {
    "text": "to find the places where the\nresponse is most likely to vary.",
    "start": "557380",
    "end": "562350"
  },
  {
    "text": "OK, so let's go back to--",
    "start": "562350",
    "end": "564690"
  },
  {
    "text": "where are we?",
    "start": "564690",
    "end": "565480"
  },
  {
    "text": "We're talking about the\nnumber of directions.",
    "start": "565480",
    "end": "568170"
  },
  {
    "text": "Yeah, so when we perform\nprincipal components regression,",
    "start": "568170",
    "end": "570959"
  },
  {
    "text": "we need to somehow select\nthe number of directions M.",
    "start": "570960",
    "end": "573810"
  },
  {
    "text": "And we just saw that, you know,\nthe test mean squared error.",
    "start": "573810",
    "end": "578580"
  },
  {
    "text": "We want it to be as\nsmall as possible.",
    "start": "578580",
    "end": "580570"
  },
  {
    "text": "So we've got to estimate\nthe test mean squared error.",
    "start": "580570",
    "end": "582910"
  },
  {
    "text": "And by now you've probably\nseen that Rob and I really",
    "start": "582910",
    "end": "585269"
  },
  {
    "text": "prefer cross-validation\nover any other approach.",
    "start": "585270",
    "end": "587860"
  },
  {
    "text": "So we would select--",
    "start": "587860",
    "end": "589680"
  },
  {
    "text": "we would suggest\nusing cross-validation",
    "start": "589680",
    "end": "592005"
  },
  {
    "text": "in order to choose the number of\nprincipal component directions",
    "start": "592005",
    "end": "594630"
  },
  {
    "text": "that you want to use.",
    "start": "594630",
    "end": "596370"
  },
  {
    "text": "So here that's what we\ndid on the credit data.",
    "start": "596370",
    "end": "600470"
  },
  {
    "text": "So on the x--",
    "start": "600470",
    "end": "601970"
  },
  {
    "text": "on the left-hand\nside, we can just",
    "start": "601970",
    "end": "603800"
  },
  {
    "text": "see the results of just\nperforming, just plain principal",
    "start": "603800",
    "end": "606800"
  },
  {
    "text": "components regression\non the data.",
    "start": "606800",
    "end": "608490"
  },
  {
    "text": "So like for instance,\nif we want to look",
    "start": "608490",
    "end": "611042"
  },
  {
    "text": "at the principal components\nregression model with six",
    "start": "611042",
    "end": "613250"
  },
  {
    "text": "principal components.",
    "start": "613250",
    "end": "614180"
  },
  {
    "text": "So m equals 6, that's\nthis blue line.",
    "start": "614180",
    "end": "618000"
  },
  {
    "text": "And so we can see that like\na few of these coefficients",
    "start": "618000",
    "end": "621290"
  },
  {
    "text": "are non-zero and then a few\nothers are basically zero.",
    "start": "621290",
    "end": "624649"
  },
  {
    "text": "Over here we've\ngot 11 components,",
    "start": "624650",
    "end": "626840"
  },
  {
    "text": "and that's actually the\nfull least squares model",
    "start": "626840",
    "end": "629570"
  },
  {
    "text": "because when m equals p, you've\njust got regular least squares",
    "start": "629570",
    "end": "632390"
  },
  {
    "text": "on the original data.",
    "start": "632390",
    "end": "635710"
  },
  {
    "text": "And we can-- on the\nright-hand side,",
    "start": "635710",
    "end": "637570"
  },
  {
    "text": "see for each of\nthose same models,",
    "start": "637570",
    "end": "639230"
  },
  {
    "text": "we can see the cross-validated\nmean squared error.",
    "start": "639230",
    "end": "641510"
  },
  {
    "text": "So this is an estimate\nof the test error.",
    "start": "641510",
    "end": "643520"
  },
  {
    "text": "And here we actually\nsee something",
    "start": "643520",
    "end": "645010"
  },
  {
    "text": "that's pretty disappointing.",
    "start": "645010",
    "end": "646177"
  },
  {
    "text": "Remember, we like to pick a\nmodel for which the mean squared",
    "start": "646177",
    "end": "648700"
  },
  {
    "text": "error is as small as possible.",
    "start": "648700",
    "end": "650500"
  },
  {
    "text": "And so here the\nmean squared error",
    "start": "650500",
    "end": "652120"
  },
  {
    "text": "is really as small\nas possible when",
    "start": "652120",
    "end": "654040"
  },
  {
    "text": "we have a model with\n10 or 11 components.",
    "start": "654040",
    "end": "656779"
  },
  {
    "text": "And remember when\nm equals 11, that",
    "start": "656780",
    "end": "659410"
  },
  {
    "text": "is just going to be\nregular least squares",
    "start": "659410",
    "end": "661660"
  },
  {
    "text": "on the original data.",
    "start": "661660",
    "end": "663040"
  },
  {
    "text": "So basically, principal\ncomponents regression just tells",
    "start": "663040",
    "end": "666040"
  },
  {
    "text": "us when you choose the\nnumber of components",
    "start": "666040",
    "end": "668110"
  },
  {
    "text": "by cross-validation on\nthis particular data set,",
    "start": "668110",
    "end": "671350"
  },
  {
    "text": "it tells us to just forget\nit and just do least squares",
    "start": "671350",
    "end": "673810"
  },
  {
    "text": "on the original data.",
    "start": "673810",
    "end": "675110"
  },
  {
    "text": "So this is disappointing.",
    "start": "675110",
    "end": "677373"
  },
  {
    "text": "It means the principal\ncomponents regression doesn't",
    "start": "677373",
    "end": "679540"
  },
  {
    "text": "give us any gains over\njust plain least squares",
    "start": "679540",
    "end": "681820"
  },
  {
    "text": "that you guys saw in chapter 3.",
    "start": "681820",
    "end": "684040"
  },
  {
    "text": "But this is also something that\nhappens in a lot of contexts.",
    "start": "684040",
    "end": "686860"
  },
  {
    "text": "You can try to\nbeat least squares,",
    "start": "686860",
    "end": "688480"
  },
  {
    "text": "but it might not work on\na particular data set.",
    "start": "688480",
    "end": "691070"
  },
  {
    "start": "691070",
    "end": "694490"
  },
  {
    "text": "So as Rob mentioned, with\nPrincipal components regression,",
    "start": "694490",
    "end": "697910"
  },
  {
    "text": "we're just coming up with these\nnew transformed variables,",
    "start": "697910",
    "end": "701870"
  },
  {
    "text": "which were the principal\ncomponents just",
    "start": "701870",
    "end": "704690"
  },
  {
    "text": "in a totally unsupervised way.",
    "start": "704690",
    "end": "706347"
  },
  {
    "text": "We're just looking\nat the x variables,",
    "start": "706347",
    "end": "707930"
  },
  {
    "text": "and we're just going\nto cross our fingers",
    "start": "707930",
    "end": "709638"
  },
  {
    "text": "that the directions on which the\nx variables really vary a lot",
    "start": "709638",
    "end": "712940"
  },
  {
    "text": "are the same directions in which\nthe variables are correlated",
    "start": "712940",
    "end": "717560"
  },
  {
    "text": "with the response y",
    "start": "717560",
    "end": "719575"
  },
  {
    "text": "But if we don't\nwant to just have",
    "start": "719575",
    "end": "720950"
  },
  {
    "text": "to keep our fingers crossed\nand hope for the best,",
    "start": "720950",
    "end": "723033"
  },
  {
    "text": "we can instead perform what's\ncalled partial least squares.",
    "start": "723033",
    "end": "725810"
  },
  {
    "text": "And partial least squares is\njust like principal components",
    "start": "725810",
    "end": "728360"
  },
  {
    "text": "regression, but it selects\nthese new predictors Z1",
    "start": "728360",
    "end": "732380"
  },
  {
    "text": "through Zm in a supervised way.",
    "start": "732380",
    "end": "736280"
  },
  {
    "text": "It's going to\nchoose Z1 through Zm",
    "start": "736280",
    "end": "738740"
  },
  {
    "text": "that are linear combinations\nof the original features that",
    "start": "738740",
    "end": "741920"
  },
  {
    "text": "are directions along which the\noriginal features vary a lot,",
    "start": "741920",
    "end": "745160"
  },
  {
    "text": "but that are also directions\nthat are related to y.",
    "start": "745160",
    "end": "748610"
  },
  {
    "text": "So instead of just looking for\na direction in which x varies,",
    "start": "748610",
    "end": "751550"
  },
  {
    "text": "we're going to look for a\ndirection on which x varies.",
    "start": "751550",
    "end": "754720"
  },
  {
    "text": "That also has to be\nrelated to the response.",
    "start": "754720",
    "end": "758060"
  },
  {
    "text": "And so the goal here is to be\nable to more effectively predict",
    "start": "758060",
    "end": "762650"
  },
  {
    "text": "the response because\nwe explicitly",
    "start": "762650",
    "end": "764330"
  },
  {
    "text": "think about the\nresponse when we're",
    "start": "764330",
    "end": "765800"
  },
  {
    "text": "choosing these new features--",
    "start": "765800",
    "end": "767700"
  },
  {
    "text": "Z1 through Zm.",
    "start": "767700",
    "end": "768400"
  },
  {
    "start": "768400",
    "end": "772780"
  },
  {
    "text": "So the idea behind\npartial least squares",
    "start": "772780",
    "end": "776590"
  },
  {
    "text": "is we get the first direction\nof partial least squares",
    "start": "776590",
    "end": "779320"
  },
  {
    "text": "by doing a regression of y\nonto x1 that gives us V11.",
    "start": "779320",
    "end": "785060"
  },
  {
    "text": "We do a regression of y onto x2.",
    "start": "785060",
    "end": "787090"
  },
  {
    "text": "That gives us v1, 2, and so on\nuntil we do a regression of y",
    "start": "787090",
    "end": "791350"
  },
  {
    "text": "onto xpe and that gives us v1p.",
    "start": "791350",
    "end": "794880"
  },
  {
    "text": "And so, in fact, the first\nprincipal component direction",
    "start": "794880",
    "end": "797790"
  },
  {
    "text": "Z1--",
    "start": "797790",
    "end": "798300"
  },
  {
    "text": "the first partial least\nsquares direction that we get.",
    "start": "798300",
    "end": "800830"
  },
  {
    "text": "Z1 is really proportional\nto the correlation between Y",
    "start": "800830",
    "end": "804780"
  },
  {
    "text": "and the data matrix X.\nSo that's how we get Z1,",
    "start": "804780",
    "end": "808740"
  },
  {
    "text": "and then we get the other\npartial least squares directions",
    "start": "808740",
    "end": "811440"
  },
  {
    "text": "just by sort of\niterating this procedure.",
    "start": "811440",
    "end": "814360"
  },
  {
    "text": "And so in principle,\npartial least squares",
    "start": "814360",
    "end": "816790"
  },
  {
    "text": "seems like it should be a huge\ngain over principal components",
    "start": "816790",
    "end": "819399"
  },
  {
    "text": "regression because\nwe're choosing",
    "start": "819400",
    "end": "821050"
  },
  {
    "text": "those Z's in such a clever way.",
    "start": "821050",
    "end": "822709"
  },
  {
    "text": "That actually involves\nlooking at the response, which",
    "start": "822710",
    "end": "825280"
  },
  {
    "text": "seems like it can only help us.",
    "start": "825280",
    "end": "827050"
  },
  {
    "text": "But in practice, we have found\nthat partial least squares often",
    "start": "827050",
    "end": "831820"
  },
  {
    "text": "does not give us a huge gain\nover principal components",
    "start": "831820",
    "end": "834300"
  },
  {
    "text": "regression.",
    "start": "834300",
    "end": "834800"
  },
  {
    "text": "It's very similar\nto ridge regression",
    "start": "834800",
    "end": "837310"
  },
  {
    "text": "and PCR principal\ncomponents regression.",
    "start": "837310",
    "end": "839420"
  },
  {
    "text": "So although some people do\nlike partial least squares,",
    "start": "839420",
    "end": "842095"
  },
  {
    "text": "I've never found it\nvery useful and found",
    "start": "842095",
    "end": "844209"
  },
  {
    "text": "that ridge and principal\ncomponents regression is work",
    "start": "844210",
    "end": "847930"
  },
  {
    "text": "as well and is\nsimpler, both simpler.",
    "start": "847930",
    "end": "849472"
  },
  {
    "text": "And actually one thing\nthat's interesting",
    "start": "849472",
    "end": "851180"
  },
  {
    "text": "is Rob mentioned\nridge regression--",
    "start": "851180",
    "end": "852640"
  },
  {
    "text": "and it might seem\nlike going back",
    "start": "852640",
    "end": "854500"
  },
  {
    "text": "ridge regression\nis really different",
    "start": "854500",
    "end": "856000"
  },
  {
    "text": "from principal components\nregression and partial least",
    "start": "856000",
    "end": "858040"
  },
  {
    "text": "squares.",
    "start": "858040",
    "end": "858680"
  },
  {
    "text": "But it turns out that\nmathematically these ideas",
    "start": "858680",
    "end": "860800"
  },
  {
    "text": "are all very closely related.",
    "start": "860800",
    "end": "863740"
  },
  {
    "text": "And principal components\nregression, for example,",
    "start": "863740",
    "end": "866950"
  },
  {
    "text": "is kind of just a discrete\nversion of ridge regression.",
    "start": "866950",
    "end": "870080"
  },
  {
    "text": "Ridge regression is\nkind of continuously",
    "start": "870080",
    "end": "872020"
  },
  {
    "text": "shrinking variables, whereas\nprincipal components is doing it",
    "start": "872020",
    "end": "875920"
  },
  {
    "text": "in a more choppy sort of way.",
    "start": "875920",
    "end": "877760"
  },
  {
    "start": "877760",
    "end": "880870"
  },
  {
    "text": "So we've covered a lot today.",
    "start": "880870",
    "end": "884380"
  },
  {
    "text": "And now you've seen a lot\nof different model selection",
    "start": "884380",
    "end": "887740"
  },
  {
    "text": "methods, which are really useful\nin settings where you might",
    "start": "887740",
    "end": "891700"
  },
  {
    "text": "have a lot of observations, but\nyou have a lot of variables.",
    "start": "891700",
    "end": "894920"
  },
  {
    "text": "So if someone comes\nto you with some data,",
    "start": "894920",
    "end": "897040"
  },
  {
    "text": "with a million observations\nand four variables,",
    "start": "897040",
    "end": "899500"
  },
  {
    "text": "then do least squares,\nknock yourself out, or even",
    "start": "899500",
    "end": "902035"
  },
  {
    "text": "use some of the approaches that\nyou're going to see in chapters",
    "start": "902035",
    "end": "904660"
  },
  {
    "text": "7, 8, and 9, which are even\nmore flexible and more complex",
    "start": "904660",
    "end": "907930"
  },
  {
    "text": "alternatives to least squares.",
    "start": "907930",
    "end": "909589"
  },
  {
    "text": "But if someone comes\nto you with data,",
    "start": "909590",
    "end": "911380"
  },
  {
    "text": "with 400 observations and\n300 variables or even 30,000",
    "start": "911380",
    "end": "915970"
  },
  {
    "text": "variables, you're going to need\nways to simplify the problem",
    "start": "915970",
    "end": "918879"
  },
  {
    "text": "and to fit really simple models\nthat are even simpler than what",
    "start": "918880",
    "end": "921572"
  },
  {
    "text": "least squares is\ngoing to give you.",
    "start": "921572",
    "end": "923030"
  },
  {
    "text": "And those are really some of\nthe ideas that we covered today.",
    "start": "923030",
    "end": "926542"
  },
  {
    "text": "So this is a really\nexciting topic",
    "start": "926542",
    "end": "929920"
  },
  {
    "text": "and a lot of modern\nstatistical research",
    "start": "929920",
    "end": "931870"
  },
  {
    "text": "really focuses on how we can\nimprove prediction in settings",
    "start": "931870",
    "end": "935650"
  },
  {
    "text": "like the ones we covered\ntoday, where we just",
    "start": "935650",
    "end": "937900"
  },
  {
    "text": "want a simple model\nthat's simpler than least",
    "start": "937900",
    "end": "940790"
  },
  {
    "text": "squares because the least\nsquares fit is really",
    "start": "940790",
    "end": "944000"
  },
  {
    "text": "going to overfit the data.",
    "start": "944000",
    "end": "946300"
  },
  {
    "start": "946300",
    "end": "948000"
  }
]