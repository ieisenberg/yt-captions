[
  {
    "start": "0",
    "end": "6190"
  },
  {
    "text": "OK, cool. Let's get started. OK, so it's kind of\ncomplicated, right?",
    "start": "6190",
    "end": "13150"
  },
  {
    "text": "It's kind of amazing, right? This technology is so advanced. So you can do all of\nthese things together.",
    "start": "13150",
    "end": "18220"
  },
  {
    "text": "But I still have to\ndo them one by one. I have 10 action items-- maybe more than 10.",
    "start": "18220",
    "end": "23500"
  },
  {
    "text": "I need to also\nconnect with Wi-Fi. That's actually\nsomething I have to do.",
    "start": "23500",
    "end": "29011"
  },
  {
    "text": "OK, but oh, let's get started. Oh, I need to have my notes. So what we're going\nto do today is",
    "start": "29011",
    "end": "36070"
  },
  {
    "text": "that we are going to continue\nwith the asymptotics last time a little bit for about\n15 to 20 minutes.",
    "start": "36070",
    "end": "42730"
  },
  {
    "text": "This is just to wrap up\nwhat we have discussed. And as I said,\nthis first lecture",
    "start": "42730",
    "end": "48880"
  },
  {
    "text": "is always kind of a\nlittle bit tricky for me to teach it, because the tools-- if you want to\nmake it formal, it",
    "start": "48880",
    "end": "55090"
  },
  {
    "text": "requires some kind\nof backgrounds. And if you don't want\nto make it formal, sometimes, there is\na lot of confusion.",
    "start": "55090",
    "end": "61720"
  },
  {
    "text": "So from this lecture, the\nsecond half of this lecture, I think we are going to\ntalk about things that",
    "start": "61720",
    "end": "68350"
  },
  {
    "text": "require less background, in some\nsense, and more self contained. OK, so the plan is the\nasymptotics, and then",
    "start": "68350",
    "end": "79750"
  },
  {
    "text": "the so-called\nuniform convergence. I'll define what it is. And uniform convergence\nwill be the main focus",
    "start": "79750",
    "end": "86880"
  },
  {
    "text": "for the first few\nweeks of the lecture. OK, so let's start by reviewing\nwhat we have done last time.",
    "start": "86880",
    "end": "92560"
  },
  {
    "text": "So what we have-- the last time was\nthis theorem, where we showed that if you assume\nconsistency, which is something",
    "start": "92560",
    "end": "106700"
  },
  {
    "text": "that we basically just assume\nwithout much justification. It's not always true.",
    "start": "106700",
    "end": "112500"
  },
  {
    "text": "And it also depends\non the problem. The consistency basically\nmeans that theta hat will converge to theta star.",
    "start": "112500",
    "end": "118340"
  },
  {
    "text": "Recall that theta\nhat is the ERM, the Empirical Risk Minimizer. And theta star is the minimizer\nof the population risk.",
    "start": "118340",
    "end": "125640"
  },
  {
    "text": "So you care about recovering\ntheta star or recovering something as good as theta star.",
    "start": "125640",
    "end": "131420"
  },
  {
    "text": "And we also assume a\nbunch of other things, like for example, the\nHessian is full rank and also",
    "start": "131420",
    "end": "138590"
  },
  {
    "text": "some regularity conditions,\nwhich I didn't even define exactly. For example, this\nrequires something",
    "start": "138590",
    "end": "144200"
  },
  {
    "text": "like some of the\nvariance is finite, and so you can\napply the theorems. And then, under these\nassumptions, we have that--",
    "start": "144200",
    "end": "152135"
  },
  {
    "text": " actually, it's\nchallenging for me, because this podium operated--\nit becomes unstable.",
    "start": "152135",
    "end": "162069"
  },
  {
    "text": "It's like I feel like I'm\nwriting while I'm on a boat. [LAUGHTER]",
    "start": "162070",
    "end": "168110"
  },
  {
    "text": "But it's probably good\nfor me to practice. What is this called?",
    "start": "168110",
    "end": "174000"
  },
  {
    "text": "I would be better with\nsome of the sports guys after we do this. Anyway, OK. So I guess we have\ndiscussed that you",
    "start": "174000",
    "end": "181952"
  },
  {
    "text": "know the order of the difference\nbetween, say, theta hat and theta star. The order is on the order\nof 1 over square root of n.",
    "start": "181952",
    "end": "189118"
  },
  {
    "text": "And formally, you\nwrite it like this. You scale back square\nroot of n, and you know that it's on the order of 1. And you also know\nsomething about the loss.",
    "start": "189118",
    "end": "198900"
  },
  {
    "text": "You know that the excess risk,\nL theta hat minus L theta star, is on the order of 1 over n.",
    "start": "198900",
    "end": "206305"
  },
  {
    "text": "And if you formally\nwrite it like this, you scale back towards n. And then, you say it's\non order of constant. And also, you know\nthat the distribution",
    "start": "206305",
    "end": "213510"
  },
  {
    "text": "of theta hat minus theta star-- this is converging to a\nGaussian distribution, which",
    "start": "213510",
    "end": "218520"
  },
  {
    "text": "means 0 and some covariance. And this covariance\nis complicated, but let me write it\nsomething like this.",
    "start": "218520",
    "end": "226130"
  },
  {
    "start": "226130",
    "end": "231815"
  },
  {
    "text": "This is just revealing what\nwe have written last time. And four, we also the\ndissolution of the excess risk.",
    "start": "231815",
    "end": "242508"
  },
  {
    "text": "This is the distribution\nof a scalar, because the excess\nrisk is a scalar. If you scale it by\nn, then you know",
    "start": "242508",
    "end": "247630"
  },
  {
    "text": "the distribution is\nconverging to the distribution of this random variable. And this random variable S\nis a Gaussian random variable",
    "start": "247630",
    "end": "256019"
  },
  {
    "text": "with covariance mean 0. And covariance-- something\nabove but not exactly.",
    "start": "256019",
    "end": "261785"
  },
  {
    "start": "261785",
    "end": "266980"
  },
  {
    "text": "You don't have to remember\nexactly what the covariance here is, because I\ndon't even remember them",
    "start": "266980",
    "end": "272140"
  },
  {
    "text": "if I don't read my notes. There are some\nintuitions about this, which I'm going to discuss.",
    "start": "272140",
    "end": "278195"
  },
  {
    "text": "But generally, this\nis just something you got from derivations. So last time, we have kind of\nroughly justified the number",
    "start": "278195",
    "end": "286930"
  },
  {
    "text": "1, number 2. And today, I'm going\nto, again, give a relatively heuristic proof\nfor 3 and 4 just very quick",
    "start": "286930",
    "end": "293410"
  },
  {
    "text": "so that we can wrap up this. So I guess just to\nvery quickly review",
    "start": "293410",
    "end": "300250"
  },
  {
    "text": "what we have done last\ntime, so the key idea to derive all of this is\nby doing Taylor expansion.",
    "start": "300250",
    "end": "308230"
  },
  {
    "text": "And Taylor expansion-- I think\nthe key equation-- let me just rewrite it, what we have\ndone last time-- is this.",
    "start": "308230",
    "end": "314180"
  },
  {
    "text": "So you look at the theta hat. The gradient of the\nempirical loss S theta hat.",
    "start": "314180",
    "end": "320170"
  },
  {
    "text": "This is guaranteed to be\n0, because theta hat is the minimizer of\nthe empirical loss.",
    "start": "320170",
    "end": "325569"
  },
  {
    "text": "And you Taylor expand\nthis around theta star. And you get something like\nthis plus higher order terms.",
    "start": "325570",
    "end": "336490"
  },
  {
    "text": " And then, you rearrange this and\nget theta hat minus theta star",
    "start": "336490",
    "end": "345590"
  },
  {
    "text": "is equal to the inverse of the\nempirical Hessian at theta star",
    "start": "345590",
    "end": "356210"
  },
  {
    "text": "times nabla L hat theta hat\nplus higher order terms.",
    "start": "356210",
    "end": "363220"
  },
  {
    "text": "And then, you say that I'm going\nto replace all the hats by L, like L hat by L, using\nsome kind of large number",
    "start": "363220",
    "end": "371440"
  },
  {
    "text": "of uniform convergence. And last time, we\nhave roughly discussed that this is on the order\nof 1 over square root of N,",
    "start": "371440",
    "end": "377980"
  },
  {
    "text": "because you have\na concentration. This is the average of-- this is roughly on the order of\n1 squared plus nabla L theta.",
    "start": "377980",
    "end": "385840"
  },
  {
    "text": "This is theta star.  Now, theta star,\nwhich is roughly",
    "start": "385840",
    "end": "391370"
  },
  {
    "text": "on the other 1 over square\nroot of n-- and this one is converting to a constant. So that's why the whole\nthing is converging",
    "start": "391370",
    "end": "397009"
  },
  {
    "text": "to something on the order\nof 1 over square root of n. And this time, we are going to\nmake it a little more formal.",
    "start": "397010",
    "end": "402300"
  },
  {
    "text": "So we've get the\nexact distribution of theta hat minus theta star. I'll make this part\nreally quick just so",
    "start": "402300",
    "end": "409160"
  },
  {
    "text": "that if you are not familiar\nwith the background, you don't get confused too much.",
    "start": "409160",
    "end": "414200"
  },
  {
    "text": "So the idea is that-- so if you look at what's\nthe distribution of this,",
    "start": "414200",
    "end": "421670"
  },
  {
    "text": "if you think about this,\nthis is the product of two random variables. And you roughly know\nwhat the distribution",
    "start": "421670",
    "end": "426920"
  },
  {
    "text": "of each of the random\nvariables is, right? So this one is going to\nconverge to a constant, which is going to converge to a\nnabla L theta star inverse.",
    "start": "426920",
    "end": "436020"
  },
  {
    "text": "And this one is going to be a\nGaussian distribution if you scale it correctly, right?",
    "start": "436020",
    "end": "441180"
  },
  {
    "text": "And basically, what\nyou need to know is that what's the product of\nthese two random variables.",
    "start": "441180",
    "end": "447757"
  },
  {
    "text": "What's the distribution\nof the product of two random variables? When you know each of them,\nwhat happens with each of them?",
    "start": "447757",
    "end": "454230"
  },
  {
    "text": "And what happens\nis from formula, what you do is you first\nscale by square root of n",
    "start": "454230",
    "end": "459240"
  },
  {
    "text": "so that each of these two random\nvariables are on the order of 1 so that you can reason\nabout them easily.",
    "start": "459240",
    "end": "466090"
  },
  {
    "text": "So you scale by square\nroot of n, you get this. And then, you have inverse.",
    "start": "466090",
    "end": "474500"
  },
  {
    "text": "And now, you scale\nthis empirical grid by square root of n. And also, you get\nthe square root of n.",
    "start": "474500",
    "end": "482770"
  },
  {
    "text": "And also, let's fill in\nthe population gradient, which is 0.",
    "start": "482770",
    "end": "488060"
  },
  {
    "text": "So this one is 0. I just read it here to make it\ncloser to something you know.",
    "start": "488060",
    "end": "495260"
  },
  {
    "text": "And then, this plus\nhigher order terms. This is still\nhigher order terms,",
    "start": "495260",
    "end": "500320"
  },
  {
    "text": "even if you multiply\nby square root of N, because I think there's a\ntypo in the lecture notes somebody pointed out,\nwhich is really nice.",
    "start": "500320",
    "end": "506480"
  },
  {
    "text": "But still, no matter\nhow you multiply, it's still higher order terms\ncompared to the other terms, right?",
    "start": "506480",
    "end": "511750"
  },
  {
    "text": "So and now, this one-- let's call it Z.",
    "start": "511750",
    "end": "518399"
  },
  {
    "text": "This Z, by law of large\nnumber, or I think, by central limit theorem--",
    "start": "518400",
    "end": "523650"
  },
  {
    "text": "Z is a Gaussian distribution\nand with some covariance.",
    "start": "523650",
    "end": "529080"
  },
  {
    "text": "And what's the covariance? The covariance will be the\ncovariance of nabla L xy theta.",
    "start": "529080",
    "end": "543730"
  },
  {
    "text": "Why? This is just because-- what is l hat theta\nstar minus this?",
    "start": "543730",
    "end": "552900"
  },
  {
    "text": "This is really just this\nthe empirical version of the right-hand side,\nthe population gradient.",
    "start": "552900",
    "end": "561390"
  },
  {
    "text": "So this is really 1 over N times\nsum of nabla l xi yi theta star",
    "start": "561390",
    "end": "572010"
  },
  {
    "text": "minus expectation\nof this, right? Of the same thing--\nmaybe you can--",
    "start": "572010",
    "end": "577350"
  },
  {
    "text": "for simplicity, let's just\nwrite xy theta star, all right? So when you apply\ncentral limit theorem,",
    "start": "577350",
    "end": "583470"
  },
  {
    "text": "you know that if you scale\nthis by square root of N, then you get a Gaussian\ndistribution, right?",
    "start": "583470",
    "end": "588720"
  },
  {
    "text": "So that's why we know\nthe random variable Z has Gaussian distribution. And we know this one will\nconvert to a constant",
    "start": "588720",
    "end": "595380"
  },
  {
    "text": "as n goes to infinity. And there is a theorem that\nspecifically deals with this.",
    "start": "595380",
    "end": "601015"
  },
  {
    "text": "But actually, if you think about\nit, this makes a lot of sense. So if you want to know\nwhat's the left-hand side, basically, it just\nbecomes the distribution",
    "start": "601015",
    "end": "608192"
  },
  {
    "text": "of the right-hand side. It's a constant times a\nGaussian distribution. It's this constant times\nthe Gaussian distribution.",
    "start": "608192",
    "end": "614620"
  },
  {
    "text": "So basically, we\nhave to figure out, what's the distribution here?",
    "start": "614620",
    "end": "619750"
  },
  {
    "text": "So what is the distribution\nof a constant times Z? So basically,\nabstractly speaking,",
    "start": "619750",
    "end": "629300"
  },
  {
    "text": "what we are dealing\nwith here is that-- so the question we're\ndealing with here is that--",
    "start": "629300",
    "end": "636230"
  },
  {
    "text": " so a different color\nfor abstraction.",
    "start": "636230",
    "end": "641950"
  },
  {
    "text": "So basically,\nyou're asking, what is the distribution of A\ntimes Z if A is a constant",
    "start": "641950",
    "end": "649650"
  },
  {
    "text": "and Z is from some\nGaussian distribution with covariant sigma? All right.",
    "start": "649650",
    "end": "655200"
  },
  {
    "text": "And I'm missing a page.",
    "start": "655200",
    "end": "660730"
  },
  {
    "text": " And you know that\nthere is a lemma, which",
    "start": "660730",
    "end": "667940"
  },
  {
    "text": "says that under this\ncase, A and Z is also",
    "start": "667940",
    "end": "674540"
  },
  {
    "text": "Gaussian distribution with\ncovariance with mean 0 and covariance A\nsigma A transpose.",
    "start": "674540",
    "end": "681560"
  },
  {
    "text": "I think this is a homework\nquestion-- homework 0 question. I'm not sure whether\nit's still there. I forgot to double-check.",
    "start": "681560",
    "end": "688040"
  },
  {
    "text": "But this is something\nyou can do-- what's a transformation of\na Gaussian distribution?",
    "start": "688040",
    "end": "694765"
  },
  {
    "text": "Still Gaussian\ndistribution, it's just the covariance\ngot transformed. And actually, the way to\ntransform the covariance",
    "start": "694765",
    "end": "700370"
  },
  {
    "text": "is that you left multiply\nthe transformation. And you right multiply\nthe transpose of it. And you get a new covariance.",
    "start": "700370",
    "end": "706800"
  },
  {
    "text": "So this is something-- it's\nnot that simple to derive this, but this is something you can\neither look up from a book,",
    "start": "706800",
    "end": "713090"
  },
  {
    "text": "or you can derive it yourself. All right. So with this small\nlemma, then we",
    "start": "713090",
    "end": "718580"
  },
  {
    "text": "know that the distribution\nof theta hat minus theta star",
    "start": "718580",
    "end": "726920"
  },
  {
    "text": "converges to-- you place the convergence\nto a Gaussian distribution",
    "start": "726920",
    "end": "733530"
  },
  {
    "text": "with mean 0. So here, A corresponds to the\nnabla l theta star, right?",
    "start": "733530",
    "end": "744390"
  },
  {
    "text": "And sigma corresponds\nto this one. And you just plug in\nthese two choices.",
    "start": "744390",
    "end": "752470"
  },
  {
    "text": "Then, what you have is this--",
    "start": "752470",
    "end": "761699"
  },
  {
    "text": "basically, what we\nintended to prove. We got nabla l, nabla\nsquare l theta star",
    "start": "761700",
    "end": "770350"
  },
  {
    "text": "minus 1 times\ncovariance of nabla l xy theta star times\nnabla l theta star.",
    "start": "770350",
    "end": "781700"
  },
  {
    "text": "OK? This is convergence intuition. Any questions so far?",
    "start": "781700",
    "end": "787720"
  },
  {
    "start": "787720",
    "end": "796089"
  },
  {
    "text": "I realize that my\ncamera is frozen. I don't know why. Something seems to be wrong.",
    "start": "796090",
    "end": "802090"
  },
  {
    "start": "802090",
    "end": "827000"
  },
  {
    "text": "For those people who are on Zoom\nmeeting, can you see my video? ",
    "start": "827000",
    "end": "835269"
  },
  {
    "text": "It's frozen. I see. Thanks. Maybe let me turn it\noff, and then turn it on.",
    "start": "835270",
    "end": "844040"
  },
  {
    "text": "OK, so it's working now?  OK, cool.",
    "start": "844040",
    "end": "849120"
  },
  {
    "text": "And you can see that hat? You can see everything? OK, thanks.",
    "start": "849120",
    "end": "854310"
  },
  {
    "text": "OK, cool. Any questions? Also, if you are\nin a Zoom meeting, also feel free to just\nunmute and ask any questions.",
    "start": "854310",
    "end": "860880"
  },
  {
    "text": " So at the end of the--",
    "start": "860880",
    "end": "866410"
  },
  {
    "text": "covariance at the end, if\nthat's the Hessian variance",
    "start": "866410",
    "end": "872073"
  },
  {
    "text": "through its negative\n[INAUDIBLE]?? ",
    "start": "872073",
    "end": "878529"
  },
  {
    "text": "This is inverse. Yeah, the covariant.",
    "start": "878530",
    "end": "884560"
  },
  {
    "text": "Sorry, which term\nare you asking? This one? The one next to it.",
    "start": "884560",
    "end": "889720"
  },
  {
    "text": "Here? The one to the right, yeah. Yeah, this is the same--",
    "start": "889720",
    "end": "896680"
  },
  {
    "text": "That's the exact same one? Yes, it's exactly the same one. It's supposed to be the same\nthing transposed, right?",
    "start": "896680",
    "end": "903650"
  },
  {
    "text": "But this is a symmetric matrix. So the transpose is the same\nas L, so this is minus 1. ",
    "start": "903650",
    "end": "920428"
  },
  {
    "text": "OK, so I guess what\nI'm going to do is I'm going to skip the\nproof for the derivation",
    "start": "920428",
    "end": "926520"
  },
  {
    "text": "for the number 4. It's kind of the same thing. It's just that you have to--",
    "start": "926520",
    "end": "932130"
  },
  {
    "text": "because you already know the\ndistribution of theta hat, you should know the\ndistribution of L theta hat.",
    "start": "932130",
    "end": "937320"
  },
  {
    "text": "And what you do is you do\nsome Taylor expansion to make it a polynomial of theta hat. And then, you can use what\nyou know about theta hat.",
    "start": "937320",
    "end": "945180"
  },
  {
    "text": "All of this is in lecture notes. I guess I'm going\nto skip this part. So if we wrote it and it\nlooks like-- for example,",
    "start": "945180",
    "end": "952029"
  },
  {
    "text": "like the [INAUDIBLE],, is\nthere a reason for that? You mean the covariance\nseems to like the new--",
    "start": "952030",
    "end": "959552"
  },
  {
    "text": "It's like, instead of the\ngradient direction module, [INAUDIBLE] about this.",
    "start": "959552",
    "end": "964660"
  },
  {
    "text": "Is there a connection\nbetween the two? I think there's a connection,\nbut I don't feel like it's--",
    "start": "964660",
    "end": "976810"
  },
  {
    "text": "this Hessian shows up very often\nin many different cases, right? So there is some\nconnection, but I",
    "start": "976810",
    "end": "982600"
  },
  {
    "text": "don't feel like it has to be-- it's not super closely related\nso that it's important enough",
    "start": "982600",
    "end": "988210"
  },
  {
    "text": "to know, yeah. Yeah, OK. So I guess I'll skip\nthe proof for number 4.",
    "start": "988210",
    "end": "995272"
  },
  {
    "text": "If you're interested,\nyou can look at the proof in the lecture notes. And what I'm going\nto do is that I'm",
    "start": "995272",
    "end": "1000590"
  },
  {
    "text": "going to spend another\n5 to 10 minutes to talk about a corollary of\nthis theorem, which",
    "start": "1000590",
    "end": "1007790"
  },
  {
    "text": "is in maybe a more\ntypical setting.",
    "start": "1007790",
    "end": "1013399"
  },
  {
    "text": "Like here, this theorem\nis very general. Because it doesn't say anything\nabout the loss function. It doesn't say anything\nabout the model.",
    "start": "1013400",
    "end": "1019970"
  },
  {
    "text": "It works for almost\neverything as long as you have the consistency. And here, let me\ninstantiate this theorem",
    "start": "1019970",
    "end": "1025910"
  },
  {
    "text": "for the so-called\nwell-specified case, where you use log likelihood. And then, we can see all\nof this covariance become",
    "start": "1025910",
    "end": "1032750"
  },
  {
    "text": "a little bit more intuitive. And things become a\nlittle bit easier.",
    "start": "1032750",
    "end": "1038609"
  },
  {
    "text": "So this is the so-called\nwell-specified case. ",
    "start": "1038609",
    "end": "1045339"
  },
  {
    "text": "So I guess in\naddition to theorem 1,",
    "start": "1045339",
    "end": "1053700"
  },
  {
    "text": "let's also assume\nthat-- let's suppose there exists some probabilistic\nmodel parameterized",
    "start": "1053700",
    "end": "1067850"
  },
  {
    "text": "by theta such that y\nis given x and theta.",
    "start": "1067850",
    "end": "1076840"
  },
  {
    "text": "So you assume that\ny is generated from this probabilistic\nmodel, right?",
    "start": "1076840",
    "end": "1082680"
  },
  {
    "text": "So what does it mean? So basically, it\nmean, so let's say, suppose there\nexists a theta star.",
    "start": "1082680",
    "end": "1088590"
  },
  {
    "text": "I'm using the subscript here\nto differentiate from the theta star defined before, which was\nthe minimizer of the population",
    "start": "1088590",
    "end": "1097630"
  },
  {
    "text": "risk. And actually, they are the same. But for now, they\nare the difference. So basically, you assume that\nthere exists a theta star such",
    "start": "1097630",
    "end": "1104310"
  },
  {
    "text": "that the yi, the data,\nis generated from-- conditional xi is generated\nfrom this probabilistic model.",
    "start": "1104310",
    "end": "1111000"
  },
  {
    "text": " All right. So assume-- so this is why\nit's called well-specified.",
    "start": "1111000",
    "end": "1119360"
  },
  {
    "text": "It means that your\ndata is generated from some probabilistic model. And also, in this case,\nsuppose you use the loss",
    "start": "1119360",
    "end": "1126590"
  },
  {
    "text": "function is the log likelihood. Right? Before, we didn't\nreally say what the loss function needs to be.",
    "start": "1126590",
    "end": "1132510"
  },
  {
    "text": "It could be anything. And now, let's say the loss\nfunction is the log likelihood",
    "start": "1132510",
    "end": "1137620"
  },
  {
    "text": "of this probabilistic model. Think of this as, for example,\nlogistic regression, right? ",
    "start": "1137620",
    "end": "1145169"
  },
  {
    "text": "Or linear regression\nwith Gaussian noise. So your log likelihood\ncould be cross entropy loss.",
    "start": "1145170",
    "end": "1150390"
  },
  {
    "text": "Could be mean square\nloss depending on what the probabilistic\nmodel you have. ",
    "start": "1150390",
    "end": "1160412"
  },
  {
    "text": "All right, so this is\nyour loss function. And when you do\nthis, then a you know a of things which are\nnicer, in some sense.",
    "start": "1160412",
    "end": "1168730"
  },
  {
    "text": "So first of all, you\nknow that the theta star is equal to the\ntheta substar, right?",
    "start": "1168730",
    "end": "1173880"
  },
  {
    "text": "So recall that this is the\nminimizer of the population loss. And this is the ground truth.",
    "start": "1173880",
    "end": "1179260"
  },
  {
    "text": "This is the one that\ngenerates our data. And in this case, you\ncan prove that when you have infinite data where\ntheta star is the minimize",
    "start": "1179260",
    "end": "1186150"
  },
  {
    "text": "of the infinite data case. You can recover\nthe ground truth-- theta substar.",
    "start": "1186150",
    "end": "1191340"
  },
  {
    "text": "So they are exactly\nthe same thing. And you also know a\nbunch of other things.",
    "start": "1191340",
    "end": "1198700"
  },
  {
    "text": "For example, you know\nthat the gradient-- this is kind of trivial.",
    "start": "1198700",
    "end": "1204730"
  },
  {
    "text": "I'm just writing\nit here because it used to be that I needed\nprove this in a proof.",
    "start": "1204730",
    "end": "1210587"
  },
  {
    "text": "But if you don't\ncare about the proof, this is just an intermediate\nstep that you know. So you know that expected\ngradient over the population",
    "start": "1210587",
    "end": "1219490"
  },
  {
    "text": "at theta star is 0. And also, you know what's the\ncovariance of the gradient.",
    "start": "1219490",
    "end": "1227503"
  },
  {
    "text": "The covariance of the\ngradient is the quantity that we care about, right? Because in the previous theorem,\nthe covariance of the gradient",
    "start": "1227503",
    "end": "1234340"
  },
  {
    "text": "shows up in the variance of\ntheta hat minus theta star. So the covariance of the\ngradient is x theta star.",
    "start": "1234340",
    "end": "1244110"
  },
  {
    "text": "I guess, from now on, we don't\ndistinguish theta superstar and theta supersuperstar,\nand theta subscript star,",
    "start": "1244110",
    "end": "1249300"
  },
  {
    "text": "because they are the same. And you know the\ncovariance actually happens to be the Hessian. ",
    "start": "1249300",
    "end": "1256990"
  },
  {
    "text": "And where the covariance\nof the gradient happens to be the Hessian,\nthen the covariance",
    "start": "1256990",
    "end": "1262270"
  },
  {
    "text": "of theta hat minus theta\nstar can be simplified. So because this used to\nbe a Gaussian distribution",
    "start": "1262270",
    "end": "1270900"
  },
  {
    "text": "with something like this, right? The covariance of the\ntheta hat minus-- it",
    "start": "1270900",
    "end": "1277220"
  },
  {
    "text": "used to be this product of\nthree things, three matrices. But now, what's in the middle\nis the same as the Hessian.",
    "start": "1277220",
    "end": "1284510"
  },
  {
    "text": "That's what we\nclaimed in number 3. So that means that you\ncan cancel this with this.",
    "start": "1284510",
    "end": "1292010"
  },
  {
    "text": "And you get only one term. So what's left is just the\ninverse of the Hessian.",
    "start": "1292010",
    "end": "1297774"
  },
  {
    "start": "1297774",
    "end": "1304890"
  },
  {
    "text": "Maybe I should just\nuse black forever. Yeah, and you also\nknow if you plug",
    "start": "1304890",
    "end": "1311625"
  },
  {
    "text": "in this, the covariance\nof the gradient, you basically plug in 3\ninto all the statements",
    "start": "1311625",
    "end": "1319020"
  },
  {
    "text": "that you had before. Then, you can also get something\nlike, for example-- well, the important thing is this--",
    "start": "1319020",
    "end": "1325080"
  },
  {
    "text": "the excess risk. I guess we have\nclaimed that it's on the order of theta star.",
    "start": "1325080",
    "end": "1331075"
  },
  {
    "text": "But actually, here, you\ncan be more precise. You know that this is converging\nto basically 1/2 times",
    "start": "1331075",
    "end": "1337420"
  },
  {
    "text": "chi square distribution\nwith degree p. So p is a dimension of theta.",
    "start": "1337420",
    "end": "1345880"
  },
  {
    "text": "So suppose you\nhave p parameters. Then, this is the distribution\nof the excess risk.",
    "start": "1345880",
    "end": "1353710"
  },
  {
    "text": "And if you take the\nexpectation of this so that you get\nall the randomness, then what you get is the\nexpectation of n times",
    "start": "1353710",
    "end": "1361740"
  },
  {
    "text": "excess risk is equal\nto the expectation",
    "start": "1361740",
    "end": "1367660"
  },
  {
    "text": "of the chi square distribution. This is equal to 1/2 times p.",
    "start": "1367660",
    "end": "1373918"
  },
  {
    "text": "By the way, chi\nsquare distribution-- you don't have to know\nanything detailed about it. This is basically\nthe distribution",
    "start": "1373918",
    "end": "1381940"
  },
  {
    "text": "of a sum of p normal\non Gaussian square. So you know a lot\nof things about it.",
    "start": "1381940",
    "end": "1388389"
  },
  {
    "text": "You know it's\npositive, and you know that the chi square\nwith p, the mean is--",
    "start": "1388390",
    "end": "1396429"
  },
  {
    "text": "if you need to know more\nabout this, just Wikipedia. It's very easy. We don't need anything\ndeep about it.",
    "start": "1396430",
    "end": "1401800"
  },
  {
    "text": "So the important thing\nis the last equation. So basically, we know that the\nexcess risk and expectation--",
    "start": "1401800",
    "end": "1407169"
  },
  {
    "text": "here, the expectation is over\nthe randomness of the data set, right? So excess risk-- if you\ndon't scale by theta star--",
    "start": "1407170",
    "end": "1415390"
  },
  {
    "text": "sorry, if you don't scale\nit by n, then you get-- this is equal to-- I guess I should\nwrite convergent to,",
    "start": "1415390",
    "end": "1422480"
  },
  {
    "text": "because it wouldn't\nbe exactly equal. This is 1/2 times p over n.",
    "start": "1422480",
    "end": "1429610"
  },
  {
    "text": "So basically, you don't even\nget the dependency on n. But you also get a\ndependency on p-- on a dimension.",
    "start": "1429610",
    "end": "1435269"
  },
  {
    "text": "So you know what's the\norder of the excess risk.",
    "start": "1435270",
    "end": "1441330"
  },
  {
    "text": "Of course, these are\nhigher order terms-- theta of 1 over N.",
    "start": "1441330",
    "end": "1446400"
  },
  {
    "text": "And actually, you\nknow the variance of excess risk, which I don't\nthink is super important. The variance is\nsmaller than the mean.",
    "start": "1446400",
    "end": "1454590"
  },
  {
    "text": "OK, so in the lecture\nnotes, I think we have proofs for all of this. But I think I'm not going\nto discuss the proof.",
    "start": "1454590",
    "end": "1460919"
  },
  {
    "text": "The most important\nthing, I think, is this one and this one.",
    "start": "1460920",
    "end": "1468510"
  },
  {
    "text": "So the first thing is saying\nthat the shape of theta hat",
    "start": "1468510",
    "end": "1476590"
  },
  {
    "text": "minus theta star,\nthe randomness-- the shape is the same as kind\nof the inverse of the Hessian.",
    "start": "1476590",
    "end": "1484600"
  },
  {
    "text": "So in those directions where\nyour Hessian is steeper, then",
    "start": "1484600",
    "end": "1489690"
  },
  {
    "text": "you have less\nstochasticity, right? And in those directions\nwhere the Hessian is smaller,",
    "start": "1489690",
    "end": "1495120"
  },
  {
    "text": "then you have more\nstochasticity. And the last one is\nsaying that it doesn't matter what the Hessian is.",
    "start": "1495120",
    "end": "1501120"
  },
  {
    "text": "The only thing that matters\nis the number of parameters. If you care about this\nkind of asymptotic regime,",
    "start": "1501120",
    "end": "1506820"
  },
  {
    "text": "the only thing that\nmatters is the parameter p, the number of parameters.",
    "start": "1506820",
    "end": "1512137"
  },
  {
    "text": "We're going to discuss\nthe limitation of all of these theorems in a moment. But this is what we got from\nthis asymptotic approach.",
    "start": "1512137",
    "end": "1519895"
  },
  {
    "start": "1519895",
    "end": "1528630"
  },
  {
    "text": "Any questions so far? ",
    "start": "1528630",
    "end": "1544080"
  },
  {
    "text": "OK, cool. So I guess if you're\ninterested in more details, you can take a look\nat the lecture notes.",
    "start": "1544080",
    "end": "1551429"
  },
  {
    "text": "So I guess now, let's move\non to uniform convergence. And often, people call this\nline of research nonasymptotic",
    "start": "1551430",
    "end": "1563500"
  },
  {
    "text": "So let's first discuss that. This is actually the\nkind of the approach",
    "start": "1563500",
    "end": "1570952"
  },
  {
    "text": "that we're going to take\nfor the rest of the lecture. We are going to care\nabout nonasymptotics instead of asymptotic ones.",
    "start": "1570952",
    "end": "1577470"
  },
  {
    "text": "So let me define what it is and\nmotivate why we care about it.",
    "start": "1577470",
    "end": "1585299"
  },
  {
    "text": "So recall that when you\nhave asymptotic bounds, just",
    "start": "1585300",
    "end": "1590680"
  },
  {
    "text": "like what I wrote\nabove, you know that this L theta hat\nminus L theta star-- the final outcome is\nsomething like, this",
    "start": "1590680",
    "end": "1596840"
  },
  {
    "text": "is equal to p over 2n\nplus theta of 1 over n.",
    "start": "1596840",
    "end": "1603640"
  },
  {
    "text": "However, the problem\nis that here, you are hiding a lot of things in\nthis little o of theta of 1,",
    "start": "1603640",
    "end": "1610720"
  },
  {
    "text": "little o of 1 over n. So you hide all dependencies\nother than the dependencies",
    "start": "1610720",
    "end": "1619740"
  },
  {
    "text": "on n, other than n. So what does it mean? So it means little o\nnotation-- you also",
    "start": "1619740",
    "end": "1627130"
  },
  {
    "text": "hide a dependency on p. So if you tell me in\nasymptotic regime, you get this bound,\nwhat happens is",
    "start": "1627130",
    "end": "1633880"
  },
  {
    "text": "that you could either have p\nover 2n plus 1 over n squared. Maybe the real rate is this.",
    "start": "1633880",
    "end": "1641000"
  },
  {
    "text": "It could also happen that\nthe real rate could be this. ",
    "start": "1641000",
    "end": "1646140"
  },
  {
    "text": "So both of these two cases would\nbe a possible situation if you tell me the bond above, right?",
    "start": "1646140",
    "end": "1652132"
  },
  {
    "text": "I wouldn't have ways\nto distinguish this, because this one is hidden\nin this little o notation.",
    "start": "1652132",
    "end": "1661298"
  },
  {
    "text": "Because the little\no notation doesn't care about any dependencies\nor anything else. It only cares about the\ndependencies on 1 over N,",
    "start": "1661298",
    "end": "1667070"
  },
  {
    "text": "at least in the\ncontext of asymptotics. So this is the problem,\nbecause clearly,",
    "start": "1667070",
    "end": "1674150"
  },
  {
    "text": "if your rate is something\non the right-hand side, then this is very\nbad rate, right?",
    "start": "1674150",
    "end": "1679310"
  },
  {
    "text": "Very bad. By the way, by rate, I mean\nhow does this depend on--",
    "start": "1679310",
    "end": "1686323"
  },
  {
    "text": "I guess maybe let's just\ncall it bound, right? So suppose your bound is\non the right-hand side. Then, it's a very bad bound,\nbecause this requires n",
    "start": "1686323",
    "end": "1696580"
  },
  {
    "text": "to be bigger than p to the\n50 so that this bound is smaller than 1, right?",
    "start": "1696580",
    "end": "1702573"
  },
  {
    "text": "Because you need a second\nterm to be smaller than 1. Then, you need n to be\nbigger than p to the 50.",
    "start": "1702573",
    "end": "1707679"
  },
  {
    "text": "So just [INAUDIBLE] definition\nof little o of 1 over n. Does that mean that\nn times the function",
    "start": "1707680",
    "end": "1713440"
  },
  {
    "text": "goes to 0 as n goes to infinity? Yes, yes. Yeah. Exactly.",
    "start": "1713440",
    "end": "1720440"
  },
  {
    "text": "So OK, I'm going back to this. So the bound on the right-hand\nside is going to be very bad.",
    "start": "1720440",
    "end": "1726970"
  },
  {
    "text": "And the bound on the\nleft-hand side-- this one-- is pretty good\nin some sense, right?",
    "start": "1726970",
    "end": "1732790"
  },
  {
    "text": "But you have no way\nto distinguish them, because these two things would\nbe coming towards p over 2n",
    "start": "1732790",
    "end": "1738910"
  },
  {
    "text": "plus little o 1 over n\nin the asymptotic sense. So that's the biggest problem.",
    "start": "1738910",
    "end": "1747250"
  },
  {
    "text": "And also, in some sense, when\nyou have other dependencies--",
    "start": "1747250",
    "end": "1754570"
  },
  {
    "text": "for example, the\ndimensionality-- even the dependencies\non n is not the only thing that matters. For example, another\nmore extreme situation",
    "start": "1754570",
    "end": "1762370"
  },
  {
    "text": "is that suppose you compare\np over square root of n versus p over 2n plus this.",
    "start": "1762370",
    "end": "1770440"
  },
  {
    "text": "Right? Suppose you have\ntwo of these bounds. And if you use\nasymptotics, if you",
    "start": "1770440",
    "end": "1777120"
  },
  {
    "text": "write in the asymptotic\nway, then you are going to conclude\nthat this is p over 2n",
    "start": "1777120",
    "end": "1782190"
  },
  {
    "text": "plus little o of 1 over n\nin the asymptotic language. And this one will be something\nlike p over square root of n",
    "start": "1782190",
    "end": "1788309"
  },
  {
    "text": "plus little o of 1\nover square root of n. So it sounds like this is bad,\nbecause this one has higher",
    "start": "1788310",
    "end": "1793740"
  },
  {
    "text": "order dependencies on n, right? Indeed, too, when\nn goes to infinity,",
    "start": "1793740",
    "end": "1799529"
  },
  {
    "text": "the right-hand side is smaller\nis than the left-hand side. But if you think about a\nmore moderate regime of n,",
    "start": "1799530",
    "end": "1805710"
  },
  {
    "text": "then it's not really true. Because for the bound\nto be less than 1--",
    "start": "1805710",
    "end": "1811075"
  },
  {
    "text": "so if you want p over square\nroot of n to be less than 1, this means that N is\nbigger than p squared.",
    "start": "1811075",
    "end": "1817080"
  },
  {
    "text": "But if you want this p\nover 2n plus p of 100 and square root\nto be less than 1,",
    "start": "1817080",
    "end": "1822750"
  },
  {
    "text": "this means n needs to be at\nleast larger than p to the 50,",
    "start": "1822750",
    "end": "1828610"
  },
  {
    "text": "right? So when N goes to infinity,\nthen the left-hand side--",
    "start": "1828610",
    "end": "1835679"
  },
  {
    "text": "it's worse. It's a worse bound. But in most of the\ncases, the left-hand side is actually a better bound. So if you want the left-hand\nside to be a better bound than",
    "start": "1835680",
    "end": "1843450"
  },
  {
    "text": "the right-hand side, I\nguess if you solve this, maybe you can even ignore the--",
    "start": "1843450",
    "end": "1849210"
  },
  {
    "text": "if you solve this,\nthis is roughly saying that if N\nis smaller than--",
    "start": "1849210",
    "end": "1854460"
  },
  {
    "text": "I think I did this\ncalculation at some point-- N is smaller than\np to the 6 6, then",
    "start": "1854460",
    "end": "1860612"
  },
  {
    "text": "actually, the bound\non the left-hand side is better than the bound on\nthe right-hand side, just because this p to the\n100 is too big, right?",
    "start": "1860612",
    "end": "1866640"
  },
  {
    "text": "So basically, the comparison-- basically, if you use\nthis asymptotic language,",
    "start": "1866640",
    "end": "1872340"
  },
  {
    "text": "things become a little weird if\nyou consider other dependencies on other parameters.",
    "start": "1872340",
    "end": "1877600"
  },
  {
    "text": "For example, if you\nhave a dependency on the a dimension for\nmodern machine learning, it's very high.",
    "start": "1877600",
    "end": "1883120"
  },
  {
    "text": "So this is why I think\nasymptotics, even though they are very powerful, they don't\nnecessarily always apply",
    "start": "1883120",
    "end": "1889049"
  },
  {
    "text": "to the modern machine\nlearning, just because it has the dependency\nfor other terms in the higher",
    "start": "1889050",
    "end": "1897090"
  },
  {
    "text": "order case, right? So this one has the\ndependency on p.",
    "start": "1897090",
    "end": "1905640"
  },
  {
    "text": "So that's the main\nissue, basically. OK, so what we do--",
    "start": "1905640",
    "end": "1911400"
  },
  {
    "text": "how do we fix this, right? The first thing we\nneed to do is to fix the language in some sense. We need to not only\nconsider n goes to infinity.",
    "start": "1911400",
    "end": "1919390"
  },
  {
    "text": "We have to also consider other\nquantities in this vault. So basically, what nonasymptotic\ndoes is that you care about--",
    "start": "1919390",
    "end": "1930240"
  },
  {
    "text": "this is just a term,\nor a kind of approach. This is basically\nsaying that you only",
    "start": "1930240",
    "end": "1937230"
  },
  {
    "text": "had absolute constant\nin your bound.",
    "start": "1937230",
    "end": "1943230"
  },
  {
    "text": "You have to hide\nsomething, because if you have to care about\nevery constant, it's going to be too\ncomplicated for theory, right?",
    "start": "1943230",
    "end": "1949110"
  },
  {
    "text": "It's going to be a\nlot of calculations. But here, we allow us to\nhide absolute constant.",
    "start": "1949110",
    "end": "1955080"
  },
  {
    "text": "But we cannot hide any other\ndependencies or any other things. So you are not allowed\nto have a dependency on p",
    "start": "1955080",
    "end": "1962580"
  },
  {
    "text": "when n goes to infinity. And the absolute\nconstant-- this really means that this is a\nuniversal concept connecting",
    "start": "1962580",
    "end": "1969840"
  },
  {
    "text": "3, 5-- something you can replace\nby a real numerical number. ",
    "start": "1969840",
    "end": "1976500"
  },
  {
    "text": "And actually, to make\neverything easier, so we are going to\nintroduce this notation--",
    "start": "1976500",
    "end": "1983250"
  },
  {
    "text": "big O notation. This is actually-- sometimes,\nthis big O notation has a little bit\ndifferent interpretation.",
    "start": "1983250",
    "end": "1991080"
  },
  {
    "text": "So I wouldn't say\nI'm redefining it, but I'm going to just be\nclear about what the big O",
    "start": "1991080",
    "end": "1997580"
  },
  {
    "text": "notation means from now on. So now, big O\nnotations from now on-- only as high as\nuniversal constant.",
    "start": "1997580",
    "end": "2004559"
  },
  {
    "text": "And let me have, actually,\na more technical definition, which is actually\nuseful in some cases",
    "start": "2004560",
    "end": "2010520"
  },
  {
    "text": "when you're really doing a\nlot of theories sometimes. I'm not sure whether\nsome of you have this confusion about whether\nyou should use big O or omega--",
    "start": "2010520",
    "end": "2019720"
  },
  {
    "text": "the big omega. Sometimes, it\ncould be confusing. So let me define what\nthis big O really means.",
    "start": "2019720",
    "end": "2025210"
  },
  {
    "text": "It really means that--\nso every occurrence-- ",
    "start": "2025210",
    "end": "2031230"
  },
  {
    "text": "at least, this is what\nit means in this course. It may not be exactly always\nthe same for every paper. But I think people\nare converging",
    "start": "2031230",
    "end": "2037080"
  },
  {
    "text": "to this interpretation. So every occurrence\nof big O of x",
    "start": "2037080",
    "end": "2042280"
  },
  {
    "text": "is a placeholder\nfor some function",
    "start": "2042280",
    "end": "2051835"
  },
  {
    "text": "like say, fx, such\nthat for every x in R,",
    "start": "2051835",
    "end": "2063570"
  },
  {
    "text": "fx is less than Cx for\nsome absolute constant C",
    "start": "2063570",
    "end": "2075469"
  },
  {
    "text": "bigger than 0. So basically, this is\nsaying that if you replace-- so maybe more\nexplicitly, it's saying",
    "start": "2075469",
    "end": "2083239"
  },
  {
    "text": "that you can\nreplace O of x by fx",
    "start": "2083239",
    "end": "2095129"
  },
  {
    "text": "such that the statement is true. ",
    "start": "2095130",
    "end": "2104310"
  },
  {
    "text": "So basically, if\nyou see a statement with a lot of O of\nx, O of something,",
    "start": "2104310",
    "end": "2110361"
  },
  {
    "text": "right, it means\nthat you can replace all of these occurrences\nof big O notations by something more explicit such\nthat the statement is still",
    "start": "2110362",
    "end": "2118590"
  },
  {
    "text": "true. So it seem to be overkill\nas a definition of big O, which you're probably\nalready familiar with.",
    "start": "2118590",
    "end": "2125270"
  },
  {
    "text": "But in some cases, at least\nI've seen so many cases where I got confused.",
    "start": "2125270",
    "end": "2130430"
  },
  {
    "text": "I have to kind of\nreally literally verify whether I satisfy\nthis definition.",
    "start": "2130430",
    "end": "2135980"
  },
  {
    "text": "Anyway, OK. And also, just for\nnotational convenience, sometimes we also\nwrite a is less than b.",
    "start": "2135980",
    "end": "2143020"
  },
  {
    "text": "This is just equivalent to\nthe exist absolute constant C",
    "start": "2143020",
    "end": "2152190"
  },
  {
    "text": "greater than 0 such that\na is less than C times b.",
    "start": "2152190",
    "end": "2157579"
  },
  {
    "text": "And technically, if you\nreally want to be very solid,",
    "start": "2157580",
    "end": "2162620"
  },
  {
    "text": "this statement should only\napply to positive a and b-- positive a and b.",
    "start": "2162620",
    "end": "2168230"
  },
  {
    "text": "Because for negative ones,\nyou probably, ideally, you should just write this\nonly for positive a and b.",
    "start": "2168230",
    "end": "2174410"
  },
  {
    "text": "That's my suggestion,\nbecause for negative ones, it just becomes a\nlittle bit confusing.",
    "start": "2174410",
    "end": "2179724"
  },
  {
    "text": " So the point here is\nthat there is no-- well, I defined this\nbig O thing, right?",
    "start": "2179724",
    "end": "2186080"
  },
  {
    "text": "So it depends on the literature. Sometimes, when\npeople define big O, they have to define some limit. But here in this\ncourse, the big O",
    "start": "2186080",
    "end": "2193400"
  },
  {
    "text": "just really means there's\nno limit taking-- you don't have to think about any limit.",
    "start": "2193400",
    "end": "2199660"
  },
  {
    "text": "So for a and b functions\nhere, because a and b are positive\nnumbers, every number is less than every\nother number, right?",
    "start": "2199660",
    "end": "2205690"
  },
  {
    "text": "Right. So a and b could be functions of\nother, more complex quantities. ",
    "start": "2205690",
    "end": "2215470"
  },
  {
    "text": "OK, cool. So these are just\nsome notations. OK, so now, the bound\nwe care about is--",
    "start": "2215470",
    "end": "2221950"
  },
  {
    "text": " so we are interested\nin this notation.",
    "start": "2221950",
    "end": "2228500"
  },
  {
    "text": "We are interested in\nbounds of the form,",
    "start": "2228500",
    "end": "2234040"
  },
  {
    "text": "like the excess risk-- ",
    "start": "2234040",
    "end": "2239480"
  },
  {
    "text": "so it's actually bound excess\nrisk l theta hat minus l theta star by something like big O\nof some function of maybe p",
    "start": "2239480",
    "end": "2250590"
  },
  {
    "text": "and n, where p could\nbe a dimension and n could be the number\nof data points.",
    "start": "2250590",
    "end": "2257369"
  },
  {
    "text": "Of course, you can replace this\nby a function of other things. But the point here is\nthat after you write this,",
    "start": "2257370",
    "end": "2263039"
  },
  {
    "text": "there's nothing else\nhidden in the big O, only a universal constant. And once you have\nthis kind of language,",
    "start": "2263040",
    "end": "2270930"
  },
  {
    "text": "you can compare things\nin a more proper way. And in the next few\nlectures, our goal",
    "start": "2270930",
    "end": "2276060"
  },
  {
    "text": "is to basically\nshow how to provide bounds of this kind of form. Sometimes, the bound could\nbe more complicated, not only",
    "start": "2276060",
    "end": "2281790"
  },
  {
    "text": "depending on the number\nof parameters and number of data points. It could depend on the normal\nparameters and so forth.",
    "start": "2281790",
    "end": "2289440"
  },
  {
    "text": "The point is that we always\nonly had universal constants. ",
    "start": "2289440",
    "end": "2295650"
  },
  {
    "text": "Any questions? So [INAUDIBLE] the theta\nis very [INAUDIBLE]",
    "start": "2295650",
    "end": "2306280"
  },
  {
    "text": "for some function\n[INAUDIBLE] but could that be for all of them?",
    "start": "2306280",
    "end": "2311500"
  },
  {
    "text": "For some, yes. That's very important, because\nif you replace it for all-- because here, there's also--",
    "start": "2311500",
    "end": "2316510"
  },
  {
    "text": " no, I think it's for some.",
    "start": "2316510",
    "end": "2323000"
  },
  {
    "text": "So you literally\nonly need existence of one function\nthat has this such that if you replace your\nstatement by f of x,",
    "start": "2323000",
    "end": "2332000"
  },
  {
    "text": "the statement is true. So yeah, I think this is\nactually a very good question. Because I got confused\nby this many times. So maybe let's give\nan example, right?",
    "start": "2332000",
    "end": "2339870"
  },
  {
    "text": "So you say, the excess\nrisk is less than O",
    "start": "2339870",
    "end": "2345910"
  },
  {
    "text": "of 1 over square root of 10. What does this mean? This means that you\ncan replace this.",
    "start": "2345910",
    "end": "2351270"
  },
  {
    "text": "This is your fn, right? You can replace this\nby [INAUDIBLE] sum of 5 over square root of 10 such\nthat this is exactly true.",
    "start": "2351270",
    "end": "2359240"
  },
  {
    "text": "But you don't need to\nsay that for every f. So if you say for\nevery f, then it means that if you place that\n0.1 over the square root of n,",
    "start": "2359240",
    "end": "2365960"
  },
  {
    "text": "it still has to be true, right? That's too much, right? You only need the\nexistence of one.",
    "start": "2365960",
    "end": "2371000"
  },
  {
    "text": "But of course, if you\nhave existence of one f, then there's always\nother f, which is bigger,",
    "start": "2371000",
    "end": "2376490"
  },
  {
    "text": "that can also be replaced. But you only need one f. And also, actually, maybe\nthis is a little bit advanced.",
    "start": "2376490",
    "end": "2383650"
  },
  {
    "text": "But this kind of\ninterpretation also allows you to have big O\nin your condition, even.",
    "start": "2383650",
    "end": "2389380"
  },
  {
    "text": "For example, this could\nbe a little bit advanced. But for example, you\ncan write for all--",
    "start": "2389380",
    "end": "2394665"
  },
  {
    "text": " if n is bigger than O of p,\nthen excess risk is less than 1.",
    "start": "2394665",
    "end": "2408619"
  },
  {
    "text": " I'm not saying this is\na correct statement, but this statement\nwould be interpreted as,",
    "start": "2408620",
    "end": "2414390"
  },
  {
    "text": "if you replace\nthis O of p by 2p, then it's going to be correct. Or if you replace this\nO of p by some function,",
    "start": "2414390",
    "end": "2420725"
  },
  {
    "text": "some constant\ntimes p, it's going to be a correct statement. And it's not omega here. It's really big O, which\nis sometimes confusing.",
    "start": "2420725",
    "end": "2430740"
  },
  {
    "text": "OK, cool. So now, let's move on to the key\nidea that we are going to have,",
    "start": "2430740",
    "end": "2437349"
  },
  {
    "text": "right? So to bound these\nexcess rates, how do we achieve a bound like this? The key idea is to somehow\nsay that L hat theta is",
    "start": "2437350",
    "end": "2447450"
  },
  {
    "text": "close to L theta, right? In some way, in some sense.",
    "start": "2447450",
    "end": "2453760"
  },
  {
    "text": "I need to specify what I\nreally mean by these two functions are close, right? Are they close at every\ntheta, or are they",
    "start": "2453760",
    "end": "2460050"
  },
  {
    "text": "close at a specific theta? So here is a small claim which\ntells what you really need.",
    "start": "2460050",
    "end": "2467920"
  },
  {
    "text": "So what you need is that--\nso suppose L hat theta",
    "start": "2467920",
    "end": "2473630"
  },
  {
    "text": "star is close to L theta star. Suppose these two\nloss functions,",
    "start": "2473630",
    "end": "2479080"
  },
  {
    "text": "empirical and population\nloss, are close at theta star. And also suppose they\nare close at theta hat.",
    "start": "2479080",
    "end": "2493230"
  },
  {
    "text": "And here, actually, you only\nneed one step closeness. So suppose you\nhave both of those.",
    "start": "2493230",
    "end": "2499349"
  },
  {
    "text": "Then this implies that L\ntheta hat minus L theta",
    "start": "2499350",
    "end": "2504870"
  },
  {
    "text": "star is less than 2 alpha. So basically, you just need\nto show that these two loss",
    "start": "2504870",
    "end": "2511127"
  },
  {
    "text": "functions-- the empirical\nloss and population loss are close at theta\nstar and at theta hat.",
    "start": "2511127",
    "end": "2517260"
  },
  {
    "text": "Then, you can can bound the\nexcess rates by 2 times alpha. And the proof is\nactually very simple.",
    "start": "2517260",
    "end": "2524079"
  },
  {
    "text": "What you do is that you know\nthat this is comparing theta hat with L theta star, right? And your condition involves\ncomparing L versus L hat.",
    "start": "2524080",
    "end": "2532230"
  },
  {
    "text": "So you have to do some\narrangement to link them, right? So what you do is you say I\nwant to compare these two.",
    "start": "2532230",
    "end": "2540840"
  },
  {
    "text": "And I write this as\na sum of three terms. L theta hat minus\nL hat theta hat.",
    "start": "2540840",
    "end": "2549200"
  },
  {
    "text": "You first compare this L theta\nhat by L hat of theta hat.",
    "start": "2549200",
    "end": "2554599"
  },
  {
    "text": "And then, you have\nL hat theta hat. You compare this with\nL hat theta star.",
    "start": "2554600",
    "end": "2560150"
  },
  {
    "text": "And then, you compare L\ntheta star with L theta star.",
    "start": "2560150",
    "end": "2566740"
  },
  {
    "text": "Anyway, I don't know why\nthe video freezes again. Let me restart it.",
    "start": "2566740",
    "end": "2572660"
  },
  {
    "start": "2572660",
    "end": "2578860"
  },
  {
    "text": "OK, and the reason why\nthis should freeze-- OK.",
    "start": "2578861",
    "end": "2584070"
  },
  {
    "text": "So why don't we do the\nthree things, right? Once you see it,\nit's kind of obvious,",
    "start": "2584070",
    "end": "2590530"
  },
  {
    "text": "because this one\nis the condition-- one of the conditions, right? This one is the\nsecond condition.",
    "start": "2590530",
    "end": "2598280"
  },
  {
    "text": " And this one is the\nfirst condition.",
    "start": "2598280",
    "end": "2605990"
  },
  {
    "text": "And you also have\nthis one, which compares directly theta\nhat and theta star. But this is comparing\nthem at L hat.",
    "start": "2605990",
    "end": "2612440"
  },
  {
    "text": "And you know that L hat theta\nhat minus L hat theta star",
    "start": "2612440",
    "end": "2617810"
  },
  {
    "text": "is less than 0, because theta\nhat is the minimizer of L hat.",
    "start": "2617810",
    "end": "2625910"
  },
  {
    "text": "So this term is less than 0. And this term is\nless than alpha.",
    "start": "2625910",
    "end": "2633319"
  },
  {
    "text": "This term is less than alpha. So in total, you\ncontinue to get 2 alpha.",
    "start": "2633320",
    "end": "2642365"
  },
  {
    "text": "OK? So basically, this is\nsaying that it suffices to show the two conditions.",
    "start": "2642365",
    "end": "2648640"
  },
  {
    "text": "The first condition\nis that L hat and L is close as theta star. The second condition\nis that L hat and L",
    "start": "2648640",
    "end": "2655290"
  },
  {
    "text": "are close at theta hat. ",
    "start": "2655290",
    "end": "2665770"
  },
  {
    "text": "So it turns out\nthat the challenge to prove these two\ninequalities are-- the difficulties are\ncompletely different.",
    "start": "2665770",
    "end": "2672400"
  },
  {
    "text": "So let's say, if this is\nnumber 1, this is number 2, number one is very,\nvery easy to prove.",
    "start": "2672400",
    "end": "2678710"
  },
  {
    "text": "And number 2 will\nrequire a lot of work, which takes you a few weeks. ",
    "start": "2678710",
    "end": "2685020"
  },
  {
    "text": "Maybe not a few\nweeks, but two weeks. Is there a reason why in the\nfirst inequality [INAUDIBLE]",
    "start": "2685020",
    "end": "2690906"
  },
  {
    "text": "value and the second\ninequality, it's not? ",
    "start": "2690906",
    "end": "2697760"
  },
  {
    "text": "The only reason\nis that of course, if you put absolute value\nhere, it's still true, right?",
    "start": "2697760",
    "end": "2704060"
  },
  {
    "text": "And actually, you can also bound\nthe actual value if you want. The only reason is that if you\ndon't have the absolute value",
    "start": "2704060",
    "end": "2710570"
  },
  {
    "text": "to show these conditions\nare satisfied, it's a little bit\neasier, slightly easier. You need one fewer step.",
    "start": "2710570",
    "end": "2717290"
  },
  {
    "text": "That's why in most of the\nbooks, you don't have that step. And also, you save a\nconstant, a factor of 2.",
    "start": "2717290",
    "end": "2723050"
  },
  {
    "text": "So actually, this is\na very good question. In my first time I teach this,\nI just have absolute value.",
    "start": "2723050",
    "end": "2728359"
  },
  {
    "text": "And then later in\nthe lecture, I have to do additional steps to\nfix that constant, which",
    "start": "2728360",
    "end": "2733400"
  },
  {
    "text": "makes it a little bit annoying. But fundamentally,\nyou are right. There is no real difference.",
    "start": "2733400",
    "end": "2739510"
  },
  {
    "text": "You don't run into\nthat problem when you show the first inequality? You don't run into that problem\nin the first inequality,",
    "start": "2739510",
    "end": "2745350"
  },
  {
    "text": "yeah, which I'm going to show\nthe first inequality just right now. The first inequality\nis very easy.",
    "start": "2745350",
    "end": "2751549"
  },
  {
    "text": "And I'll tell you why\nthey are different. It sounds like they are\nvery similar, right? So the difference is that--",
    "start": "2751550",
    "end": "2758630"
  },
  {
    "text": "let me see whether\nit's ready for me to talk about difference. ",
    "start": "2758630",
    "end": "2764278"
  },
  {
    "text": "Let me not talk about\nthe difference first. Let me first show\nthe inequality 1, and see why it's\nrelatively easy.",
    "start": "2764278",
    "end": "2770930"
  },
  {
    "text": "And so to do that, so\nthe goal is to show 1.",
    "start": "2770930",
    "end": "2776150"
  },
  {
    "text": " And the main tool\nwe are going to use",
    "start": "2776150",
    "end": "2782240"
  },
  {
    "text": "is the so-called\nconcentration inequality. ",
    "start": "2782240",
    "end": "2790180"
  },
  {
    "text": "And this is, in some sense,\na nonasymptotic version",
    "start": "2790180",
    "end": "2795220"
  },
  {
    "text": "of the law of large number. So it's trying to\nprove the same things but in a different language\nand with a stronger form.",
    "start": "2795220",
    "end": "2802610"
  },
  {
    "text": "So this is the nonasymptotic\nversion, I guess, of central limit theorem.",
    "start": "2802610",
    "end": "2808630"
  },
  {
    "text": "And now, you don't have\nto deal with the limit. You just have a bound\nthat depends on it.",
    "start": "2808630",
    "end": "2815180"
  },
  {
    "text": "And I think probably\nsome of you have heard of this inequality\ncalled Hoeffding inequality.",
    "start": "2815180",
    "end": "2820900"
  },
  {
    "text": "I think this thing probably\nis going to be taught in 109-- CS109 or some of the\nstatistics classes.",
    "start": "2820900",
    "end": "2828910"
  },
  {
    "text": "But anyway, you\ndon't have to know it before as a prerequisite. And let me define\nthe inequality.",
    "start": "2828910",
    "end": "2835100"
  },
  {
    "text": "So this is trying\nto deal with a sum of independent random variables. So let's say x1 up to xn will\nbe independent random variables.",
    "start": "2835100",
    "end": "2848110"
  },
  {
    "text": "And suppose they are bounded. ",
    "start": "2848110",
    "end": "2855059"
  },
  {
    "text": "Each of them is\nbounded by ai and bi. You can think ai and bi are\njust constant, maybe 0 and 1.",
    "start": "2855060",
    "end": "2860970"
  },
  {
    "text": "And almost surely, for every i-- and so we care about the mean.",
    "start": "2860970",
    "end": "2869170"
  },
  {
    "text": "So the mean of this random\nvariable is defined to be xi-- sorry, is defined to be mu.",
    "start": "2869170",
    "end": "2877900"
  },
  {
    "text": "And so the central\nquestion is, how different",
    "start": "2877900",
    "end": "2883609"
  },
  {
    "text": "is the empirical mean from the\naverage, from the expectation?",
    "start": "2883610",
    "end": "2889970"
  },
  {
    "text": "All right, so we care\nabout how small is this. And this is a random variable. So you have to have a\nprobabilistic statement.",
    "start": "2889970",
    "end": "2896099"
  },
  {
    "text": "So the claim is\nthat the probability that this difference\nis small is very big.",
    "start": "2896100",
    "end": "2903335"
  },
  {
    "start": "2903335",
    "end": "2908512"
  },
  {
    "text": "Alternatively, you can\nsay that the probability that this difference\nis big is very small.",
    "start": "2908512",
    "end": "2913650"
  },
  {
    "text": "They are just the same. So you get how big it is. It's very close to 1. And the difference from 1\nis this exponentially small",
    "start": "2913650",
    "end": "2921210"
  },
  {
    "text": "number. And what's in the exponential\nis something like this. ",
    "start": "2921210",
    "end": "2932520"
  },
  {
    "text": "OK, so this is the\nformal statement. Maybe let me try to\ninterpret it a little bit by instantiating it.",
    "start": "2932520",
    "end": "2938070"
  },
  {
    "text": "It's a special case. So if you define sigma\nsquared to be 1 over n times",
    "start": "2938070",
    "end": "2944730"
  },
  {
    "text": "sum of bi minus ai\nsquare i from 1 to n, so then the sigma can be viewed\nas kind of the variance of 1",
    "start": "2944730",
    "end": "2964789"
  },
  {
    "text": "over sum of of xi. This is not exactly\nthe variance, right? But it's some upper\nbound of the variance.",
    "start": "2964790",
    "end": "2972210"
  },
  {
    "text": "Why? Because if you look at\nthe variance over n times the sum of xi, you know\nthat the variance is linear.",
    "start": "2972210",
    "end": "2980540"
  },
  {
    "text": "So first of all, you\nget 1 N square in front, because the variance\nis quadratic.",
    "start": "2980540",
    "end": "2986150"
  },
  {
    "text": "It's scaling. And then, in your\nrelatives, you have the sum of the variance of xi.",
    "start": "2986150",
    "end": "2994050"
  },
  {
    "text": "And then, this is equal\nto 1 over N squared-- ",
    "start": "2994050",
    "end": "2999560"
  },
  {
    "text": "the definition xi minus\nexpectation xi squared.",
    "start": "2999560",
    "end": "3004990"
  },
  {
    "text": "And now, because\neach of these xi-- xi is always between\nbi and ai, right?",
    "start": "3004990",
    "end": "3010750"
  },
  {
    "text": "So xi is between bi and ai.  And expectation of xi as a\nconsequence also is between bi",
    "start": "3010750",
    "end": "3020170"
  },
  {
    "text": "and ai. That means that this thing\nis smaller than bi minus ai,",
    "start": "3020170",
    "end": "3030700"
  },
  {
    "text": "because both of\nthese two quantities are in this interval. So the difference of them is\nalso smaller than bi and ai.",
    "start": "3030700",
    "end": "3037315"
  },
  {
    "text": "You get bi in a square\nfor each of these terms. So that's why the\ntotal, the whole thing--",
    "start": "3037315",
    "end": "3043480"
  },
  {
    "text": " maybe, I guess, also including\nthe 1 over n squared--",
    "start": "3043480",
    "end": "3048540"
  },
  {
    "text": "so the whole thing-- is smaller than 1 over n squared\ntimes the sum of bi minus",
    "start": "3048540",
    "end": "3055230"
  },
  {
    "text": "ai squared, from 1 to n. ",
    "start": "3055230",
    "end": "3060876"
  },
  {
    "text": "Right, let me see\nwhy I'm missing a-- I think I have a typo here.",
    "start": "3060876",
    "end": "3075750"
  },
  {
    "start": "3075750",
    "end": "3083430"
  },
  {
    "text": "OK, so basically, you can\nthink of each of these-- bi and ai squared\nis the variance.",
    "start": "3083430",
    "end": "3089910"
  },
  {
    "text": "And then, they would\ntake the sum of them and you divide by\n1 over n squared. That's kind of the\nvariance of the whole xi.",
    "start": "3089910",
    "end": "3100020"
  },
  {
    "text": "And suppose you take this view. And you can see what\nis this is saying. What this inequality is\nsaying is the following.",
    "start": "3100020",
    "end": "3106780"
  },
  {
    "text": "So if you take epsilon\nto be square root, some constant c times\nsigma squared times log n,",
    "start": "3106780",
    "end": "3116370"
  },
  {
    "text": "so this is something\nlike a constant O of sigma square root of log n. So you take epsilon to\nbe something a little bit",
    "start": "3116370",
    "end": "3123270"
  },
  {
    "text": "bigger than the variance\nby square root log n. Then, you know that you plug\nthis epsilon into the Hoeffding",
    "start": "3123270",
    "end": "3131040"
  },
  {
    "text": "inequality, where c\nis a large constant. ",
    "start": "3131040",
    "end": "3138070"
  },
  {
    "text": "Then, you plug in this to-- for example, c is\nlarger than 10. And if you plug in this epsilon\nto the Hoeffding inequality,",
    "start": "3138070",
    "end": "3144020"
  },
  {
    "text": "what you got is that--  so probability 1 over n\ntimes sum of xi minus mu.",
    "start": "3144020",
    "end": "3154807"
  },
  {
    "text": "This is actually the\nmost interesting regime of this inequality when you plug\nin epsilon to be on this level.",
    "start": "3154807",
    "end": "3160607"
  },
  {
    "text": "Typically, when you\nuse it, you always use epsilon to be this level. Because this is\nthe useful regime.",
    "start": "3160607",
    "end": "3166010"
  },
  {
    "text": "So when you apply it, you\nget this is less than O of sigma log n, because I\nreplaced this for epsilon.",
    "start": "3166010",
    "end": "3173690"
  },
  {
    "text": "And it's bigger than 1\nminus 2 times exponential. Now, let's plug in epsilon.",
    "start": "3173690",
    "end": "3179100"
  },
  {
    "text": "So you get, this is 2. Maybe let's first\nnot replace epsilon.",
    "start": "3179100",
    "end": "3185110"
  },
  {
    "text": "Let's first replace\nsigma, right? So you can see that\nthe right-hand side--",
    "start": "3185110",
    "end": "3190510"
  },
  {
    "text": "by my definition\nof sigma squared, so this is the same as\nthe Hoeffding inequality. And then, plug in epsilon, I get\n1 minus 2 exponential 2 log n.",
    "start": "3190510",
    "end": "3205750"
  },
  {
    "text": "2 times big O of log n. I guess 2 is also in\nthe big O, so you get--",
    "start": "3205750",
    "end": "3211390"
  },
  {
    "text": " right.",
    "start": "3211390",
    "end": "3216635"
  },
  {
    "text": "And now, you choose\nthis big O to be your large constant, right? So recall that this big O is--\nyou can replace this big O",
    "start": "3216635",
    "end": "3224865"
  },
  {
    "text": "by a large constant, right? So then, you got this to\nbe something like 1 minus,",
    "start": "3224865",
    "end": "3231320"
  },
  {
    "text": "maybe let's say, I\nguess-- maybe here, it's easier if I just\nkeep the c especially.",
    "start": "3231320",
    "end": "3238680"
  },
  {
    "text": "I got 2 c here. This is c.",
    "start": "3238680",
    "end": "3245490"
  },
  {
    "text": "Then you get n to the minus 2c. 2 times n to the minus 2c. And if you pick this constant\nc to be something like 10,",
    "start": "3245490",
    "end": "3252900"
  },
  {
    "text": "then you get 1 minus 2\nto the n minus 20, right? So basically, this is saying\nthat you have a very, very",
    "start": "3252900",
    "end": "3262620"
  },
  {
    "text": "high probability such\nthat the difference is smaller than sigma log n.",
    "start": "3262620",
    "end": "3267810"
  },
  {
    "text": "So in other words,\nwith high probability-- so with probability,\nlet's say, larger",
    "start": "3267810",
    "end": "3273270"
  },
  {
    "text": "than 1 minus N the minus 10-- you have that the\nempirical mean is",
    "start": "3273270",
    "end": "3282820"
  },
  {
    "text": "closer to the\nexpectation in the sense that they are close\nin this sense, right?",
    "start": "3282820",
    "end": "3291922"
  },
  {
    "text": "They're bounded. The difference between\nthem are bounded by big O of sigma times log n.",
    "start": "3291923",
    "end": "3298510"
  },
  {
    "text": "So basically, this is saying\nthat if you think of sigma as the variance, as the\nquote unquote \"variance,\"",
    "start": "3298510",
    "end": "3305109"
  },
  {
    "text": "then you cannot be-- it's very hard for you\nto deviate from the mean",
    "start": "3305110",
    "end": "3314500"
  },
  {
    "text": "by something much larger\nthan the variance, right? So this is the\ndeviation from the mean.",
    "start": "3314500",
    "end": "3319880"
  },
  {
    "text": "And this is the variance up\nto times square root of log n. The log factor in this\ncourse is not very important.",
    "start": "3319880",
    "end": "3325550"
  },
  {
    "text": "So this is saying you\ncannot deviate from the mean by a large factor\nof the variance.",
    "start": "3325550",
    "end": "3331580"
  },
  {
    "text": "Of course, this variance\nis not a real variance. It's this perceived variance.",
    "start": "3331580",
    "end": "3336597"
  },
  {
    "text": "Actually, we're going to\nget back to this concept. This is sometimes called-- there's a concept called\nvariance proxy, which we're",
    "start": "3336597",
    "end": "3345336"
  },
  {
    "text": "going to talk more about it. So in some sense, it's kind\nof like if you draw this,",
    "start": "3345336",
    "end": "3350619"
  },
  {
    "text": "it's kind of like you are saying\nthat this random variable-- ",
    "start": "3350620",
    "end": "3357420"
  },
  {
    "text": "suppose you would call this x\nhat, a random variable-- if you look at the distribution\nof this random variable,",
    "start": "3357420",
    "end": "3362550"
  },
  {
    "text": "it's something like this. And the mean is mu, right? Suppose this is mu. And you look at something\ndeviate from the mu",
    "start": "3362550",
    "end": "3372715"
  },
  {
    "text": "by sigma square root log n. And then, you are saying\nthat the mass in this part",
    "start": "3372715",
    "end": "3377829"
  },
  {
    "text": "is extremely small. How small they are? They are smaller than inverse\npolynomial of n, right?",
    "start": "3377830",
    "end": "3383050"
  },
  {
    "text": "So the mass here is smaller\nthan n to the minus 2c or maybe inverse poly [INAUDIBLE].",
    "start": "3383050",
    "end": "3389189"
  },
  {
    "start": "3389189",
    "end": "3401030"
  },
  {
    "text": "So you can see that this bound\ncan now be much, much smaller. And one of the ways to see it is\nthat if this is really a sigma,",
    "start": "3401030",
    "end": "3408062"
  },
  {
    "text": "it's really the\nstandard deviation. Then your bound cannot\nbe improved much, right? Because for any random\nvariable, you always",
    "start": "3408062",
    "end": "3416110"
  },
  {
    "text": "have some probability. So the bound cannot\nbe improved much.",
    "start": "3416110",
    "end": "3426680"
  },
  {
    "text": "Of course, this is a somewhat\njust intuition, right? Because I need to define what\nthey mean by not improved much.",
    "start": "3426680",
    "end": "3433100"
  },
  {
    "text": "But intuitively,\nthis bound shouldn't be able to improve much,\nbecause for any random variable,",
    "start": "3433100",
    "end": "3441680"
  },
  {
    "text": "you always have some mass. There's always some mass within\nmean plus minus 10 deviation,",
    "start": "3441680",
    "end": "3452750"
  },
  {
    "text": "right? So if you really look\nat the interval defined",
    "start": "3452750",
    "end": "3460053"
  },
  {
    "text": "by the standard\ndeviation, there's always some mass in that, right? There's actual\nconstant mass in that. So you cannot make\nthese intervals much,",
    "start": "3460053",
    "end": "3469900"
  },
  {
    "text": "much smaller and\nget the same bound, because if you get it too\nsmall, then you have a lot of messiness.",
    "start": "3469900",
    "end": "3476390"
  },
  {
    "text": "So OK, cool. So now, let's interpret\nthis a little more. So let's say we take a, and\nwe instantiate even more.",
    "start": "3476390",
    "end": "3484369"
  },
  {
    "text": "So let's take a to be on\nthe order of maybe minus 1. It's a negative number.",
    "start": "3484370",
    "end": "3489770"
  },
  {
    "text": "And b is on the\norder of 1, right? So this is typically the\nimportant thing, right? So your random variable\nis between minus 1,",
    "start": "3489770",
    "end": "3495380"
  },
  {
    "text": "maybe minus a constant. But constant--\nthen what you have is that the empirical\nmean minus the expectation",
    "start": "3495380",
    "end": "3503840"
  },
  {
    "text": "is smaller than big O of\nsigma square root log n. This is the same\nthing I have written.",
    "start": "3503840",
    "end": "3509900"
  },
  {
    "text": "And what is sigma? Sigma is square root 1 over\nn squared times the sum of bi",
    "start": "3509900",
    "end": "3516140"
  },
  {
    "text": "minus ai square. And this is something--\neach of the bi",
    "start": "3516140",
    "end": "3521422"
  },
  {
    "text": "and ai is on the order of 1. So you get 1 over\nn squared times n, because there are\nn of these terms. So this is 1 over\nsquare root of n, right?",
    "start": "3521423",
    "end": "3529520"
  },
  {
    "text": "So sigma is on the order\nof 1 over square root of n. And that's the variance\nof your mean estimate",
    "start": "3529520",
    "end": "3536210"
  },
  {
    "text": "of the empirical mean. So that's why if you plug\nin this choice of 2 sigma, you get square root\nn, square root log n.",
    "start": "3536210",
    "end": "3544430"
  },
  {
    "text": "So basically, you\ncannot deviate by-- and sometimes, people\nwrite this as O tilde of 1",
    "start": "3544430",
    "end": "3556635"
  },
  {
    "text": "over the square root of n just\nto hide all the log factors. So if you don't care\nabout the log factor,",
    "start": "3556635",
    "end": "3562240"
  },
  {
    "text": "it's basically saying that you\ncannot deviate by more than 1 over square root of 10.",
    "start": "3562240",
    "end": "3567910"
  },
  {
    "text": "It sounds very abstract\nfor the moment. But in the long run, you'll\nsee that this kind of thinking",
    "start": "3567910",
    "end": "3575380"
  },
  {
    "text": "will be used many times. And it's actually useful to\njust burn this in your head if you really do machine\nlearning theory for life.",
    "start": "3575380",
    "end": "3582903"
  },
  {
    "text": "But you don't have to. But for me, this\nis something like-- basically, I already burned this\ninto my head, in some sense.",
    "start": "3582903",
    "end": "3592539"
  },
  {
    "text": "Any questions? Oh. ",
    "start": "3592540",
    "end": "3598370"
  },
  {
    "text": "OK, so this is a short review.",
    "start": "3598370",
    "end": "3607017"
  },
  {
    "text": "I'm not sure whether the whole-- I think probably CS109 will\nget into these kind of details, but this is just kind of a\nreview of Hoeffding inequality",
    "start": "3607017",
    "end": "3615580"
  },
  {
    "text": "with a little bit of kind of\nadditional interpretation. And now, if you apply Hoeffding\ninequality to our case,",
    "start": "3615580",
    "end": "3627070"
  },
  {
    "text": "let's see what we can get\nthrough the empirical laws,",
    "start": "3627070",
    "end": "3635520"
  },
  {
    "text": "right? Recall that our goal\nis to deal with this.",
    "start": "3635520",
    "end": "3640920"
  },
  {
    "text": "The difference between\nthis and this, right? And this one is 1\nover n times the sum",
    "start": "3640920",
    "end": "3647040"
  },
  {
    "text": "of the loss on each\nof the examples. ",
    "start": "3647040",
    "end": "3657080"
  },
  {
    "text": "And this one is really literally\nthe expectation of the sum, right? ",
    "start": "3657080",
    "end": "3666640"
  },
  {
    "text": "And so this is a perfect case\nto use Hoeffding inequality, because this one\ncorresponds to xi.",
    "start": "3666640",
    "end": "3674234"
  },
  {
    "text": "But Hoeffding\ninequality requires a bound on a random variable. So we just assume\nthat in many cases,",
    "start": "3674235",
    "end": "3679630"
  },
  {
    "text": "the loss is indeed bounded. But here, we assume the loss\nis bounded between 0 and 1.",
    "start": "3679630",
    "end": "3690549"
  },
  {
    "text": "If the loss is not bounded,\nyou need a little bit more advanced tools to deal with it. But let's say for now the loss\nis bounded between 0 and 1.",
    "start": "3690550",
    "end": "3696665"
  },
  {
    "text": "For example, if you\nuse classification, your loss is 0 and 1 loss. Loss can only be 0 or 1. So that satisfies this loss\nfor every x and y and theta,",
    "start": "3696665",
    "end": "3707230"
  },
  {
    "text": "let's say. Then, if you apply Hoeffding\ninequality, what you get",
    "start": "3707230",
    "end": "3712840"
  },
  {
    "text": "is that-- so this is a lemma. But actually, it's really\njust the application",
    "start": "3712840",
    "end": "3718350"
  },
  {
    "text": "of Hoeffding inequality. So for any fixed\ntheta, so suppose you--",
    "start": "3718350",
    "end": "3726750"
  },
  {
    "text": "so let me see. ",
    "start": "3726750",
    "end": "3735869"
  },
  {
    "text": "So L hat theta-- this is\nbasically a sum of xi, right, where xi is\nthis L xi yi theta.",
    "start": "3735870",
    "end": "3751220"
  },
  {
    "text": " And so you can compute sigma\nsquared, like the fake variance",
    "start": "3751220",
    "end": "3762385"
  },
  {
    "text": "that we are thinking about. So the sigma squared\ndefined was this-- bi minus ai squared, and from 1 to n.",
    "start": "3762385",
    "end": "3769140"
  },
  {
    "text": "And I guess we have done this\n1 over n squared times n, which is 1 over n. So that means that L hat\ntheta minus L theta, right,",
    "start": "3769140",
    "end": "3779839"
  },
  {
    "text": "is less than O of sigma\nsquare root of log",
    "start": "3779840",
    "end": "3785915"
  },
  {
    "text": "n with high probability, right? And sigma is 1 over n--",
    "start": "3785915",
    "end": "3791940"
  },
  {
    "text": "sorry, sigma\nsquared is 1 over n, so this is O of square root of\nlog n over square root of n.",
    "start": "3791940",
    "end": "3799290"
  },
  {
    "text": "And you can also write\nthis as O tilde of 1 over square root of n. So basically, for\nevery fixed theta,",
    "start": "3799290",
    "end": "3805580"
  },
  {
    "text": "the empirical loss and\nthe population loss only differ by 1\nover square root",
    "start": "3805580",
    "end": "3811335"
  },
  {
    "text": "of n with high probability. So it sounds pretty good, right?",
    "start": "3811335",
    "end": "3817787"
  },
  {
    "text": "So we showed that\nthey are very close. And how close they are? They are close by, well--",
    "start": "3817787",
    "end": "3822850"
  },
  {
    "text": "the difference is 1 over square\nroot of n, which goes to 0, I think, goes to infinity. So it's supposed to\nbe a small number.",
    "start": "3822850",
    "end": "3828340"
  },
  {
    "text": "And there's no other\nthings hidden here. Of course, you have\na log factor in n,",
    "start": "3828340",
    "end": "3833540"
  },
  {
    "text": "but you don't have any\nfactor about, for example, dimensionality. ",
    "start": "3833540",
    "end": "3841840"
  },
  {
    "text": "Any questions?  So there is a small issue--",
    "start": "3841840",
    "end": "3848670"
  },
  {
    "text": "go ahead. [INAUDIBLE] with high\nprobability here is 1 minus 1",
    "start": "3848670",
    "end": "3855780"
  },
  {
    "text": "over n to some positive? Yep, yep. Exactly. So with high probability--",
    "start": "3855780",
    "end": "3861280"
  },
  {
    "text": "so technically, I should\nwrite the probability that this is happening.",
    "start": "3861280",
    "end": "3868329"
  },
  {
    "text": " It's lower than 1\nminus n to the O of 1.",
    "start": "3868330",
    "end": "3874410"
  },
  {
    "start": "3874410",
    "end": "3879970"
  },
  {
    "text": "OK. And this is actually a good\ntime to practice this big O notation. Basically, this is saying\nthat you can replace--",
    "start": "3879970",
    "end": "3889200"
  },
  {
    "text": "actually, here, wait. Let me see. ",
    "start": "3889200",
    "end": "3895099"
  },
  {
    "text": "I think [AUDIO OUT] a big\nO of 1 and omega of 1.",
    "start": "3895100",
    "end": "3902250"
  },
  {
    "start": "3902250",
    "end": "3907280"
  },
  {
    "text": "I think I should use omega of 1. But maybe I say c. I say, there exists a c\nsuch that a constant--",
    "start": "3907280",
    "end": "3918470"
  },
  {
    "text": "there exists a constant other\nthan 0s such that this is true. Maybe-- yeah, you see,\nsometimes, this is confusing.",
    "start": "3918470",
    "end": "3929030"
  },
  {
    "text": " On the fly, I couldn't\nfigure it out. But this is what we mean.",
    "start": "3929030",
    "end": "3934911"
  },
  {
    "text": "Maybe let's say-- It's 1? Maybe let's just say this is 10. I think I think this is\ndefinitely a correct statement,",
    "start": "3934912",
    "end": "3940510"
  },
  {
    "text": "because there is the O here. You can hide\neverything in there. So that's what I mean.",
    "start": "3940510",
    "end": "3947020"
  },
  {
    "text": "OK, cool. ",
    "start": "3947020",
    "end": "3952540"
  },
  {
    "text": "OK, so this is a\ncorrect statement. But there is a\nsmall thing that--",
    "start": "3952540",
    "end": "3959560"
  },
  {
    "text": "there is an important\nthing we should note here. So what do I mean by,\nfor any fixed theta? What does this\nreally mean, right?",
    "start": "3959560",
    "end": "3965710"
  },
  {
    "text": "I have this header here. So this really means that\nyou need to first take theta. And then, you draw\nafter you take theta--",
    "start": "3965710",
    "end": "3972700"
  },
  {
    "text": "you draw iid, xi and yi\nfrom this distribution p",
    "start": "3972700",
    "end": "3980970"
  },
  {
    "text": "so that these are-- well, why do you\nhave to do this? Because you want to make sure\nthat L of xi and yi theta--",
    "start": "3980970",
    "end": "3991650"
  },
  {
    "text": "these are independently\ndistributed-- are independent\nfor different i's.",
    "start": "3991650",
    "end": "3998840"
  },
  {
    "text": "So if you pick theta first, and\nthen you draw the xi's, then indeed, this random variable\nxi, which is equal to a loss,",
    "start": "3998840",
    "end": "4006670"
  },
  {
    "text": "are independent. But this doesn't really mean\nthat you can do this for theta.",
    "start": "4006670",
    "end": "4014410"
  },
  {
    "text": "That depends on xi,\nwhich is actually what I'm going to talk about next. So first of all, you can\napply this for theta.",
    "start": "4014410",
    "end": "4023890"
  },
  {
    "text": "You can apply this for theta\nis equal to theta star. That's a lot. Because theta star is a\nuniversal quantity, right?",
    "start": "4023890",
    "end": "4030920"
  },
  {
    "text": "You know what theta star is. The theta star exists even\nbefore you draw the samples.",
    "start": "4030920",
    "end": "4036420"
  },
  {
    "text": "Why? Because theta star\nis the minimizer of the population risk. The population risk doesn't\ndepend on the samples.",
    "start": "4036420",
    "end": "4041900"
  },
  {
    "text": "It only depends on\nthe distribution. So that's why you can apply this\nwith theta equals theta star.",
    "start": "4041900",
    "end": "4048170"
  },
  {
    "text": "So that's why we got\nthis inequality 1, because got L hat theta\nstar minus L theta star.",
    "start": "4048170",
    "end": "4053704"
  },
  {
    "text": "It's less than O tilde of\n1 over square root of n.",
    "start": "4053705",
    "end": "4059080"
  },
  {
    "text": "So now the question\nis whether you can apply this to theta hat.",
    "start": "4059080",
    "end": "4065720"
  },
  {
    "text": "And the answer is, no,\nyou cannot apply it to it. And it's not only just because\nyou have some subtle kind",
    "start": "4065720",
    "end": "4071440"
  },
  {
    "text": "of mathematical rigorousness. It's really just-- it's\nvery far from correctly",
    "start": "4071440",
    "end": "4076849"
  },
  {
    "text": "applied to theta hat. It's not a small mathematical\nnuances kind of thing. And the reason is that there's\na dependency issue, right?",
    "start": "4076850",
    "end": "4084710"
  },
  {
    "text": "So as I alluded\nbefore a little bit, so the dependency is that you\nfirst have theta star, right?",
    "start": "4084710",
    "end": "4090490"
  },
  {
    "text": "Theta star depends on the\npopulation distribution. And theta star is something\nthat has existed before you",
    "start": "4090490",
    "end": "4097588"
  },
  {
    "text": "begin to draw the sample. And then, you draw the sample. ",
    "start": "4097588",
    "end": "4105049"
  },
  {
    "text": "And then, you get\ntheta hat, right? And then, you can compute, for\nexample, L theta hat or L hat",
    "start": "4105050",
    "end": "4115049"
  },
  {
    "text": "theta hat, these kind of things. But theta hat depends\non the samples. So that means that\nL of xi yi theta hat",
    "start": "4115050",
    "end": "4126469"
  },
  {
    "text": "are not independent\nwith each other. ",
    "start": "4126470",
    "end": "4136212"
  },
  {
    "text": "So you cannot apply\nHoeffding inequality, because they are not\nindependent random variables. ",
    "start": "4136212",
    "end": "4147689"
  },
  {
    "text": "And this is important, because\nif you can really apply this, actually, you always--",
    "start": "4147689",
    "end": "4154420"
  },
  {
    "text": "if you can apply this Hoeffding\ninequality for theta hat, you'll always get 1\nover square root of n. There is no dependency\non anything.",
    "start": "4154420",
    "end": "4160740"
  },
  {
    "text": "Then, machine learning\nwould be much, much easier. We don't have to think\nabout sample complexity.",
    "start": "4160740",
    "end": "4165750"
  },
  {
    "text": "It's always small.  So basically, the\nnext, well, two weeks",
    "start": "4165750",
    "end": "4171990"
  },
  {
    "text": "we are dealing with this, how do\nwe deal with theta hat, right? So the idea to fix this is\ncalled uniform convergence.",
    "start": "4171990",
    "end": "4183219"
  },
  {
    "text": " And the key idea is that\nyou want to apply this--",
    "start": "4183220",
    "end": "4190214"
  },
  {
    "text": " apply Hoeffding\nto any theta that",
    "start": "4190215",
    "end": "4200699"
  },
  {
    "text": "is predetermined before\ndrawing data, right? You can apply this\nto any theta that's",
    "start": "4200700",
    "end": "4208080"
  },
  {
    "text": "predetermined before\ndrawing the data. ",
    "start": "4208080",
    "end": "4214025"
  },
  {
    "text": "I guess this might\nsound a little bit--  by itself, a little bit vague.",
    "start": "4214025",
    "end": "4219280"
  },
  {
    "text": "So what I really mean is\nthat you want to prove that-- so what we know now is\nthat for every theta,",
    "start": "4219280",
    "end": "4230449"
  },
  {
    "text": "probability L hat theta minus-- so for every theta that has\nnothing to do with our samples,",
    "start": "4230450",
    "end": "4238820"
  },
  {
    "text": "you know this is true for some--",
    "start": "4238820",
    "end": "4244403"
  },
  {
    "text": "of course, I didn't specify\nexactly what epsilon delta is, but this is the form of the\ntheorem we are proving right now-- we can prove right now.",
    "start": "4244403",
    "end": "4251020"
  },
  {
    "text": "And we have proved this for-- and you can plug in theta\nis equal to theta star. That's fine.",
    "start": "4251020",
    "end": "4256090"
  },
  {
    "text": "But this is not the same as-- ",
    "start": "4256090",
    "end": "4261990"
  },
  {
    "text": "this is true. ",
    "start": "4261990",
    "end": "4270070"
  },
  {
    "text": "So the second\nstatement is what I'm going to prove in the\nnext one or two weeks.",
    "start": "4270070",
    "end": "4276400"
  },
  {
    "text": "But these two are two\ndifferent statements. The second statement is\nsaying that you first",
    "start": "4276400",
    "end": "4282889"
  },
  {
    "text": "draw the samples,\nand then after we draw the samples for all theta,\nthese two functions are close.",
    "start": "4282890",
    "end": "4288800"
  },
  {
    "text": "Maybe it's useful to\ndraw a figure, right? So there is a function\ncalled L theta, right?",
    "start": "4288800",
    "end": "4300820"
  },
  {
    "text": "And here, this dimension\ntheta in the y dimension is the L theta. And now, let's look at\nwhat's the empirical loss.",
    "start": "4300820",
    "end": "4308420"
  },
  {
    "text": "So the empirical loss--  so I guess maybe let me\ngive example where these two",
    "start": "4308420",
    "end": "4316239"
  },
  {
    "text": "statements are different. So let's think about-- there\nare only two, three cases. So this is a very\npoor example, right?",
    "start": "4316240",
    "end": "4323840"
  },
  {
    "text": "So consider the case where L\nhat theta is this function.",
    "start": "4323840",
    "end": "4330580"
  },
  {
    "text": " So it's the right function\nwith probability 1/3.",
    "start": "4330580",
    "end": "4337989"
  },
  {
    "text": " And it's the orange function\nwith probability 1/3.",
    "start": "4337990",
    "end": "4346579"
  },
  {
    "text": " And maybe let's say\nit's the green--",
    "start": "4346580",
    "end": "4354160"
  },
  {
    "text": "this is a sign, I guess. Didn't have different color-- ",
    "start": "4354160",
    "end": "4359610"
  },
  {
    "text": "green function,\nwith probably 1/3. ",
    "start": "4359610",
    "end": "4366390"
  },
  {
    "text": "And so what you know is\nthat for every theta--",
    "start": "4366390",
    "end": "4372250"
  },
  {
    "text": "so for any fixed theta, if\nyou look at the probability that L hat theta is\ndifferent from L theta--",
    "start": "4372250",
    "end": "4379750"
  },
  {
    "text": "let's say they are\njust different. So what's the chance\nthat they are different? So this chance is\nsomething like 2/3, right?",
    "start": "4379750",
    "end": "4391110"
  },
  {
    "text": "Because if you look at\nany point, any theta-- for some theta, actually,\nthe three functions",
    "start": "4391110",
    "end": "4397545"
  },
  {
    "text": "are always the same, right? They're always the same. But maybe, for example,\nif you pick a point here,",
    "start": "4397545",
    "end": "4406290"
  },
  {
    "text": "if you look at this point,\nthen with probably 1/3, L hat theta is this\nright point, which",
    "start": "4406290",
    "end": "4412620"
  },
  {
    "text": "is different from L theta. And with these other\ntwo possibilities, right, it's probably 2/3.",
    "start": "4412620",
    "end": "4419520"
  },
  {
    "text": "L hat is equal to\nL theta, right? So basically, for every\ntheta, you have some--",
    "start": "4419520",
    "end": "4428053"
  },
  {
    "text": "sorry, I should write this\nis equal to 1/3, right.",
    "start": "4428054",
    "end": "4438080"
  },
  {
    "text": "So basically, for every theta,\nyou have something like this, right? ",
    "start": "4438080",
    "end": "4444659"
  },
  {
    "text": "And on the other\nhand, if you look at some statement\nlike this, if you",
    "start": "4444660",
    "end": "4458969"
  },
  {
    "text": "look at this, for every theta,\nL hat theta is close to L theta. Then, what's this thing?",
    "start": "4458970",
    "end": "4465148"
  },
  {
    "text": "This is saying that\nbasically these two functions are the same globally. And clearly in any of these\nred, yellow, and green cases,",
    "start": "4465148",
    "end": "4473520"
  },
  {
    "text": "this probability is 0.  Because in both of these\nthree random cases,",
    "start": "4473520",
    "end": "4481390"
  },
  {
    "text": "the two functions are not\nalways the same, right? There's always some chance that\nthere are some differences.",
    "start": "4481390",
    "end": "4486980"
  },
  {
    "text": "So that shows that you cannot\neasily switch the probability",
    "start": "4486980",
    "end": "4492580"
  },
  {
    "text": "for all qualifiers. They are just not switchable. I guess you probably have\nseen that also sometimes, some",
    "start": "4492580",
    "end": "4502773"
  },
  {
    "text": "of you probably\nwould expect this, that this is about\nunion bound, right? When you do union\nbound, there is always",
    "start": "4502773",
    "end": "4507790"
  },
  {
    "text": "these kind of things-- whether you can\nswitch the probability with all kind of terms,\nwhich we are going",
    "start": "4507790",
    "end": "4513457"
  },
  {
    "text": "to talk about in a moment.  So basically, this is probably--",
    "start": "4513457",
    "end": "4520690"
  },
  {
    "text": "I hope this is\ndemonstrating that it's kind of more difficult to prove\nL theta hat minus L hat theta--",
    "start": "4520690",
    "end": "4533909"
  },
  {
    "text": "to prove equality 2. So the take home\npoint is that it's",
    "start": "4533910",
    "end": "4541120"
  },
  {
    "text": "more difficult to\nprove equality to 2.",
    "start": "4541120",
    "end": "4547090"
  },
  {
    "text": "What's equality to 2? Equality to 2 was the difference\nbetween L theta hat and L hat",
    "start": "4547090",
    "end": "4553309"
  },
  {
    "text": "theta hat. And the reason is that theta hat\nis a function of the data set. And you lose the independence.",
    "start": "4553310",
    "end": "4560309"
  },
  {
    "text": "And so the goal of many of\nthe rest of the lectures",
    "start": "4560310",
    "end": "4566760"
  },
  {
    "text": "is to show that this\nis indeed bounded using the so-called\nuniform convergence. And by uniform convergence,\nlet me just summarize.",
    "start": "4566760",
    "end": "4573970"
  },
  {
    "text": "I hope you've got some\nintuition here already. We need to prove\nsomething like probability",
    "start": "4573970",
    "end": "4581900"
  },
  {
    "text": "that for all theta, theta\nhat close to L theta, it's less than epsilon--",
    "start": "4581900",
    "end": "4587240"
  },
  {
    "start": "4587240",
    "end": "4592520"
  },
  {
    "text": "is larger than 1 minus delta. So we need to prove\nsomething like this using some techniques.",
    "start": "4592520",
    "end": "4598790"
  },
  {
    "text": "And you will see\nthat you're going to get much looser bounds when\nyou prove something like this.",
    "start": "4598790",
    "end": "4604830"
  },
  {
    "text": "The epsilon delta would be\ndifferent from the epsilon delta that you can get when\nthe [INAUDIBLE] quantifier is",
    "start": "4604830",
    "end": "4610950"
  },
  {
    "text": "outside the probability. So I guess I will show how\nto prove this kind of bound",
    "start": "4610950",
    "end": "4621469"
  },
  {
    "text": "in the next two lectures. But just, I guess--",
    "start": "4621470",
    "end": "4630790"
  },
  {
    "text": "so that this suffices,\nI guess as expected, if I have a claim 2-- ",
    "start": "4630790",
    "end": "4638380"
  },
  {
    "text": "so you know that L\ntheta hat minus L theta",
    "start": "4638380",
    "end": "4643530"
  },
  {
    "text": "star is less than-- I guess, by claim 1, this\nis less than the differences",
    "start": "4643530",
    "end": "4653600"
  },
  {
    "text": "between L theta star minus L\nhat theta star plus L theta",
    "start": "4653600",
    "end": "4660410"
  },
  {
    "text": "hat minus L hat theta hat. And this is less than 2 times\nthe sup over all theta--",
    "start": "4660410",
    "end": "4669370"
  },
  {
    "text": "L theta minus L\nhat theta, right? So if you can show that for\nall theta they are similar,",
    "start": "4669370",
    "end": "4676280"
  },
  {
    "text": "then you have a bound\nfor the excess risk. So maybe just, in some sense,\nif you draw the picture here,",
    "start": "4676280",
    "end": "4685072"
  },
  {
    "text": "so basically, what\nyou want to show is that suppose this is the\npopulation risk L theta. You want to show that\nwith higher probability,",
    "start": "4685072",
    "end": "4693100"
  },
  {
    "text": "your empirical risk is\nsomething like this, which is kind of uniformly\nclose to the population risk.",
    "start": "4693100",
    "end": "4701700"
  },
  {
    "text": "That's kind of the\nintuition we have. And actually, let's see.",
    "start": "4701700",
    "end": "4707500"
  },
  {
    "text": "So yeah. And actually, in the\nsecond half of the lecture,",
    "start": "4707500",
    "end": "4712890"
  },
  {
    "text": "like after week\nfive or week six, we're also going to talk\nabout that it's not actually--",
    "start": "4712890",
    "end": "4719610"
  },
  {
    "text": "this picture is actually not\nentirely accurate in the sense that indeed, in many\ncases, the empirical risk",
    "start": "4719610",
    "end": "4727800"
  },
  {
    "text": "is kind of bounded\nby some epsilon, right, within the epsilon,\nwithin the population risk.",
    "start": "4727800",
    "end": "4736000"
  },
  {
    "text": "But also, it doesn't look\nlike it's kind of fluctuating. So what really happens\nis something like,",
    "start": "4736000",
    "end": "4741920"
  },
  {
    "text": "maybe you have a function\npopulation risk like this. This is population.",
    "start": "4741920",
    "end": "4747050"
  },
  {
    "text": "And the empirical risk\nis something first of all, close to\nthe population risk,",
    "start": "4747050",
    "end": "4752890"
  },
  {
    "text": "but also in terms of the\nshape and the curvature, it's also close. So it wouldn't be\nthat fluctuating.",
    "start": "4752890",
    "end": "4758590"
  },
  {
    "text": "It would be something\nlike maybe this. ",
    "start": "4758590",
    "end": "4764560"
  },
  {
    "text": "So not only in terms of\nthe value they are close, but also in terms of some\nother properties-- maybe",
    "start": "4764560",
    "end": "4769660"
  },
  {
    "text": "the curvature, the shape--\nthey are also somewhat close. And this is useful\nfor certain kind",
    "start": "4769660",
    "end": "4775670"
  },
  {
    "text": "of cases when you especially\ncare about optimization, right? So for example, if the empirical\nrisk is so fluctuating,",
    "start": "4775670",
    "end": "4784910"
  },
  {
    "text": "then it becomes\nharder to optimize and we care about\noptimization sometimes, you want to show that the\nempirical risk can also",
    "start": "4784910",
    "end": "4794889"
  },
  {
    "text": "have nice properties for\nyour computational purposes. ",
    "start": "4794890",
    "end": "4801522"
  },
  {
    "text": "OK, I guess that's a\nperfect stopping time.",
    "start": "4801522",
    "end": "4806573"
  },
  {
    "text": "OK, thanks. ",
    "start": "4806574",
    "end": "4814000"
  }
]