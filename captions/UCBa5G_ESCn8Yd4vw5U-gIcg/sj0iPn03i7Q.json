[
  {
    "text": "Okay. Welcome back. Let's continue. Uh, so this is our sixth lecture.",
    "start": "5030",
    "end": "11430"
  },
  {
    "text": "The topics for today are the exponential family. Um, it's a- it's a, uh, a family of probability distributions,",
    "start": "11430",
    "end": "19824"
  },
  {
    "text": "and the generalized, uh, linear models, or GLMs. The two topics are kind of tightly coupled,",
    "start": "19825",
    "end": "26440"
  },
  {
    "text": "so we cover them, uh, in the same lecture. And before we get started, a quick recap to kind of,",
    "start": "26440",
    "end": "33030"
  },
  {
    "text": "um, see the motivation for exponential families and GLMs. So we've seen two- two models so far, uh,",
    "start": "33030",
    "end": "41550"
  },
  {
    "text": "regression model and the classification model, and they were linear regression and logistic regression.",
    "start": "41550",
    "end": "47290"
  },
  {
    "text": "Right? And for both the linear regression and the logistic regression, we saw their probabilistic interpretations.",
    "start": "47290",
    "end": "54110"
  },
  {
    "text": "And for the linear regression, the, um, the assumption that we made was y given x is sampled from",
    "start": "54110",
    "end": "62770"
  },
  {
    "text": "a normal distribution with mean Theta transpose x and some variance Sigma squared.",
    "start": "62770",
    "end": "69060"
  },
  {
    "text": "Um, it- it- it did not matter what Sigma squared was and, um, and- and- and that's fine.",
    "start": "69060",
    "end": "76079"
  },
  {
    "text": "In the case of classification, y given x was sampled from a Bernoulli distribution whose parameter was 1 over 1 plus,",
    "start": "76080",
    "end": "84700"
  },
  {
    "text": "um, e^minus Theta transpose x, right? And this was also, uh, we call this g of z equals 1 over 1 plus e^minus z,",
    "start": "84700",
    "end": "96585"
  },
  {
    "text": "and g of Theta transpose x becomes the,",
    "start": "96585",
    "end": "102270"
  },
  {
    "text": "um, becomes the parameter of the Bernoulli distribution.",
    "start": "102270",
    "end": "107380"
  },
  {
    "text": "Right? And then we had a smaller side, uh, mostly, uh, a digression in terms of what we're going to cover today.",
    "start": "107380",
    "end": "115340"
  },
  {
    "text": "But, you know, this is going to be useful next week. Um, think of functions as points in an infinite-dimensional functional space.",
    "start": "115340",
    "end": "124260"
  },
  {
    "text": "And you kind of get a one-to-one mapping between the domain of the function and the axes of the, uh, functional space.",
    "start": "124260",
    "end": "132440"
  },
  {
    "text": "Uh, anyway, so this is not relevant for today, but, you know, just kind of keep this in the back of your mind.",
    "start": "132440",
    "end": "137900"
  },
  {
    "text": "So we're gonna continue along the lines of- of, um, the probabilistic interpretations, and we're gonna cover generalized linear models.",
    "start": "137900",
    "end": "147629"
  },
  {
    "text": "So off the bat, we- we see a few things that are common between these two.",
    "start": "147630",
    "end": "153125"
  },
  {
    "text": "All right, first of all, we are modeling y given x in both the cases, right?",
    "start": "153125",
    "end": "158300"
  },
  {
    "text": "The difference is that in one case, y is real valued, and in the other case y is binary valued, 0 and 1.",
    "start": "158300",
    "end": "166390"
  },
  {
    "text": "And the- the datatype of y, whether it is, you know,",
    "start": "166390",
    "end": "171785"
  },
  {
    "text": "a real value or- or, um, discrete binary value, informed our choice of the distribution that we are gonna- that we used, right?",
    "start": "171785",
    "end": "180920"
  },
  {
    "text": "Uh, it doesn't make sense to define a Bernoulli- a Bernoulli over real value. Similarly, it doesn't make sense to define a normal distribution on, uh, discrete value.",
    "start": "180920",
    "end": "190195"
  },
  {
    "text": "So the choice of- the- the data type of the y-variable informed the choice of the distribution that we used.",
    "start": "190195",
    "end": "197360"
  },
  {
    "text": "Um, the other thing that we see in both- uh, both of these is the occurrence of Theta transpose x. right?",
    "start": "197360",
    "end": "208135"
  },
  {
    "text": "We have a Theta transpose x here, and we have a Theta transpose x here.",
    "start": "208135",
    "end": "213945"
  },
  {
    "text": "Right? And, um, these two seem to be,",
    "start": "213945",
    "end": "219300"
  },
  {
    "text": "you know, somewhat- somewhat superficial similarities. But what we'll see, uh, what we'll see, uh,",
    "start": "219300",
    "end": "225069"
  },
  {
    "text": "today is that there's actually a much much deeper and richer theory that unifies these too,",
    "start": "225070",
    "end": "231775"
  },
  {
    "text": "and a lot more different kinds of models under common umbrella. Right? So, and- and that's called the,",
    "start": "231775",
    "end": "239265"
  },
  {
    "text": "uh, um, that's called the GLM or the generalized linear models. We call it linear because, you know, Theta transpose x is linear, right?",
    "start": "239265",
    "end": "246375"
  },
  {
    "text": "And, uh, we call it generalized because we generalize not just to- not two types of deep types of y but,",
    "start": "246375",
    "end": "254814"
  },
  {
    "text": "you know, for- for more general kinds of, uh, y in general. Okay. So let's get started with exponential families.",
    "start": "254815",
    "end": "263060"
  },
  {
    "text": "[NOISE] I'll use a different pen.",
    "start": "263060",
    "end": "278020"
  },
  {
    "text": "Exponential family. So the exponential family can be written in the form.",
    "start": "283040",
    "end": "290759"
  },
  {
    "text": "So this is the probability density, PDF, or PMF.",
    "start": "291320",
    "end": "300415"
  },
  {
    "text": "So an exponential family distribution is a probability distribution whose, uh,",
    "start": "300415",
    "end": "306155"
  },
  {
    "text": "density has the form p of y parameterized by Eta equals b of",
    "start": "306155",
    "end": "314270"
  },
  {
    "text": "y Eta transpose T of y minus a of Eta.",
    "start": "314270",
    "end": "330210"
  },
  {
    "text": "Right. This- this- this is a cryptic expression, let's add some color to this.",
    "start": "331430",
    "end": "337140"
  },
  {
    "text": "So the data that we are- on which probability is defined,",
    "start": "337140",
    "end": "348615"
  },
  {
    "text": "let's call it y, y, y, right?",
    "start": "348615",
    "end": "355080"
  },
  {
    "text": "And the parameter of such a distribution,",
    "start": "355080",
    "end": "362169"
  },
  {
    "text": "call it, Eta, Eta, Eta, right?",
    "start": "366500",
    "end": "377295"
  },
  {
    "text": "So this is the Greek letter Eta, and if you're using LaTeX, uh, you use E-T-A,",
    "start": "377295",
    "end": "385845"
  },
  {
    "text": "Eta to, uh, get that symbol. Um, it kind of looks like n, but it's just normal Eta.",
    "start": "385845",
    "end": "392550"
  },
  {
    "text": "So the parameter of the distribution is Eta, and the support of the distribution, that is y.",
    "start": "392550",
    "end": "400685"
  },
  {
    "text": "Uh, we call it y, uh, and not x, uh, because it's a hint, we're trying to model the output",
    "start": "400685",
    "end": "407690"
  },
  {
    "text": "of our models with exponential families. So, um, so the probability density of",
    "start": "407690",
    "end": "414170"
  },
  {
    "text": "y parameterized by Eta is given by this expression, right? And there are kind of three functions over here.",
    "start": "414170",
    "end": "421069"
  },
  {
    "text": "So we have, uh, b of y,",
    "start": "421070",
    "end": "426100"
  },
  {
    "text": "and this is purely a function of y and it's generally",
    "start": "426100",
    "end": "431480"
  },
  {
    "text": "called the base measure, right?",
    "start": "431480",
    "end": "437460"
  },
  {
    "text": "And we have t of y, this is called the sufficient statistic.",
    "start": "437460",
    "end": "445000"
  },
  {
    "text": "And we have a of Eta. A of Eta is called the log partition function.",
    "start": "450350",
    "end": "457840"
  },
  {
    "text": "Right? So, um, a few observations. The base measure or the function b is purely a function of y only,",
    "start": "464660",
    "end": "476150"
  },
  {
    "text": "and it has no Eta terms in it. right? And similarly, the log partition function is purely a function of Eta,",
    "start": "476150",
    "end": "485044"
  },
  {
    "text": "and it has no y in it. Right? And similarly, T of y is- is,",
    "start": "485045",
    "end": "491360"
  },
  {
    "text": "um, is again a function of- of- of y alone, and for pretty much, you know,",
    "start": "491360",
    "end": "498330"
  },
  {
    "text": "uh, most of this course and, you know, most commonly T of y will just be equal to",
    "start": "498330",
    "end": "504030"
  },
  {
    "text": "y. I think of it as the, uh, identity function. But, uh, for- for most of the cases,",
    "start": "504030",
    "end": "510569"
  },
  {
    "text": "T of y will just be y. And why are these called,",
    "start": "510570",
    "end": "516080"
  },
  {
    "text": "um, uh, the- these, uh, particular names? So one way to, uh,",
    "start": "516080",
    "end": "523065"
  },
  {
    "text": "think of it is the way an exponential family can be constructed is by starting with some base measure b of y, right?",
    "start": "523065",
    "end": "534185"
  },
  {
    "text": "Um, assume it is some probability, um, um, density,",
    "start": "534185",
    "end": "539499"
  },
  {
    "text": "and come up with a new- new distribution whose density is-",
    "start": "539499",
    "end": "549960"
  },
  {
    "text": "Right? So what's happening here? We have some base measure Eta, and we- we can define a [NOISE] new distribution whose density is proportional",
    "start": "570830",
    "end": "581459"
  },
  {
    "text": "to the density of the base measure times the exponent of the parameter times- times the data.",
    "start": "581459",
    "end": "588450"
  },
  {
    "text": "To make this simple, let's- let's now consider, uh, um, a scalar case and where the base measure is.",
    "start": "588450",
    "end": "597245"
  },
  {
    "text": "And now, because it's a probability density measure, if we normalize this,",
    "start": "597245",
    "end": "603154"
  },
  {
    "text": "we have to normalize this. [NOISE] p of y Eta is equal to b of y times e of minus Eta to y,",
    "start": "603155",
    "end": "618165"
  },
  {
    "text": "over the integral of this with respect to y, okay? We integrate out y.",
    "start": "618165",
    "end": "623970"
  },
  {
    "text": "[NOISE] b of y times e of minus Eta of y d_y, okay?",
    "start": "623970",
    "end": "635490"
  },
  {
    "text": "Does it make sense? So we- we are just defining a new- new probability di- um, um, distribution whose density is- is defined by the b- you know,",
    "start": "635490",
    "end": "646410"
  },
  {
    "text": "some base probability measure y times the exponent of, you know, a parameter times,",
    "start": "646410",
    "end": "652200"
  },
  {
    "text": "uh, uh, e of y, right? And then we just renormalize it. And this is also, um,",
    "start": "652200",
    "end": "657464"
  },
  {
    "text": "in- in- in some literature, this is called, uh, exponential tilting, [NOISE] right?",
    "start": "657465",
    "end": "667709"
  },
  {
    "text": "And now, uh, what is this? This [NOISE] is b of y times e of minus c- Eta transpose y. Um,",
    "start": "667710",
    "end": "678435"
  },
  {
    "text": "and this is just the expectation of- [NOISE] expectation",
    "start": "678435",
    "end": "684300"
  },
  {
    "text": "of e to the minus Eta transpose y, right?",
    "start": "684300",
    "end": "690019"
  },
  {
    "text": "[NOISE] Does that makes sense? This is just the definition. So this is the- the- the- the, uh, density,",
    "start": "690020",
    "end": "695910"
  },
  {
    "text": "and this is sum function, and you're integrating out so this is the expectation. And this also happens to be called the? Anybody?",
    "start": "695910",
    "end": "702420"
  },
  {
    "text": "Log partition function. It's the log partition function, uh, but just this form alone is also called the moment-generating function, okay?",
    "start": "702420",
    "end": "711214"
  },
  {
    "text": "In- in- um- but that's not- um, um, that's not relevant to- to, uh,",
    "start": "711215",
    "end": "717335"
  },
  {
    "text": "our- our- um, our study here. So you can just call the sum function of, you know,",
    "start": "717335",
    "end": "724665"
  },
  {
    "text": "let's call it capital A of Eta because you're integrating out y, the only variable here is Eta, okay?",
    "start": "724665",
    "end": "732550"
  },
  {
    "text": "And this comes in the form, b of y times e of minus Eta transpose y.",
    "start": "732550",
    "end": "741850"
  },
  {
    "text": "So, uh, I flip the sign here, this- right?",
    "start": "745370",
    "end": "752220"
  },
  {
    "text": "Eta transpose y minus [NOISE] log A of Eta, [NOISE] right? Yeah.",
    "start": "752220",
    "end": "762510"
  },
  {
    "text": "[inaudible]",
    "start": "762510",
    "end": "768120"
  },
  {
    "text": "Yep. Uh, so the- the question is, should, uh, you know, uh, should b of y, um, satisfy the probabi- the- the properties of a probability distribution function?",
    "start": "768120",
    "end": "775995"
  },
  {
    "text": "In this form, it need not because, you know, um, there could be some common constant that get canceled between the two.",
    "start": "775995",
    "end": "784079"
  },
  {
    "text": "Um, you- so here, you can actually think of it as some b prime, b prime, b prime,",
    "start": "784080",
    "end": "790170"
  },
  {
    "text": "you know, and, you know, b prime. And after you common- cancel the common consonants, you end up with a b of y.",
    "start": "790170",
    "end": "795855"
  },
  {
    "text": "But with the- the main idea you wanna think of here is to get an exponential family distribution,",
    "start": "795855",
    "end": "801135"
  },
  {
    "text": "you start with some base measure b, do an element-wise multiplication at each term with the e- e of- you know,",
    "start": "801135",
    "end": "808620"
  },
  {
    "text": "you introduce a new parameter and, uh, multiply it by, um, uh, uh, exponent of that parameter times y,",
    "start": "808620",
    "end": "816750"
  },
  {
    "text": "and then just normalize it. And once you normalize it, what you get is the, kind of, the- uh, it's called the exponentially tilted version of b of y and that",
    "start": "816750",
    "end": "825810"
  },
  {
    "text": "will be an ex- a member of the exponential family, right? That's- that's one way to think of how exponential family distributions are",
    "start": "825810",
    "end": "833610"
  },
  {
    "text": "contrast- constructed and kind of gives you a motivation of why it's called the base measure. So this- this is also called like the pre-tilting measure.",
    "start": "833610",
    "end": "840150"
  },
  {
    "text": "And once you perform this process called tilting, you get a member- uh, uh,",
    "start": "840150",
    "end": "845160"
  },
  {
    "text": "a distribution in the exponential family, right? All right. So that's- that's just a side- that's just some motivation for why the names, um,",
    "start": "845160",
    "end": "856145"
  },
  {
    "text": "you- you call this the base measure, and up- the partition function, um, is another common name for a normalizing constant, right?",
    "start": "856145",
    "end": "865470"
  },
  {
    "text": "Um, I think that term comes from statistical physics where you- you use an-",
    "start": "865470",
    "end": "870725"
  },
  {
    "text": "a normalizing constants are called partition functions. And because this is a normalizing, uh,",
    "start": "870725",
    "end": "876290"
  },
  {
    "text": "constant, and when you take it up here, you know, you- you take, uh, uh, because the exponent,",
    "start": "876290",
    "end": "881970"
  },
  {
    "text": "this becomes a log of a normalizing constant, and this is called a of Eta.",
    "start": "881970",
    "end": "887730"
  },
  {
    "text": "And therefore, it's called the log partition function, right? That's- that's- that's just some intuitions for, you know,",
    "start": "887730",
    "end": "894930"
  },
  {
    "text": "why the names- base measure and log partition function, uh, why those names, uh, uh, came into picture.",
    "start": "894930",
    "end": "900810"
  },
  {
    "text": "But, you know, um, everything inside this box is- is not really, uh, relevant for,",
    "start": "900810",
    "end": "906450"
  },
  {
    "text": "uh, our study as such. All right. So- and- and- and the claim is that a lot of",
    "start": "906450",
    "end": "913890"
  },
  {
    "text": "probability distributions that we encounter, for example, the Bernoulli distribution or the, uh, uh, Gaussian distribution,",
    "start": "913890",
    "end": "921450"
  },
  {
    "text": "the normal distribution, uh, and many more, they are all part of the exponential family.",
    "start": "921450",
    "end": "926835"
  },
  {
    "text": "It means you can express these distributions in this form, right?",
    "start": "926835",
    "end": "933765"
  },
  {
    "text": "And any distribution that can be represented in this form belongs to the exponential family.",
    "start": "933765",
    "end": "939195"
  },
  {
    "text": "The only constraint is that- is that, the choices of b, T,",
    "start": "939195",
    "end": "945990"
  },
  {
    "text": "and a should be such that the- this whole term is always non-negative and integrates to 1 when you integrate out y.",
    "start": "945990",
    "end": "955050"
  },
  {
    "text": "Subject to those constraints, for any choice of b, T, and a, you have a distribution in the exponential family, right?",
    "start": "955050",
    "end": "965654"
  },
  {
    "text": "Now, any- any questions before we see how the, um, uh, Gaussian and- and Bernoulli belong to this family.",
    "start": "965655",
    "end": "972735"
  },
  {
    "text": "Any- any questions with- with this so far? Yes, question? [NOISE] Go ahead.",
    "start": "972735",
    "end": "979035"
  },
  {
    "text": "Just a quick question. What is [NOISE] that symbol  at the bottom, something a dot-",
    "start": "979035",
    "end": "986250"
  },
  {
    "text": "This one? Yeah. Oh, uh, just a capital A. I- I just called it some, you know, capital A of Eta and used log of capital A.",
    "start": "986250",
    "end": "991920"
  },
  {
    "text": "[NOISE] Next question? So why are we dividing by the expected [inaudible]",
    "start": "991920",
    "end": "1000230"
  },
  {
    "text": "Oh, so the question is why are we dividing by the, uh, um, the expectation? So we divide it by the integral of- of the numerator with respect to y,",
    "start": "1000230",
    "end": "1008975"
  },
  {
    "text": "which will make it normalize to 1. So- so, uh, think of it as, um,",
    "start": "1008975",
    "end": "1016264"
  },
  {
    "text": "so you have uh, um, uh, some, uh, uh, a set of numbers 1, 5, 7,",
    "start": "1016265",
    "end": "1022010"
  },
  {
    "text": "and you want to maintain their ratios but still make them add up to 1. So you divide it by 1 plus 5 plus 7,",
    "start": "1022010",
    "end": "1029285"
  },
  {
    "text": "1 plus 5 plus 7, 1 plus 5 plus 7, right?",
    "start": "1029285",
    "end": "1035089"
  },
  {
    "text": "And this makes them normalized, uh, uh, into a probability, uh, distribution, they sum up to 1 now.",
    "start": "1035090",
    "end": "1041299"
  },
  {
    "text": "And- and here, we are doing the same thing for- in- in- in a continuous scale, right?",
    "start": "1041300",
    "end": "1047660"
  },
  {
    "text": "So integrate out the numerator, and- and divide it by the- that integral.",
    "start": "1047660",
    "end": "1053510"
  },
  {
    "text": "So this- this is called, uh, um, normalization. [NOISE]",
    "start": "1053510",
    "end": "1059390"
  },
  {
    "text": "What was [inaudible]",
    "start": "1059390",
    "end": "1065420"
  },
  {
    "text": "So you think of 1 plus 5 plus 7 as the- the- the different values- this evaluates to- for different values of y, right?",
    "start": "1065420",
    "end": "1076940"
  },
  {
    "text": "[inaudible]",
    "start": "1076940",
    "end": "1083690"
  },
  {
    "text": "All right. So um, uh, maybe to- to, uh, answer this, uh, uh, uh, clearer. So let's see what happens here.",
    "start": "1083690",
    "end": "1091140"
  },
  {
    "text": "Now, uh, we have some function b of- call it b prime of y [NOISE] times e^Eta times y.",
    "start": "1092200",
    "end": "1103445"
  },
  {
    "text": "Now, let's assume this p of y is this.",
    "start": "1103445",
    "end": "1108485"
  },
  {
    "text": "And let's assume the integral of p of y equal to integral",
    "start": "1108485",
    "end": "1113915"
  },
  {
    "text": "of b prime of y times e of the Eta times y is equal to some k,",
    "start": "1113915",
    "end": "1120575"
  },
  {
    "text": "let's say equal to 100, right? But now, we want- we want this to be",
    "start": "1120575",
    "end": "1127340"
  },
  {
    "text": "a valid probability distribution which means when you integrate out y, we want it to be equal to 1, right?",
    "start": "1127340",
    "end": "1134345"
  },
  {
    "text": "So what- what you do is you wanna to divide it by 100, so that when you integrate the whole thing, you get 1, right?",
    "start": "1134345",
    "end": "1141890"
  },
  {
    "text": "And- and this 100 comes because that's what you got by integrating it out in the first place, right?",
    "start": "1141890",
    "end": "1150200"
  },
  {
    "text": "So- so the, um- so the integral of b prime of y times exponent of, um,",
    "start": "1150200",
    "end": "1160179"
  },
  {
    "text": "uh, Eta times y over integral of- [NOISE] right?",
    "start": "1160180",
    "end": "1172980"
  },
  {
    "text": "Now, this term, the- the- the entire denominator is just a",
    "start": "1177770",
    "end": "1183330"
  },
  {
    "text": "constant and this will be equal to 1 over integral of b prime of y,",
    "start": "1183330",
    "end": "1189375"
  },
  {
    "text": "x of Eta y times the numerator's integral of Eta y,",
    "start": "1189375",
    "end": "1199620"
  },
  {
    "text": "and you just cancel them and equal to 1, right? That- that's- you're just- you're just, uh,",
    "start": "1199620",
    "end": "1205230"
  },
  {
    "text": "normalizing it such that the integral is always 1. That makes sense? Yes, question.",
    "start": "1205230",
    "end": "1212360"
  },
  {
    "text": "So why did you pass the exponential function here [inaudible]",
    "start": "1212360",
    "end": "1221460"
  },
  {
    "text": "Yeah. So this is just- this is- this is the process of what you call as an exponential tilting. And, uh, there are- there are, uh,",
    "start": "1221460",
    "end": "1228465"
  },
  {
    "text": "good reasons why you wanna tilt something using the exponent function. There are, uh, there's a lot of theory of why",
    "start": "1228465",
    "end": "1234270"
  },
  {
    "text": "the exponent is- is the right choice to do, right? It's- it's, you know, um, think of that as beyond the scope of this class.",
    "start": "1234270",
    "end": "1239820"
  },
  {
    "text": "Yeah. Yes, question. [inaudible]",
    "start": "1239820",
    "end": "1248970"
  },
  {
    "text": "Oh, so here, you know, uh, just to give the intuition, we are considering Eta to be a- a scalar in this case.",
    "start": "1248970",
    "end": "1255450"
  },
  {
    "text": "So yeah, if this was a vector then sure, you know, think of this as. And then the integral would be, you know,",
    "start": "1255450",
    "end": "1261735"
  },
  {
    "text": "would be integrating over y_1, dy_2, etc. Yes, question.",
    "start": "1261735",
    "end": "1267270"
  },
  {
    "text": "[inaudible]",
    "start": "1267270",
    "end": "1273990"
  },
  {
    "text": "Very first line. So, uh, so this is- this is the assumption we are making, uh, you know,",
    "start": "1273990",
    "end": "1281070"
  },
  {
    "text": "where taking a base measure b prime of y, we wanna construct a new measure such that the new measure is",
    "start": "1281070",
    "end": "1288000"
  },
  {
    "text": "proportional to b prime of y times exponent of some parameter times y. [inaudible]",
    "start": "1288000",
    "end": "1311700"
  },
  {
    "text": "So the- the intuition is that you- you're just defining a new distribution which is defined like this, right?",
    "start": "1311700",
    "end": "1318480"
  },
  {
    "text": "And this is called, you know, the tilted distribution, the exponentially tilted distribution.",
    "start": "1318480",
    "end": "1323529"
  },
  {
    "text": "Okay. But the- the- the main point here is that, you know, we went through this just to get an- get a flavor",
    "start": "1323530",
    "end": "1330900"
  },
  {
    "text": "of why the names came into picture, right? And- and we can- we can go into details after the class,",
    "start": "1330900",
    "end": "1336330"
  },
  {
    "text": "you know, come up to the- come to the stage, we can- we can go into the details. But the main point is that if you can decompose a probability density into this form,",
    "start": "1336330",
    "end": "1346260"
  },
  {
    "text": "into these components, then, you know, uh, that distribution belongs to the exponential family.",
    "start": "1346260",
    "end": "1352230"
  },
  {
    "text": "Right? Now, let's look at a few distributions which- which",
    "start": "1352230",
    "end": "1358140"
  },
  {
    "text": "actually follow this pattern.",
    "start": "1358140",
    "end": "1364660"
  },
  {
    "text": "So let's start with the, uh, Bernoulli distribution.",
    "start": "1365990",
    "end": "1370510"
  },
  {
    "text": "The Bernoulli distribution p of y parameterized by Phi is written",
    "start": "1374660",
    "end": "1381420"
  },
  {
    "text": "as Phi to the y times 1 minus Phi to the 1 minus y, right?",
    "start": "1381420",
    "end": "1389355"
  },
  {
    "text": "This is the, uh, Bernoulli distribution. And the claim is that this- this- this form of- of",
    "start": "1389355",
    "end": "1397710"
  },
  {
    "text": "the Bernoulli distribution can be rewritten in the exponential family form, right?",
    "start": "1397710",
    "end": "1405825"
  },
  {
    "text": "How do we do that? So, um, the first thing we can think of is we can take the exponent of",
    "start": "1405825",
    "end": "1416385"
  },
  {
    "text": "the log of Phi",
    "start": "1416385",
    "end": "1424440"
  },
  {
    "text": "to the y 1 minus Phi to the 1 minus y, right?",
    "start": "1424440",
    "end": "1430575"
  },
  {
    "text": "Just took the exponent and the log that, uh, and the reason why you can take the log is because this is positive and this is positive.",
    "start": "1430575",
    "end": "1437460"
  },
  {
    "text": "So you can take the log and then, you know, take the exponent again, right? And now we can rewrite this as exponent.",
    "start": "1437460",
    "end": "1446250"
  },
  {
    "text": "So this will be y log Phi plus 1",
    "start": "1446900",
    "end": "1453840"
  },
  {
    "text": "minus y log 1 minus Phi, right?",
    "start": "1453840",
    "end": "1461265"
  },
  {
    "text": "And this rearranging terms you get exponent.",
    "start": "1461265",
    "end": "1467440"
  },
  {
    "text": "So I'm gonna collect terms that have y in them to the left and to the right are terms that do not have y.",
    "start": "1469730",
    "end": "1478424"
  },
  {
    "text": "So we will get, um, log Phi over 1 minus Phi times",
    "start": "1478425",
    "end": "1490409"
  },
  {
    "text": "y plus log 1 minus Phi, right?",
    "start": "1490410",
    "end": "1500355"
  },
  {
    "text": "So all we have done here is pure algebraic manipulation, right?",
    "start": "1500355",
    "end": "1506220"
  },
  {
    "text": "We started with this form and brought it to this form. And now we can see, um,",
    "start": "1506220",
    "end": "1511380"
  },
  {
    "text": "you know, start doing a pattern matching, right? So the, uh, exponential family says p of y given Eta is equal to b of y times",
    "start": "1511380",
    "end": "1522675"
  },
  {
    "text": "exponent [NOISE] Eta transpose y",
    "start": "1522675",
    "end": "1528990"
  },
  {
    "text": "minus a of Eta, right?",
    "start": "1528990",
    "end": "1534600"
  },
  {
    "text": "Now, we want to pattern match these two. So, uh, first thing we notice is that,",
    "start": "1534600",
    "end": "1540149"
  },
  {
    "text": "you know, this is Eta. So Eta equals log",
    "start": "1540150",
    "end": "1548625"
  },
  {
    "text": "Phi over 1 minus Phi, right?",
    "start": "1548625",
    "end": "1555615"
  },
  {
    "text": "And this implies if we invert this,",
    "start": "1555615",
    "end": "1563025"
  },
  {
    "text": "we get Phi equals 1 over 1 plus e to the minus Eta, right?",
    "start": "1563025",
    "end": "1571710"
  },
  {
    "text": "And surprisingly, and- and not coincidentally, this is the logistic function, okay?",
    "start": "1571710",
    "end": "1579160"
  },
  {
    "text": "And, um, moving on, so, uh,",
    "start": "1581900",
    "end": "1587190"
  },
  {
    "text": "um, so we have Eta equals this, and Phi equals this, and T of y equals,",
    "start": "1587190",
    "end": "1596530"
  },
  {
    "text": "T of y is just y in this case, right?",
    "start": "1598790",
    "end": "1604140"
  },
  {
    "text": "So this should be, sorry, T of y. T of y is just y, okay?",
    "start": "1604140",
    "end": "1609465"
  },
  {
    "text": "And a of Eta, so a of Eta is equal to minus because a has a-",
    "start": "1609465",
    "end": "1617970"
  },
  {
    "text": "a negative sign minus log 1 minus Phi.",
    "start": "1617970",
    "end": "1623669"
  },
  {
    "text": "And if you plug in Phi from, uh, this expression, a of Eta will end up being log of 1 plus e to the Eta.",
    "start": "1623670",
    "end": "1635200"
  },
  {
    "text": "And b of y is just 1, right?",
    "start": "1638660",
    "end": "1649770"
  },
  {
    "text": "So we started with the Bernoulli distribution and did a whole bunch of just pure algebraic manipulation.",
    "start": "1649770",
    "end": "1657225"
  },
  {
    "text": "You know, nothing more than algebraic manipulation. We didn't use any kind of logical deductions or- or anything- anything like that.",
    "start": "1657225",
    "end": "1664215"
  },
  {
    "text": "And just kind of massaged it into this form. And did a pattern match to extract the corresponding b function,",
    "start": "1664215",
    "end": "1673559"
  },
  {
    "text": "sufficient statistic, log partition function, and- and, um, and Eta.",
    "start": "1673560",
    "end": "1680745"
  },
  {
    "text": "And Eta over here, uh, forgot to mention. Eta is called the natural parameter.",
    "start": "1680745",
    "end": "1687690"
  },
  {
    "text": "[NOISE] Right.",
    "start": "1687690",
    "end": "1693429"
  },
  {
    "text": "So yeah, so start with the, the, uh, uh, density function,",
    "start": "1703100",
    "end": "1709725"
  },
  {
    "text": "massage it into this form and using pattern matching, just extract out the relevant, relevant, uh,",
    "start": "1709725",
    "end": "1716310"
  },
  {
    "text": "um, uh, terms of the exponential family. And so we have shown that",
    "start": "1716310",
    "end": "1721485"
  },
  {
    "text": "the Bernoulli distribution belongs to the exponential family, right? That's what we, that's what we showed that Bernoulli,",
    "start": "1721485",
    "end": "1727950"
  },
  {
    "text": "um, um, uh, distribution belongs to the exponential family. Yes, question. [inaudible]",
    "start": "1727950",
    "end": "1748230"
  },
  {
    "text": "So the question is, how does this Phi relate to the G function that we saw in,",
    "start": "1748230",
    "end": "1753375"
  },
  {
    "text": "in logistic, uh, um, regression, you know, last class, right? Um, hold on a few more minutes,",
    "start": "1753375",
    "end": "1759525"
  },
  {
    "text": "you know, the, the, the, uh, uh, relation will become- become clear, right? Now, similarly, let's repeat this exercise for the Gaussian distribution.",
    "start": "1759525",
    "end": "1768720"
  },
  {
    "text": "[NOISE] So the Gaussian distribution",
    "start": "1768720",
    "end": "1778845"
  },
  {
    "text": "has the form p of y given Mu, Sigma square.",
    "start": "1778845",
    "end": "1785625"
  },
  {
    "text": "Now we are just considering the univariate Gaussian, not the multivariate.",
    "start": "1785625",
    "end": "1790905"
  },
  {
    "text": "And this will be 1 over square root of 2 Pi times Sigma exponent",
    "start": "1790905",
    "end": "1803610"
  },
  {
    "text": "of minus 1/2 y minus Mu square by Sigma square, right?",
    "start": "1803610",
    "end": "1813090"
  },
  {
    "text": "This is the, the, uh, uh, standard Gaussian distribution. In our case, we will be assuming a constant variance.",
    "start": "1813090",
    "end": "1822300"
  },
  {
    "text": "And for the sake of derivation, we are just going to assume Sigma square equals 1.",
    "start": "1822300",
    "end": "1828600"
  },
  {
    "text": "You can leave it Sigma square as some constant, uh, Sigma squared itself. And we will get the same results, but,",
    "start": "1828600",
    "end": "1835590"
  },
  {
    "text": "you know, the derivations will look a little more complex. Yes, question. Can you look at that comparision to get the different factors, how come the transpose on the Eta disappears?",
    "start": "1835590",
    "end": "1848400"
  },
  {
    "text": "How come the transpose on the Eta disappeared? Good question. Thank you. So, uh, in this case, uh, Eta is just a scalar.",
    "start": "1848400",
    "end": "1855450"
  },
  {
    "text": "So, uh, yeah, when you have two scalars, transposing them will- is, is, is, uh, is the same as just leaving it as is.",
    "start": "1855450",
    "end": "1862260"
  },
  {
    "text": "Yeah. Right. So, uh, here we're going to assume Sigma squared equals 1,",
    "start": "1862260",
    "end": "1870554"
  },
  {
    "text": "which gives us a simplified version of the Gaussian. Mu is equal to",
    "start": "1870555",
    "end": "1876000"
  },
  {
    "text": "1 over square root 2 Pi exponent",
    "start": "1876000",
    "end": "1881565"
  },
  {
    "text": "of minus 1/2 y minus Mu squared.",
    "start": "1881565",
    "end": "1887500"
  },
  {
    "text": "And this we can see is, you know, expand the square and we get",
    "start": "1889400",
    "end": "1896549"
  },
  {
    "text": "1 over square root 2 Pi exponent of minus",
    "start": "1896550",
    "end": "1906555"
  },
  {
    "text": "1/2 y squared",
    "start": "1906555",
    "end": "1909580"
  },
  {
    "text": "times exponent Mu y",
    "start": "1912230",
    "end": "1920040"
  },
  {
    "text": "minus 1/2 Mu squared.",
    "start": "1920040",
    "end": "1924460"
  },
  {
    "text": "So what happened here basically expanded the square and one of the terms will have- will be- will have only y's.",
    "start": "1925190",
    "end": "1933450"
  },
  {
    "text": "So got that out. And the other terms- two terms are here. Uh, yeah.",
    "start": "1933450",
    "end": "1939510"
  },
  {
    "text": "And with this, we can again now do the same pattern-matching exercise.",
    "start": "1939510",
    "end": "1947310"
  },
  {
    "text": "P of y Eta equals b of y exponent Eta transpose",
    "start": "1947310",
    "end": "1958289"
  },
  {
    "text": "t of y minus a of Eta.",
    "start": "1958290",
    "end": "1964450"
  },
  {
    "text": "And doing this, uh, uh, pattern, pattern matching, we see that Eta equals Mu,",
    "start": "1964820",
    "end": "1971510"
  },
  {
    "text": "in this case, Eta equals Mu, and similarly, Mu equals Eta.",
    "start": "1971510",
    "end": "1980710"
  },
  {
    "text": "The sufficient statistic, T of y is just equal to y. All right.",
    "start": "1982990",
    "end": "1991460"
  },
  {
    "text": "A of Eta equals Mu squared over 2.",
    "start": "1991460",
    "end": "2000024"
  },
  {
    "text": "And that's the same as Eta squared over 2.",
    "start": "2000025",
    "end": "2005300"
  },
  {
    "text": "And finally, b of y equals",
    "start": "2005520",
    "end": "2012080"
  },
  {
    "text": "1 over square root 2 Pi exponent",
    "start": "2012630",
    "end": "2019765"
  },
  {
    "text": "of minus y square by 2.",
    "start": "2019765",
    "end": "2024710"
  },
  {
    "text": "Okay. Any questions on this? [BACKGROUND] You can assume Sigma squared to be, uh, some constant.",
    "start": "2027760",
    "end": "2035930"
  },
  {
    "text": "And, uh, you know, this is going to, uh, have a few Sigma squares in a few places, but, you know, it's just a constant.",
    "start": "2035930",
    "end": "2042170"
  },
  {
    "text": "Uh, and, and, uh, as far as, you know, constructing, uh, generalized linear models is concerned,",
    "start": "2042170",
    "end": "2047945"
  },
  {
    "text": "you know, that has no bearing. So we- we're just gonna, you know, just to make, just to make the, the algebra simple, we're gonna assume, uh, uh,",
    "start": "2047945",
    "end": "2055370"
  },
  {
    "text": "Sigma squared equals 1, and then that's completely harmless. Okay. Any questions on this?",
    "start": "2055370",
    "end": "2062000"
  },
  {
    "text": "So uh, and, and here we can see that, um, from the, the, um, um,",
    "start": "2062000",
    "end": "2070294"
  },
  {
    "text": "point of view of exponential tilting it, it kind of suggests that you get the Bernoulli if you",
    "start": "2070295",
    "end": "2075725"
  },
  {
    "text": "start with a uniform distribution and perform an exponential tilting. And over here we actually see that,",
    "start": "2075725",
    "end": "2082159"
  },
  {
    "text": "uh, for the Gaussian distribution, this is actually a standard normal distribution that has, uh,",
    "start": "2082160",
    "end": "2088370"
  },
  {
    "text": "mean 0, standard deviation 1, and it's still a Gaussian distribution, right? So for the Gaussian distribution,",
    "start": "2088370",
    "end": "2094070"
  },
  {
    "text": "if you perform an exponential tilting, then you still get a Gaussian distribution with just, you know, different, different parameters.",
    "start": "2094070",
    "end": "2101105"
  },
  {
    "text": "Anyway, that's just, uh, uh, um, you know, um, that's just to tie it back to,",
    "start": "2101105",
    "end": "2106160"
  },
  {
    "text": "uh, the, the, uh, the tilting interpretation, uh, uh, that we spoke about.",
    "start": "2106160",
    "end": "2113270"
  },
  {
    "text": "Now, exponential families have a few nice properties.",
    "start": "2113270",
    "end": "2121350"
  },
  {
    "text": "Properties of exponential families.",
    "start": "2126190",
    "end": "2132929"
  },
  {
    "text": "One property is that log p of",
    "start": "2136390",
    "end": "2144980"
  },
  {
    "text": "y Eta is concave in Eta.",
    "start": "2144980",
    "end": "2156230"
  },
  {
    "text": "That is MLE is concave,",
    "start": "2156230",
    "end": "2162680"
  },
  {
    "text": "which is the same as the last function, negative log-likelihood is convex,",
    "start": "2162680",
    "end": "2170579"
  },
  {
    "text": "is convex in Eta, in Eta, right?",
    "start": "2170920",
    "end": "2180214"
  },
  {
    "text": "Another property is that",
    "start": "2180215",
    "end": "2185520"
  },
  {
    "text": "the expectation of y Eta equal to-",
    "start": "2187030",
    "end": "2196200"
  },
  {
    "text": "So the first derivative of the log partition function is the expectation of the distribution.",
    "start": "2202030",
    "end": "2208400"
  },
  {
    "text": "And this might look a little, you know, why is this the case? Let me look and see.",
    "start": "2208400",
    "end": "2214670"
  },
  {
    "text": "Similarly, the variance of",
    "start": "2214670",
    "end": "2219740"
  },
  {
    "text": "y is the second derivative of the log partition function.",
    "start": "2219740",
    "end": "2229710"
  },
  {
    "text": "And this might look a little, um, um, surprising, initially,",
    "start": "2236320",
    "end": "2242690"
  },
  {
    "text": "but it should not be so surprising when we see the fact that",
    "start": "2242690",
    "end": "2248915"
  },
  {
    "text": "the log partition function came from the moment generating function, right? And the moment generating function has the properties",
    "start": "2248915",
    "end": "2254060"
  },
  {
    "text": "that as you keep taking derivatives, it keeps generating the moments of the distribution, right? So the, uh, the first derivative of the log partition function,",
    "start": "2254060",
    "end": "2262625"
  },
  {
    "text": "which is also the log moment generating function give us the first moment. The second derivative gives you the second moment, and so on. Yes, question.",
    "start": "2262625",
    "end": "2268550"
  },
  {
    "text": "[inaudible] Um, good question, um.",
    "start": "2268550",
    "end": "2277700"
  },
  {
    "text": "Uh, what I was about to gonna say is that, you have these three question as your homework question.",
    "start": "2277700",
    "end": "2284705"
  },
  {
    "text": "So I'm not gonna, ah, reveal a lot about that. And- and I think, you know, question number four in- in your homework 1 is- is you're asked to prove",
    "start": "2284705",
    "end": "2292490"
  },
  {
    "text": "exactly these. Um, yeah. So tho- these are- these are some very nice properties about the, um, exponential family.",
    "start": "2292490",
    "end": "2302045"
  },
  {
    "text": "And, um, they're especially nice because in general, if you're given a distribution and asked to calculate the expectation,",
    "start": "2302045",
    "end": "2308945"
  },
  {
    "text": "you are required to take the integral of something, right? You are- you are expected to take the integral of your- your variable,",
    "start": "2308945",
    "end": "2316430"
  },
  {
    "text": "um, um, with resp- multiplied by the density. And if you want to take the- uh,",
    "start": "2316430",
    "end": "2321980"
  },
  {
    "text": "calculate the variance you- you know, you're supposed to take the integral of the squared, a square of the variable with respect to the density.",
    "start": "2321980",
    "end": "2328190"
  },
  {
    "text": "Um, and- and, you know, in general, performing integration is hard. It's- it's- it's messy, it's not nice.",
    "start": "2328190",
    "end": "2333319"
  },
  {
    "text": "Okay. What- but for the exponential family, um, instead of doing integration, you can actually, you know, just differentiate the log partition function.",
    "start": "2333320",
    "end": "2340234"
  },
  {
    "text": "And then you take- you differentiate it twice you get the second moment, right?",
    "start": "2340235",
    "end": "2345335"
  },
  {
    "text": "Those are- you know, these are some nice properties about, um, um, the, um, log partition func- uh,",
    "start": "2345335",
    "end": "2351335"
  },
  {
    "text": "about, uh, exponential families. Any questions? Yes, question. [inaudible]",
    "start": "2351335",
    "end": "2361250"
  },
  {
    "text": "Is this obvious or is that a proof? Well, it- it is, um, I mean, that is a proof and no, you know, uh,",
    "start": "2361250",
    "end": "2367460"
  },
  {
    "text": "your homework question number four, homework 1 is you're asked to prove this. [BACKGROUND]",
    "start": "2367460",
    "end": "2380300"
  },
  {
    "text": "Why is it concave? I mean it's, yeah, it's- it's- it's in your homework, you need to show it. Yeah, all these are in your homework.",
    "start": "2380300",
    "end": "2386570"
  },
  {
    "text": "[LAUGHTER] Oh, NLL is negative log-likelihood.",
    "start": "2386570",
    "end": "2392870"
  },
  {
    "text": "Um, so the maximum lo- uh, likelihood estimate, um, is, uh, to calculate the maximum likelihood estimate you take the probability, you know,",
    "start": "2392870",
    "end": "2401930"
  },
  {
    "text": "or the log of the probability and you maximize this with respect to Theta. Now, if you just swi- switch the sign,",
    "start": "2401930",
    "end": "2408365"
  },
  {
    "text": "you get the negative of this, and that's generally considered as the last function which you want to minimize.",
    "start": "2408365",
    "end": "2413435"
  },
  {
    "text": "So now if in the- in the maximum likelihood, uh, interpretation you're trying to maximize, uh, uh,",
    "start": "2413435",
    "end": "2419540"
  },
  {
    "text": "something and, um, you know, in the- from a loss function perspective, you want to minimize something,",
    "start": "2419540",
    "end": "2424550"
  },
  {
    "text": "so you just flip- flip the sign and, you know, and- and that's generally considered the loss function. Yes, question? [BACKGROUND] What's,",
    "start": "2424550",
    "end": "2436220"
  },
  {
    "text": "uh, ca- ca- can you please explain what's concave down and concave up? [BACKGROUND] Oh, okay. Oh, I see- I see- I see.",
    "start": "2436220",
    "end": "2444695"
  },
  {
    "text": "So- so the question is by concave, uh, do I mean this or do I mean this? Right? Is- is- is the function, uh,",
    "start": "2444695",
    "end": "2451730"
  },
  {
    "text": "uh, shaped like this or the function shaped like this? So, uh, the common nomenclature is when you have a global maxima.",
    "start": "2451730",
    "end": "2459980"
  },
  {
    "text": "When there's a maxima you call it concave and when there is a global minima you call it convex.",
    "start": "2459980",
    "end": "2467700"
  },
  {
    "text": "Okay. So if you have a convex function, the negative of that will be a concave function and vice versa. Good question.",
    "start": "2468010",
    "end": "2476250"
  },
  {
    "text": "Any other questions? All right. So that's- that's, you know,",
    "start": "2479580",
    "end": "2484730"
  },
  {
    "text": "that's- that's it about exponential families. And now, we're gonna see how the exponential family kind of ties into,",
    "start": "2484730",
    "end": "2491405"
  },
  {
    "text": "uh, generalized linear models. [NOISE] All right, so generalized linear models.",
    "start": "2491405",
    "end": "2568770"
  },
  {
    "text": "So with the exponential families, so far we were dealing with y,",
    "start": "2571390",
    "end": "2577925"
  },
  {
    "text": "right? The variable y. So we were defining, um, um, a probability distribution or y and the parameter was Eta, right?",
    "start": "2577925",
    "end": "2586340"
  },
  {
    "text": "And there was no x anywhere, right? In- in supervised learning we're trying to learn a mapping from x to y, right?",
    "start": "2586340",
    "end": "2593210"
  },
  {
    "text": "[NOISE] So in generalized linear models, we- we now use the exponential family to introduce",
    "start": "2593210",
    "end": "2600470"
  },
  {
    "text": "a relation between a set of variables x to the variable y. And how do we do that? We make the following three assumptions, or- or,",
    "start": "2600470",
    "end": "2612530"
  },
  {
    "text": "uh, these are like the- the- the three- the three steps for constructing, uh, a GLM.",
    "start": "2612530",
    "end": "2620105"
  },
  {
    "text": "So first is, you make the assumption y given x belongs",
    "start": "2620105",
    "end": "2629150"
  },
  {
    "text": "to an exponential family [NOISE] Eta.",
    "start": "2629150",
    "end": "2640349"
  },
  {
    "text": "That is, given x and some Theta, y now belongs to some exponential family whose parameter is Eta.",
    "start": "2641500",
    "end": "2651170"
  },
  {
    "text": "Right? So far- so far uh, in this lecture, we assumed we were- uh,",
    "start": "2651170",
    "end": "2658505"
  },
  {
    "text": "we didn't assume but we were considering the cases when y belonged to [NOISE] exponential family parameter Eta.",
    "start": "2658505",
    "end": "2667654"
  },
  {
    "text": "But now we say y given x belongs to an exponential family with parameter Theta.",
    "start": "2667655",
    "end": "2675570"
  },
  {
    "text": "And our- the goal",
    "start": "2676300",
    "end": "2682940"
  },
  {
    "text": "of prediction will [NOISE] be to predict the mean.",
    "start": "2682940",
    "end": "2688385"
  },
  {
    "text": "So the hypothesis that we wanna learn [NOISE] is",
    "start": "2688385",
    "end": "2694925"
  },
  {
    "text": "the expectation of y given [NOISE] x.",
    "start": "2694925",
    "end": "2703400"
  },
  {
    "text": "And the third assumption that we are gonna make is now connecting x Theta and Theta.",
    "start": "2703400",
    "end": "2712430"
  },
  {
    "text": "So here we're gonna make, uh, reasonably strong assumption that Theta will be equal to Theta transpose x.",
    "start": "2712430",
    "end": "2722359"
  },
  {
    "text": "Right? So these two seem somewhat reasonable, uh. You know, we're just assuming y given x follows some,",
    "start": "2722360",
    "end": "2729560"
  },
  {
    "text": "ah, exponential family distribution. And also, uh, the second assumption was that the hypothesis or the,",
    "start": "2729560",
    "end": "2736325"
  },
  {
    "text": "the value that we wanna predict for an input x is gonna be the expectation of this distribution if,",
    "start": "2736325",
    "end": "2744454"
  },
  {
    "text": "if- you know, i- in general, when you- when you know, um, that a variable is distributed according to some distribution,",
    "start": "2744455",
    "end": "2752570"
  },
  {
    "text": "and you're asked to make a prediction of what value is, you know, predicting the mean of the distribution is a pretty reasonable- you know,",
    "start": "2752570",
    "end": "2758480"
  },
  {
    "text": "um, pretty reasonable value to predict. So this seems pretty reasonable. And then we make yet another assumption of the way we tie",
    "start": "2758480",
    "end": "2765890"
  },
  {
    "text": "the parameter of the exponential family with the inputs as Theta equals Theta transpose x.",
    "start": "2765890",
    "end": "2771599"
  },
  {
    "text": "Right? Now, together with these three, um, assumptions, we can,",
    "start": "2771600",
    "end": "2777444"
  },
  {
    "text": "uh, or- or- or these three steps, we can construct exponential family or generalized linear models for different exponential family, um, ah, distributions.",
    "start": "2777445",
    "end": "2787280"
  },
  {
    "text": "Now, let's see what happens. Uh, how do we construct, um,",
    "start": "2787280",
    "end": "2792320"
  },
  {
    "text": "[NOISE] kind of in a- in a- in a- in a pictorial way?",
    "start": "2792320",
    "end": "2798170"
  },
  {
    "text": "Let's- the- these three",
    "start": "2798170",
    "end": "2805325"
  },
  {
    "text": "assumptions or these three steps can be kind of visualized like this. So we have some X_i to R_d, right?",
    "start": "2805325",
    "end": "2816380"
  },
  {
    "text": "Now, I'm gonna run it through.",
    "start": "2816380",
    "end": "2820079"
  },
  {
    "text": "Theta- Theta transpose X_i.",
    "start": "2825130",
    "end": "2832220"
  },
  {
    "text": "And that is gonna output Eta i, right?",
    "start": "2832220",
    "end": "2841730"
  },
  {
    "text": "And this Eta i [NOISE]",
    "start": "2841730",
    "end": "2851329"
  },
  {
    "text": "becomes the parameter of some exponential family that has, you know, some particular, you know,",
    "start": "2851329",
    "end": "2856445"
  },
  {
    "text": "sufficient statistic lock partition function b of y. By- by choosing the three different uh,",
    "start": "2856445",
    "end": "2865519"
  },
  {
    "text": "functions of an exponential family, you know- you completely determine the exponential family. And the output of- of- of, um,",
    "start": "2865520",
    "end": "2874310"
  },
  {
    "text": "the dot product between Theta and x become then natural parameter of that exponential family,",
    "start": "2874310",
    "end": "2881660"
  },
  {
    "text": "and from this we sample y^i. This is the big picture you wanna have in your mind.",
    "start": "2881660",
    "end": "2887570"
  },
  {
    "text": "So- yes, question. [inaudible].",
    "start": "2887570",
    "end": "2899599"
  },
  {
    "text": "So the question is, uh, shouldn't- shouldn't these functions also be somewhat,",
    "start": "2899600",
    "end": "2906575"
  },
  {
    "text": "um- um parameterized by the model? [inaudible]. Yeah, yeah, yeah.",
    "start": "2906575",
    "end": "2917000"
  },
  {
    "text": "[inaudible] Yeah. So I would- I would- I would- um, the general process of how you go about constructing uh,",
    "start": "2917000",
    "end": "2923435"
  },
  {
    "text": "a GLM is to first choose a distribution. For example, choose that you want to model your output as Gaussian or choose that you",
    "start": "2923435",
    "end": "2931760"
  },
  {
    "text": "wanna model your output has a Poisson distribution or choose that you wanna model your output as Bernoulli. And by making that initial choice of what distribution you want to use, these get fixed.",
    "start": "2931760",
    "end": "2943295"
  },
  {
    "text": "All right? So these get fixed. The- you know, the t function, a function and b function, they get fixed up front when we make the choice of the distribution for y, right.",
    "start": "2943295",
    "end": "2954785"
  },
  {
    "text": "And then the question becomes, what is going to be the parameter for a particular y_i, right?",
    "start": "2954785",
    "end": "2961234"
  },
  {
    "text": "And the perimeter- the natural parameter is then um, assumed to be uh, uh,",
    "start": "2961235",
    "end": "2966850"
  },
  {
    "text": "Theta transpose x for a given example. So Theta transpose x will give you the uh, natural parameter, and from this,",
    "start": "2966850",
    "end": "2973975"
  },
  {
    "text": "y_i is- is obtained. So this is the assumption uh, that we're gonna make of how our data was generated.",
    "start": "2973975",
    "end": "2980420"
  },
  {
    "text": "Right? Uh, any questions about this? And- so think of",
    "start": "2980500",
    "end": "2991250"
  },
  {
    "text": "this as like- as like the flowchart of the data-generating process of how we believe, um, um, the data is generated.",
    "start": "2991250",
    "end": "2999560"
  },
  {
    "text": "[NOISE] A few things to note here is that Theta is global in the sense that you have just one Theta for- you know,",
    "start": "2999560",
    "end": "3008710"
  },
  {
    "text": "for- for- for- for the- for the full model. But whereas the parameter of the exponential family Eta is different for each example.",
    "start": "3008710",
    "end": "3016735"
  },
  {
    "text": "So each example- each x will output a particular natural parameter of the exponential family.",
    "start": "3016735",
    "end": "3024890"
  },
  {
    "text": "Now, we can now see how this relates to, for example, linear regression.",
    "start": "3027570",
    "end": "3035090"
  },
  {
    "text": "So ordinary least squares or linear regression.",
    "start": "3035130",
    "end": "3040869"
  },
  {
    "text": "[NOISE] Yes, question.",
    "start": "3040870",
    "end": "3047740"
  },
  {
    "text": "[inaudible].",
    "start": "3047740",
    "end": "3053619"
  },
  {
    "text": "Uh, uh, what's the question? Why is Theta local to one family? No. Eta. So um,",
    "start": "3053620",
    "end": "3059065"
  },
  {
    "text": "the, uh, the question is, why is Eta local to one family? So the- um, the exponential family,",
    "start": "3059065",
    "end": "3065905"
  },
  {
    "text": "uh, corresponds to distributions that have this form, right? [NOISE]",
    "start": "3065905",
    "end": "3080980"
  },
  {
    "text": "Now, once you choose p, a, and b, you fix the functional form of your exponential family.",
    "start": "3080980",
    "end": "3089214"
  },
  {
    "text": "And by- for example, by choosing your um, b, t,",
    "start": "3089215",
    "end": "3094569"
  },
  {
    "text": "and a according to, um, uh, you know, that set of choices, you get the Gaussian.",
    "start": "3094570",
    "end": "3100210"
  },
  {
    "text": "By choosing b, T, and a according to this set of choices, you get a Bernoulli, right? [inaudible]",
    "start": "3100210",
    "end": "3122610"
  },
  {
    "text": "So, um, now, ah, if- if- if I understood, uh, you correctly, uh, the question is Theta is- is local to x? Is that- is that the question?",
    "start": "3122610",
    "end": "3131160"
  },
  {
    "text": "[inaudible]. Yes. I- I, um,",
    "start": "3131160",
    "end": "3139200"
  },
  {
    "text": "I'm- I'm- I'm, kind of, uh, uh, trying to understand what you mean by local to your data or local to your, uh,.",
    "start": "3139200",
    "end": "3145470"
  },
  {
    "text": "[inaudible]. Yeah. [inaudible]. Yeah, so Eta is, you get a different Eta,",
    "start": "3145470",
    "end": "3150630"
  },
  {
    "text": "for different examples. So each example will out put its own Eta. right? So in this process, x i, uh,",
    "start": "3150630",
    "end": "3158145"
  },
  {
    "text": "take a dot product with Theta will give you Eta i of the ith example. Yes, question.",
    "start": "3158145",
    "end": "3164670"
  },
  {
    "text": "[inaudible]. Exactly. Exactly. We'll- we'll- we'll, uh,",
    "start": "3164670",
    "end": "3171750"
  },
  {
    "text": "I'm gonna draw a picture about- about that in, uh, shortly, right? So, um, so for the,",
    "start": "3171750",
    "end": "3183285"
  },
  {
    "text": "uh, ordinary least squares, if we make the choice that our exponential family is Gaussian,",
    "start": "3183285",
    "end": "3190890"
  },
  {
    "text": "then a natural consequence of that is- so if- if- if you construct a generalized linear family where the exponential family,",
    "start": "3190890",
    "end": "3199485"
  },
  {
    "text": "uh, uh, distribution is Gaussian, then you naturally get least-squares regression is what you should be doing, right?",
    "start": "3199485",
    "end": "3206325"
  },
  {
    "text": "What- why is that? Um, so assuming",
    "start": "3206325",
    "end": "3212760"
  },
  {
    "text": "exponential family is Gaussian,",
    "start": "3212760",
    "end": "3222255"
  },
  {
    "text": "we get h_Theta of x. We want h_Theta of x to be the expectation,",
    "start": "3222255",
    "end": "3230400"
  },
  {
    "text": "expectation of y given x, um, Theta.",
    "start": "3230400",
    "end": "3236220"
  },
  {
    "text": "And we saw that the mean of- of, uh, Gaussian is just Mu, right?",
    "start": "3236220",
    "end": "3244500"
  },
  {
    "text": "The mean of a Gaussian is Mu, and Mu as a function of Eta, was just Eta, right?",
    "start": "3244500",
    "end": "3251445"
  },
  {
    "text": "And Eta is equal to Theta transpose x. So we basically get h_Theta of x equals Theta transpose x, right?",
    "start": "3251445",
    "end": "3265365"
  },
  {
    "text": "And that was our hypothesis for- for, uh, linear regression.",
    "start": "3265365",
    "end": "3269470"
  },
  {
    "text": "Similarly for, uh, logistic regression. [NOISE].",
    "start": "3271310",
    "end": "3297450"
  },
  {
    "text": "So lo- so logistic regression is again basically a generalized linear model where we choose the exponential family,",
    "start": "3297450",
    "end": "3304770"
  },
  {
    "text": "uh, to be Bernoulli's, right? By- by just making the choice that the exponential family is Bernoulli,",
    "start": "3304770",
    "end": "3312555"
  },
  {
    "text": "logistic regression pops out as a natural consequence. So, um,",
    "start": "3312555",
    "end": "3318940"
  },
  {
    "text": "again- so assume exponential family is Bernoulli,",
    "start": "3325760",
    "end": "3335470"
  },
  {
    "text": "h_Theta of x, again is- should be expectation of y given x parameterized by Theta.",
    "start": "3340790",
    "end": "3349095"
  },
  {
    "text": "And in case of Bernoulli the- the mean of the Bernoulli was- is- is just Phi, okay?",
    "start": "3349095",
    "end": "3357390"
  },
  {
    "text": "And Phi as a function of Eta, what we saw was- is equal to 1 over 1 plus e to the minus Eta.",
    "start": "3357390",
    "end": "3368145"
  },
  {
    "text": "And then we make, uh, uh, the third assumption about, uh, GLMs was that Eta equals Theta transpose x.",
    "start": "3368145",
    "end": "3374280"
  },
  {
    "text": "So 1 over 1 plus e to the minus Theta transpose x. h_Theta of x equals, equals this, right?",
    "start": "3374280",
    "end": "3385215"
  },
  {
    "text": "And, and what we see here is that this is exactly equal to g of Theta transpose x,",
    "start": "3385215",
    "end": "3394530"
  },
  {
    "text": "where g of z equals 1 over 1 plus e to the minus z, right?",
    "start": "3394530",
    "end": "3400350"
  },
  {
    "text": "When we introduced logistic regression, we arbitrarily chose g to take this form, something that's- that squishes minus infinity to plus infinity into the range 0-1.",
    "start": "3400350",
    "end": "3411165"
  },
  {
    "text": "But this is a more principled way of deriving it, where you think of your output to be,",
    "start": "3411165",
    "end": "3417735"
  },
  {
    "text": "um, uh, following, er, the- following the Bernoulli distribution and construct",
    "start": "3417735",
    "end": "3423300"
  },
  {
    "text": "a GLM and with just those two assumptions,",
    "start": "3423300",
    "end": "3428519"
  },
  {
    "text": "we see that, uh, hypotheses must take this form. Yes, question.",
    "start": "3428519",
    "end": "3435200"
  },
  {
    "text": "[inaudible]. Was there a mathematical reason for our third assumption?",
    "start": "3435200",
    "end": "3442145"
  },
  {
    "text": "So, um, this is a modeling choice, um, where you're- you're, um,",
    "start": "3442145",
    "end": "3449430"
  },
  {
    "text": "making an assumption that the natural parameter is, is, um, a linear combination of,",
    "start": "3449430",
    "end": "3456015"
  },
  {
    "text": "of, of your x's. You can make this relation as complex as possible in general, right?",
    "start": "3456015",
    "end": "3463410"
  },
  {
    "text": "You know, beyond GLM. So you can- you can [NOISE] for example, say your Eta equals, um, you know, um,",
    "start": "3463410",
    "end": "3470190"
  },
  {
    "text": "some neural network parameterized by some, some, some Theta of your x, right?",
    "start": "3470190",
    "end": "3476339"
  },
  {
    "text": "And this is totally fine. You can construct, um, uh, um, a generalized models except now it's no more linear.",
    "start": "3476340",
    "end": "3483210"
  },
  {
    "text": "We can call it GLM, but, you know, you, you can absolutely do something like this. Uh, but for GLMs, um,",
    "start": "3483210",
    "end": "3490500"
  },
  {
    "text": "it's a linear model because we're making the assumption that Eta is a linear function of x's.",
    "start": "3490500",
    "end": "3497250"
  },
  {
    "text": "[inaudible].",
    "start": "3497250",
    "end": "3513960"
  },
  {
    "text": "So the question is, um, GLMs are convex or concave and when you look at your scatter plot of your data,",
    "start": "3513960",
    "end": "3522119"
  },
  {
    "text": "if it does not look, uh, convex or concave, should we still fit GLMs to that data, right? Uh, one thing to,",
    "start": "3522120",
    "end": "3528735"
  },
  {
    "text": "to note is that, um, the GLMs in exponential families are all convex or",
    "start": "3528735",
    "end": "3534780"
  },
  {
    "text": "concave with respect to the parameters and in a scatter plot, you're looking at data. Right? You know, they're, they're two different things, right?",
    "start": "3534780",
    "end": "3541154"
  },
  {
    "text": "Now, your data is generally kind of embedded in your loss function in some way.",
    "start": "3541155",
    "end": "3546480"
  },
  {
    "text": "It's, kind of, you know, the shape of the loss function is decided by your data. And the, the, the shape of the loss function is gonna be very,",
    "start": "3546480",
    "end": "3552810"
  },
  {
    "text": "very different from looking at the [NOISE] scatter plot of the data itself, right? Your, your data may look- may,",
    "start": "3552810",
    "end": "3559500"
  },
  {
    "text": "may look- may, may look in any shape or form, right? But that tells you nothing about how the shape of the loss function is gonna look.",
    "start": "3559500",
    "end": "3566760"
  },
  {
    "text": "[inaudible].",
    "start": "3566760",
    "end": "3577890"
  },
  {
    "text": "So, um, may, may maybe this- the, the next, next diagram I'm about to draw might answer your question,",
    "start": "3577890",
    "end": "3583620"
  },
  {
    "text": "if not, you know, please ask the question again. Um, yeah, so what's we- what- so,",
    "start": "3583620",
    "end": "3593835"
  },
  {
    "text": "uh, here we see that, um, the- the linear regression and",
    "start": "3593835",
    "end": "3599535"
  },
  {
    "text": "logistic regression pop out as natural consequences by just choosing- uh,",
    "start": "3599535",
    "end": "3606030"
  },
  {
    "text": "by just making the choice of our exponential family distribution, right? And now, how does this, kind of, um,",
    "start": "3606030",
    "end": "3614865"
  },
  {
    "text": "look visually, okay?",
    "start": "3614865",
    "end": "3621030"
  },
  {
    "text": "[NOISE].",
    "start": "3621030",
    "end": "3650780"
  },
  {
    "text": "So for linear regression, um, [NOISE]",
    "start": "3650780",
    "end": "3664940"
  },
  {
    "text": "for linear regression we assume this to be x-axis the data. Right. And there's some Theta that- so x is in Rd,",
    "start": "3664940",
    "end": "3677585"
  },
  {
    "text": "Theta is in Rd. But for this picture, we're just assuming it's one-dimensional.",
    "start": "3677585",
    "end": "3683375"
  },
  {
    "text": "And here, this line, for example, this line",
    "start": "3683375",
    "end": "3691450"
  },
  {
    "text": "is basically Theta equals Theta transpose x for some Theta, right?",
    "start": "3691450",
    "end": "3697260"
  },
  {
    "text": "And then what we- what we saw was Eta for, in case of, uh, the Gaussian distribution is also equal to Mu, right?",
    "start": "3697260",
    "end": "3707705"
  },
  {
    "text": "For- for the Gaussian distribution, Eta equals Mu. And y for a given x is now distributed as a Gaussian with mean Mu.",
    "start": "3707705",
    "end": "3717475"
  },
  {
    "text": "So if we take the example xi, we get a Mu^i, which means for this value of x,",
    "start": "3717475",
    "end": "3725335"
  },
  {
    "text": "we assume that centered around here,",
    "start": "3725335",
    "end": "3730570"
  },
  {
    "text": "we have a Gaussian distribution. So this is y and this is xi.",
    "start": "3730570",
    "end": "3740010"
  },
  {
    "text": "And from this Gaussian distribution, we sample a y. Let's say we got this one and this is yi.",
    "start": "3740010",
    "end": "3750920"
  },
  {
    "text": "Similarly, for a different, say xj, there's a Gaussian distribution.",
    "start": "3750920",
    "end": "3759455"
  },
  {
    "text": "And from this Gaussian distribution, we sample, let's say this is yj.",
    "start": "3759455",
    "end": "3768960"
  },
  {
    "text": "So for every different value of input- for every input value,",
    "start": "3769960",
    "end": "3775550"
  },
  {
    "text": "for every x, we dot it with Theta, we get Eta.",
    "start": "3775550",
    "end": "3781940"
  },
  {
    "text": "And that Eta is equal to Mu of a Gaussian distribution centered at that-centered at that- at that Theta or Mu.",
    "start": "3781940",
    "end": "3792260"
  },
  {
    "text": "And from that Gaussian distribution, you know, its- its a distribution along this axis, we sample a y, all right?",
    "start": "3792260",
    "end": "3799775"
  },
  {
    "text": "And that makes the x- xi-yi pair or the xj-yj pair. All right. So let's look at a few more- that is another Gaussian distribution here.",
    "start": "3799775",
    "end": "3810500"
  },
  {
    "text": "This point, let's assume this-",
    "start": "3810500",
    "end": "3815765"
  },
  {
    "text": "this point- this, right?",
    "start": "3815765",
    "end": "3821630"
  },
  {
    "text": "And from this, we assume that the data has generated this way.",
    "start": "3821630",
    "end": "3826920"
  },
  {
    "text": "From this we get- this is x, this is y.",
    "start": "3831730",
    "end": "3840780"
  },
  {
    "text": "So I'm just gonna try to draw those points again.",
    "start": "3841030",
    "end": "3846540"
  },
  {
    "text": "So this is x, this is y, the data was generated.",
    "start": "3849760",
    "end": "3856415"
  },
  {
    "text": "So this is the data generation process.",
    "start": "3856415",
    "end": "3862080"
  },
  {
    "text": "And now when we start our machine learning analysis, we start from here.",
    "start": "3863710",
    "end": "3868730"
  },
  {
    "text": "All right? This is the observation of the data.",
    "start": "3868730",
    "end": "3871710"
  },
  {
    "text": "And from this observation, we now want to infer what Theta is.",
    "start": "3874840",
    "end": "3881280"
  },
  {
    "text": "So each point over here has, you know, uh,",
    "start": "3883480",
    "end": "3888740"
  },
  {
    "text": "so- so the true Theta might- might be something like this, but we don't know.",
    "start": "3888740",
    "end": "3895835"
  },
  {
    "text": "Each example, each yi has two parts, right?",
    "start": "3895835",
    "end": "3901760"
  },
  {
    "text": "So each of these is- is basically,",
    "start": "3901760",
    "end": "3907790"
  },
  {
    "text": "um, Theta transpose x plus Epsilon,",
    "start": "3907790",
    "end": "3916325"
  },
  {
    "text": "where Epsilon is the Gaussian noise. So Theta transpose x. Now think of this as the signal and you have a noise,",
    "start": "3916325",
    "end": "3922205"
  },
  {
    "text": "and you're trying to separate the signal from the noise, right? We want to get what the signal is. What is Theta?",
    "start": "3922205",
    "end": "3927484"
  },
  {
    "text": "What is the- the- the invisible dotted line over here from which the y's are sampled, right?",
    "start": "3927485",
    "end": "3934250"
  },
  {
    "text": "And the way this relates to generalized linear models is that we assume for each example,",
    "start": "3934250",
    "end": "3940520"
  },
  {
    "text": "there is an exponential family distribution, in this case, the Gaussian. For each example, there is a Gaussian distribution,",
    "start": "3940520",
    "end": "3948795"
  },
  {
    "text": "and the y was sampled from the Gaussian distribution. Right? And now by just- by just starting with, uh,",
    "start": "3948795",
    "end": "3957050"
  },
  {
    "text": "the x and y pairs without seeing anything else, we want to- we want to learn what- what- what Theta is.",
    "start": "3957050",
    "end": "3965165"
  },
  {
    "text": "And- and that's where, you know, that's basically statistics machine learning, where you start from the data and you're trying to-",
    "start": "3965165",
    "end": "3970640"
  },
  {
    "text": "to construct what the model is. Yes, question. [inaudible].",
    "start": "3970640",
    "end": "3980930"
  },
  {
    "text": "So the question is, in practice, how do we know whether, you know, the noise actually follows a Gaussian distribution or not?",
    "start": "3980930",
    "end": "3987365"
  },
  {
    "text": "Right? And, uh, the- the short answer to that is our- myself,",
    "start": "3987365",
    "end": "3993260"
  },
  {
    "text": "funny, but in machine learning, we don't care. But in statistics, you care a lot. Um, uh, because, er,",
    "start": "3993260",
    "end": "3998510"
  },
  {
    "text": "in- in machine learning, our- our goal is, um, are we able to learn a model that you know,",
    "start": "3998510",
    "end": "4006295"
  },
  {
    "text": "that just, you know, does well in terms of prediction on unseen data, right? But as in- in- in classical statistics, uh,",
    "start": "4006295",
    "end": "4013435"
  },
  {
    "text": "we are trying to- your data is- is kind of, you know, hopefully, generated with a well-designed experiment",
    "start": "4013435",
    "end": "4020410"
  },
  {
    "text": "where if you perform a regression analysis, the values of, you know, Theta are meaningful in some way.",
    "start": "4020410",
    "end": "4025930"
  },
  {
    "text": "You can test a hypothesis, right? And- and over there, the- the- the assumption of whether the- the noise was actually Gaussian or not,",
    "start": "4025930",
    "end": "4033309"
  },
  {
    "text": "are- are very important, right? But uh, from machine learning point of view, we- we're mostly interested in how well our model predicts on unseen data.",
    "start": "4033310",
    "end": "4041500"
  },
  {
    "text": "For ex- for example, um, if yi is, you know if- if there's a new test example, x-star, um,",
    "start": "4041500",
    "end": "4049075"
  },
  {
    "text": "our model- and suppose it- it learned this other line to be the- the,",
    "start": "4049075",
    "end": "4056140"
  },
  {
    "text": "uh, uh, to be the hypothesis then it- it's gonna say this is, you know, um, gonna be your prediction.",
    "start": "4056140",
    "end": "4061960"
  },
  {
    "text": "And the, uh, um, yeah, so i- in general, we are- we are interested in, you know,",
    "start": "4061960",
    "end": "4067930"
  },
  {
    "text": "what is this gap basically, right? And- and in practice, it's not even possible to measure this gap because we don't know the first dotted line.",
    "start": "4067930",
    "end": "4076164"
  },
  {
    "text": "So, um, so we're gonna make this prediction, but the true data was sampled from,",
    "start": "4076165",
    "end": "4082870"
  },
  {
    "text": "you know, from- from a distribution center on the first dotted line. So your data might be here. And what we care is how far was our prediction from the observed data?",
    "start": "4082870",
    "end": "4091240"
  },
  {
    "text": "And that's all is our focus in machine learning. We are not trying to, um, we care less about, you know, what was the specific value of Theta? Is it meaningful?",
    "start": "4091240",
    "end": "4099174"
  },
  {
    "text": "You know, those -those are questions that's more classically handled by statistics and not machine learning. Good question. Next question.",
    "start": "4099175",
    "end": "4105100"
  },
  {
    "text": "[inaudible]",
    "start": "4105100",
    "end": "4131005"
  },
  {
    "text": "So- so I- I guess to summarize your question, you know, is the- is there a more principled way of-",
    "start": "4131005",
    "end": "4136539"
  },
  {
    "text": "of choosing what our distribution should be, right? Um, the- the, uh,",
    "start": "4136540",
    "end": "4142150"
  },
  {
    "text": "short answer there is, you know, um, so in machine learning, um, when we're given a set of data, right?",
    "start": "4142150",
    "end": "4148870"
  },
  {
    "text": "Uh, let's assume you have some training set, right? Generally what we do is we- we split it into a training set and a validation set,",
    "start": "4148870",
    "end": "4158049"
  },
  {
    "text": "and a test set, right? And, um, for- for,",
    "start": "4158050",
    "end": "4164005"
  },
  {
    "text": "you know questions such- such as the one you posed about, you know, what's the choice of, you know, the exponential family distribution we want to make,",
    "start": "4164005",
    "end": "4171594"
  },
  {
    "text": "the answer almost always is going to be, you know, um, try both approaches.",
    "start": "4171595",
    "end": "4176949"
  },
  {
    "text": "Try- try two different distribution, say Gaussian and Laplace, and see how well that performs on the validation test- on the validation set.",
    "start": "4176950",
    "end": "4184825"
  },
  {
    "text": "And which of the choices work better on your validation set, go with that. [NOISE]",
    "start": "4184825",
    "end": "4193719"
  },
  {
    "text": "[inaudible]. Yes, that's just a limitation that we live with. Yeah, exactly. Yes, questions? [inaudible]",
    "start": "4193720",
    "end": "4208480"
  },
  {
    "text": "Well, so, uh, the question is, here we are sampling y. Uh, the sampling is the assumption that we're",
    "start": "4208480",
    "end": "4214810"
  },
  {
    "text": "making of how the data was generated under the colors. [inaudible].",
    "start": "4214810",
    "end": "4223690"
  },
  {
    "text": "So, um- um, what we are predicting is, um, so with- with- so with linear regression,",
    "start": "4223690",
    "end": "4231355"
  },
  {
    "text": "we saw two different interpretations, right? One was just minimize the cost function; take the data that we have, minimize the cost function.",
    "start": "4231355",
    "end": "4237160"
  },
  {
    "text": "The other was, the, uh- the probabilistic interpretation, right? And right there we had made the assumption that your y^i was",
    "start": "4237160",
    "end": "4244330"
  },
  {
    "text": "sampled from a normal distribution whose mean was Theta transpose x, right? And we are making that assumption,",
    "start": "4244330",
    "end": "4251215"
  },
  {
    "text": "which may or may not be true. And if we believe in that assumption, then the maximum likelihood principle tells",
    "start": "4251215",
    "end": "4258790"
  },
  {
    "text": "us that we need to minimize the squared loss, right? And similarly over here, um, if we make a generalized,",
    "start": "4258790",
    "end": "4266280"
  },
  {
    "text": "uh- if we assume that our data is from a generalized linear model where the exponential family is- is- is Gaussian,",
    "start": "4266280",
    "end": "4272655"
  },
  {
    "text": "then our hypothesis should be, you know, uh- maybe I erased it.",
    "start": "4272655",
    "end": "4277705"
  },
  {
    "text": "Our hypothesis should be just Theta transpose x. You know, which is basically the, um- um- um, linear, uh- uh, linear regression hypotheses.",
    "start": "4277705",
    "end": "4285804"
  },
  {
    "text": "And if you perform maximum likelihood expectation on your, uh, generalized linear model, then you get the squared loss as well.",
    "start": "4285805",
    "end": "4292699"
  },
  {
    "text": "Right? So these- the linear regression and logistic regression are exactly specific cases of generalized linear models where we chose their family, exactly.",
    "start": "4293100",
    "end": "4305230"
  },
  {
    "text": "By making a choice of the family, uh, to be Gaussian, we get exactly linear regression. By making a choice to be Bernoulli,",
    "start": "4305230",
    "end": "4311860"
  },
  {
    "text": "we get exactly logistic regression, right? So that's- that's- you know, this- this is the picture you wanna have in mind for, um- uh,",
    "start": "4311860",
    "end": "4320530"
  },
  {
    "text": "linear regression that the assumption is that there is- there is some Theta,",
    "start": "4320530",
    "end": "4326484"
  },
  {
    "text": "some true Theta, which acts as the mean for each y^i that we're gonna sample from.",
    "start": "4326484",
    "end": "4335574"
  },
  {
    "text": "And the y^is that we're gonna observe are going to be sampled from this, so there is some noise coming in. And we start with these noisy observations.",
    "start": "4335575",
    "end": "4343225"
  },
  {
    "text": "And we're- you know, the- the hope is that we kind of, you know, prune away the noise and can extract the signal in there, right?",
    "start": "4343225",
    "end": "4350514"
  },
  {
    "text": "That's- that's- that's the picture you wanna have. And similarly for logistic regression-",
    "start": "4350515",
    "end": "4356179"
  },
  {
    "text": "yes, question. [inaudible].",
    "start": "4365010",
    "end": "4371050"
  },
  {
    "text": "Yeah.",
    "start": "4371050",
    "end": "4380710"
  },
  {
    "text": "[inaudible] So we- we don't get a Theta, we on- when you- when you start your machine learning task,",
    "start": "4380710",
    "end": "4386110"
  },
  {
    "text": "you're just giving your data set your x's and y's, right? And then we make an assumption that the way the- the,",
    "start": "4386110",
    "end": "4392415"
  },
  {
    "text": "uh- uh, data was generated was, you know, through a generalized linear model. If we make that assumption, then the, you know,",
    "start": "4392415",
    "end": "4401080"
  },
  {
    "text": "uh- the set of actions we need to take is to perform maximum likelihood on a generalized linear model. Which- which, uh- and if the generalized linear model has a Gaussian distribution,",
    "start": "4401080",
    "end": "4409840"
  },
  {
    "text": "then you get linear regression. [inaudible] You don't generate y's, right? We don't generate y's, we- we- we- we just- we",
    "start": "4409840",
    "end": "4417340"
  },
  {
    "text": "just take the observed x's and y's and- and start from there. Exactly. We don't- we don't sample- we don't do any kind",
    "start": "4417340",
    "end": "4423159"
  },
  {
    "text": "of sampling as part of our analysis. [inaudible].",
    "start": "4423160",
    "end": "4431739"
  },
  {
    "text": "I'm sorry. Can you please repeat that? [inaudible]",
    "start": "4431740",
    "end": "4444730"
  },
  {
    "text": "So what we, um- effectively, what we're doing is we're trying to,",
    "start": "4444730",
    "end": "4449830"
  },
  {
    "text": "um, estimate what Theta hat is. We- we try to come up with an estimate Theta hat that is as close as possible to the true Theta.",
    "start": "4449830",
    "end": "4457270"
  },
  {
    "text": "That's- that's essentially what we're trying to do, right?. The output of our model is always some parameter vector Theta.",
    "start": "4457270",
    "end": "4462415"
  },
  {
    "text": "Like for example, you know, you do linear regression, say use the normal equations, the output of a normal equation is some Theta hat,",
    "start": "4462415",
    "end": "4468565"
  },
  {
    "text": "and that's our estimate of what the slope was. All right? So for logistic regression,",
    "start": "4468565",
    "end": "4475435"
  },
  {
    "text": "um, the picture looks again somewhat similar. So this is x, and assume",
    "start": "4475435",
    "end": "4483920"
  },
  {
    "text": "this is Eta equals Theta transpose x, right?",
    "start": "4484950",
    "end": "4491815"
  },
  {
    "text": "And now we have yet another function, which is the activation function.",
    "start": "4491815",
    "end": "4498760"
  },
  {
    "text": "[NOISE] Oops, I messed this up.",
    "start": "4498760",
    "end": "4506019"
  },
  {
    "text": "Sorry. [NOISE]",
    "start": "4506020",
    "end": "4520750"
  },
  {
    "text": "This is y equals 0 and y equals 1.",
    "start": "4520750",
    "end": "4528970"
  },
  {
    "text": "Right? So for logistic regression, again, um, this is Phi equals- oh, sorry.",
    "start": "4528970",
    "end": "4538670"
  },
  {
    "text": "Equals g of Eta",
    "start": "4540720",
    "end": "4545830"
  },
  {
    "text": "[NOISE] equals 1, right?",
    "start": "4545830",
    "end": "4551995"
  },
  {
    "text": "Again, for logistic regression, you know, assume x is our dat- x is our- our- our data axis.",
    "start": "4551995",
    "end": "4557785"
  },
  {
    "text": "Um, for simplicity, uh, we're just drawing it as one-dimensional, but in general, this is in d-dimensional, right?",
    "start": "4557785",
    "end": "4564310"
  },
  {
    "text": "And Eta equals Theta transpose x is this, you know, hyperplane or,",
    "start": "4564310",
    "end": "4570579"
  },
  {
    "text": "you know, aligned in one dimension, right? And what we then have, uh, in case of linear regression, Eta was Mu,",
    "start": "4570580",
    "end": "4579985"
  },
  {
    "text": "but in case of logistic regression, Phi, that is, you know, the mean, is not equal to Eta but g of Eta,",
    "start": "4579985",
    "end": "4586465"
  },
  {
    "text": "where, you know, it takes this one. You know, this is basically our- our- our logistic regression hypothesis, right?",
    "start": "4586465",
    "end": "4591820"
  },
  {
    "text": "There's also h Theta of x [NOISE]. And now the- the,",
    "start": "4591820",
    "end": "4597730"
  },
  {
    "text": "um- the way the- the- the- the way the data is generated,",
    "start": "4597730",
    "end": "4603730"
  },
  {
    "text": "the way you want to think about it is that for any given value of x, right?",
    "start": "4603730",
    "end": "4608935"
  },
  {
    "text": "Calculate, um, g of Eta. This is the Phi, Phi^i corresponding to x^i.",
    "start": "4608935",
    "end": "4616645"
  },
  {
    "text": "And from this Phi^i, sample a y.",
    "start": "4616645",
    "end": "4621700"
  },
  {
    "text": "Right? Now, if- if it is close to 1, if- if the mean of the Bernoulli is close to 1,",
    "start": "4621700",
    "end": "4628000"
  },
  {
    "text": "then it's more likely that you're going to sample a 1 instead of a 0. So for example here, you might get here.",
    "start": "4628000",
    "end": "4635440"
  },
  {
    "text": "And for this, again, might get here. This might get here, here, here.",
    "start": "4635440",
    "end": "4641245"
  },
  {
    "text": "Maybe, you know, one of them might be here. And similarly, for- for- for,",
    "start": "4641245",
    "end": "4646659"
  },
  {
    "text": "um, low probabilities, uh, for- for those regions of x where g Theta of x will be a low probability,",
    "start": "4646660",
    "end": "4653290"
  },
  {
    "text": "you're mostly gonna get things here, and maybe occasionally one of them will be here.",
    "start": "4653290",
    "end": "4659460"
  },
  {
    "text": "And for the region where your- your, um- um, h Theta of x is close to 0.5, you would expect,",
    "start": "4659460",
    "end": "4666390"
  },
  {
    "text": "you know, roughly an equal number to be, you know, both up and down, right?",
    "start": "4666390",
    "end": "4673675"
  },
  {
    "text": "And from this data generation process, we get a data set that might look like this,",
    "start": "4673675",
    "end": "4681030"
  },
  {
    "text": "x and y. [NOISE]",
    "start": "4681030",
    "end": "4699705"
  },
  {
    "text": "So you might get a dataset that looks like this. Again, this is just visualized in one-dimension.",
    "start": "4699705",
    "end": "4705420"
  },
  {
    "text": "Um, and from this, we want to now work backwards towards, you know,",
    "start": "4705420",
    "end": "4711135"
  },
  {
    "text": "the Theta that gave rise to, you know, the- the- the hyperplane,",
    "start": "4711135",
    "end": "4716729"
  },
  {
    "text": "which then give- gave rise to this logistic function from which the data was chosen from the sample.",
    "start": "4716729",
    "end": "4723730"
  },
  {
    "text": "Right? Does that make sense?",
    "start": "4723830",
    "end": "4727510"
  },
  {
    "text": "From here, we want to then work backwards and ask, what was Theta?",
    "start": "4731720",
    "end": "4738400"
  },
  {
    "text": "So there are any questions on this?",
    "start": "4743540",
    "end": "4748650"
  },
  {
    "text": "Yes?",
    "start": "4748650",
    "end": "4751480"
  },
  {
    "text": "So the [inaudible] So the in- in, uh, I said, um, in case of linear regression,",
    "start": "4759230",
    "end": "4765930"
  },
  {
    "text": "Eta equals Mu precisely because we made the Gaussian assumption. All right, so a few things to kind of,",
    "start": "4765930",
    "end": "4777960"
  },
  {
    "text": "uh, keep in mind, there are lots of parameters moving around here, right?",
    "start": "4777960",
    "end": "4783240"
  },
  {
    "text": "So we have Eta, we have Mu's, we have Phi's, we have Theta's, and it's, you know, it's quite easy to kind of get lost in all these different parameters.",
    "start": "4783240",
    "end": "4794340"
  },
  {
    "text": "And- and so it's- it's- it can be useful to have a big picture of where the different parameters,",
    "start": "4794340",
    "end": "4801930"
  },
  {
    "text": "uh, kind of lie and what role they play and what's their relationship with each other? So we have model parameters, right?",
    "start": "4801930",
    "end": "4819900"
  },
  {
    "text": "Then we have the natural parameter.",
    "start": "4819900",
    "end": "4822820"
  },
  {
    "text": "Then we have mean parameter. Right.",
    "start": "4828800",
    "end": "4837520"
  },
  {
    "text": "So Theta Rd is the model parameter,",
    "start": "4838790",
    "end": "4846460"
  },
  {
    "text": "Eta i is the natural parameter, and the mean parameter is gonna be different for kind of distributions.",
    "start": "4847340",
    "end": "4855750"
  },
  {
    "text": "For example, for this Mu for Gaussian Phi for Bernoulli,",
    "start": "4855750",
    "end": "4866715"
  },
  {
    "text": "Lambda for Poisson,",
    "start": "4866715",
    "end": "4873489"
  },
  {
    "text": "now and whatever it could be.",
    "start": "4875690",
    "end": "4880210"
  },
  {
    "text": "And the choice of",
    "start": "4882710",
    "end": "4888780"
  },
  {
    "text": "the distribution implicitly chooses which mean parameter we are working with, okay?",
    "start": "4888780",
    "end": "4896099"
  },
  {
    "text": "And the relation between the model parameters, which must the- the- the dimension must match with x,",
    "start": "4896100",
    "end": "4904420"
  },
  {
    "text": "the model parameters dotted with x give the natural parameters, right? So Theta transpose x i gives the natural parameters.",
    "start": "4908660",
    "end": "4920370"
  },
  {
    "text": "So that's the relation between your model parameters and the natural parameters, and the natural parameters is 1 per example, right?",
    "start": "4920370",
    "end": "4927795"
  },
  {
    "text": "And then from the natural parameter to the, uh, mean parameters, we have something called the g function.",
    "start": "4927795",
    "end": "4935400"
  },
  {
    "text": "So the g function takes us from natural parameter to the mean parameter.",
    "start": "4935400",
    "end": "4940949"
  },
  {
    "text": "In case of Gaussian g equals, you know, just, you know, g of- in case of Gaussian,",
    "start": "4940950",
    "end": "4952590"
  },
  {
    "text": "g of Eta is just equal to Eta. And- and similarly, in case of, you know,",
    "start": "4952590",
    "end": "4961784"
  },
  {
    "text": "Bernoulli, Phi equals g of Eta equals 1 over 1 plus E to minus Eta, and so on.",
    "start": "4961785",
    "end": "4970785"
  },
  {
    "text": "Right? And this function, that the function g that takes you from",
    "start": "4970785",
    "end": "4975900"
  },
  {
    "text": "the natural parameter to the mean of the distribution, is generally called the canonical response function.",
    "start": "4975900",
    "end": "4982750"
  },
  {
    "text": "And the canonical response function has this property that it will always have an inverse, right?",
    "start": "4994160",
    "end": "5001040"
  },
  {
    "text": "It's always an invertible function. Um, and the inwards, which takes you from the mean back to",
    "start": "5001040",
    "end": "5006890"
  },
  {
    "text": "the natural parameter is generally called the canonical link function. It's suddenly called the canonical response function, because generally,",
    "start": "5006890",
    "end": "5020824"
  },
  {
    "text": "y is also called the response variable sometimes,",
    "start": "5020824",
    "end": "5024840"
  },
  {
    "text": "and so it is taking you from Eta's to like the mean of your y's. So it's- it's also called the canonical response function.",
    "start": "5028000",
    "end": "5035390"
  },
  {
    "text": "And the canonical link function is- is- is basically the, uh, inverse of it. And, uh, the thing you want to keep in mind is, you know,",
    "start": "5035390",
    "end": "5043475"
  },
  {
    "text": "that there are three different kinds of parameters we can talk about.",
    "start": "5043475",
    "end": "5050330"
  },
  {
    "text": "So the first is the Theta's. And when you perform gradient descent to perform maximum likelihood on your Gaussian,",
    "start": "5050330",
    "end": "5058265"
  },
  {
    "text": "or on your generalized linear models, these are the parameters that get trained, right?",
    "start": "5058265",
    "end": "5063425"
  },
  {
    "text": "These are the parameters that we are adjusting in each step of gradient descent or stochastic gradient descent. [NOISE] And once you fix these,",
    "start": "5063425",
    "end": "5071060"
  },
  {
    "text": "the natural parameters are just the output of Theta transpose x for different values of x's.",
    "start": "5071060",
    "end": "5076310"
  },
  {
    "text": "So these are kind of ephemeral in some way, they- you know, we don't store them, whereas, you know, we train and store the, uh,",
    "start": "5076310",
    "end": "5082520"
  },
  {
    "text": "uh, the Theta parameters, right? And the- the natural parameters are just the output for different values of inputs, right?",
    "start": "5082520",
    "end": "5090739"
  },
  {
    "text": "And then you take them through the- the canonical response function and you get the mean of your,",
    "start": "5090740",
    "end": "5096350"
  },
  {
    "text": "you know, y variables. And this is the prediction that we do, right?x So the prediction is- is, uh,",
    "start": "5096350",
    "end": "5103460"
  },
  {
    "text": "is [NOISE] it's Theta of x equal to expectation of y given x Theta.",
    "start": "5103460",
    "end": "5113885"
  },
  {
    "text": "And this was always g of Theta transpose x. [NOISE]",
    "start": "5113885",
    "end": "5137620"
  },
  {
    "text": "Yeah, so uh, are there other kinds of generalized linear models, ah, that's the next, ah,",
    "start": "5137620",
    "end": "5143239"
  },
  {
    "text": "um, thing we're going to discuss. [NOISE] Okay.",
    "start": "5143240",
    "end": "5153090"
  },
  {
    "text": "So -so we to-",
    "start": "5153090",
    "end": "5160469"
  },
  {
    "text": "we saw two examples for generalizing linear models, classification, and regression, and let's see if- if- let's see",
    "start": "5160470",
    "end": "5169110"
  },
  {
    "text": "a few other types of GLMs. [NOISE]",
    "start": "5169110",
    "end": "5185970"
  },
  {
    "text": "So in order to first construct the GLM, right?",
    "start": "5185970",
    "end": "5189790"
  },
  {
    "text": "In order to first construct the GLM, the first thing that we do is first look at our data, right?",
    "start": "5194240",
    "end": "5200760"
  },
  {
    "text": "You have- you're given pairs of x and y's, the first thing you wanna do is look at your y and find out what data type it is.",
    "start": "5200760",
    "end": "5209025"
  },
  {
    "text": "Your y variable might be binary, just 0s and 1s, it can be real valued,",
    "start": "5209025",
    "end": "5215040"
  },
  {
    "text": "which means it can take a value between minus infinity and plus infinity, it can be counts, which means it exists only as integers,",
    "start": "5215040",
    "end": "5222375"
  },
  {
    "text": "like counts of how many clicks is my website going to have or count of number of customers who are gonna walk in through the door, right?",
    "start": "5222375",
    "end": "5228840"
  },
  {
    "text": "It could be just counts. It could be- it could be multiclass,",
    "start": "5228840",
    "end": "5234795"
  },
  {
    "text": "discrete, and- and in the sense if you're trying to, if you're trying to perform classification of whether a given- given image is,",
    "start": "5234795",
    "end": "5243240"
  },
  {
    "text": "I don't know, a cat or a dog or an elephant, right? It can be discrete but not binary,",
    "start": "5243240",
    "end": "5249120"
  },
  {
    "text": "but a set of- set of classes, right? And it can also be just positive and real valued.",
    "start": "5249120",
    "end": "5254744"
  },
  {
    "text": "For example, you know, if the y value is the time until some event happens,",
    "start": "5254745",
    "end": "5260505"
  },
  {
    "text": "then it's real valued but just limited to positive values, right?",
    "start": "5260505",
    "end": "5265650"
  },
  {
    "text": "So the first thing that we wanna do is look at the data type of y,",
    "start": "5265650",
    "end": "5270760"
  },
  {
    "text": "and supposing the datatype of y is R, real-valued,",
    "start": "5273410",
    "end": "5279405"
  },
  {
    "text": "then- then the next thing you wanna",
    "start": "5279405",
    "end": "5289770"
  },
  {
    "text": "do is make a choice about which exponential family distribution you want to",
    "start": "5289770",
    "end": "5295590"
  },
  {
    "text": "use that has a support compatible with your data type. For example, if- if- if- if it is real value from minus infinity to infinity,",
    "start": "5295590",
    "end": "5306254"
  },
  {
    "text": "look up the table of exponential families, say on Wikipedia, Wikipedia is, you know, a page of exponential families,",
    "start": "5306254",
    "end": "5312090"
  },
  {
    "text": "gives you a table of all exponential family distributions. And look at those that have support equal to the full real line,",
    "start": "5312090",
    "end": "5320535"
  },
  {
    "text": "and those are all, you know, eligible options for you, right? So for example, you can choose Gaussian or you can use Laplace, right,",
    "start": "5320535",
    "end": "5331780"
  },
  {
    "text": "and then maybe a few more, okay? But they need to be limited to the exponential family.",
    "start": "5335360",
    "end": "5342150"
  },
  {
    "text": "And when your data type is real-valued, then the thing that we- the name that we give it is regression, right?",
    "start": "5342150",
    "end": "5350050"
  },
  {
    "text": "Similarly, if it is 0 or 1, then we chose Bernoulli and we call it classification.",
    "start": "5353690",
    "end": "5367360"
  },
  {
    "text": "For the choice of Gaussian, we got linear regression as our algorithm.",
    "start": "5369500",
    "end": "5375930"
  },
  {
    "text": "For choice of Laplace, you're gonna get a different algorithm, but it's still a valid algorithm.",
    "start": "5375930",
    "end": "5382650"
  },
  {
    "text": "For Bernoulli, we got logistic regression. Now, It could be 1 through k,",
    "start": "5382650",
    "end": "5393970"
  },
  {
    "text": "and here it can be categorical distribution,",
    "start": "5394640",
    "end": "5400300"
  },
  {
    "text": "and we call that multiclass classification.",
    "start": "5400300",
    "end": "5404670"
  },
  {
    "text": "If it is natural numbers or counts,",
    "start": "5411620",
    "end": "5417060"
  },
  {
    "text": "and one example could be Poisson. This is there in your homework,",
    "start": "5417060",
    "end": "5423120"
  },
  {
    "text": "so in your homework question 4 or question 3 or something, you're going to construct a generalized linear model for Poisson,",
    "start": "5423120",
    "end": "5430380"
  },
  {
    "text": "take a dataset, fit it, make predictions, etc. And you call this either counter regression or Poisson regression.",
    "start": "5430380",
    "end": "5439390"
  },
  {
    "text": "Similarly, if it is positive real values,",
    "start": "5443660",
    "end": "5450960"
  },
  {
    "text": "and specifically if it is like- your y variable is timed to some event, then you have choices like exponential.",
    "start": "5450960",
    "end": "5460305"
  },
  {
    "text": "The exponential distribution is distinct from exponential family, and the exponential distribution is a member of the exponential family.",
    "start": "5460305",
    "end": "5468015"
  },
  {
    "text": "It may be confusing, but this is the exponential distribution not the exponential family.",
    "start": "5468015",
    "end": "5473325"
  },
  {
    "text": "Exponential, or you can use gamma distribution and there are a few more.",
    "start": "5473325",
    "end": "5479085"
  },
  {
    "text": "And generally when you're trying to predict the time to some event, you call it survival analysis.",
    "start": "5479085",
    "end": "5484899"
  },
  {
    "text": "And you can also have your data type to be probability distributions themselves.",
    "start": "5489440",
    "end": "5497475"
  },
  {
    "text": "So this may be a little- little advanced, but if you're doing Bayesian statistics, for example,",
    "start": "5497475",
    "end": "5504300"
  },
  {
    "text": "you can define a probability distribution over Bernoulli distributions,",
    "start": "5504300",
    "end": "5510130"
  },
  {
    "text": "in which case, the exponential family for Bernoulli would be a beta distribution,",
    "start": "5513410",
    "end": "5520875"
  },
  {
    "text": "and you can give it some name. Similarly, you know, your- your- your- your y variable can be a categorical distribution itself, in which case,",
    "start": "5520875",
    "end": "5530955"
  },
  {
    "text": "you want to have a distribution over distributions like for example, the Dirichlet distribution,",
    "start": "5530955",
    "end": "5536025"
  },
  {
    "text": "and all of these can be fit in to the generalized linear model framework, right?",
    "start": "5536025",
    "end": "5541500"
  },
  {
    "text": "You start with what your y variable is, data type of y, and the data type of y is going to",
    "start": "5541500",
    "end": "5550860"
  },
  {
    "text": "inform the choice of distributions that you can use, right? So if somebody- if you're wondering is",
    "start": "5550860",
    "end": "5557820"
  },
  {
    "text": "generalized linear models used for regression or classification, the answer is, it can be used for anything.",
    "start": "5557820",
    "end": "5563535"
  },
  {
    "text": "Just use the appropriate choice of exponential family distribution, okay? A question? Yes.",
    "start": "5563535",
    "end": "5570240"
  },
  {
    "text": "[inaudible].",
    "start": "5570240",
    "end": "5581760"
  },
  {
    "text": "I'm not gonna go too deep into that. You can- you can ask me after the lecture, I can, I can explain more, all right? [NOISE] Any- any- any- any questions?",
    "start": "5581760",
    "end": "5590820"
  },
  {
    "text": "[NOISE] Now. What was the- what did we gain by putting them all in a common framework, right?",
    "start": "5590820",
    "end": "5598470"
  },
  {
    "text": "We saw that, you know, yes, it's kind of mathematically elegant to kind of put them all into a common framework,",
    "start": "5598470",
    "end": "5605070"
  },
  {
    "text": "but what have we gained anything as such, right? The answer is yes, we have actually gained.",
    "start": "5605070",
    "end": "5610840"
  },
  {
    "text": "Once we make a choice of distribution, we can go through the exercise of algebraically massaging it and doing",
    "start": "5612260",
    "end": "5621510"
  },
  {
    "text": "a pattern matching to find out what our G-function is.",
    "start": "5621510",
    "end": "5626804"
  },
  {
    "text": "So first thing you wanna do is make a choice of distribution according to the data type, right? Now-",
    "start": "5626805",
    "end": "5650460"
  },
  {
    "text": "Um, and for that distribution, um, express it in exponential family.",
    "start": "5650460",
    "end": "5656800"
  },
  {
    "text": "Exponential family, which means, you know, find out what is a, what is, you know, a of eta, b of y,",
    "start": "5660350",
    "end": "5668559"
  },
  {
    "text": "T of y. Um, and most importantly, your Mu or Phi or the- the mean parameter,",
    "start": "5669130",
    "end": "5677744"
  },
  {
    "text": "you know, express it as a function of Eta, right? So calculate the g function.",
    "start": "5677745",
    "end": "5683955"
  },
  {
    "text": "Right. That's- that's the most crucial part. Right. And this g function will decide what your hypothesis is.",
    "start": "5683955",
    "end": "5691445"
  },
  {
    "text": "So the hypothesis,",
    "start": "5691445",
    "end": "5693900"
  },
  {
    "text": "h_Theta of x is now g of theta transpose x.",
    "start": "5699230",
    "end": "5707230"
  },
  {
    "text": "Okay. And probably the",
    "start": "5708920",
    "end": "5716730"
  },
  {
    "text": "most- the most useful benefit of going through, uh, of- of building a GLM is that once you do this,",
    "start": "5716730",
    "end": "5724335"
  },
  {
    "text": "in order to perform maximum likelihood, um, uh, estimation, you don't really need to write out the log probability,",
    "start": "5724335",
    "end": "5732389"
  },
  {
    "text": "take the gradient, you know, and- and- and- and, um, derive an update rule. The update rule for all generalized linear models is the same rule.",
    "start": "5732390",
    "end": "5742080"
  },
  {
    "text": "And that- that rule is Theta equals Theta plus some step-sized Alpha",
    "start": "5742080",
    "end": "5748620"
  },
  {
    "text": "times y^i minus h_Theta of x^i times x^i, right?",
    "start": "5748620",
    "end": "5756510"
  },
  {
    "text": "You've seen this update rule over and over. And what you can see is if you start for any generalized linear model,",
    "start": "5756510",
    "end": "5764970"
  },
  {
    "text": "for any choice of the distribution, you take the log probability,",
    "start": "5764970",
    "end": "5770010"
  },
  {
    "text": "perform maximum likelihood estimation to get the gradient, and the update rule that you end up with will always take this form.",
    "start": "5770010",
    "end": "5778110"
  },
  {
    "text": "Right. So in case of logistic regression, your y^i's are 0s and 1s, your h_Theta is- you know,",
    "start": "5778110",
    "end": "5784860"
  },
  {
    "text": "goes according to this, uh, um, um. And for linear regression, your y's could be- could be,",
    "start": "5784860",
    "end": "5791310"
  },
  {
    "text": "you know, a real value. For Poisson regression, they're gonna be counts. But no matter what data type you have,",
    "start": "5791310",
    "end": "5796470"
  },
  {
    "text": "the update rule that you're gonna end up with is always the same. Just that h_Theta of x will be different according to the choice of g. Okay.",
    "start": "5796470",
    "end": "5805395"
  },
  {
    "text": "And finally, for prediction, okay,",
    "start": "5805395",
    "end": "5812415"
  },
  {
    "text": "prediction y hat is always h_Theta of x^star,",
    "start": "5812415",
    "end": "5818280"
  },
  {
    "text": "and this again is equal to g of Theta transpose x^star.",
    "start": "5818280",
    "end": "5824349"
  },
  {
    "text": "So basically, by- by, uh, with GLMs, not only did- do we have a nice and elegant unifying theory,",
    "start": "5824690",
    "end": "5833325"
  },
  {
    "text": "but it also gives you an update rule that you can go directly without writing out the log-likelihood,",
    "start": "5833325",
    "end": "5840120"
  },
  {
    "text": "without taking the gradients. You know, you can just go and start using this update rule and- you know,",
    "start": "5840120",
    "end": "5845730"
  },
  {
    "text": "and run it 'til your model converges. Yes, question. [inaudible]",
    "start": "5845730",
    "end": "5863970"
  },
  {
    "text": "That's a very good point. So, uh, the question is, uh, when we use this y hat, um, according to this, then our- our-",
    "start": "5863970",
    "end": "5871095"
  },
  {
    "text": "our prediction that we are making will not lie in the support of the data type. That- that- that is all- that is true.",
    "start": "5871095",
    "end": "5877335"
  },
  {
    "text": "Um, and for example, for logistic regression, the- the,",
    "start": "5877335",
    "end": "5882510"
  },
  {
    "text": "uh, hypotheses will output a probability value, right? And then it is up to us to choose some kind of a threshold to say,",
    "start": "5882510",
    "end": "5889530"
  },
  {
    "text": "you know, anything above this threshold, we classify it as- as- as- um, um, as positive and anything below,",
    "start": "5889530",
    "end": "5896639"
  },
  {
    "text": "you know, we classify it as negative. And similarly for you- and for Poisson regression, right, your data types are gonna be counts,",
    "start": "5896640",
    "end": "5902985"
  },
  {
    "text": "but the value that comes out of this is gonna be real value, right, again, in which case,",
    "start": "5902985",
    "end": "5908430"
  },
  {
    "text": "you can either round it to the minimum- you know, to the nearest integer, or you can, you know, just sample from the Poisson.",
    "start": "5908430",
    "end": "5914055"
  },
  {
    "text": "You know, that- you- you can- you can do something like that, exactly. Yes, question. [inaudible]",
    "start": "5914055",
    "end": "5927990"
  },
  {
    "text": "This one? No, on the last row. This one? Uh, this- this was just x^star just to say some test example,",
    "start": "5927990",
    "end": "5934485"
  },
  {
    "text": "you know, some unseen example.",
    "start": "5934485",
    "end": "5937090"
  },
  {
    "text": "So, um- yes, question. [inaudible]",
    "start": "5941060",
    "end": "5957570"
  },
  {
    "text": "So the question is, uh, the- the threshold that we wanna choose to decide whether the output of our logistic regression was 1 or 0,",
    "start": "5957570",
    "end": "5964440"
  },
  {
    "text": "can we adjust that threshold? Yes, you can adjust that threshold. It's- it's- it's generally, you know, um, it's generally a post-processing step,",
    "start": "5964440",
    "end": "5971715"
  },
  {
    "text": "which means you- you- you fit the model as it is, and after you fit the model, you know, on a validation set, you can decide what threshold you actually wanna use.",
    "start": "5971715",
    "end": "5978900"
  },
  {
    "text": "You don't always have to use 0.5. You can use a different threshold. [inaudible]",
    "start": "5978900",
    "end": "5984630"
  },
  {
    "text": "It's generally not something that you learn from the training data. So the threshold is not something that you learn from your training data. The threshold is something that you tune on a validation set.",
    "start": "5984630",
    "end": "5992114"
  },
  {
    "text": "And we'll- we'll- we'll probably cover- uh, we're gonna have a lecture on evaluation metrics in, you know, in one of the, uh, um, uh, future lectures,",
    "start": "5992115",
    "end": "5999389"
  },
  {
    "text": "and there we- we're gonna talk about thresholds and- and- and- and things like that. All right. So moving on, um,",
    "start": "5999390",
    "end": "6007310"
  },
  {
    "text": "the last topic, um, I wanna cover today is our softmax regression.",
    "start": "6007310",
    "end": "6013500"
  },
  {
    "text": "Softmax regression. So softmax regression is again, uh, a type of- it- it- it is part of the exponential family,",
    "start": "6020620",
    "end": "6029269"
  },
  {
    "text": "um, or- or a generalized linear model. When you make a choice of, you know, categorical as your- as your choice of exponential family,",
    "start": "6029270",
    "end": "6038240"
  },
  {
    "text": "um, you get multiclass classification, and this is also called softmax regression.",
    "start": "6038240",
    "end": "6043739"
  },
  {
    "text": "So, uh, the lecture notes has, uh, you know, a pretty detailed- uh,",
    "start": "6048640",
    "end": "6053989"
  },
  {
    "text": "um, it walks you step-by-step of how you actually construct, uh, softmax regression as a generalized linear model.",
    "start": "6053990",
    "end": "6060800"
  },
  {
    "text": "And, um, I highly encourage you, you know, you need to kind of go through that in the lecture notes,",
    "start": "6060800",
    "end": "6066560"
  },
  {
    "text": "um, to- to kind of, you know, go through the mechanics of it. Uh, but in- uh, but during the lecture instead,",
    "start": "6066560",
    "end": "6072545"
  },
  {
    "text": "um, I wanna provide some kind of a- um, some intuitions about how softmax regression",
    "start": "6072545",
    "end": "6078350"
  },
  {
    "text": "works so that what you read in the lecture notes, you know, which is pretty heavy on rotation, um, so that it- it makes a little more sense.",
    "start": "6078350",
    "end": "6085520"
  },
  {
    "text": "Okay. So softmax regression is for multiclass classification.",
    "start": "6085520",
    "end": "6090330"
  },
  {
    "text": "And let's start with some pictures. Right. In case of- um,",
    "start": "6096130",
    "end": "6107494"
  },
  {
    "text": "x1, xd, in case of logistic regression, um, we started off with two classes.",
    "start": "6107495",
    "end": "6115860"
  },
  {
    "text": "Let's say this is class 1, and then we have another class over here.",
    "start": "6119830",
    "end": "6127890"
  },
  {
    "text": "Um, minus, minus.",
    "start": "6127960",
    "end": "6132840"
  },
  {
    "text": "These are some examples. Right. Um, in- the- the picture we drew for the generalized, uh, linear model,",
    "start": "6134680",
    "end": "6142114"
  },
  {
    "text": "we just had one- one dimension for, uh, the x-axis and y was on the other dimension.",
    "start": "6142115",
    "end": "6147995"
  },
  {
    "text": "Over here, um, this is x1 and x- x- xd, and the y or the label is basically,",
    "start": "6147995",
    "end": "6155195"
  },
  {
    "text": "you know, embedded in the- in- in the shape or the color over here. Right. Um, and then,",
    "start": "6155195",
    "end": "6161795"
  },
  {
    "text": "um, our- our, uh, goal was to come up with some Theta such that",
    "start": "6161795",
    "end": "6169085"
  },
  {
    "text": "Theta transpose- Theta transpose x equal to 0.",
    "start": "6169085",
    "end": "6177400"
  },
  {
    "text": "This line, that does a separating hyperplane.",
    "start": "6177400",
    "end": "6183054"
  },
  {
    "text": "Right. That was- that was, uh, logistic regression. And Theta transpose x greater than 0 was like the positive classes,",
    "start": "6183055",
    "end": "6193535"
  },
  {
    "text": "and Theta transpose x less than 0 was the negative classes. This assumes that, you know,",
    "start": "6193535",
    "end": "6200510"
  },
  {
    "text": "P of y given x Theta equals 0.5 is the separating hyperplane.",
    "start": "6200510",
    "end": "6207139"
  },
  {
    "text": "So if you assume, you know, 0.5 is- is- is the, uh, separating probability, then, uh, the Theta that you learn, um,",
    "start": "6207140",
    "end": "6214390"
  },
  {
    "text": "has this property that it's- it's the separating hyperplane on one side where it's- it's, uh, greater than 0, you get- you",
    "start": "6214390",
    "end": "6220510"
  },
  {
    "text": "classify it as positive and the other side as negative. Right. Now, what happens in case of multi classes, right?",
    "start": "6220510",
    "end": "6228455"
  },
  {
    "text": "What if we have multiple classes? x1, xd.",
    "start": "6228455",
    "end": "6236780"
  },
  {
    "text": "Okay. Let's assume, um, there's one class over here and another class over here,",
    "start": "6236780",
    "end": "6251110"
  },
  {
    "text": "some examples over here, and let's use another color,",
    "start": "6251110",
    "end": "6257485"
  },
  {
    "text": "and some more examples over here.",
    "start": "6257485",
    "end": "6261350"
  },
  {
    "text": "And let's say these, you know, um, let's say this belongs to y equals 1,",
    "start": "6266710",
    "end": "6274340"
  },
  {
    "text": "y equals 2, and similarly, y equals k. We have k different classes living in a d-dimensional input space, right?",
    "start": "6274340",
    "end": "6284030"
  },
  {
    "text": "And we want to now build a multi-class classifier. So, um, the first thing we want to,",
    "start": "6284030",
    "end": "6292040"
  },
  {
    "text": "um, we want to change from this view is, first we want to figure out what- where is the Theta vector, right?",
    "start": "6292040",
    "end": "6299435"
  },
  {
    "text": "So in this case, the Theta vector will be like this, right? So this is the Theta vector.",
    "start": "6299435",
    "end": "6305180"
  },
  {
    "text": "Because any vector which is less than 90 degrees from Theta will give you a dot-product that's positive.",
    "start": "6305180",
    "end": "6311405"
  },
  {
    "text": "Any- any vector that is, you know, more than 90 degree will give a dot-product that's negative, right?",
    "start": "6311405",
    "end": "6318290"
  },
  {
    "text": "So this is our Theta vector. When we- when we, um, um,",
    "start": "6318290",
    "end": "6325355"
  },
  {
    "text": "move on to multi-class classification, here we will now have one Theta per class, right?",
    "start": "6325355",
    "end": "6333470"
  },
  {
    "text": "So this, I'm going to call it Theta_2, um,",
    "start": "6333470",
    "end": "6338885"
  },
  {
    "text": "this would be Theta_1",
    "start": "6338885",
    "end": "6343909"
  },
  {
    "text": "and [NOISE] let's call this Theta_k.",
    "start": "6343910",
    "end": "6349985"
  },
  {
    "text": "So for every class, you get your own Theta vector, right? And over here it was- it was clear.",
    "start": "6349985",
    "end": "6357769"
  },
  {
    "text": "So if Theta transpose x was, you know, bigger than 0, it was one class, it was less than 0, the other class.",
    "start": "6357770",
    "end": "6363080"
  },
  {
    "text": "But now we have, you know, multiple Theta vectors, and the way we're going to classify something as belonging to",
    "start": "6363080",
    "end": "6371300"
  },
  {
    "text": "one class is to take that input vector x, right?",
    "start": "6371300",
    "end": "6376594"
  },
  {
    "text": "Calculate Theta_1 transpose x,",
    "start": "6376595",
    "end": "6383360"
  },
  {
    "text": "calculate Theta_2 transpose x, and so on until Theta_k transpose x, right?",
    "start": "6383360",
    "end": "6393440"
  },
  {
    "text": "Each is a scalar, and pick the- pick the corresponding k for which",
    "start": "6393440",
    "end": "6402580"
  },
  {
    "text": "the Theta_k transpose x is the largest, right?",
    "start": "6402580",
    "end": "6408185"
  },
  {
    "text": "So each of these is in R, right?",
    "start": "6408185",
    "end": "6414335"
  },
  {
    "text": "And from this what we want to do is to take the max, or rather the R-max,",
    "start": "6414335",
    "end": "6420800"
  },
  {
    "text": "as the- as the- as the predictor class. The intuition to have there is that for the set of examples that belong to a class,",
    "start": "6420800",
    "end": "6434255"
  },
  {
    "text": "the- the- the Theta_k k is- is- is- is kind of pointed in that direction so that",
    "start": "6434255",
    "end": "6443525"
  },
  {
    "text": "only those examples belonging to the class will give a dot-product with this vector to be much larger than their dot-products with the other Thetas, right?",
    "start": "6443525",
    "end": "6454280"
  },
  {
    "text": "For example- for- for- you know, pick this- this blue, right? The dot-product between this example in Theta_k k,",
    "start": "6454280",
    "end": "6462949"
  },
  {
    "text": "because they are, you know, um, um, so well aligned, is going to be much bigger than the dot-product of this with, say, the red.",
    "start": "6462950",
    "end": "6471800"
  },
  {
    "text": "Because in fact, this is greater than 90 degrees, so this will be a negative value, and similarly,",
    "start": "6471800",
    "end": "6476900"
  },
  {
    "text": "the dot-product between this and the black vector will be negative value, but between the blue vector is going to be positive,",
    "start": "6476900",
    "end": "6482120"
  },
  {
    "text": "and so, you know, the- the class k will now test the max, right?",
    "start": "6482120",
    "end": "6489409"
  },
  {
    "text": "And this set of Thetas, we want to write it out in- in a matrix form,",
    "start": "6489410",
    "end": "6495660"
  },
  {
    "text": "right? Where each- each row is d-dimensional and you multiply this with vector x,",
    "start": "6510040",
    "end": "6520640"
  },
  {
    "text": "which is also d-dimensional, right?",
    "start": "6520640",
    "end": "6525775"
  },
  {
    "text": "And this will give you- um, and- and you have, um, k such vectors, right?",
    "start": "6525775",
    "end": "6530930"
  },
  {
    "text": "You have- you have one- one- one Theta vector specific to each class, and you dot it, you get the real value,",
    "start": "6530930",
    "end": "6538235"
  },
  {
    "text": "and for some example, let- let's call it x^i, the different Theta transpose x's might look like this.",
    "start": "6538235",
    "end": "6549510"
  },
  {
    "text": "So for i equals 0, um, for class 0, you might get,",
    "start": "6550210",
    "end": "6556880"
  },
  {
    "text": "um, Theta transpose x. For class 1, the dot-product might be negative,",
    "start": "6556880",
    "end": "6563284"
  },
  {
    "text": "for class two, again, the dot-product might be, you know, positive.",
    "start": "6563285",
    "end": "6568445"
  },
  {
    "text": "And the way softmax regression works is to take the set of these scalar-valued dot-products and convert them into a probability distribution, right?",
    "start": "6568445",
    "end": "6580235"
  },
  {
    "text": "Now, given a set of scalar-valued, um, um, scalar-valued numbers, how do we now convert this into a probability distribution?",
    "start": "6580235",
    "end": "6589775"
  },
  {
    "text": "So for a probability distribution, there are two properties, the first thing is that all the values should be non-negative,",
    "start": "6589775",
    "end": "6597215"
  },
  {
    "text": "and the second property is that they should sum over to 1, right? So the first thing we do is make them all",
    "start": "6597215",
    "end": "6603469"
  },
  {
    "text": "positive while still maintaining the relative orders. And how will you- how- and can you suggest one way how we can- how we",
    "start": "6603470",
    "end": "6613100"
  },
  {
    "text": "can convert all of these into positive values while- while still maintaining the relative orders?",
    "start": "6613100",
    "end": "6618230"
  },
  {
    "text": "[BACKGROUND] You can add a constant, very good. Another way is to? [BACKGROUND] I'm sorry.",
    "start": "6618230",
    "end": "6625550"
  },
  {
    "text": "[BACKGROUND] You can use- the absolute function will make it positive, but they- they- it might not maintain the order.",
    "start": "6625550",
    "end": "6632780"
  },
  {
    "text": "For example, a highly negative one might become, might- might- might become larger than this. [BACKGROUND] You can use a sigmoid function, very good.",
    "start": "6632780",
    "end": "6639275"
  },
  {
    "text": "The sigmoid function will squash everything between 0 and 1. A simple- simple, uh,",
    "start": "6639275",
    "end": "6644780"
  },
  {
    "text": "technique is to just exponentiate them, right? So, um, exponentiate each x^i transpose,",
    "start": "6644780",
    "end": "6657710"
  },
  {
    "text": "um, Theta_i, and this will now make them all positive.",
    "start": "6657710",
    "end": "6664175"
  },
  {
    "text": "So if- if- um, if- the input exponent is minus infinity,",
    "start": "6664175",
    "end": "6669800"
  },
  {
    "text": "it maps it to 0, if it's highly negative, it maps it to a very small positive number, and if the value is positive, it'll make it much,",
    "start": "6669800",
    "end": "6677660"
  },
  {
    "text": "much more positive, right? And then we normalize it.",
    "start": "6677660",
    "end": "6684110"
  },
  {
    "text": "We normalize it such that the heights add up to 1, right?",
    "start": "6684110",
    "end": "6689219"
  },
  {
    "text": "And how do we normalize it? [BACKGROUND] Dividing it by- the integral of this was continuous,",
    "start": "6694720",
    "end": "6701825"
  },
  {
    "text": "in case of discrete, you sum them up and divide them, right? And you get- I'm just going to continue it here,",
    "start": "6701825",
    "end": "6710510"
  },
  {
    "text": "so you get exponent of x transpose Theta_i",
    "start": "6710510",
    "end": "6721770"
  },
  {
    "text": "over summation over j, exponent of x transpose Theta_j,",
    "start": "6722410",
    "end": "6731640"
  },
  {
    "text": "and this will be probability that equals i given x Theta.",
    "start": "6731830",
    "end": "6741305"
  },
  {
    "text": "And this function is called the softmax function, okay? The softmax function takes a set of scalars,",
    "start": "6741305",
    "end": "6749690"
  },
  {
    "text": "real-valued scalars, and converts them into a probability distribution, right?",
    "start": "6749690",
    "end": "6754910"
  },
  {
    "text": "Where the- the larger valued scalars will have a higher probability, the smaller-valued scalars will have a smaller probability,",
    "start": "6754910",
    "end": "6762695"
  },
  {
    "text": "and they all add up to 1. That's the softmax function, right? Any questions?",
    "start": "6762695",
    "end": "6768245"
  },
  {
    "text": "Yes. [BACKGROUND] Can I write a little bigger? Sure. Um, sorry about the writing.",
    "start": "6768245",
    "end": "6774200"
  },
  {
    "text": "So may- maybe I'll just try to write it a little more clearly here. Probability y equals i given x Theta is equal to exponent",
    "start": "6774200",
    "end": "6786814"
  },
  {
    "text": "of x transpose [NOISE] Theta_i divided by the sum over all j,",
    "start": "6786814",
    "end": "6797645"
  },
  {
    "text": "exponent of x transpose Theta_j.",
    "start": "6797645",
    "end": "6803160"
  },
  {
    "text": "Is that clear? All right, so this is- this is basically the softmax function,",
    "start": "6803980",
    "end": "6810530"
  },
  {
    "text": "and the re- reason why it's called softmax function is because, um, if you're given a vector x and, um,",
    "start": "6810530",
    "end": "6818585"
  },
  {
    "text": "you want to find- you want to extract the maximum value or- of, you know, the maximum- the largest component of x.",
    "start": "6818585",
    "end": "6826385"
  },
  {
    "text": "One way to do it is to do x transpose softmax of x,",
    "start": "6826385",
    "end": "6837440"
  },
  {
    "text": "and it'll give you something that is similar to the max, uh, the largest component of x. But- and this is,",
    "start": "6837440",
    "end": "6846594"
  },
  {
    "text": "you know, differentiable, you can do gradient descent, etc. And, uh, you know, it's- it's- it's a differentiable smooth way of",
    "start": "6846595",
    "end": "6854824"
  },
  {
    "text": "extracting the maximum value of- of- of- maximum component of some X. So that's why it's called the softmax.",
    "start": "6854825",
    "end": "6861065"
  },
  {
    "text": "All right, that's- that's about it in terms of, uh, generalized linear models.",
    "start": "6861065",
    "end": "6868690"
  },
  {
    "text": "Uh, this is the big picture you want to have in your mind, and, uh, exponential family, which we already discussed and- and- and generalized linear models.",
    "start": "6868690",
    "end": "6876285"
  },
  {
    "text": "Take exponential families and relate it to some input x with the assumption Eta equals Theta transpose x.",
    "start": "6876285",
    "end": "6881795"
  },
  {
    "text": "That's the big picture. All right. Thank you.",
    "start": "6881795",
    "end": "6884880"
  }
]