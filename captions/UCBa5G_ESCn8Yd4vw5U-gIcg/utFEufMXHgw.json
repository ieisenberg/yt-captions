[
  {
    "start": "0",
    "end": "6050"
  },
  {
    "text": "Good afternoon, CS109. How are you guys doing today? [CHEERS] Oh, fantastic.",
    "start": "6050",
    "end": "11165"
  },
  {
    "text": "That's what I like to hear. I hope you guys had a\nwonderful, wonderful weekend. I was gone for a little bit\nlast week, but I am healthy.",
    "start": "11165",
    "end": "17900"
  },
  {
    "text": "I'm back. I'm here for the\nrest of the quarter-- though, as you\nknow, that's going to include a Thanksgiving\nbreak coming up pretty soon.",
    "start": "17900",
    "end": "24980"
  },
  {
    "text": "Oh, my gosh. Is this one of my\nfavorite lectures ever. You chose a fantastic\nlecture to come to.",
    "start": "24980",
    "end": "30680"
  },
  {
    "text": "It is one of the most\ninteresting lectures in terms of intellectual\nfoundations for great ideas.",
    "start": "30680",
    "end": "36440"
  },
  {
    "text": "It is one of the\nmost useful lectures. We will be building\nmachine learning upon what we learn today. And certainly, you'll\nsee things like this",
    "start": "36440",
    "end": "43760"
  },
  {
    "text": "in the final in your sections. And it also happens to be a\nbit independent of what we've talked about\nbefore, because it's",
    "start": "43760",
    "end": "50180"
  },
  {
    "text": "such an exciting day for CS109. We've done counting theory. We've done core probability.",
    "start": "50180",
    "end": "55579"
  },
  {
    "text": "We talked about\nrandom variables. We talked about lots of\nrandom variables being random together. Then, we had this\nwonderful little aside",
    "start": "55580",
    "end": "61220"
  },
  {
    "text": "where we went to\nuncertainty theory. We learned about great ideas\nin the world of probability, like the central limit\ntheorem, and we're",
    "start": "61220",
    "end": "67649"
  },
  {
    "text": "ready for the final\nsection of CS109, where we take what we've\nlearned about probability,",
    "start": "67650",
    "end": "72750"
  },
  {
    "text": "and we create something\ntruly useful out of it, which has this big, overarching\nlabel of machine learning.",
    "start": "72750",
    "end": "80400"
  },
  {
    "text": "And today is the first\nclass of that new section. How exciting. So today is the day where we\nbridge from probability theory",
    "start": "80400",
    "end": "87420"
  },
  {
    "text": "into artificial intelligence. Sound good? Yeah. Let's go do it.",
    "start": "87420",
    "end": "92485"
  },
  {
    "text": "Let's go learn some things. OK. So machine learning-- it's\na story of today's class.",
    "start": "92485",
    "end": "97710"
  },
  {
    "text": "And because I was in kind\nof, like, a Disney mood when I made this, we're going to\nuse Lion King as our narrative",
    "start": "97710",
    "end": "103980"
  },
  {
    "text": "for today. So we're going to be telling you\nthis wonderful story of machine learning.",
    "start": "103980",
    "end": "109920"
  },
  {
    "text": "On some level,\nthe questions that are posed by machine\nlearning have already showed up in CS109.",
    "start": "109920",
    "end": "116017"
  },
  {
    "text": "Remember when we did WebMD\nHealth, where we were talking about probabilistic models? We ended up with\nthese wonderful things",
    "start": "116017",
    "end": "121680"
  },
  {
    "text": "called Bayesian networks. And we got to the\npoint where if you knew all of these\nconditional probabilities,",
    "start": "121680",
    "end": "128082"
  },
  {
    "text": "you could use general\ninference, and you could answer any probability\nquestion you cared about",
    "start": "128083",
    "end": "133230"
  },
  {
    "text": "from this model. But there is this one\nquestion we've never answered,",
    "start": "133230",
    "end": "138900"
  },
  {
    "text": "which is, where do\nthose numbers come from? And a few times in class,\nI alluded to, oh, we",
    "start": "138900",
    "end": "144510"
  },
  {
    "text": "can learn them from data. And that's really what the\nfocus of machine learning is. Oh.",
    "start": "144510",
    "end": "149780"
  },
  {
    "text": "Ooh. Suspense. What's machine learning?",
    "start": "149780",
    "end": "154830"
  },
  {
    "text": "At this point in\nCS109, if you're given a model with all the\nprobabilities necessary,",
    "start": "154830",
    "end": "160080"
  },
  {
    "text": "you can make predictions,\nwhich is fantastic. That's a really important\nthing to know how to do. But what if somebody\ndoesn't just",
    "start": "160080",
    "end": "166530"
  },
  {
    "text": "give you a model with\nall the numbers required? What would you have to do?",
    "start": "166530",
    "end": "171870"
  },
  {
    "text": "You might have to actually learn\nall the numbers in your model. And this simple\nproblem turns out",
    "start": "171870",
    "end": "178830"
  },
  {
    "text": "to be so deep and\ncomplicated, it is the heart and soul of\nartificial intelligence. I want to lightly\npoint out there",
    "start": "178830",
    "end": "184650"
  },
  {
    "text": "is another problem,\nwhich is, can we also learn the structure of\na probabilistic model",
    "start": "184650",
    "end": "189724"
  },
  {
    "text": "from data, too? And I just want to lightly\nnote that that's so cool, but I'm going to\ntry and, you know, limit what I teach you in CS109.",
    "start": "189725",
    "end": "196488"
  },
  {
    "text": "If it was up to me,\nI would teach you everything I possibly know. But it's nice to have\na self-contained class.",
    "start": "196488",
    "end": "201940"
  },
  {
    "text": "So we're just going to focus\non machine learning, and not structure learning. And that simple problem--",
    "start": "201940",
    "end": "208780"
  },
  {
    "text": "where do numbers in\nmodels come from-- is the heart and soul of this\nthing called machine learning.",
    "start": "208780",
    "end": "213820"
  },
  {
    "text": "Now, machine learning,\nas you may know, is related to a few other terms.",
    "start": "213820",
    "end": "219610"
  },
  {
    "text": "Machine learning is one way of\ndoing artificial intelligence. Artificial intelligence--\nmaking programs",
    "start": "219610",
    "end": "224620"
  },
  {
    "text": "that do something\nslightly interesting. And then machine\nlearning says, I'm going to learn how\nto do something",
    "start": "224620",
    "end": "229630"
  },
  {
    "text": "slightly interesting by\nlearning parameters from data. I do want to lightly\nnote that you",
    "start": "229630",
    "end": "235110"
  },
  {
    "text": "might have heard of a\nthing called deep learning. And deep learning\nis a particular type of machine learning for a\nparticular type of model.",
    "start": "235110",
    "end": "242010"
  },
  {
    "text": "But it all falls under this big\numbrella of machine learning.",
    "start": "242010",
    "end": "248040"
  },
  {
    "text": "And this is what the rest of\nCS109 is going to look like. We will teach you deep\nlearning, but before you",
    "start": "248040",
    "end": "254010"
  },
  {
    "text": "learn deep learning,\nyou need to know two of the most classic\nalgorithms in machine",
    "start": "254010",
    "end": "259528"
  },
  {
    "text": "learning, which are called naive\nBayes and logistic regression. And in order to get there,\nwe need to build you",
    "start": "259529",
    "end": "265560"
  },
  {
    "text": "a theoretical foundation. And this theoretical\nfoundation has a name. It's called\nparameter estimation.",
    "start": "265560",
    "end": "271170"
  },
  {
    "text": "And it's a nice,\nunassuming name that covers a lot of beautiful theory.",
    "start": "271170",
    "end": "277500"
  },
  {
    "text": "I'm going to teach you three\nways that you could do machine learning, this\nfoundational theory for how",
    "start": "277500",
    "end": "284400"
  },
  {
    "text": "you get parameters from\ndata, and then we'll build these two\nalgorithms on top of it--",
    "start": "284400",
    "end": "289410"
  },
  {
    "text": "unbiased estimators, a thing\ncalled maximum likelihood, and Bayesian estimation. Sound good?",
    "start": "289410",
    "end": "297090"
  },
  {
    "text": "Oh, let's do it. I do want to have a quick aside. Hey, can't we just, like,\njump into using TensorFlow",
    "start": "297090",
    "end": "303440"
  },
  {
    "text": "and start doing deep\nlearning right now? On some level, you could. Every single person\nin this room.",
    "start": "303440",
    "end": "308600"
  },
  {
    "text": "If you wanted to just go pull\nup the most modern library and train a deep\nlearning algorithm, it wouldn't be that hard.",
    "start": "308600",
    "end": "315020"
  },
  {
    "text": "The hard part would be is if you\ntried to do something creative on top of it. If you wanted to\ndebug it, or if you",
    "start": "315020",
    "end": "320870"
  },
  {
    "text": "wanted to invent the next\nstep of machine learning. And for lots of reasons,\nwe think that there",
    "start": "320870",
    "end": "325970"
  },
  {
    "text": "are going to be many new steps. And in CS109, the idea\nI would like to give you is the theory\nbehind what happens",
    "start": "325970",
    "end": "333170"
  },
  {
    "text": "in something like TensorFlow. And it's not because I\njust, like, love theory, and I think that's the only\nthing that's important,",
    "start": "333170",
    "end": "338840"
  },
  {
    "text": "but because the\ntechnologies is on top of the theory are moving\nquite fast, knowing the theory will help you\ninvent the new technologies.",
    "start": "338840",
    "end": "345530"
  },
  {
    "text": "It will also help\nyou stay relevant as the technologies grow. So while it would be really\ncool to download TensorFlow,",
    "start": "345530",
    "end": "351380"
  },
  {
    "text": "I think it's really\ncritical that we all learn the beautiful\nmathematics that go into this great idea\nthat has changed the world.",
    "start": "351380",
    "end": "359000"
  },
  {
    "text": "Do I have you hooked? OK. Let's go do this. Oh.",
    "start": "359000",
    "end": "365000"
  },
  {
    "text": "Just to give you a sense of\nsome of the open problems, machine learning, what\nwe're going to learn about,",
    "start": "365000",
    "end": "370790"
  },
  {
    "text": "traditionally uses\na lot of data. So if you say, hey\ncomputer, learn about the concept of\na chair, computer's",
    "start": "370790",
    "end": "376297"
  },
  {
    "text": "like, I've got you if you give\nme, like, a million examples. And we're, like, OK. Here's a million of\nexamples of a chair.",
    "start": "376297",
    "end": "381530"
  },
  {
    "text": "And then the computer\nlearns what a chair is. But humans are so much smarter. And we've talked\nabout this before.",
    "start": "381530",
    "end": "387000"
  },
  {
    "text": "If I told you,\nthere's a new symbol. You've never seen it before. Do you need a million\nexamples to start recognizing",
    "start": "387000",
    "end": "392270"
  },
  {
    "text": "other versions of the symbol? No. You guys can learn after just\none single training example.",
    "start": "392270",
    "end": "398629"
  },
  {
    "text": "How clever are you? And this tells you the story of\nmachine learning is not done.",
    "start": "398630",
    "end": "406610"
  },
  {
    "text": "There is room for improvement. There are open problems\nfor you all to solve.",
    "start": "406610",
    "end": "412450"
  },
  {
    "text": "So let's learn some theory. Oh, my segue. OK.",
    "start": "412450",
    "end": "417630"
  },
  {
    "text": "And I will note that\nthe open problems that exist in machine\nlearning-- they're especially true for\nthe human problems",
    "start": "417630",
    "end": "423657"
  },
  {
    "text": "that you might care about. For the places where artificial\nintelligence interacts with needs of humans,\nthere's a lot of problems",
    "start": "423657",
    "end": "430860"
  },
  {
    "text": "that people haven't\nreally thought of, and there's a lot of\nopen opportunities for some advancement. So you guys want to push\non these grand challenges?",
    "start": "430860",
    "end": "438393"
  },
  {
    "text": "Let's understand the theory. You want to understand how\nthis thing is really working? Like, when people say artificial\nintelligence, what they really",
    "start": "438393",
    "end": "443729"
  },
  {
    "text": "mean? You want to demystify that? Let's learn the theory. And so our movie begins.",
    "start": "443730",
    "end": "448889"
  },
  {
    "text": "[HUMS DRAMATICALLY] We're going to learn\nparameter estimation. ",
    "start": "448890",
    "end": "455320"
  },
  {
    "text": "Parameter estimation\nis a simple idea. We've seen lots of\nprobability distributions.",
    "start": "455320",
    "end": "461040"
  },
  {
    "text": "And often, those\nprobability distributions, whether they have single random\nvariables or multiple ones,",
    "start": "461040",
    "end": "466530"
  },
  {
    "text": "will have these special numbers. And these special numbers tell\nyou what the distribution is.",
    "start": "466530",
    "end": "472750"
  },
  {
    "text": "The special numbers have a name. They are called parameters. So we call these\nparametric models,",
    "start": "472750",
    "end": "478870"
  },
  {
    "text": "because if you wanted to\ndefine a whole normal, you don't have to describe\nevery single point in that bell",
    "start": "478870",
    "end": "484639"
  },
  {
    "text": "curve. You just have to\ngive me two numbers-- these two parameters. If you want to\ndescribe a Poisson,",
    "start": "484640",
    "end": "489880"
  },
  {
    "text": "you don't have to describe every\nsingle point in the probability mass function of a Poisson. You would just\ngive me the lambda.",
    "start": "489880",
    "end": "496330"
  },
  {
    "text": "So any model which is\ndefined by numbers, we call parametric\nmodels, and the parameters",
    "start": "496330",
    "end": "502090"
  },
  {
    "text": "are where the real\nmodel comes from. So I can tell you that\nsomething's Poisson.",
    "start": "502090",
    "end": "508169"
  },
  {
    "text": "But if you really\nwant to solve problems using that random\nvariable, you'd need to know what lambda was.",
    "start": "508170",
    "end": "513840"
  },
  {
    "text": "And I can tell you I have a\nwhole probabilistic model. But if you don't have values\nfor all of your parameters,",
    "start": "513840",
    "end": "519150"
  },
  {
    "text": "then it won't be able to do\nvery many interesting things for you. A little bit of notation. We're going to now\nstart referring",
    "start": "519150",
    "end": "525000"
  },
  {
    "text": "to all of these different\nthings as parameters, and we're going to start using\nthe symbol theta to represent",
    "start": "525000",
    "end": "531060"
  },
  {
    "text": "parameters in general. Does that sound good? Often, theta will be just,\nlike, a single parameter.",
    "start": "531060",
    "end": "537060"
  },
  {
    "text": "But sometimes\nmodels like a normal might have more than\none number, and so you can think of theta as a vector.",
    "start": "537060",
    "end": "543879"
  },
  {
    "text": "Just to be clear, in this\nterminology of parameters-- because we're going to\nbe learning parameter estimation-- in our\nbeautiful, Bayesian networks,",
    "start": "543880",
    "end": "550840"
  },
  {
    "text": "there is a lot of numbers. And those numbers-- those\nare all parameters, too. And the name of the\ngame for today's class",
    "start": "550840",
    "end": "557620"
  },
  {
    "text": "is getting all of these\nparameters from data. Cool? Rocking.",
    "start": "557620",
    "end": "562690"
  },
  {
    "text": "Any questions on terminology? And this fits the general\nmachine learning format.",
    "start": "562690",
    "end": "569310"
  },
  {
    "text": "The general machine learning\nformat has three major stages. The first major stage is you\nhave a real world problem,",
    "start": "569310",
    "end": "576389"
  },
  {
    "text": "and you model it. You say, I think\nthis is a Poisson, or I have a Bayesian\nnetwork that's",
    "start": "576390",
    "end": "581400"
  },
  {
    "text": "going to represent this\nproblem, and you end up with a formal model. But that formal\nmodel has a bunch",
    "start": "581400",
    "end": "586800"
  },
  {
    "text": "of numbers left to be\nfilled in-- these numbers that we call parameters. So that's stage one\nin machine learning.",
    "start": "586800",
    "end": "592800"
  },
  {
    "text": "Stage two in machine\nlearning is, OK. Somebody just gave you a model. They say that there's\ngoing to be numbers,",
    "start": "592800",
    "end": "598920"
  },
  {
    "text": "but we don't know what\nthose numbers are. But they also give\nyou training data.",
    "start": "598920",
    "end": "604080"
  },
  {
    "text": "And from this\ntraining data, we're going to choose really\nwonderful values of thetas so that we're end up with a\nmodel filled with numbers,",
    "start": "604080",
    "end": "612480"
  },
  {
    "text": "and then we can answer\nprobability questions. So that second-stage\nparameter estimation",
    "start": "612480",
    "end": "618449"
  },
  {
    "text": "has another name\ncalled training, which you might sometimes\nhear, because it's",
    "start": "618450",
    "end": "623520"
  },
  {
    "text": "getting its estimate for the\nparameters from training data. OK. So here's our path.",
    "start": "623520",
    "end": "629949"
  },
  {
    "text": "And I have wild\nnews for you guys. We need to be able to\nestimate these parameters,",
    "start": "629950",
    "end": "636000"
  },
  {
    "text": "but we already have some methods\nfor estimating parameters. And I just want to revisit\nthem really quickly before we jump into\nmore general methods.",
    "start": "636000",
    "end": "643912"
  },
  {
    "text": "The first way we have\nfor estimating parameters is called unbiased estimators. So if I told you we\nhave a bunch of data,",
    "start": "643912",
    "end": "652470"
  },
  {
    "text": "and they're all IID, because\nof the central limit theorem, we know that this is\nalways the sample mean.",
    "start": "652470",
    "end": "658710"
  },
  {
    "text": "And we derived earlier that\nthe sample variance also has a closed form equation.",
    "start": "658710",
    "end": "664830"
  },
  {
    "text": "This applies for any data set. So if you have a\ndata set, we already have a method for estimating\nthe mean and the variance,",
    "start": "664830",
    "end": "671720"
  },
  {
    "text": "which means if, like,\nyou had to fit a Gaussian and you just had\ndata points, you kind of have a method for\nalready estimating parameters.",
    "start": "671720",
    "end": "679180"
  },
  {
    "text": "It's not very general. It'd be hard to\nimagine how you could use this for, like,\nyour Bayesian network.",
    "start": "679180",
    "end": "685149"
  },
  {
    "text": "But in some sense, we do already\nhave some really basic tools for estimating\nparameters from data.",
    "start": "685150",
    "end": "693310"
  },
  {
    "text": "We call these our\nunbiased estimators. And that's just because\nin expectation, this will be the right number.",
    "start": "693310",
    "end": "698770"
  },
  {
    "text": "It won't always be\nthe right average, but in expectation, it\nwill be the true average. This number, your\nguess for variance,",
    "start": "698770",
    "end": "705820"
  },
  {
    "text": "won't always be the right\nvariance, but | expectation, it will be the right value.",
    "start": "705820",
    "end": "711259"
  },
  {
    "text": "And these just leave\nso much to be desired. They just don't solve parameter\nestimation in general.",
    "start": "711260",
    "end": "716329"
  },
  {
    "text": "What we're going to\nlearn about today is maximum likelihood\nestimation-- the theory behind\na general method",
    "start": "716330",
    "end": "722740"
  },
  {
    "text": "for choosing numbers in a model. OK. ",
    "start": "722740",
    "end": "729810"
  },
  {
    "text": "And to drive that\nhome, there's lots of different algorithms--\nor things I could show you,",
    "start": "729810",
    "end": "735090"
  },
  {
    "text": "which are hard to fit. And I do want to pull\nup our course reader. By the way, we've been writing\na whole bunch of new things",
    "start": "735090",
    "end": "740490"
  },
  {
    "text": "in the course reader. There's a whole bunch\nof new examples. But you know, one\nexample of something that would be really hard to fit\nusing unbiased estimators would",
    "start": "740490",
    "end": "748260"
  },
  {
    "text": "be this new model I give you\ncalled a mixture of Gaussians. It says there's some Gaussian\nA and some Gaussian B,",
    "start": "748260",
    "end": "754140"
  },
  {
    "text": "and every data point is more\nlikely to come from B than A. And altogether, this leads to\none probability distribution.",
    "start": "754140",
    "end": "761130"
  },
  {
    "text": "I know that there's\nthese five parameters. I can tell you how\nthey're related, but you have to\nguess the numbers.",
    "start": "761130",
    "end": "766720"
  },
  {
    "text": "This is very difficult\nusing something like unbiased estimators. So we need a more general tool.",
    "start": "766720",
    "end": "773380"
  },
  {
    "text": "So enter this great idea\nin machine learning. And to do that, I want\nto go back to a Gaussian.",
    "start": "773380",
    "end": "780700"
  },
  {
    "text": "I want to go back to the problem\nof estimating parameters. I don't want to use\nunbiased estimators,",
    "start": "780700",
    "end": "786220"
  },
  {
    "text": "but I want to show you\na demo which highlights a new way of thinking. And this new way of thinking\nwill give us a general way",
    "start": "786220",
    "end": "792610"
  },
  {
    "text": "for doing parameter estimation. Ready? Let's do it.",
    "start": "792610",
    "end": "797930"
  },
  {
    "text": "I have a classic reversion of\nparameter estimation problem",
    "start": "797930",
    "end": "802940"
  },
  {
    "text": "for you. I give you all of\nthese data points.",
    "start": "802940",
    "end": "808410"
  },
  {
    "text": "I give you a model. I say that I think these data\npoints come from a Gaussian. You now need to do\nparameter estimation, which",
    "start": "808410",
    "end": "815700"
  },
  {
    "text": "means you have to\ngive me a mean and you have to give me a variance. You have to give me the\nnumbers of the parameters.",
    "start": "815700",
    "end": "821370"
  },
  {
    "text": "We already have a\nway of doing that, but I want to solve this problem\nusing a different method.",
    "start": "821370",
    "end": "827140"
  },
  {
    "text": "What I'd like to do is I'd\nlike to focus on a simple idea.",
    "start": "827140",
    "end": "837480"
  },
  {
    "text": "See, up here, I have different\nvalues of these parameters. And in this graph, I do\nsomething interesting.",
    "start": "837480",
    "end": "844529"
  },
  {
    "text": "These vertical lines\nare all the data points that you're trying to come up\nwith parameters to describe.",
    "start": "844530",
    "end": "849779"
  },
  {
    "text": "Is that making sense? And here are guesses\nfor mean and variance.",
    "start": "849780",
    "end": "855660"
  },
  {
    "text": "I calculate this expression\ncalled likelihood. And likelihood just says,\nif this was the true mean",
    "start": "855660",
    "end": "863130"
  },
  {
    "text": "and this was the true variance,\nwhat's the probability of you seeing each of these points? So what's the probability\nof seeing this point",
    "start": "863130",
    "end": "869940"
  },
  {
    "text": "under those parameters times\nthis point, times this point, times this point,\ntimes this point?",
    "start": "869940",
    "end": "875670"
  },
  {
    "text": "And the crazy idea is, I'm\ngoing to let people play around with these numbers,\nand we're going to watch likelihood\ngo up or down.",
    "start": "875670",
    "end": "882579"
  },
  {
    "text": "So if these are our data\npoints, this blue line is parameter mean equals 5 and\nstandard deviation equals 3.",
    "start": "882580",
    "end": "891240"
  },
  {
    "text": "Do you guys want to see me\nmake standard deviation bigger or smaller? Smaller.",
    "start": "891240",
    "end": "896680"
  },
  {
    "text": "OK. Watch what happens\nwhen I make it smaller. See that likelihood expression? It's currently 4 times\n10 to the negative 19th.",
    "start": "896680",
    "end": "905090"
  },
  {
    "text": "Whoa. It changed a lot. Did it get bigger or smaller?",
    "start": "905090",
    "end": "910709"
  },
  {
    "text": "Bigger. Yeah. It got bigger. So it used to be 4\ntimes 10 to the 19th. Now it's almost 7\ntimes 10 to the 19th.",
    "start": "910710",
    "end": "917190"
  },
  {
    "text": "It almost doubled in likelihood. There's still a\nvery small number, but that very small\nnumber got bigger.",
    "start": "917190",
    "end": "923160"
  },
  {
    "text": "So that's saying if you\nhad those parameters, your data looks more likely. It's more probable\nthat you would",
    "start": "923160",
    "end": "928770"
  },
  {
    "text": "have seen the data that you\nsaw under these parameters. Doesn't say that these\nare the best parameters. It's just more likely than\nthe other ones we tried.",
    "start": "928770",
    "end": "935493"
  },
  {
    "text": "But we can try other ones. Do you guys want to make this\nvariance bigger or smaller? Smaller.",
    "start": "935493",
    "end": "940930"
  },
  {
    "text": "OK. And we can keep going. And I'm going to keep going. And I'm going to stop when the\nlikelihood starts to go down.",
    "start": "940930",
    "end": "947170"
  },
  {
    "text": "So it went up. It went up. It went up.",
    "start": "947170",
    "end": "952260"
  },
  {
    "text": "It went up. Oh, this is very exciting. It went up.",
    "start": "952260",
    "end": "957330"
  },
  {
    "text": "It went up. Oh. Definitely better with\nsmaller variances.",
    "start": "957330",
    "end": "963290"
  },
  {
    "text": "Oh, this is great. Will it just keep going up? No, no, no.",
    "start": "963290",
    "end": "968510"
  },
  {
    "text": "It's no longer going up. But you know, maybe I haven't\nchanged this parameter. I can change this one. Oh, that got worse.",
    "start": "968510",
    "end": "975180"
  },
  {
    "text": "Oh, that got better. It got better when I made\nthe mean a little bit higher. Oh, fantastic. No.",
    "start": "975180",
    "end": "980580"
  },
  {
    "text": " OK. I like those ones.",
    "start": "980580",
    "end": "987763"
  },
  {
    "text": "Did you see what I did here? What a funny way of choosing\na mean and variance. So what did I do here?",
    "start": "987763",
    "end": "993340"
  },
  {
    "text": "I said, I'm going to\njust twiddle around with these numbers\nuntil the data starts to look more likely.",
    "start": "993340",
    "end": "1000089"
  },
  {
    "text": "And to be clear, the\nlikelihood in this situation-- I'm going to use\nL for likelihood--",
    "start": "1000090",
    "end": "1006660"
  },
  {
    "text": "is just the product\nover every single data point, the probability density\nfunction of that data point,",
    "start": "1006660",
    "end": "1015320"
  },
  {
    "text": "in the world where you have\nthat particular mean equals",
    "start": "1015320",
    "end": "1020840"
  },
  {
    "text": "5.5 and the standard\ndeviation equals 1.1.",
    "start": "1020840",
    "end": "1031890"
  },
  {
    "text": "So it's just saying, you know,\nevery single data point-- look at its probability\ndensity function and multiply it together.",
    "start": "1031890",
    "end": "1037990"
  },
  {
    "text": "And then my crazy\nidea was, we're just going to twiddle around the\nvalues of the parameters",
    "start": "1037990",
    "end": "1043589"
  },
  {
    "text": "until this goes up the most. Crazy? Did you guys buy\nthat that would work?",
    "start": "1043589",
    "end": "1049860"
  },
  {
    "text": "Did you know that most AI has a\nhuman being behind the screens, and they're just,\nlike, twiddling numbers",
    "start": "1049860",
    "end": "1055260"
  },
  {
    "text": "until likelihood goes up? There's a secret, like--\ninside your smartwatch, there's actually a tiny\nlittle person who's being, like, yeah, actually,\nlet's make variants go up",
    "start": "1055260",
    "end": "1061710"
  },
  {
    "text": "a little bit. No. That's not how it works. OK. So this is a really cool idea. This is a powerful idea, which\nis that for any model you have,",
    "start": "1061710",
    "end": "1069630"
  },
  {
    "text": "one very reasonable\ntheme for how you could choose\nyour parameters is choose whichever\nparameters makes",
    "start": "1069630",
    "end": "1076889"
  },
  {
    "text": "the data look pretty likely. And we haven't talked about\nhow a computer could do this",
    "start": "1076890",
    "end": "1082049"
  },
  {
    "text": "automatically. We've just given you\na perspective on how we could choose parameters.",
    "start": "1082050",
    "end": "1088110"
  },
  {
    "text": "And that perspective-- I like\nto think of this metaphor of, there's this big sound board.",
    "start": "1088110",
    "end": "1094500"
  },
  {
    "text": "And you're, like,\na sound engineer. And when you're\nchoosing parameters, you're just trying to\nchoose the values that",
    "start": "1094500",
    "end": "1100500"
  },
  {
    "text": "make your data look\nas likely as possible in the world of your model. And you can twiddle those\nsliders a little bit--",
    "start": "1100500",
    "end": "1107190"
  },
  {
    "text": "and each slider is\na parameters value-- until it's, like,\nthe perfect sound. And by \"perfect\nsound,\" I mean it",
    "start": "1107190",
    "end": "1113110"
  },
  {
    "text": "makes the data look so likely. Did you guys-- oh.",
    "start": "1113110",
    "end": "1119030"
  },
  {
    "text": "OK. The first insight, then,\nthat we've got so farr-- and this is a critically\ncool idea in the world",
    "start": "1119030",
    "end": "1125050"
  },
  {
    "text": "of probability-- is that we should find\nwhich parameters maximize",
    "start": "1125050",
    "end": "1130340"
  },
  {
    "text": "a measure of likelihood. And that's going to require\nus to think about arguments",
    "start": "1130340",
    "end": "1136100"
  },
  {
    "text": "that maximize something else. I want to think\nabout which values of mean and standard\ndeviation maximize this thing.",
    "start": "1136100",
    "end": "1143300"
  },
  {
    "text": "Arguments that maximize. Have you guys ever seen\na thing called argmax. If not, what a beautiful\nday in your lives.",
    "start": "1143300",
    "end": "1149510"
  },
  {
    "text": "We're going to be\ntalking about argmax. First of all, this is\na general conversation.",
    "start": "1149510",
    "end": "1155659"
  },
  {
    "text": "We'll take a step\nback from probability. Let me give you a function. This function is negative\nx squared plus 5.",
    "start": "1155660",
    "end": "1162260"
  },
  {
    "text": "And I have so helpfully\ndrawn our function. Oh, what a cute little function. It looks like a rocket ship.",
    "start": "1162260",
    "end": "1168560"
  },
  {
    "text": "First of all, have you guys\never seen a max function?",
    "start": "1168560",
    "end": "1173810"
  },
  {
    "text": "A max function can\ntake in an expression, and it will tell you\nwhat's the largest value",
    "start": "1173810",
    "end": "1180620"
  },
  {
    "text": "that this expression can take. So think about it for a second. I'm going to have\nyou shout it out.",
    "start": "1180620",
    "end": "1187029"
  },
  {
    "text": "What is the maximum value that\nthis expression can take on? And you can look at\nmy handy little chart.",
    "start": "1187030",
    "end": "1193210"
  },
  {
    "text": "Everybody think about it. 3, 2, 1. ALL together. 5.",
    "start": "1193210",
    "end": "1198520"
  },
  {
    "text": "I feel like that was\nkind of, like, a D-flat. But can we do that\nin, like, a C? Like, 5.",
    "start": "1198520",
    "end": "1204520"
  },
  {
    "text": "1, 2, 3-- 5. No. 5.",
    "start": "1204520",
    "end": "1209970"
  },
  {
    "text": "5. That was getting there. Let's see if we can\nnail it with the argmax. Now, max is a thing that\nwe all feel comfortable.",
    "start": "1209970",
    "end": "1217120"
  },
  {
    "text": "It's the largest value\nthat this can take on. And we've decided\nthat this is a 5. The argmax is a\ndifferent concept.",
    "start": "1217120",
    "end": "1224740"
  },
  {
    "text": "It doesn't care about\nthe largest value. It asks the question, what input\nleads to the largest value?",
    "start": "1224740",
    "end": "1231185"
  },
  {
    "text": "So if I took that\nexact same expression and instead asked, what is the\nargument that maximizes it,",
    "start": "1231185",
    "end": "1236370"
  },
  {
    "text": "I will get a different answer. And in C, in 3, 2, 1--",
    "start": "1236370",
    "end": "1241550"
  },
  {
    "text": "Zero. Zero. OK. 1, 2, 3--",
    "start": "1241550",
    "end": "1247010"
  },
  {
    "text": "Zero. OK. It's getting better. But we should definitely\nbe sticking to probability. [LAUGHTER]",
    "start": "1247010",
    "end": "1252740"
  },
  {
    "text": "I kid. I kid. So the difference\nbetween 5 and zero here is 5 is the largest\nvalue you can take on,",
    "start": "1252740",
    "end": "1257930"
  },
  {
    "text": "and zero is the input\nwhich maximizes it. And this is a very\nimportant concept, because we're not\nreally interested in,",
    "start": "1257930",
    "end": "1264770"
  },
  {
    "text": "how likely is our data? What we're really\ninterested in is, which input of mean\nand standard deviation",
    "start": "1264770",
    "end": "1271460"
  },
  {
    "text": "made the data look as likely? And that will be an argmax. A beautiful piece of theory that\nyou must know about the argmax",
    "start": "1271460",
    "end": "1279740"
  },
  {
    "text": "is, what happens if\nyou take the argmax of the log of a function? So imagine you cared about\nthe argmax of a function.",
    "start": "1279740",
    "end": "1287929"
  },
  {
    "text": "Mind-blowing claim for you. Argmax of a function\nis the exact same",
    "start": "1287930",
    "end": "1294830"
  },
  {
    "text": "as argmax of a\nlog of a function. You're, like, what? No. It can't be the same. They're close, right?",
    "start": "1294830",
    "end": "1300560"
  },
  {
    "text": "No. They're not just close. They're the same. So if you take your\nfunction from before--",
    "start": "1300560",
    "end": "1307070"
  },
  {
    "text": "so if I said, what is the argmax\nof negative x squared plus 5?",
    "start": "1307070",
    "end": "1313929"
  },
  {
    "text": "It Is-- Zero? Zero. Yeah. I'm giving up my whole C thing.",
    "start": "1313930",
    "end": "1320710"
  },
  {
    "text": "And how about, what's\nthe argmax of this? Zero. Zero.",
    "start": "1320710",
    "end": "1326210"
  },
  {
    "text": "That is my claim for you, is\nthat if you take any function, the argument which\nmaximizes is the same",
    "start": "1326210",
    "end": "1331665"
  },
  {
    "text": "as the argument which maximizes\nthe log of that function. And it's a crazy thing, but it\ncomes because of a very simple",
    "start": "1331665",
    "end": "1337220"
  },
  {
    "text": "property. Log is monotonic. If you put larger\nnumbers into a log,",
    "start": "1337220",
    "end": "1342350"
  },
  {
    "text": "you'll get a larger number back. And because of that,\nwhichever number",
    "start": "1342350",
    "end": "1347450"
  },
  {
    "text": "led to a larger value\nof your function-- or log of a function--\nwe know that it must have been the same large\noutput for the function itself.",
    "start": "1347450",
    "end": "1356880"
  },
  {
    "text": "So because log is\nmonotonic, there is this one critically\nimportant claim.",
    "start": "1356880",
    "end": "1362720"
  },
  {
    "text": "Whichever mean and\nstandard deviation maximizes this expression\nwill be the exact same mean",
    "start": "1362720",
    "end": "1371419"
  },
  {
    "text": "and standard deviation\nwhich maximizes the log of this expression.",
    "start": "1371420",
    "end": "1377510"
  },
  {
    "text": "And that expression\nis the pi of f of xi.",
    "start": "1377510",
    "end": "1382930"
  },
  {
    "text": "That is just a copy\nof that insanity. So let me pause here for a\nsecond and see, like, you know,",
    "start": "1382930",
    "end": "1390513"
  },
  {
    "text": "what's interesting\nabout this to people. ",
    "start": "1390513",
    "end": "1395850"
  },
  {
    "text": "Yes. It'd be useful in the\ncase where we're dealing with smaller probabilities. We can just take the log of\nthem and get the argument.",
    "start": "1395850",
    "end": "1401880"
  },
  {
    "text": "Yeah. You're seeing exactly\nwhere I'm going. Can I go back to\nthat demo I gave you? Did you notice how\nI had likelihood,",
    "start": "1401880",
    "end": "1408870"
  },
  {
    "text": "and it's this tiny,\ntiny, tiny number? Notice how below it, I also had\na thing called log likelihood.",
    "start": "1408870",
    "end": "1415830"
  },
  {
    "text": "That's just the log\nof this likelihood. And when I was changing, both\nof these numbers were going up.",
    "start": "1415830",
    "end": "1422250"
  },
  {
    "text": "Like, the likelihood\nwould go up, and the log likelihood\nwould go up with it, because the monotonic\nproperty of logs.",
    "start": "1422250",
    "end": "1428910"
  },
  {
    "text": "And this is a beautiful\nthing for two reasons. One, computers\nkind of are not so",
    "start": "1428910",
    "end": "1434310"
  },
  {
    "text": "good at representing very, very,\nvery, very, very small numbers, and computers are much\nbetter representing",
    "start": "1434310",
    "end": "1439710"
  },
  {
    "text": "logs of those numbers. And then there will\nbe another reason, but I want to be\nmysterious, a little bit.",
    "start": "1439710",
    "end": "1445812"
  },
  {
    "text": "So I'll hold on to\nmy other reason. But you see exactly\nwhere I'm going. So where have we\nbeen in this lecture?",
    "start": "1445812",
    "end": "1452040"
  },
  {
    "text": "A beautiful idea. Like, we have this\ncrazy important goal. Everyone even in CS109\nneeds to know how",
    "start": "1452040",
    "end": "1458280"
  },
  {
    "text": "to choose numbers for models. Maximum likelihood starts\nwith this beautiful idea--",
    "start": "1458280",
    "end": "1464070"
  },
  {
    "text": "choose the parameters\nthat make your data look as likely as possible. And then the next idea I've\ngot for you is, OK, now we've",
    "start": "1464070",
    "end": "1472890"
  },
  {
    "text": "got to choose these numbers? Well, we'll need\nto do an argmax, and we can do the argmax of\nthe log likelihood instead.",
    "start": "1472890",
    "end": "1481470"
  },
  {
    "text": "OK. So the argmax of some\nfunction is always the same as the argmax of a log.",
    "start": "1481470",
    "end": "1487400"
  },
  {
    "text": "And logs, we love. We, like, have a soft, tender\npart in our hearts for logs. We see logs, and\nyou're, like, you've",
    "start": "1487400",
    "end": "1494029"
  },
  {
    "text": "done a solid so many times. And like we, like, shed a\nlittle tear for happiness when we see logs, because so\nmany things become much easier.",
    "start": "1494030",
    "end": "1501860"
  },
  {
    "text": "A lot of equations\nwill simplify. Like, log AB-- A times B is\nequal to log A plus log B.",
    "start": "1501860",
    "end": "1508262"
  },
  {
    "text": "And actually, it is\nworth noting when I mention logs, I don't\noften write a base, because I actually\nmean the natural base.",
    "start": "1508262",
    "end": "1515360"
  },
  {
    "text": "Some people would\nwrite this as LN, But. I've gotten into the habit\nof just calling this \"log.\"",
    "start": "1515360",
    "end": "1520430"
  },
  {
    "text": "OK. We've developed a\nfew critical pieces. We're ready to start\nputting things together.",
    "start": "1520430",
    "end": "1526899"
  },
  {
    "text": "Maximum likelihood estimation--\nI wrote it out on one slide. It's so important, I wrote\nthe SparkNotes on the board",
    "start": "1526900",
    "end": "1533170"
  },
  {
    "text": "so you can always follow along. The great idea is, you have\na general method for choosing",
    "start": "1533170",
    "end": "1538870"
  },
  {
    "text": "numbers for a model. It starts by defining the\nlikelihood for one data point.",
    "start": "1538870",
    "end": "1546110"
  },
  {
    "text": "The second goal is to come up\nwith a log likelihood function. And then the simple\nidea is, we are",
    "start": "1546110",
    "end": "1552680"
  },
  {
    "text": "going to state that the optimal\nparameters are which ones-- are the argmax of the\nlog likelihood function.",
    "start": "1552680",
    "end": "1560400"
  },
  {
    "text": "And then finally, once\nyou've stated that, you would then use an\noptimization algorithm.",
    "start": "1560400",
    "end": "1565760"
  },
  {
    "text": "And the optimization algorithm\ncan help you find the argmax. There's lots of cool\ncomputer science functions",
    "start": "1565760",
    "end": "1571009"
  },
  {
    "text": "that can do argmaxes. I will tell you about\na few of them in CS109.",
    "start": "1571010",
    "end": "1577039"
  },
  {
    "text": "Now, as I wrote up\non the board, what is this likelihood function\nI've been talking about?",
    "start": "1577040",
    "end": "1582669"
  },
  {
    "text": "Well, if you have\nIID random variables, that means they're\nall independent. So it says, how likely is the\ndata, given the current status",
    "start": "1582670",
    "end": "1591070"
  },
  {
    "text": "of the parameters? And that will just be-- loop\nover all your independent data points and talk about\nhow likely each one is,",
    "start": "1591070",
    "end": "1597790"
  },
  {
    "text": "given the parameters. We want to find the\nparameters that maximize this. That's the simple idea.",
    "start": "1597790",
    "end": "1606170"
  },
  {
    "text": "When you start, you will\nwrite down this expression, and you'll replace this\nwith either the PDF, if you",
    "start": "1606170",
    "end": "1612529"
  },
  {
    "text": "have a continuous random\nvariable, the probability mass function, if you have a discrete\none, or the joint distribution,",
    "start": "1612530",
    "end": "1619100"
  },
  {
    "text": "if it's some sort of\nprobabilistic model. OK.",
    "start": "1619100",
    "end": "1625280"
  },
  {
    "text": "That's how you do step one. And step two is just\ntake the log of this.",
    "start": "1625280",
    "end": "1630899"
  },
  {
    "text": "So story so far. You can choose parameters by\nfinding the argmax of the log likelihood of our data.",
    "start": "1630900",
    "end": "1637440"
  },
  {
    "text": "And to write this\nout in equation, the likelihood is the product\nof the likelihood of each data",
    "start": "1637440",
    "end": "1644400"
  },
  {
    "text": "point for IID random variables. The log likelihood--\nwhichever parameters make this the biggest will\nalso be the parameters that",
    "start": "1644400",
    "end": "1651000"
  },
  {
    "text": "make this the biggest, but\nthis can be much easier to work with. And notice the first\nthing that happens. When you take a log\nof this expression,",
    "start": "1651000",
    "end": "1657900"
  },
  {
    "text": "that log used to live\noutside of the columns. But remember, the log\nis like a tractor.",
    "start": "1657900",
    "end": "1663510"
  },
  {
    "text": "And sees the columns,\nand it's, like, [GROWLS],, and then the columns get\ncollapsed into the summation.",
    "start": "1663510",
    "end": "1669150"
  },
  {
    "text": "The log is, like, I'm inside. That's just a general property\nof logs, if you didn't know.",
    "start": "1669150",
    "end": "1675150"
  },
  {
    "text": "So log of all your\ndata points will lead to the sum of the log of\nthe likelihood of each data",
    "start": "1675150",
    "end": "1682470"
  },
  {
    "text": "point on its own. And then we're going to state\nthat the best parameters that",
    "start": "1682470",
    "end": "1688058"
  },
  {
    "text": "we can guess-- and we put funny\nlittle hats on things that we guess-- is going to be the argmax,\nwhichever parameter makes",
    "start": "1688058",
    "end": "1694860"
  },
  {
    "text": "this as large as possible. I want to take a moment\nfor conversation. I want you to have a nice\nlittle chat with the person",
    "start": "1694860",
    "end": "1701370"
  },
  {
    "text": "next to you. So take two minutes. What is confusing about this? What's interesting about this? What questions do you have?",
    "start": "1701370",
    "end": "1706590"
  },
  {
    "text": "See if you can come up\nwith a good question with the person next to you,\nand then let's talk about this all together. What a crazy set of ideas.",
    "start": "1706590",
    "end": "1712560"
  },
  {
    "text": "Go for it. [SIDE CONVERSATION] ",
    "start": "1712560",
    "end": "1790419"
  },
  {
    "text": "OK. I can guess one\nof your questions. Like, what's this\ngoing to look like when",
    "start": "1790420",
    "end": "1795910"
  },
  {
    "text": "we apply it to a real problem? We'll get there in a second. Other questions\nthat have come up? [INAUDIBLE]",
    "start": "1795910",
    "end": "1801750"
  },
  {
    "text": "Curiosities? Yes. Isn't the argmax of a log--\nwouldn't that be infinity?",
    "start": "1801750",
    "end": "1807020"
  },
  {
    "text": "Like, what, exactly,\nare we doing there? So for any particular\nvalues of parameters,",
    "start": "1807020",
    "end": "1819179"
  },
  {
    "text": "you can calculate the log\nlikelihood of your data. And notice that as I change\nthese parameters, that changes.",
    "start": "1819180",
    "end": "1827659"
  },
  {
    "text": "And it's not the case\nthat infinity-- so saying that infinity is the argmax\nis saying, if you put infinity",
    "start": "1827660",
    "end": "1834190"
  },
  {
    "text": "into these numbers, then\nthat would be your argmax.",
    "start": "1834190",
    "end": "1839707"
  },
  {
    "text": "Of course, this is an infinity. But it's getting there, right? But if I put infinity\nin for our parameters,",
    "start": "1839708",
    "end": "1844929"
  },
  {
    "text": "look at my current\nvalue of log likelihood. It's negative 4,549.",
    "start": "1844930",
    "end": "1850390"
  },
  {
    "text": "That feels like it's better than\nnegative 300, but it's worse. So putting in infinite\nvalues into parameters",
    "start": "1850390",
    "end": "1856720"
  },
  {
    "text": "doesn't actually maximize\nthe log of the likelihood. The thing that maximizes\nthe log likelihood",
    "start": "1856720",
    "end": "1863049"
  },
  {
    "text": "will be the same\nthing that maximizes the likelihood itself, which\nwill, in fact, be pretty",
    "start": "1863050",
    "end": "1868270"
  },
  {
    "text": "good guesses of our parameters. Does that answer the question?",
    "start": "1868270",
    "end": "1873490"
  },
  {
    "text": "Very, very good question. Other questions come up? Yes. On the slide where you had,\nlike, the different steps",
    "start": "1873490",
    "end": "1880300"
  },
  {
    "text": "for the maximum\nlikelihood algorithm, the first step was\nlike deciding a model",
    "start": "1880300",
    "end": "1885429"
  },
  {
    "text": "for the distribution of your\nsamples from [INAUDIBLE] your sample. How do you do that? How do you decide\nwhat your model is?",
    "start": "1885430",
    "end": "1892510"
  },
  {
    "text": "Good question. So this is assuming\nsomebody gave you a model. So at this point, we're saying-- we're telling you\nyour data is Gaussian.",
    "start": "1892510",
    "end": "1899350"
  },
  {
    "text": "We're telling you\nyour data is Pareto. We're telling you\nthat your data are joint samples from this tiny\nlittle Bayesian network.",
    "start": "1899350",
    "end": "1906010"
  },
  {
    "text": "And there's these missing\nnumbers in all of those models, and your job is to find\nthe missing numbers.",
    "start": "1906010",
    "end": "1911529"
  },
  {
    "text": "There is a different\ntask of, you decide what the\nstructure is yourself. And I'm going to leave that\ntill [? 228. ?] Good question.",
    "start": "1911530",
    "end": "1918568"
  },
  {
    "text": "No, we can talk about it. We've done that\nin [? batches. ?] We've touched upon structure\nlearning a little bit. But this is the question\nof figuring out parameters.",
    "start": "1918568",
    "end": "1925669"
  },
  {
    "text": " Yay. Yay, Yay, Yay. You guys need to see this.",
    "start": "1925670",
    "end": "1931590"
  },
  {
    "text": "You need to see this live. Because so far, it's all\ntheory and no application. And you can solve\nso many problems",
    "start": "1931590",
    "end": "1938309"
  },
  {
    "text": "once you know how to--\nooh, but we have a bad guy. Argmax. We haven't talked\nabout how to do argmax.",
    "start": "1938310",
    "end": "1944543"
  },
  {
    "text": "This would work great,\nand we could apply this if we knew how to do argmax. But we haven't talked\nabout this at all. I just said, do argmax, and\nyou're, like, OK, great.",
    "start": "1944543",
    "end": "1951606"
  },
  {
    "text": "I'll do argmax. Two options I'm\ngoing to tell you. The first one you learned\nabout in calculus class--",
    "start": "1951607",
    "end": "1957600"
  },
  {
    "text": "and we're going to\nstart with that-- is just straight optimization. Let's say I gave you a\nfunction-- negative x squared,",
    "start": "1957600",
    "end": "1963650"
  },
  {
    "text": "but this time plus 4, just to\nmix things up a little bit-- and I said, find the\nargmax of this expression.",
    "start": "1963650",
    "end": "1972127"
  },
  {
    "text": "You might just be able to\nlook at the expression and be, like, it's zero, but\nthis is not supposed to be using your visual cortex.",
    "start": "1972127",
    "end": "1978270"
  },
  {
    "text": "We're trying to solve\nthis using math. How could we solve\nthis using math? And you're, like, oh,\nmy calculus teacher",
    "start": "1978270",
    "end": "1984420"
  },
  {
    "text": "told me something about this. He-- she-- said that I\nshould find the derivative",
    "start": "1984420",
    "end": "1990779"
  },
  {
    "text": "and set it equal to zero. How have you guys had\na calculus teacher tell you something like that? Thanks you, Mr. [? Blanton. ?] So if you look at this part\nthat maximizes the arguments,",
    "start": "1990780",
    "end": "2000660"
  },
  {
    "text": "notice that there's a\nderivative of zero out there. Must be true at a max, so\nthe derivative must be zero. So one of the things that\npeople will classically do",
    "start": "2000660",
    "end": "2007827"
  },
  {
    "text": "is they'll take the derivative. Ooh, derivatives. Scary. But we have WolframAlpha. Not so scary you put\nit in WolframAlpha,",
    "start": "2007827",
    "end": "2013356"
  },
  {
    "text": "It tells you the\nderivative of this is 2x. And you're, like, 2x? What does that mean?",
    "start": "2013357",
    "end": "2018539"
  },
  {
    "text": "Well, if x is 3, that means that\nthe derivative, at that point, is 6.",
    "start": "2018540",
    "end": "2024710"
  },
  {
    "text": "And so we can figure out\nthe derivative-- oh, wait. Where's my negative sign, here?",
    "start": "2024710",
    "end": "2030582"
  },
  {
    "text": "It should be a negative sign? Cheeky [? Chris. ?]\nTurns out zero doesn't really mess things up.",
    "start": "2030582",
    "end": "2037159"
  },
  {
    "text": "Negative 2x. So if x is 6 or 3, then your\nderivative, at that point, is negative 6.",
    "start": "2037160",
    "end": "2042380"
  },
  {
    "text": "The slope is negative\n6, at that point. We don't care about all points. We just care about the\npoints where this derivative",
    "start": "2042380",
    "end": "2047870"
  },
  {
    "text": "is equal to zero. So what we could do\nis we could say, OK. Take this derivative and\ntell me the value of x",
    "start": "2047870",
    "end": "2053940"
  },
  {
    "text": "which makes this equal to zero. And if you solve\nfor the value of x that makes this equal to zero. You get that, OK, when x is\nzero, the derivative is zero.",
    "start": "2053940",
    "end": "2064340"
  },
  {
    "text": "So this is a good hypothesis\nfor a [? maxima ?] point. And that is one way\nof doing argmax.",
    "start": "2064340",
    "end": "2070940"
  },
  {
    "text": "It has some downsides. You could find the worst\npossible parameters, because this could be a\nminimum, and obviously, it",
    "start": "2070940",
    "end": "2078888"
  },
  {
    "text": "might not scale when you get\nto bigger and bigger models. But this is just fine\nfor getting us warmed up.",
    "start": "2078889",
    "end": "2084770"
  },
  {
    "text": "We have our first\nalgorithm for doing argmax. Yeah. Question. [INAUDIBLE]",
    "start": "2084770",
    "end": "2090888"
  },
  {
    "text": " No. If there's multiple\nmaxes and mins, you have to do a\nnext-level analysis.",
    "start": "2090889",
    "end": "2096812"
  },
  {
    "text": "You have to decide,\nare these maxes? And are these mins? And you'd have to look at the\nsecond derivative at minimum.",
    "start": "2096812",
    "end": "2103505"
  },
  {
    "text": "So this works for\npretty simple things. This will just get\nus over the hump so I can show you some examples. But we're going to\nneed-- keep in mind I",
    "start": "2103505",
    "end": "2110369"
  },
  {
    "text": "want a more satisfying argmax. Everyone should want more\nsatisfying argmax OK.",
    "start": "2110370",
    "end": "2118530"
  },
  {
    "text": "OK. Yeah. This is all the notes of, like,\nthis is why this is so limited. And don't worry about it.",
    "start": "2118530",
    "end": "2123710"
  },
  {
    "text": "We'll just get a\nbetter version later. So this is your\ngeneral MLE formula.",
    "start": "2123710",
    "end": "2128750"
  },
  {
    "text": "If you want to find a parameter,\nyou say, what's the likelihood?",
    "start": "2128750",
    "end": "2133990"
  },
  {
    "text": "What's the log likelihood? And then you find the value\nwhich maximizes the likelihood.",
    "start": "2133990",
    "end": "2139840"
  },
  {
    "text": "You do the argmax. Let us start with kind of a\nmedium difficulty example.",
    "start": "2139840",
    "end": "2146030"
  },
  {
    "text": "It's not the hardest\nthing in the world, but it's going to be a\npretty good way for us",
    "start": "2146030",
    "end": "2151579"
  },
  {
    "text": "to get used to doing this. It won't be, like-- once\nyou get used to MLE, you can solve so many\ninteresting problems.",
    "start": "2151580",
    "end": "2157130"
  },
  {
    "text": "But let's start with\nsomething that's in the wheelhouse\nof what we know. An MLE problem would\ngenerally look like this.",
    "start": "2157130",
    "end": "2164240"
  },
  {
    "text": "I am telling you my model. In this case, my model\nis that every data point has a single\nvalue, and that value",
    "start": "2164240",
    "end": "2170890"
  },
  {
    "text": "is drawn from a Poisson. And I've seen, you know, 12\ndifferent independent samples.",
    "start": "2170890",
    "end": "2177690"
  },
  {
    "text": "From this thing that\nI'm going to call data, if this is my data set,\nthe challenge of MLA",
    "start": "2177690",
    "end": "2185609"
  },
  {
    "text": "is to estimate the parameters. And the parameters for\na Poisson is lambda. Remember, lambda is just\nthe word for this symbol.",
    "start": "2185610",
    "end": "2194730"
  },
  {
    "text": "XI-- when I use this\nnotation, I mean, like, the i'th value in\nmy data points. So if I gave you\nthese data points,",
    "start": "2194730",
    "end": "2202180"
  },
  {
    "text": "how could we estimate lambda? Again, for Poisson, we have\nsome pretty reasonable ways of guessing what\nlambda is from this.",
    "start": "2202180",
    "end": "2208230"
  },
  {
    "text": "You know, maybe\nyou could imagine a way of doing it without going\nto a first-principle approach,",
    "start": "2208230",
    "end": "2213480"
  },
  {
    "text": "like MLE. But let's use this as\nour first opportunity to go end-to-end with MLE.",
    "start": "2213480",
    "end": "2218880"
  },
  {
    "text": "You guys ready for it? I'm just warning you. You're going to look\nlike really, really",
    "start": "2218880",
    "end": "2225432"
  },
  {
    "text": "legit mathematicians\nonce you get used to MLE, because the math looks\nsuper impressive, but really, we're just always\nfollowing this formula.",
    "start": "2225433",
    "end": "2232037"
  },
  {
    "text": "But when you get\nback to Thanksgiving, I want you to do a little\nMLE, and just, like, let your parents see. And they're, like, wow, kid's\nlearning so much at Stanford.",
    "start": "2232037",
    "end": "2239620"
  },
  {
    "text": "OK. Let's try this out. So I'm going to use this\nnotation for our data points.",
    "start": "2239620",
    "end": "2247283"
  },
  {
    "text": "I gave you 12 data\npoints, but now we're going to call them x1 to xn. Why? Just to use a little\nbit of notation.",
    "start": "2247283",
    "end": "2253869"
  },
  {
    "text": "I'm assuming that every\nsingle one of them are IID from the same\nPoisson, and my job is to try and estimate lambda\nbased on these data points.",
    "start": "2253870",
    "end": "2262240"
  },
  {
    "text": "And I'm going to\nuse this recipe-- the MLE recipe. And the end result\nof the MLE recipe",
    "start": "2262240",
    "end": "2267730"
  },
  {
    "text": "is going to be an\nestimate for lambda. It starts very humbly. You have to say how likely is a\nsingle data point if I tell you",
    "start": "2267730",
    "end": "2276165"
  },
  {
    "text": "what lambda is? So if I told you lambda was\n5, how likely is a data point?",
    "start": "2276165",
    "end": "2282450"
  },
  {
    "text": "Hardest part of MLE\nsometimes, actually. ",
    "start": "2282450",
    "end": "2289870"
  },
  {
    "text": "No. I want people to sit\nwith it-- the tension. What is the likelihood\nof one data point?",
    "start": "2289870",
    "end": "2295505"
  },
  {
    "text": " OK. I'm going to tell you.",
    "start": "2295505",
    "end": "2301670"
  },
  {
    "text": "It's, very simply, the\nprobability [? mass ?] function for a Poisson.",
    "start": "2301670",
    "end": "2309220"
  },
  {
    "text": "So if you go back to your course\nreader, and you said, like, I have a Poisson. If I told you what\nlambda was, we",
    "start": "2309220",
    "end": "2314420"
  },
  {
    "text": "can talk about how\nlikely it is that you saw a particular value x sub i. And you can say, what's the\nlikelihood of seeing x sub i?",
    "start": "2314420",
    "end": "2320750"
  },
  {
    "text": "Well, it's e to the\npower of negative whatever your Lambda was,\nlambda to the power of x sub i, and then x sub i's factorial.",
    "start": "2320750",
    "end": "2326930"
  },
  {
    "text": "That's just the\nprobability mass function. And that's really\nyour first step. So the first step is to just\nwrite what likelihood is.",
    "start": "2326930",
    "end": "2336560"
  },
  {
    "text": "But that's just likelihood\nof one data point. If we talk about the\nlikelihood of all of our data",
    "start": "2336560",
    "end": "2342410"
  },
  {
    "text": "given a particular lambda,\nbecause our data is IID, the likelihood is\nreally this expression,",
    "start": "2342410",
    "end": "2348740"
  },
  {
    "text": "and this expression\nis just going to be the product of the\nlikelihood of each data point on its own. You can't always do this.",
    "start": "2348740",
    "end": "2354440"
  },
  {
    "text": "It has to be independent\nfor this to be true. But because it's\nindependent, this is true. And then we can just\nplug in this expression",
    "start": "2354440",
    "end": "2360562"
  },
  {
    "text": "we had for one data\npoint, and we're just going to multiply it for\ndifferent data points. If you had to\ncalculate likelihood,",
    "start": "2360562",
    "end": "2366290"
  },
  {
    "text": "you would write a four-loop. And for everything,\nyou'd look at the value from our previous slide.",
    "start": "2366290",
    "end": "2371970"
  },
  {
    "text": "You'd take 6 and plug\nit in for x sub 1, 1 for x sub 2, and so on. And each time, you'd\nbe able to calculate",
    "start": "2371970",
    "end": "2379230"
  },
  {
    "text": "this inner term, product\nthem all together, and that's the likelihood if\nI told you lambda was, say, 5.",
    "start": "2379230",
    "end": "2385350"
  },
  {
    "text": "Yes. So what would it look like if\nthis data wasn't independent?",
    "start": "2385350",
    "end": "2391920"
  },
  {
    "text": "Like, what else could we do? Right. Right. Don't even look back. Just go.",
    "start": "2391920",
    "end": "2397230"
  },
  {
    "text": "OK. If the data was not\nindependent, you'd enter a really weird\ntheoretical world.",
    "start": "2397230",
    "end": "2402810"
  },
  {
    "text": "But good news! Almost all data is\nassumed to be IID. We almost always assume\nthat each data point",
    "start": "2402810",
    "end": "2409589"
  },
  {
    "text": "is some independent draw\ncondition on the parameters. I'm having trouble,\nlike, imagining",
    "start": "2409590",
    "end": "2414690"
  },
  {
    "text": "what this much, like,\ndependent data [? would be. ?] Like, is there an example of\nsomething that is dependent?",
    "start": "2414690",
    "end": "2421245"
  },
  {
    "text": "I'm going to give\nyou an example. But then I'm going to\nyou the first thing a mathematician\nwould do would assume it's independent anyways.",
    "start": "2421245",
    "end": "2427290"
  },
  {
    "text": "So let's say these lambdas\nare, like, how many people get processed by a cashier.",
    "start": "2427290",
    "end": "2433020"
  },
  {
    "text": "Maybe, like, how long it takes\none person like one cashier to process a chunk in an\nhour influences the next one,",
    "start": "2433020",
    "end": "2439140"
  },
  {
    "text": "because it leaves a queue. So there could be a way\nthat they're not actually independent. But guess what? First thing we're going to\ndo is assume independence,",
    "start": "2439140",
    "end": "2444990"
  },
  {
    "text": "because none of the\nmath ever works out if it's not independent. So IID-- often,\nassumption that we need.",
    "start": "2444990",
    "end": "2450450"
  },
  {
    "text": "But most data actually--\nit's a very, very, very good assumption.",
    "start": "2450450",
    "end": "2455780"
  },
  {
    "text": "OK. At this point, if you gave\nme any value of lambda, I could plug it in with\nmy particular data set,",
    "start": "2455780",
    "end": "2462230"
  },
  {
    "text": "and I could say, how\nlikely does my data look in the presence of your lambda. And we could play the game\nwe played with the Gaussians",
    "start": "2462230",
    "end": "2467840"
  },
  {
    "text": "where you're just going\nto twiddle on lambdas. And you could try all the\ndifferent values of lambda. But that's not so fun. We'd like a computer to do\nthe twiddling for us, which",
    "start": "2467840",
    "end": "2476099"
  },
  {
    "text": "leads us to the next step. We're going to try and\nfind the lambda which maximizes this\nexpression, which is",
    "start": "2476100",
    "end": "2482519"
  },
  {
    "text": "the same as the\nlambda which maximizes the log of this expression. And you're like,\nI have to do logs.",
    "start": "2482520",
    "end": "2487950"
  },
  {
    "text": "No, we get to do logs. We are the humble users\nof this beautiful tool,",
    "start": "2487950",
    "end": "2493019"
  },
  {
    "text": "which is the log. So I just write the log of\nthe expression I had before. And remember, the first\nthing the log does",
    "start": "2493020",
    "end": "2498660"
  },
  {
    "text": "is like, I want to go\nlive inside the house, and I'm going to break down\nthe columns as I go there. OK, great, the log\nwent inside the house.",
    "start": "2498660",
    "end": "2504570"
  },
  {
    "text": "And you have the log of\nthis nasty expression. You don't want to do it. But then you're like,\nwait a second, log",
    "start": "2504570",
    "end": "2509790"
  },
  {
    "text": "of e is a good time, isn't it? And you're like,\nyeah, that's just going to be negative lambda. And you're like, I can do\nlog of this expression,",
    "start": "2509790",
    "end": "2515550"
  },
  {
    "text": "plus log of this expression,\nminus log of this expression. And when I do log\nof this expression, that exponent just\nbecomes something",
    "start": "2515550",
    "end": "2521401"
  },
  {
    "text": "that goes in the front. And look how nice that is. When I took the log, it\nturned this nasty expression",
    "start": "2521402",
    "end": "2528180"
  },
  {
    "text": "into something actually quite\na lot easier to work with. The log is your friend. The other reason I was going to\ntell you why we use log is it",
    "start": "2528180",
    "end": "2534660"
  },
  {
    "text": "makes math much, much easier. So at this point, we\nhave a special expression we call the log-likelihood.",
    "start": "2534660",
    "end": "2541570"
  },
  {
    "text": "And the final\nthing we want to do is we want to choose the lambda\nwhich makes this expression as",
    "start": "2541570",
    "end": "2547330"
  },
  {
    "text": "large as possible. Maybe when you put five\nin, it's large as possible. When you put three in,\nit's as large as possible.",
    "start": "2547330",
    "end": "2553369"
  },
  {
    "text": "We would like to choose the\nargmax of this log-likelihood. Notice how I put\na derivative here?",
    "start": "2553370",
    "end": "2561750"
  },
  {
    "text": "It's because no matter which\noptimization technique you use, you will most likely\nneed to derive,",
    "start": "2561750",
    "end": "2568140"
  },
  {
    "text": "take the derivative of this\nwith respect to your parameters. You guys are ready\nfor some calculus?",
    "start": "2568140",
    "end": "2573690"
  },
  {
    "text": "Definitely could use\nWolfram Alpha here. But we can do this\none on our own. In fact, this is the\nlevel of calculus",
    "start": "2573690",
    "end": "2579457"
  },
  {
    "text": "that I think would be nice\nto know how to do on our own. Derive that expression\nwith respect to lambda.",
    "start": "2579457",
    "end": "2585570"
  },
  {
    "text": " OK, you guys can think about it. First of all, the\nderivative of a sum",
    "start": "2585570",
    "end": "2592400"
  },
  {
    "text": "is just going to be\nthe sum of derivative. So the derivative will just\nmove right into this inner part. And you're like, derivative\nof that term seems not so bad.",
    "start": "2592400",
    "end": "2600260"
  },
  {
    "text": "Derivative of this term,\nwell, it's got a log. But we can do that. It seems not so bad. Derivative of this\nterm looks awful.",
    "start": "2600260",
    "end": "2606080"
  },
  {
    "text": "What's the derivative\nof factorial? Oh, my god, we're going to\nneed a smooth, continuous",
    "start": "2606080",
    "end": "2611150"
  },
  {
    "text": "approximation. Wait, somebody's\nshaking their head. Why? Why? There's no lambda.",
    "start": "2611150",
    "end": "2617870"
  },
  {
    "text": "And if there's no lambda,\nthe derivative is just zero. As lambda changes,\nhow does this change?",
    "start": "2617870",
    "end": "2624170"
  },
  {
    "text": "Zero. Fantastic, what a\nbeautiful thing. OK, now, we're feeling brave.",
    "start": "2624170",
    "end": "2629330"
  },
  {
    "text": "We go into our derivative. So the derivative of that term\nwith respect to lambda is-- well, the derivative is\ngoing to go inside the sum.",
    "start": "2629330",
    "end": "2637340"
  },
  {
    "text": "The derivative of lambda\nwith respect to lambda? Just one. So that was a negative lambda. This becomes a negative 1.",
    "start": "2637340",
    "end": "2642829"
  },
  {
    "text": "The derivative of that log,\nso x sin log of lambda. Well, the derivative of log of\nlambda is just 1 over lambda.",
    "start": "2642830",
    "end": "2649440"
  },
  {
    "text": "And that Xi looks\nlike a constant. And as we mentioned\nthat negative log, Xi factorial just goes away.",
    "start": "2649440",
    "end": "2655480"
  },
  {
    "text": "So we have that this\nis the derivative. I'm then going to do\nthis thing where I'm",
    "start": "2655480",
    "end": "2661290"
  },
  {
    "text": "going to take this derivative. And I'm going to move this\nsum as far in as possible so",
    "start": "2661290",
    "end": "2666300"
  },
  {
    "text": "that we really only\ndeal with the terms that have eyes in them. There's no eyes here, and\nthere's no eyes there.",
    "start": "2666300",
    "end": "2671790"
  },
  {
    "text": "When I move this sum\nthrough this negative 1, it's like you added up\nnegative 1 n times, that",
    "start": "2671790",
    "end": "2677700"
  },
  {
    "text": "just becomes a negative n. And when I move the sum\ninto this inner term, it's like every single term is\nmultiplied by 1 over lambda.",
    "start": "2677700",
    "end": "2685020"
  },
  {
    "text": "And then you're\nsumming up the Xi's. So this is how that\nterm gets rearranged. At this point, we have the\nderivative of log-likelihood",
    "start": "2685020",
    "end": "2693170"
  },
  {
    "text": "with respect to lambda. Your calculus\nteacher, Mr. Blanton. Didn't Mr. Blanton\nteach anyone else here?",
    "start": "2693170",
    "end": "2700800"
  },
  {
    "text": "I don't know, my\nMalaysian probability. I heard he went to Brazil,\nand he taught some people. Maybe. You?",
    "start": "2700800",
    "end": "2706000"
  },
  {
    "text": "No. [LAUGHS] OK, we all have\ndifferent probability or calculus teachers. But all of our\ncalculus teachers would",
    "start": "2706000",
    "end": "2712332"
  },
  {
    "text": "have told us\nsomething like, if you want to find the value which\nmaximizes, you can take your derivative, set it to 0.",
    "start": "2712332",
    "end": "2719520"
  },
  {
    "text": "So I'm going to take my\nderivative, set it to 0. And when I solve this\nexpression for lambda,",
    "start": "2719520",
    "end": "2724589"
  },
  {
    "text": "I end up with lambda equals\nthe sum over all my Xi's divided by n, where n is\nthe size of my data set.",
    "start": "2724590",
    "end": "2733460"
  },
  {
    "text": "Wait a second. That's like a screen\nfilled with mathematics.",
    "start": "2733460",
    "end": "2739049"
  },
  {
    "text": "And at the end, it just\nsays take the average of your data points? Right? This is saying like,\ntake all your 20 values,",
    "start": "2739050",
    "end": "2745692"
  },
  {
    "text": "add them up together,\nand then divide it by how many values you have. You're like, a page of\nmathematics for what? The sample mean?",
    "start": "2745693",
    "end": "2753539"
  },
  {
    "text": "And yeah, for Poisson, the\nmaximum likelihood estimation for lambda says you just\naverage your data points.",
    "start": "2753540",
    "end": "2760619"
  },
  {
    "text": "And that actually\nfeels pretty good. And MLE is Poisson\nfor the sample. If this is a good time,\nit's a good enough result.",
    "start": "2760620",
    "end": "2767847"
  },
  {
    "text": "It was a lot of math\nbut it led to something that feels about right. OK, I'm going to make us do one\nmore simple example before we",
    "start": "2767847",
    "end": "2777240"
  },
  {
    "text": "jump into solving\nMLE for things we haven't seen the answer to yet.",
    "start": "2777240",
    "end": "2782730"
  },
  {
    "text": "Can we do one more\nsimple example? Let's drive this home. Bernoulli. Oh, man.",
    "start": "2782730",
    "end": "2789198"
  },
  {
    "text": "Consider random\nvariables and we're going to assume that they\nall come from Bernoulli. And you want to figure\nout what is your best",
    "start": "2789198",
    "end": "2795690"
  },
  {
    "text": "estimate for the parameters. What's the parameters\nof Bernoulli? Ah, yes, p. And recall that your\ndata in this case",
    "start": "2795690",
    "end": "2803130"
  },
  {
    "text": "is going to be like\n0, 1, 0, 0, 1, 1, 0.",
    "start": "2803130",
    "end": "2808595"
  },
  {
    "text": "And we're assuming\nthat these are all IID's pulls from\nthe same Bernoulli",
    "start": "2808595",
    "end": "2815420"
  },
  {
    "text": "with the same parameter p. And we're trying to figure out\nwhat is the best value of p here. Again, you could\nprobably come up",
    "start": "2815420",
    "end": "2822140"
  },
  {
    "text": "with a pretty reasonable\nformula for how you could have gotten this. But I want to use\nthis opportunity to flex our MLE muscles.",
    "start": "2822140",
    "end": "2829115"
  },
  {
    "text": "So we're going to try\nand use all these steps and derive it using MLE. OK.",
    "start": "2829115",
    "end": "2836349"
  },
  {
    "text": "First step, you have to derive\nthe formula for log-likelihood. It's supposed to be the sum\nof the log of the likelihood",
    "start": "2836350",
    "end": "2842500"
  },
  {
    "text": "of each value. And at this point,\nyou're like, OK, what's the likelihood\nof each value?",
    "start": "2842500",
    "end": "2848240"
  },
  {
    "text": "It's a Bernoulli. What's the probability mass\nfunction of a Bernoulli?",
    "start": "2848240",
    "end": "2853480"
  },
  {
    "text": "And then you look up\nin the course reader, and you get something like this. And you're like, I\nneed to substitute this",
    "start": "2853480",
    "end": "2860030"
  },
  {
    "text": "into that equation. The Bernoulli says,\nif your a value is 1, the probability was p.",
    "start": "2860030",
    "end": "2865460"
  },
  {
    "text": "If your value is\n0, your probability was 1 minus p, whatever p was. And that's really\ncute and very helpful,",
    "start": "2865460",
    "end": "2871790"
  },
  {
    "text": "except it's not at\nall differentiable. Wait, I promise\nBernoulli would be easy.",
    "start": "2871790",
    "end": "2877986"
  },
  {
    "text": "Ah-ahh! [CHUCKLES] Oh, no. I'm going to show you\nthe slickest trick.",
    "start": "2877986",
    "end": "2884640"
  },
  {
    "text": "And you need to learn\nthis now because you're going to see this in 221. You're going to see it in 229. And you're going to see\nit in 220x, whatever.",
    "start": "2884640",
    "end": "2892778"
  },
  {
    "text": "And they're not going\nto explain it to you. They're just going to say\nit as if it was the most obvious thing in the world.",
    "start": "2892778",
    "end": "2898740"
  },
  {
    "text": "People don't use this\nas a probability mass function for a Bernoulli.",
    "start": "2898740",
    "end": "2903839"
  },
  {
    "text": "Instead, a lot of\ntimes, you'll see people write a continuous\nversion of that same table.",
    "start": "2903840",
    "end": "2909640"
  },
  {
    "text": "So to be clear, this table is\nsaying the probability of 1 is p, and the probability\nof 0 is 1 minus p.",
    "start": "2909640",
    "end": "2915570"
  },
  {
    "text": "Makes sense? That's fine. But this graph is\nnot differentiable.",
    "start": "2915570",
    "end": "2920940"
  },
  {
    "text": "And so people use a different\nexpression for the probability mass function of Bernoulli. Are you ready for it? They use that.",
    "start": "2920940",
    "end": "2927930"
  },
  {
    "text": "You're like, what is that? It is particularly p raised\nto the power of x times 1",
    "start": "2927930",
    "end": "2935490"
  },
  {
    "text": "minus p raised to the\npower of 1 minus x. And you're like,\nwhat are you guys on? Well, it turns out this\nis a continuous version",
    "start": "2935490",
    "end": "2943000"
  },
  {
    "text": "of that expression. Let's try plugging in 0's for x. If you plug in a 0 for x,\nwhat's p to the power of 0?",
    "start": "2943000",
    "end": "2951780"
  },
  {
    "text": "This whole term goes away. And 1 minus 0 is what?",
    "start": "2951780",
    "end": "2956970"
  },
  {
    "text": "1. So this whole term goes away. If you put in a 0 for x,\nyou're left with 1 minus p.",
    "start": "2956970",
    "end": "2962609"
  },
  {
    "text": "What if you put in a 1 for x? Well, then this whole term will\ngo away because 1 minus 1 is 0.",
    "start": "2962610",
    "end": "2967950"
  },
  {
    "text": "And that whole term goes away. And you're just going\nto be left with a p. If you plug in a 0 and\na 1, this crazy formula",
    "start": "2967950",
    "end": "2975360"
  },
  {
    "text": "picks up what this\ngraph left off. If you were to graph\nit, it has values for--",
    "start": "2975360",
    "end": "2980579"
  },
  {
    "text": "what's the probability that\nyour Bernoulli gave you a 0.3? And you're like, what? That doesn't make sense.",
    "start": "2980580",
    "end": "2986460"
  },
  {
    "text": "It just happens to make sense\nat the value of 0 and 1. And it happens to be derivable. And just be clear, if\nyou put p equals 0.2,",
    "start": "2986460",
    "end": "2993850"
  },
  {
    "text": "this would be a 0.2 to\nthe power of x times 0.8 to the power of 1 minus x. And that x and the 1 minus x\nare just choosing whether or not",
    "start": "2993850",
    "end": "3002150"
  },
  {
    "text": "you should use this term or\nyou should choose that term. Insanity.",
    "start": "3002150",
    "end": "3007260"
  },
  {
    "text": "But learn it. Live it. Love it now. Because people are going\nto just state this as if it were obvious in the future.",
    "start": "3007260",
    "end": "3013602"
  },
  {
    "text": "And they will not explain\nit with a nice little chart like this. OK, so before I jump back\ninto Bernoulli's, just",
    "start": "3013602",
    "end": "3020160"
  },
  {
    "text": "know that this is a\ncontinuous derivable version of the probability\nmass function of a Bernoulli.",
    "start": "3020160",
    "end": "3027480"
  },
  {
    "text": "Craziness. OK, so if we want to\ndo a Bernoulli using our good old friend MLE,\nyou write the likelihood.",
    "start": "3027480",
    "end": "3034260"
  },
  {
    "text": "And here, instead of\nwriting the bar chart, we're going to write\nthat continuous version",
    "start": "3034260",
    "end": "3039330"
  },
  {
    "text": "of the probability\nmass function. Then you say, what's the\nlikelihood of all the data? It's just the products\nover all our IID samples.",
    "start": "3039330",
    "end": "3046227"
  },
  {
    "text": "So we're going to loop\nover all these values and then either\nchoose p or 1 minus p, depending on whether or\nnot the value is 1 or 0.",
    "start": "3046227",
    "end": "3053220"
  },
  {
    "text": "Then we do the log. And we do the log\nof this expression. Yikes. Yikes.",
    "start": "3053220",
    "end": "3059010"
  },
  {
    "text": "Oh, wait, actually quite nice. Because the log of a product\nbecomes a sum of logs.",
    "start": "3059010",
    "end": "3064155"
  },
  {
    "text": "You take the log of\nexpression at this point, you have log-likelihood. And my claim is\nyou should choose",
    "start": "3064155",
    "end": "3069300"
  },
  {
    "text": "whichever p makes that\nequation as large as possible.",
    "start": "3069300",
    "end": "3074480"
  },
  {
    "text": "Argmax it. How do you argmax it? Derive it and set it equal to 0.",
    "start": "3074480",
    "end": "3080050"
  },
  {
    "text": "I'm going to let you do this\non your own if you want. But if you derive it\nand set it equal to 0,",
    "start": "3080050",
    "end": "3085390"
  },
  {
    "text": "you'll end up with\nthis wonderful result that your best value for\np is equal to, drum roll,",
    "start": "3085390",
    "end": "3092710"
  },
  {
    "text": "sum all your values\ndivided by n.  A whole page of mathematics\nand what did we derive?",
    "start": "3092710",
    "end": "3101870"
  },
  {
    "text": "Just choose your p to\nbe the sample mean. And it's a-- unbiased\nestimator is a really fancy way",
    "start": "3101870",
    "end": "3107600"
  },
  {
    "text": "of saying the sample mean. So all this math and it\nsays you should choose p to be your sample mean.",
    "start": "3107600",
    "end": "3112758"
  },
  {
    "text": "It says add up all these\nvalues and divide it by n. And that's a good\nestimate for p. That's what MLE says.",
    "start": "3112758",
    "end": "3118200"
  },
  {
    "text": "It says, that will\nbe the value which makes this data look\nas likely as possible. But on some level,\nthat feels pretty good.",
    "start": "3118200",
    "end": "3124790"
  },
  {
    "text": "For Bernoulli, again, it's\nthe unbiased estimator. I promise it won't always\nbe the sample mean. But for now, we can just be\nlike, that's a good time.",
    "start": "3124790",
    "end": "3132620"
  },
  {
    "text": "We've done our math for two\nexamples of Poisson Bernoulli. And both times, they\nlead to a result",
    "start": "3132620",
    "end": "3138080"
  },
  {
    "text": "that seems pretty reasonable. And this is where we depart\noff into the wild unknown.",
    "start": "3138080",
    "end": "3146160"
  },
  {
    "text": "Because we're going to be able\nto do this for more and more interesting problems. ",
    "start": "3146160",
    "end": "3154310"
  },
  {
    "text": "If n was 10 for\nBernoulli, I just want to make this a little\nbit substantialized.",
    "start": "3154310",
    "end": "3159319"
  },
  {
    "text": "So imagine you have\n10 data points, and you want to\nchoose a value p. Just to make sure we're\nall following along",
    "start": "3159320",
    "end": "3165740"
  },
  {
    "text": "with what we're\ntrying to do here, you're assuming that all the\ndata points are Bernoulli.",
    "start": "3165740",
    "end": "3171030"
  },
  {
    "text": "You're assuming they all\nshare the same value p. And we don't know what p is.",
    "start": "3171030",
    "end": "3176090"
  },
  {
    "text": "What is pMLE if these\nwere your data points? And I give you a few options.",
    "start": "3176090",
    "end": "3182520"
  },
  {
    "text": "You can say that MLE\nsays that p should be 1. You can say MLE is\nthat p should be 0.5.",
    "start": "3182520",
    "end": "3187730"
  },
  {
    "text": "MLE says that p should be 0.8. And MLE says p should be 0.2. Talk about with the\nperson next to you.",
    "start": "3187730",
    "end": "3193340"
  },
  {
    "text": "See if you can answer this. But also, as always,\ntry and come up with a really good\nquestion because I bet other people in the class\nwould benefit from that too.",
    "start": "3193340",
    "end": "3199830"
  },
  {
    "text": "So think about this. A quick check, what does\nMLE of a Bernoulli tell us? ",
    "start": "3199830",
    "end": "3268530"
  },
  {
    "text": "OK. In the note of C-- no, I'm just joking.",
    "start": "3268530",
    "end": "3273540"
  },
  {
    "text": "OK. Yay or nay? Nay.",
    "start": "3273540",
    "end": "3279099"
  },
  {
    "text": "Nay. Yay. Nay.",
    "start": "3279100",
    "end": "3285153"
  },
  {
    "text": "[LAUGHS] People really felt\nsad for either, like, nay.",
    "start": "3285153",
    "end": "3290460"
  },
  {
    "text": "We wanted it to be\nyou, but it's not you. Almost ever in this class. Yeah, what we're saying\nis a good estimate",
    "start": "3290460",
    "end": "3298020"
  },
  {
    "text": "for your parameter is\nsum up all your values. In this case, there's 10 values. If you add them\nup, you'll get 8,",
    "start": "3298020",
    "end": "3304647"
  },
  {
    "text": "and then divided by n, which is\nhow many data points you have. And that's the MLE estimate. For what it's worth,\na beta distribution",
    "start": "3304647",
    "end": "3311340"
  },
  {
    "text": "would give you a much richer\nrepresentation of your belief in the probability. So MLE's, maybe we can recognize\none of the limitations here.",
    "start": "3311340",
    "end": "3318940"
  },
  {
    "text": "It doesn't give us a rich\nrepresentation of expressing how certain we are. Instead, it's just\ngiving a single number.",
    "start": "3318940",
    "end": "3325160"
  },
  {
    "text": "It's just coming with a\nsingle number estimate. And that's what MLE does. Based on data and a\nmodel, it will give you",
    "start": "3325160",
    "end": "3330549"
  },
  {
    "text": "just a single number back. OK, yay, you guys got it right. Yay. [LAUGHS] And we could have\ngotten it by our MLE estimate.",
    "start": "3330550",
    "end": "3342050"
  },
  {
    "text": "I want to take a moment, though,\nand talk about likelihood on its own. So likelihood was a step\nthat we used to come up",
    "start": "3342050",
    "end": "3349970"
  },
  {
    "text": "with our prediction for theta. But it's an interesting\nstep on its own because it says how\nlikely is the data",
    "start": "3349970",
    "end": "3355770"
  },
  {
    "text": "in the case of our\nparticular power setting to our parameters.",
    "start": "3355770",
    "end": "3362220"
  },
  {
    "text": "And the likelihood, if\nyou were to write it down, it says take p.",
    "start": "3362220",
    "end": "3368640"
  },
  {
    "text": "And we saw eight examples of 1. So that will be raised\nto itself eight times. And we saw two examples of 0.",
    "start": "3368640",
    "end": "3374109"
  },
  {
    "text": "So you'll see minus p two times. And that's what\nthe likelihood is. And we're going to\nchoose the p's that make",
    "start": "3374110",
    "end": "3379290"
  },
  {
    "text": "this as large as possible. And the claim is that this\nis the value of p which makes this as large as possible.",
    "start": "3379290",
    "end": "3386430"
  },
  {
    "text": "Just a little bit of intuition\nbut not that important. What is really important\nis this formula.",
    "start": "3386430",
    "end": "3391943"
  },
  {
    "text": "You guys are going\nto use it in section. You're going to use it\non your problem set. You're going to use\nit on the final exam. You will need to\nknow this MLE formula",
    "start": "3391943",
    "end": "3398730"
  },
  {
    "text": "for starting out with\nsome data and ending up with an estimate for parameters.",
    "start": "3398730",
    "end": "3404160"
  },
  {
    "text": "Yeah, OK, there it is. This is like our little MLE. And we're so proud. You're like, welcome\nto the world, MLE.",
    "start": "3404160",
    "end": "3410910"
  },
  {
    "text": "I do want to mention that\nit's incredibly general. The point of this demo\nI'm about to show you",
    "start": "3410910",
    "end": "3418240"
  },
  {
    "text": "where we start with\ndata-- and we're going to estimate the\nparameters for mean and variance using MLE.",
    "start": "3418240",
    "end": "3424270"
  },
  {
    "text": "The point of this is actually to\ngive you two pieces of insight. And the most important\none being that MLE",
    "start": "3424270",
    "end": "3430540"
  },
  {
    "text": "works even if you have\nmore than one number you have to estimate. So for the Gaussian,\nyou have to estimate",
    "start": "3430540",
    "end": "3435850"
  },
  {
    "text": "both mean and variance. So just to be clear, my\nfirst little example, this",
    "start": "3435850",
    "end": "3442662"
  },
  {
    "text": "was the game we are playing. We're trying to choose\nmean and the variance which made our data look as\nlikely as possible.",
    "start": "3442663",
    "end": "3449810"
  },
  {
    "text": "And now, we want to do that\na little bit more precisely. We'd like to not have\nto use guess and check.",
    "start": "3449810",
    "end": "3457170"
  },
  {
    "text": "Starts out with saying, how\nlikely is a single data point if you tell me the parameters?",
    "start": "3457170",
    "end": "3462640"
  },
  {
    "text": "And that is just going to be the\nprobability density function. This step is almost always\njust plug in the PDF, the PMF,",
    "start": "3462640",
    "end": "3469109"
  },
  {
    "text": "or the joint distribution. Then we're going to\ntry to figure out what are our parameters.",
    "start": "3469110",
    "end": "3474950"
  },
  {
    "text": "To do that, we first\nwrite log-likelihood. And then we're going\nto try and figure out what is the values of mean and\nvariance which maximize this.",
    "start": "3474950",
    "end": "3483770"
  },
  {
    "text": "When we write down\nlog-likelihood, it's just going to be the\nsum of the log of each",
    "start": "3483770",
    "end": "3489260"
  },
  {
    "text": "of the likelihoods. Who feels like doing this log? Ugh. But then, you look at\nit, you're like, OK,",
    "start": "3489260",
    "end": "3495170"
  },
  {
    "text": "that'll pull this term apart. And the log of e\nto the something is actually quite nice. And when you do\nthis, it turns out",
    "start": "3495170",
    "end": "3501320"
  },
  {
    "text": "to be a negative log\nof that constant. And that whole e disappears. And you're just\nleft with what e was",
    "start": "3501320",
    "end": "3507619"
  },
  {
    "text": "raised to the power of when\nyou did the log, like logs and e's like to cancel out. So in fact, the log\nof this expression",
    "start": "3507620",
    "end": "3514279"
  },
  {
    "text": "led to something quite\nnice to work with. So at this point, we have the\nlog likelihood for a normal.",
    "start": "3514280",
    "end": "3520960"
  },
  {
    "text": "And the real learning\nexperience for this one is-- oh, by the way, and then\nI just distribute the sum.",
    "start": "3520960",
    "end": "3527990"
  },
  {
    "text": "So this expression, just\na little bit of algebra gives you this one. When you move the sums,\nhave this sum on this side",
    "start": "3527990",
    "end": "3533079"
  },
  {
    "text": "and this sum on this side. The real takeaway I\nwant you guys to get from this next example is\nwhen you have log-likelihood,",
    "start": "3533080",
    "end": "3541579"
  },
  {
    "text": "if you have more\nthan one parameter, you can choose them\nsimultaneously. If you're using our original\nidea of optimization,",
    "start": "3541580",
    "end": "3548810"
  },
  {
    "text": "you derive them with\nrespect to each parameter. So you take this expression,\nderive it with respect to mu.",
    "start": "3548810",
    "end": "3555360"
  },
  {
    "text": "And then you take this\nlog-likelihood expression and derive it with\nrespect to sigma, which is your other parameter.",
    "start": "3555360",
    "end": "3561150"
  },
  {
    "text": "If you derive it\nwith respect to mu, you'll get this equation, which\nyou can set to be equal to 0.",
    "start": "3561150",
    "end": "3566390"
  },
  {
    "text": "If you derive it with\nrespect to sigma, don't worry too much\nabout the derivations. Wolfram Alpha could\nhelp you there.",
    "start": "3566390",
    "end": "3571418"
  },
  {
    "text": "But if you did it,\nyou would end up with this derivation which you\ncould then set to equal to 0.",
    "start": "3571418",
    "end": "3577430"
  },
  {
    "text": "So here, I have my\nlog-likelihood check. Then I differentiate\nlog-likelihood with respect",
    "start": "3577430",
    "end": "3583040"
  },
  {
    "text": "to each parameter\nset to equal to 0. And one final thing\nwe could do is we could solve for both mu and\nsigma in these two equations.",
    "start": "3583040",
    "end": "3593160"
  },
  {
    "text": "You have two unknowns,\ntwo equations. You're like, hey, Chris,\ndon't I have more unknowns? These are not unknowns.",
    "start": "3593160",
    "end": "3598359"
  },
  {
    "text": "These are your data points. So n is known. Xi's are known. Those are all knowns. The only two unknowns we\nhave are mu and sigma.",
    "start": "3598360",
    "end": "3605079"
  },
  {
    "text": "And if you solve\nthem simultaneously, you would get this\nresult. You would",
    "start": "3605080",
    "end": "3611339"
  },
  {
    "text": "get that mean should be equal\nto-- wait a second, that answer",
    "start": "3611340",
    "end": "3616980"
  },
  {
    "text": "we always get, which is\nthe sum over all your data points divided by n. And if you were to solve\nfor this other equation,",
    "start": "3616980",
    "end": "3623850"
  },
  {
    "text": "plugging in your\nanswer for mean, you would end up\nwith your estimate",
    "start": "3623850",
    "end": "3629040"
  },
  {
    "text": "for MLE's variance is\ngoing to be equal to-- take each of your data points. Subtract off your mean estimate.",
    "start": "3629040",
    "end": "3635040"
  },
  {
    "text": "Square it. And then divide that by n. My big point here is not\nhow you do this optimization",
    "start": "3635040",
    "end": "3643910"
  },
  {
    "text": "with setting it equal to 0. My big point is if you\nhave multiple parameters, it's not a problem. You will have to derive\nlog-likelihood with respect",
    "start": "3643910",
    "end": "3652160"
  },
  {
    "text": "to each parameter. But then any\noptimization algorithm can take it from there. And the setting equal to 0\ntrick works in this case.",
    "start": "3652160",
    "end": "3659240"
  },
  {
    "text": "But another trick I'll teach\nyou in a bit works as well. I did also want to highlight\none funny thing here.",
    "start": "3659240",
    "end": "3667490"
  },
  {
    "text": "Who thinks that this is a\npretty reasonable statement? If you're guessing the\nmean based on data,",
    "start": "3667490",
    "end": "3674300"
  },
  {
    "text": "add all your data points\ntogether and divide it by n. Does that seem like a\npretty reasonable way to choose the mean? Yes.",
    "start": "3674300",
    "end": "3680420"
  },
  {
    "text": "The other claim\nthat MLE gives you is that if you want to\nchoose the variance, you use this equation.",
    "start": "3680420",
    "end": "3686390"
  },
  {
    "text": "And does anyone take any\nissue with this equation? ",
    "start": "3686390",
    "end": "3692130"
  },
  {
    "text": "Like, I take issue\nwith this equation. [CHUCKLES] Yeah?",
    "start": "3692130",
    "end": "3697660"
  },
  {
    "text": "I remember there being\nan equation last class or a couple classes ago where\nyou had to divide by n minus 1.",
    "start": "3697660",
    "end": "3704650"
  },
  {
    "text": "Absolutely. Divide it by n minus 1. You're right. And the claim was if you try and\nestimate the variance from data",
    "start": "3704650",
    "end": "3712599"
  },
  {
    "text": "and use this\nequation, you'll get a number, which is too small. Because your guess for the\nmean was going to be wrong.",
    "start": "3712600",
    "end": "3720369"
  },
  {
    "text": "You always guess\nstatistics wrong. And this one is going to be\ncloser to each of your values, as such you're going\nto think that you're",
    "start": "3720370",
    "end": "3726880"
  },
  {
    "text": "varying around the mean less. And it turns out this magical\nminus 1 solves it and makes it",
    "start": "3726880",
    "end": "3731950"
  },
  {
    "text": "so that you are not going to\nbe on average underestimating. We call this a biased estimator.",
    "start": "3731950",
    "end": "3737420"
  },
  {
    "text": "So this is a great guess\nfor mean based on data. But that's not a\nreally great guess",
    "start": "3737420",
    "end": "3742540"
  },
  {
    "text": "for variance based on data. You're like, MLE,\nyou led us astray. Yeah, sometimes it will do that.",
    "start": "3742540",
    "end": "3750290"
  },
  {
    "text": "So any time we have a variance\nor standard deviation, if we're not dividing\nby n minus 1, it's considered biased\nbecause by definition?",
    "start": "3750290",
    "end": "3756700"
  },
  {
    "text": "Yeah, it will\nalways be too small. Does that work for\nany other estimators? Every estimator will have\nan unbiased equation.",
    "start": "3756700",
    "end": "3763710"
  },
  {
    "text": "And so for me, this is\nthe unbiased equation. And for variance,\nit's got a minus 1. And other estimators would need\nto have an unbiased equation",
    "start": "3763710",
    "end": "3770234"
  },
  {
    "text": "too. Is this unbiased equation? Uh, ooh, how about that one?",
    "start": "3770235",
    "end": "3776810"
  },
  {
    "text": "[LAUGHS] Anything that's not\nthis is the bias equation.",
    "start": "3776810",
    "end": "3781880"
  },
  {
    "text": "OK. Yes, yes. On the estimator, when\nyou say it's biased,",
    "start": "3781880",
    "end": "3789590"
  },
  {
    "text": "does it got to do with the\nsample size or something? I was thinking of using\nminus 1 music when",
    "start": "3789590",
    "end": "3795950"
  },
  {
    "text": "the sample size is smaller. So the minus 1 does\na magical thing. It just kind of\ncounteracts the fact",
    "start": "3795950",
    "end": "3801590"
  },
  {
    "text": "that this is going to\nbe a bad guess of mean. It's going to be too\nclose to your data points. And the minus 1 really\nbalances that out.",
    "start": "3801590",
    "end": "3807529"
  },
  {
    "text": "But you said something\nvery important. You said sample size,\nwhich is a point I'd like to get deeper into.",
    "start": "3807530",
    "end": "3813150"
  },
  {
    "text": "If you have three data\npoints, this is pretty wrong. If you have 10,000 data points\nfor getting the minus 1,",
    "start": "3813150",
    "end": "3820080"
  },
  {
    "text": "it really doesn't\nmatter that much. And what I'm going to\nclaim in a little bit is that while MLE\ndoesn't always give you",
    "start": "3820080",
    "end": "3825990"
  },
  {
    "text": "the perfect estimate\nfor your parameters, as your number of data points\ngets larger and larger, it will get closer and closer.",
    "start": "3825990",
    "end": "3831390"
  },
  {
    "text": "It will converge to\nthe right number. I want to drive this point\nhome with a hilarious example.",
    "start": "3831390",
    "end": "3837119"
  },
  {
    "text": "Let's say you wanted\nto do MLE, and you wanted to choose a uniform. What parameters\ndoes a uniform have?",
    "start": "3837120",
    "end": "3845480"
  },
  {
    "text": "Alpha and beta. Alpha and beta. What do those represent? Starting and ending.",
    "start": "3845480",
    "end": "3850850"
  },
  {
    "text": "Yeah, the smallest\nvalue that uniform can take on and\nthe largest value. If you wanted to do MLE,\nyou'd start with data.",
    "start": "3850850",
    "end": "3857299"
  },
  {
    "text": "And we'd tell you what values\nyou have for alpha and beta. It turns out I actually\nconstructed these ones with alpha equals 0\nand beta equals 1.",
    "start": "3857300",
    "end": "3864350"
  },
  {
    "text": "You can know. But normally, you don't\nknow what these are, and you're trying to guess. If you change alpha and you\nlook at the likelihood value,",
    "start": "3864350",
    "end": "3873990"
  },
  {
    "text": "notice how it goes up, up,\nup, up, up, up, up, up, up. And then it plummets to 0. Why is that?",
    "start": "3873990",
    "end": "3879650"
  },
  {
    "text": "Well, if you say the minimum\nvalue that your data points can take on is 0.2 and then\nyou observe a 0.15,",
    "start": "3879650",
    "end": "3885680"
  },
  {
    "text": "this looks impossible. So the likelihood\nwill just get bigger. But if you get--",
    "start": "3885680",
    "end": "3890840"
  },
  {
    "text": "as the minimum value gets larger\nthen your smallest value here, likelihood of your\ndata goes to 0.",
    "start": "3890840",
    "end": "3896180"
  },
  {
    "text": "Same thing happens\nwith the maximum. The maximum will have a pretty\nhigh likelihood, and always",
    "start": "3896180",
    "end": "3901653"
  },
  {
    "text": "a positive likelihood. But if your maximum is\nsmaller than the large value you saw, then it says\nyour data is impossible",
    "start": "3901653",
    "end": "3908810"
  },
  {
    "text": "given this parameter. And impossible\nand probability is expressed with a probability\nof 0 or a likelihood of 0.",
    "start": "3908810",
    "end": "3915560"
  },
  {
    "text": "So if you were to\nchoose MLE values, what value do you think\nyou would choose for alpha?",
    "start": "3915560",
    "end": "3920750"
  },
  {
    "text": "You'd say that my alpha\nhere is equal to 0.15.",
    "start": "3920750",
    "end": "3926850"
  },
  {
    "text": "That is the alpha which\nmade likelihood the highest. What would you choose for\nthe max value here for beta?",
    "start": "3926850",
    "end": "3935300"
  },
  {
    "text": "What is the argmax\nof beta in this case? Of likelihood with\nrespect to beta?",
    "start": "3935300",
    "end": "3942089"
  },
  {
    "text": "0.75. I told you that the\ntrue alpha and beta that I use to generate\nthis data was 0 and 1.",
    "start": "3942090",
    "end": "3950470"
  },
  {
    "text": "But when it had to choose\nits alpha and beta, it chose different values.",
    "start": "3950470",
    "end": "3955790"
  },
  {
    "text": "In fact, it chose alpha\nto be the smallest value in this list. And it chose beta to be the\nlargest value in this list.",
    "start": "3955790",
    "end": "3962750"
  },
  {
    "text": "I want to show you this\nexample to drive home a point. ",
    "start": "3962750",
    "end": "3969440"
  },
  {
    "text": "Doo, doo, doo, doo, doo. It's this idea of when\nyour numbers are small,",
    "start": "3969440",
    "end": "3976180"
  },
  {
    "text": "it does this thing\nthat we intuitively would call overfitting. It's not trying to generalize\nto any data it hasn't seen.",
    "start": "3976180",
    "end": "3983690"
  },
  {
    "text": "It's only trying to\ndescribe the data it has. So if it's trying to\ndescribe these data points, these are pretty reasonable\nchoices for alpha and beta.",
    "start": "3983690",
    "end": "3990820"
  },
  {
    "text": "But if you're trying\nto describe data points you haven't seen as well,\nthen these aren't really good choices. Because you want to use\nsomething a little bit smaller",
    "start": "3990820",
    "end": "3997620"
  },
  {
    "text": "than what the smallest\nvalue you haven't seen. But anyways, some\nqualities of MLE. In the limit as data\npoints goes to infinity,",
    "start": "3997620",
    "end": "4005020"
  },
  {
    "text": "it's the best thing. It's potentially biased. As we've seen\nthrough the uniform and we've seen\nthrough the Gaussian,",
    "start": "4005020",
    "end": "4010800"
  },
  {
    "text": "it might choose bad estimators\nfor small data sizes. But also, it's one of the\nmost popular things ever used",
    "start": "4010800",
    "end": "4018270"
  },
  {
    "text": "in practice for\nparameter estimation. OK, I'm going to show\nyou a few examples.",
    "start": "4018270",
    "end": "4024945"
  },
  {
    "text": "Then I'll give us a\nminute to think about it. And then we'll spend 10 minutes\ndoing one problem together. On your problem set, you're\ngoing to have to do this.",
    "start": "4024945",
    "end": "4031920"
  },
  {
    "text": "You're going to do\nmaximum likelihood estimation of the wind. I'm going to give you a\nprobability density function.",
    "start": "4031920",
    "end": "4038290"
  },
  {
    "text": "And I'm giving you data. And I say, choose the parameter\ntheta based off this data. And you're going\nto do exactly this.",
    "start": "4038290",
    "end": "4044075"
  },
  {
    "text": "Write the likelihood. Write the log-likelihood. And then figure out which value\nmaximizes the log-likelihood.",
    "start": "4044075",
    "end": "4050109"
  },
  {
    "text": "In section, you're going\nto do an old exam problem. On the final exam,\nat one point, I",
    "start": "4050110",
    "end": "4055120"
  },
  {
    "text": "said, here is the distribution\nfor length of menstrual cycles. If you take a particular\nperson who has menstruation",
    "start": "4055120",
    "end": "4061510"
  },
  {
    "text": "and they give you\nsome data, could you choose the parameters that\nwould make their data look",
    "start": "4061510",
    "end": "4066910"
  },
  {
    "text": "as likely as possible? This was last time. I gave a CS 109 final.",
    "start": "4066910",
    "end": "4073840"
  },
  {
    "text": "I put this MLE question. I said, how long\na phone lasts is known to be coming\nfrom this thing called",
    "start": "4073840",
    "end": "4079360"
  },
  {
    "text": "the reliability distribution? It's a probability distribution\nwe've never seen before. But if I gave you data\npoints, like n data points,",
    "start": "4079360",
    "end": "4086150"
  },
  {
    "text": "could you choose the single\nparameter, which is alpha? And then one of my\nfavorites is this one.",
    "start": "4086150",
    "end": "4092230"
  },
  {
    "text": "And this problem, I have put\nit into our lecture for today.",
    "start": "4092230",
    "end": "4097970"
  },
  {
    "text": "If you go to today's lecture,\nwe have this exact same problem which says, I give you\nall these data points.",
    "start": "4097970",
    "end": "4104799"
  },
  {
    "text": "And you know what\nthese data points are? Their sizes, radius of sand\non the beach, which is I",
    "start": "4104800",
    "end": "4111577"
  },
  {
    "text": "got really like artistic. I was like, I want\nto make art which fits the distribution\nof the sand size",
    "start": "4111578",
    "end": "4116689"
  },
  {
    "text": "on the beach, which is\nreally not that important. But what is important is you\nmight want to fit a pareto distribution based off data.",
    "start": "4116689",
    "end": "4122100"
  },
  {
    "text": "So based off these data points,\ncould you choose the alpha for this particular\nrandom variable,",
    "start": "4122100",
    "end": "4128255"
  },
  {
    "text": "which is called the pareto? We didn't study as one of\nour core random variables. But using MLE, you\ncould figure this out.",
    "start": "4128255",
    "end": "4134659"
  },
  {
    "text": "This is the sort of\nthing you could do, sort of thing that could\nbe on your final exam. In fact, this was\nonce on a final exam,",
    "start": "4134660",
    "end": "4140660"
  },
  {
    "text": "to give you an idea. I'm going to give you guys\nyour pedagogical pause. Take a minute. And then I'd like to show you\nwhat this answer looks like.",
    "start": "4140660",
    "end": "4147157"
  },
  {
    "text": "So take a minute\nand talk about this with the person next to you. Or just take a break\nand think, whoa, MLE,",
    "start": "4147157",
    "end": "4153710"
  },
  {
    "text": "what a time to be alive. ",
    "start": "4153710",
    "end": "4225540"
  },
  {
    "text": "If I told you all\nof these data points were pulled from this\nprobability distribution",
    "start": "4225540",
    "end": "4230670"
  },
  {
    "text": "and I said choose an alpha--  now. ",
    "start": "4230670",
    "end": "4240869"
  },
  {
    "text": "I wrote this up and\nwhat you could do. So you can check along at home. But the answer to this\nstarts with saying,",
    "start": "4240870",
    "end": "4248320"
  },
  {
    "text": "OK, I'm going to figure\nout what alpha is. And the way to figure out what\nalpha is going to be using MLE.",
    "start": "4248320",
    "end": "4255599"
  },
  {
    "text": "But before I jump into\nthis, any questions come up during your\npedagogical pause? ",
    "start": "4255600",
    "end": "4263960"
  },
  {
    "text": "OK. How could you choose this alpha? What's the first thing\nthat you have to do?",
    "start": "4263960",
    "end": "4271000"
  },
  {
    "text": "The first thing you\nwould have to do is you'd have to say, well, I'm\ntold the probability density",
    "start": "4271000",
    "end": "4277090"
  },
  {
    "text": "function for one point. But the first\nthing I really need is the likelihood function.",
    "start": "4277090",
    "end": "4282400"
  },
  {
    "text": "There it goes. So the likelihood function--\nand I often think of it as a function of my parameter.",
    "start": "4282400",
    "end": "4288190"
  },
  {
    "text": "Why? Because as you change\nyour parameter, the likelihood\nfunction will change. Since all my data\npoints are independent,",
    "start": "4288190",
    "end": "4294520"
  },
  {
    "text": "it's going to be the\nproduct over all my data points of the likelihood\nof any one data",
    "start": "4294520",
    "end": "4299920"
  },
  {
    "text": "point in the presence\nof this parameter. So if I told you alpha was 5,\nand then you took all these",
    "start": "4299920",
    "end": "4305050"
  },
  {
    "text": "data points, you could loop\nover each of these Xi's. And you could plug\nthem into here. And you could get that.",
    "start": "4305050",
    "end": "4310960"
  },
  {
    "text": "This is going to be equal\nto alpha divided by-- let me see if I get this right.",
    "start": "4310960",
    "end": "4317720"
  },
  {
    "text": "x to the power of alpha plus 1. But I don't just write x.",
    "start": "4317720",
    "end": "4322790"
  },
  {
    "text": "I write Xi to represent that. First, I take the\nfirst data point. Then I take the\nsecond data point.",
    "start": "4322790",
    "end": "4328160"
  },
  {
    "text": "And each time, I plug them into\nthe probability mass function. That's the likelihood. But we don't really\nneed the likelihood.",
    "start": "4328160",
    "end": "4334640"
  },
  {
    "text": "What we really need\nis the log-likelihood. So remember, the log likes to\ngo and destroy this whole thing,",
    "start": "4334640",
    "end": "4340820"
  },
  {
    "text": "and go inside. And then you get\nthe log of whatever",
    "start": "4340820",
    "end": "4346850"
  },
  {
    "text": "your probability\ndensity function was as one of your steps in MLE.",
    "start": "4346850",
    "end": "4352880"
  },
  {
    "text": "And if you solve this, it\nlooks a little scary at first.",
    "start": "4352880",
    "end": "4359090"
  },
  {
    "text": "But then you use\nyour powers of logs. And you're like, OK, this\nis going to be log of alpha. This is going to be minus\nalpha plus 1 times log of Xi.",
    "start": "4359090",
    "end": "4368030"
  },
  {
    "text": " And at this point, we've got\nour log-likelihood function.",
    "start": "4368030",
    "end": "4374780"
  },
  {
    "text": "If we wanted to choose\na value for alpha, what we'd want to\nreally find is what's",
    "start": "4374780",
    "end": "4380300"
  },
  {
    "text": "the derivative of\nthis log-likelihood with respect to alpha. Because what we really\nwant to figure out",
    "start": "4380300",
    "end": "4386630"
  },
  {
    "text": "is which alpha makes\nthis the largest. We want to argmax this over\nall the arguments alpha.",
    "start": "4386630",
    "end": "4393600"
  },
  {
    "text": "So we have to derive this\nwith respect to alpha. So that's going to be equal to\nthe sum from i equals 1 to n.",
    "start": "4393600",
    "end": "4401690"
  },
  {
    "text": "What's the derivative of this\nterm with respect to alpha? 1 over alpha. 1 over alpha, yeah.",
    "start": "4401690",
    "end": "4408139"
  },
  {
    "text": "Fantastic. And what's the derivative\nof this term with respect to alpha? Eeh.",
    "start": "4408140",
    "end": "4413268"
  },
  {
    "text": "Well, if you expand, this\nwould be negative alpha times log of Xi plus log of Xi.",
    "start": "4413268",
    "end": "4418580"
  },
  {
    "text": "And only one of these\nhas an alpha in it. This would end up being\nnegative alpha times log of Xi.",
    "start": "4418580",
    "end": "4424730"
  },
  {
    "text": "And if you derive that, it just\nbecomes negative log of Xi.",
    "start": "4424730",
    "end": "4429890"
  },
  {
    "text": "So that's the derivative. And once you have the\nderivative, you're laughing. Lots of optimization\ntechniques end with this.",
    "start": "4429890",
    "end": "4437150"
  },
  {
    "text": "If you can write into a\ncomputer an expression for the derivative, you\ncould just pass it off to most commercial optimizers.",
    "start": "4437150",
    "end": "4443740"
  },
  {
    "text": "And it would just give\nyou back a value of alpha that maximizes this. We have been using this trick\nof setting this derivative equal",
    "start": "4443740",
    "end": "4450510"
  },
  {
    "text": "to 0. And if you set this\nderivative equal to 0-- I'm not going to\nwrite it on the board.",
    "start": "4450510",
    "end": "4455730"
  },
  {
    "text": "But just to be clear, you\ngot the log-likelihood. Then we took the derivative\nof log-likelihood,",
    "start": "4455730",
    "end": "4462090"
  },
  {
    "text": "and we got this expression. Oh, and just to be clear,\nthis expression, I often",
    "start": "4462090",
    "end": "4470010"
  },
  {
    "text": "will put the sum as\nfar inside as I can. And that would make this\nequal to n divided by alpha.",
    "start": "4470010",
    "end": "4477210"
  },
  {
    "text": "Because this is just 1\nover alpha added n times, minus the sum of log of Xi's.",
    "start": "4477210",
    "end": "4486570"
  },
  {
    "text": " Now, as I said, this is a\npretty satisfying answer.",
    "start": "4486570",
    "end": "4494460"
  },
  {
    "text": "Most times I give exams,\nthis is the final answer that I'm looking for. I'm like, give me an\nexpression for derivative. Because if you can do\nderivative, you can do argmax.",
    "start": "4494460",
    "end": "4501607"
  },
  {
    "text": "But to just drive\nthat point home, we can set the\nderivative equal to 0. And then we can solve for alpha.",
    "start": "4501607",
    "end": "4507570"
  },
  {
    "text": "And if you solve for alpha,\nyou would get this expression. And we've done this a\nwhole bunch in class.",
    "start": "4507570",
    "end": "4512929"
  },
  {
    "text": "We've done this to the point\nwhere we can get an expression. But we haven't\nactually coded it up. If you set this\nderivative equal to 0,",
    "start": "4512930",
    "end": "4519675"
  },
  {
    "text": "can somebody help me\nread this off the board? What's my expression for alpha? And I'll put it with a\nlittle cute hat on it.",
    "start": "4519675",
    "end": "4525720"
  },
  {
    "text": "OK, what is it? n divided by--",
    "start": "4525720",
    "end": "4530745"
  },
  {
    "text": " I can't see it. You have to help me. Log Xi.",
    "start": "4530745",
    "end": "4536430"
  },
  {
    "text": "Is this exponential? No? OK, I know it is. It's log Xi's and\ni equals 1 to n.",
    "start": "4536430",
    "end": "4544710"
  },
  {
    "text": "You know, this is\njust an expression. It doesn't seem\nto mean that much.",
    "start": "4544710",
    "end": "4550020"
  },
  {
    "text": "What I think really\ndrives it home is we could put this into code. You can take these observations.",
    "start": "4550020",
    "end": "4555630"
  },
  {
    "text": "And we could write\nour estimate of alpha. And it takes these observations.",
    "start": "4555630",
    "end": "4561500"
  },
  {
    "text": "And we could try\nand write some code here that instead of\nreturning 0, estimates alpha.",
    "start": "4561500",
    "end": "4567095"
  },
  {
    "text": " So if I had to code\nthat expression up,",
    "start": "4567095",
    "end": "4574270"
  },
  {
    "text": "I would probably do something\nlike calculate my log sum. So I'd start at equal to 0\nfor Xi in my observations.",
    "start": "4574270",
    "end": "4584110"
  },
  {
    "text": "I'd say log sum plus\nequals math dot log of Xi. So this calculates\nthat expression.",
    "start": "4584110",
    "end": "4591969"
  },
  {
    "text": "And I'll take n divided\nby that expression. What is n? N is equal to the length\nof my observations.",
    "start": "4591970",
    "end": "4599739"
  },
  {
    "text": "And I would return n\ndivided by log sum. And if I run this, it\ngives me back a number.",
    "start": "4599740",
    "end": "4607600"
  },
  {
    "text": "It says that my estimate for\nthis parameter alpha based on this data is equal to 2.7.",
    "start": "4607600",
    "end": "4614656"
  },
  {
    "text": "Oh, come on. Ugh. OK, great.",
    "start": "4614656",
    "end": "4619929"
  },
  {
    "text": "So this expression,\nyou could code it up. And this would give\nyou equal to 2.7,",
    "start": "4619930",
    "end": "4625929"
  },
  {
    "text": "would be my estimate for alpha\ngiven this particular data. A lot of times,\nwe use these Xi's.",
    "start": "4625930",
    "end": "4631060"
  },
  {
    "text": "But I don't want you to\nforget that in reality, Xi's will be observations. It's your data set from which\nyou're making your estimate.",
    "start": "4631060",
    "end": "4638590"
  },
  {
    "text": "OK. That is the end of\nour wonderful journey. I know that was hard. But what a cornerstone\nof machine learning",
    "start": "4638590",
    "end": "4645160"
  },
  {
    "text": "that you've learned today\nusing the Disney analogy. That brings us to the end\nof our wonderful movie.",
    "start": "4645160",
    "end": "4651110"
  },
  {
    "text": "But of course, there's\nalways a sequel. Come back on Wednesday,\nand we will continue this wonderful conversation. You guys are fantastic.",
    "start": "4651110",
    "end": "4656920"
  },
  {
    "text": "Go to section practice, MLE. Do it on your problem set. Have a good time. I miss you. We will see you on Wednesday.",
    "start": "4656920",
    "end": "4664080"
  },
  {
    "start": "4664080",
    "end": "4669000"
  }
]