[
  {
    "text": "Welcome to the validation section.",
    "start": "5000",
    "end": "8680"
  },
  {
    "text": "So we've seen so far several different predictors,",
    "start": "17690",
    "end": "24050"
  },
  {
    "text": "which given an x, can predict for us what the corresponding value of y should be.",
    "start": "24050",
    "end": "32820"
  },
  {
    "text": "And I want to talk now about how we're going to evaluate different predictors.",
    "start": "33070",
    "end": "40130"
  },
  {
    "text": "We have seen k-nearest neighbor predictors, we've seen tree-based predictors, we've seen neural network predictors,",
    "start": "40130",
    "end": "47075"
  },
  {
    "text": "we've seen linear predictors. And so somebody has come along and said,",
    "start": "47075",
    "end": "52700"
  },
  {
    "text": "well, here's my favorite predictor which I designed. And, uh, you've got to decide,",
    "start": "52700",
    "end": "57920"
  },
  {
    "text": "well, should I use that predictor or should I use some other predictor?",
    "start": "57920",
    "end": "62309"
  },
  {
    "text": "So one of the key ideas is that we need a- what's called a performance metric.",
    "start": "64480",
    "end": "71150"
  },
  {
    "text": "And what it is is it's a measure of how large prediction errors are.",
    "start": "71150",
    "end": "77930"
  },
  {
    "text": "And so what we can do is we can say, well, we've got a dataset, x_1 through x_n, and y_1 through y_n,",
    "start": "77930",
    "end": "84890"
  },
  {
    "text": "and to reach one of those data points, we can evaluate the predictor g at the ith data point,",
    "start": "84890",
    "end": "95180"
  },
  {
    "text": "x_i to get an prediction y hat i. And I ask the question,",
    "start": "95180",
    "end": "102134"
  },
  {
    "text": "how close is y hat_ i to y_i? And the performance metric is going to be the measure of how close y hat_i is to y_i.",
    "start": "102135",
    "end": "113855"
  },
  {
    "text": "And normally it's designed so that the smaller the metric,",
    "start": "113855",
    "end": "119270"
  },
  {
    "text": "the better the prediction performance. It's an error measure.",
    "start": "119270",
    "end": "124890"
  },
  {
    "text": "Some people call it prediction performance metric, the prediction error as a result.",
    "start": "125590",
    "end": "132180"
  },
  {
    "text": "So there are a few which are very commonly used. The first one is the mean square error.",
    "start": "135620",
    "end": "143960"
  },
  {
    "text": "So this here, uh, is the two-norm. Let me just mark it here.",
    "start": "143960",
    "end": "151555"
  },
  {
    "text": "See the subscript 2 there, what it means is it means- means the following.",
    "start": "151555",
    "end": "158510"
  },
  {
    "text": "If you give me a vector x, then that's equal to the sum from i is",
    "start": "158510",
    "end": "169040"
  },
  {
    "text": "1 up to n x_i squared or square rooted.",
    "start": "169040",
    "end": "177019"
  },
  {
    "text": "And the subscript two there indicates which particular norm we are using.",
    "start": "177020",
    "end": "182255"
  },
  {
    "text": "Um, there are other norms that we might use with different subscripts. So for example, we might use the 1-norm which is",
    "start": "182255",
    "end": "193260"
  },
  {
    "text": "just the absolute value of each of the components of x added up.",
    "start": "193260",
    "end": "203980"
  },
  {
    "text": "And we'll see a few other norms in this class as well as a few other performance metrics in this class.",
    "start": "205790",
    "end": "213290"
  },
  {
    "text": "So if y_i is a vector, an m dimensional vector,",
    "start": "213290",
    "end": "219590"
  },
  {
    "text": "then if you give me a particular y_i and a- and a particular corresponding prediction y hat_i,",
    "start": "219590",
    "end": "224959"
  },
  {
    "text": "I can compute its 2-norm as, uh, a measure of the error between y hat and y_i.",
    "start": "224960",
    "end": "233660"
  },
  {
    "text": "And then I can average those up, and, uh, so sum those up and divide by n. And that would be a- a prediction performance metric.",
    "start": "233660",
    "end": "245365"
  },
  {
    "text": "And of course, if y_i is just a scalar, so m is 1, then that's just the mean square error,",
    "start": "245365",
    "end": "252459"
  },
  {
    "text": "the mean squared difference between y_i and y hat_i. Very often instead of working with the mean square error,",
    "start": "252460",
    "end": "260600"
  },
  {
    "text": "we work with the root mean square error. That is the square root of 1 on n multiplied by the sum",
    "start": "260600",
    "end": "266690"
  },
  {
    "text": "from i is 1 to n of y hat_i minus y_i squared. Uh, which is, uh,",
    "start": "266690",
    "end": "273195"
  },
  {
    "text": "convenient because it has the same units as y. And, uh, another, uh,",
    "start": "273195",
    "end": "282940"
  },
  {
    "text": "very useful, uh, error metric or performance metric is the mean absolute error.",
    "start": "282940",
    "end": "290560"
  },
  {
    "text": "So that's 1 on n, the sum from i is 1 to n of the absolute value of y hat_i minus y of i.",
    "start": "290560",
    "end": "297275"
  },
  {
    "text": "If you have scalar positive y's, then we define the mean fractional error as the average of the absolute value of",
    "start": "297275",
    "end": "307040"
  },
  {
    "text": "y hat_i minus y_i divided by the minimum of y hat_i and y_i.",
    "start": "307040",
    "end": "313760"
  },
  {
    "text": "And this is something like an average percentage error. If y hat_i is greater than y_i,",
    "start": "313760",
    "end": "322055"
  },
  {
    "text": "then the absolute value of y hat_i minus y_i divided by the minimum of",
    "start": "322055",
    "end": "327770"
  },
  {
    "text": "y hat_i and y_i is the percentage by which y hat_i is greater than y_i.",
    "start": "327770",
    "end": "335190"
  },
  {
    "text": "There are many other common performance metrics. Uh, the median error, um,",
    "start": "336140",
    "end": "344000"
  },
  {
    "text": "I said are the ones very commonly used and we'll see many others in this class.",
    "start": "344000",
    "end": "349200"
  },
  {
    "text": "So now we've got a prediction performance metric that allows us to compare different predictors on a particular dataset.",
    "start": "349670",
    "end": "359600"
  },
  {
    "text": "So, for example, we might conclude that k-nearest neighbors with k is 7 does better than k-nearest neighbors with k is 12.",
    "start": "359600",
    "end": "371103"
  },
  {
    "text": "We might conclude that a particular neural network does better than a particular linear model.",
    "start": "371104",
    "end": "377600"
  },
  {
    "text": "Um, several things to notice about these kinds of conclusions,",
    "start": "377600",
    "end": "383240"
  },
  {
    "text": "they depend on several things. First of all, they depend on the performance metric. It is quite reasonable and quite common that you can have",
    "start": "383240",
    "end": "393199"
  },
  {
    "text": "a predictor that does better on one performance metric,",
    "start": "393200",
    "end": "398400"
  },
  {
    "text": "but does worse in a different performance metric- performance metric. So the neural network may do better than the linear model when using the RMS error,",
    "start": "398400",
    "end": "408395"
  },
  {
    "text": "but may do worse than the linear model when using the mean absolute error. So you got to choose the performance metric carefully so that it",
    "start": "408395",
    "end": "416660"
  },
  {
    "text": "corresponds to something that you actually care about.",
    "start": "416660",
    "end": "421500"
  },
  {
    "text": "Another important thing to notice about performance metrics and using them for analysis of predictors,",
    "start": "421740",
    "end": "429880"
  },
  {
    "text": "is that they are predictor agnostic. And that's a very good thing.",
    "start": "429880",
    "end": "435640"
  },
  {
    "text": "It means that when we evaluate the performance of a predictor,",
    "start": "435640",
    "end": "440650"
  },
  {
    "text": "we're not evaluating it using the same software that was used to develop it.",
    "start": "440650",
    "end": "446050"
  },
  {
    "text": "We're not evaluating it on the basis of the properties of the learning algorithm.",
    "start": "446050",
    "end": "451525"
  },
  {
    "text": "We're evaluating it on some objective notion of performance and anybody's predictor could be",
    "start": "451525",
    "end": "458620"
  },
  {
    "text": "evaluated using the same performance measure. So if somebody else has a method of generating a predictor,",
    "start": "458620",
    "end": "465889"
  },
  {
    "text": "even if you don't know what that method is, you can evaluate that predictor by evaluating the quality of their predictions.",
    "start": "465890",
    "end": "473905"
  },
  {
    "text": "And all that should matter is whether or not their predictions do better,",
    "start": "473905",
    "end": "479780"
  },
  {
    "text": "are better than the predictions given by some other predictor.",
    "start": "479780",
    "end": "484200"
  },
  {
    "text": "Another thing to notice about comparing predictors using a performance metric is that it depends on the dataset.",
    "start": "485470",
    "end": "492935"
  },
  {
    "text": "And so all you can do is you can use the performance metric to evaluate",
    "start": "492935",
    "end": "499940"
  },
  {
    "text": "the performance of a predictor on a particular dataset.",
    "start": "499940",
    "end": "504480"
  },
  {
    "text": "And that leads us to the question of generalization. Suppose we have a predictor and it performs well on the dataset that we use to train it.",
    "start": "510500",
    "end": "522375"
  },
  {
    "text": "Can we conclude that it's gonna perform well on a- a dataset that it's never seen before, unseen data?",
    "start": "522375",
    "end": "531585"
  },
  {
    "text": "And if it does, that's called generalization. Generalization is the ability of a predictor to perform well on unseen data.",
    "start": "531585",
    "end": "541380"
  },
  {
    "text": "And unseen here means that the data was not used to create a prediction model.",
    "start": "541380",
    "end": "547125"
  },
  {
    "text": "It was not part of the learning dataset. The person who developed",
    "start": "547125",
    "end": "552449"
  },
  {
    "text": "the predictor never looked at that data when they were developing the predictor.",
    "start": "552450",
    "end": "557740"
  },
  {
    "text": "So we'd like to answer the question of, when we can infer that good performance of a predictor on",
    "start": "557810",
    "end": "567570"
  },
  {
    "text": "one dataset implies that the perfo- the predictor will perform well on a second dataset?",
    "start": "567570",
    "end": "575010"
  },
  {
    "text": "And in order to do that, one would need to make some probabilistic assumptions. For example, one might say that both sets of data",
    "start": "575010",
    "end": "583395"
  },
  {
    "text": "are samples from some underlying probability distribution.",
    "start": "583395",
    "end": "588610"
  },
  {
    "text": "With some kind of probabilistic assumptions like that, we might well be able to conclude that performance on",
    "start": "588610",
    "end": "595490"
  },
  {
    "text": "one dataset says something about performance on another dataset. For example, if we have two sets of data,",
    "start": "595490",
    "end": "602894"
  },
  {
    "text": "both sampled from the same distribution, we might reasonably conclude that the mean of the first set of data and the mean",
    "start": "602895",
    "end": "610710"
  },
  {
    "text": "of the second set of data should be very close provided we have enough data.",
    "start": "610710",
    "end": "617085"
  },
  {
    "text": "So there is a- a framework for doing this kind of analysis,",
    "start": "617085",
    "end": "622710"
  },
  {
    "text": "we will not discuss it in this class, it's a more advanced topic. But instead what we will see is we will see some practi- practical methods.",
    "start": "622710",
    "end": "631830"
  },
  {
    "text": "Some methods for actually assessing whether or not the predictor that you've got actually generalizes.",
    "start": "631830",
    "end": "638140"
  },
  {
    "text": "The fundamental thing to do, of course, if you want to know whether your predictor generalizes to a new set of",
    "start": "641570",
    "end": "647009"
  },
  {
    "text": "data is to try it out on a new set of data. And so we think about having two sets of data.",
    "start": "647010",
    "end": "655205"
  },
  {
    "text": "One we call the training data or the in-sample data, and that's the dataset that we use in order to construct the predictor.",
    "start": "655205",
    "end": "664525"
  },
  {
    "text": "And then we have another set of data which we call the out-of-sample data, that's the unseen data on which we are going to test the performance of the predictor.",
    "start": "664525",
    "end": "674745"
  },
  {
    "text": "And if the predictor performs well on this unseen data,",
    "start": "674745",
    "end": "680145"
  },
  {
    "text": "we say the predictor generalizes. It makes good predictions on data it has never seen.",
    "start": "680145",
    "end": "687339"
  },
  {
    "text": "If it doesn't, we say it fails to generalize, it's over-fit.",
    "start": "687620",
    "end": "693180"
  },
  {
    "text": "This terminology over-fit is quite evocative and we will have much more to say about it.",
    "start": "693180",
    "end": "700630"
  },
  {
    "text": "Okay, here's a simple example. This is data downloaded from the federal government.",
    "start": "701150",
    "end": "706785"
  },
  {
    "text": "Uh, these- this plot shows the number of vehicle miles traveled in",
    "start": "706785",
    "end": "712725"
  },
  {
    "text": "each year over a range of years from 1970-2005.",
    "start": "712725",
    "end": "719279"
  },
  {
    "text": "And so vehicle miles traveled is the total number of miles traveled over the year by all of the vehicles.",
    "start": "719280",
    "end": "728279"
  },
  {
    "text": "Um, and on the left here, we have a subset of the data.",
    "start": "728280",
    "end": "733665"
  },
  {
    "text": "We have 12 data points. I will just highlight like that.",
    "start": "733665",
    "end": "740385"
  },
  {
    "text": "And what we do with those 12 data points is we fit a straight line predictor.",
    "start": "740385",
    "end": "745785"
  },
  {
    "text": "Y hat is Theta_1 plus Theta_2 times x. Where y hat is our prediction of the number of vehicle miles traveled,",
    "start": "745785",
    "end": "753405"
  },
  {
    "text": "and x is the year. And we'll just choose those parameters using least squares.",
    "start": "753405",
    "end": "758670"
  },
  {
    "text": "That gives us this nice straight line fit here,",
    "start": "758670",
    "end": "768240"
  },
  {
    "text": "which goes all the way up there.",
    "start": "768240",
    "end": "774670"
  },
  {
    "text": "And then we can say, well, um, let's look at the rest of the data. The data that we held aside,",
    "start": "774680",
    "end": "781274"
  },
  {
    "text": "which we only fit this straight line using 12 blue sample points.",
    "start": "781275",
    "end": "787155"
  },
  {
    "text": "We've got another 14 data points. This is these 14 data points right here,",
    "start": "787155",
    "end": "793450"
  },
  {
    "text": "and we can see that those 14 data points actually lie rather close to the straight line of fit.",
    "start": "804770",
    "end": "812910"
  },
  {
    "text": "And so the predictor actually does generalize to those additional points.",
    "start": "812910",
    "end": "818560"
  },
  {
    "text": "Of course, in this simple example, you can plot it all and see it all by eye.",
    "start": "818900",
    "end": "824385"
  },
  {
    "text": "It's very simple in two-dimensions, and of course, we'd like to be able to work with very large datasets and",
    "start": "824385",
    "end": "831945"
  },
  {
    "text": "a very large number of dimensions where it's no longer possible to do such simple plots.",
    "start": "831945",
    "end": "837585"
  },
  {
    "text": "And that's why we need to use the tool of a prediction error metric,",
    "start": "837585",
    "end": "843975"
  },
  {
    "text": "prediction performance metric in order to evaluate how well it perform- the predictor is doing on these different datasets.",
    "start": "843975",
    "end": "852880"
  },
  {
    "text": "And there's a specific approach to doing this. In fact there are several specific approaches to doing this kind of analysis.",
    "start": "859820",
    "end": "868095"
  },
  {
    "text": "The first one is called out-of-sample validation, and it goes like this.",
    "start": "868095",
    "end": "873389"
  },
  {
    "text": "Very often we don't have datasets which, uh, naturally fall into two categories.",
    "start": "873390",
    "end": "879630"
  },
  {
    "text": "Data that we use to, uh, do the training and data that we use to test.",
    "start": "879630",
    "end": "885180"
  },
  {
    "text": "And so what we do is we split the data. And so we'll download some dataset,",
    "start": "885180",
    "end": "890969"
  },
  {
    "text": "it may be a whole bunch of images, and we'd like to develop a predictor that can,",
    "start": "890969",
    "end": "897480"
  },
  {
    "text": "uh, look at an image and tell us whether that's a cat or that's a dog. And, uh, we take this data and we divide the data into two datasets,",
    "start": "897480",
    "end": "909584"
  },
  {
    "text": "the training set and a test set. And often we do that randomly so we'll use 80, uh,",
    "start": "909585",
    "end": "917670"
  },
  {
    "text": "an 80-20 split where 80% of the points we use for training and 20% we use for test.",
    "start": "917670",
    "end": "924209"
  },
  {
    "text": "Now we might use 90-10, it doesn't really matter very much. The exact, uh, split is not very",
    "start": "924210",
    "end": "930450"
  },
  {
    "text": "important and the results are quite insensitive to that. What we do with the training set is we train the predictor.",
    "start": "930450",
    "end": "938865"
  },
  {
    "text": "We use it to choose the predictor. And then the test set is used to validate the predictor,",
    "start": "938865",
    "end": "946140"
  },
  {
    "text": "to evaluate how well the predictor performs using the particular performance metric we've decided upon.",
    "start": "946140",
    "end": "952245"
  },
  {
    "text": "And that's the result. You have an honest test, you have an honest simulation of how the predictor works on unseen data.",
    "start": "952245",
    "end": "961950"
  },
  {
    "text": "And we hope that because it worked well on data that was not used to train it,",
    "start": "961950",
    "end": "970485"
  },
  {
    "text": "it will also work well on other unseen data that we haven't seen yet at all.",
    "start": "970485",
    "end": "978195"
  },
  {
    "text": "And this hope is founded on the assumption that all of this data looks kind of the same,",
    "start": "978195",
    "end": "984165"
  },
  {
    "text": "the data that we used for training, the data that we're using for testing and future unseen data,",
    "start": "984165",
    "end": "989279"
  },
  {
    "text": "they're all kind of similar. Very often we do this split in a random way,",
    "start": "989280",
    "end": "996209"
  },
  {
    "text": "uh, for two reasons. Um, one is, is that we want to get the training right.",
    "start": "996210",
    "end": "1002945"
  },
  {
    "text": "And so, um, if I'm for example, outside taking photographs of cats and",
    "start": "1002945",
    "end": "1009800"
  },
  {
    "text": "dogs in the hope to develop a predictor that can tell the difference between them. Well, during the day the Sun might go down,",
    "start": "1009800",
    "end": "1017000"
  },
  {
    "text": "the clouds might come over, it might become quite overcast, the conditions can change quite significantly.",
    "start": "1017000",
    "end": "1024145"
  },
  {
    "text": "And so if I simply take the last 20% of the images, then I could end up with a test set which is all cloudy images,",
    "start": "1024145",
    "end": "1033175"
  },
  {
    "text": "and a training set which is all sunny images. And that will both upset my training,",
    "start": "1033175",
    "end": "1040810"
  },
  {
    "text": "it could well be that my predictor will not learn how to distinguish cats and dogs under cloudy conditions,",
    "start": "1040810",
    "end": "1047380"
  },
  {
    "text": "but only under sunny conditions. And it might upset my test in that",
    "start": "1047380",
    "end": "1052389"
  },
  {
    "text": "all I will be doing is testing the predictor in cloudy conditions and I won't measure the performance in sunny conditions.",
    "start": "1052390",
    "end": "1058764"
  },
  {
    "text": "So the- the fact that you make this split random helps to avoid these kinds of,",
    "start": "1058765",
    "end": "1064640"
  },
  {
    "text": "uh, biases and errors which are a result of a- a particular ordering of the data in the dataset.",
    "start": "1064950",
    "end": "1073340"
  },
  {
    "text": "Now what matters is the performance on the test set.",
    "start": "1081560",
    "end": "1088000"
  },
  {
    "text": "The training set performance doesn't, um, really matter at all.",
    "start": "1088370",
    "end": "1093390"
  },
  {
    "text": "Um, we expect it to be good. And in fact, we usually expect the, uh,",
    "start": "1093390",
    "end": "1099059"
  },
  {
    "text": "the training performance to be better than the test performance. Um, usually the test performance is only",
    "start": "1099060",
    "end": "1106110"
  },
  {
    "text": "a little bit worse than the training performance. Um, sometimes the test performance is still okay,",
    "start": "1106110",
    "end": "1114360"
  },
  {
    "text": "but actually quite a lot worse than the training performance. That's fine. If your test performance is okay, that's what matters.",
    "start": "1114360",
    "end": "1123090"
  },
  {
    "text": "You can be in a situation where the training performance is perfect. For example, one nearest neighbor's predictor.",
    "start": "1123090",
    "end": "1130065"
  },
  {
    "text": "The one nearest neighbor's predictor, if you give it an element of the training set, one of the x_i's,",
    "start": "1130065",
    "end": "1135390"
  },
  {
    "text": "it predicts the corresponding y_i corresponding to that x_i, because that is the closest x to that x_i.",
    "start": "1135390",
    "end": "1145720"
  },
  {
    "text": "And so it gets zero error on the training set.",
    "start": "1145820",
    "end": "1150940"
  },
  {
    "text": "But it still makes useful predictions even for, uh, data that's not in the training set.",
    "start": "1150940",
    "end": "1157070"
  },
  {
    "text": "But the training error is no indicator of how well it will do on the test set.",
    "start": "1157070",
    "end": "1163440"
  },
  {
    "text": "So let's look at how we interpret validation results. We have two measurements.",
    "start": "1166610",
    "end": "1173760"
  },
  {
    "text": "The first is the performance on the test set, that's really what matters. And the second is the performance on the training set, that doesn't matter.",
    "start": "1173760",
    "end": "1183480"
  },
  {
    "text": "And so we'll get four numbers. We'll plot these in a table here.",
    "start": "1183480",
    "end": "1188730"
  },
  {
    "text": "Here in the first column, we have the training error, we have a small training error.",
    "start": "1188730",
    "end": "1195000"
  },
  {
    "text": "And in the second column, we have a large training error. And in the first row, we have small test error,",
    "start": "1195000",
    "end": "1202635"
  },
  {
    "text": "and in the second one, we have a large test error. So this top left entry here is really good news.",
    "start": "1202635",
    "end": "1210810"
  },
  {
    "text": "It means we got a small test error and small training error.",
    "start": "1210810",
    "end": "1216100"
  },
  {
    "text": "So it performs well when we did our training and then when we tried it out on data we haven't seen before,",
    "start": "1216100",
    "end": "1221690"
  },
  {
    "text": "it did well there too. And so it generalizes. It is possible to be over here.",
    "start": "1221690",
    "end": "1229630"
  },
  {
    "text": "To be in the case where we have small test error, but large training error.",
    "start": "1230690",
    "end": "1236880"
  },
  {
    "text": "That is luck or perhaps we've cheated, and there's some kind of fraud involved.",
    "start": "1236880",
    "end": "1242925"
  },
  {
    "text": "Um, it doesn't happen very often because when it does the training and when it sees a large training error and typically",
    "start": "1242925",
    "end": "1249570"
  },
  {
    "text": "we don't even bother to try it against unseen data. If we can't get it to work on data that we can see,",
    "start": "1249570",
    "end": "1255330"
  },
  {
    "text": "we're probably not going to get it to work on data that we can't see. Um, so this doesn't happen very often.",
    "start": "1255330",
    "end": "1261960"
  },
  {
    "text": "Uh, if we have a large test error,",
    "start": "1261960",
    "end": "1269789"
  },
  {
    "text": "the bottom row, well, then there's two possible explanations.",
    "start": "1269790",
    "end": "1274950"
  },
  {
    "text": "One is where, uh, we have small training error, but we had large test error.",
    "start": "1274950",
    "end": "1280875"
  },
  {
    "text": "Um, this is the failure to generalize. We thought we'd do well with a good predictor,",
    "start": "1280875",
    "end": "1286605"
  },
  {
    "text": "it seemed to do well when we were training it. Um, but then we tested it on data we've never seen before and it didn't do well.",
    "start": "1286605",
    "end": "1293595"
  },
  {
    "text": "Um, we would say such a predictor is over-fit.",
    "start": "1293595",
    "end": "1298120"
  },
  {
    "text": "And then the worst case of all, we have large training error and large test error.",
    "start": "1299540",
    "end": "1305460"
  },
  {
    "text": "It generalizes okay. You know, it does the same thing on the test data that it does on the- the training data.",
    "start": "1305460",
    "end": "1312960"
  },
  {
    "text": "But it doesn't do very well on the training data.",
    "start": "1312960",
    "end": "1316149"
  },
  {
    "text": "So how do we choose between different candidate predictors? Two people who have come along,",
    "start": "1323780",
    "end": "1329820"
  },
  {
    "text": "and they've both- they're both experts in machine learning, and they come to us with their predictors, but they've developed them using completely different methods.",
    "start": "1329820",
    "end": "1336434"
  },
  {
    "text": "We don't even get to see their code. We can do validation. We take their predictors,",
    "start": "1336435",
    "end": "1343725"
  },
  {
    "text": "and we try their predictors on data that they've never seen before.",
    "start": "1343725",
    "end": "1348580"
  },
  {
    "text": "And typically, what we're gonna do is we gonna choose the predictor amongst all of those candidates,",
    "start": "1349850",
    "end": "1355020"
  },
  {
    "text": "which has the smallest test error. That's not always what we do because",
    "start": "1355020",
    "end": "1361140"
  },
  {
    "text": "sometimes we're willing to back off a little bit on that requirement. We might accept a little bit larger test error",
    "start": "1361140",
    "end": "1367335"
  },
  {
    "text": "if that gives us a particularly simpler predictor. Uh, there are good reasons for this,",
    "start": "1367335",
    "end": "1374010"
  },
  {
    "text": "and we will have more to say on this later.",
    "start": "1374010",
    "end": "1378040"
  },
  {
    "text": "And let's look at a particular example. Here is simply a one-dimensional example.",
    "start": "1380530",
    "end": "1386044"
  },
  {
    "text": "Um, we have, uh, a- a- a dataset with 30 data points.",
    "start": "1386045",
    "end": "1393300"
  },
  {
    "text": "Um, 20 of which we'll use for training, and 10 of which we'll use as the test set. Here on this plot, we see 20 blue points,",
    "start": "1393300",
    "end": "1400575"
  },
  {
    "text": "those are the training set, and 10 red points.",
    "start": "1400575",
    "end": "1404740"
  },
  {
    "text": "So here's the performance of two of our favorite classes of predictors on that dataset.",
    "start": "1406190",
    "end": "1414735"
  },
  {
    "text": "Here on the top row we have the k-nearest neighbor predictors when k is 1, 2, and 3.",
    "start": "1414735",
    "end": "1420809"
  },
  {
    "text": "And on the bottom level we have the affine predictor, the quadratic predictor, and the cubic predictor.",
    "start": "1420810",
    "end": "1428129"
  },
  {
    "text": "We can see right here that the- in the- in the top left plot the k-nearest neighbors when- predictor when k is 1,",
    "start": "1428130",
    "end": "1437115"
  },
  {
    "text": "does perfectly on the training data. Let me just highlight that.",
    "start": "1437115",
    "end": "1443670"
  },
  {
    "text": "It passes perfectly through the blue data points.",
    "start": "1443670",
    "end": "1451330"
  },
  {
    "text": "If I look at the k is equal to 2 plot, well, that doesn't pass perfectly through all the data points.",
    "start": "1452060",
    "end": "1459059"
  },
  {
    "text": "But it's a bit smoother than the k is equal to 1. And k is equal to 3 also doesn't pass through all the data points,",
    "start": "1459060",
    "end": "1466575"
  },
  {
    "text": "but it's a little bit smoother than k is equal to 2.",
    "start": "1466575",
    "end": "1470860"
  },
  {
    "text": "Now the affine predictor here, that's the best straight line fit.",
    "start": "1472100",
    "end": "1478120"
  },
  {
    "text": "Here's the quadratic predictor, and here's the cubic predictor.",
    "start": "1479300",
    "end": "1486130"
  },
  {
    "text": "Just looking at these.",
    "start": "1486680",
    "end": "1489700"
  },
  {
    "text": "Now, let's look at the RMS performance error. So we're using the RMS error here as the performance metric.",
    "start": "1504820",
    "end": "1511835"
  },
  {
    "text": "And we can say, well, which is the best prediction model?",
    "start": "1511835",
    "end": "1516360"
  },
  {
    "text": "Some things to highlight, well, first of all that number right there is 0,",
    "start": "1517480",
    "end": "1524299"
  },
  {
    "text": "the k nearest neighbors predictor when k is equal to 1, that's perfectly on the training data.",
    "start": "1524300",
    "end": "1529980"
  },
  {
    "text": "Doesn't- when we increase k is equal to 2 and k is equal to 3, actually, the performance gets worse on the training data.",
    "start": "1532000",
    "end": "1540150"
  },
  {
    "text": "Um, and on the test data it changes a little bit. It goes from 0.1 when k is 1 to 0.08,",
    "start": "1541120",
    "end": "1548590"
  },
  {
    "text": "when k is 2, back up to 0.1 when k is 3. When we look at the polynomial predictors,",
    "start": "1548590",
    "end": "1557810"
  },
  {
    "text": "we see something which is quite interesting. With an affine predictor on the training data,",
    "start": "1557810",
    "end": "1565925"
  },
  {
    "text": "we have 0.08, and then when we go to the quadratic predictor, the error decreases.",
    "start": "1565925",
    "end": "1572960"
  },
  {
    "text": "This has to happen, because we're optimizing over the best possible quadratic predictor.",
    "start": "1572960",
    "end": "1581434"
  },
  {
    "text": "And of course, affine predictor is a quadratic predictor, it's just a very special one. And so the quadratic predictor is comparing all possible quadratics,",
    "start": "1581435",
    "end": "1592040"
  },
  {
    "text": "including all the affine's, and so it has to do at least as well as the best affine predictor.",
    "start": "1592040",
    "end": "1598565"
  },
  {
    "text": "So when we go from affine to quadratic, the performance has to get better. And the same when we go from quadratic to cubic.",
    "start": "1598565",
    "end": "1605885"
  },
  {
    "text": "Every quadratic predictor is a cubic predictor, and so the best cubic predictor must do at least as well as the best quadratic predictor.",
    "start": "1605885",
    "end": "1615424"
  },
  {
    "text": "And so the train- the training error has to decrease as we go from an affine to quadratic to cubic.",
    "start": "1615425",
    "end": "1621840"
  },
  {
    "text": "Those statements are true for the test data. We do see that as we go from affine to quadratic to cubic,",
    "start": "1622330",
    "end": "1632660"
  },
  {
    "text": "that the performance on the test data does get better. But there's no guarantee that that would happen,",
    "start": "1632660",
    "end": "1639935"
  },
  {
    "text": "and it doesn't have to happen. So I have all of these predictors,",
    "start": "1639935",
    "end": "1645830"
  },
  {
    "text": "which one has the best test error? Is this one right here.",
    "start": "1645830",
    "end": "1652220"
  },
  {
    "text": "That's noticeably better. That's the cubic predictor.",
    "start": "1652220",
    "end": "1657650"
  },
  {
    "text": "It's test error is 0.025. The next comparable ones",
    "start": "1657650",
    "end": "1663080"
  },
  {
    "text": "are the quadratic predictor and the two nearest neighbor predictor.",
    "start": "1663080",
    "end": "1672125"
  },
  {
    "text": "But that is so substantially worse at 0.08. So we might decide to, uh,",
    "start": "1672125",
    "end": "1677195"
  },
  {
    "text": "go for the best- the cubic predictor,",
    "start": "1677195",
    "end": "1683855"
  },
  {
    "text": "which does well in test and it does well in train. And so it both performs well and generalizes well.",
    "start": "1683855",
    "end": "1693870"
  },
  {
    "text": "We have the k nearest neighbor with k is equal to 1, which performs well on the train,",
    "start": "1695260",
    "end": "1701630"
  },
  {
    "text": "but it doesn't generalize so well.",
    "start": "1701630",
    "end": "1704370"
  },
  {
    "text": "Let's look more generally at polynomial fits. So suppose we have a scalar u and scalar v as our raw data.",
    "start": "1712300",
    "end": "1722884"
  },
  {
    "text": "And then we use a feature mapping where we construct x's from u's by x is equal to Phi of u.",
    "start": "1722885",
    "end": "1730115"
  },
  {
    "text": "And for each u, we construct a d-dimensional vector consisting of the powers of u,",
    "start": "1730115",
    "end": "1737870"
  },
  {
    "text": "1, u, u squared up to u to the power of d minus 1. And then we're gonna construct a linear predictor,",
    "start": "1737870",
    "end": "1745340"
  },
  {
    "text": "g of x theta transpose x, a linear combination of those d monomials,",
    "start": "1745340",
    "end": "1752765"
  },
  {
    "text": "a polynomial of degree d minus 1. And we're gonna choose the Theta by least squares.",
    "start": "1752765",
    "end": "1761159"
  },
  {
    "text": "And here's the kind of thing that we see. Here, we have 60 data points",
    "start": "1765310",
    "end": "1771470"
  },
  {
    "text": "and we are looking at the predictors with d is equal to 6 on the left,",
    "start": "1771470",
    "end": "1778294"
  },
  {
    "text": "in the middle d is equal to 12, and on the right, d is equal to 14. So these are degree 5, degree 11,",
    "start": "1778295",
    "end": "1784670"
  },
  {
    "text": "and degree 13 polynomial fits of the data.",
    "start": "1784670",
    "end": "1788640"
  },
  {
    "text": "Now, if we look at the RMS error, the one that has the smallest RMS error is the degree 13 plot.",
    "start": "1789700",
    "end": "1799025"
  },
  {
    "text": "And that's not surprising at all for the same reason, the cubic did better than the quadratic",
    "start": "1799025",
    "end": "1804290"
  },
  {
    "text": "in the previous example. The degree 13 has to do better than the degree 11, which has to do better than the degree 5 against the training data.",
    "start": "1804290",
    "end": "1813630"
  },
  {
    "text": "Now, we'll take those same 60 data points and we'll split the data points into 48 training points and 12 test points.",
    "start": "1821260",
    "end": "1830639"
  },
  {
    "text": "Now, for each degree d, between here, we've got degrees between 0 and 14.",
    "start": "1831550",
    "end": "1840980"
  },
  {
    "text": "What we're going to do is we're going to train a predictor on the 48 data points.",
    "start": "1840980",
    "end": "1849995"
  },
  {
    "text": "And then we'll compute two numbers. It's RMS error on the training set and its RMS error on the test set.",
    "start": "1849995",
    "end": "1858630"
  },
  {
    "text": "So for degree 2, that's this number and this number.",
    "start": "1859720",
    "end": "1869615"
  },
  {
    "text": "And then we do it again, say degree four and we end up with this number and this number.",
    "start": "1869615",
    "end": "1877565"
  },
  {
    "text": "We expect that the training error is going to be less.",
    "start": "1877565",
    "end": "1883654"
  },
  {
    "text": "It's gonna be less than the test error. And it is at every different degree that we see here.",
    "start": "1883655",
    "end": "1889220"
  },
  {
    "text": "But it doesn't have to be that way. These two- these two plots can certainly cross.",
    "start": "1889220",
    "end": "1896210"
  },
  {
    "text": "Um, it usually happens that we do better in training than we do in test but not always.",
    "start": "1896210",
    "end": "1902760"
  },
  {
    "text": "Now, what happens on the training set as you increase the degree",
    "start": "1903730",
    "end": "1909410"
  },
  {
    "text": "is that the training error has to decrease.",
    "start": "1909410",
    "end": "1915530"
  },
  {
    "text": "Increasing the class of functions, increasing the set of functions over which we're",
    "start": "1915530",
    "end": "1920750"
  },
  {
    "text": "optimizing means we're going to get better fits. And so you can see that every time we increase degree,",
    "start": "1920750",
    "end": "1927560"
  },
  {
    "text": "the training error decreases. The test error certainly doesn't have to, and it doesn't.",
    "start": "1927560",
    "end": "1934740"
  },
  {
    "text": "Now, if you just looked at the training error, you might conclude, we should conclude- we should use a degree as high as possible.",
    "start": "1935410",
    "end": "1944975"
  },
  {
    "text": "So we should use d is equal to, uh, 15 here.",
    "start": "1944975",
    "end": "1950585"
  },
  {
    "text": "And, uh, however, if you look at the test error,",
    "start": "1950585",
    "end": "1958940"
  },
  {
    "text": "that suggests quite a different story. That suggests that we should use degree 5.",
    "start": "1958940",
    "end": "1968790"
  },
  {
    "text": "What's happening out here to the right is we're seeing over fitting.",
    "start": "1970360",
    "end": "1978470"
  },
  {
    "text": "We're seeing predictors that are doing really well on the training data,",
    "start": "1978470",
    "end": "1986330"
  },
  {
    "text": "but much less well on the test data.",
    "start": "1986330",
    "end": "1992580"
  },
  {
    "text": "Let's look at our plots again.",
    "start": "1994510",
    "end": "1997830"
  },
  {
    "text": "So here we can see it. You can just about see that really",
    "start": "2002650",
    "end": "2008704"
  },
  {
    "text": "this data is kind of noisy and there's plots- points scattered everywhere.",
    "start": "2008704",
    "end": "2013924"
  },
  {
    "text": "But here, we've seen these little wiggles showing up.",
    "start": "2013925",
    "end": "2018870"
  },
  {
    "text": "Let's look at our polynomial fit again to see if we can see evidence of overfitting.",
    "start": "2020380",
    "end": "2028020"
  },
  {
    "text": "So there's two things that show up on this plot that are interesting. One is that there seems to be quite a lot of noise in the data. What do I mean by that?",
    "start": "2028120",
    "end": "2038635"
  },
  {
    "text": "If we look at just a particular region, just look at this region.",
    "start": "2038635",
    "end": "2046340"
  },
  {
    "text": "If I look at x's within this region here,",
    "start": "2046770",
    "end": "2052050"
  },
  {
    "text": "then we can see that vertically, the value of y still tends to have some variability.",
    "start": "2052050",
    "end": "2058369"
  },
  {
    "text": "It's not a smooth curve like the predictor would have us believe,",
    "start": "2058370",
    "end": "2064080"
  },
  {
    "text": "but instead, the points jiggle up and down a lot. And that suggests that those points come about- that",
    "start": "2064990",
    "end": "2072200"
  },
  {
    "text": "variation in those vertical positions of those points comes about due to noise.",
    "start": "2072200",
    "end": "2077405"
  },
  {
    "text": "Now what's happening with regard to our predictor? Our predictor is doing something interesting.",
    "start": "2077405",
    "end": "2084820"
  },
  {
    "text": "Our predictor in this degree 5 plot is just sort of averaging out the noise,",
    "start": "2084820",
    "end": "2090010"
  },
  {
    "text": "is doing something rather gentle. It's just moving gently through the center of the cloud of points.",
    "start": "2090010",
    "end": "2099964"
  },
  {
    "text": "But this predictor over here at degree 13,",
    "start": "2099965",
    "end": "2105485"
  },
  {
    "text": "we can see that it's starting to wiggle. And the reason it's starting to wiggle is that it's starting to believe in the noise.",
    "start": "2105485",
    "end": "2114290"
  },
  {
    "text": "It's starting to try and fit the noise. And this is what we mean by overfitting.",
    "start": "2114290",
    "end": "2120470"
  },
  {
    "text": "The learning algorithm is fitting a predictor to features of the training data which are due to noise or variation,",
    "start": "2120470",
    "end": "2131480"
  },
  {
    "text": "which isn't present in the test set.",
    "start": "2131480",
    "end": "2136050"
  },
  {
    "text": "And that's what we see, is that as we move down here,",
    "start": "2139240",
    "end": "2145020"
  },
  {
    "text": "we're getting better and better fit as we fit more and more closely the features in the training data.",
    "start": "2145240",
    "end": "2151970"
  },
  {
    "text": "But the reality is- is that with test data,",
    "start": "2151970",
    "end": "2157885"
  },
  {
    "text": "we're gonna move up here and we're gonna find that those features don't actually exist in the test data.",
    "start": "2157885",
    "end": "2165805"
  },
  {
    "text": "And they are going to give us greater error in the test data that we are led to believe by the performance on the training data.",
    "start": "2165805",
    "end": "2175260"
  },
  {
    "text": "Now we can do several more, slightly more sophisticated things than simply",
    "start": "2183220",
    "end": "2189200"
  },
  {
    "text": "split the data into a test set and a training set. One extension is to split the data more ways.",
    "start": "2189200",
    "end": "2198815"
  },
  {
    "text": "So we split the data into k different subsets. We'll call them k folds of the data.",
    "start": "2198815",
    "end": "2205010"
  },
  {
    "text": "And this is called k-fold cross-validation. And so we have k different buckets of data,",
    "start": "2205010",
    "end": "2213110"
  },
  {
    "text": "k different subsets of the data. And what we're gonna do is we're gonna go through",
    "start": "2213110",
    "end": "2220879"
  },
  {
    "text": "the sets of data, and the first time we fit, so let's draw say- say suppose k is 5,",
    "start": "2220879",
    "end": "2227345"
  },
  {
    "text": "and then we'll have five different subsets of the data. There's one, there's two, there's three, there's four, and there's five.",
    "start": "2227345",
    "end": "2235670"
  },
  {
    "text": "The first time we fit, we will fit using those four and we'll test using that one.",
    "start": "2235670",
    "end": "2247790"
  },
  {
    "text": "The second time we fit, we will- we will fit using this one, those four and test using that one.",
    "start": "2247790",
    "end": "2257075"
  },
  {
    "text": "The third time, we'll fit using those four and test using that one and so on.",
    "start": "2257075",
    "end": "2265055"
  },
  {
    "text": "So we'll have each time we do a run, we will train using four out of",
    "start": "2265055",
    "end": "2272030"
  },
  {
    "text": "our five buckets of data and we're going to test using the other bucket of the data.",
    "start": "2272030",
    "end": "2277605"
  },
  {
    "text": "As a result, we get rather than having just one test error and one training error.",
    "start": "2277605",
    "end": "2284915"
  },
  {
    "text": "We have five different training errors and five different corresponding test errors.",
    "start": "2284915",
    "end": "2292770"
  },
  {
    "text": "These five different test errors we can look at. We can work out the mean test error,",
    "start": "2292770",
    "end": "2299195"
  },
  {
    "text": "the standard deviation of the test errors, and just get a sense of their variation. If they are all small,",
    "start": "2299195",
    "end": "2305300"
  },
  {
    "text": "then we can feel rather confident that actually this predictor does well.",
    "start": "2305300",
    "end": "2313080"
  },
  {
    "text": "Conversely, if there is a whole lot of variability, then we start to feel a little less confident.",
    "start": "2313180",
    "end": "2319145"
  },
  {
    "text": "And of course, we haven't really got one predictor. We're going to have, if k is 5, five different predictors.",
    "start": "2319145",
    "end": "2325730"
  },
  {
    "text": "And so we can look at the different Thetas, the different predictor parameters.",
    "start": "2325730",
    "end": "2332375"
  },
  {
    "text": "If they're all very similar, that's another reason to feel confident that our methods are giving",
    "start": "2332375",
    "end": "2338900"
  },
  {
    "text": "us a certain amount or uniformity. We're getting the same predictor for each one of our different subsets of the data.",
    "start": "2338900",
    "end": "2347510"
  },
  {
    "text": "Well, on the other hand if the predictor parameters vary wildly, well, then we're less confident",
    "start": "2347510",
    "end": "2354785"
  },
  {
    "text": "that we've got a sensible choice of predictor coming out of our method.",
    "start": "2354785",
    "end": "2360660"
  },
  {
    "text": "Here's an example. This is just randomly generated data.",
    "start": "2364540",
    "end": "2369785"
  },
  {
    "text": "We can see here on the left, we have fitted a straight line through the data.",
    "start": "2369785",
    "end": "2378859"
  },
  {
    "text": "So this is determined by two parameters, Theta 1 plus Theta 2 times x is the predictor.",
    "start": "2378860",
    "end": "2384995"
  },
  {
    "text": "And- and then we do, we split the data five ways and do five-fold cross-validation.",
    "start": "2384995",
    "end": "2392615"
  },
  {
    "text": "And that gives us five different results, each of which is a training loss and it has the training loss and a test loss.",
    "start": "2392615",
    "end": "2399755"
  },
  {
    "text": "And we can see that- well typically, the tests loss is not that different from the training loss.",
    "start": "2399755",
    "end": "2407345"
  },
  {
    "text": "That's a good sign right there. And, ah, there's some variability in the test loss.",
    "start": "2407345",
    "end": "2414290"
  },
  {
    "text": "The test- smallest test loss we see here is 0.0027, the largest test loss is 0.0071.",
    "start": "2414290",
    "end": "2421805"
  },
  {
    "text": "But this that's- either way it's still very small. The Theta parameters, we will also see some variability there,",
    "start": "2421805",
    "end": "2429740"
  },
  {
    "text": "in particular in Theta 1, but it's rather a small number again. It's going to be 0.003 up to 0.012- up to minus 0.012.",
    "start": "2429740",
    "end": "2442250"
  },
  {
    "text": "And Theta 2 is about 1. You can see that, of course, when we plot the different predictors,",
    "start": "2442250",
    "end": "2451745"
  },
  {
    "text": "each of the different predictors, those are really five different plots here on this curve. There are really five lines on top of each other.",
    "start": "2451745",
    "end": "2459440"
  },
  {
    "text": "But they're all so close to each other that we can't really tell the difference. Of course, when you're in high dimensions,",
    "start": "2459440",
    "end": "2465740"
  },
  {
    "text": "you can't necessarily plot your predictors, but you can look at Theta 1 and the different Theta parameters,",
    "start": "2465740",
    "end": "2471215"
  },
  {
    "text": "and you can look at the training loss and the test loss.",
    "start": "2471215",
    "end": "2474630"
  },
  {
    "text": "We might want to be even more confident than simply having five different results.",
    "start": "2478240",
    "end": "2483980"
  },
  {
    "text": "Here's how you might do them. You take the data and split it into a training and test split,",
    "start": "2483980",
    "end": "2490670"
  },
  {
    "text": "say 80:20, but completely randomly. Train the predictor using the training data",
    "start": "2490670",
    "end": "2496670"
  },
  {
    "text": "and then evaluate the predictor on the test data. And then repeat it again a thousand different times.",
    "start": "2496670",
    "end": "2504030"
  },
  {
    "text": "So here's how you gain- might gain a little bit more confidence. You might split the data into a training and test split, say 80:20 randomly.",
    "start": "2514050",
    "end": "2524249"
  },
  {
    "text": "Then, you train the predictor using the training data and evaluate it on the test data.",
    "start": "2524249",
    "end": "2530300"
  },
  {
    "text": "Now, you repeat that a thousand times. And then, you'll have a thousand different test errors.",
    "start": "2530300",
    "end": "2540845"
  },
  {
    "text": "You can plot the histogram of those test errors and see how much variation is there.",
    "start": "2540845",
    "end": "2547425"
  },
  {
    "text": "And we can see right here that the mean test error in this example is about 0.05.",
    "start": "2547425",
    "end": "2556545"
  },
  {
    "text": "And if we look back to our previous data, that's not that inconsistent between where we see- saw errors between",
    "start": "2556545",
    "end": "2562609"
  },
  {
    "text": "0.027 and 0.071 and there's a couple here around 0.05.",
    "start": "2562610",
    "end": "2570000"
  },
  {
    "text": "And so really we can be confident that when we try this on new test data,",
    "start": "2571180",
    "end": "2577309"
  },
  {
    "text": "we would expect to see something around 0.05. At least if our lo- if our entire dataset is representative of unseen data.",
    "start": "2577310",
    "end": "2589140"
  },
  {
    "text": "And we can also see that there's gonna be some variation even with a dataset, the size that we have,",
    "start": "2589930",
    "end": "2595385"
  },
  {
    "text": "we're gonna see- sometimes we're gonna see some significant test errors of 0.015, 0.02.",
    "start": "2595385",
    "end": "2603720"
  },
  {
    "text": "Okay, one last topic in this section, and that is what do you do once you've chosen a predictor?",
    "start": "2612690",
    "end": "2619339"
  },
  {
    "text": "Well, one thing you have to be careful as- of is that you've revisited this test set too many times,",
    "start": "2620520",
    "end": "2627664"
  },
  {
    "text": "even if you kept one test set in escrow, you trained based  on the, on the training set.",
    "start": "2627665",
    "end": "2634039"
  },
  {
    "text": "And then, you evaluated your predictor on the test set and you had decided that the predictor wasn't very good.",
    "start": "2634040",
    "end": "2639600"
  },
  {
    "text": "But suddenly, you're taking information from that test set and using it in your training procedure.",
    "start": "2640210",
    "end": "2647120"
  },
  {
    "text": "And when you do it again and again and again, information is leaking from the test set into your training procedure.",
    "start": "2647120",
    "end": "2655865"
  },
  {
    "text": "You're learning based in part on the test set. And so it's no longer really an",
    "start": "2655865",
    "end": "2662600"
  },
  {
    "text": "honest simulation of how well the model would do on data you've never seen.",
    "start": "2662600",
    "end": "2667770"
  },
  {
    "text": "Of course, there's a trick to avoid this. And the trick to avoid this is to split the original data instead of into two datasets, into three datasets.",
    "start": "2668350",
    "end": "2676235"
  },
  {
    "text": "The training set, that's how we fit multiple candidate models,",
    "start": "2676235",
    "end": "2682340"
  },
  {
    "text": "and validation dataset which we evaluate the performance of our models on,",
    "start": "2682340",
    "end": "2687680"
  },
  {
    "text": "and then a test dataset which is pristine, is untouched. We keep it to one side and we never look at it or we look at it once.",
    "start": "2687680",
    "end": "2696530"
  },
  {
    "text": "That tells us how well we've done. But then we don't go back and chain our- change our predictor once we've looked at it.",
    "start": "2696530",
    "end": "2703740"
  },
  {
    "text": "Um, this of course, is a little bit more honest.",
    "start": "2705130",
    "end": "2710390"
  },
  {
    "text": "Um, uh, and some people would say that it's taking it to extremes.",
    "start": "2710390",
    "end": "2717184"
  },
  {
    "text": "Um, and this really depends on how much leakage of information there is from your test set into your training set,",
    "start": "2717185",
    "end": "2724790"
  },
  {
    "text": "from your validation set into your training set. Some practitioners do this, others do not.",
    "start": "2724790",
    "end": "2732335"
  },
  {
    "text": "Also, the names of test and validation are not really well settled.",
    "start": "2732335",
    "end": "2739805"
  },
  {
    "text": "Some people reverse the terminology and refer to the pristine set as the validation",
    "start": "2739805",
    "end": "2745190"
  },
  {
    "text": "set and the test set as the thing that's used to evaluate the performance of models.",
    "start": "2745190",
    "end": "2751525"
  },
  {
    "text": "In this course, we won't go to this extreme, we'll simply use one test set and one validation set.",
    "start": "2751525",
    "end": "2759214"
  },
  {
    "text": "And we'll use out-of-sample five-fold cross validation.",
    "start": "2759215",
    "end": "2764160"
  },
  {
    "text": "One more thing you can do is that we are satisfied. At the end of the day, you've do- you've done training,",
    "start": "2767260",
    "end": "2773600"
  },
  {
    "text": "validation and test, and you've got a predictor that you're pretty happy with. Now, you could just stop there and say that's a predictor.",
    "start": "2773600",
    "end": "2780080"
  },
  {
    "text": "We're going to use it and we're happy with it. Another thing you can do is you can say, well,",
    "start": "2780080",
    "end": "2785120"
  },
  {
    "text": "what we've already validated is not the predictor, but the procedure, the learning algorithm,",
    "start": "2785120",
    "end": "2792020"
  },
  {
    "text": "which we're using to develop the predictor. And so why not just take that learning algorithm,",
    "start": "2792020",
    "end": "2798260"
  },
  {
    "text": "which we're happy with and now apply it to the whole dataset, so that it can learn from all of the data we have",
    "start": "2798260",
    "end": "2804349"
  },
  {
    "text": "not just the piece that we used for training. And that is a very common practice,",
    "start": "2804350",
    "end": "2813245"
  },
  {
    "text": "um, and, uh, uh, there's nothing wrong with it. It works very well,",
    "start": "2813245",
    "end": "2818299"
  },
  {
    "text": "uh, many people do this. Um, so for example, you might train, uh,",
    "start": "2818300",
    "end": "2824210"
  },
  {
    "text": "k-nearest neighbor predictors for various values of k. Uh, validation suggests that k is 6, is a good choice.",
    "start": "2824210",
    "end": "2832115"
  },
  {
    "text": "And now, the final predictor you supply is a k-nearest neighbors predictor with k is 6,",
    "start": "2832115",
    "end": "2837725"
  },
  {
    "text": "but it uses all of the data. [NOISE] Okay, in this section,",
    "start": "2837725",
    "end": "2845599"
  },
  {
    "text": "we've talked about evaluating predictors.",
    "start": "2845600",
    "end": "2850950"
  },
  {
    "text": "In the previous section, we talked about different predictors that we could have.",
    "start": "2851680",
    "end": "2856819"
  },
  {
    "text": "Um, we've yet to talk about how you might make predictors and how you might learn.",
    "start": "2856820",
    "end": "2863000"
  },
  {
    "text": "And that's, of course, coming in this class. But right now, you're in a position where if somebody gives you a predictor,",
    "start": "2863000",
    "end": "2873064"
  },
  {
    "text": "you can decide whether it's good or not. But also, you're in a position where the stage for the co- for this course has been set.",
    "start": "2873064",
    "end": "2883880"
  },
  {
    "text": "We know what we're trying to do now with this class. We're trying to develop predictors.",
    "start": "2883880",
    "end": "2888995"
  },
  {
    "text": "We know how, once we've got predictors, how we're gonna evaluate them, how are we going to tell good predictors from bad predictors.",
    "start": "2888995",
    "end": "2896720"
  },
  {
    "text": "And what's next is to, uh, go ahead and make some.",
    "start": "2896720",
    "end": "2900900"
  }
]