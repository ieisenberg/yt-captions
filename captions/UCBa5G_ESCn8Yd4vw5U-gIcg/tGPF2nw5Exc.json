[
  {
    "start": "0",
    "end": "5840"
  },
  {
    "text": "Hello. Yep. Let's get started. [SIDE CONVERSATIONS]",
    "start": "5840",
    "end": "11582"
  },
  {
    "text": " Yeah.",
    "start": "11582",
    "end": "16960"
  },
  {
    "text": "So we are for the\nnext three lectures entering the next\nstep in our journey",
    "start": "16960",
    "end": "23170"
  },
  {
    "text": "about lossless compression\ngoing beyond iid data.",
    "start": "23170",
    "end": "29650"
  },
  {
    "text": "OK. So we started the course. We talked about very\nsimple prefix codes,",
    "start": "29650",
    "end": "36079"
  },
  {
    "text": "we talked about entropy,\nand then we looked at AEP.",
    "start": "36080",
    "end": "41220"
  },
  {
    "text": "We'll just provide a theoretical\nfoundation for compression. And now after that,\nthe last three or four",
    "start": "41220",
    "end": "48070"
  },
  {
    "text": "lectures we have been talking\nabout specific entropy coders. So let's take a\nquick tour of those.",
    "start": "48070",
    "end": "53510"
  },
  {
    "text": "We talked about Huffman coding. It's very fast. It is the optimal coder when\nyou are doing a single symbol",
    "start": "53510",
    "end": "58850"
  },
  {
    "text": "or you are doing a\nparticular block. And it will achieve entropy when\nyou work with larger blocks.",
    "start": "58850",
    "end": "64099"
  },
  {
    "text": "Of course, the complexity\nincreases exponentially. So it's great in many\nsettings, but you",
    "start": "64099",
    "end": "70066"
  },
  {
    "text": "run into this complexity\nlimit when you want to really get close to entropy. So that was the\nfirst coder we saw.",
    "start": "70067",
    "end": "77790"
  },
  {
    "text": "Actually, any questions about\nthe project before we move? ",
    "start": "77790",
    "end": "83250"
  },
  {
    "text": "Yep. OK. Then we saw arithmetic coding. This was a different\nstyle of algorithm",
    "start": "83250",
    "end": "89979"
  },
  {
    "text": "where the entire input\nwas a single block. And we saw that it will\nachieve entropy efficiently.",
    "start": "89980",
    "end": "97050"
  },
  {
    "text": "Basically, linear time\ninstead of exponential time as in the case of encoding. And it reads the entire\ninput as a block,",
    "start": "97050",
    "end": "102510"
  },
  {
    "text": "getting a very small 2/n\noverhead, over entropy. So it's great.",
    "start": "102510",
    "end": "109895"
  },
  {
    "text": "One main issue is that it\ninvolves certain division operations. When you actually do arithmetic\ncoding if you do it in a float",
    "start": "109895",
    "end": "115792"
  },
  {
    "text": "domain, you need floating\npoint multiplications which are expensive. If you do it as an integer\nwhich is the more common way,",
    "start": "115792",
    "end": "121170"
  },
  {
    "text": "you need division operations\nwhich are again expensive. So it is good, but you\nwon't see arithmetic coding",
    "start": "121170",
    "end": "127980"
  },
  {
    "text": "being used in fast compressors. It's still used in a lot of\nheavy-duty type compressors",
    "start": "127980",
    "end": "133500"
  },
  {
    "text": "and we'll look at some examples. One nice property of\narithmetic coding which will be important to us\ntoday and next lecture",
    "start": "133500",
    "end": "140460"
  },
  {
    "text": "is that it works with\nvery complex probability models far beyond\nanything we have",
    "start": "140460",
    "end": "146340"
  },
  {
    "text": "seen until now in this class. And we'll see some examples\nin the next lecture.",
    "start": "146340",
    "end": "151480"
  },
  {
    "text": "OK. Finally, we looked at\nANS in the last lecture. There are two variants--\nrANS and tANS.",
    "start": "151480",
    "end": "157080"
  },
  {
    "text": "We mostly talked about\nrANS, but both of them have their own strengths. Again, they achieve\nentropy efficiently",
    "start": "157080",
    "end": "162870"
  },
  {
    "text": "by treating the entire\ninput as a block. tANS doesn't quite achieve\nentropy, but it gets close.",
    "start": "162870",
    "end": "168405"
  },
  {
    "text": " There is a small\noverhead over arithmetic,",
    "start": "168405",
    "end": "173790"
  },
  {
    "text": "so it's not the best\ncompression, but close enough. Both of them are\nfaster than arithmetic",
    "start": "173790",
    "end": "179320"
  },
  {
    "text": "coding with tANS being\nparticularly fast. It almost approaches\nHuffman coding speeds.",
    "start": "179320",
    "end": "185230"
  },
  {
    "text": "And finally, the\nalgorithm we see in class is always very simplistic. It's like a Python pseudocode.",
    "start": "185230",
    "end": "190630"
  },
  {
    "text": "People have figured\nout how to implement tANS in extremely efficient ways\non modern CPU architectures.",
    "start": "190630",
    "end": "196930"
  },
  {
    "text": "And that is the reason\nwhy it's becoming more and more popular today. ",
    "start": "196930",
    "end": "209690"
  },
  {
    "text": "So there is only so much\nwe can cover in class, but here are some\nlinks if you are more interested in these topics\nabout these entropy coders.",
    "start": "209690",
    "end": "219680"
  },
  {
    "text": "The paper by Jarek\nDuda on rANS and tANS. A couple of few blog posts\nby people who have actually",
    "start": "219680",
    "end": "227060"
  },
  {
    "text": "implemented these algorithms. And really a lot of the\ngood stuff in compression",
    "start": "227060",
    "end": "234050"
  },
  {
    "text": "is on this Russian\nforum for some reason. But all the big people like\nYann Collet, the creator",
    "start": "234050",
    "end": "239330"
  },
  {
    "text": "of Zstandard and Jarek Duda,\nthe creator of ANS, all of them are on that forum, and they\ncomment on all the latest",
    "start": "239330",
    "end": "246680"
  },
  {
    "text": "developments on compression. So if you are interested. ",
    "start": "246680",
    "end": "252740"
  },
  {
    "text": "OK. And then this is a table we\nhave been seeing a few times in the last few lectures. So Huffman coding, 1950s then\n1970s, and then very recent,",
    "start": "252740",
    "end": "262710"
  },
  {
    "text": "ANS. And they have different\nspeed profiles, different compression,\nratios, and so on.",
    "start": "262710",
    "end": "268330"
  },
  {
    "text": "So Huffman coding is the\nfastest, but worst compression. Arithmetic coding is\nthe slow slower one",
    "start": "268330",
    "end": "274660"
  },
  {
    "text": "but the best compression\nbasically for the complexity. And rANS and tANS are\nfaster than arithmetic",
    "start": "274660",
    "end": "282430"
  },
  {
    "text": "coding and better\nthan Huffman coding. So somewhere in the middle. One thing you note here is that\nrANS has a very fast decoding",
    "start": "282430",
    "end": "291340"
  },
  {
    "text": "compared to arithmetic coding. And we'll look at\nthat in a minute. In many applications\ndecoding time is crucial",
    "start": "291340",
    "end": "298090"
  },
  {
    "text": "and rANS wins out on that quite\na bit, and tANS even more. ",
    "start": "298090",
    "end": "306600"
  },
  {
    "text": "OK. So last point on this is that\nall of these entropy coders are used in practice sometimes\nbecause they are the best",
    "start": "306600",
    "end": "313670"
  },
  {
    "text": "tool for the job and\nmany times because people have been using it and it's hard\nto switch compression formats,",
    "start": "313670",
    "end": "319699"
  },
  {
    "text": "so just like a C reasons. But they have their\nunique properties and we will keep\ncoming back to these",
    "start": "319700",
    "end": "326330"
  },
  {
    "text": "both in the next few lectures\nand in lossy compression. ",
    "start": "326330",
    "end": "331889"
  },
  {
    "text": "OK. Any questions on this whirlwind\ntour of entropy coders? ",
    "start": "331890",
    "end": "339580"
  },
  {
    "text": "The main thing we are doing\nwith this entropy coder is start with a known\nprobability distribution.",
    "start": "339580",
    "end": "345250"
  },
  {
    "text": "You have data drawn according\nto that iid probability distribution and you want to\ncompress it to its entropy.",
    "start": "345250",
    "end": "351790"
  },
  {
    "text": "Basically, that's the block\ndiagram version of this. ",
    "start": "351790",
    "end": "358410"
  },
  {
    "text": "So you have known P, which is\niid, and you have x1 to xn.",
    "start": "358410",
    "end": "367100"
  },
  {
    "text": "And you take your entropy\ncoder and you get bits.",
    "start": "367100",
    "end": "374730"
  },
  {
    "text": "And you want this\nnumber of bits to be roughly equal to n\ntimes the entropy of x.",
    "start": "374730",
    "end": "380415"
  },
  {
    "text": " All of these entropy coders\nfit this basic block diagram.",
    "start": "380415",
    "end": "387139"
  },
  {
    "text": " OK. Moving on to the quiz\nwhich hopefully we",
    "start": "387140",
    "end": "392970"
  },
  {
    "text": "can do very quickly because\nI wrote down the answers.",
    "start": "392970",
    "end": "399585"
  },
  {
    "text": " So the first two\nquestions were about rANS",
    "start": "399585",
    "end": "406409"
  },
  {
    "text": "encoding and decoding. You just need to follow the\nformulas as you saw in class. So we have some encoding step,\nwe have some decoding step.",
    "start": "406410",
    "end": "416320"
  },
  {
    "text": "A quick thing I want to note\nis that see, in the encoding, you need to divide by the\nfrequency of the symbol.",
    "start": "416320",
    "end": "421759"
  },
  {
    "text": "And divisions are bad. Basically, that's good to know. Whereas during encoding, you\nsee there is only one division",
    "start": "421760",
    "end": "429830"
  },
  {
    "text": "step, but it's division by M,\nwhich is usually a power of 2. So division by power of 2 is\nvery, very cheap on a computer",
    "start": "429830",
    "end": "437060"
  },
  {
    "text": "because you just shift left by\nor shift right by something. So that is the main reason why\nthe decoding is so much faster.",
    "start": "437060",
    "end": "444920"
  },
  {
    "text": "If we saw in a couple of slides\nago the decoding for rANS is almost double the\nspeed of the encoding",
    "start": "444920",
    "end": "450110"
  },
  {
    "text": "because the decoding doesn't\nhave this division operation that the encoding has.",
    "start": "450110",
    "end": "456449"
  },
  {
    "text": "So we have this formula. Let's just do it.",
    "start": "456450",
    "end": "461560"
  },
  {
    "text": "I don't want to\nspend too much time. Talk to us later if\nyou didn't get this. ",
    "start": "461560",
    "end": "469000"
  },
  {
    "text": "So you are given some alphabet-- 0, 1, 2, and you\nhave your symbols",
    "start": "469000",
    "end": "476086"
  },
  {
    "text": "with these probabilities-- 3/8, 3/8, 2/8. So first thing you\nwould do is you",
    "start": "476087",
    "end": "481303"
  },
  {
    "text": "will say that the common\ndenominator of all of these is 8, so you will set\nthis capital M equal to 8. And then you will make this\ntable, which is symbols.",
    "start": "481303",
    "end": "490160"
  },
  {
    "text": "They are frequencies--\nso 3, 3, 2, and the cumulative which is\nthe running sum of these guys.",
    "start": "490160",
    "end": "496520"
  },
  {
    "text": "And then you would\njust apply the formula. So let me do one of these. So initially the\nstate is 0, so you",
    "start": "496520",
    "end": "503419"
  },
  {
    "text": "do like 0 divided by frequency\nof 2, which is 2 multiply by 8",
    "start": "503420",
    "end": "511130"
  },
  {
    "text": "plus the cumulative of 2,\nwhich is 6 plus 0 modulo",
    "start": "511130",
    "end": "517820"
  },
  {
    "text": "frequency of 2, which is 6. ",
    "start": "517820",
    "end": "524159"
  },
  {
    "text": "Anybody had any issues\nwith this question? Let me know, otherwise, I will\njust write the answers for now.",
    "start": "524159",
    "end": "531540"
  },
  {
    "text": "You're just supposed\nto follow this formula, just do the computations. ",
    "start": "531540",
    "end": "540430"
  },
  {
    "text": "Let me do the next one. So 6 divided by frequency\nof 0 is 3 multiplied",
    "start": "540430",
    "end": "547360"
  },
  {
    "text": "by 8 again plus\ncumulative of 0, which is 0 plus 6 modulo, the\nfrequency of 0, so 3.",
    "start": "547360",
    "end": "560230"
  },
  {
    "text": "So 6 modulo 3 is 0. So only the first term\nsurvives and you get 16.",
    "start": "560230",
    "end": "565540"
  },
  {
    "text": "After you do all of\nit you should get 114.",
    "start": "565540",
    "end": "571779"
  },
  {
    "text": "Just substitute. The second one we\nasked you to decode.",
    "start": "571780",
    "end": "576850"
  },
  {
    "text": "So you were given something\nwhere the final state was 117 and you had four symbols and you\nare supposed to now decode it.",
    "start": "576850",
    "end": "583420"
  },
  {
    "text": " Again, make the table. ",
    "start": "583420",
    "end": "592920"
  },
  {
    "text": "Make the table, and\nwhat do we have here? ",
    "start": "592920",
    "end": "599380"
  },
  {
    "text": "We will follow the\nsteps which you can see. Yes. So let's do this. So block_id.",
    "start": "599380",
    "end": "604710"
  },
  {
    "text": "Block_id is 117 divided\nby M, which is 8, and you take the floor,\nwhich if you do it is 14.",
    "start": "604710",
    "end": "613680"
  },
  {
    "text": "So block_id will be 14. Slot is 117 modulo 8.",
    "start": "613680",
    "end": "619410"
  },
  {
    "text": "So 112 is a multiple of\n8, so this will be 5.",
    "start": "619410",
    "end": "624449"
  },
  {
    "text": "Then this step-- find_bin\ncumul_array, slot. So what this means is\nyou make this thing",
    "start": "624450",
    "end": "632640"
  },
  {
    "text": "where you mark the\ndifferent symbols. So the frequency of 0 is 3, so\nthis is 3; then frequency of 1",
    "start": "632640",
    "end": "640440"
  },
  {
    "text": "is 3, so 3; frequency of 2 is 2. OK. And then you need to\nlocate 5 in this thing.",
    "start": "640440",
    "end": "648540"
  },
  {
    "text": "Because your slot\nis 5, so 5 is here. So this slot was 5, and\nthen s is equal to 1",
    "start": "648540",
    "end": "655410"
  },
  {
    "text": "because this lies in the slot. And then you do this\ncalculation-- block_id times frequency plus\nslot minus cumulative s.",
    "start": "655410",
    "end": "663460"
  },
  {
    "text": "So block_id was\n14 times frequency of 1, which is 3 plus slot, 5,\nminus cumulative of 1, 3, 44.",
    "start": "663460",
    "end": "682345"
  },
  {
    "start": "682345",
    "end": "690350"
  },
  {
    "text": "Any questions? So one quick thing is again\nalways focus on the main step.",
    "start": "690350",
    "end": "698670"
  },
  {
    "text": "So main thing here\nis this step, really. So what this is doing really\nis x next is x multiplied by--",
    "start": "698670",
    "end": "707985"
  },
  {
    "start": "707985",
    "end": "722649"
  },
  {
    "text": "So you're doing x divided by\nthe probability of x really, and all of this is doing x\nmultiplied by probability of x.",
    "start": "722650",
    "end": "732070"
  },
  {
    "text": "So that is the main operation. That is what gives you the\nlog 1/px result and so on.",
    "start": "732070",
    "end": "739340"
  },
  {
    "text": "So just remember that\nthat is the core lesson. Please watch the\nlecture from last time",
    "start": "739340",
    "end": "744940"
  },
  {
    "text": "or do the last question\nin the homework if you want to learn\nmore about rANS.",
    "start": "744940",
    "end": "750954"
  },
  {
    "text": " OK.",
    "start": "750955",
    "end": "756290"
  },
  {
    "text": "Here I will ask for your help.  So the question is, when do\nyou want to use ANS as opposed",
    "start": "756290",
    "end": "763060"
  },
  {
    "text": "to arithmetic? So first part. Your application requires\nthe best compression ratio",
    "start": "763060",
    "end": "768370"
  },
  {
    "text": "and you're willing to sacrifice\nencoding or decoding speeds. So what do you think\nshould be the answer here?",
    "start": "768370",
    "end": "774310"
  },
  {
    "text": "Is the ANS the best\nanswer or something else? [INAUDIBLE]",
    "start": "774310",
    "end": "783920"
  },
  {
    "text": "OK. I see some heads shaking. So here actually arithmetic\nis typically the best answer.",
    "start": "783920",
    "end": "793380"
  },
  {
    "text": "Talk to us later if you\nthought differently. Your application\nrequires extremely fast",
    "start": "793380",
    "end": "799380"
  },
  {
    "text": "encoding/decoding and you\nare willing to sacrifice. Here the answer is\nclear, it's Huffman. If you look at that table\nfrom a few slides ago,",
    "start": "799380",
    "end": "805980"
  },
  {
    "text": "you can't beat Huffman at speed. It's just so simple.",
    "start": "805980",
    "end": "812640"
  },
  {
    "text": "Your application requires\nadaptive decoding. So we haven't talked as much\nabout this adaptive stuff yet.",
    "start": "812640",
    "end": "818070"
  },
  {
    "text": "I think we mentioned\nit in class. We will talk about it\nin the next lecture.",
    "start": "818070",
    "end": "825060"
  },
  {
    "text": "Again, talk to us\nif you are confused. But here really the best\nanswer is arithmetic coding.",
    "start": "825060",
    "end": "831900"
  },
  {
    "text": "I think this was in one of the\nslides in the last lecture. But you will understand\nwhat we mean by adaptive",
    "start": "831900",
    "end": "837629"
  },
  {
    "text": "in the next lecture. So the last two, you\ncare about achieving close-to-optimal compression\nbut also want good speed.",
    "start": "837630",
    "end": "845990"
  },
  {
    "text": "Here, I hope some of you\nthought ANS was the answer",
    "start": "845990",
    "end": "851300"
  },
  {
    "text": "and yes, indeed it is. And the last one is you're\nworking with a modern processor and want to exploit parallel\nprocessing while still",
    "start": "851300",
    "end": "858530"
  },
  {
    "text": "achieving close to\noptimal compression. So this is the same\nstuff that I was talking about, single\ninstruction, multiple data.",
    "start": "858530",
    "end": "865610"
  },
  {
    "text": "These are special\ninstructions on the modern x86 like the Intel machines. Also similar on the\nARM machines where",
    "start": "865610",
    "end": "872810"
  },
  {
    "text": "if you are doing the same\noperation again and again, you can parallelize\nthat operation.",
    "start": "872810",
    "end": "878450"
  },
  {
    "text": "And it's very interesting\nthe way they do it with rANS. I will not talk\nabout it too much, but the basic idea is that\nyou have your symbols--",
    "start": "878450",
    "end": "886850"
  },
  {
    "text": "symbol 1, symbol 2,\nsymbol, and so on. What you do is you\ncompress symbols 1, 3, 5, 7",
    "start": "886850",
    "end": "893329"
  },
  {
    "text": "and symbols 2, 4, 6, 8 in\nparallel, basically, which",
    "start": "893330",
    "end": "898490"
  },
  {
    "text": "is very, very suitable for\nthese modern processors who can do these two\noperations in parallel. Basically, you get 2x speed\nup without using two cores.",
    "start": "898490",
    "end": "906770"
  },
  {
    "text": "You need to write the\ncode in a proper way, but I think there\nwas a paper I linked",
    "start": "906770",
    "end": "915590"
  },
  {
    "text": "if you want to read more.  This paper.",
    "start": "915590",
    "end": "921029"
  },
  {
    "text": "So this paper should\ngive you a sense of that parallel\nencoding/decoding. ",
    "start": "921030",
    "end": "928900"
  },
  {
    "text": "OK. So here are the last two,\nwhat are the answers? Sometimes these are\na bit confusing. The idea is to get\nyou to appreciate",
    "start": "928900",
    "end": "937380"
  },
  {
    "text": "the relative importance\nand relative strengths and weaknesses of all\nthese compressors.",
    "start": "937380",
    "end": "943980"
  },
  {
    "text": "So in this case, the last two. OK. Any questions so far?",
    "start": "943980",
    "end": "949185"
  },
  {
    "text": "We will now start the material. Yep. Just a quick question\nabout the first slide.",
    "start": "949185",
    "end": "958410"
  },
  {
    "text": "I thought that one was\ntrue because if you look at the table of the\nspeeds, t and s actually",
    "start": "958410",
    "end": "966220"
  },
  {
    "text": "has the same optimal\ncompression ratio. It's significantly higher\nin encoding and decoding",
    "start": "966220",
    "end": "972130"
  },
  {
    "text": "speeds compared to arithmetic\nlike that in the Huffman coding. Yes. If you look at these two--",
    "start": "972130",
    "end": "977578"
  },
  {
    "text": "[INAUDIBLE] Yeah, but it is rANS. ",
    "start": "977578",
    "end": "984480"
  },
  {
    "text": "rANS is the same compression\nas arithmetic in this table and it's just\nfaster at decoding.",
    "start": "984480",
    "end": "991380"
  },
  {
    "text": "tANS is slightly\noverhead, I guess. It depends on the use case. ",
    "start": "991380",
    "end": "997950"
  },
  {
    "text": "Talk to us. Yeah. It will make a regret\nthing and we will consider.",
    "start": "997950",
    "end": "1003110"
  },
  {
    "text": "The first one I agree. It might be a bit. It's not as clear. ",
    "start": "1003110",
    "end": "1009149"
  },
  {
    "text": "OK. Honest advice, if you have iid\ndata and you know it's iid, don't use arithmetic\ncoding, use rANS or tANS.",
    "start": "1009150",
    "end": "1016250"
  },
  {
    "text": "Simple. You don't want to use\narithmetic coding for iid data. Really today the only\nreason to use arithmetic",
    "start": "1016250",
    "end": "1022520"
  },
  {
    "text": "coding if you have\nnon-iid or adaptive type of data distributions. Can we start another point?",
    "start": "1022520",
    "end": "1028589"
  },
  {
    "text": "So the table is on a\nspecific [INAUDIBLE] date. So don't [INAUDIBLE].",
    "start": "1028589",
    "end": "1034230"
  },
  {
    "text": "That is when the right one\neven that's not for [INAUDIBLE] has been saying that\nin the next lecture.",
    "start": "1034230",
    "end": "1041069"
  },
  {
    "text": "We will use case\nover [INAUDIBLE] that it allows you to adaptively\nwork with probabilities which",
    "start": "1041069",
    "end": "1046619"
  },
  {
    "text": "is sort of the best thing\nwhich you can do no matter what because of the [INAUDIBLE].",
    "start": "1046619",
    "end": "1054020"
  },
  {
    "text": "Yeah. But make a regret thing. Let's not worry too much\nabout-- these are small things.",
    "start": "1054020",
    "end": "1060900"
  },
  {
    "text": "I think as long as you\nunderstand the concepts that's what matters. OK.",
    "start": "1060900",
    "end": "1066180"
  },
  {
    "text": "We will move on.  But we will see arithmetic\ncoding is much more powerful.",
    "start": "1066180",
    "end": "1073320"
  },
  {
    "text": "We haven't seen\nthe full power yet. OK. How many of you have read this\nnovel or identified the novel?",
    "start": "1073320",
    "end": "1082679"
  },
  {
    "text": " Anyone? ",
    "start": "1082680",
    "end": "1089399"
  },
  {
    "text": "[INAUDIBLE] Yes, [INAUDIBLE]. [LAUGHS] OK. So this is The Hound of\nBaskervilles by Arthur Conan",
    "start": "1089400",
    "end": "1098580"
  },
  {
    "text": "Doyle. Sherlock Holmes. So it's a nice book. This is one of the paragraphs.",
    "start": "1098580",
    "end": "1106510"
  },
  {
    "text": "OK. So we read all these\nentropy coding and so on, so I thought let's try\nto compress this book.",
    "start": "1106510",
    "end": "1113440"
  },
  {
    "text": "And I basically call\nthis get_entropy function",
    "start": "1113440",
    "end": "1119940"
  },
  {
    "text": "to get the empirical entropy. You compute the frequency\nof each of the letters",
    "start": "1119940",
    "end": "1126990"
  },
  {
    "text": "and you do it. And I got this 199 kilobytes\nwhere the original file",
    "start": "1126990",
    "end": "1133380"
  },
  {
    "text": "was 387 kilobytes. So that's good. That's roughly a 2x compression. We are happy.",
    "start": "1133380",
    "end": "1138870"
  },
  {
    "text": "No. We are not. Because then I try to compress\nit with gzip and bzip2 which are some standard compressors.",
    "start": "1138870",
    "end": "1145612"
  },
  {
    "text": "They're not even\nstate-of-the-art. They're pretty old, 20-year-old\ncompressors, 30-year-old.",
    "start": "1145613",
    "end": "1152070"
  },
  {
    "text": "And they get much\nsmaller than entropy. But didn't we study that\nentropy is the best you can do?",
    "start": "1152070",
    "end": "1157680"
  },
  {
    "text": "Entropy is the lower bound on\nany prefix tree, any lossless compressor. ",
    "start": "1157680",
    "end": "1164990"
  },
  {
    "text": "So any suggestions? Why is entropy not\nthe best you can do?",
    "start": "1164990",
    "end": "1172290"
  },
  {
    "text": "Actually, some of you might\nhave noticed it in homework 1. I think we had a\nquestion where we asked you to do gzip on a file\nand you saw that it was better",
    "start": "1172290",
    "end": "1180028"
  },
  {
    "text": "than Huffman.  I think it has to\ndo with the fact that each letter you\ncan look at the property",
    "start": "1180028",
    "end": "1189971"
  },
  {
    "text": "given the first letter. For example, if you\nhave the letter T, there is a high probability this\nis going to be H, for example.",
    "start": "1189972",
    "end": "1197250"
  },
  {
    "text": "So maybe with that, you\nget a better performance. Yep, yep, yep. So a very, very good\nanswer which is that--",
    "start": "1197250",
    "end": "1205980"
  },
  {
    "text": "all of this is based\non iid assumption. For the first eight\nlectures-- first seven lectures we just studied iid.",
    "start": "1205980",
    "end": "1211100"
  },
  {
    "text": "So there is a risk that we\nassume the world is just iid, everything is independent\nof everything else. It is not.",
    "start": "1211100",
    "end": "1216139"
  },
  {
    "text": "We know that. So the example was if your\nprevious letter is a T,",
    "start": "1216140",
    "end": "1222200"
  },
  {
    "text": "you know with very high\nprobability or some probability that the next letter might\nbe a H. You have the, then,",
    "start": "1222200",
    "end": "1229520"
  },
  {
    "text": "this, that. A lot of words like that. Even a stronger example.",
    "start": "1229520",
    "end": "1234770"
  },
  {
    "text": "If your previous\nletter was a Q, you know with even\nhigher probability that the next letter\nis going to be a U.",
    "start": "1234770",
    "end": "1243110"
  },
  {
    "text": "Or if you are doing English and\nyou had a start of a sentence,",
    "start": "1243110",
    "end": "1248990"
  },
  {
    "text": "you know that the first\nletter will be capitalized and then you have small letters. So there are a lot\nof things you know",
    "start": "1248990",
    "end": "1255110"
  },
  {
    "text": "about English, which are not\nexplained by this iid property. iid would just say that you have\n15% E's and 10% T's and so on.",
    "start": "1255110",
    "end": "1264649"
  },
  {
    "text": "But it says nothing about if the\nprevious symbol was something, what is the next symbol? So again, iid means independent\nand identically distributed,",
    "start": "1264650",
    "end": "1275420"
  },
  {
    "text": "which says that\neach of your symbols is independent of\neach other and they have the same distribution.",
    "start": "1275420",
    "end": "1282020"
  },
  {
    "text": "But what you will see is that\ngiven your past, actually the distribution keeps\nchanging from symbol to symbol.",
    "start": "1282020",
    "end": "1287810"
  },
  {
    "text": "So that's the first thing. And there are other reasons. If you just compute\nthe entropy on a big--",
    "start": "1287810",
    "end": "1292850"
  },
  {
    "text": "you just concatenate two files. You take a Java code and you\ntake the Sherlock Holmes book",
    "start": "1292850",
    "end": "1300590"
  },
  {
    "text": "and you concatenate them. And you compute the\nentropy on both of them, the entropy will be a\nmix of the two entropies.",
    "start": "1300590",
    "end": "1307550"
  },
  {
    "text": "It's not the optimal thing to\ndo for either the Java code or for the Sherlock Holmes file.",
    "start": "1307550",
    "end": "1313309"
  },
  {
    "text": "So for these two reasons,\nbut really the first one is what we'll focus on for now.",
    "start": "1313310",
    "end": "1319039"
  },
  {
    "text": " So we'll look at non-iid data.",
    "start": "1319040",
    "end": "1326419"
  },
  {
    "text": "We'll look at\nreal-life data now. So we'll look at real-life data. We will do two things--\none is non-iid data",
    "start": "1326420",
    "end": "1335309"
  },
  {
    "text": "and the other is data\nwhose distribution we don't know a priori. So in homework 1, you\nalready saw an example",
    "start": "1335310",
    "end": "1342000"
  },
  {
    "text": "where you were given a\nfile and you computed the empirical entropy,\nyou encoded with Huffman coding you stored the codebook.",
    "start": "1342000",
    "end": "1348870"
  },
  {
    "text": "So that is one way to do it. There are other ways\nwe will look at where-- ",
    "start": "1348870",
    "end": "1354780"
  },
  {
    "text": "it is easy to say that I get\n50% 0's and 25% 1's and 25% 2's,",
    "start": "1354780",
    "end": "1361000"
  },
  {
    "text": "but who has seen a real\ndistribution like that? Oftentimes when you\nstart with a data source,",
    "start": "1361000",
    "end": "1366450"
  },
  {
    "text": "it's very complicated and you\ndon't know the distribution often. ",
    "start": "1366450",
    "end": "1374630"
  },
  {
    "text": "And we have been\ntalking just about text, but think of images. If I'm compressing\nthis image, this slide,",
    "start": "1374630",
    "end": "1385919"
  },
  {
    "text": "it's very correlated,\nthe nearby pixels. If the nearby pixel is white\nit's more likely to be white. The black guys are all bunched\nup together, the black pixels.",
    "start": "1385920",
    "end": "1396690"
  },
  {
    "text": "If you have a more scenery\nsort of thing, you have sky, the blue colors are nearby.",
    "start": "1396690",
    "end": "1402539"
  },
  {
    "text": "So there is some\ncorrelation nearby. If you think of video, video\nis also like nearby frames.",
    "start": "1402540",
    "end": "1411030"
  },
  {
    "text": "Even if there is motion,\nthere is a slow motion. So nearby frames\nlook very similar. So again, there is some\ndependence, tables.",
    "start": "1411030",
    "end": "1418470"
  },
  {
    "text": "And anything in real life,\neverything is non-iid. Very rarely you get from nature\nor from real life or thing",
    "start": "1418470",
    "end": "1424980"
  },
  {
    "text": "that's iid.  Any questions on\nthe motivation part?",
    "start": "1424980",
    "end": "1431735"
  },
  {
    "text": " So quick outline.",
    "start": "1431735",
    "end": "1437510"
  },
  {
    "text": "Today we will look at some-- for some of you who have\ndone a lot of probability",
    "start": "1437510",
    "end": "1443750"
  },
  {
    "text": "or information theory,\ntoday's lecture will be a bit redundant. So please bear with me. The next lecture will be new.",
    "start": "1443750",
    "end": "1451580"
  },
  {
    "text": "Also, if you don't like\ntheory today's lecture will have a little\nbit of theory. So again, please.",
    "start": "1451580",
    "end": "1456770"
  },
  {
    "text": "Then lecture 9. So today we will do basically\nthe equivalent of entropy but for non-iiid distributions.",
    "start": "1456770",
    "end": "1464600"
  },
  {
    "text": "And then the next lecture\nwe will start looking at some practical schemes for--",
    "start": "1464600",
    "end": "1470492"
  },
  {
    "text": "lecture 9 and\nlecture 10 are mostly about schemes for achieving\nthe optimal for non-iid data.",
    "start": "1470492",
    "end": "1477500"
  },
  {
    "text": "And really lecture 10\nwill be the last lecture of lossless compression where\nwe will summarize everything",
    "start": "1477500",
    "end": "1483890"
  },
  {
    "text": "we have learned and look\nat a bunch of case studies and see how to apply lossless\ncompression in practice. ",
    "start": "1483890",
    "end": "1491950"
  },
  {
    "text": "One thing is, I guess, I\nsaid everything is non-iid, but don't get disheartened. It's not like everything\nyou have done is useless.",
    "start": "1491950",
    "end": "1500043"
  },
  {
    "text": "You will see that all of\nthese tools we have learned are very, very\nuseful in this case also, also for\nlossy compression.",
    "start": "1500043",
    "end": "1508420"
  },
  {
    "start": "1508420",
    "end": "1514060"
  },
  {
    "text": "How many of you have\nseen Markov chains in a previous probability\ncourse or information",
    "start": "1514060",
    "end": "1519220"
  },
  {
    "text": "theory or any course at all? ",
    "start": "1519220",
    "end": "1524340"
  },
  {
    "text": "OK. A few of you have, some haven't. It's fine. You don't need to know\ntoo many properties,",
    "start": "1524340",
    "end": "1530330"
  },
  {
    "text": "but we will need some\nbasics, which I will cover. ",
    "start": "1530330",
    "end": "1538550"
  },
  {
    "text": "So recall that if you\nhave a block like you went through Un,\nin the iid case,",
    "start": "1538550",
    "end": "1543789"
  },
  {
    "text": "the joint probability\nof all of these guys is the product of\nthe probabilities.",
    "start": "1543790",
    "end": "1551750"
  },
  {
    "text": "But in general if you\nhave done chain rule, you can't just write\nit as a product.",
    "start": "1551750",
    "end": "1557310"
  },
  {
    "text": "You need to do this chain\nrule where it's like PU1, U2 is PU1 multiplied by PU2\ngiven U1, that sort of thing.",
    "start": "1557310",
    "end": "1568680"
  },
  {
    "text": "And then you can\nkeep extending this. So just the chain rule. ",
    "start": "1568680",
    "end": "1576600"
  },
  {
    "text": "If you've forgotten this\npart of your probability, we can share some material.",
    "start": "1576600",
    "end": "1583470"
  },
  {
    "text": "So we will just go all in\nfinite-length sequences now.",
    "start": "1583470",
    "end": "1593130"
  },
  {
    "text": "So given some alphabet\nU, a binary maybe or something else, alphabet\nmaybe like English alphabet,",
    "start": "1593130",
    "end": "1602020"
  },
  {
    "text": "you can create a\nstochastic process. So a stochastic process\nor a random process",
    "start": "1602020",
    "end": "1608690"
  },
  {
    "text": "is just any sequence\nof random variables with arbitrary dependence. ",
    "start": "1608690",
    "end": "1615490"
  },
  {
    "text": "And it is characterized\nby basically for every n, you have to describe the\nprobability distribution for that n-tuple.",
    "start": "1615490",
    "end": "1620998"
  },
  {
    "text": " So in the iid case,\nthis is very simple.",
    "start": "1620998",
    "end": "1626100"
  },
  {
    "text": "In the iid case for any\nn, this thing factorizes. So you write the probability\nof U1 through Un, it's just the product\nof U1, U2, U3, Un.",
    "start": "1626100",
    "end": "1633600"
  },
  {
    "text": "But in general if\nyou don't assume iid, this could have\narbitrary distributions. You have just a bunch\nof big, big distribution",
    "start": "1633600",
    "end": "1642030"
  },
  {
    "text": "tables sitting with you. And this is very, very general.",
    "start": "1642030",
    "end": "1647580"
  },
  {
    "text": "And we won't to talk about this. This is way too general to\nbe of much use in theory.",
    "start": "1647580",
    "end": "1654373"
  },
  {
    "text": "But you need to know there is\na notion of a random process where it's just a sequence of\nrandom variables with arbitrary",
    "start": "1654373",
    "end": "1660270"
  },
  {
    "text": "dependence on 1's. A random variable might depend\non something that's a million before it.",
    "start": "1660270",
    "end": "1666240"
  },
  {
    "text": "Anything can happen.  I will pause for questions\nafter a couple of these.",
    "start": "1666240",
    "end": "1673150"
  },
  {
    "text": "Then we have stationary\nstochastic process or a stationary\nprocess, which is",
    "start": "1673150",
    "end": "1679780"
  },
  {
    "text": "a stochastic or a random\nprocess that is time-invariant. So what it means\nis that whether you",
    "start": "1679780",
    "end": "1685870"
  },
  {
    "text": "are at position 0 in\nthe process or you are at position 100 or\nposition 1,000 or million, the probabilities don't change.",
    "start": "1685870",
    "end": "1691990"
  },
  {
    "text": "It's in a stationary\nstate in a way. We look at some\nexamples, but if you",
    "start": "1691990",
    "end": "1699310"
  },
  {
    "text": "like to define it very precisely\nwhat it says is that for any n",
    "start": "1699310",
    "end": "1704470"
  },
  {
    "text": "and for any l, the\nprobability that U1 through Un is equal to something. It's the same as the\nprobability at Ul plus 1",
    "start": "1704470",
    "end": "1712030"
  },
  {
    "text": "to Ul plus 1 is\nequal to something. So this is just saying\nthat if you shift your--",
    "start": "1712030",
    "end": "1720080"
  },
  {
    "text": "just an example, basically,\nwe are saying PU1, U2 will have the same\ndistribution as PU100 and U101.",
    "start": "1720080",
    "end": "1728779"
  },
  {
    "text": "So the same distribution. So this is an example of a--",
    "start": "1728780",
    "end": "1734290"
  },
  {
    "text": "a stationary process must\nsatisfy this sort of thing. Wherever you are in\nyour time series, it will have the same\ndistribution there.",
    "start": "1734290",
    "end": "1740710"
  },
  {
    "text": "Same joint distribution. ",
    "start": "1740710",
    "end": "1746409"
  },
  {
    "text": "The general definition\nis fine, but in particular, specific\nproperties are the mean, variance, entropy, all of\nthese do not change with n.",
    "start": "1746410",
    "end": "1754420"
  },
  {
    "text": "So what we are saying is the\nexpectation of U1 will be the same as expectation\nof U100, for example,",
    "start": "1754420",
    "end": "1761570"
  },
  {
    "text": "or the entropy of U1 will be\nthe same as entropy of U100 or the entropy of U1, U3 will\nbe the same as the entropy",
    "start": "1761570",
    "end": "1771490"
  },
  {
    "text": "of U1,000 and U2002. So this sort of properties.",
    "start": "1771490",
    "end": "1780600"
  },
  {
    "text": "And even the stationary\ndistributions are not necessarily very simple. They can have arbitrary\ntime dependence.",
    "start": "1780600",
    "end": "1787350"
  },
  {
    "text": "So in particular, U1 and\nU1,000,000,000 can be",
    "start": "1787350",
    "end": "1796100"
  },
  {
    "text": "dependent. It's not like they\nhave to be independent. You can have very\nfar-reaching dependence.",
    "start": "1796100",
    "end": "1801440"
  },
  {
    "text": " We will keep moving on.",
    "start": "1801440",
    "end": "1807480"
  },
  {
    "text": "We'll look at examples. You never really need to deal\nwith like a fully general stationary process.",
    "start": "1807480",
    "end": "1812520"
  },
  {
    "text": "We'll get more and more\nspecific as time goes on. ",
    "start": "1812520",
    "end": "1819810"
  },
  {
    "text": "Any questions so far?  So let's say you have\niid sequence which",
    "start": "1819810",
    "end": "1827300"
  },
  {
    "text": "is a sequence of\nfair iid coin tosses. So probability of head is\nequal to probability of tail",
    "start": "1827300",
    "end": "1832460"
  },
  {
    "text": "is equal to 1/2. Can someone tell me\nwhat is this thing? The probability that\nthe first four tosses",
    "start": "1832460",
    "end": "1838549"
  },
  {
    "text": "are in sequence H, T, T, H.",
    "start": "1838550",
    "end": "1846182"
  },
  {
    "text": "[INAUDIBLE] Yeah. I'm handing 1 over 2 to 4, which\nis correct because this is iid,",
    "start": "1846182",
    "end": "1852820"
  },
  {
    "text": "so this is just like PU1\nequal to H multiplied by PU2 equal to T, dot, dot.",
    "start": "1852820",
    "end": "1860530"
  },
  {
    "text": "Can somebody tell if I put some\nl here, so I just shift it, I'm looking further\nin the future--",
    "start": "1860530",
    "end": "1866380"
  },
  {
    "text": "maybe P101, 100, 200, and 304,\nwhat is the probability now? ",
    "start": "1866380",
    "end": "1873680"
  },
  {
    "text": "It's the same because\nagain, it's iid. And iid means independent\nand identically distributed. So all of these have\nthe same distribution,",
    "start": "1873680",
    "end": "1880370"
  },
  {
    "text": "they're all independent. So you can show it somewhere\nthat iid implies stationary.",
    "start": "1880370",
    "end": "1889419"
  },
  {
    "text": "So any iid. All the processes\nyou have seen so far are all stationary\nbecause they are iid.",
    "start": "1889420",
    "end": "1894490"
  },
  {
    "text": "iid is a very special\ncase of stationary. ",
    "start": "1894490",
    "end": "1900610"
  },
  {
    "text": "Now we'll look at a more\ncomplex thing, a Markov process. ",
    "start": "1900610",
    "end": "1909220"
  },
  {
    "text": "So a Markov process if\nyou have not seen one",
    "start": "1909220",
    "end": "1915010"
  },
  {
    "text": "is you can define it in terms\nof one-step probability.",
    "start": "1915010",
    "end": "1920760"
  },
  {
    "text": "So if you know U1 you can define\nthe probability distribution for U2 conditioned on U1.",
    "start": "1920760",
    "end": "1927000"
  },
  {
    "text": "And then if you have U3 then\nyou can define U3 given U2.",
    "start": "1927000",
    "end": "1932370"
  },
  {
    "text": "So the real property\nhere is that Un",
    "start": "1932370",
    "end": "1937840"
  },
  {
    "text": "is independent of Un minus\n2 to U1 given Un minus 1.",
    "start": "1937840",
    "end": "1951520"
  },
  {
    "text": "So what this says is that if you\nknow the just previous symbol, if you know Un minus 1,\nthen your future and past",
    "start": "1951520",
    "end": "1958419"
  },
  {
    "text": "are independent. So the only dependent\nis at one symbol level. So if you know the\nprevious symbol,",
    "start": "1958420",
    "end": "1964300"
  },
  {
    "text": "that's all you need to know. I tell you the last million\nsymbols, that doesn't matter, you only need to\nknow the last symbol.",
    "start": "1964300",
    "end": "1969970"
  },
  {
    "text": "That is the property\nof a Markov process. And I think it's easiest\nexplained through this diagram.",
    "start": "1969970",
    "end": "1976705"
  },
  {
    "text": " So we start with U1 which is\nuniformly drawn from 0, 1, 2.",
    "start": "1976705",
    "end": "1984400"
  },
  {
    "text": "So let's say you sample 0. Let's say U1 is 0.",
    "start": "1984400",
    "end": "1989960"
  },
  {
    "text": "So once you are at\n0 these arrows show what is the next symbol. So if you are 0 you see an\narrow with 0.5 probability going",
    "start": "1989960",
    "end": "1997940"
  },
  {
    "text": "to 1, and you see an\narrow with 0.5 probability is staying at 0.",
    "start": "1997940",
    "end": "2003230"
  },
  {
    "text": "So that means that if U1 is\n0 then U2 can either be 0 or it can be 1, that\nhalf of probability.",
    "start": "2003230",
    "end": "2011020"
  },
  {
    "text": "So it's a one-step process. I wrote it in another way. Maybe this will be easier\nfor some to understand.",
    "start": "2011020",
    "end": "2018390"
  },
  {
    "text": "U1 is again sampled\nuniformly and UI plus 1. So the next one is UI plus\nZi mod 3, where Zi is 0/1.",
    "start": "2018390",
    "end": "2027780"
  },
  {
    "text": "So basically, we are saying if\nyou are at 0, you can go to 0 or you can go to 1. If you are at 1 you can go\nto 1 or you can go to 2.",
    "start": "2027780",
    "end": "2035430"
  },
  {
    "text": "If you are at 2 you can go\nto 2 or you can go to 0. So mod 3. So you round back to this.",
    "start": "2035430",
    "end": "2042510"
  },
  {
    "text": "This is a very simple Markov\nprocess where at every step you either go to yourself\nor you go to 1 plus U.",
    "start": "2042510",
    "end": "2049739"
  },
  {
    "text": "And there are various\nways to represent it. You don't really need\nto worry too much,",
    "start": "2049739",
    "end": "2055080"
  },
  {
    "text": "but maybe you have seen\none of these, not the other so that's why I made all three. So this just shows this thing as\na table where if you are at 0,",
    "start": "2055080",
    "end": "2063030"
  },
  {
    "text": "then the next guy can be 0 or\n1 with 0.5, 0.5 probability. If you are at 1, then the next\nguy can be like this, too.",
    "start": "2063030",
    "end": "2069388"
  },
  {
    "text": "So that's another\nrepresentation. It's called a transition matrix. Or you can write it in the form\nof an equation, which is not",
    "start": "2069389",
    "end": "2076570"
  },
  {
    "text": "always possible,\nor you can write it in this graphical manner. We'll mostly make\nthis graphical manner",
    "start": "2076570",
    "end": "2082510"
  },
  {
    "text": "for the rest of the lecture. ",
    "start": "2082510",
    "end": "2088020"
  },
  {
    "text": "OK. Now let me do this math and stop\nme whenever you don't like it.",
    "start": "2088020",
    "end": "2095510"
  },
  {
    "start": "2095510",
    "end": "2111622"
  },
  {
    "text": "Can somebody tell\nme what this rule is called, this step\nthat I just performed? ",
    "start": "2111622",
    "end": "2131380"
  },
  {
    "text": "OK. So this is just called\nmarginalization, the opposite of this step.",
    "start": "2131380",
    "end": "2138640"
  },
  {
    "text": "If you add up U1 over\nall its values then you're just left\nwith U2, basically. So that's a marginalization.",
    "start": "2138640",
    "end": "2146650"
  },
  {
    "text": "We'll have a quiz question. So I hope this example will\nhelp you do that very easily.",
    "start": "2146650",
    "end": "2154079"
  },
  {
    "text": "Now, let's do it this way--",
    "start": "2154080",
    "end": "2161240"
  },
  {
    "text": "PU2 equal to 0. This is a very simple topology\nif you already know this,",
    "start": "2161240",
    "end": "2166490"
  },
  {
    "text": "but we will do it. OK. Can somebody tell what I\ndid in the second step? ",
    "start": "2166490",
    "end": "2178490"
  },
  {
    "text": "This is just the chain rule. ",
    "start": "2178490",
    "end": "2184240"
  },
  {
    "text": "These are not hard\nquestions, I just want to make sure we're\nvery comfortable with this.",
    "start": "2184240",
    "end": "2189460"
  },
  {
    "text": "Now let's just write it\ndown and quickly compute it. ",
    "start": "2189460",
    "end": "2200220"
  },
  {
    "text": "U1 equal to 1, U2 equal\nto 0 given U1 equal to 1.",
    "start": "2200220",
    "end": "2207869"
  },
  {
    "text": "U1 equal to 2, U2 equal\nto 0, U1 equal to 2.",
    "start": "2207870",
    "end": "2213410"
  },
  {
    "text": " Now I'm going to\nask for your help. Please help me fill\nin each of these.",
    "start": "2213410",
    "end": "2219390"
  },
  {
    "text": "So what is U1 equal to 0,\nprobability that U1 equal to 0? It's 1 over 3.",
    "start": "2219390",
    "end": "2225940"
  },
  {
    "text": "Yeah, it's 1/3. Why? Because U1 is given\nas uniform 0, 1, 2. So let me just fill\nin all three of these",
    "start": "2225940",
    "end": "2232450"
  },
  {
    "text": "because we know all\nthree of these are 1/3. OK. Somebody tell me what is\nprobability U2 equal to 0 given",
    "start": "2232450",
    "end": "2239840"
  },
  {
    "text": "U1 equal to 0. Anybody else? Half. Half.",
    "start": "2239840",
    "end": "2245150"
  },
  {
    "text": "Right. Half which is correct because\nit's basically this branch.",
    "start": "2245150",
    "end": "2250950"
  },
  {
    "text": "If you are at 0 then the next\none is 0 with probability half. What is the probability\nthat U2 equal to 0",
    "start": "2250950",
    "end": "2256329"
  },
  {
    "text": "given that U1 equal to 1?  0.",
    "start": "2256330",
    "end": "2261400"
  },
  {
    "text": "Yeah, I heard 0 which\nis again correct. Why? Because if you are at\n1, so you are here,",
    "start": "2261400",
    "end": "2267460"
  },
  {
    "text": "then the probability\nof going to 0 is 0. There is no way\nyou go back to 0.",
    "start": "2267460",
    "end": "2273670"
  },
  {
    "text": "And again this guy is half. So after you compute\nall of this you will get this thing\nis equal to 1/3.",
    "start": "2273670",
    "end": "2280000"
  },
  {
    "start": "2280000",
    "end": "2293503"
  },
  {
    "text": "[INAUDIBLE] Right. [INAUDIBLE]",
    "start": "2293503",
    "end": "2302580"
  },
  {
    "text": "Right. So a quick note is\nthat for a Markov chain",
    "start": "2302580",
    "end": "2308660"
  },
  {
    "text": "if you can show that U2 and U1\nhave same distribution, that",
    "start": "2308660",
    "end": "2316839"
  },
  {
    "text": "is enough to show\nthat it's stationary. So this implies that it's\na stationary Markov chain.",
    "start": "2316840",
    "end": "2324230"
  },
  {
    "text": "And you can easily check that it\nis stationary here because see, U1 had a uniform distribution.",
    "start": "2324230",
    "end": "2330350"
  },
  {
    "text": "And if you compute U2\nequal to 0, U2 equal to 1, U2 equal to 2, you will again\nget the uniform distribution.",
    "start": "2330350",
    "end": "2335780"
  },
  {
    "text": "So the distribution\ndidn't change in one step. And then just by\ninduction, you can show that it won't change\nin the next step also.",
    "start": "2335780",
    "end": "2342770"
  },
  {
    "text": "And then you can show that\nall joint distribution nothing changes. So it's stationary chain. At every step,\neach of these guys",
    "start": "2342770",
    "end": "2350329"
  },
  {
    "text": "have 1/3 probability\nof being in that state. ",
    "start": "2350330",
    "end": "2356440"
  },
  {
    "text": "If you don't fully\nappreciate it, it's fine. But you will have\nquestions in quiz, so just",
    "start": "2356440",
    "end": "2362410"
  },
  {
    "text": "know that this can be done. ",
    "start": "2362410",
    "end": "2370410"
  },
  {
    "text": "This I will leave as a homework. You will have a\nquiz question which is very similar, which is that\nif you start with probability",
    "start": "2370410",
    "end": "2378420"
  },
  {
    "text": "U1 equal-- sorry, if you assume\nthat U1 is equal to 0-- actually, can somebody help me?",
    "start": "2378420",
    "end": "2383970"
  },
  {
    "text": "Is the process still stationary? Can somebody tell me\nwhat is the probability that U2 equal to\n0, if U1 is known",
    "start": "2383970",
    "end": "2391140"
  },
  {
    "text": "to be 0 with probability 1? Half. Yeah. I hear that U2 is 0\nis probability half.",
    "start": "2391140",
    "end": "2398910"
  },
  {
    "text": "Why? Because U1 you know\ndefinitely is that 0. So U2 will be back here\nwith probability half.",
    "start": "2398910",
    "end": "2406530"
  },
  {
    "text": "So is this process stationary?  It's not because clearly\nthe probability has changed.",
    "start": "2406530",
    "end": "2413790"
  },
  {
    "text": "Stationary means that\nthe probability stays constant across time steps. So it's important that when\nyou have a Markov chain",
    "start": "2413790",
    "end": "2419579"
  },
  {
    "text": "you start with the\ncorrect distribution. If you don't, then it\nwon't be stationary. ",
    "start": "2419580",
    "end": "2427805"
  },
  {
    "text": "[INAUDIBLE] Yeah. That's good to know. I think many of you\nhave done probability,",
    "start": "2427805",
    "end": "2433970"
  },
  {
    "text": "so hopefully nothing new here. This is probably the\nmore interesting question",
    "start": "2433970",
    "end": "2439730"
  },
  {
    "text": "from a compression perspective.  So given the sequence\nthat we just made,",
    "start": "2439730",
    "end": "2447490"
  },
  {
    "text": "can somebody suggest me a way\nto make it into an iid sequence? This is a Markov sequence,\nit has dependence,",
    "start": "2447490",
    "end": "2455050"
  },
  {
    "text": "can you help me make it\ninto an iid sequence? ",
    "start": "2455050",
    "end": "2463210"
  },
  {
    "text": "And maybe I will\nwrite it in a way that will be easier to understand.",
    "start": "2463210",
    "end": "2468400"
  },
  {
    "text": "So you have U1 then you have U1\nplus Z1, where plus is modulo",
    "start": "2468400",
    "end": "2474670"
  },
  {
    "text": "3, so we want to write that. And then U2 plus Z2\nthen U3 plus Z3, dot,",
    "start": "2474670",
    "end": "2486350"
  },
  {
    "text": "dot, dot, where Zi our\niid Bernoulli half. ",
    "start": "2486350",
    "end": "2493260"
  },
  {
    "text": "So if I write it in\nthis way, can somebody suggest how can I convert\nthis into iid sequence?",
    "start": "2493260",
    "end": "2499365"
  },
  {
    "start": "2499365",
    "end": "2509644"
  },
  {
    "text": "This is my U1, this\nis my U2, U3, U4.",
    "start": "2509644",
    "end": "2515050"
  },
  {
    "text": "Yeah. Subtract the\nconsecutive numbers. Yeah.",
    "start": "2515050",
    "end": "2520690"
  },
  {
    "text": "Very good. So you just subtract them. You just subtract them. So you take U2 minus\nU1, which is Z1,",
    "start": "2520690",
    "end": "2528780"
  },
  {
    "text": "and then U3 minus U2,\nwhich is Z2, and so on. You just subtract them.",
    "start": "2528780",
    "end": "2533910"
  },
  {
    "text": "Again, modulo 3. And when you subtract\nthem you get these Z1, Z2 guys which are iid because,\nby definition, the way we",
    "start": "2533910",
    "end": "2540720"
  },
  {
    "text": "define this is like this. So even though originally the\nprocess looked more complicated",
    "start": "2540720",
    "end": "2546660"
  },
  {
    "text": "it had dependence. If you just subtract\nthe consecutive ones it becomes iid. ",
    "start": "2546660",
    "end": "2553940"
  },
  {
    "text": "It's not always this\nsimple, but oftentimes you will see that you can do simple\ntransformations on your data.",
    "start": "2553940",
    "end": "2559010"
  },
  {
    "text": "You start with a\nstationary process. Maybe you start with a\nnonstationary process and you just do some simple\ntricks it becomes iid,",
    "start": "2559010",
    "end": "2565580"
  },
  {
    "text": "and then you apply\nyour favorite-- Huffman coding, ANS,\nor whatever you like. And we'll see this again\nand again, especially",
    "start": "2565580",
    "end": "2571760"
  },
  {
    "text": "in the lossy compression part. So this is a very simple\ntransform in a way.",
    "start": "2571760",
    "end": "2578060"
  },
  {
    "text": "It's like delta\ncoding or diff coding.  Any questions so far?",
    "start": "2578060",
    "end": "2584085"
  },
  {
    "start": "2584085",
    "end": "2591530"
  },
  {
    "text": "OK. So we will just make\nit more general. And this will be\nsomewhat important.",
    "start": "2591530",
    "end": "2598070"
  },
  {
    "text": "We will look at codes\nfor this, but basically, the idea is you can generalize\nthis Markov thing, where the Markov thing we said was\ngiven the previous symbol,",
    "start": "2598070",
    "end": "2607670"
  },
  {
    "text": "you don't have any dependence\non the further past. Kth order Markov means that\nyour current symbol just depends",
    "start": "2607670",
    "end": "2614090"
  },
  {
    "text": "on the previous k symbols. If you know the\nprevious k symbols, that is all you need to know. Knowing the further away\npast doesn't matter.",
    "start": "2614090",
    "end": "2622190"
  },
  {
    "text": "So if you like to think\nin terms of prediction, so what we are\nsaying is Un minus 1",
    "start": "2622190",
    "end": "2628910"
  },
  {
    "text": "through Un minus k is\nenough to predict Un.",
    "start": "2628910",
    "end": "2637180"
  },
  {
    "text": "Knowing things even\nbefore n minus k won't really help you in\npredicting Un better or knowing",
    "start": "2637180",
    "end": "2643329"
  },
  {
    "text": "the distribution better. Let me just read\nthrough the definition.",
    "start": "2643330",
    "end": "2648400"
  },
  {
    "text": "So a kth Markov source is\ndefined by the condition-- probability of Un given\nits entire past is",
    "start": "2648400",
    "end": "2657080"
  },
  {
    "text": "same as the probability of Un\njust given the last k symbols for every n.",
    "start": "2657080",
    "end": "2663420"
  },
  {
    "text": "Inwards, the\nconditional probability of Un given the entire\npast depends only on the past k symbols,\nor more precisely Un",
    "start": "2663420",
    "end": "2671580"
  },
  {
    "text": "is independent of the past older\nthan k symbols given the last k symbols.",
    "start": "2671580",
    "end": "2676650"
  },
  {
    "text": "So knowing the last k symbols\nis the best predictor for Un. ",
    "start": "2676650",
    "end": "2682420"
  },
  {
    "text": "So we saw that in\nstationary sources. You could have arbitrary\ntime dependence, but in kth order Markov\nsource you have just",
    "start": "2682420",
    "end": "2687730"
  },
  {
    "text": "passed k symbols you depend on. So if you are generating this,\nyou take the last k sequence,",
    "start": "2687730",
    "end": "2693910"
  },
  {
    "text": "you generate the next one,\nthen the last k generates, and so on.",
    "start": "2693910",
    "end": "2699590"
  },
  {
    "text": "So most practical\nstationary sources can be approximated well with\na finite kth order model.",
    "start": "2699590",
    "end": "2705940"
  },
  {
    "text": "And you can go higher k and then\nyou get a better approximation with diminishing returns. ",
    "start": "2705940",
    "end": "2714400"
  },
  {
    "text": "Any questions on kth\norder Markov sources? It's a generalization\nof first-order like the normal Markov\nsource we just saw.",
    "start": "2714400",
    "end": "2720670"
  },
  {
    "text": " When we just say Markov, we\nusually mean first order.",
    "start": "2720670",
    "end": "2730285"
  },
  {
    "text": " And when we say iid\nwe mean zeroth order",
    "start": "2730285",
    "end": "2735440"
  },
  {
    "text": "because iid is you\njust depend on nobody.",
    "start": "2735440",
    "end": "2740650"
  },
  {
    "text": "So k is 0. You don't need to know any\npast to predict the future. ",
    "start": "2740650",
    "end": "2754380"
  },
  {
    "text": "OK. Let's quickly look\nat a nonexample of-- I guess I told you\nthe answer, but--",
    "start": "2754380",
    "end": "2761950"
  },
  {
    "text": "so you are at a bus stop\nwhich is reasonably regular, buses arrive at roughly\n15-minute frequency.",
    "start": "2761950",
    "end": "2768280"
  },
  {
    "text": "And these are let's\nsay the bus timings-- 4:16, 4:28, 4:46, 5:02, is\nthis a stationary process?",
    "start": "2768280",
    "end": "2775000"
  },
  {
    "text": "Is the sequence of\ntimes, bus arrival times, is this stationary? It is definitely\nrandom because buses",
    "start": "2775000",
    "end": "2781320"
  },
  {
    "text": "have some randomness\nwhen they exactly arrive, but is it stationary? Is it iid, first of all? Is it stationary?",
    "start": "2781320",
    "end": "2788658"
  },
  {
    "text": "[COUGH]  So what are the properties\nof stationary processes?",
    "start": "2788658",
    "end": "2795040"
  },
  {
    "text": "Among other things,\nwe heard that the mean doesn't change with time. So do we think the average\nbus arrival time keeps--",
    "start": "2795040",
    "end": "2805059"
  },
  {
    "text": "if the first bus is expected\nto arrive at 4:00 PM, is the second bus also\nexpected to arrive at 4:00 PM?",
    "start": "2805060",
    "end": "2812800"
  },
  {
    "text": "No. It's expected to arrive\nat 4:15 and the next one is 4:30 and so on. So this is not a\nstationary process.",
    "start": "2812800",
    "end": "2821190"
  },
  {
    "text": "In particular, the mean\nincreases with time. ",
    "start": "2821190",
    "end": "2827550"
  },
  {
    "text": "Is that clear that\nthis is not stationary? because not all\nbuses are supposed",
    "start": "2827550",
    "end": "2833850"
  },
  {
    "text": "to arrive at the\nexact same time. OK. Can you convert it to\na stationary process",
    "start": "2833850",
    "end": "2839890"
  },
  {
    "text": "by doing a simple\ntransformation? ",
    "start": "2839890",
    "end": "2848506"
  },
  {
    "text": "We need to see the right\ndifference between two halves. Yep, yep, yep, yep.",
    "start": "2848506",
    "end": "2855190"
  },
  {
    "text": "So again, the answer is\njust take the difference. This is surprisingly common.",
    "start": "2855190",
    "end": "2861800"
  },
  {
    "text": "In real-life data, you\nwill see a lot of times-- we will see a lot of\ncompressors, in this lecture,",
    "start": "2861800",
    "end": "2867110"
  },
  {
    "text": "the next lecture, and so on.  All of them will\nwork wonderfully",
    "start": "2867110",
    "end": "2872690"
  },
  {
    "text": "on stationary sequences. But if you give it a\nnonstationary like this, they just start failing.",
    "start": "2872690",
    "end": "2878599"
  },
  {
    "text": "You take the best\nstate-of-the-art Zstandard or one of these compressors, you\ngive it a file which has 1, 2,",
    "start": "2878600",
    "end": "2883819"
  },
  {
    "text": "3, 4, 5, 6, just a sequence\nof numbers which is trivial to predict. If you know the previous\none you just do plus 1.",
    "start": "2883820",
    "end": "2889940"
  },
  {
    "text": "And it will just do a terrible\njob at compressing it. But if you just do this delta\ncoding or this diff difference",
    "start": "2889940",
    "end": "2895970"
  },
  {
    "text": "between the consecutive\nones, it just becomes a sequence of very\nsimilar-looking values",
    "start": "2895970",
    "end": "2901900"
  },
  {
    "text": "and then suddenly the\ncompression improves. So again, very important to--",
    "start": "2901900",
    "end": "2909526"
  },
  {
    "text": "but that's why this\ntheory is important. You understand where do\nthese compressors do well, where do they not do well.",
    "start": "2909526",
    "end": "2915910"
  },
  {
    "text": "And when they not\ndo well, can you convert it into a\ndifferent sequence? So here again you do\nthe delta or the diff.",
    "start": "2915910",
    "end": "2925450"
  },
  {
    "text": "You just take the difference. So you just do 428 minus\n416, this is 12 minutes.",
    "start": "2925450",
    "end": "2940050"
  },
  {
    "text": "If you have done the process\nthis is the interarrival time,",
    "start": "2940050",
    "end": "2945250"
  },
  {
    "text": "usually, modeled as exponential\ndistribution in theory. ",
    "start": "2945250",
    "end": "2955450"
  },
  {
    "text": "OK. Let's move on to\ninformation-theoretic quantities for non-iid\nrandom variables",
    "start": "2955450",
    "end": "2961289"
  },
  {
    "text": "if we have no questions.  These you will see\nin your homework.",
    "start": "2961290",
    "end": "2968690"
  },
  {
    "text": "So, I guess, hopefully,\nthis will help you with the problems. ",
    "start": "2968690",
    "end": "2976890"
  },
  {
    "text": "So we saw the entropy. I hope everybody\nremembers entropy of U",
    "start": "2976890",
    "end": "2982440"
  },
  {
    "text": "was we did Pu log 1 over\nPu over all the u's.",
    "start": "2982440",
    "end": "2990020"
  },
  {
    "text": "This was the entropy. So here we define the\nconditional entropy of U given V. So now we have two\nrandom variables U and V,",
    "start": "2990020",
    "end": "2997220"
  },
  {
    "text": "which are not\nnecessarily independent. And this is defined\nas this thing.",
    "start": "2997220",
    "end": "3003369"
  },
  {
    "text": "So you can write this as\nexpectation of log 1 over Pu. ",
    "start": "3003370",
    "end": "3011640"
  },
  {
    "text": "So a very similar formula. Good. And then you can expand it out.",
    "start": "3011640",
    "end": "3018000"
  },
  {
    "text": "We will try to do\none example in class. So this is just the definition\nof the expectation, nothing",
    "start": "3018000",
    "end": "3028360"
  },
  {
    "text": "very clever here. Second step, we\nuse the chain rule. So we do Pu, v is\nlike Pv multiplied",
    "start": "3028360",
    "end": "3034330"
  },
  {
    "text": "by Pu given v. And then\nyou take this big thing",
    "start": "3034330",
    "end": "3041480"
  },
  {
    "text": "and you can write it like this. So really we are\ndefining two quantities.",
    "start": "3041480",
    "end": "3047530"
  },
  {
    "text": "We are defining two quantities.",
    "start": "3047530",
    "end": "3055240"
  },
  {
    "text": "So one is this thing. This you can read as\nconditional entropy of U given V",
    "start": "3055240",
    "end": "3069680"
  },
  {
    "text": "is equal to v. So\nif you know that V is equal to small v, then what\nis the condition entropy of U?",
    "start": "3069680",
    "end": "3075770"
  },
  {
    "text": "And this one is\nnothing different from what you have already read.",
    "start": "3075770",
    "end": "3080960"
  },
  {
    "text": "This is just the entropy if you\ntake this distribution, u given v equal to v. This one\nyou have already seen.",
    "start": "3080960",
    "end": "3087710"
  },
  {
    "text": "So let's say, for\nexample, if v equal to v implies that u is\nBernoulli half,",
    "start": "3087710",
    "end": "3094400"
  },
  {
    "text": "for example, then the entropy\nof u given v equal to v,",
    "start": "3094400",
    "end": "3099410"
  },
  {
    "text": "somebody tell me\nwhat is the answer. What is the entropy\nof Bernoulli half?",
    "start": "3099410",
    "end": "3104855"
  },
  {
    "text": "1. 1. Right. So this one we are\nvery comfortable with. We have seen this a\nmillion times this sort",
    "start": "3104855",
    "end": "3111760"
  },
  {
    "text": "of simple thing-- not a million but a few times.",
    "start": "3111760",
    "end": "3116819"
  },
  {
    "text": "And then the second\nthing we define is this, which is just\nconditional entropy of u given",
    "start": "3116820",
    "end": "3128810"
  },
  {
    "text": "v. So this is an average\nof the above quantity.",
    "start": "3128810",
    "end": "3136400"
  },
  {
    "text": "So this is on average, what is\nthe conditional entropy of u given v and therefore\nit is defined",
    "start": "3136400",
    "end": "3143690"
  },
  {
    "text": "as Pv Hu v give equal to v?",
    "start": "3143690",
    "end": "3149510"
  },
  {
    "text": "So what do we--  can somebody remind me\nwhat was the intuitive",
    "start": "3149510",
    "end": "3156640"
  },
  {
    "text": "meaning of entropy? ",
    "start": "3156640",
    "end": "3163539"
  },
  {
    "text": "If entropy is high,\nwhat does that mean? [INAUDIBLE]",
    "start": "3163540",
    "end": "3168910"
  },
  {
    "text": "Yep, yep. Like, uniform distribution\nthat [INAUDIBLE] the constant [INAUDIBLE].",
    "start": "3168910",
    "end": "3175240"
  },
  {
    "text": "Exactly, exactly. Right. So entropy was a\nmeasure of uncertainty, it was a measure of randomness. If it's a constant, it's 0\nentropy, uniform, maximum",
    "start": "3175240",
    "end": "3182560"
  },
  {
    "text": "entropy. So can you guess\nwhat is the meaning of the conditional entropy\nof u given v equal to v",
    "start": "3182560",
    "end": "3190430"
  },
  {
    "text": "based on what we know\nabout entropy already? So let's say knowing\nthat V equal to v,",
    "start": "3190430",
    "end": "3196760"
  },
  {
    "text": "small v makes U\ndeterministic, then",
    "start": "3196760",
    "end": "3201950"
  },
  {
    "text": "the entropy will be very\nsmall, the conditional entropy. So the conditional\nentropy is really--",
    "start": "3201950",
    "end": "3208340"
  },
  {
    "text": "again, the uncertainty\nleft in U or uncertainty in U given that U know\nsomething about v.",
    "start": "3208340",
    "end": "3216200"
  },
  {
    "text": "So this thing is a\nmeasure of uncertainty",
    "start": "3216200",
    "end": "3224230"
  },
  {
    "text": "in U given that v is equal to v.\nSo if you know the value of v,",
    "start": "3224230",
    "end": "3232390"
  },
  {
    "text": "what is the uncertainty in U? And this guy is the\naverage uncertainty",
    "start": "3232390",
    "end": "3244130"
  },
  {
    "text": "in U given that\ngiven v, basically.",
    "start": "3244130",
    "end": "3250329"
  },
  {
    "text": "So on average if you know\nv, how much uncertainty do you have about U? So just think of an example.",
    "start": "3250330",
    "end": "3256869"
  },
  {
    "text": "For example, you are playing\nthis murder mystery type",
    "start": "3256870",
    "end": "3264740"
  },
  {
    "text": "of game where you have\nsuspicion on a bunch of different suspects. So that is U. U is\nthe variable that",
    "start": "3264740",
    "end": "3271490"
  },
  {
    "text": "represents who is the murderer. And then you keep\ngetting specific clues.",
    "start": "3271490",
    "end": "3277220"
  },
  {
    "text": "So maybe you get some\nfingerprint clue. And maybe that will change\nthe uncertainty in who",
    "start": "3277220",
    "end": "3282850"
  },
  {
    "text": "you think the murderer is. You will find that some clues\nwill increase your uncertainty.",
    "start": "3282850",
    "end": "3288516"
  },
  {
    "text": "Earlier you thought, oh, no, I\nthink this guy is the murderer. But then you see this clue and\nthen it confuses you again.",
    "start": "3288517",
    "end": "3294340"
  },
  {
    "text": "So what you will see\nis that this quantity can be bigger than or smaller\nthan the entropy of U.",
    "start": "3294340",
    "end": "3301470"
  },
  {
    "text": "Knowing a specific\npiece of evidence can increase your entropy\nor decrease your entropy.",
    "start": "3301470",
    "end": "3306660"
  },
  {
    "text": "But you will see\nthat on average. Knowing more can't hurt you. The more you know\nabout v, it will never",
    "start": "3306660",
    "end": "3313380"
  },
  {
    "text": "hurt you in terms of U. So it will decrease\nyour uncertainty. So as you play this\nmurder game, over time",
    "start": "3313380",
    "end": "3320430"
  },
  {
    "text": "as you get more and more\nclues, your uncertainty will keep reducing. And if you are a good\ndetective like Sherlock Holmes",
    "start": "3320430",
    "end": "3325867"
  },
  {
    "text": "you will converge on\nthe correct culprit. ",
    "start": "3325867",
    "end": "3333780"
  },
  {
    "text": "Any questions? ",
    "start": "3333780",
    "end": "3340040"
  },
  {
    "text": "OK. Let's try to do this example. ",
    "start": "3340040",
    "end": "3347140"
  },
  {
    "text": "So you are given\nthis distribution. This is a joint distribution\nbetween u and v.",
    "start": "3347140",
    "end": "3353099"
  },
  {
    "text": "So the way this is\nwritten is like this cell is the probability\nthat u is 0 and v is 0.",
    "start": "3353100",
    "end": "3362550"
  },
  {
    "text": "And this cell is u is 1, v is 1. ",
    "start": "3362550",
    "end": "3369260"
  },
  {
    "text": "So if you read this,\nI would encourage you to do this\ncarefully at home,",
    "start": "3369260",
    "end": "3376010"
  },
  {
    "text": "you will see that u is actually\nuniformly distributed in 0, 1. And knowing that u is 0, so that\nis the first row in that table,",
    "start": "3376010",
    "end": "3384652"
  },
  {
    "text": "it means that v has\nequal probability of being 0 or being 1.",
    "start": "3384652",
    "end": "3389960"
  },
  {
    "text": "So what is the\nformula we use here? It's like this guy-- v equal to 0 given u equal\nto 0 is simply Pu equal to 0,",
    "start": "3389960",
    "end": "3399530"
  },
  {
    "text": "v equal to 0 divided\nby probability that u equal to 0, which is 1/4\ndivided by 1/2 which is 1/2.",
    "start": "3399530",
    "end": "3407680"
  },
  {
    "text": " And if u is known to be 1, then\nyou see v has no uncertainty,",
    "start": "3407680",
    "end": "3414060"
  },
  {
    "text": "v is definitely 1. So if u is 1 then v is 1.",
    "start": "3414060",
    "end": "3419369"
  },
  {
    "text": "This is simple stuff.  Can we try this?",
    "start": "3419370",
    "end": "3424770"
  },
  {
    "text": "So what is the entropy in v\ngiven that u is equal to 0? ",
    "start": "3424770",
    "end": "3435806"
  },
  {
    "text": "So u is equal to 0. Entropy in v given\nthat u equal to 0. 1. It's 1.",
    "start": "3435806",
    "end": "3441230"
  },
  {
    "text": "Why? Because we saw\nthat if u is 0 then v is simply Bernoulli half. So that has entropy 1.",
    "start": "3441230",
    "end": "3447290"
  },
  {
    "text": "What about this guy, entropy\nin v if u is equal to 1? It's 0.",
    "start": "3447290",
    "end": "3453760"
  },
  {
    "text": "It's 0 again. Why is 0? Because this is deterministic. Knowing u equal to 1 definitely\ntells us v equal to 1.",
    "start": "3453760",
    "end": "3461050"
  },
  {
    "text": "And now we'll just\ncalculate the entropy. The average of these\nquantities which is Pu equal to 0 times Hv given\nu equal to 0 plus Pu equal to 1",
    "start": "3461050",
    "end": "3472766"
  },
  {
    "text": "Hv u equal to 1. And then you can do 1/2\ntimes 1 plus 1/2 times 0.",
    "start": "3472767",
    "end": "3480830"
  },
  {
    "text": "Because we saw that u is only\n1/2 here, so this is just half. ",
    "start": "3480830",
    "end": "3488730"
  },
  {
    "text": "And a few things are computed at\nhome was that the entropy of u is 1 because its Bernoulli\nhalf, entropy of v is 0.811.",
    "start": "3488730",
    "end": "3499330"
  },
  {
    "text": "So you already see that\nthe entropy of this guy is bigger than or equal to this.",
    "start": "3499330",
    "end": "3505720"
  },
  {
    "text": "So you see that knowing u has\ndecreased your uncertainty about v.",
    "start": "3505720",
    "end": "3512099"
  },
  {
    "text": "So v on its own had\nsome uncertainty, 0.811, but knowing u\nreduced it to half.",
    "start": "3512100",
    "end": "3519610"
  },
  {
    "text": "But you see that if you know\nu equal to 0, that actually increases your uncertainty,\nbecause knowing u equal to 1",
    "start": "3519610",
    "end": "3529030"
  },
  {
    "text": "reduces your uncertainty\nabout v, knowing u equal to 0 increases your uncertainty\nabout v, and then on average,",
    "start": "3529030",
    "end": "3535420"
  },
  {
    "text": "it still gets lower. And this is a\ncalculation that you can do very easily,\nwhich is you just take",
    "start": "3535420",
    "end": "3543490"
  },
  {
    "text": "all four things in the table\nand do this sort of thing. ",
    "start": "3543490",
    "end": "3558890"
  },
  {
    "text": "So 1/2 plus 1/2.",
    "start": "3558890",
    "end": "3565019"
  },
  {
    "text": "I think it will be 3/2. And you can verify that this is\nactually equal to this thing--",
    "start": "3565020",
    "end": "3571745"
  },
  {
    "text": "Hu plus Hv given u. So this is the chain\nrule for entropies.",
    "start": "3571745",
    "end": "3580030"
  },
  {
    "text": "So the joint entropy\nof u and v is actually equal to the entropy of u\nplus the entropy of v given u. So the overall\nuncertainty in u and v",
    "start": "3580030",
    "end": "3586780"
  },
  {
    "text": "combined is the\nuncertainty v in u and the uncertainty\nin v given u. ",
    "start": "3586780",
    "end": "3594170"
  },
  {
    "text": "So a few properties about\nconditional entropy.",
    "start": "3594170",
    "end": "3600710"
  },
  {
    "text": "First is that conditioning\nreduces entropy, which is that\nentropy of u given v will be less than or\nequal to u with equality",
    "start": "3600710",
    "end": "3608059"
  },
  {
    "text": "if and if u and v\nare independent. So if u and v are independent\nthey will have the same.",
    "start": "3608060",
    "end": "3613310"
  },
  {
    "text": "So knowing anything about v\nwill tell you nothing about u.",
    "start": "3613310",
    "end": "3618930"
  },
  {
    "text": "And this is the thing I was\ntelling you earlier about like, a specific piece of evidence\ncan increase your uncertainty",
    "start": "3618930",
    "end": "3625670"
  },
  {
    "text": "but on average, knowledge will\nalways reduce your uncertainty. Knowing things is\ngood on average.",
    "start": "3625670",
    "end": "3631670"
  },
  {
    "text": " Let me just show all\nthe properties at once.",
    "start": "3631670",
    "end": "3637430"
  },
  {
    "text": "Then we have the chain rule\nof entropies, which is this. At the entropy\njoint entropy is--",
    "start": "3637430",
    "end": "3643560"
  },
  {
    "text": "if you remember the chain rule\nwe saw before for iid if you recall, not iid just\nfor independent,",
    "start": "3643560",
    "end": "3654850"
  },
  {
    "text": "the entropy was some\nof these entropies. But for non-iid in\na more general case,",
    "start": "3654850",
    "end": "3660560"
  },
  {
    "text": "it is actually this formula-- HU plus HV given U.",
    "start": "3660560",
    "end": "3666550"
  },
  {
    "text": "Let me quickly state it\nin terms of compression because that's the\nclass we are in. So you remember HU was\nthe best thing you can do",
    "start": "3666550",
    "end": "3674650"
  },
  {
    "text": "for compressing U on its own. HV was the best\nthing you could do for compressing V on its own.",
    "start": "3674650",
    "end": "3680620"
  },
  {
    "text": "And in fact, HV\ngiven U is the best thing you can do for\ncompressing V if you know U.",
    "start": "3680620",
    "end": "3687540"
  },
  {
    "text": "So this is the minimum number of\nbits you can use to compress V if you knew U. So\nwhat we are saying",
    "start": "3687540",
    "end": "3694210"
  },
  {
    "text": "is if you want to\njointly compress U and V, what you could do is you\nfirst compress U on its own",
    "start": "3694210",
    "end": "3699250"
  },
  {
    "text": "and then you compress V\ngiven that you know U. So it's a two-step compression\nprocess where you first",
    "start": "3699250",
    "end": "3705100"
  },
  {
    "text": "compress just U and\nthen you compress V given the knowledge of U. And the bits add up basically.",
    "start": "3705100",
    "end": "3711880"
  },
  {
    "text": "So maybe this guy uses one bit\nand this guy uses like 1/2 bit. And so overall,\nyou get 3/2 2 bits",
    "start": "3711880",
    "end": "3718540"
  },
  {
    "text": "for jointly compressing U and V. And the last thing\nwhich you see here is saying that the joint\nentropy between U and V",
    "start": "3718540",
    "end": "3726400"
  },
  {
    "text": "is either equal to or smaller\nthan the sum of the entropies,",
    "start": "3726400",
    "end": "3739329"
  },
  {
    "text": "this means that if\nyou independently compress U and V. So let's\ntake an extreme example.",
    "start": "3739330",
    "end": "3745210"
  },
  {
    "text": " Let's say you had a random\nvariable which was either",
    "start": "3745210",
    "end": "3752680"
  },
  {
    "text": "1 or 0, and then you had\na second random variable which was always the opposite\nof the first random variable.",
    "start": "3752680",
    "end": "3758049"
  },
  {
    "text": "So if the first random variable\nis 1, then the other guy is 0. If the first one is\n0, the other is 1.",
    "start": "3758050",
    "end": "3763120"
  },
  {
    "text": "So if you compress\nthem independently they will take 1 bit\neach, whereas if you",
    "start": "3763120",
    "end": "3768790"
  },
  {
    "text": "use the knowledge of the first\nguy to compress the other guy, then you can compress\nthe second one for free.",
    "start": "3768790",
    "end": "3774970"
  },
  {
    "text": "So I hope this is somewhat-- so if we do this\nsimple example where",
    "start": "3774970",
    "end": "3784030"
  },
  {
    "text": "if U is Bernoulli half\nand V is 1 minus U,",
    "start": "3784030",
    "end": "3791560"
  },
  {
    "text": "so HU will be 1\nand so will be HV, but the joint entropy of U\nand V is actually still 1.",
    "start": "3791560",
    "end": "3799460"
  },
  {
    "text": "So this is less than HU plus HV.",
    "start": "3799460",
    "end": "3805410"
  },
  {
    "text": "Here knowing one of\nthem basically tells you the other one. So compressing them\njointly is much better",
    "start": "3805410",
    "end": "3811170"
  },
  {
    "text": "than compressing\nthem independently. So that is the whole\nidea about all of this.",
    "start": "3811170",
    "end": "3816869"
  },
  {
    "text": "There is a question\nin the quiz, there is a question in the homework. So do those and I hope you\nwill get very comfortable",
    "start": "3816870",
    "end": "3825030"
  },
  {
    "text": "with these quantities.  Any questions?",
    "start": "3825030",
    "end": "3831890"
  },
  {
    "text": "Just in general you\ncan have this condition on the past n things\ninstead of just conditioning on the past thing.",
    "start": "3831890",
    "end": "3839020"
  },
  {
    "text": "Any questions? ",
    "start": "3839020",
    "end": "3848840"
  },
  {
    "text": "OK. Now we define the real\nquantity for stationary sources",
    "start": "3848840",
    "end": "3855319"
  },
  {
    "text": "which is that what we want to do\nis we want to-- we had entropy. Entropy was the best\nthing you could do",
    "start": "3855320",
    "end": "3861930"
  },
  {
    "text": "for compressing iid sequences. Now we want to compress\nstationary sequences or Markov",
    "start": "3861930",
    "end": "3869190"
  },
  {
    "text": "or any of these guys. So we will define something\ncalled entropy rate which",
    "start": "3869190",
    "end": "3875520"
  },
  {
    "text": "is defined for any stationary\nprocess and not only one",
    "start": "3875520",
    "end": "3882850"
  },
  {
    "text": "but two ways of defining it. So sort of buy one get\none free sort of deal.",
    "start": "3882850",
    "end": "3888760"
  },
  {
    "text": " There are these two definitions.",
    "start": "3888760",
    "end": "3894099"
  },
  {
    "text": "We won't go into the proofs, we\nwon't go into very specifics, but the basic idea\nis that entropy rate",
    "start": "3894100",
    "end": "3901020"
  },
  {
    "text": "of a stationary process\nis defined in two ways. The first way is you take\nthe past and symbols.",
    "start": "3901020",
    "end": "3909800"
  },
  {
    "text": "You compute up. If you know the past, how\nmuch is left in the future? Basically, that's what\nyou're trying to do.",
    "start": "3909800",
    "end": "3915920"
  },
  {
    "text": "So if you knew the\nfirst million symbols, what is the remaining\nentropy in the next symbol? How much is each\nnext symbol adding?",
    "start": "3915920",
    "end": "3923089"
  },
  {
    "text": "Basically, the uncertainty. That is your entropy rate\nand it's defined like this. This is the limit.",
    "start": "3923090",
    "end": "3929210"
  },
  {
    "text": "The second way to\ndefine it is, again, you can think in terms\nof compression if I want to jointly\ncompress n symbols at a time",
    "start": "3929210",
    "end": "3937150"
  },
  {
    "text": "and then take the per\nsymbol how much am I paying for compression. And then you take the limit.",
    "start": "3937150",
    "end": "3943090"
  },
  {
    "text": "So you make the blocks\nbigger and bigger and bigger and bigger, and you see\nper symbol how much do I need to pay for compression.",
    "start": "3943090",
    "end": "3949359"
  },
  {
    "text": "So there are two\nways of defining it. And in fact, you can\nshow that these two are equal for any stationary\nprocess and the limit is",
    "start": "3949360",
    "end": "3958180"
  },
  {
    "text": "called the entropy rate. You won't have that\nmuch use for it really.",
    "start": "3958180",
    "end": "3963670"
  },
  {
    "text": "I think if you want to remember\none of these definitions, remember this guy. This is the main one. This is the one we'll\nuse more frequently.",
    "start": "3963670",
    "end": "3971580"
  },
  {
    "text": "This is in terms of\nlike how much extra-- let me write it down. So this is on average bits\nper symbol for large blocks.",
    "start": "3971580",
    "end": "3992450"
  },
  {
    "text": "And this one is additional\nentropy first symbol",
    "start": "3992450",
    "end": "4006040"
  },
  {
    "text": "given the past. So each symbol, how\nmuch extra entropy is it adding to your\nthing, and we'll",
    "start": "4006040",
    "end": "4011080"
  },
  {
    "text": "look at a couple of\nexamples in a minute. ",
    "start": "4011080",
    "end": "4016797"
  },
  {
    "text": "I guess we'll look\nat the examples. ",
    "start": "4016797",
    "end": "4021799"
  },
  {
    "text": "So actually what happens\nis for a fair coin toss if you look at this guy U1\nthrough Un divided by n,",
    "start": "4021800",
    "end": "4030570"
  },
  {
    "text": "if you remember from\nyour previous lectures this is just this.",
    "start": "4030570",
    "end": "4036420"
  },
  {
    "text": "So this is just 1 bit. Basically, the joint entropy\nof the first and symbols",
    "start": "4036420",
    "end": "4041570"
  },
  {
    "text": "is just n times the entropy\nof the first symbol, which is 1 bit. So for iid sequences in\ngeneral, the entropy rate",
    "start": "4041570",
    "end": "4050000"
  },
  {
    "text": "is just equal to the entropy. So for iid, entropy rate\nis equal to entropy.",
    "start": "4050000",
    "end": "4058720"
  },
  {
    "text": " The Markov thing\nis a bit trickier.",
    "start": "4058720",
    "end": "4066750"
  },
  {
    "text": "Maybe let's just try\nto compute this one. ",
    "start": "4066750",
    "end": "4073599"
  },
  {
    "text": "So let me go through the logic. So the idea is--",
    "start": "4073600",
    "end": "4078740"
  },
  {
    "text": "so if you know u1, so\nyou can assume let's say u1 is equal\nto 0, for example.",
    "start": "4078740",
    "end": "4087280"
  },
  {
    "text": "u2 takes two values\nwith equal probability.",
    "start": "4087280",
    "end": "4094890"
  },
  {
    "text": "We just saw this example\nearlier in class. We saw that if u1 is 0\nthen u2 can be either 0",
    "start": "4094890",
    "end": "4102700"
  },
  {
    "text": "or it can be 1 each with\nhalf of probability. So therefore, it's easy\nto convince yourself",
    "start": "4102700",
    "end": "4110589"
  },
  {
    "text": "that actually Hu2 given\nu1 is equal to the entropy",
    "start": "4110590",
    "end": "4115810"
  },
  {
    "text": "of a Bernoulli half\nrandom variable because if you take two\nvalues with equal probability and this is just one bit.",
    "start": "4115810",
    "end": "4123799"
  },
  {
    "text": "And then what you\ncan convince yourself is that Un given Un\nminus 1 dot, dot, dot u1 is actually just\nequal to Un given Un minus 1.",
    "start": "4123800",
    "end": "4133609"
  },
  {
    "text": "And why is this? This is because of\nthe Markov property. Markov property says\nthat if the last symbol",
    "start": "4133609",
    "end": "4139278"
  },
  {
    "text": "the previous symbols\nhave got nothing to do with the next symbol. Just that last symbol is\nthe one you care about.",
    "start": "4139279",
    "end": "4144723"
  },
  {
    "start": "4144724",
    "end": "4149970"
  },
  {
    "text": "So that's it. Basically, you get the\nentropy rate is 1 bit. ",
    "start": "4149970",
    "end": "4157160"
  },
  {
    "text": "Let me pause here for a second. There is a question\nvery similar,",
    "start": "4157160",
    "end": "4163490"
  },
  {
    "text": "feel free to compute\nthis carefully. ",
    "start": "4163490",
    "end": "4177270"
  },
  {
    "text": "Let me just write it clearly. So for iid, there is an entropy\nrate written in different ways",
    "start": "4177270",
    "end": "4185620"
  },
  {
    "text": "depending on the text. I will just try to use a\nboldface italics type U",
    "start": "4185620",
    "end": "4190659"
  },
  {
    "text": "is equal to the Hu1 basically. For iid, the\nentropy rate is just the entropy of each symbol.",
    "start": "4190660",
    "end": "4197800"
  },
  {
    "text": "For kth order Markov in\ngeneral, the entropy rate",
    "start": "4197800",
    "end": "4207909"
  },
  {
    "text": "is the entropy of the k\nplus 1 at the symbol given the first case symbols.",
    "start": "4207910",
    "end": "4217040"
  },
  {
    "text": "So in these cases, the entropy\nrate is easy to compute. And basically, the\nidea is that if you",
    "start": "4217040",
    "end": "4222820"
  },
  {
    "text": "know the past case symbols,\nhow much uncertainty is in the next symbol? That's the whole idea here.",
    "start": "4222820",
    "end": "4230920"
  },
  {
    "text": "Let's look at a real example. English. So you can take a lot of\nEnglish texts like a big corpus.",
    "start": "4230920",
    "end": "4238720"
  },
  {
    "text": "And then what you\ncan try to do is try to estimate the probabilities--",
    "start": "4238720",
    "end": "4244630"
  },
  {
    "text": "you estimate the\nprobability distribution. You make a probability\nmodel out of English. You take two symbols\nat a time, you",
    "start": "4244630",
    "end": "4251199"
  },
  {
    "text": "take three symbols at a time,\nthree characters at a time, four characters at\na time, and so on, and you make this\nprobability table.",
    "start": "4251200",
    "end": "4257290"
  },
  {
    "text": "So what you could do is you\ncould find probability of u",
    "start": "4257290",
    "end": "4265630"
  },
  {
    "text": "or you can find\nprobability of u1, u2 like two\ncharacters at a time, or you do u1, u2, u3, and so on.",
    "start": "4265630",
    "end": "4273380"
  },
  {
    "text": "So you can make more and more\ncomplicated models of English. So for example, how\nwould you estimate this?",
    "start": "4273380",
    "end": "4279880"
  },
  {
    "text": "So if you do what is the\nprobability that u1 is A and u2 is C, so this will\nbe number of times",
    "start": "4279880",
    "end": "4288490"
  },
  {
    "text": "you have seen x some xn\nequal to A and xn plus 1",
    "start": "4288490",
    "end": "4293650"
  },
  {
    "text": "equal to C divided by\ntotal sort of thing.",
    "start": "4293650",
    "end": "4298760"
  },
  {
    "text": "So you just\nempirically estimate. If you have seen this AC\ncombination a million times",
    "start": "4298760",
    "end": "4303830"
  },
  {
    "text": "out of a billion times, then you\ndo like million by billion sort of thing.",
    "start": "4303830",
    "end": "4309440"
  },
  {
    "text": "So somebody did this\nfor some English corpus. And what they found was\nif you assume it's iid,",
    "start": "4309440",
    "end": "4315110"
  },
  {
    "text": "so zeroth order\nMarkov chain, they got 4.7 six bits per letter.",
    "start": "4315110",
    "end": "4320420"
  },
  {
    "text": "So this is what they got. And as you might imagine, as\nyou take more and more history,",
    "start": "4320420",
    "end": "4327760"
  },
  {
    "text": "your uncertainty will\nreduce more and more because you can use that\nto predict the next symbol. So if you take a\nfirst-order Markov chain,",
    "start": "4327760",
    "end": "4334360"
  },
  {
    "text": "so if you use the\nprevious symbol to predict the next\nsymbol, so now suddenly you can use the fact\nthat after a Q you",
    "start": "4334360",
    "end": "4340390"
  },
  {
    "text": "mostly get a U sort of\nthing, and suddenly you do slightly better. You do 4.76 to 4.03.",
    "start": "4340390",
    "end": "4347980"
  },
  {
    "text": "So your conditional\nentropy reduces.",
    "start": "4347980",
    "end": "4353340"
  },
  {
    "text": "Maybe you take four\nprevious characters. For example, if you\nsee T, H, I, then",
    "start": "4353340",
    "end": "4359820"
  },
  {
    "text": "it's maybe it's a this\nor it's a thing, so n. So you are even more certain\nabout your prediction.",
    "start": "4359820",
    "end": "4366750"
  },
  {
    "text": "And the entropy reduces further. It becomes 2.8. And then Shannon in 1952\nactually did a very interesting",
    "start": "4366750",
    "end": "4375640"
  },
  {
    "text": "experiment where he\ngot a few people. I think his wife, actually.",
    "start": "4375640",
    "end": "4380850"
  },
  {
    "text": "But he got some test\nsubjects, basically. And they gave them some\nsymbols in some English text",
    "start": "4380850",
    "end": "4388830"
  },
  {
    "text": "and asked them to\npredict the next symbol. And then they predicted the\nnext symbol and they say what is the probability of--",
    "start": "4388830",
    "end": "4395370"
  },
  {
    "text": "did they predict the\nright one or not? And then you predict\nthe next symbol. They got a human to actually\npredict each symbol.",
    "start": "4395370",
    "end": "4402360"
  },
  {
    "text": "And they use that to estimate\nthe entropy in English, how much uncertainty\nis in English,",
    "start": "4402360",
    "end": "4407400"
  },
  {
    "text": "assuming that a human\nis a perfect predictor. And they got 1.3.",
    "start": "4407400",
    "end": "4413690"
  },
  {
    "text": "You see a fourth--\nit's much better than a fourth-order model. And which is true. The previous word tells you\na lot about the next word,",
    "start": "4413690",
    "end": "4420527"
  },
  {
    "text": "the previous sentence tells you\na lot about the next sentence. So the dependence goes a\nbit further than just four",
    "start": "4420527",
    "end": "4426440"
  },
  {
    "text": "characters. And obviously, now LLMs\nhave beaten humans.",
    "start": "4426440",
    "end": "4432990"
  },
  {
    "text": "So if you look at\nthe latest models we'll look at them\nin the next lecture,",
    "start": "4432990",
    "end": "4438000"
  },
  {
    "text": "they get a rate which\nis even lower than this. So they have become\neven stronger predictors",
    "start": "4438000",
    "end": "4443730"
  },
  {
    "text": "of English than\nhumans at this point, subject to the experiment\nbeing done in a specific way.",
    "start": "4443730",
    "end": "4452039"
  },
  {
    "text": " OK. So last slide for today.",
    "start": "4452040",
    "end": "4457485"
  },
  {
    "text": " Just in the previous one you\nsaw we said bits per letter",
    "start": "4457485",
    "end": "4465290"
  },
  {
    "text": "and I've been using compression\nanalogies all the time. So basically kth order Markov\nsource, the best compression",
    "start": "4465290",
    "end": "4480059"
  },
  {
    "text": "you can do is basically this-- the Huk given Uk minus\n1 dot, dot, dot U1.",
    "start": "4480060",
    "end": "4485670"
  },
  {
    "text": "And in general for\na stationary source, entropy rate is the\nbest compression",
    "start": "4485670",
    "end": "4491780"
  },
  {
    "text": "you can hope to achieve. This is the takeaway. So just like the AEP\nproperty or typical sets",
    "start": "4491780",
    "end": "4497060"
  },
  {
    "text": "that you read\nabout in lecture 5, there is a similar\nproperty called",
    "start": "4497060",
    "end": "4502340"
  },
  {
    "text": "Shannon-McMillan-Breiman\ntheorem, which says that the probabilities\nof these blocks",
    "start": "4502340",
    "end": "4508130"
  },
  {
    "text": "are roughly equal to 2\npower minus entropy rate. So it's exactly the same\nproperty you saw before.",
    "start": "4508130",
    "end": "4513230"
  },
  {
    "text": "Just replace entropy\nby entropy rate. You need some\ntechnical conditions which we won't go into,\nbut basically, that's",
    "start": "4513230",
    "end": "4522900"
  },
  {
    "text": "why these quantities are\nimportant because, again, they give us the best you can\ndo in terms of compression. If you have a first-order\nMarkov source then Hu2 given Un",
    "start": "4522900",
    "end": "4530610"
  },
  {
    "text": "is the best compression\nyou can get. And if you have a general\nstationary source, the best compression you\ncan get is the entropy rate.",
    "start": "4530610",
    "end": "4538875"
  },
  {
    "text": " So in the next lecture what\nwe are going to look at",
    "start": "4538875",
    "end": "4545310"
  },
  {
    "text": "is we will actually\nsee how we can achieve this entropy, this\nconditional entropy,",
    "start": "4545310",
    "end": "4551369"
  },
  {
    "text": "how do we work with\nthese dependent sources. And we will look more into--",
    "start": "4551370",
    "end": "4558869"
  },
  {
    "text": "first, we'll start simple,\nthis first order, then we'll go to kth order, and then we'll\njust say any stationary source.",
    "start": "4558870",
    "end": "4564720"
  },
  {
    "text": "So just rapidly increase\nthe generality of our ideas,",
    "start": "4564720",
    "end": "4570540"
  },
  {
    "text": "and we'll use arithmetic coding,\nas you might have guessed.",
    "start": "4570540",
    "end": "4576450"
  },
  {
    "text": "Thank you. Happy to answer any questions. ",
    "start": "4576450",
    "end": "4584000"
  }
]