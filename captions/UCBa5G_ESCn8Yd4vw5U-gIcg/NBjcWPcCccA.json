[
  {
    "start": "0",
    "end": "102000"
  },
  {
    "text": "So let's get started. First, ah, some logistics. So Homework 3 is due tonight, ah,",
    "start": "4850",
    "end": "12825"
  },
  {
    "text": "and that's the last homework assignment beyond your, ah, your projects and the project milestone.",
    "start": "12825",
    "end": "19560"
  },
  {
    "text": "The first milestone, um, the only milestone is due next Wednesday. Ah, and then after that,",
    "start": "19560",
    "end": "26320"
  },
  {
    "text": "ah, you will, um, just have the, the poster session and the final, um,",
    "start": "26420",
    "end": "31440"
  },
  {
    "text": "presentation or the final, ah, report. Uh, we'll be sending out feedback on your project proposals within the next few days.",
    "start": "31440",
    "end": "40995"
  },
  {
    "text": "Great. So, uh, let's go through the plan for today. So, uh, today, we're gonna be talking about model-based reinforcement learning, um,",
    "start": "40995",
    "end": "47960"
  },
  {
    "text": "and how it can be used for multitask learning and meta-learning, and also how it contrasts with model-free learning,",
    "start": "47960",
    "end": "55040"
  },
  {
    "text": "which is the kind of reinforced learning that we've been talking about thus far in the course. Um, we'll also be talking about how we can extend",
    "start": "55040",
    "end": "61190"
  },
  {
    "text": "model-based reinforcement learning to image observations or other high-dimensional inputs. Uh, this is one of- ah, a very challenging use-case for,",
    "start": "61190",
    "end": "67575"
  },
  {
    "text": "for our model-based reinforcement learning, and so, we'll, um, we'll be covering that in, uh, in more detail.",
    "start": "67575",
    "end": "73875"
  },
  {
    "text": "And then, we'll be also talking about model-based meta reinforcement learning, um, and settings where that might be applicable.",
    "start": "73875",
    "end": "80930"
  },
  {
    "text": "Ah, and also just kind of- by the end of this lecture, some of the things that you'll hopefully be able to learn about are,",
    "start": "80930",
    "end": "87220"
  },
  {
    "text": "ah, how to understand and use, ah, and implement model-based reinforcement learning methods, uh,",
    "start": "87220",
    "end": "92840"
  },
  {
    "text": "challenges and strategies for model-based reinforcement learning with high-dimensional inputs, and also, um, how this relates to multitask learning and meta-learning.",
    "start": "92840",
    "end": "101135"
  },
  {
    "text": "Okay. So first, let's talk about, uh, reinforcement learning algorithms from a, a broader view.",
    "start": "101135",
    "end": "107970"
  },
  {
    "start": "102000",
    "end": "167000"
  },
  {
    "text": "Uh, so in previous lectures, we showed this diagram where we looked at reinforcement learning as an algorithm that, uh,",
    "start": "107970",
    "end": "114670"
  },
  {
    "text": "iterates between generating samples, fitting a model to estimate the return from those samples, and then using that to improve the policy.",
    "start": "114670",
    "end": "121850"
  },
  {
    "text": "And we talked about Q-learning based methods and policy gradient based methods that correspond to estimating a return,",
    "start": "121850",
    "end": "128450"
  },
  {
    "text": "uh, or fitting a Q function. Ah, and in contrast, model-based approaches try to fit a model of the dynamics.",
    "start": "128450",
    "end": "135760"
  },
  {
    "text": "Ah, then each of these approaches improve the policy by either applying the policy gradient, uh, by taking a,",
    "start": "135760",
    "end": "141700"
  },
  {
    "text": "a max over your Q function, or to improve your policy, um, or to optimize actions or optimize the parameters of your policy using your model.",
    "start": "141700",
    "end": "150275"
  },
  {
    "text": "So previous lectures, we focused on model-free methods like policy gradients and Q-learning. Uh, and in this lecture,",
    "start": "150275",
    "end": "156330"
  },
  {
    "text": "we'll be focusing on what's known as model-based methods. Ah, they're known as model-based methods because you're trying to fit this model of the,",
    "start": "156330",
    "end": "163580"
  },
  {
    "text": "what's known as the dynamics model. Okay. Um, so the main idea of",
    "start": "163580",
    "end": "169040"
  },
  {
    "start": "167000",
    "end": "311000"
  },
  {
    "text": "model-based reinforcement learning is to learn a model of the environment. Ah, and you might wonder,",
    "start": "169040",
    "end": "174600"
  },
  {
    "text": "hey, why do we wanna do this? The previous reinforcement learning methods seem to work pretty well too. Or maybe they don't depending on,",
    "start": "174600",
    "end": "180754"
  },
  {
    "text": "on what you found in your project, or, or in your homework, um, and there's kind of two main reasons I think,",
    "start": "180755",
    "end": "187760"
  },
  {
    "text": "um, at least from what I've seen, uh, in terms of my own, uh, experiments is that model-based reinforcement learning",
    "start": "187760",
    "end": "193760"
  },
  {
    "text": "tends to lead to better sample efficiency. So if you care about, uh, learning with not a lot of interactions in the environment,",
    "start": "193760",
    "end": "200930"
  },
  {
    "text": "fitting a model of the environment and then using that model to, uh, optimize your policy can reduce the amount of data that you need in the environment.",
    "start": "200930",
    "end": "208645"
  },
  {
    "text": "Um, and this isn't true in all cases, ah, but it's true- it has at least been empirically true in a number of, ah, different works.",
    "start": "208645",
    "end": "216650"
  },
  {
    "text": "And also, the model can be reused, ah, for different tasks and different objectives.",
    "start": "216650",
    "end": "223460"
  },
  {
    "text": "And we'll talk a bit about what that means later in the lecture. Um, and so, ah,",
    "start": "223460",
    "end": "229770"
  },
  {
    "text": "at a high-level what this- what these algorithms are, are trying to do is they're trying to estimate a, a model of the dynamics, ah,",
    "start": "229770",
    "end": "235640"
  },
  {
    "text": "and this just corresponds to a supervised learning problem where you want to maximize the likelihood of the next state given",
    "start": "235640",
    "end": "241460"
  },
  {
    "text": "the current state and the current action for all of the transitions in your buffer of data.",
    "start": "241460",
    "end": "247160"
  },
  {
    "text": "Uh, so for example, you could treat this, ah, if, if you have continuous states, ah,",
    "start": "247160",
    "end": "252590"
  },
  {
    "text": "and you wanna model the likelihood using a Gaussian, ah, you could just use the,",
    "start": "252590",
    "end": "257840"
  },
  {
    "text": "the following optimization problem where you want to be able to, ah, minimize the squared error between the predictions from",
    "start": "257840",
    "end": "264500"
  },
  {
    "text": "your model and the observed next state. And this would be an example of a deterministic model.",
    "start": "264500",
    "end": "270560"
  },
  {
    "text": "You could also imagine using probabilistic or stochastic models as well. That actually try to model but that that likelihood,",
    "start": "270560",
    "end": "277535"
  },
  {
    "text": "um, model the full distribution of the likelihood. Okay. Um, and so,",
    "start": "277535",
    "end": "282900"
  },
  {
    "text": "this is kind of the- ah, there's different ways, different model classes that you can use and different, ah,",
    "start": "282900",
    "end": "288105"
  },
  {
    "text": "ways that you'd go about maximizing that likelihood but it typically just amounts to a supervised learning problem. Um, and then, once we have our model,",
    "start": "288105",
    "end": "295655"
  },
  {
    "text": "we use that to improve our policy, um, and I'll talk a bit about the different ways that we can do that in a minute.",
    "start": "295655",
    "end": "300880"
  },
  {
    "text": "Um, and then you can use that policy or use the actions that you optimize to generate samples,",
    "start": "300880",
    "end": "306050"
  },
  {
    "text": "ah, and repeat this process. [NOISE] Okay. So now, what does this have to do with multitask learning and meta-learning?",
    "start": "306050",
    "end": "316845"
  },
  {
    "start": "311000",
    "end": "488000"
  },
  {
    "text": "So, um, let's go back to our notion of what a reinforcement learning task is,",
    "start": "316845",
    "end": "321945"
  },
  {
    "text": "ah, and in particular, we considered, um, this reinforcement learning task is basically an MDP,",
    "start": "321945",
    "end": "327740"
  },
  {
    "text": "where different tasks may have different state spaces, different action spaces, different initial state distributions, dynamics, and rewards.",
    "start": "327740",
    "end": "335349"
  },
  {
    "text": "Um, essentially, these, these tasks correspond to MDPs, um, and one kind of observation is that in",
    "start": "335350",
    "end": "341440"
  },
  {
    "text": "many practical scenarios that we might care about, ah, in multitask reinforcement learning and in meta reinforcement learning,",
    "start": "341440",
    "end": "348504"
  },
  {
    "text": "ah, it may be that the dynamics don't actually vary across tasks. Uh, that there's basically one single dynamics model that governs the, ah,",
    "start": "348505",
    "end": "357010"
  },
  {
    "text": "governs the world that your agent is living in, ah, and if this is true then we may be able to,",
    "start": "357010",
    "end": "363364"
  },
  {
    "text": "ah, kind of exploit that property. Uh, so for example, in the real-world, if your agent is manipulating objects,",
    "start": "363364",
    "end": "370205"
  },
  {
    "text": "or if it is, ah, ah, walking around on the ground, or if it is navigating, um,",
    "start": "370205",
    "end": "376885"
  },
  {
    "text": "in an environment the dynamics of the world- of the underlying world for manipulating different ob- ah, for doing different things in different objects,",
    "start": "376885",
    "end": "383875"
  },
  {
    "text": "for getting to somewhere in the environment through locomotion or through navigation, in all these settings, the underlying dynamics of the world isn't necessarily changing.",
    "start": "383875",
    "end": "392270"
  },
  {
    "text": "Um, of course, when the environment is fully observed. If you can't fully observe, ah, phy- physical information about the objects,",
    "start": "392270",
    "end": "399270"
  },
  {
    "text": "or about locomotion, ah, then there may be some variation across, ah, across tasks, or across objects.",
    "start": "399270",
    "end": "405139"
  },
  {
    "text": "[NOISE] Another example of this is character animation. So if you wanna animate a character to do things like, ah,",
    "start": "405140",
    "end": "410210"
  },
  {
    "text": "spin clicks, ah, spin kicks, or cartwheels, or, or running, or back flipping, ah, all of these- this agent lives in a single world with",
    "start": "410210",
    "end": "417020"
  },
  {
    "text": "consistent dynamics and what's varying is just the reward function and not the dynamics. Ah, and likewise, if you, ah, have an agent that wants to, ah, converse, ah,",
    "start": "417020",
    "end": "426310"
  },
  {
    "text": "and accomplish a certain task through dialogue, ah, such as helping you, um, order dinner,",
    "start": "426310",
    "end": "431675"
  },
  {
    "text": "for example, or helping you, um, reserve a, a car reservation or something, ah,",
    "start": "431675",
    "end": "437270"
  },
  {
    "text": "the underlying dynamics of interacting with that person may be the same, but the reward function of what you wanna accomplish is varying.",
    "start": "437270",
    "end": "444360"
  },
  {
    "text": "So here are a few examples of where the dynamics might be consistent across tasks, ah, and all of these cases,",
    "start": "444360",
    "end": "450470"
  },
  {
    "text": "estimating the model is a single task problem. If there's just a single model, we only need to estimate a single function.",
    "start": "450470",
    "end": "459060"
  },
  {
    "text": "And so, as a result, then this learning problem may actually be easier than, ah,",
    "start": "459100",
    "end": "464180"
  },
  {
    "text": "some of the multitask model-free methods because we only have to solve the single task learning problem, and then once we solve that single task learning problem, um,",
    "start": "464180",
    "end": "471390"
  },
  {
    "text": "we can use that to, ah, find a policy that optimizes, ah, different tasks.",
    "start": "471390",
    "end": "477550"
  },
  {
    "text": "Any questions on, on this?",
    "start": "477650",
    "end": "481660"
  },
  {
    "text": "All right. So um, how do you actually go about using our model to optimize for actions?",
    "start": "486020",
    "end": "491970"
  },
  {
    "start": "488000",
    "end": "792000"
  },
  {
    "text": "So uh, we wa- we want to be able to kind of optimize actions using the model,",
    "start": "491970",
    "end": "498150"
  },
  {
    "text": "and, and our objective might be to maximize our reward summed over time. Uh, and so one way that we might think about doing this is,",
    "start": "498150",
    "end": "507014"
  },
  {
    "text": "uh, we can use this form of computation graph where we have actions being passed into our model. Our model is predicting the next state, um,",
    "start": "507015",
    "end": "514485"
  },
  {
    "text": "which is, uh, producing the reward function and also producing, uh, sorry. The model is estimating, uh,",
    "start": "514485",
    "end": "520110"
  },
  {
    "text": "the reward function may also be estimating the next state which is then passed to our policy to produce the next action and the next, um, reward function.",
    "start": "520110",
    "end": "528480"
  },
  {
    "text": "And so if we want to optimize over the sequence of actions that, uh, maximize our reward, uh,",
    "start": "528480",
    "end": "533910"
  },
  {
    "text": "we could imagine just backpropagating the signal from our reward function into our actions through this computation graph.",
    "start": "533910",
    "end": "541964"
  },
  {
    "text": "Uh, and so for example, we could use a gradient-based optimization over our actions to optimize for our actions.",
    "start": "541965",
    "end": "549480"
  },
  {
    "text": "Uh, so what this might look like is, uh, you might run some policy, for example, a random policy.",
    "start": "549480",
    "end": "555030"
  },
  {
    "text": "I collect some data using that policy, and then fit a model to that data to minimize the, uh,",
    "start": "555030",
    "end": "561570"
  },
  {
    "text": "the prediction error of that model and then backpropagate through that model in order to optimize for our sequence of actions.",
    "start": "561570",
    "end": "569280"
  },
  {
    "text": "Uh, and then once you have those actions, you can just execute those actions, uh, to accomplish the task that you'd like to perform.",
    "start": "569280",
    "end": "575820"
  },
  {
    "text": "Uh, so this is pretty straight forward.",
    "start": "575820",
    "end": "580270"
  },
  {
    "text": "Uh, another way that we could do this is say we don't want to use, uh, backpropagation. Uh, for example, maybe the model that we learn,",
    "start": "581060",
    "end": "587790"
  },
  {
    "text": "uh, is doesn't have well conditioned gradients, uh, or maybe, uh, it's discontinuous in someway,",
    "start": "587790",
    "end": "594645"
  },
  {
    "text": "we can also optimize the reactions via sampling. Uh, this would be essentially a gradient free optimization over our actions.",
    "start": "594645",
    "end": "601845"
  },
  {
    "text": "Still, still the same underlying loss function, the same underlying optimization, we can just use a different optimization approach",
    "start": "601845",
    "end": "607395"
  },
  {
    "text": "for acquiring a sequence of actions that will maximize reward. Uh, and so what this might look like is to run some policy,",
    "start": "607395",
    "end": "614100"
  },
  {
    "text": "learn a model to minimize model error, uh, and then iteratively sample action sequences.",
    "start": "614100",
    "end": "619154"
  },
  {
    "text": "Run those action sequences through our model, uh, and the action sequence you find that achieves the best reward,",
    "start": "619155",
    "end": "625964"
  },
  {
    "text": "we will then execute those corresponding actions. Uh, and there are ways to uh, to sample action sequences,",
    "start": "625965",
    "end": "633580"
  },
  {
    "text": "uh, in a more intelligent way. So you can imagine just sampling from some uniform distribution over action sequences,",
    "start": "633580",
    "end": "639345"
  },
  {
    "text": "and then taking the best one. Uh, but you could also imagine after sampling from a uniform distribution, you could take the best, uh,",
    "start": "639345",
    "end": "645870"
  },
  {
    "text": "10% for example rather than the best one, and then refit a distribution around those 10- around those",
    "start": "645870",
    "end": "651630"
  },
  {
    "text": "top 10% of actions and resample from that distribution, uh, and do this sort of iterative process to iteratively",
    "start": "651630",
    "end": "657240"
  },
  {
    "text": "refine the sampling distribution over actions. Uh, that would be known as, uh, something like the cross entropy method, uh,",
    "start": "657240",
    "end": "663300"
  },
  {
    "text": "and that would allow you to perform a slightly better optimization or, uh, slightly better, um, slightly more powerful optimization over your actions.",
    "start": "663300",
    "end": "671444"
  },
  {
    "text": "Okay. So here are a couple different, um, approaches. What's something that might go wrong with these approaches? Any thoughts on that?",
    "start": "671445",
    "end": "684645"
  },
  {
    "text": "Sparse reward. Mm-hmm. It's probably a vector. Yeah. So if you're reward function is sparse and your optimization process isn't",
    "start": "684645",
    "end": "694065"
  },
  {
    "text": "able to sample action sequences that lead to that reward or let that actually see any reward that it may be that,",
    "start": "694065",
    "end": "699885"
  },
  {
    "text": "uh, your optimization won't be powerful enough to actually, uh, find a good sequence of actions.",
    "start": "699885",
    "end": "705135"
  },
  {
    "text": "So that's one good example. What's another good example? Yeah? And so in the previous example, you need to solve an optimization problem in order to get one single action?",
    "start": "705135",
    "end": "714120"
  },
  {
    "text": "Yeah, so in both of these cases, you need to solve an optimization problem in order to get a sequence of actions that will,",
    "start": "714120",
    "end": "720675"
  },
  {
    "text": "uh, that will try to maximize that reward. [inaudible] propagated exponentially forward.",
    "start": "720675",
    "end": "728850"
  },
  {
    "text": "Sorry, can you repeat that. When your accuracy is in the model you",
    "start": "728850",
    "end": "734190"
  },
  {
    "text": "[inaudible].",
    "start": "734190",
    "end": "742650"
  },
  {
    "text": "Yeah, absolutely. So if your model- I guess there's- I think there's two things here. One is if your model is inaccurate,",
    "start": "742650",
    "end": "748695"
  },
  {
    "text": "then the optimization can exploit that and, uh, and be overly optimistic about whether or",
    "start": "748695",
    "end": "755730"
  },
  {
    "text": "not an action sequence will accomplish high reward. And second thing is that, uh, if you're optimizing for an open loop sequence of",
    "start": "755730",
    "end": "762480"
  },
  {
    "text": "actions and then executing that sequence of actions. If one of those actions, uh, reaches a state that's slightly different from what you thought you would reach,",
    "start": "762480",
    "end": "770505"
  },
  {
    "text": "and then if at the next state you actually use another action, you'll have this compounding errors such that you move away from the trajectory that you,",
    "start": "770505",
    "end": "778000"
  },
  {
    "text": "um, that you thought you were going to follow according to your model. Okay. Any other thoughts on what might go wrong?",
    "start": "778000",
    "end": "787260"
  },
  {
    "text": "Okay. Cool. So the approach that, um, that I had written here was I guess there's a couple",
    "start": "789150",
    "end": "796200"
  },
  {
    "start": "792000",
    "end": "963000"
  },
  {
    "text": "of di- couple different things that we di- we discussed. The first is that you can potentially have,",
    "start": "796200",
    "end": "801750"
  },
  {
    "text": "uh, imprecisions in your model, uh, and that will cause you to, um, to kind of be overly optimistic about what will happen.",
    "start": "801750",
    "end": "809220"
  },
  {
    "text": "And second, that these errors can compound and cause you to kind of go off track, um,",
    "start": "809220",
    "end": "814245"
  },
  {
    "text": "and have increasing amounts of errors as you roll out your, um, as you roll out your,",
    "start": "814245",
    "end": "821308"
  },
  {
    "text": "uh, sequence of actions. One thing that will help with this, uh, I actually, do any of you have thoughts on, like,",
    "start": "821309",
    "end": "828584"
  },
  {
    "text": "how we might try to avoid some of these issues or, or these two issues in particular? Yep.",
    "start": "828585",
    "end": "836639"
  },
  {
    "text": "So I have a question, intuitively it makes sense why this is bad. But on a theoretical standpoint, isn't this like minimizing regret by going",
    "start": "836640",
    "end": "843480"
  },
  {
    "text": "the most optimistic possible choice and then negating that if it turned out to not be true?",
    "start": "843480",
    "end": "849269"
  },
  {
    "text": "Yeah. So the, the key thing is the second part that you said is that if that turned out not to be true, we need to actually take that into account, right?",
    "start": "849270",
    "end": "855975"
  },
  {
    "text": "As on the algorithm that I listed on the previous slide, we are actually just fitting a model to our data and then executing actions according to that data.",
    "start": "855975",
    "end": "863490"
  },
  {
    "text": "But if we turn out- if it turns out that we take those actions and it was actually not the correct thing to do, as you're mentioning,",
    "start": "863490",
    "end": "869190"
  },
  {
    "text": "then we should basically refit our model using the new data and use that to, uh, continuously improve our model into settings where we're overly optimistic.",
    "start": "869190",
    "end": "877350"
  },
  {
    "text": "And so what you can do is you can go back to this previous algorithm, and then actually execute those planned actions in the world,",
    "start": "877350",
    "end": "883350"
  },
  {
    "text": "append the data that you observed to your dataset, and use that to refit your model, um,",
    "start": "883350",
    "end": "890550"
  },
  {
    "text": "to this, uh, growing dataset. Does that answer your question? I guess, like, it seems like the answer is like that's really not that bad, is what you're saying?",
    "start": "890550",
    "end": "899250"
  },
  {
    "text": "Um, the- sorry. You're asking the, um-",
    "start": "899250",
    "end": "905280"
  },
  {
    "text": "Just like making an overly optimistic decision we can just use that to our advantage through this? So if you make an overly optimistic decision,",
    "start": "905280",
    "end": "914279"
  },
  {
    "text": "it can certainly- if that is, like, if you're, if you're, like, done running learning and you're not going to actually collect many more data,",
    "start": "914280",
    "end": "920850"
  },
  {
    "text": "then it can be very bad because you'll- you won't actually be making predictions that maximize reward.",
    "start": "920850",
    "end": "926040"
  },
  {
    "text": "Uh, and it can be arbitrarily bad if, if it's, like, erroneous outside of the states that you visited.",
    "start": "926040",
    "end": "931935"
  },
  {
    "text": "But if you have the ability to collect more data, then you in principle should be able to correct for those errors.",
    "start": "931935",
    "end": "938290"
  },
  {
    "text": "Okay. So this is, uh, this is one thing that we can do. Um, but still, uh,",
    "start": "938690",
    "end": "944625"
  },
  {
    "text": "even if- so this will help certainly, uh, doesn't completely solve the problem. So in general, learning a good global model is pretty hard.",
    "start": "944625",
    "end": "952274"
  },
  {
    "text": "Uh, especially if you wanna learn a good global model everywhere, uh, in your- in, in all possible states for example.",
    "start": "952275",
    "end": "959175"
  },
  {
    "text": "Another trick that can be helpful with this is, uh, what's called re-planning.",
    "start": "959175",
    "end": "965565"
  },
  {
    "start": "963000",
    "end": "1332000"
  },
  {
    "text": "So, uh, and this is, uh, a fancier name for this is, uh, model predictive control or MPC.",
    "start": "965565",
    "end": "971840"
  },
  {
    "text": "And what we can do is we can first run our policy to collect some data, fit a model to that data, use this model to optimize over our action sequence.",
    "start": "971840",
    "end": "980180"
  },
  {
    "text": "And what we can do is we can execute the first planned action, observe the resulting state, um,",
    "start": "980180",
    "end": "987230"
  },
  {
    "text": "append this to our dataset. Uh, and then after we take ones- one action,",
    "start": "987230",
    "end": "994320"
  },
  {
    "text": "we can then actually replan and reoptimize over a sequence of actions from the state that we just observed.",
    "start": "994320",
    "end": "1002155"
  },
  {
    "text": "And so what this can do is that if you end up at a state that you didn't expect,",
    "start": "1002155",
    "end": "1007175"
  },
  {
    "text": "uh, as a result of your model, you won't keep on executing actions as if you were in the state that you thought you would reach,",
    "start": "1007175",
    "end": "1013330"
  },
  {
    "text": "you'll then actually replan according to the state that you actually reached out to try to correct for your mistake at that first time step.",
    "start": "1013330",
    "end": "1021190"
  },
  {
    "text": "And so this can help address some of the compounding errors that we talked about before. Yeah. What if it's the same dataset",
    "start": "1021190",
    "end": "1027740"
  },
  {
    "text": "or what if they're different like meta dataset? Um. [inaudible] or something like that. Yeah. So we haven't got into the meta-learning setting yet,",
    "start": "1035640",
    "end": "1042645"
  },
  {
    "text": "but you could also imagine- so in this setting, uh, there might be a loop here which is to, kind of,",
    "start": "1042645",
    "end": "1049080"
  },
  {
    "text": "refit your model in this, like, slower outer loop. Um, in here all- the only thing that you're doing is you're, um,",
    "start": "1049080",
    "end": "1057270"
  },
  {
    "text": "observing the state and then using your model to replan, to like, reoptimize the action sequences from that state.",
    "start": "1057270",
    "end": "1065025"
  },
  {
    "text": "You're not actually- so you're using it to update your actions. You're not actually using it to update your model. And you could also imagine using that state to update your model,",
    "start": "1065025",
    "end": "1073020"
  },
  {
    "text": "uh, and we'll get to that actually at the- like, towards the end of the lecture. Uh, so we're a few steps ahead.",
    "start": "1073020",
    "end": "1079690"
  },
  {
    "text": "Um, so intuitively this can help with model errors because it can allow- if you, kind of, go off track of where- where your model thinks you will end up,",
    "start": "1079760",
    "end": "1086880"
  },
  {
    "text": "uh, you could in principle start to correct for those. Uh, so the- the benefit of this is you can correct some model errors.",
    "start": "1086880",
    "end": "1093435"
  },
  {
    "text": "One of the downsides of this approach which I think was maybe alluded to, uh, in one of the other questions is that this is- in general planning for",
    "start": "1093435",
    "end": "1099780"
  },
  {
    "text": "actions is a fairly compute intensive process, uh, because you need to continuously, I mean,",
    "start": "1099780",
    "end": "1104910"
  },
  {
    "text": "if you're planning at every single time step, you're doing an optimization, you need to do an optimization in real time.",
    "start": "1104910",
    "end": "1110175"
  },
  {
    "text": "Um, one thing that can help with this, uh, is to optimize over,",
    "start": "1110175",
    "end": "1116550"
  },
  {
    "text": "um, instead of optimize over a sequence of actions, you could actually back-propagate actions into your policy.",
    "start": "1116550",
    "end": "1123419"
  },
  {
    "text": "So if you have a parametric form of your policies, instead of back propagating grad- gradients into the action,",
    "start": "1123420",
    "end": "1128550"
  },
  {
    "text": "you can back-propagate gradients into the policy parameters. Um, likewise for a sampling based approach if you optimize for a sequence of",
    "start": "1128550",
    "end": "1135059"
  },
  {
    "text": "actions you could use those as targets to train a policy to produce those actions. Um, and that can reduce some of the compute  intensive challenges of things like MPC.",
    "start": "1135060",
    "end": "1145140"
  },
  {
    "text": "Um, now I also wanna get back to the problem of sparse rewards that was mentioned.",
    "start": "1145140",
    "end": "1150285"
  },
  {
    "text": "Um, learning a policy can also help with sparse rewards. Uh, in that if you, um,",
    "start": "1150285",
    "end": "1157140"
  },
  {
    "text": "in aggregate at some point you see a good reward function or a good reward then your policy, uh, the policy parameters will be trained to try to, uh, accomplish those.",
    "start": "1157140",
    "end": "1166080"
  },
  {
    "text": "Um, in practice like with very sparse rewards model-based optimization and model-free optimization will run into same- the same sorts of",
    "start": "1166080",
    "end": "1171450"
  },
  {
    "text": "issues if they don't actually ever observe rewards. Um, and things like relabeling, uh,",
    "start": "1171450",
    "end": "1177899"
  },
  {
    "text": "as you saw in the pro- problem assignment can also help with that. Okay. Um, any questions on kind of the basic algorithms before we talk",
    "start": "1177900",
    "end": "1188280"
  },
  {
    "text": "about the multitask setting? Yeah.",
    "start": "1188280",
    "end": "1193515"
  },
  {
    "text": "My understanding is that when people are more advocates of model-free, say that model-free sometimes allows behaviors to emerge that",
    "start": "1193515",
    "end": "1200549"
  },
  {
    "text": "wouldn't have been possible model-based but the way that you've presented it here where- where does that fall",
    "start": "1200550",
    "end": "1207330"
  },
  {
    "text": "apart for model-based such that you can't get everything that you have with model-free?",
    "start": "1207330",
    "end": "1212470"
  },
  {
    "text": "So I actually haven't heard that argument before. Um, so the argument you said is that there are behaviors that will emerge with",
    "start": "1212870",
    "end": "1219960"
  },
  {
    "text": "model-free that you won't be able to have emerge with the model-based method? Yeah you're essentially, like, confining yourself too much and confining the agent too much by",
    "start": "1219960",
    "end": "1225840"
  },
  {
    "text": "using the model-based method. Yeah. I don't actually see why that would be the case.",
    "start": "1225840",
    "end": "1232380"
  },
  {
    "text": "Like, uh, in both cases you are, um, you're optimizing some objective which is to maximize reward",
    "start": "1232380",
    "end": "1238680"
  },
  {
    "text": "and the behavior that emerges, like, like- in both cases you are going to",
    "start": "1238680",
    "end": "1243900"
  },
  {
    "text": "be learning some behavior to maximize that reward function. And I think that what comes out of that optimization is",
    "start": "1243900",
    "end": "1249420"
  },
  {
    "text": "more a function of this- the power- how powerful that optimization process is. Uh, and if it's- if you have a very strong optimization process then, uh,",
    "start": "1249420",
    "end": "1257055"
  },
  {
    "text": "more interesting, uh, well, and a reward function that's interesting then more interesting behaviors will emerge.",
    "start": "1257055",
    "end": "1263140"
  },
  {
    "text": "Uh, and I don't think that there's any difference, um- I think that, like, any difference that you see in the outcome of",
    "start": "1263870",
    "end": "1270180"
  },
  {
    "text": "this model- of these approaches will more have to do with the strength of the optimization and also how",
    "start": "1270180",
    "end": "1275310"
  },
  {
    "text": "hard it is to fit the model versus fitting a policy. Um, there are definitely settings where it's",
    "start": "1275310",
    "end": "1280470"
  },
  {
    "text": "harder to fit the model than to fit the policy. Uh, so for example if you wanna pour water from one container to another container,",
    "start": "1280470",
    "end": "1287445"
  },
  {
    "text": "modeling fluid dynamics, is it a hard problem? Uh, but just twisting your arm is a relatively simple function to learn.",
    "start": "1287445",
    "end": "1293910"
  },
  {
    "text": "In those settings a model-free approach, um, may be easier. In other settings, uh,",
    "start": "1293910",
    "end": "1299100"
  },
  {
    "text": "such as if you want to be able to, um, push an object to any possible position, uh,",
    "start": "1299100",
    "end": "1305610"
  },
  {
    "text": "the dynamics may be relatively simple because it just corresponds to one object on- on the table and the- the dynamics there.",
    "start": "1305610",
    "end": "1312990"
  },
  {
    "text": "Uh, whereas the policy may be more complex because for any poss- you have to represent the policy for any possible goal,",
    "start": "1312990",
    "end": "1318764"
  },
  {
    "text": "um, whereas the dynamics are just- it's just a single task problem. Okay. So we can look at,",
    "start": "1318765",
    "end": "1329580"
  },
  {
    "text": "um, what does this actually look like and reusing this question about like what does",
    "start": "1329580",
    "end": "1335160"
  },
  {
    "start": "1332000",
    "end": "1745000"
  },
  {
    "text": "this have to do with multi-task RL and meta RL? So, um, how you actually apply this to",
    "start": "1335160",
    "end": "1340350"
  },
  {
    "text": "the multitask RL and meta RL problem statement depends on whether or not you know the reward function.",
    "start": "1340350",
    "end": "1346230"
  },
  {
    "text": "Um, and in particular, there are some instances where you actually know the form of the reward function, uh, exactly.",
    "start": "1346230",
    "end": "1351645"
  },
  {
    "text": "Uh, so, um, we'll see an example of this in a second and if you know the reward function for each task,",
    "start": "1351645",
    "end": "1357705"
  },
  {
    "text": "uh, then you could just learn a single model and plan with respect to that reward function at each, uh, at test time.",
    "start": "1357705",
    "end": "1364005"
  },
  {
    "text": "Uh, so for example, um, here's an example of, uh, a work that was done by Anusha Nagabandi and colleagues,",
    "start": "1364005",
    "end": "1369840"
  },
  {
    "text": "and what they were looking at is, uh, they wanted to, to learn how to write different trajectories with a pen and they were controlling the,",
    "start": "1369840",
    "end": "1377160"
  },
  {
    "text": "ah, the hand, ah, in simulation and different reward functions corresponded to different trajectories of the tip of the pencil.",
    "start": "1377160",
    "end": "1384524"
  },
  {
    "text": "And so in this case they assumed that they could observe the tip of the pencil and then the reward function can be derived as just trying to track",
    "start": "1384525",
    "end": "1390690"
  },
  {
    "text": "a particular trajectory with the tip of the pencil. So the form of the reward function is known,",
    "start": "1390690",
    "end": "1395790"
  },
  {
    "text": "uh, but optimizing that, uh, the reward function I- by actually using the hand to draw is a very challenging problem.",
    "start": "1395790",
    "end": "1404399"
  },
  {
    "text": "And so they learned a model, um, and they actually learned a model by collecting data with this vector random trajectories and then at test time",
    "start": "1404400",
    "end": "1411510"
  },
  {
    "text": "they gave it reward functions for writing different digits. Uh, it's a little bit hard to see. I think the first digit is a six,",
    "start": "1411510",
    "end": "1416790"
  },
  {
    "text": "the second one is a seven, the next one is like a nine or a four, and the last one is a five. And you can basically use that model to plan to accomplish these different,",
    "start": "1416790",
    "end": "1425265"
  },
  {
    "text": "um, these different trajectories. Um, another example of this is, uh,",
    "start": "1425265",
    "end": "1431430"
  },
  {
    "text": "maybe you want the hand to be able to manipulate, uh, these bouncing balls, like, ah, move them in a circle for example,",
    "start": "1431430",
    "end": "1437460"
  },
  {
    "text": "here the reward function is also known it corresponds to the trajectory of the- of the two balls.",
    "start": "1437460",
    "end": "1443040"
  },
  {
    "text": "Uh, and you can also have a reward function corresponding to moving- uh, the ball to a particular location in the- in the palm,",
    "start": "1443040",
    "end": "1449655"
  },
  {
    "text": "or also, um, rotating the balls in the opposite direction.",
    "start": "1449655",
    "end": "1455200"
  },
  {
    "text": "Cool. So here- here's an example of, kind of, multi-task reinforcement learning with a single model and different reward functions.",
    "start": "1455840",
    "end": "1462615"
  },
  {
    "text": "Um, one caveat that I'd like to mention here is that, uh, even though the the dynamics may be consistent across all of the tasks,",
    "start": "1462615",
    "end": "1471810"
  },
  {
    "text": "different tasks may require you to visit different state distributions. And so if you collect data for",
    "start": "1471810",
    "end": "1478650"
  },
  {
    "text": "one task and another task is a very different state distribution, then the model that you learned for that one task",
    "start": "1478650",
    "end": "1484530"
  },
  {
    "text": "may not actually generalize to the second task, if it doesn't visit the, uh, the same states.",
    "start": "1484530",
    "end": "1490410"
  },
  {
    "text": "Um, so the reward may change how you collect the data and may affect the quality of your model in other states.",
    "start": "1490410",
    "end": "1496635"
  },
  {
    "text": "Um, in this work they found that if you actually train a model only on this first task, uh,",
    "start": "1496635",
    "end": "1502830"
  },
  {
    "text": "that model can actually be reused for the second two tasks because the distribution over states is sufficiently diverse.",
    "start": "1502830",
    "end": "1510370"
  },
  {
    "text": "What is the state representation of that case? Um, in this case,",
    "start": "1510680",
    "end": "1516450"
  },
  {
    "text": "the state representation corresponds to the position of the two balls and the, um, state information about the hands such as the joint angles.",
    "start": "1516450",
    "end": "1524370"
  },
  {
    "text": "I'm not- I'm actually thinking about this more- a bit more. I'm not quite sure how they go from two balls to one ball cause the state representation changes in that case.",
    "start": "1524370",
    "end": "1531780"
  },
  {
    "text": "Um, I would guess that maybe they just use the, the model corresponding to one of the balls and they ignore the second one,",
    "start": "1531780",
    "end": "1539685"
  },
  {
    "text": "um, but I- I'd have to check the details of the paper for that. They all seem like of different sizes.",
    "start": "1539685",
    "end": "1546640"
  },
  {
    "text": "It's possible that they're different sizes. I- my impression was that they were, um,",
    "start": "1548150",
    "end": "1553650"
  },
  {
    "text": "the same size and maybe the videos- actually, so the hand is also bigger in the left video. So I think it's just that- that the video has been scaled differently.",
    "start": "1553650",
    "end": "1561700"
  },
  {
    "text": "Um, another cool thing about this approach is that, ah, because it's able to learn a model, um,",
    "start": "1561890",
    "end": "1568664"
  },
  {
    "text": "pretty quickly with a relatively small amount of data they actually were able to run this method on a real robot, ah,",
    "start": "1568665",
    "end": "1574305"
  },
  {
    "text": "and actually run the reinforcement learning process and collect all the data on a real robot and fit a model to that, um,",
    "start": "1574305",
    "end": "1580260"
  },
  {
    "text": "and were able to get a, a real shadow hand to perform this task.",
    "start": "1580260",
    "end": "1585730"
  },
  {
    "text": "Cool. Yeah. When did the video come out? When did this get done?",
    "start": "1587150",
    "end": "1593370"
  },
  {
    "text": "Um, it's actually pretty recent work. Well, so I, I know Anusha well,",
    "start": "1593370",
    "end": "1598380"
  },
  {
    "text": "so I've seen it for awhile but, I think that the video came out like within the last month.",
    "start": "1598380",
    "end": "1603820"
  },
  {
    "text": "Um, it was published at CoRL which was last week, which is why I was actually not here. Um, yeah.",
    "start": "1604070",
    "end": "1611620"
  },
  {
    "text": "Okay. Cool. So that's what happens if we know the reward function. Uh, what if we don't know",
    "start": "1612170",
    "end": "1617549"
  },
  {
    "text": "the reward function or the- at least the form of the reward function? Um, one thing we can do is we can just learn a reward function, uh,",
    "start": "1617550",
    "end": "1624330"
  },
  {
    "text": "conditioned on the task and then use that reward function to plan to accomplish tasks. Ah, and I have a typo on the next thing but,",
    "start": "1624330",
    "end": "1633570"
  },
  {
    "text": "ah, this should say meta-RL. Uh, the only thing that you could do is you could meta-learn a reward function from a small amount of",
    "start": "1633570",
    "end": "1639090"
  },
  {
    "text": "data and use that to- use that learned reward function to plan to accomplish goals.",
    "start": "1639090",
    "end": "1644830"
  },
  {
    "text": "Uh, this is pretty straight forward. Uh, one example of the second case, um, is, some work here where the,",
    "start": "1644960",
    "end": "1650805"
  },
  {
    "text": "the training dataset corresponds to a few examples of the goal. Ah, so in this case the goal is to, ah,",
    "start": "1650805",
    "end": "1657215"
  },
  {
    "text": "place the pencil case to- on top of the- or behind the notebook and given a few positive examples,",
    "start": "1657215",
    "end": "1663890"
  },
  {
    "text": "you wanna be able to learn a classifier or a binary reward function that corresponds to whether or not the task has been accomplished in the image.",
    "start": "1663890",
    "end": "1671654"
  },
  {
    "text": "Um, so you can, you can do this task with Meta training but, but by- with meta-learning by collecting a dataset",
    "start": "1671655",
    "end": "1676950"
  },
  {
    "text": "of a bunch of positive and negative examples for different tasks, meta learn your classifiers such that given",
    "start": "1676950",
    "end": "1682680"
  },
  {
    "text": "a small number of positive examples it can quickly learn a new reward function. And then once you have that reward function you can plan",
    "start": "1682680",
    "end": "1688710"
  },
  {
    "text": "using your model to maximize reward. Uh, so here's the kind of the result of running, ah,",
    "start": "1688710",
    "end": "1693780"
  },
  {
    "text": "planning and executing those actions on the robot, ah, to accomplish the task with respect to this,",
    "start": "1693780",
    "end": "1699960"
  },
  {
    "text": "ah, meta learned reward function. Okay. Um, and I guess kind of the,",
    "start": "1699960",
    "end": "1707085"
  },
  {
    "text": "the bigger takeaway here is that, uh, model-based RL solves both the multi task RL problem",
    "start": "1707085",
    "end": "1714030"
  },
  {
    "text": "and the meta RL problem statements with these, ah, with these different types",
    "start": "1714030",
    "end": "1719760"
  },
  {
    "text": "of approaches depending on whether or not you have the reward function or not. Okay. Any questions before we move on to image observations?",
    "start": "1719760",
    "end": "1730510"
  },
  {
    "text": "So one thing worth mentioning here that I didn't actually tell you is that, this is, this is all from images.",
    "start": "1731600",
    "end": "1737850"
  },
  {
    "text": "And so how do we actually go about doing model-based RL when we have image observations? Right. Um, so, ah, in particular, ah,",
    "start": "1737850",
    "end": "1748845"
  },
  {
    "start": "1745000",
    "end": "1856000"
  },
  {
    "text": "if you only have access to images you might have a graphical model like this where you can actually observe",
    "start": "1748845",
    "end": "1754200"
  },
  {
    "text": "the underlying state- the low-dimensional underlying state of the world and you can only observe the O's shown here.",
    "start": "1754200",
    "end": "1760185"
  },
  {
    "text": "Ah, so for example maybe I have a robot that looks like this and you want it to be able to like use",
    "start": "1760185",
    "end": "1765269"
  },
  {
    "text": "the spatula to lift up an object and put it into the bowl and all it can observe is this image.",
    "start": "1765270",
    "end": "1771390"
  },
  {
    "text": "Um, so first, ah, with, with these images we have to deal with learning models,",
    "start": "1771390",
    "end": "1778110"
  },
  {
    "text": "ah, in, in some space at least in learning how to predict. And second we also don't have any reward function necessarily if we only have",
    "start": "1778110",
    "end": "1785930"
  },
  {
    "text": "those observations and so we need to think about how we might go about learning our reward function, um, such as using the meta learning approach that I showed on the previous slide.",
    "start": "1785930",
    "end": "1794460"
  },
  {
    "text": "Okay. So one option, ah, for the reward function is learning an image classifier like I showed before or to meta learning an image classifier.",
    "start": "1794650",
    "end": "1801865"
  },
  {
    "text": "Um, another example or another option is to provide an image of the goal. Ah, and this would co- correspond to the goal condition reinforcement learning setting,",
    "start": "1801865",
    "end": "1810480"
  },
  {
    "text": "ah, that we have covered previously and that you looked at in your homework. And so for example, you could give the, ah,",
    "start": "1810480",
    "end": "1816485"
  },
  {
    "text": "the robot an image of the goal like this, uh, saying I want you to, to accomplish this, ah, this goal state, ah,",
    "start": "1816485",
    "end": "1823835"
  },
  {
    "text": "and have it try to, to reach that goal state. So how might we go about doing model-based RL in the setting?",
    "start": "1823835",
    "end": "1831780"
  },
  {
    "text": "So there's a few different classes or approaches that we'll cover. Um, the first is to learn some latent representation",
    "start": "1831780",
    "end": "1837119"
  },
  {
    "text": "and then learn a model in that latent representation. The second is to try to learn a model of your observations directly.",
    "start": "1837119",
    "end": "1843210"
  },
  {
    "text": "Ah, and the last one is to try to predict alternative quantities other than your raw observations.",
    "start": "1843210",
    "end": "1849345"
  },
  {
    "text": "And we'll talk about all three of these approaches. Ah, so first let's talk about latent space.",
    "start": "1849345",
    "end": "1854955"
  },
  {
    "text": "So, ah, the key idea of learning in latent space is that you wanna learn just",
    "start": "1854955",
    "end": "1860100"
  },
  {
    "start": "1856000",
    "end": "2012000"
  },
  {
    "text": "some embedding of your observation which we'll denote as g of o, and then learn a model inside that embedding space.",
    "start": "1860100",
    "end": "1867360"
  },
  {
    "text": "Ah, so if we take the graphical model that we showed before, this corresponds to trying to, ah,",
    "start": "1867360",
    "end": "1872445"
  },
  {
    "text": "learn some form of inference network that maps from your observations to your- back to your low-dimensional states or back",
    "start": "1872445",
    "end": "1879720"
  },
  {
    "text": "to some representation of your state space. Um, so then, ah,",
    "start": "1879720",
    "end": "1886245"
  },
  {
    "text": "kind of the way it works I guess is, ah, first there's a couple of papers that have looked at this kind of approach.",
    "start": "1886245",
    "end": "1891780"
  },
  {
    "text": "Um, shown here that we'll talk about and they're- more recently there are a couple of other approaches that have looked at this kinda approach as well.",
    "start": "1891780",
    "end": "1898350"
  },
  {
    "text": "So um, the way this algorithm works is first you run some policy to collect some data, ah, and then you learn this latent space of your observation and,",
    "start": "1898350",
    "end": "1908429"
  },
  {
    "text": "uh, a model in that latent space. So you'll learn a G to go from O to S and then you learn,",
    "start": "1908430",
    "end": "1914355"
  },
  {
    "text": "learn a model that goes from S and A to predict S prime. Then you use your model to optimize over a sequence of actions.",
    "start": "1914355",
    "end": "1923355"
  },
  {
    "text": "Ah, and then execute those pan- planned actions, append the visiting tuples, ah,",
    "start": "1923355",
    "end": "1929580"
  },
  {
    "text": "in this case it may be the, the state and action and the next state or it might be the observation,",
    "start": "1929580",
    "end": "1934590"
  },
  {
    "text": "the action and then the next observation. Ah, and then you can add those, ah, tuples and then re- retrain your embedding space and retrain your, ah, your model.",
    "start": "1934590",
    "end": "1945220"
  },
  {
    "text": "All right. So this is pretty straight forward. Ah, there are a couple of questions though.",
    "start": "1945530",
    "end": "1951345"
  },
  {
    "text": "Um, the first question is what is your reward function when you're trying to optimize over your actions?",
    "start": "1951345",
    "end": "1956940"
  },
  {
    "text": "Ah, so we talked a little bit about how your reward function could correspond to a classifier, um, in the case where you're given an image of the goal,",
    "start": "1956940",
    "end": "1965145"
  },
  {
    "text": "one thing you can do is you can use your reward signal as, ah, some reward signal of your actions such as trying to minimize effort or",
    "start": "1965145",
    "end": "1972450"
  },
  {
    "text": "torque plus a distance function and that there should be a negative sign in front of that.",
    "start": "1972450",
    "end": "1977970"
  },
  {
    "text": "Some distance- some negative distance between, ah, the goal of your current observation and- sorry, the,",
    "start": "1977970",
    "end": "1983865"
  },
  {
    "text": "the representation of your current observation and the representation of your goal observation. So you can basically use the negative distance in your latent space",
    "start": "1983865",
    "end": "1991650"
  },
  {
    "text": "as a reward function for planning. Okay. Um, and this makes the assumption that distance,",
    "start": "1991650",
    "end": "2000005"
  },
  {
    "text": "distance in your latent space is an accurate metric for the things that you care about. Ah, and this, this assumption may or may",
    "start": "2000005",
    "end": "2006530"
  },
  {
    "text": "not be true depending on the form of your latent representation. Okay. And then the second big question or",
    "start": "2006530",
    "end": "2014705"
  },
  {
    "text": "maybe kinda the most salient question is how do you actually get this latent representation space? Um, there's a couple of",
    "start": "2014705",
    "end": "2020299"
  },
  {
    "text": "different pe- approaches that people have taken to try to do this. Ah, one of the more popular approaches, ah,",
    "start": "2020300",
    "end": "2026375"
  },
  {
    "text": "that was looked at in 20- 2015 and also actually more recently is to try to, ah, form a graphical model of your transitions.",
    "start": "2026375",
    "end": "2035510"
  },
  {
    "text": "Ah, and this basically corresponds to a variational autoencoder that looks at",
    "start": "2035510",
    "end": "2040940"
  },
  {
    "text": "transitions over pairs of states rather than a single variational autoencoder for a single state.",
    "start": "2040940",
    "end": "2046865"
  },
  {
    "text": "And as a result what you get is you're optimizing jointly for the latent representation, ah,",
    "start": "2046865",
    "end": "2052580"
  },
  {
    "text": "of your variational autoencoder for example as well as the transition,",
    "start": "2052580",
    "end": "2058040"
  },
  {
    "text": "ah, distribution, your, your model in that latent space. Ah, and so if you do this you can, ah, kinda get,",
    "start": "2058040",
    "end": "2066815"
  },
  {
    "text": "get a representational space that is good, ah, both- that is both low-dimensional as well as,",
    "start": "2066815",
    "end": "2072034"
  },
  {
    "text": "ah, satisfies your model effectively. Ah, and so for example they showed that, ah, you could use this,",
    "start": "2072035",
    "end": "2077615"
  },
  {
    "text": "use the algorithm with this latent space for accomplishing different kinds of goals.",
    "start": "2077615",
    "end": "2082879"
  },
  {
    "text": "So, ah, the goal image was shown on- at the beginning of the video and then in each case it's trying to reach",
    "start": "2082880",
    "end": "2089450"
  },
  {
    "text": "it's- executing actions that tried to reach that image. So we'll see another example in a minute.",
    "start": "2089450",
    "end": "2096480"
  },
  {
    "text": "So here the goal state is to, ah, curl up the arm and the left is showing the executed actions and",
    "start": "2096820",
    "end": "2105680"
  },
  {
    "text": "the right is showing the one-step predictions of the reconstructed image through that generative model.",
    "start": "2105680",
    "end": "2112670"
  },
  {
    "text": "Ah, and so one of the things you can see is that it can, ah, use a single model in a certain- a single layer representation to accomplish these different goals.",
    "start": "2112670",
    "end": "2121250"
  },
  {
    "text": "Um, the other thing worth mentioning is, ah, kinda  alluded to at the beginning of this lecture that",
    "start": "2121250",
    "end": "2126470"
  },
  {
    "text": "the model-based methods tend to be fairly efficient. This approach required about 300 trials to learn, ah, these,",
    "start": "2126470",
    "end": "2134090"
  },
  {
    "text": "these skills from the pixel representations which if you were to run that in the real world it would probably correspond to about 25 minutes of real time.",
    "start": "2134090",
    "end": "2143285"
  },
  {
    "text": "Which is pretty fast as reinforcement learning goes. Um, in practice, some of the more recent model-free methods take",
    "start": "2143285",
    "end": "2149750"
  },
  {
    "text": "around two to three hours to learn. Yeah. [inaudible] I'm guessing you're also referring to SOLAR as a more recent work?",
    "start": "2149750",
    "end": "2161285"
  },
  {
    "text": "There are a couple of more recent work, SOLAR is one of them. There's also, um, Stochastic Latent Actor-Critic which was covered, um, more recently.",
    "start": "2161285",
    "end": "2167570"
  },
  {
    "text": "That one's kind of more of a hybrid method but also has this form of graphical model. [inaudible]",
    "start": "2167570",
    "end": "2181000"
  },
  {
    "text": "Yeah. [inaudible] Yeah.",
    "start": "2181000",
    "end": "2187000"
  },
  {
    "text": "[inaudible] Yeah. So the question is this method and some of the,",
    "start": "2187000",
    "end": "2194365"
  },
  {
    "text": "the predecessors, uh, or successors of this method, um, places assumption on the latent space,",
    "start": "2194365",
    "end": "2200839"
  },
  {
    "text": "which is that you can linearly or you can basically have a local linear model on that space like it can",
    "start": "2200840",
    "end": "2206450"
  },
  {
    "text": "accurately predict the next state given the current state, um, where local means that you may have a time-varying linear model.",
    "start": "2206450",
    "end": "2212870"
  },
  {
    "text": "Uh, I think that theoretically speaking, if you have a universal function approximator that's producing your state,",
    "start": "2212870",
    "end": "2219454"
  },
  {
    "text": "it seems like it should, it should be possible or in the latent space that is, that does actually satisfy that constraint.",
    "start": "2219455",
    "end": "2226069"
  },
  {
    "text": "Um, in practice it may be- that may be a very difficult optimization problem to actually find that,",
    "start": "2226070",
    "end": "2232670"
  },
  {
    "text": "that latent space and we haven't yet seen these kinds of methods perform well on very diverse settings where you have, um,",
    "start": "2232670",
    "end": "2240740"
  },
  {
    "text": "many different objects in the scene and, and, and, uh, kind of, the kind of diversity and complexity that you see in like natural images and,",
    "start": "2240740",
    "end": "2248059"
  },
  {
    "text": "and in, uh, and in like ImageNet images for example. Uh, that is actually- oh,",
    "start": "2248060",
    "end": "2255425"
  },
  {
    "text": "I'll come back to that point in a second as well, um, as we talk about kind of modeling and latent space versus modeling",
    "start": "2255425",
    "end": "2260990"
  },
  {
    "text": "and observation space. Yeah. [inaudible] like PlaNet? Yes, PlaNet, PlaNet is also learning,",
    "start": "2260990",
    "end": "2269600"
  },
  {
    "text": "uh, a latent representation and doing planning in that representation. The PlaNet approach that they use,",
    "start": "2269600",
    "end": "2276359"
  },
  {
    "text": "um, I can't remember the exact details of which PlaNet approach they use. I think it actually may have corresponded to a model-free algorithm.",
    "start": "2278440",
    "end": "2285125"
  },
  {
    "text": "Um, sorry, what? [inaudible] Oh, Okay. Right. Okay. Yeah. So using- right.",
    "start": "2285125",
    "end": "2290315"
  },
  {
    "text": "Okay. So it's using cross entropy method, uh, which is basically the iterative sampling based approach that we talked about.",
    "start": "2290315",
    "end": "2296069"
  },
  {
    "text": "PlaNet, PlaNet is also kind of has a very similar form of",
    "start": "2296110",
    "end": "2302600"
  },
  {
    "text": "this graphical model although in addition to having the stochastic pathway they also had a deterministic pathway in their model.",
    "start": "2302600",
    "end": "2309440"
  },
  {
    "text": "Um, the PlaNet to my knowledge wasn't tested in the multitask case.",
    "start": "2309440",
    "end": "2316055"
  },
  {
    "text": "Uh, it was just tested in a single task case but in principle that the model that's learned could also be used in the multitask setting.",
    "start": "2316055",
    "end": "2323609"
  },
  {
    "text": "Yeah. So there's been a number of approaches recently that have kind of followed this form of kind of learning a latent space with",
    "start": "2325140",
    "end": "2331270"
  },
  {
    "text": "some sort of probabilistic or semi probabilistic approach and then doing learning in that latent space.",
    "start": "2331270",
    "end": "2337605"
  },
  {
    "text": "Um, one other example of, um, a latent space that, uh,",
    "start": "2337605",
    "end": "2342830"
  },
  {
    "text": "we used in 2016 and also was actually has been studied more recently as well is having representations that are have structure to them.",
    "start": "2342830",
    "end": "2351125"
  },
  {
    "text": "Um, so in this case the structure that we're looking at were latent spaces where the,",
    "start": "2351125",
    "end": "2356555"
  },
  {
    "text": "um, the, the dimensions of the latent space correspond to key points in the image.",
    "start": "2356555",
    "end": "2361925"
  },
  {
    "text": "Uh, so for example, here are two example, uh, two example key points that are in the representation and this is the trajectory that,",
    "start": "2361925",
    "end": "2369410"
  },
  {
    "text": "that, that representation follows as the robot executes the trajectory. Um, and this- more recently there's been a kind",
    "start": "2369410",
    "end": "2377090"
  },
  {
    "text": "of a trend of approaches that try to learn object-centric representations or, or key point-based representations, uh,",
    "start": "2377090",
    "end": "2382339"
  },
  {
    "text": "of images and then perform planning or perform model-based RL in the context of those representations.",
    "start": "2382340",
    "end": "2388205"
  },
  {
    "text": "Um, and so this is kind of maybe alt- an alternative view. I think that both of them have their merits. Uh, and oh, yes, specifically for,",
    "start": "2388205",
    "end": "2396589"
  },
  {
    "text": "for this, this approach to the way that you actually try to get those feature points, um, is you can take,",
    "start": "2396590",
    "end": "2402155"
  },
  {
    "text": "you can take the last convolutional layer of your network perform a softmax over the spatial extent of the image to get a distribution over 2D positions in the image.",
    "start": "2402155",
    "end": "2410795"
  },
  {
    "text": "And then once you have the, uh, all these distributions over key points you can then, uh, here's an example of a softmax for",
    "start": "2410795",
    "end": "2417740"
  },
  {
    "text": "the softmaxes over the x position and the y position. Um, then you can take an expectation where you compute the, uh,",
    "start": "2417740",
    "end": "2425060"
  },
  {
    "text": "an expectation over that 2D distribution to get the x, y coordinate of the, um, of the, of the approximately the,",
    "start": "2425060",
    "end": "2432395"
  },
  {
    "text": "the key point of maximal activation. Uh, so you can essentially view this as a form of spatial softmax.",
    "start": "2432395",
    "end": "2439130"
  },
  {
    "text": "Uh, so instead of doing a softmax over a one-dimensional operation, you can do a softmax over a 2D space, uh,",
    "start": "2439130",
    "end": "2444559"
  },
  {
    "text": "and get some, uh, key point out of it. Uh, and of course I guess the,",
    "start": "2444560",
    "end": "2450049"
  },
  {
    "text": "the important part is that this operation is actually fully differentiable so you can, uh, optimize for these kinds of representations,",
    "start": "2450050",
    "end": "2456680"
  },
  {
    "text": "um, with respect to the, um, the objective that you care about what the- things like reconstruction or something like the objective of your task.",
    "start": "2456680",
    "end": "2465065"
  },
  {
    "text": "Uh, and there are also other kind of, uh, more recent approaches that have looked at, um, kind of other,",
    "start": "2465065",
    "end": "2470855"
  },
  {
    "text": "other ways of getting these key point-like representations or object centered representations, uh, in unsupervised ways or in like weakly supervised ways.",
    "start": "2470855",
    "end": "2478790"
  },
  {
    "text": "Um, and so what- the result that you get out of this is if you train it to do reconstruction here,",
    "start": "2478790",
    "end": "2484810"
  },
  {
    "text": "two of the feature points that you might get out of it, um, out of the in this case the 16 feature points that were used.",
    "start": "2484810",
    "end": "2491839"
  },
  {
    "text": "Okay. Um, and I guess there was also- I said smooth here because there's also an auxiliary loss to",
    "start": "2491970",
    "end": "2498230"
  },
  {
    "text": "encourage these are representations to be- to have, um, similar to have constant velocity, uh, through time.",
    "start": "2498230",
    "end": "2505520"
  },
  {
    "text": "Um, but I think that that detail maybe a little bit less important.",
    "start": "2505520",
    "end": "2508950"
  },
  {
    "text": "Yeah. [inaudible]. So in this case the number of feature points tot- the number of total feature points need to be pre- predefined,",
    "start": "2518230",
    "end": "2525035"
  },
  {
    "text": "um, as part of your network, network architecture. So in this case the number of feature points is, is 16.",
    "start": "2525035",
    "end": "2531350"
  },
  {
    "text": "Um, and the dimensionality is 16 times 2. Uh, one thing you could imagine doing, uh,",
    "start": "2531350",
    "end": "2536420"
  },
  {
    "text": "that we actually did explore in this paper as well as have a larger number of feature points and then prune the feature points according to some metric.",
    "start": "2536420",
    "end": "2542900"
  },
  {
    "text": "Uh, for example, according to, um, if some feature points you observed to be very noisy, then you could prune those feature points out",
    "start": "2542900",
    "end": "2549710"
  },
  {
    "text": "if you don't think that they correspond to in, in an automatic way if you don't think they correspond to, um, things relevant to the task. Yeah.",
    "start": "2549710",
    "end": "2558140"
  },
  {
    "text": "[inaudible] So in this case it was, uh,",
    "start": "2558140",
    "end": "2563300"
  },
  {
    "text": "completely unsupervised, um, setting. So the, uh, the goal,",
    "start": "2563300",
    "end": "2568940"
  },
  {
    "text": "this autoencoder was, uh, it's an autoencoder, so it was- it's, it's, it's, its loss is  to reconstruct the image.",
    "start": "2568940",
    "end": "2574880"
  },
  {
    "text": "It needs to find these 2D key points that allow it to reconstruct the image. Um, and because different images have different positions of objects then, uh,",
    "start": "2574880",
    "end": "2585335"
  },
  {
    "text": "extracting the positions of objects or actually extracting the positions of things that change in moving the image lead to, um, good reconstructions.",
    "start": "2585335",
    "end": "2594230"
  },
  {
    "text": "So essentially, just a constraint on your latent space to have, to, to have it be representing these 2D positions and how it uses that,",
    "start": "2594230",
    "end": "2601985"
  },
  {
    "text": "that representation space is up to the model. What if you have [inaudible]",
    "start": "2601985",
    "end": "2609430"
  },
  {
    "text": "Yeah, so, uh, occlusions is challenging.",
    "start": "2609430",
    "end": "2615025"
  },
  {
    "text": "Uh, one thing you could imagine doing is having a recurrent model, uh, and then the recurrent model in principle could try to track it.",
    "start": "2615025",
    "end": "2623275"
  },
  {
    "text": "Um, in this work one of the things we did to deal with occlusions was we used a filtering based approach. Um, we basically did a form of filtering",
    "start": "2623275",
    "end": "2630520"
  },
  {
    "text": "where you can basically look at the, the softmax distribution. If it's very peaked, then it's likely that the,",
    "start": "2630520",
    "end": "2636055"
  },
  {
    "text": "the point is in view. If it's not peaked, then it's likely that the point is occluded, and if it is occluded,",
    "start": "2636055",
    "end": "2641500"
  },
  {
    "text": "then you can actually use your model to fill in, uh, like a Kalman filter style update to fill in where you think that point is.",
    "start": "2641500",
    "end": "2648470"
  },
  {
    "text": "Yeah. So there's a lot of potential details here that I wasn't planning to cover, but it's- yeah, there's different things that you could imagine doing.",
    "start": "2649200",
    "end": "2656500"
  },
  {
    "text": "Um, another challenge with this type of approach like that, I guess is worth mentioning, is that if you have two objects that are identical, um, this softmax,",
    "start": "2656500",
    "end": "2665140"
  },
  {
    "text": "since it's over the entire extent of the image, it's going to have two peaks, uh, if they look identical.",
    "start": "2665140",
    "end": "2670330"
  },
  {
    "text": "And therefore, if you take the expectation, then you'll either get the average or get- you'll get the point that dominates, um,",
    "start": "2670330",
    "end": "2677580"
  },
  {
    "text": "or you might get this flickering between the two points and so there isn't, uh, necessarily a satisfying way to deal with that. Yeah.",
    "start": "2677580",
    "end": "2686970"
  },
  {
    "text": "Is there a type of reference or is this to the, um- Oh, this is actually from 2016. Oh. So the paper reference is there.",
    "start": "2686970",
    "end": "2693510"
  },
  {
    "text": "Um, so here's an example of the learning process. So, uh, we actually- in this work we",
    "start": "2693510",
    "end": "2701410"
  },
  {
    "text": "gave it both the goal image as well as the goal position of the arm, we initialize it with a policy that could mo- that",
    "start": "2701410",
    "end": "2707170"
  },
  {
    "text": "could reach the position- the goal position of the arm but not the goal image. Uh, and then this is the course of, uh, model-based reinforcement learning,",
    "start": "2707170",
    "end": "2713635"
  },
  {
    "text": "whereas in this case it's actually optimizing for a policy that tries to reach both the goal image and the goal position of the arm.",
    "start": "2713635",
    "end": "2719395"
  },
  {
    "text": "Uh, and this is a toy task where the goal position with, uh, the goal position is just to reach, uh, to push the cube over to the left.",
    "start": "2719395",
    "end": "2726520"
  },
  {
    "text": "Uh, and then the final policy you get looks something like this where,",
    "start": "2726520",
    "end": "2731995"
  },
  {
    "text": "um, it's able to push it to the specified position on the, um, on the mat.",
    "start": "2731995",
    "end": "2738220"
  },
  {
    "text": "The colors actually seem off. This is supposed to be green. But, um, anyway, uh,",
    "start": "2738220",
    "end": "2745525"
  },
  {
    "text": "then you can also perform the task the, the, the specialized task that I showed before where the goal is to,",
    "start": "2745525",
    "end": "2751165"
  },
  {
    "text": "um, get the spatula to be in the bowl.",
    "start": "2751165",
    "end": "2755030"
  },
  {
    "text": "Cool. So one of the nice things about this approach also w- with these sorts of key point-based representations, I don't,",
    "start": "2756240",
    "end": "2763885"
  },
  {
    "text": "I guess I'm spending more time on the key point-based representations, but I don't think there's necessarily one approach that's necessarily better than the other,",
    "start": "2763885",
    "end": "2769599"
  },
  {
    "text": "but one thing that is quite convenient about this is you could actually visualize the key points on the image.",
    "start": "2769600",
    "end": "2774865"
  },
  {
    "text": "So in this case, the X's correspond to the goal position, and the blue- and the circles correspond to the current position of the key points, and you can,",
    "start": "2774865",
    "end": "2782095"
  },
  {
    "text": "uh, with this ability to, to visualize them directly on the image, it makes for a very interpretable representation,",
    "start": "2782095",
    "end": "2787855"
  },
  {
    "text": "uh, and this is very useful for debugging. Uh, because if your representation isn't capturing and isn't tracking the objects that you care about,",
    "start": "2787855",
    "end": "2794860"
  },
  {
    "text": "then it's likely that, uh, your algorithm won't work because your representation isn't capturing those things. Yeah.",
    "start": "2794860",
    "end": "2801850"
  },
  {
    "text": "So the key points are in the latent space, right? Yes. The key points are the latent representation.",
    "start": "2801850",
    "end": "2806900"
  },
  {
    "text": "Okay. Um, so these skills were learned with about 125 trials which corresponds to about 11 minutes of robot time per task.",
    "start": "2808440",
    "end": "2817960"
  },
  {
    "text": "Uh, in this case, the representation was actually learned, uh, per environment or per task.",
    "start": "2817960",
    "end": "2823945"
  },
  {
    "text": "Uh, so it's kind of learning this environment specific latent representation. Um, and so as a result the representations can",
    "start": "2823945",
    "end": "2830830"
  },
  {
    "text": "become somewhat specific to that environment. And so for example, if you took this- these representations and try to use it for a different task like the Lego block task,",
    "start": "2830830",
    "end": "2838390"
  },
  {
    "text": "they wouldn't necessarily track the objects, uh, because they weren't trained on those images.",
    "start": "2838390",
    "end": "2843350"
  },
  {
    "text": "Okay. Cool. So one thought exercise.",
    "start": "2843540",
    "end": "2848710"
  },
  {
    "start": "2846000",
    "end": "3095000"
  },
  {
    "text": "Uh, so both the approaches that we looked at were auto-encoder type approaches like generative models where we're predicting the f- the, uh, the image.",
    "start": "2848710",
    "end": "2857815"
  },
  {
    "text": "We're trying to generate the image through some bottleneck, and we may also be learning a model on that bottleneck.",
    "start": "2857815",
    "end": "2863290"
  },
  {
    "text": "So one question is, uh, why do we need to reconstruct the image, right? Uh, why don't just learn some embedding space like the feature points,",
    "start": "2863290",
    "end": "2871765"
  },
  {
    "text": "and then also learn a model on those feature points, and train the representation such that the model is accurate.",
    "start": "2871765",
    "end": "2879320"
  },
  {
    "text": "This seems like somewhat of a reasonable approach. Uh, why is this maybe not a good approach?",
    "start": "2879380",
    "end": "2885180"
  },
  {
    "text": "It's just difficult in practice?",
    "start": "2885180",
    "end": "2893319"
  },
  {
    "text": "Optimizing with respect to model error, for the representation would actually- it's actually definitely a solvable problem.",
    "start": "2893320",
    "end": "2900562"
  },
  {
    "text": "Is the [inaudible]?",
    "start": "2900562",
    "end": "2906605"
  },
  {
    "text": "Um, right. So the embedding does depend on the problem that you're solving.",
    "start": "2906930",
    "end": "2912174"
  },
  {
    "text": "And so if you're optimizing with respect to model error, then it, uh, it may",
    "start": "2912175",
    "end": "2918550"
  },
  {
    "text": "not captu- it'll capture things about the model and not necessarily about the task. Yeah. Um, if it's in latent space, it may not capture some small deviations in the real image?",
    "start": "2918550",
    "end": "2928280"
  },
  {
    "text": "Um. [inaudible]. Yeah. That's actually also a problem with- that's, that's- yes. It's definitely a problem.",
    "start": "2930810",
    "end": "2936400"
  },
  {
    "text": "It's also actually a problem with reconstruction based approaches, and we'll see that in a second. Yeah. [inaudible].",
    "start": "2936400",
    "end": "2945040"
  },
  {
    "text": "Uh-um. And so what does- what does that mean? So that means [inaudible].",
    "start": "2945040",
    "end": "2956080"
  },
  {
    "text": "Uh-um. And so what happens if you optimize for both the embedding and",
    "start": "2956080",
    "end": "2962230"
  },
  {
    "text": "the model with respect to the error of the model? [inaudible].",
    "start": "2962230",
    "end": "2968260"
  },
  {
    "text": "Sorry, what? [inaudible].",
    "start": "2968260",
    "end": "2973630"
  },
  {
    "text": "Yes. Yeah. So there is a solution to this. So this, this- to- basically to",
    "start": "2973630",
    "end": "2980890"
  },
  {
    "text": "the model error objective which is that if your embedding is always the same thing, if it's a constant, uh,",
    "start": "2980890",
    "end": "2988900"
  },
  {
    "text": "if it's like always zero for example, then it's perfectly- it's very easy to predict the next state, right?",
    "start": "2988900",
    "end": "2994690"
  },
  {
    "text": "Because this is just always zero. Um, and then as a result- I mean, that embedding isn't very useful because it's a constant, it's always zero.",
    "start": "2994690",
    "end": "3001710"
  },
  {
    "text": "Uh, but it achieves perfect model error. Um, so there's basically this degenerate solution",
    "start": "3001710",
    "end": "3007665"
  },
  {
    "text": "that comes up if you try to optimize with respect to model error, um, with- for both the embedding and the model.",
    "start": "3007665",
    "end": "3015015"
  },
  {
    "text": "Um, so it's not really a, a good idea. [LAUGHTER] So this is why we need kind of other,",
    "start": "3015015",
    "end": "3022365"
  },
  {
    "text": "other forms of objectives to optimize for these representations in addition to model error so that you can avoid that degenerate solution.",
    "start": "3022365",
    "end": "3029559"
  },
  {
    "text": "Okay. Does that make sense? What is the other entropy [inaudible].",
    "start": "3030800",
    "end": "3035970"
  },
  {
    "text": "Yeah. So that's actually, uh, an interesting point.",
    "start": "3035970",
    "end": "3042060"
  },
  {
    "text": "So if you add, um, an entropy term, interestingly that will actually correspond to maximizing the mutual information between",
    "start": "3042060",
    "end": "3049740"
  },
  {
    "text": "your representation and your, uh, your observation. Um, so the- out- well, if you, uh,",
    "start": "3049740",
    "end": "3058650"
  },
  {
    "text": "if you want to maximize your mutual information between your image observation and your latent representation,",
    "start": "3058650",
    "end": "3064515"
  },
  {
    "text": "uh, you can show that this is equal to, uh, H of z minus H of z given S. Um,",
    "start": "3064515",
    "end": "3072990"
  },
  {
    "text": "so this would correspond to maximize the entropy like we've said, and this would correspond to, um, being able to predict z from S. Uh,",
    "start": "3072990",
    "end": "3081480"
  },
  {
    "text": "and so that's actually a pretty good thing to do, and a lot of people have looked at these types of objectives for learning representations.",
    "start": "3081480",
    "end": "3088050"
  },
  {
    "text": "Cool. Um, so to wrap up the latent space approaches, uh,",
    "start": "3088050",
    "end": "3098055"
  },
  {
    "text": "the benefits of this approach is that you can learn pretty complex skills, uh, very efficiently and some structure- structured representations",
    "start": "3098055",
    "end": "3105869"
  },
  {
    "text": "enable very effective learning of these tasks. Um, the downside is that, uh,",
    "start": "3105870",
    "end": "3110910"
  },
  {
    "text": "or I guess one of the main downsides is that we need good objectives for learning these representations, um, and things like reconstruction objectives",
    "start": "3110910",
    "end": "3118290"
  },
  {
    "text": "may not actually recover the right representation. Uh, so as an example of this, uh,",
    "start": "3118290",
    "end": "3123435"
  },
  {
    "text": "when I was doing those experiments a few years ago with the- the spatula task, we also wanted the robot to do another task which,",
    "start": "3123435",
    "end": "3129869"
  },
  {
    "text": "ah, was to manipulate a ping-pong ball, uh, by like basically, like kind of kind of-transferring it from one container to another container.",
    "start": "3129870",
    "end": "3137609"
  },
  {
    "text": "And so here's a downsampled image of, uh, of that experiment where the white dot corresponds to the ping pong ball,",
    "start": "3137610",
    "end": "3144120"
  },
  {
    "text": "and the- you can see the arm of the robot as well. And so I trained an auto-encoder on these images,",
    "start": "3144120",
    "end": "3149474"
  },
  {
    "text": "and the reconstructions that I got out of it look like this. Uh, and so what you can see is that it learns to,",
    "start": "3149475",
    "end": "3155805"
  },
  {
    "text": "uh, learns a very good eraser of the ping pong ball. Uh, and instead just learns to reconstruct the arm,",
    "start": "3155805",
    "end": "3161010"
  },
  {
    "text": "uh, because that's the thing that's larger in the image. And so there's this mismatch between the objective of",
    "start": "3161010",
    "end": "3166860"
  },
  {
    "text": "the representation learning and the objective of the task that you might care about. Um, so kind of the takeaway here is that we may need",
    "start": "3166860",
    "end": "3173880"
  },
  {
    "text": "better-unsupervised representation learning methods, uh, be it reconstruction based methods or,",
    "start": "3173880",
    "end": "3178965"
  },
  {
    "text": "um, like mutual information based on the objectives. Okay, and then one other side note is that",
    "start": "3178965",
    "end": "3186660"
  },
  {
    "start": "3184000",
    "end": "3313000"
  },
  {
    "text": "low-dimensional embeddings can be- also be very useful for model-free approaches. Uh, so for example,",
    "start": "3186660",
    "end": "3191880"
  },
  {
    "text": "you could learn a- a low dimensional embedding and do model-free in that latent space, model-free RL in that latent space. Uh, so there's work back in 2012 that did this for this,",
    "start": "3191880",
    "end": "3199559"
  },
  {
    "text": "um, this slot car. Uh, where they trained an auto-encoder and then to- down to",
    "start": "3199560",
    "end": "3204570"
  },
  {
    "text": "a two-dimensional representation and then did fitted Q iteration on top of that two- two-dimensional representation. Uh, and this work actually predates things like DQM,",
    "start": "3204570",
    "end": "3211860"
  },
  {
    "text": "so they're doing deep RL back in, back in 2012. Uh, here's an approach that was able to run TRPO,",
    "start": "3211860",
    "end": "3219059"
  },
  {
    "text": "which is typically an algorithm that requires a very large number of samples. Uh, but they learned a latent space of- actually,",
    "start": "3219060",
    "end": "3224670"
  },
  {
    "text": "both the state and the actions, and were able to run TRPO on a real robot, uh, to throw an object to hit the Pikachu.",
    "start": "3224670",
    "end": "3231885"
  },
  {
    "text": "And then there are also methods that use an embedding for their reward function. So we talked a little bit about how, uh,",
    "start": "3231885",
    "end": "3238260"
  },
  {
    "text": "in the previous approaches we're using the, the- embedding both for the state representation and for the reward representation.",
    "start": "3238260",
    "end": "3243630"
  },
  {
    "text": "Um, in this case, this work was looking at, uh, acquiring a reward function from ImageNet features.",
    "start": "3243630",
    "end": "3249480"
  },
  {
    "text": "Uh, so this is actually, a supervised representation learning method. They took, uh, they took a video of a human opening a door,",
    "start": "3249480",
    "end": "3256130"
  },
  {
    "text": "run that through ImageNet, and then used that as a reward function for a robot to try to reach the same features that the,",
    "start": "3256130",
    "end": "3262025"
  },
  {
    "text": "um, that the video on the left was, uh, was reaching. Um, and the one other thing worth mentioning is that if you ha- if you have a reward,",
    "start": "3262025",
    "end": "3270599"
  },
  {
    "text": "you can actually predict it to form a better latent space, and this can- this is one way to kind of help solve that degenerate solution prob- um,",
    "start": "3270600",
    "end": "3276480"
  },
  {
    "text": "degenerate solution that we observed if you just try to predict model error, and there are a number of approaches that have looked into that as well.",
    "start": "3276480",
    "end": "3283859"
  },
  {
    "text": "Um, one reason why you may not want to predict reward, is that maybe that you don't have a good reward function.",
    "start": "3283860",
    "end": "3289155"
  },
  {
    "text": "Um, so in the case of the spatula, in the case of the, um, the embedded control paper, if you just have goal images we don't actually have reward functions.",
    "start": "3289155",
    "end": "3296865"
  },
  {
    "text": "Um, but if you do have a reward function it's, it's good to try to use it. Okay. Um, now that we've talked about latent space models,",
    "start": "3296865",
    "end": "3307770"
  },
  {
    "text": "let's talk about modeling things directly in your observation space. Uh, so we can recall the,",
    "start": "3307770",
    "end": "3315735"
  },
  {
    "start": "3313000",
    "end": "3600000"
  },
  {
    "text": "um, kind of the model-based RL approaches that we mentioned before. Uh, in this case, this is just the,",
    "start": "3315735",
    "end": "3321870"
  },
  {
    "text": "the same MPC algorithm that I showed before, but where all the states are replaced with our observations O.",
    "start": "3321870",
    "end": "3328180"
  },
  {
    "text": "And what we can do is, we can learn a model on our observations,",
    "start": "3328340",
    "end": "3334125"
  },
  {
    "text": "uh, and, and plan with that model. So, uh, first we wanna run some policy,",
    "start": "3334125",
    "end": "3339510"
  },
  {
    "text": "uh, to collect some data. Uh, so for example, we could collect data that looks like this.",
    "start": "3339510",
    "end": "3345780"
  },
  {
    "text": "Um, this is just robots randomly interacting with, with objects picking them up and such,",
    "start": "3345780",
    "end": "3351345"
  },
  {
    "text": "and then the data corresponds to the images and actions. Uh, and so it's very easy to collect data like this,",
    "start": "3351345",
    "end": "3357720"
  },
  {
    "text": "you don't need reward functions, you can just run, run off your, your robot or your agent in whatever environment. Uh, then you can learn a model to minimize prediction error.",
    "start": "3357720",
    "end": "3367320"
  },
  {
    "text": "And so this corresponds to a video prediction model. So you may get predictions that look like this for",
    "start": "3367320",
    "end": "3372540"
  },
  {
    "text": "different actions that are run through your model. Um, also because we're, uh, we're not imposing any representation on our,",
    "start": "3372540",
    "end": "3380280"
  },
  {
    "text": "uh, on our, uh, state. We can also apply these sorts of models to deformable objects because we're just predicting our raw sensory observations.",
    "start": "3380280",
    "end": "3388545"
  },
  {
    "text": "Uh, and then once you have that model we can use that model to optimize over actions, uh, by actually sampling,",
    "start": "3388545",
    "end": "3396300"
  },
  {
    "text": "check through that model, and picking the actions that we think will accomplish our goal. Uh, so this is pretty straightforward.",
    "start": "3396300",
    "end": "3402630"
  },
  {
    "text": "Uh, there are a couple of challenges though, which is that we need to learn these models, which are pretty challenging to learn,",
    "start": "3402630",
    "end": "3408540"
  },
  {
    "text": "and we need to be able to learn, uh, the models by optimizing through these large video prediction models.",
    "start": "3408540",
    "end": "3414970"
  },
  {
    "text": "So, uh, first question, how do we actually predict the video? Um, we wanna learn this model.",
    "start": "3415070",
    "end": "3421200"
  },
  {
    "text": "Uh, this is a fairly complex model, because it's a model of how images transform as a consequence of our actions.",
    "start": "3421200",
    "end": "3427605"
  },
  {
    "text": "So uh, this is a problem that people have been saying for a little while now.",
    "start": "3427605",
    "end": "3434369"
  },
  {
    "text": "Uh, maybe 5, 10 years, uh, at least. Uh, although back in, in 2016,",
    "start": "3434370",
    "end": "3441090"
  },
  {
    "text": "it turned out that the models were pretty bad. Uh, so, uh, so one of the, an example of a model that works,",
    "start": "3441090",
    "end": "3448529"
  },
  {
    "text": "ah, a bit better is something that looks like this. So this is just a big neural network, um, because the, the main points is that it's,",
    "start": "3448530",
    "end": "3455445"
  },
  {
    "text": "uh, it's deep neural network, and its recurrent, uh, and each of these, uh, each of the yellow arrows corresponds to recurrence,",
    "start": "3455445",
    "end": "3462569"
  },
  {
    "text": "and each of the green boxes and blue boxes correspond to convolutions. Uh, it's performing multiple f- multiple f- ma- multi-frame predictions,",
    "start": "3462570",
    "end": "3471210"
  },
  {
    "text": "it was predicting multiple frames into the future. Uh, it's conditioning on actions so the actions are,",
    "start": "3471210",
    "end": "3477135"
  },
  {
    "text": "are passed in here as well as any state information that you might have about like the position of the robot's arm.",
    "start": "3477135",
    "end": "3482610"
  },
  {
    "text": "Um, and then the other thing about this model is, there's actually explicitly modeling the motion of pixels.",
    "start": "3482610",
    "end": "3487875"
  },
  {
    "text": "So, um, rather than actually trying to generate pixels directly, like having a neural network with output pixels, pixel values.",
    "start": "3487875",
    "end": "3495255"
  },
  {
    "text": "Uh, what this model is doing is that it's taking the previous image, it's predicting, um, actually multiple convolution kernels,",
    "start": "3495255",
    "end": "3503145"
  },
  {
    "text": "and then applying those convolution kernels to the image to generate multiple transformations of that image and then composing those transformed images,",
    "start": "3503145",
    "end": "3512834"
  },
  {
    "text": "uh, with these also predicted masks into a single image prediction. Um, so it's essentially predicting the motion of- basically,",
    "start": "3512834",
    "end": "3519569"
  },
  {
    "text": "predicting how the previous image will transform into the next image in a way that's differentiable that can be backpropagated through.",
    "start": "3519570",
    "end": "3526690"
  },
  {
    "text": "Um, and so here are some examples of some videos from a, a robot.",
    "start": "3527300",
    "end": "3532965"
  },
  {
    "text": "And if you took, uh, some of the models back in, uh, in 2015 for example, uh,",
    "start": "3532965",
    "end": "3539985"
  },
  {
    "text": "you would get models that look like this, uh, or predictions that look like this, which don't look very good. Uh, whereas if you, um,",
    "start": "3539985",
    "end": "3546494"
  },
  {
    "text": "have recurrent models that are predicting multiple frames and are explicitly modeling motion, you get predictions that look,",
    "start": "3546495",
    "end": "3552390"
  },
  {
    "text": "uh, much cleaner, still blurrier. Uh, and in general, the video predictions that we're getting out of these models are even,",
    "start": "3552390",
    "end": "3558510"
  },
  {
    "text": "uh, in, in 2019. Uh, still leave some to be desired, but there are, um,",
    "start": "3558510",
    "end": "3563700"
  },
  {
    "text": "still things that we found can be useful for control. Yeah. Does this, this model predict control as well?",
    "start": "3563700",
    "end": "3569610"
  },
  {
    "text": "Yeah. So let's talk about the, um, let's talk about the planning approach. So once we have our model,",
    "start": "3569610",
    "end": "3575130"
  },
  {
    "text": "we need to actually optimize the action sequence. So the way that you can do this is basically with the sampling-based optimization that we described previously.",
    "start": "3575130",
    "end": "3583875"
  },
  {
    "text": "So say this is our initial image. We, we consider potential action sequences. It's probably like a hundred or a couple hundred action sequences,",
    "start": "3583875",
    "end": "3590969"
  },
  {
    "text": "uh, including these two action sequences. Then, predict the future for each action sequence by running those actions through",
    "start": "3590969",
    "end": "3596910"
  },
  {
    "text": "your model to get video representations that look like this. Uh, and then you can pick the feature that you like the best, uh,",
    "start": "3596910",
    "end": "3603915"
  },
  {
    "text": "and execute the corresponding action, um, or instead of picking the best one, you could also iteratively re-sample,",
    "start": "3603915",
    "end": "3610349"
  },
  {
    "text": "and then pick the best one. Uh, and then, what you do, is you could actually repeat these first three steps in real-time in order to re-plan and do MPC,",
    "start": "3610350",
    "end": "3619595"
  },
  {
    "text": "and basically just planning at every single time-step. Um, we felt that this is something that is practical to do.",
    "start": "3619595",
    "end": "3626965"
  },
  {
    "text": "But, uh, in the context of- uh, with video prediction models it can be a bit slow. So the sampling-based approach corresponds to",
    "start": "3626965",
    "end": "3633839"
  },
  {
    "text": "rolling out these big convolutional neural networks, uh, and rolling out batches, batches of like a hundred or hundreds of action sequences.",
    "start": "3633840",
    "end": "3641279"
  },
  {
    "text": "And so as a result, uh, the, the time it takes to plan can be on the order of one hertz,",
    "start": "3641280",
    "end": "3646740"
  },
  {
    "text": "for example, um, depending on how many GPUs you parallelize it with. That one hertz would probably be,",
    "start": "3646740",
    "end": "3652725"
  },
  {
    "text": "parallelized across like two to four GPUs. Okay. Um, so you can essentially view this as MPC but in visual space,",
    "start": "3652725",
    "end": "3662760"
  },
  {
    "text": "uh, it's like visual MPC. Okay. So that's kind of how you can do these sorts of",
    "start": "3662760",
    "end": "3668490"
  },
  {
    "text": "model-based RL methods in the raw observation space. Uh, the way that it works,",
    "start": "3668490",
    "end": "3673500"
  },
  {
    "text": "um, at test time is, you can, you need to specify some goal. There are a few different ways that you can specify",
    "start": "3673500",
    "end": "3679410"
  },
  {
    "text": "goals as we, as we talked about before. You could learn an image classifier, you could, um, you could provide an image of the goal.",
    "start": "3679410",
    "end": "3686579"
  },
  {
    "text": "Uh, one of the things that we did in some of this work is, specify the goal by clicking on a pixel and clicking on where that pixel should be moved to.",
    "start": "3686580",
    "end": "3693565"
  },
  {
    "text": "Uh, so for example, in this case, the goal would be to fold the left, uh, pant leg of these shorts,",
    "start": "3693565",
    "end": "3699810"
  },
  {
    "text": "uh, by moving the red pixel to the green pixel.",
    "start": "3699810",
    "end": "3704940"
  },
  {
    "text": "Uh, and then we also specify another pixel right here to specify that, um, the pants should stay in place, um,",
    "start": "3704940",
    "end": "3712650"
  },
  {
    "text": "if they're not, uh, if they're not part of the folding part. So once you have this goal you can run,",
    "start": "3712650",
    "end": "3719070"
  },
  {
    "text": "uh, MPC with respect to this specified goal. And then, this is the video prediction corresponding to",
    "start": "3719070",
    "end": "3726380"
  },
  {
    "text": "the action plan that was found by MPC. And then, execute the corresponding action on the robot,",
    "start": "3726380",
    "end": "3731990"
  },
  {
    "text": "uh, to try to accomplish that goal. And so here's an example of, uh, what the robot could try to accomplish by,",
    "start": "3731990",
    "end": "3738625"
  },
  {
    "text": "uh, with, with respect to the goal of moving the pixel upward. Okay. Um, and so getting back to the, kind of,",
    "start": "3738625",
    "end": "3746730"
  },
  {
    "text": "multitask learning aspect of model-based RL, one of the things that you can do is you can use the single video prediction model to accomplish multiple tasks.",
    "start": "3746730",
    "end": "3753599"
  },
  {
    "text": "So, um, for example, if your goal is to pick up an object, you can click on an object,",
    "start": "3753600",
    "end": "3759180"
  },
  {
    "text": "click on where you want to move and the robot can figure out how to pick it up. Um, also, if you want to manipulate the sleeve of a shirt, uh,",
    "start": "3759180",
    "end": "3766230"
  },
  {
    "text": "it can figure that out, um, or like a task like putting an apple onto a plate. Uh, then we can also look at a few other examples like,",
    "start": "3766230",
    "end": "3773954"
  },
  {
    "text": "uh, folding the shorts, re- like rearranging objects or, um, like covering an object with a towel.",
    "start": "3773955",
    "end": "3780510"
  },
  {
    "text": "Um, yeah. So one of the nice things about this is that it allows us to accomplish many different goals or many different reward",
    "start": "3780510",
    "end": "3787200"
  },
  {
    "text": "functions with a single model without having to retrain our model for every single task.",
    "start": "3787200",
    "end": "3792760"
  },
  {
    "text": "Um, and then the other nice thing about this is the- the model training part is s- self supervised. You don't need to provide reward functions or supervision, um,",
    "start": "3793580",
    "end": "3801330"
  },
  {
    "text": "the robot can, kind of, just collect data, uh, and train the model on that data.",
    "start": "3801330",
    "end": "3806170"
  },
  {
    "text": "Okay. So, um, the benefits of this kind of approach is that, uh,",
    "start": "3806510",
    "end": "3811650"
  },
  {
    "text": "this was, uh, able to scale to real images, uh, fairly effectively.",
    "start": "3811650",
    "end": "3816944"
  },
  {
    "text": "Um, there's also very limited human involvement., uh, and so the model training was fully self supervised.",
    "start": "3816945",
    "end": "3823035"
  },
  {
    "text": "Uh, and this was also able to accomplish many different tasks with a single model. Um, these pros are also shared with many of the latent space approaches as well.",
    "start": "3823035",
    "end": "3831630"
  },
  {
    "text": "Although in practice, we've found that latent-space approaches, uh, have trouble modeling some of the di- some of the diversity of the videos like this",
    "start": "3831630",
    "end": "3840180"
  },
  {
    "text": "because you have to capture all of those objects that you might see in a compact latent space.",
    "start": "3840180",
    "end": "3845760"
  },
  {
    "text": "Um, and some of the downsides is that despite the fact that they're real images, there's somewhat limited background variability.",
    "start": "3845760",
    "end": "3851640"
  },
  {
    "text": "So this is more variability than like the spatula example, for example, but still less variability to things,",
    "start": "3851640",
    "end": "3857474"
  },
  {
    "text": "um, like ImageNet for example. Um, you can't ha- can't yet handle as complex skills as the spatula example, for example.",
    "start": "3857475",
    "end": "3866925"
  },
  {
    "text": "These are just, um, kind of pick and place style tasks. And it's also very compute intensive, uh, test-time.",
    "start": "3866925",
    "end": "3874960"
  },
  {
    "text": "Okay. Any questions on how that works?",
    "start": "3875360",
    "end": "3880600"
  },
  {
    "text": "Okay. Um, one other quick aside, because I think we have a bit of time,",
    "start": "3883400",
    "end": "3889365"
  },
  {
    "text": "is uh, how can we think about actually doing more complex skills, uh, rather than things like pick and place?",
    "start": "3889365",
    "end": "3896295"
  },
  {
    "text": "And one thing you could imagine doing, uh, as we talked about before, is using your planner to collect",
    "start": "3896295",
    "end": "3901920"
  },
  {
    "text": "more data and then using that data to improve your model. And I would expect something like that to,",
    "start": "3901920",
    "end": "3907980"
  },
  {
    "text": "uh, perform pretty well. Although in practice, one of the challenges with that is if your planner is very compute intensive,",
    "start": "3907980",
    "end": "3914700"
  },
  {
    "text": "then it may be very expensive to collect more data using your model.",
    "start": "3914700",
    "end": "3919950"
  },
  {
    "text": "Uh, so one approach that we've looked at in the context of this work, uh, is if we can incorporate some forms of supervision such as",
    "start": "3919950",
    "end": "3927240"
  },
  {
    "text": "demonstrations in order to learn more complex skills. So, uh, what you could do is you could collect, uh,",
    "start": "3927240",
    "end": "3933960"
  },
  {
    "text": "demonstrations from many different tasks and potentially use those demonstrations to improve the complexity of skills that you could learn with this approach.",
    "start": "3933960",
    "end": "3942405"
  },
  {
    "text": "Uh, in particular, there's a few different ways that you could use these demonstrations. Um, the first is to append it to your data set and use them to improve your model.",
    "start": "3942405",
    "end": "3950220"
  },
  {
    "text": "Uh, but you can also use it to improve the other two, uh, approaches as well.",
    "start": "3950220",
    "end": "3955515"
  },
  {
    "text": "So what you could do is you could fit a model to the behavior of the demonstrator to basically",
    "start": "3955515",
    "end": "3960660"
  },
  {
    "text": "predict the kinds of actions that the demonstrator might take based on an initial image.",
    "start": "3960660",
    "end": "3966480"
  },
  {
    "text": "And if you have this model of the kinds of tasks that are interesting to perform, then you can use this to first to like direct your data collection proc-",
    "start": "3966480",
    "end": "3973740"
  },
  {
    "text": "process towards the more interesting kinds of behaviors and the more interesting tasks. And you could also use it to guide the planning process.",
    "start": "3973740",
    "end": "3980925"
  },
  {
    "text": "Um, so if you know that you can be doing kind of a task that is, uh, that may resemble some of the tasks that you saw in the demonstrations,",
    "start": "3980925",
    "end": "3986700"
  },
  {
    "text": "then you can sample actions, uh, similar to the actions that the human would- would take in addition to the actions,",
    "start": "3986700",
    "end": "3993930"
  },
  {
    "text": "uh, that you would sample from some random distribution. Uh, so an example of this is that,",
    "start": "3993930",
    "end": "4000335"
  },
  {
    "text": "uh, I guess, one, one setting where we study this problem is in the example where you want to have a robot manipulate tools.",
    "start": "4000335",
    "end": "4009035"
  },
  {
    "text": "Uh, so here are some examples of the demonstrations that, uh, that the, uh, user collected,",
    "start": "4009035",
    "end": "4016565"
  },
  {
    "text": "uh, using different tools to, uh, push them in different ways. Uh, here are some examples of the samples from the action proposal models.",
    "start": "4016565",
    "end": "4023960"
  },
  {
    "text": "So this is, kind of, the, uh, the types of actions that the robot thinks that the user might perform.",
    "start": "4023960",
    "end": "4031490"
  },
  {
    "text": "So these correspond to, kind of, grasping towards objects and moving those objects. Uh, these, these, uh,",
    "start": "4031490",
    "end": "4037700"
  },
  {
    "text": "actions are actually passed through the video picture mode. So these aren't actually videos but these are actions that I think might be interesting,",
    "start": "4037700",
    "end": "4043250"
  },
  {
    "text": "uh, as paths to the video prediction model. And then what you can do is that, uh, you can specify your goal as before.",
    "start": "4043250",
    "end": "4049535"
  },
  {
    "text": "Then run planning guided by that action proposal model with respect to your goal, uh,",
    "start": "4049535",
    "end": "4054890"
  },
  {
    "text": "to get a prediction that looks like this and a sequence of actions that corresponds to this video.",
    "start": "4054890",
    "end": "4061340"
  },
  {
    "text": "And then execute those actions on the robot to get, um, to try to accomplish the goal in the bottom, in the top-left.",
    "start": "4061340",
    "end": "4069269"
  },
  {
    "text": "Okay. So by actually incorporating these diverse demonstrations we're allowed, uh,",
    "start": "4069670",
    "end": "4076760"
  },
  {
    "text": "the robot is allowed- is- can now, kinda, perform these more complex tasks that involve grasping an object and then using that object to perform the task,",
    "start": "4076760",
    "end": "4084380"
  },
  {
    "text": "rather than just, uh, pick and place tasks and pushing tasks. Okay. Um, and again, because the,",
    "start": "4084380",
    "end": "4091715"
  },
  {
    "text": "the model is trained on a diverse set of, uh, objects and tasks, and because the demonstrations are also diverse,",
    "start": "4091715",
    "end": "4098345"
  },
  {
    "text": "it means that this single model is reusable for these different kinds of tasks. So, um, the model can be used to solve, uh,",
    "start": "4098345",
    "end": "4105980"
  },
  {
    "text": "tasks that weren't seen in the demonstrations such as, uh, using a broom to push objects into a dust pan.",
    "start": "4105980",
    "end": "4113089"
  },
  {
    "text": "Um, it can be used for trying to like use a hook to bring out of reach objects closer to the robot.",
    "start": "4113090",
    "end": "4119674"
  },
  {
    "text": "Um, so in this case, the, the robot was constrained to move in that green shaded region",
    "start": "4119675",
    "end": "4125134"
  },
  {
    "text": "such that it actually had to use the hook in order to accomplish the task of moving the, the, um, the blue object closer.",
    "start": "4125135",
    "end": "4133265"
  },
  {
    "text": "Um, it can also generalize to unseen tools. And this is, uh, by nature of the fact that it has a, a large diverse data set,",
    "start": "4133265",
    "end": "4139370"
  },
  {
    "text": "um, also unseen tools that aren't really conventional tools like water bottles. Um, and because you're sampling from both, uh,",
    "start": "4139370",
    "end": "4146869"
  },
  {
    "text": "the demonstration, the action proposal distribution and a random distribution, uh, they can also figure out when do you use a tool such as when there are",
    "start": "4146870",
    "end": "4153770"
  },
  {
    "text": "two objects that need to be pushed versus when not to use a tool, um, when only a single object needs to be pushed.",
    "start": "4153770",
    "end": "4159720"
  },
  {
    "text": "Okay. Cool. So that was one side on h- one way that you might go about",
    "start": "4160240",
    "end": "4165605"
  },
  {
    "text": "incorporating demonstration data or incorporating other forms of supervision in order to perform more complex tasks.",
    "start": "4165605",
    "end": "4170630"
  },
  {
    "text": "Um, the last kind of approach that I'd like to talk about with regard to image observations is predicting alternative qua- quantities.",
    "start": "4170630",
    "end": "4178339"
  },
  {
    "text": "So, uh, it may be that you don't want to be reconstructing images,",
    "start": "4178340",
    "end": "4183859"
  },
  {
    "text": "uh, such as, uh, video prediction models and audio encoders. And it may be that you have some supervision or",
    "start": "4183860",
    "end": "4190130"
  },
  {
    "text": "some other auxiliary information that you care about for performing your task. And in these contexts you can try to predict those sorts of things.",
    "start": "4190130",
    "end": "4197735"
  },
  {
    "text": "Uh, so for example, if you want to be able to learn how to grasp objects,",
    "start": "4197735",
    "end": "4202759"
  },
  {
    "text": "what you could do is that given a sequence of actions, you can predict whether or not that sequence of actions will lead to a grasp.",
    "start": "4202759",
    "end": "4209810"
  },
  {
    "text": "Um, and so given for example, one of these, these yellow actions, you can predict the binary event of grasping or not grasping.",
    "start": "4209810",
    "end": "4218240"
  },
  {
    "text": "Um, and grasping is actually something that you could measure on the robot by looking at how wide the- whether or not after performing those actions,",
    "start": "4218240",
    "end": "4225695"
  },
  {
    "text": "the robot was actually holding something. Um, another example is, uh, if you care about collision avoidance,",
    "start": "4225695",
    "end": "4231710"
  },
  {
    "text": "you can predict given a sequence of actions, will I collide? Will I hit an object? And so if you have a sensor that can measure whether or not you've collided,",
    "start": "4231710",
    "end": "4238505"
  },
  {
    "text": "then this is something that you can predict relatively easily. Uh, then you can also predict, um, things like in a video game,",
    "start": "4238505",
    "end": "4245630"
  },
  {
    "text": "your health your- or your damage or other, uh, information about the environment.",
    "start": "4245630",
    "end": "4250955"
  },
  {
    "text": "Uh, so this is something that's very nice, if you don't wanna- in these settings you don't have to generate images.",
    "start": "4250955",
    "end": "4256820"
  },
  {
    "text": "Um, this also has a very close connection to Q-learning because if you're predicting these types of events that will happen,",
    "start": "4256820",
    "end": "4262550"
  },
  {
    "text": "if they will happen in the future, then you are essentially, uh, trying to predict the probability of some event happening which,",
    "start": "4262550",
    "end": "4269900"
  },
  {
    "text": "um, may correspond to your reward function if you care about whether or not that event happens.",
    "start": "4269900",
    "end": "4275040"
  },
  {
    "text": "Okay. So the benefits of this approach, in general, is that you can only pred- it allows you to only predict the task relevant quantities.",
    "start": "4275230",
    "end": "4282364"
  },
  {
    "text": "Uh, and then if you're in a multi-task setting, you could predict the things that c- that are relevant for different tasks. Uh, the downside is that you need to be able to observe those quantities,",
    "start": "4282365",
    "end": "4291770"
  },
  {
    "text": "uh, which isn't true in the general case. You don't always know, for example, where objects are or maybe if you are in a dialogue",
    "start": "4291770",
    "end": "4298070"
  },
  {
    "text": "setting you don't know the sentiment of the other person automatically. You don't observe that automatically. Um, and of course,",
    "start": "4298070",
    "end": "4304730"
  },
  {
    "text": "you also need to manually pick these quantities that you think might be relevant to your task. Okay. Cool. So that was all I had on model-based RL with image observations.",
    "start": "4304730",
    "end": "4317720"
  },
  {
    "text": "Um, let's see. You have eight minutes. I think we have time to",
    "start": "4317720",
    "end": "4324320"
  },
  {
    "text": "cover though the last part rather than moving it to next week. Are there any questions on this before I move on?",
    "start": "4324320",
    "end": "4329850"
  },
  {
    "text": "Okay. So what about model-based meta-RL?",
    "start": "4330970",
    "end": "4336230"
  },
  {
    "text": "Um, in some sense, we've already been doing some form of meta-RL. Uh, and I'll talk about that in a second.",
    "start": "4336230",
    "end": "4343670"
  },
  {
    "text": "So we talked a bit before about how in many situations we have this dynamics model that doesn't vary across tasks.",
    "start": "4343670",
    "end": "4350870"
  },
  {
    "text": "Uh, and in these cases, estimating the model is a single task problem. Um, but what if we- what if the dynamics are actually changing across tasks?",
    "start": "4350870",
    "end": "4358340"
  },
  {
    "text": "Um, and so for example, uh, if you're interacting with objects and you see an object on the table and you don't know,",
    "start": "4358340",
    "end": "4366815"
  },
  {
    "text": "kind of, a priori how that object is going to move, if you just see an image of that object, you don't know the center of mass of the object.",
    "start": "4366815",
    "end": "4372830"
  },
  {
    "text": "You don't know the friction. And so you don't necessarily know how it will actually move until you start interacting with it. Uh, in, in that context,",
    "start": "4372830",
    "end": "4380275"
  },
  {
    "text": "it's actually somewhat of a partially observed problem and you need to actually adapt your model based off of a small amount of data in order to",
    "start": "4380275",
    "end": "4385570"
  },
  {
    "text": "accurately predict how that object will move. Um, and so you can essentially view- if the dynamics are changing across tasks,",
    "start": "4385570",
    "end": "4393440"
  },
  {
    "text": "you can actually turn mod- the model learning problem from a supervised learning problem into a meta learning problem.",
    "start": "4393440",
    "end": "4399080"
  },
  {
    "text": "Where you're now going to be conditioning your model on some data and then using that data to learn a better model.",
    "start": "4399080",
    "end": "4406025"
  },
  {
    "text": "Uh, in this context, any of the kind of meta learning projects that we talked about before could be applied to this context.",
    "start": "4406025",
    "end": "4413315"
  },
  {
    "text": "Um, so for example, one meta learning approach that we talked about before is using things like LSTMs or models with memory.",
    "start": "4413315",
    "end": "4420755"
  },
  {
    "text": "Uh, and we're actually already using LSTMs to- in, in recurrent models to make predictions before.",
    "start": "4420755",
    "end": "4427085"
  },
  {
    "text": "So the- those vision based models they were- um, you can essentially view them, uh, in this way in,",
    "start": "4427085",
    "end": "4432260"
  },
  {
    "text": "in a sense as a meta learning problem because they're taking in the context of the past, uh, few frames and predicting into the future.",
    "start": "4432260",
    "end": "4439445"
  },
  {
    "text": "Uh, so there's this somewhat of a blurred line between what is- what constitutes a single model and what constitutes,",
    "start": "4439445",
    "end": "4446900"
  },
  {
    "text": "uh, like a meta-learned model. Okay. So one thing you could do is simply kind of collect data,",
    "start": "4446900",
    "end": "4458330"
  },
  {
    "text": "uh, if you want to turn this into a meta-learning problem, you can collect data in different environments and adapt your model to an environment given a small amount of data.",
    "start": "4458330",
    "end": "4464660"
  },
  {
    "text": "Um, and you can actually also do this in a more online fashion. So say you have some robot that's, uh,",
    "start": "4464660",
    "end": "4470555"
  },
  {
    "text": "interacting in the environment and, uh, has this different- different parts of the environment have different dynamics such as",
    "start": "4470555",
    "end": "4477199"
  },
  {
    "text": "a terrain change or a motor malfunction that causes the dynamics to change. Now, one of the things that you can do is you can kind of flatten out the, um,",
    "start": "4477200",
    "end": "4485465"
  },
  {
    "text": "the experience of the robot, uh, and view these different, uh, changes in dynamics as happening in different points in time.",
    "start": "4485465",
    "end": "4493250"
  },
  {
    "text": "And then if you take it- uh, if you take this viewpoint, you could essentially view, uh, the- the few-shot learning problem, uh,",
    "start": "4493250",
    "end": "4500570"
  },
  {
    "text": "or kinda the meta-learning problem as one of taking a slice- taking a window of time and using that slice of data to predict",
    "start": "4500570",
    "end": "4507739"
  },
  {
    "text": "what will happen in the following slice of data. Um, so you could essentially view this problem",
    "start": "4507740",
    "end": "4514760"
  },
  {
    "text": "of adapting your model or adapting to your environment online as a few-shot learning problem where",
    "start": "4514760",
    "end": "4520940"
  },
  {
    "text": "different tasks correspond to different slices of the experience. Uh, where basically for,",
    "start": "4520940",
    "end": "4526505"
  },
  {
    "text": "uh, if you have k time steps of experience, this might correspond to the training data",
    "start": "4526505",
    "end": "4534050"
  },
  {
    "text": "set for one task and the following k time steps, or, or n time steps may correspond to the corresponding test set for that task.",
    "start": "4534050",
    "end": "4545645"
  },
  {
    "text": "And then you can- this is one window of experience, you can kind of continuously slide that window to get different tasks,",
    "start": "4545645",
    "end": "4551345"
  },
  {
    "text": "assuming you have some sort of temporal continuity in the dynamics that you are encountering.",
    "start": "4551345",
    "end": "4556739"
  },
  {
    "text": "Okay. So you can use this- you can basically use your favorite meta-learning method, uh, to solve this kind of problem.",
    "start": "4557740",
    "end": "4563929"
  },
  {
    "text": "Uh, and so what this might look like is that you have your last k time steps of experience,",
    "start": "4563930",
    "end": "4569388"
  },
  {
    "text": "you then adapt your model using, uh, your training data set and your prior, uh,",
    "start": "4569389",
    "end": "4574489"
  },
  {
    "text": "to learn a model that's specifically adopted to those k points in time. And then use that model to actually take actions and to,",
    "start": "4574490",
    "end": "4581540"
  },
  {
    "text": "to plan using MPC. Uh, and so, for example this update rule may correspond to one step of gradient descent",
    "start": "4581540",
    "end": "4588260"
  },
  {
    "text": "and theta star may correspond to the initialization if you're using an algorithm like MAML. Uh, and so the way this works is you may,",
    "start": "4588260",
    "end": "4596480"
  },
  {
    "text": "um, kind of collect data on different terrains, uh, such as the terrain shown here. This is a- a little- little six-legged robot called the VelociRoACH.",
    "start": "4596480",
    "end": "4604039"
  },
  {
    "text": "And the dynamics of the robot actually vary drastically across different terrains and across different battery levels.",
    "start": "4604040",
    "end": "4609139"
  },
  {
    "text": "And then you can train it to be able to, uh, estimate dynamics with only k time steps of experience where",
    "start": "4609140",
    "end": "4616190"
  },
  {
    "text": "k is something like eight time steps, and that, in practice, corresponds to actually less than a second of experience if you're at around like",
    "start": "4616190",
    "end": "4622970"
  },
  {
    "text": "10-20 Hertz and then evaluate the robot's ability to adapt to other types of dynamics like being on a slope,",
    "start": "4622970",
    "end": "4629420"
  },
  {
    "text": "or missing a leg, or having a payload, uh, or having calibration errors. So what you can see is that if you try to put the robot on a narrow slope- so here's a,",
    "start": "4629420",
    "end": "4639620"
  },
  {
    "text": "a visual of the slope up close. If you try to run a single model across these settings,",
    "start": "4639620",
    "end": "4645200"
  },
  {
    "text": "uh, what the robot will do is it will, uh, kind of, diverge across. It won't be able to run in a straight line because it won't have learned an accurate model.",
    "start": "4645200",
    "end": "4651590"
  },
  {
    "text": "Uh, whereas, if you use meta-learning and actually adopt online, uh,",
    "start": "4651590",
    "end": "4656869"
  },
  {
    "text": "with each window of experience to the current model and use that to plan and run in a straight line.",
    "start": "4656870",
    "end": "4663574"
  },
  {
    "text": "Uh, you could also do something like take off the front right leg of the robot, uh, and see, uh,",
    "start": "4663575",
    "end": "4669230"
  },
  {
    "text": "if you try to fit a single model, it isn't able to model the dynamics of these different situations. Whereas if you train it to quickly adapt and then use- do that adaptation,",
    "start": "4669230",
    "end": "4677900"
  },
  {
    "text": "uh, at test time, then you could effectively follow a straight line. And so this is actually getting back to one of the questions at",
    "start": "4677900",
    "end": "4684440"
  },
  {
    "text": "the beginning of the lecture where you're not only using the observed state to re-plan but you're also",
    "start": "4684440",
    "end": "4690290"
  },
  {
    "text": "using the observed state to update your model at every single time step.",
    "start": "4690290",
    "end": "4694650"
  },
  {
    "text": "Okay, cool. So I think that we're basically out of time,",
    "start": "4695620",
    "end": "4701630"
  },
  {
    "text": "um, some takeaways for model-based versus model-free learning. So some of the benefits of model-based learning is that it's very",
    "start": "4701630",
    "end": "4708605"
  },
  {
    "text": "easy to collect data in a scalable way, uh, without boards. Um, it's pretty easy to transfer across",
    "start": "4708605",
    "end": "4714560"
  },
  {
    "text": "different reward functions because if- that model only depends on, uh, the data it was trained on. It has a less direct relationship on the reward than the policy.",
    "start": "4714560",
    "end": "4723525"
  },
  {
    "text": "Um, it also typically requires, um, a smaller amount of data, or at least a smaller amount of data that's supervised based on the reward.",
    "start": "4723525",
    "end": "4730945"
  },
  {
    "text": "Um, the downsides of models is that they don't optimize for task performance, and so there may be a mismatch in the optimized the- the-",
    "start": "4730945",
    "end": "4736954"
  },
  {
    "text": "the objective you're optimizing for and the objective that you might care about. We saw the same thing in their representation learning setting if we're trying to learn",
    "start": "4736955",
    "end": "4742790"
  },
  {
    "text": "a representation for reconstruction versus for the task that we care about. Um, and sometimes, it's also hard to learn the model than to learn the policies such as",
    "start": "4742790",
    "end": "4750620"
  },
  {
    "text": "in the pouring example where you have to model fluid dynamics. Um, and then sometimes you may also need assumptions to learn complex skills,",
    "start": "4750620",
    "end": "4759530"
  },
  {
    "text": "uh, such as the spatula, for example. And then for model-free methods, uh, the benefits is that it makes very little assumptions beyond a reward function.",
    "start": "4759530",
    "end": "4766640"
  },
  {
    "text": "Uh, it's very effective for learning, uh, complex policies, uh, and learning complex skills.",
    "start": "4766640",
    "end": "4772114"
  },
  {
    "text": "Uh, the downsides is that it requires a lot of experience and can be slower to learn, um, and in the multitask-learning setting in particular,",
    "start": "4772115",
    "end": "4779060"
  },
  {
    "text": "it's a harder optimization problem because you have to learn a policy to perform all the tasks, uh, rather than just learning a model and inverting that model for an individual task.",
    "start": "4779060",
    "end": "4786935"
  },
  {
    "text": "For each individual task, you have a test pattern. Uh, and then, I guess the last thing is that I don't think we necessarily have this dichot- dichotomy,",
    "start": "4786935",
    "end": "4793580"
  },
  {
    "text": "I think that ultimately we probably want elements of both, such that when we're pouring water, we use a model-free approach and such that when we're, uh,",
    "start": "4793580",
    "end": "4799715"
  },
  {
    "text": "maybe pushing objects around we have more of a model-based approach. Okay. Um, in the next few weeks,",
    "start": "4799715",
    "end": "4805625"
  },
  {
    "text": "I will be talking about, uh, kind- this is, I guess, sort of the conclusion of some of the RL section of the course and, uh,",
    "start": "4805625",
    "end": "4812060"
  },
  {
    "text": "on Monday, next week, we'll be talking about what about seeing tasks in sequence. Uh, we will cover this both in",
    "start": "4812060",
    "end": "4817700"
  },
  {
    "text": "the supervised setting and in the reinforcement learning setting. Um, on Wednesday will be have paper presentations on some miscellaneous topics that are,",
    "start": "4817700",
    "end": "4825230"
  },
  {
    "text": "are interesting with- relating to task interference, differentiability, uh, sim2real methods and hybrid reinforcement learning methods.",
    "start": "4825230",
    "end": "4832880"
  },
  {
    "text": "Uh, and then the following three lectures will be about really the current frontiers of these approaches.",
    "start": "4832880",
    "end": "4838640"
  },
  {
    "text": "So we'll have a guest lecturer from Jeff Clune who works on evolutionary methods, lifelong-learning, and meta-learning.",
    "start": "4838640",
    "end": "4843650"
  },
  {
    "text": "Uh, we'll have a guest lecturer from Sergey Levine on information theoretic exploration approaches and how that can be used for task agnostic reinforcement learning.",
    "start": "4843650",
    "end": "4851570"
  },
  {
    "text": "And, uh, on Monday of th- a couple of weeks after Thanksgiving, I'll be giving some perspectives on some challenges and,",
    "start": "4851570",
    "end": "4857990"
  },
  {
    "text": "and frontiers of these topics. Uh, and then, just a couple of reminders, Homework 3 is due tonight and the product milestone is due next week.",
    "start": "4857990",
    "end": "4866870"
  },
  {
    "text": "I'll see- yeah, question. The question is, um,",
    "start": "4866870",
    "end": "4873830"
  },
  {
    "text": "regards to model-based versus model free. Especially in a meta-learning context which one is the more beneficial [inaudible] Like, which one is easier?",
    "start": "4873830",
    "end": "4881599"
  },
  {
    "text": "For sim2real, I've actually seen both used. I've seen both model-based methods to try to learn a model that's robust to different,",
    "start": "4881600",
    "end": "4887810"
  },
  {
    "text": "um, different contexts and then using that model, um, to plan. I've also seen model-free approaches and that we'll see- uh,",
    "start": "4887810",
    "end": "4895625"
  },
  {
    "text": "I think we'll see a model-free approach in the paper that's covered on, on Wednesday or, uh, yeah, on Wednesday next week.",
    "start": "4895625",
    "end": "4901100"
  },
  {
    "text": "[inaudible] adaptations and model changes.",
    "start": "4901100",
    "end": "4907770"
  },
  {
    "text": "Yeah. So there are definitely very recent papers that have looked at meta-learning for sim2real where you try to,",
    "start": "4908860",
    "end": "4914630"
  },
  {
    "text": "instead of learning a robust model, you would try to learn an adaptable model such that you can adapt to any possible simulator and then at test time you're",
    "start": "4914630",
    "end": "4920870"
  },
  {
    "text": "given the real world and you want to be able to adapt to the real world. Um, there's at least one paper that came out in the",
    "start": "4920870",
    "end": "4927020"
  },
  {
    "text": "last like two months that studied meta-learning for that problem. Um, so I think that's kind of like, the cusp of- of where current research is on.",
    "start": "4927020",
    "end": "4934739"
  },
  {
    "text": "Okay. Great. See everyone on Monday.",
    "start": "4936130",
    "end": "4940350"
  }
]